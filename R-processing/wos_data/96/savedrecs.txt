FN Clarivate Analytics Web of Science
VR 1.0
PT J
AU Kartashova, T
   te Pas, SF
   de Ridder, H
   Pont, SC
AF Kartashova, Tatiana
   te Pas, Susan F.
   de Ridder, Huib
   Pont, Sylvia C.
TI Light Shapes: Perception-Based Visualizations of the Global Light
   Transport
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Light transport; visualization; lighting design; perception
ID ILLUMINATION; FIELD; DIFFUSENESS
AB In computer graphics, illuminating a scene is a complex task, typically consisting of cycles of adjusting and rendering the scene to see the effects. We propose a technique for visualization of light as a tensor field via extracting its properties (i.e., intensity, direction, diffuseness) from (virtual) radiance measurements and showing these properties as a grid of shapes over a volume of a scene. Presented in the viewport, our visualizations give an understanding of the illumination conditions in the measured volume for both the local values and the global variations of light properties. Additionally, they allow quick inferences of the resulting visual appearance of (objects in) scenes without the need to render them. In our evaluation, observers performed at least as well using visualizations as using renderings when they were comparing illumination between parts of a scene and inferring the final appearance of objects in the measured volume. Therefore, the proposed visualizations are expected to help lighting artists by providing perceptually relevant information about the structure of the light field and flow in a scene.
C1 [Kartashova, Tatiana; de Ridder, Huib; Pont, Sylvia C.] Delft Univ Technol, Delft, Netherlands.
   [te Pas, Susan F.] Univ Utrecht, Utrecht, Netherlands.
C3 Delft University of Technology; Utrecht University
RP Kartashova, T (corresponding author), Delft Univ Technol, Delft, Netherlands.
EM kartashovata@gmail.com; s.tepas@uu.nl; H.deRidder@tudelft.nl;
   S.C.Pont@tudelft.nl
OI Kartashova, Tatiana/0000-0002-8299-260X; Pont,
   Sylvia/0000-0002-9834-9600
FU EU FP7 Marie Curie Initial Training Networks (ITN) project PRISM,
   Perceptual Representation of Illumination, Shape and Material
   [PITN-GA-2012-316746]
FX This work has been funded by the EU FP7 Marie Curie Initial Training
   Networks (ITN) project PRISM, Perceptual Representation of Illumination,
   Shape and Material (PITN-GA-2012-316746).
CR [Anonymous], HDB EXPT PHENOMENOLO
   [Anonymous], LIGHTING BY DESIGN
   [Anonymous], 1973, ARCHITECTURAL PSYCHO
   Ashdown I, 1998, IESNA ANNUAL CONFERENCE TECHNICAL PAPERS, P437
   Belhumeur PN, 1999, INT J COMPUT VISION, V35, P33, DOI 10.1023/A:1008154927611
   Birn J., 2013, Digital Lighting and Rendering
   Brooker Darren, 2006, ESSENTIAL CG LIGHTIN
   Chajdas Matthaus G., 2011, P SIGRAD 2011
   Cuttle C., 2013, LIGHTING RES TECHNOL, V46, P31
   Descottes Herve., 2013, Architectural lighting: designing with light and space
   Durand F, 2005, ACM T GRAPHIC, V24, P1115, DOI 10.1145/1073204.1073320
   Fleming RW, 2004, J VISION, V4, P798, DOI 10.1167/4.9.10
   Fleming RW, 2003, J VISION, V3, P347, DOI 10.1167/3.5.3
   Gerhard HE, 2010, J VISION, V10, DOI 10.1167/10.9.1
   Gershbein R, 2000, COMP GRAPH, P353, DOI 10.1145/344779.344938
   Gershun Andrei, 1939, J MATH PHYS, V18, P51, DOI [10.1002/sapm193918151, DOI 10.1002/SAPM193918151]
   Gribble C., 2012, P ACM SIGGRAPH S INT, P71
   Hullin Matthias B., 2008, P VMV 2008
   Hunter Fil., 2007, LIGHT SCI MAGIC, V3rd
   Innes M., 2012, Lighting for Interior Design
   Jacobs A., 2014, Radiance Cookbook
   Kartashova T, 2016, J VISION, V16, DOI 10.1167/16.10.9
   Kelly Richard., 1952, College Art Journal, V12, P24, DOI [DOI 10.2307/773361, 10.2307/773361]
   Kim Y, 2009, LECT NOTES COMPUT SC, V5876, P59
   Kindlmann GordonL., 2004, Joint Eurographics - IEEE TCVG Symposium on Visualization, P147, DOI DOI 10.2312/VISSYM/VISSYM04/147-154
   Koenderink JJ, 2007, PERCEPTION, V36, P1595, DOI 10.1068/p5672
   Koenderink JJ, 2003, J OPT SOC AM A, V20, P987, DOI 10.1364/JOSAA.20.000987
   Koenderink JJ., 2003, The visual neurosciences, V2, P1090, DOI DOI 10.1007/0-387-28831-7
   Lopez-Moreno J., 2010, Proceedings of APGV, P25, DOI DOI 10.1145/1836248.1836252
   Maloney LT, 2010, J VISION, V10, DOI 10.1167/10.9.19
   Marlow PJ, 2012, CURR BIOL, V22, P1909, DOI 10.1016/j.cub.2012.08.009
   Morgenstern Y, 2014, J VISION, V14, DOI 10.1167/14.9.15
   Mury AA, 2007, APPL OPTICS, V46, P7308, DOI 10.1364/AO.46.007308
   Mury AA, 2009, APPL OPTICS, V48, P5386, DOI 10.1364/AO.48.005386
   Nowrouzezahrai D, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964924
   O'Shea JP, 2010, J VISION, V10, DOI 10.1167/10.12.21
   Ostrovsky Y, 2005, PERCEPTION, V34, P1301, DOI 10.1068/p5418
   Pellacini F, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1243980.1243983, 10.1145/1276377.1276444, 10.1145/1239451.1239505]
   Pont SC, 2007, PERCEPT PSYCHOPHYS, V69, P459, DOI 10.3758/BF03193766
   Pont SC, 2006, PERCEPTION, V35, P1331, DOI 10.1068/p5440
   Pont SC, 2015, PROC SPIE, V9394, DOI 10.1117/12.2085953
   Pont Sylvia C., 2018, RETREAT DARKNESS PHE
   Ramamoorthi R, 2001, J OPT SOC AM A, V18, P2448, DOI 10.1364/JOSAA.18.002448
   Ramamoorthi R, 2001, COMP GRAPH, P497, DOI 10.1145/383259.383317
   Ramamoorthi R, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1189762.1189764, 10.1145/1186644.1186646]
   Reiner T, 2012, COMPUT GRAPH FORUM, V31, P711, DOI 10.1111/j.1467-8659.2012.03050.x
   Russell S., 2008, ARCHITECTURE LIGHT
   Schirillo J, 2013, PSYCHON B REV, V20, P905, DOI 10.3758/s13423-013-0408-1
   Schmidt TW, 2016, COMPUT GRAPH FORUM, V35, P216, DOI 10.1111/cgf.12721
   Schmidt TW, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461980
   Schultz T, 2010, IEEE T VIS COMPUT GR, V16, P1595, DOI 10.1109/TVCG.2010.199
   Simons G, 2016, VISION MODELING VISU
   Sorger J, 2016, IEEE T VIS COMPUT GR, V22, P290, DOI 10.1109/TVCG.2015.2468011
   Toscani M, 2016, J VISION, V16, DOI 10.1167/16.15.21
   van Doorn AJ, 2012, I-PERCEPTION, V3, P467, DOI 10.1068/i0504
   Velten A., 2013, ACM Trans. Graph, V32, P44, DOI [DOI 10.1145/2461912.2461928, 10. 1145/2461912.2461928]
   Ward G. J., 1994, Computer Graphics Proceedings. Annual Conference Series 1994. SIGGRAPH 94 Conference Proceedings, P459, DOI 10.1145/192161.192286
   Westin CF, 1999, LECT NOTES COMPUT SC, V1679, P441
   Xia L, 2017, LIGHTING RES TECHNOL, V49, P428, DOI 10.1177/1477153516631392
   Xia L, 2017, LIGHTING RES TECHNOL, V49, P411, DOI 10.1177/1477153516631391
   Xia L, 2017, I-PERCEPTION, V8, DOI 10.1177/2041669516686089
   Xia L, 2014, I-PERCEPTION, V5, P613, DOI 10.1068/i0654
   Zhang F, 2015, PROC SPIE, V9394, DOI 10.1117/12.2085021
   Zirr Tobias, 2015, EUROGRAPHICS C VISUA, V34
NR 64
TC 4
Z9 4
U1 0
U2 2
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD FEB
PY 2019
VL 16
IS 1
AR 4
DI 10.1145/3232851
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA HN7TC
UT WOS:000460393600004
OA Bronze
DA 2024-07-18
ER

PT J
AU Cooke, T
   Wallraven, C
   Bülthoff, HH
AF Cooke, Theresa
   Wallraven, Christian
   Buelthoff, Heinrich H.
TI Multidimensional Scaling Analysis of Haptic Exploratory Procedures
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Measurement; Design; Reliability; Haptic; exploratory procedure; shape;
   texture; similarity; multidimensional scaling
ID OBJECT RECOGNITION; SALIENCE; SIMILARITY; PERCEPTION; VISION; TOUCH;
   SHAPE
AB Previous work in real and virtual settings has shown that the way in which we interact with objects plays a fundamental role in the way we perceive them. This article uses multidimensional scaling (MDS) analysis to further characterize and quantify the effects of using different haptic exploratory procedures (EPs) on perceptual similarity spaces. In Experiment 1, 20 participants rated similarity on a set of nine novel, 3D objects varying in shape and texture after either following their contours, laterally rubbing their centers, gripping them, or sequentially touching their tips. MDS analysis was used to recover perceptual maps of the objects and relative weights of perceptual dimensions from similarity data. Both the maps and relative weights of shape/texture properties were found to vary as a function of the EP used. In addition, large individual differences in the relative weight of shape/texture were observed. In Experiment 2, 17 of the previous participants repeated Experiment 1 after an average of 105 days. The same patterns of raw similarity ratings, perceptual maps, dimension weights, and individual differences were observed, indicating that perceptual similarities remained stable over time. The findings underscore the role of hand movements and individual biases in shaping haptic perceptual similarity. A framework for validating multimodal virtual displays based on the approach used in the study is also presented.
C1 [Cooke, Theresa; Wallraven, Christian; Buelthoff, Heinrich H.] Max Planck Inst Biol Cybernet, D-72076 Tubingen, Germany.
C3 Max Planck Society
RP Cooke, T (corresponding author), Max Planck Inst Biol Cybernet, D-72076 Tubingen, Germany.
EM Theresa.cooke@gmail.com
RI Bülthoff, Heinrich H/J-6579-2012; Bülthoff, Heinrich/AAC-8818-2019
OI Bülthoff, Heinrich H/0000-0003-2568-0607; Wallraven,
   Christian/0000-0002-2604-9115
CR [Anonymous], 2002, Handbook of Psychology, DOI DOI 10.1002/0471264385.WEI0406
   [Anonymous], 1925, WORLD TOUCH
   [Anonymous], 2001, MULTIDIMENSIONAL SCA
   Barsalou LW, 1999, BEHAV BRAIN SCI, V22, P577, DOI 10.1017/S0140525X99532147
   BIEDERMAN I, 1987, PSYCHOL REV, V94, P115, DOI 10.1037/0033-295X.94.2.115
   Borg I., 2005, Modern Multidimensional Scaling: Theory and Applications
   BULTHOFF HH, 1992, P NATL ACAD SCI USA, V89, P60, DOI 10.1073/pnas.89.1.60
   CARROLL JD, 1970, PSYCHOMETRIKA, V35, P283, DOI 10.1007/BF02310791
   Cooke T, 2007, NEUROPSYCHOLOGIA, V45, P484, DOI 10.1016/j.neuropsychologia.2006.02.009
   Dostmohamed H, 2005, EXP BRAIN RES, V164, P387, DOI 10.1007/s00221-005-2262-5
   Edelman Shimon., 1999, REPRESENTATION RECOG
   Ernst MO, 2004, TRENDS COGN SCI, V8, P162, DOI 10.1016/j.tics.2004.02.002
   GARBIN CP, 1984, PERCEPT PSYCHOPHYS, V36, P104, DOI 10.3758/BF03202671
   Gibson J., 1979, The ecological approach to visual perception
   Goldratt R., 2005, THEORY CONSTRAINTS, P13, DOI DOI 10.2277/0521531012
   HOLLINS M, 1993, PERCEPT PSYCHOPHYS, V54, P697, DOI 10.3758/BF03211795
   JANSSON G, 2004, TOUCH BLINDNESS NEUR
   KLATZKY RL, 1987, J EXP PSYCHOL GEN, V116, P356
   Lakatos S, 1999, PERCEPT PSYCHOPHYS, V61, P895, DOI 10.3758/BF03206904
   LEDERMAN SJ, 1993, ACTA PSYCHOL, V84, P29, DOI 10.1016/0001-6918(93)90070-8
   Lederman SJ, 1996, PERCEPTION, V25, P983, DOI 10.1068/p250983
   LESKOVSKY P, 2006, P EUROHAPTICS IEEE L
   Newell FN, 2001, PSYCHOL SCI, V12, P37, DOI 10.1111/1467-9280.00307
   Noe A, 2006, ACTION PERCEPTION
   NOSOFSKY RM, 1992, ANNU REV PSYCHOL, V43, P25, DOI 10.1146/annurev.ps.43.020192.000325
   O'Regan JK, 2001, BEHAV BRAIN SCI, V24, P939, DOI 10.1017/S0140525X01000115
   Pont SC, 1999, PERCEPT PSYCHOPHYS, V61, P874, DOI 10.3758/BF03206903
   Riley MA, 2002, PERCEPTION, V31, P481, DOI 10.1068/p3176
   Roland PE, 1998, P NATL ACAD SCI USA, V95, P3295, DOI 10.1073/pnas.95.6.3295
   ROSCH E, 1975, COGNITIVE PSYCHOL, V7, P573, DOI 10.1016/0010-0285(75)90024-9
   SHEPARD RN, 1973, COGNITIVE PSYCHOL, V4, P351, DOI 10.1016/0010-0285(73)90018-2
   Simons DJ, 1996, PSYCHOL SCI, V7, P301, DOI 10.1111/j.1467-9280.1996.tb00378.x
   SRINIVASAN MA, 1991, WENNERGREN INT S SER, P59
   Tiest WMB, 2006, ACTA PSYCHOL, V121, P1, DOI 10.1016/j.actpsy.2005.04.005
   WIPPICH W, 1994, Z EXP ANGEW PSYCHOL, V41, P500
   YOUNG F, 2003, SPSS 12 0 COMMAND SY, P100
NR 36
TC 13
Z9 13
U1 0
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD JAN
PY 2010
VL 7
IS 1
AR 7
DI 10.1145/1658349.1658356
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA 549EK
UT WOS:000274028400007
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Durgin, FH
   Reed, C
   Tigue, C
AF Durgin, Frank H.
   Reed, Catherine
   Tigue, Cara
TI Step Frequency and Perceived Self-Motion
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Human Factors; Perception; Locomotion; gait; walk ratio; virtual reality
   (VR); head-mounted displays (HMD); treadmill
ID PERCEPTION; WALKING; SPEED; LOCOMOTION; HUMANS; LENGTH
AB There is a discrepancy between the ability to correctly match the gains of visual and motor speed in virtual reality (VR) when walking on solid ground and the failure of this ability when walking on a treadmill. Moreover, this discrepancy has been found to interact with effects of the structure of the visual environment. The authors used a high-fidelity treadmill VR system to reproduce the high interactivity of normal walking in wide-area VR. Under these conditions, it was found that gain matches in a richly structured near environment differ by only about 10% in treadmill VR from matches in wide-area VR and that trial-to-trial variations in step frequency predicted changes in perceived locomotor speed. Gait differences resulting from treadmill walking (which are shown not to be a product of wearing a head-mounted display), apparently lead to an overestimation of motor speed on treadmills. When the near visual environment represented an empty hallway, additional errors were present that could be accounted for by known illusions in the perception of visual speed during self-motion. A study of normal gait at different speeds measured by head-tracker is reported as evidence of other possible sources of perceptual estimates of locomotor speed in normal walking.
C1 [Durgin, Frank H.; Reed, Catherine; Tigue, Cara] Swarthmore Coll, Dept Psychol, Swarthmore, PA 19081 USA.
C3 Swarthmore College
RP Durgin, FH (corresponding author), Swarthmore Coll, Dept Psychol, 500 Coll Ave, Swarthmore, PA 19081 USA.
EM Fdurgin1@swarthmore.edu
OI Durgin, Frank/0000-0001-9132-0074
CR [Anonymous], 2005, ACM Transactions on Applied Perception, DOI [DOI 10.1145/1077399.1077403, DOI 10.1145/1077399.10774032,3,9]
   Banton T, 2005, PRESENCE-TELEOP VIRT, V14, P394, DOI 10.1162/105474605774785262
   Barlow H.B., 1990, Vision: Coding and effeciency, P363, DOI 10.1017/cbo9780511626197.034
   Durgin F. H., 2004, Journal of Vision, V4, P802, DOI DOI 10.1167/4.8.802
   Durgin FH, 2005, PROC SPIE, V5666, P503, DOI 10.1117/12.610864
   Durgin FH, 2005, J EXP PSYCHOL HUMAN, V31, P339, DOI 10.1037/0096-1523.31.2.339
   Ernst MO, 2002, NATURE, V415, P429, DOI 10.1038/415429a
   GRIEVE DW, 1966, ERGONOMICS, V9, P379, DOI 10.1080/00140136608964399
   Hirasaki E, 1999, EXP BRAIN RES, V127, P117, DOI 10.1007/s002210050781
   HOLT KG, 1995, J MOTOR BEHAV, V27, P164, DOI 10.1080/00222895.1995.9941708
   LISHMAN JR, 1973, PERCEPTION, V2, P287, DOI 10.1068/p020287
   Loomis JM, 2003, VIRTUAL AND ADAPTIVE ENVIRONMENTS: APPLICATIONS, IMPLICATIONS, AND HUMAN PERFORMANCE ISSUES, P21
   LOOMIS JM, 1992, J EXP PSYCHOL HUMAN, V18, P906, DOI 10.1037/0096-1523.18.4.906
   Mergner T, 1998, BRAIN RES REV, V28, P118, DOI 10.1016/S0165-0173(98)00032-0
   Mittelstaedt ML, 2001, EXP BRAIN RES, V139, P318, DOI 10.1007/s002210100735
   Mohler BJ., 2004, Assoc Comput Mach, P19, DOI DOI 10.1145/1012551.1012554
   Pelah A, 1996, NATURE, V381, P283, DOI 10.1038/381283a0
   Pelah A., 2001, J VISUAL-JAPAN, V1, P307, DOI [10.1167/1.3.307, DOI 10.1167/1.3.307]
   Prokop T, 1997, EXP BRAIN RES, V114, P63, DOI 10.1007/PL00005624
   R Core Development Team, 2006, R LANG ENV STAT COMP
   RIESER JJ, 1990, PERCEPTION, V19, P675, DOI 10.1068/p190675
   SCHAFFER ES, 2005, J VISION, V5, pA332, DOI DOI 10.1167/5.8.322
   Sekiya N, 1996, J HUM MOVEMENT STUD, V30, P241
   Terrier P, 2003, EUR J APPL PHYSIOL, V90, P554, DOI 10.1007/s00421-003-0906-3
   THIBODEAU P, 2005, ANN M VIS SCI SOC SA
   Thompson WB, 2004, PRESENCE-TELEOP VIRT, V13, P560, DOI 10.1162/1054746042545292
   THOMSON JA, 1983, J EXP PSYCHOL HUMAN, V9, P427, DOI 10.1037/0096-1523.9.3.427
   Wand M, 2005, KERNSMOOTH FUNCTIONS
   Wand M.P., 1994, KERNEL SMOOTHING
   Willemsen P., 2004, P 1 S APPL PERC GRAP, P35, DOI DOI 10.1145/1012551.1012558
NR 30
TC 28
Z9 31
U1 0
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD JAN
PY 2007
VL 4
IS 1
AR 5
DI 10.1145/1227134.1227139
PG 23
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA V04IL
UT WOS:000207052000005
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Wang, MQ
   Cooper, EA
AF Wang, Minqi
   Cooper, Emily A.
TI Perceptual Guidelines for Optimizing Field of View in Stereoscopic
   Augmented Reality Displays
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Augmented reality; binocular vision; near-eye display; field of view
AB Near-eye display systems for augmented reality (AR) aim to seamlessly merge virtual content with the user's view of the real-world. A substantial limitation of current systems is that they only present virtual content over a limited portion of the user's natural field of view (FOV). This limitation reduces the immersion and utility of these systems. Thus, it is essential to quantify FOV coverage in AR systems and understand how to maximize it. It is straightforward to determine the FOV coverage for monocular AR systems based on the system architecture. However, stereoscopic AR systems that present 3D virtual content create a more complicated scenario because the two eyes' views do not always completely overlap. The introduction of partial binocular overlap in stereoscopic systems can potentially expand the perceived horizontal FOV coverage, but it can also introduce perceptual nonuniformity artifacts. In this arrticle, we first review the principles of binocular FOV overlap for natural vision and for stereoscopic display systems. We report the results of a set of perceptual studies that examine how different amounts and types of horizontal binocular overlap in stereoscopic AR systems influence the perception of nonuniformity across the FOV. We then describe how to quantify the horizontal FOV in stereoscopic AR when taking 3D content into account. We show that all stereoscopic AR systems result in a variable horizontal FOV coverage and variable amounts of binocular overlap depending on fixation distance. Taken together, these results provide a framework for optimizing perceived FOV coverage and minimizing perceptual artifacts in stereoscopic AR systems for different use cases.
C1 [Wang, Minqi] Univ Calif Berkeley, Herbert Wertheim Sch Optometry & Vis Sci, 381 Minor Hall, Berkeley, CA 94720 USA.
   [Cooper, Emily A.] Univ Calif Berkeley, Herbert Wertheim Sch Optometry & Vis Sci, Helen Wills Neurosci Inst, 381 Minor Hall, Berkeley, CA 94720 USA.
C3 University of California System; University of California Berkeley;
   University of California System; University of California Berkeley
RP Wang, MQ (corresponding author), Univ Calif Berkeley, Herbert Wertheim Sch Optometry & Vis Sci, 381 Minor Hall, Berkeley, CA 94720 USA.
EM mwang67@berkeley.edu; emilycooper@berkeley.edu
OI Cooper, Emily/0000-0003-4889-7446
FU National Science Foundation [2041726]; Center for Innovation in Vision
   and Optics at University of California, Berkeley; Direct For Computer &
   Info Scie & Enginr [2041726] Funding Source: National Science
   Foundation; Div Of Information & Intelligent Systems [2041726] Funding
   Source: National Science Foundation
FX This work was supported by a grant from the National Science Foundation
   (2041726) and a fellowship from the Center for Innovation in Vision and
   Optics at University of California, Berkeley.
CR Adams WJ, 2016, SCI REP-UK, V6, DOI 10.1038/srep35805
   Aizenman AM, 2023, ACM T GRAPHIC, V42, DOI 10.1145/3549529
   ALAM MS, 1992, PROC NAECON IEEE NAT, P1249, DOI 10.1109/NAECON.1992.220578
   ANDERSON BL, 1994, PSYCHOL REV, V101, P414, DOI 10.1037/0033-295X.101.3.414
   Banks MS, 2016, ANNU REV VIS SCI, V2, P397, DOI 10.1146/annurev-vision-082114-035800
   Basgöze Z, 2020, J VISION, V20, DOI 10.1167/jov.20.8.10
   Blake R, 2011, VISION RES, V51, P754, DOI 10.1016/j.visres.2010.10.009
   Borisov Vladimir N., 2021, P OPTICAL ARCHITECTU, P94
   Bradski G, 2000, DR DOBBS J, V25, P120
   Cakmakci Ozan, 2019, SID Symposium Digest of Technical Papers, V50, P438, DOI 10.1002/sdtp.12950
   Cholewiak SA, 2020, OPT EXPRESS, V28, P38008, DOI 10.1364/OE.408404
   DeHoog E, 2016, APPL OPTICS, V55, P5924, DOI 10.1364/AO.55.005924
   Ellis S.R., 2002, NTRS NASA TECHNICAL
   Freepik, 2017, AUTH FREEP
   Grigsby S. S., 1994, Journal of the Society for Information Display, V2, P69, DOI 10.1889/1.1984914
   Harris JM, 2009, VISION RES, V49, P2666, DOI 10.1016/j.visres.2009.06.021
   Held RT, 2008, APGV 2008: PROCEEDINGS OF THE SYMPOSIUM ON APPLIED PERCEPTION IN GRAPHICS AND VISUALIZATION, P23
   Hou Q, 2016, Opt Des Test VII, SPIE, V10021, P52
   Jennings S, 1997, PROCEEDINGS OF THE HUMAN FACTORS AND ERGONOMICS SOCIETY 41ST ANNUAL MEETING, 1997, VOLS 1 AND 2, P32
   KLYMENKO V, 1994, P SOC PHOTO-OPT INS, V2218, P82, DOI 10.1117/12.177351
   Klymenko Victor, 2001, 200105 USAARL
   Klymenko Victor, 1994, 9429 USAARL
   Klymenko Victor, 1999, 9919 USAARL
   Kolb Helga, 2007, WEBVISION
   Koulieris GA, 2019, COMPUT GRAPH FORUM, V38, P493, DOI 10.1111/cgf.13654
   Kress B. C., 2019, DIGITAL OPTICAL TECH, V11062, P75
   Kress BC, 2021, NANOPHOTONICS-BERLIN, V10, P41, DOI 10.1515/nanoph-2020-0410
   Kress BC, 2019, PROC SPIE, V11062, DOI 10.1117/12.2544404
   Lin JJW, 2002, P IEEE VIRT REAL ANN, P164, DOI 10.1109/VR.2002.996519
   McLean B., 1987, Proceedings of the SPIE - The International Society for Optical Engineering, V778, P79, DOI 10.1117/12.940469
   Melzer J. E., 1989, Proceedings of the SPIE - The International Society for Optical Engineering, V1117, P56, DOI 10.1117/12.960922
   Melzer James E., 1991, P LARGE SCREEN PROJE, P124
   Ni T, 2006, PROC GRAPH INTERF, P139
   Patterson R, 2006, HUM FACTORS, V48, P555, DOI 10.1518/001872006778606877
   Rash Clarence E., 1999, 9908 USAARL
   Ren D, 2016, P IEEE VIRT REAL ANN, P93, DOI 10.1109/VR.2016.7504692
   Shi ZJ, 2018, PROC SPIE, V10676, DOI 10.1117/12.2315635
   Spector R. H., 1990, CLIN METHODS HIST PH, V3rd
   Trepkowski C, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P575, DOI [10.1109/vr.2019.8798312, 10.1109/VR.2019.8798312]
   Watson AB, 2018, Elect Imag, V2018, P1, DOI [10.2352/J.Percept.Imaging.2018.1.1.010505, DOI 10.2352/J.PERCEPT.IMAGING.2018.1.1.010505]
   WELLS MJ, 1989, P SOC PHOTO-OPT INS, V1116, P126
   Xiong JH, 2020, OSA CONTINUUM, V3, P2730, DOI 10.1364/OSAC.400900
   Zhan T, 2020, ISCIENCE, V23, DOI 10.1016/j.isci.2020.101397
NR 43
TC 2
Z9 2
U1 1
U2 7
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD OCT
PY 2022
VL 19
IS 4
SI SI
AR 19
DI 10.1145/3554921
PG 23
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 6J6QI
UT WOS:000886946700006
DA 2024-07-18
ER

PT J
AU Zhang, T
   O'Hare, L
   Hibbard, PB
   Nefs, HT
   Heynderickx, I
AF Zhang, Tingting
   O'Hare, Louise
   Hibbard, Paul B.
   Nefs, Harold T.
   Heynderickx, Ingrid
TI Depth of Field Affects Perceived Depth in Stereographs
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Human Factors; Depth of field; depth perception; stereoscope; 3D TV;
   binocular disparity
ID VISUAL DISCOMFORT; DISPARITY; BLUR; DISTANCE; SHAPE; SIZE; INTEGRATION;
   STEREOPSIS; OBJECTS; CUES
AB Although it has been reported that depth of field influences depth perception in nonstereo photographs, it remains unclear how depth of field affects depth perception under stereo viewing conditions. We showed participants stereo photographs with different depths of field using a Wheatstone stereoscope and a commercially available 3D TV. The depicted scene contained a floor, a background, and a measuring probe at different locations. Participants drew a floor plan of the depicted scene to scale. We found that perceived depth decreased with decreasing depth of field for shallow depths of field in scenes containing a height-in- the-field cue. For larger depths of field, different effects were found depending on the display system and the viewing distance. There was no effect on perceived depth using the 3D TV, but perceived depth decreased with increasing depth of field using the Wheatstone stereoscope. However, in the 3D TV case, we found that the perceived depth decreased with increasing depth of field in scenes in which the height-in-the-field cue was removed. This indicates that the effect of depth of field on perceived depth may be influenced by other depth cues in the scene, such as height-in-the-field cues.
C1 [Zhang, Tingting; Nefs, Harold T.] Delft Univ Technol, NL-2600 AA Delft, Netherlands.
   [O'Hare, Louise] Lincoln Univ, Lincoln, England.
   [Hibbard, Paul B.] Univ Essex, Colchester CO4 3SQ, Essex, England.
   [Heynderickx, Ingrid] Eindhoven Univ Technol, NL-5600 MB Eindhoven, Netherlands.
C3 Delft University of Technology; University of Lincoln; University of
   Essex; Eindhoven University of Technology
RP Zhang, T (corresponding author), Mekelweg 4,HB 12-100, NL-2628 CD Delft, Netherlands.
EM t.zhang@tudelft.nl; lohare@lincoln.ac.uk; phibbard@essex.ac.uk;
   h.t.nefs@tudelft.nl; Heynderickx@tue.nl
RI O'Hare, Louise/AFM-2088-2022; Hibbard, Paul/AAD-4044-2020
OI O'Hare, Louise/0000-0003-0331-3646; Hibbard, Paul/0000-0001-6133-6729
FU CSC scholarship from the Chinese government; BBSRC
FX Tingting Zhang was supported by a CSC scholarship from the Chinese
   government. Louise O'Hare was funded by a doctoral training grant from
   the BBSRC.
CR [Anonymous], 2014, ACM T APPL PERCEPTIO, V11
   ARNDT PA, 1995, BIOL CYBERN, V72, P279, DOI 10.1007/BF00202784
   Banks MS, 2012, SMPTE MOTION IMAG J, V121, P24, DOI 10.5594/j18173
   Baveye Y, 2012, LECT NOTES COMPUT SC, V7594, P280, DOI 10.1007/978-3-642-33564-8_34
   Bradshaw MF, 1996, VISION RES, V36, P1255, DOI 10.1016/0042-6989(95)00190-5
   Costa MF, 2010, OPHTHAL PHYSL OPT, V30, P660, DOI 10.1111/j.1475-1313.2010.00750.x
   Cutting JE, 1995, PERCEPTION SPACE MOT, P69, DOI [DOI 10.1016/B978-012240530-3/50005-5, 10.1016/B978-012240530-3/50005-5]
   Datta R, 2006, LECT NOTES COMPUT SC, V3953, P288, DOI 10.1007/11744078_23
   ELLIOTT D, 1987, J MOTOR BEHAV, V19, P476
   Ernst MO, 2002, NATURE, V415, P429, DOI 10.1038/415429a
   GILINSKY AS, 1951, PSYCHOL REV, V58, P460, DOI 10.1037/h0061505
   Gooding Linda, 1991, Proceedings of the SPIE - The International Society for Optical Engineering, V1457, P259, DOI 10.1117/12.46314
   HAMERLY JR, 1981, J OPT SOC AM, V71, P448, DOI 10.1364/JOSA.71.000448
   He ZJJ, 2000, PERCEPTION, V29, P1313, DOI 10.1068/p3113
   Held RT, 2012, CURR BIOL, V22, P426, DOI 10.1016/j.cub.2012.01.033
   Held RT, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1731047.1731057
   Hillaire S, 2008, IEEE COMPUT GRAPH, V28, P47, DOI 10.1109/MCG.2008.113
   Hoffman DM, 2008, J VISION, V8, DOI 10.1167/8.3.33
   Hubona G. S., 1999, ACM Transactions on Computer-Human Interaction, V6, P214, DOI 10.1145/329693.329695
   JOHNSTON EB, 1991, VISION RES, V31, P1351, DOI 10.1016/0042-6989(91)90056-B
   Kaufman L., 1974, Sight and mind: An introduction to visual perception
   Khanna R., 2010, JOINT INT C POWER EL, P1
   Lambooij M, 2009, J IMAGING SCI TECHN, V53, DOI 10.2352/J.ImagingSci.Technol.2009.53.3.030201
   Li C, 2013, PROC CVPR IEEE, P217, DOI 10.1109/CVPR.2013.35
   Ling Y, 2013, COMPUT HUM BEHAV, V29, P1519, DOI 10.1016/j.chb.2012.12.010
   Loomis JM, 1998, PERCEPT PSYCHOPHYS, V60, P966, DOI 10.3758/BF03211932
   Mather G, 1996, P ROY SOC B-BIOL SCI, V263, P169, DOI 10.1098/rspb.1996.0027
   Mather G, 2000, VISION RES, V40, P3501, DOI 10.1016/S0042-6989(00)00178-4
   Nefs HT, 2012, SEEING PERCEIVING, V25, P577, DOI 10.1163/18784763-00002400
   O'Hare L, 2013, I-PERCEPTION, V4, P156, DOI 10.1068/i0566
   O'Kane LM, 2007, PERCEPTION, V36, P696, DOI 10.1068/p5406
   PENTLAND AP, 1987, IEEE T PATTERN ANAL, V9, P523, DOI 10.1109/TPAMI.1987.4767940
   RITTER M, 1977, PERCEPT PSYCHOPHYS, V22, P400, DOI 10.3758/BF03199707
   Rogers S., 1995, PERCEPTION SPACE MOT, V5, P119
   Sauer CW, 2001, PERCEPTION, V30, P681, DOI 10.1068/p3087
   Scarfe P, 2006, VISION RES, V46, P1599, DOI 10.1016/j.visres.2005.11.002
   Scarfe P, 2011, J VISION, V11, DOI 10.1167/11.7.12
   Shibata T, 2011, J VISION, V11, DOI 10.1167/11.8.11
   Vinnikov Margarita, 2014, P S EYE TRACK RES AP, P119, DOI DOI 10.1145/2578153.2578170
   Vishwanath D, 2013, PSYCHOL SCI, V24, P1673, DOI 10.1177/0956797613477867
   Wang J., 2011, P SOC PHOTO-OPT INS, V7865
   Watt SJ, 2005, J VISION, V5, P834, DOI 10.1167/5.10.7
NR 42
TC 13
Z9 14
U1 2
U2 41
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD JAN
PY 2015
VL 11
IS 4
AR 18
DI 10.1145/2667227
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA AZ6WJ
UT WOS:000348358400003
DA 2024-07-18
ER

PT J
AU Easa, HK
   Mantiuk, RK
   Lim, IS
AF Easa, Haider K.
   Mantiuk, Rafal K.
   Lim, Ik Soo
TI Evaluation of Monocular Depth Cues on a High-Dynamic-Range Display for
   Visualization
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article; Proceedings Paper
CT 10th Symposium on Applied Perception (SAP)
CY AUG, 2013
CL Trinity Coll, Dublin, IRELAND
HO Trinity Coll
DE Experimentation; Measurement; Performance; Verification; Reliability;
   Theory; HDR; monocular depth cues; depth ordering; depth perception;
   brightness; contrast; transparency; visualization
ID PERCEPTION; TRANSPARENCY; FIGURE; CONTRAST; SHADOWS; CONTEXT; CONTOUR;
   CAST; BLUR
AB The aim of this work is to identify the depth cues that provide intuitive depth-ordering when used to visualize abstract data. In particular we focus on the depth cues that are effective on a high-dynamic-range (HDR) display: contrast and brightness. In an experiment participants were shown a visualization of the volume layers at different depths with a single isolated monocular cue as the only indication of depth. The observers were asked to identify which slice of the volume appears to be closer. The results show that brightness, contrast and relative size are the most effective monocular depth cues for providing an intuitive depth ordering.
C1 [Easa, Haider K.] Erbil Polytech Univ, Shaqlawa Tech Inst, Erbil, Kurdistan Regio, Iraq.
   [Mantiuk, Rafal K.; Lim, Ik Soo] Bangor Univ, Sch Comp Sci, Bangor LL57 1UT, Gwynedd, Wales.
C3 Erbil Polytechnic University; Bangor University
RP Easa, HK (corresponding author), Erbil Polytech Univ, Shaqlawa Tech Inst, Erbil, Kurdistan Regio, Iraq.
EM haidereesa@yahoo.com; mantiuk@bangor.ac.uk; i.s.lim@bangor.ac.uk
RI Mantiuk, Rafał K./AAP-9514-2020; Lim, Ik Soo/GQH-4248-2022; Easa, Haider
   Khalil/HLH-8203-2023; Mantiuk, Rafał K. K/I-4209-2016
OI Mantiuk, Rafał K./0000-0003-2353-0349; Lim, Ik Soo/0000-0002-9499-8515;
   Mantiuk, Rafał K. K/0000-0003-2353-0349; Easa,
   Haider/0000-0002-8370-0816
CR ADELSON EH, 1990, P AAAI WORKSH QUAL V
   AKS J. T, 1996, J EXP PSYCHOL HUMAN, V22, p[6, 1467], DOI DOI 10.1037/0096-1523.22.6.1467
   Anderson BL, 1997, PERCEPTION, V26, P419, DOI 10.1068/p260419
   [Anonymous], 2002, RenderMan in Production
   [Anonymous], HDB PERCEPTION COGNI
   BAVOIL L., 2008, 107 NVID CORP
   BECK J, 1984, PERCEPT PSYCHOPHYS, V35, P407, DOI 10.3758/BF03203917
   Bertamini M, 2003, COGNITION, V87, P33, DOI 10.1016/S0010-0277(02)00183-X
   Bertamini M, 2008, PERCEPTION, V37, P483, DOI 10.1068/p5728
   Bertamini M, 2006, PERCEPTION, V35, P883, DOI 10.1068/p5496
   Cunningham Douglas W, 2011, EXPT DESIGN USER STU
   DALY S, 2013, SPIE P, V8651, DOI DOI 10.1117/12.2013161
   Delogu F, 2010, J VISION, V10, DOI 10.1167/10.2.19
   Elder JH, 2004, PERCEPTION, V33, P1319, DOI 10.1068/p5323
   FINEMAN MB, 1981, PERCEPT MOTOR SKILL, V53, P11, DOI 10.2466/pms.1981.53.1.11
   GIBSON JJ, 1950, AM J PSYCHOL, V63, P367, DOI 10.2307/1418003
   Held RT, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1731047.1731057
   Ichihara S, 2007, PERCEPTION, V36, P686, DOI 10.1068/p5696
   KASRAI R, 2004, THESIS MCGILL U CANA
   Krüger J, 2006, IEEE T VIS COMPUT GR, V12, P941, DOI 10.1109/TVCG.2006.124
   Mamassian P, 1998, TRENDS COGN SCI, V2, P288, DOI 10.1016/S1364-6613(98)01204-2
   Mantiuk R, 2009, COMPUT GRAPH FORUM, V28, P193, DOI 10.1111/j.1467-8659.2009.01358.x
   Marshall JA, 1996, J OPT SOC AM A, V13, P681, DOI 10.1364/JOSAA.13.000681
   Mather G, 2004, VISION RES, V44, P557, DOI 10.1016/j.visres.2003.09.036
   Mather G, 1996, P ROY SOC B-BIOL SCI, V263, P169, DOI 10.1098/rspb.1996.0027
   Mendez J.M., 2010, Graphics Programming and Theory
   OSHEA RP, 1994, VISION RES, V34, P1595, DOI 10.1016/0042-6989(94)90116-3
   Rempel A.G., 2011, Proc. of the ACM SIGGRAPH Symposium on Applied Perception in Graphics and Visualization, V26, P115, DOI DOI 10.1145/2077451.2077478
   Seetzen H, 2004, ACM T GRAPHIC, V23, P760, DOI 10.1145/1015706.1015797
   Singh M, 2002, PSYCHOL REV, V109, P492, DOI 10.1037//0033-295X.109.3.492
   SURDICK RT, 1994, HUM FAC ERG SOC P, P1305
   SWAIN C. T, 2000, US patent, Patent No. [6,157,733, 6157733]
   Vecera SP, 2002, J EXP PSYCHOL GEN, V131, P194, DOI 10.1037//0096-3445.131.2.194
   Viola I, 2005, IEEE T VIS COMPUT GR, V11, P408, DOI 10.1109/TVCG.2005.62
   WANAT R, 2012, P C THEOR PRACT COMP, P9
   Zheng L, 2013, IEEE T VIS COMPUT GR, V19, P446, DOI 10.1109/TVCG.2012.144
NR 36
TC 3
Z9 4
U1 2
U2 16
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD AUG
PY 2013
VL 10
IS 3
SI SI
AR 16
DI 10.1145/2504568
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 206FS
UT WOS:000323504700005
OA Bronze
DA 2024-07-18
ER

PT J
AU Mcdonnell, R
   Jörg, S
   Hodgins, JK
   Newell, F
   O'Sullivan, C
AF Mcdonnell, Rachel
   Joerg, Sophie
   Hodgins, Jessica K.
   Newell, Fiona
   O'Sullivan, Carol
TI Evaluating the Effect of Motion and Body Shape on the Perceived Sex of
   Virtual Characters
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
CT Symposium on Applied Perception in Graphics and Visualization
CY JUL 25-27, 2007
CL Tubingen, GERMANY
DE Experimentation; Human Factors; Perception; graphics; motion capture
ID BIOLOGICAL MOTION; PERCEPTION; WALKERS; GENDER; CUES
AB In this paper, our aim is to determine factors that influence the perceived sex of virtual characters. In Experiment 1, four different model types were used: highly realistic male and female models, an androgynous character, and a point light walker. Three different types of motion were applied to all models: motion captured male and female walks, and neutral synthetic walks. We found that both form and motion influence sex perception for these characters: for neutral synthetic motions, form determines perceived sex, whereas natural motion affects the perceived sex of both androgynous and realistic forms. These results indicate that the use of neutral walks is better than creating ambiguity by assigning an incongruent motion. In Experiment 2 we investigated further the influence of body shape and motion on realistic male and female models and found that adding stereotypical indicators of sex to the body shapes influenced sex perception. Also, that exaggerated female body shapes influences sex judgements more than exaggerated male shapes. These results have implications for variety and realism when simulating large crowds of virtual characters.
C1 [Mcdonnell, Rachel] Carnegie Mellon Univ, Trinity Coll, Dublin Sch Comp Sci, Graph Res Grp, Pittsburgh, PA 15213 USA.
C3 Carnegie Mellon University
RP Mcdonnell, R (corresponding author), Carnegie Mellon Univ, Trinity Coll, Dublin Sch Comp Sci, Graph Res Grp, Pittsburgh, PA 15213 USA.
RI McDonnell, Rachel/HGC-4337-2022; Newell, Fiona/AIE-2422-2022; soetaert,
   karline/A-9839-2011
OI McDonnell, Rachel/0000-0002-1957-2506; Newell,
   Fiona/0000-0002-7363-2346; O'Sullivan, Carol/0000-0003-3772-4961;
   soetaert, karline/0000-0003-4603-7100
CR BEARDSWORTH T, 1981, B PSYCHONOMIC SOC, V18, P19
   CUTTING JE, 1977, B PSYCHONOMIC SOC, V9, P353, DOI 10.3758/BF03337021
   CUTTING JE, 1978, PERCEPTION, V7, P393, DOI 10.1068/p070393
   Dobbyn S, 2006, EUROGR TECH REP SER, P103
   HALEVINA A, 2007, VIS SCI SOC M SAR FL
   Howell D., 1999, FUNDAMENTAL STAT BEH
   JOHANSSON G, 1973, PERCEPT PSYCHOPHYS, V14, P201, DOI 10.3758/BF03212378
   JOHANSSON G, 1976, PSYCHOL RES-PSYCH FO, V38, P379, DOI 10.1007/BF00309043
   Johnson KL, 2007, P NATL ACAD SCI USA, V104, P5246, DOI 10.1073/pnas.0608181104
   Johnson KL, 2005, PSYCHOL SCI, V16, P890, DOI 10.1111/j.1467-9280.2005.01633.x
   KOZLOWSKI L, 1977, PERCEPT PSYCHOPHYS, V21, P578
   KOZLOWSKI LT, 1978, PERCEPT PSYCHOPHYS, V23, P459, DOI 10.3758/BF03204150
   MATHER G, 1994, P ROY SOC B-BIOL SCI, V258, P273, DOI 10.1098/rspb.1994.0173
   Troje NF, 2002, J VISION, V2, P371, DOI 10.1167/2.5.2
   VINAYAGAMOORTHY V, 2006, EUROGRAPHICS STATE A
NR 15
TC 18
Z9 20
U1 0
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD JAN
PY 2009
VL 5
IS 4
AR 20
DI 10.1145/1462048.1462051
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 450YL
UT WOS:000266437800003
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Su, J
   Zhou, P
AF Su, Jun
   Zhou, Peng
TI Machine Learning-based Modeling and Prediction of the Intrinsic
   Relationship between Human Emotion and Music
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Music; machine learning; human emotion; statistical modeling;
   genre-diverse music library; psychophysiology
ID QUANTITATIVE STRUCTURE; GAUSSIAN-PROCESSES; QSAR; CLASSIFICATION;
   REGRESSION; CHILLS
AB Human emotion is one of the most complex psychophysiological phenomena and has been reported to be affected significantly by music listening. It is supposed that there is an intrinsic relationship between human emotion and music, which can be modeled and predicted quantitatively in a supervised manner. Here, a heuristic clustering analysis is carried out on large-scale free music archive to derive a genre-diverse music library, to which the emotional response of participants is measured using a standard protocol, consequently resulting in a systematic emotion-to-music profile. Eight machine learning methods are employed to statistically correlate the basic sound features of music audio tracks in the library with the measured emotional response of tested people to the music tracks in a training set and to blindly predict the emotional response from sound features in a test set.
   This study found that nonlinear methods are more robust and predictable but considerably more time-consuming than linear approaches. The neural networks have strong internal fittability but are associated with a significant overfitting issue. The support vector machine and Gaussian process exhibit both high internal stability and satisfactory external predictability in all used methods; they are considered as promising tools to model, predict, and explain the intrinsic relationship between human emotion and music. The psychological basis and perceptional implication underlying the built machine learning models are also discussed to find out the key music factors that affect human emotion.
C1 [Su, Jun] Chengdu Normal Univ, Coll Mus, 99 Haike Rd East Sect, Chengdu 611130, Peoples R China.
   [Zhou, Peng] Univ Elect Sci & Technol China UESTC, Ctr Informat Biol, Sch Life Sci & Technol, 2006 Xiyuan Ave, Chengdu 611731, Peoples R China.
C3 Chengdu Normal University; University of Electronic Science & Technology
   of China
RP Su, J (corresponding author), Chengdu Normal Univ, Coll Mus, 99 Haike Rd East Sect, Chengdu 611130, Peoples R China.
EM sujun@cdnu.edu.cn; p_zhou@uestc.edu.cn
OI Su, Jun/0000-0002-1784-2023
FU Sichuan Social Science Planning Project [SC22B167]; ShuDi Music
   Institute of Chengdu Normal University
FX This work was supported by the Sichuan Social Science Planning Project
   (No. SC22B167) and the ShuDi Music Institute of Chengdu Normal
   University.
CR Badgaiyan RD, 2009, NEUROIMAGE, V47, P2041, DOI 10.1016/j.neuroimage.2009.06.008
   BARONI M, 1993, QUANT STRUCT-ACT REL, V12, P225, DOI 10.1002/qsar.19930120302
   Bishop C, 1991, NEURAL COMPUT, V3, P579, DOI 10.1162/neco.1991.3.4.579
   Breiman L, 2001, MACH LEARN, V45, P5, DOI 10.1023/A:1010933404324
   Burden FR, 2001, J CHEM INF COMP SCI, V41, P830, DOI 10.1021/ci000459c
   Celeghin A, 2017, FRONT PSYCHOL, V8, DOI 10.3389/fpsyg.2017.01432
   Chipman HA, 1998, J AM STAT ASSOC, V93, P935, DOI 10.2307/2669832
   CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1007/BF00994018
   Cowen AS, 2017, P NATL ACAD SCI USA, V114, pE7900, DOI 10.1073/pnas.1702247114
   de Fleurian R, 2021, PSYCHOL BULL, V147, P890, DOI 10.1037/bul0000341
   Defferrard M., 2017, P INT SOC MUS INF RE, P316
   Dolan RJ, 2002, SCIENCE, V298, P1191, DOI 10.1126/science.1076358
   Ferreri L, 2019, P NATL ACAD SCI USA, V116, P3793, DOI 10.1073/pnas.1811878116
   Geethanjali B., 2018, Journal of Clinical and Diagnostic Research, V12, P1, DOI DOI 10.1111/CRJ.12687
   GELADI P, 1986, ANAL CHIM ACTA, V185, P1, DOI 10.1016/0003-2670(86)80028-9
   Golbraikh A, 2002, J MOL GRAPH MODEL, V20, P269, DOI 10.1016/S1093-3263(01)00123-1
   Grewe O, 2005, ANN NY ACAD SCI, V1060, P446, DOI 10.1196/annals.1360.041
   Grewe O, 2007, EMOTION, V7, P774, DOI 10.1037/1528-3542.7.4.774
   Gu SM, 2019, FRONT PSYCHOL, V10, DOI 10.3389/fpsyg.2019.00781
   Izard C. E., 1977, Human emotions, P43, DOI [DOI 10.1007/S10853-018-3152-0, 10.1007/978-1-4899-2209-0]
   Jalali-Heravi M, 2000, J CHEM INF COMP SCI, V40, P147, DOI 10.1021/ci990314+
   Koelsch S, 2005, ANN NY ACAD SCI, V1060, P412, DOI 10.1196/annals.1360.034
   Koelsch S, 2018, PLOS ONE, V13, DOI 10.1371/journal.pone.0190057
   LeDoux J, 2016, CELL, V167, P1443, DOI 10.1016/j.cell.2016.11.029
   Li ZY, 2019, CURR DRUG METAB, V20, P170, DOI 10.2174/1389200219666181012151944
   Liu Q, 2022, FRONT GENET, V12, DOI 10.3389/fgene.2021.800857
   Meneses A, 2012, REV NEUROSCIENCE, V23, P543, DOI 10.1515/revneuro-2012-0060
   Montgomery D.C., 2015, Introduction to linear regression analysis
   Nazeer KAA, 2011, J MED IMAG HEALTH IN, V1, P66, DOI 10.1166/jmihi.2011.1010
   Niedenthal PM, 2012, ANNU REV PSYCHOL, V63, P259, DOI 10.1146/annurev.psych.121208.131605
   Obrezanova O, 2007, J CHEM INF MODEL, V47, P1847, DOI 10.1021/ci7000633
   Park J, 1991, NEURAL COMPUT, V3, P246, DOI 10.1162/neco.1991.3.2.246
   Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Serwach M., 2016, P INT C COMPUTATIONA, DOI [10.1109/CPEE.2016.7738724, DOI 10.1109/CPEE.2016.7738724]
   Su J, 2021, ADV COMPLEX SYST, V24, DOI 10.1142/S0219525922500011
   Suykens JAK, 1999, NEURAL PROCESS LETT, V9, P293, DOI 10.1023/A:1018628609742
   Svetnik V, 2003, J CHEM INF COMP SCI, V43, P1947, DOI 10.1021/ci034160g
   Tzanetakis G, 2002, IEEE T SPEECH AUDI P, V10, P293, DOI 10.1109/TSA.2002.800560
   Zhou P, 2010, AMINO ACIDS, V38, P199, DOI 10.1007/s00726-008-0228-1
   Zhou P, 2009, J CHROMATOGR A, V1216, P3107, DOI 10.1016/j.chroma.2009.01.086
NR 41
TC 2
Z9 2
U1 2
U2 14
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD JUL
PY 2022
VL 19
IS 3
AR 12
DI 10.1145/3534966
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 4L3FO
UT WOS:000852517200004
DA 2024-07-18
ER

PT J
AU Alkasasbeh, AA
   Spyridonis, F
   Ghinea, G
AF Alkasasbeh, Anas Ali
   Spyridonis, Fotios
   Ghinea, Gheorghita
TI When Scents Help Me Remember My Password
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Olfactory media; authentication; olfactory passwords; information
   recall; QoE
ID OLFACTORY DISPLAY; ODOR; SYNCHRONIZATION; EXPERIENCE; MULSEMEDIA;
   QUALITY; MEMORY; SYSTEM
AB Current authentication processes overwhelmingly rely on audiovisual data, comprising images, text or audio. However, the use of olfactory data (scents) has remained unexploited in the authentication process, notwithstanding their verified potential to act as cues for information recall. Accordingly, in this paper, a new authentication process is proposed in which olfactory media are used as cues in the login phase. To this end, PassSmell, a proof of concept authentication application, is developed in which words and olfactory media act as passwords and olfactory passwords, respectively. In order to evaluate the potential of PassSmell, two different versions were developed, namely one which was olfactory-enhanced and another which did not employ olfactory media. Forty-two participants were invited to take part in the experiment, evenly split into a control and experimental group. For assessment purposes, we recorded the time taken to logon as well as the number of failed/successful login attempts; we also asked users to complete a Quality of Experience (QoE) questionnaire. In terms of time taken, a significant difference was found between the experimental and the control groups, as determined by an independent sample t-test. Similar results were found with respect to average scores and the number of successful attempts. Regarding user QoE, having olfactory media with words influenced the users positively, emphasizing the potential of using this kind of authentication application in the future.
C1 [Alkasasbeh, Anas Ali; Ghinea, Gheorghita] Brunel Univ London, Uxbridge, Middx, England.
   [Alkasasbeh, Anas Ali] Mutah Univ Jordan, Mutah, Jordan.
   [Spyridonis, Fotios] Univ Greenwich, Old Royal Naval Coll, London SE10 9LS, England.
   [Alkasasbeh, Anas Ali; Ghinea, Gheorghita] Brunel Univ, Kingston Lane, Uxbridge UB8 3PH, Middx, England.
   [Alkasasbeh, Anas Ali] Mutah Univ, Al Karak 61710, Al Karak, Jordan.
C3 Brunel University; University of Greenwich; Brunel University; Mutah
   University
RP Alkasasbeh, AA (corresponding author), Brunel Univ London, Uxbridge, Middx, England.; Alkasasbeh, AA (corresponding author), Mutah Univ Jordan, Mutah, Jordan.; Alkasasbeh, AA (corresponding author), Brunel Univ, Kingston Lane, Uxbridge UB8 3PH, Middx, England.; Alkasasbeh, AA (corresponding author), Mutah Univ, Al Karak 61710, Al Karak, Jordan.
EM Anas.Alkasasbeh@brunel.ac.uk; F.Spyridonis@greenwich.ac.uk;
   george.ghinea@brunel.ac.uk
RI Ghinea, Gheorghita/AAG-6770-2020
OI Ghinea, Gheorghita/0000-0003-2578-5580; Alkasasbeh, Anas
   Ali/0009-0004-8586-4461
CR Ademoye OA, 2013, ACM T MULTIM COMPUT, V9, DOI 10.1145/2487268.2487270
   Ademoye OA, 2009, IEEE T MULTIMEDIA, V11, P561, DOI 10.1109/TMM.2009.2012927
   Alkasasbeh Anas Ali, 2019, 2019 Twelfth International Conference on Ubi-Media Computing (Ubi-Media). Proceedings, P288, DOI 10.1109/Ubi-Media.2019.00063
   Alkasasbeh AA, 2019, 2019 IEEE CONFERENCE ON E-LEARNING, E-MANAGEMENT & E- SERVICES (IC3E), P7, DOI [10.1109/ic3e47558.2019.8971785, 10.1109/IC3e47558.2019.8971785]
   Alkasasbeh Anas Ali., 2019, EDMEDIA INNOVATE LEA, P881
   [Anonymous], 2017, 2017 IEEE INT S BROA, DOI DOI 10.1109/BMSB.2017.7986129
   Bahirat K, 2018, IEEE T MULTIMEDIA, V20, P1489, DOI 10.1109/TMM.2017.2769447
   Bangor A, 2008, INT J HUM-COMPUT INT, V24, P574, DOI 10.1080/10447310802205776
   Bannai Y, 2006, LECT NOTES COMPUT SC, V4282, P1322
   Brooke J, 2013, J USABILITY STUD, V8, P29
   Caro-Alvaro Sergio, 2021, EXPLORING IMPACT OLF, P1015
   Comminiello D, 2015, IEEE T MULTIMEDIA, V17, P1262, DOI 10.1109/TMM.2015.2442151
   Covaci A, 2019, ACM COMPUT SURV, V51, DOI 10.1145/3233774
   Covaci Alexandra, 2019, IEEE T MULTIMED
   Garcia-Ruiz M., 2008, INTERACT COMPUT AIDE
   George D., 2016, SPSS for Windows step by step: A simple guide and reference, V14th
   Ghinea G, 2015, PROCEEDINGS OF 2015 INTERNATIONAL CONFERENCE ON INTERACTIVE MOBILE COMMUNICATION TECHNOLOGIES AND LEARNING (IMCL), P296, DOI 10.1109/IMCTL.2015.7359606
   Ghinea G, 2012, ACM T MULTIM COMPUT, V8, DOI 10.1145/2379790.2379794
   Ghinea G, 2009, IEEE INT CON MULTI, P970, DOI 10.1109/ICME.2009.5202658
   Gibson A, 2018, IEEE T HUM-MACH SYST, V48, P604, DOI 10.1109/THMS.2018.2849018
   Hasegawa K, 2018, IEEE T VIS COMPUT GR, V24, P1477, DOI 10.1109/TVCG.2018.2794118
   Howell MJ, 2016, MULTIMED TOOLS APPL, V75, P12311, DOI 10.1007/s11042-015-2971-0
   Jain A, 1997, IEEE T PATTERN ANAL, V19, P302, DOI 10.1109/34.587996
   Jalal L, 2018, IEEE T BROADCAST, V64, P552, DOI 10.1109/TBC.2018.2823914
   Kaye J.N., 2001, THESIS MIT
   Kim JD, 2015, ETRI J, V37, P88, DOI 10.4218/etrij.15.0113.0078
   Liu Xun, 2019, IEEE T MULTIMED
   Martin GN, 2014, INT J NEUROSCI, V124, P806, DOI 10.3109/00207454.2014.890619
   Matsukura H, 2013, IEEE T VIS COMPUT GR, V19, P606, DOI 10.1109/TVCG.2013.40
   Miller Joel F, 1997, METHOD PERS VERIF US
   Min WQ, 2017, IEEE T MULTIMEDIA, V19, P1100, DOI 10.1109/TMM.2016.2639382
   Murray N, 2018, IEEE T BROADCAST, V64, P539, DOI 10.1109/TBC.2018.2825297
   Murray N, 2017, IEEE T SYST MAN CY-S, V47, P2503, DOI 10.1109/TSMC.2016.2531654
   Murray N, 2016, 2016 EIGHTH INTERNATIONAL CONFERENCE ON QUALITY OF MULTIMEDIA EXPERIENCE (QOMEX)
   Murray N, 2017, ACM COMPUT SURV, V50, DOI 10.1145/3108243
   Murray N, 2014, ACM T MULTIM COMPUT, V10, DOI 10.1145/2540994
   Narciso D, 2020, ACM T APPL PERCEPT, V17, DOI 10.1145/3380903
   Niwa K, 2018, IEEE T MULTIMEDIA, V20, P2871, DOI 10.1109/TMM.2018.2829187
   Patnaik B, 2019, IEEE T VIS COMPUT GR, V25, P726, DOI 10.1109/TVCG.2018.2865237
   Rainer B, 2014, ACM T MULTIM COMPUT, V11, DOI 10.1145/2648429
   Ranasinghe N, 2017, ACM T MULTIM COMPUT, V13, DOI 10.1145/2996462
   Sauro Jeff., 2011, MeasuringU
   Stafford LD, 2009, CHEMOSENS PERCEPT, V2, P59, DOI 10.1007/s12078-009-9045-5
   Tal I, 2019, IEEE COMMUN MAG, V57, P60, DOI 10.1109/MCOM.001.1900241
   Tortell R., 2007, Virtual Reality, V11, P61, DOI 10.1007/s10055-006-0056-0
   Yuan ZH, 2015, IEEE T MULTIMEDIA, V17, P957, DOI 10.1109/TMM.2015.2431915
   Yuan ZH, 2015, IEEE T MULTIMEDIA, V17, P104, DOI 10.1109/TMM.2014.2371240
   Yuan ZH, 2014, ACM T MULTIM COMPUT, V11, DOI 10.1145/2661329
   Zhang Y, 2019, IEEE T MULTIMEDIA, V21, P617, DOI 10.1109/TMM.2018.2882744
   Zou LH, 2017, PROCEEDINGS OF THE 8TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'17), P315, DOI 10.1145/3083187.3084014
NR 50
TC 2
Z9 2
U1 1
U2 8
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD JUL
PY 2021
VL 18
IS 3
AR 16
DI 10.1145/3469889
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA UD5LZ
UT WOS:000687249300007
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Radun, J
   Nuutinen, M
   Leisti, T
   Häkkinen, J
AF Radun, Jenni
   Nuutinen, Mikko
   Leisti, Tuomas
   Hakkinen, Jukka
TI Individual Differences in Image-Quality Estimations: Estimation Rules
   and Viewing Strategies
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Subjective image-quality estimation; eye movements; image-quality
   attributes; individual differences
ID EYE-MOVEMENTS; OBSERVERS TASK; ATTENTION; PERCEPTION; YARBUS;
   PREFERENCES; INFORMATION; LUMINANCE; BLUR
AB Subjective image-quality estimation with high-quality images is often a preference-estimation task. Preferences are subjective, and individual differences exist. Individual differences are also seen in the eye movements of people. A task's subjectivity can result from people using different rules as a basis for their estimation. Using two studies, we investigated whether different preference-estimation rules are related to individual differences in viewing behaviour by examining the process of preference estimation of high-quality images. The estimation rules were measured from free subjective reports on important quality-related attributes (Study 1) and from estimations of the attributes' importance in preference estimation (Study 2). The free reports showed that the observers used both feature-based image-quality attributes (e.g., sharpness, illumination) and abstract attributes, which include an interpretation of the image features (e.g., atmosphere and naturalness). In addition, the observers were classified into three viewing-strategy groups differing in fixation durations in both studies. These groups also used different estimation rules. In both studies, the group with medium-length fixations differed in their estimation rules from the other groups. In Study 1, the observers in this group used more abstract attributes than those in the other groups; in Study 2, they considered atmosphere to be a more important image feature. The study shows that individual differences in a quality-estimation task are related to both estimation rules and viewing strategies, and that the difference is related to the level of abstraction of the estimations.
C1 [Radun, Jenni; Nuutinen, Mikko; Leisti, Tuomas; Hakkinen, Jukka] Univ Helsinki, Inst Behav Sci, Siltavuorenpenger 1A, FIN-00014 Helsinki, Finland.
C3 University of Helsinki
RP Radun, J (corresponding author), Univ Helsinki, Inst Behav Sci, Siltavuorenpenger 1A, FIN-00014 Helsinki, Finland.
EM jenni.radun@helsinki.fi; mikko.nuutinen@helsinki.fi;
   tuomas.leisti@helsinki.fi; jukka.hakkinen@helsinki.fi
RI Radun, Jenni/AAB-3943-2021; Häkkinen, Jukka/A-4122-2019; Leisti,
   Tuomas/AAF-1709-2020
OI Radun, Jenni/0000-0003-3269-2999; Häkkinen, Jukka/0000-0003-0215-2238;
   Leisti, Tuomas/0000-0002-9234-2854; Nuutinen, Mikko/0000-0002-7429-3710
FU Finnish Doctoral Program in User-Centered Information Technology (UCIT);
   Ella and Georg Ehrnrooth Foundation; Emil Aaltonen Foundation; Academy
   of Finland Research Programme on the Human Mind project 'Mind, Picture,
   Image'
FX This work was supported by the Finnish Doctoral Program in User-Centered
   Information Technology (UCIT) and by grants from The Ella and Georg
   Ehrnrooth Foundation and Emil Aaltonen Foundation to the Jenni Radun.
   This work was also supported by the Academy of Finland Research
   Programme on the Human Mind project 'Mind, Picture, Image'. We thank
   Microsoft Corporation and Jean-Luc Olives, Mikko Vaahteranoksa, and
   Petteri Valve for their kind permission to let us use their images as
   the material.
CR Andrews TJ, 1999, VISION RES, V39, P2947, DOI 10.1016/S0042-6989(99)00019-X
   Bettman JR, 1998, J CONSUM RES, V25, P187, DOI 10.1086/209535
   Bianco S, 2013, J ELECTRON IMAGING, V22, DOI 10.1117/1.JEI.22.2.023014
   Birmingham E, 2009, VISION RES, V49, P2992, DOI 10.1016/j.visres.2009.09.014
   Boot WR, 2009, J VISION, V9, DOI 10.1167/9.3.7
   Borji A, 2014, J VISION, V14, DOI 10.1167/14.3.29
   Castelhano MS, 2008, CAN J EXP PSYCHOL, V62, P1, DOI 10.1037/1196-1961.62.1.1
   Castelhano MS, 2010, ATTEN PERCEPT PSYCHO, V72, P1283, DOI 10.3758/APP.72.5.1283
   DeAngelus M, 2009, VIS COGN, V17, P790, DOI 10.1080/13506280902793843
   DiStefano C., 2013, HDB QUANTITATIVE MET, P103, DOI DOI 10.1007/978-94-6209-404-8_5
   Dorr M, 2010, J VISION, V10, DOI 10.1167/10.10.28
   Einhäuser W, 2008, J VISION, V8, DOI 10.1167/8.2.2
   Ferzli R, 2009, IEEE T IMAGE PROCESS, V18, P717, DOI 10.1109/TIP.2008.2011760
   Greene MR, 2012, VISION RES, V62, P1, DOI 10.1016/j.visres.2012.03.019
   Haji-Abolhassani A, 2014, VISION RES, V103, P127, DOI 10.1016/j.visres.2014.08.014
   Hanley JA, 2003, AM J EPIDEMIOL, V157, P364, DOI 10.1093/aje/kwf215
   Henderson JM, 2013, J EXP PSYCHOL HUMAN, V39, P318, DOI 10.1037/a0031224
   Henderson JM, 2009, PSYCHON B REV, V16, P850, DOI 10.3758/PBR.16.5.850
   Janssen TJWM, 2000, DISPLAYS, V21, P129, DOI 10.1016/S0141-9382(00)00056-1
   Jayaraman D, 2012, CONF REC ASILOMAR C, P1693, DOI 10.1109/ACSSC.2012.6489321
   Kaller CP, 2009, PSYCHOPHYSIOLOGY, V46, P818, DOI 10.1111/j.1469-8986.2009.00821.x
   Kao WC, 2006, IEEE T CONSUM ELECTR, V52, P1144, DOI 10.1109/TCE.2006.273126
   Keelan B., 2002, Handbook of Image Quality: Characterization and Prediction
   Kruglanski AW, 2011, PSYCHOL REV, V118, P97, DOI 10.1037/a0020762
   LARSON EC, 2008, 15 IEEE INT C IM PRO, P2572
   Leisti T, 2014, ACTA PSYCHOL, V145, P65, DOI 10.1016/j.actpsy.2013.11.001
   Liu HT, 2011, IEEE T CIRC SYST VID, V21, P971, DOI 10.1109/TCSVT.2011.2133770
   LOFTUS GR, 1985, J EXP PSYCHOL GEN, V114, P342, DOI 10.1037/0096-3445.114.3.342
   Marziliano P, 2004, SIGNAL PROCESS-IMAGE, V19, P163, DOI 10.1016/j.image.2003.08.003
   Mills M, 2011, J VISION, V11, DOI 10.1167/11.8.17
   Narvekar ND, 2011, IEEE T IMAGE PROCESS, V20, P2678, DOI 10.1109/TIP.2011.2131660
   Nuutinen M, 2016, BEHAV RES METHODS, V48, P138, DOI 10.3758/s13428-014-0555-y
   Nyman G, 2005, P 12 INT DISPL WORKS
   Palermo R, 2007, NEUROPSYCHOLOGIA, V45, P75, DOI 10.1016/j.neuropsychologia.2006.04.025
   Payne JW, 1999, J RISK UNCERTAINTY, V19, P243, DOI 10.1023/A:1007843931054
   Pedersen M, 2010, J ELECTRON IMAGING, V19, DOI 10.1117/1.3277145
   Radun J, 2014, J ELECTRON IMAGING, V23, DOI 10.1117/1.JEI.23.6.061103
   Radun J, 2008, ACM T APPL PERCEPT, V4, DOI 10.1145/1278760.1278762
   Radun J, 2010, ACM T APPL PERCEPT, V7, DOI 10.1145/1773965.1773967
   Ramanath R, 2005, IEEE SIGNAL PROC MAG, V22, P34, DOI 10.1109/MSP.2005.1407713
   Rayner K, 2007, VISION RES, V47, P2714, DOI 10.1016/j.visres.2007.05.007
   Rayner K, 2009, Q J EXP PSYCHOL, V62, P1457, DOI 10.1080/17470210902816461
   Reinhard E., 2008, Color Imaging: Fundamentals and Applications
   Stanovich KE, 2000, BEHAV BRAIN SCI, V23, P645, DOI 10.1017/S0140525X00003435
   Tatler BW, 2007, J VISION, V7, DOI 10.1167/7.14.4
   Vu CT, 2008, 2008 IEEE SOUTHWEST SYMPOSIUM ON IMAGE ANALYSIS & INTERPRETATION, P73, DOI 10.1109/SSIAI.2008.4512288
   Warren C, 2011, WIRES COGN SCI, V2, P193, DOI 10.1002/wcs.98
   Yarbus A. L., 1967, Eye Movements and Vision
   Zhou J., 2007, P IEEE INT S CONS EL, P1
NR 49
TC 3
Z9 3
U1 0
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD MAY
PY 2016
VL 13
IS 3
AR 14
DI 10.1145/2890504
PG 22
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DV4DU
UT WOS:000382876200004
DA 2024-07-18
ER

PT J
AU Moscoso, C
   Matusiak, B
   Svensson, UP
   Orleanski, K
AF Moscoso, Claudia
   Matusiak, Barbara
   Svensson, U. Peter
   Orleanski, Krzysztof
TI Analysis of Stereoscopic Images as a New Method for Daylighting Studies
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Design; Experimentation; Human Factors; Measurement; Reliability;
   Daylighting studies; achromatic color; stereoscopic imaging; method
   comparison; aesthetic perception; visualization
ID VIRTUAL ENVIRONMENTS; REAL; AGREEMENT
AB This article presents the comparison analysis and results of an experiment designed with two presentation modes: real environments and stereoscopic images. The aim of this article is of a methodological nature, with a main objective of analyzing the usability of stereoscopic image presentation as a research tool to evaluate the daylight impact on the perceived architectural quality of small rooms. Twenty-six participants evaluated 12 different stimuli, divided in equal parts between real rooms and stereoscopic images. The stimuli were two similar rooms of different achromatic-colored surfaces (white and black) with three different daylight openings in each room. The participants assessed nine architectural quality attributes on a semantic differential scale. A pragmatic statistical approach (Bland-Altman Approach) for assessing agreement between two methods was used. Results suggest that stereoscopic image presentation is an accurate method to be used when evaluating all nine attributes in the white room and nearly all attributes in the black room.
C1 [Moscoso, Claudia; Matusiak, Barbara] Norwegian Univ Sci & Technol, Dept Architectural Design Form & Colour Studies, N-7491 Trondheim, Norway.
   [Svensson, U. Peter; Orleanski, Krzysztof] Norwegian Univ Sci & Technol, Dept Elect & Telecommun, N-7491 Trondheim, Norway.
C3 Norwegian University of Science & Technology (NTNU); Norwegian
   University of Science & Technology (NTNU)
RP Moscoso, C (corresponding author), Norwegian Univ Sci & Technol, Dept Architectural Design Form & Colour Studies, Alfred Getz Vei 3, N-7491 Trondheim, Norway.
EM claudia.moscoso@ntnu.no; barbara.matusiak@ntnu.no; Svensson@iet.ntnu.no;
   krzysztof.orleanski@iet.ntnu.no
RI Svensson, Peter/B-4593-2011
OI Moscoso Paredes, Claudia/0000-0003-4965-4155
FU Norwegian University of Science and Technology
FX This study was carried out as a part of a PhD research project funded by
   the Norwegian University of Science and Technology.
CR Akai Caitlin, 2007, THESIS S FRASIER U B
   ALTMAN DG, 1983, J ROY STAT SOC D-STA, V32, P307, DOI 10.2307/2987937
   Billger M, 2004, PROC SPIE, V5292, P90, DOI 10.1117/12.526986
   Bland JM, 2007, J BIOPHARM STAT, V17, P571, DOI 10.1080/10543400701329422
   BLAND JM, 1986, LANCET, V1, P307, DOI 10.1016/s0140-6736(86)90837-8
   de Kort YAW, 2003, PRESENCE-TELEOP VIRT, V12, P360, DOI 10.1162/105474603322391604
   Fontoynont Marc, 2007, P 26 SESS CIE
   Geuss M., 2012, J VISION, V12, P912
   KAPLAN S, 1989, J SOC ISSUES, V45, P59, DOI 10.1111/j.1540-4560.1989.tb01533.x
   Mahdavi Ardeshir, 2002, J ILLUMINATING ENG S, V31, P123
   Matusiak B., 2008, ARCHIT SCI REV, V51, P165, DOI [10.3763/asre.2008.5120, DOI 10.3763/ASRE.2008.5120]
   Moscoso Claudia, 2013, LUX EUROPA
   Nasar JL, 2000, PERSON-ENVIRONMENT PSYCHOLOGY, SECOND EDITION, P117
   Newsham G. R., 2005, Lighting Research & Technology, V37, P93, DOI 10.1191/1365782805li132oa
   Newsham Guy R., 2002, P IESNA ANN C
   Oi Naoyuki, 2005, Journal of Physiological Anthropology and Applied Human Science, V24, P87, DOI 10.2114/jpa.24.87
   Okubo M, 2011, EMOTIONAL ENGINEERING: SERVICE DEVELOPMENT, P247
   Osgood Charles E., 1971, LIGHTING RES TECHNOL, V31, P107
   SIS-Swedish Standard Institute, 1998, STD27101 SIS, P433
   Sprow I, 2009, P IM QUAL SYST PERF, V7242
   STAMPS AE, 1990, PERCEPT MOTOR SKILL, V71, P907, DOI 10.2466/pms.1990.71.3.907
   Valberg A., 2005, Light vision color
   Villa C, 2013, LIGHTING RES TECHNOL, V45, P401, DOI 10.1177/1477153512450452
   Wienold Jan, 1998, P CIE S LIGHT QUAL O
   Zielinski S, 2008, J AUDIO ENG SOC, V56, P427
   Ziemer CJ, 2009, ATTEN PERCEPT PSYCHO, V71, P1095, DOI 10.3758/APP.71.5.1096
NR 26
TC 25
Z9 27
U1 1
U2 5
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD JAN
PY 2015
VL 11
IS 4
AR 21
DI 10.1145/2665078
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AZ6WJ
UT WOS:000348358400006
OA Bronze
DA 2024-07-18
ER

PT J
AU Tompkin, J
   Kim, MH
   Kim, KI
   Kautz, J
   Theobalt, C
AF Tompkin, James
   Kim, Min H.
   Kim, Kwang In
   Kautz, Jan
   Theobalt, Christian
TI Preference and Artifact Analysis for Video Transitions of Places
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article; Proceedings Paper
CT 10th Symposium on Applied Perception (SAP)
CY AUG, 2013
CL Trinity Coll, Dublin, IRELAND
HO Trinity Coll
DE Human factors; Video-based rendering; video transition artifacts
AB Emerging interfaces for video collections of places attempt to link similar content with seamless transitions. However, the automatic computer vision techniques that enable these transitions have many failure cases which lead to artifacts in the final rendered transition. Under these conditions, which transitions are preferred by participants and which artifacts are most objectionable? We perform an experiment with participants comparing seven transition types, from movie cuts and dissolves to image-based warps and virtual camera transitions, across five scenes in a city. This document describes how we condition this experiment on slight and considerable view change cases, and how we analyze the feedback from participants to find their preference for transition types and artifacts. We discover that transition preference varies with view change, that automatic rendered transitions are significantly preferred even with some artifacts, and that dissolve transitions are comparable to less-sophisticated rendered transitions. This leads to insights into what visual features are important to maintain in a rendered transition, and to an artifact ordering within our transitions.
C1 [Kim, Min H.] Korea Adv Inst Sci & Technol, Seoul, South Korea.
   [Kautz, Jan] UCL, London WC1E 6BT, England.
C3 Korea Advanced Institute of Science & Technology (KAIST); University of
   London; University College London
EM jtompkin@mpi-inf.mpg.de
RI Kim, Min H./G-7969-2012
OI Kim, Min H./0000-0002-5078-4005; Theobalt,
   Christian/0000-0001-6104-6625; Tompkin, James/0000-0003-2218-2899
CR [Anonymous], FILM EDITING
   [Anonymous], 2000, Psychometric scaling, a toolkit for imaging systems development
   [Anonymous], 2001, Robotica, DOI DOI 10.1017/S0263574700223217
   [Anonymous], MODERN MULTIDIMENSIO
   [Anonymous], 1958, Theory and Methods of Scaling
   Ballan L, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778824
   CHAURASIA G, 2011, P EUR S REND
   Cui CW, 2000, EIGHTH COLOR IMAGING CONFERENCE: COLOR SCIENCE AND ENGINEERING SYSTEMS, TECHNOLOGIES, APPLICATIONS, P222
   DEBEVEC P, 1998, REND TECHN 98 P EUR
   DEBUCHI J, 1982, FROZEN TIME
   Eisemann M, 2008, COMPUT GRAPH FORUM, V27, P409, DOI 10.1111/j.1467-8659.2008.01138.x
   Fincher David., 2002, PANIC ROOM
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Furukawa Y, 2010, IEEE T PATTERN ANAL, V32, P1362, DOI 10.1109/TPAMI.2009.161
   Furukawa Y, 2010, PROC CVPR IEEE, P1434, DOI 10.1109/CVPR.2010.5539802
   Goesele M, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778832
   Horry Y., 1997, SIGGRAPH 97, P225
   Kazhdan M., 2006, P 4 EUR S GEOM PROC, P61, DOI DOI 10.2312/SGP/SGP06/061-070
   Lipski C., 2010, Proceedings 2010 Conference on Visual Media Production (CVMP 2010). 7th European Conference on Visual Media Production, P33, DOI 10.1109/CVMP.2010.12
   Lourakis MIA, 2004, TR340 ICSFORTH, P340
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   MACMILLAN J, 1984, EARLY WORK
   MCCURDY N. J, 2007, THESIS U CALIFORNIA
   Morvan Y, 2009, ACM T APPL PERCEPT, V5, DOI 10.1145/1462048.1462050
   Murch W., 2001, BLINK EYE
   Mustafa M, 2012, ACM T APPL PERCEPT, V9, DOI 10.1145/2325722.2325725
   Oh BM, 2001, COMP GRAPH, P433
   SCHAEFER S., 2006, ACM T GRAPHIC, V25, P3
   Snavely N, 2006, ACM T GRAPHIC, V25, P835, DOI 10.1145/1141911.1141964
   Snavely N, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360614
   Stich T, 2011, ACM T APPL PERCEPT, V8, DOI 10.1145/1870076.1870079
   Thormahlen T., 2006, THESIS U HANNOVER
   Tompkin J, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185564
   VANGORP P, 2011, P EUR S REND
   Vangorp P, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461971
NR 35
TC 5
Z9 5
U1 0
U2 6
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD AUG
PY 2013
VL 10
IS 3
SI SI
AR 13
DI 10.1145/2501601
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 206FS
UT WOS:000323504700002
OA Bronze
DA 2024-07-18
ER

PT J
AU Wang, Q
   Hayward, V
AF Wang, Qi
   Hayward, Vincent
TI Tactile Synthesis and Perceptual Inverse Problems Seen from the
   Viewpoint of Contact Mechanics
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Human Factors; Theory; Tactile synthesis; computational tactile
   perception; tactile transducers arrays; contact mechanics; Tactile
   sensing; Lateral skin deformation; Haptics
ID ADAPTING MECHANORECEPTIVE AFFERENTS; SPATIAL-RESOLUTION; DISCRIMINATION;
   RESPONSES; SHAPE; SKIN; INFORMATION; SURFACE; EDGES; BARS
AB A contact-mechanics analysis was used to explain a tactile illusion engendered by straining the fingertip skin tangentially in a progressive wave pattern resulting in the perception of a moving undulating surface. We derived the strain tensor field induced by a sinusoidal surface sliding on a finger as well as the field created by a tactile transducer array deforming the fingerpad skin by lateral traction. We found that the first field could be well approximated by the second. Our results have several implications. First, tactile displays using lateral skin deformation can generate tactile sensations similar to those using normal skin deformation. Second, a synthesis approach can achieve this result if some constraints on the design of tactile stimulators are met. Third, the mechanoreceptors embedded in the skin must respond to the deviatoric part of the strain tensor field and not to its volumetric part. Finally, many tactile stimuli might represent, for the brain, an inverse problem to be solved, such specific examples of "tactile metameres" are given.
C1 [Wang, Qi; Hayward, Vincent] McGill Univ, Ctr Intelligent Machines, Montreal, PQ H3A 2A7, Canada.
C3 McGill University
RP Wang, Q (corresponding author), McGill Univ, Ctr Intelligent Machines, Montreal, PQ H3A 2A7, Canada.
EM hayward@cim.mcgill.ca
RI Wang, Qi/D-6997-2012; Hayward, Vincent/A-4646-2010
OI Hayward, Vincent/0000-0002-2102-1965
FU Natural Sciences and Engineering Research Council of Canada; E. L. Adler
   Fellowship
FX Supported by a Discovery Grant of the Natural Sciences and Engineering
   Research Council of Canada and a E. L. Adler Fellowship for Wang.
CR Allerkamp D, 2007, VISUAL COMPUT, V23, P97, DOI 10.1007/s00371-006-0031-5
   [Anonymous], 0604 TRCIM MCGILL U
   BLISS JC, 1970, IEEE T MAN MACHINE, VMM11, P58, DOI 10.1109/TMMS.1970.299963
   CAUNA N, 1954, ANAT RECORD, V119, P449, DOI 10.1002/ar.1091190405
   Dario P, 2003, IEEE T ROBOTIC AUTOM, V19, P782, DOI 10.1109/TRA.2003.817071
   EDIN BB, 1995, J PHYSIOL-LONDON, V487, P243, DOI 10.1113/jphysiol.1995.sp020875
   FEARING RS, 1990, INT J ROBOT RES, V9, P3, DOI 10.1177/027836499000900301
   FEARING RS, 1985, INT J ROBOT RES, V4, P40, DOI 10.1177/027836498500400304
   Ferrier NJ, 2000, INT J ROBOT RES, V19, P795, DOI 10.1177/02783640022067184
   GERLING G, 2005, 1 JOINT EUR C S HAPT, P63
   Hayward V., 2000, P 8 S HAPTIC INTERFA, P1309
   Hoffman H. G., 1998, Virtual Reality, V3, P226, DOI 10.1007/BF01408703
   JOHNSON KO, 1981, J NEUROPHYSIOL, V46, P1177, DOI 10.1152/jn.1981.46.6.1177
   Johnson KO, 2001, CURR OPIN NEUROBIOL, V11, P455, DOI 10.1016/S0959-4388(00)00234-8
   Kikuuwe R., 2005, ACM Trans. Appl. Percept, V2, P46, DOI [DOI 10.1145/1048687.1048691, 10.1145/1048687.1048691]
   Kyung KU, 2005, WORLD HAPTICS CONFERENCE: FIRST JOINT EUROHAPTICS CONFERENCE AND SYMPOSIUM ON HAPTIC INTERFACES FOR VIRUTUAL ENVIRONMENT AND TELEOPERATOR SYSTEMS, PROCEEDINGS, P600
   LAMB GD, 1983, J PHYSIOL-LONDON, V338, P551, DOI 10.1113/jphysiol.1983.sp014689
   LAMOTTE RH, 1987, J NEUROSCI, V7, P1672
   LAMOTTE RH, 1987, J NEUROSCI, V7, P1655
   LAMOTTE RH, 1986, J NEUROPHYSIOL, V56, P1109, DOI 10.1152/jn.1986.56.4.1109
   Lau CKL, 2004, 12TH INTERNATIONAL SYMPOSIUM ON HAPTIC INTERFACES FOR VIRTUAL ENVIRONMENT AND TELEOPERATOR SYSTEMS, PROCEEDINGS, P32, DOI 10.1109/HAPTIC.2004.1287175
   Laurent VM, 2002, J BIOMECH ENG-T ASME, V124, P408, DOI 10.1115/1.1485285
   Lavesque V., 2005, ACM Trans. Appl. Percept, V2, P132, DOI DOI 10.1145/1060581.1060587
   Levesque V., 2003, PROC EUROHAPTICS 200, P261
   Maeno T, 1998, JSME INT J C-MECH SY, V41, P94, DOI 10.1299/jsmec.41.94
   MARR D, 1980, PROC R SOC SER B-BIO, V207, P187, DOI 10.1098/rspb.1980.0020
   Moy G., 2000, Proceedings 2000 ICRA. Millennium Conference. IEEE International Conference on Robotics and Automation. Symposia Proceedings (Cat. No.00CH37065), P3409, DOI 10.1109/ROBOT.2000.845247
   MOY G, 2000, HAPTICS E, V1, P3
   Nakanishi M, 2006, ASIAN TEST SYMPOSIUM, P69
   Pasquero J, 2007, IEEE T MULTIMEDIA, V9, P746, DOI 10.1109/TMM.2007.895672
   PHILLIPS JR, 1981, J NEUROPHYSIOL, V46, P1204, DOI 10.1152/jn.1981.46.6.1204
   PHILLIPS JR, 1981, J NEUROPHYSIOL, V46, P1192, DOI 10.1152/jn.1981.46.6.1192
   RICKER SL, 1993, PROCEEDINGS : IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION, VOLS 1-3, P941, DOI 10.1109/ROBOT.1993.292097
   Robert J.Webster., 2005, ACM T APPL PERCEPT, V2, P150, DOI DOI 10.1145/1060581.1060588
   ROSSI DD, 1991, P IEEE INT C ROB AUT, P398
   SRINIVASAN MA, 1992, ADV BIOENG, V22, P573
   Wagner C.R., 2004, HAPTICS E, V3, P4
   WANG N, 1993, SCIENCE, V260, P1124, DOI 10.1126/science.7684161
   Wang Q., 2006, P CHI 2006, P271
   Wang Q, 2007, J BIOMECH, V40, P851, DOI 10.1016/j.jbiomech.2006.03.004
   Wang Q, 2006, SYMPOSIUM ON HAPTICS INTERFACES FOR VIRTUAL ENVIRONMENT AND TELEOPERATOR SYSTEMS 2006, PROCEEDINGS, P67
NR 41
TC 14
Z9 15
U1 0
U2 8
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD MAY
PY 2008
VL 5
IS 2
AR 7
DI 10.1145/1279920.1279921
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 450YJ
UT WOS:000266437600001
DA 2024-07-18
ER

PT J
AU Subedar, MM
   Karam, LJ
AF Subedar, Mahesh M.
   Karam, Lina J.
TI 3D Blur Discrimination
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE 3D stereo; blur discrimination; human visual system; asymmetric stereo;
   Weber model
ID QUEST
AB Blur is an important attribute in the study and modeling of the human visual system. In the blur discrimination experiments, just-noticeable additional blur required to differentiate from the reference blur level is measured. The past studies on blur discrimination have measured the sensitivity of the human visual system to blur using two-dimensional (2D) test patterns. In this study, subjective tests are performed to measure blur discrimination thresholds using stereoscopic 3D test patterns. Specifically, how the binocular disparity affects the blur sensitivity is measured on a passive stereoscopic display. A passive stereoscopic display renders the left and right eye images in a row interleaved format. The subjects have to wear circularly polarized glasses to filter the appropriate images to the left and right eyes. Positive, negative, and zero disparity values are considered in these experiments. A positive disparity value projects the objects behind the display screen, a negative disparity value projects the objects in front of the display screen, and a zero disparity value projects the objects at the display plane. The blur discrimination thresholds are measured for both symmetric and asymmetric stereo viewing cases. In the symmetric viewing case, the same level of additional blur is applied to the left and right eye stimulus. In the asymmetric viewing case, different levels of additional blur are applied to the left and right eye stimuli. The results of this study indicate that, in the symmetric stereo viewing case, binocular disparity does not affect the blur discrimination thresholds for the selected 3D test patterns. As a consequence of these findings, we conclude that the models developed for 2D blur discrimination can be used for 3D blur discrimination. We also show that the Weber model provides a good fit to the blur discrimination threshold measurements for the symmetric stereo viewing case. In the asymmetric viewing case, the blur discrimination thresholds decreased, and the decrease in threshold values is found to be dominated by eye observing the higher blur.
C1 [Subedar, Mahesh M.] Arizona State Univ, Tempe, AZ 85287 USA.
   [Subedar, Mahesh M.] 2321 W Toledo Pl, Chandler, AZ 85224 USA.
   [Karam, Lina J.] Arizona State Univ, Sch ECEE, Mail Code 5706,Goldwater 430, Tempe, AZ 85287 USA.
C3 Arizona State University; Arizona State University-Tempe; Arizona State
   University; Arizona State University-Tempe
RP Subedar, MM (corresponding author), Arizona State Univ, Tempe, AZ 85287 USA.; Subedar, MM (corresponding author), 2321 W Toledo Pl, Chandler, AZ 85224 USA.
EM mms@asu.edu; karam@asu.edu
RI Karam, Lina Jamil/ABD-6531-2021
OI Karam, Lina Jamil/0000-0003-1870-1211
CR Aflaki P, 2010, IEEE IMAGE PROC, P4021, DOI 10.1109/ICIP.2010.5650661
   Ahumada AJ, 2002, J VISION, V2, P121, DOI 10.1167/2.1.8
   [Anonymous], 2014, PR-650 spectrascan colorimeter
   Georgeson MA, 2007, J VISION, V7, DOI 10.1167/7.13.7
   Hagele G., 2013, Dominant eye test
   HAMERLY JR, 1981, J OPT SOC AM, V71, P448, DOI 10.1364/JOSA.71.000448
   Held RT, 2012, CURR BIOL, V22, P426, DOI 10.1016/j.cub.2012.01.033
   Hoffman DM, 2008, J VISION, V8, DOI 10.1167/8.3.33
   KANE JJ, 1990, SMPTE J, V99, P744, DOI 10.5594/J02590
   KINGSMITH PE, 1994, VISION RES, V34, P885, DOI 10.1016/0042-6989(94)90039-6
   Mather G, 2002, PERCEPTION, V31, P1211, DOI 10.1068/p3254
   MCALLISTER D.F., 1993, STEREO COMPUTER GRAP
   McIlhagga WH, 2012, J VISION, V12, DOI 10.1167/12.10.9
   Miles WR, 1930, J GEN PSYCHOL, V3, P412, DOI 10.1080/00221309.1930.9918218
   PAAKKONEN AK, 1994, J OPT SOC AM A, V11, P992, DOI 10.1364/JOSAA.11.000992
   Pelli DG, 1997, SPATIAL VISION, V10, P437, DOI 10.1163/156856897X00366
   Salvendy G, 2012, HUM FACTORS ERGON, pXIII
   Sawides L, 2011, J VISION, V11, DOI 10.1167/11.7.21
   Walker Bruce H., 2000, Optical design for visual systems, V45
   WATSON AB, 1983, PERCEPT PSYCHOPHYS, V33, P113, DOI 10.3758/BF03202828
   Watson AB, 2011, J VISION, V11, DOI 10.1167/11.5.10
   Watt R.J., 1990, Visual processing: Computational, psychophysical and cognitive research
   WATT RJ, 1983, VISION RES, V23, P1465, DOI 10.1016/0042-6989(83)90158-X
   Webster MA, 2002, NAT NEUROSCI, V5, P839, DOI 10.1038/nn906
   [No title captured]
   [No title captured]
   [No title captured]
NR 27
TC 5
Z9 5
U1 0
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD MAY
PY 2016
VL 13
IS 3
AR 12
DI 10.1145/2896453
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DV4DU
UT WOS:000382876200002
DA 2024-07-18
ER

PT J
AU Israr, A
   Zhao, SY
   Schwalje, K
   Klatzky, R
   Lehman, J
AF Israr, Ali
   Zhao, Siyan
   Schwalje, Kaitlyn
   Klatzky, Roberta
   Lehman, Jill
TI Feel Effects: Enriching Storytelling with Haptic Feedback
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Human Factors; Haptics media; haptic vocabulary; feel effect; authoring
   tools; vibrotactile feedback
ID SENSE
AB Despite a long history of use in communication, haptic feedback is a relatively new addition to the toolbox of special effects. Unlike artists who use sound or vision, haptic designers cannot simply access libraries of effects that map cleanly to media content, and they lack even guiding principles for creating such effects. In this article, we make progress toward both capabilities: we generate a foundational library of usable haptic vocabulary and do so with a methodology that allows ongoing additions to the library in a principled and effective way. We define a feel effect as an explicit pairing between a meaningful linguistic phrase and a rendered haptic pattern. Our initial experiment demonstrates that users who have only their intrinsic language capacities, and no haptic expertise, can generate a core set of feel effects that lend themselves via semantic inference to the design of additional effects. The resulting collection of more than 40 effects covers a wide range of situations (including precipitation, animal locomotion, striking, and pulsating events) and is empirically shown to produce the named sensation for the majority of our test users in a second experiment. Our experiments demonstrate a unique and systematic approach to designing a vocabulary of haptic sensations that are related in both the semantic and parametric spaces.
C1 [Israr, Ali; Zhao, Siyan; Schwalje, Kaitlyn; Lehman, Jill] Disney Res, Lake Buena Vista, FL 32830 USA.
   [Klatzky, Roberta] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.
C3 Carnegie Mellon University
RP Israr, A (corresponding author), Disney Res, 4720 Forbes Ave Suite 110, Pittsburgh, PA 15213 USA.
EM israr@disneyresearch.com; siyan.zhao@disneyresearch.com;
   kaischwa@gmail.com; klatzky@cmu.edu; jill.lehman@disneyresearch.com
FU Disney Research; Direct For Computer & Info Scie & Enginr; Div Of
   Information & Intelligent Systems [0964100] Funding Source: National
   Science Foundation
FX We wish to thank Kurt Kilpela and Jacklin Wu for their help in
   determining initial scale values for the FEs and implementing the data
   collection software. We are also thankful to Ivan Poupyrev and Disney
   Research to support this research.
CR Brewster S., 2004, C RES PRACT INF TECH, V28, P15, DOI DOI 10.1145/67880.1046599
   Brunet L, 2013, 2013 WORLD HAPTICS CONFERENCE (WHC), P259, DOI 10.1109/WHC.2013.6548418
   COLLINS CC, 1970, IEEE T MAN MACHINE, VMM11, P65, DOI 10.1109/TMMS.1970.299964
   Danieau Fabien, 2012, P 18 ACM S VIRTUAL R, P69, DOI DOI 10.1145/2407336.2407350
   Enriquez M., 2006, ICMI
   Enriquez MJ, 2003, 11TH SYMPOSIUM ON HAPTIC INTERFACES FOR VIRTUAL ENVIRONMENT AND TELEOPERATOR SYSTEMS - HAPTICS 2003, PROCEEDINGS, P356, DOI 10.1109/HAPTIC.2003.1191310
   Gault RH, 1927, J FRANKL INST, V204, P329, DOI 10.1016/S0016-0032(27)92101-2
   Gunther E, 2003, J NEW MUSIC RES, V32, P369, DOI 10.1076/jnmr.32.4.369.18856
   Haans Antal, 2006, Virtual Reality, P149
   Israr A., 2011, ACM SIGGRAPH 2011 EM
   Israr A, 2011, 29TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P2019
   Jonas M., 2008, TECHNICAL REPORT
   Kim SH, 2013, KJNIZEV SMOTRA, V45, P3
   Klatzky RL, 2012, IEEE T HAPTICS, V5, P139, DOI [10.1109/ToH.2011.54, 10.1109/TOH.2011.54]
   LINVILL JG, 1966, PR INST ELECTR ELECT, V54, P40, DOI 10.1109/PROC.1966.4572
   Lyons J., 1977, Semantics
   MacLean K., 2003, EuroHaptics
   MILLER GA, 1995, COMMUN ACM, V38, P39, DOI 10.1145/219717.219748
   O'Sullivan C, 2006, LECT NOTES COMPUT SC, V4129, P145
   Sahami A, 2008, LECT NOTES COMPUT SC, V5355, P210, DOI 10.1007/978-3-540-89617-3_14
   Schuler K.K., 2005, THESIS U PENNSYLVANI
   Sodhi R, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2462007
   Tan HZ, 2001, FUNDAMENTALS OF WEARABLE COMPUTERS AND AUGMENTED REALITY, P579
   Tsetserukou Dzmitry., 2009, ACII, P1, DOI [DOI 10.1109/ACII.2009.5349516, https://doi.org/10.1109/ACII.2009.5349516]
   Yohanan S, 2012, INT J SOC ROBOT, V4, P163, DOI 10.1007/s12369-011-0126-7
NR 25
TC 72
Z9 81
U1 2
U2 7
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD JUN
PY 2014
VL 11
IS 3
SI SI
AR 11
DI 10.1145/2641570
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AU0KM
UT WOS:000345311700002
DA 2024-07-18
ER

PT J
AU Ramic-Brkic, B
   Chalmers, A
AF Ramic-Brkic, Belma
   Chalmers, Alan
TI Olfactory Adaptation in Virtual Environments
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Olfaction; Multisensory Virtual Environments; Adaptation to smell;
   high-fidelity graphics
ID ODORS; CUES; MEMORIES; PLEASANT; REALITY; WORLD; SENSE; SMELL
AB Visual perception is becoming increasingly important in computer graphics. Research on human visual perception has led to the development of perception-driven computer graphics techniques, where knowledge of the human visual system (HVS) and, in particular, its weaknesses are exploited when rendering and displaying 3D graphics. Findings on limitations of the HVS have been used to maintain high perceived quality but reduce the computed quality of some of the image without this quality difference being perceived. This article investigates the amount of time for which (if at all) such limitations could be exploited in the presence of smell. The results show that for our experiment, adaptation to smell does indeed affect participants' ability to determine quality difference in the animations. Having been exposed to a smell before undertaking the experiment, participants were able to determine the quality in a similar fashion to the "no smell" condition, whereas without adaptation, participants were not able to distinguish the quality difference.
C1 [Ramic-Brkic, Belma] Univ Sarajevo, Sch Sci & Technol, Sarajevo 71000, Bosnia & Herceg.
   [Ramic-Brkic, Belma; Chalmers, Alan] Univ Warwick, Coventry CV4 7AL, W Midlands, England.
C3 University of Sarajevo; University of Warwick
RP Ramic-Brkic, B (corresponding author), Hrasnicka Cesta 3a, Sarajevo 71000, Bosnia & Herceg.
EM belma.ramic@ssst.edu.ba; alan.chalmers@warwick.ac.uk
RI Ramic-Brkic, Belma/AHB-2625-2022
OI Ramic-Brkic, Belma/0000-0002-8205-0137
FU EPSRC [EP/K014056/1] Funding Source: UKRI
CR Aggleton JP, 1999, BRIT J PSYCHOL, V90, P1, DOI 10.1348/000712699161170
   Anderson PL, 2001, B MENNINGER CLIN, V65, P78, DOI 10.1521/bumc.65.1.78.18713
   [Anonymous], WHATS THAT SMELL MAG
   [Anonymous], MODELING SIMULATION
   [Anonymous], 2007, PROC 23 SPRING C COM
   [Anonymous], P EUROHAPTICS
   [Anonymous], OLFACTION REV
   [Anonymous], HDB VIRTUAL ENV
   [Anonymous], SMELL SUCCESS
   [Anonymous], BUYING NOSE
   [Anonymous], P SPRING C COMP GRAP
   [Anonymous], GENETICA
   [Anonymous], P SPRING C COMP GRAP
   [Anonymous], DIGITAL SCENT TECHNO
   [Anonymous], 1998, THESIS NAVAL POSTGRA
   [Anonymous], P 2 S APPL PERC GRAP
   [Anonymous], TECHNICAL REPORT
   [Anonymous], SENS SMELL UND
   [Anonymous], P 13 IEEE INT WORKSH
   [Anonymous], P 25 SPRING C COMP G
   [Anonymous], 2005, P 3 INT C COMPUTER G, DOI DOI 10.1145/1101389.1101462
   [Anonymous], COMMUNICATIONS VIRTU
   [Anonymous], THESIS U BRISTOL UK
   [Anonymous], P VIRTUAL REHABILITA
   [Anonymous], OLF INT
   [Anonymous], DOES SENSE SMELL WOR
   [Anonymous], AFRIGRAPH 10 P 7 INT
   [Anonymous], P 3 S APPL PERC GRAP
   [Anonymous], 2003, HDB MACHINE OLFACTIO
   Ayabe-Kanamura S, 1998, CHEM SENSES, V23, P31, DOI 10.1093/chemse/23.1.31
   Bone P.F., 1992, MARKET LETT, V3, P289, DOI DOI 10.1007/BF00994136
   Burdea G. C., 2003, Virtual reality technology
   CANN A, 1989, AM J PSYCHOL, V102, P91, DOI 10.2307/1423118
   CERNOCH JM, 1985, CHILD DEV, V56, P1593, DOI 10.1111/j.1467-8624.1985.tb00224.x
   Chalmers A, 2009, VISUAL COMPUT, V25, P1101, DOI 10.1007/s00371-009-0389-2
   Chen Y, 2006, ICAT 2006: 16TH INTERNATIONAL CONFERENCE ON ARTIFICIAL REALITY AND TELEXISTENCE - WORSHOPS, PROCEEDINGS, P580
   Chu S, 2000, CHEM SENSES, V25, P111, DOI 10.1093/chemse/25.1.111
   Debattista K, 2007, PARALLEL COMPUT, V33, P361, DOI 10.1016/j.parco.2007.04.002
   Dorri Y, 2007, MED HYPOTHESES, V69, P508, DOI 10.1016/j.mehy.2006.12.048
   Guéguen N, 2012, J SOC PSYCHOL, V152, P397, DOI 10.1080/00224545.2011.630434
   Harel D, 2003, COMPUT BIOL CHEM, V27, P121, DOI 10.1016/S1476-9271(02)00092-0
   Heilig M.L., 1962, SENSORAMA SIMULATOR
   Hirsch Allen., 1990, Preliminary Results of Olfaction Nike Study
   Hoffman H. G., 1998, Virtual Reality, V3, P226, DOI 10.1007/BF01408703
   Kaye J."J."., 2004, INTERACTIONS, V11, P48, DOI DOI 10.1145/962342.964333
   Kaye J.N., 2001, THESIS MIT CAMBRIDGE
   Miller C., 1991, Marketing News, V25, P1
   MONCRIEFF RW, 1956, J PHYSIOL-LONDON, V133, P301, DOI 10.1113/jphysiol.1956.sp005587
   Moore F. W., 1970, In Dimensions of nutrition., P181
   MOOREGILLON V, 1987, BRIT MED J, V294, P793, DOI 10.1136/bmj.294.6575.793
   Nakamoto T, 2008, IEEE COMPUT GRAPH, V28, P75, DOI 10.1109/MCG.2008.3
   Okamura AM, 2003, IEEE INT CONF ROBOT, P828
   Park CH, 2002, P IEEE VIRT REAL ANN, P269, DOI 10.1109/VR.2002.996532
   Powers W, 2004, SCI SMELL 1
   Ramon Y., 1909, Histologie du systeme nerveux de l'homme et des vertebres
   Rizzo AA, 2004, NEUROPSYCHOL REHABIL, V14, P207, DOI 10.1080/09602010343000183
   Rolls ET, 2003, EUR J NEUROSCI, V18, P695, DOI 10.1046/j.1460-9568.2003.02779.x
   Spangenberg ER, 1996, J MARKETING, V60, P67, DOI 10.2307/1251931
   Spencer BS, 2006, IEEE T INF TECHNOL B, V10, P168, DOI 10.1109/TITB.2005.856851
   Stevenson RJ, 2003, PSYCHOL REV, V110, P340, DOI 10.1037/0033-295X.110.2.340
   Stewart JC, 2007, J NEUROENG REHABIL, V4, DOI 10.1186/1743-0003-4-21
   Tijou A, 2006, LECT NOTES COMPUT SC, V3942, P1223, DOI 10.1007/11736639_152
   Tominaga K, 2001, VSMM 2001: SEVENTH INTERNATIONAL CONFERENCE ON VIRTUAL SYSTEMS AND MULTIMEDIA, PROCEEDINGS, P507, DOI 10.1109/VSMM.2001.969706
   Tortell R., 2007, Virtual Reality, V11, P61, DOI 10.1007/s10055-006-0056-0
   Ward G. J., 1994, P 21 ANN C COMP GRAP, P459, DOI [DOI 10.1145/192161.192286, 10.1145/192161.192286]
   Wilson DA, 2003, NEUROSCI BIOBEHAV R, V27, P307, DOI 10.1016/S0149-7634(03)00050-2
   Winkler S, 2005, PROC SPIE, V5666, P139, DOI 10.1117/12.596852
   Wysocki C.J., 1991, Smell and Taste in Health and Disease, P287
NR 68
TC 11
Z9 12
U1 0
U2 9
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD MAY
PY 2014
VL 11
IS 2
AR 6
DI 10.1145/2617917
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AO3OJ
UT WOS:000341241100002
DA 2024-07-18
ER

PT J
AU Bouchara, T
   Jacquemin, C
   Katz, BFG
AF Bouchara, Tifanie
   Jacquemin, Christian
   Katz, Brian F. G.
TI Cueing Multimedia Search with Audiovisual Blur
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Experimentation; Human Factors; Performance; Audiovisual search;
   multimedia browsing; audio and visual blurs; multisensory integration
ID AIDED VISUAL-SEARCH; IDENTIFICATION; ATTENTION; CONTEXT
AB Situated in the context of multimedia browsing, this study concerns perceptual processes involved in searching for an audiovisual object displayed among several distractors. The aim of the study is to increase the perceptual saliency of the target in order to enhance the search process. As blurring distractors and maintaining the target sharp has proved to be a great facilitator of visual search, we propose combining visual blur with an audio blur analogue to improve multimodal search. Three perceptual experiments were performed in which participants had to retrieve an audiovisual object from a set of six competing stimuli. The first two experiments explored the effect of blur level on unimodal search tasks. A third experiment investigated the influence of an audio and visual modality combination with both modalities cued on an audiovisual search task. Results showed that both visual and audio blurs render stimuli distractors less prominent and thus helped users focus on a sharp target more easily. Performances were also faster and more accurate in the bimodal condition than in either unimodal search task, auditory or visual. Our work suggests that audio and audiovisual interfaces dedicated to multimedia search could benefit from different uses of blur on presentation strategies.
C1 [Bouchara, Tifanie; Jacquemin, Christian; Katz, Brian F. G.] LIMSI CNRS, F-91403 Orsay, France.
   [Bouchara, Tifanie; Jacquemin, Christian] Univ Paris 11, Paris, France.
C3 Centre National de la Recherche Scientifique (CNRS); Universite Paris
   Saclay; Universite Paris Saclay
RP Bouchara, T (corresponding author), LIMSI CNRS, BP133, F-91403 Orsay, France.
EM tifanie.bouchara@limsi.fr; christian.jacquemin@limsi.fr;
   brian.katz@limsi.fr
RI Katz, Brian F.G./I-3191-2012; Bouchara, Tifanie/KEJ-5345-2024
OI Katz, Brian F.G./0000-0001-5118-0943; 
CR [Anonymous], 1999, Auditory Scene Analysis: The Perceptual Organization of Sound, DOI DOI 10.7551/MITPRESS/1486.001.0001
   [Anonymous], 2013, ACM T APPL PERCEPTIO, V10
   [Anonymous], 1976, PATTERN RECOGN
   Bederson B. B., 2001, 01UIST. Proceedings of the 14th Annual ACM Symposium on User Interface Software and Technology, P71, DOI 10.1145/502348.502359
   BEGAULT DR, 1994, 3D SOUND VIRTUAL REA
   Best V, 2007, JARO-J ASSOC RES OTO, V8, P294, DOI 10.1007/s10162-007-0073-z
   Bolia RS, 1999, HUM FACTORS, V41, P664, DOI 10.1518/001872099779656789
   Bouchara T, 2010, P INT C AUD DISPL, P245
   Bronkhorst AW, 2000, ACUSTICA, V86, P117
   BRUNGART DS, 2005, NEW DIRECTIONS IMPRO
   Burr D, 2006, PROG BRAIN RES, V155, P243, DOI 10.1016/S0079-6123(06)55014-9
   CHAREYRON G., 2005, THESIS U JEAN MONNET
   CHERRY EC, 1953, J ACOUST SOC AM, V25, P975, DOI 10.1121/1.1907229
   Cusack R, 2003, J EXP PSYCHOL HUMAN, V29, P713, DOI 10.1037/0096-1523.29.3.713
   Driver J., 2004, CROSSMODAL SPACE CRO, DOI [10.1093/acprof:oso/9780198524861.001.0001, DOI 10.1093/ACPROF:OSO/9780198524861.001.0001]
   DUNCAN J, 1989, PSYCHOL REV, V96, P433, DOI 10.1037/0033-295X.96.3.433
   Eramudugolla R, 2008, HEARING RES, V238, P139, DOI 10.1016/j.heares.2007.10.004
   Ernst MO, 2004, TRENDS COGN SCI, V8, P162, DOI 10.1016/j.tics.2004.02.002
   FERNSTROM M, 2001, P INT C AUD DISPL, P132
   Frederiksen J.R., 1967, Journal of Personality and Social Psychology, V7, P1
   Gonzalez R. C., 2006, Digital Image Processing, V3rd
   Healey C. G., 1995, ACM Transactions on Modeling and Computer Simulation, V5, P190, DOI 10.1145/217853.217855
   Heise S., 2009, Aurally and visually enhanced audio search with soundtorch, P3241
   Iordanescu L, 2011, ACTA PSYCHOL, V137, P252, DOI 10.1016/j.actpsy.2010.07.017
   KATZ B., 2010, LIMSI SPATIALISATION
   KESTON J., 2009, J KESTONS WORK GUASS
   Kosara R, 2002, IEEE COMPUT GRAPH, V22, P22, DOI 10.1109/38.974515
   Kosara R., 2002, JOINT EUROGRAPHICS, P205
   Krummenacher J, 2001, PERCEPT PSYCHOPHYS, V63, P901, DOI 10.3758/BF03194446
   Laurienti PJ, 2004, EXP BRAIN RES, V158, P405, DOI 10.1007/s00221-004-1913-2
   Lew MS, 2006, ACM T MULTIM COMPUT, V2, P1, DOI 10.1145/1126004.1126005
   MATSUMOTO A., 2009, AKIHIKO MATSUMOTOS W
   Ngo MK, 2010, ATTEN PERCEPT PSYCHO, V72, P1654, DOI 10.3758/APP.72.6.1654
   Özcan E, 2009, ACTA PSYCHOL, V131, P110, DOI 10.1016/j.actpsy.2009.03.007
   PERROTT DR, 1991, HUM FACTORS, V33, P389, DOI 10.1177/001872089103300402
   PREZAT F., 2006, P 120 AED CONV, p6704_1
   Prinzmetal W, 2005, J EXP PSYCHOL GEN, V134, P73, DOI 10.1037/0096-3445.134.1.73
   SAKOE H, 1978, IEEE T ACOUST SPEECH, V26, P43, DOI 10.1109/TASSP.1978.1163055
   Sarter NB, 2006, INT J IND ERGONOM, V36, P439, DOI 10.1016/j.ergon.2006.01.007
   Schneider TR, 2008, EXP PSYCHOL, V55, P121, DOI 10.1027/1618-3169.55.2.121
   SIMPSON B., 2010, P INT C AUD DISPL WA, P51
   SPIETH W, 1954, J ACOUST SOC AM, V26, P391, DOI 10.1121/1.1907347
   Stein Barry E., 1993, The Merging of the Senses. The Merging of the Senses. Cognitive Neuroscience
   SUMBY WH, 1954, J ACOUST SOC AM, V26, P212, DOI 10.1121/1.1907309
   TREISMAN A, 1985, COMPUT VISION GRAPH, V31, P156, DOI 10.1016/S0734-189X(85)80004-9
   Van der Burg E, 2008, J EXP PSYCHOL HUMAN, V34, P1053, DOI 10.1037/0096-1523.34.5.1053
   VELTKAMP RC, 2001, STATE OF THE ART CON
   Zolzer U., 2002, DAFX-Digital audio effects
NR 48
TC 3
Z9 3
U1 0
U2 8
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD MAY
PY 2013
VL 10
IS 2
AR 7
DI 10.1145/2465780.2465781
PG 21
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA 214EV
UT WOS:000324114800001
DA 2024-07-18
ER

PT J
AU Rosli, RM
   Tan, Z
   Proctor, RW
   Gray, R
AF Rosli, Roslizawaty Mohd
   Tan, Z.
   Proctor, Robert W.
   Gray, Rob
TI Attentional Gradient for Crossmodal Proximal-Distal Tactile Cueing of
   Visual Spatial Attention
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Design; Experimentation; Human Factors; Attention gradient; crossmodal
   attention cueing; proximal-distal cueing; tactile cueing; eye gaze
ID MODAL LINKS; CORTEX; VISION; SEARCH; TOUCH; CUES
AB Past studies have established a crossmodal spatial attentional link among vision, audition, and touch. The present study examined the dependence of visual attention on the distance between a distal visual target (a changing element among static distractors) and the quadrant of the visual display cued by a proximal tactile stimulus. The distance between the center of the cued visual quadrant and the visual target was one of six values: 0, 90, 180, 350, 450, and 550 pixels. The distances of 0, 90, and 180 corresponded to the valid tactile cueing condition, where the tactile cue and the visual target occurred in the same quadrant. The distances of 350, 450, and 550 corresponded to the invalid tactile cueing condition, where the tactilely-cued quadrant did not match that of the visual change. Results from 10 young adults showed that mean response time increased with respect to the cue-target distance, thereby confirming a gradient of visual attention for proximal-distal tactile cueing. In addition, the response times for valid tactile cues were shorter than those for invalid tactile cues, confirming earlier findings that valid tactile cues facilitate visual search and invalid tactile cues interfere with visual search. The findings of the present study have implications for the design of multimodal attention-cueing systems for practical applications such as collision warning systems in automobiles.
C1 [Rosli, Roslizawaty Mohd; Tan, Z.] Purdue Univ, Hapt Interface Res Lab, W Lafayette, IN 47907 USA.
   [Proctor, Robert W.] Purdue Univ, Human Performance Lab, W Lafayette, IN 47907 USA.
   [Gray, Rob] Arizona State Univ, Percept & Act Lab, Mesa, AZ 85212 USA.
C3 Purdue University System; Purdue University; Purdue University System;
   Purdue University; Arizona State University
RP Rosli, RM (corresponding author), Purdue Univ, Hapt Interface Res Lab, 465 NW Av, W Lafayette, IN 47907 USA.
EM rmohdros@gmail.com; hongtan@purdue.edu; proctor@psych.purdue.edu;
   robgray@asu.edu
RI Gray, Rob/A-3951-2010; gray, robert/HJB-2567-2022
OI Tan, Hong/0000-0003-0032-9554
FU National Science Foundation [IIS-0533908]
FX This research was partially supported by a National Science Foundation
   Award Grant IIS-0533908.
CR Assad JA, 2003, CURR OPIN NEUROBIOL, V13, P194, DOI 10.1016/S0959-4388(03)00045-X
   Brefczynski JA, 1999, NAT NEUROSCI, V2, P370, DOI 10.1038/7280
   Downing C.J., 1985, ATTENTION PERFORM, P171
   Driver J, 1998, PHILOS T R SOC B, V353, P1319, DOI 10.1098/rstb.1998.0286
   Erp JBV., 2005, ACM T APPL PERCEPT, V2, P106, DOI [10.1145/1060581.1060585, DOI 10.1145/1060581.1060585]
   Föcker J, 2010, BRAIN TOPOGR, V23, P1, DOI 10.1007/s10548-009-0111-8
   Gray R, 2002, EXP BRAIN RES, V145, P50, DOI 10.1007/s00221-002-1085-x
   Gray R, 2009, ACM T APPL PERCEPT, V6, DOI 10.1145/1462055.1462059
   Harrar V, 2009, EXP BRAIN RES, V198, P403, DOI 10.1007/s00221-009-1884-4
   Ho C, 2005, TRANSPORT RES F-TRAF, V8, P397, DOI 10.1016/j.trf.2005.05.002
   Ho C, 2007, BRAIN RES, V1144, P136, DOI 10.1016/j.brainres.2007.01.091
   Jones CM, 2008, EXP BRAIN RES, V186, P659, DOI 10.1007/s00221-008-1277-0
   McMains SA, 2004, NEURON, V42, P677, DOI 10.1016/S0896-6273(04)00263-6
   McMains SA, 2005, J NEUROSCI, V25, P9444, DOI 10.1523/JNEUROSCI.2647-05.2005
   PERROTT DR, 1990, PERCEPT PSYCHOPHYS, V48, P214, DOI 10.3758/BF03211521
   POSNER MI, 1980, J EXP PSYCHOL GEN, V109, P160, DOI 10.1037/0096-3445.109.2.160
   Proctor R.W., 2006, STIMULUS RESPONSE CO
   Proctor R.W., 2008, Human Factors in Simple and Complex Systems Boca Raton
   PROCTOR RW, 2009, ATTENTION, V2
   Rensink RA, 2000, VIS COGN, V7, P345, DOI 10.1080/135062800394847
   Schmitt M, 2001, EUR J COGN PSYCHOL, V13, P343, DOI 10.1080/09541440042000089
   SHEPHERD M, 1989, PERCEPT PSYCHOPHYS, V46, P146, DOI 10.3758/BF03204974
   Shioiri S, 2002, VISION RES, V42, P2811, DOI 10.1016/S0042-6989(02)00405-4
   SHULMAN GL, 1979, J EXP PSYCHOL HUMAN, V5, P522, DOI 10.1037/0096-1523.5.3.522
   Somers DC, 1999, P NATL ACAD SCI USA, V96, P1663, DOI 10.1073/pnas.96.4.1663
   Spence C, 1998, PERCEPT PSYCHOPHYS, V60, P544, DOI 10.3758/BF03206045
   Spence C., 2004, Crossmodal space and crossmodal attention
   SPERLING G, 1995, PSYCHOL REV, V102, P503, DOI 10.1037/0033-295X.102.3.503
   Tan H. Z., 2009, P SPIE IS T ELECT IM
   Tan H. Z., 2003, HAPTICS ELECT J HAPT, V3
   Treue S, 2001, TRENDS NEUROSCI, V24, P295, DOI 10.1016/S0166-2236(00)01814-2
   Van der Burg E, 2009, NEUROSCI LETT, V450, P60, DOI 10.1016/j.neulet.2008.11.002
   Van der Heijden A.H. C., 1992, SELECTIVE ATTENTION
   Wright RD., 2008, ORIENTING ATTENTION
NR 34
TC 2
Z9 2
U1 0
U2 7
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD NOV
PY 2011
VL 8
IS 4
AR 23
DI 10.1145/2043603.2043605
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA 856IQ
UT WOS:000297633400002
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Jimenez, J
   Sundstedt, V
   Gutierrez, D
AF Jimenez, Jorge
   Sundstedt, Veronica
   Gutierrez, Diego
TI Screen-Space Perceptual Rendering of Human Skin
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Algorithms; Performance; Experimentation; Human Factors; Real-time skin
   rendering; psychophysics; perception
AB We propose a novel skin shader which translates the simulation of subsurface scattering from texture space to a screen-space diffusion approximation. It naturally scales well while maintaining a perceptually plausible result. This technique allows us to ensure real-time performance even when several characters may appear on screen at the same time. The visual realism of the resulting images is validated using a subjective psychophysical preference experiment. Our results show that, independent of distance and light position, the images rendered using our novel shader have as high visual realism as a previously developed physically-based shader.
C1 [Jimenez, Jorge; Gutierrez, Diego] Univ Zaragoza, Dept Informat & Ingn Sistemas, Zaragoza 50018, Spain.
   [Sundstedt, Veronica] Trinity Coll Dublin, Sch Comp Sci & Stat, Lloyd Inst, Dublin, Ireland.
C3 University of Zaragoza; Trinity College Dublin
RP Jimenez, J (corresponding author), Univ Zaragoza, Dept Informat & Ingn Sistemas, Edificio Ada Byron,Maria Luna 1, Zaragoza 50018, Spain.
EM jim@unizar.es; Veronica.Sundstedt@cs.tcd.ie; diegog@unizar.es
RI Sundstedt, Veronica/IAP-9305-2023
OI Sundstedt, Veronica/0000-0003-3639-9327; Gutierrez Perez,
   Diego/0000-0002-7503-7022
FU Spanish Ministry of Science and Technology [TIN2007-63025]; Gobierno de
   Aragon [OTRI 2009/0411]; Instituto de Investigacion en Ingenieria de
   Aragon
FX This research has been partially funded by the Spanish Ministry of
   Science and Technology (TIN2007-63025) and the Gobierno de Aragon (OTRI
   2009/0411). J. Jimenez was funded by a research grant from the Instituto
   de Investigacion en Ingenieria de Aragon.
CR [Anonymous], P 2 S APPL PERC GRAP
   BORSHUKOV G, 2003, ACM SIGGRAPH SKETCHE
   DEON E, 2007, P EUR S REND
   DEON E, 2007, GPU GEMS, V3, pCH14
   Donner C, 2005, ACM T GRAPHIC, V24, P1032, DOI 10.1145/1073204.1073308
   DONNER C, 2006, P EUR S REND
   Fleming R. W., 2005, ACM Transactions on Applied Perception (TAP), V2, P346, DOI DOI 10.1145/1077399.1077409
   GILLHAM D, 2006, SHADER X 5 ADV RENDE, P163
   GOSSELIN D, 2004, P GAM DEV C
   GREEN S, 2004, GPU GEMS, P263
   HABLE J, 2009, SHADER X7, P161
   Igarashi T., 2005, APPEARANCE HUMAN SKI
   Jarosz W, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1330511.1330518
   Jensen HW, 2001, COMP GRAPH, P511, DOI 10.1145/383259.383319
   Jensen HW, 2002, ACM T GRAPHIC, V21, P576, DOI 10.1145/566570.566619
   JIMENEZ J, 2008, P CEIG, P21
   KELEMEN C, 2001, P EUR SHORT PRES
   Koenderink JJ, 2001, P SOC PHOTO-OPT INS, V4299, P312, DOI 10.1117/12.429502
   Nicodemus F., 1977, GEOMETRICAL CONSIDER
   PHARR M, 2004, PHYS BASED RENDERING, P382
   Singh M, 2002, PERCEPTION, V31, P531, DOI 10.1068/p3363
   Sundstedt V, 2007, ACM T APPL PERCEPT, V4, DOI 10.1145/1278387.1278389
NR 22
TC 55
Z9 62
U1 0
U2 13
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD SEP
PY 2009
VL 6
IS 4
AR 23
DI 10.1145/1609967.1609970
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 511ZV
UT WOS:000271212300003
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Wallraven, C
   Bülthoff, HH
   Cunningham, DW
   Fischer, J
   Bartz, D
AF Wallraven, Christian
   Buelthoff, Heinrich H.
   Cunningham, Douglas W.
   Fischer, Jan
   Bartz, Dirk
TI Evaluation of Real-World and Computer-Generated Stylized Facial
   Expressions
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Experimentation; Evaluation of facial animations; avatar; facial
   expressions; psychophysics; perceptually adaptive graphics; stylization
AB The goal of stylization is to provide an abstracted representation of an image that highlights specific types of visual information. Recent advances in computer graphics techniques have made it possible to render many varieties of stylized imagery efficiently making stylization into a useful technique, not only for artistic, but also for visualization applications. In this paper, we report results from two sets of experiments that aim at characterizing the perceptual impact and effectiveness of three different stylization techniques in the context of dynamic facial expressions. In the first set of experiments, animated facial expressions are stylized using three common techniques (brush, cartoon, and illustrative stylization) and investigated using different experimental measures. Going beyond the usual questionnaire approach, these experiments compare the techniques according to several criteria ranging from subjective preference to task-dependent measures (such as recognizability, intensity) allowing us to compare behavioral and introspective approaches. The second set of experiments use the same stylization techniques on real-world video sequences in order to compare the effect of stylization on natural and artificial stimuli. Our results shed light on how stylization of image contents affects the perception and subjective evaluation of both real and computer-generated facial expressions.
C1 [Wallraven, Christian; Buelthoff, Heinrich H.; Cunningham, Douglas W.] Max Planck Inst Biol Cybernet, Tubingen, Germany.
   [Cunningham, Douglas W.; Fischer, Jan] Univ Tubingen, WSI GRIS, Tubingen, Germany.
   [Bartz, Dirk] Univ Leipzig, Visual Comp ICCAS, Leipzig, Germany.
C3 Max Planck Society; Eberhard Karls University of Tubingen; Leipzig
   University
RP Wallraven, C (corresponding author), Max Planck Inst Biol Cybernet, Tubingen, Germany.
RI Bülthoff, Heinrich/AAC-8818-2019; Bülthoff, Heinrich H/J-6579-2012
OI Bülthoff, Heinrich H/0000-0003-2568-0607
CR Adolphs Ralph, 2002, Behav Cogn Neurosci Rev, V1, P21, DOI 10.1177/1534582302001001003
   Agrawala M, 2001, COMP GRAPH, P241, DOI 10.1145/383259.383286
   [Anonymous], MPI VIDEOLAB SYSTEM
   [Anonymous], P SIGGRAPH
   [Anonymous], 2003, SIGGRAPH 03 SKETCHES
   Bull P, 2001, PSYCHOLOGIST, V14, P644
   Cunningham D. W., 2005, ACM T APPL PERCEPT, V2, P251, DOI DOI 10.1145/1077399.1077404
   Cunningham DW, 2004, COMPUT ANIMAT VIRT W, V15, P305, DOI 10.1002/cav.33
   Cunningham DW, 2003, COMP ANIM CONF PROC, P23, DOI 10.1109/CASA.2003.1199300
   Doug DeCarlo AnthonySantella., 2004, NONPHOTOREALISTIC AN, P71
   Ekman P., 1972, UNIVERSALS CULTURAL, P207
   Ferwerda JA, 2003, PROC SPIE, V5007, P290, DOI 10.1117/12.473899
   Fischer J, 2005, IEEE VISUALIZATION 2005, PROCEEDINGS, P663
   Fischer J., 2005, VRST, P155
   FISCHER J, 2006, EUR S VIRT ENV EGVE
   Freudenberg B., 2002, Proceedings of the 13th Eurographics Workshop on Rendering, P227
   GOOCH AA, 2002, NPAR 02, P105
   Gooch B, 2004, ACM T GRAPHIC, V23, P27, DOI 10.1145/966131.966133
   Haeberli P., 1990, P 17 ANN C COMP GRAP, P207, DOI [10.1145/97879.97902, DOI 10.1145/97879.97902]
   Litwinowicz P., 1997, Proceedings of the 24th annual conference on Computer graphics and interactive techniques, SIGGRAPH '97, P407
   Stokes WA, 2004, ACM T GRAPHIC, V23, P742, DOI 10.1145/1015706.1015795
   Strothotte T, 2002, NONPHOTOREALISTIC CO
   Tomasi C, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P839, DOI 10.1109/ICCV.1998.710815
   Wallraven C, 2004, P 1 S APPL PERC GRAP, P181
   WALLRAVEN C, 2005, P 2 S APPL PERC GRAP, P17
   Winnemöller H, 2006, ACM T GRAPHIC, V25, P1221, DOI 10.1145/1141911.1142018
   Yip AW, 2002, PERCEPTION, V31, P995, DOI 10.1068/p3376
NR 27
TC 12
Z9 14
U1 0
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD NOV
PY 2007
VL 4
IS 3
AR 16
DI 10.1145/1278387.1278390
PG 24
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA V04IN
UT WOS:000207052200003
DA 2024-07-18
ER

PT J
AU Ehret, J
   Bönsch, A
   Aspöck, L
   Röhr, CT
   Baumann, S
   Grice, M
   Fels, J
   Kuhlen, TW
AF Ehret, Jonathan
   Boensch, Andrea
   Aspoeck, Lukas
   Roehr, Christine T.
   Baumann, Stefan
   Grice, Martine
   Fels, Janina
   Kuhlen, Torsten W.
TI Do Prosody and Embodiment Influence the Perceived Naturalness of
   Conversational Agents' Speech?
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article; Proceedings Paper
CT 18th Symposium on Applied Perceptiion (SAP)
CY SEP, 2021
CL ELECTR NETWORK
DE Embodied conversational agents (ECAs); virtual acoustics; prosody;
   accentuation; speech; text-to-speech; audio; embodiment
AB For conversational agents' speech, either all possible sentences have to be prerecorded by voice actors or the required utterances can be synthesized. While synthesizing speech is more flexible and economic in production, it also potentially reduces the perceived naturalness of the agents among others due to mistakes at various linguistic levels. In our article, we are interested in the impact of adequate and inadequate prosody, here particularly in terms of accent placement, on the perceived naturalness and aliveness of the agents. We compare (1) inadequate prosody, as generated by off-the-shelf text-to-speech (TTS) engines with synthetic output; (2) the same inadequate prosody imitated by trained human speakers; and (3) adequate prosody produced by those speakers. The speech was presented either as audio-only or by embodied, anthropomorphic agents, to investigate the potential masking effect by a simultaneous visual representation of those virtual agents. To this end, we conducted an online study with 40 participants listening to four different dialogues each presented in the three Speech levels and the two Embodiment levels. Results confirmed that adequate prosody in human speech is perceived as more natural (and the agents are perceived as more alive) than inadequate prosody in both human (2) and synthetic speech (1). Thus, it is not sufficient to just use a human voice for an agents' speech to be perceived as natural-it is decisive whether the prosodic realisation is adequate or not. Furthermore, and surprisingly, we found no masking effect by speaker embodiment, since neither a human voice with inadequate prosody nor a synthetic voice was judged as more natural, when a virtual agent was visible compared to the audio-only condition. On the contrary, the human voice was even judged as less "alive" when accompanied by a virtual agent. In sum, our results emphasize, on the one hand, the importance of adequate prosody for perceived naturalness, especially in terms of accents being placed on important words in the phrase, while showing, on the other hand, that the embodiment of virtual agents plays a minor role in the naturalness ratings of voices.
C1 [Ehret, Jonathan; Boensch, Andrea; Kuhlen, Torsten W.] Rhein Westfal TH Aachen, Visual Comp Inst, Kopernikusstr 6, D-52074 Aachen, Germany.
   [Aspoeck, Lukas; Fels, Janina] Rhein Westfal TH Aachen, Inst Hearing Technol & Acoust, Kopernikusstr 5, D-52074 Aachen, Germany.
   [Roehr, Christine T.; Baumann, Stefan; Grice, Martine] Univ Cologne, IfL Phonet, Herbert Lewin Str 6, D-50931 Cologne, Germany.
C3 RWTH Aachen University; RWTH Aachen University; University of Cologne
RP Ehret, J (corresponding author), Rhein Westfal TH Aachen, Visual Comp Inst, Kopernikusstr 6, D-52074 Aachen, Germany.
EM ehret@vr.rwth-aachen.de; boensch@vr.rwth-aachen.de;
   lukas.aspoeck@akustik.rwth-aachen.de; christine.roehr@uni-koeln.de;
   stefan.baumann@uni-koeln.de; martine.grice@uni-koeln.de;
   janina.fels@akustik.rwth-aachen.de; kuhlen@vr.rwth-aachen.de
RI Bönsch, Andrea/GSO-1680-2022; Bönsch, Andrea/AAT-4713-2021; Fels,
   Janina/A-4336-2013
OI Bönsch, Andrea/0000-0001-5077-3675; Fels, Janina/0000-0002-8694-7750;
   Ehret, Jonathan/0000-0001-6270-5119
CR Al Moubayed Samer, 2012, Cognitive Behavioural Systems (COST 2012). International Training School. Revised Selected Papers, P114, DOI 10.1007/978-3-642-34584-5_9
   Anderson Keith, 2013, Advances in Computer Entertainment. 10th International Conference, ACE 2013. Proceedings: LNCS 8253, P476, DOI 10.1007/978-3-319-03161-3_35
   Baayen RH, 2008, J MEM LANG, V59, P390, DOI 10.1016/j.jml.2007.12.005
   Bailenson JN, 2001, PRESENCE-VIRTUAL AUG, V10, P583, DOI 10.1162/105474601753272844
   Baltrusaitis T, 2018, IEEE INT CONF AUTOMA, P59, DOI 10.1109/FG.2018.00019
   Bates D, 2015, J STAT SOFTW, V67, P1, DOI 10.18637/jss.v067.i01
   Baur T, 2013, 2013 ASE/IEEE INTERNATIONAL CONFERENCE ON SOCIAL COMPUTING (SOCIALCOM), P220, DOI 10.1109/SocialCom.2013.39
   Cabral JP, 2017, INTERSPEECH, P229, DOI 10.21437/Interspeech.2017-325
   Cambre Julia, 2019, Proceedings of the ACM on Human-Computer Interaction, V3, DOI 10.1145/3359325
   Cassell J., 2000, Embodied Conversational Agents
   Chateau N, 2005, LECT NOTES COMPUT SC, V3784, P652
   Chérif E, 2019, RECH APPL MARKET-ENG, V34, P28, DOI 10.1177/2051570719829432
   Chini JJ, 2016, PHYS REV PHYS EDUC R, V12, DOI 10.1103/PhysRevPhysEducRes.12.010117
   Chollet M, 2015, PROCEEDINGS OF THE 2015 ACM INTERNATIONAL JOINT CONFERENCE ON PERVASIVE AND UBIQUITOUS COMPUTING (UBICOMP 2015), P1143, DOI 10.1145/2750858.2806060
   Cohn M., 2020, CogSci, V2020, P220
   Cutler A., 1980, Errors in Linguistic Performance, P67
   Davis RO, 2019, COMPUT EDUC, V140, DOI 10.1016/j.compedu.2019.103605
   de Borst AW, 2015, FRONT PSYCHOL, V6, DOI 10.3389/fpsyg.2015.00576
   Ei Chew H, 2019, ID BLUSH COULD CLOSI
   Gálvez RH, 2020, SPEECH COMMUN, V124, P46, DOI 10.1016/j.specom.2020.07.007
   Georgila K, 2012, LREC 2012 - EIGHTH INTERNATIONAL CONFERENCE ON LANGUAGE RESOURCES AND EVALUATION, P3519
   Gong L, 2007, HUM COMMUN RES, V33, P163, DOI 10.1111/j.1468-2958.2007.00295.x
   Gratch J, 2016, LECT NOTES ARTIF INT, V10011, P283, DOI 10.1007/978-3-319-47665-0_25
   Hiyakumoto Laurie, 1997, CONCEPT SPEECH GENER, V21, P15
   Kang N, 2016, COMPUT HUM BEHAV, V55, P680, DOI 10.1016/j.chb.2015.10.008
   Kätsyri J, 2015, FRONT PSYCHOL, V6, DOI 10.3389/fpsyg.2015.00390
   Krenn B, 2017, AI SOC, V32, P65, DOI 10.1007/s00146-014-0569-0
   Kühne K, 2020, FRONT NEUROROBOTICS, V14, DOI 10.3389/fnbot.2020.593732
   Leiner D.J., 2021, SoSci Survey Computer software
   Lugrin J-L., 2016, FRONTIERS ICT, V3, P26, DOI [10.3389/fict.2016.00026, DOI 10.3389/FICT.2016.00026]
   Malisz Zofia, 2019, 10 ISCA SPEECH SYNTH, P257, DOI [10.21437/SSW.2019-46, DOI 10.21437/SSW.2019-46]
   Marsella Stacy, 2013, P 12 ACM SIGGRAPH EU, P25, DOI DOI 10.1145/2485895.2485900
   Oh CS, 2018, FRONT ROBOT AI, V5, DOI 10.3389/frobt.2018.00114
   Peeters D, 2019, PSYCHON B REV, V26, P894, DOI 10.3758/s13423-019-01571-3
   Pournelle G. H., 1953, Journal of Mammalogy, V34, P133, DOI 10.1890/0012-9658(2002)083[1421:SDEOLC]2.0.CO;2
   Rosenthal-von der Pütten AM, 2016, LECT NOTES ARTIF INT, V10011, P256, DOI 10.1007/978-3-319-47665-0_23
   Schröder M, 2011, 12TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2011 (INTERSPEECH 2011), VOLS 1-5, P3260
   Seaborn K, 2021, ACM COMPUT SURV, V54, DOI 10.1145/3386867
   Shen J, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P4779, DOI 10.1109/ICASSP.2018.8461368
   van der Struijk S, 2018, 18TH ACM INTERNATIONAL CONFERENCE ON INTELLIGENT VIRTUAL AGENTS (IVA'18), P159, DOI 10.1145/3267851.3267918
   WANG I, 2021, INT J HUMAN COMPUTER, P1, DOI DOI 10.1080/10447318.2021.1898851
NR 41
TC 10
Z9 10
U1 2
U2 10
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD OCT
PY 2021
VL 18
IS 4
AR 21
DI 10.1145/3486580
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA YY1DS
UT WOS:000754533100005
DA 2024-07-18
ER

PT J
AU Plaisier, MA
   Vermeer, DS
   Kappers, AML
AF Plaisier, Myrthe A.
   Vermeer, Daphne S.
   Kappers, Astrid M. L.
TI Learning the Vibrotactile Morse Code Alphabet
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Haptic communication; vibration; Morse code
AB Vibrotactile Morse code provides a way to convey words using the sense of touch with vibrations. This can be useful in applications for users with a visual and/or auditory impairment. The advantage of using vibrotactile Morse code is that it is technically easy to accomplish. The usefulness of tactile Morse code also depends on how easy it is to learn to use without providing a visual representation of the code. Here we investigated learning of the vibrotactile the Morse code alphabet without any visual representation of the code and whether the learned letters can immediately be used to recognize words. Two vibration motors were used: one was attached to the left arm (dots) and the other to the right arm (dashes). We gave the participants a learning session of 30 minutes and determined how many letters they had learned. All participants managed to learn at least 15 letters in this time. Directly afterward, they were presented with 2-, 3-, 4-, or 5-letter words consisting of only the letters they had learned. Participants were able to identify words, but correct rates decreased rapidly with word length. We can conclude that it is possible to learn vibrotactile Morse code using only a vibrotactile representation (15 to 24 letters in 30 minutes). After the learning session. it was possible to recognise words, but to increase the recognition rates extra training would be beneficial.
C1 [Plaisier, Myrthe A.; Kappers, Astrid M. L.] Eindhoven Univ Technol, Dynam Arid Control Sect, Dept Mech Engn, Eindhoven, Netherlands.
   [Vermeer, Daphne S.] Eindhoven Univ Technol, Human Technol Interact Sect, Eindhoven, Netherlands.
   [Kappers, Astrid M. L.] Eindhoven Univ Technol, Dynam & Control Sect, Control Syst Technol & Human Technol Interact, Eindhoven, Netherlands.
C3 Eindhoven University of Technology; Eindhoven University of Technology;
   Eindhoven University of Technology
RP Plaisier, MA (corresponding author), Eindhoven Univ Technol, Dynam Arid Control Sect, Dept Mech Engn, Eindhoven, Netherlands.
EM m.a.plaisier@tue.nl; d.s.vermeer@student.tue.nl; a.m.l.kappers@tue.nl
OI Kappers, Astrid/0000-0003-4101-7717
FU European Union's Horizon 2020 research and innovation programme
   [780814]; H2020 - Industrial Leadership [780814] Funding Source: H2020 -
   Industrial Leadership
FX This research was supported by funding from the European Union's Horizon
   2020 research and innovation programme under grant agreement No 780814,
   project SUITCEYES.
CR Aher A, 2014, INT J REASEARCH ADVE, V2, P151
   Arato A, 2014, LECT NOTES COMPUT SC, V8548, P393, DOI 10.1007/978-3-319-08599-9_59
   BROOKS PL, 1983, J ACOUST SOC AM, V74, P34, DOI 10.1121/1.389685
   BROOKS PL, 1985, J ACOUST SOC AM, V77, P1576, DOI 10.1121/1.392000
   de Vargas MF, 2019, 2019 IEEE WORLD HAPTICS CONFERENCE (WHC), P610, DOI [10.1109/whc.2019.8816145, 10.1109/WHC.2019.8816145]
   Duvernoy B, 2019, LECT NOTES ELECTR EN, V535, P112, DOI 10.1007/978-981-13-3194-7_26
   Gaffary Y, 2018, IEEE T HAPTICS, V11, P636, DOI 10.1109/TOH.2018.2855175
   Gollner U., 2012, P 6 INT C TANGIBLE E, P127
   HELLER MA, 1990, B PSYCHONOMIC SOC, V28, P11, DOI 10.3758/BF03337634
   Lynch M P, 1988, J Rehabil Res Dev, V25, P41
   Norberg Lena, 2014, P 10 INT C DIS VIRT, DOI [10.13140/2.1.3712.3524, DOI 10.13140/2.1.3712.3524]
   Oshima K, 2014, J VISUAL IMPAIR BLIN, V108, P122
   Pescara E, 2019, IEEE INT SYM WRBL CO, P186, DOI 10.1145/3341163.3347714
   Ranjbar P., 2017, INT J ENG TECHNOL SC, V1, P351
   Seim C, 2018, ISWC'18: PROCEEDINGS OF THE 2018 ACM INTERNATIONAL SYMPOSIUM ON WEARABLE COMPUTERS, P228, DOI 10.1145/3267242.3267269
   Seim C, 2016, IEEE INT SYM WRBL CO, P164, DOI 10.1145/2971763.2971768
   Tan HZ, 1997, PERCEPT PSYCHOPHYS, V59, P1004, DOI 10.3758/BF03205516
   Walker M, 2018, IEEE T HAPTICS, V11, P151, DOI 10.1109/TOH.2017.2743713
   Yang CH, 2008, MATH COMPUT MODEL, V47, P318, DOI 10.1016/j.mcm.2007.01.012
NR 19
TC 6
Z9 7
U1 5
U2 11
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD NOV
PY 2020
VL 17
IS 3
AR 9
DI 10.1145/3402935
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA PA3OQ
UT WOS:000595548000001
OA Green Published
DA 2024-07-18
ER

PT J
AU Ondrej, J
   Ennis, C
   Merriman, NA
   O'Sullivan, C
AF Ondrej, Jan
   Ennis, Cathy
   Merriman, Niamh A.
   O'Sullivan, Carol
TI FrankenFolk: Distinctiveness and Attractiveness of Voice and Motion
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article; Proceedings Paper
CT 13th ACM SIGGRAPH Symposium on Applied Perception (SAP)
CY JUL, 2016
CL Anaheim, CA
SP ACM SIGGRAPH
DE Virtual characters; multisensory perception; crowd variety
ID VOCAL ATTRACTIVENESS; AVERAGENESS; FACES; PITCH
AB It is common practice in movies and games to use different actors for the voice and body/face motion of a virtual character. What effect does the combination of these different modalities have on the perception of the viewer? In this article, we conduct a series of experiments to evaluate the distinctiveness and attractiveness of human motions (face and body) and voices. We also create combination characters called FrankenFolks, where we mix and match the voice, body motion, face motion, and avatar of different actors and ask which modality is most dominant when determining distinctiveness and attractiveness or whether the effects are cumulative.
C1 [Ondrej, Jan; Merriman, Niamh A.] Disney Res, Los Angeles, CA USA.
   [Ennis, Cathy] Dublin Inst Technol, Kevin St, Dublin 8, Ireland.
   [O'Sullivan, Carol] Trinity Coll Dublin, Coll Green, Dublin 2, Ireland.
   [Merriman, Niamh A.] Royal Coll Surgeons Ireland, Beaux Lane House,Lower Mercer St, Dublin 2, Ireland.
C3 Technological University Dublin; Trinity College Dublin; Royal College
   of Surgeons - Ireland
RP Ondrej, J (corresponding author), Disney Res LA, 1401 Flower St, Glendale, CA 91201 USA.
EM jan.ondrej@disneyresearch.com; cathy.ennis@dit.ie;
   niamhmerriman@rcsi.ie; Carol.OSullivan@scss.tcd.ie
RI Merriman, Niamh/I-8281-2016; Ondrej, Jan/N-1947-2016
OI O'Sullivan, Carol/0000-0003-3772-4961; Ennis, Cathy/0000-0002-1274-5347;
   Ondrej, Jan/0000-0002-5409-1521; Merriman, Niamh A./0000-0002-5327-7762
CR Borkowska B, 2011, ANIM BEHAV, V82, P55, DOI 10.1016/j.anbehav.2011.03.024
   Carter EJ, 2010, ACM T APPL PERCEPT, V7, DOI 10.1145/1823738.1823741
   Cohen J., 1988, STAT POWER ANAL BEHA
   Collins SA, 2003, ANIM BEHAV, V65, P997, DOI 10.1006/anbe.2003.2123
   Ennis C, 2013, P MOT GAM MIG 13
   Ennis C., 2010, ACM SIGGRAPH 2010 PA, P91
   Feinberg DR, 2008, PERCEPTION, V37, P615, DOI 10.1068/p5514
   Fraccaro PJ, 2013, ANIM BEHAV, V85, P127, DOI 10.1016/j.anbehav.2012.10.016
   Hodges-Simeon CR, 2010, HUM NATURE-INT BIOS, V21, P406, DOI 10.1007/s12110-010-9101-5
   Hodgins J, 2010, ACM T APPL PERCEPT, V7, DOI 10.1145/1823738.1823740
   Hoyet L, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508367
   Kamachi M, 2003, CURR BIOL, V13, P1709, DOI 10.1016/j.cub.2003.09.005
   Lander K, 2008, ANIM BEHAV, V75, P817, DOI 10.1016/j.anbehav.2007.07.001
   Lander K, 2007, J EXP PSYCHOL HUMAN, V33, P905, DOI 10.1037/0096-1523.33.4.905
   McDonnell R, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531361
   McDonnell Rachel, 2008, ACM SIGGRAPH 2008 SI
   MCGURK H, 1976, NATURE, V264, P746, DOI 10.1038/264746a0
   Praz'ak M., 2011, P ACM SIGGRAPH S APP, P87, DOI [10.1145/2077451.2077468, DOI 10.1145/2077451.2077468]
   Rhodes G, 1996, PSYCHOL SCI, V7, P105, DOI 10.1111/j.1467-9280.1996.tb00338.x
   Schweinberger SR, 2007, Q J EXP PSYCHOL, V60, P1446, DOI 10.1080/17470210601063589
   Yovel G, 2013, TRENDS COGN SCI, V17, P263, DOI 10.1016/j.tics.2013.04.004
   ZUCKERMAN M, 1989, J NONVERBAL BEHAV, V13, P67
NR 22
TC 4
Z9 5
U1 2
U2 9
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD JUL
PY 2016
VL 13
IS 4
SI SI
AR 20
DI 10.1145/2948066
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA DV4DY
UT WOS:000382876600003
DA 2024-07-18
ER

PT J
AU Piryankova, IV
   Stefanucci, JK
   Romero, J
   de la Rosa, S
   Black, MJ
   Mohler, BJ
AF Piryankova, Ivelina V.
   Stefanucci, Jeanine K.
   Romero, Javier
   de la Rosa, Stephan
   Black, Michael J.
   Mohler, Betty J.
TI Can I Recognize My Body's Weight? The Influence of Shape and Texture on
   the Perception of Self
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Three-Dimensional Graphics and Realism; Vision and Scene Understanding;
   Body perception; virtual environments; human perception and performance;
   visual psychophysics; texture; 3D shape; BMI; avatar
ID ANOREXIA-NERVOSA; SIZE ESTIMATION; IMAGE
AB The goal of this research was to investigate women's sensitivity to changes in their perceived weight by altering the body mass index (BMI) of the participants' personalized avatars displayed on a large-screen immersive display. We created the personalized avatars with a full-body 3D scanner that records the participants' body geometry and texture. We altered the weight of the personalized avatars to produce changes in BMI while keeping height, arm length, and inseam fixed and exploited the correlation between body geometry and anthropometric measurements encapsulated in a statistical body shape model created from thousands of body scans. In a 2 x 2 psychophysical experiment, we investigated the relative importance of visual cues, namely shape (own shape vs. an average female body shape with equivalent height and BMI to the participant) and texture (own photorealistic texture or checkerboard pattern texture) on the ability to accurately perceive own current body weight (by asking the participant, "Is it the same weight as you?"). Our results indicate that shape (where height and BMI are fixed) had little effect on the perception of body weight. Interestingly, the participants perceived their body weight veridically when they saw their own photo-realistic texture. As compared to avatars with photo-realistic texture, the avatars with checkerboard texture needed to be significantly thinner in order to represent the participants' current weight. This suggests that in general the avatars with checkerboard texture appeared bigger. The range that the participants accepted as their own current weight was approximately a 0.83% to -6.05% BMI change tolerance range around their perceived weight. Both the shape and the texture had an effect on the reported similarity of the body parts and the whole avatar to the participant's body. This work has implications for new measures for patients with body image disorders, as well as researchers interested in creating personalized avatars for games, training applications, or virtual reality.
C1 [Piryankova, Ivelina V.; de la Rosa, Stephan; Mohler, Betty J.] Max Planck Inst Biol Cybernet, D-72076 Tubingen, Germany.
   [Stefanucci, Jeanine K.] Univ Utah, Salt Lake City, UT 84112 USA.
   [Romero, Javier; Black, Michael J.] Max Planck Inst Intelligent Syst, Stuttgart, Germany.
C3 Max Planck Society; Utah System of Higher Education; University of Utah;
   Max Planck Society
RP Piryankova, IV (corresponding author), Max Planck Inst Biol Cybernet, Spemannstr 38, D-72076 Tubingen, Germany.
EM ivelina.piryankova@tuebingen.mpg.de
OI de la Rosa, Stephan/0000-0003-2759-9589
FU Alexander von Humboldt Foundation; National Science Foundation (NSF)
   [IIS-11-16636]; EC FP7 project VR-HYPERSPACE [AAT-2011-RTD-1-285681];
   Div Of Information & Intelligent Systems; Direct For Computer & Info
   Scie & Enginr [1116636] Funding Source: National Science Foundation
FX We would like to thank Naureen Mahmood for useful suggestions and
   discussions related to the features of the body scans; Emma-Jayne
   Holderness and Sophie Lupas for scanning the participants; Joachim Tesch
   for help with Unity programming, specifically Asset Bundles; Trevor
   Dodds and Aurelie Saulton for suggestions and discussions about the
   psychophysics experiment and perception of body size; Eric Rachlin and
   Jessica Purmort for early discussions; and the Alexander von Humboldt
   Foundation and the National Science Foundation (NSF IIS-11-16636), as
   well as the EC FP7 project VR-HYPERSPACE (AAT-2011-RTD-1-285681).
CR Anguelov D, 2005, ACM T GRAPHIC, V24, P408, DOI 10.1145/1073204.1073207
   [Anonymous], 2014, P IEEE C COMP VIS PA
   ASKEVOLD F, 1975, PSYCHOTHER PSYCHOSOM, V26, P71, DOI 10.1159/000286913
   BRODIE DA, 1989, PERCEPT MOTOR SKILL, V69, P723, DOI 10.2466/pms.1989.69.3.723
   BULTHOFF HH, 1992, P NATL ACAD SCI USA, V89, P60, DOI 10.1073/pnas.89.1.60
   Cash TF, 1997, INT J EAT DISORDER, V22, P107, DOI 10.1002/(SICI)1098-108X(199709)22:2<107::AID-EAT1>3.0.CO;2-J
   CASH TF, 1983, PERS SOC PSYCHOL B, V9, P351, DOI 10.1177/0146167283093004
   de Vignemont F, 2011, CONSCIOUS COGN, V20, P82, DOI 10.1016/j.concog.2010.09.004
   Doolen J, 2009, J AM ACAD NURSE PRAC, V21, P160, DOI 10.1111/j.1745-7599.2008.00382.x
   FALLON AE, 1985, J ABNORM PSYCHOL, V94, P102, DOI 10.1037/0021-843X.94.1.102
   Farrell C, 2005, EUR EAT DISORD REV, V13, P75, DOI 10.1002/erv.622
   FREEMAN RJ, 1984, PSYCHOL MED, V14, P411, DOI 10.1017/S0033291700003652
   Fuentes CT, 2013, ACTA PSYCHOL, V144, P344, DOI 10.1016/j.actpsy.2013.06.012
   GLUCKSMAN ML, 1968, PSYCHOSOM MED, V30, P1
   Grogan S., 2007, BODY IMAGE UNDERSTAN, DOI 10.4324/9780203004340
   Hashimoto T, 2013, EUR J NEUROSCI, V37, P1747, DOI 10.1111/ejn.12187
   Hirshberg DA, 2012, LECT NOTES COMPUT SC, V7577, P242, DOI 10.1007/978-3-642-33783-3_18
   Kathleen M, 1999, P 2 INT C 3 D DIG IM, P380
   Longo MR, 2012, ACTA PSYCHOL, V141, P164, DOI 10.1016/j.actpsy.2012.07.015
   Longo MR, 2012, CURR DIR PSYCHOL SCI, V21, P140, DOI 10.1177/0963721411434982
   McCabe RE, 2001, INT J EAT DISORDER, V29, P59, DOI 10.1002/1098-108X(200101)29:1<59::AID-EAT9>3.0.CO;2-#
   Mischner IHS, 2013, BODY IMAGE, V10, P26, DOI 10.1016/j.bodyim.2012.08.004
   Perceiving Systems MPI IS, 2011, BOD VIS
   Piryankova Ivelina V., 2010, PLOS ONE IN PRESS
   Riva Giuseppe, 2011, J Diabetes Sci Technol, V5, P283
   Riva Giuseppe, 2010, NEUROSCIENCE EATING
   Rosenberg M., 1965, SOC ADOLESCENT SELF, DOI DOI /10.1515/9781400876136
   Schilder P., 1935, J NERV MENT DIS, V1, DOI DOI 10.1097/00005053-193602000-00051
   Schofield Andrew J., 2013, SHAPE PERCEPTION HUM
   SLADE PD, 1973, PSYCHOL MED, V3, P188, DOI 10.1017/S0033291700048510
   Smeets MAM, 1997, BRIT J CLIN PSYCHOL, V36, P263, DOI 10.1111/j.2044-8260.1997.tb01412.x
   Stuart A., 1994, KENDALLS ADV THEORY
   Stunckard A.J., 1983, GENETICS NEUROLOGICA
   Thompson P, 2011, I-PERCEPTION, V2, P69, DOI 10.1068/i0405
   TRAUB AC, 1964, ARCH GEN PSYCHIAT, V11, P53
   Wallraven C, 2002, LECT NOTES COMPUT SC, V2525, P651
   Weiss A, 2011, IEEE I CONF COMP VIS, P1951, DOI 10.1109/ICCV.2011.6126465
   Wichmann FA, 2001, PERCEPT PSYCHOPHYS, V63, P1293, DOI 10.3758/BF03194544
   Willemsen P, 2008, PRESENCE-TELEOP VIRT, V17, P91, DOI 10.1162/pres.17.1.91
NR 39
TC 40
Z9 44
U1 4
U2 22
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD JUN
PY 2014
VL 11
IS 3
SI SI
AR 13
DI 10.1145/2641568
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA AU0KM
UT WOS:000345311700004
OA Green Submitted, Bronze
DA 2024-07-18
ER

PT J
AU Pacchierotti, C
   Tirmizi, A
   Prattichizzo, D
AF Pacchierotti, Claudio
   Tirmizi, Asad
   Prattichizzo, Domenico
TI Improving Transparency in Teleoperation by Means of Cutaneous Tactile
   Force Feedback
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Cutaneous force feedback; tactile force feedback; robotic teleoperation;
   wearable haptics
ID EQUIVALENCE; PERFORMANCE
AB A study on the role of cutaneous and kinesthetic force feedback in teleoperation is presented. Cutaneous cues provide less transparency than kinesthetic force, but they do not affect the stability of the teleoperation system. On the other hand, kinesthesia provides a compelling illusion of telepresence but affects the stability of the haptic loop. However, when employing common grounded haptic interfaces, it is not possible to independently control the cutaneous and kinesthetic components of the interaction. For this reason, many control techniques ensure a stable interaction by scaling down both kinesthetic and cutaneous force feedback, even though acting on the cutaneous channel is not necessary.
   We discuss here the feasibility of a novel approach. It aims at improving the realism of the haptic rendering, while preserving its stability, by modulating cutaneous force to compensate for a lack of kinesthesia. We carried out two teleoperation experiments, evaluating (1) the role of cutaneous stimuli when reducing kinesthesia and (2) the extent to which an overactuation of the cutaneous channel can fully compensate for a lack of kinesthetic force feedback. Results showed that, to some extent, it is possible to compensate for a lack of kinesthesia with the aforementioned technique, without significant performance degradation. Moreover, users showed a high comfort level in using the proposed system.
C1 [Pacchierotti, Claudio; Tirmizi, Asad; Prattichizzo, Domenico] Univ Siena, Dept Informat Engn & Math, I-53100 Siena, SI, Italy.
   [Pacchierotti, Claudio; Prattichizzo, Domenico] Ist Italiano Tecnol, Genoa, Italy.
C3 University of Siena; Istituto Italiano di Tecnologia - IIT
RP Pacchierotti, C (corresponding author), Univ Siena, Dept Informat Engn & Math, Via Roma 56, I-53100 Siena, SI, Italy.
RI Pacchierotti, Claudio/G-7304-2011
OI Pacchierotti, Claudio/0000-0002-8006-9168; PRATTICHIZZO,
   Domenico/0000-0001-9051-9698
FU European Commission [270460]; Active Constraints Technologies for
   Ill-defined or Volatile Environments [FP7-ICT-2009-6-2.1]; Cognitive
   Systems and Robotics [601165]; WEARable HAPtics for Humans and Robots"
   (WEARHAP) [FP7-ICT-2011-9-2.1]
FX This work has been partially supported by the European Commission with
   the Collaborative Project no. 270460, "Active Constraints Technologies
   for Ill-defined or Volatile Environments" (ACTIVE), within the
   FP7-ICT-2009-6-2.1 program "Cognitive Systems and Robotics", and with
   the Collaborative Project no. 601165, "WEARable HAPtics for Humans and
   Robots" (WEARHAP), within the FP7-ICT-2011-9-2.1 program "Cognitive
   Systems and Robotics."
CR Abbott JJ, 2007, SPRINGER TRAC ADV RO, V28, P49
   Birznieks I, 2001, J NEUROSCI, V21, P8222, DOI 10.1523/JNEUROSCI.21-20-08222.2001
   Chen C, 2010, BIOPHARM INT, V23, P40
   Chinello F., 2012, 2012 IEEE Haptics Symposium (HAPTICS), P71, DOI 10.1109/HAPTIC.2012.6183772
   De Lorenzo D, 2011, INT J MED ROBOT COMP, V7, P268, DOI 10.1002/rcs.391
   Draper JV, 1998, HUM FACTORS, V40, P354, DOI 10.1518/001872098779591386
   EDIN BB, 1995, J PHYSIOL-LONDON, V487, P243, DOI 10.1113/jphysiol.1995.sp020875
   Franken M, 2011, IEEE T ROBOT, V27, P741, DOI 10.1109/TRO.2011.2142430
   Gleeson BT, 2010, IEEE T HAPTICS, V3, P297, DOI 10.1109/ToH.2010.8
   Hannaford B., 1987, P WORKSH SPAC TEL, V2
   Hashtrudi-Zaad K, 2002, IEEE T ROBOTIC AUTOM, V18, P108, DOI 10.1109/70.988981
   Hayward V., 2004, SENSOR REV, V24, P16
   Hokayem PF, 2006, AUTOMATICA, V42, P2035, DOI 10.1016/j.automatica.2006.06.027
   Johnson KO, 2001, CURR OPIN NEUROBIOL, V11, P455, DOI 10.1016/S0959-4388(00)00234-8
   Kitagawa M, 2005, J THORAC CARDIOV SUR, V129, P151, DOI 10.1016/j.jtcvs.2004.05.029
   Kuchenbecker KJ, 2008, SYMPOSIUM ON HAPTICS INTERFACES FOR VIRTUAL ENVIRONMENT AND TELEOPERATOR SYSTEMS 2008, PROCEEDINGS, P239
   Lauzon C, 2009, AM STAT, V63, P147, DOI 10.1198/tast.2009.0029
   LAWRENCE DA, 1993, IEEE T ROBOTIC AUTOM, V9, P624, DOI 10.1109/70.258054
   Limentani GB, 2005, ANAL CHEM, V77, p221A, DOI 10.1021/ac053390m
   MASSIMINO MJ, 1994, HUM FACTORS, V36, P145, DOI 10.1177/001872089403600109
   Minamizawa K., 2007, P IEEE, P133
   Moody L, 2002, STUD HEALTH TECHNOL, V85, P304
   Pacchierotti Claudio, 2012, Haptics: Perception, Devices, Mobility, and Communication. Proceedings International Conference (EuroHaptics 2012), P373, DOI 10.1007/978-3-642-31401-8_34
   Pacchierotti C, 2012, P IEEE RAS-EMBS INT, P32, DOI 10.1109/BioRob.2012.6290853
   Prattichizzo D, 2012, IEEE T HAPTICS, V5, P289, DOI [10.1109/TOH.2012.15, 10.1109/ToH.2012.15]
   Prattichizzo D, 2010, 2010 IEEE RO-MAN, P676, DOI 10.1109/ROMAN.2010.5598606
   Prattichizzo D, 2010, LECT NOTES COMPUT SC, V6191, P125, DOI 10.1007/978-3-642-14064-8_19
   Salcudean S. E., 1998, Control Problems in Robotics and Automation, P51, DOI 10.1007/BFb0015076
   Schoomnaker RE, 2006, IEEE SYS MAN CYBERN, P2464, DOI 10.1109/ICSMC.2006.385233
   SHERIDAN TB, 1995, CONTROL ENG PRACT, V3, P205, DOI 10.1016/0967-0661(94)00078-U
   Solazzi Massimiliano., 2010, 19 INT S ROBOT HUMAN, P1
   Stein H.L., 1988, ENG MAT HDB, V2, P167
   Tirmizi A, 2013, 2013 WORLD HAPTICS CONFERENCE (WHC), P371, DOI 10.1109/WHC.2013.6548437
   Tsagarakis NG, 2005, WORLD HAPTICS CONFERENCE: FIRST JOINT EUROHAPTICS CONFERENCE AND SYMPOSIUM ON HAPTIC INTERFACES FOR VIRUTUAL ENVIRONMENT AND TELEOPERATOR SYSTEMS, PROCEEDINGS, P214
   Wagner CR, 2002, 10TH SYMPOSIUM ON HAPTIC INTERFACES FOR VIRTUAL ENVIRONMENT AND TELEOPERATOR SYSTEMS, PROCEEDINGS, P73
   Welkenhuysen-Gybels J, 2003, J CROSS CULT PSYCHOL, V34, P702, DOI 10.1177/0022022103257070
   ZILLES CB, 1995, IROS '95 - 1995 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS: HUMAN ROBOT INTERACTION AND COOPERATIVE ROBOTS, PROCEEDINGS, VOL 3, P146, DOI 10.1109/IROS.1995.525876
NR 37
TC 81
Z9 89
U1 1
U2 30
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD APR
PY 2014
VL 11
IS 1
AR 4
DI 10.1145/2604969
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AG7CF
UT WOS:000335574900004
DA 2024-07-18
ER

PT J
AU Turchet, L
   Serafin, S
   Cesari, P
AF Turchet, Luca
   Serafin, Stefania
   Cesari, Paola
TI Walking Pace Affected by Interactive Sounds Simulating Stepping on
   Different Terrains
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Human Factors; Walking; interactive auditory feedback; gait patterns
ID RHYTHMIC AUDITORY-STIMULATION; PHYSICAL-THERAPY; GAIT PERFORMANCE;
   FEEDBACK; STROKE; REHABILITATION; FACILITATION; OUTCOMES; SPEECH; WORLD
AB This article investigates whether auditory feedback affects natural locomotion patterns. Individuals were provided with footstep sounds simulating different surface materials. The sounds were interactively generated using shoes with pressure sensors. Results showed that subjects' walking speed changed as a function of the type of simulated ground material. This effect may arise due to the presence of conflicting information between the auditory and foot-haptic modality, or because of an adjustment of locomotion to the physical properties evoked by the sounds simulating the ground materials. The results reported in this study suggest that auditory feedback may be more important in the regulation of walking in natural environments than has been acknowledged. Furthermore, auditory feedback could be used to develop novel approaches to the design of therapeutic and rehabilitation procedures for locomotion.
C1 [Turchet, Luca; Serafin, Stefania] Aalborg Univ, Dept Architecture Design & Media Technol, DK-2450 Copenhagen, Denmark.
   [Cesari, Paola] Univ Verona, Dept Neurol Neuropsychol Morphol & Movement Sci, I-37100 Verona, Italy.
C3 Aalborg University; University of Verona
RP Turchet, L (corresponding author), Aalborg Univ, Dept Architecture Design & Media Technol, AC Meyers Vaenge 15, DK-2450 Copenhagen, Denmark.
EM tur@create.aau.dk; sts@create.aau.dk; paola.cesari@univr.it
RI Turchet, Luca/M-9679-2013; Serafin, Stefania/AAX-9409-2020
OI Turchet, Luca/0000-0003-0711-8098; Cesari, Paola/0000-0002-1705-6065;
   Serafin, Stefania/0000-0001-6971-1132
FU European Community's Seventh Framework Programme under FET-Open grant
   [222107]; Danish Council for Independent Research-Technology and
   Production Sciences (FTP) [12-131985]
FX The research leading to these results has received funding from the
   European Community's Seventh Framework Programme under FET-Open grant
   agreement 222107 NIW-Natural Interactive Walking and from the Danish
   Council for Independent Research-Technology and Production Sciences
   (FTP), grant no. 12-131985.
CR Avanzini F., 2001, Proc. COST-G6 Conf. Digital Audio Effects (DAFX-01), P61
   Avanzini F, 2006, LECT NOTES COMPUT SC, V4129, P24
   Baram Y, 2007, J NEUROL SCI, V254, P90, DOI 10.1016/j.jns.2007.01.003
   Baram Y, 2009, 2009 VIRTUAL REHABILITATION INTERNATIONAL CONFERENCE, P146, DOI 10.1109/ICVR.2009.5174222
   Bresin R., 2010, P INT SON WORKSH, P51
   CHASE RA, 1959, SCIENCE, V129, P903, DOI 10.1126/science.129.3353.903
   Conklyn D, 2010, NEUROREHAB NEURAL RE, V24, P835, DOI 10.1177/1545968310372139
   Cook PR, 1997, COMPUT MUSIC J, V21, P38, DOI 10.2307/3681012
   DiFranco D. E., 1997, Proceedings of the ASME Dynamic Systems and Control Division, P17
   Ferrell WR, 1965, IEEE T HUM FACT ENG, P1
   Fontana F, 2012, WALKING SENSES PERCE
   GAVER WW, 1993, ECOL PSYCHOL, V5, P1, DOI 10.1207/s15326969eco0501_1
   GAVER WW, 1993, ECOL PSYCHOL, V5, P285, DOI 10.1207/s15326969eco0504_2
   Giordano BL, 2012, J ACOUST SOC AM, V131, P4002, DOI 10.1121/1.3699205
   Hayden R, 2009, INT J NEUROSCI, V119, P2183, DOI 10.3109/00207450903152609
   HUNT KH, 1975, J APPL MECH-T ASME, V42, P440, DOI 10.1115/1.3423596
   KALMUS H, 1955, NATURE, V175, P1078, DOI 10.1038/1751078a0
   Kim S, 2011, N KOREAN REV, V7, P3
   Kobayashi Y, 2008, IEEE T NEUR SYS REH, V16, P99, DOI 10.1109/TNSRE.2007.910283
   Kwak EE, 2007, J MUSIC THER, V44, P198, DOI 10.1093/jmt/44.3.198
   Lécuyer A, 2009, PRESENCE-TELEOP VIRT, V18, P39, DOI 10.1162/pres.18.1.39
   Lee BS, 1951, J SPEECH HEAR DISORD, V16, P53, DOI 10.1044/jshd.1601.53
   Lim I, 2005, CLIN REHABIL, V19, P695, DOI 10.1191/0269215505cr906oa
   McIntosh GC, 1997, J NEUROL NEUROSUR PS, V62, P22, DOI 10.1136/jnnp.62.1.22
   Nordahl R, 2010, P IEEE VIRT REAL ANN, P147, DOI 10.1109/VR.2010.5444796
   Perkell J, 1997, SPEECH COMMUN, V22, P227, DOI 10.1016/S0167-6393(97)00026-5
   Queralt A, 2008, J PHYSIOL-LONDON, V586, P4453, DOI 10.1113/jphysiol.2008.156042
   Rocchesso D., 2004, Proceedings Of The Int. Workshop On Interactive Sonification, P1
   Roerdink M, 2011, GAIT POSTURE, V33, P690, DOI 10.1016/j.gaitpost.2011.03.001
   Rubinstein TC, 2002, MOVEMENT DISORD, V17, P1148, DOI 10.1002/mds.10259
   Schauer M, 2003, CLIN REHABIL, V17, P713, DOI 10.1191/0269215503cr668oa
   Scott DJ, 2007, NEURON, V55, P325, DOI 10.1016/j.neuron.2007.06.028
   Serafin S., 2010, Proceedings of Eurohaptics, P61
   Sheridan T., 1992, Presence: Teleoperators and Virtual Environments, V1, P120, DOI DOI 10.1162/PRES.1992.1.1.120
   STAUM MJ, 1983, J MUSIC THER, V20, P69, DOI 10.1093/jmt/20.2.69
   Styns F, 2007, HUM MOVEMENT SCI, V26, P769, DOI 10.1016/j.humov.2007.07.007
   Thaut M.H., 1993, J NEUROL REHABIL, V7, P9, DOI [10.1177/136140969300700103, DOI 10.1177/136140969300700103]
   Thaut MH, 1997, J NEUROL SCI, V151, P207, DOI 10.1016/S0022-510X(97)00146-9
   Thaut MH, 2010, MUSIC PERCEPT, V27, P263, DOI 10.1525/MP.2010.27.4.263
   Thaut MichaelH., 2005, Rhythm, Music, and the Brain: Scientific Foundations and Clinical Applications
   Turchet Luca, 2010, 2010 IEEE 12th International Workshop on Multimedia Signal Processing (MMSP), P269, DOI 10.1109/MMSP.2010.5662031
   Turchet L, 2010, P DIG AUD EFF C, P161
   Turchet L, 2013, APPL ACOUST, V74, P566, DOI 10.1016/j.apacoust.2012.10.010
   Van Peppen RPS, 2004, CLIN REHABIL, V18, P833, DOI 10.1191/0269215504cr843oa
   VANBERGEIJK WA, 1959, PERCEPT MOTOR SKILL, V9, P347, DOI DOI 10.2466/PMS.9.7.347-357
   Ware C., 1994, ACM T COMPUT-HUM INT, V1, P331, DOI [DOI 10.1145/198425.198426[39]B, DOI 10.1145/198425.198426]
   YATES AJ, 1963, PSYCHOL BULL, V60, P213, DOI 10.1037/h0044155
NR 47
TC 26
Z9 28
U1 0
U2 18
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD OCT
PY 2013
VL 10
IS 4
AR 23
DI 10.1145/2536764.2536770
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA 281YK
UT WOS:000329136700006
DA 2024-07-18
ER

PT J
AU Selmanovic, E
   Debattista, K
   Bashford-Rogers, T
   Chalmers, A
AF Selmanovic, Elmedin
   Debattista, Kurt
   Bashford-Rogers, Thomas
   Chalmers, Alan
TI Generating Stereoscopic HDR Images Using HDR-LDR Image Pairs
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Design; Experimentation; Performance; Stereoscopy; high dynamic range
   imaging; stereo correspondence; expansion operators
ID CAMERA
AB A number of novel imaging technologies have been gaining popularity over the past few years. Foremost among these are stereoscopy and high dynamic range (HDR) Imaging. While a large body of research has looked into each of these imaging technologies independently, very little work has attempted to combine them. This is mostly due to the current limitations in capture and display. In this article, we mitigate problems of capturing Stereoscopic HDR (SHDR) that would potentially require two HDR cameras, by capturing an HDR and LDR pair and using it to generate 3D stereoscopic HDR content. We ran a detailed user study to compare four different methods of generating SHDR content. The methods investigated were the following: two based on expanding the luminance of the LDR image, and two utilizing stereo correspondence methods, which were adapted for our purposes. Results demonstrate that one of the stereo correspondence methods may be considered perceptually indistinguishable from the ground truth (image pair captured using two HDR cameras), while the other methods are all significantly distinct from the ground truth.
C1 [Selmanovic, Elmedin; Debattista, Kurt; Bashford-Rogers, Thomas; Chalmers, Alan] Univ Warwick, Coventry CV4 7AL, W Midlands, England.
C3 University of Warwick
RP Selmanovic, E (corresponding author), Univ Warwick, WMG, Int Digital Lab, Coventry CV4 7AL, W Midlands, England.
EM elmedins@gmail.com
RI Selmanović, Elmedin/JCD-7596-2023; Selmanović, Elmedin/ABA-1403-2020;
   Selmanović, Elmedin/KIE-9609-2024
OI Selmanović, Elmedin/0000-0003-4245-3588
FU EPSRC grant [EP/1038780/1]; goHDR Ltd. [14345-87267]; EPSRC
   [EP/I006192/1] Funding Source: UKRI
FX This work was supported in part by EPSRC grant EP/1038780/1. E.
   Selmanovic is funded by goHDR Ltd. within TSR project 14345-87267. This
   work is part of EU COST Action IC1005.
CR Akyüz AG, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1276377.1276425
   [Anonymous], EUR 2002 EUR ASS SEP
   [Anonymous], BIOMETRIKA
   Banterle F, 2011, ADVANCED HIGH DYNAMIC RANGE IMAGING: THEORY AND PRACTICE, P1
   Banterle F., 2006, P 4 INT C COMP GRAPH, P349
   Banterle F, 2009, COMPUT GRAPH FORUM, V28, P2343, DOI 10.1111/j.1467-8659.2009.01541.x
   Banterle F, 2009, COMPUT GRAPH FORUM, V28, P13, DOI 10.1111/j.1467-8659.2008.01176.x
   Bhat P., 2007, P ESRT GREN FRANC, P327
   Chalmers A., 2009, ACM SIGGRAPH ASIA 2009 Art Gallery #38; Emerging Technologies: Adaptation, SIGGRAPH ASIA '09, P71
   CRONE RA, 1992, DOC OPHTHALMOL, V81, P1, DOI 10.1007/BF00155009
   CYGANEK B, 2009, INTRO 3D COMP VIS TE
   David H. A., 1988, The Method of Paired Comparisons
   Dodgson NA, 2004, PROC SPIE, V5291, P36, DOI 10.1117/12.529999
   Heidrich Wolfgang, ERIK REINHARD
   Kolmogorov V, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P508, DOI 10.1109/ICCV.2001.937668
   Lang M., 2010, ACM SIGGRAPH PAPERS
   Ledda P, 2005, ACM T GRAPHIC, V24, P640, DOI 10.1145/1073204.1073242
   Lin HY, 2009, IEEE IMAGE PROC, P4305, DOI 10.1109/ICIP.2009.5413665
   Lo CH, 2010, VISUAL COMPUT, V26, P97, DOI 10.1007/s00371-009-0379-4
   Mei X, 2011, PROC CVPR IEEE, P1257
   Mendiburu Bernard., 2009, 3D Movie Making: Stereoscopic Digital Cinema From Scrip to Screen
   NAKAYAMA K, 1990, VISION RES, V30, P1811, DOI 10.1016/0042-6989(90)90161-D
   Navarro F, 2011, ACM T APPL PERCEPT, V8, DOI 10.1145/2010325.2010330
   Peaeson E., 1976, BIOMETRIKA TABLES ST
   Pouli T, 2011, COMPUT GRAPH-UK, V35, P67, DOI 10.1016/j.cag.2010.11.003
   Reinhard E, 2002, ACM T GRAPHIC, V21, P267, DOI 10.1145/566570.566575
   Rubinstein M, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866186
   Ruppertsberg AI, 2007, J VIS COMMUN IMAGE R, V18, P429, DOI 10.1016/j.jvcir.2007.06.007
   Sawhney HS, 2001, COMP GRAPH, P451, DOI 10.1145/383259.383312
   Scharstein D, 2001, IEEE WORKSHOP ON STEREO AND MULTI-BASELINE VISION, PROCEEDINGS, P131, DOI 10.1023/A:1014573219977
   Seetzen H, 2004, ACM T GRAPHIC, V23, P760, DOI 10.1145/1015706.1015797
   Selmanovic E., 2012, THEORY PRACTICE COMP
   SERVOS P, 1992, VISION RES, V32, P1513, DOI 10.1016/0042-6989(92)90207-Y
   Shirley P, 2009, Fundamentals of Computer Graphics.
   Tocci MD, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964936
   WARD G., 2006, ACM SIGGRAPH COURSES, P8
   Ware C, 2008, ACM T APPL PERCEPT, V5, DOI 10.1145/1279640.1279642
   Wilburn B, 2005, ACM T GRAPHIC, V24, P765, DOI 10.1145/1073204.1073259
   [No title captured]
NR 39
TC 9
Z9 9
U1 0
U2 5
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD FEB
PY 2013
VL 10
IS 1
AR 3
DI 10.1145/2422105.2422108
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AN8YO
UT WOS:000340892200003
DA 2024-07-18
ER

PT J
AU Newsham, GR
   Cetegen, D
   Veitch, JA
   Whitehead, L
AF Newsham, Guy R.
   Cetegen, Duygu
   Veitch, Jennifer A.
   Whitehead, Lorne
TI Comparing Lighting Quality Evaluations of Real Scenes with those from
   High Dynamic Range and Conventional Images
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Design; Experimentation; Human Factors; Measurement; High dynamic range;
   lighting; luminance; virtual reality
ID SIMULATION
AB Thirty-nine participants viewed six interior scenes in an office/laboratory building and rated them for brightness, uniformity, pleasantness, and glare. The scenes were viewed in three presentation modes: participants saw the real space and images of the spaces on a 17-inch computer monitor in both conventional and high dynamic range (HDR) mode. HDR mode allowed the high range of luminances in the real scene to be accurately reproduced, with maximum luminances more than 10 times higher than those in the conventional images. For those participants who saw the images before the real spaces (the most relevant order for practical applications), the HDR images were rated as significantly more realistic than the conventional images. However, this effect was limited to scenes with relatively large areas of high luminance, which in this study was represented by scenes with windows and daylight. Ratings of the HDR images were significantly related to simple photometric descriptors of the images in the expected manner: Brightness and glare ratings were positively correlated with overall and elevated luminance, and nonuniformity ratings were positively correlated with luminance variability. These results suggest that for evaluations of visual appearance of interior scenes featuring large areas of high luminance, the HDR method may be used as a surrogate for experiencing a real space both for lighting quality research, and in the design process.
C1 [Newsham, Guy R.; Cetegen, Duygu; Veitch, Jennifer A.] Natl Res Council Canada, Inst Res Construct, Ottawa, ON K1A 0R6, Canada.
   [Whitehead, Lorne] Univ British Columbia, Dept Phys & Astron, Vancouver, BC V6T 1Z1, Canada.
C3 National Research Council Canada; University of British Columbia
RP Newsham, GR (corresponding author), Natl Res Council Canada, Inst Res Construct, Ottawa, ON K1A 0R6, Canada.
OI Veitch, Jennifer/0000-0003-3183-4537; Whitehead,
   Lorne/0000-0002-4170-0033
FU National Research Council Canada (NRC), Istanbul Technical University;
   University of British Columbia (UBC)
FX This work was sponsored by the National Research Council Canada (NRC),
   Istanbul Technical University, and the NSERC/3M Industrial Chair Program
   of the University of British Columbia (UBC).
CR Akyüz AG, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1276377.1276425
   [Anonymous], 1999, P 1999 IEEE COMP SOC
   Bellman K, 2000, APPL ARTIF INTELL, V14, P93, DOI 10.1080/088395100117179
   Debevec P., 1997, P ACM SIGGRAPH, P369, DOI DOI 10.1145/258734.258884
   Dockerty T, 2006, AGR ECOSYST ENVIRON, V114, P103, DOI 10.1016/j.agee.2005.11.008
   HENDRICK C, 1977, ENVIRON BEHAV, V9, P491, DOI 10.1177/001391657794003
   Inanici MN, 2006, LIGHTING RES TECHNOL, V38, P123, DOI [10.1191/1365782806li164oa, 10.1191/13657828061i164oa]
   Kemeny A, 2003, TRENDS COGN SCI, V7, P31, DOI 10.1016/S1364-6613(02)00011-6
   Kline R., 1998, PRINCIPLE PRACTICE S
   Mahdavi A, 2002, J ILLUM ENG SOC, V31, P11, DOI 10.1080/00994480.2002.10748388
   MANN S, 1995, IS&T'S 48TH ANNUAL CONFERENCE - IMAGING ON THE INFORMATION SUPERHIGHWAY, FINAL PROGRAM AND PROCEEDINGS, P442
   Newsham G. R., 2005, Lighting Research & Technology, V37, P93, DOI 10.1191/1365782805li132oa
   NEWSHAM GR, 2002, P ARCC EAAE C ARCH R
   Oi Naoyuki, 2005, Journal of Physiological Anthropology and Applied Human Science, V24, P87, DOI 10.2114/jpa.24.87
   Reinhard E., 2006, HIGH DYNAMIC RANGE I, DOI 10.1016/B978-012585263-0/50005-1
   SEETZEN H, 2003, P SOC INF DISPL INT
   Seetzen H, 2006, SID Symp Dig Tech Pap, V37, P1229
   Sheppard SRJ, 2005, ENVIRON SCI POLICY, V8, P637, DOI 10.1016/j.envsci.2005.08.002
   Stamps AE, 1998, ENVIRON PLANN B, V25, P825, DOI 10.1068/b250825
   STAMPS AE, 1990, PERCEPT MOTOR SKILL, V71, P907, DOI 10.2466/pms.1990.71.3.907
   Tuaycharoen N., 2005, Lighting Research & Technology, V37, P329, DOI 10.1191/1365782805li147oa
   Whitehead L., 2005, US Patent, Patent No. [6,891,672, 6891672]
   YOSHIDA A, 2006, P EUR, P415
NR 23
TC 16
Z9 19
U1 2
U2 14
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD FEB
PY 2010
VL 7
IS 2
AR 13
DI 10.1145/1670671.1670677
PG 26
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 563HF
UT WOS:000275118100006
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Van den Berg, R
   Cornelissen, FW
   Roerdink, JBTM
AF Van den Berg, Ronald
   Cornelissen, Frans W.
   Roerdink, Jos B. T. M.
TI Perceptual Dependencies in Information Visualization Assessed by Complex
   Visual Search
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Design; Experimentation; Human Factors; Measurement; Performance;
   Verification; Color; feature hierarchy; feature interaction; human
   vision; information visualization; node-link diagrams; orientation;
   perceptual dependencies; psychophysics; visual features; visual search
ID SCIENTIFIC VISUALIZATION; PSYCHOPHYSICS TOOLBOX; TEXTURE SEGREGATION;
   EYE-MOVEMENTS; COLOR-VISION; ORIENTATION; DIMENSIONS; MECHANISMS;
   ATTENTION; SOFTWARE
AB A common approach for visualizing data sets is to map them to images in which distinct data dimensions are mapped to distinct visual features, such as color, size and orientation. Here, we consider visualizations in which different data dimensions should receive equal weight and attention. Many of the end-user tasks performed on these images involve a form of visual search. Often, it is simply assumed that features can be judged independently of each other in such tasks. However, there is evidence for perceptual dependencies when simultaneously presenting multiple features. Such dependencies could potentially affect information visualizations that contain combinations of features for encoding information and, thereby, bias subjects into unequally weighting the relevance of different data dimensions. We experimentally assess (1) the presence of judgment dependencies in a visualization task (searching for a target node in a node-link diagram) and (2) how feature contrast relates to salience. From a visualization point of view, our most relevant findings are that (a) to equalize saliency (and thus bottom-up weighting) of size and color, color contrasts have to become very low. Moreover, orientation is less suitable for representing information that consists of a large range of data values, because it does not show a clear relationship between contrast and salience; (b) color and size are features that can be used independently to represent information, at least as far as the range of colors that were used in our study are concerned; (c) the concept of (static) feature salience hierarchies is wrong; how salient a feature is compared to another is not fixed, but a function of feature contrasts; (d) final decisions appear to be as good an indicator of perceptual performance as indicators based on measures obtained from individual fixations. Eye tracking, therefore, does not necessarily present a benefit for user studies that aim at evaluating performance in search tasks.
C1 [Van den Berg, Ronald; Roerdink, Jos B. T. M.] Univ Groningen, Inst Math & Comp Sci, NL-9700 AV Groningen, Netherlands.
   [Cornelissen, Frans W.] Univ Groningen, Univ Med Ctr Groningen, Sch Behav & Cognit Neurosci, Lab Expt Ophthalmol, NL-9700 RB Groningen, Netherlands.
   [Cornelissen, Frans W.] Univ Groningen, Univ Med Ctr Groningen, Sch Behav & Cognit Neurosci, BCN NeuroImaging Ctr, NL-9700 RB Groningen, Netherlands.
C3 University of Groningen; University of Groningen; University of
   Groningen
RP Van den Berg, R (corresponding author), Univ Groningen, Inst Math & Comp Sci, POB 800, NL-9700 AV Groningen, Netherlands.
EM r.van.den.berg@rug.nl; f.w.cornelissen@rug.nl; j.b.t.m.roerdink@rug.nl
RI Roerdink, Jos B.T.M/B-3631-2008
CR [Anonymous], 1986, VISUAL DISPLAY QUANT
   [Anonymous], 2000, Information Visualization: Perception for Design
   Bertin J., 1983, SEMIOLOGY GRAPHICS D
   Brainard DH, 1997, SPATIAL VISION, V10, P433, DOI 10.1163/156856897X00357
   Brenner E, 2002, PERCEPTION, V31, P225, DOI 10.1068/p02sp
   BRENNER E, 1991, NATURWISSENSCHAFTEN, V78, P70, DOI 10.1007/BF01206259
   CALLAGHAN TC, 1989, PERCEPT PSYCHOPHYS, V46, P299, DOI 10.3758/BF03204984
   CORNELISSEN FW, 2000, BEHAV RES METH INSTR, V34, P613
   DESIMONE R, 1995, ANNU REV NEUROSCI, V18, P193, DOI 10.1146/annurev-psych-122414-033400
   EBERT D, 2004, EXTENDING VISUALIZAT, P771
   Gegenfurtner KR, 2003, NAT REV NEUROSCI, V4, P563, DOI 10.1038/nrn1138
   Hannus A, 2006, J VISION, V6, P523, DOI 10.1167/6.4.15
   Healey CG, 1999, IEEE T VIS COMPUT GR, V5, P145, DOI 10.1109/2945.773807
   Healey CG, 1998, VISUALIZATION '98, PROCEEDINGS, P111, DOI 10.1109/VISUAL.1998.745292
   Healey ChristopherG., 2001, IJCAI, P371
   HOUSE D, 2002, P ADV VIS INT AVI 02, P148
   INTERRANTE V, 2004, ART SCI VISUALIZATIO
   Ives HE, 1912, PHILOS MAG, V24, P149, DOI 10.1080/14786440708637317
   Johnson C, 2004, IEEE COMPUT GRAPH, V24, P13, DOI 10.1109/MCG.2004.20
   Kosara R, 2003, IEEE COMPUT GRAPH, V23, P20, DOI 10.1109/MCG.2003.1210860
   Li ZP, 2002, TRENDS COGN SCI, V6, P9, DOI 10.1016/S1364-6613(00)01817-9
   LIVINGSTONE M, 1988, SCIENCE, V240, P740, DOI 10.1126/science.3283936
   LURIA SM, 1975, PERCEPT PSYCHOPHYS, V17, P303, DOI 10.3758/BF03203215
   Nothdurft HC, 2000, VISION RES, V40, P1183, DOI 10.1016/S0042-6989(00)00031-6
   NOTHDURFT HC, 1993, SPATIAL VISION, V7, P341, DOI 10.1163/156856893X00487
   Pelli DG, 1997, SPATIAL VISION, V10, P437, DOI 10.1163/156856897X00366
   Roe AW, 1999, J NEUROPHYSIOL, V82, P2719, DOI 10.1152/jn.1999.82.5.2719
   Shannon P, 2003, GENOME RES, V13, P2498, DOI 10.1101/gr.1239303
   Snowden RJ, 1998, J EXP PSYCHOL HUMAN, V24, P1354, DOI 10.1037/0096-1523.24.5.1354
   Tory M, 2004, IEEE T VIS COMPUT GR, V10, P72, DOI 10.1109/TVCG.2004.1260759
   TREISMAN AM, 1980, COGNITIVE PSYCHOL, V12, P97, DOI 10.1016/0010-0285(80)90005-5
   VONDERHEYDT R, 2003, SEARCHING NEURAL MEC, P106
   WARE C, 1995, ACM T GRAPHIC, V14, P3, DOI 10.1145/200972.200974
   Weigle C, 2000, PROC GRAPH INTERF, P163
   Williams DE, 2001, PSYCHON B REV, V8, P476, DOI 10.3758/BF03196182
   Yoshioka T, 1996, BEHAV BRAIN RES, V76, P71, DOI 10.1016/0166-4328(95)00184-0
   ZEKI S, 1988, NATURE, V335, P311, DOI 10.1038/335311a0
NR 37
TC 5
Z9 7
U1 1
U2 11
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD JAN
PY 2008
VL 4
IS 4
AR 22
DI 10.1145/1278760.1278763
PG 21
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA 450YH
UT WOS:000266437400003
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Drakopoulos, P
   Koulieris, GA
   Mania, K
AF Drakopoulos, Panagiotis
   Koulieris, George-alex
   Mania, Katerina
TI Eye Tracking Interaction on Unmodified Mobile VR Headsets Using the
   Selfie Camera
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Mobile VR; eye tracking
ID ROBUST PUPIL DETECTION; GAZE
AB Input methods for interaction in smartphone-based virtual and mixed reality (VR/MR) are currently based on uncomfortable head tracking controlling a pointer on the screen. User fixations are a fast and natural input method for VR/MR interaction. Previously, eye tracking in mobile VR suffered from low accuracy, long processing time, and the need for hardware add-ons such as anti-reflective lens coating and infrared emitters. We present an innovative mobile VR eye tracking methodology utilizing only the eye images from the front-facing (selfie) camera through the headset's lens, without any modifications. Our system first enhances the low-contrast, poorly lit eye images by applying a pipeline of customised low-level image enhancements suppressing obtrusive lens reflections. We then propose an iris region-of-interest detection algorithm that is run only once. This increases the iris tracking speed by reducing the iris search space in mobile devices. We iteratively fit a customised geometric model to the iris to refine its coordinates. We display a thin bezel of light at the top edge of the screen for constant illumination. A confidence metric calculates the probability of successful iris detection. Calibration and linear gaze mapping between the estimated iris centroid and physical pixels on the screen results in low latency, real-time iris tracking. A formal study confirmed that our system's accuracy is similar to eye trackers in commercial VR headsets in the central part of the headset's field-of-view. In a VR game, gaze-driven user completion time was as fast as with head-tracked interaction, without the need for consecutive head motions. In a VR panorama viewer, users could successfully switch between panoramas using gaze.
C1 [Drakopoulos, Panagiotis; Mania, Katerina] Tech Univ Crete, Sch Elect & Comp Engn, Univ Campus Kounoupidiana, Khania 73100, Greece.
   [Koulieris, George-alex] Univ Durham, Math Sci & Comp Sci Bldg, Upper Mountjoy Campus,Stockton Rd, Durham DH1 3LE, England.
C3 Technical University of Crete; Durham University
RP Drakopoulos, P (corresponding author), Tech Univ Crete, Sch Elect & Comp Engn, Univ Campus Kounoupidiana, Khania 73100, Greece.
EM panosdrak@hotmail.com; georgios.a.koulieris@durham.ac.uk;
   k.mania@ced.tuc.gr
RI Koulieris, George/AAJ-5666-2020
OI Koulieris, George/0000-0003-1610-6240
CR Ahuja Karan, 2018, Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, V2, DOI 10.1145/3214260
   [Anonymous], 2017, P 15 ACM C EMB NETW
   BALOH RW, 1975, NEUROLOGY, V25, P1065, DOI 10.1212/WNL.25.11.1065
   Bradski G., 2008, LEARNING OPENCV
   Cerrolaza JJ, 2012, ACM T COMPUT-HUM INT, V19, DOI 10.1145/2240156.2240158
   Dongheng L., 2005, 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops, P79
   Drakopoulos P, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES WORKSHOPS (VRW 2020), P643, DOI [10.1109/VRW50115.2020.0-103, 10.1109/VRW50115.2020.00172]
   Duchowski AT, 2018, COMPUT GRAPH-UK, V73, P59, DOI 10.1016/j.cag.2018.04.002
   Feit AM, 2017, PROCEEDINGS OF THE 2017 ACM SIGCHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'17), P1118, DOI 10.1145/3025453.3025599
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Fitzgibbon A, 1999, IEEE T PATTERN ANAL, V21, P476, DOI 10.1109/34.765658
   Fuhl W, 2016, 2016 ACM SYMPOSIUM ON EYE TRACKING RESEARCH & APPLICATIONS (ETRA 2016), P123, DOI 10.1145/2857491.2857505
   Fuhl W, 2015, LECT NOTES COMPUT SC, V9256, P39, DOI 10.1007/978-3-319-23192-1_4
   Gao WS, 2010, INT CONF COMP SCI, P67, DOI 10.1109/ICCSIT.2010.5563693
   Greenwald SW, 2016, 22ND ACM CONFERENCE ON VIRTUAL REALITY SOFTWARE AND TECHNOLOGY (VRST 2016), P19
   Hakoda H, 2017, UIST'17 ADJUNCT: ADJUNCT PUBLICATION OF THE 30TH ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P15, DOI 10.1145/3131785.3131809
   Hornof AJ, 2002, BEHAV RES METH INS C, V34, P592, DOI 10.3758/BF03195487
   Huang MX, 2017, PROCEEDINGS OF THE 2017 ACM SIGCHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'17), P2546, DOI 10.1145/3025453.3025794
   ILLINGWORTH J, 1988, COMPUT VISION GRAPH, V44, P87, DOI 10.1016/S0734-189X(88)80033-1
   Jakob R., 1998, Readings in Intelligent User Interfaces, P65
   Javadi Amir-Homayoun, 2015, Front Neuroeng, V8, P4, DOI 10.3389/fneng.2015.00004
   John B, 2019, ETRA 2019: 2019 ACM SYMPOSIUM ON EYE TRACKING RESEARCH & APPLICATIONS, DOI 10.1145/3314111.3319816
   Kar A, 2017, IEEE ACCESS, V5, P16495, DOI 10.1109/ACCESS.2017.2735633
   Kasprowski P, 2014, ADV INTELL SYST, V284, P225, DOI 10.1007/978-3-319-06596_21
   Kassner M, 2014, PROCEEDINGS OF THE 2014 ACM INTERNATIONAL JOINT CONFERENCE ON PERVASIVE AND UBIQUITOUS COMPUTING (UBICOMP'14 ADJUNCT), P1151, DOI 10.1145/2638728.2641695
   Katrychuk D, 2019, ETRA 2019: 2019 ACM SYMPOSIUM ON EYE TRACKING RESEARCH & APPLICATIONS, DOI 10.1145/3314111.3319821
   Kenneth HolmqvistMarcus Nystrom., 2011, Eye Tracking: A Comprehensive Guide to Methods and Measures
   Koulieris GA, 2019, COMPUT GRAPH FORUM, V38, P493, DOI 10.1111/cgf.13654
   Koulieris GA, 2016, P IEEE VIRT REAL ANN, P113, DOI 10.1109/VR.2016.7504694
   Krafka K, 2016, PROC CVPR IEEE, P2176, DOI 10.1109/CVPR.2016.239
   Pelisson Denis, 2009, EYE HEAD COORDINATIO, P1545, DOI [10.1007/978-3-540-29678-2_3257, DOI 10.1007/978-3-540-29678-2_3257]
   Santini T, 2018, COMPUT VIS IMAGE UND, V170, P40, DOI 10.1016/j.cviu.2018.02.002
   Sidorakis N, 2015, 2015 IEEE 1ST WORKSHOP ON EVERYDAY VIRTUAL REALITY (WEVR), P15, DOI 10.1109/WEVR.2015.7151689
   Sipatchin A, 2020, INVEST OPHTH VIS SCI, V61
   Sipatchin Alexandra, 2020, EYE TRACKING LOW VIS
   STAMPE DM, 1993, BEHAV RES METH INSTR, V25, P137, DOI 10.3758/BF03204486
   Swirski L., 2012, P S EYE TRACK RES AP, P173
   TSUJI S, 1978, IEEE T COMPUT, V27, P777, DOI 10.1109/TC.1978.1675191
   Wood E., 2014, P S EYE TRACK RES AP, P207, DOI DOI 10.1145/2578153.2578185
   Zhu ZW, 2006, INT C PATT RECOG, P1132
   Zhu ZW, 2004, MACH VISION APPL, V15, P139, DOI 10.1007/s00138-004-0139-4
   Zuiderveld K., 1994, Graphics Gems, P474, DOI 10.1016/B978-0-12-336156-1.50061-6
NR 42
TC 10
Z9 11
U1 0
U2 15
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD JUL
PY 2021
VL 18
IS 3
AR 11
DI 10.1145/3456875
PG 20
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA UD5LZ
UT WOS:000687249300002
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Breeden, K
   Hanrahan, P
AF Breeden, Katherine
   Hanrahan, Pat
TI Gaze Data for the Analysis of Attention in Feature Films
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Eye tracking; gaze behavior; gaze direction; psychophysics; film studies
ID WATCHING MOVIES; EYE; PERCEPTION; FACES; SIZE
AB Film directors are masters at controlling what we look at when we watch a film. However, there have been few quantitative studies of how gaze responds to cinematographic conventions thought to influence attention. We have collected and are releasing a dataset designed to help investigate eye movements in response to higher level features such as faces, dialogue, camera movements, image composition, and edits. The dataset, which will be released to the community, includes gaze information for 21 viewers watching 15 clips from live action 2D films, which have been hand annotated for high level features. This work has implications for the media studies, display technology, immersive reality, and human cognition.
C1 [Breeden, Katherine; Hanrahan, Pat] Stanford Univ, 353 Serra Mall, Stanford, CA 94305 USA.
C3 Stanford University
RP Breeden, K (corresponding author), Stanford Univ, 353 Serra Mall, Stanford, CA 94305 USA.
EM kbreeden@cs.stanford.edu; hanrahan@cs.stanford.edu
FU Stanford's Google Graduate Fellowship Fund; NSF [CCF-1111943]
FX This work was supported by Stanford's Google Graduate Fellowship Fund
   and NSF Grant No. CCF-1111943.
CR Alton John., 2013, PAINTING WITH LIGHT
   [Anonymous], 2009, P IEEE INT C COMP VI
   [Anonymous], COMPUT VIS PATT RECO, DOI DOI 10.1109/CVPR.2009.5206557
   [Anonymous], 2010, THESIS
   Bailey R, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1559755.1559757
   Block BruceA., 2001, VISUAL STORY SEEING
   Bordwell David., 2012, FILM ART INTRO, V10th
   Buchan JN, 2007, SOC NEUROSCI-UK, V2, P1, DOI 10.1080/17470910601043644
   Cole F., 2006, EUROGRAPHICS S RENDE, P377
   Crouzet SM, 2010, J VISION, V10, DOI 10.1167/10.4.16
   Cutting JE, 2016, COGN RES, V1, DOI 10.1186/s41235-016-0029-0
   Cutting JE, 2016, ATTEN PERCEPT PSYCHO, V78, P891, DOI 10.3758/s13414-015-1003-5
   Cutting JE, 2011, I-PERCEPTION, V2, P569, DOI 10.1068/i0441aap
   Dorr M, 2010, J VISION, V10, DOI 10.1167/10.10.28
   Doyle D, 2001, EXP BRAIN RES, V139, P333, DOI 10.1007/s002210100742
   Goldstein RB, 2007, COMPUT BIOL MED, V37, P957, DOI 10.1016/j.compbiomed.2006.08.018
   Guenter B, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366183
   Haxby JV, 2000, TRENDS COGN SCI, V4, P223, DOI 10.1016/S1364-6613(00)01482-0
   JAHNKE MT, 1995, EUR J NEUROL, V2, P275, DOI 10.1111/j.1468-1331.1995.tb00130.x
   Le Meur O, 2013, BEHAV RES METHODS, V45, P251, DOI 10.3758/s13428-012-0226-9
   Levy DL, 2010, CURR TOP BEHAV NEURO, V4, P311, DOI 10.1007/7854_2010_60
   Li J, 2010, INT J COMPUT VISION, V90, P150, DOI 10.1007/s11263-010-0354-6
   Mascelli J., 1998, 5 CS CINEMATOGRAPHY
   McNamara A, 2009, ACM T APPL PERCEPT, V6, DOI 10.1145/1577755.1577760
   McPartland JC, 2011, J AUTISM DEV DISORD, V41, P148, DOI 10.1007/s10803-010-1033-8
   Mital PK, 2011, COGN COMPUT, V3, P5, DOI 10.1007/s12559-010-9074-z
   Pambakian ALM, 2000, J NEUROL NEUROSUR PS, V69, P751, DOI 10.1136/jnnp.69.6.751
   Patney A, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980246
   Peters R., 2007, Advances in Neural Information Processing Systems, P1145
   Rahman A, 2014, J EYE MOVEMENT RES, V7
   Ross NM, 2013, J VISION, V13, DOI 10.1167/13.4.1
   SASLOW MG, 1967, J OPT SOC AM, V57, P1024, DOI 10.1364/JOSA.57.001024
   Serrano A, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073668
   Sitzmann Vincent, 2016, CORRABS161204335
   Smith TJ, 2013, J VISION, V13, DOI 10.1167/13.8.16
   Stuyven E, 2000, ACTA PSYCHOL, V104, P69, DOI 10.1016/S0001-6918(99)00054-2
   Taya S, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0039060
   Troscianko T, 2012, I-PERCEPTION, V3, P414, DOI 10.1068/i0475aap
   Van Gompel Roger P. G., 2007, P1, DOI 10.1016/B978-008044980-7/50003-3
   Yarbus A.L., 1967, EYE MOVEMENTS VISION, DOI [10.1007/978-1-4899-5379-7, DOI 10.1007/978-1-4899-5379-7]
NR 40
TC 14
Z9 15
U1 0
U2 10
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD SEP
PY 2017
VL 14
IS 4
SI SI
AR 23
DI 10.1145/3127588
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA FM9EI
UT WOS:000415407300002
DA 2024-07-18
ER

PT J
AU Perrotin, O
   D'Alessandro, C
AF Perrotin, Olivier
   D'Alessandro, Christophe
TI Seeing, Listening, Drawing: Interferences between Sensorimotor
   Modalities in the Use of a Tablet Musical Interface
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Sensorimotor modalities; target acquisition; graphic tablet
ID PERCEPTION; REPRESENTATION; MOVEMENTS; FEEDBACK; SOUND
AB Audio, visual, and proprioceptive actions are involved when manipulating a graphic tablet musical interface. Previous works suggested a possible dominance of the visual over the auditory modality in this situation. The main goal of the present study is to examine the interferences between these modalities in visual, audio, and audio-visual target acquisition tasks. Experiments are based on a movement replication paradigm, where a subject controls a cursor on a screen or the pitch of a synthesized sound by changing the stylus position on a covered graphic tablet. The experiments consisted of the following tasks: (1) a target acquisition task that was aimed at a visual target (reaching a cue with the cursor displayed on a screen), an audio target (reaching a reference note by changing the pitch of the sound played in headsets), or an audio-visual target, and (2) the replication of the target acquisition movement in the opposite direction. In the return phase, visual and audio feedback were suppressed. Different gain factors perturbed the relationships among the stylus movements, visual cursor movements, and audio pitch movements. The deviations between acquisition and return movements were analyzed. The results showed that hand amplitudes varied in accordance with visual, audio, and audio-visual perturbed gains, showing a larger effect for the visual modality. This indicates that visual, audio, and audio-visual actions interfered with the motor modality and confirms the spatial representation of pitch reported in previous studies. In the audio-visual situation, vision dominated over audition, as the latter had no significant influence on motor movement. Consequently, visual feedback is helpful for musical targeting of pitch on a graphic tablet, at least during the learning phase of the instrument. This result is linked to the underlying spatial organization of pitch perception. Finally, this work brings a complementary approach to previous studies showing that audition may dominate over vision for other aspects of musical sound (e. g., timing, rhythm, and timbre).
C1 [Perrotin, Olivier; D'Alessandro, Christophe] Univ Paris Saclay, LIMSI, CNRS, Paris, France.
   [Perrotin, Olivier; D'Alessandro, Christophe] CNRS, LIMSI, Batiment 508,Rue John Neumann, F-91405 Orsay, France.
C3 Universite Paris Saclay; Centre National de la Recherche Scientifique
   (CNRS); Universite Paris Cite; Universite Paris Saclay; Centre National
   de la Recherche Scientifique (CNRS)
RP Perrotin, O (corresponding author), Univ Paris Saclay, LIMSI, CNRS, Paris, France.; Perrotin, O (corresponding author), CNRS, LIMSI, Batiment 508,Rue John Neumann, F-91405 Orsay, France.
EM olivier.perrotin@limsi.fr; cda@limsi.fr
RI d'Alessandro, Christophe/AAM-4566-2021
OI d'Alessandro, Christophe/0000-0002-2629-8752; Perrotin,
   Olivier/0000-0002-9909-6078
FU Agence Nationale de la Recherche [ANR-13-CORD-0011]; Agence Nationale de
   la Recherche (ANR) [ANR-13-CORD-0011] Funding Source: Agence Nationale
   de la Recherche (ANR)
FX This work was supported by the Agence Nationale de la Recherche, under
   grant ANR-13-CORD-0011.
CR AKAMATSU M, 1995, ERGONOMICS, V38, P816, DOI 10.1080/00140139508925152
   Andersen TH, 2010, ACM T APPL PERCEPT, V7, DOI 10.1145/1773965.1773968
   [Anonymous], 1988, Proceedings of the SIGCHI conference on Human factors in computing systems-CHI, DOI DOI 10.1145/57167.57203
   Boyer Eric O., 2013, INT S COMP MUS MULT, P218
   Caramiaux B, 2010, LECT NOTES ARTIF INT, V5934, P158, DOI 10.1007/978-3-642-12553-9_14
   Charoenchaimonkon Eakachai, 2010, Proceedings of the 2010 International Conference on User Science and Engineering (i-USEr 2010), P238, DOI 10.1109/IUSER.2010.5716759
   Cockburn A, 2005, ERGONOMICS, V48, P1129, DOI 10.1080/00140130500197260
   Crawley M.J., 2013, R BOOK 2 EDITION, VSecond
   d'Alessandro C, 2014, J ACOUST SOC AM, V135, P3601, DOI 10.1121/1.4875718
   d'Alessandro C, 2011, J ACOUST SOC AM, V129, P1594, DOI 10.1121/1.3531802
   D'Alessandro N., 2006, Proc. International Conference on New Interfaces for Musical Expression, P266
   dAlessandro Nicolas, 2009, HANDSKETCH BI MANUAL
   Danna J, 2015, HUM MOVEMENT SCI, V43, P216, DOI 10.1016/j.humov.2014.12.002
   Feugere Lionel, 2013, THESIS
   Ghez C, 2000, P INT C AUD DISPL IC
   GREENWALD AG, 1970, PSYCHOL REV, V77, P73, DOI 10.1037/h0028689
   Hommel B, 2001, BEHAV BRAIN SCI, V24, P849, DOI 10.1017/S0140525X01000103
   Kadosh RC, 2008, CORTEX, V44, P470, DOI 10.1016/j.cortex.2007.08.002
   Kessous Loic, 2004, P SOUND MUS COMP IRC
   Ladwig S, 2013, EXP BRAIN RES, V231, P457, DOI 10.1007/s00221-013-3710-2
   Ladwig S, 2012, Z PSYCHOL, V220, P10, DOI 10.1027/2151-2604/a000085
   LUSCHEI E, 1967, EXP NEUROL, V18, P429, DOI 10.1016/0014-4886(67)90060-X
   Müsseler J, 2009, CONSCIOUS COGN, V18, P359, DOI 10.1016/j.concog.2009.02.004
   Ortega L, 2014, ATTEN PERCEPT PSYCHO, V76, P1485, DOI 10.3758/s13414-014-0663-x
   Perrotin O, 2016, ACM T COMPUT-HUM INT, V23, DOI 10.1145/2897513
   Repp BH, 2004, PSYCHOL RES-PSYCH FO, V68, P252, DOI 10.1007/s00426-003-0143-8
   Rieger M, 2005, EXP BRAIN RES, V163, P487, DOI 10.1007/s00221-004-2203-8
   Rodger MWM, 2011, EXP BRAIN RES, V214, P393, DOI 10.1007/s00221-011-2837-2
   Rusconi E, 2006, COGNITION, V99, P113, DOI 10.1016/j.cognition.2005.01.004
   Sun Minghui, 2010, INF PROCESS SOC JPN, V51, P2375
   Sutter C, 2011, BEHAV INFORM TECHNOL, V30, P415, DOI 10.1080/01449291003660349
   Sutter C, 2013, NEW IDEAS PSYCHOL, V31, P247, DOI 10.1016/j.newideapsych.2012.12.001
   TEGHTSOONIAN R, 1978, PERCEPT PSYCHOPHYS, V24, P305, DOI 10.3758/BF03204247
   Thoret E, 2014, LECT NOTES COMPUT SC, V8905, P234, DOI 10.1007/978-3-319-12976-1_15
   Thoret E, 2014, J EXP PSYCHOL HUMAN, V40, P983, DOI 10.1037/a0035441
   Thoret Etienne, 2014, INT MULT RES FOR IMR
   Varlet M, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0044082
   Wang L, 2012, FRONT PSYCHOL, V3, DOI 10.3389/fpsyg.2012.00289
   Wendker N, 2014, FRONT PSYCHOL, V5, DOI 10.3389/fpsyg.2014.00225
   WRIGHT CE, 1990, ATTENTION PERFORM, P294
   Zbyszynski M., 2007, NIME'07 Proceedings of the 2007 international conference on New interfaces for musical expression, P100
NR 41
TC 2
Z9 4
U1 0
U2 12
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD FEB
PY 2017
VL 14
IS 2
AR 10
DI 10.1145/2990501
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA EM8VC
UT WOS:000395588200003
OA Green Submitted, Bronze
DA 2024-07-18
ER

PT J
AU Bernhard, M
   Stavrakis, E
   Hecher, M
   Wimmer, M
AF Bernhard, Matthias
   Stavrakis, Efstathios
   Hecher, Michael
   Wimmer, Michael
TI Gaze-to-Object Mapping during Visual Search in 3D Virtual Environments
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Algorithms; Experimentation; Human Factors; Gaze-to-object mapping; gaze
   analysis; eye-tracking; gaze processing; visual attention; object-based
   attention; video games; virtual environments
ID ATTENTION
AB Stimuli obtained from highly dynamic 3D virtual environments and synchronous eye-tracking data are commonly used by algorithms that strive to correlate gaze to scene objects, a process referred to as gaze-to-object mapping (GTOM). We propose to address this problem with a probabilistic approach using Bayesian inference. The desired result of the inference is a predicted probability density function (PDF) specifying for each object in the scene a probability to be attended by the user. To evaluate the quality of a predicted attention PDF, we present a methodology to assess the information value (i.e., likelihood) in the predictions of different approaches that can be used to infer object attention. To this end, we propose an experiment based on a visual search task, which allows us to determine the object of attention at a certain point in time under controlled conditions. We perform this experiment with a wide range of static and dynamic visual scenes to obtain a ground-truth evaluation dataset, allowing us to assess GTOM techniques in a set of 30 particularly challenging cases.
C1 [Bernhard, Matthias; Hecher, Michael; Wimmer, Michael] Vienna Univ Technol, Vienna, Austria.
   [Stavrakis, Efstathios] Cyprus Inst, Nicosia, Cyprus.
C3 Technische Universitat Wien
RP Bernhard, M (corresponding author), Vienna Univ Technol, Vienna, Austria.
EM matthias.bernhard@cg.tuwien.ac.at
RI Stavrakis, Efstathios/I-8232-2014
OI Stavrakis, Efstathios/0000-0002-9213-7690
FU Austrian Science Fund (FWF) [P23700-N23]; Cypriot Research Promotion
   Foundation (RPF) [PROION/0311/37]; Vienna Science and Technology Fund
   (WWTF) [VRG11-010]; EC Marie Curie Career Integration Grant
   [PCIG13-GA-2013-618680]; Austrian Science Fund (FWF) [P23700] Funding
   Source: Austrian Science Fund (FWF)
FX This work has been mainly supported by the Austrian Science Fund (FWF)
   contract no. P23700-N23. Efstathios Stavrakis was supported by the
   Cypriot Research Promotion Foundation (RPF) under contract no.
   PROION/0311/37. Matthias Bernhard has been partially also supported by
   the Vienna Science and Technology Fund (WWTF) through project VRG11-010,
   and by the EC Marie Curie Career Integration Grant through project
   PCIG13-GA-2013-618680.
CR [Anonymous], 2007, Eye tracking methodology: Theory and practice, DOI DOI 10.1007/978-3-319-57883-5
   Bernhard M, 2010, ACM T APPL PERCEPT, V8, DOI 10.1145/1857893.1857897
   Chen Z, 2012, ATTEN PERCEPT PSYCHO, V74, P784, DOI 10.3758/s13414-012-0322-z
   DUNCAN J, 1984, J EXP PSYCHOL GEN, V113, P501, DOI 10.1037/0096-3445.113.4.501
   HENDERSON JM, 1993, CAN J EXP PSYCHOL, V47, P79, DOI 10.1037/h0078776
   Hollingworth A, 2012, J EXP PSYCHOL HUMAN, V38, P1596, DOI 10.1037/a0030237
   Huang LQ, 2010, J EXP PSYCHOL GEN, V139, P162, DOI 10.1037/a0018034
   Kravitz DJ, 2011, ATTEN PERCEPT PSYCHO, V73, P2434, DOI 10.3758/s13414-011-0201-z
   Mantiuk R, 2013, COMPUT GRAPH FORUM, V32, P163, DOI 10.1111/cgf.12036
   Papenmeier F, 2010, BEHAV RES METHODS, V42, P179, DOI 10.3758/BRM.42.1.179
   Peters RJ, 2008, ACM T APPL PERCEPT, V5, DOI 10.1145/1279920.1279923
   Pfeiffer T., 2012, P S EYE TRACKING RES, P29, DOI [10.1145/2168556.2168560, DOI 10.1145/2168556.2168560]
   Salvucci DD, 2000, 2000 S EYE TRACKING, P71, DOI [10.1145/355017.355028, DOI 10.1145/355017.355028]
   Spakov O., 2011, P 1 C NOV GAZ CONTR
   Starker I., 1990, SIGCHI Bulletin, P3
   Stellmach Sophie., 2010, P INT C ADV VISUAL I, P345, DOI DOI 10.1145/1842993.1843058
   Sundstedt V, 2008, APGV 2008: PROCEEDINGS OF THE SYMPOSIUM ON APPLIED PERCEPTION IN GRAPHICS AND VISUALIZATION, P43
   Sundstedt Veronica., 2013, GAME ANAL, P543
   Tole J. R., 1981, FIGITAL FILTERS SACC, V185-199.
   Tomasi C, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P839, DOI 10.1109/ICCV.1998.710815
   TREISMAN AM, 1980, COGNITIVE PSYCHOL, V12, P97, DOI 10.1016/0010-0285(80)90005-5
   WEGHORST H, 1984, ACM T GRAPHIC, V3, P52, DOI 10.1145/357332.357335
   Wolfe J.M., 2000, SEEING, P335, DOI DOI 10.1016/B978-012443760-9/50010-6
   Xu SH, 2008, RECSYS'08: PROCEEDINGS OF THE 2008 ACM CONFERENCE ON RECOMMENDER SYSTEMS, P83
   Yanulevskaya V, 2013, J VISION, V13, DOI 10.1167/13.13.27
   Zhang XY, 2008, CHI 2008: 26TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS VOLS 1 AND 2, CONFERENCE PROCEEDINGS, P525
NR 26
TC 10
Z9 11
U1 0
U2 6
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD JUN
PY 2014
VL 11
IS 3
SI SI
AR 14
DI 10.1145/2644812
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA AU0KM
UT WOS:000345311700005
DA 2024-07-18
ER

PT J
AU Chen, FM
   Xu, Y
   Zhang, D
AF Chen, Fangmei
   Xu, Yong
   Zhang, David
TI A New Hypothesis on Facial Beauty Perception
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Experimentation; Human Factors; Convex hull; face beautification; face
   perception; hypothesis; machine learning; regression
ID ATTRACTIVE FACES; SYMMETRY; PREDICTION; AESTHETICS; ASYMMETRY; MODELS;
   SHAPE
AB In this article, a new hypothesis on facial beauty perception is proposed: the weighted average of two facial geometric features is more attractive than the inferior one between them. Extensive evidences support the new hypothesis. We collected 390 well-known beautiful face images (e.g., Miss Universe, movie stars, and super models) as well as 409 common face images from multiple sources. Dozens of volunteers rated the face images according to their attractiveness. Statistical regression models are trained on this database. Under the empirical risk principle, the hypothesis is tested on 318,801 pairs of images and receives consistently supportive results. A corollary of the hypothesis is attractive facial geometric features construct a convex set. This corollary derives a convex hull based face beautification method, which guarantees attractiveness and minimizes the before-after difference. Experimental results show its superiority to state-of-the-art geometric based face beautification methods. Moreover, the mainstream hypotheses on facial beauty perception (e.g., the averageness, symmetry, and golden ratio hypotheses) are proved to be compatible with the proposed hypothesis.
C1 [Chen, Fangmei] Tsinghua Univ, Grad Sch Shenzhen, Dept Elect Engn, Shenzhen 518055, Peoples R China.
   [Xu, Yong] Harbin Inst Technol, Biocomp Res Ctr, Shenzhen Grad Sch, Shenzhen 518055, Peoples R China.
   [Zhang, David] Hong Kong Polytech Univ, Biometr Res Ctr, Kowloon, Hong Kong, Peoples R China.
   [Zhang, David] Hong Kong Polytech Univ, Dept Comp, Kowloon, Hong Kong, Peoples R China.
C3 Tsinghua Shenzhen International Graduate School; Tsinghua University;
   Harbin Institute of Technology; Hong Kong Polytechnic University; Hong
   Kong Polytechnic University
RP Chen, FM (corresponding author), Tsinghua Univ, Grad Sch Shenzhen, Dept Elect Engn, Room F306C,Tsinghua Campus,Univ Town, Shenzhen 518055, Peoples R China.
EM fmchen08@gmail.com; yongxu@ymail.com; csdzhang@comp.polyu.edu.hk
RI Zhang, Hao/HHM-1940-2022; Zhang, David D/O-9396-2016
OI Zhang, David D/0000-0002-5027-5286
FU GRF fund from the HKSAR Government; central fund from the Hong Kong
   Polytechnic University; NSFC Oversea fund, China [61020106004]
FX The work is partially supported by the GRF fund from the HKSAR
   Government, the central fund from the Hong Kong Polytechnic University,
   and the NSFC Oversea fund (61020106004), China.
CR ALLEY TR, 1991, PSYCHOL SCI, V2, P123, DOI 10.1111/j.1467-9280.1991.tb00113.x
   [Anonymous], 1998, Statistical shape analysis: Wiley series in probability and statistics". In
   [Anonymous], 2004, CONVEX OPTIMIZATION
   [Anonymous], 2008, EXPLAINING PSYCHOL S
   Atiyeh BS, 2008, AESTHET PLAST SURG, V32, P209, DOI 10.1007/s00266-007-9074-x
   Chen FM, 2010, LECT NOTES COMPUT SC, V6165, P21, DOI 10.1007/978-3-642-13923-9_3
   COOTES TF, 1995, COMPUT VIS IMAGE UND, V61, P38, DOI 10.1006/cviu.1995.1004
   CUNNINGHAM MR, 1995, J PERS SOC PSYCHOL, V68, P261, DOI 10.1037/0022-3514.68.2.261
   Eisenthal Y, 2006, NEURAL COMPUT, V18, P119, DOI 10.1162/089976606774841602
   Fan JT, 2012, PATTERN RECOGN, V45, P2326, DOI 10.1016/j.patcog.2011.11.024
   Grant M., 2012, CVX: Matlab software for disciplined convex programming
   Grant MC, 2008, LECT NOTES CONTR INF, V371, P95, DOI 10.1007/978-1-84800-155-8_7
   Gray D, 2010, LECT NOTES COMPUT SC, V6316, P434, DOI 10.1007/978-3-642-15567-3_32
   Holland E, 2008, AESTHET PLAST SURG, V32, P200, DOI 10.1007/s00266-007-9080-z
   Jefferson Yosh, 2004, Int J Orthod Milwaukee, V15, P9
   Kagian A, 2008, VISION RES, V48, P235, DOI 10.1016/j.visres.2007.11.007
   Kowner R, 1996, J EXP PSYCHOL HUMAN, V22, P662, DOI 10.1037/0096-1523.22.3.662
   LANGLOIS JH, 1990, PSYCHOL SCI, V1, P115, DOI 10.1111/j.1467-9280.1990.tb00079.x
   Langlois JH, 2000, PSYCHOL BULL, V126, P390, DOI 10.1037/0033-2909.126.3.390
   Leyvand T, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360637
   Little AC, 2006, PERS INDIV DIFFER, V41, P1107, DOI 10.1016/j.paid.2006.04.015
   Little AC, 2011, PHILOS T R SOC B, V366, P1638, DOI 10.1098/rstb.2010.0404
   Marquardt Stephen R, 2002, J Clin Orthod, V36, P339
   Melacci S, 2010, PATTERN ANAL APPL, V13, P289, DOI 10.1007/s10044-009-0155-0
   Messer K., 1999, 2 INT C AUD VID BAS, V964, P965
   Mu YD, 2013, NEUROCOMPUTING, V99, P59, DOI 10.1016/j.neucom.2012.06.020
   Penton-Voak I., 2011, Journal of Evolutionary Psychology, V9, P173, DOI DOI 10.1556/JEP.9.2011.2.5
   Penton-Voak IS, 2001, P ROY SOC B-BIOL SCI, V268, P1617, DOI 10.1098/rspb.2001.1703
   Peron Ana Paula Lazzari Marques, 2012, Dental Press J. Orthod., V17, P124
   PERRETT DI, 1994, NATURE, V368, P239, DOI 10.1038/368239a0
   Perrett DI, 1999, EVOL HUM BEHAV, V20, P295, DOI 10.1016/S1090-5138(99)00014-8
   Phillips PJ, 2000, IEEE T PATTERN ANAL, V22, P1090, DOI 10.1109/34.879790
   Phillips PJ, 1998, IMAGE VISION COMPUT, V16, P295, DOI 10.1016/S0262-8856(97)00070-X
   Rhodes G, 2006, ANNU REV PSYCHOL, V57, P199, DOI 10.1146/annurev.psych.57.102904.190208
   Schaefer S, 2006, ACM T GRAPHIC, V25, P533, DOI 10.1145/1141911.1141920
   Schmid K, 2008, PATTERN RECOGN, V41, P2710, DOI 10.1016/j.patcog.2007.11.022
   Sutic Davor, 2010, 2010 33rd International Convention on Information and Communication Technology, Electronics and Microelectronics (MIPRO), P1339
   SWADDLE JP, 1995, P ROY SOC B-BIOL SCI, V261, P111, DOI 10.1098/rspb.1995.0124
   THOMPSON B, 1995, EDUC PSYCHOL MEAS, V55, P525, DOI 10.1177/0013164495055004001
   Vapnik V., 1999, NATURE STAT LEARNING
   Whitehill J., 2008, P 8 IEEE INT C AUTOM, P1
   Zhang D, 2011, PATTERN RECOGN, V44, P940, DOI 10.1016/j.patcog.2010.10.013
NR 42
TC 5
Z9 6
U1 0
U2 38
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD MAY
PY 2014
VL 11
IS 2
AR 8
DI 10.1145/2622655
PG 20
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA AO3OJ
UT WOS:000341241100004
DA 2024-07-18
ER

PT J
AU Gaffary, Y
   Eyharabide, V
   Martin, JC
   Ammi, M
AF Gaffary, Yoren
   Eyharabide, Victoria
   Martin, Jean-Claude
   Ammi, Mehdi
TI Clustering Approach to Characterize Haptic Expressions of Emotions
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Measurement; Experimentation; Emotion; haptic
ID RECOGNITION; DESIGN
AB Several studies have investigated the relevance of haptics to physically convey various types of emotion. However, they use basic analysis approaches to identify the relevant features for an effective communication of emotion. This article presents an advanced analysis approach, based on the clustering technique, that enables the extraction of the general features of affective haptic expressions as well as the identification of specific features in order to discriminate between close emotions that are difficult to differentiate. This approach was tested in the context of affective communication through a virtual handshake. It uses a haptic device, which enables the expression of 3D movements. The results of this research were compared to those of the standard Analysis of Variance method in order to highlight the advantages and limitations of each approach.
C1 [Gaffary, Yoren; Eyharabide, Victoria; Martin, Jean-Claude; Ammi, Mehdi] Univ Paris Sud, Paris, France.
C3 Universite Paris Cite; Universite Paris Saclay
RP Gaffary, Y (corresponding author), LIMSI CNRSB, P 133, F-91403 Orsay, France.
EM yoren.gaffary@limsi.fr
RI Eyharabide, Victoria/JCE-5713-2023; Eyharabide, Victoria/ABF-3562-2020
OI Eyharabide, Victoria/0000-0002-3775-1495
CR Ahn S. J., 2009, ANN M NCA 95 ANN CON
   [Anonymous], 2005, MORGAN KAUFMANN SERI
   Bailenson JN, 2007, HUM-COMPUT INTERACT, V22, P325
   Basori Ahmad Hoirul, 2008, International Journal of Virtual Reality, V7, P27
   Bhatti MW, 2004, 2004 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS, VOL 2, PROCEEDINGS, P181
   Bonnet D., 2011, 2011 IEEE International Workshop on Haptic Audio Visual Environments and Games (HAVE 2011), P81, DOI 10.1109/HAVE.2011.6088396
   Casale S, 2010, EUR SIGNAL PR CONF, P1174
   Castellano G., 2008, THESIS U GENOA ITALY
   Chang J, 2010, LECT NOTES COMPUT SC, V6191, P385, DOI 10.1007/978-3-642-14064-8_56
   Coulson M, 2004, J NONVERBAL BEHAV, V28, P117, DOI 10.1023/B:JONB.0000023655.25550.be
   Courgeon M., 2008, NOUS, P12
   Courgeon M., 2009, INT WORKSH AFF AW VI, P1
   Dael N, 2012, EMOTION, V12, P1085, DOI 10.1037/a0025737
   Dellaert F, 1996, ICSLP 96 - FOURTH INTERNATIONAL CONFERENCE ON SPOKEN LANGUAGE PROCESSING, PROCEEDINGS, VOLS 1-4, P1970, DOI 10.1109/ICSLP.1996.608022
   DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x
   Eisenstein J, 2001, PROCEEDINGS OF 2001 INTERNATIONAL SYMPOSIUM ON INTELLIGENT MULTIMEDIA, VIDEO AND SPEECH PROCESSING, P259, DOI 10.1109/ISIMP.2001.925383
   EKMAN P, 1992, PSYCHOL REV, V99, P550, DOI 10.1037/0033-295X.99.3.550
   Ekman P., 2003, UNMASKING FACE GUIDE
   Golan O, 2006, J AUTISM DEV DISORD, V36, P169, DOI 10.1007/s10803-005-0057-y
   Hartmann B, 2006, LECT NOTES ARTIF INT, V3881, P188
   Hertenstein MJ, 2006, EMOTION, V6, P528, DOI 10.1037/1528-3542.6.3.528
   KLEMA VC, 1980, IEEE T AUTOMAT CONTR, V25, P164, DOI 10.1109/TAC.1980.1102314
   Knight H, 2009, 2009 IEEE-RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, P3715, DOI 10.1109/IROS.2009.5354169
   Lemmens P, 2009, WORLD HAPTICS 2009: THIRD JOINT EUROHAPTICS CONFERENCE AND SYMPOSIUM ON HAPTIC INTERFACES FOR VIRTUAL ENVIRONMENT AND TELEOPERATOR SYSTEMS, PROCEEDINGS, P7, DOI 10.1109/WHC.2009.4810832
   Lu Yijuan, 2007, P 15 ACM INT C MULT, P301
   MEHRABIAN A, 1967, J CONSULT PSYCHOL, V31, P248, DOI 10.1037/h0024648
   Mower E, 2011, INT CONF ACOUST SPEE, P2372
   Nicholson J, 2000, NEURAL COMPUT APPL, V9, P290, DOI 10.1007/s005210070006
   Nwe TL, 2003, SPEECH COMMUN, V41, P603, DOI 10.1016/S0167-6393(03)00099-2
   Olausson HW, 2008, NEUROSCI LETT, V436, P128, DOI 10.1016/j.neulet.2008.03.015
   Parkinson B., 2004, EMOTION SOCIAL RELAT
   Picard R., 1997, USER MODEL USER-ADAP, V12, P85
   Polzehl T, 2009, INTERSPEECH 2009: 10TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2009, VOLS 1-5, P340
   RUSSELL JA, 1977, J RES PERS, V11, P273, DOI 10.1016/0092-6566(77)90037-X
   Scherer K.R., 2000, INTRO SOCIAL PSYCHOL, V3, P151
   Scherer KR, 2005, SOC SCI INFORM, V44, P695, DOI 10.1177/0539018405058216
   Schuller B, 2003, INT CONF ACOUST SPEE, P1
   Smith J, 2007, INT J HUM-COMPUT ST, V65, P376, DOI 10.1016/j.ijhcs.2006.11.006
   Tsetserukou D., 2010, P 1 AUGM HUM INT C A, P47
   Ververidis D, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOL I, PROCEEDINGS, P593
   Vogt T, 2005, 2005 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO (ICME), VOLS 1 AND 2, P474, DOI 10.1109/ICME.2005.1521463
   Wallbott HG, 1998, EUR J SOC PSYCHOL, V28, P879, DOI 10.1002/(SICI)1099-0992(1998110)28:6<879::AID-EJSP901>3.0.CO;2-W
   Yohanan S, 2011, ACMIEEE INT CONF HUM, P473, DOI 10.1145/1957656.1957820
NR 43
TC 4
Z9 4
U1 0
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD OCT
PY 2013
VL 10
IS 4
AR 21
DI 10.1145/2536764.2536768
PG 20
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA 281YK
UT WOS:000329136700004
DA 2024-07-18
ER

PT J
AU Lylykangas, J
   Surakka, V
   Rantala, J
   Raisamo, R
AF Lylykangas, Jani
   Surakka, Veikko
   Rantala, Jussi
   Raisamo, Roope
TI Intuitiveness of Vibrotactile Speed Regulation Cues
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Design; Experimentation; Human Factors; Human-computer interaction;
   heart rate monitor; haptic feedback; iconic information; priming;
   intuitive decision making
ID PERCEPTION
AB Interpretations of vibrotactile stimulations were compared between two participant groups. In both groups, the task was to evaluate specifically designed tactile stimulations presented to the wrist or chest. Ascending, constant, and descending vibration frequency profiles of the stimuli represented information for three different speed regulation instructions: "accelerate your speed," " keep your speed constant," and " decelerate your speed," respectively. The participants were treated differently so that one of the groups was first taught (i.e., primed) the meanings of the stimuli, whereas the other group was not taught (i.e., unprimed). The results showed that the stimuli were evaluated nearly equally in the primed and the unprimed groups. The best performing stimuli communicated the three intended meanings in the rate of 88% to 100% in the primed group and in the unprimed group in the rate of 71% to 83%. Both groups performed equally in evaluating " keep your speed constant" and " decelerate your speed" information. As the unprimed participants performed similarly to the primed participants, the results suggest that vibrotactile stimulation can be intuitively understood. The results suggest further that carefully designed vibrotactile stimulations could be functional in delivering easy-to-understand feedback on how to regulate the speed of movement, such as in physical exercise and rehabilitation applications.
C1 [Lylykangas, Jani; Surakka, Veikko; Rantala, Jussi; Raisamo, Roope] Univ Tampere, TAUCHI, Sch Informat Sci, FI-33014 Tampere, Finland.
C3 Tampere University
RP Lylykangas, J (corresponding author), Univ Tampere, TAUCHI, Sch Informat Sci, Kanslerinrinne 1, FI-33014 Tampere, Finland.
EM jani.lylykangas@uta.fi; veikko.surakka@uta.fi; jussi.rantala@uta.fi;
   roope.raisamo@uta.fi
RI ; Raisamo, Roope/P-8398-2018
OI Surakka, Veikko/0000-0003-3986-0713; Raisamo, Roope/0000-0003-3276-7866;
   Rantala, Jussi/0000-0002-6308-7874
FU Finnish Funding Agency for Technology and Innovation (Tekes) [40159/09];
   Doctoral Programme in User-Centered Information Technology (UCIT)
FX This research was part of the HAPIMM project supported by the Finnish
   Funding Agency for Technology and Innovation (Tekes), decision number
   40159/09 and Doctoral Programme in User-Centered Information Technology
   (UCIT).
CR [Anonymous], 2005, P SIGCHI C HUMAN FAC, DOI DOI 10.1145/1054972.1055101
   Bächlin M, 2009, UBICOMP'09: PROCEEDINGS OF THE 11TH ACM INTERNATIONAL CONFERENCE ON UBIQUITOUS COMPUTING, P215
   Bidargaddi NP, 2008, METHOD INFORM MED, V47, P208, DOI 10.3414/ME9112
   Bradley MM, 2000, PSYCHOPHYSIOLOGY, V37, P204, DOI 10.1017/S0048577200990012
   BRADLEY MM, 1994, J BEHAV THER EXP PSY, V25, P49, DOI 10.1016/0005-7916(94)90063-9
   Brewster S.A., 2004, CHI 04 EXTENDED ABST, V28, P787, DOI [https://doi.org/10.1145/985921.985936, DOI 10.1145/985921.985936]
   Brown LornaM., 2006, CHI '06 extended abstracts on Human factors in computing systems, CHI EA '06, P604, DOI DOI 10.1145/1125451.112557
   Enriquez M, 2008, SYMPOSIUM ON HAPTICS INTERFACES FOR VIRTUAL ENVIRONMENT AND TELEOPERATOR SYSTEMS 2008, PROCEEDINGS, P49
   F J.B., 2006, PROC EUROHAPTICS, P90
   Gay V., 2009, Proc. of the 2nd International Conference on Pervasive Technologies Related to Assistive Environments, P1, DOI [10.1145/1579114.1579135, DOI 10.1145/1579114.1579135]
   Ho C., 2005, P HCI INT 2005 VEGAS, P1
   Kahneman D, 2003, AM PSYCHOL, V58, P697, DOI 10.1037/0003-066X.58.9.697
   Karuei I, 2011, 29TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P3267
   Lamble D, 1999, ACCIDENT ANAL PREV, V31, P617, DOI 10.1016/S0001-4575(99)00018-4
   Lylykangas J., 2009, P 23 BRIT HCI GROUP, P112
   Naumann A, 2007, LECT NOTES ARTIF INT, V4562, P128
   Neisser U., 1976, Cognition and reality
   Pakkanen Toni., 2008, Proceedings of the 10th International Conference on Multimodal Interfaces, ICMI'08, P281
   POST LJ, 1994, EXP BRAIN RES, V100, P107
   Raisamo R, 2009, J AMB INTEL SMART EN, V1, P37, DOI 10.3233/AIS-2009-0005
   RETTIG M, 1991, COMMUN ACM, V34, P19, DOI 10.1145/105783.105788
   ROVERS AF, 2006, P EUROHAPTICS 2006, P447
   Salminen K, 2008, CHI 2008: 26TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS VOLS 1 AND 2, CONFERENCE PROCEEDINGS, P1555
   Schneider W., 2002, E-Prime User's Guide
   Spelmezan D, 2009, CHI2009: PROCEEDINGS OF THE 27TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1-4, P2243
   Su SW, 2010, ANN BIOMED ENG, V38, P758, DOI 10.1007/s10439-009-9849-0
   Tipper SP, 1998, NEUROREPORT, V9, P1741, DOI 10.1097/00001756-199806010-00013
NR 27
TC 7
Z9 7
U1 0
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD OCT
PY 2013
VL 10
IS 4
AR 24
DI 10.1145/2536764.2536771
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA 281YK
UT WOS:000329136700007
DA 2024-07-18
ER

PT J
AU Nunez-Varela, J
   Wyatt, JL
AF Nunez-Varela, Jose
   Wyatt, Jeremy L.
TI Models of Gaze Control for Manipulation Tasks
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Experimentation; Performance; Theory; Decision making; gaze control;
   reinforcement learning
ID EYE; COORDINATION
AB Human studies have shown that gaze shifts are mostly driven by the current task demands. In manipulation tasks, gaze leads action to the next manipulation target. One explanation is that fixations gather information about task relevant properties, where task relevance is signalled by reward. This work presents new computational models of gaze shifting, where the agent imagines ahead in time the informational effects of possible gaze fixations. Building on our previous work, the contributions of this article are: (i) the presentation of two new gaze control models, (ii) comparison of their performance to our previous model, (iii) results showing the fit of all these models to previously published human data, and (iv) integration of a visual search process. The first new model selects the gaze that most reduces positional uncertainty of landmarks (Unc), and the second maximises expected rewards by reducing positional uncertainty (RU). Our previous approach maximises the expected gain in cumulative reward by reducing positional uncertainty (RUG). In experiment ii the models are tested on a simulated humanoid robot performing a manipulation task, and each model's performance is characterised by varying three environmental variables. This experiment provides evidence that the RUG model has the best overall performance. In experiment iii, we compare the hand-eye coordination timings of the models in a robot simulation to those obtained from human data. This provides evidence that only the models that incorporate both uncertainty and reward (RU and RUG) match human data.
C1 [Nunez-Varela, Jose; Wyatt, Jeremy L.] Univ Birmingham, Sch Comp Sci, Birmingham B15 2TT, W Midlands, England.
C3 University of Birmingham
RP Nunez-Varela, J (corresponding author), Univ Birmingham, Sch Comp Sci, Birmingham B15 2TT, W Midlands, England.
EM jose.nunez@uaslp.mx; j.l.wyatt@cs.bham.ac.uk
OI Wyatt, Jeremy/0000-0001-6991-356X; Nunez-Varela,
   Jose/0000-0002-9633-3453
FU CONACYT-Mexico [179604]; UKIERI [SA06-0031]; CogX [FP7-ICT-215181]; GeRT
   [FP7-ICT-248273]; PaCMan [FP7-ICT-600918]
FX We are grateful for the support of CONACYT-Mexico (Reg. 179604), UKIERI
   (SA06-0031), CogX (FP7-ICT-215181), GeRT (FP7-ICT-248273), PaCMan
   (FP7-ICT-600918), and the invaluable help of Dr. B. Ravindran and the
   reviewers.
CR [Anonymous], THESIS BROWN U PROVI
   [Anonymous], P 25 C ART INT
   BAJCSY R, 1988, P IEEE, V76, P996, DOI 10.1109/5.5968
   BALLARD DH, 1992, PHILOS T R SOC B, V337, P331, DOI 10.1098/rstb.1992.0111
   Bradtke S. J., 1995, Advances in Neural Information Processing Systems 7, P393
   Findlay J.M., 2003, ACTIVE VISION
   Johansson R.S., 2009, ENCY NEUROSCIENCE, P593, DOI [10.1016/B978-008045046-9.01920-3, DOI 10.1016/B978-008045046-9.01920-3]
   Johansson RS, 2001, J NEUROSCI, V21, P6917, DOI 10.1523/JNEUROSCI.21-17-06917.2001
   Land MF, 2009, VISUAL NEUROSCI, V26, P51, DOI 10.1017/S0952523808080899
   Levenshtein V. I., 1966, SOV PHYS DOKL, V10, P707
   Metta G., 2008, 8th Workshop on Performance Metrics for Intelligent Systems, Gaithersburg, Maryland, Association for Computing Machinery: Gaithersburg, Maryland, P50, DOI DOI 10.1145/1774674.1774683
   Najemnik J, 2005, NATURE, V434, P387, DOI 10.1038/nature03390
   Navalpakkam V, 2010, P NATL ACAD SCI USA, V107, P5232, DOI 10.1073/pnas.0911972107
   Nunez-Varela Jose, 2012, From Animals to Animats 12. Proceedings of the 12th International Conference on Simulation of Adaptive Behavior, SAB 2012, P44, DOI 10.1007/978-3-642-33093-3_5
   Nunez-Varela J, 2012, IEEE INT CONF ROBOT, P4444, DOI 10.1109/ICRA.2012.6225226
   Pattacini U, 2010, IEEE INT C INT ROBOT, P1668, DOI 10.1109/IROS.2010.5650851
   Puterman ML., 2014, MARKOV DECISION PROC, DOI DOI 10.1002/9780470316887
   Renninger L., 2010, J VIS, V7, P1
   Rothkopf CA, 2007, J VISION, V7, DOI 10.1167/7.14.16
   Schutz A. C., 2010, J VISION, V10, p[551, 551a], DOI [10.1167/10.7.551, DOI 10.1167/10.7.551]
   Shibata T, 2001, ADAPT BEHAV, V9, P189, DOI 10.1177/10597123010093005
   Sprague N, 2007, ACM T APPL PERCEPT, V4, DOI 10.1145/1265957.1265960
   Steinman R.M., 2003, VISUAL NEUROSCI, P1339
   Sullivan B., 2012, J VIS, V12, P1259
   Sutton RS, 1999, ARTIF INTELL, V112, P181, DOI 10.1016/S0004-3702(99)00052-1
   Sutton RS., 2020, REINFORCEMENT LEARNI
   Thrun S., 2008, PROBABILISTIC ROBOTI
   Yarbus A. L., 1967, Eye Movements and Vision
NR 28
TC 7
Z9 7
U1 0
U2 8
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD OCT
PY 2013
VL 10
IS 4
AR 20
DI 10.1145/2536764.2536767
PG 22
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 281YK
UT WOS:000329136700003
DA 2024-07-18
ER

PT J
AU Sugano, Y
   Matsushita, Y
   Sato, Y
AF Sugano, Yusuke
   Matsushita, Yasuyuki
   Sato, Yoichi
TI Graph-Based Joint Clustering of Fixations and Visual Entities
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Human Factors; Clustering; image segmentation; ROI detection; gaze
   analysis; gaze visualization
AB We present a method that extracts groups of fixations and image regions for the purpose of gaze analysis and image understanding. Since the attentional relationship between visual entities conveys rich information, automatically determining the relationship provides us a semantic representation of images. We show that, by jointly clustering human gaze and visual entities, it is possible to build meaningful and comprehensive metadata that offer an interpretation about how people see images. To achieve this, we developed a clustering method that uses a joint graph structure between fixation points and over-segmented image regions to ensure a cross-domain smoothness constraint. We show that the proposed clustering method achieves better performance in relating attention to visual entities in comparison with standard clustering techniques.
C1 [Sugano, Yusuke; Sato, Yoichi] Univ Tokyo, Inst Ind Sci, Meguro Ku, Tokyo 1538505, Japan.
   [Matsushita, Yasuyuki] Microsoft Res Asia, Beijing 100080, Peoples R China.
C3 University of Tokyo; Microsoft; Microsoft Research Asia
RP Sugano, Y (corresponding author), Univ Tokyo, Inst Ind Sci, Meguro Ku, 4-6-1 Komaba, Tokyo 1538505, Japan.
EM sugano@iis.u-tokyo.ac.jp
RI Sugano, Yusuke/X-3689-2019
OI Sugano, Yusuke/0000-0003-4206-710X; Matsushita,
   Yasuyuki/0000-0002-1935-4752
FU CREST, JST
FX This work was supported by CREST, JST.
CR [Anonymous], 2006, Conference on Computer Vision Pattern Recognition Workshop, DOI [DOI 10.1109/CVPRW.2006.48, 10.1109/CVPRW.2006.48]
   [Anonymous], J VIS
   Arbeláez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161
   Boykov YY, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P105, DOI 10.1109/ICCV.2001.937505
   Caldara R, 2011, BEHAV RES METHODS, V43, P864, DOI 10.3758/s13428-011-0092-x
   Catanzaro B, 2009, IEEE I CONF COMP VIS, P2381, DOI 10.1109/ICCV.2009.5459410
   De Berg M., 2008, Computational Geometry: Algorithms and Applications, V17
   Delong A, 2012, INT J COMPUT VISION, V96, P1, DOI 10.1007/s11263-011-0437-z
   Engbert R, 2006, PROG BRAIN RES, V154, P177, DOI 10.1016/S0079-6123(06)54009-9
   Hansen DW, 2010, IEEE T PATTERN ANAL, V32, P478, DOI 10.1109/TPAMI.2009.30
   HUBERT L, 1985, J CLASSIF, V2, P193, DOI 10.1007/BF01908075
   Judd T., 2009, P 12 IEEE INT C COMP
   Kenneth HolmqvistMarcus Nystrom., 2011, Eye Tracking: A Comprehensive Guide to Methods and Measures
   MAIRE M., 2008, P IEEE C COMPUTER VI, P1
   Maji S., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2057, DOI 10.1109/CVPR.2011.5995630
   MEYER F, 1994, SIGNAL PROCESS, V38, P113, DOI 10.1016/0165-1684(94)90060-4
   Mishra A, 2009, IEEE I CONF COMP VIS, P468, DOI 10.1109/ICCV.2009.5459254
   Pedregosa F, 2011, J MACH LEARN RES, V12, P2825
   Ramanathan S, 2010, LECT NOTES COMPUT SC, V6314, P30, DOI 10.1007/978-3-642-15561-1_3
   RAND WM, 1971, J AM STAT ASSOC, V66, P846, DOI 10.2307/2284239
   Rosenberg A, 2007, P 2007 JOINT C EMP M, P410, DOI [10 . 7916 / D80V8N84, DOI 10.7916/D80V8N84]
   Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720
   Santella A., 2004, P S EYE TRACKING RES, P27, DOI DOI 10.1145/968363.968368
   Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688
   Slaney M, 2011, IEEE MULTIMEDIA, V18, P12, DOI 10.1109/MMUL.2011.34
   Subramanian Ramanathan., 2011, Proceedings of the 19th ACM International Conference on Multimedia, P33
NR 26
TC 4
Z9 5
U1 0
U2 7
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD MAY
PY 2013
VL 10
IS 2
AR 10
DI 10.1145/2465780.2465784
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 214EV
UT WOS:000324114800004
DA 2024-07-18
ER

PT J
AU Phillips, PJ
   Jiang, F
   Narvekar, A
   Ayyad, J
   O'Toole, AJ
AF Phillips, P. Jonathon
   Jiang, Fang
   Narvekar, Abhijit
   Ayyad, Julianne
   O'Toole, Alice J.
TI An Other-Race Effect for Face Recognition Algorithms
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Algorithms; Human Factors; Verification; Experimentation; Face
   recognition; human-machine comparisons
ID OWN-RACE; BIAS
AB Psychological research indicates that humans recognize faces of their own race more accurately than faces of other races. This "other-race effect" occurs for algorithms tested in a recent international competition for state-of-the-art face recognition algorithms. We report results for a Western algorithm made by fusing eight algorithms from Western countries and an East Asian algorithm made by fusing five algorithms from East Asian countries. At the low false accept rates required for most security applications, the Western algorithm recognized Caucasian faces more accurately than East Asian faces and the East Asian algorithm recognized East Asian faces more accurately than Caucasian faces. Next, using a test that spanned all false alarm rates, we compared the algorithms with humans of Caucasian and East Asian descent matching face identity in an identical stimulus set. In this case, both algorithms performed better on the Caucasian faces-the "majority" race in the database. The Caucasian face advantage, however, was far larger for the Western algorithm than for the East Asian algorithm. Humans showed the standard other-race effect for these faces, but showed more stable performance than the algorithms over changes in the race of the test faces. State-of-the-art face recognition algorithms, like humans, struggle with " other-race face" recognition.
C1 [Phillips, P. Jonathon] Natl Inst Stand & Technol, Gaithersburg, MD 20899 USA.
   [Jiang, Fang; Narvekar, Abhijit; Ayyad, Julianne; O'Toole, Alice J.] Univ Texas Dallas, Sch Behav & Brain Sci, Richardson, TX 75083 USA.
C3 National Institute of Standards & Technology (NIST) - USA; University of
   Texas System; University of Texas Dallas
RP Phillips, PJ (corresponding author), Natl Inst Stand & Technol, 100 Bur Dr,MS 8940, Gaithersburg, MD 20899 USA.
EM jonathon@nist.gov; otoole@utdallas.edu
OI O'Toole, Alice/0000-0001-7981-1508
FU Technical Support Working Group of the Department of Defense; Federal
   Bureau of Investigation
FX This work was supported by funding from the Technical Support Working
   Group of the Department of Defense and A. O'Toole from. P. J. Phillips
   was supported in part by funding from the Federal Bureau of
   Investigation. The identification of any commercial product or trade
   name does not imply endorsement or recommendation by NIST.
CR [Anonymous], 1991, Detection theory: A user's guide
   BEVERIDGE JR, 2008, P 8 INT C AUT FAC GE
   BOTHWELL RK, 1989, PERS SOC PSYCHOL B, V15, P19, DOI 10.1177/0146167289151002
   BRYATT G, 1998, VISION RES, V38, P2455
   Furl N, 2002, COGNITIVE SCI, V26, P797, DOI 10.1016/S0364-0213(02)00084-8
   Givens G, 2004, PROC CVPR IEEE, P381
   Gross R, 2005, HANDBOOK OF FACE RECOGNITION, P193, DOI 10.1007/0-387-27257-7_10
   GROTHER P, 2004, 7083 NISTIR
   Kelly DJ, 2007, PSYCHOL SCI, V18, P1084, DOI 10.1111/j.1467-9280.2007.02029.x
   KUHL PK, 1992, SCIENCE, V255, P606, DOI 10.1126/science.1736364
   Levin DT, 2000, J EXP PSYCHOL GEN, V129, P559, DOI 10.1037//0096-3445.129.4.559
   MALPASS RS, 1969, J PERS SOC PSYCHOL, V13, P330, DOI 10.1037/h0028434
   Meissner CA, 2001, PSYCHOL PUBLIC POL L, V7, P3, DOI 10.1037//1076-8971.7.1.3
   Moon H, 2001, PERCEPTION, V30, P303, DOI 10.1068/p2896
   Nelson CA, 2001, INFANT CHILD DEV, V10, P3, DOI 10.1002/icd.239
   O'Toole AJ, 2007, IEEE T PATTERN ANAL, V29, P1642, DOI 10.1109/TPAMI.2007.1107
   OTOOLE AJ, 1994, MEM COGNITION, V22, P208, DOI 10.3758/BF03208892
   OTOOLE AJ, 2008, P 8 INT C AUT FAC GE
   Phillips PJ, 2010, IEEE T PATTERN ANAL, V32, P831, DOI 10.1109/TPAMI.2009.59
   Phillips PJ, 2005, PROC CVPR IEEE, P947
   Phillips PJ, 2000, IEEE T PATTERN ANAL, V22, P1090, DOI 10.1109/34.879790
   PHILLIPS PJ, 2003, 6965 NISTIR
   POLLACK I, 1964, PSYCHON SCI, V1, P125, DOI 10.3758/BF03342823
   Sangrigoli S, 2005, PSYCHOL SCI, V16, P440
   SHAPIRO PN, 1986, PSYCHOL BULL, V100, P139, DOI 10.1037/0033-2909.100.2.139
   Slone AE, 2000, BASIC APPL SOC PSYCH, V22, P71, DOI 10.1207/15324830051036162
   TANG X, 2004, IEEE T CIRCUITS SYST, P50
   Tang XO, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P687, DOI 10.1109/ICCV.2003.1238414
   Walker PM, 2003, PERCEPTION, V32, P1117, DOI 10.1068/p5098
NR 29
TC 111
Z9 120
U1 1
U2 9
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD JAN
PY 2011
VL 8
IS 2
AR 14
DI 10.1145/1870076.1870082
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA 748AR
UT WOS:000289362900006
DA 2024-07-18
ER

PT J
AU Balasubramanian, JK
   Kumar, R
   Muniyandi, M
AF Balasubramanian, Jagan Krishnasamy
   Kumar, Rahul
   Muniyandi, Manivannan
TI Effect of Subthreshold Electrotactile Stimulation on the Perception of
   Electrovibration
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Subthreshold; electrovibration; electrotactile displays; texture
   rendering
ID TACTILE PERCEPTION; SURFACE; VIBROTACTILE; DISPLAY; MASKING
AB Electrovibration is used in touch enabled devices to render different textures. Tactile sub-modal stimuli can enhance texture perception when presented along with electrovibration stimuli. Perception of texture depends on the threshold of electrovibration. In the current study, we have conducted a psychophysical experiment on 13 participants to investigate the effect of introducing a subthreshold electrotactile stimulus (SES) to the perception of electrovibration. Interaction of tactile sub-modal stimuli causes masking of a stimulus in the presence of another stimulus. This study explored the occurrence of tactile masking of electrovibration by electrotactile stimulus. The results indicate the reduction of electrovibration threshold by 12.46% and 6.75% when the electrotactile stimulus was at 90% and 80% of its perception threshold, respectively. This method was tested over a wide range of frequencies from 20 Hz to 320 Hz in the tuning curve, and the variation in percentage reduction with frequency is reported. Another experiment was conducted to measure the perception of combined stimuli on the Likert scale. The results showed that the perception was more inclined towards the electrovibration at 80% of SES and was indifferent at 90% of SES. The reduction in the threshold of electrovibration reveals that the effect of tactile masking by electrotactile stimulus was not prevalent under subthreshold conditions. This study provides significant insights into developing a texture rendering algorithm based on tactile sub-modal stimuli in the future.
C1 [Balasubramanian, Jagan Krishnasamy; Kumar, Rahul] Indian Inst Technol Madras, TouchLab, Mechnical Sci Block, Chennai, Tamil Nadu, India.
   [Muniyandi, Manivannan] Indian Inst Technol Madras, Dept Appl Mech, Chennai, Tamil Nadu, India.
C3 Indian Institute of Technology System (IIT System); Indian Institute of
   Technology (IIT) - Madras; Indian Institute of Technology System (IIT
   System); Indian Institute of Technology (IIT) - Madras
RP Balasubramanian, JK (corresponding author), Indian Inst Technol Madras, TouchLab, Mechnical Sci Block, Chennai, Tamil Nadu, India.; Muniyandi, M (corresponding author), Indian Inst Technol Madras, Dept Appl Mech, Chennai, Tamil Nadu, India.
EM jagan.kb38@gmail.com; rahulraiecb@gmail.com; mani@iitm.ac.in
OI Ray, Dr. Rahul Kumar/0000-0002-6050-3158
CR Balasubramanian JK, 2022, LECT NOTES COMPUT SC, V13417, P37, DOI 10.1007/978-3-031-15019-7_4
   Basdogan C, 2020, IEEE T HAPTICS, V13, P450, DOI 10.1109/TOH.2020.2990712
   Bau O., 2010, P 23 ANN ACM S US IN, P283, DOI DOI 10.1145/1866029.1866074
   BOLANOWSKI SJ, 1988, J ACOUST SOC AM, V84, P1680, DOI 10.1121/1.397184
   BUTIKOFER R, 1978, IEEE T BIO-MED ENG, V25, P526, DOI 10.1109/TBME.1978.326286
   Chubb EC, 2010, IEEE T HAPTICS, V3, P189, DOI [10.1109/ToH.2010.7, 10.1109/TOH.2010.7]
   Chubb EC, 2009, WORLD HAPTICS 2009: THIRD JOINT EUROHAPTICS CONFERENCE AND SYMPOSIUM ON HAPTIC INTERFACES FOR VIRTUAL ENVIRONMENT AND TELEOPERATOR SYSTEMS, PROCEEDINGS, P18, DOI 10.1109/WHC.2009.4810861
   COLLINS JJ, 1995, NATURE, V376, P236, DOI 10.1038/376236a0
   Dideriksen J, 2022, IEEE T HAPTICS, V15, P222, DOI 10.1109/TOH.2021.3117628
   Edward Colgate J., 2017, US Patent, Patent No. [9,733,746, 9733746]
   Gescheider G.A., 2010, INFORM PROCESSING CH
   Gescheider GA, 1995, J ACOUST SOC AM, V98, P3188, DOI 10.1121/1.413808
   Gescheider GA, 2002, SOMATOSENS MOT RES, V19, P114, DOI 10.1080/08990220220131505
   Giraud F, 2013, 2013 WORLD HAPTICS CONFERENCE (WHC), P199, DOI 10.1109/WHC.2013.6548408
   Guo XW, 2019, IEEE T HAPTICS, V12, P571, DOI 10.1109/TOH.2019.2897768
   Hess M, 2019, LUBRICANTS, V7, DOI 10.3390/lubricants7120102
   Iggo A., 1974, PERIPHERAL NERVOUS S, P347, DOI [10.1007/978-1-4615-8699-9_14, DOI 10.1007/978-1-4615-8699-9_14]
   Isleyen A, 2020, IEEE T HAPTICS, V13, P562, DOI 10.1109/TOH.2019.2959993
   Jamalzadeh M, 2021, IEEE T HAPTICS, V14, P132, DOI 10.1109/TOH.2020.3025772
   Jones LA, 2013, IEEE T HAPTICS, V6, P268, DOI 10.1109/TOH.2012.74
   KACZMAREK KA, 1991, IEEE T BIO-MED ENG, V38, P1, DOI 10.1109/10.68204
   Kaczmarek KA, 2006, IEEE T BIO-MED ENG, V53, P2047, DOI 10.1109/TBME.2006.881804
   Kajimoto H, 2012, IEEE T HAPTICS, V5, P184, DOI [10.1109/ToH.2011.39, 10.1109/TOH.2011.39]
   KALIA YN, 1995, PHARMACEUT RES, V12, P1605, DOI 10.1023/A:1016228730522
   Kang J, 2017, IEEE T HAPTICS, V10, P371, DOI 10.1109/TOH.2016.2635145
   Kim H, 2015, IEEE T HAPTICS, V8, P492, DOI 10.1109/TOH.2015.2476810
   Kitamura N, 2015, J MICROMECH MICROENG, V25, DOI 10.1088/0960-1317/25/2/025016
   Komurasaki S, 2019, MICROMACHINES-BASEL, V10, DOI 10.3390/mi10050301
   Loomis J.M., 1986, Handbook of perception and human performances, V2, P2
   Madhan Kumar V., 2021, Computational Intelligence in Healthcare, P379
   McAdams ET, 1996, MED BIOL ENG COMPUT, V34, P397, DOI 10.1007/BF02523842
   McGee S., 2018, Evidence-Based Physical Diagnosis, VFourth, P569
   Mullenbach J., 2012, 2012 IEEE Haptics Symposium (HAPTICS), P407, DOI 10.1109/HAPTIC.2012.6183823
   Mullenbach Joe, 2013, P HAID
   Mullenbach Joe., 2013, Proceedings_of_the_Adjunct_Publication of_the_26th_Annual_ACM_Symposium_on_User_Interface_Software_and_Technology, UIST'13 Adjunct, page, P7
   Nakamura T, 2016, IEEE T HAPTICS, V9, P311, DOI 10.1109/TOH.2016.2556660
   Osgouei R.H., 2020, Modern Applications of Electrostatics and Dielectrics
   Ray Rahul Kumar, 2022, Human Interaction, Emerging Technologies and Future Systems V: Proceedings of the 5th International Virtual Conference on Human Interaction and Emerging Technologies, IHIET 2021, and the 6th IHIET: Future Systems (IHIET-FS 2021). Lecture Notes in Networks and System (319), P331, DOI 10.1007/978-3-030-85540-6_42
   Ray Rahul Kumar, 2021, Human Interaction, Emerging Technologies and Future Applications IV. Proceedings of the 4th International Conference on Human Interaction and Emerging Technologies: Future Applications (IHIET - AI 2021). Advances in Intelligent Systems and Computing (AISC 1378), P463, DOI 10.1007/978-3-030-74009-2_59
   Ray RK, 2021, DISPLAYS, V69, DOI 10.1016/j.displa.2021.102056
   ROLLMAN GB, 1987, PERCEPT PSYCHOPHYS, V42, P257, DOI 10.3758/BF03203077
   Shultz CD, 2018, IEEE HAPTICS SYM, P151, DOI 10.1109/HAPTICS.2018.8357168
   Sirin O, 2019, J R SOC INTERFACE, V16, DOI 10.1098/rsif.2019.0166
   STRONG RM, 1970, IEEE T MAN MACHINE, VMM11, P72, DOI 10.1109/TMMS.1970.299965
   Vardar Y, 2018, IEEE T HAPTICS, V11, P623, DOI 10.1109/TOH.2018.2855124
   Vardar Y, 2017, IEEE T HAPTICS, V10, P488, DOI 10.1109/TOH.2017.2704603
   Vardar Yasemin, 2020, Tactile Perception by Electrovibration
   Vasudevan Madhan Kumar, 2020, Haptics: Science, Technology, Applications. 12th International Conference, EuroHaptics 2020. Proceedings. Lecture Notes in Computer Science (LNCS 12272), P203, DOI 10.1007/978-3-030-58147-3_23
   Vezzoli E, 2015, IEEE T HAPTICS, V8, P235, DOI 10.1109/TOH.2015.2430353
   Vodlak T., 2016, BIOTRIBOLOGY, V8, P12, DOI [DOI 10.1016/J.BI0TRI.2016.09.001), 10.1016/j.biotri.2016.09.001, DOI 10.1016/J.BIOTRI.2016.09.001]
   Withana A, 2018, UIST 2018: PROCEEDINGS OF THE 31ST ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P365, DOI 10.1145/3242587.3242645
   Xiaowei Dai, 2012, 2012 IEEE Haptics Symposium (HAPTICS), P7, DOI 10.1109/HAPTIC.2012.6183753
   Yamamoto A, 2006, IEEE T VIS COMPUT GR, V12, P168, DOI 10.1109/TVCG.2006.28
   Yixin Chen, 2019, 2019 IEEE The 2nd International Conference On Micro/Nano Sensors for AI, Healthcare and Robotics (NSENS), P87, DOI 10.1109/NSENS49395.2019.9293989
NR 54
TC 0
Z9 0
U1 2
U2 2
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD JUL
PY 2023
VL 20
IS 3
AR 9
DI 10.1145/3599970
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA U6DF9
UT WOS:001085681100001
DA 2024-07-18
ER

PT J
AU Aygar, E
   Ware, C
   Rogers, D
AF Aygar, Erol
   Ware, Colin
   Rogers, David
TI The Contribution of Stereoscopic and Motion Depth Cues to the Perception
   of Structures in 3D Point Clouds
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Point cloud data; 3D perception; depth cues
ID SEX-DIFFERENCES; STEREOPSIS; DISPLAYS
AB Particle-based simulations are used across many science domains, and it is well known that stereoscopic viewing and kinetic depth enhance our ability to perceive the 3D structure of such data. But the relative advantages of stereo and kinetic depth have not been studied for point cloud data, although they have been studied for 3D networks. This article reports two experiments assessing human ability to perceive 3D structures in point clouds as a function of different viewing parameters. In the first study, the number of discrete views was varied to determine the extent to which smooth motion is needed. Also, half the trials had stereoscopic viewing and half had no stereo. The results showed kinetic depth to be more beneficial than stereo viewing in terms of accuracy and so long as the motion was smooth. The second experiment varied the amplitude of oscillatory motion from 0 to 16 degrees. The results showed an increase in detection rate with amplitude, with the best amplitudes being 4 degrees and greater. Overall, motion was shown to yield greater accuracy, but at the expense of longer response times in comparison with stereoscopic viewing.
C1 [Aygar, Erol] Univ New Hampshire, CCOM, 24 Colovos Rd, Durham, NH 03824 USA.
   [Ware, Colin] Univ New Hampshire, Durham, NH 03824 USA.
   [Ware, Colin] Jere A Chase Ocean Engn Lab, 24 Colovos Rd, Durham, NH 03824 USA.
   [Rogers, David] Los Alamos Natl Lab, POB 1663, Los Alamos, NM 87545 USA.
C3 University System Of New Hampshire; University of New Hampshire;
   University System Of New Hampshire; University of New Hampshire; United
   States Department of Energy (DOE); Los Alamos National Laboratory
RP Aygar, E (corresponding author), Univ New Hampshire, CCOM, 24 Colovos Rd, Durham, NH 03824 USA.
EM ea2003@wildcats.unh.edu; cware@ccom.unh.edu; dhr@lanl.go
FU US Department of Energy [DE-SC0012438]; U.S. Department of Energy (DOE)
   [DE-SC0012438] Funding Source: U.S. Department of Energy (DOE)
FX We gratefully acknowledge funding from the US Department of Energy Grant
   No. DE-SC0012438.
CR Ahern Sean, 2011, SCI DISC EX DOE ASCR
   Ahrens J, 2014, INT CONF HIGH PERFOR, P424, DOI 10.1109/SC.2014.40
   [Anonymous], 2012, INFORM VISUAL
   Arsenault R, 2004, PRESENCE-TELEOP VIRT, V13, P549, DOI 10.1162/1054746042545300
   Barfield W, 1997, P IEEE VIRT REAL ANN, P114, DOI 10.1109/VRAIS.1997.583052
   Bonneau G.-P., 2006, Scientific Visualization: The Visual Extraction of Knowledge from Data
   Bradshaw MF, 2000, VISION RES, V40, P3725, DOI 10.1016/S0042-6989(00)00214-5
   Cho I, 2012, PROCEEDINGS OF THE INTERNATIONAL WORKING CONFERENCE ON ADVANCED VISUAL INTERFACES, P266, DOI 10.1145/2254556.2254606
   Habib S, 2009, J PHYS CONF SER, V180, DOI 10.1088/1742-6596/180/1/012019
   Hassaine D, 2010, ACM T APPL PERCEPT, V8, DOI 10.1145/1857893.1857901
   Holliman NS, 2011, IEEE T BROADCAST, V57, P362, DOI 10.1109/TBC.2011.2130930
   Howard Ian P, 1995, Binocular Vision and Stereopsis
   Hubona GS, 1997, INT J HUM-COMPUT ST, V47, P609, DOI 10.1006/ijhc.1997.0154
   Ijsselsteijn W, 2001, PRESENCE-TELEOP VIRT, V10, P298, DOI 10.1162/105474601300343621
   LINN MC, 1985, CHILD DEV, V56, P1479, DOI 10.1111/j.1467-8624.1985.tb00213.x
   Muller M., 2003, ACM SIGGRAPH/Eurographics Symposium on Computer Animation, P154
   Naepflin U, 2001, DISPLAYS, V22, P157, DOI 10.1016/S0141-9382(01)00067-1
   NORMAN JF, 1995, PERCEPT PSYCHOPHYS, V57, P629
   Palmer S., 1999, VISION SCI PHOTONS P
   Shirley P., 1990, Computer Graphics, V24, P63, DOI 10.1145/99308.99322
   SOLLENBERGER RL, 1993, HUM FACTORS, V35, P483, DOI 10.1177/001872089303500306
   Sweet G, 2004, PROC GRAPH INTERF, P97
   Tarini M, 2006, IEEE T VIS COMPUT GR, V12, P1237, DOI 10.1109/TVCG.2006.115
   van Beurden Maurice H. P. H., 2010, Proceedings of the 2010 Second International Workshop on Quality of Multimedia Experience (QoMEX 2010), P176, DOI 10.1109/QOMEX.2010.5516268
   VOYER D, 1995, PSYCHOL BULL, V117, P250, DOI 10.1037/0033-2909.117.2.250
   WALLACH H, 1953, J EXP PSYCHOL, V45, P205, DOI 10.1037/h0056880
   Wand M, 2008, COMPUT GRAPH-UK, V32, P204, DOI 10.1016/j.cag.2008.01.010
   WANGER LR, 1992, IEEE COMPUT GRAPH, V12, P44, DOI 10.1109/38.135913
   Ware C, 1998, IEEE T SYST MAN CY A, V28, P56, DOI 10.1109/3468.650322
   WARE C, 1993, HUMAN FACTORS IN COMPUTING SYSTEMS, P37
   Ware C, 1996, ACM T GRAPHIC, V15, P121, DOI 10.1145/234972.234975
   Ware C., 2005, Proceedings of the 2nd Symposium on Applied Perception in Graphics and Visualization (A Corona, Spain, August 26 - 28, V95, P51, DOI DOI 10.1145/1080402.1080411
NR 32
TC 10
Z9 12
U1 0
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD APR
PY 2018
VL 15
IS 2
AR 9
DI 10.1145/3147914
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA GH5YV
UT WOS:000433515400002
DA 2024-07-18
ER

PT J
AU Noceti, N
   Odone, F
   Sciutti, A
   Sandini, G
AF Noceti, Nicoletta
   Odone, Francesca
   Sciutti, Alessandra
   Sandini, Giulio
TI Exploring Biological Motion Regularities of Human Actions: A New
   Perspective on Video Analysis
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Biological motion perception; two-thirds power law; motion perception
   development
ID POWER-LAW; RECOGNITION; MOVEMENTS; PERCEPTION; FEATURES; VELOCITY;
   SYSTEMS
AB The ability to detect potentially interacting agents in the surrounding environment is acknowledged to be one of the first perceptual tasks developed by humans, supported by the ability to recognise biological motion. The precocity of this ability suggests that it might be based on rather simple motion properties, and it can be interpreted as an atomic building block of more complex perception tasks typical of interacting scenarios, as the understanding of non-verbal communication cues based on motion or the anticipation of others' action goals.
   In this article, we propose a novel perspective for video analysis, bridging cognitive science and machine vision, which lever-ages the use of computational models of the perceptual primitives that are at the basis of biological motion perception in humans.
   Our work offers different contributions. In a first part, we propose an empirical formulation for the Two-Thirds Power Law, a well-known invariant law of human movement, and thoroughly discuss its readability in experimental settings of increasing complexity. In particular, we consider unconstrained video analysis scenarios, where, to the best of our knowledge, the invariant law has not found application so far.
   The achievements of this analysis pave the way for the second part of the work, in which we propose and evaluate a general representation scheme for biological motion characterisation to discriminate biological movements with respect to non-biological dynamic events in video sequences. The method is proposed as the first layer of a more complex architecture for behaviour analysis and human-machine interaction, providing in particular a new way to approach the problem of human action understanding.
C1 [Noceti, Nicoletta; Odone, Francesca] Univ Genoa, DIBRIS, Via Dodecaneso 35, IT-16146 Genoa, Italy.
   [Sciutti, Alessandra; Sandini, Giulio] Ist Italiano Tecnol, RBCS, Via Enrico Melen 83, IT-16152 Genoa, Italy.
C3 University of Genoa; Istituto Italiano di Tecnologia - IIT
RP Noceti, N (corresponding author), Univ Genoa, DIBRIS, Via Dodecaneso 35, IT-16146 Genoa, Italy.
EM nicoletta.noceti@unige.it; francesca.odone@unige.it;
   alessandra.sciutti@iit.it; giulio.sandini@iit.it
RI ; Sciutti, Alessandra/E-4291-2017
OI Sandini, Giulio/0000-0003-3324-985X; Sciutti,
   Alessandra/0000-0002-1056-3398
FU European CODEFROR project [FP7-PIRSES-2013-612555]
FX This work is partially supported by the European CODEFROR project
   (FP7-PIRSES-2013-612555).
CR Aggarwal JK, 2011, ACM COMPUT SURV, V43, DOI 10.1145/1922649.1922653
   Aggarwal JK, 1999, COMPUT VIS IMAGE UND, V73, P428, DOI 10.1006/cviu.1998.0744
   [Anonymous], 2017, ACM T APPL PERCEPTIO, V14
   Casile A, 2005, J VISION, V5, P348, DOI 10.1167/5.4.6
   CEDRAS C, 1995, IMAGE VISION COMPUT, V13, P129, DOI 10.1016/0262-8856(95)93154-K
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   deSperati C, 1997, J NEUROSCI, V17, P3932
   Fabbri-Destro M, 2008, PHYSIOLOGY, V23, P171, DOI 10.1152/physiol.00004.2008
   Fanello SR, 2013, J MACH LEARN RES, V14, P2617
   Farnebäck G, 2003, LECT NOTES COMPUT SC, V2749, P363, DOI 10.1007/3-540-45103-x_50
   Flach R, 2004, VIS COGN, V11, P461, DOI 10.1080/13506280344000392
   Greene P. H., 1972, PROGR THEORETICAL BI, V2, P123
   Hogan N, 2007, EXP BRAIN RES, V181, P13, DOI 10.1007/s00221-007-0899-y
   Huh D, 2015, P NATL ACAD SCI USA, V112, pE3950, DOI 10.1073/pnas.1510208112
   Jolliffe I.T., 1986, Principal component analysis, DOI DOI 10.1016/0169-7439(87)80084-9
   Kandel S, 2000, PERCEPT PSYCHOPHYS, V62, P706, DOI 10.3758/BF03206917
   Kao CY, 2011, PROCEDIA ENGINEER, V15, DOI 10.1016/j.proeng.2011.08.700
   LACQUANITI F, 1983, ACTA PSYCHOL, V54, P115, DOI 10.1016/0001-6918(83)90027-6
   Lacquaniti F., 1984, GLOBAL METRIC PROPER, P357
   MARQUARDT DW, 1963, J SOC IND APPL MATH, V11, P431, DOI 10.1137/0111030
   Marr D., 1982, Proceedings of the Royal Society of London. Series B, V214, P501, DOI DOI 10.1098/RSPB.1982.0024
   MATHER G, 1992, P ROY SOC B-BIOL SCI, V249, P149, DOI 10.1098/rspb.1992.0097
   Metta G, 2010, NEURAL NETWORKS, V23, P1125, DOI 10.1016/j.neunet.2010.08.010
   Muñoz-Salinas R, 2007, IMAGE VISION COMPUT, V25, P995, DOI 10.1016/j.imavis.2006.07.012
   Noceti N, 2015, LECT NOTES COMPUT SC, V9280, P676, DOI 10.1007/978-3-319-23234-8_62
   Noceti N, 2012, IMAGE VISION COMPUT, V30, P875, DOI 10.1016/j.imavis.2012.07.005
   Papaxanthis C, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0051191
   Plamondon R, 1998, ACTA PSYCHOL, V100, P85, DOI 10.1016/S0001-6918(98)00027-4
   Rao C, 2002, INT J COMPUT VISION, V50, P203, DOI 10.1023/A:1020350100748
   Rautaray SS, 2015, ARTIF INTELL REV, V43, P1, DOI 10.1007/s10462-012-9356-9
   Ren Z, 2013, IEEE T MULTIMEDIA, V15, P1110, DOI 10.1109/TMM.2013.2246148
   Richardson MJE, 2002, J NEUROSCI, V22, P8201
   Sigala R, 2005, LECT NOTES COMPUT SC, V3696, P241, DOI 10.1007/11550822_39
   Simion F, 2008, P NATL ACAD SCI USA, V105, P809, DOI 10.1073/pnas.0707021105
   Sternad D, 1999, EXP BRAIN RES, V124, P118, DOI 10.1007/s002210050606
   Tommasi T., 2008, PATTERN RECOGN LETT, V29, P15
   Troje NF, 2006, CURR BIOL, V16, P821, DOI 10.1016/j.cub.2006.03.022
   Troje NF, 2002, J VISION, V2, P371, DOI 10.1167/2.5.2
   Vadakkepat P, 2008, IEEE T IND ELECTRON, V55, P1385, DOI 10.1109/TIE.2007.903993
   Vielledent S, 2001, NEUROSCI LETT, V305, P65, DOI 10.1016/S0304-3940(01)01798-0
   Vignolo A, 2016, IEEE-RAS INT C HUMAN, P338, DOI 10.1109/HUMANOIDS.2016.7803298
   Viviani P, 1997, J EXP PSYCHOL HUMAN, V23, P1232, DOI 10.1037/0096-1523.23.4.1232
   VIVIANI P, 1989, PERCEPT PSYCHOPHYS, V46, P266, DOI 10.3758/BF03208089
   VIVIANI P, 1992, J EXP PSYCHOL HUMAN, V18, P603, DOI 10.1037/0096-1523.18.3.603
   VIVIANI P, 1985, J EXP PSYCHOL HUMAN, V11, P828, DOI 10.1037/0096-1523.11.6.828
   VIVIANI P, 1995, J EXP PSYCHOL HUMAN, V21, P32, DOI 10.1037/0096-1523.21.1.32
   VIVIANI P, 1982, NEUROSCIENCE, V7, P431, DOI 10.1016/0306-4522(82)90277-9
   VIVIANI P, 1983, NEUROSCIENCE, V10, P211, DOI 10.1016/0306-4522(83)90094-5
   VIVIANI P, 1991, J EXP PSYCHOL HUMAN, V17, P198, DOI 10.1037/0096-1523.17.1.198
   Wang XG, 2009, IEEE T PATTERN ANAL, V31, P539, DOI 10.1109/TPAMI.2008.87
   Weinland D, 2011, COMPUT VIS IMAGE UND, V115, P224, DOI 10.1016/j.cviu.2010.10.002
NR 51
TC 8
Z9 8
U1 1
U2 9
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD JUL
PY 2017
VL 14
IS 3
AR 21
DI 10.1145/3086591
PG 20
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA FB9AL
UT WOS:000406431900008
DA 2024-07-18
ER

PT J
AU Blissing, B
   Bruzelius, F
   Eriksson, O
AF Blissing, Bjorn
   Bruzelius, Fredrik
   Eriksson, Olle
TI Effects of Visual Latency on Vehicle Driving Behavior
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Latency; head-mounted display; video see-through; vehicle driving
ID SIMULATOR SICKNESS; DELAY
AB Using mixed reality in vehicles provides a potential alternative to using driving simulators when studying driver-vehicle interaction. However, virtual reality systems introduce latency in the visual system that may alter driving behavior, which, in turn, results in questionable validity. Previous studies have mainly focused on visual latency as a separate phenomenon. In this work, latency is studied from a task-dependent viewpoint to investigate how participants' driving behavior changed with increased latency. In this study, the investigation was performed through experiments in which regular drivers were subjected to different levels of visual latency while performing a simple slalom driving task. The drivers' performances were recorded and evaluated in both lateral and longitudinal directions along with self-assessment questionnaires regarding task performance and difficulty. All participants managed to complete the driving tasks successfully, even under high latency conditions, but were clearly affected by the increased visual latency. The results suggest that drivers compensate for longer latencies by steering more and increasing the safety margins but without reducing their speed.
C1 [Blissing, Bjorn; Bruzelius, Fredrik; Eriksson, Olle] Swedish Natl Rd & Transport Res Inst VTI, Linkoping, Sweden.
C3 VTI
RP Blissing, B (corresponding author), VTI, SE-58195 Linkoping, Sweden.
EM bjorn.blissing@vti.se; fredrik.bruzelius@vti.se; olle.eriksson@vti.se
OI Blissing, Bjorn/0000-0001-5057-4043
FU VINNOVA/FFI project "Next Generation Test Methods for Active Safety
   Functions"; Swedish National Road and Transport Research Institute via
   the strategic research area, TRENoP
FX This project was funded primarily by the VINNOVA/FFI project "Next
   Generation Test Methods for Active Safety Functions." Additional funding
   was provided by the Swedish National Road and Transport Research
   Institute via the strategic research area, TRENoP.
CR Adelstein B. D., 2003, P HUMAN FACTORS ERGO, V47, P2083, DOI [DOI 10.1177/1541931203047020, DOI 10.1177/154193120304702001]
   Allen Wade R., 1984, P 20 ANN C MAN CONTR, P185
   Allison RS, 2001, P IEEE VIRT REAL ANN, P247, DOI 10.1109/VR.2001.913793
   [Anonymous], 2015, 2015 ROAD SAFETY SIM
   Berg Guy, 2013, FAHRER 21 JAHRHUNDER, V2205, P225
   Bock T, 2007, 2007 IEEE INTELLIGENT VEHICLES SYMPOSIUM, VOLS 1-3, P219
   Bock Thomas, 2005, 4 IEEE ACM INT S MIX
   Buker TJ, 2012, HUM FACTORS, V54, P235, DOI 10.1177/0018720811428734
   Cunningham DW, 2001, J VISION, V1, P88, DOI 10.1167/1.2.3
   Dagdelen M., 2002, P DRIVING SIMULATION, P109
   Dodgson NA, 2004, PROC SPIE, V5291, P36, DOI 10.1117/12.529999
   Ellis S.R., 2004, Proceedings of the Human Factors and Ergonomics Society 48th annual meeting, P2632, DOI DOI 10.1177/154193120404802306
   Gawron VJ, 2010, INT J AVIAT PSYCHOL, V20, P221, DOI 10.1080/10508414.2010.487007
   Heitbrink D., 2015, IMAGE 2015 C
   Jacobs M. C., 1997, Proceedings 1997 Symposium on Interactive 3D Graphics, P49, DOI 10.1145/253284.253306
   Jerald J, 2009, IEEE VIRTUAL REALITY 2009, PROCEEDINGS, P211, DOI 10.1109/VR.2009.4811025
   Karl I, 2013, IEEE INTEL TRANSP SY, V5, P42, DOI 10.1109/MITS.2012.2217995
   Kemeny A, 2003, TRENDS COGN SCI, V7, P31, DOI 10.1016/S1364-6613(02)00011-6
   Mania Katerina., 2004, Proceedings of the 1st Symposium on Applied Perception in Graphics and Visualization, APGV '04, P39, DOI [10.1145/1012551.1012559, DOI 10.1145/1012551.1012559]
   Meehan M, 2003, P IEEE VIRT REAL ANN, P141, DOI 10.1109/VR.2003.1191132
   Moss JD, 2010, DISPLAYS, V31, P143, DOI 10.1016/j.displa.2010.04.002
   Papadakis Giorgos, 2012, P 10 INT C VIRT REAL, P475
   Regenbrecht H, 2005, IEEE COMPUT GRAPH, V25, P48, DOI 10.1109/MCG.2005.124
NR 23
TC 10
Z9 12
U1 0
U2 14
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD AUG
PY 2016
VL 14
IS 1
AR 5
DI 10.1145/2971320
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DV4EB
UT WOS:000382876900005
DA 2024-07-18
ER

PT J
AU Boi, P
   Fenu, G
   Spano, LD
   Vargiu, V
AF Boi, Paolo
   Fenu, Gianni
   Spano, Lucio Davide
   Vargiu, Valentino
TI Reconstructing User's Attention on the Web through Mouse Movements and
   Perception-Based Content Identification
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE User's attention; content segmentation; mouse-eye correlation; layout
   analysis; machine learning; Human Factors; Measurement
ID PAGES
AB Eye tracking is one of the most exploited techniques in literature for finding usability problems in web-based user interfaces (UIs). However, it is usually employed in a laboratory setting, considering that an eye-tracker is not commonly used in web browsing. In contrast, web application providers usually exploit remote techniques for large-scale user studies (e.g. A/B testing), tracking low-level interactions such as mouse clicks and movements. In this article, we discuss a method for predicting whether the user is looking at the content pointed by the cursor, exploiting the mouse movement data and a segmentation of the contents in a web page. We propose an automatic method for segmenting content groups inside a web page that, applying both image and code analysis techniques, identifies the user-perceived group of contents with a mean pixel-based error around the 20%. In addition, we show through a user study that such segmentation information enhances the precision and the accuracy in predicting the correlation between between the user's gaze and the mouse position at the content level, without relaying on user-specific features.
C1 [Boi, Paolo; Fenu, Gianni; Spano, Lucio Davide; Vargiu, Valentino] Univ Cagliari, Dept Math & Comp Sci, I-09124 Cagliari, Italy.
   [Boi, Paolo; Fenu, Gianni; Spano, Lucio Davide; Vargiu, Valentino] Univ Cagliari, Dipartimento Matemat & Informat, Via Osped 72, I-09124 Cagliari, Italy.
C3 University of Cagliari; University of Cagliari
RP Spano, LD (corresponding author), Univ Cagliari, Dipartimento Matemat & Informat, Via Osped 72, I-09124 Cagliari, Italy.
EM paoloboi87@gmail.com; fenu@unica.it; davide.spano@unica.it;
   valentinovargiu@gmail.com
RI Spano, Lucio Davide/AAD-7209-2021
OI Spano, Lucio Davide/0000-0001-7106-0463; FENU,
   GIANNI/0000-0003-4668-2476
FU Sardinia Regional Government (P.O.R. Sardegna F.S.E., European Social
   Fund)
FX This work was supported by the Sardinia Regional Government (P.O.R.
   Sardegna F.S.E., European Social Fund 2007-2013 - Axis IV, Obj 1.3, LoA
   1.3.1).
CR [Anonymous], 2006, Proceedings of the 15th International Conference on World Wide Web, DOI [DOI 10.1145/1135777.1135811, 10.1145/1135777.1135811]
   Bernhard M, 2014, ACM T APPL PERCEPT, V11, DOI 10.1145/2644812
   Bernhard M, 2010, ACM T APPL PERCEPT, V8, DOI 10.1145/1857893.1857897
   BURGET R, 2009, P 1 AS C INT INF DAT, P67
   Cai D, 2003, LECT NOTES COMPUT SC, V2642, P406
   Cai D., 2003, TECHNICAL REPORT
   Cao JX, 2010, INT J PARALLEL EMERG, V25, P93, DOI 10.1080/17445760802429585
   Carta T., 2011, P 29 ACM INT C DESIG, P129
   Chen Y, 2005, IEEE INTERNET COMPUT, V9, P50, DOI 10.1109/MIC.2005.5
   Di Eugenio B, 2004, COMPUT LINGUIST, V30, P95, DOI 10.1162/089120104773633402
   Diaz F, 2013, PROCEEDINGS OF THE 22ND ACM INTERNATIONAL CONFERENCE ON INFORMATION & KNOWLEDGE MANAGEMENT (CIKM'13), P1451, DOI 10.1145/2505515.2505717
   Diriye A., 2012, P 21 ACM INT C INF K, P1025, DOI [10.1145/2396761.2398399, DOI 10.1145/2396761.2398399]
   Friedman J, 2000, ANN STAT, V28, P337, DOI 10.1214/aos/1016218223
   Guo Q., 2010, CHI 10 HUM FACT COMP, P3601, DOI DOI 10.1145/1753846.1754025
   Huang J., 2012, P SIGCHI C HUM FACT, P1341, DOI DOI 10.1145/2207676.2208591
   Huang J, 2011, 29TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P1225
   Iqbal S. T., 2004, GRACE HOPPER CELEBRA, P5
   Jaekyu Ha, 1995, Proceedings of the Third International Conference on Document Analysis and Recognition, P952, DOI 10.1109/ICDAR.1995.602059
   Liu Y, 2004, LECT NOTES COMPUT SC, V3129, P280
   Mantiuk R, 2012, LECT NOTES COMPUT SC, V7131, P115
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Papenmeier F, 2010, BEHAV RES METHODS, V42, P179, DOI 10.3758/BRM.42.1.179
   Reinecke Katharina, 2013, P SIGCHI C HUM FACT, P2049, DOI [10.1145/2470654.2481281, DOI 10.1145/2470654.2481281, 10]
   Rodden K., 2008, CHI 08 EXTENDED ABST, P2997, DOI DOI 10.1145/1358628.1358797
   San Agustin J., 2009, Proceedings of the 27th international conference extended abstracts on Human factors in computing systems - CHI EA '09, P4453
   Sundstedt V, 2008, APGV 2008: PROCEEDINGS OF THE SYMPOSIUM ON APPLIED PERCEPTION IN GRAPHICS AND VISUALIZATION, P43
   Witten I. H., 2005, DATA MINING PRACTICA
   Xiang PF, 2007, 2007 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOLS 1-5, P2253
   Xiao-Dong Gu, 2002, Adaptive Hypermedia and Adaptive Web-Based Systems. Second International Conference, AH 2002. Proceedings (Lecture Notes in Computer Science Vol.2347), P164
   Yang X, 2009, 2009 IEEE/WIC/ACM INTERNATIONAL JOINT CONFERENCES ON WEB INTELLIGENCE (WI) AND INTELLIGENT AGENT TECHNOLOGIES (IAT), VOL 3, P46
   Yendrikhovskij SN, 1998, SIXTH COLOR IMAGING CONFERENCE: COLOR SCIENCE, SYSTEMS AND APPLICATIONS, P140
   Yesilada Y., 2011, TECHNICAL REPORT
   Yin X., 2004, PROC INT WORLD WIDE, P338, DOI DOI 10.1145/988672.988718
NR 33
TC 13
Z9 13
U1 0
U2 13
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD MAY
PY 2016
VL 13
IS 3
AR 15
DI 10.1145/2912124
PG 21
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DV4DU
UT WOS:000382876200005
DA 2024-07-18
ER

PT J
AU Wang, YY
   Tree, JEF
   Walker, M
   Neff, M
AF Wang, Yingying
   Tree, Jean E. Fox
   Walker, Marilyn
   Neff, Michael
TI Assessing the Impact of Hand Motion on Virtual Character Personality
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Human Factors; Personality; hand motion; conversational and non-verbal
   behavior; evaluation
ID HUMAN-COMPUTER INTERACTION; LANGUAGE; GENERATION; PERCEPTION; GESTURE
AB Designing virtual characters that are capable of conveying a sense of personality is important for generating realistic experiences, and thus a key goal in computer animation research. Though the influence of gesture and body motion on personality perception has been studied, little is known about which attributes of hand pose and motion convey particular personality traits. Using the "Big Five" model as a framework for evaluating personality traits, this work examines how variations in hand pose and motion impact the perception of a character's personality. As has been done with facial motion, we first study hand motion in isolation as a requirement for running controlled experiments that avoid the combinatorial explosion of multimodal communication (all combinations of facial expressions, arm movements, body movements, and hands) and allow us to understand the communicative content of hands. We determined a set of features likely to reflect personality, based on research in psychology and previous human motion perception work: shape, direction, amplitude, speed, and manipulation. Then we captured realistic hand motion varying these attributes and conducted three perceptual experiments to determine the contribution of these attributes to the character's personalities. Both hand poses and the amplitude of hand motion affected the perception of all five personality traits. Speed impacted all traits except openness. Direction impacted extraversion and openness. Manipulation was perceived as an indicator of introversion, disagreeableness, neuroticism, and less openness to experience. From these results, we generalize guidelines for designing detailed hand motion that can add to the expressiveness and personality of characters. We performed an evaluation study that combined hand motion with gesture and body motion. Even in the presence of body motion, hand motion still significantly impacted the perception of a character's personality and could even be the dominant factor in certain situations.
C1 [Wang, Yingying; Neff, Michael] Univ Calif Davis, Dept Comp Sci, One Shields Ave, Davis, CA 95616 USA.
   [Tree, Jean E. Fox] Univ Calif Santa Cruz, Dept Psychol, Social Sci 2, Santa Cruz, CA 95064 USA.
   [Walker, Marilyn] Univ Calif Santa Cruz, Dept Comp Sci SOE3, Nat Language & Dialogue Syst Lab, Santa Cruz, CA 95064 USA.
C3 University of California System; University of California Davis;
   University of California System; University of California Santa Cruz;
   University of California System; University of California Santa Cruz
RP Wang, YY; Neff, M (corresponding author), Univ Calif Davis, Dept Comp Sci, One Shields Ave, Davis, CA 95616 USA.; Tree, JEF (corresponding author), Univ Calif Santa Cruz, Dept Psychol, Social Sci 2, Santa Cruz, CA 95064 USA.; Walker, M (corresponding author), Univ Calif Santa Cruz, Dept Comp Sci SOE3, Nat Language & Dialogue Syst Lab, Santa Cruz, CA 95064 USA.
EM yiwang@ucdavis.edu; foxtree@ucsc.edu; maw@soe.ucsc.edu;
   mpneff@ucdavis.edu
RI wang, yan/JBJ-7462-2023; Wang, Yingying/HOA-6227-2023; wang,
   yingying/JCE-4984-2023; Li, Yan/JUU-5189-2023; wang, yi/GVT-8516-2022;
   wang, yan/GSE-6489-2022; wang, yingying/GRS-3058-2022
FU NSF [IIS 1115872]
FX Financial support for this work was provided by the NSF through grant
   IIS 1115872.
CR André E, 2000, EMBODIED CONVERSATIONAL AGENTS, P220
   [Anonymous], 2007, P 45 ANN M ASS COMP
   [Anonymous], 2016, ACM T APPL PERCEPTIO, V13
   Argyle Michael, 1975, BODILY COMMUNICATION
   Autodesk, 2012, MAYA 3D COMP GRAPH S
   Badler N, 2002, COMP ANIM CONF PROC, P133, DOI 10.1109/CA.2002.1017521
   Brebner J., 1985, PERSONALITY THEORY M, P27
   Burgoon J.K., 1978, The unspoken dialogue: An introduction to nonverbal communication
   Chou T. S., 2000, P HUM INT TECHN 2000
   ElKoura G., 2003, ACM SIGGRAPH/Eurographics Symposium on Computer Animation, P110
   Frank K., 2007, IASI YB, V2007, P27
   Funder D.C., 1997, PERSONALITY PUZZLE
   Giles H., 1994, HDB INTERPERSONAL CO, V2nd, P103
   GOLDBERG LR, 1990, J PERS SOC PSYCHOL, V59, P1216, DOI 10.1037/0022-3514.59.6.1216
   Gosling SD, 2003, J RES PERS, V37, P504, DOI 10.1016/S0092-6566(03)00046-1
   Grin W.B., 2000, ASME IMECE 2000 Symposium on Haptic Interfaces for Virtual Environments and Teleoperator Systems, P1
   Hartmann B, 2002, COMP ANIM CONF PROC, P111, DOI 10.1109/CA.2002.1017516
   Hartmann B, 2006, LECT NOTES ARTIF INT, V3881, P188
   Heloir A, 2009, LECT NOTES ARTIF INT, V5773, P393, DOI 10.1007/978-3-642-04380-2_43
   Hoyet L., 2012, P ACM SIGGRAPH S INT, P79, DOI DOI 10.1145/2159616.2159630.6,17
   Hu C, 2015, LECT NOTES ARTIF INT, V9238, P181, DOI 10.1007/978-3-319-21996-7_19
   Hu HY, 2004, IEEE INT CONF ROBOT, P4571
   Huenerfauth M., 2010, ACM T ACCESSIBLE COM, V3, P2
   Isbister K, 2000, INT J HUM-COMPUT ST, V53, P251, DOI 10.1006/ijhc.2000.0368
   Jörg S, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366208
   Kahlesz F, 2004, COMPUTER GRAPHICS INTERNATIONAL, PROCEEDINGS, P403, DOI 10.1109/CGI.2004.1309241
   Kang Chris, 2012, Motion in Games. 5th International Conference (MIG 2012). Proceedings, P244, DOI 10.1007/978-3-642-34710-8_23
   Kessler GD., 1995, ACM Transactions on Computer-Human Interaction, V2, P263
   Knapp M.L., 1978, NONVERBAL COMMUNICAT
   Kopp S, 2004, COMPUT ANIMAT VIRT W, V15, P39, DOI 10.1002/cav.6
   KUCH JJ, 1994, CONF REC ASILOMAR C, P1252, DOI 10.1109/ACSSC.1994.471659
   LEE JT, 1995, IEEE COMPUT GRAPH, V15, P77, DOI 10.1109/38.403831
   Lippa R, 1998, J RES PERS, V32, P80, DOI 10.1006/jrpe.1997.2189
   Liu C.K., 2008, ACM SIGGRAPH EUR S C, P163
   Liu CK, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531365
   Liu K., 2015, IEEE T AFFECT COMPUT, P99
   Mairesse F, 2011, COMPUT LINGUIST, V37, P455, DOI 10.1162/COLI_a_00063
   Mairesse F, 2010, USER MODEL USER-ADAP, V20, P227, DOI 10.1007/s11257-010-9076-2
   Mairesse Francois, 2008, ACL, P165
   McQuiggan SW, 2008, USER MODEL USER-ADAP, V18, P81, DOI 10.1007/s11257-007-9040-y
   Mehl MR, 2006, J PERS SOC PSYCHOL, V90, P862, DOI 10.1037/0022-3514.90.5.862
   MEHRABIAN A, 1969, PSYCHOL BULL, V71, P359, DOI 10.1037/h0027349
   Menon A., 2003, PROC INT C COMP GRAP, P1
   Neff Michael, 2011, Intelligent Virtual Agents. Proceedings 11th International Conference, IVA 2011, P398, DOI 10.1007/978-3-642-23974-8_43
   Neff M, 2010, LECT NOTES ARTIF INT, V6356, P222, DOI 10.1007/978-3-642-15892-6_24
   Neff M, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1330511.1330516
   NORMAN WT, 1963, J ABNORM PSYCHOL, V66, P574, DOI 10.1037/h0040291
   North Marion., 1972, Personality Assessment through Movement
   Pavlovic VI, 1997, IEEE T PATTERN ANAL, V19, P677, DOI 10.1109/34.598226
   Pennebaker JW, 1999, J PERS SOC PSYCHOL, V77, P1296, DOI 10.1037/0022-3514.77.6.1296
   Piwek P., 2003, Proceedings of the tenth conference on European chapter of the Association for Computational Linguistics-Volume 2, V2, P151
   RIGGIO RE, 1986, J PERS SOC PSYCHOL, V50, P421, DOI 10.1037/0022-3514.50.2.421
   Ruhland Kerstin, 2015, P ACM SIGGRAPH S APP, P19, DOI DOI 10.1145/2804408.28044241,2
   Samadani Ali-Akbar, 2011, 2011 RO-MAN: The 20th IEEE International Symposium on Robot and Human Interactive Communication, P93, DOI 10.1109/ROMAN.2011.6005276
   Samadani AA, 2013, INT J SOC ROBOT, V5, P35, DOI 10.1007/s12369-012-0169-4
   Steffen J, 2011, LECT NOTES ARTIF INT, V7102, P34, DOI 10.1007/978-3-642-25489-5_4
   Takala M., 1953, STUDIES PSYCHOMOTOR, V1
   Thiebaux Marcus., 2008, P AAMAS 08, P151
   Turner M. L., 2001, THESIS CITESEER
   Wang N, 2005, FRONT ARTIF INTEL AP, V125, P686
   Wang YG, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2462000
   Wang YG, 2013, KEY ENG MATER, V567, P15, DOI 10.4028/www.scientific.net/KEM.567.15
   WHEATLAND N., 2013, P MOTION GAMES, P175
   Yazadi F., 2009, CYBERGLOVE SYSTEMS C
   Ye YT, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185537
   Zhao WP, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508412
   Zhao Wenping, 2012, Proceedings of the ACM SIGGRAPH/eurographics symposium on computer animation. Eurographics Association, P33, DOI [10.2312/SCA/SCA12/033-042, DOI 10.2312/SCA/SCA12/033-042]
   Zhu YF, 2013, COMPUT ANIMAT VIRT W, V24, P445, DOI 10.1002/cav.1477
   Zibrek K., 2014, P ACM S APPL PERCEPT, P111, DOI DOI 10.1145/2628257.2628270
NR 69
TC 16
Z9 17
U1 0
U2 20
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD MAR
PY 2016
VL 13
IS 2
AR 9
DI 10.1145/2874357
PG 23
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DJ0OM
UT WOS:000373903800004
OA Green Published
DA 2024-07-18
ER

PT J
AU Stransky, D
   Wilcox, LM
   Allison, RS
AF Stransky, Debi
   Wilcox, Laurie M.
   Allison, Robert S.
TI Effects of Long-Term Exposure on Sensitivity and Comfort with
   Stereoscopic Displays
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Experimentation; Human Factors; Stereoscopic 3D; S3D; stereopsis; stereo
   displays; depth discrimination
ID PANUMS FUSIONAL AREA; VISUAL PERFORMANCE; STEREOPSIS; ACCOMMODATION
AB Stereoscopic 3Dmedia has recently increased in appreciation and availability. This popularity has led to concerns over the health effects of habitual viewing of stereoscopic 3D content; concerns that are largely hypothetical. Here we examine the effects of repeated, long-term exposure to stereoscopic 3D in the workplace on several measures of stereoscopic sensitivity (discrimination, depthmatching, and fusion limits) along with reported negative symptoms associated with viewing stereoscopic 3D. We recruited a group of adult stereoscopic 3D industry experts and compared their performance with observers who were (i) inexperienced with stereoscopic 3D, (ii) researchers who study stereopsis, and (iii) vision researchers with little or no experimental stereoscopic experience. Unexpectedly, we found very little difference between the four groups on all but the depth discrimination task, and the differences that did occur appear to reflect task-specific training or experience. Thus, we found no positive or negative consequences of repeated and extended exposure to stereoscopic 3D in these populations.
C1 [Stransky, Debi; Wilcox, Laurie M.; Allison, Robert S.] York Univ, N York, ON M3J 1P3, Canada.
C3 York University - Canada
RP Stransky, D (corresponding author), 4700 Keele St, N York, ON M3J 1P3, Canada.
EM debis@yorku.ca
OI Allison, Robert/0000-0002-4485-2665; Wilcox, Laurie/0000-0002-3594-6192
FU Natural Sciences and Engineering Research Council of Canada (NSERC);
   Ontario Media Development Corporation; Ontario Centres of Excellence
   funding as part of the 3D Film Innovation Consortium
FX This study was funded by the Natural Sciences and Engineering Research
   Council of Canada (NSERC) to D.S. and L.W. and by the Ontario Media
   Development Corporation and the Ontario Centres of Excellence funding to
   R.A. and L.W. as part of the 3D Film Innovation Consortium.
CR Allison RS, 2007, J IMAGING SCI TECHN, V51, P317, DOI 10.2352/J.ImagingSci.Technol.(2007)51:4(317)
   American Optometric Association, 2013, 3D VIS EYE HLTH
   BASHINSKI HS, 1980, PERCEPT PSYCHOPHYS, V28, P241, DOI 10.3758/BF03204380
   Cohen J., 1988, STAT POWER ANAL BEHA
   CRONE RA, 1973, A GRAEF ARCH KLIN EX, V188, P1, DOI 10.1007/BF00410860
   DINER DB, 1987, J OPT SOC AM A, V4, P1814, DOI 10.1364/JOSAA.4.001814
   Ding J, 2011, P NATL ACAD SCI USA, V108, pE733, DOI 10.1073/pnas.1105183108
   FENDER D, 1967, J OPT SOC AM, V57, P819, DOI 10.1364/JOSA.57.000819
   FENDICK M, 1983, VISION RES, V23, P145, DOI 10.1016/0042-6989(83)90137-2
   FOLEY JM, 1974, AM J OPTOM PHYS OPT, V51, P935
   Gantz L, 2007, VISION RES, V47, P2170, DOI 10.1016/j.visres.2007.04.014
   GLASS GV, 1966, EDUC PSYCHOL MEAS, V26, P623, DOI 10.1177/001316446602600307
   HAMPTON DR, 1983, J OPT SOC AM, V73, P7, DOI 10.1364/JOSA.73.000007
   HECKMANN T, 1989, PERCEPT PSYCHOPHYS, V45, P297, DOI 10.3758/BF03204944
   Held RT, 2008, APGV 2008: PROCEEDINGS OF THE SYMPOSIUM ON APPLIED PERCEPTION IN GRAPHICS AND VISUALIZATION, P23
   Hess RF, 2010, OPTOMETRY VISION SCI, V87, P697, DOI 10.1097/OPX.0b013e3181ea18e9
   Hoffman DM, 2008, J VISION, V8, DOI 10.1167/8.3.33
   Howard IP., 2002, SEEING DEPTH
   JUDGE SJ, 1985, PERCEPTION, V14, P617, DOI 10.1068/p140617
   Kooi FL, 2004, DISPLAYS, V25, P99, DOI 10.1016/j.displa.2004.07.004
   Lambooij M, 2011, IEEE T CIRC SYST VID, V21, P1913, DOI 10.1109/TCSVT.2011.2157193
   Lambooij M, 2009, J IMAGING SCI TECHN, V53, DOI 10.2352/J.ImagingSci.Technol.2009.53.3.030201
   Mckee SP, 2010, J VISION, V10, DOI 10.1167/10.10.5
   Nintendo, 2013, HLTH SAF PREC
   Ogle K.N., 1964, Researches in Binocular Vision
   OGLE KN, 1958, ARCH OPHTHALMOL-CHIC, V59, P4
   OTOOLE AJ, 1992, PERCEPTION, V21, P227, DOI 10.1068/p210227
   Percival A.S., 1920, PRESCRIBING SPECTACL
   RICHARDS W, 1970, EXP BRAIN RES, V10, P380, DOI 10.1007/BF02324765
   Samsung, 2013, VIEW TV US 3D FUNCT
   Schmitt C, 2002, GRAEF ARCH CLIN EXP, V240, P704, DOI 10.1007/s00417-002-0458-y
   SCHOR C, 1989, VISION RES, V29, P837, DOI 10.1016/0042-6989(89)90095-3
   SCHOR C, 1984, VISION RES, V24, P661, DOI 10.1016/0042-6989(84)90207-4
   SCHOR CM, 1983, VISION RES, V23, P1649, DOI 10.1016/0042-6989(83)90179-7
   Schwarz N., 2009, PROCESSING MED INFOR, P99
   Shibata T, 2011, J VISION, V11, DOI 10.1167/11.8.11
   Sowden P, 1996, PERCEPTION, V25, P1043, DOI 10.1068/p251043
   Ukai K, 2008, DISPLAYS, V29, P106, DOI 10.1016/j.displa.2007.09.004
   Wann J. P., 1997, Computer Graphics, V31, P53, DOI 10.1145/271283.271307
   Yang SN, 2012, OPTOMETRY VISION SCI, V89, P1068, DOI 10.1097/OPX.0b013e31825da430
NR 40
TC 4
Z9 4
U1 0
U2 15
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD APR
PY 2014
VL 11
IS 1
AR 2
DI 10.1145/2536810
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AG7CF
UT WOS:000335574900002
DA 2024-07-18
ER

PT J
AU Rosenholtz, R
   Dorai, A
   Freeman, R
AF Rosenholtz, Ruth
   Dorai, Amal
   Freeman, Rosalind
TI Do Predictions of Visual Perception Aid Design?
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Design; Experimentation; Human Factors; Visual perception; saliency;
   clutter; design tool; field study; human-computer interaction
ID ASYMMETRIES; ATTENTION; SEARCH; MODEL
AB Understanding and exploiting the abilities of the human visual system is an important part of the design of usable user interfaces and information visualizations. Designers traditionally learn qualitative rules of thumb for how to enable quick, easy, and veridical perception of their design. More recently, work in human and computer vision has produced more quantitative models of human perception, which take as input arbitrary, complex images of a design. In this article, we ask whether models of perception aid the design process, using our tool DesignEye as a working example of a perceptual tool incorporating such models. Through a series of interactions with designers and design teams, we find that the models can help, but in somewhat unexpected ways. DesignEye was capable of facilitating A/B comparisons between designs, and judgments about the quality of a design. However, overall "goodness" values were not very useful, showed signs of interfering with a natural process of trading off perceptual vs. other design issues, and would likely interfere with acceptance of a perceptual tool by professional designers. Perhaps most surprisingly, DesignEye, by providing in essence a simple visualization of the design, seemed to facilitate communication about not only perceptual aspects of design, but also about design goals and how to achieve those goals. We discuss resulting design principles for making perceptual tools useful in general.
C1 [Rosenholtz, Ruth] MIT, Cambridge, MA 02139 USA.
   [Dorai, Amal] Stanford Univ, Stanford, CA 94305 USA.
   [Freeman, Rosalind] Skidmore Coll, Saratoga Springs, NY USA.
C3 Massachusetts Institute of Technology (MIT); Stanford University;
   Skidmore College
RP Rosenholtz, R (corresponding author), MIT, Cambridge, MA 02139 USA.
EM rruth@mit.edu
OI Rosenholtz, Ruth/0000-0001-5299-0331
FU NSF [BCS-0518157]
FX Funded by NSF Grant BCS-0518157.
CR Aaron A., 2004, P 9 INT C INTELLIGEN, P214
   Adamczyk PD, 2007, CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1 AND 2, P1073
   [Anonymous], CULTURES COMPUTING
   Anstis S, 1998, PERCEPTION, V27, P817, DOI 10.1068/p270817
   BALAS BJ, J VISION, V9, P1
   Bodker S., 2000, Proceedings of the conference on Designing interactive systems processes, practices, methods, and techniques - DIS '00, DOI [10.1145/347642.347757, DOI 10.1145/347642.347757]
   Bravo MJ, 2008, J VISION, V8, DOI 10.1167/8.1.23
   Bruce NDB, 2009, J VISION, V9, DOI 10.1167/9.3.5
   Bucciarelli L.L., 1988, DESIGN STUD, V9, P159, DOI [DOI 10.1016/0142-694X(88)90045-2, 10.1016/0142-694X(88)90045-2]
   CLEVELAND W, 1980, ELEMENTS GRAPHING DA
   Hammond T, 2005, COMPUT GRAPH-UK, V29, P518, DOI 10.1016/j.cag.2005.05.005
   Hendry D.G., 2004, 2004 conference on Designing interactive systems: processes, practices, methods, and techniques, P123
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   KOSSLYN SM, 1989, APPL COGNITIVE PSYCH, V3, P185, DOI 10.1002/acp.2350030302
   Landay JA, 2001, COMPUTER, V34, P56, DOI 10.1109/2.910894
   Li ZP, 2002, TRENDS COGN SCI, V6, P9, DOI 10.1016/S1364-6613(00)01817-9
   Lohrenz M. C., 2008, J MANAGEMENT ENG INT, V1, P83
   Mackinlay J., 1988, Proceedings of the ACM SIGGRAPH Symposium on User Interface Software, P179, DOI 10.1145/62402.62431
   Newman MW, 2003, HUM-COMPUT INTERACT, V18, P259, DOI 10.1207/S15327051HCI1803_3
   Paris S, 2008, LECT NOTES COMPUT SC, V5303, P460, DOI 10.1007/978-3-540-88688-4_34
   Rosenholtz R, 2004, J VISION, V4, P224, DOI 10.1167/4.3.9
   Rosenholtz R, 1999, VISION RES, V39, P3157, DOI 10.1016/S0042-6989(99)00077-2
   Rosenholtz R, 2001, J EXP PSYCHOL HUMAN, V27, P985, DOI 10.1037//0096-1523.27.4.985
   Rosenholtz R, 2001, PERCEPT PSYCHOPHYS, V63, P476, DOI 10.3758/BF03194414
   Rosenholtz R., 2005, Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, CHI '05, P761
   ROSENHOLTZ R, 2005, J VISION, V5, pA777, DOI DOI 10.1167/5.8.777
   Rosenholtz R, 2007, J VISION, V7, DOI 10.1167/7.2.17
   Rosenholtz R, 2009, CHI2009: PROCEEDINGS OF THE 27TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1-4, P1331
   Shi JB, 1997, PROC CVPR IEEE, P731, DOI 10.1109/CVPR.1997.609407
   Sinha Anoop K, 2001, P 2001 WORKSHOP PERC, P1, DOI [10.1145/971478.971501, DOI 10.1145/971478.971501]
   Sonnenwald D. H., 1996, Design Studies, V17, P277, DOI 10.1016/0142-694X(96)00002-6
   STAGGERS N, 1993, INT J MAN MACH STUD, V39, P775, DOI 10.1006/imms.1993.1083
   STAR SL, 1989, READINGS DISTRIBUTED, V3
   Torralba A, 2006, PSYCHOL REV, V113, P766, DOI 10.1037/0033-295X.113.4.766
   Tufte ER, 1990, Envisioning Information
   van den Berg R, 2009, J VISION, V9, DOI 10.1167/9.4.24
   Wagner I, 2000, COLLABORATIVE DESIGN, P379
   Ware C., 2020, INFORM VISUALIZATION
   Wattenberg M., 2004, Information Visualization, V3, P123, DOI 10.1057/palgrave.ivs.9500070
   Wharton C., 1994, Usability inspection methods, P105
   Wolfe J. M., 2007, INTEGRATED MODELS CO, P99, DOI [10.1093/acprof:oso/9780195189193.003.0008, DOI 10.1093/ACPROF:OSO/9780195189193.003.0008]
NR 41
TC 48
Z9 58
U1 1
U2 14
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD JAN
PY 2011
VL 8
IS 2
AR 12
DI 10.1145/1870076.1870080
PG 20
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA 748AR
UT WOS:000289362900004
DA 2024-07-18
ER

PT J
AU Hassaine, D
   Holliman, NS
   Liversedge, SP
AF Hassaine, Djamel
   Holliman, Nicolas S.
   Liversedge, Simon P.
TI Investigating the Performance of Path-Searching Tasks in Depth on
   Multiview Displays
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Graphics; Experimentation; Human Factors; Stereopsis; motion parallax;
   multiview displays; viewpoint density
ID CUE COMBINATION; MOTION PARALLAX; BINOCULAR DISPARITY; PERCEPTION;
   TEXTURE; SHAPE; STEREOPSIS; FUSION
AB Multiview auto-stereoscopic displays support both stereopsis and head motion parallax depth cues and could be superior for certain tasks. Previous work suggests that a high viewpoint density (100 views/10cm at the eye) is required to convincingly support motion parallax. However, it remains unclear how viewpoint density affects task performance, and this factor is critical in determining display and system design requirements. Therefore, we present a simulated multiview display apparatus to undertake experiments using a path-searching task in which we control two independent variables: the stereoscopic depth and the viewpoint density. In the first experiment, we varied both cues and found that even small amounts of stereo depth (2cm) reliably improved task accuracy and reduced latency, whereas there was no evidence of dependence on viewpoint density. In the second experiment, we switched off the stereoscopic cue and varied viewpoint density alone. We found that for these monoscopic images increasing viewpoint density resulted in some reduction in response latency (up to eight views/10cm) but had no effect on accuracy. We conclude for cases where occlusion is not an overriding factor that low viewpoint densities may be sufficient to enable effective path-searching task performance.
C1 [Hassaine, Djamel; Holliman, Nicolas S.] Univ Durham, Dept Comp Sci, Durham DH1 3LE, England.
   [Liversedge, Simon P.] Univ Southampton, Sch Psychol, Southampton SO9 5NH, Hants, England.
C3 Durham University; University of Southampton
RP Hassaine, D (corresponding author), Univ Durham, Dept Comp Sci, Durham DH1 3LE, England.
EM djamel.hassaine@dunelm.ac.uk; n.s.holliman@durham.ac.uk;
   s.p.liversedge@soton.ac.uk
FU department of Computer Science; department of Psychology; MRC [G0601757]
   Funding Source: UKRI
FX This work was supported by the department of Computer Science grant
   scheme and the subjects were paid with monies from the department of
   Psychology.
CR Bradshaw MF, 2000, VISION RES, V40, P3725, DOI 10.1016/S0042-6989(00)00214-5
   BRAUNSTEIN ML, 1968, J EXP PSYCHOL, V78, P247, DOI 10.1037/h0026269
   BRUNO N, 1988, J EXP PSYCHOL GEN, V117, P161, DOI 10.1037/0096-3445.117.2.161
   DOSHER BA, 1986, VISION RES, V26, P973, DOI 10.1016/0042-6989(86)90154-9
   Gilson SJ, 2006, J NEUROSCI METH, V154, P175, DOI 10.1016/j.jneumeth.2005.12.013
   Glennerster A, 1996, VISION RES, V36, P3441, DOI 10.1016/0042-6989(96)00090-9
   Hodges L.F., 1993, STEREO COMPUTER GRAP, P71
   Hsu J, 1996, IEEE T SYST MAN CY A, V26, P810, DOI 10.1109/3468.541339
   JOHNSTON EB, 1991, VISION RES, V31, P1351, DOI 10.1016/0042-6989(91)90056-B
   JOHNSTON EB, 1994, VISION RES, V34, P2259, DOI 10.1016/0042-6989(94)90106-6
   KOENDERINK JJ, 1991, J OPT SOC AM A, V8, P377, DOI 10.1364/JOSAA.8.000377
   LANDY MS, 1995, VISION RES, V35, P389, DOI 10.1016/0042-6989(94)00176-M
   Landy MS, 2001, J OPT SOC AM A, V18, P2307, DOI 10.1364/JOSAA.18.002307
   MCALLISTER D.F., 1993, STEREO COMPUTER GRAP
   MONWILLIAMS M, 1993, OPHTHAL PHYSL OPT, V13, P387, DOI 10.1111/j.1475-1313.1993.tb00496.x
   OGLE KN, 1953, AMA ARCH OPHTHALMOL, V49, P313
   ONO ME, 1986, J EXP PSYCHOL HUMAN, V12, P331, DOI 10.1037/0096-1523.12.3.331
   Oruç I, 2003, VISION RES, V43, P2451, DOI 10.1016/S0042-6989(03)00435-8
   Pastoor S., 1989, Proceedings of the S.I.D., V30, P217
   ROGERS B, 1979, PERCEPTION, V8, P125, DOI 10.1068/p080125
   Runde D, 2000, IEEE T CIRC SYST VID, V10, P376, DOI 10.1109/76.836282
   SHARMA K, 1992, AM J OPHTHALMOL, V114, P636, DOI 10.1016/S0002-9394(14)74499-X
   SOLLENBERGER RL, 1993, HUM FACTORS, V35, P483, DOI 10.1177/001872089303500306
   Takeda T, 1999, VISION RES, V39, P2087, DOI 10.1016/S0042-6989(98)00258-2
   TITTLE JS, 1993, PERCEPT PSYCHOPHYS, V54, P157, DOI 10.3758/BF03211751
   VALYUS NA, 1966, STEREOSCOPY
   Ware C, 1996, ACM T GRAPHIC, V15, P121, DOI 10.1145/234972.234975
   Ware C., 2005, Proceedings of the 2nd Symposium on Applied Perception in Graphics and Visualization (A Corona, Spain, August 26 - 28, V95, P51, DOI DOI 10.1145/1080402.1080411
   YEH YY, 1990, HUM FACTORS, V32, P45, DOI 10.1177/001872089003200104
   YOUNG MJ, 1993, VISION RES, V33, P2685, DOI 10.1016/0042-6989(93)90228-O
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
NR 44
TC 6
Z9 7
U1 0
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD OCT
PY 2010
VL 8
IS 1
AR 8
DI 10.1145/1857893.1857901
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA 748AJ
UT WOS:000289362100008
DA 2024-07-18
ER

PT J
AU Watters, P
   Martin, F
   Stripf, HS
AF Watters, Paul
   Martin, Frances
   Stripf, H. Steffen
TI Visual Detection of LSB-Encoded Natural Image Steganography
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Security; Experimentation; Human Factors; Steganography;
   counterterrorism
AB Many steganographic systems embed hidden messages inside the least significant bit layers of colour natural images. The presence of these messages can be difficult to detect by using statistical steganalysis. However, visual steganalysis by humans may be more successful in natural image discrimination. This study examined whether humans could detect least-significant bit steganography in 15 color natural images from the VisTex database using a controlled same/different task (N = 58) and a yes/no task (N = 61). While d' > 1 was observed for color layers 4-8, layers 1-3 had d' < 1 in both experiments. Thus, layers 1-3 appear to be highly resistant to visual steganalysis.
C1 [Watters, Paul] MRC, Unit Lifelong Hlth & Ageing, London WC1E 6BT, England.
   [Martin, Frances] Univ Tasmania, Sch Psychol, Sandy Bay, Tas 7005, Australia.
   [Stripf, H. Steffen] Macquarie Univ, Postgrad Profess Dev Program, Div ICS, N Ryde, NSW 2109, Australia.
C3 University of Tasmania; Macquarie University
RP Watters, P (corresponding author), MRC, Unit Lifelong Hlth & Ageing, 1-19 Torrington Pl, London WC1E 6BT, England.
OI Martin, Frances/0000-0002-4917-105X; Watters, Paul/0000-0002-1399-7175
CR [Anonymous], 2000, Digital Watermarking
   Barlow H. B., 1961, P331
   BROPHY AL, 1986, BEHAV RES METH INSTR, V18, P285, DOI 10.3758/BF03204400
   FARELL B, 1985, PSYCHOL BULL, V98, P419, DOI 10.1037/0033-2909.98.3.419
   FIELD DJ, 1987, J OPT SOC AM A, V4, P2379, DOI 10.1364/JOSAA.4.002379
   Green D., 1966, SIGNAL DETECTION THE
   KELLY J, 2001, US TODAY        0205
   LEE Y, 1999, P IEE VIS IM SIGN PR
   Lee YK, 2003, INT J PATTERN RECOGN, V17, P967, DOI 10.1142/S021800140300268X
   MAES M, 1998, TWIN PEAKS HISTOGRAM
   Marvel LM, 1999, IEEE T IMAGE PROCESS, V8, P1075, DOI 10.1109/83.777088
   Olshausen BA, 2004, CURR OPIN NEUROBIOL, V14, P481, DOI 10.1016/j.conb.2004.07.007
   Petitcolas FAP, 1999, P IEEE, V87, P1062, DOI 10.1109/5.771065
   Provos N, 2001, 0111 CITI, V1001, P48103
   Rescorla E., 2001, SSL TLS DESIGNING BU
   RINNE T, 2000, PGMSTEALTH COMPUTER
   ROSENBAUM R, 2000, THESIS ROSTOCK U GER
   STRIPF S, 2004, STEGOKIT STEGA UNPUB
   Waldman M., 2000, P 9 USENIX SEC S DEN
   Watters PA, 2003, INT J PATTERN RECOGN, V17, P1431, DOI 10.1142/S0218001403002940
   Watters Paul A, 2003, J Integr Neurosci, V2, P249, DOI 10.1142/S0219635203000251
   Wayner Peter., 2002, DISAPPEARING CRYPTOG
   WESTFELD A., 2001, P 4 INT WORKSH INF H, V2137, P289, DOI DOI 10.1007/3-540-45496-9_21
NR 23
TC 4
Z9 5
U1 0
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD JAN
PY 2008
VL 5
IS 1
AR 5
DI 10.1145/1328775
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 450YI
UT WOS:000266437500005
DA 2024-07-18
ER

PT J
AU Miyashita, Y
   Sawahata, Y
   Sakai, A
   Harasawa, M
   Hara, K
   Morita, T
   Komine, K
AF Miyashita, Yamato
   Sawahata, Yasuhito
   Sakai, Akihiro
   Harasawa, Masamitsu
   Hara, Kazuhiro
   Morita, Toshiya
   Komine, Kazuteru
TI Display-Size Dependent Effects of 3D Viewing on Subjective Impressions
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Binocular disparities; motion parallax; impression; 3D display
ID RESOLUTION; DISTANCE; MOTION
AB This paper describes how the screen size of 3D displays affect the subjective impressions of 3D-visualized content. The key requirement for 3D displays is the presentation of depth cues comprising binocular disparities and/or motion parallax; however, the development of displays and production of content that include these cues leads to an increase in costs. Given the variety of screen sizes, it is expected that 3D characteristics are experienced differently by viewers depending on the screen size. We asked 48 participants to evaluate the 3D experience when using three different-sized stereoscopic displays (11.5, 55, and 200 inches) with head trackers. The participants were asked to score presented stimuli on 20 opposite-term pairs based on the semantic differential method after viewing each of six stimuli. Using factor analysis, we extracted three principal factors: power, related to strong three-dimensionality, real, etc.; visibility, related to stable, natural, etc.; and space, related to agile, open, etc., which had proportions of variances of 0.317, 0.277, and 0.251, respectively; their cumulation was 0.844. We confirmed that the three different-sized displays did not produce the same subjective impressions of the 3D characteristics. In particular, on the small-sized display, we found larger effects on power and space impressions from motion parallax (eta(2) = 0.133 and 0.161, respectively) than for the other two sizes. We found degradation of the visibility impressions from binocular disparities, which might be caused by artifacts from stereoscopy. The effects of 3D viewing on subjective impression depends on the display size, and small-sized displays offer the largest benefits by adding 3D characteristics to 2D visualization.
C1 [Miyashita, Yamato; Sawahata, Yasuhito; Sakai, Akihiro; Harasawa, Masamitsu; Hara, Kazuhiro; Komine, Kazuteru] NHK Japan Broadcasting Corp, Setagaya Ku, 1-10-11 Kinuta, Tokyo 1578510, Japan.
   [Morita, Toshiya] NHK Engn Syst, Setagaya Ku, 1-10-11 Kinuta, Tokyo 1578540, Japan.
C3 NHK Japan Broadcasting Corp; NHK Japan Broadcasting Corp
RP Miyashita, Y (corresponding author), NHK Japan Broadcasting Corp, Setagaya Ku, 1-10-11 Kinuta, Tokyo 1578510, Japan.
EM miyashita.y-fc@nhk.or.jp; sawahata.y-jq@nhk.or.jp; sakai.a-je@nhk.or.jp;
   harasawa.m-ii@nhk.or.jp; hara.kmg@nhk.or.jp; morita.toshiya@nes.or.jp;
   komine.k-cy@nhk.or.jp
OI Harasawa, Masamitsu/0000-0003-3297-4275; Sawahata,
   Yasuhito/0000-0002-2212-8780; Morita, Toshiya/0000-0003-3452-0429;
   Miyashita, Yamato/0000-0003-4944-924X; Komine,
   Kazuteru/0000-0002-0059-4326
CR ADELSON EH, 1985, J OPT SOC AM A, V2, P284, DOI 10.1364/JOSAA.2.000284
   [Anonymous], 2011, ACM SIGGRAPH ASIA 20, DOI DOI 10.1145/2077434.2077447
   [Anonymous], 2021, RDOCUMENTATION PSYCH
   [Anonymous], 2021, RULES THUMB MAGNITUD
   ARDITO M, 1994, SMPTE J, V103, P517, DOI 10.5594/J06461
   Cohen J., 1988, Statistical power analyses for behavioral sciences, V2nd, DOI [10.4324/9780203771587, DOI 10.4324/9780203771587]
   Cutting JE, 1995, PERCEPTION SPACE MOT, P69, DOI [DOI 10.1016/B978-012240530-3/50005-5, 10.1016/B978-012240530-3/50005-5]
   DeFanti TA, 1993, Proceedings of the 20th annual conference on Computer graphics and interactive techniques, P135, DOI 10.1145/166117.166134.
   Fafard D, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300763
   Guttman L., 1954, PSYCHOMETRIKA, V19, P149, DOI [10.1007/BF02289162, DOI 10.1007/BF02289162]
   Harasawa Masamitsu, 2017, 5 VISUAL SCI ART C, P337, DOI [10.1163/22134913-00002099, DOI 10.1163/22134913-00002099]
   Hoshino H, 1998, J OPT SOC AM A, V15, P2059, DOI 10.1364/JOSAA.15.002059
   Hubona G. S., 1999, ACM Transactions on Computer-Human Interaction, V6, P214, DOI 10.1145/329693.329695
   Ijsselsteijn W, 2001, PRESENCE-TELEOP VIRT, V10, P298, DOI 10.1162/105474601300343621
   KAISER HF, 1960, EDUC PSYCHOL MEAS, V20, P141, DOI 10.1177/001316446002000116
   Kellnhofer P, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980230
   Kellnhofer P, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925866
   Kongsilp S, 2017, DISPLAYS, V49, P72, DOI 10.1016/j.displa.2017.07.001
   Kulshreshth A, 2016, 34TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, CHI 2016, P177, DOI 10.1145/2858036.2858078
   Lambooij M, 2009, J IMAGING SCI TECHN, V53, DOI 10.2352/J.ImagingSci.Technol.2009.53.3.030201
   Lang M, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778812
   LUND AM, 1993, SMPTE J, V102, P406, DOI 10.5594/J15915
   McIntire JP, 2012, PROC SPIE, V8383, DOI 10.1117/12.920017
   Miyashita Y, 2019, SIGGRAPH '19 -ACM SIGGRAPH 2019 TALKS, DOI 10.1145/3306307.3328193
   Nagata S., 1991, PICTORIAL COMMUNICAT, P527
   Neubauer AC, 2010, INTELLIGENCE, V38, P529, DOI 10.1016/j.intell.2010.06.001
   Sawahata Y, 2021, IEEE T BROADCAST, V67, P473, DOI 10.1109/TBC.2020.3047218
   Sawahata Y, 2018, IEEE T BROADCAST, V64, P488, DOI 10.1109/TBC.2017.2786022
   Schild Jonas., 2012, Proceedings of the 2012 Association for Computing Machinery annual conference on Human Factors in Computing Systems, P89, DOI DOI 10.1145/2207676.2207690
   Seuntiens Pieter J. H., 2005, Proceedings of the SPIE - The International Society for Optical Engineering, V6016, P1, DOI 10.1117/12.627515
   Shibata T, 2011, J VISION, V11, DOI 10.1167/11.8.11
   Siegel M, 2000, IEEE T CIRC SYST VID, V10, P387, DOI 10.1109/76.836283
   SOLLENBERGER RL, 1993, HUM FACTORS, V35, P483, DOI 10.1177/001872089303500306
   Sutherland IE., 1968, Assoc. Comput. Machinery, V68, P757, DOI [DOI 10.1145/1476589.1476686, 10.1145/1476589.1476686, 10.1145/1476589.1476686.2.2.1]
   Terzic K, 2016, SIGNAL PROCESS-IMAGE, V47, P402, DOI 10.1016/j.image.2016.08.002
   Tsushima Y, 2016, FRONT PSYCHOL, V7, DOI 10.3389/fpsyg.2016.00242
   Tsushima Y, 2014, SCI REP-UK, V4, DOI 10.1038/srep06687
   van Beurden MHPH, 2011, PROC SPIE, V7863, DOI 10.1117/12.872566
   Vorderer P., 2004, REPORT EUROPEAN COMM
   WARE C, 1993, HUMAN FACTORS IN COMPUTING SYSTEMS, P37
   Witmer BG, 2005, PRESENCE-TELEOP VIRT, V14, P298, DOI 10.1162/105474605323384654
   Yang R, 2008, IEEE T VIS COMPUT GR, V14, P84, DOI 10.1109/70410
   Zhou Q, 2020, PROCEEDINGS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'20), DOI 10.1145/3313831.3376601
NR 43
TC 0
Z9 0
U1 0
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD APR
PY 2022
VL 19
IS 2
AR 5
DI 10.1145/3510461
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 3A7CY
UT WOS:000827414800001
OA Bronze
DA 2024-07-18
ER

PT J
AU Seifi, H
   Chun, M
   MacLean, KE
AF Seifi, Hasti
   Chun, Mattew
   MacLean, Karon E.
TI Toward Affective Handles for Tuning Vibrations
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Affective haptics; design and personalization tools; end-user
   perception; emotion dimensions
ID HAPTIC ICONS
AB When relining or personalizing a design, we count on being able to modify or move an element by changing its parameters rather than creating it anew in a different form or location-a standard utility in graphic and auditory authoring tools. Similarly, we need to tune vibrotactile sensations to fit new use cases, distinguish members of communicative icon sets, and personalize items. For tactile vibration display, however, we lack knowledge of the human perceptual mappings that must underlie such tools. Based on evidence that affective dimensions are a natural way to tune vibrations for practical purposes, we attempted to manipulate perception along three emotion dimensions (agitation, liveliness, and strangeness) using engineering parameters of hypothesized relevance. Results from two user studies show that an automatable algorithm can increase a vibration's perceived agitation and liveliness to different degrees via signal energy, while increasing its discontinuity or randomness makes it more strange. These continuous mappings apply across diverse base vibrations; the extent of achievable emotion change varies. These results illustrate the potential for developing vibrotactile emotion controls as efficient tuning for designers and end-users.
C1 [Seifi, Hasti; Chun, Mattew; MacLean, Karon E.] Univ British Columbia, 2366 Main Mall, Vancouver, BC V6T 1Z4, Canada.
C3 University of British Columbia
RP Seifi, H (corresponding author), Univ British Columbia, 2366 Main Mall, Vancouver, BC V6T 1Z4, Canada.
EM seifi@cs.ubc.ca; mchun345@cs.ubc.ca; maclean@cs.ubc.ca
OI MacLean, Karon/0000-0003-2969-4627
CR Adjust and exaggerate facial features, 2016, ADOBE PHOTOSHOP
   Audacity, 2015, AUDACITY SOFTWARE
   Blom JO, 2003, HUM-COMPUT INTERACT, V18, P193, DOI 10.1207/S15327051HCI1803_1
   Brown L.M., 2006, CHI 2006 Extended Abstracts on Human Factors in Computing Systems, P610, DOI DOI 10.1145/1125451.1125578
   Brunet L, 2013, 2013 WORLD HAPTICS CONFERENCE (WHC), P259, DOI 10.1109/WHC.2013.6548418
   Chan A, 2008, INT J HUM-COMPUT ST, V66, P333, DOI 10.1016/j.ijhcs.2007.11.002
   CONOVER WJ, 1981, AM STAT, V35, P124, DOI 10.2307/2683975
   Evening M., 2013, ADOBE PHOTOSHOP LIGH
   Google, 2016, GOOGL PLAY MUS
   Hertenstein MJ, 2011, HANDBOOK OF TOUCH: NEUROSCIENCE, BEHAVIORAL, AND HEALTH PERSPECTIVES, P299
   Hoggan E, 2007, ICMI'07: PROCEEDINGS OF THE NINTH INTERNATIONAL CONFERENCE ON MULTIMODAL INTERFACES, P162
   Israr A, 2014, ACM T APPL PERCEPT, V11, DOI 10.1145/2641570
   Koskinen E, 2008, SPAA'08: PROCEEDINGS OF THE TWENTIETH ANNUAL SYMPOSIUM ON PARALLELISM IN ALGORITHMS AND ARCHITECTURES, P297
   Kwak DH., 2010, SPORT MARKET Q, V19, P217
   Lee J, 2009, WORLD HAPTICS 2009: THIRD JOINT EUROHAPTICS CONFERENCE AND SYMPOSIUM ON HAPTIC INTERFACES FOR VIRTUAL ENVIRONMENT AND TELEOPERATOR SYSTEMS, PROCEEDINGS, P302, DOI 10.1109/WHC.2009.4810816
   Lieberman H, 2006, HUM COM INT, V9, P1
   Mackay W. E., 1991, Human Factors in Computing Systems. Reaching Through Technology. CHI '91. Conference Proceedings, P153, DOI 10.1145/108844.108867
   MacLean Karon E., 2017, HDB MULTIMODAL MULTI
   Marathe S, 2011, 29TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P781
   McGlone F, 2007, CAN J EXP PSYCHOL, V61, P173, DOI 10.1037/cjep2007019
   McGrenere J, 2007, ACM T COMPUT-HUM INT, V14, DOI 10.1145/1229855.1229858
   Nurkka P, 2013, LECT NOTES COMPUT SC, V8120, P384
   O'Sullivan Conor, 2006, ACTIVITY CLASSIFICAT, P145, DOI [10.1007/11821731_14, DOI 10.1007/11821731_14]
   Oh Uran., 2013, P SIGCHI C HUMAN FAC, P1129, DOI DOI 10.1145/2470654.2466145
   Olli Parviainen, 2016, SOUNDTOUCH ALGORITHM
   Paterson Mark., 2007, SENSES TOUCH HAPTICS
   Propellerhead, 2016, FIG GOOGL PLAY STOR
   Ryu J, 2010, IEEE T HAPTICS, V3, P138, DOI 10.1109/ToH.2010.1
   Saul G, 2011, TEI 2011: PROCEEDINGS OF THE FIFTH INTERNATIONAL CONFERENCE ON TANGIBLE EMBEDDED AND EMBODIED INTERACTION, P73
   Schneider OS, 2016, 34TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, CHI 2016, P3248, DOI 10.1145/2858036.2858279
   Schneider OS, 2015, UIST'15: PROCEEDINGS OF THE 28TH ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P21, DOI 10.1145/2807442.2807470
   Schneider OS, 2016, IEEE HAPTICS SYM, P52, DOI 10.1109/HAPTICS.2016.7463155
   Schneider OS, 2014, IEEE HAPTICS SYM, P327, DOI 10.1109/HAPTICS.2014.6775476
   Seifi H, 2015, 2015 IEEE WORLD HAPTICS CONFERENCE (WHC), P254, DOI 10.1109/WHC.2015.7177722
   Seifi H, 2014, IEEE HAPTICS SYM, P251, DOI 10.1109/HAPTICS.2014.6775463
   Seifi Hasti, 2017, EXPLOITING HAPTIC FA
   Snapseed, 2016, SNAPSEED GOOGLE PLAY
   Tam Diane., 2013, Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, P1689, DOI DOI 10.1145/2470654.2466223
   Ternes D, 2008, LECT NOTES COMPUT SC, V5024, P199, DOI 10.1007/978-3-540-69057-3_24
   van Erp J., 2003, P EUROHAPTICS, V2003, P111
   Wobbrock JO, 2011, 29TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P143, DOI 10.1145/1978942.1978963
   Ying Zheng, 2012, 2012 IEEE Haptics Symposium (HAPTICS), P463, DOI 10.1109/HAPTIC.2012.6183832
   Yool Y, 2015, 2015 IEEE WORLD HAPTICS CONFERENCE (WHC), P235, DOI 10.1109/WHC.2015.7177719
   Zhao S., 2014, UIST 2014 - Adjun. Publ. 27th Annu. ACM Symp. User Interface Softw. Technol, P51, DOI DOI 10.1145/2658779.2659109
NR 44
TC 8
Z9 9
U1 0
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD AUG
PY 2018
VL 15
IS 3
AR 22
DI 10.1145/3230645
PG 23
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GY2UV
UT WOS:000448400300008
DA 2024-07-18
ER

PT J
AU Yildiz, ZC
   Bulbul, A
   Capin, T
AF Yildiz, Zeynep Cipiloglu
   Bulbul, Abdullah
   Capin, Tolga
TI A Framework for Applying the Principles of Depth Perception to
   Information Visualization
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Algorithms; Experimentation; Human Factors; Depth perception; depth
   cues; information visualization; cue combination; fuzzy logic
ID MOTION
AB During the visualization of 3D content, using the depth cues selectively to support the design goals and enabling a user to perceive the spatial relationships between the objects are important concerns. In this novel solution, we automate this process by proposing a framework that determines important depth cues for the input scene and the rendering methods to provide these cues. While determining the importance of the cues, we consider the user's tasks and the scene's spatial layout. The importance of each depth cue is calculated using a fuzzy logic-based decision system. Then, suitable rendering methods that provide the important cues are selected by performing a cost-profit analysis on the rendering costs of the methods and their contribution to depth perception. Possible cue conflicts are considered and handled in the system. We also provide formal experimental studies designed for several visualization tasks. A statistical analysis of the experiments verifies the success of our framework.
C1 [Yildiz, Zeynep Cipiloglu] Celal Bayar Univ, Manisa, Turkey.
   [Yildiz, Zeynep Cipiloglu; Bulbul, Abdullah; Capin, Tolga] Bilkent Univ, TR-06800 Ankara, Turkey.
C3 Celal Bayar University; Ihsan Dogramaci Bilkent University
RP Yildiz, ZC (corresponding author), Bilkent Univ, Dept Comp Engn, Bilkent Univ Comp Engn, TR-06800 Ankara, Turkey.
EM zeynep.cipiloglu@cbu.edu.tr; tcapin@cs.bilkent.edu.tr
RI Çapın, Tolga Kurtuluş/G-6172-2018; Yildiz, Zeynep
   Cipiloglu/AAF-6305-2020; Yildiz, Zeynep Cipiloglu/T-3389-2017; Bulbul,
   Abdullah/AAC-6616-2020
OI Yildiz, Zeynep Cipiloglu/0000-0003-4129-591X; Yildiz, Zeynep
   Cipiloglu/0000-0003-4129-591X; Bulbul, Abdullah/0000-0002-2527-2729
FU Scientific and Technical Research Council of Turkey (TUBITAK) [110E029]
FX This work is supported by the Scientific and Technical Research Council
   of Turkey (TUBITAK, project number 110E029). We would also like to thank
   all those who participated in the experiments for this study.
CR Akenine-Moller T., 2008, REAL TIME RENDERING, P147
   Brackstone M, 2000, ERGONOMICS, V43, P528, DOI 10.1080/001401300184396
   Bradshaw MF, 2000, VISION RES, V40, P3725, DOI 10.1016/S0042-6989(00)00214-5
   Bulbul A, 2010, VISUAL COMPUT, V26, P311, DOI 10.1007/s00371-010-0419-0
   Bunnel M., 2004, Dynamic Ambient Occlusion and Indirect Lighting
   Card SK, 1997, IEEE SYMPOSIUM ON INFORMATION VISUALIZATION, PROCEEDINGS, P92, DOI 10.1109/INFVIS.1997.636792
   Cipiloglu Z., 2010, P 7 S APPL PERC GRAP, P141
   Cutting JE, 1995, PERCEPTION SPACE MOT, P69, DOI [DOI 10.1016/B978-012240530-3/50005-5, 10.1016/B978-012240530-3/50005-5]
   Dodgson NA, 2005, COMPUTER, V38, P31, DOI 10.1109/MC.2005.252
   Gershon N, 1997, IEEE COMPUT GRAPH, V17, P66
   Gooch A., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P447, DOI 10.1145/280814.280950
   Haeberli P., 1990, Computer Graphics, V24, P309, DOI 10.1145/97880.97913
   Hoffman DM, 2008, J VISION, V8, DOI 10.1167/8.3.33
   Howard IanP., 2008, SEEING DEPTH
   Hubona G. S., 1999, ACM Transactions on Computer-Human Interaction, V6, P214, DOI 10.1145/329693.329695
   Keim DA, 2002, IEEE T VIS COMPUT GR, V8, P1, DOI 10.1109/2945.981847
   Kersten D, 1996, NATURE, V379, P31, DOI 10.1038/379031a0
   Kersten MA, 2006, IEEE T VIS COMPUT GR, V12, P1117, DOI 10.1109/TVCG.2006.139
   Knill DC, 2005, J VISION, V5, P103, DOI 10.1167/5.2.2
   Luft T, 2006, ACM T GRAPHIC, V25, P1206, DOI 10.1145/1141911.1142016
   MALONEY LT, 1989, P SOC PHOTO-OPT INS, V1199, P1154
   Oruç I, 2003, VISION RES, V43, P2451, DOI 10.1016/S0042-6989(03)00435-8
   Robertson G., 2000, CHI 2000 Conference Proceedings. Conference on Human Factors in Computing Systems. CHI 2000. The Future is Here, P494, DOI 10.1145/332040.332482
   Robertson G., 1998, 11th Annual Symposium on User Interface Software and Technology. UIST. Proceedings of the ACM Symposium, P153, DOI 10.1145/288392.288596
   Robertson G. G., 1991, Human Factors in Computing Systems. Reaching Through Technology. CHI '91. Conference Proceedings, P189, DOI 10.1145/108844.108883
   Ropinski T, 2006, LECT NOTES COMPUT SC, V4073, P93
   Rosenholtz R., 2005, P SIGCHI C HUMAN FAC, P761, DOI [DOI 10.1145/1054972.1055078, 10.1145/1054972.1055078]
   RUNKLER TA, 1993, SECOND IEEE INTERNATIONAL CONFERENCE ON FUZZY SYSTEMS, VOLS 1 AND 2, P1161, DOI 10.1109/FUZZY.1993.327350
   Russell J. A., 1997, CIRCUMPLEX MODELS PE, P205
   Schrater PR, 2000, INT J COMPUT VISION, V40, P73, DOI 10.1023/A:1026557704054
   Sears A., 2007, HUMAN COMPUTER INTER
   Shirley P., 2002, FUNDAMENTALS COMPUTE, V1st
   SWAIN C. T., 2009, US Patent, Patent No. [6,157,733, 6157733]
   Tarini M, 2006, IEEE T VIS COMPUT GR, V12, P1237, DOI 10.1109/TVCG.2006.115
   Trommershauser J., 2011, SENSORY CUE INTEGRAT, P5
   WANGER LR, 1992, IEEE COMPUT GRAPH, V12, P44, DOI 10.1109/38.135913
   Ware C, 1998, IEEE T SYST MAN CY A, V28, P56, DOI 10.1109/3468.650322
   WARE C., 2004, INFORM VISUALIZATION, P259
   Ware C, 2008, MORG KAUF SER INTER, P87, DOI 10.1016/B978-0-12-370896-0.00005-6
   Ware C, 2008, ACM T APPL PERCEPT, V5, DOI 10.1145/1279640.1279642
   WEISKOPF D., 2002, TR200208 U STUTTG VI
NR 41
TC 0
Z9 0
U1 0
U2 9
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD OCT
PY 2013
VL 10
IS 4
AR 19
DI 10.1145/2536764.2536766
PG 22
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 281YK
UT WOS:000329136700002
DA 2024-07-18
ER

PT J
AU Mustafa, M
   Guthe, S
   Magnor, M
AF Mustafa, Maryam
   Guthe, Stefan
   Magnor, Marcus
TI Single-Trial EEG Classification of Artifacts in Videos
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Human Factors; Perception; rendering; perception of rendering artifacts;
   EEG; SVM; wavelets; human visual system
ID COMPLEX WAVELET TRANSFORM; VISUAL-PERCEPTION
AB In this article we use an ElectroEncephaloGraph (EEG) to explore the perception of artifacts that typically appear during rendering and determine the perceptual quality of a sequence of images. Although there is an emerging interest in using an EEG for image quality assessment, one of the main impediments to the use of an EEG is the very low Signal-to-Noise Ratio (SNR) which makes it exceedingly difficult to distinguish neural responses from noise. Traditionally, event-related potentials have been used for analysis of EEG data. However, they rely on averaging and so require a large number of participants and trials to get meaningful data. Also, due the the low SNR ERP's are not suited for single-trial classification.
   We propose a novel wavelet-based approach for evaluating EEG signals which allows us to predict the perceived image quality from only a single trial. Our wavelet-based algorithm is able to filter the EEG data and remove noise, eliminating the need for many participants or many trials. With this approach it is possible to use data from only 10 electrode channels for single-trial classification and predict the presence of an artifact with an accuracy of 85%. We also show that it is possible to differentiate and classify a trial based on the exact type of artifact viewed. Our work is particularly useful for understanding how the human visual system responds to different types of degradations in images and videos. An understanding of the perception of typical image-based rendering artifacts forms the basis for the optimization of rendering and masking algorithms.
C1 [Mustafa, Maryam; Guthe, Stefan; Magnor, Marcus] Tech Univ Carolo Wilhelmina Braunschweig, Inst Comp Graph, D-38106 Braunschweig, Germany.
C3 Braunschweig University of Technology
RP Mustafa, M (corresponding author), Tech Univ Carolo Wilhelmina Braunschweig, Inst Comp Graph, Muhlenpfordtstr 23, D-38106 Braunschweig, Germany.
EM mustafa@cg.cs.tu-bs.de
OI Magnor, Marcus/0000-0003-0579-480X
FU ERC [256941]; European Research Council (ERC) [256941] Funding Source:
   European Research Council (ERC)
FX This work was funded in part by ERC grant no. 256941 'Reality CG'.
CR Agam Y, 2007, NEUROIMAGE, V36, P933, DOI 10.1016/j.neuroimage.2007.04.014
   Anderson EW, 2011, COMPUT GRAPH FORUM, V30, P791, DOI 10.1111/j.1467-8659.2011.01928.x
   [Anonymous], EUROGRAPHICS STATE A
   Barri A, 2012, J MATH ANAL APPL, V389, P1303, DOI 10.1016/j.jmaa.2012.01.010
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Emotiv, 2012, EPOC NEUR
   Farrugia JP, 2004, COMPUT GRAPH FORUM, V23, P605, DOI 10.1111/j.1467-8659.2004.00792.x
   Ferwerda J. A., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P249, DOI 10.1145/237170.237262
   Haar A, 1910, MATH ANN, V69, P331, DOI 10.1007/BF01456326
   INTERNATIONAL TELECOMMUNICATION UNION, 2006, ITU T REC P 800 1
   Kosara R, 2003, IEEE COMPUT GRAPH, V23, P20, DOI 10.1109/MCG.2003.1210860
   KRIVANEK J., 2010, ACM SIGGRAPH COURS
   Lindemann, 2011, P IEEE INT C IM PROC, P3170
   LINDEMANN L., 2011, P ACM APPL PERC COMP, P1
   Luebke D., 2001, PERCEPTUALLY DRIVEN
   McNamara A, 2001, COMPUT GRAPH FORUM, V20, P211, DOI 10.1111/1467-8659.00550
   MCNAMARA A., 2010, P ACM SIGGRAPH C, P1
   Moulson MC, 2011, NEUROPSYCHOLOGIA, V49, P3847, DOI 10.1016/j.neuropsychologia.2011.09.046
   MUSTAFA M., 2012, P ACM HUM FACT COMP
   Olkkonen H, 2006, J NEUROSCI METH, V151, P106, DOI 10.1016/j.jneumeth.2005.06.028
   Oppenheim A. V., 1999, DISCRETE TIME SIGNAL, DOI DOI 10.1049/EP.1977.0078
   OSullivan C., 2004, Eurographics state of the art reports, V4, P1
   Ponomarenko N., 2009, ADV MODERN RADIOELEC, V10, P30, DOI 10.1109/TIP.2015.2439035
   Rossion B, 2011, VISION RES, V51, P1297, DOI 10.1016/j.visres.2011.04.003
   Rousselet GA, 2007, NEUROIMAGE, V36, P843, DOI 10.1016/j.neuroimage.2007.02.052
   Rusinkiewicz S, 2000, COMP GRAPH, P343, DOI 10.1145/344779.344940
   Seshadrinathan K, 2010, IEEE T IMAGE PROCESS, V19, P335, DOI 10.1109/TIP.2009.2034992
   Shenoy P, 2008, CHI 2008: 26TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS VOLS 1 AND 2, CONFERENCE PROCEEDINGS, P845
   Subasi A, 2005, NEURAL NETWORKS, V18, P985, DOI 10.1016/j.neunet.2005.01.006
   SWELDENS W, 2000, COMPUTER, V5, P15
   Vangorp P, 2011, COMPUT GRAPH FORUM, V30, P1241, DOI 10.1111/j.1467-8659.2011.01983.x
NR 31
TC 31
Z9 33
U1 0
U2 5
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD JUL
PY 2012
VL 9
IS 3
AR 12
DI 10.1145/2325722.2325725
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 984EE
UT WOS:000307171700003
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Ennis, C
   Peters, C
   O'Sullivan, C
AF Ennis, Cathy
   Peters, Christopher
   O'Sullivan, Carol
TI Perceptual Effects of Scene Context and Viewpoint for Virtual Pedestrian
   Crowds
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Experimentation; Human Factors; Perception; virtual crowd formation
AB In this article, we evaluate the effects of position, orientation, and camera viewpoint on the plausibility of pedestrian formations. In a set of three perceptual studies, we investigated how humans perceive characteristics of virtual crowds in static scenes reconstructed from annotated still images, where the orientations and positions of the individuals have been modified. We found that by applying rules based on the contextual information of the scene, we improved the perceived realism of the crowd formations when compared to random formations. We also examined the effect of camera viewpoint on the plausibility of virtual pedestrian scenes, and we found that an eye-level viewpoint is more effective for disguising random behaviors, while a canonical viewpoint results in these behaviors being perceived as less realistic than an isometric or top-down viewpoint. Results from these studies can help in the creation of virtual crowds, such as computer graphics pedestrian models or architectural scenes, and identify situations when users' perception is less accurate.
C1 [Ennis, Cathy] Trinity Coll Dublin, Graph Vis & Visualisat Grp, Dublin, Ireland.
C3 Trinity College Dublin
RP Ennis, C (corresponding author), Trinity Coll Dublin, Graph Vis & Visualisat Grp, Dublin, Ireland.
EM ennisca@cs.tcd.ie
OI O'Sullivan, Carol/0000-0003-3772-4961; Ennis, Cathy/0000-0002-1274-5347
FU Science Foundation Ireland
FX This work was funded by the Science Foundation Ireland.
CR [Anonymous], 1981, Attention and Performance
   [Anonymous], 1997, COMPUTER ANIMATION S
   Blanz V, 1999, PERCEPTION, V28, P575, DOI 10.1068/p2897
   BULTHOFF HH, 1995, CEREB CORTEX, V5, P247, DOI 10.1093/cercor/5.3.247
   Chenney Stephen., 2004, Proceedings of the 2004 ACM SIGGRAPH/Euro- graphics symposium on Computer animation, P233, DOI [10.1145/1028523.1028553, DOI 10.1145/1028523.1028553.]
   Ennis C, 2008, APGV 2008: PROCEEDINGS OF THE SYMPOSIUM ON APPLIED PERCEPTION IN GRAPHICS AND VISUALIZATION, P75
   Gallese V, 1996, BRAIN, V119, P593, DOI 10.1093/brain/119.2.593
   Gomez P, 2008, PSYCHON B REV, V15, P940, DOI 10.3758/PBR.15.5.940
   HELBING D, 1995, PHYS REV E, V51, P4282, DOI 10.1103/PhysRevE.51.4282
   HENRY D, 1993, IEEE VIRTUAL REALITY ANNUAL INTERNATIONAL SYMPOSIUM, P33, DOI 10.1109/VRAIS.1993.380801
   Lamarche F, 2004, COMPUT GRAPH FORUM, V23, P509, DOI 10.1111/j.1467-8659.2004.00782.x
   Lee KH, 2007, SYMPOSIUM ON COMPUTER ANIMATION 2007: ACM SIGGRAPH/ EUROGRAPHICS SYMPOSIUM PROCEEDINGS, P109
   Lerner A, 2007, COMPUT GRAPH FORUM, V26, P655, DOI 10.1111/j.1467-8659.2007.01089.x
   Lyons MJ, 2000, P ROY SOC B-BIOL SCI, V267, P2239, DOI 10.1098/rspb.2000.1274
   McDonnell R, 2007, SYMPOSIUM ON COMPUTER ANIMATION 2007: ACM SIGGRAPH/ EUROGRAPHICS SYMPOSIUM PROCEEDINGS, P259
   Pelechano N, 2007, SYMPOSIUM ON COMPUTER ANIMATION 2007: ACM SIGGRAPH/ EUROGRAPHICS SYMPOSIUM PROCEEDINGS, P99
   PETERS C, 2008, P EUR SHORT PAP
   PREMACK D, 1978, BEHAV BRAIN SCI, V1, P515, DOI 10.1017/S0140525X00076512
   Reitsma PSA, 2003, ACM T GRAPHIC, V22, P537, DOI 10.1145/882262.882304
   Reynolds CW., 1987, SIGGRAPH Comput. Graph., V21, P25, DOI [10.1145/37402.37406, DOI 10.1145/37402.37406]
   Schilbach L, 2006, NEUROPSYCHOLOGIA, V44, P718, DOI 10.1016/j.neuropsychologia.2005.07.017
   SHAO W, 2005, SCA 05, P19
   TARR MJ, 1995, PSYCHON B REV, V2, P55, DOI 10.3758/BF03214412
   Yarbus A. L., 1973, EYE MOVEMENTS VISION
NR 24
TC 30
Z9 33
U1 0
U2 7
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD JAN
PY 2011
VL 8
IS 2
AR 10
DI 10.1145/1870076.1870078
PG 22
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 748AR
UT WOS:000289362900002
DA 2024-07-18
ER

PT J
AU Bernhard, M
   Stavrakis, E
   Wimmer, M
AF Bernhard, Matthias
   Stavrakis, Efstathios
   Wimmer, Michael
TI An Empirical Pipeline to Derive Gaze Prediction Heuristics for 3D Action
   Games
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Algorithms; Experimentation; Human Factors; Gaze analysis; eye-tracking;
   visual attention; high-level properties; importance map; gaze predictor;
   video games; virtual environments
ID VISUAL-ATTENTION; EYE-MOVEMENTS; BOTTOM-UP; OBJECTS; ALLOCATION;
   OCCLUSION; SHIFTS; OVERT; MODEL; TASK
AB Gaze analysis and prediction in interactive virtual environments, such as games, is a challenging topic since the 3D perspective and variations of the viewpoint as well as the current task introduce many variables that affect the distribution of gaze. In this article, we present a novel pipeline to study eye-tracking data acquired from interactive 3D applications. The result of the pipeline is an importance map which scores the amount of gaze spent on each object. This importance map is then used as a heuristic to predict a user's visual attention according to the object properties present at runtime. The novelty of this approach is that the analysis is performed in object space and the importance map is defined in the feature space of high-level properties. High-level properties are used to encode task relevance and other attributes, such as eccentricity, which may have an impact on gaze behavior.
   The pipeline has been tested with an exemplary study on a first-person shooter game. In particular, a protocol is presented describing the data acquisition procedure, the learning of different importance maps from the data, and finally an evaluation of the performance of the derived gaze predictors. A metric measuring the degree of correlation between attention predicted by the importance map and the actual gaze yielded clearly positive results. The correlation becomes particularly strong when the player is attentive to an in-game task.
C1 [Bernhard, Matthias; Wimmer, Michael] Vienna Univ Technol, Vienna, Austria.
   [Stavrakis, Efstathios] INRIA REVES, Sophia Antipolis, France.
C3 Technische Universitat Wien
RP Bernhard, M (corresponding author), Vienna Univ Technol, Vienna, Austria.
EM matthias.bernhard@cg.tuwien.ac.at
RI Stavrakis, Efstathios/I-8232-2014
OI Stavrakis, Efstathios/0000-0002-9213-7690; Wimmer,
   Michael/0000-0002-9370-2663
FU EU CROSSMOD [FP6-14891]; FWF-General Purpose Visibility [P-21130]
FX This work was funded by the EU CROSSMOD FP6-14891 and the FWF-General
   Purpose Visibility P-21130 Projects.
CR [Anonymous], P SIGCHI C HUM FACT
   [Anonymous], 1890, PRINCIPLES PSYCHOL, DOI DOI 10.1037/10538-000
   BAYLIS GC, 1993, J EXP PSYCHOL HUMAN, V19, P451, DOI 10.1037/0096-1523.19.3.451
   Behrmann M, 1998, J EXP PSYCHOL HUMAN, V24, P1011, DOI 10.1037/0096-1523.24.4.1011
   Bittner J, 2004, COMPUT GRAPH FORUM, V23, P615, DOI 10.1111/j.1467-8659.2004.00793.x
   Canosa RL, 2003, P SOC PHOTO-OPT INS, V5007, P240, DOI 10.1117/12.477375
   Cater K., 2003, Eurographics Symposium on Rendering. 14th Eurographics Workshop on Rendering, P270
   Cater K., 2002, Proceedings of the ACM symposium on Virtual reality software and technology, VRST '02, P17, DOI [10.1145/585740.585744, DOI 10.1145/585740.585744]
   Cerf M, 2008, PROCEEDINGS OF THE EYE TRACKING RESEARCH AND APPLICATIONS SYMPOSIUM (ETRA 2008), P143, DOI 10.1145/1344471.1344508
   Collewijn H., 1992, The head-neck sensory motor system, P412, DOI 10.1093/acprof:oso/9780195068207.003.0064
   DEGRAEF P, 1990, PSYCHOL RES-PSYCH FO, V52, P317, DOI 10.1007/BF00868064
   Duchowski A., 2003, Eye Tracking Methodology: Theory and Practice
   DUNCAN J, 1984, J EXP PSYCHOL GEN, V113, P501, DOI 10.1037/0096-3445.113.4.501
   Einhäuser W, 2008, J VISION, V8, DOI 10.1167/8.2.2
   ELAZARY L, 2003, J VISION, V8, P1
   Hayhoe MM, 2003, J VISION, V3, P49, DOI 10.1167/3.1.6
   Henderson JM, 1999, J EXP PSYCHOL HUMAN, V25, P210, DOI 10.1037/0096-1523.25.1.210
   Henderson JM, 2003, TRENDS COGN SCI, V7, P498, DOI 10.1016/j.tics.2003.09.006
   Henderson JM, 2003, PERCEPT PSYCHOPHYS, V65, P725, DOI 10.3758/BF03194809
   ITTI, 2006, ADV NEURAL INFORM PR, V19, P547
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Itti L, 2001, NAT REV NEUROSCI, V2, P194, DOI 10.1038/35058500
   Itti L, 2000, VISION RES, V40, P1489, DOI 10.1016/S0042-6989(99)00163-7
   Jie L, 2007, LECT NOTES COMPUT SC, V4740, P345
   Kenny A, 2005, Simulation in Wider Europe, P733
   KOCH C, 1985, HUM NEUROBIOL, V4, P219
   KOMOGORTSEV O, 2006, P 2006 S EYE TRACK R, P101, DOI DOI 10.1145/1117309
   Land M, 1999, PERCEPTION, V28, P1311, DOI 10.1068/p2935
   Lee S, 2007, VRST 2007: ACM SYMPOSIUM ON VIRTUAL REALITY SOFTWARE AND TECHNOLOGY, PROCEEDINGS, P29
   LUEBKE D, 2000, P EUR WORKSH REND
   MACK J, 1998, INATTENTIONAL BLINDN
   MARMITT G, 2002, P EUR, P217
   MURPHY H, 2001, P EUR
   Navalpakkam V, 2005, VISION RES, V45, P205, DOI 10.1016/j.visres.2004.07.042
   O'Craven KM, 1999, NATURE, V401, P584, DOI 10.1038/44134
   Oliva A., 2003, P IEEE INT C IM PROC
   Palmer S., 1999, VISION SCI PHOTONS P
   Parkhurst D, 2002, VISION RES, V42, P107, DOI 10.1016/S0042-6989(01)00250-4
   Pelz JB, 2001, VISION RES, V41, P3587, DOI 10.1016/S0042-6989(01)00245-0
   Peters R.J., 2007, P IEEE C COMPUTER VI, P1
   Peters RJ, 2005, VISION RES, V45, P2397, DOI 10.1016/j.visres.2005.03.019
   Peters RJ, 2008, ACM T APPL PERCEPT, V5, DOI 10.1145/1279920.1279923
   Rothkopf CA, 2007, J VISION, V7, DOI 10.1167/7.14.16
   SALTHOUSE TA, 1981, J EXP PSYCHOL HUMAN, V7, P611, DOI 10.1037/0096-1523.7.3.611
   Scholl BJ, 2001, COGNITION, V80, P1, DOI 10.1016/S0010-0277(00)00152-9
   Seif El-Nasr M., 2006, P 2006 ACM SIGCHI IN, DOI DOI 10.1145/1178823.1178849
   Simons DJ, 1999, PERCEPTION, V28, P1059, DOI 10.1068/p2952
   Sundstedt V., 2005, Proceedings of the 21st spring conference on Computer graphics, P169, DOI [10.1145/1090122.1090150, DOI 10.1145/1090122.1090150]
   Sundstedt V, 2007, ACM T APPL PERCEPT, V4, DOI 10.1145/1278387.1278389
   Sundstedt V, 2008, APGV 2008: PROCEEDINGS OF THE SYMPOSIUM ON APPLIED PERCEPTION IN GRAPHICS AND VISUALIZATION, P43
   TREISMAN AM, 1980, COGNITIVE PSYCHOL, V12, P97, DOI 10.1016/0010-0285(80)90005-5
   van Zoest W, 2004, PERCEPTION, V33, P927, DOI 10.1068/p5158
   Wolfe J.M., 2000, SEEING, P335, DOI DOI 10.1016/B978-012443760-9/50010-6
   Wolfe J. M., 2007, INTEGRATED MODELS CO, P99, DOI [10.1093/acprof:oso/9780195189193.003.0008, DOI 10.1093/ACPROF:OSO/9780195189193.003.0008]
   WOLFE JM, 1994, PSYCHON B REV, V1, P202, DOI 10.3758/BF03200774
   Yarbus A.L., 1967, EYE MOVEMENTS VISION, DOI [10.1007/978-1-4899-5379-7, DOI 10.1007/978-1-4899-5379-7]
NR 56
TC 13
Z9 14
U1 1
U2 13
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD OCT
PY 2010
VL 8
IS 1
AR 4
DI 10.1145/1857893.1857897
PG 30
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA 748AJ
UT WOS:000289362100004
DA 2024-07-18
ER

PT J
AU Llobera, J
   Spanlang, B
   Ruffini, G
   Slater, M
AF Llobera, Joan
   Spanlang, Bernhard
   Ruffini, Giulio
   Slater, Mel
TI Proxemics with Multiple Dynamic Characters in an Immersive Virtual
   Environment
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Experimentation; Human Factors; Human-computer interaction; proxemics;
   virtual characters; avatars
AB An experiment was carried out to examine the impact on electrodermal activity of people when approached by groups of one or four virtual characters at varying distances. It was premised on the basis of proxemics theory that the closer the approach of the virtual characters to the participant, the greater the level of physiological arousal. Physiological arousal was measured by the number of skin conductance responses within a short time period after the approach, and the maximum change in skin conductance level 5 s after the approach. The virtual characters were each either female or a cylinder of human size, and one or four characters approached each subject a total of 12 times. Twelve male subjects were recruited for the experiment. The results suggest that the number of skin conductance responses after the approach and the change in skin conductance level increased the closer the virtual characters approached toward the participants. Moreover, these response variables were inversely correlated with the number of visits, showing a typical adaptation effect. There was some evidence to suggest that the number of characters who simultaneously approached (one or four) was positively associated with the responses. Surprisingly there was no evidence of a difference in response between the humanoid characters and cylinders on the basis of this physiological data. It is suggested that the similarity in this quantitative arousal response to virtual characters and virtual objects might mask a profound difference in qualitative response, an interpretation supported by questionnaire and interview results. Overall the experiment supported the premise that people exhibit heightened physiological arousal the closer they are approached by virtual characters.
C1 [Llobera, Joan; Slater, Mel] Univ Barcelona, Fac Psicol, ICREA, Barcelona 08035, Spain.
   [Llobera, Joan; Ruffini, Giulio] Starlab Barcelona, Barcelona 08035, Spain.
   [Slater, Mel] UCL, London WC1E 6BT, England.
   [Slater, Mel] Univ Politecn Cataluna, E-08028 Barcelona, Spain.
C3 ICREA; University of Barcelona; University of London; University College
   London; Universitat Politecnica de Catalunya
RP Llobera, J (corresponding author), Univ Barcelona, Fac Psicol, ICREA, Campus Mundet,Edifici Teatre,Passeig Vall dHebron, Barcelona 08035, Spain.
EM joan.llobera@ub.edu; bspanlang@ub.edu; giulio.ruffini@starlab.es;
   melslater@ub.edu
RI Llobera, Joan/G-3918-2016; Llobera, Joan/T-9094-2017; Slater,
   Mel/M-5210-2014
OI Llobera, Joan/0000-0002-9471-1334; Spanlang,
   Bernhard/0000-0002-9659-635X; Slater, Mel/0000-0002-6223-0050
FU EU [27731]; Torres Quevedo Program [PTQ06-1-0042]; ICREA Funding Source:
   Custom
FX This work was funded under the EU FET PRESENCCIA project contract Number
   27731, with help from the Spanish Ministry of Science and Innovation. J.
   Llobera's work was supported by the Torres Quevedo Program reference
   number PTQ06-1-0042. M. Slater is also affiliated with University
   College London; B. Spanlang is also affiliated with the Universitat
   Politecnica de Catalunya.
CR Bailenson JN, 2001, PRESENCE-VIRTUAL AUG, V10, P583, DOI 10.1162/105474601753272844
   Bailenson JN, 2003, PERS SOC PSYCHOL B, V29, P819, DOI 10.1177/0146167203029007002
   Beltran FS, 2006, JASSS-J ARTIF SOC S, V9
   Bideau B, 2003, PRESENCE-TELEOP VIRT, V12, P411, DOI 10.1162/105474603322391631
   Blascovich J, 2002, PSYCHOL INQ, V13, P103, DOI 10.1207/S15327965PLI1302_01
   Blascovich J, 2002, COMP SUPP COMP W SER, P127
   Boucsein W, 1992, ELECTRODERMAL ACTIVI
   Brooks AG, 2007, AUTON ROBOT, V22, P55, DOI 10.1007/s10514-006-9005-8
   DeFanti TA, 1993, Proceedings of the 20th annual conference on Computer graphics and interactive techniques, P135, DOI 10.1145/166117.166134.
   Draper JV, 1998, HUM FACTORS, V40, P354, DOI 10.1518/001872098779591386
   Dror HA, 2006, TECHNOMETRICS, V48, P520, DOI 10.1198/004017006000000318
   Friedman D, 2007, LECT NOTES ARTIF INT, V4722, P252
   Guye-Vuilleme A., 1999, Virtual Reality, V4, P49, DOI 10.1007/BF01434994
   HALL ET, 1973, LEONARDO
   JARQUE CM, 1980, ECON LETT, V6, P255, DOI 10.1016/0165-1765(80)90024-5
   MCBRIDE G, 1965, J PSYCHOL, V61, P153, DOI 10.1080/00223980.1965.10544805
   McCullagh, 1989, Generalized Linear Models, V2nd
   NOE A, 2004, BRADFORD BOOKS SERIE
   Pertaub DP, 2002, PRESENCE-TELEOP VIRT, V11, P68, DOI 10.1162/105474602317343668
   Sanchez-Vives MV, 2005, NAT REV NEUROSCI, V6, P332, DOI 10.1038/nrn1651
   Sandro B., 2005, Proceedings of the 2005 ACM SIGCHI International Con- ference on Advances in Computer Entertainment Technology, P270, DOI DOI 10.1145/1178477.1178524
   Sheridan T., 1992, Presence: Teleoperators and Virtual Environments, V1, P120, DOI DOI 10.1162/PRES.1992.1.1.120
   Sheridan TB, 1996, PRESENCE-TELEOP VIRT, V5, P241, DOI 10.1162/pres.1996.5.2.241
   SLATER M, 2009, PHILOS T R IN PRESS
   Slater M, 2006, PLOS ONE, V1, DOI 10.1371/journal.pone.0000039
   SPANLANG B, 2009, HALCA HARDWARE ACCEL
   Taylor R.M., 2001, Proceedings of the ACM Symposium on Virtual Reality Software and Technology - VRST '01, P55, DOI [10.1145/505008.505019, DOI 10.1145/505008.505019, 10.1145/505008.505019., DOI 10.1145/505008.5050192]
   Wilcox Laurie M., 2006, ACM Trans. on Perception, V3, P412, DOI [DOI 10.1145/1190036.1190041, 10.1145/1190036.1190041]
NR 28
TC 92
Z9 100
U1 0
U2 24
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD OCT
PY 2010
VL 8
IS 1
AR 3
DI 10.1145/1857893.1857896
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA 748AJ
UT WOS:000289362100003
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Kim, J
   Palmisano, SA
   Ash, A
   Allison, RS
AF Kim, Juno
   Palmisano, Stephen A.
   Ash, April
   Allison, Robert S.
TI Pilot Gaze and Glideslope Control
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Experimentation; Human Factors; Theory; Vision; aviation; glideslope
   control; gaze; landing
ID PERCEPTION; DIRECTION; MODEL
AB We examined the eye movements of pilots as they carried out simulated aircraft landings under day and night lighting conditions. Our five students and five certified pilots were instructed to quickly achieve and then maintain a constant 3-degree glideslope relative to the runway. However, both groups of pilots were found to make significant glideslope control errors, especially during simulated night approaches. We found that pilot gaze was directed most often toward the runway and to the ground region located immediately in front of the runway, compared to other visual scene features. In general, their gaze was skewed toward the near half of the runway and tended to follow the runway threshold as it moved on the screen. Contrary to expectations, pilot gaze was not consistently directed at the aircraft's simulated aimpoint (i.e., its predicted future touchdown point based on scene motion). However, pilots did tend to fly the aircraft so that this point was aligned with the runway threshold. We conclude that the supplementary out-of-cockpit visual cues available during day landing conditions facilitated glideslope control performance. The available evidence suggests that these supplementary visual cues are acquired through peripheral vision, without the need for active fixation.
C1 [Kim, Juno] Univ Wollongong, Sch Psychol, Wollongong, NSW 2522, Australia.
   [Allison, Robert S.] York Univ, N York, ON M3J 1P3, Canada.
C3 University of Wollongong; York University - Canada
RP Kim, J (corresponding author), Univ Wollongong, Sch Psychol, Wollongong, NSW 2522, Australia.
EM juno@uow.edu.au
RI Palmisano, Stephen/O-1553-2018; Ash, April/K-8474-2013
OI Palmisano, Stephen/0000-0002-9140-5681; Allison,
   Robert/0000-0002-4485-2665; Ash, April/0000-0002-0204-3069
FU Australian Research Council (ARC) [DP0772398]; Australian Research
   Council [DP0772398] Funding Source: Australian Research Council
FX This study was funded in full by an Australian Research Council (ARC)
   Discovery grant DP0772398 awarded to Drs Palmisano and Allison.
CR Allison RS, 1996, IEEE T BIO-MED ENG, V43, P1073, DOI 10.1109/10.541249
   BASLER M, 2008, FLIGHTGEAR MANUAL
   Calvert E.S., 1954, J NAVIGATION, V7, P233
   Cohen J., 1988, STAT POWER ANAL BEHA
   Duchowski AT, 2002, BEHAV RES METH INS C, V34, P455, DOI 10.3758/BF03195475
   FLACH JM, 1992, PERCEPT PSYCHOPHYS, V51, P557, DOI 10.3758/BF03211653
   Galanis G, 1998, INT J AVIAT PSYCHOL, V8, P83, DOI 10.1207/s15327108ijap0802_1
   Gibb RW, 2007, AVIAT SPACE ENVIR MD, V78, P801
   GIBSON J J, 1955, Am J Psychol, V68, P372, DOI 10.2307/1418521
   GROSZ J, 1995, LOCAL APPL ECOLOGICA, P104
   LAND MF, 1994, NATURE, V369, P742, DOI 10.1038/369742a0
   LINTERN G, 1991, HUM FACTORS, V33, P401, DOI 10.1177/001872089103300403
   LINTERN G, 1991, INT J AVIAT PSYCHOL, V1, P57
   Liu Zhong-qi, 2002, Space Med Med Eng (Beijing), V15, P379
   MACKWORTH JF, 1958, J OPT SOC AM, V48, P439, DOI 10.1364/JOSA.48.000439
   MAJENDIE AMA, 1960, J I NAVIGATION, V13, P447
   MERTENS HW, 1981, AVIAT SPACE ENVIR MD, V52, P373
   MERTENS HW, 1978, AVIAT SPACE ENVIR MD, V49, P457
   MOORE ST, 2005, P HUM SPAC S
   Mulder M, 2000, INT J AVIAT PSYCHOL, V10, P291, DOI 10.1207/S15327108IJAP1003_05
   Ottati WL, 1999, HUM FAC ERG SOC P, P66
   Palmisano S, 2005, J EXP PSYCHOL-APPL, V11, P19, DOI 10.1037/1076-898X.11.1.19
   PERRONE JA, 1984, AVIAT SPACE ENVIR MD, V55, P1020
   REGAN D, 1982, SCIENCE, V215, P194, DOI 10.1126/science.7053572
   Robertshaw KD, 2008, J VISION, V8, DOI 10.1167/8.4.18
   THOMAS EL, 1963, AEROSPACE MED, V34, P424
   VANDEGRIND WA, 1983, J OPT SOC AM, V73, P1674, DOI 10.1364/JOSA.73.001674
   WARREN WH, 1992, PERCEPT PSYCHOPHYS, V51, P443, DOI 10.3758/BF03211640
   Wilkie RM, 2008, J EXP PSYCHOL HUMAN, V34, P1150, DOI 10.1037/0096-1523.34.5.1150
   Wilkie RM, 2006, J EXP PSYCHOL HUMAN, V32, P88, DOI 10.1037/0096-1523.32.1.88
   Wilkie RM, 2005, J EXP PSYCHOL HUMAN, V31, P901, DOI 10.1037/0096-1523.31.5.901
   Wilkie RM, 2002, CURR BIOL, V12, P2014, DOI 10.1016/S0960-9822(02)01337-4
NR 32
TC 19
Z9 20
U1 0
U2 11
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD JUN
PY 2010
VL 7
IS 3
AR 18
DI 10.1145/1773965.1773969
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA 618NF
UT WOS:000279361800004
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Rienks, R
   Poppe, R
   Heylen, D
AF Rienks, Rutger
   Poppe, Ronald
   Heylen, Dirk
TI Differences in Head Orientation Behavior for Speakers and Listeners: An
   Experiment in a Virtual Environment
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Experimentation; Human factors; Measurement; Performance; Head
   orientation; multiparty conversation; focus of attention; gaze behavior;
   virtual environments; perception of gaze
ID EYE-CONTACT; PERCEPTION; GAZE; LOOKING
AB An experiment was conducted to investigate whether human observers use knowledge of the differences in focus of attention in multiparty interaction to identify the speaker amongst the meeting participants. A virtual environment was used to have good stimulus control. Head orientations were displayed as the only cue for focus attention. The orientations were derived from a corpus of tracked head movements. We present some properties of the relation between head orientations and speaker-listener status, as found in the corpus. With respect to the experiment, it appears that people use knowledge of the patterns in focus of attention to distinguish the speaker from the listeners. However, the human speaker identification results were rather low. Head orientations ( or focus of attention) alone do not provide a sufficient cue for reliable identification of the speaker in a multiparty setting.
C1 [Rienks, Rutger; Poppe, Ronald; Heylen, Dirk] Univ Twente, Human Media Interact Grp, NL-7500 AE Enschede, Netherlands.
C3 University of Twente
RP Rienks, R (corresponding author), Univ Twente, Human Media Interact Grp, POB 217, NL-7500 AE Enschede, Netherlands.
EM rienks@ewi.utwente.nl; poppe@ewi.utwente.nl; heylen@ewi.utwente.nl
OI Poppe, Ronald/0000-0002-0843-7878
FU European IST [FP6-033812]
FX This work was supported by the European IST Programme Project FP6-033812
   (Augmented Multi-party Interaction with Distant Access).
CR [Anonymous], MSRTR200081
   [Anonymous], P SIG CHI C HUM FACT
   [Anonymous], P INT CLASS WORKSH N
   ARGYLE M, 1965, SOCIOMETRY, V28, P289, DOI 10.2307/2786027
   Argyle M., 1976, Gaze and Mutual Gaze
   Argyle Michael, 1975, BODILY COMMUNICATION
   Bailenson JN, 2004, PRESENCE-VIRTUAL AUG, V13, P428, DOI 10.1162/1054746041944803
   BEALL AC, 2003, P HCI INT 2003 CRET, P1108
   Cassell J, 1999, APPL ARTIF INTELL, V13, P519, DOI 10.1080/088395199117360
   CLINE MG, 1967, AM J PSYCHOL, V80, P41, DOI 10.2307/1420539
   DUNCAN S, 1974, J EXP SOC PSYCHOL, V10, P234, DOI 10.1016/0022-1031(74)90070-5
   EXLINE RV, 1963, J PERS, V31, P1, DOI 10.1111/j.1467-6494.1963.tb01836.x
   Fussell S. R., 2005, OTHER MINDS HUMANS B, P91
   GIBSON JJ, 1963, AM J PSYCHOL, V76, P386, DOI 10.2307/1419779
   Goodwin C., 1984, Structures of social action, P225
   Heylen D, 2006, INT J HUM ROBOT, V3, P241, DOI 10.1142/S0219843606000746
   KENDON A, 1967, ACTA PSYCHOL, V26, P22, DOI 10.1016/0001-6918(67)90005-4
   KLEINKE CL, 1986, PSYCHOL BULL, V100, P78, DOI 10.1037/0033-2909.100.1.78
   KRUGER K, 1969, Z EXP ANGEW PSYCHOL, V16, P452
   Langton SRH, 2000, Q J EXP PSYCHOL-A, V53, P825, DOI 10.1080/027249800410562
   Loomis JM, 1999, BEHAV RES METH INS C, V31, P557, DOI 10.3758/BF03200735
   McClave EZ, 2000, J PRAGMATICS, V32, P855, DOI 10.1016/S0378-2166(99)00079-X
   Nielsen G., 1962, Studies in self-confrontation
   Otsuka K., 2005, P 7 INT C MULT INT, P191, DOI DOI 10.1145/1088463.1088497
   PERRETT DI, 1994, CAH PSYCHOL COGN, V13, P683
   Poggi I, 2000, AI COMMUN, V13, P169
   Poppe R, 2007, PERCEPTION, V36, P971, DOI 10.1068/p5753
   RAY JJ, 1990, J SOC PSYCHOL, V130, P397, DOI 10.1080/00224545.1990.9924595
   Reidsma D, 2007, AI SOC, V22, P133, DOI 10.1007/s00146-007-0129-y
   Sagiv N, 2001, J COGNITIVE NEUROSCI, V13, P937, DOI 10.1162/089892901753165854
   SELLEN AJ, 1992, P CHI 92, P49
   Stiefelhagen R, 2002, FOURTH IEEE INTERNATIONAL CONFERENCE ON MULTIMODAL INTERFACES, PROCEEDINGS, P273, DOI 10.1109/ICMI.2002.1167006
   Symons LA, 2004, J GEN PSYCHOL, V131, P451
   Vertegaal R, 2000, PROC GRAPH INTERF, P95
   VERTEGAAL R, 2000, P INT COMP HUM INT C, P257
   Weisbrod R.M., 1965, LOOKING BEHAV DISCUS
   Whittaker S, 2003, HANDBOOK OF DISCOURSE PROCESSES, P243
   Wilson HR, 2000, VISION RES, V40, P459, DOI 10.1016/S0042-6989(99)00195-9
NR 38
TC 13
Z9 15
U1 0
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD JAN
PY 2010
VL 7
IS 1
AR 2
DI 10.1145/1658349.1658351
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA 549EK
UT WOS:000274028400002
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Morvan, Y
   O'Sullivan, C
AF Morvan, Yann
   O'Sullivan, Carol
TI A Perceptual Approach to Trimming and Tuning Unstructured Lumigraphs
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article; Proceedings Paper
CT Symposium on Applied Perception in Graphics and Visualization
CY JUL 25-27, 2007
CL Tubingen, GERMANY
DE Algorithms; experimentation; human factors; performance; Image-based
   rendering; perceptual metrics
AB We present a novel perceptual method to reduce the visual redundancy of unstructured lumigraphs, an image based representation designed for interactive rendering. We combine features of the unstructured lumigraph algorithm and image fidelity metrics to efficiently rank the perceptual impact of the removal of subregions of input views (subviews). We use a greedy approach to estimate the order in which subviews should be pruned to minimize perceptual degradation at each step. Renderings using varying numbers of subviews can then be easily visualized with confidence that the retained subviews are well chosen, thus facilitating the choice of how many to retain. The regions of the input views that are left are repacked into a texture atlas. Our method takes advantage of any scene geometry information available but only requires a very coarse approximation. We perform a user study to validate its behaviour, as well as investigate the impact of the choice of image fidelity metric as well as that of user parameters. The three metrics considered fall in the physical, statistical and perceptual categories. The overall benefit of our method is the semiautomation of the view selection process, resulting in unstructured lumigraphs that are thriftier in texture memory use and faster to render. Using the same framework, we adjust the parameters of the unstructured lumigraph algorithm to optimise it on a scene by scene basis.
C1 [Morvan, Yann; O'Sullivan, Carol] Univ Dublin Trinity Coll, Graph Vis & Visualisat Grp, Dublin 2, Ireland.
C3 Trinity College Dublin
RP Morvan, Y (corresponding author), Univ Dublin Trinity Coll, Graph Vis & Visualisat Grp, Dublin 2, Ireland.
EM ymorvan@cs.tcd.ie
OI O'Sullivan, Carol/0000-0003-3772-4961
CR [Anonymous], RENDERING TECHNIQUES
   Buehler C, 2001, COMP GRAPH, P425, DOI 10.1145/383259.383309
   DEBEVEC P, 1998, REND TECHN 98 P EUR
   Dumont R, 2003, ACM T GRAPHIC, V22, P152, DOI 10.1145/636886.636888
   Fleishman S, 2000, COMPUT GRAPH FORUM, V19, P101, DOI 10.1111/1467-8659.00447
   Gortler S. J., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P43, DOI 10.1145/237170.237200
   HALE JG, 1998, TEXTURE REMAPPING DE
   Heigl Benno, 1999, MUSTERERKENNUNG 1999, P94
   HLAVAC V, 1996, P ECCV, P526
   Lubin J., 1995, Vision Models for Target Detection and Recognition, P245
   Mantiuk R, 2005, PROC SPIE, V5666, P204, DOI 10.1117/12.586757
   MYSZKOWSKI K, 1999, P 10 EUR WORKSH REND, P5
   Ramasubramanian M, 1999, COMP GRAPH, P73, DOI 10.1145/311535.311543
   Schirmacher H, 1999, COMPUT GRAPH FORUM, V18, pC151, DOI 10.1111/1467-8659.00336
   STOKES WA, 2004, ACM SIGGRAPH C P
   Tong X, 2003, IEEE T CIRC SYST VID, V13, P1080, DOI 10.1109/TCSVT.2003.817626
   Vazquez P.-P., 2001, Proceedings of Vision Modeling and Visualization Conference, P273
   Video Quality Experts Group, 2000, Final report from the video quality experts group on the validation of objective models of video quality assessment march 2000
   WALTER B, 2002, EUROGRAPHICS COMPUTE, V21
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang Z, 2002, INT CONF ACOUST SPEE, P3313
   Watson A.B., 1993, DIGITAL IMAGES HUMAN, P179
   Williams MC, 2003, ELEC SOC S, V2003, P3
   Yee H, 2001, ACM T GRAPHIC, V20, P39, DOI 10.1145/383745.383748
   YEE YH, 2004, SIGGRAPH 04 ACM SIGG, P121
NR 25
TC 3
Z9 4
U1 0
U2 2
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD JAN
PY 2009
VL 5
IS 4
AR 19
DI 10.1145/1462048.1462050
PG 24
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 450YL
UT WOS:000266437800002
DA 2024-07-18
ER

PT J
AU Choi, J
   Yook, S
   Kim, IY
   Jeong, MK
   Jang, DP
AF Choi, Jeongbong
   Yook, Soonhyun
   Kim, In Young
   Jeong, Mok Kun
   Jang, Dong Pyo
TI Quantification of Displacement for Tactile Sensation in a Contact-type
   Low Intensity Focused Ultrasound Haptic Device
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Tactile threshold; displacement; low intensity focused ultrasound
ID GLABROUS SKIN; TOUCH
AB Tactile threshold of low-intensity focused ultrasound (LIFU) haptic devices has been defined as the minimum pressure required for tactile sensation. However, in contact-type LIFU haptic devices using an elastomer as a conductive medium, the tactile threshold is affected by the mechanical properties of the elastomer. Therefore, the tactile threshold needs to be redefined as a parameter that does not change with the mechanical properties of the elastomer. In this study, we used the LIFU haptic device to investigate the displacement of the elastomer surface at the tactile threshold while controlling the pulse duration, pulse repetition frequency, and pressure. We analyzed the displacement magnitude and rate to determine their relationship to the pressure. The displacement magnitude is the spatiotemporal peak of the displacement, and the displacement rate is the initial slope of the displacement at the starting point of LIFU pulse. The tactile threshold measured by the applied pressure showed the U-shaped graph, and the minimum pressure of 475 kPa at 2 ms and 407 kPa at 300 Hz was measured. The tactile threshold measured by the displacement show that the tactile sensation can be evoked at the small displacement magnitude (<3 mu m) when the high displacement rate is present (>1.56 mm/s). Furthermore, the large displacement magnitude is required to induce the tactile sensation when the displacement rate is low. This result shows that the tactile threshold of a contact-type LIFU haptic device is affected by both the displacement magnitude and rate of the conductive medium. Our findings can be used as a guideline for developing a contact-type LIFU haptic device regardless of the elastomer used.
C1 [Choi, Jeongbong; Yook, Soonhyun; Kim, In Young; Jang, Dong Pyo] Hanyang Univ, Grad Sch Biomed Sci & Engn, Dept Biomed Engn, Seoul, South Korea.
   [Jeong, Mok Kun] Daejin Univ, Dept Elect Engn, Pochon, Gyeonggi Do, South Korea.
C3 Hanyang University; Daejin University
RP Choi, J (corresponding author), Hanyang Univ, Grad Sch Biomed Sci & Engn, Dept Biomed Engn, Seoul, South Korea.
EM cjungbong@gmail.com; soonuyny@hanyang.ac.kr; iykim@hanyang.ac.kr;
   jmk@daejin.ac.kr; dongpjang@gmail.com
OI Choi, Jeongbong/0000-0002-9021-0612
FU Samsung Research Funding Center of Samsung Electronics
   [SRFC-IT-1502-05]; Korean Health Technology R&D Project through the
   Korea Health Industry Development Institute (KHIDI) - Ministry of Health
   & Welfare, Republic of Korea [HI19C0753]
FX This work was supported by Samsung Research Funding Center of Samsung
   Electronics under project number SRFC-IT-1502-05 and a grant of the
   Korean Health Technology R&D Project through the Korea Health Industry
   Development Institute (KHIDI), funded by the Ministry of Health &
   Welfare, Republic of Korea (grant number HI19C0753). Device composition
   and clinical data collection were conducted with the support of
   SRFC-IT-1502-05. Analysis of the collected data was conducted with the
   support of HI19C0753. This work was approved by Institutional Review
   Board of Hanyang University (Approval Number HYI-17-214-2). All details
   of the work are registered in Clinical Research Information Service
   (CRIS) of Korea Centers for Disease Control and Prevention (Register
   Number KCT0004301), a member of World Health Organization's
   International Clinical Trials Registry Platform (WHO ICTRP).
CR BOLANOWSKI SJ, 1988, J ACOUST SOC AM, V84, P1680, DOI 10.1121/1.397184
   Carter T., 2013, P 26 ANN ACM S US IN, P505
   Choi J, 2017, 2017 IEEE WORLD HAPTICS CONFERENCE (WHC), P552, DOI 10.1109/WHC.2017.7989961
   DALECKI D, 1995, J ACOUST SOC AM, V97, P3165, DOI 10.1121/1.411877
   GREENSPAN JD, 1984, SOMATOSENS RES, V1, P379, DOI 10.3109/07367228409144556
   Iwamoto T, 2005, WORLD HAPTICS CONFERENCE: FIRST JOINT EUROHAPTICS CONFERENCE AND SYMPOSIUM ON HAPTIC INTERFACES FOR VIRUTUAL ENVIRONMENT AND TELEOPERATOR SYSTEMS, PROCEEDINGS, P220
   JOHANSSON RS, 1979, J PHYSIOL-LONDON, V297, P405, DOI 10.1113/jphysiol.1979.sp013048
   Kajimoto H, 2004, IEEE COMPUT GRAPH, V24, P36, DOI 10.1109/MCG.2004.1255807
   Kim JW, 2014, INT CONF UBIQ ROBOT, P98, DOI 10.1109/URAI.2014.7057405
   KNIBESTO.M, 1973, J PHYSIOL-LONDON, V232, P427, DOI 10.1113/jphysiol.1973.sp010279
   KNIBESTOL M, 1975, J PHYSIOL-LONDON, V245, P63, DOI 10.1113/jphysiol.1975.sp010835
   Lee W, 2014, INT J IMAG SYST TECH, V24, P167, DOI 10.1002/ima.22091
   LINDBLOM U, 1965, J NEUROPHYSIOL, V28, P966, DOI 10.1152/jn.1965.28.5.966
   Obrist M., 2013, P SIGCHI C HUMAN FAC, P1659, DOI [10.1145/2470654.2466220, DOI 10.1145/2470654.2466220]
   PUBOLS BH, 1987, SOMATOSENS RES, V4, P273, DOI 10.3109/07367228709144611
   Sato K, 2011, IEEE INT CONF ROBOT, P1120, DOI 10.1109/ICRA.2011.5980271
   Tsirulnikov, 2002, NONLINEAR ACOUST BEG, P445
   WETHERILL GB, 1965, BRIT J MATH STAT PSY, V18, P1, DOI 10.1111/j.2044-8317.1965.tb00689.x
   Yang GH, 2006, IEEE INT CONF ROBOT, P3917, DOI 10.1109/ROBOT.2006.1642302
NR 19
TC 0
Z9 0
U1 1
U2 7
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD JAN
PY 2021
VL 18
IS 1
AR 1
DI 10.1145/3422820
PG 8
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA PZ2NW
UT WOS:000612577900001
DA 2024-07-18
ER

PT J
AU Hooge, KDO
   Baragchizadeh, A
   Karnowski, TP
   Bolme, DS
   Ferrell, R
   Jesudasen, PR
   Castillo, CD
   O'toole, AJ
AF Hooge, Kimberley D. Orsten
   Baragchizadeh, Asal
   Karnowski, Thomas P.
   Bolme, David S.
   Ferrell, Regina
   Jesudasen, Parisa R.
   Castillo, Carlos D.
   O'toole, Alice J.
TI Evaluating Automated Face Identity-Masking Methods with Human Perception
   and a Deep Convolutional Neural Network
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE De-identification; DCNN; masking algorithms; privacy
ID RECOGNITION; PRIVACY
AB Face de-identification (or "masking") algorithms have been developed in response to the prevalent use of video recordings in public places. We evaluated the success of face identity masking for human perceivers and a deep convolutional neural network (DCNN). Eight de-identification algorithms were applied to videos of drivers' faces, while they actively operated a motor vehicle. These masks were pre-selected to be applicable to low-quality video and to maintain coarse information about facial actions. Humans studied high-resolution images to learn driver identities and were tested on their recognition of active drivers in low-resolution videos. Faces in the videos were either unmasked or were masked by one of the eight algorithms. When participants were tested immediately after learning (Experiment 1), all masks reduced identification, with six of eight masks reducing identification to extremely poor performance. In a second experiment, two of the most effective masks were tested after a delay of 7 or 28 days. The delay did not further reduce identification of the masked faces. In all masked conditions, participants maintained stringent decision criteria, with low confidence in recognition, further indicating the effectiveness of the masks. Next, the DCNN performed an identity-matching task between high-resolution images and masked videos-a task analogous to that done by humans. The pattern of accuracy for the DCNN mirrored some, but not all, aspects of human performance, highlighting the need to test the effectiveness of identity masking for both humans and machines. The DCNN was also tested on its ability to match identity between masked and unmasked versions of the same video, based only on the face. DCNN performance for the eight masks offers insight into the nature of the information in faces that is coded in these networks.
C1 [Hooge, Kimberley D. Orsten; Baragchizadeh, Asal; Jesudasen, Parisa R.; O'toole, Alice J.] Univ Texas Dallas, 800 West Campbell Rd, Richardson, TX 75080 USA.
   [Karnowski, Thomas P.; Bolme, David S.; Ferrell, Regina] Oak Ridge Natl Lab, 1 Bethel Valley Rd, Oak Ridge, TN 37831 USA.
   [Castillo, Carlos D.] Univ Maryland, 8600 Datapoint Dr, College Pk, MD 20742 USA.
C3 University of Texas System; University of Texas Dallas; United States
   Department of Energy (DOE); Oak Ridge National Laboratory; University
   System of Maryland; University of Maryland College Park
RP Hooge, KDO (corresponding author), Univ Texas Dallas, 800 West Campbell Rd, Richardson, TX 75080 USA.
EM kdoh@utdallas.edu; asal.baragchizadeh@utdallas.edu;
   karnowskitp@ornl.gov; bolmeds@ornl.gov; ferrellrk@ornl.gov;
   Parisa.Jesudasen@utdallas.edu; carlos@umiacs.umd.edu;
   otoole@utdallas.edu
OI Orsten-Hooge, Kimberley/0000-0001-7192-1278; Karnowski,
   Thomas/0000-0002-0376-4917
FU Oak Ridge National Laboratory; Federal Highway Administration under the
   Exploratory Advanced Research Program; Intelligence Advanced Research
   Projects Activity (IARPA); Office of the Director of National
   Intelligence (ODNI), Intelligence Advanced Research Projects Activity
   (IARPA), via IARPA RD [2014-14071600012]
FX This work was supported through collaboration with Oak Ridge National
   Laboratory and the Federal Highway Administration under the Exploratory
   Advanced Research Program (Contracting Officer's Representative: Lincoln
   Cobb). The human experiment and analysis was subcontracted to the
   University of Texas at Dallas from Oak Ridge National Laboratory. The
   CNN feature extraction was carried out at the University of Maryland by
   C. C., who was supported by the Intelligence Advanced Research Projects
   Activity (IARPA). The UMD part of the research is based upon work
   supported by the Office of the Director of National Intelligence (ODNI),
   Intelligence Advanced Research Projects Activity (IARPA), via IARPA R&D
   Contract No. 2014-14071600012.
CR Alonso VE, 2017, J ELECTRON IMAGING, V26, DOI 10.1117/1.JEI.26.2.023015
   [Anonymous], 2015, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2015.7298803
   [Anonymous], 1996, CSCW '96: Proceedings of the 1996 ACM conference on Computer supported cooperative work
   [Anonymous], 2014, DMASK REL ID MASK SY
   [Anonymous], 1999, Handbook of Computer Vision and Applications
   [Anonymous], 2018, ABS180401159 CORR
   [Anonymous], 2018, ACM T MULTIMEDIA COM
   Bansal Ankan, 2017, 2017 IEEE International Joint Conference on Biometrics (IJCB), P464, DOI 10.1109/BTAS.2017.8272731
   Bansal A, 2017, IEEE INT CONF COMP V, P2545, DOI 10.1109/ICCVW.2017.299
   Baragchizadeh A, 2017, IEEE INT CONF AUTOMA, P378, DOI 10.1109/FG.2017.54
   Blanz V, 2003, IEEE T PATTERN ANAL, V25, P1063, DOI 10.1109/TPAMI.2003.1227983
   Boyle Michael., 2000, Proc. ACM CSCW 2000, P1
   Brainard DH, 1997, SPATIAL VISION, V10, P433, DOI 10.1163/156856897X00357
   Brkic K, 2017, IEEE COMPUT SOC CONF, P1319, DOI 10.1109/CVPRW.2017.173
   Bruce V, 1999, J EXP PSYCHOL-APPL, V5, P339, DOI 10.1037/1076-898X.5.4.339
   Burton AM, 2010, BEHAV RES METHODS, V42, P286, DOI 10.3758/BRM.42.1.286
   Burton AM, 1999, PSYCHOL SCI, V10, P243, DOI 10.1111/1467-9280.00144
   Campbell K.L., 2012, TRANSPORTATION RES N, P30
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851
   Chao YW, 2017, PROC CVPR IEEE, P3643, DOI 10.1109/CVPR.2017.388
   Chen JW, 2018, IEEE COMPUT SOC CONF, P1651, DOI 10.1109/CVPRW.2018.00207
   Cheung S.-C.S., 2009, PROTECTING PRIVACY V, P11
   Chriskos P, 2017, IEEE GLOB CONF SIG, P403, DOI 10.1109/GlobalSIP.2017.8308673
   Gross R., 2009, Protecting privacy in video surveillance, P129
   He K., 2016, PROC CVPR IEEE, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]
   Huang D, 2012, LECT NOTES COMPUT SC, V7573, P144, DOI 10.1007/978-3-642-33709-3_11
   Huang G. B., 2008, WORKSH FAC REAL LIF
   Jourabloo A, 2015, INT CONF BIOMETR, P278, DOI 10.1109/ICB.2015.7139096
   KAZEMI V, 2014, PROC CVPR IEEE, P1867, DOI [DOI 10.1109/CVPR.2014.241, 10.1109/CVPR.2014.241]
   Kemp R, 1997, APPL COGNITIVE PSYCH, V11, P211
   King DE, 2009, J MACH LEARN RES, V10, P1755
   Kleiner M, 2007, PERCEPTION, V36, P14
   Letournel G, 2015, IEEE IMAGE PROC, P4366, DOI 10.1109/ICIP.2015.7351631
   LI T., 2019, P IEEE C COMP VIS PA, P0
   Li YF, 2017, IEEE COMPUT SOC CONF, P1343, DOI 10.1109/CVPRW.2017.176
   Lin WA, 2017, IEEE INT CONF AUTOMA, P294, DOI 10.1109/FG.2017.134
   Ma XJ, 2018, IEEE CONF COMPUT, P813, DOI 10.1109/INFCOMW.2018.8406880
   Macmillan N. A., 2005, Detection theory: A user's guide, V2nd
   MATLAB, 2014, VERS 7 10 0 R2014AB
   Maze B, 2018, INT CONF BIOMETR, P158, DOI 10.1109/ICB2018.2018.00033
   Meden B, 2017, IET SIGNAL PROCESS, V11, P1046, DOI 10.1049/iet-spr.2017.0049
   Mirjalili V, 2018, INT CONF BIOMETR, P82, DOI 10.1109/ICB2018.2018.00023
   Newton EM, 2005, IEEE T KNOWL DATA EN, V17, P232, DOI 10.1109/TKDE.2005.32
   O'Toole AJ, 2007, IEEE T PATTERN ANAL, V29, P1642, DOI 10.1109/TPAMI.2007.1107
   OToole A. J., 2008, 8 INT C AUT FAC GEST
   Paone J, 2015, IEEE INT VEH SYM, P174, DOI 10.1109/IVS.2015.7225682
   Parkhi OM, 2015, DEEP FACE RECOGNITIO
   Pelli DG, 1997, SPATIAL VISION, V10, P437, DOI 10.1163/156856897X00366
   Phillips P. J., 2007, 7408 NISTIR
   Phillips PJ, 2018, P NATL ACAD SCI USA, V115, P6171, DOI 10.1073/pnas.1721355115
   Phillips PJ, 1998, IEEE T IMAGE PROCESS, V7, P1150, DOI 10.1109/83.704308
   Ranjan R, 2018, IEEE SIGNAL PROC MAG, V35, P66, DOI 10.1109/MSP.2017.2764116
   Ranjan R, 2017, IEEE INT CONF AUTOMA, P17, DOI 10.1109/FG.2017.137
   Ribaric S, 2015, IEEE INT CONF AUTOMA
   Russell R, 2006, PERCEPTION, V35, P749, DOI 10.1068/p5490
   Sankaranarayanan S., 2016, 2016 IEEE 8th international conference on biometrics theory, applications and systems (BTAS), P1
   Shi FG, 1998, IEEE INT INTERC TECH, P73, DOI 10.1109/IITC.1998.704755
   Sinha P, 2006, P IEEE, V94, P1948, DOI 10.1109/JPROC.2006.884093
   Sun Y, 2015, PROC CVPR IEEE, P2892, DOI 10.1109/CVPR.2015.7298907
   TURK MA, 1991, 1991 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, P586
   White D, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0139827
   Whitelam Cameron, 2017, CVPR WORKSH, V2
   Wu YF, 2019, J COMPUT SCI TECH-CH, V34, P47, DOI 10.1007/s11390-019-1898-8
   Xiong XH, 2013, PROC CVPR IEEE, P532, DOI 10.1109/CVPR.2013.75
   YIN RK, 1969, J EXP PSYCHOL, V81, P141, DOI 10.1037/h0027474
   Yip AW, 2002, PERCEPTION, V31, P995, DOI 10.1068/p3376
NR 66
TC 2
Z9 2
U1 0
U2 14
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD JAN
PY 2021
VL 18
IS 1
AR 3
DI 10.1145/3422988
PG 20
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA PZ2NW
UT WOS:000612577900003
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Wang, X
   Bylinskii, Z
   Hertzmann, A
   Pepperell, R
AF Wang, Xi
   Bylinskii, Zoya
   Hertzmann, Aaron
   Pepperell, Robert
TI Toward Quantifying Ambiguities in Artistic Images
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article; Proceedings Paper
CT 17th ACM Symposium on Applied Perception (SAP)
CY SEP, 2020
CL ELECTR NETWORK
SP ACM
DE Datasets; generative adversarial networks (GAN); image descriptions;
   text tagging; aesthetics
ID LONG-TERM-MEMORY; INDETERMINACY
AB It has long been hypothesized that perceptual ambiguities play an important role in aesthetic experience: A work with some ambiguity engages a viewer more than one that does not. However, current frameworks for testing this theory are limited by the availability of stimuli and data collection methods. This article presents an approach to measuring the perceptual ambiguity of a collection of images. Crowdworkers are asked to describe image content, after different viewing durations. Experiments are performed using images created with Generative Adversarial Networks, using the Artbreeder website. We show that text processing of viewer responses can provide a fine-grained way to measure and describe image ambiguities.
C1 [Wang, Xi] TU Berlin, Marchstr 23, D-10587 Berlin, Germany.
   [Wang, Xi] MIT, Marchstr 23, D-10587 Berlin, Germany.
   [Bylinskii, Zoya] Adobe Res, 1 Broadway, Cambridge, MA 02142 USA.
   [Hertzmann, Aaron] Adobe Res, 601 Townsend St, San Francisco, CA 94103 USA.
   [Pepperell, Robert] Cardiff Metropolitan Univ, Fovolab, 200 Western Ave, Cardiff CF5 2YB, Wales.
C3 Technical University of Berlin; Adobe Systems Inc.; Adobe Systems Inc.;
   Cardiff Metropolitan University
RP Wang, X (corresponding author), TU Berlin, Marchstr 23, D-10587 Berlin, Germany.; Wang, X (corresponding author), MIT, Marchstr 23, D-10587 Berlin, Germany.
EM xi.wang@tu-berlin.de; bylinski@adobe.com; hertzman@dgp.toronto.edu;
   rpepperell@cardiffmet.ac.uk
OI Wang, Xi/0000-0001-5442-1116; Pepperell, Robert/0000-0002-4728-4122
CR [Anonymous], 2019, P INT C LEARN REPR I
   Bird Steven, 2009, NATURAL LANGUAGE PRO, DOI DOI 10.1007/S10579-010-9124-X
   Boros Tiberiu, 2018, P CONLL 2018 SHARED, P171
   Brady TF, 2008, P NATL ACAD SCI USA, V105, P14325, DOI 10.1073/pnas.0803390105
   Brady TF, 2013, PSYCHOL SCI, V24, P981, DOI 10.1177/0956797612465439
   Carbon CC, 2017, I-PERCEPTION, V8, DOI 10.1177/2041669517694184
   Cowling Elizabeth., 2006, Visiting Picasso: The Notebooks and Letters of Roland Penrose
   Daw Nathaniel D., 2007, P C NEUR INF PROC SY
   Fairhall SL, 2008, CONSCIOUS COGN, V17, P923, DOI 10.1016/j.concog.2007.07.005
   Fei-Fei L, 2007, J VISION, V7, DOI 10.1167/7.1.10
   Fosco C., 2020, P IEEE CVF C COMP VI
   Gamboni Dario., 2004, Potential Images: Ambiguity and Indeterminacy in Modern Art
   Gingold Y, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2231816.2231817
   Goodfellow Ian J., 2014, P C NEUR INF PROC SY
   Hertzmann A, 2020, LEONARDO, V53, P424, DOI [10.1162/LEON_a_01930, 10.1145/3386567.3388574]
   Hertzmann Aaron, 2010, P INT S NONPH AN REN
   Hofmann T, 1999, SIGIR'99: PROCEEDINGS OF 22ND INTERNATIONAL CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P50, DOI 10.1145/312624.312649
   Ishai Alumit, 2007, BRAIN RES B, V73, P4
   Jakesch M, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0074084
   Koenderink JJ, 2001, PERCEPTION, V30, P431, DOI 10.1068/p3030
   Muth C, 2018, PSYCHOL AESTHET CREA, V12, P11, DOI 10.1037/aca0000113
   Muth C, 2016, FRONT HUM NEUROSCI, V10, DOI 10.3389/fnhum.2016.00043
   Muth C, 2016, ART PERCEPT, V4, P145, DOI 10.1163/22134913-00002049
   Muth C, 2015, PSYCHOL AESTHET CREA, V9, P206, DOI 10.1037/a0038814
   Muth C, 2013, ACTA PSYCHOL, V144, P25, DOI 10.1016/j.actpsy.2013.05.001
   Oliva A., 2009, ENCY PERCEPTION, P1112, DOI [10.4135/9781412972000.n355, DOI 10.4135/9781412972000.N355]
   Pepperell R, 2015, FRONT HUM NEUROSCI, V9, DOI 10.3389/fnhum.2015.00295
   Pepperell R, 2011, FRONT HUM NEUROSCI, V5, DOI 10.3389/fnhum.2011.00084
   POTTER MC, 1975, SCIENCE, V187, P965, DOI 10.1126/science.1145183
   Potter MC, 1999, MIT BRAD COGN PSYCH, P13
   Richter Gerhard Nicolaus., 2009, Gerhard Richter-Text: Writings, Interviews and Letters, 1961-2007
   Russell Bryan, 2007, P INT C COMP VIS ICC
   SCHYNS PG, 1994, PSYCHOL SCI, V5, P195, DOI 10.1111/j.1467-9280.1994.tb00500.x
   Torralba A, 2009, VISUAL NEUROSCI, V26, P123, DOI 10.1017/S0952523808080930
   Van de Cruys Sander, 2011, I PERCEPT, V2, P9
   Wallraven C., 2007, COMPUTATIONAL AESTHE, P121
   Wallraven C, 2007, APGV 2007: SYMPOSIUM ON APPLIED PERCEPTION IN GRAPHICS AND VISUALIZATION, PROCEEDINGS, P115
NR 37
TC 4
Z9 4
U1 1
U2 8
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD DEC
PY 2020
VL 17
IS 4
SI SI
AR 13
DI 10.1145/3418054
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA PH1UI
UT WOS:000600206200002
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Grogorick, S
   Albuquerque, G
   Tauscher, JP
   Magnor, M
AF Grogorick, Steve
   Albuquerque, Georgia
   Tauscher, Jan-Philipp
   Magnor, Marcus
TI Comparison of Unobtrusive Visual Guidance Methods in an Immersive Dome
   Environment
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article; Proceedings Paper
CT 15th ACM Symposium on Applied Perception (SAP) colocated with SIGGRAPH
   Conference
CY AUG, 2018
CL Vancouver, CANADA
SP ACM, SIGGRAPH
DE Virtual Reality; dome; unobtrusive gaze guidance; perception; eye
   tracking; post-processing
ID SALIENCY
AB In this article, we evaluate various image-space modulation techniques that aim to unobtrusively guide viewers' attention. While previous evaluations mainly target desktop settings, we examine their applicability to ultrawide field of view immersive environments, featuring technical characteristics expected for future-generation head-mounted displays. A custom-built, high-resolution immersive dome environment with high-precision eye tracking is used in our experiments. We investigate gaze guidance success rates and unobtrusiveness of five different techniques. Our results show promising guiding performance for four of the tested methods. With regard to unobtrusiveness we find that-while no method remains completely unnoticed-many participants do not report any distractions. The evaluated methods show promise to guide users' attention also in a wide field of virtual environment applications, e.g., virtually guided tours or field operation training.
C1 [Grogorick, Steve; Albuquerque, Georgia; Tauscher, Jan-Philipp; Magnor, Marcus] TU Braunschweig, Braunschweig, Germany.
C3 Braunschweig University of Technology
RP Grogorick, S (corresponding author), TU Braunschweig, Braunschweig, Germany.
EM grogorick@cg.cs.tu-bs.de; georgia@cg.cs.tu-bs.de;
   tauscher@cg.cs.tu-bs.de; magnor@cg.cs.tu-bs.de
OI Magnor, Marcus/0000-0003-0579-480X; Grogorick,
   Steve/0000-0003-2837-2642; Tauscher, Jan-Philipp/0000-0003-2407-391X
FU German Science Foundation dfg [DFG MA2555/15-1, DFG INST 188/409-1 FUGG]
FX The authors gratefully acknowledge funding by the German Science
   Foundation dfg under Grant No. DFG MA2555/15-1, "Immersive Digital
   Reality," and DFG INST 188/409-1 FUGG, "ICG Dome." All 360. images CC-BY
   (in order of appearance in Figure 1): heiwa4126, Angel M. Felicisimo,
   Igors Jefimovs, MartinThoma, Wdejager, David Iliff, Ansgar Koreng (x2),
   Galopax, Triffski, XL Catlin Seaview Survey (x2) [Wikimedia Commons]. We
   thank Linda Eckardt (wi2, TU Braunschweig) for recruiting participants.
CR [Anonymous], 2015, NEUROLOGY EYE MOVEME
   Bailey R, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1559755.1559757
   Barth Erhardt, 2006, ELECT IMAGING 2006
   Booth Thomas, 2013, Proceedings of the ACM Symposium on Applied Perception, P75, DOI 10.1145/2492494.2492508
   Breeden K, 2017, ACM T APPL PERCEPT, V14, DOI 10.1145/3127588
   Cole F., 2006, EUROGRAPHICS S RENDE, P377
   Dodge R, 1900, PSYCHOL REV, V7, P454, DOI 10.1037/h0067215
   Dorr M, 2004, DYNAMIC PERCEPTION W, P89
   Grogorick S, 2017, ACM SYMPOSIUM ON APPLIED PERCEPTION (SAP 2017), DOI 10.1145/3119881.3119890
   Grogorick Steve, 2018, 1 TU BRAUNSCHW I F C
   Hagiwara Aiko, 2011, P 1 INT WORKSH PERV
   Hata Hajime., 2016, Proceedings of the International Working Conference on Advanced Visual Interfaces - AVI'16, P28
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Kennedy RS, 1993, The international journal of aviation psychology, V3, P203, DOI [DOI 10.1207/S15327108IJAP0303, 10.1207/s15327108ijap0303_3]
   Kosara R, 2002, IEEE COMPUT GRAPH, V22, P22, DOI 10.1109/38.974515
   Lantz E., 2003, SIGGRAPH COURSES PRO
   Lin YC, 2017, PROCEEDINGS OF THE 2017 ACM SIGCHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'17), P2535, DOI 10.1145/3025453.3025757
   Lintu A, 2009, GAZE GUIDANCE PERIPH
   McNamara A, 2009, ACM T APPL PERCEPT, V6, DOI 10.1145/1577755.1577760
   Rosenholtz R, 2016, ANNU REV VIS SCI, V2, P437, DOI 10.1146/annurev-vision-082114-035733
   Serrano A, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073668
   Sheikh A., 2016, Directing attention in 360-degree video [Paper presentation]. IBC 2016 Conference, DOI [10.1049/ibc.2016.0029, DOI 10.1049/IBC.2016.0029]
   Sitzmann V, 2018, IEEE T VIS COMPUT GR, V24, P1633, DOI 10.1109/TVCG.2018.2793599
   Veas E, 2011, 29TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P1471
NR 24
TC 10
Z9 10
U1 1
U2 9
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD OCT
PY 2018
VL 15
IS 4
AR 27
DI 10.1145/3238303
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA HJ4HT
UT WOS:000457135800005
DA 2024-07-18
ER

PT J
AU Ramesh, G
   Turner, M
   Schröder, B
   Wortmann, F
AF Ramesh, Girish
   Turner, Martin
   Schroeder, Bjoern
   Wortmann, Franz
TI Analysis of Hair Shine Using Rendering and Subjective Evaluation
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article; Proceedings Paper
CT 15th ACM Symposium on Applied Perception (SAP) colocated with SIGGRAPH
   Conference
CY AUG, 2018
CL Vancouver, CANADA
SP ACM, SIGGRAPH
DE Hair shine; hair rendering; perception
ID LIGHT-SCATTERING; PERCEPTION; DIAMETER; COLOR
AB Hair shine is a highly desirable attribute to consumers within the cosmetic industry and is also an important indicator of hair health. However, perceptual evaluation of shine is a complex task as it is known that even subtle manipulation of local hair properties such as colour, thickness, or style and global properties such as lighting or environment can affect the evaluation. In this article, we are interested in the physical, optical, and chemical characteristics that affect the realism of hair along with the perception of shine. We have constructed a Computer Graphics (CG) setup, based on current physical testing systems, that reduces the number of variables that affect the perspective. Physically based shading models were used to create the images that participants assessed on realism, health, naturalness, and shine through three different evaluation experiments. Our results provide new insights on how hair is perceived, the factors that affect its realism, and the potential of using CG techniques in the cosmetic industry to replace physical testing.
C1 [Ramesh, Girish] Univ Manchester, D33,Sackville St Bldg, Manchester M13 9PL, Lancs, England.
   [Turner, Martin] Univ Manchester, IT Serv, Room B39,Sackville St Bldg, Manchester M13 9PL, Lancs, England.
   [Schroeder, Bjoern] BASF Personal Care & Nutr GmbH, Henkelstr 67, D-40589 Dusseldorf, Germany.
   [Wortmann, Franz] Univ Manchester, Mezz 5,Sackville St Bldg, Manchester M13 9PL, Lancs, England.
C3 University of Manchester; University of Manchester; BASF; University of
   Manchester
RP Ramesh, G (corresponding author), Univ Manchester, D33,Sackville St Bldg, Manchester M13 9PL, Lancs, England.
EM ramesh@manchester.ac.uk; martin.turner@manchester.ac.uk;
   bjoern.a.schroeder@basf.com; franz.wortmann@manchester.ac.uk
OI Turner, Martin/0000-0003-0117-8049; Ramesh, Girish/0000-0003-3148-803X
CR Blender Online Community, 2017, BLEND 3D MOD REND PA
   BUSTARD HK, 1991, APPL OPTICS, V30, P3485, DOI 10.1364/AO.30.003485
   Chiang Matt Jen-Yuan, 2015, Proc. of ACM SIGGRAPH Talks, DOI DOI 10.1145/2775280.2792559
   d'Eon E, 2011, COMPUT GRAPH FORUM, V30, P1181, DOI 10.1111/j.1467-8659.2011.01976.x
   dEon Eugene, 2013, P SIGGRAPH AS 2013 T, DOI 10.1145/2542355.2542386
   Donner C., 2006, P 17 EUROGRAPHICS C, P409
   Eisfeld Wolf, 2015, IFSCC MAGAZINE, V18, P3
   Fink B, 2013, J COSMET DERMATOL-US, V12, P78, DOI 10.1111/jocd.12017
   Frost P, 2006, EVOL HUM BEHAV, V27, P85, DOI 10.1016/j.evolhumbehav.2005.07.002
   Gao T, 2009, J COSMET SCI, V60, P187
   Hery Christophe., 2012, Journal of Computer Graphics Techniques (JCGT), V1, P1
   Kajiya J. T., 1989, Computer Graphics, V23, P271, DOI 10.1145/74334.74361
   Keis K, 2004, J COSMET SCI, V55, P423
   Keis K., 2004, Int. J. Cosmet. Sci, DOI [10.1111/j.0142-5463.2004.00223_4.x, DOI 10.1111/J.0142-5463.2004.00223_4.X]
   Marschner SR, 2003, ACM T GRAPHIC, V22, P780, DOI 10.1145/882262.882345
   Mesko N, 2004, HUM NATURE-INT BIOS, V15, P251, DOI 10.1007/s12110-004-1008-6
   Moon JT, 2006, ACM T GRAPHIC, V25, P1067, DOI 10.1145/1141911.1141995
   Moon JT, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360630
   Ou JW, 2012, COMPUT GRAPH FORUM, V31, P1537, DOI 10.1111/j.1467-8659.2012.03150.x
   Ramesh Girish, 2013, THEORY PRACTICE COMP, P73
   Robbins C, 2012, BRIT J DERMATOL, V167, P324, DOI 10.1111/j.1365-2133.2012.11010.x
   ROBBINS CR, 1986, J SOC COSMET CHEM, V37, P141
   SAATY RW, 1987, MATH MODELLING, V9, P161, DOI 10.1016/0270-0255(87)90473-8
   Ski Hugh, 2012, COLORHUG2 OPEN SOURC
   STAMM RF, 1977, J SOC COSMET CHEM, V28, P571
   Worley S., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P291, DOI 10.1145/237170.237267
   Wortmann FJ, 2004, J COSMET SCI, V55, P81
   Wortmann Franz, 2013, P 18 INT HAIR SCI S
   Yan LQ, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073600
   Yu Yizhou, 2001, P 9 PAC C COMP GRAPH
   Zinke A, 2007, IEEE T VIS COMPUT GR, V13, P342, DOI 10.1109/TVCG.2007.43
   Zinke A, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360631
NR 32
TC 0
Z9 0
U1 1
U2 7
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD OCT
PY 2018
VL 15
IS 4
AR 25
DI 10.1145/3274478
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA HJ4HT
UT WOS:000457135800003
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Kneusel, RT
   Mozer, MC
AF Kneusel, Ronald T.
   Mozer, Michael C.
TI Improving Human-Machine Cooperative Visual Search With Soft Highlighting
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Visual search; soft highlighting; target localization
ID COMPUTER-AIDED DETECTION; DETECTION CAD; MAMMOGRAPHY; CLASSIFICATION;
   ENHANCEMENT; PERFORMANCE; UNCERTAINTY; MASSES; SYSTEM
AB Advances in machine learning have produced systems that attain human-level performance on certain visual tasks, e.g., object identification. Nonetheless, other tasks requiring visual expertise are unlikely to be entrusted to machines for some time, e.g., satellite and medical imagery analysis. We describe a human-machine cooperative approach to visual search, the aim of which is to outperform either human or machine acting alone. The traditional route to augmenting human performance with automatic classifiers is to draw boxes around regions of an image deemed likely to contain a target. Human experts typically reject this type of hard highlighting. We propose instead a soft highlighting technique in which the saliency of regions of the visual field is modulated in a graded fashion based on classifier confidence level. We report on experiments with both synthetic and natural images showing that soft highlighting achieves a performance synergy surpassing that attained by hard highlighting.
C1 [Kneusel, Ronald T.; Mozer, Michael C.] Univ Colorado, Dept Comp Sci, Boulder, CO 80309 USA.
C3 University of Colorado System; University of Colorado Boulder
RP Kneusel, RT (corresponding author), Univ Colorado, Dept Comp Sci, Boulder, CO 80309 USA.
EM ron@kneusel.org; mozer@colorado.edu
FU NSF [SES-1461535, DRL-1631428, SBE-0542013, SMA-1041755]
FX This research was supported by NSF grants SES-1461535, DRL-1631428,
   SBE-0542013, and SMA-1041755.
CR Alberdi E, 2004, ACAD RADIOL, V11, P909, DOI 10.1016/j.acra.2004.05.012
   ANDERSON JR, 1971, PHOTOGRAMM ENG, V37, P379
   Balleyguier C, 2005, EUR J RADIOL, V54, P90, DOI 10.1016/j.ejrad.2004.11.021
   Bankman I., 2008, HDB MED IMAGE PROCES
   Bastin L, 2002, COMPUT GEOSCI-UK, V28, P337, DOI 10.1016/S0098-3004(01)00051-6
   Cheng HD, 2003, PATTERN RECOGN, V36, P2967, DOI 10.1016/S0031-3203(03)00192-4
   Cunningham C. A., 2016, ANALOG COMPUTE UNPUB
   D'Orsi CJ, 2001, RADIOLOGY, V221, P585, DOI 10.1148/radiol.2213011476
   Drew T, 2012, ACAD RADIOL, V19, P1260, DOI 10.1016/j.acra.2012.05.013
   Dzindolet MT, 2003, INT J HUM-COMPUT ST, V58, P697, DOI 10.1016/S1071-5819(03)00038-7
   Freer TW, 2001, RADIOLOGY, V220, P781, DOI 10.1148/radiol.2203001282
   GIGER ML, 1993, RADIOGRAPHICS, V13, P647, DOI 10.1148/radiographics.13.3.8316671
   Green D. M., 1966, SIGNAL DETECTION THE
   Hengl T., 2006, 7th International Symposium on Spatial Accuracy Assessment in Natural Resources and Environmental Sciences, P805
   Hupse R, 2013, RADIOLOGY, V266, P123, DOI 10.1148/radiol.12120218
   Kaur M, 2011, INT J ADV COMPUT SC, V2, P137
   Kneusel R. T., 2016, THESIS
   KRUPINSKI EA, 1993, PERCEPT PSYCHOPHYS, V53, P519, DOI 10.3758/BF03205200
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Lee SK, 2000, INT J MED INFORM, V60, P29, DOI 10.1016/S1386-5056(00)00067-8
   Lehman C. D., 2016, JAMA INTERN MED, V11, P1828
   MacEachren A. M., 2005, CARTOGR GOEGR INFOR, V32, P139, DOI [10.1559/1523040054738936, DOI 10.1559/1523040054738936]
   Masson MEJ, 2003, CAN J EXP PSYCHOL, V57, P203, DOI 10.1037/h0087426
   Mozer M.C., 1991, PERCEPTION MULTIPLE
   Parasuraman R, 1997, HUM FACTORS, V39, P230, DOI 10.1518/001872097778543886
   Philpotts LE, 2009, RADIOLOGY, V253, P17, DOI 10.1148/radiol.2531090689
   Richards J.A., 2006, Remote Sensing Digital Image Analysis: An Introduction, Vfourth
   Sahiner B, 2009, ACAD RADIOL, V16, P1518, DOI 10.1016/j.acra.2009.08.006
   Samulski M., 2009, P SOC PHOTO-OPT INS, V7263
   Samulski M, 2010, EUR RADIOL, V20, P2323, DOI 10.1007/s00330-010-1821-8
   Snoek J., 2012, Advances in Neural Information Processing Systems, V25, DOI DOI 10.48550/ARXIV.1206.2944
   TREISMAN AM, 1980, COGNITIVE PSYCHOL, V12, P97, DOI 10.1016/0010-0285(80)90005-5
   van der Wel FJM, 1998, COMPUT GEOSCI-UK, V24, P335, DOI 10.1016/S0098-3004(97)00120-9
   VANGENDEREN JL, 1978, REMOTE SENS ENVIRON, V7, P3, DOI 10.1016/0034-4257(78)90003-2
   Yasmin Mussarat., 2012, World Applied Sciences Journal, V17, P1192
   Zheng B, 2004, ACAD RADIOL, V11, P398, DOI 10.1016/S1076-6332(03)00677-9
   Zheng B, 2001, RADIOLOGY, V221, P633, DOI 10.1148/radiol.2213010308
NR 38
TC 9
Z9 13
U1 0
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD NOV
PY 2017
VL 15
IS 1
AR 3
DI 10.1145/3129669
PG 21
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA FU0CU
UT WOS:000423519800003
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Piorkowski, R
   Mantiuk, R
   Siekawa, A
AF Piorkowski, Rafal
   Mantiuk, Radoslaw
   Siekawa, Adam
TI Automatic Detection of Game Engine Artifacts Using Full Reference Image
   Quality Metrics
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Game engine artifacts; peter panning; shadow acne; aliasing; z-fighting;
   shadow mapping artifacts; image quality metrics; perceptual experiments
AB Contemporary game engines offer an outstanding graphics quality but they are not free from visual artifacts. A typical example is aliasing, which, despite advanced antialiasing techniques, is still visible to the game players. Essential deteriorations are the shadow acne and peter panning responsible for deficiency of the shadow mapping technique. Also Z-fighting, caused by the incorrect order of drawing polygons, significantly affects the quality of the graphics and makes the gameplay more difficult. In this work, we propose a technique, in which visibility of deteriorations is uncovered by the objective image quality metrics (IQMs). We test the efficiency of a simple mathematically based metric and advanced IQMs: a Spatial extension of CIELAB (S-CIELAB), the Structural SIMilarity Index (SSIM), the Multiscale Structural SIMilarity Index (MS-SSIM), and the High Dynamic Range Visual Difference Predictor-2 (HDR-VDP-2). Additionally, we evaluate the Color Image Difference (CID) metric, which is recommended to detect the differences in colors. To find out which metric is the most effective for the detection of the game engine artifacts, we build a database of manually marked images with representative set of artifacts. We conduct subjective experiments in which people manually mark the visible local artifacts in the screenshots from the games. Then the detection maps averaged over a number of observers are compared with results generated by IQMs. The obtained results show that SSIM and MS-SSIM metrics outperform other techniques. However, the results are not indisputable, because, for small and scattered aliasing artifacts, HDR-VDP-2 metrics report the results most consistent with the average human observer. As a proof of concept, we propose an application in which resolution of the shadow maps is controlled by the SSIM metric to avoid perceptually visible aliasing artifacts on the shadow edges.
C1 [Piorkowski, Rafal; Mantiuk, Radoslaw; Siekawa, Adam] West Pomeranian Univ Technol, Zolnierska 52, PL-71210 Szczecin, Poland.
C3 West Pomeranian University of Technology
RP Piorkowski, R (corresponding author), West Pomeranian Univ Technol, Zolnierska 52, PL-71210 Szczecin, Poland.
EM rpiorkowski@wi.zut.edu.pl; rmantiuk@wi.zut.edu.pl;
   asiekawa@wi.zut.edu.pl
RI Mantiuk, Radoslaw/I-5622-2012
OI Mantiuk, Radoslaw/0000-0002-1959-3497
FU Polish National Science Centre [DEC-2013/09/B/ST6/02270]
FX This work is supported by the Polish National Science Centre (grant
   number DEC-2013/09/B/ST6/02270).
CR Alam MM, 2014, J VISION, V14, DOI 10.1167/14.8.22
   [Anonymous], IEEE AS C SIGN SYST
   [Anonymous], 2018, Real-Time Rendering
   [Anonymous], 2006, MODERN IMAGE QUALITY
   Baldi P., 2000, BIOINFORMATICS, V16, P640
   Benoit A, 2008, EURASIP J IMAGE VIDE, DOI 10.1155/2008/659024
   Cadík M, 2013, COMPUT GRAPH FORUM, V32, P401, DOI 10.1111/cgf.12248
   Cadík M, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366166
   Corsini M, 2013, COMPUT GRAPH FORUM, V32, P101, DOI 10.1111/cgf.12001
   Daly S, 2003, CONF REC ASILOMAR C, P1383
   Foi A, 2016, INT J COMPUT VISION, V120, P78, DOI 10.1007/s11263-016-0898-1
   Gregory Jason., 2009, GAME ENGINE ARCHITEC
   Lavoue G., 2015, Visual Signal Quality Assessment, P243, DOI [10.1007/978-3-319-10368-69, DOI 10.1007/978-3-319-10368-69]
   Lissner I, 2013, IEEE T IMAGE PROCESS, V22, P435, DOI 10.1109/TIP.2012.2216279
   Mantiuk R, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964935
   Pajak D., 2014, COMPUT GRAPH FORUM, V33, P2
   Pedersen M, 2011, FOUND TRENDS COMPUT, V7, P1, DOI 10.1561/0600000037
   Piorkowski R., 2015, P ACM SIGGRAPH S APP, P83
   Rogowitz BE, 2001, P SOC PHOTO-OPT INS, V4299, P340, DOI 10.1117/12.429504
   Rushmeier H, 2000, P SOC PHOTO-OPT INS, V3959, P372, DOI 10.1117/12.387174
   SCHROEDER WJ, 1992, COMP GRAPH, V26, P65, DOI 10.1145/142920.134010
   Sergej T, 2014, LECT NOTES COMPUT SC, V8814, P38, DOI 10.1007/978-3-319-11758-4_5
   Vása L, 2011, IEEE T VIS COMPUT GR, V17, P220, DOI 10.1109/TVCG.2010.38
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Williams L., 1978, P ANN C COMP GRAPH I, P270
   Wu H.R., 2005, DIGITAL VIDEO IMAGE
   Zhang X., 1997, Journal of the Society for Information Display, V5, P61, DOI 10.1889/1.1985127
   Zhang XM, 1997, FIFTH COLOR IMAGING CONFERENCE: COLOR SCIENCE, SYSTEMS, AND APPLICATIONS, P120
   Zhang XM, 1998, SIGNAL PROCESS, V70, P201, DOI 10.1016/S0165-1684(98)00125-X
NR 29
TC 7
Z9 7
U1 0
U2 5
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD JUL
PY 2017
VL 14
IS 3
AR 14
DI 10.1145/3047407
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FB9AL
UT WOS:000406431900001
DA 2024-07-18
ER

PT J
AU Rafian, P
   Legge, GE
AF Rafian, Paymon
   Legge, Gordon E.
TI Remote Sighted Assistants for Indoor Location Sensing of Visually
   Impaired Pedestrians
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Wayfinding; crowdsourcing; indoor navigation; visual impairment; blind;
   low vision; location sensing
ID NAVIGATION
AB Because indoor navigation is difficult for people with visual impairment, there is a need for the development of assistive technology. Indoor location sensing, the ability to identify a pedestrian's location and orientation, is a key component of such technology. We tested the accuracy of a potential crowdsourcing-based indoor location sensing method. Normally sighted subjects were asked to identify the location and facing direction of photos taken by a pedestrian in a building. The subjects had available a floor plan and a small number of representative photos from key locations within the floor plan. Subjects were able to provide accurate location estimates (median location accuracy 3.87ft). This finding indicates that normally sighted subjects, with minimal training, using a simple graphical representation of a floor plan, can provide accurate location estimates based on a single, suitable photo taken by a pedestrian. We conclude that indoor localization is possible using remote, crowdsourced, human assistance. This method has the potential to be used for the location-sensing component of an indoor navigation aid for people with visual impairment.
C1 [Rafian, Paymon; Legge, Gordon E.] Univ Minnesota Twin Cities, 75 East River Rd, Minneapolis, MN 55455 USA.
C3 University of Minnesota System; University of Minnesota Twin Cities
RP Rafian, P (corresponding author), Univ Minnesota Twin Cities, 75 East River Rd, Minneapolis, MN 55455 USA.
EM rafia003@umn.edu; legge@umn.edu
OI Legge, Gordon/0000-0002-3742-1680
FU Department of Psychology at the University of Minnesota
FX This work is supported by research funds from the Department of
   Psychology at the University of Minnesota.
CR [Anonymous], 2008, The Engineering Handbook of Smart Technology for Aging, Disability, and Independence, DOI DOI 10.1002/9780470379424.CH25
   [Anonymous], 2010, P 23ND ANN ACM S USE
   Ariadne GPS, 2017, ARIADNE GPS
   Be My Eyes, 2017, BE MY EYES
   Bigham Jeffrey P, 2006, P 8 INT ACM SIGACCES
   Dutoit Ryan C., 2016, COMPUT RES REPOS
   Hile H, 2008, IEEE COMPUT GRAPH, V28, P32, DOI 10.1109/MCG.2008.80
   Holton Bill, 2013, REV TAPTAPSEE CAMFIN
   Iozzio C., 2014, INDOOR MAPPING LETS
   Kawaji Hisato, 2010, P 1 ACM INT WORKSH M, V1
   Legge GE, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0076783
   Legge Gordon E., 2003, J VIS, V3, P136
   Liu H, 2007, IEEE T SYST MAN CY C, V37, P1067, DOI 10.1109/TSMCC.2007.905750
   Liu J.J., IEEE C COMPUTER VISI, DOI [DOI 10.1109/CVPRW.2010.5543581, 10.1109/CVPRW.2010.5543581.]
   Mulloni A, 2009, IEEE PERVAS COMPUT, V8, P22, DOI 10.1109/MPRV.2009.30
   Ran L, 2004, SECOND IEEE ANNUAL CONFERENCE ON PERVASIVE COMPUTING AND COMMUNICATIONS, PROCEEDINGS, P23, DOI 10.1109/PERCOM.2004.1276842
   Roentgen UR, 2011, J VISUAL IMPAIR BLIN, V105, P612, DOI 10.1177/0145482X1110501008
   Roentgen UR, 2009, J VISUAL IMPAIR BLIN, V103, P743, DOI 10.1177/0145482X0910301104
   Seeing Eye GPS, 2017, SEEING EYE GPS
   Takagi H, 2013, LECT NOTES COMPUT SC, V8117, P587
   TapTapSee, 2017, TAPTAPSEE
   Thomas Gallagher ElyseWise, 2012, J LOCAT BASED SERV I, V8, P54
   Tomasi Matteo, 2013, P 2015 INT C IND POS, P2007
   VizWiz, 2017, VIZWIZ
NR 24
TC 8
Z9 9
U1 1
U2 18
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD JUL
PY 2017
VL 14
IS 3
AR 19
DI 10.1145/3047408
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FB9AL
UT WOS:000406431900006
OA Bronze
DA 2024-07-18
ER

PT J
AU Duchowski, AT
   Bate, D
   Stringfellow, P
   Thakur, K
   Melloy, BJ
   Gramopadhye, AK
AF Duchowski, Andrew T.
   Bate, David
   Stringfellow, Paris
   Thakur, Kaveri
   Melloy, Brian J.
   Gramopadhye, Anand K.
TI On Spatiochromatic Visual Sensitivity and Peripheral Color LOD
   Management
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Visual Perception; Color; Gaze-contingent displays
ID ZONE MAP; DISPLAYS; VISION; RESOLUTION; SEARCH; MODEL; HUE
AB Empirical findings from a gaze-contingent color degradation study report the effects of artificial reduction of the human visual system's sensitivity to peripheral chromaticity on visual search performance. To our knowledge, this is the first such investigation of peripheral color reduction. For unimpeded performance, results suggest that, unlike spatiotemporal content, peripheral chromaticity cannot be reduced within the central 20 degrees visual angle. Somewhat analogous to dark adaptation, reduction of peripheral color tends to simulate scotopic viewing conditions. This holds significant implications for chromatic Level Of Detail management. Specifically, while peripheral spatiotemporal detail can be attenuated without affecting visual search, often dramatically (e. g., spatial detail can be so reduced up to 50% at about 5 degrees), peripheral chromatic reduction is likely to be noticed much sooner. Therefore, color LOD reduction (e. g., via compression), should be maintained isotropically across the central 20 degrees visual field.
C1 [Duchowski, Andrew T.; Bate, David] Clemson Univ, Sch Comp, Clemson, SC 29634 USA.
C3 Clemson University
RP Duchowski, AT (corresponding author), Clemson Univ, Sch Comp, 100 McAdams Hall, Clemson, SC 29634 USA.
EM andrewd@ces.clemson.edu
FU University Innovation [1-20-1906-51-4087]; NSF [9984278]; Div Of
   Information & Intelligent Systems; Direct For Computer & Info Scie &
   Enginr [9984278] Funding Source: National Science Foundation
FX This work was supported in part by a University Innovation grant(#
   1-20-1906-51-4087) and NSF CAREER award # 9984278.
CR [Anonymous], 1996, P 23 ANN C COMP GRAP, DOI DOI 10.1145/237170.237262
   AYAMA M, 2004, HUMAN VISION ELECT I
   Baudisch P, 2003, COMMUN ACM, V46, P60, DOI 10.1145/636772.636799
   BOHME M, 2006, P EY TRACK RES APPL, P109
   CLARK JH, 1976, COMMUN ACM, V19, P547, DOI 10.1145/360349.360354
   Coltekin A., 2006, THESIS HELSINKI U TE
   Daly S, 2001, J ELECTRON IMAGING, V10, P30, DOI 10.1117/1.1333679
   Duchowski AT, 2004, CYBERPSYCHOL BEHAV, V7, P621, DOI 10.1089/cpb.2004.7.621
   DUCHOWSKI AT, 2007, T MULTIMEDIA COMPUT, V3, P1
   Foley J.D., 1990, Computer graphics: Principles and practice
   FUNKHOUSER TA, 1993, P 20 ANN C COMP GRAP
   Geisler W. S., 2002, Proceedings ETRA 2002. Eye Tracking Research and Applications Symposium, P83, DOI 10.1145/507072.507090
   Geisler WS, 2006, J VISION, V6, P858, DOI 10.1167/6.9.1
   GORDON J, 1977, J OPT SOC AM, V67, P202, DOI 10.1364/JOSA.67.000202
   HABER R.N., 1973, PSYCHOL VISUAL PERCE
   Hahn P. J., 1997, Proceedings DCC '97. Data Compression Conference (Cat. No.97TB100108), DOI 10.1109/DCC.1997.582100
   Hamada H, 2004, OPT REV, V11, P240, DOI 10.1007/s10043-004-0240-y
   Komogortsev Oleg., 2004, Proc. of the 12th Annual ACM Int. Conf. on Multimedia, P220, DOI [10.1145/1027527.1027577, DOI 10.1145/1027527.1027577]
   Levoy M., 1990, Computer Graphics, V24, P217, DOI 10.1145/91394.91449
   Loschky LC, 2005, VIS COGN, V12, P1057, DOI 10.1080/13506280444000652
   Loschky LC, 2002, J EXP PSYCHOL-APPL, V8, P99, DOI 10.1037/1076-898X.8.2.99
   Loschky LC, 2000, ETRA '00, P97, DOI DOI 10.1145/355017.355032
   LOSCHKY LC, 2007, T MULTIMEDIA COMPUT, V3, P1
   LUEBKE D, 1997, P 24 ANN C COMP GRAP
   Martin PR, 2001, NATURE, V410, P933, DOI 10.1038/35073587
   MCCONKIE GW, 1975, PERCEPT PSYCHOPHYS, V17, P578, DOI 10.3758/BF03203972
   McConkie GW, 2002, BEHAV RES METH INS C, V34, P481, DOI 10.3758/BF03195477
   Melloy BJ, 2006, HUM FACTORS, V48, P540, DOI 10.1518/001872006778606840
   Mullen KT, 2005, PERCEPTION, V34, P951, DOI 10.1068/p5374
   MURPHY H, 2002, P ACM SIGGRAPH C ABS
   MURPHY H, 2001, P 2001 EUROGRAPHICS
   Najemnik J, 2005, NATURE, V434, P387, DOI 10.1038/nature03390
   O'Sullivan C, 2003, ACM T GRAPHIC, V22, P527, DOI 10.1145/882262.882303
   OSHIMA T, 1996, P 1996 IEEE VIRT REA, P103
   Park DS, 2000, PHARMACOLOGY OF CEREBRAL ISCHEMIA 2000, P105, DOI 10.1145/355017.355033
   Parkhurst Derrick., 2004, APGV'04: Proceedings of the 1st Symposium on Applied perception in graphics and visualization, P49, DOI [10.1145/1012551, DOI 10.1145/1012551]
   Parkhurst DJ, 2002, HUM FACTORS, V44, P611, DOI 10.1518/0018720024497015
   Pellacini F, 2000, COMP GRAPH, P55, DOI 10.1145/344779.344812
   Rasche K, 2005, COMPUT GRAPH FORUM, V24, P423, DOI 10.1111/j.1467-8659.2005.00867.x
   Reddy M, 2001, IEEE COMPUT GRAPH, V21, P68, DOI 10.1109/38.946633
   Reingold EM, 2003, HUM FACTORS, V45, P307, DOI 10.1518/hfes.45.2.307.27235
   Sakurai M, 2003, J OPT SOC AM A, V20, P1997, DOI 10.1364/JOSAA.20.001997
   Shebilske W.L., 1983, Eye Movements and Psychological Functions: International Views, P303
   Solomon SG, 2005, J NEUROSCI, V25, P4527, DOI 10.1523/JNEUROSCI.3921-04.2005
   *TOB TECHN AB, 2003, TOB ET 17 EY TRACK P
   Vince J., 1995, Virtual reality systems
   Watson B., 1997, ACM Transactions on Computer-Human Interaction, V4, P323, DOI 10.1145/267135.267137
   WOLFE JM, 1994, PSYCHON B REV, V1, P202, DOI 10.3758/BF03200774
   Wyszecki G., 1982, Color science: Concepts and methods, quantitative data and formulae
NR 49
TC 14
Z9 16
U1 0
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD FEB
PY 2009
VL 6
IS 2
AR 9
DI 10.1145/1498700.1498703
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 450YN
UT WOS:000266438000003
DA 2024-07-18
ER

PT J
AU Nees, MA
   Walker, BN
AF Nees, Michael A.
   Walker, Bruce N.
TI Data Density and Trend Reversals in Auditory Graphs: Effects on
   Point-Estimation and Trend-Identification Tasks
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Experimentation; Human factors; Auditory graphs; sonification; auditory
   display
ID CONCEPTUAL DATA DIMENSIONS; CROSS-MODAL EQUIVALENCE; PERFORMANCE;
   PERCEPTION; PSYCHOLOGY; SEQUENCES; DISPLAYS; SPEECH; TABLES; SOUND
AB Auditory graphs-displays that represent quantitative information with sound-have the potential to make data (and therefore science) more accessible for diverse user populations. No research to date, however, has systematically addressed the attributes of data that contribute to the complexity (the ease or difficulty of comprehension) of auditory graphs. A pair of studies examined the role of data density (i.e., the number of discrete data points presented per second) and the number of trend reversals for both point-estimation and trend-identification tasks with auditory graphs. For the point-estimation task, more trend reversals led to performance decrements. For the trend-identification task, a large main effect was again observed for trend reversals, but an interaction suggested that the effect of the number of trend reversals was different across lower data densities (i.e., as density increased from 1 to 2 data points per second). Results are discussed in terms of data sonification applications and rhythmic theories of auditory pattern perception.
C1 [Nees, Michael A.; Walker, Bruce N.] Georgia Inst Technol, Sch Psychol, Atlanta, GA 30332 USA.
C3 University System of Georgia; Georgia Institute of Technology
RP Nees, MA (corresponding author), Georgia Inst Technol, Sch Psychol, 654 Cherry St, Atlanta, GA 30332 USA.
EM Bruce.walker@psych.gatech.edu
OI Nees, Michael/0000-0003-2313-2295
FU NSF [IIS-0644076]
FX Portions of this research were supported by NSF Career Award
   #IIS-0644076 to Bruce N. Walker.
CR American Psychological Association, 2001, Publication manual of the American Psychological Association, V5th, DOI 10.1037/emo0000296
   Bonebright T.L., 2001, Proceedings of ICAD 2001, P62
   Bregman A. S., 1990, Auditory Scene Analysis: The Perceptual Organization of Sound, DOI [10.7551/mitpress/1486.001.0001, DOI 10.7551/MITPRESS/1486.001.0001]
   Brewster S., 2000, Personal Technologies, V4, P209, DOI 10.1007/BF02391559
   Brewster SA, 1997, DISPLAYS, V17, P179, DOI 10.1016/S0141-9382(96)01034-7
   Brewster S, 2002, PERS UBIQUIT COMPUT, V6, P188, DOI 10.1007/s007790200019
   Brown L. M., 2003, P ICAD, P152
   BROWN LM, 2002, P 16 BRIT HCI C LOND
   BROWN ML, 1989, P SIGCHI C HUM FACT, P339
   BUTLER DL, 1993, BEHAV RES METH INSTR, V25, P81, DOI 10.3758/BF03204481
   Carpenter PA, 1998, J EXP PSYCHOL-APPL, V4, P75, DOI 10.1037/1076-898X.4.2.75
   Carswell C.M., 1992, Percepts, concepts and categories: The representation and processing of information, P605, DOI DOI 10.1016/S0166-4115(08)61027-4
   Carswell CM, 1997, BEHAV INFORM TECHNOL, V16, P61, DOI 10.1080/014492997119905
   CARSWELL CM, 1993, APPL COGNITIVE PSYCH, V7, P341, DOI 10.1002/acp.2350070407
   CHILDS E, 2005, P INT C AUD DISPL IC
   DEUTSCH D, 1981, PSYCHOL REV, V88, P503, DOI 10.1037/0033-295X.88.6.503
   DOWLING WJ, 1978, PSYCHOL REV, V85, P341, DOI 10.1037/0033-295X.85.4.341
   DRAKE C, 1993, PERCEPT PSYCHOPHYS, V54, P277, DOI 10.3758/BF03205262
   EDU A, ASDF
   Flowers J.H., 2005, ACM Transactions on Applied Perception, V2, P467472, DOI [DOI 10.1145/1101530.1101544, 10.1145/1101530.1101544]
   FLOWERS JH, 1993, BEHAV RES METH INSTR, V25, P242, DOI 10.3758/BF03204505
   FLOWERS JH, 1992, BEHAV RES METH INSTR, V24, P258, DOI 10.3758/BF03203504
   Flowers JH, 1997, HUM FACTORS, V39, P341, DOI 10.1518/001872097778827151
   Flowers JH, 1995, HUM FACTORS, V37, P553, DOI 10.1518/001872095779049264
   FOLDS DJ, 2006, P HUM FACT ERG SOC 5, P1576
   Fraisse P., 1978, HDB PERCEPTION, V8, P203, DOI 10.1016/B978-0-12-161908-4.50012-7
   FRAISSE P, 1982, PSYCHOL MUSIC, P249
   Friel SN, 2001, J RES MATH EDUC, V32, P124, DOI 10.2307/749671
   FRYSINGER SP, 2005, P INT C AUD DISPL IC
   GILLAN DJ, 1994, HUM FACTORS, V36, P419, DOI 10.1177/001872089403600303
   Gillan DJ, 1998, HUM FACTORS, V40, P28, DOI 10.1518/001872098779480640
   Gobbo C, 1994, COMPREHENSION GRAPHI, P227
   Grudin J., 2001, P CHI 01 C HUMAN FAC, P458
   Jones MR, 2006, COGNITIVE PSYCHOL, V53, P59, DOI 10.1016/j.cogpsych.2006.01.003
   JONES MR, 1976, PSYCHOL REV, V83, P323, DOI 10.1037/0033-295X.83.5.323
   Jones RW, 1996, BEHAV RES METH INSTR, V28, P265, DOI 10.3758/BF03204778
   Keppel G., 2004, DESIGN ANAL RES HDB
   KOSSLYN SM, 1989, APPL COGNITIVE PSYCH, V3, P185, DOI 10.1002/acp.2350030302
   KRAMER G, 1994, SFI S SCI C, V18, P1
   Kramer G., 1999, SONIFICATION REPORT
   Kubovy M., 1981, PERCEPTUAL ORG, P55
   LEE JCK, 1999, J EDUC RES, V43, P19
   Liu B., 1999, INT J COGNITIVE ERGO, V3, P289
   Mansur D L, 1985, J Med Syst, V9, P163, DOI 10.1007/BF00996201
   MARTIN JG, 1972, PSYCHOL REV, V79, P487, DOI 10.1037/h0033467
   Meyer J, 2000, ERGONOMICS, V43, P1840, DOI 10.1080/00140130050174509
   Meyer J, 1997, HUM FACTORS, V39, P268, DOI 10.1518/001872097778543921
   *MIT, MIT WEB ACC GUID
   Moore P.J., 1993, Learning and Instruction, V3, P215, DOI [DOI 10.1016/0959-4752(93)90005-K, 10.1016/0959-4752(93)90005-K]
   Nees MichaelA., 2007, P INT C AUDITORY DIS, P266
   Newell A., 1981, COGNITIVE SKILLS THE, P1, DOI DOI 10.4324/9780203728178
   OLIVER F, 1998, PRESENT INFORM GRAPH
   PALOMAKI H, 2006, P 12 INT C AUD DISPL
   Peden BF, 2000, TEACH PSYCHOL, V27, P93, DOI 10.1207/S15328023TOP2702_03
   Peres S.C., 2005, P INT C AUDITORY DIS, P169
   PERES SC, 2003, P INT C AUD DISPL IC
   Pinker S., 1990, Artif. Intell. Future Testing, P73
   POVEL DJ, 1985, MUSIC PERCEPT, V2, P411
   QUESENBERY W, 1999, USABILITY INTERFACE
   Roth P, 2002, J VISUAL IMPAIR BLIN, V96, P420, DOI 10.1177/0145482X0209600605
   SANDERSON PM, 1989, HUM FACTORS, V31, P183, DOI 10.1177/001872088903100207
   SCHUTZ HG, 1961, HUM FACTORS, V3, P99, DOI 10.1177/001872086100300204
   Shepard R.N., 1982, The psychology of music
   Smith DR, 2005, APPL COGNITIVE PSYCH, V19, P1065, DOI 10.1002/acp.1146
   SMITH DR, 2002, P 8 C AUD DISPL KYOT, P362
   Stockman T., 2005, P 11 INT C AUDITORY, P134
   Turnage KD, 1996, BEHAV RES METH INSTR, V28, P270, DOI 10.3758/BF03204779
   Turnbull WW, 1944, J EXP PSYCHOL, V34, P302, DOI 10.1037/h0063434
   *W3C, 2000, COR TECHN WEB ACC GU
   Walker B.N., 2005, P HUMAN FACTORS ERGO, P1598
   Walker BN, 2002, J EXP PSYCHOL-APPL, V8, P211, DOI 10.1037/1076-898X/8.4.211
   WALKER BN, 2004, P REH ENG ASS TECHN
   Walker BN, 2007, APPL COGNITIVE PSYCH, V21, P579, DOI 10.1002/acp.1291
   Walker Bruce N., 2003, Proceedings of the Ninth International Conference on Auditory Display ICAD2003, P161
   Walker Bruce N., 2005, P 11 INT C AUDITORY, P428
   WENZEL EM, 1993, J ACOUST SOC AM, V94, P111, DOI 10.1121/1.407089
   Wickens C. D., 2002, Theor Issues Ergon Sci, V3, P159, DOI [10.1080/14639220210123806, DOI 10.1080/14639220210123806]
   Zacks J, 2000, DIAGRAMMATIC REPRESENTATION AND REASONING, P187
NR 78
TC 4
Z9 5
U1 0
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD AUG
PY 2008
VL 5
IS 3
AR 13
DI 10.1145/1402236.1402237
PG 24
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA 450YK
UT WOS:000266437700001
DA 2024-07-18
ER

PT J
AU Zibrek, K
   Niay, B
   Olivier, AH
   Hoyet, L
   Pettre, J
   McDonnell, R
AF Zibrek, Katja
   Niay, Benjamin
   Olivier, Anne-Helene
   Hoyet, Ludovic
   Pettre, Julien
   McDonnell, Rachel
TI The Effect of Gender and Attractiveness of Motion on Proximity in
   Virtual Reality
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article; Proceedings Paper
CT 17th ACM Symposium on Applied Perception (SAP)
CY SEP, 2020
CL ELECTR NETWORK
SP ACM
DE Perception; proximity; virtual reality; gender
ID PERSONAL-SPACE; INTERPERSONAL DISTANCE; SEX; PERCEPTION; ENVIRONMENTS
AB In human interaction, people will keep different distances from each other depending on their gender. For example, males will stand further away from males and closer to females. Previous studies in virtual reality (VR), where people were interacting with virtual humans, showed a similar result. However, many other variables influence proximity, such as appearance characteristics of the virtual character (e.g., attractiveness). Our study focuses on proximity to virtual walkers, where gender could be recognised from motion only, since previous studies using point-light displays found walking motion is rich in gender cues. In our experiment, a walking wooden mannequin approached the participant embodied in a virtual avatar using the HTC Vive Pro HMD and controllers. The mannequin animation was motion captured from several male and female actors and each motion was displayed individually on the character. Participants used the controller to stop the approaching mannequin when they felt it was uncomfortably close to them. Based on previous work, we hypothesised that proximity will be affected by the gender of the character, but unlike previous research, the gender in our experiment could only be determined from character's motion. We also expected differences in proximity according to the gender of the participant. We additionally expected some motions to be rated more attractive than others and that attractive motions would reduce the proximity measure. Our results show support for the last two assumptions, but no difference in proximity was found according to the gender of the character's motion. Our findings have implications for the design of virtual characters in interactive virtual environments.
C1 [Zibrek, Katja; Niay, Benjamin; Olivier, Anne-Helene; Hoyet, Ludovic; Pettre, Julien] Inria Rennes, Rennes, France.
   [McDonnell, Rachel] Trinity Coll Dublin, Coll Green 2, Dublin, Ireland.
   [Zibrek, Katja; Niay, Benjamin; Olivier, Anne-Helene; Hoyet, Ludovic] Univ Rennes, MimeTIC, INRIA, CNRS,IRISA, M2S,Campus Beaulieu, F-35042 Rennes, France.
   [Pettre, Julien] Univ Rennes, Rainbow, INRIA, CNRS,IRISA, M2S,Campus Beaulieu, F-35042 Rennes, France.
C3 Universite de Rennes; Trinity College Dublin; Inria; Universite de
   Rennes; Centre National de la Recherche Scientifique (CNRS); Universite
   de Rennes; Inria; Centre National de la Recherche Scientifique (CNRS)
RP Zibrek, K (corresponding author), Inria Rennes, Rennes, France.; Zibrek, K (corresponding author), Univ Rennes, MimeTIC, INRIA, CNRS,IRISA, M2S,Campus Beaulieu, F-35042 Rennes, France.
EM katja.zibrek@inria.fr; benjamin.niay@inria.fr;
   anne-helene.olivier@irisa.fr; ludovic.hoyet@inria.fr;
   julien.pettre@inria.fr; ramcdonn@tcd.ie
RI Zibrek, Katja/JQW-2981-2023; McDonnell, Rachel/HGC-4337-2022; Olivier,
   Anne-Hélène/AAH-7378-2020; Hoyet, Ludovic/IWU-9100-2023; Pettré,
   Julien/AAB-2590-2022
OI Zibrek, Katja/0000-0002-0204-3472; McDonnell,
   Rachel/0000-0002-1957-2506; Olivier, Anne-Hélène/0000-0002-2833-020X;
   Hoyet, Ludovic/0000-0002-7373-6049; 
FU Science Foundation Ireland as part of the "Game Face" project
   [13/CDA/2135]; ADAPT Centre for Digital Content Technology [13/RC/2106];
   French ANR JCJC Per2 project [ANR-18-CE33-0013]; H2020 ICT-25 RIA
   PRESENT project [856879]; Science Foundation Ireland (SFI) [13/CDA/2135]
   Funding Source: Science Foundation Ireland (SFI); Agence Nationale de la
   Recherche (ANR) [ANR-18-CE33-0013] Funding Source: Agence Nationale de
   la Recherche (ANR); H2020 - Industrial Leadership [856879] Funding
   Source: H2020 - Industrial Leadership
FX We wish to thank all the reviewers for their comments, and the
   participants in our experiments. This work was funded by Science
   Foundation Ireland as part of the "Game Face" project (Grant
   13/CDA/2135) and under the ADAPT Centre for Digital Content Technology
   (Grant 13/RC/2106), the French ANR JCJC Per2 project (ANR-18-CE33-0013),
   as well as the H2020 ICT-25 RIA PRESENT project (#856879).
CR AHMED SMS, 1979, PERCEPT MOTOR SKILL, V49, P85, DOI 10.2466/pms.1979.49.1.85
   [Anonymous], 2008, P 2008 ACM S VIRTUAL, DOI DOI 10.1145/1450579.1450614
   Anthony Ann Strong, 2004, Nurse Educ, V29, P121, DOI 10.1097/00006223-200405000-00011
   Bailenson Jeremy N., 2004, P 7 ANN INT WORKSH P, P1864
   Bailenson JN, 2001, PRESENCE-VIRTUAL AUG, V10, P583, DOI 10.1162/105474601753272844
   Bailenson JN, 2003, PERS SOC PSYCHOL B, V29, P819, DOI 10.1177/0146167203029007002
   BANZIGER G, 1984, J SOC PSYCHOL, V124, P255, DOI 10.1080/00224545.1984.9922857
   BARCLAY CD, 1978, PERCEPT PSYCHOPHYS, V23, P145, DOI 10.3758/BF03208295
   Bönsch A, 2018, 25TH 2018 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P199
   BRADY AT, 1978, BRIT J SOC CLIN PSYC, V17, P127, DOI 10.1111/j.2044-8260.1978.tb00254.x
   Cameron JJ, 2019, SOC PERSONAL PSYCHOL, V13, DOI 10.1111/spc3.12506
   Chaminade T, 2007, SOC COGN AFFECT NEUR, V2, P206, DOI 10.1093/scan/nsm017
   Eastman S. T., 2000, Journal of Sport and Social Issues, V24, P192, DOI 10.1177/0193723500242006
   Ennis C, 2012, COMPUT ANIMAT VIRT W, V23, P321, DOI 10.1002/cav.1453
   European Commission, 2014, GEND EU FUND RES
   Fischer AH, 2004, EMOTION, V4, P87, DOI 10.1037/1528-3542.4.1.87
   Hackney AL, 2015, ACTA PSYCHOL, V162, P62, DOI 10.1016/j.actpsy.2015.10.007
   Halevina A., 2007, VIS SCI SOC M
   Hall E.T., 1992, The Hidden Dimension
   Hartmann T, 2006, J COMPUT-MEDIAT COMM, V11, P910, DOI 10.1111/j.1083-6101.2006.00301.x
   HAYDUK LA, 1981, CAN J BEHAV SCI, V13, P274, DOI 10.1037/h0081182
   HAYDUK LA, 1983, PSYCHOL BULL, V94, P293, DOI 10.1037/0033-2909.94.2.293
   Hecht H, 2019, ACTA PSYCHOL, V193, P113, DOI 10.1016/j.actpsy.2018.12.009
   HEWITT J, 1987, PERCEPT MOTOR SKILL, V64, P809, DOI 10.2466/pms.1987.64.3.809
   Hoyet L, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508367
   Iachini T, 2016, J ENVIRON PSYCHOL, V45, P154, DOI 10.1016/j.jenvp.2016.01.004
   Iachini T, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0111511
   ICKES W, 1977, J PERS SOC PSYCHOL, V35, P315, DOI 10.1037/0022-3514.35.5.315
   Interrante V, 2006, P IEEE VIRT REAL ANN, P3, DOI 10.1109/VR.2006.52
   Johnson KL, 2007, P NATL ACAD SCI USA, V104, P5246, DOI 10.1073/pnas.0608181104
   Johnson KL, 2005, PSYCHOL SCI, V16, P890, DOI 10.1111/j.1467-9280.2005.01633.x
   Jung E, 2016, FRONT PSYCHOL, V7, DOI 10.3389/fpsyg.2016.00217
   KMIECIK C, 1979, J SOC PSYCHOL, V108, P277, DOI 10.1080/00224545.1979.9711646
   KOZLOWSKI LT, 1977, PERCEPT PSYCHOPHYS, V21, P575, DOI 10.3758/BF03198740
   Lakens D., 2014, OBSERVED POWER WHAT
   MATHER G, 1994, P ROY SOC B-BIOL SCI, V258, P273, DOI 10.1098/rspb.1994.0173
   Mcdonnell R, 2009, ACM T APPL PERCEPT, V5, DOI 10.1145/1462048.1462051
   McDonnell R, 2007, APGV 2007: SYMPOSIUM ON APPLIED PERCEPTION IN GRAPHICS AND VISUALIZATION, PROCEEDINGS, P7
   Mohler BJ, 2010, PRESENCE-TELEOP VIRT, V19, P230, DOI 10.1162/pres.19.3.230
   Mohler BettyJ., 2006, P 3 S APPL PERCEPTIO, P9, DOI [10.1145/1140491.1140493, DOI 10.1145/1140491.1140493]
   Murakami Rumi, 2017, J Phys Ther Sci, V29, P722, DOI 10.1589/jpts.29.722
   Nassiri N, 2010, VIRTUAL REAL-LONDON, V14, P229, DOI 10.1007/s10055-010-0169-3
   Naylor YK, 2008, PERCEPT PSYCHOPHYS, V70, P199, DOI 10.3758/PP.70.2.199
   Peck TC, 2018, IEEE T VIS COMPUT GR, V24, P1604, DOI 10.1109/TVCG.2018.2793598
   Plant EA, 2000, PSYCHOL WOMEN QUART, V24, P81, DOI 10.1111/j.1471-6402.2000.tb01024.x
   Renner RS, 2013, ACM COMPUT SURV, V46, DOI 10.1145/2543581.2543590
   Sanz FA, 2015, P IEEE VIRT REAL ANN, P75, DOI 10.1109/VR.2015.7223327
   Sekiya N, 1996, J HUM MOVEMENT STUD, V30, P241
   Slater M, 2010, PLOS ONE, V5, DOI 10.1371/journal.pone.0010564
   Slater M, 2009, PHILOS T R SOC B, V364, P3549, DOI 10.1098/rstb.2009.0138
   Sukhera J, 2018, ACAD MED, V93, P35, DOI 10.1097/ACM.0000000000001819
   Thaler A, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES WORKSHOPS (VRW 2020), P679, DOI [10.1109/VRW50115.2020.00190, 10.1109/VRW50115.2020.00-85]
   THOMAS DR, 1973, PERCEPT MOTOR SKILL, V36, P15, DOI 10.2466/pms.1973.36.1.15
   Troje N., 2003, IGSN Report, P40
   UNGER RK, 1979, AM PSYCHOL, V34, P1085, DOI 10.1037/0003-066X.34.11.1085
   Vinayagamoorthy V., 2006, Eurogrpahics State of The Art Report, P21
   Zibrek K, 2015, ACM T APPL PERCEPT, V12, DOI 10.1145/2767130
NR 57
TC 19
Z9 19
U1 3
U2 20
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD DEC
PY 2020
VL 17
IS 4
SI SI
AR 14
DI 10.1145/3419985
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA PH1UI
UT WOS:000600206200003
OA Green Published
DA 2024-07-18
ER

PT J
AU Jones, JA
   Krum, DM
   Bolas, MT
AF Jones, J. Adam
   Krum, David M.
   Bolas, Mark T.
TI Vertical Field-of-View Extension and Walking Characteristics in
   Head-Worn Virtual Environments
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Spatial Perception; Virtual Environments; Head-worn displays;
   head-mounted displays; field of view; distance perception; blind
   walking; gait
ID DISTANCE PERCEPTION; DEPTH JUDGMENTS; CALIBRATION; LOCOMOTION; REAL;
   GAIT; DISPLAY; SCALE
AB In this article, we detail a series of experiments that examines the effect of vertical field-of-view extension and the addition of non-specific peripheral visual stimulation on gait characteristics and distance judgments in a head-worn virtual environment. Specifically, we examined four field-of-view configurations: a common 60. diagonal field of view (48 degrees x 40 degrees), a 60 degrees diagonal field of view with the addition of a luminous white frame in the far periphery, a field of view with an extended upper edge, and a field of view with an extended lower edge. We found that extension of the field of view, either with spatially congruent or spatially non-informative visuals, resulted in improved distance judgments and changes in observed posture. However, these effects were not equal across all field-of-view configurations, suggesting that some configurations may be more appropriate than others when balancing performance, cost, and ergonomics.
C1 [Jones, J. Adam] Univ Mississippi, Comp & Informat Sci, 201 Weir Hall, University, MS 38677 USA.
   [Krum, David M.; Bolas, Mark T.] Univ Southern Calif, Los Angeles, CA 90089 USA.
   [Krum, David M.; Bolas, Mark T.] Inst Creat Technol, MxR Lab, 12015 Waterfront Dr, Playa Vista, CA 90094 USA.
C3 University of Mississippi; University of Southern California
RP Jones, JA (corresponding author), Univ Mississippi, Comp & Informat Sci, 201 Weir Hall, University, MS 38677 USA.
EM jadamj@acm.org; krum@ict.usc.edu; bolas@ict.usc.edu
RI Krum, David/AAS-2694-2020
OI Krum, David/0000-0003-1051-5385
FU Office of Naval Research [N00014-13-1-0237]
FX This material is based on work supported by the Office of Naval Research
   under Grant No. N00014-13-1-0237. Any opinions, findings, and
   conclusions expressed in this material are those of the authors and do
   not necessarily reflect the views of ONR.
CR Adam Jones J., 2012, P ACM S APPL PERC, P11, DOI 10.1145/2338676.2338679
   Adam Jones J., 2014, COMP IEEE VIRT REAL
   Creem-Regehr SH, 2015, PSYCHOL LEARN MOTIV, V62, P195, DOI 10.1016/bs.plm.2014.09.006
   Creem-Regehr SH, 2005, PERCEPTION, V34, P191, DOI 10.1068/p5144
   EBY DW, 1995, PERCEPTION, V24, P981, DOI 10.1068/p240981
   EDWARDS W, 1954, J EXP PSYCHOL, V48, P493, DOI 10.1037/h0055867
   Grechkin TY, 2014, IEEE T VIS COMPUT GR, V20, P596, DOI 10.1109/TVCG.2014.18
   Hollman JH, 2006, GAIT POSTURE, V23, P441, DOI 10.1016/j.gaitpost.2005.05.005
   Hua H, 2007, J SOC INF DISPLAY, V15, P905, DOI 10.1889/1.2812991
   Hua H, 2007, IEEE T SYST MAN CY A, V37, P416, DOI 10.1109/TSMCA.2007.893471
   Interrante V, 2006, P IEEE VIRT REAL ANN, P3, DOI 10.1109/VR.2006.52
   Intraub H, 1996, J MEM LANG, V35, P118, DOI 10.1006/jmla.1996.0007
   INTRAUB H, 1989, J EXP PSYCHOL LEARN, V15, P179, DOI 10.1037/0278-7393.15.2.179
   Jones J.A., 2011, Proc. Symposium on Applied perception in Graphics and Visualization, P29
   Jones JA, 2013, IEEE T VIS COMPUT GR, V19, P701, DOI 10.1109/TVCG.2013.37
   Jones JA, 2008, APGV 2008: PROCEEDINGS OF THE SYMPOSIUM ON APPLIED PERCEPTION IN GRAPHICS AND VISUALIZATION, P9
   Kelly JW, 2014, IEEE T VIS COMPUT GR, V20, P588, DOI 10.1109/TVCG.2014.36
   Kline PB, 1996, PROCEEDINGS OF THE HUMAN FACTORS AND ERGONOMICS SOCIETY - 40TH ANNUAL MEETING, VOLS 1 AND 2, P1112
   Knapp J. M., 1999, THESIS
   Knapp JM, 2004, PRESENCE-TELEOP VIRT, V13, P572, DOI 10.1162/1054746042545238
   Kuhl S.A., 2006, J VISION, V6, P726
   Kunz BR, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0054446
   Leyrer M., 2011, P ACM SIGGRAPH S APP, DOI 10.1145/2077451.2077464
   Lin Qiufeng., 2011, P ACM SIGGRAPH S APP, P75
   Loomis JM, 2003, VIRTUAL AND ADAPTIVE ENVIRONMENTS: APPLICATIONS, IMPLICATIONS, AND HUMAN PERFORMANCE ISSUES, P21
   Mohler Betty., 2007, 13th Eurographics Symposium on Virtual Environments and 10th Immersive Projection Technology Workshop (IPT-EGVE 2007), P85, DOI DOI 10.2312/PE/VE2007SHORT/085-088
   Mohler BJ, 2007, ACM T APPL PERCEPT, V4, DOI [10.1145/1227134.1227138, 10.1145/1227134/1227138]
   Mohler BettyJ., 2006, P 3 S APPL PERCEPTIO, P9, DOI [10.1145/1140491.1140493, DOI 10.1145/1140491.1140493]
   Nasr S, 2014, J NEUROSCI, V34, P6721, DOI 10.1523/JNEUROSCI.4802-13.2014
   Nguyen Tien Dat, 2010, APPL PERCEPTION GRAP, P159
   Nilsson NC, 2014, IEEE T VIS COMPUT GR, V20, P569, DOI 10.1109/TVCG.2014.21
   Philbeck JW, 2008, PERCEPT PSYCHOPHYS, V70, P1459, DOI 10.3758/PP.70.8.1459
   Phillips L, 2012, PRESENCE-VIRTUAL AUG, V21, P119, DOI 10.1162/PRES_a_00100
   Richardson AR, 2007, HUM FACTORS, V49, P507, DOI 10.1518/001872007X200139
   RIESER JJ, 1995, J EXP PSYCHOL HUMAN, V21, P480, DOI 10.1037/0096-1523.21.3.480
   Rubino FA, 2002, NEUROLOGIST, V8, P254, DOI 10.1097/00127893-200207000-00005
   Singh G, 2012, IEEE VIRTUAL REALITY CONFERENCE 2012 PROCEEDINGS, P165, DOI 10.1109/VR.2012.6180933
   Singh Gurjot., 2010, Proceedings of the 7th Symposium on Applied Perception in Graphics and Visualization, P149, DOI DOI 10.1145/1836248.1836277
   Souman JL, 2011, ACM T APPL PERCEPT, V8, DOI 10.1145/2043603.2043607
   Swan JE, 2007, IEEE T VIS COMPUT GR, V13, P429, DOI 10.1109/TVCG.2007.1035
   Swan JE, 2006, P IEEE VIRT REAL ANN, P19, DOI 10.1109/VR.2006.13
   Thompson WB, 2004, PRESENCE-TELEOP VIRT, V13, P560, DOI 10.1162/1054746042545292
   Tuceryan M, 2000, IEEE AND ACM INTERNATIONAL SYMPOSIUM ON AUGMENTED REALITY, PROCEEDING, P149, DOI 10.1109/ISAR.2000.880938
   WAPNER S, 1953, J EXP PSYCHOL, V46, P300, DOI 10.1037/h0056203
   Willemsen P, 2002, P IEEE VIRT REAL ANN, P275, DOI 10.1109/VR.2002.996536
   Willemsen P., 2004, P 1 S APPL PERC GRAP, P35, DOI DOI 10.1145/1012551.1012558
   Willemsen P, 2009, ACM T APPL PERCEPT, V6, DOI 10.1145/1498700.1498702
   Witmer BG, 1998, HUM FACTORS, V40, P478, DOI 10.1518/001872098779591340
   Wu B, 2004, NATURE, V428, P73, DOI 10.1038/nature02350
   Young M. K., 2014, P ACM S APPL PERCEPT, P83
NR 50
TC 26
Z9 28
U1 0
U2 14
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD FEB
PY 2017
VL 14
IS 2
AR 9
DI 10.1145/2983631
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA EM8VC
UT WOS:000395588200002
DA 2024-07-18
ER

PT J
AU Rungta, A
   Rust, S
   Morales, N
   Klatzky, R
   Lin, M
   Manocha, D
AF Rungta, Atul
   Rust, Sarah
   Morales, Nicolas
   Klatzky, Roberta
   Lin, Ming
   Manocha, Dinesh
TI Psychoacoustic Characterization of Propagation Effects in Virtual
   Environments
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article; Proceedings Paper
CT 13th ACM SIGGRAPH Symposium on Applied Perception (SAP)
CY JUL, 2016
CL Anaheim, CA
SP ACM SIGGRAPH
DE Auditory perception; virtual environments/reality
ID SOUND-PROPAGATION; RECTANGULAR DECOMPOSITION; EDGE-DIFFRACTION; ROOM
   ACOUSTICS; LOCALIZATION; EFFICIENT; TIME; REVERBERATION; SURFACE; MEDIA
AB As sound propagation algorithms become faster and more accurate, the question arises as to whether the additional efforts to improve fidelity actually offer perceptual benefits over existing techniques. Could environmental sound effects go the way of music, where lower-fidelity compressed versions are actually favored by listeners? Here we address this issue with two acoustic phenomena that are known to have perceptual effects on humans and that, accordingly, might be expected to heighten their experience with simulated environments. We present two studies comparing listeners' perceptual response to both accurate and approximate algorithms simulating two key acoustic effects: diffraction and reverberation. For each effect, we evaluate whether increased numerical accuracy of a propagation algorithm translates into increased perceptual differentiation in interactive virtual environments. Our results suggest that auditory perception does benefit from the increased accuracy, with subjects showing better perceptual differentiation when experiencing the more accurate rendering method: the diffraction experiment shows a more linearly decaying sound field (with respect to the diffraction angle) for the accurate diffraction method, whereas the reverberation experiment shows that more accurate reverberation, after modest user experience, results in near-logarithmic response to increasing room volume.
C1 [Rungta, Atul; Rust, Sarah; Morales, Nicolas; Lin, Ming; Manocha, Dinesh] Univ North Carolina Chapel Hill, Chapel Hill, NC 27599 USA.
   [Klatzky, Roberta] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.
C3 University of North Carolina School of Medicine; University of North
   Carolina; University of North Carolina Chapel Hill; Carnegie Mellon
   University
RP Rungta, A (corresponding author), Univ North Carolina Chapel Hill, Chapel Hill, NC 27599 USA.
EM rungta@cs.unc.edu; serust@live.unc.edu; nmorales@cs.unc.edu;
   klatzky@cmu.edu; lin@cs.unc.edu; dm@cs.unc.edu
FU Div Of Information & Intelligent Systems; Direct For Computer & Info
   Scie & Enginr [1320644] Funding Source: National Science Foundation
CR ALLEN JB, 1979, J ACOUST SOC AM, V65, P943, DOI 10.1121/1.382599
   [Anonymous], 2014, SENSATION PERCEPTION
   [Anonymous], 2007, PSYCHOACOUSTICS FACT
   [Anonymous], 2004, 116 AES CONVENTION
   [Anonymous], 1962, J. Audio Eng. Soc.
   Antani L, 2012, APPL ACOUST, V73, P218, DOI 10.1016/j.apacoust.2011.09.004
   BEGAULT DR, 1992, J AUDIO ENG SOC, V40, P895
   Begault DurandR., 1994, 3-D sound for virtual reality and multimedia
   BERENGER JP, 1994, J COMPUT PHYS, V114, P185, DOI 10.1006/jcph.1994.1159
   BIOT MA, 1957, J ACOUST SOC AM, V29, P381, DOI 10.1121/1.1908899
   BORISH J, 1984, J ACOUST SOC AM, V75, P1827, DOI 10.1121/1.390983
   Cabrera Densil, 2005, P 2005 C EXP NOIS CO
   Chandak A, 2008, IEEE T VIS COMPUT GR, V14, P1707, DOI 10.1109/TVCG.2008.111
   Cheng AHD, 2005, ENG ANAL BOUND ELEM, V29, P268, DOI 10.1016/j.enganabound.2004.12.001
   Chu D, 2007, J ACOUST SOC AM, V122, P3177, DOI 10.1121/1.2783001
   Djelani T, 2001, ACUSTICA, V87, P253
   Funkhouser T, 2004, J ACOUST SOC AM, V115, P739, DOI 10.1121/1.1641020
   Galster J. A., 2007, THESIS
   GIGUERE C, 1993, J ACOUST SOC AM, V94, P769, DOI 10.1121/1.408206
   HAAS H, 1951, ACUSTICA, V1, P49
   HARTMANN WM, 1983, J ACOUST SOC AM, V74, P1380, DOI 10.1121/1.390163
   Jot J.-M., 1991, P 90 AUD ENG SOC CON
   KAWAI T, 1981, J SOUND VIB, V79, P229, DOI 10.1016/0022-460X(81)90370-9
   Knudsen V.O., 1932, ARCHITECTURAL ACOUST
   KOUYOUMJIAN RG, 1974, P IEEE, V62, P1448, DOI 10.1109/PROC.1974.9651
   KROKSTAD A, 1968, J SOUND VIB, V8, P118, DOI 10.1016/0022-460X(68)90198-3
   Kuttruff H., 2007, ACOUSTICS INTRO
   Larsson Pontus, 2002, P 22 INT AUD ENG SOC
   Lauterbach C, 2007, IEEE T VIS COMPUT GR, V13, P1672, DOI 10.1109/TVCG.2007.70567
   Mehra R, 2015, IEEE T VIS COMPUT GR, V21, P434, DOI 10.1109/TVCG.2015.2391858
   Mehra R, 2012, APPL ACOUST, V73, P83, DOI 10.1016/j.apacoust.2011.05.012
   Morales N, 2015, APPL ACOUST, V97, P104, DOI 10.1016/j.apacoust.2015.03.017
   Pop Claudiu B., 2005, P AUSTR AC SOC C
   Raghuvanshi N, 2009, IEEE T VIS COMPUT GR, V15, P789, DOI [10.1109/TVCG.2009.27, 10.1109/TVCG.2009.28]
   RAKERD B, 1985, J ACOUST SOC AM, V78, P524, DOI 10.1121/1.392474
   Schissler C, 2016, PROCEEDINGS I3D 2016: 20TH ACM SIGGRAPH SYMPOSIUM ON INTERACTIVE 3D GRAPHICS AND GAMES, P71, DOI 10.1145/2856400.2856414
   Schissler C, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601216
   Taylor M, 2012, IEEE T VIS COMPUT GR, V18, P1797, DOI 10.1109/TVCG.2012.27
   Taylor MicahT., 2009, MM 09, P271
   Torres R.R., 1998, The Journal of the Acoustical Society of America, V103, P2789
   Torres R. R., 2001, P SIGGRAPH CAMPF
   Torres RR, 2001, J ACOUST SOC AM, V109, P600, DOI 10.1121/1.1340647
   Tsingos N, 2001, COMP GRAPH, P545, DOI 10.1145/383259.383323
   VORLANDER M, 1989, J ACOUST SOC AM, V86, P172, DOI 10.1121/1.398336
   Webb Craig, 2013, P M AC, V19
   YEE KS, 1966, IEEE T ANTENN PROPAG, VAP14, P302
   Yeh HC, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508420
   Zannini C. M., 2011, P INT C DIG SIGN PRO, P1
   Zienkiewicz O. C., 2005, THESIS
   ZWISLOCKI JJ, 1980, PERCEPT PSYCHOPHYS, V28, P28, DOI 10.3758/BF03204312
NR 50
TC 6
Z9 9
U1 1
U2 10
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD JUL
PY 2016
VL 13
IS 4
SI SI
AR 21
DI 10.1145/2947508
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA DV4DY
UT WOS:000382876600004
OA Bronze, Green Submitted
DA 2024-07-18
ER

PT J
AU Healey, CG
   Sawant, AP
AF Healey, Christopher G.
   Sawant, Amit P.
TI On the Limits of Resolution and Visual Angle in Visualization
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Experimentation; Human Factors; Hue; orientation; luminance; resolution;
   size; visual acuity; visual angle; visual perception; visualization
ID TEXTURES; PSYCHOPHYSICS; THRESHOLD; CONTRAST
AB This article describes a perceptual level-of-detail approach for visualizing data. Properties of a dataset that cannot be resolved in the current display environment need not be shown, for example, when too few pixels are used to render a data element, or when the element's subtended visual angle falls below the acuity limits of our visual system. To identify these situations, we asked: (1) What type of information can a human user perceive in a particular display environment? (2) Can we design visualizations that control what they represent relative to these limits? and (3) Is it possible to dynamically update a visualization as the display environment changes, to continue to effectively utilize our perceptual abilities? To answer these questions, we conducted controlled experiments that identified the pixel resolution and subtended visual angle needed to distinguish different values of luminance, hue, size, and orientation. This information is summarized in a perceptual display hierarchy, a formalization describing how many pixels-resolution-and how much physical area on a viewer's retina-visual angle-is required for an element's visual properties to be readily seen. We demonstrate our theoretical results by visualizing historical climatology data from the International Panel for Climate Change.
C1 [Healey, Christopher G.] N Carolina State Univ, Dept Comp Sci, Raleigh, NC 27695 USA.
   [Sawant, Amit P.] NetApp RTP, Res Triangle Pk, NC 27709 USA.
C3 North Carolina State University
RP Healey, CG (corresponding author), N Carolina State Univ, Dept Comp Sci, 890 Oval Dr 8206, Raleigh, NC 27695 USA.
EM healey@csc.ncsu.ed
RI Healey, Christopher/ABH-9682-2020
CR [Anonymous], 2012, INFORM VISUAL
   [Anonymous], 2012, ACM T APPL PERCEPTIO, V9
   Anstis S, 1998, PERCEPTION, V27, P817, DOI 10.1068/p270817
   Ball R, 2007, CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1 AND 2, P191
   CAMPBELL FW, 1965, NATURE, V208, P191, DOI 10.1038/208191a0
   Commission Internationale de l'Eclairage (CIE), 1978, CIE PUBL, V15
   Cook K. A., 2005, Illuminating the path: The research and development agenda for visual analytics
   Furnas G. W., 1986, ACM Sigchi Bull., V17, P16, DOI DOI 10.1145/22339.22342
   Glassner A. S., 1995, Principles of Digital Image Synthesis
   Grinstein G., 1989, Proceedings. Graphics Interface'89, P254
   Healey CG, 1996, IEEE VISUAL, P263, DOI 10.1109/VISUAL.1996.568118
   Healey CG, 1999, IEEE T VIS COMPUT GR, V5, P145, DOI 10.1109/2945.773807
   Huber DE, 2005, IEEE VISUALIZATION 2005, PROCEEDINGS, P527
   Interrante V, 2000, IEEE COMPUT GRAPH, V20, P6, DOI 10.1109/MCG.2000.888001
   Johnson Chris., 2006, NIH-NSF Visualization Research Challenges Report
   Joseph JS, 1997, NATURE, V387, P805, DOI 10.1038/42940
   JULESZ B, 1973, PERCEPTION, V2, P391, DOI 10.1068/p020391
   Lamping J, 1996, J VISUAL LANG COMPUT, V7, P33, DOI 10.1006/jvlc.1996.0003
   LEGGE GE, 1987, VISION RES, V27, P1165, DOI 10.1016/0042-6989(87)90028-9
   LEGGE GE, 1990, J OPT SOC AM A, V7, P2002, DOI 10.1364/JOSAA.7.002002
   Liu BX, 2002, PERCEPT PSYCHOPHYS, V64, P1227, DOI 10.3758/BF03194768
   MALIK J, 1990, J OPT SOC AM A, V7, P923, DOI 10.1364/JOSAA.7.000923
   Marr D., 1982, Vision
   McCormick B.H., 1988, SIGBIO NEWSL, V10, P15, DOI [DOI 10.1145/41997, DOI 10.1145/43965.43966]
   POLLACK I, 1968, PERCEPT PSYCHOPHYS, V3, P285, DOI 10.3758/BF03212746
   Rao A. R., 1993, Proceedings Visualization '93. (Cat. No.93CH3354-8), P220, DOI 10.1109/VISUAL.1993.398872
   Rheingans P., 1990, Computer Graphics, V24, P145, DOI 10.1145/91394.91436
   Rogowitz B. E., 1993, Proceedings Visualization '93. (Cat. No.93CH3354-8), P236, DOI 10.1109/VISUAL.1993.398874
   SAGI D, 1987, Spatial Vision, V2, P39, DOI 10.1163/156856887X00042
   SMITH PH, 1998, CVD WORKSHOP SERIES
   Ulrich R, 2004, PERCEPT PSYCHOPHYS, V66, P517, DOI 10.3758/BF03194898
   WALES R, 1970, J OPT SOC AM, V60, P284
   WARE C, 1988, IEEE COMPUT GRAPH, V8, P41, DOI 10.1109/38.7760
   WARE C, 1995, ACM T GRAPHIC, V14, P3, DOI 10.1145/200972.200974
   Watson AB, 2005, J VISION, V5, P717, DOI 10.1167/5.9.6
   Wyszecki G., 1982, Color science: Concepts and methods, quantitative data and formulae
   Yost B, 2007, CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1 AND 2, P101
   Zhang X., 1997, Journal of the Society for Information Display, V5, P61, DOI 10.1889/1.1985127
NR 38
TC 20
Z9 22
U1 0
U2 9
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD OCT
PY 2012
VL 9
IS 4
AR 20
DI 10.1145/2355598.2355603
PG 21
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 025DJ
UT WOS:000310164900005
DA 2024-07-18
ER

PT J
AU Thumfart, S
   Jacobs, RHAH
   Lughofer, E
   Eitzinger, C
   Cornelissen, FW
   Groissboeck, W
   Richter, R
AF Thumfart, Stefan
   Jacobs, Richard H. A. H.
   Lughofer, Edwin
   Eitzinger, Christian
   Cornelissen, Frans W.
   Groissboeck, Werner
   Richter, Roland
TI Modeling Human Aesthetic Perception of Visual Textures
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Algorithms; Experimentation; Human Factors; Texture analysis; human
   perception; modeling; machine learning; mediator analysis; human
   judgments; aesthetic space
ID IMAGE STATISTICS; DISCRIMINATION; FEATURES; CLASSIFICATION
AB Texture is extensively used in areas such as product design and architecture to convey specific aesthetic information. Using the results of a psychological experiment, we model the relationship between computational texture features and aesthetic properties of visual textures. Contrary to previous approaches, we build a layered model, which provides insights into hierarchical relationships involved in human aesthetic texture perception. This model uses a set of intermediate judgements to link computational texture features with aesthetic texture properties. We pursue two different approaches for modeling. (1) Supervised machine-learning methods are used to generate linear and nonlinear models from the experimental data automatically. The quality of these models is discussed, mainly focusing on interpretability and accuracy. (2) We apply a psychological-based approach that models the processing pathways in human perception of naturalness, introducing judgement dimensions (principal components) mediating the relationship between texture features and naturalness judgements. This multiple mediator model serves as a verification of the machine-learning approach. We conclude with a comparison of these two approaches, highlighting the similarities and discrepancies in terms of identified relationships between computational texture features and aesthetic properties of visual textures.
C1 [Thumfart, Stefan; Eitzinger, Christian] Profactor GmbH, Machine Vis Grp, A-4407 Steyr Gleink, Austria.
   [Jacobs, Richard H. A. H.; Cornelissen, Frans W.] Univ Groningen, Univ Med Ctr Groningen, Sch Behav & Cognit Neurosci, Lab Expt Ophthalmol, NL-9700 RB Groningen, Netherlands.
   [Lughofer, Edwin; Groissboeck, Werner; Richter, Roland] Johannes Kepler Univ Linz, Dept Knowledge Based Math Syst, A-4040 Linz, Austria.
C3 Upper Austrian Research GmbH; PROFACTOR; University of Groningen;
   Johannes Kepler University Linz
RP Thumfart, S (corresponding author), Profactor GmbH, Machine Vis Grp, Stadtgut A2, A-4407 Steyr Gleink, Austria.
EM stefan.thumfart@profactor.at
RI Jacobs, Richard/HJY-1161-2023
FU EC [043157]
FX This work was funded by the EC under grant 043157, project SynTex.
CR AMADASUN M, 1989, IEEE T SYST MAN CYB, V19, P1264, DOI 10.1109/21.44046
   Anderson BL, 2009, J VISION, V9, DOI 10.1167/9.11.10
   [Anonymous], 2001, Schooling for Tomorrow
   [Anonymous], 2006, CONTEMPLATING ART
   [Anonymous], P 23 INT TECHN C CIR
   [Anonymous], FUZZY SYSTEMS COMPUT
   Balas BJ, 2006, VISION RES, V46, P299, DOI 10.1016/j.visres.2005.04.013
   Beck J., 1983, HUMAN MACHINE VISION
   Bergen J., 1991, Computational models of visual processing, DOI [10.7551/mitpress/2002.003.0004, DOI 10.7551/MITPRESS/2002.003.0004]
   BOVIK AC, 1990, IEEE T PATTERN ANAL, V12, P55, DOI 10.1109/34.41384
   Brodatz P., 1966, PHOTOGRAPHIC ALBUM A
   CAELLI T, 1978, BIOL CYBERN, V29, P201, DOI 10.1007/BF00337276
   CAMPBELL FW, 1968, J PHYSIOL-LONDON, V197, P551, DOI 10.1113/jphysiol.1968.sp008574
   Chuang YL, 2008, INT J DES, V2, P31
   Datta R, 2006, LECT NOTES COMPUT SC, V3953, P288, DOI 10.1007/11744078_23
   Fleming R. W., 2005, ACM Transactions on Applied Perception (TAP), V2, P346, DOI DOI 10.1145/1077399.1077409
   Fleming RW, 2003, J VISION, V3, P347, DOI 10.1167/3.5.3
   Friedman J., 2001, ELEMENTS STAT LEARNI, V1, DOI DOI 10.1007/978-0-387-84858-7
   HARALICK RM, 1973, IEEE T SYST MAN CYB, VSMC3, P610, DOI 10.1109/TSMC.1973.4309314
   Hayton JC, 2004, ORGAN RES METHODS, V7, P191, DOI 10.1177/1094428104263675
   JACOBS RHA, 2011, THESIS U GRONINGEN
   JAIN AK, 1991, PATTERN RECOGN, V24, P1167, DOI 10.1016/0031-3203(91)90143-S
   JULESZ B, 1981, NATURE, V290, P91, DOI 10.1038/290091a0
   JULESZ B, 1975, SCI AM, V232, P34, DOI 10.1038/scientificamerican0475-34
   KAWAMOTO N, 1993, COLOR RES APPL, V18, P260, DOI 10.1002/col.5080180409
   KEEBLE DRT, 1995, VISION RES, V35, P1991, DOI 10.1016/0042-6989(94)00284-S
   Kim M, 2005, LECT NOTES COMPUT SC, V3568, P550
   KIM NY, 2007, P IEEE INT S CONS EL, P1
   Kim SJ, 2006, LECT NOTES COMPUT SC, V4292, P9
   LAM C, 2005, THESIS U UTARA MALAY
   Liu F, 1996, IEEE T PATTERN ANAL, V18, P722, DOI 10.1109/34.506794
   LO I, 2006, P 6 AS DES C
   LONG H, 2001, P 17 INT JOINT C ART, P1391
   Lughofer ED, 2008, IEEE T FUZZY SYST, V16, P1393, DOI 10.1109/TFUZZ.2008.925908
   MALIK J, 1990, J OPT SOC AM A, V7, P923, DOI 10.1364/JOSAA.7.000923
   Miller A., 2002, SUBSET SELECTION REG
   *MIT MED LAB CAMBR, 1995, VIST VIS TEXT DAT
   Motoyoshi I, 2007, NATURE, V447, P206, DOI 10.1038/nature05724
   Norman D.A., 2007, EMOTIONAL DESIGN WHY
   Preacher KJ, 2008, BEHAV RES METHODS, V40, P879, DOI 10.3758/BRM.40.3.879
   Provost F., 2000, P AAAI 2000 WORKSH I, V68, P1, DOI DOI 10.1109/SOCPAR.2011
   Randen T, 1999, IEEE T PATTERN ANAL, V21, P291, DOI 10.1109/34.761261
   Rao AR, 1996, VISION RES, V36, P1649, DOI 10.1016/0042-6989(95)00202-2
   ROSENHOLTZ R, 2000, P EUR C COMP VIS, P197
   Sebe N, 2001, ADV PTRN RECOGNIT, P51
   Sharan L, 2008, J OPT SOC AM A, V25, P846, DOI 10.1364/JOSAA.25.000846
   SOEN T, 1987, COLOR RES APPL, V12, P187, DOI 10.1002/col.5080120406
   STONE M, 1974, J R STAT SOC B, V36, P111, DOI 10.1111/j.2517-6161.1974.tb00994.x
   TAKAHASHI S, 1995, PSYCHOL REV, V102, P671, DOI 10.1037/0033-295X.102.4.671
   TAMURA H, 1978, IEEE T SYST MAN CYB, V8, P460, DOI 10.1109/TSMC.1978.4309999
   THUMFART S, 2008, P MAT SENS
   THUMFART S, 2009, P MINET C MEAS SENS, V1, P19
   Tuceryan M., 1993, HDB PATTERN RECOGNIT, V2, P207, DOI DOI 10.1142/9789814343138_0010
   TURNER MR, 1986, BIOL CYBERN, V55, P71
   Tyler CW, 2004, VISION RES, V44, P2179, DOI 10.1016/j.visres.2004.03.029
   Wei LY, 2000, COMP GRAPH, P479, DOI 10.1145/344779.345009
   Yoo HW, 2006, J INF SCI ENG, V22, P1205
   Zhu SC, 2005, INT J COMPUT VISION, V62, P121, DOI 10.1007/s11263-005-4638-1
NR 58
TC 25
Z9 27
U1 3
U2 32
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD NOV
PY 2011
VL 8
IS 4
AR 27
DI 10.1145/2043603.2043609
PG 29
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 856IQ
UT WOS:000297633400006
DA 2024-07-18
ER

PT J
AU Hasic, J
   Chalmers, A
   Sikudova, E
AF Hasic, Jasminka
   Chalmers, Alan
   Sikudova, Elena
TI Perceptually Guided High-Fidelity Rendering Exploiting Movement Bias in
   Visual Attention
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Algorithms; Performance; Experimentation; Human Factors; Movement
   saliency; selective rendering; perception; saliency map; attention
AB A major obstacle for real-time rendering of high-fidelity graphics is computational complexity. A key point to consider in the pursuit of "realism in real time" in computer graphics is that the Human Visual System (HVS) is a fundamental part of the rendering pipeline. The human eye is only capable of sensing image detail in a 2 degrees foveal region, relying on rapid eye movements, or saccades, to jump between points of interest. These points of interest are prioritized based on the saliency of the objects in the scene or the task the user is performing. Such "glimpses" of a scene are then assembled by the HVS into a coherent, but inevitably imperfect, visual perception of the environment. In this process, much detail, that the HVS deems unimportant, may literally go unnoticed.
   Visual science research has identified that movement in the background of a scene may substantially influence how subjects perceive foreground objects. Furthermore, recent computer graphics work has shown that both fixed viewpoint and dynamic scenes can be selectively rendered without any perceptual loss of quality, in a significantly reduced time, by exploiting knowledge of any high-saliency movement that may be present. A high-saliency movement can be generated in a scene if an otherwise static objects starts moving. In this article, we investigate, through psychophysical experiments, including eye-tracking, the perception of rendering quality in dynamic complex scenes based on the introduction of a moving object in a scene. Two types of object movement are investigated: (i) rotation in place and (ii) rotation combined with translation. These were chosen as the simplest movement types. Future studies may include movement with varied acceleration. The object's geometry and location in the scene are not salient. We then use this information to guide our high-fidelity selective renderer to produce perceptually high-quality images at significantly reduced computation times. We also show how these results can have important implications for virtual environment and computer games applications.
C1 [Hasic, Jasminka] Sarajevo Sch Sci & Technol, Dept Comp Sci, Sarajevo 71000, Bosnia & Herceg.
   [Chalmers, Alan; Sikudova, Elena] Univ Warwick, WMG, Int Digital Lab, Coventry CV4 7AL, W Midlands, England.
C3 University of Warwick
RP Hasic, J (corresponding author), Sarajevo Sch Sci & Technol, Dept Comp Sci, Bistrik 7, Sarajevo 71000, Bosnia & Herceg.
EM jasminka.hasic@ssst.edu.ba; alan.chalmers@warwick.ac.uk;
   E.Sikudova@warwick.ac.uk
RI Šikudová, Elena/T-4763-2017; Hasic Telalovic, Jasminka/S-9016-2019;
   Šikudová, Elena/S-1078-2019
OI Šikudová, Elena/0000-0003-4572-4064; Hasic Telalovic,
   Jasminka/0000-0002-8540-5052
CR [Anonymous], 1996, P 23 ANN C COMP GRAP, DOI DOI 10.1145/237170.237262
   [Anonymous], 1998, THESIS NAVAL POSTGRA
   [Anonymous], P 2 S APPL PERC GRAP
   [Anonymous], 1890, PRINCIPLES PSYCHOL, DOI DOI 10.1037/10538-000
   *ARIS, 2003, AUGM REAL IM SYNTH I
   BARTZ D, 2008, P EUR 2008 S PAR GRA
   BOLIN MR, 1998, P 25 ANN C COMP GRAP, P299
   Bruce V., 2003, Visual perception: Physiology, psychology, and ecology
   Cater K., 2003, Eurographics Symposium on Rendering. 14th Eurographics Workshop on Rendering, P270
   Cater K., 2002, Proceedings of the ACM symposium on Virtual reality software and technology, VRST '02, P17, DOI [10.1145/585740.585744, DOI 10.1145/585740.585744]
   DEBATTISTA K, 2006, THESIS BRISTOL U
   DEBATTISTA K, 2005, P THEOR PRACT COMP G
   ELLIS G, 2006, P SPRING C COMP GRAP
   Flannery KA, 2003, CYBERPSYCHOL BEHAV, V6, P151, DOI 10.1089/109493103321640347
   Franconeri SL, 2003, PERCEPT PSYCHOPHYS, V65, P999, DOI 10.3758/BF03194829
   Haber J, 2001, COMPUT GRAPH FORUM, V20, pC142, DOI 10.1111/1467-8659.00507
   HASIC J, 2007, P SPRING C COMP GRAP
   HASIC J, 2007, P WINT SCH COMP GRAP
   HULUSIC V, 2008, P WINT SCH COMP GRAP
   Itti L, 2000, VISION RES, V40, P1489, DOI 10.1016/S0042-6989(99)00163-7
   Longhurst P., 2006, P 4 INT C COMPUTER G, P21
   LUEBKE D, 2001, P 13 EUR WORKSH REND, P221
   MACK A, 1998, P S INT 3D GRAPH
   Mania K, 2005, PRESENCE-TELEOP VIRT, V14, P606, DOI 10.1162/105474605774918769
   MARMITT G, 2002, P EUR, P217
   MASTOROPOULOU G, 2005, P C COMP GRAPH INT T
   Mather G., 2006, Foundations of Perception
   MCNAMARA A, 2000, P 12 EUR WORKSH REND
   Myszkowski K, 2001, COMP GRAPH, P221, DOI 10.1145/383259.383284
   Myszkowski K., 1998, Rendering Techniques '98. Proceedings of the Eurographics Workshop, P223
   OSullivan C., 2004, Eurographics state of the art reports, V4, P1
   Pattanaik S.N., 1998, P SIGGRAPH 98, P287
   Peters RJ, 2008, ACM T APPL PERCEPT, V5, DOI 10.1145/1279920.1279923
   PETERSON LR, 1959, J EXP PSYCHOL, V58, P193, DOI 10.1037/h0049234
   PETTERSSON R, 1999, DOC DES, V2, P114
   Ramasubramanian M, 1999, COMP GRAPH, P73, DOI 10.1145/311535.311543
   Reddy M., 1997, THESIS U EDINBURGH
   ROJAN K, 1992, BR J SOC PSYCH, V31, P81109
   Simons D.J., 1999, Gorillas in our midst: Sustained inattentional blindness for dynamic events
   Tatler BW, 2007, J VISION, V7, DOI 10.1167/7.14.4
   *USC, 2002, U SO CAL ILAB NEUR V
   WALD I, 2007, P 19 EUR WORKSH REND
   Ward G., 2003, RENDERING RADIANCE
   Watson A.B., 1993, DIGITAL IMAGES HUMAN, P179
   Watson B, 2001, COMP GRAPH, P213, DOI 10.1145/383259.383283
   Yarbus A.L., 1967, EYE MOVEMENTS VISION, DOI [10.1007/978-1-4899-5379-7, DOI 10.1007/978-1-4899-5379-7]
   Yee H, 2001, ACM T GRAPHIC, V20, P39, DOI 10.1145/383745.383748
NR 47
TC 6
Z9 7
U1 0
U2 5
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD OCT
PY 2010
VL 8
IS 1
AR 6
DI 10.1145/1857893.1857899
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 748AJ
UT WOS:000289362100006
DA 2024-07-18
ER

PT J
AU Khan, MM
   Ward, RD
   Ingleby, M
AF Khan, Masood Mehmood
   Ward, Robert D.
   Ingleby, Michael
TI Classifying Pretended and Evoked Facial Expressions of Positive and
   Negative Affective States using Infrared Measurement of Skin Temperature
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Review
DE Design; Experimentation; Physiology-based automated affect recognition;
   facial expression classification; affective computing and thermal
   infrared imaging
ID THERMAL IMAGE-ANALYSIS; FACE RECOGNITION; EMOTION; RESPONSES;
   INFORMATION; DISGUST; THERMOGRAPHY; SPECIFICITY; SEQUENCES; MUSCLES
AB Earlier researchers were able to extract the transient facial thermal features from thermal infrared images (TIRIs) to make binary distinctions between the expressions of affective states. However, effective human-computer interaction would require machines to distinguish between the subtle facial expressions of affective states. This work, for the first time, attempts to use the transient facial thermal features for recognizing a much wider range of facial expressions. A database of 324 time-sequential, visible-spectrum, and thermal facial images was developed representing different facial expressions from 23 participants in different situations. A novel facial thermal feature extraction, selection, and classification approach was developed and invoked on various Gaussian mixture models constructed using: neutral and pretended happy and sad faces, faces with multiple positive and negative facial expressions, faces with neutral and six (pretended) basic facial expressions, and faces with evoked happiness, sadness, disgust, and anger. This work demonstrates that (1) infrared imaging can be used to observe the affective-state-specific facial thermal variations, (2) pixel-grey level analysis of TIRIs can help localise significant facial thermal feature points along the major facial muscles, and (3) cluster-analytic classification of transient thermal features can help distinguish between the facial expressions of affective states in an optimized eigenspace of input thermal feature vectors. The observed classification results exhibited influence of a Gaussian mixture model's structure on classifier-performance. The work also unveiled some pertinent aspects of future research on the use of facial thermal features in automated facial expression classification and affect recognition.
C1 [Khan, Masood Mehmood] Curtin Univ Technol, Fac Sci & Engn, Perth, WA 6845, Australia.
   [Ward, Robert D.] Univ Huddersfield, Dept Behav Sci, Huddersfield HD1 3DH, W Yorkshire, England.
   [Ingleby, Michael] Univ Huddersfield, Sch Comp & Engn, Huddersfield HD1 3DH, W Yorkshire, England.
C3 Curtin University; University of Huddersfield; University of
   Huddersfield
RP Khan, MM (corresponding author), Curtin Univ Technol, Fac Sci & Engn, GPO Box U1987, Perth, WA 6845, Australia.
RI Khan, Masood/AAP-6939-2020
OI Khan, Masood/0000-0002-2769-2380
CR Abidi B, 2004, 38TH ANNUAL 2004 INTERNATIONAL CARNAHAN CONFERENCE ON SECURITY TECHNOLOGY, PROCEEDINGS, P325, DOI 10.1109/CCST.2004.1405413
   Acharya T, 2005, IMAGE PROCESSING: PRINCIPLES AND APPLICATIONS, P1, DOI 10.1002/0471745790
   Allanson J, 2004, INTERACT COMPUT, V16, P857, DOI 10.1016/j.intcom.2004.08.001
   ANG LBP, 1997, BRIT J PSYCHIAT, V88, P519
   [Anonymous], 2004, SPR PRO COM, DOI 10.1007/978-1-4757-4036-3
   Bartlett MS, 1999, PSYCHOPHYSIOLOGY, V36, P253, DOI 10.1017/S0048577299971664
   Black MJ, 1997, INT J COMPUT VISION, V25, P23, DOI 10.1023/A:1007977618277
   BOULIC LER, 1998, IEEE COMPUT GRAPH, V98, P8
   Bradley MA, 2003, BEHAV NEUROSCI, V117, P369, DOI 10.1037/0735-7044.117.2.369
   Breitsprecher L, 2002, ANN ANAT, V184, P27, DOI 10.1016/S0940-9602(02)80030-9
   Bulut M., 2004, P 6 INT C MULT INT, P205
   Calder AJ, 2001, VISION RES, V41, P1179, DOI 10.1016/S0042-6989(01)00002-5
   *CANTR SYST INC, 2001, IR 860 US MAN
   Chen XW, 2003, PATTERN RECOGN LETT, V24, P1295, DOI 10.1016/S0167-8655(02)00371-9
   Christie IC, 2004, INT J PSYCHOPHYSIOL, V51, P143, DOI 10.1016/j.ijpsycho.2003.08.002
   Cohen I, 2003, COMPUT VIS IMAGE UND, V91, P160, DOI 10.1016/S1077-3142(03)00081-X
   Collet C, 1997, J AUTONOM NERV SYST, V62, P45, DOI 10.1016/S0165-1838(96)00108-7
   CRISSEY JT, 2004, CLIN DERMATOL, V11, P197
   Critchley HD, 2000, BRAIN, V123, P2203, DOI 10.1093/brain/123.11.2203
   De Silva LC, 1997, ICICS - PROCEEDINGS OF 1997 INTERNATIONAL CONFERENCE ON INFORMATION, COMMUNICATIONS AND SIGNAL PROCESSING, VOLS 1-3, P397, DOI 10.1109/ICICS.1997.647126
   *DHEW, 1979, PUBL DHEW
   DIMBERG U, 1990, SCAND J PSYCHOL, V31, P228, DOI 10.1111/j.1467-9450.1990.tb00835.x
   DIMBERG U, 1990, PSYCHOPHYSIOLOGY, V27, P481, DOI 10.1111/j.1469-8986.1990.tb01962.x
   Dror IE, 2005, APPL COGNITIVE PSYCH, V19, P799, DOI 10.1002/acp.1130
   Dubuisson S, 2002, SIGNAL PROCESS-IMAGE, V17, P657, DOI 10.1016/S0923-5965(02)00076-0
   Duda R. O., 2000, PATTERN CLASSIFICATI
   Edwards G., 1998, PROC 5 EUROPEAN C CO, V2, P581
   EKMAN P, 1983, SCIENCE, V221, P1208, DOI 10.1126/science.6612338
   Ekman P., 1982, EMOTION HUMAN FACE
   EKMAN P, 2000, J PERS SOC PSYCHOL, V58, P342
   Essa IA, 1997, IEEE T PATTERN ANAL, V19, P757, DOI 10.1109/34.598232
   Eveland CK, 2003, IMAGE VISION COMPUT, V21, P579, DOI 10.1016/S0262-8856(03)00056-8
   Everitt B.S., 1991, APPL MULTIVARIATE DA
   Fujimasa I, 1998, IEEE ENG MED BIOL, V17, P34, DOI 10.1109/51.687961
   Fujimasa I, 2000, IEEE ENG MED BIOL, V19, P71, DOI 10.1109/51.844383
   Garbey M, 2004, PROC CVPR IEEE, P356
   Gonzalez R. C., 2006, Digital Image Processing, V3rd
   Healey JA, 2005, IEEE T INTELL TRANSP, V6, P156, DOI 10.1109/TITS.2005.848368
   Herry CL, 2002, P ANN INT IEEE EMBS, P1157, DOI 10.1109/IEMBS.2002.1106324
   HESS U, 1992, INT J PSYCHOPHYSIOL, V12, P251, DOI 10.1016/0167-8760(92)90064-I
   Huang CL, 1997, J VIS COMMUN IMAGE R, V8, P278, DOI 10.1006/jvci.1997.0359
   *IAPS, 1997, INT AFF PICT SYST TE
   Iwase M, 2002, NEUROIMAGE, V17, P758, DOI 10.1006/nimg.2002.1225
   Jiang GT, 2007, 2007 IEEE/ICME INTERNATIONAL CONFERENCE ON COMPLEX MEDICAL ENGINEERING, VOLS 1-4, P824, DOI 10.1109/ICCME.2007.4381856
   Jolliffe I. T., 2002, PRINCIPAL COMPONENT
   Jones BF, 1998, IEEE T MED IMAGING, V17, P1019, DOI 10.1109/42.746635
   JONES CH, 1988, APPL THERMAL IMAGING, P156
   Kakadiaris IA, 2005, PROC CVPR IEEE, P1183
   Kall R., 1990, CLIN EMG SURFACE REC, V2
   Khan M. M., 2008, THESIS U HUDDERSFIEL
   Khan MM, 2006, ACM T AUTON ADAP SYS, V1, P91, DOI 10.1145/1152934.1152939
   Khan MM, 2004, CONF CYBERN INTELL S, P202
   KHAN MM, 2005, P 19 BRIT HCI GROUP
   Kim KH, 2004, MED BIOL ENG COMPUT, V42, P419, DOI 10.1007/BF02344719
   Kinnear P.R., 2000, SPSS for windows made simple: Release 10
   Kobayashi H., 2004, International Journal of Cosmetic Science, V26, P91, DOI 10.1111/j.0412-5463.2004.00208.x
   Kong SG, 2005, COMPUT VIS IMAGE UND, V97, P103, DOI 10.1016/j.cviu.2004.04.001
   Kunzmann U, 2005, PSYCHOL AGING, V20, P47, DOI 10.1037/0882-7974.20.1.47
   Lisetti CL, 2004, EURASIP J APPL SIG P, V2004, P1672, DOI 10.1155/S1110865704406192
   Lyons M., 1999, P 2 INT C COGN SCI, P113
   Matsuzaki H., 1996, Progress in Biophysics and Molecular Biology, V65, P185
   MCFARLAND RA, 1991, INT J PSYCHOPHYSIOL, V11, P295, DOI 10.1016/0167-8760(91)90024-R
   McGimpsey JG, 2000, BRIT J ORAL MAX SURG, V38, P581, DOI 10.1054/bjom.2000.0524
   MCLACHLAN G., 2004, WILEY SER PROB STAT
   MURPHY ST, 1993, J PERS SOC PSYCHOL, V64, P723, DOI 10.1037/0022-3514.64.5.723
   NAEMURA A, 1993, JPN J PSYCHOL, V64, P51, DOI 10.4992/jjpsy.64.51
   Nakayama K, 2005, PHYSIOL BEHAV, V84, P783, DOI 10.1016/j.physbeh.2005.03.009
   Niedenthal PM, 2000, EUR J SOC PSYCHOL, V30, P211, DOI 10.1002/(SICI)1099-0992(200003/04)30:2<211::AID-EJSP988>3.0.CO;2-3
   Ogasawara T, 2001, ORAL SURG ORAL MED O, V92, P473, DOI 10.1067/moe.2001.116508
   Olivier B, 2003, EUR J PHARMACOL, V463, P117, DOI 10.1016/S0014-2999(03)01326-8
   Otsuka K, 2002, IEEE ENG MED BIOL, V21, P49, DOI 10.1109/MEMB.2002.1175138
   Palomba D, 2000, INT J PSYCHOPHYSIOL, V36, P45, DOI 10.1016/S0167-8760(99)00099-9
   Partala T, 2006, INTERACT COMPUT, V18, P208, DOI 10.1016/j.intcom.2005.05.002
   Pavlidis I, 2002, IEEE ENG MED BIOL, V21, P56, DOI 10.1109/MEMB.2002.1175139
   PAVLIDIS I, 2004, P SPIE THERM 26 APR, P270
   Pessa JE, 1998, CLIN ANAT, V11, P310, DOI 10.1002/(SICI)1098-2353(1998)11:5<310::AID-CA3>3.0.CO;2-T
   Pham TH, 2000, ENCEPHALE, V26, P45
   PHILLIPS TJ, 2002, ADV SEMICONDUCTOR MA, V15, P32
   Picard R.W., 2000, Affective Computing
   Pizzagalli D, 1998, NEUROPSYCHOLOGIA, V36, P323, DOI 10.1016/S0028-3932(97)00117-6
   Pollina DA, 2006, ANN BIOMED ENG, V34, P1182, DOI 10.1007/s10439-006-9143-3
   Posamentier MT, 2003, NEUROPSYCHOL REV, V13, P113, DOI 10.1023/A:1025519712569
   Prokoski F.J., 1999, BIOMETRICS PERSONAL, P191
   Puri C., 2005, CHI 05 EXTENDED ABST, P1725
   RimmKaufman SE, 1996, MOTIV EMOTION, V20, P63, DOI 10.1007/BF02251007
   Root AA, 2003, J PHYSIOL-LONDON, V549, P289, DOI 10.1113/jphysiol.2002.035691
   Sarlo M, 2005, NEUROSCI LETT, V382, P291, DOI 10.1016/j.neulet.2005.03.037
   SCHWARTZ GE, 1976, SCIENCE, V192, P489, DOI 10.1126/science.1257786
   Schwarz S, 2002, COMPUT METH PROG BIO, V67, P55, DOI 10.1016/S0169-2607(00)00150-4
   Sharma S., 1996, APPL MULTIVARIATE TE, P384
   Sinha R, 1996, COGNITION EMOTION, V10, P173, DOI 10.1080/026999396380321
   Socolinsky DA, 2003, COMPUT VIS IMAGE UND, V91, P72, DOI 10.1016/S1077-3142(03)00075-4
   Sugimoto Y, 2000, ROBOT AUTON SYST, V31, P147, DOI 10.1016/S0921-8890(99)00104-9
   Tian YL, 2002, FIFTH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, PROCEEDINGS, P229, DOI 10.1109/AFGR.2002.1004159
   Toivanen J, 2004, LANG SPEECH, V47, P383, DOI 10.1177/00238309040470040301
   *US GOV, 1979, ETH PRINC GUID PROT
   Vianna DML, 2005, EUR J NEUROSCI, V21, P2505, DOI 10.1111/j.1460-9568.2005.04073.x
   Vrana SR, 2004, BIOL PSYCHOL, V66, P63, DOI 10.1016/j.biopsycho.2003.07.004
   VRANA SR, 1993, PSYCHOPHYSIOLOGY, V30, P279, DOI 10.1111/j.1469-8986.1993.tb03354.x
   Webb A.R., 2003, Statistical Pattern Recognition
   WHITESIDE SP, 1998, P ICSLP, P699
   Wild B, 2001, PSYCHIAT RES, V102, P109, DOI 10.1016/S0165-1781(01)00225-6
   WOLF K, 2005, PAIN RES MANAG, V10, P9
   Wright P, 2004, NEUROREPORT, V15, P2347, DOI 10.1097/00001756-200410250-00009
   Yoshitomi Y, 2000, IEEE RO-MAN 2000: 9TH IEEE INTERNATIONAL WORKSHOP ON ROBOT AND HUMAN INTERACTIVE COMMUNICATION, PROCEEDINGS, P178, DOI 10.1109/ROMAN.2000.892491
   ZAJONC RB, 1985, SCIENCE, V228, P15, DOI 10.1126/science.3883492
NR 106
TC 59
Z9 69
U1 2
U2 54
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD FEB
PY 2009
VL 6
IS 1
AR 6
DI 10.1145/1462055.1462061
PG 22
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA 450YM
UT WOS:000266437900006
OA Bronze, Green Published
DA 2024-07-18
ER

PT J
AU Lavoué, G
AF Lavoue, Guillaume
TI A Local Roughness Measure for 3D Meshes and its Application to Visual
   Masking
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article; Proceedings Paper
CT Symposium on Applied Perception in Graphics and Visualization
CY JUL 25-27, 2007
CL Tubingen, GERMANY
DE Algorithms; Measurement; 3D mesh; Roughness; Curvature; Masking;
   subjective evaluation
ID QUALITY
AB 3D models are subject to a wide variety of processing operations such as compression, simplification or watermarking, which may introduce some geometric artifacts on the shape. The main issue is to maximize the compression/simplification ratio or the watermark strength while minimizing these visual degradations. However few algorithms exploit the human visual system to hide these degradations, while perceptual attributes could be quite relevant for this task. Particularly, the masking effect defines the fact that one visual pattern can hide the visibility of another. In this context we introduce an algorithm for estimating the roughness of a 3D mesh, as a local measure of geometric noise on the surface. Indeed, a textured (or rough) region is able to hide geometric distortions much better than a smooth one. Our measure is based on curvature analysis on local windows of the mesh and is independent of the resolution/connectivity of the object. The accuracy and the robustness of our measure, together with its relevance regarding visual masking have been demonstrated through extensive comparisons with state-of-the-art and subjective experiment. Two applications are also presented, in which the roughness is used to lead (and improve) respectively compression and watermarking algorithms.
C1 Univ Lyon, CNRS, UMR 5205, INSA Lyon,LIRIS, F-69621 Villeurbanne, France.
C3 Institut National des Sciences Appliquees de Lyon - INSA Lyon; Centre
   National de la Recherche Scientifique (CNRS)
RP Lavoué, G (corresponding author), Univ Lyon, CNRS, UMR 5205, INSA Lyon,LIRIS, F-69621 Villeurbanne, France.
CR Alliez P, 2003, ACM T GRAPHIC, V22, P485, DOI 10.1145/882262.882296
   [Anonymous], 2005, ACM Trans. Appl. Percep.
   Bolin M. R., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P299, DOI 10.1145/280814.280924
   CAMPBELL FW, 1966, J PHYSIOL-LONDON, V187, P437, DOI 10.1113/jphysiol.1966.sp008101
   *CCIR, 1986, REC REP CCIR INT TEL
   Chow MM, 1997, VISUALIZATION '97 - PROCEEDINGS, P347, DOI 10.1109/VISUAL.1997.663902
   Cohen-Steiner D, 2004, ACM T GRAPHIC, V23, P905, DOI 10.1145/1015706.1015817
   Cohen-Steiner D., 2003, P 19 ANN S COMPUTATI, P3121
   Corsini M, 2007, IEEE T MULTIMEDIA, V9, P247, DOI 10.1109/TMM.2006.886261
   Dumont R, 2003, ACM T GRAPHIC, V22, P152, DOI 10.1145/636886.636888
   Eckert MP, 1998, SIGNAL PROCESS, V70, P177, DOI 10.1016/S0165-1684(98)00124-8
   Ferwerda J. A., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P143, DOI 10.1145/258734.258818
   HARMON LD, 1973, SCIENCE, V180, P1194, DOI 10.1126/science.180.4091.1194
   ISENBURG M, 2001, ACM SIGGRAPH, P263
   Jones TR, 2003, ACM T GRAPHIC, V22, P943, DOI 10.1145/882262.882367
   Karni Z, 2000, COMP GRAPH, P279, DOI 10.1145/344779.344924
   Kim SJ, 2002, 10TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P276, DOI 10.1109/PCCGA.2002.1167871
   LAVOUE G, 2008, EUR WORKSH 3D OBJ RE, P25
   Lavoué G, 2006, PROC SPIE, V6312, DOI 10.1117/12.686964
   Lee CH, 2005, ACM T GRAPHIC, V24, P659, DOI 10.1145/1073204.1073244
   Luebke D, 2001, SPRING EUROGRAP, P223
   Ohbuchi R, 2002, COMPUT GRAPH FORUM, V21, P373, DOI 10.1111/1467-8659.t01-1-00597
   Pan YX, 2005, IEEE T MULTIMEDIA, V7, P269, DOI 10.1109/TMM.2005.843364
   QU L, 2006, I3D 06, P199
   Reddy M, 2001, IEEE COMPUT GRAPH, V21, P68, DOI 10.1109/38.946633
   Rogowitz BE, 2001, P SOC PHOTO-OPT INS, V4299, P340, DOI 10.1117/12.429504
   RONDAOALFACE P, 2005, SPIE ELECT IMAGING S, P230
   TAUBIN G, 1995, ACM SIGGRAPH, P351
   Tian Dihong, 2004, P 12 ANN ACM INT C M, P684, DOI [10.1145/1027527.1027684, DOI 10.1145/1027527.1027684]
   UCCHEDDU F, 2007, THESIS U FLORENCE IT
   Wang K, 2007, 2007 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOLS 1-5, P1235
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Watson A.B., 1993, DIGITAL IMAGES HUMAN, P179
   Watson B, 2001, COMP GRAPH, P213, DOI 10.1145/383259.383283
   WILLIAMS N, 2003, ACM S INT 3D GRAPH, P113
   Wolfgang RB, 1999, P IEEE, V87, P1108, DOI 10.1109/5.771067
   Wu JH, 2001, NINTH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P12, DOI 10.1109/PCCGA.2001.962853
NR 37
TC 64
Z9 68
U1 0
U2 7
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD JAN
PY 2009
VL 5
IS 4
AR 21
DI 10.1145/1462048.1462052
PG 23
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 450YL
UT WOS:000266437800004
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Fortenbaugh, FC
   Chaudhury, S
   Hicks, JC
   Hao, L
   Turano, KA
AF Fortenbaugh, Francesca C.
   Chaudhury, Sidhartha
   Hicks, John C.
   Hao, Lei
   Turano, Kathleen A.
TI Gender Differences in Cue Preference During Path Integration in Virtual
   Environments
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Human Factors; Gender differences; path integration; egocentric
   reference frame; cue preference
ID SPATIAL ABILITY; SEX-DIFFERENCES; NAVIGATION; PERCEPTION; DISTANCE;
   ILLUSION; BLIND
AB Three studies were conducted to examine whether men and women differ in how they recalibrate their path-integration systems when walking without vision in virtual environments. Distance cues provided by a scene and a tone, which ended each trial, were placed in conflict. Participants briefly viewed a room with a target, which was offset from their midlines and hung inside a doorframe on the far wall. After viewing, participants walked to the target's position until a tone sounded, ending the trial. In two experiments the doorframe was placed at 6 m and the tone sounded at 4 or 8 m. The rooms had minimal or photorealistic texturing applied. The third experiment used photorealistic texturing, but here the tone sounded at 6 m and the doorframe was presented at 4 or 8 m. Path angles were recorded to estimate perceived distance to the target. In all conditions tested, the women failed to scale their path angles. The men, however, scaled their path-angles with the auditory cue in the minimal-texture condition, but with the visual cue in the photorealistic-texture conditions. These results suggest that gender differences exist in the way that humans recalibrate their path-integration systems when walking without vision in virtual environments.
C1 [Fortenbaugh, Francesca C.; Chaudhury, Sidhartha; Hicks, John C.; Hao, Lei; Turano, Kathleen A.] Lions Vis Ctr, Baltimore, MD 21205 USA.
RP Fortenbaugh, FC (corresponding author), Lions Vis Ctr, 550 N Broadway, Baltimore, MD 21205 USA.
EM kturano@jhmi.edu
CR [Anonymous], 2005, ACM Transactions on Applied Perception, DOI [DOI 10.1145/1077399.1077403, DOI 10.1145/1077399.10774032,3,9]
   Chaudhury S, 2004, EXP BRAIN RES, V159, P360, DOI 10.1007/s00221-004-1961-7
   Cutmore TRH, 2000, INT J HUM-COMPUT ST, V53, P223, DOI 10.1006/ijhc.2000.0389
   Dabbs JM, 1998, EVOL HUM BEHAV, V19, P89, DOI 10.1016/S1090-5138(97)00107-4
   Dassonville P, 2004, PLOS BIOL, V2, P1936, DOI 10.1371/journal.pbio.0020364
   Dassonville P, 2004, VISION RES, V44, P1025, DOI 10.1016/j.visres.2003.10.018
   Dassonville P, 2004, VISION RES, V44, P603, DOI 10.1016/j.visres.2003.10.017
   Ellard CG, 2003, PERCEPTION, V32, P567, DOI 10.1068/p5041
   Grön G, 2000, NAT NEUROSCI, V3, P404, DOI 10.1038/73980
   Ismail AR, 1999, J BIOMECH, V32, P317, DOI 10.1016/S0021-9290(98)00171-7
   Lackner JR, 2005, ANNU REV PSYCHOL, V56, P115, DOI 10.1146/annurev.psych.55.090902.142023
   LINN MC, 1985, CHILD DEV, V56, P1479, DOI 10.1111/j.1467-8624.1985.tb00213.x
   Loomis JM, 1999, WAYFINDING BEHAVIOR, P125
   Loomis JM, 2003, VIRTUAL AND ADAPTIVE ENVIRONMENTS: APPLICATIONS, IMPLICATIONS, AND HUMAN PERFORMANCE ISSUES, P21
   LOOMIS JM, 1993, J EXP PSYCHOL GEN, V122, P73, DOI 10.1037/0096-3445.122.1.73
   Loomis JM, 2002, J EXP PSYCHOL LEARN, V28, P335, DOI 10.1037//0278-7393.28.2.335
   MACKENZIE IS, 1993, HUMAN FACTORS IN COMPUTING SYSTEMS, P488
   Maguire EA, 1999, CURR OPIN NEUROBIOL, V9, P171, DOI 10.1016/S0959-4388(99)80023-3
   Mittelstaedt ML, 2001, EXP BRAIN RES, V139, P318, DOI 10.1007/s002210100735
   Philbeck JW, 1997, J EXP PSYCHOL HUMAN, V23, P72, DOI 10.1037/0096-1523.23.1.72
   RIESER JJ, 1990, PERCEPTION, V19, P675, DOI 10.1068/p190675
   Sandstrom NJ, 1998, COGNITIVE BRAIN RES, V6, P351, DOI 10.1016/S0926-6410(98)00002-0
   Stanney KM, 1998, PRESENCE-TELEOP VIRT, V7, P327, DOI 10.1162/105474698565767
   Thompson E., 2004, J MENS STUDIES, V13, P5, DOI [DOI 10.3149/JMS.1301.5, https://doi.org/10.3149/jms.1301.5, 10.3149/jms.1301.5]
   TURANO KA, 2005, SPIE IS T ELECT IMAG, P416
   Viaud-Delmon I, 1998, NAT NEUROSCI, V1, P15, DOI 10.1038/215
   VOYER D, 1995, PSYCHOL BULL, V117, P250, DOI 10.1037/0033-2909.117.2.250
   WLOKA MM, 1995, PRESENCE-TELEOP VIRT, V4, P50, DOI 10.1162/pres.1995.4.1.50
NR 28
TC 4
Z9 5
U1 0
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD JAN
PY 2007
VL 4
IS 1
AR 6
DI 10.1145/1227134.1227140
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA V04IL
UT WOS:000207052000006
DA 2024-07-18
ER

PT J
AU Adkins, A
   Lin, L
   Normoyle, A
   Canales, R
   Ye, YT
   Jörg, S
AF Adkins, Alex
   Lin, Lorraine
   Normoyle, Aline
   Canales, Ryan
   Ye, Yuting
   Jorg, Sophie
TI Evaluating Grasping Visualizations and Control Modes in a VR Game
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Virtual reality; game; input devices; virtual hands; grasping
   visualization
ID VISUAL FEEDBACK
AB A primary goal of the Virtual Reality (VR) community is to build fully immersive and presence-inducing environments with seamless and natural interactions. To reach this goal, researchers are investigating how to best directly use our hands to interact with a virtual environment using hand tracking. Most studies in this field require participants to perform repetitive tasks. In this article, we investigate if results of such studies translate into a real application and game-like experience. We designed a virtual escape room in which participants interact with various objects to gather clues and complete puzzles. In a between-subjects study, we examine the effects of two input modalities (controllers vs. hand tracking) and two grasping visualizations (continuously tracked hands vs. virtual hands that disappear when grasping) on ownership, realism, efficiency, enjoyment, and presence.
   Our results show that ownership, realism, enjoyment, and presence increased when using hand tracking compared to controllers. Visualizing the tracked hands during grasps leads to higher ratings in one of our ownership questions and one of our enjoyment questions compared to having the virtual hands disappear during grasps as is common in many applications. We also confirm some of the main results of two studies that have a repetitive design in a more realistic gaming scenario that might be closer to a typical user experience.
C1 [Adkins, Alex; Lin, Lorraine; Canales, Ryan; Jorg, Sophie] Clemson Univ, Sch Comp, Clemson, SC 29634 USA.
   [Normoyle, Aline] Bryn Mawr Coll, Bryn Mawr, PA USA.
   [Ye, Yuting] Facebook Real Labs, 9845 Willows Rd, Redmond, WA 98052 USA.
C3 Clemson University; Bryn Mawr College; Facebook Inc
RP Adkins, A (corresponding author), Clemson Univ, Sch Comp, Clemson, SC 29634 USA.
EM adkins4@clemson.edu; lorrain@clemson.edu; anormoyle@brynmawr.edu;
   rcanale@clemson.edu; yuting.ye@oculus.com; sjoerg@clemson.edu
RI Ye, Yuting/ABB-9360-2020
OI Ye, Yuting/0000-0003-2643-7457
FU Facebook Reality Labs; National Science Foundation [IIS-1652210]
FX This material was supported in part by Facebook Reality Labs. Alex
   Adkins, Ryan Canales, and Sophie Jorg's contribution is furthermore
   based in part upon work supported by the National Science Foundation
   under Grant No. IIS-1652210.
CR Ali M, 2020, ACM SYMPOSIUM ON APPLIED PERCEPTION (SAP 2020), DOI 10.1145/3385955.3407923
   Argelaguet F, 2016, P IEEE VIRT REAL ANN, P3, DOI 10.1109/VR.2016.7504682
   Brown B, 2011, 29TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P1657
   Canales R, 2019, ACM CONFERENCE ON APPLIED PERCEPTION (SAP 2019), DOI 10.1145/3343036.3343132
   Center for Self-Determination Theory, 2000, INTRINSIC MOTIVATION
   Denisova A, 2016, CHI PLAY 2016: PROCEEDINGS OF THE 2016 ANNUAL SYMPOSIUM ON COMPUTER-HUMAN INTERACTION IN PLAY, P33, DOI 10.1145/2967934.2968095
   Dewez D, 2021, CHI '21: PROCEEDINGS OF THE 2021 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3411764.3445379
   Han SC, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201399
   Henze N, 2011, INT J MOB HUM COMPUT, V3, P71, DOI 10.4018/jmhci.2011100105
   Lam MC, 2018, MULTIMED TOOLS APPL, V77, P16367, DOI 10.1007/s11042-017-5205-9
   Lin Lorraine, 2019, 2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR), P510, DOI 10.1109/VR.2019.8797787
   Lin L, 2017, ACM SYMPOSIUM ON APPLIED PERCEPTION (SAP 2017), DOI 10.1145/3119881.3119884
   Lin Lorraine., 2016, Proceedings of the ACM Symposium on Applied Perception, P69, DOI [DOI 10.1145/2931002.2931006, 10.1145/2931002.2931006]
   Lougiakis C, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR 2020), P510, DOI [10.1109/VR46266.2020.00-32, 10.1109/VR46266.2020.1581086151885]
   Lugrin JL, 2018, 25TH 2018 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P17, DOI 10.1109/VR.2018.8446229
   Ma K, 2013, FRONT PSYCHOL, V4, DOI 10.3389/fpsyg.2013.00604
   McMahan RP, 2010, IEEE SYMPOSIUM ON 3D USER INTERFACES (3DUI 2010), P11, DOI 10.1109/3DUI.2010.5444727
   Moehring M, 2011, P IEEE VIRT REAL ANN, P131, DOI 10.1109/VR.2011.5759451
   Normoyle A., 2014, Proceedings of the ACM symposium on applied perception, P117
   Prachyabrued M., 2012, 2012 IEEE Symposium on 3D User Interfaces (3DUI), P39, DOI 10.1109/3DUI.2012.6184182
   Prachyabrued M, 2016, IEEE T VIS COMPUT GR, V22, P1718, DOI 10.1109/TVCG.2015.2456917
   Prachyabrued M, 2014, 2014 IEEE SYMPOSIUM ON 3D USER INTERFACES (3DUI), P19, DOI 10.1109/3DUI.2014.6798835
   Rigby S, 2007, The player experience of need satisfaction (PENS) model
   Seinfeld S, 2022, IEEE T VIS COMPUT GR, V28, P1545, DOI 10.1109/TVCG.2020.3021342
   Sheridan T., 1992, Presence: Teleoperators and Virtual Environments, V1, P120, DOI DOI 10.1162/PRES.1992.1.1.120
   Skalski P, 2011, NEW MEDIA SOC, V13, P224, DOI 10.1177/1461444810370949
   Slater M, 2008, FRONT HUM NEUROSCI, V2, DOI 10.3389/neuro.09.006.2008
   Steed A, 2016, IEEE T VIS COMPUT GR, V22, P1406, DOI 10.1109/TVCG.2016.2518135
   Tamborini R, 2006, LEA COMMUN SER, P225
   Unity, 2012, UN 4 0 MEC AN TUT
   Vosinakis S, 2018, VIRTUAL REAL-LONDON, V22, P47, DOI 10.1007/s10055-017-0313-4
   Yuan Y, 2010, P IEEE VIRT REAL ANN, P95, DOI 10.1109/VR.2010.5444807
NR 32
TC 9
Z9 10
U1 0
U2 17
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD OCT
PY 2021
VL 18
IS 4
AR 19
DI 10.1145/3486582
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA YY1DS
UT WOS:000754533100003
DA 2024-07-18
ER

PT J
AU Gagnon, HC
   Rosales, CS
   Mileris, R
   Stefanucci, JK
   Creem-Regehr, SH
   Bodenheimer, RE
AF Gagnon, Holly C.
   Rosales, Carlos Salas
   Mileris, Ryan
   Stefanucci, Jeanine K.
   Creem-Regehr, Sarah H.
   Bodenheimer, Robert E.
TI Estimating Distances in Action Space in Augmented Reality
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Augmented reality; distance perception; avatars
ID SCALING APPARENT DISTANCE; VIRTUAL ENVIRONMENTS; EGOCENTRIC DISTANCE;
   PERCEPTION; JUDGMENTS; GRAPHICS; QUALITY; WALKING
AB Augmented reality (AR) is important for training complex tasks, such as navigation, assembly, and medical procedures. The effectiveness of such training may depend on accurate spatial localization of AR objects in the environment. This article presents two experiments that test egocentric distance perception in augmented reality within and at the boundaries of action space (up to 35 m) in comparison with distance perception in a matched real-world (RW) environment. Using the Microsoft HoloLens, in Experiment 1, participants in two different RW settings judged egocentric distances (ranging from 10 to 35 m) to an AR avatar or a real person using a visual matching measure. Distances to augmented targets were underestimated compared to real targets in the two indoor, RW contexts. Experiment 2 aimed to generalize the results to an absolute distance measure using verbal reports in one of the indoor environments. Similar to Experiment 1, distances to augmented targets were underestimated compared to real targets. We discuss these findings with respect to the importance of methodologies that directly compare performance in real and mediated environments, as well as the inherent differences present in mediated environments that are "matched" to the real world.
C1 [Gagnon, Holly C.] Univ Utah, Psychol, 380 South 1530 East,Room 502, Salt Lake City, UT 84112 USA.
   [Rosales, Carlos Salas; Bodenheimer, Robert E.] Vanderbilt Univ, PMB 351679,2301 Vanderbilt Pl, Nashville, TN 37235 USA.
   [Mileris, Ryan; Stefanucci, Jeanine K.; Creem-Regehr, Sarah H.] Univ Utah, Salt Lake City, UT 84112 USA.
C3 Utah System of Higher Education; University of Utah; Vanderbilt
   University; Utah System of Higher Education; University of Utah
RP Gagnon, HC (corresponding author), Univ Utah, Psychol, 380 South 1530 East,Room 502, Salt Lake City, UT 84112 USA.
EM holly.gagnon@psych.utah.edu
FU Office of Naval Research [N00014-18-1-2964]
FX This material is based in part upon work supported by the Office of
   Naval Research under Grant No. N00014-18-1-2964.
CR Adams H, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES WORKSHOPS (VRW 2020), P547, DOI [10.1109/VRW50115.2020.0-151, 10.1109/VRW50115.2020.00125]
   Akçayir M, 2017, EDUC RES REV-NETH, V20, P1, DOI 10.1016/j.edurev.2016.11.002
   Andre J, 2006, PERCEPT PSYCHOPHYS, V68, P353, DOI 10.3758/BF03193682
   Bodenheimer B, 2007, APGV 2007: SYMPOSIUM ON APPLIED PERCEPTION IN GRAPHICS AND VISUALIZATION, PROCEEDINGS, P35
   Buck LE, 2018, ACM T APPL PERCEPT, V15, DOI 10.1145/3196885
   Creem-Regehr SH, 2016, SAP 2015: ACM SIGGRAPH SYMPOSIUM ON APPLIED PERCEPTION, P47, DOI 10.1145/2804408.2804422
   Creem-Regehr SH, 2015, PSYCHOL LEARN MOTIV, V62, P195, DOI 10.1016/bs.plm.2014.09.006
   Creem-Regehr SH, 2005, PERCEPTION, V34, P191, DOI 10.1068/p5144
   Cutting JE, 1995, PERCEPTION SPACE MOT, P69, DOI [DOI 10.1016/B978-012240530-3/50005-5, 10.1016/B978-012240530-3/50005-5]
   Dey A., 2010, Proceedings of the 17th ACM Symposium on Virtual Reality Software and Technology (VRST), P211, DOI DOI 10.1145/1889863.1889911
   Dey A, 2018, FRONT ROBOT AI, V5, DOI 10.3389/frobt.2018.00037
   Dey A, 2012, INT SYM MIX AUGMENT, P187, DOI 10.1109/ISMAR.2012.6402556
   Diaz C, 2017, PROCEEDINGS OF THE 2017 IEEE INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY (ISMAR), P111, DOI 10.1109/ISMAR.2017.28
   Gagnon HC, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR 2020), P922, DOI [10.1109/VR46266.2020.00117, 10.1109/VR46266.2020.00112]
   Gao Y, 2020, J SOC INF DISPLAY, V28, P117, DOI 10.1002/jsid.832
   Interrante V, 2007, IEEE VIRTUAL REALITY 2007, PROCEEDINGS, P11
   Interrante V, 2006, P IEEE VIRT REAL ANN, P3, DOI 10.1109/VR.2006.52
   Kelly JW, 2017, ACM T APPL PERCEPT, V15, DOI 10.1145/3106155
   Kunz BR, 2009, ATTEN PERCEPT PSYCHO, V71, P1284, DOI 10.3758/APP.71.6.1284
   Kuparinen Liisa, 2013, P 26 INT CART C INT
   Kytö M, 2014, J ELECTRON IMAGING, V23, DOI 10.1117/1.JEI.23.1.011006
   Lappin JS, 2006, PERCEPT PSYCHOPHYS, V68, P571, DOI 10.3758/BF03208759
   Liu JJ, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES WORKSHOPS (VRW 2020), P196, DOI [10.1109/VRW50115.2020.0-234, 10.1109/VRW50115.2020.00042]
   Livingston MA, 2009, IEEE VIRTUAL REALITY 2009, PROCEEDINGS, P55, DOI 10.1109/VR.2009.4810999
   Loomis Jack M., 2008, EMBODIMENT EGO SPACE
   LOOMIS JM, 1992, J EXP PSYCHOL HUMAN, V18, P906, DOI 10.1037/0096-1523.18.4.906
   Mohler BettyJ., 2006, P 3 S APPL PERCEPTIO, P9, DOI [10.1145/1140491.1140493, DOI 10.1145/1140491.1140493]
   Napieralski PE, 2011, ACM T APPL PERCEPT, V8, DOI 10.1145/2010325.2010328
   Pärsch N, 2019, LECT NOTES COMPUT SC, V11596, P161, DOI 10.1007/978-3-030-22666-4_12
   Pagano CC, 1998, J EXP PSYCHOL HUMAN, V24, P1037, DOI 10.1037/0096-1523.24.4.1037
   Pagano CC, 2001, ECOL PSYCHOL, V13, P197, DOI 10.1207/S15326969ECO1303_2
   Parekh P, 2020, VIS COMPUT IND BIOME, V3, DOI 10.1186/s42492-020-00057-7
   Philbeck JW, 1997, J EXP PSYCHOL HUMAN, V23, P72, DOI 10.1037/0096-1523.23.1.72
   Philbeck JW, 1997, PERCEPT PSYCHOPHYS, V59, P601, DOI 10.3758/BF03211868
   Plumert Jodie., 2005, ACM Transactions on Applied Perception, V2, P216, DOI DOI 10.1145/1077399.1077402
   Pointon G, 2018, ACM SYMPOSIUM ON APPLIED PERCEPTION (SAP 2018), DOI 10.1145/3225153.3225168
   Renner RS, 2013, ACM COMPUT SURV, V46, DOI 10.1145/2543581.2543590
   Richardson AR, 2007, HUM FACTORS, V49, P507, DOI 10.1518/001872007X200139
   Rosales CS, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P237, DOI [10.1109/VR.2019.8798095, 10.1109/vr.2019.8798095]
   Sahm C. S., 2005, ACM Trans. Appl. Percept. (TAP), V2, P35, DOI DOI 10.1145/1048687.1048690
   Siegel ZD, 2017, ATTEN PERCEPT PSYCHO, V79, P39, DOI 10.3758/s13414-016-1243-z
   Swan JE, 2007, IEEE T VIS COMPUT GR, V13, P429, DOI 10.1109/TVCG.2007.1035
   Swan JE, 2006, P IEEE VIRT REAL ANN, P19, DOI 10.1109/VR.2006.13
   Swan JE, 2017, INT J HUM-COMPUT INT, V33, P576, DOI 10.1080/10447318.2016.1265783
   TEGHTSOO.M, 1969, PSYCHON SCI, V16, P281, DOI 10.3758/BF03332689
   TEGHTSOONIAN R, 1970, PSYCHON SCI, V21, P215
   Thompson WB, 2004, PRESENCE-TELEOP VIRT, V13, P560, DOI 10.1162/1054746042545292
   Warren W.H., 2019, Perception as Information Detection, P151
   Wither J, 2005, NINTH IEEE INTERNATIONAL SYMPOSIUM ON WEARABLE COMPUTERS, PROCEEDINGS, P92, DOI 10.1109/ISWC.2005.41
   Witmer BG, 1998, HUM FACTORS, V40, P478, DOI 10.1518/001872098779591340
   Ziemer CJ, 2009, ATTEN PERCEPT PSYCHO, V71, P1095, DOI 10.3758/APP.71.5.1096
NR 51
TC 9
Z9 11
U1 1
U2 23
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD JUN
PY 2021
VL 18
IS 2
AR 7
DI 10.1145/3449067
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA SR6EX
UT WOS:000661137000003
DA 2024-07-18
ER

PT J
AU Mihelac, L
   Povh, J
AF Mihelac, Lorena
   Povh, Janez
TI The Impact of the Complexity of Harmony on the Acceptability of Music
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Music complexity; regularity; entropy of harmony; regression tree; music
   acceptability; dataset
ID PREFERENCES; PERCEPTION; PERSONALITY; COGNITION; EMOTIONS; TONALITY;
   TENSION; ENTROPY; SYNTAX; EVENT
AB In this article, we contribute to the longstanding challenge of how to explain the listener's acceptability for a particular piece of music, using harmony as one of the crucial dimensions in music, one of the least examined in this context. We propose three measures for the complexity of harmony: (i) the complexity based on usage of the basic tonal functions and parallels in the harmonic progression, (ii) the entropies of unigrams and bigrams in the sequence of chords, and (iii) the regularity of the harmonic progression. Additionally, we propose four measures for the acceptability of musical pieces (perceptual variables): difficulty, pleasantness, recognition, and repeatability.
   These measures have been evaluated in each musical example within our dataset, consisting of 160 carefully selected musical excerpts from different musical styles. The first and the third complexity measures and the musical style of excerpts were determined by the first author using criteria described in the article, while the entropies were computed by computer using Shannon's formula, after the harmonic progression was determined. The four perceptual variables were obtained by a group of 21 participants, taking their mean values as the final score.
   A statistical analysis of this dataset shows that all the measures of complexity are consistent and are together with the musical style important features in explaining the musical acceptability. These relations were further elaborated by regression tree analysis for difficulty and pleasantness after unigram entropy was eliminated due to high correlation with bigram entropy. Results offer reasonable interpretations and also illuminate the relative importance of the predictor variables. In particular, the regularity of the harmonic progression is in both cases the most important predictor.
C1 [Mihelac, Lorena] Sch Ctr Novo Mesto, Segova 112, Novo Mesto 8000, Slovenia.
   [Povh, Janez] Univ Ljubljana, Fac Mech Engn, Askerceva Cesta 6, Ljubljana 1000, Slovenia.
C3 University of Ljubljana
RP Mihelac, L (corresponding author), Sch Ctr Novo Mesto, Segova 112, Novo Mesto 8000, Slovenia.
EM lorena.mihelac@sc-nm.si; janez.povh@fs.uni-lj.si
RI Mihelač, Lorena/GWZ-6458-2022
FU School Center Novo Mesto; Slovenian Research Agency (ARRS) [J1-8155,
   P2-0256]
FX This research work was supported by School Center Novo Mesto and by the
   Slovenian Research Agency (ARRS) through the research project J1-8155
   and research program P2-0256.
CR Adami C, 2002, BIOESSAYS, V24, P1085, DOI 10.1002/bies.10192
   Agres K., 2018, P 15 INT C MUS PERC
   [Anonymous], 1957, The Journal of Aesthetics and Art Criticism, DOI [DOI 10.2307/427154, 10.2307/427154]
   [Anonymous], 2011, HIST MUSICAL STYLE
   Benward B., 2008, MUSIC THEORY PRACTIC
   Berlyne D.E., 1971, Aesthetics and Psychobiology
   Bigand E, 1996, PERCEPT PSYCHOPHYS, V58, P125, DOI 10.3758/BF03205482
   Bigand E, 2001, COGNITION, V81, pB11, DOI 10.1016/S0010-0277(01)00117-2
   Bland JM, 1997, BRIT MED J, V314, P572, DOI 10.1136/bmj.314.7080.572
   Bowling DL, 2018, P NATL ACAD SCI USA, V115, P216, DOI 10.1073/pnas.1713206115
   Breiman L., 2017, CLASSIFICATION REGRE, DOI [10.1201/9781315139470, DOI 10.1201/9781315139470]
   Bucar J, 2018, LANG RESOUR EVAL, V52, P895, DOI 10.1007/s10579-018-9413-3
   Bueno JLO, 2002, PERCEPT MOTOR SKILL, V94, P541, DOI 10.2466/pms.2002.94.2.541
   Burns E. W., 2012, INTERVALS SCALES TUN, P215
   Collechia Regina A., 2009, THESIS
   Dahlhaus Carl., 2014, Studies on the Origin of Harmonic Tonality
   Despic D., 2007, HARMONIJA SA HARMONS
   Dibben N, 2004, MUSIC PERCEPT, V22, P79, DOI 10.1525/mp.2004.22.1.79
   Edmonds B., 1995, The evolution of complexity
   Eerola Tuomas, 2003, THESIS
   Febres Gerardo, 2007, MUSIC VIEWED ITS ENT
   Finnas L., 1989, B COUN RES MUSIC ED, V102, P1
   Foscarin Francesco, 2017, THESIS
   Gordon J, 2013, CREATIVITY RES J, V25, P143, DOI 10.1080/10400419.2013.752303
   Grassberger Peter, 2004, HELV PHYS ACTA, V62, P498
   GRAY R., 2013, Entropy and Information Theory, VFirst
   Hahn M, 1999, PSYCHOL MARKET, V16, P659, DOI 10.1002/(SICI)1520-6793(199912)16:8<659::AID-MAR3>3.0.CO;2-S
   Harrison P. M. C., 2018, P INT C MUS PERC COG
   HEYDUK RG, 1975, PERCEPT PSYCHOPHYS, V17, P84, DOI 10.3758/BF03204003
   Hiller Lejaren, 1966, J MUS THEORY, V10, P60
   Hiller Lejaren., 1967, J MUSIC THEORY, V11, P60, DOI DOI 10.2307/842949
   Honing H., 2011, Musical cognition. A science of listening
   Huron D. B., 2006, Sweet anticipation, DOI DOI 10.7551/MITPRESS/6575.001.0001
   Juslin PN, 2004, J NEW MUSIC RES, V33, P217, DOI 10.1080/0929821042000317813
   Kempf Davorin, 1988, INT REV AESTH SOCIOL, V27, P155
   Koelsch S, 2000, J COGNITIVE NEUROSCI, V12, P520, DOI 10.1162/089892900562183
   Kopacz M, 2005, J MUSIC THER, V42, P216, DOI 10.1093/jmt/42.3.216
   Kramer JonathanD., 1988, The Time of Music: New Meanings, New Temporalities, New Listening Strategies
   Krumhansl CL, 2004, J NEW MUSIC RES, V33, P253, DOI 10.1080/0929821042000317831
   Krumhansl CL, 2002, CURR DIR PSYCHOL SCI, V11, P45, DOI 10.1111/1467-8721.00165
   Ladislav Marsik, 2014, P ANN INT WORKSH DAT
   Lahdelma I, 2016, PSYCHOL MUSIC, V44, P37, DOI 10.1177/0305735614552006
   Latham Alison., 2004, OXFORD DICT MUSICAL
   Liu C, 2017, FRONT HUM NEUROSCI, V11, DOI 10.3389/fnhum.2017.00611
   Madison G, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00147
   Margulis EH, 2008, COMPUT MUSIC J, V32, P64, DOI 10.1162/comj.2008.32.4.64
   Marsik L., 2014, DATESO, P1
   Marsik Ladislav, 2013, THESIS
   Marsik Ladislav, 2017, P S APPL COMP SAC 17, P963, DOI [10.1145/3019612.3019939, DOI 10.1145/3019612.3019939]
   Martínez-Berumen HA, 2014, PROCEDIA COMPUT SCI, V28, P389, DOI 10.1016/j.procs.2014.03.048
   McCown W, 1997, PERS INDIV DIFFER, V23, P543, DOI 10.1016/S0191-8869(97)00085-8
   Mihelac L., 2017, THESIS
   Mihelac Lorena, 2017, P 14 INT S OP RES SO, V14, P371
   Morgan RobertP., 1991, 20 CENTURY MUSIC HIS
   Muller M., 2015, FUNDAMENTALS MUSIC P
   Pachet F., 2011, PITCH HARMONY NEURAL, P306
   Poulin-Charronnat B, 2006, J COGNITIVE NEUROSCI, V18, P1545, DOI 10.1162/jocn.2006.18.9.1545
   Rentfrow PJ, 2011, J PERS SOC PSYCHOL, V100, P1139, DOI 10.1037/a0022406
   Rentfrow PJ, 2006, PSYCHOL SCI, V17, P236, DOI 10.1111/j.1467-9280.2006.01691.x
   Rohrmeier M., 2012, P 9 INT S COMP MUS M
   Rohrmeier M, 2011, J MATH MUSIC, V5, P35, DOI 10.1080/17459737.2011.573676
   Rohrmeier Martin, 2016, MUSICAL SYNTAX
   Schonfuss Dirk, 2010, P INT S COMP MUS MOD, P356
   Shannon C. E., 1948, BELL SYST TECH J, V27, P379, DOI DOI 10.1002/J.1538-7305.1948.TB01338.X
   Shmulevich Ilya., 2000, Rhythm Perception and Production, P239
   Snyder B., 2000, Music and Memory: An Introduction
   Solomon J.W., 2019, Music Theory Essentials: A Streamlined Approach to Fundamentals, Tonal Harmony, and Post-Tonal Materials
   Steinbeis N, 2006, J COGNITIVE NEUROSCI, V18, P1380, DOI 10.1162/jocn.2006.18.8.1380
   Streich Sebastian., 2006, MUSIC COMPLEXITY MUL
   Temperley D, 2013, J NEW MUSIC RES, V42, P187, DOI 10.1080/09298215.2013.788039
   Therneau Terry., 2011, INTRO RECURSIVE PART
   Toiviainen P, 2003, PERCEPTION, V32, P741, DOI 10.1068/p3312
   Tseng Yuen Hsien, 1999, SPEC INTEREST GROUP, V22, P172
   Virtala Huotilainen M., 2013, NEWBORN INFANTS AUDI
   WIDMER G, 2016, ACM T INTEL SYST TEC, V8, P19
   Williams LR, 2011, INT J MUSIC EDUC, V29, P72, DOI 10.1177/0255761410372725
   Youngblood JosephE., 1958, J MUSIC THEORY, V2, P24, DOI DOI 10.2307/842928
NR 77
TC 6
Z9 7
U1 1
U2 7
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD MAR
PY 2020
VL 17
IS 1
AR 3
DI 10.1145/3375014
PG 27
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA OH5JA
UT WOS:000582618000003
DA 2024-07-18
ER

PT J
AU Narciso, D
   Melo, M
   Vasconcelos-Raposo, J
   Bessa, M
AF Narciso, David
   Melo, Miguel
   Vasconcelos-Raposo, Jose
   Bessa, Maximino
TI The Impact of Olfactory and Wind Stimuli on 360 Videos Using
   Head-mounted Displays
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Virtual reality; multisensory stimulation; presence; cybersickness;
   olfactory; tactile; immersion
ID VIRTUAL ENVIRONMENTS; CYBERSICKNESS; PERFORMANCE; MOVEMENT; SENSE
AB Consuming 360 audiovisual content using a Head-Mounted Display (HMD) has become a standard feature for Immersive Virtual Reality (IVR). However, most applications rely only on visual and auditory feedback whereas other senses are often disregarded. The main goal of this work was to study the effect of tactile and olfactory stimuli on participants' sense of presence and cybersickness while watching a 360 video using an HMD-based IVR setup. An experiment with 48 participants and three experimental conditions (360 video, 360 video with olfactory stimulus, and 360 video with tactile stimulus) was performed. Presence and cybersickness were reported via post-test questionnaires. Statistical analysis showed a significant difference in presence between the control and the olfactory conditions. From the control to the tactile condition, mean values were higher but failed to show statistical significance. Thus, results suggest that adding an olfactory stimulus increases presence significantly while the addition of a tactile stimulus only shows a positive effect. Regarding cybersickness, no significant differences were found across conditions. We conclude that an olfactory stimulus contributes to higher presence and that a tactile stimulus, delivered in the form of cutaneous perception of wind, has no influence in presence. We further conclude that multisensory cues do not affect cybersickness.
C1 [Narciso, David; Vasconcelos-Raposo, Jose] Univ Tras Os Montes & Alto Douro, Escola Ciencias & Tecnol, P-5001801 Vila Real, Portugal.
   [Melo, Miguel; Bessa, Maximino] Inst Engn Sistemas & Comp Tecnol & Ciencia, Rua Dr Roberto Frias, P-4200465 Porto, Portugal.
C3 University of Tras-os-Montes & Alto Douro; Universidade do Porto
RP Narciso, D (corresponding author), Univ Tras Os Montes & Alto Douro, Escola Ciencias & Tecnol, P-5001801 Vila Real, Portugal.
EM davidgnarciso@gmail.com; mcmelo@inesctec.pt; jvraposo@utad.pt;
   maxbessa@utad.pt
RI Melo, Miguel/AAN-1855-2020; VASCONCELOS-RAPOSO, JOSÉ/G-3743-2010;
   VASCONCELOS-RAPOSO, JOSE/JMB-6306-2023
OI Melo, Miguel/0000-0003-4050-3473; VASCONCELOS-RAPOSO,
   JOSÉ/0000-0002-3456-9727; Bessa, Maximino/0000-0002-3002-704X; Narciso,
   David/0000-0001-9630-310X
FU ERDF - European Regional Development Fund through the Operational
   Programme for Competitiveness and Internationalisation - COMPETE 2020
   Programme; National Funds through the Portuguese funding agency, FCT -
   Fundacao para a Ciencia e a Tecnologia [POCI-01-0145-FEDER-028618]
FX This work is financed by the ERDF - European Regional Development Fund
   through the Operational Programme for Competitiveness and
   Internationalisation - COMPETE 2020 Programme and by National Funds
   through the Portuguese funding agency, FCT - Fundacao para a Ciencia e a
   Tecnologia within project POCI-01-0145-FEDER-028618 entitled PERFECT -
   Perceptual Equivalence in virtual Reality For authEntiC Training. All
   the works were conducted at INESC TEC's MASSIVE VR Laboratory.
CR García AA, 2016, VIRTUAL REAL-LONDON, V20, P27, DOI 10.1007/s10055-015-0280-6
   Barfield W, 1995, PRESENCE-TELEOP VIRT, V5, P109, DOI 10.1162/pres.1996.5.1.109
   Bhagat KK, 2016, VIRTUAL REAL-LONDON, V20, P127, DOI 10.1007/s10055-016-0284-x
   BRISLIN RW, 1970, J CROSS CULT PSYCHOL, V1, P185, DOI 10.1177/135910457000100301
   Cardenas S, 2007, PA STUD HUM RIGHTS, P101
   Cater J. P., 1994, P IEEE INT C SYST MA
   Chen KP, 2013, P ROY SOC B-BIOL SCI, V280, DOI 10.1098/rspb.2013.1729
   Cruz-Neira C., 1993, Computer Graphics Proceedings, P135, DOI 10.1145/166117.166134
   Davide F., 2001, EMERG COMMUNICAT, P193
   Dinh HQ, 1999, P IEEE VIRT REAL ANN, P222, DOI 10.1109/VR.1999.756955
   Gallace Alberto, 2012, Multiple sensorial media advances and applications, P1, DOI DOI 10.4018/978-1-60960-821-7.CH001
   George D., 2016, SPSS for Windows step by step: A simple guide and reference, V14th
   GRUBBS FE, 1969, TECHNOMETRICS, V11, P1, DOI 10.2307/1266761
   Hambleton RK., 2011, Cross-cultural research methods in psychology, P46, DOI DOI 10.1017/CBO9780511779381.004
   Harvey C, 2018, COMPUT GRAPH FORUM, V37, P350, DOI 10.1111/cgf.13295
   Heilig M. L., 1962, Google Patents, Patent No. [US3050870A, 3050870A]
   Hirota Koichi, 2013, Virtual, Augmented and Mixed Reality. Systems and Applications. 5th International Conference, VAMR 2013 Held as Part of HCI International 2013. Proceedings, Part II: LNCS 8022, P372, DOI 10.1007/978-3-642-39420-1_39
   Hulsmann F, 2014, J VIRTUAL REAL BROAD, V11, P1
   Ischer M, 2014, FRONT PSYCHOL, V5, DOI 10.3389/fpsyg.2014.00736
   Jones L, 2004, HUMAN PERFORMANCE, SITUATION AWARENESS AND AUTOMATION: CURRENT RESEARCH AND TRENDS, VOL 2, P282
   Kadowaki A., 2010, 2010 10th IEEE/IPSJ International Symposium on Applications and the Internet (SAINT), P1, DOI 10.1109/SAINT.2010.39
   Keller Paul E, 1995, INTERACTIVE TECHNOLO, V18, P168
   Kennedy RS, 1993, The international journal of aviation psychology, V3, P203, DOI [DOI 10.1207/S15327108IJAP0303, 10.1207/s15327108ijap0303_3]
   Keshavarz B, 2015, EXP BRAIN RES, V233, P1353, DOI 10.1007/s00221-015-4209-9
   KIMMELMAN CP, 1993, AM J OTOLARYNG, V14, P227, DOI 10.1016/0196-0709(93)90065-F
   La Viola J. J.  Jr., 2000, SIGCHI Bulletin, V32, P47, DOI 10.1145/333329.333344
   Larsen CR, 2009, BMJ-BRIT MED J, V338, DOI 10.1136/bmj.b1802
   Lehmann A, 2009, 3DUI : IEEE SYMPOSIUM ON 3D USER INTERFACES 2009, PROCEEDINGS, P151, DOI 10.1109/3DUI.2009.4811231
   Lessiter J, 2001, PRESENCE-TELEOP VIRT, V10, P282, DOI 10.1162/105474601300343612
   Lombard M., 2006, J. Comput. Mediat. Commun, V3, P72, DOI [DOI 10.1111/J.1083-6101.1997.TB00072.X, https://doi.org/10.1111/j.1083-6101.1997.tb00072.x]
   Meehan M, 2002, ACM T GRAPHIC, V21, P645, DOI 10.1145/566570.566630
   Melo M, 2018, COMPUT GRAPH-UK, V71, P159, DOI 10.1016/j.cag.2017.11.007
   Merchant Z, 2014, COMPUT EDUC, V70, P29, DOI 10.1016/j.compedu.2013.07.033
   Moon T., 2004, 11 ACM S VIRTUAL REA, P122, DOI [10.1145/1077534.1077558, DOI 10.1145/1077534.1077558]
   Munyan BG, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0157568
   Nakamoto T, 2007, IEEE VIRTUAL REALITY 2007, PROCEEDINGS, P179
   Padgett LS, 2006, J PEDIATR PSYCHOL, V31, P65, DOI 10.1093/jpepsy/jsj030
   Paillard AC, 2014, NEUROSCI LETT, V566, P326, DOI 10.1016/j.neulet.2014.02.049
   PAPIN JP, 2003, P 5 INT C VIRT REAL, P113
   Richard E., 2006, Virtual Reality, V10, P207, DOI DOI 10.1007/S10055-006-0040-8
   Sallnas E.-L., 1999, P 2 INT C PRESENCE, P6
   Sanders MS, 1993, HUMAN FACTORS ENG DE
   Schubert T, 2001, PRESENCE-VIRTUAL AUG, V10, P266, DOI 10.1162/105474601300343603
   Seymour NE, 2002, ANN SURG, V236, P458, DOI 10.1097/00000658-200210000-00008
   Slater M, 1998, HUM FACTORS, V40, P469, DOI 10.1518/001872098779591368
   So RHY, 2001, PRESENCE-TELEOP VIRT, V10, P193, DOI 10.1162/105474601750216803
   Stanney KM, 2002, HUM PERFORM, V15, P339, DOI 10.1207/S15327043HUP1504_03
   Steed A, 1996, P IEEE VIRT REAL ANN, P163, DOI 10.1109/VRAIS.1996.490524
   Tijou A, 2006, LECT NOTES COMPUT SC, V3942, P1223, DOI 10.1007/11736639_152
   Tortell R., 2007, Virtual Reality, V11, P61, DOI 10.1007/s10055-006-0056-0
   Van Baren J., 2004, OmniPres Project IST-2001-39237
   Vasconcelos-Raposo J, 2016, PRESENCE-VIRTUAL AUG, V25, P191, DOI 10.1162/PRES_a_00261
   Verlinden JC, 2013, PROCEDIA ENGINEER, V60, P435, DOI 10.1016/j.proeng.2013.07.050
   Wiederhold B.K., 2001, CYBERPSYCHOLOGY MIND, P175
   Witmer BG, 1998, PRESENCE-TELEOP VIRT, V7, P225, DOI 10.1162/105474698565686
   Yanagida Y, 2004, P IEEE VIRT REAL ANN, P43, DOI 10.1109/VR.2004.1310054
NR 56
TC 18
Z9 22
U1 2
U2 14
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD MAR
PY 2020
VL 17
IS 1
AR 4
DI 10.1145/3380903
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA OH5JA
UT WOS:000582618000004
DA 2024-07-18
ER

PT J
AU Buck, LE
   Young, MK
   Bodenheimer, B
AF Buck, Lauren E.
   Young, Mary K.
   Bodenheimer, Bobby
TI A Comparison of Distance Estimation in HMD-Based Virtual Environments
   with Different HMD-Based Conditions
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Head-mounted displays; distance perception; field-of-view
ID VISUAL-PERCEPTION; REAL; LOCOMOTION; TARGET; SCALE
AB Underestimation of egocentric distances in immersive virtual environments using various head-mounted displays (HMDs) has been a puzzling topic of research interest for several years. As more commodity-level systems become available to developers, it is important to test the variation of underestimation in each system since reasons for underestimation remain elusive. In this article, we examine several different systems in two experiments and comparatively evaluate how much users underestimate distances in each one. To observe distance estimation behavior, a standard indirect blind walking task was used. An Oculus Rift DK1, weighted Oculus Rift DK1, Oculus Rift DK1 with an artificially restricted field of view, Nvis SX60, Nvis SX111, Oculus Rift DK2, Oculus Rift consumer version (CV1), and HTC Vive were tested. The weighted and restricted field of view HMDs were evaluated to determine the effect of these factors on distance underestimation; the other systems were evaluated because they are popular systems that are widely available. We found that weight and field of view restrictions heightened underestimation in the Rift DK1. Results from these conditions were comparable to the Nvis SX60 and SX111. The Oculus Rift DK1 and CV1 possessed the least amount of distance underestimation, but in general, commodity-level HMDs provided more accurate estimates of distance than the prior generation of HMDs.
C1 [Buck, Lauren E.; Young, Mary K.; Bodenheimer, Bobby] Vanderbilt Univ, 368 Jacobs Hall,400 24th Ave S, Nashville, TN 37212 USA.
C3 Vanderbilt University
RP Buck, LE (corresponding author), Vanderbilt Univ, 368 Jacobs Hall,400 24th Ave S, Nashville, TN 37212 USA.
EM lauren.e.buck.1@vanderbilt.edu; mary.k.young@vanderbilt.edu;
   robert.e.bodenheimer@vanderbilt.edu
OI Buck, Lauren/0000-0002-7220-3558
FU National Science Foundation [1116988, 1526448]; Direct For Computer &
   Info Scie & Enginr; Div Of Information & Intelligent Systems [1116988]
   Funding Source: National Science Foundation
FX The authors thank Richard Paris, Gayathri Narasimham, and John Rieser
   for support during the project. William B. Thompson offered advice about
   the weighted and limited FOV conditions, which we appreciate. This
   material is based upon work supported by the National Science Foundation
   under grants 1116988 and 1526448.
CR Andrus S.M., 2014, Proceedings of the ACM Symposium on Applied Perception, P130
   Bodenheimer B., 2007, P 4 S APPL PERC GRAP, DOI [10.1145/1272582.1272589, DOI 10.1145/1272582.1272589]
   Creem-Regehr SH, 2016, SAP 2015: ACM SIGGRAPH SYMPOSIUM ON APPLIED PERCEPTION, P47, DOI 10.1145/2804408.2804422
   Creem-Regehr SH, 2015, PSYCHOL LEARN MOTIV, V62, P195, DOI 10.1016/bs.plm.2014.09.006
   Creem-Regehr SH, 2005, PERCEPTION, V34, P191, DOI 10.1068/p5144
   Cutting JE, 1995, PERCEPTION SPACE MOT, P69, DOI [DOI 10.1016/B978-012240530-3/50005-5, 10.1016/B978-012240530-3/50005-5]
   Grechkin TY, 2010, ACM T APPL PERCEPT, V7, DOI 10.1145/1823738.1823744
   Interrante V, 2006, P IEEE VIRT REAL ANN, P3, DOI 10.1109/VR.2006.52
   Jeffreys H., 1998, The theory of probability
   Jones J.A., 2011, Proc. Symposium on Applied perception in Graphics and Visualization, P29
   Jones JA, 2013, IEEE T VIS COMPUT GR, V19, P701, DOI 10.1109/TVCG.2013.37
   Jones JA, 2008, APGV 2008: PROCEEDINGS OF THE SYMPOSIUM ON APPLIED PERCEPTION IN GRAPHICS AND VISUALIZATION, P9
   Jones JB, 2012, PLANT NUTRITION AND SOIL FERTILITY MANUAL, 2ND EDITION, P119
   Kelly JW, 2017, ACM T APPL PERCEPT, V15, DOI 10.1145/3106155
   Kenyon RV, 2008, ANN BIOMED ENG, V36, P342, DOI 10.1007/s10439-007-9414-7
   Knapp JM, 2004, PRESENCE-TELEOP VIRT, V13, P572, DOI 10.1162/1054746042545238
   Langbehn E, 2016, 22ND ACM CONFERENCE ON VIRTUAL REALITY SOFTWARE AND TECHNOLOGY (VRST 2016), P241, DOI 10.1145/2993369.2993379
   Leyrer M., 2011, P ACM SIGGRAPH S APP, DOI 10.1145/2077451.2077464
   Li B, 2016, PROCEEDINGS 2016 INTERNATIONAL CONFERENCE ON NETWORKING AND NETWORK APPLICATIONS NANA 2016, P53, DOI 10.1109/NaNA.2016.84
   Li BC, 2016, SAP 2015: ACM SIGGRAPH SYMPOSIUM ON APPLIED PERCEPTION, P55, DOI 10.1145/2804408.2804427
   Lin Qiufeng., 2011, P ACM SIGGRAPH S APP, P75
   Loomis Jack M., 2008, P CARN S COGN 2006 P
   Loomis JM, 2003, VIRTUAL AND ADAPTIVE ENVIRONMENTS: APPLICATIONS, IMPLICATIONS, AND HUMAN PERFORMANCE ISSUES, P21
   LOOMIS JM, 1992, J EXP PSYCHOL HUMAN, V18, P906, DOI 10.1037/0096-1523.18.4.906
   Loomis JM, 1996, CURR DIR PSYCHOL SCI, V5, P72, DOI 10.1111/1467-8721.ep10772783
   Mohler BJ, 2010, PRESENCE-TELEOP VIRT, V19, P230, DOI 10.1162/pres.19.3.230
   Plumert Jodie., 2005, ACM Transactions on Applied Perception, V2, P216, DOI DOI 10.1145/1077399.1077402
   Renner RS, 2013, ACM COMPUT SURV, V46, DOI 10.1145/2543581.2543590
   Richardson AR, 2007, HUM FACTORS, V49, P507, DOI 10.1518/001872007X200139
   Ries B., 2009, Proceedings of the 16th ACM Symposium on Virtual Reality Software and Technology, VRST '09, P59, DOI [10.1145/1643928.1643943, DOI 10.1145/1643928.16439433, DOI 10.1145/1643928.1643943]
   RIESER JJ, 1990, PERCEPTION, V19, P675, DOI 10.1068/p190675
   Rouder JN, 2009, PSYCHON B REV, V16, P225, DOI 10.3758/PBR.16.2.225
   SATAVA RM, 1995, J MED SYST, V19, P275, DOI 10.1007/BF02257178
   Thompson WB, 2004, PRESENCE-TELEOP VIRT, V13, P560, DOI 10.1162/1054746042545292
   Waller D, 2008, J EXP PSYCHOL-APPL, V14, P61, DOI 10.1037/1076-898X.14.1.61
   Wiederhold Brenda K., 2005, VIRTUAL REALITY THER, V8, P95
   Willemsen P, 2002, P IEEE VIRT REAL ANN, P275, DOI 10.1109/VR.2002.996536
   Willemsen P, 2009, ACM T APPL PERCEPT, V6, DOI 10.1145/1498700.1498702
   Witmer BG, 1998, HUM FACTORS, V40, P478, DOI 10.1518/001872098779591340
   Witt JK, 2007, PERCEPTION, V36, P1752, DOI 10.1068/p5617
   Wu B, 2004, NATURE, V428, P73, DOI 10.1038/nature02350
   Young M. K., 2014, P ACM S APPL PERCEPT, P83
NR 42
TC 69
Z9 73
U1 0
U2 11
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD AUG
PY 2018
VL 15
IS 3
AR 21
DI 10.1145/3196885
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA GY2UV
UT WOS:000448400300007
DA 2024-07-18
ER

PT J
AU Martin, MV
   Cho, V
   Aversano, G
AF Martin, Miguel Vargas
   Cho, Victor
   Aversano, Gabriel
TI Detection of Subconscious Face Recognition Using Consumer-Grade
   Brain-Computer Interfaces
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Subconscious mind; face recognition; brain-computer interfaces
ID IDENTIFICATION; FAMILIARITY; REPETITION; ERPS; NAME
AB We test the possibility of tapping the subconscious mind for face recognition using consumer-grade BCIs. To this end, we performed an experiment whereby subjects were presented with photographs of famous persons with the expectation that about 20% of them would be (consciously) recognized; and since the photos are of famous persons, we expected that subjects would have seen before some of the 80% they didn't (consciously) recognize. Further, we expected that their subconscious would have recognized some of those in the 80% pool that they had seen before. An exit questionnaire and a set of criteria allowed us to label recognitions as conscious, false, no recognitions, or subconscious recognitions. We analyzed a number of event related potentials training and testing a support vector machine. We found that our method is capable of differentiating between no recognitions and subconscious recognitions with promising accuracy levels, suggesting that tapping the subconscious mind for face recognition is feasible.
C1 [Martin, Miguel Vargas; Cho, Victor; Aversano, Gabriel] Univ Ontario, Inst Technol, Oshawa, ON, Canada.
C3 Ontario Tech University
RP Martin, MV (corresponding author), 2000 Simcoe St N, Oshawa, ON L1H 7K4, Canada.
EM miguel.vargasmartin@uoit.ca; victor.cho@uoit.net;
   gabriel.aversano@uoit.net
FU Natural Sciences and Engineering Research Council of Canada
FX This work is supported by the Natural Sciences and Engineering Research
   Council of Canada.
CR Akhawe D., 2013, 22 USENIX SEC S
   [Anonymous], ACM T COMPUT HUM INT
   [Anonymous], 2000, 9 USENIX SEC S
   [Anonymous], P SIGCHI C HUM FACT
   [Anonymous], P 2014 ACM INT JOINT
   Bentin S, 2000, COGN NEUROPSYCHOL, V17, P35, DOI 10.1080/026432900380472
   Boehm SG, 2005, COGNITIVE BRAIN RES, V23, P153, DOI 10.1016/j.cogbrainres.2004.10.008
   Bojinov H., 2012, 21st {USENIX} Security Symposium ({USENIX} Security 12), P129
   Bonaci T., 2014, the Proceedings of the 2014 IEEE Conference on Norbert Wiener in the 21st Century, P1
   Bougrain L., 2012, TOBI WORKSH LLL TOOL
   Brunner C, 2015, BRAIN-COMPUT INTERFA, V2, P1, DOI 10.1080/2326263X.2015.1008956
   Caharel S, 2002, INT J NEUROSCI, V112, P1499, DOI 10.1080/00207450290158368
   Chuang J., 2013, WORKSH US SEC USEC 1
   Clausen J, 2014, HDB NEUROETHICS, P699
   Cleary AM, 2000, J EXP PSYCHOL LEARN, V26, P1063, DOI 10.1037/0278-7393.26.4.1063
   Cleary AM, 2002, MEM COGNITION, V30, P758, DOI 10.3758/BF03196431
   Cleary AM, 2010, MEM COGNITION, V38, P452, DOI 10.3758/MC.38.4.452
   Curran T, 2003, COGNITIVE BRAIN RES, V15, P191, DOI 10.1016/S0926-6410(02)00192-1
   Debruille JB, 1998, NEUROREPORT, V9, P3349, DOI 10.1097/00001756-199810260-00002
   Eimer M, 2000, CLIN NEUROPHYSIOL, V111, P694, DOI 10.1016/S1388-2457(99)00285-0
   Fawcett T, 2006, PATTERN RECOGN LETT, V27, P861, DOI 10.1016/j.patrec.2005.10.010
   Fisher RA, 1936, ANN EUGENIC, V7, P179, DOI 10.1111/j.1469-1809.1936.tb02137.x
   Frank M., 2013, ABS13126052 CORR
   George N, 1997, NEUROREPORT, V8, P1417, DOI 10.1097/00001756-199704140-00019
   Greenwald AG, 2009, J PERS SOC PSYCHOL, V97, P17, DOI 10.1037/a0015575
   InteraXon, 2016, MUS BRAIN SENS HEADB
   Kaper M, 2004, IEEE T BIO-MED ENG, V51, P1073, DOI 10.1109/TBME.2004.826698
   Klem G H, 1999, Electroencephalogr Clin Neurophysiol Suppl, V52, P3
   Kostic B, 2009, J EXP PSYCHOL GEN, V138, P146, DOI 10.1037/a0014584
   Krusienski DJ, 2006, J NEURAL ENG, V3, P299, DOI 10.1088/1741-2560/3/4/007
   Lee JS, 2006, INT OFFSHORE POLAR E, P81, DOI 10.1145/1166253.1166268
   Marcel S, 2007, IEEE T PATTERN ANAL, V29, P743, DOI 10.1109/TPAMI.2007.1012
   Martinovic I., 2012, 21 USENIX SECURITY S, P143
   Mnatsakanian EV, 2003, INT J PSYCHOPHYSIOL, V47, P217, DOI 10.1016/S0167-8760(02)00154-X
   Mustafa M., 2012, P SIGCHI C HUM FACT, P513, DOI [10.1145/2207676.2207746, DOI 10.1145/2207676.2207746]
   Neupane A., 2014, NETW DISTR SYST SEC
   Newman MEJ, 2005, CONTEMP PHYS, V46, P323, DOI 10.1080/00107510500052444
   Passfaces Co, 2016, PASSF 2 FACT AUTH EN
   Peck E. M. M., 2013, P SIGCHI C HUM FACT, P473, DOI DOI 10.1145/2470654.2470723
   Pfütze EM, 2002, PSYCHOL AGING, V17, P140, DOI 10.1037//0882-7974.17.1.140
   Rosburg T, 2005, BEHAV NEUROSCI, V119, P876, DOI 10.1037/0735-7044.119.4.876
   Sanguinetti JL, 2014, PSYCHOL SCI, V25, P256, DOI 10.1177/0956797613502814
   Seeck M, 1997, NEUROREPORT, V8, P2749, DOI 10.1097/00001756-199708180-00021
   Shalgi S, 2012, FRONT HUM NEUROSCI, V6, DOI 10.3389/fnhum.2012.00124
   Solovey ET, 2011, 29TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P383
   STANDING L, 1973, Q J EXP PSYCHOL, V25, P207, DOI 10.1080/14640747308400340
   STERNBERG S, 1966, SCIENCE, V153, P652, DOI 10.1126/science.153.3736.652
   Stroop JR, 1935, J EXP PSYCHOL, V18, P643, DOI 10.1037/h0054651
   SUTTON S, 1965, SCIENCE, V150, P1187, DOI 10.1126/science.150.3700.1187
   Szafir D, 2012, P SIGCHI C HUM FACT, P11, DOI DOI 10.1145/2207676.2207679
   Watts Duncan J., 2000, Small worlds: The dynamics of networks between order and randomness
NR 51
TC 17
Z9 18
U1 0
U2 18
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD AUG
PY 2016
VL 14
IS 1
AR 7
DI 10.1145/2955097
PG 20
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA DV4EB
UT WOS:000382876900007
DA 2024-07-18
ER

PT J
AU Albrecht, R
   Lokki, T
AF Albrecht, Robert
   Lokki, Tapio
TI Auditory Distance Presentation in an Urban Augmented Reality Environment
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Design; Experimentation; Human Factors; Auditory display; spatial sound;
   localization; distance perception
ID PERCEPTION; NAVIGATION; NOISE
AB Presenting points of interest in the environment by means of audio augmented reality offers benefits compared with traditional visual augmented reality and map-based approaches. However, presentation of distant virtual sound sources is problematic. This study looks at combining well-known auditory distance cues to convey the distance of points of interest. The results indicate that although the provided cues are intuitively mapped to relatively short distances, users can with only little training learn to map these cues to larger distances.
C1 [Albrecht, Robert; Lokki, Tapio] Aalto Univ, Dept Comp Sci, FI-00076 Aalto, Finland.
C3 Aalto University
RP Albrecht, R (corresponding author), Aalto Univ, Dept Comp Sci, POB 15500, FI-00076 Aalto, Finland.
EM robert.albrecht@aalto.fi; tapio.lokki@aalto.fi
RI Lokki, Tapio/E-7402-2012
OI Lokki, Tapio/0000-0001-7700-1448
FU Nokia Research Center
FX The research leading to these results has received funding from Nokia
   Research Center.
CR Algazi VR, 2001, PROCEEDINGS OF THE 2001 IEEE WORKSHOP ON THE APPLICATIONS OF SIGNAL PROCESSING TO AUDIO AND ACOUSTICS, P99, DOI 10.1109/ASPAA.2001.969552
   [Anonymous], LECT NOTES COMPUTER
   [Anonymous], 2014, SOUNDSCAPE SEMIOTICS
   [Anonymous], ACM T APPL PERCEPTIO
   [Anonymous], 1995, PERCEPTION SPACE MOT, DOI DOI 10.1016/B978-012240530-3/50005-5
   [Anonymous], P 19 INT C AUD DISPL
   [Anonymous], P 3 INT PUR DAT CONV
   [Anonymous], ICGTR1101 GRAZ U TEC
   Blum JR, 2013, MOBILE NETW APPL, V18, P295, DOI 10.1007/s11036-012-0425-8
   BOLL SF, 1979, IEEE T ACOUST SPEECH, V27, P113, DOI 10.1109/TASSP.1979.1163209
   Brungart DS, 2002, PRESENCE-TELEOP VIRT, V11, P93, DOI 10.1162/105474602317343686
   Brungart DS, 2001, J ACOUST SOC AM, V110, P425, DOI 10.1121/1.1379730
   Cabrera Densil., 2002, P 2002 INT C AUDITOR
   Fisher N. I., 1993, STAT ANAL CIRCULAR D, DOI DOI 10.1017/CBO9780511564345
   Holland S, 2002, PERS UBIQUIT COMPUT, V6, P253, DOI 10.1007/s007790200025
   Lehmann Erich Leo, 1975, Nonparametrics: statistical methods based on ranks
   MERSHON DH, 1989, PERCEPTION, V18, P403, DOI 10.1068/p180403
   POLLACK I, 1952, J ACOUST SOC AM, V24, P158, DOI 10.1121/1.1906871
   Tran TV, 2000, ERGONOMICS, V43, P807, DOI 10.1080/001401300404760
   Zahorik P, 2005, ACTA ACUST UNITED AC, V91, P409
NR 20
TC 3
Z9 3
U1 0
U2 15
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD APR
PY 2015
VL 12
IS 2
AR 5
DI 10.1145/2723568
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CH5BA
UT WOS:000354047900001
DA 2024-07-18
ER

PT J
AU Marentakis, G
   McAdams, S
AF Marentakis, G.
   McAdams, S.
TI Perceptual Impact of Gesture Control of Spatialization
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Human Factors; Multimodal integration; auditory movement; gesture
   control; 3D audio
ID AUDIOVISUAL SPEECH; STIMULUS FREQUENCY; SOUND; MOTION; LOCALIZATION;
   REVERBERATION; ATTENTION; ANGLES; CUES
AB In two experiments, visual cues from gesture control of spatialization were found to affect auditory movement perception depending on the identifiability of auditory motion trajectories, the congruency of audiovisual stimulation, the sensory focus of attention, and the attentional process involved. Visibility of the performer's gestures improved spatial audio trajectory identification, but it shifted the listeners' attention to vision, impairing auditory motion encoding in the case of incongruent stimulation. On the other hand, selectively directing attention to audition resulted in interference from the visual cues for acoustically ambiguous trajectories. Auditory motion information was poorly preserved when dividing attention between auditory and visual movement feedback from performance gestures. An auditory focus of attention is a listener strategy that maximizes performance, due to the improvement caused by congruent visual stimulation and its robustness to interference from incongruent stimulation for acoustically unambiguous trajectories. Attentional strategy and auditory motion calibration are two aspects that need to be considered when employing gesture control of spatialization.
C1 [Marentakis, G.] Univ Mus & Performing, IEM, Graz, Austria.
   [McAdams, S.] McGill Univ, Schulich Sch Mus, CIRMMT, Montreal, PQ H3A 1E3, Canada.
C3 McGill University
RP Marentakis, G (corresponding author), Kunstuniv Graz, Inst Elect Mus & Acoust, Infelldgasse 10-3, A-8010 Graz, Austria.
RI McAdams, Stephen/GQB-0225-2022
OI McAdams, Stephen/0000-0002-6744-9035; Marentakis,
   Georgios/0000-0002-6563-9601
FU Stephen McAdams's Canada Research Chair; New Media Initiative grant;
   Natural Sciences and Engineering Research Council of Canada; Canada
   Council for the Arts
FX This work was supported by Stephen McAdams's Canada Research Chair and a
   New Media Initiative grant to Stephen McAdams funded jointly by the
   Natural Sciences and Engineering Research Council of Canada and the
   Canada Council for the Arts. The experiments were performed while the
   first author was a postdoctoral fellow in CIRMMT at McGill University.
   We are grateful to Nils Peters for assisting with the implementation of
   the first experiment, to Joe Malloch for designing the user interface
   that was used by the performer in the first experiment, to Cathryn
   Gryffiths for testing the participants in the second experiment, and to
   three anonymous reviewers for invaluable comments.
CR Ahrens J, 2011, J ACOUST SOC AM, V130, P2807, DOI 10.1121/1.3640850
   BEGAULT DR, 1992, J AUDIO ENG SOC, V40, P895
   BERKHOUT AJ, 1993, J ACOUST SOC AM, V93, P2764, DOI 10.1121/1.405852
   Bertelson P, 1998, PSYCHON B REV, V5, P482, DOI 10.3758/BF03208826
   Blauert J., 1997, SPATIAL HEARING PSYC
   Bronkhorst AW, 1999, NATURE, V397, P517, DOI 10.1038/17374
   CHANDLER DW, 1992, J ACOUST SOC AM, V91, P1624, DOI 10.1121/1.402443
   Chowning J., 1977, Computer Music Journal, V1, P48, DOI [DOI 10.2307/3679609, 10.2307/3679609]
   Davidson J.W., 1993, PSYCHOL MUSIC, V21, P103, DOI [DOI 10.1177/030573569302100201, 10.1177/030573569302100201]
   DRIVER J, 1994, ATTENTION PERFORM, V15, P311
   Driver J., 2004, CROSSMODAL SPACE CRO, DOI [10.1093/acprof:oso/9780198524861.001.0001, DOI 10.1093/ACPROF:OSO/9780198524861.001.0001]
   Féron FX, 2010, J ACOUST SOC AM, V128, P3703, DOI 10.1121/1.3502456
   Gerzon M. A., 1992, P 92 CONV AUD ENG SO
   GIGUERE C, 1993, J ACOUST SOC AM, V94, P769, DOI 10.1121/1.408206
   Grant KW, 1998, J ACOUST SOC AM, V103, P2677, DOI 10.1121/1.422788
   Grant KW, 2000, J ACOUST SOC AM, V108, P1197, DOI 10.1121/1.1288668
   Grantham D., 1986, J ACOUST SOC AM, V79, P1624
   Grantham DW, 2003, J ACOUST SOC AM, V114, P1009, DOI 10.1121/1.1590970
   Grohn M., 2002, P 22 AES INT C VIRT
   Guastavino C., 2007, P 13 INT C AUDITORY, P53
   Harley M. A., 1994, THESIS
   HARTMANN WM, 1983, J ACOUST SOC AM, V74, P1380, DOI 10.1121/1.390163
   Lakatos S, 1997, J EXP PSYCHOL HUMAN, V23, P1050, DOI 10.1037/0096-1523.23.4.1050
   Lutfi RA, 1999, J ACOUST SOC AM, V106, P919, DOI 10.1121/1.428033
   Malham D.G., 1999, Proc. Int. Comput. Music Conf, P484
   Marentakis G., 2008, P AES 124 CONV
   Marshall M., 2007, P 4 INT C EN INT, P388
   Marshall MT, 2009, LECT NOTES ARTIF INT, V5085, P227
   MCGURK H, 1976, NATURE, V264, P746, DOI 10.1038/264746a0
   Meyer GF, 2005, EXP BRAIN RES, V166, P538, DOI 10.1007/s00221-005-2394-7
   MILLS AW, 1958, J ACOUST SOC AM, V30, P237, DOI 10.1121/1.1909553
   Oruc I, 2008, BRAIN RES, V1242, P200, DOI 10.1016/j.brainres.2008.04.014
   Peters N., 2011, TAG DTSCH JAHR AK
   Pulkki V, 2005, IEEE T SPEECH AUDI P, V13, P105, DOI 10.1109/TSA.2004.838533
   Pulkki V., 2001, THESIS
   Reynolds R., 2002, Form and Method: Composing Music, The Rothschild Essays
   ROSENBLUM LD, 1987, PERCEPTION, V16, P175, DOI 10.1068/p160175
   SABERI K, 1990, J ACOUST SOC AM, V88, P2639, DOI 10.1121/1.399984
   SABERI K, 1991, ACUSTICA, V75, P57
   Schacher J.C., 2007, Conference on New Interfaces for Musical Expression, P358
   Soto-Faraco S, 2004, CURR DIR PSYCHOL SCI, V13, P29, DOI 10.1111/j.0963-7214.2004.01301008.x
   Soto-Faraco S, 2003, NEUROPSYCHOLOGIA, V41, P1847, DOI 10.1016/S0028-3932(03)00185-4
   Spence C, 1997, PERCEPT PSYCHOPHYS, V59, P1, DOI 10.3758/BF03206843
   Spence C, 2000, PERCEPT PSYCHOPHYS, V62, P410, DOI 10.3758/BF03205560
   Spence C., 2004, CROSSMODAL SPACE CRO, P277
   Spence C, 2007, ACOUST SCI TECHNOL, V28, P61, DOI 10.1250/ast.28.61
   SPENCE CJ, 1994, J EXP PSYCHOL HUMAN, V20, P555, DOI 10.1037/0096-1523.20.3.555
   SUMBY WH, 1954, J ACOUST SOC AM, V26, P212, DOI 10.1121/1.1907309
   SUMMERFIELD Q, 1979, PHONETICA, V36, P314, DOI 10.1159/000259969
   Thomas GJ, 1941, J EXP PSYCHOL, V28, P163, DOI 10.1037/h0055183
   Vatakis A, 2007, PERCEPT PSYCHOPHYS, V69, P744, DOI 10.3758/BF03193776
   Vines BW, 2006, COGNITION, V101, P80, DOI 10.1016/j.cognition.2005.09.003
   Vroomen J, 2001, PERCEPT PSYCHOPHYS, V63, P651, DOI 10.3758/BF03194427
   Vroomen J, 2011, COGNITION, V118, P75, DOI 10.1016/j.cognition.2010.10.002
   WALLACH H., 1949, J ACOUST SOC AM, V21, P468
   WELCH RB, 1980, PSYCHOL BULL, V88, P638, DOI 10.1037/0033-2909.88.3.638
   Wijnans S., 2010, THESIS
   Zotter F, 2012, ACTA ACUST UNITED AC, V98, P37, DOI 10.3813/AAA.918490
NR 58
TC 4
Z9 4
U1 1
U2 12
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD OCT
PY 2013
VL 10
IS 4
AR 22
DI 10.1145/2536764.2536769
PG 21
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA 281YK
UT WOS:000329136700005
OA Green Submitted
DA 2024-07-18
ER

PT J
AU To, MPS
   Gilchrist, ID
   Troscianko, T
   Kho, JSB
   Tolhurst, DJ
AF To, M. P. S.
   Gilchrist, I. D.
   Troscianko, T.
   Kho, J. S. B.
   Tolhurst, D. J.
TI Perception of Differences in Natural-Image Stimuli: Why is Peripheral
   Viewing Poorer than Foveal?
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Human Factors; Peripheral vision; VDP models; psychophysical testing;
   peripheral vision; crowding; image difference metrics
ID CORTICAL MAGNIFICATION; CONTRAST SENSITIVITY; STRIATE CORTEX;
   VISUAL-FIELD; REPRESENTATION; MODEL; DISCRIMINATION; ACUITY
AB Visual Difference Predictor (VDP) models have played a key role in digital image applications such as the development of image quality metrics. However, little attention has been paid to their applicability to peripheral vision. Central (i.e., foveal) vision is extremely sensitive for the contrast detection of simple stimuli such as sinusoidal gratings, but peripheral vision is less sensitive. Furthermore, crowding is a well-documented phenomenon whereby differences in suprathreshold peripherally viewed target objects (such as individual letters or patches of sinusoidal grating) become more difficult to discriminate when surrounded by other objects (flankers). We examine three factors that might influence the degree of crowding with natural-scene stimuli (cropped from photographs of natural scenes): (1) location in the visual field, (2) distance between target and flankers, and (3) flanker-target similarity. We ask how these factors affect crowding in a suprathreshold discrimination experiment where observers rate the perceived differences between two sequentially presented target patches of natural images. The targets might differ in the shape, size, arrangement, or color of items in the scenes. Changes in uncrowded peripheral targets are perceived to be less than for the same changes viewed foveally. Consistent with previous research on simple stimuli, we find that crowding in the periphery (but not in the fovea) reduces the magnitudes of perceived changes even further, especially when the flankers are closer and more similar to the target. We have tested VDP models based on the response behavior of neurons in visual cortex and the inhibitory interactions between them. The models do not explain the lower ratings for peripherally viewed changes even when the lower peripheral contrast sensitivity was accounted for; nor could they explain the effects of crowding, which others have suggested might arise from errors in the spatial localization of features in the peripheral image. This suggests that conventional VDP models do not port well to peripheral vision.
C1 [To, M. P. S.; Kho, J. S. B.; Tolhurst, D. J.] Univ Cambridge, Dept Physiol Dev & Neurosci, Cambridge CB2 1TN, England.
   [Gilchrist, I. D.; Troscianko, T.] Univ Bristol, Dept Expt Psychol, Bristol BS8 1TH, Avon, England.
C3 University of Cambridge; University of Bristol
RP To, MPS (corresponding author), Univ Cambridge, Dept Physiol Dev & Neurosci, Cambridge CB2 1TN, England.
EM mpst2@cam.ac.uk; i.d.gilchrist@bris.ac.uk; tom.troscianko@bris.ac.uk;
   jsbk2@cam.ac.uk; djt12@cam.ac.uk
RI To, Michelle/JPX-8030-2023; Gilchrist, Iain D/E-8627-2010
OI To, Michelle/0000-0002-3880-9425; Gilchrist, Iain D./0000-0003-2070-6679
FU EPSRC/Dstl of the U.K.; EPSRC [EP/E037097/1, EP/E037372/1] Funding
   Source: UKRI
FX This work was supported by project grants to D.J. Tolhurst and T.
   Troscianko from the EPSRC/Dstl of the U.K.
CR AZZOPARDI P, 1993, NATURE, V361, P719, DOI 10.1038/361719a0
   BOUMA H, 1970, NATURE, V226, P177, DOI 10.1038/226177a0
   Doll TJ, 1998, OPT ENG, V37, P2006, DOI 10.1117/1.601898
   DUNCAN J, 1989, PSYCHOL REV, V96, P433, DOI 10.1037/0033-295X.96.3.433
   Duncan RO, 2003, NEURON, V38, P659, DOI 10.1016/S0896-6273(03)00265-4
   HORTON JC, 1991, ARCH OPHTHALMOL-CHIC, V109, P816, DOI 10.1001/archopht.1991.01080060080030
   LEVI DM, 1985, VISION RES, V25, P963, DOI 10.1016/0042-6989(85)90207-X
   LOVELL PG, 2006, ACM T APPL PERCEPT, V3, P155, DOI DOI 10.1145/1166087.1166089
   Lubin J., 1995, Vision Models for Target Detection and Recognition, P245
   MOVSHON JA, 1978, J PHYSIOL-LONDON, V283, P101, DOI 10.1113/jphysiol.1978.sp012490
   Mullen KT, 2002, VISUAL NEUROSCI, V19, P109, DOI 10.1017/S0952523802191103
   Párraga CA, 2005, VISION RES, V45, P3145, DOI 10.1016/j.visres.2005.08.006
   Pelli DG, 2008, NAT NEUROSCI, V11, P1129, DOI 10.1038/nn.2187
   Poder E, 2007, J VISION, V7, DOI 10.1167/7.2.23
   Popovic Z, 2001, VISION RES, V41, P1313, DOI 10.1016/S0042-6989(00)00290-X
   RIPAMONTI C, 2005, J VIS, V5, pA595
   Rohaly AM, 1997, VISION RES, V37, P3225, DOI 10.1016/S0042-6989(97)00156-9
   Saadane A, 2007, J ELECTRON IMAGING, V16, DOI 10.1117/1.2437728
   Schultze M., 1866, Archiv fur Mikroskopische Anatomie, V2, P175, DOI [10.1007/BF02962033, DOI 10.1007/BF02962033]
   Thibos LN, 1996, VISION RES, V36, P249, DOI 10.1016/0042-6989(95)00109-D
   To M, 2008, P ROY SOC B-BIOL SCI, V275, P2299, DOI 10.1098/rspb.2008.0692
   To M, 2007, PERCEPTION, V36, P157
   TOET A, 1992, VISION RES, V32, P1349, DOI 10.1016/0042-6989(92)90227-A
   TOLHURST DJ, 1988, HUM NEUROBIOL, V6, P247
   VANESSEN DC, 1984, VISION RES, V24, P429, DOI 10.1016/0042-6989(84)90041-5
   Watson A.B., 1993, DIGITAL IMAGES HUMAN, P179
   Watson AB, 2005, J VISION, V5, P717, DOI 10.1167/5.9.6
   Watson AB, 1997, J OPT SOC AM A, V14, P2379, DOI 10.1364/JOSAA.14.002379
   WATSON AB, 1987, J OPT SOC AM A, V4, P2401, DOI 10.1364/JOSAA.4.002401
NR 29
TC 5
Z9 6
U1 0
U2 12
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD SEP
PY 2009
VL 6
IS 4
AR 26
DI 10.1145/1609967.1609973
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 511ZV
UT WOS:000271212300006
DA 2024-07-18
ER

PT J
AU Akyüz, AO
   Reinhard, E
AF Akyuez, Ahmet Oguz
   Reinhard, Erik
TI Perceptual Evaluation of Tone-Reproduction Operators Using the
   Cornsweet-Craik-O'Brien Illusion
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Experimentation; Human Factors; Tone-mapping operators; high
   dynamic-range imaging; dynamic-range compression; visual psychophysics
ID MAPPING OPERATORS; CONTRAST; COLOR
AB High dynamic-range images cannot be directly displayed on conventional display devices, but have to be tone-mapped first. For this purpose, a large set of tone-reproduction operators is currently available. However, it is unclear which operator is most suitable for any given task. In addition, different tasks may place different requirements upon each operator. In this paper we evaluate several tone-reproduction operators using a paradigm that does not require the construction of a real high dynamic-range scene, nor does it require the availability of a high dynamic-range display device. The user study involves a task that relates to the evaluation of contrast, which is an important attribute that needs to be preserved under tone reproduction.
C1 [Akyuez, Ahmet Oguz] Max Planck Inst Biol Cybernet, Tubingen, Germany.
   [Reinhard, Erik] Univ Bristol, Bristol BS8 1TH, Avon, England.
   [Reinhard, Erik] Univ Cent Florida, Orlando, FL 32816 USA.
C3 Max Planck Society; University of Bristol; State University System of
   Florida; University of Central Florida
RI Akyuz, Ahmet O/A-7956-2018
OI Reinhard, Erik/0000-0001-9079-6572; Akyuz, Ahmet/0000-0001-7685-5572
CR [Anonymous], IEEE T VISUALIZATION
   [Anonymous], 2005, High Dynamic Range Imaging: Acquisition, Display, and Image-Based Lighting (The Morgan Kaufmann Series in Computer Graphics
   [Anonymous], 1987, The Retina: An Approachable Part of the Brain
   [Anonymous], 2002, P 13 EUR WORKSH REND
   [Anonymous], 2002, MPII20024002
   [Anonymous], 2006, Proc. 14th Pacific Conf. on Comput. Graph. Appl
   Ashikhmin M., 2006, ACM T APPL PERCEPT, V3, P399
   Cornsweet T.N., 1970, Visual Perception
   CRAIK KJW, 1966, NATURE PSYCHOL, P94
   DOOLEY RP, 1977, J OPT SOC AM, V67, P761, DOI 10.1364/JOSA.67.000761
   Durand F, 2002, ACM T GRAPHIC, V21, P257, DOI 10.1145/566570.566574
   Elder JH, 1999, INT J COMPUT VISION, V34, P97, DOI 10.1023/A:1008183703117
   Fairchild M.D., 2002, ISTSID 10 COLOR IMAG, P33
   Fairchild M.D., 2005, Color Appearance Models, V2nd
   Fairchild M D, 2004, J ELECT IMAGING
   Fairchild M.D., 2004, 1 ACM S APPL PERCEPT, P159
   Fattal R, 2002, ACM T GRAPHIC, V21, P249
   GROSSBERG S, 1985, PSYCHOL REV, V92, P173, DOI 10.1037/0033-295X.92.2.173
   JAMES AF, 2001, IEEE COMPUT GRAPH, V21, P22
   Johnson GM, 2003, ELEVENTH COLOR IMAGING CONFERENCE: COLOR SCIENCE AND ENGINEERING - SYSTEMS, TECHNOLOGIES, APPLICATIONS, P36
   Kingdom F, 1988, Spat Vis, V3, P225, DOI 10.1163/156856888X00140
   KRAUSKOPF J, 1963, J OPT SOC AM, V53, P741, DOI 10.1364/JOSA.53.000741
   Kuang JT, 2004, 12TH COLOR IMAGING CONFERENCE: COLOR SCIENCE AND ENGINEERING SYSTEMS, TECHNOLOGIES, APPLICATIONS, P315
   KUFFLER SW, 1953, J NEUROPHYSIOL, V16, P37, DOI 10.1152/jn.1953.16.1.37
   LAND EH, 1971, J OPT SOC AM, V61, P1, DOI 10.1364/JOSA.61.000001
   Ledda P, 2005, ACM T GRAPHIC, V24, P640, DOI 10.1145/1073204.1073242
   Ledda P., 2004, APGV 04, P159
   Marr D., 1982, Visual perception
   OBRIEN V, 1958, J OPT SOC AM, V48, P112, DOI 10.1364/JOSA.48.000112
   Palmer S., 1999, VISION SCI PHOTONS P
   Pattanaik SN, 2000, COMP GRAPH, P47, DOI 10.1145/344779.344810
   PATTANAIK SN, 1998, SIGGRAPH 98 C P, P287
   Pessoa L, 1998, BEHAV BRAIN SCI, V21, P723
   Press W. H., 2002, Numerical Recipes in C: the Art of Scientific Computing, V2nd ed., DOI DOI 10.1119/1.14981
   Reinhard E, 2005, IEEE T VIS COMPUT GR, V11, P13, DOI 10.1109/TVCG.2005.9
   Reinhard E, 2002, ACM T GRAPHIC, V21, P267, DOI 10.1145/566570.566575
   Reinhard E., 2003, J GRAPHICS TOOLS, V7, P45
   Schlick Christophe., 1994, Quantization techniques for visualization of high dynamic range pictures, P7
   Tomasi C, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P839, DOI 10.1109/ICCV.1998.710815
   Tumblin J, 1999, COMP GRAPH, P83, DOI 10.1145/311535.311544
   Tumblin J, 1999, ACM T GRAPHIC, V18, P56, DOI 10.1145/300776.300783
   Wachtler T, 1997, PERCEPTION, V26, P1423, DOI 10.1068/p261423
   WALLS G L, 1954, Am J Optom Arch Am Acad Optom, V31, P329
   Yoshida A, 2005, PROC SPIE, V5666, P192, DOI 10.1117/12.587782
   [No title captured]
NR 45
TC 10
Z9 11
U1 0
U2 5
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD JAN
PY 2008
VL 4
IS 4
AR 20
DI 10.1145/1278760.1278761
PG 29
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 450YH
UT WOS:000266437400001
DA 2024-07-18
ER

PT J
AU Tan, HZ
   Srinivasan, MA
   Reed, CM
   Durlach, NI
AF Tan, Hong Z.
   Srinivasan, Mandayam A.
   Reed, Charlotte M.
   Durlach, Nathaniel I.
TI Discrimination and Identification of Finger Joint-Angle Position Using
   Active Motion
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Experimentation; Human Factors; Performance; Joint position;
   kinesthesis; discrimination; identification; JND; haptic perception
AB The authors report six experiments on the human ability to discriminate and identify finger joint-angle positions using active motion. The PIP (proximal interphalangeal) joint of the index finger was examined in Exps. 1-3 and the MCP (metacarpophalangeal) joint in Exps. 4-6. In Exp. 1, the just noticeable difference (JND) of PIP joint-angle position was measured when the MCP joint was either fully extended or halfway bent. In Exp. 2, the JND of PIP joint-angle position as a function of PIP joint-angle reference position was measured when the PIP joint was almost fully extended, halfway bent, or almost fully flexed. In Exp. 3, the information transfer of PIP joint-angle position was estimated with the MCP joint in a fully extended position. In Exps. 4-6, the JND and the information transfer of MCP joint-angle position were studied with a similar experimental design. The results show that the JNDs of the PIP joint-angle position were roughly constant (2.5.-2.7.) independent of the PIP joint-angle reference position or the MCP joint-angle position used (Exps. 1 and 2). The JNDs of the MCP joint-angle position, however, increased with the flexion of both the PIP and MCP joints and ranged from 1.7. to 2.7. (Exps. 4 and 5). The information transfer of the PIP and MCP joint-angle position were similar, indicating 3-4 perfectly identifiable joint-angle positions for both joints (Exps. 3 and 6). The results provide the basic data needed for estimating, for example, the resolution of fingertip position during active free motion. They are compared to the results from previous studies on joint position, length, and thickness perception.
C1 [Tan, Hong Z.] Purdue Univ, Hapt Interface Res Lab, W Lafayette, IN 47907 USA.
   [Srinivasan, Mandayam A.] MIT, Touch Lab, Cambridge, MA 02139 USA.
   [Reed, Charlotte M.; Durlach, Nathaniel I.] MIT, Elect Res Lab, Cambridge, MA 02139 USA.
C3 Purdue University System; Purdue University; Massachusetts Institute of
   Technology (MIT); Massachusetts Institute of Technology (MIT)
RP Tan, HZ (corresponding author), Purdue Univ, Hapt Interface Res Lab, 465 NW Ave, W Lafayette, IN 47907 USA.
EM hongtan@purdue.edu; srini@mit.edu; cmreed@mit.edu; durlach@mit.edu
OI Tan, Hong/0000-0003-0032-9554; Reed, Charlotte/0000-0003-1680-1913
CR [Anonymous], 1962, Uncertainty and structure as psychological concepts
   Biggs J, 1999, EXP BRAIN RES, V125, P221, DOI 10.1007/s002210050677
   BRAIDA LD, 1972, J ACOUST SOC AM, V51, P483, DOI 10.1121/1.1912868
   Choi S, 2005, PRESENCE-TELEOP VIRT, V14, P463, DOI 10.1162/105474605774785271
   Clark F., 1986, HDB PERCEPTION HUMAN, V1
   CLARK FJ, 1985, J NEUROPHYSIOL, V54, P1529, DOI 10.1152/jn.1985.54.6.1529
   CLARK FJ, 1986, BRAIN, V109, P1195, DOI 10.1093/brain/109.6.1195
   CLARK FJ, 1995, EXP BRAIN RES, V107, P73
   CLARK FJ, 1992, BEHAV BRAIN SCI, V15, P725
   CLARK FJ, 1979, J NEUROPHYSIOL, V42, P877, DOI 10.1152/jn.1979.42.3.877
   DEDOMENICO G, 1987, EXP BRAIN RES, V65, P471, DOI 10.1007/BF00236321
   DURLACH NI, 1989, PERCEPT PSYCHOPHYS, V46, P293, DOI 10.3758/BF03208094
   DURLACH NI, 1989, PERCEPT PSYCHOPHYS, V46, P29, DOI 10.3758/BF03208071
   DURLACH NI, 1969, J ACOUST SOC AM, V46, P372, DOI 10.1121/1.1911699
   Erickson R.P., 1974, NEUROSCIENCES 3 STUD
   FERRELL WR, 1989, J PHYSIOL-LONDON, V411, P575, DOI 10.1113/jphysiol.1989.sp017591
   FERRELL WR, 1988, J PHYSIOL-LONDON, V399, P49
   HALL LA, 1983, J PHYSIOL-LONDON, V335, P519, DOI 10.1113/jphysiol.1983.sp014548
   HO C, 1997, 6 MIT TOUCH LAB
   HORCH KW, 1975, J NEUROPHYSIOL, V38, P1436, DOI 10.1152/jn.1975.38.6.1436
   HOUTSMA AJM, 1983, J ACOUST SOC AM, V74, P1626, DOI 10.1121/1.390125
   JOHN KT, 1989, EXP BRAIN RES, V78, P62
   Laidlaw R.W., 1937, Bulletin of the Neurological Institute of New York, V6, P268
   Macmillan N.A., 2004, Detection Theory: A User's Guide, P3
   Miller G.A., 1954, Information Theory in Psychophysics, P95
   MONSTER AW, 1973, NEW DEV ELECTROMYOGR, V3
   Paillard J., 1968, NEUROPSYCHOLOGY SPAT
   PANG XD, 1991, PERCEPT PSYCHOPHYS, V49, P531, DOI 10.3758/BF03212187
   Tan H., 1997, P 6 INT S HAPT INT V, V61, P197
   TAN HZ, 1995, PERCEPT PSYCHOPHYS, V57, P495, DOI 10.3758/BF03213075
   van Beers RJ, 1998, EXP BRAIN RES, V122, P367, DOI 10.1007/s002210050525
   [No title captured]
NR 32
TC 31
Z9 35
U1 1
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD JUL
PY 2007
VL 4
IS 2
AR 10
DI 10.1145/1265957.1265959
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA V04IM
UT WOS:000207052100002
DA 2024-07-18
ER

PT J
AU Hoh, WK
   Zhang, FL
   Dodgson, NA
AF Hoh, Weng Khuan
   Zhang, Fang-Lue
   Dodgson, Neil A.
TI Salient-Centeredness and Saliency Size in Computational Aesthetics
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Computational aesthetics; image aesthetics; image saliency; visual
   balance; salient-centeredness; Rule-of-Thirds; photographic composition
ID SPATIAL COMPOSITION; OBJECT DETECTION; PHOTO; MODEL; APPRECIATION;
   ATTENTION; POSITION; ISSUES; SCENE; ART
AB We investigate the optimal aesthetic location and size of a single dominant salient region in a photographic image. Existing algorithms for photographic composition do not take full account of the spatial positioning or sizes of these salient regions. We present a set of experiments to assess aesthetic preferences, inspired by theories of centeredness, principal lines, and Rule-of-Thirds. Our experimental results show a clear preference for the salient region to be centered in the image and that there is a preferred size of non-salient border around this salient region. We thus propose a novel image cropping mechanism for images containing a single salient region to achieve the best aesthetic balance. Our results show that the Rule-of-Thirds guideline is not generally valid but also allow us to hypothesize in which situations it is useful and in which it is inappropriate.
C1 [Hoh, Weng Khuan; Zhang, Fang-Lue; Dodgson, Neil A.] Victoria Univ Wellington, Wellington 6011, New Zealand.
C3 Victoria University Wellington
RP Hoh, WK (corresponding author), Victoria Univ Wellington, Wellington 6011, New Zealand.
EM wengkhuan.hoh@vuw.ac.nz; fanglue.zhang@vuw.ac.nz; neil.dodgson@vuw.ac.nz
RI Dodgson, Neil/A-4506-2009
OI Dodgson, Neil/0000-0001-7649-8528; Hoh, Weng Khuan/0000-0003-4342-4228
CR Alpert S, 2007, PROC CVPR IEEE, P359
   ANDERSON TW, 1952, ANN MATH STAT, V23, P193, DOI 10.1214/aoms/1177729437
   [Anonymous], 2007, PSYCHOL AESTHET CREA, DOI DOI 10.1037/1931-3896.1.1.8
   [Anonymous], 2009, P 17 ACM INT C MULT
   [Anonymous], 1965, Art and visual perception: A psychology of the creative eye
   [Anonymous], 1998, Visual intelligence
   Aydin TO, 2015, IEEE T VIS COMPUT GR, V21, P31, DOI 10.1109/TVCG.2014.2325047
   Aytekin Ç, 2014, INT C PATT RECOG, P112, DOI 10.1109/ICPR.2014.29
   Bauerly M, 2006, INT J HUM-COMPUT ST, V64, P670, DOI 10.1016/j.ijhcs.2006.01.002
   Bhattacharya S, 2011, ACM T MULTIM COMPUT, V7, DOI 10.1145/2037676.2037678
   Birkhoff G.D., 2013, Aesthetic Measure
   Borji A, 2015, Arxiv, DOI arXiv:1505.03581
   Borji A, 2015, IEEE T IMAGE PROCESS, V24, P5706, DOI 10.1109/TIP.2015.2487833
   Borji A, 2013, VISION RES, V91, P62, DOI 10.1016/j.visres.2013.07.016
   Chen YC, 2022, COGNITION, V225, DOI 10.1016/j.cognition.2022.105114
   Chen YC, 2018, COGNITION, V176, P209, DOI 10.1016/j.cognition.2018.02.010
   Cheng MM, 2015, IEEE T PATTERN ANAL, V37, P569, DOI 10.1109/TPAMI.2014.2345401
   Cheng MM, 2013, IEEE I CONF COMP VIS, P1529, DOI 10.1109/ICCV.2013.193
   Cheng MM, 2014, VISUAL COMPUT, V30, P443, DOI 10.1007/s00371-013-0867-4
   Cheng MM, 2011, PROC CVPR IEEE, P409, DOI 10.1109/CVPR.2011.5995344
   Cupchick G.C., 2007, Psychology of Aesthetics, Creativity, and the Arts, V1, P16, DOI [10.1037/1931-3896.1.1.16, DOI 10.1037/1931-3896.1.1.16, 10.1037/1931 -3896.1.1.16]
   Datta R, 2006, LECT NOTES COMPUT SC, V3953, P288, DOI 10.1007/11744078_23
   Dhar S, 2011, PROC CVPR IEEE, P1657, DOI 10.1109/CVPR.2011.5995467
   Dodgson NA, 2009, COMPUT GRAPH-UK, V33, P475, DOI 10.1016/j.cag.2009.04.001
   DPChallenge.com, 2020, DPCHALLENGE COM
   Forman IR, 2021, ATTEN PERCEPT PSYCHO, V83, P2151, DOI 10.3758/s13414-021-02289-y
   Gombrich EH, 2000, J CONSCIOUSNESS STUD, V7, P17
   Graves M., 1951, The art of color and design, V2d
   Guo YW, 2012, COMPUT GRAPH FORUM, V31, P2193, DOI 10.1111/j.1467-8659.2012.03212.x
   Harel J, 2006, Advances in Neural Information Processing Systems, V19
   Henry C., 1885, REV CONTEMPORAINE
   Henry Charles, 2018, ART TRANSLATION, V10, P198
   Hoh Weng Khuan, 2022, DISSERTATION
   Hou XD, 2012, IEEE T PATTERN ANAL, V34, P194, DOI 10.1109/TPAMI.2011.146
   Hou XD, 2007, PROC CVPR IEEE, P2280
   Hume David, 1826, ESSAYS MORAL POLITIC, V3
   Hyman J, 2010, BOST STUD PHILOS HIS, V262, P245, DOI 10.1007/978-90-481-3851-7_11
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Jahanian A, 2015, PROC SPIE, V9394, DOI 10.1117/12.2084548
   Jiang BW, 2013, IEEE I CONF COMP VIS, P1665, DOI 10.1109/ICCV.2013.209
   Jiang HZ, 2013, PROC CVPR IEEE, P2083, DOI 10.1109/CVPR.2013.271
   Judd T, 2009, IEEE I CONF COMP VIS, P2106, DOI 10.1109/ICCV.2009.5459462
   JULESZ B, 1975, SCI AM, V232, P34, DOI 10.1038/scientificamerican0475-34
   Julesz B., 1995, DIALOGUES PERCEPTION
   Ke Y., 2006, P IEEE COMP SOC C CO, V1, P419, DOI DOI 10.1109/CVPR.2006.303
   Khan S.S., 2012, P 8 ANN S COMPUTATIO, P55
   Kong S, 2016, LECT NOTES COMPUT SC, V9905, P662, DOI 10.1007/978-3-319-46448-0_40
   Konkle T, 2011, J EXP PSYCHOL HUMAN, V37, P23, DOI 10.1037/a0020413
   Leder H, 2004, BRIT J PSYCHOL, V95, P489, DOI 10.1348/0007126042369811
   Leder H, 2014, BRIT J PSYCHOL, V105, P443, DOI 10.1111/bjop.12084
   Li C, 2010, P ACM INT C MULT, P827, DOI DOI 10.1145/1873951.1874089
   Li Y, 2014, PR IEEE COMP DESIGN, P521, DOI 10.1109/ICCD.2014.6974732
   Linsen S, 2011, PERCEPTION, V40, P291, DOI 10.1068/p6835
   Liu J, 2020, IEEE SIGNAL PROC LET, V27, P2014, DOI 10.1109/LSP.2020.3035065
   Liu LG, 2010, COMPUT GRAPH FORUM, V29, P469, DOI 10.1111/j.1467-8659.2009.01616.x
   Lo KY, 2012, INT C PATT RECOG, P2186
   Locher P., 1996, EMPIR STUD ARTS, V14, P17
   Luo W, 2011, IEEE I CONF COMP VIS, P2206, DOI 10.1109/ICCV.2011.6126498
   Luo YW, 2008, LECT NOTES COMPUT SC, V5304, P386
   Ma S, 2017, PROC CVPR IEEE, P722, DOI 10.1109/CVPR.2017.84
   Machado P., 1998, Advances in Artificial Intelligence. 14th Brazilian Symposium on Artificial Intelligence, SBIA'98. Proceedings, P219
   Martindale C., 1999, Journal of Consciousness Studies, V6, P52
   Mavridaki E, 2015, IEEE IMAGE PROC, P887, DOI 10.1109/ICIP.2015.7350927
   McIver Lopes D, 2002, J AESTHET ART CRITIC, V60, P365
   McManus IC, 2011, I-PERCEPTION, V2, P615, DOI 10.1068/i0445aap
   McManus IC, 2011, PERCEPTION, V40, P332, DOI 10.1068/p6700
   MCMANUS IC, 1980, BRIT J PSYCHOL, V71, P505, DOI 10.1111/j.2044-8295.1980.tb01763.x
   Moles A. A., 1966, THEORIE INFORM PERCE
   Murray N, 2012, PROC CVPR IEEE, P2408, DOI 10.1109/CVPR.2012.6247954
   Neumann L., 2005, Computational aesthetics in graphics, visualization and imaging, P13, DOI DOI 10.2312/COMPAESTH/COMPAESTH05/013-018
   Nishiyama M, 2011, PROC CVPR IEEE, P33, DOI 10.1109/CVPR.2011.5995539
   Obrador P, 2010, IEEE IMAGE PROC, P3185, DOI 10.1109/ICIP.2010.5654231
   OTSU N, 1979, IEEE T SYST MAN CYB, V9, P62, DOI 10.1109/TSMC.1979.4310076
   PALMER SE, 1991, PERCEPTION OF STRUCTURE, P23, DOI 10.1037/10101-001
   Palmer SE, 2008, SPATIAL VISION, V21, P421, DOI 10.1163/156856808784532662
   Palmer SE, 2013, ANNU REV PSYCHOL, V64, P77, DOI 10.1146/annurev-psych-120710-100504
   Ramachandran V. S., 1999, Journal of Consciousness Studies, V6, P15
   Ross Denman., 1907, A Theory of Pure Design: Harmony, Balance and Rhythm
   Rublee E, 2011, IEEE I CONF COMP VIS, P2564, DOI 10.1109/ICCV.2011.6126544
   Sammartino J, 2012, J EXP PSYCHOL HUMAN, V38, P865, DOI 10.1037/a0027736
   Setlur Vidya., 2005, MUM, V154, P59, DOI DOI 10.1145/1149488.1149499
   SHAPIRO SS, 1965, BIOMETRIKA, V52, P591, DOI 10.1093/biomet/52.3-4.591
   Tang XO, 2013, IEEE T MULTIMEDIA, V15, P1930, DOI 10.1109/TMM.2013.2269899
   Tong HG, 2004, LECT NOTES COMPUT SC, V3331, P198
   TREISMAN A, 1982, J EXP PSYCHOL HUMAN, V8, P194, DOI 10.1037/0096-1523.8.2.194
   TREISMAN A, 1985, COMPUT VISION GRAPH, V31, P156, DOI 10.1016/S0734-189X(85)80004-9
   Vessel EA, 2014, PROC SPIE, V9014, DOI 10.1117/12.2043126
   Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb
   Wallen Ruth, 1999, J CONSCIOUSNESS STUD, V6, P68
   Wang WN, 2019, IEEE IMAGE PROC, P1875, DOI [10.1109/icip.2019.8803119, 10.1109/ICIP.2019.8803119]
   Wang YQ, 2016, INT C PATT RECOG, P3554, DOI 10.1109/ICPR.2016.7900185
   Wei ZJ, 2018, PROC CVPR IEEE, P5437, DOI 10.1109/CVPR.2018.00570
   Yan Q, 2013, PROC CVPR IEEE, P1155, DOI 10.1109/CVPR.2013.153
   Yang C, 2013, PROC CVPR IEEE, P3166, DOI 10.1109/CVPR.2013.407
   Zeng H, 2022, IEEE T PATTERN ANAL, V44, P1304, DOI 10.1109/TPAMI.2020.3024207
   Zhang FL, 2013, IEEE T MULTIMEDIA, V15, P1480, DOI 10.1109/TMM.2013.2268051
   Zhang JM, 2013, IEEE I CONF COMP VIS, P153, DOI 10.1109/ICCV.2013.26
   Zhang MJ, 2005, 2005 IEEE International Conference on Multimedia and Expo (ICME), Vols 1 and 2, P438
   Zhou YM, 2014, LECT NOTES COMPUT SC, V8588, P357, DOI 10.1007/978-3-319-09333-8_39
   Zhu T, 2020, THIRD INTERNATIONAL CONFERENCE ON MULTIMEDIA INFORMATION PROCESSING AND RETRIEVAL (MIPR 2020), P360, DOI 10.1109/MIPR49039.2020.00079
NR 100
TC 0
Z9 0
U1 1
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD APR
PY 2023
VL 20
IS 2
DI 10.1145/3588317
PG 23
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA H6MI8
UT WOS:000997078100003
DA 2024-07-18
ER

PT J
AU Fukiage, T
   Oishi, T
AF Fukiage, Taiki
   Oishi, Takeshi
TI A Content-adaptive Visibility Predictor for Perceptually Optimized Image
   Blending
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Alpha blending; image blending; human visual system; contrast
   perception; visibility
ID CONTRAST CONSTANCY; VISION; COLOR; NORMALIZATION; PERCEPTION;
   MECHANISMS; MODEL
AB The visibility of an image semi-transparently overlaid on another image varies significantly, depending on the content of the images. This makes it difficult to maintain the desired visibility level when the image content changes. To tackle this problem, we developed a perceptual model to predict the visibility of the blended results of arbitrarily combined images. Conventional visibility models cannot reflect the dependence of the suprathreshold visibility of the blended images on the appearance of the pre-blended image content. Therefore, we have proposed a visibility model with a content-adaptive feature aggregation mechanism, which integrates the visibility for each image feature (i.e., such as spatial frequency and colors) after applying weights that are adaptively determined according to the appearance of the input image. We conducted a large-scale psychophysical experiment to develop the visibility predictor model. Ablation studies revealed the importance of the adaptive weighting mechanism in accurately predicting the visibility of blended images. We have also proposed a technique for optimizing the image opacity such that users can set the visibility of the target image to an arbitrary level. Our evaluation revealed that the proposed perceptually optimized image blending was effective under practical conditions.
C1 [Fukiage, Taiki] NTT Corp, NTT Commun Sci Labs, 3-1 Wakamiya, Atsugi, Kanagawa 2430198, Japan.
   [Oishi, Takeshi] Univ Tokyo, Inst Ind Sci, Meguro Ku, 4-6-1 Komaba, Tokyo, Japan.
C3 Nippon Telegraph & Telephone Corporation; University of Tokyo
RP Fukiage, T (corresponding author), NTT Corp, NTT Commun Sci Labs, 3-1 Wakamiya, Atsugi, Kanagawa 2430198, Japan.
EM t.fukiage@gmail.com; oishi@cvl.iis.u-tokyo.ac.jp
OI Oishi, Takeshi/0000-0002-2010-2608
CR [Anonymous], 2015, BT7096 ITU R
   BRADY N, 1995, VISION RES, V35, P739, DOI 10.1016/0042-6989(94)00172-I
   BURT PJ, 1983, ACM T GRAPHIC, V2, P217, DOI 10.1145/245.247
   CAMPBELL FW, 1968, J PHYSIOL-LONDON, V197, P551, DOI 10.1113/jphysiol.1968.sp008574
   Chan MY, 2009, IEEE T VIS COMPUT GR, V15, P1283, DOI 10.1109/TVCG.2009.172
   Chuang J, 2009, IEEE T VIS COMPUT GR, V15, P1275, DOI 10.1109/TVCG.2009.150
   Cimpoi M, 2014, PROC CVPR IEEE, P3606, DOI 10.1109/CVPR.2014.461
   DALY S, 1992, P SOC PHOTO-OPT INS, V1666, P2, DOI 10.1117/12.135952
   DALY S, 1989, P SOC PHOTO-OPT INS, V1077, P217
   Ding KY, 2022, IEEE T PATTERN ANAL, V44, P2567, DOI 10.1109/TPAMI.2020.3045810
   Faul F, 2017, ACM T APPL PERCEPT, V14, DOI 10.1145/3022732
   Fleming RW, 2011, PSYCHOL SCI, V22, P812, DOI 10.1177/0956797611408734
   Foley John M., 2019, J VISUAL-JAPAN, V19, P9
   Fukiage T, 2019, IEEE T VIS COMPUT GR, V25, P2061, DOI 10.1109/TVCG.2019.2898738
   Fukiage T, 2014, INT SYM MIX AUGMENT, P63, DOI 10.1109/ISMAR.2014.6948410
   Gabbard JL, 2007, IEEE VIRTUAL REALITY 2007, PROCEEDINGS, P35
   GEORGESON MA, 1975, J PHYSIOL-LONDON, V252, P627, DOI 10.1113/jphysiol.1975.sp011162
   Geusebroek JM, 2005, INT J COMPUT VISION, V61, P103, DOI 10.1023/B:VISI.0000042993.50813.60
   Grundhöfer A, 2008, IEEE T VIS COMPUT GR, V14, P97, DOI 10.1109/TVCG.2007.1052
   Grundland M, 2006, COMPUT GRAPH FORUM, V25, P577, DOI 10.1111/j.1467-8659.2006.00977.x
   HEEGER DJ, 1992, VISUAL NEUROSCI, V9, P181, DOI 10.1017/S0952523800009640
   Itthipuripat S, 2019, J VISION, V19, DOI 10.1167/19.14.8
   Kalkofen D, 2013, INT SYM MIX AUGMENT, P1
   Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453
   KELLY DH, 1979, J OPT SOC AM, V69, P1340, DOI 10.1364/JOSA.69.001340
   Kim YJ, 2016, J VISION, V16, DOI 10.1167/16.11.15
   Kingma D, 2014, ICLR P, V2014, P1
   Laparra V, 2016, ELECT IMAG, V2016, P1
   Laparra V, 2017, J OPT SOC AM A, V34, P1511, DOI 10.1364/JOSAA.34.001511
   Laparra V, 2010, J OPT SOC AM A, V27, P852, DOI 10.1364/JOSAA.27.000852
   Li YZ, 2005, ACM T GRAPHIC, V24, P836, DOI 10.1145/1073204.1073271
   Liu YC, 2014, PROC SPIE, V9016, DOI 10.1117/12.2038599
   LOSADA MA, 1994, VISION RES, V34, P331, DOI 10.1016/0042-6989(94)90091-4
   Lyu SW, 2008, PROC CVPR IEEE, P3721
   Mantiuk R, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964935
   MULLEN KT, 1985, J PHYSIOL-LONDON, V359, P381, DOI 10.1113/jphysiol.1985.sp015591
   Myers L., 2004, Encyclopedia of Statistical Sciences
   Neri P, 2017, PLOS BIOL, V15, DOI 10.1371/journal.pbio.1002611
   Olmos A, 2004, PERCEPTION, V33, P1463, DOI 10.1068/p5321
   Picard Rosalind, 1995, MIT VISION TEXTURES
   Porter T., 1984, Computers & Graphics, V18, P253
   Ramasubramanian M, 1999, COMP GRAPH, P73, DOI 10.1145/311535.311543
   Rideaux R, 2022, J VISION, V22, DOI 10.1167/jov.22.1.4
   Sandor C., 2010, 2010 9th IEEE International Symposium on Mixed and Augmented Reality (ISMAR). Science & Technology Papers, P27, DOI 10.1109/ISMAR.2010.5643547
   Santos MEC, 2016, MULTIMED TOOLS APPL, V75, P9563, DOI 10.1007/s11042-015-2954-1
   Schwartz O, 2001, NAT NEUROSCI, V4, P819, DOI 10.1038/90526
   TEO PC, 1994, P SOC PHOTO-OPT INS, V2179, P127, DOI 10.1117/12.172664
   Teufel C, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-018-28845-5
   Tursun OT, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3322985
   Wang YH, 2011, COMPUT GRAPH FORUM, V30, P2117, DOI 10.1111/j.1467-8659.2011.02045.x
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang Z, 2006, IEEE IMAGE PROC, P2945, DOI 10.1109/ICIP.2006.313136
   Watson AB, 1997, J OPT SOC AM A, V14, P2379, DOI 10.1364/JOSAA.14.002379
   Wilber MJ, 2017, IEEE I CONF COMP VIS, P1211, DOI 10.1109/ICCV.2017.136
   Wolski K, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3196493
   Wuerger SM, 2020, J VISION, V20, DOI 10.1167/jov.20.4.23
   Ye NY, 2019, PROC CVPR IEEE, P5429, DOI 10.1109/CVPR.2019.00558
   Zeng WJ, 2000, IEEE IMAGE PROC, P657, DOI 10.1109/ICIP.2000.901044
   Zhang JM, 2013, IEEE I CONF COMP VIS, P153, DOI 10.1109/ICCV.2013.26
   Zhang LL, 2021, INT SYM MIX AUGMENT, P115, DOI 10.1109/ISMAR-Adjunct54149.2021.00033
   Zhang L, 2014, IEEE T IMAGE PROCESS, V23, P4270, DOI 10.1109/TIP.2014.2346028
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhang YJ, 2022, IEEE T VIS COMPUT GR, V28, P4490, DOI 10.1109/TVCG.2021.3091686
   Zheng L, 2013, IEEE T VIS COMPUT GR, V19, P446, DOI 10.1109/TVCG.2012.144
NR 66
TC 0
Z9 0
U1 2
U2 7
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD JAN
PY 2023
VL 20
IS 1
AR 3
DI 10.1145/3565972
PG 29
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 7S6AV
UT WOS:000910834800003
OA Bronze
DA 2024-07-18
ER

PT J
AU Salagean, A
   Hadnett-Hunter, J
   Finnegan, DJ
   De Sousa, AA
   Proulx, MJ
AF Salagean, Anca
   Hadnett-Hunter, Jacob
   Finnegan, Daniel J.
   De Sousa, Alexandra A.
   Proulx, Michael J.
TI A Virtual Reality Application of the Rubber Hand Illusion Induced by
   Ultrasonic Mid-air Haptic Stimulation
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Virtual hand illusion; body ownership; ultrasonic mid-air haptics;
   UltraLeap; skin type; immersion
ID BODY OWNERSHIP; EMBODIMENT; SENSE; TOUCH; SKIN; MOVEMENTS; ME
AB Ultrasonic mid-air haptic technologies, which provide haptic feedback through airwaves produced using ultrasound, could be employed to investigate the sense of body ownership and immersion in virtual reality (VR) by inducing the virtual hand illusion (VHI). Ultrasonic mid-air haptic perception has solely been investigated for glabrous (hairless) skin, which has higher tactile sensitivity than hairy skin. In contrast, the VHI paradigm typically targets hairy skin without comparisons to glabrous skin. The aim of this article was to investigate illusory body ownership, the applicability of ultrasonic mid-air haptics, and perceived immersion in VR using the VHI. Fifty participants viewed a virtual hand being stroked by a feather synchronously and asynchronously with the ultrasonic stimulation applied to the glabrous skin on the palmar surface and the hairy skin on the dorsal surface of their hands. Questionnaire responses revealed that synchronous stimulation induced a stronger VHI than asynchronous stimulation. In synchronous conditions, the VHI was stronger for palmar stimulation than dorsal stimulation. The ultrasonic stimulation was also perceived as more intense on the palmar surface compared to the dorsal surface. Perceived immersion was not related to illusory body ownership per se but was enhanced by the provision of synchronous stimulation.
C1 [Salagean, Anca; Proulx, Michael J.] Univ Bath, Dept Psychol, Bath BA2 7AY, Avon, England.
   [Hadnett-Hunter, Jacob] Univ Bath, Dept Comp Sci, Bath BA2 7AY, Avon, England.
   [Finnegan, Daniel J.] Cardiff Univ, Sch Comp Sci & Amp Informat, Cardiff CF24 3AA, Wales.
   [De Sousa, Alexandra A.] Bath Spa Univ, Ctr Hlth & Cognit, Bath BA2 9BN, Avon, England.
C3 University of Bath; University of Bath; Cardiff University; Bath Spa
   University
RP Salagean, A (corresponding author), Univ Bath, Dept Psychol, Bath BA2 7AY, Avon, England.
EM as3101@bath.ac.uk; jmehh20@bath.ac.uk; FinneganD@cardiff.ac.uk;
   a.desousa@bathspa.ac.uk; m.j.proulx@bath.ac.uk
RI Proulx, Michael J/A-1045-2008; de Sousa, Alexandra Allison/C-3556-2013
OI de Sousa, Alexandra Allison/0000-0003-2379-3894; Salagean,
   Anca/0000-0001-9465-1223; Proulx, Michael/0000-0003-4066-3645
FU SWCTN Immersion Grant; CAMERA 2.0, the UKRI Centre for the Analysis of
   Motion, Entertainment Research and Applications [EP/T014865/1]; EPSRC
   [EP/T022523/1] Funding Source: UKRI
FX This project was funded by a SWCTN Immersion Grant to Dr. Alexandra de
   Sousa. Michael J. Proulx's research is partly funded by CAMERA 2.0, the
   UKRI Centre for the Analysis of Motion, Entertainment Research and
   Applications (Grant No. EP/T014865/1).
CR Alais D, 2010, SEEING PERCEIVING, V23, P3, DOI 10.1163/187847510X488603
   Arabzadeh E, 2008, PSYCHOL SCI, V19, P635, DOI 10.1111/j.1467-9280.2008.02134.x
   Ariza O, 2016, SUI'16: PROCEEDINGS OF THE 2016 SYMPOSIUM ON SPATIAL USER INTERACTION, P61, DOI 10.1145/2983310.2985760
   Armel KC, 2003, P ROY SOC B-BIOL SCI, V270, P1499, DOI 10.1098/rspb.2003.2364
   BENSHAKHAR G, 1987, PSYCHOPHYSIOLOGY, V24, P247, DOI 10.1111/j.1469-8986.1987.tb00287.x
   Bharaj G, 2012, COMPUT GRAPH FORUM, V31, P755, DOI 10.1111/j.1467-8659.2012.03034.x
   Blanca MJ, 2017, PSICOTHEMA, V29, P552, DOI 10.7334/psicothema2016.383
   Botvinick M, 1998, NATURE, V391, P756, DOI 10.1038/35784
   Boucsein W, 2012, PSYCHOPHYSIOLOGY, V49, P1017, DOI 10.1111/j.1469-8986.2012.01384.x
   Braithwaite J.J., 2015, Issues surrounding the normalization and standardisation of skin conductance responses (SCRs). Technical Research Note
   Braithwaite JJ, 2015, A guide for Analysing Electrodermal Activity (EDA) & skin conductance responses. (SCRs) for Psychological Experiments
   Braun N, 2018, FRONT PSYCHOL, V9, DOI 10.3389/fpsyg.2018.00535
   Carl E, 2019, J ANXIETY DISORD, V61, P27, DOI 10.1016/j.janxdis.2018.08.003
   Carter T., 2013, P 26 ANN ACM S US IN, P505
   Chessa M, 2019, HUM-COMPUT INTERACT, V34, P51, DOI 10.1080/07370024.2016.1243478
   Crucianelli L, 2018, CORTEX, V104, P180, DOI 10.1016/j.cortex.2017.04.018
   Culbertson H, 2018, ANNU REV CONTR ROBOT, V1, P385, DOI 10.1146/annurev-control-060117-105043
   Dawson ME, 2007, HANDBOOK OF PSYCHOPHYSIOLOGY, 3RD EDITION, P159, DOI 10.1017/cbo9780511546396.007
   De Vignemont F, 2007, MIND LANG, V22, P427, DOI 10.1111/j.1468-0017.2007.00315.x
   de Vignemont F, 2011, CONSCIOUS COGN, V20, P82, DOI 10.1016/j.concog.2010.09.004
   Dieguez S, 2017, ANN PHYS REHABIL MED, V60, P198, DOI 10.1016/j.rehab.2016.04.007
   Dummer T, 2009, PERCEPTION, V38, P271, DOI 10.1068/p5921
   Egan D, 2016, 2016 EIGHTH INTERNATIONAL CONFERENCE ON QUALITY OF MULTIMEDIA EXPERIENCE (QOMEX)
   Ehrsson HH, 2020, MULTISENSORY PERCEPTION: FROM LABORATORY TO CLINIC, P179, DOI 10.1016/B978-0-12-812492-5.00008-5
   FaeWright, 2017, THESIS U COLL LONDON
   Fang W, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00261
   Farmer H, 2012, CONSCIOUS COGN, V21, P1242, DOI 10.1016/j.concog.2012.04.011
   Filippetti ML, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-38880-5
   Flindall JW, 2019, LATERALITY, V24, P176, DOI 10.1080/1357650X.2018.1494184
   Freeman D, 2017, PSYCHOL MED, V47, P2393, DOI 10.1017/S003329171700040X
   Gallace A., 2014, In touch with the future: The sense of touch from cognitive neuroscience to virtual reality
   Gillmeister H, 2007, BRAIN RES, V1160, P58, DOI 10.1016/j.brainres.2007.03.041
   Gonzalez-Franco M, 2018, FRONT ROBOT AI, V5, DOI 10.3389/frobt.2018.00074
   Graham KT, 2015, ATTEN PERCEPT PSYCHO, V77, P207, DOI 10.3758/s13414-014-0748-6
   Haggard P, 2017, NAT REV NEUROSCI, V18, P197, DOI 10.1038/nrn.2017.14
   Hasler BS, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0174965
   Hatzfeld C., 2014, ENG HAPTIC DEVICES B, V2nd, DOI DOI 10.1007/978-1-4471-6518-7
   Horiuchi Y, 2017, 2017 IEEE WORLD HAPTICS CONFERENCE (WHC), P490, DOI 10.1109/WHC.2017.7989950
   IJsselsteijn WA, 2006, PRESENCE-TELEOP VIRT, V15, P455, DOI 10.1162/pres.15.4.455
   Kaas J.H., 2004, HUMAN NERVOUS SYSTEM, P1059, DOI DOI 10.1016/B978-012547626-3/50029-6
   Kalckert A, 2014, CONSCIOUS COGN, V26, P117, DOI 10.1016/j.concog.2014.02.003
   Kalckert Andreas, 2019, FRONT HUM NEUROSCI, V13, P88, DOI DOI 10.3389/FNHUM.2019.00088
   Kilteni K, 2012, PRESENCE-TELEOP VIRT, V21, P373, DOI 10.1162/PRES_a_00124
   Kokkinara E, 2016, SCI REP-UK, V6, DOI 10.1038/srep28879
   Kokkinara E, 2015, ACM T APPL PERCEPT, V13, DOI 10.1145/2818998
   Kokkinara E, 2014, PERCEPTION, V43, P43, DOI 10.1068/p7545
   Kothgassner O. D., 2020, Annals of the International Communication Association, V44, P210, DOI [DOI 10.1080/23808985.2020.1792790, https://doi.org/10.1080/23808985.2020.1792790]
   Lier EJ, 2018, PLOS ONE, V13, DOI 10.1371/journal.pone.0208405
   Limbasiya Himanshu, 2018, THESIS U DUBLIN DUBL
   Lin Lorraine., 2016, Proceedings of the ACM Symposium on Applied Perception, P69, DOI [DOI 10.1145/2931002.2931006, 10.1145/2931002.2931006]
   Lira M, 2017, SCI REP-UK, V7, DOI 10.1038/s41598-017-16137-3
   Llobera J, 2013, EXP BRAIN RES, V225, P105, DOI 10.1007/s00221-012-3352-9
   Longo MR, 2008, COGNITION, V107, P978, DOI 10.1016/j.cognition.2007.12.004
   Ma K, 2013, FRONT PSYCHOL, V4, DOI 10.3389/fpsyg.2013.00604
   Mahns DA, 2006, J NEUROPHYSIOL, V95, P1442, DOI 10.1152/jn.00483.2005
   Maister L, 2013, COGNITION, V128, P170, DOI 10.1016/j.cognition.2013.04.002
   Neely G, 2006, INT J IND ERGONOM, V36, P135, DOI 10.1016/j.ergon.2005.09.003
   Nierula B, 2017, J PAIN, V18, P645, DOI 10.1016/j.jpain.2017.01.003
   Nierula Birgit, 2018, THESIS U BARCELONA B
   Obrist M., 2013, P SIGCHI C HUMAN FAC, P1659, DOI [10.1145/2470654.2466220, DOI 10.1145/2470654.2466220]
   Obrist M, 2015, CHI 2015: PROCEEDINGS OF THE 33RD ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P2053, DOI 10.1145/2702123.2702361
   Olausson H, 2010, NEUROSCI BIOBEHAV R, V34, P185, DOI 10.1016/j.neubiorev.2008.09.011
   Pan XN, 2018, BRIT J PSYCHOL, V109, P395, DOI 10.1111/bjop.12290
   Parsons TD, 2015, FRONT HUM NEUROSCI, V9, DOI 10.3389/fnhum.2015.00660
   Pasqualotto A, 2015, MULTISENS RES, V28, P101, DOI 10.1163/22134808-00002473
   Peck TC, 2013, CONSCIOUS COGN, V22, P779, DOI 10.1016/j.concog.2013.04.016
   Perez-Marcos D, 2012, COGN NEURODYNAMICS, V6, P295, DOI 10.1007/s11571-011-9178-5
   Petkova VI, 2011, FRONT PSYCHOL, V2, DOI 10.3389/fpsyg.2011.00035
   Piryankova IV, 2014, ACM T APPL PERCEPT, V11, DOI 10.1145/2641568
   Pittera D, 2018, PROCEEDINGS OF THE TWELFTH INTERNATIONAL CONFERENCE ON TANGIBLE, EMBEDDED, AND EMBODIED INTERACTION (TEI'18), P709, DOI 10.1145/3173225.3173340
   Pittera D, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300362
   Pittera D, 2019, IEEE T HAPTICS, V12, P615, DOI 10.1109/TOH.2019.2897303
   Pritchard SC, 2016, FRONT PSYCHOL, V7, DOI 10.3389/fpsyg.2016.01649
   Pyasik M, 2019, PSYCHOL RES-PSYCH FO, V83, P185, DOI 10.1007/s00426-018-1137-x
   Riva G, 2019, CYBERPSYCH BEH SOC N, V22, P82, DOI 10.1089/cyber.2017.29099.gri
   Rizzo A, 2017, NEUROPSYCHOLOGY, V31, P877, DOI 10.1037/neu0000405
   Sanchez-Vives MV, 2010, PLOS ONE, V5, DOI 10.1371/journal.pone.0010381
   Sanchez-Vives MV, 2005, NAT REV NEUROSCI, V6, P332, DOI 10.1038/nrn1651
   Schwind V, 2018, ACM SYMPOSIUM ON APPLIED PERCEPTION (SAP 2018), DOI 10.1145/3225153.3225158
   Schwind V, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300590
   Skarbez R, 2017, IEEE T VIS COMPUT GR, V23, P1322, DOI 10.1109/TVCG.2017.2657158
   Slater M, 2016, FRONT ROBOT AI, V3, DOI 10.3389/frobt.2016.00074
   Slater M, 2009, FRONT NEUROSCI-SWITZ, V3, P214, DOI 10.3389/neuro.01.029.2009
   Slater M, 2010, PLOS ONE, V5, DOI 10.1371/journal.pone.0010564
   Slater M, 2009, PHILOS T R SOC B, V364, P3549, DOI 10.1098/rstb.2009.0138
   Slater M, 2008, FRONT HUM NEUROSCI, V2, DOI 10.3389/neuro.09.006.2008
   Steed A, 2016, IEEE T VIS COMPUT GR, V22, P1406, DOI 10.1109/TVCG.2016.2518135
   Sung YT, 2018, BEHAV RES METHODS, V50, P1694, DOI 10.3758/s13428-018-1041-8
   Huynh TV, 2019, PLOS ONE, V14, DOI 10.1371/journal.pone.0210058
   Tieri G, 2015, SCI REP-UK, V5, DOI 10.1038/srep17139
   Tsakiris M, 2005, J EXP PSYCHOL HUMAN, V31, P80, DOI 10.1037/0096-1523.31.1.80
   van Stralen HE, 2014, COGNITION, V131, P147, DOI 10.1016/j.cognition.2013.11.020
   Waltemate T, 2018, IEEE T VIS COMPUT GR, V24, P1643, DOI 10.1109/TVCG.2018.2794629
   Wilson G, 2014, 32ND ANNUAL ACM CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2014), P1133, DOI 10.1145/2556288.2557033
   Yuan Y, 2010, P IEEE VIRT REAL ANN, P95, DOI 10.1109/VR.2010.5444807
   Zopf R, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-017-18492-7
NR 96
TC 3
Z9 4
U1 3
U2 9
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD JAN
PY 2022
VL 19
IS 1
AR 3
DI 10.1145/3487563
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 9R2WY
UT WOS:000945516600003
OA Green Submitted, Green Accepted
DA 2024-07-18
ER

PT J
AU Abebe, MA
   Pouli, T
   Larabi, MC
   Reinhard, E
AF Abebe, Mekides Assefa
   Pouli, Tania
   Larabi, Mohamed-Chaker
   Reinhard, Erik
TI Perceptual Lightness Modeling for High-Dynamic-Range Imaging
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Lightness modeling; color appearance modeling; ITMO; high dynamic range
   imaging; psycho-visual experiment
ID APPEARANCE MODEL; COLOR-APPEARANCE
AB The human visual system (HVS) non-linearly processes light from the real world, allowing us to perceive detail over a wide range of illumination. Although models that describe this non-linearity are constructed based on psycho-visual experiments, they generally apply to a limited range of illumination and therefore may not fully explain the behavior of theHVS under more extreme illumination conditions. We propose a modified experimental protocol for measuring visual responses to emissive stimuli that do not require participant training, nor requiring the exclusion of non-expert participants. Furthermore, the protocol can be applied to stimuli covering an extended luminance range. Based on the outcome of our experiment, we propose a new model describing lightness response over an extended luminance range. The model can be integrated with existing color appearance models or perceptual color spaces. To demonstrate the effectiveness of our model in high dynamic range applications, we evaluate its suitability for dynamic range expansion relative to existing solutions.
C1 [Abebe, Mekides Assefa] Univ Poitiers & Tech Res Innovat, Poitiers, France.
   [Pouli, Tania; Reinhard, Erik] Tech Res & Innovat, Poitiers, France.
   [Larabi, Mohamed-Chaker] Univ Poitiers, Poitiers, France.
C3 Universite de Poitiers; Universite de Poitiers
RP Abebe, MA (corresponding author), Univ Poitiers & Tech Res Innovat, Poitiers, France.
EM mekides123@gmail.com; taniapouli@gmail.com;
   chaker.larabi@univ-poitiers.fr; Erik.Reinhard@technicolor.com
RI Abebe, Mekides Assefa/AAC-8761-2022
OI Abebe, Mekides Assefa/0000-0001-6756-9769; Pouli,
   Tania/0000-0002-5941-086X; Reinhard, Erik/0000-0001-9079-6572
CR Abebe MA, 2015, ACM T APPL PERCEPT, V12, DOI 10.1145/2808232
   Abebe Mekides Assefa, 2015, EUR S REND EXP ID IM
   Akyüz AG, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1276377.1276425
   [Anonymous], 2013, EUROGRAPHICS 2013 SH
   [Anonymous], 2011, THESIS
   [Anonymous], P SPIE
   [Anonymous], 2000, WILEY SERIES PURE AP
   Banterle F, 2011, ADVANCED HIGH DYNAMIC RANGE IMAGING: THEORY AND PRACTICE, P1
   Banterle F, 2009, COMPUT GRAPH FORUM, V28, P2343, DOI 10.1111/j.1467-8659.2009.01541.x
   Banterle F, 2009, COMPUT GRAPH FORUM, V28, P13, DOI 10.1111/j.1467-8659.2008.01176.x
   Chang KH, 2012, EUR J OPER RES, V220, P684, DOI 10.1016/j.ejor.2012.02.028
   Chen PH, 2010, COLOR IMAG CONF, P42
   CIE, 1998, TECHNICAL REPORT, V131
   Daly Scott, 2013, P INT SOC OPT PHOT I
   Deb P, 1996, ECON LETT, V51, P123, DOI 10.1016/0165-1765(95)00784-9
   Fairchild M.D., 2005, Color Appearance Models, V2nd
   Fairchild MD, 2007, FIFTEENTH COLOR IMAGING CONFERENCE: COLOR SCIENCE AND ENGINEERING SYSTEMS, TECHNOLOGIES, AND APPLICATIONS, FINAL PROGRAM AND PROCEEDINGS, P233
   Fairchild MD, 2010, COLOR IMAG CONF, P322
   Fairchild MD, 2004, J ELECTRON IMAGING, V13, P126, DOI 10.1117/1.1635368
   Fairchild MD, 2001, COLOR RES APPL, V26, P418, DOI 10.1002/col.1061
   Gilchrist A., 2006, Seeing black and white
   Heidrich Wolfgang, ERIK REINHARD
   Hore Alain, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P2366, DOI 10.1109/ICPR.2010.579
   Hunt R W G, 1997, P CIE EXP S SCOTTSD
   HUNT RWG, 1952, J OPT SOC AM, V42, P190, DOI 10.1364/JOSA.42.000190
   JARQUE CM, 1987, INT STAT REV, V55, P163, DOI 10.2307/1403192
   Jewell G, 2000, NEUROPSYCHOLOGIA, V38, P93, DOI 10.1016/S0028-3932(99)00045-7
   Kim M., 2010, THESIS
   Kim MH, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531333
   Kozamernik F, 2005, SMPTE MOTION IMAG J, V114, P152, DOI 10.5594/J11535
   Kuang JT, 2007, J VIS COMMUN IMAGE R, V18, P406, DOI 10.1016/j.jvcir.2007.06.003
   Kuang XT, 2007, FIFTEENTH COLOR IMAGING CONFERENCE: COLOR SCIENCE AND ENGINEERING SYSTEMS, TECHNOLOGIES, AND APPLICATIONS, FINAL PROGRAM AND PROCEEDINGS, P249
   LUO MR, 1991, COLOR RES APPL, V16, P166, DOI 10.1002/col.5080160307
   Maloney Laurence T, 2003, J VISION, V3, P5, DOI DOI 10.1167/3.8.5
   Mantiuk R, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964935
   MILNER AD, 1992, NEUROPSYCHOLOGIA, V30, P515, DOI 10.1016/0028-3932(92)90055-Q
   Moroney N., 2002, P COL IM C, P23
   Munsell AEO, 1933, J OPT SOC AM, V23, P394, DOI 10.1364/JOSA.23.000394
   NAYATANI Y, 1990, COLOR RES APPL, V15, P210, DOI 10.1002/col.5080150407
   OSA, 1973, PSYCH CONC PERC AFF, P145
   Pouli T., 2013, IMAGE STAT VISUAL CO
   Radonjic A, 2011, CURR BIOL, V21, P1931, DOI 10.1016/j.cub.2011.10.013
   Reinhard E, 2002, ACM T GRAPHIC, V21, P267, DOI 10.1145/566570.566575
   Reinhard E, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366220
   Rempel AG, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239490
   Sharma G, 2003, EL EN AP SI, P1
   Sugawara M, 2014, IEEE SIGNAL PROC MAG, V31, P170, DOI 10.1109/MSP.2014.2302331
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wiebel CB, 2015, VISION RES, V115, P175, DOI 10.1016/j.visres.2015.04.010
   Wyszecki G., 1982, Color science: Concepts and methods, quantitative data and formulae
   Zhang X., 1997, Journal of the Society for Information Display, V5, P61, DOI 10.1889/1.1985127
   Zhang XM, 1997, COMPCON IEEE, P44, DOI 10.1109/CMPCON.1997.584669
NR 52
TC 3
Z9 5
U1 0
U2 2
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD NOV
PY 2017
VL 15
IS 1
AR 1
DI 10.1145/3086577
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FU0CU
UT WOS:000423519800001
DA 2024-07-18
ER

PT J
AU Li, YN
   Zhang, K
   Li, DJ
AF Li, Yi-Na
   Zhang, Kang
   Li, Dong-Jin
TI How Dimensional and Semantic Attributes of Visual Sign Influence
   Relative Value Estimation
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Information visualization; relative value estimation; perceptual bias;
   dimensional information; visual cue
ID VIVIDNESS; VOLUME; INFORMATION; PERCEPTION; JUDGMENT
AB High-quality decision making requires accurate estimation of relative values. The perceptual bias when estimating relative values displayed by a visual sign may weaken the accuracy and cause misjudgment. This research explores the heuristic estimation of relative values using visual cues, namely linear, areal, and volumetric information. We conduct experiments to empirically test the influences of dimensional information on perceptual biases. First, we investigate the conspicuity of areal information. Our experiments indicate that the responses of participants instructed to estimate rates defined by either linear or volumetric information are biased by the corresponding rates determined by areal information. Second, visual cues implying three-dimensional information (e.g., depth) can lead to overestimation. Third, we probe the influence of vividness as the boundary condition on relative value estimation. Empirical evidence on perceptual bias sheds light on the pragmatics of visual signs, helps suggest guidelines for visual persuasions, and improves decision-making quality.
C1 [Li, Yi-Na; Li, Dong-Jin] Nankai Univ, Business Sch, Baidi Rd, Tianjin 300071, Peoples R China.
   [Zhang, Kang] Univ Texas Dallas, Dept Comp Sci, Richardson, TX 75080 USA.
   [Li, Yi-Na] Texas A&M Univ, Mays Business Sch, College Stn, TX 77843 USA.
   [Zhang, Kang] Macau Univ Sci & Technol, Fac Informat Technol, Macau, Peoples R China.
C3 Nankai University; University of Texas System; University of Texas
   Dallas; Texas A&M University System; Texas A&M University College
   Station; Mays Business School; Macau University of Science & Technology
RP Li, YN (corresponding author), Nankai Univ, Business Sch, Baidi Rd, Tianjin 300071, Peoples R China.; Li, YN (corresponding author), Texas A&M Univ, Mays Business Sch, College Stn, TX 77843 USA.
EM yina@nankai.edu.cn; kzhang@utdallas.edu; djli1280@163.com
RI Li, Yi-Na/JGB-4336-2023
OI Li, Yi-Na/0000-0002-5479-5174
FU China Postdoctoral Science Foundation [2014M561178]; International
   Postdoctoral Exchange Fellowship [20150046]; National Nature Science
   Foundation of China [71372099]
FX This work is supported by China Postdoctoral Science Foundation (No.
   2014M561178), International Postdoctoral Exchange Fellowship (No.
   20150046), and National Nature Science Foundation of China (No.
   71372099). We thank Haipeng (Allan) Chen of Texas A&M University and
   three anonymous reviewers of VINCI'2016 and ACM TAP for their insightful
   comments and suggestions.
CR Baddeley A. D., 1998, ACM T APPL PERCEPTIO
   Baddeley AD, 2000, J EXP PSYCHOL GEN, V129, P126, DOI 10.1037//0096-3445.129.1.126
   BEEN RT, 1964, J ENG PSYCHOL, V3, P23
   Blondé J, 2016, SOC INFLUENCE, V11, P111, DOI 10.1080/15534510.2016.1157096
   Brewer C., 1998, Cartographic Perspectives, P6
   BURNS AC, 1993, J ADVERTISING, V22, P71, DOI 10.1080/00913367.1993.10673405
   Chandon P, 2009, J MARKETING RES, V46, P739, DOI 10.1509/jmkr.46.6.739
   CLEVELAND WS, 1987, J ROY STAT SOC A STA, V150, P192, DOI 10.2307/2981473
   Cox C.W., 1976, The American Cartographer, V3, P65
   DIXON P, 1978, MEM COGNITION, V6, P454, DOI 10.3758/BF03197479
   Flannery James John, 1971, Cartographica, V8, P96, DOI DOI 10.3138/J647-1776-745H-3667
   Folkes V, 2004, J CONSUM RES, V31, P390, DOI 10.1086/422117
   Gebuis T, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0037426
   Hayes A. F., 2013, Introduction to mediation, moderation, and conditional process analysis: a regression -based approach
   Hayes AF, 2009, BEHAV RES METHODS, V41, P924, DOI 10.3758/BRM.41.3.924
   Holmberg L., 1975, Psychological Research Bulletin, V15, P1
   House DH, 2006, IEEE T VIS COMPUT GR, V12, P509, DOI 10.1109/TVCG.2006.58
   HOUTHAKKER HS, 1957, ECONOMETRICA, V25, P532, DOI 10.2307/1905382
   Hsee CK, 2005, CURR DIR PSYCHOL SCI, V14, P234, DOI 10.1111/j.0963-7214.2005.00371.x
   Hsee CK, 2004, J PERS SOC PSYCHOL, V86, P680, DOI 10.1037/0022-3514.86.5.680
   Hsee CK, 1996, ORGAN BEHAV HUM DEC, V67, P247, DOI 10.1006/obhd.1996.0077
   Huff D., 1954, How to Lie with Statistics
   Jacques Bertin, 1983, SEMIOLOGY GRAPHICS D
   Johnson P.O., 1950, Psychometrika, V15, P349, DOI [10.1007/BF02288864, DOI 10.1007/BF02288864]
   Keller PA, 1997, J CONSUM RES, V24, P295, DOI 10.1086/209511
   Krider RE, 2001, MARKET SCI, V20, P405, DOI 10.1287/mksc.20.4.405.9756
   Krishna A, 1997, MEM COGNITION, V25, P492, DOI 10.3758/BF03201125
   Li Y. N., 2016, P 9 INT S VIS INF CO, P83
   Li YN, 2015, 8TH INTERNATIONAL SYMPOSIUM ON VISUAL INFORMATION COMMUNICATION AND INTERACTION (VINCI 2015), P121, DOI 10.1145/2801040.2801062
   MacEachern A., 1995, HOW MAPS WORK
   Messaris P., 1996, VISUAL PERSUASION RO
   Nisbett R., 1980, Human inference: Strategies and shortcomings of social judgment, DOI DOI 10.2307/2184495
   Ozimec AM, 2010, J MARKETING, V74, P94, DOI 10.1509/jmkg.74.6.94
   Pandey AV, 2014, IEEE T VIS COMPUT GR, V20, P2211, DOI 10.1109/TVCG.2014.2346419
   Pandey AV., 2015, P 33 ANN ACM C HUMAN, DOI [DOI 10.1145/2702123.2702608, 10.1145/2702123.2702608]
   PEARSON R G, 1964, Percept Mot Skills, V18, P889
   Raghubir P, 1999, J MARKETING RES, V36, P313, DOI 10.2307/3152079
   TAYLOR SE, 1982, PSYCHOL REV, V89, P155, DOI 10.1037/0033-295X.89.2.155
   TEGHTSOONIAN M, 1965, AM J PSYCHOL, V78, P392, DOI 10.2307/1420573
   Tory M, 2004, IEEE T VIS COMPUT GR, V10, P72, DOI 10.1109/TVCG.2004.1260759
   Tufte E. R., 2006, BEAUTIFUL EVIDENCE, P152
   Tufte E. R., 1985, TLS-TIMES LIT SUPPL, V7
   Zhang MJ, 2016, PROCEEDINGS VRCAI 2016: 15TH ACM SIGGRAPH CONFERENCE ON VIRTUAL-REALITY CONTINUUM AND ITS APPLICATIONS IN INDUSTRY, P381, DOI 10.1145/3013971.3013983
NR 43
TC 1
Z9 2
U1 2
U2 14
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD JUL
PY 2017
VL 14
IS 3
AR 18
DI 10.1145/3059006
PG 20
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA FB9AL
UT WOS:000406431900005
DA 2024-07-18
ER

PT J
AU Schuwerk, C
   Xu, X
   Chaudhari, R
   Steinbach, E
AF Schuwerk, Clemens
   Xu, Xiao
   Chaudhari, Rahul
   Steinbach, Eckehard
TI Compensating the Effect of Communication Delay in Client-Server Based
   Shared Haptic Virtual Environments
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Algorithms; Shared haptic virtual environment; multi-user;
   collaboration; haptic rendering; communication delay; perceived
   transparency
ID STABILITY; COLLABORATION; ARCHITECTURES; TRANSPARENCY
AB Shared haptic virtual environments can be realized using a client-server architecture. In this architecture, each client maintains a local copy of the virtual environment (VE). A centralized physics simulation running on a server calculates the object states based on haptic device position information received from the clients. The object states are sent back to the clients to update the local copies of the VE, which are used to render interaction forces displayed to the user through a haptic device. Communication delay leads to delayed object state updates and increased force feedback rendered at the clients. In this article, we analyze the effect of communication delay on the magnitude of the rendered forces at the clients for cooperative multi-user interactions with rigid objects. The analysis reveals guidelines on the tolerable communication delay. If this delay is exceeded, the increased force magnitude becomes haptically perceivable. We propose an adaptive force rendering scheme to compensate for this effect, which dynamically changes the stiffness used in the force rendering at the clients. Our experimental results, including a subjective user study, verify the applicability of the analysis and the proposed scheme to compensate the effect of time-varying communication delay in a multi-user SHVE.
C1 [Schuwerk, Clemens; Xu, Xiao; Chaudhari, Rahul; Steinbach, Eckehard] Tech Univ Munich, Chair Media Technol, Arcisstr 21, D-80333 Munich, Germany.
C3 Technical University of Munich
RP Schuwerk, C (corresponding author), Tech Univ Munich, Chair Media Technol, Arcisstr 21, D-80333 Munich, Germany.
EM clemens.schuwerk@tum.de; xiao.xu@tum.de; rahul.chaudhari@tum.de;
   eckehard.steinbach@tum.de
OI Steinbach, Eckehard/0000-0001-8853-2703; xu, xiao/0000-0002-4375-3884
FU European Research Council under the European Union's Seventh Framework
   Programme (FP7)/ERC Grant [258941]
FX This work has been supported by the European Research Council under the
   European Union's Seventh Framework Programme (FP7/2007-2013)/ERC Grant
   agreement no. 258941.
CR Adams RJ, 1999, IEEE T ROBOTIC AUTOM, V15, P465, DOI 10.1109/70.768179
   [Anonymous], 1997, PSYCHOPHYSICS FUNDAM
   [Anonymous], 1991, Detection theory: A user's guide
   Armbrust M, 2010, COMMUN ACM, V53, P50, DOI 10.1145/1721654.1721672
   Basdogan C., 2000, ACM Transactions on Computer-Human Interaction, V7, P443, DOI 10.1145/365058.365082
   Bovy C. J., 2002, P PASS ACT MEAS WORK, P1
   Buttolo P, 1997, COMPUT GRAPH-UK, V21, P421, DOI 10.1016/S0097-8493(97)00019-8
   Cheong Joono, 2005, IEEE WORLD HAPT C
   Delaney D, 2006, PRESENCE-TELEOP VIRT, V15, P465, DOI 10.1162/pres.15.4.465
   Dellaney D, 2006, PRESENCE-VIRTUAL AUG, V15, P218, DOI 10.1162/pres.2006.15.2.218
   El Saddik A, 2007, IEEE INSTRU MEAS MAG, V10, P10, DOI 10.1109/MIM.2007.339540
   Ernst MO, 2004, TRENDS COGN SCI, V8, P162, DOI 10.1016/j.tics.2004.02.002
   Fotoohi M, 2007, INT J ROBOT RES, V26, P977, DOI 10.1177/0278364907082049
   Fujimoto M, 2004, 8TH WORLD MULTI-CONFERENCE ON SYSTEMICS, CYBERNETICS AND INFORMATICS, VOL III, PROCEEDINGS, P30
   Gil JJ, 2004, IEEE T CONTR SYST T, V12, P583, DOI 10.1109/TCST.2004.825134
   Glencross M, 2007, IEEE VIRTUAL REALITY 2007, PROCEEDINGS, P115
   Hannaford B, 2002, IEEE T ROBOTIC AUTOM, V18, P1, DOI 10.1109/70.988969
   Hikichi K, 2002, GLOB TELECOMM CONF, P1492
   Hikichi K., 2001, P IEEE INT C MULT EX, P744
   Hirche S, 2012, P IEEE, V100, P623, DOI 10.1109/JPROC.2011.2175150
   Huang K, 2013, IEEE T ROBOT, V29, P417, DOI 10.1109/TRO.2012.2229672
   Iglesias R, 2008, MULTIMEDIA SYST, V13, P263, DOI 10.1007/s00530-007-0108-7
   Ishibashi Y, 2002, IEEE WORKSHOP ON KNOWLEDGE MEDIA NETWORKING, PROCEEDINGS, P11, DOI 10.1109/KMN.2002.1115156
   ISHIBASHI Y, 2005, P IEEE INT C MULT EX
   Jiménez P, 2001, COMPUT GRAPH-UK, V25, P269, DOI 10.1016/S0097-8493(00)00130-8
   Jones L., 2000, Human and Machine Haptics, P1, DOI DOI 10.1109/TMMS.1970.299971
   Kammerl J, 2010, PRESENCE-TELEOP VIRT, V19, P450, DOI 10.1162/pres_a_00008
   Kaufman DM, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409117
   Kim J, 2004, PRESENCE-TELEOP VIRT, V13, P328, DOI 10.1162/1054746041422370
   LAWRENCE DA, 1993, IEEE T ROBOTIC AUTOM, V9, P624, DOI 10.1109/70.258054
   Lee S, 2009, COMPUT COMMUN, V32, P992, DOI 10.1016/j.comcom.2008.12.028
   Liang YJ, 2003, IEEE T MULTIMEDIA, V5, P532, DOI 10.1109/TMM.2003.819095
   Marsh J, 2006, IEEE T VIS COMPUT GR, V12, P405, DOI 10.1109/TVCG.2006.40
   Matsumotoy S., 2000, P 5 PHANTOM US GROUP
   Niemeyer G, 2004, INT J ROBOT RES, V23, P873, DOI 10.1177/0278364904045563
   RAJU GJ, 1989, PROCEEDINGS - 1989 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION, VOL 1-3, P1316, DOI 10.1109/ROBOT.1989.100162
   Robles-De-La-Torre G, 2006, IEEE MULTIMEDIA, V13, P24, DOI 10.1109/MMUL.2006.69
   Ruspini D. C., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P345, DOI 10.1145/258734.258878
   Sankaranarayanan G, 2008, SYMPOSIUM ON HAPTICS INTERFACES FOR VIRTUAL ENVIRONMENT AND TELEOPERATOR SYSTEMS 2008, PROCEEDINGS, P259
   Schuwerk C, 2014, PRESENCE-TELEOP VIRT, V23, P320, DOI 10.1162/PRES_a_00196
   Schuwerk Clemens, 2014, P IEEE HAPT S HAPTIC
   Souayed R., 2004, EUROHAPICS, P260
   Srinivasan MA, 1997, COMPUT GRAPH-UK, V21, P393, DOI 10.1016/S0097-8493(97)00030-7
   Steinbach E, 2012, P IEEE, V100, P937, DOI 10.1109/JPROC.2011.2182100
   Suzuki S, 2014, 2014 IEEE 3RD GLOBAL CONFERENCE ON CONSUMER ELECTRONICS (GCCE), P669, DOI 10.1109/GCCE.2014.7031184
   TINTA SP, 2009, P IEEE S COMP COMM, P321
   Xu Xiao, 2015, P 6 IEEE INT WORKSH
   YOKOKOHJI Y, 1994, IEEE T ROBOTIC AUTOM, V10, P605, DOI 10.1109/70.326566
   Zadeh MH, 2008, MULTIMEDIA SYST, V13, P275, DOI 10.1007/s00530-007-0106-9
NR 49
TC 5
Z9 6
U1 0
U2 2
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD DEC
PY 2015
VL 13
IS 1
AR 5
DI 10.1145/2835176
PG 22
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DD2SV
UT WOS:000369773400005
DA 2024-07-18
ER

PT J
AU Pflüger, H
   Höferlin, B
   Raschke, M
   Ertl, T
AF Pflueger, Hermann
   Hoeferlin, Benjamin
   Raschke, Michael
   Ertl, Thomas
TI Simulating Fixations When Looking at Visual Arts
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Algorithms; Experimentation; Human Factors; Visual attention; eye
   movement; perception
ID EYE; ATTENTION; MODEL; SALIENCY; VIDEO
AB When people look at pictures, they fixate on specific areas. The sequences of such fixations are so characteristic for certain pictures that metrics can be derived that allow successful grouping of similar pieces of visual art. However, determining enough fixation sequences by eye tracking is not practically feasible for large groups of people and pictures. In order to get around this limitation, we present a novel algorithm that simulates eye movements by calculating scan paths for images and time frames in real time. The basis of our algorithm is an attention model that combines and optimizes rectangle features with Adaboost. The model is adapted to the characteristics of the retina, and its input is dependent on a few earlier fixations. This method results in significant improvements compared to previous approaches. Our simulation process delivers the same data structures as an eye tracker, thus can be analyzed by standard eye-tracking software. A comparison with recorded data from eye tracking experiments shows that our algorithm for simulating fixations has a very good prediction quality for the stimulus areas on which many subjects focus. We also compare the results with those from earlier works. Finally, we demonstrate how the presented algorithm can be used to calculate the similarity of pictures in terms of human perception.
C1 [Pflueger, Hermann; Hoeferlin, Benjamin; Raschke, Michael; Ertl, Thomas] Univ Stuttgart, Inst Visualizat & Interact Syst VIS, D-70569 Stuttgart, Germany.
C3 University of Stuttgart
RP Pflüger, H (corresponding author), Univ Stuttgart, Inst Visualizat & Interact Syst VIS, Univ Str 38, D-70569 Stuttgart, Germany.
EM Hermann.Pflueger@vis.uni-stuttgart.de;
   Benjamin.Hoeferlin@vis.uni-stuttgart.de;
   Michael.Raschke@vis.uni-stuttgart.de; thomas.ertl@vis.uni-stuttgart.de
OI Pfluger, Hermann/0000-0003-1173-8716
CR Alaa Mohamed, 2011, THESIS CALIFORNIA I
   Davis JW, 2007, MACH VISION APPL, V18, P41, DOI 10.1007/s00138-006-0047-x
   Hoferlin Benjamin, 2012, Proceedings of the 1st International Conference on Pattern Recognition Applications and Methods. ICPRAM 2012, P25
   HOFFMAN JE, 1995, PERCEPT PSYCHOPHYS, V57, P787, DOI 10.3758/BF03206794
   Holmqvist K., 2010, Eye Tracking: A Comprehensive Guide to Methods and Measures, V1
   Itti L, 2005, VIS COGN, V12, P1093, DOI 10.1080/13506280444000661
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Itti L, 2003, PROC SPIE, V5200, P64, DOI 10.1117/12.512618
   Itti Laurent, 2013, BOTTOM UP VISUAL ATT
   Jasso H, 2007, LECT NOTES ARTIF INT, V4840, P106
   Judd T, 2009, IEEE I CONF COMP VIS, P2106, DOI 10.1109/ICCV.2009.5459462
   Kienzle W, 2007, LECT NOTES COMPUT SC, V4713, P405
   Mikolajczyk K, 2005, INT J COMPUT VISION, V65, P43, DOI 10.1007/s11263-005-3848-x
   Nake F., 1974, Asthetik als Informationsverarbeitung: Grundlagen und Anwendungen der Informatik im Bereich asthetischer Produktion und Kritik
   Nataraju Sunaad, 2009, P 2009 INT C MOT VID, P134
   Niu YQ, 2012, ACM T APPL PERCEPT, V9, DOI 10.1145/2325722.2325726
   Peters R.J., 2007, P IEEE C COMPUTER VI, P1
   Privitera CM, 2000, IEEE T PATTERN ANAL, V22, P970, DOI 10.1109/34.877520
   Rajashekar U, 2008, IEEE T IMAGE PROCESS, V17, P564, DOI 10.1109/TIP.2008.917218
   Rodieck R. W., 1998, The First Steps in Seeing
   SAARINEN J, 1993, VISION RES, V33, P1113, DOI 10.1016/0042-6989(93)90244-Q
   Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb
   WOLFE JM, 1994, PSYCHON B REV, V1, P202, DOI 10.3758/BF03200774
   Zhao Qi., 2011, Information Sciences and Systems (CISS), 2011 45th Annual Conference on, P1
NR 24
TC 4
Z9 4
U1 0
U2 5
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD JUL
PY 2015
VL 12
IS 3
AR 9
DI 10.1145/2736286
PG 20
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Arts &amp; Humanities Citation Index (A&amp;HCI)
SC Computer Science
GA CP6OI
UT WOS:000360006600003
DA 2024-07-18
ER

PT J
AU Blom, KJ
   Beckhaus, S
AF Blom, Kristopher J.
   Beckhaus, Steffi
TI Virtual Travel Collisions: Response Method Influences Perceived Realism
   of Virtual Environments
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Human Factors; Virtual Collisions; Collision Feedback; Realism;
   Presence; Virtual Environments; Virtual Travel
ID PERFORMANCE
AB Travel methods are the most basic and widespread interaction method with virtual environments. They are the primary and often the only way the user interactively experiences the environment. We present a study composed of three experiments that investigates how virtual collisions methods and feedback impact user perception of the realism of collisions and the virtual environment. A wand-based virtual travel method was used to navigate maze environments in an immersive projective system. The results indicated that the introduction of collision handling significantly improved the user's perception of the realism of the environment and collisions. An effect of feedback on the perceived level of realism of collisions and solidity of the environment was also found. Our results indicate that feedback should be context appropriate, e. g. fitting to a collision with the object; yet, the modality and richness of feedback were only important in that traditional color change feedback did not perform as well as audio or haptic feedback. In combination, the experiments indicated that in immersive virtual environments the stop collision handling method produced a more realistic impression than the slide method that is popular in games. In total, the study suggests that feedback fitting the collision context, coupled with the stop handling method, provides the best perceived realism of collisions and scene.
C1 [Blom, Kristopher J.] Univ Barcelona, Barcelona 08035, Spain.
   [Blom, Kristopher J.; Beckhaus, Steffi] Univ Hamburg, D-22527 Hamburg, Germany.
C3 University of Barcelona; University of Hamburg
RP Blom, KJ (corresponding author), Univ Barcelona, EVENT Lab, Fac Psicol, Passeig Vall dHebron 171, Barcelona 08035, Spain.
RI Beckhaus, Steffi/ABB-2565-2021
FU European Research Council project TRAVERSE [227985]; European Research
   Council (ERC) [227985] Funding Source: European Research Council (ERC)
FX The experiments were designed and performed at the University of
   Hamburg, Germany. Analysis of the results and writing was performed by
   K. J. Blom while at the University of Barcelona, Spain, where he was
   funded under the European Research Council project TRAVERSE (227985). We
   would like to thank Neele Stockler and Kim Reichert for their help in
   performing data entry and Sofia Osimo for proofreading the text.
CR Albrecht K., 2009, P INT C PRES PRES 09
   [Anonymous], 2008, P 2008 ACM S VIRTUAL, DOI DOI 10.1145/1450579.1450614
   [Anonymous], 1990, P C APPL STAT AGR US
   Blom KJ, 2010, IEEE SYMPOSIUM ON 3D USER INTERFACES (3DUI 2010), P35, DOI 10.1109/3DUI.2010.5444723
   Blomkvist K., 2012, INNOVATION GROWTH R, P57
   Bloomfield A, 2008, PRESENCE-TELEOP VIRT, V17, P103, DOI 10.1162/pres.17.2.103
   Burke J. L., 2006, P 8 INT C MULT INT, P108, DOI [10.1145/1180995.1181017, DOI 10.1145/1180995.1181017, DOI 10.1145/1180995]
   Davis ET, 1999, HUM FAC ERG SOC P, P1197
   de Barros P. G., 2011, Proceedings 2011 IEEE Symposium on 3D User Interfaces (3DUI 2011), P47, DOI 10.1109/3DUI.2011.5759216
   de Kort YAW, 2003, PRESENCE-TELEOP VIRT, V12, P360, DOI 10.1162/105474603322391604
   Fox J., 2002, Cox Proportional-Hazads Regression for Survival Data
   Herbst I, 2005, 2005 IEEE International Workshop on Haptic Audio Visual Environments and their Applications, P67
   Hess N., 2008, P 2008 ACM S VIRT RE, P39
   Hoffman HG, 1998, P IEEE VIRT REAL ANN, P59, DOI 10.1109/VRAIS.1998.658423
   Jacobson J, 1997, PROCEEDINGS OF THE HUMAN FACTORS AND ERGONOMICS SOCIETY 41ST ANNUAL MEETING, 1997, VOLS 1 AND 2, P1273
   Jiménez P, 2001, COMPUT GRAPH-UK, V25, P269, DOI 10.1016/S0097-8493(00)00130-8
   Lecuyer A., 2002, P IMM PROJ TECHN S I
   Mania K, 2010, ACM T APPL PERCEPT, V7, DOI 10.1145/1670671.1670673
   Mohler BJ, 2010, PRESENCE-TELEOP VIRT, V19, P230, DOI 10.1162/pres.19.3.230
   Pullen W. D., 2013, DAEDALUS 2 5 SOFTWAR
   Richard P., 2006, The International Journal of Virtual Reality, V2, P37
   Ruddle RA, 2001, ENGINEERING PSYCHOLOGY AND COGNITIVE ERGONOMICS VOLUME SIX, P135
   Ryu J., 2004, Proc. ACM Symp. Virtual Real. Softw. Technol. VRST 04, P89, DOI DOI 10.1145/1077534.1077551
   Slater M, 1998, HUM FACTORS, V40, P469, DOI 10.1518/001872098779591368
   Slater M, 2009, ANU PSICOL, V40, P193
   Slater M, 2009, IEEE COMPUT GRAPH, V29, P76, DOI 10.1109/MCG.2009.55
   Steinicke F, 2009, IEEE VIRTUAL REALITY 2009, PROCEEDINGS, P203, DOI 10.1109/VR.2009.4811024
   Suma EA, 2007, 3DUI: IEEE SYMPOSIUM ON 3D USER INTERFACES 2007, PROCEEDINGS, P147
   Tichon J., 2011, J HLTH SAFETY RES PR, V3, P33
   Vinayagamoorthy V., 2004, 7 ANN INT PRESENCE W, P148
   Visell Y, 2009, INT J HUM-COMPUT ST, V67, P947, DOI 10.1016/j.ijhcs.2009.07.007
   Visell Y, 2008, LECT NOTES COMPUT SC, V5024, P420, DOI 10.1007/978-3-540-69057-3_55
   Wallet G., 2009, J VIRTUAL REALITY BR, V6, P572
   Whitton M., 2005, P 11 INT C HUM COMP
   Witmer BG, 1998, PRESENCE-TELEOP VIRT, V7, P225, DOI 10.1162/105474698565686
   Wobbrock JO, 2011, 29TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P143, DOI 10.1145/1978942.1978963
   Yu I, 2012, IEEE COMPUT GRAPH, V32, P36, DOI 10.1109/MCG.2012.121
   Zimmons P, 2003, P IEEE VIRT REAL ANN, P293, DOI 10.1109/VR.2003.1191170
NR 38
TC 6
Z9 7
U1 1
U2 17
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD OCT
PY 2013
VL 10
IS 4
AR 25
DI 10.1145/2536764.2536772
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 281YK
UT WOS:000329136700008
DA 2024-07-18
ER

PT J
AU Kyto, M
   Mäkinen, A
   Häkkinen, J
   Oittinen, P
AF Kyto, Mikko
   Makinen, Aleksi
   Hakkinen, Jukka
   Oittinen, Pirkko
TI Improving Relative Depth Judgments in Augmented Reality with Auxiliary
   Augmentations
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Design; Experimentation; Human Factors; Performance; Depth perception;
   augmented reality; cue theory; stereoscopic viewing; X-ray visualization
ID DISTANCE PERCEPTION; LOCALIZATION; CONTOURS; COLOR; CUES; USER
AB Significant depth judgment errors are common in augmented reality. This study presents a visualization approach for improving relative depth judgments in augmented reality. The approach uses auxiliary augmented objects in addition to the main augmentation to support ordinal and interval depth judgment tasks. The auxiliary augmentations are positioned spatially near real-world objects, and the location of the main augmentation can be deduced based on the relative depth cues between the augmented objects. In the experimental part, the visualization approach was tested in the "X-ray" visualization case with a video see-through system. Two relative depth cues, in addition to motion parallax, were used between graphical objects: relative size and binocular disparity. The results show that the presence of auxiliary objects significantly reduced errors in depth judgment. Errors in judging the ordinal location with respect to a wall (front, at, or behind) and judging depth intervals were reduced. In addition to reduced errors, the presence of auxiliary augmentation increased the confidence in depth judgments, and it was subjectively preferred. The visualization approach did not have an effect on the viewing time.
C1 [Kyto, Mikko; Makinen, Aleksi; Hakkinen, Jukka; Oittinen, Pirkko] Aalto Univ, Sch Sci, Dept Media Technol, FI-00076 Aalto, Finland.
C3 Aalto University
RP Kyto, M (corresponding author), Aalto Univ, Sch Sci, Dept Media Technol, PL 15500, FI-00076 Aalto, Finland.
EM mikko.kyto@aalto.fi; aleksi.makinen@aalto.fi; jukka.hakkinen@aalto.fi;
   pirkko.oittinen@aalto.fi
RI Häkkinen, Jukka/A-4122-2019; Kytö, Mikko/AAM-7798-2020
OI Häkkinen, Jukka/0000-0003-0215-2238; Kyto, Mikko/0000-0002-4936-3502
CR [Anonymous], 2013, ACM T APPL PERCEPTIO, V10
   [Anonymous], 2005, ACM INT C PROCEEDING, DOI DOI 10.1145/1152399.1152412
   Avery B, 2008, INT SYM MIX AUGMENT, P69, DOI 10.1109/ISMAR.2008.4637327
   Avery B, 2009, IEEE VIRTUAL REALITY 2009, PROCEEDINGS, P79
   Bane R, 2004, ISMAR 2004: THIRD IEEE AND ACM INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY, P231, DOI 10.1109/ISMAR.2004.36
   Birkfellner W, 2002, IEEE T MED IMAGING, V21, P991, DOI 10.1109/TMI.2002.803099
   COLLETT TS, 1985, PROC R SOC SER B-BIO, V224, P43, DOI 10.1098/rspb.1985.0020
   Cutting JE, 1995, PERCEPTION SPACE MOT, P69, DOI [DOI 10.1016/B978-012240530-3/50005-5, 10.1016/B978-012240530-3/50005-5]
   De Silva V, 2011, IEEE T MULTIMEDIA, V13, P498, DOI 10.1109/TMM.2011.2129500
   Dey A., 2010, Proceedings of the 17th ACM Symposium on Virtual Reality Software and Technology (VRST), P211, DOI DOI 10.1145/1889863.1889911
   Drascic D, 1996, P SOC PHOTO-OPT INS, V2653, P123, DOI 10.1117/12.237425
   Drascic D., 1991, P SPIE STER DISPL AP, V1457, P623
   Ellis SR, 1998, HUM FACTORS, V40, P415, DOI 10.1518/001872098779591278
   ERKELENS CJ, 1985, VISION RES, V25, P583, DOI 10.1016/0042-6989(85)90164-6
   FAUBERT J, 2001, P SOC PHOTO-OPT INS, P168
   FEINER SK, 1995, PRESENCE-TELEOP VIRT, V4, P318, DOI 10.1162/pres.1995.4.3.318
   Foley J.M., 1991, PICTORIAL COMMUNICAT, V2nd
   Fukuda K, 2009, J VISION, V9, DOI 10.1167/9.2.1
   Furmanski C, 2002, INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY, PROCEEDINGS, P215, DOI 10.1109/ISMAR.2002.1115091
   GILINSKY AS, 1951, PSYCHOL REV, V58, P460, DOI 10.1037/h0061505
   Grechkin TY, 2010, ACM T APPL PERCEPT, V7, DOI 10.1145/1823738.1823744
   Hakkinen J, 1996, VISION RES, V36, P3815, DOI 10.1016/0042-6989(96)00099-5
   Howard HJ., 1919, T AM OPHTHALMOLOGICA, V17, P195
   Hubona G. S., 1999, ACM Transactions on Computer-Human Interaction, V6, P214, DOI 10.1145/329693.329695
   JOHNSTON EB, 1991, VISION RES, V31, P1351, DOI 10.1016/0042-6989(91)90056-B
   Jurgens V., 2006, Proceedings of the 7th ACM SIGCHI New Zealand chapter's international conference on Computer-human interaction: design centered HCI, P117
   Kanizsa G., 1979, Organization in Vision: Essays on Gestalt Perception
   Kruijff Ernst, 2010, 2010 9th IEEE International Symposium on Mixed and Augmented Reality (ISMAR). Science & Technology Papers, P3, DOI 10.1109/ISMAR.2010.5643530
   Kyto M., 2011, Proceedings of the 2011 8th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS 2011), P49, DOI 10.1109/AVSS.2011.6027293
   Kytö M, 2011, PROC SPIE, V7864, DOI 10.1117/12.872015
   Lambooij M, 2009, J IMAGING SCI TECHN, V53, DOI 10.2352/J.ImagingSci.Technol.2009.53.3.030201
   LANDY MS, 1995, VISION RES, V35, P389, DOI 10.1016/0042-6989(94)00176-M
   Liu Y, 2008, J VISION, V8, DOI 10.1167/8.11.19
   Livingston MA, 2003, SECOND IEEE AND ACM INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY, PROCEEDINGS, P56, DOI 10.1109/ISMAR.2003.1240688
   Livingston MA, 2009, INT SYM MIX AUGMENT, P53, DOI 10.1109/ISMAR.2009.5336496
   LOOMIS JM, 1992, J EXP PSYCHOL HUMAN, V18, P906, DOI 10.1037/0096-1523.18.4.906
   McCandless JW, 2000, PRESENCE-TELEOP VIRT, V9, P15, DOI 10.1162/105474600566583
   Mckee SP, 2010, J VISION, V10, DOI 10.1167/10.10.5
   Nagata S., 1991, PICTORIAL COMMUNICAT, P527
   Nakayama K, 1996, P NATL ACAD SCI USA, V93, P634, DOI 10.1073/pnas.93.2.634
   NAKAYAMA K, 1989, PERCEPTION, V18, P55, DOI 10.1068/p180055
   Napieralski PE, 2011, ACM T APPL PERCEPT, V8, DOI 10.1145/2010325.2010328
   Ooi TL, 2001, NATURE, V414, P197, DOI 10.1038/35102562
   OSHEA RP, 1994, VISION RES, V34, P1595, DOI 10.1016/0042-6989(94)90116-3
   Palmisano S, 2010, J VISION, V10, DOI 10.1167/10.6.19
   Peterson S, 2008, IEEE VIRTUAL REALITY 2008, PROCEEDINGS, P169
   Peterson SD, 2009, COMPUT GRAPH-UK, V33, P23, DOI 10.1016/j.cag.2008.11.006
   Philbeck JW, 1997, J EXP PSYCHOL HUMAN, V23, P72, DOI 10.1037/0096-1523.23.1.72
   Polys NF, 2011, INT J HUM-COMPUT ST, V69, P30, DOI 10.1016/j.ijhcs.2010.05.007
   RAMACHANDRAN VS, 1985, NATURE, V317, P527, DOI 10.1038/317527a0
   Reinhart William F., 1991, Proceedings of the SPIE - The International Society for Optical Engineering, V1457, P221, DOI 10.1117/12.46310
   REINHART WF, 1990, P SOC PHOTO-OPT INS, V1256, P12, DOI 10.1117/12.19884
   ROGERS BJ, 1989, Q J EXP PSYCHOL-A, V41, P697, DOI 10.1080/14640748908402390
   Rolland JP, 2000, PRESENCE-VIRTUAL AUG, V9, P287, DOI 10.1162/105474600566808
   Sandor C., 2010, 2010 9th IEEE International Symposium on Mixed and Augmented Reality (ISMAR). Science & Technology Papers, P27, DOI 10.1109/ISMAR.2010.5643547
   Shibata T, 2011, J VISION, V11, DOI 10.1167/11.8.11
   Sielhorst T, 2006, LECT NOTES COMPUT SC, V4190, P364
   Singh Gurjot., 2010, Proceedings of the 7th Symposium on Applied Perception in Graphics and Visualization, P149, DOI DOI 10.1145/1836248.1836277
   Sugano N, 2003, SECOND IEEE AND ACM INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY, PROCEEDINGS, P76, DOI 10.1109/ISMAR.2003.1240690
   SUNDET JM, 1978, SCAND J PSYCHOL, V19, P133, DOI 10.1111/j.1467-9450.1978.tb00313.x
   SUZUKI S, 1985, COMPUT VISION GRAPH, V30, P32, DOI 10.1016/0734-189X(85)90016-7
   Swan JE, 2006, P IEEE VIRT REAL ANN, P19, DOI 10.1109/VR.2006.13
   TAKEICHI H, 1992, PERCEPTION, V21, P177, DOI 10.1068/p210177
   Ware C., 2020, INFORM VISUALIZATION
   Willemsen P, 2008, PRESENCE-TELEOP VIRT, V17, P91, DOI 10.1162/pres.17.1.91
   Wither J, 2005, NINTH IEEE INTERNATIONAL SYMPOSIUM ON WEARABLE COMPUTERS, PROCEEDINGS, P92, DOI 10.1109/ISWC.2005.41
   YEH YY, 1990, HUM FACTORS, V32, P45, DOI 10.1177/001872089003200104
   Zhai S., 1996, ACM T COMPUT-HUM INT, V3, P254, DOI [DOI 10.1145/234526.234532, 10.1145/234526.234532]
NR 68
TC 14
Z9 14
U1 0
U2 15
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD FEB
PY 2013
VL 10
IS 1
AR 6
DI 10.1145/2422105.2422111
PG 21
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AN8YO
UT WOS:000340892200006
DA 2024-07-18
ER

PT J
AU Bernhard, M
   Grosse, K
   Wimmer, M
AF Bernhard, Matthias
   Grosse, Karl
   Wimmer, Michael
TI Bimodal Task-Facilitation in a Virtual Traffic Scenario through
   Spatialized Sound Rendering
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Experimentation; Human Factors; Audio-visual perception; bimodal;
   spatialized audio rendering; task facilitation; pedestrian simulator;
   pedestrian safety; virtual environments; human-computer interaction
ID REALITY; INFORMATION; INTEGRATION; INTERFACES; SIMULATION; TIME
AB Audio rendering is generally used to increase the realism of virtual environments (VE). In addition, audio rendering may also improve the performance in specific tasks carried out in interactive applications such as games or simulators.
   In this article we investigate the effect of the quality of sound rendering on task performance in a task which is inherently vision-dominated. The task is a virtual traffic gap-crossing scenario with two elements: first, to discriminate crossable and uncrossable gaps in oncoming traffic, and second, to find the right timing to start crossing the street without an accident. A study was carried out with 48 participants in an immersive virtual environment setup with a large screen and headphones. Participants were grouped into three different scenarios. In the first one, spatialized audio rendering with head-related transfer function (HRTF) filtering was used. The second group was tested with conventional stereo rendering, and the remaining group ran the experiment in a mute condition. Our results give a clear evidence that spatialized audio improves task performance compared to the unimodal mute condition. Since all task-relevant information was in the participants' field-of-view, we conclude that an enhancement of task performance results from a bimodal advantage due to the integration of visual and auditory spatial cues.
C1 [Bernhard, Matthias; Grosse, Karl; Wimmer, Michael] Vienna Univ Technol, Vienna, Austria.
C3 Technische Universitat Wien
RP Bernhard, M (corresponding author), Vienna Univ Technol, Vienna, Austria.
EM matthias.bernhard@cg.tuwien.ac.at
OI Wimmer, Michael/0000-0002-9370-2663
FU Austrian Science Fund (FWF) [P21130-N13]; Austrian Science Fund (FWF)
   [P21130] Funding Source: Austrian Science Fund (FWF)
FX This work was supported by the Austrian Science Fund (FWF) contract
   P21130-N13.
CR Alais D, 2004, CURR BIOL, V14, P257, DOI 10.1016/j.cub.2004.01.029
   Alais D, 2006, P R SOC B, V273, P1339, DOI 10.1098/rspb.2005.3420
   [Anonymous], LISTEN HRTF database
   BARNECUTT P, 1995, CURRENT PSYCH, V17, P93
   Barreto AB, 2007, COMPUT HUM BEHAV, V23, P1211, DOI 10.1016/j.chb.2004.12.001
   BART O, 2008, OTJR OCCUP PARTICIPA, V28
   Barton BK, 2007, J PEDIATR PSYCHOL, V32, P475, DOI 10.1093/jpepsy/jsl028
   Blauert J., 1999, Spatial Hearing: The Psychophysics of Human Sound Localization
   BONNEEL N, 2010, ACM T APPL IN PRESS
   Bormann K, 2005, PRESENCE-TELEOP VIRT, V14, P278, DOI 10.1162/105474605323384645
   Bormann K., 2006, VIRTUAL REAL-LONDON, V9, P226, DOI DOI 10.1007/S10055-006-0019-5
   Burstedde C, 2001, PHYSICA A, V295, P507, DOI 10.1016/S0378-4371(01)00141-8
   CAELLI T, 1980, HUM FACTORS, V22, P719, DOI 10.1177/001872088002200607
   CARRE JR, 2005, NEW METHOD ANAL PEDE
   CAVALLO V, 2009, P 5 INT DRIV S HUM F
   Clancy TA, 2006, J CLIN CHILD ADOLESC, V35, P203, DOI 10.1207/s15374424jccp3502_4
   Connelly ML, 1998, ACCIDENT ANAL PREV, V30, P443, DOI 10.1016/S0001-4575(97)00109-7
   Davis ET, 1999, HUM FAC ERG SOC P, P1197
   DELL W, 2000, USE 3D AUDIO IMPROVE
   Ernst MO, 2002, NATURE, V415, P429, DOI 10.1038/415429a
   Ernst MO, 2004, TRENDS COGN SCI, V8, P162, DOI 10.1016/j.tics.2004.02.002
   FRUIN J.J., 1971, Metropolitan Association of Urban Designers and Environmental Planners
   Hendrix C., 1995, Proceedings. Virtual Reality Annual International Symposium '95 (Cat. No.95CH35761), P74, DOI 10.1109/VRAIS.1995.512482
   HOLM S, 1979, SCAND J STAT, V6, P65
   Jiang W, 2002, J COGNITIVE NEUROSCI, V14, P1240, DOI 10.1162/089892902760807230
   KINCHLA RA, 1974, PERCEPT PSYCHOPHYS, V15, P149, DOI 10.3758/BF03205843
   KING AJ, 1985, EXP BRAIN RES, V60, P492
   Laurienti PJ, 2004, EXP BRAIN RES, V158, P405, DOI 10.1007/s00221-004-1913-2
   MASTOROPOULOU G, 2005, P 3 INT C COMP GRAPH
   McComas J, 2002, CYBERPSYCHOL BEHAV, V5, P185, DOI 10.1089/109493102760147150
   Michon P. E., 2001, Spatial Information Theory. Foundations of Geographic Information Science. International Conference, COSIT 2001. Proceedings (Lecture Notes in Computer Science Vol.2205), P292
   MILLER J, 1982, COGNITIVE PSYCHOL, V14, P247, DOI 10.1016/0010-0285(82)90010-X
   MOECK T, 2007, P S INT 3D GRAPH GAM, P189
   Naveh Y., 2000, P 3 INT C DISABILITY, P243
   NGUYEN KV, 2009, J VIRTUAL REALITY BR, V6, P5
   *OGRE3D, OGR 3D OBJ OR GRAPH
   Oxley Jennifer, 2008, Ann Adv Automot Med, V52, P215
   Plumert JM, 2007, CURR DIR PSYCHOL SCI, V16, P255, DOI 10.1111/j.1467-8721.2007.00515.x
   RAAB DH, 1962, T NEW YORK ACAD SCI, V24, P574, DOI 10.1111/j.2164-0947.1962.tb01433.x
   Riecke BE, 2009, ACM T APPL PERCEPT, V6, DOI 10.1145/1498700.1498701
   SCHIFF W, 1990, J EXP PSYCHOL HUMAN, V16, P303, DOI 10.1037/0096-1523.16.2.303
   Schwebel DC, 2008, ACCIDENT ANAL PREV, V40, P1394, DOI 10.1016/j.aap.2008.03.005
   Schwebel DC, 2010, INJURY PREV, V16, pE1, DOI 10.1136/ip.2009.025288
   Seward AE, 2007, ACM T APPL PERCEPT, V4, DOI 10.1145/1278387.1278392
   SIDAWAY B, 1990, HUM FACTORS, V38, P101
   Simpson G, 2003, ACCIDENT ANAL PREV, V35, P787, DOI 10.1016/S0001-4575(02)00081-7
   Sodnik J., 2006, P 18 AUSTR C COMPUTE, P111, DOI DOI 10.1145/1228175.1228197
   Sodnik J, 2008, INT J HUM-COMPUT ST, V66, P318, DOI 10.1016/j.ijhcs.2007.11.001
   Stein Barry E., 1993, The Merging of the Senses. The Merging of the Senses. Cognitive Neuroscience
   Storms RL, 2000, PRESENCE-TELEOP VIRT, V9, P557, DOI 10.1162/105474600300040385
   Suied C, 2009, EXP BRAIN RES, V194, P91, DOI 10.1007/s00221-008-1672-6
   Sundareswaran V, 2003, SECOND IEEE AND ACM INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY, PROCEEDINGS, P296, DOI 10.1109/ISMAR.2003.1240728
   TAKAYUKI S, 2004, LECT NOTES COMPUTER, V3118, P476
   Tom A, 2004, APPL COGNITIVE PSYCH, V18, P1213, DOI 10.1002/acp.1045
   Treisman A.M., 1973, Attention and performance IV, P101
   TUNG YC, 2008, P 9 AS PAC IND ENG M, P6
   VALDE AT, 2005, ACCIDENT ANAL PREV, V37, P399
   Wan BH, 2004, TRANSPORT RES REC, P58
   Yang JG, 2006, TRANSPORT RES A-POL, V40, P280, DOI 10.1016/j.tra.2005.08.001
NR 59
TC 5
Z9 5
U1 0
U2 8
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD NOV
PY 2011
VL 8
IS 4
AR 24
DI 10.1145/2043603.2043606
PG 22
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA 856IQ
UT WOS:000297633400003
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Andersen, TH
   Zhai, SM
AF Andersen, Tue Haste
   Zhai, Shumin
TI "Writing with Music": Exploring the Use of Auditory Feedback in Gesture
   Interfaces
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Experimentation; Human Factors; Audio; auditory interface; sound; music;
   gesture; pen; text input; feedback
AB We investigate the use of auditory feedback in pen-gesture interfaces in a series of informal and formal experiments. Initial iterative exploration showed that gaining performance advantage with auditory feedback was possible using absolute cues and state feedback after the gesture was produced and recognized. However, gaining learning or performance advantage from auditory feedback tightly coupled with the pen-gesture articulation and recognition process was more difficult. To establish a systematic baseline, Experiment 1 formally evaluated gesture production accuracy as a function of auditory and visual feedback. Size of gestures and the aperture of the closed gestures were influenced by the visual or auditory feedback, while other measures such as shape distance and directional difference were not, supporting the theory that feedback is too slow to strongly influence the production of pen stroke gestures. Experiment 2 focused on the subjective aspects of auditory feedback in pen-gesture interfaces. Participants' rating on the dimensions of being wonderful and stimulating was significantly higher with musical auditory feedback. Several lessons regarding pen gestures and auditory feedback are drawn from our exploration: a few simple functions such as indicating the pen-gesture recognition results can be achieved, gaining performance and learning advantage through tightly coupled process-based auditory feedback is difficult, pen-gesture sets and their recognizers can be designed to minimize visual dependence, and people's subjective experience of gesture interaction can be influenced using musical auditory feedback. These lessons may serve as references and stepping stones toward future research and development in pen-gesture interfaces with auditory feedback.
C1 [Andersen, Tue Haste] Univ Copenhagen, DK-1168 Copenhagen, Denmark.
   [Zhai, Shumin] IBM Corp, Almaden Res Ctr, San Jose, CA USA.
C3 University of Copenhagen; International Business Machines (IBM)
RP Andersen, TH (corresponding author), Univ Copenhagen, DK-1168 Copenhagen, Denmark.
CR [Anonymous], 1988, Proceedings of the SIGCHI conference on Human factors in computing systems-CHI, DOI DOI 10.1145/57167.57203
   [Anonymous], 2001, IEEE Workshop on Applications of Signal Processing to Audio and Acoustics, DOI [DOI 10.1109/ASPAA.2001.969552, 10.1109/ASPAA.2001.969552]
   BLATTNER M, 1990, HUM-COMPUT INTERACT, V16, P523
   Blythe M., 2003, FUNOLOGY USABILITY U
   Brewster S., 2003, Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, CHI '03, ACM New York, NY, USA, P473, DOI DOI 10.1145/642611.642694
   BROADBENT DE, 1977, AM PSYCHOL, V32, P109, DOI 10.1037/0003-066X.32.2.109
   BUXTON W, 1995, READINGS HUMAN COMPU, P525
   Clarke E.F., 1999, PSYCHOL MUSIC, V2nd, P473, DOI [DOI 10.1016/B978-012213564-4/50014-7, 10.1016/b978-012213564-4/50014-7]
   Deatherage B.H., 1972, HUMAN ENG GUIDE EQUI, P123
   FLETCHER H, 1962, J ACOUST SOC AM, V34, P749, DOI 10.1121/1.1918192
   Gaver W. W., 1989, Human-Computer Interaction, V4, P67, DOI 10.1207/s15327051hci0401_3
   GHEZ C, 2000, P INT C AUD DISPL AC
   Jarvelainen Hanna., 2001, Acoustics Research Letters Online, V2, P79, DOI DOI 10.1121/1.1374756
   KRAMER G, 1994, SFI S SCI C, V18, P1
   Kristensson P., 2004, P 17 ANN ACM S USER, P43, DOI DOI 10.1145/1029632.1029640
   KURTENBACH G, 1994, HUMAN FACTORS IN COMPUTING SYSTEMS, CHI '94 CONFERENCE PROCEEDINGS - CELEBRATING INTERDEPENDENCE, P258, DOI 10.1145/191666.191759
   LACQUANITI F, 1983, ACTA PSYCHOL, V54, P115, DOI 10.1016/0001-6918(83)90027-6
   LEGGE DAVID, 1964, PERCEPT MOT SKILLS, V18, P549
   Loeb RG, 2002, ANESTH ANALG, V94, P362, DOI 10.1097/00000539-200202000-00025
   LUSCHEI E, 1967, EXP NEUROL, V18, P429, DOI 10.1016/0014-4886(67)90060-X
   MacKenzie IS, 2002, LECT NOTES COMPUT SC, V2411, P195
   MacMillan K., 2001, Proceedings of the International Computer Music Conference, P259
   Norman D.A., 2004, EMOTIONAL DESIGN WHY
   Olive T, 2002, INT J PSYCHOL, V37, P209, DOI 10.1080/00207590244000089
   PATTERSON R, 1989, P I AC SPRING C I AC, V2, P17
   Pirhonen A., 2002, P SIGCHI C HUM FACT, P291
   Sears A, 2002, INTERACT COMPUT, V14, P413, DOI 10.1016/S0953-5438(01)00060-1
   Shea CH, 2001, J MOTOR BEHAV, V33, P127, DOI 10.1080/00222890109603145
   SINGH PG, 1987, J ACOUST SOC AM, V82, P886, DOI 10.1121/1.395287
   SMYTH MM, 1987, ACTA PSYCHOL, V65, P47, DOI 10.1016/0001-6918(87)90046-1
   TEULINGS HL, 1993, ACTA PSYCHOL, V82, P69, DOI 10.1016/0001-6918(93)90005-C
   Tractinsky N, 2000, INTERACT COMPUT, V13, P127, DOI 10.1016/S0953-5438(00)00031-X
   VANDOORN RRA, 1992, ACTA PSYCHOL, V81, P269, DOI 10.1016/0001-6918(92)90021-5
   VANDOORN RRA, 1993, ACTA PSYCHOL, V82, P275, DOI 10.1016/0001-6918(93)90016-K
   Wobbrock Jacob O., 2003, P 16 ANN ACM S US IN, P61, DOI DOI 10.1145/964696.964703
   WOODS DD, 1995, ERGONOMICS, V38, P2371, DOI 10.1080/00140139508925274
   WRIGHT CE, 1990, ATTENTION PERFORM, P294
   Yost WilliamA., 1994, FUNDAMENTALS HEARING, V3rd
   Zhai S., 2003, Proceedings of the ACM Conference on Human Factors in Computing Systems (CHI 2003), P97
NR 39
TC 14
Z9 14
U1 0
U2 6
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD JUN
PY 2010
VL 7
IS 3
AR 17
DI 10.1145/1773965.1773968
PG 24
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA 618NF
UT WOS:000279361800003
DA 2024-07-18
ER

PT J
AU Giudice, NA
   Bakdash, JZ
   Legge, GE
   Roy, R
AF Giudice, Nicholas A.
   Bakdash, Jonathan Z.
   Legge, Gordon E.
   Roy, Rudrava
TI Spatial Learning and Navigation Using A Virtual Verbal Display
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Experimentation; Performance; Navigation; wayfinding; verbal learning;
   virtual environments; virtual verbal display; human-computer interaction
ID 3-D SOUND; MAPS; KNOWLEDGE; REAL; REPRESENTATIONS; ENVIRONMENTS;
   LANGUAGE; SEARCH; SYSTEM; TRAVEL
AB We report on three experiments that investigate the efficacy of a new type of interface called a virtual verbal display (VVD) for nonvisual learning and navigation of virtual environments (VEs). Although verbal information has been studied for route-guidance, little is known about the use of context-sensitive, speech-based displays (e.g., the VVD) for supporting free exploration and wayfinding behavior. During training, participants used the VVD (Experiments I and II) or a visual display (Experiment III) to search the VEs and find four hidden target locations. At test, all participants performed a route-finding task in the corresponding real environment, navigating with vision (Experiments I and III) or from verbal descriptions (Experiment II). Training performance between virtual display modes was comparable, but wayfinding in the real environment was worse after VVD learning than visual learning, regardless of the testing modality. Our results support the efficacy of the VVD for searching computer-based environments but indicate a difference in the cognitive maps built up between verbal and visual learning, perhaps due to lack of physical movement in the VVD.
C1 [Giudice, Nicholas A.] Univ Maine, Dept Spatial Informat Sci & Engn, Orono, ME 04469 USA.
   [Bakdash, Jonathan Z.] Univ Virginia, Dept Psychol, Charlottesville, VA 22904 USA.
   [Roy, Rudrava] Univ Minnesota, Dept Comp Sci, Minneapolis, MN 55455 USA.
   [Legge, Gordon E.] Univ Minnesota, Dept Psychol, Minneapolis, MN 55455 USA.
C3 University of Maine System; University of Maine Orono; University of
   Virginia; University of Minnesota System; University of Minnesota Twin
   Cities; University of Minnesota System; University of Minnesota Twin
   Cities
RP Giudice, NA (corresponding author), Univ Maine, Dept Spatial Informat Sci & Engn, Orono, ME 04469 USA.
EM giudice@spatial.maine.edu; jzb3e@virginia.edu; legge@umn.edu;
   rudrava@theroyweb.com
RI Bakdash, Jonathan/ABI-7594-2020
OI Bakdash, Jonathan/0000-0002-1409-4779; Legge, Gordon/0000-0002-3742-1680
FU NIDRR [H133A011903]; NIH [5T32EY07133, EY-02857]
FX This research was supported by NIDRR grant H133A011903, NIH training
   grant 5T32EY07133, and NIH grant EY-02857. Authors' addresses: Nicholas
   A. Giudice, Department of Spatial Information Science and Engineering,
   University of Maine, Orono ME, 04469; email: giudice@spatial.maine.edu;
   Jonathan Z. Bakdash, Department of Psychology, University of Virginia,
   Charlottesville, VA 22904- 4400; email: jzb3e@virginia.edu; Gordon E.
   Legge, Department of Psychology, and Rudrava Roy, Department of Computer
   Science, University of Minnesota, Minneapolis, MN 55455-0344; email:
   legge@umn.edu and rudrava@theroyweb.com.
CR Aguirre GK, 1996, CEREB CORTEX, V6, P823, DOI 10.1093/cercor/6.6.823
   Allen GL, 2000, APPL COGNITIVE PSYCH, V14, P333
   Allen GL, 1997, LECT NOTES COMPUT SC, V1329, P363, DOI 10.1007/3-540-63623-4_61
   [Anonymous], 1983, MENTAL MODELS
   [Anonymous], J VIS
   ARETZ AJ, 1991, HUM FACTORS, V33, P85, DOI 10.1177/001872089103300107
   Avraamides MN, 2004, J EXP PSYCHOL LEARN, V30, P801, DOI 10.1037/0278-7393.30.4.804
   Avraamides MN, 2003, COGNITIVE PSYCHOL, V47, P402, DOI 10.1016/S0010-0285(03)00098-7
   Bliss JP, 1997, PRESENCE-TELEOP VIRT, V6, P73, DOI 10.1162/pres.1997.6.1.73
   Bryant DJ, 1997, MIND LANG, V12, P239, DOI 10.1111/1468-0017.00047
   Chance SS, 1998, PRESENCE-TELEOP VIRT, V7, P168, DOI 10.1162/105474698565659
   CLAWSON DM, 1998, P 42 ANN M HUM FACT, P136
   de Vega M, 2001, EUR J COGN PSYCHOL, V13, P369
   DENIS M, 1992, PSYCHOL RES-PSYCH FO, V54, P286, DOI 10.1007/BF01358266
   DENIS MICHEL., 1989, EUR J COGN PSYCHOL, V1, P293, DOI [DOI 10.1080/09541448908403090, 10.1080/09541448908403090]
   FERGUSON EL, 1994, MEM COGNITION, V22, P455, DOI 10.3758/BF03200870
   GIUDICE NA, 2004, THESIS U MINNESOTA M
   GIUDICE NA, 2008, LECT NOTES ARTIFICIA
   Giudice NA, 2007, PSYCHOL RES-PSYCH FO, V71, P347, DOI 10.1007/s00426-006-0089-8
   Kalia AA, 2008, PERCEPTION, V37, P1677, DOI 10.1068/p5915
   Klatzky RL, 2002, LEARN MEMORY, V9, P364, DOI 10.1101/lm.51702
   Klatzky RL, 2003, EXP BRAIN RES, V149, P48, DOI 10.1007/s00221-002-1334-z
   Klatzky RL, 1998, PSYCHOL SCI, V9, P293, DOI 10.1111/1467-9280.00058
   Loomis JM, 1999, BEHAV RES METH INS C, V31, P557, DOI 10.3758/BF03200735
   Loomis JM, 2005, J VISUAL IMPAIR BLIN, V99, P219, DOI 10.1177/0145482x0509900404
   Loomis JM, 1998, PRESENCE-TELEOP VIRT, V7, P193, DOI 10.1162/105474698565677
   Loomis JM, 2002, J EXP PSYCHOL LEARN, V28, P335, DOI 10.1037//0278-7393.28.2.335
   Loomis JM, 2001, FUNDAMENTALS OF WEARABLE COMPUTERS AND AUGMENTED REALITY, P429
   Lovelace KL, 1999, LECT NOTES COMPUT SC, V1661, P65
   Marston JamesR., 2006, ACM T APPL PERCEPT, V3, P110, DOI [DOI 10.1145/1141897.1141900, 10.1145/1141897.1141900]
   PERRIG W, 1985, J MEM LANG, V24, P503, DOI 10.1016/0749-596X(85)90042-7
   Peruch Patrick., 2000, Cognitive Mapping Past, Present and Future, P108
   Petrie H, 1996, J NAVIGATION, V49, P45, DOI 10.1017/S0373463300013084
   Richardson AE, 1999, MEM COGNITION, V27, P741, DOI 10.3758/BF03211566
   Ruddle RA, 1997, J EXP PSYCHOL-APPL, V3, P143, DOI 10.1037/1076-898X.3.2.143
   Ruddle RA, 2004, INT J HUM-COMPUT ST, V60, P299, DOI 10.1016/j.ijhcs.2003.10.001
   Ruddle RA, 1999, J EXP PSYCHOL-APPL, V5, P54, DOI 10.1037/1076-898X.5.1.54
   Ruddle RA, 2006, PSYCHOL SCI, V17, P460, DOI 10.1111/j.1467-9280.2006.01728.x
   SCHLICHT EJ, 2001, P ANN M ASS RES VIS
   Siegel A., 1975, ADV CHILD DEV BEHAV
   Stankiewicz BJ, 2006, J EXP PSYCHOL HUMAN, V32, P688, DOI 10.1037/0096-1523.32.3.688
   TAYLOR HA, 1992, J MEM LANG, V31, P261, DOI 10.1016/0749-596X(92)90014-O
   Tom A, 2003, LECT NOTES COMPUT SC, V2825, P362
   TVERSKY B, 1996, LANGUAGE SPEECH COMM
   Waller D, 1998, PRESENCE-TELEOP VIRT, V7, P129, DOI 10.1162/105474698565631
   Waller D, 2001, HUM FACTORS, V43, P147, DOI 10.1518/001872001775992561
   Wilson PN, 1997, ECOL PSYCHOL, V9, P207, DOI 10.1207/s15326969eco0903_3
   Wingrave CA, 2006, IEEE SYMPOSIUM ON 3D USER INTERFACES 2006, PROCEEDINGS, P11, DOI 10.1109/TRIDUI.2006.1618264
   Witmer BG, 1996, INT J HUM-COMPUT ST, V45, P413, DOI 10.1006/ijhc.1996.0060
   [No title captured]
NR 50
TC 11
Z9 14
U1 0
U2 12
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD JAN
PY 2010
VL 7
IS 1
AR 3
DI 10.1145/1658349.1658352
PG 22
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA 549EK
UT WOS:000274028400003
DA 2024-07-18
ER

PT J
AU Hinde, SJ
   Noland, KC
   Thomas, GA
   Bull, DR
   Gilchrist, ID
AF Hinde, Stephen J.
   Noland, Katy C.
   Thomas, Graham A.
   Bull, David R.
   Gilchrist, Iain D.
TI On the Immersive Properties of High Dynamic Range Video
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Immersion; broadcast; high dynamic range; quality of experience
AB This paper presents the results from two studies which used a dual-task methodology to measure an audience's experience of immersion while watching video under typical television viewing conditions. Immersion was measured while participants watched either a high dynamic range, wide color gamut video or a standard dynamic range, standard color gamut video, in high definition or ultra-high definition. Other video parameters were carefully measured and controlled.
   The study found that high dynamic range, wide color gamut video is significantly more immersive than standard dynamic range, standard color gamut video in the chosen configuration. However, there was no evidence of significant differences in immersion between high-definition and ultra-high-definition resolutions.
C1 [Hinde, Stephen J.; Gilchrist, Iain D.] Univ Bristol, Sch Psychol Sci, Bristol, Avon, England.
   [Noland, Katy C.] BBC Res & Dev, Lighthouse,White City Pl,201 Wood Lane, London W12 7TQ, England.
   [Thomas, Graham A.] BBC Res & Dev, 5th Floor,Dock House, Salford M50 2LH, Lancs, England.
   [Bull, David R.] Univ Bristol, Bristol Vis Inst, Bristol, Avon, England.
C3 University of Bristol; University of Bristol
RP Hinde, SJ (corresponding author), Univ Bristol, Sch Psychol Sci, Bristol, Avon, England.
EM s.j.hinde@bristol.ac.uk; Katy.Noland@bbc.co.uk; Graham.Thomas@bbc.co.uk;
   Dave.Bull@bristol.ac.uk; I.D.Gilchrist@bristol.ac.uk
RI Gilchrist, Iain D/E-8627-2010; Noland, Katy/GOV-5365-2022
OI Noland, Katy/0000-0002-0445-1613; Gilchrist, Iain D./0000-0003-2070-6679
FU University of Bristol; UK Engineering and Physical Sciences Research
   Council [EP/M000885/1]; UKRI Strength in Places Fund [SIPF00006/1]
FX This work was supported by the University of Bristol and by grants from
   the UK Engineering and Physical Sciences Research Council to Professor
   David Bull and Professor Iain D. Gilchrist (EP/M000885/1) and the UKRI
   Strength in Places Fund (SIPF00006/1). The experimental protocols were
   approved by the Ethics Board at the University of Bristol, UK.
CR Akyüz AG, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1276377.1276425
   [Anonymous], 2017, ITUTP1203
   [Anonymous], 2015, ITURBT7096
   [Anonymous], 2019, ITURBT50014
   [Anonymous], 2018, ITURBT21002
   [Anonymous], 2015, 2015 7 INT WORKSHOP, DOI DOI 10.1109/QOMEX.2015.7148114
   [Anonymous], 2012, 2012 ANN TECHN C EXH, DOI DOI 10.5594/M001446
   [Anonymous], 2011, ITU-R BT.1886-0
   Arnau-Gonzalez P, 2017, INT WORK QUAL MULTIM
   Azimi M., 2014, INT C MULT SIGN PROC
   BBC, BBC 1203
   BBC Earth, 2016, IG VS SNAK PLAN EART
   BBC Research and Development, 2018, HLG LOOK UP TABL LIC
   Bezdek MA, 2017, MEDIA PSYCHOL, V20, P60, DOI 10.1080/15213269.2015.1121830
   Blackmagic Design, 2018, REF MAN DAVINCI RES
   Bompas A, 2015, NEUROIMAGE, V107, P34, DOI 10.1016/j.neuroimage.2014.11.057
   Borer T., 2015, A "display independent" high dynamic range television system
   EBU, 2013, REP EBU WORKSH 2013
   Evans M., 2018, PROC 36 ANN ACM C HU
   Fairchild M., The HDR Photographic Survey
   Froehlich J, 2014, PROC SPIE, V9023, DOI 10.1117/12.2040003
   Hanhart P., 2015, SMPTE MOTION IMAGING, V124, P1
   Hinde S. J., 2022, IMMERSIVE PROPERTIES, DOI [10.17605/OSF.IO/5FWKA, DOI 10.17605/OSF.IO/5FWKA]
   Hinde SJ, 2018, COGN RES, V3, DOI 10.1186/s41235-018-0140-5
   ITU-R, 2017, PRES STAT ULTR DEF T
   ITU-R, 2012, GEN VIEW COND SUBJ A
   Jia S, 2017, IEEE IMAGE PROC, P765, DOI 10.1109/ICIP.2017.8296384
   Juluri P, 2016, IEEE COMMUN SURV TUT, V18, P401, DOI 10.1109/COMST.2015.2401424
   Kahneman D., 1973, Attention and effort
   Kara P. A., 2019, QUALITY USER EXPERIE, V4, DOI 10.1007/s41233-019-0027-3
   KELLY DH, 1979, J OPT SOC AM, V69, P1340, DOI 10.1364/JOSA.69.001340
   Kundu D., 2016, ESPL LIVE HDR IMAGE
   Kunkel Timo., 2010, P 7 S APPL PERCEPTIO, P17, DOI DOI 10.1145/1836248.1836251
   Lang A, 2000, J COMMUN, V50, P46, DOI 10.1111/j.1460-2466.2000.tb02833.x
   Larson GW, 1997, IEEE T VIS COMPUT GR, V3, P291, DOI 10.1109/2945.646233
   Layton P., 2018, SUMMER FOOTBALL TENN
   Le Callet P., 2013, Qualinet White Paper on Definitions of Quality of Experience
   Lessiter J, 2001, PRESENCE-TELEOP VIRT, V10, P282, DOI 10.1162/105474601300343612
   Liu M., 2014, IEEE INTERNA TIONAL
   Ma KD, 2015, IEEE T IMAGE PROCESS, V24, P3086, DOI [10.1109/TIP.2015.2436340, 10.1109/TIP.2015.2456638]
   Mantiuk R, 2004, ACM T GRAPHIC, V23, P733, DOI 10.1145/1015706.1015794
   Moss FM, 2016, IEEE T CIRC SYST VID, V26, P1977, DOI 10.1109/TCSVT.2015.2461971
   Mukherjee R, 2016, SIGNAL PROCESS-IMAGE, V47, P426, DOI 10.1016/j.image.2016.08.001
   Nafchi HZ, 2015, IEEE SIGNAL PROC LET, V22, P1026, DOI 10.1109/LSP.2014.2381458
   Narwaria M, 2015, SIGNAL PROCESS-IMAGE, V35, P46, DOI 10.1016/j.image.2015.04.009
   Narwaria M, 2015, J ELECTRON IMAGING, V24, DOI 10.1117/1.JEI.24.1.010501
   Narwaria Manish., 2015, Visual Signal Quality Assess-ment: Quality of Experience (QoE), P129
   Nasiopoulos E, 2014, 2014 10TH INTERNATIONAL CONFERENCE ON HETEROGENEOUS NETWORKING FOR QUALITY, RELIABILITY, SECURITY AND ROBUSTNESS (QSHINE), P13, DOI [10.1109/QSHINE.2014.6928653, 10.4108/icst.qshine.2014.256428]
   Noland K. C., 2015, 287 BBC R D WHP
   Noland KC, 2017, INT WORK QUAL MULTIM
   Reinhard E, 2007, SID INT SYMP DIG TEC, V38, P1049, DOI 10.1889/1.2785486
   Shishikui Y, 2018, IEEE INT SYM BROADB
   Shishikui Y, 2018, IEEE T BROADCAST, V64, P498, DOI 10.1109/TBC.2018.2829118
   Slater M, 1997, PRESENCE-VIRTUAL AUG, V6, P603, DOI 10.1162/pres.1997.6.6.603
   Song Y, 2017, IEEE IMAGE PROC, P1012, DOI 10.1109/ICIP.2017.8296434
   Van Wallendael G, 2016, 2016 EIGHTH INTERNATIONAL CONFERENCE ON QUALITY OF MULTIMEDIA EXPERIENCE (QOMEX)
   Wageningen Bioveterinary Research, 2018, MARAN 2018 MON ANT R
   Winkler S, 2008, IEEE T BROADCAST, V54, P660, DOI 10.1109/TBC.2008.2000733
   Zhang F, 2016, IEEE T CIRC SYST VID, V26, P1017, DOI 10.1109/TCSVT.2015.2428551
   Zhang Y, 2016, IEEE T CIRC SYST VID, V26, P950, DOI 10.1109/TCSVT.2015.2426552
NR 60
TC 2
Z9 2
U1 1
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD APR
PY 2022
VL 19
IS 2
AR 7
DI 10.1145/3524692
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 3A7CY
UT WOS:000827414800003
OA Green Submitted, Bronze
DA 2024-07-18
ER

PT J
AU Jiang, YY
   O'Neal, EE
   Zhou, SW
   Plumert, JM
   Kearney, JK
AF Jiang, Yuanyuan
   O'Neal, Elizabeth E.
   Zhou, Shiwen
   Plumert, Jodie M.
   Kearney, Joseph K.
TI Crossing Roads with a Computer-generated Agent: Persistent Effects on
   Perception-Action Tuning
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Immersive virtual environments; virtual agents; stereo displays; joint
   action; pedestrian road crossing; large screen VE
ID TIME SCALES; AFFORDANCES; PEDESTRIANS; BEHAVIOR; PASSAGE
AB This study investigated how people coordinate their decisions and actions with a risky or safe computer-generated agent in a humanoid or non-humanoid form and how this experience influences later behavior when acting alone. In Experiment 1, participants first repeatedly crossed continuous traffic in a virtual environment with a humanoid computer-generated agent (Figure 1). Participants were specifically instructed to cross with an agent that was programmed to be either safe (taking only large gaps) or risky (also taking relatively small gaps). Participants then repeatedly crossed the same roadway alone. We found that participants' experiences with crossing safe vs. risky gaps with an agent persisted in later trials when the participants crossed alone, such that participants accepted tighter gaps if they were previously paired with a risky than a safe agent. In Experiment 2 (Figure 2), we tested whether experience crossing with a risky or safe non-humanoid object (a floating box) also influenced later behavior when crossing alone. We again found that participants who crossed with the risky object partner took tighter gaps when later crossing alone than those who crossed with the safe object partner. The Discussion focuses on the impact of experiences with virtual agents on perception-action tuning and the potential of using virtual agents for training safe road-crossing behavior.
C1 [Jiang, Yuanyuan] Calif State Univ San Marcos, Comp Sci & Informat Syst, San Marcos, CA 92096 USA.
   [O'Neal, Elizabeth E.; Zhou, Shiwen; Plumert, Jodie M.] Univ Iowa, Psychol & Brain Sci, Iowa City, IA USA.
   [Kearney, Joseph K.] Univ Iowa, Comp Sci, Iowa City, IA USA.
C3 California State University System; California State University San
   Marcos; University of Iowa; University of Iowa
RP Jiang, YY (corresponding author), Calif State Univ San Marcos, Comp Sci & Informat Syst, San Marcos, CA 92096 USA.
EM yjiang@csusm.edu; elizabeth-oneal@uiowa.edu; shiwen-zhou@uiowa.edu;
   jodie-plumert@uiowa.edu; joe-kearney@uiowa.edu
RI jiang, anyi/GPT-0379-2022
OI O'Neal, Elizabeth/0000-0002-3934-4009
FU National Science Foundation [BCS-1251694, CNS-1305131]; US Department of
   Transportation, Research and Innovative Technology Administration, Prime
   DFDA [20.701, DTRT13-G-UTC53]
FX This research was supported by National Science Foundation awards
   BCS-1251694 and CNS-1305131, and by the US Department of Transportation,
   Research and Innovative Technology Administration, Prime DFDA No.
   20.701, Award No. DTRT13-G-UTC53.
CR Babu SV, 2011, IEEE T VIS COMPUT GR, V17, P14, DOI 10.1109/TVCG.2009.211
   Buck LE, 2019, IEEE T VIS COMPUT GR, V25, P2123, DOI 10.1109/TVCG.2019.2899232
   Chang CH, 2009, J MOTOR BEHAV, V41, P495, DOI 10.3200/35-08-095
   CRUZNEIRA C, 1992, COMMUN ACM, V35, P64, DOI 10.1145/129888.129892
   Davis TJ, 2010, PERCEPTION, V39, P1624, DOI 10.1068/p6712
   Duan JY, 2012, TRAFFIC INJ PREV, V13, P442, DOI 10.1080/15389588.2012.655430
   Faria JJ, 2010, BEHAV ECOL, V21, P1236, DOI 10.1093/beheco/arq141
   Franchak JM, 2012, EXP BRAIN RES, V223, P301, DOI 10.1007/s00221-012-3261-y
   Gibson J., 1979, The ecological approach to visual perception
   Guéguen N, 2001, J SOC PSYCHOL, V141, P413, DOI 10.1080/00224540109600562
   Jiang Y., 2016, P ACM S APPL PERCEPT, P57, DOI [DOI 10.1145/2931002.2931003, 10.1145/2931002.2931003, DOI 10.1145/2931002]
   Jiang YY, 2019, IEEE T VIS COMPUT GR, V25, P2886, DOI 10.1109/TVCG.2018.2865945
   Jiang YY, 2018, ACM T APPL PERCEPT, V15, DOI 10.1145/3147884
   Kilner JM, 2003, CURR BIOL, V13, P522, DOI 10.1016/S0960-9822(03)00165-9
   LEFKOWITZ M, 1955, J Abnorm Psychol, V51, P704, DOI 10.1037/h0042000
   Malik J., 2021, P 2021 CHI C HUMAN F, P1
   O'Neal EE, 2019, J PEDIATR PSYCHOL, V44, P726, DOI 10.1093/jpepsy/jsz020
   O'Neal EE, 2018, J EXP PSYCHOL HUMAN, V44, P18, DOI 10.1037/xhp0000378
   Plumert JM, 2011, J EXP CHILD PSYCHOL, V108, P322, DOI 10.1016/j.jecp.2010.07.005
   Rahimian P, 2016, P IEEE VIRT REAL ANN, P141, DOI 10.1109/VR.2016.7504697
   Rahimian Pooya, 2015, ROAD SAF SIM INT C 2, P828
   RIESER JJ, 1995, J EXP PSYCHOL HUMAN, V21, P480, DOI 10.1037/0096-1523.21.3.480
   Scholl BJ, 2000, TRENDS COGN SCI, V4, P299, DOI 10.1016/S1364-6613(00)01506-0
   Schwebel DC, 2014, HEALTH PSYCHOL, V33, P628, DOI 10.1037/hea0000032
   Schwebel DC, 2010, INJURY PREV, V16, pE1, DOI 10.1136/ip.2009.025288
   Thomson JA, 2005, J EXP PSYCHOL-APPL, V11, P175, DOI 10.1037/1076-898X.11.3.175
NR 26
TC 3
Z9 3
U1 0
U2 8
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD JAN
PY 2021
VL 18
IS 1
AR 4
DI 10.1145/3431923
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA PZ2NW
UT WOS:000612577900004
OA Bronze
DA 2024-07-18
ER

PT J
AU Fang, YC
   Zhang, W
   Liu, NJ
AF Fang, Yuchun
   Zhang, Wei
   Liu, Ningjie
TI On the Perception Analysis of User Feedback for Interactive Face
   Retrieval
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Interactive retrieval; human face perception; face retrieval; relevance
   feedback
ID RECOGNITION; FAMILIAR; SEX
AB In this article, we explore the coherence of face perception between human and machine in the scenario of interactive face retrieval. In the part of human perception, we collect user feedback to the stimuli of a target face and groups of displayed candidate face images in a face database with a large number of subjects. In the part of machine vision, we compare the benchmark features and general metrics to measure face similarity. We propose a series of coherence measurements to evaluate the statistic characteristic of human and machine face perception. We discover that despite the unfamiliarity of users to most faces in the database, the coherence between human and machine remains in a stable level across multiple variations in metrics, features, size of databases, and demographics. The simulation experiments with the coherence distributions demonstrate that the embedded information is valuable to speed up interactive retrieval. The comparisons over multiple parameter settings provide feasible instructions in designing the interactive face retrieval system with more consideration of human factors.
C1 [Fang, Yuchun; Zhang, Wei; Liu, Ningjie] Shanghai Univ, Sch Comp Engn & Sci, 99 Shangda Rd, Shanghai 200444, Peoples R China.
C3 Shanghai University
RP Fang, YC (corresponding author), Shanghai Univ, Sch Comp Engn & Sci, 99 Shangda Rd, Shanghai 200444, Peoples R China.
EM ycfang@shu.edu.cn
RI Fang, Yuchun/GPX-4154-2022
OI Fang, Yuchun/0000-0002-7085-8876
FU National Natural Science Foundation of China [61976132, 61170115];
   National Natural Science Foundation of Shanghai [19ZR1419200]
FX The work is supported by the National Natural Science Foundation of
   China under Grants No. 61976132 and No. 61170115 and the National
   Natural Science Foundation of Shanghai under Grant No. 19ZR1419200.
CR Ahonen T, 2006, IEEE T PATTERN ANAL, V28, P2037, DOI 10.1109/TPAMI.2006.244
   [Anonymous], 2014, 3 INT C LEARN REPR
   Armann R, 2011, J VISION, V11, DOI 10.1167/11.13.9
   Belhumeur P.N., 1997, EIGENFACES VS FISHER
   BRUCE V, 1987, CAN J PSYCHOL, V41, P510, DOI 10.1037/h0084165
   Cao Q, 2018, IEEE INT CONF AUTOMA, P67, DOI 10.1109/FG.2018.00020
   Chang L, 2017, CELL, V169, P1013, DOI 10.1016/j.cell.2017.05.011
   Chen FM, 2014, ACM T APPL PERCEPT, V11, DOI 10.1145/2622655
   ELLIS HD, 1979, PERCEPTION, V8, P431, DOI 10.1068/p080431
   Fang YC, 2005, LECT NOTES COMPUT SC, V3546, P637
   Fang YC, 2018, EURASIP J IMAGE VIDE, DOI 10.1186/s13640-018-0282-x
   Fang Yuchun, 2005, P 7 ACM SIGMM INT WO, P193, DOI [DOI 10.1145/1101826, 10.1145/1101826.1101858, DOI 10.1145/1101826.1101858]
   Freiwald WA, 2010, SCIENCE, V330, P845, DOI 10.1126/science.1194908
   Gong Cheng, 2011, 2011 4th International Congress on Image and Signal Processing (CISP 2011), P828, DOI 10.1109/CISP.2011.6100290
   Han XF, 2015, PROC CVPR IEEE, P3279, DOI 10.1109/CVPR.2015.7298948
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang G.B., 2008, WORKSH FAC REAL LIF
   Landi SM, 2017, SCIENCE, V357, P591, DOI 10.1126/science.aan1139
   Martin MV, 2016, ACM T APPL PERCEPT, V14, DOI 10.1145/2955097
   Niewiadomski R, 2015, ACM T APPL PERCEPT, V12, P27, DOI 10.1145/2699255
   O'Toole AJ, 2007, IEEE T PATTERN ANAL, V29, P1642, DOI 10.1109/TPAMI.2007.1107
   O'Toole AJ, 2012, ACM T APPL PERCEPT, V9, DOI 10.1145/2355598.2355599
   OToole AJ, 1996, PERCEPTION, V25, P669, DOI 10.1068/p250669
   Park U, 2010, IEEE T INF FOREN SEC, V5, P406, DOI 10.1109/TIFS.2010.2049842
   Parkhi OM, 2015, DEEP FACE RECOGNITIO
   Phillips PJ, 2018, P NATL ACAD SCI USA, V115, P6171, DOI 10.1073/pnas.1721355115
   Phillips PJ, 2011, ACM T APPL PERCEPT, V8, DOI 10.1145/1870076.1870082
   Phillips PJ, 2000, IEEE T PATTERN ANAL, V22, P1090, DOI 10.1109/34.879790
   Pourdamghani N, 2012, INT C PATT RECOG, P3386
   Russell R, 2003, PERCEPTION, V32, P1093, DOI 10.1068/p5101
   Schumacher M, 2012, ACM T APPL PERCEPT, V9, DOI 10.1145/2325722.2325724
   Sun Y, 2014, PROC CVPR IEEE, P1891, DOI 10.1109/CVPR.2014.244
   TURK M, 1991, J COGNITIVE NEUROSCI, V3, P71, DOI 10.1162/jocn.1991.3.1.71
   Wang Dai, 2011, 2011 4th International Congress on Image and Signal Processing (CISP 2011), P1358, DOI 10.1109/CISP.2011.6100433
   Xie SF, 2010, IEEE T IMAGE PROCESS, V19, P1349, DOI 10.1109/TIP.2010.2041397
   Xu XX, 2013, CRYSTENGCOMM, V15, P6159, DOI 10.1039/c3ce40363g
   YAMAGUCHI MK, 1995, PERCEPTION, V24, P563, DOI 10.1068/p240563
   Yen CT, 2011, PR ELECTROMAGN RES S, P393
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
NR 39
TC 3
Z9 3
U1 2
U2 10
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD NOV
PY 2020
VL 17
IS 3
AR 10
DI 10.1145/3403964
PG 20
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA PA3OQ
UT WOS:000595548000002
DA 2024-07-18
ER

PT J
AU Toscani, M
   Guarnera, D
   Guarnera, GC
   Hardeberg, JY
   Gegenfurtner, KR
AF Toscani, Matteo
   Guarnera, Dar'ya
   Guarnera, Giuseppe Claudio
   Hardeberg, Jon Yngve
   Gegenfurtner, Karl R.
TI Three Perceptual Dimensions for Specular and Diffuse Reflection
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Perception; BRDF; dimensionality
ID VISUAL-PERCEPTION; IMAGE STATISTICS; SURFACE GLOSS; COLOR; SHAPE;
   ILLUMINATION; RECOGNITION; INFORMATION; HIGHLIGHTS; MODEL
AB Previous research investigated the perceptual dimensionality of achromatic reflection of opaque surfaces, by using either simple analytic models of reflection or measured reflection properties of a limited sample of materials. Here, we aim to extend this work to a broader range of simulated materials. In a first experiment, we used sparse multidimensional scaling techniques to represent a set of rendered stimuli in a perceptual space that is consistent with participants' similarity judgments. Participants were presented with one reference object and four comparisons, rendered with different material properties. They were asked to rank the comparisons according to their similarity to the reference, resulting in an efficient collection of a large number of similarity judgments. To interpret the space individuated by multidimensional scaling, we ran a second experiment in which observers were asked to rate our experimental stimuli according to a list of 30 adjectives referring to their surface reflectance properties. Our results suggest that perception of achromatic reflection is based on at least three dimensions, which we labelled "Lightness," "Gloss," and "Metallicity," in accordance with the rating results. These dimensions are characterized by a relatively simple relationship with the parameters of the physically based rendering model used to generate our stimuli, indicating that they correspond to different physical properties of the rendered materials. Specifically, "Lightness" relates to diffuse reflections, "Gloss" to the presence of high contrast sharp specular highlights, and "Metallicity" to spread out specular reflections.
C1 [Toscani, Matteo; Gegenfurtner, Karl R.] Justus Liebig Univ Giessen, Otto Behaghel Str 10F, D-35394 Giessen, Germany.
   [Guarnera, Dar'ya; Guarnera, Giuseppe Claudio; Hardeberg, Jon Yngve] NTNU Norwegian Univ Sci & Technol, Teknologivegen 22, N-2815 Ametyst Bygget, Gjovik, Norway.
   [Guarnera, Giuseppe Claudio] Univ York, Dept Comp Sci, Deramore Lane, York YO10 5GH, N Yorkshire, England.
C3 Justus Liebig University Giessen; Norwegian University of Science &
   Technology (NTNU); University of York - UK
RP Toscani, M (corresponding author), Justus Liebig Univ Giessen, Otto Behaghel Str 10F, D-35394 Giessen, Germany.
EM matteo.toscani@psychol.uni-giessen.de; darya@ntnu.no;
   claudio.guarnera@york.ac.uk; jon.hardeberg@ntnu.no;
   karl.r.gegenfurtner@psychol.uni-giessen.de
RI Toscani, Matteo/AAI-5201-2021; Guarnera, Giuseppe Claudio/ABC-8635-2022;
   Toscani, Matteo/R-7347-2018
OI Guarnera, Giuseppe Claudio/0000-0002-7703-5194; Toscani,
   Matteo/0000-0002-1884-5533
FU Deutsche Forschungsgemeinschaft (DFG, German Research Foundation)
   [222641018-SFB/TRR 135]; Project MUVApp - Research Council of Norway
   [250293]; Project Spectraskin - Research Council of Norway [288670]
FX The study was funded by the Deutsche Forschungsgemeinschaft (DFG, German
   Research Foundation)-Project No. 222641018-SFB/TRR 135, TP A8, and C2.
   This work was also supported by Projects MUVApp N-250293 and Spectraskin
   N-288670, both funded by the Research Council of Norway.
CR Adelson E. H., 1996, Perception as Bayesian inference, P409
   Agarwal Sameer, 2007, Artificial Intelligence and Statistics, P11
   Anderson BL, 2011, CURR BIOL, V21, pR978, DOI 10.1016/j.cub.2011.11.022
   Anderson BL, 2009, J VISION, V9, DOI 10.1167/9.11.10
   [Anonymous], 1760, Photometria sive de mensura et gradibus luminis, colorum et umbrae
   [Anonymous], 2009, J VISUAL-JAPAN, DOI [10.1167/9.8.784, DOI 10.1167/9.8.784]
   [Anonymous], 2005, EUR S REND
   Bagher MM, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2907941
   Baumgartner E, 2013, MULTISENS RES, V26, P429, DOI 10.1163/22134808-00002429
   BECK J, 1964, AM J PSYCHOL, V77, P54, DOI 10.2307/1419271
   BECK J, 1981, PERCEPT PSYCHOPHYS, V30, P407, DOI 10.3758/BF03206160
   Berzhanskaya J, 2005, PERCEPTION, V34, P565, DOI 10.1068/p5401
   BILLMEYER FW, 1987, COLOR RES APPL, V12, P315, DOI 10.1002/col.5080120606
   BLAKE A, 1990, NATURE, V343, P165, DOI 10.1038/343165a0
   Bousseau A, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2451236.2451244
   Burley Brent, 2012, P ACM SIGGRAPH 2012
   Chadwick AC, 2015, VISION RES, V109, P221, DOI 10.1016/j.visres.2014.10.026
   Cholewiak S., 2015, J VISION, V15, P965, DOI [10.1167/15.12.965, DOI 10.1167/15.12.965]
   Cholewiak S. A., 2013, J VISION, V13, P258
   Cholewiak S. A., 2014, J VISION, V14, P1113
   CHURCH EL, 1989, P SOC PHOTO-OPT INS, V1165, P136
   Cook R. L., 1981, Computer Graphics, V15, P307, DOI 10.1145/965161.806819
   Doerschner K, 2011, CURR BIOL, V21, P2010, DOI 10.1016/j.cub.2011.10.036
   Doerschner K, 2010, J VISION, V10, DOI 10.1167/10.4.8
   Dupuy J, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275059
   Ferwerda JA, 2001, PROC SPIE, V4299, P291, DOI 10.1117/12.429501
   Filip J, 2014, COMPUT GRAPH FORUM, V33, P91, DOI 10.1111/cgf.12477
   Filip J, 2019, ACM T APPL PERCEPT, V16, DOI 10.1145/3301412
   Fleming RW, 2014, VISION RES, V94, P62, DOI 10.1016/j.visres.2013.11.004
   Fleming RW, 2013, J VISION, V13, DOI 10.1167/13.8.9
   Fleming RW, 2012, CURR BIOL, V22, pR865, DOI 10.1016/j.cub.2012.08.030
   Fleming RW, 2004, J VISION, V4, P798, DOI 10.1167/4.9.10
   Fleming RW, 2003, J VISION, V3, P347, DOI 10.1167/3.5.3
   Fresnel Augustin Jean, 1834, MEMOIRE LOI MODIFICA
   Gegenfurtner KR, 2003, ANNU REV NEUROSCI, V26, P181, DOI 10.1146/annurev.neuro.26.041002.131116
   Gegenfurtner KR, 2000, CURR BIOL, V10, P805, DOI 10.1016/S0960-9822(00)00563-7
   Guarnera D, 2016, COMPUT GRAPH FORUM, V35, P625, DOI 10.1111/cgf.12867
   Guarnera D, 2018, SIGGRAPH'18: ACM SIGGRAPH 2018 TALKS, DOI 10.1145/3214745.3214807
   Guarnera D, 2020, IEEE T VIS COMPUT GR, V26, P2258, DOI 10.1109/TVCG.2018.2886877
   Guarnera GC, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3132187
   Hecht S, 1930, J OPT SOC AM, V20, P231, DOI 10.1364/JOSA.20.000231
   Helmholtz H., 1852, POGGEND ANN PHYS CHE, V87, P45, DOI DOI 10.1002/ANDP.18521630904
   Ho YX, 2008, PSYCHOL SCI, V19, P196, DOI 10.1111/j.1467-9280.2008.02067.x
   Holzschuch N, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073621
   Holzschuch Nicolas, 2017, P WORKSH MAT APP MOD, P25, DOI [10.2312/mam.20171328, DOI 10.2312/MAM.20171328]
   Hunter Richard S., 1937, 958 NBS RP
   Kendall D. G., 1989, STAT SCI, V4, P87, DOI DOI 10.1214/SS/1177012582
   Kim J, 2012, NAT NEUROSCI, V15, P1590, DOI 10.1038/nn.3221
   Kim J, 2011, J VISION, V11, DOI 10.1167/11.9.4
   Kim J, 2010, J VISION, V10, DOI 10.1167/10.9.3
   Kleiner M, 2007, PERCEPTION, V36, P14
   KNILL DC, 1991, NATURE, V351, P228, DOI 10.1038/351228a0
   KOENDERINK JJ, 1980, OPT ACTA, V27, P981, DOI 10.1080/713820338
   Lagunas Manuel, 2019, 190501562 ARXIV
   Löw J, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2077341.2077350
   MacAdam DL, 1942, J OPT SOC AM, V32, P247, DOI 10.1364/JOSA.32.000247
   Marlow PJ, 2016, J VISION, V16, DOI 10.1167/16.1.5
   Marlow PJ, 2015, CURR BIOL, V25, pR221, DOI 10.1016/j.cub.2015.01.062
   Marlow PJ, 2013, J VISION, V13, DOI 10.1167/13.14.2
   Marlow PJ, 2012, CURR BIOL, V22, P1909, DOI 10.1016/j.cub.2012.08.009
   Marr D., 1982, Visual perception
   Matusik W., 2003, THESIS
   Maxwell J. C., 1857, Earth Environ. Sci. Trans. R. Soc. Edinb, V21, P275, DOI [10.1017/S0080456800032117, DOI 10.1017/S0080456800032117]
   McAuley S., 2012, ACM SIGGRAPH 2012 Courses, SIGGRAPH'12, p10:1, DOI DOI 10.1145/2343483.2343493
   Milojevic Z, 2018, VISION RES, V151, P18, DOI 10.1016/j.visres.2018.01.008
   MOLLON JD, 1989, J EXP BIOL, V146, P21
   Motoyoshi I, 2007, NATURE, V447, P206, DOI 10.1038/nature05724
   Muryy AA, 2016, P ROY SOC B-BIOL SCI, V283, DOI 10.1098/rspb.2016.0383
   Muryy AA, 2013, P NATL ACAD SCI USA, V110, P2413, DOI 10.1073/pnas.1212417110
   Nicodemus F. E., 1992, Geometrical Considerations and Nomenclature for Reflectance, P94
   Nishida S, 1998, J OPT SOC AM A, V15, P2951, DOI 10.1364/JOSAA.15.002951
   Nishio A, 2014, J NEUROSCI, V34, P11143, DOI 10.1523/JNEUROSCI.1451-14.2014
   O'Donnell F.X. D., 1986, REV EVALUATION APPEA, P14
   Olkkonen M, 2011, I-PERCEPTION, V2, P1014, DOI 10.1068/i0480
   Olkkonen M, 2010, J VISION, V10, DOI 10.1167/10.9.5
   Paulun V. C., 2017, J VIS, V17, P1
   Pellacini F, 2000, COMP GRAPH, P55, DOI 10.1145/344779.344812
   Phillips JB, 2010, PROC SPIE, V7527, DOI 10.1117/12.845399
   Pizlo Z, 2001, VISION RES, V41, P3145, DOI 10.1016/S0042-6989(01)00173-0
   POGGIO T, 1985, PROC R SOC SER B-BIO, V226, P303, DOI 10.1098/rspb.1985.0097
   Poggio T., 1987, Readings in computer vision, P638, DOI [DOI 10.1016/B978-0-08-051581-6.50061-1, 10.1016/B978-0-08-051581-6.50061-1]
   Serrano A, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980242
   Sharan L, 2008, J OPT SOC AM A, V25, P846, DOI 10.1364/JOSAA.25.000846
   Soler C, 2018, COMPUT GRAPH FORUM, V37, P135, DOI 10.1111/cgf.13348
   Sumner P, 2000, J EXP BIOL, V203, P1987
   Sun TC, 2017, COMPUT GRAPH FORUM, V36, P47, DOI 10.1111/cgf.13223
   Thomas JB, 2017, LECT NOTES COMPUT SC, V10213, P233, DOI 10.1007/978-3-319-56010-6_20
   Thompson William., 2016, Visual perception from a computer graphics perspective
   Todd JT, 2018, J VISION, V18, DOI 10.1167/18.3.9
   Todd JT, 2004, PSYCHOL SCI, V15, P33, DOI 10.1111/j.0963-7214.2004.01501006.x
   TORRANCE KE, 1967, J OPT SOC AM, V57, P1105, DOI 10.1364/JOSA.57.001105
   Toscani M, 2019, I-PERCEPTION, V10, DOI 10.1177/2041669519884335
   Toscani M, 2017, VISION RES, V131, P82, DOI 10.1016/j.visres.2016.12.004
   Toscani M, 2016, J VISION, V16, DOI 10.1167/16.15.21
   Toscani M, 2013, P NATL ACAD SCI USA, V110, P11163, DOI 10.1073/pnas.1216954110
   USC Institute for Creative Technologies, 2017, HIGH RES LIGHT PROF
   Vangorp P, 2008, APGV 2008: PROCEEDINGS OF THE SYMPOSIUM ON APPLIED PERCEPTION IN GRAPHICS AND VISUALIZATION, P123
   von Helmholtz Hermann, 1962, HELMHOLTZS TREATISE, V1
   WARD GJ, 1992, COMP GRAPH, V26, P265, DOI 10.1145/142920.134078
   Wiebel CB, 2015, VISION RES, V115, P175, DOI 10.1016/j.visres.2015.04.010
   Wiebel CB, 2014, J VISION, V14, DOI 10.1167/14.7.10
   Wiebel CB, 2013, ATTEN PERCEPT PSYCHO, V75, P954, DOI 10.3758/s13414-013-0436-y
   Wijntjes MWA, 2010, J VISION, V10, DOI 10.1167/10.9.13
   Wills J, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1559755.1559760
   Witzel C, 2018, ANNU REV VIS SCI, V4, P475, DOI 10.1146/annurev-vision-091517-034231
   Xiao B, 2008, VISUAL NEUROSCI, V25, P371, DOI 10.1017/S0952523808080267
NR 106
TC 17
Z9 17
U1 3
U2 8
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD MAY
PY 2020
VL 17
IS 2
AR 6
DI 10.1145/3380741
PG 26
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA OH5JF
UT WOS:000582618500002
OA Green Accepted, Green Published
DA 2024-07-18
ER

PT J
AU Chapiro, A
   Kunkel, T
   Atkins, R
   Daly, S
AF Chapiro, Alexandre
   Kunkel, Timo
   Atkins, Robin
   Daly, Scott
TI Influence of Screen Size and Field of View on Perceived Brightness
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Perception; brightness; size effects; computational display; viewing
   distance
ID COLOR APPEARANCE; CONTRAST; AREA
AB We present a study into the perception of display brightness as related to the physical size and distance of the screen from the observer. Brightness perception is a complex topic, which is influenced by a number of lower- and higher-order factors-with empirical evidence from the cinema industry suggesting that display size may play a significant role. To test this hypothesis, we conducted a series of user studies exploring brightness perception for a range of displays and distances from the observer that span representative use scenarios. Our results suggest that retinal size is not sufficient to explain the range of discovered brightness variations, but is sufficient in combination with physical distance from the observer. The resulting model can be used as a step toward perceptually correcting image brightness perception based on target display parameters. This can be leveraged for energy management and the preservation of artistic intent. A pilot study suggests that adaptation luminance is an additional factor for the magnitude of the effect.
C1 [Chapiro, Alexandre; Kunkel, Timo; Atkins, Robin; Daly, Scott] Dolby Labs, 432 Lakeside Dr, Sunnyvale, CA 94085 USA.
C3 Dolby Laboratories, Inc.
RP Chapiro, A (corresponding author), Dolby Labs, 432 Lakeside Dr, Sunnyvale, CA 94085 USA.
EM alex@chapiro.net; tkunk@dolby.com; ratki@dolby.com; sdaly@dolby.com
OI Chapiro, Alexandre/0000-0002-7367-0131
CR Allred Sarah R., 2012, J VISION, V12, P7
   [Anonymous], 2007, SPATIAL VISION
   ANTER KF, 2000, THESIS
   Bradley A, 2014, OPHTHAL PHYSL OPT, V34, P309, DOI 10.1111/opo.12114
   Braun KM, 1996, COLOR RES APPL, V21, P6, DOI 10.1002/(SICI)1520-6378(199602)21:1<6::AID-COL1>3.0.CO;2-#
   BURGH P, 1962, Q J EXP PSYCHOL, V14, P89, DOI 10.1080/17470216208416518
   BURNHAM RW, 1951, AM J PSYCHOL, V64, P521, DOI 10.2307/1418191
   BURNHAM RW, 1952, AM J PSYCHOL, V65, P27, DOI 10.2307/1418825
   Gilchrist A, 1999, PSYCHOL REV, V106, P795, DOI 10.1037/0033-295X.106.4.795
   Han Taeseong, 2016, COL IM C, V24
   Haun AM, 2013, J VISION, V13, DOI 10.1167/13.13.3
   HUBNER M, 1988, PERCEPT PSYCHOPHYS, V43, P319, DOI 10.3758/BF03208801
   Hunt Robert William Gainer, 1998, COLOR RES APPL, V23, P116
   Kim MH, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1944846.1944853
   Kutas G, 2008, COLOR RES APPL, V33, P45, DOI 10.1002/col.20367
   Kutas G, 2004, CGIV 2004: SECOND EUROPEAN CONFERENCE ON COLOR IN GRAPHICS, IMAGING, AND VISION - CONFERENCE PROCEEDINGS, P70
   McCarthy Sean T., 2016, P CABL TEC EXP SOC C
   Nezamabadi M, 2005, THIRTEENTH COLOR IMAGING CONFERENCE, FINAL PROGRAM AND PROCEEDINGS, P79
   Nezamabadi M, 2007, PROC SPIE, V6493, DOI 10.1117/12.704282
   Nezamabadi M, 2006, J SOC INF DISPLAY, V14, P773, DOI 10.1889/1.2358571
   Noland K. C., 2017, 2017 9 INT C QUAL MU, P1, DOI [10.1109/QoMEX.2017.7965633, DOI 10.1109/QOMEX.2017.7965633]
   OPPENHEIM AV, 1981, P IEEE, V69, P529, DOI 10.1109/PROC.1981.12022
   PELI E, 1990, J OPT SOC AM A, V7, P2032, DOI 10.1364/JOSAA.7.002032
   Poynton C, 2012, DIGITAL VIDEO AND HD: ALGORITHMS AND INTERFACES, 2ND EDITION, P1
   Purves D, 2004, PSYCHOL REV, V111, P142, DOI 10.1037/0033-295X.111.1.142
   STEVENS JC, 1963, J OPT SOC AM, V53, P375, DOI 10.1364/JOSA.53.000375
   Tan Mun Chieng, 2015, Clin Nutr Res, V4, P18, DOI 10.7762/cnr.2015.4.1.18
   WATSON AB, 1983, PERCEPT PSYCHOPHYS, V33, P113, DOI 10.3758/BF03202828
   WATSON AB, 1983, NATURE, V302, P419, DOI 10.1038/302419a0
   Xiao K, 2004, CGIV 2004: SECOND EUROPEAN CONFERENCE ON COLOR IN GRAPHICS, IMAGING, AND VISION - CONFERENCE PROCEEDINGS, P12
   Xiao K, 2003, ELEVENTH COLOR IMAGING CONFERENCE: COLOR SCIENCE AND ENGINEERING - SYSTEMS, TECHNOLOGIES, APPLICATIONS, P308
   Xiao KD, 2011, COLOR RES APPL, V36, P201, DOI 10.1002/col.20610
NR 32
TC 5
Z9 5
U1 1
U2 13
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD AUG
PY 2018
VL 15
IS 3
AR 18
DI 10.1145/3190346
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA GY2UV
UT WOS:000448400300004
DA 2024-07-18
ER

PT J
AU Kelly, JW
   Cherep, LA
   Klesel, B
   Siegel, ZD
   George, S
AF Kelly, Jonathan W.
   Cherep, Lucia A.
   Klesel, Brenna
   Siegel, Zachary D.
   George, Seth
TI Comparison of Two Methods for Improving Distance Perception in Virtual
   Reality
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Depth perception; stereoscopic displays; virtual environments
ID EGOCENTRIC DISTANCE; PERCEIVED DISTANCE; ENVIRONMENTS; SIZE;
   RECALIBRATION; TRANSFERS; GRAPHICS; QUALITY; WALKING
AB Distance is commonly underperceived in virtual environments (VEs) compared to real environments. Past work suggests that displaying a replica VE based on the real surrounding environment leads to more accurate judgments of distance, but that work has lacked the necessary control conditions to firmly make this conclusion. Other research indicates that walking through a VE with visual feedback improves judgments of distance and size. This study evaluated and compared those two methods for improving perceived distance in VEs. All participants experienced a replica VE based on the real lab. In one condition, participants visually previewed the real lab prior to experiencing the replica VE, and in another condition they did not. Participants performed blind-walking judgments of distance and also judgments of size in the replica VE before and after walking interaction. Distance judgments were more accurate in the preview compared to no preview condition, but size judgments were unaffected by visual preview. Distance judgments and size judgments increased after walking interaction, and the improvement was larger for distance than for size judgments. After walking interaction, distance judgments did not differ based on visual preview, and walking interaction led to a larger improvement in judged distance than did visual preview. These data suggest that walking interaction may be more effective than visual preview as a method for improving perceived space in a VE.
C1 [Kelly, Jonathan W.; Cherep, Lucia A.; Klesel, Brenna; Siegel, Zachary D.; George, Seth] Iowa State Univ, Dept Psychol, W112 Lagomarcino Hall,901 Stange Rd, Ames, IA 50011 USA.
C3 Iowa State University
RP Kelly, JW (corresponding author), Iowa State Univ, Dept Psychol, W112 Lagomarcino Hall,901 Stange Rd, Ames, IA 50011 USA.
EM jonkelly@iastate.edu; lacherep@iastate.edu; bklesel@iastate.edu;
   zsiegel@iastate.edu; sethg13@gmail.com
RI Kelly, Jonathan W/A-4793-2013
OI Siegel, Zachary/0000-0002-3700-8140
CR [Anonymous], P 6 S APPL PERC GRAP
   Brenner E, 1999, VISION RES, V39, P975, DOI 10.1016/S0042-6989(98)00162-X
   Buck LE, 2018, ACM T APPL PERCEPT, V15, DOI 10.1145/3196885
   Creem-Regehr SH, 2016, SAP 2015: ACM SIGGRAPH SYMPOSIUM ON APPLIED PERCEPTION, P47, DOI 10.1145/2804408.2804422
   Creem-Regehr SH, 2015, PSYCHOL LEARN MOTIV, V62, P195, DOI 10.1016/bs.plm.2014.09.006
   Creem-Regehr SH, 2005, PERCEPTION, V34, P191, DOI 10.1068/p5144
   Durgin FH, 2011, ATTEN PERCEPT PSYCHO, V73, P1856, DOI 10.3758/s13414-011-0143-5
   EPSTEIN W, 1961, PSYCHOL BULL, V58, P491, DOI 10.1037/h0042260
   Gallistel CR, 2009, PSYCHOL REV, V116, P439, DOI 10.1037/a0015251
   GILINSKY AS, 1951, PSYCHOL REV, V58, P460, DOI 10.1037/h0061505
   GOGEL WC, 1985, PERCEPT PSYCHOPHYS, V37, P17, DOI 10.3758/BF03207134
   Grechkin TY, 2010, ACM T APPL PERCEPT, V7, DOI 10.1145/1823738.1823744
   Hutchison JJ, 2006, SPAN J PSYCHOL, V9, P332, DOI 10.1017/S1138741600006235
   Interrante V., 2006, P IEEE VIRT REAL C
   Kelly JW, 2014, IEEE T VIS COMPUT GR, V20, P588, DOI 10.1109/TVCG.2014.36
   Kelly JW, 2013, ATTEN PERCEPT PSYCHO, V75, P1473, DOI 10.3758/s13414-013-0503-4
   Knapp JM, 2004, PRESENCE-TELEOP VIRT, V13, P572, DOI 10.1162/1054746042545238
   Kunz BR, 2009, ATTEN PERCEPT PSYCHO, V71, P1284, DOI 10.3758/APP.71.6.1284
   Li BC, 2016, SAP 2015: ACM SIGGRAPH SYMPOSIUM ON APPLIED PERCEPTION, P55, DOI 10.1145/2804408.2804427
   Li Bochao., 2014, P ACM S APPL PERCEPT, P91
   Loomis JM, 2003, VIRTUAL AND ADAPTIVE ENVIRONMENTS: APPLICATIONS, IMPLICATIONS, AND HUMAN PERFORMANCE ISSUES, P21
   Mohler BettyJ., 2006, P 3 S APPL PERCEPTIO, P9, DOI [10.1145/1140491.1140493, DOI 10.1145/1140491.1140493]
   Renner RS, 2013, ACM COMPUT SURV, V46, DOI 10.1145/2543581.2543590
   Richardson AR, 2007, HUM FACTORS, V49, P507, DOI 10.1518/001872007X200139
   Richardson AR, 2005, APPL COGNITIVE PSYCH, V19, P1089, DOI 10.1002/acp.1140
   Riecke B.E., 2009, Proceedings of the 6th Symposium on Applied Perception in Graphics and Visualization, P15
   Sedgwick HA., 1986, HDB PERCEPTION HUMAN, P1
   Siegel ZD, 2017, J EXP PSYCHOL HUMAN, V43, P1805, DOI 10.1037/xhp0000401
   Siegel ZD, 2017, ATTEN PERCEPT PSYCHO, V79, P39, DOI 10.3758/s13414-016-1243-z
   Thompson WB, 2004, PRESENCE-TELEOP VIRT, V13, P560, DOI 10.1162/1054746042545292
   Waller D, 2008, J EXP PSYCHOL-APPL, V14, P61, DOI 10.1037/1076-898X.14.1.61
   Willemsen P, 2009, ACM T APPL PERCEPT, V6, DOI 10.1145/1498700.1498702
   Young M. K., 2014, P ACM S APPL PERCEPT, P83
   Ziemer CJ, 2009, ATTEN PERCEPT PSYCHO, V71, P1095, DOI 10.3758/APP.71.5.1096
NR 34
TC 23
Z9 26
U1 0
U2 16
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD APR
PY 2018
VL 15
IS 2
AR 11
DI 10.1145/3165285
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA GH5YV
UT WOS:000433515400004
OA Green Published
DA 2024-07-18
ER

PT J
AU Rummukainen, O
   Mendonça, C
AF Rummukainen, Olli
   Mendonca, Catarina
TI Reproducing Reality: Multimodal Contributions in Natural Scene
   Discrimination
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Audiovisual; immersive environment; spatial sound; sensory integration;
   natural scenes
ID INTEGRATION; AUDIO; PERCEPTION
AB Most research on multisensory processing focuses on impoverished stimuli and simple tasks. In consequence, very little is known about the sensory contributions in the perception of real environments. Here, we presented 23 participants with paired comparison tasks, where natural scenes were discriminated in three perceptually meaningful attributes: movement, openness, and noisiness. The goal was to assess the auditory and visual modality contributions in scene discrimination with short (<= 500ms) natural scene exposures. The scenes were reproduced in an immersive audiovisual environment with 3D sound and surrounding visuals. Movement and openness were found to be mainly visual attributes with some input from auditory information. In some scenes, the auditory system was able to derive information about movement and openness that was comparable with audiovisual condition already after 500ms stimulation. Noisiness was mainly auditory, but visual information was found to have a facilitatory role in a few scenes. The sensory weights were highly imbalanced in favor of the stronger modality, but the weaker modality was able to affect the bimodal estimate in some scenes.
C1 [Rummukainen, Olli; Mendonca, Catarina] Aalto Univ, Sch Elect Engn, Otakaari 5 A, Espoo 02150, Finland.
C3 Aalto University
RP Rummukainen, O (corresponding author), Aalto Univ, Sch Elect Engn, Otakaari 5 A, Espoo 02150, Finland.
EM olli.rummukainen@aalto.fi; catarina.mendonca@aalto.fi
RI Mendonca, Catarina/L-8877-2013
OI Mendonca, Catarina/0000-0002-4333-898X
FU European Research Council under the European Community's Seventh
   Framework Programme (FP7) / ERC [240453]; Academy of Finland [266239];
   European Union's Horizon 2020 research and innovation programme under
   the Marie Sklodowska-Curie [659114]; Marie Curie Actions (MSCA) [659114]
   Funding Source: Marie Curie Actions (MSCA); Academy of Finland (AKA)
   [266239] Funding Source: Academy of Finland (AKA)
FX The research leading to these results has received funding from the
   European Research Council under the European Community's Seventh
   Framework Programme (FP7/2007-2013) / ERC grant agreement no. 240453 and
   from the Academy of Finland project Audiovisual Space no. 266239. This
   project has received funding from the European Union's Horizon 2020
   research and innovation programme under the Marie Sklodowska-Curie grant
   no. 659114.
CR Alais D, 2004, CURR BIOL, V14, P257, DOI 10.1016/j.cub.2004.01.029
   Armstrong Alan, 2014, EXPT BRAIN RES, V232, P11
   Bates D, 2015, J STAT SOFTW, V67, P1, DOI 10.18637/jss.v067.i01
   Chen YC, 2011, FRONT PSYCHOL, V2, DOI 10.3389/fpsyg.2011.00212
   Chen YC, 2010, COGNITION, V114, P389, DOI 10.1016/j.cognition.2009.10.012
   Cheng K, 2007, PSYCHOL BULL, V133, P625, DOI 10.1037/0033-2909.133.4.625
   COLAVITA FB, 1974, PERCEPT PSYCHOPHYS, V16, P409, DOI 10.3758/BF03203962
   Conrad V, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0070710
   De Gelder B, 2003, TRENDS COGN SCI, V7, P460, DOI 10.1016/j.tics.2003.08.014
   Doehrmann O, 2008, BRAIN RES, V1242, P136, DOI 10.1016/j.brainres.2008.03.071
   Ecker AJ, 2005, PERCEPTION, V34, P59, DOI 10.1068/p5368
   Ernst MO, 2004, TRENDS COGN SCI, V8, P162, DOI 10.1016/j.tics.2004.02.002
   Evans KK, 2010, J VISION, V10, DOI 10.1167/10.1.6
   Gomez Bolanos  J., 2012, AUD ENG SOC 132 CONV, P1
   Hecht D, 2009, EXP BRAIN RES, V193, P307, DOI 10.1007/s00221-008-1626-z
   Koene A, 2007, J VISION, V7, DOI 10.1167/7.11.14
   LANDIS JR, 1977, BIOMETRICS, V33, P159, DOI 10.2307/2529310
   Laurienti PJ, 2004, EXP BRAIN RES, V158, P405, DOI 10.1007/s00221-004-1913-2
   López-Moliner J, 2007, J VISION, V7, DOI 10.1167/7.13.11
   Mendonça C, 2011, EXP BRAIN RES, V213, P185, DOI 10.1007/s00221-011-2620-4
   Mendonca Catarina, 2015, 21 INT C AUD DISPL I
   Pérez-Bellido A, 2013, J NEUROPHYSIOL, V109, P1065, DOI 10.1152/jn.00226.2012
   Pheasant RJ, 2010, J ENVIRON PSYCHOL, V30, P501, DOI 10.1016/j.jenvp.2010.03.006
   Poli A, 2011, ARCHAEA, V2011, DOI 10.1155/2011/693253
   Pournelle G. H., 1953, Journal of Mammalogy, V34, P133, DOI 10.1890/0012-9658(2002)083[1421:SDEOLC]2.0.CO;2
   Pulkki V, 2007, J AUDIO ENG SOC, V55, P503
   Rummukainen O, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0095848
   Spence C, 2003, CURR BIOL, V13, pR519, DOI 10.1016/S0960-9822(03)00445-7
   Tan JS, 2015, J EXP PSYCHOL HUMAN, V41, P1325, DOI 10.1037/xhp0000074
   Treisman M, 1998, PSYCHOL METHODS, V3, P252, DOI 10.1037/1082-989X.3.2.252
   Vilkamo J, 2009, J AUDIO ENG SOC, V57, P709
   Wozny DR, 2008, J VISION, V8, DOI 10.1167/8.3.24
   Yuval-Greenberg S, 2009, EXP BRAIN RES, V193, P603, DOI 10.1007/s00221-008-1664-6
NR 33
TC 1
Z9 1
U1 0
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD AUG
PY 2016
VL 14
IS 1
AR 1
DI 10.1145/2915917
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA DV4EB
UT WOS:000382876900001
DA 2024-07-18
ER

PT J
AU Fan, SJ
   Wang, RD
   Ng, TT
   Tan, CYC
   Herberg, JS
   Koenig, BL
AF Fan, Shaojing
   Wang, Rangding
   Ng, Tian-Tsong
   Tan, Cheston Y. -C.
   Herberg, Jonathan S.
   Koenig, Bryan L.
TI Human Perception of Visual Realism for Photo and Computer-Generated Face
   Images
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Design; Experimentation; Visual realism; human perception; holistic face
   perception; piecemeal face perception
ID CONFIGURAL INFORMATION; OWN-RACE; INVERTED FACES; RECOGNITION;
   INVERSION; COMPONENT; WHOLE; SPECIFICITY; MECHANISMS; ADVANTAGE
AB Computer-generated (CG) face images are common in video games, advertisements, and other media. CG faces vary in their degree of realism, a factor that impacts viewer reactions. Therefore, efficient control of visual realism of face images is important. Efficient control is enabled by a deep understanding of visual realism perception: the extent to which viewers judge an image as a real photograph rather than a CG image. Across two experiments, we explored the processes involved in visual realism perception of face images. In Experiment 1, participants made visual realism judgments on original face images, inverted face images, and images of faces that had the top and bottom halves misaligned. In Experiment 2, participants made visual realism judgments on original face images, scrambled faces, and images that showed different parts of faces. Our findings indicate that both holistic and piecemeal processing are involved in visual realism perception of faces, with holistic processing becoming more dominant when resolution is lower. Our results also suggest that shading information is more important than color for holistic processing, and that inversion makes visual realism judgments harder for realistic images but not for unrealistic images. Furthermore, we found that eyes are the most influential face part for visual realism, and face context is critical for evaluating realism of face parts. To the best of our knowledge, this work is a first realism-centric study attempting to bridge the human perception of visual realism on face images with general face perception tasks.
C1 [Fan, Shaojing; Wang, Rangding] Ningbo Univ, Coll Informat & Comp Sci, Ningbo 315000, Zhejiang, Peoples R China.
   [Ng, Tian-Tsong; Tan, Cheston Y. -C.] Inst Infocomm Res, Singapore, Singapore.
   [Herberg, Jonathan S.] Natl Univ Singapore, Inst High Performance Comp, Singapore 117548, Singapore.
   [Koenig, Bryan L.] Lindenwood Univ, St Charles, MO 63301 USA.
C3 Ningbo University; Agency for Science Technology & Research (A*STAR);
   A*STAR - Institute for Infocomm Research (I2R); National University of
   Singapore; Agency for Science Technology & Research (A*STAR); A*STAR -
   Institute of High Performance Computing (IHPC)
RP Fan, SJ (corresponding author), Ningbo Univ, Coll Informat & Comp Sci, Ningbo 315000, Zhejiang, Peoples R China.
EM fanshaojing@gmail.com; wangrangding@nbu.edu.cn; ttng@i2r.a-star.edu.sg;
   cheston-tan@i2r.a-star.edu.sg; herbergjs@ihpc.a-star.edu.sg;
   bkoenig@lindenwood.edu
RI Fan, Shaojing/AAG-3168-2019
OI Fan, Shaojing/0000-0002-7744-1133; Ng, Tian Tsong/0000-0002-6713-2310
FU National Natural Science Foundation of China [61301247]; Ningbo Natural
   Science Foundation of China [2013A610059]; Open Fund of Zhejiang
   Provincial Top Key Discipline of Information and Communication
   Engineering [XKXL1313]
FX This work was supported in part by the National Natural Science
   Foundation of China #61301247, Ningbo Natural Science Foundation of
   China #2013A610059, and Open Fund of Zhejiang Provincial Top Key
   Discipline of Information and Communication Engineering #XKXL1313.
CR Akinyinka Omigbodun and Garrison Cottrell, 2013, J VISION, V13
   Amishav R, 2010, PSYCHON B REV, V17, P743, DOI 10.3758/PBR.17.5.743
   [Anonymous], SIGGRAPH ASIA 2012 T
   [Anonymous], 2013, DIGITAL IMAGE FORENS
   [Anonymous], 2005, P 2 S APP PERC GRAPH, DOI DOI 10.1145/1080402.1080425
   Bailey Rosemary A., 2008, DESIGN COMP EXPT, V25
   Bernstein MJ, 2007, PSYCHOL SCI, V18, P706, DOI 10.1111/j.1467-9280.2007.01964.x
   Blanz V, 1999, COMP GRAPH, P187, DOI 10.1145/311535.311556
   BROWN E, 1993, PERCEPTION, V22, P829, DOI 10.1068/p220829
   Bruce V., 2012, Face perception
   Bukach CM, 2006, TRENDS COGN SCI, V10, P159, DOI 10.1016/j.tics.2006.02.004
   Cabeza R, 2000, PSYCHOL SCI, V11, P429, DOI 10.1111/1467-9280.00283
   Cecchini M, 2013, SOC NEUROSCI-UK, V8, P314, DOI 10.1080/17470919.2013.797020
   Collishaw SM, 2000, PERCEPTION, V29, P893, DOI 10.1068/p2949
   Crookes K, 2013, FRONT PSYCHOL, V4, DOI 10.3389/fpsyg.2013.00029
   Deng Z.et., 2008, Proc. of 2008 ACM SIGGRAPH/Eurographics Symposium on Computer Animation, P67
   Durand K, 2007, J EXP CHILD PSYCHOL, V97, P14, DOI 10.1016/j.jecp.2006.12.001
   Farah MJ, 1998, PSYCHOL REV, V105, P482, DOI 10.1037/0033-295X.105.3.482
   Farid H, 2012, DIGIT INVEST, V8, P226, DOI 10.1016/j.diin.2011.06.003
   Favelle SK, 2012, FRONT PSYCHOL, V3, DOI 10.3389/fpsyg.2012.00563
   Galton F., 1883, INQUIRIES HUMAN FACU
   Gao ZF, 2011, J EXP PSYCHOL HUMAN, V37, P1051, DOI 10.1037/a0023091
   Garces E, 2012, COMPUT GRAPH FORUM, V31, P1415, DOI 10.1111/j.1467-8659.2012.03137.x
   Gauthier I, 1997, VISION RES, V37, P1673, DOI 10.1016/S0042-6989(96)00286-6
   Gilad S, 2009, P NATL ACAD SCI USA, V106, P5353, DOI 10.1073/pnas.0812396106
   Goffaux V, 2006, J EXP PSYCHOL HUMAN, V32, P1023, DOI 10.1037/0096-1523.32.4.1023
   Goffaux V, 2013, FRONT PSYCHOL, V3, DOI 10.3389/fpsyg.2012.00604
   Goffaux V, 2011, CEREB CORTEX, V21, P467, DOI 10.1093/cercor/bhq112
   Golby AJ, 2001, NAT NEUROSCI, V4, P845, DOI 10.1038/90565
   Gold JM, 2012, PSYCHOL SCI, V23, P427, DOI 10.1177/0956797611427407
   Gorban AN, 2010, PHYSICA A, V389, P3193, DOI 10.1016/j.physa.2010.03.035
   Grosse R, 2009, IEEE I CONF COMP VIS, P2335, DOI 10.1109/ICCV.2009.5459428
   Hancock PJB, 2000, TRENDS COGN SCI, V4, P330, DOI 10.1016/S1364-6613(00)01519-9
   Haxby JV, 2000, TRENDS COGN SCI, V4, P223, DOI 10.1016/S1364-6613(00)01482-0
   Hayward WG, 2008, COGNITION, V106, P1017, DOI 10.1016/j.cognition.2007.04.002
   Hills PJ, 2012, BRIT J PSYCHOL, V103, P520, DOI 10.1111/j.2044-8295.2011.02091.x
   Hills PJ, 2011, J EXP PSYCHOL HUMAN, V37, P1396, DOI 10.1037/a0024247
   Ingvalson EM, 2005, PERCEPT PSYCHOPHYS, V67, P14, DOI 10.3758/BF03195010
   Itier RJ, 2002, NEUROIMAGE, V15, P353, DOI 10.1006/nimg.2001.0982
   Itier RJ, 2007, J COGNITIVE NEUROSCI, V19, P1815, DOI 10.1162/jocn.2007.19.11.1815
   Jack RE, 2009, CURR BIOL, V19, P1543, DOI 10.1016/j.cub.2009.07.051
   Kanwisher N, 2000, NAT NEUROSCI, V3, P759, DOI 10.1038/77664
   Kanwisher N, 2006, PHILOS T R SOC B, V361, P2109, DOI 10.1098/rstb.2006.1934
   Kasinski A., 2008, Image Processing and Communications, V13, P59
   Kimchi R, 2010, VIS COGN, V18, P1034, DOI 10.1080/13506281003619986
   Laguesse R, 2013, PERCEPTION, V42, P1013, DOI 10.1068/p7534
   Larkin M., 2011, Proceedings of the ACM SIGGRAPH Symposium on Applied Perception in Graphics and Visualization, APGV '11, P93
   Le Grand R, 2004, PSYCHOL SCI, V15, P762, DOI 10.1111/j.0956-7976.2004.00753.x
   Leder H, 2000, Q J EXP PSYCHOL-A, V53, P513, DOI 10.1080/027249800390583
   Liu CH, 2006, VISION RES, V46, P768, DOI 10.1016/j.visres.2005.10.008
   Lobmaier JS, 2007, PERCEPTION, V36, P1660, DOI 10.1068/p5642
   MacDorman KF, 2006, INTERACT STUD, V7, P297, DOI 10.1075/is.7.3.03mac
   MacDorman KF, 2009, COMPUT HUM BEHAV, V25, P695, DOI 10.1016/j.chb.2008.12.026
   Maurer D, 2002, TRENDS COGN SCI, V6, P255, DOI 10.1016/S1364-6613(02)01903-4
   McCarthy G, 1997, J COGNITIVE NEUROSCI, V9, P605, DOI 10.1162/jocn.1997.9.5.605
   McDonnell R, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185587
   McKeeff TJ, 2010, COGNITION, V117, P355, DOI 10.1016/j.cognition.2010.09.002
   McKone E, 2007, TRENDS COGN SCI, V11, P8, DOI 10.1016/j.tics.2006.11.002
   McKone E, 2013, FRONT PSYCHOL, V4, DOI 10.3389/fpsyg.2013.00033
   Meissner CA, 2001, PSYCHOL PUBLIC POL L, V7, P3, DOI 10.1037//1076-8971.7.1.3
   MEYER GW, 1986, ACM T GRAPHIC, V5, P30, DOI 10.1145/7529.7920
   Michel C, 2006, PSYCHOL SCI, V17, P608, DOI 10.1111/j.1467-9280.2006.01752.x
   Mori M., 1970, Energy, V7, P33, DOI [DOI 10.1109/MRA.2012.2192811, 10.1109/MRA.2012.2192811]
   Moscovitch M, 1997, J COGNITIVE NEUROSCI, V9, P555, DOI 10.1162/jocn.1997.9.5.555
   Natu V, 2011, NEUROIMAGE, V54, P2547, DOI 10.1016/j.neuroimage.2010.10.006
   Nelson CA, 2001, INFANT CHILD DEV, V10, P3, DOI 10.1002/icd.239
   Nowak KL, 2008, PRESENCE-TELEOP VIRT, V17, P256, DOI 10.1162/pres.17.3.256
   Ohm J.R., 2004, SIG COM TEC
   Paolacci G, 2010, JUDGM DECIS MAK, V5, P411
   Piepers DW, 2012, FRONT PSYCHOL, V3, DOI 10.3389/fpsyg.2012.00559
   Rademacher P, 2001, SPRING EUROGRAP, P235
   Ramanarayanan G, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1276377.1276472, 10.1145/1239451.1239527]
   Rhodes G, 2006, PSYCHON B REV, V13, P499, DOI 10.3758/BF03193876
   Richler JJ, 2012, FRONT PSYCHOL, V3, DOI 10.3389/fpsyg.2012.00553
   Riesenhuber M, 1999, NAT NEUROSCI, V2, P1019, DOI 10.1038/14819
   Rossion B, 2002, PSYCHOL SCI, V13, P250, DOI 10.1111/1467-9280.00446
   Rossion B, 2013, VIS COGN, V21, P139, DOI 10.1080/13506285.2013.772929
   Rossion B, 2012, BRAIN COGNITION, V79, P138, DOI 10.1016/j.bandc.2012.01.001
   Ryan KF, 2013, PERCEPTION, V42, P330, DOI 10.1068/p7359
   Sadr J, 2003, PERCEPTION, V32, P285, DOI 10.1068/p5027
   Schwaninger A, 2003, DEVELOPMENT OF FACE PROCESSING, P81
   Schwaninger A, 2003, VISION RES, V43, P1501, DOI 10.1016/S0042-6989(03)00171-8
   Schwaninger A, 2011, VISION RES, V51, P969, DOI 10.1016/j.visres.2011.02.006
   Schwaninger A, 2009, COGNITIVE SCI, V33, P1413, DOI 10.1111/j.1551-6709.2009.01059.x
   SERGENT J, 1984, BRIT J PSYCHOL, V75, P221, DOI 10.1111/j.2044-8295.1984.tb01895.x
   Sinha P, 2006, P IEEE, V94, P1948, DOI 10.1109/JPROC.2006.884093
   Song YY, 2013, J COGNITIVE NEUROSCI, V25, P1261, DOI 10.1162/jocn_a_00406
   Tan C., 2013, MITCSAILTR2013004
   TANAKA JW, 1993, Q J EXP PSYCHOL-A, V46, P225, DOI 10.1080/14640749308401045
   VALENTINE T, 1988, BRIT J PSYCHOL, V79, P471, DOI 10.1111/j.2044-8295.1988.tb02747.x
   Van Belle G, 2010, J VISION, V10, DOI 10.1167/10.5.10
   Vangorp P, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461971
   Wallraven C, 2008, ACM T APPL PERCEPT, V4, DOI 10.1145/1278760.1278764
   Wang RS, 2012, PSYCHOL SCI, V23, P169, DOI 10.1177/0956797611420575
   Watson TL, 2013, FRONT PSYCHOL, V4, DOI 10.3389/fpsyg.2013.00414
   Wickens T. D., 2001, Elementary signal detection theory
   Wong Alan C.-N., 2012, J VISION, V12
   Wong YK, 2012, J EXP PSYCHOL GEN, V141, P682, DOI 10.1037/a0027822
   Xu YD, 2005, CEREB CORTEX, V15, P1234, DOI 10.1093/cercor/bhi006
   Xue S, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185580
   YIN RK, 1969, J EXP PSYCHOL, V81, P141, DOI 10.1037/h0027474
   YOUNG AW, 1987, PERCEPTION, V16, P747, DOI 10.1068/p160747
NR 102
TC 12
Z9 12
U1 3
U2 30
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD MAY
PY 2014
VL 11
IS 2
AR 7
DI 10.1145/2620030
PG 21
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA AO3OJ
UT WOS:000341241100003
DA 2024-07-18
ER

PT J
AU Leroy, L
   Fuchs, P
   Moreau, G
AF Leroy, Laure
   Fuchs, Philippe
   Moreau, Guillaume
TI Real-Time Adaptive Blur for Reducing Eye Strain in Stereoscopic Displays
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Human Factors; Blur; visual fatigue; tracked point of view; virtual
   reality; stereoscopy
AB Stereoscopic devices are widely used (immersion-based working environments, stereoscopically-viewed movies, auto-stereoscopic screens). In some instances, exposure to stereoscopic immersion techniques can be lengthy, and so eye strain sets in. We propose a method for reducing eye strain induced by stereoscopic vision. After reviewing sources of eye strain linked to stereoscopic vision, we focus on one of these sources: images with high frequency content associated with large disparities. We put forward an algorithm for removing the irritating high frequencies in high horizontal disparity zones (i.e., for virtual objects appearing far from the real screen level). We elaborate on our testing protocol to establish that our image processing method reduces eye strain caused by stereoscopic vision, both objectively and subjectively. We subsequently quantify the positive effects of our algorithm on the relief of eye strain and discuss further research perspectives.
C1 [Moreau, Guillaume] LUNAM Univ, Ecole Cent Nantes, CERMA UMR 1563, Nantes, France.
   [Leroy, Laure; Fuchs, Philippe] Mines ParisTech, Robot Ctr, Paris, France.
C3 Nantes Universite; Ecole Centrale de Nantes; Centre National de la
   Recherche Scientifique (CNRS); CNRS - Institute for Humanities & Social
   Sciences (INSHS); Universite PSL; MINES ParisTech
RP Moreau, G (corresponding author), LUNAM Univ, Ecole Cent Nantes, CERMA UMR 1563, Nantes, France.
EM guillaume.moreau@ec-nantes.fr
RI Moreau, Guillaume/I-3153-2013
OI Moreau, Guillaume/0000-0003-2215-1865
CR [Anonymous], P SPIE
   Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014
   BENJAMIN W, 2007, CLIN REFRACTION
   Blohm W., 1997, Journal of the Society for Information Display, V5, P307, DOI 10.1889/1.1985167
   BLUM T., 2010, P IEEE SCI TECHN INT
   Brooker JP, 2001, P SOC PHOTO-OPT INS, V4297, P408, DOI 10.1117/12.430841
   Fuchs P., 2011, VIRTUAL REALITY CONC
   Harris Mark., 2005, ACM SIGGRAPH 2005 CO
   Hillaire S, 2008, IEEE COMPUT GRAPH, V28, P47, DOI 10.1109/MCG.2008.113
   Hillaire S, 2008, IEEE VIRTUAL REALITY 2008, PROCEEDINGS, P47
   KENNY A., 2005, P EUROPEAN C MODELLI, P146
   LAMBOOIJ M, 2009, P SPIE, V7237
   Lemmer N, 2003, P SOC PHOTO-OPT INS, V5006, P283, DOI 10.1117/12.474118
   LEROY L., 2008, P INT C ART REAL TEL, P144
   NEVEU P, 2008, THESIS ECOLE MINES P
   PERRIN J., 1998, P SPIE VIS INF PROC, P3387
   Rushton SK, 1999, APPL ERGON, V30, P69, DOI 10.1016/S0003-6870(98)00044-1
   VALYUS NA, 1962, STEREOSCOPY
   Viola P, 2001, PROC CVPR IEEE, P511, DOI 10.1109/cvpr.2001.990517
   Wopking M., 1995, Journal of the Society for Information Display, V3, P101
   Yano S, 2002, DISPLAYS, V23, P191, DOI 10.1016/S0141-9382(02)00038-0
NR 21
TC 15
Z9 17
U1 0
U2 11
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD JUN
PY 2012
VL 9
IS 2
AR 9
DI 10.1145/2207216.2207220
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 959PR
UT WOS:000305325800004
DA 2024-07-18
ER

PT J
AU Riecke, BE
   Feuereissen, D
   Rieser, JJ
AF Riecke, Bernhard E.
   Feuereissen, Daniel
   Rieser, John J.
TI Auditory Self-Motion Simulation is Facilitated by Haptic and Vibrational
   Cues Suggesting the Possibility of Actual Motion
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Experimentation; Human factors; Measurement; Self-motion illusions;
   circular vection; auditory vection; self-motion simulation; spatial
   sound; individualized binaural recordings; HRTF; human factors;
   psychophysics; virtual reality; cue-integration; vibrations;
   higher-level/cognitive influences
ID VECTION
AB Sound fields rotating around stationary blindfolded listeners sometimes elicit auditory circular vection, the illusion that the listener is physically rotating. Experiment 1 investigated whether auditory circular vection depends on participants' situational awareness of "movability," that is, whether they sense/know that actual motion is possible or not. While previous studies often seated participants on movable chairs to suspend the disbelief of self-motion, it has never been investigated whether this does, in fact, facilitate auditory vection. To this end, 23 blindfolded participants were seated on a hammock chair with their feet either on solid ground ("movement impossible") or suspended ("movement possible") while listening to individualized binaural recordings of two sound sources rotating synchronously at 60/s. Although participants never physically moved, situational awareness of movability facilitated auditory vection. Moreover, adding slight vibrations like the ones resulting from actual chair rotation increased the frequency and intensity of vection. Experiment 2 extended these findings and showed that nonindividualized binaural recordings were as effective in inducing auditory circular vection as individualized recordings. These results have important implications both for our theoretical understanding of self-motion perception and for the applied field of self-motion simulations, where vibrations, nonindividualized binaural sound, and the cognitive/perceptual framework of movability can typically be provided at minimal cost and effort.
C1 [Riecke, Bernhard E.; Feuereissen, Daniel; Rieser, John J.] Vanderbilt Univ, Nashville, TN USA.
C3 Vanderbilt University
RP Riecke, BE (corresponding author), Simon Fraser Univ, SIAT, 250-13450 102nd Ave, Surrey, BC V3T 0A3, Canada.
EM ber1@sfu.ca; feuereissen@gmail.com; j.rieser@vanderbilt.edu
RI Riecke, Bernhard E./C-6399-2011
FU NIMH [2-R01-MH57868]; NSF [HCC 0705863]; Max Planck Society; Simon
   Fraser University
FX This research was funded by NIMH Grant 2-R01-MH57868 and NSF Grant HCC
   0705863 both to Vanderbilt University, and by the Max Planck Society and
   by Simon Fraser University.
CR ANDERSEN GJ, 1985, J EXP PSYCHOL HUMAN, V11, P122, DOI 10.1037/0096-1523.11.2.122
   Banbury Simon., 2004, COGNITIVE APPROACH S
   Begault DurandR., 1994, 3-D sound for virtual reality and multimedia
   BEIERHOLM U, 2008, ADV NEURAL INF PROCE, V20, P81
   BERTHOZ A, 1975, EXP BRAIN RES, V23, P471
   Blauert J., 1997, SPATIAL HEARING PSYC
   Bles W., 1981, Attention and Performance IX, P47
   Cohen J., 1988, STAT POWER ANAL BEHA
   Dichgans Johannes, 1978, Perception, P755, DOI [DOI 10.1007/978-3-642-46354-9253F, DOI 10.1007/978-3-642-46354-9_25, DOI 10.1007/978-3-642-46354-925]
   Dodge R, 1923, J EXP PSYCHOL, V6, P107, DOI 10.1037/h0076105
   Endsley M.R., 2003, Designing for situation awareness: an approach to User-Centered design
   Ernst MO, 2002, NATURE, V415, P429, DOI 10.1038/415429a
   Ernst MO, 2004, TRENDS COGN SCI, V8, P162, DOI 10.1016/j.tics.2004.02.002
   FEUEREISSEN D, 2008, THESIS FURTWANGEN U
   GEKHMAN B I, 1991, Sensornye Sistemy, V5, P71
   HENNEBERT PE, 1960, J AUD RES, V1, P84
   Hettinger LJ, 2002, HUM FAC ER, P471
   Khasnis A, 2003, J Postgrad Med, V49, P169
   LACKNER JR, 1977, AVIAT SPACE ENVIR MD, V48, P129
   Langendijk EHA, 2002, J ACOUST SOC AM, V112, P1583, DOI 10.1121/1.1501901
   Larsson P., 2004, Proceedings of 7th Annual Workshop of Presence, P252
   LEPECQ JC, 1995, PERCEPTION, V24, P435, DOI 10.1068/p240435
   Loomis JM, 1999, MIXED REALITY, P201
   Mach E., 1875, GRUNDLINIEN LEHRE BE
   MARMEKARELSE AM, 1977, AGRESSOLOGIE, V18, P329
   Mergner T., 1990, Perception and Control of Self-Motion, P219
   Moller H, 1996, J AUDIO ENG SOC, V44, P451
   Palmisano S, 2003, PERCEPTION, V32, P97, DOI 10.1068/p3468
   Palmisano S, 2000, PERCEPTION, V29, P57, DOI 10.1068/p2990
   PAVARD B, 1977, PERCEPTION, V6, P529, DOI 10.1068/p060529
   Pick HL, 1999, J EXP PSYCHOL HUMAN, V25, P1179, DOI 10.1037/0096-1523.25.5.1179
   Riecke B. E., 2009, ACM T APPL PERCEPTIO, V6, P20
   Riecke B.E., 2005, ACM Transactions on Applied Perception (TAP), V2, P183, DOI [10.1145/1077399.1077401, DOI 10.1145/1077399.1077401]
   Riecke B.E., 2005, PRESENCE 2005, P49
   Riecke B.E., 2006, ACM T APPL PERCEPT, V3, DOI DOI 10.1145/1166087.1166091
   Riecke BE, 2005, P IEEE VIRT REAL ANN, P131
   RIECKE BE, 2005, P HCI INT 2005, P1
   RIECKE BE, 2006, P 7 INT MULT RES FOR
   RIECKE BE, 2008, P CYB WALK WORKSH SP
   Riecke BE, 2008, APGV 2008: PROCEEDINGS OF THE SYMPOSIUM ON APPLIED PERCEPTION IN GRAPHICS AND VISUALIZATION, P147
   RIESER JJ, 1995, J EXP PSYCHOL HUMAN, V21, P480, DOI 10.1037/0096-1523.21.3.480
   Sakamoto S., 2004, Acoustical Science and Technology, V25, P100, DOI 10.1250/ast.25.100
   Schulte-Pelkum J, 2008, THESIS RUHR U BOCHUM
   SCHULTEPELKUM J, 2004, P INT MULT RES FOR I
   SCHULTEPELKUM J, 2009, IMMERSED MEDIA EXPER
   Shilling RD, 2002, HUM FAC ER, P65
   VALJAMAE A, 2005, THESIS CHALMERS U TE
   Valjamae A., 2004, P 7 INT C PRESENCE, P141
   Valjamae A., 2007, PhD Thesis,
   VALJAMAE A, 2009, J ACOUST EN IN PRESS
   Väljamäe A, 2006, J AUDIO ENG SOC, V54, P954
   von Helmholtz H., 1867, HDB PHYSL OPTIK
   Warren R., 1990, PERCEPTION CONTROL S
   WONG SCP, 1981, PERCEPT PSYCHOPHYS, V30, P228, DOI 10.3758/BF03214278
   Wright WG, 2006, J VESTIBUL RES-EQUIL, V16, P23
NR 55
TC 16
Z9 16
U1 0
U2 7
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD AUG
PY 2009
VL 6
IS 3
AR 20
DI 10.1145/1577755.1577763
PG 22
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA 511ZS
UT WOS:000271212000008
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Klatzky, RL
   Wu, B
   Shelton, D
   Stetten, G
AF Klatzky, Roberta L.
   Wu, Bing
   Shelton, Damion
   Stetten, George
TI Effectiveness of Augmented-Reality Visualization versus Cognitive
   Mediation for Learning Actions in Near Space
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Performance; Experimentation; Human Factors; Perception; learning;
   augmented reality; motor control; spatial cognition
ID MOTOR-SKILL; CONTEXTUAL INTERFERENCE; BRAIN ACTIVATION; SPATIAL
   LANGUAGE; 3-D SOUND; ACQUISITION; ACCURACY; GUIDANCE; VISION; AREAS
AB The present study examined the impact of augmented-reality visualization, in comparison to conventional ultrasound (CUS), on the learning of ultrasound-guided needle insertion. Whereas CUS requires cognitive processes for localizing targets, our augmented-reality device, called the "sonic flashlight" (SF) enables direct perceptual guidance. Participants guided a needle to an ultrasound-localized target within opaque fluid. In three experiments, the SF showed higher accuracy and lower variability in aiming and endpoint placements than did CUS. The SF, but not CUS, readily transferred to new targets and starting points for action. These effects were evident in visually guided action (needle and target continuously visible) and visually directed action (target alone visible). The results have application to learning to visualize surgical targets through ultrasound.
C1 [Klatzky, Roberta L.] Carnegie Mellon Univ, Dept Psychol, Human Comp Interact Inst, Pittsburgh, PA 15213 USA.
   [Wu, Bing] Carnegie Mellon Univ, Inst Robot, Dept Psychol, Pittsburgh, PA 15213 USA.
   [Stetten, George] Univ Pittsburgh, Dept Bioengn, Pittsburgh, PA 15261 USA.
C3 Carnegie Mellon University; Carnegie Mellon University; Pennsylvania
   Commonwealth System of Higher Education (PCSHE); University of
   Pittsburgh
RP Klatzky, RL (corresponding author), Carnegie Mellon Univ, Dept Psychol, Human Comp Interact Inst, Pittsburgh, PA 15213 USA.
EM klatzky@andrew.cmu.edu
FU National Institutes of Health [R01EB00860-03]; National Science
   Foundation [0308096]; Direct For Computer & Info Scie & Enginr; Div Of
   Information & Intelligent Systems [0308096] Funding Source: National
   Science Foundation
FX This research was supported by National Institutes of Health Grant #
   R01EB00860-03 and National Science Foundation Grant # 0308096.
CR ANDERSON JR, 1982, PSYCHOL REV, V89, P369, DOI 10.1037/0033-295X.89.4.369
   BAJURA M, 1992, P SIGGRAPH 92, P203
   Chang WM, 2006, RADIOLOGY, V241, P771, DOI 10.1148/radiol.2413051595
   Chang WM, 2002, J ULTRAS MED, V21, P1131, DOI 10.7863/jum.2002.21.10.1131
   FIOYERLEA A, 2004, J NEUROPHYSIOL, V92, P2405
   Fitts P., 1964, CATEGORIES HUM LEARN, P243, DOI DOI 10.1016/C2013-0-12392-6
   Fuchs H, 1996, LECT NOTES COMPUT SC, V1131, P591, DOI 10.1007/BFb0047002
   Gazzaniga M.S., 2002, Cognitive neuroscience: The biology of the mind, V2nd
   JENKINS IH, 1994, J NEUROSCI, V14, P3775
   Klatzky RL, 2003, EXP BRAIN RES, V149, P48, DOI 10.1007/s00221-002-1334-z
   LEE TD, 1983, J EXP PSYCHOL LEARN, V9, P730, DOI 10.1037/0278-7393.9.4.730
   LOGAN GD, 1990, COGNITIVE PSYCHOL, V22, P1, DOI 10.1016/0010-0285(90)90002-L
   LOGAN GD, 1988, PSYCHOL REV, V95, P492, DOI 10.1037/0033-295X.95.4.492
   Loomis JM, 2002, J EXP PSYCHOL LEARN, V28, P335, DOI 10.1037//0278-7393.28.2.335
   MACKAY DG, 1982, PSYCHOL REV, V89, P483, DOI 10.1037/0033-295X.89.5.483
   MASAMUNE K, 2002, LECT NOTES COMPUTE 2, V2488, P77
   Pollmann S, 2005, NAT NEUROSCI, V8, P1494, DOI 10.1038/nn1552
   Rosenbaum D.A., 1991, HUMAN MOTOR CONTROL
   Sakai K, 1998, J NEUROSCI, V18, P1827
   Sauer F, 2001, IEEE AND ACM INTERNATIONAL SYMPOSIUM ON AUGMENTED REALITY, PROCEEDINGS, P30, DOI 10.1109/ISAR.2001.970513
   SCHMIDT RA, 1992, PSYCHOL SCI, V3, P207, DOI 10.1111/j.1467-9280.1992.tb00029.x
   Seidler RD, 2004, J COGNITIVE NEUROSCI, V16, P65, DOI 10.1162/089892904322755566
   SHEA JB, 1979, J EXP PSYCHOL-HUM L, V5, P179, DOI 10.1037/0278-7393.5.2.179
   State A., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P439, DOI 10.1145/237170.237283
   STETTEN G, 2000, Patent No. 6599247
   Stetten GD, 2000, 29TH APPLIED IMAGERY PATTERN RECOGNITION WORKSHOP, PROCEEDINGS, P200, DOI 10.1109/AIPRW.2000.953626
   Stetten GD, 2001, J ULTRAS MED, V20, P235
   Thorndike EL, 1901, PSYCHOL REV, V8, P247, DOI 10.1037/h0071363
   THORNDIKE EL., 1913, ED PSYCHOL
   Tracy J, 2003, CEREB CORTEX, V13, P904, DOI 10.1093/cercor/13.9.904
   Welford A.T., 1968, FUNDAMENTALS SKILL
   Wu B, 2005, IEEE T VIS COMPUT GR, V11, P684, DOI 10.1109/TVCG.2005.104
   ZELAZNIK HN, 1981, J EXP PSYCHOL HUMAN, V7, P1007, DOI 10.1037/0096-1523.7.5.1007
NR 33
TC 25
Z9 32
U1 0
U2 11
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD JAN
PY 2008
VL 5
IS 1
AR 1
DI 10.1145/1279640.1279641
PG 23
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA 450YI
UT WOS:000266437500001
DA 2024-07-18
ER

PT J
AU Zhang, JS
   Yang, ZY
   Jin, LCY
   Lu, ZT
   Yu, JH
AF Zhang, Junsong
   Yang, Zuyi
   Jin, Linchengyu
   Lu, Zhitang
   Yu, Jinhui
TI Creating Word Paintings Jointly Considering Semantics, Attention, and
   Aesthetics
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Datasets; neural networks; gaze detection; text tagging
ID VISUALIZATION
AB In this article, we present a content-aware method for generating a word painting. Word painting is a composite artwork made from the assemblage of words extracted from a given text, which carries similar semantics and visual features to a given source image. However, word painting, usually created by skilled artists, involves tedious manual processes, especially when generating streamlines and laying out text. Hence, we provide an easy method to create word paintings for users. How to design textural layout that simultaneously conveys the input image and enables easy access to the semantic theme is the key challenge to generating a visually pleasing word painting. To address this issue, given an image and its content-related text, we first decompose the input image into several regions and approximate each region with a smooth vector field. At the same time, by analyzing the input text, we extract some weighted keywords as the graphic elements. Then, to measure the likelihood of positions in the input image that attract the observers' attention, we generate a saliency map with our trained visual attention model. Finally, jointly considering visual attention and aesthetic rules, we propose an energy-based optimization framework to arrange extracted keywords into the decomposed regions and synthesize a word painting. Experimental results and user studies show that this method is able to generate a fashionable and appealing word painting.
C1 [Zhang, Junsong; Yang, Zuyi; Jin, Linchengyu; Lu, Zhitang] Xiamen Univ, Minist Culture & Tourism, Mind Art & Computat Grp, Dept Artificial Intelligence,Key Lab Digital Prot, Xiamen 361000, Fujian, Peoples R China.
   [Yu, Jinhui] Zhejiang Univ, State Key Lab CAD&CG, Hangzhou 310000, Zhejiang, Peoples R China.
C3 Xiamen University; Zhejiang University
RP Zhang, JS (corresponding author), Xiamen Univ, Minist Culture & Tourism, Mind Art & Computat Grp, Dept Artificial Intelligence,Key Lab Digital Prot, Xiamen 361000, Fujian, Peoples R China.
EM zhangjs@xmu.edu.cn; zuyiyang1996@qq.com; jinlinchengyu@163.com;
   luzhitang@gmail.com; jhyu@cad.zju.edu.cn
RI Zhang, Junsong/HKW-6976-2023; Zhang, JunSong/HTQ-4981-2023
OI Lu, Zhitang/0000-0002-9235-6615
FU National Nature Science Foundation of China [61772440, 61772463]
FX This work is supported by the National Nature Science Foundation of
   China (Grants No. 61772440 and No. 61772463).
CR Chen GN, 2012, IEEE T VIS COMPUT GR, V18, P1717, DOI 10.1109/TVCG.2011.290
   Chi MT, 2015, IEEE T VIS COMPUT GR, V21, P1415, DOI 10.1109/TVCG.2015.2440241
   Dollár P, 2013, IEEE I CONF COMP VIS, P1841, DOI 10.1109/ICCV.2013.231
   Frintrop S, 2010, ACM T APPL PERCEPT, V7, DOI 10.1145/1658349.1658355
   Hu ZZ, 2014, IEEE T MULTIMEDIA, V16, P1156, DOI 10.1109/TMM.2014.2305635
   IBM, 2019, WATS NAT LANG UND
   Jahanian Ali, 2013, P 2013 INT C INTELLI, P95
   Jie Xu, 2007, Proceedings Graphics Interface 2007, P43
   Judd T, 2009, IEEE I CONF COMP VIS, P2106, DOI 10.1109/ICCV.2009.5459462
   Krause J., 2004, Design basics index. Cincinnati
   Lee B, 2010, IEEE T VIS COMPUT GR, V16, P1182, DOI 10.1109/TVCG.2010.194
   Li CL, 2018, VIS INFORM, V2, P50, DOI 10.1016/j.visint2018.04.006
   Maharik R, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964995
   Paulovich FV, 2012, COMPUT GRAPH FORUM, V31, P1145, DOI 10.1111/j.1467-8659.2012.03107.x
   Saputra Reza Adhitya, 2017, P 43 GRAPH INT C, P8, DOI DOI 10.20380/GI2017.02
   Still J, 2019, ACM T APPL PERCEPT, V16, DOI 10.1145/3301413
   Surazhsky T, 2002, COMPUT GRAPH FORUM, V21, P99, DOI 10.1111/1467-8659.00570
   Viégas FB, 2009, IEEE T VIS COMPUT GR, V15, P1137, DOI 10.1109/TVCG.2009.171
   Wang YH, 2020, IEEE T VIS COMPUT GR, V26, P991, DOI 10.1109/TVCG.2019.2934783
   Xu X, 2015, P 5 INT S NONPH AN R, P183
   Xu XM, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778789
   Yang XY, 2016, ACM T MULTIM COMPUT, V12, DOI 10.1145/2818709
   Yeh YT, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185552
   Yin W., 2013, P 21 ACM INT C MULT, P927, DOI [DOI 10.1145/2502081.2502116, 10.1145/2502081]
   Zheng XR, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3322971
   Zou CQ, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925887
NR 26
TC 1
Z9 1
U1 2
U2 8
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD JUL
PY 2022
VL 19
IS 3
AR 13
DI 10.1145/3539610
PG 21
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 4L3FO
UT WOS:000852517200005
DA 2024-07-18
ER

PT J
AU Ferwerda, J
AF Ferwerda, James
TI The FechDeck: A Hand Tool for Exploring Psychophysics
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Vision; psychophysics; education; open source
ID PSYCHOMETRIC FUNCTION
AB Learning the methods of psychophysics is an essential part of training for perceptual experimentation, and hands-on experience is vital, but gaining this experience is difficult because good tools for learning are not available. The FechDeck is an ordinary deck of playing cards that has been modified to support learning the methods of psychophysics. Card backs are printed with noise patterns that span a range of densities. Faces are augmented with line segments arranged in "L" patterns. Jokers are printed with ruled faces and with backs that serve as standards. Instructions provided with the FechDeck allow users to perform threshold experiments using Fechner's methods of adjustment, limits, and constant stimuli; scaling experiments using Thurstone's ranking, paired comparison, and successive categories methods; and Stevens's magnitude estimation method. Spreadsheets provided with the deck support easy data entry and meaningful data analysis. An online repository supporting the FechDeck has been established to facilitate dissemination and to encourage open source development of the deck.
C1 [Ferwerda, James] Rochester Inst Technol, Chester F Carlson Ctr Imaging Sci, Lomb Mem Dr, Rochester, NY 14623 USA.
C3 Rochester Institute of Technology
RP Ferwerda, J (corresponding author), Rochester Inst Technol, Chester F Carlson Ctr Imaging Sci, Lomb Mem Dr, Rochester, NY 14623 USA.
EM jaf@cis.rit.edu
CR [Anonymous], 1997, PSYCHOPHYSICS FUNDAM
   Brainard DH, 1997, SPATIAL VISION, V10, P433, DOI 10.1163/156856897X00357
   Fechner G. T., 1860, Elemente der Psychophysik
   Green D., 1966, SIGNAL DETECTION THE
   Klein SA, 2001, PERCEPT PSYCHOPHYS, V63, P1421, DOI 10.3758/BF03194552
   Kleiner M, 2007, PERCEPTION, V36, P14
   Lu ZL, 2014, VISUAL PSYCHOPHYSICS: FROM LABORATORY TO THEORY, P1
   Macmillan NA, 2001, PERCEPT PSYCHOPHYS, V63, P1277, DOI 10.3758/BF03194542
   Maloney Laurence T, 2003, J VISION, V3, P5, DOI DOI 10.1167/3.8.5
   MathWorks, 2017, IM PROC TOOLB
   Peirce JW, 2007, J NEUROSCI METH, V162, P8, DOI 10.1016/j.jneumeth.2006.11.017
   Prins N., 2016, PSYCHOPHYSICS PRACTI
   Schutt HH, 2016, VISION RES, V122, P105, DOI 10.1016/j.visres.2016.02.002
   STEVENS SS, 1957, PSYCHOL REV, V64, P153, DOI 10.1037/h0046162
   Strasburger H, 2001, PERCEPT PSYCHOPHYS, V63, P1348, DOI 10.3758/BF03194547
   Strasburger H., 2018, SOFTWARE VISUAL PSYC
   Straw Andrew D, 2008, Front Neuroinform, V2, P4, DOI 10.3389/neuro.11.004.2008
   Thurstone LL, 1927, PSYCHOL REV, V34, P273, DOI 10.1037/h0070288
   Treutwein B, 1999, PERCEPT PSYCHOPHYS, V61, P87, DOI 10.3758/BF03211951
   TREUTWEIN B, 1995, VISION RES, V35, P2503, DOI 10.1016/0042-6989(95)00016-X
   Urban FM, 1910, PSYCHOL REV, V17, P229, DOI 10.1037/h0074515
   WATSON AB, 1983, PERCEPT PSYCHOPHYS, V33, P113, DOI 10.3758/BF03202828
   Watson AB, 2001, PROC SPIE, V4299, P79, DOI 10.1117/12.429526
   Watson AB, 2017, J VISION, V17, DOI 10.1167/17.3.10
   Wichmann FA, 2001, PERCEPT PSYCHOPHYS, V63, P1293, DOI 10.3758/BF03194544
NR 25
TC 0
Z9 0
U1 0
U2 5
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD AUG
PY 2019
VL 16
IS 2
AR 9
DI 10.1145/3313186
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA JC8UL
UT WOS:000489551500003
DA 2024-07-18
ER

PT J
AU Singhal, A
   Jones, LA
AF Singhal, Anshul
   Jones, Lynette A.
TI Creating Thermal Icons-A Model-Based Approach
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Thermal display; thermal feedback; thermal perception; hand-object
   interaction
ID VIRTUAL ENVIRONMENTS; TACTILE DISPLAYS; TEMPERATURE; WARM;
   DISCRIMINATION; IDENTIFICATION; STIMULATION; INFORMATION; THRESHOLDS;
   DIMENSIONS
AB The objective of this set of experiments was to evaluate thermal pattern recognition on the hand and arm and to determine which features of thermal stimuli are encoded by cutaneous thermoreceptors and perceived by the user of a thermal display. Thermal icons were created by varying the direction, rate, and magnitude of change in temperature. It was found that thermal icons were identified more accurately when presented on the thenar eminence or the wrist, as compared to the fingertips and that thermal patterns as brief as 8s could be reliably identified. In these experiments, there was no difference in performance when identifying warm or cool stimuli. A dynamic model of the change in skin temperature as a function of the thermal input was developed based on linear system identification techniques. This model was able to predict the change in skin temperature from an unrelated experiment involving thermal icons. This opens the possibility of using a model-based approach to the development of thermal icons.
C1 [Singhal, Anshul; Jones, Lynette A.] MIT, Dept Mech Engn, Room 3-137,77 Massachusetts Ave, Cambridge, MA 02139 USA.
C3 Massachusetts Institute of Technology (MIT)
RP Singhal, A (corresponding author), MIT, Dept Mech Engn, Room 3-137,77 Massachusetts Ave, Cambridge, MA 02139 USA.
EM anshuls@mit.edu; ljones@mit.edu
OI Jones, Lynette/0000-0003-1361-8654
FU National Science Foundation (US) [IIS-1318215]
FX Research supported by the National Science Foundation (US) under grant
   IIS-1318215.
CR Azadi M, 2014, IEEE T HAPTICS, V7, P14, DOI 10.1109/TOH.2013.2296051
   Brown LM, 2005, WORLD HAPTICS CONFERENCE: FIRST JOINT EUROHAPTICS CONFERENCE AND SYMPOSIUM ON HAPTIC INTERFACES FOR VIRUTUAL ENVIRONMENT AND TELEOPERATOR SYSTEMS, PROCEEDINGS, P167
   Cholewiak SA, 2008, SYMPOSIUM ON HAPTICS INTERFACES FOR VIRTUAL ENVIRONMENT AND TELEOPERATOR SYSTEMS 2008, PROCEEDINGS, P87
   Dionisio J, 1997, COMPUT GRAPH, V21, P459, DOI 10.1016/S0097-8493(97)00029-0
   DURLACH NI, 1989, PERCEPT PSYCHOPHYS, V46, P293, DOI 10.3758/BF03208094
   GREEN BG, 1977, PERCEPT PSYCHOPHYS, V22, P331, DOI 10.3758/BF03199698
   Halvey Martin, 2012, Haptic and Audio Interaction Design. Proceedings 7th International Conference, HAID 2012, P91, DOI 10.1007/978-3-642-32796-4_10
   Harding LM, 2005, SOMATOSENS MOT RES, V22, P45, DOI 10.1080/08990220500084784
   Ho HN, 2007, ACM T APPL PERCEPT, V4, DOI 10.1145/1265957.1265962
   Ho HN, 2017, IEEE T HAPTICS, V10, P84, DOI 10.1109/TOH.2016.2583424
   Ho HN, 2008, J BIOMECH ENG-T ASME, V130, DOI 10.1115/1.2899574
   JOHNSON KO, 1973, J NEUROPHYSIOL, V36, P347, DOI 10.1152/jn.1973.36.2.347
   Jones LA, 2008, HUM FACTORS, V50, P90, DOI 10.1518/001872008X250638
   Jones LA, 2013, IEEE T HAPTICS, V6, P268, DOI 10.1109/TOH.2012.74
   Jones LA, 2011, PROG BRAIN RES, V192, P113, DOI 10.1016/B978-0-444-53355-5.00008-7
   Jones LA, 2009, PERCEPTION, V38, P52, DOI 10.1068/p5914
   Jones LA, 2008, IEEE T HAPTICS, V1, P53, DOI 10.1109/ToH.2008.2
   KENSHALO DR, 1968, PERCEPT PSYCHOPHYS, V3, P81, DOI 10.3758/BF03212769
   Kron A, 2003, 11TH SYMPOSIUM ON HAPTIC INTERFACES FOR VIRTUAL ENVIRONMENT AND TELEOPERATOR SYSTEMS - HAPTICS 2003, PROCEEDINGS, P16, DOI 10.1109/HAPTIC.2003.1191219
   Lécuyer A, 2003, P IEEE VIRT REAL ANN, P251, DOI 10.1109/VR.2003.1191147
   Lederman SJ, 1997, J EXP PSYCHOL HUMAN, V23, P1680, DOI 10.1037/0096-1523.23.6.1680
   Ljung L, 1999, PRENTICE HALL INFORM, P503
   MaClean K. E., 1999, 1999 IEEE/ASME International Conference on Advanced Intelligent Mechatronics (Cat. No.99TH8399), P203, DOI 10.1109/AIM.1999.803167
   R Kenshalo D., 1976, SENSORY FUNCTIONS SK, P305, DOI 10.1016/C2013-0-05758-1
   RABINOWITZ WM, 1987, J ACOUST SOC AM, V82, P1243, DOI 10.1121/1.395260
   Singhal A, 2016, IEEE HAPTICS SYM, P92, DOI 10.1109/HAPTICS.2016.7463161
   Singhal A, 2015, 2015 IEEE WORLD HAPTICS CONFERENCE (WHC), P469, DOI 10.1109/WHC.2015.7177756
   SPRAY DC, 1986, ANNU REV PHYSIOL, V48, P625
   Stevens JC, 1998, SOMATOSENS MOT RES, V15, P13
   Summers IR, 2005, J ACOUST SOC AM, V118, P2527, DOI 10.1121/1.2031979
   Tan HZ, 2010, IEEE T HAPTICS, V3, P98, DOI 10.1109/ToH.2009.46
   Tiest WMB, 2009, ATTEN PERCEPT PSYCHO, V71, P481, DOI 10.3758/APP.71.3.481
   Van Erp JBF, 2005, ERGONOMICS, V48, P302, DOI 10.1080/0014013042000327670
   Verrillo RT, 1998, SOMATOSENS MOT RES, V15, P93, DOI 10.1080/08990229870826
   Wall C, 2003, IEEE ENG MED BIOL, V22, P84, DOI 10.1109/MEMB.2003.1195701
   Wettach R., 2007, P 9 INT C HUM COMP I, P182, DOI [10.1145/1377999.1378004, DOI 10.1145/1377999.1378004]
   Wilson Graham, 2013, Haptic and Audio Interaction Design. 8th International Workshop (HAID 2013). Revised Selected Papers: LNCS 7989, P10, DOI 10.1007/978-3-642-41068-0_2
   Wilson G, 2011, 29TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P2555
   Wilson Graham., 2012, P 14 INT C HUMAN COM, P309, DOI [10.1145/2371574.2371621, DOI 10.1145/2371574.2371621]
   Yamamoto A, 2004, IEEE INT CONF ROBOT, P1536, DOI 10.1109/ROBOT.2004.1308042
   Yang GH, 2008, PRESENCE-VIRTUAL AUG, V17, P29, DOI 10.1162/pres.17.1.29
   YARNITSKY D, 1991, BRAIN, V114, P1819, DOI 10.1093/brain/114.4.1819
   Zerkus M., 1994, P 4 INT S MEAS CONTR, P107
   [No title captured]
NR 44
TC 14
Z9 14
U1 0
U2 6
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD APR
PY 2018
VL 15
IS 2
AR 14
DI 10.1145/3182175
PG 22
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA GH5YV
UT WOS:000433515400007
OA Bronze
DA 2024-07-18
ER

PT J
AU Vanhoey, K
   Sauvage, B
   Kraemer, P
   Lavoué, G
AF Vanhoey, Kenneth
   Sauvage, Basile
   Kraemer, Pierre
   Lavoue, Guillaume
TI Visual Quality Assessment of 3D Models: On the Influence of
   Light-Material Interaction
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Quality assessment; 3D object; perceptual study; surface mesh
   reflectance; appearance
ID IMAGE; SIMPLIFICATION; MESHES
AB Geometric modifications of three-dimensional (3D) digital models are commonplace for the purpose of efficient rendering or compact storage. Modifications imply visual distortions that are hard to measure numerically. They depend not only on the model itself but also on how the model is visualized. We hypothesize that the model's light environment and the way it reflects incoming light strongly influences perceived quality. Hence, we conduct a perceptual study demonstrating that the same modifications can be masked, or conversely highlighted, by different light-matter interactions. Additionally, we propose a new metric that predicts the perceived distortion of 3D modifications for a known interaction. It operates in the space of 3D meshes with the object's appearance, that is, the light emitted by its surface in any direction given a known incoming light. Despite its simplicity, this metric outperforms 3D mesh metrics and competes with sophisticated perceptual image-based metrics in terms of correlation to subjective measurements. Unlike image-based methods, it has the advantage of being computable prior to the costly rendering steps of image projection and rasterization of the scene for given camera parameters.
C1 [Vanhoey, Kenneth] Swiss Fed Inst Technol, Comp Vis Lab, Zurich, Switzerland.
   [Sauvage, Basile; Kraemer, Pierre] Univ Strasbourg, CNRS, ICube, Strasbourg, France.
   [Lavoue, Guillaume] Univ Lyon, CNRS, LIRIS, Lyon, France.
C3 Swiss Federal Institutes of Technology Domain; ETH Zurich; Universites
   de Strasbourg Etablissements Associes; Universite de Strasbourg; Centre
   National de la Recherche Scientifique (CNRS); Institut National des
   Sciences Appliquees de Lyon - INSA Lyon; Centre National de la Recherche
   Scientifique (CNRS)
RP Vanhoey, K (corresponding author), Swiss Fed Inst Technol, Comp Vis Lab, Zurich, Switzerland.
EM kenneth@research.kvanhoey.eu; sauvage@unistra.fr; kraemer@unistra.fr;
   glavoue@liris.cnrs.fr
OI Vanhoey, Kenneth/0000-0001-8816-6168
FU European Research Council, under the ERC grant "VarCity" [273940];
   Auvergne-Rhone-Alpes region, under the COOPERA grant "ComplexLoD"
FX This work is supported by the European Research Council, under the ERC
   grant "VarCity" (#273940), and by the Auvergne-Rhone-Alpes region, under
   the COOPERA grant "ComplexLoD."
CR [Anonymous], 2011, Analysis, Restoration, and Reconstruction of Ancient Artworks
   [Anonymous], 2006, MODERN IMAGE QUALITY
   [Anonymous], BIOMETRIKA
   Aydin TO, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866187
   Banterle F, 2009, COMPUT GRAPH FORUM, V28, P13, DOI 10.1111/j.1467-8659.2008.01176.x
   Cadík M, 2013, COMPUT GRAPH FORUM, V32, P401, DOI 10.1111/cgf.12248
   Corsini M, 2013, COMPUT GRAPH FORUM, V32, P101, DOI 10.1111/cgf.12001
   Corsini M, 2007, IEEE T MULTIMEDIA, V9, P247, DOI 10.1109/TMM.2006.886261
   Daly Scott, 1993, P179
   Filip J, 2017, COMPUT GRAPH FORUM, V36, P89, DOI 10.1111/cgf.12789
   Filip J, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409091
   Fores A, 2012, COLOR IMAG CONF, P142
   Gao F, 2015, IEEE T NEUR NET LEAR, V26, P2275, DOI 10.1109/TNNLS.2014.2377181
   Garland M, 1998, VISUALIZATION '98, PROCEEDINGS, P263, DOI 10.1109/VISUAL.1998.745312
   Guo JJ, 2017, ACM T APPL PERCEPT, V14, DOI 10.1145/2996296
   Guthe M, 2009, COMPUT GRAPH FORUM, V28, P101, DOI 10.1111/j.1467-8659.2008.01299.x
   Herzog R, 2012, COMPUT GRAPH FORUM, V31, P545, DOI 10.1111/j.1467-8659.2012.03055.x
   Jarabo Adrian, 2014, IEEE T VIS COMPUT GR, P1
   Lavoué G, 2011, COMPUT GRAPH FORUM, V30, P1427, DOI 10.1111/j.1467-8659.2011.02017.x
   Lavoue Guillaume, 2016, IEEE T VIS COMPUT GR, V22, P12
   Ledda P, 2005, ACM T GRAPHIC, V24, P640, DOI 10.1145/1073204.1073242
   Leloup FB, 2010, J OPT SOC AM A, V27, P2046, DOI 10.1364/JOSAA.27.002046
   Lindstrom P, 2000, ACM T GRAPHIC, V19, P204, DOI 10.1145/353981.353995
   Liu YM, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508391
   Lubin J., 1995, VISION MODELS TARGET, P245
   Lubin Jeffrey, 1993, P163
   Mantiuk R, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964935
   Mantiuk RK, 2012, COMPUT GRAPH FORUM, V31, P2478, DOI 10.1111/j.1467-8659.2012.03188.x
   Menzel N, 2010, COMPUT GRAPH FORUM, V29, P2261, DOI 10.1111/j.1467-8659.2010.01815.x
   Nader G, 2016, IEEE T VIS COMPUT GR, V22, P2423, DOI 10.1109/TVCG.2015.2507578
   Pan YX, 2005, IEEE T MULTIMEDIA, V7, P269, DOI 10.1109/TMM.2005.843364
   PHONG BT, 1975, COMMUN ACM, V18, P311, DOI 10.1145/360825.360839
   Preiss J, 2014, IEEE T IMAGE PROCESS, V23, DOI 10.1109/TIP.2014.2302684
   Qu LJ, 2008, IEEE T VIS COMPUT GR, V14, P1015, DOI 10.1109/TVCG.2008.51
   Thurstone LL, 1927, PSYCHOL REV, V34, P273, DOI 10.1037/h0070288
   Tian DH, 2008, IEEE T CIRC SYST VID, V18, P23, DOI 10.1109/TCSVT.2007.903319
   Torkhani F, 2015, SIGNAL PROCESS-IMAGE, V31, P185, DOI 10.1016/j.image.2014.12.008
   Torkhani F, 2012, LECT NOTES COMPUT SC, V7594, P253, DOI 10.1007/978-3-642-33564-8_31
   Vanhoey K, 2013, COMPUT GRAPH FORUM, V32, P101, DOI 10.1111/cgf.12073
   Vanhoey K, 2015, VISUAL COMPUT, V31, P1011, DOI 10.1007/s00371-015-1124-9
   Vása L, 2012, COMPUT GRAPH FORUM, V31, P1715, DOI 10.1111/j.1467-8659.2012.03176.x
   Walter B, 2002, COMPUT GRAPH FORUM, V21, P393, DOI 10.1111/1467-8659.t01-1-00599
   Wang K, 2012, COMPUT GRAPH-UK, V36, P808, DOI 10.1016/j.cag.2012.06.004
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Watson B, 2001, COMP GRAPH, P213, DOI 10.1145/383259.383283
   WILLIAMS N., 2003, I3D'03: Proceedings of the ACM Symposium on Interactive 3D Graphics, P113
   Wood DN, 2000, COMP GRAPH, P287, DOI 10.1145/344779.344925
   Yeganeh H, 2013, IEEE T IMAGE PROCESS, V22, P657, DOI 10.1109/TIP.2012.2221725
NR 49
TC 12
Z9 13
U1 0
U2 10
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD NOV
PY 2017
VL 15
IS 1
AR 5
DI 10.1145/3129505
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FU0CU
UT WOS:000423519800005
DA 2024-07-18
ER

PT J
AU Tauscher, JP
   Mustafa, M
   Magnor, M
AF Tauscher, Jan-Philipp
   Mustafa, Maryam
   Magnor, Marcus
TI Comparative Analysis of Three Different Modalities for Perception of
   Artifacts in Videos
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Eye tracking; EEG; ERP; user rating; artifacts; video; perceptual
   quality; implicit perception
ID VISUAL-ATTENTION; QUALITY
AB This study compares three popular modalities for analyzing perceived video quality; user ratings, eye tracking, and EEG. We contrast these three modalities for a given video sequence to determine if there is a gap between what humans consciously see and what we implicitly perceive. Participants are shown a video sequence with different artifacts appearing at specific distances in their field of vision; near foveal, middle peripheral, and far peripheral. Our results show distinct differences between what we saccade to (eye tracking), how we consciously rate video quality, and our neural responses (EEG data). Our findings indicate that the measurement of perceived quality depends on the specific modality used.
C1 [Tauscher, Jan-Philipp; Mustafa, Maryam; Magnor, Marcus] TU Braunschweig, Inst Comp Graph, Muhlenpfordtstr 23,Gallery Floor, D-38106 Braunschweig, Germany.
C3 Braunschweig University of Technology
RP Tauscher, JP (corresponding author), TU Braunschweig, Inst Comp Graph, Muhlenpfordtstr 23,Gallery Floor, D-38106 Braunschweig, Germany.
EM tauscher@cg.cs.tu-bs.de; mustafa@cg.cs.tu-bs.de; magnor@cg.cs.tu-bs.de
OI Tauscher, Jan-Philipp/0000-0003-2407-391X; Magnor,
   Marcus/0000-0003-0579-480X
FU German Science Foundation (DFG) [MA2555/15-1, INST 188/409-1 FUGG]
FX This work is funded by the German Science Foundation (DFG MA2555/15-1
   Immersive Digital Reality and DFG INST 188/409-1 FUGG ICG Dome).
CR Acqualagna L, 2015, J NEURAL ENG, V12, DOI 10.1088/1741-2560/12/2/026012
   [Anonymous], 2005, Electric Fields of the Brain: The Neurophysics of Eeg
   [Anonymous], 2017, ACM T GRAPHIC, DOI [DOI 10.1145/3072959.3073642, 10.1145/3072959.3073642]
   [Anonymous], 2013, IEEE INT S BROADB MU
   Arndt S., 2011, Proceedings of the 2011 IEEE International Symposium on Multimedia (ISM 2011), P518, DOI 10.1109/ISM.2011.91
   Arndt S, 2014, IEEE J-STSP, V8, P366, DOI 10.1109/JSTSP.2014.2313026
   Arndt S, 2012, INT WORK QUAL MULTIM, P284, DOI 10.1109/QoMEX.2012.6263836
   Bonkon Koo, 2015, 2015 3rd International Winter Conference on Brain-Computer Interface (BCI), P1, DOI 10.1109/IWW-BCI.2015.7073028
   Bosse S, 2014, IEEE IMAGE PROC, P1987, DOI 10.1109/ICIP.2014.7025398
   Duncan CC, 2009, CLIN NEUROPHYSIOL, V120, P1883, DOI 10.1016/j.clinph.2009.07.045
   Engelke U, 2017, IEEE J-STSP, V11, P6, DOI 10.1109/JSTSP.2016.2609843
   ENGELKEN EJ, 1991, AVIAT SPACE ENVIR MD, V62, P315
   Gould S, 2007, 20TH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2115
   Gulliver SR, 2004, IEEE T SYST MAN CY A, V34, P472, DOI 10.1109/TSMCA.2004.826309
   HORRIDGE GA, 1987, PROC R SOC SER B-BIO, V230, P279, DOI 10.1098/rspb.1987.0020
   Juluri P, 2016, IEEE COMMUN SURV TUT, V18, P401, DOI 10.1109/COMST.2015.2401424
   Kosara R, 2003, IEEE COMPUT GRAPH, V23, P20, DOI 10.1109/MCG.2003.1210860
   Krassanakis V, 2014, J EYE MOVEMENT RES, V7
   Kroupi E, 2014, EUR SIGNAL PR CONF, P2135
   Le Meur O, 2010, SIGNAL PROCESS-IMAGE, V25, P547, DOI 10.1016/j.image.2010.05.006
   Li ZC, 2011, IMAGE VISION COMPUT, V29, P1, DOI 10.1016/j.imavis.2010.07.001
   Lindemann L., 2011, 2011 18th IEEE International Conference on Image Processing (ICIP 2011), P3109, DOI 10.1109/ICIP.2011.6116324
   Lindemann Lea., 2011, Proceedings of the ACM SIGGRAPH Symposium on Applied Perception in Graphics and Visualization, P53
   Liu HT, 2011, IEEE T CIRC SYST VID, V21, P971, DOI 10.1109/TCSVT.2011.2133770
   Liu SD, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P82, DOI 10.1145/2964284.2967187
   Luck SJ, 2014, INTRODUCTION TO THE EVENT-RELATED POTENTIAL TECHNIQUE, 2ND EDITION, P1
   Mustafa M, 2012, ACM T APPL PERCEPT, V9, DOI 10.1145/2325722.2325725
   PICTON TW, 1992, J CLIN NEUROPHYSIOL, V9, P456, DOI 10.1097/00004691-199210000-00002
   Ponomarenko N., 2009, ADV MODERN RADIOELEC, V10, P30, DOI 10.1109/TIP.2015.2439035
   Robertson D. G. E., 1992, J BIOMECH, V26, DOI [10.1016/0021-9290(93)90410-G, DOI 10.1016/0021-9290(93)90410-G]
   ROBINSON DA, 1968, PR INST ELECTR ELECT, V56, P1032, DOI 10.1109/PROC.1968.6455
   Seshadrinathan K, 2010, IEEE T IMAGE PROCESS, V19, P1427, DOI 10.1109/TIP.2010.2042111
   Wenzel MA, 2016, FRONT NEUROSCI-SWITZ, V10, DOI [10.3389/fnins.7016.00023, 10.3389/fnins.2016.00023]
   Zhang W, 2017, IEEE T IMAGE PROCESS, V26, P1275, DOI 10.1109/TIP.2017.2651410
   Zhu Y, 2015, COMPUT HUM BEHAV, V49, P412, DOI 10.1016/j.chb.2015.02.054
NR 35
TC 9
Z9 10
U1 0
U2 11
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD SEP
PY 2017
VL 14
IS 4
SI SI
AR 22
DI 10.1145/3129289
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FM9EI
UT WOS:000415407300001
DA 2024-07-18
ER

PT J
AU Jain, E
   Anthony, L
   Aloba, A
   Castonguay, A
   Cuba, I
   Shaw, A
   Woodward, J
AF Jain, Eakta
   Anthony, Lisa
   Aloba, Aishat
   Castonguay, Amanda
   Cuba, Isabella
   Shaw, Alex
   Woodward, Julia
TI Is the Motion of a Child Perceivably Different from the Motion of an
   Adult?
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article; Proceedings Paper
CT 13th ACM SIGGRAPH Symposium on Applied Perception (SAP)
CY JUL, 2016
CL Anaheim, CA
SP ACM SIGGRAPH
DE Perception of motion; point light displays; biological motion; child
   motion; markerless motion capture
ID POINT-LIGHT; BIOLOGICAL MOTION; GAIT PERCEPTION; GENDER; KINEMATICS
AB Artists and animators have observed that children's movements are quite different from adults performing the same action. Previous computer graphics research on human motion has primarily focused on adult motion. There are open questions as to how different child motion actually is, and whether the differences will actually impact animation and interaction. We report the first explicit study of the perception of child motion (ages 5 to 9 years old), compared to analogous adult motion. We used markerless motion capture to collect an exploratory corpus of child and adult motion, and conducted a perceptual study with point light displays to discover whether naive viewers could identify a motion as belonging to a child or an adult. We find that people are generally successful at this task. This work has implications for creating more engaging and realistic avatars for games, online social media, and animated videos and movies.
C1 [Jain, Eakta; Anthony, Lisa; Aloba, Aishat; Shaw, Alex; Woodward, Julia] Univ Florida, Dept CISE, POB 116120, Gainesville, FL 32611 USA.
   [Castonguay, Amanda] Univ Southern Maine, POB 9300, Portland, ME 04104 USA.
   [Castonguay, Amanda; Cuba, Isabella] Univ Florida, Gainesville, FL 32611 USA.
   [Cuba, Isabella] Vassar Coll, 124 Raymond Ave, Poughkeepsie, NY 12604 USA.
C3 State University System of Florida; University of Florida; University of
   Maine System; University of Southern Maine; State University System of
   Florida; University of Florida; Vassar College
RP Jain, E (corresponding author), Univ Florida, Dept CISE, POB 116120, Gainesville, FL 32611 USA.
EM ejain@cise.ufl.edu
FU Div Of Information & Intelligent Systems; Direct For Computer & Info
   Scie & Enginr [1552598] Funding Source: National Science Foundation
CR AnimationAddicts, 2013, CHALL AN UND AD
   Assaiante C, 1998, NEUROSCI BIOBEHAV R, V22, P527, DOI 10.1016/S0149-7634(97)00040-7
   Atkinson AP, 2004, PERCEPTION, V33, P717, DOI 10.1068/p5096
   BARCLAY CD, 1978, PERCEPT PSYCHOPHYS, V23, P145, DOI 10.3758/BF03208295
   BEARDSWORTH T, 1981, B PSYCHONOMIC SOC, V18, P19
   Blake R, 2007, ANNU REV PSYCHOL, V58, P47, DOI 10.1146/annurev.psych.57.102904.190152
   Brooks A, 2008, CURR BIOL, V18, pR728, DOI 10.1016/j.cub.2008.06.054
   Chaminade T, 2007, SOC COGN AFFECT NEUR, V2, P206, DOI 10.1093/scan/nsm017
   Chia LC, 2013, GAIT POSTURE, V38, P264, DOI 10.1016/j.gaitpost.2012.11.028
   CUTTING JE, 1977, B PSYCHONOMIC SOC, V9, P353, DOI 10.3758/BF03337021
   CUTTING JE, 1978, PERCEPTION, V7, P393, DOI 10.1068/p070393
   Davis JW, 2001, LECT NOTES COMPUT SC, V2091, P295
   Delp S., 2007, IEEE T BIOMEDICAL EN, V54, P11, DOI DOI 10.1109/TBME.2007.901024.1940-50
   Dittrich WH, 1996, PERCEPTION, V25, P727, DOI 10.1068/p250727
   FOX R, 1982, SCIENCE, V218, P486, DOI 10.1126/science.7123249
   Golinkoff RM, 2002, DEV PSYCHOL, V38, P604, DOI 10.1037//0012-1649.38.4.604
   Harrison J, 2004, ACM T GRAPHIC, V23, P569, DOI 10.1145/1015706.1015761
   Hodgins JK, 1998, IEEE T VIS COMPUT GR, V4, P307, DOI 10.1109/2945.765325
   Huelke DF, 1998, P ANN C ASS, P93
   Ivanenko YP, 2013, J NEUROSCI, V33, P3025, DOI 10.1523/JNEUROSCI.2722-12.2013
   JOHANSSON G, 1973, PERCEPT PSYCHOPHYS, V14, P201, DOI 10.3758/BF03212378
   Klin A, 2009, NATURE, V459, P257, DOI 10.1038/nature07868
   KOZLOWSKI LT, 1977, PERCEPT PSYCHOPHYS, V21, P575, DOI 10.3758/BF03198740
   Loula F, 2005, J EXP PSYCHOL HUMAN, V31, P210, DOI 10.1037/0096-1523.31.1.210
   Mcdonnell R, 2009, ACM T APPL PERCEPT, V5, DOI 10.1145/1462048.1462051
   McDonnell R, 2008, APGV 2008: PROCEEDINGS OF THE SYMPOSIUM ON APPLIED PERCEPTION IN GRAPHICS AND VISUALIZATION, P67
   Nader PR, 2008, JAMA-J AM MED ASSOC, V300, P295, DOI 10.1001/jama.300.3.295
   Neri P, 1998, NATURE, V395, P894, DOI 10.1038/27661
   Pavlova M, 2001, PERCEPTION, V30, P925, DOI 10.1068/p3157
   Pollick FE, 2002, VISION RES, V42, P2345, DOI 10.1016/S0042-6989(02)00196-7
   Rosengren KS, 2009, GAIT POSTURE, V29, P225, DOI 10.1016/j.gaitpost.2008.08.005
   Sakuma Ryusuke, 2012, No To Hattatsu, V44, P320
   Sandlund M, 2009, PHYS THER REV, V14, P348, DOI 10.1179/174328809X452854
   Stadler W, 2012, PSYCHOL RES-PSYCH FO, V76, P395, DOI 10.1007/s00426-012-0431-2
   THELEN E, 1995, AM PSYCHOL, V50, P79, DOI 10.1037/0003-066X.50.2.79
   Wellerdiek AC, 2013, P ACM S APPL PERC SA, P138, DOI [10.1145/2492494.2501895, DOI 10.1145/2492494.2501895]
NR 36
TC 14
Z9 18
U1 1
U2 9
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD JUL
PY 2016
VL 13
IS 4
SI SI
AR 22
DI 10.1145/2947616
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA DV4DY
UT WOS:000382876600005
DA 2024-07-18
ER

PT J
AU Koulieris, GA
   Drettakis, G
   Cunningham, D
   Mania, K
AF Koulieris, George Alex
   Drettakis, George
   Cunningham, Douglas
   Mania, Katerina
TI An Automated High-Level Saliency Predictor for Smart Game Balancing
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Human Factors; Computer graphics; game balancing; scene schemata
ID EYE-MOVEMENTS; ATTENTION; OBJECTS; DESIGN; MEMORY; SERIAL
AB Successfully predicting visual attention can significantly improve many aspects of computer graphics: scene design, interactivity and rendering. Most previous attention models are mainly based on low-level image features, and fail to take into account high-level factors such as scene context, topology, or task. Low-level saliency has previously been combined with task maps, but only for predetermined tasks. Thus, the application of these methods to graphics (e.g., for selective rendering) has not achieved its full potential. In this article, we present the first automated high-level saliency predictor incorporating two hypotheses from perception and cognitive science that can be adapted to different tasks. The first states that a scene is comprised of objects expected to be found in a specific context as well objects out of context which are salient (scene schemata) while the other claims that viewer's attention is captured by isolated objects (singletons). We propose a new model of attention by extending Eckstein's Differential Weighting Model. We conducted a formal eye-tracking experiment which confirmed that object saliency guides attention to specific objects in a game scene and determined appropriate parameters for a model. We present a GPU-based system architecture that estimates the probabilities of objects to be attended in real-time. We embedded this tool in a game level editor to automatically adjust game level difficulty based on object saliency, offering a novel way to facilitate game design. We perform a study confirming that game level completion time depends on object topology as predicted by our system.
C1 [Koulieris, George Alex; Mania, Katerina] Tech Univ Crete, Dept Elect & Comp Engn, Khania 73100, Crete, Greece.
   [Cunningham, Douglas] Brandenburg Tech Univ Cottbus, Cottbus, Germany.
C3 Technical University of Crete; Brandenburg University of Technology
   Cottbus
RP Koulieris, GA (corresponding author), Tech Univ Crete, Dept Elect & Comp Engn, Akrotiri Campus, Khania 73100, Crete, Greece.
EM gkoulieris@isc.tuc.gr; George.Drettakis@inria.fr;
   douglas.cunningham@tu-cottbus.de; k.mania@ced.tuc.gr
RI Koulieris, George/AAJ-5666-2020; Mania, Katerina/AAO-7013-2021
OI Koulieris, George/0000-0003-1610-6240; 
FU European Union (European Social Fund); Greek national funds through the
   Operational Program "Education and Lifelong Learning" of the National
   Strategic Reference Framework Research Funding Program: Heracleitus II:
   Investing in knowledge society through the European Social Fund; EU
   [ICT-611089-CR-PLAY]
FX This research has been co-financed by the European Union (European
   Social Fund) and Greek national funds through the Operational Program
   "Education and Lifelong Learning" of the National Strategic Reference
   Framework Research Funding Program: Heracleitus II: Investing in
   knowledge society through the European Social Fund. This work was partly
   supported by EU FP7 project ICT-611089-CR-PLAY (www.cr-play.eu).
CR [Anonymous], 1987, Shifts in selective visual attention: Towards the underlying neural circuitry. matters of intelligence
   [Anonymous], 1932, REMEMBERING EXPT SOC
   ARIELY D, 2001, J EXPT PSYCHOL HUMAN, V26, P834
   Bar M, 1996, PERCEPTION, V25, P343, DOI 10.1068/p250343
   Becker MW, 2007, J EXP PSYCHOL HUMAN, V33, P20, DOI 10.1037/0096-1523.33.1.20
   Bernhard M., 2011, 2011 IEEE 10th IVMSP Workshop: Perception and Visual Signal Analysis, P153, DOI 10.1109/IVMSPW.2011.5970371
   Bernhard M, 2010, ACM T APPL PERCEPT, V8, DOI 10.1145/1857893.1857897
   Borji A, 2013, IEEE T PATTERN ANAL, V35, P185, DOI 10.1109/TPAMI.2012.89
   BREWER WF, 1981, COGNITIVE PSYCHOL, V13, P207, DOI 10.1016/0010-0285(81)90008-6
   Cater K., 2003, Eurographics Symposium on Rendering. 14th Eurographics Workshop on Rendering, P270
   Desurvire H., 2004, EXTENDED ABSTRACTS 2, P1509, DOI [DOI 10.1145/985921.986102, 10.1145/985921.986102]
   DOUGLAS W, 2011, FLOW PSYCHOL OPTIMAL
   Eckstein M. P., 2002, J VISION, V2
   Eckstein MP, 2006, PSYCHOL SCI, V17, P973, DOI 10.1111/j.1467-9280.2006.01815.x
   Eckstein MP, 1998, PSYCHOL SCI, V9, P111, DOI 10.1111/1467-9280.00020
   Einhauser W., 2008, J VISION, V8
   Feil J.H., 2005, BEGINNING GAME LEVEL
   Frintrop S, 2010, ACM T APPL PERCEPT, V7, DOI 10.1145/1658349.1658355
   Green DM, 1966, SIGNAL DETECTION THE, V1
   Grillon H, 2009, COMPUT ANIMAT VIRT W, V20, P111, DOI 10.1002/cav.293
   Henderson JM, 1999, ANNU REV PSYCHOL, V50, P243, DOI 10.1146/annurev.psych.50.1.243
   Henderson JM, 1999, J EXP PSYCHOL HUMAN, V25, P210, DOI 10.1037/0096-1523.25.1.210
   HILLAIRE S, 2010, P 17 ACM S VIRT REAL, P191
   Hwang AD, 2011, VISION RES, V51, P1192, DOI 10.1016/j.visres.2011.03.010
   Itti L, 2001, NAT REV NEUROSCI, V2, P194, DOI 10.1038/35058500
   Johnson D, 2003, ERGONOMICS, V46, P1332, DOI 10.1080/00140130310001610865
   Ju E, 1997, DATA BASE ADV INF SY, V28, P78
   Judd T, 2009, IEEE I CONF COMP VIS, P2106, DOI 10.1109/ICCV.2009.5459462
   KOULIERIS GA, 2014, COMPUTER GRAPHICS FO, V33
   Lee S, 2009, IEEE T VIS COMPUT GR, V15, P6, DOI 10.1109/TVCG.2008.82
   Longhurst P., 2006, P 4 INT C COMPUTER G, P21
   Mania K, 2005, PRESENCE-TELEOP VIRT, V14, P606, DOI 10.1162/105474605774918769
   Markou M, 2003, SIGNAL PROCESS, V83, P2499, DOI 10.1016/j.sigpro.2003.07.019
   Marr D., 1982, Visual perception
   Moehring M, 2009, IEEE VIRTUAL REALITY 2009, PROCEEDINGS, P223, DOI 10.1109/VR.2009.4811027
   Mourkoussis N, 2010, ACM T APPL PERCEPT, V8, DOI 10.1145/1857893.1857895
   NAKAYAMA K, 1986, NATURE, V320, P264, DOI 10.1038/320264a0
   O'Craven KM, 1999, NATURE, V401, P584, DOI 10.1038/44134
   OYEKOYA O, 2009, P 16 ACM S VIRT REAL, P199
   Pagulayan R.J., 2003, HUM FAC ER, P883
   Palmer S., 1999, VISION SCI PHOTONS P
   Rayner K, 2009, Q J EXP PSYCHOL, V62, P1457, DOI 10.1080/17470210902816461
   Rensink RA, 2000, VIS COGN, V7, P17, DOI 10.1080/135062800394667
   Salvucci DD, 2000, 2000 S EYE TRACKING, P71, DOI [10.1145/355017.355028, DOI 10.1145/355017.355028]
   Seif El-Nasr M., 2006, P 2006 ACM SIGCHI IN, DOI DOI 10.1145/1178823.1178849
   Shipley T.F., 2001, From fragments to objects: Segmentation and grouping in vision, V130
   Steinmetz PN, 2000, NATURE, V404, P187, DOI 10.1038/35004588
   Sundstedt V., 2005, Proceedings of the 21st spring conference on Computer graphics, P169, DOI [10.1145/1090122.1090150, DOI 10.1145/1090122.1090150]
   SUNDSTEDT V, 2004, VISION MODELING VISU, P209
   Sundstedt V, 2008, APGV 2008: PROCEEDINGS OF THE SYMPOSIUM ON APPLIED PERCEPTION IN GRAPHICS AND VISUALIZATION, P43
   Sundstedt Veronica., 2013, GAME ANAL, P543
   Sweetser P, 2005, COMPUTERS ENTERTAINM, V3, P3, DOI [10.1145/1077246.1077253, DOI 10.1145/1077246.1077253]
   Theeuwes J, 2002, PERCEPT PSYCHOPHYS, V64, P764, DOI 10.3758/BF03194743
   Theeuwes J, 2010, ACTA PSYCHOL, V135, P77, DOI 10.1016/j.actpsy.2010.02.006
   TOLHURST DJ, 1983, VISION RES, V23, P775, DOI 10.1016/0042-6989(83)90200-6
   TREISMAN AM, 1980, COGNITIVE PSYCHOL, V12, P97, DOI 10.1016/0010-0285(80)90005-5
   Walther D, 2006, NEURAL NETWORKS, V19, P1395, DOI 10.1016/j.neunet.2006.10.001
   WOLFE JM, 1994, PSYCHON B REV, V1, P202, DOI 10.3758/BF03200774
   ZOTOS A, 2009, P 6 S APPL PERC GRAP, P85
NR 59
TC 7
Z9 7
U1 0
U2 8
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD JAN
PY 2015
VL 11
IS 4
AR 17
DI 10.1145/2637479
PG 21
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA AZ6WJ
UT WOS:000348358400002
OA Green Submitted, Green Accepted
DA 2024-07-18
ER

PT J
AU Hecher, M
   Bernhard, M
   Mattausch, O
   Scherzer, D
   Wimmer, M
AF Hecher, Michael
   Bernhard, Matthias
   Mattausch, Oliver
   Scherzer, Daniel
   Wimmer, Michael
TI A Comparative Perceptual Study of Soft-Shadow Algorithms
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Measurement; Human Factors; Soft shadows; perception
AB We performed a perceptual user study of algorithms that approximate soft shadows in real time. Although a huge body of soft-shadow algorithms have been proposed, to our knowledge this is the first methodical study for comparing different real-time shadow algorithms with respect to their plausibility and visual appearance. We evaluated soft-shadow properties like penumbra overlap with respect to their relevance to shadow perception in a systematic way, and we believe that our results can be useful to guide future shadow approaches in their methods of evaluation. In this study, we also capture the predominant case of an inexperienced user observing shadows without comparing to a reference solution, such as when watching a movie or playing a game. One important result of this experiment is to scientifically verify that real-time soft-shadow algorithms, despite having become physically based and very realistic, can nevertheless be intuitively distinguished from a correct solution by untrained users.
C1 [Hecher, Michael; Bernhard, Matthias; Wimmer, Michael] Vienna Univ Technol, Vienna, Austria.
   [Mattausch, Oliver] Univ Zurich, CH-8006 Zurich, Switzerland.
   [Scherzer, Daniel] HS Ravensburg Weingarten, Weingarten, Germany.
C3 Technische Universitat Wien; University of Zurich
RP Hecher, M (corresponding author), Favoritenstr 9-11-E186, A-1040 Vienna, Austria.
EM hecher@cg.tuwien.ac.at
OI Wimmer, Michael/0000-0002-9370-2663
FU EU FP7 People Programme (Marie Curie Actions) under REA Grant [290227];
   Austrian Science Fund (FWF) [P23700-N23]; Austrian Science Fund (FWF)
   [P23700] Funding Source: Austrian Science Fund (FWF)
FX This work has been supported by the EU FP7 People Programme (Marie Curie
   Actions) under REA Grant Agreement no. 290227 and the Austrian Science
   Fund (FWF) contract no. P23700-N23.
CR Akyüz AG, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1276377.1276425
   [Anonymous], 1990, C GRIFFIN BOOK SERIE
   Atty L, 2006, COMPUT GRAPH FORUM, V25, P725, DOI 10.1111/j.1467-8659.2006.00995.x
   Bavoil Louis, 2008, Journal of Graphics Tools, V13, P19
   Cadík M, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366166
   DAVIDSON RR, 1977, BIOMETRICS, V33, P693, DOI 10.2307/2529467
   Eisemann E., 2011, Real-time shadows, DOI DOI 10.1201/B11030
   Fernando Randima., 2005, SIGGRAPH 05, P35
   Ferwerda J. A., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P143, DOI 10.1145/258734.258818
   Guennebaud Gael., 2006, Proceedings of the Eurographics Symposium on Rendering, P227
   Gutierrez D, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409073
   Hatzinger R, 2012, J STAT SOFTW, V48, P1
   Hu HH, 2000, IEEE VISUAL, P179, DOI 10.1109/VISUAL.2000.885692
   Jarabo A, 2012, COMPUT GRAPH FORUM, V31, P565, DOI 10.1111/j.1467-8659.2012.03057.x
   Knill DC, 1997, J OPT SOC AM A, V14, P3216, DOI 10.1364/JOSAA.14.003216
   Ledda P, 2005, ACM T GRAPHIC, V24, P640, DOI 10.1145/1073204.1073242
   Leonard C.Wanger, 1992, IEEE COMPUT GRAPH, V12, P54
   Madison C, 2001, PERCEPT PSYCHOPHYS, V63, P187, DOI 10.3758/BF03194461
   Pauly M, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360642
   Reeves W. T., 1987, SIGGRAPH Comput. Graph., P283
   Rubinstein M, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866186
   Sattler M., 2005, Proceedings of the 2nd symposium on Appied perception in graphics and visualization - APGV '05, P131, DOI DOI 10.1145/1080402.1080426
   SCHEFFE H, 1952, J AM STAT ASSOC, V47, P381, DOI 10.2307/2281310
   Scherzer Daniel, 2009, THESIS VIENNA U TECH
   SCHWARZ M, 2008, GI 08, P147
   SCHWARZ M, 2008, EUROGRAPHICS 2008, P295
   Schwarzler M., 2013, P ACM SIGGRAPH S INT, P79
   Setyawan Iwan, 2004, P SPIE SECURITY STEG, V5306
   Steele R.G.D., 1997, PRINCIPLES PROCEDURE, V3rd
   Vangorp Peter, 2006, P ACM SIGGRAPH SKETC
   Wanger L., 1992, ACM I3D Symposium on Interactive 3D Graphics, P39, DOI DOI 10.1145/147156.147161
   WANGER LR, 1992, IEEE COMPUT GRAPH, V12, P44, DOI 10.1109/38.135913
   Yang BG, 2009, COMPUT GRAPH FORUM, V28, P1121, DOI 10.1111/j.1467-8659.2009.01489.x
   Yu IS, 2009, ACM T APPL PERCEPT, V6, DOI 10.1145/1609967.1609971
NR 34
TC 2
Z9 2
U1 2
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD MAY
PY 2014
VL 11
IS 2
AR 5
DI 10.1145/2620029
PG 21
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AO3OJ
UT WOS:000341241100001
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Bojrab, M
   Abdul-Massih, M
   Benes, B
AF Bojrab, Micah
   Abdul-Massih, Michel
   Benes, Bedrich
TI Perceptual Importance of Lighting Phenomena in Rendering of Animated
   Water
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Design; Experimentation; Performance; Perceptual importance; water
   animation; shading effects; illumination
AB Recent years have seen increasing research in perceptually-driven reductions in the costs of realistically rendered imagery. Water is complex and recognizable, and continues to be in the forefront of research. However, the contribution of individual lighting phenomena to the perceived realism of virtual water has not been addressed. All these phenomena have costs associated with their rendering, but does the visual benefit outweigh these costs? This study investigates the human perception of various illumination components found in water-rich virtual environments. The investigation uses a traditional psychophysical analysis to examine viewer perception of these lighting phenomena as they relate to the rendering cost, and ultimately reveals common trends in perceptual value. Five different scenes with a wide range of water and lighting dynamics were tested for perceptual value by one hundred participants. Our results provide an importance comparison for lighting phenomena in the rendering of water, and cost reductions can be made with little or no effect on the perceived quality of the imagery if viewed in a scenario similar to our testing.
C1 [Bojrab, Micah; Abdul-Massih, Michel; Benes, Bedrich] Purdue Univ, W Lafayette, IN 47907 USA.
C3 Purdue University System; Purdue University
RP Benes, B (corresponding author), Knoy Hall Technol, Dept Comp Graph Technol, Room 313 401 N Grant St, W Lafayette, IN 47907 USA.
EM bbenes@purdue.edu
RI Benes, Bedrich/A-8150-2016
OI Benes, Bedrich/0000-0002-5293-2112
CR [Anonymous], P SPR C COMP GRAPH S
   [Anonymous], P SPR C COMP GRAPH S
   [Anonymous], ACM T APPL PERCEPTIO
   [Anonymous], ACM SIGGRAPH ASIA 20
   [Anonymous], P 3 S APPL PERC GRAP
   [Anonymous], P 6 S APPL PERC GRAP
   [Anonymous], 2005, P 2 S APP PERC GRAPH, DOI DOI 10.1145/1080402.1080425
   [Anonymous], SMPTE 196M 2003 MOT
   Cater K., 2003, P 1 INT C COMPUTER G, P39, DOI DOI 10.1145/604471.604483
   Cater K., 2002, Proceedings of the ACM symposium on Virtual reality software and technology, VRST '02, P17, DOI [10.1145/585740.585744, DOI 10.1145/585740.585744]
   Chalmers A., 2006, P 4 INT C COMPUTER G, P9, DOI [DOI 10.1145/1174429.1174431, 10.1145/1174429.1174431]
   Debattista K., 2006, EG PGV 2006. 6th Eurographics Symposium on Parallel Graphics and Visualization, P27
   Debattista K., 2005, P 3 INT C COMPUTER G, P13, DOI DOI 10.1145/1101389.1101393
   Didyk Piotr., 2010, COMPUT GRAPH FORUM
   Dmitriev K., 2002, Rendering Techniques 2002. Eurographics Workshop Proceedings, P25
   Donner C, 2005, ACM T GRAPHIC, V24, P1032, DOI 10.1145/1073204.1073308
   Donner C, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531336
   Dumont R, 2003, ACM T GRAPHIC, V22, P152, DOI 10.1145/636886.636888
   Enright D, 2002, ACM T GRAPHIC, V21, P736, DOI [10.1145/566570.566581, 10.1145/566570.566645]
   Hachisuka T, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618487
   Hachisuka T, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360632
   Irawan P., 2005, Rendering Techniques, P231
   Jensen HW., 2001, REALISTIC IMAGE SYNT, DOI [10.1201/9780429294907, DOI 10.1201/9780429294907]
   Kozlowski O, 2007, APGV 2007: SYMPOSIUM ON APPLIED PERCEPTION IN GRAPHICS AND VISUALIZATION, PROCEEDINGS, P91
   Ledda P, 2005, ACM T GRAPHIC, V24, P640, DOI 10.1145/1073204.1073242
   Masia B, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618506
   McDonnell R, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360625
   McDowall RM, 2009, REV FISH BIOL FISHER, V19, P1, DOI 10.1007/s11160-008-9085-y
   Ramanarayanan G, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360659
   Sattler M., 2005, Proceedings of the 2nd symposium on Appied perception in graphics and visualization - APGV '05, P131, DOI DOI 10.1145/1080402.1080426
   Seif El-Nasr M., 2006, P 2006 ACM SIGCHI IN, DOI DOI 10.1145/1178823.1178849
   Stokes WA, 2004, ACM T GRAPHIC, V23, P742, DOI 10.1145/1015706.1015795
   Tolhurst D.J., 2005, P 2 S APPL PERC GRAP, P135
   Walpole R., 1985, PROBABILITY STAT ENG
NR 34
TC 3
Z9 4
U1 0
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD FEB
PY 2013
VL 10
IS 1
AR 2
DI 10.1145/2422105.2422107
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AN8YO
UT WOS:000340892200002
DA 2024-07-18
ER

PT J
AU Alonso-Arevalo, MA
   Shelley, S
   Hermes, D
   Hollowood, J
   Pettitt, M
   Sharples, S
   Kohlrausch, A
AF Alonso-Arevalo, Miguel A.
   Shelley, Simon
   Hermes, Dik
   Hollowood, Jacqueline
   Pettitt, Michael
   Sharples, Sarah
   Kohlrausch, Armin
TI Curve Shape and Curvature Perception Through Interactive Sonification
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Human Factors; Experimentation; Performance; Sonification; sound
   synthesis; modal synthesis; haptics
ID CONCEPTUAL DATA DIMENSIONS; REPRESENTATION
AB In this article we present an approach that uses sound to communicate geometrical data related to a virtual object. This has been developed in the framework of a multimodal interface for product design. The interface allows a designer to evaluate the quality of a 3-D shape using touch, vision, and sound. Two important considerations addressed in this article are the nature of the data that is sonified and the haptic interaction between the user and the interface, which in fact triggers the sound and influences its characteristics. Based on these considerations, we present a number of sonification strategies that are designed to map the geometrical data of interest into sound. The fundamental frequency of various sounds was used to convey the curve shape or the curvature to the listeners. Two evaluation experiments are described, one involves partipants with a varied background, the other involved the intended users, i.e. participants with a background in industrial design. The results show that independent of the sonification method used and independent of whether the curve shape or the curvature were sonified, the sonification was quite successful. In the first experiment participants had a success rate of about 80% in a multiple choice task, in the second experiment it took the participants on average less than 20 seconds to find the maximum, minimum or inflection points of the curvature of a test curve.
C1 [Alonso-Arevalo, Miguel A.] CICESE, Div Appl Phys, Dept Elect & Telecommun, Ensenada 22860, Baja California, Mexico.
   [Shelley, Simon; Hermes, Dik] Eindhoven Univ Technol, NL-5600 MB Eindhoven, Netherlands.
   [Hollowood, Jacqueline; Sharples, Sarah] Univ Nottingham, Nottingham NG7 2RD, England.
   [Kohlrausch, Armin] Eindhoven Univ Technol & Philips Res Eindhoven, Eindhoven, Netherlands.
C3 CICESE - Centro de Investigacion Cientifica y de Educacion Superior de
   Ensenada; Eindhoven University of Technology; University of Nottingham;
   Philips; Philips Research
RP Alonso-Arevalo, MA (corresponding author), CICESE, Div Appl Phys, Dept Elect & Telecommun, Km 107 Carretera Tijuana Ensenada, Ensenada 22860, Baja California, Mexico.
EM aalonso@cicese.mx
RI Alonso Arevalo, Miguel A./KVY-1464-2024
OI Alonso Arevalo, Miguel A./0000-0001-5453-3142; Sharples,
   Sarah/0000-0003-0288-915X
FU European Commission [FP6-IST-5-054525]; Tangible Interfaces for Novel
   Product Design
FX The research presented in this article was supported by the European
   Commission under the project FP6-IST-5-054525 SATIN Sound and Tangible
   Interfaces for Novel Product Design.
CR Alonso M, 2008, 2008 IEEE INTERNATIONAL WORKSHOP ON HAPTIC AUDIO VISUAL ENVIRONMENTS AND THEIR APPLICATIONS, P154, DOI 10.1109/HAVE.2008.4685316
   [Anonymous], 2007, EMBODIED MUSIC COGNI, DOI DOI 10.7551/MITPRESS/7476.001.0001
   [Anonymous], 2010, MUSICAL GESTURES SOU
   AXEN U., 1996, P 3 INT C AUD DISPL
   Axen Ulrike., 1998, Mathematical Visualization, P223
   Barrass Stephen, 1997, doctoral dissertation
   BORDEGONI M., 2010, SATIN PROJECT CHANNE
   Bordegoni M, 2011, IEEE T HAPTICS, V4, P111, DOI [10.1109/TOH.2011.1, 10.1109/ToH.2011.1]
   Bordegoni M, 2010, PRESENCE-TELEOP VIRT, V19, P341, DOI 10.1162/PRES_a_00010
   Catalano CE., 2002, Journal of Computing and Information Science and Engineering, V2, P11
   GOSSMANN J., 2005, P ICAD 05 11 M INT C, P264
   HERMANN T, 2002, THESIS BIELEFELD U G
   HERMES D. J., 2008, P 4 INT WORKSH HAPT, VII, P18
   HERMES D. J., 2009, P 4 INT HAPT AUD INT, P8
   HOLLANDER A. J., 1994, P 2 INT C AUD DISPL, P157
   HOLLOWOOD J., 2012, SEEING WHAT EA UNPUB
   JANATA P., 2004, P 10 INT C AUD DISPL
   JOSEPH A. J., 2002, P INT C AUD DISPL
   Kamel H.M., 2001, P 7 INT C AUDITORY D, P261
   Kennel A., 1996, P 2 ANN ACM C ASSIST, P51
   Kohler E, 2002, SCIENCE, V297, P846, DOI 10.1126/science.1070311
   Lacey S, 2007, PERCEPTION, V36, P1513, DOI 10.1068/p5850
   Lakatos S, 1999, PERCEPT PSYCHOPHYS, V61, P895, DOI 10.3758/BF03206904
   Leissa A.W., 1969, Vibration of plates
   LODHA S. K., 1997, P INT C AUD DISPL IC
   Mansur D L, 1985, J Med Syst, V9, P163, DOI 10.1007/BF00996201
   Mauney B. S., 2004, P ICAD 04 10 M INT C
   MEZRICH JJ, 1984, J AM STAT ASSOC, V79, P34, DOI 10.2307/2288331
   Minghim R, 1995, VISUALIZATION '95 - PROCEEDINGS, P110, DOI 10.1109/VISUAL.1995.480802
   MIS, 2009, MUS INSTR SAMPL
   MOOG, 2010, MOOG HAPT
   MOULINES E, 1990, SPEECH COMMUN, V9, P453, DOI 10.1016/0167-6393(90)90021-Z
   Nesbitt KV, 2004, IEEE COMPUT GRAPH, V24, P45, DOI 10.1109/MCG.2004.28
   Norman JF, 2004, PERCEPT PSYCHOPHYS, V66, P342, DOI 10.3758/BF03194883
   Rizzolatti G, 2004, ANNU REV NEUROSCI, V27, P169, DOI 10.1146/annurev.neuro.27.070203.144230
   Roseman D, 1997, VISUALIZATION AND MATHEMATICS, P67
   Rossing T.D Fletcher N.H., 2004, PRINCIPLES VIBRATION, VSecond, DOI DOI 10.1007/978-1-4757-3822-3
   Roth P, 2002, J VISUAL IMPAIR BLIN, V96, P420, DOI 10.1177/0145482X0209600605
   SAURO J., 2010, P ACM C HUM FACT COM
   SCALETTI C, 1994, SFI S SCI C, V18, P223
   SHELLEY S., 2009, P INT WORKSH HAPT AU
   SMITH S., 1990, P AAACM C COMPUTER I, P125
   Stevens RD, 1997, HUM-COMPUT INTERACT, V12, P47, DOI 10.1207/s15327051hci1201&2_3
   STOCKMAN T., 2005, P INT C AUD DISPL IC
   VAN DEN DOEL K., 2004, P 10 INT C AUD DISPL
   VANDENDOEL K, 2004, AUDIO ANECDOTES TOOL
   Varela Francisco J., 1992, The Embodied Mind. Cognitive Science and Human Experience
   von Bismarck G., 1974, Acustica, V30, P159
   von Bismarck G., 1974, Acustica, V30, P146
   WALKER B. N., 2003, P INT C AUD DISPL IC
   Walker B. N., 2005, ACM Transactions on Applied Perception, V2, P407, DOI [10.1145/1101530.1101534, DOI 10.1145/1101530.1101534]
   Walker BN, 2002, J EXP PSYCHOL-APPL, V8, P211, DOI 10.1037/1076-898X/8.4.211
   Walker BN, 2007, APPL COGNITIVE PSYCH, V21, P579, DOI 10.1002/acp.1291
   WEISSTEIN E. W., 2011, CUBIC SPLINE
   WEISSTEIN E. W., 2011, CURVATURE
   WIER CC, 1977, J ACOUST SOC AM, V61, P178, DOI 10.1121/1.381251
   WILSON C. M., 1996, P INT C AUD DISPL IC
NR 57
TC 15
Z9 15
U1 0
U2 11
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD OCT
PY 2012
VL 9
IS 4
AR 17
DI 10.1145/2355598.2355600
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 025DJ
UT WOS:000310164900002
DA 2024-07-18
ER

PT J
AU Morvan, Y
   O'Sullivan, C
AF Morvan, Yann
   O'Sullivan, Carol
TI Handling Occluders in Transitions from Panoramic Images: A Perceptual
   Study
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Experimentation; Human Factors; Algorithms; User study; panorama;
   transitioning; content mixing; occlusion
AB Panoramic images are very effective at conveying a visual sense of presence at very low cost and great ease of authoring. They are, however, limited in the navigation options they offer, unlike 3D representations. It is therefore desirable to provide pleasing transitions from one panorama to another, or from a panorama to a 3D model. We focus on motions where the viewers move toward an area of interest, and on the problem of dealing with occluders in their path. We discuss existing transition approaches, with emphasis on the additional information they require and on the constraints they place on the authoring process. We propose a compromise approach based on faking the parallax effect with occluder mattes. We conduct a user study to determine whether additional information does in fact increase the visual appeal of transitions. We observe that the creation of occluder mattes alone is only justified if the fake parallax effect can be synchronized with the camera motion ( but not necessarily consistent with it), and if viewpoint discrepancies at occlusion boundaries are small. The faster the transition, the less perceptual value there is in creating mattes. Information on view alignment is always useful, as a dissolve effect is always preferred to fading to black and back.
C1 [Morvan, Yann; O'Sullivan, Carol] Trinity Coll Dublin, Sch Comp Sci & Stat, Coll Green, Dublin 2, Ireland.
C3 Trinity College Dublin
RP Morvan, Y (corresponding author), Trinity Coll Dublin, Sch Comp Sci & Stat, Coll Green, Dublin 2, Ireland.
EM ymorvan@cs.tcd.ie
OI O'Sullivan, Carol/0000-0003-3772-4961
CR Bertalmio M, 2000, COMP GRAPH, P417, DOI 10.1145/344779.344972
   CHIANG CC, 1997, P ACM S VIRT REAL SO, P147
   Cohen J., 1988, STAT POWER ANAL BEHA
   Criminisi A, 2000, INT J COMPUT VISION, V40, P123, DOI 10.1023/A:1026598000963
   Frueh C, 2004, 2ND INTERNATIONAL SYMPOSIUM ON 3D DATA PROCESSING, VISUALIZATION, AND TRANSMISSION, PROCEEDINGS, P396, DOI 10.1109/TDPVT.2004.1335266
   Gledhill D, 2003, COMPUT GRAPH-UK, V27, P435, DOI 10.1016/S0097-8493(03)00038-4
   Horry Y., 1997, SIGGRAPH 97, P225
   Kazhdan M., 2006, P 4 EUR S GEOM PROC, P61, DOI DOI 10.2312/SGP/SGP06/061-070
   Lowe, 1999, P INT C COMP VIS, P1150, DOI DOI 10.1109/ICCV.1999.790410
   McMillan L., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P39, DOI 10.1145/218380.218398
   Müller P, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1276377.1276484, 10.1145/1239451.1239536]
   Pollefeys M, 2008, INT J COMPUT VISION, V78, P143, DOI 10.1007/s11263-007-0086-4
   SNAVELY N, 2007, INT J COMP IN PRESS
   Snavely N, 2008, PROC CVPR IEEE, P2617
   Stich T, 2008, APGV 2008: PROCEEDINGS OF THE SYMPOSIUM ON APPLIED PERCEPTION IN GRAPHICS AND VISUALIZATION, P97
   Vaina L.M., 2004, OPTIC FLOW
   Vincent L, 2007, COMPUTER, V40, P118, DOI 10.1109/MC.2007.442
   Watanabe M, 1998, INT J COMPUT VISION, V27, P203, DOI 10.1023/A:1007905828438
   Zhu ZG, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P723, DOI 10.1109/ICCV.2001.937698
NR 19
TC 5
Z9 5
U1 0
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD SEP
PY 2009
VL 6
IS 4
AR 25
DI 10.1145/1609967.1609972
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 511ZV
UT WOS:000271212300005
DA 2024-07-18
ER

PT J
AU Kuhl, SA
   Creem-Regehr, SH
   Thompson, WB
AF Kuhl, Scott A.
   Creem-Regehr, Sarah H.
   Thompson, William B.
TI Recalibration of Rotational Locomotion in Immersive Virtual Environments
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Experimentation; Human Factors; Perception; recalibration; rotation;
   virtual environments
ID CALIBRATION; ADAPTATION
AB This work uses an immersive virtual environment (IVE) to examine how people maintain a calibration between biomechanical and visual information for rotational self-motion. First, we show that no rotational recalibration occurs when visual and biomechanical rates of rotation are matched. Next, we demonstrate that mismatched physical and visual rotation rates cause rotational recalibration. Although previous work has shown that rotational locomotion can be recalibrated in real environments, this work extends the finding to virtual environments. We further show that people do not completely recalibrate left and right rotations independently when different visual-biomechanical discrepancies are used for left and right rotations during a recalibration phase. Finally, since the majority of participants did not notice mismatched physical and visual rotation rates, we discuss the implications of using such mismatches to enable IVE users to explore a virtual space larger than the physical space they are in.
C1 [Kuhl, Scott A.; Thompson, William B.] Univ Utah, Sch Comp, Salt Lake City, UT 84112 USA.
   [Creem-Regehr, Sarah H.] Univ Utah, Dept Psychol, Salt Lake City, UT 84112 USA.
C3 Utah System of Higher Education; University of Utah; Utah System of
   Higher Education; University of Utah
RP Kuhl, SA (corresponding author), Univ Utah, Sch Comp, 50 So Cent Campus Dr,Room 3190, Salt Lake City, UT 84112 USA.
EM skuhl@cs.utah.edu; sarah.creem@psych.utah.edu; thompson@cs.utah.edu
FU National Science Foundation [IIS-0080986, IIS-0080999, IIS-0121084]
FX This work was supported by National Science Foundation grants
   IIS-0080986, IIS-0080999, and IIS-0121084.
CR [Anonymous], 2001, P EUR
   BLES W, 1984, ACTA OTO-LARYNGOL, V97, P213, DOI 10.3109/00016488409130982
   Durgin F.H., 2002, ABSTRACTS PSYCHONOMI, V7, P103
   ISRAEL I, 1995, ACTA OTO-LARYNGOL, V115, P3, DOI 10.3109/00016489509133338
   Ivanenko YP, 1998, NEUROSCI LETT, V241, P167
   Jaekl P, 2003, J VESTIBUL RES-EQUIL, V13, P265
   KUHL SA, 2004, P S APPL PERC GRAPH, P23
   Loomis JM, 1999, BEHAV RES METH INS C, V31, P557, DOI 10.3758/BF03200735
   Mohler BJ, 2007, ACM T APPL PERCEPT, V4, DOI [10.1145/1227134.1227138, 10.1145/1227134/1227138]
   Pick HL, 1999, J EXP PSYCHOL HUMAN, V25, P1179, DOI 10.1037/0096-1523.25.5.1179
   RIESER JJ, 1995, J EXP PSYCHOL HUMAN, V21, P480, DOI 10.1037/0096-1523.21.3.480
   Viaud-Delmon I, 1999, COGNITIVE BRAIN RES, V7, P507, DOI 10.1016/S0926-6410(98)00052-4
   WILLIAMS B, 2006, P 3 S APPL PERC GRAP
   Withagen R, 2002, ECOL PSYCHOL, V14, P223, DOI 10.1207/S15326969ECO1404_2
   [No title captured]
NR 15
TC 11
Z9 12
U1 0
U2 0
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD AUG
PY 2008
VL 5
IS 3
AR 17
DI 10.1145/1402236.1402241
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA 450YK
UT WOS:000266437700005
DA 2024-07-18
ER

PT J
AU Scott, JJ
   Dodgson, NA
AF Scott, Joshua J.
   Dodgson, Neil A.
TI Evaluating Realism in Example-based Terrain Synthesis
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Terrain; example-based; evaluation; believability; realism
ID GENERATION; VALLEY
AB We report two studies that investigate the use of subjective believability in the assessment of objective realism of terrain. The first demonstrates that there is a clear subjective feature bias that depends on the types of terrain being evaluated: Our participants found certain natural terrains to be more believable than others. This confounding factor means that any comparison experimentmust not ask participants to compare terrains with different types of features. Our second experiment assesses four methods of example-based terrain synthesis, comparing them against each other and against real terrain. Our results show that, while all tested methods can produce terrain that is indistinguishable from reality, all also can produce poor terrain; that there is no one method that is consistently better than the others; and that those who have professional expertise in geology, cartography, or image analysis are better able to distinguish real terrain from synthesized terrain than the general population, but those who have professional expertise in the visual arts are not.
C1 [Scott, Joshua J.; Dodgson, Neil A.] Victoria Univ Wellington, Sch Engn & Comp Sci, POB 600, Wellington 6140, New Zealand.
C3 Victoria University Wellington
RP Scott, JJ (corresponding author), Victoria Univ Wellington, Sch Engn & Comp Sci, POB 600, Wellington 6140, New Zealand.
EM jjscott.nz@gmail.com; neil.dodgson@vuw.ac.nz
RI Dodgson, Neil/A-4506-2009
OI Dodgson, Neil/0000-0001-7649-8528; Scott, Joshua/0000-0002-0243-0413
FU Wellington Doctoral Scholarship from Victoria University of Wellington
FX Joshua Scott was supported by a Wellington Doctoral Scholarship from
   Victoria University of Wellington.
CR Adams Daniel, 2012, P ACM SIGGRAPH S INT, P208, DOI [10.1145/2159616.2159654, DOI 10.1145/2159616.2159654]
   [Anonymous], 2013, P INT S SKETCH BAS I, DOI DOI 10.1145/2487381.2487382
   [Anonymous], 2005, GRAPHITE 05 P 3 INT, DOI [DOI 10.1145/1101389.1101479, 10.1145/1101389.1101479.3, DOI 10.1145/1101389.1101479.3]
   [Anonymous], 2008, Proceedings of the 2008 ACM SIGGRAPH/Eurographics Symposium on Computer Animation, SCA '08
   [Anonymous], 2004, Shuttle Radar Topography Mission
   Becher M., 2017, P ACM SIGGRAPH S INT, DOI DOI 10.1145/3023368.3023383
   Bene B, 2006, P 22 SPRING C COMP G, P17
   Benes B, 2006, COMPUT ANIMAT VIRT W, V17, P99, DOI 10.1002/cav.77
   Benes J, 2017, COMPUT GRAPH FORUM, V36, P225, DOI 10.1111/cgf.13121
   Bernhardt A., 2011, Proceedings of the 2011 24th SIBGRAPI Conference on Graphics, Patterns and Images (SIBGRAPI 2011), P64, DOI 10.1109/SIBGRAPI.2011.28
   BRADLEY RA, 1952, BIOMETRIKA, V39, P324, DOI 10.1093/biomet/39.3-4.324
   Brosz J, 2007, COMM COM INF SC, V4, P58
   Chang YC, 1998, COMPUT GEOSCI, V24, P83, DOI 10.1016/S0098-3004(97)00078-2
   Chiba N, 1998, J VISUAL COMP ANIMAT, V9, P185, DOI 10.1002/(SICI)1099-1778(1998100)9:4<185::AID-VIS178>3.0.CO;2-2
   Cordonnier G, 2018, IEEE T VIS COMPUT GR, V24, P1756, DOI 10.1109/TVCG.2017.2689022
   Cordonnier G, 2016, COMPUT GRAPH FORUM, V35, P165, DOI 10.1111/cgf.12820
   Cruz Leandro, 2015, Proceedings of the 10th International Conference on Computer Graphics Theory and Applications (GRAPP 2015), P189
   Cruz Leandro, 2015, THESIS I NACL MATEMA
   DAVIDSON RR, 1976, BIOMETRICS, V32, P241
   Elhelw M, 2008, ACM T APPL PERCEPT, V5, DOI 10.1145/1279640.1279643
   FOURNIER A, 1982, COMMUN ACM, V25, P371, DOI 10.1145/358523.358553
   Gain J, 2015, COMPUT GRAPH FORUM, V34, P105, DOI 10.1111/cgf.12545
   Gain J., 2009, P 2009 S INT 3D GRAP, V1, P31, DOI [10.1145/1507149.1507155, DOI 10.1145/1507149.1507155]
   Galin E, 2019, COMPUT GRAPH FORUM, V38, P553, DOI 10.1111/cgf.13657
   Golubev K, 2016, PROCEDIA COMPUT SCI, V101, P152, DOI 10.1016/j.procs.2016.11.019
   Goral C. M., 1984, Computers & Graphics, V18, P213
   Guérin E, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130804
   Han C, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360650
   Hnaidi H, 2010, COMPUT GRAPH FORUM, V29, P2179, DOI 10.1111/j.1467-8659.2010.01806.x
   HOLM S, 1979, SCAND J STAT, V6, P65
   Kolár M, 2017, COMPUT GRAPH FORUM, V36, P189, DOI 10.1111/cgf.13118
   Kwatra V, 2005, ACM T GRAPHIC, V24, P795, DOI 10.1145/1073204.1073263
   Lefebvre S, 2006, ACM T GRAPHIC, V25, P541, DOI 10.1145/1141911.1141921
   LEWIS JP, 1987, ACM T GRAPHIC, V6, P167, DOI 10.1145/35068.35069
   Mandelbrot B. B., 1982, FRACTAL GEOMETRY NAT
   MANDELBROT BB, 1975, P NATL ACAD SCI USA, V72, P3825, DOI 10.1073/pnas.72.10.3825
   Mandelbrot BB., 1988, The science of fractal images, V1st, P243
   Miller G. S. P., 1986, Computer Graphics, V20, P39, DOI 10.1145/15886.15890
   Mossman Jim, 2001, NEW COLOR SYSTEM ENH
   Musgrave F. K., 1989, Computer Graphics, V23, P41, DOI 10.1145/74334.74337
   Nagashima K, 1997, VISUAL COMPUT, V13, P456, DOI 10.1007/s003710050117
   Natali M., 2013, Eurographics (stars), P155
   Neidhold B., 2005, Natural Phenomena, P25
   QGIS Development Team, 2020, AllWISE Source Catalog, IPAC, DOI [DOI 10.26131/IRSA1, 10.26131/IRSA1]
   Qualtrics Labs Inc., 2009, QUALTR
   Rademacher P, 2001, SPRING EUROGRAP, P235
   Rajasekaran SD, 2022, ACM T APPL PERCEPT, V19, DOI 10.1145/3514244
   Reinhard E., 2004, Proceedings of the 1st Symposium on Applied perception in graphics and visualization, APGV '04, P99, DOI [10.1145/1012551.1012568, DOI 10.1145/1012551.1012568]
   Rusnell B, 2009, VISUAL COMPUT, V25, P573, DOI 10.1007/s00371-009-0332-6
   Saunders R. L, 2006, THESIS TEXAS A M U
   Scott JJ, 2021, COMPUT GRAPH-UK, V99, P43, DOI 10.1016/j.cag.2021.06.012
   Scott Joshua James, 2020, THESIS VICTORIA U WE
   Soille P, 2004, PATTERN RECOGN LETT, V25, P543, DOI 10.1016/j.patrec.2003.12.007
   Stachniak S., 2005, Computer Graphics and Artificial Intelligence, V1, P64
   Tasse FP, 2012, COMPUT GRAPH FORUM, V31, P1959, DOI 10.1111/j.1467-8659.2012.03076.x
   TUKEY JW, 1949, BIOMETRICS, V5, P99, DOI 10.2307/3001913
   Vanhoey K, 2017, ACM T APPL PERCEPT, V15, DOI 10.1145/3129505
   Wu Q, 2004, ACM T GRAPHIC, V23, P364, DOI 10.1145/1015706.1015730
   Zhou H, 2007, IEEE T VIS COMPUT GR, V13, P834, DOI 10.1109/TVCG.2007.1027
NR 59
TC 2
Z9 2
U1 0
U2 2
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD JUL
PY 2022
VL 19
IS 3
AR 11
DI 10.1145/3531526
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 4L3FO
UT WOS:000852517200003
OA Green Submitted, Bronze
DA 2024-07-18
ER

PT J
AU Ferstl, Y
   McKay, M
   McDonnell, R
AF Ferstl, Ylva
   McKay, Michael
   McDonnell, Rachel
TI Facial Feature Manipulation for Trait Portrayal in Realistic and
   Cartoon-Rendered Characters
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Character design; physiognomy; face perception
ID PERSONALITY; WIDTH
AB Previous perceptual studies on human faces have shown that specific facial features have consistent effects on perceived personality and appeal, but it remains unclear if and how findings relate to perception of virtual characters. For example, wider human faces have been found to appear more aggressive and dominant, whereas studies on virtual characters have shown opposite trends but have suffered from significant eeriness of exaggerated features. In this study, we use highly realistic virtual faces obtained from 3D scanning, as well as cartoon-rendered counterparts retaining facial proportions. We assess the effects of facial width and eye size on perceptions of appeal, trustworthiness, aggressiveness, dominance, and eeriness. Our manipulations did not affect eeriness, and we find the same perceptual trends previously reported for human faces.
C1 [Ferstl, Ylva; McKay, Michael; McDonnell, Rachel] Trinity Coll Dublin, Coll Green, Dublin 2, Ireland.
C3 Trinity College Dublin
RP Ferstl, Y (corresponding author), Trinity Coll Dublin, Coll Green, Dublin 2, Ireland.
EM yferstl@tcd.ie; MCKAYMI@tcd.ie; RAMCDONN@tcd.ie
RI McDonnell, Rachel/HGC-4337-2022
OI McDonnell, Rachel/0000-0002-1957-2506; Ferstl, Ylva/0000-0001-7259-0378
FU ADAPT Centre for Digital Content Technology [13/RC/2106_P2, 19/FFP/6409]
FX This research was funded under the ADAPT Centre for Digital Content
   Technology (Grant No. 13/RC/2106_P2) and RADICal (Grant No.
   19/FFP/6409).
CR ALBRIGHT L, 1988, J PERS SOC PSYCHOL, V55, P387, DOI 10.1037/0022-3514.55.3.387
   Carré JM, 2009, PSYCHOL SCI, V20, P1194, DOI 10.1111/j.1467-9280.2009.02423.x
   DiSalvo C.F., 2002, P 4 C DES INT SYST P, P321, DOI [10.1145/778712.778756, DOI 10.1145/778712.778756]
   Ferstl Y., 2016, P ACM S APPL PERC, DOI [https://doi.org/10.1145/2931002.2931014, DOI 10.1145/2931002.2931014]
   Ferstl Y, 2018, 18TH ACM INTERNATIONAL CONFERENCE ON INTELLIGENT VIRTUAL AGENTS (IVA'18), P281, DOI 10.1145/3267851.3267891
   Geniole SN, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0132726
   KEATING CF, 1985, SOC PSYCHOL QUART, V48, P61, DOI 10.2307/3033782
   Kramer RSS, 2010, Q J EXP PSYCHOL, V63, P2273, DOI 10.1080/17470211003770912
   Little AC, 2007, BRIT J PSYCHOL, V98, P111, DOI 10.1348/000712606X109648
   McDonnell R, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185587
   Ormiston ME, 2017, PERS INDIV DIFFER, V105, P40, DOI 10.1016/j.paid.2016.09.017
   Pound N, 2007, PERS INDIV DIFFER, V43, P1572, DOI 10.1016/j.paid.2007.04.014
   Song Y, 2021, APPL ERGON, V94, DOI 10.1016/j.apergo.2021.103420
   Song Y, 2021, COMPUT HUM BEHAV, V116, DOI 10.1016/j.chb.2020.106620
   Stirrat M, 2010, PSYCHOL SCI, V21, P349, DOI 10.1177/0956797610362647
   Torre I., 2019, MOTION INTERACTION G, P1
   Wang YQ, 2013, INT CONF AFFECT, P479, DOI 10.1109/ACII.2013.85
   Wisessing P, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3383195
   Zebrowitz LA, 1996, PERS SOC PSYCHOL B, V22, P1258, DOI 10.1177/01461672962212006
   Zibrek K., 2014, P ACM S APPL PERCEPT, P111, DOI DOI 10.1145/2628257.2628270
NR 20
TC 3
Z9 3
U1 3
U2 19
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD OCT
PY 2021
VL 18
IS 4
AR 22
DI 10.1145/3486579
PG 8
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA YY1DS
UT WOS:000754533100006
OA Bronze
DA 2024-07-18
ER

PT J
AU Tennison, JL
   Uesbeck, PM
   Giudice, NA
   Stefik, A
   Smith, DW
   Gorlewicz, JL
AF Tennison, Jennifer L.
   Uesbeck, P. Merlin
   Giudice, Nicholas A.
   Stefik, Andreas
   Smith, Derrick W.
   Gorlewicz, Jenna L.
TI Establishing Vibration-Based Tactile Line Profiles for Use in Multimodal
   Graphics
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Haptics; touchscreen; perception; HCI
ID VISION; TOUCH; INFORMATION; DISPLAYS; MEMORY
AB Vibration plays a significant role in the way users interact with touchscreens. For many users, vibration affords tactile alerts and other enhancements. For eyes-free users and users with visual impairments, vibration can also serve a more primary role in the user interface, such as indicating streets on maps, conveying information about graphs, or even specifying basic graphics. However, vibration is rarely used in current user interfaces beyond basic cuing. Furthermore, designers and developers who do actually use vibration more extensively are often unable to determine the exact properties of the vibration signals they are implementing, due to out-of-the-box software and hardware limitations. We make two contributions in this work. First, we investigate the contextual properties of touchscreen vibrations and how vibrations can be used to effectively convey traditional, embossed elements, such as dashes and dots. To do so, we developed an open source, Android-based library to generate vibrations that are perceptually salient and intuitive, improving upon existing vibration libraries. Second, we conducted a user study with 26 blind or visually impaired users to evaluate and categorize the effects with respect to traditional tactile line profiles. We have established a range of vibration effects that can be reliably generated by our haptic library and are perceptible and distinguishable by users.
C1 [Tennison, Jennifer L.; Gorlewicz, Jenna L.] St Louis Univ, 3450 Lindell Blvd, St Louis, MO 63103 USA.
   [Uesbeck, P. Merlin; Stefik, Andreas] Univ Nevada, 4505 S Maryland Pkwy, Las Vegas, NV 89154 USA.
   [Giudice, Nicholas A.] Univ Maine, 168 Coll Ave, Orono, ME 04469 USA.
   [Smith, Derrick W.] Univ Alabama, Shelby Ctr Sci & Technol, 301 Sparkman Dr NW, Huntsville, AL 35899 USA.
C3 Saint Louis University; Nevada System of Higher Education (NSHE);
   University of Nevada Las Vegas; University of Maine System; University
   of Maine Orono; University of Alabama System; University of Alabama
   Huntsville
RP Tennison, JL (corresponding author), St Louis Univ, 3450 Lindell Blvd, St Louis, MO 63103 USA.
EM jen.tennison@slu.edu; uesbeck@unlv.nevada.edu;
   nicholas.giudice@maine.edu; andreas.stefik@unlv.edu;
   derrick.smith@uah.edu; jenna.gorlewicz@slu.edu
FU National Science Foundation [1644538, 1644491, 1644471, 1644476];
   Graduate Research Fellowship Program (GRFP); Division Of Research On
   Learning; Direct For Education and Human Resources [1644538] Funding
   Source: National Science Foundation; Division Of Research On Learning;
   Direct For Education and Human Resources [1644476, 1644491, 1644471]
   Funding Source: National Science Foundation
FX This work was supported by the National Science Foundation under grants
   1644538, 1644491, 1644471, and 1644476, and the Graduate Research
   Fellowship Program (GRFP). Any opinions, findings, and conclusions or
   recommendations expressed in this material are those of the author(s)
   and do not necessarily reflect the views of the National Science
   Foundation.
CR Al-Qudah Z., 2011, Proceedings of the 2011 International Conference on User Science and Engineering (i-USEr 2011), P118, DOI 10.1109/iUSEr.2011.6150549
   American Printing House for the Blind, 2018, GRAPH
   [Anonymous], 2004, Cogn. Processing, DOI DOI 10.1007/S10339-004-0012-4
   Apple iOS, 2019, HAPT US INT
   Choi S, 2013, P IEEE, V101, P2093, DOI 10.1109/JPROC.2012.2221071
   Chua B, 2004, BRIT J OPHTHALMOL, V88, P1119, DOI 10.1136/bjo.2004.041863
   CLARKCARTER DD, 1986, ERGONOMICS, V29, P779, DOI 10.1080/00140138608968314
   Erickson W., 2012, Disability statistics from the 2011 American community survey (ACS)
   Gescheider George A., 2013, PSYCHOPHYSICS FUNDAM
   Gescheider George A., 2010, INFORM PROCESSING CH
   Giudice N. A., 2012, P 14 INT ACM SIGACCE, P103, DOI [DOI 10.1145/2384916.2384935, 10.1145/2384916.2384935]
   Goldstein B., 2002, Sensation and perception, V6th
   Goncu C, 2011, LECT NOTES COMPUT SC, V6946, P30, DOI 10.1007/978-3-642-23774-4_5
   Gorlewicz JL, 2014, J SPEC EDUC TECHNOL, V29, P17, DOI 10.1177/016264341402900202
   Gorlewicz JennaL., 2018, Interactive Multimedia
   Grussenmeyer W, 2017, ACM T ACCESS COMPUT, V9, DOI 10.1145/3022701
   Hahn M., J VISUAL IMPAIRMENT
   Hoggan E., 2008, PROC INT C MULTIMODA, P157
   Hoggan E, 2007, LECT NOTES COMPUT SC, V4813, P22
   Hoggan E, 2008, CHI 2008: 26TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS VOLS 1 AND 2, CONFERENCE PROCEEDINGS, P1573
   Jayant C, 2010, ASSETS 2010: PROCEEDINGS OF THE 12TH INTERNATIONAL ACM SIGACCESS CONFERENCE ON COMPUTERS AND ACCESSIBILITY, P295
   Johansson RS, 2009, NAT REV NEUROSCI, V10, P345, DOI 10.1038/nrn2621
   Jones L. A., 2006, HUMAN HAND FUNCTION
   Jones LA, 2008, HUM FACTORS, V50, P90, DOI 10.1518/001872008X250638
   Klatzky RL, 2014, MULTISENS RES, V27, P359, DOI 10.1163/22134808-00002447
   Martino G, 2000, PERCEPTION, V29, P745, DOI 10.1068/p2984
   Nees MichaelA., 2009, Auditory Interfaces and Sonification
   Nicolau Hugo., 2013, P 15 INT ACM SIGACCE, V23, P1, DOI DOI 10.1145/2513383.2513437
   Nyman SR, 2010, BRIT J OPHTHALMOL, V94, P1427, DOI 10.1136/bjo.2009.164814
   O'Modhrain S, 2015, IEEE T HAPTICS, V8, P248, DOI 10.1109/TOH.2015.2466231
   Okamura A.M., 2008, Haptic Rendering, P440
   Palani H.P., 2014, P 16 INT ACM SIGACCE, DOI [10.1145/2661334.2661336, DOI 10.1145/2661334.2661336]
   Palani H, 2016, LECT NOTES COMPUT SC, V9738, P162, DOI 10.1007/978-3-319-40244-4_16
   Palani HariPrasath., 2018, International Conference on Applied Human Factors and Ergonomics, P837
   Pensky AEC, 2008, PSYCHON B REV, V15, P574, DOI 10.3758/PBR.15.3.574
   Pew Research Center, 2019, MOB FACT SHEET DEM M
   Poppinga B., 2011, P 13 INT C HUM COMP, P545, DOI [DOI 10.1145/2037373.2037458, 10.1145/2037373.2037458]
   Poupyrev I., 2002, P 15 ANN ACM S USER, P51
   Rantala J, 2009, IEEE T HAPTICS, V2, P28, DOI 10.1109/ToH.2009.3
   Ricciardi E, 2006, NEUROSCIENCE, V139, P339, DOI 10.1016/j.neuroscience.2005.08.045
   Roth WM, 1999, J RES SCI TEACH, V36, P977, DOI 10.1002/(SICI)1098-2736(199911)36:9<977::AID-TEA3>3.0.CO;2-V
   Russomanno A, 2015, 2015 IEEE WORLD HAPTICS CONFERENCE (WHC), P177, DOI 10.1109/WHC.2015.7177710
   Ryu J, 2010, PRESENCE-TELEOP VIRT, V19, P364, DOI 10.1162/PRES_a_00011
   Stefik Andreas, 2020, ANDROID HAPTIC LIB
   Stefik Andreas., 2020, Android Vibration Tutorial
   Stefik Andreas, 2020, ANDROIDHAPTIC LIB
   Stevens JC, 1996, J EXP PSYCHOL-APPL, V2, P91, DOI 10.1037/1076-898X.2.2.91
   Swerdfeger B.A., 2009, Proceedings of Graphics Interface 2009. Canadian Information Processing Society, P133
   Tan HZ, 1999, PERCEPT PSYCHOPHYS, V61, P993, DOI 10.3758/BF03207608
   Tennison JL, 2019, ACM T APPL PERCEPT, V16, DOI 10.1145/3301415
   Tennison JL, 2018, OPTOMETRY VISION SCI, V95, P720, DOI 10.1097/OPX.0000000000001274
   Tennison JL, 2016, LECT NOTES COMPUT SC, V9775, P384, DOI 10.1007/978-3-319-42324-1_38
   Van Boven RW, 2000, NEUROLOGY, V54, P2230
   van Erp J., 2003, P EUROHAPTICS, V2003, P111
   Walker, 2010, ACM Transactions on Accessible Computing (TACCESS), V2, P12, DOI DOI 10.1145/1714458.1714459
   Walker BruceN., 2003, Sonification Sandbox: A graphical toolkit for auditory graphs
   Yatani K, 2009, UIST 2009: PROCEEDINGS OF THE 22ND ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P111
NR 57
TC 10
Z9 10
U1 0
U2 6
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD MAY
PY 2020
VL 17
IS 2
AR 7
DI 10.1145/3383457
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA OH5JF
UT WOS:000582618500003
DA 2024-07-18
ER

PT J
AU Tadros, T
   Cullen, NC
   Greene, MR
   Cooper, EA
AF Tadros, Timothy
   Cullen, Nicholas C.
   Greene, Michelle R.
   Cooper, Emily A.
TI Assessing Neural Network Scene Classification from Degraded Images
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Human perception; human scene recognition
ID CONFIDENCE-INTERVALS; VISION; CATEGORIZATION; RATIO
AB Scene recognition is an essential component of both machine and biological vision. Recent advances in computer vision using deep convolutional neural networks (CNNs) have demonstrated impressive sophistication in scene recognition, through training on large datasets of labeled scene images (Zhou et al. 2018, 2014). One criticism of CNN-based approaches is that performance may not generalize well beyond the training image set (Torralba and Efros 2011), and may be hampered by minor image modifications, which in some cases are barely perceptible to the human eye (Goodfellow et al. 2015; Szegedy et al. 2013). While these "adversarial examples" may be unlikely in natural contexts, during many real-world visual tasks scene information can be degraded or limited due to defocus blur, camera motion, sensor noise, or occluding objects. Here, we quantify the impact of several image degradations (some common, and some more exotic) on indoor/outdoor scene classification using CNNs. For comparison, we use human observers as a benchmark, and also evaluate performance against classifiers using limited, manually selected descriptors. While the CNNs outperformed the other classifiers and rivaled human accuracy for intact images, our results show that their classification accuracy is more affected by image degradations than human observers. On a practical level, however, accuracy of the CNNs remained well above chance for a wide range of image manipulations that disrupted both local and global image statistics. We also examine the level of image-by-image agreement with human observers, and find that the CNNs' agreement with observers varied as a function of the nature of image manipulation. In many cases, this agreement was not substantially different from the level one would expect to observe for two independent classifiers. Together, these results suggest that CNN-based scene classification techniques are relatively robust to several image degradations. However, the pattern of classifications obtained for ambiguous images does not appear to closely reflect the strategies employed by human observers.
C1 [Tadros, Timothy] Univ Calif San Diego, Neurosci Grad Program, 9500 Gilman Dr, La Jolla, CA 92092 USA.
   [Cullen, Nicholas C.] Univ Penn, Perelman Sch Med, John Morgan Bldg,Room 132, Philadelphia, PA 19104 USA.
   [Greene, Michelle R.] Bates Coll, Program Neurosci, 3 Andrews Rd,Hathorn Hall, Lewiston, ME 04240 USA.
   [Cooper, Emily A.] Univ Calif Berkeley, Sch Optometry, Helen Wills Neurosci Inst, Minor Hall, Berkeley, CA 94720 USA.
C3 University of California System; University of California San Diego;
   University of Pennsylvania; University of California System; University
   of California Berkeley
RP Tadros, T (corresponding author), Univ Calif San Diego, Neurosci Grad Program, 9500 Gilman Dr, La Jolla, CA 92092 USA.
EM tttadros@ucsd.edu; ncullen@seas.upenn.edu; mgreene2@bates.edu;
   emilycooper@berkeley.edu
CR Agrawal P., 2014, PIXELS VOXELS MODELI
   [Anonymous], DEEP NEURAL NETWORKS
   [Anonymous], 2015, NATURE, DOI [DOI 10.1038/NATURE14539, 10.1038/nature14539]
   [Anonymous], 2010, P NIPS
   [Anonymous], 2014, P ICLR, DOI DOI 10.1021/CT2009208
   [Anonymous], INT C NEUR INT PROC
   [Anonymous], 2016, CoRR, DOI DOI 10.1109/QOMEX.2016.7498955
   [Anonymous], EXAMINING IMPACT BLU
   BENJAMINI Y, 1995, J R STAT SOC B, V57, P289, DOI 10.1111/j.2517-6161.1995.tb02031.x
   BIEDERMA.I, 1972, SCIENCE, V177, P77, DOI 10.1126/science.177.4043.77
   BIEDERMAN I, 1973, J EXP PSYCHOL, V97, P22, DOI 10.1037/h0033776
   Borji A, 2014, PROC CVPR IEEE, P113, DOI 10.1109/CVPR.2014.22
   Boucart M, 2013, VISION RES, V86, P35, DOI 10.1016/j.visres.2013.04.006
   Cadieu CF, 2014, PLOS COMPUT BIOL, V10, DOI 10.1371/journal.pcbi.1003963
   Dodge S, 2019, ACM T APPL PERCEPT, V16, DOI 10.1145/3306241
   Everingham MR, 2003, INT J HUM-COMPUT INT, V15, P231, DOI 10.1207/S15327590IJHC1502_3
   FUKUSHIMA K, 1988, NEURAL NETWORKS, V1, P119, DOI 10.1016/0893-6080(88)90014-7
   Geirhos Robert, 2018, Advances in Neural Information Processing Systems, P7549
   Goodfellow I. J., 2015, 3 INT C LEARNING REP
   Greene MR, 2018, PLOS COMPUT BIOL, V14, DOI 10.1371/journal.pcbi.1006327
   Greene MR, 2016, J EXP PSYCHOL GEN, V145, P82, DOI 10.1037/xge0000129
   Greene MR, 2013, FRONT PSYCHOL, V4, DOI 10.3389/fpsyg.2013.00777
   Greene MR, 2009, PSYCHOL SCI, V20, P464, DOI 10.1111/j.1467-9280.2009.02316.x
   Greene MR, 2009, COGNITIVE PSYCHOL, V58, P137, DOI 10.1016/j.cogpsych.2008.06.001
   Groen IIA, 2018, ELIFE, V7, DOI 10.7554/eLife.32962
   Groen IIA, 2017, PHILOS T R SOC B, V372, DOI 10.1098/rstb.2016.0102
   Güçlü U, 2015, J NEUROSCI, V35, P10005, DOI 10.1523/JNEUROSCI.5023-14.2015
   Guo CA, 2017, PR MACH LEARN RES, V70
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Jozwik KM, 2017, FRONT PSYCHOL, V8, DOI 10.3389/fpsyg.2017.01726
   Karagiannopoulos Stavros, 2016, 2016 Power Systems Computation Conference (PSCC), P1, DOI 10.1109/PSCC.2016.7540956
   KATZ D, 1978, BIOMETRICS, V34, P469, DOI 10.2307/2530610
   Khaligh-Razavi SM, 2014, PLOS COMPUT BIOL, V10, DOI 10.1371/journal.pcbi.1003915
   KOOPMAN PAR, 1984, BIOMETRICS, V40, P513, DOI 10.2307/2531405
   Kriegeskorte N, 2015, ANNU REV VIS SCI, V1, P417, DOI 10.1146/annurev-vision-082114-035447
   Kriegeskorte N, 2008, FRONT SYST NEUROSCI, V2, DOI 10.3389/neuro.06.004.2008
   Kubilius J, 2016, PLOS COMPUT BIOL, V12, DOI 10.1371/journal.pcbi.1004896
   Lazebnik S., COMPUTER VISION PATT, V2, P2169
   Manduchi R, 2012, COMMUN ACM, V55, P96, DOI 10.1145/2063176.2063200
   Manduchi R, 2010, ASSETS 2010: PROCEEDINGS OF THE 12TH INTERNATIONAL ACM SIGACCESS CONFERENCE ON COMPUTERS AND ACCESSIBILITY, P241
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   Parikh D, 2011, IEEE I CONF COMP VIS, P519, DOI 10.1109/ICCV.2011.6126283
   Peterson MF, 2016, J VISION, V16, DOI 10.1167/16.7.12
   Rajalingham R, 2018, J NEUROSCI, V38, P7255, DOI 10.1523/JNEUROSCI.0388-18.2018
   Renninger LW, 2004, VISION RES, V44, P2301, DOI 10.1016/j.visres.2004.04.006
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Stanislaw H, 1999, BEHAV RES METH INS C, V31, P137, DOI 10.3758/BF03207704
   Tatler BW, 2007, J VISION, V7, DOI 10.1167/7.14.4
   Torralba A, 2003, NETWORK-COMP NEURAL, V14, P391, DOI 10.1088/0954-898X/14/3/302
   Torralba A, 2011, PROC CVPR IEEE, P1521, DOI 10.1109/CVPR.2011.5995347
   Torralba A, 2009, VISUAL NEUROSCI, V26, P123, DOI 10.1017/S0952523808080930
   TVERSKY B, 1983, COGNITIVE PSYCHOL, V15, P121, DOI 10.1016/0010-0285(83)90006-3
   Vogel J, 2007, ACM T APPL PERCEPT, V4, DOI 10.1145/1278387.1278393
   Walther DB, 2014, PSYCHOL SCI, V25, P851, DOI 10.1177/0956797613512662
   Wang SH, 2014, J VIS COMMUN IMAGE R, V25, P263, DOI 10.1016/j.jvcir.2013.11.005
   Watson DM, 2017, SCI REP-UK, V7, DOI 10.1038/s41598-017-03974-5
   Xiao JX, 2010, PROC CVPR IEEE, P3485, DOI 10.1109/CVPR.2010.5539970
   Yamins D. L., 2013, Advances in Neural Information Processing Systems, V26
   Yamins DLK, 2016, NAT NEUROSCI, V19, P356, DOI 10.1038/nn.4244
   Zhao YH, 2016, UBICOMP'16: PROCEEDINGS OF THE 2016 ACM INTERNATIONAL JOINT CONFERENCE ON PERVASIVE AND UBIQUITOUS COMPUTING, P73, DOI 10.1145/2971648.2971730
   Zhou BL, 2014, ADV NEUR IN, V27
   Zhou BL, 2018, IEEE T PATTERN ANAL, V40, P1452, DOI 10.1109/TPAMI.2017.2723009
   Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319
   Zhou YR, 2017, INT CONF ACOUST SPEE, P1213, DOI 10.1109/ICASSP.2017.7952349
NR 65
TC 12
Z9 12
U1 0
U2 5
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD SEP
PY 2019
VL 16
IS 4
AR 21
DI 10.1145/3342349
PG 20
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA JE2HU
UT WOS:000490516700003
OA Bronze, Green Published
DA 2024-07-18
ER

PT J
AU Dodge, S
   Karam, L
AF Dodge, Samuel
   Karam, Lina
TI Human and DNN Classification Performance on Images With Quality
   Distortions: A Comparative Study
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Robust visual recognition; deep learning; human study
ID REPRESENTATION; OBJECT; SCENE
AB Image quality is an important practical challenge that is often overlooked in the design of machine vision systems. Commonly, machine vision systems are trained and tested on high-quality image datasets, yet in practical applications the input images cannot be assumed to be of high quality. Modern deep neural networks (DNNs) have been shown to perform poorly on images affected by blur or noise distortions. In this work, we investigate whether human subjects also perform poorly on distorted stimuli and provide a direct comparison with the performance of DNNs. Specifically, we study the effect of Gaussian blur and additive Gaussian noise on human and DNN classification performance. We perform two experiments: one crowd-sourced experiment with unlimited stimulus display time, and one lab experiment with 100ms display time. In both cases, we found that humans outperform neural networks on distorted stimuli, even when the networks are retrained with distorted data.
C1 [Dodge, Samuel; Karam, Lina] Arizona State Univ, GWC 439,650 E Tyler Mall, Tempe, AZ 85281 USA.
C3 Arizona State University; Arizona State University-Tempe
RP Dodge, S (corresponding author), Arizona State Univ, GWC 439,650 E Tyler Mall, Tempe, AZ 85281 USA.
EM sfdodge@asu.edu; karam@asu.edu
RI Karam, Lina Jamil/ABD-6531-2021
OI Karam, Lina Jamil/0000-0003-1870-1211
CR Andrews TJ, 1999, VISION RES, V39, P2947, DOI 10.1016/S0042-6989(99)00019-X
   [Anonymous], ARXIV170502406
   [Anonymous], 2017, Comparing deep neural networks against humans: object recognition when the signal gets weaker
   [Anonymous], ARXIV181105819
   [Anonymous], 2017, P IEEE C COMP VIS PA
   [Anonymous], PROC CVPR IEEE
   [Anonymous], 2015, PROC CVPR IEEE
   [Anonymous], 2016, LECT NOTES COMPUT SC, DOI DOI 10.1007/978-3-319-44781-0_45
   [Anonymous], 2014, P INT C LEARN REPR
   [Anonymous], 2016, CoRR, DOI DOI 10.1109/QOMEX.2016.7498955
   [Anonymous], P IEEE C COMP VIS PA
   [Anonymous], ARXIV170106487
   [Anonymous], IS T SPIE ELECT IMAG
   [Anonymous], 2017, ARXIV171008864
   BACHMANN T, 1991, European Journal of Cognitive Psychology, V3, P87, DOI 10.1080/09541449108406221
   Borji A, 2014, PROC CVPR IEEE, P113, DOI 10.1109/CVPR.2014.22
   Cadieu CF, 2014, PLOS COMPUT BIOL, V10, DOI 10.1371/journal.pcbi.1003963
   Chen L, 2017, IUI'17: PROCEEDINGS OF THE 22ND INTERNATIONAL CONFERENCE ON INTELLIGENT USER INTERFACES, P17, DOI 10.1145/3025171.3025173
   Chen Y, 2015, PSYCHIAT RES, V225, P619, DOI 10.1016/j.psychres.2014.11.035
   Dodge Samuel, 2017, ARXIV170308119
   Fleuret F, 2011, P NATL ACAD SCI USA, V108, P17621, DOI 10.1073/pnas.1109168108
   Goodfellow I. J., 2015, 3 INT C LEARNING REP
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Karras T., 2018, INT CONFLEARN REPRES
   Keysers C, 2001, J COGNITIVE NEUROSCI, V13, P90, DOI 10.1162/089892901564199
   Kheradpisheh SR, 2016, SCI REP-UK, V6, DOI 10.1038/srep32672
   Li FF, 2007, COMPUT VIS IMAGE UND, V106, P59, DOI 10.1016/j.cviu.2005.09.012
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   Potter MC, 2014, ATTEN PERCEPT PSYCHO, V76, P270, DOI 10.3758/s13414-013-0605-z
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Szegedy Christian, 2015, IEEE C COMP VIS PATT, DOI [10.1109/cvpr.2015.7298594, DOI 10.1109/CVPR.2015.7298594]
   Tao JW, 2016, NEURAL NETWORKS, V76, P135, DOI 10.1016/j.neunet.2016.01.008
   Torralba A, 2008, IEEE T PATTERN ANAL, V30, P1958, DOI 10.1109/TPAMI.2008.128
   Ullman S, 2016, P NATL ACAD SCI USA, V113, P2744, DOI 10.1073/pnas.1513198113
   Vasiljevic Igor, 2016, arXiv
   Zhou YR, 2017, INT CONF ACOUST SPEE, P1213, DOI 10.1109/ICASSP.2017.7952349
NR 36
TC 27
Z9 29
U1 0
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD AUG
PY 2019
VL 16
IS 2
AR 7
DI 10.1145/3306241
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA JC8UL
UT WOS:000489551500001
DA 2024-07-18
ER

PT J
AU Still, J
   Still, M
AF Still, Jeremiah
   Still, Mary
TI Influence of Visual Salience on Webpage Product Searches
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Web design; visual search; eye movements; salience; attention
ID TOP-DOWN; GUIDANCE; ATTENTION; MIND
AB Visual salience can increase search efficiency in complex displays but does that influence persist when completing a specific search? In two experiments, participants were asked to search webpages for the prices of specific products. Those products were located near an area of high visual salience or low visual salience. In Experiment 1, participants were read the name of the product before searching; in Experiment 2, participants were shown an image of the exact product before searching. In both cases, participants completed their search more quickly in the high-salience condition. This was true even when there was no ambiguity about the visual characteristics of the product. Our findings suggest that salience guides users through complex displays under realistic, goal-driven task conditions. Designers can use this knowledge to create interfaces that are easier to search by aligning salience and task-critical elements.
C1 [Still, Jeremiah; Still, Mary] Old Dominion Univ, Dept Psychol, 250 Mills Godwin Bldg, Norfolk, VA 23529 USA.
C3 Old Dominion University
RP Still, J (corresponding author), Old Dominion Univ, Dept Psychol, 250 Mills Godwin Bldg, Norfolk, VA 23529 USA.
EM jstill@odu.edu; mstill@odu.edu
OI Still, Mary/0000-0002-8511-8280; Still, Jeremiah/0000-0002-3060-4417
CR Anderson NC, 2016, PSYCHON B REV, V23, P1794, DOI 10.3758/s13423-016-1035-4
   [Anonymous], 1996, ACM Transactions, DOI DOI 10.1145/230562.230563
   [Anonymous], J HUM COMPUT INTER
   [Anonymous], 2012, COGNITIVELY INFORM I, DOI DOI 10.4018/978-1-4666-1628-8.CH006
   Awh E, 2012, TRENDS COGN SCI, V16, P437, DOI 10.1016/j.tics.2012.06.010
   BACON WF, 1994, PERCEPT PSYCHOPHYS, V55, P485, DOI 10.3758/BF03205306
   Dowd EW, 2013, J EXP PSYCHOL HUMAN, V39, P1786, DOI 10.1037/a0032548
   Faraday P., 2000, P 6 C HUM FACT WEB A, P1
   Foulsham T, 2014, CAN J EXP PSYCHOL, V68, P8, DOI 10.1037/cep0000004
   Grier R.A., 2007, Human computer interaction research in web design and evaluation, P22
   Harel J., 2006, NEURAL INFORM PROCES, P1
   Hicks John M., 2017, Proceedings of the Human Factors and Ergonomics Society Annual Meeting, V61, P1114, DOI 10.1177/1541931213601883
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Johansen S.A., 2006, CHI 06 EXTENDED ABST, P923, DOI DOI 10.1145/1125451.1125630
   Kim MS, 1999, J GEN PSYCHOL, V126, P326, DOI 10.1080/00221309909595370
   Malcolm GL, 2009, J VISION, V9, DOI 10.1167/9.11.8
   McMains S, 2011, J NEUROSCI, V31, P587, DOI 10.1523/JNEUROSCI.3766-10.2011
   Nielsen J., 2008, How little do users read?
   Parkhurst D, 2002, VISION RES, V42, P107, DOI 10.1016/S0042-6989(01)00250-4
   Rayner K, 1998, PSYCHOL BULL, V124, P372, DOI 10.1037/0033-2909.124.3.372
   Still J. D., 2017, P 8 INT C COGN NEUR, P255
   Still JD, 2018, COMPUT HUM BEHAV, V84, P352, DOI 10.1016/j.chb.2018.03.014
   Still JD, 2017, COGN TECHNOL WORK, V19, P363, DOI 10.1007/s10111-017-0411-9
   Theeuwes J, 2004, PSYCHON B REV, V11, P65, DOI 10.3758/BF03206462
   THEEUWES J, 1992, PERCEPT PSYCHOPHYS, V51, P599, DOI 10.3758/BF03211656
   Theeuwes J, 2010, ACTA PSYCHOL, V135, P77, DOI 10.1016/j.actpsy.2010.02.006
   TREISMAN AM, 1980, COGNITIVE PSYCHOL, V12, P97, DOI 10.1016/0010-0285(80)90005-5
   Wolfe JM, 2003, J EXP PSYCHOL HUMAN, V29, P483, DOI 10.1037/0096-1523.29.2.483
   Wolfe JM, 2004, VISION RES, V44, P1411, DOI 10.1016/j.visres.2003.11.024
   Wolfe JM, 2004, NAT REV NEUROSCI, V5, P495, DOI 10.1038/nrn1411
NR 30
TC 3
Z9 3
U1 4
U2 14
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD FEB
PY 2019
VL 16
IS 1
AR 3
DI 10.1145/3301413
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA HN7TC
UT WOS:000460393600003
DA 2024-07-18
ER

PT J
AU Berman, L
   Gallagher, K
   Kozaitis, S
AF Berman, Lewis
   Gallagher, Keith
   Kozaitis, Suzanne
TI Evaluating the Use of Sound in Static Program Comprehension
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Sonification; interactive sonification; program comprehension; sound
   design; auditory display; applied sound
ID DESIGN
AB Comprehension of computer programs is daunting, due in part to clutter in the software developer's visual environment and the need for frequent visual context changes. Previous research has shown that nonspeech sound can be useful in understanding the runtime behavior of a program. We explore the viability and advantages of using nonspeech sound in an ecological framework to help understand the static structure of software. We describe a novel concept for auditory display of program elements in which sounds indicate characteristics and relationships among a Java program's classes, interfaces, and methods. An empirical study employing this concept was used to evaluate 24 sighted software professionals and students performing maintenance-oriented tasks using a 2 x 2 crossover. Viability is strong for differentiation and characterization of software entities, less so for identification. The results suggest that sonification can be advantageous under certain conditions, though they do not indicate the overall advantage of using sound in terms of task duration at a 5% level of significance. The results uncover other findings such as differences in comprehension strategy based on the available tool environment. The participants reported enthusiasm for the idea of software sonification, mitigated by lack of familiarity with the concept and the brittleness of the tool. Limitations of the present research include restriction to particular types of comprehension tasks, a single sound mapping, a single programming language, and limited training time, but the use of sound in program comprehension shows sufficient promise for continued research.
C1 [Berman, Lewis] Digital Innovat Inc, 404 Selby Ct, Baltimore, MD 21212 USA.
   [Gallagher, Keith] Florida Inst Technol, Sch Comp, 150 West Univ Blvd, Melbourne, FL 32901 USA.
   [Kozaitis, Suzanne] Florida Inst Technol, Evans Lib, 150 West Univ Blvd, Melbourne, FL 32901 USA.
C3 Florida Institute of Technology; Florida Institute of Technology
RP Berman, L (corresponding author), Digital Innovat Inc, 404 Selby Ct, Baltimore, MD 21212 USA.
EM lewis.berman@verizon.net; kgallagher@fit.edu; skozaiti@fit.edu
CR Alhindawi Nouh., 2014, J SOFTWARE ENG APPL, V7, P413
   Alonso-Arevalo MA, 2012, ACM T APPL PERCEPT, V9, DOI 10.1145/2355598.2355600
   [Anonymous], J SOFTWARE MAINTENAN
   Berman L., 2011, THESIS
   Berman L., 2006, P 12 INT C AUD DISPL
   Blattner M. M., 1989, Human-Computer Interaction, V4, P11, DOI 10.1207/s15327051hci0401_1
   Boardman D., 1995, P INT COMP SOFTW APP
   Boccuzzo S., 2008, P 24 IEEE INT C SOFT
   BOOCH G, 1998, UNIFIED MODELING LAN
   Brown M, 2003, P 2003 INT C AUD DIS, P6
   Budgen D., 2013, P 2013 ACM IEEE INT
   Csapó A, 2013, ACM COMPUT SURV, V46, DOI 10.1145/2543581.2543586
   Dingler Tilman, 2008, P 14 INT C AUD DISPL, P1
   Dubus G, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0082491
   Finlayson J., 2005, P 11 INT C AUD DISPL
   Frauenberger C., 2009, THESIS
   Gaver William W, 1986, Human-computer interaction, V2, P167, DOI [10.1207/s15327051hci0202_3, DOI 10.1207/S15327051HCI0202_3]
   Gilfix M, 2000, USENIX ASSOCIATION PROCEEDINGS OF THE FOURTEENTH SYSTEMS ADMINISTRATION CONFERENCE (LISA XIV), P109
   Hermann T., 2011, The sonification handbook
   Hussein K, 2009, INT C PROGRAM COMPRE, P120, DOI 10.1109/ICPC.2009.5090035
   Interactive-sonification.org, 2016, P INT SON WORKSH
   Jameson D., 1996, P 1996 IEEE REAL TIM, P253
   Kildal J., 2005, P 11 INT C AUD DISPL
   Leplatre G., 2000, Proceedings of the 6th International Conference on Auditory Display (ICAD '00), P190
   Ludovico LA, 2016, INT J HUM-COMPUT ST, V85, P72, DOI 10.1016/j.ijhcs.2015.08.008
   McLachlan R., 2012, P 18 INT C AUD DISPL
   Murgia M, 2016, PSYCHOL RES-PSYCH FO, V80, P76, DOI 10.1007/s00426-015-0647-z
   Mustonen M., 2008, P 16 INT C AUD DISPL
   Neate T., 2013, P 4 INT SON WORKSH I, P29
   Nees MichaelA., 2007, P INT C AUDITORY DIS, P266
   North KJ, 2016, PROCEEDINGS OF THE 8TH INTERNATIONAL WORKSHOP ON SOCIAL SOFTWARE ENGINEERING (SSE'16), P1, DOI 10.1145/2993283.2993285
   Parnin C, 2006, INT C PROGRAM COMPRE, P13, DOI 10.1109/ICPC.2006.14
   Parseihian G, 2016, IEEE T MULTIMEDIA, V18, P674, DOI 10.1109/TMM.2016.2531978
   Pauletto S, 2016, INT J HUM-COMPUT ST, V85, P1, DOI 10.1016/j.ijhcs.2015.08.005
   Roddy S, 2014, ORGAN SOUND, V19, P70, DOI 10.1017/S1355771813000423
   Shneiderman B., 1999, READINGS INFORMATION, P236
   Sonnenwald D. H., 1990, Proceedings of the Twenty-Third Annual Hawaii International Conference on System Sciences, P541, DOI 10.1109/HICSS.1990.205229
   Stefik A, 2011, INT J HUM-COMPUT ST, V69, P820, DOI 10.1016/j.ijhcs.2011.07.002
   Stefik A, 2009, INT C PROGRAM COMPRE, P110, DOI 10.1109/ICPC.2009.5090034
   Susini P, 2014, NEW SOUNDTRACK, V4, P103, DOI 10.3366/sound.2014.0057
   Vargas M.L. M., 2003, Proceedings of the 2003 International Conference on Auditory Display, P38
   Vickers P, 2002, INTERACT COMPUT, V14, P793, DOI 10.1016/S0953-5438(02)00026-7
   Vickers P., 2005, ACM T APPL PERCEPT, V12, P490
   Vickers P., 2003, Communications of the ACM, V46, P86
   Vickers P, 2017, DISPLAYS, V47, P12, DOI 10.1016/j.displa.2016.05.002
   VONMAYRHAUSER A, 1995, IEEE COMPUT, V28, P44, DOI DOI 10.1109/2A02076
NR 46
TC 1
Z9 1
U1 0
U2 2
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD NOV
PY 2017
VL 15
IS 1
AR 7
DI 10.1145/3129456
PG 20
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FU0CU
UT WOS:000423519800007
DA 2024-07-18
ER

PT J
AU Zhang, JJ
   Yu, JH
   Zhang, K
   Zheng, XSJ
   Zhang, JS
AF Zhang, Jiajing
   Yu, Jinhui
   Zhang, Kang
   Zheng, Xianjun Sam
   Zhang, Junsong
TI Computational Aesthetic Evaluation of Logos
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Computational aesthetics; design principle; evaluation; human judgments;
   logo designs
ID INFORMATION
AB Computational aesthetics has become an active research field in recent years, but there have been few attempts in computational aesthetic evaluation of logos. In this article, we restrict our study on black-and-white logos, which are professionally designed for name-brand companies with similar properties, and apply perceptual models of standard design principles in computational aesthetic evaluation of logos. We define a group of metrics to evaluate some aspects in design principles such as balance, contrast, and harmony of logos. We also collect human ratings of balance, contrast, harmony, and aesthetics of 60 logos from 60 volunteers. Statistical linear regression models are trained on this database using a supervised machine-learning method. Experimental results show that our model-evaluated balance, contrast, and harmony have highly significant correlation of over 0.87 with human evaluations on the same dimensions. Finally, we regress human-evaluated aesthetics scores on model-evaluated balance, contrast, and harmony. The resulted regression model of aesthetics can predict human judgments on perceived aesthetics with a high correlation of 0.85. Our work provides a machine-learning-based reference framework for quantitative aesthetic evaluation of graphic design patterns and also the research of exploring the relationship between aesthetic perceptions of human and computational evaluation of design principles extracted from graphic designs.
C1 [Zhang, Jiajing; Yu, Jinhui] Zhejiang Univ, State Key Lab CAD&CG, 866 Yuhangtang Rd, Hangzhou 310058, Zhejiang, Peoples R China.
   [Zhang, Kang] Univ Texas Dallas, Dept Comp Sci, Richardson, TX 75080 USA.
   [Zheng, Xianjun Sam] Tsinghua Univ, Dept Psychol, 30 Shuangqing Rd, Beijing 100084, Peoples R China.
   [Zhang, Junsong] Xiamen Univ, Cognit Sci Dept, Mind Art&Computat Grp, 422 Siming South Rd, Xiamen 361005, Fujian, Peoples R China.
C3 Zhejiang University; University of Texas System; University of Texas
   Dallas; Tsinghua University; Xiamen University
RP Zhang, JJ (corresponding author), Zhejiang Univ, State Key Lab CAD&CG, 866 Yuhangtang Rd, Hangzhou 310058, Zhejiang, Peoples R China.
EM stou@zju.edu.cn; jhyu@cad.zju.edu.cn; kzhang@utdallas.edu;
   sam.zheng@gmail.com; zhangjs@xmu.edu.cn
RI Zhang, JunSong/HTQ-4981-2023; Zhang, Junsong/HKW-6976-2023
FU National Natural Science Foundation of China [61379069]; Key
   Technologies RD Program [2014BAK09B04]; Open Project Program of the
   State Key Lab of CADCG [A1706]; Zhejiang University; Aeronautical
   Science Foundation of China [20165168007]; Science and Technology on
   Electro-optic Control Laboratory
FX This work is supported by the National Natural Science Foundation of
   China under Grant No. 61379069 and the Key Technologies R&D Program
   (2014BAK09B04), the Open Project Program of the State Key Lab of CAD&CG
   (Grant No. A1706), Zhejiang University, and Aeronautical Science
   Foundation of China (No. 20165168007) & Science and Technology on
   Electro-optic Control Laboratory.
CR [Anonymous], 2007, Fundamentals of Data Mining in Genomics and Proteomics.
   [Anonymous], 2011, Quantitative Data Analysis with IBM SPSS 17, 18 and 19
   [Anonymous], ACM T APPL PERCEPTIO, V14
   ATTNEAVE F, 1954, PSYCHOL REV, V61, P183, DOI 10.1037/h0054663
   Bar M, 2006, PSYCHOL SCI, V17, P645, DOI 10.1111/j.1467-9280.2006.01759.x
   Dhar S, 2011, PROC CVPR IEEE, P1657, DOI 10.1109/CVPR.2011.5995467
   Feldman J, 2005, PSYCHOL REV, V112, P243, DOI 10.1037/0033-295X.112.1.243
   Fishwick P., 2008, Aesthetic computing
   Galanter P., 2012, P ACM SIGGRAPH COURS
   Garces Elena, 2014, ACM Transactions on Graphics, V33, DOI 10.1145/2601097.2601131
   He XC, 2004, INT C PATT RECOG, P791, DOI 10.1109/ICPR.2004.1334377
   Henderson PW, 1998, J MARKETING, V62, P14, DOI 10.2307/1252158
   Ke Y., 2006, P IEEE COMP SOC C CO, V1, P419, DOI DOI 10.1109/CVPR.2006.303
   Laursen L. F., 2016, PACIFIC GRAPHICS SHO
   Li CC, 2009, IEEE J-STSP, V3, P236, DOI 10.1109/JSTSP.2009.2015077
   Li Y.-N., 2014, LEONARDO, P173
   Neumann L., 2005, Computational aesthetics in graphics, visualization and imaging, P13, DOI DOI 10.2312/COMPAESTH/COMPAESTH05/013-018
   O'Donovan P, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601110
   Obrador P, 2012, LECT NOTES COMPUT SC, V7131, P63
   Page DL, 2003, IEEE IMAGE PROC, P229
   Rauschenberger R., 2009, P HUM FACT ERG SOC A, P1101
   Reber R, 2004, PERS SOC PSYCHOL REV, V8, P364, DOI 10.1207/s15327957pspr0804_3
   Reinecke Katharina, 2013, P SIGCHI C HUM FACT, P2049, DOI [10.1145/2470654.2481281, DOI 10.1145/2470654.2481281, 10]
   Saleh B., 2015, Proceedings of the 41st Graphics Interface Conference, P59
   Sartori A, 2015, ACM T INTERACT INTEL, V5, DOI 10.1145/2768209
   SHANNON CE, 1948, BELL SYST TECH J, V27, P379, DOI 10.1002/j.1538-7305.1948.tb01338.x
   Thumfart S, 2011, ACM T APPL PERCEPT, V8, DOI 10.1145/2043603.2043609
   Wallraven C., 2009, P INT S COMP AESTH G
   Wang QB, 2009, INT C COMP AID IND D, P277, DOI 10.1109/CAIDCD.2009.5375017
   Wen-Hung Liao, 2014, Smart Graphics. 12th International Symposium (SG 2014). Proceedings: LNCS 8698, P73, DOI 10.1007/978-3-319-11650-1_7
   White A.W., 2011, ELEMENTS GRAPHIC DES, V2nd
   Wong LK, 2009, IEEE IMAGE PROC, P997, DOI 10.1109/ICIP.2009.5413825
   Zheng XJS, 2009, CHI2009: PROCEEDINGS OF THE 27TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1-4, P1
NR 33
TC 11
Z9 11
U1 5
U2 43
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD JUL
PY 2017
VL 14
IS 3
AR 20
DI 10.1145/3058982
PG 21
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA FB9AL
UT WOS:000406431900007
DA 2024-07-18
ER

PT J
AU Carter, EJ
   Sharan, L
   Trutoiu, L
   Matthews, I
   Hodgins, JK
AF Carter, Elizabeth J.
   Sharan, Lavanya
   Trutoiu, Laura
   Matthews, Iain
   Hodgins, Jessica K.
TI Perceptually Motivated Guidelines for Voice Synchronization in Film
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Documentation; Languages; Multisensory perception and integration; human
   perception and performance; auditory perceptual research; visual
   psychophysics
ID SPEECH-PERCEPTION
AB We consume video content in a multitude of ways, including in movie theaters, on television, on DVDs and Blu-rays, online, on smart phones, and on portable media players. For quality control purposes, it is important to have a uniform viewing experience across these various platforms. In this work, we focus on voice synchronization, an aspect of video quality that is strongly affected by current post-production and transmission practices. We examined the synchronization of an actor's voice and lip movements in two distinct scenarios. First, we simulated the temporal mismatch between the audio and video tracks that can occur during dubbing or during broadcast. Next, we recreated the pitch changes that result from conversions between formats with different frame rates. We show, for the first time, that these audio visual mismatches affect viewer enjoyment. When temporal synchronization is noticeably absent, there is a decrease in the perceived performance quality and the perceived emotional intensity of a performance. For pitch changes, we find that higher pitch voices are not preferred, especially for male actors. Based on our findings, we advise that mismatched audio and video signals negatively affect viewer experience.
C1 [Carter, Elizabeth J.; Trutoiu, Laura; Hodgins, Jessica K.] Carnegie Mellon Univ, Inst Robot, Pittsburgh, PA 15213 USA.
   [Sharan, Lavanya; Matthews, Iain; Hodgins, Jessica K.] Disney Res Pittsburgh, Pittsburgh, PA 15213 USA.
C3 Carnegie Mellon University
RP Carter, EJ (corresponding author), Carnegie Mellon Univ, Inst Robot, 5000 Forbes Ave, Pittsburgh, PA 15213 USA.
EM lizcarter@cmu.edu
RI Carter, Elizabeth J/G-6958-2012
OI Carter, Elizabeth J./0000-0002-2735-148X
FU NSF [0811450]; Microsoft Research; Division of Computing and
   Communication Foundations; Direct For Computer & Info Scie & Enginr
   [0811450] Funding Source: National Science Foundation
FX This study was funded in part by NSF Grant 0811450 and a Microsoft
   Research grant awarded to J. K. Hodgins.
CR ABELIN A, 2007, P 7 INT C EP ROB MOD
   [Anonymous], 1972, Emotion in the Human Face: guidelines for Research and an Integration of Findings
   Brainard DH, 1997, SPATIAL VISION, V10, P433, DOI 10.1163/156856897X00357
   Chion Michel, 2019, Audio-Vision: Sound on Screen
   DIXON NF, 1980, PERCEPTION, V9, P719, DOI 10.1068/p090719
   GAROFOLO J, 1993, 4930 NIST SISTIR
   GRANT K, 2003, P AUD VIS SPEECH PRO
   GRANT KW, 2001, P AUD VIS SPEECH PRO
   HUANG E, 2007, SEARCHING IDEAL LIVE
   LASS NJ, 1976, J ACOUST SOC AM, V59, P1232, DOI 10.1121/1.380958
   MASON A, 2008, P AUD ENG SOC CONV A
   MCGURK H, 1976, NATURE, V264, P746, DOI 10.1038/264746a0
   Munhall KG, 1996, PERCEPT PSYCHOPHYS, V58, P351, DOI 10.3758/BF03206811
   Pelli DG, 1997, SPATIAL VISION, V10, P437, DOI 10.1163/156856897X00366
   Phillips Gretchen, 2010, COMMUNICATION
   SAKAMOTO S, 2007, P AUD VIS SPEECH PRO
   SUMBY WH, 1954, J ACOUST SOC AM, V26, P212, DOI 10.1121/1.1907309
   SUMMERFIELD Q, 1992, PHILOS T ROY SOC B, V335, P71, DOI 10.1098/rstb.1992.0009
   THEOBALD BJ, 2003, P 7 INT C AUD VIS SP
   TINWELL A, 2009, THINKING AFTER DARK
   TUCKMAN J, 2006, DUB YOU HAVE BE GOOD
   van Wassenhove V, 2007, NEUROPSYCHOLOGIA, V45, P598, DOI 10.1016/j.neuropsychologia.2006.01.001
NR 22
TC 3
Z9 3
U1 0
U2 9
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD JUL
PY 2010
VL 7
IS 4
AR 23
DI 10.1145/1823738.1823741
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 633ZI
UT WOS:000280546500003
DA 2024-07-18
ER

PT J
AU Kim, Y
   Varshney, A
   Jacobs, DW
   Guimbretière, F
AF Kim, Youngmin
   Varshney, Amitabh
   Jacobs, David W.
   Guimbretiere, Francois
TI Mesh Saliency and Human Eye Fixations
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Algorithms; Human Factors; Verification; Visual perception; mesh
   saliency; eye-tracker
ID VISUAL-ATTENTION; REGIONS
AB Mesh saliency has been proposed as a computational model of perceptual importance for meshes, and it has been used in graphics for abstraction, simplification, segmentation, illumination, rendering, and illustration. Even though this technique is inspired by models of low-level human vision, it has not yet been validated with respect to human performance. Here, we present a user study that compares the previous mesh saliency approaches with human eye movements. To quantify the correlation between mesh saliency and fixation locations for 3D rendered images, we introduce the normalized chance-adjusted saliency by improving the previous chance-adjusted saliency measure. Our results show that the current computational model of mesh saliency can model human eye movements significantly better than a purely random model or a curvature-based model.
C1 [Kim, Youngmin; Varshney, Amitabh; Jacobs, David W.; Guimbretiere, Francois] Univ Maryland, Dept Comp Sci, College Pk, MD 20742 USA.
C3 University System of Maryland; University of Maryland College Park
RP Kim, Y (corresponding author), Univ Maryland, Dept Comp Sci, College Pk, MD 20742 USA.
EM ymkim@cs.umd.edu; varshney@cs.umd.edu; djacobs@cs.umd.edu;
   francois@cs.umd.edu
OI Varshney, Amitabh/0000-0002-9873-2212
FU NSF [IIS 04-14699, CCF 04-29753, CNS 04-03313, CCF 05-41120, CMMI
   08-35572]; ARO [W911NF-08-1-0466]
FX This work has been supported in part by the NSF grants: IIS 04-14699,
   CCF 04-29753, CNS 04-03313, CCF 05-41120, CMMI 08-35572 and ARO Grant #
   W911NF-08-1-0466. Any opinions, findings, conclusions, or
   recommendations expressed in this article are those of the authors and
   do not necessarily reflect the views of the research sponsors.
CR [Anonymous], 2005, ACM Trans. Appl. Percep.
   Cole F., 2006, EUROGRAPHICS S RENDE, P377
   ENNS JT, 1990, PSYCHOL SCI, V1, P323, DOI 10.1111/j.1467-9280.1990.tb00227.x
   Feixas M., 2008, ACM T APPL PERCEPT
   Gal R, 2006, ACM T GRAPHIC, V25, P130, DOI 10.1145/1122501.1122507
   Goldsmith Steven R, 2004, Curr Heart Fail Rep, V1, P45, DOI 10.1007/s11897-004-0024-5
   Henderson J.M., 1998, EYE GUIDANCE READING
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   KIM Y, 2008, CARTR1026 UMIACS U M
   Kim Y, 2006, IEEE T VIS COMPUT GR, V12, P925, DOI 10.1109/TVCG.2006.174
   KOCH C, 1985, HUM NEUROBIOL, V4, P219
   Lee CH, 2005, ACM T GRAPHIC, V24, P659, DOI 10.1145/1073204.1073244
   LU A, 2006, P EUR IEEE VGTC S VI, P147
   Mann KG, 1998, BLOOD COAGUL FIBRIN, V9, pS3
   Meyer M., 2002, VISUALIZATION MATH, V6, P35, DOI DOI 10.1007/978-3-662-05105-4_2
   Oliva A, 2003, IEEE IMAGE PROC, P253, DOI 10.1109/icip.2003.1246946
   Palmer S., 1999, VISION SCI PHOTONS P
   Parkhurst D, 2002, VISION RES, V42, P107, DOI 10.1016/S0042-6989(01)00250-4
   Privitera CM, 2000, IEEE T PATTERN ANAL, V22, P970, DOI 10.1109/34.877520
   Santella A., 2004, P NPAR, P71, DOI [DOI 10.1145/987657.987669, 10.1145/987657.987669]
   Shilane P, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1243980.1243981
   STAMPE DM, 1993, BEHAV RES METH INSTR, V25, P137, DOI 10.3758/BF03204486
   TAUBIN G, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P902, DOI 10.1109/ICCV.1995.466840
   Watanabe K, 2001, COMPUT GRAPH FORUM, V20, pC385, DOI 10.1111/1467-8659.00531
   Yang Y.-L., 2006, P 4 EUR S GEOM PROC, P223
NR 25
TC 59
Z9 71
U1 0
U2 11
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD FEB
PY 2010
VL 7
IS 2
AR 12
DI 10.1145/1670671.1670676
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 563HF
UT WOS:000275118100005
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Berger, DR
   Schulte-Pelkum, J
   Bülthoff, HH
AF Berger, Daniel R.
   Schulte-Pelkum, Joerg
   Buelthoff, Heinrich H.
TI Simulating Believable Forward Accelerations on a Stewart Motion Platform
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Experimentation; Human Factors; Measurement; Linear vection;
   multisensory integration; self-motion perception; simulator design;
   vestibular; virtual reality
ID WHOLE-BODY; INTERNAL-MODELS; VISUAL CUES; OPTIC FLOW; PERCEPTION;
   THRESHOLDS; ORIENTATION; PERSPECTIVE; DIRECTION; MOVEMENT
AB It is still an unsolved problem how to optimally simulate self-motion using motion simulators. We investigated how a forward acceleration can be simulated as believably as possible on a hexapod motion platform equipped with a projection screen.
   Human participants rated the believability of brief forward accelerations. These were simulated as visual forward accelerations over a ground plane with people as size cues, presented together with brief forward surge translations and backward pitches of the platform, and synchronous random up-down movements of the camera in the visual scene and the platform. The magnitudes of all of the parameters were varied independently across trials.
   Even though variability between participants was high, the most believable simulation occurred when visual accelerations were combined with backward pitches of the platform, which changed the gravitoinertial vector direction approximately consistent with the visual acceleration. However, a wide range of platform pitches was accepted as believable. With high visual acceleration cues most participants reported trials as realistic even when the platform tilt rate was above vestibular canal thresholds reported in other works. Other manipulated parameters had only a mild influence on the responses. These results can be used to optimize motion-cueing algorithms for simulating linear accelerations in motion simulators.
C1 [Berger, Daniel R.; Schulte-Pelkum, Joerg; Buelthoff, Heinrich H.] Max Planck Inst Biol Cybernet, D-72076 Tubingen, Germany.
C3 Max Planck Society
RP Berger, DR (corresponding author), Max Planck Inst Biol Cybernet, Spemannstr 38, D-72076 Tubingen, Germany.
EM dberger@mit.edu; joerg.sp@tuebingen.mpg.de;
   hein-rich.buelthoff@tuebingen.mpg.de
RI Bülthoff, Heinrich/AAC-8818-2019; Bülthoff, Heinrich H/J-6579-2012
OI Bülthoff, Heinrich H/0000-0003-2568-0607
FU EU [POEMS-IST-2001-39223]; Max Planck Society
FX This research was funded by the EU grant POEMS-IST-2001-39223 and the
   Max Planck Society.
CR Angelaki DE, 2004, NATURE, V430, P560, DOI 10.1038/nature02754
   [Anonymous], 1999, PROC AIAA MODEL SIMU
   BENSON AJ, 1989, AVIAT SPACE ENVIR MD, V60, P205
   BENSON AJ, 1986, AVIAT SPACE ENVIR MD, V57, P1088
   BERGER DR, 2003, P SIGGRAPH SKETCH AP
   BERTHOZ A, 1975, EXP BRAIN RES, V23, P471
   BERTHOZ A, 1995, SCIENCE, V269, P95, DOI 10.1126/science.7604286
   Berthoz A., 1982, TUTORIALS MOTION PER, P157, DOI 10.1007/978-1-4613-3569-6
   Bringoux L, 2003, Q J EXP PSYCHOL-A, V56, P909, DOI 10.1080/02724980245000016
   Bringoux L, 2002, NEUROPSYCHOLOGIA, V40, P367, DOI 10.1016/S0028-3932(01)00103-8
   Carriot J, 2004, AVIAT SPACE ENVIR MD, V75, P795
   Dichgans Johannes, 1978, Perception, P755, DOI [DOI 10.1007/978-3-642-46354-9253F, DOI 10.1007/978-3-642-46354-9_25, DOI 10.1007/978-3-642-46354-925]
   Ernst MO, 2004, TRENDS COGN SCI, V8, P162, DOI 10.1016/j.tics.2004.02.002
   Gdowski GT, 2000, EXP BRAIN RES, V135, P511, DOI 10.1007/s002210000542
   Gianna C, 1996, BRAIN RES BULL, V40, P443, DOI 10.1016/0361-9230(96)00140-2
   Groen EL, 2004, J VESTIBUL RES-EQUIL, V14, P375
   Groen EL, 2001, J AIRCRAFT, V38, P600, DOI 10.2514/2.2827
   GROEN EL, 2000, P AIAA MOD SIM TECHN, P1
   GUNDRY AJ, 1978, AVIAT SPACE ENVIR MD, V49, P679
   HENN V, 1974, BRAIN RES, V71, P144, DOI 10.1016/0006-8993(74)90198-X
   Howard I.P., 1982, HUMAN VISUAL ORIENTA
   Howard IP, 2001, PERCEPTION, V30, P583, DOI 10.1068/p3106
   Israël I, 2004, INT J PSYCHOPHYSIOL, V53, P21, DOI 10.1016/j.ijpsycho.2004.01.002
   Jaekl PM, 2005, EXP BRAIN RES, V163, P388, DOI 10.1007/s00221-004-2191-8
   Jessell TM., 2000, PRINCIPLES NEURAL SC
   Kemeny A, 2003, TRENDS COGN SCI, V7, P31, DOI 10.1016/S1364-6613(02)00011-6
   Kingma H, 2005, BMC EAR NOSE THROAT, V5, DOI 10.1186/1472-6815-5-5
   Knapp JM, 2004, PRESENCE-TELEOP VIRT, V13, P572, DOI 10.1162/1054746042545238
   Lappe M, 2007, EXP BRAIN RES, V180, P35, DOI 10.1007/s00221-006-0835-6
   LEE DN, 1980, PHILOS T R SOC B, V290, P169, DOI 10.1098/rstb.1980.0089
   LEIBOWITZ HW, 1988, HUMAN FACTORS AVIATI, P83
   Lepecq JC, 1999, PERCEPTION, V28, P63, DOI 10.1068/p2749
   LISHMAN JR, 1973, PERCEPTION, V2, P287, DOI 10.1068/p020287
   Loomis JM, 2003, VIRTUAL AND ADAPTIVE ENVIRONMENTS: APPLICATIONS, IMPLICATIONS, AND HUMAN PERFORMANCE ISSUES, P21
   MacNeilage PR, 2007, EXP BRAIN RES, V179, P263, DOI 10.1007/s00221-006-0792-0
   Merfeld DM, 1999, NATURE, V398, P615, DOI 10.1038/19303
   MITTELSTAEDT H, 1983, NATURWISSENSCHAFTEN, V70, P272, DOI 10.1007/BF00404833
   Palmisano S, 2000, PERCEPTION, V29, P57, DOI 10.1068/p2990
   REID LD, 1988, J AIRCRAFT, V25, P639, DOI 10.2514/3.45635
   Reymond G, 2002, BIOL CYBERN, V87, P301, DOI 10.1007/s00422-002-0357-7
   Reymond G, 2000, VEHICLE SYST DYN, V34, P249, DOI 10.1076/vesd.34.4.249.2059
   Sun HJ, 2004, PERCEPTION, V33, P49, DOI 10.1068/p5145
   Sun HJ, 2004, EXP BRAIN RES, V154, P246, DOI 10.1007/s00221-003-1652-9
   TELBAN RJ, 2005, 213747 NASACR, P1
   Thompson E., 2004, J MENS STUDIES, V13, P5, DOI [DOI 10.3149/JMS.1301.5, https://doi.org/10.3149/jms.1301.5, 10.3149/jms.1301.5]
   van der Steen FAM, 2000, PERCEPT PSYCHOPHYS, V62, P89, DOI 10.3758/BF03212063
   Willett WC, 2008, ASIA PAC J CLIN NUTR, V17, P1
   WONG SCP, 1981, PERCEPT PSYCHOPHYS, V30, P228, DOI 10.3758/BF03214278
   Wright WG, 2005, J VESTIBUL RES-EQUIL, V15, P185
   Yates BJ, 2000, EXP BRAIN RES, V130, P151, DOI 10.1007/s002219900238
   Yong NA, 2007, J NEUROPHYSIOL, V97, P1100, DOI 10.1152/jn.00694.2006
   YOUNG LR, 1973, ACTA OTO-LARYNGOL, V76, P24, DOI 10.3109/00016487309121479
   ZACHARIAS GL, 1981, EXP BRAIN RES, V41, P159
   Zupan LH, 2002, BIOL CYBERN, V86, P209, DOI 10.1007/s00422-001-0290-1
NR 54
TC 35
Z9 37
U1 0
U2 16
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD JAN
PY 2010
VL 7
IS 1
AR 5
DI 10.1145/1658349.1658354
PG 27
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA 549EK
UT WOS:000274028400005
DA 2024-07-18
ER

PT J
AU Fink, PW
   Foo, PS
   Warren, WH
AF Fink, Philip W.
   Foo, Patrick S.
   Warren, William H.
TI Obstacle Avoidance During Walking in Real and Virtual Environments
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Verification; Experimentation; Theory; Locomotion; virtual reality;
   modeling
ID VISUAL GUIDANCE; EGOCENTRIC DISTANCE; PERCEPTION; ILLUSION; DIRECTION;
   ABSOLUTE; DYNAMICS; MOTION
AB Immersive virtual environments are a promising research tool for the study of perception and action, on the assumption that visual-motor behavior in virtual and real environments is essentially similar. We investigated this issue for locomotor behavior and tested the generality of Fajen and Warren's [2003] steering dynamics model. Participants walked to a stationary goal while avoiding a stationary obstacle in matched physical and virtual environments. There were small, but reliable, differences in locomotor paths, with a larger maximum deviation (Delta = 0.16 m), larger obstacle clearance (Delta = 0.16 m), and slower walking speed (Delta = 0.13 m/s) in the virtual environment. Separate model fits closely captured the mean virtual and physical paths (R-2 > 0.98). Simulations implied that the path differences are not because of walking speed or a 50% distance compression in virtual environments, but might be a result of greater uncertainty about the egocentric location of virtual obstacles. On the other hand, paths had similar shapes in the two environments with no difference in median curvature and could be modeled with a single set of parameter values (R-2 > 0.95). Fajen and Warren's original parameters successfully generalized to new virtual and physical object configurations (R-2 > 0.95). These results justify the use of virtual environments to study locomotor behavior.
C1 [Fink, Philip W.] Florida Atlantic Univ, Ctr Complex Syst & Brain Sci, Boca Raton, FL 33431 USA.
   [Foo, Patrick S.] Univ N Carolina, Dept Psychol, Asheville, NC 28801 USA.
   [Warren, William H.] Brown Univ, Dept Cognit & Linguist Sci, Providence, RI 02912 USA.
C3 State University System of Florida; Florida Atlantic University;
   University of North Carolina; University of North Carolina - Asheville;
   Brown University
RP Fink, PW (corresponding author), Florida Atlantic Univ, Ctr Complex Syst & Brain Sci, Boca Raton, FL 33431 USA.
RI Fink, Philip/AAE-5330-2020; Warren, William H./AAX-7781-2021
OI Fink, Philip/0000-0001-7911-0015
CR Abramson MA, 2004, MATH PROGRAM, V100, P3, DOI 10.1007/s10107-003-0484-5
   AGLIOTI S, 1995, CURR BIOL, V5, P679, DOI 10.1016/S0960-9822(95)00133-3
   [Anonymous], 2005, ACM Transactions on Applied Perception, DOI [DOI 10.1145/1077399.1077403, DOI 10.1145/1077399.10774032,3,9]
   BEALL AC, 1995, P SOC PHOTO-OPT INS, V2411, P288, DOI 10.1117/12.207547
   Cohen J.A., 2005, Journal of Vision, V5, p312a
   Creem-Regehr SH, 2005, PERCEPTION, V34, P191, DOI 10.1068/p5144
   Dixon MW, 2000, J EXP PSYCHOL HUMAN, V26, P582, DOI 10.1037/0096-1523.26.2.582
   Fajen BR, 2004, PERCEPTION, V33, P689, DOI 10.1068/p5236
   Fajen BR, 2003, J EXP PSYCHOL HUMAN, V29, P343, DOI 10.1037/0096-1523.29.2.343
   FAJEN BR, EXPT BRAIN IN PRESS
   Franz VH, 2000, PSYCHOL SCI, V11, P20, DOI 10.1111/1467-9280.00209
   Franz VH, 2003, SPATIAL VISION, V16, P211, DOI 10.1163/156856803322467491
   Gérin-Lajoie M, 2005, MOTOR CONTROL, V9, P242, DOI 10.1123/mcj.9.3.242
   Glennerster A, 2006, CURR BIOL, V16, P428, DOI 10.1016/j.cub.2006.01.019
   Glover SR, 2001, J EXP PSYCHOL HUMAN, V27, P560, DOI 10.1037/0096-1523.27.3.560
   LLEWELLYN KR, 1971, J EXP PSYCHOL, V91, P245, DOI 10.1037/h0031788
   Loomis JM, 2003, VIRTUAL AND ADAPTIVE ENVIRONMENTS: APPLICATIONS, IMPLICATIONS, AND HUMAN PERFORMANCE ISSUES, P21
   Milner AD., 1995, The visual brain in action
   MOHLER BJ, 2006, INFLUENCE FEED UNPUB
   NAKAYAMA K, 1974, PERCEPTION, V3, P63, DOI 10.1068/p030063
   Ooi TL, 2001, NATURE, V414, P197, DOI 10.1038/35102562
   Opitz D, 1996, PERCEPTION, V25, P92
   Philbeck JW, 1997, J EXP PSYCHOL HUMAN, V23, P72, DOI 10.1037/0096-1523.23.1.72
   Rushton SK, 2002, LECT NOTES COMPUT SC, V2525, P576
   Sahm C. S., 2005, ACM Trans. Appl. Percept. (TAP), V2, P35, DOI DOI 10.1145/1048687.1048690
   Schoner G, 1995, ROBOT AUTON SYST, V16, P213, DOI 10.1016/0921-8890(95)00049-6
   SEDGWICK H, 1980, PERCEPTION PICTURES, P1
   Smeets JBJ, 2003, SPATIAL VISION, V16, P311, DOI 10.1163/156856803322467554
   Smeets JBJ, 1999, MOTOR CONTROL, V3, P237, DOI 10.1123/mcj.3.3.237
   Tarr MJ, 2002, NAT NEUROSCI, V5, P1089, DOI 10.1038/nn948
   Thompson WB, 2004, PRESENCE-TELEOP VIRT, V13, P560, DOI 10.1162/1054746042545292
   Vishton PM, 1999, J EXP PSYCHOL HUMAN, V25, P1659, DOI 10.1037/0096-1523.25.6.1659
   WARREN WH, 1987, J EXP PSYCHOL HUMAN, V13, P371, DOI 10.1037/0096-1523.13.3.371
   Wilkie R, 2003, J EXP PSYCHOL HUMAN, V29, P363, DOI 10.1037/0096-1523.29.2.363
   Willemsen P., 2004, P 1 S APPL PERC GRAP, P35, DOI DOI 10.1145/1012551.1012558
NR 35
TC 85
Z9 96
U1 0
U2 12
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD JAN
PY 2007
VL 4
IS 1
AR 2
DI 10.1145/1227134.1227136
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA V04IL
UT WOS:000207052000002
DA 2024-07-18
ER

PT J
AU Choudhary, Z
   Erickson, A
   Norouzi, N
   Kim, K
   Bruder, G
   Welch, G
AF Choudhary, Zubin
   Erickson, Austin
   Norouzi, Nahal
   Kim, Kangsoo
   Bruder, Gerd
   Welch, Gregory
TI Virtual Big Heads in Extended Reality: Estimation of Ideal Head Scales
   and Perceptual Thresholds for Comfort and Facial Cues
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Virtual environments; social virtual reality; outdoor augmented reality;
   non verbal communication
ID EXPRESSION; DISTANCE; GENDER; GAZE; EYE
AB Extended reality (XR) technologies, such as virtual reality (VR) and augmented reality (AR), provide users, their avatars, and embodied agents a shared platform to collaborate in a spatial context. Although traditional face-to-face communication is limited by users' proximity, meaning that another human's non-verbal embodied cues become more difficult to perceive the farther one is away from that person, researchers and practitioners have started to look into ways to accentuate or amplify such embodied cues and signals to counteract the effects of distance with XR technologies. In this article, we describe and evaluate the Big Head technique, in which a human's head in VR/AR is scaled up relative to their distance from the observer as a mechanism for enhancing the visibility of non-verbal facial cues, such as facial expressions or eye gaze. To better understand and explore this technique, we present two complimentary human-subject experiments in this article. In our first experiment, we conducted a VR study with a head-mounted display to understand the impact of increased or decreased head scales on participants' ability to perceive facial expressions as well as their sense of comfort and feeling of "uncannniness" over distances of up to 10 m. We explored two different scaling methods and compared perceptual thresholds and user preferences. Our second experiment was performed in an outdoor AR environment with an optical see-through head-mounted display. Participants were asked to estimate facial expressions and eye gaze, and identify a virtual human over large distances of 30, 60, and 90 m. In both experiments, our results show significant differences in minimum, maximum, and ideal head scales for different distances and tasks related to perceiving faces, facial expressions, and eye gaze, and we also found that participants were more comfortable with slightly bigger heads at larger distances. We discuss our findings with respect to the technologies used, and we discuss implications and guidelines for practical applications that aim to leverage XR-enhanced facial cues.
C1 [Choudhary, Zubin; Erickson, Austin; Norouzi, Nahal; Bruder, Gerd; Welch, Gregory] Univ Cent Florida, 4000 Cent Florida Blvd, Orlando, FL 32816 USA.
   [Kim, Kangsoo] Univ Calgary, 2500 Univ Dr NW, Calgary, AB T2N 1N4, Canada.
C3 State University System of Florida; University of Central Florida;
   University of Calgary
RP Choudhary, Z (corresponding author), Univ Cent Florida, 4000 Cent Florida Blvd, Orlando, FL 32816 USA.
EM zubinchoudhary@knights.ucf.edu; ericksona@knights.ucf.edu;
   nahal.norouzi@ucf.edu; kangsoo.kim@ucalgary.ca; gerd.bruder@ucf.edu;
   welch@ucf.edu
RI Erickson, Austin/AAV-9677-2020; Kim, Kangsoo/AAL-9592-2020
OI Kim, Kangsoo/0000-0002-0925-378X; Welch, Gregory/0000-0002-8243-646X;
   Erickson, Austin/0000-0002-3146-8023; Choudhary,
   Zubin/0000-0003-4303-5759
FU National Science Foundation [1564065, 1800961, 1800947, 1800922]; Office
   of Naval Research [N00014-21-1-2578, N00014-21-1-2882, 34]; Advent
   Health Endowed Chair in Healthcare Simulation
FX This material includes work supported in part by the National Science
   Foundation under Award Number 1564065 (Dr. Ephraim P. Glinert, IIS) and
   Collaborative Award Numbers 1800961, 1800947, and 1800922 (Dr. Ephraim
   P. Glinert, IIS) to the University of Central Florida, University of
   Florida, and Stanford University, respectively; the Office of Naval
   Research under Award Numbers N00014-21-1-2578 and N00014-21-1-2882 (Dr.
   Peter Squire, Code 34); and the AdventHealth Endowed Chair in Healthcare
   Simulation (Prof. Welch).
CR Adcock Matt., 2010, ACM SIGGRAPH ASIA 2010 Posters. SA'10, p62:1, DOI DOI 10.1145/1900354.1900423
   Andrist S., 2012, Proceedings of the International Conference on Human Factors in Computing, CHI '12, P705
   Andrist Sean., 2012, Proceedings of the 4th Workshop on Eye Gaze in Intelligent Human Machine Interaction, page, P4, DOI [DOI 10.1145/2401836.2401840, 10.1145/2401836.2401840]
   Bailenson JN, 2003, PERS SOC PSYCHOL B, V29, P819, DOI 10.1177/0146167203029007002
   Bekerman I, 2014, J OPHTHALMOL, V2014, DOI 10.1155/2014/503645
   Bélisle JF, 2010, PSYCHOL MARKET, V27, P741, DOI 10.1002/mar.20354
   Billinghurst M, 2001, IEEE COMPUT GRAPH, V21, P6, DOI 10.1109/38.920621
   Breuss-Schneeweis P, 2016, UBICOMP'16 ADJUNCT: PROCEEDINGS OF THE 2016 ACM INTERNATIONAL JOINT CONFERENCE ON PERVASIVE AND UBIQUITOUS COMPUTING, P1484, DOI 10.1145/2968219.2974044
   Bruder Gerd., 2012, P ACM S APPL PERCEPT, P111
   Cafaro Angelo, 2012, Intelligent Virtual Agents. Proceedings 12th International Conference, IVA 2012, P67, DOI 10.1007/978-3-642-33197-8_7
   Choudhary Z, 2021, 2021 IEEE VIRTUAL REALITY AND 3D USER INTERFACES (VR), P788, DOI 10.1109/VR50410.2021.00106
   Choudhary Z, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR 2020), P425, DOI [10.1109/VR46266.2020.00-41, 10.1109/VR46266.2020.1581089101511]
   Choudhary Zubin, 2021, P WORKSHOP USER EMBO
   Choudhary Zubin, 2021, P S SPATIAL USER INT, P1
   Colburn Alex., 2000, ROLE EYE GAZE AVATAR
   Davis MC, 2016, WORLD NEUROSURG, V86, P103, DOI 10.1016/j.wneu.2015.08.053
   de Melo Celso M., 2012, Intelligent Virtual Agents. Proceedings 12th International Conference, IVA 2012, P53, DOI 10.1007/978-3-642-33197-8_6
   DiSalvo C.F., 2002, P 4 C DES INT SYST P, P321, DOI [10.1145/778712.778756, DOI 10.1145/778712.778756]
   Du SC, 2013, J VISION, V13, DOI 10.1167/13.4.13
   Ducheneaut N, 2009, CHI2009: PROCEEDINGS OF THE 27TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1-4, P1151
   Dunn RA, 2012, COMPUT HUM BEHAV, V28, P97, DOI 10.1016/j.chb.2011.08.015
   EKMAN P, 1993, AM PSYCHOL, V48, P384, DOI 10.1037/0003-066X.48.4.384
   Ekman P., 1989, Handbook of Social Psychophysiology, P143
   Ekman Paul, 1997, What the Face Reveals: Basic and Applied Studies of Spontaneous Expression using the Facial Action Coding System (FACS)
   Erickson A, 2021, ACM T APPL PERCEPT, V18, DOI 10.1145/3456874
   Erickson A, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR 2020), P434, DOI [10.1109/VR46266.2020.1580695145399, 10.1109/VR46266.2020.00-40]
   Erickson Austin, 2020, P INT C ART REAL TEL, P1
   Ericson A., 2020, 3 INT S SMALL SCALE, P1, DOI [DOI 10.1109/SIMS49386.2020, 10.1145/3385959.3418445, DOI 10.1145/3385959.3418445]
   Fernández-Caballero A, 2017, FRONT NEUROINFORM, V11, DOI 10.3389/fninf.2017.00064
   Gabbard JL, 2010, P IEEE VIRT REAL ANN, P79, DOI 10.1109/VR.2010.5444808
   Garau Maia, 2003, P SIGCHI C HUM FACT, P529, DOI DOI 10.1145/642611.642703
   Giant Bomb, 2021, BIG HEAD MOD
   Guven S., 2009, IEEE INT C PERV COMP, P1, DOI DOI 10.1109/PERCOM.2009.4912803
   HAGER JC, 1979, ETHOL SOCIOBIOL, V1, P77, DOI 10.1016/0162-3095(79)90007-4
   Hall E., 1969, The hidden dimension: man's use of space in public and private
   HALL ET, 1963, AM ANTHROPOL, V65, P1003, DOI 10.1525/aa.1963.65.5.02a00020
   Iachini T, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0111511
   Kennedy DP, 2009, NAT NEUROSCI, V12, P1226, DOI 10.1038/nn.2381
   Kim K, 2019, 2019 IEEE INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND VIRTUAL REALITY (AIVR), P17, DOI 10.1109/AIVR46125.2019.00013
   Kim K, 2018, IEEE T VIS COMPUT GR, V24, P2947, DOI 10.1109/TVCG.2018.2868591
   Kiyokawa K., 1999, IEEE SMC'99 Conference Proceedings. 1999 IEEE International Conference on Systems, Man, and Cybernetics (Cat. No.99CH37028), P48, DOI 10.1109/ICSMC.1999.816444
   Kramida G, 2016, IEEE T VIS COMPUT GR, V22, P1912, DOI 10.1109/TVCG.2015.2473855
   Krauss V, 2021, CHI '21: PROCEEDINGS OF THE 2021 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3411764.3445335
   Kress BC, 2019, DIGITAL OPTICAL TECH, V1062
   Kress B, 2014, PROC SPIE, V9202, DOI 10.1117/12.2064351
   Kruijff Ernst, 2010, 2010 9th IEEE International Symposium on Mixed and Augmented Reality (ISMAR). Science & Technology Papers, P3, DOI 10.1109/ISMAR.2010.5643530
   Latoschik ME, 2017, VRST'17: PROCEEDINGS OF THE 23RD ACM SYMPOSIUM ON VIRTUAL REALITY SOFTWARE AND TECHNOLOGY, DOI 10.1145/3139131.3139156
   Lee JER, 2014, CYBERPSYCH BEH SOC N, V17, P248, DOI 10.1089/cyber.2013.0358
   Lee Michael D., 2006, P 28 ANN C COGNITIVE, P1675
   Lee M, 2021, IEEE T VIS COMPUT GR, V27, P3534, DOI 10.1109/TVCG.2019.2959575
   Lee M, 2017, P IEEE VIRT REAL ANN, P105, DOI 10.1109/VR.2017.7892237
   McDonnell R, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185587
   McGill M, 2017, AUTOMOTIVEUI'17: PROCEEDINGS OF THE 9TH INTERNATIONAL CONFERENCE ON AUTOMOTIVE USER INTERFACES AND INTERACTIVE VEHICULAR APPLICATIONS, P251, DOI 10.1145/3131726.3131876
   Mori M, 2012, IEEE ROBOT AUTOM MAG, V19, P98, DOI 10.1109/MRA.2012.2192811
   Norouzi* N., 2019, S SPATIAL USER INTER, P1, DOI DOI 10.1007/978-3-030-04110-61
   Norouzi N, 2019, ACM CONFERENCE ON SPATIAL USER INTERACTION (SUI 2019), DOI 10.1145/3357251.3357587
   Oh SY, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0161794
   Palmarini R, 2018, ROBOT CIM-INT MANUF, V49, P215, DOI 10.1016/j.rcim.2017.06.002
   Peck TC, 2022, IEEE T VIS COMPUT GR, V28, P2179, DOI 10.1109/TVCG.2022.3150521
   Pidel C, 2020, LECT NOTES COMPUT SC, V12242, P141, DOI 10.1007/978-3-030-58465-8_10
   Piumsomboon Thammathip, 2017, 2017 International Symposium on Ubiquitous Virtual Reality (ISUVR). Proceedings, P38, DOI 10.1109/ISUVR.2017.20
   Piumsomboon T, 2018, PROCEEDINGS OF THE 2018 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2018), DOI 10.1145/3173574.3173620
   Piumsomboon T, 2017, SA'17: SIGGRAPH ASIA 2017 MOBILE GRAPHICS & INTERACTIVE APPLICATIONS, DOI 10.1145/3132787.3139200
   Renner RS, 2013, ACM COMPUT SURV, V46, DOI 10.1145/2543581.2543590
   Ruhland K, 2015, COMPUT GRAPH FORUM, V34, P299, DOI 10.1111/cgf.12603
   Salkind NJ, 2010, Encyclopedia of research design, DOI DOI 10.4135/9781412961288
   Sanz FA, 2015, P IEEE VIRT REAL ANN, P75, DOI 10.1109/VR.2015.7223327
   Schrammel F, 2009, PSYCHOPHYSIOLOGY, V46, P922, DOI 10.1111/j.1469-8986.2009.00831.x
   Seidel EM, 2010, J EXP PSYCHOL HUMAN, V36, P500, DOI 10.1037/a0018169
   Seyama J, 2007, PRESENCE-TELEOP VIRT, V16, P337, DOI 10.1162/pres.16.4.337
   Shibata T, 2011, J VISION, V11, DOI 10.1167/11.8.11
   Slater M, 1999, IEEE COMPUT GRAPH, V19, P6, DOI 10.1109/38.749116
   Smith FW, 2009, PSYCHOL SCI, V20, P1202, DOI 10.1111/j.1467-9280.2009.02427.x
   Smith HJ, 2018, PROCEEDINGS OF THE 2018 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2018), DOI 10.1145/3173574.3173863
   Vera L, 2011, LECT NOTES COMPUT SC, V6949, P483, DOI 10.1007/978-3-642-23768-3_63
   Walker Michael E., 2019, 2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR), P538, DOI 10.1109/VR.2019.8798152
   Wallraven C, 2007, ACM T APPL PERCEPT, V4, DOI 10.1145/1278387.1278390
   Ware C., 2020, INFORM VISUALIZATION
   Welch G.F., 2019, Anticipating widespread augmented reality: Insights from the 2018 ar visioning workshop
   Yee N, 2007, CYBERPSYCHOL BEHAV, V10, P115, DOI 10.1089/cpb.2006.9984
   Yoon B, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P547, DOI [10.1109/vr.2019.8797719, 10.1109/VR.2019.8797719]
   Zabels R, 2019, APPL SCI-BASEL, V9, DOI 10.3390/app9153147
   Zell E, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818126
   Zhang YL, 2017, COMPUT HUM BEHAV, V68, P378, DOI 10.1016/j.chb.2016.11.052
NR 84
TC 2
Z9 2
U1 4
U2 7
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD JAN
PY 2023
VL 20
IS 1
AR 4
DI 10.1145/3571074
PG 31
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 7S6AV
UT WOS:000910834800004
DA 2024-07-18
ER

PT J
AU Otake, K
   Okamoto, S
   Akiyama, Y
   Yamada, Y
AF Otake, Kazuya
   Okamoto, Shogo
   Akiyama, Yasuhiro
   Yamada, Yoji
TI Tactile Texture Display Combining Vibrotactile and
   Electrostatic-friction Stimuli: Substantial Effects on Realism and
   Moderate Effects on Behavioral Responses
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Tactile texture display; vibrotactile stimuli; electrostatic-friction
   stimuli
ID ROUGHNESS PERCEPTION; SURFACE; DISCRIMINATION; GRATINGS; GEOMETRY;
   FEEDBACK; MEDIATE; CODES; TOUCH; SHAPE
AB There is increasing demand for tactile feedback functions for touch panels. We investigated whether virtual roughness texture quality can be improved through simultaneous use of vibrotactile and electrostatic-friction stimuli. This conjunctive use is expected to improve the perceptual quality of texture stimuli, because vibrotactile and electrostatic-friction stimuli have complementary characteristics. Our previous studies confirmed that these conjunct stimuli yield enhanced realism for simple grating roughness. In this study, we conducted experiments using simple and complex sinusoidal surface profiles consisting of one or two spatial wave components. Three different evaluation criteria were employed. The first criterion concerned the subjective realism, i.e., similarity with actual roughness textures, of virtual roughness textures. Participants compared the following three stimulus conditions: vibrotactile stimuli only, electrostatic-friction stimuli only, and their conjunct stimuli. The conjunct stimuli yielded the greatest realism. The second criterion concerned roughness texture identification under each of the three stimulus conditions for five different roughness textures. The highest identification accuracy rate was achieved under the conjunct stimulus condition; however, the performance difference was marginal. The third criterion concerned the discrimination threshold of the grating-scale spatial wavelength. There were no marked differences among the results for the three conditions. The findings of this study will improve virtual texture quality for touch-panel-type surface tactile displays.
C1 [Otake, Kazuya; Akiyama, Yasuhiro; Yamada, Yoji] Nagoya Univ, Chikusa Ku, Furo Cho, Nagoya, Aichi 4648603, Japan.
   [Okamoto, Shogo] Tokyo Metropolitan Univ, 6-6 Asahigaoka, Hino, Tokyo 1910065, Japan.
C3 Nagoya University; Tokyo Metropolitan University
RP Otake, K (corresponding author), Nagoya Univ, Chikusa Ku, Furo Cho, Nagoya, Aichi 4648603, Japan.
EM otake.kazuya.d6@s.mail.nagoya-u.ac.jp; okamotos@tmu.ac.jp;
   yasuhiro.akiyama@mae.nagoya-u.ac.jp; yoji.yamada@mae.nagoya-u.ac.jp
OI Akiyama, Yasuhiro/0000-0002-4169-3734; Okamoto,
   Shogo/0000-0003-2116-7734
FU MEXT KAKENHI [20H04263]
FX This study was partly supported by MEXT KAKENHI (#20H04263).
CR Asano S, 2014, ADV ROBOTICS, V28, P1079, DOI 10.1080/01691864.2014.913502
   Basdogan C, 2020, IEEE T HAPTICS, V13, P450, DOI 10.1109/TOH.2020.2990712
   Bau O., 2010, P 23 ANN ACM S US IN, P283, DOI DOI 10.1145/1866029.1866074
   Bochereau S, 2018, ACM T APPL PERCEPT, V15, DOI 10.1145/3152764
   BOLANOWSKI SJ, 1988, J ACOUST SOC AM, V84, P1680, DOI 10.1121/1.397184
   Chapman CE, 2002, BEHAV BRAIN RES, V135, P225, DOI 10.1016/S0166-4328(02)00168-7
   CONNOR CE, 1990, J NEUROSCI, V10, P3823, DOI 10.1523/JNEUROSCI.10-12-03823.1990
   Culbertson H, 2017, IEEE-ASME T MECH, V22, P1839, DOI 10.1109/TMECH.2017.2700467
   Culbertson H, 2017, IEEE T HAPTICS, V10, P63, DOI 10.1109/TOH.2016.2598751
   Culbertson H, 2014, IEEE T HAPTICS, V7, P381, DOI 10.1109/TOH.2014.2316797
   Ernst MO, 2002, NATURE, V415, P429, DOI 10.1038/415429a
   Fielder T., 2019, P INTERNATIONALWORKS
   Fujii Y, 2016, ADV ROBOTICS, V30, P1341, DOI 10.1080/01691864.2016.1208591
   Gueorguiev D, 2017, J R SOC INTERFACE, V14, DOI 10.1098/rsif.2017.0641
   Hassen R., 2020, IEEE T MULTIMEDIA, P1
   Hassen R, 2020, IEEE T HAPTICS, V13, P25, DOI 10.1109/TOH.2019.2962446
   Hassen R, 2019, 2019 IEEE WORLD HAPTICS CONFERENCE (WHC), P301, DOI [10.1109/WHC.2019.8816110, 10.1109/whc.2019.8816110]
   HELLER MA, 1983, PERCEPTION, V12, P607, DOI 10.1068/p120607
   Imaizumi A, 2014, LECT NOTES COMPUT SC, V8619, P11, DOI 10.1007/978-3-662-44196-1_2
   Isleyen A, 2020, IEEE T HAPTICS, V13, P562, DOI 10.1109/TOH.2019.2959993
   Ito K, 2019, ACM T APPL PERCEPT, V16, DOI 10.1145/3340961
   Ito K, 2017, IEEE SYS MAN CYBERN, P2343, DOI 10.1109/SMC.2017.8122972
   Jiao J, 2018, IEEE HAPTICS SYM, P169, DOI 10.1109/HAPTICS.2018.8357171
   JOHNSON KO, 1994, CAN J PHYSIOL PHARM, V72, P488, DOI 10.1139/y94-072
   Jones LA, 2019, 2019 IEEE WORLD HAPTICS CONFERENCE (WHC), P545, DOI [10.1109/WHC.2019.8816078, 10.1109/whc.2019.8816078]
   Klatzky RL, 2003, PERCEPT PSYCHOPHYS, V65, P613, DOI 10.3758/BF03194587
   Konyo M, 2008, LECT NOTES COMPUT SC, V5024, P619, DOI 10.1007/978-3-540-69057-3_79
   LAMB GD, 1983, J PHYSIOL-LONDON, V338, P551, DOI 10.1113/jphysiol.1983.sp014689
   Lawrence MA, 2007, PERCEPTION, V36, P547, DOI 10.1068/p5746
   Lederman S.J., 1999, HAPTICS E ELECT J HA, V1, P1
   Liu GH, 2020, IEEE T HAPTICS, V13, P733, DOI 10.1109/TOH.2020.2979182
   MORLEY JW, 1983, EXP BRAIN RES, V49, P291
   Nakamura Taku, 2017, ROBOMECH Journal, V4, DOI 10.1186/s40648-017-0085-3
   Nakamura T, 2013, 2013 WORLD HAPTICS CONFERENCE (WHC), P37, DOI 10.1109/WHC.2013.6548381
   Okamoto S, 2014, ATTEN PERCEPT PSYCHO, V76, P877, DOI 10.3758/s13414-013-0588-9
   Okamoto S, 2020, IEEE T HAPTICS, V13, P66, DOI 10.1109/TOH.2020.2964538
   Okamoto S, 2013, IEEE T HAPTICS, V6, P81, DOI [10.1109/ToH.2012.32, 10.1109/TOH.2012.32]
   Okamoto S, 2012, IEEE T HAPTICS, V5, P85, DOI [10.1109/ToH.2011.48, 10.1109/TOH.2011.48]
   Otake K, 2020, C HUM SYST INTERACT, P147, DOI [10.1109/HSI49210.2020.9142626, 10.1109/hsi49210.2020.9142626]
   Otto TU, 2013, J NEUROSCI, V33, P7463, DOI 10.1523/JNEUROSCI.4678-12.2013
   Persson BNJ, 2018, J CHEM PHYS, V148, DOI 10.1063/1.5024038
   Peters RM, 2009, J NEUROSCI, V29, P15756, DOI 10.1523/JNEUROSCI.3684-09.2009
   Robles-De-La-Torre G, 2001, NATURE, V412, P445, DOI 10.1038/35086588
   Romano JM, 2012, IEEE T HAPTICS, V5, P109, DOI [10.1109/TOH.2011.38, 10.1109/ToH.2011.38]
   Ryu S, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-018-22865-x
   Ryu S, 2018, LECT NOTES ELECTR EN, V432, P83, DOI 10.1007/978-981-10-4157-0_14
   Saga S., 2012, 2012 IEEE Haptics Symposium (HAPTICS), P15, DOI 10.1109/HAPTIC.2012.6183764
   Saga S, 2013, 2013 WORLD HAPTICS CONFERENCE (WHC), P437, DOI 10.1109/WHC.2013.6548448
   SATHIAN K, 1989, J NEUROSCI, V9, P1273
   Smith S, 2015, J VIBROENG, V17, P1004
   Steinbach E, 2011, IEEE SIGNAL PROC MAG, V28, P87, DOI 10.1109/MSP.2010.938753
   Strese M, 2019, IEEE T HAPTICS, V12, P18, DOI 10.1109/TOH.2018.2864751
   Sun X., 2020, COMPUT J
   Sutu A, 2013, J NEUROPHYSIOL, V109, P1403, DOI 10.1152/jn.00717.2012
   Tanaka Y, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0093363
   Tennison JL, 2020, ACM T APPL PERCEPT, V17, DOI 10.1145/3383457
   Tomita H, 2018, IEEE HAPTICS SYM, P158, DOI 10.1109/HAPTICS.2018.8357169
   Vardar Y, 2017, 2017 IEEE WORLD HAPTICS CONFERENCE (WHC), P263, DOI 10.1109/WHC.2017.7989912
   Vezzoli E, 2015, IEEE T HAPTICS, V8, P235, DOI 10.1109/TOH.2015.2430353
   Weber AI, 2013, P NATL ACAD SCI USA, V110, P17107, DOI 10.1073/pnas.1305509110
   Wiertlewski M, 2011, IEEE T ROBOT, V27, P461, DOI 10.1109/TRO.2011.2132830
   Wijekoon Dinesh, 2012, Haptics: Perception, Devices, Mobility, and Communication. Proceedings International Conference (EuroHaptics 2012), P613, DOI 10.1007/978-3-642-31401-8_54
   Yamauchi Takahiro, 2010, 2010 IEEE International Conference on Robotics and Automation (ICRA 2010), P1753, DOI 10.1109/ROBOT.2010.5509926
   Yoshioka T, 2007, SOMATOSENS MOT RES, V24, P53, DOI 10.1080/08990220701318163
   Yoshioka T, 2009, ADV ROBOTICS, V23, P747, DOI 10.1163/156855309X431703
NR 65
TC 3
Z9 3
U1 2
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD OCT
PY 2022
VL 19
IS 4
SI SI
AR 18
DI 10.1145/3539733
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 6J6QI
UT WOS:000886946700005
OA hybrid
DA 2024-07-18
ER

PT J
AU Devlin, SP
   Byham, JK
   Riggs, SL
AF Devlin, Shannon P.
   Byham, Jennifer K.
   Riggs, Sara Lu
TI Does What We See Shape History? Examining Workload History as a Function
   of Performance and Ambient/Focal Visual Attention
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Workload history; eye tracking; UAV
ID DEMAND TRANSITIONS; MENTAL WORKLOAD; EYE-MOVEMENTS; TASK; STRESS;
   OPERATORS; MEMORY; MODEL
AB Changes in task demands can have delayed adverse impacts on performance. This phenomenon, known as the workload history effect, is especially of concern in dynamic work domains where operators manage fluctuating task demands. The existing workload history literature does not depict a consistent picture regarding how these effects manifest, prompting research to consider measures that are informative on the operator's process. One promising measure is visual attention patterns, due to its informativeness on various cognitive processes. To explore its ability to explain workload history effects, participants completed a task in an unmanned aerial vehicle command and control testbed where workload transitioned gradually and suddenly. The participants' performance and visual attention patterns were studied over time to identify workload history effects. The eye-tracking analysis consisted of using a recently developed eye-tracking metric called coefficient K, as it indicates whether visual attention is more focal or ambient. The performance results found workload history effects, but it depended on the workload level, time elapsed, and performance measure. The eye-tracking analysis suggested performance suffered when focal attention was deployed during low workload, which was an unexpected finding. When synthesizing these results, they suggest unexpected visual attention patterns can impact performance immediately over time. Further research is needed; however, this work shows the value of including a real-time visual attention measure, such as coefficient K, as a means to understand how the operator manages varying task demands in complex work environments.
C1 [Devlin, Shannon P.; Riggs, Sara Lu] Univ Virginia, 151 Engineers Way, Charlottesville, VA 22904 USA.
   [Byham, Jennifer K.] Clemson Univ, 211 Fernow St, Clemson, SC 29634 USA.
C3 University of Virginia; Clemson University
RP Devlin, SP (corresponding author), Univ Virginia, 151 Engineers Way, Charlottesville, VA 22904 USA.
EM spd7t@virginia.edu; jbyham@g.clemson.edu; sriggs@virginia.edu
OI McGarry, Shannon/0000-0003-2652-012X
FU National Science Foundation (NSF) [1566346, 1750850]; Div Of Information
   & Intelligent Systems; Direct For Computer & Info Scie & Enginr
   [1566346, 1750850] Funding Source: National Science Foundation
FX This study was supported in part by the National Science Foundation (NSF
   Grant No. 1566346, Program Manager: Dr. Ephraim Glinert; NSF Grant No.
   1750850, Program Manager: Dr. Andrew Kerne).
CR Abich J, 2017, ERGONOMICS, V60, P791, DOI 10.1080/00140139.2016.1216171
   [Anonymous], 1935, How people look at pictures: A study of the psychology and perception in art
   [Anonymous], 2004, P 2004 S EYE TRACK R, DOI DOI 10.1145/968363.968391
   Bowers M.A., 2014, Proceedings of the Human Factors and Ergonomics Society Annual Meeting, V58, P220, DOI DOI 10.1177/1541931214581046
   Cohen J., 1988, STAT POWER ANAL BEHA
   Coral Melissa Patricia, THESIS WRIGHT STATE
   Cox-Fuenzalida LE, 2006, J RES PERS, V40, P432, DOI 10.1016/j.jrp.2005.02.003
   Cox-Fuenzalida LE, 2005, CURR PSYCHOL, V24, P171, DOI 10.1007/s12144-005-1020-y
   Cox-Fuenzalida LE, 2004, PERS INDIV DIFFER, V36, P447, DOI 10.1016/S0191-8869(03)00108-9
   Cox-Fuenzalida LE, 2007, HUM FACTORS, V49, P277, DOI 10.1518/001872007X312496
   CUMMING RW, 1973, ERGONOMICS, V16, P581, DOI 10.1080/00140137308924548
   Cutrell E, 2007, CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1 AND 2, P407
   Devlin SP, 2020, IISE T OCCUP ERG HUM, V8, P72, DOI 10.1080/24725838.2020.1770898
   Di Nocera F, 2007, J COGN ENG DECIS MAK, V1, P271, DOI 10.1518/155534307X255627
   Duchowski A. T., 2017, EYE TRACKING METHODO, DOI [10.1007/978-3-319-57883-5, DOI 10.1007/978-3-319-57883-5]
   Duchowski AT, 2020, IEEE T VIS COMPUT GR, V26, P2904, DOI 10.1109/TVCG.2019.2901881
   Duchowski AT, 2002, BEHAV RES METH INS C, V34, P455, DOI 10.3758/BF03195475
   Edwards T., 2017, Communications in Computer and Information Science, V76, P120, DOI [10.1007/978-3-319-61061-0_8, DOI 10.1007/978-3-319-61061-0_8]
   EyeTracking Inc, 2011, HARDW EYE TRACK SYST
   Fallahi M, 2016, INT J IND ERGONOM, V54, P170, DOI 10.1016/j.ergon.2016.06.005
   Fallahi M, 2016, HEALTH PROMOT PERSPE, V6, P96, DOI 10.15171/hpp.2016.17
   Farrell PSE, 1999, HUM FACTORS, V41, P226, DOI 10.1518/001872099779591259
   Feigh KM, 2012, HUM FACTORS, V54, P1008, DOI 10.1177/0018720812443983
   GLUCKMAN JP, 1993, J GEN PSYCHOL, V120, P323, DOI 10.1080/00221309.1993.9711151
   Goldberg JH, 1999, INT J IND ERGONOM, V24, P631, DOI 10.1016/S0169-8141(98)00068-7
   GOLDBERG RA, 1980, ERGONOMICS, V23, P1173, DOI 10.1080/00140138008924824
   Hancock PA, 2019, HUM FACTORS, V61, P374, DOI 10.1177/0018720818809590
   HANCOCK PA, 1989, HUM FACTORS, V31, P519
   Hancock PA, 1995, INT J AVIAT PSYCHOL, V5, P63, DOI 10.1207/s15327108ijap0501_5
   Helmert Jens R, 2005, P ANN M COGN SCI, V27
   Helton WS, 2008, ANXIETY STRESS COPIN, V21, P173, DOI 10.1080/10615800801911305
   Hockey GRJ, 1997, BIOL PSYCHOL, V45, P73
   Hooey BL, 2018, IEEE T HUM-MACH SYST, V48, P452, DOI 10.1109/THMS.2017.2759758
   [Huey B.M. National Research Council (U.S.) National Research Council (U.S.)], 1993, WORKLOAD TRANSITION
   Irwin DE, 2002, PERCEPT PSYCHOPHYS, V64, P882, DOI 10.3758/BF03196793
   Jacob RJK, 2003, MIND'S EYE: COGNITIVE AND APPLIED ASPECTS OF EYE MOVEMENT RESEARCH, P573, DOI 10.1016/B978-044451020-4/50031-1
   Jansen RJ, 2016, HUM FACTORS, V58, P1143, DOI 10.1177/0018720816669271
   Jarodzka H, 2010, LEARN INSTR, V20, P146, DOI 10.1016/j.learninstruc.2009.02.019
   Jiang JP, 2014, SIGIR'14: PROCEEDINGS OF THE 37TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P607, DOI 10.1145/2600428.2609633
   Kim N., 2018, Proceedings of the Human Factors and Ergonomics Society Annual Meeting, P11, DOI [10.1177/1541931218621003, DOI 10.1177/1541931218621003]
   Kim NY, 2019, FRONT HUM NEUROSCI, V12, DOI 10.3389/fnhum.2018.00535
   Krejtz K., 2014, P S EYE TRACK RES AP, P159, DOI DOI 10.1145/2578153.2578176
   Krejtz K., 2018, PROC 2018 ACM S EYE, P1
   Krejtz K, 2017, J EYE MOVEMENT RES, V10, DOI 10.16910/jemr.10.2.3
   Krejtz K, 2016, ACM T APPL PERCEPT, V13, DOI 10.1145/2896452
   Krejtz K, 2016, J EYE MOVEMENT RES, V9
   KRULEWITZ JE, 1975, PERCEPT PSYCHOPHYS, V18, P245, DOI 10.3758/BF03199369
   Lakens D, 2013, FRONT PSYCHOL, V4, DOI 10.3389/fpsyg.2013.00863
   MATTHEWS ML, 1986, HUM FACTORS, V28, P623, DOI 10.1177/001872088602800601
   Moacdieh NM, 2020, J COGN ENG DECIS MAK, V14, P132, DOI 10.1177/1555343419892184
   Morgan JF, 2011, HUM FACTORS, V53, P75, DOI 10.1177/0018720810393505
   MORONEY BW, 1995, HUM FAC ERG SOC P, P1375
   Over EAB, 2007, VISION RES, V47, P2272, DOI 10.1016/j.visres.2007.05.002
   Pannasch S, 2008, J EYE MOVEMENT RES, V2, DOI 10.16910/jemr.2.2.4
   Poole A., 2006, Encyclopedia of human computer interaction, P211, DOI [10.4018/978-1-59140-562-7.ch034, DOI 10.4018/978-1-59140-562-7.CH034]
   Pournelle G. H., 1953, Journal of Mammalogy, V34, P133, DOI 10.1890/0012-9658(2002)083[1421:SDEOLC]2.0.CO;2
   Previc FH, 1998, PSYCHOL BULL, V124, P123, DOI 10.1037/0033-2909.124.2.123
   Prytz EG, 2015, THEOR ISS ERGON SCI, V16, P586, DOI 10.1080/1463922X.2015.1084397
   Rothrock L., 2002, THEOR ISS ERGON SCI, V3, P47, DOI [10.1080/14639220110110342, DOI 10.1080/14639220110110342]
   Rowe Allen, 2008, P AIAA GUID NAV CONT, P6309
   Salvucci DD, 2000, 2000 S EYE TRACKING, P71, DOI [10.1145/355017.355028, DOI 10.1145/355017.355028]
   Sibley Ciara, 2015, P AAAI SPRING S, P454
   Thomas L.C., 2004, P HUMAN FACTORS ERGO, V48, P223, DOI DOI 10.1177/154193120404800148
   Winnefeld James A., 2013, APPROVED OPEN PUBLIC
NR 64
TC 3
Z9 3
U1 2
U2 5
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD JUN
PY 2021
VL 18
IS 2
AR 8
DI 10.1145/3449066
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA SR6EX
UT WOS:000661137000004
DA 2024-07-18
ER

PT J
AU Brickler, D
   Teather, RJ
   Duchowski, AT
   Babu, SV
AF Brickler, David
   Teather, Robert J.
   Duchowski, Andrew T.
   Babu, Sabarish, V
TI A Fitts' Law Evaluation of Visuo-haptic Fidelity and Sensory Mismatch on
   User Performance in a Near-field Disc Transfer Task in Virtual Reality
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article; Proceedings Paper
CT 17th ACM Symposium on Applied Perception (SAP)
CY SEP, 2020
CL ELECTR NETWORK
SP ACM
DE Fitts' law; near-field virtual reality; haptics; stereo
ID SIMULATOR; FEEDBACK; SYSTEM; MOUSE
AB The trade-off between speed and accuracy in precision tasks is important to evaluate during user interaction with input devices. When different sensory cues are added or altered in such interactions, those cues have an effect on this trade-off, and thus, they affect overall user performance. For instance, adding cues like haptic feedback and stereoscopic viewing will result in more realistic user interaction, thus improving performance in these tasks. Also, adding a noticeable disparity between physical and virtual movements creates a mismatch between visual and proprioceptive systems, which generally has a negative effect on performance. In this study, we investigate the effects of haptic feedback, stereoscopic viewing, and visuo-proprioceptive mismatch on how quickly and accurately users complete a virtual pick-and-place task using the PHANToM OMNI. Through this experiment, we find that in the movement phase of a ring transfer, movement time and user performance are affected by haptic feedback and visuo-proprioceptive mismatch, and the main effects of stereoscopic viewing appears to be limited to the more precise step when the ring is around the target peg.
C1 [Brickler, David; Duchowski, Andrew T.; Babu, Sabarish, V] Clemson Univ, Sch Comp, 100 McAdams Hall, Clemson, SC 29634 USA.
   [Teather, Robert J.] Carleton Univ, Sch Informat Technol, 230 Azrieli Pavil,1125 Colonel Dr, Ottawa, ON K1S 5B6, Canada.
C3 Clemson University; Carleton University
RP Brickler, D (corresponding author), Clemson Univ, Sch Comp, 100 McAdams Hall, Clemson, SC 29634 USA.
EM dbrickl@g.clemson.edu; rob.teather@carleton.ca; duchowski@clemson.edu;
   sbabu@clemson.edu
CR [Anonymous], 1994, P ASME WINT ANN M S, DOI DOI 10.1145/1029632.1029682
   Batmaz Anil Ufuk, 2019, P CHI C HUM FACT COM
   Berger CC, 2018, SCI ROBOT, V3, DOI 10.1126/scirobotics.aar7010
   Best DS, 2016, 2016 ACM SYMPOSIUM ON EYE TRACKING RESEARCH & APPLICATIONS (ETRA 2016), P69, DOI 10.1145/2857491.2857527
   Bhargava A, 2018, IEEE T VIS COMPUT GR, V24, P1418, DOI 10.1109/TVCG.2018.2794639
   Brickler D, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P28, DOI [10.1109/vr.2019.8797744, 10.1109/VR.2019.8797744]
   Bruder G, 2015, P IEEE VIRT REAL ANN, P27, DOI 10.1109/VR.2015.7223320
   CARD SK, 1978, ERGONOMICS, V21, P601, DOI 10.1080/00140137808931762
   Chun K, 2004, 3RD IEEE INTERNATIONAL WORKSHOP ON HAPTIC, AUDIO AND VISUAL ENVIRONMENTS AND THEIR APPLICATIONS - HAVE 2004, P53, DOI 10.1109/HAVE.2004.1391881
   Culbertson H, 2018, ANNU REV CONTR ROBOT, V1, P385, DOI 10.1146/annurev-control-060117-105043
   Culbertson H, 2017, IEEE T HAPTICS, V10, P63, DOI 10.1109/TOH.2016.2598751
   Douglas SarahA., 1999, Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI'99), P215, DOI DOI 10.1145/302979.303042
   Ebrahimi Elham., 2014, Proceedings of the ACM Symposium on Applied Perception - SAP '14, P103, DOI [10.1145/2628257.2628268, DOI 10.1145/2628257]
   Esmaeili S, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR 2020), P453, DOI [10.1109/VR46266.2020.1581285352835, 10.1109/VR46266.2020.00-38]
   Figueiredo LucasS., 2014, International Conference of Design, User Experience, and Usability, P560
   FITTS PM, 1954, J EXP PSYCHOL, V47, P381, DOI 10.1037/h0055392
   Fried GM, 2008, J GASTROINTEST SURG, V12, P210, DOI 10.1007/s11605-007-0355-0
   Fu MJ, 2011, IEEE INT C INT ROBOT, P3460, DOI 10.1109/IROS.2011.6048296
   Garstka J., 2011, 8th IEEE International Workshop on Projector-camera Systems, P52
   Grossman Tovi., 2005, CHI 05, P281, DOI DOI 10.1145/1054972.1055012
   Guilford JP, 1948, J APPL PSYCHOL, V32, P24, DOI 10.1037/h0063610
   HART S G, 1988, P139
   John NW, 2016, PRESENCE-TELEOP VIRT, V25, P289, DOI 10.1162/PRES_a_00270
   JOHNSGARD T, 1994, GRAPH INTER, P8
   Jones JA, 2016, 2016 IEEE SYMPOSIUM ON 3D USER INTERFACES (3DUI), P221, DOI 10.1109/3DUI.2016.7460055
   Kabbash P., 1995, Human Factors in Computing Systems. CHI'95 Conference Proceedings, P273, DOI 10.1145/223904.223939
   Kaber DB, 2012, IEEE T SYST MAN CY A, V42, P1562, DOI 10.1109/TSMCA.2012.2201466
   Khalifa A, 2014, 2014 19TH INTERNATIONAL CONFERENCE ON METHODS AND MODELS IN AUTOMATION AND ROBOTICS (MMAR), P675, DOI 10.1109/MMAR.2014.6957435
   Kohli L, 2012, 3D US INT 3DUI 2012, P105, DOI DOI 10.1109/3DUI.2012.6184193
   KVALSETH TO, 1980, B PSYCHONOMIC SOC, V16, P371
   LEWIS JR, 1995, INT J HUM-COMPUT INT, V7, P57, DOI 10.1080/10447319509526110
   Li J., 2018, P S SPAT US INT SUI, P120
   Lubos P, 2014, 2014 IEEE SYMPOSIUM ON 3D USER INTERFACES (3DUI), P11, DOI 10.1109/3DUI.2014.6798834
   Machuca Barrera Mayra Donaji, 2018, P 2018 CHI C HUM FAC
   MacKenzie I. S., 1992, CHI '92 Conference Proceedings. ACM Conference on Human Factors in Computing Systems. Striking a Balance, P219, DOI 10.1145/142750.142794
   MacKenzie IS, 2008, CHI 2008: 26TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS VOLS 1 AND 2, CONFERENCE PROCEEDINGS, P1633
   MACKENZIE IS, 1989, J MOTOR BEHAV, V21, P323
   MacKenzie Scott, 2012, P 7 NORD C HUM COMP, P568
   McGuffin M., 2002, Conference Proceedings. Conference on Human Factors in Computing Systems. CHI 2002, P57, DOI 10.1145/503376.503388
   McIntire JP, 2014, DISPLAYS, V35, P18, DOI 10.1016/j.displa.2013.10.004
   McMahan R.P., 2011, THESIS
   Minamizawa Kouta, 2010, 2010 IEEE Haptics Symposium (Formerly known as Symposium on Haptic Interfaces for Virtual Environment and Teleoperator Systems), P257, DOI 10.1109/HAPTIC.2010.5444646
   Morris D, 2007, WORLD HAPTICS 2007: SECOND JOINT EUROHAPTICS CONFERENCE AND SYMPOSIUM ON HAPTIC INTERFACES FOR VIRTUAL ENVIRONMENT AND TELEOPERATOR SYSTEMS, PROCEEDINGS, P21
   Murata A, 2001, HUM MOVEMENT SCI, V20, P791, DOI 10.1016/S0167-9457(01)00058-6
   Natapov D., 2009, Proceedings of Graphics Interface 2009, P223
   Nieuwenhuizen K, 2009, IEEE COMPUT GRAPH, V29, P44, DOI 10.1109/MCG.2009.121
   Nisky I, 2011, IEEE T HAPTICS, V4, P155, DOI [10.1109/ToH.2011.30, 10.1109/TOH.2011.30]
   Okamura AM, 1998, IEEE INT CONF ROBOT, P674, DOI 10.1109/ROBOT.1998.677050
   Pagano CC, 1998, J EXP PSYCHOL HUMAN, V24, P1037, DOI 10.1037/0096-1523.24.4.1037
   Pinho M. S., 2002, VRST 02, P171
   Poyade M., 2012, JOINT VIRT REAL C IC, P85
   Poyade M., 2014, HAPTIC PLUG IN UNITY
   Qian Y, 2017, SUI'17: PROCEEDINGS OF THE 2017 SYMPOSIUM ON SPATIAL USER INTERACTION, P91, DOI 10.1145/3131277.3132182
   Ramcharitar A, 2018, P GRAPHICS INTERFACE, P114
   Schlunsen R., 2019, P MENSCH COMPUTER 20, P223, DOI DOI 10.1145/3340764.3340791
   Schorr SB, 2017, PROCEEDINGS OF THE 2017 ACM SIGCHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'17), P3115, DOI 10.1145/3025453.3025744
   Soukoreff RW, 2004, INT J HUM-COMPUT ST, V61, P751, DOI 10.1016/j.ijhcs.2004.09.001
   Sroka G, 2010, AM J SURG, V199, P115, DOI 10.1016/j.amjsurg.2009.07.035
   Teather R. J., 2011, Proceedings 2011 IEEE Symposium on 3D User Interfaces (3DUI 2011), P87, DOI 10.1109/3DUI.2011.5759222
   Teather RJ, 2010, P IEEE VIRT REAL ANN, P307, DOI 10.1109/VR.2010.5444753
   Teather RobertJ., 2013, Conference on Human Factors in Computing Systems - Proceedings, P159, DOI DOI 10.1145/2470654.2470677
   WARE C, 1994, GRAPH INTER, P1
   Woodworth RS, 1899, PSYCHOL REV-MONOGR S, V3, P1
   Worden Aileen., 1997, P SIGCHI C HUMAN FAC, P266, DOI DOI 10.1145/258549.258724
   Zenner A, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P47, DOI [10.1109/vr.2019.8798143, 10.1109/VR.2019.8798143]
   Zhou T, 2018, ACMIEEE INT CONF HUM, P285, DOI 10.1145/3173386.3176981
NR 66
TC 4
Z9 5
U1 2
U2 11
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD DEC
PY 2020
VL 17
IS 4
SI SI
AR 15
DI 10.1145/3419986
PG 20
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA PH1UI
UT WOS:000600206200004
DA 2024-07-18
ER

PT J
AU Lin, YX
   Venkatakrishnan, R
   Venkatakrishnan, R
   Ebrahimi, E
   Lin, WC
   Babu, SV
AF Lin, Yun-Xuan
   Venkatakrishnan, Rohith
   Venkatakrishnan, Roshan
   Ebrahimi, Elham
   Lin, Wen-Chieh
   Babu, Sabarish, V
TI How the Presence and Size of Static Peripheral Blur Affects
   Cybersickness in Virtual Reality
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article; Proceedings Paper
CT 17th ACM Symposium on Applied Perception (SAP)
CY SEP, 2020
CL ELECTR NETWORK
SP ACM
DE Virtual reality; cybersickness; blurring; foveated rendering
ID FIELD-OF-VIEW; MOTION SICKNESS; PERFORMANCE; INFORMATION; DISPLAY;
   VECTION; TRAVEL; DEPTH
AB Cybersickness (CS) is one of the challenges that has hindered the widespread adoption of Virtual Reality and its applications. Consequently, a number of studies have focused on extensively understanding and reducing CS. Inspired by previous work that has sought to reduce CS using foveated rendering and Field of View (FOV) restrictions, we investigated how the presence and size of a static central window in peripheral FOV blurring affects CS. To facilitate this peripheral FOV blur, we applied a Gaussian blur effect in the display peripheral region, provisioning a full-resolution central window. Thirty participants took part in a three-session, within-subjects experiment, performing search and spatial updating tasks in a first-person, slow-walking, maze-traveling scenario. Two different central window sizes (small and large) were tested against a baseline condition that didn't feature display peripheral blurring. Results revealed that the baseline condition produced higher levels of CS than both conditions with a central window. While there were no significant differences between the small and large windows, we observed interaction effects suggesting an influence of window size on "adaptation to CS." When the central window is small, adaptation to CS seems to take more time but is more pronounced. The interventions had no effect on spatial updating and presence, but were detectable when the blurred area was larger (small central window). Lower sickness levels observed in both window conditions supports the use of peripheral FOV blurring to reduce CS, reducing our dependence on eye tracking. This being said, researchers must strive to find the right balance between window size and detectability to ensure seamless virtual experiences.
C1 [Lin, Yun-Xuan; Lin, Wen-Chieh] Natl Chiao Tung Univ, 1001 Daxue Rd, Hsinchu 300, Taiwan.
   [Venkatakrishnan, Rohith; Venkatakrishnan, Roshan; Babu, Sabarish, V] Clemson Univ, Sch Comp, 821 McMillan Rd, Clemson, SC 29631 USA.
   [Ebrahimi, Elham] Univ North Carolina Wilmington, Dept Comp Sci, Congdon Hall,601 South Coll Rd, Wilmington, NC 28403 USA.
C3 National Yang Ming Chiao Tung University; Clemson University; University
   of North Carolina; University of North Carolina Wilmington
RP Lin, YX (corresponding author), Natl Chiao Tung Univ, 1001 Daxue Rd, Hsinchu 300, Taiwan.
EM librechat2514@gmail.com; rohithv@g.clemson.edu; rvenkat@g.clemson.edu;
   ebrahimie@uncw.edu; wclin@cs.nctu.edu.tw; sbabu@clemson.edu
RI Venkatakrishnan, Roshan/JDC-3508-2023; Venkatakrishnan,
   Rohith/JCE-8736-2023
OI Venkatakrishnan, Roshan/0000-0002-6538-627X; Venkatakrishnan,
   Rohith/0000-0002-8484-3915; Ebrahimi, Elham/0000-0001-9431-557X
FU Ministry of Science and Technology of Taiwan [109-2221-E-009-123-MY3]
FX This work was supported in part by the Ministry of Science and
   Technology of Taiwan under Grant No. 109-2221-E-009-123-MY3. Authors'
   addresses: Y.-X. Lin, National Chiao Tung University, No. 1001, Daxue
   Road, East District, Hsinchu City, Taiwan 300; email:
   librechat2514@gmail.com;R.Venkatakrishnan, Clemson University, School of
   Computing, 821 McMillan Rd, Clemson, South Carolina 29631, USA; email:
   rohithv@g.clemson.edu;R.Venkatakrishnan and S. V. Babu, Clemson
   University, School of Computing, 821 McMillan Rd, Clemson, South
   Carolina -29631, USA; emails:
   rvenkat@g.clemson.edu,sbabu@clemson.edu;E.Ebrahimi, Department of
   Computer Science, University of North Carolina Wilmington, Congdon Hall,
   601 South College Road, Wilmington, North Carolina -28403, USA; email:
   ebrahimie@uncw.edu;W.-C.Lin, National Chiao Tung University, No. 1001,
   Daxue Road, East District, Hsinchu City, Taiwan 300; email:
   wclin@cs.nctu.edu.tw.
CR Adhanom IB, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR 2020), P645, DOI [10.1109/VR46266.2020.00-17, 10.1109/VR46266.2020.1581314696458]
   ALFANO PL, 1990, PERCEPT MOTOR SKILL, V70, P35, DOI 10.2466/PMS.70.1.35-45
   [Anonymous], 1992, Presence, DOI DOI 10.1162/PRES.1992.1.3.344
   [Anonymous], 2017, P CHIN C IM GRAPH TE
   [Anonymous], 2003, MAN UN TRAFF CONTR D
   Arafat Imtiaz Muhammad, 2018, 2018 IEEE C VIRT REA, P1, DOI [10.1109/VR.2018.8446194, DOI 10.1109/VR.2018.8446194]
   Arcioni B, 2019, DISPLAYS, V58, P3, DOI 10.1016/j.displa.2018.07.001
   Barhorst-Cates EM, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0163785
   Bowman DA, 1997, P IEEE VIRT REAL ANN, P45, DOI 10.1109/VRAIS.1997.583043
   Budhiraja Pulkit., 2017, Rotation Blurring: Use of Artificial Blurring to Reduce Cybersickness in Virtual Reality First Person Shooters
   Cao Zekun, P IEEE C VIRT REAL 3, P105
   Carnegie K, 2015, IEEE COMPUT GRAPH, V35, P34, DOI 10.1109/MCG.2015.98
   Chance SS, 1998, PRESENCE-TELEOP VIRT, V7, P168, DOI 10.1162/105474698565659
   Chen YT, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P172, DOI [10.1109/VR.2019.8798338, 10.1109/vr.2019.8798338]
   DiZio P, 1997, ADV HUM FACT ERGON, V21, P893
   Ebrahimi Elham, 2015, 2015 IEEE Symposium on 3D User Interfaces (3DUI), P97, DOI 10.1109/3DUI.2015.7131732
   Ebrahimi Elham., 2014, Proceedings of the ACM Symposium on Applied Perception - SAP '14, P103, DOI [10.1145/2628257.2628268, DOI 10.1145/2628257]
   Fang Y, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0121035
   Farmani Yasin, P C GRAPH INT GI 18, P159
   Fernandes Ajoy S., P IEEE S 3D US INT 3, P201
   Fisher DL, 2006, INJURY PREV, V12, P25, DOI 10.1136/ip.2006.012021
   Golding JF, 1998, BRAIN RES BULL, V47, P507, DOI 10.1016/S0361-9230(98)00091-4
   Heinrichs WL, 2008, WORLD J SURG, V32, P161, DOI 10.1007/s00268-007-9354-2
   Hettinger L.J., 1992, Presence: Teleoperators & Virtual Environments, P306, DOI [10.1162/pres.1992.1.3.306, DOI 10.1162/PRES.1992.1.3.306]
   Hillaire S, 2008, IEEE COMPUT GRAPH, V28, P47, DOI 10.1109/MCG.2008.113
   HOWARD IP, 1989, PERCEPTION, V18, P657, DOI 10.1068/p180657
   Hsu CF, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P55, DOI 10.1145/3123266.3123434
   Igroup, 2018, IGR PRES QUEST IPQ
   Kennedy RS, 1993, The international journal of aviation psychology, V3, P203, DOI [DOI 10.1207/S15327108IJAP0303, 10.1207/s15327108ijap0303_3]
   Keshavarz B, 2015, FRONT PSYCHOL, V6, DOI 10.3389/fpsyg.2015.00472
   Keshavarz B, 2011, HUM FACTORS, V53, P415, DOI 10.1177/0018720811403736
   Keshavarz Behrang., 2014, Handbook of Virtual Environments: Design, Implementation, and Applications Issue September, P647, DOI [DOI 10.1201/B17360-32, https://doi.org/10.1201/b17360-32]
   Kolasinski EM, 1995, TECHNICAL REPORT
   La Viola J. J.  Jr., 2000, SIGCHI Bulletin, V32, P47, DOI 10.1145/333329.333344
   Lee TM, 2019, IEEE T VIS COMPUT GR, V25, P1919, DOI 10.1109/TVCG.2019.2899186
   McCauley M. E., 1992, Presence: Teleoperators & Virtual Environments, V1, P311, DOI DOI 10.1162/PRES.1992.1.3.311
   Meng XX, 2018, P ACM COMPUT GRAPH, V1, DOI 10.1145/3203199
   Moghadam K, 2020, IEEE T VIS COMPUT GR, V26, P2273, DOI 10.1109/TVCG.2018.2884468
   Napieralski PE, 2011, ACM T APPL PERCEPT, V8, DOI 10.1145/2010325.2010328
   Ni T, 2006, PROC GRAPH INTERF, P139
   Nie GY, 2020, IEEE T VIS COMPUT GR, V26, P2535, DOI 10.1109/TVCG.2019.2893668
   Norouzi N, 2018, 18TH ACM INTERNATIONAL CONFERENCE ON INTELLIGENT VIRTUAL AGENTS (IVA'18), P17, DOI 10.1145/3267851.3267901
   Oculus from Facebook, 2018, OC GO FIX FOV REND
   Padmanaban N, 2018, IEEE T VIS COMPUT GR, V24, P1594, DOI 10.1109/TVCG.2018.2793560
   Patney A, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980246
   Polys NF, 2007, COMPUT ANIMAT VIRT W, V18, P19, DOI 10.1002/cav.159
   REASON JT, 1978, J ROY SOC MED, V71, P819, DOI 10.1177/014107687807101109
   Rebenitsch L., 2015, XRDS: Crossroads, The ACM Magazine for Students, V22, P46, DOI [10.1145/2810054, DOI 10.1145/2810054]
   RICCIO G E, 1991, Ecological Psychology, V3, P195, DOI 10.1207/s15326969eco0303_2
   Sherman CR, 2002, J TRAVEL MED, V9, P251
   Swafford N. T., 2016, P ACM S APPL PERC SA, P7, DOI DOI 10.1145/2931002.2931011
   Venkatakrishnan R, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR 2020), P682, DOI [10.1109/VR46266.2020.00-13, 10.1109/VR46266.2020.1581195115265]
   Venkatakrishnan R, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR 2020), P672, DOI [10.1109/VR46266.2020.00-14, 10.1109/VR46266.2020.1581256520838]
   Wald J, 2000, J BEHAV THER EXP PSY, V31, P249, DOI 10.1016/S0005-7916(01)00009-X
   Weier M, 2016, COMPUT GRAPH FORUM, V35, P289, DOI 10.1111/cgf.13026
   Weissker Tim, P IEEE C VIRT REAL 3, P97
   WELLS MJ, 1990, OPT ENG, V29, P870, DOI 10.1117/12.55672
   Yao Richard., 2014, Oculus VR, V4, P27
NR 58
TC 20
Z9 27
U1 0
U2 5
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD DEC
PY 2020
VL 17
IS 4
SI SI
AR 16
DI 10.1145/3419984
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA PH1UI
UT WOS:000600206200005
DA 2024-07-18
ER

PT J
AU Company, P
   Plumed, R
   Varley, PAC
   Camba, JD
AF Company, Pedro
   Plumed, Raquel
   Varley, Peter A. C.
   Camba, Jorge D.
TI Algorithmic Perception of Vertices in Sketched Drawings of Polyhedral
   Shapes
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article; Proceedings Paper
CT 16th Symposium on Applied Perception (SAP)
CY SEP, 2019
CL Barcelona, SPAIN
DE Algorithmic perception; polyhedral shapes; vertices; junctions
ID OPTIMIZATION-BASED RECONSTRUCTION; 3D OBJECT
AB In this article, visual perception principles were used to build an artificial perception model aimed at developing an algorithm for detecting junctions in line drawings of polyhedral objects that are vectorized from hand-drawn sketches. The detection is performed in two dimensions (2D), before any 3D model is available and minimal information about the shape depicted by the sketch is used. The goal of this approach is to not only detect junctions in careful sketches created by skilled engineers and designers but also detect junctions when skilled people draw casually to quickly convey rough ideas. Current approaches for extracting junctions from digital images are mostly incomplete, as they simply merge endpoints that are near each other, thus ignoring the fact that different vertices may be represented by different (but close) junctions and that the endpoints of lines that depict edges that share a common vertex may not necessarily be close to each other, particularly in quickly sketched drawings. We describe and validate a new algorithm that uses these perceptual findings to merge tips of line segments into 2D junctions that are assumed to depict 3D vertices.
C1 [Company, Pedro] Univ Jaume 1, Inst New Imaging Technol, Av Sos Baynat S-N, Castellon de La Plana 12071, Spain.
   [Plumed, Raquel; Varley, Peter A. C.] Univ Jaume 1, Dept Mech Engn & Construct, Av Sos Baynat S-N, Castellon de La Plana 12071, Spain.
   [Camba, Jorge D.] Purdue Univ, Dept Comp Graph Technol, Knoy Hall 327, W Lafayette, IN 47907 USA.
C3 Universitat Jaume I; Universitat Jaume I; Purdue University System;
   Purdue University
RP Company, P (corresponding author), Univ Jaume 1, Inst New Imaging Technol, Av Sos Baynat S-N, Castellon de La Plana 12071, Spain.
EM pcompany@uji.es; plumed@uji.es; varley@uji.es; jdorribo@purdue.edu
RI Company, Pedro/G-2567-2016; Plumed, Raquel/C-4873-2015
OI Company, Pedro/0000-0001-6399-4717; Plumed, Raquel/0000-0001-8018-8039
CR [Anonymous], 2009, Found. Trends Hum.-Comput. Interact, DOI DOI 10.1561/1100000013
   [Anonymous], 1998, Visual intelligence
   Company Pedro, 2019, Progress in Pattern Recognition, Image Analysis, Computer Vision, and Applications. 23rd Iberoamerican Congress, CIARP 2018. Proceedings: Lecture Notes in Computer Science (LNCS 11401), P376, DOI 10.1007/978-3-030-13469-3_44
   Company P, 2004, COMPUT GRAPH-UK, V28, P955, DOI 10.1016/j.cag.2004.08.007
   Ekwaro-Osire S, 2016, J INTEGR DES PROCESS, V20, P43, DOI 10.3233/jid-2016-0022
   Favreau JD, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925946
   Ferguson E., 1992, ENG MINDS EYE
   Fiser J, 2016, COMPUT GRAPH-UK, V56, P46, DOI 10.1016/j.cag.2016.02.003
   Governi L, 2013, COMPUT IND, V64, P1290, DOI 10.1016/j.compind.2013.02.003
   Jenkins D. L., 1992, THESIS
   Jorge J. A., 1999, GREC99
   Kanizsa G., 1979, Organization in Vision: Essays on Gestalt Perception
   LECLERC YG, 1992, INT J COMPUT VISION, V9, P113, DOI 10.1007/BF00129683
   Lipson H, 1996, COMPUT AIDED DESIGN, V28, P651, DOI 10.1016/0010-4485(95)00081-X
   LIU X., 2015, ACM T GRAPHIC, V34, P6
   Orbay G, 2011, IEEE T VIS COMPUT GR, V17, P694, DOI 10.1109/TVCG.2010.105
   Pache M. W., 2005, THESIS
   Perkins D. N., 1972, CUBIC CORNERS OBLIQU
   Simo-Serra E., 2016, ACM T GRAPHICS, V35
   Varley P. A. C., 2017, TECHNICAL REPORT
   Xu BX, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601128
   Yuan S, 2008, PATTERN RECOGN LETT, V29, P1486, DOI 10.1016/j.patrec.2008.03.003
NR 22
TC 4
Z9 4
U1 0
U2 2
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD SEP
PY 2019
VL 16
IS 3
SI SI
AR 18
DI 10.1145/3345507
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA JA3KH
UT WOS:000487720000006
OA Green Published
DA 2024-07-18
ER

PT J
AU Tennison, JL
   Gorlewicz, JL
AF Tennison, Jennifer L.
   Gorlewicz, Jenna L.
TI Non-visual Perception of Lines on a Multimodal Touchscreen Tablet
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Haptics; touchscreen; perception; HCI
ID TOUCH; BLIND; EDUCATION; HAPTICS; DISPLAY
AB While text-to-speech software has largely made textual information accessible in the digital space, analogous access to graphics still remains an unsolved problem. Because of their portability and ubiquity, several studies have alluded to touchscreens as a potential platform for such access, yet there is still a gap in our understanding of multimodal information transfer in the context of graphics. The current research demonstrates feasibility for following lines, a fundamental graphical concept, via vibrations and sounds on commercial touchscreens. Two studies were run with 21 blind and visually impaired participants (N = 12; N= 9). The first study examined the presentation of straight, linear lines using a multitude of line representations, such as vibration-only, auditory-only, vibration lines with auditory borders, and auditory lines with vibration borders. The results of this study demonstrated that both auditory and vibratory bordered lines were optimal for precise tracing, although both vibration- and auditory-only lines were also sufficient for following, with minimal deviations. The second study examined the presentation of curving, non-linear lines. Conditions differed on the number of auditory reference points presented at the inflection and deflection points. Participants showed minimal deviation from the lines during tracing, performing nearly equally in both 1- and 3-point conditions. From these studies, we demonstrate that line following via multimodal feedback is possible on touchscreens, and we present guidelines for the presentation of such non-visual graphical concepts.
C1 [Tennison, Jennifer L.; Gorlewicz, Jenna L.] St Louis Univ, Aerosp & Mech Engn Dept, Pk Coll Engn, 3450 Lindell Blvd, St Louis, MO 63103 USA.
C3 Saint Louis University
RP Tennison, JL (corresponding author), St Louis Univ, Aerosp & Mech Engn Dept, Pk Coll Engn, 3450 Lindell Blvd, St Louis, MO 63103 USA.
FU National Science Foundation [1549009, 1644538]; Directorate For
   Engineering; Div Of Industrial Innovation & Partnersh [1549009] Funding
   Source: National Science Foundation; Division Of Research On Learning;
   Direct For Education and Human Resources [1644538] Funding Source:
   National Science Foundation
FX This work is supported by the National Science Foundation, under Grants
   No. 1549009 and No. 1644538.
CR [Anonymous], 2011, THESIS
   Brewster S., 2003, Univ. Access Inf. Soc, V2, P105, DOI [DOI 10.1007/S10209-002-0042-6, 10.1007/s10209-002-0042-6]
   Brown L. M., 2003, P INT C AUD DISPL
   Chua B, 2004, BRIT J OPHTHALMOL, V88, P1119, DOI 10.1136/bjo.2004.041863
   CRAIG JC, 1993, NEUROPSYCHOLOGIA, V31, P277, DOI 10.1016/0028-3932(93)90092-E
   Dandekar K, 2003, J BIOMECH ENG-T ASME, V125, P682, DOI 10.1115/1.1613673
   DAVIDSON PW, 1972, J EXP PSYCHOL, V93, P43, DOI 10.1037/h0032632
   DO- IT, 2012, WORLD WID ACC ACC WE
   Fritz J. P., 1996, P CSUN C TECHN DIS C
   Gentaz E, 2004, PSYCHON B REV, V11, P31, DOI 10.3758/BF03206457
   Giudice N. A., 2012, P 14 INT ACM SIGACCE, P103, DOI [DOI 10.1145/2384916.2384935, 10.1145/2384916.2384935]
   Giudice N. A., 2006, P C ASS TECHN VIS HE
   Giudice NA, 2008, LECT NOTES ARTIF INT, V5248, P121, DOI 10.1007/978-3-540-87601-4_11
   Giudice NA, 2007, PSYCHOL RES-PSYCH FO, V71, P347, DOI 10.1007/s00426-006-0089-8
   Giudice NA, 2011, J EXP PSYCHOL LEARN, V37, P621, DOI 10.1037/a0022331
   Giudice NA, 2009, SPAT COGN COMPUT, V9, P287, DOI 10.1080/13875860903305664
   Goncu C, 2011, LECT NOTES COMPUT SC, V6946, P30, DOI 10.1007/978-3-642-23774-4_5
   Gorlewicz JL, 2014, J SPEC EDUC TECHNOL, V29, P17, DOI 10.1177/016264341402900202
   Grussenmeyer W., 2017, T ACCESS COMPUT, V9
   Immersion, 2015, IMM DEV ZON
   KLATZKY RL, 1985, PERCEPT PSYCHOPHYS, V37, P299, DOI 10.3758/BF03211351
   KLATZKY RL, 1987, J EXP PSYCHOL GEN, V116, P356
   Klatzky RL, 2014, MULTISENS RES, V27, P359, DOI 10.1163/22134808-00002447
   Lederman SJ, 2009, ATTEN PERCEPT PSYCHO, V71, P1439, DOI 10.3758/APP.71.7.1439
   LEDERMAN SJ, 1987, COGNITIVE PSYCHOL, V19, P342, DOI 10.1016/0010-0285(87)90008-9
   Leung R, 2007, ICMI'07: PROCEEDINGS OF THE NINTH INTERNATIONAL CONFERENCE ON MULTIMODAL INTERFACES, P374
   MAGEE LE, 1980, NATURE, V283, P287, DOI 10.1038/283287a0
   Minogue J, 2006, REV EDUC RES, V76, P317, DOI 10.3102/00346543076003317
   Mullenbach J, 2014, 32ND ANNUAL ACM CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2014), P3963, DOI 10.1145/2556288.2557343
   Nyman SR, 2010, BRIT J OPHTHALMOL, V94, P1427, DOI 10.1136/bjo.2009.164814
   O'Modhrain S, 2015, IEEE T HAPTICS, V8, P248, DOI 10.1109/TOH.2015.2466231
   Spence C., 2008, Proceedings of the 22nd British HCI Group Annual Conference on People and Computers: Culture, Creativity, Interaction - Volume 1, V1, P185
   Van Scoy AF, 2006, J MOD OPTIC, V53, P1287, DOI 10.1080/09500340600618652
   Vinter A, 2012, RES DEV DISABIL, V33, P1819, DOI 10.1016/j.ridd.2012.05.001
   Withagen A, 2012, ACTA PSYCHOL, V139, P261, DOI 10.1016/j.actpsy.2011.11.012
   Wolbers T, 2011, CURR BIOL, V21, P984, DOI 10.1016/j.cub.2011.04.038
   Yu W, 2003, 11TH SYMPOSIUM ON HAPTIC INTERFACES FOR VIRTUAL ENVIRONMENT AND TELEOPERATOR SYSTEMS - HAPTICS 2003, PROCEEDINGS, P318
   Yu W, 2001, LECT NOTES COMPUT SC, V2058, P41
NR 38
TC 8
Z9 11
U1 0
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD FEB
PY 2019
VL 16
IS 1
AR 6
DI 10.1145/3301415
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA HN7TC
UT WOS:000460393600006
OA Bronze
DA 2024-07-18
ER

PT J
AU Guo, JJ
   Vidal, V
   Cheng, I
   Basu, A
   Baskurt, A
   Lavoue, G
AF Guo, Jinjiang
   Vidal, Vincent
   Cheng, Irene
   Basu, Anup
   Baskurt, Atilla
   Lavoue, Guillaume
TI Subjective and Objective Visual Quality Assessment of Textured 3D Meshes
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Textured mesh; visual quality assessment; subjective study
ID IMAGE; METRICS; TRANSMISSION; COMPRESSION
AB Objective visual quality assessment of 3D models is a fundamental issue in computer graphics. Quality assessment metrics may allow a wide range of processes to be guided and evaluated, such as level of detail creation, compression, filtering, and so on. Most computer graphics assets are composed of geometric surfaces on which several texture images can be mapped to make the rendering more realistic. While some quality assessment metrics exist for geometric surfaces, almost no research has been conducted on the evaluation of texture-mapped 3D models. In this context, we present a new subjective study to evaluate the perceptual quality of textured meshes, based on a paired comparison protocol. We introduce both texture and geometry distortions on a set of 5 reference models to produce a database of 136 distorted models, evaluated using two rendering protocols. Based on analysis of the results, we propose two new metrics for visual quality assessment of textured mesh, as optimized linear combinations of accurate geometry and texture quality measurements. These proposed perceptual metrics outperform their counterparts in terms of correlation with human opinion. The database, along with the associated subjective scores, will be made publicly available online.
C1 [Guo, Jinjiang; Vidal, Vincent; Baskurt, Atilla; Lavoue, Guillaume] Univ Lyon, CNRS, LIRIS, Lyon, France.
   [Cheng, Irene; Basu, Anup] Univ Alberta, Multimedia Res Ctr, Dept Comp Sci, 114St 89 Av, Edmonton, AB T6G 2E8, Canada.
   [Guo, Jinjiang; Vidal, Vincent; Baskurt, Atilla; Lavoue, Guillaume] INSA Lyon, LIRIS CNRS, Batiment Jules Verne,20 Ave Albert Einstein, F-69621 Villeurbanne, France.
C3 Institut National des Sciences Appliquees de Lyon - INSA Lyon; Centre
   National de la Recherche Scientifique (CNRS); University of Alberta;
   Institut National des Sciences Appliquees de Lyon - INSA Lyon
RP Guo, JJ (corresponding author), Univ Lyon, CNRS, LIRIS, Lyon, France.; Guo, JJ (corresponding author), INSA Lyon, LIRIS CNRS, Batiment Jules Verne,20 Ave Albert Einstein, F-69621 Villeurbanne, France.
EM jinjiang.guo@insa-lyon.fr; vvidal@liris.cnrs.fr; locheng@ualberta.ca;
   basu@ualberta.ca; atilla.baskurt@liris.cnrs.fr; glavoue@liris.cnrs.fr
FU China Scholarship Council; Alberta Innovates Techology Futures
FX We thank Massimiliano Corsini and the Visual Computing Laboratory of
   ISTI-CNR for the Dwarf model, as well as Mark Pauly and the EPFL
   Computer Graphics and Geometry Laboratory for the Squirrel and Statue
   models. This work is supported in part by the China Scholarship Council
   and Alberta Innovates Techology Futures.
CR Aydin TO, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866187
   Cadík M, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366166
   Cadik Martin, 2013, PACIFIC GRAPHICS, V32
   Chandler DM, 2007, IEEE T IMAGE PROCESS, V16, P2284, DOI 10.1109/TIP.2007.901820
   Cheng I, 2007, IEEE T MULTIMEDIA, V9, P386, DOI 10.1109/TMM.2006.886291
   Corsini M, 2013, COMPUT GRAPH FORUM, V32, P101, DOI 10.1111/cgf.12001
   Corsini M, 2007, IEEE T MULTIMEDIA, V9, P247, DOI 10.1109/TMM.2006.886261
   Daly Scott, 1993, P179
   Feng Xiao, 2000, TECHNICAL REPORT
   Ferwerda J. A., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P143, DOI 10.1145/258734.258818
   Garland M., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P209, DOI 10.1145/258734.258849
   Govindarajulu Z., 1992, J AM STAT ASSOC, V34, P108, DOI [DOI 10.2307/2290477, 10.1080/00401706.1992.10485252]
   Griffin W, 2015, IEEE T VIS COMPUT GR, V21, P970, DOI 10.1109/TVCG.2015.2429576
   Guo J, 2015, P ACM SIGGRAPH S APP, P9198
   Herzog R, 2012, COMPUT GRAPH FORUM, V31, P545, DOI 10.1111/j.1467-8659.2012.03055.x
   Karni Z, 2000, COMP GRAPH, P279, DOI 10.1145/344779.344924
   Lavoue G., 2015, Visual Signal Quality Assessment, P243, DOI [10.1007/978-3-319-10368-69, DOI 10.1007/978-3-319-10368-69]
   Lavoué G, 2016, IEEE T VIS COMPUT GR, V22, P1987, DOI 10.1109/TVCG.2015.2480079
   Lavoué G, 2011, COMPUT GRAPH FORUM, V30, P1427, DOI 10.1111/j.1467-8659.2011.02017.x
   Ledda P, 2005, ACM T GRAPHIC, V24, P640, DOI 10.1145/1073204.1073242
   Lubin Jeffrey, 1993, P163
   MANNOS JL, 1974, IEEE T INFORM THEORY, V20, P525, DOI 10.1109/TIT.1974.1055250
   Mantiuk R, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964935
   Mantiuk RK, 2012, COMPUT GRAPH FORUM, V31, P2478, DOI 10.1111/j.1467-8659.2012.03188.x
   Nader G, 2016, IEEE T VIS COMPUT GR, V22, P2423, DOI 10.1109/TVCG.2015.2507578
   OShea J. P., 2008, P S APPL PERC GRAPH
   Pan YX, 2005, IEEE T MULTIMEDIA, V7, P269, DOI 10.1109/TMM.2005.843364
   Qu LJ, 2008, IEEE T VIS COMPUT GR, V14, P1015, DOI 10.1109/TVCG.2008.51
   Rogowitz BE, 2001, P SOC PHOTO-OPT INS, V4299, P340, DOI 10.1117/12.429504
   Seshadrinathan K, 2010, IEEE T IMAGE PROCESS, V19, P1427, DOI 10.1109/TIP.2010.2042111
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P430, DOI 10.1109/TIP.2005.859378
   Silverstein DA, 2001, J ELECTRON IMAGING, V10, P394, DOI 10.1117/1.1344187
   Strom J., 2005, ACM SIGGRAPH EUROGRA, P177
   Sun J, 1998, NAT NEUROSCI, V1, P183, DOI 10.1038/630
   Taubin G., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P351, DOI 10.1145/218380.218473
   Thurstone LL, 1927, PSYCHOL REV, V34, P273, DOI 10.1037/h0070288
   Tian DH, 2008, IEEE T CIRC SYST VID, V18, P23, DOI 10.1109/TCSVT.2007.903319
   Tian Dihong, 2004, P 12 ANN ACM INT C M
   Torkhani Fakhri, 2012, P INT C COMP VIS GRA
   Vása L, 2012, COMPUT GRAPH FORUM, V31, P1715, DOI 10.1111/j.1467-8659.2012.03176.x
   Wang K, 2012, COMPUT GRAPH-UK, V36, P808, DOI 10.1016/j.cag.2012.06.004
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang Z, 2011, IEEE T IMAGE PROCESS, V20, P1185, DOI 10.1109/TIP.2010.2092435
   Watson B, 2001, COMP GRAPH, P213, DOI 10.1145/383259.383283
   Yang S., 2004, Proc. ACM Multimedia, P676, DOI 10.1145/1027527.1027683
   Yeganeh H, 2013, IEEE T IMAGE PROCESS, V22, P657, DOI 10.1109/TIP.2012.2221725
   Zhang L, 2012, IEEE IMAGE PROC, P1477, DOI 10.1109/ICIP.2012.6467150
   Zhou Wang, 2006, MODERN IMAGE QUALITY, V2
NR 49
TC 37
Z9 40
U1 0
U2 10
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD FEB
PY 2017
VL 14
IS 2
AR 11
DI 10.1145/2996296
PG 20
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA EM8VC
UT WOS:000395588200004
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Bernardo, MV
   Pinheiro, AMG
   Fiadeiro, PT
   Pereira, M
AF Bernardo, Marco V.
   Pinheiro, Antonio M. G.
   Fiadeiro, Paulo T.
   Pereira, Manuela
TI Image Quality under Chromatic Impairments
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Quality of experience; mean opinion score; quality metrics; image
   quality
ID NATURAL SCENE STATISTICS; FIDELITY-CRITERION; INFORMATION; BLUR
AB The influence of chromatic impairments on the perceived image quality is studied in this article. Under the D65 standard illuminant, a set of hyperspectral images were represented into the CIELAB color space, and the corresponding chromatic coordinates were subdivided into clusters with the k-means algorithm. Each color cluster was shifted by a predefined chromatic impairment Delta E*(ab) with random direction in a*b* chromatic coordinates only. Applying impairments of 3, 6, 9, 12, and 15 in a*b* coordinates to five hyperspectral images a set of modified images was generated. Those images were shown to subjects that were asked to rank their quality based on their naturalness. The Mean Opinion Score of the subjective evaluations was computed to quantify the sensitivity to the chromatic variations. This article is also complemented with an objective evaluation of the quality using several state-of-the-art metrics and using the CIEDE2000 color difference among others. Analyzing the correlations between subjective and objective quality evaluation helps us to conclude that the proposed quality estimators based on the CIEDE2000 provide the best representation. Moreover, it was concluded that the established quality metrics only become reliable by averaging their results on each color component.
C1 [Bernardo, Marco V.; Pinheiro, Antonio M. G.; Pereira, Manuela] Univ Beira Interior, IT, P-6201001 Covilha, Portugal.
   [Bernardo, Marco V.; Pinheiro, Antonio M. G.; Fiadeiro, Paulo T.] Univ Beira Interior, Opt Ctr, P-6201001 Covilha, Portugal.
   [Fiadeiro, Paulo T.] Univ Beira Interior, Fiber Mat & Environm Technol FibEnTech, P-6201001 Covilha, Portugal.
C3 Instituto de Telecomunicacoes; Universidade da Beira Interior;
   Universidade da Beira Interior; Universidade da Beira Interior
RP Bernardo, MV (corresponding author), Univ Beira Interior, IT, P-6201001 Covilha, Portugal.; Bernardo, MV (corresponding author), Univ Beira Interior, Opt Ctr, P-6201001 Covilha, Portugal.
EM mbernardo@ubi.pt; pinheiro@ubi.pt; fiadeiro@ubi.pt; mpereira@ubi.pt
RI Pereira, Manuela/Q-3456-2019; Bernardo, Marco v./HNQ-4992-2023;
   Fiadeiro, Paulo T/J-2017-2012; Pinheiro, Antonio/B-2723-2012
OI Pereira, Manuela/0000-0002-8648-6464; Bernardo, Marco
   v./0000-0003-0046-8685; Fiadeiro, Paulo T/0000-0002-7374-3636; Pinheiro,
   Antonio/0000-0002-5968-9901
FU Instituto de Telecomunicacoes-Fundacao para a Ciencia e a Tecnologia
   [UID/EEA/50008/2013]; Remote Sensing Unit [PEst-OE- FIS/UI0524/2014];
   Fundacao para a Ciencia e a Tecnologia [PTDC-AUR-URB-113635-2009];
   Fundação para a Ciência e a Tecnologia [PTDC/AUR-URB/113635/2009,
   PEst-OE/FIS/UI0524/2014] Funding Source: FCT
FX The authors are very grateful to the Instituto de
   Telecomunicacoes-Fundacao para a Ciencia e a Tecnologia (project
   UID/EEA/50008/2013), to the Remote Sensing Unit (project PEst-OE-
   FIS/UI0524/2014), to the Fundacao para a Ciencia e a Tecnologia (project
   PTDC-AUR-URB-113635-2009), and to the Optics Center of Universidade da
   Beira Interior where this work has been conducted.
CR Aja-Fernandez Santiago, 2006, Conf Proc IEEE Eng Med Biol Soc, V2006, P4815
   Aldaba MA, 2006, VISUAL NEUROSCI, V23, P555, DOI 10.1017/S0952523806233467
   [Anonymous], REPROD COLOUR
   [Anonymous], 2004, WAVEFRONT CUSTOMIZED
   [Anonymous], 2005, Digital Video Quality: Vision Models and Metrics
   [Anonymous], 2004, CIE PUBLICATION
   Batten Christopher F., 2000, THESIS
   Bernardo Marco V., 2012, P 20 ACM INT C MULT, P1009
   BERNARDO MV, 2013, QUAL MULT EXP QOMEX, P1
   BRAINARD DH, 1989, COLOR RES APPL, V14, P23, DOI 10.1002/col.5080140107
   Chandler DM, 2007, IEEE T IMAGE PROCESS, V16, P2284, DOI 10.1109/TIP.2007.901820
   CIE, 1986, CIE PUBLICATION
   CIE Industrial Colour Difference Evaluation, 1995, CIE PUBLICATION, V116-1995
   CLARKE FJJ, 1984, J SOC DYERS COLOUR, V100, P128
   Deng Beixing, 2006, Wuhan University Journal of Natural Sciences, V11, P370, DOI 10.1007/BF02832124
   Egiazarian K., 2006, Proceedings of the second international workshop on video processing and quality metrics, P1
   ERASMUS SJ, 1982, J MICROSC-OXFORD, V127, P185, DOI 10.1111/j.1365-2818.1982.tb00412.x
   Ferzli R, 2009, IEEE T IMAGE PROCESS, V18, P717, DOI 10.1109/TIP.2008.2011760
   Ferzli Rony, 2007, 3 INT WORKSH VID PRO
   Ferzli Rony, 2005, P 1 INT WORKSH VID P
   Fiadeiro P., 2005, Proceedings of the 10th Congress of the AIC, P1191
   FIRESTONE L, 1991, CYTOMETRY, V12, P195, DOI 10.1002/cyto.990120302
   Forsyth D., 2011, Computer Vision: A Modern Approach
   Foster DH, 2004, VISUAL NEUROSCI, V21, P331, DOI 10.1017/S0952523804213335
   Gibbons JD, 2014, Nonparametric Statistical Inference
   Hanhart P, 2015, EURASIP J IMAGE VIDE, DOI 10.1186/s13640-015-0091-4
   ITU-T Tutorial, 2004, TECHNICAL REPORT
   Johnson GM, 2001, NINTH COLOR IMAGING CONFERENCE: COLOR SCIENCE AND ENGINEERING SYSTEMS, TECHNOLOGIES, APPLICATIONS, P108
   Katoh N, 1996, FOURTH COLOR IMAGING CONFERENCE: COLOR SCIENCE, SYSTEMS AND APPLICATIONS, P126
   KRUSKAL WH, 1952, J AM STAT ASSOC, V47, P583, DOI 10.1080/01621459.1952.10483441
   Laparra V, 2010, J OPT SOC AM A, V27, P852, DOI 10.1364/JOSAA.27.000852
   Larson EC, 2010, J ELECTRON IMAGING, V19, DOI 10.1117/1.3267105
   Li SN, 2011, IEEE T MULTIMEDIA, V13, P935, DOI 10.1109/TMM.2011.2152382
   Long F, 2003, P NATL ACAD SCI USA, V100, P15190, DOI 10.1073/pnas.2036361100
   Luo MR, 2001, COLOR RES APPL, V26, P340, DOI 10.1002/col.1049
   Linhares JMM, 2008, J OPT SOC AM A, V25, P2918, DOI 10.1364/JOSAA.25.002918
   MANNOS JL, 1974, IEEE T INFORM THEORY, V20, P525, DOI 10.1109/TIT.1974.1055250
   Mantiuk R, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964935
   Mantiuk RK, 2012, COMPUT GRAPH FORUM, V31, P2478, DOI 10.1111/j.1467-8659.2012.03188.x
   Marziliano P, 2004, SIGNAL PROCESS-IMAGE, V19, P163, DOI 10.1016/j.image.2003.08.003
   Mitsa T., 1993, ICASSP-93. 1993 IEEE International Conference on Acoustics, Speech, and Signal Processing (Cat. No.92CH3252-4), P301, DOI 10.1109/ICASSP.1993.319807
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Mollon J. D., 1989, J PHYSL, V414
   Moorthy AK, 2011, IEEE T IMAGE PROCESS, V20, P3350, DOI 10.1109/TIP.2011.2147325
   Murthy Adithya V., 2010, Proceedings of the 2010 Second International Workshop on Quality of Multimedia Experience (QoMEX 2010), P242, DOI 10.1109/QOMEX.2010.5516091
   Narvekar ND, 2011, IEEE T IMAGE PROCESS, V20, P2678, DOI 10.1109/TIP.2011.2131660
   Nascimento SMC, 2002, J OPT SOC AM A, V19, P1484, DOI 10.1364/JOSAA.19.001484
   Parraga C., 1998, MEASUREMENT, V15, P1
   Pedersen M., 2009, Journal of Vision, V9, P343
   Pinson M., 2003, P SPIE VID COMM IM P
   POINTER MR, 1980, COLOR RES APPL, V5, P145, DOI 10.1002/col.5080050308
   Ponomarenko N., 2007, P 4 INT WORKSH VID P
   Reichl Peter, 2008, P ITC SPEC SEM Q EXP
   Ruderman DL, 1998, J OPT SOC AM A, V15, P2036, DOI 10.1364/JOSAA.15.002036
   Saad MA, 2012, IEEE T IMAGE PROCESS, V21, P3339, DOI 10.1109/TIP.2012.2191563
   Shaked D, 2005, IEEE IMAGE PROC, P841
   SHAPIRO SS, 1965, BIOMETRIKA, V52, P591, DOI 10.1093/biomet/52.3-4.591
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P430, DOI 10.1109/TIP.2005.859378
   Sheikh HR, 2005, IEEE T IMAGE PROCESS, V14, P2117, DOI 10.1109/TIP.2005.859389
   Shnayderman A, 2006, IEEE T IMAGE PROCESS, V15, P422, DOI 10.1109/TIP.2005.860605
   Smith T., 1932, Transactions of the Optical Society, V33, P73
   Spearman C, 1910, BRIT J PSYCHOL, V3, P271, DOI 10.1111/j.2044-8295.1910.tb00206.x
   VQEG, 2000, PHAS 2 VQEG
   WALLACE WE, 1991, P SOC PHOTO-OPT INS, V1460, P20, DOI 10.1117/12.44407
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang Z, 2002, IEEE SIGNAL PROC LET, V9, P81, DOI 10.1109/97.995823
   Wang Z, 2011, IEEE T IMAGE PROCESS, V20, P1185, DOI 10.1109/TIP.2010.2092435
   Winkler S, 2003, PROC SPIE, V5007, P104, DOI 10.1117/12.477766
   WINTRINGHAM WT, 1951, P IRE, V39, P1135, DOI 10.1109/JRPROC.1951.273777
   Wu JJ, 2013, IEEE T IMAGE PROCESS, V22, P43, DOI 10.1109/TIP.2012.2214048
   Wyszecki G., 1967, Color Science
   Xue WF, 2014, IEEE T IMAGE PROCESS, V23, P684, DOI 10.1109/TIP.2013.2293423
   Yang XK, 2005, SIGNAL PROCESS-IMAGE, V20, P662, DOI 10.1016/j.image.2005.04.001
   You JY, 2010, SIGNAL PROCESS-IMAGE, V25, P482, DOI 10.1016/j.image.2010.02.002
   Zhang L, 2011, IEEE T IMAGE PROCESS, V20, P2378, DOI 10.1109/TIP.2011.2109730
   Zhang L, 2010, IEEE IMAGE PROC, P321, DOI 10.1109/ICIP.2010.5649275
   Zhang N., 2003, Proceedings of Section of Physical and Engineering Sciences of American Statistical Society, P4730
   Zhang X., 1997, Journal of the Society for Information Display, V5, P61, DOI 10.1889/1.1985127
NR 80
TC 5
Z9 5
U1 0
U2 5
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD AUG
PY 2016
VL 14
IS 1
AR 6
DI 10.1145/2964908
PG 20
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DV4EB
UT WOS:000382876900006
DA 2024-07-18
ER

PT J
AU Kaaresoja, T
   Brewster, S
   Lantz, V
AF Kaaresoja, Topi
   Brewster, Stephen
   Lantz, Vuokko
TI Towards the Temporally Perfect Virtual Button: Touch-Feedback
   Simultaneity and Perceived Quality in Mobile Touchscreen Press
   Interactions
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Human Factors; Temporal perception; simultaneity; touch; feedback;
   mobile device; touchscreen; tactile; audio
ID DELAYS
AB Pressing a virtual button is still the major interaction method in touchscreen mobile phones. Although phones are becoming more and more powerful, operating system software is getting more and more complex, causing latency in interaction. We were interested in gaining insight into touch-feedback simultaneity and the effects of latency on the perceived quality of touchscreen buttons. In an experiment, we varied the latency between touch and feedback between 0 and 300 ms for tactile, audio, and visual feedback modalities. We modelled the proportion of simultaneity perception as a function of latency for each modality condition. We used a Gaussian model fitted with the maximum likelihood estimation method to the observations. These models showed that the point of subjective simultaneity (PSS) was 5ms for tactile, 19ms for audio, and 32ms for visual feedback. Our study included the scoring of perceived quality for all of the different latency conditions. The perceived quality dropped significantly between latency conditions 70 and 100 ms when the feedback modality was tactile or audio, and between 100 and 150 ms when the feedback modality was visual. When the latency was 300ms for all feedback modalities, the quality of the buttons was rated significantly lower than in all of the other latency conditions, suggesting that a long latency between a touch on the screen and feedback is problematic for users. Together with PSS and these quality ratings, a 75% threshold was established to define a guideline for the recommended latency range between touch and feedback. Our guideline suggests that tactile feedback latency should be between 5 and 50 ms, audio feedback latency between 20 and 70 ms, and visual feedback latency between 30 and 85 ms. Using these values will ensure that users will perceive the feedback as simultaneous with the finger's touch. These values also ensure that the users do not perceive reduced quality. These results will guide engineers and designers of touchscreen interactions by showing the trade-offs between latency and user preference and the effects that their choices might have on the quality of the interactions and feedback they design.
C1 [Kaaresoja, Topi; Lantz, Vuokko] Nokia Res Ctr, Espoo 02150, Finland.
   [Brewster, Stephen] Univ Glasgow, Glasgow G12 8RZ, Lanark, Scotland.
C3 Nokia Corporation; Siemens AG; Nokia Siemens Networks; Nokia Finland;
   University of Glasgow
RP Kaaresoja, T (corresponding author), Nokia Res Ctr, Otaniementie 19, Espoo 02150, Finland.
EM topi.kaaresoja@nokia.com; stephen@dcs.gla.ac.uk; vuokko.lantz@nokia.com
RI brewster, stephen/J-9003-2017
OI brewster, stephen/0000-0001-9720-3899
CR Adelstein B., 2003, P 5 INT C MULTIMODAL, P73, DOI [DOI 10.1145/958432.958448, 10.1145/958432.958448]
   [Anonymous], 2010, SPACE TIME PERCEPTIO, DOI [DOI 10.1017/CB09780511750540.015, DOI 10.1017/CBO9780511750540.015]
   BORING E, 1923, HIST EXPT PSYCHOL
   Brewster S, 2007, P SIGCHI C HUM FACT
   Brewster S, 2002, PERS UBIQUIT COMPUT, V6, P188, DOI 10.1007/s007790200019
   Coren Stanley., 2003, SENSATION PERCEPTION, V5th
   Exner S, 1875, PFLUGERS ARCH GESAMT, V11, P403, DOI [DOI 10.1007/BF01659311, 10.1007/BF01659311]
   Fujisaki W, 2004, NAT NEUROSCI, V7, P773, DOI 10.1038/nn1268
   Fukumoto M., 2001, CHI 01 EXTENDED ABST, P121, DOI DOI 10.1145/634067.634141
   Harrar V, 2005, EXP BRAIN RES, V166, P465, DOI 10.1007/s00221-005-2386-7
   He D., 2000, P IPT2000 INT IMM PR
   Hoggan E, 2008, CHI 2008: 26TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS VOLS 1 AND 2, CONFERENCE PROCEEDINGS, P1573
   Jay C, 2005, World Haptics Conference: First Joint Eurohaptics Conference and Symposium on Haptic Interfaces for Virutual Environment and Teleoperator Systems, Proceedings, P655
   Jota Ricardo, 2013, P CHI 2013 C HUMAN F, P2291, DOI [DOI 10.1145/2470654.2481317, 10.1145/2470654.2481317]
   Kaaresoja T., 2011, 2011 IEEE World Haptics Conference (WHC 2011), P65, DOI 10.1109/WHC.2011.5945463
   Kaaresoja T., 2006, Proceedings of Eurohaptics, P565
   Kaaresoja T, 2010, P ICMI MLMI 2010
   Kaaresoja T, 2011, LECT NOTES COMPUT SC, V6947, P554, DOI 10.1007/978-3-642-23771-3_42
   Levitin D. J., 1999, P 3 INT C COMP ANT S
   MACKENZIE IS, 1993, HUMAN FACTORS IN COMPUTING SYSTEMS, P488
   Miall RC, 2006, EXP BRAIN RES, V172, P77, DOI 10.1007/s00221-005-0306-5
   Millar RB, 2011, STAT PRACT, P1, DOI 10.1002/9780470094846
   Miller D., 2002, P SPIE
   MILLER GA, 1956, PSYCHOL REV, V63, P81, DOI 10.1037/h0043158
   Ng A, 2012, UIST'12: PROCEEDINGS OF THE 25TH ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P453
   Poupyrev I., 2004, CHI 04 HUM FACT COMP, P1309, DOI DOI 10.1145/985921.986051
   Poupyrev I., 2003, UIST 03, P217, DOI DOI 10.1145/964696.964721
   Stone JV, 2001, P ROY SOC B-BIOL SCI, V268, P31, DOI 10.1098/rspb.2000.1326
   Tukey J.W., 1977, EXPLORATORY DATA ANA, V2
   Vogels IMLC, 2004, HUM FACTORS, V46, P118, DOI 10.1518/hfes.46.1.118.30394
   Winter R, 2008, BRAIN RES, V1242, P54, DOI 10.1016/j.brainres.2008.06.090
   Zampini M, 2005, PERCEPT PSYCHOPHYS, V67, P531, DOI 10.3758/BF03193329
NR 32
TC 47
Z9 48
U1 2
U2 22
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD MAY
PY 2014
VL 11
IS 2
AR 9
DI 10.1145/2611387
PG 25
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AO3OJ
UT WOS:000341241100005
DA 2024-07-18
ER

PT J
AU Wijntjes, MWA
   Pont, SC
AF Wijntjes, Maarten W. A.
   Pont, Sylvia C.
TI Pointing in Pictorial Space: Quantifying the Perceived Relative Depth
   Structure in Mono and Stereo Images of Natural Scenes
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Experimentation; Human Factors; Measurement; Theory; Depth perception;
   binocular disparity; natural scenes
ID PERCEPTION; TEXTURE
AB Although there has recently been a large increase in commercial 3D applications, relatively little is known about the quantitative perceptual improvement from binocular disparity. In this study we developed a method to measure the perceived relative depth structure of natural scenes. Observers were instructed to adjust the direction of a virtual pointer from one object to another. The pointing data was used to reconstruct the relative logarithmic depths of the objects in pictorial space. The results showed that the relative depth structure is more similar between observers for stereo images than for mono images in two out of three scenes. A similar result was found for the depth range: for the same two scenes the stereo images were perceived as having more depth than the monocular images. In addition, our method allowed us to determine the subjective center of projection. We found that the pointing settings fitted the reconstructed depth best for substantially wider fields of view than the veridical center of projection for both mono and stereo images. The results indicate that the improvement from binocular disparity depends on the scene content: scenes with sufficient monocular information may not profit much from binocular disparity.
C1 [Wijntjes, Maarten W. A.; Pont, Sylvia C.] Delft Univ Technol, Perceptual Intelligence Lab, NL-2628 CE Delft, Netherlands.
C3 Delft University of Technology
RP Wijntjes, MWA (corresponding author), Delft Univ Technol, Perceptual Intelligence Lab, Landbergstr 15, NL-2628 CE Delft, Netherlands.
EM m.w.a.wijntjes@tudelft.nl
OI Wijntjes, Maarten/0000-0002-3657-735X; Pont, Sylvia/0000-0002-9834-9600
FU Netherlands Organisation of Scientific Research (NWO)
FX This work was supported by a grant from the Netherlands Organisation of
   Scientific Research (NWO).
CR Hillis JM, 2004, J VISION, V4, P967, DOI 10.1167/4.12.1
   Hoffman DM, 2008, J VISION, V8, DOI 10.1167/8.3.33
   Knill DC, 2003, VISION RES, V43, P2539, DOI 10.1016/S0042-6989(03)00458-9
   KOENDERINK JJ, 1992, PERCEPT PSYCHOPHYS, V52, P487, DOI 10.3758/BF03206710
   Koenderink JJ, 2001, PERCEPTION, V30, P431, DOI 10.1068/p3030
   NORMAN JF, 1995, PERCEPT PSYCHOPHYS, V57, P629
   Seuntiens P., 2005, P SPIE, V6016
   Todd JT, 1996, J EXP PSYCHOL HUMAN, V22, P695
   Wheatstone Charles, 1838, Philosophical Transactions of the Royal Society of London, V128, P371
NR 9
TC 3
Z9 3
U1 0
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD JUL
PY 2010
VL 7
IS 4
AR 24
DI 10.1145/1823738.1823742
PG 8
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA 633ZI
UT WOS:000280546500004
DA 2024-07-18
ER

PT J
AU Mania, K
   Badariah, S
   Coxon, M
   Watten, P
AF Mania, Katerina
   Badariah, Shahrul
   Coxon, Matthew
   Watten, Phil
TI Cognitive Transfer of Spatial Awareness States from Immersive Virtual
   Environments to Reality
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Design; Experimentation; Human Factors; Perceptual graphics;
   human-computer interaction
ID MEMORY AWARENESS; RECOGNITION; FIDELITY
AB An individual's prior experience will influence how new visual information in a scene is perceived and remembered. Accuracy of memory performance per se is an imperfect reflection of the cognitive activity (awareness states) that underlies performance in memory tasks. The aim of this research is to investigate the effect of varied visual fidelity of training environments on the transfer of training to the real world after exposure to immersive simulations representing a real-world scene. A between groups experiment was carried out to explore the effect of rendering quality on measurements of location-based recognition memory for objects and associated states of awareness. The immersive simulation consisted of one room that was either rendered flat-shaded or using radiosity rendering. The simulation was displayed on a stereo head-tracked head mounted display. Post exposure to the synthetic simulation, participants completed a memory recognition task conducted in a real-world scene by physically arranging objects in their physical form in a real-world room. Participants also reported one of four states of awareness following object recognition. They were given several options of awareness states that reflected the level of visual mental imagery involved during retrieval, the familiarity of the recollection and related guesses. The scene incorporated objects that "fitted" into the specific context of the real-world scene, referred to as consistent objects, and objects that were not related to the specific context of the real-world scene, referred to as inconsistent objects. A follow-up study was conducted a week after the initial test. Interestingly, results revealed a higher proportion of correct object recognition associated with mental imagery when participants were exposed to low-fidelity flat-shaded training scenes rather than the radiosity rendered ones. Memory psychology indicates that awareness states based on visual imagery require stronger attentional processing in the first instance than those based on familiarity. A tentative claim would, therefore, be that those immersive environments that are distinctive because of their variation from "real," such as flat-shaded environments, recruit stronger attentional resources. This additional attentional processing may bring about a change in participants' subjective experiences of "remembering" when they later transfer the training from that environment into a real-world situation.
C1 [Mania, Katerina] Tech Univ Crete, Dept Elect & Comp Engn, Khania 73100, Crete, Greece.
   [Mania, Katerina; Badariah, Shahrul; Watten, Phil] Univ Sussex, Brighton BN1 9QJ, E Sussex, England.
   [Coxon, Matthew] York St John Univ, York YO31 7EX, N Yorkshire, England.
C3 Technical University of Crete; University of Sussex; York Saint John
   University
RP Mania, K (corresponding author), Tech Univ Crete, Dept Elect & Comp Engn, Khania 73100, Crete, Greece.
EM k.mania@ced.tuc.gr
RI Mania, Katerina/AAO-7013-2021
OI Coxon, Matthew/0000-0001-5882-0966
CR BAILEY JH, 1994, HUM FAC ERG SOC P, P1158, DOI 10.1177/154193129403801803
   Billinghurst M., 1995, Proceedings. Virtual Reality Annual International Symposium '95 (Cat. No.95CH35761), P40, DOI 10.1109/VRAIS.1995.512478
   Bliss JP, 1997, PRESENCE-TELEOP VIRT, V6, P73, DOI 10.1162/pres.1997.6.1.73
   Brandt KR, 2003, MEMORY, V11, P89, DOI 10.1080/741938169
   BRANDT KR, 2006, BR J PSYCH, P269
   BREWER WF, 1981, COGNITIVE PSYCHOL, V13, P207, DOI 10.1016/0010-0285(81)90008-6
   Conway MA, 1997, J EXP PSYCHOL GEN, V126, P393, DOI 10.1037/0096-3445.126.4.393
   Dewhurst SA, 2006, CONSCIOUS COGN, V15, P147, DOI 10.1016/j.concog.2005.05.002
   DIHN HO, 1999, P IEEE VR 1999, P222
   Fink PW, 2007, ACM T APPL PERCEPT, V4, DOI 10.1145/1227134.1227136
   Frenz H, 2007, ACM T APPL PERCEPT, V4, DOI 10.1145//1227134./1227137
   GARDINER JM, 1992, HDB MEMORY
   Hollingworth A, 1998, J EXP PSYCHOL GEN, V127, P398, DOI 10.1037/0096-3445.127.4.398
   KORIAT A, 1994, J EXP PSYCHOL GEN, V123, P297, DOI 10.1037/0096-3445.123.3.297
   Lathrop WB, 2002, PRESENCE-TELEOP VIRT, V11, P19, DOI 10.1162/105474602317343631
   Mania K, 2006, IEEE T VIS COMPUT GR, V12, P396, DOI 10.1109/TVCG.2006.55
   Mania K, 2003, PRESENCE-TELEOP VIRT, V12, P296, DOI 10.1162/105474603765879549
   Mania K, 2001, CYBERPSYCHOL BEHAV, V4, P247, DOI 10.1089/109493101300117938
   Mania Katerina., 2004, Proceedings of the 1st Symposium on Applied Perception in Graphics and Visualization, APGV '04, P39, DOI [10.1145/1012551.1012559, DOI 10.1145/1012551.1012559]
   MINSKY M., 1974, FRAMEWORK REPRESENTI
   Mourkoussis N, 2010, ACM T APPL PERCEPT, V8, DOI 10.1145/1857893.1857895
   Parkin AJ, 1995, CONSCIOUS COGN, V4, P387, DOI 10.1006/ccog.1995.1046
   PICHERT JW, 1966, J EDUC PSYCHOL, V69, P309
   Schank R.C., 1999, Dynamic memory revisited
   Travis D., 1991, Computers and people series
   Tulving E., 1992, ELEMENTS EPISODIC ME
   Williams B, 2007, ACM T APPL PERCEPT, V4, DOI 10.1145/1265957.1265961
   ZIMMONS P, 2005, THESIS
NR 28
TC 22
Z9 24
U1 4
U2 16
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD FEB
PY 2010
VL 7
IS 2
AR 9
DI 10.1145/1670671.1670673
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA 563HF
UT WOS:000275118100002
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Canosa, RL
AF Canosa, Roxanne L.
TI Real-World Vision: Selective Perception and Task
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Experimentation; Algorithms; Active vision; saliency modeling;
   eye-tracking
ID SACCADIC EYE-MOVEMENTS; VISUAL-ATTENTION; REPRESENTATION; RECOGNITION;
   STRATEGIES; SALIENCE; FEATURES; COMPLEX; MEMORY; MODEL
AB Visual perception is an inherently selective process. To understand when and why a particular region of a scene is selected, it is imperative to observe and describe the eye movements of individuals as they go about performing specific tasks. In this sense, vision is an active process that integrates scene properties with specific, goal-oriented oculomotor behavior. This study is an investigation of how task influences the visual selection of stimuli from a scene. Four eye tracking experiments were designed and conducted to determine how everyday tasks affect oculomotor behavior. A portable eyetracker was created for the specific purpose of bringing the experiments out of the laboratory and into the real world, where natural behavior is most likely to occur. The experiments provide evidence that the human visual system is not a passive collector of salient environemental stimuli, nor is vision general-purpose. Rather, vision is active and specific, tightly coupled to the requirements of a task and a plan of action. The experiments support the hypothesis that the purpose of selective attention is to maximize task efficiency by fixating relevant objects in the scene. A computational model of visual attention is presented that imposes a high-level constraint on the bottom-up salient properties of a scene for the purpose of locating regions that are likely to correspond to foreground objects rather than background or other salient nonobject stimuli. In addition to improving the correlation to human subject fixation densities over a strictly bottom-up model [Itti et al. 1998; Parkhurst et al. 2002], this model predicts a central fixation tendency when that tendency is warranted, and not as an artificially primed location bias.
C1 Rochester Inst Technol, Dept Comp Sci, Rochester, NY 14623 USA.
C3 University of Rochester; Rochester Institute of Technology
RP Canosa, RL (corresponding author), Rochester Inst Technol, Dept Comp Sci, Rochester, NY 14623 USA.
EM rlc@cs.rit.edu
CR Andrews TJ, 1999, VISION RES, V39, P2947, DOI 10.1016/S0042-6989(99)00019-X
   [Anonymous], 1935, How people look at pictures: A study of the psychology and perception in art
   [Anonymous], 1921, Visuell wahrgenommene figuren: Studien in psychologischer analyse
   [Anonymous], 2005, Neurobiology of attention
   [Anonymous], 2003, Active vision: The psychology of looking and seeing
   ARRINGTON KF, 2000, HIST EYE TRACKING ME
   BALLARD DH, 1995, J COGNITIVE NEUROSCI, V7, P66, DOI 10.1162/jocn.1995.7.1.66
   BONAIUTO JJ, 2005, P IEEE CVPR WORKSH A, P1
   Bruce N., 2006, P ADV NEUR INF PROC, P155
   BURT PJ, 1983, IEEE T COMMUN, V31, P532, DOI 10.1109/TCOM.1983.1095851
   Canny J., 1986, IEEE T PATTERN ANAL, V8, P769, DOI DOI 10.1109/TPAMI.1986.4767851
   Canosa R, 2005, INT J ARTIF INTELL T, V14, P233, DOI 10.1142/S0218213005002089
   Carmi R, 2006, VISION RES, V46, P4333, DOI 10.1016/j.visres.2006.08.019
   CARPENTER DHS, 1988, MOVEMENTS EYES
   Chapman D., 1991, VISION INSTRUCTION A
   DESIMONE R, 1995, ANNU REV NEUROSCI, V18, P193, DOI 10.1146/annurev-psych-122414-033400
   Egeth HE, 1997, ANNU REV PSYCHOL, V48, P269, DOI 10.1146/annurev.psych.48.1.269
   Epelboim J, 2001, VISION RES, V41, P1561, DOI 10.1016/S0042-6989(00)00256-X
   Fairchild M.D., 1998, COLOR APPEARANCE MOD
   Fecteau JH, 2006, TRENDS COGN SCI, V10, P382, DOI 10.1016/j.tics.2006.06.011
   Gabor D., 1946, Journal of the Institution of Electrical Engineers-part III: radio and communication engineering, V93, P429, DOI [DOI 10.1049/JI-3-2.1946.0074, 10.1049/ji-3-2.1946.0074]
   Godijn R, 2002, J EXP PSYCHOL HUMAN, V28, P1039, DOI 10.1037//0096-1523.28.5.1039
   Goldberg ME., 1991, principles of neuronal science, P660
   Gottlieb JP, 1998, NATURE, V391, P481, DOI 10.1038/35135
   HARNAD S, 1990, PHYSICA D, V42, P335, DOI 10.1016/0167-2789(90)90087-6
   Hayhoe M, 2000, VIS COGN, V7, P43, DOI 10.1080/135062800394676
   HAYHOE MM, 1994, P WORKSH VIS BEH
   Hebb D.O., 1949, ORG BEHAV NEUROPSYCH
   HUBEL DH, 1968, J PHYSIOL-LONDON, V195, P215, DOI 10.1113/jphysiol.1968.sp008455
   Hunt R.W.G., 1995, The Reproduction of Color
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Ivanov YA, 2000, IEEE T PATTERN ANAL, V22, P852, DOI 10.1109/34.868686
   KAUFMAN L, 1969, PERCEPT PSYCHOPHYS, V5, P85, DOI 10.3758/BF03210527
   KOWLER E, 1995, VISUAL COGNITION, V39
   Land M, 1999, PERCEPTION, V28, P1311, DOI 10.1068/p2935
   LEMEUR O, 2005, T PATT ANAL MACH INT, V28, P802
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   MAGEE D, 2004, P WORKSH ANCH SYMB S, V4, P17
   MANNOS JL, 1974, IEEE T INFORM THEORY, V20, P525, DOI 10.1109/TIT.1974.1055250
   MILANESE R, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P781, DOI 10.1109/CVPR.1994.323898
   MORAN J, 1985, SCIENCE, V229, P782, DOI 10.1126/science.4023713
   MORGAN MJ, 1990, VISION RES, V30, P1793, DOI 10.1016/0042-6989(90)90160-M
   Navalpakkam V, 2006, J VISION, V6, P1180, DOI 10.1167/6.11.4
   Navalpakkam V, 2007, NEURON, V53, P605, DOI 10.1016/j.neuron.2007.01.018
   Navalpakkam Vidhya, 2002, LECT NOTES COMPUTER
   NORTHDURFT HC, 2005, NEUROBIOL ATTENTION, P233
   NOTON D, 1971, VISION RES, V11, P929, DOI 10.1016/0042-6989(71)90213-6
   Palmer S., 1999, VISION SCI PHOTONS P
   Park RM, 2002, AM J IND MED, V42, P1, DOI 10.1002/ajim.10082
   Pattanaik S.N., 1998, P SIGGRAPH 98, P287
   Pelz JB, 2001, VISION RES, V41, P3587, DOI 10.1016/S0042-6989(01)00245-0
   Peters R.J., 2007, P IEEE C COMPUTER VI, P1
   Peters RJ, 2005, VISION RES, V45, P2397, DOI 10.1016/j.visres.2005.03.019
   Peters RJ, 2008, ACM T APPL PERCEPT, V5, DOI 10.1145/1279920.1279923
   POSNER MI, 1980, Q J EXP PSYCHOL, V32, P3, DOI 10.1080/00335558008248231
   Privitera CM, 2000, IEEE T PATTERN ANAL, V22, P970, DOI 10.1109/34.877520
   Rayner K., 1983, Eye Movements in Reading
   Rensink RA, 1995, PERCEPTION, V24, P26
   Rensink RA, 2000, VIS COGN, V7, P17, DOI 10.1080/135062800394667
   Rensink RA, 2000, VISION RES, V40, P1469, DOI 10.1016/S0042-6989(00)00003-1
   SANTOS P, 2004, P 16 EUR C ART INT, V16, P544
   Tatler BW, 2005, VISION RES, V45, P643, DOI 10.1016/j.visres.2004.09.017
   Theeuwes J, 2004, PSYCHON B REV, V11, P65, DOI 10.3758/BF03206462
   Torralba A, 2003, J OPT SOC AM A, V20, P1407, DOI 10.1364/JOSAA.20.001407
   Torralba A, 2006, PSYCHOL REV, V113, P766, DOI 10.1037/0033-295X.113.4.766
   TREISMAN AM, 1980, COGNITIVE PSYCHOL, V12, P97, DOI 10.1016/0010-0285(80)90005-5
   ULLMAN S, 1984, COGNITION, V18, P97, DOI 10.1016/0010-0277(84)90023-4
   van Zoest W, 2004, J EXP PSYCHOL HUMAN, V30, P746, DOI 10.1037/0096-1523.30.4.749
   Walther D, 2005, COMPUT VIS IMAGE UND, V100, P41, DOI 10.1016/j.cviu.2004.09.004
   Walther D, 2006, NEURAL NETWORKS, V19, P1395, DOI 10.1016/j.neunet.2006.10.001
   Yarbus A. L., 1967, Eye Movements and Vision
NR 71
TC 25
Z9 30
U1 0
U2 15
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD FEB
PY 2009
VL 6
IS 2
AR 11
DI 10.1145/1498700.1498705
PG 34
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA 450YN
UT WOS:000266438000005
DA 2024-07-18
ER

PT J
AU Gao, ZH
   Wang, HQ
   Feng, GS
   Lv, HW
AF Gao, Zihan
   Wang, Huiqiang
   Feng, Guangsheng
   Lv, Hongwu
TI Exploring Sonification Mapping Strategies for Spatial Auditory Guidance
   in Immersive Virtual Environments
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Auditory user interface; non-visual guidance; interactive sonification;
   spatial audio; 3D sound; auditory feedback
ID 3D SOUND; NAVIGATION; LOCALIZATION; AUDIO; DISPLAY; SYSTEM; CUES;
   PERFORMANCE; ANOVA; PITCH
AB Spatial auditory cues are important for many tasks in immersive virtual environments, especially guidance tasks. However, due to the limited fidelity of spatial sounds rendered by generic Head-Related Transfer Functions (HRTFs), sound localization usually has a limited accuracy, especially in elevation, which can potentially impact the effectiveness of auditory guidance. To address this issue, we explored whether integrating sonification with spatial audio can enhance the perceptions of auditory guidance cues so user performance in auditory guidance tasks can be improved. Specifically, we investigated the effects of sonification mapping strategy using a controlled experiment that compared four elevation sonification mapping strategies: absolute elevation mapping, unsigned relative elevation mapping, signed relative elevation mapping, and binary relative elevation mapping. In addition, we examined whether azimuth sonification mapping can further benefit the perception of spatial sounds. The results demonstrate that spatial auditory cues can be effectively enhanced by integrating elevation and azimuth sonification, where the accuracy and speed of guidance tasks can be significantly improved. In particular, the overall results suggest that binary relative elevation mapping is generally the most effective strategy among four elevation sonification mapping strategies, which indicates that auditory cues with clear directional information are key to efficient auditory guidance.
C1 [Gao, Zihan; Wang, Huiqiang; Feng, Guangsheng; Lv, Hongwu] Harbin Engn Univ, 145 Nantong St, Harbin 150001, Heilongjiang, Peoples R China.
   [Wang, Huiqiang] Peng Cheng Lab, 2 Xingke 1st St, Shenzhen 150001, Guangdong, Peoples R China.
C3 Harbin Engineering University; Peng Cheng Laboratory
RP Wang, HQ (corresponding author), Harbin Engn Univ, 145 Nantong St, Harbin 150001, Heilongjiang, Peoples R China.; Wang, HQ (corresponding author), Peng Cheng Lab, 2 Xingke 1st St, Shenzhen 150001, Guangdong, Peoples R China.
EM zihan@hrbeu.edu.cn; wanghuiqiang@hrbeu.edu.cn; ica@hrbeu.edu.cn;
   lvhongwu@hrbeu.edu.cn
RI Gao, Zihan/GWQ-5679-2022
OI Gao, Zihan/0000-0001-9624-5587; Wang, Huiqiang/0000-0002-1007-5589; LV,
   Hongwu/0000-0002-1917-3978
FU Natural Science Foundation of China [61872104]; Fundamental Research
   Fund for the Central Universities in China [3072020CF0603,
   XK2060021015]; Tianjin Key Laboratory of Advanced Networking (TANK),
   College of Intelligence and Computing, Tianjin University, Tianjin
   China; project "PCL Future Greater-Bay Area Network Facilities for
   Large-scale Experiments and Applications" [LZC0019]
FX This work is supported by the Natural Science Foundation of China (No.
   61872104), the Fundamental Research Fund for the Central Universities in
   China (Nos. 3072020CF0603, XK2060021015). This work is partially
   supported by the project "PCL Future Greater-Bay Area Network Facilities
   for Large-scale Experiments and Applications (LZC0019)." This work is
   partially supported by Tianjin Key Laboratory of Advanced Networking
   (TANK), College of Intelligence and Computing, Tianjin University,
   Tianjin China, 300350.
CR Ahmetovic D, 2019, INT CONF PERVAS COMP, DOI 10.1109/percom.2019.8767407
   Bauer V, 2019, LECT NOTES COMPUT SC, V11883, P305, DOI 10.1007/978-3-030-31908-3_20
   Begault D.R., 2000, 3 D SOUND VIRTUAL RE
   BEGAULT DR, 1993, HUM FACTORS, V35, P361, DOI 10.1177/001872089303500210
   BEGAULT DR, 1991, J AUDIO ENG SOC, V39, P864
   Begault Durand R., 1992, INT J AVIAT PSYCHOL, V2, P1, DOI [10.1207/s15327108ijap0201_1, DOI 10.1207/S15327108IJAP0201_1]
   Berger CC, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00021
   Blanca MJ, 2017, PSICOTHEMA, V29, P552, DOI 10.7334/psicothema2016.383
   Bormann K, 2005, PRESENCE-TELEOP VIRT, V14, P278, DOI 10.1162/105474605323384645
   Brown Lorna M, 2003, INT C AUDITORY DISPL
   Davis ET, 1999, HUM FAC ERG SOC P, P1197
   Oliveira VAD, 2017, IEEE T VIS COMPUT GR, V23, P1340, DOI 10.1109/TVCG.2017.2657238
   Doerr KU, 2007, IEEE T VIS COMPUT GR, V13, P204, DOI 10.1109/TVCG.2007.37
   Dubus G, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0082491
   Fiannaca A, 2014, P ACM SIGACCESS C CO, P19, DOI DOI 10.1145/2661334.2661453
   Gardner B., 1994, HRTF MEASUREMENTS KE
   Grohn M., 2005, ACM T APPL PERCEPT, V2, P564, DOI DOI 10.1145/1101530.1101558
   Gunther R, 2004, BEHAV INFORM TECHNOL, V23, P435, DOI 10.1080/01449290410001723364
   Hart S. G., 2006, P HUM FACT ERG SOC A, V50, P904, DOI DOI 10.1177/154193120605000909
   HEBRANK J, 1974, J ACOUST SOC AM, V56, P1829, DOI 10.1121/1.1903520
   Heller F, 2016, PROCEEDINGS OF THE 18TH INTERNATIONAL CONFERENCE ON HUMAN-COMPUTER INTERACTION WITH MOBILE DEVICES AND SERVICES (MOBILEHCI'16), P278, DOI 10.1145/2935334.2935365
   Holland S, 2002, PERS UBIQUIT COMPUT, V6, P253, DOI 10.1007/s007790200025
   Huber D.M., 2007, MIDI MANUAL PRACTICA
   Jamal Y, 2017, MULTISENS RES, V30, P287, DOI 10.1163/22134808-00002553
   Katz BFG, 2012, VIRTUAL REAL-LONDON, V16, P253, DOI 10.1007/s10055-012-0213-6
   Kolarik AJ, 2022, EXP BRAIN RES, V240, P81, DOI 10.1007/s00221-021-06235-0
   Kolarik Andrew J., 2020, SCI REP-UK, V10, P1
   Lokki T, 2005, IEEE MULTIMEDIA, V12, P80, DOI 10.1109/MMUL.2005.33
   Loomis JM, 1998, PRESENCE-TELEOP VIRT, V7, P193, DOI 10.1162/105474698565677
   Marquardt A, 2020, IEEE T VIS COMPUT GR, V26, P3389, DOI 10.1109/TVCG.2020.3023605
   Marquardt A, 2019, INT SYM MIX AUGMENT, P190, DOI 10.1109/ISMAR.2019.000-3
   Marston JR, 2007, J VISUAL IMPAIR BLIN, V101, P203, DOI 10.1177/0145482X0710100403
   Mascetti S, 2016, INT J HUM-COMPUT ST, V85, P16, DOI 10.1016/j.ijhcs.2015.08.003
   May AJ, 2003, PERS UBIQUIT COMPUT, V7, P331, DOI 10.1007/s00779-003-0248-5
   Moller H, 1996, J AUDIO ENG SOC, V44, P451
   Neuhoff J.G., 2011, The Sonification Handbook, P63
   Parise CV, 2014, P NATL ACAD SCI USA, V111, P6104, DOI 10.1073/pnas.1322705111
   Parseihian G, 2016, IEEE T MULTIMEDIA, V18, P674, DOI 10.1109/TMM.2016.2531978
   Parseihian Gaetan, 2017, P 13 INT S COMP MUS, P283
   Prud'homme L, 2020, J ACOUST SOC AM, V148, P1614, DOI 10.1121/10.0001954
   Schmider E, 2010, METHODOLOGY-EUR, V6, P147, DOI 10.1027/1614-2241/a000016
   Scholz DS, 2014, FRONT NEUROSCI-SWITZ, V8, DOI 10.3389/fnins.2014.00332
   Shilling RD, 2002, HUM FAC ER, P65
   Spagnol S, 2018, WIREL COMMUN MOB COM, DOI 10.1155/2018/3918284
   Walker B. N., 2005, ACM Transactions on Applied Perception, V2, P407, DOI [10.1145/1101530.1101534, DOI 10.1145/1101530.1101534]
   Walker BN, 2006, HUM FACTORS, V48, P265, DOI 10.1518/001872006777724507
   Walker P, 2018, DEV PSYCHOBIOL, V60, P216, DOI 10.1002/dev.21603
   Wallach H, 1940, J EXP PSYCHOL, V27, P339, DOI 10.1037/h0054629
   Wenzel E.M., 1992, PRESENCE, V1, P80
   WENZEL EM, 1993, J ACOUST SOC AM, V94, P111, DOI 10.1121/1.407089
   Xie B., 2013, Head-related transfer function and virtual auditory display, V2nd
   Zahorik P, 2009, J ACOUST SOC AM, V126, P776, DOI 10.1121/1.3167842
   Ziemer T, 2019, 25 INT C AUD DISPL I, P277, DOI [10.21785/icad2019.018, DOI 10.21785/ICAD2019.018]
   Ziemer T, 2018, P INT C AUD DISPL, P136, DOI [10.21785/icad2018.007, DOI 10.21785/ICAD2018.007]
   Zotkin DN, 2004, IEEE T MULTIMEDIA, V6, P553, DOI [10.1109/TMM.2004.827516, 10.1109/tmm.2004.827516]
NR 55
TC 0
Z9 0
U1 0
U2 12
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD JUL
PY 2022
VL 19
IS 3
AR 9
DI 10.1145/3528171
PG 21
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 4L3FO
UT WOS:000852517200001
DA 2024-07-18
ER

PT J
AU Filip, J
   Kolafová, M
AF Filip, Jiri
   Kolafova, Martina
TI Perceptual Attributes Analysis of Real-world Materials
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE BRDF; attributes; perception; visual; tactile; user study
ID TEXTURE; ILLUMINATION; APPEARANCE; FEATURES
AB Material appearance is often represented by a bidirectional reflectance distribution function (BRDF). Although the concept of the BRDF is widely used in computer graphics and related applications, the number of actual captured BRDFs is limited due to a time and resources demanding measurement process. Several BRDF databases have already been provided publicly, yet subjective properties of underlying captured material samples, apart from single photographs, remain unavailable for users. In this article, we analyzed material samples, used in the creation of the UTIA BRDF database, in a psychophysical study with nine subjects and assessed its 12 visual, tactile, and subjective attributes. Further, we evaluated the relationship between the attributes and six material categories. We consider the presented perceptual analysis as valuable and complementary information to the database; that could aid users to select appropriate materials for their applications.
C1 [Filip, Jiri; Kolafova, Martina] Czech Acad Sci, Inst Informat Theory & Automat UTIA, Vodarenskou Vezi 4, Prague 18208 8, Czech Republic.
C3 Czech Academy of Sciences; Institute of Information Theory & Automation
   of the Czech Academy of Sciences
RP Filip, J (corresponding author), Czech Acad Sci, Inst Informat Theory & Automat UTIA, Vodarenskou Vezi 4, Prague 18208 8, Czech Republic.
EM filipj@utia.cas.cz; kolafova@utia.cas.cz
RI Filip, Jiri/D-3396-2012
FU Czech Science Foundation [GA17-18407S]
FX This research has been supported by the Czech Science Foundation grant
   GA17-18407S.
CR [Anonymous], 1977, NATL BUR STAND MONOG, DOI [10.1109/LPT.2009.2020494, DOI 10.1109/LPT.2009.2020494]
   Brodatz P., 1966, PHOTOGRAPHIC ALBUM A
   Filip J, 2014, COMPUT GRAPH FORUM, V33, P91, DOI 10.1111/cgf.12477
   Filip J, 2014, INT C PATT RECOG, P2047, DOI 10.1109/ICPR.2014.357
   Filip J, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409091
   Fleming RW, 2014, VISION RES, V94, P62, DOI 10.1016/j.visres.2013.11.004
   Fleming RW, 2013, J VISION, V13, DOI 10.1167/13.8.9
   Fleming RW, 2003, J VISION, V3, P347, DOI 10.1167/3.5.3
   Gunther Johannes, 2005, P 10 WORKSH VIS MOD
   Hayes AF, 2007, COMMUN METHODS MEAS, V1, P77, DOI 10.1080/19312450709336664
   Heaps C, 1999, J EXP PSYCHOL HUMAN, V25, P299, DOI 10.1037/0096-1523.25.2.299
   Ho YX, 2008, PSYCHOL SCI, V19, P196, DOI 10.1111/j.1467-9280.2008.02067.x
   ITU, 2008, SUBJ AUD QUAL ASS ME
   Jarabo A, 2014, IEEE T VIS COMPUT GR, V20, P880, DOI 10.1109/TVCG.2014.2312016
   Keelan B. W., 2003, Proceedings of the SPIE - The International Society for Optical Engineering, V5294, P181, DOI 10.1117/12.532064
   Khang BG, 2006, PERCEPTION, V35, P625, DOI 10.1068/p5485
   Landy M. S., 2004, The visual neurosciences, V2, P1106
   Long HH, 2002, INT C PATT RECOG, P135, DOI 10.1109/ICPR.2002.1044631
   MALIK J, 1990, J OPT SOC AM A, V7, P923, DOI 10.1364/JOSAA.7.000923
   Marschner SR, 2000, APPL OPTICS, V39, P2592, DOI 10.1364/AO.39.002592
   Martín R, 2016, SAP 2015: ACM SIGGRAPH SYMPOSIUM ON APPLIED PERCEPTION, P33, DOI 10.1145/2804408.2804420
   Matusik W, 2003, ACM T GRAPHIC, V22, P759, DOI 10.1145/882262.882343
   Mojsilovic A, 2000, IEEE T IMAGE PROCESS, V9, P417, DOI 10.1109/83.826779
   Motoyoshi I, 2007, NATURE, V447, P206, DOI 10.1038/nature05724
   Ngan A., 2005, Eurographics Symposium on Rendering, P117, DOI [DOI 10.2312/EGWR/EGSR05/117-1261,2,8, DOI 10.2312/EGWR/EGSR05/117-126]
   Padilla S, 2008, VISION RES, V48, P1791, DOI 10.1016/j.visres.2008.05.015
   Pas SFT, 2005, PERCEPTION, V34, P212
   Pont SC, 2007, APGV 2007: SYMPOSIUM ON APPLIED PERCEPTION IN GRAPHICS AND VISUALIZATION, PROCEEDINGS, P69
   Ramanarayanan G, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1276377.1276472, 10.1145/1239451.1239527]
   Rao AR, 1996, VISION RES, V36, P1649, DOI 10.1016/0042-6989(95)00202-2
   Schwartz G, 2015, PROC CVPR IEEE, P3565, DOI 10.1109/CVPR.2015.7298979
   Schwartz G, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P883, DOI 10.1109/ICCVW.2013.121
   Serrano A, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980242
   Sharan L, 2013, INT J COMPUT VISION, V103, P348, DOI 10.1007/s11263-013-0609-0
   TAMURA H, 1978, IEEE T SYST MAN CYB, V8, P460, DOI 10.1109/TSMC.1978.4309999
   Tanaka M, 2015, VISION RES, V115, P246, DOI 10.1016/j.visres.2014.11.016
   te Pas S. F., 2005, P 2 S APPL PERC GRAP, P57
   Vangorp P, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1276377.1276473, 10.1145/1239451.1239528]
   Vanrell M, 1997, MACH VISION APPL, V9, P262, DOI 10.1007/s001380050047
   Vanrell M., 1997, PATTERN RECOGN, V1, P245
   Wills J, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1559755.1559760
NR 41
TC 5
Z9 5
U1 0
U2 9
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD FEB
PY 2019
VL 16
IS 1
AR 1
DI 10.1145/3301412
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HN7TC
UT WOS:000460393600001
DA 2024-07-18
ER

PT J
AU Ferstl, Y
   Kokkinara, E
   McDonnell, R
AF Ferstl, Ylva
   Kokkinara, Elena
   McDonnell, Rachel
TI Facial Features of Non-player Creatures Can Influence Moral Decisions in
   Video Games
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Moral dilemmas; personality perception; video games; virtual characters
ID UTILITARIAN; PERSONALITY; JUDGMENTS; DILEMMAS; EMOTIONS; AVATARS
AB With the development of increasingly sophisticated computer graphics, there is a continuous growth of the variety and originality of virtual characters used in movies and games. So far, however, their design has mostly been led by the artist's preferences, not by perceptual studies. In this article, we explored how effective non-player character design can be used to influence gameplay. In particular, we focused on abstract virtual characters with few facial features. In experiment 1, we sought to find rules for how to use a character's facial features to elicit the perception of certain personality traits, using prior findings for human face perception as a basis. In experiment 2, we then tested how perceived personality traits of a nonplayer character could influence a player's moral decisions in a video game. We found that the appearance of the character interacting with the subject modulated aggressive behavior towards a non-present individual. Our results provide us with a better understanding of the perception of abstract virtual characters, their employment in video games, as well as giving us some insights about the factors underlying aggressive behavior in video games.
C1 [Ferstl, Ylva; Kokkinara, Elena; McDonnell, Rachel] Trinity Coll Dublin, Coll Green, Dublin 2, Ireland.
C3 Trinity College Dublin
RP Ferstl, Y (corresponding author), Trinity Coll Dublin, Coll Green, Dublin 2, Ireland.
EM yferstl@scss.tcd.ie; elena@inflight-vr.com; ramcdonn@scss.tcd.ie
RI Ferstl, Ylva/AAU-7379-2021; McDonnell, Rachel/HGC-4337-2022
OI Ferstl, Ylva/0000-0001-7259-0378; McDonnell, Rachel/0000-0002-1957-2506
FU Science Foundation Ireland under the ADAPT Centre for Digital Content
   Technology [13/RC/2106]; Science Foundation Ireland under Game Face
   project [13/CDA/2135]; Horizon EU POPULATE project, under the research
   and innovation programme [644655]; Science Foundation Ireland (SFI)
   [13/CDA/2135] Funding Source: Science Foundation Ireland (SFI)
FX This research was sponsored by Science Foundation Ireland under the
   ADAPT Centre for Digital Content Technology (Grant 13/RC/2106) and the
   Game Face (13/CDA/2135) project. This research was also funded by the
   Horizon 2020 EU POPULATE (644655) project, funded under the research and
   innovation programme.
CR [Anonymous], 2013, P 10 IEEE INT C WORK
   Banakou Domna, 2010, J VIRTUAL WORLDS RES, V2, P5, DOI DOI 10.4101/JVWR.V2I5.779
   Bar M, 2006, EMOTION, V6, P269, DOI 10.1037/1528-3542.6.2.269
   Bates D, 2015, J STAT SOFTW, V67, P1, DOI 10.18637/jss.v067.i01
   Bethesda Game Studios, 2008, FALL 3 GAM MICR WIND
   Bethesda Game Studios, 2015, FALL 4 GAM MICR WIND
   Black Isle Studios, 1998, FALL 2 GAM MICR WIND
   Boothroyd LG, 2008, EVOL HUM BEHAV, V29, P211, DOI 10.1016/j.evolhumbehav.2007.12.009
   Carré JM, 2009, PSYCHOL SCI, V20, P1194, DOI 10.1111/j.1467-9280.2009.02423.x
   CD Project and CD Project RED, 2011, WITCH 2 GAM XBOX 360
   CD Project and CD Project RED, 2007, WITCH GAM MICR WIND
   CD Project RED, 2015, WITCH 3 GAM XBOX ON
   Chan YL, 2016, ASIAN J SOC PSYCHOL, V19, P55, DOI 10.1111/ajsp.12123
   Choe SY, 2011, JUDGM DECIS MAK, V6, P580
   Eastin MS, 2006, HUM COMMUN RES, V32, P351, DOI 10.1111/j.1468-2958.2006.00279.x
   Ferstl Y., 2016, P ACM S APPL PERC, DOI [https://doi.org/10.1145/2931002.2931014, DOI 10.1145/2931002.2931014]
   Fox J, 2013, COMPUT HUM BEHAV, V29, P930, DOI 10.1016/j.chb.2012.12.027
   Gao Y, 2013, J CRIM JUST, V41, P342, DOI 10.1016/j.jcrimjus.2013.06.012
   Geniole SN, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0132726
   Greene J, 2002, TRENDS COGN SCI, V6, P517, DOI 10.1016/S1364-6613(02)02011-9
   Greene JD, 2004, NEURON, V44, P389, DOI 10.1016/j.neuron.2004.09.027
   Greene JD, 2001, SCIENCE, V293, P2105, DOI 10.1126/science.1062872
   Headleand CJ, 2016, LECT NOTES COMPUT SC, V9590, P88, DOI 10.1007/978-3-662-53090-0_5
   Hollingdale J, 2013, J APPL SOC PSYCHOL, V43, P1862, DOI 10.1111/jasp.12148
   Interplay Productions, 1997, FALL GAM MICR WIND M
   KEATING CF, 1985, SOC PSYCHOL QUART, V48, P61, DOI 10.2307/3033782
   Klimmt C, 2010, MEDIA PSYCHOL, V13, P323, DOI 10.1080/15213269.2010.524911
   Kramer RSS, 2012, EVOL PSYCHOL-US, V10, P320
   Kramer RSS, 2011, EVOL HUM BEHAV, V32, P179, DOI 10.1016/j.evolhumbehav.2010.10.005
   Kramer RSS, 2010, Q J EXP PSYCHOL, V63, P2273, DOI 10.1080/17470211003770912
   Little AC, 2007, BRIT J PSYCHOL, V98, P111, DOI 10.1348/000712606X109648
   Messinger P.R., 2008, Journal of Virtual Worlds Research, V1, DOI 10.4101/jvwr.v1i2.352
   Moll J, 2007, TRENDS COGN SCI, V11, P319, DOI 10.1016/j.tics.2007.06.001
   Moore AB, 2011, JUDGM DECIS MAK, V6, P186
   Obsidian Entertainment, 2010, FALL NEW VEG GAM MIC
   Peña J, 2009, COMMUN RES, V36, P838, DOI 10.1177/0093650209346802
   Royzman E.B., 2002, SOC JUSTICE RES, V15, P165, DOI [DOI 10.1023/A:1019923923537, 10.1023/A:1019923923537]
   Schlicht EJ, 2010, PLOS ONE, V5, DOI 10.1371/journal.pone.0011663
   Sell A, 2009, P R SOC B, V276, P575, DOI 10.1098/rspb.2008.1177
   Sicart M, 2013, DES ISSUES, V29, P28, DOI 10.1162/DESI_a_00219
   Stirrat M, 2010, PSYCHOL SCI, V21, P349, DOI 10.1177/0956797610362647
   Thomson J., 1986, RIGHTS RESTITUTION R
   van't Wout M, 2008, COGNITION, V108, P796, DOI 10.1016/j.cognition.2008.07.002
   Wang YQ, 2013, INT CONF AFFECT, P479, DOI 10.1109/ACII.2013.85
   Willis J, 2006, PSYCHOL SCI, V17, P592, DOI 10.1111/j.1467-9280.2006.01750.x
   Wilson JP, 2015, PSYCHOL SCI, V26, P1325, DOI 10.1177/0956797615590992
   Yee N, 2007, HUM COMMUN RES, V33, P271, DOI 10.1111/j.1468-2958.2007.00299.x
   Yee Nick, 2009, COMMUN RES
   Zebrowitz LA, 1996, PERS SOC PSYCHOL B, V22, P1258, DOI 10.1177/01461672962212006
   Zell E, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818126
   Zibrek K., 2014, P ACM S APPL PERCEPT, P111, DOI DOI 10.1145/2628257.2628270
NR 51
TC 8
Z9 10
U1 2
U2 18
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD NOV
PY 2017
VL 15
IS 1
AR 4
DI 10.1145/3129561
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA FU0CU
UT WOS:000423519800004
DA 2024-07-18
ER

PT J
AU Holmes, O
   Banks, MS
   Farid, H
AF Holmes, Olivia
   Banks, Martin S.
   Farid, Hany
TI Assessing and Improving the Identification of Computer-Generated
   Portraits
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Human Factors; Legal Aspects; Computer graphics; photorealistic; photo
   forensics
ID DISCRIMINATION
AB Modern computer graphics are capable of generating highly photorealistic images. Although this can be considered a success for the computer graphics community, it has given rise to complex forensic and legal issues. A compelling example comes from the need to distinguish between computer-generated and photographic images as it pertains to the legality and prosecution of child pornography in the United States. We performed psychophysical experiments to determine the accuracy with which observers are capable of distinguishing computer-generated from photographic images. We find that observers have considerable difficulty performing this task-more difficulty than we observed 5 years ago when computer-generated imagery was not as photorealistic. We also find that observers are more likely to report that an image is photographic rather than computer generated, and that resolution has surprisingly little effect on performance. Finally, we find that a small amount of training greatly improves accuracy.
C1 [Holmes, Olivia; Farid, Hany] Dartmouth Coll, Dept Comp Sci, Hanover, NH 03755 USA.
   [Banks, Martin S.] Univ Calif Berkeley, Sch Optometry, Vis Sci Program, Berkeley, CA 94720 USA.
C3 Dartmouth College; University of California System; University of
   California Berkeley
RP Holmes, O; Farid, H (corresponding author), Dartmouth Coll, Dept Comp Sci, Hanover, NH 03755 USA.; Banks, MS (corresponding author), Univ Calif Berkeley, Sch Optometry, Vis Sci Program, Berkeley, CA 94720 USA.
EM olivia.b.holmes@gmail.com; martybanks@berkeley.edu;
   farid@cs.dartmouth.edu
FU NSF [BCS-1354029]
FX This work was supported by NSF BCS-1354029.
CR [Anonymous], P IEEE INT C MULT EX
   [Anonymous], P IEEE INT C AC SPEE
   [Anonymous], P IEEE INT C IM PROC
   [Anonymous], P IEEE INT C IM PROC
   [Anonymous], SIGGRAPH ASIA 2012 T
   Dehnie S, 2006, IEEE IMAGE PROC, P2313, DOI 10.1109/ICIP.2006.312849
   Dang-Nguyen DT, 2012, EUR SIGNAL PR CONF, P1234
   Farid H, 2012, DIGIT INVEST, V8, P226, DOI 10.1016/j.diin.2011.06.003
   Gallagher Andrew C., 2008, 2008 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops (CVPR Workshops), P1, DOI 10.1109/CVPRW.2008.4562984
   Green D., 1966, SIGNAL DETECTION THE
   Khanna N, 2008, INT CONF ACOUST SPEE, P1653, DOI 10.1109/ICASSP.2008.4517944
   Lalonde Jean- Francois, 2007, P IEEE INT C COMPUTE, P1
   Looser CE, 2010, PSYCHOL SCI, V21, P1854, DOI 10.1177/0956797610388044
   Lyu S, 2005, IEEE T SIGNAL PROCES, V53, P845, DOI 10.1109/TSP.2004.839896
   Tian-Tsong Ng, 2005, 13th Annual ACM International Conference on Multimedia, P239
NR 15
TC 20
Z9 20
U1 1
U2 8
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD MAR
PY 2016
VL 13
IS 2
AR 7
DI 10.1145/2871714
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DJ0OM
UT WOS:000373903800002
OA Green Published
DA 2024-07-18
ER

PT J
AU Kawabe, T
   Fukiage, T
   Sawayama, M
   Nishida, S
AF Kawabe, Takahiro
   Fukiage, Taiki
   Sawayama, Masataka
   Nishida, Shin'ya
TI Deformation Lamps: A Projection Technique to Make Static Objects
   Perceptually Dynamic
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Visual Perception; Motion perception; motion-pattern interaction; light
   projection; illusion
ID MOTION; COLOR; MOVEMENT; PERCEPTION; SIGNALS; SHAPE
AB Light projection is a powerful technique that can be used to edit the appearance of objects in the real world. Based on pixel-wise modification of light transport, previous techniques have successfully modified static surface properties such as surface color, dynamic range, gloss, and shading. Here, we propose an alternative light projection technique that adds a variety of illusory yet realistic distortions to a wide range of static 2D and 3D projection targets. The key idea of our technique, referred to as (Deformation Lamps), is to project only dynamic luminance information, which effectively activates the motion (and shape) processing in the visual system while preserving the color and texture of the original object. Although the projected dynamic luminance information is spatially inconsistent with the color and texture of the target object, the observer's brain automatically combines these sensory signals in such a way as to correct the inconsistency across visual attributes. We conducted a psychophysical experiment to investigate the characteristics of the inconsistency correction and found that the correction was critically dependent on the retinal magnitude of the inconsistency. Another experiment showed that the perceived magnitude of image deformation produced by our techniques was underestimated. The results ruled out the possibility that the effect obtained by our technique stemmed simply from the physical change in an object's appearance by light projection. Finally, we discuss how our techniques can make the observers perceive a vivid and natural movement, deformation, or oscillation of a variety of static objects, including drawn pictures, printed photographs, sculptures with 3D shading, and objects with natural textures including human bodies.
C1 [Kawabe, Takahiro; Fukiage, Taiki; Sawayama, Masataka; Nishida, Shin'ya] NTT Corp, Commun Sci Labs, Tokyo, Japan.
   [Kawabe, Takahiro; Fukiage, Taiki; Sawayama, Masataka; Nishida, Shin'ya] 3-1 Morinosatowakamiya, Atsugi, Kanagawa 2430198, Japan.
C3 Nippon Telegraph & Telephone Corporation
RP Kawabe, T; Fukiage, T; Sawayama, M; Nishida, S (corresponding author), NTT Corp, Commun Sci Labs, Tokyo, Japan.; Kawabe, T; Fukiage, T; Sawayama, M; Nishida, S (corresponding author), 3-1 Morinosatowakamiya, Atsugi, Kanagawa 2430198, Japan.
EM kawabe.takahiro@lab.ntt.co.jp; fukiage.taiki@lab.ntt.co.jp;
   sawayama.masataka@lab.ntt.co.jp; shinyanishida@mac.com
RI Sawayama, Masataka/IAQ-4450-2023
FU Japanese Ministry of Education, Culture, Sports, Science, and Technology
   [22135004, 15H05915]; Grants-in-Aid for Scientific Research [15H05915,
   22135004] Funding Source: KAKEN
FX This research was supported by Grants-in-Aid for Scientific Research on
   Innovative Areas (22135004 and 15H05915) from the Japanese Ministry of
   Education, Culture, Sports, Science, and Technology.
CR ADELSON EH, 1985, J OPT SOC AM A, V2, P284, DOI 10.1364/JOSAA.2.000284
   Aliaga D. G., 2008, ACM T GRAPHIC, V27
   Amano T., 2014, P INT C ART REAL TEL, P49
   Amano T, 2013, IEEE COMPUT SOC CONF, P918, DOI 10.1109/CVPRW.2013.135
   [Anonymous], QUANTITATIVE METHODS, DOI DOI 10.3758/S13428-013-0441-Z
   [Anonymous], 2005, Spatial Augmented Reality: Merging Real and Virtual Worlds
   [Anonymous], 2012, ACM T GRAPHIC, DOI DOI 10.1145/2159516.2159518
   Anstis S., 2012, J VISION, V12, p[2, 1], DOI DOI 10.1167/12.12.12
   ANSTIS SM, 1970, VISION RES, V10, P1411, DOI 10.1016/0042-6989(70)90092-1
   ANSTIS SM, 1975, VISION RES, V15, P957, DOI 10.1016/0042-6989(75)90236-9
   Ashdown M, 2006, P IEEE INT WORKSH PR, P60
   Batista M. A., 2011, P COMP GRAPH INT 201
   Bermano A, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508416
   Bimber O, 2005, COMPUTER, V38, P48, DOI 10.1109/MC.2005.17
   Bimber O., 2008, ACM SIGGRAPH 2008 CL
   Bimber O, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409103
   Blanz V, 1999, COMP GRAPH, P187, DOI 10.1145/311535.311556
   Burr D, 2011, VISION RES, V51, P1431, DOI 10.1016/j.visres.2011.02.008
   CAVANAGH P, 1984, J OPT SOC AM A, V1, P893, DOI 10.1364/JOSAA.1.000893
   Chi MT, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360661
   Chuang YY, 2005, ACM T GRAPHIC, V24, P853, DOI 10.1145/1073204.1073273
   Cropper Simon J, 2005, Behav Cogn Neurosci Rev, V4, P192, DOI 10.1177/1534582305285120
   DEVALOIS RL, 1991, VISION RES, V31, P1619, DOI 10.1016/0042-6989(91)90138-U
   Duncker K, 1929, PSYCHOL FORSCH, V12, P180, DOI 10.1007/BF02409210
   FRASER A, 1979, NATURE, V281, P565, DOI 10.1038/281565a0
   FREEMAN WT, 1991, COMP GRAPH, V25, P27, DOI 10.1145/127719.122721
   Goda N, 1997, PERCEPTION, V26, P1413, DOI 10.1068/p261413
   Grundhöfer A, 2013, IEEE COMPUT SOC CONF, P924, DOI 10.1109/CVPRW.2013.136
   Kawabe T, 2015, P NATL ACAD SCI USA, V112, pE4620, DOI 10.1073/pnas.1500913112
   Kelly D., 1980, J OPT SOC AM, V69, P1340
   Kitaoka A., 2003, Vision, V15, P261, DOI [DOI 10.11247/JSSDJ.58, DOI 10.24636/VISION.15.4_261]
   Legarda-Sáenz R, 2004, OPT ENG, V43, P464, DOI 10.1117/1.1635373
   LIVINGSTONE MS, 1985, NATURE, V315, P285, DOI 10.1038/315285b0
   LIVINGSTONE MS, 1987, J NEUROSCI, V7, P3416
   LU ZL, 1995, VISION RES, V35, P2697, DOI 10.1016/0042-6989(95)00025-U
   Mark W. R., 1997, Proceedings 1997 Symposium on Interactive 3D Graphics, P7, DOI 10.1145/253284.253292
   Mukaigawa Y., 2004, P INT C VIRTUAL SYST, P544
   Nishida S, 1999, NATURE, V397, P610, DOI 10.1038/17600
   Nishida S, 2004, CURR BIOL, V14, P830, DOI 10.1016/j.cub.2004.04.044
   Nishida S, 2007, CURR BIOL, V17, P366, DOI 10.1016/j.cub.2006.12.041
   Nishida S, 2011, J VISION, V11, DOI 10.1167/11.5.11
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   Olmos A, 2004, PERCEPTION, V33, P1463, DOI 10.1068/p5321
   RAMACHANDRAN VS, 1978, NATURE, V275, P55, DOI 10.1038/275055a0
   RAMACHANDRAN VS, 1987, NATURE, V328, P645, DOI 10.1038/328645a0
   RAMACHANDRAN VS, 1990, PERCEPTION, V19, P611, DOI 10.1068/p190611
   Raskar R, 2001, SPRING EUROGRAP, P89
   Raskar R., 1998, P ACM SIGGRAPH, P1
   Raskar R., 2002, P 2 INT S NONPHOTORE, P7, DOI [10.1145/508530.508532, DOI 10.1145/508530.5085322]
   Siegl C, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818111
   Smit FA, 2007, VRST 2007: ACM SYMPOSIUM ON VIRTUAL REALITY SOFTWARE AND TECHNOLOGY, PROCEEDINGS, P153
   Sueishi T, 2015, P IEEE VIRT REAL ANN, P97, DOI 10.1109/VR.2015.7223330
   Underkoffler J., 1997, Personal Technologies, V1, P49
   VANSANTEN JPH, 1985, J OPT SOC AM A, V2, P300, DOI 10.1364/JOSAA.2.000300
   Wandell B. A., 1995, FDN VISUAL SCI
   Xue TF, 2014, LECT NOTES COMPUT SC, V8691, P767, DOI 10.1007/978-3-319-10578-9_50
NR 56
TC 37
Z9 37
U1 0
U2 14
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD MAR
PY 2016
VL 13
IS 2
AR 10
DI 10.1145/2874358
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DJ0OM
UT WOS:000373903800005
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Legde, K
   Castillo, S
   Cunningham, DW
AF Legde, Katharina
   Castillo, Susana
   Cunningham, Douglas W.
TI Multimodal Affect: Perceptually Evaluating an Affective Talking Head
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article; Proceedings Paper
CT ACM SIGGRAPH Symposium on Applied Perception (SAP)
CY SEP 13-14, 2015
CL Tubingen, GERMANY
SP ACM SIGGRAPH, ACM, Disney Res, Max Planck Inst Biol Cybernet
DE Experimentation; Human Factors; Affective interfaces; emotion; speech;
   facial animation
ID OF-THE-ART; HEARING LIPS; RECOGNITION; BEHAVIOR; SPEECH
AB Many tasks such as driving or rapidly sorting items can be best achieved by direct actions. Other tasks such as giving directions, being guided through a museum, or organizing a meeting are more easily solved verbally. Since computers are increasingly being used in all aspects of daily life, it would be of great advantage if we could communicate verbally with them. Although advanced interactions with computers are possible, a vast majority of interactions are still based on the WIMP (Window, Icon, Menu, Point) metaphor [Hevner and Chatterjee 2010] and are, therefore, via simple text and gesture commands. The field of affective interfaces is working toward making computers more accessible by giving them (rudimentary) natural-language abilities, including using synthesized speech, facial expressions, and virtual body motions. Once the computer is granted a virtual body, however, it must be given the ability to use it to nonverbally convey socio-emotional information (such as emotions, intentions, mental state, and expectations) or it will likely be misunderstood. Here, we present a simple affective talking head along with the results of an experiment on the multimodal expression of emotion. The results show that although people can sometimes recognize the intended emotion from the semantic content of the text even when the face does not convey affect, they are considerably better at it when the face also shows emotion. Moreover, when both face and text convey emotion, people can detect different levels of emotional intensity.
C1 [Legde, Katharina; Castillo, Susana; Cunningham, Douglas W.] Brandenburg Tech Univ Cottbus, Inst Informat, Lehrstuhl Graf Syst, D-03046 Cottbus, Germany.
C3 Brandenburg University of Technology Cottbus
RP Legde, K (corresponding author), Brandenburg Tech Univ Cottbus, Inst Informat, Lehrstuhl Graf Syst, Konrad Wachsmann Allee 5, D-03046 Cottbus, Germany.
EM legdekat@b-tu.de; castillo@b-tu.de; douglas.cunningham@b-tu.de
RI Castillo, Susana/U-6432-2019
OI Castillo, Susana/0000-0003-1245-4758
FU German research council
FX This work was sponsored in part by a grant from the German research
   council.
CR Albrecht I, 2002, WSCG'2002, VOLS I AND II, CONFERENCE PROCEEDINGS, P9
   Albrecht I., 2005, J VIRTUAL REALITY, V8, P201, DOI DOI 10.1007/S10055-005-0153-5
   Ambadar Z, 2005, PSYCHOL SCI, V16, P403, DOI 10.1111/j.0956-7976.2005.01548.x
   [Anonymous], P WORK C ADV VIS INT
   [Anonymous], 2008, Computer Facial Animation
   ARCHER D, 1977, J PERS SOC PSYCHOL, V35, P443, DOI 10.1037/0022-3514.35.6.443
   Balyan A., 2013, IJERT, V2, P57
   Bavelas JB, 2000, J PERS SOC PSYCHOL, V79, P941, DOI 10.1037/0022-3514.79.6.941
   BAVELAS JB, 1986, J PERS SOC PSYCHOL, V50, P322, DOI 10.1037/0022-3514.50.2.322
   Birkholz P., 2013, 1 USER MANUAL
   Boucenna S, 2014, COGN COMPUT, V6, P722, DOI 10.1007/s12559-014-9276-x
   Brainard DH, 1997, SPATIAL VISION, V10, P433, DOI 10.1163/156856897X00357
   Breidt Martin, 2003, P SIGGRAPH 2003 NEIL
   Bridwhistell R., 1970, KINETICS AND CONTEXT
   Brown R., 1986, SOC PSYCHOL-GERMANY
   Bull P, 2001, PSYCHOLOGIST, V14, P644
   Burkhardt F, 2005, P INT 05 LISB PORT, P509
   Cahn J., 2000, Journal of the American Voice I/O Society, V8, P1
   CARRERALEVILLAIN P, 1994, J NONVERBAL BEHAV, V18, P281, DOI 10.1007/BF02172290
   Cassell J, 1999, APPL ARTIF INTELL, V13, P519, DOI 10.1080/088395199117360
   Cassell J, 2001, KNOWL-BASED SYST, V14, P55, DOI 10.1016/S0950-7051(00)00102-7
   Cassell J., 2000, Embodied Conversational Agents
   Castillo S, 2014, COMPUT ANIMAT VIRT W, V25, P225, DOI 10.1002/cav.1593
   Chateau N., 2005, CHI WORKSH AFF INT I
   Choe B, 2001, J VISUAL COMP ANIMAT, V12, P67, DOI 10.1002/vis.246
   Cohen M. M., 1993, Models and Techniques in Computer Animation, P139
   Cosker D., 2004, PROC APGV, P151
   Coull Alasdair D., 2006, THESIS U GLASGOW
   Cunningham D. W., 2005, ACM T APPL PERCEPT, V2, P251, DOI DOI 10.1145/1077399.1077404
   Cunningham DW, 2009, J VISION, V9, DOI 10.1167/9.13.7
   De Carolis B, 2004, COG TECH, P65
   DIXON NF, 1980, PERCEPTION, V9, P719, DOI 10.1068/p090719
   Duchenne G.B., 1862, The Mechanism of Human Facial Expression
   Dutoit Thierry, 2001, INTRO TEXT TO SPEECH
   EKMAN P, 1992, COGNITION EMOTION, V6, P169, DOI 10.1080/02699939208411068
   Ekman P, 1978, FACIAL ACTION CODING
   Ekman P., 1969, Semiotica, V1, P49, DOI [10.1515/semi.1969.1.1.49, DOI 10.1515/SEMI.1969.1.1.49]
   ELLSWORTH PC, 1972, J COMMUN, V22, P375, DOI 10.1111/j.1460-2466.1972.tb00164.x
   Ezzat T, 2000, INT J COMPUT VISION, V38, P45, DOI 10.1023/A:1008166717597
   Farnetani E., 1999, COARTICULATION, P31
   Griesser RT, 2007, APGV 2007: SYMPOSIUM ON APPLIED PERCEPTION IN GRAPHICS AND VISUALIZATION, PROCEEDINGS, P11
   Harwood NK, 1999, AM J MENT RETARD, V104, P270, DOI 10.1352/0895-8017(1999)104<0270:ROFEEF>2.0.CO;2
   Ibáñez J, 2008, KNOWL ENG REV, V23, P339, DOI 10.1017/S0269888908000039
   Isaacs E., 1993, ACM MULTIMEDIA 93, P496
   Isbister Katherine, 2005, CHI 05 EA, P2119
   Kahler K., 2001, GRAPHICS INTERFACE 2, P37, DOI DOI 10.20380/GI2001.05
   Kopp S, 2005, LECT NOTES ARTIF INT, V3661, P329
   Krahmer E., 2002, Proceedings of the 1st International Conference on Speech Prosody (SP2002), P443
   Kröger BJ, 2009, LECT NOTES ARTIF INT, V5398, P306
   Lee C, 2007, 2007 RO-MAN: 16TH IEEE INTERNATIONAL SYMPOSIUM ON ROBOT AND HUMAN INTERACTIVE COMMUNICATION, VOLS 1-3, P793
   Lee SP, 2002, ACM T GRAPHIC, V21, P637
   Lehiste I., 1970, SUPRASEGMENTALS
   Lhommet M, 2015, AAAI CONF ARTIF INTE, P4303
   LOFQVIST A, 1990, NATO ADV SCI I D-BEH, V55, P289
   Maier JX, 2011, J EXP PSYCHOL HUMAN, V37, P245, DOI 10.1037/a0019952
   Marsella Stacy, 2013, P 12 ACM SIGGRAPH EU, P25, DOI DOI 10.1145/2485895.2485900
   MCGURK H, 1976, NATURE, V264, P746, DOI 10.1038/264746a0
   MEHRABIAN A, 1968, PSYCHOL TODAY, V2, P53
   MEHRABIAN A, 1967, J PERS SOC PSYCHOL, V6, P109, DOI 10.1037/h0024532
   Mower E, 2009, IEEE T MULTIMEDIA, V11, P843, DOI 10.1109/TMM.2009.2021722
   Niewiadomski Radoslaw, 2012, Intelligent Virtual Agents. Proceedings 12th International Conference, IVA 2012, P231, DOI 10.1007/978-3-642-33197-8_24
   OHALA JJ, 1993, LANG SPEECH, V36, P155, DOI 10.1177/002383099303600303
   Paiva Ana, 2003, ICMI 03, P60, DOI DOI 10.1145/958432.958446
   Pantic Maja, 2001, VISUAL ANAL HUMANS, P511
   PELACHAUD C, 1991, COMPUTER ANIMATION 9, P15
   Picard R.W., 1995, AFFECTIVE COMPUTING
   Poggi I, 2000, EMBODIED CONVERSATIONAL AGENTS, P155
   Reeves B., 1996, MEDIA EQUATION PEOPL
   Robert Vincent, 2008, ISSP
   Rosenblum LD, 2005, BLACKW HBK LINGUIST, P51, DOI 10.1002/9780470757024.ch3
   Sacks Harvey, 1995, FALL 1964 SPRONG 196
   Schroder M., 2003, International Journal of Speech Technology, V6, P365, DOI 10.1023/A:1025708916924
   Skipper JI, 2007, CEREB CORTEX, V17, P2387, DOI 10.1093/cercor/bhl147
   Tang SS, 2004, PROCEEDINGS OF THE 2004 INTERNATIONAL SYMPOSIUM ON INTELLIGENT MULTIMEDIA, VIDEO AND SPEECH PROCESSING, P246, DOI 10.1109/ISIMP.2004.1434046
   Taylor P., 1998, 3 ESCA WORKSHOP SPEE, P147
   Thorisson K. R., 1996, TOONFACE SYSTEM CREA
   Turkmani A., 2007, THESIS U SURREY
   van Welbergen H, 2006, IEEE INTELL SYST, V21, P47, DOI 10.1109/MIS.2006.101
   Vertegaal R., 1997, EXTENDED ABSTRACTS C, P496
   Wallraven C, 2008, ACM T APPL PERCEPT, V4, DOI 10.1145/1278760.1278764
   Watzlawick Paul., 1985, Menschliche Kommunikation. Formen Storungen Paradoxien. 7.
   Yngve VictorH., 1970, PAPERS 6 REGIONAL M, V6, P567
NR 82
TC 0
Z9 0
U1 3
U2 18
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD SEP
PY 2015
VL 12
IS 4
SI SI
AR 17
DI 10.1145/2811265
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA CR1ER
UT WOS:000361067300005
DA 2024-07-18
ER

PT J
AU Zibrek, K
   Hoyet, L
   Ruhland, K
   McDonnell, R
AF Zibrek, Katja
   Hoyet, Ludovic
   Ruhland, Kerstin
   McDonnell, Rachel
TI Exploring the Effect of Motion Type and Emotions on the Perception of
   Gender in Virtual Humans
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Experimentation; Human Factors; Facial animation; emotions; gender
ID BIOLOGICAL MOTION; POINT-LIGHT; SEX; EXPRESSIONS; MOVEMENT
AB In this article, we investigate the perception of gender from the motion of virtual humans under different emotional conditions and explore the effect of emotional bias on gender perception (e.g., anger being attributed to males more than females). As motion types can present different levels of physiological cues, we also explore how two types of motion (walking and conversations) are affected by emotional bias. Walking typically displays more physiological cues about gender (e.g., hip sway) and therefore is expected to be less affected by emotional bias. To investigate these effects, we used a corpus of captured facial and body motions from four male and four female actors, performing basic emotions through conversation and walk. We expected that the appearance of the model would also influence gender perception; therefore, we displayed both male and female motions on two virtual models of different sex. Two experiments were then conducted to assess gender judgments from these motions. In both experiments, participants were asked to rate how male or female they considered the motions to be under different emotional states, then classified the emotions to determine how accurately they were portrayed by actors. Overall, both experiments showed that gender ratings were affected by the displayed emotion. However, we found that conversations were influenced by gender stereotypes to a greater extent than walking motions. This was particularly true for anger, which was perceived as male on both male and female motions, and sadness, which was perceived as less male when portrayed by male actors. We also found a slight effect of the model when observing gender on different types of virtual models. These results have implications for the design and animation of virtual humans.
C1 [Zibrek, Katja; Ruhland, Kerstin; McDonnell, Rachel] Univ Dublin Trinity Coll, Graph Vis & Visualisat Grp, Dublin 2, Ireland.
   [Hoyet, Ludovic] INRIA Rennes, Bretagne Atlantique Ctr, F-35042 Rennes, France.
C3 Trinity College Dublin; Universite de Rennes
RP Zibrek, K (corresponding author), Univ Dublin Trinity Coll, Graph Vis & Visualisat Grp, Dublin 2, Ireland.
EM zibrekka@tcd.ie; ludovic.hoyet@inria.fr; ruhlandk@tcd.ie;
   Rachel.McDonnell@scss.tcd.ie
RI Zibrek, Katja/JQW-2981-2023; McDonnell, Rachel/HGC-4337-2022; Hoyet,
   Ludovic/IWU-9100-2023
OI Zibrek, Katja/0000-0002-0204-3472; McDonnell,
   Rachel/0000-0002-1957-2506; Hoyet, Ludovic/0000-0002-7373-6049
FU Science Foundation Ireland [303.G20586, 10/IN.1/13003]
FX This work was sponsored by Science Foundation Ireland as part of the
   Cartoon Motion (303.G20586) and Captavatar (10/IN.1/13003) projects.
CR [Anonymous], 2013, P MOT GAM NOV
   Atkinson AP, 2004, PERCEPTION, V33, P717, DOI 10.1068/p5096
   Aviezer H, 2012, SCIENCE, V338, P1225, DOI 10.1126/science.1224313
   BASSILI JN, 1978, J EXP PSYCHOL HUMAN, V4, P373, DOI 10.1037/0096-1523.4.3.373
   Battocchi A., 2005, Proceedings of the Int. Conf. on Multimodal Interfaces, P214, DOI DOI 10.1145/1088463.1088501
   Ben-David BM, 2011, BRAIN INJURY, V25, P206, DOI 10.3109/02699052.2010.536197
   Brody L.R., 2000, HDB EMOTIONS, V2nd, P338
   Chaminade T, 2007, SOC COGN AFFECT NEUR, V2, P206, DOI 10.1093/scan/nsm017
   Clavel C, 2009, LECT NOTES ARTIF INT, V5773, P287
   Crane E, 2007, LECT NOTES COMPUT SC, V4738, P95
   CUTTING JE, 1978, PERCEPTION, V7, P393, DOI 10.1068/p070393
   DEAUX K, 1993, PSYCHOL SCI, V4, P125, DOI 10.1111/j.1467-9280.1993.tb00474.x
   DEMEIJER M, 1989, J NONVERBAL BEHAV, V13, P247, DOI 10.1007/BF00990296
   EKMAN P, 1992, COGNITION EMOTION, V6, P169, DOI 10.1080/02699939208411068
   Fischer AH, 2004, EMOTION, V4, P87, DOI 10.1037/1528-3542.4.1.87
   Hess U, 2004, EMOTION, V4, P378, DOI 10.1037/1528-3542.4.4.378
   Hill H, 2001, CURR BIOL, V11, P880, DOI 10.1016/S0960-9822(01)00243-3
   Hodgins J, 2010, ACM T APPL PERCEPT, V7, DOI 10.1145/1823738.1823740
   Hugill N, 2010, EVOL PSYCHOL-US, V8, P66
   JOHANSSON G, 1973, PERCEPT PSYCHOPHYS, V14, P201, DOI 10.3758/BF03212378
   Johnson KL, 2011, COGNITION, V119, P265, DOI 10.1016/j.cognition.2011.01.016
   Johnson KL, 2005, PSYCHOL SCI, V16, P890, DOI 10.1111/j.1467-9280.2005.01633.x
   Jorg S., 2010, P 7 S APPL PERCEPTIO, P129, DOI DOI 10.1145/1836248.1836273
   KOZLOWSKI LT, 1977, PERCEPT PSYCHOPHYS, V21, P575, DOI 10.3758/BF03198740
   Marilynn B. Brewer, 1988, DUAL PROCESS MODEL I
   Mcdonnell R, 2009, ACM T APPL PERCEPT, V5, DOI 10.1145/1462048.1462051
   Morrison ER, 2007, EVOL HUM BEHAV, V28, P186, DOI 10.1016/j.evolhumbehav.2007.01.001
   Plant EA, 2000, PSYCHOL WOMEN QUART, V24, P81, DOI 10.1111/j.1471-6402.2000.tb01024.x
   Pollick FE, 2001, COGNITION, V82, pB51, DOI 10.1016/S0010-0277(01)00147-0
   Schyns PG, 2009, PLOS ONE, V4, DOI 10.1371/journal.pone.0005625
   UNGER RK, 1979, AM PSYCHOL, V34, P1085, DOI 10.1037/0003-066X.34.11.1085
   Volkova E.P., 2014, Frontiers in Psychology, V5, P1
   Wellerdiek AC, 2013, P ACM S APPL PERC SA, P138, DOI [10.1145/2492494.2501895, DOI 10.1145/2492494.2501895]
NR 33
TC 20
Z9 22
U1 2
U2 11
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD JUL
PY 2015
VL 12
IS 3
AR 11
DI 10.1145/2767130
PG 20
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA CP6OI
UT WOS:000360006600005
DA 2024-07-18
ER

PT J
AU Lin, QF
   Rieser, J
   Bodenheimer, B
AF Lin, Qiufeng
   Rieser, John
   Bodenheimer, Bobby
TI Affordance Judgments in HMD-Based Virtual Environments: Stepping over a
   Pole and Stepping off a Ledge
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Experimentation; Human Factors; Performance; Virtual reality;
   head-mounted displays; affordances; height perception
ID PERCEIVING AFFORDANCES; SCALED INFORMATION; REALITY EXPOSURE; VISUAL
   GUIDANCE; PERCEPTION; MECHANISMS; OWNERSHIP; DISTANCE; QUALITY; FEAR
AB People judge what they can and cannot do all the time when acting in the physical world. Can I step over that fence or do I need to duck under it? Can I step off of that ledge or do I need to climb off of it? These qualities of the environment that people perceive that allow them to act are called affordances. This article compares people's judgments of affordances on two tasks in both the real world and in virtual environments presented with head-mounted displays. The two tasks were stepping over or ducking under a pole, and stepping straight off of a ledge. Comparisons between the real world and virtual environments are important because they allow us to evaluate the fidelity of virtual environments. Another reason is that virtual environment technologies enable precise control of the myriad perceptual cues at work in the physical world and deepen our understanding of how people use vision to decide how to act. In the experiments presented here, the presence or absence of a self-avatar-an animated graphical representation of a person embedded in the virtual environment-was a central factor. Another important factor was the presence or absence of action, that is, whether people performed the task or reported that they could or could not perform the task. The results show that animated self-avatars provide critical information for people deciding what they can and cannot do in virtual environments, and that action is significant in people's affordance judgments.
C1 [Lin, Qiufeng; Bodenheimer, Bobby] Vanderbilt Univ, Dept Elect Engn & Comp Sci, Nashville, TN 37212 USA.
   [Rieser, John] Vanderbilt Univ, Dept Psychol & Human Dev, Nashville, TN 37203 USA.
C3 Vanderbilt University; Vanderbilt University
RP Lin, QF (corresponding author), Vanderbilt Univ, Dept Elect Engn & Comp Sci, 254 Featheringill Hall, Nashville, TN 37212 USA.
EM qiufeng.lin@vanderbilt.edu; j.rieser@vanderbilt.edu;
   bobby.bodenheimer@vanderbilt.edu
FU National Science Foundation [0705863, 1116988]; Div Of Information &
   Intelligent Systems; Direct For Computer & Info Scie & Enginr [1116988,
   0705863] Funding Source: National Science Foundation
FX The authors would like to thank the reviewers for their insightful and
   constructive comments, which improved the article. We also thank Xianshi
   Xie and Gayathri Narasimham for support throughout the project, and Alan
   Peters for help with the figures. This material is based on work
   supported by the National Science Foundation under grants 0705863 and
   1116988. Any opinions, findings, and conclusions or recommendations
   expressed in this material are those of the authors and do not
   necessarily reflect the views of the sponsors.
CR [Anonymous], 2008, P 2008 ACM S VIRTUAL, DOI DOI 10.1145/1450579.1450614
   Blanke O, 2012, NAT REV NEUROSCI, V13, P556, DOI 10.1038/nrn3292
   Bodenheimer B, 2007, APGV 2007: SYMPOSIUM ON APPLIED PERCEPTION IN GRAPHICS AND VISUALIZATION, PROCEEDINGS, P35
   Creem-Regehr SH, 2010, WIRES COGN SCI, V1, P800, DOI 10.1002/wcs.82
   Dalgarno B, 2010, BRIT J EDUC TECHNOL, V41, P10, DOI 10.1111/j.1467-8535.2009.01038.x
   de Vignemont F, 2011, CONSCIOUS COGN, V20, P82, DOI 10.1016/j.concog.2010.09.004
   EA MCMANUS., 2011, P ACM SIGGRAPH S APP, P37, DOI DOI 10.1145/2077451.2077458
   Ernst MO, 2004, TRENDS COGN SCI, V8, P162, DOI 10.1016/j.tics.2004.02.002
   Evans N, 2013, NEUROIMAGE, V64, P216, DOI 10.1016/j.neuroimage.2012.09.027
   Flach JM, 1998, PRESENCE-TELEOP VIRT, V7, P90, DOI 10.1162/105474698565550
   Franchak J, 2014, ECOL PSYCHOL, V26, P109, DOI 10.1080/10407413.2014.874923
   Geuss Michael., 2010, P 7 S APPL PERCEPTIO, P61, DOI [10.1145/1836248.1836259, DOI 10.1145/1836248.1836259]
   GIBSON EJ, 1960, SCI AM, V202, P64, DOI 10.1038/scientificamerican0460-64
   Gibson J., 1979, The ecological approach to visual perception
   GOODALE MA, 1992, TRENDS NEUROSCI, V15, P20, DOI 10.1016/0166-2236(92)90344-8
   Grassi M, 2009, BEHAV RES METHODS, V41, P20, DOI 10.3758/BRM.41.1.20
   Grechkin TY, 2010, ACM T APPL PERCEPT, V7, DOI 10.1145/1823738.1823744
   Green D., 1966, SIGNAL DETECTION THE
   Gross DC, 2005, PRESENCE-VIRTUAL AUG, V14, P482, DOI 10.1162/105474605774785244
   H WU., 2009, P 6 S APPL PERCEPTIO, P35
   Hillis JM, 2002, SCIENCE, V298, P1627, DOI 10.1126/science.1075396
   Jones JA, 2008, APGV 2008: PROCEEDINGS OF THE SYMPOSIUM ON APPLIED PERCEPTION IN GRAPHICS AND VISUALIZATION, P9
   Kilteni K, 2012, PRESENCE-TELEOP VIRT, V21, P373, DOI 10.1162/PRES_a_00124
   Kretch KS, 2013, CHILD DEV, V84, P226, DOI 10.1111/j.1467-8624.2012.01842.x
   Lepecq JC, 2009, VIRTUAL REAL-LONDON, V13, P141, DOI 10.1007/s10055-009-0118-1
   Lin Q, 2012, P ACM S APPL PERC 3, P7
   LIN Q., 2013, P ACM S APPL PERCEPT, P107
   Loomis JM, 2003, VIRTUAL AND ADAPTIVE ENVIRONMENTS: APPLICATIONS, IMPLICATIONS, AND HUMAN PERFORMANCE ISSUES, P21
   MARK L S, 1990, Ecological Psychology, V2, P325, DOI 10.1207/s15326969eco0204_2
   MARK LS, 1987, J EXP PSYCHOL HUMAN, V13, P361, DOI 10.1037/0096-1523.13.3.361
   Meehan M, 2002, ACM T GRAPHIC, V21, P645, DOI 10.1145/566570.566630
   Michaels CF, 2003, ECOL PSYCHOL, V15, P135, DOI 10.1207/S15326969ECO1502_3
   Milner AD, 2008, NEUROPSYCHOLOGIA, V46, P774, DOI 10.1016/j.neuropsychologia.2007.10.005
   Mohler BJ, 2010, PRESENCE-TELEOP VIRT, V19, P230, DOI 10.1162/pres.19.3.230
   Norman Don, 2013, The design of everyday things
   Peck TC, 2012, IEEE T VIS COMPUT GR, V18, P1053, DOI 10.1109/TVCG.2011.289
   PUFALL P B, 1992, Ecological Psychology, V4, P17
   Regia-Corte T, 2013, VIRTUAL REAL-LONDON, V17, P17, DOI 10.1007/s10055-012-0216-3
   Ries B., 2009, Proceedings of the 16th ACM Symposium on Virtual Reality Software and Technology, VRST '09, P59, DOI [10.1145/1643928.1643943, DOI 10.1145/1643928.16439433, DOI 10.1145/1643928.1643943]
   Rizzo AA, 2006, CNS SPECTRUMS, V11, P35, DOI 10.1017/S1092852900024196
   Rothbaum BO, 1999, BEHAV MODIF, V23, P507, DOI 10.1177/0145445599234001
   Rothbaum BO, 2000, J CONSULT CLIN PSYCH, V68, P1020, DOI 10.1037/0022-006X.68.6.1020
   Slater M, 2009, FRONT NEUROSCI-SWITZ, V3, P214, DOI 10.3389/neuro.01.029.2009
   Slater M, 2009, IEEE COMPUT GRAPH, V29, P76, DOI 10.1109/MCG.2009.55
   Slater Mel, 1995, ACM Transactions on Computer-Human Interaction, V2, P201, DOI DOI 10.1145/210079.210084
   SMETS G., 1995, Simulated and Virtual Realities: Elements of Perception, P189
   Stefanucci JK, 2010, ATTEN PERCEPT PSYCHO, V72, P1338, DOI 10.3758/APP.72.5.1338
   Stefanucci JK, 2009, PERCEPTION, V38, P1782, DOI 10.1068/p6437
   Stefanucci JK, 2009, J EXP PSYCHOL HUMAN, V35, P424, DOI 10.1037/a0013894
   STREUBER S., 2009, Eurographics 2009, P1
   THELEN E, 1995, AM PSYCHOL, V50, P79, DOI 10.1037/0003-066X.50.2.79
   Thompson WB, 2004, PRESENCE-TELEOP VIRT, V13, P560, DOI 10.1162/1054746042545292
   Usoh M, 1999, COMP GRAPH, P359, DOI 10.1145/311535.311589
   Van Wyk E., 2009, P 6 INT C COMP GRAPH, P53, DOI DOI 10.1145/1503454.1503465
   WARREN WH, 1984, J EXP PSYCHOL HUMAN, V10, P683, DOI 10.1037/0096-1523.10.5.683
   WARREN WH, 1987, J EXP PSYCHOL HUMAN, V13, P371, DOI 10.1037/0096-1523.13.3.371
   Willemsen P., 2004, P 1 S APPL PERC GRAP, P35, DOI DOI 10.1145/1012551.1012558
   Williams B, 2007, ACM T APPL PERCEPT, V4, DOI 10.1145/1265957.1265961
   Witt JK, 2005, J EXP PSYCHOL HUMAN, V31, P880, DOI 10.1037/0096-1523.31.5.880
   Xie X., 2010, Proc. 7th Symposium on Applied Perception in Graphics and Visualization (APGV), P65, DOI DOI 10.1145/1836248.1836260
   Yee N, 2009, COMMUN RES, V36, P285, DOI 10.1177/0093650208330254
   Zahorik P, 1998, PRESENCE-VIRTUAL AUG, V7, P78, DOI 10.1162/105474698565541
   Zimmons P, 2003, P IEEE VIRT REAL ANN, P293, DOI 10.1109/VR.2003.1191170
NR 63
TC 39
Z9 42
U1 0
U2 21
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD APR
PY 2015
VL 12
IS 2
AR 6
DI 10.1145/2720020
PG 21
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA CH5BA
UT WOS:000354047900002
DA 2024-07-18
ER

PT J
AU Kellnhofer, P
   Ritschel, T
   Vangorp, P
   Myszkowski, K
   Seidel, HP
AF Kellnhofer, Petr
   Ritschel, Tobias
   Vangorp, Peter
   Myszkowski, Karol
   Seidel, Hans-Peter
TI Stereo Day-for-Night: Retargeting Disparity for Scotopic Vision
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Human Factors; Night vision; scotopic vision; stereoscopic 3D
ID DEPTH-DISCRIMINATION THRESHOLDS; LUMINANCE-CONTRAST; IMAGE; ACUITY;
   FUSION
AB Several approaches attempt to reproduce the appearance of a scotopic low-light night scene on a photopic display ("day-for-night") by introducing color desaturation, loss of acuity, and the Purkinje shift toward blue colors. We argue that faithful stereo reproduction of night scenes on photopic stereo displays requires manipulation of not only color but also binocular disparity. To this end, we performed a psychophysics experiment to devise a model of disparity at scotopic luminance levels. Using this model, we can match binocular disparity of a scotopic stereo content displayed on a photopic monitor to the disparity that would be perceived if the scene was actually scotopic. The model allows for real-time computation of common stereo content as found in interactive applications such as simulators or computer games.
C1 [Kellnhofer, Petr; Ritschel, Tobias; Vangorp, Peter; Myszkowski, Karol; Seidel, Hans-Peter] Max Planck Inst Informat, D-66123 Saarbrucken, Germany.
   [Ritschel, Tobias] Univ Saarland, D-66123 Saarbrucken, Germany.
C3 Max Planck Society; Saarland University
RP Kellnhofer, P (corresponding author), Max Planck Inst Informat, Campus E1-4, D-66123 Saarbrucken, Germany.
EM pkellnho@mpi-inf.mpg.de; ritschel@mpi-inf.mpg.de;
   Ipeter.vangorp@mpi-inf.mpg.de; karol@mpi-inf.mpg.de;
   hpseidel@mpi-sb.mpg.de
RI Vangorp, Peter/H-7330-2016
OI Vangorp, Peter/0000-0003-3132-270X
CR [Anonymous], 1838, Philos. Trans. R. Soc., DOI DOI 10.1098/RSTL.1838.0019
   Banks MS, 2004, J NEUROSCI, V24, P2077, DOI 10.1523/JNEUROSCI.3852-02.2004
   BURT PJ, 1983, IEEE T COMMUN, V31, P532, DOI 10.1109/TCOM.1983.1095851
   CORMACK LK, 1991, VISION RES, V31, P2195, DOI 10.1016/0042-6989(91)90172-2
   Didyk P., 2012, ACM T GRAPHIC, V31
   Didyk P, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964991
   Durand F, 2000, SPRING COMP SCI, P219
   Fehn C, 2004, PROC SPIE, V5291, P93, DOI 10.1117/12.524762
   Ferwerda J. A., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P249, DOI 10.1145/237170.237262
   Filippini HR, 2009, J VISION, V9, DOI 10.1167/9.1.8
   FRISBY JP, 1978, PERCEPTION, V7, P423, DOI 10.1068/p070423
   Haro G, 2006, INT J COMPUT VISION, V69, P109, DOI 10.1007/s11263-006-6858-4
   HECKMANN T, 1989, VISION RES, V29, P593, DOI 10.1016/0042-6989(89)90045-X
   Heidrich Wolfgang, ERIK REINHARD
   Hess R., 1990, NIGHT VISION BASIC C
   Hoffman DM, 2008, J VISION, V8, DOI 10.1167/8.3.33
   Kellnhofer P, 2013, COMPUT GRAPH FORUM, V32, P143, DOI 10.1111/cgf.12160
   Kirk AG, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964937
   Kuang JT, 2007, J VIS COMMUN IMAGE R, V18, P406, DOI 10.1016/j.jvcir.2007.06.003
   Lambooij M, 2009, J IMAGING SCI TECHN, V53, DOI 10.2352/J.ImagingSci.Technol.2009.53.3.030201
   Lang M, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778812
   Larson GW, 1997, IEEE T VIS COMPUT GR, V3, P291, DOI 10.1109/2945.646233
   LEGGE GE, 1989, VISION RES, V29, P989, DOI 10.1016/0042-6989(89)90114-4
   LIT A, 1966, J OPT SOC AM, V56, P510, DOI 10.1364/JOSA.56.000510
   LIT A, 1959, J OPT SOC AM, V49, P746, DOI 10.1364/JOSA.49.000746
   Livingstone M., 2002, Vision and art: The biology of seeing
   LIVINGSTONE MS, 1994, VISION RES, V34, P799, DOI 10.1016/0042-6989(94)90217-8
   Mantiuk R, 2006, PROC SPIE, V6057, DOI 10.1117/12.639140
   Mantiuk R, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964935
   OSHEA RP, 1994, PERCEPTION, V23, P771, DOI 10.1068/p230771
   Patel SS, 2009, J OPT SOC AM A, V26, P847, DOI 10.1364/JOSAA.26.000847
   Pattanaik SN, 2000, COMP GRAPH, P47, DOI 10.1145/344779.344810
   Qin D, 2006, OPT REV, V13, P34, DOI 10.1007/s10043-006-0034-5
   Scharstein D, 2002, INT J COMPUT VISION, V47, P7, DOI 10.1023/A:1014573219977
   Shlaer S, 1937, J GEN PHYSIOL, V21, P165, DOI 10.1085/jgp.21.2.165
   Simmons DR, 2002, VISION RES, V42, P1535, DOI 10.1016/S0042-6989(02)00080-9
   Simmons DR, 1997, VISION RES, V37, P1271, DOI 10.1016/S0042-6989(96)00273-8
   Subr K., 2012, P CVMP ACM, P84
   Thompson W. B., 2002, Journal of Graphics Tools, V7, P1, DOI 10.1080/10867651.2002.10487550
   Wandell B. A, 1995, Foundations of vision
   WATSON AB, 1983, PERCEPT PSYCHOPHYS, V33, P113, DOI 10.3758/BF03202828
NR 41
TC 4
Z9 5
U1 0
U2 5
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD JUN
PY 2014
VL 11
IS 3
SI SI
AR 15
DI 10.1145/2644813
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AU0KM
UT WOS:000345311700006
OA Green Published
DA 2024-07-18
ER

PT J
AU Chen, JH
   Allison, RS
AF Chen, Jianhui
   Allison, Robert S.
TI Shape Perception of Thin Transparent Objects with Stereoscopic Viewing
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article; Proceedings Paper
CT 10th Symposium on Applied Perception (SAP)
CY AUG, 2013
CL Trinity Coll, Dublin, IRELAND
HO Trinity Coll
DE Experimentation; Human Factors; 3D shape perception; transparent;
   shape-from-texture; stereoscopic; material perception
ID SURFACES
AB Many materials, including water surfaces, jewels, and glassware exhibit transparent refractions. The human visual system can somehow recover 3D shape from refracted images. While previous research has elucidated various visual cues that can facilitate visual perception of transparent objects, most of them focused on monocular material perception. The question of shape perception of transparent objects is much more complex and few studies have been undertaken, particular in terms of binocular vision.
   In this article, we first design a system for stereoscopic surface orientation estimation with photo-realistic stimuli. It displays pre-rendered stereoscopic images and a real-time S3D (Stereoscopic 3D) shape probe simultaneously. Then we estimate people's perception of the shape of thin transparent objects using a gauge figure task. Our results suggest that people can consistently perceive the surface orientation of thin transparent objects, and stereoscopic viewing improves the precision of estimates. To explain the results, we present an edge-aware orientation map based on image gradients and structure tensors to illustrate the orientation information in images. We also decomposed the normal direction of the surface into azimuth angle and slant angle to explain why additional depth information can improve the accuracy of perceived normal direction.
C1 [Chen, Jianhui; Allison, Robert S.] York Univ, Dept Comp Sci & Engn, Ctr Vis Res, Toronto, ON M3J 1P3, Canada.
C3 York University - Canada
RP Chen, JH (corresponding author), York Univ, Dept Comp Sci & Engn, Ctr Vis Res, 4700 Keele St, Toronto, ON M3J 1P3, Canada.
EM jianhui@cse.yorku.ca; allison@cse.yorku.ca
RI Chen, Jianhui/I-7286-2016
OI Allison, Robert/0000-0002-4485-2665
CR ALLISON R. S, 2009, J VISION, V9, P12
   [Anonymous], P ALV VIS C
   BARROW HG, 1981, P IEEE, V69, P572, DOI 10.1109/PROC.1981.12026
   Belhumeur PN, 1999, INT J COMPUT VISION, V35, P33, DOI 10.1023/A:1008154927611
   Blake A., 1988, Second International Conference on Computer Vision (IEEE Cat. No.88CH2664-1), P394, DOI 10.1109/CCV.1988.590016
   Bradski G., 2008, LEARNING OPENCV
   Bridge H, 2008, CURR OPIN NEUROBIOL, V18, P425, DOI 10.1016/j.conb.2008.09.003
   Cole F, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531334
   Debevec P., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P189, DOI 10.1145/280814.280864
   Flavell L., 2010, Beginning Blender-Open Source 3D Modeling, Animation and Game Design
   Fleming RW, 2011, PSYCHOL SCI, V22, P812, DOI 10.1177/0956797611408734
   Fleming RW, 2004, J VISION, V4, P798, DOI 10.1167/4.9.10
   Fleming RW, 2003, J VISION, V3, P347, DOI 10.1167/3.5.3
   GIBSON JJ, 1950, AM J PSYCHOL, V63, P367, DOI 10.2307/1418003
   Kersten MA, 2006, IEEE T VIS COMPUT GR, V12, P1117, DOI 10.1109/TVCG.2006.139
   Knutsson H, 2011, LECT NOTES COMPUT SC, V6688, P545, DOI 10.1007/978-3-642-21227-7_51
   KOENDERINK JJ, 1992, PERCEPT PSYCHOPHYS, V52, P487, DOI 10.3758/BF03206710
   Koenderink JJ, 2001, PERCEPTION, V30, P431, DOI 10.1068/p3030
   Lee YL, 2011, J VISION, V11, DOI 10.1167/11.9.6
   Marr D., 1982, Visual perception
   MURYY A A, 2012, Journal of Vision, V12, P869, DOI DOI 10.1167/12.9.869.
   Norman JF, 1996, PERCEPTION, V25, P381, DOI 10.1068/p250381
   SCHNEIDER S, 2012, STEREOSCOPIC CAMERAS, V2, P6
   STEVENS KA, 1983, BIOL CYBERN, V46, P183, DOI 10.1007/BF00336800
   Sun J, 1998, NAT NEUROSCI, V1, P183, DOI 10.1038/630
   Thompson WB, 2011, VISUAL PERCEPTION FROM A COMPUTER GRAPHICS PERSPECTIVE, P1
   Todd JT, 2004, PSYCHOL SCI, V15, P40, DOI 10.1111/j.0963-7214.2004.01501007.x
   VANNES FL, 1967, J OPT SOC AM, V57, P401, DOI 10.1364/JOSA.57.000401
NR 28
TC 4
Z9 4
U1 0
U2 12
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD AUG
PY 2013
VL 10
IS 3
SI SI
AR 15
DI 10.1145/2506206.2506208
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 206FS
UT WOS:000323504700004
DA 2024-07-18
ER

PT J
AU Couture, V
   Langer, MS
   Roy, S
AF Couture, Vincent
   Langer, Michael S.
   Roy, Sebastien
TI Perception of Blending in Stereo Motion Panoramas
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Experimentation; Measurement; Performance; Stereo; perception; visual
   perception; blending; motion; omnistereo
ID IMAGE
AB Most methods for synthesizing panoramas assume that the scene is static. A few methods have been proposed for synthesizing stereo or motion panoramas, but there has been little attempt to synthesize panoramas that have both stereo and motion. One faces several challenges in synthesizing stereo motion panoramas, for example, to ensure temporal synchronization between left and right views in each frame, to avoid spatial distortion of moving objects, and to continuously loop the video in time. We have recently developed a stereo motion panorama method that tries to address some of these challenges. The method blends space-time regions of a video XYT volume, such that the blending regions are distinct and translate over time. This article presents a perception experiment that evaluates certain aspects of the method, namely how well observers can detect such blending regions. We measure detection time thresholds for different blending widths and for different scenes, and for monoscopic versus stereoscopic videos. Our results suggest that blending may be more effective in image regions that do not contain coherent moving objects that can be tracked over time. For example, we found moving water and partly transparent smoke were more effectively blended than swaying branches. We also found that performance in the task was roughly the same for mono versus stereo videos.
C1 [Couture, Vincent; Roy, Sebastien] Univ Montreal, Montreal, PQ H3T 1J4, Canada.
   [Langer, Michael S.] McGill Univ, Montreal, PQ H3A 0E9, Canada.
C3 Universite de Montreal; McGill University
RP Couture, V (corresponding author), Univ Montreal, Pavillon Andre Aisenstadt DIRO,2920 Chemin Tour, Montreal, PQ H3T 1J4, Canada.
EM chapdelv@iro.umontreal.ca
CR Agarwala A, 2005, ACM T GRAPHIC, V24, P821, DOI 10.1145/1073204.1073268
   BURT PJ, 1983, ACM T GRAPHIC, V2, P217, DOI 10.1145/245.247
   COUTURE V., 2010, P 10 WORKSH OMN VIS
   Couture V, 2011, IEEE I CONF COMP VIS, P1251, DOI 10.1109/ICCV.2011.6126376
   Daly SJ, 2011, IEEE T BROADCAST, V57, P347, DOI 10.1109/TBC.2011.2127630
   Derpanis KG, 2012, IEEE T PATTERN ANAL, V34, P1193, DOI 10.1109/TPAMI.2011.221
   DIXON WJ, 1948, J AM STAT ASSOC, V43, P109, DOI 10.2307/2280071
   Held RT, 2008, APGV 2008: PROCEEDINGS OF THE SYMPOSIUM ON APPLIED PERCEPTION IN GRAPHICS AND VISUALIZATION, P23
   Huang HC, 1998, GRAPH MODEL IM PROC, V60, P196, DOI 10.1006/gmip.1998.0467
   ISHIGURO H, 1992, IEEE T PATTERN ANAL, V14, P257, DOI 10.1109/34.121792
   Kingdom F. A. A., 2010, Psychophysics: A Practical Introduction, V1
   Kwatra V, 2003, ACM T GRAPHIC, V22, P277, DOI 10.1145/882262.882264
   Mendiburu Bernard., 2009, 3D Movie Making: Stereoscopic Digital Cinema From Scrip to Screen
   Naemura T, 1998, 1998 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL 1, P903, DOI 10.1109/ICIP.1998.723666
   Peleg S, 2001, IEEE T PATTERN ANAL, V23, P279, DOI 10.1109/34.910880
   Pérez P, 2003, ACM T GRAPHIC, V22, P313, DOI 10.1145/882262.882269
   Rav-Acha A, 2007, IEEE T PATTERN ANAL, V29, P1789, DOI 10.1109/TPAMI.2007.1091
   Schödl A, 2000, COMP GRAPH, P489, DOI 10.1145/344779.345012
   Seitz SM, 2002, INT J COMPUT VISION, V48, P159, DOI 10.1023/A:1016342731674
   Szeliski R, 2011, TEXTS COMPUT SCI, P1, DOI 10.1007/978-1-84882-935-0
   Tardif JP, 2003, FOURTH INTERNATIONAL CONFERENCE ON 3-D DIGITAL IMAGING AND MODELING, PROCEEDINGS, P217, DOI 10.1109/im.2003.1240253
   Vangorp P, 2011, COMPUT GRAPH FORUM, V30, P1241, DOI 10.1111/j.1467-8659.2011.01983.x
   VISION3D, 2011, VISION3D
   WETHERILL GB, 1965, BRIT J MATH STAT PSY, V18, P1, DOI 10.1111/j.2044-8317.1965.tb00689.x
   Woeste H., 2009, MASTERING DIGITAL PA
NR 25
TC 2
Z9 2
U1 0
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD JUL
PY 2012
VL 9
IS 3
AR 15
DI 10.1145/2325722.2325728
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 984EE
UT WOS:000307171700006
DA 2024-07-18
ER

PT J
AU Shamir, L
   Macura, T
   Orlov, N
   Eckley, DM
   Goldberg, IG
AF Shamir, Lior
   Macura, Tomasz
   Orlov, Nikita
   Eckley, D. Mark
   Goldberg, Ilya G.
TI Impressionism, Expressionism, Surrealism: Automated Recognition of
   Painters and Schools of Art
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Design; Experimentation; Measurement; Art; painting; image similarity;
   perceptual reasoning
ID CLASSIFICATION; FEATURES
AB We describe a method for automated recognition of painters and schools of art based on their signature styles and studied the computer-based perception of visual art. Paintings of nine artists, representing three different schools of art-impressionism, surrealism and abstract expressionism-were analyzed using a large set of image features and image transforms. The computed image descriptors were assessed using Fisher scores, and the most informative features were used for the classification and similarity measurements of paintings, painters, and schools of art. Experimental results show that the classification accuracy when classifying paintings into nine painter classes is 77%, and the accuracy of associating a given painting with its school of art is 91%. An interesting feature of the proposed method is its ability to automatically associate different artists that share the same school of art in an unsupervised fashion. The source code used for the image classification and image similarity described in this article is available for free download.
C1 [Shamir, Lior; Macura, Tomasz; Orlov, Nikita; Eckley, D. Mark; Goldberg, Ilya G.] NIA, Image Informat Grp, Genet Lab, NIH, Baltimore, MD 21224 USA.
C3 National Institutes of Health (NIH) - USA; NIH National Institute on
   Aging (NIA)
RP Shamir, L (corresponding author), NIA, Image Informat Grp, Genet Lab, NIH, 333 Cassell Dr, Baltimore, MD 21224 USA.
EM shamirl@mail.nih.gov
RI Eckley, David Mark/I-9245-2019; Orlov, Nikita/AAF-4404-2021; Eckley, D.
   Mark/M-3526-2014; Goldberg, Ilya/H-5307-2011
OI Eckley, D. Mark/0000-0003-2296-5164; Goldberg, Ilya/0000-0001-8514-6110
FU NIH, National Institute on Aging
FX This research was supported entirely by the Intramural Research Program
   of the NIH, National Institute on Aging.
CR [Anonymous], 2006, P IPCV
   [Anonymous], 1994, Table of integrals, series, and products
   [Anonymous], 1990, 2 DIMENSIONAL SIGNAL
   Barnard K, 2001, PROC CVPR IEEE, P434
   Bhattacharya J, 2002, COGNITIVE BRAIN RES, V13, P179, DOI 10.1016/S0926-6410(01)00110-0
   Bishop Christopher M, 2006, PATTERN RECOGN, V128, P1
   Blake A., 1992, Active Vision
   Chang EY, 2000, IEEE WORKSHOP ON CONTENT-BASED ACCESS OF IMAGE AND VIDEO LIBRARIES, PROCEEDINGS, P101, DOI 10.1109/IVL.2000.853848
   Cherkassky V, 1997, IEEE Trans Neural Netw, V8, P1564, DOI 10.1109/TNN.1997.641482
   Felsenstein, 2002, PHYLIP PHYLOGENY INF
   Gabor D., 1946, Journal of the Institution of Electrical Engineers-part III: radio and communication engineering, V93, P429, DOI [DOI 10.1049/JI-3-2.1946.0074, 10.1049/ji-3-2.1946.0074]
   GRAY SB, 1971, IEEE T COMPUT, VC 20, P551, DOI 10.1109/T-C.1971.223289
   Grigorescu SE, 2002, IEEE T IMAGE PROCESS, V11, P1160, DOI 10.1109/TIP.2002.804262
   Gurevich I. B., 2006, Pattern Recognition and Image Analysis, V16, P265, DOI 10.1134/S1054661806030023
   Hadjidemetriou E, 2001, PROC CVPR IEEE, P702
   HARALICK RM, 1973, IEEE T SYST MAN CYB, VSMC3, P610, DOI 10.1109/TSMC.1973.4309314
   Joachims T., 2006, P 12 ACM SIGKDD INT, P217, DOI [10.1145/1150402.1150429, DOI 10.1145/1150402.1150429]
   Jones-Smith K, 2006, NATURE, V444, pE9, DOI 10.1038/nature05398
   Kammerer P, 2007, PATTERN RECOGN LETT, V28, P710, DOI 10.1016/j.patrec.2006.08.003
   Keren D, 2002, INT C PATT RECOG, P474, DOI 10.1109/ICPR.2002.1048341
   KIRSCH JL, 1988, LEONARDO, V21, P437, DOI 10.2307/1578708
   Li J, 2004, IEEE T IMAGE PROCESS, V13, P338, DOI 10.1109/TIP.2003.821349
   Litt R., 1995, ARTFUL EYE
   Murphy RF, 2001, 2ND ANNUAL IEEE INTERNATIONAL SYMPOSIUM ON BIOINFORMATICS AND BIOENGINEERING, PROCEEDINGS, P119, DOI 10.1109/BIBE.2001.974420
   Orlov Nikita, 2007, Vision Systems - Segmentation and Pattern Recognition, P221
   Orlov N, 2008, PATTERN RECOGN LETT, V29, P1684, DOI 10.1016/j.patrec.2008.04.013
   OTSU N, 1979, IEEE T SYST MAN CYB, V9, P62, DOI 10.1109/TSMC.1979.4310076
   Prewitt J. M., 1970, Picture processing and psychopictorics, V10, P15
   Ramachandran V. S., 1999, Journal of Consciousness Studies, V6, P15
   REWALD J, 1963, PISSARRO, P118
   Rodenacker K, 2003, ANAL CELL PATHOL, V25, P1
   Shamir L, 2008, SOURCE CODE BIOL MED, V3, DOI 10.1186/1751-0473-3-13
   Solso RL, 2000, J CONSCIOUSNESS STUD, V7, P75
   Solso RobertL., 1994, Cognition and the Visual Arts
   Tamura H., 1978, IEEE Transactions on Systems, Man and Cybernetics, VSMC-8, P460, DOI 10.1109/TSMC.1978.4309999
   Taylor RP, 2007, PATTERN RECOGN LETT, V28, P695, DOI 10.1016/j.patrec.2006.08.012
   Taylor RP, 1999, NATURE, V399, P422, DOI 10.1038/20833
   TEAGUE MR, 1980, J OPT SOC AM, V70, P920, DOI 10.1364/JOSA.70.000920
   van den Herik HJ, 2000, STUD FUZZ SOFT COMP, V45, P129
   Wallraven C, 2007, APGV 2007: SYMPOSIUM ON APPLIED PERCEPTION IN GRAPHICS AND VISUALIZATION, PROCEEDINGS, P115
   Widjaja I, 2003, IEEE IMAGE PROC, P845
   Zeki Semir., 1999, Inner Vision
NR 42
TC 118
Z9 131
U1 2
U2 57
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD FEB
PY 2010
VL 7
IS 2
AR 8
DI 10.1145/1670671.1670672
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Arts &amp; Humanities Citation Index (A&amp;HCI)
SC Computer Science
GA 563HF
UT WOS:000275118100001
DA 2024-07-18
ER

PT J
AU Grave, J
   Bremond, R
AF Grave, Justine
   Bremond, Roland
TI A Tone-Mapping Operator for Road Visibility Experiments
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Experimentation; Human Factors; Measurement; HDR images; road
   visibility; visual performance; psychophysics
AB One may wish to use computer graphic images to carry out road visibility studies. Unfortunately, most display devices still have a limited luminance dynamic range, especially in driving simulators. In this paper, we propose a tone-mapping operator (TMO) to compress the luminance dynamic range while preserving the driver's performance for a visual task relevant for a driving situation. We address three display issues of some consequences for road image display: luminance dynamics, image quantization, and high minimum displayable luminance. Our TMO characterizes the effects of local adaptation with a bandpass decomposition of the image using a Laplacian pyramid, and processes the levels separately in order to mimic the human visual system. The contrast perception model uses the visibility level, a usual index in road visibility engineering applications. To assess our algorithm, a psychophysical experiment devoted to a target detection task was designed. Using a Landolt ring, the visual performances of 30 observers were measured: they stared first at a high-dynamic range image and then at the same image processed by a TMO and displayed on a low-dynamic range monitor, for comparison. The evaluation was completed with a visual appearance evaluation. Our operator gives good performances for three typical road situations (one in daylight and two at night), after comparison with four standard TMOs from the literature. The psychovisual assessment of our TMO is limited to these driving situations.
C1 [Grave, Justine; Bremond, Roland] Lab Cent Ponts & Chaussees, Lyon, France.
C3 Universite Gustave-Eiffel; Laboratoire Central des Ponts et Chaussees
   (LCPC)
RP Grave, J (corresponding author), Sagem Secur, Paris, France.
EM justine.grave@sagem.com; roland.bremond@lcpc.fr
RI Bremond, Roland/AAT-1408-2021
OI Bremond, Roland/0000-0003-3150-7624
CR Adrian W., 1989, Lighting Research and Technology, V21, P181
   [Anonymous], 2005, High Dynamic Range Imaging: Acquisition, Display, and Image-Based Lighting (The Morgan Kaufmann Series in Computer Graphics
   [Anonymous], 1994, Graph. Gems, DOI DOI 10.1016/B978-0-12-336156-1.50054-9
   [Anonymous], 1996, P 23 ANN C COMP GRAP, DOI DOI 10.1145/237170.237262
   [Anonymous], 2006, P DRIV SIM C PAR FRA
   BREMOND R, 2002, P DRIV SIM C 2002 IN
   BURT PJ, 1983, IEEE T COMMUN, V31, P532, DOI 10.1109/TCOM.1983.1095851
   CAMPBELL FW, 1968, J PHYSIOL-LONDON, V197, P551, DOI 10.1113/jphysiol.1968.sp008574
   *CO, 1988, 74 CO
   *CO, 1996, 122 CO
   *CO, 1981, 1921 CO
   *CO, 1999, 1355 CO
   *CO, 1992, 95 CO
   Drago Frederic., 2003, Proceedings of ACM SIGGRAPH 2003 Sketches Applications, P1
   FERWERDA JA, 2001, IEEE COMPUT GRAPH, P21
   GINSBURG AP, 1986, HDB PERCEPTION HUMAN, P1
   GRAVE J, 2005, APGV 05
   HILLS BL, 1980, PERCEPTION, V9, P183, DOI 10.1068/p090183
   Hoc J., 2001, THEORETICAL ISSUES E, V2, P278, DOI DOI 10.1080/14639220110104970
   Ishii E, 2003, REC RES DEV HAEMATOL, V1, P13
   KELLY DH, 1985, J OPT SOC AM A, V2, P216, DOI 10.1364/JOSAA.2.000216
   Kuang JT, 2004, 12TH COLOR IMAGING CONFERENCE: COLOR SCIENCE AND ENGINEERING SYSTEMS, TECHNOLOGIES, APPLICATIONS, P315
   Larson GW, 1997, IEEE T VIS COMPUT GR, V3, P291, DOI 10.1109/2945.646233
   Ledda P., 2005, P ACM SIGGRAPH 05, P249
   Ledda P., 2004, Proceedings of the 3rd international conference on Com- puter graphics, virtual reality, visualisation and interaction in Africa, P151
   Li YZ, 2005, ACM T GRAPHIC, V24, P836, DOI 10.1145/1073204.1073271
   MOON P, 1945, J OPT SOC AM, V35, P233, DOI 10.1364/JOSA.35.000233
   *MOVE, 2005, PERF BAS MOD MES PHO
   Pattanaik S. N., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P287, DOI 10.1145/280814.280922
   Pattanaik SN, 2000, COMP GRAPH, P47, DOI 10.1145/344779.344810
   PELI E, 1990, J OPT SOC AM A, V7, P2032, DOI 10.1364/JOSAA.7.002032
   Reinhard E, 2002, ACM T GRAPHIC, V21, P267, DOI 10.1145/566570.566575
   Seetzen H, 2004, ACM T GRAPHIC, V23, P760, DOI 10.1145/1015706.1015797
   SHAPLEY R, 1991, SPATIAL VISION, P290
   Smith K, 2006, COMPUT GRAPH FORUM, V25, P427, DOI 10.1111/j.1467-8659.2006.00962.x
   Spencer G., 1995, P ACM SIGGRAPH 95, P325
   VIENOT F, 2002, DRIV SIM C INRETS RE
   Yoshida A, 2005, PROC SPIE, V5666, P192, DOI 10.1117/12.587782
   Yoshida A, 2006, COMPUT GRAPH FORUM, V25, P415, DOI 10.1111/j.1467-8659.2006.00961.x
NR 39
TC 6
Z9 6
U1 0
U2 6
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD MAY
PY 2008
VL 5
IS 2
AR 12
DI 10.1145/1279920.1361704
PG 24
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 450YJ
UT WOS:000266437600006
DA 2024-07-18
ER

PT J
AU Ho, HN
   Jones, LA
AF Ho, Hsin-Ni
   Jones, Lynette A.
TI Development and Evaluation of a Thermal Display for Material
   Identification and Discrimination
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Design; Experimentation; Human factors; Measurement; Performance;
   Theory; Haptic interface; material identification; semi-infinite body
   model; thermal display; thermal feedback; thermal perception;
   hand-object interaction; virtual environment
AB The objective of this study was to develop and evaluate a thermal display that assists in object identification in virtual environments by simulating the thermal cues associated with making contact with materials with different thermal properties. The thermal display was developed based on a semi-infinite body model. Three experiments were conducted to evaluate the performance of the display. The first experiment compared the ability of subjects' to identify various materials, which were presented physically or simulated with the thermal display. The second experiment examined the capacity of subjects to discriminate between a real and simulated material based on thermal cues. In the third experiment, the changes in skin temperature that occurred when making contact with real and simulated materials were measured to evaluate how these compare to theoretical predictions. The results indicated that there was no significant difference in material identification and discrimination when subjects were presented with real or simulated materials. The changes in skin temperature were comparable for real and simulated materials and were related to the contact coefficient of the material palpated, consistent with the semi-infinite body model. These findings suggest that a thermal display is capable of facilitating object recognition when visual cues are limited.
C1 [Ho, Hsin-Ni; Jones, Lynette A.] MIT, Dept Mech Engn, Cambridge, MA 02139 USA.
C3 Massachusetts Institute of Technology (MIT)
RP Ho, HN (corresponding author), MIT, Dept Mech Engn, 77 Massachusetts Ave,Room 3-137, Cambridge, MA 02139 USA.
EM ljones@mit.edu
RI Ho, Hsin-Ni/AAV-5590-2021
OI Ho, Hsin-Ni/0000-0002-9940-9089
CR [Anonymous], 1981, THERMORECEPTION TEMP
   BenaliKhoudja M, 2004, IRCICA INT SCI WORKS
   BENALIKHOUDJA M, 2003, 2003 INT S MICR HUM, P153
   Brown M.B., 1974, Journal of the American Statistical Association, V69, P364, DOI DOI 10.1137/1003016
   Caldwell DG, 1996, IEEE INT CONF ROBOT, P3215, DOI 10.1109/ROBOT.1996.509202
   CALDWELL DG, 1993, PROCEEDINGS : IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION, VOLS 1-3, P955, DOI 10.1109/ROBOT.1993.292099
   Dionisio J, 1997, COMPUT GRAPH, V21, P459, DOI 10.1016/S0097-8493(97)00029-0
   DYCK PJ, 1974, NEUROLOGY, V24, P325, DOI 10.1212/WNL.24.4.325
   EBERHART RC, 1985, HEAT TRANSFER MED BI, V1, P269
   Ho H, 2004, P ANN INT IEEE EMBS, V26, P2462
   Ho HN, 2006, PERCEPT PSYCHOPHYS, V68, P118, DOI 10.3758/BF03193662
   Incropera F.P., 1990, FUNDAMENTALS HEAT MA
   Ino S., 1993, Proceedings 2nd IEEE International Workshop on Robot and Human Communication (Cat. No.93TH0577-7), P220, DOI 10.1109/ROMAN.1993.367718
   Jones LA, 2003, 11TH SYMPOSIUM ON HAPTIC INTERFACES FOR VIRTUAL ENVIRONMENT AND TELEOPERATOR SYSTEMS - HAPTICS 2003, PROCEEDINGS, P171, DOI 10.1109/HAPTIC.2003.1191267
   Kron A, 2003, 11TH SYMPOSIUM ON HAPTIC INTERFACES FOR VIRTUAL ENVIRONMENT AND TELEOPERATOR SYSTEMS - HAPTICS 2003, PROCEEDINGS, P16, DOI 10.1109/HAPTIC.2003.1191219
   Lederman SJ, 1997, J EXP PSYCHOL HUMAN, V23, P1680, DOI 10.1037/0096-1523.23.6.1680
   Mills A.F., 1999, Heat Transfer, V2
   OTTENSMEYER M, 1997, P 2 PHANTOM US GROUP
   Stevens JC., 1991, PSYCHOL TOUCH, P61
   Verrillo RT, 1998, SOMATOSENS MOT RES, V15, P93, DOI 10.1080/08990229870826
   Yamamoto A, 2004, IEEE INT CONF ROBOT, P1536, DOI 10.1109/ROBOT.2004.1308042
NR 21
TC 46
Z9 53
U1 0
U2 9
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD JUL
PY 2007
VL 4
IS 2
AR 13
DI 10.1145/1265957.1265962
PG 24
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA V04IM
UT WOS:000207052100005
DA 2024-07-18
ER

PT J
AU Kohm, K
   Porter, J
   Robb, A
AF Kohm, Kristopher
   Porter, John
   Robb, Andrew
TI Sensitivity to Hand Offsets and Related Behavior in Virtual Environments
   over Time
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article; Proceedings Paper
CT 19th Symposium on Applied Perception (SAP)
CY SEP, 2022
CL ELECTR NETWORK
DE Body awareness; hand offsets; calibration; longitudinal
ID REALITY; PERFORMANCE; RETENTION; MOVEMENTS; PACKAGE; HEAD; EYE
AB This work explored how users' sensitivity to offsets in their avatars' virtual hands changes as they gain exposure to virtual reality. We conducted an experiment using a two-alternative forced choice (2-AFC) design over the course of 4 weeks, split into four sessions. The trials in each session had a variety of eight offset distances paired with eight offset directions (across a two-dimensional plane). While we did not find evidence that users became more sensitive to the offsets over time, we did find evidence of behavioral changes. Specifically, participants' head-hand coordination and completion time varied significantly as the sessions went on. We discuss the implications of both results and how they could influence our understanding of long-term calibration for perception-action coordination in virtual environments.
C1 [Kohm, Kristopher; Porter, John; Robb, Andrew] Clemson Univ, Sch Comp, 100 McAdams Hall, Clemson, SC 29634 USA.
C3 Clemson University
RP Kohm, K (corresponding author), Clemson Univ, Sch Comp, 100 McAdams Hall, Clemson, SC 29634 USA.
EM kckohm@clemson.edu; jjporte@clemson.edu; arobb@clemson.edu
OI Porter III, John/0000-0003-2534-3619; Robb, Andrew/0000-0002-0398-5576;
   Kohm, Kristopher/0000-0002-7525-991X
FU US National Science Foundation (CISE HCC) [1717937]; Div Of Information
   & Intelligent Systems; Direct For Computer & Info Scie & Enginr
   [1717937] Funding Source: National Science Foundation
FX This work was supported in part by the US National Science Foundation
   (CISE HCC) under Grant #1717937.
CR Azmandian M, 2016, 34TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, CHI 2016, P1968, DOI 10.1145/2858036.2858226
   Azmandian M, 2016, 2016 IEEE 2ND WORKSHOP ON EVERYDAY VIRTUAL REALITY (WEVR), P9, DOI 10.1109/WEVR.2016.7859537
   Bailenson JN, 2006, PRESENCE-TELEOP VIRT, V15, P699, DOI 10.1162/pres.15.6.699
   Bates D, 2015, J STAT SOFTW, V67, P1, DOI 10.18637/jss.v067.i01
   Benda B, 2020, INT SYM MIX AUGMENT, P269, DOI 10.1109/ISMAR50242.2020.00050
   Bingham GP, 1999, J EXP PSYCHOL HUMAN, V25, P1331
   Blau J., 2022, Introduction to Ecological Psychology: a Lawful Approach to Perceiving, Acting, and Cognizing
   BRING J, 1994, AM STAT, V48, P209, DOI 10.2307/2684719
   Burns E, 2005, P IEEE VIRT REAL ANN, P3
   Cnaan A, 1997, STAT MED, V16, P2349, DOI 10.1002/(SICI)1097-0258(19971030)16:20<2349::AID-SIM667>3.0.CO;2-E
   Dewez D, 2019, INT SYM MIX AUGMENT, P123, DOI 10.1109/ISMAR.2019.00-12
   Mendes FAD, 2012, PHYSIOTHERAPY, V98, P217, DOI 10.1016/j.physio.2012.06.001
   Duzmanska N, 2018, FRONT PSYCHOL, V9, DOI 10.3389/fpsyg.2018.02132
   Esmaeili S, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR 2020), P453, DOI [10.1109/VR46266.2020.1581285352835, 10.1109/VR46266.2020.00-38]
   Fang Y, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0121035
   Faria A. L., 2016, P 11 INT C DISABILIT
   Feuchtner T, 2018, UIST 2018: PROCEEDINGS OF THE 31ST ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P31, DOI 10.1145/3242587.3242594
   Freeman D, 2013, PSYCHOL MED, V43, P2673, DOI 10.1017/S003329171300038X
   Gibson JJ., 1977, The theory of affordances
   Ginestet C, 2011, J ROY STAT SOC A, V174, P245, DOI 10.1111/j.1467-985X.2010.00676_9.x
   Gonzalez EJ, 2019, 25TH ACM SYMPOSIUM ON VIRTUAL REALITY SOFTWARE AND TECHNOLOGY (VRST 2019), DOI 10.1145/3359996.3364248
   Hayashi D, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P386, DOI [10.1109/VR.2019.8797989, 10.1109/vr.2019.8797989]
   Hillel T., 2021, 21 SWISS TRANSP RES, P1, DOI [10.17863/CAM.40710, DOI 10.17863/CAM.40710]
   Huang XL, 2021, SCI REP-UK, V11, DOI 10.1038/s41598-021-81458-3
   Khojasteh Negar, 2021, FRONTIERS VIRTUAL RE, V2, P53, DOI DOI 10.3389/FRVIR.2021.643331
   Kim HJ, 2020, J MED INTERNET RES, V22, DOI 10.2196/23024
   Krueger Charlene, 2004, Biol Res Nurs, V6, P151, DOI 10.1177/1099800404267682
   Kuznetsova A, 2017, J STAT SOFTW, V82, P1, DOI 10.18637/jss.v082.i13
   Li YJ, 2021, 2021 IEEE VIRTUAL REALITY AND 3D USER INTERFACES (VR), P95, DOI [10.1109/VR50410.2021.00030, 10.1109/ISHC54333.2021.00026]
   Linares D, 2016, R J, V8, P122
   Ludecke D, 2021, J. Open Source Softw., V6, P3139, DOI DOI 10.21105/JOSS.03139
   Matthews BJ, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P19, DOI [10.1109/vr.2019.8797974, 10.1109/VR.2019.8797974]
   Meteyard L, 2020, J MEM LANG, V112, DOI 10.1016/j.jml.2020.104092
   Moustafa F, 2018, 24TH ACM SYMPOSIUM ON VIRTUAL REALITY SOFTWARE AND TECHNOLOGY (VRST 2018), DOI 10.1145/3281505.3281527
   Nakagawa S, 2017, J R SOC INTERFACE, V14, DOI 10.1098/rsif.2017.0213
   Nordahl Rolf, 2019, P IEEE VR WORKSHOP I
   Ogawa N, 2021, IEEE T VIS COMPUT GR, V27, P3182, DOI 10.1109/TVCG.2020.2964758
   Pelz J, 2001, EXP BRAIN RES, V139, P266, DOI 10.1007/s002210100745
   Porter J, 2019, CHI PLAY'19: PROCEEDINGS OF THE ANNUAL SYMPOSIUM ON COMPUTER-HUMAN INTERACTION IN PLAY, P277, DOI 10.1145/3311350.3347159
   Porter J, 2018, PROCEEDINGS OF THE 2018 ANNUAL SYMPOSIUM ON COMPUTER-HUMAN INTERACTION IN PLAY (CHI PLAY 2018), P405, DOI 10.1145/3242671.3242677
   Pournelle G. H., 1953, Journal of Mammalogy, V34, P133, DOI 10.1890/0012-9658(2002)083[1421:SDEOLC]2.0.CO;2
   Prins N., 2016, PSYCHOPHYSICS PRACTI
   Ricca A, 2021, 2021 IEEE VIRTUAL REALITY AND 3D USER INTERFACES (VR), P103, DOI 10.1109/VR50410.2021.00031
   Rohde M, 2011, PLOS ONE, V6, DOI 10.1371/journal.pone.0021659
   SATTERTHWAITE FE, 1946, BIOMETRICS BULL, V2, P110, DOI 10.2307/3002019
   SCHWARZ G, 1978, ANN STAT, V6, P461, DOI 10.1214/aos/1176344136
   Smith SJ, 2016, NURS EDUC PERSPECT, V37, P210, DOI 10.1097/01.NEP.0000000000000035
   Steinicke F, 2014, P 2 ACM S SPAT US IN, P66, DOI DOI 10.1145/2659766.2659767
   Stillman B.C., 2002, Physiotherapy, V88, P667, DOI [DOI 10.1016/S0031-9406(05)60109-5, 10.1016/S0031-9406(05)60109-5]
   Suhail M, 2017, IEEE SYMP 3D USER, P245, DOI 10.1109/3DUI.2017.7893363
   Takala TM, 2016, INFORM EDUC, V15, P287, DOI 10.15388/infedu.2016.15
   Tarnanas I, 2013, JMIR SERIOUS GAMES, V1, P16, DOI 10.2196/games.2778
   Venkatesh V, 2002, PERS PSYCHOL, V55, P661, DOI 10.1111/j.1744-6570.2002.tb00125.x
   Voeten C. C., 2019, R package version, V1, P1
   Warren WH, 2006, PSYCHOL REV, V113, P358, DOI 10.1037/0033-295X.113.2.358
   Winkler-Schwartz A, 2016, J SURG EDUC, V73, P942, DOI 10.1016/j.jsurg.2016.04.013
   ZENNER A, 2021, HART VIRTUAL REALITY, DOI DOI 10.1145/3411763.3451814
   Zenner A, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P47, DOI [10.1109/vr.2019.8798143, 10.1109/VR.2019.8798143]
   Zielasko D, 2021, 2021 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES ABSTRACTS AND WORKSHOPS (VRW 2021), P165, DOI 10.1109/VRW52623.2021.00038
NR 59
TC 2
Z9 2
U1 0
U2 2
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD OCT
PY 2022
VL 19
IS 4
SI SI
AR 17
DI 10.1145/3561055
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 6J6QI
UT WOS:000886946700004
DA 2024-07-18
ER

PT J
AU Reed, CM
   Tan, HZ
   Jiao, Y
   Perez, ZD
   Wilson, EC
AF Reed, Charlotte M.
   Tan, Hong Z.
   Jiao, Yang
   Perez, Zachary D.
   Wilson, E. Courtenay
TI Identification of Words and Phrases Through a Phonemic-Based Haptic
   Display: Effects of Inter-Phoneme and Inter-Word Interval Durations
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Human haptics; speech communication; phoneme codes; tactile devices
ID TADOMA METHOD; HEARING; SPEECH; COMMUNICATION; VIBROTACTILE;
   VARIABILITY; ACQUISITION; VOCABULARY; RECEPTION; MASKING
AB Stand-alone devices for tactile speech reception serve a need as communication aids for persons with profound sensory impairments as well as in applications such as human-computer interfaces and remote communication when the normal auditory and visual channels are compromised or overloaded. The current research is concerned with perceptual evaluations of a phoneme-based tactile speech communication device in which a unique tactile code was assigned to each of the 24 consonants and 15 vowels of English. The tactile phonemic display was conveyed through an array of 24 tactors that stimulated the dorsal and ventral surfaces of the forearm. Experiments examined the recognition of individual words as a function of the inter-phoneme interval (Study 1) and two-word phrases as a function of the inter-word interval (Study 2). Following an average training period of 4.3 hrs on phoneme and word recognition tasks, mean scores for the recognition of individual words in Study 1 ranged from 87.7% correct to 74.3% correct as the inter-phoneme interval decreased from 300 to 0 ms. In Study 2, following an average of 2.5 hours of training on the two-word phrase task, both words in the phrase were identified with an accuracy of 75% correct using an inter-word interval of 1 sec and an inter-phoneme interval of 150 ms. Effective transmission rates achieved on this task were estimated to be on the order of 30 to 35 words/min.
C1 [Reed, Charlotte M.; Perez, Zachary D.; Wilson, E. Courtenay] MIT, Res Lab Elect, 77 Massachusetts Ave, Cambridge, MA 02139 USA.
   [Tan, Hong Z.; Jiao, Yang] Purdue Univ, Sch Elect & Comp Engn, Hapt Interface Res Lab, 465 Northwestern Ave, W Lafayette, IN 47907 USA.
   [Reed, Charlotte M.; Perez, Zachary D.; Wilson, E. Courtenay] MIT, Room 36-751,77 Massachusetts Ave, Cambridge, MA 02139 USA.
   [Jiao, Yang] Tsinghua Univ, Future Lab, 160 Cheng Fu Rd, Beijing 100086, Peoples R China.
C3 Massachusetts Institute of Technology (MIT); Purdue University System;
   Purdue University; Massachusetts Institute of Technology (MIT); Tsinghua
   University
RP Reed, CM (corresponding author), MIT, Res Lab Elect, 77 Massachusetts Ave, Cambridge, MA 02139 USA.
EM cmreed@mit.edu; hongtan@purdue.edu; jymars@live.cn; zdperez@gmail.com;
   ecwilson828@yahoo.com
OI Reed, Charlotte/0000-0003-1680-1913
FU Facebook Inc., Menlo Park, CA; National Science Foundation through NSF
   [1954842, 1954886]; Direct For Computer & Info Scie & Enginr; Div Of
   Information & Intelligent Systems [1954842, 1954886] Funding Source:
   National Science Foundation
FX This work was partially supported by a research grant funded by Facebook
   Inc., Menlo Park, CA, and by support from the National Science
   Foundation through NSF award numbers 1954842 and 1954886.
CR [Anonymous], 1899, PSYCHOL REV, DOI DOI 10.1037/H0073117
   Benzeghiba M, 2007, SPEECH COMMUN, V49, P763, DOI 10.1016/j.specom.2007.02.006
   BERNSTEIN LE, 1991, J ACOUST SOC AM, V90, P2971, DOI 10.1121/1.401771
   BOOTHROYD A, 1989, VOLTA REV, V91, P101
   Brooks P L, 1986, J Rehabil Res Dev, V23, P129
   BROOKS PL, 1983, J ACOUST SOC AM, V74, P34, DOI 10.1121/1.389685
   BROOKS PL, 1985, J ACOUST SOC AM, V77, P1576, DOI 10.1121/1.392000
   Chen J., 2018, P ACM S APPL PERC SA
   de Vargas MF, 2019, 2019 IEEE WORLD HAPTICS CONFERENCE (WHC), P610, DOI [10.1109/whc.2019.8816145, 10.1109/WHC.2019.8816145]
   Dunkelberger N, 2021, IEEE T HAPTICS, V14, P188, DOI 10.1109/TOH.2020.3009581
   Dunkelberger N, 2018, ISWC'18: PROCEEDINGS OF THE 2018 ACM INTERNATIONAL SYMPOSIUM ON WEARABLE COMPUTERS, P25, DOI 10.1145/3267242.3267244
   ENGELMANN S, 1975, EXCEPT CHILDREN, V41, P243, DOI 10.1177/001440297504100402
   Galvin KL, 1999, J ACOUST SOC AM, V106, P1084, DOI 10.1121/1.428054
   Gescheider GA, 1995, J ACOUST SOC AM, V98, P3195, DOI 10.1121/1.413809
   GESCHEIDER GA, 1989, J ACOUST SOC AM, V85, P2059, DOI 10.1121/1.397858
   HANIN L, 1988, EAR HEARING, V9, P335, DOI 10.1097/00003446-198812000-00010
   Jiao Y., 2018, COMP STUDY OFPHONEME
   Jones LA, 2011, PROG BRAIN RES, V192, P113, DOI 10.1016/B978-0-444-53355-5.00008-7
   Jung JH, 2018, LECT NOTES COMPUT SC, V10919, P447, DOI 10.1007/978-3-319-91803-7_34
   Krause JC, 2002, J ACOUST SOC AM, V112, P2165, DOI 10.1121/1.1509432
   Luzhnica G, 2019, PROCEEDINGS OF IUI 2019, P57, DOI 10.1145/3301275.3302282
   Luzhnica G, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300465
   Luzhnica G, 2016, IEEE INT SYM WRBL CO, P148, DOI 10.1145/2971763.2971769
   Lynch M P, 1988, J Rehabil Res Dev, V25, P41
   Martinez JS, 2021, IEEE T HAPTICS, V14, P200, DOI 10.1109/TOH.2020.3008869
   NORTON SJ, 1977, J SPEECH HEAR RES, V20, P574, DOI 10.1044/jshr.2003.574
   Novich S., 2015, THESIS RICE U HOUSTA
   PICHENY MA, 1986, J SPEECH HEAR RES, V29, P434, DOI 10.1044/jshr.2904.434
   Reed C.M., 1992, Tactile Aids for the Hearing Impaired, P218
   Reed CM, 2019, IEEE T HAPTICS, V12, P2, DOI 10.1109/TOH.2018.2861010
   Reed Charlotte M., 1995, Seminars in Hearing, V16, P305, DOI 10.1055/s-0028-1083728
   REED CM, 1985, J ACOUST SOC AM, V77, P247, DOI 10.1121/1.392266
   REED CM, 1990, J SPEECH HEAR RES, V33, P786, DOI 10.1044/jshr.3304.786
   Reed CM, 1998, PRESENCE-TELEOP VIRT, V7, P509, DOI 10.1162/105474698565893
   REED CM, 1989, VOLTA REV, V91, P65
   REED CM, 1995, J SPEECH HEAR RES, V38, P477, DOI 10.1044/jshr.3802.477
   REED CM, 1982, J SPEECH HEAR RES, V25, P216, DOI 10.1044/jshr.2502.216
   Reed CM., 1995, Profound Deafness and Speech Communication, P40
   Ronnberg J, 1998, J Deaf Stud Deaf Educ, V3, P143
   Sreelakshmi M., 2017, Materials Today: Proceedings, V4, P4182, DOI 10.1016/j.matpr.2017.02.120
   STUDEBAKER GA, 1985, J SPEECH HEAR RES, V28, P455, DOI 10.1044/jshr.2803.455
   Tan HZ, 2020, P IEEE, V108, P945, DOI 10.1109/JPROC.2020.2992561
   Tan HZ, 2020, IEEE T HAPTICS, V13, P745, DOI 10.1109/TOH.2020.2973135
   Tan HZ, 2010, IEEE T HAPTICS, V3, P98, DOI 10.1109/ToH.2009.46
   Tan HZ, 2003, J ACOUST SOC AM, V114, P3295, DOI 10.1121/1.1623788
   Turcott R, 2018, LECT NOTES COMPUT SC, V10894, P600, DOI 10.1007/978-3-319-93399-3_51
   Uchanski RM, 1998, PERCEPT PSYCHOPHYS, V60, P533, DOI 10.3758/BF03206044
   Wade T, 2005, J ACOUST SOC AM, V118, P2618, DOI 10.1121/1.2011156
   WEISENBERGER JM, 1989, J ACOUST SOC AM, V86, P1764, DOI 10.1121/1.398608
   Yuan HF, 2005, J ACOUST SOC AM, V118, P1003, DOI 10.1121/1.1945787
   Zhao S., 2018, P 2018 CHI C HUMAN F, P1, DOI [10.1109/ICNSC.2018.8361365, DOI 10.1109/ICNSC.2018.8361365]
NR 51
TC 7
Z9 8
U1 0
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD JUL
PY 2021
VL 18
IS 3
AR 13
DI 10.1145/3458725
PG 22
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA UD5LZ
UT WOS:000687249300004
OA Bronze, Green Published
DA 2024-07-18
ER

PT J
AU Wei, H
   Li, JM
AF Wei, Hui
   Li, Jingmeng
TI Computational Model for Global Contour Precedence Based on Primary
   Visual Cortex Mechanisms
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Global contour precedence (GCP); multi-layer neural computational model;
   perceptual organization; object proposal generation
ID HORIZONTAL CONNECTIONS; COMBINING BOUNDARY; STRIATE CORTEX; NEURAL
   MODEL; ORIENTATION; SHAPE; INTEGRATION; ATTENTION; PERCEPTION; PATTERNS
AB The edges of an image contains rich visual cognitive cues. However, the edge information of a natural scene usually is only a set of disorganized unorganized pixels for a computer. In psychology, the phenomenon of quickly perceiving global information from a complex pattern is called the global precedence effect (GPE). For example, when one observes the edge map of an image, some contours seem to automatically "pop out" from the complex background. This is a manifestation of GPE on edge information and is called global contour precedence (GCP). The primary visual cortex (V1) is closely related to the processing of edges. In this article, a neural computational model to simulate GCP based on the mechanisms of V1 is presented. There are three layers in the proposed model: the representation of line segments, organization of edges, and perception of global contours. In experiments, the ability to group edges is tested on the public dataset BSDS500. The results show that the grouping performance, robustness, and time cost of the proposed model are superior to those of other methods. In addition, the outputs of the proposed model can also be applied to the generation of object proposals, which indicates that the proposed model can contribute significantly to high-level visual tasks.
C1 [Wei, Hui; Li, Jingmeng] Fudan Univ, Sch Comp Sci, Shanghai Key Lab Data Sci, Lab Algorithms Cognit Models, SongHu Rd 2005, Shanghai 200082, Peoples R China.
C3 Fudan University
RP Wei, H (corresponding author), Fudan Univ, Sch Comp Sci, Shanghai Key Lab Data Sci, Lab Algorithms Cognit Models, SongHu Rd 2005, Shanghai 200082, Peoples R China.
EM weihui@fudan.edu.cn; jmli17@fudan.edu.cn
RI Wei, Hui/K-5819-2019
OI Li, Jingmeng/0000-0001-9108-5211; Wei, Hui/0000-0003-2696-0707
FU NSFC Project [61771146, 61375122]; National Thirteen 5-Year Plan for
   Science and Technology [2017YFC1703303]
FX This work was supported by the NSFC Project (Project Nos. 61771146 and
   61375122) and the National Thirteen 5-Year Plan for Science and
   Technology (Project No. 2017YFC1703303).
CR Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   Arbeláez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161
   Ben-Shahar O, 2004, NEURAL COMPUT, V16, P445, DOI 10.1162/089976604772744866
   BIEDERMAN I, 1987, PSYCHOL REV, V94, P115, DOI 10.1037/0033-295X.94.2.115
   Borji A, 2012, PROC CVPR IEEE, P478, DOI 10.1109/CVPR.2012.6247711
   Bosking WH, 1997, J NEUROSCI, V17, P2112
   Boynton GM, 1996, J NEUROSCI, V16, P4207, DOI 10.1523/jneurosci.16-13-04207.1996
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851
   Carreira J, 2012, IEEE T PATTERN ANAL, V34, P1312, DOI 10.1109/TPAMI.2011.231
   Chakraverty S., 2019, CONCEPT SOFT COMPUTI, P175
   CHEN L, 1982, SCIENCE, V218, P699, DOI 10.1126/science.7134969
   Claessens PME, 2008, J VISION, V8, DOI 10.1167/8.7.33
   de Lange FP, 2018, TRENDS COGN SCI, V22, P764, DOI 10.1016/j.tics.2018.06.002
   Desolneux A, 2003, IEEE T PATTERN ANAL, V25, P508, DOI 10.1109/TPAMI.2003.1190576
   DiCarlo JJ, 2012, NEURON, V73, P415, DOI 10.1016/j.neuron.2012.01.010
   Dollár P, 2015, IEEE T PATTERN ANAL, V37, P1558, DOI 10.1109/TPAMI.2014.2377715
   Elder JH, 2018, ANNU REV VIS SCI, V4, P423, DOI 10.1146/annurev-vision-091517-034110
   Endres I, 2010, LECT NOTES COMPUT SC, V6315, P575, DOI 10.1007/978-3-642-15555-0_42
   FIELD DJ, 1993, VISION RES, V33, P173, DOI 10.1016/0042-6989(93)90156-Q
   Geisler WS, 2001, VISION RES, V41, P711, DOI 10.1016/S0042-6989(00)00277-7
   Gevers T, 1999, PATTERN RECOGN, V32, P453, DOI 10.1016/S0031-3203(98)00036-3
   Ghose T, 2010, J VISION, V10, DOI 10.1167/10.8.3
   GRICE GR, 1983, PERCEPT PSYCHOPHYS, V33, P121, DOI 10.3758/BF03202829
   Grossberg S, 1999, SPATIAL VISION, V12, P163, DOI 10.1163/156856899X00102
   HABER R.N., 1973, PSYCHOL VISUAL PERCE
   Han SH, 1999, PERCEPT PSYCHOPHYS, V61, P661, DOI 10.3758/BF03205537
   Harter M.R., 1984, Varieties of Attention, P293
   HUBEL DH, 1978, J COMP NEUROL, V177, P361, DOI 10.1002/cne.901770302
   HUBEL DH, 1977, PROC R SOC SER B-BIO, V198, P1, DOI 10.1098/rspb.1977.0085
   HUBEL DH, 1959, J PHYSIOL-LONDON, V148, P574, DOI 10.1113/jphysiol.1959.sp006308
   HUGHES HC, 1984, PERCEPT PSYCHOPHYS, V35, P361, DOI 10.3758/BF03206340
   HUMPHREY AL, 1980, J COMP NEUROL, V192, P531, DOI 10.1002/cne.901920311
   Im HY, 2016, VISION RES, V126, P291, DOI 10.1016/j.visres.2015.08.013
   Kanizsa G., 1979, Organization in Vision: Essays on Gestalt Perception
   Kim J, 2016, PROC SPIE, V0011, DOI 10.1117/12.2242783
   Kim J, 2016, J VISION, V16, DOI 10.1167/16.6.5
   KIMCHI R, 1982, J EXP PSYCHOL HUMAN, V8, P521, DOI 10.1037/0096-1523.8.4.521
   KINCHLA RA, 1979, PERCEPT PSYCHOPHYS, V25, P225, DOI 10.3758/BF03202991
   Kunsberg B, 2018, INTERFACE FOCUS, V8, DOI 10.1098/rsfs.2018.0019
   Kwon T, 2016, VISION RES, V126, P143, DOI 10.1016/j.visres.2015.06.007
   Lawlor M., 2013, NIPS, P1763
   Lezama J, 2016, VISION RES, V126, P183, DOI 10.1016/j.visres.2015.09.004
   Li ZP, 1998, NEURAL COMPUT, V10, P903, DOI 10.1162/089976698300017557
   Liu L, 2017, PLOS BIOL, V15, DOI 10.1371/journal.pbio.2003646
   Loffler G, 2008, VISION RES, V48, P2106, DOI 10.1016/j.visres.2008.03.006
   LUCK SJ, 1995, INT J NEUROSCI, V80, P281, DOI 10.3109/00207459508986105
   Mel BW, 1997, NEURAL COMPUT, V9, P777, DOI 10.1162/neco.1997.9.4.777
   MILLER J, 1981, J EXP PSYCHOL HUMAN, V7, P1161, DOI 10.1037/0096-1523.7.6.1161
   Ming YS, 2016, IEEE T IMAGE PROCESS, V25, P3597, DOI 10.1109/TIP.2016.2564646
   MITCHISON G, 1982, P NATL ACAD SCI-BIOL, V79, P3661, DOI 10.1073/pnas.79.11.3661
   Movahedi V, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.128
   NAVON D, 1977, COGNITIVE PSYCHOL, V9, P353, DOI 10.1016/0010-0285(77)90012-3
   OBrien Alexander M, 2007, WHOLE IS CREATED SUM
   Palmer SE, 2008, PSYCHOL SCI, V19, P77, DOI 10.1111/j.1467-9280.2008.02049.x
   Palmer SE, 2008, J EXP PSYCHOL HUMAN, V34, P1353, DOI 10.1037/a0012729
   PARENT P, 1989, IEEE T PATTERN ANAL, V11, P823, DOI 10.1109/34.31445
   Poirel N, 2008, ACTA PSYCHOL, V127, P1, DOI 10.1016/j.actpsy.2006.12.001
   POMERANTZ JR, 1983, J EXP PSYCHOL GEN, V112, P516, DOI 10.1037/0096-3445.112.4.516
   Qi YG, 2015, PROC CVPR IEEE, P1856, DOI 10.1109/CVPR.2015.7298795
   Rajaei B, 2018, IMAGE PROCESS ON LIN, V8, P37, DOI 10.5201/ipol.2018.194
   Ren XF, 2008, INT J COMPUT VISION, V77, P47, DOI 10.1007/s11263-007-0092-6
   ROCKLAND KS, 1982, SCIENCE, V215, P1532, DOI 10.1126/science.7063863
   Schmidt KE, 1997, EUR J NEUROSCI, V9, P1083, DOI 10.1111/j.1460-9568.1997.tb01459.x
   Shams L, 2010, TRENDS COGN SCI, V14, P425, DOI 10.1016/j.tics.2010.07.001
   Shouval HZ, 2000, J NEUROSCI, V20, P1119, DOI 10.1523/JNEUROSCI.20-03-01119.2000
   Sobel I., 1990, An Isotropic 3x3 Image Gradient Operator, P376, DOI [10.13140/RG.2.1.1912.4965, DOI 10.13140/RG.2.1.1912.4965]
   Somers DC, 1998, CEREB CORTEX, V8, P204, DOI 10.1093/cercor/8.3.204
   Stahl JS, 2008, IEEE T PATTERN ANAL, V30, P395, DOI 10.1109/TPAMI.2007.1186
   Stahl JS, 2007, IEEE T IMAGE PROCESS, V16, P2590, DOI 10.1109/TIP.2007.904463
   Troncoso XG, 2011, VISUAL PROSTHETICS: PHYSIOLOGY, BIOENGINEERING, REHABILITATION, P23, DOI 10.1007/978-1-4419-0754-7_2
   Uijlings JRR, 2013, INT J COMPUT VISION, V104, P154, DOI 10.1007/s11263-013-0620-5
   Versace E, 2018, TRENDS COGN SCI, V22, P963, DOI 10.1016/j.tics.2018.07.005
   von Gioi RG, 2008, J MATH IMAGING VIS, V32, P313, DOI 10.1007/s10851-008-0102-5
   Wagemans J, 2012, PSYCHOL BULL, V138, P1172, DOI 10.1037/a0029333
   Wei H, 2017, J VIS COMMUN IMAGE R, V48, P292, DOI 10.1016/j.jvcir.2017.07.003
   Wei H, 2017, APPL SOFT COMPUT, V52, P333, DOI 10.1016/j.asoc.2016.10.031
   Wei H, 2017, INFORM SCIENCES, V385, P395, DOI 10.1016/j.ins.2016.12.039
   Wei H, 2014, IEEE T NEUR NET LEAR, V25, P1346, DOI 10.1109/TNNLS.2013.2293178
   Wei H, 2014, COGN COMPUT, V6, P164, DOI 10.1007/s12559-013-9222-3
   Wei H, 2013, NEURAL PROCESS LETT, V38, P205, DOI 10.1007/s11063-012-9249-6
   Wei H, 2013, COGN NEURODYNAMICS, V7, P361, DOI 10.1007/s11571-012-9235-8
   Wei H, 2013, APPL SOFT COMPUT, V13, P302, DOI 10.1016/j.asoc.2012.08.036
   Wertheimer M, 1923, PSYCHOL FORSCH, V4, P301, DOI 10.1007/BF00410640
   Williams C.K.I., PASCAL VISUAL OBJECT
   Yu Q, 2017, PATTERN RECOGN, V65, P82, DOI 10.1016/j.patcog.2016.11.020
   Zitnick CL, 2014, LECT NOTES COMPUT SC, V8693, P391, DOI 10.1007/978-3-319-10602-1_26
NR 86
TC 2
Z9 2
U1 0
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD JUL
PY 2021
VL 18
IS 3
AR 14
DI 10.1145/3459999
PG 21
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA UD5LZ
UT WOS:000687249300005
DA 2024-07-18
ER

PT J
AU Nehmé, Y
   Farrugia, JP
   Dupont, F
   Le Callet, P
   Lavoué, G
AF Nehme, Yana
   Farrugia, Jean-Philippe
   Dupont, Florent
   Le Callet, Patrick
   Lavoue, Guillaume
TI Comparison of Subjective Methods for Quality Assessment of 3D Graphics
   in Virtual Reality
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Visual quality assessment; 3D graphics; subjective methodologies; single
   stimulus; double stimulus; SAMVIQ; accuracy; time-effort
ID SCALES
AB Numerous methodologies for subjective quality assessment exist in the field of image processing. In particular, the Absolute Category Rating with Hidden Reference (ACR-HR), the Double Stimulus Impairment Scale (DSIS), and the Subjective Assessment Methodology for Video Quality (SAMVIQ) are considered three of the most prominent methods for assessing the visual quality of 2D images and videos. Are these methods valid/accurate to evaluate the perceived quality of 3D graphics data? Is the presence of an explicit reference necessary, due to the lack of human prior knowledge on 3D graphics data compared to natural images/videos? To answer these questions, we compare these three subjective methods (ACR-HR, DSIS, and SAMVIQ) on a dataset of high-quality colored 3D models, impaired with various distortions. These subjective experiments were conducted in a virtual reality environment. Our results show differences in the performance of the methods depending on the 3D contents and the types of distortions. We show that DSIS and SAMVIQ outperform ACR-HR in terms of accuracy and point out a stable performance. In regard to the time-effort, DSIS achieves the highest accuracy in the shortest assessment time. Results also yield interesting conclusions on the importance of a reference for judging the quality of 3D graphics. We finally provide recommendations regarding the influence of the number of observers on the accuracy.
C1 [Nehme, Yana; Farrugia, Jean-Philippe; Dupont, Florent; Lavoue, Guillaume] Univ Lyon, CNRS, LIRIS, Batiment Nautibus,43 Bd 11 Novembre 1918, F-69100 Villeurbanne, France.
   [Le Callet, Patrick] Univ Nantes, CNRS, LS2N, Rue Christian Pauc, F-44300 Nantes, France.
C3 Centre National de la Recherche Scientifique (CNRS); Institut National
   des Sciences Appliquees de Lyon - INSA Lyon; Nantes Universite; Centre
   National de la Recherche Scientifique (CNRS)
RP Nehmé, Y (corresponding author), Univ Lyon, CNRS, LIRIS, Batiment Nautibus,43 Bd 11 Novembre 1918, F-69100 Villeurbanne, France.
EM yana.nehme@insa-lyon.fr; jean-philippe.farrugia@univ-lyon1.fr;
   Florent.Dupont@liris.cnrs.fr; Patrick.Le-Callet@univ-nantes.fr;
   glavoue@liris.cnrs.fr
RI Le Callet, Patrick/F-5772-2010
OI Dupont, Florent/0000-0001-6611-4420
FU French National Research Agency as part of ANR-PISCo project
   [ANR-17-CE33-0005]
FX This work was supported by French National Research Agency as part of
   ANR-PISCo project (Grant No. ANR-17-CE33-0005).
CR Alexiou E., 2020, P IEEE INT C MULT EX, P1
   Alexiou E., 2017, P 9 INT C QUAL MULT, P1
   Alexiou E, 2019, APSIPA TRANS SIGNAL, V8, DOI 10.1017/ATSIP.2019.20
   Alexiou E, 2017, PROC SPIE, V10396, DOI 10.1117/12.2275142
   [Anonymous], 2018, 26918 TR 3GPP
   [Anonymous], 2009, P INT TEL UN
   [Anonymous], 2007, P INT TEL UN
   [Anonymous], 2012, P INT TEL UN
   Brunnström K, 2018, J ELECTRON IMAGING, V27, DOI 10.1117/1.JEI.27.5.053013
   Corriveau P, 1999, SIGNAL PROCESS, V77, P1, DOI 10.1016/S0165-1684(99)00018-3
   Corsini M, 2007, IEEE T MULTIMEDIA, V9, P247, DOI 10.1109/TMM.2006.886261
   da Silva Cruz Luis A., 2019, P INT C QUAL MULT EX
   EBU, 2003, 056 EBU SAMVIQ BPN
   Gao P., 2020, P 12 INT C QUAL MULT, P1
   Garland M., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P209, DOI 10.1145/258734.258849
   Guo JJ, 2017, ACM T APPL PERCEPT, V14, DOI 10.1145/2996296
   Hands David, 2007, MULTIMEDIA GROUP TES
   Javaheri Alireza, 2019, POINT CLOUD RENDERIN
   Kawano T, 2014, IEICE T COMMUN, VE97B, P738, DOI 10.1587/transcom.E97.B.738
   Kawashima K, 2018, IEICE T COMMUN, VE101B, P933, DOI 10.1587/transcom.2017EBP3123
   La Viola J. J.  Jr., 2000, SIGCHI Bulletin, V32, P47, DOI 10.1145/333329.333344
   Lavoue G., 2015, Visual Signal Quality Assessment, P243, DOI [10.1007/978-3-319-10368-69, DOI 10.1007/978-3-319-10368-69]
   Lavoué G, 2006, PROC SPIE, V6312, DOI 10.1117/12.686964
   Lavoué G, 2009, ACM T APPL PERCEPT, V5, DOI 10.1145/1462048.1462052
   Lee H, 2012, VISUAL COMPUT, V28, P137, DOI 10.1007/s00371-011-0602-y
   Mantiuk RK, 2012, COMPUT GRAPH FORUM, V31, P2478, DOI 10.1111/j.1467-8659.2012.03188.x
   McGraw KO, 1996, PSYCHOL METHODS, V1, P30, DOI 10.1037/1082-989X.1.1.30
   Pan YX, 2005, IEEE T MULTIMEDIA, V7, P269, DOI 10.1109/TMM.2005.843364
   Pechard Stephane, 2008, P INT WORKSH IM MED
   Piorkowski R, 2017, ACM T APPL PERCEPT, V14, DOI 10.1145/3047407
   Quan HT, 2011, IEEE T BROADCAST, V57, P1, DOI 10.1109/TBC.2010.2086750
   Quan Huynh-Thua, 2007, P 3 INT WORKSH VID P
   Regal G, 2018, INT WORK QUAL MULTIM, P81
   Rogowitz BE, 2001, P SOC PHOTO-OPT INS, V4299, P340, DOI 10.1117/12.429504
   Seshadrinathan K, 2010, IEEE T IMAGE PROCESS, V19, P1427, DOI 10.1109/TIP.2010.2042111
   Singla Ashutosh, 2018, ELECT IMAGING, V14, DOI 10.2352/ISSN.2470-1173. 2018.14.HVEI-525
   Subramanyam S, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR 2020), P127, DOI [10.1109/VR46266.2020.1581260728335, 10.1109/VR46266.2020.00-73]
   Tominaga Toshiko, 2010, Proceedings of the 2010 Second International Workshop on Quality of Multimedia Experience (QoMEX 2010), P82, DOI 10.1109/QOMEX.2010.5517948
   VANDIJK AM, 1995, P SOC PHOTO-OPT INS, V2451, P90, DOI 10.1117/12.201231
   Vanhoey K, 2017, ACM T APPL PERCEPT, V15, DOI 10.1145/3129505
   Watson B, 2001, COMP GRAPH, P213, DOI 10.1145/383259.383283
   Wolski K, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3196493
   Zerman E., 2019, ELECT IMAG, V2019, P1
NR 43
TC 15
Z9 15
U1 1
U2 14
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD JAN
PY 2021
VL 18
IS 1
AR 2
DI 10.1145/3427931
PG 23
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA PZ2NW
UT WOS:000612577900002
DA 2024-07-18
ER

PT J
AU Zhang, X
   Lyu, YQ
   Qu, T
   Qiu, PF
   Luo, XM
   Zhang, JY
   Fan, SJ
   Shi, YC
AF Zhang, Xiao
   Lyu, Yongqiang
   Qu, Tong
   Qiu, Pengfei
   Luo, Xiaoming
   Zhang, Jingyu
   Fan, Shunjie
   Shi, Yuanchun
TI Photoplethysmogram-based Cognitive Load Assessment Using Multi-Feature
   Fusion Model
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Cognitive load; photoplethysmogram; multi-feature fusion; real-time
   assessment
ID HEART-RATE-VARIABILITY; TASK-DIFFICULTY; MENTAL STRESS; WORKLOAD; TIME;
   TECHNOLOGY; EEG
AB Cognitive load assessment is crucial for user studies and human-computer interaction designs. As a noninvasive and easy-touse category of measures, current photoplethysmogram- (PPG) based assessment methods rely on single or small-scale predefined features to recognize responses induced by people's cognitive load, which are not stable in assessment accuracy. In this study, we propose a machine-learning method by using 46 kinds of PPG features together to improve the measurement accuracy for cognitive load. We test the method on 16 participants through the classical n-back tasks (0-back, 1-back, and 2-back). The accuracy of the machine-learning method in differentiating different levels of cognitive loads induced by task difficulties can reach 100% in 0-back vs. 2-back tasks, which outperformed the traditional HRV-based and single-PPG-feature-based methods by 12-55%. When using "leave-one-participant-out" subject-independent cross validation, 87.5% binary classification accuracy was reached, which is at the state-of-the-art level. The proposed method can also support real-time cognitive load assessment by beat-to-beat classifications with better performance than the traditional single-feature-based real-time evaluation method.
C1 [Zhang, Xiao] Beijing Univ Technol, 100 Pingyue Pk, Beijing 100124, Peoples R China.
   [Zhang, Xiao; Lyu, Yongqiang; Qu, Tong; Qiu, Pengfei; Shi, Yuanchun] Tsinghua Univ, 30 Shuangqing Rd, Beijing 100084, Peoples R China.
   [Luo, Xiaoming] Beijing Genom Inst, Tibet Branch, 189 Jinzhu Rd, Lhasa 850032, Peoples R China.
   [Zhang, Jingyu] Chinese Acad Sci, 16 Lincui Rd, Beijing 100101, Peoples R China.
   [Fan, Shunjie] Siemens Ltd, 7 Wangjing Zhonghuan Nanlu, Beijing 100102, Peoples R China.
C3 Beijing University of Technology; Tsinghua University; Beijing Genomics
   Institute (BGI); Chinese Academy of Sciences; Siemens AG
RP Lyu, YQ (corresponding author), Tsinghua Univ, 30 Shuangqing Rd, Beijing 100084, Peoples R China.
EM zhangxiaohappier@163.com; luyq@tsinghua.edu.cn; qutong7@foxmail.com;
   qpf15@mails.tsinghua.edu.cn; luo.xiaomin@139.com;
   zhangjingyu@psych.ac.cn; shunjie.fan@siemens.com; shiyc@tsinghua.edu.cn
RI Lyu, Yongqiang/JUF-0554-2023
FU National Key R&D Program of China [2017YFB0403404]
FX We gratefully acknowledge the grant from National Key R&D Program of
   China (Grant No. 2017YFB0403404).
CR Abdelrahman Yomna, 2017, Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, V1, DOI 10.1145/3130898
   Alty SR, 2007, IEEE T BIO-MED ENG, V54, P2268, DOI 10.1109/TBME.2007.897805
   [Anonymous], 2012, ACM T COMPUTER HUMAN, DOI DOI 10.1145/2395131.2395138]
   Antonenko P, 2010, EDUC PSYCHOL REV, V22, P425, DOI 10.1007/s10648-010-9130-y
   Ayaz H, 2007, 2007 3RD INTERNATIONAL IEEE/EMBS CONFERENCE ON NEURAL ENGINEERING, VOLS 1 AND 2, P342, DOI 10.1109/CNE.2007.369680
   Brouwer AM, 2014, INT J PSYCHOPHYSIOL, V93, P242, DOI 10.1016/j.ijpsycho.2014.05.004
   Brouwer AM, 2012, J NEURAL ENG, V9, DOI 10.1088/1741-2560/9/4/045008
   Brünken R, 2003, EDUC PSYCHOL-US, V38, P53, DOI 10.1207/S15326985EP3801_7
   Chen F., 2012, ACM T INTERACT INTEL, V2, P1, DOI DOI 10.1145/2395123.2395127
   Cho Y, 2017, INT CONF AFFECT, P456, DOI 10.1109/ACII.2017.8273639
   DeLeeuw KE, 2008, J EDUC PSYCHOL, V100, P223, DOI 10.1037/0022-0663.100.1.223
   Dennerlein Jack, 2003, P INT ERG ASS
   Durantin G, 2014, BEHAV BRAIN RES, V259, P16, DOI 10.1016/j.bbr.2013.10.042
   Elgendi M, 2012, CURR CARDIOL REV, V8, P14, DOI 10.2174/157340312801215782
   Goedhart AD, 2007, PSYCHOPHYSIOLOGY, V44, P203, DOI 10.1111/j.1469-8986.2006.00490.x
   Grimes D, 2008, CHI 2008: 26TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS VOLS 1 AND 2, CONFERENCE PROCEEDINGS, P835
   Haapalainen E, 2010, UBICOMP 2010: PROCEEDINGS OF THE 2010 ACM CONFERENCE ON UBIQUITOUS COMPUTING, P301
   Harris CW, 2000, AM HEART J, V139, P405, DOI 10.1067/mhj.2000.101784
   Harrison J, 2014, IEEE T HUM-MACH SYST, V44, P429, DOI 10.1109/THMS.2014.2319822
   Hjortskov N, 2004, EUR J APPL PHYSIOL, V92, P84, DOI 10.1007/s00421-004-1055-z
   Hogervorst MA, 2014, FRONT NEUROSCI-SWITZ, V8, DOI 10.3389/fnins.2014.00322
   Iani C, 2004, PSYCHOPHYSIOLOGY, V41, P789, DOI 10.1111/j.1469-8986.2004.00200.x
   Izzetoglu M, 2007, IEEE ENG MED BIOL, V26, P38, DOI 10.1109/MEMB.2007.384094
   Joachims T., 2006, P 12 ACM SIGKDD INT, P217, DOI [10.1145/1150402.1150429, DOI 10.1145/1150402.1150429]
   Johnson M.D., 2014, Annu. Meet. Southern Agricultural Association, P1, DOI [DOI 10.1145/2667239.2667296, 10.1145/2667239]
   Jubadi Warsuzarina Mat, 2009, Proceedings of the 2009 IEEE Symposium on Industrial Electronics & Applications (ISIEA 2009), P1, DOI 10.1109/ISIEA.2009.5356491
   KALSBEEK J. W. H., 1963, ERGONOMICS, V6, P306
   Kavsaoglu AR, 2014, COMPUT BIOL MED, V49, P1, DOI 10.1016/j.compbiomed.2014.03.005
   KIRCHNER WK, 1958, J EXP PSYCHOL, V55, P352, DOI 10.1037/h0043688
   Kumar N, 2016, PROCEDIA COMPUT SCI, V84, P70, DOI 10.1016/j.procs.2016.04.068
   Li Luo, 2012, 2012 International Conference on Biomedical Engineering and Biotechnology (iCBEB), P929, DOI 10.1109/iCBEB.2012.437
   Lyu YQ, 2015, CHI 2015: PROCEEDINGS OF THE 33RD ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P857, DOI 10.1145/2702123.2702399
   Mayer RE, 2003, WEB-BASED LEARNING: WHAT DO WE KNOW? WHERE DO WE GO?, P23
   McDuff D, 2014, IEEE ENG MED BIO, P2957, DOI 10.1109/EMBC.2014.6944243
   McDuff D, 2014, IEEE T BIO-MED ENG, V61, P2593, DOI 10.1109/TBME.2014.2323695
   McDuff DJ, 2016, 34TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, CHI 2016, P4000, DOI 10.1145/2858036.2858247
   Or Calvin K. L., 2007, Occupational Ergonomics, V7, P83
   Oviatt S., 2006, Proc. 14th Annu. ACM Int. Conf. Multimed.-Multimed. '06, P871, DOI [10.1145/1180639.1180831, DOI 10.1145/1180639.1180831]
   Paas F, 2003, EDUC PSYCHOL-US, V38, P63, DOI 10.1207/S15326985EP3801_8
   PAAS FGWC, 1994, EDUC PSYCHOL REV, V6, P351, DOI 10.1007/BF02213420
   Pedregosa F, 2011, J MACH LEARN RES, V12, P2825
   Poh MZ, 2011, IEEE T BIO-MED ENG, V58, P7, DOI 10.1109/TBME.2010.2086456
   Shi Y., 2007, CHI 07 EXTENDED ABST, P2651, DOI DOI 10.1145/1240866.1241057
   SHIN KG, 1994, P IEEE, V82, P6, DOI 10.1109/5.259423
   Solovey ET, 2014, 32ND ANNUAL ACM CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2014), P4057, DOI 10.1145/2556288.2557068
   Verwey WB, 1996, J EXP PSYCHOL-APPL, V2, P270, DOI 10.1037/1076-898X.2.3.270
   Wang L., 2009, P INT C IEEE ENG MED
   Wang SY, 2016, IEEE T HUM-MACH SYST, V46, P424, DOI 10.1109/THMS.2015.2476818
   Zhang JH, 2015, IEEE T HUM-MACH SYST, V45, P200, DOI 10.1109/THMS.2014.2366914
   Zhao GZ, 2018, IEEE T HUM-MACH SYST, V48, P149, DOI 10.1109/THMS.2018.2803025
   Zijlstra F.R.H., 1993, THESIS
NR 51
TC 10
Z9 10
U1 5
U2 47
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD SEP
PY 2019
VL 16
IS 4
AR 19
DI 10.1145/3340962
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA JE2HU
UT WOS:000490516700001
DA 2024-07-18
ER

PT J
AU Faul, F
AF Faul, Franz
TI Toward a Perceptually Uniform Parameter Space for Filter Transparency
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Color perception; transparency perception; visualization; transparency
   picker
ID COLOR TRANSPARENCY; MIXTURE; MODEL
AB Filter models of perceptual transparency relate to regularities in the retinal projections caused by light transmitting objects like clear liquids or glass and have been found to predict the color conditions for perceptual transparency more accurately than alternative models. An important but unsolved problem is how exactly the model parameters are related to the properties of the perceived transparent layer. We previously proposed a parametrization in terms of hue, saturation, overall transmittance and clarity of the filter that seems to capture important dimensions of the phenomenal impressions. However, these parameters are not independent and the corresponding scales are not perceptually uniform. Here, an invertible transformation of this parameter space is proposed that strongly mitigates these problems. This results in a more intuitively interpretable parameter set that seems well suited for the analysis of existing stimuli and the generation of transparent overlays with predefined perceptual properties. The latter property makes it suitable for graphics and visualization applications.
C1 [Faul, Franz] Univ Kiel, Inst Psychol, Olshausenstr 62, D-24098 Kiel, Germany.
C3 University of Kiel
RP Faul, F (corresponding author), Univ Kiel, Inst Psychol, Olshausenstr 62, D-24098 Kiel, Germany.
EM ffaul@psychologie.uni-kiel.de
FU DFG [FA425/2-1]
FX This work is supported by DFG grant FA425/2-1.
CR [Anonymous], 1978, ACM SIGGRAPH COMPUT, DOI [10.1145/800248.807361, DOI 10.1145/965139.807361]
   BECK J, 1978, PERCEPT PSYCHOPHYS, V23, P265, DOI 10.3758/BF03204137
   BRAINARD DH, 1989, COLOR RES APPL, V14, P23, DOI 10.1002/col.5080140107
   Chen VJ, 1998, PERCEPTION, V27, P595, DOI 10.1068/p270595
   DaPos O., 1989, TRASPARENZE
   DZmura M, 1997, PERCEPTION, V26, P471, DOI 10.1068/p260471
   EVANS RM, 1959, J OPT SOC AM, V49, P1049, DOI 10.1364/JOSA.49.001049
   Faul F, 2002, J OPT SOC AM A, V19, P1084, DOI 10.1364/JOSAA.19.001084
   Faul F., 1997, THESIS
   Faul F, 2012, J VISION, V12, DOI 10.1167/12.12.7
   Faul F, 2011, J VISION, V11, DOI 10.1167/11.7.7
   Golz J, 2003, J OPT SOC AM A, V20, P769, DOI 10.1364/JOSAA.20.000769
   Khang BG, 2002, J VISION, V2, DOI 10.1167/2.6.3
   Koenderink J. J., 2010, Color for the sciences
   Maloney LT, 2003, J VISION, V3, P573, DOI 10.1167/3.8.5
   METELLI F, 1974, SCI AM, V230, P91, DOI 10.1038/scientificamerican0474-90
   Nakauchi S, 1999, J OPT SOC AM A, V16, P2612, DOI 10.1364/JOSAA.16.002612
   Newhall SM, 1943, J OPT SOC AM, V33, P385, DOI 10.1364/JOSA.33.000385
   Porter T., 1984, Computers & Graphics, V18, P253
   Richards W, 2009, J OPT SOC AM A, V26, P1119, DOI 10.1364/JOSAA.26.001119
   Singh M, 2002, PSYCHOL REV, V109, P492, DOI 10.1037//0033-295X.109.3.492
   Singh M, 2006, VISION RES, V46, P879, DOI 10.1016/j.visres.2005.10.022
   STILES WS, 1977, J OPT SOC AM, V67, P779, DOI 10.1364/JOSA.67.000779
   STOCKMAN A, 1993, J OPT SOC AM A, V10, P2491, DOI 10.1364/JOSAA.10.002491
   Westland S, 2000, J OPT SOC AM A, V17, P255, DOI 10.1364/JOSAA.17.000255
   Wyszecki G., 1982, Color science: Concepts and methods, quantitative data and formulae
NR 26
TC 11
Z9 12
U1 0
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD FEB
PY 2017
VL 14
IS 2
AR 13
DI 10.1145/3022732
PG 21
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA EM8VC
UT WOS:000395588200006
DA 2024-07-18
ER

PT J
AU Jun, E
   Stefanucci, JK
   Creem-Regehr, SH
   Geuss, MN
   Thompson, WB
AF Jun, Eunice
   Stefanucci, Jeanine K.
   Creem-Regehr, Sarah H.
   Geuss, Michael N.
   Thompson, William B.
TI Big Foot: Using the Size of a Virtual Foot to Scale Gap Width
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article; Proceedings Paper
CT ACM SIGGRAPH Symposium on Applied Perception (SAP)
CY SEP 13-14, 2015
CL Tubingen, GERMANY
SP ACM SIGGRAPH, ACM, Disney Res, Max Planck Inst Biol Cybernet
DE Experimentation; Human Factors; Avatars; partial self-avatars;
   affordances; body perception; perception; virtual environments
ID EYE HEIGHT; BODY; OWNERSHIP
AB Spatial perception research in the real world and in virtual environments suggests that the body (e.g., hands) plays a role in the perception of the scale of the world. However, little research has closely examined how varying the size of virtual body parts may influence judgments of action capabilities and spatial layout. Here, we questioned whether changing the size of virtual feet would affect judgments of stepping over and estimates of the width of a gap. Participants viewed their disembodied virtual feet as small or large and judged both their ability to step over a gap and the size of gaps shown in the virtual world. Foot size affected both affordance judgments and size estimates such that those with enlarged virtual feet estimated they could step over larger gaps and that the extent of the gap was smaller. Shrunken feet led to the perception of a reduced ability to step over a gap and smaller estimates of width. The results suggest that people use their visually perceived foot size to scale virtual spaces. Regardless of foot size, participants felt that they owned the feet rendered in the virtual world. Seeing disembodied, but motion-tracked, virtual feet affected spatial judgments, suggesting that the presentation of a single tracked body part is sufficient to produce similar effects on perception, as has been observed with the presence of fully co-located virtual self-avatars or other body parts in the past.
C1 [Jun, Eunice] Vanderbilt Univ, Dept Elect Engn & Comp Sci, Nashville, TN 37212 USA.
   [Jun, Eunice] Vanderbilt Univ, Dept Psychol & Human Dev, Nashville, TN 37212 USA.
   [Stefanucci, Jeanine K.; Creem-Regehr, Sarah H.] Univ Utah, Dept Psychol, Salt Lake City, UT 84112 USA.
   [Geuss, Michael N.] Max Planck Inst Biol Cybernet, D-72076 Tubingen, Germany.
   [Thompson, William B.] Univ Utah, Sch Comp, Salt Lake City, UT 84112 USA.
C3 Vanderbilt University; Vanderbilt University; Utah System of Higher
   Education; University of Utah; Max Planck Society; Utah System of Higher
   Education; University of Utah
RP Jun, E (corresponding author), Vanderbilt Univ, Dept Elect Engn & Comp Sci, 254 Featheringill Hall, Nashville, TN 37212 USA.
EM eunice.m.jun@vanderbilt.edu; jeanine.stefanucci@psych.utah.edu;
   sarah.creem@psych.utah.edu; michael.geuss@tuebingen.mpg.de;
   thompson@cs.utah.edu
OI Jun, Eunice/0000-0002-4050-4284
FU National Science Foundation [0914488, 1116636]; Direct For Computer &
   Info Scie & Enginr; Div Of Information & Intelligent Systems [1116636]
   Funding Source: National Science Foundation; Direct For Computer & Info
   Scie & Enginr; Div Of Information & Intelligent Systems [0914488]
   Funding Source: National Science Foundation
FX This work was supported by the National Science Foundation under Grants
   No. 0914488 and 1116636.
CR Banakou D, 2013, P NATL ACAD SCI USA, V110, P12846, DOI 10.1073/pnas.1306779110
   Creem-Regehr SH, 2015, PSYCHOL LEARN MOTIV, V62, P195, DOI 10.1016/bs.plm.2014.09.006
   Cutting JE, 1995, PERCEPTION SPACE MOT, P69, DOI [DOI 10.1016/B978-012240530-3/50005-5, 10.1016/B978-012240530-3/50005-5]
   Dixon MW, 2000, J EXP PSYCHOL HUMAN, V26, P582, DOI 10.1037/0096-1523.26.2.582
   Dobricki M, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0083840
   Geuss Michael., 2010, P 7 S APPL PERCEPTIO, P61, DOI [10.1145/1836248.1836259, DOI 10.1145/1836248.1836259]
   Gibson J., 1979, The ecological approach to visual perception
   Kokkinara E, 2014, PERCEPTION, V43, P43, DOI 10.1068/p7545
   Kunz BR, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0054446
   Leyrer M., 2011, P ACM SIGGRAPH S APP, DOI 10.1145/2077451.2077464
   Lin Q, 2012, P ACM S APPL PERC 3, P7
   LIN Q., 2013, P ACM S APPL PERCEPT, P107
   Linkenauger S. A., 2012, J VISION, V12, P902
   Linkenauger SA, 2015, J EXP PSYCHOL GEN, V144, P103, DOI 10.1037/xge0000028
   Linkenauger SA, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0068594
   Linkenauger SA, 2011, J EXP PSYCHOL HUMAN, V37, P1432, DOI 10.1037/a0024248
   MARK LS, 1987, J MOTOR BEHAV, V19, P367
   Piryankova IV, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0103428
   Proffitt DR, 2013, ACTION SCIENCE: FOUNDATIONS OF AN EMERGING DISCIPLINE, P171
   Stefanucci JK, 2010, ATTEN PERCEPT PSYCHO, V72, P1338, DOI 10.3758/APP.72.5.1338
   Stefanucci JK, 2009, PERCEPTION, V38, P1782, DOI 10.1068/p6437
   van der Hoort B, 2011, PLOS ONE, V6, DOI 10.1371/journal.pone.0020195
   WARREN WH, 1987, J EXP PSYCHOL HUMAN, V13, P371, DOI 10.1037/0096-1523.13.3.371
   Witt JK, 2005, J EXP PSYCHOL HUMAN, V31, P880, DOI 10.1037/0096-1523.31.5.880
   Wraga M, 1999, PERCEPT PSYCHOPHYS, V61, P490, DOI 10.3758/BF03211968
NR 25
TC 37
Z9 41
U1 1
U2 10
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD SEP
PY 2015
VL 12
IS 4
SI SI
AR 16
DI 10.1145/2811266
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA CR1ER
UT WOS:000361067300004
DA 2024-07-18
ER

PT J
AU Vicovaro, M
   Hoyet, L
   Burigana, L
   O'Sullivan, C
AF Vicovaro, Michele
   Hoyet, Ludovic
   Burigana, Luigi
   O'Sullivan, Carol
TI Perceptual Evaluation of Motion Editing for Realistic Throwing
   Animations
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Experimentation; Human Factors; Perception; graphics; motion capture;
   physics; motion editing
ID DYNAMICS
AB Animation budget constraints during the development of a game often call for the use of a limited set of generic motions. Editing operations are thus generally required to animate virtual characters with a sufficient level of variety. Evaluating the perceptual plausibility of edited animations can therefore contribute greatly towards producing visually plausible animations. In this article, we study observers' sensitivity to manipulations of overarm and underarm biological throwing animations. In the first experiment, we modified the release velocity of the ball while leaving the motion of the virtual thrower and the angle of release of the ball unchanged. In the second experiment, we evaluated the possibility of further modifying throwing animations by simultaneously editing the motion of the thrower and the release velocity of the ball, using dynamic time warping. In both experiments, we found that participants perceived shortened underarm throws to be particularly unnatural. We also found that modifying the thrower's motion in addition to modifying the release velocity of the ball does not significantly improve the perceptual plausibility of edited throwing animations. In the third experiment, we modified the angle of release of the ball while leaving the magnitude of release velocity and the motion of the thrower unchanged, and found that this editing operation is efficient for improving the perceptual plausibility of shortened underarm throws. Finally, in Experiment 4, we replaced the virtual human thrower with a mechanical throwing device (a ramp) and found the opposite pattern of sensitivity to modifications of the release velocity, indicating that biological and physical throws are subject to different perceptual rules. Our results provide valuable guidelines for developers of games and virtual reality applications by specifying thresholds for the perceptual plausibility of throwing manipulations while also providing several interesting insights for researchers in visual perception of biological motion.
C1 [Vicovaro, Michele; Burigana, Luigi] Univ Padua, Dipartimento Psicol Gen, I-35131 Padua, Italy.
   [Hoyet, Ludovic] Trinity Coll Dublin, Lloyd Inst, Dublin 2, Ireland.
   [O'Sullivan, Carol] Disney Res, Glendale, CA 91201 USA.
C3 University of Padua; Trinity College Dublin
RP Vicovaro, M (corresponding author), Univ Padua, Dipartimento Psicol Gen, Via Venezia 8, I-35131 Padua, Italy.
EM vicovaro85@gmail.com; hoyetl@tcd.ie; luigi.burigana@unipd.it;
   carol.a.osullivan@disney.com
RI VICOVARO, MICHELE/KDO-6712-2024; Hoyet, Ludovic/IWU-9100-2023
OI VICOVARO, MICHELE/0000-0002-9656-1640; Hoyet,
   Ludovic/0000-0002-7373-6049; O'Sullivan, Carol/0000-0003-3772-4961;
   Burigana, Luigi/0000-0001-9898-8317
FU Science Foundation Ireland as part of the Captavatar project
   [10/IN.1/13003]
FX We wish to thank all reviewers for their comments and the participants
   in our experiments. This work was sponsored by Science Foundation
   Ireland as part of the Captavatar project (S.F.I. 10/IN.1/13003).
CR [Anonymous], 1998, Proc. SIGGRAPH, DOI 10.1145/280814.280820
   Barzel R., 1996, Computer Animation and Simulation '96. Proceedings of the Eurographics Workshop, P183
   Bozzi P, 1959, RIV PSICOL, V52, P281
   Brainard DH, 1997, SPATIAL VISION, V10, P433, DOI 10.1163/156856897X00357
   Bruderlin A., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P97, DOI 10.1145/218380.218421
   Chaminade T, 2007, SOC COGN AFFECT NEUR, V2, P206, DOI 10.1093/scan/nsm017
   CLEMENT J, 1982, AM J PHYS, V50, P66, DOI 10.1119/1.12989
   CORNSWEET TN, 1962, AM J PSYCHOL, V75, P485, DOI 10.2307/1419876
   García-Pérez MA, 2001, OPTOMETRY VISION SCI, V78, P56, DOI 10.1097/00006324-200101010-00015
   Gleicher M., 1997, Proceedings 1997 Symposium on Interactive 3D Graphics, P139, DOI 10.1145/253284.253321
   Gleicher M., 2001, P 2001 S INTERACTIVE, P195
   Hecht H, 2000, J EXP PSYCHOL HUMAN, V26, P730, DOI 10.1037/0096-1523.26.2.730
   Hodgins JK, 1998, IEEE T VIS COMPUT GR, V4, P307, DOI 10.1109/2945.765325
   Hoyet L., 2012, P ACM SIGGRAPH S INT, P79, DOI DOI 10.1145/2159616.2159630.6,17
   Hoyet L, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185586
   Hsu E, 2007, SYMPOSIUM ON COMPUTER ANIMATION 2007: ACM SIGGRAPH/ EUROGRAPHICS SYMPOSIUM PROCEEDINGS, P45
   KAISER MK, 1992, J EXP PSYCHOL HUMAN, V18, P669, DOI 10.1037/0096-1523.18.3.669
   Kleiner M, 2007, PERCEPTION, V36, P14
   Knoblich G, 2001, PSYCHOL SCI, V12, P467, DOI 10.1111/1467-9280.00387
   Komura T, 2005, COMPUT ANIMAT VIRT W, V16, P213, DOI 10.1002/cav.101
   Liu Karen, 2006, P 2006 ACM SIGGRAPHE, P215
   Majkowska A, 2007, SYMPOSIUM ON COMPUTER ANIMATION 2007: ACM SIGGRAPH/ EUROGRAPHICS SYMPOSIUM PROCEEDINGS, P35
   MCCLOSKEY M, 1980, SCIENCE, V210, P1139, DOI 10.1126/science.210.4474.1139
   Munzert J, 2010, EUR J COGN PSYCHOL, V22, P247, DOI 10.1080/09541440902757975
   Nusseck M, 2007, APGV 2007: SYMPOSIUM ON APPLIED PERCEPTION IN GRAPHICS AND VISUALIZATION, PROCEEDINGS, P27
   O'Sullivan C, 2003, ACM T GRAPHIC, V22, P527, DOI 10.1145/882262.882303
   Park SI, 2004, COMPUT ANIMAT VIRT W, V15, P125, DOI 10.1002/cav.15
   Pelli DG, 1997, SPATIAL VISION, V10, P437, DOI 10.1163/156856897X00366
   Pollard N.S., 2001, P 11 YAL WORKSH AD L, VVolume 2
   PROFFITT DR, 1989, J EXP PSYCHOL HUMAN, V15, P384, DOI 10.1037/0096-1523.15.2.384
   Reitsma PSA, 2008, COMPUT GRAPH FORUM, V27, P201, DOI 10.1111/j.1467-8659.2008.01117.x
   Reitsma PSA, 2009, ACM T APPL PERCEPT, V6, DOI [10.1145//1577755.1577758, 10.1145/1577755.1577758]
   RUNESON S, 1983, J EXP PSYCHOL GEN, V112, P585, DOI 10.1037/0096-3445.112.4.585
   Shang Guo, 1996, Computer Animation and Simulation '96. Proceedings of the Eurographics Workshop, P95
   Shin H.J., 2006, Proceedings of ACM SIGGRAPH/Eurographics Symposium on Computer Animation, P291
   Shin HJ, 2003, 11TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P194
   Tak S, 2005, ACM T GRAPHIC, V24, P98, DOI 10.1145/1037957.1037963
   Vicovaro M., 2012, Proceedings of the ACM SIGGRAPH/Eurographics 124 Symposium on Computer Animation, SCA '12, P175, DOI DOI 10.2312/SCA/SCA12/175-182
   Yamane K, 2003, IEEE T ROBOTIC AUTOM, V19, P421, DOI 10.1109/TRA.2003.810579
   Yeh TY, 2009, ACM T GRAPHIC, V29, DOI 10.1145/1640443.1640448
   Zordan VB, 2005, ACM T GRAPHIC, V24, P697, DOI 10.1145/1073204.1073249
NR 41
TC 10
Z9 12
U1 0
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD MAY
PY 2014
VL 11
IS 2
AR 10
DI 10.1145/2617916
PG 23
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA AO3OJ
UT WOS:000341241100006
DA 2024-07-18
ER

PT J
AU Ziat, M
   Au, C
   Abolhassani, AH
   Clark, JJ
AF Ziat, Mounia
   Au, Carmen
   Abolhassani, Amin Haji
   Clark, James J.
TI Enhancing Visuospatial Map Learning through Action on Cellphones
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Experimentation; Human Factors; Performance; Cell phones; Human
   attention; multimodal perception; auditory feedback; map exploration;
   map learning
ID ENVIRONMENTS
AB The visuospatial learning of a map on cellphone displays was examined. The spatial knowledge of human participants was assessed after they had learned the relative positions of London Underground stations on a map via passive, marginally active, or active exploration. Following learning, the participants were required to answer questions in relation to the spatial representation and distribution of the stations on the map. Performances were compared between conditions involving (1) without auditory cues versus continuous auditory cues; (2) without auditory cues versus noncontinuous auditory cues; and (3) continuous auditory cues versus noncontinuous auditory cues. Results showed that the participants perfomed better following active and marginally-active explorations, as compared to purely passive learning. These results also suggest that under specific conditions (i.e., continuous sound with extremely fast tempo) there is no benefit to spatial abilities from active exploration over passive observation; while continuous sound with moderate to fast tempo is effective for simple actions (i.e., key press).
C1 [Ziat, Mounia] No Michigan Univ, Dept Psychol, Marquette, MI 49855 USA.
   [Ziat, Mounia; Au, Carmen; Abolhassani, Amin Haji; Clark, James J.] McGill Univ, Ctr Intelligent Machines, Montreal, PQ H3A 2T5, Canada.
C3 Northern Michigan University; McGill University
RP Ziat, M (corresponding author), No Michigan Univ, Dept Psychol, Marquette, MI 49855 USA.
EM mziat@nmu.edu
RI Ziat, Mounia/B-6150-2011; Ziat, Mounia/JRX-8101-2023
OI Ziat, Mounia/0000-0003-4620-7886; Clark, James/0000-0002-4512-6171
CR [Anonymous], P UIST 96
   Arbel T, 2002, IMAGE VISION COMPUT, V20, P639, DOI 10.1016/S0262-8856(02)00053-7
   Brewster S. A., 1998, ACM Transactions on Computer-Human Interaction, V5, P224, DOI 10.1145/292834.292839
   Carassa A., 2002, B PEOPLE ENV STUDIES, V20, P15
   CHATTING D, 2008, P TEI 08, P187
   Christou CG, 1999, MEM COGNITION, V27, P996, DOI 10.3758/BF03201230
   Crowe DA, 2008, J NEUROSCI, V28, P5218, DOI 10.1523/JNEUROSCI.5105-07.2008
   DODIYA J, 2008, P ACM S VIRT REAL SO
   DONG L, 2005, P 7 INT C HUM COMP I, P235
   Duchamp D., 1991, IEEE Network, V5, P12, DOI 10.1109/65.103804
   Farrell MJ, 2003, J EXP PSYCHOL-APPL, V9, P219, DOI 10.1037/1076-898X.9.4.219
   FORMAN GH, 1994, COMPUTER, V27, P38, DOI 10.1109/2.274999
   Guiard Y., 1999, P SIGCHI C HUMAN FAC, P450, DOI [https://doi.org/10.1145/302979.303128, DOI 10.1145/302979.303128]
   Hinckley Ken., 2000, Proceedings of the 13th annual ACM symposium on User interface software and technology - UIST'00, P91, DOI DOI 10.1145/354401.354417
   Husain G, 2002, MUSIC PERCEPT, V20, P151, DOI 10.1525/mp.2002.20.2.151
   Ishikawa T, 2008, J ENVIRON PSYCHOL, V28, P74, DOI 10.1016/j.jenvp.2007.09.002
   Klatzky R. L., 1998, Spatial Cognition. An Interdisciplinary Approach to Representing and Processing Spatial Knowledge, P1
   KRUGER A., 2004, P MOB HCI, P446
   Lokki T., 2000, P INT C AUD DISPL IC, P145
   MUNZER S., 2000, J ENVIRON PSYCHOL, V26, P300
   NOKIA, 2008, NOK UK NOK 5500 TECH
   OAKLEY I., 2000, P EUR 04, P316
   PERUCH P, 1995, ECOL PSYCHOL, V7, P1, DOI 10.1207/s15326969eco0701_1
   Tran TV, 2000, ERGONOMICS, V43, P807, DOI 10.1080/001401300404760
   Walker B.N., 2003, Proceedings of the 9th International Conference on Auditory Display, P204
   WILLIS K., 2008, COMPUTERS ENV URBAN
   [No title captured]
NR 27
TC 1
Z9 1
U1 0
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD MAR
PY 2012
VL 9
IS 1
AR 5
DI 10.1145/2134203.2134208
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA 949YM
UT WOS:000304614400005
DA 2024-07-18
ER

PT J
AU Au, CE
   Clark, JJ
AF Au, Carmen E.
   Clark, James J.
TI Integrating Multiple Views with Virtual Mirrors to Facilitate Scene
   Understanding
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Design; Experimentation; Human Factors; Performance; Image integration;
   psychophysics; visual task; scene identification; virtual mirror;
   attentional load
ID OBJECT RECOGNITION; DEPENDENCE; REFLECTIONS; INFORMATION
AB In this article, an image integration technique called Virtual Mirroring (VM) is evaluated. VM is a technique that combines multiple 2D views of a 3D scene into a single composite image by overlaying views onto virtual mirrors. Given multiple views of a scene, one view is augmented with the remaining views by placing virtual mirrors on the first view and overlaying onto them the corresponding remaining views. Unlike a standard array presentation, where 2D views are not integrated and simply placed adjacent to one another, the VM presentation preserves the relative location, orientation, and scale between views. As such, it is our contention that humans will fare better at performing certain visual tasks, such as scene identification, when viewing a 3D scene via a VM presentation than when viewing an array presentation. We performed an experiment on 12 participants, where participants were required to identify 96 scenes both with a VM and an array presentation and we compared their % correctness and response times. Moreover, we studied the effects of adding an auditory attentional load on performance. We found that regardless of load, participants were able to identify scenes using VM presentation with greater accuracy and at greater speeds.
C1 [Au, Carmen E.; Clark, James J.] McGill Univ, Montreal, PQ H3A 2T5, Canada.
C3 McGill University
RP Au, CE (corresponding author), 3480 Sherbrooke W, Montreal, PQ H3A 2A7, Canada.
EM au@cim.mcgill.ca
OI Clark, James/0000-0002-4512-6171
FU EC [043157]
FX This work was funded by the EC under grant 043157, project SynTex.
CR Alais D, 1999, NAT NEUROSCI, V2, P1015, DOI 10.1038/14814
   [Anonymous], P IEEE COMP SOC C CO
   [Anonymous], CSD981003 U CAL
   [Anonymous], P 9 EUR C COMP VIS E
   [Anonymous], P WORKSH STAT METH V
   Au CE, 2008, PROCEEDINGS OF THE FIFTH CANADIAN CONFERENCE ON COMPUTER AND ROBOT VISION, P286, DOI 10.1109/CRV.2008.39
   Beis JS, 1997, PROC CVPR IEEE, P1000, DOI 10.1109/CVPR.1997.609451
   Bertamini M, 2005, COGNITION, V98, P85, DOI 10.1016/j.cognition.2004.11.002
   Bertamini M, 2003, PERCEPTION, V32, P593, DOI 10.1068/p3418
   BIEDERMAN I, 1995, J EXP PSYCHOL HUMAN, V21, P1506, DOI 10.1037/0096-1523.21.6.1506
   BIEDERMAN I, 1987, PSYCHOL REV, V94, P115, DOI 10.1037/0033-295X.94.2.115
   BIEDERMAN I, 1993, J EXP PSYCHOL HUMAN, V19, P1162, DOI 10.1037/0096-1523.19.6.1162
   Broadbent DE, 2013, PERCEPTION COMMUNICA
   Brown M, 2007, INT J COMPUT VISION, V74, P59, DOI 10.1007/s11263-006-0002-3
   BULTHOFF HH, 1995, CEREB CORTEX, V5, P247, DOI 10.1093/cercor/5.3.247
   Castelhano MS, 2009, ATTEN PERCEPT PSYCHO, V71, P490, DOI 10.3758/APP.71.3.490
   Christou CG, 1999, MEM COGNITION, V27, P996, DOI 10.3758/BF03201230
   Croucher CJ, 2002, J EXP PSYCHOL HUMAN, V28, P546, DOI 10.1037//0096-1523.28.3.546
   EDELMAN S, 1991, BIOL CYBERN, V64, P209, DOI 10.1007/BF00201981
   Garsoffky B, 2002, J EXP PSYCHOL LEARN, V28, P1035, DOI 10.1037//0278-7393.28.6.1035
   Hartley R, 2007, IEEE T PATTERN ANAL, V29, P1309, DOI 10.1109/TPAMI.2007.1147
   Hayward WG, 2000, PSYCHOL SCI, V11, P7, DOI 10.1111/1467-9280.00207
   HOCK HS, 1980, MEM COGNITION, V8, P543, DOI 10.3758/BF03213774
   Irani M., 2000, Vision Algorithms: Theory and Practice. International Workshop on Vision Algorithms. Proceedings (Lecture Notes in Computer Science Vol. 1883), P267
   Kannala J, 2006, IEEE T PATTERN ANAL, V28, P1335, DOI 10.1109/TPAMI.2006.153
   Lawson R, 2006, PERCEPTION, V35, P1265, DOI 10.1068/p5498
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   MARR D, 1978, TECHNOL REV, V81, P28
   Nakatani C, 2002, Q J EXP PSYCHOL-A, V55, P115, DOI 10.1080/02724980143000190
   POGGIO T, 1990, NATURE, V343, P263, DOI 10.1038/343263a0
   Rankov V, 2005, PROC SPIE, V5701, P190, DOI 10.1117/12.590536
   Rees G, 1997, SCIENCE, V278, P1616, DOI 10.1126/science.278.5343.1616
   Tardif JP, 2009, IEEE T PATTERN ANAL, V31, P1552, DOI 10.1109/TPAMI.2008.202
   TARR MJ, 1989, COGNITIVE PSYCHOL, V21, P233, DOI 10.1016/0010-0285(89)90009-1
   Tarr MJ, 1998, NAT NEUROSCI, V1, P275, DOI 10.1038/1089
   Ullman S., 1991, IEEE Transactions on Pattern and Analysis and Machine Intelligence, V13, P193
   VETTER T, 1994, CURR BIOL, V4, P18, DOI 10.1016/S0960-9822(00)00004-X
   Wallis G, 1999, TRENDS COGN SCI, V3, P22, DOI 10.1016/S1364-6613(98)01261-3
   Zhang ZY, 2000, IEEE T PATTERN ANAL, V22, P1330, DOI 10.1109/34.888718
   ZHENG Y., 2008, P 19 INT C PATTERN R, P1
NR 40
TC 0
Z9 0
U1 0
U2 2
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD NOV
PY 2011
VL 8
IS 4
AR 28
DI 10.1145/2043603.2043610
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA 856IQ
UT WOS:000297633400007
DA 2024-07-18
ER

PT J
AU Hodgson, E
   Bachmann, E
   Waller, D
AF Hodgson, Eric
   Bachmann, Eric
   Waller, David
TI Redirected Walking to Explore Virtual Environments: Assessing the
   Potential for Spatial Interference
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Algorithms; Experimentation; Human Factors; Measurement; Performance;
   Redirected walking; spatial cognition; spatial memory; spatial senses;
   multimodal sensory integration; human computer interaction; virtual
   reality; 3D interfaces
ID MOTION COMPRESSION; VISUAL DOMINANCE; PERCEPTION; INFORMATION; POSITION
AB Redirected walking has gained popularity in recent years as a way of enhancing the safety of users immersed in a virtual reality simulation and of extending the amount of space that can be simulated in a virtual environment (VE). Limits imposed by the available physical space and functional tracking area are overcome by inducing immersed users to veer imperceptibly in a way that prevents them from leaving the confines of the tracking space. Redirected walking has been shown to be feasible at levels below noticeable thresholds and to function without increasing the incidence of simulator sickness. The present studies demonstrate that redirected walking can function without negatively impacting memory for spatial locations of landmarks in a VE, despite introducing discrepancies between various spatial senses and distorting the spatial mapping of movement onto the environment. Additionally, the present studies implement what, to our knowledge, is the first generalized redirected walking algorithm that is independent of any task or environment structure, and can adaptively steer users in real time as they engage in spontaneous, unconstrained navigation. The studies also demonstrate that such an algorithm can be implemented successfully in a gymnasium-sized space.
C1 [Hodgson, Eric; Waller, David] Miami Univ, Dept Psychol, Oxford, OH 45056 USA.
   [Bachmann, Eric] Miami Univ, Dept Comp Sci, Oxford, OH 45056 USA.
C3 University System of Ohio; Miami University; University System of Ohio;
   Miami University
RP Hodgson, E (corresponding author), Miami Univ, Dept Psychol, 90 N Patterson Ave, Oxford, OH 45056 USA.
EM eric.hodgson@muohio.edu; bachmaer@muohio.edu; wallerda@muohio.edu
FU Army Research Office; Ohio Board of Regents; Miami University
FX This research was funded by grants from the Army Research Office, the
   Ohio Board of Regents, and Miami University.
CR Burns E, 2005, P IEEE VIRT REAL ANN, P3
   Fukusima SS, 1997, J EXP PSYCHOL HUMAN, V23, P86, DOI 10.1037/0096-1523.23.1.86
   Gibson JJ, 1933, J EXP PSYCHOL, V16, P1, DOI 10.1037/h0074626
   Jaekl PM, 2005, EXP BRAIN RES, V163, P388, DOI 10.1007/s00221-004-2191-8
   Jerald J., 2009, Proceedings of IEEE Virtual Reality Workshop on Perceptual Illusions in Virtual Environments (PIVE), P4
   Jerald J, 2008, APGV 2008: PROCEEDINGS OF THE SYMPOSIUM ON APPLIED PERCEPTION IN GRAPHICS AND VISUALIZATION, P155
   Jürgens R, 1999, EXP BRAIN RES, V128, P563, DOI 10.1007/s002210050882
   Klatzky RL, 1998, PSYCHOL SCI, V9, P293, DOI 10.1111/1467-9280.00058
   Kohli L., 2005, Proceedings of the 2005 international conference on Augmented tele-existence, P253
   LIN JJW, 2005, P INT S THEOR ISS ER
   LOOMIS JM, 1992, J EXP PSYCHOL HUMAN, V18, P906, DOI 10.1037/0096-1523.18.4.906
   Mou WM, 2004, J EXP PSYCHOL LEARN, V30, P142, DOI 10.1037/0278-7393.30.1.142
   Nitzsche N, 2004, PRESENCE-TELEOP VIRT, V13, P44, DOI 10.1162/105474604774048225
   PECK TC, 2009, IEEE T VISUALIZATION, V15, P3
   POSNER MI, 1976, PSYCHOL REV, V83, P157, DOI 10.1037/0033-295X.83.2.157
   Razzaque S., 2005, REDIRECTED WALKING
   Razzaque S., 2001, P EUR
   RIESER JJ, 1995, J EXP PSYCHOL HUMAN, V21, P480, DOI 10.1037/0096-1523.21.3.480
   Souman JL, 2009, CURR BIOL, V19, P1538, DOI 10.1016/j.cub.2009.07.053
   Steinicke F., 2008, Proceedings of the ACM Symposium on Virtual Reality Software and Technology (VRST), P149, DOI 10.1145/1450579.1450611
   Steinicke F., 2008, Proceedings of the Virtual Reality International Conference (VRIC), P15
   STEINICKE F, 2009, IEEE T VISUALIZATION, P17
   Steinicke F., 2009, J VIRTUAL REALITY BR, V6
   Su JB, 2007, PRESENCE-VIRTUAL AUG, V16, P385, DOI 10.1162/pres.16.4.385
   van Beers RJ, 1999, J NEUROPHYSIOL, V81, P1355, DOI 10.1152/jn.1999.81.3.1355
   Waller D, 2007, BEHAV RES METHODS, V39, P835, DOI 10.3758/BF03192976
   Waller D, 2006, J EXP PSYCHOL LEARN, V32, P867, DOI 10.1037/0278-7393.32.4.867
   Wang RF, 2003, PSYCHON B REV, V10, P981, DOI 10.3758/BF03196562
   Wang RXF, 2004, PERCEPT PSYCHOPHYS, V66, P68, DOI 10.3758/BF03194862
   Williams B, 2007, APGV 2007: SYMPOSIUM ON APPLIED PERCEPTION IN GRAPHICS AND VISUALIZATION, PROCEEDINGS, P41
NR 30
TC 42
Z9 53
U1 0
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD NOV
PY 2011
VL 8
IS 4
AR 22
DI 10.1145/2043603.2043604
PG 22
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA 856IQ
UT WOS:000297633400001
DA 2024-07-18
ER

PT J
AU Wilkie, RM
   Wann, JP
   Allison, RS
AF Wilkie, Richard M.
   Wann, John P.
   Allison, Robert S.
TI Modeling Locomotor Control: The Advantages of Mobile Gaze
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Algorithms; Design; Experimentation; Performance; Theory; Robot;
   locomotion; steering; gaze; eye movements; active vision
ID EYE-MOVEMENTS; VISUAL DIRECTION; JUMPING SPIDERS; RETINAL FLOW; OPTIC
   FLOW; GUIDANCE; TARGET; FLIGHT; HEAD; COORDINATION
AB In 1958, JJ Gibson put forward proposals on the visual control of locomotion. Research in the last 50 years has served to clarify the sources of visual and nonvisual information that contribute to successful steering, but has yet to determine how this information is optimally combined under conditions of uncertainty. Here, we test the conditions under which a locomotor robot with a mobile camera can steer effectively using simple visual and extra-retinal parameters to examine how such models cope with the noisy real-world visual and motor estimates that are available to humans. This applied modeling gives us an insight into both the advantages and limitations of using active gaze to sample information when steering.
C1 [Wilkie, Richard M.] Univ Leeds, Inst Psychol Sci, Leeds LS2 9JT, W Yorkshire, England.
   [Wann, John P.] Univ London, Dept Psychol, London WC1E 7HU, England.
   [Allison, Robert S.] York Univ, Ctr Vis Res, Toronto, ON M3J 2R7, Canada.
C3 University of Leeds; University of London; York University - Canada
RP Wilkie, RM (corresponding author), Univ Leeds, Inst Psychol Sci, Leeds LS2 9JT, W Yorkshire, England.
EM r.m.wilkie@leeds.ac.uk
RI Wilkie, Richard M./G-5977-2012
OI Wilkie, Richard M./0000-0003-4299-7171; Allison,
   Robert/0000-0002-4485-2665
FU UK EPSRC [GR/S86358, EP/D055342/1]; Centre for Vision Research,
   University of York, Toronto; EPSRC [EP/D055342/1] Funding Source: UKRI
FX Research supported by the UK EPSRC GR/S86358 and EP/D055342/1.; Thanks
   to the Centre for Vision Research, University of York, Toronto for their
   support during the collaborative research visit that contributed to the
   work presented here. Thanks also to Georgios Kountouriotis for his
   technical help with the manuscript.
CR [Anonymous], 2003, Control Theory for Humans: Quantitative Approaches to Modeling Performance
   Bajracharya M, 2008, COMPUTER, V41, P44, DOI 10.1109/MC.2008.479
   Bansal M, 2008, PROCEEDINGS OF THE 11TH INTERNATIONAL IEEE CONFERENCE ON INTELLIGENT TRANSPORTATION SYSTEMS, P434, DOI 10.1109/ITSC.2008.4732699
   Bicho E, 1997, ROBOT AUTON SYST, V21, P23, DOI 10.1016/S0921-8890(97)00004-3
   BRINDLEY GS, 1960, J PHYSIOL-LONDON, V153, P127, DOI 10.1113/jphysiol.1960.sp006523
   Darms MS, 2009, IEEE T INTELL TRANSP, V10, P475, DOI 10.1109/TITS.2009.2018319
   Fajen BR, 2004, PERCEPTION, V33, P689, DOI 10.1068/p5236
   Fajen BR, 2003, J EXP PSYCHOL HUMAN, V29, P343, DOI 10.1037/0096-1523.29.2.343
   Ghose K, 2006, J NEUROSCI, V26, P1704, DOI 10.1523/JNEUROSCI.4315-05.2006
   GIBSON JJ, 1958, BRIT J PSYCHOL, V49, P182, DOI 10.1111/j.2044-8295.1958.tb00656.x
   GNADT JW, 1991, VISION RES, V31, P693, DOI 10.1016/0042-6989(91)90010-3
   HELD R, 1961, J COMP PHYSIOL PSYCH, V54, P33, DOI 10.1037/h0046207
   Kim NG, 1999, ECOL PSYCHOL, V11, P233, DOI 10.1207/s15326969eco1103_3
   Kim WS, 2009, J FIELD ROBOT, V26, P243, DOI 10.1002/rob.20283
   LAND MF, 1992, NATURE, V359, P318, DOI 10.1038/359318a0
   Land MF, 2001, CURR BIOL, V11, P1215, DOI 10.1016/S0960-9822(01)00351-7
   LAND MF, 1994, NATURE, V369, P742, DOI 10.1038/369742a0
   Land MF, 1997, PHILOS T ROY SOC B, V352, P1231, DOI 10.1098/rstb.1997.0105
   Land MF, 2001, VISION RES, V41, P3559, DOI 10.1016/S0042-6989(01)00102-X
   Land MF, 2005, J COMP PHYSIOL A, V191, P639, DOI 10.1007/s00359-005-0616-x
   Land MF, 2004, EXP BRAIN RES, V159, P151, DOI 10.1007/s00221-004-1951-9
   LAND MF, 1969, J EXP BIOL, V51, P471
   LAND MF, 1971, J EXP BIOL, V54, P119
   Lee DN, 1998, ECOL PSYCHOL, V10, P221, DOI 10.1207/s15326969eco103&4_4
   LLEWELLYN KR, 1971, J EXP PSYCHOL, V91, P245, DOI 10.1037/h0031788
   *MOT SAF FDN I, 1992, MOT EXC SKILLS KNOWL
   Murray DW, 1997, PERCEPTION, V26, P1519, DOI 10.1068/p261519
   Robertshaw KD, 2008, J VISION, V8, DOI 10.1167/8.4.18
   ROYDEN CS, 1992, NATURE, V360, P583, DOI 10.1038/360583a0
   RUSHTON S, 2002, P INT WORKSH BIOL MO, P576
   Rushton SK, 1998, CURR BIOL, V8, P1191, DOI 10.1016/S0960-9822(07)00492-7
   Salvucci DD, 2004, PERCEPTION, V33, P1233, DOI 10.1068/p5343
   Schoner G, 1995, ROBOT AUTON SYST, V16, P213, DOI 10.1016/0921-8890(95)00049-6
   Tucker VA, 2000, J EXP BIOL, V203, P3745
   Wann J, 2000, TRENDS COGN SCI, V4, P319, DOI 10.1016/S1364-6613(00)01513-8
   Wann JP, 2000, NAT NEUROSCI, V3, P647, DOI 10.1038/76602
   WANN JP, 1992, EXP BRAIN RES, V91, P162
   Warren WH, 2001, NAT NEUROSCI, V4, P213, DOI 10.1038/84054
   WHITE JM, 1994, VISION RES, V34, P79, DOI 10.1016/0042-6989(94)90259-3
   Wilkie R, 2003, J EXP PSYCHOL HUMAN, V29, P363, DOI 10.1037/0096-1523.29.2.363
   WILKIE R, 2003, J VISION, V3, pS551
   Wilkie RM, 2008, J EXP PSYCHOL HUMAN, V34, P1150, DOI 10.1037/0096-1523.34.5.1150
   Wilkie RM, 2003, J VISION, V3, P677, DOI 10.1167/3.11.3
   Wilkie RM, 2006, J EXP PSYCHOL HUMAN, V32, P88, DOI 10.1037/0096-1523.32.1.88
   Wilkie RM, 2005, J EXP PSYCHOL HUMAN, V31, P901, DOI 10.1037/0096-1523.31.5.901
   Wilkie RM, 2002, CURR BIOL, V12, P2014, DOI 10.1016/S0960-9822(02)01337-4
NR 46
TC 4
Z9 4
U1 0
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD JAN
PY 2011
VL 8
IS 2
AR 9
DI 10.1145/1870076.1870077
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA 748AR
UT WOS:000289362900001
DA 2024-07-18
ER

PT J
AU Bonneel, N
   Suied, C
   Viaud-Delmon, I
   Drettakis, G
AF Bonneel, Nicolas
   Suied, Clara
   Viaud-Delmon, Isabelle
   Drettakis, George
TI Bimodal Perception of Audio-Visual Material Properties for Virtual
   Environments
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Algorithms; Experimentation; Audio-visual rendering; crossmodal
   perception; material perception; bimodal perception
AB High-quality rendering of both audio and visual material properties is very important in interactive virtual environments, since convincingly rendered materials increase realism and the sense of immersion. We studied how the level of detail of auditory and visual stimuli interact in the perception of audio-visual material rendering quality. Our study is based on perception of material discrimination, when varying the levels of detail of modal synthesis for sound, and bidirectional reflectance distribution functions for graphics. We performed an experiment for two different models ( a Dragon and a Bunny model) and two material types ( plastic and gold). The results show a significant interaction between auditory and visual level of detail in the perception of material similarity, when comparing approximate levels of detail to a high-quality audio-visual reference rendering. We show how this result can contribute to significant savings in computation time in an interactive audio-visual rendering system. To our knowledge, this is the first study that shows interaction of audio and graphics representation in a material perception task.
C1 [Bonneel, Nicolas; Drettakis, George] INRIA Sophia Antipolis, REVES, Sophia Antipolis, France.
   [Suied, Clara; Viaud-Delmon, Isabelle] UPMC, CNRS, UMR 7593, Paris, France.
C3 Centre National de la Recherche Scientifique (CNRS); Sorbonne Universite
RP Bonneel, N (corresponding author), INRIA Sophia Antipolis, REVES, Sophia Antipolis, France.
EM Nicolas.Bonneel@sophia.inria.fr
RI Bonneel, Nicolas/AAK-9935-2021
OI Viaud-Delmon, Isabelle/0000-0002-8664-3941; Bonneel,
   Nicolas/0000-0001-5243-4810
FU EU IST FET OPEN [014891-2]
FX This research was funded by the EU IST FET OPEN project CROSSMOD
   (014891-2 http://www.crossmod.org).
CR [Anonymous], P 2 S APPL PERC GRAP
   Ben-Artzi A, 2006, ACM T GRAPHIC, V25, P945, DOI 10.1145/1141911.1141979
   BONNEEL N, 2008, P SIGGRAPH C ACM NEW
   Cook R. L., 1982, ACM T GRAPHIC, V1, P7, DOI DOI 10.1145/357290.357293
   DEBEVEC P, 1998, ACM SIGGRAPH 98 ELEC, P166
   DOEL KVD, 2002, P INT C AUD DISPL, P345
   Fleming RW, 2003, J VISION, V3, P347, DOI 10.1167/3.5.3
   Fujisaki W, 2004, NAT NEUROSCI, V7, P773, DOI 10.1038/nn1268
   Giordano BL, 2006, J ACOUST SOC AM, V119, P1171, DOI 10.1121/1.2149839
   Green R., 2003, P GAM DEV C
   GRELAUD D, 2009, P S INT 3D GRAPH GAM
   James DL, 2006, ACM T GRAPHIC, V25, P987, DOI 10.1145/1141911.1141983
   KAUTZ J, 2002, P 13 EUR WORKSH REND, P291
   Klatzky RL, 2000, PRESENCE-TELEOP VIRT, V9, P399, DOI 10.1162/105474600566907
   Kristensen AW, 2005, ACM T GRAPHIC, V24, P1208, DOI 10.1145/1073204.1073334
   Matusik W, 2003, ACM T GRAPHIC, V22, P759, DOI 10.1145/882262.882343
   McAdams S, 2004, J ACOUST SOC AM, V115, P1306, DOI 10.1121/1.1645855
   Ng R, 2003, ACM T GRAPHIC, V22, P376, DOI 10.1145/882262.882280
   OBrien J.F., 2002, Proceedings of the 2002 ACM SIGGRAPH/Eurographics symposium on Computer animation, P175
   Raghuvanshi N, 2007, IEEE COMPUT GRAPH, V27, P14, DOI 10.1109/MCG.2007.16
   Ramamoorthi R, 2001, COMP GRAPH, P497, DOI 10.1145/383259.383317
   RAMAMOORTHI R, 2001, J OPTICAL SOC AM
   Ramanarayanan G, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1276377.1276472, 10.1145/1239451.1239527]
   Ramantoorthi R, 2002, ACM T GRAPHIC, V21, P517, DOI 10.1145/566570.566611
   Reinhard E, 2002, ACM T GRAPHIC, V21, P267, DOI 10.1145/566570.566575
   Sloan PP, 2005, ACM T GRAPHIC, V24, P1216, DOI 10.1145/1073204.1073335
   SLOAN PP, 2002, P ACM SIGGRAPH, P527
   Storms RL, 2000, PRESENCE-TELEOP VIRT, V9, P557, DOI 10.1162/105474600300040385
   VANDENDOEL K, 2003, MODAL SYNTHESIS VIBR
   Vangorp P, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1276377.1276473, 10.1145/1239451.1239528]
   William D., 2006, P 2006 S INT 3D GRAP, P161, DOI DOI 10.1145/1111411.1111440
NR 31
TC 29
Z9 30
U1 2
U2 10
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD JAN
PY 2010
VL 7
IS 1
AR 1
DI 10.1145/1658349.1658350
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 549EK
UT WOS:000274028400001
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Jay, C
   Stevens, R
   Hubbold, R
   Glencross, M
AF Jay, Caroline
   Stevens, Robert
   Hubbold, Roger
   Glencross, Mashhuda
TI Using Haptic Cues to Aid Nonvisual Structure Recognition
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Design; Experimentation; Human Factors; Multimodal cues; haptic
   perception; visual disability; accessibility
AB Retrieving information presented visually is difficult for visually disabled users. Current accessibility technologies, such as screen readers, fail to convey presentational layout or structure. Information presented in graphs or images is almost impossible to convey through speech alone. In this paper, we present the results of an experimental study investigating the role of touch (haptic) and auditory cues in aiding structure recognition when visual presentation is missing. We hypothesize that by guiding users toward nodes in a graph structure using force fields, users will find it easier to recognize overall structure. Nine participants were asked to explore simple 3D structures containing nodes (spheres or cubes) laid out in various spatial configurations and asked to identify the nodes and draw their overall structure. Various combinations of haptic and auditory feedback were explored. Our results demonstrate that haptic cues significantly helped participants to quickly recognize nodes and structure. Surprisingly, auditory cues alone did not speed up node recognition; however, when they were combined with haptics both node identification and structure recognition significantly improved. This result demonstrates that haptic feedback plays an important role in enabling people to recall spatial layout.
C1 [Jay, Caroline; Stevens, Robert; Hubbold, Roger; Glencross, Mashhuda] Univ Manchester, Manchester, Lancs, England.
C3 University of Manchester
RP Jay, C (corresponding author), Univ Manchester, Manchester, Lancs, England.
EM caroline.jay@mancester.ac.u
RI Glencross, Mashhuda/ABF-1656-2020
OI Glencross, Mashhuda/0000-0002-8964-2143; Jay,
   Caroline/0000-0002-6080-1382
CR ALDRICH FK, 2003, RNIB VISABILITY, V39, P25
   ANDO H, 2006, SIGGRAPH 06 ACM SIGG
   Brown A., 2006, P INT C AUD DISPL 20
   Colwell C., 1998, ASSETS'98. Third International ACM Conference on Assistive Technologies, P92, DOI 10.1145/274497.274515
   EDMAN PK, 1991, TACTILE GRAPHICS
   FACONTI GP, 2000, COMPUT GRAPH FORUM, V19, P3
   KILDAL J, 2006, P ACM CHI, P947
   LEDERMAN SJ, 1987, COGNITIVE PSYCHOL, V19, P342, DOI 10.1016/0010-0285(87)90008-9
   McGookin DavidK., 2006, CHI '06 extended abstracts on Human factors in computing systems, CHI EA '06, P267
   MCGOOKIN DK, 2006, ICAD 2006
   PETRIE H., 1997, British Journal of Visual Impairment, V15, P63, DOI DOI 10.1177/026461969701500205
   Ramloll Rameshsharma, 2002, CHI 02 EXTENDED ABST, P770, DOI [10.1145/506443.506589, DOI 10.1145/506443.506589]
   SJOSTROM C, 2001, P 3 INT WORKSH WEBS
   SJOSTROM C, 2001, AAATE
   Wall S., 2006, Conference on Human Factors in Computing Systems. CHI2006, P1123
   WALL SA, 2006, NORDICHI 06, P9
   Yu W, 2001, LECT NOTES COMPUT SC, V2058, P41
   YU W, 2002, ASSETS 02 P 5 INT AC, P57
NR 18
TC 10
Z9 11
U1 0
U2 6
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD MAY
PY 2008
VL 5
IS 2
AR 8
DI 10.1145/1279920.1279922
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 450YJ
UT WOS:000266437600002
OA Green Published, Green Accepted
DA 2024-07-18
ER

PT J
AU Vogel, J
   Schwaninger, A
   Wallraven, C
   Bülthoff, HH
AF Vogel, Julia
   Schwaninger, Adrian
   Wallraven, Christian
   Buelthoff, Heinrich H.
TI Categorization of Natural Scenes: Local versus Global Information and
   the Role of Color
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Human perception; Algorithms; Scene perception; scene classification;
   computational modeling; semantic modeling; computational gist; local
   region-based information; global configural information
ID MEMORY
AB Categorization of scenes is a fundamental process of human vision that allows us to efficiently and rapidly analyze our surroundings. Several studies have explored the processes underlying human scene categorization, but they have focused on processing global image information. In this study, we present both psychophysical and computational experiments that investigate the role of local versus global image information in scene categorization. In a first set of human experiments, categorization performance is tested when only local or only global image information is present. Our results suggest that humans rely on local, region-based information as much as on global, configural information. In addition, humans seem to integrate both types of information for intact scene categorization. In a set of computational experiments, human performance is compared to two state-of-the-art computer vision approaches that have been shown to be psychophysically plausible and that model either local or global information. In addition to the influence of local versus global information, in a second series of experiments, we investigated the effect of color on the categorization performance of both the human observers and the computational model. Analysis of the human data suggests that color is an additional channel of perceptual information that leads to higher categorization results at the expense of increased reaction times in the intact condition. However, it does not affect reaction times when only local information is present. When color is removed, the employed computational model follows the relative performance decrease of human observers for each scene category and can thus be seen as a perceptually plausible model for human scene categorization based on local image information.
C1 [Vogel, Julia] Univ British Columbia, Vancouver, BC V5Z 1M9, Canada.
   [Schwaninger, Adrian; Wallraven, Christian; Buelthoff, Heinrich H.] Max Planck Inst Biol Cybernet, Tubingen, Germany.
   [Schwaninger, Adrian] Univ Zurich, Dept Psychol, Zurich, Switzerland.
C3 University of British Columbia; Max Planck Society; University of Zurich
RP Vogel, J (corresponding author), Univ British Columbia, Vancouver, BC V5Z 1M9, Canada.
RI Bülthoff, Heinrich H/J-6579-2012; Bülthoff, Heinrich/AAC-8818-2019
OI Bülthoff, Heinrich H/0000-0003-2568-0607; Schwaninger,
   Adrian/0000-0001-7753-106X
CR BIEDERMA.I, 1972, SCIENCE, V177, P77, DOI 10.1126/science.177.4043.77
   Bosch A, 2006, LECT NOTES COMPUT SC, V3954, P517
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Delorme A, 2000, VISION RES, V40, P2187, DOI 10.1016/S0042-6989(00)00083-3
   Fei-Fei L, 2005, PROC CVPR IEEE, P524
   Fei-Fei L., 2005, VIS COGN, V12, P893, DOI DOI 10.1080/13506280444000571
   Fei-Fei L, 2007, J VISION, V7, DOI 10.1167/7.1.10
   Goffaux V, 2005, VIS COGN, V12, P878, DOI 10.1080/13506280444000562
   Hayward WG, 2003, TRENDS COGN SCI, V7, P425, DOI 10.1016/j.tics.2003.08.004
   Hayward WG, 2008, COGNITION, V106, P1017, DOI 10.1016/j.cognition.2007.04.002
   HENDERSON J, 2005, VISUAL COGNITION SPE, V12
   Henderson JM, 2005, VIS COGN, V12, P849, DOI 10.1080/13506280444000544
   Jain R., 1995, MACHINE VISION
   McCotter M, 2005, VIS COGN, V12, P938, DOI 10.1080/13506280444000599
   Mojsilovic A, 2004, INT J COMPUT VISION, V56, P79, DOI 10.1023/B:VISI.0000004833.39906.33
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   Oliva A, 2000, COGNITIVE PSYCHOL, V41, P176, DOI 10.1006/cogp.1999.0728
   Oliva A, 2006, PROG BRAIN RES, V155, P23, DOI 10.1016/S0079-6123(06)55002-2
   Oliva Aude, 2005, P251, DOI 10.1016/B978-012375731-9/50045-8
   Renninger LW, 2004, VISION RES, V44, P2301, DOI 10.1016/j.visres.2004.04.006
   ROGOWITZ B, 1997, SPIE C HUM VIS EL IM, P576
   ROSCH E, 1976, J EXP PSYCHOL HUMAN, V2, P491, DOI 10.1037/0096-1523.2.4.491
   Schwaninger A, 2003, DEVELOPMENT OF FACE PROCESSING, P81
   SCHWANINGER A, 2002, LECT NOTES COMPUTER, V2525
   SCHWANINGER A, 2006, ACM T APPL PERCEPT, V3, P333
   SCHYNS PG, 1994, PSYCHOL SCI, V5, P195, DOI 10.1111/j.1467-9280.1994.tb00500.x
   SZUMMER M, 1998, WORKSH CONT BAS ACC
   Thorpe S, 1996, NATURE, V381, P520, DOI 10.1038/381520a0
   TORRALBA A, 2004, AIM2004008 MIT AI LA
   TVERSKY B, 1983, COGNITIVE PSYCHOL, V15, P121, DOI 10.1016/0010-0285(83)90006-3
   Vailaya A, 2001, IEEE T IMAGE PROCESS, V10, P117, DOI 10.1109/83.892448
   Vogel J, 2007, INT J COMPUT VISION, V72, P133, DOI 10.1007/s11263-006-8614-1
   Wallraven C, 2005, NETWORK-COMP NEURAL, V16, P401, DOI 10.1080/09548980500508844
   Wichmann FA, 2002, J EXP PSYCHOL LEARN, V28, P509, DOI 10.1037//0278-7393.28.3.509
   Wolfe JM, 1998, CURR BIOL, V8, pR303, DOI 10.1016/S0960-9822(98)70192-7
NR 35
TC 30
Z9 36
U1 0
U2 7
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD NOV
PY 2007
VL 4
IS 3
AR 19
DI 10.1145/1278387.1278393
PG 21
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA V04IN
UT WOS:000207052200006
DA 2024-07-18
ER

PT J
AU Liu, WY
   Magalhaes, MA
   Mackay, WE
   Beaudouin-Lafon, M
   Bevilacqua, F
AF Liu, Wanyu
   Magalhaes, Michelle Agnes
   Mackay, Wendy E.
   Beaudouin-Lafon, Michel
   Bevilacqua, Frederic
TI Motor Variability in Complex Gesture Learning: Effects of Movement
   Sonification and Musical Background
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Motor variability; complex gesture learning; auditory feedback; movement
   sonification; musical background
ID PRINCIPLES; PERCEPTION; DYNAMICS; FEEDBACK; SYSTEMS
AB With the increasing interest in movement sonification and expressive gesture-based interaction, it is important to understand which factors contribute to movement learning and how. We explore the effects of movement sonification and users' musical background on motor variability in complex gesture learning. We contribute an empirical study in which musicians and non-musicians learn two gesture sequences over three days, with and without movement sonification. Results show the interlaced interaction effects of these factors and howthey unfold in the three-day learning process. For gesture 1, which is fast and dynamic with a direct "action-sound" sonification, movement sonification induces higher variability for both musicians and non-musicians on day 1. While musicians reduce this variability to a similar level as no auditory feedback condition on day 2 and day 3, non-musicians remain to have significantly higher variability. Across three days, musicians also have significantly lower variability than non-musicians. For gesture 2, which is slow and smooth with an "action-music" metaphor, there are virtually no effects. Based on these findings, we recommend future studies to take into account participants'musical background, consider longitudinal study to examine these effects on complex gestures, and use awareness when interpreting the results given a specific design of gesture and sound.
C1 [Liu, Wanyu; Magalhaes, Michelle Agnes; Bevilacqua, Frederic] Sorbonne Univ, CNRS, IRCAM, STMS, F-75004 Paris, France.
   [Mackay, Wendy E.; Beaudouin-Lafon, Michel] Univ Paris Saclay, INRIA, CNRS, F-91400 Orsay, France.
C3 Centre National de la Recherche Scientifique (CNRS); Sorbonne
   Universite; Inria; Centre National de la Recherche Scientifique (CNRS);
   Universite Paris Saclay; Microsoft; Universite Paris Cite
RP Liu, WY (corresponding author), Sorbonne Univ, CNRS, IRCAM, STMS, F-75004 Paris, France.
EM AbbyWanyu.Liu@ircam.fr; michelleagness@gmail.com; mackay@lri.fr;
   mbl@lri.fr; Frederic.Bevilacqua@ircam.fr
FU ELEMENT -Enabling Learnability in Embodied Movement Interaction
   [ANR-18-CE33-0002]; European Research Council (ERC) [695464, 321135];
   Insead-Sorbonne University Behavioral Lab; Agence Nationale de la
   Recherche (ANR) [ANR-18-CE33-0002] Funding Source: Agence Nationale de
   la Recherche (ANR)
FX This research was partially funded by ELEMENT -Enabling Learnability in
   Embodied Movement Interaction (ANR-18-CE33-0002), by European Research
   Council (ERC) grants n degrees 695464 "ONE: Unified Principles of
   Interaction" and n degrees 321135 "CREATIV: Creating Co-Adaptive
   Human-Computer Partnerships", and by Insead-Sorbonne University
   Behavioral Lab.
CR ADAMS JA, 1971, J MOTOR BEHAV, V3, P111
   Alaoui S.F., 2012, Proceedings of the Designing Interactive Systems Conference, P761, DOI DOI 10.1145/2317956.2318071
   Alvina J, 2016, UIST 2016: PROCEEDINGS OF THE 29TH ANNUAL SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P583, DOI 10.1145/2984511.2984560
   [Anonymous], 2015, OPEN PSYCHOL J
   Bettens Frederic, 2009, P SMC SOUND MUSIC CO, P30
   Bevilacqua F., 2016, MOCO 16 P 3 INT S MO, P1
   Bevilacqua F, 2017, ROUTLEDGE COMPANION TO EMBODIED MUSIC INTERACTION, P391
   Bevilacqua F, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00385
   Bevilacqua Frederic., 2007, P 7 INT C NEW INTERF, P124, DOI [10.1145/1279740.1279762, DOI 10.1145/1279740.1279762]
   Boyer EO, 2020, EXP BRAIN RES, V238, P1011, DOI 10.1007/s00221-020-05770-6
   Braun DA, 2009, CURR BIOL, V19, P352, DOI 10.1016/j.cub.2009.01.036
   Bresin R, 2020, INT J HUM-COMPUT ST, V144, DOI 10.1016/j.ijhcs.2020.102500
   Camurri A, 2016, UBICOMP'16 ADJUNCT: PROCEEDINGS OF THE 2016 ACM INTERNATIONAL JOINT CONFERENCE ON PERVASIVE AND UBIQUITOUS COMPUTING, P973, DOI 10.1145/2968219.2968261
   Caramiaux B, 2018, PLOS ONE, V13, DOI 10.1371/journal.pone.0193580
   Caramiaux B, 2015, CHI 2015: PROCEEDINGS OF THE 33RD ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P3943, DOI 10.1145/2702123.2702515
   DANIELSSON PE, 1980, COMPUT VISION GRAPH, V14, P227, DOI 10.1016/0146-664X(80)90054-4
   Danna J, 2017, IEEE T HUM-MACH SYST, V47, P299, DOI 10.1109/THMS.2016.2641397
   Danna J, 2015, HUM MOVEMENT SCI, V43, P216, DOI 10.1016/j.humov.2014.12.002
   Danna Jeremy, 2013, P 10 INT S COMPUTER, P200
   Davids K, 2003, SPORTS MED, V33, P245, DOI 10.2165/00007256-200333040-00001
   Dhawale AK, 2017, ANNU REV NEUROSCI, V40, P479, DOI 10.1146/annurev-neuro-072116-031548
   Douglas D.H., 2011, Algorithms for the Reduction of the Number of Points Required to Represent a Digitized Line or its Caricature, P15, DOI DOI 10.1002/9780470669488.CH2
   Dourish P., 2004, ACTION IS FDN EMBODI
   Dubus G, 2015, SPORTS ENG, V18, P29, DOI 10.1007/s12283-014-0164-0
   Dubus G, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0082491
   Dyer JF, 2017, EXP BRAIN RES, V235, P3129, DOI 10.1007/s00221-017-5047-8
   Effenberg AO, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00219
   Effenberg AO, 2005, IEEE MULTIMEDIA, V12, P53, DOI 10.1109/MMUL.2005.31
   El Raheb K, 2019, ACM COMPUT SURV, V52, DOI 10.1145/3323335
   Fitts P. M., 1967, Human performance
   FITTS PM, 1954, J EXP PSYCHOL, V47, P381, DOI 10.1037/h0055392
   Francoise Jules., 2016, P 2016 CHI C EXT ABS, P2829, DOI DOI 10.1145/2851581.2892420
   Franinovic K, 2013, SONIC INTERACTION DESIGN, P39
   Godoy R.I., 2009, Musical Gestures: Sound, Movement, and Meaning
   Godoy Rolf Inge, 2010, MUSICAL GESTURES SOU
   Guerra J, 2020, SCI SPORT, V35, P119, DOI 10.1016/j.scispo.2019.12.004
   HAMMING RW, 1950, BELL SYST TECH J, V29, P147, DOI 10.1002/j.1538-7305.1950.tb00463.x
   Hopper LS, 2018, RES DANC EDUC, V19, P229, DOI 10.1080/14647893.2017.1420156
   Hummel Jessica, 2010, P INTERACTIVE SONIFI, P17
   Kirby R., 2009, Sports Technology, V2, P43, DOI DOI 10.1080/19346182.2009.9648498
   Krakauer JW, 2011, CURR OPIN NEUROBIOL, V21, P636, DOI 10.1016/j.conb.2011.06.012
   Lemaitre G, 2012, IEEE T AFFECT COMPUT, V3, P335, DOI 10.1109/T-AFFC.2012.1
   Leman M, 2016, EXPRESSIVE MOMENT: HOW INTERACTION (WITH MUSIC) SHAPES HUMAN EMPOWERMENT, P1
   Leman M., 2008, Embodied Music Cognition and Mediation Technology, DOI [DOI 10.7551/MITPRESS/7476.001.0001, 0.7551/mitpress/7476.001.0001, 10.7551/mitpress/7476.001.0001]
   LEVENSHT.VI, 1965, DOKL AKAD NAUK SSSR+, V163, P845
   Maes PJ, 2019, J MULTIMODAL USER IN, V13, P155, DOI 10.1007/s12193-018-0279-x
   Magalhaes Michelle Agnes, 2020, CONSTELLA C TIONS
   Magill R.A., 2007, MOTOR LEARNING CONTR, V11
   Michaels C.F., 1981, Direct perception
   Mueller F, 2014, 32ND ANNUAL ACM CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2014), P2191, DOI 10.1145/2556288.2557163
   Nava E, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-020-59979-0
   Neuhoff J.G., 2002, P 2002 INT C AUDITOR
   Newbold JW, 2016, 34TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, CHI 2016, P5698, DOI 10.1145/2858036.2858302
   Newell KM, 1998, MOTOR BEHAVIOR AND HUMAN SKILL, P143
   Niewiadomski R, 2019, J MULTIMODAL USER IN, V13, P191, DOI 10.1007/s12193-018-0284-0
   Nikmaram N, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.01378
   O'Brien B, 2020, EXP BRAIN RES, V238, P883, DOI 10.1007/s00221-020-05757-3
   Ramer U., 1972, Comput. Graph. Image Process., V1, P244, DOI DOI 10.1016/S0146-664X(72)80017-0
   Ripollés P, 2016, BRAIN IMAGING BEHAV, V10, P1289, DOI 10.1007/s11682-015-9498-x
   Rodger MWM, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00272
   Rosati G, 2013, COMPUT INTEL NEUROSC, V2013, DOI 10.1155/2013/586138
   Ruffaldi E, 2009, WORLD HAPTICS 2009: THIRD JOINT EUROHAPTICS CONFERENCE AND SYMPOSIUM ON HAPTIC INTERFACES FOR VIRTUAL ENVIRONMENT AND TELEOPERATOR SYSTEMS, PROCEEDINGS, P350, DOI 10.1109/WHC.2009.4810849
   SAVITZKY A, 1964, ANAL CHEM, V36, P1627, DOI 10.1021/ac60214a047
   Schaffert N, 2019, FRONT PSYCHOL, V10, DOI 10.3389/fpsyg.2019.00244
   Schiavio A, 2015, FRONT NEUROL, V6, DOI 10.3389/fneur.2015.00217
   SCHMIDT RA, 1991, NATO ADV SCI I D-BEH, V62, P59
   SCHMIDT RA, 1975, PSYCHOL REV, V82, P225, DOI 10.1037/h0076770
   Schmidt RA, 2018, Motor Control and Learning: A Behavioral Emphasis
   Schmitz G, 2013, BMC NEUROSCI, V14, DOI 10.1186/1471-2202-14-32
   Seifert L, 2011, HUM MOVEMENT SCI, V30, P550, DOI 10.1016/j.humov.2010.12.003
   Seifert L, 2013, SPORTS MED, V43, P167, DOI 10.1007/s40279-012-0011-z
   Sigrist R, 2013, PSYCHON B REV, V20, P21, DOI 10.3758/s13423-012-0333-8
   Spilka MJ, 2010, EXP BRAIN RES, V204, P549, DOI 10.1007/s00221-010-2322-3
   Stergiou N, 2011, HUM MOVEMENT SCI, V30, P869, DOI 10.1016/j.humov.2011.06.002
   Sülzenbrück S, 2011, ERGONOMICS, V54, P34, DOI 10.1080/00140139.2010.535023
   Tajadura-Jiménez A, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-018-23229-1
   Tajadura-Jiménez A, 2015, IEEE MULTIMEDIA, V22, P48, DOI 10.1109/MMUL.2015.14
   Tanaka Atau., 2019, New Directions in Music and Human-Computer Interaction, P135, DOI DOI 10.1007/978-3-319-92069-6_9
   Thaut M.H., 2014, Handbook of neurologic music therapy
   Trenado C, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2018.00850
   Winkler William E., 1990, P SURV RES METH SECT
   Wolpert DM, 2011, NAT REV NEUROSCI, V12, P739, DOI 10.1038/nrn3112
   Wu HG, 2014, NAT NEUROSCI, V17, P312, DOI 10.1038/nn.3616
   Wulf G, 2002, PSYCHON B REV, V9, P185, DOI 10.3758/BF03196276
   Wulf G, 2013, INT REV SPORT EXER P, V6, P77, DOI 10.1080/1750984X.2012.723728
   Yamamoto Goh, 2004, P WORKSHOP MOBILE HC
NR 86
TC 4
Z9 4
U1 1
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD JAN
PY 2022
VL 19
IS 1
AR 2
DI 10.1145/3482967
PG 21
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 9R2WY
UT WOS:000945516600002
OA Green Published
DA 2024-07-18
ER

PT J
AU Parikh, SS
   Kalva, H
AF Parikh, Saurin S.
   Kalva, Hari
TI Feature Weighted Linguistics Classifier for Predicting Learning
   Difficulty Using Eye Tracking
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Predicting learning difficulty; eye response analysis; pupillary
   response analysis; e-learning; predicting levels of learning; eye
   tracking
ID MOVEMENTS; COMPREHENSION; GAZE
AB This article presents a new approach to predict learning difficulty in applications such as e-learning using eye movement and pupil response. We have developed 12 eye response features based on psycholinguistics, contextual information processing, anticipatory behavior analysis, recurrence fixation analysis, and pupillary response. A key aspect of the proposed approach is the temporal analysis of the feature response to the same concept. Results show that variations in eye response to the same concept over time are indicative of learning difficulty. A Feature Weighted Linguistics Classifier (FWLC) was developed to predict learning difficulty in real time. The proposed approach predicts learning difficulty with an accuracy of 90%.
C1 [Parikh, Saurin S.] Nirma Univ, Ahmadabad 382481, Gujarat, India.
   [Parikh, Saurin S.; Kalva, Hari] Florida Atlantic Univ, Boca Raton, FL 33431 USA.
C3 Nirma University; State University System of Florida; Florida Atlantic
   University
RP Parikh, SS (corresponding author), Nirma Univ, Ahmadabad 382481, Gujarat, India.; Parikh, SS (corresponding author), Florida Atlantic Univ, Boca Raton, FL 33431 USA.
EM saurin.parikh@nirmauni.ac.in; hari.kalva@fau.edu
RI Parikh, Saurin/ABE-3161-2021
OI Kalva, Hari/0000-0002-7165-5499; Parikh, Saurin/0000-0003-2488-1415
CR Anderson NC, 2013, BEHAV RES METHODS, V45, P842, DOI 10.3758/s13428-012-0299-5
   [Anonymous], 2012, P COMPSYSTECH 12 13, DOI DOI 10.1145/2383276.2383331
   Barrios V.M. G., 2004, Proceedings of IKNOW, (IICM), P609
   Calvi C, 2008, 8TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED LEARNING TECHNOLOGIES, PROCEEDINGS, P376, DOI 10.1109/ICALT.2008.35
   Chaffin R, 2001, J EXP PSYCHOL LEARN, V27, P225, DOI 10.1037//0278-7393.27.1.225
   Conati C, 2007, KNOWL-BASED SYST, V20, P557, DOI 10.1016/j.knosys.2007.04.010
   Copeland L., 2016, IEEE T EMERG TOP COM, V99, P1
   Copeland L., 2013, P 25 AUSTR COMP HUM, P285
   Copeland L, 2015, INT CONF COGN INFO, P407, DOI 10.1109/CogInfoCom.2015.7390628
   Copeland L, 2013, INT CONF COGN INFO, P791, DOI 10.1109/CogInfoCom.2013.6719207
   Davies, 2008, The Corpus of Contemporary American English (COCA): 520 million words, 1990-present
   Dobesova Z, 2015, 2015 13TH INTERNATIONAL CONFERENCE ON EMERGING ELEARNING TECHNOLOGIES AND APPLICATIONS (ICETA), P59
   FRAZIER L, 1982, COGNITIVE PSYCHOL, V14, P178, DOI 10.1016/0010-0285(82)90008-1
   Gwizdka J, 2017, J ASSOC INF SCI TECH, V68, P2299, DOI 10.1002/asi.23904
   Hua Wang, 2006, Proceedings. ETRA 2006. Symposium on Eye Tracking Research and Applications, P73, DOI 10.1145/1117309.1117346
   Huettig F, 2015, DYSLEXIA, V21, P97, DOI 10.1002/dys.1497
   Husain Samar, 2015, Journal of Eye Movement Research, V8, DOI DOI 10.16910/JEMR.8.2.3
   Parikh S, 2018, UMAP'18: ADJUNCT PUBLICATION OF THE 26TH CONFERENCE ON USER MODELING, ADAPTATION AND PERSONALIZATION, P131, DOI 10.1145/3213586.3226224
   Porta M., 2012, Global Engineering Education Conference (EDUCON), 2012 IEEE, P1, DOI DOI 10.1109/EDUCON.2012.6201145
   Rayner K, 1996, J EXP PSYCHOL HUMAN, V22, P1188, DOI 10.1037/0096-1523.22.5.1188
   RAYNER K, 1986, MEM COGNITION, V14, P191, DOI 10.3758/BF03197692
   RAYNER K, 1976, VISION RES, V16, P829, DOI 10.1016/0042-6989(76)90143-7
   Rayner K, 2006, SCI STUD READ, V10, P241, DOI 10.1207/s1532799xssr1003_3
   Rayner K, 2009, Q J EXP PSYCHOL, V62, P1457, DOI 10.1080/17470210902816461
   Rello L, 2017, UNIVERSAL ACCESS INF, V16, P29, DOI 10.1007/s10209-015-0438-8
   Sibert J. L., 2000, UIST. Proceedings of the 13th Annual ACM Symposium on User Interface Software and Technology, P101, DOI 10.1145/354401.354418
   Sibley C., 2011, Proceedings of the Human Factors and Ergonomics Society Annual Meeting, V55, P237, DOI DOI 10.1177/1071181311551049
   Siegle GJ, 2003, NEUROIMAGE, V20, P114, DOI 10.1016/S1053-8119(03)00298-2
   Smith R, 2007, PROC INT CONF DOC, P629, DOI 10.1109/icdar.2007.4376991
   Sungkur RK, 2016, EDUC INF TECHNOL, V21, P1785, DOI 10.1007/s10639-015-9418-0
   Vo T, 2010, LECT NOTES COMPUT SC, V6444, P124, DOI 10.1007/978-3-642-17534-3_16
   Traxler MJ, 2014, TRENDS COGN SCI, V18, P605, DOI 10.1016/j.tics.2014.08.001
   Vagge A, 2015, ANN DYSLEXIA, V65, P24, DOI 10.1007/s11881-015-0098-7
   Vosskühler A, 2008, BEHAV RES METHODS, V40, P1150, DOI [10.3758/BRM.40.4.1150, 10.3758/BRM.40.4.U50]
   Williams RS, 2004, EUR J COGN PSYCHOL, V16, P312, DOI 10.1080/09541440340000196
   Zhan ZH, 2016, SENSORS-BASEL, V16, DOI 10.3390/s16091457
NR 36
TC 1
Z9 1
U1 1
U2 9
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD MAY
PY 2020
VL 17
IS 2
AR 5
DI 10.1145/3380877
PG 25
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA OH5JF
UT WOS:000582618500001
DA 2024-07-18
ER

PT J
AU Hadnett-Hunter, J
   Nicolaou, G
   O'Neill, E
   Proulx, MJ
AF Hadnett-Hunter, Jacob
   Nicolaou, George
   O'Neill, Eamonn
   Proulx, Michael J.
TI The Effect of Task on Visual Attention in Interactive Virtual
   Environments
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article; Proceedings Paper
CT 16th Symposium on Applied Perception (SAP)
CY SEP, 2019
CL Barcelona, SPAIN
DE Saliency; visual attention; virtual environments
ID EYE-MOVEMENTS; SACCADE AMPLITUDE; SEARCH; HEAD; OBJECTS; GAZE;
   ALLOCATION; GUIDANCE
AB Virtual environments for gaming and simulation provide dynamic and adaptive experiences, but, despite advances in multisensory interfaces, these are still primarily visual experiences. To support real-time dynamic adaptation, interactive virtual environments could implement techniques to predict and manipulate human visual attention. One promising way of developing such techniques is to base them on psychophysical observations, an approach that requires a sound understanding of visual attention allocation. Understanding how this allocation of visual attention changes depending on a user's task offers clear benefits in developing these techniques and improving virtual environment design. With this aim, we investigated the effect of task on visual attention in interactive virtual environments. We recorded fixation data from participants completing freeview, search, and navigation tasks in three different virtual environments. We quantified visual attention differences between conditions by identifying the predictiveness of a low-level saliency model and its corresponding color, intensity, and orientation feature-conspicuity maps, as well as measuring fixation center bias, depth, duration, and saccade amplitude. Our results show that task does affect visual attention in virtual environments. Navigation relies more than search or freeview on intensity conspicuity to allocate visual attention. Navigation also produces fixations that are more central, longer, and deeper into the scenes. Further, our results suggest that it is difficult to distinguish between freeview and search tasks. These results provide important guidance for designing virtual environments for human interaction, as well as identifying future avenues of research for developing "attention-aware" virtual worlds.
C1 [Hadnett-Hunter, Jacob; Nicolaou, George; O'Neill, Eamonn; Proulx, Michael J.] Univ Bath, Bath BA2 7AY, Somerset, England.
C3 University of Bath
RP Hadnett-Hunter, J (corresponding author), Univ Bath, Bath BA2 7AY, Somerset, England.
EM J.M.Elliott.Hadnett-Hunter@bath.ac.uk; E.ONeill@bath.ac.uk;
   M.J.Proulx@bath.ac.uk
RI Proulx, Michael J/A-1045-2008
OI Proulx, Michael/0000-0003-4066-3645
FU CAMERA; RCUK Centre for the Analysis of Motion, Entertainment Research
   and Applications [EP/M023281/1]; EPSRC [1789379, EP/M023281/1] Funding
   Source: UKRI
FX Eamonn O'Neill's research is partly funded by CAMERA, the RCUK Centre
   for the Analysis of Motion, Entertainment Research and Applications,
   EP/M023281/1.
CR [Anonymous], 2012, P 8 AUSTR C INT ENT
   [Anonymous], 1987, Shifts in selective visual attention: Towards the underlying neural circuitry. matters of intelligence
   [Anonymous], 2018, IEEE T PATTERN ANAL
   [Anonymous], 2012, Technical Report
   BALOH RW, 1975, NEUROLOGY, V25, P1065, DOI 10.1212/WNL.25.11.1065
   Boisvert JFG, 2016, NEUROCOMPUTING, V207, P653, DOI 10.1016/j.neucom.2016.05.047
   Borji A, 2014, J VISION, V14, DOI 10.1167/14.3.29
   Bylinskii Z., 2015, MIT saliency benchmark
   Castelhano MS, 2009, J VISION, V9, DOI 10.1167/9.3.6
   Collins CJS, 1999, J PHYSIOL-LONDON, V515, P299, DOI 10.1111/j.1469-7793.1999.299ad.x
   Cornia M, 2016, INT C PATT RECOG, P3488, DOI 10.1109/ICPR.2016.7900174
   Coutrot A, 2018, BEHAV RES METHODS, V50, P362, DOI 10.3758/s13428-017-0876-8
   Deubel H, 1996, VISION RES, V36, P1827, DOI 10.1016/0042-6989(95)00294-4
   El-Nasr MS, 2009, IEEE T COMP INTEL AI, V1, P145, DOI 10.1109/TCIAIG.2009.2024532
   Franchak JM, 2010, VISION RES, V50, P2766, DOI 10.1016/j.visres.2010.09.024
   Freedman EG, 2008, EXP BRAIN RES, V190, P369, DOI 10.1007/s00221-008-1504-8
   Gao D, 2008, J VISION, V8, DOI 10.1167/8.7.13
   Gramann K, 2009, INT J NEUROSCI, V119, P1755, DOI 10.1080/00207450903170361
   Haji-Abolhassani A, 2014, VISION RES, V103, P127, DOI 10.1016/j.visres.2014.08.014
   Hajimirza SN, 2012, IEEE T MULTIMEDIA, V14, P805, DOI 10.1109/TMM.2012.2186792
   Harel J, 2006, Advances in Neural Information Processing Systems, V19
   Hayhoe M, 2005, TRENDS COGN SCI, V9, P188, DOI 10.1016/j.tics.2005.02.009
   Henderson John M., 2007, P537, DOI 10.1016/B978-008044980-7/50027-6
   Henderson JM, 2017, NAT HUM BEHAV, V1, P743, DOI 10.1038/s41562-017-0208-0
   Henderson JM, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0064937
   Hillaire S., 2009, P 16 ACM S VIRT REAL, DOI [10.1145/1643928.1643941, DOI 10.1145/1643928.1643941]
   Hillaire S, 2012, IEEE T VIS COMPUT GR, V18, P356, DOI 10.1109/TVCG.2011.154
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Itti L, 2002, PROC SPIE, V4662, P235, DOI 10.1117/12.469519
   Jiang YHV, 2015, ATTEN PERCEPT PSYCHO, V77, P50, DOI 10.3758/s13414-014-0747-7
   Khan AZ, 2009, J NEUROPHYSIOL, V101, P198, DOI 10.1152/jn.90815.2008
   Kruthiventi SSS, 2017, IEEE T IMAGE PROCESS, V26, P4446, DOI 10.1109/TIP.2017.2710620
   Land MF, 2001, VISION RES, V41, P3559, DOI 10.1016/S0042-6989(01)00102-X
   Le Meur O, 2007, VISION RES, V47, P2483, DOI 10.1016/j.visres.2007.06.015
   Lee S, 2009, IEEE T VIS COMPUT GR, V15, P6, DOI 10.1109/TVCG.2008.82
   MCLEOD P, 1988, NATURE, V332, P154, DOI 10.1038/332154a0
   Mills M, 2011, J VISION, V11, DOI 10.1167/11.8.17
   Nacke LE, 2011, 29TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P103
   Olsen A., 2012, TOBII TECHNOL, V21, P4
   OToole AJ, 1997, PERCEPT PSYCHOPHYS, V59, P202, DOI 10.3758/BF03211889
   Parkhurst D, 2002, VISION RES, V42, P107, DOI 10.1016/S0042-6989(01)00250-4
   Perona, 2006, SALIENCY IMPLEMENTAT
   Peters RJ, 2005, VISION RES, V45, P2397, DOI 10.1016/j.visres.2005.03.019
   Peters RJ, 2008, ACM T APPL PERCEPT, V5, DOI 10.1145/1279920.1279923
   Proulx MJ, 2007, J EXP PSYCHOL HUMAN, V33, P48, DOI 10.1037/0096-1523.33.1.48
   Proulx MJ, 2011, J VISION, V11, DOI 10.1167/11.13.21
   Rothkopf CA, 2007, J VISION, V7, DOI 10.1167/7.14.16
   Smith TJ, 2012, CURR DIR PSYCHOL SCI, V21, P107, DOI 10.1177/0963721412437407
   Spivey MJ, 2001, PSYCHOL SCI, V12, P282, DOI 10.1111/1467-9280.00352
   Swafford N. T., 2016, P ACM S APPL PERC SA, P7, DOI DOI 10.1145/2931002.2931011
   Tatler BW, 2006, VISION RES, V46, P1857, DOI 10.1016/j.visres.2005.12.005
   Tatler BW, 2011, J VISION, V11, DOI 10.1167/11.5.5
   TREISMAN AM, 1980, COGNITIVE PSYCHOL, V12, P97, DOI 10.1016/0010-0285(80)90005-5
   Tseng PH, 2009, J VISION, V9, DOI 10.1167/9.7.4
   Vickery TJ, 2005, J VISION, V5, P81, DOI 10.1167/5.1.8
   Weier M, 2016, COMPUT GRAPH FORUM, V35, P289, DOI 10.1111/cgf.13026
   Wolfe J. M., 2007, INTEGRATED MODELS CO, P99, DOI [10.1093/acprof:oso/9780195189193.003.0008, DOI 10.1093/ACPROF:OSO/9780195189193.003.0008]
   Wolfe JM, 2017, NAT HUM BEHAV, V1, DOI 10.1038/s41562-017-0058
   Wolfe JM, 2011, ATTEN PERCEPT PSYCHO, V73, P1650, DOI 10.3758/s13414-011-0153-3
   Wolfe JM, 2004, NAT REV NEUROSCI, V5, P495, DOI 10.1038/nrn1411
   Wolfe JM, 2002, VISION RES, V42, P2985, DOI 10.1016/S0042-6989(02)00388-7
   Yarbus A.L., 1967, EYE MOVEMENTS VISION, DOI [10.1007/978-1-4899-5379-7, DOI 10.1007/978-1-4899-5379-7]
   Zhao Q, 2011, J VISION, V11, DOI 10.1167/11.3.9
NR 63
TC 18
Z9 19
U1 3
U2 21
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD SEP
PY 2019
VL 16
IS 3
SI SI
AR 17
DI 10.1145/3352763
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA JA3KH
UT WOS:000487720000005
OA Green Published
DA 2024-07-18
ER

PT J
AU Kelly, JW
   Klesel, BC
   Cherep, LA
AF Kelly, Jonathan W.
   Klesel, Brenna C.
   Cherep, Lucia A.
TI Visual Stabilization of Balance in Virtual Reality Using the HTC Vive
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Balance; posture; stereoscopic displays; virtual environments
ID POSTURAL RESPONSES; MOTION SICKNESS; SWAY; MAINTENANCE
AB Vision in real environments stabilizes balance compared to an eyes-closed condition. For virtual reality to be safe and fully effective in applications such as physical rehabilitation, vision in virtual reality should stabilize balance as much as vision in the real world. Older virtual reality technology was previously found to stabilize balance but by less than half as much as real-world vision. Recent advancements in display technology might allow for vision in virtual reality to be as stabilizing as vision in the real world. This study evaluated whether viewing a virtual environment through the HTC Vive-a new consumer-grade head-mounted display-stabilizes balance, and whether visual stabilization is similar to that provided by real-world vision. Participants viewed the real laboratory or a virtual replica of the laboratory and attempted to maintain an unstable stance with eyes open or closed while standing at one of two viewing distances. Vision was significantly stabilizing in all conditions, but the virtual environment provided less visual stabilization than did the real environment. Regardless of the environment, near viewing led to greater visual stabilization than did far viewing. The smaller stabilizing influence of viewing a virtual compared to real environment might lead to greater risk of falls in virtual reality and smaller gains in physical rehabilitation using virtual reality.
C1 [Kelly, Jonathan W.; Klesel, Brenna C.; Cherep, Lucia A.] Iowa State Univ, Dept Psychol, W112 Lagomarcino Hall,901 Stange Rd, Ames, IA 50011 USA.
C3 Iowa State University
RP Kelly, JW (corresponding author), Iowa State Univ, Dept Psychol, W112 Lagomarcino Hall,901 Stange Rd, Ames, IA 50011 USA.
EM jonkelly@iastate.edu; bklesel@iastate.edu; lacherep@iastate.edu
RI Kelly, Jonathan/A-4793-2013
OI Kelly, Jonathan/0000-0002-4317-273X
FU Seed Grant for Social Sciences from the Iowa State University College of
   Liberal Arts and Sciences
FX This research was supported by a Seed Grant for Social Sciences from the
   Iowa State University College of Liberal Arts and Sciences.
CR BEGBIE G. H., 1967, CIBA FOUND SYMP, P80
   Bryanton C, 2006, CYBERPSYCHOL BEHAV, V9, P123, DOI 10.1089/cpb.2006.9.123
   Buck LE, 2018, ACM T APPL PERCEPT, V15, DOI 10.1145/3196885
   Carlson P, 2015, IEEE T VIS COMPUT GR, V21, P770, DOI 10.1109/TVCG.2015.2393871
   Creem-Regehr SH, 2016, SAP 2015: ACM SIGGRAPH SYMPOSIUM ON APPLIED PERCEPTION, P47, DOI 10.1145/2804408.2804422
   Cunningham DW, 2006, P IEEE VIRT REAL ANN, P111, DOI 10.1109/VR.2006.14
   David S, 2014, PROCEEDINGS OF INTERNATIONAL CONFERENCE INFORMATION SYSTEMS AND DESIGN OF COMMUNICATION (ISDOC2014), P1, DOI 10.1145/2618168.2618169
   DIENER HC, 1984, ELECTROEN CLIN NEURO, V57, P134, DOI 10.1016/0013-4694(84)90172-X
   DIJKSTRA TMH, 1992, HUM MOVEMENT SCI, V11, P195, DOI 10.1016/0167-9457(92)90060-O
   EDWARDS AS, 1946, J EXP PSYCHOL, V36, P526, DOI 10.1037/h0059909
   Gavish N, 2015, INTERACT LEARN ENVIR, V23, P778, DOI 10.1080/10494820.2013.815221
   Howard MC, 2017, COMPUT HUM BEHAV, V70, P317, DOI 10.1016/j.chb.2017.01.013
   Kelly JW, 2008, PERCEPT PSYCHOPHYS, V70, P158, DOI 10.3758/PP.70.1.158
   Kelly JW, 2017, ACM T APPL PERCEPT, V15, DOI 10.1145/3106155
   Kelly JW, 2005, EXP BRAIN RES, V161, P285, DOI 10.1007/s00221-004-2069-9
   Kuhl SA, 2008, ACM T APPL PERCEPT, V5, DOI 10.1145/1402236.1402241
   Li Bochao., 2014, P ACM S APPL PERCEPT, P91
   Lihavainen K, 2010, J GERONTOL A-BIOL, V65, P990, DOI 10.1093/gerona/glq052
   Lin DD, 2008, GAIT POSTURE, V28, P337, DOI 10.1016/j.gaitpost.2008.01.005
   Merhi O, 2007, HUM FACTORS, V49, P920, DOI 10.1518/001872007X230262
   Mohler BJ, 2007, ACM T APPL PERCEPT, V4, DOI [10.1145/1227134.1227138, 10.1145/1227134/1227138]
   PAULUS WM, 1984, BRAIN, V107, P1143, DOI 10.1093/brain/107.4.1143
   Piromchai P, 2015, COCHRANE DB SYST REV, DOI 10.1002/14651858.CD010198.pub2
   Prieto TE, 1996, IEEE T BIO-MED ENG, V43, P956, DOI 10.1109/10.532130
   Rebenitsch L, 2016, VIRTUAL REAL-LONDON, V20, P101, DOI 10.1007/s10055-016-0285-9
   Renner RS, 2013, ACM COMPUT SURV, V46, DOI 10.1145/2543581.2543590
   Smart LJ, 2002, HUM FACTORS, V44, P451, DOI 10.1518/0018720024497745
   Stoffregen TA, 2004, PRESENCE-TELEOP VIRT, V13, P601, DOI 10.1162/1054746042545274
   VANPARYS JAP, 1976, AGRESSOLOGIE, V17, P95
   WITKIN HA, 1950, AM J PSYCHOL, V63, P31, DOI 10.2307/1418418
NR 30
TC 9
Z9 10
U1 1
U2 15
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD AUG
PY 2019
VL 16
IS 2
AR 8
DI 10.1145/3313902
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA JC8UL
UT WOS:000489551500002
OA Bronze, Green Published
DA 2024-07-18
ER

PT J
AU Zannoli, M
   Banks, MS
AF Zannoli, Marina
   Banks, Martin S.
TI The Perceptual Consequences of Curved Screens
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Television screen; field of view; reflections; perceptual distortions;
   focal length
ID GLARE; SIZE
AB Flat panels are by far the most common type of television screen. There are reasons, however, to believe that curved screens create a greater sense of immersion, reduce distracting reflections, and minimize some perceptual distortions that are commonplace with large televisions. To examine these possibilities, we calculated how curving the screen affects the field of view and the probability of seeing reflections of ambient lights. We find that screen curvature has a small beneficial effect on field of view and a large beneficial effect on the probability of seeing reflections. We also collected behavioral data to characterize perceptual distortions in various viewing configurations. We find that curved screens can in fact reduce problematic perceptual distortions on large screens, but that the benefit depends on the geometry of the projection on such screens.
C1 [Zannoli, Marina] Oculus VR, 8747 148th Ave NE, Redmond, WA 98052 USA.
   [Banks, Martin S.] Univ Calif Berkeley, Sch Optometry, Berkeley, CA 94720 USA.
C3 University of California System; University of California Berkeley
RP Zannoli, M (corresponding author), Oculus VR, 8747 148th Ave NE, Redmond, WA 98052 USA.
EM marina.zannoli@oculus.com; martybanks@berkeley.edu
FU Samsung Display
FX This work was supported by Samsung Display.
CR Agrawala M, 2000, SPRING COMP SCI, P125
   [Anonymous], 2012, BT2022 ITUR
   Archer John, 2016, CURVED TVS PROS CONS
   Cooper EA, 2012, J VISION, V12, DOI 10.1167/12.5.8
   Edgerton G., 2010, The Columbia history of American television
   Hands P, 2015, J VISION, V15, DOI 10.1167/15.5.6
   HATADA T, 1980, SMPTE J, V89, P560, DOI 10.5594/J01582
   Holladay LL, 1926, J OPT SOC AM REV SCI, V12, P271, DOI 10.1364/JOSA.12.000271
   ITU-R BT, BT22461 ITUR
   Katzmaier David, 2014, TROUBLE CURVE WHAT Y
   Kingslake R, 1963, LENSES PHOTOGRAPHY P
   Lombard M, 2000, HUM COMMUN RES, V26, P75, DOI 10.1111/j.1468-2958.2000.tb00750.x
   London B., 2010, Photography, V10th
   PAULSSON LE, 1980, INVEST OPHTH VIS SCI, V19, P401
   Pirenne M. H., 1970, OPTICS PAINTING PHOT
   REGAN D, 1992, VISION RES, V32, P1845, DOI 10.1016/0042-6989(92)90046-L
   ROSINSKI RR, 1980, PERCEPT PSYCHOPHYS, V28, P521, DOI 10.3758/BF03198820
   Stiles WS, 1929, P R SOC LOND B-CONTA, V105, P131, DOI 10.1098/rspb.1929.0033
   Troscianko T, 2012, I-PERCEPTION, V3, P414, DOI 10.1068/i0475aap
   Vishwanath D, 2005, NAT NEUROSCI, V8, P1401, DOI 10.1038/nn1553
   WALLACH H, 1986, PERCEPT PSYCHOPHYS, V39, P233, DOI 10.3758/BF03204929
   Zorin D., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P257, DOI 10.1145/218380.218449
NR 22
TC 6
Z9 8
U1 1
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD NOV
PY 2017
VL 15
IS 1
AR 6
DI 10.1145/3106012
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FU0CU
UT WOS:000423519800006
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Alexanderson, S
   O'Sullivan, C
   Neff, M
   Beskow, J
AF Alexanderson, Simon
   O'Sullivan, Carol
   Neff, Michael
   Beskow, Jonas
TI Mimebot-Investigating the Expressibility of Non-Verbal Communication
   Across Agent Embodiments
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Motion capture; perception; cross-mapping
ID EXPRESSIONS; PERCEPTION; MODEL
AB Unlike their human counterparts, artificial agents such as robots and game characters may be deployed with a large variety of face and body configurations. Some have articulated bodies but lack facial features, and others may be talking heads ending at the neck. Generally, they have many fewer degrees of freedom than humans through which they must express themselves, and there will inevitably be a filtering effect when mapping human motion onto the agent. In this article, we investigate filtering effects on three types of embodiments: (a) an agent with a body but no facial features, (b) an agent with a head only, and (c) an agent with a body and a face. We performed a full performance capture of a mime actor enacting short interactions varying the non-verbal expression along five dimensions (e.g., level of frustration and level of certainty) for each of the three embodiments. We performed a crowd-sourced evaluation experiment comparing the video of the actor to the video of an animated robot for the different embodiments and dimensions. Our findings suggest that the face is especially important to pinpoint emotional reactions but is also most volatile to filtering effects. The body motion, on the other hand, had more diverse interpretations but tended to preserve the interpretation after mapping and thus proved to be more resilient to filtering.
C1 [Alexanderson, Simon; Beskow, Jonas] KTH Speech Mus & Hearing, Lindstedtsv 24, Stockholm, Sweden.
   [O'Sullivan, Carol] Trinity Coll Dublin, Graph Vis & Visualisat Grp, Dublin 2, Ireland.
   [Neff, Michael] Univ Calif Davis, Dept Comp Sci, Davis, CA 95616 USA.
C3 Royal Institute of Technology; Trinity College Dublin; University of
   California System; University of California Davis
RP Alexanderson, S (corresponding author), KTH Speech Mus & Hearing, Lindstedtsv 24, Stockholm, Sweden.
EM simonal@kth.se; Carol.OSullivan@scss.tcd.ie; mpneff@ucdavis.edu;
   beskow@kth.se
OI Alexanderson, Simon/0000-0002-7801-7617; O'Sullivan,
   Carol/0000-0003-3772-4961
FU Science foundation Ireland PI grant [S.F.10/IN.1/13003]; KTH/SRA ICT The
   Next Generation
FX This work was funded by KTH/SRA ICT The Next Generation and Science
   foundation Ireland PI grant S.F.10/IN.1/13003.
CR Alexanderson S., 2016, Proceedings of the 9th International Conference on Motion in Games, MIG '16, P7
   [Anonymous], 2011, VRIPHYS
   [Anonymous], 2013, P 10 IEEE INT C WORK
   [Anonymous], 2009, MOVING BODY CORPS PO
   Atkinson AP, 2004, PERCEPTION, V33, P717, DOI 10.1068/p5096
   BARBA E., 1991, DICT THEATRE ANTHR S
   Beck A, 2012, ACM T INTERACT INTEL, V2, DOI 10.1145/2133366.2133368
   Bickmore T. W., 2005, ACM Transactions on Computer-Human Interaction, V12, P293, DOI 10.1145/1067860.1067867
   de Melo CelsoM., 2011, 10 INT C AUTONOMOUS, V3, P937
   Dorcy Jean, 1961, THE MIME
   EKMAN P, 1992, COGNITION EMOTION, V6, P169, DOI 10.1080/02699939208411068
   Feng WW, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360690
   Gielniak MJ, 2012, ACMIEEE INT CONF HUM, P375
   Haring M., 2011, 2011 RO-MAN: The 20th IEEE International Symposium on Robot and Human Interactive Communication, P204, DOI 10.1109/ROMAN.2011.6005263
   JOHANSSON G, 1973, PERCEPT PSYCHOPHYS, V14, P201, DOI 10.3758/BF03212378
   Lawson Joan., 1957, MIME THEORY PRACTICE
   MACDORMAN K. F., 2009, COMPUT HUM BEHAV, V25, P3
   Mathur MB, 2016, COGNITION, V146, P22, DOI 10.1016/j.cognition.2015.09.008
   McDonnell R, 2009, ACM T APPL PERCEPT, V6, DOI 10.1145/1577755.1577757
   McDonnell R, 2008, APGV 2008: PROCEEDINGS OF THE SYMPOSIUM ON APPLIED PERCEPTION IN GRAPHICS AND VISUALIZATION, P67
   Meeren HKM, 2005, P NATL ACAD SCI USA, V102, P16518, DOI 10.1073/pnas.0507650102
   Oertel C, 2016, ICMI'16: PROCEEDINGS OF THE 18TH ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P21, DOI 10.1145/2993148.2993188
   OSIPA J., 2010, Stop Staring: Facial Modeling and Animation Done Right, V3rd
   Posner J, 2005, DEV PSYCHOPATHOL, V17, P715, DOI 10.1017/S0954579405050340
   Ribeiro T, 2012, ACMIEEE INT CONF HUM, P383
   Song SC, 2017, ACMIEEE INT CONF HUM, P2, DOI 10.1145/2909824.3020239
   Swerts M, 2005, J MEM LANG, V53, P81, DOI 10.1016/j.jml.2005.02.003
   Takayama L, 2011, ACMIEEE INT CONF HUM, P69, DOI 10.1145/1957656.1957674
   Tausczik YR, 2010, J LANG SOC PSYCHOL, V29, P24, DOI 10.1177/0261927X09351676
   Thomas Frank, 1995, The illusion of life: Disney animation, V1st
   Wang YQ, 2015, SOC INFLUENCE, V10, P236, DOI 10.1080/15534510.2015.1081856
NR 31
TC 5
Z9 6
U1 0
U2 5
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD SEP
PY 2017
VL 14
IS 4
SI SI
AR 24
DI 10.1145/3127590
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA FM9EI
UT WOS:000415407300003
OA Green Published
DA 2024-07-18
ER

PT J
AU Balas, B
   Conlin, C
   Shipman, D
AF Balas, Benjamin
   Conlin, Catherine
   Shipman, Dylan
TI Summary Statistics and Material Categorization in the Visual Periphery
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Material categorization; texture synthesis; peripheral vision; natural
   images
ID TEXTURE SYNTHESIS; IMAGE STATISTICS; PERCEPTION; MODELS; GLOSS
AB Material categorization from natural texture images proceeds quickly and accurately, supporting a number of visual and motor behaviors. In real-world settings, mechanisms for material categorization must function effectively based on the input from foveal vision, where image representation is high fidelity, and the input from peripheral vision, which is comparatively impoverished. What features support successful material categorization in the visual periphery, given the known reductions in acuity, contrast sensitivity, and other lossy transforms that reduce the fidelity of image representations? In general, the visual features that support material categorization remain largely unknown, but recent work suggests that observers' abilities in a number of tasks that depend on peripheral vision can be accounted for by assuming that the visual system has access to only summary statistics (texture-like descriptors) of image structure. We therefore hypothesized that a model of peripheral vision based on the Portilla-Simoncelli texture synthesis algorithm might account for material categorization abilities in the visual periphery. Using natural texture images and synthetic images made from these stimuli, we compared performance across material categories to determine whether observer performance with natural inputs could be predicted by their performance with synthetic images that reflect the constraints of a texture code.
C1 [Balas, Benjamin; Conlin, Catherine; Shipman, Dylan] North Dakota State Univ, 1210 Albrecht Blvd, Fargo, ND 58102 USA.
C3 North Dakota State University Fargo
RP Balas, B (corresponding author), North Dakota State Univ, 1210 Albrecht Blvd, Fargo, ND 58102 USA.
EM Benjamin.balas@ndsu.edu; catconlin@gmail.com; dylan.shipman@ndsu.edu
RI Balas, Ben/R-4003-2019
FU NIH [EY-024375-01]
FX This work is supported by NIH grant no. EY-024375-01 awarded to B.
   Balas.
CR Anderson BL, 2009, J VISION, V9, DOI 10.1167/9.11.10
   Anstis S, 1998, PERCEPTION, V27, P817, DOI 10.1068/p270817
   Balas B.J., 2010, J VISION, V10, P28
   Balas B, 2008, PATTERN RECOGN, V41, P972, DOI 10.1016/j.patcog.2007.08.007
   Balas B, 2015, VISION RES, V115, P271, DOI 10.1016/j.visres.2015.01.022
   Balas B, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0136471
   Balas B, 2009, J VISION, V9, DOI 10.1167/9.12.13
   Balas BJ, 2006, VISION RES, V46, P299, DOI 10.1016/j.visres.2005.04.013
   Brainard DH, 1997, SPATIAL VISION, V10, P433, DOI 10.1163/156856897X00357
   Briand T, 2014, IMAGE PROCESS ON LIN, V4, P276, DOI 10.5201/ipol.2014.79
   Buckingham G, 2009, J NEUROPHYSIOL, V102, P3111, DOI 10.1152/jn.00515.2009
   Fleming RW, 2014, VISION RES, V94, P62, DOI 10.1016/j.visres.2013.11.004
   Freeman J, 2011, NAT NEUROSCI, V14, P1195, DOI 10.1038/nn.2889
   Gatys LA, 2015, ADV NEUR IN, V28
   Hiramatsu C, 2011, NEUROIMAGE, V57, P482, DOI 10.1016/j.neuroimage.2011.04.056
   Kleiner M, 2007, PERCEPTION, V36, P14
   Maloney LT, 2010, J VISION, V10, DOI 10.1167/10.9.19
   Motoyoshi I, 2007, NATURE, V447, P206, DOI 10.1038/nature05724
   Pelli DG, 2008, NAT NEUROSCI, V11, P1129, DOI 10.1038/nn.2187
   Pelli DG, 1997, SPATIAL VISION, V10, P437, DOI 10.1163/156856897X00366
   Portilla J, 2000, INT J COMPUT VISION, V40, P49, DOI 10.1023/A:1026553619983
   Rosenholtz R, 2012, J VISION, V12, DOI 10.1167/12.4.14
   Rosenholtz Ruth, 2016, ANN REV VIS IN PRESS, V2
   Sharan L, 2008, J OPT SOC AM A, V25, P846, DOI 10.1364/JOSAA.25.000846
   Sharan Lavanya, 2013, Int J Comput Vis, V103, P348
   Sharan L, 2014, J VISION, V14, DOI 10.1167/14.9.12
   Tartavel G, 2015, J MATH IMAGING VIS, V52, P124, DOI 10.1007/s10851-014-0547-7
   Wandell Brian A., 1995, FDN VISION SINAUER A
   Wiebel CB, 2014, J VISION, V14, DOI 10.1167/14.7.10
   Wiebel CB, 2013, ATTEN PERCEPT PSYCHO, V75, P954, DOI 10.3758/s13414-013-0436-y
   김진호, 2011, [The Journal of the Korea Contents Association, 한국콘텐츠학회 논문지], V11, P1
NR 31
TC 4
Z9 4
U1 0
U2 2
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD FEB
PY 2017
VL 14
IS 2
AR 8
DI 10.1145/2967498
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA EM8VC
UT WOS:000395588200001
DA 2024-07-18
ER

PT J
AU Giraud, T
   Focone, F
   Demulier, V
   Martin, JC
   Isableu, B
AF Giraud, Tom
   Focone, Florian
   Demulier, Virginie
   Martin, Jean Claude
   Isableu, Brice
TI Perception of Emotion and Personality through Full-Body Movement
   Qualities: A Sport Coach Case Study
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Experimentation; Human Factors; Full-body movement; kinematics; emotion
   perception; personality perception; sport coach
ID NONVERBAL BEHAVIOR; DISCRIMINANT VALIDATION; EFFORT-SHAPE; EXPRESSION;
   ACCURACY; MODEL; TRAITS; RECOGNITION; PERFORMANCE; CONVERGENT
AB Virtual sport coaches guide users through their physical activity and provide motivational support. Users' motivation can rapidly decay if the movements of the virtual coach are too stereotyped. Kinematic patterns generated while performing a predefined fitness movement can elicit and help to prolong users' interaction and interest in training. Human body kinematics has been shown to convey various social attributes such as gender, identity, and acted emotions. To date, no study provides information regarding how spontaneous emotions and personality traits together are perceived from full-body movements. In this article, we study how people make reliable inferences regarding spontaneous emotional dimensions and personality traits of human coaches from kinematic patterns they produced when performing a fitness sequence. Movements were presented to participants via a virtual mannequin to isolate the influence of kinematics on perception. Kinematic patterns of biological movement were analyzed in terms of movement qualities according to the effort-shape [Dell 1977] notation proposed by Laban [1950]. Three studies were performed to provide an analysis of the process leading to perception: from coaches' states and traits through bodily movements to observers' social perception. Thirty-two participants (i.e., observers) were asked to rate the movements of the virtual mannequin in terms of conveyed emotion dimensions, personality traits (five-factor model of personality), and perceived movement qualities (effort-shape) from 56 fitness movement sequences. The results showed high reliability for most of the evaluated dimensions, confirming interobserver agreement from kinematics at zero acquaintance. A large expressive halo merging emotional (e.g., perceived intensity) and personality aspects (e.g., extraversion) was found, driven by perceived kinematic impulsivity and energy. Observers' perceptions were partially accurate for emotion dimensions and were not accurate for personality traits. Together, these results contribute to both the understanding of dimensions of social perception through movement and the design of expressive virtual sport coaches.
C1 [Giraud, Tom; Focone, Florian; Demulier, Virginie; Martin, Jean Claude] Univ Paris South, LIMSI CNRS UPR 3251, Bldg 508,Off 204,BP 133, F-91403 Orsay, France.
   [Isableu, Brice] Univ Paris South, UR CIAMS EA 4532, F-91403 Orsay, France.
C3 Centre National de la Recherche Scientifique (CNRS); CNRS - Institute
   for Information Sciences & Technologies (INS2I); Universite Paris
   Saclay; Universite Paris Saclay
RP Giraud, T (corresponding author), Univ Paris South, LIMSI CNRS UPR 3251, Bldg 508,Off 204,BP 133, F-91403 Orsay, France.
EM tom.giraud@limsi.fr; florian.focone@limsi.fr; demulier@limsi.fr;
   martin@limsi.fr; brice.isableu@u-psud.fr
RI ISABLEU, Brice/KEJ-6773-2024
OI ISABLEU, Brice/0000-0002-5399-1405
FU ANR INGREDIBLE project [ANR-12-CORD-001]
FX This work is supported by ANR INGREDIBLE project: ANR-12-CORD-001
   (http://www.ingredible.fr).
CR Alvarado N, 1997, MOTIV EMOTION, V21, P323, DOI 10.1023/A:1024484306654
   Ambady N, 2000, ADV EXP SOC PSYCHOL, V32, P201, DOI 10.1016/S0065-2601(00)80006-4
   [Anonymous], 1956, Perception and the representative design of psychological experiments
   [Anonymous], 1987, POSTURE AND GESTURE
   App B, 2011, EMOTION, V11, P603, DOI 10.1037/a0023164
   Atkinson AP, 2004, PERCEPTION, V33, P717, DOI 10.1068/p5096
   Barliya A, 2013, EXP BRAIN RES, V225, P159, DOI 10.1007/s00221-012-3357-4
   Bernieri FJ, 1996, J PERS SOC PSYCHOL, V71, P110, DOI 10.1037/0022-3514.71.1.110
   Berry DS, 2000, PERS SOC PSYCHOL B, V26, P278, DOI 10.1177/0146167200265002
   Brown TA., 2006, CONFIRMATORY FACTOR, DOI DOI 10.19030/AJBE.V6I3.7818
   CAMPBELL DT, 1959, PSYCHOL BULL, V56, P81, DOI 10.1037/h0046016
   Camurri A, 2003, INT J HUM-COMPUT ST, V59, P213, DOI 10.1016/S1071-5819(03)00050-8
   COOPER WH, 1981, PSYCHOL BULL, V90, P218, DOI 10.1037/0033-2909.90.2.218
   Costa P. T., 1992, Personality and Individual Differences, V13, P653, DOI DOI 10.1016/0191-8869(92)90236-I
   Crane EA, 2013, J NONVERBAL BEHAV, V37, P91, DOI 10.1007/s10919-013-0144-2
   Cumming G, 2005, AM PSYCHOL, V60, P170, DOI 10.1037/0003-066X.60.2.170
   Dacey M, 2008, AM J HEALTH BEHAV, V32, P570, DOI 10.5555/ajhb.2008.32.6.570
   Dael N, 2013, PERCEPTION, V42, P642, DOI 10.1068/p7364
   Dael N, 2012, J NONVERBAL BEHAV, V36, P97, DOI 10.1007/s10919-012-0130-0
   Dell Cecily., 1977, A Primer for Movement Description Using Effort-Shape and Supplementary Concepts
   DEMEIJER M, 1989, J NONVERBAL BEHAV, V13, P247, DOI 10.1007/BF00990296
   Ellsworth PC, 2003, SER AFFECTIVE SCI, P572
   Engell AD, 2007, J COGNITIVE NEUROSCI, V19, P1508, DOI 10.1162/jocn.2007.19.9.1508
   Fredrickson Barbara L, 1998, Rev Gen Psychol, V2, P300
   Frijda N. H., 1986, EMOTIONS
   GALLAHER PE, 1992, J PERS SOC PSYCHOL, V63, P133, DOI 10.1037/0022-3514.63.1.133
   GANGESTAD SW, 1992, J PERS SOC PSYCHOL, V62, P688, DOI 10.1037/0022-3514.62.4.688
   GIFFORD R, 1991, J PERS SOC PSYCHOL, V61, P279, DOI 10.1037/0022-3514.61.2.279
   Giles Howard, 2006, SAGE HDB NONVERBAL C
   Glowinski D, 2011, IEEE T AFFECT COMPUT, V2, P106, DOI 10.1109/T-AFFC.2011.7
   Graham MH, 2003, ECOLOGY, V84, P2809, DOI 10.1890/02-3114
   Grammer K, 1997, NEW ASPECTS OF HUMAN ETHOLOGY, P91, DOI 10.1007/978-0-585-34289-4_6
   Graziano WG, 2007, J PERS SOC PSYCHOL, V93, P583, DOI 10.1037/0022-3514.93.4.583
   GROSS JJ, 1995, PERS INDIV DIFFER, V19, P555, DOI 10.1016/0191-8869(95)00055-B
   Gross MM, 2012, HUM MOVEMENT SCI, V31, P202, DOI 10.1016/j.humov.2011.05.001
   Gross MM, 2010, J NONVERBAL BEHAV, V34, P223, DOI 10.1007/s10919-010-0094-x
   Hall JA, 2008, J RES PERS, V42, P1476, DOI 10.1016/j.jrp.2008.06.013
   Harrigan JinniA., 2008, NEW HDB METHODS NONV, DOI [DOI 10.1093/ACPROF:OSO/9780198529620.001.0001, DOI 10.1093/ACPROF:OSO/9780198529620.003.0005]
   Hietanen JK, 2004, J NONVERBAL BEHAV, V28, P53, DOI 10.1023/B:JONB.0000017867.70191.68
   HOAGLIN DC, 1986, J AM STAT ASSOC, V81, P991, DOI 10.2307/2289073
   Jackson B, 2011, PSYCHOL SPORT EXERC, V12, P222, DOI 10.1016/j.psychsport.2010.11.005
   JOHANSSON G, 1973, PERCEPT PSYCHOPHYS, V14, P201, DOI 10.3758/BF03212378
   John O.P., 1991, BIG 5 INVENTORY VERS, DOI DOI 10.1037/T07550-000
   JOHNSON DM, 1963, J APPL PSYCHOL, V47, P46, DOI 10.1037/h0044759
   KENDON A, 1972, AM J PSYCHOL, V85, P441, DOI 10.2307/1420845
   Kendon A., 1994, Research on language and social interaction, V27, P175, DOI [DOI 10.1207/S15327973RLSI2703_2, 10.1207/s15327973rlsi2703_2]
   KENNY DA, 1991, PSYCHOL REV, V98, P155, DOI 10.1037/0033-295X.98.2.155
   Kikhia B, 2014, SENSORS-BASEL, V14, P5725, DOI 10.3390/s140305725
   Kleinsmith A, 2013, IEEE T AFFECT COMPUT, V4, P15, DOI 10.1109/T-AFFC.2012.16
   Koppensteiner M, 2013, J EXP SOC PSYCHOL, V49, P1137, DOI 10.1016/j.jesp.2013.08.002
   Koppensteiner M, 2010, J RES PERS, V44, P374, DOI 10.1016/j.jrp.2010.04.002
   Kraha A, 2012, FRONT PSYCHOL, V3, DOI 10.3389/fpsyg.2012.00044
   La France BH, 2004, COMMUN MONOGR, V71, P28, DOI 10.1080/03634520410001693148
   LANDY FJ, 1980, J APPL PSYCHOL, V65, P501, DOI 10.1037/0021-9010.65.5.501
   Levy J., 2003, Individual Differences Research, V1, P39
   Lowe S, 2012, PROCEDIA ENGINEER, V34, P242, DOI 10.1016/j.proeng.2012.04.042
   Luck G, 2010, J RES PERS, V44, P714, DOI 10.1016/j.jrp.2010.10.001
   Lyons EJ, 2014, HEALTH PSYCHOL, V33, P174, DOI 10.1037/a0031947
   MCGOWAN J, 1976, J PERS SOC PSYCHOL, V34, P791
   Montepare J, 1999, J NONVERBAL BEHAV, V23, P133, DOI 10.1023/A:1021435526134
   Montepare JM, 2003, J NONVERBAL BEHAV, V27, P237, DOI 10.1023/A:1027332800296
   Morewedge CK, 2010, TRENDS COGN SCI, V14, P435, DOI 10.1016/j.tics.2010.07.004
   Nakagawa S, 2004, BEHAV ECOL, V15, P1044, DOI 10.1093/beheco/arh107
   Naumann LP, 2009, PERS SOC PSYCHOL B, V35, P1661, DOI 10.1177/0146167209346309
   OUSS L, 1990, Encephale, V16, P453
   Pelachaud C, 2009, SPEECH COMMUN, V51, P630, DOI 10.1016/j.specom.2008.04.009
   Plaisant O, 2010, ANN MED-PSYCHOL, V168, P97, DOI 10.1016/j.amp.2009.09.003
   Podsakoff PM, 2003, J APPL PSYCHOL, V88, P879, DOI 10.1037/0021-9010.88.5.879
   Pollick FE, 2001, COGNITION, V82, pB51, DOI 10.1016/S0010-0277(01)00147-0
   Raedeke TD, 2007, J APPL SPORT PSYCHOL, V19, P105, DOI 10.1080/10413200601113638
   Riggio HR, 2002, J NONVERBAL BEHAV, V26, P195, DOI 10.1023/A:1022117500440
   Roether CL, 2009, J VISION, V9, DOI 10.1167/9.6.15
   RUNESON S, 1983, J EXP PSYCHOL GEN, V112, P585, DOI 10.1037/0096-3445.112.4.585
   RUSHTON JP, 1981, PSYCHOL REV, V88, P582, DOI 10.1037/0033-295X.88.6.582
   RUSSELL JA, 1977, J RES PERS, V11, P273, DOI 10.1016/0092-6566(77)90037-X
   Samadani AA, 2013, INT CONF AFFECT, P343, DOI 10.1109/ACII.2013.63
   Scherer KR, 2013, COMPUT SPEECH LANG, V27, P40, DOI 10.1016/j.csl.2011.11.003
   SCHERER KR, 1978, EUR J SOC PSYCHOL, V8, P467, DOI 10.1002/ejsp.2420080405
   Scherer KR., 2010, BLUEPRINT AFFECTIVE, P3
   Sheets-Johnstone M, 1999, The primacy of movement
   Shikanai N, 2013, J NONVERBAL BEHAV, V37, P107, DOI 10.1007/s10919-013-0148-y
   Sneddon I, 2011, COGN TECHNOL, P753, DOI 10.1007/978-3-642-15184-2_41
   Thoresen JC, 2012, COGNITION, V124, P261, DOI 10.1016/j.cognition.2012.05.018
   Thorndike EL, 1920, J APPL PSYCHOL, V4, P25, DOI 10.1037/h0071663
   Tilmanne Joelle, 2012, Transactions on Computational Science XVI, P34, DOI 10.1007/978-3-642-32663-9_3
   Van Dyck E, 2013, J NONVERBAL BEHAV, V37, P175, DOI 10.1007/s10919-013-0153-1
   Vinciarelli A, 2014, IEEE T AFFECT COMPUT, V5, P273, DOI 10.1109/TAFFC.2014.2330816
   von Laban Rudolf, 1950, MASTERY MOVEMENT STA
   WALLBOTT HG, 1985, J CLIN PSYCHOL, V41, P345, DOI 10.1002/1097-4679(198505)41:3<345::AID-JCLP2270410307>3.0.CO;2-9
   Wallbott HG, 1998, EUR J SOC PSYCHOL, V28, P879, DOI 10.1002/(SICI)1099-0992(1998110)28:6<879::AID-EJSP901>3.0.CO;2-W
   Zebrowitz LA, 2007, PERS SOC PSYCHOL B, V33, P648, DOI 10.1177/0146167206297399
NR 91
TC 6
Z9 8
U1 2
U2 51
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD DEC
PY 2015
VL 13
IS 1
AR 2
DI 10.1145/2791294
PG 27
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA DD2SV
UT WOS:000369773400002
DA 2024-07-18
ER

PT J
AU Hakala, JH
   Oittinen, P
   Hakkinen, JP
AF Hakala, Jussi H.
   Oittinen, Pirkko
   Hakkinen, Jukka P.
TI Depth Artifacts Caused by Spatial Interlacing in Stereoscopic 3D
   Displays
SO ACM Transactions on Applied Perception
LA English
DT Article
DE Experimentation; Design; Human Factors; Stereoscopic displays; passive
   3D displays; depth artifacts; binocular perception
ID PERCEPTION; RESOLUTION; DISPARITY; VERGENCE; SIZE
AB Most spatially interlacing stereoscopic 3D displays display odd and even rows of an image to either the left or the right eye of the viewer. The visual system then fuses the interlaced image into a single percept. This row-based interlacing creates a small vertical disparity between the images; however, interlacing may also induce horizontal disparities, thus generating depth artifacts. Whether people perceive the depth artifacts and, if so, what is the magnitude of the artifacts are unknown. In this study, we hypothesized and tested if people perceive interlaced edges on different depth levels. We tested oblique edge orientations ranging from 2 degrees to 32 degrees and pixel sizes ranging from 16 to 79 arcsec of visual angle in a depth probe experiment. Five participants viewed the visual stimuli through a stereoscope under three viewing conditions: noninterlaced, interlaced, and row averaged (i.e., where even and odd rows are averaged). Our results indicated that people perceive depth artifacts when viewing interlaced stereoscopic images and that these depth artifacts increase with pixel size and decrease with edge orientation angle. A pixel size of 32 arcsec of visual angle still evoked depth percepts, whereas 16 arcsec did not. Row-averaging images effectively eliminated these depth artifacts. These findings have implications for display design, content production, image quality studies, and stereoscopic games and software.
C1 [Hakala, Jussi H.; Oittinen, Pirkko] Aalto Univ, FI-00076 Aalto, Finland.
   [Hakkinen, Jukka P.] Univ Helsinki, FIN-00014 Helsinki, Finland.
C3 Aalto University; University of Helsinki
RP Hakala, JH (corresponding author), Aalto Univ, Dept Media Technol, POB 15500, FI-00076 Aalto, Finland.
EM jussi.h.hakala@aalto.fi; pirkko.oittinen@aalto.fi;
   jukka.hakkinen@helsinki.fi
RI Häkkinen, Jukka/A-4122-2019; Hakala, Jussi/J-4080-2012
OI Häkkinen, Jukka/0000-0003-0215-2238; Hakala, Jussi/0000-0002-4103-0404
FU Academy of Finland [264323]; Academy of Finland (AKA) [264323] Funding
   Source: Academy of Finland (AKA)
FX This research was supported by Academy of Finland project "Computational
   Psychology of Experience in Human-Computer Interaction" (project number
   264323).
CR ARDITI A, 1981, VISION RES, V21, P755, DOI 10.1016/0042-6989(81)90173-5
   Backus BT, 1999, VISION RES, V39, P1143, DOI 10.1016/S0042-6989(98)00139-4
   Banks MS, 2012, SMPTE MOTION IMAG J, V121, P24, DOI 10.5594/j18173
   Boev Atanas, 2009, Proceedings of the SPIE - The International Society for Optical Engineering, V7237, DOI 10.1117/12.807185
   Brewster D, 1844, T ROY SOC EDINBURGH, V15, P663, DOI DOI 10.1017/S0080456800030246
   BURT P, 1980, PERCEPTION, V9, P671, DOI 10.1068/p090671
   DUWAER AL, 1981, VISION RES, V21, P1727, DOI 10.1016/0042-6989(81)90205-4
   Geisler WS, 2008, ANNU REV PSYCHOL, V59, P167, DOI 10.1146/annurev.psych.58.110405.085632
   Held RT, 2008, APGV 2008: PROCEEDINGS OF THE SYMPOSIUM ON APPLIED PERCEPTION IN GRAPHICS AND VISUALIZATION, P23
   Hong J, 2011, APPL OPTICS, V50, pH87, DOI 10.1364/AO.50.000H87
   Howard I. P., 2012, Perceiving in Depth, V2
   Howard IP, 2000, EXP BRAIN RES, V130, P124, DOI 10.1007/s002210050014
   JULESZ B, 1960, AT&T TECH J, V39, P1125, DOI 10.1002/j.1538-7305.1960.tb03954.x
   Kelley Edward F., 2011, Information Display, V27, P18
   Kim JS, 2012, SID S DIG TEC, P879
   MITCHISON GJ, 1987, VISION RES, V27, P285, DOI 10.1016/0042-6989(87)90191-X
   Mitchison Graeme J., 1985, NATURE, V315, P402
   Naiman AC, 1998, ACM T GRAPHIC, V17, P238, DOI 10.1145/293145.293147
   Park M, 2014, APPL OPTICS, V53, P520, DOI 10.1364/AO.53.000520
   Park Minyoung, 2013, DIGITAL HOLOGRAPHY 3
   RAMACHANDRAN VS, 1985, NATURE, V317, P527, DOI 10.1038/317527a0
   Stransky D, 2014, ACM T APPL PERCEPT, V11, DOI 10.1145/2536810
   Ukwade Michael T., 2000, OPTOMETRY VISION SCI, V77, P309
   van Ee R, 2000, VISION RES, V40, P151, DOI 10.1016/S0042-6989(99)00174-1
   Vlaskamp BS, 2013, FRONT PSYCHOL, V4, DOI 10.3389/fpsyg.2013.00836
   Yun JD, 2013, J DISP TECHNOL, V9, P106, DOI 10.1109/JDT.2012.2228252
   Zhang ZL, 2001, VISION RES, V41, P2995, DOI 10.1016/S0042-6989(01)00179-1
NR 27
TC 4
Z9 4
U1 0
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD MAR
PY 2015
VL 12
IS 1
BP 49
EP 61
DI 10.1145/2699266
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CG8AI
UT WOS:000353528600003
OA Green Published
DA 2024-07-18
ER

PT J
AU Trutoiu, LC
   Carter, EJ
   Pollard, N
   Cohn, JF
   Hodgins, JK
AF Trutoiu, Laura C.
   Carter, Elizabeth J.
   Pollard, Nancy
   Cohn, Jeffrey F.
   Hodgins, Jessica K.
TI Spatial and Temporal Linearities in Posed and Spontaneous Smiles
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Facial Animation; Realism; Smile animation
ID FALSE; FELT
AB Creating facial animations that convey an animator's intent is a difficult task because animation techniques are necessarily an approximation of the subtle motion of the face. Some animation techniques may result in linearization of the motion of vertices in space (blendshapes, for example), and other, simpler techniques may result in linearization of the motion in time. In this article, we consider the problem of animating smiles and explore how these simplifications in space and time affect the perceived genuineness of smiles. We create realistic animations of spontaneous and posed smiles from high-resolution motion capture data for two computer-generated characters. The motion capture data is processed to linearize the spatial or temporal properties of the original animation. Through perceptual experiments, we evaluate the genuineness of the resulting smiles. Both space and time impact the perceived genuineness. We also investigate the effect of head motion in the perception of smiles and show similar results for the impact of linearization on animations with and without head motion. Our results indicate that spontaneous smiles are more heavily affected by linearizing the spatial and temporal properties than posed smiles. Moreover, the spontaneous smiles were more affected by temporal linearization than spatial linearization. Our results are in accordance with previous research on linearities in facial animation and allow us to conclude that a model of smiles must include a nonlinear model of velocities.
C1 [Trutoiu, Laura C.; Carter, Elizabeth J.; Pollard, Nancy; Cohn, Jeffrey F.; Hodgins, Jessica K.] Carnegie Mellon Univ, Inst Robot, Pittsburgh, PA 15213 USA.
   [Cohn, Jeffrey F.] Univ Pittsburgh, Pittsburgh, PA 15260 USA.
C3 Carnegie Mellon University; Pennsylvania Commonwealth System of Higher
   Education (PCSHE); University of Pittsburgh
RP Trutoiu, LC (corresponding author), Carnegie Mellon Univ, Inst Robot, 5000 Forbes Ave, Pittsburgh, PA 15213 USA.
EM ltrutoiu@cs.cmu.edu; ejcarter@andrew.cmu.edu; nsp@cs.cmu.edu;
   jeffcohn@cs.cmu.edu; jkh@cs.cmu.edu
RI Carter, Elizabeth J/G-6958-2012
OI Carter, Elizabeth J./0000-0002-2735-148X; Pollard,
   Nancy/0000-0001-6464-839X
FU NSF; Disney Research, Pittsburgh
FX This study was funded by the NSF and Disney Research, Pittsburgh.
CR Akhter I., 2012, ACM T GRAPHIC, V31, P1
   Ambadar Z, 2009, J NONVERBAL BEHAV, V33, P17, DOI 10.1007/s10919-008-0059-5
   [Anonymous], 2011, International Journal of Wavelets Multiresolution and Information Processing, DOI DOI 10.1142/S021969130400041X
   Cosker D., 2010, P S APPL PERCEPTION, P101, DOI DOI 10.1145/1836248.1836268
   Dryden Ian L., 2002, STAT SHAPE ANAL
   EKMAN P, 1982, J NONVERBAL BEHAV, V6, P238, DOI 10.1007/BF00987191
   EKMAN P, 1990, J PERS SOC PSYCHOL, V58, P342, DOI 10.1037/0022-3514.58.2.342
   Ekman P., 2001, TELLING LIES CLUES D, V2nd
   Ekman P, 1978, FACIAL ACTION CODING
   HESS U, 1994, EUR J SOC PSYCHOL, V24, P367, DOI 10.1002/ejsp.2420240306
   Hyde J., 2013, P 10 IEEE INT C AUT
   Kalwick David J., 2006, ANIMATING FACIAL FEA
   Krumhuber E, 2005, J NONVERBAL BEHAV, V29, P3, DOI 10.1007/s10919-004-0887-x
   Krumhuber EG, 2009, EMOTION, V9, P807, DOI 10.1037/a0017844
   Liu XC, 2011, COMPUT GRAPH FORUM, V30, P1655, DOI 10.1111/j.1467-8659.2011.01852.x
   McDonnell R, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185587
   Schmidt KL, 2006, LATERALITY, V11, P540, DOI 10.1080/13576500600832758
   Tena JR, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964971
   Trutoiu LC, 2011, ACM T APPL PERCEPT, V8, DOI 10.1145/2010325.2010327
   Trutoiu Laura C., 2013, P 10 IEEE INT C AUT
   Wallraven C, 2008, ACM T APPL PERCEPT, V4, DOI 10.1145/1278760.1278764
NR 21
TC 6
Z9 7
U1 0
U2 2
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD JUN
PY 2014
VL 11
IS 3
SI SI
AR 12
DI 10.1145/2641569
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AU0KM
UT WOS:000345311700003
DA 2024-07-18
ER

PT J
AU Zhang, RM
   Nordman, A
   Walker, J
   Kuhl, SA
AF Zhang, Ruimin
   Nordman, Anthony
   Walker, James
   Kuhl, Scott A.
TI Minification Affects Verbal- and Action-Based Distance Judgments
   Differently in Head-Mounted Displays
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Human Factors; Virtual environments; distance judgments; perception;
   head-mounted display; minification
ID PERCEPTION; LOCOMOTION; GRAPHICS; QUALITY
AB Numerous studies report that people underestimate egocentric distances in Head-Mounted Display (HMD) virtual environments compared to real environments as measured by direct blind walking. Geometric minification, or rendering graphics with a larger field of view than the display's field of view, has been shown to eliminate this underestimation in a virtual hallway environment [Kuhl et al. 2006, 2009]. This study demonstrates that minification affects blind walking in a sparse classroom and does not influence verbal reports of distance. Since verbal reports of distance have been reported to be compressed in real environments, we speculate that minification in an HMD replicates peoples' real-world blind walking and verbal report distance judgments. We also demonstrate a new method for quantifying any unintentional miscalibration in our experiments. This process involves using the HMD in an augmented reality configuration and having each participant indicate where the targets and horizon appeared after each experiment. More work is necessary to understand how and why minification changes verbal- and walking-based egocentric distance judgments differently.
C1 [Zhang, Ruimin; Nordman, Anthony; Walker, James; Kuhl, Scott A.] Michigan Technol Univ, Dept Comp Sci, Houghton, MI 49931 USA.
C3 Michigan Technological University
RP Kuhl, SA (corresponding author), Michigan Technol Univ, Dept Comp Sci, 1400 Townsend Dr, Houghton, MI 49931 USA.
EM kuhl@mtu.edu
CR Alexandrova I.V., 2010, Proceedings of the 7th Symposium on Applied Perception in Graphics and Visualization, APGV'10, P57, DOI DOI 10.1145/1836248.1836258
   Andre J, 2006, PERCEPT PSYCHOPHYS, V68, P353, DOI 10.3758/BF03193682
   [Anonymous], 2008, P 2008 ACM S VIRTUAL, DOI DOI 10.1145/1450579.1450614
   [Anonymous], 2005, ACM Transactions on Applied Perception, DOI [DOI 10.1145/1077399.1077403, DOI 10.1145/1077399.10774032,3,9]
   Bodenheimer B, 2007, APGV 2007: SYMPOSIUM ON APPLIED PERCEPTION IN GRAPHICS AND VISUALIZATION, PROCEEDINGS, P35
   Goodale M.A., 1995, Visual Cognition, P167
   Grechkin TY, 2010, ACM T APPL PERCEPT, V7, DOI 10.1145/1823738.1823744
   Interrante V, 2008, PRESENCE-TELEOP VIRT, V17, P176, DOI 10.1162/pres.17.2.176
   Kelly JW, 2004, PERCEPTION, V33, P443, DOI 10.1068/p5218
   Klein E, 2009, IEEE VIRTUAL REALITY 2009, PROCEEDINGS, P107, DOI 10.1109/VR.2009.4811007
   Knapp J. M., 1999, ProQuest Diss. Theses,, P125
   Knapp JM, 2004, PRESENCE-TELEOP VIRT, V13, P572, DOI 10.1162/1054746042545238
   Kuhl SA, 2009, ACM T APPL PERCEPT, V6, DOI 10.1145/1577755.1577762
   Kuhl ScottA., 2006, P 3 S APPL PERCEPTIO, P15, DOI DOI 10.1145/1140491.1140494
   Kunz BR, 2009, ATTEN PERCEPT PSYCHO, V71, P1284, DOI 10.3758/APP.71.6.1284
   Leyrer M., 2011, P ACM SIGGRAPH S APP, DOI 10.1145/2077451.2077464
   Loomis JM, 1998, PERCEPT PSYCHOPHYS, V60, P966, DOI 10.3758/BF03211932
   LOOMIS JM, 1992, J EXP PSYCHOL HUMAN, V18, P906, DOI 10.1037/0096-1523.18.4.906
   Mohler BJ, 2010, PRESENCE-TELEOP VIRT, V19, P230, DOI 10.1162/pres.19.3.230
   Mohler BettyJ., 2006, P 3 S APPL PERCEPTIO, P9, DOI [10.1145/1140491.1140493, DOI 10.1145/1140491.1140493]
   MOHLER BJ, 2008, P S APPL PERC GRAPH
   Obaid Mohammad, 2011, Intelligent Virtual Agents. Proceedings 11th International Conference, IVA 2011, P363, DOI 10.1007/978-3-642-23974-8_39
   Ooi TL, 2001, NATURE, V414, P197, DOI 10.1038/35102562
   Philbeck JW, 1997, J EXP PSYCHOL HUMAN, V23, P72, DOI 10.1037/0096-1523.23.1.72
   Proffitt DR, 2003, PSYCHOL SCI, V14, P106, DOI 10.1111/1467-9280.t01-1-01427
   Richardson AR, 2007, HUM FACTORS, V49, P507, DOI 10.1518/001872007X200139
   Ries B., 2009, Proceedings of the 16th ACM Symposium on Virtual Reality Software and Technology, VRST '09, P59, DOI [10.1145/1643928.1643943, DOI 10.1145/1643928.16439433, DOI 10.1145/1643928.1643943]
   RIESER JJ, 1990, PERCEPTION, V19, P675, DOI 10.1068/p190675
   RIESER JJ, 1995, J EXP PSYCHOL HUMAN, V21, P480, DOI 10.1037/0096-1523.21.3.480
   Sahm C. S., 2005, ACM Trans. Appl. Percept. (TAP), V2, P35, DOI DOI 10.1145/1048687.1048690
   Sedgwick HA., 1986, HDB PERCEPTION HUMAN, P1
   Steinicke F, 2011, IEEE T VIS COMPUT GR, V17, P888, DOI 10.1109/TVCG.2010.248
   Thompson WB, 2004, PRESENCE-TELEOP VIRT, V13, P560, DOI 10.1162/1054746042545292
   Thompson WB, 2007, PERCEPTION, V36, P1559, DOI 10.1068/p5667
   Willemsen P, 2009, ACM T APPL PERCEPT, V6, DOI 10.1145/1498700.1498702
   Williams B., 2009, Proc. Sixth Symposium on Applied Perception in Graphics and Visualization, P7
   Witmer BG, 1998, HUM FACTORS, V40, P478, DOI 10.1518/001872098779591340
   Witmer BG, 1998, PRESENCE-TELEOP VIRT, V7, P144, DOI 10.1162/105474698565640
   Wu B, 2004, NATURE, V428, P73, DOI 10.1038/nature02350
NR 39
TC 19
Z9 23
U1 0
U2 8
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD JUL
PY 2012
VL 9
IS 3
AR 14
DI 10.1145/2325722.2325727
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA 984EE
UT WOS:000307171700005
DA 2024-07-18
ER

PT J
AU Vanhala, T
   Surakka, V
   Courgeon, M
   Martin, JC
AF Vanhala, Toni
   Surakka, Veikko
   Courgeon, Matthieu
   Martin, Jean-Claude
TI Voluntary Facial Activations Regulate Physiological Arousal and
   Subjective Experiences During Virtual Social Stimulation
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Experimentation; Human Factors; Affective computing; virtual characters;
   social anxiety; facial muscles; electrodermal activity
ID REALITY EXPOSURE THERAPY; PHOBIA INVENTORY SPIN; GENERAL-POPULATION;
   ANXIETY DISORDERS; EMOTION; FEAR; EXPRESSIONS; RESPONSES; CHARACTERS;
   ATTENTION
AB Exposure to distressing computer-generated stimuli and feedback of physiological changes during exposure have been effective in the treatment of anxiety disorders (e. g., social phobia). Here we studied voluntary facial activations as a method for regulating more spontaneous physiological changes during virtual social stimulation. Twenty-four participants with a low or high level of social anxiety activated either the corrugator supercilii (used in frowning) or the zygomaticus major (used in smiling) facial muscle to keep a female or a male computer character walking towards them. The more socially anxious participants had a higher level of skin conductance throughout the trials as compared to less anxious participants. Within both groups, short-term skin conductance responses were enhanced both during and after facial activations; and corrugator supercilii activations facilitated longer term electrodermal relaxation. Zygomaticus major activations had opposite effects on subjective emotional ratings of the less and the more socially anxious. In sum, voluntary facial activations were effective in regulating emotional arousal during virtual social exposure. Corrugator supercilii activation was found an especially promising method for facilitating autonomic relaxation.
C1 [Surakka, Veikko] Univ Tampere, Sch Informat Sci, Tampere Unit Computer Human Interact TAUCHI, Res Grp Emot Social & Comp, Tampere 33014, Finland.
   [Courgeon, Matthieu; Martin, Jean-Claude] Paris S XI Univ, Natl Ctr Sci Res, Comp Sci Lab Mech & Engn Sci LIMSI, F-91403 Paris, France.
C3 Tampere University
RP Vanhala, T (corresponding author), VTT Tech Res Ctr Finland, POB 1300, Tampere 33101, Finland.
EM toni.vanhala@vtt.fi
OI Surakka, Veikko/0000-0003-3986-0713
FU Graduate School in User-Centered Information Technology; Academy of
   Finland [1115997]
FX This study was financially supported by the Graduate School in
   User-Centered Information Technology and the Academy of Finland, project
   number 1115997.
CR Anderson P, 2003, COGN BEHAV PRACT, V10, P240, DOI 10.1016/S1077-7229(03)80036-6
   Beale R, 2009, INT J HUM-COMPUT ST, V67, P755, DOI 10.1016/j.ijhcs.2009.05.001
   Beck JG, 2007, BEHAV THER, V38, P39, DOI 10.1016/j.beth.2006.04.001
   Boiten F, 1996, PSYCHOPHYSIOLOGY, V33, P123, DOI 10.1111/j.1469-8986.1996.tb02116.x
   Bradley MM, 2007, HANDBOOK OF PSYCHOPHYSIOLOGY, 3RD EDITION, P581, DOI 10.1017/CBO9780511546396.025
   Bradley MM, 2001, EMOTION, V1, P276, DOI 10.1037//1528-3542.1.3.276
   BRADLEY MM, 1994, J BEHAV THER EXP PSY, V25, P49, DOI 10.1016/0005-7916(94)90063-9
   Cacioppo J.T., 2000, Handbook of Emotions, P173
   Carroll JM, 1997, J PERS SOC PSYCHOL, V72, P164, DOI 10.1037/0022-3514.72.1.164
   Christie IC, 2004, INT J PSYCHOPHYSIOL, V51, P143, DOI 10.1016/j.ijpsycho.2003.08.002
   Coan JA, 2001, PSYCHOPHYSIOLOGY, V38, P912, DOI 10.1111/1469-8986.3860912
   Codispoti M, 2007, PSYCHOPHYSIOLOGY, V44, P680, DOI 10.1111/j.1469-8986.2007.00545.x
   Connor KM, 2000, BRIT J PSYCHIAT, V176, P379, DOI 10.1192/bjp.176.4.379
   COURGEON M., 2008, P ICM MLMI 2008 WORK
   Courgeon M, 2009, LECT NOTES ARTIF INT, V5773, P201, DOI 10.1007/978-3-642-04380-2_24
   Courtney CG, 2010, INT J PSYCHOPHYSIOL, V78, P107, DOI 10.1016/j.ijpsycho.2010.06.028
   Dawson M.E., 2000, HDB PSYCHOPHYSIOLOGY
   EKMAN P, 1992, COGNITION EMOTION, V6, P169, DOI 10.1080/02699939208411068
   EKMAN P, 1979, HUMAN ETHOLOGY
   Ekman P., 2004, Language, knowledge, and representation, P39, DOI DOI 10.1007/978-1-4020-2783-3_3
   FOA EB, 1986, PSYCHOL BULL, V99, P20, DOI 10.1037/0033-2909.99.1.20
   FOWLES DC, 1988, PSYCHOPHYSIOLOGY, V25, P373, DOI 10.1111/j.1469-8986.1988.tb01873.x
   FRIDLUND AJ, 1986, PSYCHOPHYSIOLOGY, V23, P567, DOI 10.1111/j.1469-8986.1986.tb00676.x
   FRITH CD, 1983, BIOL PSYCHOL, V17, P27, DOI 10.1016/0301-0511(83)90064-9
   Garcia-Palacios A, 2007, CYBERPSYCHOL BEHAV, V10, P722, DOI 10.1089/cpb.2007.9962
   GATCHEL RJ, 1976, J CONSULT CLIN PSYCH, V44, P381, DOI 10.1037/0022-006X.44.3.381
   GEARY DC, 1993, PSYCHOL AGING, V8, P242, DOI 10.1037/0882-7974.8.2.242
   Gerardi M, 2010, CURR PSYCHIAT REP, V12, P298, DOI 10.1007/s11920-010-0128-4
   Gramer M, 2007, BIOL PSYCHOL, V74, P67, DOI 10.1016/j.biopsycho.2006.07.004
   GRAYSON JB, 1982, BEHAV RES THER, V20, P323, DOI 10.1016/0005-7967(82)90091-2
   Harris SR, 2002, CYBERPSYCHOL BEHAV, V5, P543, DOI 10.1089/109493102321018187
   HEIMBERG RG, 1990, COGNITIVE THER RES, V14, P1, DOI 10.1007/BF01173521
   HELLSTROM K, 1995, BEHAV RES THER, V33, P959, DOI 10.1016/0005-7967(95)00028-V
   Hess U, 2004, EMOTION, V4, P378, DOI 10.1037/1528-3542.4.4.378
   Hietanen JK, 1998, PSYCHOPHYSIOLOGY, V35, P530, DOI 10.1017/S0048577298970445
   James LK, 2003, CYBERPSYCHOL BEHAV, V6, P237, DOI 10.1089/109493103322011515
   Josman N, 2008, CYBERPSYCHOL BEHAV, V11, P775, DOI 10.1089/cpb.2008.0048
   Kessler RC, 2003, ACTA PSYCHIAT SCAND, V108, P19, DOI 10.1034/j.1600-0447.108.s417.2.x
   KOZAK M. J., 1988, BEHAV THER, V1.9, P15
   Krijn M, 2004, CLIN PSYCHOL REV, V24, P259, DOI 10.1016/j.cpr.2004.04.001
   Larkin KT, 1998, INT J PSYCHOPHYSIOL, V29, P311, DOI 10.1016/S0167-8760(98)00019-1
   Larsen JT, 2003, PSYCHOPHYSIOLOGY, V40, P776, DOI 10.1111/1469-8986.00078
   LEVENSON RW, 1990, PSYCHOPHYSIOLOGY, V27, P363, DOI 10.1111/j.1469-8986.1990.tb02330.x
   Levenson RW, 2002, PSYCHOPHYSIOLOGY, V39, P397, DOI 10.1017/S0048577201393150
   LEVENSON RW, 1992, J PERS SOC PSYCHOL, V62, P972, DOI 10.1037/0022-3514.62.6.972
   Llobera J, 2010, ACM T APPL PERCEPT, V8, DOI 10.1145/1857893.1857896
   Loftus GR, 2005, PSYCHON B REV, V12, P43, DOI 10.3758/BF03196348
   Mauss IB, 2005, EMOTION, V5, P175, DOI 10.1037/1528-3542.5.2.175
   Mauss I, 2009, COGNITION EMOTION, V23, P209, DOI 10.1080/02699930802204677
   Mick MA, 1998, J ANXIETY DISORD, V12, P1, DOI 10.1016/S0887-6185(97)00046-7
   NASS S., 1994, P SIGCHI C HUM FACT
   Niedenthal PM, 2007, SCIENCE, V316, P1002, DOI 10.1126/science.1136930
   North MM, 1998, ST HEAL T, V58, P112
   Olfson M, 2000, AM J PSYCHIAT, V157, P521, DOI 10.1176/appi.ajp.157.4.521
   OST LG, 1989, BEHAV RES THER, V27, P1, DOI 10.1016/0005-7967(89)90113-7
   Partala T, 2006, INTERACT COMPUT, V18, P208, DOI 10.1016/j.intcom.2005.05.002
   PARTALA T., 2004, P 3 NORD C HUM COMP
   Pertaub DP, 2002, PRESENCE-TELEOP VIRT, V11, P68, DOI 10.1162/105474602317343668
   Powers MB, 2008, J ANXIETY DISORD, V22, P561, DOI 10.1016/j.janxdis.2007.04.006
   Rainville P, 2006, INT J PSYCHOPHYSIOL, V61, P5, DOI 10.1016/j.ijpsycho.2005.10.024
   Ranta K, 2007, EUR PSYCHIAT, V22, P244, DOI 10.1016/j.eurpsy.2006.12.002
   Rapee RM, 1997, BEHAV RES THER, V35, P741, DOI 10.1016/S0005-7967(97)00022-3
   Reeves B., 1999, MEDIA PSYCHOL, V1, P49, DOI [DOI 10.1207/S1532785XMEP0101_4, https://doi.org/10.1207/s1532785xmep01014]
   Rinck M, 2010, COGNITION EMOTION, V24, P1269, DOI 10.1080/02699930903309268
   Rothbaum BO, 2000, J CONSULT CLIN PSYCH, V68, P1020, DOI 10.1037/0022-006X.68.6.1020
   SCHLOSBERG H, 1954, PSYCHOL REV, V61, P81, DOI 10.1037/h0054570
   Schuemie MJ, 2001, CYBERPSYCHOL BEHAV, V4, P183, DOI 10.1089/109493101300117884
   Tassinary L.G., 2000, HDB PSYCHOPHYSIOLOGY, V2nd
   TEGHTSOONIAN R, 1982, J BEHAV THER EXP PSY, V13, P181, DOI 10.1016/0005-7916(82)90002-7
   VANBOXTEL A, 1993, PSYCHOPHYSIOLOGY, V30, P589, DOI 10.1111/j.1469-8986.1993.tb02085.x
   Vanhala Toni, 2008, Affective Computing. Focus on Emotion Expression, Synthesis and Recognition, P405
   Vanhala T, 2007, LECT NOTES COMPUT SC, V4738, P278
   Vanhala T, 2010, COMPUT ANIMAT VIRT W, V21, P215, DOI 10.1002/cav.366
   Ward RD, 2003, INT J HUM-COMPUT ST, V59, P199, DOI 10.1016/S1071-5819(03)00019-3
   WATERINK W, 1994, BIOL PSYCHOL, V37, P183, DOI 10.1016/0301-0511(94)90001-9
   Wiederhold BK, 2005, APPL PSYCHOPHYS BIOF, V30, P183, DOI 10.1007/s10484-005-6375-1
   Wiederhold BK, 2003, CYBERPSYCHOL BEHAV, V6, P441, DOI 10.1089/109493103322278844
   WITVLIET CV, 1995, PSYCHOPHYSIOLOGY, V32, P436
   Zanbaka C, 2007, CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1 AND 2, P1561
NR 79
TC 0
Z9 0
U1 0
U2 14
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD MAR
PY 2012
VL 9
IS 1
AR 1
DI 10.1145/2134203.2134204
PG 21
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA 949YM
UT WOS:000304614400001
DA 2024-07-18
ER

PT J
AU McNamara, A
   Bailey, R
   Grimm, C
AF McNamara, Ann
   Bailey, Reynold
   Grimm, Cindy
TI Search Task Performance Using Subtle Gaze Direction with the Presence of
   Distractions
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Eye tracking; psychophysics; image manipulation; gaze direction;
   luminance
ID MAMMOGRAPHIC MASSES
AB A new experiment is presented that demonstrates the usefulness of an image space modulation technique called subtle gaze direction (SGD) for guiding the user in a simple searching task. SGD uses image space modulations in the luminance channel to guide a viewer's gaze about a scene without interrupting their visual experience. The goal of SGD is to direct a viewer's gaze to certain regions of a scene without introducing noticeable changes in the image. Using a simple searching task, we compared performance using no modulation, using subtle modulation, and using obvious modulation. Results from the experiments show improved performance when using subtle gaze direction, without affecting the user's perception of the image. We then extend the experiment to evaluate performance with the presence of distractors. The distractors took the form of extra modulations, which do not correspond to a target in the image. Experimentation shows, that, even in the presence of distractors, more accurate results are returned on a simple search task using SGD, as compared to results returned when no modulation at all is used. Results establish the potential of the method for a wide range of applications including gaming, perceptually based rendering, navigation in virtual environments, and medical search tasks.
C1 [McNamara, Ann] Texas A&M Univ, College Stn, TX 77843 USA.
   [Bailey, Reynold] Rochester Inst Technol, Rochester, NY USA.
   [Grimm, Cindy] Washington Univ, St Louis, MO 63130 USA.
C3 Texas A&M University System; Texas A&M University College Station;
   Rochester Institute of Technology; Washington University (WUSTL)
RP McNamara, A (corresponding author), Texas A&M Univ, College Stn, TX 77843 USA.
EM ann@viz.tamu.edu; rjb@cs.rit.edu; cmg@cse.wustl.edu
OI Grimm, Cindy/0000-0002-1711-7112
CR [Anonymous], 2003, Level of detail for 3D graphics
   [Anonymous], P SIGGRAPH
   BAILEY R, 2009, ACM T GRAPH IN PRESS
   Bilska-Wolak AO, 2003, MED PHYS, V30, P949, DOI 10.1118/1.1565339
   Boot WR, 2009, ATTEN PERCEPT PSYCHO, V71, P950, DOI 10.3758/APP.71.4.950
   Cater K., 2002, Proceedings of the ACM symposium on Virtual reality software and technology, VRST '02, P17, DOI [10.1145/585740.585744, DOI 10.1145/585740.585744]
   Duchowski AT, 2002, BEHAV RES METH INS C, V34, P455, DOI 10.3758/BF03195475
   El-Baz A, 2006, INT C PATT RECOG, P611
   Kim Y, 2006, IEEE T VIS COMPUT GR, V12, P925, DOI 10.1109/TVCG.2006.174
   KOSARA R, 2001, P IEEE S INF VIS INF
   Larson G.W., 2004, Rendering with radiance: the art and science of lighting visualization
   Mack Arien, 1998, Inattentional Blindness
   MANCAS M, 2004, P IEEE VIS C IEEE LO, P33
   Mitchell G. E., 2004, TAKING CONTROL DEPTH
   Neider MB, 2006, VISION RES, V46, P2217, DOI 10.1016/j.visres.2006.01.006
   O'Sullivan C., 2003, EYE MOVEMENTS INTERA
   OGDEN TE, 1966, VISION RES, V6, P485, DOI 10.1016/0042-6989(66)90001-0
   RAYNER K, 1975, COGNITIVE PSYCHOL, V7, P65, DOI 10.1016/0010-0285(75)90005-5
   Schwaninger A, 2004, 38TH ANNUAL 2004 INTERNATIONAL CARNAHAN CONFERENCE ON SECURITY TECHNOLOGY, PROCEEDINGS, P258, DOI 10.1109/CCST.2004.1405402
   SCHWANINGER A, 2007, P 4 S APPL PERC GRAP, P123, DOI DOI 10.1145/1272582.1272606
   SIMON C, 1995, SEMIN REPROD ENDOCR, V13, P142, DOI 10.1055/s-2007-1016353
   Spillmann L., 1990, Visual perception: the neurophysiological foundations
   Tourassi GD, 2003, MED PHYS, V30, P2123, DOI 10.1118/1.1589494
   ZHUOWEN T, 2006, P IEEE C COMP VIS PA, P1544
NR 24
TC 17
Z9 23
U1 1
U2 9
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD AUG
PY 2009
VL 6
IS 3
AR 17
DI 10.1145/1577755.1577760
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 511ZS
UT WOS:000271212000005
DA 2024-07-18
ER

PT J
AU Hattenberger, TJ
   Fairchild, MD
   Johnson, GM
   Salvaggio, C
AF Hattenberger, Timothy J.
   Fairchild, Mark D.
   Johnson, Garrett M.
   Salvaggio, Carl
TI A Psychophysical Investigation of Global Illumination Algorithms Used in
   Augmented Reality
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Experimentation; Image difference; psychophysics; global illumination;
   perception; rendering
AB The overarching goal of this research was to compare different rendering solutions in order to understand why some yield better results specifically when applied to rendering synthetic objects into real photographs. A psychophysical experiment was conducted in which the composite images were judged for accuracy against the original photograph. In addition, iCAM, an image color appearance model was also used to calculate image differences for the same set of images. Conclusions obtained included the effect of global illumination on the accuracy of the final composite rendering. Also, it was discovered that the original rendering with all of its artifacts is not necessarily an indicator of the final composite image's judged accuracy. Finally, initial results show promise in using iCAM to predict a relationship similar to the psychophysics, which could eventually be used in-the-rendering-loop to achieve photorealism with minimized computation.
C1 [Hattenberger, Timothy J.] Rochester Inst Technol, Digital Imaging & Remote Sensing Lab, Rochester, NY 14623 USA.
C3 Rochester Institute of Technology
RP Hattenberger, TJ (corresponding author), Rochester Inst Technol, Digital Imaging & Remote Sensing Lab, 54 Lomb Mem Dr, Rochester, NY 14623 USA.
EM tim@cis.rit.edu
RI Salvaggio, Carl/GZH-0043-2022
OI Salvaggio, Carl/0000-0001-9293-9696
CR ANDAUER C, 2004, BLENDER DOCUMENTATIO, V1
   DEBEVEC P, 1998, P ACM SPEC INT GROUP
   Ebner F, 1998, SIXTH COLOR IMAGING CONFERENCE: COLOR SCIENCE, SYSTEMS AND APPLICATIONS, P8
   Fairchild MD, 2005, Thirteenth Color Imaging Conference, Final Program and Proceedings, P333
   Fairchild MD, 2004, J ELECTRON IMAGING, V13, P126, DOI 10.1117/1.1635368
   FERWERDA JA, 2003, P SPIE HUM VIS ELECT
   JOHNSON GM, 2003, THESIS ROCHESTER I T
   MCNAMARA A, 2001, P EUR ASS COMP GRAPH
   MCNAMARA A, 2000, P EUR WORKSH REND TE, P207
   MCNAMARA A, 1998, P 9 EUR REND WOKSH
   MEYER GW, 1986, ACM T GRAPHIC, V5, P30, DOI 10.1145/7529.7920
   Montag ED, 2006, J ELECTRON IMAGING, V15, DOI 10.1117/1.2181547
   Pharr M., 2004, Physically Based Rendering: From Theory to Implementation
   RADEMACHER P, 2001, P 12 EUR WORKSH REND, P235
   RUSHMEIER HE, 1995, P 6 EUR WORKSH REND
   SELAN JA, 2003, THESIS CORNELL U
   STOKES WA, 2004, P ACM SIGGRAPH, P742
   *STR INC, RAP PROT CAD PLAST P
   Watson A.B., 1993, DIGITAL IMAGES HUMAN, P179
NR 19
TC 0
Z9 0
U1 0
U2 2
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD FEB
PY 2009
VL 6
IS 1
AR 2
DI 10.1145/1462055.1462057
PG 22
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 450YM
UT WOS:000266437900002
DA 2024-07-18
ER

PT J
AU Jagnow, R
   Dorsey, J
   Rushmeier, H
AF Jagnow, Robert
   Dorsey, Julie
   Rushmeier, Holly
TI Evaluation of Methods for Approximating Shapes Used to Synthesize 3D
   Solid Textures
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Algorithms; Human Factors; Shape estimation; shape perception; texture
   synthesis; solid textures; volumetric textures
ID IMAGES
AB In modern computer graphics applications, textures play an important role in conveying the appearance of real-world materials. But while surface appearance can often be effectively captured with a photograph, it is difficult to use example imagery to synthesize fully three-dimensional (3D) solid textures that are perceptually similar to their inputs. Specifically, this research focuses on human perception of 3D solid textures composed of aggregate particles in a binding matrix. Holding constant an established algorithm for approximating particle distributions, we examine the problem of estimating particle shape. We consider four methods for approximating plausible particle shapes-including two methods of our own contribution. We compare the performance of these methods under a variety of input conditions using automated, perceptually motivated metrics, as well as a psychophysical experiment. In the course of assessing the relative performance of the four algorithms, we also evaluate the reliability of the automated metrics in predicting the results of the experiment.
C1 [Jagnow, Robert] MIT, Cambridge, MA 02139 USA.
   [Dorsey, Julie; Rushmeier, Holly] Yale Univ, New Haven, CT 06520 USA.
C3 Massachusetts Institute of Technology (MIT); Yale University
RP Jagnow, R (corresponding author), MIT, Cambridge, MA 02139 USA.
EM Holly.rushmeier@yale.edu
OI Rushmeier, Holly/0000-0001-5241-0886
CR Amenta Nina, 2001, P 6 ACM S SOL MOD AP
   [Anonymous], 1994, TEXTURING MODELING P
   [Anonymous], P SIGGRAPHS, DOI DOI 10.1145/218380.218446
   Ben-Shahar O, 2003, J PHYSIOL-PARIS, V97, P191, DOI 10.1016/j.jphysparis.2003.09.004
   BINFORD T, 1971, P IEEE C SYST SCI CY
   DEBONET JS, 1997, P SIGGRAPH 97, P361, DOI DOI 10.1145/258734.258882
   Dischler JM, 1999, IEEE COMPUT GRAPH, V19, P66, DOI 10.1109/38.736470
   DOBBINS A, 1987, NATURE, V329, P438, DOI 10.1038/329438a0
   EDVARDSON H, 2002, COMPUTER METHODS PRO, V72, P89
   Efros A. A., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1033, DOI 10.1109/ICCV.1999.790383
   Efros AA, 2001, COMP GRAPH, P341, DOI 10.1145/383259.383296
   Gardner G. Y., 1984, Computers & Graphics, V18, P11
   Gardner RJ, 2005, J MICROSC-OXFORD, V217, P49, DOI 10.1111/j.0022-2720.2005.01435.x
   Ghazanfarpour D, 1996, COMPUT GRAPH FORUM, V15, pC311, DOI 10.1111/1467-8659.1530311
   GORLA G, 2001, ACM SIGGRAPH 2001 SK
   Gurnsey R, 2001, VISION RES, V41, P745, DOI 10.1016/S0042-6989(00)00307-2
   HAGWOOD C, 1990, NISTIR, V4370, P1
   Hobolth A, 2003, BIOSTATISTICS, V4, P583, DOI 10.1093/biostatistics/4.4.583
   Hobolth A., 2002, IMAGE ANAL STEREOL, V21, P23
   Igarashi T, 1999, COMP GRAPH, P409, DOI 10.1145/311535.311602
   JAGNOW R, 2004, ANN C SERIES ACM, P329
   KEIDING N, 1972, BIOMETRICS, V28, P813, DOI 10.2307/2528765
   KIM S, 2004, APGV 04, P119
   Ledda P, 2005, ACM T GRAPHIC, V24, P640, DOI 10.1145/1073204.1073242
   Lefebvre L, 2000, PROC GRAPH INTERF, P77
   Li Q, 2003, MED PHYS, V30, P2584, DOI 10.1118/1.1605351
   Loncaric S, 1998, PATTERN RECOGN, V31, P983, DOI 10.1016/S0031-2023(97)00122-2
   MARR D, 1978, PROC R SOC SER B-BIO, V200, P269, DOI 10.1098/rspb.1978.0020
   MARTENS WL, 1998, P IEEE VIS 98, P49
   MARTIN WN, 1983, IEEE T PATTERN ANAL, V5, P150, DOI 10.1109/TPAMI.1983.4767367
   Matusik W, 2000, COMP GRAPH, P369, DOI 10.1145/344779.344951
   MCNAMARA A, 2000, P EUR WORKSH BRNO
   MEYER GW, 1986, ACM T GRAPHIC, V5, P30, DOI 10.1145/7529.7920
   Mortenson M. E., 1999, Mathematics for computer graphics applications
   OR YH, 1989, SPATIAL VISION, V4, P131
   Peachey D. R., 1985, Computer Graphics, V19, P279, DOI 10.1145/325165.325246
   Pellacini F, 2000, COMP GRAPH, P55, DOI 10.1145/344779.344812
   Perlin K., 1985, Computer Graphics, V19, P287, DOI 10.1145/325165.325247
   Rushmeier H, 2000, P SOC PHOTO-OPT INS, V3959, P372, DOI 10.1117/12.387174
   SCASSELLATI B, 1994, P SOC PHOTO-OPT INS, V2185, P2, DOI 10.1117/12.171777
   Tong X, 2002, ACM T GRAPHIC, V21, P665, DOI 10.1145/566570.566634
   Turk G, 2001, COMP GRAPH, P347, DOI 10.1145/383259.383297
   Underwood E., 1970, QUANTITATIVE STEREOL
   Van Verth J.M., 2004, ESSENTIAL MATH GAMES
   Watson B, 2001, COMP GRAPH, P213, DOI 10.1145/383259.383283
   WATSON B, 2000, P ACM COMP HUM INT C, P113
   WEI L, 2003, ACM SIGGRAPH 2003 SK
   Wei LY, 2001, COMP GRAPH, P355, DOI 10.1145/383259.383298
   WEISTRAND O, 2001, SCIA01
   Zhang JD, 2003, ACM T GRAPHIC, V22, P295, DOI 10.1145/882262.882266
NR 50
TC 9
Z9 12
U1 0
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD JAN
PY 2008
VL 4
IS 4
AR 24
DI 10.1145/1278760.1278765
PG 27
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 450YH
UT WOS:000266437400005
DA 2024-07-18
ER

PT J
AU Mulot, L
   Howard, T
   Pacchierotti, C
   Marchal, M
AF Mulot, Lendy
   Howard, Thomas
   Pacchierotti, Claudio
   Marchal, Maud
TI Improving the Perception of Mid-air Tactile Shapes with
   Spatio-temporally-modulated Tactile Pointers
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article; Proceedings Paper
CT ACM Symposium on Applied Perception (SAP)
CY AUG 05-06, 2023
CL Los Angeles, CA
SP Assoc Comp Machinery, ACM SIGGRAPH
DE Ultrasound haptics; mid-air haptics; haptic perception
AB Ultrasound mid-air haptic (UMH) devices can remotely render vibrotactile shapes on the skin of unequipped users, e.g., to draw haptic icons or render virtual object shapes. Spatio-temporal modulation (STM), the state-of-the-art UMH shape-rendering method, provides large freedom in shape design and produces the strongest possible stimuli for this technology. Yet, STM shapes are often reported to be blurry, complicating shape identification. Dynamic tactile pointers (DTP) were recently introduced as a technique to overcome this issue. By tracing a contour with an amplitude-modulated focal point, they significantly improve shape identification accuracy over STM, but at the cost of much lower stimulus intensity. Building upon this, we propose spatio-temporally-modulated Tactile Pointers (STP), a novel method for rendering clearer and sharper UMH shapes while at the same time producing strong vibrotactile sensations. We ran two human participant experiments, which show that STP shapes are perceived as significantly stronger than DTP shapes, while shape identification accuracy is significantly improved over STM and on par with that obtained with DTP. Our work has implications for effective shape rendering with UMH and provides insights that could inform future psychophysical investigation into vibrotactile shape perception in UMH.
C1 [Mulot, Lendy; Howard, Thomas; Marchal, Maud] Univ Rennes, CNRS, INSA Rennes, IRISA,Inria, Rennes, France.
   [Pacchierotti, Claudio] Univ Rennes, INRIA, CNRS, IRISA, Rennes, France.
   [Marchal, Maud] IUF, Rennes, France.
C3 Inria; Centre National de la Recherche Scientifique (CNRS); Institut
   National des Sciences Appliquees de Rennes; Universite de Rennes; Centre
   National de la Recherche Scientifique (CNRS); Inria; Universite de
   Rennes
RP Mulot, L (corresponding author), Univ Rennes, CNRS, INSA Rennes, IRISA,Inria, Rennes, France.
EM lendy.mulot@irisa.fr; thomas.howard@irisa.fr;
   claudio.pacchierotti@irisa.fr; maud.marchal@irisa.fr
RI Pacchierotti, Claudio/G-7304-2011; Mulot, Lendy/JZT-7427-2024
OI Pacchierotti, Claudio/0000-0002-8006-9168; Mulot,
   Lendy/0000-0003-2991-5573; Howard, Thomas/0000-0003-4904-375X; Marchal,
   Maud/0000-0002-6080-7178
FU ANR project "MIMESIS"
FX This project has received funding under the ANR project "MIMESIS".
CR BOLANOWSKI SJ, 1988, J ACOUST SOC AM, V84, P1680, DOI 10.1121/1.397184
   Brown Eddie, 2021, Contemporary Ergonomics & Human Factors, V2022, P82
   Carter T., 2013, P 26 ANN ACM S US IN, P505
   Freeman E., 2022, Ultrasound Mid-Air Haptics for Touchless Interfaces, P71
   Frier W, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300351
   Frier W, 2018, LECT NOTES COMPUT SC, V10893, P270, DOI 10.1007/978-3-319-93445-7_24
   Georgiou Orestis, 2022, Ultrasound Mid-air Haptics for Touchless Interfaces, DOI [10.1007/978-3-031-04043-6, DOI 10.1007/978-3-031-04043-6]
   Hajas D, 2020, IEEE T HAPTICS, V13, P806, DOI 10.1109/TOH.2020.2966445
   Hoshi T., 2012, 2012 IEEE Haptics Symposium (HAPTICS), P399, DOI 10.1109/HAPTIC.2012.6183821
   Howard T., 2022, Ultrasound Mid-Air Haptics for TouchlessInterfaces, P147
   Howard T, 2019, 2019 IEEE WORLD HAPTICS CONFERENCE (WHC), P503, DOI [10.1109/whc.2019.8816127, 10.1109/WHC.2019.8816127]
   Inoue S, 2015, 2015 IEEE WORLD HAPTICS CONFERENCE (WHC), P362, DOI 10.1109/WHC.2015.7177739
   Ito M, 2016, LECT NOTES COMPUT SC, V9774, P57, DOI 10.1007/978-3-319-42321-0_6
   Iwamoto T, 2008, LECT NOTES COMPUT SC, V5024, P504, DOI 10.1007/978-3-540-69057-3_64
   Jones LA, 2013, IEEE T HAPTICS, V6, P268, DOI 10.1109/TOH.2012.74
   Kappus Brian, 2018, Journal of the Acoustical Society of America, V143, P1836
   Komandur S, 2009, IEEE ENG MED BIO, P823, DOI 10.1109/IEMBS.2009.5333195
   Korres G, 2016, IEEE ACCESS, V4, P7758, DOI 10.1109/ACCESS.2016.2608835
   Long B, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2661229.2661257
   Martinez J, 2019, 2019 IEEE INTERNATIONAL SYMPOSIUM ON HAPTIC AUDIO-VISUAL ENVIRONMENTS & GAMES (HAVE 2019), DOI [10.1109/CLEOE-EQEC.2019.8873045, 10.1109/have.2019.8921211]
   Mulot L, 2021, ACM SYMPOSIUM ON APPLIED PERCEPTION (SAP 2021), DOI 10.1145/3474451.3476232
   Obrist M., 2013, P SIGCHI C HUMAN FAC, P1659, DOI [10.1145/2470654.2466220, DOI 10.1145/2470654.2466220]
   Peirce J, 2019, BEHAV RES METHODS, V51, P195, DOI 10.3758/s13428-018-01193-y
   Rakkolainen I, 2021, IEEE T HAPTICS, V14, P2, DOI 10.1109/TOH.2020.3018754
   Reardon G, 2023, SCI ADV, V9, DOI 10.1126/sciadv.adf2037
   Romanus T, 2019, ADJUNCT PROCEEDINGS OF THE 2019 IEEE INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY (ISMAR-ADJUNCT 2019), P348, DOI 10.1109/ISMAR-Adjunct.2019.00-14
   Rutten I, 2019, CHI EA '19 EXTENDED ABSTRACTS: EXTENDED ABSTRACTS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290607.3313004
   Sun Chongyang, 2019, Virtual Reality & Intelligent Hardware, V1, P265
   Takahashi R, 2018, LECT NOTES COMPUT SC, V10894, P276, DOI 10.1007/978-3-319-93399-3_25
NR 29
TC 0
Z9 0
U1 3
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD OCT
PY 2023
VL 20
IS 4
SI SI
AR 13
DI 10.1145/3611388
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA Z8RR5
UT WOS:001114697800002
OA Green Published
DA 2024-07-18
ER

PT J
AU Feijóo-García, PG
   Wrenn, C
   Stuart, J
   De Siqueira, AG
   Lok, B
AF Feijoo-Garcia, Pedro Guillermo
   Wrenn, Chase
   Stuart, Jacob
   De Siqueira, Alexandre Gomes
   Lok, Benjamin
TI Participatory Design of Virtual Humans for Mental Health Support Among
   North American Computer Science Students: Voice, Appearance, and the
   Similarity-attraction Effect
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Embodied conversational agents; virtual humans; mental health;
   similarity-attraction; participatory design
ID THERAPIST AFFECT; GENDER MATCH; PATIENT; ACCENT; ETHNICITY; IDENTITY
AB Virtual humans (VHs) have the potential to support mental wellness among college computer science (CS) students. However, designing effective VHs for counseling purposes requires a clear understanding of students' demographics, backgrounds, and expectations. To this end, we conducted two user studies with 216 CS students from a major university in North America. In the first study, we explored how students co-designed VHs to support mental wellness conversations and found that the VHs' demographics, appearance, and voice closely resembled the characteristics of their designers. In the second study, we investigated how the interplay between the VH's appearance and voice impacted the agent's effectiveness in promoting CS students' intentions toward gratitude journaling. Our findings suggest that the active participation of CS students in VH design leads to the creation of agents that closely resemble their designers. Moreover, we found that the interplay between the VH's appearance and voice impacts the agent's effectiveness in promoting CS students' intentions toward mental wellness techniques.
C1 [Feijoo-Garcia, Pedro Guillermo; Wrenn, Chase; Stuart, Jacob; De Siqueira, Alexandre Gomes; Lok, Benjamin] Univ Florida, 432 Newell Dr, Gainesville, FL 32611 USA.
C3 State University System of Florida; University of Florida
RP Feijóo-García, PG (corresponding author), Univ Florida, 432 Newell Dr, Gainesville, FL 32611 USA.
EM pfeijoogarcia@ufl.edu; chasewrenn@ufl.edu; jacobstuart@ufl.edu;
   agomesdesiqueira@ufl.edu; lok@cise.ufl.edu
OI Feijoo Garcia, Pedro Guillermo/0000-0002-3024-1257; Lok,
   Benjamin/0000-0002-1190-3729; Stuart, Jacob/0000-0003-2103-5782; Gomes
   de Siqueira, Alexandre/0000-0002-9213-9602
FU Fulbright Scholar Program; Colombian Ministry of Science and Technology
   - National Science Foundation [1800961, 1800947]
FX This work was sponsored by the Fulbright Scholar Program and the
   Colombian Ministry of Science and Technology. This work was funded by
   the National Science Foundation Awards No. 1800961 and No. 1800947.
CR Ajzen I., 1985, UNDERSTANDING ATTITU, P11, DOI 10.1007/978-3-642-69746-3_2
   Ames DR, 2004, J PERS SOC PSYCHOL, V87, P340, DOI 10.1037/0022-3514.87.3.340
   Antin J., 2012, P SIGCHI C HUM FACT, P2925, DOI [10.1145/2207676.2208699, DOI 10.1145/2207676.2208699]
   BACKNER BL, 1970, J HIGH EDUC, V41, P630, DOI 10.2307/1977665
   Baylor AL, 2004, LECT NOTES COMPUT SC, V3220, P592
   Baylor Amy., 2003, P WORLD C E LEARN CO, P1507
   Bearman M, 2001, MED EDUC, V35, P824, DOI 10.1046/j.1365-2923.2001.00999.x
   Bernier Emily P., 2010, 2010 IEEE 9th International Conference on Development and Learning (ICDL 2010), P286, DOI 10.1109/DEVLRN.2010.5578828
   Bhati KS, 2014, PSYCHOL REP, V115, P565, DOI 10.2466/21.02.PR0.115c23z1
   Blow AJ, 2008, J FEM FAM THER, V20, P66, DOI 10.1080/0895280801907150
   Braa Jorn, 2013, Routledge international handbook of participatory design
   Cabral RR, 2011, J COUNS PSYCHOL, V58, P537, DOI 10.1037/a0025266
   Cargile AC, 2000, J EMPLOYMENT COUNS, V37, P165, DOI 10.1002/j.2161-1920.2000.tb00483.x
   CLANCE PR, 1978, PSYCHOTHER-THEOR RES, V15, P241, DOI 10.1037/h0086006
   Cowan B.R., 2016, P CHI 2016, P2805
   Craig SD, 2017, COMPUT EDUC, V114, P193, DOI 10.1016/j.compedu.2017.07.003
   CSUSM, 2022, Inclusive Language Guidelines: Gender Identity
   Dahlbäck N, 2007, CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1 AND 2, P1553
   Danowitz Andrew, 2018, P COLL NETW ENG COMP
   Eiband M, 2017, PROCEEDINGS OF THE 2017 ACM SIGCHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'17), P4254, DOI 10.1145/3025453.3025636
   Evans TM, 2018, NAT BIOTECHNOL, V36, P282, DOI 10.1038/nbt.4089
   Feijóo-García PG, 2021, PROCEEDINGS OF THE 21ST ACM INTERNATIONAL CONFERENCE ON INTELLIGENT VIRTUAL AGENTS (IVA), P68, DOI 10.1145/3472306.3478367
   Flinchbaugh CL, 2012, J MANAG EDUC, V36, P191, DOI 10.1177/1052562911430062
   Frauenberger C, 2011, CODESIGN, V7, P1, DOI 10.1080/15710882.2011.587013
   GORENSTEIN C, 1995, PSYCHOL REP, V77, P635, DOI 10.2466/pr0.1995.77.2.635
   Guadagno RE, 2011, COMPUT HUM BEHAV, V27, P2380, DOI 10.1016/j.chb.2011.07.017
   Halan S, 2015, LECT NOTES ARTIF INT, V9238, P239, DOI 10.1007/978-3-319-21996-7_24
   Halan S, 2014, IEEE INT CONF ADV LE, P249, DOI 10.1109/ICALT.2014.79
   Hatcher SL, 2005, PSYCHOTHERAPY, V42, P198, DOI 10.1037/0033-3204.42.2.198
   Khooshabeh P, 2017, AI SOC, V32, P9, DOI 10.1007/s00146-014-0568-1
   King BM, 2020, SEX EDUC-SEX SOC LEA, V20, P101, DOI 10.1080/14681811.2019.1606793
   Koda T, 2018, HAI'18: PROCEEDINGS OF THE 6TH INTERNATIONAL CONFERENCE ON HUMAN-AGENT INTERACTION, P138, DOI 10.1145/3284432.3284472
   KUHN S, 1993, COMMUN ACM, V36, P24
   Lacobelli F, 2007, LECT NOTES ARTIF INT, V4722, P57
   Lee E. J., 2000, CHI 00 EXTENDED ABST, P289, DOI [10.1145/633292.633461, DOI 10.1145/633292.633461]
   Lev-Ari S, 2012, DISCOURSE PROCESS, V49, P523, DOI 10.1080/0163853X.2012.698493
   Lev-Ari S, 2010, J EXP SOC PSYCHOL, V46, P1093, DOI 10.1016/j.jesp.2010.05.025
   Ming J, 2021, 23RD INTERNATIONAL ACM SIGACCESS CONFERENCE ON COMPUTERS AND ACCESSIBILITY, ASSETS 2021, DOI 10.1145/3441852.3471216
   Morales AC, 2012, J ADVERTISING, V41, P33, DOI 10.2753/JOA0091-3367410103
   Obremski D, 2021, J MULTIMODAL USER IN, V15, P229, DOI 10.1007/s12193-021-00369-9
   Pratt JA, 2007, INTERACT COMPUT, V19, P512, DOI 10.1016/j.intcom.2007.02.003
   Ragins BR, 1997, ACAD MANAGE REV, V22, P482, DOI 10.2307/259331
   Rosenstein A, 2020, SIGCSE 2020: PROCEEDINGS OF THE 51ST ACM TECHNICAL SYMPOSIUM ON COMPUTER SCIENCE EDUCATION, P30, DOI 10.1145/3328778.3366815
   SIMONS HW, 1970, PSYCHOL BULL, V73, P1, DOI 10.1037/h0028429
   SINGH R, 1974, J RES PERS, V8, P294, DOI 10.1016/0092-6566(74)90040-3
   Passos LMS, 2020, SIGCSE 2020: PROCEEDINGS OF THE 51ST ACM TECHNICAL SYMPOSIUM ON COMPUTER SCIENCE EDUCATION, P316, DOI 10.1145/3328778.3366836
   STEWART MA, 1985, PERS SOC PSYCHOL B, V11, P98, DOI 10.1177/0146167285111009
   Synthesia, 2022, Synthesia: AI video generation platform
   Van Mechelen M, 2021, IDC '21: PROCEEDINGS OF INTERACTION DESIGN AND CHILDREN 2021, P119, DOI 10.1145/3459990.3460701
   Vasalou A., 2021, INT J CHILD COMPUTER, V27, P100241
   Whittaker S, 2021, ACM T HUM-ROBOT INTE, V10, DOI 10.1145/3424153
   Wintersteen MB, 2005, PROF PSYCHOL-RES PR, V36, P400, DOI 10.1037/0735-7028.36.4.400
   Wobbrock JO, 2009, CHI2009: PROCEEDINGS OF THE 27TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1-4, P1083
   Zalake M, 2021, PROCEEDINGS OF THE 21ST ACM INTERNATIONAL CONFERENCE ON INTELLIGENT VIRTUAL AGENTS (IVA), P216, DOI 10.1145/3472306.3478345
   Zane N, 2014, ASIAN AM J PSYCHOL, V5, P66, DOI 10.1037/a0036078
   Zlotnick C, 1998, J CONSULT CLIN PSYCH, V66, P655, DOI 10.1037/0022-006X.66.4.655
NR 56
TC 0
Z9 0
U1 9
U2 9
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD JUL
PY 2023
VL 20
IS 3
AR 11
DI 10.1145/3613961
PG 27
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA U6DF9
UT WOS:001085681100003
DA 2024-07-18
ER

PT J
AU Rajasekaran, SD
   Kang, H
   Cadík, M
   Galin, E
   Guérin, E
   Peytavie, A
   Slavík, P
   Benes, B
AF Rajasekaran, Suren Deepak
   Kang, Hao
   Cadik, Martin
   Galin, Eric
   Guerin, Eric
   Peytavie, Adrien
   Slavik, Pavel
   Benes, Bedrich
TI PTRM: Perceived Terrain Realism Metric
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Procedural modeling; terrains; visual perception; feature transfer;
   neural networks
ID SPATIAL STATISTICS; QUALITY; CLASSIFICATION; SENSITIVITY; BACKGROUNDS;
   IMAGERY
AB Terrains are visually prominent and commonly needed objects in many computer graphics applications. While there are many algorithms for synthetic terrain generation, it is rather difficult to assess the realism of a generated output. This article presents a first step toward the direction of perceptual evaluation for terrain models. We gathered and categorized several classes of real terrains, and we generated synthetic terrain models using computer graphics methods. The terrain geometries were rendered by using the same texturing, lighting, and camera position. Two studies on these image sets were conducted, ranking the terrains perceptually, and showing that the synthetic terrains are perceived as lacking realism compared to the real ones. We provide insight into the features that affect the perceived realism by a quantitative evaluation based on localized geomorphology-based landform features (geomorphons) that categorize terrain structures such as valleys, ridges, hollows, and so forth. We show that the presence or absence of certain features has a significant perceptual effect. The importance and presence of the terrain features were confirmed by using a generative deep neural network that transferred the features between the geometric models of the real terrains and the synthetic ones. The feature transfer was followed by another perceptual experiment that further showed their importance and effect on perceived realism. We then introduce Perceived Terrain Realism Metrics (PTRM), which estimates human-perceived realism of a terrain represented as a digital elevation map by relating the distribution of terrain features with their perceived realism. This metric can be used on a synthetic terrain, and it will output an estimated level of perceived realism. We validated the proposed metrics on real and synthetic data and compared them to the perceptual studies.
C1 [Rajasekaran, Suren Deepak; Kang, Hao; Benes, Bedrich] Purdue Univ, Dept Comp Sci, 305 N Univ St, W Lafayette, IN 47907 USA.
   [Cadik, Martin] Brno Univ Technol, FIT, Bozetechova 2, Brno 61200, Czech Republic.
   [Cadik, Martin; Slavik, Pavel] Czech Tech Univ, FEL, Prague, Czech Republic.
   [Galin, Eric; Guerin, Eric; Peytavie, Adrien] Univ Lyon, Lyon, France.
   [Galin, Eric; Guerin, Eric; Peytavie, Adrien] Univ Claude Bernard Lyon 1, Lab LIRIS CNRS, Villeurbanne, France.
   [Slavik, Pavel] Karlovo Namesti 13,E-321, Prague 2, Czech Republic.
C3 Purdue University System; Purdue University; Brno University of
   Technology; Czech Technical University Prague; Universite Claude Bernard
   Lyon 1
RP Rajasekaran, SD (corresponding author), Purdue Univ, Dept Comp Sci, 305 N Univ St, W Lafayette, IN 47907 USA.
EM surendeepak.rajasekaran@gmail.com; kayheseri@gmail.com;
   cadik@fit.vutbr.cz; eric.galin@univ-lyon1.fr; eric.guerin@insa-lyon.fr;
   adrien.peytavie@liris.cnrs.fr; slavik@fel.cvut.cz; bbenes@purdue.edu
RI Benes, Bedrich/A-8150-2016; Cadik, Martin/O-4824-2014
OI Benes, Bedrich/0000-0002-5293-2112; Cadik, Martin/0000-0001-7058-9912;
   Rajasekaran, Suren Deepak/0000-0003-4137-7288
FU National Science Foundation [10001387, HDWANR-16-CE33-0001];
   Deep-Learning Approach to Topographical Image Analysis [LTAIZ19004];
   Ministry of Education, Youth and Sports of the Czech Republic
   [SMSM2019LTAIZ]; Research Center for Informatics
   [CZ.02.1.01/0.0/0.0/16_019/0000765]
FX This research was funded in part by National Science Foundation grants
   #10001387, Functional Proceduralization of 3D Geometric Models, and
   project HDWANR-16-CE33-0001. This work was further supported by project
   no. LTAIZ19004 Deep-Learning Approach to Topographical Image Analysis;
   by the Ministry of Education, Youth and Sports of the Czech Republic
   within the activity INTER-EXCELENCE (LT), subactivity INTER-ACTION
   (LTA), ID: SMSM2019LTAIZ; and by Research Center for Informatics No.
   CZ.02.1.01/0.0/0.0/16_019/0000765. Authors' addresses: S. D.
   Rajasekaran, H. Kang, and B. Benes, Department of Computer Science
   Purdue University, 305 N University St., West Lafayette, IN, 47907-2021,
   USA;
   emails:surendeepak.rajasekaran@gmail.com,kayheseri@gmail.com,bbenes@purd
   ue.edu;M.Cadik, FIT, Brno University of Technology, Bozetechova 2, 612
   00 Brno, Czech Republic; email: cadik@fit.vutbr.cz;E.Galin,E.Guerin, and
   A. Peytavie, Laboratoire LIRIS -CNRS, Universite Claude Bernard Lyon 1,
   France;
   emails:eric.galin@univ-lyon1.fr,eric.guerin@insa-lyon.fr,adrien.peytavie
   @liris.cnrs.fr;P.Slavik, Praha 2, Karlovo namesti 13, E-321, Czech
   Republic; email: slavik@fel.cvut.cz.
CR Anh NH, 2007, GRAPHITE 2007: 5TH INTERNATIONAL CONFERENCE ON COMPUTER GRAPHICS AND INTERACTIVE TECHNIQUES IN AUSTRALASIA AND SOUTHERN ASIA, PROCEEDINGS, P257
   [Anonymous], 2007, PROC 3 EUR C COMPUTA
   [Anonymous], 2008, Proceedings of the 2008 ACM SIGGRAPH/Eurographics Symposium on Computer Animation, SCA '08
   Argudo O, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356535
   Aydin TO, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866187
   Bartz D., 2008, Eurographics (stars), P59
   Benes B, 2006, COMPUT ANIMAT VIRT W, V17, P99, DOI 10.1002/cav.77
   Benes B, 2002, WSCG'2002, VOLS I AND II, CONFERENCE PROCEEDINGS, P79
   BERGEN SD, 1995, J ENVIRON PSYCHOL, V15, P135, DOI 10.1016/0272-4944(95)90021-7
   Bertilone DC, 1997, APPL OPTICS, V36, P9167, DOI 10.1364/AO.36.009167
   Bertilone DC, 1997, APPL OPTICS, V36, P9177, DOI 10.1364/AO.36.009177
   Bojrab M, 2013, ACM T APPL PERCEPT, V10, DOI 10.1145/2422105.2422107
   Cordonnier G, 2018, IEEE T VIS COMPUT GR, V24, P1756, DOI 10.1109/TVCG.2017.2689022
   Cordonnier G, 2016, COMPUT GRAPH FORUM, V35, P165, DOI 10.1111/cgf.12820
   Daniel TC, 2001, LANDSCAPE URBAN PLAN, V54, P267, DOI 10.1016/S0169-2046(01)00141-4
   Dragut L, 2006, GEOMORPHOLOGY, V81, P330, DOI 10.1016/j.geomorph.2006.04.013
   Farr TG., 2000, EOS T AM GEOPHYS UN, V81, P583585, DOI [DOI 10.1029/EO081I048P00583, 10.1029/EO081i048p00583]
   Ferwerda JA, 2003, PROC SPIE, V5007, P290, DOI 10.1117/12.473899
   FOURNIER A, 1982, COMMUN ACM, V25, P371, DOI 10.1145/358523.358553
   Frintrop S, 2015, PROC CVPR IEEE, P82, DOI 10.1109/CVPR.2015.7298603
   Gain J., 2009, P 2009 S INT 3D GRAP, V1, P31, DOI [10.1145/1507149.1507155, DOI 10.1145/1507149.1507155]
   Galin E, 2019, COMPUT GRAPH FORUM, V38, P553, DOI 10.1111/cgf.13657
   Gatys L., 2016, Journal of Vision, V16, P326, DOI DOI 10.1167/16.12.326
   Gooch A., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P447, DOI 10.1145/280814.280950
   Guérin E, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130804
   Guérin E, 2016, COMPUT GRAPH FORUM, V35, P177, DOI 10.1111/cgf.12821
   Guo J., 2015, Proceedings of the ACM SIGGRAPH Symposium on Applied Perception, P91
   Guo JJ, 2017, ACM T APPL PERCEPT, V14, DOI 10.1145/2996296
   Hagerhall CM, 2004, J ENVIRON PSYCHOL, V24, P247, DOI 10.1016/j.jenvp.2003.12.004
   Herzog R, 2012, COMPUT GRAPH FORUM, V31, P545, DOI 10.1111/j.1467-8659.2012.03055.x
   Huggett Richard, 2016, FUNDAMENTALS GEOMORP
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jasiewicz J, 2013, GEOMORPHOLOGY, V182, P147, DOI 10.1016/j.geomorph.2012.11.005
   Kelley A. D., 1988, Computer Graphics, V22, P263, DOI 10.1145/378456.378519
   Kristof P, 2009, COMPUT GRAPH FORUM, V28, P219, DOI 10.1111/j.1467-8659.2009.01361.x
   Lange E, 2001, LANDSCAPE URBAN PLAN, V54, P163, DOI 10.1016/S0169-2046(01)00134-7
   Lavoué G, 2016, IEEE T VIS COMPUT GR, V22, P1987, DOI 10.1109/TVCG.2015.2480079
   Liu J, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0130335
   Mandelbrot BB., 1988, The science of fractal images, V1st, P243
   Mantiuk R., 2006, ACM Transactions on Applied Perception, V3, P286, DOI DOI 10.1145/1166087.1166095
   Mantiuk R, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964935
   Miller G. S. P., 1986, Computer Graphics, V20, P39, DOI 10.1145/15886.15890
   Milne JA, 1997, INT J GEOGR INF SCI, V11, P499, DOI 10.1080/136588197242275
   Musgrave F. K., 1989, Computer Graphics, V23, P41, DOI 10.1145/74334.74337
   Nader G, 2016, COMPUT GRAPH FORUM, V35, P497, DOI 10.1111/cgf.13046
   Neidhold B., 2005, Natural Phenomena, P25
   Neteler M., 2013, OPEN SOURCE GIS GRAS, V689
   O'Sullivan C., 2004, Eurographics 2004 - stars, DOI DOI 10.2312/EGST.20041029
   Odena A., 2016, DISTILL, V1, P3, DOI [10.23915/distill.00003., DOI 10.23915/DISTILL, 10.23915/distill.00003, DOI 10.23915/DISTILL.00003]
   Ondrej J, 2016, ACM T APPL PERCEPT, V13, DOI 10.1145/2948066
   Palmer J., 2003, TRENDS LANDSCAPE MOD
   Perez-Ortiz Maria, 2017, arXiv preprint arXiv:1712.03686
   Perlin K., 1985, Computer Graphics, V19, P287, DOI 10.1145/325165.325247
   Polasek T, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3478513.3480519
   Rajasekaran Suren Deepak, 2019, THESIS PURDUE U GRAD
   Reddy M, 2001, IEEE COMPUT GRAPH, V21, P68, DOI 10.1109/38.946633
   Reinhard E., 2004, Proceedings of the 1st Symposium on Applied perception in graphics and visualization, APGV '04, P99, DOI [10.1145/1012551.1012568, DOI 10.1145/1012551.1012568]
   Reitsma PSA, 2003, ACM T GRAPHIC, V22, P537, DOI 10.1145/882262.882304
   Riche N, 2013, IEEE I CONF COMP VIS, P1153, DOI 10.1109/ICCV.2013.147
   Rushmeier H, 2000, P SOC PHOTO-OPT INS, V3959, P372, DOI 10.1117/12.387174
   Schwarz Michael, 2009, PROC APGV, P93, DOI [10.1145/1620993.1621012, DOI 10.1145/1620993.1621012]
   Scott JJ, 2021, COMPUT GRAPH-UK, V99, P43, DOI 10.1016/j.cag.2021.06.012
   Smith K, 2008, COMPUT GRAPH FORUM, V27, P193, DOI 10.1111/j.1467-8659.2008.01116.x
   Travers RM., 1984, Human information processing
   Tremblet A., 2016, Journal of Alpine Research, P104, DOI DOI 10.4000/RGA.3395
   Tveit MariSundli., 2012, ENV PSYCHOL INTRO, P37
   Um K, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073633
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wang H, 2017, IEEE T VIS COMPUT GR, V23, P1454, DOI 10.1109/TVCG.2016.2642963
   Wang Tao, 2017, 2017 IEEE 5th International Symposium on Electromagnetic Compatibility (EMC-Beijing). Proceedings, P1, DOI 10.1109/EMC-B.2017.8260394
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Weier M, 2017, COMPUT GRAPH FORUM, V36, P611, DOI 10.1111/cgf.13150
   Wen-Hung Liao, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P1003, DOI 10.1109/ICPR.2010.251
   Winkler S, 2008, IEEE T BROADCAST, V54, P660, DOI 10.1109/TBC.2008.2000733
   Wolski K, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3196493
   Wu JL, 2013, GRAPH MODELS, V75, P255, DOI 10.1016/j.gmod.2013.05.002
   Ye P, 2014, PROC CVPR IEEE, P4241, DOI 10.1109/CVPR.2014.540
   Zhou H, 2007, IEEE T VIS COMPUT GR, V13, P834, DOI 10.1109/TVCG.2007.1027
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 79
TC 3
Z9 3
U1 1
U2 6
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD APR
PY 2022
VL 19
IS 2
AR 6
DI 10.1145/3514244
PG 22
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 3A7CY
UT WOS:000827414800002
OA Bronze
DA 2024-07-18
ER

PT J
AU Thorpe, A
   Nesbitt, K
   Eidels, A
AF Thorpe, Alexander
   Nesbitt, Keith
   Eidels, Ami
TI A Systematic Review of Empirical Measures of Workload Capacity
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Review
DE Workload capacity; mental workload; perceptual load; multitasking;
   working memory; HCI; usability
ID SITUATION AWARENESS; COGNITIVE LOAD
AB The usability of the human-machine interface is dependent on the quality of its design and testing. Defining clear criteria that the interface must meet can assist the implementation and evaluation process. These criteria may be based on performance, the quality of users' experience, error prevention, or the broad utility of the interface. In this article, we motivate the use for workload capacity as an empirical measure of usability. We first describe generic and specific uses for workload measures in terms of adaptive interfaces. We then carry out a systematic review of how workload capacity has been empirically measured, based on 172 relevant literature sources from psychology, neuroscience, engineering, and computer science. We then analyse and report on how workload capacity and related constructs, such as perceptual load, attention, and working memory have been defined and measured in these sources. We discuss similarities and differences between constructs and identify opportunities for integrating real-time workload capacity measures into dynamic interfaces.
C1 [Thorpe, Alexander; Nesbitt, Keith; Eidels, Ami] Univ Newcastle, Univ Dr, Callaghan, NSW 2308, Australia.
C3 University of Newcastle
RP Thorpe, A (corresponding author), Univ Newcastle, Univ Dr, Callaghan, NSW 2308, Australia.
EM Alexander.Thorpe@newcastle.edu.au; Keith.Nesbitt@newcastle.edu.au;
   Ami.Eidels@newcastle.edu.au
RI Nesbitt, Keith V/H-9760-2012
CR Arthur JJ, 2013, PROC SPIE, V8737, DOI 10.1117/12.2016386
   BENYON D, 1988, COMPUT J, V31, P465, DOI 10.1093/comjnl/31.5.465
   Borghini G, 2014, NEUROSCI BIOBEHAV R, V44, P58, DOI 10.1016/j.neubiorev.2012.10.003
   ENDSLEY MR, 1995, HUM FACTORS, V37, P32, DOI 10.1518/001872095779049543
   ERIKSEN BA, 1974, PERCEPT PSYCHOPHYS, V16, P143, DOI 10.3758/BF03203267
   Haapalainen E, 2010, UBICOMP 2010: PROCEEDINGS OF THE 2010 ACM CONFERENCE ON UBIQUITOUS COMPUTING, P301
   Harding B, 2016, QUANT METH PSYCHOL, V12, P39, DOI 10.20982/tqmp.12.1.p039
   Hawkins F.H., 1993, Human Factors in flight
   International Organization for Standardization, 2016, ISO 17488:2016
   Johnston Neil, 2001, AVIATION PSYCHOL PRA
   Kahneman D., 1973, Attention and effort
   Kasper RW, 2014, J COGNITIVE NEUROSCI, V26, P476, DOI 10.1162/jocn_a_00480
   Keightley A., 2004, HUMAN FACTORS STUDY
   Kim S, 2016, MULTIMED TOOLS APPL, V75, P9587, DOI 10.1007/s11042-015-2712-4
   Klemen J, 2010, J COGNITIVE NEUROSCI, V22, P437, DOI 10.1162/jocn.2009.21204
   Lancaster JA, 2008, HUM FACTORS, V50, P183, DOI 10.1518/001872008X250737
   Landsberg CR, 2010, ADAPTIVE TRAINING CO
   LAVIE N, 1995, J EXP PSYCHOL HUMAN, V21, P451, DOI 10.1037/0096-1523.21.3.451
   Lavie N, 2004, J EXP PSYCHOL GEN, V133, P339, DOI 10.1037/0096-3445.133.3.339
   Little DR, 2017, SYSTEMS FACTORIAL TECHNOLOGY: A THEORY DRIVEN METHODOLOGY FOR THE IDENTIFICATION OF PERCEPTUAL AND COGNITIVE MECHANISMS, P1
   Liu CC, 2009, INT J HUM-COMPUT INT, V25, P506, DOI 10.1080/10447310902963944
   Lyu YQ, 2018, ACM T APPL PERCEPT, V15, DOI 10.1145/3185665
   Mayhew Deborah J., 1999, USABILITY ENG LIFECY, P122
   Miller T, 2015, PSYCHOPHYSIOLOGY, V52, P1140, DOI 10.1111/psyp.12446
   NASA, 1986, Nasa Task Load Index (TLX) v. 1.0 Manual
   Norman D., 1988, PSYCHOL EVERYDAY THI
   Paas F, 2003, EDUC PSYCHOL-US, V38, P63, DOI 10.1207/S15326985EP3801_8
   Parasuraman R, 2008, J COGN ENG DECIS MAK, V2, P140, DOI 10.1518/155534308X284417
   ROUSE WB, 1988, HUM FACTORS, V30, P431, DOI 10.1177/001872088803000405
   Scerbo M.W., 1996, HUM FAC TRANSP, P37
   Sivils P, 2017, C HUM SYST INTERACT, P277, DOI 10.1109/HSI.2017.8005045
   Strayer DL, 2015, HUM FACTORS, V57, P1300, DOI 10.1177/0018720815575149
   SWELLER J, 1988, COGNITIVE SCI, V12, P257, DOI 10.1207/s15516709cog1202_4
   Townsend JT, 2011, PSYCHON B REV, V18, P659, DOI 10.3758/s13423-011-0106-9
   Townsend JT, 1995, J MATH PSYCHOL, V39, P321, DOI 10.1006/jmps.1995.1033
   Wiener E.L., 1988, Human Factors in Aviation
   Wikman AS, 1998, ERGONOMICS, V41, P358, DOI 10.1080/001401398187080
   Yu JC, 2014, FRONT PSYCHOL, V5, DOI 10.3389/fpsyg.2014.01465
NR 38
TC 2
Z9 3
U1 1
U2 47
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD NOV
PY 2020
VL 17
IS 3
AR 12
DI 10.1145/3422869
PG 23
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA PA3OQ
UT WOS:000595548000004
DA 2024-07-18
ER

PT J
AU Bhargava, A
   Martin, J
   Babu, SV
AF Bhargava, Ayush
   Martin, James
   Babu, Sabarish, V
TI Comparative Evaluation of User Perceived Quality Assessment of Design
   Strategies for HTTP-based Adaptive Streaming
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Video quality; empirical evaluation; dynamic adaptive streaming over
   HTTP; adaptation algorithm; quality of experience; streaming media
AB HTTP-based Adaptive Streaming (HAS) is the dominant Internet video streaming application. One specific HAS approach, Dynamic Adaptive Streaming over HTTP (DASH), is of particular interest, as it is a widely deployed, standardized implementation. Prior academic research has focused on networking and protocol issues, and has contributed an accepted understanding of the performance and possible performance issues in large deployment scenarios. Our work extends the current understanding of HAS by focusing directly on the impacts of choice of the video quality adaptation algorithm on end-user perceived quality. In congested network scenarios, the details of the adaptation algorithm determine the amount of bandwidth consumed by the application as well as the quality of the rendered video stream. HAS will lead to user-perceived changes in video quality due to intentional changes in quality video segments, or unintentional perceived quality impairments caused by video decoder artifacts such as pixelation, stutters, or short or long stalls in the rendered video when the playback buffer becomes empty. The HAS adaptation algorithm attempts to find the optimal solution to mitigate the conflict between avoiding buffer stalls and maximizing video quality. In this article, we present results from a user study that was designed to provide insights into "best practice guidelines" for a HAS adaptation algorithm. Our findings suggest that a buffer-based strategy might provide a better experience under higher network impairment conditions. For the two network scenarios considered, the buffer-based strategy is effective in avoiding stalls but does so at the cost of reduced video quality. However, the buffer-based strategy does yield a lower number of quality switches as a result of infrequent bitrate adaptations. Participants in buffer-based strategy do notice the drop in video quality causing a decrease in perceived QoE, but the perceived levels of video quality, viewer frustration, and opinions of video clarity and distortion are significantly worse due to artifacts such as stalls in capacity-based strategy. The capacity-based strategy tries to provide the highest video quality possible but produces many more artifacts during playback. The results suggest that player video quality has more of an impact on perceived quality when stalls are infrequent. The study methodology also contributes a unique method for gathering continuous quantitative subjective measure of user perceived quality using a Wii remote.
C1 [Bhargava, Ayush; Martin, James; Babu, Sabarish, V] Clemson Univ, Sch Comp, McAdams Hall, Clemson, SC 29631 USA.
C3 Clemson University
RP Bhargava, A (corresponding author), Clemson Univ, Sch Comp, McAdams Hall, Clemson, SC 29631 USA.
EM ayushb@g.clemson.edu; jmarty@clemson.edu; sbabu@clemson.edu
RI Bhargava, Ayush/AAJ-2387-2021
OI Bhargava, Ayush/0000-0001-8957-1317
FU CableLabs; Comcast; Intel
FX This work has been supported in part by CableLabs, Comcast, and Intel.
CR Agboma F, 2007, MOB INF SYST, V3, P153, DOI 10.1155/2007/719840
   Alberti C, 2013, INT WORK QUAL MULTIM, P58, DOI 10.1109/QoMEX.2013.6603211
   Alzahrani Ibrahim Rizqallah, 2018, REAL TIME IMAGE VIDE
   [Anonymous], 2017, INGEMMET B C
   [Anonymous], 2011, Proc. second annu. acm conf. multimed. syst.-mmsys'11, DOI DOI 10.1145/1943552.1943574
   [Anonymous], 2006, P WORKSH VID PROC QU
   [Anonymous], 2012, Proceedings of the 22nd international workshop on Network and Operating System Support for Digital Audio and Video
   [Anonymous], 2013, P IEEE 20 INT PACK V
   [Anonymous], IEEE INT C COMM ICC
   Balachandran A, 2012, PROCEEDINGS OF THE 11TH ACM WORKSHOP ON HOT TOPICS IN NETWORKS (HOTNETS-XI), P97
   Bentaleb A, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P573, DOI 10.1145/3240508.3240589
   Bentaleb A, 2019, IEEE COMMUN SURV TUT, V21, P562, DOI 10.1109/COMST.2018.2862938
   Bhat D., 2017, P 27 WORKSH NETW OP, P13, DOI [10.1145/3083165.3083175, DOI 10.1145/3083165.3083175]
   Chandrasekaran Balakrishnan, 2018, ARXIV180910270
   Cisco Visual Networking Index, 2016, CISCO GLOBAL CLOUD I, P1
   Cranley N, 2006, INT J HUM-COMPUT ST, V64, P637, DOI 10.1016/j.ijhcs.2005.12.002
   De Cicco L, 2014, IEEE ACM T NETWORK, V22, P526, DOI 10.1109/TNET.2013.2253797
   De Pessemier T, 2013, IEEE T BROADCAST, V59, P47, DOI 10.1109/TBC.2012.2220231
   Degrande N, 2008, IEEE COMMUN MAG, V46, P94, DOI 10.1109/MCOM.2008.4473090
   Dobrian F, 2011, ACM SIGCOMM COMP COM, V41, P362, DOI 10.1145/2043164.2018478
   Dubin R, 2019, MULTIMED TOOLS APPL, V78, P11203, DOI 10.1007/s11042-018-6615-z
   ETSI, 2012, 26247 ETSI 3GPP TS
   Houdaille R., 2012, P 3 MULTIMEDIA SYSTE, P1
   Huang T.Y., 2012, P 2012 ACM C INT MEA, P225, DOI 10.1145/2398776.2398800
   Huang TY, 2014, ACM SIGCOMM COMP COM, V44, P187, DOI 10.1145/2740070.2626296
   Huysegems Raf, 2012, P IEEE 20 INT WORKSH, P15
   Jackson France, 2015, P INT C HUM COMP INT
   Jiang J, 2012, 2012 IEEE/WIC/ACM INTERNATIONAL CONFERENCE ON WEB INTELLIGENCE AND INTELLIGENT AGENT TECHNOLOGY (WI-IAT 2012), VOL 2, P90, DOI 10.1109/WI-IAT.2012.40
   Kakhki AM, 2017, PROCEEDINGS OF THE 2017 INTERNET MEASUREMENT CONFERENCE (IMC'17), P290, DOI 10.1145/3131365.3131368
   Langley A, 2017, SIGCOMM '17: PROCEEDINGS OF THE 2017 CONFERENCE OF THE ACM SPECIAL INTEREST GROUP ON DATA COMMUNICATION, P183, DOI 10.1145/3098822.3098842
   Li Z., 2014, Proceedings of the 5th ACM Multimedia Systems Conference, MMSys '14, P248
   Liu CH, 2012, SIGNAL PROCESS-IMAGE, V27, P288, DOI 10.1016/j.image.2011.10.001
   Muller C., 2012, 4th ACM Workshop on Mobile Video (MoVID), P37, DOI DOI 10.1145/2151677.2151686
   Ni P., 2011, P 19 ACM INT C MULT, P463, DOI DOI 10.1145/2072298.2072359
   P ITU-T RECOMMENDATION, 1999, SUBJ VID QUAL ASS ME
   Rosskopf A., 2014, IEEE Int. Elec. Drives Prod. Conf. (EDPC), P1, DOI [10.1007/s13398-014-01737.2, DOI 10.1016/J.NEUROPSYCHOLOGIA.2015.01.019]
   Sandvine Corporation, 2018, 2018 GLOB INT PHEN
   Sani Y, 2017, IEEE COMMUN SURV TUT, V19, P2985, DOI 10.1109/COMST.2017.2725241
   Seufert M, 2015, IEEE COMMUN SURV TUT, V17, P469, DOI 10.1109/COMST.2014.2360940
   Shahid M, 2014, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2014-40
   Stockhammer T., 2011, INFORM TECHNOLOGY 6
   Tavakoli Samira, 2014, P IS T SPIE EL IM C
   Telecommunication Standardization Sector, 1998, SUBJ AUD QUAL ASS ME
   Timmerer Christian, 2014, IEEE ACM T NETWORK, V22, P526
NR 44
TC 3
Z9 3
U1 0
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD SEP
PY 2019
VL 16
IS 4
AR 22
DI 10.1145/3345313
PG 20
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA JE2HU
UT WOS:000490516700004
DA 2024-07-18
ER

PT J
AU Kuzovkin, D
   Pouli, T
   Le Meur, O
   Cozot, R
   Kervec, J
   Bouatouch, K
AF Kuzovkin, Dmitry
   Pouli, Tania
   Le Meur, Olivier
   Cozot, Remi
   Kervec, Jonathan
   Bouatouch, Kadi
TI Context in Photo Albums: Understanding and Modeling User Behavior in
   Clustering and Selection
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Photo albums clustering; photo collection organization; image
   assessment; photo selection
AB Recent progress in digital photography and storage availability has drastically changed our approach to photo creation. While in the era of film cameras, careful forethought would usually precede the capture of a photo; nowadays, a large number of pictures can be taken with little effort. One of the consequences is the creation of numerous photos depicting the same moment in slightly different ways, which makes the process of organizing photos laborious for the photographer. Nevertheless, photo collection organization is important both for exploring photo albums and for simplifying the ultimate task of selecting the best photos. In this work, we conduct a user study to explore how users tend to organize or cluster similar photos in albums, to what extent different users agree in their clustering decisions, and to investigate how the clustering-defined photo context affects the subsequent photo-selection process. We also propose an automatic hierarchical clustering solution for modeling user clustering decisions. To demonstrate the usefulness of our approach, we apply it to the task of automatic photo evaluation within photo albums and propose a clustering-based context adaptation.
C1 [Kuzovkin, Dmitry] Univ Rennes, IRISA, Technicolor, Rennes, France.
   [Pouli, Tania; Kervec, Jonathan] Technicolor, 975 Ave Champs Blancs, F-35576 Cesson Sevigne, France.
   [Le Meur, Olivier; Cozot, Remi; Bouatouch, Kadi] Univ Rennes, CNRS, IRISA, Rennes, France.
   [Kuzovkin, Dmitry; Le Meur, Olivier; Cozot, Remi; Bouatouch, Kadi] IRISA, 263 Ave Gen Leclerc, F-35042 Rennes, France.
C3 Technicolor SA; Universite de Rennes; Technicolor SA; Centre National de
   la Recherche Scientifique (CNRS); Universite de Rennes; Universite de
   Rennes
RP Kuzovkin, D (corresponding author), Univ Rennes, IRISA, Technicolor, Rennes, France.; Kuzovkin, D (corresponding author), IRISA, 263 Ave Gen Leclerc, F-35042 Rennes, France.
EM kuzovkin.dmitry@gmail.com; tania.pouli@technicolor.com;
   olivier.le_meur@irisa.fr; remi.cozot@irisa.fr;
   jonathan.kervec@technicolor.com; kadi.bouatouch@irisa.fr
OI Pouli, Tania/0000-0002-5941-086X
CR [Anonymous], P S COMP AESTH
   [Anonymous], P 15 ACM INT S ADV G
   [Anonymous], C HUM FACT COMP
   [Anonymous], P ACM INT C MULT RET
   [Anonymous], P IEEE INT C MULT EX
   [Anonymous], P 26 IEEE C COMP VIS
   Bossard L, 2013, IEEE I CONF COMP VIS, P1193, DOI 10.1109/ICCV.2013.151
   Ceroni A, 2015, ICMR'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P187, DOI 10.1145/2671188.2749372
   Chang HW, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925908
   Chu Wei-Ta., 2008, P 16 ACM INT C MULTI, P829, DOI DOI 10.1145/1459359.1459498
   Cooper Matthew., 2005, ACM T MULTIM COMPUT, V1, P269, DOI [DOI 10.1145/1083314.1083317, 10.1145/1083314.1083317]
   Datta R, 2006, LECT NOTES COMPUT SC, V3953, P288, DOI 10.1007/11744078_23
   Dueck D, 2007, IEEE I CONF COMP VIS, P198
   Frey BJ, 2007, SCIENCE, V315, P972, DOI 10.1126/science.1136800
   Gomi A, 2008, IEEE INT CONF INF VI, P82, DOI 10.1109/IV.2008.8
   Gozali JP, 2012, IEEE INT CONF MULTI, P25, DOI 10.1109/ICMEW.2012.12
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   HUBERT L, 1985, J CLASSIF, V2, P193, DOI 10.1007/BF01908075
   Jin B, 2016, IEEE IMAGE PROC, P2291, DOI 10.1109/ICIP.2016.7532767
   Kang L, 2014, PROC CVPR IEEE, P1733, DOI 10.1109/CVPR.2014.224
   Ke Y., 2006, P IEEE COMP SOC C CO, V1, P419, DOI DOI 10.1109/CVPR.2006.303
   Kirk D. S., 2006, Conference on Human Factors in Computing Systems. CHI2006, P761
   Kong S, 2016, LECT NOTES COMPUT SC, V9905, P662, DOI 10.1007/978-3-319-46448-0_40
   Kreyszig Erwin, 2007, Advanced engineering mathematics, V9th
   Krishnamachari S, 1999, IEEE SYMP COMP COMMU, P301, DOI 10.1109/ISCC.1999.780837
   Kustanowitz J, 2005, ACM-IEEE J CONF DIG, P188, DOI 10.1145/1065385.1065431
   Levenberg K., 1944, Quarterly of Applied Mathematics, V2, P164, DOI [10.1090/QAM/10666, DOI 10.1090/QAM/10666]
   Loui AC, 2003, IEEE T MULTIMEDIA, V5, P390, DOI 10.1109/TMM.2003.814723
   Lowe D. G., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1150, DOI 10.1109/ICCV.1999.790410
   Lu X, 2015, IEEE I CONF COMP VIS, P990, DOI 10.1109/ICCV.2015.119
   Lu X, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P457, DOI 10.1145/2647868.2654927
   Lu Z., 2008, P 23 NAT C ART INT A, P665
   Luo YW, 2008, LECT NOTES COMPUT SC, V5304, P386
   Marchesotti L, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.7
   Marchesotti L, 2011, IEEE I CONF COMP VIS, P1784, DOI 10.1109/ICCV.2011.6126444
   Mavridaki E, 2015, IEEE IMAGE PROC, P887, DOI 10.1109/ICIP.2015.7350927
   Murray N, 2012, PROC CVPR IEEE, P2408, DOI 10.1109/CVPR.2012.6247954
   Park K, 2017, IEEE WINT CONF APPL, P1206, DOI 10.1109/WACV.2017.139
   Perronnin F, 2010, PROC CVPR IEEE, P3384, DOI 10.1109/CVPR.2010.5540009
   Platt JC, 2003, ICICS-PCM 2003, VOLS 1-3, PROCEEDINGS, P6
   Radenovic F, 2019, IEEE T PATTERN ANAL, V41, P1655, DOI 10.1109/TPAMI.2018.2846566
   Redi Miriam, 2015, 2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG), P1, DOI 10.1109/FG.2015.7163086
   Rodden K., 2003, P SIGCHI C HUMAN FAC, P409, DOI DOI 10.1145/642611.642682
   Talebi H, 2018, IEEE T IMAGE PROCESS, V27, P3998, DOI 10.1109/TIP.2018.2831899
   Tang XO, 2013, IEEE T MULTIMEDIA, V15, P1930, DOI 10.1109/TMM.2013.2269899
   Thomee B, 2016, COMMUN ACM, V59, P64, DOI 10.1145/2812802
   Walber T, 2014, 32ND ANNUAL ACM CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2014), P2065, DOI 10.1145/2556288.2557025
   Wang YF, 2016, PROC CVPR IEEE, P4810, DOI 10.1109/CVPR.2016.520
   WARD JH, 1963, J AM STAT ASSOC, V58, P236, DOI 10.2307/2282967
   Yeh C.-H., 2010, Proceedings of the international conference on Multimedia - MM'10, page, P211
NR 50
TC 6
Z9 6
U1 1
U2 7
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD AUG
PY 2019
VL 16
IS 2
AR 11
DI 10.1145/3333612
PG 20
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA JC8UL
UT WOS:000489551500005
DA 2024-07-18
ER

PT J
AU Marentakis, G
   Griffiths, C
   McAdams, S
AF Marentakis, Georgios
   Griffiths, Cathryn
   McAdams, Stephen
TI Top-Down Influences in the Detection of Spatial Displacement in a
   Musical Scene
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Spatial attention; auditory perception; 3d audio
ID INTERAURAL TIME DIFFERENCES; INFORMATIONAL MASKING;
   SPEECH-INTELLIGIBILITY; BINAURAL INTERFERENCE; PERCEPTUAL SEPARATION;
   COCKTAIL PARTY; ATTENTION; RELEASE; IDENTIFICATION; LOCALIZATION
AB We investigated the detection of sound displacement in a four-voice musical piece under conditions that manipulated the attentional setting (selective or divided attention), the sound source numerosity, the spatial dispersion of the voices, and the tonal complexity of the piece. Detection was easiest when each voice was played in isolation and performance deteriorated when source numerosity increased and uncertainty with respect to the voice in which displacement would occur was introduced. Restricting the area occupied by the voices improved performance in agreement with the auditory spotlight hypothesis as did reducing the tonal complexity of the piece. Performance under increased numerosity conditions depended on the voice in which displacement occurred. The results highlight the importance of top-down processes in the context of the detection of spatial displacement in a musical scene.
C1 [Marentakis, Georgios] Univ Mus & Performing Arts Graz, Inst Elect Mus & Acoust, Inffeldgasse 10-3, A-8010 Graz, Austria.
   [Griffiths, Cathryn; McAdams, Stephen] McGill Univ, Montreal, PQ H3A 2T5, Canada.
   [Griffiths, Cathryn; McAdams, Stephen] Schulich Sch Mus, 555 Sherbrooke St West, Montreal, PQ H3A 1E3, Canada.
C3 McGill University
RP Marentakis, G (corresponding author), Univ Mus & Performing Arts Graz, Inst Elect Mus & Acoust, Inffeldgasse 10-3, A-8010 Graz, Austria.
EM marentakis@iem.at; cathryn.griffiths@gmail.com; smc@music.mcgill.ca
RI McAdams, Stephen/GQB-0225-2022
OI McAdams, Stephen/0000-0002-6744-9035; Marentakis,
   Georgios/0000-0002-6563-9601
FU Canada Research Chair [950-223484]; Canadian Natural Sciences and
   Engineering Research Council (NSERC) [RGPIN 2015-05280]; NSERC [RGPIN
   127-2007]; Zukunftsfonds Steiermark Klangraume Project [PN:6067]
FX This work was supported by a Canada Research Chair (950-223484) and a
   grant from the Canadian Natural Sciences and Engineering Research
   Council (NSERC, RGPIN 2015-05280) to Stephen McAdams, an NSERC (RGPIN
   127-2007) grant to Albert Bregman, and the Zukunftsfonds Steiermark
   Klangraume Project (PN:6067) awarded to Georgios Marentakis.
CR Adler Samuel, 2002, STUDY ORCHESTRATION, Vthird
   Agres K.R., 2008, Proceedings of the 30th Annual Conference of the Cognitive Science Society, P969
   Ahveninen J, 2006, P NATL ACAD SCI USA, V103, P14608, DOI 10.1073/pnas.0510480103
   Alain C, 2001, P NATL ACAD SCI USA, V98, P12301, DOI 10.1073/pnas.211209098
   Allen K, 2011, J ACOUST SOC AM, V130, P2043, DOI 10.1121/1.3631666
   Allen K, 2009, ATTEN PERCEPT PSYCHO, V71, P164, DOI 10.3758/APP.71.1.164
   [Anonymous], 1999, Auditory Scene Analysis: The Perceptual Organization of Sound, DOI DOI 10.7551/MITPRESS/1486.001.0001
   Arbogast TL, 2000, J ACOUST SOC AM, V108, P1803, DOI 10.1121/1.1289366
   Arbogast TL, 2002, J ACOUST SOC AM, V112, P2086, DOI 10.1121/1.1510141
   Best V, 2006, J ACOUST SOC AM, V120, P1506, DOI 10.1121/1.2234849
   Bigand E, 2000, INT J PSYCHOL, V35, P270, DOI 10.1080/002075900750047987
   Bizley JK, 2013, NAT REV NEUROSCI, V14, P693, DOI 10.1038/nrn3565
   BRONKHORST AW, 1988, J ACOUST SOC AM, V83, P1508, DOI 10.1121/1.395906
   BRONKHORST AW, 1992, J ACOUST SOC AM, V92, P3132, DOI 10.1121/1.404209
   Brungart DS, 2005, J ACOUST SOC AM, V118, P3241, DOI 10.1121/1.2082557
   Chandler DW, 2005, ACTA ACUST UNITED AC, V91, P513
   CHERRY EC, 1954, J ACOUST SOC AM, V26, P554, DOI 10.1121/1.1907373
   CHERRY EC, 1953, J ACOUST SOC AM, V25, P975, DOI 10.1121/1.1907229
   Crawley EJ, 2002, J EXP PSYCHOL HUMAN, V28, P367, DOI 10.1037//0096-1523.28.2.367
   Croghan NBH, 2010, J ACOUST SOC AM, V127, P3085, DOI 10.1121/1.3311862
   CULLING JF, 1993, J ACOUST SOC AM, V93, P3454, DOI 10.1121/1.405675
   CULLING JF, 1995, J ACOUST SOC AM, V98, P785, DOI 10.1121/1.413571
   Darwin Christopher J., 2008, V29, P215
   Darwin CJ, 1999, J EXP PSYCHOL HUMAN, V25, P617, DOI 10.1037/0096-1523.25.3.617
   Darwin CJ, 1997, J ACOUST SOC AM, V102, P2316, DOI 10.1121/1.419641
   DEUTSCH D, 1979, PERCEPT PSYCHOPHYS, V25, P399, DOI 10.3758/BF03199848
   ERIKSEN CW, 1986, PERCEPT PSYCHOPHYS, V40, P225, DOI 10.3758/BF03211502
   Fan WL, 2008, J ACOUST SOC AM, V124, P36, DOI 10.1121/1.2932257
   Farina A, 2013, ACTA ACUST UNITED AC, V99, P118, DOI 10.3813/AAA.918595
   Freyman RL, 1999, J ACOUST SOC AM, V106, P3578, DOI 10.1121/1.428211
   Freyman RL, 2001, J ACOUST SOC AM, V109, P2112, DOI 10.1121/1.1354984
   Gallun FJ, 2005, J ACOUST SOC AM, V118, P1614, DOI 10.1121/1.1984876
   Gallun FJ, 2008, J ACOUST SOC AM, V124, P439, DOI 10.1121/1.2924127
   Gatehouse Stuart, 2008, Trends Amplif, V12, P145, DOI 10.1177/1084713808317395
   Grantham D. Wesley, 1995, P297, DOI 10.1016/B978-012505626-7/50011-X
   Gregory A.H., 1990, Psych. Music, V18, P163, DOI DOI 10.1177/0305735690182005
   Harley M. A., 1994, THESIS
   HARTMANN WM, 1991, MUSIC PERCEPT, V9, P155
   HARTMANN WM, 1989, J ACOUST SOC AM, V85, P2031, DOI 10.1121/1.397855
   Hawley M. L., 2004, J ACOUSTICAL SOC AM, V115, P833
   Hawley ML, 1999, J ACOUST SOC AM, V105, P3436, DOI 10.1121/1.424670
   Heller LM, 2010, J ACOUST SOC AM, V128, P310, DOI 10.1121/1.3436524
   HENNING GB, 1980, J ACOUST SOC AM, V68, P446, DOI 10.1121/1.384756
   HUKIN RW, 1995, J ACOUST SOC AM, V98, P1380, DOI 10.1121/1.414348
   Ihiefeld A, 2008, J ACOUST SOC AM, V123, P4369, DOI 10.1121/1.2904826
   Ihlefeld A, 2008, J ACOUST SOC AM, V123, P4380, DOI 10.1121/1.2904825
   Janata P, 2002, COGN AFFECT BEHAV NE, V2, P121, DOI 10.3758/CABN.2.2.121
   Jones GL, 2008, J ACOUST SOC AM, V124, P3818, DOI 10.1121/1.2996336
   Jones M. R., 2001, THINKING SOUND COGNI, P69
   Kidd G, 2005, J ACOUST SOC AM, V118, P3804, DOI 10.1121/1.2109187
   Kidd G, 2005, ACTA ACUST UNITED AC, V91, P526
   Kidd G, 1998, J ACOUST SOC AM, V104, P422, DOI 10.1121/1.423246
   Kitterick PT, 2010, J ACOUST SOC AM, V127, P2498, DOI 10.1121/1.3327507
   Lindemann E, 2007, IEEE SIGNAL PROC MAG, V24, P80, DOI 10.1109/MSP.2007.323267
   Macmillan N. A., 2005, Detection theory: A user's guide, V2nd
   Maeder PP, 2001, NEUROIMAGE, V14, P802, DOI 10.1006/nimg.2001.0888
   MCFADDEN D, 1976, J ACOUST SOC AM, V59, P634, DOI 10.1121/1.380913
   MILLS AW, 1958, J ACOUST SOC AM, V30, P237, DOI 10.1121/1.1909553
   Mondor TA, 1998, J EXP PSYCHOL HUMAN, V24, P66, DOI 10.1037/0096-1523.24.1.66
   MONDOR TA, 1995, J EXP PSYCHOL HUMAN, V21, P387, DOI 10.1037/0096-1523.21.2.387
   Oxenham AJ, 2003, J ACOUST SOC AM, V114, P1543, DOI 10.1121/1.1598197
   Pulkki V, 2001, J AUDIO ENG SOC, V49, P739
   Pulkki V., 2001, THESIS
   Reynolds R., 2002, Form and Method: Composing Music, The Rothschild Essays
   SABERI K, 1991, ACUSTICA, V75, P57
   Sach AJ, 2000, J EXP PSYCHOL HUMAN, V26, P717, DOI 10.1037/0096-1523.26.2.717
   Sach AJ, 2004, PERCEPT PSYCHOPHYS, V66, P1379, DOI 10.3758/BF03195005
   Saupe K, 2010, J ACOUST SOC AM, V127, P472, DOI 10.1121/1.3271422
   Shinn-Cunningham BG, 2008, TRENDS COGN SCI, V12, P182, DOI 10.1016/j.tics.2008.02.003
   Shinn-Cunningham BG, 2005, ACTA ACUST UNITED AC, V91, P967
   Sloboda J., 1981, PSYCHOL MUSIC, V9, P39, DOI [DOI 10.1177/03057356810090010701, 10.1177/03057356810090010701]
   SOKOLOV EN, 1963, ANNU REV PHYSIOL, V25, P545, DOI 10.1146/annurev.ph.25.030163.002553
   Spence C., 2004, Crossmodal space and crossmodal attention
   Stainsby TH, 2011, J ACOUST SOC AM, V130, P904, DOI 10.1121/1.3605540
   Teder-Salejarvi WA, 1998, PERCEPT PSYCHOPHYS, V60, P1228, DOI 10.3758/BF03206172
   WIGHTMAN FL, 1992, J ACOUST SOC AM, V91, P1648, DOI 10.1121/1.402445
   Woods DL, 2001, J EXP PSYCHOL HUMAN, V27, P65, DOI 10.1037/0096-1523.27.1.65
   Yeomans JS, 1995, BRAIN RES REV, V21, P301, DOI 10.1016/0165-0173(96)00004-5
   Yost WA, 1996, PERCEPT PSYCHOPHYS, V58, P1026, DOI 10.3758/BF03206830
NR 79
TC 1
Z9 1
U1 0
U2 8
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD AUG
PY 2016
VL 14
IS 1
AR 3
DI 10.1145/2911985
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DV4EB
UT WOS:000382876900003
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Caramiaux, B
   Bevilacqua, F
   Bianco, T
   Schnell, N
   Houix, O
   Susini, P
AF Caramiaux, B.
   Bevilacqua, F.
   Bianco, T.
   Schnell, N.
   Houix, O.
   Susini, P.
TI The Role of Sound Source Perception in Gestural Sound Description
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Experimentation; Human Factors; Gesture; environmental sound perception;
   cross-modal relationships; sound source identification; sound tracing;
   sound mimicry; embodied cognition
ID IDENTIFICATION; CATEGORIZATION; WORLD; HEAR
AB We investigated gesture description of sound stimuli performed during a listening task. Our hypothesis is that the strategies in gestural responses depend on the level of identification of the sound source and specifically on the identification of the action causing the sound. To validate our hypothesis, we conducted two experiments. In the first experiment, we built two corpora of sounds. The first corpus contains sounds with identifiable causal actions. The second contains sounds for which no causal actions could be identified. These corpora properties were validated through a listening test. In the second experiment, participants performed arm and hand gestures synchronously while listening to sounds taken from these corpora. Afterward, we conducted interviews asking participants to verbalize their experience while watching their own video recordings. They were questioned on their perception of the listened sounds and on their gestural strategies. We showed that for the sounds where causal action can be identified, participants mainly mimic the action that has produced the sound. In the other case, when no action can be associated with the sound, participants trace contours related to sound acoustic features. We also found that the interparticipants' gesture variability is higher for causal sounds compared to noncausal sounds. Variability demonstrates that, in the first case, participants have several ways of producing the same action, whereas in the second case, the sound features tend to make the gesture responses consistent.
C1 [Caramiaux, B.] Univ London, Dept Comp, New Cross London SE14 6NW, England.
   [Bevilacqua, F.; Bianco, T.; Schnell, N.; Houix, O.; Susini, P.] UMR STMS IRCAM CNRS UPMC, Paris, France.
C3 University of London; Sorbonne Universite; Centre National de la
   Recherche Scientifique (CNRS)
RP Caramiaux, B (corresponding author), Univ London, Dept Comp, New Cross London SE14 6NW, England.
EM b.caramiaux@gold.ac.uk
OI Caramiaux, Baptiste/0000-0002-4590-106X
CR [Anonymous], 1993, Thinking in sound: the cognitive psychology of human audition
   BALLAS JA, 1993, J EXP PSYCHOL HUMAN, V19, P250, DOI 10.1037/0096-1523.19.2.250
   Caramiaux B., 2010, TECHNICAL REPORT
   Caramiaux B, 2010, LECT NOTES ARTIF INT, V5934, P158, DOI 10.1007/978-3-642-12553-9_14
   Caramiaux Baptiste, 2012, THESIS U P M CURIE P
   GAVER WW, 1993, ECOL PSYCHOL, V5, P1, DOI 10.1207/s15326969eco0501_1
   GAVER WW, 1993, ECOL PSYCHOL, V5, P285, DOI 10.1207/s15326969eco0504_2
   Gerard Y., 2004, THESIS U BOURGOGNE
   Giordano BL, 2010, BRAIN COGNITION, V73, P7, DOI 10.1016/j.bandc.2010.01.005
   Godoy R. I., 2006, LECT NOTES COMPUTER
   Godoy R.I., 2006, Organised Sound, V11, P149, DOI DOI 10.1017/S1355771806001439
   Godoy R. I., 2006, P COST287 CONGAS 2 I
   Guastavino C, 2007, CAN J EXP PSYCHOL, V61, P54, DOI 10.1037/cjep2007006
   Gygi B, 2004, J ACOUST SOC AM, V115, P1252, DOI 10.1121/1.1635840
   Gygi B, 2007, PERCEPT PSYCHOPHYS, V69, P839, DOI 10.3758/BF03193921
   Houix O, 2012, J EXP PSYCHOL-APPL, V18, P52, DOI 10.1037/a0026240
   Kendon Adam, 2004, Gesture: Visible Action as utterance, DOI DOI 10.1017/CBO9780511807572
   Kohler E, 2002, SCIENCE, V297, P846, DOI 10.1126/science.1070311
   Large EW, 2002, COGNITIVE SCI, V26, P1, DOI 10.1207/s15516709cog2601_1
   Large EW, 2000, HUM MOVEMENT SCI, V19, P527, DOI 10.1016/S0167-9457(00)00026-9
   LEMAITRE G, 2012, J ACOUSTICAL SOC AM, V131, P1337
   Lemaitre G, 2010, J EXP PSYCHOL-APPL, V16, P16, DOI 10.1037/a0018762
   Leman M, 2009, MUSIC PERCEPT, V26, P263, DOI 10.1525/MP.2009.26.3.263
   Lewis JW, 2004, CEREB CORTEX, V14, P1008, DOI 10.1093/cercor/bhh061
   Loehr JD, 2007, EXP BRAIN RES, V178, P518, DOI 10.1007/s00221-006-0760-8
   Ma XJ, 2010, CHI2010: PROCEEDINGS OF THE 28TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1-4, P1945
   Maes P.-J., 2013, THESIS GHENT U
   Marcell ME, 2000, J CLIN EXP NEUROPSYC, V22, P830, DOI 10.1076/jcen.22.6.830.949
   McNeill D., 1996, HAND MIND WHAT GESTU
   Nymoen K., 2011, ACM MULT MI IN PRESS, V2011
   Nymoen K., 2010, P C NEW INT MUS EXPR
   Nymoen K, 2013, ACM T APPL PERCEPT, V10, DOI 10.1145/2465780.2465783
   Pizzamiglio L, 2005, NEUROIMAGE, V24, P852, DOI 10.1016/j.neuroimage.2004.09.025
   Ramsay J. O., 1997, FUNCTIONAL DATA ANAL, DOI [10.1007/978-1-4757-7107-7, DOI 10.1007/978-1-4757-7107-7]
   Scavone G. P., 2002, P INT C AUD DISPL
   Schaeffer P., 1966, Traite des objets musicaux
   Shafiro V, 2008, EAR HEARING, V29, P401, DOI 10.1097/AUD.0b013e31816a0cf1
   Smalley D., 1997, ORGAN SOUND, V2, P107, DOI [10.1017/S1355771897009059, DOI 10.1017/S1355771897009059]
   Stevens SS, 1937, J ACOUST SOC AM, V8, P185, DOI 10.1121/1.1915893
   Tardieu J, 2009, APPL ACOUST, V70, P1183, DOI 10.1016/j.apacoust.2009.04.004
   VanDerveer N. J., 1980, THESIS PROQUEST INFO
   Vermersch P., 1990, PSYCHOL FR, V35, P227
   Wanderley MM, 2004, P IEEE, V92, P632, DOI 10.1109/JPROC.2004.825882
   Zatorre RJ, 2007, NAT REV NEUROSCI, V8, P547, DOI 10.1038/nrn2152
NR 44
TC 23
Z9 25
U1 0
U2 7
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD APR
PY 2014
VL 11
IS 1
AR 1
DI 10.1145/2536811
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA AG7CF
UT WOS:000335574900001
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Höver, R
   Di Luca, M
   Harders, M
AF Hoever, Raphael
   Di Luca, Massimiliano
   Harders, Matthias
TI User-Based Evaluation of Data-Driven Haptic Rendering
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Algorithms; Human Factors; Measurement; Verification; Discrimination
   study; force discrimination; haptic rendering; deformable models
ID MANUAL DISCRIMINATION; FORCE; TELEPRESENCE; WEIGHT; GRASP
AB In this article, the data-driven haptic rendering approach presented in our earlier work is assessed. The approach relies on recordings from real objects from which a data-driven model is derived that captures the haptic properties of the object. We conducted two studies. In the first study, the Just Noticeable Difference (JND) for small forces, as encountered in our set-up, was determined. JNDs were obtained both for active and passive user interaction. A conservative threshold curve was derived that was then used to guide the model generation in the second study. The second study examined the achievable rendering fidelity for two objects with different stiffnesses. Subjects directly compared data-driven virtual feedback with the real objects. Results indicated that it is crucial to include dynamic material effects to achieve haptic feedback that cannot be distinguished from real objects. Results also showed that the fidelity is considerably decreased for stiffer objects due to limits of the display hardware.
C1 [Hoever, Raphael] Swiss Fed Inst Technol, Comp Vis Lab, Virtual Real Med Grp, CH-8092 Zurich, Switzerland.
   [Di Luca, Massimiliano] Max Planck Inst Biol Cybernet, D-72076 Tubingen, Germany.
C3 Swiss Federal Institutes of Technology Domain; ETH Zurich; Max Planck
   Society
RP Höver, R (corresponding author), Swiss Fed Inst Technol, Comp Vis Lab, Virtual Real Med Grp, Sternwartstr 7, CH-8092 Zurich, Switzerland.
EM hoever@vision.ee.ethz.ch
RI Di Luca, Massimiliano/A-7850-2012
OI Di Luca, Massimiliano/0000-0003-3085-7251
FU EU [IST-2006-27141]
FX This work was supported by the EU project Immersence IST-2006-27141.
CR Allin S, 2002, 10TH SYMPOSIUM ON HAPTIC INTERFACES FOR VIRTUAL ENVIRONMENT AND TELEOPERATOR SYSTEMS, PROCEEDINGS, P299, DOI 10.1109/HAPTIC.2002.998972
   Andrews S, 2007, 3DIM 2007: SIXTH INTERNATIONAL CONFERENCE ON 3-D DIGITAL IMAGING AND MODELING, PROCEEDINGS, P99
   [Anonymous], 1963, Handbook of mathematical psychology
   Colton MB, 2007, IEEE INT CONF ROBOT, P497, DOI 10.1109/ROBOT.2007.363835
   Colton MB, 2007, WORLD HAPTICS 2007: SECOND JOINT EUROHAPTICS CONFERENCE AND SYMPOSIUM ON HAPTIC INTERFACES FOR VIRTUAL ENVIRONMENT AND TELEOPERATOR SYSTEMS, PROCEEDINGS, P243
   ENGEN T, 1971, EXPT PSYCHOL, P11
   Gescheider G.A., 1985, PSYCHOPHYSICS, V2nd, P1
   Hinterseer P, 2005, INT CONF ACOUST SPEE, P1097
   Hinterseer P, 2006, SYMPOSIUM ON HAPTICS INTERFACES FOR VIRTUAL ENVIRONMENT AND TELEOPERATOR SYSTEMS 2006, PROCEEDINGS, P35
   Hirche S, 2007, PRESENCE-TELEOP VIRT, V16, P523, DOI 10.1162/pres.16.5.523
   Höver R, 2008, SYMPOSIUM ON HAPTICS INTERFACES FOR VIRTUAL ENVIRONMENT AND TELEOPERATOR SYSTEMS 2008, PROCEEDINGS, P201
   Höver R, 2009, IEEE T HAPTICS, V2, P15, DOI 10.1109/ToH.2009.2
   Höver R, 2009, WORLD HAPTICS 2009: THIRD JOINT EUROHAPTICS CONFERENCE AND SYMPOSIUM ON HAPTIC INTERFACES FOR VIRTUAL ENVIRONMENT AND TELEOPERATOR SYSTEMS, PROCEEDINGS, P39, DOI 10.1109/WHC.2009.4810814
   JONES LA, 1989, PERCEPTION, V18, P681, DOI 10.1068/p180681
   JONES LA, 1990, EXP BRAIN RES, V79, P150
   Kawai S, 2003, EXP BRAIN RES, V153, P297, DOI 10.1007/s00221-003-1622-2
   Kuchenbecker KJ, 2006, IEEE T VIS COMPUT GR, V12, P219, DOI 10.1109/TVCG.2006.32
   Kuchenbecker KJ, 2005, World Haptics Conference: First Joint Eurohaptics Conference and Symposium on Haptic Interfaces for Virutual Environment and Teleoperator Systems, Proceedings, P381
   Leskovsky P., 2006, P EUROHAPTICS, P289
   MacLean K. E., 1996, Proceedings of the ASME Dynamic Systems and Control Division, P459
   Norwich KH, 1997, PERCEPT PSYCHOPHYS, V59, P929, DOI 10.3758/BF03205509
   Okamura AM, 2003, IEEE INT CONF ROBOT, P828
   PANG XD, 1991, PERCEPT PSYCHOPHYS, V49, P531, DOI 10.3758/BF03212187
   Richard Christopher., 1999, PROC ASME DYNAMIC SY, V67, P327
   ROSS HE, 1987, Q J EXP PSYCHOL-A, V39, P77, DOI 10.1080/02724988743000042
   Ruffaldi E, 2006, SYMPOSIUM ON HAPTICS INTERFACES FOR VIRTUAL ENVIRONMENT AND TELEOPERATOR SYSTEMS 2006, PROCEEDINGS, P225
   SWINDELLS C, 2007, P WORLD HAPT 2007 2, P194
   Swindells C, 2009, IEEE T HAPTICS, V2, P200, DOI 10.1109/ToH.2009.23
   TAN HZ, 1995, PERCEPT PSYCHOPHYS, V57, P495, DOI 10.3758/BF03213075
   WEBER EH, 1978, SUBTILITATE TACTUS S
NR 30
TC 22
Z9 23
U1 0
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD OCT
PY 2010
VL 8
IS 1
AR 7
DI 10.1145/1857893.1857900
PG 23
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 748AJ
UT WOS:000289362100007
DA 2024-07-18
ER

PT J
AU Grechkin, TY
   Nguyen, TD
   Plumert, JM
   Cremer, JF
   Kearney, JK
AF Grechkin, Timofey Y.
   Nguyen, Tien Dat
   Plumert, Jodie M.
   Cremer, James F.
   Kearney, Joseph K.
TI How Does Presentation Method and Measurement Protocol Affect Distance
   Estimation in Real and Virtual Environments?
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Experimentation; Human Factors; Distance estimation; egocentric depth
   perception; large-screen immersive displays; perception; head-mounted
   displays; virtual environments
AB We conducted two experiments that compared distance perception in real and virtual environments in six visual presentation methods using either timed imagined walking or direct blindfolded walking, while controlling for several other factors that could potentially impact distance perception. Our presentation conditions included unencumbered real world, real world seen through an HMD, virtual world seen through an HMD, augmented reality seen through an HMD, virtual world seen on multiple, large immersive screens, and photo-based presentation of the real world seen on multiple, large immersive screens. We found that there was a similar degree of underestimation of distance in the HMD and large-screen presentations of virtual environments. We also found that while wearing the HMD can cause some degree of distance underestimation, this effect depends on the measurement protocol used. Finally, we found that photo-based presentation did not help to improve distance perception in a large-screen immersive display system. The discussion focuses on points of similarity and difference with previous work on distance estimation in real and virtual environments.
C1 [Grechkin, Timofey Y.; Nguyen, Tien Dat; Plumert, Jodie M.; Cremer, James F.; Kearney, Joseph K.] Univ Iowa, Dept Comp Sci, Iowa City, IA 52242 USA.
   [Grechkin, Timofey Y.; Nguyen, Tien Dat; Plumert, Jodie M.; Cremer, James F.; Kearney, Joseph K.] Univ Iowa, Dept Psychol, Iowa City, IA 52242 USA.
C3 University of Iowa; University of Iowa
RP Grechkin, TY (corresponding author), Univ Iowa, Dept Comp Sci, 14 MacLean Hall, Iowa City, IA 52242 USA.
EM timofey-grechkin@uiowa.edu; tiendat-nguyen@uiowa.edu;
   jodie-plumert@uiowa.edu; james-cremer@uiowa.edu; joe-kearney@uiowa.edu
RI Gorzel, Marcin/F-4365-2010
FU National Science Foundation [CNS-0750677]; National Institute of Child
   Health and Human Development [R01-HD052875]; Vietnam Education
   Foundation
FX This research was supported by grants awarded to J. Plumert, J. Kearney,
   and J. Cremer from the National Science Foundation (CNS-0750677) and the
   National Institute of Child Health and Human Development (R01-HD052875),
   and by a fellowship awarded to T. D. Nguyen from the Vietnam Education
   Foundation.
CR [Anonymous], 2005, ACM Transactions on Applied Perception, DOI [DOI 10.1145/1077399.1077403, DOI 10.1145/1077399.10774032,3,9]
   Bodenheimer B., 2007, P 4 S APPL PERC GRAP, DOI [10.1145/1272582.1272589, DOI 10.1145/1272582.1272589]
   Creem-Regehr SH, 2005, PERCEPTION, V34, P191, DOI 10.1068/p5144
   Jones VP, 2008, J INSECT SCI, V8, DOI 10.1673/031.008.1401
   Klein E, 2009, IEEE VIRTUAL REALITY 2009, PROCEEDINGS, P107, DOI 10.1109/VR.2009.4811007
   Kunz BR, 2009, J EXP PSYCHOL HUMAN, V35, P1458, DOI 10.1037/a0015786
   Lappin JS, 2006, PERCEPT PSYCHOPHYS, V68, P571, DOI 10.3758/BF03208759
   Loomis JM, 2003, VIRTUAL AND ADAPTIVE ENVIRONMENTS: APPLICATIONS, IMPLICATIONS, AND HUMAN PERFORMANCE ISSUES, P21
   Ooi TL, 2001, NATURE, V414, P197, DOI 10.1038/35102562
   Plumert Jodie., 2005, ACM Transactions on Applied Perception, V2, P216, DOI DOI 10.1145/1077399.1077402
   Thompson E., 2004, J MENS STUDIES, V13, P5, DOI [DOI 10.3149/JMS.1301.5, https://doi.org/10.3149/jms.1301.5, 10.3149/jms.1301.5]
   Willemsen P, 2009, ACM T APPL PERCEPT, V6, DOI 10.1145/1498700.1498702
   Witmer BG, 1998, HUM FACTORS, V40, P478, DOI 10.1518/001872098779591340
   Wu B, 2004, NATURE, V428, P73, DOI 10.1038/nature02350
   Ziemer CJ, 2009, ATTEN PERCEPT PSYCHO, V71, P1095, DOI 10.3758/APP.71.5.1096
NR 15
TC 111
Z9 122
U1 1
U2 21
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD JUL
PY 2010
VL 7
IS 4
AR 26
DI 10.1145/1823738.1823744
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 633ZI
UT WOS:000280546500006
DA 2024-07-18
ER

PT J
AU Hodgins, J
   Jörg, S
   O'Sullivan, C
   Park, SI
   Mahler, M
AF Hodgins, Jessica
   Jorg, Sophie
   O'Sullivan, Carol
   Park, Sang Il
   Mahler, Moshe
TI The Saliency of Anomalies in Animated Human Characters
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Experimentation; Human Factors; Human animation; virtual characters;
   perception of human motion; motion capture; eye tracking
AB Virtual characters are much in demand for animated movies, games, and other applications. Rapid advances in performance capture and advanced rendering techniques have allowed the movie industry in particular to create characters that appear very human-like. However, with these new capabilities has come the realization that such characters are yet not quite "right." One possible hypothesis is that these virtual humans fall into an "Uncanny Valley", where the viewer's emotional response is repulsion or rejection, rather than the empathy or emotional engagement that their creators had hoped for. To explore these issues, we created three animated vignettes of an arguing couple with detailed motion for the face, eyes, hair, and body. In a set of perceptual experiments, we explore the relative importance of different anomalies using two different methods: a questionnaire to determine the emotional response to the full-length vignettes, with and without facial motion and audio; and a 2AFC (two alternative forced choice) task to compare the performance of a virtual "actor" in short clips (extracts from the vignettes) depicting a range of different facial and body anomalies. We found that the facial anomalies are particularly salient, even when very significant body animation anomalies are present.
C1 [Jorg, Sophie; O'Sullivan, Carol] Trinity Coll Dublin, Coll Green, Dublin 2, Ireland.
   [Hodgins, Jessica; Mahler, Moshe] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.
   [Hodgins, Jessica] Disney Res, Pittsburgh, PA USA.
   [Park, Sang Il] Sejong Univ, Seoul 143747, South Korea.
C3 Trinity College Dublin; Carnegie Mellon University; Sejong University
RP O'Sullivan, C (corresponding author), Trinity Coll Dublin, Coll Green, Dublin 2, Ireland.
EM carol.osullivan@cs.tcd.ie
OI Mahler, Moshe/0009-0001-4004-3158; O'Sullivan, Carol/0000-0003-3772-4961
FU Disney Research Pittsburgh; Irish Research Council for Science
   Engineering and Technology (IRCSET); NSF [CCF-0811450]; Division of
   Computing and Communication Foundations; Direct For Computer & Info Scie
   & Enginr [0811450] Funding Source: National Science Foundation
FX This research was supported in part by Disney Research Pittsburgh, the
   Irish Research Council for Science Engineering and Technology (IRCSET),
   and NSF CCF-0811450.
CR Anders S, 2004, HUM BRAIN MAPP, V23, P200, DOI 10.1002/hbm.20048
   Bartneck C, 2007, 2007 RO-MAN: 16TH IEEE INTERNATIONAL SYMPOSIUM ON ROBOT AND HUMAN INTERACTIVE COMMUNICATION, VOLS 1-3, P367
   Carter EJ, 2010, ACM T APPL PERCEPT, V7, DOI 10.1145/1823738.1823741
   Geller T, 2008, IEEE COMPUT GRAPH, V28, P11, DOI 10.1109/MCG.2008.79
   Giorgolo G., 2008, P INT C AUD VIS SPEE, P31
   HANSON D, 2005, P NAT C ART INT AAI
   Harrison J, 2004, ACM T GRAPHIC, V23, P569, DOI 10.1145/1015706.1015761
   Ho Chin-Chang., 2008, Proceedings of the 3rd ACM/IEEE Conference on Human-Robot Interaction, 2008, P169, DOI [DOI 10.1145/1349822.1349845, 10.1145/1349822.1349845.]
   Hodgins JK, 1998, IEEE T VIS COMPUT GR, V4, P307, DOI 10.1109/2945.765325
   Hunt AR, 2007, VIS COGN, V15, P513, DOI 10.1080/13506280600843346
   Levi S., 2004, NEWSWEEK, V650, P305
   LOTTRIDGE D, 2008, ACM HUM FACT COMP SY, P2617
   MACDORMAN K, 2006, P ICCS COGSCI LONG S
   MacDorman K. F., 2005, P 27 ANN M COGN SCI
   MacDorman KF, 2009, COMPUT HUM BEHAV, V25, P695, DOI 10.1016/j.chb.2008.12.026
   MacDorman KF, 2005, IEEE-RAS INT C HUMAN, P399
   MCDONNELL R, 2007, P EUR ACM SIGGRAPH S, P259
   McDowall RM, 2009, REV FISH BIOL FISHER, V19, P1, DOI 10.1007/s11160-008-9085-y
   Meehan M, 2002, ACM T GRAPHIC, V21, P645, DOI 10.1145/566570.566630
   Mori M., 1970, Energy, V7, P33, DOI [DOI 10.1109/MRA.2012.2192811, 10.1109/MRA.2012.2192811]
   Park SI, 2006, ACM T GRAPHIC, V25, P881, DOI 10.1145/1141911.1141970
   Reitsma PSA, 2003, ACM T GRAPHIC, V22, P537, DOI 10.1145/882262.882304
   SCHNEIDER E, 2007, SITUATED PLAY, P546
   TINWELL A, 2004, THINK DARK INT C
   Tinwell A., 2009, P 13 INT MINDTREK C, P66, DOI [DOI 10.1145/1621841.1621855, https://doi.org/10.1145/1621841.1621855]
   Wang J., 2004, Proceedings of the 2004 ACM SIGGRAPH/Eurographics symposium on Computer animation, P335
NR 26
TC 40
Z9 47
U1 0
U2 11
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD JUL
PY 2010
VL 7
IS 4
AR 22
DI 10.1145/1823738.1823740
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 633ZI
UT WOS:000280546500002
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Kjellin, A
   Pettersson, LW
   Seipel, S
   Lind, M
AF Kjellin, Andreas
   Pettersson, Lars Winkler
   Seipel, Stefan
   Lind, Mats
TI Evaluating 2D and 3D Visualizations of Spatiotemporal Information
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Experimentation; Human Factors; 2D; 3D; animation; space-time cube;
   spatiotemporal; user studies
ID RESPONSE-TIME DISTRIBUTIONS; AFFINE STRUCTURE; SHAPE CONSTANCY;
   PERCEPTION; MOTION; USABILITY; ATTENTION; BLINDNESS
AB Time-varying geospatial data presents some specific challenges for visualization. Here, we report the results of three experiments aiming at evaluating the relative efficiency of three existing visualization techniques for a class of such data. The class chosen was that of object movement, especially the movements of vehicles in a fictitious landscape. Two different tasks were also chosen. One was to predict where three vehicles will meet in the future given a visualization of their past movement history. The second task was to estimate the order in which four vehicles arrived at a specific place. Our results reveal that previous findings had generalized human perception in these situations and that large differences in user efficiency exist for a given task between different types of visualizations depicting the same data. Furthermore, our results are in line with earlier general findings on the nature of human perception of both object shape and scene changes. Finally, the need for new taxonomies of data and tasks based on results from perception research is discussed.
C1 [Kjellin, Andreas; Lind, Mats] Uppsala Univ, Dept Informat & Media Human Comp Interact, SE-75120 Uppsala, Sweden.
   [Pettersson, Lars Winkler; Seipel, Stefan] Uppsala Univ, Dept Informat Technol, SE-75105 Uppsala, Sweden.
   [Seipel, Stefan] Univ Gavle, Gavle, Sweden.
C3 Uppsala University; Uppsala University; University of Gavle
RP Kjellin, A (corresponding author), Uppsala Univ, Dept Informat & Media Human Comp Interact, Box 513, SE-75120 Uppsala, Sweden.
EM kjellin.andreas@gmail.com; mail@larspettersson.se; ss@hig.se;
   mats.lind@dis.uu.se
OI Seipel, Stefan/0000-0003-0085-5829
FU Swedish National Defence College (SNDC)
FX The work presented here is the result of a subproject funded by Swedish
   National Defence College (SNDC) in the framework of project AQUA.
CR Alvarez GA, 2005, J EXP PSYCHOL HUMAN, V31, P643, DOI 10.1037/0096-1523.31.4.643
   Andrienko G, 2003, ISPRS J PHOTOGRAMM, V57, P380, DOI 10.1016/S0924-2716(02)00166-1
   Andrienko N, 2003, IEEE INFOR VIS, P237, DOI 10.1109/IV.2003.1217985
   Andrienko N, 2003, J VISUAL LANG COMPUT, V14, P503, DOI 10.1016/S1045-926X(03)00046-6
   ANDRIENKO N, 2003, P 10 INT C HUM COMP, P1153
   Andrienko N., 2000, Proceedings of the working conference on advanced visual interfaces, P217, DOI [10.1145/345513.345319, DOI 10.1145/345513.345319]
   [Anonymous], 1939, Elementary Mathematics from an Advanced Standpoint
   [Anonymous], 2000, Information Visualization: Perception for Design
   [Anonymous], 1999, READINGS INFORM VISU
   [Anonymous], 1998, ISO 9241-11
   Bertin J, 1983, IN PRESS
   Bingham GP, 2008, PERCEPT PSYCHOPHYS, V70, P524, DOI 10.3758/PP.70.3.524
   BROWN B, 1972, VISION RES, V12, P293, DOI 10.1016/0042-6989(72)90119-8
   Chung WY, 2005, INT J HUM-COMPUT ST, V62, P127, DOI 10.1016/j.ijhcs.2004.08.005
   FORSELL C, 2007, THESIS ACTA U UPSALI
   Hagerstrand T, 1970, PAPERS REGIONAL SCI, V24, P7, DOI [10.1007/BF01936872, DOI 10.1111/J.1435-5597.1970.TB01464.X]
   Harrower M., 2002, CARTOGRAPHIC PERSPEC, V39, P30
   HEATHCOTE A, 1991, PSYCHOL BULL, V109, P340, DOI 10.1037/0033-2909.109.2.340
   HOCKLEY WE, 1984, J EXP PSYCHOL LEARN, V10, P598, DOI 10.1037/0278-7393.10.4.598
   House DH, 2006, IEEE T VIS COMPUT GR, V12, P509, DOI 10.1109/TVCG.2006.58
   *ITV MIN, 2007, WHAT HAS ITC DON MIN
   Kapler T, 2004, IEEE SYMPOSIUM ON INFORMATION VISUALIZATION 2004, PROCEEDINGS, P25, DOI 10.1109/INFVIS.2004.27
   Koua EL, 2006, INT J GEOGR INF SCI, V20, P425, DOI 10.1080/13658810600607550
   Kulyk O, 2007, LECT NOTES COMPUT SC, V4417, P13
   Lightner NJ, 2001, INT J HUM-COMPUT INT, V13, P53, DOI 10.1207/S15327590IJHC1301_4
   Lind M, 2005, Ninth International Conference on Information Visualisation, Proceedings, P896, DOI 10.1109/IV.2005.50
   Lind M., 2003, Information Visualization, V2, P51, DOI 10.1057/palgrave.ivs.9500038
   MCCORMACK PD, 1964, CAN J PSYCHOLOGY, V18, P43, DOI 10.1037/h0083285
   NORMAN JF, 1996, J EXP PSYCH HUM PERC, V173, P3
   Ogao P.J., 2002, Int. J. Appl. Earth Obs. Geoinf, V4, P23, DOI DOI 10.1016/S0303-2434(02)00005-3
   Paillard J., 1985, Brain mechanisms in spatial vision, P299
   PETTERSSON LW, 2004, P ANN SIGRAD C
   PEUQUET DJ, 1994, ANN ASSOC AM GEOGR, V84, P441, DOI 10.1111/j.1467-8306.1994.tb01869.x
   PIZLO Z, 1994, VISION RES, V34, P1637, DOI 10.1016/0042-6989(94)90123-6
   Pizlo Z, 1999, PERCEPT PSYCHOPHYS, V61, P1299, DOI 10.3758/BF03206181
   PYLYSHYN Z W, 1988, Spatial Vision, V3, P179, DOI 10.1163/156856888X00122
   SCHIPPER L, 1956, 5672 WADC OH STAT U
   Scholl BJ, 2001, COGNITION, V80, P1, DOI 10.1016/S0010-0277(00)00152-9
   Simons DJ, 1999, PERCEPTION, V28, P1059, DOI 10.1068/p2952
   Simons DJ, 2005, TRENDS COGN SCI, V9, P16, DOI 10.1016/j.tics.2004.11.006
   Simons DJ, 2000, VIS COGN, V7, P1, DOI 10.1080/135062800394658
   SUPPES P, 1977, SYNTHESE, V35, P397, DOI 10.1007/BF00485624
   TITTLE JS, 1995, J EXP PSYCHOL HUMAN, V21, P663, DOI 10.1037/0096-1523.21.3.663
   TODD JT, 1990, PERCEPT PSYCHOPHYS, V48, P419, DOI 10.3758/BF03211585
   Todd JT, 2003, PERCEPT PSYCHOPHYS, V65, P31, DOI 10.3758/BF03194781
   Todd JT, 2001, PSYCHOL SCI, V12, P191, DOI 10.1111/1467-9280.00335
   Todd JT, 1998, PERCEPTION, V27, P273, DOI 10.1068/p270273
   TRESILIAN JR, 1995, PERCEPT PSYCHOPHYS, V57, P231, DOI 10.3758/BF03206510
   TUFTE ER, 2006, BEAUTIFUL EVIDENCE G
   WEHREND C, 1990, P 1 C VIS IEEE LOS A, V1, P139
   Wiegmann DA, 2005, IEEE T POWER SYST, V20, P1233, DOI 10.1109/TPWRS.2005.851967
   YANTIS S, 1992, COGN PSYCH, V295, P340
   Yattaw N.J., 1999, CARTOGR GEOGR INF SC, V26, P85
NR 54
TC 38
Z9 43
U1 0
U2 7
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD JUN
PY 2010
VL 7
IS 3
AR 19
DI 10.1145/1773965.1773970
PG 23
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA 618NF
UT WOS:000279361800005
DA 2024-07-18
ER

PT J
AU Lécuyer, A
   Burkhardt, JM
   Tan, CH
AF Lecuyer, Anatole
   Burkhardt, Jean-Marie
   Tan, Chee-Hian
TI A Study of the Modification of the Speed and Size of the Cursor for
   Simulating Pseudo-haptic Bumps and Holes
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Design; Experimentation; Human Factors; Performance; Pseudo-haptic;
   texture; cursor; control/display ratio; size; speed; bump; hole
ID TACTILE DISPLAY; PERCEPTION; SHAPE
AB In previous work on so-called pseudo-haptic textures, we investigated the possibility of simulating sensations of texture without haptic devices by using the sole manipulation of the speed of a mouse cursor (a technique called speed technique). In this paper, we describe another technique (called Size technique) to enhance the speed technique and simulate texture sensations by varying the size of the cursor according to the local height of the texture displayed on the computer screen. With the size technique, the user would see an increase (decrease) in cursor size corresponding to a positive (negative) slope of the texture. We have conducted a series of experiments to study and compare the use of both the size and speed technique for simulating simple shapes like bumps and holes. In Experiment 1, our results showed that participants could successfully identify bumps and holes using the size technique alone. Performances obtained with the size technique reached a similar level of accuracy as found previously with the speed technique alone. In Experiment 2, we determined a point of subjective equality between bumps simulated by each technique separately, which suggests that the two techniques provide information that can be perceptually equivalent. In Experiment 3, using paradoxical situations of conflict between the two techniques, we have found that participants' answers were more influenced by the size technique, suggesting a dominance of the size over the speed technique. Furthermore, we have found a mutual reinforcement of the techniques, i.e., when the two techniques were consistently combined, the participants were more efficient in identifying the simulated shapes. In Experiment 4, we further observed the complex interactions between the information associated with the two techniques in the perception and in the decision process related to the accurate identification of bumps and holes. Taken together, our results promote the use of both techniques for the low-cost simulation of texture sensations in applications, such as videogames, internet, and graphical user interfaces.
C1 [Lecuyer, Anatole; Tan, Chee-Hian] INRIA Rennes, F-35042 Rennes, France.
   [Burkhardt, Jean-Marie] Paris Descartes Univ, F-75020 Paris 06, France.
C3 Universite de Rennes; Universite Paris Cite
RP Lécuyer, A (corresponding author), INRIA Rennes, Campus Beaulieu, F-35042 Rennes, France.
EM anatole.lecuyer@irisa.fr
RI Burkhardt, Jean-Marie/AAF-5544-2020; Burkhardt, Jean-Marie/B-8675-2008
OI Burkhardt, Jean-Marie/0000-0003-4417-6430; 
CR [Anonymous], 1996, FORCE TOUCH FEEDBACK
   Avanzini F, 2005, IEEE T SPEECH AUDI P, V13, P1073, DOI 10.1109/TSA.2005.852984
   BASDOGAN C, 1997, P ASME DYN SYST CONT, V61, P77
   BEDERSON BB, 2000, P ACM C US INT SOFTW, P217
   Boring EG, 1941, AM J PSYCHOL, V53, P293
   Dominjon L, 2005, P IEEE VIRT REAL ANN, P19
   Emmert E., 1881, KLINISCHEMONATSBL TT, V19, P443
   ERKELENS CJ, 1986, J PHYSIOL-LONDON, V379, P145, DOI 10.1113/jphysiol.1986.sp016245
   Ernst MO, 2002, NATURE, V415, P429, DOI 10.1038/415429a
   Gescheider G.A., 1985, Psychophysics: Methods, Theory and Application
   *HAPT, 2005, VIRTUOSE API V2 0 PR
   Hatwell Y., 2003, TOUCHING KNOWING
   Hershenson Maurice., 1999, Visual Space Perception
   Ikei Y, 1997, IEEE COMPUT GRAPH, V17, P53, DOI 10.1109/38.626970
   Jones LA, 2002, 10TH SYMPOSIUM ON HAPTIC INTERFACES FOR VIRTUAL ENVIRONMENT AND TELEOPERATOR SYSTEMS, PROCEEDINGS, P137, DOI 10.1109/HAPTIC.2002.998951
   LECUYER A, 2001, P IEEE INT C VIRT RE
   LECUYER A, 2000, P IEEE INT C VIRT RE, P239
   LECUYER A, 2004, P ACM C HUM FACT COM
   LEDERMAN SJ, 1981, J EXP PSYCHOL HUMAN, V7, P902, DOI 10.1037/0096-1523.7.4.902
   LIPPINCOTTWILLI, 2005, PERIPHERAL NERVE BLO
   MANDRYK RL, 2005, P 2005 ACM C HUM FAC, P1621
   MINSKY MDR, 1995, THESIS MIT
   Nakamizo Sachio, 2004, J Physiol Anthropol Appl Human Sci, V23, P325, DOI 10.2114/jpa.23.325
   Paljic A, 2004, 12TH INTERNATIONAL SYMPOSIUM ON HAPTIC INTERFACES FOR VIRTUAL ENVIRONMENT AND TELEOPERATOR SYSTEMS, PROCEEDINGS, P216, DOI 10.1109/HAPTIC.2004.1287199
   Robles-De-La-Torre G, 2001, NATURE, V412, P445, DOI 10.1038/35086588
   RODGERS M, 2005, THESIS DALHOUSIE U
   Shimojo M, 1999, IEEE T SYST MAN CY A, V29, P637, DOI 10.1109/3468.798067
   VANMENSVOORT K, 2002, P DES INT SYST 2002, P345
   Watanabe K., 2003, P HUM INT S, P541
   YOUNG FA, 1951, AM J PSYCHOL, V64, P124, DOI 10.2307/1418608
NR 30
TC 34
Z9 38
U1 0
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD AUG
PY 2008
VL 5
IS 3
AR 14
DI 10.1145/1402236.1402238
PG 21
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 450YK
UT WOS:000266437700002
DA 2024-07-18
ER

PT J
AU Palmer, EM
   Clausner, TC
   Kellman, PJ
AF Palmer, Evan M.
   Clausner, Timothy C.
   Kellman, Philip J.
TI Enhancing Air Traffic Displays via Perceptual Cues
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Design; Experimentation; Human Factors; Human-computer interaction; air
   traffic control; applied cognitive science; visualization
ID VISUAL-SEARCH; GUIDED SEARCH; MODEL
AB We examined graphical representations of aircraft altitude in simulated air traffic control (ATC) displays. In two experiments, size and contrast cues correlated with altitude improved participants' ability to detect future aircraft collisions (conflicts). Experiment 1 demonstrated that, across several set sizes, contrast and size cues to altitude improved accuracy at identifying conflicts. Experiment 2 demonstrated that graphical cues for representing altitude both improved accuracy and reduced search time for finding conflicts in large set size displays. The addition of size and contrast cues to ATC displays may offer specific benefits in aircraft conflict detection.
C1 [Kellman, Philip J.] Univ Calif Los Angeles, Dept Psychol, Los Angeles, CA 90095 USA.
C3 University of California System; University of California Los Angeles
RP Palmer, EM (corresponding author), Wichita State Univ, Dept Psychol, Wichita, KS 67260 USA.
EM evan.palmer@wichita.edu
OI Palmer, Evan/0000-0003-2791-4390
CR [Anonymous], 1996, ACM Transactions, DOI DOI 10.1145/230562.230563
   ANSTIS SM, 1974, VISION RES, V14, P589, DOI 10.1016/0042-6989(74)90049-2
   BECK J, 1994, PERCEPT PSYCHOPHYS, V56, P424, DOI 10.3758/BF03206734
   BENEL RA, 1998, HUMAN FACTORS AIR TR, P17
   BURNETT MS, 1991, P HUM FACT SOC 35 M, P87
   Clausner TC, 1997, COGNITIVE SCI, V21, P247, DOI 10.1016/S0364-0213(99)80024-X
   Clausner TC, 1999, COGN LINGUIST, V10, P1, DOI 10.1515/cogl.1999.001
   CLAUSNER TC, 2002, P 24 ANN C COGN SCI, P208
   DAVINCI L, 1883, NOTEBOOKS L DAVINCI, P159
   DUNCAN J, 1989, PSYCHOL REV, V96, P433, DOI 10.1037/0033-295X.96.3.433
   EGETH HE, 1984, J EXP PSYCHOL HUMAN, V10, P32, DOI 10.1037/0096-1523.10.1.32
   ELLIS SR, 1987, HUM FACTORS, V29, P371, DOI 10.1177/001872088702900401
   ELLIS SR, 1981, 81341 NASA TM AM RES
   Gibson E. J., 1969, Principles of perceptual learning and development
   HE ZJ, 1992, NATURE, V359, P231, DOI 10.1038/359231a0
   HOCHBERG J E, 1955, Am J Psychol, V68, P294, DOI 10.2307/1418903
   Holway AH, 1941, AM J PSYCHOL, V54, P21, DOI 10.2307/1417790
   HUNT SMJ, 1994, BEHAV RES METH INSTR, V26, P345, DOI 10.3758/BF03204643
   Johnson Mark, 1987, The Body in the Mind: The Bodily Basis. Meaning, Imagination, and Reason
   JOHNSTON JC, 1993, P 7 INT S AV PSYCH O
   Kellman P.J., 2002, Steven's Handbook of Experimental Psychology, (Vol. 3). Learning, Motivation, and Emotion, V3
   KELLMAN PJ, 1994, HUM FAC ERG SOC P, P1183
   LAKOFF G, 1980, COGNITIVE SCI, V4, P195, DOI 10.1016/S0364-0213(80)80017-6
   LAKOFF G, 1993, METAPHOR THOUGHT ORT
   Nichols S, 2002, APPL ERGON, V33, P251, DOI 10.1016/S0003-6870(02)00020-0
   RAYNER K, 1980, COGNITIVE PSYCHOL, V12, P206, DOI 10.1016/0010-0285(80)90009-2
   Remington RW, 2000, HUM FACTORS, V42, P349, DOI 10.1518/001872000779698105
   Roske-Hofstrand R.J., 1998, Human Factors in Air Traffic Control, P65
   SMITH JD, 1984, HUM FACTORS, V26, P33, DOI 10.1177/001872088402600104
   St John M, 2001, HUM FACTORS, V43, P79, DOI 10.1518/001872001775992534
   STENGER AJ, 1981, 8061 US AFHRL TR
   TREISMAN AM, 1980, COGNITIVE PSYCHOL, V12, P97, DOI 10.1016/0010-0285(80)90005-5
   WOLFE JM, 1989, J EXP PSYCHOL HUMAN, V15, P419, DOI 10.1037/0096-1523.15.3.419
   WOLFE JM, 1994, PSYCHON B REV, V1, P202, DOI 10.3758/BF03200774
   Wolfe JM, 2004, NAT REV NEUROSCI, V5, P495, DOI 10.1038/nrn1411
   Wolfe JM, 1998, PSYCHOL SCI, V9, P33, DOI 10.1111/1467-9280.00006
NR 36
TC 7
Z9 11
U1 1
U2 6
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD JAN
PY 2008
VL 5
IS 1
AR 4
DI 10.1145/1279640.1279644
PG 22
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA 450YI
UT WOS:000266437500004
DA 2024-07-18
ER

PT J
AU Williams, B
   Narasimham, G
   Westerman, C
   Rieser, J
   Bodenheimer, B
AF Williams, Betsy
   Narasimham, Gayathri
   Westerman, Claire
   Rieser, John
   Bodenheimer, Bobby
TI Functional Similarities in Spatial Representations Between Real and
   Virtual Environments
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Experimentation; Human Factors; Measurement; Virtual reality (VR); space
   perception
ID FIELD-OF-VIEW; LOCOMOTION; ORIENTATION; DISPLAYS; BODY
AB This paper presents results that demonstrate functional similarities in subjects' access to spatial knowledge (or spatial representation) between real and virtual environments. Such representations are important components of the transfer of reasoning ability and knowledge between these two environments. In particular, we present two experiments aimed at investigating similarities in spatial knowledge derived from exploring on foot both physical environments and virtual environments presented through a head-mounted display. In the first experiment, subjects were asked to learn the locations of target objects in the real or virtual environment and then rotate the perspective by either physically locomoting to a new facing direction or imagining moving. The latencies and errors were generally worse after imagining locomoting and for greater degrees of rotation in perspective; they did not differ significantly across knowledge derived from exploring the physical versus virtual environments. In the second experiment, subjects were asked to imagine simple rotations versus simple translations in perspective. The errors and latencies indicated that the to-be-imagined disparity was linearly related after learning the physical and virtual environment. These results demonstrate functional similarities in access to knowledge of new perspective when it is learned by exploring physical environments and virtual renderings of the same environment.
C1 [Williams, Betsy; Narasimham, Gayathri; Westerman, Claire; Rieser, John; Bodenheimer, Bobby] Vanderbilt Univ, Nashville, TN 37235 USA.
   [Narasimham, Gayathri; Westerman, Claire; Rieser, John] Dept Psychol & Human Dev, Nashville, TN 37203 USA.
C3 Vanderbilt University
RP Williams, B (corresponding author), Vanderbilt Univ, VU Stn B 351679,2301 Vanderbilt Pl, Nashville, TN 37235 USA.
EM betsy.williams@vanderbilt.edu; gayathri.narasimham@vanderbilt.edu;
   claire.b.westerman@vanderbilt.edu; john.j.rieser.2@vanderbilt.edu;
   bobby.bodenheimer@vanderbilt.edu
FU National Science Foundation [IIS-0237621, IIS-0121084]
FX The authors would like to thank the reviewers for their insightful and
   constructive comments. We further thank Tim McNamara and Kaysi Holman
   for help and advice. This material is based upon work supported by the
   National Science Foundation under Grants IIS-0237621 and IIS-0121084,
   and by a Vanderbilt Discovery Grant. Any opinions, findings, and
   conclusions or recommendations expressed in this material are those of
   the authors and do not necessarily reflect the views of the sponsors.
CR Brockmole JR, 2003, COGNITION, V87, pB59, DOI 10.1016/S0010-0277(02)00231-7
   Chance SS, 1998, PRESENCE-TELEOP VIRT, V7, P168, DOI 10.1162/105474698565659
   EASTON RD, 1995, J EXP PSYCHOL LEARN, V21, P483, DOI 10.1037/0278-7393.21.2.483
   FARRELL M.J., 1998, J. Exp. Psych: Learn., Mem., V24, P993
   Harrell F.E., 2001, Regression modeling strategies: with applications to linear models, logistic regression, and survival analysis
   Kay BA, 2001, BIOL CYBERN, V85, P89, DOI 10.1007/PL00008002
   Klatzky RL, 1998, PSYCHOL SCI, V9, P293, DOI 10.1111/1467-9280.00058
   Knapp JM, 2004, PRESENCE-TELEOP VIRT, V13, P572, DOI 10.1162/1054746042545238
   Loomis J.M., 2003, Virtual and adaptive environments, P21
   Loomis JM, 1999, BEHAV RES METH INS C, V31, P557, DOI 10.3758/BF03200735
   Loomis JM, 1999, WAYFINDING BEHAVIOR, P125
   May M, 2000, J EXP PSYCHOL LEARN, V26, P169, DOI 10.1037//0278-7393.26.1.169
   May M, 2004, COGNITIVE PSYCHOL, V48, P163, DOI 10.1016/S0010-0285(03)00127-0
   May M, 1996, MEMORY AND PROCESSING OF VISUAL AND SPATIAL INFORMATION, P418
   MOHLER B.J., 2004, Symposium on Appliced Perception in Graphics and Visualization, P19
   Mohler BJ, 2007, ACM T APPL PERCEPT, V4, DOI [10.1145/1227134.1227138, 10.1145/1227134/1227138]
   Myers R., 2002, GEN LINEAR MODELS
   Newcombe N., 2000, Making space: The development of spatial representation and reasoning
   Oman Charles M, 2002, Spat Cogn Comput, V2, P355, DOI 10.1023/A:1015548105563
   Philbeck JW, 2001, J EXP PSYCHOL HUMAN, V27, P141, DOI 10.1037/0096-1523.27.1.141
   Plumert JM, 2004, CHILD DEV, V75, P1243, DOI 10.1111/j.1467-8624.2004.00736.x
   Plumert Jodie., 2005, ACM Transactions on Applied Perception, V2, P216, DOI DOI 10.1145/1077399.1077402
   PRESSON CC, 1994, PERCEPTION, V23, P1447, DOI 10.1068/p231447
   Psotka J, 1998, PRESENCE-TELEOP VIRT, V7, P352, DOI 10.1162/105474698565776
   Riecke B.E., 2005, ACM Transactions on Applied Perception (TAP), V2, P183, DOI [10.1145/1077399.1077401, DOI 10.1145/1077399.1077401]
   RIESER JJ, 1994, CHILD DEV, V65, P1262, DOI 10.1111/j.1467-8624.1994.tb00816.x
   RIESER JJ, 1989, J EXP PSYCHOL LEARN, V15, P1157, DOI 10.1037/0278-7393.15.6.1157
   RIESER JJ, 1986, PERCEPTION, V15, P173, DOI 10.1068/p150173
   SATTERTHWAITE FE, 1946, BIOMETRICS BULL, V2, P110, DOI 10.2307/3002019
   Satterthwaite FE, 1941, PSYCHOMETRIKA, V6, P309, DOI 10.1007/BF02288586
   Thompson WB, 2004, PRESENCE-TELEOP VIRT, V13, P560, DOI 10.1162/1054746042545292
   Waller D, 2004, J ENVIRON PSYCHOL, V24, P105, DOI 10.1016/S0272-4944(03)00051-3
   Wang RXF, 2005, PSICOLOGICA, V26, P25
   Willemsen P, 2003, P IEEE VIRT REAL ANN, P79, DOI 10.1109/VR.2003.1191124
   WILLEMSEN P., 2004, APGV '04: Proceedings of the 1st Symposium on Applied perception in graphics and visualization, P36
   Witmer BG, 1998, HUM FACTORS, V40, P478, DOI 10.1518/001872098779591340
   Wraga M, 2003, J EXP PSYCHOL LEARN, V29, P993, DOI 10.1037/0278-7393.29.5.993
   Wraga M, 2004, MEM COGNITION, V32, P399, DOI 10.3758/BF03195834
   Wu B, 2004, NATURE, V428, P73, DOI 10.1038/nature02350
NR 39
TC 22
Z9 27
U1 0
U2 2
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD JUL
PY 2007
VL 4
IS 2
AR 12
DI 10.1145/1265957.1265961
PG 22
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA V04IM
UT WOS:000207052100004
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Surace, L
   Wernikowski, M
   Tursun, C
   Myszkowski, K
   Mantiuk, R
   Didyk, P
AF Surace, Luca
   Wernikowski, Marek
   Tursun, Cara
   Myszkowski, Karol
   Mantiuk, Radoslaw
   Didyk, Piotr
TI Learning GAN-Based Foveated Reconstruction to Recover Perceptually
   Important Image Features
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Foveated rendering; image reconstruction; generative adversarial
   network; perception
ID SPATIAL PHASE; QUALITY; UNCERTAINTY; VISIBILITY; MULTISCALE; RESOLUTION;
   ERROR
AB A foveated image can be entirely reconstructed from a sparse set of samples distributed according to the retinal sensitivity of the human visual system, which rapidly decreases with increasing eccentricity. The use of generative adversarial networks (GANs) has recently been shown to be a promising solution for such a task, as they can successfully hallucinate missing image information. As in the case of other supervised learning approaches, the definition of the loss function and the training strategy heavily influence the quality of the output. In this work,we consider the problem of efficiently guiding the training of foveated reconstruction techniques such that they are more aware of the capabilities and limitations of the human visual system, and thus can reconstruct visually important image features. Our primary goal is to make the training procedure less sensitive to distortions that humans cannot detect and focus on penalizing perceptually important artifacts. Given the nature of GAN-based solutions, we focus on the sensitivity of human vision to hallucination in case of input samples with different densities. We propose psychophysical experiments, a dataset, and a procedure for training foveated image reconstruction. The proposed strategy renders the generator network flexible by penalizing only perceptually important deviations in the output. As a result, the method emphasized the recovery of perceptually important image features. We evaluated our strategy and compared it with alternative solutions by using a newly trained objective metric, a recent foveated video quality metric, and user experiments. Our evaluations revealed significant improvements in the perceived image reconstruction quality compared with the standard GAN-based training approach.
C1 [Surace, Luca; Didyk, Piotr] Univ Svizzera Italiana, Via Giuseppe Buffi 13, CH-6900 Lugano, Switzerland.
   [Wernikowski, Marek; Mantiuk, Radoslaw] West Pomeranian Univ Technol, Al Piastow 17, PL-70310 Szczecin, Poland.
   [Tursun, Cara] Univ Groningen, Bldg 5161, NL-6GRP 3G Groningen, Netherlands.
   [Myszkowski, Karol] Max Planck Inst Informat, D-66123 Saarbrucken, Germany.
C3 Universita della Svizzera Italiana; West Pomeranian University of
   Technology; University of Groningen; Max Planck Society
RP Surace, L (corresponding author), Univ Svizzera Italiana, Via Giuseppe Buffi 13, CH-6900 Lugano, Switzerland.
EM luca.surace@usi.ch; marek.wernikowski@zut.edu.pl; cara.tursun@rug.nl;
   karol@mpi-inf.mpg.de; rmantiuk@wi.zut.edu.pl; piotr.didyk@usi.ch
OI /0000-0002-9729-5460; Didyk, Piotr/0000-0003-0768-8939; Wernikowski,
   Marek/0000-0001-7862-1155; Myszkowski, Karol/0000-0002-8505-4141
FU European Research Council (ERC) under the European Union [804226];
   European Research Council (ERC) [804226] Funding Source: European
   Research Council (ERC)
FX This project received funding from the European Research Council (ERC)
   under the European Union's Horizon 2020 research and innovation program
   (grant agreement no. 804226 PERDY).
CR Adhikarla VK, 2017, PROC CVPR IEEE, P3720, DOI 10.1109/CVPR.2017.396
   Arjovsky M, 2017, Arxiv, DOI [arXiv:1701.07875, DOI 10.48550/ARXIV.1701.07875]
   Balas B, 2009, J VISION, V9, DOI 10.1167/9.12.13
   Barten Peter G. J, 1999, Contrast sensitivity of the human eye and its effects on image quality
   Branch MA, 1999, SIAM J SCI COMPUT, V21, P1, DOI 10.1137/S1064827595289108
   Bruder Valentin, 2019, EUROVIS, V5
   Chwesiuk M, 2019, ACM CONFERENCE ON APPLIED PERCEPTION (SAP 2019), DOI 10.1145/3343036.3343123
   CURCIO CA, 1990, J COMP NEUROL, V292, P497, DOI 10.1002/cne.902920402
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Deza Arturo, 2017, Towards metamerism via foveated style transfer
   Dosovitskiy A., 2016, Advances in Neural Information Processing Systems, P658
   Feather J, 2019, ADV NEUR IN, V32
   Fridman L, 2017, Arxiv, DOI arXiv:1706.04568
   Gatys L., 2015, NIPS
   Goodfellow I, 2014, ADV NEURAL INFORM PR, P2672
   Guenter B, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366183
   Guo PY, 2018, Arxiv, DOI arXiv:1802.09065
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hepburn A, 2020, Arxiv, DOI arXiv:1908.04347
   HESS RF, 1993, VISION RES, V33, P2663, DOI 10.1016/0042-6989(93)90226-M
   Hsu CF, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P55, DOI 10.1145/3123266.3123434
   Gulrajani I, 2017, ADV NEUR IN, V30
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kaplanyan A, 2019, SIGGRAPH '19 -ACM SIGGRAPH 2019 TALKS, DOI 10.1145/3306307.3328186
   Kim J, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3322987
   Kim Kil Joong, 2013, HUMAN VISION ELECT I, V8651, P1
   Koob GF, 2010, ENCYCLOPEDIA OF BEHAVIORAL NEUROSCIENCE, VOL 3: P-Z, pXXX
   Lee S, 2002, IEEE T MULTIMEDIA, V4, P129, DOI 10.1109/6046.985561
   Levenberg K., 1944, Quarterly of Applied Mathematics, V2, P164, DOI [10.1090/QAM/10666, DOI 10.1090/QAM/10666]
   LEVI DM, 1987, VISION RES, V27, P581, DOI 10.1016/0042-6989(87)90044-7
   MANNOS JL, 1974, IEEE T INFORM THEORY, V20, P525, DOI 10.1109/TIT.1974.1055250
   Mantiuk RK, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459831
   Meng XX, 2018, P ACM COMPUT GRAPH, V1, DOI 10.1145/3203199
   MORRONE MC, 1989, VISION RES, V29, P433, DOI 10.1016/0042-6989(89)90007-2
   Odena A., 2016, DISTILL, V1, pe3, DOI 10.23915/distill.00003.-URL
   Patney A, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980246
   PELI E, 1991, J OPT SOC AM A, V8, P1762, DOI 10.1364/JOSAA.8.001762
   Portilla J, 2000, INT J COMPUT VISION, V40, P49, DOI 10.1023/A:1026553619983
   RENTSCHLER I, 1985, NATURE, V313, P308, DOI 10.1038/313308a0
   RICHARDS FJ, 1959, J EXP BOT, V10, P290, DOI 10.1093/jxb/10.2.290
   Rimac-Drlje S, 2010, MULTIMED TOOLS APPL, V49, P425, DOI 10.1007/s11042-009-0442-1
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Rosenholtz R, 2016, ANNU REV VIS SCI, V2, P437, DOI 10.1146/annurev-vision-082114-035733
   Rosenholtz R, 2012, J VISION, V12, DOI 10.1167/12.4.14
   Rosenholtz R, 2012, FRONT PSYCHOL, V3, DOI 10.3389/fpsyg.2012.00013
   Rosenholtz R, 2011, PROC SPIE, V7865, DOI 10.1117/12.876659
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Solari F, 2012, PATTERN RECOGN LETT, V33, P41, DOI 10.1016/j.patrec.2011.09.021
   Stengel M, 2016, COMPUT GRAPH FORUM, V35, P129, DOI 10.1111/cgf.12956
   Strasburger H, 2011, J VISION, V11, DOI 10.1167/11.5.13
   Swafford Nicholas T., 2016, P ACM S APPL PERC, P7, DOI [10.1145/2931002.2931011, DOI 10.1145/2931002.2931011]
   Tran HTT, 2019, Arxiv, DOI arXiv:1908.06239
   Tariq Taimoor, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12367), P445, DOI 10.1007/978-3-030-58542-6_27
   Tariq T, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530101
   THIBOS LN, 1987, J OPT SOC AM A, V4, P1524, DOI 10.1364/JOSAA.4.001524
   Tsai WJ, 2014, 2014 IEEE VISUAL COMMUNICATIONS AND IMAGE PROCESSING CONFERENCE, P25, DOI 10.1109/VCIP.2014.7051495
   Tursun OT, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3322985
   ULICHNEY R, 1993, P SOC PHOTO-OPT INS, V1913, P332, DOI 10.1117/12.152707
   Vranjes M, 2018, MULTIMED TOOLS APPL, V77, P21053, DOI 10.1007/s11042-017-5544-6
   Walton DR, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459943
   Wandell Brian, 1997, Psyccritiques, V42, P1
   Wang PQ, 2017, J VISION, V17, DOI 10.1167/17.4.9
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang Z, 2001, PROC SPIE, V4472, P42, DOI 10.1117/12.449797
   Watson AB, 2014, J VISION, V14, DOI 10.1167/14.7.15
   Weier M, 2017, COMPUT GRAPH FORUM, V36, P611, DOI 10.1111/cgf.13150
   Wolski K, 2019, COMPUT GRAPH FORUM, V38, P685, DOI 10.1111/cgf.13871
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
NR 70
TC 0
Z9 0
U1 0
U2 0
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD APR
PY 2023
VL 20
IS 2
DI 10.1145/3583072
PG 23
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA H6MI8
UT WOS:000997078100002
OA Green Published, Green Submitted, Bronze
DA 2024-07-18
ER

PT J
AU Brickler, D
   Babu, SV
AF Brickler, David
   Babu, Sabarish, V
TI An Evaluation of Screen Parallax, Haptic Feedback, and Sensory-Motor
   Mismatch on Near-Field Perception-Action Coordination in VR
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article; Proceedings Paper
CT 18th Symposium on Applied Perceptiion (SAP)
CY SEP, 2021
CL ELECTR NETWORK
DE Screen parallax; near-field VR; perception-action; haptics; stereoscopy;
   voxalization; VR
ID PERFORMANCE
AB Virtual reality (VR) displays have factors such as vergence-accommodation conflicts that negatively impact depth perception and cause users to misjudge distances to select objects. In addition, popular large-screen immersive displays present the depth of any target rendered through screen parallax information of points, which are encapsulated within stereoscopic voxels that are a distinct unit of space dictating how far an object is placed in front of or behind the screen. As they emanate from the viewers' eyes (left and right center of projection), the density of voxels is higher in front of the screen (in regions of negative screen parallax) than it is behind the screen (in regions of positive screen parallax), implying a higher spatial resolution of depth in front of the screen than behind the screen. Our experiment implements a near-field fine-motor pick-and-place task in which users pick up a ring and place it around a targeted peg. The targets are arranged in a linear configuration of 3, 5, and 7 pegs along the front-and-back axis with the center peg placed in the same depth as the screen. We use this to evaluate how users manipulate objects in positive versus negative screen parallax space by the metrics of efficiency, accuracy, and economy of movement. In addition, we evaluate how users' performance is moderated by haptic feedback and mismatch between visual and proprioceptive information. Our results reveal that users perform more efficiently in negative screen parallax space and that haptic feedback and visuo-proprioceptive mismatch have effects on placement efficiency. The implications of these findings are described in the later sections of the article.
C1 [Brickler, David; Babu, Sabarish, V] Clemson Univ, 105 Sikes Hall, Clemson, SC 29634 USA.
C3 Clemson University
RP Brickler, D (corresponding author), Clemson Univ, 105 Sikes Hall, Clemson, SC 29634 USA.
EM davidpbrick-ler@gmail.com; sbabu@clemson.edu
FU National Science Foundation [2007435]; Direct For Computer & Info Scie &
   Enginr; Div Of Information & Intelligent Systems [2007435] Funding
   Source: National Science Foundation
FX The authors acknowledge all the participants for graciously providing
   their time in participating in our empirical evaluation. This
   contribution is based upon work partially supported by the National
   Science Foundation under Grant No. 2007435.
CR Adelson S. J., 1991, Proceedings of the S.I.D., V32, P25
   [Anonymous], 1994, P ASME WINT ANN M S, DOI DOI 10.1145/1029632.1029682
   Batmaz AnilUfuk., 2019, Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems, P1
   Bhargava A, 2018, IEEE T VIS COMPUT GR, V24, P1418, DOI 10.1109/TVCG.2018.2794639
   Brickler D, 2020, ACM T APPL PERCEPT, V17, DOI 10.1145/3419986
   Brickler D, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P28, DOI [10.1109/vr.2019.8797744, 10.1109/VR.2019.8797744]
   Bruder G, 2015, P IEEE VIRT REAL ANN, P27, DOI 10.1109/VR.2015.7223320
   Coles TR, 2011, IEEE T HAPTICS, V4, P199, DOI [10.1109/TOH.2011.32, 10.1109/ToH.2011.32]
   Culbertson H, 2017, IEEE T HAPTICS, V10, P63, DOI 10.1109/TOH.2016.2598751
   Cutting JE, 1995, PERCEPTION SPACE MOT, P69, DOI [DOI 10.1016/B978-012240530-3/50005-5, 10.1016/B978-012240530-3/50005-5]
   Ebrahimi E, 2016, ACM T APPL PERCEPT, V13, DOI 10.1145/2947617
   Esmaeili S, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR 2020), P453, DOI [10.1109/VR46266.2020.1581285352835, 10.1109/VR46266.2020.00-38]
   Figueiredo LucasS., 2014, International Conference of Design, User Experience, and Usability, P560
   Fried GM, 2008, J GASTROINTEST SURG, V12, P210, DOI 10.1007/s11605-007-0355-0
   Fu MJ, 2011, IEEE INT C INT ROBOT, P3460, DOI 10.1109/IROS.2011.6048296
   Guilford JP, 1948, J APPL PSYCHOL, V32, P24, DOI 10.1037/h0063610
   HART S G, 1988, P139
   Hodges LarryF., 1993, Presence: Teleoperators Virtual Environments, V2, P34, DOI 10.1162/pres.1993.2.1.34
   Hodges LarryF., 2020, Electro-Optical Displays, P291
   Hoffman DM, 2008, J VISION, V8, DOI 10.1167/8.3.33
   Jones JA, 2016, 2016 IEEE SYMPOSIUM ON 3D USER INTERFACES (3DUI), P221, DOI 10.1109/3DUI.2016.7460055
   Jones Michael Wayne, 1996, THESIS U ALABAMA HUN
   Khalifa A, 2014, 2014 19TH INTERNATIONAL CONFERENCE ON METHODS AND MODELS IN AUTOMATION AND ROBOTICS (MMAR), P675, DOI 10.1109/MMAR.2014.6957435
   Kohli L., 2012, 2012 IEEE Symposium on 3D User Interfaces (3DUI), P105, DOI 10.1109/3DUI.2012.6184193
   Li JL, 2016, SUI'18: PROCEEDINGS OF THE 2018 SYMPOSIUM ON SPATIAL USER INTERACTION, P120, DOI 10.1145/3267782.3267797
   Machuca MDB, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300437
   MacKenzie I. S., 1992, CHI '92 Conference Proceedings. ACM Conference on Human Factors in Computing Systems. Striking a Balance, P219, DOI 10.1145/142750.142794
   MacKenzie IS, 2008, CHI 2008: 26TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS VOLS 1 AND 2, CONFERENCE PROCEEDINGS, P1633
   McIntire JP, 2014, DISPLAYS, V35, P18, DOI 10.1016/j.displa.2013.10.004
   McWhorter ShaneWilliam., 1991, Evaluation of display parameters affecting user performance of an interactive task in a virtual environment
   Minamizawa Kouta, 2010, 2010 IEEE Haptics Symposium (Formerly known as Symposium on Haptic Interfaces for Virtual Environment and Teleoperator Systems), P257, DOI 10.1109/HAPTIC.2010.5444646
   Morris D, 2007, WORLD HAPTICS 2007: SECOND JOINT EUROHAPTICS CONFERENCE AND SYMPOSIUM ON HAPTIC INTERFACES FOR VIRTUAL ENVIRONMENT AND TELEOPERATOR SYSTEMS, PROCEEDINGS, P21
   Napieralski PE, 2011, ACM T APPL PERCEPT, V8, DOI 10.1145/2010325.2010328
   Okamura AM, 1998, IEEE INT CONF ROBOT, P674, DOI 10.1109/ROBOT.1998.677050
   Panait L, 2009, J SURG RES, V156, P312, DOI 10.1016/j.jss.2009.04.018
   Poyade M., 2012, JOINT VIRT REAL C IC, P85
   Poyade M., 2014, HAPTIC PLUG IN UNITY
   Sroka G, 2010, AM J SURG, V199, P115, DOI 10.1016/j.amjsurg.2009.07.035
   Wartrell Z, 1999, COMP GRAPH, P351, DOI 10.1145/311535.311587
   Woodworth RS, 1899, PSYCHOL REV-MONOGR S, V3, P1
NR 40
TC 0
Z9 0
U1 1
U2 5
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD OCT
PY 2021
VL 18
IS 4
AR 20
DI 10.1145/3486583
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA YY1DS
UT WOS:000754533100004
DA 2024-07-18
ER

PT J
AU Kenny, S
   Mahmood, N
   Honda, C
   Black, MJ
   Troje, NF
AF Kenny, Sophie
   Mahmood, Naureen
   Honda, Claire
   Black, Michael J.
   Troje, Nikolaus F.
TI Perceptual Effects of Inconsistency in Human Animations
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Retargeting; human animation; animated avatars; shape capture; realism;
   inconsistency; perception; discrimination; action
ID BIOLOGICAL MOTION; VISUAL-PERCEPTION; UNCANNY VALLEY; PERSON
   IDENTIFICATION; LIFTED WEIGHT; DISPLAYS
AB The individual shape of the human body, including the geometry of its articulated structure and the distribution of weight over that structure, influences the kinematics of a person's movements. How sensitive is the visual system to inconsistencies between shape and motion introduced by retargeting motion from one person onto the shape of another? We used optical motion capture to record five pairs of male performers with large differences in body weight, while they pushed, lifted, and threw objects. From these data, we estimated both the kinematics of the actions as well as the performer's individual body shape. To obtain consistent and inconsistent stimuli, we created animated avatars by combining the shape and motion estimates from either a single performer or from different performers. Using these stimuli we conducted three experiments in an immersive virtual reality environment. First, a group of participants detected which of two stimuli was inconsistent. Performance was very low, and results were only marginally significant. Next, a second group of participants rated perceived attractiveness, eeriness, and humanness of consistent and inconsistent stimuli, but these judgements of animation characteristics were not affected by consistency of the stimuli. Finally, a third group of participants rated properties of the objects rather than of the performers. Here, we found strong influences of shape-motion inconsistency on perceived weight and thrown distance of objects. This suggests that the visual system relies on its knowledge of shape and motion and that these components are assimilated into an altered perception of the action outcome. We propose that the visual system attempts to resist inconsistent interpretations of human animations. Actions involving object manipulations present an opportunity for the visual system to reinterpret the introduced inconsistencies as a change in the dynamics of an object rather than as an unexpected combination of body shape and body motion.
C1 [Kenny, Sophie; Honda, Claire; Troje, Nikolaus F.] Queens Univ, Dept Psychol, Kingston, ON K7L 3N6, Canada.
   [Mahmood, Naureen; Black, Michael J.] Max Planck Inst Intelligent Syst, D-72076 Tubingen, Germany.
   [Troje, Nikolaus F.] York Univ, Ctr Vis Res, Toronto, ON M3J 1P3, Canada.
C3 Queens University - Canada; Max Planck Society; York University - Canada
RP Kenny, S (corresponding author), Queens Univ, Dept Psychol, Kingston, ON K7L 3N6, Canada.
EM kenny.s@queensu.ca; nmahmood@tue.mpg.de; claire.honda@queensu.ca;
   black@tuebingen.mpg.de; troje@yorku.ca
RI Troje, Nikolaus/F-1107-2010
OI Troje, Nikolaus/0000-0002-1533-2847; Kenny, Sophie/0000-0002-7908-1506;
   Mahmood, Naureen/0000-0001-6285-785X
FU NSERC; Humboldt Research Award; NSERC Alexander Graham-Bell Graduate
   Scholarship; International Research and Training Grant "The Brain in
   Action" (CREATE/DFG); Intel; Nvidia; Adobe; Facebook; Amazon; MPI
FX This work was supported by a NSERC Discovery grant and the Humboldt
   Research Award to NFT, as well as a NSERC Alexander Graham-Bell Graduate
   Scholarship to S.K. Additional travel funds were provided to S.K. by the
   International Research and Training Grant "The Brain in Action"
   (CREATE/DFG). M.J.B. has received research gift funds from Intel,
   Nvidia, Adobe, Facebook, and Amazon. While M.J.B. is a part-time
   employee of Amazon, his research was performed solely at, and funded
   solely by, MPI. N.M. is a founder and CEO at Meshcapade GmbH, which is
   commercializing body modeling technology. Her research was performed
   solely at, and funded solely by, MPI.
CR Anguelov D, 2005, ACM T GRAPHIC, V24, P408, DOI 10.1145/1073204.1073207
   [Anonymous], 2013, PEOPLE WATCHING SOCI
   [Anonymous], 2014, UNCANNY VALLEY GAMES
   BINGHAM GP, 1987, J EXP PSYCHOL HUMAN, V13, P155, DOI 10.1037/0096-1523.13.2.155
   BINGHAM GP, 1993, ECOL PSYCHOL, V5, P31, DOI 10.1207/s15326969eco0501_2
   Brenton H., 2005, Proceedings of the 19th British HCI Group Annual Conference, P1
   COHEN CE, 1981, J PERS SOC PSYCHOL, V40, P441
   Douketis JD, 2005, CAN MED ASSOC J, V172, P995, DOI 10.1503/cmaj.045170
   Ennis Cathy, 2015, EUROGRAPHICS, P47
   Geng WD, 2003, LECT NOTES COMPUT SC, V2669, P620
   Gibson J., 1979, The ecological approach to visual perception
   Gleicher M., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P33, DOI 10.1145/280814.280820
   Grèzes J, 2004, J NEUROSCI, V24, P5500, DOI 10.1523/JNEUROSCI.0219-04.2004
   Hamilton AFD, 2007, PSYCHOL RES-PSYCH FO, V71, P13, DOI 10.1007/s00426-005-0032-4
   Hirshberg DA, 2012, LECT NOTES COMPUT SC, V7577, P242, DOI 10.1007/978-3-642-33783-3_18
   Ho CC, 2010, COMPUT HUM BEHAV, V26, P1508, DOI 10.1016/j.chb.2010.05.015
   Hodgins J, 2010, ACM T APPL PERCEPT, V7, DOI 10.1145/1823738.1823740
   Hoffman James, 2014, THESIS
   Hoyet L., 2013, ACM T GRAPHIC, V32, P1
   JOHANSSON G, 1973, PERCEPT PSYCHOPHYS, V14, P201, DOI 10.3758/BF03212378
   Johnson KL, 2007, P NATL ACAD SCI USA, V104, P5246, DOI 10.1073/pnas.0608181104
   Johnson MH, 2006, CURR BIOL, V16, pR376, DOI 10.1016/j.cub.2006.04.008
   Jokisch D, 2003, J VISION, V3, P252, DOI 10.1167/3.4.1
   Kätsyri J, 2015, FRONT PSYCHOL, V6, DOI 10.3389/fpsyg.2015.00390
   Klüver M, 2016, EVOL HUM BEHAV, V37, P40, DOI 10.1016/j.evolhumbehav.2015.07.001
   LOPER M. M., 2014, ACM T GRAPHICS, V33, P6
   MacDorman KF, 2006, INTERACT STUD, V7, P297, DOI 10.1075/is.7.3.03mac
   MATHER G, 1994, P ROY SOC B-BIOL SCI, V258, P273, DOI 10.1098/rspb.1994.0173
   Matsui D., 2005, IEEE/RSJ International Conference on Intelligent Robots and Systems, P1
   McDonnell R, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360625
   McDonnell R, 2007, APGV 2007: SYMPOSIUM ON APPLIED PERCEPTION IN GRAPHICS AND VISUALIZATION, PROCEEDINGS, P7
   Menache A, 2011, UNDERSTANDING MOTION CAPTURE FOR COMPUTER ANIMATION, 2ND EDITION, P1
   Munzert J, 2010, EUR J COGN PSYCHOL, V22, P247, DOI 10.1080/09541440902757975
   Piwek L, 2014, COGNITION, V130, P271, DOI 10.1016/j.cognition.2013.11.001
   Raievsky Clement, 2009, SCITECH BOOK NEWS, P247
   RUNESON S, 1983, J EXP PSYCHOL GEN, V112, P585, DOI 10.1037/0096-3445.112.4.585
   RUNESON S, 1981, J EXP PSYCHOL HUMAN, V7, P733, DOI 10.1037/0096-1523.7.4.733
   RUNESON S, 1977, SCAND J PSYCHOL, V18, P172, DOI 10.1111/j.1467-9450.1977.tb00274.x
   Saunders DR, 2010, J VISION, V10, DOI 10.1167/10.11.9
   Saygin AP, 2012, SOC COGN AFFECT NEUR, V7, P413, DOI 10.1093/scan/nsr025
   Shim J, 2004, PERCEPTION, V33, P277, DOI 10.1068/p3434
   Shim J, 1997, J MOTOR BEHAV, V29, P131, DOI 10.1080/00222899709600828
   Thompson JC, 2011, PERCEPTION, V40, P695, DOI 10.1068/p6900
   Tinwell A, 2013, COMPUT HUM BEHAV, V29, P1617, DOI 10.1016/j.chb.2013.01.008
   Troje N. F., 2008, UNDERSTANDING EVENTS, P308, DOI [10.1093/acprof:oso/9780195188370.003.0014, DOI 10.1093/ACPROF:OSO/9780195188370.003.0014]
   Troje N.F., 2012, Journal of Vision, V12, P466
   Troje N.F., 2002, Dynamic Perception, P115
   Troje NF, 2005, PERCEPT PSYCHOPHYS, V67, P667, DOI 10.3758/BF03193523
   Troje NF, 2002, J VISION, V2, P371, DOI 10.1167/2.5.2
   Troje NF, 2013, SOCIAL PERCEPTION: DETECTION AND INTERPRETATION OF ANIMACY, AGENCY, AND INTENTION, P13
   Wages R, 2004, LECT NOTES COMPUT SC, V3166, P216
   Westhoff C, 2007, PERCEPT PSYCHOPHYS, V69, P241, DOI 10.3758/BF03193746
   Zhu Q, 2014, ECOL PSYCHOL, V26, P229, DOI 10.1080/10407413.2014.957969
NR 53
TC 5
Z9 6
U1 1
U2 6
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD FEB
PY 2019
VL 16
IS 1
AR 2
DI 10.1145/3301411
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA HN7TC
UT WOS:000460393600002
OA Green Submitted, Bronze
DA 2024-07-18
ER

PT J
AU Weier, M
   Roth, T
   Hinkenjann, A
   Slusallek, P
AF Weier, Martin
   Roth, Thorsten
   Hinkenjann, Andre
   Slusallek, Philipp
TI Foveated Depth-of-Field Filtering in Head-Mounted Displays
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article; Proceedings Paper
CT 15th ACM Symposium on Applied Perception (SAP) colocated with SIGGRAPH
   Conference
CY AUG, 2018
CL Vancouver, CANADA
SP ACM, SIGGRAPH
DE Gaze-contingent depth-of-field; foveated rendering; ray tracing;
   eye-tracking
AB In recent years, a variety of methods have been introduced to exploit the decrease in visual acuity of peripheral vision, known as foveated rendering. As more and more computationally involved shading is requested and display resolutions increase, maintaining low latencies is challenging when rendering in a virtual reality context. Here, foveated rendering is a promising approach for reducing the number of shaded samples. However, besides the reduction of the visual acuity, the eye is an optical system, filtering radiance through lenses. The lenses create depth-of-field (DoF) effects when accommodated to objects at varying distances. The central idea of this article is to exploit these effects as a filtering method to conceal rendering artifacts. To showcase the potential of such filters, we present a foveated rendering system, tightly integrated with a gaze-contingent DoF filter. Besides presenting benchmarks of the DoF and rendering pipeline, we carried out a perceptual study, showing that rendering quality is rated almost on par with full rendering when using DoF in our foveated mode, while shaded samples are reduced by more than 69%.
C1 [Weier, Martin; Roth, Thorsten; Hinkenjann, Andre] Hsch Bonn Rhein Sieg, IVC, Grantham Allee 20, D-53757 St Augustin, Germany.
   [Weier, Martin] Saarland Univ, Saarbrucken, Germany.
   [Roth, Thorsten] Brunel Univ, Uxbridge, Middx, England.
   [Slusallek, Philipp] Saarland Univ, DFKI GmbH, Campus D3 2, D-66123 Saarbrucken, Germany.
   [Slusallek, Philipp] DFKI Saarbrucken, Saarbrucken, Germany.
C3 Hochschule Bonn Rhein Sieg; Saarland University; Brunel University;
   Saarland University; German Research Center for Artificial Intelligence
   (DFKI); German Research Center for Artificial Intelligence (DFKI)
RP Weier, M (corresponding author), Hsch Bonn Rhein Sieg, IVC, Grantham Allee 20, D-53757 St Augustin, Germany.; Weier, M (corresponding author), Saarland Univ, Saarbrucken, Germany.
EM martin.weier@h-brs.de; thorsten.roth@h-brs.de;
   andre.hinkenjann@h-brs.de; philipp.slusallek@dfki.de
OI Hinkenjann, Andre/0000-0002-8391-7652; Slusallek,
   Philipp/0000-0002-2189-2429; Weier, Martin/0000-0001-9685-5238
FU German Federal Ministry for Economic Affairs and Energy (BMWi)
   [ZF4120902]
FX The work is supported by the German Federal Ministry for Economic
   Affairs and Energy (BMWi) funding the MoVISO ZIM-project under Grant No.
   ZF4120902.
CR Albert R, 2017, ACM T APPL PERCEPT, V14, DOI 10.1145/3127589
   [Anonymous], 2011, P ACM SIGGRAPH S APP, DOI [10.1145/2077451.2077454, DOI 10.1145/2077451.2077454]
   [Anonymous], 2014, P SIGCHI C HUM FACT, DOI DOI 10.1145/2556288.2557089
   Arabadzhiyska E, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073642
   Barsky BA, 2008, REC ADV COMPUT ENG, P999
   Bukowski Michael, 2013, GPU PRO 4 ADV RENDER, V4, P175
   Demers Joe, 2004, GPU GEMS, V1
   Duchowski A.T., 2014, P ACM S APPL PERC, P39, DOI DOI 10.1145/2628257.2628259
   Fujita Masahiro, 2014, SIGGRAPH ASIA 14
   Gross H., 2005, HDB OPTICAL SYSTEMS, V4
   Guenter B, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366183
   Hillaire S, 2008, IEEE VIRTUAL REALITY 2008, PROCEEDINGS, P47
   Igehy H, 1999, COMP GRAPH, P179, DOI 10.1145/311535.311555
   Koulieris GA, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073622
   Lindeberg Tim, 2016, THESIS
   Mantiuk R, 2013, COMPUT GRAPH FORUM, V32, P163, DOI 10.1111/cgf.12036
   Mantiuk R, 2011, LECT NOTES COMPUT SC, V6944, P1, DOI 10.1007/978-3-642-23834-5_1
   Marroquim Ricardo, 2007, PBG@ Eurographics, P101
   McIntosh L, 2012, COMPUT GRAPH FORUM, V31, P1810, DOI 10.1111/j.1467-8659.2012.02097.x
   Meng X. X., 2018, P ACM COMPUT GRAPH, V1
   Mulder J.D., 2000, P ACM S VIRTUAL REAL, P129
   Patney A, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980246
   Pérard-Gayot A, 2017, COMPUT GRAPH FORUM, V36, P477, DOI 10.1111/cgf.13142
   Stengel M, 2016, COMPUT GRAPH FORUM, V35, P129, DOI 10.1111/cgf.12956
   TEMME LA, 1989, OPTOMETRY VISION SCI, V66, P106, DOI 10.1097/00006324-198902000-00008
   Vaidyanathan K., 2014, P HIGH PERF GRAPH, P9, DOI DOI 10.2312/HPG.20141089
   Vinnikov M, 2016, INT J HUM-COMPUT ST, V91, P37, DOI 10.1016/j.ijhcs.2016.03.001
   Weier M, 2017, COMPUT GRAPH FORUM, V36, P611, DOI 10.1111/cgf.13150
   Weier M, 2016, COMPUT GRAPH FORUM, V35, P289, DOI 10.1111/cgf.13026
   Weier Martin, 2018, P ACM ETRA 18, P19
   Yang L, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618481
NR 31
TC 16
Z9 19
U1 1
U2 8
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD OCT
PY 2018
VL 15
IS 4
AR 26
DI 10.1145/3238301
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA HJ4HT
UT WOS:000457135800004
DA 2024-07-18
ER

PT J
AU Albert, R
   Patney, A
   Luebke, D
   Kim, J
AF Albert, Rachel
   Patney, Anjul
   Luebke, David
   Kim, Joohwan
TI Latency Requirements for Foveated Rendering in Virtual Reality
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Foveated rendering; eye-tracking; latency
ID SACCADIC SUPPRESSION; CONTINGENT; RESOLUTION; DISPLAYS; LIMITS; TIME
AB Foveated rendering is a performance optimization based on thewell-known degradation of peripheral visual acuity. It reduces computational costs by showing a high-quality image in the user's central (foveal) vision and a lower quality image in the periphery. Foveated rendering is a promising optimization for Virtual Reality (VR) graphics, and generally requires accurate and low-latency eye tracking to ensure correctness even when a user makes large, fast eye movements such as saccades. However, due to the phenomenon of saccadic omission, it is possible that these requirements may be relaxed.
   In this article, we explore the effect of latency for foveated rendering in VR applications. We evaluated the detectability of visual artifacts for three techniques capable of generating foveated images and for three different radii of the high-quality foveal region. Our results show that larger foveal regions allow for more aggressive foveation, but this effect is more pronounced for temporally stable foveation techniques. Added eye tracking latency of 80-150ms causes a significant reduction in acceptable amount of foveation, but a similar decrease in acceptable foveation was not found for shorter eye-tracking latencies of 20-40ms, suggesting that a total system latency of 50-70ms could be tolerated.
C1 [Albert, Rachel] Univ Calif Berkeley, 523 Soda Hall, Berkeley, CA 94720 USA.
   [Patney, Anjul] NVIDIA, 11431 Willows Rd 200, Redmond, WA 98052 USA.
   [Luebke, David] NVIDIA, 410 Water St E Suite 200, Charlottesville, VA 22902 USA.
   [Kim, Joohwan] NVIDIA, 2701 San Tomas Expressway, Santa Clara, CA 95050 USA.
C3 University of California System; University of California Berkeley;
   Nvidia Corporation
RP Albert, R (corresponding author), Univ Calif Berkeley, 523 Soda Hall, Berkeley, CA 94720 USA.
OI Brown, Rachel/0000-0003-4779-7873
FU NVIDIA Research
FX This work was supported by NVIDIA Research.
CR Allison RS, 2010, EXP BRAIN RES, V202, P155, DOI 10.1007/s00221-009-2120-y
   ANDERSON SJ, 1991, J PHYSIOL-LONDON, V442, P47, DOI 10.1113/jphysiol.1991.sp018781
   BAHILL A T, 1975, Mathematical Biosciences, V24, P191, DOI 10.1016/0025-5564(75)90075-9
   Benty Nir, 2016, FALCOR RENDERING FRA
   Bernard JB, 2007, VISION RES, V47, P3447, DOI 10.1016/j.visres.2007.10.005
   BLAKEMORE C, 1970, J PHYSIOL-LONDON, V211, P599, DOI 10.1113/jphysiol.1970.sp009296
   Bockisch CJ, 1999, VISION RES, V39, P1025, DOI 10.1016/S0042-6989(98)00205-3
   BURR DC, 1994, NATURE, V371, P511, DOI 10.1038/371511a0
   Diamond MR, 2000, J NEUROSCI, V20, P3449, DOI 10.1523/JNEUROSCI.20-09-03449.2000
   Dorr Michael, 2011, IS T SPIE ELECT IMAG
   Duchowski A. T., 2007, ACM T MULTIM COMPUT, V3, P6
   Duchowski AT, 2004, CYBERPSYCHOL BEHAV, V7, P621, DOI 10.1089/cpb.2004.7.621
   Guenter B, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366183
   Hansen T, 2009, J VISION, V9, DOI 10.1167/9.4.26
   HENDERSON JM, 1997, ATTEN PERCEPT PSYCHO, V59, P323
   Hongbin Zha, 1999, Second International Conference on 3-D Digital Imaging and Modeling (Cat. No.PR00062), P321, DOI 10.1109/IM.1999.805362
   Hunt C., 2016, FIELD VIEW FACE OFF
   Ibbotson MR, 2009, CURR BIOL, V19, pR493, DOI 10.1016/j.cub.2009.05.010
   Jerald J, 2009, IEEE VIRTUAL REALITY 2009, PROCEEDINGS, P211, DOI 10.1109/VR.2009.4811025
   Levoy M., 1990, Computer Graphics, V24, P217, DOI 10.1145/91394.91449
   Loschky LC, 2000, ETRA '00, P97, DOI DOI 10.1145/355017.355032
   Loschky LC, 2007, ACM T MULTIM COMPUT, V3, DOI 10.1145/1314303.1314310
   Luebke D, 2001, SPRING EUROGRAP, P223
   Mania Katerina., 2004, Proceedings of the 1st Symposium on Applied Perception in Graphics and Visualization, APGV '04, P39, DOI [10.1145/1012551.1012559, DOI 10.1145/1012551.1012559]
   MATIN E, 1974, PSYCHOL BULL, V81, P899, DOI 10.1037/h0037368
   McConkie GW, 2002, BEHAV RES METH INS C, V34, P481, DOI 10.3758/BF03195477
   MCCONKIE GW, 1981, BEHAV RES METH INSTR, V13, P97, DOI 10.3758/BF03207916
   Murphy HA, 2009, ACM T APPL PERCEPT, V5, DOI 10.1145/1462048.1462053
   NVIDIA, 2016, VR WORKS MULT SHAD
   NVIDIA, 2016, VR WORKS LENS MATCH
   Ohshima T, 1996, P IEEE VIRT REAL ANN, P103, DOI 10.1109/VRAIS.1996.490517
   Patney A, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980246
   Patney Anjul, 2017, P NVIDIA GPU TECHN C
   Patney Anjul, 2016, FOVEATED RENDERING G
   REDER SM, 1973, BEHAV RES METH INSTR, V5, P218, DOI 10.3758/BF03200168
   Reingold EM, 2003, HUM FACTORS, V45, P307, DOI 10.1518/hfes.45.2.307.27235
   Ridder WH, 1997, VISION RES, V37, P3171, DOI 10.1016/S0042-6989(97)00110-7
   Ross J, 2001, TRENDS NEUROSCI, V24, P113, DOI 10.1016/S0166-2236(00)01685-4
   Santini Fabrizio, 2005, J VISION, V5, P594
   Saunders DR, 2014, BEHAV RES METHODS, V46, P439, DOI 10.3758/s13428-013-0375-5
   Schutt HH, 2016, VISION RES, V122, P105, DOI 10.1016/j.visres.2016.02.002
   Sekuler Robert., 1985, Perception
   SIDEROV J, 1995, VISION RES, V35, P2329, DOI 10.1016/0042-6989(94)00307-8
   Stengel M, 2016, COMPUT GRAPH FORUM, V35, P129, DOI 10.1111/cgf.12956
   THIBOS LN, 1987, J OPT SOC AM A, V4, P1524, DOI 10.1364/JOSAA.4.001524
   Thunstrom Robin., 2014, Passive gaze-contingent techniques relation to system latency
   Triesch J., 2002, Proceedings ETRA 2002. Eye Tracking Research and Applications Symposium, P95, DOI 10.1145/507072.507092
   TYLER CW, 1987, J OPT SOC AM A, V4, P1612, DOI 10.1364/JOSAA.4.001612
   vanderSchaaf A, 1996, VISION RES, V36, P2759, DOI 10.1016/0042-6989(96)00002-8
   Vincent Peter, 2017, S7797 TOBII EYE TRAC
   Vlachos A., 2015, ADV VR RENDERING
   Watson AB, 2014, J VISION, V14, DOI 10.1167/14.7.15
   Whiting Nick, 2016, LESSONS INTEGRATING
   Wiens S, 2004, PSYCHOL SCI, V15, P282, DOI 10.1111/j.0956-7976.2004.00667.x
   Zhang X, 2011, EURASIP J WIREL COMM, DOI 10.1155/2011/765143
NR 55
TC 105
Z9 122
U1 1
U2 22
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD SEP
PY 2017
VL 14
IS 4
SI SI
AR 25
DI 10.1145/3127589
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA FM9EI
UT WOS:000415407300004
DA 2024-07-18
ER

PT J
AU Gao, XH
   Brooks, S
   Arnold, DV
AF Gao, Xihe
   Brooks, Stephen
   Arnold, Dirk V.
TI A Feature-Based Quality Metric for Tone Mapped Images
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Image quality metrics; high dynamic range images; tone mapping; visual
   perception
ID SIMILARITY
AB With the development of high-dynamic-range images and tone mapping operators comes a need for image quality evaluation of tone mapped images. However, because of the significant difference in dynamic range between high-dynamic-range images and tone mapped images, conventional image quality assessment algorithms that predict distortion based on the magnitude of intensity or normalized contrast are not suitable for this task. In this article, we present a feature-based quality metric for tone mapped images that predicts the perceived quality by measuring the distortion in important image features that affect quality judgment. Our metric utilizes multi-exposed virtual photographs taken from the original high-dynamic-range images to bridge the gap between dynamic ranges in image feature analysis. By combining measures for brightness distortion, visual saliency distortion, and detail distortion in light and dark areas, the metric measures the overall perceptual distortion and assigns a score to a tone mapped image. Experiments on a subject-rated database indicate that the proposed metric is more consistent with subjective evaluation results than alternative approaches.
C1 [Gao, Xihe; Brooks, Stephen; Arnold, Dirk V.] Dalhousie Univ, Fac Comp Sci, Halifax, NS B3H 4R2, Canada.
C3 Dalhousie University
RP Gao, XH (corresponding author), Dalhousie Univ, Fac Comp Sci, Halifax, NS B3H 4R2, Canada.
EM xgao@cs.dal.ca; sbrooks@cs.dal.ca; dirk@cs.dal.ca
OI Arnold, Dirk/0000-0001-5367-6862
CR [Anonymous], ACM T GRAPH
   [Anonymous], ACM T GRAPH P SIGGRA
   [Anonymous], 2006, MODERN IMAGE QUALITY
   [Anonymous], 2006, Proc. 14th Pacific Conf. on Comput. Graph. Appl
   Ashikhmin M., 2002, Rendering Techniques 2002. Eurographics Workshop Proceedings, P145
   Aydin TO, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360668
   DRAGO F., 2003, ACM SIGGRAPH 2003 Sketches Applications
   Fairchild M., 2002, Tenth Color Imaging Conference: Color Science and Engineering System s, Technologies, Applications 10, P33
   Gao X., 2013, P S COMP AESTH, P87
   Gao X., 2014, P 11 EUR C VIS MED P
   Gao XH, 2016, IEEE C EVOL COMPUTAT, P3855, DOI 10.1109/CEC.2016.7744278
   Gao XH, 2015, COMPUT GRAPH-UK, V52, P171, DOI 10.1016/j.cag.2015.05.028
   Gu K, 2014, IEEE INT SYMP CIRC S, P518, DOI 10.1109/ISCAS.2014.6865186
   Heidrich Wolfgang, ERIK REINHARD
   Itti L, 2000, VISION RES, V40, P1489, DOI 10.1016/S0042-6989(99)00163-7
   Kuang JT, 2007, ACM T APPL PERCEPT, V4, DOI 10.1145/1265957.1265958
   Ledda P, 2005, ACM T GRAPHIC, V24, P640, DOI 10.1145/1073204.1073242
   Ma KD, 2015, IEEE T IMAGE PROCESS, V24, P3086, DOI [10.1109/TIP.2015.2436340, 10.1109/TIP.2015.2456638]
   Mantiuk R, 2005, PROC SPIE, V5666, P204, DOI 10.1117/12.586757
   Mertens T, 2007, PACIFIC GRAPHICS 2007: 15TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, P382, DOI 10.1109/PG.2007.17
   Nafchi HZ, 2015, IEEE SIGNAL PROC LET, V22, P1026, DOI 10.1109/LSP.2014.2381458
   Nasrinpour HR, 2015, IEEE IMAGE PROC, P4947, DOI 10.1109/ICIP.2015.7351748
   Nemoto H., 2015, 9 INT WORKSHOP VIDEO
   Reinhard E., 2002, Journal of Graphics Tools, V7, P45, DOI 10.1080/10867651.2002.10487554
   Stroebel LD, 2000, BASIC PHOTOGRAPHIC M
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   Yeganeh H, 2013, IEEE T IMAGE PROCESS, V22, P657, DOI 10.1109/TIP.2012.2221725
NR 27
TC 1
Z9 2
U1 0
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD SEP
PY 2017
VL 14
IS 4
SI SI
AR 26
DI 10.1145/3129675
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FM9EI
UT WOS:000415407300005
DA 2024-07-18
ER

PT J
AU Ebrahimi, E
   Babu, SV
   Pagano, CC
   Jörg, S
AF Ebrahimi, Elham
   Babu, Sabarish V.
   Pagano, Christopher C.
   Jorg, Sophie
TI An Empirical Evaluation of Visuo-Haptic Feedback on Physical Reaching
   Behaviors During 3D Interaction in Real and Immersive Virtual
   Environments
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article; Proceedings Paper
CT 13th ACM SIGGRAPH Symposium on Applied Perception (SAP)
CY JUL, 2016
CL Anaheim, CA
SP ACM SIGGRAPH
DE Virtual worlds; immersive virtual environments; multisensory perception;
   visuo-haptic feedback; motor control
ID HUMAN LOCOMOTION; INERTIA TENSOR; PERCEPTION; EIGENVECTORS; DISTANCE;
   OBJECT; DEPTH; HAND
AB In an initial study, we characterized the properties of human reach motion in the presence or absence of visuo-haptic feedback in real and Immersive Virtual Environments (IVEs) or virtual reality within a participant's maximum arm reach. Our goal is to understand how physical reaching actions to the perceived location of targets in the presence or absence of visuo-haptic feedback are different between real and virtual viewing conditions. Typically, participants reach to the perceived location of objects in the three-dimensional (3D) environment to perform selection and manipulation actions during 3D interaction in applications such as virtual assembly or rehabilitation. In these tasks, participants typically have distorted perceptual information in the IVE as compared to the real world, in part due to technological limitations such as minimal visual field of view, resolution, latency, and jitter. In an empirical evaluation, we asked the following questions: (i) how do the perceptual differences between virtual and real world affect our ability to accurately reach to the locations of 3D objects, and (ii) how do the motor responses of participants differ between the presence or absence of visual and haptic feedback? We examined factors such as velocity and distance of physical reaching behavior between the real world and IVE, both in the presence or absence of visuo-haptic information. The results suggest that physical reach responses vary systematically between real and virtual environments, especially in situations involving the presence or absence of visuo-haptic feedback. The implications of our study provide a methodological framework for the analysis of reaching motions for selection and manipulation with novel 3D interaction metaphors and to successfully characterize visuo-haptic versus non-visuo-haptic physical reaches in virtual and real-world situations.
C1 [Ebrahimi, Elham; Babu, Sabarish V.; Pagano, Christopher C.; Jorg, Sophie] Clemson Univ, Sch Comp, Clemson, SC 29634 USA.
   [Ebrahimi, Elham; Babu, Sabarish V.; Pagano, Christopher C.; Jorg, Sophie] Clemson Univ, Dept Psychol, Clemson, SC 29634 USA.
C3 Clemson University; Clemson University
RP Ebrahimi, E (corresponding author), Clemson Univ, Sch Comp, Clemson, SC 29634 USA.; Ebrahimi, E (corresponding author), Clemson Univ, Dept Psychol, Clemson, SC 29634 USA.
EM eebrahi@g.clemson.edu
OI Ebrahimi, Elham/0000-0001-9431-557X
CR Altenhoff BlissM., 2012, P ACM S APPL PERCEPT, V1, P71, DOI DOI 10.1145/2338676.2338691
   [Anonymous], 1996, FORCE TOUCH FEEDBACK
   Bingham GP, 1998, J EXP PSYCHOL HUMAN, V24, P145, DOI 10.1037/0096-1523.24.1.145
   Bruderlin A., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P97, DOI 10.1145/218380.218421
   Buekers M, 1999, NEUROSCI LETT, V275, P171, DOI 10.1016/S0304-3940(99)00750-8
   Chihak BJ, 2010, J EXP PSYCHOL HUMAN, V36, P1535, DOI 10.1037/a0020560
   Cutting JE, 1995, PERCEPTION SPACE MOT, P69, DOI [DOI 10.1016/B978-012240530-3/50005-5, 10.1016/B978-012240530-3/50005-5]
   Dukes PS, 2013, 2013 IEEE SYMPOSIUM ON 3D USER INTERFACES (3DUI), P47, DOI 10.1109/3DUI.2013.6550196
   Ebrahimi E., 2015, CARRYOVER EFFECTS CA, P97
   Ebrahimi Elham., 2014, Proceedings of the ACM Symposium on Applied Perception - SAP '14, P103, DOI [10.1145/2628257.2628268, DOI 10.1145/2628257]
   Keogh E, 2005, KNOWL INF SYST, V7, P358, DOI 10.1007/s10115-004-0154-9
   Kohli L, 2013, 2013 IEEE SYMPOSIUM ON 3D USER INTERFACES (3DUI), P79, DOI 10.1109/3DUI.2013.6550201
   Kunz BR, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0054446
   Linkenauger SA, 2009, J EXP PSYCHOL HUMAN, V35, P1649, DOI 10.1037/a0016875
   Loomis JM, 2003, VIRTUAL AND ADAPTIVE ENVIRONMENTS: APPLICATIONS, IMPLICATIONS, AND HUMAN PERFORMANCE ISSUES, P21
   Mohler BettyJ., 2006, P 3 S APPL PERCEPTIO, P9, DOI [10.1145/1140491.1140493, DOI 10.1145/1140491.1140493]
   Nabiyouni Mahdi, 2015, 2015 IEEE Symposium on 3D User Interfaces (3DUI), P3, DOI 10.1109/3DUI.2015.7131717
   Napieralski PE, 2011, ACM T APPL PERCEPT, V8, DOI 10.1145/2010325.2010328
   Pagano CC, 1998, J APPL BIOMECH, V14, P331, DOI 10.1123/jab.14.4.331
   PAGANO CC, 1992, PERCEPT PSYCHOPHYS, V52, P617, DOI 10.3758/BF03211699
   PRACHYABRUED M, 2014, IEEE S 3D US INT 3DU, P19
   Proffitt DR, 2003, PSYCHOL SCI, V14, P106, DOI 10.1111/1467-9280.t01-1-01427
   RIESER JJ, 1995, J EXP PSYCHOL HUMAN, V21, P480, DOI 10.1037/0096-1523.21.3.480
   ROLLAND JP, 1995, PRESENCE-TELEOP VIRT, V4, P24, DOI 10.1162/pres.1995.4.1.24
   Sanz FA, 2015, P IEEE VIRT REAL ANN, P75, DOI 10.1109/VR.2015.7223327
   Sergio LE, 1998, EXP BRAIN RES, V122, P157, DOI 10.1007/s002210050503
   Seymour NE, 2008, WORLD J SURG, V32, P182, DOI 10.1007/s00268-007-9307-9
   TAYLOR C L, 1955, Artif Limbs, V2, P22
   Wexler M, 2005, TRENDS COGN SCI, V9, P431, DOI 10.1016/j.tics.2005.06.018
   Willemsen P, 2008, PRESENCE-TELEOP VIRT, V17, P91, DOI 10.1162/pres.17.1.91
   Willemsen P, 2009, ACM T APPL PERCEPT, V6, DOI 10.1145/1498700.1498702
   Witmer BG, 1998, PRESENCE-TELEOP VIRT, V7, P144, DOI 10.1162/105474698565640
   Ziemer CJ, 2013, ATTEN PERCEPT PSYCHO, V75, P1260, DOI 10.3758/s13414-013-0473-6
NR 33
TC 29
Z9 31
U1 2
U2 22
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD JUL
PY 2016
VL 13
IS 4
SI SI
AR 19
DI 10.1145/2947617
PG 21
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA DV4DY
UT WOS:000382876600002
DA 2024-07-18
ER

PT J
AU Tan, MH
   Lalonde, JFO
   Sharan, L
   Rushmeier, H
   O'Sullivan, C
AF Tan, Minghui
   Lalonde, Jean-Franc Ois
   Sharan, Lavanya
   Rushmeier, Holly
   O'Sullivan, Carol
TI The Perception of Lighting Inconsistencies in Composite Outdoor Scenes
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article; Proceedings Paper
CT ACM SIGGRAPH Symposium on Applied Perception (SAP)
CY SEP 13-14, 2015
CL Tubingen, GERMANY
SP ACM SIGGRAPH, ACM, Disney Res, Max Planck Inst Biol Cybernet
DE Human Factors; Experimentation; Lighting; Perception; Images
ID DIRECTION
AB It is known that humans can be insensitive to large changes in illumination. For example, if an object of interest is extracted from one digital photograph and inserted into another, we do not always notice the differences in illumination between the object and its new background. This inability to spot illumination inconsistencies is often the key to success in digital "doctoring" operations. We present a set of experiments in which we explore the perception of illumination in outdoor scenes. Our results can be used to predict when and why inconsistencies go unnoticed. Applications of the knowledge gained from our studies include smarter digital "cut-and-paste" and digital "fake" detection tools, and image-based composite scene backgrounds for layout and previsualization.
C1 [Tan, Minghui] Yale Univ, Comp Graph Grp, New Haven, CT 06520 USA.
   [Lalonde, Jean-Franc Ois] Univ Laval, Comp Vis & Syst Lab, Quebec City, PQ G1V 0A6, Canada.
   [Sharan, Lavanya] MIT, Dept Brain & Cognit Sci, Cambridge, MA 02139 USA.
   [Sharan, Lavanya] MIT, CSAIL, Cambridge, MA 02139 USA.
   [O'Sullivan, Carol] Disney Res Los Angeles, Glendale, CA 91201 USA.
C3 Yale University; Laval University; Massachusetts Institute of Technology
   (MIT); Massachusetts Institute of Technology (MIT)
RP Tan, MH (corresponding author), Yale Univ, Comp Graph Grp, New Haven, CT 06520 USA.
EM tanminghui.ivy@gmail.com; jflalonde@gel.ulaval.ca; lavanya@csail.mit;
   holly.rushmeier@yale.edu; carol.osullivan@tcd.ie
OI Lalonde, Jean-Francois/0000-0002-6583-2364; O'Sullivan,
   Carol/0000-0003-3772-4961; Rushmeier, Holly/0000-0001-5241-0886
FU US National Science Foundation [1302267]; Direct For Computer & Info
   Scie & Enginr; Div Of Information & Intelligent Systems [1302267]
   Funding Source: National Science Foundation
FX This work was supported in part by grant #1302267 from the US National
   Science Foundation.
CR [Anonymous], ACM T GRAPH
   Cavanagh P, 2005, NATURE, V434, P301, DOI 10.1038/434301a
   Ennis C, 2011, ACM T APPL PERCEPT, V8, DOI 10.1145/1870076.1870078
   Ennis C, 2008, APGV 2008: PROCEEDINGS OF THE SYMPOSIUM ON APPLIED PERCEPTION IN GRAPHICS AND VISUALIZATION, P75
   Hamill J, 2005, COMPUT GRAPH FORUM, V24, P623, DOI 10.1111/j.1467-8659.2005.00887.x
   Hospedales T, 2009, PLOS ONE, V4, DOI 10.1371/journal.pone.0004205
   Karst KL, 2011, SUPREME COURT REV, P1
   Koenderink JJ, 2003, J OPT SOC AM A, V20, P987, DOI 10.1364/JOSAA.20.000987
   Koenderink JJ, 2004, PERCEPTION, V33, P1405, DOI 10.1068/p5287
   Lalonde J., 2007, IEEE INT C COMPUTER, P1
   Lalonde Jean-Francois, 2014, 2014 2nd International Conference on 3D Vision (3DV). Proceedings, P131, DOI 10.1109/3DV.2014.112
   Lalonde JF, 2012, INT J COMPUT VISION, V98, P123, DOI 10.1007/s11263-011-0501-8
   Lalonde JF, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618477
   Lopez-Moreno J., 2010, Proceedings of APGV, P25, DOI DOI 10.1145/1836248.1836252
   Lopez-Moreno J, 2013, COMPUT GRAPH FORUM, V32, P170, DOI 10.1111/cgf.12195
   O'Sullivan C., 2011, Proceedings of the 2011 3rd International Conference on Games and Virtual Worlds for Serious Applications (VS-GAMES 2011), P1, DOI 10.1109/VS-GAMES.2011.9
   Ostrovsky Y, 2005, PERCEPTION, V34, P1301, DOI 10.1068/p5418
   Preetham AJ, 1999, COMP GRAPH, P91, DOI 10.1145/311535.311545
   Reinhard E, 2001, IEEE COMPUT GRAPH, V21, P34, DOI 10.1109/38.946629
   Xiaowu Chen, 2011, 2011 12th International Conference on Computer-Aided Design and Computer Graphics, P450, DOI 10.1109/CAD/Graphics.2011.19
   Xue S, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185580
NR 21
TC 5
Z9 6
U1 1
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD SEP
PY 2015
VL 12
IS 4
SI SI
DI 10.1145/2810038
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA CR1ER
UT WOS:000361067300006
DA 2024-07-18
ER

PT J
AU De Poli, G
   Canazza, S
   Rodá, A
   Schubert, E
AF De Poli, Giovanni
   Canazza, Sergio
   Roda, Antonio
   Schubert, Emery
TI The Role of Individual Difference in Judging Expressiveness of
   Computer-Assisted Music Performances by Experts
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Human Factors; Experimentation; Cognitive styles; human-centered
   computing; music performance
ID COGNITIVE STYLES; SYSTEMS; MODELS; PERSONALITY; CREATIVITY; SIGHT
AB Computational systems for generating expressive musical performances have been studied for several decades now. These models are generally evaluated by comparing their predictions with actual performances, both from a performance parameter and a subjective point of view, often focusing on very specific aspects of the model. However, little is known about how listeners evaluate the generated performances and what factors influence their judgement and appreciation. In this article, we present two studies, conducted during two dedicated workshops, to start understanding how the audience judges entire performances employing different approaches to generating musical expression. In the preliminary study, 40 participants completed a questionnaire in response to five different computer-generated and computer-assisted performances, rating preference and describing the expressiveness of the performances. In the second, "GATM" (Gruppo di Analisi e Teoria Musicale) study, 23 participants also completed the Music Cognitive Style questionnaire. Results indicated that music systemizers tend to describe musical expression in terms of the formal aspects of the music, and music empathizers tend to report expressiveness in terms of emotions and characters. However, high systemizers did not differ from high empathizers in their mean preference score across the five pieces. We also concluded that listeners tend not to focus on the basic technical aspects of playing when judging computer-assisted and computer-generated performances. Implications for the significance of individual differences in judging musical expression are discussed.
C1 [De Poli, Giovanni; Canazza, Sergio; Roda, Antonio] Univ Padua, Dept Informat Engn, CSC, I-35131 Padua, Italy.
   [Schubert, Emery] Univ New S Wales, Sch Arts & Media, Sydney, NSW 2052, Australia.
C3 University of Padua; University of New South Wales Sydney
RP De Poli, G (corresponding author), Univ Padua, Dept Informat Engn, CSC, Via Gradenigo 6-A, I-35131 Padua, Italy.
EM depoli@dei.unipd.it; canazza@dei.unipd.it; roda@dei.unipd.it;
   e.schubert@unsw.edu.au
RI Canazza, Sergio/Y-8733-2019; De Poli, Giovanni/T-6497-2019
OI Schubert, Emery/0000-0002-6541-8083; De Poli,
   Giovanni/0000-0001-8487-7093
CR Adorno TheodorW., 2004, The Performance of Reading: An Essay in the Philosophy of Literature, P254
   [Anonymous], 2012, STRUCTURING MUSIC MA, DOI DOI 10.4018/978-1-4666-2497-9.CH008
   Ariza C, 2009, COMPUT MUSIC J, V33, P48, DOI 10.1162/comj.2009.33.2.48
   Baron-Cohen S, 2005, SCIENCE, V310, P819, DOI 10.1126/science.1115455
   Bisesi Erica, 2011, P INT S PERF SCI ISP, P27
   Boden MA, 2009, AI MAG, V30, P23, DOI 10.1609/aimag.v30i3.2254
   Bresin Roberto, 2013, Guide to Computing for Expressive Music Performance, P181
   Canazza S, 2004, P IEEE, V92, P686, DOI 10.1109/JPROC.2004.825889
   Dahlhaus Carl., 1982, Esthetics of Music
   de Ciuffardi Viviana Noemi Lemos, 2000, INTERDISCIPLINARIA, V17, P1
   De Poli G, 2004, J NEW MUSIC RES, V33, P189, DOI 10.1080/0929821042000317796
   Dunn PG, 2012, PSYCHOL MUSIC, V40, P411, DOI 10.1177/0305735610388897
   Garrido S, 2011, MUSIC PERCEPT, V28, P279, DOI 10.1525/MP.2011.28.3.279
   Garrido Sandra, 2011, P SOUND MUS COMP C S, P1
   Gillespie W., 2000, PSYCHOL MUSIC, V28, P154, DOI DOI 10.1177/0305735600282004
   Grachten M, 2012, J NEW MUSIC RES, V41, P311, DOI 10.1080/09298215.2012.731071
   Hintze JL, 1998, AM STAT, V52, P181, DOI 10.2307/2685478
   HIRAGA R, 2002, P INT COMP MUS C, P357
   Hiraga R., 2004, Proceedings of the 2004 conference on New interfaces for musical expression, P120
   Jordanous A, 2012, COGN COMPUT, V4, P246, DOI 10.1007/s12559-012-9156-1
   Kampstra P., 2008, Journal of Statistical Software, V28, P1, DOI [10.18637/jss.v028.c01, DOI 10.18637/JSS.V028.C01, 10.18637/jss.v028.c01.papers3://publication/uuid/692988CE-7E10-498E-96EC-E7A0CE3620A3]
   Katayose H., 2004, P C NEW INT MUS EXP, P124
   Katayose H, 2012, J NEW MUSIC RES, V41, P299, DOI 10.1080/09298215.2012.745579
   Kemp A.E., 1996, MUSICAL TEMPERAMENT
   Kirke A, 2009, ACM COMPUT SURV, V42, DOI 10.1145/1592451.1592454
   Kirke Alexis., 2013, Guide to Computing for Expressive Music Performance, DOI DOI 10.1007/978-1-4471-4123-5
   Knobloch S, 2002, J COMMUN, V52, P351, DOI 10.1093/joc/52.2.351
   Kozhevnikov M, 2007, PSYCHOL BULL, V133, P464, DOI 10.1037/0033-2909.133.3.464
   Kreutz G, 2008, MUSIC PERCEPT, V26, P57, DOI 10.1525/MP.2008.26.1.57
   Mcpherson G.E., 1998, Research Studies in Music Education, v, V10, n, P12
   McPherson G.E., 2004, Musical excellence: Strategies and techniques to enhance performance, P61
   North AC, 2010, AM J PSYCHOL, V123, P199, DOI 10.5406/amerjpsyc.123.2.0199
   Palmer C, 1997, ANNU REV PSYCHOL, V48, P115, DOI 10.1146/annurev.psych.48.1.115
   Reimer B., 1997, International Journal of Music Education, V29, P4, DOI DOI 10.1177/02557614970
   Reimer B., 1989, A philosophy of music education, V2nd
   Rentfrow PJ, 2011, J PERS SOC PSYCHOL, V100, P1139, DOI 10.1037/a0022406
   Rentfrow PJ, 2003, J PERS SOC PSYCHOL, V84, P1236, DOI 10.1037/0022-3514.84.6.1236
   Sadler-Smith E, 2001, PERS INDIV DIFFER, V30, P609, DOI 10.1016/S0191-8869(00)00059-3
   Stanley M., 2002, RES STUD MUSIC EDUC, V18, P46, DOI [10.1177/1321103X020180010601, DOI 10.1177/1321103X020180010601]
   SUNDBERG J, 1983, COMPUT MUSIC J, V7, P37, DOI 10.2307/3679917
   Tanaka S., 2011, 37th European Conference and Exhibition on Optical Communication, P1
   Thompson S, 2003, MUSIC PERCEPT, V21, P21, DOI 10.1525/mp.2003.21.1.21
   Thompson S., 2007, Psychology of Music, V35, P20, DOI [10.1177/0305735607068886, DOI 10.1177/0305735607068886]
   Tsay CJ, 2014, ORGAN BEHAV HUM DEC, V124, P24, DOI 10.1016/j.obhdp.2013.10.003
   Tsay CJ, 2013, P NATL ACAD SCI USA, V110, P14580, DOI 10.1073/pnas.1221454110
   Tunks Thomas W., 1987, B COUNCIL RES MUSIC, V90, P53
   Widmer G, 2004, J NEW MUSIC RES, V33, P203, DOI 10.1080/0929821042000317804
   Widmer G, 2009, AI MAG, V30, P35, DOI 10.1609/aimag.v30i3.2249
   Wöllner C, 2012, PSYCHOL AESTHET CREA, V6, P214, DOI 10.1037/a0027392
   Wrigley WJ, 2013, PSYCHOL MUSIC, V41, P97, DOI 10.1177/0305735611418552
   ZIV N, 2006, EMPIR STUD ARTS, V24, P177, DOI DOI 10.2190/E4EN-1X32-KUU1-LDHT
NR 51
TC 6
Z9 6
U1 0
U2 10
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD JAN
PY 2015
VL 11
IS 4
AR 22
DI 10.1145/2668124
PG 20
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA AZ6WJ
UT WOS:000348358400007
DA 2024-07-18
ER

PT J
AU Kelly, JW
   Burton, M
   Pollock, B
   Rubio, E
   Curtis, M
   De la Cruz, J
   Gilbert, S
   Winer, E
AF Kelly, Jonathan W.
   Burton, Melissa
   Pollock, Brice
   Rubio, Eduardo
   Curtis, Michael
   De la Cruz, Julio
   Gilbert, Stephen
   Winer, Eliot
TI Space Perception in Virtual Environments: Displacement from the Center
   of Projection Causes Less Distortion than Predicted by Cue-Based Models
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Experimentation; Human Factors; Depth perception; stereoscopic displays;
   virtual environments
ID DISTANCE PERCEPTION; REALITY; PERSPECTIVE; OBSERVER; LOCATION; PICTURES;
   SIZE; VIEW
AB Virtual reality systems commonly include both monocular and binocular depth cues, which have the potential to provide viewers with a realistic impression of spatial properties of the virtual environment. However, when multiple viewers share the same display, only one viewer typically receives the projectively correct images. All other viewers experience the same images despite displacement from the center of projection (CoP). Three experiments evaluated perceptual distortions caused by displacement from the CoP and compared those percepts to predictions of models based on monocular and binocular viewing geometry. Leftward and rightward displacement from the CoP caused virtual angles on the ground plane to be judged as larger and smaller, respectively, compared to judgments from the CoP. Backward and forward displacement caused rectangles on the ground plane to be judged as larger and smaller in depth, respectively, compared to judgments from the CoP. Judgment biases were in the same direction as cue-based model predictions but of smaller magnitude. Displacement from the CoP had asymmetric effects on perceptual judgments, unlike model predictions. Perceptual distortion occurred with monocular cues alone but was exaggerated when binocular cues were added. The results are grounded in terms of practical implications for multiuser virtual environments.
C1 [Kelly, Jonathan W.; Burton, Melissa; Pollock, Brice; Rubio, Eduardo; Curtis, Michael; Gilbert, Stephen; Winer, Eliot] Iowa State Univ, Ames, IA USA.
   [De la Cruz, Julio] US Army Res Lab, Human Res & Engn Directorate, Simulat & Training Technol Ctr, Adelphi, MD USA.
C3 Iowa State University; United States Department of Defense; United
   States Army; US Army Research, Development & Engineering Command
   (RDECOM); US Army Research Laboratory (ARL)
RP Kelly, JW (corresponding author), Iowa State Univ, Iowa State Univ Psychol, W112 Lagomarcino Hall, Ames, IA 50011 USA.
EM jonkelly@iastate.edu
RI Gilbert, Stephen/F-3138-2018; Kelly, Jonathan W/A-4793-2013
OI Gilbert, Stephen/0000-0002-5332-029X; 
FU United States Army Research Laboratory's Human Research and Engineering
   Directorate, Simulation and Training Technology Center
FX This work was supported by a grant from the United States Army Research
   Laboratory's Human Research and Engineering Directorate, Simulation and
   Training Technology Center. Results from Experiment 1 were presented at
   the 2012 meeting of the SPIE.
CR Adams K.R., 1972, Leonardo, P209, DOI DOI 10.2307/1572377
   [Anonymous], 2005, ACM Transactions on Applied Perception, DOI [DOI 10.1145/1077399.1077403, DOI 10.1145/1077399.10774032,3,9]
   Banks Martin S, 2009, Inf Disp (1975), V25, P12
   BENGSTON JK, 1980, J EXP PSYCHOL HUMAN, V6, P751, DOI 10.1037/0096-1523.6.4.751
   Bodenheimer B, 2007, APGV 2007: SYMPOSIUM ON APPLIED PERCEPTION IN GRAPHICS AND VISUALIZATION, PROCEEDINGS, P35
   Carlson V.R., 1977, STABILITY CONSTANCY, P217
   Cruz-Neira C., 2002, P 40 AIAA AER SCI M
   GILINSKY AS, 1951, PSYCHOL REV, V58, P460, DOI 10.1037/h0061505
   Glantz K, 2003, PSYCHOTHERAPY, V40, P55, DOI 10.1037/0033-3204.40.1-2.55
   GOGEL WC, 1974, PSYCHOLOGIA, V17, P213
   GOGEL WC, 1973, PERCEPT PSYCHOPHYS, V13, P284, DOI 10.3758/BF03214141
   GOGEL WC, 1969, VISION RES, V9, P1079, DOI 10.1016/0042-6989(69)90049-2
   GOGEL WC, 1985, PERCEPT PSYCHOPHYS, V37, P17, DOI 10.3758/BF03207134
   GOLDSTEIN EB, 1987, J EXP PSYCHOL HUMAN, V13, P256, DOI 10.1037/0096-1523.13.2.256
   Gooch A.A., 2002, Proc. Int. Sym. NPR Animat. Render, P105
   Grantcharov TP, 2004, BRIT J SURG, V91, P146, DOI 10.1002/bjs.4407
   HAGEN MA, 1978, PERCEPT MOTOR SKILL, V46, P875, DOI 10.2466/pms.1978.46.3.875
   Held RT, 2008, APGV 2008: PROCEEDINGS OF THE SYMPOSIUM ON APPLIED PERCEPTION IN GRAPHICS AND VISUALIZATION, P23
   Hillis JM, 2004, J VISION, V4, P967, DOI 10.1167/4.12.1
   Hoffman DM, 2008, J VISION, V8, DOI 10.1167/8.3.33
   Hutchison JJ, 2006, SPAN J PSYCHOL, V9, P332, DOI 10.1017/S1138741600006235
   Jack D, 2001, IEEE T NEUR SYS REH, V9, P308, DOI 10.1109/7333.948460
   Julesz B., 1971, Foundation of Cyclopean Perception
   Kelly JW, 2004, PRESENCE-TELEOP VIRT, V13, P442, DOI 10.1162/1054746041944786
   Knapp JM, 2004, PRESENCE-TELEOP VIRT, V13, P572, DOI 10.1162/1054746042545238
   KRAFT RN, 1989, PERCEPT PSYCHOPHYS, V45, P459, DOI 10.3758/BF03210720
   Kuhl SA, 2009, ACM T APPL PERCEPT, V6, DOI 10.1145/1577755.1577762
   Loomis JM, 2006, PSYCHOL SCI, V17, P214, DOI 10.1111/j.1467-9280.2006.01688.x
   Loomis JM, 2003, VIRTUAL AND ADAPTIVE ENVIRONMENTS: APPLICATIONS, IMPLICATIONS, AND HUMAN PERFORMANCE ISSUES, P21
   Loomis JM, 1998, PERCEPT PSYCHOPHYS, V60, P966, DOI 10.3758/BF03211932
   LUMSDEN EA, 1983, PERCEPT PSYCHOPHYS, V33, P177, DOI 10.3758/BF03202836
   Macuga KL, 2006, PERCEPT PSYCHOPHYS, V68, P872, DOI 10.3758/BF03193708
   Ooi TL, 2001, NATURE, V414, P197, DOI 10.1038/35102562
   Philbeck JW, 1997, PERCEPT PSYCHOPHYS, V59, P601, DOI 10.3758/BF03211868
   Pollock B, 2012, IEEE T VIS COMPUT GR, V18, P581, DOI 10.1109/TVCG.2012.58
   Sedgwick H.A., 1991, Pictorial communication in virtual and real environments, P460
   Sedgwick HA., 1986, HDB PERCEPTION HUMAN, P1
   SMITH O. W., 1958, PERCEPT MOT SKILLS, V8, P307
   Steinicke F., 2009, Proceedings of the 6th Symposium on Applied Perception in Graphics and Visualization, P19, DOI 10.1145/1620993.1620998
   Thompson WB, 2004, PRESENCE-TELEOP VIRT, V13, P560, DOI 10.1162/1054746042545292
   Todorovic D, 2009, ATTEN PERCEPT PSYCHO, V71, P183, DOI 10.3758/APP.71.1.183
   Vishwanath D, 2005, NAT NEUROSCI, V8, P1401, DOI 10.1038/nn1553
   WALLACH H, 1963, AM J PSYCHOL, V76, P659, DOI 10.2307/1419717
   Waller D, 2008, J EXP PSYCHOL-APPL, V14, P61, DOI 10.1037/1076-898X.14.1.61
   Willemsen P, 2008, PRESENCE-TELEOP VIRT, V17, P91, DOI 10.1162/pres.17.1.91
   Witmer BG, 1998, HUM FACTORS, V40, P478, DOI 10.1518/001872098779591340
   WOODS A, 1993, P SOC PHOTO-OPT INS, V1915, P36, DOI 10.1117/12.157041
   Wu B, 2004, NATURE, V428, P73, DOI 10.1038/nature02350
   Yang T, 1999, PERCEPT PSYCHOPHYS, V61, P456, DOI 10.3758/BF03211966
   Ziemer CJ, 2009, ATTEN PERCEPT PSYCHO, V71, P1095, DOI 10.3758/APP.71.5.1096
NR 50
TC 5
Z9 7
U1 0
U2 12
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD OCT
PY 2013
VL 10
IS 4
AR 18
DI 10.1145/2536764.2536765
PG 23
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA 281YK
UT WOS:000329136700001
DA 2024-07-18
ER

PT J
AU Schumacher, M
   Blanz, V
AF Schumacher, Matthaeus
   Blanz, Volker
TI Which Facial Profile Do Humans Expect After Seeing a Frontal View? A
   Comparison with a Linear Face Model
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Algorithms; Experimentation; Theory; Verification; Depth; faces; shape
   perception; Morphable Model
ID RECOGNITION; STIMULUS; SHAPE
AB Manipulated versions of three-dimensional faces that have different profiles, but almost the same appearance in frontal views, provide a novel way to investigate if and how humans use class-specific knowledge to infer depth from images of faces. After seeing a frontal view, participants have to select the profile that matches that view. The profiles are original (ground truth), average, random other, and two solutions computed with a linear face model (3D Morphable Model). One solution is based on 2D vertex positions, the other on pixel colors in the frontal view. The human responses demonstrate that humans neither guess nor just choose the average profile. The results also indicate that humans actually use the information from the front view, and not just rely on the plausibility of the profiles per se. All our findings are perfectly consistent with a correlation-based inference in a linear face model. The results also verify that the 3D reconstructions from our computational algorithms (stimuli 4 and 5) are similar to what humans expect, because they are chosen to be the true profile equally often as the ground-truth profiles. Our experiments shed new light on the mechanisms of human face perception and present a new quality measure for 3D reconstruction algorithms.
C1 [Schumacher, Matthaeus; Blanz, Volker] Univ Siegen, Inst Vis & Graph, D-57076 Siegen, Germany.
C3 Universitat Siegen
RP Schumacher, M (corresponding author), Univ Siegen, Inst Vis & Graph, Hoelderlinstr 3, D-57076 Siegen, Germany.
EM Schumacher@informatik.uni-siegen.de
FU German Federal Ministry for Education and Research (BMBF) [FKZ:
   12N10785]
FX This work was funded by the German Federal Ministry for Education and
   Research (BMBF) as part of the project INBEKI (FKZ: 12N10785).
CR Belhumeur PN, 1999, INT J COMPUT VISION, V35, P33, DOI 10.1023/A:1008154927611
   Blanz V, 1999, COMP GRAPH, P187, DOI 10.1145/311535.311556
   Blanz V, 2004, 2ND INTERNATIONAL SYMPOSIUM ON 3D DATA PROCESSING, VISUALIZATION, AND TRANSMISSION, PROCEEDINGS, P293, DOI 10.1109/TDPVT.2004.1335212
   EZZAT T., 2002, PHIL T R SOC LOND B, V352, p[388, 1121]
   Gummersbach N., 2010, P 7 S APPL PERC GRAP, V109
   Hassner T., 2006, P IEEE C COMP VIS PA
   Hill H, 2007, PERCEPTION, V36, P199, DOI 10.1068/p5523
   Jiang F, 2006, PSYCHOL SCI, V17, P493, DOI 10.1111/j.1467-9280.2006.01734.x
   Koenderink JJ, 1997, PERCEPT PSYCHOPHYS, V59, P828, DOI 10.3758/BF03205501
   Leopold DA, 2001, NAT NEUROSCI, V4, P89, DOI 10.1038/82947
   Mallot H.A., 2000, Computational Vision : Information Processing in Perception and Visual Behavior
   O'Toole AJ, 1998, VISION RES, V38, P2351, DOI 10.1016/S0042-6989(98)00042-X
   OToole AJ, 1997, INVEST OPHTH VIS SCI, V38, P4662
   SAXENA A., 2008, IEEE T PATTERN ANAL, V31, P5
   Troje NF, 1996, VISION RES, V36, P1761, DOI 10.1016/0042-6989(95)00230-8
   VALENTINE T, 1991, Q J EXP PSYCHOL-A, V43, P161, DOI 10.1080/14640749108400966
   Yuille A, 2006, TRENDS COGN SCI, V10, P301, DOI 10.1016/j.tics.2006.05.002
NR 17
TC 6
Z9 6
U1 0
U2 2
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD JUL
PY 2012
VL 9
IS 3
AR 11
DI 10.1145/2325722.2325724
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA 984EE
UT WOS:000307171700002
DA 2024-07-18
ER

PT J
AU Souman, JL
   Giordano, PR
   Schwaiger, M
   Frissen, I
   Thümmel, T
   Ulbrich, H
   De Luca, A
   Bülthoff, HH
   Ernst, MO
AF Souman, J. L.
   Giordano, P. Robuffo
   Schwaiger, M.
   Frissen, I.
   Thuemmel, T.
   Ulbrich, H.
   De Luca, A.
   Buelthoff, H. H.
   Ernst, M. O.
TI CyberWalk: Enabling Unconstrained Omnidirectional Walking through
   Virtual Environments
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Algorithms; Design; Experimentation; Human Factors; Virtual reality;
   locomotion; treadmill; spatial navigation; control system
ID PATH-INTEGRATION; SELF-MOTION; REALITY; LOCOMOTION; GAIT; PERCEPTION;
   TREADMILL; HUMANS; ORIENTATION; VARIABILITY
AB Despite many recent developments in virtual reality, an effective locomotion interface which allows for normal walking through large virtual environments was until recently still lacking. Here, we describe the new CyberWalk omnidirectional treadmill system, which makes it possible for users to walk endlessly in any direction, while never leaving the confines of the limited walking surface. The treadmill system improves on previous designs, both in its mechanical features and in the control system employed to keep users close to the center of the treadmill. As a result, users are able to start walking, vary their walking speed and direction, and stop walking as they would on a normal, stationary surface. The treadmill system was validated in two experiments, in which both the walking behavior and the performance in a basic spatial updating task were compared to that during normal overground walking. The results suggest that walking on the CyberWalk treadmill is very close to normal walking, especially after some initial familiarization. Moreover, we did not find a detrimental effect of treadmill walking in the spatial updating task. The CyberWalk system constitutes a significant step forward to bringing the real world into the laboratory or workplace.
C1 [Souman, J. L.; Giordano, P. Robuffo; Ernst, M. O.] Max Planck Inst Biol Cybernet, D-72076 Tubingen, Germany.
   [Schwaiger, M.; Thuemmel, T.; Ulbrich, H.] Tech Univ Munich, Inst Appl Mech, D-85748 Garching, Germany.
   [De Luca, A.] Univ Roma La Sapienza, Dipartimento Informat & Sistemist, I-00185 Rome, Italy.
   [Frissen, I.] McGill Univ, Montreal, PQ, Canada.
C3 Max Planck Society; Technical University of Munich; Sapienza University
   Rome; McGill University
RP Souman, JL (corresponding author), Max Planck Inst Biol Cybernet, Spemannstr 38, D-72076 Tubingen, Germany.
RI Bülthoff, Heinrich H/J-6579-2012; Giordano, Paolo Robuffo/F-5835-2011;
   Bülthoff, Heinrich/AAC-8818-2019; De Luca, Alessandro/F-3835-2011
OI Bülthoff, Heinrich H/0000-0003-2568-0607; De Luca,
   Alessandro/0000-0002-0713-5608
FU European 6th Framework Program CyberWalk [FP6-511092]; European 6th
   Framework Program ImmerSence [IST-2006-27141]; WCU (World Class
   University) through the National Research Foundation of Korea; Ministry
   of Education, Science and Technology [R31-10008]
FX This research was funded by the European 6th Framework Programs
   CyberWalk (FP6-511092) and ImmerSence (IST-2006-27141). H. H. Bulthoff
   was also supported by the WCU (World Class University) program through
   the National Research Foundation of Korea funded by the Ministry of
   Education, Science and Technology (R31-10008).
CR Aggarwal R, 2009, SURGERY, V145, P1, DOI 10.1016/j.surg.2008.07.010
   Alton F, 1998, CLIN BIOMECH, V13, P434, DOI 10.1016/S0268-0033(98)00012-6
   [Anonymous], 2007, 13 EUR S VIRT ENV 10
   [Anonymous], VISION ATTENTION
   [Anonymous], 2001, P EUR
   BAKKER HN, 1999, PRESENCE, V8, P36
   Bertin R.J.V., 2005, DRIV SIM C N AM, P280
   BRENIERE Y, 1991, J MOTOR BEHAV, V23, P235, DOI 10.1080/00222895.1991.9942034
   Chance SS, 1998, PRESENCE-TELEOP VIRT, V7, P168, DOI 10.1162/105474698565659
   CHECCACCI D, 2003, EUROHAPTICS, P53
   Choy Y, 2007, CLIN PSYCHOL REV, V27, P266, DOI 10.1016/j.cpr.2006.10.002
   Christensen RR, 2000, PRESENCE-VIRTUAL AUG, V9, P1, DOI 10.1162/105474600566574
   De Luca A, 2009, 2009 IEEE-RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, P5051, DOI 10.1109/IROS.2009.5354610
   DURGIN FH, 2007, T APPL PERCEPTION, V4, P1
   Edgar GK, 2007, DISPLAYS, V28, P45, DOI 10.1016/j.displa.2007.04.009
   Erren-Wolters CV, 2007, INT J REHABIL RES, V30, P91, DOI 10.1097/MRR.0b013e32813a2e00
   FINK P, 2007, ACM T APPL PERCEPT, V4, P1
   Fitzpatrick RC, 2006, CURR BIOL, V16, P1509, DOI 10.1016/j.cub.2006.05.063
   Fritschi M, 2008, SPRINGER TRAC ADV RO, V45, P179, DOI 10.1007/978-3-540-79035-8_9
   FUNG J, 2004, P 26 ANN INT C IEEE
   Gibson J.J., 1950, PERCEPTION VISUAL WO
   Gibson J. J., 1966, The ecological approach to visual perception
   GLASAUER S, 1994, EXP BRAIN RES, V98, P323
   Gregg L, 2007, SOC PSYCH PSYCH EPID, V42, P343, DOI 10.1007/s00127-007-0173-4
   Hicheur H, 2005, EXP BRAIN RES, V162, P145, DOI 10.1007/s00221-004-2122-8
   Holden MK, 2005, CYBERPSYCHOL BEHAV, V8, P187, DOI 10.1089/cpb.2005.8.187
   Hollerbach J.M., 2000, HAPTICS S P ASME DYN, P1293
   Hollman JH, 2007, GAIT POSTURE, V26, P289, DOI 10.1016/j.gaitpost.2006.09.075
   Ivanenko Y, 1997, EXP BRAIN RES, V117, P419, DOI 10.1007/s002210050236
   Ivanenko YP, 1997, J PHYSIOL-LONDON, V502, P223, DOI 10.1111/j.1469-7793.1997.223bl.x
   Iwata H, 1999, P IEEE VIRT REAL ANN, P286, DOI 10.1109/VR.1999.756964
   Iwata H, 1999, PRESENCE-TELEOP VIRT, V8, P587, DOI 10.1162/105474699566503
   Iwata Hiroo, 2008, P355, DOI 10.1007/978-3-7643-7612-3_29
   Jahn K, 2000, NEUROREPORT, V11, P1745, DOI 10.1097/00001756-200006050-00029
   Jones MB, 2004, PRESENCE-VIRTUAL AUG, V13, P589, DOI 10.1162/1054746042545247
   Kallie CS, 2007, J EXP PSYCHOL HUMAN, V33, P183, DOI 10.1037/0096-1523.33.1.183
   Kapralos B, 2008, PRESENCE-VIRTUAL AUG, V17, P527, DOI 10.1162/pres.17.6.527
   Kavanagh JJ, 2008, GAIT POSTURE, V28, P1, DOI 10.1016/j.gaitpost.2007.10.010
   Kemeny A, 2003, TRENDS COGN SCI, V7, P31, DOI 10.1016/S1364-6613(02)00011-6
   KEMENY A, 2009, P DRIV SIM C EUR, P16
   Klatzky RL, 1998, PSYCHOL SCI, V9, P293, DOI 10.1111/1467-9280.00058
   KOENDERINK JJ, 1987, BIOL CYBERN, V56, P247, DOI 10.1007/BF00365219
   Lappe M, 1999, TRENDS COGN SCI, V3, P329, DOI 10.1016/S1364-6613(99)01364-9
   Laycock SD, 2003, COMPUT GRAPH FORUM, V22, P117, DOI 10.1111/1467-8659.00654
   Macefield VG, 2009, CLIN AUTON RES, V19, P76, DOI 10.1007/s10286-009-0524-1
   MEILINGER T, 2010, P 32 ANN C COGN SCI, P2500
   Menegoni F., 2009, Gait Posture, V30, pS21, DOI DOI 10.1016/J.GAITPOST.2009.07.103
   Mittelstaedt ML, 2001, EXP BRAIN RES, V139, P318, DOI 10.1007/s002210100735
   Moe-Nilssen R, 2004, J BIOMECH, V37, P121, DOI 10.1016/S0021-9290(03)00233-1
   Mohan Arvind, 2006, Acta Orthop Belg, V72, P659
   Müller P, 2006, ACM T GRAPHIC, V25, P614, DOI 10.1145/1141911.1141931
   MURRAY MP, 1985, J APPL PHYSIOL, V59, P87, DOI 10.1152/jappl.1985.59.1.87
   Pais ARV, 2009, PRESENCE-TELEOP VIRT, V18, P200, DOI 10.1162/pres.18.3.200
   Patterson R, 2006, HUM FACTORS, V48, P555, DOI 10.1518/001872006778606877
   Powers MB, 2008, J ANXIETY DISORD, V22, P561, DOI 10.1016/j.janxdis.2007.04.006
   Riecke BE, 2002, PRESENCE-VIRTUAL AUG, V11, P443, DOI 10.1162/105474602320935810
   Riecke BE, 2007, PSYCHOL RES-PSYCH FO, V71, P298, DOI 10.1007/s00426-006-0085-z
   Riva G, 2005, CYBERPSYCHOL BEHAV, V8, P220, DOI 10.1089/cpb.2005.8.220
   Rizzo AA, 2006, CNS SPECTRUMS, V11, P35, DOI 10.1017/S1092852900024196
   RUDDLE R, 2011, ACM T COMPU IN PRESS
   Ruddle RA, 2006, PSYCHOL SCI, V17, P460, DOI 10.1111/j.1467-9280.2006.01728.x
   Ruddle RA, 2009, ACM T COMPUT-HUM INT, V16, DOI 10.1145/1502800.1502805
   Sanchez-Vives MV, 2005, NAT REV NEUROSCI, V6, P332, DOI 10.1038/nrn1651
   SCHWAIGER M, 2008, OFFENENLEGUNGSSCHRIF
   SCHWAIGER M, 2007, P IEEE INT WORKSH AU
   SCHWAIGER M, 2008, Patent No. 2008025550
   Seymour NE, 2008, WORLD J SURG, V32, P182, DOI 10.1007/s00268-007-9307-9
   Souman JL, 2010, ACM T APPL PERCEPT, V7, DOI 10.1145/1670671.1670675
   Souman JL, 2009, CURR BIOL, V19, P1538, DOI 10.1016/j.cub.2009.07.053
   SREENIVASA MN, 2010, GAIT POSTUR IN PRESS
   Steinicke F., 2008, Proceedings of the Virtual Reality International Conference (VRIC), P15
   Stolze H, 1997, ELECTROMYOGR MOTOR C, V105, P490, DOI 10.1016/S0924-980X(97)00055-6
   Sutherland K.R., 2006, Human Walking, P33
   Takei Y, 1997, EXP BRAIN RES, V115, P361, DOI 10.1007/PL00005705
   Takei Y, 1996, BRAIN RES BULL, V40, P491, DOI 10.1016/0361-9230(96)00147-5
   Tarr MJ, 2002, NAT NEUROSCI, V5, P1089, DOI 10.1038/nn948
   Terrier P, 2003, EUR J APPL PHYSIOL, V90, P554, DOI 10.1007/s00421-003-0906-3
   Terrier Philippe, 2005, J Neuroeng Rehabil, V2, P28, DOI 10.1186/1743-0003-2-28
   Teufel H, 2007, AIAA MOD SIM TECHN C, P1
   Torres D, 2008, KNOWL ENG REV, V23, P389, DOI 10.1017/S0269888908000040
   Usoh M, 1999, COMP GRAPH, P359, DOI 10.1145/311535.311589
   van Veen HAHC, 1998, FUTURE GENER COMP SY, V14, P231, DOI 10.1016/S0167-739X(98)00027-2
   Vidal FP, 2006, COMPUT GRAPH FORUM, V25, P113, DOI 10.1111/j.1467-8659.2006.00822.x
   Vielledent S, 2001, NEUROSCI LETT, V305, P65, DOI 10.1016/S0304-3940(01)01798-0
   WARREN WH, 1988, NATURE, V336, P162, DOI 10.1038/336162a0
   Weaver T.D., 2006, Human Walking, P23
   2011, ACM T APPL PERCEPTIO, V8
NR 87
TC 98
Z9 101
U1 1
U2 21
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD NOV
PY 2011
VL 8
IS 4
AR 25
DI 10.1145/2043603.2043607
PG 22
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 856IQ
UT WOS:000297633400004
DA 2024-07-18
ER

PT J
AU Amemiya, T
   Ando, H
   Maeda, T
AF Amemiya, Tomohiro
   Ando, Hideyuki
   Maeda, Taro
TI Lead-Me Interface for a Pulling Sensation from Hand-held Devices
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Human Factors; Haptic perception; mobile device; interface using
   illusionary sensation
ID HUMAN MULTIJOINT ARM
AB When a small mass in a hand-held device oscillates along a single axis with asymmetric acceleration (strongly peaked in one direction and diffuse in the other), the holder typically experiences a kinesthetic illusion characterized by the sensation of being continuously pushed or pulled by the device. This effect was investigated because of its potential application to a hand-held, nongrounded, haptic device that can convey a sense of a continuous translational force in one direction, which is a key missing piece in haptic research. A 1 degree-of-freedom (DOF) haptic device based on a crank-slider mechanism was constructed. The device converts the constant rotation of an electric motor into the constrained movement of a small mass with asymmetric acceleration. The frequency that maximizes the perceived movement offered by the haptic device was investigated. Tests using three subjects showed that for the prototype, the best frequencies were 5 and 10 cycles per second.
C1 [Amemiya, Tomohiro; Ando, Hideyuki] NTT Corp, NTT Commun Sci Labs, Kanagawa 2430198, Japan.
C3 Nippon Telegraph & Telephone Corporation
RP Amemiya, T (corresponding author), NTT Corp, NTT Commun Sci Labs, 3-1 Morinosato Wakamiya, Kanagawa 2430198, Japan.
EM t-amemiya@avg.brl.ntt.co.jp
OI ANDO, HIDEYUKI/0000-0002-8405-3120; Amemiya,
   Tomohiro/0000-0002-7079-9167
CR Amemiya T, 2004, P IEEE VIRT REAL ANN, P165, DOI 10.1109/VR.2004.1310070
   Amemiya T., 2005, P 2005 INT C AUGM TE, P201
   Amemiya T., 2008, P EUROHAPTICS 2006, P317
   Ando H, 2004, IEICE T INF SYST, VE87D, P1354
   [Anonymous], 2002, Proc. Eurohaptics
   Chang Angela., 2005, CHI 05 EXTENDED ABST, P1264, DOI [DOI 10.1145/1056808.1056892, 10.1145/1056808.1056892]
   Chapman CD, 2001, BRAIN COGNITION, V46, P62, DOI 10.1016/S0278-2626(01)80035-X
   CUTKOSKY MR, 1990, DEXTROUS ROBOT HANDS, P5
   Deng M, 2006, INT J INNOV COMPUT I, V2, P705
   Gemperle F, 2001, FIFTH INTERNATIONAL SYMPOSIUM ON WEARABLE COMPUTERS, PROCEEDINGS, P5, DOI 10.1109/ISWC.2001.962082
   Gomi H, 1998, J NEUROSCI, V18, P8965
   Hirose M, 2001, P IEEE VIRT REAL ANN, P123, DOI 10.1109/VR.2001.913778
   Luk J., 2006, P SIGCHI C HUM FACT, P171, DOI DOI 10.1145/1124772.1124800
   MASSIE TH, 1994, P ASME DYN SYST CONT, V55, P295, DOI DOI 10.1234/12345678
   MUSSAIVALDI FA, 1985, J NEUROSCI, V5, P2732
   Nakamura N, 2005, World Haptics Conference: First Joint Eurohaptics Conference and Symposium on Haptic Interfaces for Virutual Environment and Teleoperator Systems, Proceedings, P633
   OULASVIRTA A, 2003, P CHI 2005, P919
   Poupyrev I., 2002, P 15 ANN ACM S US IN, P51, DOI [https://doi.org/10.1145/571985.571993, DOI 10.1145/571985.571993]
   ROTO V, 2005, WWW 05, P775
   Sato M, 2002, TSI PRESS S, V13, P17, DOI 10.1109/WAC.2002.1049515
   SWINDELLS C, 2003, ICMI 03, P52
   Tan HZ, 1997, FIRST INTERNATIONAL SYMPOSIUM ON WEARABLE COMPUTERS - DIGEST OF PAPERS, P84, DOI 10.1109/ISWC.1997.629923
   TANAKA Y, 2001, ICAT 2001, P115
   Ullmer B, 2000, IBM SYST J, V39, P915, DOI 10.1147/sj.393.0915
   Yano H, 2003, 11TH SYMPOSIUM ON HAPTIC INTERFACES FOR VIRTUAL ENVIRONMENT AND TELEOPERATOR SYSTEMS - HAPTICS 2003, PROCEEDINGS, P32, DOI 10.1109/HAPTIC.2003.1191223
NR 25
TC 56
Z9 61
U1 0
U2 0
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD AUG
PY 2008
VL 5
IS 3
AR 15
DI 10.1145/1402236.1402239
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 450YK
UT WOS:000266437700003
DA 2024-07-18
ER

PT J
AU Fontana, F
   Rocchesso, D
AF Fontana, Federico
   Rocchesso, Davide
TI Auditory Distance Perception in an Acoustic Pipe
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Experimentation; Measurement; Performance; Acoustic pipe; auditory
   display; distance perception
ID SOUND LOCALIZATION
AB In a study of auditory distance perception, we investigated the effects of exaggeration the acoustic cue of reverberation where the intensity of sound did not vary noticeably. The set of stimuli was obtained by moving a sound source inside a 10.2-m long pipe having a 0.3-m diameter. Twelve subjects were asked to listen to a speech sound while keeping their head inside the pipe and then to estimate the egocentric distance from the sound source using a magnitude production procedure. The procedure was repeated eighteen times using six different positions of the sound source. Results show that the point at which perceived distance equals physical distance is located approximately 3.5 m away from the listening point, with an average range of distance estimates of approximately 3.3 m, i.e., 1.65 to 4.9 m. The absence of intensity cues makes the acoustic pipe a potentially interesting modeling paradigm for the design of auditory interfaces in which distance is rendered independently
C1 [Fontana, Federico] Univ Verona, Dept Comp Sci, I-37134 Verona, Italy.
   [Rocchesso, Davide] Univ Venice, Dept Art & Ind Design, IUAV, I-30123 Venice, Italy.
C3 University of Verona; Universita Ca Foscari Venezia; IUAV University
   Venice
RP Fontana, F (corresponding author), Univ Verona, Dept Comp Sci, I-37134 Verona, Italy.
EM federico.fontana@univr.it; roc@iuav.it
RI Rocchesso, Davide/H-4711-2019
OI Rocchesso, Davide/0000-0002-0849-7766; Fontana,
   Federico/0000-0002-1692-2603
FU European project [FP6-NEST-29085]
FX This work has been partially supported by the European project
   FP6-NEST-29085 CLOSED - Closing the Loop Of Sound Evaluation and Design
   under the path "Measuring the impossible."
CR Algazi VR, 2001, PROCEEDINGS OF THE 2001 IEEE WORKSHOP ON THE APPLICATIONS OF SIGNAL PROCESSING TO AUDIO AND ACOUSTICS, P103, DOI 10.1109/ASPAA.2001.969553
   [Anonymous], 1987, Theoretical Acoustics
   Avendano C., 1999, Proceedings of the 1999 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics. WASPAA'99 (Cat. No.99TH8452), P179, DOI 10.1109/ASPAA.1999.810879
   BARRASS S, 2005, ACM T APPL PERCEPT, V4, P389
   Blauert J., 1983, SPATIAL HEARING PSYC
   Bronkhorst AW, 1999, NATURE, V397, P517, DOI 10.1038/17374
   Brown CP, 1998, IEEE T SPEECH AUDI P, V6, P476, DOI 10.1109/89.709673
   Brungart DS, 2002, PRESENCE-TELEOP VIRT, V11, P93, DOI 10.1162/105474602317343686
   COHEN M, 2002, P INT C AUD DISPL KY, P93
   Duda RO, 1998, J ACOUST SOC AM, V104, P3048, DOI 10.1121/1.423886
   Fontana F, 2003, P 2003 INT C AUD DIS, P79
   Fusiello A, 2002, FOURTH IEEE INTERNATIONAL CONFERENCE ON MULTIMODAL INTERFACES, PROCEEDINGS, P39, DOI 10.1109/ICMI.2002.1166966
   Gardner WG, 1998, SPRING INT SER ENG C, P85
   Gardner WilliamG., 1998, 3 D AUDIO USING LOUD
   Jenison RL, 1997, ECOL PSYCHOL, V9, P131, DOI 10.1207/s15326969eco0902_2
   Kramer Gregory, 1994, Auditory Display: Sonification, Audification and Auditory Interfaces
   Kulkarni A, 1998, NATURE, V396, P747, DOI 10.1038/25526
   Loomis JM, 1998, PERCEPT PSYCHOPHYS, V60, P966, DOI 10.3758/BF03211932
   Loomis JM, 1999, MIXED REALITY, P201
   LYON RH, 1983, J ACOUST SOC AM, V73, P1223, DOI 10.1121/1.389269
   Marston JamesR., 2006, ACM T APPL PERCEPT, V3, P110, DOI [DOI 10.1145/1141897.1141900, 10.1145/1141897.1141900]
   Moorer J. A., 1979, Computer Music Journal, V3, P13, DOI 10.2307/3680280
   OTTAVIANI L, 2002, P C DIG AUD EFF DAFX, P187
   RIFE DD, 1989, J AUDIO ENG SOC, V37, P419
   Rumsey F, 2012, Spatial audio
   Shinn-Cunningham BG, 2000, J ACOUST SOC AM, V107, P1627, DOI 10.1121/1.428447
   Ware C., 2020, INFORM VISUALIZATION
   Wightman FL, 1997, J ACOUST SOC AM, V101, P1050, DOI 10.1121/1.418029
   Zahorik P, 2002, J ACOUST SOC AM, V111, P1832, DOI 10.1121/1.1458027
   Zahorik P, 2002, J ACOUST SOC AM, V112, P2110, DOI 10.1121/1.1506692
   Zahorik P., 2002, P 2002 INT C AUDITOR
   Zahorik P, 2006, J ACOUST SOC AM, V120, P343, DOI 10.1121/1.2208429
   Zhai S., 1996, ACM T COMPUT-HUM INT, V3, P254, DOI [DOI 10.1145/234526.234532, 10.1145/234526.234532]
NR 33
TC 8
Z9 11
U1 0
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD AUG
PY 2008
VL 5
IS 3
AR 16
DI 10.1145/1402236.1402240
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 450YK
UT WOS:000266437700004
DA 2024-07-18
ER

PT J
AU Ware, C
   Mitchell, P
AF Ware, Colin
   Mitchell, Peter
TI Visualizing Graphs in Three Dimensions
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Design; Experimentation; Human Factors; Visualization; network
   visualization; graph visualization; stereoscopic displays
ID STEREO; TASK
AB It has been known for some time that larger graphs can be interpreted if laid out in 3D and displayed with stereo and/or motion depth cues to support spatial perception. However, prior studies were carried out using displays that provided a level of detail far short of what the human visual system is capable of resolving. Therefore, we undertook a graph comprehension study using a very high resolution stereoscopic display. In our first experiment, we examined the effect of stereoscopic display, kinetic depth, and using 3D tubes versus lines to display the links. The results showed a much greater benefit for 3D viewing than previous studies. For example, with both motion and stereoscopic depth cues, unskilled observers could see paths between nodes in 333 node graphs with less than a 10% error rate. Skilled observers could see up to a 1000-node graph with less than a 10% error rate. This represented an order of magnitude increase over 2D display. In our second experiment, we varied both nodes and links to understand the constraints on the number of links and the size of graph that can be reliably traced. We found the difference between number of links and number of nodes to best account for error rates and suggest that this is evidence for a "perceptual phase transition." These findings are discussed in terms of their implications for information display.
C1 [Ware, Colin; Mitchell, Peter] Univ New Hampshire, Data Visualizat Res Lab, Ctr Coastal & Ocean Mapping, Durham, NH 03824 USA.
C3 University System Of New Hampshire; University of New Hampshire
RP Ware, C (corresponding author), Univ New Hampshire, Data Visualizat Res Lab, Ctr Coastal & Ocean Mapping, Durham, NH 03824 USA.
EM cware@ccom.unh.edu
OI Mitchell, Peter/0000-0001-5215-320X
FU NSF ITR [0324899]; Advanced Research and Development Activity (ARDA)
   [HM1582-05-C-0023]; Direct For Computer & Info Scie & Enginr; Division
   of Computing and Communication Foundations [0324899] Funding Source:
   National Science Foundation
FX Funding for the first experiment was provided by NSF ITR grant 0324899
   and for the second experiment was provided by the Advanced Research and
   Development Activity (ARDA) under contract number HM1582-05-C-0023.
CR [Anonymous], 1838, Philos. Trans. R. Soc., DOI DOI 10.1098/RSTL.1838.0019
   ARTHUR KW, 1993, ACM T INFORM SYST, V11, P239, DOI 10.1145/159161.155359
   BOLLOBAS B, 2001, RANDOM GRAPHS, P151
   CAMPBELL FW, 1965, NATURE, V208, P191, DOI 10.1038/208191a0
   Di Battista G., 1999, Graph Drawing: Algorithms for the Visualization of Graphs, V357
   Frisby JP, 1996, PERCEPTION, V25, P129, DOI 10.1068/p250129
   FRUCHTERMAN TMJ, 1991, SOFTWARE PRACT EXPER, V21, P1129, DOI 10.1002/spe.4380211102
   Howard I, 1995, OXFORD PSYCHOL SERIE, V29
   Huang ML, 1998, J VISUAL LANG COMPUT, V9, P623, DOI 10.1006/jvlc.1998.0094
   Kleiberg E, 2001, IEEE SYMPOSIUM ON INFORMATION VISUALIZATION 2001, PROCEEDINGS, P87, DOI 10.1109/INFVIS.2001.963285
   Munzner T, 1997, IEEE SYMPOSIUM ON INFORMATION VISUALIZATION, PROCEEDINGS, P2, DOI 10.1109/INFVIS.1997.636718
   Munzner T., 1999, Proceedings 1999 IEEE Symposium on Information Visualization (InfoVis'99), P132, DOI 10.1109/INFVIS.1999.801869
   Norman JF, 1996, J EXP PSYCHOL HUMAN, V22, P173, DOI 10.1037/0096-1523.22.1.173
   Parker G, 1998, J VISUAL LANG COMPUT, V9, P299, DOI 10.1006/jvlc.1998.0086
   Plaisant C, 2002, INFOVIS 2002: IEEE SYMPOSIUM ON INFORMATION VISUALIZATION 2002, P57, DOI 10.1109/INFVIS.2002.1173148
   ROBERTSON GG, 1993, COMMUN ACM, V36, P57, DOI 10.1145/255950.153577
   ROGERS B, 1989, NATURE, V339, P135, DOI 10.1038/339135a0
   SOLLENBERGER RL, 1993, HUM FACTORS, V35, P483, DOI 10.1177/001872089303500306
   TYLER CW, 1975, VISION RES, V15, P583, DOI 10.1016/0042-6989(75)90306-5
   UOMORI KY, 1994, PERCEPT PSYCHOPHYS, V55, P526, DOI 10.3758/BF03205310
   van Ee R, 2000, VISION RES, V40, P151, DOI 10.1016/S0042-6989(99)00174-1
   WALLACH H, 1953, J EXP PSYCHOL, V45, P205, DOI 10.1037/h0056880
   Ware C., 2005, Information Visualization, V4, P49, DOI 10.1057/palgrave.ivs.9500090
   Ware C, 1996, ACM T GRAPHIC, V15, P121, DOI 10.1145/234972.234975
   WARE C, 1993, INTERCHI, P37
   Wills GJ, 1999, J COMPUT GRAPH STAT, V8, P190, DOI 10.2307/1390633
NR 26
TC 94
Z9 104
U1 0
U2 14
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD JAN
PY 2008
VL 5
IS 1
AR 2
DI 10.1145/1279640.1279642
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 450YI
UT WOS:000266437500002
DA 2024-07-18
ER

PT J
AU Sundstedt, V
   Gutierrez, D
   Anson, O
   Banterle, F
   Chalmers, A
AF Sundstedt, Veronica
   Gutierrez, Diego
   Anson, Oscar
   Banterle, Francesco
   Chalmers, Alan
TI Perceptual Rendering of Participating Media
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Algorithms; Performance; Experimentation; Human Factors; Participating
   media; selective rendering; perception; saliency map; extinction map;
   light map; attention
ID VISUAL-ATTENTION; EFFICIENT; MODEL
AB High-fidelity image synthesis is the process of computing images that are perceptually indistinguishable from the real world they are attempting to portray. Such a level of fidelity requires that the physical processes of materials and the behavior of light are accurately simulated. Most computer graphics algorithms assume that light passes freely between surfaces within an environment. However, in many applications, we also need to take into account how the light interacts with media, such as dust, smoke, fog, etc., between the surfaces. The computational requirements for calculating the interaction of light with such participating media are substantial. This process can take many hours and rendering effort is often spent on computing parts of the scene that may not be perceived by the viewer. In this paper, we present a novel perceptual strategy for physically based rendering of participating media. By using a combination of a saliency map with our new extinction map (X map), we can significantly reduce rendering times for inhomogeneous media. The visual quality of the resulting images is validated using two objective difference metrics and a subjective psychophysical experiment. Although the average pixel errors of these metric are all less than 1%, the subjective validation indicates that the degradation in quality still is noticeable for certain scenes. We thus introduce and validate a novel light map (L map) that accounts for salient features caused by multiple light scattering around light sources.
C1 [Sundstedt, Veronica; Banterle, Francesco; Chalmers, Alan] Univ Bristol, Dept Comp Sci, Bristol BS8 1UB, Avon, England.
   [Gutierrez, Diego; Anson, Oscar] Univ Zaragoza, Dept Informat & Ingn Sistemas, Grp Informat Graf Avanzada GIGA, Zaragoza 50018, Spain.
C3 University of Bristol; University of Zaragoza
RP Sundstedt, V (corresponding author), Univ Bristol, Dept Comp Sci, Merchant Venturers Bldg,Woodland Rd, Bristol BS8 1UB, Avon, England.
EM veronica@cs.bris.ac.uk; diegog@unizar.es
RI Banterle, Francesco/AAE-5953-2020
OI Banterle, Francesco/0000-0002-6374-6657; Sundstedt,
   Veronica/0000-0003-3639-9327
CR ANSON O, 2006, APGV 06, P135
   BOLIN MR, 1998, COMPUTER GRAPHICS, V32, P299
   BOLIN MR, 1995, SIGGRAPH 95 C P LOS, P409
   Cater K., 2003, Eurographics Symposium on Rendering. 14th Eurographics Workshop on Rendering, P270
   CHALMERS A, 2006, P GRAPHITE 2006 ACM
   Debattista K, 2005, SIBGRAPI 2005: XVIII BRAZILIAN SYMPOSIUM ON COMPUTER GRAPHICS AND IMAGE PROCESSING, CONFERENCE PROCEEDINGS, P375
   Dumont R, 2003, ACM T GRAPHIC, V22, P152, DOI 10.1145/636886.636888
   FERWERDA JA, 1997, COMPUTER GRAPHICS, V31, P143
   Glassner A.S., 1994, PRINCIPLES DIGITAL I
   GUTIERREZ D., 2005, Eurographics Symposium on Rendering, P291
   Haber J, 2001, COMPUT GRAPH FORUM, V20, pC142, DOI 10.1111/1467-8659.00507
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   JENSEN H.W., 1998, SIGGRAPH 98 Conference Proceedings, Annual Conference Series, P311
   Kajiya J. T., 1986, Computer Graphics, V20, P143, DOI 10.1145/15886.15902
   Longhurst P, 2005, SIBGRAPI 2005: XVIII BRAZILIAN SYMPOSIUM ON COMPUTER GRAPHICS AND IMAGE PROCESSING, CONFERENCE PROCEEDINGS, P359
   Longhurst P., 2006, P 4 INT C COMPUTER G, P21
   MARTENS W, 1998, P IEEE VIS 98, P18
   MITCHELL DP, 1987, SIGGRAPH 87, P65
   Myszkowski K., 1998, Rendering Techniques '98. Proceedings of the Eurographics Workshop, P223
   MYSZKOWSKI K, 2002, SCCG 02, P13
   NARASIMHAN SG, 2003, IEEE C COMP VIS PATT, V1, P65
   NEUMANN D, 2002, PERCEPTION BASED IMA
   PRIKRYL J, 2001, THESIS VIENNA U TECH
   Ramasubramanian M, 1999, COMP GRAPH, P73, DOI 10.1145/311535.311543
   RUSHMEIER H, 1994, P 5 EUR WORKSH REND, P117
   Siegel R., 1992, Thermal Radiation Heat Transfer
   Stokes WA, 2004, ACM T GRAPHIC, V23, P742, DOI 10.1145/1015706.1015795
   Sun B, 2005, ACM T GRAPHIC, V24, P1040, DOI 10.1145/1073204.1073309
   SUNDSTEDT V, 2005, SPRING C COMP GRAPH, P162
   SUNDSTEDT V, 2005, APGV 05, P166
   *TOB, 2006, US MAN TOB EY TRACK
   Watson A.B., 1993, DIGITAL IMAGES HUMAN, P179
   WOOLLEY C, 2003, SI3D 03, P143
   Yee H, 2001, ACM T GRAPHIC, V20, P39, DOI 10.1145/383745.383748
NR 34
TC 11
Z9 12
U1 0
U2 2
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD NOV
PY 2007
VL 4
IS 3
AR 15
DI 10.1145/1278387.1278389
PG 22
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA V04IN
UT WOS:000207052200002
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Nasiri, M
   Porter, J
   Kohm, K
   Robb, A
AF Nasiri, Moloud
   Porter, John
   Kohm, Kristopher
   Robb, Andrew
TI Changes in Navigation over Time: A Comparison of Teleportation and
   Joystick-Based Locomotion
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article; Proceedings Paper
CT ACM Symposium on Applied Perception (SAP)
CY AUG 05-06, 2023
CL Los Angeles, CA
SP Assoc Comp Machinery, ACM SIGGRAPH
DE Virtual reality; locomotion; long-term use
ID VIRTUAL ENVIRONMENTS; PERFORMANCE; WALKING; EXPERIENCE; REAL
AB Little research has studied how people use Virtual Reality (VR) changes as they experience VR. This article reports the results of an experiment investigating how users' behavior with two locomotion methods changed over 4 weeks: teleportation and joystick-based locomotion. Twenty novice VR users (with no more than 1 hour prior experience with any form of walking in VR) were recruited. They loaned an Oculus Quest for 4 weeks on their own time, including an activity we provided them with. Results showed that the time required to complete the navigation task decreased faster for joystick-based locomotion. Spatial memory improved with time, particularly when using teleportation (which starts disadvantaged to joystick-based locomotion). In addition, overall cybersickness decreased slightly over time; however, two dimensions of cybersickness (nausea and disorientation) increased notably over time using joystick-based navigation.
C1 [Nasiri, Moloud; Porter, John; Kohm, Kristopher; Robb, Andrew] Clemson Univ, Clemson, SC 29631 USA.
C3 Clemson University
RP Nasiri, M (corresponding author), Clemson Univ, Clemson, SC 29631 USA.
EM mnasiri@clemson.edu; jporte@morehouse.edu; kckohm@clemson.edu;
   arobb@clemson.edu
RI Kohm, Kristopher/KVY-1160-2024
OI Kohm, Kristopher/0000-0002-7525-991X; Robb, Andrew/0000-0002-0398-5576
FU U.S. National Science Foundation (CISE HCC) [1717937]
FX This work was supported in part by U.S. National Science Foundation
   (CISE HCC) grant #1717937.
CR Bailenson JN, 2006, PRESENCE-TELEOP VIRT, V15, P699, DOI 10.1162/pres.15.6.699
   Bates D, 2015, J STAT SOFTW, V67, P1, DOI 10.18637/jss.v067.i01
   Bhandari J., 2018, P 44 GRAPHICS INTERF, P162, DOI [DOI 10.20380/GI2018.22, 10.20380/GI2018.223, DOI 10.20380/GI2018.223]
   Boletsis Costas, 2017, Multimodal Technologies and Interaction, V1, DOI 10.3390/mti1040024
   Boletsis C, 2019, ADV HUM-COMPUT INTER, V2019, DOI 10.1155/2019/7420781
   Bowman DA, 1999, PRESENCE-TELEOP VIRT, V8, P618, DOI 10.1162/105474699566521
   Bozgeyikli E, 2016, SUI'16: PROCEEDINGS OF THE 2016 SYMPOSIUM ON SPATIAL USER INTERACTION, P33, DOI 10.1145/2983310.2985763
   Bozgeyikli E, 2016, CHI PLAY 2016: PROCEEDINGS OF THE 2016 ANNUAL SYMPOSIUM ON COMPUTER-HUMAN INTERACTION IN PLAY, P205, DOI 10.1145/2967934.2968105
   Burigat S, 2007, INT J HUM-COMPUT ST, V65, P945, DOI 10.1016/j.ijhcs.2007.07.003
   Calandra D, 2019, IEEE ICCE, P348, DOI [10.1109/icce-berlin47944.2019.8966165, 10.1109/ICCE-Berlin47944.2019.8966165]
   Coomer N, 2018, ACM SYMPOSIUM ON APPLIED PERCEPTION (SAP 2018), DOI 10.1145/3225153.3225175
   Cummings JJ, 2016, MEDIA PSYCHOL, V19, P272, DOI 10.1080/15213269.2015.1015740
   Extend Reality Ltd, ExtendRealityLtd/VRTK.Prefabs: *deprecated*-A collection of productive prefabs for rapidly building spatial computing solutions in the unity software
   Fernandes AS, 2016, 2016 IEEE SYMPOSIUM ON 3D USER INTERFACES (3DUI), P201, DOI 10.1109/3DUI.2016.7460053
   Frey A, 2007, COMPUT HUM BEHAV, V23, P2026, DOI 10.1016/j.chb.2006.02.010
   Hale K.S., 2014, Handbook of virtual environments: Design, implementation, and applications
   Jacobs DM, 2006, J EXP PSYCHOL HUMAN, V32, P443, DOI 10.1037/0096-1523.32.2.443
   Jul Susanne., 1997, SIGCHI BULL, V29, P44
   Kelly JW, 2008, PERCEPT PSYCHOPHYS, V70, P158, DOI 10.3758/PP.70.1.158
   Kennedy RS, 1993, The international journal of aviation psychology, V3, P203, DOI [DOI 10.1207/S15327108IJAP0303, 10.1207/s15327108ijap0303_3]
   Kohm K, 2022, ACM T APPL PERCEPT, V19, DOI 10.1145/3561055
   Langbehn E, 2018, PROCEEDINGS OF THE VIRTUAL REALITY INTERNATIONAL CONFERENCE - LAVAL VIRTUAL (ACM VRIC 2018), DOI 10.1145/3234253.3234291
   Lin J, 2019, ADV ENG INFORM, V39, P53, DOI 10.1016/j.aei.2018.11.007
   Martelli D, 2019, GAIT POSTURE, V67, P251, DOI 10.1016/j.gaitpost.2018.10.029
   Murias K, 2016, COMPUT HUM BEHAV, V58, P398, DOI 10.1016/j.chb.2016.01.020
   Nasiri M, 2021, 2021 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES ABSTRACTS AND WORKSHOPS (VRW 2021), P733, DOI 10.1109/VRW52623.2021.00249
   Peck TC, 2010, P IEEE VIRT REAL ANN, P35, DOI 10.1109/VR.2010.5444816
   Pournelle G. H., 1953, Journal of Mammalogy, V34, P133, DOI 10.1890/0012-9658(2002)083[1421:SDEOLC]2.0.CO;2
   Razzaque S., 2005, REDIRECTED WALKING
   Richardson AE, 2011, COMPUT HUM BEHAV, V27, P552, DOI 10.1016/j.chb.2010.10.003
   Robb A, 2022, ACM T APPL PERCEPT, V19, DOI 10.1145/3560818
   Sarupuri Bhuvaneswari, 2017, P 27 INT C ART REAL, P133, DOI [10.2312/egve.20171350, DOI 10.2312/EGVE.20171350]
   SATTERTHWAITE FE, 1946, BIOMETRICS BULL, V2, P110, DOI 10.2307/3002019
   Schubert T, 2001, PRESENCE-VIRTUAL AUG, V10, P266, DOI 10.1162/105474601300343603
   SCHWARZ G, 1978, ANN STAT, V6, P461, DOI 10.1214/aos/1176344136
   Smith SR, 2009, 3DUI : IEEE SYMPOSIUM ON 3D USER INTERFACES 2009, PROCEEDINGS, P3, DOI 10.1109/3DUI.2009.4811198
   Templeman JN, 1999, PRESENCE-TELEOP VIRT, V8, P598, DOI 10.1162/105474699566512
   Trenholme David, 2008, Virtual Reality, V12, P181, DOI 10.1007/s10055-008-0092-z
   Unity, Dungeon Architect: Utilities Tools
   Vlahovic S, 2018, INT WORK QUAL MULTIM, P177
   Voeten C., 2020, Using 'buildmer' to automatically find & compare maximal (mixed) models
   Wickham H, 2009, USE R, P1, DOI 10.1007/978-0-387-98141-3
   Xu MX, 2017, P IEEE VIRT REAL ANN, P315, DOI 10.1109/VR.2017.7892303
NR 43
TC 1
Z9 1
U1 4
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD OCT
PY 2023
VL 20
IS 4
SI SI
AR 16
DI 10.1145/3613902
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA Z8RR5
UT WOS:001114697800005
OA hybrid
DA 2024-07-18
ER

PT J
AU Arabadzhiyska, E
   Tursun, C
   Seidel, HP
   Didyk, P
AF Arabadzhiyska, Elena
   Tursun, Cara
   Seidel, Hans-Peter
   Didyk, Piotr
TI Practical Saccade Prediction for Head-Mounted Displays: Towards a
   Comprehensive Model
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Eye tracking; saccade prediction; gaze contingency; smooth pursuit
   eye-motion
ID EYE-TRACKER SIGNAL; VISUAL THRESHOLD; SMOOTH-PURSUIT; BINOCULAR
   COORDINATION; VERGENCE; TARGETS; MOVEMENTS; GAZE; SUPPRESSION; DYNAMICS
AB Eye-tracking technology has started to become an integral component of new display devices such as virtual and augmented reality headsets. Applications of gaze information range from new interaction techniques that exploit eye patterns to gaze-contingent digital content creation. However, system latency is still a significant issue in many of these applications because it breaks the synchronization between the current and measured gaze positions. Consequently, it may lead to unwanted visual artifacts and degradation of the user experience. In this work, we focus on foveated rendering applications where the quality of an image is reduced towards the periphery for computational savings. In foveated rendering, the presence of system latency leads to delayed updates to the rendered frame, making the quality degradation visible to the user. To address this issue and to combat system latency, recent work proposes using saccade landing position prediction to extrapolate gaze information from delayed eye tracking samples. Although the benefits of such a strategy have already been demonstrated, the solutions range from simple and efficient ones, which make several assumptions about the saccadic eye movements, to more complex and costly ones, which use machine learning techniques. However, it is unclear to what extent the prediction can benefit from accounting for additional factors and how more complex predictions can be performed efficiently to respect the latency requirements. This paper presents a series of experiments investigating the importance of different factors for saccades prediction in common virtual and augmented reality applications. In particular, we investigate the effects of saccade orientation in 3D space and smooth pursuit eye-motion (SPEM) and how their influence compares to the variability across users. We also present a simple, yet efficient post-hoc correction method that adapts existing saccade prediction methods to handle these factors without performing extensive data collection. Furthermore, our investigation and the correction technique may also help future developments of machine-learning-based techniques by limiting the required amount of training data.
C1 [Arabadzhiyska, Elena; Seidel, Hans-Peter] Max Planck Inst Informat, Campus E1 4, D-66123 Saarbrucken, Germany.
   [Arabadzhiyska, Elena] Univ Saarland, Saarbrucken, Germany.
   [Tursun, Cara; Didyk, Piotr] Univ Svizzera Italiana, Via Giuseppe Buffi 13, CH-6900 Lugano, Switzerland.
   [Tursun, Cara] Univ Groningen, Dept Comp Sci, Nijenborgh 9, NL-9747 AG Groningen, Netherlands.
C3 Max Planck Society; Saarland University; Universita della Svizzera
   Italiana; University of Groningen
RP Arabadzhiyska, E (corresponding author), Max Planck Inst Informat, Campus E1 4, D-66123 Saarbrucken, Germany.
OI Seidel, Hans-Peter/0000-0002-1343-8613; Didyk,
   Piotr/0000-0003-0768-8939; /0000-0002-9729-5460
FU European Research Council (ERC) under the European Union's Horizon 2020
   research and innovation program [804226 PERDY]
FX This project has received funding from the European Research Council
   (ERC) under the European Union's Horizon 2020 research and innovation
   program (grant agreement No. 804226 PERDY).
CR Albert R, 2017, ACM T APPL PERCEPT, V14, DOI 10.1145/3127589
   Anliker J., 1976, EYE MOVEMENTS PSYCHO, P185
   Arabadzhiyska E, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073642
   BAHILL A T, 1975, Mathematical Biosciences, V24, P191, DOI 10.1016/0025-5564(75)90075-9
   BAHILL AT, 1975, INVEST OPHTH VISUAL, V14, P468
   Baker JT, 2003, J NEUROPHYSIOL, V89, P2564, DOI 10.1152/jn.00610.2002
   BECKER W, 1979, VISION RES, V19, P967, DOI 10.1016/0042-6989(79)90222-0
   BEELER GW, 1967, VISION RES, V7, P769, DOI 10.1016/0042-6989(67)90039-9
   Binda P, 2018, ANNU REV VIS SCI, V4, P193, DOI 10.1146/annurev-vision-091517-034317
   BOGHEN D, 1974, INVEST OPHTH VISUAL, V13, P619
   Bouman Maarten A., 1965, CORTICAL CONTROL EYE
   Bremmer F, 2009, J NEUROSCI, V29, P12374, DOI 10.1523/JNEUROSCI.2908-09.2009
   BURR DC, 1994, NATURE, V371, P511, DOI 10.1038/371511a0
   CAMPBELL FW, 1978, VISION RES, V18, P1297, DOI 10.1016/0042-6989(78)90219-5
   Carpenter Roger., 1988, Movements of the Eyes, V2
   Castet E, 2000, NAT NEUROSCI, V3, P177, DOI 10.1038/72124
   Collewijn H, 1997, VISION RES, V37, P1049, DOI 10.1016/S0042-6989(96)00245-3
   COLLEWIJN H, 1988, J PHYSIOL-LONDON, V404, P183, DOI 10.1113/jphysiol.1988.sp017285
   COLLEWIJN H, 1988, J PHYSIOL-LONDON, V404, P157, DOI 10.1113/jphysiol.1988.sp017284
   Costela FM, 2019, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00960
   DEUBEL H, 1987, BIOL CYBERN, V57, P37, DOI 10.1007/BF00318714
   Ditchburn R. W., 1955, OPT ACTA, V1, P171, DOI [DOI 10.1080/713818684, 10.1080/713818684]
   Drewes J, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0111197
   ENRIGHT JT, 1984, J PHYSIOL-LONDON, V350, P9, DOI 10.1113/jphysiol.1984.sp015186
   ENRIGHT JT, 1986, J PHYSIOL-LONDON, V371, P69, DOI 10.1113/jphysiol.1986.sp015962
   ERKELENS CJ, 1989, PROC R SOC SER B-BIO, V236, P441, DOI 10.1098/rspb.1989.0031
   GELLMAN RS, 1992, EXP BRAIN RES, V89, P425
   Griffith H., 2018, P FUT TECHN C FTC 20, P79
   Griffith H, 2020, 2020 10TH ANNUAL COMPUTING AND COMMUNICATION WORKSHOP AND CONFERENCE (CCWC), P18, DOI [10.1109/ccwc47524.2020.9031274, 10.1109/CCWC47524.2020.9031274]
   Griffith Henry, 2020, ACM S EYE TRACKING R, DOI [10.1145/3379157.3388935, DOI 10.1145/3379157.3388935]
   Guenter B, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366183
   Han P, 2013, J VISION, V13, DOI 10.1167/13.8.27
   Hendrickson A, 2005, MACULAR DEGENERATION, P1, DOI 10.1007/3-540-26977-0_1
   Herter TM, 1998, J NEUROPHYSIOL, V80, P2785, DOI 10.1152/jn.1998.80.5.2785
   Hooge I, 2016, VISION RES, V128, P6, DOI 10.1016/j.visres.2016.09.002
   Hooge I, 2015, VISION RES, V112, P55, DOI 10.1016/j.visres.2015.03.015
   Hooge ITC, 2019, VISION RES, V156, P1, DOI 10.1016/j.visres.2019.01.004
   Hua H, 2006, OPT EXPRESS, V14, P4328, DOI 10.1364/OE.14.004328
   Ibbotson MR, 2009, CURR BIOL, V19, pR493, DOI 10.1016/j.cub.2009.05.010
   Irving EL, 2019, EXP EYE RES, V183, P38, DOI 10.1016/j.exer.2018.08.020
   Jaschinski W, 2016, J EYE MOVEMENT RES, V9, DOI 10.16910/jemr.9.4.2
   JURGENS R, 1975, BASIC MECHANISMS OCU, P525
   Komogortsev Oleg V., 2009, Journal of Control Theory and Applications, V7, P14, DOI 10.1007/s11768-009-7218-z
   Komogortsev OV, 2008, PROCEEDINGS OF THE EYE TRACKING RESEARCH AND APPLICATIONS SYMPOSIUM (ETRA 2008), P229, DOI 10.1145/1344471.1344525
   LATOUR PL, 1962, VISION RES, V2, P261, DOI 10.1016/0042-6989(62)90031-7
   Leigh RJ., 2015, The neurology of eye movements, V5th ed., DOI [10.1093/MED/9780199969289.001.0001, DOI 10.1093/MED/9780199969289.001.0001]
   Lesmes LA, 2010, J VISION, V10, DOI 10.1167/10.3.17
   Majaranta P., 2014, ADV PHYSL COMPUTING, P39, DOI DOI 10.1007/978-1-4471-6392-3_3
   MCKENZIE A, 1986, J NEUROPHYSIOL, V56, P196, DOI 10.1152/jn.1986.56.1.196
   Meng XX, 2018, P ACM COMPUT GRAPH, V1, DOI 10.1145/3203199
   Moraes AG, 2020, PHYSIOTHER THEOR PR, V36, P259, DOI 10.1080/09593985.2018.1484534
   Morales A, 2021, IEEE ACCESS, V9, P52474, DOI [10.1109/ACCESS.2021.3070511, 10.1109/access.2021.3070511]
   Nyström M, 2016, VISION RES, V121, P95, DOI 10.1016/j.visres.2016.01.009
   Nyström M, 2013, VISION RES, V92, P59, DOI 10.1016/j.visres.2013.09.009
   OHTSUKA K, 1994, INVEST OPHTH VIS SCI, V35, P509
   ONO H, 1978, VISION RES, V18, P735, DOI 10.1016/0042-6989(78)90152-9
   Paeye C, 2016, J VISION, V16, DOI 10.1167/16.10.15
   Patney A, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980246
   PELISSON D, 1988, VISION RES, V28, P87, DOI 10.1016/S0042-6989(88)80009-9
   RITTER M, 1976, PSYCHOL RES-PSYCH FO, V39, P67, DOI 10.1007/BF00308946
   SCHLAG J, 1990, J NEUROPHYSIOL, V64, P575, DOI 10.1152/jn.1990.64.2.575
   Schor CM., 2011, Adler's Physiology of the eye. 11th, P220
   Schweitzer R, 2020, BEHAV RES METHODS, V52, P1122, DOI 10.3758/s13428-019-01304-3
   Shibata T, 2011, J VISION, V11, DOI 10.1167/11.8.11
   Sipatchin A, 2021, HEALTHCARE-BASEL, V9, DOI 10.3390/healthcare9020180
   Smeets JBJ, 2000, HUM MOVEMENT SCI, V19, P275, DOI 10.1016/S0167-9457(00)00015-4
   Stein N, 2021, I-PERCEPTION, V12, DOI 10.1177/2041669520983338
   Swafford N. T., 2016, P ACM S APPL PERC SA, P7, DOI DOI 10.1145/2931002.2931011
   Tursun OT, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3322985
   VANOPSTAL AJ, 1987, VISION RES, V27, P731, DOI 10.1016/0042-6989(87)90071-X
   VOLKMANN FC, 1962, J OPT SOC AM, V52, P571, DOI 10.1364/JOSA.52.000571
   VOLKMANN FC, 1978, VISION RES, V18, P1193, DOI 10.1016/0042-6989(78)90104-9
   Wang S, 2017, J VISION, V17, DOI 10.1167/17.14.3
   WESTHEIMER G, 1954, AMA ARCH OPHTHALMOL, V52, P710
   Whitmire E, 2016, IEEE INT SYM WRBL CO, P184, DOI 10.1145/2971763.2971771
   Yang Q, 2004, EXP BRAIN RES, V156, P212, DOI 10.1007/s00221-003-1773-1
   Yang Q, 2002, INVEST OPHTH VIS SCI, V43, P2939
   Yarbus AL., 1956, Biofizika, V1, P76
   YOUNG LR, 1963, IEEE T HUM FACT ENG, VHFE4, P38, DOI 10.1109/THFE.1963.231285
   ZEE DS, 1992, J NEUROPHYSIOL, V68, P1624, DOI 10.1152/jn.1992.68.5.1624
   Zhou W, 2009, INT J NEURAL SYST, V19, P309, DOI 10.1142/S0129065709002051
   Zivotofsky AZ, 1996, J NEUROPHYSIOL, V76, P3617, DOI 10.1152/jn.1996.76.6.3617
   ZUBER BL, 1966, EXP NEUROL, V16, P65, DOI 10.1016/0014-4886(66)90087-2
NR 83
TC 1
Z9 1
U1 1
U2 5
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD JAN
PY 2023
VL 20
IS 1
AR 2
DI 10.1145/3568311
PG 23
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 7S6AV
UT WOS:000910834800002
OA Green Published, Green Submitted
DA 2024-07-18
ER

PT J
AU Um, K
   Hu, XY
   Wang, B
   Thuerey, N
AF Um, Kiwon
   Hu, Xiangyu
   Wang, Bing
   Thuerey, Nils
TI Spot the Difference: Accuracy of Numerical Simulations via the Human
   Visual System
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Evaluation of numerical simulation; essentially non-oscillatory schemes;
   human visual system
ID SCHEMES; FLOW; PERFORMANCE
AB Comparative evaluation lies at the heart of science, and determining the accuracy of a computational method is crucial for evaluating its potential as well as for guiding future efforts. However, metrics that are typically used have inherent shortcomings when faced with the under-resolved solutions of real-world simulation problems. We show how to leverage the human visual system in conjunction with crowd-sourced user studies to address the fundamental problems of widely used classical evaluation metrics. We demonstrate that such user studies driven by visual perception yield a very robust metric and consistent answers for complex phenomena without any requirements for proficiency regarding the physics at hand. This holds even for cases away from convergence where traditional metrics often end up with inconclusive results. More specifically, we evaluate results of different essentially non-oscillatory (ENO) schemes in different fluid flow settings. Our methodology represents a novel and practical approach for scientific evaluations that can give answers for previously unsolved problems.
C1 [Um, Kiwon] Tech Univ Munich, Munich, Germany.
   [Um, Kiwon] IP Paris, Telecom Paris, LTCI, Paris, France.
   [Hu, Xiangyu; Thuerey, Nils] Tech Univ Munich, Dept Mech Engn, Boltzmannstr 15, D-100084 Munich, Germany.
   [Wang, Bing] Tsinghua Univ, Sch Aerosp Engn, Beijing 100084, Peoples R China.
   [Um, Kiwon] Telecom Paris, LTCI, IDS Dept, 19 Pl Marguerite Perey, F-91120 Palaiseau, France.
C3 Technical University of Munich; IMT - Institut Mines-Telecom; Institut
   Polytechnique de Paris; Telecom Paris; Technical University of Munich;
   Tsinghua University; IMT - Institut Mines-Telecom; Institut
   Polytechnique de Paris; Telecom Paris
RP Thuerey, N (corresponding author), Tech Univ Munich, Dept Mech Engn, Boltzmannstr 15, D-100084 Munich, Germany.
EM kiwon.um@telecom-paris.fr; xiangyu.hu@tum.de; wbing@tsinghua.edu.cn;
   nils.thuerey@tum.de
RI Hu, Xiangyu/O-9987-2019
OI Hu, Xiangyu/0000-0003-0932-6659; /0000-0002-4139-9308
FU ERC [637014]; National Natural Science Foundation of China [11628206];
   European Research Council (ERC) [637014] Funding Source: European
   Research Council (ERC)
FX This work was supported by the ERC Starting Grant realFlow (637014) and
   the National Natural Science Foundation of China (No. 11628206).
CR Adami S, 2012, J COMPUT PHYS, V231, P7057, DOI 10.1016/j.jcp.2012.05.005
   Albright TD, 2002, ANNU REV NEUROSCI, V25, P339, DOI 10.1146/annurev.neuro.25.112701.142900
   [Anonymous], 2001, Measurement Science and Technology
   [Anonymous], 2006, MODERN IMAGE QUALITY
   Balsara DS, 2009, J COMPUT PHYS, V228, P2480, DOI 10.1016/j.jcp.2008.12.003
   Battaglia PW, 2013, P NATL ACAD SCI USA, V110, P18327, DOI 10.1073/pnas.1306572110
   Borges R, 2008, J COMPUT PHYS, V227, P3191, DOI 10.1016/j.jcp.2007.11.038
   BRACHET ME, 1983, J FLUID MECH, V130, P411, DOI 10.1017/S0022112083001159
   BRADLEY RA, 1952, BIOMETRIKA, V39, P324, DOI 10.1093/biomet/39.3-4.324
   Brehm C, 2015, COMPUT FLUIDS, V122, P184, DOI 10.1016/j.compfluid.2015.08.023
   Cheng X, 2011, SCIENCE, V333, P1276, DOI 10.1126/science.1207032
   Christie M.A., 2005, Los Alamos Science, P6
   Cornsweet T.N., 1970, Visual Perception
   Crump MJC, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0057410
   Daru V, 2001, COMPUT FLUIDS, V30, P89, DOI 10.1016/S0045-7930(00)00006-2
   Dumbser M, 2007, J COMPUT PHYS, V221, P693, DOI 10.1016/j.jcp.2006.06.043
   Fechner G. T., 1860, Elemente der Psychophysik
   Fu L, 2016, J COMPUT PHYS, V305, P333, DOI 10.1016/j.jcp.2015.10.037
   HARTEN A, 1987, SIAM J NUMER ANAL, V24, P279, DOI 10.1137/0724022
   Hu XY, 2010, J COMPUT PHYS, V229, P8952, DOI 10.1016/j.jcp.2010.08.019
   Hunter DR, 2004, ANN STAT, V32, P384
   Irshad H, 2017, SCI REP-UK, V7, DOI 10.1038/srep43286
   Issa R., 2017, SPHERIC VALIDATION T
   Jagnow R, 2008, ACM T APPL PERCEPT, V4, DOI 10.1145/1278760.1278765
   Jiang GS, 1996, J COMPUT PHYS, V126, P202, DOI 10.1006/jcph.1996.0130
   Johnsen E, 2010, J COMPUT PHYS, V229, P1213, DOI 10.1016/j.jcp.2009.10.028
   Johnsen Eric, 2011, P 20 AIAA COMP FLUID
   Kat CJ, 2012, MATH COMP MODEL DYN, V18, P487, DOI 10.1080/13873954.2012.663392
   Kendall MG, 1938, BIOMETRIKA, V30, P81, DOI 10.2307/2332226
   Kjellin A, 2010, ACM T APPL PERCEPT, V7, DOI 10.1145/1773965.1773970
   LARKIN J, 1980, SCIENCE, V208, P1335, DOI 10.1126/science.208.4450.1335
   Lin Z, 1998, SCIENCE, V281, P1835, DOI 10.1126/science.281.5384.1835
   Mehta UB., 2016, SIMULATION CREDIBILI
   Montecinos G, 2012, J COMPUT PHYS, V231, P6472, DOI 10.1016/j.jcp.2012.06.011
   Neri P, 2002, NAT NEUROSCI, V5, P812, DOI 10.1038/nn886
   Oberkampf W. L., 2004, Applied Mechanics Review, V57, P345, DOI 10.1115/1.1767847
   Peshkov I, 2019, J COMPUT PHYS, V387, P481, DOI 10.1016/j.jcp.2019.02.039
   Press W. H., 2007, NUM REC ART SCI COMP
   RHIE CM, 1983, AIAA J, V21, P1525, DOI 10.2514/3.8284
   Serra T, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-40777-2
   Smyth WD, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-39869-w
   Stieger T, 2017, NAT COMMUN, V8, P1, DOI 10.1038/ncomms15550
   Taylor GI, 1937, PROC R SOC LON SER-A, V158, P0499, DOI 10.1098/rspa.1937.0036
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang Z, 2009, IEEE SIGNAL PROC MAG, V26, P98, DOI 10.1109/MSP.2008.930649
   Zhang W, 2017, SCI REP-UK, V7, DOI 10.1038/s41598-017-13484-z
   Zhao GY, 2019, J COMPUT PHYS, V376, P924, DOI 10.1016/j.jcp.2018.10.013
   Zhao S, 2014, COMPUT FLUIDS, V95, P74, DOI 10.1016/j.compfluid.2014.02.017
   Zhou GZ, 2018, PHYS FLUIDS, V30, DOI 10.1063/1.4998300
   Zhu YN, 2005, ACM T GRAPHIC, V24, P965, DOI 10.1145/1073204.1073298
NR 50
TC 0
Z9 0
U1 1
U2 12
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD JUN
PY 2021
VL 18
IS 2
AR 6
DI 10.1145/3449064
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SR6EX
UT WOS:000661137000002
DA 2024-07-18
ER

PT J
AU Zibrek, K
   Martin, S
   Mcdonnell, R
AF Zibrek, Katja
   Martin, Sean
   Mcdonnell, Rachel
TI Is Photorealism Important for Perception of Expressive Virtual Humans in
   Virtual Reality?
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article; Proceedings Paper
CT 16th Symposium on Applied Perception (SAP)
CY SEP, 2019
CL Barcelona, SPAIN
DE Virtual reality; proximity; render style; virtual humans
ID EMOTION RECOGNITION; COPRESENCE; APPEARANCE; CHARACTERS; BEHAVIOR; FACE
AB In recent years, the quality of real-time rendering has reached new heights-realistic reflections, physically based materials, and photometric lighting are all becoming commonplace in modern game engines and even interactive virtual environments, such as virtual reality (VR). As the strive for realism continues, there is a need to investigate the effect of photorealism on users' perception, particularly for interactive, emotional scenarios in VR. In this article, we explored three main topics, where we predicted photorealism will make a difference: the illusion of being present with the virtual person and in an environment, altered emotional response toward the character, and a subtler response-comfort of being in close proximity to the character. We present a perceptual experiment, with an interactive expressive virtual character in VR, which was designed to induce particular social responses in people. Our participant pool was large (N = 797) and diverse in terms of demographics. We designed a between-group experiment, where each group saw either the realistic rendering or one of our stylized conditions (simple and sketch style), expressing one of three attitudes: Friendly, Unfriendly, or Sad. While the render style did not particularly effect the level of comfort with the character or increase the illusion of presence with it, our main finding shows that the photorealistic character changed the emotional responses of participants, compared to the stylized versions. We also found a preference for realism in VR, reflected in the affinity and higher place illusion in the scenario, rendered in the realistic render style.
C1 [Zibrek, Katja; Martin, Sean; Mcdonnell, Rachel] Trinity Coll Dublin, Coll Green, Sch Comp Sci & Stat, Dublin 2, Ireland.
C3 Trinity College Dublin
RP Zibrek, K (corresponding author), Trinity Coll Dublin, Coll Green, Sch Comp Sci & Stat, Dublin 2, Ireland.
EM kzibrek@tcd.ie; martins7@tcd.ie; ramcdonn@scss.tcd.ie
RI Zibrek, Katja/JQW-2981-2023; McDonnell, Rachel/HGC-4337-2022
OI Zibrek, Katja/0000-0002-0204-3472; McDonnell,
   Rachel/0000-0002-1957-2506; Martin, Sean/0000-0001-7600-0291
FU Science Foundation Ireland [13/CDA/2135]; ADAPT Centre for Digital
   Content Technology [13/RC/2106]; Science Foundation Ireland (SFI)
   [13/CDA/2135] Funding Source: Science Foundation Ireland (SFI)
FX This research was funded by the Science Foundation Ireland as part of
   the "Game Face" project (Grant No. 13/CDA/2135) and ADAPT Centre for
   Digital Content Technology (Grant No. 13/RC/2106).
CR [Anonymous], 2013, P 10 IEEE INT C WORK
   [Anonymous], 2003, THESIS
   Bailenson JN, 2006, PRESENCE-VIRTUAL AUG, V15, P359, DOI 10.1162/pres.15.4.359
   Bailenson JN, 2001, PRESENCE-VIRTUAL AUG, V10, P583, DOI 10.1162/105474601753272844
   Bailenson JN, 2005, PRESENCE-VIRTUAL AUG, V14, P379, DOI 10.1162/105474605774785235
   Bailenson JN, 2003, PERS SOC PSYCHOL B, V29, P819, DOI 10.1177/0146167203029007002
   Blascovich J, 2002, COMP SUPP COMP W SER, P127
   Bönsch A, 2018, 25TH 2018 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P199
   Carter E.J., 2013, Proceedings of the ACM Symposium on Applied Perception, P35, DOI DOI 10.1145/2492494.2502059
   Chaminade T, 2007, SOC COGN AFFECT NEUR, V2, P206, DOI 10.1093/scan/nsm017
   DAVIS MH, 1983, J PERS SOC PSYCHOL, V44, P113, DOI 10.1037/0022-3514.44.1.113
   Ferwerda JA, 2003, PROC SPIE, V5007, P290, DOI 10.1117/12.473899
   Fink PW, 2007, ACM T APPL PERCEPT, V4, DOI 10.1145/1227134.1227136
   Garau M., 2001, CHI 2001 Conference Proceedings. Conference on Human Factors in Computing Systems, P309, DOI 10.1145/365024.365121
   Garau Maia, 2003, P SIGCHI C HUM FACT, P529, DOI DOI 10.1145/642611.642703
   Golan O, 2006, J AUTISM DEV DISORD, V36, P169, DOI 10.1007/s10803-005-0057-y
   Hu XF, 2016, INT CONF INSTR MEAS, P69, DOI 10.1109/IMCCC.2016.109
   Jung S, 2017, SUI'17: PROCEEDINGS OF THE 2017 SYMPOSIUM ON SPATIAL USER INTERACTION, P3, DOI 10.1145/3131277.3132186
   Klein KJK, 2001, PERS SOC PSYCHOL B, V27, P720, DOI 10.1177/0146167201276007
   Latoschik ME, 2017, VRST'17: PROCEEDINGS OF THE 23RD ACM SYMPOSIUM ON VIRTUAL REALITY SOFTWARE AND TECHNOLOGY, DOI 10.1145/3139131.3139156
   LATTA RM, 1978, PERS SOC PSYCHOL B, V4, P143, DOI 10.1177/014616727800400131
   Lee M, 2018, IEEE T VIS COMPUT GR, V24, P1525, DOI 10.1109/TVCG.2018.2794074
   MacDorman KF, 2009, COMPUT HUM BEHAV, V25, P695, DOI 10.1016/j.chb.2008.12.026
   McDonnell R, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185587
   Mitchell WJ, 2011, I-PERCEPTION, V2, P10, DOI 10.1068/i0415
   Mori M., 1970, Energy, V7, P33, DOI [DOI 10.1109/MRA.2012.2192811, 10.1109/MRA.2012.2192811]
   Nowak Kristin, 2001, P ANN CONV INT COMM
   Ring L, 2014, LECT NOTES ARTIF INT, V8637, P374, DOI 10.1007/978-3-319-09767-1_49
   Schultze U, 2010, J INF TECHNOL-UK, V25, P434, DOI 10.1057/jit.2010.25
   Skarbez R, 2017, IEEE T VIS COMPUT GR, V23, P1322, DOI 10.1109/TVCG.2017.2657158
   Slater M, 2002, COMP SUPP COMP W SER, P146
   Slater M, 2009, PHILOS T R SOC B, V364, P3549, DOI 10.1098/rstb.2009.0138
   UE4, 2018, UNR ENG 4 REAL REND
   UE4, 2018, PAR PHAS UNR ENG 4 P
   Volonte M, 2016, IEEE T VIS COMPUT GR, V22, P1326, DOI 10.1109/TVCG.2016.2518158
   WATSON D, 1988, J PERS SOC PSYCHOL, V54, P1063, DOI 10.1037/0022-3514.54.6.1063
   Yamada Y, 2013, JPN PSYCHOL RES, V55, P20, DOI 10.1111/j.1468-5884.2012.00538.x
   Yee N, 2007, HUM COMMUN RES, V33, P271, DOI 10.1111/j.1468-2958.2007.00299.x
   Zell E, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818126
   Zibrek K, 2017, ACM SYMPOSIUM ON APPLIED PERCEPTION (SAP 2017), DOI 10.1145/3119881.3119887
   Zibrek K, 2018, IEEE T VIS COMPUT GR, V24, P1681, DOI 10.1109/TVCG.2018.2794638
NR 41
TC 49
Z9 54
U1 3
U2 26
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD SEP
PY 2019
VL 16
IS 3
SI SI
AR 14
DI 10.1145/3349609
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA JA3KH
UT WOS:000487720000002
DA 2024-07-18
ER

PT J
AU Agethen, P
   Sekar, VS
   Gaisbauer, F
   Pfeiffer, T
   Otto, M
   Rukzio, E
AF Agethen, Philipp
   Sekar, Viswa Subramanian
   Gaisbauer, Felix
   Pfeiffer, Thies
   Otto, Michael
   Rukzio, Enrico
TI Behavior Analysis of Human Locomotion in the Real World and Virtual
   Reality for the Manufacturing Industry
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Virtual reality; human locomotion behavior; gait analysis; comparison;
   user studies
AB With the rise of immersive visualization techniques, many domains within the manufacturing industry are increasingly validating production processes in virtual reality (VR). The validity of the results gathered in such simulations, however, is widely unknown-in particular, with regard to human locomotion behavior. To bridge this gap, this article presents an experiment analyzing the behavioral disparity between human locomotion being performed without any equipment and in immersive VR while wearing a head-mounted display (I IMD). The presented study (n = 30) is split up in three sections and covers linear walking, non-linear walking, and obstacle avoidance. Special care has been given to design the experiment so that findings are generally valid and can be applied to a wide range of domains beyond the manufacturing industry. The findings provide novel insights into the effect of immersive VR on specific gait parameters. In total, a comprehensive sample of 18.09km is analyzed. The results reveal that the HMD had a medium effect (up to 13%) on walking velocity, on non-linear walking toward an oriented target, and on clearance distance. The overall differences are modeled using multiple regression models, thus allowing the general usage within various domains. Summarizing, it can be concluded that VR can be used to analyze and plan human locomotion; however, specific details may have to be adjusted to transfer findings to the real world.
C1 [Agethen, Philipp; Sekar, Viswa Subramanian; Gaisbauer, Felix; Otto, Michael] Daimler AG, Wilhelm Runge Str 11, D-89081 Ulm, Germany.
   [Agethen, Philipp; Gaisbauer, Felix; Otto, Michael] Ulm Univ, Bielefeld, Germany.
   [Pfeiffer, Thies] Bielefeld Univ, CITEC, Inspirat 1, D-33619 Bielefeld, Germany.
   [Rukzio, Enrico] Ulm Univ, Inst Media Res & Media Dev, D-89081 Bielefeld, Germany.
C3 Daimler AG; Ulm University; University of Bielefeld; Ulm University
RP Agethen, P (corresponding author), Daimler AG, Wilhelm Runge Str 11, D-89081 Ulm, Germany.; Agethen, P (corresponding author), Ulm Univ, Bielefeld, Germany.
EM philipp.agethen@daimler.com; viswa.subramanian_sekar@daimler.com;
   felix.gaisbauer@daimler.com; ttpfeiffe@techfak.uni-bielefeld.de;
   michael.m.otto@daimler.com; enrico.rukzio@uni-ulm.de
RI Pfeiffer, Thies/H-6036-2019
OI Pfeiffer, Thies/0000-0001-6619-749X; Agethen,
   Philipp/0000-0001-7968-2558
CR Auvinet B, 2002, GAIT POSTURE, V16, P124, DOI 10.1016/S0966-6362(01)00203-X
   Bailey Joshua, 2017, Int J Exerc Sci, V10, P1067
   Bradski G., 2000, Opencv. Dr. Dobb's journal of software tools
   Brooks F. P.  Jr., 1987, Proceedings of the 1986 Workshop on Interactive 3D Graphics, P9, DOI 10.1145/319120.319122
   Christensen RR, 2000, PRESENCE-VIRTUAL AUG, V9, P1, DOI 10.1162/105474600566574
   Cirio G, 2013, IEEE T VIS COMPUT GR, V19, P671, DOI 10.1109/TVCG.2013.34
   Darken R. P., 1997, Proceedings of the ACM Symposium on User Interface Software and Technology. 10th Annual Symposium. UIST '97, P213, DOI 10.1145/263407.263550
   Faul F, 2007, BEHAV RES METHODS, V39, P175, DOI 10.3758/BF03193146
   Feasel J, 2008, 3DUI: IEEE SYMPOSIUM ON 3D USER INTERFACES 2008, PROCEEDINGS, P97
   Fink PW, 2007, ACM T APPL PERCEPT, V4, DOI 10.1145/1227134.1227136
   Gérin-Lajoie M, 2008, GAIT POSTURE, V27, P239, DOI 10.1016/j.gaitpost.2007.03.015
   Gibson J. J., 2014, The ecological approach to visual perception, Vclassic
   Giorgino T, 2009, J STAT SOFTW, V31, P1, DOI 10.18637/jss.v031.i07
   Hicheur H, 2007, EUR J NEUROSCI, V26, P2376, DOI 10.1111/j.1460-9568.2007.05836.x
   HTC, 2017, DISC VIRT REAL IM
   Iwata K, 1997, CIRP ANNALS 1997 MANUFACTURING TECHNOLOGY, VOLUME 46/1/1997, V46, P335
   Janeh O, 2017, ACM T APPL PERCEPT, V14, DOI 10.1145/3022731
   Jerald J, 2009, IEEE VIRTUAL REALITY 2009, PROCEEDINGS, P211, DOI 10.1109/VR.2009.4811025
   Jones E., 2001, SciPy: Open source scientific tools for Python
   Jones JA, 2017, ACM T APPL PERCEPT, V14, DOI 10.1145/2983631
   Kelly JW, 2017, ACM T APPL PERCEPT, V15, DOI 10.1145/3106155
   Mohler Betty., 2007, 13th Eurographics Symposium on Virtual Environments and 10th Immersive Projection Technology Workshop (IPT-EGVE 2007), P85, DOI DOI 10.2312/PE/VE2007SHORT/085-088
   Mohler BettyJ., 2006, P 3 S APPL PERCEPTIO, P9, DOI [10.1145/1140491.1140493, DOI 10.1145/1140491.1140493]
   Mujber TS, 2004, J MATER PROCESS TECH, V155, P1834, DOI 10.1016/j.jmatprotec.2004.04.401
   Nabioyuni M., 2015, The Eurographics Association, P167, DOI DOI 10.2312/EGVE.20151325
   Ohshima T., 2016, SIGGRAPH ASIA 2016 Posters, P18
   Otto M, 2016, PROC CIRP, V44, P38, DOI 10.1016/j.procir.2016.02.140
   Razzaque S., 2005, THESIS
   Ruddle RA, 2013, ACM T APPL PERCEPT, V10, DOI 10.1145/2465780.2465785
   Slater M., 1995, Virtual Environments '95. Selected Papers of the Eurographics Workshops, P135
   Slater Mel, 1995, ACM Transactions on Computer-Human Interaction, V2, P201, DOI DOI 10.1145/210079.210084
   Smith D., 2003, MONITOR, V34, P56, DOI DOI 10.1037/E300062003-028
   Souman JL, 2011, ACM T APPL PERCEPT, V8, DOI 10.1145/2043603.2043607
   Steinicke F., 2013, HUMAN WALKING VIRTUA, V2, DOI [10.1007/978-1-4419-8432-6, DOI 10.1007/978-1-4419-8432-6]
   SUMA E., 2010, IEEE T VISUALIZATION, V16, P4
   Usoh M, 1999, COMP GRAPH, P359, DOI 10.1145/311535.311589
   Whitton MC, 2005, P IEEE VIRT REAL ANN, P123
NR 37
TC 23
Z9 24
U1 1
U2 16
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD AUG
PY 2018
VL 15
IS 3
AR 20
DI 10.1145/3230648
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GY2UV
UT WOS:000448400300006
OA Green Published
DA 2024-07-18
ER

PT J
AU Shamir, L
   Nissel, J
   Winner, E
AF Shamir, Lior
   Nissel, Jenny
   Winner, Ellen
TI Distinguishing between Abstract Art by Artists vs. Children and Animals:
   Comparison between Human and Machine Perception
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Art; abstract expressionism
ID VAN GOGH; CLASSIFICATION; FEATURES; PAINTINGS; DATASETS; TOOLS
AB expressionism is a school of art characterized by nonrepresentational paintings where color, composition, and brush strokes are used to express emotion. These works are often misunderstood by the public who see them as requiring no skill and as images that even a child could have created. However, a recent series of studies has shown that ordinary adults untrained in art or art history, as well as young children, can differentiate paintings by abstract expressionists and superficially similar works by preschool children and even animals (monkeys, apes, elephants). Adults perform this distinction with an accuracy rate of similar to 64%, significantly higher than chance. Here we ask whether machine perception can do as well. Using the same paintings, we show that in similar to 68% of the cases the computer algorithm can discriminate between abstract paintings and the work of children and animals. We also applied a method that computes the correlation between the degree of artisticity deduced from human perception of the paintings and the visual content of the images, and we show significant correlation between perceived artisticity and visual content. The image content descriptor that was the strongest predictor of correct identification was the fractality of the painting. We also show that the computer algorithm predicts the perceived intentionality of the paintings by humans. These results confirm perceptible differences between works by abstract expressionists and superficially similar ones by the untrained and show that people see more than they think they see when looking at abstract expressionism.
C1 [Shamir, Lior] Lawrence Technol Univ, Dept Comp Sci, 21000 W Ten Mile Rd, South Field, MI 48075 USA.
   [Nissel, Jenny] Boston Coll, Dept Psychol, McGuinn 300140 Commonwealth Ave, Chestnut Hill, MA 02467 USA.
   [Winner, Ellen] Boston Coll, Dept Psychol, McGuinn 343,140 Commonwealth Ave, Chestnut Hill, MA 02467 USA.
C3 Boston College; Boston College
RP Shamir, L (corresponding author), Lawrence Technol Univ, Dept Comp Sci, 21000 W Ten Mile Rd, South Field, MI 48075 USA.
EM lshamir@mtu.edu; Jenny.Nissel@gmail.com; ellen.winner@bc.edu
RI Li, Mengqi/AAG-6804-2021
OI Nissel, Jenny/0000-0001-7854-2545
CR [Anonymous], 2012, J. Comput. Cult. Herit., DOI DOI 10.1145/2307723.2307726
   Cetinic Eva, 2013, Proceedings of the 2013 55th International Symposium. ELMAR-2013, P19
   Gabor D., 1946, Journal of the Institution of Electrical Engineers-part III: radio and communication engineering, V93, P429, DOI [DOI 10.1049/JI-3-2.1946.0074, 10.1049/ji-3-2.1946.0074]
   Gradshtein I., 1994, TABLE INTEGRALS SERI, V5
   Hawley-Dolan A, 2011, PSYCHOL SCI, V22, P435, DOI 10.1177/0956797611400915
   Jones-Smith K, 2006, NATURE, V444, pE9, DOI 10.1038/nature05398
   Kammerer P, 2007, PATTERN RECOGN LETT, V28, P710, DOI 10.1016/j.patrec.2006.08.003
   KEREN D, 2002, P 16 INT C PATT REC
   Khan FS, 2014, MACH VISION APPL, V25, P1385, DOI 10.1007/s00138-014-0621-6
   Kim M., 2013, P INT C HUM COMP INT, P258
   Kroner S, 1998, INT C PATT RECOG, P462, DOI 10.1109/ICPR.1998.711180
   Li J, 2004, IEEE T IMAGE PROCESS, V13, P338, DOI 10.1109/TIP.2003.821349
   Li J, 2012, IEEE T PATTERN ANAL, V34, P1159, DOI 10.1109/TPAMI.2011.203
   Lim J.S., 1990, Two-dimensional Signal and Image Processing
   Lyu S, 2004, P NATL ACAD SCI USA, V101, P17006, DOI 10.1073/pnas.0406398101
   Mandelbrot B. B., 1982, FRACTAL GEOMETRY NAT
   MATTISON D, 2004, SEARCHER, V12, P8, DOI DOI 10.1073/pnas.0406398101
   Nissel J., 2015, J COGN DEV IN PRESS
   Orlov N, 2008, PATTERN RECOGN LETT, V29, P1684, DOI 10.1016/j.patrec.2008.04.013
   Ramachandran V. S., 1999, Journal of Consciousness Studies, V6, P15
   Roussopoulos P, 2010, ACM J COMPUT CULT HE, V3, DOI 10.1145/1841317.1841318
   Shamir Lior, 2009, Proceedings of the 2009 International Conference on Image Processing, Computer Vision, & Pattern Recognition. IPCV 2009, P37
   Shamir L, 2009, OSTEOARTHR CARTILAGE, V17, P1307, DOI 10.1016/j.joca.2009.04.010
   Shamir L, 2008, INT J COMPUT VISION, V79, P225, DOI 10.1007/s11263-008-0143-7
   Shamir L, 2015, INT J ARTS TECHNOL, V8, P1, DOI 10.1504/IJART.2015.067389
   Shamir L, 2013, ASTRON COMPUT, V2, P67, DOI 10.1016/j.ascom.2013.09.002
   Shamir L, 2014, J ACOUST SOC AM, V135, P953, DOI 10.1121/1.4861348
   Shamir L, 2012, LEONARDO, V45, P149, DOI 10.1162/LEON_a_00281
   Shamir L, 2011, INT J COMPUT ASS RAD, V6, P699, DOI 10.1007/s11548-011-0550-z
   Shamir L, 2010, ACM T APPL PERCEPT, V7, DOI 10.1145/1670671.1670672
   Shamir L, 2009, IEEE T BIO-MED ENG, V56, P407, DOI 10.1109/TBME.2008.2006025
   Shamir L, 2008, SOURCE CODE BIOL MED, V3, DOI 10.1186/1751-0473-3-13
   SHANMUGAM K, 1973, IEEE T SYST MAN CYB, VSMC3, P202, DOI 10.1109/TSMC.1973.5408507
   Snapper L, 2015, COGNITION, V137, P154, DOI 10.1016/j.cognition.2014.12.009
   TAMURA H, 1978, IEEE T SYST MAN CYB, V8, P460, DOI 10.1109/TSMC.1978.4309999
   Taylor RP, 2007, PATTERN RECOGN LETT, V28, P695, DOI 10.1016/j.patrec.2006.08.012
   Taylor RP, 2006, NATURE, V444, pE10, DOI 10.1038/nature05399
   Taylor RP, 1999, NATURE, V399, P422, DOI 10.1038/20833
   Taylor RP, 2002, LEONARDO, V35, P203, DOI 10.1162/00240940252940603
   TEAGUE MR, 1980, J OPT SOC AM, V70, P920, DOI 10.1364/JOSA.70.000920
   Tsai CF, 2007, ONLINE INFORM REV, V31, P185, DOI 10.1108/14684520710747220
   Varnedoe Kirk., 2006, PICTURES NOTHING ABS
   WU CM, 1992, IEEE T MED IMAGING, V11, P141, DOI 10.1109/42.141636
   Zeki S., 1999, INNER VISION EXPLORA, P415
NR 44
TC 3
Z9 4
U1 1
U2 18
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD MAY
PY 2016
VL 13
IS 3
AR 17
DI 10.1145/2912125
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Arts &amp; Humanities Citation Index (A&amp;HCI)
SC Computer Science
GA DV4DU
UT WOS:000382876200007
DA 2024-07-18
ER

PT J
AU Nymoen, K
   Godoy, RI
   Jensenius, AR
   Torresen, J
AF Nymoen, Kristian
   Godoy, Rolf Inge
   Jensenius, Alexander Refsum
   Torresen, Jim
TI Analyzing Correspondence between Sound Objects and Body Motion
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Experimentation; Human Factors; Sound-tracing; music-related motion
ID MOTOR THEORY; PERCEPTION; MUSIC; TOOLBOX; GESTURE; IMAGES
AB Links between music and body motion can be studied through experiments called sound-tracing. One of the main challenges in such research is to develop robust analysis techniques that are able to deal with the multidimensional data that musical sound and body motion present. The article evaluates four different analysis methods applied to an experiment in which participants moved their hands following perceptual features of short sound objects. Motion capture data has been analyzed and correlated with a set of quantitative sound features using four different methods: (a) a pattern recognition classifier, (b) t-tests, (c) Spearman's. correlation, and (d) canonical correlation. This article shows how the analysis methods complement each other, and that applying several analysis techniques to the same data set can broaden the knowledge gained from the experiment.
C1 [Nymoen, Kristian; Torresen, Jim] Univ Oslo, Dept Informat, N-0316 Oslo, Norway.
   [Godoy, Rolf Inge; Jensenius, Alexander Refsum] Univ Oslo, Dept Musicol, N-0316 Oslo, Norway.
C3 University of Oslo; University of Oslo
RP Nymoen, K (corresponding author), Univ Oslo, Dept Informat, Postboks 1080 Blindern, N-0316 Oslo, Norway.
EM krisny@ifi.uio.no
RI Nymoen, Kristian/H-7551-2017; Jensenius, Alexander Refsum/F-3061-2012
OI Nymoen, Kristian/0000-0002-0050-0473; Jensenius, Alexander
   Refsum/0000-0001-6171-8743
FU European Union Seventh Framework Programme [257906]; Engineering
   Proprioception in Computer Systems (EPiCS); Norwegian Research Council
   [183180]
FX The research leading to these results has received funding from the
   European Union Seventh Framework Programme under grant agreement no.
   257906, Engineering Proprioception in Computer Systems (EPiCS), and the
   Norwegian Research Council, project no. 183180 Sensing Music-Related
   Actions (SMA).
CR [Anonymous], 1999, Auditory Scene Analysis: The Perceptual Organization of Sound, DOI DOI 10.7551/MITPRESS/1486.001.0001
   [Anonymous], 2006, ACM SIGKDD INT C KNO, DOI DOI 10.1145/1150402.1150531
   [Anonymous], 2010, MUSICAL GESTURES SOU
   [Anonymous], THESIS TU EINDHOVEN
   [Anonymous], 2002, MUSICAL PERFORMANCE
   [Anonymous], 2008, Embodied Music Cognition and Mediation Technology
   [Anonymous], P 10 INT MUS INF RET
   Burger B., 2012, P 12 INT C MUS PERC, P183
   Camurri A, 2003, INT J HUM-COMPUT ST, V59, P213, DOI 10.1016/S1071-5819(03)00050-8
   Caramiaux B, 2010, LECT NOTES ARTIF INT, V5934, P158, DOI 10.1007/978-3-642-12553-9_14
   DIPELLEGRINO G, 1992, EXP BRAIN RES, V91, P176, DOI 10.1007/BF00230027
   Duda R., 1973, Pattern Classification and Scene Analysis
   Eitan Z, 2006, MUSIC PERCEPT, V23, P221, DOI 10.1525/mp.2006.23.3.221
   Galantucci B, 2006, PSYCHON B REV, V13, P361, DOI 10.3758/BF03193857
   GODOY R. I., 2006, P 2 CONGAS INT S GES
   Godoy R.I., 2006, Organised Sound, V11, P149, DOI DOI 10.1017/S1355771806001439
   Godoy RI, 2010, ORGAN SOUND, V15, P54, DOI 10.1017/S1355771809990264
   GREY JM, 1977, J ACOUST SOC AM, V61, P1270, DOI 10.1121/1.381428
   Haga E., 2008, THESIS U OSLO
   HOLM S, 1979, SCAND J STAT, V6, P65
   Hotelling H, 1936, BIOMETRIKA, V28, P321, DOI 10.1093/biomet/28.3-4.321
   Huron D, 2009, EMPIR MUSICOL REV, V4, P93, DOI 10.18061/1811/44530
   Kohler E, 2002, SCIENCE, V297, P846, DOI 10.1126/science.1070311
   KOZAK M., 2011, P 9 INT GEST WORKSH, P20
   KUSSNER M., 2012, P 4 INT C STUD SYST
   Lartillot O, 2008, ST CLASS DAT ANAL, P261, DOI 10.1007/978-3-540-78246-9_31
   Leman M., 2004, Gesture-Based Communication in Human-Computer Interaction. 5th International Gesture Workshop, GW 2003. Selected Revised Papers (Lecture Notes in Comput. Sci. Vol.2915), P40
   LIBERMAN AM, 1985, COGNITION, V21, P1, DOI 10.1016/0010-0277(85)90021-6
   MCADAMS S, 1995, PSYCHOL RES-PSYCH FO, V58, P177, DOI 10.1007/BF00419633
   Merer A, 2008, LECT NOTES COMPUT SC, V4969, P139
   Nymoen Kristian, 2012, Speech, Sound and Music Processing: Embracing Research in India. 8th International Symposium, CMMR 2011 20th International Symposium, FRSM 2011. Revised Selected Papers, P120, DOI 10.1007/978-3-642-31980-8_11
   Nymoen K., 2011, Conference on New Interfaces for Musical Expression, P312
   Nymoen K., 2010, Proceedings of the International Conference on New Interfaces for Musical Expression, P259
   Nymoen Kristian, 2011, MM 11 P 2011 ACM MUL, P39, DOI [10.1145/2072529.2072541, DOI 10.1145/2072529.2072541]
   Peeters G, 2011, J ACOUST SOC AM, V130, P2902, DOI 10.1121/1.3642604
   Pirhonen A., 2007, P 13 INT C AUD DISPL, P319
   REPP BH, 1995, MUSIC PERCEPT, V13, P39
   ROSENBAUM D., 2001, HUMAN MOTOR CONTROL
   Schaeffer P., 1966, Traite des objets musicaux
   Schubert Emery., 2002, Musicae Scientiae, P213
   Stein Barry E., 1993, The Merging of the Senses. The Merging of the Senses. Cognitive Neuroscience
   Styns F, 2007, HUM MOVEMENT SCI, V26, P769, DOI 10.1016/j.humov.2007.07.007
   Van Nort D, 2009, ORGAN SOUND, V14, P177, DOI 10.1017/S1355771809000284
   Vassilakis P. N., 2001, THESIS U CALIF
   Vines BW, 2006, COGNITION, V101, P80, DOI 10.1016/j.cognition.2005.09.003
   Vroomen J, 2000, J EXP PSYCHOL HUMAN, V26, P1583, DOI 10.1037/0096-1523.26.5.1583
   WALKER B. N., 2000, THESIS RICE U HOUSTO
   WALKER R, 1987, PERCEPT PSYCHOPHYS, V42, P491, DOI 10.3758/BF03209757
   Wessel D. L., 1979, Computer Music Journal, V3, P45, DOI 10.2307/3680283
   Zatorre R, 2005, NATURE, V434, P312, DOI 10.1038/434312a
NR 50
TC 29
Z9 33
U1 0
U2 6
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD MAY
PY 2013
VL 10
IS 2
AR 9
DI 10.1145/2465780.2465783
PG 22
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA 214EV
UT WOS:000324114800003
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Ruddle, RA
   Volkova, E
   Bülthoff, HH
AF Ruddle, Roy A.
   Volkova, Ekaterina
   Buelthoff, Heinrich H.
TI Learning to Walk in Virtual Reality
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Experimentation; Human Factors; Performance; Virtual reality interfaces;
   navigation; travel; metrics
ID ENVIRONMENTS; MOVEMENT; TRAVEL
AB This article provides longitudinal data for when participants learned to travel with a walking metaphor through virtual reality (VR) worlds, using interfaces that ranged from joystick-only, to linear and omnidirectional treadmills, and actual walking in VR. Three metrics were used: travel time, collisions (a measure of accuracy), and the speed profile. The time that participants required to reach asymptotic performance for traveling, and what that asymptote was, varied considerably between interfaces. In particular, when a world had tight turns (0.75 m corridors), participants who walked were more proficient than those who used a joystick to locomote and turned either physically or with a joystick, even after 10 minutes of training. The speed profile showed that this was caused by participants spending a notable percentage of the time stationary, irrespective of whether or not they frequently played computer games. The study shows how speed profiles can be used to help evaluate participants' proficiency with travel interfaces, highlights the need for training to be structured to addresses specific weaknesses in proficiency (e. g., start-stop movement), and for studies to measure and report that proficiency.
C1 [Ruddle, Roy A.] Univ Leeds, Sch Comp, Leeds LS2 9JT, W Yorkshire, England.
   [Ruddle, Roy A.; Volkova, Ekaterina; Buelthoff, Heinrich H.] Max Planck Inst Biol Cybernet, D-72076 Tubingen, Germany.
   [Buelthoff, Heinrich H.] Korea Univ, Dept Brain & Cognit Engn, Seoul, South Korea.
C3 University of Leeds; Max Planck Society; Korea University
RP Ruddle, RA (corresponding author), Univ Leeds, Sch Comp, Leeds LS2 9JT, W Yorkshire, England.
EM royr@comp.leeds.ac.uk
RI Bülthoff, Heinrich/AAC-8818-2019; Bülthoff, Heinrich H/J-6579-2012
OI Bülthoff, Heinrich H/0000-0003-2568-0607
FU Alexander von Humboldt Fellowship for Experienced Researchers; Max
   Planck Society; WCU (World Class University) programme through the
   National Research Foundation of Korea; Ministry of Education, Science
   and Technology [R31-10008]
FX This research was supported by an Alexander von Humboldt Fellowship for
   Experienced Researchers awarded to R. A. Ruddle, the Max Planck Society,
   and the WCU (World Class University) programme through the National
   Research Foundation of Korea funded by the Ministry of Education,
   Science and Technology (R31-10008).
CR Blascovich J., 2011, Infinite reality
   Bowman D. A., 1997, Proceedings 1997 Symposium on Interactive 3D Graphics, P35, DOI 10.1145/253284.253301
   Bowman DA, 1997, P IEEE VIRT REAL ANN, P45, DOI 10.1109/VRAIS.1997.583043
   Bowman DA, 2001, PRESENCE-TELEOP VIRT, V10, P75, DOI 10.1162/105474601750182333
   Bowman Doug, 2004, 3D user interfaces: Theory and practice
   Chen M., 1988, Computer Graphics, V22, P121, DOI 10.1145/378456.378497
   De Luca A, 2009, 2009 IEEE-RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, P5051, DOI 10.1109/IROS.2009.5354610
   Feasel J, 2008, 3DUI: IEEE SYMPOSIUM ON 3D USER INTERFACES 2008, PROCEEDINGS, P97
   Fink PW, 2007, ACM T APPL PERCEPT, V4, DOI 10.1145/1227134.1227136
   Hollerbach JM, 2002, HUM FAC ER, P239
   Jacobson J, 1997, PROCEEDINGS OF THE HUMAN FACTORS AND ERGONOMICS SOCIETY 41ST ANNUAL MEETING, 1997, VOLS 1 AND 2, P1273
   Lampton DR., 1994, Presence: Teleoperators and Virtual Environments, V3, P145
   Lessells S, 2005, PRESENCE-TELEOP VIRT, V14, P580, DOI 10.1162/105474605774918778
   Loomis JM, 1999, WAYFINDING BEHAVIOR, P125
   Peck TC, 2009, IEEE T VIS COMPUT GR, V15, P383, DOI 10.1109/TVCG.2008.191
   Pelah A, 2007, ACM T APPL PERCEPT, V4, DOI 10.1145/1227134.1227135
   RUDDLE R. A., 2011, EFFECT TRANSLA UNPUB
   Ruddle RA, 2001, PRESENCE-TELEOP VIRT, V10, P511, DOI 10.1162/105474601753132687
   Ruddle RA, 2006, PRESENCE-TELEOP VIRT, V15, P637, DOI 10.1162/pres.15.6.637
   Ruddle RA, 2011, ACM T COMPUT-HUM INT, V18, DOI 10.1145/1970378.1970384
   Ruddle RA, 2011, MEM COGNITION, V39, P686, DOI 10.3758/s13421-010-0054-z
   Ruddle RA, 2009, ACM T COMPUT-HUM INT, V16, DOI 10.1145/1502800.1502805
   Slater Mel, 1995, ACM Transactions on Computer-Human Interaction, V2, P201, DOI DOI 10.1145/210079.210084
   Smith SR, 2009, 3DUI : IEEE SYMPOSIUM ON 3D USER INTERFACES 2009, PROCEEDINGS, P3, DOI 10.1109/3DUI.2009.4811198
   Souman JL, 2010, ACM T APPL PERCEPT, V7, DOI 10.1145/1670671.1670675
   Stone RJ, 2002, HUM FAC ER, P827
   Suma EA, 2010, IEEE T VIS COMPUT GR, V16, P690, DOI 10.1109/TVCG.2009.93
   Usoh M, 1999, COMP GRAPH, P359, DOI 10.1145/311535.311589
   van Schaik P, 2005, INT J HUM-COMPUT INT, V18, P309, DOI 10.1207/s15327590ijhc1803_4
   Waller D, 2000, J EXP PSYCHOL-APPL, V6, P307, DOI 10.1037//1076-898X.6.4.307
   Whitton MC, 2005, P IEEE VIRT REAL ANN, P123
   Winter D., 1990, BIOMECHANICS HUMAN M
   Zanbaka CA, 2005, IEEE T VIS COMPUT GR, V11, P694, DOI 10.1109/TVCG.2005.92
   Zhai SM, 1999, J VISUAL LANG COMPUT, V10, P3, DOI 10.1006/jvlc.1998.0113
NR 34
TC 41
Z9 47
U1 0
U2 25
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD MAY
PY 2013
VL 10
IS 2
AR 11
DI 10.1145/2465780.2465785
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA 214EV
UT WOS:000324114800005
OA Green Published
DA 2024-07-18
ER

PT J
AU Navarro, F
   Castillo, S
   Serón, FJ
   Gutierrez, D
AF Navarro, Fernando
   Castillo, Susana
   Seron, Francisco J.
   Gutierrez, Diego
TI Perceptual Considerations for Motion Blur Rendering
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Algorithms; Performance; Experimentation; Human Factors; Motion blur;
   temporal antialiasing; perceptual rendering
ID TEMPORAL CONTRAST-SENSITIVITY; MODEL; ATTENTION
AB Motion blur is a frequent requirement for the rendering of high-quality animated images. However, the computational resources involved are usually higher than those for images that have not been temporally antialiased. In this article we study the influence of high-level properties such as object material and speed, shutter time, and antialiasing level. Based on scenes containing variations of these parameters, we design different psychophysical experiments to determine how influential they are in the perception of image quality.
   This work gives insights on the effects these parameters have and exposes certain situations where motion blurred stimuli may be indistinguishable from a gold standard. As an immediate practical application, images of similar quality can be produced while the computing requirements are reduced.
   Algorithmic efforts have traditionally been focused on finding new improved methods to alleviate sampling artifacts by steering computation to the most important dimensions of the rendering equation. Concurrently, rendering algorithms can take advantage of certain perceptual limits to simplify and optimize computations. To our knowledge, none of them has identified nor used these limits in the rendering of motion blur. This work can be considered a first step in that direction.
C1 [Castillo, Susana; Seron, Francisco J.; Gutierrez, Diego] Univ Zaragoza, E-50009 Zaragoza, Spain.
C3 University of Zaragoza
EM Navarrog@gmail.com
RI Castillo, Susana/U-6432-2019; Seron Arbeloa, Francisco Jose/L-3146-2014
OI Castillo, Susana/0000-0003-1245-4758; Gutierrez Perez,
   Diego/0000-0002-7503-7022; Seron Arbeloa, Francisco
   Jose/0000-0003-1683-4694
FU Marie Curie grant from the Seventh Framework Programme [251415]; Spanish
   Ministry of Science and Technology [TIN2010-21543]; Gobierno de Aragon
   [OTRI 2009/0411, CTPP05/09]
FX All sequences used in this study have been rendered using Arnold
   licenses loaned by Solid Angle. This research has been funded by a Marie
   Curie grant from the Seventh Framework Programme (grant agreement no.
   251415), the Spanish Ministry of Science and Technology (TIN2010-21543)
   and the Gobierno de Aragon (projects OTRI 2009/0411 and CTPP05/09).
CR Abrams J, 2010, ATTEN PERCEPT PSYCHO, V72, P1510, DOI 10.3758/APP.72.6.1510
   ADELSON EH, 1985, J OPT SOC AM A, V2, P284, DOI 10.1364/JOSAA.2.000284
   [Anonymous], THESIS U TORONTO
   [Anonymous], NEURONAL MECH VISUAL
   [Anonymous], ACM SIGGRAPH 2007 PA
   [Anonymous], P S APPL PERC GRAPH
   [Anonymous], SOC PHOTOOPTICAL INS
   [Anonymous], 1966, BIOMETR TABLES ST
   [Anonymous], 2009, Visual equivalence in dynamic scenes
   [Anonymous], ACM SIGGRAPH ASIA 20
   [Anonymous], PSYCH SCI
   [Anonymous], ACM SIGGRAPH COURSE
   [Anonymous], P SPIE SEC STEG WAT
   [Anonymous], P 3 S APPL PERC GRAP
   [Anonymous], DCPSR200404 CZECH TE
   [Anonymous], 2008, ACM SIGGRAPH ASIA 20
   [Anonymous], 2006, ACM Trans. Appl. Percept.
   [Anonymous], P 6 S APPL PERC GRAP
   [Anonymous], ROY SOC LONDON P B
   [Anonymous], J OPT SOC AM
   [Anonymous], 1963, METHOD PAIRED COMP
   [Anonymous], P INT C IM PROC
   [Anonymous], ACM SIGGRAPH 2008 CL
   BURR D, 1980, NATURE, V284, P164, DOI 10.1038/284164a0
   BURR DC, 1986, PROC R SOC SER B-BIO, V227, P249, DOI 10.1098/rspb.1986.0022
   Burr DC, 1997, P ROY SOC B-BIOL SCI, V264, P431, DOI 10.1098/rspb.1997.0061
   Didyk P, 2010, COMPUT GRAPH FORUM, V29, P713, DOI 10.1111/j.1467-8659.2009.01641.x
   FLEET DJ, 1994, VISION RES, V34, P3057, DOI 10.1016/0042-6989(94)90278-X
   Fleming RW, 2003, J VISION, V3, P347, DOI 10.1167/3.5.3
   GOREA A, 1986, J OPT SOC AM A, V3, P52, DOI 10.1364/JOSAA.3.000052
   Hegdé J, 2004, J VISION, V4, P838, DOI 10.1167/4.10.1
   Jimenez J, 2009, ACM T APPL PERCEPT, V6, DOI 10.1145/1609967.1609970
   Kozlowski O, 2007, APGV 2007: SYMPOSIUM ON APPLIED PERCEPTION IN GRAPHICS AND VISUALIZATION, PROCEEDINGS, P91
   Longhurst P., 2006, P 4 INT C COMPUTER G, P21
   LOOMIS JM, 1973, PERCEPTION, V2, P425, DOI 10.1068/p020425
   Mack Arien, 1998, Inattentional Blindness
   Mantiuk R, 2005, PROC SPIE, V5666, P204, DOI 10.1117/12.586757
   Mantiuk R, 2004, IEEE SYS MAN CYBERN, P2763
   McDonnell R, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360625
   MORAN PAP, 1947, BIOMETRIKA, V34, P363, DOI 10.1093/biomet/34.3-4.363
   Navarro F, 2011, COMPUT GRAPH FORUM, V30, P3, DOI 10.1111/j.1467-8659.2010.01840.x
   PYLYSHYN Z W, 1988, Spatial Vision, V3, P179, DOI 10.1163/156856888X00122
   RAMACHAN.VS, 1974, PERCEPTION, V3, P97, DOI 10.1068/p030097
   Ramanarayanan G, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360659
   Ramasubramanian M, 1999, COMP GRAPH, P73, DOI 10.1145/311535.311543
   Rensink RA, 1997, PSYCHOL SCI, V8, P368, DOI 10.1111/j.1467-9280.1997.tb00427.x
   ROBSON JG, 1966, J OPT SOC AM, V56, P1141, DOI 10.1364/JOSA.56.001141
   Shapiro KL, 1997, TRENDS COGN SCI, V1, P291, DOI 10.1016/S1364-6613(97)01094-2
   Stephenson I., 2007, Journal of Graphics Tools, V12, P9
   Sung K, 2002, IEEE T VIS COMPUT GR, V8, P144, DOI 10.1109/2945.998667
   Tang CW, 2007, IEEE T MULTIMEDIA, V9, P231, DOI 10.1109/TMM.2006.886328
   TREISMAN A, 1990, J EXP PSYCHOL HUMAN, V16, P459, DOI 10.1037/0096-1523.16.3.459
   Vangorp P, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1276377.1276473, 10.1145/1239451.1239528]
   VIRSU V, 1982, VISION RES, V22, P1211, DOI 10.1016/0042-6989(82)90087-6
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang Z, 2007, J OPT SOC AM A, V24, pB61, DOI 10.1364/JOSAA.24.000B61
   Watson A.B., 2006, SID Symposium Digest of Technical Papers, V37, P1312
   WATSON AB, 1985, J OPT SOC AM A, V2, P322, DOI 10.1364/JOSAA.2.000322
   WATSON AB, 1986, J OPT SOC AM A, V3, P300, DOI 10.1364/JOSAA.3.000300
   Watson AB, 1999, PROC SPIE, V3644, P168, DOI 10.1117/12.348437
   Xia J, 2009, SIGNAL PROCESS-IMAGE, V24, P548, DOI 10.1016/j.image.2009.04.002
   Zhai Y., 2006, P 14 ACM INT C MULT, P815, DOI [DOI 10.1145/1180639.1180824, 10.1145/1180639.1180824]
NR 62
TC 11
Z9 12
U1 0
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD AUG
PY 2011
VL 8
IS 3
AR 20
DI 10.1145/2010325.2010330
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA 819HK
UT WOS:000294815800005
DA 2024-07-18
ER

PT J
AU Mion, L
   De Poli, G
   Rapanà, E
AF Mion, Luca
   De Poli, Giovanni
   Rapana, Ennio
TI Perceptual Organization of Affective and Sensorial Expressive Intentions
   in Music Performance
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Design; Human Factors; Expression; music performance
ID MODEL
AB Expression communication is the added value of a musical performance. It is part of the reason why music is interesting to listen to and sounds alive. Previous work on the analysis of acoustical features yielded relevant features for the recognition of different expressive intentions, inspired both by emotional and sensorial adjectives. In this article, machine learning techniques are employed to understand how expressive performances represented by the selected features are clustered on a low-dimensional space, and to define a measure of acoustical similarity. Being that expressive intentions are similar according to the features used for the recognition, and since recognition implies subjective evaluation, we hypothesized that performances are similar also from a perceptual point of view. We then compared and integrated the clustering of acoustical features with the results of two listening experiments. A first experiment aims at verifying whether subjects can distinguish different categories of expressive intentions, and a second experiment aims at understanding which expressions are perceptually clustered together in order to derive common evaluation criteria adopted by listeners, and to obtain the perceptual organization of affective and sensorial expressive intentions. An interpretation of the resulting spatial representation based on action is proposed and discussed.
C1 [Mion, Luca; De Poli, Giovanni; Rapana, Ennio] Univ Padua, Dept Informat Engn, I-35131 Padua, Italy.
C3 University of Padua
RP Mion, L (corresponding author), TasLab Informat Trentina SpA, Via G Gilli 2, I-38100 Trento, Italy.
EM luca.mion@infotn.it; depoli@dei.unipd.it
OI De Poli, Giovanni/0000-0001-8487-7093
FU EU [IST-1-002114]
FX This research was supported by the EU Network of Excellence "Enactive
   Interfaces" IST-1-002114.
CR [Anonymous], 2008, Embodied Music Cognition and Mediation Technology
   Bigand E, 2005, COGNITION EMOTION, V19, P1113, DOI 10.1080/02699930500204250
   Bresin R, 2000, COMPUT MUSIC J, V24, P44, DOI 10.1162/014892600559515
   Camurri A, 2005, IEEE MULTIMEDIA, V12, P43, DOI 10.1109/MMUL.2005.2
   Canazza S, 2004, P IEEE, V92, P686, DOI 10.1109/JPROC.2004.825889
   Canazza S, 2003, J NEW MUSIC RES, V32, P281, DOI 10.1076/jnmr.32.3.281.16862
   CARROLL JD, 1970, PSYCHOMETRIKA, V35, P283, DOI 10.1007/BF02310791
   Chi D, 2000, COMP GRAPH, P173, DOI 10.1145/344779.352172
   Dannenberg R., 1997, P INT COMPUTER MUSIC, P344
   De Poli G, 2004, J NEW MUSIC RES, V33, P189, DOI 10.1080/0929821042000317796
   DEPOLI G, 2002, MUSIC MATH, P243
   FRIBERG A, 1991, COMPUT MUSIC J, V15, P49, DOI 10.2307/3680916
   Friberg A., 2002, Proceedings of the International Computer Music Conference, ICMC 2002, Goteborg, P365
   Gabrielsson A., 1995, Music and the mind machine, P35
   Gabrielsson A., 1999, The Psychology of Music, Vsecond edition ed., P501, DOI [10.1016/B978-012213564-4/50015-9, DOI 10.1016/B978-012213564-4/50015-9]
   Gibson J., 1979, The ecological approach to visual perception
   Greenacre M.J., 1984, Theory and applications of correspondence analysis
   Hanslick Eduard., 1957, The Beautiful in Music
   HASHIMOTO S, 2004, P IEEE 92, P656
   HSIEH CH, 2005, P GRAPH C, P85
   JUSLIN PN, 2001, MUSIC EMOTION THEORY, P303
   LEMAN M, 2000, P COST G 6 C DIG AUD
   LESAFFRE ML, 2003, P STOCKH MUS AC C SM
   Lu L, 2006, IEEE T AUDIO SPEECH, V14, P5, DOI 10.1109/TSA.2005.860344
   Mion L, 2008, IEEE T AUDIO SPEECH, V16, P458, DOI 10.1109/TASL.2007.913743
   Palmer C, 1997, ANNU REV PSYCHOL, V48, P115, DOI 10.1146/annurev.psych.48.1.115
   REPP BH, 1992, J ACOUST SOC AM, V92, P2546, DOI 10.1121/1.404425
   REPP BH, 1990, J ACOUST SOC AM, V88, P622, DOI 10.1121/1.399766
   ROUSSEEUW PJ, 1987, J COMPUT APPL MATH, V20, P53, DOI 10.1016/0377-0427(87)90125-7
   RUSSELL JA, 1980, J PERS SOC PSYCHOL, V39, P1161, DOI 10.1037/h0077714
   Serra X., 1997, Music. signal Process., P91
   TODD N, 1985, MUSIC PERCEPT, V3, P33
   TODD NPM, 1992, J ACOUST SOC AM, V91, P3540, DOI 10.1121/1.402843
   Varela F., 1991, EMBODIED MIND
   Vines BW, 2006, COGNITION, V101, P80, DOI 10.1016/j.cognition.2005.09.003
   Widmer G, 2004, J NEW MUSIC RES, V33, P203, DOI 10.1080/0929821042000317804
NR 36
TC 8
Z9 8
U1 0
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD FEB
PY 2010
VL 7
IS 2
AR 14
DI 10.1145/1670671.1670678
PG 21
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI); Arts &amp; Humanities Citation Index (A&amp;HCI)
SC Computer Science
GA 563HF
UT WOS:000275118100007
DA 2024-07-18
ER

PT J
AU Filip, J
   Chantler, MJ
   Haindl, M
AF Filip, Jiri
   Chantler, Michael J.
   Haindl, Michal
TI On Uniform Resampling and Gaze Analysis of Bidirectional Texture
   Functions
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Human Factors; Experimentation; BTF; uniform resampling; visual
   degradation; phychophysical experiment; texture compression; eye
   tracking
ID REFLECTANCE
AB The use of illumination and view-dependent texture information is recently the best way to capture the appearance of real-world materials accurately. One example is the Bidirectional Texture Function. The main disadvantage of these data is their massive size. In this article, we employ perceptually-based methods to allow more efficient handling of these data. In the first step, we analyse different uniform resampling by means of a psychophysical study with 11 subjects, comparing original data with rendering of a uniformly resampled version over the hemisphere of illumination and view-dependent textural measurements. We have found that down-sampling in view and illumination azimuthal angles is less apparent than in elevation angles and that illumination directions can be down-sampled more than view directions without loss of visual accuracy. In the second step, we analyzed subjects gaze fixation during the experiment. The gaze analysis confirmed results from the experiment and revealed that subjects were fixating at locations aligned with direction of main gradient in rendered stimuli. As this gradient was mostly aligned with illumination gradient, we conclude that subjects were observing materials mainly in direction of illumination gradient. Our results provide interesting insights in human perception of real materials and show promising consequences for development of more efficient compression and rendering algorithms using these kind of massive data.
C1 [Filip, Jiri; Chantler, Michael J.] Heriot Watt Univ, Edinburgh EH14 4AS, Midlothian, Scotland.
C3 Heriot Watt University
RP Filip, J (corresponding author), Heriot Watt Univ, Edinburgh EH14 4AS, Midlothian, Scotland.
EM filipj@utia.cas.cz
RI Haindl, Michal/R-4909-2019; Filip, Jiri/D-3396-2012; Haindl,
   Michal/H-4323-2014
OI Haindl, Michal/0000-0001-8159-3685; Chantler, Mike/0000-0002-8381-1751
FU EPSRC SMI [GR/S12395/01, EP/F02553X/1]; GACR [102/08/0593]; MSMT
   [1M0572]; EPSRC [EP/F02553X/1] Funding Source: UKRI
FX This research was also supported by the EPSRC SMI grants GR/S12395/01
   and EP/F02553X/1, the GACR grant 102/08/0593, and grant MSMT 1M0572
   (DAR).
CR [Anonymous], P 28 ANN C COMP GRAP, DOI DOI 10.1145/383259.383266
   [Anonymous], 1977, NBS MONOGR
   COCHRAN WG, 1950, BIOMETRIKA, V37, P256, DOI 10.1093/biomet/37.3-4.256
   Daly Scott, 1993, P179
   Dana KJ, 1999, ACM T GRAPHIC, V18, P1, DOI 10.1145/300776.300778
   Deng ZG, 2005, IEEE COMPUT GRAPH, V25, P24, DOI 10.1109/MCG.2005.35
   Duchowski AT, 2004, CYBERPSYCHOL BEHAV, V7, P621, DOI 10.1089/cpb.2004.7.621
   Duchowski AT, 2002, BEHAV RES METH INS C, V34, P455, DOI 10.3758/BF03195475
   Elhelw M, 2008, ACM T APPL PERCEPT, V5, DOI 10.1145/1279640.1279643
   FILIP J, 2009, IEEE T PATT IN PRESS
   Filip J, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409091
   Fleming RW, 2003, J VISION, V3, P347, DOI 10.1167/3.5.3
   HAVRAN V, 2005, P EUR S REND, V31, P311
   Kautz J, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1239451.1239504, 10.1145/1276377.1276443]
   Lawson R, 2003, PERCEPTION, V32, P1465, DOI 10.1068/p5031
   Lee SP, 2002, ACM T GRAPHIC, V21, P637
   Matusik W, 2003, ACM T GRAPHIC, V22, P759, DOI 10.1145/882262.882343
   Meseth J., 2006, APGV '06, P127
   Müller G, 2005, COMPUT GRAPH FORUM, V24, P83, DOI 10.1111/j.1467-8659.2005.00830.x
   Over EAB, 2007, VISION RES, V47, P2272, DOI 10.1016/j.visres.2007.05.002
   Pas SFT, 2005, PERCEPTION, V34, P212
   PAS ST, 2005, P 2 S APPL PERC GRAP, P57
   Pellacini F, 2000, COMP GRAPH, P55, DOI 10.1145/344779.344812
   PELLACINI F., 2007, ACM T GRAPH, V26, P2
   Pomplun M, 2006, VISION RES, V46, P1886, DOI 10.1016/j.visres.2005.12.003
   Ramanarayanan G, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1276377.1276472, 10.1145/1239451.1239527]
   Sundstedt V, 2008, APGV 2008: PROCEEDINGS OF THE SYMPOSIUM ON APPLIED PERCEPTION IN GRAPHICS AND VISUALIZATION, P43
   Tatler BW, 2007, J VISION, V7, DOI 10.1167/7.14.4
NR 28
TC 2
Z9 2
U1 0
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD AUG
PY 2009
VL 6
IS 3
AR 18
DI 10.1145/1577755.1577761
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 511ZS
UT WOS:000271212000006
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Feixas, M
   Sbert, M
   González, F
AF Feixas, Miquel
   Sbert, Mateu
   Gonzalez, Francisco
TI A Unified Information-Theoretic Framework for Viewpoint Selection and
   Mesh Saliency
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Algorithms; Human Factors; Experimentation; Viewpoint selection; mesh
   saliency; visual perception; information theory
ID ENTROPY; IMAGE; VIEW
AB Viewpoint selection is an emerging area in computer graphics with applications in fields such as scene exploration, image-based modeling, and volume visualization. In particular, best view selection algorithms are used to obtain the minimum number of views (or images) in order to understand or model an object or scene better. In this article, we present a unified framework for viewpoint selection and mesh saliency based on the definition of an information channel between a set of viewpoints (input) and the set of polygons of an object (output). The mutual information of this channel is shown to be a powerful tool to deal with viewpoint selection, viewpoint stability, object exploration and viewpoint-based saliency. In addition, viewpoint mutual information is extended using saliency as an importance factor, showing how perceptual criteria can be incorporated to our method. Although we use a sphere of viewpoints around an object, our framework is also valid for any set of viewpoints in a closed scene. A number of experiments demonstrate the robustness of our approach and the good behavior of the proposed measures.
C1 [Feixas, Miquel; Sbert, Mateu; Gonzalez, Francisco] Univ Girona, Girona 17071, Spain.
C3 Universitat de Girona
RP Feixas, M (corresponding author), Univ Girona, Campus Montiliv P4, Girona 17071, Spain.
EM feixas@ima.udg.edu; mateu@ima.udg.edu; gonzalez@ima.udg.edu
RI Feixas, Miquel/F-6762-2016; Sbert, Mateu/G-6711-2011
OI Feixas, Miquel/0000-0001-6512-7588; Sbert, Mateu/0000-0003-2164-6858
FU Spanish Government [TIN2007-68066-C04-01]; VIth European Framework
   [IST-2-004363]
FX This project has been funded in part with grant numbers
   TIN2007-68066-C04-01 of the Spanish Government and IST-2-004363
   (GameTools: Advanced Tools for Developing Highly Realistic Computer
   Games) from the VIth European Framework.
CR Andújar C, 2004, COMPUT GRAPH FORUM, V23, P499, DOI 10.1111/j.1467-8659.2004.00781.x
   [Anonymous], 1981, Attention and Performance
   [Anonymous], P 23 ANN INT ACM SIG
   Blanz V, 1999, PERCEPTION, V28, P575, DOI 10.1068/p2897
   Bordoloi UD, 2005, IEEE VISUALIZATION 2005, PROCEEDINGS, P487
   BULTHOFF HH, 1995, CEREB CORTEX, V5, P247, DOI 10.1093/cercor/5.3.247
   BURBEA J, 1982, IEEE T INFORM THEORY, V28, P489, DOI 10.1109/TIT.1982.1056497
   Castelló P, 2007, WSCG 2007, FULL PAPERS PROCEEDINGS I AND II, P249
   CHRISTENSEN P, PHOTOREALISTIC RENDE
   Cover T. M., 1991, ELEMENTS INFORM THEO
   FEIXAS M, 2002, THESIS U POLITECNICA
   Gal R, 2006, ACM T GRAPHIC, V25, P130, DOI 10.1145/1122501.1122507
   González-Baños HH, 2002, INT J ROBOT RES, V21, P829, DOI 10.1177/0278364902021010834
   Gooch B, 2001, SPRING EUROGRAP, P83
   Iones A, 2003, IEEE COMPUT GRAPH, V23, P54, DOI 10.1109/MCG.2003.1198263
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Itti L., 2006, ADV NEURAL INFORM PR, V19, P1
   Kim Y, 2006, IEEE T VIS COMPUT GR, V12, P925, DOI 10.1109/TVCG.2006.174
   LANDIS H, 2002, COURSE NOTES ACM SIG
   Lee CH, 2005, ACM T GRAPHIC, V24, P659, DOI 10.1145/1073204.1073244
   Lu A., 2006, P EUR VIS 06, P655
   PLEMENOS D, 1996, P GRAPH CON 96
   Polonsky O, 2005, VISUAL COMPUT, V21, P840, DOI 10.1007/s00371-005-0326-y
   Sbert Mateu., 2005, COMPUTATIONAL AESTHE, P185
   Slonim N, 2000, ADV NEUR IN, V12, P617
   Sokolov D, 2006, VISUAL COMPUT, V22, P506, DOI 10.1007/s00371-006-0025-3
   Takahashi S, 2005, IEEE VISUALIZATION 2005, PROCEEDINGS, P495
   Tarr MJ, 1997, PSYCHOL SCI, V8, P282, DOI 10.1111/j.1467-9280.1997.tb00439.x
   Vazquez P.-P., 2001, Proceedings of Vision Modeling and Visualization Conference, P273
   Vázquez PP, 2006, COMPUT GRAPH-UK, V30, P98, DOI 10.1016/j.cag.2005.10.022
   Vázquez PP, 2003, COMPUT GRAPH FORUM, V22, P689, DOI 10.1111/j.1467-8659.2003.00717.x
   Viola I, 2006, IEEE T VIS COMPUT GR, V12, P933, DOI 10.1109/TVCG.2006.152
   Yamauchi H, 2006, IEEE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS 2006, PROCEEDINGS, P265
   ZHUKOV S, 1998, P EUR WORKSH REND TE, P45
NR 34
TC 97
Z9 114
U1 0
U2 17
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD FEB
PY 2009
VL 6
IS 1
AR 1
DI 10.1145/1462055.1462056
PG 23
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 450YM
UT WOS:000266437900001
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Gray, R
   Mohebbi, R
   Tan, HZ
AF Gray, Rob
   Mohebbi, Rayka
   Tan, Hong Z.
TI The Spatial Resolution of Crossmodal Attention: Implications for the
   Design of Multimodal Interfaces
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Experimentation; Human Factors; Attention; warnings
ID VISUAL-ATTENTION; AUDITORY CUES; MODAL LINKS; VIBROTACTILE; VISION;
   TOUCH
AB Previous research on crossmodal attentional orienting has reported speeded reaction times (RT) when the stimuli from the different modalities are in the same spatial location and slowed RTs when the stimuli are presented in very different locations (e. g., opposite sides of the body). However, little is known about what occurs for spatial interactions between these two extremes. We systematically varied the separation between cues and targets to quantify the spatial distribution of crossmodal attention. The orthogonal cueing paradigm [Spence et al. 1998] was used. Visual targets presented above or below the forearm were preceded by either vibrotactile cues presented on the forearm, auditory cues presented below the forearm, or visual cues presented on the forearm. The presentation of both unimodal and crossmodal cues led to a roughly monotonic increase in RT as a function of the cue-target separation. Unimodal visual cueing resulted in an attentional focus that was significantly narrower than that produced by crossmodal cues: the distribution of visual attention for visual cues had roughly half of the lateral extent of that produced by tactile cueing and roughly one fourth of the lateral extent as that produced by auditory cueing. This occurred when both seven (Experiment 1) and three (Experiment 2) cue locations were used suggesting that the effects are not primarily due to differences in the ability to localize the cues. These findings suggest that the location of tactile and auditory warning signals does not have to be controlled as precisely as the location of visual warning signals to facilitate a response to the critical visual event.
C1 [Gray, Rob; Mohebbi, Rayka] Arizona State Univ, Dept Appl Psychol, Mesa, AZ 85212 USA.
   [Tan, Hong Z.] Purdue Univ, Sch Elect & Comp Engn, W Lafayette, IN 47907 USA.
C3 Arizona State University; Purdue University System; Purdue University
RP Gray, R (corresponding author), Arizona State Univ, Dept Appl Psychol, Sutton 340E,7001 E Williams Field Rd, Mesa, AZ 85212 USA.
RI Gray, Rob/A-3951-2010; gray, robert/HJB-2567-2022
OI Tan, Hong/0000-0003-0032-9554
FU National Science Foundation [0533908]; Div Of Information & Intelligent
   Systems; Direct For Computer & Info Scie & Enginr [0533908] Funding
   Source: National Science Foundation
FX This research was supported by a National Science Foundation grant
   (award #0533908) to authors R.G. and H.Z.T.
CR [Anonymous], 1980, Attention Per- form. VIII
   CHASTAIN G, 1992, PSYCHOL RES-PSYCH FO, V54, P175, DOI 10.1007/BF00922096
   Colonius H, 2001, PERCEPT PSYCHOPHYS, V63, P126, DOI 10.3758/BF03200508
   Diederich A, 2003, EXP BRAIN RES, V148, P328, DOI 10.1007/s00221-002-1302-7
   DIEDERICH A, 2007, PERCEP PSYCHOPHY
   Downing C.J., 1985, ATTENTION PERFORM, P171
   Driver J, 1998, PHILOS T R SOC B, V353, P1319, DOI 10.1098/rstb.1998.0286
   EDWORTHY J, 1991, HUM FACTORS, V33, P205, DOI 10.1177/001872089103300206
   Edworthy J., 1996, Warning design: A research prospective
   Eimer M, 2005, EXP BRAIN RES, V166, P402, DOI 10.1007/s00221-005-2380-0
   FARAH MJ, 1989, NEUROPSYCHOLOGIA, V27, P461, DOI 10.1016/0028-3932(89)90051-1
   Ferris T., 2006, Proceedings of the Human Factors and Ergonomics Society Annual Meeting, V50, P406, DOI DOI 10.1177/154193120605000341
   Groh JM, 1996, J NEUROPHYSIOL, V75, P428, DOI 10.1152/jn.1996.75.1.428
   Ho C, 2005, J EXP PSYCHOL-APPL, V11, P157, DOI 10.1037/1076-898X.11.3.157
   Ho C, 2005, TRANSPORT RES F-TRAF, V8, P397, DOI 10.1016/j.trf.2005.05.002
   Ho C, 2006, ERGONOMICS, V49, P724, DOI 10.1080/00140130600589887
   JONES CM, 2007, P WORLD HAPT C WHC07
   Kennett S, 2002, PERCEPT PSYCHOPHYS, V64, P1083, DOI 10.3758/BF03194758
   Rupert AH, 2000, IEEE ENG MED BIOL, V19, P71, DOI 10.1109/51.827409
   Rupert AH, 2000, AVIAT SPACE ENVIR MD, V71, pA92
   SHEPHERD M, 1989, PERCEPT PSYCHOPHYS, V46, P146, DOI 10.3758/BF03204974
   SHULMAN GL, 1979, J EXP PSYCHOL HUMAN, V5, P522, DOI 10.1037/0096-1523.5.3.522
   SHULMAN GL, 1985, PERCEPT PSYCHOPHYS, V37, P59, DOI 10.3758/BF03207139
   Spence C, 1998, PERCEPT PSYCHOPHYS, V60, P544, DOI 10.3758/BF03206045
   Spence C, 1996, J EXP PSYCHOL HUMAN, V22, P1005, DOI 10.1037/0096-1523.22.4.1005
   Spence C, 2000, J EXP PSYCHOL HUMAN, V26, P1298, DOI 10.1037//0096-1523.26.4.1298
   SPENCE C, 2001, ADV PSYCHOL, V133, P231
   Spence C., 1997, International Journal of Cognitive Ergonomics, V1, P351
   Stein Barry E., 1993, The Merging of the Senses. The Merging of the Senses. Cognitive Neuroscience
   Tan H.Z., 2003, Haptics-e: The Electronic Journal of Haptics Research, V3
   Wiener E.L., 1988, Human Factors in Aviation
NR 31
TC 12
Z9 13
U1 0
U2 7
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD FEB
PY 2009
VL 6
IS 1
AR 4
DI 10.1145/1462055.1462059
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA 450YM
UT WOS:000266437900004
DA 2024-07-18
ER

PT J
AU De Pra, Y
   Fontana, F
   Järveläinen, H
   Papetti, S
   Simonato, M
AF De Pra, Yuri
   Fontana, Federico
   Jaervelaeinen, Hanna
   Papetti, Stefano
   Simonato, Michele
TI Does It Ping or Pong? Auditory and Tactile Classification of Materials
   by Bouncing Events
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Material classification; auditory feedback; tactile feedback;
   multisensory integration; virtual buttons
ID PERCEPTION; IDENTIFICATION; INTENSITY; DISCRIMINATION; INTEGRATION;
   INFORMATION; NORMALITY; HEARING; WOOD
AB Two experiments studied the role of impact sounds and vibrations in classification of materials. The task consisted of feeling on an actuated surface and listening through headphones to the recorded feedback of a ping-pong ball hitting three flat objects respectively made of wood, plastic, and metal, and then identifying their material. In Experiment 1, sounds and vibrations were recorded by keeping the objects in mechanical isolation. In Experiment 2, recordings were taken while the same objects stood on a table, causing their resonances to fade faster due to mechanical coupling with the support. A control experiment, where participants listened to and touched the real objects in mechanical isolation, showed high accuracy of classification from either sounds (90% correct) or vibrations (67% correct). Classification of reproduced bounces in Experiments 1 and 2 was less precise. In both experiments, the main effect of material was statistically significant; conversely, the main effect of modality (auditory or tactile) was significant only in the control. Identification of plastic and especially metal was less accurate in Experiment 2, suggesting that participants, when possible, classified materials by longer resonance tails. Audiotactile summation of classification accuracy was found, suggesting that multisensory integration influences the perception of materials. Such results have prospective application to the nonvisual design of virtual buttons, which is the object of our current research.
C1 [De Pra, Yuri; Fontana, Federico] Univ Udine, Via Sci 206, I-33100 Udine, Italy.
   [Jaervelaeinen, Hanna; Papetti, Stefano] Zurich Univ Arts, Pfingstweidstr 96, CH-8005 Zurich, Switzerland.
   [Simonato, Michele] Electrolux Profess SpA, Viale Treviso 40, I-33170 Pordenone, Italy.
C3 University of Udine; AB Electrolux
RP De Pra, Y (corresponding author), Univ Udine, Via Sci 206, I-33100 Udine, Italy.
EM yuri.depra@uniud.it; federico.fontana@uniud.it;
   hanna.jarvelainen@zhdk.ch; stefano.papetti@zhdk.ch;
   michele.simonato@electrolux.com
RI De Pra, Yuri/AAD-2994-2021; Papetti, Stefano/ABC-9182-2020
OI Papetti, Stefano/0000-0002-7490-3574; Fontana,
   Federico/0000-0002-1692-2603; DE PRA, Yuri/0000-0002-0876-4828
FU PRID project SMARTLAND - University of Udine; UNIUD plan PSA -
   University of Udine; project HAPTEEV (Haptic technology and evaluation
   for digital musical interfaces 2018-2022) - Swiss National Science
   Foundation; Research Hub by Electrolux Professional SpA
FX This research was partially supported by PRID project SMARTLAND and by
   UNIUD plan PSA, both funded by the University of Udine, and by project
   HAPTEEV (Haptic technology and evaluation for digital musical interfaces
   2018-2022) funded by the Swiss National Science Foundation. Y. De Pra's
   Ph.D. was funded with a scholarship from the Research Hub by Electrolux
   Professional SpA.
CR Banu A, 2019, HUCAPP: PROCEEDINGS OF THE 14TH INTERNATIONAL JOINT CONFERENCE ON COMPUTER VISION, IMAGING AND COMPUTER GRAPHICS THEORY AND APPLICATIONS - VOL 2: HUCAPP, P97, DOI 10.5220/0007347100970102
   Baumgartner E, 2013, MULTISENS RES, V26, P429, DOI 10.1163/22134808-00002429
   Bensmaïa S, 2005, PERCEPT PSYCHOPHYS, V67, P828, DOI 10.3758/BF03193536
   Bensmaïa SJ, 2000, J ACOUST SOC AM, V108, P1236, DOI 10.1121/1.1288937
   Bresciani JP, 2005, EXP BRAIN RES, V162, P172, DOI 10.1007/s00221-004-2128-2
   Castiello U, 2010, PLOS ONE, V5, DOI 10.1371/journal.pone.0013199
   DAGOSTINO RB, 1971, BIOMETRIKA, V58, P341, DOI 10.1093/biomet/58.2.341
   De Pra Yuri, 2019, P INT WORKSH HAPT AU
   DiFilippo Derek, 2005, ACM SIGGRAPH 2005 CO, P164
   Dupont P, 2002, IEEE T AUTOMAT CONTR, V47, P787, DOI 10.1109/TAC.2002.1000274
   Ernst MO, 2002, NATURE, V415, P429, DOI 10.1038/415429a
   Fontana F., 2016, P INT C SYST MAN CYB, P143
   Fontana F, 2017, J ACOUST SOC AM, V142, P2953, DOI 10.1121/1.5009659
   Foxe JJ, 2009, CURR BIOL, V19, pR373, DOI 10.1016/j.cub.2009.03.029
   Friedman M, 1937, J AM STAT ASSOC, V32, P675, DOI 10.2307/2279372
   Fujisaki W, 2015, VISION RES, V109, P185, DOI 10.1016/j.visres.2014.11.020
   GAVER WW, 1993, HUMAN FACTORS IN COMPUTING SYSTEMS, P228
   Giordano BL, 2006, J ACOUST SOC AM, V119, P1171, DOI 10.1121/1.2149839
   Giordano Bruno L., 2014, PERCEPTION SYNTHESIS, P49, DOI [10.1007/978-1-4471-6533-0_4, DOI 10.1007/978-1-4471-6533-0_4]
   Guest S, 2002, EXP BRAIN RES, V146, P161, DOI 10.1007/s00221-002-1164-z
   Hachisu Taku, 2012, Haptics: Perception, Devices, Mobility, and Communication. Proceedings International Conference (EuroHaptics 2012), P173, DOI 10.1007/978-3-642-31401-8_16
   Hachisu Taku., 2011, PAPER PRESENTED 24 A, P73, DOI [10.1145/2046396, DOI 10.1145/2046396]
   Hettmansperger T. P., 2010, ROBUST NONPARAMETRIC
   Higashi K, 2017, 2017 IEEE WORLD HAPTICS CONFERENCE (WHC), P37, DOI 10.1109/WHC.2017.7989853
   Higashi K, 2016, LECT NOTES COMPUT SC, V9774, P3, DOI 10.1007/978-3-319-42321-0_1
   Higashi K, 2015, IEEE SYS MAN CYBERN, P1539, DOI 10.1109/SMC.2015.272
   Higashi Kosuke., 2017, IEEE 6th Global Conference on Consumer Electronics, P1
   Ho HN, 2007, ACM T APPL PERCEPT, V4, DOI 10.1145/1265957.1265962
   Huang JY, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0038626
   HWANG C, 2011, J HIGH ENERGY PHYS, DOI DOI 10.1145/2037373.2037376
   Jousmäki V, 1998, CURR BIOL, V8, pR190, DOI 10.1016/S0960-9822(98)70120-4
   Kaaresoja T, 2014, ACM T APPL PERCEPT, V11, DOI 10.1145/2611387
   Kim YS, 2006, SYMPOSIUM ON HAPTICS INTERFACES FOR VIRTUAL ENVIRONMENT AND TELEOPERATOR SYSTEMS 2006, PROCEEDINGS, P133
   Klatzky RL, 2000, PRESENCE-TELEOP VIRT, V9, P399, DOI 10.1162/105474600566907
   Koskinen E, 2008, SPAA'08: PROCEEDINGS OF THE TWENTIETH ANNUAL SYMPOSIUM ON PARALLELISM IN ALGORITHMS AND ARCHITECTURES, P297
   Kunkler-Peck AJ, 2000, J EXP PSYCHOL HUMAN, V26, P279, DOI 10.1037/0096-1523.26.1.279
   Levitin DJ, 2000, AIP CONF PROC, V517, P323, DOI 10.1063/1.1291270
   Lutfi RA, 1997, J ACOUST SOC AM, V102, P3647, DOI 10.1121/1.420151
   Lutfi RA, 2010, J ACOUST SOC AM, V127, P350, DOI 10.1121/1.3263606
   Maeder PP, 2001, NEUROIMAGE, V14, P802, DOI 10.1006/nimg.2001.0888
   Martín R, 2018, PROCEEDINGS OF THE 2018 ACM INTERNATIONAL CONFERENCE ON INTERACTIVE SURFACES AND SPACES (ISS'18), P305, DOI 10.1145/3279778.3281455
   Ndengue JD, 2017, IEEE T HAPTICS, V10, P409, DOI 10.1109/TOH.2016.2643662
   Okazaki R, 2013, 2013 WORLD HAPTICS CONFERENCE (WHC), P663, DOI 10.1109/WHC.2013.6548487
   Ro T, 2009, EXP BRAIN RES, V195, P135, DOI 10.1007/s00221-009-1759-8
   Romagnoli Marco, 2011, Haptic and Audio Interaction Design. Proceedings 6th International Workshop, HAID 2011, P91, DOI 10.1007/978-3-642-22950-3_10
   Russo FA, 2012, J EXP PSYCHOL HUMAN, V38, P822, DOI 10.1037/a0029046
   Schürmann M, 2004, J ACOUST SOC AM, V115, P830, DOI 10.1121/1.1639909
   Senna I, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0091688
   Smith S. J., 2004, TECHNICAL REPORT
   Soto-Faraco S, 2004, COGN AFFECT BEHAV NE, V4, P208, DOI 10.3758/CABN.4.2.208
   Soto-Faraco S, 2009, BEHAV BRAIN RES, V196, P145, DOI 10.1016/j.bbr.2008.09.018
   TERHARDT E, 1982, J ACOUST SOC AM, V71, P679, DOI 10.1121/1.387544
   Tsai YY, 2013, J EXP PSYCHOL HUMAN, V39, P925, DOI 10.1037/a0030400
   Tucker Simon, 2002, TECHNICAL REPORT
   VERRILLO RT, 1992, MUSIC PERCEPT, V9, P281
   Vorländer M, 2000, J ACOUST SOC AM, V107, P2082, DOI 10.1121/1.428490
   Wilson EC, 2009, J ACOUST SOC AM, V126, P1960, DOI 10.1121/1.3204305
   Yau JM, 2009, CURR BIOL, V19, P561, DOI 10.1016/j.cub.2009.02.013
   Zampini M, 2004, J SENS STUD, V19, P347, DOI 10.1111/j.1745-459x.2004.080403.x
   Zhao L, 2019, CHI EA '19 EXTENDED ABSTRACTS: EXTENDED ABSTRACTS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290607.3312778
NR 60
TC 1
Z9 1
U1 1
U2 6
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD MAY
PY 2020
VL 17
IS 2
AR 8
DI 10.1145/3393898
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA OH5JF
UT WOS:000582618500004
OA Green Published
DA 2024-07-18
ER

PT J
AU Usevitch, DE
   Sperry, AJ
   Abbott, JJ
AF Usevitch, David E.
   Sperry, Adam J.
   Abbott, Jake J.
TI Translational and Rotational Arrow Cues (TRAC) Navigation Method for
   Manual Alignment Tasks
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Image-guided surgery; visual guidance; pose matching
ID ORIENTATION; SURGERY; SYSTEM
AB Many tasks in image-guided surgery require a clinician to manually position an instrument in space, with respect to a patient, with five or six degrees of freedom (DOF). Displaying the current and desired pose of the object on a 2D display such as a computer monitor is straightforward. However, providing guidance to accurately and rapidly navigate the object in 5-DOF or 6-DOF is challenging. Guidance is typically accomplished by showing distinct orthogonal viewpoints of the workspace, requiring simultaneous alignment in all views. Although such methods are commonly used, they can be quite unintuitive, and it can take a long time to perform an accurate 5-DOF or 6-DOF alignment task. In this article, we describe a method of visually communicating navigation instructions using translational and rotational arrow cues (TRAC) defined in an object-centric frame, while displaying a single principal view that approximates the human's egocentric view of the physical object. The target pose of the object is provided but typically is used only for the initial gross alignment. During the accurate-alignment stage, the user follows the unambiguous arrow commands. In a series of human-subject studies, we show that the TRAC method outperforms two common orthogonal-view methods-the triplanar display, and a sight-alignment method that closely approximates the Acrobot Navigation System-in terms of time to complete 5-DOF and 6-DOF navigation tasks. We also find that subjects can achieve 1 mm and 1 degrees accuracy using the TRAC method with a median completion time of less than 20 seconds.
C1 [Usevitch, David E.; Sperry, Adam J.; Abbott, Jake J.] Univ Utah, Dept Mech Engn, 1495 East 100 S, Salt Lake City, UT 84112 USA.
   [Usevitch, David E.; Sperry, Adam J.; Abbott, Jake J.] Univ Utah, Robot Ctr, 1495 East 100 S, Salt Lake City, UT 84112 USA.
C3 Utah System of Higher Education; University of Utah; Utah System of
   Higher Education; University of Utah
RP Usevitch, DE (corresponding author), Univ Utah, Dept Mech Engn, 1495 East 100 S, Salt Lake City, UT 84112 USA.; Usevitch, DE (corresponding author), Univ Utah, Robot Ctr, 1495 East 100 S, Salt Lake City, UT 84112 USA.
EM david.usevitch@utah.edu; adam.sperry@utah.edu; jake.abbott@utah.edu
OI Sperry, Adam/0000-0001-5078-135X; Usevitch, David/0000-0002-6044-0388
FU National Institute of Deafness and Other Communication Disorders of the
   National Institutes of Health [R01DC013168]
FX Research reported in this publication was supported in part by the
   National Institute of Deafness and Other Communication Disorders of the
   National Institutes of Health under Award Number R01DC013168.
CR ADAMS L, 1990, IEEE COMPUT GRAPH, V10, P43, DOI 10.1109/38.55152
   Barrett ARW, 2007, P I MECH ENG H, V221, P773, DOI 10.1243/09544119JEIM283
   Boctor EM, 2004, INT CONGR SER, V1268, P503, DOI 10.1016/j.ics.2004.03.261
   Bruns TL, 2017, PROC SPIE, V10135, DOI 10.1117/12.2256043
   Coluccia E, 2004, J ENVIRON PSYCHOL, V24, P329, DOI 10.1016/j.jenvp.2004.08.006
   Dagnino G, 2016, INT J COMPUT ASS RAD, V11, P1831, DOI 10.1007/s11548-016-1418-z
   DiGioia A M, 1998, Clin Orthop Relat Res, P8
   GALLOWAY RL, 1992, IEEE T BIO-MED ENG, V39, P1226, DOI 10.1109/10.184698
   Jacob R.J. K., 1994, ACM Transactions Computer-Human Interaction, V1, P3, DOI [DOI 10.1145/174630.174631, 10.1145/174630.174631]
   Joskowicz L, 1998, Comput Aided Surg, V3, P271, DOI 10.1002/(SICI)1097-0150(1998)3:6<271::AID-IGS1>3.0.CO;2-Y
   Krombach GA, 2000, NEURORADIOLOGY, V42, P838, DOI 10.1007/s002340000433
   Labadie R. F., 2016, IMAGE GUIDED SURG FU
   Leon Lisandro, 2018, J Med Robot Res, V3, DOI 10.1142/S2424905X18500046
   Lynch K. M., 2019, MODERN ROBOTICS MECH
   Moscatelli A, 2012, J VISION, V12, DOI 10.1167/12.11.26
   National Eye Institute, 2015, FACTS COLOR BLINDNES
   PARSONS LM, 1995, J EXP PSYCHOL HUMAN, V21, P1259, DOI 10.1037/0096-1523.21.6.1259
   Sun SY, 2013, I S BIOMED IMAGING, P21
   Traub J, 2006, LECT NOTES COMPUT SC, V4091, P179
   Usevitch D. E., 2018, P HAML S MED ROB, P104
   Ware C., 2004, Proceedings of the 1st Symposium on Applied perception in graphics and visualization, P135
   Yaniv Z, 2010, IEEE T BIO-MED ENG, V57, P922, DOI 10.1109/TBME.2009.2035688
NR 22
TC 3
Z9 4
U1 0
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD MAR
PY 2020
VL 17
IS 1
AR 1
DI 10.1145/3375001
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OH5JA
UT WOS:000582618000001
PM 34113222
OA Bronze, Green Accepted
DA 2024-07-18
ER

PT J
AU Schmidtler, J
   Korber, M
AF Schmidtler, Jonas
   Koerber, Moritz
TI Human Perception of Inertial Mass for Joint Human-Robot Object
   Manipulation
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Haptic human-robot collaboration; human factors; psychophysics; haptics;
   differential thresholds; human modelling
ID WEIGHT; STABILITY; DISCRIMINATION; COOPERATION; MOVEMENTS; FEEDBACK;
   FORCES; HAND
AB In this article, we investigate human perception of inertial mass discrimination in active planar manipulations, as they are common in daily tasks, such as moving heavy and bulky objects. Psychophysical experiments were conducted to develop a human inertial mass perception model to improve usability and acceptance of novel haptically collaborating robotic systems. In contrast to existing literature, large-scale movements involving a broad selection of reference stimuli and larger sample sizes were used. Linear mixed models were fitted to model dependent errors from the longitudinal perceptual data. Differential thresholds near the perception boundary exponentially increased and resulted in constant behavior for higher stimuli. No effect of different directions (sagittal and transversal) was found; however, a large effect of different movement types (precise and imprecise) was present in the data. Recommendations to implement the findings in novel physical assist devices are given.
C1 [Schmidtler, Jonas; Koerber, Moritz] Tech Univ Munich, Chair Ergon, Boltzmannstr 15, D-85747 Garching, Germany.
C3 Technical University of Munich
RP Schmidtler, J (corresponding author), Tech Univ Munich, Chair Ergon, Boltzmannstr 15, D-85747 Garching, Germany.
EM jonas.schmidtler@tum.de; moritz.koerber@tum.de
FU German Federal Ministry of Education and Research, under grant
   KobotAERGO (BMBF) [16SV6159]
FX This is work is supported by the German Federal Ministry of Education
   and Research, under grant KobotAERGO (BMBF, 16SV6159).
CR Abbink DA, 2012, COGN TECHNOL WORK, V14, P19, DOI 10.1007/s10111-011-0192-5
   Akella P, 1999, ICRA '99: IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION, VOLS 1-4, PROCEEDINGS, P728, DOI 10.1109/ROBOT.1999.770061
   Allin S, 2002, 10TH SYMPOSIUM ON HAPTIC INTERFACES FOR VIRTUAL ENVIRONMENT AND TELEOPERATOR SYSTEMS, PROCEEDINGS, P299, DOI 10.1109/HAPTIC.2002.998972
   Amazeen EL, 1996, J EXP PSYCHOL HUMAN, V22, P213, DOI 10.1037/0096-1523.22.1.213
   [Anonymous], P IEEE RSJ INT C INT
   [Anonymous], 2012, 1021822011 DIN EN IS
   [Anonymous], 2012, ADV HUM-COMPUT INTER
   [Anonymous], HUMAN CTR ROBOTICS S
   [Anonymous], HUMAN HYBRID ROBOT N
   [Anonymous], Z ARBEITSWISSENSCHAF
   [Anonymous], TOUCH VIRTUAL ENV HA
   [Anonymous], IEEE T HAPTICS UNPUB
   [Anonymous], IND ROBOT INT J
   [Anonymous], SIZEGERMANY NEW GERM
   [Anonymous], 102181 DIN EN ISO
   [Anonymous], P HUM FACT ERG SOC E
   [Anonymous], WORLD REC 248000 IND
   [Anonymous], 1997, PSYCHOPHYSICS FUNDAM
   [Anonymous], PROCEDIA MANUF
   [Anonymous], R LANG ENV STAT COMP
   [Anonymous], 2016, 15066 ISO TS
   [Anonymous], P VIRT REAL ANN INT
   [Anonymous], INT J HUMANOID ROBOT
   [Anonymous], 1996, FORCE TOUCH FEEDBACK
   [Anonymous], OXFORD HDB COGNITIVE
   [Anonymous], INFLUENCE SHARED MEN
   [Anonymous], 2006, Neuromuscular analysis of haptic gas pedal feedback during car following
   [Anonymous], PRESENCE TELEOPERATO
   [Anonymous], ERGONOMICS
   Baayen RH, 2008, J MEM LANG, V59, P390, DOI 10.1016/j.jml.2007.12.005
   Baird JohnC., 1978, FUNDAMENTALS SCALING
   Bates D, 2015, J STAT SOFTW, V67, P1, DOI 10.18637/jss.v067.i01
   Baty F, 2015, J STAT SOFTW, V66, P1
   Box GEP, 1986, Empirical Model-Building and Response Surface
   Brewer BR, 2005, IEEE T NEUR SYS REH, V13, P1, DOI 10.1109/TNSRE.2005.843443
   BRODIE EE, 1985, AM J PSYCHOL, V98, P469, DOI 10.2307/1422630
   Burdet E., 2013, HUMAN ROBOTICS NEURO
   Burghart C, 2002, INTELLIGENT AUTONOMOUS SYSTEMS 7, P38
   Campeau-Lecours A, 2016, INT J ADV ROBOT SYST, V13, DOI 10.1177/1729881416658167
   CHAPMAN CE, 1994, CAN J PHYSIOL PHARM, V72, P558, DOI 10.1139/y94-080
   Cherubini A, 2016, ROBOT CIM-INT MANUF, V40, P1, DOI 10.1016/j.rcim.2015.12.007
   De Santis A, 2008, MECH MACH THEORY, V43, P253, DOI 10.1016/j.mechmachtheory.2007.03.003
   Debats NB, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0042941
   Dimeas F, 2016, IEEE T HAPTICS, V9, P267, DOI 10.1109/TOH.2016.2518670
   Doerrer C., 2002, P EUROHAPTICS
   Dorjgotov E, 2008, SYMPOSIUM ON HAPTICS INTERFACES FOR VIRTUAL ENVIRONMENT AND TELEOPERATOR SYSTEMS 2008, PROCEEDINGS, P121
   Duchaine V, 2008, IEEE INT CONF ROBOT, P2189, DOI 10.1109/ROBOT.2008.4543531
   Duchaine V, 2012, IEEE T HAPTICS, V5, P148, DOI [10.1109/TOH.2011.49, 10.1109/ToH.2011.49]
   Feyzabadi S, 2013, IEEE T HAPTICS, V6, P309, DOI 10.1109/TOH.2013.4
   Flanagan JR, 2000, NAT NEUROSCI, V3, P737, DOI 10.1038/76701
   Fogliatto FS, 2012, INT J PROD ECON, V138, P14, DOI 10.1016/j.ijpe.2012.03.002
   Goldstein E.Bruce., 2014, Sensation and Perception, V9th
   Grafakos S, 2016, IEEE SYS MAN CYBERN, P1900, DOI 10.1109/SMC.2016.7844516
   Hale KS, 2004, IEEE COMPUT GRAPH, V24, P33, DOI 10.1109/MCG.2004.1274059
   Hatzfeld C., 2014, ENG HAPTIC DEVICES
   Helbig HB., 2008, Human Haptic Perception: Basics and Applications, P235, DOI [10.1007/978-3-7643-7612-3_18, DOI 10.1007/978-3-7643-7612-3_18]
   Heyer C, 2010, IEEE INT C INT ROBOT, P4749, DOI 10.1109/IROS.2010.5651294
   Hirche S., 2005, Haptic telepresence in packet switched communication networks
   Höver R, 2010, ACM T APPL PERCEPT, V8, DOI 10.1145/1857893.1857900
   Hurmuzlu Y, 1998, PRESENCE-TELEOP VIRT, V7, P290, DOI 10.1162/105474698565721
   Jones LA, 2003, EXP BRAIN RES, V151, P197, DOI 10.1007/s00221-003-1434-4
   JONES LA, 1986, PSYCHOL BULL, V100, P29, DOI 10.1037/0033-2909.100.1.29
   JONES LA, 1989, PERCEPTION, V18, P681, DOI 10.1068/p180681
   Karniel A, 2012, IEEE T HAPTICS, V5, P193, DOI 10.1109/TOH.2012.47
   Kazerooni H., 2008, ASME 2008 Dynamic Systems and Control Conference, Anonymous American Society of Mechanical Engineers, P1539, DOI DOI 10.1115/DSCC2008-2407
   Krüger J, 2009, CIRP ANN-MANUF TECHN, V58, P628, DOI 10.1016/j.cirp.2009.09.009
   Lecours A, 2012, IEEE INT CONF ROBOT, P3903, DOI 10.1109/ICRA.2012.6224586
   LEDERMAN SJ, 1987, COGNITIVE PSYCHOL, V19, P342, DOI 10.1016/0010-0285(87)90008-9
   Leek MR, 2001, PERCEPT PSYCHOPHYS, V63, P1279, DOI 10.3758/BF03194543
   LESTIENNE F, 1979, EXP BRAIN RES, V35, P407
   Madan CE, 2015, IEEE T HAPTICS, V8, P54, DOI 10.1109/TOH.2014.2384049
   Marayong P, 2004, HUM FACTORS, V46, P518
   Moscatelli A, 2012, J VISION, V12, DOI 10.1167/12.11.26
   Nakagawa S, 2013, METHODS ECOL EVOL, V4, P133, DOI 10.1111/j.2041-210x.2012.00261.x
   Newberry AC, 2007, P I MECH ENG D-J AUT, V221, P405, DOI 10.1243/09544070JAUTO415
   Okamura AM, 2004, IND ROBOT, V31, P499, DOI 10.1108/01439910410566362
   PANG XD, 1991, PERCEPT PSYCHOPHYS, V49, P531, DOI 10.3758/BF03212187
   ROSS HE, 1982, PERCEPT PSYCHOPHYS, V31, P429, DOI 10.3758/BF03204852
   ROSS HE, 1987, Q J EXP PSYCHOL-A, V39, P77, DOI 10.1080/02724988743000042
   SARTER NB, 1995, HUM FACTORS, V37, P5, DOI 10.1518/001872095779049516
   Schmidtler Jonas, 2015, Occupational Ergonomics, V12, P83, DOI 10.3233/OER-150226
   Schmidtler J, 2016, IEEE SYS MAN CYBERN, P217, DOI 10.1109/SMC.2016.7844244
   Schmidtler J, 2016, IEEE ROMAN, P874, DOI 10.1109/ROMAN.2016.7745222
   Seltman H.J., 2012, Experimental Design and Analysis
   Sheridan TB, 2016, HUM FACTORS, V58, P525, DOI 10.1177/0018720816644364
   SMEETS JBJ, 1990, EXP BRAIN RES, V81, P303, DOI 10.1007/BF00228120
   Srinivasan MA, 1997, COMPUT GRAPH-UK, V21, P393, DOI 10.1016/S0097-8493(97)00030-7
   TICHAUER ER, 1966, JOM-J OCCUP MED, V8, P63
   Tiest W.M. B., 2015, Scholarpedia, V10, P32732, DOI 10.4249/scholarpedia.32732
   Tiest WMB, 2010, ATTEN PERCEPT PSYCHO, V72, P1144, DOI 10.3758/APP.72.4.1144
   Tsumugiwa T, 2007, J ADV MECH DES SYST, V1, P113, DOI 10.1299/jamdsm.1.113
   Verbeek JH, 2012, WORK, V41, P2299, DOI 10.3233/WOR-2012-0455-2299
   Verbeke G., 1997, Linear mixed models in practice: A SAS oriented approach, V126
   Vicentini M, 2010, ACM T APPL PERCEPT, V8, DOI 10.1145/1857893.1857894
   Waddell ML, 2016, J EXP PSYCHOL HUMAN, V42, P363, DOI 10.1037/xhp0000151
   West B., 2015, LINEAR MIXED MODELS, V2nd
   Wheat HE, 2004, J NEUROSCI, V24, P3394, DOI 10.1523/JNEUROSCI.4822-03.2004
   WINTER B, 2013, LINEAR MODELS LINEAR, DOI DOI 10.48550/ARXIV.1308.5499
   Xu RH, 2003, STAT MED, V22, P3527, DOI 10.1002/sim.1572
   Yang XD, 2008, IEEE INT CONF ROBOT, P2061, DOI 10.1109/ROBOT.2008.4543510
NR 100
TC 3
Z9 4
U1 1
U2 12
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD AUG
PY 2018
VL 15
IS 3
AR 15
DI 10.1145/3182176
PG 20
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA GY2UV
UT WOS:000448400300001
DA 2024-07-18
ER

PT J
AU Kelly, JW
   Cherep, LA
   Siegel, ZD
AF Kelly, Jonathan W.
   Cherep, Lucia A.
   Siegel, Zachary D.
TI Perceived Space in the HTC Vive
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Virtual Environments; Experimentation; Depth perception; stereoscopic
   displays; virtual environments
ID DISTANCE PERCEPTION; VIRTUAL ENVIRONMENTS; EGOCENTRIC DISTANCE; VIEWING
   CONDITIONS; JUDGMENTS; RECALIBRATION; LOCOMOTION; GRAPHICS; QUALITY;
   WALKING
AB Underperception of egocentric distance in virtual reality has been a persistent concern for almost 20 years. Modern headmounted displays (HMDs) appear to have begun to ameliorate underperception. The current study examined several aspects of perceived space in the HTC Vive. Blind-walking distance judgments, verbal distance judgments, and size judgments were measured in two distinct virtual environments (VEs)-a high-quality replica of a real classroom and an empty grass field-as well as the real classroom upon which the classroom VE was modeled. A brief walking interaction was also examined as an intervention for improving anticipated underperception in the VEs. Results from the Vive were compared to existing data using two older HMDs (nVisor SX111 and ST50). Blind-walking judgments were more accurate in the Vive compared to the older displays, and did not differ substantially from the real world nor across VEs. Size judgments were more accurate in the classroom VE than the grass VE and in the Vive compared to the older displays. Verbal judgments were significantly smaller in the classroom VE compared to the real classroom and did not significantly differ across VEs. Blind-walking and size judgments were more accurate after walking interaction, but verbal judgments were unaffected. The results indicate that underperception of distance in the HTC Vive is less than in older displays but has not yet been completely resolved. With more accurate space perception afforded by modern HMDs, alternative methods for improving judgments of perceived space-such as walking interaction-may no longer be necessary.
C1 [Kelly, Jonathan W.; Cherep, Lucia A.; Siegel, Zachary D.] Iowa State Univ, Dept Psychol, W112 Lagomarcino Hall,901 Stange Rd, Ames, IA 50011 USA.
C3 Iowa State University
RP Kelly, JW (corresponding author), Iowa State Univ, Dept Psychol, W112 Lagomarcino Hall,901 Stange Rd, Ames, IA 50011 USA.
EM jonkelly@iastate.edu; lacherep@iastate.edu; zsiegel@iastate.edu
RI Kelly, Jonathan W/A-4793-2013
OI Siegel, Zachary/0000-0002-3700-8140; Cherep, Lucia/0000-0002-3054-6018
FU Seed Grant for Social Sciences from the Iowa State University College of
   Liberal Arts and Sciences
FX This research was supported by a Seed Grant for Social Sciences from the
   Iowa State University College of Liberal Arts and Sciences.
CR [Anonymous], P 6 S APPL PERC GRAP
   [Anonymous], 2005, ACM Transactions on Applied Perception, DOI [DOI 10.1145/1077399.1077403, DOI 10.1145/1077399.10774032,3,9]
   Badiqué E, 2002, HUM FAC ER, P1143
   Berg L. P., 2017, VIRTUAL REALITY
   Brenner E, 1999, VISION RES, V39, P975, DOI 10.1016/S0042-6989(98)00162-X
   Creem-Regehr SH, 2016, SAP 2015: ACM SIGGRAPH SYMPOSIUM ON APPLIED PERCEPTION, P47, DOI 10.1145/2804408.2804422
   Creem-Regehr SH, 2015, PSYCHOL LEARN MOTIV, V62, P195, DOI 10.1016/bs.plm.2014.09.006
   Creem-Regehr SH, 2005, PERCEPTION, V34, P191, DOI 10.1068/p5144
   EPSTEIN W, 1961, PSYCHOL BULL, V58, P491, DOI 10.1037/h0042260
   GOGEL WC, 1985, PERCEPT PSYCHOPHYS, V37, P17, DOI 10.3758/BF03207134
   Grechkin TY, 2010, ACM T APPL PERCEPT, V7, DOI 10.1145/1823738.1823744
   Hutchison JJ, 2006, SPAN J PSYCHOL, V9, P332, DOI 10.1017/S1138741600006235
   Interrante V., 2006, P IEEE VIRT REAL C
   Kelly JW, 2014, IEEE T VIS COMPUT GR, V20, P588, DOI 10.1109/TVCG.2014.36
   Kelly JW, 2013, ATTEN PERCEPT PSYCHO, V75, P1473, DOI 10.3758/s13414-013-0503-4
   Kelly JW, 2004, PERCEPTION, V33, P443, DOI 10.1068/p5218
   Knapp JM, 2004, PRESENCE-TELEOP VIRT, V13, P572, DOI 10.1162/1054746042545238
   Kunz BR, 2015, PERCEPTION, V44, P446, DOI 10.1068/p7929
   Kunz BR, 2009, ATTEN PERCEPT PSYCHO, V71, P1284, DOI 10.3758/APP.71.6.1284
   Li BC, 2016, SAP 2015: ACM SIGGRAPH SYMPOSIUM ON APPLIED PERCEPTION, P55, DOI 10.1145/2804408.2804427
   Li Bochao., 2014, P ACM S APPL PERCEPT, P91
   Loomis JM, 2003, VIRTUAL AND ADAPTIVE ENVIRONMENTS: APPLICATIONS, IMPLICATIONS, AND HUMAN PERFORMANCE ISSUES, P21
   Loomis JM, 1998, PERCEPT PSYCHOPHYS, V60, P966, DOI 10.3758/BF03211932
   Milner AD., 1995, The visual brain in action
   Mohler BettyJ., 2006, P 3 S APPL PERCEPTIO, P9, DOI [10.1145/1140491.1140493, DOI 10.1145/1140491.1140493]
   Murgia Alessio, 2009, International Journal of Virtual Reality, V8, P67
   Ooi TL, 2001, NATURE, V414, P197, DOI 10.1038/35102562
   Plumert Jodie., 2005, ACM Transactions on Applied Perception, V2, P216, DOI DOI 10.1145/1077399.1077402
   Renner RS, 2013, ACM COMPUT SURV, V46, DOI 10.1145/2543581.2543590
   Richardson AR, 2007, HUM FACTORS, V49, P507, DOI 10.1518/001872007X200139
   Richardson AR, 2005, APPL COGNITIVE PSYCH, V19, P1089, DOI 10.1002/acp.1140
   RIESER JJ, 1995, J EXP PSYCHOL HUMAN, V21, P480, DOI 10.1037/0096-1523.21.3.480
   Sahm C. S., 2005, ACM Trans. Appl. Percept. (TAP), V2, P35, DOI DOI 10.1145/1048687.1048690
   Sedgwick H.A., 1986, Handbook of Perception and Human Performance, p21
   Siegel Z. D., 2017, J EXPT PSYCHOL HUM P
   Siegel ZD, 2017, ATTEN PERCEPT PSYCHO, V79, P39, DOI 10.3758/s13414-016-1243-z
   Thompson WB, 2004, PRESENCE-TELEOP VIRT, V13, P560, DOI 10.1162/1054746042545292
   Waller D, 2008, J EXP PSYCHOL-APPL, V14, P61, DOI 10.1037/1076-898X.14.1.61
   Willemsen P, 2008, PRESENCE-TELEOP VIRT, V17, P91, DOI 10.1162/pres.17.1.91
   Willemsen P, 2009, ACM T APPL PERCEPT, V6, DOI 10.1145/1498700.1498702
   Winn W, 1999, PRESENCE-TELEOP VIRT, V8, P283, DOI 10.1162/105474699566233
   Witmer BG, 1998, HUM FACTORS, V40, P478, DOI 10.1518/001872098779591340
   Young M. K., 2014, P ACM S APPL PERCEPT, P83
NR 43
TC 101
Z9 110
U1 1
U2 22
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD NOV
PY 2017
VL 15
IS 1
AR 2
DI 10.1145/3106155
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA FU0CU
UT WOS:000423519800002
DA 2024-07-18
ER

PT J
AU Williams, D
   Kirke, A
   Miranda, E
   Daly, I
   Hwang, F
   Weaver, J
   Nasuto, S
AF Williams, Duncan
   Kirke, Alexis
   Miranda, Eduardo
   Daly, Ian
   Hwang, Faustina
   Weaver, James
   Nasuto, Slawomir
TI Affective Calibration of Musical Feature Sets in an Emotionally
   Intelligent Music Composition System
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Algorithmic composition; music perception; emotional congruence
ID AROUSAL; PERFORMANCE; VALENCE; TIME; FELT
AB Affectively driven algorithmic composition (AAC) is a rapidly growing field that exploits computer-aided composition in order to generate new music with particular emotional qualities or affective intentions. An AAC system was devised in order to generate a stimulus set covering nine discrete sectors of a two-dimensional emotion space by means of a 16-channel feed-forward artificial neural network. This system was used to generate a stimulus set of short pieces of music, which were rendered using a sampled piano timbre and evaluated by a group of experienced listeners who ascribed a two-dimensional valence-arousal coordinate to each stimulus. The underlying musical feature set, initially drawn from the literature, was subsequently adjusted by amplifying or attenuating the quantity of each feature in order to maximize the spread of stimuli in the valence-arousal space before a second listener evaluation was conducted. This process was repeated a third time in order to maximize the spread of valence-arousal coordinates ascribed to the generated stimulus set in comparison to a spread taken from an existing prerated database of stimuli, demonstrating that this prototype AAC system is capable of creating short sequences of music with a slight improvement on the range of emotion found in a stimulus set comprised of real-world, traditionally composed musical excerpts.
C1 [Williams, Duncan; Kirke, Alexis; Miranda, Eduardo] Plymouth Univ, Interdisciplinary Ctr Comp Mus Res, Plymouth PL4 8AA, Devon, England.
   [Daly, Ian; Hwang, Faustina; Weaver, James; Nasuto, Slawomir] Univ Reading, Brain Embodiment Lab, White Knights Rd, Reading RG6 6AH, Berks, England.
C3 University of Plymouth; University of Reading
RP Williams, D (corresponding author), Plymouth Univ, Interdisciplinary Ctr Comp Mus Res, Plymouth PL4 8AA, Devon, England.
EM duncan.williams@plymouth.ac.uk; alexis.kirke@plymouth.ac.uk;
   eduardo.miranda@plymouth.ac.uk; i.daly@plymouth.ac.uk;
   f.hwang@plymouth.ac.uk; j.e.weaver@plymouth.ac.uk;
   s.j.nasuto@reading.ac.uk
RI Miranda, Eduardo/KGL-5127-2024; Daly, Ian/Q-7322-2017
OI Daly, Ian/0000-0001-5489-0393; Kirke, Alexis/0000-0001-8783-6182;
   Miranda, Eduardo/0000-0002-8306-9585; Nasuto,
   Slawomir/0000-0001-9414-9049; Williams, Duncan/0000-0003-4793-8330
FU Engineering and Physical Sciences Research Council (EPSRC)
   [EP/J002135/1, EP/J003077/1]; EPSRC [EP/J003077/1, EP/J002135/1] Funding
   Source: UKRI
FX This work was supported by the Engineering and Physical Sciences
   Research Council (EPSRC), under grants EP/J002135/1 and EP/J003077/1 and
   develops initial work presented at the Ninth Triennial Conference of the
   European Society for the Cognitive Sciences of Music (ESCOM). Royal
   Northern College of Music, Manchester, UK, 17-22 August 2015.
CR [Anonymous], ACM T APPL PERCEPTIO, V14
   [Anonymous], 2008, P LANGTECH
   [Anonymous], ORGANIZED SOUND
   Bown O, 2006, LECT NOTES COMPUT SC, V3907, P652
   BRADLEY MM, 1994, J BEHAV THER EXP PSY, V25, P49, DOI 10.1016/0005-7916(94)90063-9
   Bresin R, 1998, J NEW MUSIC RES, V27, P239, DOI 10.1080/09298219808570748
   Bresin R, 2011, CORTEX, V47, P1068, DOI 10.1016/j.cortex.2011.05.009
   Brown E., 2004, CHI 04 HUM FACT COMP, P1297, DOI DOI 10.1145/985921.986048
   CARPENTER GA, 1992, IEEE COMMUN MAG, V30, P38, DOI 10.1109/35.156802
   Casey M., 2001, Organised Sound, V6, P153, DOI DOI 10.1017/S1355771801002126
   Dahlstedt P., 2001, ORGAN SOUND, V6, P121
   den Brinker B, 2012, EURASIP J AUDIO SPEE, DOI 10.1186/1687-4722-2012-24
   Eaton J., 2014, P JOINT ICMC SMC 201
   Eerola T, 2011, PSYCHOL MUSIC, V39, P18, DOI 10.1177/0305735610362821
   Egermann H, 2013, MUSIC PERCEPT, V31, P139, DOI 10.1525/MP.2013.31.2.139
   Eigenfeldt A, 2011, ORGAN SOUND, V16, P145, DOI 10.1017/S1355771811000094
   Gabrielsson A., 2001, Music And Emotion, P223, DOI DOI 10.1525/MP.2004.21.4.561
   Gabrielsson A, 2001, MUSIC SCI, V5, P123, DOI 10.1177/10298649020050S105
   Gomez P, 2007, EMOTION, V7, P377, DOI 10.1037/1528-3542.7.2.377
   Grewe O, 2005, ANN NY ACAD SCI, V1060, P446, DOI 10.1196/annals.1360.041
   Grewe O, 2007, EMOTION, V7, P774, DOI 10.1037/1528-3542.7.4.774
   Ilie G, 2006, MUSIC PERCEPT, V23, P319, DOI 10.1525/mp.2006.23.4.319
   Javela JJ, 2008, PSYCHOL REP, V103, P663, DOI 10.2466/PR0.103.3.663-681
   Jefferies LN, 2008, PSYCHOL SCI, V19, P290, DOI 10.1111/j.1467-9280.2008.02082.x
   Kallinen K, 2006, MUSIC SCI, V10, P191, DOI 10.1177/102986490601000203
   Kirke A, 2009, ACM COMPUT SURV, V42, DOI 10.1145/1592451.1592454
   Marin MM, 2010, EDUC COMPET GLOB WOR, P1
   Mattek A., 2011, P 26 ANN C SOC EL MU
   Mehrabian A, 1996, CURR PSYCHOL, V14, P261, DOI 10.1007/BF02686918
   Nielzen S., 1982, PSYCHOL MUSIC, V10, P17
   Rentfrow PJ, 2003, J PERS SOC PSYCHOL, V84, P1236, DOI 10.1037/0022-3514.84.6.1236
   Rowe Robert., 1992, INTERACTIVE MUSIC SY
   RUSSELL JA, 1980, J PERS SOC PSYCHOL, V39, P1161, DOI 10.1037/h0077714
   Scherer KR, 2004, J NEW MUSIC RES, V33, P239, DOI 10.1080/0929821042000317822
   Shapiro S, 2002, J ADVERTISING, V31, P15, DOI 10.1080/00913367.2002.10673682
   Visell Y., 2004, ORG SOUND, V9, P151
   Vuoskoski JK, 2012, PSYCHOL AESTHET CREA, V6, P204, DOI 10.1037/a0026937
   Vuoskoski JK, 2012, MUSIC PERCEPT, V29, P311, DOI 10.1525/MP.2012.29.3.311
   Williamon A, 2002, MUSIC SCI, V6, P53, DOI 10.1177/102986490200600103
   Williams D., 2014, P 40 INT COMP MUS C
   Williams D, 2015, PSYCHOL MUSIC, V43, P831, DOI 10.1177/0305735614543282
   Williams D, 2015, ACM T APPL PERCEPT, V12, DOI 10.1145/2749466
NR 42
TC 14
Z9 17
U1 1
U2 10
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD JUL
PY 2017
VL 14
IS 3
AR 17
DI 10.1145/3059005
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA FB9AL
UT WOS:000406431900004
OA Green Submitted, Green Accepted
DA 2024-07-18
ER

PT J
AU Komogortsev, O
   Holland, C
   Karpov, A
   Price, LR
AF Komogortsev, Oleg
   Holland, Corey
   Karpov, Alex
   Price, Larry R.
TI Biometrics via Oculomotor Plant Characteristics: Impact of Parameters in
   Oculomotor Plant Model
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Biometrics; Human oculomotor system; biological system modeling;
   mathematical model; security and protection
ID EYE-MOVEMENTS; FACE RECOGNITION; IDENTIFICATION
AB This article proposes and evaluates a novel biometric approach utilizing the internal, nonvisible, anatomical structure of the human eye. The proposed method estimates the anatomical properties of the human oculomotor plant from the measurable properties of human eye movements, utilizing a two-dimensional linear homeomorphic model of the oculomotor plant. The derived properties are evaluated within a biometric framework to determine their efficacy in both verification and identification scenarios. The results suggest that the physical properties derived from the oculomotor plant model are capable of achieving 20.3% equal error rate and 65.7% rank-1 identification rate on high-resolution equipment involving 32 subjects, with biometric samples taken over four recording sessions; or 22.2% equal error rate and 12.6% rank-1 identification rate on low-resolution equipment involving 172 subjects, with biometric samples taken over two recording sessions.
C1 [Komogortsev, Oleg; Holland, Corey; Karpov, Alex] SW Texas State Univ, Dept Comp Sci, San Marcos, TX 78666 USA.
   [Price, Larry R.] SW Texas State Univ, San Marcos, TX 78666 USA.
C3 Texas State University System; Texas State University San Marcos; Texas
   State University System; Texas State University San Marcos
RP Komogortsev, O (corresponding author), SW Texas State Univ, Dept Comp Sci, San Marcos, TX 78666 USA.
EM ok11@txstate.edu; ch1570@txstate.edu; ak26@txstate.edu; lp11@txstate.edu
RI Price, Larry/AAE-4089-2020; Karpov, Alexey/E-6199-2013
OI Karpov, Alexey/0000-0001-8563-7243
FU Texas State University; NSF CAREER Grant [CNS-1250718]; NSF GRFP Grant
   [DGE-11444666]; NIST Grant [60NANB12D234]
FX Special gratitude is expressed to Katie Holland for her aid with
   technical illustrations. This work is supported in part by Texas State
   University, NSF CAREER Grant #CNS-1250718, NSF GRFP Grant #DGE-11444666,
   and NIST Grant #60NANB12D234.
CR Abate AF, 2007, PATTERN RECOGN LETT, V28, P1885, DOI 10.1016/j.patrec.2006.12.018
   [Anonymous], P IEEE IARP INT C BI
   [Anonymous], 2007, HDB BIOMETRICS HDB B
   BAHILL AT, 1980, CRC CR REV BIOM ENG, V4, P311
   Bartlett MS, 1937, PROC R SOC LON SER-A, V160, P0268, DOI 10.1098/rspa.1937.0109
   Bednarik R, 2005, LECT NOTES COMPUT SC, V3540, P780
   Breiman L., 2001, Machine Learning, V45, P5, DOI 10.1023/A:1010933404324
   CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1007/BF00994018
   Crihalmeanu S, 2012, PATTERN RECOGN LETT, V33, P1860, DOI 10.1016/j.patrec.2011.11.006
   Daugman J, 2004, IEEE T CIRC SYST VID, V14, P21, DOI 10.1109/TCSVT.2003.818350
   Duchowski A. T., 2017, EYE TRACKING METHODO
   Hair J.F., 1998, Multivariate Data Analysis, P5
   HARMON LD, 1981, PATTERN RECOGN, V13, P97, DOI 10.1016/0031-3203(81)90008-X
   Holland C. D., 2012, 5 INT C BIOM THEOR A, P1
   Holland Corey, 2011, 2011 INT JOINT C BIO, P1, DOI [DOI 10.1109/IJCB.2011.6117536, 10.1109/IJCB.2011.6117536.]
   Hotelling H, 1931, ANN MATH STAT, V2, P360, DOI 10.1214/aoms/1177732979
   Jain A, 2000, COMMUN ACM, V43, P90, DOI 10.1145/328236.328110
   Jain A.K., 1999, 2 INT C AUDIO VISUAL, P182
   Joachims T., 2008, SVM Light-Support Vector Machine
   Kasprowski P, 2004, LECT NOTES COMPUT SC, V3087, P248
   Komarinski P., 2004, AUTOMATED FINGERPRIN
   Komogortsev O., 2008, BIBE, P1
   Komogortsev O. V., 2012, TECHNICAL REPORT
   Komogortsev O. V., 2012, EYE MOVEMENT BIOMETR
   Komogortsev O. V., 2012, IEEE 5 INT C BIOM TH, P1
   Komogortsev OV, 2013, INT CONF BIOMETR
   Komogortsev OV, 2010, IEEE T BIO-MED ENG, V57, P2635, DOI 10.1109/TBME.2010.2057429
   Komogortsev OV, 2008, PROCEEDINGS OF THE EYE TRACKING RESEARCH AND APPLICATIONS SYMPOSIUM (ETRA 2008), P229, DOI 10.1145/1344471.1344525
   Lagarias JC, 1998, SIAM J OPTIMIZ, V9, P112, DOI 10.1137/S1052623496303470
   Leigh RJ., 2006, NEUROLOGY EYE MOVEME, V4th
   Nandakumar K, 2008, IEEE T PATTERN ANAL, V30, P342, DOI 10.1109/TPAMI.2007.70796
   NOTON D, 1971, SCIENCE, V171, P308, DOI 10.1126/science.171.3968.308
   Park U, 2009, 2009 IEEE 3RD INTERNATIONAL CONFERENCE ON BIOMETRICS: THEORY, APPLICATIONS AND SYSTEMS, P153
   Quaia Christian, 2003, Strabismus, V11, P17, DOI 10.1076/stra.11.1.17.14088
   Rayner K, 1998, PSYCHOL BULL, V124, P372, DOI 10.1037/0033-2909.124.3.372
   Rigas I, 2012, PATTERN RECOGN LETT, V33, P786, DOI 10.1016/j.patrec.2012.01.003
   Salvucci D.D., 2000, P 2000 S EYE TRACK R, P71, DOI [DOI 10.1145/355017.355028, 10.1145/355017.355028]
   San Agustin J., 2009, Proceedings of the 27th international conference extended abstracts on Human factors in computing systems - CHI EA '09, P4453
   Schnitzer BS, 2006, VISION RES, V46, P1611, DOI 10.1016/j.visres.2005.09.023
   Silver D.L., 2006, P IC, P344
   Tabachnick B.G., 2001, Using Multivariate Statistics
   Vapnik V., 1999, NATURE STAT LEARNING
   Wilkie D. R., 1970, MUSCLE
   Willems JanC., 2002, AICHE SYM SER, P97
   Wiskott L, 1997, IEEE T PATTERN ANAL, V19, P775, DOI 10.1109/34.598235
NR 45
TC 2
Z9 2
U1 0
U2 2
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD JAN
PY 2015
VL 11
IS 4
AR 20
DI 10.1145/2668891
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AZ6WJ
UT WOS:000348358400005
DA 2024-07-18
ER

PT J
AU Zhan, C
   Li, WQ
   Ogunbona, P
AF Zhan, Ce
   Li, Wanqing
   Ogunbona, Philip
TI Measuring the Degree of Face Familiarity Based on Extended NMF
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Algorithms; Design; Human Factors; NMF; face familiarity; face analysis;
   face recognition
ID NONNEGATIVE MATRIX FACTORIZATION; RECOGNITION; ALGORITHMS; PARTS
AB Getting familiar with a face is an important cognitive process in human perception of faces, but little study has been reported on how to objectively measure the degree of familiarity. In this article, a method is proposed to quantitatively measure the familiarity of a face with respect to a set of reference faces that have been seen previously. The proposed method models the context-free and context-dependent forms of familiarity suggested by psychological studies and accounts for the key factors, namely exposure frequency, exposure intensity and similar exposure, that affect human perception of face familiarity. Specifically, the method divides the reference set into nonexclusive groups and measures the familiarity of a given face by aggregating the similarities of the face to the individual groups. In addition, the nonnegative matrix factorization (NMF) is extended in this paper to learn a compact and localized subspace representation for measuring the similarities of the face with respect to the individual groups. The proposed method has been evaluated through experiments that follow the protocols commonly used in psychological studies and has been compared with subjective evaluation. Results have shown that the proposed measurement is highly consistent with the subjective judgment of face familiarity. Moreover, a face recognition method is devised using the concept of face familiarity and the results on the standard FERET evaluation protocols have further verified the efficacy of the proposed familiarity measurement.
C1 [Zhan, Ce; Li, Wanqing; Ogunbona, Philip] Univ Wollongong, Sch Comp Sci & Software Engn, Wollongong, NSW 2522, Australia.
C3 University of Wollongong
RP Li, WQ (corresponding author), Univ Wollongong, Sch Comp Sci & Software Engn, Wollongong, NSW 2522, Australia.
EM wanqing@uow.edu.au
RI Li, Wanqing/ABG-2620-2020
OI Li, Wanqing/0000-0002-4427-2687; Ogunbona, Philip O./0000-0003-4119-2873
CR Ahonen T, 2006, IEEE T PATTERN ANAL, V28, P2037, DOI 10.1109/TPAMI.2006.244
   Bekios-Calfa J, 2011, IEEE T PATTERN ANAL, V33, P858, DOI 10.1109/TPAMI.2010.208
   Belhumeur PN, 1997, IEEE T PATTERN ANAL, V19, P711, DOI 10.1109/34.598228
   Bertsekas D. P., 1999, Nonlinear Program, V2nd
   BERTSEKAS DP, 1976, IEEE T AUTOMAT CONTR, V21, P174, DOI 10.1109/TAC.1976.1101194
   Bicego M, 2008, ACM T APPL PERCEPT, V5, DOI 10.1145/1279920.1279925
   Bruyer R, 2007, ACTA PSYCHOL, V124, P159, DOI 10.1016/j.actpsy.2006.03.001
   Cai D, 2011, IEEE T PATTERN ANAL, V33, P1548, DOI 10.1109/TPAMI.2010.231
   Fasel B, 2003, PATTERN RECOGN, V36, P259, DOI 10.1016/S0031-3203(02)00052-3
   Frischholz RW, 2000, COMPUTER, V33, P64, DOI 10.1109/2.820041
   Fu Y, 2010, IEEE T PATTERN ANAL, V32, P1955, DOI 10.1109/TPAMI.2010.36
   Geng X, 2007, IEEE T PATTERN ANAL, V29, P2234, DOI 10.1109/TPAMI.2007.70733
   Gu WF, 2012, PATTERN RECOGN, V45, P80, DOI 10.1016/j.patcog.2011.05.006
   Guan NY, 2011, IEEE T IMAGE PROCESS, V20, P2030, DOI 10.1109/TIP.2011.2105496
   Hoyer PO, 2004, J MACH LEARN RES, V5, P1457
   Huynh-Thu Q, 2008, ELECTRON LETT, V44, P800, DOI 10.1049/el:20080522
   Kendall M., 1990, Correlation methods
   Lee DD, 1999, NATURE, V401, P788, DOI 10.1038/44565
   Lee DD, 2001, ADV NEUR IN, V13, P556
   Li S. Z., 2004, HDB FACE RECOGNITION
   Li SZ, 2001, PROC CVPR IEEE, P207
   Lin CJ, 2007, IEEE T NEURAL NETWOR, V18, P1589, DOI 10.1109/TNN.2007.895831
   Lin CJ, 2007, NEURAL COMPUT, V19, P2756, DOI 10.1162/neco.2007.19.10.2756
   Lin CJ, 1999, SIAM J OPTIMIZ, V9, P1100, DOI 10.1137/S1052623498345075
   Liu XB, 2010, IEEE T IMAGE PROCESS, V19, P1126, DOI 10.1109/TIP.2009.2039050
   Mäkinen E, 2008, IEEE T PATTERN ANAL, V30, P541, DOI 10.1109/TPAMI.2007.70800
   MANDLER G, 1980, PSYCHOL REV, V87, P252, DOI 10.1037/0033-295X.87.3.252
   Martinez A., 1998, AR FACE DATABASE
   Naseem I, 2010, IEEE T PATTERN ANAL, V32, P2106, DOI 10.1109/TPAMI.2010.128
   PAATERO P, 1994, ENVIRONMETRICS, V5, P111, DOI 10.1002/env.3170050203
   Pantic M, 2000, IEEE T PATTERN ANAL, V22, P1424, DOI 10.1109/34.895976
   Phillips PJ, 1998, IMAGE VISION COMPUT, V16, P295, DOI 10.1016/S0262-8856(97)00070-X
   Rakover S., 2001, Face Recognition: Cognitive and Computational Processes
   Rossion B, 2002, VIS COGN, V9, P1003, DOI 10.1080/13506280143000485
   Samaria F. S., 1994, Proceedings of the Second IEEE Workshop on Applications of Computer Vision (Cat. No.94TH06742), P138, DOI 10.1109/ACV.1994.341300
   Schwaninger A, 2009, COGNITIVE SCI, V33, P1413, DOI 10.1111/j.1551-6709.2009.01059.x
   Sinha P, 2006, P IEEE, V94, P1948, DOI 10.1109/JPROC.2006.884093
   SOLSO RL, 1981, BRIT J PSYCHOL, V72, P499, DOI 10.1111/j.2044-8295.1981.tb01779.x
   STEIN S., 2011, P FG 2011
   Tistarelli M, 2009, IMAGE VISION COMPUT, V27, P222, DOI 10.1016/j.imavis.2007.05.006
   TURK M, 1991, J COGNITIVE NEUROSCI, V3, P71, DOI 10.1162/jocn.1991.3.1.71
   VALENTINE T, 1991, Q J EXP PSYCHOL-A, V43, P161, DOI 10.1080/14640749108400966
   Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb
   Wallis G, 2008, J VISION, V8, DOI 10.1167/8.3.20
   Wang CH, 2009, PROC CVPR IEEE, P389, DOI 10.1109/CVPRW.2009.5206865
   Wiskott L, 1997, IEEE T PATTERN ANAL, V19, P775, DOI 10.1109/34.598235
   Yang J., 2008, IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), P1
   YONELINAS A. P., 2002, J MEMORY LANG, V46
   Zafeiriou S, 2006, IEEE T NEURAL NETWOR, V17, P683, DOI 10.1109/TNN.2006.873291
   Zhan C, 2007, LECT NOTES COMPUT SC, V4810, P88
   Zhang TP, 2008, IEEE T IMAGE PROCESS, V17, P574, DOI 10.1109/TIP.2008.918957
   Zhao W, 2003, ACM COMPUT SURV, V35, P399, DOI 10.1145/954339.954342
   Zhi RC, 2011, IEEE T SYST MAN CY B, V41, P38, DOI 10.1109/TSMCB.2010.2044788
NR 53
TC 7
Z9 7
U1 1
U2 13
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD MAY
PY 2013
VL 10
IS 2
AR 8
DI 10.1145/2465780.2465782
PG 22
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA 214EV
UT WOS:000324114800002
DA 2024-07-18
ER

PT J
AU Gamper, H
   Dicke, C
   Billinghurst, M
   Puolamäki, K
AF Gamper, Hannes
   Dicke, Christina
   Billinghurst, Mark
   Puolamaki, Kai
TI Sound Sample Detection and Numerosity Estimation Using Auditory Display
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Experimentation; Human Factors; Spatial sound; diotic headphone
   playback; speech synthesis; earcons; SOA
ID PSYCHOMETRIC FUNCTION; ENERGETIC MASKING; TASK
AB This article investigates the effect of various design parameters of auditory information display on user performance in two basic information retrieval tasks. We conducted a user test with 22 participants in which sets of sound samples were presented. In the first task, the test participants were asked to detect a given sample among a set of samples. In the second task, the test participants were asked to estimate the relative number of instances of a given sample in two sets of samples. We found that the stimulus onset asynchrony (SOA) of the sound samples had a significant effect on user performance in both tasks. For the sample detection task, the average error rate was about 10% with an SOA of 100 ms. For the numerosity estimation task, an SOA of at least 200 ms was necessary to yield average error rates lower than 30%. Other parameters, including the samples' sound type (synthesized speech or earcons) and spatial quality (multichannel loudspeaker or diotic headphone playback), had no substantial effect on user performance. These results suggest that diotic, or indeed monophonic, playback with appropriately chosen SOA may be sufficient in practical applications for users to perform the given information retrieval tasks, if information about the sample location is not relevant. If location information was provided through spatial playback of the samples, test subjects were able to simultaneously detect and localize a sample with reasonable accuracy.
C1 [Gamper, Hannes; Puolamaki, Kai] Aalto Univ, FI-00076 Aalto, Finland.
   [Dicke, Christina; Billinghurst, Mark] Univ Canterbury, Canterbury, New Zealand.
C3 Aalto University; University of Canterbury
RP Gamper, H (corresponding author), Aalto Univ, Dept Media Technol, Sch Sci, FI-00076 Aalto, Finland.
EM hannes.gamper@aalto.fi
RI Puolamäki, Kai/C-9016-2017; Billinghurst, Mark/AAJ-4236-2020
OI Puolamäki, Kai/0000-0003-1819-1047; Billinghurst,
   Mark/0000-0003-4172-6759
FU Helsinki Graduate School in Computer Science and Engineering (HeCSE);
   [MIDE program] of Aalto University; Nokia Research Foundation; European
   Research Council under the European Community's Seventh Framework
   Program (FP7)/ERC grant [203636]; Tekniikan edistamissaation(TES);
   Finnish Center of Excellence for Algorithmic Data Analysis Research
   (ALGODAN)
FX This work was supported by the Helsinki Graduate School in Computer
   Science and Engineering (HeCSE), the [MIDE program] of Aalto University,
   the Nokia Research Foundation, the European Research Council under the
   European Community's Seventh Framework Program (FP7/2007-2013)/ERC grant
   agreement [203636], Tekniikan edistamissaation(TES), and the Finnish
   Center of Excellence for Algorithmic Data Analysis Research (ALGODAN).
CR [Anonymous], 1999, Auditory Scene Analysis: The Perceptual Organization of Sound, DOI DOI 10.7551/MITPRESS/1486.001.0001
   [Anonymous], 1996, ACM Transactions, DOI DOI 10.1145/230562.230563
   [Anonymous], 2013, ACM T APPL PERCEPTIO, V10
   Blattner M. M., 1989, Human-Computer Interaction, V4, P11, DOI 10.1207/s15327051hci0401_1
   Bonebright TL, 2009, APPL COGNITIVE PSYCH, V23, P431, DOI 10.1002/acp.1457
   Brewster S. A., 1995, P BCS HCI, P155
   BREWSTER S. A., 1995, P BCS HCI
   Brewster S, 2002, PERS UBIQUIT COMPUT, V6, P188, DOI 10.1007/s007790200019
   Bronkhorst AW, 2000, ACUSTICA, V86, P117
   Brown Lorna M., 2002, P BCS HCI C, P6
   Brungart D. S., 2002, P 8 INT C AUD DISPL
   Brungart DS, 2001, J ACOUST SOC AM, V110, P2527, DOI 10.1121/1.1408946
   Brungart DS, 2002, J ACOUST SOC AM, V112, P664, DOI 10.1121/1.1490592
   CHERRY EC, 1953, J ACOUST SOC AM, V25, P975, DOI 10.1121/1.1907229
   Darwin CJ, 2000, J ACOUST SOC AM, V107, P970, DOI 10.1121/1.428278
   Dingler Tilman, 2008, P 14 INT C AUD DISPL, P1
   Dudoit S, 2003, STAT SCI, V18, P71, DOI 10.1214/ss/1056397487
   Garzonis S, 2009, CHI2009: PROCEEDINGS OF THE 27TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1-4, P1513
   Gaver William W, 1986, Human-computer interaction, V2, P167, DOI [10.1207/s15327051hci0202_3, DOI 10.1207/S15327051HCI0202_3]
   HOLM S, 1979, SCAND J STAT, V6, P65
   Hornof AJ, 2010, CHI2010: PROCEEDINGS OF THE 28TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1-4, P2103
   Ihiefeld A, 2008, J ACOUST SOC AM, V123, P4369, DOI 10.1121/1.2904826
   Ihlefeld A, 2008, J ACOUST SOC AM, V123, P4380, DOI 10.1121/1.2904825
   JULESZ B, 1987, READINGS COMPUTER VI, P243
   Karshmer A. I., 1994, P 1 ANN ACM C ASS TE, P123
   Kidd G, 2005, J ACOUST SOC AM, V118, P3804, DOI 10.1121/1.2109187
   Kidd G, 2010, J ACOUST SOC AM, V128, P1965, DOI 10.1121/1.3478781
   Klein SA, 2001, PERCEPT PSYCHOPHYS, V63, P1421, DOI 10.3758/BF03194552
   Larsen E, 2008, J ACOUST SOC AM, V124, P450, DOI 10.1121/1.2936368
   McGookin D., 2004, P 10 INT C AUD DISPL
   McGookin D.K., 2004, ACM Transactions on Applied Perception (TAP), V1, P130, DOI DOI 10.1145/1024083.1024087
   Michalski R, 2008, INT J IND ERGONOM, V38, P321, DOI 10.1016/j.ergon.2007.11.002
   Nees M.A., 2009, UNIVERSAL ACCESS HDB, P507
   Peres SC, 2008, MORG KAUF SER INTER, P147, DOI 10.1016/B978-0-12-374017-5.00005-5
   Ramloll R, 2001, BCS CONF SERIES, P515
   SAGI D, 1985, SCIENCE, V228, P1217, DOI 10.1126/science.4001937
   Sawhney N., 2000, ACM Transactions on Computer-Human Interaction, V7, P353, DOI 10.1145/355324.355327
   Sheskin D. J., 2000, Handbook of Parametric and Nonparametric Statistical Procedures
   Shinn-Cunningham B., 1997, BINAURAL SPATIAL HEA, P611
   Therneau TerryM., 2011, RPART RECURSIVE PART
   Tran TV, 2000, ERGONOMICS, V43, P807, DOI 10.1080/001401300404760
   TREISMAN A, 1986, 2 WORKSH HUM MACH VI, P313
   TREISMAN A, 1986, 2 WORKSH HUM MACH VI, V3, P313
   Treutwein B, 1999, PERCEPT PSYCHOPHYS, V61, P87, DOI 10.3758/BF03211951
   Vargas M.L. M., 2003, Proceedings of the 2003 International Conference on Auditory Display, P38
   Walker BruceN., 2006, SPEARCONS SPEECH BAS
   Wichmann FA, 2001, PERCEPT PSYCHOPHYS, V63, P1293, DOI 10.3758/BF03194544
   [No title captured]
NR 48
TC 1
Z9 1
U1 0
U2 5
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD FEB
PY 2013
VL 10
IS 1
AR 4
DI 10.1145/2422105.2422109
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AN8YO
UT WOS:000340892200004
DA 2024-07-18
ER

PT J
AU Merer, A
   Aramaki, M
   Ystad, S
   Kronland-Martinet, R
AF Merer, Adrien
   Aramaki, Mitsuko
   Ystad, Solvi
   Kronland-Martinet, Richard
TI Perceptual Characterization of Motion Evoked by Sounds for Synthesis
   Control Purposes
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Design; Experimentation; Performance; Theory; Description; motion;
   perception; trajectories; synthesis control; mapping; sound perception;
   drawing
ID AUDITORY-PERCEPTION; MUSIC; DISCRIMINATION; PERFORMANCE; FEEDBACK;
   MODEL; MOVES
AB This article addresses the question of synthesis and control of sound attributes from a perceptual point of view. We focused on an attribute related to the general concept of motion evoked by sounds. To investigate this concept, we tested 40 monophonic abstract sounds on listeners via a questionnaire and drawings, using a parametrized custom interface. This original procedure, which was defined with synthesis and control perspectives in mind, provides an alternative means of determining intuitive control parameters for synthesizing sounds evoking motion. Results showed that three main shape categories (linear, with regular oscillations, and with circular oscillations) and three types of direction (rising, descending, and horizontal) were distinguished by the listeners. In addition, the subjects were able to perceive the low-frequency oscillations (below 8 Hz) quite accurately. Three size categories (small, medium, and large) and three levels of randomness (none, low amplitude irregularities, and high amplitude irregularities) and speed (constant speed and speeds showing medium and large variations) were also observed in our analyses of the participants' drawings. We further performed a perceptual test to confirm the relevance of the contribution of some variables with synthesized sounds combined with visual trajectories. Based on these results, a general typology of evoked motion was drawn up and an intuitive control strategy was designed, based on a symbolic representation of continuous trajectories (provided by devices such as motion capture systems, pen tablets, etc.). These generic tools could be used in a wide range of applications such as sound design, virtual reality, sonification, and music.
C1 [Merer, Adrien; Aramaki, Mitsuko; Ystad, Solvi; Kronland-Martinet, Richard] Aix Marseille Univ, CNRS UPR 7051, LMA, Marseille, France.
C3 Centre National de la Recherche Scientifique (CNRS); CNRS - Institute
   for Engineering & Systems Sciences (INSIS); Aix-Marseille Universite
RP Merer, A (corresponding author), 31 Chemin Joseph Aiguier, F-13402 Marseille 20, France.
EM merer@lma.cnrs-mrs.fr
RI Ystad, Sølvi/AAN-4517-2020; Kronland-Martinet, Richard/M-3095-2016
OI Ystad, Sølvi/0000-0001-9022-9690; Kronland-Martinet,
   Richard/0000-0002-7325-4920
CR Alais D, 2004, COGNITIVE BRAIN RES, V19, P185, DOI 10.1016/j.cogbrainres.2003.11.011
   Andersen TH, 2010, ACM T APPL PERCEPT, V7, DOI 10.1145/1773965.1773968
   [Anonymous], P INT COMP MUS C ICM
   [Anonymous], 2016, Pattern Recognition and Machine Learning, Softcover Reprint of the Original 1st ed., Information Science and Statistics, DOI DOI 10.1117/1.2819119
   Aramaki M, 2011, IEEE T AUDIO SPEECH, V19, P301, DOI 10.1109/TASL.2010.2047755
   Arrighi R, 2006, J VISION, V6, P260, DOI 10.1167/6.3.6
   Bevilacqua F., 2005, P C NEW INTERFACES M, P85
   Carlile S, 2002, J ACOUST SOC AM, V111, P1026, DOI 10.1121/1.1436067
   Chen Y., 2008, EXPERIENTIAL MEDIA S
   CHOWNING JM, 1971, J AUDIO ENG SOC, V19, P2
   DACK J., 1999, P ARTS S INT C ADV S
   DIPELLEGRINO G, 1992, EXP BRAIN RES, V91, P176, DOI 10.1007/BF00230027
   DIXON NF, 1980, PERCEPTION, V9, P719, DOI 10.1068/p090719
   Eitan Z, 2006, MUSIC PERCEPT, V23, P221, DOI 10.1525/mp.2006.23.3.221
   Fremiot M., 1996, LES UNITES SEMIOTIQU
   Friberg A, 1999, J ACOUST SOC AM, V105, P1469, DOI 10.1121/1.426687
   GAVER WW, 1993, ECOL PSYCHOL, V5, P1, DOI 10.1207/s15326969eco0501_1
   Glasberg BR, 2002, J AUDIO ENG SOC, V50, P331
   Gounaropoulos A, 2006, LECT NOTES COMPUT SC, V3907, P664
   Honing H, 2003, COMPUT MUSIC J, V27, P66, DOI 10.1162/014892603322482538
   Johnson ML, 2003, METAPHOR SYMBOL, V18, P63, DOI 10.1207/S15327868MS1802_1
   Jot J.-M., 1991, P 90 AUD ENG SOC CON
   Kaczmarek T, 2005, J ACOUST SOC AM, V117, P3149, DOI 10.1121/1.1880832
   Kronman U., 1987, Action and perception in rhythm and music, V55, P57
   Larsson P, 2010, LECT NOTES COMPUT SC, V5954, P1
   Le Groux S., 2008, P IEEE INT C AC SPEE
   LIBERMAN AM, 1985, COGNITION, V21, P1, DOI 10.1016/0010-0277(85)90021-6
   Lutfi RA, 1999, J ACOUST SOC AM, V106, P919, DOI 10.1121/1.428033
   Malloch J, 2008, LECT NOTES COMPUT SC, V4969, P401
   McDermott J., 2008, ART ARTIFICIAL EVOLU, P81, DOI [10.1007/978-3-540-72877-1_4, DOI 10.1007/978-3-540-72877-1_4]
   Merer A., 2010, P 7 INT S COMP MUS M, P207
   Merer A, 2008, LECT NOTES COMPUT SC, V4969, P139
   Miranda ER, 1997, LEONARDO MUSIC J, V7, P49, DOI 10.2307/1513245
   Olivero A, 2010, EUR SIGNAL PR CONF, P507
   Riecke BE, 2009, ACM T APPL PERCEPT, V6, DOI 10.1145/1577755.1577763
   Schaeffer P., 1966, Traite des objets musicaux
   Schaffert N, 2010, LECT NOTES COMPUT SC, V5954, P143
   Schön D, 2010, J COGNITIVE NEUROSCI, V22, P1026, DOI 10.1162/jocn.2009.21302
   Shove Patrick., 1995, The Practice of Performance: Studies in Musical Interpretation, P55, DOI DOI 10.1017/CBO9780511552366.004
   Steiner Hans-Christoph., 2006, Proceedings of the Conference on New Interfaces for Musical Expression. NIME'06, P106
   SUMMERFIELD Q, 1984, Q J EXP PSYCHOL-A, V36, P51, DOI 10.1080/14640748408401503
   Thiebaut J.-B., 2008, P INT COMP MUS C ICM
   Vogt K, 2010, LECT NOTES COMPUT SC, V5954, P103
   WARREN WH, 1984, J EXP PSYCHOL HUMAN, V10, P704, DOI 10.1037/0096-1523.10.5.704
   Zwicker E., 2013, Psychoacoustics: Facts and Models
NR 45
TC 18
Z9 18
U1 0
U2 6
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD FEB
PY 2013
VL 10
IS 1
AR 1
DI 10.1145/2422105.2422106
PG 24
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA AN8YO
UT WOS:000340892200001
DA 2024-07-18
ER

PT J
AU Oulasvirta, A
   Nurminen, A
   Suomalainen, T
AF Oulasvirta, Antti
   Nurminen, Antti
   Suomalainen, Tiia
TI How Real Is Real Enough? Optimal Reality Sampling for Fast Recognition
   of Mobile Imagery
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Experimentation; Human Factors; Mobile imagery; recognition
ID MODEL
AB We present the first study to discover optimal reality sampling for mobile imagery. In particular, we identify the minimum information required for fast recognition of images of directly perceivable real-world buildings displayed on a mobile device. Resolution, image size, and JPEG compression of images of facades were manipulated in a same-different recognition task carried out in the field. Best-effort performance is shown to be reachable with significantly lower detail granularity than previously thought. For best user performance, we recommend presenting images as large as possible on the screen and decreasing resolution accordingly.
C1 [Nurminen, Antti; Suomalainen, Tiia] Aalto Univ, Helsinki Inst Informat Technol HIIT, Helsinki, Finland.
C3 University of Helsinki; Aalto University
EM antti.oulasvirta@mpii.de
RI Oulasvirta, Antti/G-8066-2011
OI Oulasvirta, Antti/0000-0002-2498-7837
FU Academy of Finland project Ganzheit; Academy of Finland project 3DWIKI;
   Max Planck Center for Visual Computing and Communication
FX The Academy of Finland projects Ganzheit and 3DWIKI have funded this
   work. A. Oulasvirta was supported by the Max Planck Center for Visual
   Computing and Communication.
CR [Anonymous], 2005, P SIGCHI C HUMAN FAC, DOI DOI 10.1145/1054972.1055101
   BRUCE V., 2003, VISUAL PERCEPTION PH
   Cater K., 2003, Eurographics Symposium on Rendering. 14th Eurographics Workshop on Rendering, P270
   Cohen J., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P115, DOI 10.1145/280814.280832
   FERGUS R., 2007, 024 CSAIL MIT
   Fröhlich P, 2011, COMMUN ACM, V54, P132, DOI 10.1145/1866739.1866766
   Greenberg DP, 1999, COMMUN ACM, V42, P44, DOI 10.1145/310930.310970
   Gulliver SR, 2006, ACM T MULTIM COMPUT, V2, P241, DOI 10.1145/1201730.1201731
   Guo CL, 2010, IEEE T IMAGE PROCESS, V19, P185, DOI 10.1109/TIP.2009.2030969
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   JAYANT N, 1993, P IEEE, V81, P1385, DOI 10.1109/5.241504
   JOLICOEUR P, 1987, MEM COGNITION, V15, P531, DOI 10.3758/BF03198388
   LEVINE M, 1982, ENVIRON BEHAV, V14, P221, DOI 10.1177/0013916584142006
   Lobben AK, 2004, PROF GEOGR, V56, P270
   Mack Arien, 1998, Inattentional Blindness
   Nadenau M.J., 2000, P IEEE
   NAVON D, 1977, COGNITIVE PSYCHOL, V9, P353, DOI 10.1016/0010-0285(77)90012-3
   NURMINEN A., 2008, IEEE COMPUT GRAPH, V28, P20
   NUSSECK M., 2005, P EUR C VIS PERC ECV
   OSullivan C., 2004, Eurographics state of the art reports, V4, P1
   OULASVIRTA A., 2008, PERS UBIQUIT COMPUT, V13, P303
   Partala T., 2010, PROC BSC HCI 2010, P428
   Pasman W, 2003, IEEE T VIS COMPUT GR, V9, P226, DOI 10.1109/TVCG.2003.1196009
   Ramanarayanan G, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1276377.1276472, 10.1145/1239451.1239527]
   RHYNE T.-M., 2008, SIGGRAPH 2008 C
   Riva O, 2008, COMPUTER, V41, P23, DOI 10.1109/MC.2008.414
   Snavely N, 2008, INT J COMPUT VISION, V80, P189, DOI 10.1007/s11263-007-0107-3
   Soulodre GA, 1998, J AUDIO ENG SOC, V46, P164
   Torralba A, 2009, VISUAL NEUROSCI, V26, P123, DOI 10.1017/S0952523808080930
   Watson A.B., 1993, DIGITAL IMAGES HUMAN, P179
NR 30
TC 1
Z9 1
U1 0
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD OCT
PY 2012
VL 9
IS 4
AR 21
DI 10.1145/2355598.2355604
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 025DJ
UT WOS:000310164900006
DA 2024-07-18
ER

PT J
AU Napieralski, PE
   Altenhoff, BM
   Bertrand, JW
   Long, LO
   Babu, SV
   Pagano, CC
   Kern, J
   Davis, TA
AF Napieralski, Phillip E.
   Altenhoff, Bliss M.
   Bertrand, Jeffrey W.
   Long, Lindsay O.
   Babu, Sabarish V.
   Pagano, Christopher C.
   Kern, Justin
   Davis, Timothy A.
TI Near-Field Distance Perception in Real and Virtual Environments Using
   Both Verbal and Action Responses
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Human Factors; Depth perception; distance estimation; virtual reality;
   immersive virtual environments; human factors and usability
ID EGOCENTRIC DISTANCE; JUDGMENTS; INFORMATION; CALIBRATION; GRAPHICS;
   QUALITY; OBJECTS
AB Few experiments have been performed to investigate near-field egocentric distance estimation in an Immersive Virtual Environment (IVE) as compared to the Real World (RW). This article investigates near-field distance estimation in IVEs and RW conditions using physical reach and verbal report measures, by using an apparatus similar to that used by Bingham and Pagano [1998]. Analysis of our experiment shows distance compression in both the IVE and RW conditions in participants' perceptual judgments to targets. This is consistent with previous research in both action space in an IVE and reach space with Augmented Reality (AR). Analysis of verbal responses from participants revealed that participants underestimated significantly less in the virtual world as compared to the RW. We also found that verbal reports and reaches provided different results in both IVEs and RW environments.
C1 [Napieralski, Phillip E.; Altenhoff, Bliss M.; Bertrand, Jeffrey W.; Long, Lindsay O.; Babu, Sabarish V.; Pagano, Christopher C.; Kern, Justin; Davis, Timothy A.] Clemson Univ, Sch Comp, Clemson, SC 29631 USA.
   [Napieralski, Phillip E.; Altenhoff, Bliss M.; Bertrand, Jeffrey W.; Long, Lindsay O.; Babu, Sabarish V.; Pagano, Christopher C.; Kern, Justin; Davis, Timothy A.] Clemson Univ, Dept Psychol, Clemson, SC 29634 USA.
C3 Clemson University; Clemson University
RP Napieralski, PE (corresponding author), Clemson Univ, Sch Comp, Clemson, SC 29631 USA.
EM pnapier@clemson.edu
OI Bertrand, Jeffrey/0000-0002-3921-4693
FU NSF [CNS-0850695]
FX This research was supported in part by NSF Research Experience for
   Undergraduates (REU) Site Grant CNS-0850695.
CR [Anonymous], 1956, Perception and the representative design of psychological experiments
   [Anonymous], 2005, ACM Transactions on Applied Perception, DOI [DOI 10.1145/1077399.1077403, DOI 10.1145/1077399.10774032,3,9]
   BINGHAM GP, 1994, ECOL PSYCHOL, V6, P219, DOI 10.1207/s15326969eco0603_4
   Bingham GP, 1998, J EXP PSYCHOL HUMAN, V24, P145, DOI 10.1037/0096-1523.24.1.145
   BRIDGEMAN B, 1981, PERCEPT PSYCHOPHYS, V29, P336, DOI 10.3758/BF03207342
   Brooks FP, 1999, IEEE COMPUT GRAPH, V19, P16, DOI 10.1109/38.799723
   Cutting JE, 1995, PERCEPTION SPACE MOT, P69, DOI [DOI 10.1016/B978-012240530-3/50005-5, 10.1016/B978-012240530-3/50005-5]
   Ellis SR, 1997, PRESENCE-TELEOP VIRT, V6, P452, DOI 10.1162/pres.1997.6.4.452
   Ellis SR, 1998, HUM FACTORS, V40, P415, DOI 10.1518/001872098779591278
   Ferwerda JA, 2003, PROC SPIE, V5007, P290, DOI 10.1117/12.473899
   FOLEY JM, 1985, J EXP PSYCHOL HUMAN, V11, P133, DOI 10.1037/0096-1523.11.2.133
   FOLEY JM, 1977, PERCEPTION, V6, P449, DOI 10.1068/p060449
   Gogel W C, 1968, Contrib Sens Physiol, V3, P125
   GOGEL WC, 1979, VISION RES, V19, P1161, DOI 10.1016/0042-6989(79)90013-0
   GOGEL WC, 1993, ADV PSYCH, V99, P113
   GOZA SM, 2004, P SIGCHI C HUM FACT, P623
   Grechkin TY, 2010, ACM T APPL PERCEPT, V7, DOI 10.1145/1823738.1823744
   Haller M., 2004, ACM International Conference on Virtual Reality Continuum and its Applications in Industry, P189, DOI [10.1145/1044588.1044627, DOI 10.1145/1044588.1044627]
   Hill R., 2003, Kunstliche Intelligenz (KI Journal), V17, P5, DOI DOI 10.1002/J.2162-6057.20041B01234.X
   Hine B., 1994, P 2 WORKSH MOB ROB S, P117
   Hodges LF, 2001, IEEE COMPUT GRAPH, V21, P25, DOI 10.1109/38.963458
   Interrante V, 2006, P IEEE VIRT REAL ANN, P3, DOI 10.1109/VR.2006.52
   Johnsen K, 2006, PRESENCE-TELEOP VIRT, V15, P33, DOI 10.1162/pres.2006.15.1.33
   Kessler GD, 2000, PRESENCE-TELEOP VIRT, V9, P187, DOI 10.1162/105474600566718
   Klein E, 2009, IEEE VIRTUAL REALITY 2009, PROCEEDINGS, P107, DOI 10.1109/VR.2009.4811007
   Kunz BR, 2009, ATTEN PERCEPT PSYCHO, V71, P1284, DOI 10.3758/APP.71.6.1284
   Loomis JM, 2003, VIRTUAL AND ADAPTIVE ENVIRONMENTS: APPLICATIONS, IMPLICATIONS, AND HUMAN PERFORMANCE ISSUES, P21
   Milner AD, 2008, NEUROPSYCHOLOGIA, V46, P774, DOI 10.1016/j.neuropsychologia.2007.10.005
   Milner AD., 1995, The visual brain in action
   Mon-Williams M, 1999, EXP BRAIN RES, V126, P578, DOI 10.1007/s002210050766
   Pagano CC, 1998, J EXP PSYCHOL HUMAN, V24, P1037, DOI 10.1037/0096-1523.24.4.1037
   Pagano CC, 2001, ECOL PSYCHOL, V13, P197, DOI 10.1207/S15326969ECO1303_2
   Pagano CC, 2008, PSYCHON B REV, V15, P437, DOI 10.3758/PBR.15.2.437
   Peters TM, 2008, LECT NOTES COMPUT SC, V5128, P1, DOI 10.1007/978-3-540-79982-5_1
   Philbeck JW, 1997, J EXP PSYCHOL HUMAN, V23, P72, DOI 10.1037/0096-1523.23.1.72
   Phillips Lane., 2009, P 6 S APPL PERCEPTIO, P11
   RIESER JJ, 1995, J EXP PSYCHOL HUMAN, V21, P480, DOI 10.1037/0096-1523.21.3.480
   Sahm C. S., 2005, ACM Trans. Appl. Percept. (TAP), V2, P35, DOI DOI 10.1145/1048687.1048690
   Seymour NE, 2008, WORLD J SURG, V32, P182, DOI 10.1007/s00268-007-9307-9
   Singh Gurjot., 2010, Proceedings of the 7th Symposium on Applied Perception in Graphics and Visualization, P149, DOI DOI 10.1145/1836248.1836277
   Sutherland I.E., 1965, The Ultimate Display, P506, DOI DOI 10.1109/MC.2005.274
   Thompson WB, 2004, PRESENCE-TELEOP VIRT, V13, P560, DOI 10.1162/1054746042545292
   Wang RF, 2004, COGNITION, V94, P185, DOI 10.1016/j.cognition.2004.05.001
   Warren W.H., 1995, GLOBAL PERSPECTIVES, V1, P210
   Willemsen P, 2009, ACM T APPL PERCEPT, V6, DOI 10.1145/1498700.1498702
   Withagen R, 2005, J EXP PSYCHOL HUMAN, V31, P1379, DOI 10.1037/0096-1523.31.6.1379
   Witmer BG, 1998, PRESENCE-TELEOP VIRT, V7, P144, DOI 10.1162/105474698565640
NR 47
TC 46
Z9 51
U1 0
U2 11
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD AUG
PY 2011
VL 8
IS 3
AR 18
DI 10.1145/2010325.2010328
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA 819HK
UT WOS:000294815800003
DA 2024-07-18
ER

PT J
AU Couture, V
   Langer, MS
   Roy, S
AF Couture, Vincent
   Langer, Michael S.
   Roy, Sebastien
TI Analysis of Disparity Distortions in Omnistereoscopic Displays
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Algorithms; Measurement; Experimentation; Panorama; median plane; depth
   acuity; stereo; perception
AB An omnistereoscopic image is a pair of panoramic images that enables stereoscopic depth perception all around an observer. An omnistereo projection on a cylindrical display does not require tracking of the observer's viewing direction. However, such a display introduces stereo distortions. In this article, we investigate two projection models for rendering 3D scenes in omnistereo. The first is designed to give zero disparity errors at the center of the visual field. The second is the well-known slit-camera model. For both models, disparity errors are shown to increase gradually in the periphery, as visual stereo acuity decreases. We use available data on human stereoscopic acuity limits to argue that depth distortions caused by these models are so small that they cannot be perceived.
C1 [Couture, Vincent; Roy, Sebastien] Univ Montreal, DIRO, Quebec City, PQ, Canada.
   [Langer, Michael S.] McGill Univ, Montreal, PQ, Canada.
C3 Universite de Montreal; McGill University
RP Couture, V (corresponding author), Univ Montreal, DIRO, Pavillon Andre Aisenstadt, Quebec City, PQ, Canada.
EM chapdelv@iro.umontreal.ca
CR Adams W, 1996, PERCEPTION, V25, P165, DOI 10.1068/p250165
   BANKS MS, 1991, J OPT SOC AM A, V8, P1775, DOI 10.1364/JOSAA.8.001775
   Bourke P, 2009, COMPUTER GAMES ALLIE, V1, P136
   Bourke P, 2006, LECT NOTES COMPUT SC, V4270, P147
   CRUZNEIRA C, 1992, COMMUN ACM, V35, P64, DOI 10.1145/129888.129892
   DeFanti TA, 1993, Proceedings of the 20th annual conference on Computer graphics and interactive techniques, P135, DOI 10.1145/166117.166134.
   Held RT, 2008, APGV 2008: PROCEEDINGS OF THE SYMPOSIUM ON APPLIED PERCEPTION IN GRAPHICS AND VISUALIZATION, P23
   Hess RF, 1999, VISION RES, V39, P559, DOI 10.1016/S0042-6989(98)00127-8
   Hoffman DM, 2008, J VISION, V8, DOI 10.1167/8.3.33
   Howard IP., 2002, SEEING DEPTH
   Huang HC, 1998, GRAPH MODEL IM PROC, V60, P196, DOI 10.1006/gmip.1998.0467
   ISHIGURO H, 1992, IEEE T PATTERN ANAL, V14, P257, DOI 10.1109/34.121792
   Naemura T, 1998, 1998 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL 1, P903, DOI 10.1109/ICIP.1998.723666
   Peleg S, 2001, IEEE T PATTERN ANAL, V23, P279, DOI 10.1109/34.910880
   PELEG S, 1999, P IEEE C COMP VIS PA, V1, P1395
   Prince SJD, 1998, VISION RES, V38, P2533, DOI 10.1016/S0042-6989(98)00118-7
   RASKAR R, 1999, P IEEE VIS C
   READ J, 2010, J VIS, V10
   Simon A, 2004, P IEEE VIRT REAL ANN, P67, DOI 10.1109/VR.2004.1310057
   Tardif JP, 2005, FIFTH INTERNATIONAL CONFERENCE ON 3-D DIGITAL IMAGING AND MODELING, PROCEEDINGS, P22, DOI 10.1109/3DIM.2005.11
   Tardif JP, 2003, FOURTH INTERNATIONAL CONFERENCE ON 3-D DIGITAL IMAGING AND MODELING, PROCEEDINGS, P217, DOI 10.1109/im.2003.1240253
   Vishwanath D, 2005, NAT NEUROSCI, V8, P1401, DOI 10.1038/nn1553
   WOODS A, 1993, P SOC PHOTO-OPT INS, V1915, P36, DOI 10.1117/12.157041
NR 23
TC 4
Z9 4
U1 0
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD JUL
PY 2010
VL 7
IS 4
AR 25
DI 10.1145/1823738.1823743
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 633ZI
UT WOS:000280546500005
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Riecke, BE
   Väljamäe, A
   Schulte-Pelkum, J
AF Riecke, Bernhard E.
   Valjamae, Aleksander
   Schulte-Pelkum, Joerg
TI Moving Sounds Enhance the Visually-Induced Self-Motion Illusion
   (Circular Vection) in Virtual Reality
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Experimentation; Human Factors; Measurement; Audiovisual interactions;
   presence; psychophysics; self-motion simulation; spatial sound; vection;
   virtual reality
ID LOCOMOTION; PERCEPTION; INDUCTION; POSTURE
AB While rotating visual and auditory stimuli have long been known to elicit self-motion illusions ("circular vection"), audiovisual interactions have hardly been investigated. Here, two experiments investigated whether visually induced circular vection can be enhanced by concurrently rotating auditory cues that match visual landmarks (e. g., a fountain sound). Participants sat behind a curved projection screen displaying rotating panoramic renderings of a market place. Apart from a no-sound condition, headphone-based auditory stimuli consisted of mono sound, ambient sound, or low-/high-spatial resolution auralizations using generic head-related transfer functions (HRTFs). While merely adding nonrotating (mono or ambient) sound showed no effects, moving sound stimuli facilitated both vection and presence in the virtual environment. This spatialization benefit was maximal for a medium (20 degrees x 15 degrees) FOV, reduced for a larger (54 degrees x 45 degrees) FOV and unexpectedly absent for the smallest (10 degrees x 7.5 degrees) FOV. Increasing auralization spatial fidelity (from low, comparable to five-channel home theatre systems, to high, 5 degrees resolution) provided no further benefit, suggesting a ceiling effect. In conclusion, both self-motion perception and presence can benefit from adding moving auditory stimuli. This has important implications both for multimodal cue integration theories and the applied challenge of building affordable yet effective motion simulators.
C1 [Valjamae, Aleksander] Chalmers Univ Technol, Div Appl Acoust, CRAGmsa, S-41296 Gothenburg, Sweden.
   [Riecke, Bernhard E.; Schulte-Pelkum, Joerg] Max Planck Inst Biol Cybernet, D-72076 Tubingen, Germany.
C3 Chalmers University of Technology; Max Planck Society
RP Riecke, BE (corresponding author), Simon Fraser Univ, 250-13450 102nd Ave, Surrey, BC V3T 0A3, Canada.
EM ber1@sfu.ca; aleksander.valjamae@chalmers.se; joerg.sp@tuebingen.mpg.de
RI Valjamae, Aleksander/C-9952-2013; Riecke, Bernhard/C-6399-2011
OI Valjamae, Aleksander/0000-0001-6071-3211; Riecke,
   Bernhard/0000-0001-7974-0850
FU EC [POEMS-IST-2001-39223]; Swedish Science Council; Max Planck Society
FX This research was funded by the EC grant POEMS-IST-2001-39223 (see
   www.poems-project.info), the Swedish Science Council (VR), and the Max
   Planck Society.
CR Algazi VR, 2001, PROCEEDINGS OF THE 2001 IEEE WORKSHOP ON THE APPLICATIONS OF SIGNAL PROCESSING TO AUDIO AND ACOUSTICS, P99, DOI 10.1109/ASPAA.2001.969552
   [Anonymous], P SIGCHI C HUM FACT
   Best V, 2005, ACTA ACUST UNITED AC, V91, P421
   BOER ER, 2000, P DRIV SIM C DSC 00
   BRANDT T, 1973, PFLUG ARCH EUR J PHY, V339, P97
   BRANDT T, 1973, EXP BRAIN RES, V16, P476, DOI 10.1007/BF00234474
   Brungart DS, 2005, ACTA ACUST UNITED AC, V91, P471
   BURKICOHEN J, 2003, P 12 INT S AV PSYCH, P182
   Cater K., 2003, Eurographics Symposium on Rendering. 14th Eurographics Workshop on Rendering, P270
   Chance SS, 1998, PRESENCE-TELEOP VIRT, V7, P168, DOI 10.1162/105474698565659
   Dichgans Johannes, 1978, Perception, P755, DOI [DOI 10.1007/978-3-642-46354-9253F, DOI 10.1007/978-3-642-46354-9_25, DOI 10.1007/978-3-642-46354-925]
   Dodge R, 1923, J EXP PSYCHOL, V6, P107, DOI 10.1037/h0076105
   Durgin FH, 2005, J EXP PSYCHOL HUMAN, V31, P398, DOI 10.1037/0096-1523.31.3.398
   Durlach N., 1995, Virtual Reality: Scientific and Technological Challenges, DOI 10.17226/4761
   Ernst MO, 2004, TRENDS COGN SCI, V8, P162, DOI 10.1016/j.tics.2004.02.002
   Fischer MH, 1930, J PSYCHOL NEUROL, V41, P273
   Flanagan MB, 2004, J VESTIBUL RES-EQUIL, V14, P335
   GARDNER WG, 1995, J ACOUST SOC AM, V97, P3907, DOI 10.1121/1.412407
   GEKHMAN B I, 1991, Sensornye Sistemy, V5, P71
   Hendrix C, 1996, PRESENCE-TELEOP VIRT, V5, P290, DOI 10.1162/pres.1996.5.3.290
   HENNEBERT PE, 1960, J AUD RES, V1, P84
   Hettinger LJ, 2002, HUM FAC ER, P471
   Hollerbach JM, 2002, HUM FAC ER, P239
   Howard I. P., 1986, HDB HUMAN PERCEPTION, V1
   Howard I.P., 1982, HUMAN VISUAL ORIENTA
   KAPRALOS B, 2004, P 116 AES CONV
   KLEINER M, 1993, J AUDIO ENG SOC, V41, P861
   LACKNER JR, 1977, AVIAT SPACE ENVIR MD, V48, P129
   Langendijk EHA, 2000, J ACOUST SOC AM, V107, P528, DOI 10.1121/1.428321
   LARSSON P, 2009, ENG MIXED REALITY SY
   Larsson P., 2004, Proceedings of 7th Annual Workshop of Presence, P252
   LEPECQ JC, 1995, PERCEPTION, V24, P435, DOI 10.1068/p240435
   LOMBARD M, 1999, J COMPUT MEDIATED CO, V3
   LUCIANI A, 2004, P 7 ANN WORKSH PRES, P96
   Mach E., 1875, GRUNDLINIEN LEHRE BE
   MARMEKARELSE AM, 1977, AGRESSOLOGIE, V18, P329
   MCFARLAND WH, 1969, J AUD RES, V9, P236
   MOECK T, 2007, P S INT 3D GRAPH GAM, P189
   Mulder M, 2004, PRESENCE-TELEOP VIRT, V13, P535, DOI 10.1162/1054746042545256
   OZAWA K, 2004, ACOUSTICAL SCI TECH, V24, P42
   POPE J, 1999, P 7 INT C CENTR EUR, P233
   Riecke B. E., 2004, P 7 ANN WORKSH PRES, P125, DOI 10.1.1.122.5636
   Riecke B.E., 2005, PRESENCE 2005, P49
   Riecke B.E., 2006, ACM T APPL PERCEPT, V3, DOI DOI 10.1145/1166087.1166091
   Riecke BE, 2005, P IEEE VIRT REAL ANN, P131
   Riecke BE, 2005, PROC SPIE, V5666, P344, DOI 10.1117/12.610846
   RIECKE BE, 2009, ACM T APPL PER UNPUB
   RIECKE BE, 2005, P HCI INT 2005, P1
   RIECKE BE, 2008, P CYBERWALK WORKSH
   Riecke BE, 2007, IEEE VIRTUAL REALITY 2007, PROCEEDINGS, P3
   Riecke BE, 2008, APGV 2008: PROCEEDINGS OF THE SYMPOSIUM ON APPLIED PERCEPTION IN GRAPHICS AND VISUALIZATION, P147
   Rumsey F, 2002, J AUDIO ENG SOC, V50, P651
   Sakamoto S., 2004, Acoustical Science and Technology, V25, P100, DOI 10.1250/ast.25.100
   SCHINAUER T, 1993, CONTRIB PSYCHOL, P373
   Schubert T, 2001, PRESENCE-VIRTUAL AUG, V10, P266, DOI 10.1162/105474601300343603
   Schulte-Pelkum J., 2004, INT MULTISENSORY RES
   SCHULTEPELKUM J, 2009, INTEGRATIVE APPROACH
   SCHULTEPELKUM J, 2004, 122 MPI BIOL CYB
   Soto-Faraco S, 2003, NEUROPSYCHOLOGIA, V41, P1847, DOI 10.1016/S0028-3932(03)00185-4
   SUNDSTEDT V, 2004, P 1 S APPL PERC GRAP, P175
   Tan KP, 2006, MECH ADV MATER STRUC, V13, P1, DOI 10.1080/15376490500343717
   VALJAMAE A, 2008, HDB PRESENCE
   Valjamae A., 2004, P 7 INT C PRESENCE, P141
   VALJAMAE A, 2008, IEEE MULTIMEDIA
   VALJAMAE A, 2005, P 11 M INT C AUD DIS
   VALJAMAE A, 2008, J ACOUSTIC ENG UNPUB
   Väljamäe A, 2008, PRESENCE-TELEOP VIRT, V17, P43, DOI 10.1162/pres.17.1.43
   Väljamäe A, 2006, J AUDIO ENG SOC, V54, P954
   van der Steen FAM, 2000, PERCEPT PSYCHOPHYS, V62, P89, DOI 10.3758/BF03212063
   von Helmholtz H., 1896, HDB PHYSL OPTIK
   Warren R., 1990, PERCEPTION CONTROL S
   Watson J.E., 1968, Journal for Auditory Research, V8, P161
   Wright WG, 2006, J VESTIBUL RES-EQUIL, V16, P23
NR 73
TC 46
Z9 54
U1 0
U2 17
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD FEB
PY 2009
VL 6
IS 2
AR 7
DI 10.1145/1498700.1498701
PG 27
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 450YN
UT WOS:000266438000001
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Murphy, HA
   Duchowski, AT
   Tyrrell, RA
AF Murphy, Hunter A.
   Duchowski, Andrew T.
   Tyrrell, Richard A.
TI Hybrid Image/Model-Based Gaze-Contingent Rendering
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article; Proceedings Paper
CT Symposium on Applied Perception in Graphics and Visualization
CY JUL 25-27, 2007
CL Tubingen, GERMANY
DE Gaze-Contingent Display; Eye tracking; Level of Detail
ID RESOLUTION; ATTENTION; DISPLAYS
AB A nonisotropic hybrid image/model-based gaze-contingent rendering technique utilizing ray casting on a GPU is discussed. Empirical evidence derived from human subject experiments indicates an inverse relationship between a peripherally degraded scene's high-resolution inset size and mean search time, a trend consistent with existing image-based and model-based techniques. In addition, the data suggest that maintaining a target's silhouette edges decreases search times when compared to targets with degraded edges. However, analysis suggests a point of diminishing returns with an inset larger than 15 degrees when target discrimination is a component of visual search. Benefits of the hybrid technique include simplicity of design and parallelizability, both conducive to GPU implementation.
C1 [Murphy, Hunter A.; Duchowski, Andrew T.] Clemson Univ, Sch Comp, Clemson, SC 29634 USA.
   [Tyrrell, Richard A.] Clemson Univ, Dept Psychol, Clemson, SC 29634 USA.
C3 Clemson University; Clemson University
RP Murphy, HA (corresponding author), Clemson Univ, Sch Comp, 100 McAdams Hall, Clemson, SC 29634 USA.
CR [Anonymous], 2000, CS200004 U VIRG
   [Anonymous], 1983, The Psychology of Human-Computer Interaction
   [Anonymous], LECT NOTES
   [Anonymous], 2007, Eye tracking methodology: Theory and practice, DOI DOI 10.1007/978-3-319-57883-5
   BAHILL A T, 1975, Mathematical Biosciences, V24, P191, DOI 10.1016/0025-5564(75)90075-9
   Baron J., 2007, NOTES USE R PSYCHOL
   Castet E, 2002, P NATL ACAD SCI USA, V99, P15159, DOI 10.1073/pnas.232377199
   Cave KR, 1999, PSYCHON B REV, V6, P204, DOI 10.3758/BF03212327
   CLARK JH, 1976, COMMUN ACM, V19, P547, DOI 10.1145/360349.360354
   Crawford TJ, 1998, AM J PSYCHIAT, V155, P1703, DOI 10.1176/ajp.155.12.1703
   Daly S, 2001, J ELECTRON IMAGING, V10, P30, DOI 10.1117/1.1333679
   Danforth R., 2000, Smart Graphics. Papers from the 2000 AAAI Symposium, P66
   Duchowski A, 2002, BEHAV RES METH INS C, V34, P573, DOI 10.3758/BF03195486
   Duchowski AT, 2007, ACM T MULTIM COMPUT, V3, DOI 10.1145/1314303.1314309
   Ferwerda JA, 2003, PROC SPIE, V5007, P290, DOI 10.1117/12.473899
   Geisler W. S., 1998, HUMAN VISION ELECT I
   Geisler WS, 2006, J VISION, V6, P858, DOI 10.1167/6.9.1
   HOPPE H, 1997, COMPUTER GRAPHICS SI
   Levoy M., 1990, Computer Graphics, V24, P217, DOI 10.1145/91394.91449
   Lindstrom P., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P109, DOI 10.1145/237170.237217
   Loschky LC, 2005, VIS COGN, V12, P1057, DOI 10.1080/13506280444000652
   Loschky LC, 2002, J EXP PSYCHOL-APPL, V8, P99, DOI 10.1037/1076-898X.8.2.99
   Loschky LC, 2007, ACM T MULTIM COMPUT, V3, DOI 10.1145/1314303.1314310
   Louie EG, 2007, J VISION, V7, DOI 10.1167/7.2.24
   Luebke D., 1997, COMPUTER GRAPHICS SI
   LUEBKE D, 2001, P 2001 EUR WORKSH RE
   MacCracken R., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P181, DOI 10.1145/237170.237247
   Murphy H., 2001, EUROGRAPHICS
   Murphy H, 2007, APGV 2007: SYMPOSIUM ON APPLIED PERCEPTION IN GRAPHICS AND VISUALIZATION, PROCEEDINGS, P107
   Ohshima T, 1996, P IEEE VIRT REAL ANN, P103, DOI 10.1109/VRAIS.1996.490517
   PARKHURST D, 2004, APGV 04 P 1 S APPL P, P49
   Parkhurst DJ, 2002, HUM FACTORS, V44, P611, DOI 10.1518/0018720024497015
   Pelli DG, 2004, J VISION, V4, P1136, DOI 10.1167/4.12.12
   Popov S, 2007, COMPUT GRAPH FORUM, V26, P415, DOI 10.1111/j.1467-8659.2007.01064.x
   POSNER MI, 1980, J EXP PSYCHOL GEN, V109, P160, DOI 10.1037/0096-3445.109.2.160
   Purcell TJ, 2002, ACM T GRAPHIC, V21, P703, DOI 10.1145/566570.566640
   Reddy M., 1998, Virtual Reality, V3, P132, DOI 10.1007/BF01417674
   Reingold EM, 2003, HUM FACTORS, V45, P307, DOI 10.1518/hfes.45.2.307.27235
   Schmalstieg D, 1997, P IEEE VIRT REAL ANN, P12, DOI 10.1109/VRAIS.1997.583039
   SCHUMACHER J, 2004, EUR S VIRT ENV
   Shafiq-Antonacci R, 2003, ARCH NEUROL-CHICAGO, V60, P1272, DOI 10.1001/archneur.60.9.1272
   Smeets JBJ, 2003, J NEUROPHYSIOL, V90, P12, DOI 10.1152/jn.01075.2002
   *TOBII TECHN AB, 2003, TOB ET 17 EYE TRACK
   Watson B, 2004, ACM T GRAPHIC, V23, P750, DOI 10.1145/1015706.1015796
   Watson B., 1997, ACM Transactions on Computer-Human Interaction, V4, P323, DOI 10.1145/267135.267137
   ZORIN D, 1997, COMPUTER GRAPHICS
   ZORIN D, 2000, COURSE, V23
   [No title captured]
NR 48
TC 15
Z9 19
U1 0
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD JAN
PY 2009
VL 5
IS 4
AR 22
DI 10.1145/1462048.1462053
PG 21
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 450YL
UT WOS:000266437800005
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Mohler, BJ
   Thompson, WB
   Creem-Regehr, SH
   Willemsen, P
   Pick, HL
   Rieser, JJ
AF Mohler, Betty J.
   Thompson, William B.
   Creem-Regehr, Sarah H.
   Willemsen, Peter
   Pick, Herbert L., Jr.
   Rieser, John J.
TI Calibration of Locomotion Resulting from Visual Motion in a
   Treadmill-Based Virtual Environment
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Experimentation; Human Factors; Treadmill virtual environments;
   locomotion; visual self-motion
ID SPACE-PERCEPTION; WALKING; POSTURE; SPEED
AB This paper describes the use of a treadmill-based virtual environment (VE) to investigate the influence of visual motion on locomotion. First, we establish that a computer-controlled treadmill coupled with a wide field of view computer graphics display can be used to study interactions between perception and action. Previous work has demonstrated that humans recalibrate their visually directed actions to changing circumstances in their environment. Using a treadmill VE, we show that recalibration of action is reflected in the real world as a result of manipulating the relation between the visual indication of speed, presented using computer graphics, and the biomechanical speed of walking on a treadmill. We then extend this methodology to investigate whether the recalibration is based on perception of the speed of movement through the world or on the magnitude of optic flow itself. This was done by utilizing two different visual displays, which had essentially the same magnitude of optic flow, but which differed in the information present for the speed of forward motion. These results indicate that changes in optic flow are not necessary for recalibration to occur. The recalibration effect is dependent, at least in part, on visual perception of the speed of self-movement.
C1 [Mohler, Betty J.; Thompson, William B.; Willemsen, Peter] Univ Utah, Sch Comp, Salt Lake City, UT 84112 USA.
   [Creem-Regehr, Sarah H.] Univ Utah, Dept Psychol, Salt Lake City, UT 84112 USA.
   [Pick, Herbert L., Jr.] Univ Utah, Inst Child Dev, Minneapolis, MN 55455 USA.
   [Pick, Herbert L., Jr.] Univ Minnesota, Minneapolis, MN 55455 USA.
   [Rieser, John J.] Vanderbilt Univ, Dept Psychol & Human Dev, Nashville, TN 37203 USA.
C3 Utah System of Higher Education; University of Utah; Utah System of
   Higher Education; University of Utah; Utah System of Higher Education;
   University of Utah; University of Minnesota System; University of
   Minnesota Twin Cities; Vanderbilt University
RP Mohler, BJ (corresponding author), Univ Utah, Sch Comp, 50 So Cent Campus Dr,Room MEB-3190, Salt Lake City, UT 84112 USA.
EM bmohler@cs.utah.edu; thompson@cs.utah.edu; sarah.creem@psych.utah.edu;
   willemsn@cs.utah.edu; herbpick@umn.edu; J.Rieser@vanderbilt.edu
FU National Science Foundation [0080999, 0121084]; Direct For Computer &
   Info Scie & Enginr; Div Of Information & Intelligent Systems [0080999]
   Funding Source: National Science Foundation; Div Of Information &
   Intelligent Systems; Direct For Computer & Info Scie & Enginr [0121084]
   Funding Source: National Science Foundation
FX This material is based upon work supported by the National Science
   Foundation under grants 0080999 and 0121084.
CR BANTON T, 2005, PRESENCE-TELEOP VIRT, V14, P4
   Durgin FH, 1999, EXP BRAIN RES, V127, P12, DOI 10.1007/s002210050769
   DURGIN FH, 2005, J EXPT PSYCHOL HUMAN, V31, P2
   DURGIN FH, 2002, 43 ANN M PSYCH SOC
   DURGIN FH, 2005, J EXPT PSYCHOL HUMAN, V31, P3
   FRENZ H, 2005, VISION RES
   Gibson J.J., 1950, PERCEPTION VISUAL WO
   GIBSON JJ, 1958, BRIT J PSYCHOL, V49, P182, DOI 10.1111/j.2044-8295.1958.tb00656.x
   HOLLERBACH J, 2000, HAPTICS S P ASME DYN, V692, P1293
   Kay BA, 2001, BIOL CYBERN, V85, P89, DOI 10.1007/PL00008002
   Kelly JW, 2005, EXP BRAIN RES, V161, P285, DOI 10.1007/s00221-004-2069-9
   LAPPE M, 1999, TRENDS COGNITIVE SCI, V3
   LEE DN, 1976, PERCEPTION, V5, P437, DOI 10.1068/p050437
   Loomis JM, 2003, VIRTUAL AND ADAPTIVE ENVIRONMENTS: APPLICATIONS, IMPLICATIONS, AND HUMAN PERFORMANCE ISSUES, P21
   LOOMIS JM, 1992, J EXP PSYCHOL HUMAN, V18, P906, DOI 10.1037/0096-1523.18.4.906
   Loomis JM, 1998, ECOL PSYCHOL, V10, P271, DOI 10.1207/s15326969eco103&4_6
   LOOMIS JM, PSYCHOL SCI IN PRESS
   Pelah A, 1996, NATURE, V381, P283, DOI 10.1038/381283a0
   Pelah A., 1997, Investigative Ophthalmology and Visual Science, V38, pS1007
   PELAH A, 2002, J VISION, V2, pA628
   RIESER JJ, 1990, PERCEPTION, V19, P675, DOI 10.1068/p190675
   RIESER JJ, 1995, J EXP PSYCHOL HUMAN, V21, P480, DOI 10.1037/0096-1523.21.3.480
   ROGERS S, 1995, HDB PERCEPTION COGNI, V5, P119
   Sedgwick H.A., 1986, Handbook of Perception and Human Performance, p21
   Stappers PJ, 1996, PERCEPT MOTOR SKILL, V83, P1353, DOI 10.2466/pms.1996.83.3f.1353
   Thompson E., 2004, J MENS STUDIES, V13, P5, DOI [DOI 10.3149/JMS.1301.5, https://doi.org/10.3149/jms.1301.5, 10.3149/jms.1301.5]
   Thurrell AEI, 1998, PERCEPTION, V27, P147
   THURRELL AEI, 2002, J VISION, V1, pA307
   Usoh M, 1999, COMP GRAPH, P359, DOI 10.1145/311535.311589
   Warren R., 1990, PERCEPTION CONTROL S
   Warren W.H., 1995, PERCEPTION SPACE MOT, P263, DOI [DOI 10.1016/B978-012240530-3/50010-9, 10.1016/B978-012240530-3/50010-9]
   Warren WH, 1996, J EXP PSYCHOL HUMAN, V22, P818, DOI 10.1037/0096-1523.22.4.818
   Withagen R, 2002, ECOL PSYCHOL, V14, P223, DOI 10.1207/S15326969ECO1404_2
   Witmer BG, 1998, HUM FACTORS, V40, P478, DOI 10.1518/001872098779591340
   Wraga M, 1999, J EXP PSYCHOL HUMAN, V25, P518, DOI 10.1037/0096-1523.25.2.518
   [No title captured]
NR 36
TC 61
Z9 72
U1 0
U2 8
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD JAN
PY 2007
VL 4
IS 1
AR 4
DI 10.1145/1227134.1227138
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA V04IL
UT WOS:000207052000004
DA 2024-07-18
ER

PT J
AU Kasahara, S
   Takada, K
AF Kasahara, Shunichi
   Takada, Kazuma
TI Stealth Updates of Visual Information by Leveraging Change Blindness and
   Computational Visual Morphing
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article; Proceedings Paper
CT 18th Symposium on Applied Perceptiion (SAP)
CY SEP, 2021
CL ELECTR NETWORK
DE Change blindness; eye tracking; blink detection; morphing; generative
   adversarial network
ID BLINKS
AB We present an approach for covert visual updates by leveraging change blindness with computationally generated morphed images. To clarify the design parameters for intentionally suppressing change detection with morphing visuals, we investigated the visual change detection in three temporal behaviors: visual blank, eye-blink, and step-sequential changes. The results showed a robust trend of change blindness with a blank of more than 33.3 ms and with eye blink. Our sequential change study revealed that participants did not recognize changes until an average of 57% morphing toward another face in small change steps. In addition, changes went unnoticed until the end of morphing in more than 10% of all trials. Our findings should contribute to the design of covert visual updates without consuming users' attention by leveraging change blindness with computational visual morphing.
C1 [Kasahara, Shunichi; Takada, Kazuma] Sony Comp Sci Labs Inc, Shinagawa Ku, 3-14-13 Higashigotanda, Tokyo 1410022, Japan.
C3 Sony Corporation
RP Kasahara, S (corresponding author), Sony Comp Sci Labs Inc, Shinagawa Ku, 3-14-13 Higashigotanda, Tokyo 1410022, Japan.
EM kasahara@csl.sony.co.jp; k-takada@csl.sony.co.jp
FU JST Moonshot RD Program [JPMJMS2013]
FX This work was supported by JST Moonshot R&D Program Grant No.
   JPMJMS2013.
CR Adamczyk Piotr D., 2004, P SIGCHI C HUMAN FAC, P271, DOI DOI 10.1145/985692.985727
   Andermane N, 2019, NEUROSCI CONSCIOUS, V5, DOI 10.1093/nc/niy010
   Becker MW, 2000, PERCEPTION, V29, P273
   Bolte B, 2015, IEEE T VIS COMPUT GR, V21, P545, DOI 10.1109/TVCG.2015.2391851
   Brock AM, 2018, PROCEEDINGS PERVASIVE DISPLAYS 2018: THE 7TH ACM INTERNATIONAL SYMPOSIUM ON PERVASIVE DISPLAYS, DOI 10.1145/3205873.3205877
   Brock M, 2018, PROCEEDINGS OF THE 2018 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2018)
   Cech J., 2016, P 21 COMP VIS WINT W
   Davies Thomas, 2012, P SIGCHI C HUMAN FAC, P1451, DOI DOI 10.1145/2207676.2208606
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Devlin J., 2018, BERT PRE TRAINING DE
   DiVita J, 2004, HUM FACTORS, V46, P205, DOI 10.1518/hfes.46.2.205.37340
   Durlach PJ, 2004, HUM-COMPUT INTERACT, V19, P423, DOI 10.1207/s15327051hci1904_10
   Goddard E, 2013, J VISION, V13, DOI 10.1167/13.5.20
   Grimes J., 1996, PERCEPTION VANCOUVER, V2, P89, DOI DOI 10.1093/ACPROF:OSO/9780195084627.003.0004
   Hollingworth A, 2000, VIS COGN, V7, P213, DOI 10.1080/135062800394775
   Karras Tero, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8107, DOI 10.1109/CVPR42600.2020.00813
   Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453
   Kwon KA, 2013, J R SOC INTERFACE, V10, DOI 10.1098/rsif.2013.0227
   Laloyaux C, 2008, CONSCIOUS COGN, V17, P646, DOI 10.1016/j.concog.2007.03.002
   Langbehn E, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201335
   Ma LQ, 2013, IEEE T VIS COMPUT GR, V19, P1808, DOI 10.1109/TVCG.2013.99
   Mancero G., 2007, Proceedings of the 14th European Conference on Cognitive Ergonomics: Invent! Explore! ECCE'07, P167, DOI [DOI 10.1145/1362550, 10.1145/1362550]
   Marwecki S, 2019, PROCEEDINGS OF THE 32ND ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY (UIST 2019), P777, DOI 10.1145/3332165.3347919
   Nakamura Yoshiko, 2008, Nippon Ganka Gakkai Zasshi, V112, P1059
   O'Regan JK, 1999, NATURE, V398, P34, DOI 10.1038/17953
   O'Regan JK, 2000, VIS COGN, V7, P191, DOI 10.1080/135062800394766
   Peck TC, 2020, IEEE T VIS COMPUT GR, V26, P1945, DOI 10.1109/TVCG.2020.2973498
   Poth CH, 2018, BEHAV RES METHODS, V50, P26, DOI 10.3758/s13428-017-1003-6
   Rensink RA, 2002, ANNU REV PSYCHOL, V53, P245, DOI 10.1146/annurev.psych.53.100901.135125
   Rensink RA, 2000, VIS COGN, V7, P127, DOI 10.1080/135062800394720
   Rensink RA, 1997, PSYCHOL SCI, V8, P368, DOI 10.1111/j.1467-9280.1997.tb00427.x
   Rensink Ronald A., 2005, P76, DOI 10.1016/B978-012375731-9/50017-3
   Ridder WH, 1997, VISION RES, V37, P3171, DOI 10.1016/S0042-6989(97)00110-7
   Schiffman Harvey Richard, 1990, SENSATION PERCEPTION, V3rd, P555
   Simons DJ, 1999, PERCEPTION, V28, P1059, DOI 10.1068/p2952
   Simons DJ, 2000, PERCEPTION, V29, P1143, DOI 10.1068/p3104
   Suma EA, 2011, P IEEE VIRT REAL ANN, P159, DOI 10.1109/VR.2011.5759455
   Tanaka JW, 2004, COGNITION, V93, pB1, DOI 10.1016/j.cognition.2003.09.011
   Varakin DA, 2004, HUM-COMPUT INTERACT, V19, P389, DOI 10.1207/s15327051hci1904_9
   VOLKMANN FC, 1980, SCIENCE, V207, P900, DOI 10.1126/science.7355270
   VOLKMANN FC, 1986, VISION RES, V26, P1401, DOI 10.1016/0042-6989(86)90164-1
   Wilson Andrew D., 2018, P ACM S EYE TRACK RE, V2, P1
NR 42
TC 2
Z9 2
U1 0
U2 0
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD OCT
PY 2021
VL 18
IS 4
AR 23
DI 10.1145/3486581
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA YY1DS
UT WOS:000754533100007
DA 2024-07-18
ER

PT J
AU Jörg, S
   Duchowski, A
   Krejtz, K
   Niedzielska, A
AF Jorg, Sophie
   Duchowski, Andrew
   Krejtz, Krzysztof
   Niedzielska, Anna
TI Perceptual Adjustment of Eyeball Rotation and Pupil Size Jitter for
   Virtual Characters
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article; Proceedings Paper
CT 15th ACM Symposium on Applied Perception (SAP) colocated with SIGGRAPH
   Conference
CY AUG, 2018
CL Vancouver, CANADA
SP ACM, SIGGRAPH
DE Character animation; eye motion; perceptual study; jitter; pupil
   dilation
ID EYE-MOVEMENTS; MICROSACCADES; INFORMATION; MOTION; REAL
AB Eye motions constitute an important part of our daily face-to-face interactions. Even subtle details in the eyes' motions give us clues about a person's thoughts and emotions. Believable and natural animation of the eyes is therefore crucial when creating appealing virtual characters. In this article, we investigate the perceived naturalness of detailed eye motions, more specifically of jitter of the eyeball rotation and pupil diameter on three virtual characters with differing levels of realism. Participants watched stimuli with six scaling factors from 0 to 1 in increments of 0.2, varying eye rotation and pupil size jitter individually, and they had to indicate if they would like to increase or decrease the level of jitter to make the animation look more natural. Based on participants' responses, we determine the scaling factors for noise attenuation perceived as most natural for each character when using motion-captured eye motions. We compute the corresponding average jitter amplitudes for the eyeball rotation and pupil size to serve as guidelines for other characters. We find that the amplitudes perceived as most natural depend on the character, with our character with a medium level of realism requiring the largest scaling factors.
C1 [Jorg, Sophie; Duchowski, Andrew] Clemson Univ, Sch Comp, Clemson, SC 29631 USA.
   [Krejtz, Krzysztof; Niedzielska, Anna] SWPS Univ Social Sci & Humanities, Psychol Dept, Warsaw, Poland.
C3 Clemson University; SWPS University of Social Sciences & Humanities
RP Jörg, S (corresponding author), Clemson Univ, Sch Comp, Clemson, SC 29631 USA.
EM sjoerg@clemson.edu; duchowski@siggraph.org; kkrejtz@swps.edu.pl;
   aniedzielska@st.swps.edu.pl
RI Krejtz, Krzysztof/AAP-6165-2021
OI Krejtz, Krzysztof/0000-0002-9558-3039
FU National Science Foundation [IIS-1423189]
FX This material is based in part upon work supported by the National
   Science Foundation under Grant No. IIS-1423189.
CR Ahissar E, 2016, VISION RES, V118, P25, DOI 10.1016/j.visres.2014.12.004
   Andrist S., 2012, Proceedings of the International Conference on Human Factors in Computing, CHI '12, P705
   Bérard P, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925962
   Bradley MM, 2008, PSYCHOPHYSIOLOGY, V45, P602, DOI 10.1111/j.1469-8986.2008.00654.x
   Deng ZG, 2005, IEEE COMPUT GRAPH, V25, P24, DOI 10.1109/MCG.2005.35
   Duchowski Andrew., 2015, P 8 ACM SIGGRAPH C M, P47
   Duchowski AT, 2016, 2016 ACM SYMPOSIUM ON EYE TRACKING RESEARCH & APPLICATIONS (ETRA 2016), P147, DOI 10.1145/2857491.2857528
   Engbert R, 2003, VISION RES, V43, P1035, DOI 10.1016/S0042-6989(03)00084-1
   Engbert R, 2006, PROG BRAIN RES, V154, P177, DOI 10.1016/S0079-6123(06)54009-9
   EVINGER C, 1991, INVEST OPHTH VIS SCI, V32, P387
   Garau M., 2001, CHI 2001 Conference Proceedings. Conference on Human Factors in Computing Systems, P309, DOI 10.1145/365024.365121
   Garau Maia, 2003, P SIGCHI C HUM FACT, P529, DOI DOI 10.1145/642611.642703
   Hodgins J, 2010, ACM T APPL PERCEPT, V7, DOI 10.1145/1823738.1823740
   Hodgins JK, 1998, IEEE T VIS COMPUT GR, V4, P307, DOI 10.1109/2945.765325
   Johansson B, 2018, CONNECT SCI, V30, P5, DOI 10.1080/09540091.2016.1271401
   Jorg S., 2010, P 7 S APPL PERCEPTIO, P129, DOI DOI 10.1145/1836248.1836273
   Krejtz K, 2018, COMPUT ANIMAT VIRT W, V29, DOI 10.1002/cav.1745
   Kret ME, 2013, FRONT HUM NEUROSCI, V7, DOI 10.3389/fnhum.2013.00810
   Lee SP, 2002, ACM T GRAPHIC, V21, P637
   LOEWENFELD I. E., 1958, DOCUMENTA OPHTHALMOL, V12, P185, DOI 10.1007/BF00913471
   Looser CE, 2010, PSYCHOL SCI, V21, P1854, DOI 10.1177/0956797610388044
   LOWENSTEIN O, 1963, INVEST OPHTH VISUAL, V2, P138
   Ma XH, 2009, IEEE VIRTUAL REALITY 2009, PROCEEDINGS, P143, DOI 10.1109/VR.2009.4811014
   MacDorman KF, 2009, COMPUT HUM BEHAV, V25, P695, DOI 10.1016/j.chb.2008.12.026
   Martinez-Conde S, 2004, NAT REV NEUROSCI, V5, P229, DOI 10.1038/nrn1348
   Martinez-Conde S, 2009, TRENDS NEUROSCI, V32, P463, DOI 10.1016/j.tins.2009.05.006
   McDonnell R, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185587
   McDonnell R, 2007, APGV 2007: SYMPOSIUM ON APPLIED PERCEPTION IN GRAPHICS AND VISUALIZATION, PROCEEDINGS, P7
   MCLAREN JW, 1992, INVEST OPHTH VIS SCI, V33, P671
   Normoyle Aline., 2013, Proceedings of motion on games, P141, DOI DOI 10.1145/2522628.25226301,2
   Ohshima T, 1996, P IEEE VIRT REAL ANN, P103, DOI 10.1109/VRAIS.1996.490517
   Peirce JW, 2009, FRONT NEUROINFORM, V2, DOI 10.3389/neuro.11.010.2008
   PRITCHARD R, 1961, SCI AM, V204, P72, DOI 10.1038/scientificamerican0661-72
   Rayner K, 1998, PSYCHOL BULL, V124, P372, DOI 10.1037/0033-2909.124.3.372
   Ritschel T, 2009, COMPUT GRAPH FORUM, V28, P183, DOI 10.1111/j.1467-8659.2009.01357.x
   Ruhland Kerstin., 2014, Eurographics state-of-the-art report, P69, DOI DOI 10.2312/EGST.20141036
   Sagar M, 2016, COMMUN ACM, V59, P82, DOI 10.1145/2950041
   STARK L, 1958, NATURE, V182, P857, DOI 10.1038/182857a0
   Steptoe W, 2010, COMPUT ANIMAT VIRT W, V21, P161, DOI 10.1002/cav.354
   Takashima K., 2008, Proceedings of Graphics Interface 2008, P169, DOI DOI 10.1145/1375714.1375744
   Trutoiu LC, 2011, ACM T APPL PERCEPT, V8, DOI 10.1145/2010325.2010327
   Wang CY, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925947
   Wang D, 2017, BEHAV RES METHODS, V49, P947, DOI 10.3758/s13428-016-0755-8
   Yeo SH, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185538
NR 44
TC 7
Z9 8
U1 1
U2 6
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD OCT
PY 2018
VL 15
IS 4
AR 24
DI 10.1145/3238302
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA HJ4HT
UT WOS:000457135800002
OA Bronze
DA 2024-07-18
ER

PT J
AU Lau, M
   Dev, K
   Dorsey, J
   Rushmeier, H
AF Lau, Manfred
   Dev, Kapil
   Dorsey, Julie
   Rushmeier, Holly
TI A Human-Perceived Softness Measure of Virtual 3D Objects
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE 3D modeling; fabrication; crowdsourcing; learning
ID PERCEPTION
AB We introduce the problem of computing a human-perceived softness measure for virtual 3D objects. As the virtual objects do not exist in the real world, we do not directly consider their physical properties but instead compute the human-perceived softness of the geometric shapes. In an initial experiment, we find that humans are highly consistent in their responses when given a pair of vertices on a 3D model and asked to select the vertex that they perceive to be more soft. This motivates us to take a crowdsourcing and machine learning framework. We collect crowdsourced data for such pairs of vertices. We then combine a learning-to-rank approach and a multi-layer neural network to learn a non-linear softness measure mapping any vertex to a softness value. For a new 3D shape, we can use the learned measure to compute the relative softness of every vertex on its surface. We demonstrate the robustness of our framework with a variety of 3D shapes and compare our non-linear learning approach with a linear method from previous work. Finally, we demonstrate the accuracy of our learned measure with user studies comparing our measure with the human-perceived softness of both virtual and real objects, and we show the usefulness of our measure with some applications.
C1 [Lau, Manfred] Univ Lancaster, Lancaster, England.
   [Lau, Manfred] City Univ Hong Kong, Hong Kong, Hong Kong, Peoples R China.
   [Dev, Kapil] Univ Lancaster, Infolab21, Lancaster, England.
   [Dorsey, Julie; Rushmeier, Holly] Yale Univ, Dept Comp Sci, POB 208285, New Haven, CT 06520 USA.
C3 Lancaster University; City University of Hong Kong; Lancaster
   University; Yale University
RP Lau, M (corresponding author), Run Run Shaw Creat Media Ctr, 18 Tat Hong Ave, Kowloon Tong, Hong Kong, Peoples R China.
EM manfred.lau@gmail.com; kapil.saini@hotmail.com; julie.dorsey@yale.edu;
   holly.rushmeier@yale.edu
RI bodha, kapil deo/HJI-6121-2023
OI Rushmeier, Holly/0000-0001-5241-0886; Dev, Kapil/0000-0002-0908-5114;
   Lau, Manfred/0000-0001-9373-6620
FU Microsoft Research PhD scholarship; NSF [IIS-1064412, IIS-1218515]
FX Kapil Dev was funded by a Microsoft Research PhD scholarship. This work
   was funded in part by NSF grants IIS-1064412 and IIS-1218515.
CR [Anonymous], 2010, ACM SIGGRAPH 2010 papers
   Bächer M, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185543
   Bell S, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766959
   Bickel B, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778800
   Chang A, 2015, PROCEEDINGS OF THE 53RD ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 7TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING, VOL 1, P53
   Chapelle O, 2010, INFORM RETRIEVAL, V13, P201, DOI 10.1007/s10791-009-9109-9
   Chen DY, 2003, COMPUT GRAPH FORUM, V22, P223, DOI 10.1111/1467-8659.00669
   Chen J, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618492
   Chen X, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601189
   Chen XB, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185525
   Cholewiak SA, 2008, SYMPOSIUM ON HAPTICS INTERFACES FOR VIRTUAL ENVIRONMENT AND TELEOPERATOR SYSTEMS 2008, PROCEEDINGS, P87
   Cignoni P, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2537852
   Dominjon L, 2005, P IEEE VIRT REAL ANN, P19
   Drewing K, 2009, WORLD HAPTICS 2009: THIRD JOINT EUROHAPTICS CONFERENCE AND SYMPOSIUM ON HAPTIC INTERFACES FOR VIRTUAL ENVIRONMENT AND TELEOPERATOR SYSTEMS, PROCEEDINGS, P640, DOI 10.1109/WHC.2009.4810828
   Foskey M, 2002, P IEEE VIRT REAL ANN, P119, DOI 10.1109/VR.2002.996514
   Garces E, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601131
   García M, 2010, COMPUT ANIMAT VIRT W, V21, P245, DOI 10.1002/cav.371
   Gingold Y, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2231816.2231817
   Gingold Yotam, 2012, P AAAI WORKSH HUM CO, P1
   Gurari N, 2009, WORLD HAPTICS 2009: THIRD JOINT EUROHAPTICS CONFERENCE AND SYMPOSIUM ON HAPTIC INTERFACES FOR VIRTUAL ENVIRONMENT AND TELEOPERATOR SYSTEMS, PROCEEDINGS, P121, DOI 10.1109/WHC.2009.4810845
   Hildebrand K, 2013, COMPUT GRAPH-UK, V37, P669, DOI 10.1016/j.cag.2013.05.011
   HORN BKP, 1984, P IEEE, V72, P1671, DOI 10.1109/PROC.1984.13073
   Huang QX, 2009, COMPUT GRAPH FORUM, V28, P407, DOI 10.1111/j.1467-8659.2009.01380.x
   Hudson SE, 2014, 32ND ANNUAL ACM CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2014), P459, DOI 10.1145/2556288.2557338
   Jain A, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366162
   Järvelin K, 2002, ACM T INFORM SYST, V20, P422, DOI 10.1145/582415.582418
   Kass M., 1987, Proceedings of the First International Conference on Computer Vision (Cat. No.87CH2465-3), P259, DOI 10.1007/BF00133570
   Lau M, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925927
   Lau M, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964980
   Lau Manfred, 2016, ACM S APPL PERC SAP, P65
   Lecuyer A., 2004, P SIGCHI C HUM FACT, P239, DOI DOI 10.1145/985692.985723
   Liu TQ, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766898
   Lun ZL, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766929
   Martin S, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964967
   MURAKAMI T, 1994, HUMAN FACTORS IN COMPUTING SYSTEMS, CHI '94 CONFERENCE PROCEEDINGS - CELEBRATING INTERDEPENDENCE, P465, DOI 10.1145/191666.191823
   O'Donovan P, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601110
   Osada R, 2001, INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, PROCEEDING, P154, DOI 10.1109/SMA.2001.923386
   Parikh D, 2011, IEEE I CONF COMP VIS, P503, DOI 10.1109/ICCV.2011.6126281
   Peng H, 2015, CHI 2015: PROCEEDINGS OF THE 33RD ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P1789, DOI 10.1145/2702123.2702327
   Petrie H., 2004, INVESTIGATIONS SENSA, P1
   Piovarci M, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925885
   Punpongsanon P, 2015, IEEE T VIS COMPUT GR, V21, P1279, DOI 10.1109/TVCG.2015.2459792
   Sanz Ferran Argelaguet, 2013, ACM T APPL PERCEPT, V10
   Schumacher C, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766926
   Schwartzburg Y, 2013, COMPUT GRAPH FORUM, V32, P317, DOI 10.1111/cgf.12051
   Shilane P, 2004, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, P167, DOI 10.1109/smi.2004.1314504
   Surazhsky T, 2003, IEEE INT CONF ROBOT, P1021, DOI 10.1109/ROBOT.2003.1241726
   Takouachet N, 2012, VISUAL COMPUT, V28, P799, DOI 10.1007/s00371-012-0695-y
   Terzopoulos D., 1987, COMPUT GRAPH, P205, DOI DOI 10.1145/37402.37427
   Thurmer G., 1998, Journal of Graphics Tools, V3, P43, DOI 10.1080/10867651.1998.10487487
   Tiest WMB, 2010, VISION RES, V50, P2775, DOI 10.1016/j.visres.2010.10.005
NR 51
TC 0
Z9 0
U1 0
U2 5
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD AUG
PY 2018
VL 15
IS 3
AR 19
DI 10.1145/3193107
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GY2UV
UT WOS:000448400300005
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Jiang, YY
   O'Neal, EE
   Yon, JP
   Franzen, L
   Rahimian, P
   Plumert, JM
   Kearney, JK
AF Jiang, Yuanyuan
   O'Neal, Elizabeth E.
   Yon, Junghum Paul
   Franzen, Luke
   Rahimian, Pooya
   Plumert, Jodie M.
   Kearney, Joseph K.
TI Acting Together: Joint Pedestrian Road Crossing in an Immersive Virtual
   Environment
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Immersive virtual environments; stereo displays; joint action;
   pedestrian road crossing; co-occupied virtual environments; large screen
   VE; joint affordance
ID BIOLOGICAL MOTION; PERCEPTION; MOVEMENT; BEHAVIOR; AFFORDANCES;
   CHILDREN; MIMICRY; TASK; CUES
AB We investigated how two people jointly coordinate their decisions and actions in a co-occupied, large-screen virtual environment. The task for participants was to physically cross a virtual road with continuous traffic without getting hit by a car. Participants performed this task either alone or with another person (see Figure 1). Two separate streams of non-stereo images were generated based on the dynamic locations of the two viewers' eye-points. Stereo shutter glasses were programmed to display a single image stream to each viewer so that they saw perspectively correct non-stereo images for their eyepoint. We found that participant pairs often crossed the same gap together and closely synchronized their movements when crossing. Pairs also chose larger gaps than individuals, presumably to accommodate the extra time needed to cross through gaps together. These results demonstrate how two people interact and coordinate their behaviors in performing whole-body, joint motions in a co-occupied virtual environment. This study also provides a foundation for future studies examining joint actions in shared VEs where participants are represented by graphic avatars.
C1 [Jiang, Yuanyuan; Yon, Junghum Paul; Franzen, Luke; Rahimian, Pooya; Kearney, Joseph K.] Univ Iowa, Dept Comp Sci, 14 MacLean Hall, Iowa City, IA 52242 USA.
   [O'Neal, Elizabeth E.; Plumert, Jodie M.] Univ Iowa, Iowa City, IA 52242 USA.
   [O'Neal, Elizabeth E.; Plumert, Jodie M.] Dept Psychol & Brain Sci, W311 Seashore Hall, Iowa City, IA 52242 USA.
C3 University of Iowa; University of Iowa
RP Jiang, YY (corresponding author), Univ Iowa, Dept Comp Sci, 14 MacLean Hall, Iowa City, IA 52242 USA.
EM yuanyuan-jiang@uiowa.edu; elizabeth-oneal@uiowa.edu;
   paulyon123@gmail.com; luke-franzen@uiowa.edu; pooya-rahimian@uiowa.edu;
   jodie-plumert@uiowa.edu; joe-kearney@uiowa.edu
RI Jiang, Yuanyuan/IQS-5599-2023; jiang, anyi/GPT-0379-2022
OI Jiang, Yuanyuan/0000-0001-7758-7450; O'Neal,
   Elizabeth/0000-0002-3934-4009
FU National Science Foundation [BCS-1251694, CNS-1305131]; US Department of
   Transportation, Research and Innovative Technology Administration, Prime
   DFDA [20.701, DTRT13-G-UTC53]; Direct For Social, Behav & Economic Scie;
   Division Of Behavioral and Cognitive Sci [1251694] Funding Source:
   National Science Foundation
FX This research was supported by National Science Foundation awards
   BCS-1251694 and CNS-1305131, and by the US Department of Transportation,
   Research and Innovative Technology Administration, Prime DFDA No.
   20.701, Award No. DTRT13-G-UTC53.
CR [Anonymous], 2013, PLOS ONE, DOI DOI 10.1371/J0URNAL.P0NE.0079876
   [Anonymous], 1995, ACM Transactions on Computer-Human Interaction (TOCHI), DOI DOI 10.1145/210079.210088
   Babu SV, 2011, IEEE T VIS COMPUT GR, V17, P14, DOI 10.1109/TVCG.2009.211
   Bailenson JN, 2002, J VISUAL COMP ANIMAT, V13, P313, DOI 10.1002/vis.297
   Blake R, 2007, ANNU REV PSYCHOL, V58, P47, DOI 10.1146/annurev.psych.57.102904.190152
   Brass M, 2001, ACTA PSYCHOL, V106, P3, DOI 10.1016/S0001-6918(00)00024-X
   Chang CH, 2009, J MOTOR BEHAV, V41, P495, DOI 10.3200/35-08-095
   Chartrand TL, 2009, ADV EXP SOC PSYCHOL, V41, P219, DOI 10.1016/S0065-2601(08)00405-X
   Chihak BJ, 2010, J EXP PSYCHOL HUMAN, V36, P1535, DOI 10.1037/a0020560
   CRUZNEIRA C, 1992, COMMUN ACM, V35, P64, DOI 10.1145/129888.129892
   CUTTING JE, 1977, B PSYCHONOMIC SOC, V9, P353, DOI 10.3758/BF03337021
   Duan JY, 2012, TRAFFIC INJ PREV, V13, P442, DOI 10.1080/15389588.2012.655430
   EA MCMANUS., 2011, P ACM SIGGRAPH S APP, P37, DOI DOI 10.1145/2077451.2077458
   Faria JJ, 2010, BEHAV ECOL, V21, P1236, DOI 10.1093/beheco/arq141
   Garau Maia, 2003, P SIGCHI C HUM FACT, P529, DOI DOI 10.1145/642611.642703
   Gibson J., 1979, The ecological approach to visual perception
   Grechkin TY, 2013, J EXP PSYCHOL HUMAN, V39, P23, DOI 10.1037/a0029716
   Guéguen N, 2001, J SOC PSYCHOL, V141, P413, DOI 10.1080/00224540109600562
   Heyes C, 2011, PSYCHOL BULL, V137, P463, DOI 10.1037/a0022288
   Isbister K., 2000, CHI 2000 Conference Proceedings. Conference on Human Factors in Computing Systems. CHI 2000. The Future is Here, P57, DOI 10.1145/332040.332407
   Jiang Yuanyuan, 2016, P S APPL PERC
   Jiang Yuanyuan, 2017, P ACM S APPL PERC
   Kilner JM, 2004, NAT NEUROSCI, V7, P1299, DOI 10.1038/nn1355
   Lakin JL, 2003, PSYCHOL SCI, V14, P334, DOI 10.1111/1467-9280.14481
   LEFKOWITZ M, 1955, J Abnorm Psychol, V51, P704, DOI 10.1037/h0042000
   Normand JM, 2012, PRESENCE-TELEOP VIRT, V21, P229, DOI 10.1162/PRES_a_00089
   Raij AB, 2007, IEEE T VIS COMPUT GR, V13, P443, DOI [10.1109/TVCG.2007.1036, 10.1109/TVCG.2007.1030]
   Ries B., 2009, Proceedings of the 16th ACM Symposium on Virtual Reality Software and Technology, VRST '09, P59, DOI [10.1145/1643928.1643943, DOI 10.1145/1643928.16439433, DOI 10.1145/1643928.1643943]
   Rizzolatti G, 1996, COGNITIVE BRAIN RES, V3, P131, DOI 10.1016/0926-6410(95)00038-0
   Sacheli LM, 2013, EXP BRAIN RES, V226, P473, DOI 10.1007/s00221-013-3459-7
   Sebanz N, 2006, TRENDS COGN SCI, V10, P70, DOI 10.1016/j.tics.2005.12.009
   Slater M, 2000, PRESENCE-TELEOP VIRT, V9, P37, DOI 10.1162/105474600566600
   Stevens E, 2013, J PEDIATR PSYCHOL, V38, P285, DOI 10.1093/jpepsy/jss116
   Troje NF, 2005, PERCEPT PSYCHOPHYS, V67, P667, DOI 10.3758/BF03193523
   Vanrie J, 2004, BEHAV RES METH INS C, V36, P625, DOI 10.3758/BF03206542
   YOUNG DS, 1987, ACCIDENT ANAL PREV, V19, P327, DOI 10.1016/0001-4575(87)90020-0
NR 36
TC 20
Z9 22
U1 1
U2 14
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD APR
PY 2018
VL 15
IS 2
AR 8
DI 10.1145/3147884
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA GH5YV
UT WOS:000433515400001
DA 2024-07-18
ER

PT J
AU Koenderink, J
   Van Doorn, A
   Ekroll, V
AF Koenderink, Jan
   Van Doorn, Andrea
   Ekroll, Vebjorn
TI Color Picking: The Initial 20s
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Color pickers
AB Color pickers are widely used in all kinds of display applications. They vary greatly in their utility, depending on user expertise. We focus on nonprofessional, occasional users. Such users may spend from a few seconds up to a few minutes to select a color. Yet, typically they reach final accuracy within the initial 20s. Additional effort leads to random walks in the neighbor-hood of the target. We explore the efficaciousness of five generic color pickers, analyzing the results in terms of generic user interface properties. There is a major dichotomy between three-slider interfaces, and those that offer some form of 2D selectivity. The accuracy in RGB coordinates is about one-tenth to one- twentieth of the full scale (often 0-255 in R, G, and B), whereas a little over 100 hues are resolved. The most efficient color picker, which is presently rarely used in popular applications, is much more efficient than the worst one. We speculate that this derives from a closer match to the user's internal representation of color space. The study results in explicit recommendations for the implementation of user-friendly and efficient color tools.
C1 [Koenderink, Jan; Ekroll, Vebjorn] Univ Leuven KU Leuven, Dept Brain & Cognit, Lab Expt Psychol, Tiensestr 102,Box 3711, Leuven, Belgium.
   [Van Doorn, Andrea] Univ Utrecht, Expt Psychol, Heidelberglaan 2, NL-3584 CS Utrecht, Netherlands.
C3 KU Leuven; Utrecht University
RP Koenderink, J (corresponding author), Univ Leuven KU Leuven, Dept Brain & Cognit, Lab Expt Psychol, Tiensestr 102,Box 3711, Leuven, Belgium.
EM koenderinkjan@gmail.com; andrea.vandoorn@telfort.nl;
   veb-jorn.ekroll@kuleuven.be
OI Ekroll, Vebjorn/0000-0002-9383-7322
FU Methusalem program by the Flemish Government [METH/08/02]
FX This work was supported by the Methusalem program by the Flemish
   Government (METH/08/02), awarded to Johan Wagemans.
CR Agoston M.K., 2005, COMPUTER GRAPHICS GE
   [Anonymous], 1978, ACM SIGGRAPH COMPUT, DOI [10.1145/800248.807361, DOI 10.1145/965139.807361]
   [Anonymous], 1860, PHILOS T R SOC LOND
   Bratkova M, 2009, IEEE COMPUT GRAPH, V29, P42, DOI 10.1109/MCG.2009.13
   Brundage Barbara, 2012, PHOTOSHOP ELEMENTS
   Cox Brad, 1983, ACM SIGPLAN NOTICES, V18, P1
   Douglas SA, 1999, ACM T GRAPHIC, V18, P96, DOI 10.1145/318009.318011
   Foley J., 2005, COMPUTER GRAPHICS PR
   Geem ZW, 2001, SIMULATION, V76, P60, DOI 10.1177/003754970107600201
   Heller J., 1961, Catch 22
   Hering E., 1964, OUTLINES THEORY LIGH
   Hillegass A., 2008, Cocoa Programming for Mac OS X
   HURVICH LM, 1957, PSYCHOL REV, V64, P384, DOI 10.1037/h0041403
   Joblove G.H., 1978, P 5 ANN C COMP GRAPH, VVolume 12, P20, DOI DOI 10.1145/965139.807362
   Kelly KL, 1976, SPECIAL PUBLICATION, V440
   Koenderink J. J., 2010, Color for the sciences
   Koenderink JJ, 2010, J OPT SOC AM A, V27, P206, DOI 10.1364/JOSAA.27.000206
   LEVKOWITZ H, 1993, CVGIP-GRAPH MODEL IM, V55, P271, DOI 10.1006/cgip.1993.1019
   MacAdam DL, 1943, J OPT SOC AM, V33, P18, DOI 10.1364/JOSA.33.000018
   MacDonald L, 1997, WILEY S DISP TECHNOL, P17
   MacEvoy Bruce, 2014, EXTENSIVE SITE COLOR
   Mobius A. F., 1827, Der barycentrische calcul
   Munsell AR, 1905, A COLOR NOTATION
   Munshell AH, 1912, AM J PSYCHOL, V23, P236, DOI 10.2307/1412843
   Newton I., 1998, OPTICKS TREATISE REF
   Ostwald W., 1917, DIE FARBENFIBEL
   Ostwald Wilhelm, 1919, FARBKOERPER SEINE AN
   Poynton C, 1998, P SOC PHOTO-OPT INS, V3299, P232, DOI 10.1117/12.320126
   Runge PhilippOtto., 1810, FARBEN KUGEL ODER CO
   Schopenhauer A, 1994, ON VISION AND COLORS
   SCHWARZ MW, 1987, ACM T GRAPHIC, V6, P123, DOI 10.1145/31336.31338
   Smith A. R., 1996, J GRAPHICS GPU GAME, V1, P3
   von Goethe JohannWolfgang., 1982, Theory of Colours
   Von Helmholtz H., 1867, Handbuch Der Physiologischen Optik
   Wyszecki Gunther, 1928, COLOR SCI
   Yule John A. C., 2001, PRINCIPLES COLOR REP
   Zimmer Mark, 2012, RELATIVISTIC OBSERVE
NR 37
TC 7
Z9 8
U1 0
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD MAY
PY 2016
VL 13
IS 3
AR 13
DI 10.1145/2883613
PG 25
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DV4DU
UT WOS:000382876200003
DA 2024-07-18
ER

PT J
AU Rigas, I
   Komogortsev, O
   Shadmehr, R
AF Rigas, Ioannis
   Komogortsev, Oleg
   Shadmehr, Reza
TI Biometric Recognition via Eye Movements: Saccadic Vigor and Acceleration
   Cues
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Human Factors; Eye movement biometrics; saccadic vigor; saccadic
   acceleration
ID BINOCULAR COORDINATION; VARIABILITY; PERCEPTION; REWARD; ADAPTATION;
   SCANPATHS; ATTENTION; AMPLITUDE; ACCURACY; DYNAMICS
AB Previous research shows that human eye movements can serve as a valuable source of information about the structural elements of the oculomotor system and they also can open a window to the neural functions and cognitive mechanisms related to visual attention and perception. The research field of eye movement-driven biometrics explores the extraction of individual-specific characteristics from eye movements and their employment for recognition purposes. In this work, we present a study for the incorporation of dynamic saccadic features into a model of eye movement-driven biometrics. We show that when these features are added to our previous biometric framework and tested on a large database of 322 subjects, the biometric accuracy presents a relative improvement in the range of 31.6-33.5% for the verification scenario, and in range of 22.3-53.1% for the identification scenario. More importantly, this improvement is demonstrated for different types of visual stimulus (random dot, text, video), indicating the enhanced robustness offered by the incorporation of saccadic vigor and acceleration cues.
C1 [Rigas, Ioannis; Komogortsev, Oleg] SW Texas State Univ, Dept Comp Sci, 601 Univ Dr, San Marcos, TX 78666 USA.
   [Shadmehr, Reza] Johns Hopkins Univ, 720 Rutland Ave,410 Traylor Bldg, Baltimore, MD 21205 USA.
C3 Texas State University System; Texas State University San Marcos; Johns
   Hopkins University
RP Rigas, I; Komogortsev, O (corresponding author), SW Texas State Univ, Dept Comp Sci, 601 Univ Dr, San Marcos, TX 78666 USA.; Shadmehr, R (corresponding author), Johns Hopkins Univ, 720 Rutland Ave,410 Traylor Bldg, Baltimore, MD 21205 USA.
EM rigas@txstate.edu; ok11@txstate.edu; shadmehr@jhu.edu
FU NSF CAREER Grant [CNS-1250718]; NIST [60NANB14D274]
FX This work is supported in part by the NSF CAREER Grant #CNS-1250718 and
   the NIST Grant #60NANB14D274. Special gratitude is expressed to Dr.
   Evgeny Abdulin, T. Miller, Ch. Heinich, and N. Myers for proctoring eye
   movement recordings.
CR Abdulin Evgeniy., 2015, Proceedings of the 33rd Annual ACM Conference Extended Abstracts on Human Factors in Computing Systems, P1265, DOI DOI 10.1145/2702613.2732812
   ABRAMS RA, 1989, J EXP PSYCHOL HUMAN, V15, P529, DOI 10.1037/0096-1523.15.3.529
   [Anonymous], 1935, How people look at pictures: A study of the psychology and perception in art
   [Anonymous], 2013, P 2013 INT C BIOM IC, DOI DOI 10.1109/ICB.2013.6612953
   [Anonymous], 2016, ACM T APPL PERCEPT, V13
   [Anonymous], ACM T APPL PERCEPT
   BAHILL A T, 1975, Mathematical Biosciences, V27, P287, DOI 10.1016/0025-5564(75)90107-8
   BAHILL A T, 1975, Mathematical Biosciences, V24, P191, DOI 10.1016/0025-5564(75)90075-9
   BAHILL AT, 1983, IEEE T BIO-MED ENG, V30, P191, DOI 10.1109/TBME.1983.325108
   BAHILL AT, 1981, INVEST OPHTH VIS SCI, V21, P116
   BALOH RW, 1975, NEUROLOGY, V25, P1065, DOI 10.1212/WNL.25.11.1065
   Bednarik R, 2005, LECT NOTES COMPUT SC, V3540, P780
   BOLLEN E, 1993, INVEST OPHTH VIS SCI, V34, P3700
   Breiman L., 2001, Machine Learning, V45, P5, DOI 10.1023/A:1010933404324
   Cantoni V, 2015, PATTERN RECOGN, V48, P1027, DOI 10.1016/j.patcog.2014.02.017
   CARLTON LG, 1988, J EXP PSYCHOL HUMAN, V14, P24
   Choi JES, 2014, J NEUROSCI, V34, P1212, DOI 10.1523/JNEUROSCI.2798-13.2014
   Cifu DX, 2015, J HEAD TRAUMA REHAB, V30, P21, DOI 10.1097/HTR.0000000000000036
   COLLEWIJN H, 1988, J PHYSIOL-LONDON, V404, P183, DOI 10.1113/jphysiol.1988.sp017285
   COLLEWIJN H, 1988, J PHYSIOL-LONDON, V404, P157, DOI 10.1113/jphysiol.1988.sp017284
   Collins T, 2008, INVEST OPHTH VIS SCI, V49, P604, DOI 10.1167/iovs.07-0753
   Collins T, 2006, VISION RES, V46, P3659, DOI 10.1016/j.visres.2006.04.004
   Eckstein MP, 2007, J NEUROSCI, V27, P1266, DOI 10.1523/JNEUROSCI.3975-06.2007
   Fawcett T, 2006, PATTERN RECOGN LETT, V27, P861, DOI 10.1016/j.patrec.2005.10.010
   FRICKER SJ, 1971, INVEST OPHTH VISUAL, V10, P724
   Haith AM, 2012, J NEUROSCI, V32, P11727, DOI 10.1523/JNEUROSCI.0424-12.2012
   HOLCOMB JH, 1977, PERCEPT MOTOR SKILL, V44, P639, DOI 10.2466/pms.1977.44.2.639
   Holland CD, 2013, IEEE T INF FOREN SEC, V8, P2115, DOI 10.1109/TIFS.2013.2285884
   Ikeda T, 2007, J NEUROPHYSIOL, V98, P3163, DOI 10.1152/jn.00975.2007
   Javal E., 1878, ANN DOCULISTIQUE, V79, P97
   Javal E., 1878, ANN OCUL, P135
   Jiao YY, 2014, IEEE IJCNN, P4035, DOI 10.1109/IJCNN.2014.6889615
   JUST MA, 1980, PSYCHOL REV, V87, P329, DOI 10.1037/0033-295X.87.4.329
   Kasprowski P, 2004, LECT NOTES COMPUT SC, V3087, P248
   Kinnunen T, 2010, P S EYE TRACK RES AP, P187
   Komogortsev O. V., 2012, 2012 IEEE Fifth International Conference On Biometrics: Theory, Applications And Systems (BTAS 2012), P209, DOI 10.1109/BTAS.2012.6374579
   Komogortsev O. V., 2012, 2012 5th IAPR International Conference on Biometrics (ICB), P413, DOI 10.1109/ICB.2012.6199786
   Komogortsev O. V., 2014, BIOMETRIC SURVEILLAN, VXI
   Komogortsev Oleg, 2014, CHI 14 EXTENDED ABST, P1711, DOI [10.1145/2559206.2581150, DOI 10.1145/2559206.2581150]
   Komogortsev OV, 2015, IEEE T INF FOREN SEC, V10, P716, DOI 10.1109/TIFS.2015.2405345
   KOWLER E, 1995, VISION RES, V35, P1897, DOI 10.1016/0042-6989(94)00279-U
   Leigh RJ., 2006, NEUROLOGY EYE MOVEME, V4th
   Di Stasi LL, 2011, APPL ERGON, V42, P807, DOI 10.1016/j.apergo.2011.01.003
   Marcel S, 2014, ADV COMPUT VIS PATT, P1, DOI 10.1007/978-1-4471-6524-8
   Miltner WHR, 2004, EMOTION, V4, P323, DOI 10.1037/1528-3542.4.4.323
   Nandakumar K, 2008, IEEE T PATTERN ANAL, V30, P342, DOI 10.1109/TPAMI.2007.70796
   Nawrot M, 2004, PSYCHOL SCI, V15, P858, DOI 10.1111/j.0956-7976.2004.00767.x
   Niinuma K, 2010, IEEE T INF FOREN SEC, V5, P771, DOI 10.1109/TIFS.2010.2075927
   NOTON D, 1971, VISION RES, V11, P929, DOI 10.1016/0042-6989(71)90213-6
   NOTON D, 1971, SCIENCE, V171, P308, DOI 10.1126/science.171.3968.308
   Rigas I., 2012, 2012 IEEE Fifth International Conference On Biometrics: Theory, Applications And Systems (BTAS 2012), P217, DOI 10.1109/BTAS.2012.6374580
   Rigas I., 2015, INFORM FUSION
   Rigas I, 2014, IEEE T INF FOREN SEC, V9, P1743, DOI 10.1109/TIFS.2014.2350960
   Rigas I, 2012, PATTERN RECOGN LETT, V33, P786, DOI 10.1016/j.patrec.2012.01.003
   ROBINSON DA, 1964, J PHYSIOL-LONDON, V174, P245, DOI 10.1113/jphysiol.1964.sp007485
   Salvucci D.D., 2000, P 2000 S EYE TRACK R, P71, DOI [DOI 10.1145/355017.355028, 10.1145/355017.355028]
   Schnitzer BS, 2006, VISION RES, V46, P1611, DOI 10.1016/j.visres.2005.09.023
   Schütz AC, 2011, J VISION, V11, DOI 10.1167/11.5.9
   Shadmehr R, 2010, J NEUROSCI, V30, P10507, DOI 10.1523/JNEUROSCI.1343-10.2010
   Snelick R, 2005, IEEE T PATTERN ANAL, V27, P450, DOI 10.1109/TPAMI.2005.57
   THOMAS JG, 1969, J PHYSIOL-LONDON, V200, P109, DOI 10.1113/jphysiol.1969.sp008684
   TOBII, 2015, GLASS 2
   van Beers RJ, 2007, J NEUROSCI, V27, P8757, DOI 10.1523/JNEUROSCI.2311-07.2007
   Yarbus A.L., 1967, EYE MOVEMENTS VISION, DOI [10.1007/978-1-4899-5379-7, DOI 10.1007/978-1-4899-5379-7]
   Yoon H.-J., 2014, SPIE
   Zhang Y., 2012, P THE 2 INT C ADV IN, P85
   ZUBER BL, 1965, SCIENCE, V150, P1459, DOI 10.1126/science.150.3702.1459
NR 67
TC 32
Z9 35
U1 1
U2 7
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD MAR
PY 2016
VL 13
IS 2
AR 6
DI 10.1145/2842614
PG 21
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DJ0OM
UT WOS:000373903800001
OA Bronze
DA 2024-07-18
ER

PT J
AU Kiiski, H
   Hoyet, L
   Woods, AT
   O'Sullivan, C
   Newell, FN
AF Kiiski, Hanni
   Hoyet, Ludovic
   Woods, Andy T.
   O'Sullivan, Carol
   Newell, Fiona N.
TI Strutting Hero, Sneaking Villain: Utilizing Body Motion Cues to Predict
   the Intentions of Others
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Human Factors; Experimentation; Body motion; traits; intentions; social
   inferences; cognitive dimensions; "Effort Shape" analysis; virtual
   humans
ID BODILY EXPRESSION; EFFORT-SHAPE; POINT-LIGHT; THIN SLICES; PERCEPTION;
   EMOTION; FACES; DISTINCTIVENESS; ATTRACTIVENESS; RECOGNITION
AB A better understanding of how intentions and traits are perceived from body movements is required for the design of more effective virtual characters that behave in a socially realistic manner. For this purpose, realistic body motion, captured from human movements, is being used more frequently for creating characters with natural animations in games and entertainment. However, it is not always clear for programmers and designers which specific motion parameters best convey specific information such as certain emotions, intentions, or traits. We conducted two experiments to investigate whether the perceived traits of actors could be determined from their body motion, and whether these traits were associated with their perceived intentions. We first recorded body motions from 26 professional actors, who were instructed to move in a "hero"-like or a "villain"-like manner. In the first experiment, 190 participants viewed individual video recordings of these actors and were required to provide ratings to the body motion stimuli along a series of different cognitive dimensions (intentions, attractiveness, dominance, trustworthiness, and distinctiveness). The intersubject ratings across observers were highly consistent, suggesting that social traits are readily determined from body motion. Moreover, correlational analyses between these ratings revealed consistent associations across traits, for example, that perceived "good" intentions were associated with higher ratings of attractiveness and dominance. Experiment 2 was designed to elucidate the qualitative body motion cues that were critical for determining specific intentions and traits from the hero- and villain-like body movements. The results revealed distinct body motions that were readily associated with the perception of either "good" or "bad" intentions. Moreover, regression analyses revealed that these ratings accurately predicted the perception of the portrayed character type. These findings indicate that intentions and social traits are communicated effectively via specific sets of body motion features. Furthermore, these results have important implications for the design of the motion of virtual characters to convey desired social information.
C1 [Kiiski, Hanni; Newell, Fiona N.] Trinity Coll Dublin, Sch Psychol, Dublin 2, Ireland.
   [Kiiski, Hanni; Newell, Fiona N.] Trinity Coll Dublin, Inst Neurosci, Dublin 2, Ireland.
   [Hoyet, Ludovic] Trinity Coll Dublin, Sch Stat & Comp Sci, Dublin 2, Ireland.
   [Woods, Andy T.] Xperiment Mobi, CH-1010 Lausanne, Switzerland.
   [O'Sullivan, Carol] Disney Res, Los Angeles, CA USA.
C3 Trinity College Dublin; Trinity College Dublin; Trinity College Dublin
RP Kiiski, H (corresponding author), Trinity Coll Dublin, Inst Neurosci, Dublin 2, Ireland.
EM hkiiski@tcd.ie; hoyetl@tcd.ie; Andy.Woods@xperiment.mobi;
   Carol.OSullivan@scss.tcd.ie; Fiona.Newell@tcd.ie
RI Kiiski, Hanni/O-2827-2014; Hoyet, Ludovic/IWU-9100-2023; Newell,
   Fiona/AIE-2422-2022
OI Hoyet, Ludovic/0000-0002-7373-6049; Newell, Fiona/0000-0002-7363-2346;
   O'Sullivan, Carol/0000-0003-3772-4961; Woods, Andy/0000-0003-3797-3845
FU Science Foundation Ireland Principal Investigator Award [S.F.I.
   10/IN.1/13003]
FX This work is supported by a Science Foundation Ireland Principal
   Investigator Award to FNN and CO'S (S.F.I. 10/IN.1/13003).
CR ALBRIGHT L, 1988, J PERS SOC PSYCHOL, V55, P387, DOI 10.1037/0022-3514.55.3.387
   Allison T, 2000, TRENDS COGN SCI, V4, P267, DOI 10.1016/S1364-6613(00)01501-1
   AMBADY N, 1993, J PERS SOC PSYCHOL, V64, P431, DOI 10.1037/0022-3514.64.3.431
   [Anonymous], PLOS ONE
   [Anonymous], PRIM MOV DESCR
   Atkinson AP, 2004, PERCEPTION, V33, P717, DOI 10.1068/p5096
   Borkenau P, 2004, J PERS SOC PSYCHOL, V86, P599, DOI 10.1037/0022-3514.86.4.599
   Brainard DH, 1997, SPATIAL VISION, V10, P433, DOI 10.1163/156856897X00357
   Chi D, 2000, COMP GRAPH, P173, DOI 10.1145/344779.352172
   Cornwell RE, 2006, PHILOS T R SOC B, V361, P2143, DOI 10.1098/rstb.2006.1936
   Crane EA, 2013, J NONVERBAL BEHAV, V37, P91, DOI 10.1007/s10919-013-0144-2
   Crump MJC, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0057410
   Dael N, 2012, EMOTION, V12, P1085, DOI 10.1037/a0025737
   de Gelder B, 2010, NEUROSCI BIOBEHAV R, V34, P513, DOI 10.1016/j.neubiorev.2009.10.008
   DION K, 1972, J PERS SOC PSYCHOL, V24, P285, DOI 10.1037/h0033731
   Fink B, 2010, PERS INDIV DIFFER, V49, P436, DOI 10.1016/j.paid.2010.04.013
   FLEISHMAN JJ, 1976, PERCEPT MOTOR SKILL, V43, P709, DOI 10.2466/pms.1976.43.3.709
   Freeman JB, 2012, FRONT INTEGR NEUROSC, V6, DOI 10.3389/fnint.2012.00081
   Germine L, 2012, PSYCHON B REV, V19, P847, DOI 10.3758/s13423-012-0296-9
   Gross MM, 2012, HUM MOVEMENT SCI, V31, P202, DOI 10.1016/j.humov.2011.05.001
   Gross MM, 2010, J NONVERBAL BEHAV, V34, P223, DOI 10.1007/s10919-010-0094-x
   Gunns RE, 2002, J NONVERBAL BEHAV, V26, P129, DOI 10.1023/A:1020744915533
   Hamlin JK, 2007, NATURE, V450, P557, DOI 10.1038/nature06288
   Hamlin JK, 2011, P NATL ACAD SCI USA, V108, P19931, DOI 10.1073/pnas.1110306108
   Hoyet L, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508367
   Hugill N, 2010, EVOL PSYCHOL-US, V8, P66
   Johnson KL, 2005, PSYCHOL SCI, V16, P890, DOI 10.1111/j.1467-9280.2005.01633.x
   Kleiner M, 2007, PERCEPTION, V36, P14
   Koppensteiner M, 2011, PERS INDIV DIFFER, V51, P743, DOI 10.1016/j.paid.2011.06.014
   Loula F, 2005, J EXP PSYCHOL HUMAN, V31, P210, DOI 10.1037/0096-1523.31.1.210
   Manera V, 2011, EXP BRAIN RES, V211, P547, DOI 10.1007/s00221-011-2649-4
   MONTEPARE JM, 1988, J PERS SOC PSYCHOL, V55, P547, DOI 10.1037/0022-3514.55.4.547
   Newell FN, 1999, Q J EXP PSYCHOL-A, V52, P509, DOI 10.1080/027249899391179
   Oosterhof NN, 2008, P NATL ACAD SCI USA, V105, P11087, DOI 10.1073/pnas.0805664105
   Pelli DG, 1997, SPATIAL VISION, V10, P437, DOI 10.1163/156856897X00366
   Rhodes G, 1996, PSYCHOL SCI, V7, P105, DOI 10.1111/j.1467-9280.1996.tb00338.x
   Roether CL, 2009, J VISION, V9, DOI 10.1167/9.6.15
   RUNESON S, 1983, J EXP PSYCHOL GEN, V112, P585, DOI 10.1037/0096-3445.112.4.585
   SADALLA EK, 1987, J PERS SOC PSYCHOL, V52, P730, DOI 10.1037/0022-3514.52.4.730
   Scherer KR, 2007, EMOTION, V7, P158, DOI 10.1037/1528-3542.7.1.158
   Schindler K, 2008, NEURAL NETWORKS, V21, P1238, DOI 10.1016/j.neunet.2008.05.003
   SHAPIRO PN, 1986, PSYCHOL BULL, V100, P139, DOI 10.1037/0033-2909.100.2.139
   Sutherland CAM, 2013, COGNITION, V127, P105, DOI 10.1016/j.cognition.2012.12.001
   Thoresen JC, 2012, COGNITION, V124, P261, DOI 10.1016/j.cognition.2012.05.018
   Todorov A, 2008, TRENDS COGN SCI, V12, P455, DOI 10.1016/j.tics.2008.10.001
   VALENTINE T, 1991, Q J EXP PSYCHOL-A, V43, P161, DOI 10.1080/14640749108400966
   Valentine T, 2016, Q J EXP PSYCHOL, V69, P1996, DOI 10.1080/17470218.2014.990392
   Wallbott HG, 1998, EUR J SOC PSYCHOL, V28, P879, DOI 10.1002/(SICI)1099-0992(1998110)28:6<879::AID-EJSP901>3.0.CO;2-W
   WISEMAN R, 1995, NATURE, V373, P391, DOI 10.1038/373391a0
   ZEBROWITZ LA, 1991, LAW HUMAN BEHAV, V15, P603, DOI 10.1007/BF01065855
   Zebrowitz LA, 2008, SOC PERSONAL PSYCHOL, V2, P1497, DOI 10.1111/j.1751-9004.2008.00109.x
NR 51
TC 2
Z9 2
U1 0
U2 13
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD DEC
PY 2015
VL 13
IS 1
AR 1
DI 10.1145/2791293
PG 21
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA DD2SV
UT WOS:000369773400001
DA 2024-07-18
ER

PT J
AU Krejtz, K
   Duchowski, A
   Szmidt, T
   Krejtz, I
   Perillj, FG
   Fires, A
   Vilaro, A
   Villalobos, N
AF Krejtz, Krzysztof
   Duchowski, Andrew
   Szmidt, Tomasz
   Krejtz, Izabela
   Perillj, Fernando Gonzalez
   Fires, Ana
   Vilaro, Anna
   Villalobos, Natalia
TI Gaze Transition Entropy
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Human Factors; Eye movement transitions; entropy; Markov chain
ID EYE-MOVEMENTS
AB This article details a two-step method of quantifying eye movement transitions between areas of interest (AOIs). First, individuals' gaze switching patterns, represented by fixated AOI sequences, are modeled as Markov chains. Second, Shannon's entropy coefficient of the fit Markov model is computed to quantify the complexity of individual switching patterns. To determine the overall distribution of attention over AOIs, the entropy coefficient of individuals' stationary distribution of fixations is calculated. The novelty of the method is that it captures the variability of individual differences in eye movement characteristics, which are then summarized statistically. The method is demonstrated on gaze data collected from two studies, during free viewing of classical art paintings. Normalized Shannon's entropy, derived from individual transition matrices, is related to participants' individual differences as well as to either their aesthetic impression or recognition of artwork. Low transition and high stationary entropies suggest greater curiosity mixed with a higher subjective aesthetic affinity toward artwork, possibly indicative of visual scanning of the artwork in a more deliberate way. Meanwhile, both high transition and stationary entropies may be indicative of recognition of familiar artwork.
C1 [Krejtz, Krzysztof; Krejtz, Izabela] Univ Social Sci & Human, Warsaw, Poland.
   [Krejtz, Krzysztof] Natl Informat Proc Inst, Warsaw, Poland.
   [Duchowski, Andrew] Clemson Univ, Clemson, SC USA.
   [Szmidt, Tomasz] Polish Acad Sci, Inst Fundamental Technol Res, Warsaw, Poland.
   [Perillj, Fernando Gonzalez; Fires, Ana] Univ Autonoma Barcelona, Ctr Basic Res Psychol, Univ Republ Uruguay, E-08193 Barcelona, Spain.
   [Perillj, Fernando Gonzalez; Fires, Ana] Univ Autonoma Barcelona, Dept Bas Evolutionary & Educ Psychol, E-08193 Barcelona, Spain.
   [Vilaro, Anna] Univ Autonoma Barcelona, Ctr Res Ambient Intelligence & Accessibil Catalon, E-08193 Barcelona, Spain.
   [Villalobos, Natalia] Univ Republ Uruguay, Ctr Basic Res Psychol, Montevideo, Uruguay.
C3 SWPS University of Social Sciences & Humanities; Information Processing
   Center - National Research Institute; Clemson University; Polish Academy
   of Sciences; Institute of Fundamental Technological Research of the
   Polish Academy of Sciences; Autonomous University of Barcelona;
   Autonomous University of Barcelona; Autonomous University of Barcelona;
   Universidad de la Republica, Uruguay
RP Krejtz, K (corresponding author), Chodakowska 19-31, PL-03815 Warsaw, Poland.
EM kkrejtz@swps.edu.pl; duchowski@clemson.edu; tomasz.szmidt@gmail.com;
   ikrejtz@swps.edu.pl; fernando.gonzalez@psico.edu.uy;
   apires@psico.edu.uy; annavilaro@gmail.com;
   natvillalobosmanriquez@gmail.com
RI Krejtz, Izabela/H-3411-2016; Krejtz, Krzysztof/AAP-6165-2021; Krejtz,
   Izabela/AAC-9145-2021; Szmidt, Tomasz/T-5517-2017; Gonzalez-Perilli,
   Fernando/J-8023-2016
OI Krejtz, Krzysztof/0000-0002-9558-3039; Krejtz,
   Izabela/0000-0002-9827-8371; Pires, Ana Cristina/0000-0001-7747-7112;
   Szmidt, Tomasz/0000-0002-2676-1882; Gonzalez-Perilli,
   Fernando/0000-0001-5832-716X
CR Acarturk C., 2012, WORKSH TECHN ENH DIA
   Althoff R. R, 1998, THESIS U ILLINOIS UR
   [Anonymous], P 2 INT WORKSH EYE T
   [Anonymous], 2006, Elements of Information Theory
   [Anonymous], 2013, P C WAS F 2013 C EYE
   [Anonymous], P INT C MULT INT DES
   Bednarik R, 2005, 2005 IEEE SYMPOSIUM ON VISUAL LANGUAGE AND HUMAN-CENTRIC COMPUTING, PROCEEDINGS, P302, DOI 10.1109/VLHCC.2005.20
   Besag J, 2013, BIOMETRICS, V69, P488, DOI 10.1111/biom.12009
   Ciuperca G., 2005, P 11 INT S APPL STOC
   Duchowski AndrewT., 2010, P 2010 S EYE TRACKIN, P219, DOI DOI 10.1145/1743666.1743719
   EKROOT L, 1993, IEEE T INFORM THEORY, V39, P1418, DOI 10.1109/18.243461
   ELLIS SR, 1986, HUM FACTORS, V28, P421, DOI 10.1177/001872088602800405
   Feixas M, 1999, COMPUT GRAPH FORUM, V18, pC95, DOI 10.1111/1467-8659.00331
   Fischer Peter, 2007, P EUR C EYE MOV ECEM
   Goldberg JH, 1999, INT J IND ERGONOM, V24, P631, DOI 10.1016/S0169-8141(98)00068-7
   Grindinger T., 2010, Proceedings of the 2010 Symposium on Eye-Tracking Research Applications, P101, DOI [DOI 10.1145/1743666.1743691, 10.1145/1743666.1743691]
   Grindinger Thomas J., 2010, Computer Vision. International Workshops (ACCV 2010). Revised Selected Papers, P390, DOI 10.1007/978-3-642-22822-3_39
   Hausser J, 2009, J MACH LEARN RES, V10, P1469
   Hume E., 2013, ORIGINS SOUND PATTER, P29
   Hwang AD, 2011, VISION RES, V51, P1192, DOI 10.1016/j.visres.2011.03.010
   Jarodzka H., 2010, P S EYE TRACK RES AP, P211, DOI [10.1145/1743666, DOI 10.1145/1743666, DOI 10.1145/1743666.1743718]
   Kashdan TB, 2009, J RES PERS, V43, P987, DOI 10.1016/j.jrp.2009.04.011
   Kruizinga Anje, 2006, DEV HUMAN FACTORS TR
   KULLBACK S, 1951, ANN MATH STAT, V22, P79, DOI 10.1214/aoms/1177729694
   Liechty J, 2003, PSYCHOMETRIKA, V68, P519, DOI 10.1007/BF02295608
   PONSODA V, 1995, ACTA PSYCHOL, V88, P167, DOI 10.1016/0001-6918(95)94012-Y
   R Development Core Team, 2011, R LANG ENV STAT COMP
   Shic F, 2008, INT C DEVEL LEARN, P73, DOI 10.1109/DEVLRN.2008.4640808
   Vandeberg L, 2013, J MEM LANG, V69, P445, DOI 10.1016/j.jml.2013.05.006
   Velichkovsky Boris M, 2005, P 27 C COGN SCI SOC, V1
   WEISS RS, 1989, BEHAV RES METH INSTR, V21, P348, DOI 10.3758/BF03202796
   [No title captured]
NR 32
TC 75
Z9 85
U1 1
U2 14
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD DEC
PY 2015
VL 13
IS 1
AR 4
DI 10.1145/2834121
PG 20
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA DD2SV
UT WOS:000369773400004
DA 2024-07-18
ER

PT J
AU Stenholt, R
AF Stenholt, Rasmus
TI On the Benefits of Using Constant Visual Angle Glyphs in Interactive
   Exploration of 3D Scatterplots
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Human Factors; 3D shape perception; glyphs; scatterplots; visualization
ID DESK-TOP; VISUALIZATION; INVARIANCE; OBJECTS; DEPTH; CUES
AB Visual exploration of clouds of data points is an important application of virtual environments. The common goal of this activity is to use the strengths of human perception to identify interesting structures in data, which are often not detected using traditional, computational analysis methods. In this article, we seek to identify some of the parameters that affect how well structures in visualized data clouds can be identified by a human observer. Two of the primary factors tested are the volumetric densities of the visualized structures and the presence/absence of clutter around the displayed structures. Furthermore, we introduce a new approach to glyph visualization-constant visual angle (CVA) glyphs-which has the potential to mitigate the effect of clutter at the cost of dispensing with the common real-world depth cue of relative size. In a controlled experiment where test subjects had to locate and select visualized structures in an immersive virtual environment, we identified several significant results. One result is that CVA glyphs ease perception of structures in cluttered environments while not deteriorating it when clutter is absent. Another is the existence of threshold densities, above which perception of structures becomes easier and more precise.
C1 [Stenholt, Rasmus] Aalborg Univ, Dept Architecture Design & Media Technol, DK-9000 Aalborg, Denmark.
C3 Aalborg University
RP Stenholt, R (corresponding author), Aalborg Univ, Dept Architecture Design & Media Technol, Rendsburggade 14, DK-9000 Aalborg, Denmark.
EM rs@create.aau.dk
FU Fonden Vision Nord
FX The author wishes to thank Fonden Vision Nord for generously donating
   the funds needed to acquire some of the equipment used in this study.
CR [Anonymous], 2018, The Visual Display of Quantitative Information
   [Anonymous], 2014, ACM T APPL PERCEPTIO, V11
   BIEDERMAN I, 1993, J EXP PSYCHOL HUMAN, V19, P1162, DOI 10.1037/0096-1523.19.6.1162
   BLESSING WW, 1967, AM J PSYCHOL, V80, P250, DOI 10.2307/1420984
   Bowman D.A., 2005, 3D User Interfaces: Theory and Practice
   Buja A., 1991, Proceedings Visualization '91 (Cat. No.91CH3046-0), P156, DOI 10.1109/VISUAL.1991.175794
   Carpendale MST, 1996, IEEE SYMPOSIUM ON INFORMATION VISUALIZATION '96, PROCEEDINGS, P46, DOI 10.1109/INFVIS.1996.559215
   Carpenter L., 1984, ACM SIGGRAPH COMPUTE, V18, P103
   Cutting JE, 1995, PERCEPTION SPACE MOT, P69, DOI [DOI 10.1016/B978-012240530-3/50005-5, 10.1016/B978-012240530-3/50005-5]
   DONOHO AW, 1988, IEEE COMPUT GRAPH, V8, P51, DOI 10.1109/38.7749
   Elmqvist N, 2007, COMPUT GRAPH-UK, V31, P864, DOI 10.1016/j.cag.2007.09.006
   Emmert E., 1881, KLINISCHEMONATSBL TT, V19, P443
   Ericson C., 2005, Real-time collision detection
   Feng D., 2009, P 6 S APPL PERC GRAP, P61
   Forsell C., 2006, Information Visualization, V5, P112, DOI 10.1057/palgrave.ivs.9500119
   Granum E, 2002, CONSTRUCTING VIRTUAL, P112
   Gross M., 2007, POINT BASED GRAPHICS
   Healey CG, 2004, ACM T GRAPHIC, V23, P64, DOI 10.1145/966131.966135
   Irani P., 2003, ACM Transactions on Computer-Human Interaction, V10, P1, DOI 10.1145/606658.606659
   Isenberg P, 2009, COMPUT GRAPH FORUM, V28, P1031, DOI 10.1111/j.1467-8659.2009.01444.x
   Koffka Kurt, 1935, PRINCIPLES GESTALT P, V7
   Kratz Andrea, 2006, P WORKSH AUGM ENV ME
   Krüger J, 2005, IEEE T VIS COMPUT GR, V11, P744, DOI 10.1109/TVCG.2005.87
   Mazeika A, 2008, LECT NOTES COMPUT SC, V4404, P91, DOI 10.1007/978-3-540-71080-6_7
   Montgomery D., 2013, DESIGN ANAL EXPT, V8
   Nagel HR, 2008, LECT NOTES COMPUT SC, V4404, P281, DOI 10.1007/978-3-540-71080-6_18
   Nagel Henrik R., 2005, THESIS AALBORG U
   Norman G, 2010, ADV HEALTH SCI EDUC, V15, P625, DOI 10.1007/s10459-010-9222-y
   Polys NF, 2011, INT J HUM-COMPUT ST, V69, P30, DOI 10.1016/j.ijhcs.2010.05.007
   Qi W., 2006, Proc. Symp. on Applied Perception in Graphics and Visualization (APGV), P51, DOI DOI 10.1145/1140491.1140502
   R Development Core Team, 2011, R LANG ENV STAT COMP
   Ropinski T, 2011, COMPUT GRAPH-UK, V35, P392, DOI 10.1016/j.cag.2011.01.011
   SHAH P, 1995, J EXP PSYCHOL GEN, V124, P43, DOI 10.1037/0096-3445.124.1.43
   Simoff SJ, 2008, LECT NOTES COMPUT SC, V4404, P1, DOI 10.1007/978-3-540-71080-6
   Stenholt R, 2011, P IEEE VIRT REAL ANN, P103, DOI 10.1109/VR.2011.5759445
   Suffern K., 2007, Ray tracing from the ground up
   TREISMAN A, 1982, J EXP PSYCHOL HUMAN, V8, P194, DOI 10.1037/0096-1523.8.2.194
   Ulinski Amy, 2007, P IEEE S US INT 3DUI
   Ulinski Amy Catherine, 2008, THESIS U N CAROLINA
   Viega John., 1996, Proc. of the ACM Symp. on User Interfaces and Technology (UIST), P51
   Wang Nan, 2010, P 20 INT C ART REAL, P145
   Ware C., 2020, INFORM VISUALIZATION
   Westover Lee A., 1991, TECHNICAL REPORT
   Zhai Shumin, 1995, THESIS U TORONTO TOR
   Zhang HT, 2006, IEEE T VIS COMPUT GR, V12, P1267, DOI 10.1109/TVCG.2006.153
   Zudilova EV, 2003, LECT NOTES COMPUT SC, V2659, P1025
   Zwicker M, 2001, COMP GRAPH, P371, DOI 10.1145/383259.383300
NR 47
TC 1
Z9 1
U1 0
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD JAN
PY 2015
VL 11
IS 4
AR 19
DI 10.1145/2677971
PG 23
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA AZ6WJ
UT WOS:000348358400004
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Zhao, MT
   Zhu, SC
AF Zhao, Mingtian
   Zhu, Song-Chun
TI Abstract Painting with Interactive Control of Perceptual Entropy
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Algorithms; Experimentation; Human Factors; Abstract art; entropy; image
   parsing; painterly rendering; perceptual ambiguity; semantics
ID STYLE
AB This article presents a framework for generating abstract art from photographs. The aesthetics of abstract art is largely attributed to its greater perceptual ambiguity than photographs. According to psychological theories [Berlyne 1971], the ambiguity tends to invoke moderate mental effort in the viewer for interpreting the underlying contents, and this process is usually accompanied by subtle aesthetic pleasure. We study this phenomenon through human experiments comparing the subjects' interpretations of abstract art and photographs, and quantitatively verify, the increased perceptual ambiguities in terms of recognition accuracy and response time. Based on the studies, we measure the level of perceptual ambiguity using entropy, as it measures uncertainty levels in information theory, and propose a painterly rendering method with interactive control of the ambiguity levels. Given an input photograph, we first segment it into regions corresponding to different objects and parts in an interactive manner and organize them into a hierarchical parse tree representation. Then we execute a painterly rendering process with image obscuring operators to transfer the photograph into an abstract painting style with increased perceptual ambiguities in both the scene and individual objects. Finally, using kernel density estimation and message-passing algorithms, we compute and control the ambiguity levels numerically to the desired levels, during which we may predict and control the viewer's perceptual path among the image contents by assigning different ambiguity levels to different objects. We have evaluated the rendering results using a second set of human experiments, and verified that they achieve similar abstract effects to original abstract paintings.
C1 [Zhao, Mingtian; Zhu, Song-Chun] Univ Calif Los Angeles, Los Angeles, CA USA.
   [Zhao, Mingtian; Zhu, Song-Chun] Lotus Hill Inst, Hong Kong, Hong Kong, Peoples R China.
C3 University of California System; University of California Los Angeles
RP Zhao, MT (corresponding author), UCLA Dept Stat, 8125 Math Sci Bldg,Box 951554, Los Angeles, CA 90095 USA.
EM mtzhao@ucla.edu; sczhu@stat.ucla.edu
OI Zhao, Mingtian/0000-0002-0570-7342
FU NSF [IIS-1018751]; ONR MURI grant [N000141010933]; NSFC grant [60970156]
FX The work at UCLA was partially supported by NSF grant IIS-1018751 and
   ONR MURI grant N000141010933; the work at LHI was supported by NSFC
   grant 60970156.
CR ACM, 2013, TRANSACTIONS ON APPL, V10
   [Anonymous], 1982, VISION
   [Anonymous], 2006, ELEMNTS INFORM THEOR
   ARNHEIM R, 1971, ENTROPY AND ART AN E
   BARRODALE I, 1993, PATTERN RECOGN, V26, P375, DOI 10.1016/0031-3203(93)90045-X
   Berlyne D.E., 1971, AESTHETICS AND PSYCH
   Collomosse JP, 2003, IEEE T VIS COMPUT GR, V9, P443, DOI 10.1109/TVCG.2003.1260739
   DeCarlo D, 2002, ACM T GRAPHIC, V21, P769, DOI 10.1145/566570.566650
   Duda R., 1973, Pattern Classification and Scene Analysis
   Finkelstein A., 1998, Electronic Publishing, Artistic Imaging, and Digital Typography. 7th International Conference on Electronic Publishing, EP'98, Held Jointly with the 4th International Conference on Raster Imaging and Digital Typography, RIDT'98 Proceedings, P11, DOI 10.1007/BFb0053259
   Funch B. S., 1997, THE PSYCHOLOGY OF AR
   Gerstner T., 2012, P S NONPH AN REND, P29
   Gooch B, 2004, ACM T GRAPHIC, V23, P27, DOI 10.1145/966131.966133
   GOOCH B, 2001, NON PHOTOREALISTIC R
   Haeberli P., 1990, P SIGGRAPH, P207
   Han F, 2009, IEEE T PATTERN ANAL, V31, P59, DOI [10.1109/TPAMI.2008.65, 10.1109/TPAMI.2008.55]
   Hertzmann A., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P453, DOI 10.1145/280814.280951
   Hertzmann A., 2010, Proc. NPAR '10, P147
   Hughes JM, 2010, P NATL ACAD SCI USA, V107, P1279, DOI 10.1073/pnas.0910530107
   Jones-Smith K, 2006, NATURE, V444, pE9, DOI 10.1038/nature05398
   KERSTEN D, 1987, J OPT SOC AM A, V4, P2395, DOI 10.1364/JOSAA.4.002395
   Konecni V.J., 1978, Am. J. Psychol, V91, P133
   Kyprianidis J.E., 2011, Proc. NPAR '11, P55, DOI [10.1145/2024676.2024686, DOI 10.1145/2024676.2024686]
   Lee S., 2006, Proc. NPAR '06, P97, DOI DOI 10.1145/1124728.1124745
   Lindsay K. C., 1994, KANDINSKY THE COMPLE
   Litwinowicz P., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P407, DOI 10.1145/258734.258893
   Lombaert H, 2005, IEEE I CONF COMP VIS, P259
   MANDRYK RL, 2011, P ACM SIGGRAPH EUR S, P7, DOI DOI 10.1145/2024676.2024678
   Meier B. J., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P477, DOI 10.1145/237170.237288
   Mi X., 2009, Proc. NPAR '09, P15
   Mitra NJ, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618509
   Morel J.-M., 2006, TEXTURE SYNTHESIS BY
   MOULD D, 2012, P NPAR GOSL GERM EUR, P75
   Mureika JR, 2005, PHYS REV E, V72, DOI 10.1103/PhysRevE.72.046101
   Oliva A, 2007, TRENDS COGN SCI, V11, P520, DOI 10.1016/j.tics.2007.09.009
   Olsen S., 2011, ACM SIGGRAPH, P65
   Orchard J., 2008, INT S NONPH AN REND, P79
   Orzan A, 2007, NPAR 2007: 5TH INTERNATIONAL SYMPOSIUM ON NON-PHOTOREALISTIC ANIMATION AND RENDERING, PROCEEDINGS, P103
   Redmond N., 2009, P S APPL PERC GRAPH, P121
   Rigau J, 2008, IEEE COMPUT GRAPH, V28, P24, DOI 10.1109/MCG.2008.34
   Strothotte T., 2002, NON PHOTOREALISTIC C
   SY BK, 1993, INT J APPROX REASON, V8, P17, DOI 10.1016/S0888-613X(05)80004-0
   Taylor RP, 2007, PATTERN RECOGN LETT, V28, P695, DOI 10.1016/j.patrec.2006.08.012
   Tu ZW, 2005, INT J COMPUT VISION, V63, P113, DOI 10.1007/s11263-005-6642-x
   Wallraven C., 2007, ACM T APPL PERCEPT, V4, P3
   Wallraven C, 2009, COMPUT GRAPH-UK, V33, P484, DOI 10.1016/j.cag.2009.04.003
   Yao B, 2007, LECT NOTES COMPUT SC, V4679, P169
   YEDIDIA JS, 2001, IJCAI 2001 DISTINGUI
   Yevin Igor, 2006, ComPlexUs, V3, P74, DOI 10.1159/000094190
   Zeng K, 2009, ACM T GRAPHIC, V29, DOI 10.1145/1640443.1640445
   Zhao M., 2010, PROC INT S NONPHOTOR, P99, DOI DOI 10.1145/1809939.1809951
   Zhao M., 2011, Proc. NPAR '11, P137, DOI DOI 10.1145/2024676.2024698
NR 52
TC 6
Z9 8
U1 1
U2 10
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD FEB
PY 2013
VL 10
IS 1
AR 5
DI 10.1145/2422105.2422110
PG 21
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AN8YO
UT WOS:000340892200005
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Laitinen, MV
   Pihlajamäki, T
   Erkut, C
   Pulkki, V
AF Laitinen, Mikko-Ville
   Pihlajamaki, Tapani
   Erkut, Cumhur
   Pulkki, Ville
TI Parametric Time-Frequency Representation of Spatial Sound in Virtual
   Worlds
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Algorithms; Spatial sound; time-frequency processing
ID PERCEPTION; SYSTEM
AB Directional audio coding (DirAC) is a parametric time-frequency domain method for processing spatial audio based on psychophysical assumptions and on energetic analysis of the sound field. Methods to use DirAC in spatial sound synthesis for virtual worlds are presented in this article. Formal listening tests are used to show that DirAC can be used to position and to control the spatial extent of virtual sound sources with good audio quality. It is also shown that DirAC can be used to generate reverberation for N-channel horizontal listening with only two monophonic reverberators without a prominent loss in quality when compared with quality obtained with N-channel reverberators.
C1 [Laitinen, Mikko-Ville; Pihlajamaki, Tapani; Erkut, Cumhur; Pulkki, Ville] Aalto Univ, Dept Signal Proc & Acoust, Espoo 02150, Finland.
C3 Aalto University
RP Laitinen, MV (corresponding author), Aalto Univ, Dept Signal Proc & Acoust, Otakaari 5, Espoo 02150, Finland.
EM mikkoville.laitinen@aalto.fi
RI Erkut, Cumhur/E-2398-2012; Pulkki, Ville/G-2394-2013; Erkut,
   Cumhur/HTO-0023-2023
OI Erkut, Cumhur/0000-0003-0750-1919; Pulkki, Ville/0000-0003-3460-9677; 
FU Academy of Finland; Fraunhofer IIS; European Research Council under the
   European Community [FP7/2007-2013, 240453]
FX This project has been supported by the Academy of Finland and Fraunhofer
   IIS. The research leading to these results has received funding from the
   European Research Council under the European Community's Seventh
   Framework Program (FP7/2007-2013)/ERC grant agreement [240453].
CR ALLEN JB, 1979, J ACOUST SOC AM, V65, P943, DOI 10.1121/1.382599
   Baumgarte F, 2003, IEEE T SPEECH AUDI P, V11, P509, DOI 10.1109/TSA.2003.818109
   Begault DR, 2001, J AUDIO ENG SOC, V49, P904
   BENNETT JC, 1985, J AUDIO ENG SOC, V33, P314
   BERKHOUT AJ, 1993, J ACOUST SOC AM, V93, P2005, DOI 10.1121/1.406714
   BERKHOUT AJ, 1993, J ACOUST SOC AM, V93, P2764, DOI 10.1121/1.405852
   Blauert J., 1997, SPATIAL HEARING, DOI [10.7551/mitpress/6391.001.0001, DOI 10.7551/MITPRESS/6391.001.0001]
   BLUMLEIN A. D., 1958, J AUDIO ENG SOC, V6, P2
   DEL GALDO G., 2009, P AES 126 CONV
   FURNESS R. K., 1990, P AES 8 INT C
   GOODWIN M. M., 2006, P 120 AES CONV
   Herre J, 2008, J AUDIO ENG SOC, V56, P932
   HIRVONEN T., 2007, THESIS HELSINKI U TE
   HIRVONEN T., 2009, P AES 126 CONV
   *ITU, 2003, ITURBS15341
   ITU, 1997, ITU-R BS.1116-1 Recommendation
   KIRSZENSTEIN J, 1984, APPL ACOUST, V17, P275, DOI 10.1016/0003-682X(84)90011-2
   LAITINEN M.-V., 2011, P IEEE INT C ACOUST
   LAITINEN M.-V., 2009, P IEEE WORKSH APPL S
   Lentz T., 2007, EURASIP J ADV SINGAL, V2007, P187
   MANOCHA D., 2009, ACM SIGGRAPH 09 COUR
   MOLLER H, 1995, J AUDIO ENG SOC, V43, P300
   Moore B.C. J., 1995, HEARING, V2nd
   Moore BrianC. J., 1982, An Introduction to the Psychology of Hearing
   OPENAL, 2000, OPENAL OP SOURC 3D S
   Potard G., 2004, P 7 INT C DIG AUD EF
   Pulkki V, 1997, J AUDIO ENG SOC, V45, P456
   Pulkki V., 2001, THESIS HELSINKI U TE
   Pulkki V., 2011, DAFX DIGITAL AUDIO E
   PULKKI V., 2009, P AES 35TH INT C
   Pulkki V, 2007, J AUDIO ENG SOC, V55, P503
   Riecke BE, 2009, ACM T APPL PERCEPT, V6, DOI 10.1145/1498700.1498701
   Santala O, 2011, J ACOUST SOC AM, V129, P1522, DOI 10.1121/1.3533727
   Savioja L, 1999, J AUDIO ENG SOC, V47, P675
   Seeber BU, 2010, HEARING RES, V260, P1, DOI 10.1016/j.heares.2009.11.004
   TSINGOS N., 2004, P SIGGRAPH 31 INT C
   Verron C, 2010, IEEE T AUDIO SPEECH, V18, P1550, DOI 10.1109/TASL.2009.2037402
   Vilkamo J, 2009, J AUDIO ENG SOC, V57, P709
   WALSH M., 2006, P AES 121 CONV
NR 39
TC 17
Z9 21
U1 0
U2 6
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD JUN
PY 2012
VL 9
IS 2
AR 8
DI 10.1145/2207216.2207219
PG 20
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 959PR
UT WOS:000305325800003
DA 2024-07-18
ER

PT J
AU Ziemek, T
   Creem-Regehr, S
   Thompson, W
   Whitaker, R
AF Ziemek, Tina
   Creem-Regehr, Sarah
   Thompson, William
   Whitaker, Ross
TI Evaluating the Effectiveness of Orientation Indicators with an Awareness
   of Individual Differences
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Experimentation; Measurement; Human Factors; Mental rotation; cognitive
   support; methodology; visualization
ID 3-DIMENSIONAL OBJECTS; MENTAL ROTATIONS; SPATIAL ABILITY; VISUALIZATION;
   RECOGNITION; SOFTWARE; ANATOMY; MR
AB Understanding how users perceive 3D geometric objects can provide a basis for creating more effective tools for visualization in applications such as CAD or medical imaging. This article examines how orientation indicators affect users' accuracy in perceiving the shape of a 3D object shown as multiple views. Multiple views force users to infer the orientation of an object and recognize corresponding features between distinct vantage points. These are difficult tasks, and not all users are able to carry them out accurately. We use a cognitive experimental paradigm to evaluate the effectiveness of two types of orientation indicators on a person's ability to compare views of objects presented in different orientations. The orientation indicators implemented were colocated, which shared a center-point with the 3D object, or noncolocated with (displaced from) the 3D object. The study accounts for additional factors including object complexity, axis of rotation, and users' individual differences in spatial abilities. Our results show that an orientation indicator helps users in comparing multiple views, and that the effect is influenced by the type of aid, a person's spatial ability, and the difficulty of the task. In addition to establishing an effect of an orientation indicator, this article helps demonstrate the application of a particular experimental paradigm and analysis, as well as the importance of considering individual differences when designing interface aids.
C1 [Ziemek, Tina; Creem-Regehr, Sarah; Thompson, William; Whitaker, Ross] Univ Utah, Salt Lake City, UT 84112 USA.
C3 Utah System of Higher Education; University of Utah
RP Ziemek, T (corresponding author), Univ Utah, Salt Lake City, UT 84112 USA.
EM tziemek@cs.utah.edu
FU National Science Foundation [0745131, 0914488]; Direct For Computer &
   Info Scie & Enginr; Div Of Information & Intelligent Systems [0745131]
   Funding Source: National Science Foundation; Div Of Information &
   Intelligent Systems; Direct For Computer & Info Scie & Enginr [0914488]
   Funding Source: National Science Foundation
FX This material is based on work supported by the National Science
   Foundation under grants 0745131 and 0914488.
CR ANDERSON J., 1998, RADIOLOGICAL SOC N A
   [Anonymous], 2012, ACM T APPL PERCEPTIO, V9
   [Anonymous], 1986, Mental Images and Their Transformations
   [Anonymous], 2006, P SIGCHI C HUM FACT, DOI DOI 10.1145/1124772.1124808
   BARFIELD W, 1988, INT J MAN MACH STUD, V29, P669, DOI 10.1016/S0020-7373(88)80073-7
   Beckwith Laura, 2005, P SIGCHI C HUM FACT, P869
   Brady MJ, 2003, J VISION, V3, P413, DOI 10.1167/3.6.2
   Brooks F. P.  Jr., 1990, Computer Graphics, V24, P177, DOI 10.1145/97880.97899
   BULTHOFF HH, 1992, P NATL ACAD SCI USA, V89, P60, DOI 10.1073/pnas.89.1.60
   Chen CM, 2000, J AM SOC INFORM SCI, V51, P499, DOI 10.1002/(SICI)1097-4571(2000)51:6<499::AID-ASI2>3.0.CO;2-K
   Cole F, 2008, ACM T GRAPHIC, V27, DOI [10.1145/1360612.1360657, 10.1145/1360612.1360687]
   Cox RW, 1996, COMPUT BIOMED RES, V29, P162, DOI 10.1006/cbmr.1996.0014
   Czerwinski M., 2002, ACM C HUMAN FACTORS, P195, DOI DOI 10.1145/503376.503412
   Dillon A, 1996, INT J HUM-COMPUT ST, V45, P619, DOI 10.1006/ijhc.1996.0071
   Dunser A., 2006, P 7 ACM SIGCHI NZ CH, P125, DOI 10.1145/1152760.1152776
   Ekstrom R.B., 1976, KIT FACTORREFERENCED
   Fitzmaurice G, 2008, I3D 2008: SYMPOSIUM ON INTERACTIVE 3D GRAPHICS AND GAMES, PROCEEDINGS, P7
   Garg A, 1999, ACAD MED, V74, pS87, DOI 10.1097/00001888-199910000-00049
   Garg AX, 2001, LANCET, V357, P363, DOI 10.1016/S0140-6736(00)03649-7
   GERING D, 1999, THESIS MIT CAMBRIDGE
   Gering DT, 2001, J MAGN RESON IMAGING, V13, P967, DOI 10.1002/jmri.1139
   GLUECK M., 2009, P S INT 3D GRAPH GAM, P225
   GOLLAND P., 1998, P 1 INT C MED IMAG C
   Halpern E.F., 2005, CAMBRIDGE HDB VISUOS, P170, DOI DOI 10.1017/CBO9780511610448.006
   Hata N, 2001, RADIOLOGY, V220, P263, DOI 10.1148/radiology.220.1.r01jl44263
   Hegarty M, 2004, LEARN INSTR, V14, P343, DOI 10.1016/j.learninstruc.2004.06.007
   Hegarty M., 2007, APPL SPATIAL COGNITI, P285
   Hegarty Mary., 2005, The Cambridge handbook of visuospatial thinking, P121, DOI DOI 10.1017/CBO9780511610448.005
   HINTON GE, 1981, ATTENTION PERFORM, V9, P261
   Hubona G. S., 1997, Human Factors in Computing Systems. CHI 97 Extended Abstracts, P345
   Huff C., 2002, SIGCSE Bulletin, V34, P112, DOI 10.1145/543812.543842
   HUFF C., 2002, SIGCSE B, V34, P2
   Huk T, 2006, J COMPUT ASSIST LEAR, V22, P392, DOI 10.1111/j.1365-2729.2006.00180.x
   JASTROW H., 2006, J COMPUT ASSIST LEAR, V22, P392
   Keehner M., 2008, User Centered Design for Medical Visualization, P1
   Keehner M., 2002, Proceedings of the AAAI Spring Symposium Series, Reasoning with Mental and External Diagrams, P12
   Keehner M, 2008, COGNITIVE SCI, V32, P1099, DOI 10.1080/03640210801898177
   Keehner MM, 2004, AM J SURG, V188, P71, DOI 10.1016/j.amjsurg.2003.12.059
   Khan A, 2008, I3D 2008: SYMPOSIUM ON INTERACTIVE 3D GRAPHICS AND GAMES, PROCEEDINGS, P17
   Lawson R, 1998, PERCEPT PSYCHOPHYS, V60, P1052, DOI 10.3758/BF03211939
   MacLeod RS, 2008, COMPUTERS IN CARDIOLOGY 2008, VOLS 1 AND 2, P77, DOI 10.1109/CIC.2008.4748981
   Modjeska D, 2003, J AM SOC INF SCI TEC, V54, P216, DOI 10.1002/asi.10197
   North C., 2000, Proceedings of the the working conference on Advanced visual interfaces (AVI) 2000, P128, DOI [DOI 10.1145/345513.345282, 10.1145/345513.345282]
   PANI JR, 1993, PERCEPTION, V22, P785, DOI 10.1068/p220785
   PARSONS LM, 1987, PERCEPT PSYCHOPHYS, V42, P49, DOI 10.3758/BF03211513
   PARSONS LM, 1995, J EXP PSYCHOL HUMAN, V21, P1259, DOI 10.1037/0096-1523.21.6.1259
   PETERS M, 1995, BRAIN COGNITION, V28, P39, DOI 10.1006/brcg.1995.1032
   Rizzo AA, 1997, ST HEAL T, V44, P123
   SANDO T., 2009, P 6 S APPL PERC GRAP, P69
   SHEPARD RN, 1971, SCIENCE, V171, P701, DOI 10.1126/science.171.3972.701
   SHIFFRAR MM, 1991, J EXP PSYCHOL HUMAN, V17, P44
   SILEN C., 2008, MED TEACH, V30, P5
   Silén C, 2008, MED TEACH, V30, pE115, DOI 10.1080/01421590801932228
   STULL A. T., 2009, J EDUC PSYCHOL, V101, P4
   Stull AT, 2009, J EDUC PSYCHOL, V101, P803, DOI 10.1037/a0016849
   TARR MJ, 1995, PSYCHON B REV, V2, P55, DOI 10.3758/BF03214412
   Tokuda J, 2010, COMPUT MED IMAG GRAP, V34, P3, DOI 10.1016/j.compmedimag.2009.07.004
   Tory M, 2005, IEEE COMPUT GRAPH, V25, P8, DOI 10.1109/MCG.2005.102
   Tory M, 2003, IEEE VISUALIZATION 2003, PROCEEDINGS, P371, DOI 10.1109/VISUAL.2003.1250396
   Tory M, 2004, IEEE T VIS COMPUT GR, V10, P72, DOI 10.1109/TVCG.2004.1260759
   VANDENBERG SG, 1978, PERCEPT MOTOR SKILL, V47, P599, DOI 10.2466/pms.1978.47.2.599
   Velez MC, 2005, IEEE VISUALIZATION 2005, PROCEEDINGS, P511
   WALENSTEIN A, 2002, THESIS SIMON FRASER
   Wang Baldonado M. Q., 2000, P WORK C ADV VIS INT, P110, DOI [10.1145/345513.345271, DOI 10.1145/345513.345271]
   Ware C., 2004, Proceedings of the 1st Symposium on Applied perception in graphics and visualization, P135
   Wraga M, 1999, ACTA PSYCHOL, V102, P247, DOI 10.1016/S0001-6918(98)00057-2
   ZACKS J. M., 2005, BEHAV COGNITIVE NEUR, V4, P2
   Zacks Jeffrey M, 2005, Behav Cogn Neurosci Rev, V4, P96, DOI 10.1177/1534582305281085
   ZHANG JJ, 1994, COGNITIVE SCI, V18, P87, DOI 10.1207/s15516709cog1801_3
   ZIEMEK T. R., 2008, P S APPL PERC GRAPH
NR 70
TC 5
Z9 7
U1 0
U2 7
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD JUN
PY 2012
VL 9
IS 2
AR 7
DI 10.1145/2207216.2207218
PG 23
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA 959PR
UT WOS:000305325800002
DA 2024-07-18
ER

PT J
AU Bulling, A
   Ward, JA
   Gellersen, H
AF Bulling, Andreas
   Ward, Jamie A.
   Gellersen, Hans
TI Multimodal Recognition of Reading Activity in Transit Using Body-Worn
   Sensors
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Algorithms; Experimentation; Measurement; Recognition of reading; eye
   movement analysis; multimodal sensing; sensorimotor coordination; head
   movements; electrooculography (EOG)
ID EYE-HAND COORDINATION; MOVEMENTS; EOG; GAZE; INFORMATION; PERCEPTION;
   CONTEXT; SYSTEM; HEAD
AB Reading is one of the most well-studied visual activities. Vision research traditionally focuses on understanding the perceptual and cognitive processes involved in reading. In this work we recognize reading activity by jointly analyzing eye and head movements of people in an everyday environment. Eye movements are recorded using an electrooculography (EOG) system; body movements using body-worn inertial measurement units. We compare two approaches for continuous recognition of reading: String matching (STR) that explicitly models the characteristic horizontal saccades during reading, and a support vector machine (SVM) that relies on 90 eye movement features extracted from the eye movement data. We evaluate both methods in a study performed with eight participants reading while sitting at a desk, standing, walking indoors and outdoors, and riding a tram. We introduce a method to segment reading activity by exploiting the sensorimotor coordination of eye and head movements during reading. Using person-independent training, we obtain an average precision for recognizing reading of 88.9% (recall 72.3%) using STR and of 87.7% (recall 87.9%) using SVM over all participants. We show that the proposed segmentation scheme improves the performance of recognizing reading events by more than 24%. Our work demonstrates that the joint analysis of eye and body movements is beneficial for reading recognition and opens up discussion on the wider applicability of a multimodal recognition approach to other visual and physical activities.
C1 [Bulling, Andreas] Univ Cambridge, Comp Lab, Cambridge CB3 0FD, England.
   [Ward, Jamie A.; Gellersen, Hans] Univ Lancaster, Sch Comp & Commun, Lancaster LA1 4WA, England.
C3 University of Cambridge; Lancaster University
RP Bulling, A (corresponding author), Univ Cambridge, Comp Lab, 15 JJ Thomson Ave,William Gates Bldg, Cambridge CB3 0FD, England.
EM andreas.bulling@acm.org; j.ward@comp.lancs.ac.uk; hwg@comp.lancs.ac.uk
RI Gellersen, Hans/HJP-8760-2023; Ward, Jamie/AAH-2775-2020; Bulling,
   Andreas/A-3947-2009
OI Ward, Jamie/0000-0002-9637-6066; Bulling, Andreas/0000-0001-6317-7303;
   Gellersen, Hans/0000-0003-2233-2121
FU EPSRC [EP/H005064/1] Funding Source: UKRI
CR [Anonymous], 2012, ACM T APPL PERCEPT, V9
   [Anonymous], 2006, CHI 06 EXTENDED ABST, DOI DOI 10.1145/1125451.1125655
   [Anonymous], 2007, Eye tracking methodology: Theory and practice, DOI DOI 10.1007/978-3-319-57883-5
   Bannach D, 2008, IEEE PERVAS COMPUT, V7, P22, DOI 10.1109/MPRV.2008.36
   Bao L, 2004, LECT NOTES COMPUT SC, V3001, P1, DOI 10.1007/978-3-540-24646-6_1
   Barea R, 2002, IEEE T NEUR SYS REH, V10, P209, DOI 10.1109/TNSRE.2002.806829
   Biedert Ralf, 2010, CHI 10 EXTENDED ABST, P4003, DOI [DOI 10.1145/1753846.1754093, 10.1145/1753846.1754093]
   Brown M, 2006, DOC OPHTHALMOL, V113, P205, DOI 10.1007/s10633-006-9030-0
   Bulling A, 2008, LECT NOTES COMPUT SC, V5013, P19, DOI 10.1007/978-3-540-79576-6_2
   Bulling A, 2011, IEEE T PATTERN ANAL, V33, P741, DOI 10.1109/TPAMI.2010.86
   Bulling A, 2011, IEEE PERVAS COMPUT, V10, P48, DOI 10.1109/MPRV.2010.49
   Bulling A, 2009, J AMB INTEL SMART EN, V1, P157, DOI 10.3233/AIS-2009-0020
   Campbell C.S., 2001, P 2001 WORKSHOP PERC, P1, DOI DOI 10.1145/971478.971503
   Canosa RL, 2009, ACM T APPL PERCEPT, V6, DOI 10.1145/1498700.1498705
   Chen YX, 2004, IEEE INT CONF ROBOT, P243
   Crammer K, 2003, J MACH LEARN RES, V3, P951, DOI 10.1162/jmlr.2003.3.4-5.951
   Davies N, 2008, IEEE PERVAS COMPUT, V7, P20, DOI 10.1109/MPRV.2008.26
   Dempere-Marco L, 2002, IEEE T MED IMAGING, V21, P741, DOI 10.1109/TMI.2002.801153
   Ding QP, 2005, P ANN INT IEEE EMBS, P6829
   Elhelw M, 2008, ACM T APPL PERCEPT, V5, DOI 10.1145/1279640.1279643
   HACISALIHZADE SS, 1992, IEEE T SYST MAN CYB, V22, P474, DOI 10.1109/21.155948
   Hayhoe M, 2005, TRENDS COGN SCI, V9, P188, DOI 10.1016/j.tics.2005.02.009
   Henderson JM, 2003, TRENDS COGN SCI, V7, P498, DOI 10.1016/j.tics.2003.09.006
   Ji Q, 2002, REAL-TIME IMAGING, V8, P357, DOI 10.1006/rtim.2002.0279
   Johansson RS, 2001, J NEUROSCI, V21, P6917, DOI 10.1523/JNEUROSCI.21-17-06917.2001
   KARSON CN, 1981, PSYCHIAT RES, V5, P243, DOI 10.1016/0165-1781(81)90070-6
   Keat FT, 2003, TENCON IEEE REGION, P825
   Kern N, 2007, PERS UBIQUIT COMPUT, V11, P251, DOI 10.1007/s00779-006-0086-3
   Levenshtein V. I., 1966, SOV PHYS DOKL, V10, P707
   LIN C.-J., 2008, LIBLINEARA LIB LARGE
   Liversedge SP, 2000, TRENDS COGN SCI, V4, P6, DOI 10.1016/S1364-6613(99)01418-7
   Logan B, 2007, LECT NOTES COMPUT SC, V4717, P483
   Maglio PP, 2000, LECT NOTES COMPUT SC, V1948, P1
   Manor BR, 2003, J NEUROSCI METH, V128, P85, DOI 10.1016/S0165-0270(03)00151-1
   Mitra S, 2007, IEEE T SYST MAN CY C, V37, P311, DOI 10.1109/TSMCC.2007.893280
   Najafi B, 2003, IEEE T BIO-MED ENG, V50, P711, DOI 10.1109/TBME.2003.812189
   Pelz J, 2001, EXP BRAIN RES, V139, P266, DOI 10.1007/s002210100745
   Peng H., 2008, MRMR FEATURE SELECTI
   Peng HC, 2005, IEEE T PATTERN ANAL, V27, P1226, DOI 10.1109/TPAMI.2005.159
   Rayner K, 1998, PSYCHOL BULL, V124, P372, DOI 10.1037/0033-2909.124.3.372
   Sailer U, 2005, J NEUROSCI, V25, P8833, DOI 10.1523/JNEUROSCI.2658-05.2005
   Salvucci DD, 2001, HUM-COMPUT INTERACT, V16, P39, DOI 10.1207/S15327051HCI1601_2
   Schiffman HarveyRichard., 2001, SENSATION PERCEPTION
   Schleicher R, 2008, ERGONOMICS, V51, P982, DOI 10.1080/00140130701817062
   Sibert J. L., 2000, UIST. Proceedings of the 13th Annual ACM Symposium on User Interface Software and Technology, P101, DOI 10.1145/354401.354418
   Huynh T, 2008, PROCEEDINGS OF THE 10TH INTERNATIONAL CONFERENCE ON UBIQUITOUS COMPUTING (UBICOMP 2008), P10
   Tinati MA, 2006, INT J BIOMED IMAGING, V2006, DOI 10.1155/IJBI/2006/97157
   Turaga P, 2008, IEEE T CIRC SYST VID, V18, P1473, DOI 10.1109/TCSVT.2008.2005594
   Vehkaoja AT, 2005, P ANN INT IEEE EMBS, P5865, DOI 10.1109/IEMBS.2005.1615824
   Ward JA, 2006, IEEE T PATTERN ANAL, V28, P1553, DOI 10.1109/TPAMI.2006.197
   Ward JA, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1889681.1889687
   Widdel H., 1984, THEORETICAL APPL ASP, P21
   Wijesoma WS, 2005, IEEE INT CONF ROBOT, P1085
   Young S, 2010, IEEE SIGNAL PROC MAG, V27, P128, DOI 10.1109/MSP.2010.935874
NR 54
TC 62
Z9 68
U1 0
U2 22
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD MAR
PY 2012
VL 9
IS 1
AR 2
DI 10.1145/2134203.2134205
PG 21
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 949YM
UT WOS:000304614400002
DA 2024-07-18
ER

PT J
AU Trutoiu, LC
   Carter, EJ
   Matthews, I
   Hodgins, JK
AF Trutoiu, Laura C.
   Carter, Elizabeth J.
   Matthews, Iain
   Hodgins, Jessica K.
TI Modeling and Animating Eye Blinks
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Measurement; Human Factors; Eye blinks
ID MOVEMENTS; HUMANS
AB Facial animation often falls short in conveying the nuances present in the facial dynamics of humans. In this article, we investigate the subtleties of the spatial and temporal aspects of eye blinks. Conventional methods for eye blink animation generally employ temporally and spatially symmetric sequences; however, naturally occurring blinks in humans show a pronounced asymmetry on both dimensions. We present an analysis of naturally occurring blinks that was performed by tracking data from high-speed video using active appearance models. Based on this analysis, we generate a set of key-frame parameters that closely match naturally occurring blinks. We compare the perceived naturalness of blinks that are animated based on real data to those created using textbook animation curves. The eye blinks are animated on two characters, a photorealistic model and a cartoon model, to determine the influence of character style. We find that the animated blinks generated from the human data model with fully closing eyelids are consistently perceived as more natural than those created using the various types of blink dynamics proposed in animation textbooks.
C1 [Trutoiu, Laura C.; Carter, Elizabeth J.; Hodgins, Jessica K.] Carnegie Mellon Univ, Inst Robot, Pittsburgh, PA 15213 USA.
   [Matthews, Iain; Hodgins, Jessica K.] Disney Res, Pittsburgh, PA 15213 USA.
C3 Carnegie Mellon University
RP Trutoiu, LC (corresponding author), Carnegie Mellon Univ, Inst Robot, 5000 Forbes Ave, Pittsburgh, PA 15213 USA.
EM ltrutoiu@cs.cmu.edu
RI Carter, Elizabeth J/G-6958-2012
OI Carter, Elizabeth J./0000-0002-2735-148X
FU NSF [CCF-0811450]
FX Partial support was provided by NSF CCF-0811450.
CR [Anonymous], 2001, The animator's survival kit
   [Anonymous], CMURITR0335
   Bacher LF, 2004, DEV PSYCHOBIOL, V44, P95, DOI 10.1002/dev.10162
   Bacivarov I, 2008, IEEE T CONSUM ELECTR, V54, P1312, DOI 10.1109/TCE.2008.4637622
   Blount WP, 1928, Q J EXP PHYSIOL, V18, P110
   Caffier PP, 2003, EUR J APPL PHYSIOL, V89, P319, DOI 10.1007/s00421-003-0807-5
   Codispoti M, 2001, PSYCHOPHYSIOLOGY, V38, P474, DOI 10.1111/1469-8986.3830474
   Cootes T., 1998, Proc. ECCV, V2, P484
   Culhane Shamus., 1988, Animation: From Script to Screen
   DARGIS M, 2007, NY TIMES        1116
   Deng ZG, 2005, IEEE COMPUT GRAPH, V25, P24, DOI 10.1109/MCG.2005.35
   DOUGHTY MJ, 2001, OPTOM VIS SCI, V78
   EVINGER C, 1984, J NEUROPHYSIOL, V52, P323, DOI 10.1152/jn.1984.52.2.323
   EVINGER C, 1991, INVEST OPHTH VIS SCI, V32, P387
   GUITTON D, 1991, INVEST OPHTH VIS SCI, V32, P3298
   LASSETER J, 1987, COMPUTER GRAPHICS, V21, P35, DOI DOI 10.1145/37401.37407
   Leal S, 2008, J NONVERBAL BEHAV, V32, P187, DOI 10.1007/s10919-008-0051-0
   Lee SP, 2002, ACM T GRAPHIC, V21, P637
   MAESTRI G., 1996, Digital Character Animation
   Matthews I, 2004, INT J COMPUT VISION, V60, P135, DOI 10.1023/B:VISI.0000029666.37597.d3
   Porter S, 2008, PSYCHOL SCI, V19, P508, DOI 10.1111/j.1467-9280.2008.02116.x
   Steptoe W, 2010, COMPUT ANIMAT VIRT W, V21, P161, DOI 10.1002/cav.354
   Sun WS, 1997, INVEST OPHTH VIS SCI, V38, P92
   TANAKA Y, 1993, PERCEPT MOTOR SKILL, V77, P55, DOI 10.2466/pms.1993.77.1.55
   VanderWerf F, 2003, J NEUROPHYSIOL, V89, P2784, DOI 10.1152/jn.00557.2002
   Veltman JA, 1998, ERGONOMICS, V41, P656, DOI 10.1080/001401398186829
   WILSON GF, 1987, HUM FACTORS ERGONOM, V31, P779
NR 27
TC 35
Z9 38
U1 0
U2 6
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD AUG
PY 2011
VL 8
IS 3
AR 17
DI 10.1145/2010325.2010327
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA 819HK
UT WOS:000294815800002
DA 2024-07-18
ER

PT J
AU Radun, J
   Leisti, T
   Virtanen, T
   Häkkinen, J
   Vuori, T
   Nyman, G
AF Radun, Jenni
   Leisti, Tuomas
   Virtanen, Toni
   Hakkinen, Jukka
   Vuori, Tero
   Nyman, Gote
TI Evaluating the Multivariate Visual Quality Performance of
   Image-Processing Components
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Human Factors; Experimentation; Performance; Measurement; Image quality;
   subjective measurements; qualitative methodology; quality dimensions;
   correspondence analysis
ID NATURALNESS
AB The estimation of image quality is a demanding task, especially when estimating different high-quality imaging products or their components. The challenge is the multivariate nature of image quality as well as the need to use naive observers as test subjects, since they are the actual end-users of the products. Here, we use a subjective approach suitable for estimating the quality performance of different imaging device components with naive observers-the interpretation-based quality (IBQ) approach. From two studies with 61 naive observers, 17 natural image contents, and 13 different camera image signal processor pipelines, we determined the subjectively crucial image quality attributes and dimensions and the description of each pipeline's perceived image quality performance. We found that the subjectively most important image quality dimensions were color shift/naturalness, darkness, and sharpness. The first dimension, which was related to naturalness and colors, distinguished the good-quality pipelines from the middle-and low-quality groups, and the dimensions of darkness and sharpness described why the quality failed in the low-quality pipelines. The study suggests that the high-level concept naturalness is a requirement for high-quality images, whereas quality can fail for other reasons in low-quality images, and this failure can be described by low-level concepts, such as darkness and sharpness.
C1 [Radun, Jenni; Leisti, Tuomas; Virtanen, Toni; Hakkinen, Jukka; Nyman, Gote] Univ Helsinki, Inst Behav Sci, Helsinki, Finland.
   [Hakkinen, Jukka; Vuori, Tero] Nokia Electr Ltd, Espoo, Finland.
C3 University of Helsinki; Nokia Corporation; Nokia Finland
RP Radun, J (corresponding author), Univ Helsinki, Inst Behav Sci, Helsinki, Finland.
EM jenni.radun@helsinki.fi
RI Häkkinen, Jukka/A-4122-2019; Leisti, Tuomas/AAF-1709-2020; Häkkinen,
   Jukka/D-2271-2009; Radun, Jenni/AAB-3943-2021; Virtanen,
   Toni/E-9181-2015
OI Häkkinen, Jukka/0000-0003-0215-2238; Leisti, Tuomas/0000-0002-9234-2854;
   Radun, Jenni/0000-0003-3269-2999; Virtanen, Toni/0000-0001-5191-2438
CR [Anonymous], 2000, Psychometric scaling, a toolkit for imaging systems development
   Black W., 1998, Multivariate data analysis: With readings
   deRidder H, 1996, J IMAGING SCI TECHN, V40, P487
   Eckert MP, 1998, SIGNAL PROCESS, V70, P177, DOI 10.1016/S0165-1684(98)00124-8
   Engeldrum PG, 2004, J IMAGING SCI TECHN, V48, P160
   Faye P, 2004, FOOD QUAL PREFER, V15, P781, DOI 10.1016/j.foodqual.2004.04.009
   Greenacre M, 2007, CORRES ANAL PRACTICE, P2
   HALONEN R, 2008, P 2S TS INT C DIG PR
   Janssen TJWM, 2000, DISPLAYS, V21, P129, DOI 10.1016/S0141-9382(00)00056-1
   Keelan B., 2002, Handbook of Image Quality: Characterization and Prediction
   Martens JB, 2002, P IEEE, V90, P133, DOI 10.1109/5.982411
   NYMAN G, 2005, P 12 INT DISPL WORKS, P1825
   Picard D, 2003, ACTA PSYCHOL, V114, P165, DOI 10.1016/j.actpsy.2003.08.001
   RADUN J, 2007, P EL IM SCI TECHN SP
   Radun J, 2006, ICIS '06: INTERNATIONAL CONGRESS OF IMAGING SCIENCE, FINAL PROGRAM AND PROCEEDINGS, P119
   Radun J, 2008, ACM T APPL PERCEPT, V4, DOI 10.1145/1278760.1278762
   SHIBATA T, 2009, P EL IM SCI TECHN SP
   To M, 2008, P ROY SOC B-BIOL SCI, V275, P2299, DOI 10.1098/rspb.2008.0692
   VIRTANEN T, 2008, P EL IM SCI TECHN SP
   Yendrikhovskij S, 1999, IMAGING SCI J, V47, P197, DOI 10.1080/13682199.1999.11736360
   Yendrikhovskij SN, 1999, COLOR RES APPL, V24, P52, DOI 10.1002/(SICI)1520-6378(199902)24:1<52::AID-COL10>3.0.CO;2-4
NR 21
TC 17
Z9 17
U1 0
U2 5
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD JUN
PY 2010
VL 7
IS 3
AR 16
DI 10.1145/1773965.1773967
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 618NF
UT WOS:000279361800002
DA 2024-07-18
ER

PT J
AU Majumder, A
   Irani, S
AF Majumder, Aditi
   Irani, Sandy
TI Perception-Based Contrast Enhancement of Images
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Contrast; Color; Perception; Displays; Human perception; contrast
   sensitivity; contrast enhancement
AB Study of contrast sensitivity of the human eye shows that our suprathreshold contrast sensitivity follows the Weber Law and, hence, increases proportionally with the increase in the mean local luminance. In this paper, we effectively apply this fact to design a contrast-enhancement method for images that improves the local image contrast by controlling the local image gradient with a single parameter. Unlike previous methods, we achieve this without explicit segmentation of the image, either in the spatial (multiscale) or frequency (multiresolution) domain. We pose the contrast enhancement as an optimization problem that maximizes the average local contrast of an image strictly constrained by a perceptual constraint derived directly from the Weber Law. We then propose a greedy heuristic, controlled by a single parameter, to approximate this optimization problem.
C1 [Majumder, Aditi; Irani, Sandy] Univ Calif Irvine, Dept Comp Sci, Irvine, CA 92697 USA.
C3 University of California System; University of California Irvine
RP Majumder, A (corresponding author), Univ Calif Irvine, Dept Comp Sci, Irvine, CA 92697 USA.
CR [Anonymous], 2005, High Dynamic Range Imaging: Acquisition, Display, and Image-Based Lighting (The Morgan Kaufmann Series in Computer Graphics
   [Anonymous], IEEE INT C IM PROC
   Barten Peter G. J, 1999, Contrast sensitivity of the human eye and its effects on image quality
   Boccignone G, 1997, INT CONF ACOUST SPEE, P2789, DOI 10.1109/ICASSP.1997.595368
   BURT PJ, 1983, ACM T GRAPHIC, V2, P217, DOI 10.1145/245.247
   Debevec P., 1997, P ACM SIGGRAPH, P369, DOI DOI 10.1145/258734.258884
   Fattal R, 2002, ACM T GRAPHIC, V21, P249
   GEORGESON MA, 1975, J PHYSIOL-LONDON, V252, P627, DOI 10.1113/jphysiol.1975.sp011162
   Giorgianni E.I., 1998, DIGITAL COLOR MANAGE
   HANMANDLU M, 2001, P IEEE INT C INF TEC
   HANMANDLU M, 2000, P INT C PATT REC
   Kingdom FAA, 1996, VISION RES, V36, P817, DOI 10.1016/0042-6989(95)00164-6
   KOENDERINK JJ, 1984, BIOL CYBERN, V50, P363, DOI 10.1007/BF00336961
   LAND EH, 1971, J OPT SOC AM, V61, P1, DOI 10.1364/JOSA.61.000001
   LAND EH, 1964, AM SCI, V52, P247
   Mantiuk R., 2006, ACM T APPL PERCEPT, V3, P3
   MUKHOPADHYAY S, 2002, IND C COMP VIS GRAPH
   Munteanu C, 2001, NEURAL NETWORKS FOR SIGNAL PROCESSING XI, P393, DOI 10.1109/NNSP.2001.943143
   Oakley JP, 1998, IEEE T IMAGE PROCESS, V7, P167, DOI 10.1109/83.660994
   PELI E, 1990, J OPT SOC AM A, V7, P2032, DOI 10.1364/JOSAA.7.002032
   Pérez P, 2003, ACM T GRAPHIC, V22, P313, DOI 10.1145/882262.882269
   Reinhard E, 2002, ACM T GRAPHIC, V21, P267, DOI 10.1145/566570.566575
   Shyu MS, 1998, PATTERN RECOGN, V31, P871, DOI 10.1016/S0031-3203(97)00073-3
   Starck JL, 2003, IEEE T IMAGE PROCESS, V12, P706, DOI 10.1109/TIP.2003.813140
   TOET A, 1990, PATTERN RECOGN LETT, V11, P267, DOI 10.1016/0167-8655(90)90065-A
   TOET A, 1992, PATTERN RECOGN LETT, V13, P167, DOI 10.1016/0167-8655(92)90056-6
   Valois R.L. D., 1990, Spatial Vision
   VELDE KV, 1999, P INT C IM PROC, V3, P584
   WHITTLE P, 1986, VISION RES, V26, P1677, DOI 10.1016/0042-6989(86)90055-6
   Wilson H.R., 1991, SPATIAL VISION, P64
   Witkin A.P., 1983, P INT JOINT C ART IN, P1019, DOI DOI 10.1007/978-3-8348-9190-729
NR 31
TC 33
Z9 39
U1 0
U2 10
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD NOV
PY 2007
VL 4
IS 3
AR 17
DI 10.1145/1278387.1278391
PG 22
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA V04IN
UT WOS:000207052200004
DA 2024-07-18
ER

PT J
AU Cabral, JP
   Remijn, GB
AF Cabral, Joao P.
   Remijn, Gerard B.
TI The Duration of an Auditory Icon Can Affect How the Listener Interprets
   Its Meaning
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Auditory icon; auditory alarm; acoustic characteristics; user preference
AB Initially introduced in the field of informatics, an auditory icon consists of a short sound that is present in everyday life, used to represent a specific event, object, function, or action. Auditory icons have been studied in various fields, and overall, compared to other types of auditory alarms, they can be very efficient in informing the listener about a situation or event. So far, auditory icons have been used with a wide range of durations, ranging from a few hundreds of milliseconds up to several seconds. Still little is known, however, about whether and how icon duration influences its interpretation. In the present study, we therefore asked listeners to rate 12 auditory icons, divided into four different sound categories (nonverbal human sounds, machine sounds, human activities, and animal vocalizations), in five different durations (200, 400, 800, 1,600, and 3,200 ms). They rated (1) how appropriately the icon sound itself represented the icon's referent and (2) how appropriately each duration of the icon sound represented the icon's referent. Overall, results demonstrate that the duration of the auditory icons in this stimulus set can directly affect howthe icon represents the referent. Auditory icons in the test set characterized by human activities represented their referent most appropriately in a relatively shorter duration (400 or 800 ms). Themajority of the auditory icons in the set consisting of machine sounds, nonverbal human sounds, and animal vocalizations, however, were considered as more appropriately representing their referent in longer durations (800 ms and 1,600 ms). Further systematic research is necessary to determine whether the duration effects shown here may generalize to other stimulus sets.
C1 [Cabral, Joao P.] Kyushu Univ, Grad Sch Design, Dept Human Sci, Minami Ku, 4-9-1 Shiobaru, Fukuoka 8158540, Japan.
   [Remijn, Gerard B.] Kyushu Univ, Res Ctr Appl Perceptual Sci, Dept Human Sci, Minami Ku, 4-9-1 Shiobaru, Fukuoka 8158540, Japan.
C3 Kyushu University; Kyushu University
RP Cabral, JP (corresponding author), Kyushu Univ, Grad Sch Design, Dept Human Sci, Minami Ku, 4-9-1 Shiobaru, Fukuoka 8158540, Japan.
EM joaopaulocabral10@hotmail.com; remijn@design.kyushu-u.ac.jp
OI Cabral, Joao Paulo/0000-0003-0328-5817; Remijn,
   Gerard/0000-0002-8681-9951
FU Japan Student Services Organization (JASSO)
FX J.P.C. was supported by the Japan Student Services Organization (JASSO)
   with the "Monbukagakusho Honors Scholarship for PrivatelyFinanced
   International Students."
CR [Anonymous], 2018, FREE SOUND
   [Anonymous], 2017, FREE SOUND AUDIO 002
   Arbernaut (pseud.), 2018, FREE SOUND AUDIO 002
   Audacity Team, 2017, AUD R FREE AUD ED RE
   BALLAS JA, 1993, J EXP PSYCHOL HUMAN, V19, P250, DOI 10.1037/0096-1523.19.2.250
   BALLAS JA, 1987, ENVIRON BEHAV, V19, P91, DOI 10.1177/0013916587191005
   Cabral JP, 2019, APPL ERGON, V78, P224, DOI 10.1016/j.apergo.2019.02.008
   Cohen J., 1988, STAT POWER ANAL BEHA
   Edworthy Judy R., 2006, INT ENCY ERGONOMICS, V2nd, P1026
   Fabiani M, 1996, PSYCHOPHYSIOLOGY, V33, P462, DOI 10.1111/j.1469-8986.1996.tb01072.x
   Garcia-Ruiz MA, 2008, INT J EMERG TECHNOL, V3, P59
   Garzonis S, 2009, CHI2009: PROCEEDINGS OF THE 27TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1-4, P1513
   Gaver W. W., 1989, Human-Computer Interaction, V4, P67, DOI 10.1207/s15327051hci0401_3
   Gaver William W, 1986, Human-computer interaction, V2, P167, DOI [10.1207/s15327051hci0202_3, DOI 10.1207/S15327051HCI0202_3]
   GAVER WW, 1993, ECOL PSYCHOL, V5, P1, DOI 10.1207/s15326969eco0501_1
   Gibson J., 1979, The ecological approach to visual perception
   Graham R, 1999, ERGONOMICS, V42, P1233, DOI 10.1080/001401399185108
   Gygi B, 2004, J ACOUST SOC AM, V115, P1252, DOI 10.1121/1.1635840
   Hoferlin B., 2011, 17 INT C AUDITORY DI, P156
   InspectorJ (pseud.), 2017, FREE SOUND AUDIO 001
   Isherwood SJ, 2017, ERGONOMICS, V60, P1014, DOI 10.1080/00140139.2016.1237677
   Jgrzinich, 2013, FREE SOUND AUDIO 044
   Jie Xu, 2010, 2010 IEEE International Conference on Industrial Engineering & Engineering Management (IE&EM 2010), P2435, DOI 10.1109/IEEM.2010.5674377
   Keller P, 2004, J EXP PSYCHOL-APPL, V10, P3, DOI 10.1037/1076-898X.10.1.3
   Leung Y. K, 1997, 3 INT C AUD DISPL IC, P129
   Marcell ME, 2000, J CLIN EXP NEUROPSYC, V22, P830, DOI 10.1076/jcen.22.6.830.949
   Marcell M, 2007, BEHAV RES METHODS, V39, P561, DOI 10.3758/BF03193026
   Matthews_sounds (pseud.), 2019, FREE SOUND AUDIO 002
   McKeown D, 2007, HUM FACTORS, V49, P417, DOI 10.1518/001872007X200067
   McKeown D, 2010, HUM FACTORS, V52, P54, DOI 10.1177/0018720810366861
   Mynatt E.D., 1994, P C AUDITORY DISPLAY, P109
   Mynatt ED, 1997, HUM-COMPUT INTERACT, V12, P7, DOI 10.1207/s15327051hci1201&2_2
   Nees MA, 2016, HUM FACTORS, V58, P416, DOI 10.1177/0018720816629279
   Perry NC, 2007, HUM FACTORS, V49, P1061, DOI 10.1518/001872007X249929
   Radio_ fragola_gorizia, 2017, FREE SOUND AUDIO 001
   RSilveira_88, 2014, FREE SOUND AUDIO 001
   Stevens Catherine J, 2009, Adv Cogn Psychol, V5, P84, DOI 10.2478/v10053-008-0064-6
   Turchinoa (pseud.), 2017, FREE SOUND AUDIO 000
   Vataaa (pseud.), 2012, FREE SOUND AUDIO 000
   VKProduktion (pseud.), 2016, FREE SOUND AUDIO 010
   Wang Min J., 2012, PROC 4 INT C AUTOMOT, P77, DOI 10.1145/2390256.2390268
   Winters J. J, 1998, THESIS
NR 42
TC 0
Z9 0
U1 3
U2 13
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD APR
PY 2022
VL 19
IS 2
AR 8
DI 10.1145/3527269
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 3A7CY
UT WOS:000827414800004
DA 2024-07-18
ER

PT J
AU Gigilashvili, D
   Shi, WQ
   Wang, ZY
   Pedersen, M
   Hardeberg, JY
   Rushmeier, H
AF Gigilashvili, Davit
   Shi, Weiqi
   Wang, Zeyu
   Pedersen, Marius
   Hardeberg, Jon Yngve
   Rushmeier, Holly
TI The Role of Subsurface Scattering in Glossiness Perception
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Material appearance; gloss perception; translucency perception;
   subsurface light transport; MTurk
ID VISUAL-PERCEPTION; SURFACE GLOSS; ILLUMINATION; REFLECTANCE; SHAPE
AB This study investigates the potential impact of subsurface light transport on gloss perception for the purposes of broadening our understanding of visual appearance in computer graphics applications. Gloss is an important attribute for characterizing material appearance. We hypothesize that subsurface scattering of light impacts the glossiness perception. However, gloss has been traditionally studied as a surface-related quality and the findings in the state-of-the-art are usually based on fully opaque materials, although the visual cues of glossiness can be impacted by light transmission as well. To address this gap and to test our hypothesis, we conducted psychophysical experiments and found that subjects are able to tell the difference in terms of gloss between stimuli that differ in subsurface light transport but have identical surface qualities and object shape. This gives us a clear indication that subsurface light transport contributes to a glossy appearance. Furthermore, we conducted additional experiments and found that the contribution of subsurface scattering to gloss varies across different shapes and levels of surface roughness. We argue that future research on gloss should include transparent and translucent media and to extend the perceptual models currently limited to surface scattering to more general ones inclusive of subsurface light transport.
C1 [Gigilashvili, Davit; Pedersen, Marius; Hardeberg, Jon Yngve] Norwegian Univ Sci & Technol, 22 Teknol Veien, N-2815 Ametyst Bygget, Gjovik, Norway.
   [Shi, Weiqi; Wang, Zeyu; Rushmeier, Holly] Yale Univ, Graph Lab, Dept Comp Sci, POB 208285, New Haven, CT 06520 USA.
C3 Norwegian University of Science & Technology (NTNU); Yale University
RP Gigilashvili, D (corresponding author), Norwegian Univ Sci & Technol, 22 Teknol Veien, N-2815 Ametyst Bygget, Gjovik, Norway.
EM davit.gigilashvili@ntnu.no; weiqi.shi@yale.edu; zeyu.wang@yale.edu;
   marius.pedersen@ntnu.no; jon.hardeberg@ntnu.no; holly.rushmeier@yale.edu
RI Gigilashvili, Davit/AAB-7545-2022; Pedersen, Marius/AFT-7128-2022
OI Gigilashvili, Davit/0000-0002-6956-6569; Rushmeier,
   Holly/0000-0001-5241-0886; Wang, Zeyu/0000-0001-5374-6330
FU NSF [IIS-2007283, 250293, 288187]; Research Council of Norway
FX This work was supported in part by NSF Grant No. IIS-2007283. The work
   was also supported by MUVApp (Grant No. 250293) and MANER (Grant No.
   288187) projects of the Research Council of Norway
CR Adelson EH, 2001, PROC SPIE, V4299, P1, DOI 10.1117/12.429489
   Anderson BL, 2009, J VISION, V9, DOI 10.1167/9.11.10
   [Anonymous], 2000, Psychometric scaling, a toolkit for imaging systems development
   ASTM, 2017, STAND TERM APP, DOI [10.1520/E0284-17, DOI 10.1520/E0284-17]
   BECK J, 1981, PERCEPT PSYCHOPHYS, V30, P407, DOI 10.3758/BF03206160
   Chadwick AC, 2018, J VISION, V18, DOI 10.1167/18.11.18
   Chowdhury NS, 2017, J VISION, V17, DOI 10.1167/17.3.17
   Cohen J., 1988, STAT POWER ANAL BEHA
   Dastan Alireza, 2020, GAUSSIAN MEAN CURVAT
   Doerschner K, 2011, CURR BIOL, V21, P2010, DOI 10.1016/j.cub.2011.10.036
   Eugene C., 2008, Symposium on Man, Science Measurement, P61
   Faul F, 2019, J VISION, V19, DOI 10.1167/19.13.1
   Fleming R. W., 2005, ACM Transactions on Applied Perception (TAP), V2, P346, DOI DOI 10.1145/1077399.1077409
   Fleming RW, 2014, VISION RES, V94, P62, DOI 10.1016/j.visres.2013.11.004
   Fleming RW, 2003, J VISION, V3, P347, DOI 10.1167/3.5.3
   Frey BJ, 2007, SCIENCE, V315, P972, DOI 10.1126/science.1136800
   Gigilashvili D., 2019, Color and imaging conference, V2019, P37, DOI [10.2352/issn.2169-2629.2019.27.8, DOI 10.2352/ISSN.2169-2629.2019.27.8]
   Gigilashvili D., 2019, COL IM C SOC IM SCI, V27, P126, DOI 10.2352/issn.2169-2629.2019.27.24
   Gigilashvili Davit, 2019, P IS T INT S EL IM S
   Gigilashvili Davit., 2019, COLOR IMAGING C, V2019, P132
   Gigilashvili Davit, 2018, COLOR IMAGING C, V2018, P294
   Gigilashvili Davit, 2020, P 10 COL VIS COMP S
   Gkioulekas I, 2015, PROC CVPR IEEE, P5528, DOI 10.1109/CVPR.2015.7299192
   Gkioulekas I, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2516971.2516972
   Green Phil J., 2003, COL ENG TOOLB
   Guarnera D, 2018, SIGGRAPH'18: ACM SIGGRAPH 2018 TALKS, DOI 10.1145/3214745.3214807
   Ho YX, 2008, PSYCHOL SCI, V19, P196, DOI 10.1111/j.1467-9280.2008.02067.x
   HOLM S, 1979, SCAND J STAT, V6, P65
   Hunter R., 1937, J. Res. Natl. Bur. Stand, V18, P19, DOI [DOI 10.6028/JRES.018.006, 10.6028/jres.018.006]
   Jakob Wenzel, 2010, Mitsuba renderer
   Kerrigan IS, 2013, J VISION, V13, DOI 10.1167/13.1.9
   Kim J, 2011, J VISION, V11, DOI 10.1167/11.9.4
   Lagunas M, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3323036
   Landy MS, 2007, NATURE, V447, P158, DOI 10.1038/nature05714
   Leloup FB, 2014, COLOR RES APPL, V39, P559, DOI 10.1002/col.21846
   Marlow P, 2011, J VISION, V11, DOI 10.1167/11.9.16
   Marlow PJ, 2015, CURR BIOL, V25, pR221, DOI 10.1016/j.cub.2015.01.062
   Marlow PJ, 2013, J VISION, V13, DOI 10.1167/13.14.2
   Marlow PJ, 2012, CURR BIOL, V22, P1909, DOI 10.1016/j.cub.2012.08.009
   Meyer N, 2003, VISUALIZATION AND MATHEMATICS III, P35
   mfa Boston CAMEO, 2020, PARAFFIN WAX
   Motoyoshi I, 2007, NATURE, V447, P206, DOI 10.1038/nature05724
   Motoyoshi I, 2010, J VISION, V10, DOI 10.1167/10.9.6
   Nagai T, 2013, I-PERCEPTION, V4, P407, DOI 10.1068/i0576
   Nishida S.y., 2008, Journal of Vision, V8, P339, DOI DOI 10.1167/8.6.339
   Obein G, 2004, J VISION, V4, P711, DOI 10.1167/4.9.4
   Olkkonen M, 2011, I-PERCEPTION, V2, P1014, DOI 10.1068/i0480
   Pellacini F, 2000, COMP GRAPH, P55, DOI 10.1145/344779.344812
   Pereira T, 2012, COMPUT GRAPH FORUM, V31, P1557, DOI 10.1111/j.1467-8659.2012.03152.x
   Pizlo Z, 2001, VISION RES, V41, P3145, DOI 10.1016/S0042-6989(01)00173-0
   Qi L, 2015, VISION RES, V115, P209, DOI 10.1016/j.visres.2015.04.014
   Qi L, 2012, PROCEEDINGS OF THE ACM SIGSPATIAL INTERNATIONAL WORKSHOP ON GEOSTREAMING (IWGS) 2012, P48
   Qi L, 2014, J OPT SOC AM A, V31, P935, DOI 10.1364/JOSAA.31.000935
   Rosnow RL, 2003, CAN J EXP PSYCHOL, V57, P221, DOI 10.1037/h0087427
   Sakano Y, 2010, J VISION, V10, DOI 10.1167/10.9.15
   Sawayama Masataka, 2019, VISUAL DISCRIMINATIO, DOI [10.1101/800870v2.full, DOI 10.1101/800870V2.FULL]
   Schmid Alexandra C., 2020, MAT CATEGORY DETERMI, DOI [10.1101/2019.12.31.892083v1, DOI 10.1101/2019.12.31.892083V1]
   Scientific Polymer Products Inc., 2020, REFR IND POL
   Sharan L, 2014, J VISION, V14, DOI 10.1167/14.9.12
   Stanford University Computer Graphics Laboratory, 1994, STANF 3D SCANN RESP
   Sun TC, 2017, COMPUT GRAPH FORUM, V36, P47, DOI 10.1111/cgf.13223
   Thomas JB, 2017, LECT NOTES COMPUT SC, V10213, P233, DOI 10.1007/978-3-319-56010-6_20
   Thurstone LL, 1927, PSYCHOL REV, V34, P273, DOI 10.1037/h0070288
   Toscani M, 2020, ACM T APPL PERCEPT, V17, DOI 10.1145/3380741
   Tsukida Kristi, 2011, ANAL PAIRED COMPARIS
   Vangorp P, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1276377.1276473, 10.1145/1239451.1239528]
   Walter B., 2007, EUROGRAPHICS C RENDE
   WARD GJ, 1992, COMP GRAPH, V26, P265, DOI 10.1145/142920.134078
   Wendt G, 2010, J VISION, V10, DOI 10.1167/10.9.7
   Wijntjes MWA, 2010, J VISION, V10, DOI 10.1167/10.9.13
   Wills J, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1559755.1559760
   Xiao B, 2008, VISUAL NEUROSCI, V25, P371, DOI 10.1017/S0952523808080267
   Xiao B, 2014, J VISION, V14, DOI 10.1167/14.3.17
   Xiao Bei, 2019, EFFECT GEOMETRIC SHA, DOI [10.1101/795294v1, DOI 10.1101/795294V1]
NR 74
TC 11
Z9 13
U1 1
U2 8
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD JUL
PY 2021
VL 18
IS 3
AR 10
DI 10.1145/3458438
PG 26
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA UD5LZ
UT WOS:000687249300001
OA Bronze
DA 2024-07-18
ER

PT J
AU Park, W
   Jamil, MH
   Gebremedhin, RG
   Eid, M
AF Park, Wanjoo
   Jamil, Muhammad Hassan
   Gebremedhin, Ruth Ghidey
   Eid, Mohamad
TI Effects of Tactile Textures on Preference in Visuo-Tactile Exploration
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Haptic texture; tactile perception; affective computing
ID SURFACE; PERCEPTION
AB The use of haptic technologies has recently become immensely essential in Human-Computer Interaction to improve user experience and performance. With the introduction of tactile feedback on a touchscreen device, commonly known as surface haptics, several applications and interaction paradigms have become a reality. However, the effects of tactile feedback on the preference of 2D images in visuo-tactile exploration task on touchscreen devices remain largely unknown. In this study, we investigated differences of preference score (the tendency of participants to like/dislike a 2D image based on its visual and tactile properties), reach time, interaction time, and response time under four conditions of feedback: no tactile feedback, high-quality of tactile information (sharp tactile texture), low-quality of tactile information (blurred tactile texture), and incorrect tactile information (mismatch tactile texture). The tactile feedback is rendered in the form of roughness that is simulated by modulating the friction between the finger and the surface and is derived from the 2D image. Thirty-six participants completed visuo-tactile exploration tasks for a total of 36 trials (3 2D images x 4 tactile textures x 3 repetitions). Results showed that the presence of tactile feedback enhanced users' preference (tactile feedback conditions were rated significantly higher than the no tactile feedback condition for preference regardless of the quality/correctness of tactile feedback). This finding is also supported through results from self-reporting where 88.89% of participants preferred to experience the 2D image with tactile feedback. Additionally, the presence of tactile feedback resulted in significantly larger interaction time and response time compared to the no tactile feedback condition. Furthermore, the quality and correctness of tactile information significantly impacted the preference rating (sharp tactile textures were rated statistically higher than blurred tactile and mismatched tactile textures). All of these findings demonstrate that tactile feedback plays a crucial role in users' preference and thus motivates further the development of surface haptic technologies.
C1 [Park, Wanjoo; Jamil, Muhammad Hassan; Gebremedhin, Ruth Ghidey; Eid, Mohamad] New York Univ Abu Dhabi, POB 129188, Abu Dhabi, U Arab Emirates.
C3 New York University Abu Dhabi
RP Park, W (corresponding author), New York Univ Abu Dhabi, POB 129188, Abu Dhabi, U Arab Emirates.
EM wanjoo@nyu.edu; hassan.jamil@nyu.edu; ruth.gebremedhin@nyu.edu;
   mohamad.eid@nyu.edu
OI Park, Wanjoo/0000-0003-1467-4156
CR Abdouni A, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-018-32724-4
   Banter Bruce, 2010, Information Display, V26, P26
   Basdogan C, 2020, IEEE T HAPTICS, V13, P450, DOI 10.1109/TOH.2020.2990712
   Bau O., 2010, P 23 ANN ACM S US IN, P283, DOI DOI 10.1145/1866029.1866074
   Bochereau S, 2018, ACM T APPL PERCEPT, V15, DOI 10.1145/3152764
   Brewster S, 2007, CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1 AND 2, P159
   Buxton W., 1985, Computer Graphics, V19, P215, DOI 10.1145/325165.325239
   Chen XJ, 2009, INT J DES, V3, P67
   Chubb EC, 2010, IEEE T HAPTICS, V3, P189, DOI [10.1109/ToH.2010.7, 10.1109/TOH.2010.7]
   Giraud F, 2018, IEEE HAPTICS SYM, P210, DOI 10.1109/HAPTICS.2018.8357178
   Hamam A, 2013, MULTIMED TOOLS APPL, V67, P455, DOI 10.1007/s11042-012-0990-7
   HARTMAN ML, 1992, PERS INDIV DIFFER, V13, P805, DOI 10.1016/0191-8869(92)90054-S
   Hecht D, 2009, EXP BRAIN RES, V193, P307, DOI 10.1007/s00221-008-1626-z
   Herz RS, 1997, HUM NATURE-INT BIOS, V8, P275, DOI 10.1007/BF02912495
   Hoggan E, 2008, CHI 2008: 26TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS VOLS 1 AND 2, CONFERENCE PROCEEDINGS, P1573
   Kang J, 2017, IEEE T HAPTICS, V10, P371, DOI 10.1109/TOH.2016.2635145
   Kim DR, 2019, INT J PRECIS ENG MAN, V20, P749, DOI 10.1007/s12541-019-00022-2
   Klöcker A, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0101361
   Klöcker A, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0079085
   Krzywinski M, 2014, NAT METHODS, V11, P119, DOI 10.1038/nmeth.2813
   Luk J., 2006, P SIGCHI C HUM FACT, P171, DOI DOI 10.1145/1124772.1124800
   Maggioni E., 2017, 2017 9 INT C QUAL MU, P1
   McGlone F, 2014, NEURON, V82, P737, DOI 10.1016/j.neuron.2014.05.001
   Meyer DJ, 2014, IEEE HAPTICS SYM, P63, DOI 10.1109/HAPTICS.2014.6775434
   Meyer DJ, 2013, 2013 WORLD HAPTICS CONFERENCE (WHC), P43, DOI 10.1109/WHC.2013.6548382
   Mullenbach J., 2013, TPAD FIR SURF HAPT T
   Oliver RichardL., 1981, ADV CONSUM RES, V8, P88
   Oum RE, 2011, COGNITION EMOTION, V25, P717, DOI 10.1080/02699931.2010.496997
   Park W, 2018, IEEE ACCESS, V6, P22324, DOI 10.1109/ACCESS.2018.2827023
   Shultz CD, 2015, 2015 IEEE WORLD HAPTICS CONFERENCE (WHC), P57, DOI 10.1109/WHC.2015.7177691
   SPORER SL, 1993, J APPL PSYCHOL, V78, P22, DOI 10.1037/0021-9010.78.1.22
   Steinbach E, 2019, P IEEE, V107, P447, DOI 10.1109/JPROC.2018.2867835
   TAMURA H, 1978, IEEE T SYST MAN CYB, V8, P460, DOI 10.1109/TSMC.1978.4309999
   Wollschlaeger M, 2017, IEEE IND ELECTRON M, V11, P17, DOI 10.1109/MIE.2017.2649104
   Yokosaka T, 2018, IEEE T HAPTICS, V11, P192, DOI 10.1109/TOH.2017.2775631
NR 35
TC 3
Z9 3
U1 3
U2 24
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD JUN
PY 2021
VL 18
IS 2
AR 9
DI 10.1145/3449065
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SR6EX
UT WOS:000661137000005
DA 2024-07-18
ER

PT J
AU Jacobs, J
   Wang, X
   Alexa, M
AF Jacobs, Jochen
   Wang, Xi
   Alexa, Marc
TI Keep It Simple: Depth-based Dynamic Adjustment of Rendering for
   Head-mounted Displays Decreases Visual Comfort
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article; Proceedings Paper
CT 16th Symposium on Applied Perception (SAP)
CY SEP, 2019
CL Barcelona, SPAIN
DE Head-mounted displays; vergence; fatigue
ID 3D
AB Head-mounted displays cause discomfort. This is commonly attributed to conflicting depth cues, most prominently between vergence, which is consistent with object depth, and accommodation, which is adjusted to the near eye displays.
   It is possible to adjust the camera parameters, specifically interocular distance and vergence angles, for rendering the virtual environment to minimize this conflict. This requires dynamic adjustment of the parameters based on object depth. In an experiment based on a visual search task, we evaluate how dynamic adjustment affects visual comfort compared to fixed camera parameters. We collect objective as well as subjective data. Results show that dynamic adjustment decreases common objective measures of visual comfort such as pupil diameter and blink rate by a statistically significant margin. The subjective evaluation of categories such as fatigue or eye irritation shows a similar trend but was inconclusive. This suggests that rendering with fixed camera parameters is the better choice for head-mounted displays, at least in scenarios similar to the ones used here.
C1 [Jacobs, Jochen; Wang, Xi; Alexa, Marc] TU Berlin, Marchstr 23, D-10587 Berlin, Germany.
C3 Technical University of Berlin
RP Jacobs, J (corresponding author), TU Berlin, Marchstr 23, D-10587 Berlin, Germany.
EM jochen.jacobs@campus.tu-berlin.de; xi.wang@tu-berlin.de;
   marc.alexa@tu-berlin.de
OI Wang, Xi/0000-0001-5442-1116; Jacobs, Jochen/0000-0002-7060-3929
CR Atchison D., 2000, OPTICS HUMAN EYE, DOI DOI 10.1016/B978-0-7506-3775-6.50022-5
   Cardona G, 2011, CURR EYE RES, V36, P190, DOI 10.3109/02713683.2010.544442
   Chen W, 2011, PROC SPIE, V7863, DOI 10.1117/12.872332
   Duchowski A.T., 2014, P ACM S APPL PERC, P39, DOI DOI 10.1145/2628257.2628259
   Fisker Martin, 2013, EUROGRAPHICS
   Garbin Stephan J., 2019, CORR
   Hoffman DM, 2008, J VISION, V8, DOI 10.1167/8.3.33
   Iatsun I, 2013, PROC SPIE, V8648, DOI 10.1117/12.2008206
   Johnson PV, 2016, OPT EXPRESS, V24, P1808, DOI 10.1364/OE.24.011808
   Kellnhofer P, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925866
   Kenneth HolmqvistMarcus Nystrom., 2011, Eye Tracking: A Comprehensive Guide to Methods and Measures
   Kim DI, 2011, J IND ENG CHEM, V17, P1, DOI 10.1016/j.jiec.2010.12.010
   Konrad R, 2016, 34TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, CHI 2016, P1211, DOI 10.1145/2858036.2858140
   Kooi FL, 2004, DISPLAYS, V25, P99, DOI 10.1016/j.displa.2004.07.004
   Koulieris GA, 2016, P IEEE VIRT REAL ANN, P113, DOI 10.1109/VR.2016.7504694
   Koulieris GA, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073622
   Lang M, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778812
   Lockhart TE, 2010, ERGONOMICS, V53, P892, DOI 10.1080/00140139.2010.489968
   Lüdtke H, 1998, VISION RES, V38, P2889, DOI 10.1016/S0042-6989(98)00081-9
   Maimone A, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2503144
   Mendiburu Bernard, 2012, 3D movie making: stereoscopic digital cinema from script to screen
   Morad Y, 2000, CURR EYE RES, V21, P535, DOI 10.1076/0271-3683(200007)21:1;1-Z;FT535
   Oskam T, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024223
   Padmanaban N, 2017, P NATL ACAD SCI USA, V114, P2183, DOI 10.1073/pnas.1617251114
   PELI E., 2001, SID INT SYMP DIG, V53, P1296, DOI DOI 10.1889/1.1831799
   Schor CM, 1999, VISION RES, V39, P3769, DOI 10.1016/S0042-6989(99)00094-2
   Schumann Andy, 2017, Current Directions in Biomedical Engineering, V3, P583, DOI 10.1515/cdbme-2017-0121
   Sherstyuk A., 2012, Proceedings of the ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games, P23
   Shibata T, 2011, J VISION, V11, DOI 10.1167/11.8.11
   Shiwa S., 1996, Journal of the Society for Information Display, V4, P255, DOI 10.1889/1.1987395
   Stelmach LB, 2003, P SOC PHOTO-OPT INS, V5006, P269, DOI 10.1117/12.474093
   Tam WJ, 2011, IEEE T BROADCAST, V57, P335, DOI 10.1109/TBC.2011.2125070
   Terzic K, 2016, SIGNAL PROCESS-IMAGE, V47, P402, DOI 10.1016/j.image.2016.08.002
   Wang X, 2017, MATH VIS, P169, DOI 10.1007/978-3-319-47024-5_10
   WOODS A, 1993, P SOC PHOTO-OPT INS, V1915, P36, DOI 10.1117/12.157041
   Yamada Y, 2018, ARTIF INTELL MED, V91, P39, DOI 10.1016/j.artmed.2018.06.005
   Yamada Y, 2017, 2017 IEEE INTERNATIONAL CONFERENCE ON HEALTHCARE INFORMATICS (ICHI), P275, DOI 10.1109/ICHI.2017.74
   Zelle J. M., 2004, SIGCSE Bulletin, V36, P348, DOI 10.1145/1028174.971421
   Zhao S. Y., 2010, P INT C E PROD E SER, P1, DOI DOI 10.1109/ICMULT.2010.5630864
NR 39
TC 9
Z9 10
U1 0
U2 2
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD SEP
PY 2019
VL 16
IS 3
SI SI
AR 16
DI 10.1145/3353902
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA JA3KH
UT WOS:000487720000004
DA 2024-07-18
ER

PT J
AU Rosa, N
   Veltkamp, RC
   Hürst, W
   Nijboer, T
   Gilbers, C
   Werkhoven, P
AF Rosa, Nina
   Veltkamp, Remco C.
   Hurst, Wolfgang
   Nijboer, Tanja
   Gilbers, Carolien
   Werkhoven, Peter
TI The Supernumerary Hand Illusion in Augmented Reality
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Sense of embodiment; rubber hand illusion; virtual hand illusion;
   supernumerary hand illusion; multisensory; ownership; agency;
   self-location; augmented reality; skin conductance habituation
ID VIRTUAL-REALITY; RUBBER; EMBODIMENT; IMMERSION; SENSE
AB The classic rubber hand illusion (RHI) experiment studies the sense of embodiment over a fake limb. Distinguished subcomponents of embodiment are ownership (sense of self-attribution of a body), agency (sense of having motor control), and self-location (the spatial experience of being inside a body), and are typically evoked in either reality or virtual reality. In augmented reality (AR), however, visually present real limbs can be augmented with (multiple) fake virtual limbs, which results in a variation of the RHI, the augmented reality supernumerary hand illusion (ARSHI). Such conditions occur, for example, in first-person AR games and in AR-interfaces for tele-robotics. In this article, we examined to what extent humans can experience the sense of embodiment over a supernumerary virtual arm in addition to one or two real arms. We also examine how embodiment is affected by the perceptual visual-tactile synchronicity of the virtual and real limbs, and by the synchronicity of active movement of the virtual and real hand. Embodiment was measured subjectively by questionnaire and objectively by skin conductance responses (SCRs). Questionnaire responses show that ownership, agency, and self-location can be evoked over the virtual arm in the presence of a real arm, and that they are significantly stronger for synchronous conditions than for asynchronous conditions. The perceptual and motorical synchronous condition with three visible hands led to an experience of owning the virtual hand. These responses further show that agency was also strongly experienced over the supernumerary virtual arm, and responses regarding self-location suggest a shift in sensed location when one real arm was in view and an additional location when both real arms where in view. SCRs show no significant effect of condition, but do show a significant habituation effect as a function of the number of conditions performed by participants. When analyzing the relations at the individual participant level between the questionnaire data and skin conductance, we found two clusters of participants: (1) participants with low questionnaire responses and low-medium SCRs and (2) participants with high questionnaire responses and low-high SCRs. Finally, we discuss how virtual hand appearance/realism and willingness to accept virtual limbs could play an important role in the ARSHI, and provide insights on intricacies involved with measuring and evaluating RHIs.
C1 [Rosa, Nina; Veltkamp, Remco C.; Hurst, Wolfgang; Gilbers, Carolien; Werkhoven, Peter] Univ Utrecht, Dept Informat & Comp Sci, Princetonpl 5, NL-3584 CC Utrecht, Netherlands.
   [Nijboer, Tanja] Univ Utrecht, Utrecht, Netherlands.
   [Nijboer, Tanja] Univ Med Ctr Utrecht, Utrecht, Netherlands.
   [Nijboer, Tanja] Rehabil Ctr Hoogstr, Utrecht, Netherlands.
   [Nijboer, Tanja] Univ Utrecht, Dept Psychol, Univ Med Ctr Utrecht, Rehabil Med,Rehabil Ctr Hoogstr, Utrecht, Netherlands.
   [Werkhoven, Peter] Univ Utrecht, Dept Informat & Comp Sci, TNO, Tech Sci, Utrecht, Netherlands.
C3 Utrecht University; Utrecht University; Utrecht University; Utrecht
   University Medical Center; Utrecht University; Utrecht University
   Medical Center; Utrecht University; Netherlands Organization Applied
   Science Research
RP Rosa, N (corresponding author), Univ Utrecht, Dept Informat & Comp Sci, Princetonpl 5, NL-3584 CC Utrecht, Netherlands.
EM n.e.rosa@uu.nl; r.c.veltkamp@uu.nl; huerst@uu.nl; t.c.w.nijboer@uu.nl;
   p.j.werkhoven@uu.nl
RI Nijboer, Tanja C/A-4509-2010
OI Rosa, Nina/0000-0002-1892-9702
FU Netherlands Organization for Scientific Research (NWO) [022.005.017]
FX This work was supported by the Netherlands Organization for Scientific
   Research (NWO), project number 022.005.017.
CR Abdulkarim Z, 2016, ATTEN PERCEPT PSYCHO, V78, P707, DOI 10.3758/s13414-015-1016-0
   Armel KC, 2003, P ROY SOC B-BIOL SCI, V270, P1499, DOI 10.1098/rspb.2003.2364
   Botvinick M, 1998, NATURE, V391, P756, DOI 10.1038/35784
   Chen WY, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-018-19662-x
   DEPASCALIS V, 1995, INT J PSYCHOPHYSIOL, V20, P21, DOI 10.1016/0167-8760(95)00023-L
   Ehrsson HH, 2009, PERCEPTION, V38, P310, DOI 10.1068/p6304
   Ferri AnthonyJ., 2007, Willing Suspension of Disbelief: Poetic Faith in Film
   Feuchtner T, 2017, PROCEEDINGS OF THE 2017 ACM SIGCHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'17), P5145, DOI 10.1145/3025453.3025689
   Folegatti A, 2012, CONSCIOUS COGN, V21, P799, DOI 10.1016/j.concog.2012.02.008
   Gilbers Carolien, 2017, THESIS
   Guterstam A, 2011, PLOS ONE, V6, DOI 10.1371/journal.pone.0017208
   IJsselsteijn WA, 2006, PRESENCE-TELEOP VIRT, V15, P455, DOI 10.1162/pres.15.4.455
   Kilteni K, 2012, PRESENCE-TELEOP VIRT, V21, P373, DOI 10.1162/PRES_a_00124
   Kilteni K, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0040867
   Lira M, 2017, SCI REP-UK, V7, DOI 10.1038/s41598-017-16137-3
   Lombard M., 2006, J. Comput. Mediat. Commun, V3, P72, DOI [DOI 10.1111/J.1083-6101.1997.TB00072.X, https://doi.org/10.1111/j.1083-6101.1997.tb00072.x]
   Longo MR, 2008, COGNITION, V107, P978, DOI 10.1016/j.cognition.2007.12.004
   MILGRAM P, 1994, IEICE T INF SYST, VE77D, P1321
   MONTAGU JD, 1963, J PSYCHOSOM RES, V7, P199, DOI 10.1016/0022-3999(63)90004-7
   Moon S, 2018, 2018 32ND INTERNATIONAL CONFERENCE ON INFORMATION NETWORKING (ICOIN), P67, DOI 10.1109/ICOIN.2018.8343086
   Newport R, 2010, EXP BRAIN RES, V204, P385, DOI 10.1007/s00221-009-2104-y
   Rohde M, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0080688
   Rosa N, 2016, PROCEEDINGS OF THE 2016 WORKSHOP ON MULTIMODAL VIRTUAL AND AUGMENTED REALITY (MVAR 2016), DOI 10.1145/3001959.3001961
   Ryan ML, 1999, SUB-STANCE, P110
   Schaefer M, 2009, HUM BRAIN MAPP, V30, P1413, DOI 10.1002/hbm.20609
   Slater M, 2009, FRONT NEUROSCI-SWITZ, V3, P214, DOI 10.3389/neuro.01.029.2009
   Suzuki K, 2013, NEUROPSYCHOLOGIA, V51, P2909, DOI 10.1016/j.neuropsychologia.2013.08.014
   Tsakiris M, 2010, EXP BRAIN RES, V204, P343, DOI 10.1007/s00221-009-2039-3
   Weibel D, 2010, CYBERPSYCH BEH SOC N, V13, P251, DOI 10.1089/cyber.2009.0171
NR 29
TC 14
Z9 15
U1 0
U2 25
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD AUG
PY 2019
VL 16
IS 2
AR 12
DI 10.1145/3341225
PG 20
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA JC8UL
UT WOS:000489551500006
OA Green Published
DA 2024-07-18
ER

PT J
AU Aviles-Rivero, AI
   Alsaleh, SM
   Philbeck, J
   Raventos, SP
   Younes, N
   Hahn, JK
   Casals, A
AF Aviles-Rivero, Angelica, I
   Alsaleh, Samar M.
   Philbeck, John
   Raventos, Stella P.
   Younes, Naji
   Hahn, James K.
   Casals, Alicia
TI Sensory Substitution for Force Feedback Recovery: A Perception
   Experimental Study
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Robotic teleoperation; flow visualization; visualization
ID ROBOTIC SURGERY; HAPTIC FEEDBACK; ASSISTED SURGERY; TELEOPERATION;
   TRANSPARENCY; PERFORMANCE; INTERFACES; ENSURE; FUTURE; CORTEX
AB Robotic-assisted surgeries are commonly used today as a more efficient alternative to traditional surgical options. Both surgeons and patients benefit from those systems, as they offer many advantages, including less trauma and blood loss, fewer complications, and better ergonomics. However, a remaining limitation of currently available surgical systems is the lack of force feedback due to the teleoperation setting, which prevents direct interaction with the patient. Once the force information is obtained by either a sensing device or indirectly through vision-based force estimation, a concern arises on how to transmit this information to the surgeon. An attractive alternative is sensory substitution, which allows transcoding information from one sensory modality to present it in a different sensory modality. In the current work, we used visual feedback to convey interaction forces to the surgeon. Our overarching goal was to address the following question: How should interaction forces be displayed to support efficient comprehension by the surgeon without interfering with the surgeon's perception and workflow during surgery? Until now, the use the visual modality for force feedback has not been carefully evaluated. For this reason, we conducted an experimental study with two aims: (1) to demonstrate the potential benefits of using this modality and (2) to understand the surgeons' perceptual preferences. The results derived from our study of 28 surgeons revealed a strong positive acceptance of the users (96%) using this modality. Moreover, we found that for surgeons to easily interpret the information, their mental model must be considered, meaning that the design of the visualizations should fit the perceptual and cognitive abilities of the end user. To our knowledge, this is the first time that these principles have been analyzed for exploring sensory substitution in medical robotics. Finally, we provide user-centered recommendations for the design of visual displays for robotic surgical systems.
C1 [Aviles-Rivero, Angelica, I] Univ Cambridge, Wilberforce Rd, Cambridge CB3 0WA, England.
   [Alsaleh, Samar M.; Philbeck, John; Younes, Naji; Hahn, James K.] George Washington Univ, 2121 Eye St NW, Washington, DC 20052 USA.
   [Raventos, Stella P.] Josep Trueta Univ Hosp, Ave Franca, Girona, Spain.
   [Casals, Alicia] Univ Politecn Cataluna, Jordi Girona 1-3, Barcelona, Spain.
C3 University of Cambridge; George Washington University; Universitat de
   Girona; Girona University Hospital Dr. Josep Trueta; Universitat
   Politecnica de Catalunya
RP Aviles-Rivero, AI (corresponding author), Univ Cambridge, Wilberforce Rd, Cambridge CB3 0WA, England.
EM ai323@cam.ac.uk; sm57@gwu.edu; philbeck@gwu.edu;
   mpie.girona.ics@gencat.cat; naji@gwu.edu; hahn@gwu.edu;
   alicia.casals@upc.edu
RI Casals, Ariadna/C-7006-2009; Hahn, James/AAF-8272-2021
OI Aviles-Rivero, Angelica I./0000-0002-8878-0325; Casals,
   Alicia/0000-0003-4706-5533
FU Centre for Mathematical Imaging in Healthcare (CMIH), University of
   Cambridge; EPSRC [EP/N014588/1] Funding Source: UKRI
FX Support from the Centre for Mathematical Imaging in Healthcare (CMIH),
   University of Cambridge, is greatly acknowledged. We would like to thank
   to the surgical departments of Obstetrics and Gynecology, Pediatrics,
   Cardiology, Cardiovascular Medicine, and Abdominal and General Surgery
   from the Josep Trueta University Hospital in Girona, Spain, and the
   surgical departments of Pediatrics, Obstetrics and Gynecology, and
   Pediatric Cardiology from the Vall d'Hebron University Hospital in
   Barcelona, Spain. Additionally, the surgical department of Obstetrics
   and Gynecology from the Sant Joan de Deu Hospital in Manresa, Spain.
CR Akinbiyi Takintope, 2006, Conf Proc IEEE Eng Med Biol Soc, V2006, P567
   Andreas Dunser, 2007, P INT WORKSH MIX REA
   [Anonymous], 2013, PERCEPTION COMMUNICA
   [Anonymous], 1962, Uncertainty and structure as psychological concepts
   Aviles AI, 2017, IEEE T HAPTICS, V10, P431, DOI 10.1109/TOH.2016.2640289
   Aviles Angelica I., 2015, P 37 ANN INT C IEEE
   BACH P, 1969, NATURE, V221, P963, DOI 10.1038/221963a0
   Bach-y-Rita P, 2003, TRENDS COGN SCI, V7, P541, DOI 10.1016/j.tics.2003.10.013
   Bach-y-Rita P., 1972, Brain mechanisms in sensory stimulation
   Ben Shneiderman, 2010, DESIGNING USER INTER
   Bensmaïa SJ, 2005, J NEUROPHYSIOL, V94, P3023, DOI 10.1152/jn.00002.2005
   Bethea BT, 2004, J LAPAROENDOSC ADV A, V14, P191, DOI 10.1089/1092642041255441
   Broll W, 2005, IEEE T VIS COMPUT GR, V11, P722, DOI 10.1109/TVCG.2005.90
   Burke J. L., 2006, P 8 INT C MULT INT, P108, DOI [10.1145/1180995.1181017, DOI 10.1145/1180995.1181017, DOI 10.1145/1180995]
   Cuthbert L., 1999, EXPERT NOVICE DIFFER
   Diaz I.n., 2006, Virtual Reality, V10, P31
   Driver J, 2001, BRIT J PSYCHOL, V92, P53, DOI 10.1348/000712601162103
   Enayati Nima, 2016, IEEE Rev Biomed Eng, V9, P49, DOI 10.1109/RBME.2016.2538080
   Faragasso A., 2014, IEEE INT C ROB AUT, P2934
   Faulkner Wendy., 1998, EXPLORING EXPERTISE
   Feygin D, 2002, 10TH SYMPOSIUM ON HAPTIC INTERFACES FOR VIRTUAL ENVIRONMENT AND TELEOPERATOR SYSTEMS, PROCEEDINGS, P40, DOI 10.1109/HAPTIC.2002.998939
   Greminger MA, 2004, IEEE T PATTERN ANAL, V26, P290, DOI 10.1109/TPAMI.2004.1262305
   Gwilliam J.C, 2009, P IEEE INT C ROB AUT, P677
   Helton Kristen L, 2011, J Diabetes Sci Technol, V5, P632
   Hoffman R.R., 1998, How can expertise be defined?: Implications of research from cognitive psychology
   Hollerbach J. M., 2000, Proceedings 2000 ICRA. Millennium Conference. IEEE International Conference on Robotics and Automation. Symposia Proceedings (Cat. No.00CH37065), P757, DOI 10.1109/ROBOT.2000.844142
   Hughes JF., 2014, Computer Graphics: Principles and Practice, V3
   Hughes-Hallett A, 2015, INT J MED ROBOT COMP, V11, P8, DOI 10.1002/rcs.1596
   Jacobs S, 2007, SURG LAPARO ENDO PER, V17, P402, DOI 10.1097/SLE.0b013e3180f60c23
   Judkins TN, 2009, SURG ENDOSC, V23, P590, DOI 10.1007/s00464-008-9933-9
   Karimirad F, 2014, J BIOMECH, V47, P1157, DOI 10.1016/j.jbiomech.2013.12.007
   Kitagawa M, 2004, ST HEAL T, V98, P157
   Koehn JK, 2015, SURG ENDOSC, V29, P2970, DOI 10.1007/s00464-014-4030-8
   Kristjánsson A, 2016, RESTOR NEUROL NEUROS, V34, P769, DOI 10.3233/RNN-160647
   Kroh Matthew., 2015, Essentials of robotic surgery
   Laumann K., 2017, ADV HUMAN ERROR RELI, V589, P63
   Lenay Charles, 2013, TOUCHING KNOWING COG, P275
   Lendvay TS, 2013, CANCER J, V19, P109, DOI 10.1097/PPO.0b013e31828bf822
   LEWIS JR, 1995, INT J HUM-COMPUT INT, V7, P57, DOI 10.1080/10447319509526110
   Mahvash M, 2007, IEEE T ROBOT, V23, P1240, DOI 10.1109/TRO.2007.909825
   Massimino Michael J., 1992, P 5 IFAC S AN DES EV
   Meccariello G, 2016, J ROBOT SURG, V10, P57, DOI 10.1007/s11701-015-0541-0
   Meli L, 2014, IEEE T BIO-MED ENG, V61, P1318, DOI 10.1109/TBME.2014.2303052
   Mountney P, 2010, IEEE SIGNAL PROC MAG, V27, P14, DOI 10.1109/MSP.2010.936728
   Murphy MC, 2016, NEUROIMAGE, V125, P932, DOI 10.1016/j.neuroimage.2015.11.021
   Nagel SK, 2005, J NEURAL ENG, V2, pR13, DOI 10.1088/1741-2560/2/4/R02
   Noohi E, 2014, IEEE INT C INT ROBOT, P4297, DOI 10.1109/IROS.2014.6943169
   Okamoto S, 2011, IEEE T HAPTICS, V4, P307, DOI [10.1109/TOH.2011.16, 10.1109/ToH.2011.16]
   Okamura AM, 2011, SURGICAL ROBOTICS: SYSTEMS APPLICATIONS AND VISIONS, P419, DOI 10.1007/978-1-4419-1126-1_18
   Pacchierotti C, 2015, INT J ROBOT RES, V34, P1773, DOI 10.1177/0278364915603135
   Pacchierotti C, 2014, ACM T APPL PERCEPT, V11, DOI 10.1145/2604969
   Paxton EW, 2010, J BONE JOINT SURG AM, V92A, P117, DOI 10.2106/JBJS.J.00807
   Payne CJ, 2014, IEEE INT CONF ROBOT, P284, DOI 10.1109/ICRA.2014.6906623
   PERLMAN G, 1985, BEHAV RES METH INS C, V17, P203, DOI 10.3758/BF03214383
   Prasad SK, 2003, LECT NOTES COMPUT SC, V2878, P279
   Puangmali P, 2012, IEEE-ASME T MECH, V17, P646, DOI 10.1109/TMECH.2011.2116033
   RAUSCHECKER JP, 1995, TRENDS NEUROSCI, V18, P36, DOI 10.1016/0166-2236(95)93948-W
   Reiley CE, 2008, J THORAC CARDIOV SUR, V135, P196, DOI 10.1016/j.jtcvs.2007.08.043
   Renier Laurent, 2005, Journal of Integrative Neuroscience, V4, P489, DOI 10.1142/S0219635205000999
   Rognini G, 2016, TRENDS COGN SCI, V20, P162, DOI 10.1016/j.tics.2015.12.002
   Root R.W., 1983, P SIGCHI C HUMAN FAC, P83, DOI DOI 10.1145/800045.801586
   Ruurda JP, 2004, SURG ENDOSC, V18, P1249, DOI 10.1007/s00464-003-9191-9
   Schoonmaker Ryan E., 1992, P HUM FACT ERG SOC A, P1029
   Schorr SB, 2015, IEEE T HUM-MACH SYST, V45, P714, DOI 10.1109/THMS.2015.2463090
   Shanteau J., 1992, Expertise and decision support, P11
   Sokhanvar Saeed, 2012, TACTILE SENSING DISP, P245
   Soper N.J., 2008, MASTERY ENDOSCOPIC L
   Spinoglio Giuseppe., 2015, Robotic surgery: current applications and new trends
   Sun Minghui, 2011, J INFORM PROCESSING, V18, P284
   van der Meijden OAJ, 2009, SURG ENDOSC, V23, P1180, DOI 10.1007/s00464-008-0298-x
   Ware C., 2020, INFORM VISUALIZATION
   Weber B, 2014, LECT NOTES COMPUT SC, V8619, P150, DOI 10.1007/978-3-662-44196-1_19
   Wickens C. D., 2002, Theor Issues Ergon Sci, V3, P159, DOI [10.1080/14639220210123806, DOI 10.1080/14639220210123806]
   Wilson EB, 2014, ROBOTICS GEN SURG, P17
   Yip MC, 2010, IEEE T BIO-MED ENG, V57, P1008, DOI 10.1109/TBME.2009.2039570
NR 75
TC 8
Z9 8
U1 0
U2 11
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD AUG
PY 2018
VL 15
IS 3
AR 16
DI 10.1145/3176642
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GY2UV
UT WOS:000448400300002
OA Green Published
DA 2024-07-18
ER

PT J
AU Bochereau, S
   Sinclair, S
   Hayward, V
AF Bochereau, Serena
   Sinclair, Stephen
   Hayward, Vincent
TI Perceptual Constancy in the Reproduction of Virtual Tactile Textures
   With Surface Displays
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Tactile stimulation; haptic texture rendering; design requirements;
   speed perception
ID HAPTIC PERCEPTION; ARM MOVEMENT; ROUGHNESS; VIBRATION; TIME;
   DISCRIMINATION; SPEED; METACOGNITION; SENSATION; ILLUSION
AB For very rough surfaces, friction-induced vibrations contain frequencies that change in proportion to sliding speed. Given the poor capacity of the somatosensory system to discriminate frequencies, this fact raises the question of how accurately finger sliding speed must be known during the reproduction of virtual textures with a surface tactile display. During active touch, ten observers were asked to discriminate texture recordings corresponding to different speeds. The samples were constructed from a common texture, which was resampled at various frequencies to give a set of stimuli of different swiping speeds. In trials, they swiped their finger in rapid succession over a glass plate, which vibrated to accurately reproduce three texture recordings. Two of these recordings were identical and a third differed in that the sample represented a texture swiped at a speed different from the other two. Observers identified which of the three samples felt different. For a metal mesh texture recording, seven observers reported differences when the speed varied by 60, 80, and 100mm/s while the other three did not reach a discrimination threshold. For a finer leather chamois texture recording, thresholds were never reached in the 100mm/s range. These results show that the need for high-accuracy measurement of swiping speed during texture reproduction may actually be quite limited compared to what is commonly found in the literature.
C1 [Bochereau, Serena; Sinclair, Stephen; Hayward, Vincent] Univ Paris 06, Paris, France.
   [Bochereau, Serena; Sinclair, Stephen; Hayward, Vincent] Sorbonne Univ, ISIR, F-75005 Paris, France.
C3 Sorbonne Universite; Sorbonne Universite
RP Bochereau, S (corresponding author), Univ Paris 06, Paris, France.; Bochereau, S (corresponding author), Sorbonne Univ, ISIR, F-75005 Paris, France.
EM bochereau@isir.upmc.fr; sinclair@isir.upmc.fr;
   vincent.hayward@sorbonne-universite.fr
RI Hayward, Vincent/A-4646-2010
OI Hayward, Vincent/0000-0002-2102-1965
FU FP7 Marie Curie Initial Training Network PROTOTOUCH [317100]; European
   Research Council (FP7) ERC Advanced Grant (patch) [247300]; European
   Research Council (ERC) [247300] Funding Source: European Research
   Council (ERC)
FX The authors thank OpheliaDeroy for her suggestion to employ in this
   study ideas from the field of metacognition. This study was funded by
   the FP7 Marie Curie Initial Training Network PROTOTOUCH (Grant No.
   317100) and by the European Research Council (FP7) ERC Advanced Grant
   (patch) to V.H. (Grant No. 247300).
CR Altinsoy ME, 2012, IEEE T HAPTICS, V5, P6, DOI [10.1109/TOH.2011.56, 10.1109/ToH.2011.56]
   Armstrong L, 1999, PERCEPT PSYCHOPHYS, V61, P1211, DOI 10.3758/BF03207624
   Asano S, 2015, IEEE T HUM-MACH SYST, V45, P393, DOI 10.1109/THMS.2014.2376519
   Atkeson C. G., 1984, AIM790 MIT
   Biet M, 2007, IEEE T ULTRASON FERR, V54, P2678, DOI 10.1109/TUFFC.2007.596
   Bochereau S, 2015, 2015 IEEE WORLD HAPTICS CONFERENCE (WHC), P119, DOI 10.1109/WHC.2015.7177701
   Bochereau Serena., 2017, IEEE Transactions on Haptics
   Campion G, 2005, WORLD HAPTICS CONFERENCE: FIRST JOINT EUROHAPTICS CONFERENCE AND SYMPOSIUM ON HAPTIC INTERFACES FOR VIRUTUAL ENVIRONMENT AND TELEOPERATOR SYSTEMS, PROCEEDINGS, P263
   Cascio CJ, 2001, J NEUROSCI, V21, P5289, DOI 10.1523/JNEUROSCI.21-14-05289.2001
   COREN S, 1980, J EXP PSYCHOL HUMAN, V6, P404, DOI 10.1037/0096-1523.6.3.404
   Culbertson H, 2015, 2015 IEEE WORLD HAPTICS CONFERENCE (WHC), P106, DOI 10.1109/WHC.2015.7177699
   Culbertson H, 2014, IEEE T HAPTICS, V7, P381, DOI 10.1109/TOH.2014.2316797
   Dallmann CJ, 2015, J NEUROPHYSIOL, V114, P3131, DOI 10.1152/jn.00621.2015
   Delhaye B, 2012, FRONT BEHAV NEUROSCI, V6, DOI 10.3389/fnbeh.2012.00037
   Dépeault A, 2008, J NEUROPHYSIOL, V99, P1422, DOI 10.1152/jn.01209.2007
   Deroy O, 2016, TRENDS COGN SCI, V20, P736, DOI 10.1016/j.tics.2016.08.006
   Dupin L, 2015, P NATL ACAD SCI USA, V112, P619, DOI 10.1073/pnas.1419539112
   Giraud F, 2013, 2013 WORLD HAPTICS CONFERENCE (WHC), P199, DOI 10.1109/WHC.2013.6548408
   GOFF GD, 1967, J EXP PSYCHOL, V74, P294, DOI 10.1037/h0024561
   GOODWIN AW, 1989, J NEUROSCI, V9, P1280
   Hartcher-O'Brien J, 2014, LECT NOTES COMPUT SC, V8618, P77, DOI 10.1007/978-3-662-44193-0_11
   HOCHBERG JE, 1957, PSYCHOL REV, V64, P73, DOI 10.1037/h0043738
   HOLLERBACH JM, 1982, BIOL CYBERN, V44, P67, DOI 10.1007/BF00353957
   Hollins M, 2000, PERCEPT PSYCHOPHYS, V62, P695, DOI 10.3758/BF03206916
   Hollins M, 2002, BEHAV BRAIN RES, V135, P51, DOI 10.1016/S0166-4328(02)00154-7
   Hughes B, 2007, WORLD HAPTICS 2007: SECOND JOINT EUROHAPTICS CONFERENCE AND SYMPOSIUM ON HAPTIC INTERFACES FOR VIRTUAL ENVIRONMENT AND TELEOPERATOR SYSTEMS, PROCEEDINGS, P66
   Jadhav SP, 2009, NAT NEUROSCI, V12, P792, DOI 10.1038/nn.2328
   Janabi-Sharifi F, 2000, IEEE T CONTR SYST T, V8, P1003, DOI 10.1109/87.880606
   Jones LM, 2004, SCIENCE, V304, P1986, DOI 10.1126/science.1097779
   Klatzky RL, 2003, PERCEPT PSYCHOPHYS, V65, P613, DOI 10.3758/BF03194587
   LAMB GD, 1983, J PHYSIOL-LONDON, V338, P551, DOI 10.1113/jphysiol.1983.sp014689
   Lederman S.J., 1999, HAPTICS E ELECT J HA, V1, P1
   LEDERMAN SJ, 1987, J EXP PSYCHOL LEARN, V13, P606, DOI 10.1037/0278-7393.13.4.606
   LEDERMAN SJ, 1983, CAN J PSYCHOL, V37, P498, DOI 10.1037/h0080750
   Libouton X, 2012, BEHAV BRAIN RES, V229, P273, DOI 10.1016/j.bbr.2012.01.018
   Manfredi LR, 2014, J NEUROPHYSIOL, V111, P1792, DOI 10.1152/jn.00680.2013
   Meftah E, 2000, EXP BRAIN RES, V132, P351, DOI 10.1007/s002210000348
   Meyer DJ, 2013, 2013 WORLD HAPTICS CONFERENCE (WHC), P43, DOI 10.1109/WHC.2013.6548382
   Moscatelli A, 2015, SCI REP-UK, V5, DOI 10.1038/srep14584
   Murray DJ, 1999, PERCEPT PSYCHOPHYS, V61, P1681, DOI 10.3758/BF03213127
   Nara T, 2001, IEEE COMPUT GRAPH, V21, P56, DOI 10.1109/38.963461
   Ono A., 1969, TOHOKU PSYCHOL FOLIA, V28, P29
   PLAMONDON R, 1995, BIOL CYBERN, V72, P309, DOI 10.1007/BF00202786
   Rolfe J, 2009, ECOL ECON, V68, P1140, DOI 10.1016/j.ecolecon.2008.08.007
   Samur E., 2012, PERFORMANCE METRICS, DOI DOI 10.1007/978-1-4471-4225-6
   SOECHTING JF, 1981, J NEUROSCI, V1, P710, DOI 10.1523/JNEUROSCI.01-07-00710.1981
   Tiest W. M. Bergmann, 2011, 2011 IEEE World Haptics Conference (WHC 2011), P593, DOI 10.1109/WHC.2011.5945552
   Tiest WMB, 2006, ACTA PSYCHOL, V121, P1, DOI 10.1016/j.actpsy.2005.04.005
   UNO Y, 1989, BIOL CYBERN, V61, P89, DOI 10.1007/BF00204593
   WAPNER S, 1967, AM J PSYCHOL, V80, P608, DOI 10.2307/1421193
   WATANABE T, 1995, IEEE INT CONF ROBOT, P1134, DOI 10.1109/ROBOT.1995.525433
   Weber AI, 2013, P NATL ACAD SCI USA, V110, P17107, DOI 10.1073/pnas.1305509110
   Wexler M., 2011, 2011 IEEE World Haptics Conference (WHC 2011), P605, DOI 10.1109/WHC.2011.5945554
   Wiertlewski M, 2010, ACTUATOR, P520
   Wiertlewski M, 2014, LECT NOTES COMPUT SC, V8619, P241, DOI 10.1007/978-3-662-44196-1_30
   Wiertlewski M, 2011, IEEE T ROBOT, V27, P461, DOI 10.1109/TRO.2011.2132830
   Winfield L, 2007, WORLD HAPTICS 2007: SECOND JOINT EUROHAPTICS CONFERENCE AND SYMPOSIUM ON HAPTIC INTERFACES FOR VIRTUAL ENVIRONMENT AND TELEOPERATOR SYSTEMS, PROCEEDINGS, P421
   Yeung N, 2012, PHILOS T R SOC B, V367, P1310, DOI 10.1098/rstb.2011.0416
   Yoshioka T, 2011, J NEUROSCI, V31, P17603, DOI 10.1523/JNEUROSCI.3907-11.2011
   Ziat M, 2010, EXP BRAIN RES, V206, P299, DOI 10.1007/s00221-010-2407-z
NR 60
TC 15
Z9 16
U1 2
U2 13
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD APR
PY 2018
VL 15
IS 2
AR 10
DI 10.1145/3152764
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA GH5YV
UT WOS:000433515400003
DA 2024-07-18
ER

PT J
AU Moffat, D
   Reiss, JD
AF Moffat, David
   Reiss, Joshua D.
TI Perceptual Evaluation of Synthesized Sound Effects
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Sound synthesis; evaluation; procedural audio; perception; sound effects
AB Sound synthesis is the process of generating artificial sounds through some form of simulation or modelling. This article aims to identify which sound synthesis methods achieve the goal of producing a believable audio sample that may replace a recorded sound sample. A perceptual evaluation experiment of five different sound synthesis techniques was undertaken. Additive synthesis, statistical modelling synthesis with two different feature sets, physically inspired synthesis, concatenative synthesis, and sinusoidal modelling synthesis were all compared. Evaluation using eight different sound class stimuli and 66 different samples was undertaken. The additive synthesizer is the only synthesis method not considered significantly different from the reference sample across all sounds classes. The results demonstrate that sound synthesis can be considered as realistic as a recorded sample and makes recommendations for use of synthesis methods, given different sound class contexts.
C1 [Moffat, David; Reiss, Joshua D.] Queen Mary Univ London, Sch Elect Engn & Comp Sci, Ctr Digital Mus, 10 Godward Sq, London E1 4FZ, England.
C3 University of London; Queen Mary University London
RP Moffat, D (corresponding author), Queen Mary Univ London, Sch Elect Engn & Comp Sci, Ctr Digital Mus, 10 Godward Sq, London E1 4FZ, England.
EM d.j.moffat@qmul.ac.uk; joshua.reiss@qmul.ac.uk
RI Moffat, David/AAG-4978-2020
OI Moffat, David/0000-0003-4885-7276
FU EPSRC [EP/M506394/1]; EPSRC [EP/L019981/1, EP/R005435/1] Funding Source:
   UKRI
FX This work was supported EPSRC grant number EP/M506394/1.
CR Amatriain X, 2002, DAFX - DIGITAL AUDIO EFFECTS, P373
   [Anonymous], THESIS
   [Anonymous], P 19 INT C DIG AUD E
   [Anonymous], P INT COMP MUS C
   [Anonymous], P 12 INT AUD MOSTL C
   [Anonymous], P SIGCHI C HUM FACT
   [Anonymous], P INT C SOUND MUS CO
   [Anonymous], P 35 INT C AUD ENG S
   [Anonymous], P 29 INT C AUD ENG S
   [Anonymous], P AUDIO ENG SOC CONV
   [Anonymous], 13871 ITUR BS
   [Anonymous], P INT COMP MUS C ICM
   [Anonymous], P 115 AUD ENG SOC CO
   [Anonymous], P C EN INT
   [Anonymous], P 20 INT C DIG AUD E
   [Anonymous], P C SOUND MUS COMP 2
   [Anonymous], P 61 INT C AUD ENG S
   [Anonymous], P 1 WORKSH SON INT V
   [Anonymous], 15343 ITUR BS
   [Anonymous], CREATING REAL UNPUB
   [Anonymous], P 2 WEB AUD C
   [Anonymous], P INT COMP MUS C ICM
   [Anonymous], APPL SCI
   [Anonymous], P AUDIO ENG SOC CONV
   [Anonymous], P INT C SOUND VIBR L
   Aramaki Mitsuko, 2012, Speech, Sound and Music Processing: Embracing Research in India. 8th International Symposium, CMMR 2011 20th International Symposium, FRSM 2011. Revised Selected Papers, P172, DOI 10.1007/978-3-642-31980-8_13
   BALLAS JA, 1993, J EXP PSYCHOL HUMAN, V19, P250, DOI 10.1037/0096-1523.19.2.250
   Bilbao S., 2009, NUMERICAL SOUND SYNT
   Bilbao S, 2013, J ACOUST SOC AM, V134, P3860, DOI 10.1121/1.4822479
   Bogdanov D., 2013, P 14 INT SOC MUS INF, P493, DOI DOI 10.1145/2502081.2502229
   Bonebright T.L., 2005, ACM Trans. Appl. Percept, V2, P505, DOI [10.1145/1101530.1101550, DOI 10.1145/1101530.1101550]
   Böttcher N, 2013, INT J COMPUT GAMES T, V2013, DOI 10.1155/2013/371374
   Caramiaux B, 2014, ACM T APPL PERCEPT, V11, DOI 10.1145/2536811
   Cook P.R., 2007, Real sound synthesis for interactive applications
   Farnell A, 2010, DESIGNING SOUND, P1
   Fröjd M, 2009, J AUDIO ENG SOC, V57, P29
   Gabrielli L., 2011, P 131 AUD ENG SOC CO
   Heise S., 2009, Proceedings of the Audio Engineering Society Convention, V127
   Hoffman M. D., 2006, P 7 INT C MUSIC INFO, P361
   JAFFE DA, 1995, COMPUT MUSIC J, V19, P76, DOI 10.2307/3681301
   Järveläinen H, 2002, J NEW MUSIC RES, V31, P311, DOI 10.1076/jnmr.31.4.311.14167
   Lakatos S, 1997, PERCEPT PSYCHOPHYS, V59, P1180, DOI 10.3758/BF03214206
   McDermott Josh H., 2009, 2009 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA), P297, DOI 10.1109/ASPAA.2009.5346467
   McDermott JH, 2011, NEURON, V71, P926, DOI 10.1016/j.neuron.2011.06.032
   Merer A, 2013, ACM T APPL PERCEPT, V10, DOI 10.1145/2422105.2422106
   Merer A, 2011, LECT NOTES COMPUT SC, V6684, P176, DOI 10.1007/978-3-642-23126-1_12
   Miner N. E., 2005, ACM Trans. Appl. Percept., V2, P521, DOI [10.1145/1101530.1101552, DOI 10.1145/1101530.1101552]
   Moffat D, 2015, INT CONF DIGIT AUDIO, P277
   Nordahl R, 2010, P IEEE VIRT REAL ANN, P147, DOI 10.1109/VR.2010.5444796
   O'Leary S, 2014, EUR SIGNAL PR CONF, P939
   Peltola L, 2007, IEEE T AUDIO SPEECH, V15, P1021, DOI 10.1109/TASL.2006.885924
   Puronas V, 2014, NEW SOUNDTRACK, V4, P181, DOI 10.3366/sound.2014.0062
   Rocchesso D, 2003, IEEE MULTIMEDIA, V10, P42, DOI 10.1109/MMUL.2003.1195160
   Rocchesso D., 2003, The Sounding Object
   Scavone G., 2001, P INT S MUS AC
   Schwarz D., 2011, P 14 INT C DIGITAL A, P221
   SERRA X, 1990, COMPUT MUSIC J, V14, P12, DOI 10.2307/3680788
   Thiede T, 2000, J AUDIO ENG SOC, V48, P3
   Tolonen T., 1998, EVALUATION MODERN SO
   Verron C, 2010, IEEE T AUDIO SPEECH, V18, P1550, DOI 10.1109/TASL.2009.2037402
NR 60
TC 10
Z9 11
U1 2
U2 8
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD APR
PY 2018
VL 15
IS 2
AR 13
DI 10.1145/3165287
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GH5YV
UT WOS:000433515400006
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Katsunuma, T
   Hirai, K
   Horiuchi, T
AF Katsunuma, Takafumi
   Hirai, Keita
   Horiuchi, Takahiko
TI Fabric Appearance Control System for Example-Based Interactive Texture
   and Color Design
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Texture extraction; color transfer; appearance control; interactive
   system
ID PERCEPTION
AB Texture and color are important factors of fabric appearance. A system that could intuitively manipulate and design fabric texture and color would be a very powerful tool. This article presents an interactive fabric appearance design system that modulates the texture patterns of input fabric example images and transfers the color patterns from other input images onto them. For this purpose, we propose a method to synthesize a natural texture image based on our findings from subjective experiments: (1) intensity and its deviation of two input images are significantly related to the realistic appearance of synthesized textures and (2) the spatial-frequency and edge intensity of two different input images significantly influence the natural appearance of synthesized texture perception. In our procedure, first, the texture pattern of an input fabric image is modulated in terms of undulation, thickness, and roughness. Next, we transfer the color pattern of an original color image onto the modulated texture pattern in the YIQ color space. To perform this color transfer, we use the IQ component of the color image. To reduce the unnatural appearance of the output color-transfer image, we remove the high-frequency components of the original color image. In addition, the Y component of the color-transfer image is obtained by adding the deviation of the texture pattern Y component to the texture pattern of the color image. These algorithms for reducing unnaturalness and synthesizing images were developed based on our findings from several subjective experiments on natural appearance. Finally, we implemented our algorithm on a smart device. Our system allows us to interactively design the texture and color of fabric by using images.
C1 [Katsunuma, Takafumi; Hirai, Keita; Horiuchi, Takahiko] Chiba Univ, Grad Sch Adv Integrat Sci, Inage Ku, 1-33,Yayoi Cho, Chiba 2638522, Japan.
C3 Chiba University
RP Katsunuma, T (corresponding author), Chiba Univ, Grad Sch Adv Integrat Sci, Inage Ku, 1-33,Yayoi Cho, Chiba 2638522, Japan.
EM aana2749@chiba-u.jp; hirai@faculty.chiba-u.jp;
   horiuchi@faculty.chiba-u.jp
RI Horiuchi, Takahiko/ABH-1481-2021
OI Horiuchi, Takahiko/0000-0002-1770-1087
FU JSPS KAKENHI [JP15H05926]; Grants-in-Aid for Scientific Research
   [15H05926] Funding Source: KAKEN
FX This work was supported by JSPS KAKENHI Grant Number JP15H05926
   (Grant-in-Aid for Scientific Research on Innovative Areas "Innovative
   SHITSUKSAN Science and Technology"). We are very grateful to our
   laboratory members for their valuable assistance with our experiments.
   We would like to thank Editage (www.editage.jp) for English language
   editing.
CR Adelson EH, 2001, PROC SPIE, V4299, P1, DOI 10.1117/12.429489
   Anderson BL, 2011, CURR BIOL, V21, pR978, DOI 10.1016/j.cub.2011.11.022
   [Anonymous], 2009, State of the Art in Example-Based Texture Synthesis R
   Boyadzhiev I, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2809796
   Boyadzhiev Ivaylo, 2017, ACM T APPL PERCEPT, V14
   Diamanti O, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766906
   Fleming RW, 2014, VISION RES, V94, P62, DOI 10.1016/j.visres.2013.11.004
   Gastal ESL, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964964
   Giesel M, 2013, J VISION, V13, DOI 10.1167/13.14.7
   Katsunuma Takafumi, 2016, ELECT IMAGING MEASUR, P1
   Kim MH, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531333
   Kuang JT, 2007, J VIS COMMUN IMAGE R, V18, P406, DOI 10.1016/j.jvcir.2007.06.003
   Lan YX, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461989
   Lin S, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461988
   Liu C, 2010, PROC CVPR IEEE, P239, DOI [10.1109/CVPR.2010.5540207, 10.1109/ICCET.2010.5485248]
   Maloney LT, 2010, J VISION, V10, DOI 10.1167/10.9.19
   Matusik W, 2005, ACM T GRAPHIC, V24, P787, DOI 10.1145/1073204.1073262
   Moroney N., 2002, P COL IM C, P23
   Ngan Addy, 2006, P 17 EUROGRAPHICS C, P31
   O'Donovan P, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964958
   Ochiai Y, 2014, LECT NOTES COMPUT SC, V8618, P409, DOI 10.1007/978-3-662-44193-0_51
   Olkkonen M, 2008, J VISION, V8, DOI 10.1167/8.5.13
   Panetta J, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766937
   RAVANDI SAH, 1995, TEXT RES J, V65, P676
   Reinhard E, 2001, IEEE COMPUT GRAPH, V21, P34, DOI 10.1109/38.946629
   Reinhard E, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366220
   Sawayama M, 2015, VISION RES, V109, P209, DOI 10.1016/j.visres.2014.11.017
   Serrano A, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980242
   Wang X, 2011, IEEE T INSTRUM MEAS, V60, P44, DOI 10.1109/TIM.2010.2069850
   Weinmann M, 2014, LECT NOTES COMPUT SC, V8691, P156, DOI 10.1007/978-3-319-10578-9_11
   Witzel C, 2011, I-PERCEPTION, V2, P13, DOI 10.1068/i0396
   Xiao B, 2016, J VISION, V16, DOI 10.1167/16.3.34
NR 32
TC 2
Z9 2
U1 0
U2 8
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD JUL
PY 2017
VL 14
IS 3
AR 16
DI 10.1145/3054953
PG 20
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FB9AL
UT WOS:000406431900003
DA 2024-07-18
ER

PT J
AU Wilcox, LM
   Allison, RS
   Helliker, J
   Dunk, B
   Anthony, RC
AF Wilcox, Laurie M.
   Allison, Robert S.
   Helliker, John
   Dunk, Bert
   Anthony, Roy C.
TI Evidence that Viewers Prefer Higher Frame-Rate Film
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article; Proceedings Paper
CT ACM SIGGRAPH Symposium on Applied Perception (SAP)
CY SEP 13-14, 2015
CL Tubingen, GERMANY
SP ACM SIGGRAPH, ACM, Disney Res, Max Planck Inst Biol Cybernet
DE Human Factors; Perception; high frame rate; preference; cinema;
   stereoscopic 3D
ID VISUAL-ACUITY; IMAGE QUALITY; MOTION; VISIBILITY
AB High frame-rate (HFR) movie-making refers to the capture and projection of movies at frame rates several times higher than the traditional 24 frames per second. This higher frame rate theoretically improves the quality of motion portrayed in movies, and helps avoid motion blur, judder, and other undesirable artifacts. However, there is considerable debate in the cinema industry regarding the acceptance of HFR content given anecdotal reports of hyper-realistic imagery that reveals too much set and costume detail. Despite the potential theoretical advantages, there has been little empirical investigation of the impact of high frame-rate techniques on the viewer experience. In this study, we use stereoscopic 3D content, filmed and projected at multiple frame rates (24, 48, and 60 fps), with shutter angles ranging from 180. to 358., to evaluate viewer preferences. In a pairedcomparison paradigm, we assessed preferences along a set of five attributes (realism, motion smoothness, blur/ clarity, quality of depth, and overall preference). The resulting data show a clear preference for higher frame rates, particularly when contrasting 24 fps with 48 or 60 fps. We found little impact of shutter angle on viewers' choices, with the exception of one measure (motion smoothness) for one clip type. These data are the first empirical evidence of the advantages afforded by high frame-rate capture and presentation in a cinema context.
C1 [Wilcox, Laurie M.] York Univ, Dept Psychol, Toronto, ON M3J 1P3, Canada.
   [Allison, Robert S.] York Univ, Dept Elect Engn & Comp Sci, Toronto, ON M3J 1P3, Canada.
   [Helliker, John; Dunk, Bert] Sheridan Coll, Oakville, ON, Canada.
   [Anthony, Roy C.] Christie Digital, Res & Innovat, Kitchener, ON N2G 4Y7, Canada.
C3 York University - Canada; York University - Canada
RP Wilcox, LM (corresponding author), York Univ, Dept Psychol, 4700 Keele St, Toronto, ON M3J 1P3, Canada.
EM lwilcox@yorku.ca; allison@eecs.yorku.ca;
   john.helliker@sheridancollege.ca; albert.dunk@sheridancollege.ca;
   roy.anthony@christiedigital.com
OI Wilcox, Laurie/0000-0002-3594-6192; Allison, Robert/0000-0002-4485-2665
FU NSERC [CUI2I 437691-12]; Sheridan College; Christie Digital Systems
   Canada Inc.
FX This work was supported by the NSERC under grant CUI2I 437691-12 to York
   University and Sheridan College in partnership with Christie Digital
   Systems Canada Inc.
CR [Anonymous], HOLLYWOOD REPORTER
   Armat Thomas, 1897, Patent No, Patent No. 578185
   Banitalebi-Dehkordi A, 2014, I SYMP CONSUM ELECTR, P418
   Banks MS, 2012, SMPTE MOTION IMAG J, V121, P24, DOI 10.5594/j18173
   BEX PJ, 1995, VISION RES, V35, P2539, DOI 10.1016/0042-6989(95)00060-D
   BURR DC, 1986, VISION RES, V26, P643, DOI 10.1016/0042-6989(86)90012-X
   Claypool KT, 2007, MULTIMEDIA SYST, V13, P3, DOI 10.1007/s00530-007-0081-1
   DEBRUYN B, 1989, J OPT SOC AM A, V6, P323, DOI 10.1364/JOSAA.6.000323
   Engstrom EW, 1935, P IRE, V23, P295, DOI 10.1109/JRPROC.1935.227972
   Giardina Carolyn, 2012, CINEMACON 2012 PET
   GREEN PE, 1990, J MARKETING, V54, P3, DOI 10.2307/1251756
   Hoffman DM, 2011, J SOC INF DISPLAY, V19, P271, DOI 10.1889/JSID19.3.271
   Kuroki Y, 2007, J SOC INF DISPLAY, V15, P61, DOI 10.1889/1.2451560
   Kuroki Y, 2012, J SOC INF DISPLAY, V20, P566, DOI 10.1002/jsid.107
   Lindholm Julie M., 1993, ADA261562 DTIC
   LUCE RD, 1964, J MATH PSYCHOL, V1, P1, DOI 10.1016/0022-2496(64)90015-X
   MILLER JW, 1958, J OPT SOC AM, V48, P803, DOI 10.1364/JOSA.48.000803
   Paul Johnson Joohwan Kim David M. Hoffman Andres Vargas and Martin S. Banks, 2014, SID S, V45, P797
   Quesnel Denise, 2013, TECHNICAL REPORT
   SHEEDY JE, 1984, AM J OPTOM PHYS OPT, V61, P595
   Trumbull Douglas, 1985, Patent No, Patent No. 4560260
   Watson Andrew B., 2013, Motion Imaging Journal, V122, P18
   WATSON AB, 1986, J OPT SOC AM A, V3, P300, DOI 10.1364/JOSAA.3.000300
   Winterbottom MD, 2007, SID INT SYMP DIG TEC, V38, P334, DOI 10.1889/1.2785299
NR 24
TC 15
Z9 16
U1 2
U2 13
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD SEP
PY 2015
VL 12
IS 4
SI SI
AR 15
DI 10.1145/2810039
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA CR1ER
UT WOS:000361067300003
OA Green Published
DA 2024-07-18
ER

PT J
AU Williams, D
   Kirke, A
   Miranda, E
   Daly, I
   Hallowell, J
   Weaver, J
   Malik, A
   Roesch, E
   Hwang, F
   Nasuto, S
AF Williams, Duncan
   Kirke, Alexis
   Miranda, Eduardo
   Daly, Ian
   Hallowell, James
   Weaver, James
   Malik, Asad
   Roesch, Etienne
   Hwang, Faustina
   Nasuto, Slawomir
TI Investigating Perceived Emotional Correlates of Rhythmic Density in
   Algorithmic Music Composition
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Human Factors; Algorithmic composition; affect; music perception; rhythm
ID CORE AFFECT; SAD; INTELLIGENCE; EXPRESSION; INTENSITY; RESPONSES;
   MODELS; HAPPY
AB Affective algorithmic composition is a growing field that combines perceptually motivated affective computing strategies with novel music generation. This article presents work toward the development of one application. The long-term goal is to develop a responsive and adaptive system for inducing affect that is both controlled and validated by biophysical measures. Literature documenting perceptual responses to music identifies a variety of musical features and possible affective correlations, but perceptual evaluations of these musical features for the purposes of inclusion in a music generation system are not readily available. A discrete feature, rhythmic density (a function of note duration in each musical bar, regardless of tempo), was selected because it was shown to be well-correlated with affective responses in existing literature. A prototype system was then designed to produce controlled degrees of variation in rhythmic density via a transformative algorithm. A two-stage perceptual evaluation of a stimulus set created by this prototype was then undertaken. First, listener responses from a pairwise scaling experiment were analyzed via Multidimensional Scaling Analysis (MDS). The statistical best-fit solution was rotated such that stimuli with the largest range of variation were placed across the horizontal plane in two dimensions. In this orientation, stimuli with deliberate variation in rhythmic density appeared farther from the source material used to generate them than from stimuli generated by random permutation. Second, the same stimulus set was then evaluated according to the order suggested in the rotated two-dimensional solution in a verbal elicitation experiment. A Verbal Protocol Analysis (VPA) found that listener perception of the stimulus set varied in at least two commonly understood emotional descriptors, which might be considered affective correlates of rhythmic density. Thus, these results further corroborate previous studies wherein musical parameters are monitored for changes in emotional expression and that some similarly parameterized control of perceived emotional content in an affective algorithmic composition system can be achieved and provide a methodology for evaluating and including further possible musical features in such a system. Some suggestions regarding the test procedure and analysis techniques are also documented here.
C1 [Williams, Duncan; Kirke, Alexis; Miranda, Eduardo] Univ Plymouth, Interdisciplinary Ctr Comp Mus Res, Plymouth PL4 8AA, Devon, England.
   [Daly, Ian; Hallowell, James; Weaver, James; Malik, Asad; Roesch, Etienne; Hwang, Faustina; Nasuto, Slawomir] Unviers Reading, Brain Embodiments Lab, Reading RG6 6AH, Berks, England.
C3 University of Plymouth
RP Williams, D (corresponding author), Univ Plymouth, Interdisciplinary Ctr Comp Mus Res, Plymouth PL4 8AA, Devon, England.
EM duncan.williams@plymouth.ac.uk; alexis.kirke@plymouth.ac.uk;
   eduardo.miranda@plymouth.ac.uk; i.daly@reading.ac.uk;
   j.hallowell@reading.ac.uk; j.weaver@reading.ac.uk;
   a.malik@reading.ac.uk; e.roesch@reading.ac.uk; f.hwang@reading.ac.uk;
   s.nasuto@reading.ac.uk
RI Roesch, Etienne B/B-9164-2008; Miranda, Eduardo/KGL-5127-2024; Daly,
   Ian/Q-7322-2017
OI Miranda, Eduardo/0000-0002-8306-9585; Kirke, Alexis/0000-0001-8783-6182;
   Hwang, Faustina/0000-0002-3243-3869; Nasuto,
   Slawomir/0000-0001-9414-9049; Williams, Duncan/0000-0003-4793-8330;
   Roesch, Etienne/0000-0002-8913-4173; Daly, Ian/0000-0001-5489-0393;
   Malik, Asad/0000-0003-0981-6375
FU EPSRC [EP/J003077/1, EP/J002135/1]; EPSRC [EP/J003077/1, EP/J002135/1]
   Funding Source: UKRI
FX The authors gratefully acknowledge the support of EPSRC grants
   EP/J003077/1 and EP/J002135/1.
CR AMES C, 1989, LEONARDO, V22, P175, DOI 10.2307/1575226
   [Anonymous], 1999, AISB S MUSICAL CREAT
   [Anonymous], MEASURED INTERPRETAT
   Astill B., 1994, P AUSTR ASS RES ED C
   Aucouturier JJ, 2005, IEEE T MULTIMEDIA, V7, P1028, DOI 10.1109/TMM.2005.858380
   Bigand E, 2005, COGNITION EMOTION, V19, P1113, DOI 10.1080/02699930500204250
   Bigand E, 2005, ANN NY ACAD SCI, V1060, P429, DOI 10.1196/annals.1360.036
   Bolger D., 2004, THESIS U LIMERICK
   Bresson J., 2005, 10 BRAZ S COMP MUS B
   Collins N, 2009, CONTEMP MUSIC REV, V28, P103, DOI 10.1080/07494460802664064
   COPE D, 1989, INTERFACE-J NEW MUS, V18, P117, DOI 10.1080/09298218908570541
   COPE D, 1992, COMPUT MUSIC J, V16, P69, DOI 10.2307/3680717
   Cope D., 1996, EXPT MUSICAL INTELLI
   Eerola T, 2011, PSYCHOL MUSIC, V39, P18, DOI 10.1177/0305735610362821
   Gabrielsson A, 2001, MUSIC SCI, V5, P123, DOI 10.1177/10298649020050S105
   Gagnon L, 2003, COGNITION EMOTION, V17, P25, DOI 10.1080/02699930302279
   Hannon EE, 2004, J EXP PSYCHOL HUMAN, V30, P956, DOI 10.1037/0096-1523.30.5.956
   Hevner K, 1936, AM J PSYCHOL, V48, P246, DOI 10.2307/1415746
   Juslin P.N., 2010, HDB MUSIC EMOTION
   Juslin PN, 2004, J NEW MUSIC RES, V33, P217, DOI 10.1080/0929821042000317813
   Kirke A., 2011, P 2011 INT COMP MUS
   Kirke Alexis, 2013, P SOUND MUSIC COMPUT, DOI [10.5281/ZENODO.850216, DOI 10.5281/ZENODO.850216]
   KOHONEN T, 1989, IEE CONF PUBL, P1, DOI 10.1109/IJCNN.1989.118552
   Kratus J., 1993, PSYCHOL MUSIC, V21, P3, DOI [DOI 10.1177/030573569302100101, DOI 10.1177/0305735603031001325]
   KRUSKAL JB, 1964, PSYCHOMETRIKA, V29, P1, DOI 10.1007/BF02289565
   Ladinig O, 2012, PSYCHOL AESTHET CREA, V6, P146, DOI 10.1037/a0024671
   Lamont A, 2011, MUSIC SCI, V15, P139, DOI 10.1177/1029864911403366
   LERDAHL F, 1983, MUSIC PERCEPT, V1, P229
   Livingstone S. R., 2006, MUSIC HUMAN COMMUNIC
   Marin MM, 2010, EDUC COMPET GLOB WOR, P1
   Mattek A., 2011, P 26 ANN C SOC EL MU
   Miranda Eduardo, 2001, Composing music with computers, DOI DOI 10.4324/9780080502403
   Nierhaus G., 2009, Algorithmic Composition: Paradigms of Automated Music Generation
   Oliveira AP, 2010, KNOWL-BASED SYST, V23, P901, DOI 10.1016/j.knosys.2010.06.006
   Raphael C., 2001, P ISMIR 2001
   Rowe Robert., 1992, INTERACTIVE MUSIC SY
   Russell JA, 1999, J PERS SOC PSYCHOL, V76, P805, DOI 10.1037/0022-3514.76.5.805
   RUSSELL JA, 1980, J PERS SOC PSYCHOL, V39, P1161, DOI 10.1037/h0077714
   Russell JA, 2003, PSYCHOL REV, V110, P145, DOI 10.1037/0033-295X.110.1.145
   SCHACHTER S, 1962, PSYCHOL REV, V69, P379, DOI 10.1037/h0046234
   Scherer KR, 2004, J NEW MUSIC RES, V33, P239, DOI 10.1080/0929821042000317822
   Schmidt LA, 2001, COGNITION EMOTION, V15, P487, DOI 10.1080/0269993004200187
   Schubert E, 2006, ACTA ACUST UNITED AC, V92, P820
   Sugimoto T, 2008, KNOWL-BASED SYST, V21, P200, DOI 10.1016/j.knosys.2007.11.010
   Taruffi L, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0110490
   Västfjäll D, 2001, MUSIC SCI, V5, P173
   VieIllard S, 2008, COGNITION EMOTION, V22, P720, DOI 10.1080/02699930701503567
   Vuoskoski JK, 2012, PSYCHOL AESTHET CREA, V6, P204, DOI 10.1037/a0026937
   Vuoskoski JK, 2012, MUSIC PERCEPT, V29, P311, DOI 10.1525/MP.2012.29.3.311
   Vuoskoski JK, 2011, MUSIC SCI, V15, P159, DOI 10.1177/1029864911403367
   Wallis I., 2011, P SOUND MUS COMP C S
   Wassermann KC, 2003, IEEE MULTIMEDIA, V10, P82, DOI 10.1109/MMUL.2003.1237553
   Wessel D. L., 1979, Computer Music Journal, V3, P45, DOI 10.2307/3680283
   Whiteley N, 2007, INT CONF ACOUST SPEE, P1321
   Williams D, 2013, P 3 INT C MUS EM ICM
   Wingstedt J, 2005, P 2005 C NEW INT MUS, P232
   Wu TL, 2008, LECT NOTES COMPUT SC, V4903, P487
   Zentner M, 2008, EMOTION, V8, P494, DOI 10.1037/1528-3542.8.4.494
NR 58
TC 7
Z9 7
U1 0
U2 11
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD JUL
PY 2015
VL 12
IS 3
AR 8
DI 10.1145/2749466
PG 21
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI); Arts &amp; Humanities Citation Index (A&amp;HCI)
SC Computer Science
GA CP6OI
UT WOS:000360006600002
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Renner, RS
   Steindecker, E
   Müller, M
   Velichkovsky, BM
   Stelzer, R
   Pannasch, S
   Helmert, JR
AF Renner, Rebekka S.
   Steindecker, Erik
   Mueller, Mathias
   Velichkovsky, Boris M.
   Stelzer, Ralph
   Pannasch, Sebastian
   Helmert, Jens R.
TI The Influence of the Stereo Base on Blind and Sighted Reaches in a
   Virtual Environment
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Human Factors; Depth perception; distance estimation; stereoscopic
   displays; virtual environments
ID BINOCULAR DISPARITY; VISUAL FEEDBACK; FAMILIAR SIZE; OBJECTS SIZES;
   DISTANCE; REALITY; PERCEPTION; DEPTH; MOTION; PERSPECTIVE
AB In virtual environments, perceived distances are frequently reported to be shorter than intended. One important parameter for spatial perception in a stereoscopic virtual environment is the stereo base-that is, the distance between the two viewing cameras. We systematically varied the stereo base relative to the interpupillary distance (IPD) and examined influences on distance and size perception. Furthermore, we tested whether an individual adjustment of the stereo base through an alignment task would reduce the errors in distance estimation. Participants performed reaching movements toward a virtual tennis ball either with closed eyes (blind reaches) or open eyes (sighted reaches). Using the participants' individual IPD, the stereo base was set to (a) the IPD, (b) proportionally smaller, (c) proportionally larger, or (d) adjusted according to the individual performance in an alignment task that was conducted beforehand. Overall, consistent with previous research, distances were underestimated. As expected, with a smaller stereo base, the virtual object was perceived as being farther away and bigger, in contrast to a larger stereo base, where the virtual object was perceived to be nearer and smaller. However, the manipulation of the stereo base influenced blind reaching estimates to a smaller extent than expected, which might be due to a combination of binocular disparity and pictorial depth cues. In sighted reaching, when visual feedback was available, presumably the use of disparity matching led to a larger effect of the stereo base. The use of an individually adjusted stereo base diminished the average underestimation but did not reduce interindividual variance. Interindividual differences were task specific and could not be explained through differences in stereo acuity or fixation disparity.
C1 [Renner, Rebekka S.; Velichkovsky, Boris M.; Pannasch, Sebastian; Helmert, Jens R.] Tech Univ Dresden, Dept Psychol 3, D-01062 Dresden, Germany.
   [Steindecker, Erik; Stelzer, Ralph] Tech Univ Dresden, Chair Engn Design & CAD, D-01062 Dresden, Germany.
   [Mueller, Mathias] Tech Univ Dresden, Chair Media Design, D-01062 Dresden, Germany.
   [Velichkovsky, Boris M.] Kurchatov Res Ctr, Inst Cognit Studies, Moscow, Russia.
C3 Technische Universitat Dresden; Technische Universitat Dresden;
   Technische Universitat Dresden; National Research Centre - Kurchatov
   Institute
RP Renner, RS (corresponding author), Tech Univ Dresden, Fac Sci, Chair Engn Psychol & Appl Cognit Res, D-01062 Dresden, Germany.
EM rebekka.renner@tu-dresden.de; erik.steindecker@tu-dresden.de;
   mathias.mueller@tu-dresden.de; boris.velichkovsky@tu-dresden.de;
   ralph.stelzer@tu-dresden.de; sebastian.pannasch@tu-dresden.de;
   jens.helmert@tu-dresden.de
OI Mueller, Matthias/0000-0003-2545-5258; Muller,
   Mathias/0000-0001-8963-8403; Helmert, Jens R./0000-0003-1000-1915
FU European Social Fund; Free State of Saxony, Germany, in the framework of
   the junior research group CogITo
FX R. S. Renner, M. Muller, and J. R. Helmert were co-financed by the
   European Social Fund and the Free State of Saxony, Germany, in the
   framework of the junior research group CogITo.
CR [Anonymous], TAFELN PRUFUNG FARBE
   Backlund P, 2007, IEEE INT CONF INF VI, P899
   Berkinblit MB, 1995, EXP BRAIN RES, V107, P326
   Bingham GP, 2001, J EXP PSYCHOL HUMAN, V27, P1314, DOI 10.1037//0096-1523.27.6.1314
   Bohil CJ, 2011, NAT REV NEUROSCI, V12, P752, DOI 10.1038/nrn3122
   Bruder G., 2012, P ACM S APPL PERC SA, P111
   Bruder G, 2013, LECT NOTES COMPUT SC, V8117, P278
   Carrozzino M, 2010, J CULT HERIT, V11, P452, DOI 10.1016/j.culher.2010.04.001
   CAVANAGH P, 1987, COMPUT VISION GRAPH, V37, P171, DOI 10.1016/S0734-189X(87)80001-4
   Chan JCP, 2011, IEEE T LEARN TECHNOL, V4, P187, DOI 10.1109/TLT.2010.27
   CHAN LW, 2010, P 28 INT C HUM FACT, P2625, DOI DOI 10.1145/1753326.1753725
   Creem-Regehr SH, 2010, WIRES COGN SCI, V1, P800, DOI 10.1002/wcs.82
   CRUZNEIRA C, 1992, COMMUN ACM, V35, P64, DOI 10.1145/129888.129892
   Cutting JE, 1995, PERCEPTION SPACE MOT, P69, DOI [DOI 10.1016/B978-012240530-3/50005-5, 10.1016/B978-012240530-3/50005-5]
   Dodgson NA, 2004, PROC SPIE, V5291, P36, DOI 10.1117/12.529999
   ELLIOTT D, 1985, Q J EXP PSYCHOL-A, V37, P407, DOI 10.1080/14640748508400942
   ELLIOTT D, 1987, Q J EXP PSYCHOL-A, V39, P541, DOI 10.1080/14640748708401802
   GILLAM BJ, 1968, J EXP PSYCHOL, V78, P299, DOI 10.1037/h0026271
   HARTUNG B., 2001, J VISION, V1, P256, DOI DOI 10.1016/BRAINRES.2005.01.107
   Heath M, 2004, MOTOR CONTROL, V8, P76, DOI 10.1123/mcj.8.1.76
   Hibbard PB, 2003, EXP BRAIN RES, V148, P196, DOI 10.1007/s00221-002-1295-2
   JOHNSTON EB, 1993, VISION RES, V33, P813, DOI 10.1016/0042-6989(93)90200-G
   Jones G, 2001, P SOC PHOTO-OPT INS, V4297, P42, DOI 10.1117/12.430855
   Jorke H, 2006, PROC SPIE, V6055, DOI 10.1117/12.650348
   Kakusho K., 2000, Proceedings IEEE Virtual Reality 2000 (Cat. No.00CB37048), P99, DOI 10.1109/VR.2000.840487
   Keefe BD, 2011, NEUROPSYCHOLOGIA, V49, P1246, DOI 10.1016/j.neuropsychologia.2011.02.047
   KEELE SW, 1968, J EXP PSYCHOL, V77, P155, DOI 10.1037/h0025754
   Kellner F, 2012, IEEE T VIS COMPUT GR, V18, P589, DOI 10.1109/TVCG.2012.45
   Levine TR, 2002, HUM COMMUN RES, V28, P612, DOI 10.1093/hcr/28.4.612
   Li Bochao., 2014, P ACM S APPL PERCEPT, P91
   Loftin RB, 2004, IEEE COMPUT GRAPH, V24, P18, DOI 10.1109/MCG.2004.21
   LOOMIS JM, 1992, J EXP PSYCHOL HUMAN, V18, P906, DOI 10.1037/0096-1523.18.4.906
   Lubos P, 2014, 2014 IEEE SYMPOSIUM ON 3D USER INTERFACES (3DUI), P11, DOI 10.1109/3DUI.2014.6798834
   Marotta JJ, 2001, J COGNITIVE NEUROSCI, V13, P8, DOI 10.1162/089892901564135
   Masaoka K, 2006, J ELECTRON IMAGING, V15, DOI 10.1117/1.2181178
   Mohler BettyJ., 2006, P 3 S APPL PERCEPTIO, P9, DOI [10.1145/1140491.1140493, DOI 10.1145/1140491.1140493]
   Mon-Williams M, 1999, PERCEPTION, V28, P167, DOI 10.1068/p2737
   Mujber TS, 2004, J MATER PROCESS TECH, V155, P1834, DOI 10.1016/j.jmatprotec.2004.04.401
   Napieralski PE, 2011, ACM T APPL PERCEPT, V8, DOI 10.1145/2010325.2010328
   Nordstokke DW, 2010, PSICOLOGICA, V31, P401
   OLDFIELD RC, 1971, NEUROPSYCHOLOGIA, V9, P97, DOI 10.1016/0028-3932(71)90067-4
   OLEARY A, 1980, PERCEPT PSYCHOPHYS, V27, P131, DOI 10.3758/BF03204300
   Patterson RE, 2013, J SOC INF DISPLAY, V21, P142, DOI 10.1002/jsid.153
   Ponto K, 2013, IEEE T VIS COMPUT GR, V19, P691, DOI 10.1109/TVCG.2013.36
   Powers MB, 2008, J ANXIETY DISORD, V22, P561, DOI 10.1016/j.janxdis.2007.04.006
   Proffitt Dennis R., 2002, HDB PSYCHOL EXPT PSY, V4, P213
   Renner R.S., 2013, Proceedings of the ACM Symposium on Applied Perception, P130
   Renner RS, 2013, ACM COMPUT SURV, V46, DOI 10.1145/2543581.2543590
   Richardson AR, 2007, HUM FACTORS, V49, P507, DOI 10.1518/001872007X200139
   Richardson AR, 2005, APPL COGNITIVE PSYCH, V19, P1089, DOI 10.1002/acp.1140
   Riva G, 2005, CYBERPSYCHOL BEHAV, V8, P220, DOI 10.1089/cpb.2005.8.220
   Robinett W., 1992, Presence: Teleoper. Virtual Environ, V1, P45, DOI DOI 10.1162/pres.1992.1.1.45
   ROGERS BJ, 1989, Q J EXP PSYCHOL-A, V41, P697, DOI 10.1080/14640748908402390
   Seth A, 2011, VIRTUAL REAL-LONDON, V15, P5, DOI 10.1007/s10055-009-0153-y
   Singh Gurjot., 2010, Proceedings of the 7th Symposium on Applied Perception in Graphics and Visualization, P149, DOI DOI 10.1145/1836248.1836277
   Smith Michael D., 2012, P SPIE STER DISPL AP, V8288
   Sousa R, 2013, J VISION, V13, DOI 10.1167/13.2.2
   Sousa R, 2012, J VISION, V12, DOI 10.1167/12.10.6
   Sousa R, 2011, J VISION, V11, DOI 10.1167/11.9.10
   Stelzer R, 2014, PROCEEDINGS OF THE ASME INTERNATIONAL DESIGN ENGINEERING TECHNICAL CONFERENCES AND COMPUTERS AND INFORMATION IN ENGINEERING CONFERENCE, 2014, VOL 1B
   Stelzer R, 2011, PROCEEDINGS OF THE ASME WORLD CONFERENCE ON INNOVATIVE VIRTUAL REALITY - 2011, P51
   STEVENS KA, 1988, VISION RES, V28, P371, DOI 10.1016/0042-6989(88)90180-0
   Tresilian JR, 1999, J EXP PSYCHOL HUMAN, V25, P677, DOI 10.1037/0096-1523.25.3.677
   UTSUMI A, 1994, HUM FAC ERG SOC P, P250
   Waller D, 2008, J EXP PSYCHOL-APPL, V14, P61, DOI 10.1037/1076-898X.14.1.61
   Wartrell Z, 1999, COMP GRAPH, P351, DOI 10.1145/311535.311587
   Weichert F, 2013, SENSORS-BASEL, V13, P6380, DOI 10.3390/s130506380
   Westwood DA, 2001, CAN J EXP PSYCHOL, V55, P304, DOI 10.1037/h0087377
   Willemsen P, 2008, PRESENCE-TELEOP VIRT, V17, P91, DOI 10.1162/pres.17.1.91
   WOODS A, 1993, P SOC PHOTO-OPT INS, V1915, P36, DOI 10.1117/12.157041
   Yoshida S, 2001, VISUAL COMPUT, V17, P46, DOI 10.1007/s003710000095
   YOUNGS WM, 1976, VISION RES, V16, P79, DOI 10.1016/0042-6989(76)90079-1
NR 72
TC 10
Z9 10
U1 0
U2 16
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD APR
PY 2015
VL 12
IS 2
AR 7
DI 10.1145/2724716
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CH5BA
UT WOS:000354047900003
DA 2024-07-18
ER

PT J
AU Komogortsev, O
   Holland, C
   Jayarathna, S
   Karpov, A
AF Komogortsev, Oleg
   Holland, Corey
   Jayarathna, Sampath
   Karpov, Alex
TI 2D Linear Oculomotor Plant Mathematical Model: Verification and
   Biometric Applications
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Verification; Human visual system; biological system modeling;
   mathematical model; biometrics
ID SACCADIC EYE-MOVEMENTS; BINOCULAR COORDINATION; HOMEOMORPHIC MODEL;
   VERTICAL SACCADES; OBLIQUE SACCADES; BRAIN-STEM; STIMULATION
AB This article assesses the ability of a two-dimensional (2D) linear homeomorphic oculomotor plant mathematical model to simulate normal human saccades on a 2D plane. The proposed model is driven by a simplified pulse-step neuronal control signal and makes use of linear simplifications to account for the unique characteristics of the eye globe and the extraocular muscles responsible for horizontal and vertical eye movement. The linear nature of the model sacrifices some anatomical accuracy for computational speed and analytic tractability, and may be implemented as two one-dimensional models for parallel signal simulation. Practical applications of the model might include improved noise reduction and signal recovery facilities for eye tracking systems, additional metrics from which to determine user effort during usability testing, and enhanced security in biometric identification systems. The results indicate that the model is capable of produce oblique saccades with properties resembling those of normal human saccades and is capable of deriving muscle constants that are viable as biometric indicators. Therefore, we conclude that sacrifice in the anatomical accuracy of the model produces negligible effects on the accuracy of saccadic simulation on a 2D plane and may provide a usable model for applications in computer science, human-computer interaction, and related fields.
C1 [Komogortsev, Oleg; Holland, Corey; Karpov, Alex] SW Texas State Univ, Dept Comp Sci, San Marcos, TX 78666 USA.
   [Jayarathna, Sampath] Texas A&M Univ, Dept Comp Sci & Engn, College Stn, TX 77843 USA.
C3 Texas State University System; Texas State University San Marcos; Texas
   A&M University System; Texas A&M University College Station
RP Komogortsev, O (corresponding author), SW Texas State Univ, Dept Comp Sci, San Marcos, TX 78666 USA.
EM ok11@txstate.edu; ch1570@txstate.edu; uksjayarathna@tamu.edu;
   ak26@txstate.edu
RI Jayarathna, Sampath/AAM-6816-2021; Karpov, Alexey/E-6199-2013
OI Karpov, Alexey/0000-0001-8563-7243
FU NSF CAREER [CNS-1250718]; NSF GRFP [DGE-11444666]; NIST [60NANB10D213,
   60NANB12D234]; Division Of Computer and Network Systems; Direct For
   Computer & Info Scie & Enginr [1250718] Funding Source: National Science
   Foundation
FX Special gratitude is expressed to Dr. Christian Quaia for his comments
   and suggestions, and Katie J. Holland for her aid with technical
   illustrations. This work is supported in part by NSF CAREER Grant
   #CNS-1250718 and NSF GRFP Grant #DGE-11444666, and NIST Grants
   #60NANB10D213 and #60NANB12D234.
CR [Anonymous], EYELINK 1000 EYE TRA
   [Anonymous], 2007, HDB BIOMETRICS HDB B
   [Anonymous], 2007, Eye tracking methodology: Theory and practice, DOI DOI 10.1007/978-3-319-57883-5
   BAHILL A T, 1975, Mathematical Biosciences, V24, P191, DOI 10.1016/0025-5564(75)90075-9
   BAHILL AT, 1980, CRC CR REV BIOM ENG, V4, P311
   BAHILL AT, 1980, IEEE T BIO-MED ENG, V27, P631, DOI 10.1109/TBME.1980.326703
   BAHILL AT, 1977, ARCH OPHTHALMOL-CHIC, V95, P1258, DOI 10.1001/archopht.1977.04450070156016
   Carpenter R. H. S., 1977, MOVEMENTS EYES
   CIUFFREDA KJ, 1975, AM J OPTOM PHYS OPT, V52, P663
   CLARK M R, 1974, Mathematical Biosciences, V20, P191, DOI 10.1016/0025-5564(74)90001-7
   COLLEWIJN H, 1988, J PHYSIOL-LONDON, V404, P183, DOI 10.1113/jphysiol.1988.sp017285
   COLLEWIJN H, 1988, J PHYSIOL-LONDON, V404, P157, DOI 10.1113/jphysiol.1988.sp017284
   Collins C.C., 1975, BASIC MECH OCULAR MO, P145
   Eggert Thomas, 2007, Dev Ophthalmol, V40, P15
   Enderle J. D., 2010, MODELS HORIZONTAL EY
   ENDERLE JD, 1991, IEEE T BIO-MED ENG, V38, P1235, DOI 10.1109/10.137289
   FIORAVANTI F, 1995, VISION RES, V35, P3217, DOI 10.1016/0042-6989(95)00152-5
   Goossens HHLM, 1997, EXP BRAIN RES, V114, P542, DOI 10.1007/PL00005663
   GROSSMAN GE, 1988, BIOL CYBERN, V58, P13, DOI 10.1007/BF00363952
   Harwood MR, 2008, J NEUROSCI, V28, P7455, DOI 10.1523/JNEUROSCI.1817-08.2008
   Hotelling H, 1931, ANN MATH STAT, V2, P360, DOI 10.1214/aoms/1177732979
   HSU FK, 1976, COMPUT PROG BIOMED, V6, P108, DOI 10.1016/0010-468X(76)90032-5
   HUAMAN AG, 1993, INVEST OPHTH VIS SCI, V34, P2588
   Jayarathna U. K. S., 2011, TR20110731 TEX STAT
   KING WM, 1986, J NEUROPHYSIOL, V56, P769, DOI 10.1152/jn.1986.56.3.769
   Komogortsev O., 2008, BIBE, P1
   Komogortsev O. V., 2012, 2012 5th IAPR International Conference on Biometrics (ICB), P413, DOI 10.1109/ICB.2012.6199786
   Komogortsev Oleg V., 2009, Journal of Control Theory and Applications, V7, P14, DOI 10.1007/s11768-009-7218-z
   Komogortsev O.V., 2007, P 12 INT C HUM COMP, P1
   Komogortsev O. V., 2012, IEEE 5 INT C BIOM TH, P1
   Komogortsev O.V., 2009, Acm International Conference Proceeding Series, P140
   Komogortsev OV, 2010, IEEE T BIO-MED ENG, V57, P2635, DOI 10.1109/TBME.2010.2057429
   Komogortsev OV, 2008, PROCEEDINGS OF THE EYE TRACKING RESEARCH AND APPLICATIONS SYMPOSIUM (ETRA 2008), P229, DOI 10.1145/1344471.1344525
   Komogortsev Oleg Vladimirovich, 2007, Eye movement prediction by oculomotor plant modeling with kalman filter 2003-2007
   Kumar AN, 2005, ANN NY ACAD SCI, V1039, P404, DOI 10.1196/annals.1325.038
   Lagarias JC, 1998, SIAM J OPTIMIZ, V9, P112, DOI 10.1137/S1052623496303470
   Leigh RJ., 2006, NEUROLOGY EYE MOVEME, V4th
   MARTIN C, 1998, J MATH SYSTEMS ESTIM, V8, P1
   MASSEY FJ, 1951, J AM STAT ASSOC, V46, P68, DOI 10.2307/2280095
   Nichols MJ, 1996, J NEUROPHYSIOL, V76, P4080, DOI 10.1152/jn.1996.76.6.4080
   Nichols MJ, 1996, J NEUROPHYSIOL, V76, P582, DOI 10.1152/jn.1996.76.1.582
   PELISSON D, 1988, VISION RES, V28, P87, DOI 10.1016/S0042-6989(88)80009-9
   Quaia C, 1997, J NEUROPHYSIOL, V78, P1120, DOI 10.1152/jn.1997.78.2.1120
   Quaia Christian, 2003, Strabismus, V11, P17, DOI 10.1076/stra.11.1.17.14088
   Quaia C, 2010, PLOS ONE, V5, pA236, DOI 10.1371/journal.pone.0009595
   Rayner K, 1998, PSYCHOL BULL, V124, P372, DOI 10.1037/0033-2909.124.3.372
   ROBINSON DA, 1973, KYBERNETIK, V14, P71, DOI 10.1007/BF00288906
   RODGERS JL, 1988, AM STAT, V42, P59, DOI 10.2307/2685263
   SMIT AC, 1990, EXP BRAIN RES, V81, P325, DOI 10.1007/BF00228123
   Sparks DL, 2002, NAT REV NEUROSCI, V3, P952, DOI 10.1038/nrn986
   Tamir D., 2008, Proceedings of the 6th International Workshop on Software Quality, P47
   TWEED D, 1987, J NEUROPHYSIOL, V58, P832, DOI 10.1152/jn.1987.58.4.832
   VANOPSTAL AJ, 1987, VISION RES, V27, P731, DOI 10.1016/0042-6989(87)90071-X
   WESTHEIMER G, 1954, AMA ARCH OPHTHALMOL, V52, P710
   Wilkie D. R., 1970, MUSCLE
   YEE RD, 1985, INVEST OPHTH VIS SCI, V26, P938
NR 56
TC 13
Z9 16
U1 0
U2 5
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD OCT
PY 2013
VL 10
IS 4
AR 27
DI 10.1145/2536764.2536774
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 281YK
UT WOS:000329136700010
DA 2024-07-18
ER

PT J
AU Jerald, J
   Whitton, M
   Brooks, FP
AF Jerald, Jason
   Whitton, Mary
   Brooks, Frederick P., Jr.
TI Scene-Motion Thresholds During Head Yaw for Immersive Virtual
   Environments
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Experimentation; Human Factors; Measurement; Psychophysics; scene-motion
   thresholds; head motion; redirected walking; latency
ID LATENCY
AB In order to better understand how scene motion is perceived in immersive virtual environments, we measured scene-motion thresholds under different conditions across three experiments. Thresholds were measured during quasi-sinusoidal head yaw, single left-to-right or right-to-left head yaw, different phases of head yaw, slow to fast head yaw, scene motion relative to head yaw, and two scene-illumination levels. We found that across various conditions (1) thresholds are greater when the scene moves with head yaw (corresponding to gain < 1.0) than when the scene moves against head yaw (corresponding to gain > 1.0), and (2) thresholds increase as head motion increases.
C1 [Whitton, Mary; Brooks, Frederick P., Jr.] Univ N Carolina, Chapel Hill, NC USA.
C3 University of North Carolina; University of North Carolina Chapel Hill
EM jason@digitalartforms.com
RI Whitton, Mary/AAN-2378-2021
FU NIBIB NIH HHS [P41 EB002025] Funding Source: Medline
CR Adelstein B.D., 2005, Proceedings of the Human Factors and Ergonomics Society Annual Meeting, V49, P2221
   Adelstein B. D., 2003, P HUMAN FACTORS ERGO, V47, P2083, DOI [DOI 10.1177/1541931203047020, DOI 10.1177/154193120304702001]
   Adelstein B.D., 2006, Proceedings of the 50th Annual Meeting of Human Factors and Ergonomics Society, P2678
   Allison RS, 2001, P IEEE VIRT REAL ANN, P247, DOI 10.1109/VR.2001.913793
   [Anonymous], 2001, P EUR
   ANSTIS S, 1986, HDB PERCEPTION HUMAN, pCH16
   Bruder G., 2009, Proceedings of the 15th Joint virtual reality Eurographics conference on Virtual Environments, P145
   Coren S., 2004, Sensation and Perception
   DRAPER MH, 1998, THESIS U WASHINGTON
   Dyde RT, 2008, J VISION, V8, DOI 10.1167/8.14.5
   Ellis S.R., 2004, Proceedings of the 48th Annual Meeting of the Human Factors and Ergonomics Society, P2083
   Ellis SR, 1999, P IEEE VIRT REAL ANN, P218, DOI 10.1109/VR.1999.756954
   Engel David., 2008, P ACM S VIRTUAL REAL, P157, DOI [DOI 10.1145/1450579.1450612, 10.1145/1450579.1450612]
   Freeman TCA, 1998, VISION RES, V38, P941, DOI 10.1016/S0042-6989(97)00395-7
   GOGEL WC, 1990, PERCEPT PSYCHOPHYS, V48, P105, DOI 10.3758/BF03207077
   Goldstein E.B., 2007, Sensation and Perception
   GRAHAM CH, 1965, VISION VISUAL PERCEP, pCH20
   HOCHBERG Y, 1988, BIOMETRIKA, V75, P800, DOI 10.2307/2336325
   Holloway RL, 1997, PRESENCE-VIRTUAL AUG, V6, P413, DOI 10.1162/pres.1997.6.4.413
   Jaekl PM, 2005, EXP BRAIN RES, V163, P388, DOI 10.1007/s00221-004-2191-8
   JERALD J, 2009, TR10013 U N CAR CHAP
   Jerald J., 2009, Proceedings of IEEE Virtual Reality Workshop on Perceptual Illusions in Virtual Environments (PIVE), P4
   Jerald Jason, 2009, ACHI Int Conf Adv Comput Hum Interact, V2009, P69
   Jerald J, 2009, IEEE VIRTUAL REALITY 2009, PROCEEDINGS, P211, DOI 10.1109/VR.2009.4811025
   Jerald J, 2008, APGV 2008: PROCEEDINGS OF THE SYMPOSIUM ON APPLIED PERCEPTION IN GRAPHICS AND VISUALIZATION, P155
   Li L, 2009, ACM T APPL PERCEPT, V6, DOI 10.1145/1462055.1462060
   Loomis JM, 2003, VIRTUAL AND ADAPTIVE ENVIRONMENTS: APPLICATIONS, IMPLICATIONS, AND HUMAN PERFORMANCE ISSUES, P21
   Loose R, 2001, PERCEPTION, V30, P511, DOI 10.1068/p3097
   Mania Katerina., 2004, Proceedings of the 1st Symposium on Applied Perception in Graphics and Visualization, APGV '04, P39, DOI [10.1145/1012551.1012559, DOI 10.1145/1012551.1012559]
   Meehan M, 2003, P IEEE VIRT REAL ANN, P141, DOI 10.1109/VR.2003.1191132
   MILLER D, 2002, COMMUNICATION
   Peck TC, 2008, IEEE VIRTUAL REALITY 2008, PROCEEDINGS, P121
   SO RHY, 1995, AVIAT SPACE ENVIR MD, V66, P550
   Steinicke F, 2010, IEEE T VIS COMPUT GR, V16, P17, DOI 10.1109/TVCG.2009.62
   WALLACH H, 1965, PSYCHON SCI, V2, P217, DOI 10.3758/BF03343414
   WALLACH H, 1987, ANNU REV PSYCHOL, V38, P1, DOI 10.1146/annurev.ps.38.020187.000245
NR 36
TC 10
Z9 16
U1 0
U2 2
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD MAR
PY 2012
VL 9
IS 1
AR 4
DI 10.1145/2134203.2134207
PG 23
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 949YM
UT WOS:000304614400004
PM 25705137
OA Green Accepted, Green Published
DA 2024-07-18
ER

PT J
AU Pineo, D
   Ware, C
AF Pineo, Daniel
   Ware, Colin
TI Neural Modeling of Flow Rendering Effectiveness
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Theory; Contour perception; flow visualization; perceptual theory;
   visual cortex; visualization
ID FUNCTIONAL ARCHITECTURE; CONTOUR INTEGRATION; RECEPTIVE FIELDS;
   VISUAL-CORTEX
AB It has been previously proposed that understanding the mechanisms of contour perception can provide a theory for why some flow rendering methods allow for better judgments of advection pathways than others. In this article, we develop this theory through a numerical model of the primary visual cortex of the brain (Visual Area 1) where contour enhancement is understood to occur according to most neurological theories. We apply a two-stage model of contour perception to various visual representations of flow fields evaluated using the advection task of Laidlaw et al. In the first stage, contour enhancement is modeled based on Li's cortical model. In the second stage, a model of streamline tracing is proposed, designed to support the advection task. We examine the predictive power of the model by comparing its performance to that of human subjects on the advection task with four different visualizations. The results show the same overall pattern for humans and the model. In both cases, the best performance was obtained with an aligned streamline based method, which tied with a LIC-based method. Using a regular or jittered grid of arrows produced worse results. The model yields insights into the relative strengths of different flow visualization methods for the task of visualizing advection pathways.
C1 [Ware, Colin] Univ New Hampshire, Jere A Chase Ocean Engn Lab, Durham, NH 03824 USA.
C3 University System Of New Hampshire; University of New Hampshire
RP Pineo, D (corresponding author), Kingsbury Hall,33 Acad Way, Durham, NH 03824 USA.
EM dspineo@comcast.net; cware@ccom.unh.edu
CR Cabral B., 1993, P 20 ANN C COMP GRAP, P263, DOI DOI 10.1145/166117.166151
   DAUGMAN JG, 1985, J OPT SOC AM A, V2, P1160, DOI 10.1364/JOSAA.2.001160
   FIELD DJ, 1993, VISION RES, V33, P173, DOI 10.1016/0042-6989(93)90156-Q
   Fowler D., 1989, Proceedings. Graphics Interface'89, P249
   HUBEL DH, 1968, J PHYSIOL-LONDON, V195, P215, DOI 10.1113/jphysiol.1968.sp008455
   HUBEL DH, 1962, J PHYSIOL-LONDON, V160, P106, DOI 10.1113/jphysiol.1962.sp006837
   Laidlaw DH, 2001, IEEE VISUAL, P143, DOI 10.1109/VISUAL.2001.964505
   Li ZP, 1998, NEURAL COMPUT, V10, P903, DOI 10.1162/089976698300017557
   Lund N., 2001, ATTENTION PATTERN RE
   PINEO D, 2008, P 5 S APPL PERC GRAP, P171
   Turk G., 1996, Proceedings of the 23rd Annual Conference on Computer Graphics and Interactive Techniques, New Orleans, USA, August 1996, P453
   Ware C., 2020, INFORM VISUALIZATION
   Ware C, 2008, IEEE COMPUT GRAPH, V28, P6, DOI 10.1109/MCG.2008.39
   [No title captured]
NR 14
TC 8
Z9 9
U1 0
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD JUN
PY 2010
VL 7
IS 3
AR 20
DI 10.1145/1773965.1773971
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 618NF
UT WOS:000279361800006
DA 2024-07-18
ER

PT J
AU Kuang, JT
   Yamaguchi, H
   Liu, CM
   Johnson, GM
   Fairchild, MD
AF Kuang, Jiangtao
   Yamaguchi, Hiroshi
   Liu, Changmeng
   Johnson, Garrett M.
   Fairchild, Mark D.
TI Evaluating HDR Rendering Algorithms
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Algorithms; Experimentation; Human Factors; High dynamic-range imaging;
   tone-mapping algorithms evaluation; psychophysical experiments
AB A series of three experiments has been performed to test both the preference and accuracy of high dynamic-range (HDR) rendering algorithms in digital photography application. The goal was to develop a methodology for testing a wide variety of previously published tone-mapping algorithms for overall preference and rendering accuracy. A number of algorithms were chosen and evaluated first in a paired-comparison experiment for overall image preference. A rating-scale experiment was then designed for further investigation of individual image attributes that make up overall image preference. This was designed to identify the correlations between image attributes and the overall preference results obtained from the first experiments. In a third experiment, three real-world scenes with a diversity of dynamic range and spatial configuration were designed and captured to evaluate seven HDR rendering algorithms for both of their preference and accuracy performance by comparing the appearance of the physical scenes and the corresponding tone-mapped images directly. In this series of experiments, a modified Durand and Dorsey's bilateral filter technique consistently performed well for both preference and accuracy, suggesting that it is a good candidate for a common algorithm that could be included in future HDR algorithm testing evaluations. The results of these experiments provide insight for understanding of perceptual HDR image rendering and should aid in design strategies for spatial processing and tone mapping. The results indicate ways to improve and design more robust rendering algorithms for general HDR scenes in the future. Moreover, the purpose of this research was not simply to find out the "best" algorithms, but rather to find a more general psychophysical experiment based methodology to evaluate HDR image-rendering algorithms. This paper provides an overview of the many issues involved in an experimental framework that can be used for these evaluations.
C1 [Kuang, Jiangtao; Yamaguchi, Hiroshi; Liu, Changmeng; Johnson, Garrett M.; Fairchild, Mark D.] Rochester Inst Technol, Munsell Color Sci Lab, Rochester, NY 14623 USA.
C3 Rochester Institute of Technology
RP Kuang, JT (corresponding author), Rochester Inst Technol, Munsell Color Sci Lab, 54 Lomb Mem Dr, Rochester, NY 14623 USA.
EM jxk4031@cis.rit.edu
CR [Anonymous], 2000, Psychometric scaling, a toolkit for imaging systems development
   [Anonymous], 2002, PROC ACM T GRAPH SIG, DOI DOI 10.1145/566570.566574
   [Anonymous], P C ACM SIGGRAPH 200
   BARTLESON CJ, 1984, OPTICAL RAD MEASUREM, V5, P467
   BIGGS W, 2004, THESIS DALHOUSIE U
   BRAU GJ, 1999, IS T SPIE EL IM 99 C, V4, P96
   CALABRIA AJ, 2002, 10 COL IM C
   Day EA, 2004, COLOR RES APPL, V29, P365, DOI 10.1002/col.20046
   Debevec P., 1997, P ACM SIGGRAPH, P369, DOI DOI 10.1145/258734.258884
   DEVLIN K, 2002, CSTR02005 U BRIST DE
   Drago F., 2003, ACM SIGGRAPH C ABSTR
   Draper N., 1981, APPL REGRESSION ANAL, V2nd, P307
   Frankle J. A., 1983, US Patent, Patent No. [4 384 336, 4384336, US, 4384336]
   Funt B, 2000, EIGHTH COLOR IMAGING CONFERENCE: COLOR SCIENCE AND ENGINEERING SYSTEMS, TECHNOLOGIES, APPLICATIONS, P112
   FUNT B, 2002, P IS T SPIE EL IM C
   Ikeda E., 1998, U.S. Patent, Patent No. 5801773
   Johnson GM, 2003, ELEVENTH COLOR IMAGING CONFERENCE: COLOR SCIENCE AND ENGINEERING - SYSTEMS, TECHNOLOGIES, APPLICATIONS, P36
   JOHNSON GM, 2005, SPIE IS T EL IM C SA
   Jones LA, 1941, J OPT SOC AM, V31, P651, DOI 10.1364/JOSA.31.000651
   Keelan B., 2002, Handbook of Image Quality: Characterization and Prediction
   KUANG J, 2006, C ICIS
   KUANG J, 2005, IS T SID 13 COL IM C
   KUANG J, 2004, IS T SID 12 COL IM C
   Larson G. W., 1998, Journal of Graphics Tools, V3, P15, DOI 10.1080/10867651.1998.10487485
   Larson GW, 1997, IEEE T VIS COMPUT GR, V3, P291, DOI 10.1109/2945.646233
   Ledda P, 2004, IEEE SYS MAN CYBERN, P2777
   Ledda P, 2004, P 3 INT C COMP GRAPH
   McCann J, 1999, SEVENTH COLOR IMAGING CONFERENCE: COLOR SCIENCE, SYSTEMS AND APPLICATIONS, P1
   MCCANN J, 2004, J ELECTRON IMAGING, V13, P139
   MEYLAN L., 2005, IEEE Transactions on Image Processing
   MONTAG E, 2004, P SPIE IS T ELECT IM, P222
   MURPHY E, 2005, P IS T 2 IM ARCH C
   NAKA KI, 1966, J PHYSIOL-LONDON, V185, P536, DOI 10.1113/jphysiol.1966.sp008001
   Nayar SK, 2000, PROC CVPR IEEE, P472, DOI 10.1109/CVPR.2000.855857
   Nishisato S., 1994, Elements of Dual Scaling, Vfirst
   Reinhard E, 2002, ACM T GRAPHIC, V21, P267, DOI 10.1145/566570.566575
   REINHARD E, 2006, HIGH DYNAMIC RANGE I, P223
   REINHARD E, 2005, IEEE T VISUALIZATION, P12
   Robertson Mark A., 1999, IEEE INT C IM PROC
   Schlick Christophe., 1994, Quantization techniques for visualization of high dynamic range pictures, P7
   Thurstone LL, 1927, PSYCHOL REV, V34, P273, DOI 10.1037/h0070288
   Ward G, 2005, THIRTEENTH COLOR IMAGING CONFERENCE, FINAL PROGRAM AND PROCEEDINGS, P283
   Yoshida A, 2005, PROC SPIE, V5666, P192, DOI 10.1117/12.587782
NR 43
TC 88
Z9 102
U1 0
U2 13
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD JUL
PY 2007
VL 4
IS 2
AR 9
DI 10.1145/1265957.1265958
PG 27
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA V04IM
UT WOS:000207052100001
DA 2024-07-18
ER

PT J
AU Gagnon, H
   Stefanucci, J
   Creem-Regehr, S
   Bodenheimer, B
AF Gagnon, Holly
   Stefanucci, Jeanine
   Creem-Regehr, Sarah
   Bodenheimer, Bobby
TI Calibrated Passability Perception in Virtual Reality Transfers to
   Augmented Reality
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article; Proceedings Paper
CT ACM Symposium on Applied Perception (SAP)
CY AUG 05-06, 2023
CL Los Angeles, CA
SP Assoc Comp Machinery, ACM SIGGRAPH
DE Affordances; calibration; augmented reality; virtual reality; perception
ID BODY; AFFORDANCES; INFORMATION; LOCOMOTION; APERTURES; DISTANCE; SPACE
AB As applications for virtual reality (VR) and augmented reality (AR) technology increase, it will be important to understand how users perceive their action capabilities in virtual environments. Feedback about actions may help to calibrate perception for action opportunities (affordances) so that action judgments in VR and AR mirror actors' real abilities. Previous work indicates that walking through a virtual doorway while wielding an object can calibrate the perception of one's passability through feedback from collisions. In the current study, we aimed to replicate this calibration through feedback using a different paradigm in VR while also testing whether this calibration transfers to AR. Participants held a pole at 45 degrees and made passability judgments in AR (pretest phase). Then, they made passability judgments in VR and received feedback on those judgments by walking through a virtual doorway while holding the pole (calibration phase). Participants then returned to AR to make posttest passability judgments. Results indicate that feedback calibrated participants' judgments in VR. Moreover, this calibration transferred to the AR environment. In other words, after experiencing feedback in VR, passability judgments in VR and in AR became closer to an actor's actual ability, which could make training applications in these technologies more effective.
C1 [Gagnon, Holly; Stefanucci, Jeanine; Creem-Regehr, Sarah] Univ Utah, 380 S 1530 E,BEH S 502, Salt Lake City, UT 84112 USA.
   [Bodenheimer, Bobby] Vanderbilt Univ, PMB 351679, Nashville, TN 37235 USA.
C3 Utah System of Higher Education; University of Utah; Vanderbilt
   University
RP Gagnon, H (corresponding author), Univ Utah, 380 S 1530 E,BEH S 502, Salt Lake City, UT 84112 USA.
EM holly.gagnon@psych.utah.edu; jeanine.stefanucci@psych.utah.edu;
   sarah.creem@psych.utah.edu; bobby.bodenheimer@vanderbilt.edu
OI Bodenheimer, Bobby/0000-0002-0616-5936; Gagnon,
   Holly/0000-0003-2409-6759
FU National Science Foundation [1763254, 1763966]; Office of Naval Research
   [N0014-21-1-2583]
FX This work was supported by National Science Foundation Grants No.
   1763254 and No. 1763966 and Office of Naval Research Grant No.
   N0014-21-1-2583.
CR Altenhoff BlissM., 2012, P ACM S APPL PERCEPT, V1, P71, DOI DOI 10.1145/2338676.2338691
   Andersen SAW, 2016, J SURG EDUC, V73, P45, DOI 10.1016/j.jsurg.2015.09.010
   Berti A, 2000, J COGNITIVE NEUROSCI, V12, P415, DOI 10.1162/089892900562237
   Bertram J, 2015, COMPUT HUM BEHAV, V43, P284, DOI 10.1016/j.chb.2014.10.032
   Bhargava A, 2022, IEEE T VIS COMPUT GR, V28, P4198, DOI 10.1109/TVCG.2021.3083423
   Day B, 2019, J EXP PSYCHOL-APPL, V25, P1, DOI 10.1037/xap0000192
   Day BM, 2015, ACTA PSYCHOL, V158, P26, DOI 10.1016/j.actpsy.2015.03.010
   Ebrahimi Elham, 2015, 2015 IEEE Symposium on 3D User Interfaces (3DUI), P97, DOI 10.1109/3DUI.2015.7131732
   Ebrahimi E, 2018, ACM SYMPOSIUM ON APPLIED PERCEPTION (SAP 2018), DOI 10.1145/3225153.3225170
   Ebrahimi E, 2016, ACM T APPL PERCEPT, V13, DOI 10.1145/2947617
   Ebrahimi Elham., 2014, Proceedings of the ACM Symposium on Applied Perception - SAP '14, P103, DOI [10.1145/2628257.2628268, DOI 10.1145/2628257]
   Franchak JM, 2018, EXP BRAIN RES, V236, P1699, DOI 10.1007/s00221-018-5252-0
   Franchak JM, 2017, ATTEN PERCEPT PSYCHO, V79, P1816, DOI 10.3758/s13414-017-1339-0
   Franchak JM, 2014, ATTEN PERCEPT PSYCHO, V76, P460, DOI 10.3758/s13414-013-0578-y
   Franchak JM, 2010, VISION RES, V50, P2758, DOI 10.1016/j.visres.2010.09.019
   Gagnon HC, 2021, FRONT VIRTUAL REAL, V2, DOI 10.3389/frvir.2021.654656
   Gagnon HC, 2021, INT SYM MIX AUGMENT, P266, DOI 10.1109/ISMAR-Adjunct54149.2021.00061
   Gagnon HC, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR 2020), P922, DOI [10.1109/VR46266.2020.00117, 10.1109/VR46266.2020.00112]
   Ganier F, 2014, ERGONOMICS, V57, P828, DOI 10.1080/00140139.2014.899628
   Gibson J., 1979, The ecological approach to visual perception
   GIBSON JAMES J., 1966
   Gonzalez-Franco M, 2017, FRONT ROBOT AI, V4, DOI 10.3389/frobt.2017.00003
   Hackney AL, 2014, J MOTOR BEHAV, V46, P319, DOI 10.1080/00222895.2014.913002
   Hancock P. A., 2017, J ASTRO SOCIOLOGY, V2, P103
   Higuchi T, 2004, J EXP PSYCHOL-APPL, V10, P55, DOI 10.1037/1076-898X.10.1.55
   Higuchi T, 2006, EXP BRAIN RES, V175, P50, DOI 10.1007/s00221-006-0525-4
   Hyltander A, 2002, SURG ENDOSC, V16, P1324, DOI 10.1007/s00464-001-9184-5
   Jun E, 2015, ACM T APPL PERCEPT, V12, DOI 10.1145/2811266
   Kaplan AD, 2021, HUM FACTORS, V63, P706, DOI 10.1177/0018720820904229
   Kondo Y, 2021, ARCH GERONTOL GERIAT, V92, DOI 10.1016/j.archger.2020.104265
   Larrue F, 2014, J COGN PSYCHOL, V26, P906, DOI 10.1080/20445911.2014.965714
   Lepecq JC, 2009, VIRTUAL REAL-LONDON, V13, P141, DOI 10.1007/s10055-009-0118-1
   Lin LPY, 2020, J EXP PSYCHOL HUMAN, V46, P474, DOI 10.1037/xhp0000724
   Linkenauger SA, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0068594
   Maravita A, 2004, TRENDS COGN SCI, V8, P79, DOI 10.1016/j.tics.2003.12.008
   MARK L S, 1990, Ecological Psychology, V2, P325, DOI 10.1207/s15326969eco0204_2
   Mestre Daniel R., 2016, Human-Computer Interaction. Interaction Platforms and Techniques. 18th International Conference, HCI International 2016. Proceedings: LNCS 9732, P222, DOI 10.1007/978-3-319-39516-6_21
   MILGRAM P, 1994, IEICE T INF SYST, VE77D, P1321
   Petrucci MN, 2016, ECOL PSYCHOL, V28, P108, DOI 10.1080/10407413.2016.1163987
   Pointon G, 2018, ACM SYMPOSIUM ON APPLIED PERCEPTION (SAP 2018), DOI 10.1145/3225153.3225168
   Pointon Grant, 2018, P 2018 IEEE VR 2018, P1
   Skarbez R, 2021, FRONT VIRTUAL REAL, V2, DOI 10.3389/frvir.2021.647997
   Stefanucci JK, 2010, ATTEN PERCEPT PSYCHO, V72, P1338, DOI 10.3758/APP.72.5.1338
   Stefanucci JK, 2009, PERCEPTION, V38, P1782, DOI 10.1068/p6437
   Wagman JB, 2005, ECOL PSYCHOL, V17, P105, DOI 10.1207/s15326969eco1702_3
   WARREN WH, 1987, J EXP PSYCHOL HUMAN, V13, P371, DOI 10.1037/0096-1523.13.3.371
   Witt JK, 2005, J EXP PSYCHOL HUMAN, V31, P880, DOI 10.1037/0096-1523.31.5.880
   Wu HS, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P1775, DOI [10.1109/VR.2019.8797965, 10.1109/vr.2019.8797965]
   Xie B, 2021, FRONT VIRTUAL REAL, V2, DOI 10.3389/frvir.2021.645153
   Zhao Y, 2021, ACM SYMPOSIUM ON APPLIED PERCEPTION (SAP 2021), DOI 10.1145/3474451.3476239
NR 50
TC 0
Z9 0
U1 4
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD OCT
PY 2023
VL 20
IS 4
SI SI
AR 14
DI 10.1145/3613450
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA Z8RR5
UT WOS:001114697800003
OA Bronze
DA 2024-07-18
ER

PT J
AU Wang, YH
   Zhang, Q
   Aubuchon, C
   Kemp, J
   Domini, F
   Tompkin, J
AF Wang, Yuanhao
   Zhang, Qian
   Aubuchon, Celine
   Kemp, Jovan
   Domini, Fulvio
   Tompkin, James
TI On Human-like Biases in Convolutional Neural Networks for the Perception
   of Slant from Texture
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article; Proceedings Paper
CT ACM Symposium on Applied Perception (SAP)
CY AUG 05-06, 2023
CL Los Angeles, CA
SP Assoc Comp Machinery, ACM SIGGRAPH
DE Perception; slant; texture; convolutional neural networks; deep learning
ID SHAPE; CUES
AB Depth estimation is fundamental to 3D perception, and humans are known to have biased estimates of depth. This study investigates whether convolutional neural networks (CNNs) can be biased when predicting the sign of curvature and depth of surfaces of textured surfaces under different viewing conditions (field of view) and surface parameters (slant and texture irregularity). This hypothesis is drawn fromthe idea that texture gradients described by local neighborhoods-a cue identified in human vision literature-are also representable within convolutional neural networks. To this end, we trained both unsupervised and supervised CNN models on the renderings of slanted surfaces with random Polka dot patterns and analyzed their internal latent representations. The results show that the unsupervisedmodels have similar prediction biases as humans across all experiments, while supervised CNN models do not exhibit similar biases. The latent spaces of the unsupervised models can be linearly separated into axes representing field of view and optical slant. For supervised models, this ability varies substantially with model architecture and the kind of supervision (continuous slant vs. sign of slant). Even though this study says nothing of any shared mechanism, these findings suggest that unsupervised CNN models can share similar predictions to the human visual system. Code: github.com/brownvc/Slant-CNN-Biases
C1 [Wang, Yuanhao; Zhang, Qian; Tompkin, James] Brown Univ, Dept Comp Sci, Providence, RI 02912 USA.
   [Aubuchon, Celine; Kemp, Jovan; Domini, Fulvio] Brown Univ, Dept Cognit Linguist & Psychol Sci, Providence, RI 02912 USA.
C3 Brown University; Brown University
RP Wang, YH (corresponding author), Brown Univ, Dept Comp Sci, Providence, RI 02912 USA.
EM yuanhao_wang@alumni.brown.edu; qian_zhang@brown.edu;
   celine_aubuchon@alumni.brown.edu; jovan_kemp@brown.edu;
   fulvio_domini@brown.edu; james_tompkin@brown.edu
OI Wang, Yuanhao/0009-0002-2737-4689; Domini, Fulvio/0000-0002-5510-0397
FU NSF [BCS-2120610, IIS-2107409, CAREER-2144956]; NIH [1R21EY033182-01A1]
FX YW, QZ, and JT thank NSF IIS-2107409 and CAREER-2144956. CA, JK, and FD
   thank NSF BCS-2120610 and NIH 1R21EY033182-01A1.
CR Islam MA, 2021, Arxiv, DOI arXiv:2101.11604
   Campagnoli C, 2022, VISION RES, V190, DOI 10.1016/j.visres.2021.107961
   Domini F, 2003, TRENDS COGN SCI, V7, P444, DOI 10.1016/j.tics.2003.08.007
   Geirhos R, 2019, Arxiv, DOI arXiv:1811.12231
   GIBSON JJ, 1950, AM J PSYCHOL, V63, P367, DOI 10.2307/1418003
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   JOHNSTON EB, 1991, VISION RES, V31, P1351, DOI 10.1016/0042-6989(91)90056-B
   Kubilius J, 2019, ADV NEUR IN, V32
   Langer MS, 2015, VISION RES, V107, P15, DOI 10.1016/j.visres.2014.10.036
   Lindsay GW, 2021, J COGNITIVE NEUROSCI, V33, P2017, DOI 10.1162/jocn_a_01544
   Liu BX, 2004, VISION RES, V44, P2135, DOI 10.1016/j.visres.2004.03.024
   Malik J, 1997, INT J COMPUT VISION, V23, P149, DOI 10.1023/A:1007958829620
   NELDER JA, 1972, J R STAT SOC SER A-G, V135, P370, DOI 10.2307/2344614
   Nili H, 2014, PLOS COMPUT BIOL, V10, DOI 10.1371/journal.pcbi.1003553
   Kingma DP, 2014, Arxiv, DOI arXiv:1312.6114
   Ponce CR, 2019, CELL, V177, P999, DOI 10.1016/j.cell.2019.04.005
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Rosas P, 2004, VISION RES, V44, P1511, DOI 10.1016/j.visres.2004.01.013
   Schrimpf M., bioRxiv
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Storrs KR, 2021, NAT HUM BEHAV, V5, P1402, DOI 10.1038/s41562-021-01097-6
   Todd JT, 2007, J VISION, V7, DOI 10.1167/7.12.9
   Todd JT, 2005, VISION RES, V45, P1501, DOI 10.1016/j.visres.2005.01.003
   Watt SJ, 2005, J VISION, V5, P834, DOI 10.1167/5.10.7
   Xu YD, 2021, NAT COMMUN, V12, DOI 10.1038/s41467-021-22244-7
   Yu N, 2019, PROC CVPR IEEE, P12156, DOI 10.1109/CVPR.2019.01244
   Zhang R, 2019, PR MACH LEARN RES, V97
NR 27
TC 0
Z9 0
U1 3
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD OCT
PY 2023
VL 20
IS 4
SI SI
AR 15
DI 10.1145/3613451
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA Z8RR5
UT WOS:001114697800004
OA Bronze
DA 2024-07-18
ER

PT J
AU Glaholt, MG
   Hollands, JG
   Sim, G
   Spivak, T
   Sacripanti, B
AF Glaholt, Mackenzie G.
   Hollands, Justin G.
   Sim, Grace
   Spivak, Tzvi
   Sacripanti, Beatrice
TI Visual Information Requirements for Dismounted Soldier Target
   Acquisition
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Image resolution; target acquisition; Johnson criteria; dismounted
   soldier; target detection; target identification
ID PERFORMANCE
AB We conducted an empirical investigation of the visual information requirements for target detection and threat identification decisions in the dismounted soldier context. Forty soldiers viewed digital photographs of a person standing against a forested background. The soldiers made two-alternative detection decisions requiring them to determine whether the target was present in the scene, and two-alternative threat identification decisions that required discrimination of the objects held by the target, the clothing worn by the target, and target postures. The images were presented to subjects on a computer display, and variation in the apparent target distance was simulated through digital image magnification and by varying the viewing distance to the display. Image resolution was degraded progressively by spatial frequency filtering and we estimated the resolution threshold in each task. These threshold values were compared with the historical Johnson criteria for predicting imaging device performance. Our data are broadly consistent with the previously reported values, though our threat identification decisions required subjects to perceive information with a larger spatial scale than the Johnson criterion for identification of standing human targets. In a second experiment, we employed a four-alternative identification decision and found results that were consistent with those from Experiment 1. We also confirmed that the spatial scale of visual information used for target acquisition is highly task-specific, and provided a novel demonstration of changes in visual information requirements as a function of target range. These findings pose challenges for models of target acquisition with imaging devices.
C1 [Glaholt, Mackenzie G.; Hollands, Justin G.; Sim, Grace; Spivak, Tzvi; Sacripanti, Beatrice] Toronto Res Ctr, Def Res & Dev Canada, Toronto, ON, Canada.
C3 Defence Research & Development Canada
RP Glaholt, MG (corresponding author), Toronto Res Ctr, Def Res & Dev Canada, Toronto, ON, Canada.
EM Mackenzie.Glaholt@drdc-rddc.gc.ca
CR Adomeit U, 2012, PROC SPIE, V8541, DOI 10.1117/12.979288
   Barten P. G., 2003, IMAGE QUALITY SYSTEM
   Bijl P., 1998, SPIE
   D'Agostino J. A., 1995, ACQUIRE RANGE PERFOR
   Donohue J., 1991, Introductory Review of Target Discrimination Criteria
   Hollands JG, 2018, HUM FACTORS, V60, P363, DOI 10.1177/0018720818760331
   Johnson J., 1958, P IM INT S
   Ke<ss>ler S., 2017, SPIE DEFENSE SECURIT
   Moyer S, 2006, OPT ENG, V45, DOI 10.1117/1.2213997
   OConnor J. D., 1998, AEROSPACE DEFENSE SE
   OKane B. L., 1992, PASSIVE SENSORS
   Preece B. L., 2011, SPIE DEFENSE SECURIT
   RATCHES J A, 1976, OPTICAL ENG, P15
   Ratches JA, 2001, IEEE SENS J, V1, P31, DOI 10.1109/JSEN.2001.923585
   SCHMIEDER DE, 1983, IEEE T AERO ELEC SYS, V19, P622, DOI 10.1109/TAES.1983.309351
   Smith Collin S., 2015, SAND20156368 SANDIA
   Teaney B. P., 2012, INFRARED IMAGING SYS
   Teaney B. P., 2016, SPIE DEFENSE SECURIT
   Teaney B. P., 2007, INFRARED IMAGING SYS
   Vollmerhausen R. H., 2004, NEW METRIC PREDICTIN
NR 20
TC 0
Z9 0
U1 0
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD MAR
PY 2020
VL 17
IS 1
AR 2
DI 10.1145/3375000
PG 20
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OH5JA
UT WOS:000582618000002
DA 2024-07-18
ER

PT J
AU Janeh, O
   Langbehn, E
   Steinicke, F
   Bruder, G
   Gulberti, A
   Poetter-Nerger, M
AF Janeh, Omar
   Langbehn, Eike
   Steinicke, Frank
   Bruder, Gerd
   Gulberti, Alessandro
   Poetter-Nerger, Monika
TI Walking in Virtual Reality: Effects of Manipulated Visual Self-Motion on
   Walking Biomechanics
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Virtual environments; real walking; translation gains; biomechanics;
   gait
ID GAIT; PERCEPTION; SPEED; LOCOMOTION; INTERFACE
AB Walking constitutes the predominant form of self-propelled movement from one geographic location to another in our real world. Likewise, walking in virtual environments (VEs) is an essential part of a users experience in many application domains requiring a high degree of interactivity. However, researchers and practitioners often observe that basic implementations of virtual walking, in which head-tracked movements are mapped isometrically to a VE are not estimated as entirely natural. Instead, users estimate a virtual walking velocity as more natural when it is slightly increased compared to the users physical body movement.
   In this article, we investigate the effects of such nonisometric mappings between physical movements and virtual motions in the VE on walking velocity and biomechanics of the gait cycle. Therefore, we performed an experiment in which we measured and analyzed parameters of the biomechanics of walking under conditions with isometric as well as nonisometric mappings. Our results show significant differences in most gait parameters when walking in the VE in the isometric mapping condition compared to the corresponding parameters in the real world. For nonisometric mappings we found an increased divergence of gait parameters depending on the velocity of visual self-motion feedback. The results revealed a symmetrical effect of gait detriments for up-or down-scaled virtual velocities, which we discuss in the scope of the previous findings.
C1 [Janeh, Omar; Langbehn, Eike; Steinicke, Frank] Univ Hamburg, Human Comp Interact, Dept Informat, Vogt Koelln Str 30, D-22527 Hamburg, Germany.
   [Bruder, Gerd] Univ Cent Florida, Inst Simulat & Training, 3100 Technol Pkwy, Orlando, FL 32826 USA.
   [Gulberti, Alessandro] Univ Med Ctr Hamburg Eppendorf, Dept Neurophysiol & Pathophysiol, Martini Str 52, D-20251 Hamburg, Germany.
   [Poetter-Nerger, Monika] Univ Med Ctr Hamburg Eppendorf, Dept Neurol, Martini Str 52, D-20251 Hamburg, Germany.
C3 University of Hamburg; State University System of Florida; University of
   Central Florida; University of Hamburg; University Medical Center
   Hamburg-Eppendorf; University of Hamburg; University Medical Center
   Hamburg-Eppendorf
RP Janeh, O (corresponding author), Univ Hamburg, Human Comp Interact, Dept Informat, Vogt Koelln Str 30, D-22527 Hamburg, Germany.
EM janeh@informatik.uni-hamburg.de; langbehn@informatik.uni-hamburg.de;
   steinicke@informatik.uni-hamburg.de; bruder@ucf.edu; a.gulberti@uke.de;
   m.poetter-nerger@uke.de
RI Janeh, Omar/ABC-9250-2020; Janeh, Omar/O-3504-2017; Steinicke,
   Frank/AAC-2976-2020; Janeh, Omar/ABA-1104-2020; Janeh,
   Omar/AAD-6428-2019
OI Janeh, Omar/0000-0002-3887-0517; Steinicke, Frank/0000-0001-9879-7414;
   Janeh, Omar/0000-0002-3887-0517; Gulberti,
   Alessandro/0000-0003-0538-5109
FU Office of Naval Research (ONR) [N00014-14-1-0248, N00014-12-1-1003];
   German Research Foundation (DFG); German Academic Exchange Service
   (DAAD)
FX Authors of this work are supported in part by the Office of Naval
   Research (ONR) Code 30 under Dr. Peter Squire, Program Officer (ONR
   awards N00014-14-1-0248 and N00014-12-1-1003). Additionally, authors of
   this work receive financial support in part from the German Research
   Foundation (DFG), and the German Academic Exchange Service (DAAD).
CR Adam Jones J., 2012, P ACM S APPL PERC, P11, DOI 10.1145/2338676.2338679
   Alexander R. M., 2003, PRINCIPLES ANIMAL LO
   [Anonymous], 1981, Human Walking
   Banton T, 2005, PRESENCE-TELEOP VIRT, V14, P394, DOI 10.1162/105474605774785262
   Bogey R., 2014, GAIT ANAL
   Bouguila L., 2002, Virtual Environments 2002. Eurographics Workshop Proceedings, P197
   Cohen J., 1973, ED PSYCHOL MEASUREME
   Durgin F. H., 2004, Journal of Vision, V4, P802, DOI DOI 10.1167/4.8.802
   Durgin FH, 2005, J EXP PSYCHOL HUMAN, V31, P339, DOI 10.1037/0096-1523.31.2.339
   Durgin FH, 2007, ACM T APPL PERCEPT, V4, DOI [10.1145/1227134.1227139, 10.1145/1227134/1227139]
   Durgin Frank H., 2002, J VISION, V2, P429
   FRENZ H, 2007, ACM T APPL PERCEPT, V3, P419
   Gouelle A, 2014, J REHABIL RES DEV, V51, P665, DOI 10.1682/JRRD.2013.09.0198
   Gouelle A, 2011, J MOTOR BEHAV, V43, P95, DOI 10.1080/00222895.2010.538768
   GREENHOUSE SW, 1959, PSYCHOMETRIKA, V24, P95, DOI 10.1007/BF02289823
   Gretz HR, 1998, NEUROREHABILITATION, V11, P211, DOI 10.3233/NRE-1998-11305
   Harris L. R., 2002, Virtual Reality, V6, P75, DOI 10.1007/s100550200008
   Hoffman DM, 2008, J VISION, V8, DOI 10.1167/8.3.33
   Hollman JH, 2007, GAIT POSTURE, V26, P289, DOI 10.1016/j.gaitpost.2006.09.075
   Interrante V, 2008, PRESENCE-TELEOP VIRT, V17, P176, DOI 10.1162/pres.17.2.176
   Interrante V, 2006, P IEEE VIRT REAL ANN, P3, DOI 10.1109/VR.2006.52
   Jones J.A., 2011, Proc. Symposium on Applied perception in Graphics and Visualization, P29
   Kennedy John J., 1970, ED PSYCHOL MEASUREME
   Kennedy RS, 1993, The international journal of aviation psychology, V3, P203, DOI [DOI 10.1207/S15327108IJAP0303, 10.1207/s15327108ijap0303_3]
   Likert R., ARCH PSYCHOL, P1
   Loomis JM, 2003, VIRTUAL AND ADAPTIVE ENVIRONMENTS: APPLICATIONS, IMPLICATIONS, AND HUMAN PERFORMANCE ISSUES, P21
   Marsh WE, 2013, 2013 IEEE SYMPOSIUM ON 3D USER INTERFACES (3DUI), P15, DOI 10.1109/3DUI.2013.6550191
   Mauchly JW, 1940, ANN MATH STAT, V11, P204, DOI 10.1214/aoms/1177731915
   Mohler Betty., 2007, 13th Eurographics Symposium on Virtual Environments and 10th Immersive Projection Technology Workshop (IPT-EGVE 2007), P85, DOI DOI 10.2312/PE/VE2007SHORT/085-088
   Mohler BJ, 2007, EXP BRAIN RES, V181, P221, DOI 10.1007/s00221-007-0917-0
   Multon F., 2013, Human Walking in Virtual Environments, P55, DOI [10.1007/978-1-4419-8432- 6_3, DOI 10.1007/978-1-4419-8432-6_3]
   Nelson A., 2008, GAITRite Operating manual
   NUTT JG, 1993, NEUROLOGY, V43, P268, DOI 10.1212/WNL.43.2.268
   Pynn LK, 2013, VISION RES, V76, P124, DOI 10.1016/j.visres.2012.10.019
   Renner RS, 2013, ACM COMPUT SURV, V46, DOI 10.1145/2543581.2543590
   Riecke BE, 2007, IEEE VIRTUAL REALITY 2007, PROCEEDINGS, P3
   Ruddle RA, 2004, INT J HUM-COMPUT ST, V60, P299, DOI 10.1016/j.ijhcs.2003.10.001
   Ruddle RA, 2009, ACM T COMPUT-HUM INT, V16, DOI 10.1145/1502800.1502805
   Schwaiger M, 2007, LECT NOTES COMPUT SC, V4551, P926
   Sheik-Nainar MA, 2007, HUM FACTORS, V49, P696, DOI 10.1518/001872007X215773
   Slater M, 1999, PRESENCE-TELEOP VIRT, V8, P560, DOI 10.1162/105474699566477
   Steinicke F, 2013, Human walking in virtual environments
   Steinicke F, 2010, IEEE T VIS COMPUT GR, V16, P17, DOI 10.1109/TVCG.2009.62
   Steinicke F, 2008, PROCEEDINGS OF THE 2008 INTERNATIONAL CONFERENCE ON CYBERWORLDS, P217, DOI 10.1109/CW.2008.53
   Suma EA, 2012, IEEE VIRTUAL REALITY CONFERENCE 2012 PROCEEDINGS, P43, DOI 10.1109/VR.2012.6180877
   Templeman JN, 1999, PRESENCE-TELEOP VIRT, V8, P598, DOI 10.1162/105474699566512
   Usoh M, 1999, COMP GRAPH, P359, DOI 10.1145/311535.311589
   Watt SJ, 2005, J VISION, V5, P834, DOI 10.1167/5.10.7
   Willemsen P., 2004, P 1 S APPL PERC GRAP, P35, DOI DOI 10.1145/1012551.1012558
NR 49
TC 51
Z9 54
U1 0
U2 17
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD FEB
PY 2017
VL 14
IS 2
AR 12
DI 10.1145/3022731
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA EM8VC
UT WOS:000395588200005
DA 2024-07-18
ER

PT J
AU Abebe, MA
   Pouli, T
   Kervec, J
AF Abebe, Mekides Assefa
   Pouli, Tania
   Kervec, Jonathan
TI Evaluating the Color Fidelity of ITMOs and HDR Color Appearance Models
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article; Proceedings Paper
CT ACM SIGGRAPH Symposium on Applied Perception (SAP)
CY SEP 13-14, 2015
CL Tubingen, GERMANY
SP ACM SIGGRAPH, ACM, Disney Res, Max Planck Inst Biol Cybernet
DE Color appearance modeling; ITMO; high-dynamic-range imaging; subjective
   evaluation
ID IMAGE APPEARANCE; TONE; ENHANCEMENT; FRAMEWORK; VIDEO
AB With the increasing availability of high-dynamic-range (HDR) displays comes the need to remaster existing content in a way that takes advantage of the extended range of luminance and contrast that such displays offer. At the same time, it is crucial that the creative intent of the director is preserved through such changes as much as possible. In this article, we compare several approaches for dynamic range extension to assess their ability to correctly reproduce the color appearance of standard dynamic range (SDR) images on HDR displays. A number of state-of-the-art inverse tone mapping operators (ITMOs) combined with a standard chromatic adaptation transform (CAT) as well as some HDR color appearance models have been evaluated through a psychophysical study, making use of an HDR display as well as HDR ground-truth data. We found that global ITMOs lead to the most reliable performance when combined with a standard CAT, while more complex methods were found to be more scene dependent, and often less preferred than the unprocessed SDR image. HDR color appearance models, albeit being the most complete solutions for accurate color reproduction, were found to not be well suited to the problem of dynamic range expansion, suggesting that further research may be necessary to provide accurate color management in the context of inverse tone mapping.
C1 [Abebe, Mekides Assefa] Technicolor Res & Innovat, Cesson Sevigne, France.
   [Abebe, Mekides Assefa] Univ Poitiers, Poitiers, France.
   [Pouli, Tania; Kervec, Jonathan] Technicolor, Cesson Sevigne, France.
C3 Technicolor SA; Universite de Poitiers; Technicolor SA
RP Abebe, MA (corresponding author), Technicolor Res & Innovat, Cesson Sevigne, France.
EM mekides123@gmail.com; Tania.Pouli@technicolor.com;
   jonathan.kervec@technicolor.com
RI Abebe, Mekides Assefa/AAC-8761-2022
OI Abebe, Mekides Assefa/0000-0001-6756-9769; Pouli,
   Tania/0000-0002-5941-086X
CR Akyüz AG, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1276377.1276425
   Akyüz AO, 2006, J ELECTRON IMAGING, V15, DOI 10.1117/1.2238891
   Akyuz Ahmet Oguz, 2004, 1 ACM S APPL PERC GR, P166
   [Anonymous], 2012, ITUR Recommendation BT. 500-13
   [Anonymous], THESIS
   [Anonymous], 2000, 801 CIETC
   [Anonymous], 2002, MPII20024002
   [Anonymous], 2013, Colour Appearance Models
   Banterle F., 2006, P 4 INT C COMP GRAPH, P349
   Banterle F, 2007, VISUAL COMPUT, V23, P467, DOI 10.1007/s00371-007-0124-9
   Banterle F, 2009, COMPUT GRAPH FORUM, V28, P13, DOI 10.1111/j.1467-8659.2008.01176.x
   Banterle Francesco, 2011, ADV HUGH DYNAMIC RAN
   Cadík M, 2008, COMPUT GRAPH-UK, V32, P330, DOI 10.1016/j.cag.2008.04.003
   Daly S, 2004, PROC SPIE, V5292, P130, DOI 10.1117/12.526937
   Daly S, 2003, P SOC PHOTO-OPT INS, V5008, P455, DOI 10.1117/12.472016
   Didyk P, 2008, COMPUT GRAPH FORUM, V27, P1265, DOI 10.1111/j.1467-8659.2008.01265.x
   Ebner F, 1998, SIXTH COLOR IMAGING CONFERENCE: COLOR SCIENCE, SYSTEMS AND APPLICATIONS, P8
   Fairchild M., 2002, Tenth Color Imaging Conference: Color Science and Engineering System s, Technologies, Applications 10, P33
   Fairchild MD, 2007, FIFTEENTH COLOR IMAGING CONFERENCE: COLOR SCIENCE AND ENGINEERING SYSTEMS, TECHNOLOGIES, AND APPLICATIONS, FINAL PROGRAM AND PROCEEDINGS, P233
   Fairchild Mark D., 2007, ICAM06 HDR IM APP
   Fairchild MD, 2004, J ELECTRON IMAGING, V13, P126, DOI 10.1117/1.1635368
   Gryaditskya Y, 2014, COMPUT GRAPH FORUM, V33, P61, DOI 10.1111/cgf.12474
   Huo YQ, 2014, SCI WORLD J, DOI 10.1155/2014/168564
   IEC, 1999, MULT SYST EQ COL MEA
   Kim MH, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531333
   Kovaleski RP, 2009, VISUAL COMPUT, V25, P539, DOI 10.1007/s00371-009-0327-3
   Kuang JT, 2007, ACM T APPL PERCEPT, V4, DOI 10.1145/1265957.1265958
   Kuang JT, 2007, J VIS COMMUN IMAGE R, V18, P406, DOI 10.1016/j.jvcir.2007.06.003
   Landis Hayden, 2002, SOGGRAPH COURSE NOTE, V16
   Ledda Patrick, 2004, P 1 ACM S APPL PERC, P159
   Mantiuk R, 2009, COMPUT GRAPH FORUM, V28, P193, DOI 10.1111/j.1467-8659.2009.01358.x
   Masia B, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618506
   Masia Belen, 2011, TECHNICAL REPORT
   Meylan L, 2007, PROC SPIE, V6492, DOI 10.1117/12.706472
   Meylan L, 2006, IEEE T IMAGE PROCESS, V15, P2820, DOI 10.1109/TIP.2006.877312
   Pouli Tania, 2013, Twenty-first Color and Imaging Conference. Color Science and Engineering Systems, Technologies, and Applications (CIC21). Proceedings, P215
   Ramanarayanan G, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1276377.1276472, 10.1145/1239451.1239527]
   Reinhard E., 2008, Color Imaging: Fundamentals and Applications
   Reinhard E, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366220
   Rempel AG, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239490
   Seetzen H, 2004, ACM T GRAPHIC, V23, P760, DOI 10.1145/1015706.1015797
   Sharma G., 2002, DIGITAL COLOR IMAGIN
   SMPTE, 2002, 1771993 SMPTE
   Wanat Robert., 2012, Theory and Practice of Computer Graphics, Rutherford, United Kingdom, P9
   Wang L., 2007, Rendering Techniques, P321
   Yoshida A, 2005, PROC SPIE, V5666, P192, DOI 10.1117/12.587782
   Zhang XM, 1997, COMPCON IEEE, P44, DOI 10.1109/CMPCON.1997.584669
NR 47
TC 10
Z9 10
U1 0
U2 9
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD SEP
PY 2015
VL 12
IS 4
SI SI
AR 14
DI 10.1145/2808232
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA CR1ER
UT WOS:000361067300002
DA 2024-07-18
ER

PT J
AU Allison, RS
   Wilcox, LM
AF Allison, Robert S.
   Wilcox, Laurie M.
TI Perceptual Tolerance to Stereoscopic 3D Image Distortion
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Human Factors; Stereoscopic 3D; scaling; film; interaxial; convergence
ID SLANT; DEPTH; CUES
AB An intriguing aspect of picture perception is the viewer's tolerance to variation in viewing position, perspective, and display size. These factors are also present in stereoscopic media, where there are additional parameters associated with the camera arrangement (e.g., separation, orientation). The predicted amount of depth from disparity can be obtained trigonometrically; however, perceived depth in complex scenes often differs from geometric predictions based on binocular disparity alone. To evaluate the extent and the cause of deviations from geometric predictions of depth from disparity in naturalistic scenes, we recorded stereoscopic footage of an indoor scene with a range of camera separations (camera interaxial (IA) ranged from 3 to 95 mm) and displayed them on a range of screen sizes. In a series of experiments participants estimated 3D distances in the scene relative to a reference scene, compared depth between shots with different parameters, or reproduced the depth between pairs of objects in the scene using reaching or blind walking. The effects of IA and screen size were consistently and markedly smaller than predicted from the binocular viewing geometry, suggesting that observers are able to compensate for the predicted distortions. We conclude that the presence of multiple realistic monocular depth cues drives normalization of perceived depth from binocular disparity. It is not clear to what extent these differences are due to cognitive as opposed to perceptual factors. However, it is notable that these normalization processes are not task specific; they are evident in both perception- and action-oriented tasks.
C1 [Allison, Robert S.] York Univ, Dept Elect Engn & Comp Sci, Toronto, ON M3J 1P3, Canada.
   [Wilcox, Laurie M.] York Univ, Dept Psychol, Toronto, ON M3J 1P3, Canada.
C3 York University - Canada; York University - Canada
RP Allison, RS (corresponding author), York Univ, Dept Elect Engn & Comp Sci, 4700 Keele St, Toronto, ON M3J 1P3, Canada.
EM allison@cse.yorku.ca; lwilcox@yorku.ca
OI Wilcox, Laurie/0000-0002-3594-6192; Allison, Robert/0000-0002-4485-2665
FU 3DFlic project from the Ontario Media Development Corporation; Ontario
   Centres of Excellence
FX This work is supported by grants to the 3DFlic project from the Ontario
   Media Development Corporation and the Ontario Centres of Excellence.
   Thanks to Ali Kazimi for the S3D content and to Karim Benzeroual for
   conducting the experiments that collected the data reported here.
CR Allison RS, 2007, J IMAGING SCI TECHN, V51, P317, DOI 10.2352/J.ImagingSci.Technol.(2007)51:4(317)
   Allison RS, 2013, PUBLIC, V24, P149
   Allison RS, 2000, VISION RES, V40, P1869, DOI 10.1016/S0042-6989(00)00034-1
   Banks MS, 2014, ECOL PSYCHOL, V26, P30, DOI 10.1080/10407413.2014.877284
   Banks Martin S, 2009, Inf Disp (1975), V25, P12
   Benzeroual Karim, 2011, P SMPTE INT C STER 3, P61
   Benzeroual Karim, 2011, P 2011 INT C 3D IM I, P591
   BULTHOFF HH, 1988, J OPT SOC AM A, V5, P1749, DOI 10.1364/JOSAA.5.001749
   Du SP, 2013, IEEE T VIS COMPUT GR, V19, P1288, DOI 10.1109/TVCG.2013.14
   FOLEY JM, 1980, PSYCHOL REV, V87, P411, DOI 10.1037/0033-295X.87.5.411
   Glennerster A, 2006, CURR BIOL, V16, P428, DOI 10.1016/j.cub.2006.01.019
   Goodale MA, 2014, P ROY SOC B-BIOL SCI, V281, DOI 10.1098/rspb.2014.0337
   Hillis JM, 2004, J VISION, V4, P967, DOI 10.1167/4.12.1
   Kelly JW, 2013, ACM T APPL PERCEPT, V10, DOI 10.1145/2536764.2536765
   LOOMIS JM, 1992, J EXP PSYCHOL HUMAN, V18, P906, DOI 10.1037/0096-1523.18.4.906
   Sedgwick HA, 1993, PICTORIAL COMMUNICAT, P460
   Shibata T, 2011, J VISION, V11, DOI 10.1167/11.8.11
   Spottiswoode R., 1953, The theory of stereoscopic transmission its application to the motion picture
   STEVENS SS, 1962, AM PSYCHOL, V17, P29, DOI 10.1037/h0045795
   Vangorp P, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461971
   Vishwanath D, 2005, NAT NEUROSCI, V8, P1401, DOI 10.1038/nn1553
   Wartrell Z, 1999, COMP GRAPH, P351, DOI 10.1145/311535.311587
   Wilcox LM, 2009, VISION RES, V49, P2653, DOI 10.1016/j.visres.2009.06.004
   WOODS A, 1993, P SOC PHOTO-OPT INS, V1915, P36, DOI 10.1117/12.157041
NR 24
TC 11
Z9 12
U1 0
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD JUL
PY 2015
VL 12
IS 3
AR 10
DI 10.1145/2770875
PG 20
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA CP6OI
UT WOS:000360006600004
DA 2024-07-18
ER

PT J
AU O'Toole, AJ
   An, XB
   Dunlop, J
   Natu, V
   Phillips, PJ
AF O'Toole, Alice J.
   An, Xiaobo
   Dunlop, Joseph
   Natu, Vaidehi
   Phillips, P. Jonathon
TI Comparing Face Recognition Algorithms to Humans on Challenging Tasks
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Algorithms; Human Factors; Verification; Experimentation; Face
   recognition; human-machine comparisons
ID FAMILIAR
AB We compared face identification by humans and machines using images taken under a variety of uncontrolled illumination conditions in both indoor and outdoor settings. Natural variations in a person's day-to-day appearance (e.g., hair style, facial expression, hats, glasses, etc.) contributed to the difficulty of the task. Both humans and machines matched the identity of people (same or different) in pairs of frontal view face images. The degree of difficulty introduced by photometric and appearance-based variability was estimated using a face recognition algorithm created by fusing three top-performing algorithms from a recent international competition. The algorithm computed similarity scores for a constant set of same-identity and different-identity pairings from multiple images. Image pairs were assigned to good, moderate, and poor accuracy groups by ranking the similarity scores for each identity pairing, and dividing these rankings into three strata. This procedure isolated the role of photometric variables from the effects of the distinctiveness of particular identities. Algorithm performance for these constant identity pairings varied dramatically across the groups. In a series of experiments, humans matched image pairs from the good, moderate, and poor conditions, rating the likelihood that the images were of the same person (1: sure same -5: sure different). Algorithms were more accurate than humans in the good and moderate conditions, but were comparable to humans in the poor accuracy condition. To date, these are the most variable illumination-and appearance-based recognition conditions on which humans and machines have been compared. The finding that machines were never less accurate than humans on these challenging frontal images suggests that face recognition systems may be ready for applications with comparable difficulty. We speculate that the superiority of algorithms over humans in the less challenging conditions may be due to the algorithms' use of detailed, view-specific identity information. Humans may consider this information less important due to its limited potential for robust generalization in suboptimal viewing conditions.
C1 [O'Toole, Alice J.; An, Xiaobo; Dunlop, Joseph] Univ Texas Dallas, Sch Behav & Brain Sci, Richardson, TX 75083 USA.
   [Phillips, P. Jonathon] NIST, Gaithersburg, MD 20899 USA.
C3 University of Texas System; University of Texas Dallas; National
   Institute of Standards & Technology (NIST) - USA
RP O'Toole, AJ (corresponding author), Univ Texas Dallas, Sch Behav & Brain Sci, GR4-1, Richardson, TX 75083 USA.
EM otoole@utdallas.edu; jonathon@nist.gov
OI O'Toole, Alice/0000-0001-7981-1508
FU Technical Support Working Group of the Department of Defense, USA;
   Federal Bureau of Investigation
FX This work was supported by funding from the Technical Support Working
   Group of the Department of Defense, USA. P.J. Phillips was supported in
   part by funding from the Federal Bureau of Investigation. The
   identification of any commercial product or trade name does not imply
   endorsement or recommendation by NIST.
CR Adini Y, 1997, IEEE T PATTERN ANAL, V19, P721, DOI 10.1109/34.598229
   [Anonymous], 1991, Detection theory: A user's guide
   Braje WL, 1998, PSYCHOBIOLOGY, V26, P371
   Braje WL, 2003, J VISION, V3, P161, DOI 10.1167/3.2.4
   Burton AM, 2011, BRIT J PSYCHOL, V102, P943, DOI 10.1111/j.2044-8295.2011.02039.x
   GOBBINI M. I., 2011, NEUROPSYCHOLOGICA, V45, P32
   Gross R, 2005, HANDBOOK OF FACE RECOGNITION, P193, DOI 10.1007/0-387-27257-7_10
   Hancock PJB, 2000, TRENDS COGN SCI, V4, P330, DOI 10.1016/S1364-6613(00)01519-9
   Haxby JV, 2000, TRENDS COGN SCI, V4, P223, DOI 10.1016/S1364-6613(00)01482-0
   Hill H, 1996, J EXP PSYCHOL HUMAN, V22, P986, DOI 10.1037/0096-1523.22.4.986
   Jenkins R, 2011, COGNITION, V121, P313, DOI 10.1016/j.cognition.2011.08.001
   JOHNSTON A, 1992, PERCEPTION, V21, P365, DOI 10.1068/p210365
   Johnston RA, 2009, MEMORY, V17, P577, DOI 10.1080/09658210902976969
   Natu V, 2011, BRIT J PSYCHOL, V102, P726, DOI 10.1111/j.2044-8295.2011.02053.x
   O'Toole AJ, 2002, TRENDS COGN SCI, V6, P261, DOI 10.1016/S1364-6613(02)01908-3
   O'Toole AJ, 2007, IEEE T SYST MAN CY B, V37, P1149, DOI 10.1109/TSMCB.2007.907034
   O'Toole AJ, 2007, IEEE T PATTERN ANAL, V29, P1642, DOI 10.1109/TPAMI.2007.1107
   O'Toole AJ, 2012, IMAGE VISION COMPUT, V30, P169, DOI 10.1016/j.imavis.2011.12.007
   OTOOLE AJ, 2008, P 8 INT C AUT FAC GE
   Phillips P.J., 2011, P 9 INT C AUT FAC GE
   Phillips PJ, 2012, IMAGE VISION COMPUT, V30, P177, DOI 10.1016/j.imavis.2012.01.004
   Phillips PJ, 2011, ACM T APPL PERCEPT, V8, DOI 10.1145/1870076.1870082
   Phillips PJ, 2010, IEEE T PATTERN ANAL, V32, P831, DOI 10.1109/TPAMI.2009.59
   Phillips PJ, 2005, PROC CVPR IEEE, P947
   RICE A., 2012, J VISION
NR 25
TC 57
Z9 65
U1 1
U2 21
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD OCT
PY 2012
VL 9
IS 4
AR 16
DI 10.1145/2355598.2355599
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA 025DJ
UT WOS:000310164900001
DA 2024-07-18
ER

PT J
AU Rocchesso, D
   Delle Monache, S
AF Rocchesso, Davide
   Delle Monache, Stefano
TI Perception and Replication of Planar Sonic Gestures
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Human Factors; Auditory localization; sonic gestures
ID AUDITORY SALTATION; SOUND; LOCALIZATION; MOVEMENT; ILLUSION
AB As tables, boards, and walls become surfaces where interaction can be supported by auditory displays, it becomes important to know how accurately and effectively a spatial gesture can be rendered by means of an array of loudspeakers embedded in the surface. Two experiments were designed and performed to assess: (i) how sequences of sound pulses are perceived as gestures when the pulses are distributed in space and time along a line; (ii) how the timing of pulses affects the perceived and reproduced continuity of sequences; and (iii) how effectively a second parallel row of speakers can extend sonic gestures to a two-dimensional space. Results show that azimuthal trajectories can be effectively replicated and that switching between discrete and continuous gestures occurs within the range of inter-pulse interval from 75 to 300ms. The vertical component of sonic gestures cannot be reliably replicated.
C1 [Rocchesso, Davide; Delle Monache, Stefano] Iuav Univ Venice, I-30123 Venice, Italy.
C3 IUAV University Venice
RP Rocchesso, D (corresponding author), Iuav Univ Venice, Dorsoduro 2206, I-30123 Venice, Italy.
EM roc@iuav.it
RI DELLE MONACHE, STEFANO/S-8570-2019; Rocchesso, Davide/H-4711-2019
OI DELLE MONACHE, STEFANO/0000-0003-2210-8410; Rocchesso,
   Davide/0000-0002-0849-7766
FU COST Action on Sonic Interaction Design [IC0601]
FX The initial ideas for this study emerged at the Product Sound Design
   Summer School held at Aalto University, Finland 2010, organized by
   Cumhur Erkut and supported by the COST Action IC0601 on Sonic
   Interaction Design.
CR ADCOCK M., 2004, P INT C AUD DISPL
   Andersen TH, 2010, ACM T APPL PERCEPT, V7, DOI 10.1145/1773965.1773968
   [Anonymous], 2007, INFORM RETRIEVAL MUS
   Bakker S, 2010, LECT NOTES COMPUT SC, V6306, P55, DOI 10.1007/978-3-642-15841-4_7
   Begault DR, 2006, J AUDIO ENG SOC, V54, P276
   BREMER CD, 1977, AM J PSYCHOL, V90, P645, DOI 10.2307/1421738
   Brown L. M., 2003, P ICAD, P152
   BUTLER RA, 1977, J ACOUST SOC AM, V61, P1264, DOI 10.1121/1.381427
   Caramiaux B., 2011, P INT C NEW INT MUS, P144
   Carlile S, 2002, J ACOUST SOC AM, V111, P1026, DOI 10.1121/1.1436067
   De Bruyn L., 2011, MUSIC MED, V4, P28, DOI [10.1177/1943862111415116, DOI 10.1177/1943862111415116]
   Delle Monache S., 2011, P 9 ACM SIGCHI IT CH, P79
   Delle Monache S., 2010, P 5 AUD MOSTL C C IN, P1
   Desmet F, 2010, STUD CLASS DATA ANAL, P399, DOI 10.1007/978-3-642-01044-6_36
   Deutsch D, 2010, PHYS TODAY, V63, P40, DOI 10.1063/1.3326988
   Erkut C, 2013, SONIC INTERACTION DESIGN, P341
   Getzmann S, 2008, EXP PSYCHOL, V55, P64, DOI 10.1027/1618-3169.55.1.64
   Goldreich D, 2007, PLOS ONE, V2, DOI 10.1371/journal.pone.0000333
   Hartmann WH, 1999, PHYS TODAY, V52, P24, DOI 10.1063/1.882727
   Ihlefeld A, 2011, J ACOUST SOC AM, V130, P324, DOI 10.1121/1.3596476
   Jensenius AlexanderRefsum., 2010, MUSICAL GESTURES SOU, V1st, P12, DOI [10.4324/9780203863411, DOI 10.4324/9780203863411]
   Kilian K., 2009, Audio Branding: Brands, Sound and Communication, P35
   Kubovy M, 2010, REV PHILOS PSYCHOL, V1, P41, DOI 10.1007/s13164-009-0004-5
   LAKATOS S, 1993, PERCEPTION, V22, P363, DOI 10.1068/p220363
   LAKATOS S, 1993, PERCEPT PSYCHOPHYS, V54, P139, DOI 10.3758/BF03211749
   Lemmens P, 2009, WORLD HAPTICS 2009: THIRD JOINT EUROHAPTICS CONFERENCE AND SYMPOSIUM ON HAPTIC INTERFACES FOR VIRTUAL ENVIRONMENT AND TELEOPERATOR SYSTEMS, PROCEEDINGS, P7, DOI 10.1109/WHC.2009.4810832
   MILLS AW, 1958, J ACOUST SOC AM, V30, P237, DOI 10.1121/1.1909553
   Muller-Tomfelde C., 2001, Proc. ICAD 2001. ICAD, P267
   Neuhoff J.G., 2004, Ecological Psychoacoustics, P87
   Patel Neel S., 2012, Interactions, V19, P34, DOI 10.1145/2065327.2065336
   PERROTT DR, 1990, J ACOUST SOC AM, V87, P1728, DOI 10.1121/1.399421
   Phillips DP, 2001, J ACOUST SOC AM, V110, P1539, DOI 10.1121/1.1396329
   Raisamo J., 2009, P 2009 INT C MULT IN, P319, DOI DOI 10.1145/1647314
   RAKERD B, 1985, J ACOUST SOC AM, V78, P524, DOI 10.1121/1.392474
   ROCCHESSO D., 2011, P SOUND MUS COMP C, P265
   RUFF RM, 1982, PERCEPT MOTOR SKILL, V55, P155, DOI 10.2466/pms.1982.55.1.155
   Rusconi E, 2006, COGNITION, V99, P113, DOI 10.1016/j.cognition.2005.01.004
   SABERI K, 1990, J ACOUST SOC AM, V88, P2639, DOI 10.1121/1.399984
   Shams L, 2002, COGNITIVE BRAIN RES, V14, P147, DOI 10.1016/S0926-6410(02)00069-1
   Shore DI, 1998, J ACOUST SOC AM, V103, P3730, DOI 10.1121/1.423093
   TOLKMITT FJ, 1977, AM J PSYCHOL, V90, P73, DOI 10.2307/1421642
   Van Valkenburg D, 2003, COGNITION, V87, P225, DOI 10.1016/S0010-0277(03)00005-2
   Wang ZQ, 1996, IEEE T SPEECH AUDI P, V4, P446, DOI 10.1109/89.544529
   Ware C., 2020, INFORM VISUALIZATION
   Wright Matthew., 2004, Proceedings of the International Computer Music Conference, P423
   Zahorik P, 2005, ACTA ACUST UNITED AC, V91, P409
   Zbyszynski M., 2007, NIME'07 Proceedings of the 2007 international conference on New interfaces for musical expression, P100
NR 47
TC 4
Z9 4
U1 0
U2 5
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD OCT
PY 2012
VL 9
IS 4
AR 18
DI 10.1145/2355598.2355601
PG 21
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 025DJ
UT WOS:000310164900003
DA 2024-07-18
ER

PT J
AU Stich, T
   Linz, C
   Wallraven, C
   Cunningham, D
   Magnor, M
AF Stich, Timo
   Linz, Christian
   Wallraven, Christian
   Cunningham, Douglas
   Magnor, Marcus
TI Perception-Motivated Interpolation of Image Sequences
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Algorithms; Experimentation; Perception; morphing; image interpolation
ID VIEW
AB We present a method for image interpolation that is able to create high-quality, perceptually convincing transitions between recorded images. By implementing concepts derived from human vision, the problem of a physically correct image interpolation is relaxed to that of image interpolation which is perceived as visually correct by human observers. We find that it suffices to focus on exact edge correspondences, homogeneous regions and coherent motion to compute convincing results. A user study confirms the visual quality of the proposed image interpolation approach. We show how each aspect of our approach increases perceived quality of the result. We compare the results to other methods and assess achievable quality for different types of scenes.
C1 [Stich, Timo; Linz, Christian; Magnor, Marcus] TU Braunschweig, Braunschweig, Germany.
   [Wallraven, Christian] Max Planck Inst Biol Cybernet, D-72076 Tubingen, Germany.
   [Cunningham, Douglas] Univ Tubingen, D-72074 Tubingen, Germany.
C3 Braunschweig University of Technology; Max Planck Society; Eberhard
   Karls University of Tubingen
RP Stich, T (corresponding author), TU Braunschweig, Braunschweig, Germany.
EM stich@cg.tu-bs.de
OI Cunningham, Douglas William/0000-0003-1419-2552; Wallraven,
   Christian/0000-0002-2604-9115; Magnor, Marcus/0000-0003-0579-480X
CR Adelson E.H., 1991, Computational Models of Visual Processing, P3
   [Anonymous], P EGWR
   [Anonymous], P SIGGRAPH
   [Anonymous], 2001, FRAGMENTS OBJECTS SE, DOI DOI 10.1016/S0166-4115(01)80038-8
   [Anonymous], 1983, VISION COMPUTATIONAL
   [Anonymous], 1981, IJCAI
   Baker S, 2004, INT J COMPUT VISION, V56, P221, DOI 10.1023/B:VISI.0000011205.11775.fd
   Baker Simon., 2007, ICCV
   BARRON JL, 1994, INT J COMPUT VISION, V12, P43, DOI 10.1007/BF01420984
   BEIER T, 1992, COMP GRAPH, V26, P35, DOI 10.1145/142920.134003
   Belongie S, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P454, DOI 10.1109/ICCV.2001.937552
   Bertsekas D., 1992, Computational optimization and applications, V1, P7
   Black MJ, 1996, COMPUT VIS IMAGE UND, V63, P75, DOI 10.1006/cviu.1996.0006
   Bouguet J-Y, 1999, Pyramidal implementation of the Lucas Kanade feature tracker
   BROX, 2004, HIGH ACCURACY OPTICA, P25
   Bruhn A, 2005, INT J COMPUT VISION, V61, P211, DOI 10.1023/B:VISI.0000045324.43199.43
   Buehler C, 2001, COMP GRAPH, P425, DOI 10.1145/383259.383309
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851
   CARRANZA J, 2003, P SIGGRAPH 03, P569
   CHEN SE, 1993, SIGGRAPH 93, P279
   Cooke T, 2004, LECT NOTES COMPUT SC, V3175, P407
   Cunningham DW, 1998, PERCEPTION, V27, P403, DOI 10.1068/p270403
   Exner S., 1894, ENTWURF PHYSL ERKLAR
   Felzenszwalb PF, 2004, INT J COMPUT VISION, V59, P167, DOI 10.1023/B:VISI.0000022288.19776.77
   Fu MF, 2002, 2002 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL III, PROCEEDINGS, P393, DOI 10.1109/ICIP.2002.1038988
   Gibson J. J., 1955, PERCEPTION VISUAL WO
   GIBSON JJ, 1969, PERCEPT PSYCHOPHYS, V5, P113, DOI 10.3758/BF03210533
   Gortler S. J., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P43, DOI 10.1145/237170.237200
   HARTLEY, 2000, MULTIPLE VIEW GEOMET
   Heeger DJ, 1999, J NEUROSCI, V19, P7162, DOI 10.1523/JNEUROSCI.19-16-07162.1999
   HORN BKP, 1981, ARTIF INTELL, V17, P185, DOI 10.1016/0004-3702(81)90024-2
   Hubel D. H., 1995, Eye, Brain, and Vision
   Isaksen A, 2000, COMP GRAPH, P297, DOI 10.1145/344779.344929
   JAIN JR, 1981, IEEE T COMMUN, V29, P1799, DOI 10.1109/TCOM.1981.1094950
   Kellman P.J., 1992, CURR DIR PSYCHOL SCI, V1, P193, DOI [10.1111/1467-8721.ep10770407, DOI 10.1111/1467-8721.EP10770407]
   Levoy M., 1996, P ACM SIGGRAPH
   Mark W. R., 1997, Proceedings 1997 Symposium on Interactive 3D Graphics, P7, DOI 10.1145/253284.253292
   Matusik W, 2004, ACM T GRAPHIC, V23, P814, DOI 10.1145/1015706.1015805
   Matusik W, 2000, COMP GRAPH, P369, DOI 10.1145/344779.344951
   McMillan L., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P39, DOI 10.1145/218380.218398
   *MICR CORP, MED PLAY 9 VID QUAL
   OSULLIVAN C, 2004, 6 EUR
   PERONA P, 1990, IEEE T PATTERN ANAL, V12, P629, DOI 10.1109/34.56205
   Qian N, 1997, VISION RES, V37, P1683, DOI 10.1016/S0042-6989(96)00164-2
   RAMANARAYANAN G, 2007, P ACM C COMP GRAPH S, P654
   RAMANARAYANAN G, 2008, P C COMP INT TECHN S, P1
   Reichardt Werner., 1961, AUTOCORRELATION PRIN, P303
   Ruzon M. A., 1999, Proceedings. 1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No PR00149), P160, DOI 10.1109/CVPR.1999.784624
   Schaefer S, 2006, ACM T GRAPHIC, V25, P533, DOI 10.1145/1141911.1141920
   SEITZ SM, 1996, SIGGRAPH, P21
   Snavely N, 2006, ACM T GRAPHIC, V25, P835, DOI 10.1145/1141911.1141964
   Stich T, 2008, APGV 2008: PROCEEDINGS OF THE SYMPOSIUM ON APPLIED PERCEPTION IN GRAPHICS AND VISUALIZATION, P97
   Stich T, 2008, COMPUT GRAPH FORUM, V27, P1781, DOI 10.1111/j.1467-8659.2008.01323.x
   Tourapis AM, 2001, 2001 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL III, PROCEEDINGS, P895, DOI 10.1109/ICIP.2001.958268
   VANGORP P, 2007, P ACM C COMP GRAPH S, P1
   Vangorp P, 2008, APGV 2008: PROCEEDINGS OF THE SYMPOSIUM ON APPLIED PERCEPTION IN GRAPHICS AND VISUALIZATION, P123
   VANSANTEN JPH, 1985, J OPT SOC AM A, V2, P300, DOI 10.1364/JOSAA.2.000300
   Vedula S, 2005, ACM T GRAPHIC, V24, P240, DOI 10.1145/1061347.1061351
   Wallach H, 1935, PSYCHOL FORSCH, V20, P325, DOI 10.1007/BF02409790
   Wallraven C, 2007, ACM T APPL PERCEPT, V4, DOI 10.1145/1278387.1278390
   Wertheimer M., 1938, SOURCE BOOK GESTALT, DOI [10.1037/11496-0053, DOI 10.1037/11496-0053, DOI 10.1037/11496-005]
   Wolberg G, 1998, VISUAL COMPUT, V14, P360, DOI 10.1007/s003710050148
   Wood DN, 2000, COMP GRAPH, P287, DOI 10.1145/344779.344925
   YUILLE AL, 1986, IEEE T PATTERN ANAL, V8, P15, DOI 10.1109/TPAMI.1986.4767748
   Zitnick CL, 2005, IEEE I CONF COMP VIS, P1308
NR 65
TC 25
Z9 27
U1 0
U2 5
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD JAN
PY 2011
VL 8
IS 2
AR 11
DI 10.1145/1870076.1870079
PG 25
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 748AR
UT WOS:000289362900003
DA 2024-07-18
ER

PT J
AU Ten Holt, GA
   Van Doorn, AJ
   Reinders, MJT
   Hendriks, EA
   De Ridder, H
AF Ten Holt, Gineke A.
   Van Doorn, Andrea J.
   Reinders, Marcel J. T.
   Hendriks, Emile A.
   De Ridder, Huib
TI Human-Inspired Search for Redundancy in Automatic Sign Language
   Recognition
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Algorithms; Experimentation; Performance; Sign language perception;
   automatic sign language recognition
AB Human perception of sign language can serve as inspiration for the improvement of automatic recognition systems. Experiments with human signers show that sign language signs contain redundancy over time. In this article, experiments are conducted to investigate whether comparable redundancies also exist for an automatic sign language recognition system. Such redundancies could be exploited, for example, by reserving more processing resources for the more informative phases of a sign, or by discarding uninformative phases. In the experiments, an automatic system is trained and tested on isolated fragments of sign language signs. The stimuli used were similar to those of the human signer experiments, allowing us to compare the results. The experiments show that redundancy over time exists for the automatic recognizer. The central phase of a sign is the most informative phase, and the first half of a sign is sufficient to achieve a recognition performance similar to that of the entire sign. These findings concur with the results of the human signer studies. However, there are differences as well, most notably the fact that human signers score better on the early phases of a sign than the automatic system. The results can be used to improve the automatic recognizer, by using only the most informative phases of a sign as input.
C1 [Ten Holt, Gineke A.] Delft Univ Technol, Fac EEMCS, ICT Grp, NL-2628 CD Delft, Netherlands.
C3 Delft University of Technology
RP Ten Holt, GA (corresponding author), Delft Univ Technol, Fac EEMCS, ICT Grp, Mekelweg 4, NL-2628 CD Delft, Netherlands.
OI de ridder, huib/0000-0002-7723-1785
CR [Anonymous], 1992, HAND MIND WHAT HANDS
   Arendsen J., 2007, Gesture, V7, P305
   Arendsen J, 2009, GESTURE, V9, P207, DOI 10.1075/gest.9.2.03are
   Bowden R, 2004, LECT NOTES COMPUT SC, V3021, P390
   Brentari Diane, 1998, A prosodic model of sign language phonology
   Campbell LW, 1996, PROCEEDINGS OF THE SECOND INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, P157, DOI 10.1109/AFGR.1996.557258
   CLARK LE, 1982, LANG SPEECH, V25, P325, DOI 10.1177/002383098202500402
   Derpanis KG, 2008, IMAGE VISION COMPUT, V26, P1650, DOI 10.1016/j.imavis.2008.04.007
   Ding LY, 2009, IMAGE VISION COMPUT, V27, P1826, DOI 10.1016/j.imavis.2009.02.005
   Duda, 2001, PATTERN CLASSIFICATI
   EMMOREY K, 1990, PERCEPT MOTOR SKILL, V71, P1227, DOI 10.2466/PMS.71.8.1227-1252
   Emmorey K., 2002, Language, Cognition, and the Brain: Insights from Sign Language Research
   GEBARENCENTRUM SN, 2002, STANDAARD LEXICON 1
   Grosjean F., 1981, Sign Language Studies, V32, P195, DOI DOI 10.1353/SLS.1982.0003
   Han JW, 2009, PATTERN RECOGN LETT, V30, P623, DOI 10.1016/j.patrec.2008.12.010
   Kita S., 1998, Gesture and Sign Language in Human-Computer Interaction. International Gesture Workshop Proceedings, P23, DOI 10.1007/BFb0052986
   Kruskal JosephB., 1983, TIME WARPS STRING ED, P125
   Lichtenauer JF, 2008, IEEE T PATTERN ANAL, V30, P2040, DOI 10.1109/TPAMI.2008.123
   LICHTENAUER JF, 2009, P 7 INT WORKSH GEST, P69
   LICHTENAUER JF, 2006, P 27 S INF THEOR BEN
   Sandler W, 2006, SIGN LANGUAGE AND LINGUISTIC UNIVERSALS, P1, DOI 10.2277/ 0521483956
   Spaai G., 2005, P INT C ED DEAF ICED
   Starner T, 1998, IEEE T PATTERN ANAL, V20, P1371, DOI 10.1109/34.735811
   Stokoe W., 1965, DICT AM SIGN LANGUAG
   STOKOE WC, 1960, STUD LING, V8
   TAX D, 2002, P INT C PATT REC
   ten Holt GA, 2009, SIGN LANG STUD, V9, P211, DOI 10.1353/sls.0.0012
   TENHOLT GA, 2010, GESTURE EMB IN PRESS
   TENHOLT GA, 2009, P C HUM VIS EL IM, V14
   VOGLER C, 2004, P 5 INT GEST WORKSH, P247
   Von Agris U., 2008, PROC 8 IEEE INT 825, P1
   Waldron M. B., 1995, IEEE Transactions on Rehabilitation Engineering, V3, P261, DOI 10.1109/86.413199
   Wilson AD, 1996, PROCEEDINGS OF THE SECOND INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, P66, DOI 10.1109/AFGR.1996.557245
   Zieren J, 2005, LECT NOTES COMPUT SC, V3522, P520
NR 34
TC 2
Z9 2
U1 0
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD JAN
PY 2011
VL 8
IS 2
AR 15
DI 10.1145/1870076.1870083
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 748AR
UT WOS:000289362900007
DA 2024-07-18
ER

PT J
AU Souman, JL
   Giordano, PR
   Frissen, I
   De Luca, A
   Ernst, MO
AF Souman, Jan L.
   Giordano, Paolo Robuffo
   Frissen, Ilja
   De Luca, Alessandro
   Ernst, Marc O.
TI Making Virtual Walking Real: Perceptual Evaluation of a New Treadmill
   Control Algorithm
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Experimentation; Human Factors; Measurement; Virtual reality; treadmill
   walking; motion control
ID LOCOMOTION; FEEDBACK
AB For us humans, walking is our most natural way of moving through the world. One of the major challenges in present research on navigation in virtual reality is to enable users to physically walk through virtual environments. Although treadmills, in principle, allow users to walk for extended periods of time through large virtual environments, existing setups largely fail to produce a truly immersive sense of navigation. Partially, this is because of inadequate control of treadmill speed as a function of walking behavior. Here, we present a new control algorithm that allows users to walk naturally on a treadmill, including starting to walk from standstill, stopping, and varying walking speed. The treadmill speed control consists of a feedback loop based on the measured user position relative to a given reference position, plus a feed-forward term based on online estimation of the user's walking velocity. The purpose of this design is to make the treadmill compensate fully for any persistent walker motion, while keeping the accelerations exerted on the user as low as possible.
   We evaluated the performance of the algorithm by conducting a behavioral experiment in which we varied its most important parameters. Participants walked at normal walking speed and then, on an auditory cue, abruptly stopped. After being brought back to the center of the treadmill by the control algorithm, they rated how smoothly the treadmill had changed its velocity in response to the change in walking speed. Ratings, in general, were quite high, indicating good control performance. Moreover, ratings clearly depended on the control algorithm parameters that were varied. Ratings were especially affected by the way the treadmill reversed its direction of motion. In conclusion, controlling treadmill speed in such a way that changes in treadmill speed are unobtrusive and do not disturb VR immersiveness is feasible on a normal treadmill with a straightforward control algorithm.
C1 [Souman, Jan L.] Max Planck Inst Biol Cybernet, Multisensory Percept & Act Grp, D-72076 Tubingen, Germany.
   [Giordano, Paolo Robuffo; De Luca, Alessandro] Univ Roma La Sapienza, DIS, Rome, Italy.
   [Frissen, Ilja] McGill Univ, Montreal, PQ, Canada.
C3 Max Planck Society; Sapienza University Rome; McGill University
RP Souman, JL (corresponding author), Max Planck Inst Biol Cybernet, Multisensory Percept & Act Grp, Spemannstr 41, D-72076 Tubingen, Germany.
EM jan.souman@tuebingen.mpg.de
RI De Luca, Alessandro/F-3835-2011; Giordano, Paolo Robuffo/F-5835-2011;
   Frissen, Ilja/HTQ-9850-2023
OI De Luca, Alessandro/0000-0002-0713-5608; Ernst,
   Marc/0000-0003-4197-8569; Frissen, Ilja/0000-0001-8658-7874; Souman, Jan
   L./0000-0003-3027-1090
FU European research project Cyberwalk [FP6-511092]
FX The research described in this article was funded by the European
   research project Cyberwalk (Contract FP6-511092; see
   http://www.cyberwalk-project.org). A short video showing the behavior of
   the control system can be found at
   http://www.cyberwalkproject.org/img/Media/LTMctrl.mpg.
CR Cathers I, 2005, J PHYSIOL-LONDON, V563, P229, DOI 10.1113/jphysiol.2004.079525
   Checcacci D, 2003, P EUROHAPTICS C, P53
   Christensen RR, 2000, PRESENCE-VIRTUAL AUG, V9, P1, DOI 10.1162/105474600566574
   Darken R. P., 1997, Proceedings of the ACM Symposium on User Interface Software and Technology. 10th Annual Symposium. UIST '97, P213, DOI 10.1145/263407.263550
   De Luca A, 2007, IEEE INT CONF ROBOT, P2330, DOI 10.1109/ROBOT.2007.363667
   De Luca A, 2006, IEEE INT CONF ROBOT, P3532, DOI 10.1109/ROBOT.2006.1642241
   DELUCA A, 2006, P IEEE S ROB CONTR
   Durgin FH, 2005, J EXP PSYCHOL HUMAN, V31, P398, DOI 10.1037/0096-1523.31.3.398
   Fernandes KJ, 2003, COMMUN ACM, V46, P141, DOI 10.1145/903893.903929
   Fitzpatrick RC, 2006, CURR BIOL, V16, P1509, DOI 10.1016/j.cub.2006.05.063
   Hollerbach J.M., 2000, HAPTICS S P ASME DYN, P1293
   Huang JY, 2003, IEEE T MULTIMEDIA, V5, P39, DOI 10.1109/TMM.2003.808822
   Iwata H, 1999, P IEEE VIRT REAL ANN, P286, DOI 10.1109/VR.1999.756964
   Jahn K, 2000, NEUROREPORT, V11, P1745, DOI 10.1097/00001756-200006050-00029
   Krsti M., 1995, NONLINEAR ADAPTIVE C
   Lichtenstein L, 2007, ACM T APPL PERCEPT, V4, DOI 10.1145/1227134.1227141
   Minetti AE, 2003, J APPL PHYSIOL, V95, P838, DOI 10.1152/japplphysiol.00128.2003
   Mittelstaedt ML, 2001, EXP BRAIN RES, V139, P318, DOI 10.1007/s002210100735
   MOGHADDAM M, 1993, P IEEE RSJ INT C INT, P63
   Mohler BJ, 2007, ACM T APPL PERCEPT, V4, DOI [10.1145/1227134.1227138, 10.1145/1227134/1227138]
   NAGAMORI A, 2005, IEEE VIRTUAL REALITY, P3
   Noma H, 2003, P IEEE VIRT REAL ANN, P217, DOI 10.1109/VR.2003.1191142
   Noma H., 1998, Proceedings of the ASME Dynamic Systems and Control Division-1998, P111
   Panteley E, 1998, SYST CONTROL LETT, V33, P131, DOI 10.1016/S0167-6911(97)00119-9
   RIESER JJ, 1995, J EXP PSYCHOL HUMAN, V21, P480, DOI 10.1037/0096-1523.21.3.480
NR 25
TC 45
Z9 52
U1 1
U2 20
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD FEB
PY 2010
VL 7
IS 2
AR 11
DI 10.1145/1670671.1670675
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 563HF
UT WOS:000275118100004
DA 2024-07-18
ER

PT J
AU Van Mensvoort, K
   Vos, P
   Hermes, DJ
   Van Liere, R
AF Van Mensvoort, Koert
   Vos, Peter
   Hermes, Dik J.
   Van Liere, Robert
TI Perception of Mechanically and Optically Simulated Bumps and Holes
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Design; Experimentation; Human Factors; Optically simulated haptic
   feedback; simulation; force feedback; experimentation; multisensory
   perception
ID FORCE FEEDBACK; TOUCH; VISION; CONFLICT; WEIGHT
AB In this article, we investigate the perception of optically simulated haptic feedback. The perception of optically and mechanically simulated bumps and holes was tested experimentally. In an earlier article, we have described the active cursor technique, a method to simulate haptic feedback optically without resorting to special mechanical force feedback devices, commonly applied to produce haptic percepts in computer interfaces. The operation of the force feedback device is substituted by tiny displacements on the cursor position relative to the intended force. This method exploits the domination of the visual over the haptic modality. Results show that people can recognize optically simulated bump and hole structures and that active cursor displacements influence the haptic perception of bumps and holes. Depending on the simulated strength of the force, optically simulated haptic feedback can take precedence over mechanically simulated haptic feedback and also the other way around. When optically simulated and mechanically simulated haptic feedback counteract each other, however, the weight attributed to each source of haptic information differs from user to user. It is concluded that active cursor displacements can be used to simulate the operation of mechanical force feedback devices.
C1 [Van Mensvoort, Koert] Eindhoven Univ Technol, Ind Design Dept, NL-5600 MB Eindhoven, Netherlands.
C3 Eindhoven University of Technology
RP Van Mensvoort, K (corresponding author), Eindhoven Univ Technol, Ind Design Dept, POB 513, NL-5600 MB Eindhoven, Netherlands.
EM k.m.v.mensvoort@tue.nl
FU Eindhoven University of Technology
FX This research was supported by the Eindhoven University of Technology.
CR AHLSTROM D, 2002, P DSVIS 2002, P185
   AHLSTROM D, 2006, P 4 NORD C HUM COMP, P58
   AHLSTROM D, 2005, P SIGCHI C HUM FACT
   AKAMATSU M, 1994, INT J HUM-COMPUT ST, V40, P443, DOI 10.1006/ijhc.1994.1020
   AKAMATSU M, 1994, PRESENCE, V3, P73
   [Anonymous], 1998, P SIGCHI C HUMAN FAC
   [Anonymous], P ACM C HUM FACT COM
   [Anonymous], 1891, Archiv Physiol Norm Pathol
   [Anonymous], 1996, FORCE TOUCH FEEDBACK
   Balakrishnan R, 2004, INT J HUM-COMPUT ST, V61, P857, DOI 10.1016/j.ijhcs.2004.09.002
   Bejczy A. K., 1990, Proceedings 1990 IEEE International Conference on Robotics and Automation (Cat. No.90CH2876-1), P546, DOI 10.1109/ROBOT.1990.126037
   BEVAN M, 1995, VR NEWS VIRTUAL REAL, V4, P23
   Blanch R., 2004, P SIGCHI C HUMAN FAC, P519, DOI DOI 10.1145/985692.985758
   Brooks F. P.  Jr., 1990, Computer Graphics, V24, P177, DOI 10.1145/97880.97899
   BURDEA G, 1993, J ROBOTICS MECHATRON, V5
   Carr K., 1995, SIMULATED VIRTUAL RE
   ENGEL FL, 1994, INT J HUM-COMPUT ST, V41, P949, DOI 10.1006/ijhc.1994.1087
   Engelbart Doug., 1988, AUGMENTED KNOWLEDGE, P185, DOI DOI 10.1145/61975.66918
   ENGLISH WK, 1967, IEEE TRANS HUM FACT, VHFE8, P5, DOI 10.1109/THFE.1967.232994
   Ernst MO, 2002, NATURE, V415, P429, DOI 10.1038/415429a
   Flanagan JR, 2001, NATURE, V412, P389, DOI 10.1038/35086674
   FLOURNEY T, 1894, ANN PSYCHOL, V1, P1989
   Gibson J. J., 1966, The ecological approach to visual perception
   Heller MA, 1999, PERCEPT PSYCHOPHYS, V61, P1384, DOI 10.3758/BF03206188
   Horvitz Eric, 1999, P SIGCHI C HUM FACT, P159
   KERSTNER W, 1994, P INT C COMP HAND PE
   Keyson DV, 1997, ERGONOMICS, V40, P1287, DOI 10.1080/001401397187379
   KLATZKY RL, 1987, J EXP PSYCHOL GEN, V116, P356
   Lecuyer A., 2004, P SIGCHI C HUM FACT, P239, DOI DOI 10.1145/985692.985723
   LECUYER A, 2000, P IEEE INT C VIRT RE, P239
   Lécuyer A, 2008, ACM T APPL PERCEPT, V5, DOI 10.1145/1402236.1402238
   LEDERMAN SJ, 1986, J EXP PSYCHOL HUMAN, V12, P169, DOI 10.1037/0096-1523.12.2.169
   Marks L., 1978, UNITY SENSES
   Massaro D.W., 1990, RELATIONSHIPS PERCEP, P133, DOI DOI 10.1007/978-3-642-75348-0_6
   MINER N, 1996, P IMAGE C
   Murray DJ, 1999, PERCEPT PSYCHOPHYS, V61, P1681, DOI 10.3758/BF03213127
   Park J, 2006, INT J IND ERGONOM, V36, P721, DOI 10.1016/j.ergon.2006.05.004
   Prinz W., 1984, Cognition and Motor Processes, P185, DOI DOI 10.1007/978-3-642-69382-3_13
   Prinz W., 2005, AGENCY SELF AWARENES, P165
   Rizzolatti G, 2004, ANNU REV NEUROSCI, V27, P169, DOI 10.1146/annurev.neuro.27.070203.144230
   Robles-De-La-Torre G, 2001, NATURE, V412, P445, DOI 10.1038/35086588
   ROCK I, 1964, SCIENCE, V143, P594, DOI 10.1126/science.143.3606.594
   RUNESON S, 1981, J EXP PSYCHOL HUMAN, V7, P733, DOI 10.1037/0096-1523.7.4.733
   SHNEIDERMAN B, 1981, P JOINT C EAS MOR PR, P143
   Stein E.B., 1993, The Merging of Senses
   SUZUKI Y, 1986, OUTRUN CLASSIC ARCAD
   Thomas, 1986, HDB PERCEPTION HUMAN
   van Mensvoort K, 2008, INT J HUM-COMPUT ST, V66, P438, DOI 10.1016/j.ijhcs.2007.12.004
   VANMENSVOORT K, 2002, P DES INT SYST 2002, P345
   VANMENSVOORT K, 2007, POWERCURSOR TOOLKIT
   Worden Aileen., 1997, P SIGCHI C HUMAN FAC, P266, DOI DOI 10.1145/258549.258724
NR 51
TC 4
Z9 4
U1 0
U2 5
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD FEB
PY 2010
VL 7
IS 2
AR 10
DI 10.1145/1670671.1670674
PG 24
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA 563HF
UT WOS:000275118100003
DA 2024-07-18
ER

PT J
AU Frintrop, S
   Rome, E
   Christensen, HI
AF Frintrop, Simone
   Rome, Erich
   Christensen, Henrik I.
TI Computational Visual Attention Systems and Their Cognitive Foundations:
   A Survey
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Review
DE Algorithms; Design; Visual attention; saliency; regions of interest;
   biologically motivated computer vision; robot vision
ID SELECTIVE ATTENTION; EYE-MOVEMENTS; NEURAL MECHANISMS;
   PATTERN-RECOGNITION; CONNECTIONIST MODEL; CORTICAL MECHANISMS; OBJECT
   RECOGNITION; TARGET SELECTION; GUIDED SEARCH; REPRESENTATION
AB Based on concepts of the human visual system, computational visual attention systems aim to detect regions of interest in images. Psychologists, neurobiologists, and computer scientists have investigated visual attention thoroughly during the last decades and profited considerably from each other. However, the interdisciplinarity of the topic holds not only benefits but also difficulties: Concepts of other fields are usually hard to access due to differences in vocabulary and lack of knowledge of the relevant literature. This article aims to bridge this gap and bring together concepts and ideas from the different research areas. It provides an extensive survey of the grounding psychological and biological research on visual attention as well as the current state of the art of computational systems. Furthermore, it presents a broad range of applications of computational attention systems in fields like computer vision, cognitive systems, and mobile robotics. We conclude with a discussion on the limitations and open questions in the field.
C1 [Frintrop, Simone] Univ Bonn, Inst Comp Sci 3, D-53117 Bonn, Germany.
   [Rome, Erich] IAIS, Fraunhofer Inst Intelligent Anal & Informat Syst, D-53757 Schloss Birlinghoven, Sankt Augustin, Germany.
   [Christensen, Henrik I.] Georgia Tech, Coll Comp, Atlanta, GA 30308 USA.
C3 University of Bonn; Fraunhofer Gesellschaft; University System of
   Georgia; Georgia Institute of Technology
RP Frintrop, S (corresponding author), Univ Bonn, Inst Comp Sci 3, Romerstr 164, D-53117 Bonn, Germany.
EM frintrop@iai.uni-bonn.de; erich.rome@iais.fraunhofer.de;
   hic@cc.gatech.edu
RI Christensen, Henrik I/A-2261-2009
OI Christensen, Henrik Iskov/0000-0002-7465-7502
CR Abdi Herve., 2007, ENCY MEASUREMENT STA, P1
   ALOIMONOS J, 1987, INT J COMPUT VISION, V1, P333
   [Anonymous], THESIS CALTECH PASAD
   [Anonymous], 2008, UCBEECS20088
   [Anonymous], P 5 INT C COMP VIS S
   [Anonymous], IEEE INT S IND EL
   [Anonymous], P INT C INT ROB SYST
   [Anonymous], HDB COMPUTER VISION
   [Anonymous], 2003, ATTENTION THEORY PRA
   [Anonymous], 1993, A vision of the brain
   ARISTOTLE, SENSE SENSIBLE INTER
   Awh E, 2000, J EXP PSYCHOL HUMAN, V26, P834, DOI 10.1037/0096-1523.26.2.834
   AZIZ MZ, 2007, P ICVS WORKSH COMP A
   Backer G, 2001, IEEE T PATTERN ANAL, V23, P1415, DOI 10.1109/34.977565
   BACKER G, 2004, THESIS U HAMBURG GER
   BACON WF, 1994, PERCEPT PSYCHOPHYS, V55, P485, DOI 10.3758/BF03205306
   Baddeley RJ, 2006, VISION RES, V46, P2824, DOI 10.1016/j.visres.2006.02.024
   Balkenius C., 2000, COGNITIVE SCI Q, V1, P171
   Baluja S, 1997, ROBOT AUTON SYST, V22, P329, DOI 10.1016/S0921-8890(97)00046-8
   Belardinelli A., 2008, THESIS SAPIENZA U RO
   Ben-Shahar O, 2007, VISION RES, V47, P845, DOI 10.1016/j.visres.2006.10.019
   Bichot NP, 2005, SCIENCE, V308, P529, DOI 10.1126/science.1109676
   BICHOT NP, 2001, VISION ATTENTION
   Bisley JW, 2003, SCIENCE, V299, P81, DOI 10.1126/science.1077395
   Björkman M, 2006, INT J IMAG SYST TECH, V16, P189, DOI 10.1002/ima.20087
   Bollmann M., 1999, Computer Vision Systems. First International Conference, ICVS'99. Proceedings, P392
   BOLLMANN M, 1999, THESIS U HAMBURG GER
   BORJI A, 2009, THESIS SCS TEHRAN
   Breazeal C, 1999, IJCAI-99: PROCEEDINGS OF THE SIXTEENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, VOLS 1 & 2, P1146
   Bruce N, 2005, P CAN C COMP ROB VIS
   Bruce N., 2005, P NEUR INF PROC SYST
   Bundesen C, 1998, PHILOS T ROY SOC B, V353, P1271, DOI 10.1098/rstb.1998.0282
   BUNDESEN C, 1990, PSYCHOL REV, V97, P523, DOI 10.1037/0033-295X.97.4.523
   Bundesen C., 2005, Handbook of Cognition
   BUR A, 2007, P SPIE C HUM VIS EL
   CAMERON E, 2004, SPATIAL VISION, V17, P4
   CARRASCO M, 1995, PERCEPT PSYCHOPHYS, V57, P1241, DOI 10.3758/BF03208380
   Cassin B., 1990, DICT EYE TERMINOLOGY
   Cave KR, 1999, PSYCHOL RES-PSYCH FO, V62, P182, DOI 10.1007/s004260050050
   CAVE KR, 1990, COGNITIVE PSYCHOL, V22, P225, DOI 10.1016/0010-0285(90)90017-X
   CHERRY EC, 1953, J ACOUST SOC AM, V25, P975, DOI 10.1121/1.1907229
   CHOI SB, 2004, NEURAL INFORM PROCES, V2
   Chun MM, 1998, COGNITIVE PSYCHOL, V36, P28, DOI 10.1006/cogp.1998.0681
   CLARK JJ, 1989, PROCEEDINGS - 1989 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION, VOL 1-3, P826, DOI 10.1109/ROBOT.1989.100085
   CLARK JJ, 1988, P 2 INT C COMP VIS I
   CLARK JJ, 1992, INTRO ACTIVE VISION
   Connor CE, 2004, CURR BIOL, V14, pR850, DOI 10.1016/j.cub.2004.09.041
   Corbetta M, 1998, P NATL ACAD SCI USA, V95, P831, DOI 10.1073/pnas.95.3.831
   Corbetta M, 2002, NAT REV NEUROSCI, V3, P201, DOI 10.1038/nrn755
   Dankers A., 2007, P 5 INT C COMP VIS S
   DESIMONE R, 1995, ANNU REV NEUROSCI, V18, P193, DOI 10.1146/annurev-psych-122414-033400
   Deubel H, 1996, VISION RES, V36, P1827, DOI 10.1016/0042-6989(95)00294-4
   Draper BA, 2005, COMPUT VIS IMAGE UND, V100, P152, DOI 10.1016/j.cviu.2004.08.006
   Driver J, 1998, ATTENTIVE BRAIN, P299
   DUNCAN J, 1984, J EXP PSYCHOL GEN, V113, P501, DOI 10.1037/0096-3445.113.4.501
   Eckstein MP, 2000, PERCEPT PSYCHOPHYS, V62, P425, DOI 10.3758/BF03212096
   Egeth HE, 1997, ANNU REV PSYCHOL, V48, P269, DOI 10.1146/annurev.psych.48.1.269
   Einhäuser W, 2008, J VISION, V8, DOI 10.1167/8.2.2
   Elazary L, 2008, J VISION, V8, DOI 10.1167/8.3.3
   ERIKSEN CW, 1986, PERCEPT PSYCHOPHYS, V40, P225, DOI 10.3758/BF03211502
   Findlay JM, 1999, BEHAV BRAIN SCI, V22, P661, DOI 10.1017/S0140525X99002150
   Findlay John M., 2001, P83
   Fink GR, 1997, BRAIN, V120, P2013, DOI 10.1093/brain/120.11.2013
   Fleming KA, 2006, 2006 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, VOLS 1-12, P241, DOI 10.1109/IROS.2006.281688
   Fragopanagos N, 2006, NEUROCOMPUTING, V69, P1977, DOI 10.1016/j.neucom.2005.11.016
   Fraundorfer F., 2003, P INT WORKSH ATT PER, P17
   Frey HP, 2008, J VISION, V8, DOI 10.1167/8.14.6
   Frintrop S, 2005, COMPUT VIS IMAGE UND, V100, P124, DOI 10.1016/j.cviu.2004.08.005
   FRINTROP S, 2008, P WORKSH EFF STRAT C
   FRINTROP S, 2005, P ANN M GERM ASS PAT
   FRINTROP S, 2008, IEEE T ROB, V24
   FRINTROP S, 2010, INT J SO ROB
   FRINTROP S, 2007, P EUR C MOB ROB
   FRINTROP S, 2005, LECT NOTES ARTIFICIA, V3899
   FRINTROP S, 2004, P IROS 2004, P2167
   FRINTROP S, 2009, P IEEE INT C ROB AUT
   FRITZ G, 2004, P 2 INT WORKSH ATT P, P136
   Fritz JB, 2007, CURR OPIN NEUROBIOL, V17, P437, DOI 10.1016/j.conb.2007.07.011
   Garey M.R., 1979, COMPUTERS INTRACTABI
   Gegenfurtner KR, 2003, NAT REV NEUROSCI, V4, P563, DOI 10.1038/nrn1138
   Ghazanfar AA, 2006, TRENDS COGN SCI, V10, P278, DOI 10.1016/j.tics.2006.04.008
   Giesbrecht B, 2003, NEUROIMAGE, V19, P496, DOI 10.1016/S1053-8119(03)00162-9
   Gottlieb JP, 1998, NATURE, V391, P481, DOI 10.1038/35135
   Gray W., 2007, INTEGRATED MODELS CO
   Green D., 1966, SIGNAL DETECTION THE
   Hamker FH, 2005, COMPUT VIS IMAGE UND, V100, P64, DOI 10.1016/j.cviu.2004.09.005
   Hamker FH, 2006, BIOSYSTEMS, V86, P91, DOI 10.1016/j.biosystems.2006.03.010
   Harel J, 2006, Advances in Neural Information Processing Systems, V19
   Heidemann G, 2004, MACH VISION APPL, V16, P64, DOI 10.1007/s00138-004-0157-2
   Heinke D, 2003, PSYCHOL REV, V110, P29, DOI 10.1037//0033-295X.110.1.29
   Heinke D, 2005, STUD COGN, P273
   Henderson John M., 2007, P537, DOI 10.1016/B978-008044980-7/50027-6
   Horowitz TS, 2003, VIS COGN, V10, P257, DOI 10.1080/13506280143000005
   HUMPHREYS GW, 1993, COGNITIVE PSYCHOL, V25, P43, DOI 10.1006/cogp.1993.1002
   Itti L, 2005, VIS COGN, V12, P1093, DOI 10.1080/13506280444000661
   Itti L, 2004, IEEE T IMAGE PROCESS, V13, P1304, DOI 10.1109/TIP.2004.834657
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Itti L, 2001, NAT REV NEUROSCI, V2, P194, DOI 10.1038/35058500
   Itti L, 2001, J ELECTRON IMAGING, V10, P161, DOI 10.1117/1.1333677
   Itti L., 2002, P SPIE C HUM VIS EL
   Itti L., 2003, P SPIE 48 ANN INT S
   Itti L, 2009, VISION RES, V49, P1295, DOI 10.1016/j.visres.2008.09.007
   Johansson RS, 2001, J NEUROSCI, V21, P6917, DOI 10.1523/JNEUROSCI.21-17-06917.2001
   Jonides J., 1981, ATTENTION PERFORM, P187, DOI DOI 10.1037/0096-1523.29.5.835
   Kadir T, 2001, INT J COMPUT VISION, V45, P83, DOI 10.1023/A:1012460413855
   KAHNEMAN D, 1992, COGNITIVE PSYCHOL, V24, P175, DOI 10.1016/0010-0285(92)90007-O
   Kandel E.R., 1996, ESSENTIALS NEURAL SC
   Kastner S, 2001, NEUROPSYCHOLOGIA, V39, P1263, DOI 10.1016/S0028-3932(01)00116-6
   KOCH C, 1985, HUM NEUROBIOL, V4, P219
   Kootstra G., 2008, P BRIT MACH VIS C
   Kunar MA, 2008, PERCEPT PSYCHOPHYS, V70, P314, DOI 10.3758/PP.70.2.314
   Land MF, 2006, PROG RETIN EYE RES, V25, P296, DOI 10.1016/j.preteyeres.2006.01.002
   Lee K.W., 2003, INT WORKSHOP ATTENTI, P55
   Levin DT, 1996, J EXP PSYCHOL LEARN, V22, P1364, DOI 10.1037/0278-7393.22.6.1364
   Li Z., 2005, NEUROBIOLOGY ATTENTI
   LIU T, 2003, CEREBRAL CORTEX, V13
   LIVINGSTONE MS, 1987, J NEUROSCI, V7, P3416
   Logan GD, 1996, PSYCHOL REV, V103, P603, DOI 10.1037/0033-295X.103.4.603
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Maki A, 2000, COMPUT VIS IMAGE UND, V78, P351, DOI 10.1006/cviu.2000.0840
   Marr D., 1982, Visual perception
   MAUNSELL JHR, 1995, SCIENCE, V270, P764, DOI 10.1126/science.270.5237.764
   MAY S, 2007, P INT C INT ROB SYST, P3385
   Mazer JA, 2003, NEURON, V40, P1241, DOI 10.1016/S0896-6273(03)00764-5
   McMains SA, 2004, NEURON, V42, P677, DOI 10.1016/S0896-6273(04)00263-6
   Miau F, 2001, PROC SPIE, V4479, P12, DOI 10.1117/12.448343
   MILANESE R, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P781, DOI 10.1109/CVPR.1994.323898
   Milanese R., 1993, THESIS U GENEVA SWIT
   Mitri S, 2005, IEEE INT CONF ROBOT, P125
   MOZER MC, 1987, ATTENTION PERFORM, P83
   Muhl C., 2007, P 30 GERM C ART INT
   NAGAI Y, 2009, P 8 INT IEEE C DEV L
   NAKAYAMA K, 1986, NATURE, V320, P264, DOI 10.1038/320264a0
   Navalpakkam V, 2005, VISION RES, V45, P205, DOI 10.1016/j.visres.2004.07.042
   NAVALPAKKAM V, 2006, P C COMP VIS PATT RE
   NAVALPAKKAM V, 2004, J VISION, V4, P690
   Navalpakkam V, 2006, J VISION, V6, P1180, DOI 10.1167/6.11.4
   Neisser U, 1967, COGNITIVE PSYCHOL
   Nickerson SB, 1998, ROBOT AUTON SYST, V25, P83, DOI 10.1016/S0921-8890(98)00032-3
   Nothdurft Hans-Christoph, 2005, P233, DOI 10.1016/B978-012375731-9/50042-2
   Ogawa T, 2004, J NEUROSCI, V24, P6371, DOI 10.1523/JNEUROSCI.0569-04.2004
   Oliva A, 2003, IEEE IMAGE PROC, P253, DOI 10.1109/icip.2003.1246946
   Oliva Aude, 2005, P251, DOI 10.1016/B978-012375731-9/50045-8
   Olshausen B A., 2006, 23 Problems in Systems Neuroscience, Vvol 23, ppp 182
   Olshausen BA, 2005, NEURAL COMPUT, V17, P1665, DOI 10.1162/0899766054026639
   OLSHAUSEN BA, 1993, J NEUROSCI, V13, P4700
   Ouerhani N, 2000, INT C PATT RECOG, P375, DOI 10.1109/ICPR.2000.905356
   OUERHANI N, 2006, P INT COGN VIS WORKS
   OUERHANI N, 2005, P EUR C MOB ROB ECMR
   Ouerhani N., 2004, ELECT LETT COMPUTER, V3, P13, DOI [10.5565/rev/elcvia.66, DOI 10.5565/REV/ELCVIA.66]
   Ouerhani N, 2003, THESIS U NEUCHATEL S
   PALMER J, 1993, J EXP PSYCHOL HUMAN, V19, P108, DOI 10.1037/0096-1523.19.1.108
   Palmer S., 1999, VISION SCI PHOTONS P
   Park RM, 2002, AM J IND MED, V42, P1, DOI 10.1002/ajim.10082
   Pashler H., 1997, PSYCHOL ATTENTION
   PESSOA L, 1999, P IWANN 1999, P850
   Peters RJ, 2005, VISION RES, V45, P2397, DOI 10.1016/j.visres.2005.03.019
   Peters RJ, 2008, ACM T APPL PERCEPT, V5, DOI 10.1145/1279920.1279923
   PHAF RH, 1990, COGNITIVE PSYCHOL, V22, P273, DOI 10.1016/0010-0285(90)90006-P
   POSNER MI, 1980, Q J EXP PSYCHOL, V32, P3, DOI 10.1080/00335558008248231
   POSNER MI, 1990, ANNU REV NEUROSCI, V13, P25, DOI 10.1146/annurev.ne.13.030190.000325
   POSNER MI, 1984, ATTENTION PERFORM, V10, P531
   Postma E., 1994, THESIS RIJKSUNIVERSI
   PYLYSHYN Z W, 1988, Spatial Vision, V3, P179, DOI 10.1163/156856888X00122
   Pylyshyn Z.W., 2003, Seeing and Visualizing: It's Not What You Think
   RAE R, 2000, THESIS U BIELEFELD G
   RAMSTROM O, 2004, P INT WORKSH ATT PER, P9
   RAMSTROM O, 2002, P WORKSH BIOL MOT CO
   Rao RPN, 2002, VISION RES, V42, P1447, DOI 10.1016/S0042-6989(02)00040-8
   RASOLZADEH B, 2009, INT J ROB RES
   Rauschenberger R, 2003, PSYCHON B REV, V10, P814, DOI 10.3758/BF03196545
   Rensink RA, 2000, VIS COGN, V7, P17, DOI 10.1080/135062800394667
   Rensink RA, 1997, PSYCHOL SCI, V8, P368, DOI 10.1111/j.1467-9280.1997.tb00427.x
   Riesenhuber M, 1999, NAT NEUROSCI, V2, P1019, DOI 10.1038/14819
   Rosenholtz R, 2001, PERCEPT PSYCHOPHYS, V63, P476, DOI 10.3758/BF03194414
   Rotenstein A, 2007, P 2 INT C TECHN AG
   Rothenstein AL, 2006, P INT C ART NEUR NET
   Rothenstein AL, 2008, IMAGE VISION COMPUT, V26, P114, DOI 10.1016/j.imavis.2005.08.011
   Rybak IA, 1998, VISION RES, V38, P2387, DOI 10.1016/S0042-6989(98)00020-0
   Sabra A.I., 1989, OPTICS IBN AL HAYTHA
   Salah AA, 2002, IEEE T PATTERN ANAL, V24, P420, DOI 10.1109/34.990146
   SANDINI G, 2002, SENSORS SENSING BIOL
   SCHAUERTE B, 2009, P MULT INT WORKSH MA
   Scholl BJ, 2001, COGNITION, V80, P1, DOI 10.1016/S0010-0277(00)00152-9
   SHULMAN GL, 1979, J EXP PSYCHOL HUMAN, V5, P522, DOI 10.1037/0096-1523.5.3.522
   Siagian C, 2009, IEEE T ROBOT, V25, P861, DOI 10.1109/TRO.2009.2022424
   Simons DJ, 1997, TRENDS COGN SCI, V1, P261, DOI 10.1016/S1364-6613(97)01080-2
   Styles E.A., 1997, PSYCHOL ATTENTION, DOI 10.4324/9780203016435
   Sumner P, 2000, J EXP BIOL, V203, P1963
   Sun YR, 2003, ARTIF INTELL, V146, P77, DOI 10.1016/S0004-3702(02)00399-5
   Tatler BW, 2006, VISION RES, V46, P1857, DOI 10.1016/j.visres.2005.12.005
   Tatler BW, 2007, J VISION, V7, DOI 10.1167/7.14.4
   Tatler BW, 2005, VISION RES, V45, P643, DOI 10.1016/j.visres.2004.09.017
   Theeuwes J, 2004, PSYCHON B REV, V11, P65, DOI 10.3758/BF03206462
   Torralba A, 2003, J OPT SOC AM A, V20, P1407, DOI 10.1364/JOSAA.20.001407
   Torralba A, 2003, INT J COMPUT VISION, V53, P169, DOI 10.1023/A:1023052124951
   TREISMAN A, 1988, PSYCHOL REV, V95, P15, DOI 10.1037/0033-295X.95.1.15
   Treisman A., 1993, Attention: Selection, awareness, and control, P5
   TREISMAN AM, 1980, COGNITIVE PSYCHOL, V12, P97, DOI 10.1016/0010-0285(80)90005-5
   Tsotsos J.K., 1987, P INT C COMP VIS HUM
   TSOTSOS JK, 1995, ARTIF INTELL, V78, P507, DOI 10.1016/0004-3702(95)00025-9
   Tsotsos JK, 2005, COMPUT VIS IMAGE UND, V100, P3, DOI 10.1016/j.cviu.2004.10.011
   TSOTSOS JK, 1990, BEHAV BRAIN SCI, V13, P423, DOI 10.1017/S0140525X00079577
   TSOTSOS JK, 1993, SPATIAL VISION IN HUMANS AND ROBOTS, P313
   Tsotsos JK, 1998, IMAGE VISION COMPUT, V16, P275, DOI 10.1016/S0262-8856(97)00088-7
   Tsotsos JK, 2008, BRAIN RES, V1225, P119, DOI 10.1016/j.brainres.2008.05.038
   Tuytelaars T, 2007, FOUND TRENDS COMPUT, V3, P177, DOI 10.1561/0600000017
   VANOEFFELEN MP, 1982, MEM COGNITION, V10, P396, DOI 10.3758/BF03202432
   VECERA SP, 1994, J EXP PSYCHOL GEN, V123, P146, DOI 10.1037/0096-3445.123.2.146
   Verghese P, 2001, NEURON, V31, P523, DOI 10.1016/S0896-6273(01)00392-0
   Vickery TJ, 2005, J VISION, V5, P81, DOI 10.1167/5.1.8
   Vijayakumar S, 2001, IROS 2001: PROCEEDINGS OF THE 2001 IEEE/RJS INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, VOLS 1-4, P2332, DOI 10.1109/IROS.2001.976418
   Vincent BT, 2007, VISION RES, V47, P1809, DOI 10.1016/j.visres.2007.02.014
   Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb
   von Helmholtz H., 1896, HDB PHYSIOLOGISCHEN, V2nd
   WALTHER D, 2004, P INT C COMP VIS PAT
   Walther DB, 2007, PROG BRAIN RES, V165, P57, DOI 10.1016/S0079-6123(06)65005-X
   Wells A., 1994, ATTENTION EMOTION CL
   Wolfe J., 2001, J VISION, V1, p349a
   Wolfe J.M., 1996, GUIDED SEARCH 30 BAS, P189
   Wolfe Jeremy M, 2010, Curr Biol, V20, pR346, DOI 10.1016/j.cub.2010.02.016
   WOLFE JM, 1989, J EXP PSYCHOL HUMAN, V15, P419, DOI 10.1037/0096-1523.15.3.419
   Wolfe JM, 2004, VISION RES, V44, P1411, DOI 10.1016/j.visres.2003.11.024
   WOLFE JM, 1994, PSYCHON B REV, V1, P202, DOI 10.3758/BF03200774
   Wolfe JM, 2004, NAT REV NEUROSCI, V5, P495, DOI 10.1038/nrn1411
   Wolfe JM, 1998, PSYCHOL SCI, V9, P33, DOI 10.1111/1467-9280.00006
   Wolfe JM, 2001, PERCEPT PSYCHOPHYS, V63, P381, DOI 10.3758/BF03194406
   XU T, 2009, P INT C ROB AUT IEEE
   Yantis S, 2003, CURR OPIN NEUROBIOL, V13, P187, DOI 10.1016/S0959-4388(03)00033-3
   Yantis S, 2002, NAT NEUROSCI, V5, P995, DOI 10.1038/nn921
   YANTIS S, 2000, ATTENTION PERFORMANC, V18
   Yarbus A. L., 1967, Eye Movements and Vision
   Zelinsky GJ, 1997, J EXP PSYCHOL HUMAN, V23, P244, DOI 10.1037/0096-1523.23.1.244
NR 233
TC 282
Z9 320
U1 2
U2 111
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD JAN
PY 2010
VL 7
IS 1
AR 6
DI 10.1145/1658349.1658355
PG 39
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA 549EK
UT WOS:000274028400006
DA 2024-07-18
ER

PT J
AU Lu, AD
   Maciejewski, R
   Ebert, DS
AF Lu, Aidong
   Maciejewski, Ross
   Ebert, David S.
TI Volume Composition and Evaluation Using Eye-Tracking Data
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Algorithms; Experimentation; Usability and human factors in
   visualization; eye tracker; interaction; illustrative visualization;
   volume rendering
ID DESIGN
AB This article presents a method for automating rendering parameter selection to simplify tedious user interaction and improve the usability of visualization systems. Our approach acquires the important/interesting regions of a dataset through simple user interaction with an eye tracker. Based on this importance information, we automatically compute reasonable rendering parameters using a set of heuristic rules, which are adapted from visualization experience and psychophysical experiments. A user study has been conducted to evaluate these rendering parameters, and while the parameter selections for a specific visualization result are subjective, our approach provides good preliminary results for general users while allowing additional control adjustment. Furthermore, our system improves the interactivity of a visualization system by significantly reducing the required amount of parameter selections and providing good initial rendering parameters for newly acquired datasets of similar types.
C1 [Lu, Aidong] Univ N Carolina, Charlotte, NC 28223 USA.
   [Maciejewski, Ross; Ebert, David S.] Purdue Univ, W Lafayette, IN 47907 USA.
C3 University of North Carolina; University of North Carolina Charlotte;
   Purdue University System; Purdue University
RP Lu, AD (corresponding author), Univ N Carolina, Charlotte, NC 28223 USA.
EM Aidong.lu@uncc.edu
OI Ebert, David/0000-0001-6177-1296
FU DOE [DE-FG02-06ER25733]; NSF [0081581, 0121288, 0328984, 0633150];
   Direct For Computer & Info Scie & Enginr; Division of Computing and
   Communication Foundations [0328984] Funding Source: National Science
   Foundation; Division of Computing and Communication Foundations; Direct
   For Computer & Info Scie & Enginr [0121288, 0081581] Funding Source:
   National Science Foundation; Division Of Undergraduate Education; Direct
   For Education and Human Resources [0633150] Funding Source: National
   Science Foundation
FX This research is supported by DOE DE-FG02-06ER25733, NSF 0081581,
   0121288, 0328984, and 0633150.
CR Agrawala M, 2003, ACM T GRAPHIC, V22, P828, DOI 10.1145/882262.882352
   [Anonymous], P SIGGRAPH
   [Anonymous], 1987, The Retina: An Approachable Part of the Brain
   [Anonymous], P SIGCHI C HUM FACT
   [Anonymous], P EUR 2003 COMP GRAP
   [Anonymous], 2000, P SIGCHI C HUM FACT, DOI DOI 10.1145/332040.332443
   BEDNARIK R, 2006, P 18 ANN PSYCH PROGR, P68
   Bergman LD, 1995, VISUALIZATION '95 - PROCEEDINGS, P118, DOI 10.1109/VISUAL.1995.480803
   BESHERS C, 1993, IEEE COMPUT GRAPH, V13, P41, DOI 10.1109/38.219450
   Blanz V, 1999, PERCEPTION, V28, P575, DOI 10.1068/p2897
   Bordoloi UD, 2005, IEEE VISUALIZATION 2005, PROCEEDINGS, P487
   Chung A.J., 2004, P S EYE TRACK RES AP, P49
   Cole F., 2006, EUROGRAPHICS S RENDE, P377
   Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236
   DENTON T, 2004, P IEEE C COMP VIS PA
   Egenhofer M. J., 1990, Proceedings of the 4th International Symposium on Spatial Data Handling, P803
   Fairchild M.D., 1998, COLOR APPEARANCE MOD
   Georgescu B, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P456
   Gooch B, 2001, SPRING EUROGRAP, P83
   Gooch B., 2001, Non-photorealistic rendering
   HAMEL J, 1999, P EUR IEEE LOS AL CA
   HE LW, 1996, P 23 ANN C COMP GRAP, P217
   Healey CG, 1996, IEEE VISUAL, P263, DOI 10.1109/VISUAL.1996.568118
   HELBING R, 1998, P EUR C ART INT
   Jerald J., 2002, Proceedings ETRA 2002. Eye Tracking Research and Applications Symposium, P77, DOI 10.1145/507072.507088
   JI G, 2006, P IEEE C VIS IEEE LO
   KAWAI JK, 1993, P SIGGRAPH C ACM NEW
   Kim S, 2004, IEEE T VIS COMPUT GR, V10, P471, DOI 10.1109/TVCG.2004.5
   Kindlmann G, 2002, VIS 2002: IEEE VISUALIZATION 2002, PROCEEDINGS, P299, DOI 10.1109/VISUAL.2002.1183788
   Kindlmann G, 1998, IEEE SYMPOSIUM ON VOLUME VISUALIZATION, P79, DOI 10.1109/SVV.1998.729588
   Kowalski MichaelA., 2001, Proceedings of the 2001 symposium on Interactive 3D graphics, P99
   KRULL R, 2003, P INT PROF COMM C IE
   Laeng B, 2001, LATERALITY, V6, P193, DOI 10.1080/13576500042000115
   Law B., 2004, P 2004 S EYE TRACKIN, P41, DOI [10.1145/968363.968370, DOI 10.1145/968363.968370]
   Levoy M., 1990, Computer Graphics, V24, P217, DOI 10.1145/91394.91449
   Livio M., 2008, The Golden Ratio: The Story of Phi, the World's Most Astonishing Number
   Lu AD, 2003, IEEE T VIS COMPUT GR, V9, P127, DOI 10.1109/TVCG.2003.1196001
   MACKINLAY J, 1986, ACM T GRAPHIC, V5, P110, DOI 10.1145/22949.22950
   Majaranta P, 2002, P EYE TRACK RES APPL
   McGuffin MJ, 2003, IEEE VISUALIZATION 2003, PROCEEDINGS, P401, DOI 10.1109/VISUAL.2003.1250400
   Ohshima T, 1996, P IEEE VIRT REAL ANN, P103, DOI 10.1109/VRAIS.1996.490517
   OSULLIVAN C, 2001, ACM T GRAPH, V20
   Press W. H., 2002, Numerical Recipes in C: the Art of Scientific Computing, V2nd ed., DOI DOI 10.1119/1.14981
   RAYNER K, 2004, P EYE TRACK RES APPL, P9
   Reddy M, 2001, IEEE COMPUT GRAPH, V21, P68, DOI 10.1109/38.946633
   Rheingans P., 1990, Computer Graphics, V24, P145, DOI 10.1145/91394.91436
   Rheingans P., 1995, PERCEPTUAL ISSUES VI, P59, DOI [10.1007/978-3-642-79057-7_6, DOI 10.1007/978-3-642-79057-7_6]
   Rist T., 1994, Proceedings of the Workshop on Advanced Visual Interfaces AVI '94, P59, DOI 10.1145/192309.192326
   Santella A., 2004, P NPAR, P71, DOI [DOI 10.1145/987657.987669, 10.1145/987657.987669]
   Sekuler R., 1994, PERCEPTION, V3rd
   SELIGMAN DD, 1991, P SIGGRAPH ACM NEW Y
   STROTHOTTE T, 1994, COMPUT GRAPH FORUM, V13, pC455, DOI 10.1111/1467-8659.1330455
   Strothotte T, 2002, NONPHOTOREALISTIC CO
   Sullivan CO, 2003, MIND'S EYE: COGNITIVE AND APPLIED ASPECTS OF EYE MOVEMENT RESEARCH, P555
   Svakhine N, 2005, IEEE COMPUT GRAPH, V25, P31, DOI 10.1109/MCG.2005.60
   TAKAHASHI S, 2005, P IEEE C VIS IEEE LO
   TORY M, 2005, P IEEE C VIS IEEE LO
   Vazquez P.-P., 2001, Proceedings of Vision Modeling and Visualization Conference, P273
   Viola I, 2004, IEEE VISUALIZATION 2004, PROCEEEDINGS, P139, DOI 10.1109/VISUAL.2004.48
   Viola I, 2006, IEEE T VIS COMPUT GR, V12, P933, DOI 10.1109/TVCG.2006.152
   ZLATANOVA S, 2002, P INT S EXH GEOINF
   Zwicker M, 2001, IEEE VISUAL, P29, DOI 10.1109/VISUAL.2001.964490
NR 62
TC 5
Z9 5
U1 0
U2 9
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD JAN
PY 2010
VL 7
IS 1
AR 4
DI 10.1145/1658349.1658353
PG 20
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 549EK
UT WOS:000274028400004
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Yu, IS
   Cox, A
   Kim, MH
   Ritschel, T
   Grosch, T
   Dachsbacher, C
   Kautz, J
AF Yu, Insu
   Cox, Andrew
   Kim, Min H.
   Ritschel, Tobias
   Grosch, Thorsten
   Dachsbacher, Carsten
   Kautz, Jan
TI Perceptual Influence of Approximate Visibility in Indirect Illumination
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Algorithms; Global illumination; perception; visibility
ID PRECOMPUTED RADIANCE TRANSFER; GLOBAL ILLUMINATION
AB In this article we evaluate the use of approximate visibility for efficient global illumination. Traditionally, accurate visibility is used in light transport. However, the indirect illumination we perceive on a daily basis is rarely of high-frequency nature, as the most significant aspect of light transport in real-world scenes is diffuse, and thus displays a smooth gradation. This raises the question of whether accurate visibility is perceptually necessary in this case. To answer this question, we conduct a psychophysical study on the perceptual influence of approximate visibility on indirect illumination. This study reveals that accurate visibility is not required and that certain approximations may be introduced.
C1 [Yu, Insu; Cox, Andrew; Kim, Min H.; Kautz, Jan] UCL, Dept Comp Sci, London WC1E 6BT, England.
   [Ritschel, Tobias; Grosch, Thorsten] MPI Informat, Dept Comp Graph, D-66123 Saarbrucken, Germany.
   [Dachsbacher, Carsten] Univ Stuttgart, Visualizat Res Ctr VISUS, D-7000 Stuttgart, Germany.
C3 University of London; University College London; Max Planck Society;
   University of Stuttgart
RP Kautz, J (corresponding author), UCL, Dept Comp Sci, Gower St, London WC1E 6BT, England.
EM j.kautz@cs.ucl.ac.uk
RI Cox, Andrew/J-9568-2016; Kim, Min H./G-7969-2012
OI Kim, Min H./0000-0002-5078-4005
FU Technology Strategy Board [Q2047E]; Geomerics Ltd.; EPSRC [EP/E047343/1]
   Funding Source: UKRI
FX This project was supported by the Technology Strategy Board (Q2047E) and
   by Geomerics Ltd. (use of their assets).
CR AKENINEMOELLER T, 2004, ACM SIGGRAPH COURSE
   Annen T, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360633
   [Anonymous], 1958, Theory and Methods of Scaling
   [Anonymous], 2006, Advanced Global Illumination
   [Anonymous], 2009, P S INT 3D GRAPH GAM, DOI 10.1145/1507149.1507161.5,7
   Arikan O, 2005, ACM T GRAPHIC, V24, P1108, DOI 10.1145/1073204.1073319
   ARVO J, 1994, P ACM SIGGRAPH, P75
   BARTLESON CJ, 1984, OPTICAL RAD MEASUREM, V5, pCH8
   Bunnell Michael., 2005, GPU GEMS, V2, P223
   Christensen PH, 2003, COMPUT GRAPH FORUM, V22, P543, DOI 10.1111/1467-8659.t01-1-00702
   Cohen-Or D, 2003, IEEE T VIS COMPUT GR, V9, P412, DOI 10.1109/TVCG.2003.1207447
   Dachsbacher C., 2005, Proc. Symp. Interactive Graph. and Games, P203, DOI DOI 10.1145/1053427.1053460
   Dachsbacher C., 2006, Proc. Symp. Interactive 3D Graph. and Games, Redwood City, P93, DOI DOI 10.1145/1111411.1111428
   Dachsbacher C, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239512
   DEBATTISTA K, 2005, P GRAPH
   Dong Z, 2007, PACIFIC GRAPHICS 2007: 15TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, P77, DOI 10.1109/PG.2007.37
   DRETTAKIS G., 2007, P EUR S REND, P297
   Durand F, 2005, ACM T GRAPHIC, V24, P1115, DOI 10.1145/1073204.1073320
   Fernando Randima., 2005, SIGGRAPH 05, P35
   Ferwerda J. A., 1997, Proc. ACM SIGGRAPH, P143
   Green P., 2007, PROC EGSR 2007, P495
   Guilford J.P., 1954, Journal of Educational Psychology, Vsecond
   Iwasaki Kei, 2007, Pro- ceedings of the Eurographics Symposium on Rendering, P35
   KAJIYA JT, 1986, COMPUT GRAPH, V20, P143
   Keller Alexander., 1997, SIGGRAPH 97 P 24 ANN, P49
   Kozlowski O, 2007, APGV 2007: SYMPOSIUM ON APPLIED PERCEPTION IN GRAPHICS AND VISUALIZATION, PROCEEDINGS, P91
   Meilgaard M., 1999, SENSORY EVALUATION T
   Myszkowski K, 2001, COMP GRAPH, P221, DOI 10.1145/383259.383284
   Pan M, 2007, COMPUT GRAPH FORUM, V26, P485, DOI 10.1111/j.1467-8659.2007.01071.x
   Ritschel T., 2008, ACM T GRAPHIC, V27, P1
   Ritschel Tobias., 2008, Proceedings of Graphics Interface, P185
   Rushmeier H., 1993, Proceedings Graphics Interface '93, P227
   SCHEFFE H, 1952, J AM STAT ASSOC, V47, P381, DOI 10.2307/2281310
   SILLION FX, 1995, IEEE T VIS COMPUT GR, V1, P240, DOI 10.1109/2945.466719
   SILLION FX, 1995, P SIGGRAPH 95, P145
   Sloan PP, 2007, PACIFIC GRAPHICS 2007: 15TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, P97, DOI 10.1109/PG.2007.28
   Sloan PP, 2002, ACM T GRAPHIC, V21, P527, DOI 10.1145/566570.566612
   Stokes WA, 2004, ACM T GRAPHIC, V23, P742, DOI 10.1145/1015706.1015795
   Tabellion E, 2004, ACM T GRAPHIC, V23, P469, DOI 10.1145/1015706.1015748
   Thurstone LL, 1927, PSYCHOL REV, V34, P273, DOI 10.1037/h0070288
   Vangorp P, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1276377.1276473, 10.1145/1239451.1239528]
   Volevich V, 2000, ACM T GRAPHIC, V19, P122, DOI 10.1145/343593.343611
   Wald I., 2003, Eurographics Symposium on Rendering. 14th Eurographics Workshop on Rendering, P74
   Walter B, 2005, ACM T GRAPHIC, V24, P1098, DOI 10.1145/1073204.1073318
   Watson A.B., 1993, DIGITAL IMAGES HUMAN, P179
   ZHUKOV S, 1998, P EUR WORKSH REND TE, P45
NR 46
TC 18
Z9 20
U1 0
U2 6
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD SEP
PY 2009
VL 6
IS 4
AR 24
DI 10.1145/1609967.1609971
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 511ZV
UT WOS:000271212300004
DA 2024-07-18
ER

PT J
AU Willemsen, P
   Colton, MB
   Creem-Regehr, SH
   Thompson, WB
AF Willemsen, Peter
   Colton, Mark B.
   Creem-Regehr, Sarah H.
   Thompson, William B.
TI The Effects of Head-Mounted Display Mechanical Properties and Field of
   View on Distance Judgments in Virtual Environments
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Human factors; Perception; distance judgments; head-mounted displays
ID EGOCENTRIC DISTANCE; VISUAL-PERCEPTION; GUIDED LOCOMOTION; REAL
AB Research has shown that people are able to judge distances accurately in full-cue, real-world environments using visually directed actions. However, in virtual environments viewed with head-mounted display (HMD) systems, there is evidence that people act as though the virtual space is smaller than intended. This is a surprising result given how well people act in real environments. The behavior in the virtual setting may be linked to distortions in the available visual cues or to a person's ability to locomote without vision. Either could result from issues related to added mass, moments of inertia, and restricted field of view in HMDs. This article describes an experiment in which distance judgments based on normal real-world and HMD viewing are compared with judgments based on real-world viewing while wearing two specialized devices. One is a mock HMD, which replicated the mass, moments of inertia, and field of view of the HMD and the other an inertial headband designed to replicate the mass and moments of inertia of the HMD, but constructed to not restrict the field of view of the observer or otherwise feel like wearing a helmet. Distance judgments using the mock HMD showed a statistically significant underestimation relative to the no restriction condition but not of a magnitude sufficient to account for all the distance compression seen in the HMD. Indicated distances with the inertial headband were not significantly smaller than those made with no restrictions.
C1 [Creem-Regehr, Sarah H.] Univ Utah, Dept Psychol, Salt Lake City, UT 84112 USA.
   [Thompson, William B.] Univ Utah, Sch Comp, Salt Lake City, UT 84112 USA.
C3 Utah System of Higher Education; University of Utah; Utah System of
   Higher Education; University of Utah
RP Willemsen, P (corresponding author), Univ Minnesota, Dept Comp Sci, 331 HH,1114 Kirby Dr, Duluth, MN 55812 USA.
EM willemsn@d.umn.edu; colton@byu.edu; sarah.creem@psych.utah.edu;
   thompson@cs.utah.edu
FU National Science Foundation [0080999, 0121084]; Div Of Information &
   Intelligent Systems; Direct For Computer & Info Scie & Enginr [0080999,
   0121084] Funding Source: National Science Foundation
FX This material is based on work supported by the National Science
   Foundation under grants 0080999 and 0121084.
CR BEALL AC, 1995, P SOC PHOTO-OPT INS, V2411, P288, DOI 10.1117/12.207547
   Creem-Regehr SH, 2005, PERCEPTION, V34, P191, DOI 10.1068/p5144
   Cutting J., 1995, PERCEPTION SPACE MOT, P69
   Ellis SR, 1997, PRESENCE-TELEOP VIRT, V6, P452, DOI 10.1162/pres.1997.6.4.452
   Fukusima SS, 1997, J EXP PSYCHOL HUMAN, V23, P86, DOI 10.1037/0096-1523.23.1.86
   Klein E., 2006, PROC 3 S APPL PERCEP, P147
   Knapp JM, 2004, PRESENCE-TELEOP VIRT, V13, P572, DOI 10.1162/1054746042545238
   Loomis JM, 2003, VIRTUAL AND ADAPTIVE ENVIRONMENTS: APPLICATIONS, IMPLICATIONS, AND HUMAN PERFORMANCE ISSUES, P21
   LOOMIS JM, 1992, J EXP PSYCHOL HUMAN, V18, P906, DOI 10.1037/0096-1523.18.4.906
   Ooi TL, 2001, NATURE, V414, P197, DOI 10.1038/35102562
   Philbeck JW, 1997, PERCEPT PSYCHOPHYS, V59, P601, DOI 10.3758/BF03211868
   Plumert Jodie., 2005, ACM Transactions on Applied Perception, V2, P216, DOI DOI 10.1145/1077399.1077402
   Richardson AR, 2007, HUM FACTORS, V49, P507, DOI 10.1518/001872007X200139
   RIESER JJ, 1990, PERCEPTION, V19, P675, DOI 10.1068/p190675
   RYU J, 2005, P INT C CYB
   Sahm C. S., 2005, ACM Trans. Appl. Percept. (TAP), V2, P35, DOI DOI 10.1145/1048687.1048690
   Sedgwick H.A., 1986, HDB PERCEPTION PERFO, p21
   Surdick RT, 1997, PRESENCE-TELEOP VIRT, V6, P513, DOI 10.1162/pres.1997.6.5.513
   Thompson E., 2004, J MENS STUDIES, V13, P5, DOI [DOI 10.3149/JMS.1301.5, https://doi.org/10.3149/jms.1301.5, 10.3149/jms.1301.5]
   THOMSON JA, 1983, J EXP PSYCHOL HUMAN, V9, P427, DOI 10.1037/0096-1523.9.3.427
   Willett WC, 2008, ASIA PAC J CLIN NUTR, V17, P1
   Witmer BG, 1998, HUM FACTORS, V40, P478, DOI 10.1518/001872098779591340
   Witmer BG, 1998, PRESENCE-TELEOP VIRT, V7, P144, DOI 10.1162/105474698565640
   Wu B, 2004, NATURE, V428, P73, DOI 10.1038/nature02350
NR 24
TC 108
Z9 122
U1 0
U2 13
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD FEB
PY 2009
VL 6
IS 2
AR 8
DI 10.1145/1498700.1498702
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA 450YN
UT WOS:000266438000002
DA 2024-07-18
ER

PT J
AU Elhelw, M
   Nicolaou, M
   Chung, A
   Yang, GZ
   Atkins, MS
AF Elhelw, Mohamed
   Nicolaou, Marios
   Chung, Adrian
   Yang, Guang-Zhong
   Atkins, M. Stella
TI A Gaze-Based Study for Investigating the Perception of Visual Realism in
   Simulated Scenes
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Experimentation; Measurement; Performance; Visual perception; visual
   realism; photorealistic rendering; eye tracking; human-computer
   interaction; simulation environment
ID TONE REPRODUCTION; EFFICIENT; SEARCH; REFLECTANCE; INFORMATION;
   ATTENTION
AB Visual realism has been a major objective of computer graphics since the inception of the field. However, the perception of visual realism is not a well-understood process and is usually attributed to a combination of visual cues and image features that are difficult to define or measure. For highly complex images, the problem is even more involved. The purpose of this paper is to present a study based on eye tracking for investigating the perception of visual realism of static images with different visual qualities. The eye-fixation clusters helped to define salient image features corresponding to 3D surface details and light transfer properties that attract observers' attention. This enabled the definition and categorization of image attributes affecting the perception of photorealism. The dynamics of the visual behavior of different observer groups were examined by analyzing saccadic eye movements. We also demonstrated how the different image categories used in the experiments were perceived with varying degrees of visual realism. The results presented can be used as a basis for investigating the impact of individual image features on the perception of visual realism. This study suggests that post-recall or simple abstraction of visual experience is not accurate and the use of eye tracking provides an effective way of determining relevant features that affect visual realism, thus allowing for improved rendering techniques that target these features.
C1 [Elhelw, Mohamed; Nicolaou, Marios; Chung, Adrian; Yang, Guang-Zhong] Univ London Imperial Coll Sci Technol & Med, Dept Comp, Visual Informat Proc Grp, London SW7 2BZ, England.
   [Atkins, M. Stella] Simon Fraser Univ, Sch Comp Sci, Burnaby, BC V5A 1S6, Canada.
C3 Imperial College London; Simon Fraser University
RP Elhelw, M (corresponding author), Univ London Imperial Coll Sci Technol & Med, Dept Comp, Visual Informat Proc Grp, 180 Queens Gate, London SW7 2BZ, England.
EM me@doc.ic.ac.uk; g.z.yang@imperial.ac.uk
RI Yang, Guangzhong/ABB-7316-2021; Nicolaou, Marios/IUO-6702-2023
OI Yang, Guangzhong/0000-0002-7289-5806; Nicolaou,
   Marios/0000-0003-3874-8244
FU EPSRC [DT/E011101/1] Funding Source: UKRI
CR Aldridge R., 1995, Fifth International Conference on Image Processing and its Applications (Conf. Publ. No.410), P336, DOI 10.1049/cp:19950676
   [Anonymous], 1994, Graph. Gems, DOI DOI 10.1016/B978-0-12-336156-1.50054-9
   [Anonymous], 1996, P 23 ANN C COMP GRAP, DOI DOI 10.1145/237170.237262
   ATKINS MS, 2005, ACM T APPL PERCEPT, V3, P136
   Bertin J., 1983, SEMIOLOGY GRAPHICS D
   BERTRAND M, 2000, DO PEOPLE MEAN UNPUB
   Blinn J., 1978, Proceedings of the 5th annual conference on Computer graphics and interactive techniques-SIGGRAPH'78, V12, P286
   Bolin M. R., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P409, DOI 10.1145/218380.218497
   BOLIN MR, 1999, SPIE HUMAN VISION EL, V4, P106
   Bruce V., 1996, Visual Perception, physiology, psychology and ecology
   Cater K., 2003, Eurographics Symposium on Rendering. 14th Eurographics Workshop on Rendering, P270
   CATMULL E, 1974, THESIS U UTAH SALT L
   CHIU K, 1994, PHOTOREALISTIC RENDE, P21
   CHUNG AJ, 2004, LECT NOTES COMPUTER
   Cook R. L., 1982, ACM T GRAPHIC, V1, P7, DOI DOI 10.1145/357290.357293
   *CORN U PROGR COMP, 2006, RES FRAM REAL IM SYN
   Dana KJ, 1999, ACM T GRAPHIC, V18, P1, DOI 10.1145/300776.300778
   Dempere-Marco L, 2002, IEEE T MED IMAGING, V21, P741, DOI 10.1109/TMI.2002.801153
   DEMPEREMARCO L, 2004, THESIS U LONDON LOND
   Duchowski AT, 2002, BEHAV RES METH INS C, V34, P455, DOI 10.3758/BF03195475
   DUMONT O, 2005, 417 TR KATH U LEUV D
   Ebert D.S., 1998, Texturing Modeling A Procedural Approach, VSecond
   ElHelw MA, 2004, LECT NOTES COMPUT SC, V3150, P346
   FERNANDO R, 2003, CG TUTORIAL
   FOLEY J, 2004, COMPUTER GRAPHICS GR
   Freeman J, 1999, PRESENCE-TELEOP VIRT, V8, P1, DOI 10.1162/105474699566017
   Goral C. M., 1984, Computers & Graphics, V18, P213
   Gortler S. J., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P43, DOI 10.1145/237170.237200
   Greenberg DP, 1999, COMMUN ACM, V42, P44, DOI 10.1145/310930.310970
   HACISALIHZADE SS, 1992, IEEE T SYST MAN CYB, V22, P474, DOI 10.1109/21.155948
   Hanrahan P., 1993, P ACM SIGGRAPH, P165
   HE XD, 1991, P 18 ANN C COMP GRAP, P175
   HECKBERT PS, 1994, GRAPH INTER, P43
   Henderson J. M., 2004, INTERFACE LANGUAGE V, DOI DOI 10.4324/9780203488430
   Henderson J.M., 1998, EYE GUIDANCE READING
   Henderson JM, 1999, ANNU REV PSYCHOL, V50, P243, DOI 10.1146/annurev.psych.50.1.243
   Howlett S., 2004, Proceedings of the 1st Symposium on Applied perception in graphics and visualization, APGV'04, P57
   Hu XP, 2003, IEEE T MED IMAGING, V22, P1152, DOI 10.1109/TMI.2003.816959
   IJsselsteijn WA, 2000, PROC SPIE, V3959, P520, DOI 10.1117/12.387188
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   KUNDEL HL, 1978, INVEST RADIOL, V13, P175, DOI 10.1097/00004424-197805000-00001
   Kundel HL, 2004, PROC SPIE, V5372, P1, DOI 10.1117/12.542717
   Larson GW, 1997, IEEE T VIS COMPUT GR, V3, P291, DOI 10.1109/2945.646233
   Law B, 2003, STUD HEALTH TECHNOL, V94, P184
   Lawrence J, 2004, ACM T GRAPHIC, V23, P496, DOI 10.1145/1015706.1015751
   Levoy M., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P31, DOI 10.1145/237170.237199
   Li B, 1998, P SOC PHOTO-OPT INS, V3299, P98, DOI 10.1117/12.320101
   Liu A, 2003, PRESENCE-VIRTUAL AUG, V12, P599, DOI 10.1162/105474603322955905
   Longhurst P, 2004, THEORY AND PRACTICE OF COMPUTER GRAPHICS 2004, PROCEEDINGS, P196, DOI 10.1109/TPCG.2004.1314471
   Loschky LC, 2000, ETRA '00, P97, DOI DOI 10.1145/355017.355032
   Lubin J., 1995, Vision Models for Target Detection and Recognition, P245
   Mack Arien, 1998, Inattentional Blindness
   Matusik W, 2003, ACM T GRAPHIC, V22, P759, DOI 10.1145/882262.882343
   McCool MD, 2001, COMP GRAPH, P171, DOI 10.1145/383259.383276
   McMillan L, 1997, IMAGE BASED APPROACH
   McNamara A, 2001, COMPUT GRAPH FORUM, V20, P211, DOI 10.1111/1467-8659.00550
   MCNAMARA A, 2000, P EUR WORKSH REND TE, P207
   MITCHELL D, 1987, P SIGGRAPH 87, P65
   Murphy Hunter A., 2001, Eurographics (short presentations)
   MYSZKOWSKI K, 1998, EUR WORKSH REND, P223
   Nicolaou M, 2004, LECT NOTES COMPUT SC, V3217, P97
   Nodine CF, 1987, EYE MOVEMENTS PSYCHO, P572
   Ohshima T, 1996, P IEEE VIRT REAL ANN, P103, DOI 10.1109/VRAIS.1996.490517
   Ost D, 2001, AM J RESP CRIT CARE, V164, P2248, DOI 10.1164/ajrccm.164.12.2102087
   Perlin K., 1985, Computer Graphics, V19, P287, DOI 10.1145/325165.325247
   Pomplun M, 2001, COGNITION, V81, pB57, DOI 10.1016/S0010-0277(01)00123-8
   Rayner K, 1998, PSYCHOL BULL, V124, P372, DOI 10.1037/0033-2909.124.3.372
   Rensink RA, 2000, VISION RES, V40, P1469, DOI 10.1016/S0042-6989(00)00003-1
   Rushmeier H, 1995, SPRING COMP SCI, P82
   SIDAK Z, 1999, PROBABILITY MATH STA
   Sillion F., 1989, Computer Graphics, V23, P335, DOI 10.1145/74334.74368
   SRADEMACHER P, 2001, P 12 EUR WORKSH REND, P235
   Stokes WA, 2004, ACM T GRAPHIC, V23, P742, DOI 10.1145/1015706.1015795
   Sullivan CO, 2003, MIND'S EYE: COGNITIVE AND APPLIED ASPECTS OF EYE MOVEMENT RESEARCH, P555
   SUNDSTEDT V, 2005, SPRING C COMP GRAPH, P162
   *TOB, 2006, TOB TECHN US MAN
   TUMBLIN J, 1993, IEEE COMPUT GRAPH, V13, P42, DOI 10.1109/38.252554
   Velichkovsky B.M., 2000, Proceedings of the 2000 symposium on eye tracking research and applications, P79, DOI 10.1145/355017.355029
   WALLACE JR, 1987, P SIGGRAPH 87, P311
   WARD GJ, 1992, COMP GRAPH, V26, P265, DOI 10.1145/142920.134078
   Watson A.B., 1993, DIGITAL IMAGES HUMAN, P179
   Watt A., 1992, ADV ANIMATION RENDER
   WHITAKER JW, 1980, PHYSIOLOGIST, V23, P6
   Yang GZ, 2002, IMAGE VISION COMPUT, V20, P291, DOI 10.1016/S0262-8856(02)00022-7
   Yarbus A.L., 1967, EYE MOVEMENTS VISION, DOI [10.1007/978-1-4899-5379-7, DOI 10.1007/978-1-4899-5379-7]
   Yee H, 2001, ACM T GRAPHIC, V20, P39, DOI 10.1145/383745.383748
NR 86
TC 27
Z9 31
U1 0
U2 8
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD JAN
PY 2008
VL 5
IS 1
AR 3
DI 10.1145/1279640.1279643
PG 20
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 450YI
UT WOS:000266437500003
DA 2024-07-18
ER

PT J
AU Parde, CJ
   Strehle, VE
   Banerjee, V
   Hu, Y
   Cavazos, JG
   Castillo, CD
   O'Toole, AJ
AF Parde, Connor J.
   Strehle, Virginia E.
   Banerjee, Vivekjyoti
   Hu, Ying
   Cavazos, Jacqueline G.
   Castillo, Carlos D.
   O'Toole, Alice J.
TI Twin Identification over Viewpoint Change: A Deep Convolutional Neural
   Network Surpasses Humans
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Face recognition; deep convolutional neural network; human face
   recognition; human-machine comparison
ID FACE RECOGNITION; REPRESENTATION; FEATURES
AB Deep convolutional neural networks (DCNNs) have achieved human-level accuracy in face identification (Phillips et al., 2018), though it is unclear how accurately they discriminate highly-similar faces. Here, humans and a DCNN performed a challenging face-identity matching task that included identical twins. Participants (N = 87) viewed pairs of face images of three types: same-identity, general imposters (different identities from similar demographic groups), and twin imposters (identical twin siblings). The task was to determine whether the pairs showed the same person or different people. Identity comparisons were tested in three viewpoint-disparity conditions: frontal to frontal, frontal to 45 degrees profile, and frontal to 90 degrees profile. Accuracy for discriminating matched-identity pairs from twin-imposter pairs and general-imposter pairs was assessed in each viewpoint-disparity condition. Humans were more accurate for general-imposter pairs than twin-imposter pairs, and accuracy declined with increased viewpoint disparity between the images in a pair. A DCNN trained for face identification (Ranjan et al., 2018) was tested on the same image pairs presented to humans. Machine performance mirrored the pattern of human accuracy, but with performance at or above all humans in all but one condition. Human and machine similarity scores were compared across all image-pair types. This item-level analysis showed that human and machine similarity ratings correlated significantly in six of nine image-pair types [range r = 0.38 to r = 0.63], suggesting general accord between the perception of face similarity by humans and the DCNN. These findings also contribute to our understanding of DCNN performance for discriminating high-resemblance faces, demonstrate that the DCNN performs at a level at or above humans, and suggest a degree of parity between the features used by humans and the DCNN.
C1 [Parde, Connor J.; Strehle, Virginia E.; Hu, Ying; O'Toole, Alice J.] Univ Texas Dallas, Sch Behav & Brain Sci, 800 Campbell Rd,Mail Stop GR41, Richardson, TX 75080 USA.
   [Banerjee, Vivekjyoti] Univ Maryland, Inst Adv Comp Studies, 8125 Paint Branch Dr, College Pk, MD 20740 USA.
   [Cavazos, Jacqueline G.] Univ Calif Irvine, Sch Educ, 401 Peltason Dr,Suite 3200, Irvine, CA 92617 USA.
   [Castillo, Carlos D.] Johns Hopkins Univ, Whiting Sch Engn, 3400 N Charles St, Baltimore, MD 21218 USA.
C3 University of Texas System; University of Texas Dallas; University
   System of Maryland; University of Maryland College Park; University of
   California System; University of California Irvine; Johns Hopkins
   University
RP Parde, CJ (corresponding author), Univ Texas Dallas, Sch Behav & Brain Sci, 800 Campbell Rd,Mail Stop GR41, Richardson, TX 75080 USA.
EM connor.parde@utdallas.edu; ginni.strehle@utdallas.edu;
   vivekjyoti24@gmail.com; ying.hu@utdallas.edu;
   jacqueline.cavazos@uci.edu; carlosdc@jhu.edu; otoole@utdallas.edu
OI Cavazos, Jacqueline/0000-0001-6593-0813; O'Toole,
   Alice/0000-0001-7981-1508
FU National Eye Institute [R01EY029692-04]
FX Funding provided by National Eye Institute Grant R01EY029692-04 to
   A.O.T. and C.D.C.
CR Abudarham N, 2019, COGNITION, V182, P73, DOI 10.1016/j.cognition.2018.09.002
   Afaneh A, 2017, EURASIP J IMAGE VIDE, DOI 10.1186/s13640-017-0231-0
   Ahmad B, 2019, IEEE GLOBE WORK, DOI 10.1109/gcwkshps45667.2019.9024704
   [Anonymous], 2010, P IEEE COMP SOC C CO
   Bansal A, 2017, IEEE INT CONF COMP V, P2545, DOI 10.1109/ICCVW.2017.299
   Biswas Soma, 2011, P IEEE INT WORKSH IN, P1
   Blauch NM, 2021, COGNITION, V208, DOI 10.1016/j.cognition.2020.104341
   Bowyer Kevin W., 2016, P IEEE 8 INT C BIOM, P1
   Braje WL, 1998, PSYCHOBIOLOGY, V26, P371
   CAREY S, 1977, SCIENCE, V195, P312, DOI 10.1126/science.831281
   CAREY S, 1992, PHILOS T ROY SOC B, V335, P95, DOI 10.1098/rstb.1992.0012
   Cavazos Jacqueline G, 2021, IEEE Trans Biom Behav Identity Sci, V3, P101, DOI 10.1109/TBIOM.2020.3027269
   Chen JC, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P360, DOI 10.1109/ICCVW.2015.55
   Dai Xiyang, 2013, P 30 INT C MACH LEAR, V28
   DIAMOND R, 1986, J EXP PSYCHOL GEN, V115, P107, DOI 10.1037/0096-3445.115.2.107
   Farkas JP, 2013, PRS-GLOB OPEN, V1, DOI 10.1097/GOX.0b013e31828ed1da
   Fraga MF, 2005, P NATL ACAD SCI USA, V102, P10604, DOI 10.1073/pnas.0500398102
   Gellman M.D., 2013, ENCY BEHAV MED
   Guyuron B, 2009, PLAST RECONSTR SURG, V123, P1321, DOI 10.1097/PRS.0b013e31819c4d42
   Hanaoka Kayee, 2022, NIST Interagency/Internal Report (NISTIR), DOI [10.6028/NIST.IR.8439, DOI 10.6028/NIST.IR.8439]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hill MQ, 2019, NAT MACH INTELL, V1, P522, DOI 10.1038/s42256-019-0111-7
   Jain AK, 2002, PATTERN RECOGN, V35, P2653, DOI 10.1016/S0031-3203(01)00218-7
   Jenkins R, 2011, COGNITION, V121, P313, DOI 10.1016/j.cognition.2011.08.001
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li HQ, 2014, LECT NOTES COMPUT SC, V8833, P288, DOI 10.1007/978-3-319-12484-1_33
   Liu Ziwei, 2018, Largescale celebfaces attributes (celeba) dataset, P11
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Lynch Jennifer, 2020, Face off: Law enforcement use of face recognition technology
   Maurer D, 2002, TRENDS COGN SCI, V6, P255, DOI 10.1016/S1364-6613(02)01903-4
   Maze B, 2018, INT CONF BIOMETR, P158, DOI 10.1109/ICB2018.2018.00033
   McCauley J, 2021, Biometrics Special I, V315, DOI 10.1109/BIOSIG52210.2021.9548299
   Megreya AM, 2006, MEM COGNITION, V34, P865, DOI 10.3758/BF03193433
   Mousavi S, 2021, MULTIMED TOOLS APPL, V80, P15765, DOI 10.1007/s11042-020-10360-3
   Nechyba MC, 2008, LECT NOTES COMPUT SC, V4625, P126
   Noyes E., 2017, Face processing: Systems, disorders, and cultural differences
   Noyes E, 2021, COGNITION, V211, DOI 10.1016/j.cognition.2021.104611
   O'Toole AJ, 1998, VISION RES, V38, P2351, DOI 10.1016/S0042-6989(98)00042-X
   O'Toole AJ, 2007, IEEE T PATTERN ANAL, V29, P1642, DOI 10.1109/TPAMI.2007.1107
   O'Toole AJ, 2021, ANNU REV VIS SCI, V7, P543, DOI 10.1146/annurev-vision-093019-111701
   Paone JR, 2014, IEEE T INF FOREN SEC, V9, P285, DOI 10.1109/TIFS.2013.2296373
   Parde CJ, 2021, J VISION, V21, DOI 10.1167/jov.21.8.15
   Parde CJ, 2017, IEEE INT CONF AUTOMA, P673, DOI 10.1109/FG.2017.85
   Phillips P. J., 2011, Proceedings 2011 IEEE International Conference on Automatic Face & Gesture Recognition (FG 2011), P185, DOI 10.1109/FG.2011.5771395
   Phillips PJ, 2018, P NATL ACAD SCI USA, V115, P6171, DOI 10.1073/pnas.1721355115
   Phillips PJ, 2014, IMAGE VISION COMPUT, V32, P74, DOI 10.1016/j.imavis.2013.12.002
   Pruitt Matthew T., 2011, P INT JOINT C BIOM I, P1
   Ramon M, 2018, VIS COGN, V26, P179, DOI 10.1080/13506285.2017.1405134
   Ranjan R, 2019, Arxiv, DOI arXiv:1804.01159
   Ranjan R, 2017, IEEE INT CONF AUTOMA, P17, DOI 10.1109/FG.2017.137
   RDU Frontex, 2012, Best practice operational guidelines for automated border control (ABC) systems
   Ricanek K, 2013, COMPUTER, V46, P94, DOI 10.1109/MC.2013.82
   Schneiderman H, 2004, PROC CVPR IEEE, P639
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Srinivas N, 2012, IEEE T INF FOREN SEC, V7, P1536, DOI 10.1109/TIFS.2012.2206027
   Sun Xiaoxia, 2018, Deep Learning in Biometrics, P65
   Sun Y, 2014, PROC CVPR IEEE, P1891, DOI 10.1109/CVPR.2014.244
   Sun ZN, 2010, PROC SPIE, V7667, DOI 10.1117/12.851369
   Sundaresan V, 2021, IMAGE VISION COMPUT, V116, DOI 10.1016/j.imavis.2021.104331
   Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220
   TURK M, 1991, J COGNITIVE NEUROSCI, V3, P71, DOI 10.1162/jocn.1991.3.1.71
   Weinhold B, 2006, ENVIRON HEALTH PERSP, V114, pA160, DOI 10.1289/ehp.114-a160
   White D, 2015, P ROY SOC B-BIOL SCI, V282, P73, DOI 10.1098/rspb.2015.1292
NR 63
TC 0
Z9 0
U1 0
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD JUL
PY 2023
VL 20
IS 3
AR 10
DI 10.1145/3609224
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA U6DF9
UT WOS:001085681100002
OA Green Submitted, Bronze
DA 2024-07-18
ER

PT J
AU Shamy, M
   Feitelson, DG
AF Shamy, Mor
   Feitelson, Dror G.
TI Identifying Lines and Interpreting Vertical Jumps in Eye Tracking
   Studies of Reading Text and Code
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Eye tracking; reading order; behavior model
ID PROGRAM COMPREHENSION; MOVEMENTS
AB Eye tracking studies have shown that reading code, in contradistinction to reading text, includes many vertical jumps. As different lines of code may have quite different functions (e.g., variable definition, flow control, or computation), it is important to accurately identify the lines being read. We design experiments that require a specific line of text to be scrutinized. Using the distribution of gazes around this line, we then calculate how the precision with which we can identify the line being read depends on the font size and spacing. The results indicate that, even after correcting for systematic bias, unnaturally large fonts and spacing may be required for reliable line identification.
   Interestingly, during the experiments, the participants also repeatedly re-checked their task and if they were looking at the correct line, leading to vertical jumps similar to those observed when reading code. This suggests that observed reading patterns may be "inefficient," in the sense that participants feel the need to repeat actions beyond the minimal number apparently required for the task. This may have implications regarding the interpretation of reading patterns. In particular, reading does not reflect only the extraction of information from the text or code. Rather, reading patterns may also reflect other types of activities, such as getting a general orientation, and searching for specific locations in the context of performing a particular task.
C1 [Shamy, Mor; Feitelson, Dror G.] Hebrew Univ Jerusalem, Dept Comp Sci, IL-91904 Jerusalem, Israel.
C3 Hebrew University of Jerusalem
RP Shamy, M (corresponding author), Hebrew Univ Jerusalem, Dept Comp Sci, IL-91904 Jerusalem, Israel.
EM mor.shamy@mail.huji.ac.il; feit@cs.huji.ac.il
OI Feitelson, Dror/0000-0002-2733-7709
FU Israel Science Foundation [832/18]
FX This research was supported by the Israel Science Foundation (grant no.
   832/18).
CR Abid NJ, 2019, ETRA 2019: 2019 ACM SYMPOSIUM ON EYE TRACKING RESEARCH & APPLICATIONS, DOI 10.1145/3314111.3319834
   [Anonymous], 2014, P C INT COMP ED RES
   Bauer J, 2019, INT C PROGRAM COMPRE, P154, DOI 10.1109/ICPC.2019.00033
   Bednarik R., 2006, Proceedings. ETRA 2006. Symposium on Eye Tracking Research and Applications, P125, DOI 10.1145/1117309.1117356
   Binkley D, 2013, EMPIR SOFTW ENG, V18, P219, DOI 10.1007/s10664-012-9201-4
   Blascheck T, 2019, ETRA 2019: 2019 ACM SYMPOSIUM ON EYE TRACKING RESEARCH & APPLICATIONS, DOI 10.1145/3314111.3319917
   Booth RW, 2013, MEM COGNITION, V41, P82, DOI 10.3758/s13421-012-0244-y
   Bottos S, 2019, 2019 22ND INTERNATIONAL CONFERENCE ON INFORMATION FUSION (FUSION 2019), DOI 10.23919/fusion43075.2019.9011436
   Busjahn T., 2011, Proceedings of the 11th Koli Calling International Conference on Computing Education Research, P1, DOI DOI 10.1145/2094131.2094133
   Busjahn T, 2015, INT C PROGRAM COMPRE, P255, DOI 10.1109/ICPC.2015.36
   Carr JW, 2022, BEHAV RES METHODS, V54, P287, DOI 10.3758/s13428-021-01554-0
   Cowan N, 2010, CURR DIR PSYCHOL SCI, V19, P51, DOI 10.1177/0963721409359277
   CROSBY ME, 1990, COMPUTER, V23, P24, DOI 10.1109/2.48797
   Feitelson DG, 2022, EMPIR SOFTW ENG, V27, DOI 10.1007/s10664-022-10160-3
   Holmqvist K, 2003, MIND'S EYE: COGNITIVE AND APPLIED ASPECTS OF EYE MOVEMENT RESEARCH, P657, DOI 10.1016/B978-044451020-4/50035-9
   Holmqvist K., 2012, Eye tracker data quality: what it is and how to measure it, P45, DOI DOI 10.1145/2168556.2168563
   Hooge ITC, 2019, BEHAV RES METHODS, V51, P2712, DOI 10.3758/s13428-018-1135-3
   Hornof AJ, 2002, BEHAV RES METH INS C, V34, P592, DOI 10.3758/BF03195487
   Huang Y, 2020, PROCEEDINGS OF THE 28TH ACM JOINT MEETING ON EUROPEAN SOFTWARE ENGINEERING CONFERENCE AND SYMPOSIUM ON THE FOUNDATIONS OF SOFTWARE ENGINEERING (ESEC/FSE '20), P456, DOI 10.1145/3368089.3409681
   Jbara A, 2017, EMPIR SOFTW ENG, V22, P1440, DOI 10.1007/s10664-016-9477-x
   Kean M, 2003, MIND'S EYE: COGNITIVE AND APPLIED ASPECTS OF EYE MOVEMENT RESEARCH, P27, DOI 10.1016/B978-044451020-4/50003-7
   Krejtz K, 2017, J EYE MOVEMENT RES, V10, DOI 10.16910/jemr.10.2.3
   Krejtz K, 2016, ACM T APPL PERCEPT, V13, DOI 10.1145/2896452
   LETOVSKY S, 1987, J SYST SOFTWARE, V7, P325, DOI 10.1016/0164-1212(87)90032-X
   Levy O, 2021, EMPIR SOFTW ENG, V26, DOI 10.1007/s10664-021-09938-8
   LITTMAN DC, 1987, J SYST SOFTWARE, V7, P341, DOI 10.1016/0164-1212(87)90033-1
   Marter T, 2016, ONWARD!'16: PROCEEDINGS OF THE 2016 ACM INTERNATIONAL SYMPOSIUM ON NEW IDEAS, NEW PARADIGMS, AND REFLECTIONS ON PROGRAMMING AND SOFTWARE, P1, DOI 10.1145/2986012.2986020
   Minelli R, 2015, INT C PROGRAM COMPRE, P25, DOI 10.1109/ICPC.2015.12
   Mishra A., 2012, Proceedings of the First Workshop on Eye-tracking and Natural Language Processing, P71
   Narcizo Fabricio Batista, 2013, 2013 26th Conference on Graphics, Patterns and Images - Tutorials (SIBGRAPI-T), P15, DOI 10.1109/SIBGRAPI-T.2013.8
   Nyström M, 2013, BEHAV RES METHODS, V45, P272, DOI 10.3758/s13428-012-0247-4
   Obaidellah U, 2018, ACM COMPUT SURV, V51, DOI 10.1145/3145904
   Palmer C, 2016, 2016 ACM SYMPOSIUM ON EYE TRACKING RESEARCH & APPLICATIONS (ETRA 2016), P65, DOI 10.1145/2857491.2857544
   Peitek N, 2020, INT C PROGRAM COMPRE, P342, DOI 10.1145/3387904.3389279
   Peterson CS, 2019, ETRA 2019: 2019 ACM SYMPOSIUM ON EYE TRACKING RESEARCH & APPLICATIONS, DOI 10.1145/3314111.3319833
   Pi J., 2019, P 11 ACM S EYE TRACK, P8, DOI DOI 10.1145/3314111.3319845
   Ramkumar N, 2020, ETRA'20 FULL PAPERS: ACM SYMPOSIUM ON EYE TRACKING RESEARCH AND APPLICATIONS, DOI 10.1145/3379155.3391328
   Rayner K, 1998, PSYCHOL BULL, V124, P372, DOI 10.1037/0033-2909.124.3.372
   Rayner K, 2006, SCI STUD READ, V10, P241, DOI 10.1207/s1532799xssr1003_3
   Rayner K, 2009, Q J EXP PSYCHOL, V62, P1457, DOI 10.1080/17470210902816461
   Rodeghero P, 2015, INT SYMP EMP SOFTWAR, P11
   Rodeghero P, 2015, IEEE T SOFTWARE ENG, V41, P1038, DOI 10.1109/TSE.2015.2442238
   Roehm T, 2012, PROC INT CONF SOFTW, P255, DOI 10.1109/ICSE.2012.6227188
   Shaffer TR, 2015, 2015 10TH JOINT MEETING OF THE EUROPEAN SOFTWARE ENGINEERING CONFERENCE AND THE ACM SIGSOFT SYMPOSIUM ON THE FOUNDATIONS OF SOFTWARE ENGINEERING (ESEC/FSE 2015) PROCEEDINGS, P954, DOI 10.1145/2786805.2803188
   Sharafi Z, 2022, IEEE T SOFTWARE ENG, V48, P1692, DOI 10.1109/TSE.2020.3032064
   Sharafi Z, 2020, EMPIR SOFTW ENG, V25, P3128, DOI 10.1007/s10664-020-09829-4
   Sharafi Z, 2015, INFORM SOFTWARE TECH, V67, P79, DOI 10.1016/j.infsof.2015.06.008
   Spakov O, 2019, BEHAV RES METHODS, V51, P2661, DOI 10.3758/s13428-018-1120-x
   Spitzer L, 2022, 2022 ACM SYMPOSIUM ON EYE TRACKING RESEARCH AND APPLICATIONS, ETRA 2022, DOI 10.1145/3517031.3529644
   Strasburger H, 2011, J VISION, V11, DOI 10.1167/11.5.13
   Talsma Renske, 2020, P 9 COMP SCI ED RES, P1, DOI [10.1145/3442481.3442505, DOI 10.1145/3442481.3442505]
   Uwano H., 2005, Eye Tracking Research and Applications Symposium (ETRA), V2005, P133
   Velichkovsky BM, 2019, SOVREM TEHNOL MED, V11, P7, DOI 10.17691/stm2019.11.4.01
   Velichkovsky Boris M, 2005, P 27 C COGN SCI SOC, V1
   Xia X, 2018, IEEE T SOFTWARE ENG, V44, P951, DOI 10.1109/TSE.2017.2734091
   Yamaya Akito, 2017, J INFORM PROCESSING, V25, P100, DOI [10.2197/ipsjjip.25.100, DOI 10.2197/IPSJJIP.25.100]
   YaoWang Maurice Koch, 2022, P 2022 S EYE TRACKIN, DOI [10.1145/3517031.3531166, DOI 10.1145/3517031.3531166]
NR 57
TC 0
Z9 0
U1 2
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD APR
PY 2023
VL 20
IS 2
DI 10.1145/3579357
PG 20
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA H6MI8
UT WOS:000997078100001
OA Bronze
DA 2024-07-18
ER

PT J
AU Homayouni, M
   Aflaki, P
   Hannuksela, MM
   Gabbouj, M
AF Homayouni, Maryam
   Aflaki, Payman
   Hannuksela, Miska M.
   Gabbouj, Moncef
TI Row-Interleaved Sampling for Depth-Enhanced 3D Video Coding for
   Polarized Displays
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Design; Performance; Experimentation; 3D video; compression; polarized
   display; sub-sampling
ID STEREOSCOPIC DISPLAYS; QUALITY
AB Passive stereoscopic displays create the illusion of three dimensions by employing orthogonal polarizing filters and projecting two images onto the same screen. In this article, a coding scheme targeting depth-enhanced stereoscopic video coding for polarized displays is introduced. We propose to use asymmetric row-interleaved sampling for texture and depth views prior to encoding. The performance of the proposed scheme is compared with several other schemes, and the objective results confirm the superior performance of the proposed method. Furthermore, subjective evaluation proves that no quality degradation is introduced by the proposed coding scheme compared to the reference method.
C1 [Homayouni, Maryam; Gabbouj, Moncef] Tampere Univ Technol, Dept Signal Proc, Tampere, Finland.
   [Aflaki, Payman; Hannuksela, Miska M.] Nokia Technol, Hatanpaan Valtatie 30, Tampere 33100, Finland.
   [Homayouni, Maryam; Gabbouj, Moncef] Korkeakoulunkatu 10, Tampere 33720, Finland.
C3 Tampere University; Nokia Corporation; Nokia Finland
RP Homayouni, M (corresponding author), Tampere Univ Technol, Dept Signal Proc, Tampere, Finland.; Homayouni, M (corresponding author), Korkeakoulunkatu 10, Tampere 33720, Finland.
EM maryam.homayouni@tut.fi; payman.aflaki@nokia.com;
   miska.hannuksela@nokia.com; moncef.gabbouj@tut.fi
RI Gabbouj, Moncef/G-4293-2014
OI Gabbouj, Moncef/0000-0002-9788-2323
CR Aflaki P., 2014, P INT C 3D IM IC3D 1, P1
   Aflaki P, 2015, SIGNAL IMAGE VIDEO P, V9, P331, DOI 10.1007/s11760-013-0439-0
   Aflaki P, 2011, INT SYMP IMAGE SIG, P396
   Aflaki P, 2010, IEEE IMAGE PROC, P4021, DOI 10.1109/ICIP.2010.5650661
   [Anonymous], 2013, H2642013 ITUT
   [Anonymous], 2002, BT500112002 ITUR
   [Anonymous], 2004, INT SOC OPTICS PHOTO, DOI DOI 10.1117/12.524762
   Bjotegaard G., 2001, VCEGM33
   BLAKE R, 1977, J EXP PSYCHOL HUMAN, V3, P251, DOI 10.1037/0096-1523.3.2.251
   Chen Y, 2014, J VIS COMMUN IMAGE R, V25, P679, DOI 10.1016/j.jvcir.2013.03.013
   Chen Y, 2009, EURASIP J ADV SIG PR, DOI 10.1155/2009/786015
   Cook R. L., 1984, Computers & Graphics, V18, P137
   Dong J., 2012, M24499 ISOIECJTC1SC2
   Hakala JH, 2015, ACM T APPL PERCEPT, V12, P49, DOI 10.1145/2699266
   Hanhart P, 2012, INT WORK QUAL MULTIM, P236, DOI 10.1109/QoMEX.2012.6263854
   Hannuksela MM, 2013, IEEE T IMAGE PROCESS, V22, P3449, DOI 10.1109/TIP.2013.2269274
   Hoffman DM, 2011, J SOC INF DISPLAY, V19, P271, DOI 10.1889/JSID19.3.271
   Johnson PV, 2015, OPT EXPRESS, V23, P9252, DOI 10.1364/OE.23.009252
   Konrad J, 2007, IEEE SIGNAL PROC MAG, V24, P97, DOI 10.1109/MSP.2007.905706
   Merkle P, 2007, IEEE IMAGE PROC, P201
   Park M, 2014, APPL OPTICS, V53, P520, DOI 10.1364/AO.53.000520
   Ramachandra V, 2008, IEEE IMAGE PROC, P2436, DOI 10.1109/ICIP.2008.4712285
   Rusanovskyy D., 2013, JCT3VC1100
   Saygili G, 2010, IEEE IMAGE PROC, P4009, DOI 10.1109/ICIP.2010.5653339
   Saygili G, 2009, IEEE IMAGE PROC, P717, DOI 10.1109/ICIP.2009.5414317
   Seuntiens P., 2006, ACM T APPL PERCEPT, V3, P95
   Shibata T, 2011, J VISION, V11, DOI 10.1167/11.8.11
   Smolic A, 2009, PCS: 2009 PICTURE CODING SYMPOSIUM, P389
   Stelmach L, 2000, IEEE T CIRC SYST VID, V10, P188, DOI 10.1109/76.825717
   Tam WJ, 2007, IMAGE DEPTH QUALITY
   Vetro A, 2011, P IEEE, V99, P626, DOI 10.1109/JPROC.2010.2098830
   WILCOXON F, 1945, BIOMETRICS BULL, V1, P80, DOI 10.1093/jee/39.2.269
NR 32
TC 1
Z9 1
U1 0
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD JUL
PY 2017
VL 14
IS 3
AR 15
DI 10.1145/3047409
PG 23
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FB9AL
UT WOS:000406431900002
DA 2024-07-18
ER

PT J
AU Akyüz, AO
   Kaya, O
AF Akyuz, Ahmet Oguz
   Kaya, Osman
TI A Proposed Methodology for Evaluating HDR False Color Maps
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE HDR imaging; false color; visualization
AB Color mapping, which involves assigning colors to the individual elements of an underlying data distribution, is a commonly used method for data visualization. Although color maps are used in many disciplines and for a variety of tasks, in this study we focus on its usage for visualizing luminance maps. Specifically, we ask ourselves the question of how to best visualize a luminance distribution encoded in a high-dynamic-range (HDR) image using false colors such that the resulting visualization is the most descriptive. To this end, we first propose a definition for descriptiveness. We then propose a methodology to evaluate it subjectively. Then, we propose an objective metric that correlates well with the subjective evaluation results. Using this metric, we evaluate several false coloring strategies using a large number of HDR images. Finally, we conduct a second psychophysical experiment using images representing a diverse set of scenes. Our results indicate that the luminance compression method has a significant effect and the commonly used logarithmic compression is inferior to histogram equalization. Furthermore, we find that the default color scale of the Radiance global illumination software consistently performs well when combined with histogram equalization. On the other hand, the commonly used rainbow color scale was found to be inferior. We believe that the proposed methodology is suitable for evaluating future color mapping strategies as well.
C1 [Akyuz, Ahmet Oguz; Kaya, Osman] Middle East Tech Univ, Dept Comp Engn, TR-06800 Ankara, Turkey.
C3 Middle East Technical University
RP Akyüz, AO (corresponding author), Middle East Tech Univ, Dept Comp Engn, TR-06800 Ankara, Turkey.
EM akyuz@ceng.metu.edu.tr; osman.kaya@ceng.metu.edu.tr
RI Akyuz, Ahmet O/A-7956-2018
OI Akyuz, Ahmet/0000-0001-7685-5572
CR Akyüz AO, 2008, ACM T APPL PERCEPT, V4, DOI 10.1145/1278760.1278761
   Akyuz Ahmet Oguz, 2013, HDRI2013
   [Anonymous], 2002, MPII20024002
   [Anonymous], 1992, R. woods digital image processing
   [Anonymous], 2006, 2006 C COMPUTER VISI
   Beltran Liliana O., 2005, ISES SOL WORLD C ISE
   Borland D, 2007, IEEE COMPUT GRAPH, V27, P14, DOI 10.1109/MCG.2007.323435
   Brown KC., 2010, J FORENSIC IDENTIFIC, V60, P449
   Cai H, 2013, LIGHTING RES TECHNOL, V45, P230, DOI 10.1177/1477153512453273
   Chicken E., 2013, NONPARAMETRIC STAT M, V751
   Daly Scott, 1993, P179
   Drago F, 2003, COMPUT GRAPH FORUM, V22, P419, DOI 10.1111/1467-8659.00689
   Durand F, 2002, ACM T GRAPHIC, V21, P257, DOI 10.1145/566570.566574
   Fairchild MD, 2007, FIFTEENTH COLOR IMAGING CONFERENCE: COLOR SCIENCE AND ENGINEERING SYSTEMS, TECHNOLOGIES, AND APPLICATIONS, FINAL PROGRAM AND PROCEEDINGS, P233
   Fattal R, 2002, ACM T GRAPHIC, V21, P249
   Grinzato E., 2009, Proceedings of the SPIE - The International Society for Optical Engineering, V7299, DOI 10.1117/12.818610
   Happa J, 2010, P 11 INT C VIRT REAL, P17
   Heidrich Wolfgang, ERIK REINHARD
   Larson G.W., 1998, MKS COMP GRAPH GEOME
   Larson Greg Ward, 2013, COMMUNICATION
   Larson GW, 1997, IEEE T VIS COMPUT GR, V3, P291, DOI 10.1109/2945.646233
   LEVKOWITZ H, 1992, IEEE COMPUT GRAPH, V12, P72, DOI 10.1109/38.135886
   MacDonald LW, 1999, IEEE COMPUT GRAPH, V19, P20, DOI 10.1109/38.773961
   Mantiuk R, 2009, COMPUT GRAPH FORUM, V28, P193, DOI 10.1111/j.1467-8659.2009.01358.x
   Mantiuk RK, 2012, COMPUT GRAPH FORUM, V31, P2478, DOI 10.1111/j.1467-8659.2012.03188.x
   NAKA KI, 1966, J PHYSIOL-LONDON, V185, P587, DOI 10.1113/jphysiol.1966.sp008003
   Reinhard E, 2002, ACM T GRAPHIC, V21, P267, DOI 10.1145/566570.566575
   Reinhard E., 2008, Color Imaging: Fundamentals and Applications
   Rogowitz BE, 1998, IEEE SPECTRUM, V35, P52, DOI 10.1109/6.736450
   Sharma G, 2005, COLOR RES APPL, V30, P21, DOI 10.1002/col.20070
   Silva S, 2011, COMPUT GRAPH-UK, V35, P320, DOI 10.1016/j.cag.2010.11.015
   STARKS TH, 1961, BIOMETRIKA, V48, P95, DOI 10.1093/biomet/48.1-2.95
   Theodor JM, 2009, PALAEONTOL ELECTRON, V12
   TRUMBO BE, 1981, AM STAT, V35, P220, DOI 10.2307/2683294
   TUMBLIN J, 1991, GITGVU9113
   Wang LJ, 2008, IEEE T VIS COMPUT GR, V14, P1739, DOI 10.1109/TVCG.2008.118
   Williams T., 2010, Gnuplot 4.4: An Interactive Plotting Program
NR 37
TC 1
Z9 1
U1 1
U2 5
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD AUG
PY 2016
VL 14
IS 1
AR 2
DI 10.1145/2911986
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DV4EB
UT WOS:000382876900002
DA 2024-07-18
ER

PT J
AU Mihtsentu, MT
   Ware, C
AF Mihtsentu, Mezgeb Tesfayesus
   Ware, Colin
TI Discrete Versus Solid: Representing Quantity Using Linear, Area, and
   Volume Glyphs
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Human Factors; Glyphs; representing quantity; subitizing; visualization
ID DISCRIMINATION; PERCEPTION; NUMBER; SIZE
AB It is common in infographics for quantities to be represented by stacks of discrete blocks. For example, a magazine illustration showing automobile production in different countries might use stacks of blocks with each block representing a thousand cars. This is unlike what is done to represent quantity in the charts used by statisticians, or for quantitative glyphs used in maps. In these cases, solid bars or solid area glyphs such as circles are commonly used to represent quantity. This raises the question of whether breaking bars, area, or volume glyphs into discrete blocks can improve the rapid estimation of quantity. We report on a study where participants compared quantities represented using bar, area, and volume glyphs in both solid and discrete variants. The discrete variants used up to 4, 4 x 4, and 4 x 4 x 4 blocks or 10, 10 x 10, and 10 x 10 x 10 blocks for bar, area, and volume, respectively. The results show that people are significantly more accurate in estimating quantities using the discrete versions, but they take somewhat longer. For both areas and volumes, the accuracy gains were considerable.
C1 [Mihtsentu, Mezgeb Tesfayesus; Ware, Colin] Univ New Hampshire, Durham, NH 03824 USA.
C3 University System Of New Hampshire; University of New Hampshire
RP Mihtsentu, MT (corresponding author), Univ New Hampshire, Durham, NH 03824 USA.
EM mezgebth@gmail.com; cware@ccom.unh.edu
FU NOAA [NA05NOS4001153]
FX This work was supported by NOAA grant NA05NOS4001153.
CR [Anonymous], 2013, Information Visualization: Perception for Design
   [Anonymous], 1936, International Picture Language, The first rules of isotype
   BAIRD JC, 1970, PERCEPT MOTOR SKILL, V30, P495, DOI 10.2466/pms.1970.30.2.495
   BECKWITH M, 1966, PSYCHOL REV, V73, P437, DOI 10.1037/h0023650
   Chevalier F, 2013, IEEE T VIS COMPUT GR, V19, P2426, DOI 10.1109/TVCG.2013.210
   CLARKE JI, 1959, GEOGRAPHY, V44, P96
   Clements D.H., 1999, Teaching Children Mathematics, P400, DOI [DOI 10.5951/TCM.5.7.0400, 10.5951/tcm.5.7.0400]
   CLEVELAND WS, 1984, J AM STAT ASSOC, V79, P531, DOI 10.2307/2288400
   Croxton FE, 1932, J AM STAT ASSOC, V27, P54, DOI 10.2307/2277880
   EKMAN G, 1961, SCAND J PSYCHOL, V2, P1, DOI 10.1111/j.1467-9450.1961.tb01215.x
   Ekman G., 1961, Perceptual and Motor Skills, V13, P355
   Few S., 2013, PERCEPTUAL EDGE VISU
   Freeman FN, 1912, ELEM SCH TEACH, V12, P306, DOI 10.1086/454132
   Gallistel C.R., 1991, Memories, thoughts, and emotions: Essays in honor of George Mandler, P65
   GALLISTEL CR, 1992, COGNITION, V44, P43, DOI 10.1016/0010-0277(92)90050-R
   KAUFMAN EL, 1949, AM J PSYCHOL, V62, P498, DOI 10.2307/1418556
   Peterson SA, 2000, COGNITIVE SCI, V24, P93, DOI 10.1207/s15516709cog2401_3
   SIMKIN D, 1987, J AM STAT ASSOC, V82, P454, DOI 10.2307/2289447
   TEGHTSOONIAN M, 1965, AM J PSYCHOL, V78, P392, DOI 10.2307/1420573
   TRICK LM, 1994, PSYCHOL REV, V101, P80, DOI 10.1037/0033-295X.101.1.80
NR 20
TC 0
Z9 0
U1 0
U2 2
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD JUL
PY 2015
VL 12
IS 3
AR 12
DI 10.1145/2767129
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA CP6OI
UT WOS:000360006600006
DA 2024-07-18
ER

PT J
AU Abhari, K
   Baxter, JSH
   Khan, AR
   Peters, TM
   De Ribaupierre, S
   Eagleson, R
AF Abhari, Kamyar
   Baxter, John S. H.
   Khan, Al R.
   Peters, Terry M.
   De Ribaupierre, Sandrine
   Eagleson, Roy
TI Visual Enhancement of MR Angiography Images to Facilitate Planning of
   Arteriovenous Malformation Interventions
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Medical Image Visualization; Human Factors; MR angiography;
   arteriovenous malformation; contour enhancement; eel shading; stereopsis
ID VISUALIZATION; MODEL
AB The primary purpose of medical image visualization is to improve patient outcomes by facilitating the inspection, analysis, and interpretation of patient data. This is only possible if the users' perceptual and cognitive limitations are taken into account during every step of design, implementation, and evaluation of interactive displays. Visualization of medical images, if executed effectively and efficiently, can empower physicians to explore patient data rapidly and accurately with minimal cognitive effort. This article describes a specific case study in biomedical visualization system design and evaluation, which is the visualization of MR angiography images for planning arteriovenous malformation (AVM) interventions. The success of an AVM intervention greatly depends on the surgeon gaining a full understanding of the anatomy of the malformation and its surrounding structures. Accordingly, the purpose of this study was to investigate the usability of visualization modalities involving contour enhancement and stereopsis in the identification and localization of vascular structures using objective user studies. Our preliminary results indicate that contour enhancement, particularly when combined with stereopsis, results in improved performance enhancement of the perception of connectivity and relative depth between different structures.
C1 [Abhari, Kamyar; Baxter, John S. H.; Khan, Al R.; Peters, Terry M.; De Ribaupierre, Sandrine; Eagleson, Roy] Univ Western Ontario, Robarts Res Inst, London, ON N6A 5B7, Canada.
C3 Western University (University of Western Ontario)
RP Abhari, K (corresponding author), Univ Western Ontario, London, ON N6A 5B7, Canada.
EM kabhari@robarts.ca; jbaxter@robarts.ca; alik@robarts.ca;
   tpeters@robarts.ca; sderibau@uwo.ca; eagleson@uwo.ca
RI Peters, Terry Malcolm/AAD-7797-2022; Khan, Ali R./P-1353-2014; Peters,
   Terry M/K-6853-2013; de Ribaupierre, Sandrine/B-7707-2015; Baxter, John
   S.H./ABI-1243-2020
OI Peters, Terry Malcolm/0000-0003-1440-7488; Khan, Ali
   R./0000-0002-0760-8647; de Ribaupierre, Sandrine/0000-0001-7096-2289;
   Baxter, John S.H./0000-0003-3548-4343
FU Canadian Institutes for Health Research [MOP 74626]; National Science
   and Engineering Research Council of Canada [R314GA01, A2680A02];
   NCE-Grand; Ontario Research and Development Challenge Fund; Canadian
   Foundation for Innovation; Ontario Innovation Trust; National Science
   and Engineering Research Councigovernment of Ontario (OGS),l of Canada
FX This project was supported by the Canadian Institutes for Health
   Research (Grant MOP 74626), the National Science and Engineering
   Research Council of Canada (Grants #R314GA01 and #A2680A02), NCE-Grand,
   the Ontario Research and Development Challenge Fund, and the Canadian
   Foundation for Innovation and Ontario Innovation Trust. Graduate student
   funding for K. Abhari was provided by scholarships from the National
   Science and Engineering Research Council of Canada, the government of
   Ontario (OGS), and Western University.
CR Baldassi S, 2006, PLOS BIOL, V4, P387, DOI 10.1371/journal.pbio.0040056
   Bregman MR, 2012, COGNITION, V122, P51, DOI 10.1016/j.cognition.2011.08.008
   Bullitt E, 2001, NEUROSURGERY, V48, P576, DOI 10.1097/00006123-200103000-00024
   Chen SJS, 2012, PROC SPIE, V8316, DOI 10.1117/12.911684
   Cohen J., 1988, STAT POWER ANAL BEHA
   Cutting JE, 1997, BEHAV RES METH INS C, V29, P27, DOI 10.3758/BF03200563
   Ehrenstein WalterH., 1999, Modern Techniques in Neuroscience Research, P1211, DOI DOI 10.1007/978-3-642-58552-4_43
   Feng D, 2001, IEEE VISUAL, P387, DOI 10.1109/VISUAL.2001.964537
   Fishman EK, 2006, RADIOGRAPHICS, V26, P905, DOI 10.1148/rg.263055186
   FRIZZEL RT, 1995, NEUROSURGERY, V37, P1031, DOI 10.1227/00006123-199512000-00001
   Hansen C, 2010, INT J COMPUT ASS RAD, V5, P133, DOI 10.1007/s11548-009-0365-3
   Healey Christopher G., 1999, SIGGRAPH99 COURSE
   Howard Ian P., 1995, BINOCULAR VISION STE
   Immel D. S., 1986, Computer Graphics, V20, P133, DOI 10.1145/15886.15901
   Joshi A, 2008, IEEE T VIS COMPUT GR, V14, P1603, DOI 10.1109/TVCG.2008.123
   Kajiya J. T., 1986, SIGGRAPH, P143, DOI 10.1145/15886.15902
   Kersten-Oertel M, 2014, IEEE T VIS COMPUT GR, V20, P391, DOI 10.1109/TVCG.2013.240
   Kniss J, 2001, IEEE VISUAL, P255, DOI 10.1109/VISUAL.2001.964519
   LIU YL, 1992, HUM FACTORS, V34, P165, DOI 10.1177/001872089203400203
   MARR D, 1979, PROC R SOC SER B-BIO, V204, P301, DOI 10.1098/rspb.1979.0029
   Martin N. A., 1999, CLIN NEUROSURG, V46, P295
   Moore CM, 1998, PSYCHOL SCI, V9, P104, DOI 10.1111/1467-9280.00019
   NINDS, 2011, ART MALF OTH VASC LE
   ONDRA SL, 1990, J NEUROSURG, V73, P387, DOI 10.3171/jns.1990.73.3.0387
   PYLYSHYN ZW, 1994, INVEST OPHTH VIS SCI, V35, P2007
   Ritter F, 2006, IEEE T VIS COMPUT GR, V12, P877, DOI 10.1109/TVCG.2006.172
   Ropinski T, 2006, LECT NOTES COMPUT SC, V4073, P93
   Rosenholtz R, 2007, J VISION, V7, DOI 10.1167/7.2.17
   Schaller Carlo., 2000, Brain, V123, P190
   Soltészová V, 2010, COMPUT GRAPH FORUM, V29, P883, DOI 10.1111/j.1467-8659.2009.01695.x
   Tendick F, 1997, Comput Aided Surg, V2, P24
   Thomas Lisa C, 2005, P 13 INT S AV PSYCH
   TREISMAN A, 1985, COMPUT VISION GRAPH, V31, P156, DOI 10.1016/S0734-189X(85)80004-9
   WALLIS JW, 1989, IEEE T MED IMAGING, V8, P297, DOI 10.1109/42.41482
   Ware C., 2020, INFORM VISUALIZATION
   Weiler Florian., 2011, Spring Eurograp, V6, P2
NR 36
TC 3
Z9 4
U1 0
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD MAR
PY 2015
VL 12
IS 1
BP 63
EP 77
DI 10.1145/2701425
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CG8AI
UT WOS:000353528600004
DA 2024-07-18
ER

PT J
AU Bhardwaj, A
   Chaudhuri, S
   Dabeer, O
AF Bhardwaj, Amit
   Chaudhuri, Subhasis
   Dabeer, Onkar
TI Design and Analysis of Predictive Sampling of Haptic Signals
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Experimentation; Human factors; Adaptive sampling; Weber's law; level
   crossings; linear regression; decision tree; random forest;
   rate-distortion curve
ID DATA REDUCTION; TELEPRESENCE; TRANSMISSION; PERCEPTION; SYSTEMS
AB In this article, we identify adaptive sampling strategies for haptic signals. Our approach relies on experiments wherein we record the response of several users to haptic stimuli. We then learn different classifiers to predict the user response based on a variety of causal signal features. The classifiers that have good prediction accuracy serve as candidates to be used in adaptive sampling. We compare the resultant adaptive samplers based on their rate-distortion tradeoff using synthetic as well as natural data. For our experiments, we use a haptic device with a maximum force level of 3 N and 10 users. Each user is subjected to several piecewise constant haptic signals and is required to click a button whenever he perceives a change in the signal. For classification, we not only use classifiers based on level crossings and Weber's law but also random forests using a variety of causal signal features. The random forest typically yields the best prediction accuracy and a study of the importance of variables suggests that the level crossings and Weber's classifier features are most dominant. The classifiers based on level crossings and Weber's law have good accuracy (more than 90%) and are only marginally inferior to random forests. The level crossings classifier consistently outperforms the one based on Weber's law even though the gap is small. Given their simple parametric form, the level crossings and Weber's law-based classifiers are good candidates to be used for adaptive sampling. We study their rate-distortion performance and find that the level crossing sampler is superior. For example, for haptic signals obtained while exploring various rendered objects, for an average sampling rate of 10 samples per second, the level crossings adaptive sampler has a mean square error about 3dB less than the Weber sampler.
C1 [Bhardwaj, Amit; Chaudhuri, Subhasis] Indian Inst Technol, Dept Elect Engn, Bombay 400076, Maharashtra, India.
   [Dabeer, Onkar] Tata Inst Fundamental Res, Sch Technol & Comp Sci, Bombay 400005, Maharashtra, India.
C3 Indian Institute of Technology System (IIT System); Indian Institute of
   Technology (IIT) - Bombay; Tata Institute of Fundamental Research (TIFR)
RP Bhardwaj, A (corresponding author), Indian Inst Technol, Dept Elect Engn, Bombay 400076, Maharashtra, India.
EM bhardwajamit@ee.iitb.ac.in; sc@ee.iitb.ac.in; onkar@tcs.tifr.res.in
OI Bhardwaj, Amit/0000-0001-8590-2342
FU DST; DeiTY
FX We thank the referees for their constructive comments. We are also
   thankful to the DST and DeiTY for the partial fundings.
CR ANDERSON RJ, 1989, IEEE T AUTOMAT CONTR, V34, P494, DOI 10.1109/9.24201
   [Anonymous], 2001, P IEEE INT C MULT EX
   [Anonymous], 2006, 2006 IEEE INT WORKSH
   Barbagli F., 2006, ACM T APPL PERCEPT, V3, P125, DOI [10.1145/1141897.1141901, DOI 10.1145/1141897.1141901]
   Berestesky P, 2004, IEEE INT CONF ROBOT, P4557, DOI 10.1109/ROBOT.2004.1302436
   Bhardwaj A, 2014, IEEE HAPTICS SYM, P473, DOI 10.1109/HAPTICS.2014.6775501
   Bhardwaj Amit, 2014, P 2014 EUR HAPT C
   Bizo LA, 2006, BEHAV PROCESS, V71, P201, DOI 10.1016/j.beproc.2005.11.006
   Blewitt ME, 2008, NAT GENET, V40, P663, DOI 10.1038/ng.142
   Breiman L., 2001, Machine Learning, V45, P5, DOI 10.1023/A:1010933404324
   BRILL MH, 1983, B MATH BIOL, V45, P139, DOI 10.1007/BF02459392
   Chaudhuri Subhasis, 2014, ACM TAP 2014DATA
   Dabeer O, 2011, IEEE T SIGNAL PROCES, V59, P1868, DOI 10.1109/TSP.2010.2101071
   Gamble EA., 1898, AM J PSYCHOL, V10, P82
   Geomagic, 2012, PHANT OMN DEV
   Hayward V, 2011, PHILOS T R SOC B, V366, P3115, DOI 10.1098/rstb.2011.0150
   Hinterseer P, 2005, INT CONF ACOUST SPEE, P1097
   Hinterseer P, 2008, IEEE T SIGNAL PROCES, V56, P588, DOI 10.1109/TSP.2007.906746
   Hinterseer P, 2006, SYMPOSIUM ON HAPTICS INTERFACES FOR VIRTUAL ENVIRONMENT AND TELEOPERATOR SYSTEMS 2006, PROCEEDINGS, P35
   Hinterseer R., 2006, P 2006 INT C AC SPEE, V5
   Hirche S, 2005, IEEE INTL CONF CONTR, P328
   Hirche S, 2007, PRESENCE-TELEOP VIRT, V16, P523, DOI 10.1162/pres.16.5.523
   Jae-young Lee, 2011, 2011 IEEE World Haptics Conference (WHC 2011), P137, DOI 10.1109/WHC.2011.5945475
   Jarillo-Silva A, 2009, CERMA: 2009 ELECTRONICS ROBOTICS AND AUTOMOTIVE MECHANICS CONFERENCE, P193, DOI 10.1109/CERMA.2009.55
   Kadlecek P., 2011, P CENTR EUR SEM COMP
   KIRKPATRICK S, 1984, J STAT PHYS, V34, P975, DOI 10.1007/BF01009452
   Kohavi R., 1995, STUDY CROSS VALIDATI, DOI DOI 10.1067/MOD.2000.109031
   Kron A, 2004, IEEE INT CONF ROBOT, P1968, DOI 10.1109/ROBOT.2004.1308112
   Luce R.D., 2002, REPRESENTATIONAL MEA
   Mitchell T., 1997, Machine Learning, P52
   Moore B. C., 2007, COCHELEAR HEARING LO
   Myles H., 1999, NONPARAMETRIC STAT M, V2nd
   Pongrac Helena, 2006, P HUM CTR ROB SYST
   Rabiner L.R., 1993, FUNDAMENTALS SPEECH, VVolume 14
   Rosner B, 2006, BIOMETRICS, V62, P185, DOI 10.1111/j.1541-0420.2005.00389.x
   Sakr N, 2009, WORLD HAPTICS 2009: THIRD JOINT EUROHAPTICS CONFERENCE AND SYMPOSIUM ON HAPTIC INTERFACES FOR VIRTUAL ENVIRONMENT AND TELEOPERATOR SYSTEMS, PROCEEDINGS, P214, DOI 10.1109/WHC.2009.4810839
   Sakr N, 2009, IEEE T INSTRUM MEAS, V58, P1727, DOI 10.1109/TIM.2008.2009146
   Shahabi C, 2002, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL I AND II, PROCEEDINGS, P657, DOI 10.1109/ICME.2002.1035867
   Snyman JA, 2005, APPL OPTIM, V97, P1, DOI 10.1007/b105200
   Sreeni K. G., 2012, 2012 IEEE Haptics Symposium (HAPTICS), P333, DOI 10.1109/HAPTIC.2012.6183811
   Steinbach E, 2011, IEEE SIGNAL PROC MAG, V28, P87, DOI 10.1109/MSP.2010.938753
   Stiles W. S., 1978, MECH COLOUR VISION
   Tanaka Hiroyuki, 2010, Proceedings of the 11th IEEE International Workshop on Advanced Motion Control (AMC 2010), P756, DOI 10.1109/AMC.2010.5464034
   Vittorias I, 2009, WORLD HAPTICS 2009: THIRD JOINT EUROHAPTICS CONFERENCE AND SYMPOSIUM ON HAPTIC INTERFACES FOR VIRTUAL ENVIRONMENT AND TELEOPERATOR SYSTEMS, PROCEEDINGS, P208, DOI 10.1109/WHC.2009.4810811
NR 44
TC 10
Z9 10
U1 0
U2 8
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD JAN
PY 2015
VL 11
IS 4
AR 16
DI 10.1145/2670533
PG 20
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AZ6WJ
UT WOS:000348358400001
DA 2024-07-18
ER

PT J
AU Niu, YQ
   Todd, RM
   Kyan, M
   Anderson, AK
AF Niu, Yaqing
   Todd, Rebecca M.
   Kyan, Matthew
   Anderson, Adam K.
TI Visual and Emotional Salience Influence Eye Movements
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Algorithms; Design; Experimentation; Human Factors; Measurement;
   Performance; Theory; Emotional salience; visual salience; eye movements;
   attention; top-down; bottom-up
ID ATTENTION; FACES; FEATURES; SEARCH; SHIFTS; OVERT
AB In natural vision both stimulus features and cognitive/affective factors influence an observer's attention. However, the relationship between stimulus-driven (bottom-up) and cognitive/affective (top-down) factors remains controversial: How well does the classic visual salience model account for gaze locations? Can emotional salience counteract strong visual stimulus signals and shift attention allocation irrespective of bottom-up features? Here we compared Itti and Koch's [2000] and Spectral Residual (SR) visual salience model and explored the impact of visual salience and emotional salience on eye movement behavior, to understand the competition between visual salience and emotional salience and how they affect gaze allocation in complex scenes viewing. Our results show the insufficiency of visual salience models in predicting fixation. Emotional salience can override visual salience and can determine attention allocation in complex scenes. These findings are consistent with the hypothesis that cognitive/affective factors play a dominant role in active gaze control.
C1 [Niu, Yaqing; Todd, Rebecca M.] Univ Toronto, Dept Psychol, Affect & Cognit Lab, Toronto, ON M5S 3G3, Canada.
   [Kyan, Matthew] Ryerson Univ, Dept Elect & Comp Engn, Toronto, ON M5B 2K3, Canada.
   [Anderson, Adam K.] Univ Toronto, Dept Psychol, Affect & Cognit Lab, Toronto, ON M5S 3G3, Canada.
C3 University of Toronto; Toronto Metropolitan University; University of
   Toronto
RP Niu, YQ (corresponding author), Univ Toronto, Dept Psychol, Affect & Cognit Lab, 100 St George St, Toronto, ON M5S 3G3, Canada.
EM yaqing0930@hotmail.com
CR [Anonymous], 1999, Network: Computation in Neural Systems
   Birmingham E, 2009, VISION RES, V49, P2992, DOI 10.1016/j.visres.2009.09.014
   Birmingham E, 2009, VIS COGN, V17, P904, DOI 10.1080/13506280902758044
   Calvo MG, 2008, J EXP PSYCHOL GEN, V137, P471, DOI 10.1037/a0012771
   Calvo MG, 2009, COGNITION EMOTION, V23, P782, DOI 10.1080/02699930802151654
   Calvo MG, 2008, EXP PSYCHOL, V55, P359, DOI 10.1027/1618-3169.55.6.359
   CERF M., 2008, PREDICTING HUMAN GAZ, V20
   Cerf M, 2009, J VISION, V9, DOI 10.1167/9.12.10
   Corbetta M, 2002, NAT REV NEUROSCI, V3, P201, DOI 10.1038/nrn755
   Fox E, 2002, COGNITION EMOTION, V16, P355, DOI 10.1080/02699930143000527
   Henderson JM, 1999, J EXP PSYCHOL HUMAN, V25, P210, DOI 10.1037/0096-1523.25.1.210
   Humphrey K, 2012, J VISION, V12, DOI 10.1167/12.1.22
   Itti L, 2000, VISION RES, V40, P1489, DOI 10.1016/S0042-6989(99)00163-7
   Knight M, 2007, EMOTION, V7, P705, DOI 10.1037/1528-3542.7.4.705
   KOCH C, 1985, HUM NEUROBIOL, V4, P219
   Krieger G, 2000, SPATIAL VISION, V13, P201, DOI 10.1163/156856800741216
   LaBar KS, 2000, NEUROPSYCHOLOGIA, V38, P1734, DOI 10.1016/S0028-3932(00)00077-4
   LANG PJ, 1993, PSYCHOPHYSIOLOGY, V30, P261, DOI 10.1111/j.1469-8986.1993.tb03352.x
   Mannan SK, 1996, SPATIAL VISION, V10, P165, DOI 10.1163/156856896X00123
   Nummenmaa L, 2006, EMOTION, V6, P257, DOI 10.1037/1528-3542.6.2.257
   Nummenmaa L, 2009, J EXP PSYCHOL HUMAN, V35, P305, DOI 10.1037/a0013626
   Parkhurst D, 2002, VISION RES, V42, P107, DOI 10.1016/S0042-6989(01)00250-4
   Parkhurst DJ, 2003, SPATIAL VISION, V16, P125, DOI 10.1163/15685680360511645
   RIZZOLATTI G, 1987, NEUROPSYCHOLOGIA, V25, P31, DOI 10.1016/0028-3932(87)90041-8
   Rosenholtz R, 1999, VISION RES, V39, P3157, DOI 10.1016/S0042-6989(99)00077-2
   Rowe G, 2007, P NATL ACAD SCI USA, V104, P383, DOI 10.1073/pnas.0605198104
   Schmitz TW, 2009, J NEUROSCI, V29, P7199, DOI 10.1523/JNEUROSCI.5387-08.2009
   Stirk JA, 2007, J VISION, V7, DOI 10.1167/7.10.3
   Torralba A., 2003, J OPT SOC AM, V20
   UNDERWOOD G, 2007, CONSCIOUSNESS COGNIT
   ZHANG L., 2007, P OG C COMP VIS PATT
NR 31
TC 15
Z9 17
U1 1
U2 31
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD JUL
PY 2012
VL 9
IS 3
AR 13
DI 10.1145/2325722.2325726
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA 984EE
UT WOS:000307171700004
DA 2024-07-18
ER

PT J
AU Endres, D
   Christensen, A
   Omlor, L
   Giese, MA
AF Endres, Dominik
   Christensen, Andrea
   Omlor, Lars
   Giese, Martin A.
TI Emulating Human Observers with Bayesian Binning: Segmentation of Action
   Streams
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Algorithms; Human Factors; Theory; Motion capture; action segmentation;
   unsupervised learning; Bayesian methods
ID MOTION; INFERENCE
AB Natural body movements arise in the form of temporal sequences of individual actions. During visual action analysis, the human visual system must accomplish a temporal segmentation of the action stream into individual actions. Such temporal segmentation is also essential to build hierarchical models for action synthesis in computer animation. Ideally, such segmentations should be computed automatically in an unsupervised manner. We present an unsupervised segmentation algorithm that is based on Bayesian Binning (BB) and compare it to human segmentations derived from psychophysical data. BB has the advantage that the observation model can be easily exchanged. Moreover, being an exact Bayesian method, BB allows for the automatic determination of the number and positions of segmentation points. We applied this method to motion capture sequences from martial arts and compared the results to segmentations provided by humans from movies that showed characters that were animated with the motion capture data. Human segmentation was then assessed by an interactive adjustment paradigm, where participants had to indicate segmentation points by selection of the relevant frames. Results show a good agreement between automatically generated segmentations and human performance when the trajectory segments between the transition points were modeled by polynomials of at least third order. This result is consistent with theories about differential invariants of human movements.
C1 [Endres, Dominik; Christensen, Andrea; Omlor, Lars; Giese, Martin A.] Univ Tubingen, Univ Clin Tubingen, Dept Cognit Neurol, Sect Computat Sensomotor, D-72070 Tubingen, Germany.
   [Endres, Dominik; Christensen, Andrea; Omlor, Lars; Giese, Martin A.] Hertie Inst Computat Clin Brain Res, D-72070 Tubingen, Germany.
   [Endres, Dominik; Christensen, Andrea; Omlor, Lars; Giese, Martin A.] Ctr Integrat Neurosci, D-72070 Tubingen, Germany.
C3 Eberhard Karls University of Tubingen; Eberhard Karls University
   Hospital; Eberhard Karls University of Tubingen; Eberhard Karls
   University Hospital; Eberhard Karls University of Tubingen
RP Endres, D (corresponding author), Univ Tubingen, Univ Clin Tubingen, Dept Cognit Neurol, Sect Computat Sensomotor, Frondsbergstr 23, D-72070 Tubingen, Germany.
EM dominik.endres@klinikum.uni-tuebingen.de
OI Giese, Martin/0000-0003-1178-2768
FU EU [FP7-ICT-215866 SEARISE, FP7-249858-TP3 TANGO, FP7-ICT-248311
   AMARSi]; DFG
FX This work was supported by EU projects FP7-ICT-215866 SEARISE,
   FP7-249858-TP3 TANGO, FP7-ICT-248311 AMARSi and the DFG.
CR Agam Y, 2008, J VISION, V8, DOI 10.1167/8.1.11
   Arikan O, 2002, ACM T GRAPHIC, V21, P483, DOI 10.1145/566570.566606
   Barbic J, 2004, PROC GRAPH INTERF, P185
   Dickman H.R., 1963, The stream of behavior, P23, DOI DOI 10.1037/11177-002
   Endres D, 2005, IEEE T INFORM THEORY, V51, P3766, DOI 10.1109/TIT.2005.856954
   Endres D, 2010, J COMPUT NEUROSCI, V29, P149, DOI 10.1007/s10827-009-0157-3
   Fearnhead P, 2006, STAT COMPUT, V16, P203, DOI 10.1007/s11222-006-8450-8
   FLASH T, 1985, J NEUROSCI, V5, P1688, DOI 10.1523/jneurosci.05-07-01688.1985
   Green R. D., 2003, P IM VIS COMP NZ, P163
   Hutter M, 2007, BAYESIAN ANAL, V2, P635, DOI 10.1214/07-BA225
   Ilg W., 2004, International Journal of Humanoid Robotics, V1, P613
   Kovar L, 2002, ACM T GRAPHIC, V21, P473, DOI 10.1145/566570.566605
   Kschischang FR, 2001, IEEE T INFORM THEORY, V47, P498, DOI 10.1109/18.910572
   Marida K., 2000, Directional Statistics
   Nasrabadi N.M, 2007, Pattern recognition and machine learning, V16
   NEWTSON D, 1976, J EXP SOC PSYCHOL, V12, P436, DOI 10.1016/0022-1031(76)90076-7
   Petersen K. B., 2012, MATRIX COOKBOOK
   Polyakov F, 2009, BIOL CYBERN, V100, P159, DOI 10.1007/s00422-008-0287-0
   Roether CL, 2009, J VISION, V9, DOI 10.1167/9.6.15
   Rose C, 1998, IEEE COMPUT GRAPH, V18, P32, DOI 10.1109/38.708559
   Safonova A, 2004, ACM T GRAPHIC, V23, P514, DOI 10.1145/1015706.1015754
   Segouat J., 2009, Proceedings of Advances in Computer-Human Interactions, P369
   SHIPLEY TF, 2004, J VIS, V4
   Zacks JM, 2009, COGNITION, V112, P201, DOI 10.1016/j.cognition.2009.03.007
NR 24
TC 10
Z9 10
U1 2
U2 9
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD AUG
PY 2011
VL 8
IS 3
AR 16
DI 10.1145/2010325.2010326
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 819HK
UT WOS:000294815800001
DA 2024-07-18
ER

PT J
AU Li, B
   Xiong, WH
   Xu, D
   Bao, H
AF Li, Bing
   Xiong, Weihua
   Xu, De
   Bao, Hong
TI A Supervised Combination Strategy for Illumination Chromaticity
   Estimation
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Algorithms; Experimentation; Combination strategy; color constancy;
   illumination estimation; extreme learning machine
ID EXTREME LEARNING-MACHINE; COLOR CONSTANCY; CLASSIFICATION; MODEL
AB Color constancy is an important perceptual ability of humans to recover the color of objects invariant of light information. It is also necessary for a robust machine vision system. Until now, a number of color constancy algorithms have been proposed in the literature. In particular, the edge-based color constancy uses the edge of an image to estimate light color. It is shown to be a rich framework that can represent many existing illumination estimation solutions with various parameter settings. However, color constancy is an ill-posed problem; every algorithm is always given out under some assumptions and can only produce the best performance when these assumptions are satisfied. In this article, we have investigated a combination strategy relying on the Extreme Learning Machine (ELM) technique that integrates the output of edge-based color constancy with multiple parameters. Experiments on real image data sets show that the proposed method works better than most single-color constancy methods and even some current state-of-the-art color constancy combination strategies.
C1 [Li, Bing] Chinese Acad Sci, NLPR, Inst Automat, Beijing 100190, Peoples R China.
   [Xiong, Weihua] OmniVis Technol, Sunnyvale, CA 95014 USA.
   [Xu, De; Bao, Hong] Beijing Jiaotong Univ, Inst Comp Sci & Engn, Beijing 100044, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Automation, CAS; Beijing
   Jiaotong University
RP Li, B (corresponding author), Chinese Acad Sci, NLPR, Inst Automat, Beijing 100190, Peoples R China.
EM bjtulb@gmail.com
RI Bao, H/AAK-6001-2020
FU National Nature Science Foundation of China [61005030, 60825204,
   60935002, 60803072, 60972145]; China Postdoctoral Science Foundation; K.
   C. Wong Education Foundation, Hong Kong
FX This work is partly supported by National Nature Science Foundation of
   China (No. 61005030, 60825204, 60935002, 60803072, and 60972145) and
   China Postdoctoral Science Foundation.; The authors would like to thank
   Computational Vision Laboratory of Simon Fraser University for providing
   the image datasets. We are also very appreciative to the anonymous
   reviewers for their very valuable comments and suggestions. The author
   Bing Li also gratefully acknowledges the support of K. C. Wong Education
   Foundation, Hong Kong.
CR [Anonymous], 2007, Color Constancy
   [Anonymous], 2004, 2004 IEEE INT JOINT
   Barnard K, 2002, IEEE T IMAGE PROCESS, V11, P972, DOI 10.1109/TIP.2002.802531
   Barnard K, 2002, IEEE T IMAGE PROCESS, V11, P985, DOI 10.1109/TIP.2002.802529
   Barnard K, 2002, COLOR RES APPL, V27, P147, DOI 10.1002/col.10049
   BARNARD K, 2000, P 6 EUR C COMP VIS, P275
   Bartlett PL, 1998, IEEE T INFORM THEORY, V44, P525, DOI 10.1109/18.661502
   Bianco S, 2008, J ELECTRON IMAGING, V17, DOI 10.1117/1.2921013
   Brainard DH, 1997, J OPT SOC AM A, V14, P1393, DOI 10.1364/JOSAA.14.001393
   BUCHSBAUM G, 1980, J FRANKLIN I, V310, P1, DOI 10.1016/0016-0032(80)90058-7
   Cardei VC, 1999, SEVENTH COLOR IMAGING CONFERENCE: COLOR SCIENCE, SYSTEMS AND APPLICATIONS, P311
   Cardei VC, 2002, J OPT SOC AM A, V19, P2374, DOI 10.1364/JOSAA.19.002374
   Ciurea F, 2003, ELEVENTH COLOR IMAGING CONFERENCE: COLOR SCIENCE AND ENGINEERING - SYSTEMS, TECHNOLOGIES, APPLICATIONS, P160
   Finayson GD, 2001, IEEE T PATTERN ANAL, V23, P1209, DOI 10.1109/34.969113
   Finlayson GD, 2004, 12TH COLOR IMAGING CONFERENCE: COLOR SCIENCE AND ENGINEERING SYSTEMS, TECHNOLOGIES, APPLICATIONS, P37
   FINLAYSON GD, 1994, J OPT SOC AM A, V11, P3011, DOI 10.1364/JOSAA.11.003011
   Funt B, 2004, 12TH COLOR IMAGING CONFERENCE: COLOR SCIENCE AND ENGINEERING SYSTEMS, TECHNOLOGIES, APPLICATIONS, P47
   GIJSENIJ A, 2007, P IEEE COMP VIS PATT, P148
   Hordley SD, 2006, J OPT SOC AM A, V23, P1008, DOI 10.1364/JOSAA.23.001008
   Hordley SD, 2006, COLOR RES APPL, V31, P303, DOI 10.1002/col.20226
   Huang GB, 2003, IEEE T NEURAL NETWOR, V14, P274, DOI 10.1109/TNN.2003.809401
   Huang GB, 2006, NEUROCOMPUTING, V70, P489, DOI 10.1016/j.neucom.2005.12.126
   LAND EH, 1971, J OPT SOC AM, V61, P1, DOI 10.1364/JOSA.61.000001
   Li B, 2010, COLOR RES APPL, V35, P304, DOI 10.1002/col.20574
   Li MB, 2005, NEUROCOMPUTING, V68, P306, DOI 10.1016/j.neucom.2005.03.002
   Nizar AH, 2008, IEEE T POWER SYST, V23, P946, DOI 10.1109/TPWRS.2008.926431
   Rosenberg C, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P239, DOI 10.1109/ICCV.2001.937524
   Sapiro G, 1999, IEEE T PATTERN ANAL, V21, P1210, DOI 10.1109/34.809114
   Spitzer H, 2002, PATTERN RECOGN, V35, P1645, DOI 10.1016/S0031-3203(01)00160-1
   Van de Weijer J, 2007, IEEE T IMAGE PROCESS, V16, P2207, DOI 10.1109/TIP.2007.901808
   XIONG W, 2007, THESIS S FRASER U CA
   Xiong WH, 2006, J IMAGING SCI TECHN, V50, P341, DOI 10.2352/J.ImagingSci.Technol.(2006)50:4(341)
   Xiong W, 2007, FIFTEENTH COLOR IMAGING CONFERENCE: COLOR SCIENCE AND ENGINEERING SYSTEMS, TECHNOLOGIES, AND APPLICATIONS, FINAL PROGRAM AND PROCEEDINGS, P25
   Zhang RX, 2007, IEEE ACM T COMPUT BI, V4, P485, DOI 10.1109/TCBB.2007.1012
NR 34
TC 34
Z9 37
U1 0
U2 24
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD OCT
PY 2010
VL 8
IS 1
AR 5
DI 10.1145/1857893.1857898
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 748AJ
UT WOS:000289362100005
DA 2024-07-18
ER

PT J
AU Mourkoussis, N
   Rivera, FM
   Troscianko, T
   Dixon, T
   Hawkes, R
   Mania, K
AF Mourkoussis, Nicholaos
   Rivera, Fiona M.
   Troscianko, Tom
   Dixon, Tim
   Hawkes, Rycharde
   Mania, Katerina
TI Quantifying Fidelity for Virtual Environment Simulations Employing
   Memory Schema Assumptions
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Design; Experimentation; Human Factors; Computer graphics;
   human-computer interaction; visual cognition
ID REAL; RECOGNITION; KNOWLEDGE; SCENE
AB In a virtual environment (VE), efficient techniques are often needed to economize on rendering computation without compromising the information transmitted. The reported experiments devise a functional fidelity metric by exploiting research on memory schemata. According to the proposed measure, similar information would be transmitted across synthetic and real-world scenes depicting a specific schema. This would ultimately indicate which areas in a VE could be rendered in lower quality without affecting information uptake. We examine whether computationally more expensive scenes of greater visual fidelity affect memory performance after exposure to immersive VEs, or whether they are merely more aesthetically pleasing than their diminished visual quality counterparts. Results indicate that memory schemata function in VEs similar to real-world environments. "High-level" visual cognition related to late visual processing is unaffected by ubiquitous graphics manipulations such as polygon count and depth of shadow rendering; "normal" cognition operates as long as the scenes look acceptably realistic. However, when the overall realism of the scene is greatly reduced, such as in wireframe, then visual cognition becomes abnormal. Effects that distinguish schema-consistent from schema-inconsistent objects change because the whole scene now looks incongruent. We have shown that this effect is not due to a failure of basic recognition.
C1 [Mourkoussis, Nicholaos; Rivera, Fiona M.] Univ Sussex, Sch Informat, Brighton BN1 9QJ, E Sussex, England.
   [Troscianko, Tom; Dixon, Tim] Univ Bristol, Dept Expt Psychol, Bristol BS8 1TU, Avon, England.
   [Hawkes, Rycharde] Hewlett Packard Labs, Brighton BS34 8QZ, E Sussex, England.
   [Mania, Katerina] Tech Univ Crete, Dept Elect & Comp Engn, Khania 73100, Crete, Greece.
C3 University of Sussex; University of Bristol; Hewlett-Packard; Technical
   University of Crete
RP Mourkoussis, N (corresponding author), Univ Sussex, Sch Informat, Chichester 1 Room 010, Brighton BN1 9QJ, E Sussex, England.
EM N.Mourkoussis@sussex.ac.uk; F.M.Rivera@sussex.ac.uk;
   tom.troscianko@bristol.ac.uk; Timothy.Dixon@bristol.ac.uk;
   rycharde.hawkes@hp.com; k.mania@ced.tuc.gr
RI Mania, Katerina/AAO-7013-2021
FU EPSRC, U.K. [GR/S58386/01]
FX This work was supported by EPSRC, U.K., grant GR/S58386/01.
CR [Anonymous], 1932, REMEMBERING
   BREWER WF, 1981, COGNITIVE PSYCHOL, V13, P207, DOI 10.1016/0010-0285(81)90008-6
   Cater K., 2003, Eurographics Symposium on Rendering. 14th Eurographics Workshop on Rendering, P270
   FERWERDA J, 2001, P CAMPF PERC AD GRAP
   Ferwerda JA, 2003, PROC SPIE, V5007, P290, DOI 10.1117/12.473899
   Flannery KA, 2003, CYBERPSYCHOL BEHAV, V6, P151, DOI 10.1089/109493103321640347
   FRIEDMAN A, 1979, J EXP PSYCHOL GEN, V108, P316, DOI 10.1037/0096-3445.108.3.316
   GOODMAN GS, 1980, COGNITIVE PSYCHOL, V12, P473, DOI 10.1016/0010-0285(80)90017-1
   Haber J, 2001, COMPUT GRAPH FORUM, V20, pC142, DOI 10.1111/1467-8659.00507
   HOCK HS, 1978, MEM COGNITION, V6, P423, DOI 10.3758/BF03197475
   KUIPERS BJ, 1975, STUDIES COGNITIVE SC
   Land MF, 1999, J COMP PHYSIOL A, V185, P341, DOI 10.1007/s003590050393
   Liu G, 2005, J EXP PSYCHOL HUMAN, V31, P235, DOI 10.1037/0096-1523.31.2.235
   LOFTUS GR, 1978, J EXP PSYCHOL HUMAN, V4, P565, DOI 10.1037/0096-1523.4.4.565
   Mania K, 2003, PRESENCE-TELEOP VIRT, V12, P296, DOI 10.1162/105474603765879549
   Mania K, 2005, PRESENCE-TELEOP VIRT, V14, P606, DOI 10.1162/105474605774918769
   Mania Katerina., 2004, Proceedings of the 1st Symposium on Applied Perception in Graphics and Visualization, APGV '04, P39, DOI [10.1145/1012551.1012559, DOI 10.1145/1012551.1012559]
   MARMITT G, 2002, EUROGRAPHICS 2002, P217
   MCCONCIE GW, 1997, P 1 ADV DISPL INT DI, P25
   NEMIRE K, 1994, HUM FACTORS, V36, P79, DOI 10.1177/001872089403600105
   PEZDEK K, 1989, J EXP PSYCHOL LEARN, V15, P587, DOI 10.1037/0278-7393.15.4.587
   Rodger JC, 2000, IEEE COMPUT GRAPH, V20, P20, DOI 10.1109/38.824528
   ROJAHN K, 1992, BRIT J SOC PSYCHOL, V31, P81, DOI 10.1111/j.2044-8309.1992.tb00958.x
   SAAB Z, 1984, PSYCHOL REP, V54, P607, DOI 10.2466/pr0.1984.54.2.607
   WAGNER L, 1987, P S INT 3D GRAPH, P39
   Waller D, 1998, PRESENCE-TELEOP VIRT, V7, P129, DOI 10.1162/105474698565631
   Watson B, 2001, COMP GRAPH, P213, DOI 10.1145/383259.383283
   Watson B, 1997, PRESENCE-TELEOP VIRT, V6, P630, DOI 10.1162/pres.1997.6.6.630
   Wickens T. D., 2001, Elementary signal detection theory
NR 29
TC 13
Z9 14
U1 0
U2 8
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD OCT
PY 2010
VL 8
IS 1
AR 2
DI 10.1145/1857893.1857895
PG 21
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA 748AJ
UT WOS:000289362100002
DA 2024-07-18
ER

PT J
AU McDonnell, R
   Jörg, S
   McHugh, J
   Newell, FN
   O'Sullivan, C
AF McDonnell, Rachel
   Joerg, Sophie
   McHugh, Joanna
   Newell, Fiona N.
   O'Sullivan, Carol
TI Investigating the Role of Body Shape on the Perception of Emotion
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Experimentation; Human Factors; Perception; graphics; motion capture
ID BIOLOGICAL MOTION; ANTHROPOMORPHISM; AGENCY; REAL
AB In order to analyze the emotional content of motions portrayed by different characters, we created real and virtual replicas of an actor exhibiting six basic emotions: sadness, happiness, surprise, fear, anger, and disgust. In addition to the video of the real actor, his actions were applied to five virtual body shapes: a low- and high-resolution virtual counterpart, a cartoon-like character, a wooden mannequin, and a zombie-like character (Figures 1 and 2). In a point light condition, we also tested whether the absence of a body affected the perceived emotion of the movements. Participants were asked to rate the actions based on a list of 41 more complex emotions. We found that the perception of emotional actions is highly robust and to the most part independent of the character's body, so long as form is present. When motion alone is present, emotions were generally perceived as less intense than in the cases where form was present.
C1 [McDonnell, Rachel] Trinity Coll Dublin, Graph Res Grp, Dublin, Ireland.
C3 Trinity College Dublin
RP McDonnell, R (corresponding author), Trinity Coll Dublin, Graph Res Grp, Dublin, Ireland.
EM Rachel.McDonnell@cs.tsd.ie
RI Newell, Fiona/AIE-2422-2022
OI Newell, Fiona/0000-0002-7363-2346; McDonnell,
   Rachel/0000-0002-1957-2506; McHugh Power, Joanna/0000-0002-7387-3107;
   O'Sullivan, Carol/0000-0003-3772-4961
FU Science Foundation Ireland as part of the Metropolis Project
FX This research was sponsored by Science Foundation Ireland as part of the
   Metropolis Project.
CR Atkinson AP, 2004, PERCEPTION, V33, P717, DOI 10.1068/p5096
   Chaminade T, 2007, SOC COGN AFFECT NEUR, V2, P206, DOI 10.1093/scan/nsm017
   Coulson M, 2004, J NONVERBAL BEHAV, V28, P117, DOI 10.1023/B:JONB.0000023655.25550.be
   Crane E, 2007, LECT NOTES COMPUT SC, V4738, P95
   CUTTING JE, 1977, B PSYCHONOMIC SOC, V9, P353, DOI 10.3758/BF03337021
   EKMAN P, 1992, PSYCHOL REV, V99, P550, DOI 10.1037/0033-295X.99.3.550
   Han SH, 2005, NEUROIMAGE, V24, P928, DOI 10.1016/j.neuroimage.2004.09.046
   Hodgins JK, 1998, IEEE T VIS COMPUT GR, V4, P307, DOI 10.1109/2945.765325
   JOHANSSON G, 1973, PERCEPT PSYCHOPHYS, V14, P201, DOI 10.3758/BF03212378
   Mar RA, 2007, SOC COGN AFFECT NEUR, V2, P199, DOI 10.1093/scan/nsm011
   MATHER G, 1994, P ROY SOC B-BIOL SCI, V258, P273, DOI 10.1098/rspb.1994.0173
   MCDONNELL R, 2005, P 1 INT WORKSH CROWD, P101
   McDonnell R, 2007, APGV 2007: SYMPOSIUM ON APPLIED PERCEPTION IN GRAPHICS AND VISUALIZATION, PROCEEDINGS, P7
   Mori M., 1970, Energy, V7, P33, DOI [DOI 10.1109/MRA.2012.2192811, 10.1109/MRA.2012.2192811]
   Nowak KL, 2003, PRESENCE-TELEOP VIRT, V12, P481, DOI 10.1162/105474603322761289
   Pasch M, 2007, LECT NOTES COMPUT SC, V4738, P83
   Perani D, 2001, NEUROIMAGE, V14, P749, DOI 10.1006/nimg.2001.0872
   Plutchik R, 2001, AM SCI, V89, P344, DOI 10.1511/2001.4.344
   Reeves B., 1996, MEDIA EQUATION PEOPL
   Roether CL, 2008, CURR BIOL, V18, pR329, DOI 10.1016/j.cub.2008.02.044
   Slater M, 2002, COMP SUPP COMP W SER, P146
   Wallbott HG, 1998, EUR J SOC PSYCHOL, V28, P879, DOI 10.1002/(SICI)1099-0992(1998110)28:6<879::AID-EJSP901>3.0.CO;2-W
NR 22
TC 18
Z9 23
U1 0
U2 5
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD AUG
PY 2009
VL 6
IS 3
AR 14
DI 10.1145/1577755.1577757
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA 511ZS
UT WOS:000271212000002
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Munn, SM
   Pelz, JB
AF Munn, Susan M.
   Pelz, Jeff B.
TI FixTag: An Algorithm for Identifying and Tagging Fixations to Simplify
   the Analysis of Data Collected by Portable Eye Trackers
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Algorithms; Fixations; eye tracking; portable; wearable; coding
AB Video-based eye trackers produce an output video showing where a subject is looking, the subject's Point-of-Regard (POR), for each frame of a video of the scene. This information can be extremely valuable, but its analysis can be overwhelming. Analysis of eye-tracked data from portable (wearable) eye trackers is especially daunting, as the scene video may be constantly changing, rendering automatic analysis more difficult. A common way to begin analysis of POR data is to group these data into fixations. In a previous article, we compared the fixations identified (i.e., start and end marked) automatically by an algorithm to those identified manually by users (i.e., manual coders). Here, we extend this automatic identification of fixations to tagging each fixation to a Region-of-Interest (ROI). Our fixation tagging algorithm, FixTag, requires the relative 3D positions of the vertices of ROIs and calibration of the scene camera. Fixation tagging is performed by first calculating the camera projection matrices for keyframes of the scene video (captured by the eye tracker) via an iterative structure and motion recovery algorithm. These matrices are then used to project 3D ROI vertices into the keyframes. A POR for each fixation is matched to a point in the closest keyframe, which is then checked against the 2D projected ROI vertices for tagging. Our fixation tags were compared to those produced by three manual coders tagging the automatically identified fixations for two different scenarios. For each scenario, eight ROIs were defined along with the 3D positions of eight calibration points. Therefore, 17 tags were available for each fixation: 8 for ROIs, 8 for calibration points, and 1 for "other." For the first scenario, a subject was tracked looking through products on four store shelves, resulting in 182 automatically identified fixations. Our automatic tagging algorithm produced tags that matched those produced by at least one manual coder for 181 out of the 182 fixations (99.5% agreement). For the second scenario, a subject was tracked looking at two posters on adjoining walls of a room. Our algorithm matched at least one manual coder's tag for 169 fixations out of 172 automatically identified (98.3% agreement).
C1 [Munn, Susan M.; Pelz, Jeff B.] Rochester Inst Technol, Chester F Carlson Ctr Imaging Sci, Multidisciplinary Vis Res Lab, Rochester, NY 14623 USA.
C3 Rochester Institute of Technology
RP Munn, SM (corresponding author), Rochester Inst Technol, Chester F Carlson Ctr Imaging Sci, Multidisciplinary Vis Res Lab, Rochester, NY 14623 USA.
EM Smk8165@rit.edu
RI Pelz, Jeff B./A-8272-2009
OI Pelz, Jeff B./0000-0002-0649-3876
CR [Anonymous], 2001, Robotica, DOI DOI 10.1017/S0263574700223217
   [Anonymous], 2007, Eye tracking methodology: Theory and practice, DOI DOI 10.1007/978-3-319-57883-5
   [Anonymous], 2001, P ACM S VIRTUAL REAL
   Babcock J.S., 2004, ETRA 2004, P109
   Bouguet J.Y., 2007, CAMERA CALIBRATION T
   Carpenter Roger., 1988, Movements of the Eyes, V2
   Duchowski A. T., 2002, Proceedings ETRA 2002. Eye Tracking Research and Applications Symposium, P103, DOI 10.1145/507072.507094
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Harris C., 1988, P 4 ALV VIS C, V15, P10
   HUYNH D, 2004, SOFTWARE FUNCTIONS
   Irwin DavidE., 1992, EYE MOVEMENTS VISUAL, P146, DOI DOI 10.1007/978-1-4612-2852-3_9
   JUST MA, 1976, COGNITIVE PSYCHOL, V8, P441, DOI 10.1016/0010-0285(76)90015-3
   KNORR S, 2006, P EUR SIGN PROC C EU
   Kolakowski S., 2006, P 2006 S EYE TRACKIN, P79, DOI DOI 10.1145/1117309.1117348
   Kovesi P.D., 2007, MATLAB OCTAVE FUNCTI
   Li D., 2006, P 2006 S EYE TRACKIN, P95, DOI DOI 10.1145/1117309.1117350
   Li D., 2005, IEEE C COMPUTER VISI, V3, P79
   Li F, 2008, J MOD OPTIC, V55, P503, DOI 10.1080/09500340701467827
   Loy G, 2003, IEEE T PATTERN ANAL, V25, P959, DOI 10.1109/TPAMI.2003.1217601
   MERCHANT J, 1974, IEEE T BIO-MED ENG, VBM21, P309, DOI 10.1109/TBME.1974.324318
   MIERLE K, 2008, THESIS U TORONTO
   Morellec E., 2007, Review of Finance, V11, P1
   MUNN SM, 2009, P INT C ART INT PATT
   Munn SM, 2008, APGV 2008: PROCEEDINGS OF THE SYMPOSIUM ON APPLIED PERCEPTION IN GRAPHICS AND VISUALIZATION, P33
   Munn SM, 2008, PROCEEDINGS OF THE EYE TRACKING RESEARCH AND APPLICATIONS SYMPOSIUM (ETRA 2008), P181, DOI 10.1145/1344471.1344517
   Ohno T., 2004, Proc. ACM Symp. Eye Tracking Res. Appl, V22, P115, DOI [DOI 10.1145/968363.968387, 10.1145/968363.968387]
   Pelz JB, 2001, VISION RES, V41, P3587, DOI 10.1016/S0042-6989(01)00245-0
   Pollefeys M, 2004, INT J COMPUT VISION, V59, P207, DOI 10.1023/B:VISI.0000025798.50602.3a
   Repko J, 2005, FIFTH INTERNATIONAL CONFERENCE ON 3-D DIGITAL IMAGING AND MODELING, PROCEEDINGS, P150, DOI 10.1109/3DIM.2005.4
   Rothkopf ConstantinA., 2004, ETRA 04, P123, DOI DOI 10.1145/968363.968388
   SALYVUCCI DD, 2000, P S EY TRACK RES APP, P71
   Santella A., 2004, P S EYE TRACKING RES, P27, DOI DOI 10.1145/968363.968368
   Sun Feng-Mei, 2005, Acta Automatica Sinica, V31, P402
   Torr P, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P485, DOI 10.1109/ICCV.1998.710762
   Trucco E., 1998, INTRO TECHNIQUES 3D
   *VIS GEOM GROUP, 2005, MATL FUNCT MULT VIEW
NR 36
TC 7
Z9 12
U1 0
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD AUG
PY 2009
VL 6
IS 3
AR 16
DI 10.1145/1577755.1577759
PG 25
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 511ZS
UT WOS:000271212000004
DA 2024-07-18
ER

PT J
AU Reitsma, PSA
   O'Sullivan, C
AF Reitsma, Paul S. A.
   O'Sullivan, Carol
TI Effect of Scenario on Perceptual Sensitivity to Errors in Animation
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Experimentation; Human Factors; Measurement; Animation; graphics;
   perception; psychophysics
ID MOTION
AB A deeper understanding of what makes animation perceptually plausible would benefit a number of applications, such as approximate collision detection and goal-directed animation. In a series of psychophysical experiments, we examine how measurements of perceptual sensitivity in realistic physical simulations compare to similar measurements done in more abstract settings. We find that participant tolerance for certain types of errors is significantly higher in a realistic snooker scenario than in the abstract test settings previously used to examine those errors. By contrast, we find tolerance for errors displayed in realistic but more neutral environments was not different from tolerance for those errors in abstract settings. Additionally, we examine the interaction of auditory and visual cues in determining participant sensitivity to spatiotemporal errors in rigid body collisions. We find that participants are predominantly affected by visual cues. Finally, we find that tolerance for spatial gaps during collision events is constant for a wide range of viewing angles if the effect of foreshortening and occlusion caused by the viewing angle is taken into account.
C1 [Reitsma, Paul S. A.; O'Sullivan, Carol] Trinity Coll Dublin, Dublin, Ireland.
C3 Trinity College Dublin
RP Reitsma, PSA (corresponding author), Carnegie Mellon Univ, CS Dept, 5000 Forbes Ave, Pittsburgh, PA 15213 USA.
EM psar@cs.cmu.edu
OI O'Sullivan, Carol/0000-0003-3772-4961
CR Alais D, 2004, COGNITIVE BRAIN RES, V19, P185, DOI 10.1016/j.cogbrainres.2003.11.011
   [Anonymous], 1991, Detection theory: A user's guide
   Barzel Ronen, 1996, P EUR WORKSH COMP AN
   Chenney S, 2000, COMP GRAPH, P219, DOI 10.1145/344779.344882
   COHEN RL, 1964, UPPSALS SWEDEN LUNDE
   CORNSWEET T, 1962, AM J PSYCHIAT, V485, P491
   Hicheur H, 2005, EXP BRAIN RES, V162, P145, DOI 10.1007/s00221-004-2122-8
   Hodgins JK, 1998, IEEE T VIS COMPUT GR, V4, P307, DOI 10.1109/2945.765325
   KAISER MK, 1987, PERCEPT PSYCHOPHYS, V42, P275, DOI 10.3758/BF03203079
   LEVITT H, 1971, J ACOUST SOC AM, V49, P467, DOI 10.1121/1.1912375
   MCGURK H, 1976, NATURE, V264, P746, DOI 10.1038/264746a0
   Metzger W, 1934, PSYCHOL FORSCH, V19, P1, DOI 10.1007/BF02409733
   Michotte A., 1963, PERCEPTION CAUSALITY
   O'Sullivan C, 2003, ACM T GRAPHIC, V22, P527, DOI 10.1145/882262.882303
   O'Sullivan C, 2001, ACM T GRAPHIC, V20, P151, DOI 10.1145/501786.501788
   Oesker M, 2000, J VISUAL COMP ANIMAT, V11, P105, DOI 10.1002/1099-1778(200005)11:2<105::AID-VIS222>3.0.CO;2-Q
   Popovic Z, 2000, COMMUN ACM, V43, P50, DOI 10.1145/341852.341863
   Reitsma PSA, 2008, COMPUT GRAPH FORUM, V27, P201, DOI 10.1111/j.1467-8659.2008.01117.x
   Reitsma PSA, 2003, ACM T GRAPHIC, V22, P537, DOI 10.1145/882262.882304
   REITSMA PSA, 2008, P 5 S APPL PERC GRAP
   Sekuler R, 1997, NATURE, V385, P308, DOI 10.1038/385308a0
   STAPPERS PJ, 1993, B PSYCHONOMIC SOC, V31, P125
   TWIGG CD, 2007, ACM T GRAPHIC, V26
NR 23
TC 11
Z9 12
U1 0
U2 0
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD AUG
PY 2009
VL 6
IS 3
AR 15
DI 10.1145/1577755.1577758
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 511ZS
UT WOS:000271212000003
DA 2024-07-18
ER

PT J
AU Apfelbaum, H
   Pelah, A
   Peli, E
AF Apfelbaum, Henry
   Pelah, Adar
   Peli, Eli
TI Heading Assessment by "Tunnel Vision" Patients and Control Subjects
   Standing or Walking in a Virtual Reality Environment
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Experimentation; Human Factors; Measurement; Locomotion; optic flow; low
   vision; rehabilitation; retinitis pigmentosa (RP); surrogates;
   treadmill; tunnel vision; verisimilitude; walking
ID VISUAL-FIELD; NONVISUAL SIGNALS; AUGMENTED-VIEW; EYE-MOVEMENTS;
   DIRECTION; DEVICE; PERCEPTION; MOBILITY; SEARCH; MOTION
AB Virtual reality locomotion simulators are a promising tool for evaluating the effectiveness of vision aids to mobility for people with low vision. This study examined two factors to gain insight into the verisimilitude requirements of the test environment: the effects of treadmill walking and the suitability of using controls as surrogate patients. Ten "tunnel vision" patients with retinitis pigmentosa (RP) were tasked with identifying which side of a clearly visible obstacle their heading through the virtual environment would lead them and were scored both on accuracy and on their distance from the obstacle when they responded. They were tested both while walking on a treadmill and while standing, as they viewed a scene representing progress through a shopping mall. Control subjects, each wearing a head-mounted field restriction to simulate the vision of a paired patient, were also tested. At wide angles of approach, controls and patients performed with a comparably high degree of accuracy, and made their choices at comparable distances from the obstacle. At narrow angles of approach, patients' accuracy increased when walking, while controls' accuracy decreased. When walking, both patients and controls delayed their decisions until closer to the obstacle. We conclude that a head-mounted field restriction is not sufficient for simulating tunnel vision, but that the improved performance observed for walking compared to standing suggests that a walking interface (such as a treadmill) may be essential for eliciting natural perceptually guided behavior in virtual reality locomotion simulators.
C1 [Apfelbaum, Henry; Pelah, Adar; Peli, Eli] Harvard Univ, Sch Med, Schepens Eye Res Inst, Boston, MA 02114 USA.
   [Pelah, Adar] Univ York, Dept Elect, York YO10 5DD, N Yorkshire, England.
   [Pelah, Adar] Univ Cambridge, Dept Engn, Cambridge CB2 1PZ, England.
C3 Harvard University; Schepens Eye Research Institute; Harvard Medical
   School; University of York - UK; University of Cambridge
RP Apfelbaum, H (corresponding author), Harvard Univ, Sch Med, Schepens Eye Res Inst, 20 Staniford St, Boston, MA 02114 USA.
EM Henry.Apfelbaum@Schepens.Harvard.edu; ap23@york.ac.uk;
   Eli.Peli@Schepens.Harvard.edu
OI Peli, Eli/0000-0002-1340-9257
FU NIH [EY12890]
FX This work was supported in part by NIH grant EY12890 to E. Peli.
CR ANSTIS S, 1995, EXP BRAIN RES, V103, P476
   Barabas J, 2005, P SOC PHOTO-OPT INS, V5666, P424, DOI 10.1117/12.610855
   Bowers AR, 2004, OPHTHAL PHYSL OPT, V24, P296, DOI 10.1111/j.1475-1313.2004.00228.x
   Cornelissen F. W., 1999, VISUAL IMPAIRMENT RE, V1, P71, DOI [10.1076/vimr.1.2.71.4412, DOI 10.1076/VIMR.1.2.71.4412]
   CROWELL JA, 1993, PERCEPT PSYCHOPHYS, V53, P325, DOI 10.3758/BF03205187
   CUTTING JE, 1995, PSYCHOL REV, V102, P627, DOI 10.1037/0033-295X.102.4.627
   CUTTING JE, 1992, J EXP PSYCHOL GEN, V121, P41, DOI 10.1037/0096-3445.121.1.41
   Durgin FH, 2005, J EXP PSYCHOL HUMAN, V31, P339, DOI 10.1037/0096-1523.31.2.339
   Durgin FH, 1999, EXP BRAIN RES, V127, P12, DOI 10.1007/s002210050769
   Durgin FH, 2007, ACM T APPL PERCEPT, V4, DOI [10.1145/1227134.1227139, 10.1145/1227134/1227139]
   Fung J, 2006, CYBERPSYCHOL BEHAV, V9, P157, DOI 10.1089/cpb.2006.9.157
   Grant SC, 1998, HUM FACTORS, V40, P489, DOI 10.1518/001872098779591296
   Li L, 2002, OPTOMETRY VISION SCI, V79, P581, DOI 10.1097/00006324-200209000-00009
   LOVIEKITCHIN J, 1990, CLIN VISION SCI, V5, P249
   Luo G, 2006, J VIS, V6, P505, DOI DOI 10.1167/6.6.505.[
   Luo G, 2006, INVEST OPHTH VIS SCI, V47, P4152, DOI 10.1167/iovs.05-1672
   MARRON JA, 1982, AM J OPTOM PHYS OPT, V59, P413
   Mohler BJ, 2007, ACM T APPL PERCEPT, V4, DOI [10.1145/1227134.1227138, 10.1145/1227134/1227138]
   Pelah A, 1996, NATURE, V381, P283, DOI 10.1038/381283a0
   Pelah A., 2001, J VISUAL-JAPAN, V1, P307, DOI [10.1167/1.3.307, DOI 10.1167/1.3.307]
   Peli E, 2000, OPTOMETRY VISION SCI, V77, P453, DOI 10.1097/00006324-200009000-00006
   Ruddle RA, 2006, PSYCHOL SCI, V17, P460, DOI 10.1111/j.1467-9280.2006.01728.x
   Rushton SK, 1998, CURR BIOL, V8, P1191, DOI 10.1016/S0960-9822(07)00492-7
   Thurrell A, 2005, P SOC PHOTO-OPT INS, V5666, P434, DOI 10.1117/12.610856
   Thurrell AEI, 1998, PERCEPTION, V27, P147
   Turano KA, 2001, OPTOMETRY VISION SCI, V78, P667, DOI 10.1097/00006324-200109000-00012
   Vargas-Martín F, 2002, OPTOMETRY VISION SCI, V79, P715, DOI 10.1097/00006324-200211000-00009
   Vargas-Martín F, 2001, INVEST OPHTH VIS SCI, V42, pS858
   Vargas-Martín F, 2006, INVEST OPHTH VIS SCI, V47, P5295, DOI 10.1167/iovs.05-1043
   WARREN WH, 1992, PERCEPT PSYCHOPHYS, V51, P443, DOI 10.3758/BF03211640
   WARREN WH, 1988, NATURE, V336, P162, DOI 10.1038/336162a0
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
NR 36
TC 11
Z9 14
U1 2
U2 8
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD JAN
PY 2007
VL 4
IS 1
AR 8
DI 10.1145/1227134.1227142
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA V04IL
UT WOS:000207052000008
PM 18167511
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Brown, R
   Dutell, V
   Walter, B
   Rosenholtz, R
   Shirley, P
   McGuire, M
   Luebke, D
AF Brown, Rachel
   Dutell, Vasha
   Walter, Bruce
   Rosenholtz, Ruth
   Shirley, Peter
   McGuire, Morgan
   Luebke, David
TI Efficient Dataflow Modeling of Peripheral Encoding in the Human Visual
   System
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Human vision; perception; foveated rendering; image compression
ID RECEPTIVE-FIELDS; FEATURES; PERCEPTION; REPRESENTATION; ORIENTATION;
   STATISTICS; ATTENTION; CONTRAST; PREDICTS; LIMITS
AB Computer graphics seeks to deliver compelling images, generated within a computing budget, targeted at a specific display device, and ultimately viewed by an individual user. The foveated nature of human vision offers an opportunity to efficiently allocate computation and compression to appropriate areas of the viewer's visual field, of particular importance with the rise of high-resolution and wide field-of-view display devices. However, while variations in acuity and contrast sensitivity across the field of view have been well-studied and modeled, a more consequential variation concerns peripheral vision's degradation in the face of clutter, known as crowding. Understanding of peripheral crowding has greatly advanced in recent years, in terms of both phenomenology and modeling. Accurately leveraging this knowledge is critical for many applications, as peripheral vision covers a majority of pixels in the image. We advance computational models for peripheral vision aimed toward their eventual use in computer graphics. In particular, researchers have recently developed high-performing models of peripheral crowding, known as "pooling" models, which predict a wide range of phenomena but are computationally inefficient. We reformulate the problem as a dataflow computation, which enables faster processing and operating on larger images. Further, we account for the explicit encoding of "end stopped" features in the image, which was missing from previous methods. We evaluate our model in the context of perception of textures in the periphery, including a novel texture dataset and updated textural descriptors. Our improved computational framework may simplify development and testing of more sophisticated, complete models in more robust and realistic settings relevant to computer graphics.
C1 [Brown, Rachel; Shirley, Peter; McGuire, Morgan; Luebke, David] NVIDIA Res, 2788 San Tomas Expy, Santa Clara, CA 95051 USA.
   [Dutell, Vasha] Univ Calif Berkeley, Minor Hall, Berkeley, CA 94720 USA.
   [Walter, Bruce] Cornell Univ, 402 Bill & Melinda Gates Hall,107 Hoy Rd, Ithaca, NY 14850 USA.
   [Rosenholtz, Ruth] MIT, 32 Vassar St, Cambridge, MA 02139 USA.
C3 University of California System; University of California Berkeley;
   Cornell University; Massachusetts Institute of Technology (MIT)
RP Brown, R (corresponding author), NVIDIA Res, 2788 San Tomas Expy, Santa Clara, CA 95051 USA.
EM rabrown@nvidia.com; vasha@berkeley.edu; bruce.walter@cornell.edu;
   rruth@mit.edu; pshirley@nvidia.com; morgan3d@gmail.com;
   dluebke@nvidia.com
OI McGuire, Morgan/0000-0003-1074-0953; Walter, Bruce/0000-0003-2619-6120
CR Gatys LA, 2015, Arxiv, DOI arXiv:1505.07376
   [Anonymous], 2021, CADHATCH WEBSITE
   [Anonymous], 2021, TEXTURE CAN WEBSITE
   [Anonymous], 2009, J VISUAL-JAPAN, DOI [10.1167/9.8.784, DOI 10.1167/9.8.784]
   Arslan Ozgen Karagol, 2021, SHARE TEXTURES WEBSI
   Balas B, 2009, J VISION, V9, DOI 10.1167/9.12.13
   Balas BJ, 2006, VISION RES, V46, P299, DOI 10.1016/j.visres.2005.04.013
   Basinger James D., 1982, TECHNICAL REPORT
   BENNETT PJ, 1991, VISION RES, V31, P1759, DOI 10.1016/0042-6989(91)90025-Z
   Bex PJ, 2005, VISION RES, V45, P1385, DOI 10.1016/j.visres.2004.12.001
   BOUMA H, 1970, NATURE, V226, P177, DOI 10.1038/226177a0
   Brian, 2021, FREE PBR WEBSITE
   Brodatz P., 1966, TEXTURES PHOTOGRAPHI
   Chang HH, 2016, J VISION, V16, DOI 10.1167/16.10.13
   Cimpoi M, 2014, PROC CVPR IEEE, P3606, DOI 10.1109/CVPR.2014.461
   Cornelius Gaius., 2017, IMAGE CLOUDS DIRECTL
   Cusano C, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21031010
   Demes Lennart., 2021, CC0 TEXTURES WEBSITE
   Deza Arturo, 2017, Towards metamerism via foveated style transfer
   Ebbinger Chris., 2021, PIXEL FURANCE FREE G
   Ehinger Krista A, 2016, J Vis, V16, P13, DOI 10.1167/16.2.13
   Freeman J, 2011, NAT NEUROSCI, V14, P1195, DOI 10.1038/nn.2889
   FREEMAN WT, 1991, IEEE T PATTERN ANAL, V13, P891, DOI 10.1109/34.93808
   Gao Dashan, 2007, P IEEE 11 INT C COMP, P1
   Grenci Erica., 2021, PBR TEXTURE WEBSITE
   HEITGER F, 1992, VISION RES, V32, P963, DOI 10.1016/0042-6989(92)90039-L
   Herzog MH, 2015, J VISION, V15, DOI 10.1167/15.6.5
   Hoffmann Frederic., 2021, PUBLIC DOMAIN TEXTUR
   Holcombe AO, 2009, TRENDS COGN SCI, V13, P216, DOI 10.1016/j.tics.2009.02.005
   HUBEL DH, 1965, J NEUROPHYSIOL, V28, P229, DOI 10.1152/jn.1965.28.2.229
   HUBEL DH, 1959, J PHYSIOL-LONDON, V148, P574, DOI 10.1113/jphysiol.1959.sp006308
   IMAGE PROMOTION ASSOCIATION, 2021, SKETCH TEXTURE CLUBW
   JULESZ B, 1981, NATURE, V290, P91, DOI 10.1038/290091a0
   Kaplanyan AS, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356557
   Keshvari S., 2016, J VISION, V16, P641, DOI DOI 10.1167/16.12.641
   Keshvari S, 2016, J VISION, V16, DOI 10.1167/16.3.39
   Koenderink J, 2012, I-PERCEPTION, V3, P159, DOI 10.1068/i0490sas
   Korte W, 1923, Z PSYCHOL PHYSIOL SI, V93, P17
   Lettvin Jerome Y., 1976, The Sciences, V16, P10
   Levi DM, 2008, VISION RES, V48, P635, DOI 10.1016/j.visres.2007.12.009
   LEVI DM, 1985, VISION RES, V25, P963, DOI 10.1016/0042-6989(85)90207-X
   LEVI DM, 1986, NATURE, V320, P360, DOI 10.1038/320360a0
   LINDSTROM MJ, 1988, J AM STAT ASSOC, V83, P1014
   Liu F, 1996, IEEE T PATTERN ANAL, V18, P722, DOI 10.1109/34.506794
   Luo Ming Ronnier., 2015, CIELAB ENCYC COLOR S, V2015, P1
   MANN HB, 1947, ANN MATH STAT, V18, P50, DOI 10.1214/aoms/1177730491
   McGuire Morgan, 2017, G3D INNOVATION ENGIN
   Motoyoshi I, 2007, NATURE, V447, P206, DOI 10.1038/nature05724
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   Oliva A, 2003, IEEE IMAGE PROC, P253, DOI 10.1109/icip.2003.1246946
   Osterberg GA, 1935, ACTA OPHTHALMOL    S, V13, P1
   Parkes L, 2001, NAT NEUROSCI, V4, P739, DOI 10.1038/89532
   Paszke A, 2019, ADV NEUR IN, V32
   Patney A, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980246
   Paulo Joao, 2021, 3D TEXTURES WEBSITE
   Peirce J, 2019, BEHAV RES METHODS, V51, P195, DOI 10.3758/s13428-018-01193-y
   Pelli DG, 2008, NAT NEUROSCI, V11, P1129, DOI 10.1038/nn.2187
   Pelli DG, 2004, J VISION, V4, P1136, DOI 10.1167/4.12.12
   Poly Haven, 2021, TEXTURE HAVEN WEBSIT
   Portilla J, 2000, INT J COMPUT VISION, V40, P49, DOI 10.1023/A:1026553619983
   POTTER MC, 1976, J EXP PSYCHOL-HUM L, V2, P509, DOI 10.1037/0278-7393.2.5.509
   Rao AR, 1996, VISION RES, V36, P1649, DOI 10.1016/0042-6989(95)00202-2
   RENTSCHLER I, 1985, NATURE, V313, P308, DOI 10.1038/313308a0
   Rosen S, 2014, J VISION, V14, DOI 10.1167/14.6.10
   Rosenholtz R, 1999, VISION RES, V39, P3157, DOI 10.1016/S0042-6989(99)00077-2
   Rosenholtz R, 2019, J VISION, V19, DOI 10.1167/19.7.15
   Rosenholtz R, 2016, ANNU REV VIS SCI, V2, P437, DOI 10.1146/annurev-vision-082114-035733
   Rosenholtz R, 2012, J VISION, V12, DOI 10.1167/12.4.14
   Rosenholtz R, 2012, FRONT PSYCHOL, V3, DOI 10.3389/fpsyg.2012.00013
   Rosenholtz R, 2011, PROC SPIE, V7865, DOI 10.1117/12.876659
   Rosenholtz Ruth, 2015, TEXTURE PERCEPTION, DOI [10.1167/15.3.9, DOI 10.1167/15.3.9]
   ROVAMO J, 1978, NATURE, V271, P54, DOI 10.1038/271054a0
   Sareen P, 2016, BEHAV RES METHODS, V48, P1343, DOI 10.3758/s13428-015-0640-x
   Scott-Brown KC, 2000, VIS COGN, V7, P253, DOI 10.1080/135062800394793
   Siess Andreas., 2021, PATTERN PANDA WEBSIT
   Sillet Julio., 2021, TEXTURES WEBPAGE
   Simons DJ, 1997, TRENDS COGN SCI, V1, P261, DOI 10.1016/S1364-6613(97)01080-2
   Stommel John, 2016, COMPACT ORANGE PEPPE
   Strasburger H, 2014, PERCEPTION, V43, P963, DOI 10.1068/p7726
   Strasburger H, 2011, J VISION, V11, DOI 10.1167/11.5.13
   TAMURA H, 1978, IEEE T SYST MAN CYB, V8, P460, DOI 10.1109/TSMC.1978.4309999
   Texturebox Game Technologies Inc, 2021, TEXTURE BOX WEBSITE
   TREISMAN AM, 1980, COGNITIVE PSYCHOL, V12, P97, DOI 10.1016/0010-0285(80)90005-5
   Wallis TSA, 2019, ELIFE, V8, DOI 10.7554/eLife.42512
   Wallis TSA, 2017, J VISION, V17, DOI 10.1167/17.12.5
   Walton DR, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459943
   Williams L., 1983, Computer Graphics, V17, P1, DOI 10.1145/964967.801126
   ZETZSCHE C, 1990, VISION RES, V30, P1111, DOI 10.1016/0042-6989(90)90120-A
   Zetzsche Christof, 1993, P109
   Zgraggen Dorian., 2021, CG BOOKCASE WEBSITE
   Zhang XT, 2015, J VISION, V15, DOI 10.1167/15.3.9
NR 91
TC 1
Z9 1
U1 1
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD JAN
PY 2023
VL 20
IS 1
AR 1
DI 10.1145/3564605
PG 22
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 7S6AV
UT WOS:000910834800001
OA Green Submitted, hybrid
DA 2024-07-18
ER

PT J
AU Robb, A
   Kohm, K
   Porter, J
AF Robb, Andrew
   Kohm, Kristopher
   Porter, John
TI Experience Matters: Longitudinal Changes in Sensitivity to Rotational
   Gains in Virtual Reality
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article; Proceedings Paper
CT 19th Symposium on Applied Perception (SAP)
CY SEP, 2022
CL ELECTR NETWORK
DE Rotational gain; perception; longitudinal; virtual reality
ID PERFORMANCE; PERCEPTION; RETENTION; WALKING
AB Redirected walking techniques use rotational gains to guide users away from physical obstacles as theywalk in a virtualworld, effectively creating the illusion of a larger virtual space than is physically present. Designers often want to keep users unaware of this manipulation, which is made possible by limitations in human perception that render rotational gains imperceptible below a certain threshold. Many aspects of these thresholds have been studied; however, no research has yet considered whether these thresholds may change over time as users gain more experience with them. To study this, we recruited 20 novice VR users (no more than 1 hour of prior experience with an HMD) and provided them with an Oculus Quest to use for 4 weeks on their own time. They were tasked to complete an activity assessing their sensitivity to rotational gain once each week, in addition to whatever other activities they wanted to perform. No feedback was provided to participants about their performance during each activity, minimizing the possibility of learning effects accounting for any observed changes over time. We observed that participants became significantly more sensitive to rotation gains over time, underscoring the importance of considering prior user experience in applications involving rotational gain, aswell as howprior user experience may affect other, broader applications of VR.
C1 [Robb, Andrew; Kohm, Kristopher; Porter, John] Clemson Univ, Clemson, SC 29643 USA.
C3 Clemson University
RP Robb, A (corresponding author), Clemson Univ, Clemson, SC 29643 USA.
EM arobb@clemson.edu; kckohm@clemson.edu; jjporte@clemson.edu
OI Kohm, Kristopher/0000-0002-7525-991X; Robb, Andrew/0000-0002-0398-5576;
   Porter III, John/0000-0003-2534-3619
FU US National Science Foundation (CISE HCC) [1717937]; Direct For Computer
   & Info Scie & Enginr; Div Of Information & Intelligent Systems [1717937]
   Funding Source: National Science Foundation
FX This work was supported in part by US National Science Foundation (CISE
   HCC) grant #1717937.
CR [Anonymous], 2016, P 11 INT C DISABILIT
   Bailenson JN, 2006, PRESENCE-TELEOP VIRT, V15, P699, DOI 10.1162/pres.15.6.699
   Bates D, 2015, J STAT SOFTW, V67, P1, DOI 10.18637/jss.v067.i01
   Bingham GP, 1998, J EXP PSYCHOL HUMAN, V24, P145, DOI 10.1037/0096-1523.24.1.145
   Bingham GP, 1999, J EXP PSYCHOL HUMAN, V25, P1331
   Bruder G, 2012, IEEE T VIS COMPUT GR, V18, P538, DOI 10.1109/TVCG.2012.55
   Brument Hugo, 2020, Virtual Reality and Augmented Reality. 17th EuroVR International Conference, EuroVR 2020. Proceedings. Lecture Notes in Computer Science (LNCS 12499), P20, DOI 10.1007/978-3-030-62655-6_2
   Cesko C. Voeten, 2021, BUILDMER STEPWISE EL
   Dalton P, 2002, NAT NEUROSCI, V5, P199, DOI 10.1038/nn803
   Mendes FAD, 2012, PHYSIOTHERAPY, V98, P217, DOI 10.1016/j.physio.2012.06.001
   Duzmanska N, 2018, FRONT PSYCHOL, V9, DOI 10.3389/fpsyg.2018.02132
   Ebrahimi E, 2018, 25TH 2018 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P1, DOI 10.1109/VR.2018.8446539
   Freeman D, 2013, PSYCHOL MED, V43, P2673, DOI 10.1017/S003329171300038X
   Ginestet C, 2011, J ROY STAT SOC A, V174, P245, DOI 10.1111/j.1467-985X.2010.00676_9.x
   Han E., 2022, P 72 ANN INT COMMUNI
   Hernandez-Mocholi MA, 2016, J MUSCULOSKEL NEURON, V16, P12
   Hillel T., 2021, 21 SWISS TRANSP RES, P1, DOI [10.17863/CAM.40710, DOI 10.17863/CAM.40710]
   Huang XL, 2021, SCI REP-UK, V11, DOI 10.1038/s41598-021-81458-3
   Kennedy RS, 1993, The international journal of aviation psychology, V3, P203, DOI [DOI 10.1207/S15327108IJAP0303, 10.1207/s15327108ijap0303_3]
   Khojasteh Negar, 2021, FRONTIERS VIRTUAL RE, V2, P53, DOI DOI 10.3389/FRVIR.2021.643331
   Kim HJ, 2020, J MED INTERNET RES, V22, DOI 10.2196/23024
   KREFT IGG, 1995, MULTIVAR BEHAV RES, V30, P1, DOI 10.1207/s15327906mbr3001_1
   Krueger Charlene, 2004, Biol Res Nurs, V6, P151, DOI 10.1177/1099800404267682
   Kuhl SA, 2008, ACM T APPL PERCEPT, V5, DOI 10.1145/1402236.1402241
   Linares D, 2016, R J, V8, P122
   Meteyard L, 2020, J MEM LANG, V112, DOI 10.1016/j.jml.2020.104092
   Moustafa F, 2018, 24TH ACM SYMPOSIUM ON VIRTUAL REALITY SOFTWARE AND TECHNOLOGY (VRST 2018), DOI 10.1145/3281505.3281527
   Nilsson NC, 2016, P IEEE VIRT REAL ANN, P241, DOI 10.1109/VR.2016.7504743
   Oyanagi Akimi, 2021, Human Interface and the Management of Information Information Presentation and Visualization. Thematic Area, HIMI 2021. Held as Part of the 23rd HCI International Conference, HCII 2021. Proceedings. Lecture Notes in Computer Science (LNCS 12765), P322, DOI 10.1007/978-3-030-78321-1_25
   Paludan A, 2016, P IEEE VIRT REAL ANN, P259, DOI 10.1109/VR.2016.7504752
   Peck TC, 2020, IEEE T VIS COMPUT GR, V26, P1945, DOI 10.1109/TVCG.2020.2973498
   Peckmann C, 2022, COMPUT HUM BEHAV, V131, DOI 10.1016/j.chb.2022.107233
   Polat U, 2004, P NATL ACAD SCI USA, V101, P6692, DOI 10.1073/pnas.0401200101
   Porter J, 2019, CHI PLAY'19: PROCEEDINGS OF THE ANNUAL SYMPOSIUM ON COMPUTER-HUMAN INTERACTION IN PLAY, P277, DOI 10.1145/3311350.3347159
   Porter J, 2018, PROCEEDINGS OF THE 2018 ANNUAL SYMPOSIUM ON COMPUTER-HUMAN INTERACTION IN PLAY (CHI PLAY 2018), P405, DOI 10.1145/3242671.3242677
   Pournelle G. H., 1953, Journal of Mammalogy, V34, P133, DOI 10.1890/0012-9658(2002)083[1421:SDEOLC]2.0.CO;2
   Ricca A, 2021, 2021 IEEE VIRTUAL REALITY AND 3D USER INTERFACES (VR), P103, DOI 10.1109/VR50410.2021.00031
   Sakono H, 2021, IEEE T VIS COMPUT GR, V27, P4278, DOI 10.1109/TVCG.2021.3106501
   Serafin S, 2013, P IEEE VIRT REAL ANN, P161, DOI 10.1109/VR.2013.6549412
   Smith SJ, 2016, NURS EDUC PERSPECT, V37, P210, DOI 10.1097/01.NEP.0000000000000035
   Steinicke F., 2008, Proceedings of the ACM Symposium on Virtual Reality Software and Technology (VRST), P149, DOI 10.1145/1450579.1450611
   Steinicke F, 2010, IEEE T VIS COMPUT GR, V16, P17, DOI 10.1109/TVCG.2009.62
   Takala TM, 2016, INFORM EDUC, V15, P287, DOI 10.15388/infedu.2016.15
   Tarnanas I, 2013, JMIR SERIOUS GAMES, V1, P16, DOI 10.2196/games.2778
   Venkatesh V, 2002, PERS PSYCHOL, V55, P661, DOI 10.1111/j.1744-6570.2002.tb00125.x
   Williams NL, 2019, IEEE T VIS COMPUT GR, V25, P3158, DOI 10.1109/TVCG.2019.2932213
   Winkler-Schwartz A, 2016, J SURG EDUC, V73, P942, DOI 10.1016/j.jsurg.2016.04.013
   Zhang J, 2018, IEEE T VIS COMPUT GR, V24, P1671, DOI 10.1109/TVCG.2018.2793679
   Zhang R., 2013, Proc. Proceedings of the ACM Symposium on Applied Perception (SAP), P71
NR 49
TC 4
Z9 4
U1 0
U2 0
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD OCT
PY 2022
VL 19
IS 4
SI SI
AR 16
DI 10.1145/3560818
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 6J6QI
UT WOS:000886946700003
DA 2024-07-18
ER

PT J
AU Bressolette, B
   Denjean, S
   Roussarie, V
   Aramaki, M
   Ystad, S
   Kronland-Martinet, R
AF Bressolette, Benjamin
   Denjean, Sebastien
   Roussarie, Vincent
   Aramaki, Mitsuko
   Ystad, Solvi
   Kronland-Martinet, Richard
TI MovEcho: A Gesture-Sound Interface Allowing Blind Manipulations in a
   Driving Context
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Multisensory perception; cognitive load; virtual reality
ID INTUITIVE CONTROL; REAL
AB Most recent vehicles are equipped with touchscreens, which replace arrays of buttons that control secondary driving functions, such as temperature level, strength of ventilation, GPS, or choice of radio stations. While driving, manipulating such interfaces can be problematic in terms of safety, because they require the drivers' sight. In this article, we develop an innovative interface, MovEcho, which is piloted with gestures and associated with sounds that are used as informational feedback. We compare this interface to a touchscreen in a perceptual experiment that took place in a driving simulator. The results show that MovEcho allows for a better visual task completion related to traffic and is preferred by the participants. These promising results in a simulator condition have to be confirmed in future studies, in a real vehicle with a comparable expertise for both interfaces.
C1 [Bressolette, Benjamin; Aramaki, Mitsuko; Ystad, Solvi; Kronland-Martinet, Richard] Aix Marseille Univ, CNRS, PRISM Percept Representat Image Sound Mus, 31 Chemin J Aiguier, F-13402 Marseille 20, France.
   [Denjean, Sebastien; Roussarie, Vincent] Grp PSA, 2 Route Gisy, Velizy Villacoublay, France.
C3 Aix-Marseille Universite; Centre National de la Recherche Scientifique
   (CNRS)
RP Bressolette, B (corresponding author), Aix Marseille Univ, CNRS, PRISM Percept Representat Image Sound Mus, 31 Chemin J Aiguier, F-13402 Marseille 20, France.
EM bressolette@prism.cnrs.fr; sebastien.denjean@mpsa.com;
   vincent.roussarie@mpsa.com; aramaki@prism.cnrs.fr; ystad@prism.cnrs.fr;
   kronland@prism.cnrs.fr
FU French National Research Agency [ANR-14-CE24-0018-01]; Openlab PSA-AMU
   Automotive Motion Lab; Agence Nationale de la Recherche (ANR)
   [ANR-14-CE24-0018] Funding Source: Agence Nationale de la Recherche
   (ANR)
FX This work was funded by the French National Research Agency under the
   SoniMove project (Grant No. ANR-14-CE24-0018-01), and was supported by
   the Openlab PSA-AMU Automotive Motion Lab' part of the StelLab network.
CR Alonso-Arevalo MA, 2012, ACM T APPL PERCEPT, V9, DOI 10.1145/2355598.2355600
   [Anonymous], 2001, P HUM FACT ERG SOC A, DOI DOI 10.1177/154193120104502305
   Aramaki M, 2010, LECT NOTES COMPUT SC, V5954, P408, DOI 10.1007/978-3-642-12439-6_21
   Aramaki Mitsuko, 2009, P INT C AUD DISPL, P119
   Bazilinskyy P, 2018, ADV INTELL SYST, V597, P457, DOI 10.1007/978-3-319-60441-1_45
   Beaudouin-Lafon M., 2004, Proceedings of the working conference on Advanced visual interfaces, P15, DOI DOI 10.1145/989863.989865
   Blattner M. M., 1989, Human-Computer Interaction, V4, P11, DOI 10.1207/s15327051hci0401_1
   Bonneel N, 2010, ACM T APPL PERCEPT, V7, DOI 10.1145/1658349.1658350
   Bressolette B, 2018, IEEE CONSUM ELECTR M, V7, P91, DOI 10.1109/MCE.2017.2721678
   Buchhop K, 2017, AUTOMOTIVEUI 2017: PROCEEDINGS OF THE 9TH INTERNATIONAL CONFERENCE ON AUTOMOTIVE USER INTERFACES AND INTERACTIVE VEHICULAR APPLICATIONS, P21, DOI 10.1145/3122986.3123001
   Conan S, 2014, IEEE-ACM T AUDIO SPE, V22, P1260, DOI 10.1109/TASLP.2014.2327297
   Danna Jeremy, 2013, P 10 INT S COMPUTER, P200
   Del Signore Mauro, 1997, U.S. Patent, Patent No. [5682136A, 5682136]
   Dingus Thomas A., 2006, 100 CAR NATURALISTIC
   Engström J, 2005, TRANSPORT RES F-TRAF, V8, P97, DOI 10.1016/j.trf.2005.04.012
   FITTS PM, 1954, J EXP PSYCHOL, V47, P381, DOI 10.1037/h0055392
   Flowers J.H., 2005, ACM Transactions on Applied Perception, V2, P467472, DOI [DOI 10.1145/1101530.1101544, 10.1145/1101530.1101544]
   Fujimura K., 2013, P 5 INT C AUT US INT, P56
   GAVER WW, 1993, ECOL PSYCHOL, V5, P1, DOI 10.1207/s15326969eco0501_1
   GOPHER D, 1984, HUM FACTORS, V26, P519, DOI 10.1177/001872088402600504
   Green Paul, 1999, UMTRI9816 U MICH TRA
   Guna J, 2014, SENSORS-BASEL, V14, P3702, DOI 10.3390/s140203702
   Harbluk J. L., 2002, 13889E TP
   Hermann T., 2005, ACM T APPL PERCEPT T, V2, P559, DOI [10.1145/1101530.1101557, DOI 10.1145/1101530.1101557]
   Houben MMJ, 2004, SPEECH COMMUN, V43, P331, DOI 10.1016/j.specom.2004.03.004
   Hurwitz J.B., 2002, Proceedings of the Human Factors and Ergonomics Society Annual Meeting, V46, P1804, DOI [10.1177/154193120204602206, DOI 10.1177/154193120204602206]
   Kramer Gregory, 1999, P INT C AUD DISPL
   Lauber Felix., 2014, Proceedings of the 2014 conference on Designing interactive systems, P171, DOI [DOI 10.1145/2598510, 10.1145/2598510.2598521]
   Lee SH, 2020, J COMPUT DES ENG, V7, P700, DOI 10.1093/jcde/qwaa052
   Lemaitre G, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0181786
   Liang YL, 2010, ACCIDENT ANAL PREV, V42, P881, DOI 10.1016/j.aap.2009.05.001
   MacKenzie I. S., 1992, Human-Computer Interaction, V7, P91, DOI 10.1207/s15327051hci0701_3
   Meister D., 1976, BEHAV FDN SYSTEM DEV
   MUCKLER FA, 1992, HUM FACTORS, V34, P441, DOI 10.1177/001872089203400406
   Nymoen Kristian, 2011, MM 11 P 2011 ACM MUL, P39, DOI [10.1145/2072529.2072541, DOI 10.1145/2072529.2072541]
   Occelli V, 2009, NEUROREPORT, V20, P793, DOI 10.1097/WNR.0b013e32832b8069
   Pickering C.A., 2007, Automotive Electronics, 2007 3rd Institution of Engineering and Technology Conference on, P1
   Pollock E, 2002, LEARN INSTR, V12, P61, DOI 10.1016/S0959-4752(01)00016-0
   Santos J, 2005, TRANSPORT RES F-TRAF, V8, P135, DOI 10.1016/j.trf.2005.04.001
   Spence C, 2011, ATTEN PERCEPT PSYCHO, V73, P971, DOI 10.3758/s13414-010-0073-7
   Sweller J, 1998, EDUC PSYCHOL REV, V10, P251, DOI 10.1023/A:1022193728205
   Törnros JEB, 2005, ACCIDENT ANAL PREV, V37, P902, DOI 10.1016/j.aap.2005.04.007
   van Merriënboer JJG, 2002, ETR&D-EDUC TECH RES, V50, P39, DOI 10.1007/BF02504993
   Walker B. N., 2005, ACM Transactions on Applied Perception, V2, P407, DOI [10.1145/1101530.1101534, DOI 10.1145/1101530.1101534]
   Walker BN, 2002, J EXP PSYCHOL-APPL, V8, P211, DOI 10.1037/1076-898X/8.4.211
   Walker Bruce N., 2005, ACM T APPL PERCEPT, V2, P413
   Zijlstra F.R.H., 1993, Efficiency in Work Behavior
NR 47
TC 1
Z9 1
U1 0
U2 7
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD JUL
PY 2021
VL 18
IS 3
AR 15
DI 10.1145/3464692
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA UD5LZ
UT WOS:000687249300006
DA 2024-07-18
ER

PT J
AU Adhanom, IB
   Al-Zayer, M
   Macneilage, P
   Folmer, E
AF Adhanom, Isayas Berhe
   Al-Zayer, Majed
   Macneilage, Paul
   Folmer, Eelke
TI Field-of-View Restriction to Reduce VR Sickness Does Not Impede Spatial
   Learning in Women
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Virtual reality; VR sickness; virtual locomotion; field-of-view
   manipulation
ID MOTION SICKNESS; VIRTUAL ENVIRONMENTS; SEX-DIFFERENCES; SIMULATOR
   SICKNESS; PERFORMANCE; NAVIGATION; GENDER; MEMORY; DISPLAY; VECTION
AB Women are more likely to experience virtual reality (VR) sickness than men, which could pose a major challenge to the mass market success of VR. Because VR sickness often results from a visual-vestibular conflict, an effective strategy to mitigate conflict is to restrict the user's field-of-view (FOV) during locomotion. Sex differences in spatial cognition have been well researched, with several studies reporting that men exhibit better spatial navigation performance in desktop three-dimensional environments than women. However, additional research suggests that this sex difference can be mitigated by providing a larger FOV as this increases the availability of landmarks, which women tend to rely on more than men. Though FOV restriction is already a widely used strategy for VR headsets to minimize VR sickness, it is currently not well understood if it impedes spatial learning in women due to decreased availability of landmarks. Our study (n = 28, 14 men and 14 women) found that a dynamic FOV restrictor was equally effective in reducing VR sickness in both sexes, and no sex differences in VR sickness incidence were found. Our study did find a sex difference in spatial learning ability, but an FOV restrictor did not impede spatial learning in either sex.
C1 [Adhanom, Isayas Berhe; Al-Zayer, Majed; Folmer, Eelke] Univ Nevada, Dept Comp Sci & Engn, Reno, NV 89557 USA.
   [Macneilage, Paul] Univ Nevada, Dept Psychol, Reno, NV 89557 USA.
C3 Nevada System of Higher Education (NSHE); University of Nevada Reno;
   Nevada System of Higher Education (NSHE); University of Nevada Reno
RP Adhanom, IB (corresponding author), Univ Nevada, Dept Comp Sci & Engn, Reno, NV 89557 USA.
EM iadhanom@nevada.unr.edu; malzayer@cse.unr.edu; pmacneilage@unr.edu;
   efolmer@unr.edu
OI Adhanom, Isayas/0000-0003-4798-7415; Al Zayer, Majed/0000-0002-7704-1308
FU NSF [1911041]; NIH COBRE Award [P20GM103650]; Direct For Computer & Info
   Scie & Enginr; Div Of Information & Intelligent Systems [1911041]
   Funding Source: National Science Foundation
FX This research project was supported by NSF grant 1911041 and NIH COBRE
   Award P20GM103650 as well as gifts from Google and Mozilla.
CR Al Zayer M, 2020, IEEE T VIS COMPUT GR, V26, P2315, DOI 10.1109/TVCG.2018.2887379
   Al Zayer Majed., 2019, Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems, P1
   [Anonymous], 1975, Motion sickness
   Astur RS, 1998, BEHAV BRAIN RES, V93, P185, DOI 10.1016/S0166-4328(98)00019-9
   Bardzell S, 2010, CHI2010: PROCEEDINGS OF THE 28TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1-4, P1301
   Bhandari J., 2018, P 44 GRAPHICS INTERF, P162, DOI [DOI 10.20380/GI2018.22, 10.20380/GI2018.223, DOI 10.20380/GI2018.223]
   Biocca Frank., 1992, Presence: Teleoperators Virtual Environments, V1, P334, DOI [DOI 10.1162/PRES.1992.1.3.334, 10.1162/pres.1992.1.3.334]
   Bolas Mark, 2017, US Patent, Patent No. [9,645,395, 9645395]
   Bos JE, 2010, APPL ERGON, V41, P516, DOI 10.1016/j.apergo.2009.11.007
   Bowman DA, 1997, P IEEE VIRT REAL ANN, P45, DOI 10.1109/VRAIS.1997.583043
   Burgess N, 2006, TRENDS COGN SCI, V10, P551, DOI 10.1016/j.tics.2006.10.005
   Cherep LA, 2020, J EXP PSYCHOL-APPL, V26, P480, DOI 10.1037/xap0000263
   Clernes SA, 2005, J BIOL RHYTHM, V20, P71, DOI 10.1177/0748730404272567
   COHEN J, 1992, PSYCHOL BULL, V112, P155, DOI 10.1037/0033-2909.112.1.155
   Czerwinski M., 2002, ACM C HUMAN FACTORS, P195, DOI DOI 10.1145/503376.503412
   Dabbs JM, 1998, EVOL HUM BEHAV, V19, P89, DOI 10.1016/S1090-5138(97)00107-4
   Darken R. P., 1997, Proceedings of the ACM Symposium on User Interface Software and Technology. 10th Annual Symposium. UIST '97, P213, DOI 10.1145/263407.263550
   Driscoll I, 2005, HORM BEHAV, V47, P326, DOI 10.1016/j.yhbeh.2004.11.013
   Duh HBL, 2004, PRESENCE-TELEOP VIRT, V13, P578, DOI 10.1162/1054746042545283
   Ebenholtz S.M., 1992, Teleoperators and Virtual Environments, V1, P302, DOI DOI 10.1162/PRES.1992.1.3.302
   Faul F, 2007, BEHAV RES METHODS, V39, P175, DOI 10.3758/BF03193146
   Ferguson TD, 2019, BEHAV BRAIN RES, V364, P281, DOI 10.1016/j.bbr.2019.02.032
   Fernandes AS, 2016, 2016 IEEE SYMPOSIUM ON 3D USER INTERFACES (3DUI), P201, DOI 10.1109/3DUI.2016.7460053
   Fernandes KJ, 2003, COMMUN ACM, V46, P141, DOI 10.1145/903893.903929
   Flanagan MB, 2005, AVIAT SPACE ENVIR MD, V76, P642
   Garcia A., 2010, Proceedings of the Human Factors and Ergonomics Society Annual Meeting, V54, P1551, DOI [DOI 10.1177/154193121005401941, 10.1518/107118110X12829370088967, DOI 10.1518/107118110X12829370088967]
   Golding JF, 2006, AUTON NEUROSCI-BASIC, V129, P67, DOI 10.1016/j.autneu.2006.07.019
   Gordon CC., 2014, 2012 Anthropometric Survey Of U.S. Army Personnel: Methods And Summary Statistics
   Graeber DA., 2002, Proceedings of the Human Factors and Ergonomics Society 46th Annual Meeting, P2109, DOI [10.1177/154193120204602602, DOI 10.1177/154193120204602602]
   Griffin NN, 2019, 25TH ACM SYMPOSIUM ON VIRTUAL REALITY SOFTWARE AND TECHNOLOGY (VRST 2019), DOI 10.1145/3359996.3364243
   Henson D.B., 2000, Visual Fields
   Johnson VE, 2013, P NATL ACAD SCI USA, V110, P19313, DOI 10.1073/pnas.1313476110
   Kelly JW, 2020, IEEE T VIS COMPUT GR, V26, P1841, DOI 10.1109/TVCG.2020.2973051
   Kemeny A., 2017, The Engineering Reality of Virtual Reality, P48
   Kennedy RS, 1993, The international journal of aviation psychology, V3, P203, DOI [DOI 10.1207/S15327108IJAP0303, 10.1207/s15327108ijap0303_3]
   Keshavarz B., 2014, HDB VIRTUAL ENV DESI, P648
   Keshavarz B, 2015, FRONT PSYCHOL, V6, DOI 10.3389/fpsyg.2015.00472
   Keshavarz B, 2011, DISPLAYS, V32, P181, DOI 10.1016/j.displa.2011.05.009
   Kolasinski E. M., 1995, SIMULATOR SICKNESS V
   La Viola J. J.  Jr., 2000, SIGCHI Bulletin, V32, P47, DOI 10.1145/333329.333344
   Lapointe JF, 2002, FIRST INTERNATIONAL SYMPOSIUM ON 3D DATA PROCESSING VISUALIZATION AND TRANSMISSION, P490, DOI 10.1109/TDPVT.2002.1024104
   Lin JJW, 2002, P IEEE VIRT REAL ANN, P164, DOI 10.1109/VR.2002.996519
   Llorach Gerard., 2014, P 20 ACM S VIRTUAL R, P137, DOI DOI 10.1145/2671015.2671120
   LOOMIS JM, 1993, J EXP PSYCHOL GEN, V122, P73, DOI 10.1037/0096-3445.122.1.73
   Loomis JM, 2001, OPTOMETRY VISION SCI, V78, P282, DOI 10.1097/00006324-200105000-00011
   McCauley M. E., 1992, Presence: Teleoperators & Virtual Environments, V1, P311, DOI DOI 10.1162/PRES.1992.1.3.311
   McCullough M, 2016, SAP 2015: ACM SIGGRAPH SYMPOSIUM ON APPLIED PERCEPTION, P107, DOI 10.1145/2804408.2804416
   Merhi O, 2007, HUM FACTORS, V49, P920, DOI 10.1518/001872007X230262
   MORRIS RGM, 1981, LEARN MOTIV, V12, P239, DOI 10.1016/0023-9690(81)90020-5
   Munafo J., 2016, EXP BRAIN RES, P1
   Ni T, 2006, PROC GRAPH INTERF, P139
   Nie GY, 2017, ADJUNCT PROCEEDINGS OF THE 2017 IEEE INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY (ISMAR-ADJUNCT), P75, DOI 10.1109/ISMAR-Adjunct.2017.35
   Nori R, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00204
   Oculus, 2017, OC BEST PRACT MOT
   Park G.R., 2006, Proceedings of the Human Factors and Ergonomics Society 50 Annual Meeting, P2702, DOI DOI 10.1177/154193120605002607
   Piber D, 2018, BEHAV BRAIN RES, V336, P44, DOI 10.1016/j.bbr.2017.08.034
   Plouzeau J., 2015, P INT C ARTIFICIAL R, P1
   Polys NF, 2007, COMPUT ANIMAT VIRT W, V18, P19, DOI 10.1002/cav.159
   Prothero JD, 1999, AVIAT SPACE ENVIR MD, V70, P277
   Rebenitsch L, 2016, VIRTUAL REAL-LONDON, V20, P101, DOI 10.1007/s10055-016-0285-9
   RICCIO G E, 1991, Ecological Psychology, V3, P195, DOI 10.1207/s15326969eco0303_2
   Rieser JohnJ., 1982, Journal of Visual Impairment and Blindness
   Rine RM, 1999, PHYS THER, V79, P949, DOI 10.1093/ptj/79.10.949
   Sandstrom NJ, 1998, COGNITIVE BRAIN RES, V6, P351, DOI 10.1016/S0926-6410(98)00002-0
   Seay AF, 2001, P IEEE VIRT REAL ANN, P299, DOI 10.1109/VR.2001.913806
   Stanney K, 2020, FRONT ROBOT AI, V7, DOI 10.3389/frobt.2020.00004
   Stanney KM, 2003, HUM FACTORS, V45, P504, DOI 10.1518/hfes.45.3.504.27254
   Stanney KM, 1998, PRESENCE-TELEOP VIRT, V7, P447, DOI 10.1162/105474698565848
   Starrett MJ, 2018, FRONT HUM NEUROSCI, V12, DOI 10.3389/fnhum.2018.00281
   Swapp D, 2010, IEEE SYMPOSIUM ON 3D USER INTERFACES (3DUI 2010), P71, DOI 10.1109/3DUI.2010.5444717
   Tan DS, 2006, HUM FACTORS, V48, P318, DOI 10.1518/001872006777724381
   Tanaka Nobuhisa, 2004, J Physiol Anthropol Appl Human Sci, V23, P313, DOI 10.2114/jpa.23.313
   Thompson Luke., Unity vr tunneling
   Tregillus S, 2016, 34TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, CHI 2016, P1250, DOI 10.1145/2858036.2858084
   TREISMAN M, 1977, SCIENCE, V197, P493, DOI 10.1126/science.301659
   Venkatakrishnan R, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR 2020), P672, DOI [10.1109/VR46266.2020.00-14, 10.1109/VR46266.2020.1581256520838]
   Viaud-Delmon I, 1998, NAT NEUROSCI, V1, P15, DOI 10.1038/215
   Vive, 2017, HTC VIVE SURV
   VOYER D, 1995, PSYCHOL BULL, V117, P250, DOI 10.1037/0033-2909.117.2.250
   Webb NA, 2003, AVIAT SPACE ENVIR MD, V74, P622
   WELLS MJ, 1990, OPT ENG, V29, P870, DOI 10.1117/12.55672
   Woolley DG, 2010, BEHAV BRAIN RES, V208, P408, DOI 10.1016/j.bbr.2009.12.019
NR 82
TC 12
Z9 13
U1 0
U2 11
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD JUN
PY 2021
VL 18
IS 2
AR 5
DI 10.1145/3448304
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SR6EX
UT WOS:000661137000001
DA 2024-07-18
ER

PT J
AU Li, BC
   Walker, J
   Kuhl, SA
AF Li, Bochao
   Walker, James
   Kuhl, Scott A.
TI The Effects of Peripheral Vision and Light Stimulation on Distance
   Judgments Through HMDs
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Distance perception; blind walking; peripheral vision; field of view;
   oculus rift HMDs; DK2
ID VIEWING CONDITIONS; PERCEPTION; LOCOMOTION; GRAPHICS; QUALITY; REAL
AB Egocentric distances are often underestimated in virtual environments through head-mounted displays (HMDs). Previous studies suggest that peripheral vision can influence distance perception. Specifically, light in the periphery may improve distance judgments in HMDs. In this study, we conducted a series of experiments with varied peripheral treatments around the viewport. First, we found that the peripheral brightness significantly influences distance judgments when the periphery is brighter than a certain threshold, and found a possible range where the threshold was in. Second, we extended our previous research by changing the size of the peripheral treatment. A larger visual field (field of view of the HMD) resulted in significantly more accurate distance judgments compared to our original experiments with black peripheral treatment. Last, we found that applying a pixelated peripheral treatment can also improve distance judgments. The result implies that augmenting peripheral vision with secondary low-resolution displays may improve distance judgments in HMDs.
C1 [Li, Bochao; Walker, James; Kuhl, Scott A.] Michigan Technol Univ, 1400 Townsend Dr, Houghton, MI 49931 USA.
C3 Michigan Technological University
RP Li, BC (corresponding author), Michigan Technol Univ, 1400 Townsend Dr, Houghton, MI 49931 USA.
EM bochaol@mtu.edu; jwwalker@mtu.edu; kuhl@mtu.edu
CR Andre J, 2006, PERCEPT PSYCHOPHYS, V68, P353, DOI 10.3758/BF03193682
   [Anonymous], 2008, P 2008 ACM S VIRTUAL, DOI DOI 10.1145/1450579.1450614
   Boring E.G. E., 1948, Foundations of psychology.
   Buck LE, 2018, ACM T APPL PERCEPT, V15, DOI 10.1145/3196885
   Creem-Regehr SH, 2016, SAP 2015: ACM SIGGRAPH SYMPOSIUM ON APPLIED PERCEPTION, P47, DOI 10.1145/2804408.2804422
   Creem-Regehr SH, 2005, PERCEPTION, V34, P191, DOI 10.1068/p5144
   Grechkin TY, 2010, ACM T APPL PERCEPT, V7, DOI 10.1145/1823738.1823744
   Interrante V, 2008, PRESENCE-TELEOP VIRT, V17, P176, DOI 10.1162/pres.17.2.176
   Jones J.A., 2011, Proc. Symposium on Applied perception in Graphics and Visualization, P29
   Jones JA, 2017, ACM T APPL PERCEPT, V14, DOI 10.1145/2983631
   Jones JA, 2013, IEEE T VIS COMPUT GR, V19, P701, DOI 10.1109/TVCG.2013.37
   Knapp JM, 2004, PRESENCE-TELEOP VIRT, V13, P572, DOI 10.1162/1054746042545238
   Kuhl SA, 2009, ACM T APPL PERCEPT, V6, DOI 10.1145/1577755.1577762
   Kunz BR, 2009, ATTEN PERCEPT PSYCHO, V71, P1284, DOI 10.3758/APP.71.6.1284
   Li B, 2016, PROCEEDINGS 2016 INTERNATIONAL CONFERENCE ON NETWORKING AND NETWORK APPLICATIONS NANA 2016, P53, DOI 10.1109/NaNA.2016.84
   Li BC, 2016, SAP 2015: ACM SIGGRAPH SYMPOSIUM ON APPLIED PERCEPTION, P55, DOI 10.1145/2804408.2804427
   Li Bochao., 2014, P ACM S APPL PERCEPT, P91
   LOOMIS JM, 1992, J EXP PSYCHOL HUMAN, V18, P906, DOI 10.1037/0096-1523.18.4.906
   Mohler BJ, 2010, PRESENCE-TELEOP VIRT, V19, P230, DOI 10.1162/pres.19.3.230
   MOHLER BJ, 2008, P S APPL PERC GRAPH
   Parker J. F., 1973, BIOASTRONAUTICS DATA, P611
   Ries B., 2009, Proceedings of the 16th ACM Symposium on Virtual Reality Software and Technology, VRST '09, P59, DOI [10.1145/1643928.1643943, DOI 10.1145/1643928.16439433, DOI 10.1145/1643928.1643943]
   RIESER JJ, 1990, PERCEPTION, V19, P675, DOI 10.1068/p190675
   Sahm C. S., 2005, ACM Trans. Appl. Percept. (TAP), V2, P35, DOI DOI 10.1145/1048687.1048690
   Sahm Cynthia S., 2004, P S APPL PERC GRAPH, V179
   Strasburger H, 2011, J VISION, V11, DOI 10.1167/11.5.13
   Thompson WB, 2004, PRESENCE-TELEOP VIRT, V13, P560, DOI 10.1162/1054746042545292
   Webb NA, 2003, AVIAT SPACE ENVIR MD, V74, P622
   Willemsen P, 2008, PRESENCE-TELEOP VIRT, V17, P91, DOI 10.1162/pres.17.1.91
   Willemsen P, 2009, ACM T APPL PERCEPT, V6, DOI 10.1145/1498700.1498702
   Witmer BG, 1998, HUM FACTORS, V40, P478, DOI 10.1518/001872098779591340
   Witmer BG, 1998, PRESENCE-TELEOP VIRT, V7, P144, DOI 10.1162/105474698565640
   Wu B, 2004, NATURE, V428, P73, DOI 10.1038/nature02350
   Xiao R, 2016, 34TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, CHI 2016, P1221, DOI 10.1145/2858036.2858212
   Young M. K., 2014, P ACM S APPL PERCEPT, P83
   Zhang RM, 2012, ACM T APPL PERCEPT, V9, DOI 10.1145/2325722.2325727
NR 36
TC 17
Z9 20
U1 0
U2 15
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD APR
PY 2018
VL 15
IS 2
AR 12
DI 10.1145/3165286
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA GH5YV
UT WOS:000433515400005
DA 2024-07-18
ER

PT J
AU Krejtz, K
   Duchowski, A
   Krejtz, I
   Szarkowska, A
   Kopacz, A
AF Krejtz, Krzysztof
   Duchowski, Andrew
   Krejtz, Izabela
   Szarkowska, Agnieszka
   Kopacz, Agata
TI Discerning Ambient/Focal Attention with Coefficient <i>K</i>
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Ambient-focal attention; visual attention dynamics; serial versus
   parallel search
ID SCENE PERCEPTION; EYE-MOVEMENTS; MEMORY; GUIDANCE; LEVEL; SOUND
AB We introduce coefficient K, defined on a novel parametric scale, derived from processing a traditionally eye-tracked time course of eye movements. Positive and negative ordinates of K indicate focal or ambient viewing, respectively, while the abscissa serves to indicate time, so that K acts as a dynamic indicator of fluctuation between ambient/focal visual behavior. The coefficient indicates the difference between fixation duration and its subsequent saccade amplitude expressed in standard deviation units, facilitating parametric statistical testing. To validate K empirically, we test its utility by capturing ambient and focal attention during serial and parallel visual search tasks (Study 1). We then show how K quantitatively depicts the difference in scanning behaviors when attention is guided by audio description during perception of art (Study 2).
C1 [Krejtz, Krzysztof; Krejtz, Izabela] SWPS Univ Social Sci & Humanities, Dept Psychol, Ul Chodakowska 19-31, PL-03815 Warsaw, Poland.
   [Duchowski, Andrew] Clemson Univ, Sch Comp, 100 McAdams Hall, Clemson, SC 29634 USA.
   [Szarkowska, Agnieszka] Univ Warsaw, Fac Appl Linguist, Ul Dobra 55, PL-00312 Warsaw, Poland.
   [Kopacz, Agata] Natl Informat Proc Inst, Warsaw, Poland.
   [Kopacz, Agata] Informat Proc Inst, Interact Technol Lab, Al Niepodleglosci 188b, PL-00608 Warsaw, Poland.
C3 SWPS University of Social Sciences & Humanities; Clemson University;
   University of Warsaw; Information Processing Center - National Research
   Institute; Information Processing Center - National Research Institute
RP Krejtz, K (corresponding author), SWPS Univ Social Sci & Humanities, Dept Psychol, Ul Chodakowska 19-31, PL-03815 Warsaw, Poland.
EM kkrejtz@swps.edu.pl; duchowski@clemson.edu; ikrejtz@swps.edu.pl;
   a.szarkowska@gmail.com; akopacz@opi.org.pl
RI Krejtz, Izabela/AAC-9145-2021; Krejtz, Krzysztof/AAP-6165-2021; Krejtz,
   Izabela/H-3411-2016; Szarkowska, Agnieszka/Q-5177-2019; Szarkowska,
   Agnieszka/A-2935-2016
OI Krejtz, Izabela/0000-0002-9827-8371; Krejtz,
   Krzysztof/0000-0002-9558-3039; Szarkowska,
   Agnieszka/0000-0002-0048-993X; Szarkowska, Agnieszka/0000-0002-0048-993X
FU Faculty of Applied Languages, University of Warsaw
FX This work has been partly supported by research grant "Audio description
   in education" awarded by the Faculty of Applied Languages, University of
   Warsaw.
CR [Anonymous], P 2 INT WORKSH EYE T
   [Anonymous], P 1 WORKSH EYE TRACK
   [Anonymous], 2012, P ACM S APPL PERCEPT
   [Anonymous], 2010, Media for All 2: New Insights into Audiovisual Translation and Media Accessibility
   [Anonymous], ATTENTION PERFORMANC
   [Anonymous], MEDIA ALL SUBTITLING
   [Anonymous], P INT C MULT INT DES
   Bailey Reynold., 2012, P S EYE TRACKING RES, P67
   Bednarik Roman., 2012, Proceedings of the symposium on eye tracking research and applications, P83, DOI [DOI 10.1145/2168556.2168569, 10.1145/2168556.2168569]
   BOSELIE F, 1985, AM J PSYCHOL, V98, P1, DOI 10.2307/1422765
   Bulling A., 2013, P SIGCHI C HUM FACT, P305, DOI [DOI 10.1145/2470654.2470697, 10.1145/2470654.2470697]
   Castelhano MS, 2007, J EXP PSYCHOL HUMAN, V33, P753, DOI 10.1037/0096-1523.33.4.753
   Cupchik GC, 2009, BRAIN COGNITION, V70, P84, DOI 10.1016/j.bandc.2009.01.003
   Duchowski AT, 2002, BEHAV RES METH INS C, V34, P455, DOI 10.3758/BF03195475
   Follet B, 2011, I-PERCEPTION, V2, P592, DOI 10.1068/i0414
   Hekkert P, 1996, ACTA PSYCHOL, V94, P117, DOI 10.1016/0001-6918(95)00055-0
   Holland A, 2008, AUDIOVISUAL TRANSLATION: LANGUAGE TRANSFER ON SCREEN, P170
   Hollingworth A, 2002, J EXP PSYCHOL HUMAN, V28, P113, DOI 10.1037//0096-1523.28.1.113
   Holmqvist K, 2011, BEHAV RES METHODS, V43, P987, DOI 10.3758/s13428-011-0104-x
   Irwin DE, 2002, PERCEPT PSYCHOPHYS, V64, P882, DOI 10.3758/BF03196793
   Jacobsen T, 2002, PERCEPT MOTOR SKILL, V95, P755, DOI 10.2466/PMS.95.7.755-766
   Kenneth HolmqvistMarcus Nystrom., 2011, Eye Tracking: A Comprehensive Guide to Methods and Measures
   Klein R., 2004, COGNTIVE NEUROSCIENC, P29
   Krejtz Izabela., 2012, P S EYE TRACKING RES, P99, DOI [10.1145/2168556.2168572, DOI 10.1145/2168556.2168572]
   Krejtz K, 2015, ACM T APPL PERCEPT, V13, DOI 10.1145/2834121
   Kruger JL, 2010, PERSPECT STUD TRANSL, V18, P231, DOI 10.1080/0907676X.2010.485686
   Le Meur O, 2010, IEEE T IMAGE PROCESS, V19, P2801, DOI 10.1109/TIP.2010.2052262
   Locher P.J., 2006, Psychology Science, V48, P106
   MACKWORTH NH, 1967, PERCEPT PSYCHOPHYS, V2, P547, DOI 10.3758/BF03210264
   MARTINDALE C, 1988, J EXP PSYCHOL HUMAN, V14, P661, DOI 10.1037/0096-1523.14.4.661
   Massaro D, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0037285
   Matamala A., 2007, Media for all: subtitling for the deaf, audio description, and sign language p, P201
   Mayer RE, 2002, PSYCHOL LEARN MOTIV, V41, P85, DOI 10.1016/S0079-7421(02)80005-6
   Melcher D, 2001, VISION RES, V41, P3597, DOI 10.1016/S0042-6989(01)00203-6
   Nothdurft HC, 1999, VISION RES, V39, P2305, DOI 10.1016/S0042-6989(99)00006-1
   Nummenmaa L, 2006, EMOTION, V6, P257, DOI 10.1037/1528-3542.6.2.257
   Orero P., 2007, MEDIA ALL SUBTITLING, P111
   Pannasch S, 2008, J EYE MOVEMENT RES, V2, DOI 10.16910/jemr.2.2.4
   Pannasch S, 2011, ATTEN PERCEPT PSYCHO, V73, P1120, DOI 10.3758/s13414-011-0090-1
   Peirce JW, 2009, FRONT NEUROINFORM, V2, DOI 10.3389/neuro.11.010.2008
   Peli E, 1996, J VISUAL IMPAIR BLIN, V90, P378
   POSNER MI, 1980, Q J EXP PSYCHOL, V32, P3, DOI 10.1080/00335558008248231
   R Development Core Team, 2011, R LANG ENV STAT COMP
   Ramachandran VS, 2000, J CONSCIOUSNESS STUD, V7, P17
   RAYNER K, 1978, PSYCHOL BULL, V85, P618, DOI 10.1037/0033-2909.85.3.618
   Rayner K, 2009, J EYE MOVEMENT RES, V2
   Rothkopf CA, 2007, J VISION, V7, DOI 10.1167/7.14.16
   Sadoski M., 2004, Theoretical models and processes of reading, V5th, P1329, DOI [10.1598/0872075028.47, DOI 10.1598/0872075028.47]
   Santella A., 2004, P S EYE TRACKING RES, P27, DOI DOI 10.1145/968363.968368
   Schmeidler E, 2001, J VISUAL IMPAIR BLIN, V95, P197, DOI 10.1177/0145482X0109500402
   Sekuler R, 1997, NATURE, V385, P308, DOI 10.1038/385308a0
   Toker Dereck., 2013, proceedings of the SIGCHI Conference on Human Factors in Computing Systems, P295
   Unema PJA, 2005, VIS COGN, V12, P473, DOI 10.1080/13506280444000409
   van Zoest W, 2004, PERCEPTION, V33, P927, DOI 10.1068/p5158
   Velichkovsky Boris M, 2005, P 27 C COGN SCI SOC, V1
   Vilaró A, 2012, PERSPECT STUD TRANSL, V20, P55, DOI 10.1080/0907676X.2011.632682
   Yarbus A. L., 1967, Eye Movements and Vision
   ZANGEMEISTER WH, 1995, OPTOMETRY VISION SCI, V72, P467, DOI 10.1097/00006324-199507000-00006
NR 58
TC 60
Z9 60
U1 1
U2 7
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD MAY
PY 2016
VL 13
IS 3
AR 11
DI 10.1145/2896452
PG 20
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DV4DU
UT WOS:000382876200001
DA 2024-07-18
ER

PT J
AU Hyde, J
   Carter, EJ
   Kiesler, S
   Hodgins, JK
AF Hyde, Jennifer
   Carter, Elizabeth J.
   Kiesler, Sara
   Hodgins, Jessica K.
TI Evaluating Animated Characters: Facial Motion Magnitude Influences
   Personality Perceptions
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Facial motion; animation; personality perception; rendering style
ID NONVERBAL BEHAVIOR; FACE; EXPRESSIONS; MOVEMENT; TIME; EXTROVERSION;
   ATTRACTION; COMPETENCE; EXPOSURE; WARMTH
AB Animated characters are expected to fulfill a variety of social roles across different domains. To be successful and effective, these characters must display a wide range of personalities. Designers and animators create characters with appropriate personalities by using their intuition and artistic expertise. Our goal is to provide evidence-based principles for creating social characters. In this article, we describe the results of two experiments that show how exaggerated and damped facial motion magnitude influence impressions of cartoon and more realistic animated characters. In our first experiment, participants watched animated characters that varied in rendering style and facial motion magnitude. The participants then rated the different animated characters on extroversion, warmth, and competence, which are social traits that are relevant for characters used in entertainment, therapy, and education. We found that facial motion magnitude affected these social traits in cartoon and realistic characters differently. Facial motion magnitude affected ratings of cartoon characters' extroversion and competence more than their warmth. In contrast, facial motion magnitude affected ratings of realistic characters' extroversion but not their competence nor warmth. We ran a second experiment to extend the results of the first. In the second experiment, we added emotional valence as a variable. We also asked participants to rate the characters on more specific aspects of warmth, such as respectfulness, calmness, and attentiveness. Although the characters' emotional valence did not affect ratings, we found that facial motion magnitude influenced ratings of the characters' respectfulness and calmness but not attentiveness. These findings provide a basis for how animators can fine-tune facial motion to control perceptions of animated characters' personalities.
C1 [Hyde, Jennifer] Carnegie Mellon Univ, Dept Comp Sci, 5000 Forbes Ave, Pittsburgh, PA 15213 USA.
   [Carter, Elizabeth J.] Disney Res, 4720 Forbes Ave,Suite 110, Pittsburgh, PA 15213 USA.
   [Kiesler, Sara] Carnegie Mellon Univ, Human Comp Interact Inst, 5000 Forbes Ave, Pittsburgh, PA 15213 USA.
   [Hodgins, Jessica K.] Carnegie Mellon Univ, Inst Robot, 5000 Forbes Ave, Pittsburgh, PA 15213 USA.
C3 Carnegie Mellon University; Carnegie Mellon University; Carnegie Mellon
   University
RP Hyde, J (corresponding author), Carnegie Mellon Univ, Dept Comp Sci, 5000 Forbes Ave, Pittsburgh, PA 15213 USA.; Carter, EJ (corresponding author), Disney Res, 4720 Forbes Ave,Suite 110, Pittsburgh, PA 15213 USA.; Kiesler, S (corresponding author), Carnegie Mellon Univ, Human Comp Interact Inst, 5000 Forbes Ave, Pittsburgh, PA 15213 USA.; Hodgins, JK (corresponding author), Carnegie Mellon Univ, Inst Robot, 5000 Forbes Ave, Pittsburgh, PA 15213 USA.
EM jdtam@cs.cmu.edu; liz.carter@disneyresearch.com; kiesler@cs.cmu.edu;
   jkh@cs.cmu.edu
RI Carter, Elizabeth J/G-6958-2012
FU Eunice Kennedy Shriver Institute of Child Health and Human Development
   of the National Institutes of Health [R03HD068816]
FX This work is supported in part by Disney Research and award R03HD068816
   from the Eunice Kennedy Shriver Institute of Child Health and Human
   Development of the National Institutes of Health.
CR Ackerman SJ, 2003, CLIN PSYCHOL REV, V23, P1, DOI 10.1016/s0272-7358(02)00146-0
   Ambadar Z, 2005, PSYCHOL SCI, V16, P403, DOI 10.1111/j.0956-7976.2005.01548.x
   Anderson C, 2001, J PERS SOC PSYCHOL, V81, P116, DOI 10.1037//0022-3514.81.1.116
   [Anonymous], 2013, P 10 IEEE INT C WORK
   [Anonymous], 2016, ACM T APPL PERCEPTIO, V13
   [Anonymous], P SIGGRAPH ASIA 2009
   [Anonymous], 2014, P ACM S APPL PERCEPT
   Back E, 2009, VIS COGN, V17, P1271, DOI 10.1080/13506280802479998
   Bente G, 2001, J NONVERBAL BEHAV, V25, P151, DOI 10.1023/A:1010690525717
   Boker SM, 2009, PHILOS T R SOC B, V364, P3485, DOI 10.1098/rstb.2009.0152
   Brainard DH, 1997, SPATIAL VISION, V10, P433, DOI 10.1163/156856897X00357
   Cassell J, 2000, EMBODIED CONVERSATIONAL AGENTS, P1
   Cootes TF, 2002, IMAGE VISION COMPUT, V20, P657, DOI 10.1016/S0262-8856(02)00055-0
   Cootes TF, 2001, IEEE T PATTERN ANAL, V23, P681, DOI 10.1109/34.927467
   DIGMAN JM, 1990, ANNU REV PSYCHOL, V41, P417, DOI 10.1146/annurev.psych.41.1.417
   Donath J, 2001, LECT NOTES ARTIF INT, V2117, P373
   Doucet C, 1997, PERS INDIV DIFFER, V23, P775, DOI 10.1016/S0191-8869(97)00104-9
   EKMAN P, 1988, J PERS SOC PSYCHOL, V54, P414, DOI 10.1037/0022-3514.54.3.414
   EKMAN P, 1980, J PERS SOC PSYCHOL, V38, P270, DOI 10.1037/0022-3514.38.2.270
   Ellis PM, 2005, LECT NOTES ARTIF INT, V3661, P394
   Fiske ST, 2002, J PERS SOC PSYCHOL, V82, P878, DOI 10.1037//0022-3514.82.6.878
   FRIEDMAN HS, 1988, PERS SOC PSYCHOL B, V14, P203, DOI 10.1177/0146167288141020
   GALLAHER PE, 1992, J PERS SOC PSYCHOL, V63, P133, DOI 10.1037/0022-3514.63.1.133
   GIFFORD R, 1991, J PERS SOC PSYCHOL, V61, P279, DOI 10.1037/0022-3514.61.2.279
   Gosling SD, 2003, J RES PERS, V37, P504, DOI 10.1016/S0092-6566(03)00046-1
   Hess U, 1997, J NONVERBAL BEHAV, V21, P241, DOI 10.1023/A:1024952730333
   IZARD CE, 1994, PSYCHOL BULL, V115, P288, DOI 10.1037/0033-2909.115.2.288
   Kleiner M, 2007, PERCEPTION, V36, P14
   Lasseter John., 1987, Proceedings of the 14th annual conference on Computer graphics and interactive techniques, P35
   Lewis J.R., 1989, P HUMAN FACTORS ERGO, V33, P1223, DOI DOI 10.1177/154193128903301812
   Matthews I, 2004, INT J COMPUT VISION, V60, P135, DOI 10.1023/B:VISI.0000029666.37597.d3
   McDonnell Rachel, 2012, ACM T GRAPHIC, V31
   MORELAND RL, 1982, J EXP SOC PSYCHOL, V18, P395, DOI 10.1016/0022-1031(82)90062-2
   Niewiadomski R, 2010, LECT NOTES ARTIF INT, V6356, P272, DOI 10.1007/978-3-642-15892-6_29
   Oosterhof NN, 2008, P NATL ACAD SCI USA, V105, P11087, DOI 10.1073/pnas.0805664105
   Pelli DG, 1997, SPATIAL VISION, V10, P437, DOI 10.1163/156856897X00366
   Riggio HR, 2002, J NONVERBAL BEHAV, V26, P195, DOI 10.1023/A:1022117500440
   Schilbach L, 2006, NEUROPSYCHOLOGIA, V44, P718, DOI 10.1016/j.neuropsychologia.2005.07.017
   Sproull L, 1996, HUM-COMPUT INTERACT, V11, P97, DOI 10.1207/s15327051hci1102_1
   STELMACK RM, 1993, J PERS SOC PSYCHOL, V65, P399, DOI 10.1037/0022-3514.65.2.399
   TAKASHIMA K, 2008, P GRAPH INT, P169, DOI DOI 10.1145/1375714.1375744
   Theobald BJ, 2009, LANG SPEECH, V52, P369, DOI 10.1177/0023830909103181
   Thomas F., 1981, The Illusion of Life: Disney Animation
   Weibel D., 2010, Journal of Media Psychology: Theories, Methods, and Applications, V22, P37, DOI DOI 10.1027/1864-1105/A000005
   Wickett JC, 2000, PERS INDIV DIFFER, V28, P205, DOI 10.1016/S0191-8869(99)00063-X
   Willis J, 2006, PSYCHOL SCI, V17, P592, DOI 10.1111/j.1467-9280.2006.01750.x
   Young S, 1999, J HIGH EDUC, V70, P670, DOI 10.2307/2649170
   Zebrowitz L.A., 1997, READING FACES WINDOW
   Zebrowitz LA, 2008, SOC PERSONAL PSYCHOL, V2, P1497, DOI 10.1111/j.1751-9004.2008.00109.x
NR 49
TC 7
Z9 9
U1 1
U2 12
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD MAR
PY 2016
VL 13
IS 2
AR 8
DI 10.1145/2851499
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA DJ0OM
UT WOS:000373903800003
DA 2024-07-18
ER

PT J
AU Argelaguet, F
   Jáuregui, DAG
   Marchal, M
   Lécuyer, A
AF Argelaguet, Ferran
   Jauregui, David Antonio Gomez
   Marchal, Maud
   Lecuyer, Anatole
TI Elastic Images: Perceiving Local Elasticity of Images through a Novel
   Pseudo-Haptic Deformation Effect
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article; Proceedings Paper
CT 10th Symposium on Applied Perception (SAP)
CY AUG, 2013
CL Trinity Coll, Dublin, IRELAND
HO Trinity Coll
DE Human Factors; Pseudo-haptic; texture; elasticity; stiffness
AB We introduce the Elastic Images, a novel pseudo-haptic feedback technique which enables the perception of the local elasticity of images without the need of any haptic device. The proposed approach focus on whether visual feedback is able to induce a sensation of stiffness when the user interacts with an image using a standard mouse. The user, when clicking on a Elastic Image, is able to deform it locally according to its elastic properties. To reinforce the effect, we also propose the generation of procedural shadows and creases to simulate the compressibility of the image and several mouse cursors replacements to enhance pressure and stiffness perception. A psychophysical experiment was conducted to quantify this novel pseudo-haptic perception and determine its perceptual threshold (or its Just Noticeable Difference). The results showed that users were able to recognize up to eight different stiffness values with our proposed method and confirmed that it provides a perceivable and exploitable sensation of elasticity. The potential applications of the proposed approach range from pressure sensing in product catalogs and games, or its usage in graphical user interfaces for increasing the expressiveness of widgets.
C1 [Argelaguet, Ferran; Jauregui, David Antonio Gomez; Lecuyer, Anatole] Inria Rennes, Rennes, France.
   [Marchal, Maud] Inria INSA Rennes, Rennes, France.
C3 Universite de Rennes; Universite de Rennes
RP Argelaguet, F (corresponding author), Inria Rennes, Rennes, France.
EM Fernando.argelaguet_sanz@inria.fr
RI Jauregui, David Antonio Gomez/W-1226-2019
OI Gomez Jauregui, David Antonio/0000-0002-5898-0342
CR [Anonymous], 2008, HAPTIC RENDERING FDN
   [Anonymous], 2011, CHI 11 EXT ABSTR HUM, DOI DOI 10.1145/1979742.1979702
   Argelaguet Ferran, 2012, Haptics: Perception, Devices, Mobility, and Communication. Proceedings International Conference (EuroHaptics 2012), P1, DOI 10.1007/978-3-642-31401-8_1
   Cechanowicz J, 2007, CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1 AND 2, P1385
   Chan A, 2008, INT J HUM-COMPUT ST, V66, P333, DOI 10.1016/j.ijhcs.2007.11.002
   Cholewiak SA, 2008, SYMPOSIUM ON HAPTICS INTERFACES FOR VIRTUAL ENVIRONMENT AND TELEOPERATOR SYSTEMS 2008, PROCEEDINGS, P87
   Drewing K, 2009, WORLD HAPTICS 2009: THIRD JOINT EUROHAPTICS CONFERENCE AND SYMPOSIUM ON HAPTIC INTERFACES FOR VIRTUAL ENVIRONMENT AND TELEOPERATOR SYSTEMS, PROCEEDINGS, P640, DOI 10.1109/WHC.2009.4810828
   Forrest N, 2009, WORLD HAPTICS 2009: THIRD JOINT EUROHAPTICS CONFERENCE AND SYMPOSIUM ON HAPTIC INTERFACES FOR VIRTUAL ENVIRONMENT AND TELEOPERATOR SYSTEMS, PROCEEDINGS, P646, DOI 10.1109/WHC.2009.4810800
   García M, 2010, COMPUT ANIMAT VIRT W, V21, P245, DOI 10.1002/cav.371
   Gescheider G.A., 1985, Psychophysics: Methods, Theory and Application
   Gurari N, 2009, WORLD HAPTICS 2009: THIRD JOINT EUROHAPTICS CONFERENCE AND SYMPOSIUM ON HAPTIC INTERFACES FOR VIRTUAL ENVIRONMENT AND TELEOPERATOR SYSTEMS, PROCEEDINGS, P121, DOI 10.1109/WHC.2009.4810845
   Hachisu T., 2011, P INT S VR INN ISVRI, P331
   JONES LA, 1990, EXP BRAIN RES, V79, P150
   Lécuyer A, 2001, P IEEE VIRT REAL ANN, P115, DOI 10.1109/VR.2001.913777
   Lecuyer A., 2004, P SIGCHI C HUM FACT, P239, DOI DOI 10.1145/985692.985723
   Lécuyer A, 2008, ACM T APPL PERCEPT, V5, DOI 10.1145/1402236.1402238
   Lécuyer A, 2009, PRESENCE-TELEOP VIRT, V18, P39, DOI 10.1162/pres.18.1.39
   Levesque V., 2012, 2012 IEEE Haptics Symposium (HAPTICS), P23, DOI 10.1109/HAPTIC.2012.6183765
   Mizobuchi S., 2005, Proceedings of the CHI '05 extended abstracts on Human factors in computing systems, P1661, DOI DOI 10.1145/1056808.1056991
   MOODY L, 2008, VIRTUAL REALITY, V13, P59
   Ramos Gonzalo, 2004, P SIGCHI C HUM FACT, P487
   Rodgers ME, 2006, LECT NOTES COMPUT SC, V4073, P194
   SRINIVASAN M., 1995, J NEUROPHYSIOL, V33, P88
   Srinivasan M. A., 1996, Proceedings of the ASME Dynamic Systems and Control Division, P555
NR 24
TC 31
Z9 35
U1 2
U2 9
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD AUG
PY 2013
VL 10
IS 3
SI SI
AR 17
DI 10.1145/2501599
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 206FS
UT WOS:000323504700006
OA Bronze
DA 2024-07-18
ER

PT J
AU McCrae, J
   Mitra, NJ
   Singh, K
AF McCrae, James
   Mitra, Niloy J.
   Singh, Karan
TI Surface Perception of Planar Abstractions
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article; Proceedings Paper
CT 10th Symposium on Applied Perception (SAP)
CY AUG, 2013
CL Trinity Coll, Dublin, IRELAND
HO Trinity Coll
DE Design; Experimentation; Human Factors; Measurement; Performance;
   Crowd-sourced user study; perception; planar abstraction; surface
   representation
AB Various algorithms have been proposed to create planar abstractions of 3D models, but there has been no systematic effort to evaluate the effectiveness of such abstractions in terms of perception of the abstracted surfaces. In this work, we perform a large crowd-sourced study involving approximately 70k samples to evaluate how well users can orient gauges on planar abstractions of commonly occurring models. We test four styles of planar abstractions against ground truth surface representations, and analyze the data to discover a wide variety of correlations between task error and measurements relating to surface-specific properties such as curvature, local thickness and medial axis distance, and abstraction-specific properties. We use these discovered correlations to create linear models to predict error in surface understanding at a given point, for both surface representations and planar abstractions. Our predictive models reveal the geometric causes most responsible for error, and we demonstrate their potential use to build upon existing planar abstraction techniques in order to improve perception of the abstracted surface.
C1 [McCrae, James; Singh, Karan] Univ Toronto, Toronto, ON M5S 1A1, Canada.
   [Mitra, Niloy J.] UCL, London WC1E 6BT, England.
C3 University of Toronto; University of London; University College London
RP McCrae, J (corresponding author), Univ Toronto, Toronto, ON M5S 1A1, Canada.
EM mccrae@dgp.toronto.edu
CR [Anonymous], 2012, R LANG ENV STAT COMP
   Belhumeur PN, 1999, INT J COMPUT VISION, V35, P33, DOI 10.1023/A:1008154927611
   Caniard F, 2007, APGV 2007: SYMPOSIUM ON APPLIED PERCEPTION IN GRAPHICS AND VISUALIZATION, PROCEEDINGS, P99
   Cignoni P, 1998, COMPUT GRAPH-UK, V22, P37, DOI 10.1016/S0097-8493(97)00082-4
   Cohen-Steiner D, 2004, ACM T GRAPHIC, V23, P905, DOI 10.1145/1015706.1015817
   Cole F., 2012, ACM T GRAPHIC, V55, P1
   Cole F, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531334
   Décoret X, 2003, ACM T GRAPHIC, V22, P689, DOI 10.1145/882262.882326
   Faisman Arthur., 2012, Proc. of the ACM Symp. on Applied Perception, P123
   Friedman J, 2010, J STAT SOFTW, V33, P1, DOI 10.18637/jss.v033.i01
   Giorgi D., 2007, Shrec: shape retrieval contest: Watertight models track
   Hildebrand K, 2012, COMPUT GRAPH FORUM, V31, P583, DOI 10.1111/j.1467-8659.2012.03037.x
   Kavan L, 2008, I3D 2008: SYMPOSIUM ON INTERACTIVE 3D GRAPHICS AND GAMES, PROCEEDINGS, P149
   KNILL DC, 1992, J OPT SOC AM A, V9, P1449, DOI 10.1364/JOSAA.9.001449
   KOENDERINK JJ, 1992, PERCEPT PSYCHOPHYS, V52, P487, DOI 10.3758/BF03206710
   Li XY, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964993
   MCCRAE J, 2011, ACM T GRAPHIC, V6, p[1, 12]
   Mehra R, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618483
   Mitra NJ, 2006, ACM T GRAPHIC, V25, P560, DOI 10.1145/1141911.1141924
   NORMAN JF, 1995, PERCEPT PSYCHOPHYS, V57, P629
   O'Shea JP, 2008, APGV 2008: PROCEEDINGS OF THE SYMPOSIUM ON APPLIED PERCEPTION IN GRAPHICS AND VISUALIZATION, P135
   Palágyi K, 2002, PATTERN RECOGN LETT, V23, P663, DOI 10.1016/S0167-8655(01)00142-8
   Phillips F, 2003, PERCEPT PSYCHOPHYS, V65, P747, DOI 10.3758/BF03194811
   RAMACHANDRAN VS, 1988, SCI AM, V259, P76, DOI 10.1038/scientificamerican0888-76
   RUSINKIEWICZ S, 2004, P 2 INT S 3D DAT PRO
   Saito Takafumi, 1990, P 17 ANN C COMP GRAP
   SCHWARTZBURG Y., 2011, P DES MOD S
   Secord A, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2019627.2019628
   Shao C, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185541
   SIMARI P., 2006, P EUR S GEOM PROC
   STEVENS KA, 1981, ARTIF INTELL, V17, P47, DOI 10.1016/0004-3702(81)90020-5
   Sweet G, 2004, PROC GRAPH INTERF, P97
   Tibshirani R, 1996, J ROY STAT SOC B, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x
   Willis KDD, 2010, TEI 2010, P5
   Winnemöller H, 2007, NPAR 2007: 5TH INTERNATIONAL SYMPOSIUM ON NON-PHOTOREALISTIC ANIMATION AND RENDERING, PROCEEDINGS, P85
   Zhang JY, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2167076.2167079
NR 36
TC 4
Z9 7
U1 0
U2 7
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD AUG
PY 2013
VL 10
IS 3
SI SI
AR 14
DI 10.1145/2501853
PG 20
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 206FS
UT WOS:000323504700003
OA Bronze, Green Submitted
DA 2024-07-18
ER

PT J
AU Nguyen, TD
   Ziemer, CJ
   Grechkin, T
   Chihak, B
   Plumert, JM
   Cremer, JF
   Kearney, JK
AF Tien Dat Nguyen
   Ziemer, Christine J.
   Grechkin, Timofey
   Chihak, Benjamin
   Plumert, Jodie M.
   Cremer, James F.
   Kearney, Joseph K.
TI Effects of Scale Change on Distance Perception in Virtual Environments
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Design; Experimentation; Human Factors; Performance; Virtual
   environments; head-mounted displays; distance estimation; perception;
   scale
ID SIZE; REAL; CUE
AB We conducted a series of experiments to investigate effects of scale changes on distance perception in virtual environments. All experiments were carried out in an HMD. Participants first made distance estimates with feedback in a virtual tunnel (adaptation) and then made distance estimates without feedback in a differently-scaled virtual environment (test). We examined several types of scale changes, including changing the size of (1) the tunnel, (2) the targets, and (3) the separation of the two targets. Changes in target size always affected distance estimates at test. When the targets became smaller, participants overshot distance and when the targets became larger, participants undershot distance. Changes in the size of the tunnel or the separation between the targets (without a change in the size of the targets) had a minimal effect on distance estimates. These results indicate that distance estimates at test were strongly influenced by familiar size cues for distance. The discussion focuses on the stability of calibration processes and mechanisms for cue integration for perceiving distance in virtual environments.
C1 [Tien Dat Nguyen; Ziemer, Christine J.; Grechkin, Timofey; Chihak, Benjamin; Plumert, Jodie M.; Cremer, James F.; Kearney, Joseph K.] Univ Iowa, Iowa City, IA 52242 USA.
C3 University of Iowa
RP Nguyen, TD (corresponding author), Univ Iowa, Iowa City, IA 52242 USA.
EM cremer@cs.uiowa.edu
FU Vietnam Education Foundation; National Science Foundation [CNS-0750677];
   National Institute of Child Health and Human Development [R01-HD052875];
   Division Of Computer and Network Systems; Direct For Computer & Info
   Scie & Enginr [0750677] Funding Source: National Science Foundation
FX This research was supported by a fellowship awarded to Tien Dat Nguyen
   from the Vietnam Education Foundation and by grants awarded to Jodie
   Plumert, Joseph Kearney, and James Cremer from the National Science
   Foundation (CNS-0750677) and the National Institute of Child Health and
   Human Development (R01-HD052875).
CR [Anonymous], 2005, ACM Transactions on Applied Perception, DOI [DOI 10.1145/1077399.1077403, DOI 10.1145/1077399.10774032,3,9]
   [Anonymous], VIRTUAL REALITY TRAI
   Creem-Regehr SH, 2005, PERCEPTION, V34, P191, DOI 10.1068/p5144
   Crompton A, 2006, ENVIRON BEHAV, V38, P656, DOI 10.1177/0013916505281571
   EPSTEIN W, 1961, PSYCHOL BULL, V58, P491, DOI 10.1037/h0042260
   Glennerster A, 2006, CURR BIOL, V16, P428, DOI 10.1016/j.cub.2006.01.019
   HARVEY LO, 1967, J OPT SOC AM, V57, P249, DOI 10.1364/JOSA.57.000249
   Holway AH, 1941, AM J PSYCHOL, V54, P21, DOI 10.2307/1417790
   Interrante V, 2006, P IEEE VIRT REAL ANN, P3, DOI 10.1109/VR.2006.52
   ITTELSON WH, 1951, AM J PSYCHOL, V64, P54, DOI 10.2307/1418595
   Johnson DM, 1999, MIL PSYCHOL, V11, P129, DOI 10.1207/s15327876mp1102_1
   Jones JA, 2008, APGV 2008: PROCEEDINGS OF THE SYMPOSIUM ON APPLIED PERCEPTION IN GRAPHICS AND VISUALIZATION, P9
   Kenyon RV, 2007, PRESENCE-TELEOP VIRT, V16, P172, DOI 10.1162/pres.16.2.172
   Mohler BettyJ., 2006, P 3 S APPL PERCEPTIO, P9, DOI [10.1145/1140491.1140493, DOI 10.1145/1140491.1140493]
   Richardson AR, 2005, APPL COGNITIVE PSYCH, V19, P1089, DOI 10.1002/acp.1140
   Steinicke F., 2009, Proceedings of the 6th Symposium on Applied Perception in Graphics and Visualization, P19, DOI 10.1145/1620993.1620998
   Swan JE, 2007, IEEE T VIS COMPUT GR, V13, P429, DOI 10.1109/TVCG.2007.1035
   Waller D, 2008, J EXP PSYCHOL-APPL, V14, P61, DOI 10.1037/1076-898X.14.1.61
   Witmer BG, 1998, HUM FACTORS, V40, P478, DOI 10.1518/001872098779591340
   Yang ZY, 2003, NAT NEUROSCI, V6, P632, DOI 10.1038/nn1059
   Ziemer CJ, 2009, ATTEN PERCEPT PSYCHO, V71, P1095, DOI 10.3758/APP.71.5.1096
NR 21
TC 12
Z9 17
U1 0
U2 15
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD NOV
PY 2011
VL 8
IS 4
AR 26
DI 10.1145/2043603.2043608
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA 856IQ
UT WOS:000297633400005
DA 2024-07-18
ER

PT J
AU Huckauf, A
   Urbina, MH
AF Huckauf, Anke
   Urbina, Mario H.
TI Object Selection in Gaze Controlled Systems: What You Don't Look At Is
   What You Get
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Experimentation; Human Factors; Performance; Gaze control
ID EYE-MOVEMENTS; ANTISACCADE
AB Controlling computers using eye movements can provide a fast and efficient alternative to the computer mouse. However, implementing object selection in gaze-controlled systems is still a challenge. Dwell times or fixations on a certain object typically used to elicit the selection of this object show several disadvantages. We studied deviations of critical thresholds by an individual and task-specific adaptation method. This demonstrated an enormous variability of optimal dwell times. We developed an alternative approach using antisaccades for selection. For selection by antisaccades, highlighted objects are copied to one side of the object. The object is selected when fixating to the side opposed to that copy requiring to inhibit an automatic gaze shift toward new objects. Both techniques were compared in a selection task. Two experiments revealed superior performance in terms of errors for the individually adapted dwell times. Antisaccades provide an alternative approach to dwell time selection, but they did not show an improvement over dwell time. We discuss potential improvements in the antisaccade implementation with which antisaccades might become a serious alternative to dwell times for object selection in gaze-controlled systems.
C1 [Huckauf, Anke; Urbina, Mario H.] Univ Ulm, Inst Psychol & Pedag, D-89081 Ulm, Germany.
C3 Ulm University
RP Huckauf, A (corresponding author), Univ Ulm, Inst Psychol & Pedag, Albert Einstein Allee 47, D-89081 Ulm, Germany.
EM anke.huckauf@uniulm.de
RI Huckauf, Anke/AAJ-7730-2021
CR [Anonymous], P SIGCHI C HUM FACT
   Brainard DH, 1997, SPATIAL VISION, V10, P433, DOI 10.1163/156856897X00357
   DREWES H, 2007, P INT C HUM COMP INT
   Everling S, 1998, NEUROPSYCHOLOGIA, V36, P885, DOI 10.1016/S0028-3932(98)00020-7
   HALLETT PE, 1978, VISION RES, V18, P1279, DOI 10.1016/0042-6989(78)90218-3
   Hodgson TL, 2004, J COGNITIVE NEUROSCI, V16, P318, DOI 10.1162/089892904322984599
   Huckauf A, 2008, J EYE MOVEMENT RES, V2
   Huckauf A, 2008, PROCEEDINGS OF THE EYE TRACKING RESEARCH AND APPLICATIONS SYMPOSIUM (ETRA 2008), P51, DOI 10.1145/1344471.1344483
   Jacob R.J., 1990, P SIGCHI C HUM FACT, P11, DOI DOI 10.1145/97243.97246
   JACOB RJK, 1991, ACM T INFORM SYST, V9, P152, DOI 10.1145/123078.128728
   JACOB RJK, 1993, ADV HUMAN COMPUT INT, V16, P151
   KAERNBACH C, 1991, PERCEPT PSYCHOPHYS, V49, P227, DOI 10.3758/BF03214307
   KAHN DA, 1999, P 14 INT C TECHN PER
   Kristjánsson A, 2004, EXP BRAIN RES, V155, P231, DOI 10.1007/s00221-003-1717-9
   Lankford Chris., 2000, P 2000 S EYE TRACKIN, P23, DOI DOI 10.1145/355017.355021
   LIEBERMAN HR, 1982, BEHAV RES METH INSTR, V14, P21, DOI 10.3758/BF03202110
   Majaranta P., 2002, Proceedings ETRA 2002. Eye Tracking Research and Applications Symposium, P15, DOI 10.1145/507072.507076
   Majaranta P., 2003, CHI'03: ACM Conference on Human Factors in Computing Systems, P766, DOI [10.1145/765891.765979doi.org/10.1145/765891.765979, DOI 10.1145/765891.765979DOI.ORG/10.1145/765891.765979]
   Majaranta P, 2009, CHI2009: PROCEEDINGS OF THE 27TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1-4, P357
   Mosimann UP, 2004, EXP BRAIN RES, V159, P263, DOI 10.1007/s00221-004-2086-8
   Ohno T, 1998, 3RD ASIA PACIFIC COMPUTER HUMAN INTERACTION, PROCEEDINGS, P176, DOI 10.1109/APCHI.1998.704190
   Pannasch S, 2008, J EYE MOVEMENT RES, V2, DOI 10.16910/jemr.2.2.4
   Pelli DG, 1997, SPATIAL VISION, V10, P437, DOI 10.1163/156856897X00366
   Porta M, 2008, PROCEEDINGS OF THE EYE TRACKING RESEARCH AND APPLICATIONS SYMPOSIUM (ETRA 2008), P27, DOI 10.1145/1344471.1344477
   Spakov O., 2004, P 3 NORDIC C HUMANCO, P203, DOI [DOI 10.1145/1028014.1028045DOI.ORG/10.1145/1028014.1028045, DOI 10.1145/1028014.1028045]
   SURAKKA V, 2003, ACM T APPL PERCEPT, V1, P40
   Urbina Mario H., 2010, P 2010 S EYE TRACK R, DOI [DOI 10.1145/1743666.1743738, 10.1145/1743666.1743738]
   URBINA MH, 2009, P 5 INT C COMM GAZ I
   WARE C, 1987, P SIGCHI GI C HUM FA, P183
   Wobbrock JO, 2008, PROCEEDINGS OF THE EYE TRACKING RESEARCH AND APPLICATIONS SYMPOSIUM (ETRA 2008), P11, DOI 10.1145/1344471.1344475
   Zhai S, 2003, COMMUN ACM, V46, P34, DOI 10.1145/636772.636795
   Zhai Shumin., 1999, Proceedings of CHI, P246, DOI [10.1145/302979.303053 10.1145/302979.303053, DOI 10.1145/302979.3030532, DOI 10.1145/302979.303053]
NR 32
TC 19
Z9 21
U1 0
U2 5
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD JAN
PY 2011
VL 8
IS 2
AR 13
DI 10.1145/1870076.1870081
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA 748AR
UT WOS:000289362900005
DA 2024-07-18
ER

PT J
AU Aydin, TO
   Cadík, M
   Myszkowski, K
   Seidel, HP
AF Aydin, Tunc Ozan
   Cadik, Martin
   Myszkowski, Karol
   Seidel, Hans-Peter
TI Visually Significant Edges
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Algorithms; Exprimentation; Human Factors; Edge strength; visual
   perception; HDR
AB Numerous image processing and computer graphics methods make use of either explicitly computed strength of image edges, or an implicit edge strength definition that is integrated into their algorithms. In both cases, the end result is highly affected by the computation of edge strength. We address several shortcomings of the widely used gradient magnitude-based edge strength model through the computation of a hypothetical Human Visual System (HVS) response at edge locations. Contrary to gradient magnitude, the resulting "visual significance" values account for various HVS mechanisms such as luminance adaptation and visual masking, and are scaled in perceptually linear units that are uniform across images. The visual significance computation is implemented in a fast multiscale second-generation wavelet framework which we use to demonstrate the differences in image retargeting, HDR image stitching, and tone mapping applications with respect to the gradient magnitude model. Our results suggest that simple perceptual models provide qualitative improvements on applications utilizing edge strength at the cost of a modest computational burden.
C1 [Aydin, Tunc Ozan; Cadik, Martin; Myszkowski, Karol; Seidel, Hans-Peter] MPI Informat, D-66123 Saarbrucken, Germany.
C3 Max Planck Society
RP Myszkowski, K (corresponding author), MPI Informat, Stuhlsatzenhausweg 85, D-66123 Saarbrucken, Germany.
EM karol@mpi-sb.mpg.de
RI Cadik, Martin/O-4824-2014
OI Cadik, Martin/0000-0001-7058-9912; Myszkowski, Karol/0000-0002-8505-4141
CR [Anonymous], COMP VIS PATT REC 19, DOI [10.1109/CVPR.1999.784624, DOI 10.1109/CVPR.1999.784624]
   Avidan S, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239461
   Aydin TO, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360668
   Barten Peter G. J, 1999, Contrast sensitivity of the human eye and its effects on image quality
   BOLIN MR, 1998, ANN C SERIES, V98, P299
   Cadík M, 2008, COMPUT GRAPH-UK, V32, P330, DOI 10.1016/j.cag.2008.04.003
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851
   Cole F, 2008, ACM T GRAPHIC, V27, DOI [10.1145/1360612.1360657, 10.1145/1360612.1360687]
   DeCarlo D, 2002, ACM T GRAPHIC, V21, P769, DOI 10.1145/566570.566650
   Drago F, 2003, COMPUT GRAPH FORUM, V22, P419, DOI 10.1111/1467-8659.00689
   Durand F, 2002, ACM T GRAPHIC, V21, P257, DOI 10.1145/566570.566574
   Elder JH, 2001, IEEE T PATTERN ANAL, V23, P291, DOI 10.1109/34.910881
   Farbman Z, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360666
   Fattal R, 2002, ACM T GRAPHIC, V21, P249
   FERWERDA JA, 1997, ANN C SERIES, V97, P143
   FREEMAN WT, 1991, IEEE T PATTERN ANAL, V13, P891, DOI 10.1109/34.93808
   GEORGESON MA, 1975, J PHYSIOL-LONDON, V252, P627, DOI 10.1113/jphysiol.1975.sp011162
   GEORGESON MA, 2007, J VIS, V7, P13
   Jansen M., 2005, Second generation wavelets and applications
   Kuang JT, 2007, ACM T APPL PERCEPT, V4, DOI 10.1145/1265957.1265958
   LEGGE GE, 1980, J OPT SOC AM, V70, P1458, DOI 10.1364/JOSA.70.001458
   Lindeberg T., 1996, INT J COMPUTER VISIO, V30, P77
   Mantiuk R., 2006, ACM Transactions on Applied Perception, V3, P286, DOI DOI 10.1145/1166087.1166095
   Mantiuk R, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360667
   MARR D, 1980, PROC R SOC SER B-BIO, V207, P187, DOI 10.1098/rspb.1980.0020
   ORZAN A, 2007, P INT S NONPH AN REN
   PATTANAIK SN, 1998, ANN C SERIES, V98, P287
   Pellegrino FA, 2004, IEEE T SYST MAN CY B, V34, P1500, DOI 10.1109/TSMCB.2004.824147
   Pérez P, 2003, ACM T GRAPHIC, V22, P313, DOI 10.1145/882262.882269
   PERONA P, 1990, IEEE T PATTERN ANAL, V12, P629, DOI 10.1109/34.56205
   Reinhard E, 2002, ACM T GRAPHIC, V21, P267, DOI 10.1145/566570.566575
   Rubinstein M, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531329
   Sweldens W, 1998, SIAM J MATH ANAL, V29, P511, DOI 10.1137/S0036141095289051
   TAYLOR MM, 1967, J ACOUST SOC AM, V41, P782, DOI 10.1121/1.1910407
   Tumblin J, 1999, COMP GRAPH, P83, DOI 10.1145/311535.311544
   Uytterhoeven G., 1997, WAVELET TRANSFORMS U
   Wandell B. A, 1995, Foundations of vision
   Wang J, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1330511.1330512
   WARD G, 2006, P 3 S APPL PERC GRAP, V150, P3
   Watson A.B., 1993, DIGITAL IMAGES HUMAN, P179
   Watson AB, 1997, J OPT SOC AM A, V14, P2379, DOI 10.1364/JOSAA.14.002379
   WHITTLE P, 1986, VISION RES, V26, P1677, DOI 10.1016/0042-6989(86)90055-6
   WILSON HR, 1980, BIOL CYBERN, V38, P171, DOI 10.1007/BF00337406
   ZENG W, 2000, P IEEE INT C IM PROC, V2, P37
   ZIOU D, 1997, INT J PATTERN RECOGN
NR 45
TC 3
Z9 3
U1 0
U2 0
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD JUL
PY 2010
VL 7
IS 4
AR 27
DI 10.1145/1823738.1823745
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 633ZI
UT WOS:000280546500007
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Li, YF
   Patoglu, V
   O'Malley, MK
AF Li, Yanfang
   Patoglu, Volkan
   O'Malley, Marcia K.
TI Negative Efficacy of Fixed Gain Error Reducing Shared Control for
   Training in Virtual Environments
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Experimentation; Human Factors; Shared control; manual control; haptic
   assistance; virtual training; motor skill training
ID HAPTIC INTERFACE; PERFORMANCE; DYNAMICS; FORCE
AB Virtual reality with haptic feedback provides a safe and versatile practice medium for many manual control tasks. Haptic guidance has been shown to improve performance of manual control tasks in virtual environments; however, the efficacy of haptic guidance for training in virtual environments has not been studied conclusively. This article presents experimental results that show negative efficacy of haptic guidance during training in virtual environments. The haptic guidance in this study is a fixed-gain error-reducing shared controller, with the control effort overlaid on the dynamics of the manual control task during training. Performance of the target-hitting manual control task in the absence of guidance is compared for three training protocols. One protocol contained no haptic guidance and represented virtual practice. Two protocols utilized haptic guidance, varying the duration of exposure to guidance during the training sessions. Exposure to the fixed-gain error-reducing shared controller had a detrimental effect on performance of the target-hitting task at the conclusion of a month-long training protocol, regardless of duration of exposure. While the shared controller was designed with knowledge of the task and an intuitive sense of the motions required to achieve good performance, the results indicate that the acquisition of motor skill is a complex phenomenon that is not aided with haptic guidance during training as implemented in this experiment.
C1 [Li, Yanfang; O'Malley, Marcia K.] Rice Univ, Houston, TX 77005 USA.
   [Patoglu, Volkan] Sabanci Univ, TR-34956 Istanbul, Turkey.
C3 Rice University; Sabanci University
RP O'Malley, MK (corresponding author), Rice Univ, Houston, TX 77005 USA.
EM yvonneli@rice.edu; vpatoglu@sabanciuniv.edu; omalleym@rice.edu
RI li, chunyuan/IQW-1618-2023; Patoglu, Volkan/AAC-5081-2019; li,
   yan/GTI-4638-2022
OI Patoglu, Volkan/0000-0001-6644-3937; O'Malley,
   Marcia/0000-0002-3563-1051
FU Office of Naval Research [N00014-04-1-0517]
FX This work was supported in part by the Office of Naval Research, grant
   number N00014-04-1-0517.
CR Adams R. J., 2001, HAPTICS E, V2
   Bernstein N., 1967, COORDNATION REGULATI
   Bettini A, 2002, 2002 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION, VOLS I-IV, PROCEEDINGS, P3354, DOI 10.1109/ROBOT.2002.1014229
   BrashersKrug T, 1996, NATURE, V382, P252, DOI 10.1038/382252a0
   Emken JL, 2005, IEEE T NEUR SYS REH, V13, P33, DOI 10.1109/TNSRE.2004.843173
   Feygin D, 2002, 10TH SYMPOSIUM ON HAPTIC INTERFACES FOR VIRTUAL ENVIRONMENT AND TELEOPERATOR SYSTEMS, PROCEEDINGS, P40, DOI 10.1109/HAPTIC.2002.998939
   Gamberini L, 2000, CYBERPSYCHOL BEHAV, V3, P337, DOI 10.1089/10949310050078779
   GILLESPIE RB, 1998, P ASME INT MECH ENG, P3354
   Griffiths PG, 2005, HUM FACTORS, V47, P574, DOI 10.1518/001872005774859944
   Haanpaa DP, 1997, COMPUT GRAPH, V21, P443, DOI 10.1016/S0097-8493(97)00017-4
   Henmi K, 1998, IEEE INT CONF ROBOT, P1275, DOI 10.1109/ROBOT.1998.677278
   Huang FC, 2007, J MOTOR BEHAV, V39, P179, DOI 10.3200/JMBR.39.3.179-193
   Ivanchenko V, 2003, NEURAL COMPUT, V15, P2051, DOI 10.1162/089976603322297287
   Kahn LE, 2004, P ANN INT IEEE EMBS, V26, P2722
   Kikuuwe R, 2001, IEEE INT CONF ROBOT, P868, DOI 10.1109/ROBOT.2001.932659
   KOZAK JJ, 1993, ERGONOMICS, V36, P777, DOI 10.1080/00140139308967941
   Li Y., 2006, EUROHAPTICS 2006
   LINTERN G, 1991, HUM FACTORS, V33, P251, DOI 10.1177/001872089103300302
   LINTERN G, 1990, HUM FACTORS, V32, P299, DOI 10.1177/001872089003200304
   Lintern G., 1980, VISUAL CUE AUGMENTAT
   MASSIMINO MJ, 1994, HUM FACTORS, V36, P145, DOI 10.1177/001872089403600109
   Meech J.F., 1996, VIRTUAL REALITY USER, V68, P1
   Nudehi SS, 2005, IEEE T CONTR SYST T, V13, P588, DOI 10.1109/TCST.2004.843131
   O'Malley MK, 2006, J DYN SYST-T ASME, V128, P75, DOI 10.1115/1.2168160
   O'Malley MK, 2003, 11TH SYMPOSIUM ON HAPTIC INTERFACES FOR VIRTUAL ENVIRONMENT AND TELEOPERATOR SYSTEMS - HAPTICS 2003, PROCEEDINGS, P348, DOI 10.1109/HAPTIC.2003.1191308
   OMALLEY MK, 2003, P AIAA SPAC C
   Patton JL, 2004, IEEE T BIO-MED ENG, V51, P636, DOI 10.1109/TBME.2003.821035
   Patton JL, 2001, P ANN INT IEEE EMBS, V23, P1356, DOI 10.1109/IEMBS.2001.1020448
   Richard P, 1995, RO-MAN'95 TOKYO: 4TH IEEE INTERNATIONAL WORKSHOP ON ROBOT AND HUMAN COMMUNICATION, PROCEEDINGS, P301, DOI 10.1109/ROMAN.1995.531976
   ROSENBERG LB, 1993, IEEE VIRTUAL REALITY ANNUAL INTERNATIONAL SYMPOSIUM, P76, DOI 10.1109/VRAIS.1993.380795
   SHADMEHR R, 1994, J NEUROSCI, V14, P3208
   Shadmehr R., 1995, Advances in Neural Information Processing Systems 7, P1117
   SIIPOLA EM, 1993, AM J PSYCHIAT, V45, P205
   Sterr A, 2002, ARCH PHYS MED REHAB, V83, P1374, DOI 10.1053/apmr.2002.35108
   Todorov E, 1997, J MOTOR BEHAV, V29, P147, DOI 10.1080/00222899709600829
   Williams LEP, 2002, 2002 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION, VOLS I-IV, PROCEEDINGS, P1249, DOI 10.1109/ROBOT.2002.1014714
   WOLPERT DM, 1995, EXP BRAIN RES, V103, P460
   Yokokohji Y, 1996, RO-MAN '96 - 5TH IEEE INTERNATIONAL WORKSHOP ON ROBOT AND HUMAN COMMUNICATION, PROCEEDINGS, P32, DOI 10.1109/ROMAN.1996.568646
   Yoneda M, 1999, ICRA '99: IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION, VOLS 1-4, PROCEEDINGS, P2924, DOI 10.1109/ROBOT.1999.774041
NR 39
TC 40
Z9 53
U1 0
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD FEB
PY 2009
VL 6
IS 1
AR 3
DI 10.1145/1462055.1462058
PG 21
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA 450YM
UT WOS:000266437900003
DA 2024-07-18
ER

PT J
AU Peters, RJ
   Itti, L
AF Peters, Robert J.
   Itti, Laurent
TI Applying Computational Tools to Predict Gaze Direction in Interactive
   Visual Environments
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Algorithms; Experimentation; Human factors; Active vision; computational
   modeling; eye-movements; immersive environments; video games; visual
   attention
ID EYE-MOVEMENTS; ATTENTION; STRATEGIES; ALGORITHMS; PATTERNS; REGIONS;
   MEMORY; FILM
AB Future interactive virtual environments will be "attention-aware," capable of predicting, reacting to, and ultimately influencing the visual attention of their human operators. Before such environments can be realized, it is necessary to operationalize our understanding of the relevant aspects of visual perception, in the form of fully automated computational heuristics that can efficiently identify locations that would attract human gaze in complex dynamic environments. One promising approach to designing such heuristics draws on ideas from computational neuroscience. We compared several neurobiologically inspired heuristics with eye-movement recordings from five observers playing video games, and found that human gaze was better predicted by heuristics that detect outliers from the global distribution of visual features than by purely local heuristics. Heuristics sensitive to dynamic events performed best overall. Further, heuristic prediction power differed more between games than between different human observers. While other factors clearly also influence eye position, our findings suggest that simple neurally inspired algorithmic methods can account for a significant portion of human gaze behavior in a naturalistic, interactive setting. These algorithms may be useful in the implementation of interactive virtual environments, both to predict the cognitive state of human operators, as well as to effectively endow virtual agents in the system with humanlike visual behavior.
C1 [Peters, Robert J.; Itti, Laurent] Univ So Calif, Dept Comp Sci, Los Angeles, CA 90007 USA.
   [Itti, Laurent] Univ So Calif, Dept Neurosci, Los Angeles, CA 90007 USA.
   [Itti, Laurent] Univ So Calif, Dept Psychol, Los Angeles, CA 90007 USA.
C3 University of Southern California; University of Southern California;
   University of Southern California
RP Peters, RJ (corresponding author), Univ So Calif, Dept Comp Sci, Los Angeles, CA 90007 USA.
EM rjpeters@usc.edu
FU National Geospatial-Intelligence Agency (NGA); Directorate of Central
   Intelligence (DCI)
FX Support for this work was provided by the National
   Geospatial-Intelligence Agency (NGA) and the Directorate of Central
   Intelligence (DCI). The authors affirm that the views expressed herein
   are solely their own, and do not represent the views of the United
   States government or any agency thereof.
CR Bailenson JN, 2005, PSYCHOL SCI, V16, P814, DOI 10.1111/j.1467-9280.2005.01619.x
   BALLARD DH, 1995, J COGNITIVE NEUROSCI, V7, P66, DOI 10.1162/jocn.1995.7.1.66
   BURT PJ, 1983, IEEE T COMMUN, V31, P532, DOI 10.1109/TCOM.1983.1095851
   CARMI R, 2004, P VIS SCI SOC ANN M, P20
   Finney SA, 2001, BEHAV RES METH INS C, V33, P167, DOI 10.3758/BF03195362
   GREENSPAN H, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P222, DOI 10.1109/CVPR.1994.323833
   Hayhoe M. M., 2002, Proceedings ETRA 2002. Eye Tracking Research and Applications Symposium, P7, DOI 10.1145/507072.507074
   Hayhoe MM, 2003, J VISION, V3, P49, DOI 10.1167/3.1.6
   Hayhoe MM, 2004, INFANCY, V6, P267, DOI 10.1207/s15327078in0602_7
   Henderson JM, 1999, ANNU REV PSYCHOL, V50, P243, DOI 10.1146/annurev.psych.50.1.243
   Itti L, 2005, PROC CVPR IEEE, P631
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Itti L, 2003, PROC SPIE, V5200, P64, DOI 10.1117/12.512618
   Itti L, 2001, J ELECTRON IMAGING, V10, P161, DOI 10.1117/1.1333677
   KOCH C, 1985, HUM NEUROBIOL, V4, P219
   Land M, 1999, PERCEPTION, V28, P1311, DOI 10.1068/p2935
   Land MF, 2001, VISION RES, V41, P3559, DOI 10.1016/S0042-6989(01)00102-X
   Mannan SK, 1997, PERCEPTION, V26, P1059, DOI 10.1068/p261059
   May J, 2003, HUM-COMPUT INTERACT, V18, P325, DOI 10.1207/S15327051HCI1804_1
   Najemnik J, 2005, NATURE, V434, P387, DOI 10.1038/nature03390
   Navalpakkam V, 2005, VISION RES, V45, P205, DOI 10.1016/j.visres.2004.07.042
   NAVALPAKKAM V, 2006, ADV NEURAL INFORM PR, V19, P1
   Park RM, 2002, AM J IND MED, V42, P1, DOI 10.1002/ajim.10082
   PARKER RE, 1978, J EXP PSYCHOL HUMAN, V4, P284, DOI 10.1037/0096-1523.4.2.284
   Parkhurst DJ, 2004, EUR J NEUROSCI, V19, P783, DOI 10.1111/j.0953-816X.2003.03183.x
   PELI V, 2005, P 4 STARKF C VIS MOV, P18
   Peters C, 2003, COMP ANIM CONF PROC, P111, DOI 10.1109/CASA.2003.1199311
   Peters RJ, 2005, VISION RES, V45, P2397, DOI 10.1016/j.visres.2005.03.019
   Pomplun M, 2006, VISION RES, V46, P1886, DOI 10.1016/j.visres.2005.12.003
   Privitera CM, 2000, IEEE T PATTERN ANAL, V22, P970, DOI 10.1109/34.877520
   Privitera CM, 1998, PATTERN RECOGN LETT, V19, P1037, DOI 10.1016/S0167-8655(98)00077-4
   Rao RPN, 2002, VISION RES, V42, P1447, DOI 10.1016/S0042-6989(02)00040-8
   Rayner K, 1998, PSYCHOL BULL, V124, P372, DOI 10.1037/0033-2909.124.3.372
   Reinagel P, 1999, NETWORK-COMP NEURAL, V10, P341, DOI 10.1088/0954-898X/10/4/304
   Rensink RA, 2000, VIS COGN, V7, P17, DOI 10.1080/135062800394667
   Sodhi M., 2002, Proceedings ETRA 2002. Eye Tracking Research and Applications Symposium, P61, DOI 10.1145/507072.507086
   STAMPE DM, 1993, BEHAV RES METH INSTR, V25, P137, DOI 10.3758/BF03204486
   Terzopoulos D., 1997, Videre: Journal of Computer Vision Research, V1, P2
   Toet A, 2006, COMPUT HUM BEHAV, V22, P615, DOI 10.1016/j.chb.2005.12.010
   Torralba A, 2003, J OPT SOC AM A, V20, P1407, DOI 10.1364/JOSAA.20.001407
   Tosi V, 1997, INT J NEUROSCI, V92, P47, DOI 10.3109/00207459708986388
   TREISMAN AM, 1980, COGNITIVE PSYCHOL, V12, P97, DOI 10.1016/0010-0285(80)90005-5
   Voge S, 1999, LEONARDO, V32, P325
   Wolfe JM, 2004, NAT REV NEUROSCI, V5, P495, DOI 10.1038/nrn1411
   Yarbus A. L., 1967, Eye Movements and Vision
   ZETZSCHE C, 1998, P 5 INT C SOC AD BEH, V5, P120
NR 46
TC 63
Z9 69
U1 1
U2 21
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD MAY
PY 2008
VL 5
IS 2
AR 9
DI 10.1145/1279920.1279923
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA 450YJ
UT WOS:000266437600003
DA 2024-07-18
ER

PT J
AU Seward, AE
   Ashmead, DH
   Bodenheimer, B
AF Seward, A. Elizabeth
   Ashmead, Daniel H.
   Bodenheimer, Bobby
TI Using Virtual Environments to Assess Time-to-Contact Judgments from
   Pedestrian Viewpoints
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Experimentation; Human Factors; Measurement; Virtual reality (VR);
   time-to-contact (TTC)
AB This paper describes the use of desktop and immersive virtual environments to study judgments that pedestrians make when deciding to cross a street. In particular, we assess the ability of people to discriminate and estimate time-to-contact (TTC) for approaching vehicles under a variety of conditions. Four experiments observing TTC judgments under various conditions are described. We examine the effect of type of vehicle, viewpoint, presentation mode, and TTC value on TTC judgments. We find no significant effect of type of vehicle or of viewpoint, extending prior work to cover all views typically encountered by pedestrians. Discrimination of short values for TTC judgments is generally consistent with the literature, but performance degrades significantly for long TTC values. Finally, we find no significant difference between judgments made in a desktop environment versus a head-mounted display, indicating that tracking the approaching vehicle with one's head does not aid discrimination. In general, people appear to use strategies similar to those that pedestrians use to make real-world, street-crossing decisions.
C1 [Seward, A. Elizabeth; Bodenheimer, Bobby] Vanderbilt Univ, VU Stn B 351679, Nashville, TN 37235 USA.
C3 Vanderbilt University
RP Seward, AE (corresponding author), Vanderbilt Univ, VU Stn B 351679, 2301 Vanderbilt Pl, Nashville, TN 37235 USA.
EM anne.e.seward@vanderbilt.edu; bobby.bodenheimer@vanderbilt.edu;
   daniel.h.ashmead@vanderbilt.edu
CR Ashmead DH, 2005, J TRANSP ENG, V131, P812, DOI 10.1061/(ASCE)0733-947X(2005)131:11(812)
   Benguigui N, 2003, J EXP PSYCHOL HUMAN, V29, P1083, DOI 10.1037/0096-1523.29.6.1083
   Bootsma RJ, 2002, PERCEPTION, V31, P901, DOI 10.1068/p3230
   CAIRD JK, 1994, ECOL PSYCHOL, V6, P83, DOI 10.1207/s15326969eco0602_1
   Caljouw SR, 2004, EXP BRAIN RES, V155, P427, DOI 10.1007/s00221-003-1739-3
   CAVALLO V, 1988, PERCEPTION, V17, P623, DOI 10.1068/p170623
   DeLucia PR, 1997, PERCEPT PSYCHOPHYS, V59, P913, DOI 10.3758/BF03205508
   Gray R, 1999, PERCEPTION, V28, P1257, DOI 10.1068/p2895
   Guth D, 2005, HUM FACTORS, V47, P314, DOI 10.1518/0018720054679533
   HEUER H, 1993, PERCEPTION, V22, P549, DOI 10.1068/p220549
   Hofsten C. V., 1985, Persistence and change: Proceedings of the first international conference on event perception, P231
   *IOW DEP TRANSP, 2005, GS01008 IOW DEP TRAN
   KIEFER R, 2006, HUM FACTORS, V48, P384
   Kim NG, 2006, VISION RES, V46, P1946, DOI 10.1016/j.visres.2005.12.011
   LEE DN, 1976, PERCEPTION, V5, P437, DOI 10.1068/p050437
   Manser MP, 1996, ECOL PSYCHOL, V8, P71, DOI 10.1207/s15326969eco0801_4
   MCLEOD RW, 1983, PERCEPTION, V12, P417, DOI 10.1068/p120417
   *NAT CTR STAT AN, 2005, TRAFF SAF FACTS 2005
   PITT R, 1990, ACCIDENT ANAL PREV, V22, P549, DOI 10.1016/0001-4575(90)90027-I
   Plumert JM, 2004, CHILD DEV, V75, P1243, DOI 10.1111/j.1467-8624.2004.00736.x
   REGAN D, 1993, VISION RES, V33, P447, DOI 10.1016/0042-6989(93)90252-R
   SCHIFF W, 1990, J EXP PSYCHOL HUMAN, V16, P303, DOI 10.1037/0096-1523.16.2.303
   SCHIFF W, 1979, PERCEPTION, V8, P647, DOI 10.1068/p080647
   Servos P, 1998, EXP BRAIN RES, V119, P92, DOI 10.1007/s002210050323
   SEWARD AE, 2006, APGV 2006, P29
   Sidaway B, 1996, HUM FACTORS, V38, P101, DOI 10.1518/001872096778940813
   Smeets JBJ, 1996, PERCEPTION, V25, P583, DOI 10.1068/p250583
   TODD JT, 1981, J EXP PSYCHOL HUMAN, V7, P795, DOI 10.1037/0096-1523.7.4.795
   TRESILIAN JR, 1995, PERCEPT PSYCHOPHYS, V57, P231, DOI 10.3758/BF03206510
   TRESILIAN JR, 1994, J EXP PSYCHOL HUMAN, V20, P154, DOI 10.1037/0096-1523.20.1.154
   VONHOFSTEN C, 1992, J MOTOR BEHAV, V24, P328
   WETHERILL GB, 1965, BRIT J MATH STAT PSY, V18, P1, DOI 10.1111/j.2044-8317.1965.tb00689.x
NR 32
TC 14
Z9 19
U1 0
U2 12
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD NOV
PY 2007
VL 4
IS 3
AR 18
DI 10.1145/1278387.1278392
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA V04IN
UT WOS:000207052200005
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Sprague, N
   Ballard, D
   Robinson, A
AF Sprague, Nathan
   Ballard, Dana
   Robinson, Al
TI Modeling Embodied Visual Behaviors
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Theory; Experimentation; Visual routines; visual attention;
   reinforcement learning
ID CAPACITY; BINDING; VISION; SEARCH; MEMORY
AB To make progess in understanding human visuomotor behavior, we will need to understand its basic components at an abstract level. One way to achieve such an understanding would be to create a model of a human that has a sufficient amount of complexity so as to be capable of generating such behaviors. Recent technological advances have been made that allow progress to be made in this direction. Graphics models that simulate extensive human capabilities can be used as platforms from which to develop synthetic models of visuomotor behavior. Currently, such models can capture only a small portion of a full behavioral repertoire, but for the behaviors that they do model, they can describe complete visuomotor subsystems at a useful level of detail. The value in doing so is that the body's elaborate visuomotor structures greatly simplify the specification of the abstract behaviors that guide them. The net result is that, essentially, one is faced with proposing an embodied "operating system" model for picking the right set of abstract behaviors at each instant. This paper outlines one such model. A centerpiece of the model uses vision to aid the behavior that has the most to gain from taking environmental measurements. Preliminary tests of the model against human performance in realistic VR environments show that main features of the model show up in human behavior.
C1 [Sprague, Nathan] Kalamazoo Coll, Kalamazoo, MI 49007 USA.
   [Ballard, Dana; Robinson, Al] Univ Rochester, Rochester, NY USA.
C3 Kalamazoo College; University of Rochester
RP Sprague, N (corresponding author), Kalamazoo Coll, Kalamazoo, MI 49007 USA.
EM dana@cs.utexas.edu
CR Aivar MP, 2005, J VISION, V5, P177, DOI 10.1167/5.3.3
   [Anonymous], 1996, ADV NEURAL INFORM PR
   [Anonymous], P 4 INT C SIM AD BEH
   [Anonymous], 2004, ACM Transactions on Applied Perception (TAP), DOI DOI 10.1145/1008722.1008727
   Arkin R., 1998, BEHAV BASED ROBOTICS
   BABCOCK J, 2000, ACM SIGCHI EYE TRACK
   BADDELEY A, 1989, WORKING MEMORY
   BALLARD D, 2002, J VISION, V2, pA568
   BALLARD DH, 1995, J COGNITIVE NEUROSCI, V7, P66, DOI 10.1162/jocn.1995.7.1.66
   Ballard DH, 1997, BEHAV BRAIN SCI, V20, P723, DOI 10.1017/S0140525X97001611
   BROOKS WP, 1986, LETT APPL MICROBIOL, V2, P1, DOI 10.1111/j.1472-765X.1986.tb01502.x
   Bryson J. J., 2001, INT JOINT C ART INT
   Clark A, 1999, TRENDS COGN SCI, V3, P345, DOI 10.1016/S1364-6613(99)01361-3
   Faloutsos P, 2001, COMPUT GRAPH-UK, V25, P933, DOI 10.1016/S0097-8493(01)00171-6
   Firby RJ, 1995, INT JOINT CONF ARTIF, P72
   HARTLEY R, 1991, P INT CONV ROB AUT
   Hayhoe MM, 2003, J VISION, V3, P49, DOI 10.1167/3.1.6
   Hayhoe MM, 1998, VISION RES, V38, P125, DOI 10.1016/S0042-6989(97)00116-8
   Itti L, 2000, VISION RES, V40, P1489, DOI 10.1016/S0042-6989(99)00163-7
   JAGERSAND M, 1996, P 4 EUR C COMP VIS, P603
   JOHANSSON R, 1999, PERCEPTION, V28, P1311
   Kaelbling LP, 1996, J ARTIF INTELL RES, V4, P237, DOI 10.1613/jair.301
   KARLSSON J, 1997, THESIS U ROCHESTER R
   Kosslyn S.M., 1977, Cognitive Science, V1, P265, DOI [10.1207/s15516709cog0103_2, DOI 10.1207/S15516709COG0103_2]
   Land M, 1999, PERCEPTION, V28, P1311, DOI 10.1068/p2935
   Luck SJ, 1997, NATURE, V390, P279, DOI 10.1038/36846
   Marr D., 1982, Vision
   MILLER GA, 1956, PSYCHOL REV, V63, P81, DOI 10.1037/h0043158
   MORAY N, 1986, HDB PERCEPTION HUMAN, V2
   Newell A., 1994, UNIFIED THEORIES COG
   Newell FN, 2001, PSYCHOL SCI, V12, P37, DOI 10.1111/1467-9280.00307
   NILSSON N, 1984, 223 SRI INT
   Noe A., 2005, ACTION PERCEPTION
   O'Regan JK, 2001, BEHAV BRAIN SCI, V24, P939, DOI 10.1017/S0140525X01000115
   Pashler H, 1998, PSYCHOL ATTENTION
   Pelz J, 2001, EXP BRAIN RES, V139, P266, DOI 10.1007/s002210100745
   Razzaque Sharif, 2002, 8 EUR WORKSH VIRT EN
   Roelfsema PR, 2000, VISION RES, V40, P1385, DOI 10.1016/S0042-6989(00)00004-3
   Roelfsema PR, 2003, P NATL ACAD SCI USA, V100, P5467, DOI 10.1073/pnas.0431051100
   Roskies AL, 1999, NEURON, V24, P7, DOI 10.1016/S0896-6273(00)80817-X
   Roy DK, 2002, COGNITIVE SCI, V26, P113, DOI 10.1207/s15516709cog2601_4
   SPRAGUE N, 2003, ADV NEURAL INFORM PR, V15
   Sprague N., 2003, INT JOINT C ART INT
   SPRAGUE N, 2003, 798 U ROCH COMP SCI
   SPRAGUE N, 2004, THESIS U ROCHESTER D
   Suri RE, 2001, NEURAL COMPUT, V13, P841, DOI 10.1162/089976601300014376
   Sutton R., 1998, Reinforcement Learning: An Introduction
   Terzopoulos D., 1997, Videre: Journal of Computer Vision Research, V1, P2
   Triesch J, 2003, J VISION, V3, P86, DOI 10.1167/3.1.9
   Trommershäuser J, 2003, J OPT SOC AM A, V20, P1419, DOI 10.1364/JOSAA.20.001419
   ULLMAN S, 1984, COGNITION, V18, P97, DOI 10.1016/0010-0277(84)90023-4
   VanRullen R, 2004, J COGNITIVE NEUROSCI, V16, P4, DOI 10.1162/089892904322755502
   von der Malsburg C, 1999, NEURON, V24, P95, DOI 10.1016/S0896-6273(00)80825-9
   Watkins C.J.C.H., 1989, THESIS KINGS COLL OX
   WATKINS CJCH, 1992, MACH LEARN, V8, P279, DOI 10.1007/BF00992698
NR 55
TC 73
Z9 89
U1 0
U2 14
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD JUL
PY 2007
VL 4
IS 2
AR 11
DI 10.1145/1265957.1265960
PG 23
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA V04IM
UT WOS:000207052100003
DA 2024-07-18
ER

PT J
AU Lichtenstein, L
   Barabas, J
   Woods, RL
   Peli, E
AF Lichtenstein, Lee
   Barabas, James
   Woods, Russell L.
   Peli, Eli
TI A Feedback-Controlled Interface for Treadmill Locomotion in Virtual
   Environments
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Experimentation; Human Factors; Measurement; Locomotion; low-vision;
   speed-matching; treadmill
ID PERIPHERAL-VISION; WALKING SPEED; PERCEPTION; TARGET
AB Virtual environments (VEs) allow safe, repeatable, and controlled evaluations of obstacle avoidance and navigation performance of people with visual impairments using visual aids. Proper simulation of mobility in a VE requires an interface, which allows subjects to set their walking pace. Using conventional treadmills, the subject can change their walking speed by pushing the tread with their feet, while leveraging handrails or ropes (self-propelled mode). We developed a feedback-controlled locomotion interface that allows the VE workstation to control the speed of the treadmill, based on the position of the user. The position and speed information is also used to implement automated safety measures, so that the treadmill can be halted in case of erratic behavior.
   We compared the feedback-controlled to the self-propelled mode by using speed-matching tasks (follow a moving object or match the speed of an independently moving scene) to measure the efficacy of each mode in maintaining constant subject position, subject control of the treadmill, and subject pulse rates. In addition, we measured the perception of speed in the VE on each mode.
   The feedback-controlled mode required less physical exertion than self-propelled. The average position of subjects on the feedback-controlled treadmill was always within a centimeter of the desired position. There was a smaller standard deviation in subject position when using the self-propelled mode than when using the feedback-controlled mode, but the difference averaged less than 6 cm across all subjects walking at a constant speed. Although all subjects underestimated the speed of an independently moving scene at higher speeds, their estimates were more accurate when using the feedback-controlled treadmill than the self-propelled.
C1 [Lichtenstein, Lee; Barabas, James; Woods, Russell L.; Peli, Eli] Harvard Univ, Sch Med, Schepens Eye Res Inst, Boston, MA 02114 USA.
C3 Harvard University; Schepens Eye Research Institute; Harvard Medical
   School
RP Lichtenstein, L (corresponding author), Percept Informat, Waltham, MA USA.
EM eli.peli@schepens.havard.edu
RI ; Woods, Russell/O-2600-2015
OI Peli, Eli/0000-0002-1340-9257; Woods, Russell/0000-0002-7193-1211
FU NIH [EY12890]
FX Supported in part by NIH grant # EY12890. We thank Richard Price of the
   Physics Department, University of Texas at Brownsville, for useful
   comments on drafts of this manuscript. We thank Aaron J. Mandel of SERI
   for his help gathering treadmill calibration data.
CR [Anonymous], 2004, Control System Engineering
   Apfelbaum H, 2007, ACM T APPL PERCEPT, V4, DOI 10.1145/1227134.1227142
   BAKDASH JZ, 2005, J VISION, V5, pA747
   Banton T, 2005, PRESENCE-TELEOP VIRT, V14, P394, DOI 10.1162/105474605774785262
   Barabas J, 2004, BEHAV RES METH INS C, V36, P757, DOI 10.3758/BF03206556
   Bardy BG, 1999, PERCEPT PSYCHOPHYS, V61, P1356, DOI 10.3758/BF03206186
   Chaudhury S, 2004, EXP BRAIN RES, V159, P360, DOI 10.1007/s00221-004-1961-7
   Cutting JE, 2002, PERCEPT PSYCHOPHYS, V64, P415, DOI 10.3758/BF03194714
   CUTTING JE, 1995, PSYCHOL REV, V102, P627, DOI 10.1037/0033-295X.102.4.627
   Darken R. P., 1997, Proceedings of the ACM Symposium on User Interface Software and Technology. 10th Annual Symposium. UIST '97, P213, DOI 10.1145/263407.263550
   Distler HK, 1998, PERCEPTION, V27, P139
   Durgin FH, 2005, J EXP PSYCHOL HUMAN, V31, P398, DOI 10.1037/0096-1523.31.3.398
   Durgin FH, 2005, J EXP PSYCHOL HUMAN, V31, P339, DOI 10.1037/0096-1523.31.2.339
   Fajen BR, 2004, PERCEPTION, V33, P689, DOI 10.1068/p5236
   Fajen BR, 2003, J EXP PSYCHOL HUMAN, V29, P343, DOI 10.1037/0096-1523.29.2.343
   Foo P, 2005, J EXP PSYCHOL LEARN, V31, P195, DOI 10.1037/0278-7393.31.2.195
   Gottlieb D D, 1992, J Am Optom Assoc, V63, P581
   Harris LR, 2000, EXP BRAIN RES, V135, P12, DOI 10.1007/s002210000504
   HOLLERBACH JM, 2000, HAPTICS S P ASME DYN, V6972, P1293
   Iwata H, 1999, P IEEE VIRT REAL ANN, P286, DOI 10.1109/VR.1999.756964
   IWATA H, 1999, ROBOTICS RES, P220
   KNAPP CH, 1976, IEEE T ACOUST SPEECH, V24, P320, DOI 10.1109/TASSP.1976.1162830
   Li L, 2000, VISION RES, V40, P3873, DOI 10.1016/S0042-6989(00)00196-6
   LICHTENSTEIN L, 2006, SID INT S SAN FRANC, V37, P295
   Loomis JM, 2003, VIRTUAL AND ADAPTIVE ENVIRONMENTS: APPLICATIONS, IMPLICATIONS, AND HUMAN PERFORMANCE ISSUES, P21
   LOOMIS JM, 1992, J EXP PSYCHOL HUMAN, V18, P906, DOI 10.1037/0096-1523.18.4.906
   LOOMIS JM, 1992, INT SOC OPTICAL ENG, V3, P590
   Minetti AE, 2003, J APPL PHYSIOL, V95, P838, DOI 10.1152/japplphysiol.00128.2003
   MURRAY MP, 1985, J APPL PHYSIOL, V59, P87, DOI 10.1152/jappl.1985.59.1.87
   Peli E, 2000, OPTOMETRY VISION SCI, V77, P453, DOI 10.1097/00006324-200009000-00006
   Peli Eli., 2000, Vision'99: International Conference on Low Vision, P70
   Proffitt DR, 2003, PSYCHOL SCI, V14, P106, DOI 10.1111/1467-9280.t01-1-01427
   Prokop T, 1997, EXP BRAIN RES, V114, P63, DOI 10.1007/PL00005624
   RIZZO M, 2005, CARSS COORDINATED AS
   Schubert M, 2005, MOVEMENT DISORD, V20, P141, DOI 10.1002/mds.20281
   Soong GP, 2004, OPHTHAL PHYSL OPT, V24, P291, DOI 10.1111/j.1475-1313.2004.00196.x
   Thurrell AEI, 1998, PERCEPTION, V27, P147
   Wann JP, 2000, VISION RES, V40, P2533, DOI 10.1016/S0042-6989(00)00115-2
   WELLS M, 1996, VIRT REAL ANN INT S, P1
   Willemsen Peter., 2002, P IEEE VIRTUAL REALI, P1
   Witmer BG, 1998, PRESENCE-TELEOP VIRT, V7, P144, DOI 10.1162/105474698565640
   WOODS R, 2003, PERCEIVED COLLISION, P4321
   WOODS RL, 2005, J VISION, V4, P814
   *WOODW, 2004, WOODW US TREADM CONT
   Yates D., 1999, The Practice of Statistics, V1st
   [No title captured]
NR 46
TC 33
Z9 43
U1 0
U2 6
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD JAN
PY 2007
VL 4
IS 1
AR 7
DI 10.1145/1227134.1227141
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA V04IL
UT WOS:000207052000007
PM 18167515
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Howard, T
   Driller, K
   Frier, W
   Pacchierotti, C
   Marchal, M
   Hartcher-O'Brien, J
AF Howard, Thomas
   Driller, Karina
   Frier, William
   Pacchierotti, Claudio
   Marchal, Maud
   Hartcher-O'Brien, Jessica
TI Gap Detection in Pairs of Ultrasound Mid-air Vibrotactile Stimuli
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Ultrasound haptics; mid-air haptics; haptic perception
ID TACTILE; DISCRIMINATION
AB Ultrasound mid-air haptic (UMH) devices are a novel tool for haptic feedback, capable of providing localized vibrotactile stimuli to users at a distance. UMH applications largely rely on generating tactile shape outlines on the users' skin. Here we investigate how to achieve sensations of continuity or gaps within such two-dimensional curves by studying the perception of pairs of amplitude-modulated focused ultrasound stimuli. On the one hand, we aim to investigate perceptual effects that may arise from providing simultaneous UMH stimuli. On the other hand, we wish to provide perception-based rendering guidelines for generating continuous or discontinuous sensations of tactile shapes. Finally, we hope to contribute toward a measure of the perceptually achievable resolution of UMH interfaces. We performed a user study to identify how far apart two focal points need to be to elicit a perceptual experience of two distinct stimuli separated by a gap. Mean gap detection thresholds were found at 32.3-mm spacing between focal points, but a high within- and between-subject variability was observed. Pairs spaced below 15 mm were consistently (>95%) perceived as a single stimulus, while pairs spaced 45 mm apart were consistently (84%) perceived as two separate stimuli. To investigate the observed variability, we resort to acoustic simulations of the resulting pressure fields. These show a non-linear evolution of actual peak pressure spacing as a function of nominal focal point spacing. Beyond an initial threshold in spacing (between 15 and 18 mm), which we believe to be related to the perceived size of a focal point, the probability of detecting a gap between focal points appears to linearly increase with spacing. Our work highlights physical interactions and perceptual effects to consider when designing or investigating the perception of UMH shapes.
C1 [Howard, Thomas; Marchal, Maud] Univ Rennes, CNRS, INRIA, IRISA,INSA, Rennes, France.
   [Driller, Karina; Hartcher-O'Brien, Jessica] Delft Univ Technol, Fac Ind Design Engn, Landbergstr 15, F-2628 CE Paris, France.
   [Driller, Karina] Sorbonne Univ, ISIR, 4 Pl Jussieu 65, F-75005 Paris, France.
   [Frier, William] Ultraleap, West Wing, Bristol BS2 0EL, Avon, England.
   [Pacchierotti, Claudio] Univ Rennes, IRISA, INRIA, CNRS, Rennes, France.
   [Marchal, Maud] IUF, Rennes, France.
C3 Universite de Rennes; Inria; Centre National de la Recherche
   Scientifique (CNRS); Sorbonne Universite; Centre National de la
   Recherche Scientifique (CNRS); Inria; Universite de Rennes
RP Howard, T (corresponding author), Univ Rennes, CNRS, INRIA, IRISA,INSA, Rennes, France.
EM thomas.howard@irisa.fr; K.K.Driller@tudelft.nl;
   william.frier@ultraleap.com; claudio.pacchierotti@irisa.fr;
   maud.marchal@irisa.fr; J.Hartcher-OBrien@tudelft.nl
RI Pacchierotti, Claudio/G-7304-2011
OI Pacchierotti, Claudio/0000-0002-8006-9168; Howard,
   Thomas/0000-0003-4904-375X; Marchal, Maud/0000-0002-6080-7178
FU European Union's Horizon 2020 programme [801413]; ANR project "MIMESIS"
FX This project has received funding from the European Union'sHorizon 2020
   programme under grant agreement no. 801413, project "H-Reality," as well
   as the ANR project "MIMESIS".
CR B&K, BRUEL KJAER SOUND VI
   Beattie D, 2020, ACM SYMPOSIUM ON APPLIED PERCEPTION (SAP 2020), DOI 10.1145/3385955.3407927
   BOLANOWSKI SJ, 1988, J ACOUST SOC AM, V84, P1680, DOI 10.1121/1.397184
   Carter T., 2013, P 26 ANN ACM S US IN, P505
   Freeman Euan, 2021, ICMI '21: Proceedings of the 2021 International Conference on Multimodal Interaction, P697, DOI 10.1145/3462244.3479950
   Frier W, 2022, IEEE ACCESS, V10, P15443, DOI 10.1109/ACCESS.2022.3147725
   Frier W, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300351
   Frier W, 2018, LECT NOTES COMPUT SC, V10893, P270, DOI 10.1007/978-3-319-93445-7_24
   Georgiou Orestis, 2022, Ultrasound Mid-air Haptics for Touchless Interfaces, DOI [10.1007/978-3-031-04043-6, DOI 10.1007/978-3-031-04043-6]
   Gescheider GA, 2005, SOMATOSENS MOT RES, V22, P255, DOI 10.1080/08990220500420236
   Hajas D, 2020, IEEE T HAPTICS, V13, P806, DOI 10.1109/TOH.2020.2966445
   Harrington K, 2018, AUTOMOTIVEUI'18: PROCEEDINGS OF THE 10TH ACM INTERNATIONAL CONFERENCE ON AUTOMOTIVE USER INTERFACES AND INTERACTIVE VEHICULAR APPLICATIONS, P11, DOI 10.1145/3239060.3239089
   Hasegawa K, 2018, IEEE T HAPTICS, V11, P367, DOI 10.1109/TOH.2018.2799220
   Hoshi T, 2010, IEEE T HAPTICS, V3, P155, DOI [10.1109/ToH.2010.4, 10.1109/TOH.2010.4]
   Howard T., 2022, Ultrasound Mid-Air Haptics for TouchlessInterfaces, P147
   Howard T, 2019, 2019 IEEE WORLD HAPTICS CONFERENCE (WHC), P503, DOI [10.1109/whc.2019.8816127, 10.1109/WHC.2019.8816127]
   Ito M, 2016, LECT NOTES COMPUT SC, V9774, P57, DOI 10.1007/978-3-319-42321-0_6
   Iwamoto T, 2008, LECT NOTES COMPUT SC, V5024, P504, DOI 10.1007/978-3-540-69057-3_64
   JONES LA, 1989, J HAND SURG-AM, V14, P221, DOI 10.1016/0363-5023(89)90010-5
   Kappus Brian, 2018, Journal of the Acoustical Society of America, V143, P1836
   Korres G, 2016, IEEE ACCESS, V4, P7758, DOI 10.1109/ACCESS.2016.2608835
   Kowalzik R, 1996, FOOT ANKLE INT, V17, P629, DOI 10.1177/107110079601701008
   Large DR, 2019, APPL ERGON, V81, DOI 10.1016/j.apergo.2019.102909
   LEVITT H, 1971, J ACOUST SOC AM, V49, P467, DOI 10.1121/1.1912375
   Mulot L, 2021, ACM SYMPOSIUM ON APPLIED PERCEPTION (SAP 2021), DOI 10.1145/3474451.3476232
   Obrist M., 2013, P SIGCHI C HUMAN FAC, P1659, DOI [10.1145/2470654.2466220, DOI 10.1145/2470654.2466220]
   OLDFIELD RC, 1971, NEUROPSYCHOLOGIA, V9, P97, DOI 10.1016/0028-3932(71)90067-4
   Perez CA, 1998, P ANN INT IEEE EMBS, V20, P2542, DOI 10.1109/IEMBS.1998.744968
   Perez CA, 2000, MED BIOL ENG COMPUT, V38, P74, DOI 10.1007/BF02344692
   Price A, 2018, IEEE INT ULTRA SYM
   Rakkolainen I, 2021, IEEE T HAPTICS, V14, P2, DOI 10.1109/TOH.2020.3018754
   Rutten I, 2019, CHI EA '19 EXTENDED ABSTRACTS: EXTENDED ABSTRACTS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290607.3313004
   Sand Antti, 2020, Haptics: Science, Technology, Applications. 12th International Conference, EuroHaptics 2020. Proceedings. Lecture Notes in Computer Science (LNCS 12272), P253, DOI 10.1007/978-3-030-58147-3_28
   Shakeri G, 2019, CHI EA '19 EXTENDED ABSTRACTS: EXTENDED ABSTRACTS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290607.3313264
   Takahashi R, 2018, LECT NOTES COMPUT SC, V10894, P276, DOI 10.1007/978-3-319-93399-3_25
   Tsumoto K, 2021, 2021 IEEE WORLD HAPTICS CONFERENCE (WHC), P602, DOI 10.1109/WHC49131.2021.9517249
   Ultraleap, ULTRALEAP HAPTICS
NR 37
TC 2
Z9 2
U1 1
U2 8
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD JAN
PY 2023
VL 20
IS 1
AR 5
DI 10.1145/3570904
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 7S6AV
UT WOS:000910834800005
OA Green Published
DA 2024-07-18
ER

PT J
AU Venkatakrishnan, R
   Venkatakrishnan, R
   Chung, CH
   Wang, YS
   Babu, S
AF Venkatakrishnan, Roshan
   Venkatakrishnan, Rohith
   Chung, Chih-Han
   Wang, Yu-Shuen
   Babu, Sabarish
TI Investigating a Combination of Input Modalities, Canvas Geometries, and
   Inking Triggers on On-Air Handwriting in Virtual Reality
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Virtual reality; writing; interfaces; interaction; text entry
ID HAPTIC FEEDBACK
AB Humans communicate by writing, often taking notes that assist thinking. With the growing popularity of collaborative Virtual Reality (VR) applications, it is imperative that we better understand aspects that affect writing in these virtual experiences. On-air writing in VR is a popular writing paradigm due to its simplicity in implementation without any explicit needs for specialized hardware. A host of factors can affect the efficacy of this writing paradigm and in this work, we delved into investigating the same. Along these lines, we investigated the effects of a combination of factors on users' on-air writing performance, aiming to understand the circumstances under which users can both effectively and efficiently write in VR. We were interested in studying the effects of the following factors: (1) input modality: brush vs. near-field raycast vs. pointing gesture, (2) inking triggermethod: haptic feedback vs. button based trigger, and (3) canvas geometry: plane vs. hemisphere. To evaluate the writing performance, we conducted an empirical evaluation with thirty participants, requiring them to write the words we indicated under different combinations of these factors. Dependent measures including the writing speed, accuracy rates, perceived workloads, and so on, were analyzed. Results revealed that the brush based input modality produced the best results in writing performance, that haptic feedback was not always effective over button based triggering, and that there are trade-offs associated with the different types of canvas geometries used. This work attempts at laying a foundation for future investigations that seek to understand and further improve the on-air writing experience in immersive virtual environments.
C1 [Venkatakrishnan, Roshan; Venkatakrishnan, Rohith; Babu, Sabarish] Clemson Univ, 100 McAdams Hall, Clemson, SC 29631 USA.
   [Chung, Chih-Han; Wang, Yu-Shuen] Natl Yang Ming Chiao Tung Univ, 343 Engn Bldg 3,1001 Daxue Rd, Hsinchu 300093, Taiwan.
C3 Clemson University; National Yang Ming Chiao Tung University
RP Venkatakrishnan, R (corresponding author), Clemson Univ, 100 McAdams Hall, Clemson, SC 29631 USA.
EM rvenkat@g.clemson.edu; rohithv@g.clemson.edu; chihhichihhi@gmail.com;
   yushuen@g2.nctu.edu.tw; sbabu@clemson.edu
RI Venkatakrishnan, Rohith/JCE-8736-2023; Venkatakrishnan,
   Roshan/JDC-3508-2023
OI Venkatakrishnan, Rohith/0000-0002-8484-3915; Venkatakrishnan,
   Roshan/0000-0002-6538-627X; Wang, Yu-Shuen/0000-0003-2550-2990; Babu,
   Sabarish/0000-0002-8348-0534
FU US National Science Foundation (CISE HCC) [2007435]; Direct For Computer
   & Info Scie & Enginr; Div Of Information & Intelligent Systems [2007435]
   Funding Source: National Science Foundation
FX This work was supported in part by the US National Science Foundation
   (CISE HCC) under Grant No. 2007435.
CR Argelaguet F, 2013, COMPUT GRAPH-UK, V37, P121, DOI 10.1016/j.cag.2012.12.003
   Azenkot S., 2012, P 14 INT C HUMAN COM, P251
   Boletsis Costas, 2019, International Journal of Virtual Reality (IJVR), V19, P3
   Bowman D. A., 2002, Proceedings of the Human Factors and Ergonomics Society 46th Annual Meeting, P2154
   Bowman D. A., 1997, Proceedings 1997 Symposium on Interactive 3D Graphics, P35, DOI 10.1145/253284.253301
   Bowman Doug, 2004, 3D user interfaces: Theory and practice
   Chen SB, 2019, CHI EA '19 EXTENDED ABSTRACTS: EXTENDED ABSTRACTS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290607.3312762
   Chen YT, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P172, DOI [10.1109/VR.2019.8798338, 10.1109/vr.2019.8798338]
   Danieau F, 2013, IEEE T HAPTICS, V6, P193, DOI [10.1109/TOH.2012.70, 10.1109/ToH.2012.70]
   Das K, 2010, LECT NOTES COMPUT SC, V6453, P719
   Dube T. J., 2019, HUMAN COMPUTER INTER, P419, DOI [DOI 10.1007/978-3-030-22643-5, 10.1007/978-3-030-22643-5\33/TABLES/5, https://doi.org/10.1007/978-3-030-22643-5_33, DOI 10.1007/978-3-030-22643-5_33]
   Elmgren Rasmus., 2017, Handwriting in VR as a Text Input Method
   Fisher DL, 2006, INJURY PREV, V12, P25, DOI 10.1136/ip.2006.012021
   Fu ZJ, 2019, IEEE T MOBILE COMPUT, V18, P473, DOI 10.1109/TMC.2018.2831709
   Hamza-Lup FG, 2019, Arxiv, DOI [arXiv:1903.03272, 10.48550/arXiv.1903.03272]
   Galambos P, 2012, ACTA POLYTECH HUNG, V9, P41
   Google Inc, 2018, TILT BRUSH GOOGL WEB
   Grossman T, 2007, SECOND ANNUAL IEEE INTERNATIONAL WORKSHOP ON HORIZONTAL INTERACTIVE HUMAN-COMPUTER SYSTEMS, PROCEEDINGS, P137, DOI 10.1109/TABLETOP.2007.18
   Grubert J, 2018, 25TH 2018 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P151, DOI 10.1109/VR.2018.8446250
   Gugenheimer J, 2016, UIST 2016: PROCEEDINGS OF THE 29TH ANNUAL SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P49, DOI 10.1145/2984511.2984576
   Hand C, 1997, COMPUT GRAPH FORUM, V16, P269, DOI 10.1111/1467-8659.00194
   HART S G, 1988, P139
   Heinrichs WL, 2008, WORLD J SURG, V32, P161, DOI 10.1007/s00268-007-9354-2
   Hsu CH, 2021, 2021 IEEE VIRTUAL REALITY AND 3D USER INTERFACES (VR), P635, DOI 10.1109/VR50410.2021.00089
   Hsu TW, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR 2020), P363, DOI [10.1109/VR46266.2020.1581231362069, 10.1109/VR46266.2020.00-48]
   Jankowski Jacek., 2013, Eurographics 2013-STAR
   Keysers D, 2017, IEEE T PATTERN ANAL, V39, P1180, DOI 10.1109/TPAMI.2016.2572693
   Kiefer M, 2015, ADV COGN PSYCHOL, V11, P136, DOI 10.5709/acp-0178-7
   Kunert Andre, 2009, ACM S VIRTUAL REALIT, P183
   Lécuyer A, 2009, PRESENCE-TELEOP VIRT, V18, P39, DOI 10.1162/pres.18.1.39
   Lee KC, 2008, COMPUT HUM BEHAV, V24, P88, DOI 10.1016/j.chb.2007.01.018
   Lin Jia-Wei., 2017, ACM SIGGRAPH 2017 Posters on - SIGGRAPH'17, P1, DOI [DOI 10.1145/3102163.3102175, DOI 10.1145/3102163]
   Lubos P, 2014, 2014 IEEE SYMPOSIUM ON 3D USER INTERFACES (3DUI), P11, DOI 10.1109/3DUI.2014.6798834
   Mangen A, 2015, J WRIT RES, V7, P227, DOI 10.17239/jowr-2015.07.02.01
   McGill M, 2015, CHI 2015: PROCEEDINGS OF THE 33RD ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P2143, DOI 10.1145/2702123.2702382
   Pham DM, 2019, 25TH ACM SYMPOSIUM ON VIRTUAL REALITY SOFTWARE AND TECHNOLOGY (VRST 2019), DOI 10.1145/3359996.3364264
   Poupyrev I, 1998, P IEEE VIRT REAL ANN, P126, DOI 10.1109/VRAIS.1998.658467
   Schwind V, 2017, PROCEEDINGS OF THE 2017 ACM SIGCHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'17), P1577, DOI 10.1145/3025453.3025602
   Smoker Timothy J., 2009, P HUM FACT ERG SOC, V53, P1744
   Speicher M, 2018, PROCEEDINGS OF THE 2018 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2018), DOI 10.1145/3173574.3174221
   Sridhar S, 2015, CHI 2015: PROCEEDINGS OF THE 33RD ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P3643, DOI 10.1145/2702123.2702136
   Tung Ying-Chao, 2018, COMPARING SPATIAL IN
   Våpenstad C, 2013, SURG ENDOSC, V27, P2391, DOI 10.1007/s00464-012-2745-y
   Venkatakrishnan R, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR 2020), P682, DOI [10.1109/VR46266.2020.00-13, 10.1109/VR46266.2020.1581195115265]
   Venkatakrishnan R, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P1201, DOI [10.1109/VR.2019.8797728, 10.1109/vr.2019.8797728]
   Wald J, 2000, J BEHAV THER EXP PSY, V31, P249, DOI 10.1016/S0005-7916(01)00009-X
   Walker J, 2017, PROCEEDINGS OF THE 2017 ACM SIGCHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'17), P5457, DOI 10.1145/3025453.3025783
   Wang CH, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P331, DOI [10.1109/vr.2019.8798255, 10.1109/VR.2019.8798255]
   Wu YF, 2013, AASRI PROC, V5, P200, DOI 10.1016/j.aasri.2013.10.079
   Yu C, 2017, PROCEEDINGS OF THE 2017 ACM SIGCHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'17), P4479, DOI 10.1145/3025453.3025964
NR 50
TC 5
Z9 7
U1 0
U2 5
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD OCT
PY 2022
VL 19
IS 4
SI SI
AR 15
DI 10.1145/3560817
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 6J6QI
UT WOS:000886946700002
DA 2024-07-18
ER

PT J
AU AEarsson, EA
   Asgeirsdóttir, T
   Pind, F
   Kristjánsson, A
   Unnthorsson, R
AF Aevarsson, Elvar Atli
   Asgeirsdottir, Thorhildur
   Pind, Finnur
   Kristjansson, Arni
   Unnthorsson, Runar
TI Vibrotactile Threshold Measurements at the Wrist Using Parallel
   Vibration Actuators
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Cochlear implants; music enjoyment; vibrotactile detection thresholds;
   parallel vibration; psychophysical measurements
ID SPATIAL ACUITY; CHANNELS
AB This article presents an investigation into the perceptual vibrotactile thresholds for a range of frequencies on both the inside and outside areas of the wrist when exciting the skin with parallel vibrations, realized using the L5 actuator made by Lofelt GmbH. The vibrotactile threshold of 30 participants was measured using a modified audiometry test for the frequency range of 25-1,000 Hz. The average threshold across the respective frequencies was then ultimately determined from acceleration minima. The results show that maximum sensitivity lies in the range of 100-275 Hz (peaking at 200 Hz) for the inside and 75-250 Hz (peaking at 125 Hz) for the outside of the wrist and that thresholds are overall higher for the hairy skin on the outside of the wrist than for the glabrous skin on the inside. The results also show that the vibrotactile thresholds varied highly between individuals. Hence, personalized threshold measurements at the actuator locations will be required to fine-tune a device for the user. This study is a part of an ongoing research and development project where the aim is to develop a tactile display device and a music encoding scheme with the purpose of augmenting the musical enjoyment of cochlear implant recipients. These results, along with results from planned follow-up experiments, will be used to determine the appropriate frequency range and to cast light on the dynamic range on offer for the tactile device.
C1 [Aevarsson, Elvar Atli] Univ Iceland, Skolabraut 3, IS-220 Hafnarfjordur, Iceland.
   [Asgeirsdottir, Thorhildur] KTH Royal Inst Technol, Lindstedtsvugen 3, S-11428 Stockholm, Sweden.
   [Pind, Finnur; Unnthorsson, Runar] Univ Iceland, Dunhaga 5, IS-107 Reykjavik, Iceland.
   [Kristjansson, Arni] Univ Iceland, Sturlugotu 3, IS-102 Reykjavik, Iceland.
C3 University of Iceland; Royal Institute of Technology; University of
   Iceland; University of Iceland
RP AEarsson, EA (corresponding author), Univ Iceland, Skolabraut 3, IS-220 Hafnarfjordur, Iceland.
EM elvaratli@hi.is; thorhildurkristin@gmail.com; finnurpind@hi.is;
   ak@hi.is; runson@hi.is
RI Unnthorsson, Runar/L-1884-2013
OI Unnthorsson, Runar/0000-0002-1960-0263; Asgeirsdottir,
   Thorhildur/0000-0001-6640-2254; Kristjansson, Arni/0000-0003-4168-4886
FU Nordic Sound and Music Computing [86892]; Icelandic Student Innovation
   Fund [206562-0091]; RANNIS Technology Development Fund [176713, 1910271]
FX The work described in this article was funded by Nordic Sound and Music
   Computing (project no. 86892), Icelandic Student Innovation Fund
   (project no. 206562-0091), and RANNIS Technology Development Fund
   (project nos. 176713 and 1910271).
CR [Anonymous], 2020, L5 ACTUATOR
   BOLANOWSKI SJ, 1994, SOMATOSENS MOT RES, V11, P279, DOI 10.3109/08990229409051395
   BOLANOWSKI SJ, 1988, J ACOUST SOC AM, V84, P1680, DOI 10.1121/1.397184
   Brisben AJ, 1999, J NEUROPHYSIOL, V81, P1548, DOI 10.1152/jn.1999.81.4.1548
   Caldwell MT, 2016, OTOL NEUROTOL, V37, P229, DOI 10.1097/MAO.0000000000000960
   CRAIG JC, 1968, PERCEPT PSYCHOPHYS, V4, P351, DOI 10.3758/BF03209532
   DONAHUE AM, 1985, AUDIOLOGY, V24, P362
   Goble AK, 1996, J ACOUST SOC AM, V99, P2256, DOI 10.1121/1.415413
   Grassi M, 2009, BEHAV RES METHODS, V41, P20, DOI 10.3758/BRM.41.1.20
   GREEN DM, 1993, J ACOUST SOC AM, V93, P2096, DOI 10.1121/1.406696
   Hoffmann R, 2019, J NEUROPHYSIOL, V122, P1810, DOI 10.1152/jn.00125.2019
   Hoffmann Rebekka, 2019, J NEUROPHYSIOL
   Jóhannesson OI, 2017, EXP BRAIN RES, V235, P3505, DOI 10.1007/s00221-017-5073-6
   Jóhannesson OI, 2016, BRAIN SCI, V6, DOI 10.3390/brainsci6030020
   Kristjánsson A, 2016, RESTOR NEUROL NEUROS, V34, P769, DOI 10.3233/RNN-160647
   Landsberger DM, 2012, HEARING RES, V284, P16, DOI 10.1016/j.heares.2011.12.009
   Lee J, 2015, CHI 2015: PROCEEDINGS OF THE 33RD ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P1229, DOI 10.1145/2702123.2702530
   Lofelt GmbH, 2019, LOF L5 ACT L5 DAT
   Looi V, 2004, INT CONGR SER, V1273, P197, DOI 10.1016/j.ics.2004.08.038
   Loomis J.M., 2012, Assistive Technology for Blindness and Low Vision
   Mahns DA, 2006, J NEUROPHYSIOL, V95, P1442, DOI 10.1152/jn.00483.2005
   Makarov I., 2022, HAPTIC INTENSI UNPUB
   Mancini F, 2014, ANN NEUROL, V75, P917, DOI 10.1002/ana.24179
   Marozeau J, 2019, SPRINGER HANDB AUDIT, V69, P273, DOI 10.1007/978-3-030-14832-4_10
   Piccinin M. A., 2022, StatPearls
   Quindlen JC, 2015, PLOS COMPUT BIOL, V11, DOI 10.1371/journal.pcbi.1004370
   Renier L., 2013, OXFORD HDB SYNAESTHE
   Sofia KO, 2013, IEEE T HAPTICS, V6, P320, DOI 10.1109/TOH.2013.1
   VERRILLO RT, 1963, J ACOUST SOC AM, V35, P1962, DOI 10.1121/1.1918868
   VERRILLO RT, 1971, PERCEPT PSYCHOPHYS, V9, P329, DOI 10.3758/BF03208688
NR 30
TC 7
Z9 7
U1 0
U2 0
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD JUL
PY 2022
VL 19
IS 3
AR 10
DI 10.1145/3529259
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 4L3FO
UT WOS:000852517200002
OA Bronze
DA 2024-07-18
ER

PT J
AU Kim, HJ
   Neff, M
   Lee, SH
AF Kim, Hye Ji
   Neff, Michael
   Lee, Sung-Hee
TI The Perceptual Consistency and Association of the LMA Effort Elements
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Laban Movement Analysis (LMA); LMA Effort; motion style perception;
   style consistency; style association
ID FEATURES
AB Laban Movement Analysis (LMA) and its Effort element provide a conceptual framework through which we can observe, describe, and interpret the intention of movement. Effort attributes provide a link between how people move and how their movement communicates to others. It is crucial to investigate the perceptual characteristics of Effort to validate whether it can serve as an effective framework to support a wide range of applications in animation and robotics that require a system for creating or perceiving expressive variation in motion. To this end, we first constructed an Effort motion database of short video clips of five different motions: walk, sit down, pass, put, wave performed in eight ways corresponding to the extremes of the Effort elements. We then performed a perceptual evaluation to examine the perceptual consistency and perceived associations among Effort elements: Space (Indirect/Direct), Time (Sustained/Sudden), Weight (Light/Strong), and Flow (Free/Bound) that appeared in the motion stimuli. The results of the perceptual consistency evaluation indicate that although the observers do not perceive the LMA Effort element 100% as intended, true response rates of seven Effort elements are higher than false response rates except for light Effort. The perceptual consistency results showed varying tendencies by motion. The perceptual association between LMA Effort elements showed that a single LMA Effort element tends to co-occur with the elements of other factors, showing significant correlation with one or two factors (e.g., indirect and free, light and free).
C1 [Kim, Hye Ji; Lee, Sung-Hee] Korea Adv Inst Sci & Technol, Seoul, South Korea.
   [Neff, Michael] Univ Calif Davis, 2063 Kemper Hall,One Shields Ave, Davis, CA 95616 USA.
   [Kim, Hye Ji; Lee, Sung-Hee] Seoul Natl Univ, 650C,Bldg 220,Gwanak Ro 1, Seoul 08826, South Korea.
C3 Korea Advanced Institute of Science & Technology (KAIST); University of
   California System; University of California Davis; Seoul National
   University (SNU)
RP Kim, HJ (corresponding author), Korea Adv Inst Sci & Technol, Seoul, South Korea.; Kim, HJ (corresponding author), Seoul Natl Univ, 650C,Bldg 220,Gwanak Ro 1, Seoul 08826, South Korea.
EM kimhyejee923@kaist.ac.kr; mpneff@ucdavis.edu; sunghee.lee@kaist.ac.kr
OI Neff, Michael/0000-0003-0226-2808; Lee, Sung-Hee/0000-0001-6604-4709
FU National Research Foundation, Korea [NRF-2020R1A2C2011541]
FX This work was supported by National Research Foundation, Korea
   (NRF-2020R1A2C2011541).
CR Ajili I, 2019, VISUAL COMPUT, V35, P1411, DOI 10.1007/s00371-018-01619-w
   [Anonymous], 1984, THESIS HAHNEMANN U P
   [Anonymous], 2013, Proceedings of the ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games, I3D'13
   [Anonymous], 2003, P 2 INT C MOBILE UBI
   [Anonymous], 2001, Doctoral thesis,
   Aristidou A, 2017, ACM SIGGRAPH / EUROGRAPHICS SYMPOSIUM ON COMPUTER ANIMATION (SCA 2017), DOI 10.1145/3099564.3099566
   Aristidou A, 2015, COMPUT GRAPH FORUM, V34, P262, DOI 10.1111/cgf.12598
   Bacula A, 2018, PROCEEDINGS OF THE 5TH INTERNATIONAL CONFERENCE ON MOVEMENT AND COMPUTING (MOCO'18), DOI 10.1145/3212721.3212836
   Bernardet U, 2019, PLOS ONE, V14, DOI 10.1371/journal.pone.0218179
   Bishko L., 2014, NONVERBAL COMMUNICAT, P177
   Bouchard D, 2007, LECT NOTES ARTIF INT, V4722, P37
   Chao SP, 2006, COMPUT ANIMAT VIRT W, V17, P167, DOI 10.1002/cav.120
   Chi D, 2000, COMP GRAPH, P173, DOI 10.1145/344779.352172
   Cui H, 2019, ROBOTICS, V8, DOI 10.3390/robotics8020024
   Davis M., 1987, MOVEMENT STUDIES, V2, P7
   Durupinar F, 2017, ACM T GRAPHIC, V36, DOI 10.1145/2983620
   Fdili AlaouiS., 2015, Proceedings of the 2Nd International Workshop on Movement and Computing, MOCO '15, P84, DOI DOI 10.1145/2790994.2791000
   Foroud A, 2006, J NEUROSCI METH, V158, P137, DOI 10.1016/j.jneumeth.2006.05.007
   Francoise J., 2014, Proceedings of the 2014 Conference on Designing Interactive Systems, DIS'14, page, P1079, DOI DOI 10.1145/2598510.2598582
   Hartmann B, 2006, LECT NOTES ARTIF INT, V3881, P188
   Konie Robin., 2011, Movement Has Meaning: LMA Workshop Sheet
   Maranan DS, 2014, 32ND ANNUAL ACM CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2014), P991, DOI 10.1145/2556288.2557251
   Mentis Helena M., 2013, P SIGCHI C HUMAN FAC, P3375, DOI [DOI 10.1145/2470654.2466462, 10.1145/2470654.2466462]
   Okajima S, 2012, STUD COMPUT INTELL, V376, P117
   Santos L, 2009, 2009 IEEE-RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, P4984, DOI 10.1109/IROS.2009.5354564
   Sarahlawrence Digitalcommons, 2018, J HEAD TRAUMA REHAB, V42
   Subyen Pattarawut, 2013, CREATIV COGN, DOI [10.13140/RG.2.1.3238.9282, DOI 10.13140/RG.2.1.3238.9282]
   Torresani L., 2006, Neural Information Processing Systems, NIPS, P1393
   Wakayama Y, 2010, LECT NOTES ARTIF INT, V6279, P251, DOI 10.1007/978-3-642-15384-6_27
   Yu T, 2005, COMPUT ANIMAT VIRT W, V16, P273, DOI 10.1002/cav.89
NR 30
TC 1
Z9 1
U1 0
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD JAN
PY 2022
VL 19
IS 1
AR 1
DI 10.1145/3473041
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 9R2WY
UT WOS:000945516600001
DA 2024-07-18
ER

PT J
AU Beshai, P
   Caceffo, R
   Booth, KS
AF Beshai, Peter
   Caceffo, Ricardo
   Booth, Kellogg S.
TI Providing Semi-private Feedback on a Shared Public Screen by Controlling
   Presentation Onset
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Abrupt no-onset; abrupt onset; gradual no-onset; shared display; visual
   feedback
ID ABRUPT VISUAL ONSETS; SELECTIVE ATTENTION; BENEFITS; SEARCH
AB We describe a novel technique to provide semi-private feedback on a shared public screen. The technique uses a no-onset presentation that takes advantage of perceptual limitations in human vision to avoid alerting other users to feedback directed at one individual user by suppressing the sudden onset of the feedback. Three experiments evaluated the effectiveness of the technique and appropriate timing parameters and alternatives for presentation onset. Our experiments indicated that an 80 ms no-onset presentation allows participants to interpret information directed to them with over 90% accuracy, but their ability to interpret simultaneously presented information intended for others will be close to random chance. The technique initially camouflages the information being presented by overlaying additional visual elements and then removes those elements to reveal only the elements encoding the information being presented. We discuss applications for the technique, including classroom clicker usage, which was our original motivation for the study.
C1 [Beshai, Peter; Caceffo, Ricardo; Booth, Kellogg S.] Univ British Columbia, 201-2366 Main Mall, Vancouver, BC V6T 1Z4, Canada.
   [Caceffo, Ricardo] Univ Estadual Campinas, UNICAMP, Av Albert Einstein 1251,Cidade Univ, BR-13083852 Campinas, SP, Brazil.
C3 University of British Columbia; Universidade Estadual de Campinas
RP Beshai, P (corresponding author), Univ British Columbia, 201-2366 Main Mall, Vancouver, BC V6T 1Z4, Canada.
EM peter.beshai@gmail.com; caceffo@ic.unicamp.br; ksbooth@cs.ubc.ca
OI Booth, Kellogg/0000-0002-3481-7962
FU Fundacao de Amparo a Pesquisa do Estado de Sao Paulo (FAPESP)
   [2014/07502-4, 2015/08668-6]; Natural Sciences and Engineering Research
   Council of Canada (NSERC) [RGPIN-116412-11, RGPIN-2016-0432];
   Universidade Estadual de Campinas (Unicamp); University of British
   Columbia; Canada Foundation for Innovation
FX Funding for work reported here was provided by Fundacao de Amparo a
   Pesquisa do Estado de Sao Paulo (FAPESP) under grants #2014/07502-4 and
   #2015/08668-6, the Natural Sciences and Engineering Research Council of
   Canada (NSERC) under grants RGPIN-116412-11 and RGPIN-2016-0432,
   Universidade Estadual de Campinas (Unicamp), and the University of
   British Columbia. Facilities were provided by the Institute for
   Computing, Information and Cognitive Systems under funding from the
   Canada Foundation for Innovation.
CR ATKINSON RC, 1969, PERCEPT PSYCHOPHYS, V6, P321, DOI 10.3758/BF03212784
   Azad Alec., 2012, Proceedings of the Designing Interactive Systems Conference, DIS'12, P468, DOI DOI 10.1145/2317956.2318025
   Bakeman R, 2005, BEHAV RES METHODS, V37, P379, DOI 10.3758/BF03192707
   Benaloh J., 2011, P EL VOT TECHN WORKS, P1
   Beshai Peter, 2014, THESIS
   BROADBENT DE, 1982, ACTA PSYCHOL, V50, P253, DOI 10.1016/0001-6918(82)90043-9
   Brown EA, 2014, J HOSP LEIS SPORT TO, V15, P80, DOI 10.1016/j.jhlste.2014.06.002
   Caceffo Ricardo, 2018, IC1808 I COMP U CAMP
   Cao H, 2008, SOC SCI COMPUT REV, V26, P87, DOI 10.1177/0894439307307696
   Cohen J., 1988, STAT POWER ANAL BEHA
   Dill David, 2016, ELECTION SECURITY IS
   ERIKSEN CW, 1972, PERCEPT PSYCHOPHYS, V12, P201, DOI 10.3758/BF03212870
   Gauci SA, 2009, ADV PHYSIOL EDUC, V33, P60, DOI 10.1152/advan.00109.2007
   Hall R. H., 2005, P AM C INF SYST, P621
   HOFFMAN JE, 1981, PERCEPT PSYCHOPHYS, V30, P283, DOI 10.3758/BF03214284
   HOLMGREN JE, 1974, PERCEPT PSYCHOPHYS, V15, P544, DOI 10.3758/BF03199300
   iClicker, 2007, IC CL WIND PC APPL U
   Lanir J., 2008, MM 08, P519, DOI DOI 10.1145/1459359.1459428
   Lanir J, 2013, HUM-COMPUT INTERACT, V28, P335, DOI 10.1080/07370024.2012.697037
   Lanir J, 2010, COMPUT EDUC, V55, P892, DOI 10.1016/j.compedu.2010.03.020
   Lanir J, 2008, CHI 2008: 26TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS VOLS 1 AND 2, CONFERENCE PROCEEDINGS, P695
   Lanir Joel, 2008, P 3 ACM INT WORKSH H, P61, DOI [10.1145/1462027.1462037, DOI 10.1145/1462027.1462037]
   Mahyar N, 2016, PROCEEDINGS OF THE 2016 ACM INTERNATIONAL CONFERENCE ON INTERACTIVE SURFACES AND SPACES, (ISS 2016), P109, DOI 10.1145/2992154.2992163
   Medina MS, 2008, AM J PHARM EDUC, V72, DOI 10.5688/aj720238
   Morandi Patrick J., 2015, ICLICKER TESTING
   POSNER MI, 1980, J EXP PSYCHOL GEN, V109, P160, DOI 10.1037/0096-3445.109.2.160
   REEF Education, 2005, REEF POLL STUD QUICK
   Shi J., 2013, THESIS
   Shoemaker G. B. D., 2001, CHI 2001 Conference Proceedings. Conference on Human Factors in Computing Systems, P522, DOI 10.1145/365024.365349
   Siddhpuria S, 2018, PROCEEDINGS OF THE 2018 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2018), DOI 10.1145/3173574.3173747
   Starner Thad, 2013, P 2013 INT S WEAR CO, P125, DOI [10.1145/2493988.2501097, DOI 10.1145/2493988.2501097]
   Stowell JR, 2007, TEACH PSYCHOL, V34, P253, DOI 10.1080/00986280701700391
   Stowell JR, 2015, COMPUT EDUC, V82, P329, DOI 10.1016/j.compedu.2014.12.008
   Thimbleby Harold., 2013, P SIGCHI C HUMAN FAC, p1431. isbn, DOI DOI 10.1145/2470654.2466190
   TODD JT, 1979, J EXP PSYCHOL HUMAN, V5, P625, DOI 10.1037/0096-1523.5.4.625
   TREISMAN A, 1988, PSYCHOL REV, V95, P15, DOI 10.1037/0033-295X.95.1.15
   TREISMAN AM, 1980, COGNITIVE PSYCHOL, V12, P97, DOI 10.1016/0010-0285(80)90005-5
   Vogel Daniel., 2004, Proc. of the 17th Annual ACM Sypm. on User Interface Software and Technology (UIST), P137, DOI [DOI 10.1145/1029632.1029656, https://doi.org/10.1145/1029632.1029656]
   von Ahn L, 2003, LECT NOTES COMPUT SC, V2656, P294
   YANTIS S, 1990, J EXP PSYCHOL HUMAN, V16, P121, DOI 10.1037/0096-1523.16.1.121
   YANTIS S, 1984, J EXP PSYCHOL HUMAN, V10, P601, DOI 10.1037/0096-1523.10.5.601
NR 41
TC 0
Z9 0
U1 0
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD NOV
PY 2020
VL 17
IS 3
AR 11
DI 10.1145/3419983
PG 32
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA PA3OQ
UT WOS:000595548000003
DA 2024-07-18
ER

PT J
AU Ito, K
   Okamoto, S
   Yamada, Y
   Kajimoto, H
AF Ito, Ken
   Okamoto, Shogo
   Yamada, Yoji
   Kajimoto, Hiroyuki
TI Tactile Texture Display with Vibrotactile and Electrostatic Friction
   Stimuli Mixed at Appropriate Ratio Presents Better Roughness Textures
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Tactile texture display; vibrotactile display; electrostatic friction
   display; surface roughness
ID PERCEPTUAL SPACE; FEEDBACK; SURFACE; FORCE; MODEL; SHAPE; FINGER; RULE
AB Vibrotactile and friction texture displays are good options for artificially presenting the roughness and frictional properties of textures, respectively. These two types of displays are compatible with touch panels and exhibit complementary characteristics. We combine vibrotactile and electrostatic friction texture displays to improve the quality of virtual textures, considering that actual textured surfaces are composed of both properties. We investigate their composition ratios when displaying roughness textures. Grating roughness scales with one of the six surface wavelengths are generated under 11 display conditions, and in 9 of which, vibrotactile and friction stimuli are combined with different composition ratios. A forced-choice experiment regarding subjective realism indicates that a vibrotactile stimulus with a slight variable-friction stimulus is effective for presenting quality textures for surface wavelengths greater than or equal to 1.0mm.
C1 [Ito, Ken; Okamoto, Shogo; Yamada, Yoji] Nagoya Univ, Chikusa Ku, Furo Cho, Nagoya, Aichi 4648603, Japan.
   [Kajimoto, Hiroyuki] Univ Electrocommun, 1-5-1 Nunogaoka, Chofu, Tokyo, Japan.
C3 Nagoya University; University of Electro-Communications - Japan
RP Okamoto, S (corresponding author), Nagoya Univ, Chikusa Ku, Furo Cho, Nagoya, Aichi 4648603, Japan.
EM itou.ken@a.mbox.nagoya-u.ac.jp; okamoto-shogo@nagoya-u.jp;
   yamada-yoji@mech.nagoya-u.ac.jp; kajimoto@hc.uec.ac.jp
FU MEXT Kakenhi [15H05923, 17H04697]; Grants-in-Aid for Scientific Research
   [17H04697] Funding Source: KAKEN
FX The work was supported by MEXT Kakenhi under Grants No. 15H05923 and No.
   17H04697.
CR Aktar T, 2017, J TEXTURE STUD, V48, P181, DOI 10.1111/jtxs.12245
   [Anonymous], 2005, Individual choice behavior: A theoretical analysis
   Asano S, 2015, IEEE T HUM-MACH SYST, V45, P393, DOI 10.1109/THMS.2014.2376519
   Ballesteros S, 2005, WORLD HAPTICS CONFERENCE: FIRST JOINT EUROHAPTICS CONFERENCE AND SYMPOSIUM ON HAPTIC INTERFACES FOR VIRUTUAL ENVIRONMENT AND TELEOPERATOR SYSTEMS, PROCEEDINGS, P635
   Bau O., 2010, P 23 ANN ACM S US IN, P283, DOI DOI 10.1145/1866029.1866074
   Bensmaïa S, 2005, PERCEPT PSYCHOPHYS, V67, P842, DOI 10.3758/BF03193537
   Bensmaia SJ, 2009, SCHOLARPEDIA, V4, P7956, DOI [10.4249/scholarpedia.7956, DOI 10.4249/SCHOLARPEDIA.7956]
   Biggs J, 2002, 10TH SYMPOSIUM ON HAPTIC INTERFACES FOR VIRTUAL ENVIRONMENT AND TELEOPERATOR SYSTEMS, PROCEEDINGS, P121, DOI 10.1109/HAPTIC.2002.998949
   CALDWELL DG, 1993, IROS 93 : PROCEEDINGS OF THE 1993 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, VOL 1-3, P1487, DOI 10.1109/IROS.1993.583836
   CLARKE FR, 1957, J ACOUST SOC AM, V29, P715, DOI 10.1121/1.1909023
   Culbertson H, 2017, IEEE T HAPTICS, V10, P63, DOI 10.1109/TOH.2016.2598751
   Duvefelt K, 2016, TRIBOL INT, V96, P389, DOI 10.1016/j.triboint.2014.12.020
   Fagiani R, 2011, TRIBOL INT, V44, P1100, DOI 10.1016/j.triboint.2011.03.019
   Fujii Y, 2016, ADV ROBOTICS, V30, P1341, DOI 10.1080/01691864.2016.1208591
   Ghenna S, 2017, IEEE T HAPTICS, V10, P296, DOI 10.1109/TOH.2016.2607200
   Guest S, 2011, ATTEN PERCEPT PSYCHO, V73, P531, DOI 10.3758/s13414-010-0037-y
   Hachisu T, 2017, IEEE T HAPTICS, V10, P288, DOI 10.1109/TOH.2016.2628900
   Hasegawa Hikaru, 2018, P IEEE INT C SYST MA
   Hollins M, 2000, PERCEPT PSYCHOPHYS, V62, P695, DOI 10.3758/BF03206916
   Hollins M, 2000, PERCEPT PSYCHOPHYS, V62, P1534, DOI 10.3758/BF03212154
   Imaizumi Akihiro, 2017, ROBOMECH Journal, V4, DOI 10.1186/s40648-017-0080-8
   Ito K, 2017, IEEE SYS MAN CYBERN, P2343, DOI 10.1109/SMC.2017.8122972
   Ito K, 2018, LECT NOTES ELECTR EN, V432, P125, DOI 10.1007/978-981-10-4157-0_22
   Janko M, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-018-23150-7
   Kammermeier P, 2004, PRESENCE-VIRTUAL AUG, V13, P1, DOI 10.1162/105474604774048199
   Kim S.-C., 2013, P 26 ANN ACM S US IN, P531, DOI DOI 10.1145/2501988.2502020
   Klatzky RL, 2013, P IEEE, V101, P2081, DOI 10.1109/JPROC.2013.2248691
   Kurogi T, 2013, P IEEE VIRT REAL ANN, P137, DOI 10.1109/VR.2013.6549400
   Lévesque V, 2011, 29TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P2481
   Levesque Vincent, 2017, U.S. Patent US, Patent No. [9639158B2, 9639158]
   Mukaibo Y, 2005, IEEE INT CONF ROBOT, P2565
   Mullenbach J, 2017, IEEE T HAPTICS, V10, P358, DOI 10.1109/TOH.2016.2630057
   Nakamura T, 2016, IEEE T HAPTICS, V9, P311, DOI 10.1109/TOH.2016.2556660
   Okamoto S, 2014, ATTEN PERCEPT PSYCHO, V76, P877, DOI 10.3758/s13414-013-0588-9
   Okamoto S, 2013, IEEE T HAPTICS, V6, P81, DOI [10.1109/ToH.2012.32, 10.1109/TOH.2012.32]
   Okamura AM, 1998, IEEE INT CONF ROBOT, P674, DOI 10.1109/ROBOT.1998.677050
   Osgouei RH, 2017, IEEE T HAPTICS, V10, P533, DOI 10.1109/TOH.2017.2710314
   Platkiewicz J, 2014, LECT NOTES COMPUT SC, V8618, P521, DOI 10.1007/978-3-662-44193-0_65
   Pyo D, 2014, LECT NOTES COMPUT SC, V8618, P487, DOI 10.1007/978-3-662-44193-0_61
   Robles-De-La-Torre G, 2001, NATURE, V412, P445, DOI 10.1038/35086588
   Ryu S, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-018-22865-x
   Ryu S, 2018, LECT NOTES ELECTR EN, V432, P83, DOI 10.1007/978-981-10-4157-0_14
   Sato Shunsuke, 2017, ROBOMECH Journal, V4, DOI 10.1186/s40648-017-0087-1
   Shirado H, 2005, World Haptics Conference: First Joint Eurohaptics Conference and Symposium on Haptic Interfaces for Virutual Environment and Teleoperator Systems, Proceedings, P629
   Skedung L, 2011, TRIBOL INT, V44, P505, DOI 10.1016/j.triboint.2010.04.010
   Smith AM, 2002, EXP BRAIN RES, V144, P211, DOI 10.1007/s00221-002-1015-y
   Smith TA, 2017, 2017 IEEE WORLD HAPTICS CONFERENCE (WHC), P635, DOI 10.1109/WHC.2017.7989975
   Sutu A, 2013, J NEUROPHYSIOL, V109, P1403, DOI 10.1152/jn.00717.2012
   Tanaka Y, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0093363
   TAYLOR MM, 1975, PERCEPT PSYCHOPHYS, V17, P23, DOI 10.3758/BF03203993
   Tiest WMB, 2010, VISION RES, V50, P2775, DOI 10.1016/j.visres.2010.10.005
   TOWNSEND JT, 1982, J MATH PSYCHOL, V25, P119, DOI 10.1016/0022-2496(82)90009-8
   van Kuilenburg J, 2015, P I MECH ENG J-J ENG, V229, P243, DOI 10.1177/1350650113504908
   Vardar Y, 2017, 2017 IEEE WORLD HAPTICS CONFERENCE (WHC), P263, DOI 10.1109/WHC.2017.7989912
   Weber AI, 2013, P NATL ACAD SCI USA, V110, P17107, DOI 10.1073/pnas.1305509110
   Wiertlewski M, 2011, IEEE T ROBOT, V27, P461, DOI 10.1109/TRO.2011.2132830
   Winfield L, 2007, WORLD HAPTICS 2007: SECOND JOINT EUROHAPTICS CONFERENCE AND SYMPOSIUM ON HAPTIC INTERFACES FOR VIRTUAL ENVIRONMENT AND TELEOPERATOR SYSTEMS, PROCEEDINGS, P421
   Wu JZ, 2007, MED ENG PHYS, V29, P718, DOI 10.1016/j.medengphy.2006.07.005
   Yamauchi Takahiro, 2010, 2010 IEEE International Conference on Robotics and Automation (ICRA 2010), P1753, DOI 10.1109/ROBOT.2010.5509926
   Yoshioka T, 2007, SOMATOSENS MOT RES, V24, P53, DOI 10.1080/08990220701318163
NR 60
TC 12
Z9 12
U1 4
U2 22
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD SEP
PY 2019
VL 16
IS 4
AR 20
DI 10.1145/3340961
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA JE2HU
UT WOS:000490516700002
OA Bronze
DA 2024-07-18
ER

PT J
AU Spicker, M
   Götz-Hahn, F
   Lindemeier, T
   Saupe, D
   Deussen, O
AF Spicker, Marc
   Goetz-Hahn, Franz
   Lindemeier, Thomas
   Saupe, Dietmar
   Deussen, Oliver
TI Quantifying Visual Abstraction Quality for Computer-Generated
   Illustrations
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Visual abstraction; user study; perception; stippling;
   non-photorealistic rendering
ID RANKING VISUALIZATIONS; LAW
AB We investigate how the perceived abstraction quality of computer-generated illustrations is related to the number of primitives (points and small lines) used to create them. Since it is difficult to find objective functions that quantify the visual quality of such illustrations, we propose an approach to derive perceptual models from a user study. By gathering comparative data in a crowdsourcing user study and employing a paired comparison model, we can reconstruct absolute quality values. Based on an exemplary study for stippling, we show that it is possible to model the perceived quality of stippled representations based on the properties of an input image. The generalizability of our approach is demonstrated by comparing models for different stippling methods. By showing that our proposed approach also works for small lines, we demonstrate its applicability toward quantifying different representational drawing elements. Our results can be related to Weber-Fechner's law from psychophysics and indicate a logarithmic relationship between number of rendering primitives in an illustration and the perceived abstraction quality thereof.
C1 [Spicker, Marc; Goetz-Hahn, Franz; Lindemeier, Thomas; Saupe, Dietmar; Deussen, Oliver] Univ Konstanz, Dept Comp & Informat Sci, Univ Str 10, D-78457 Constance, Germany.
C3 University of Konstanz
RP Spicker, M (corresponding author), Univ Konstanz, Dept Comp & Informat Sci, Univ Str 10, D-78457 Constance, Germany.
EM marc.spicker@googlemail.com; hahn.franz@gmail.com;
   thomas.lindemeier@gmail.com; dietmar.saupe@uni-konstanz.de;
   oliver.deussen@uni-konstanz.de
RI Deussen, Oliver/HKF-2004-2023
OI Lindemeier, Thomas/0009-0003-7715-8439; Spicker,
   Marc/0000-0002-3848-6101
FU German Research Foundation (DFG) [SFB/Transregio 161]
FX The authors thank the German Research Foundation (DFG) for financial
   support within projects A04 and A05 of SFB/Transregio 161.
CR [Anonymous], P INT S COMP AESTH G
   [Anonymous], 1860, ELEMENTE PSYCHOPHYSI
   [Anonymous], 2002, P 2 INT S NONPH AN R, DOI DOI 10.1145/508535.508537
   [Anonymous], 2011, Technical Report
   [Anonymous], 2009, P 7 INT S NONPH AN R, DOI DOI 10.1145/1572614.1572622
   Balzer M, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531392
   Barla P, 2006, COMPUT GRAPH FORUM, V25, P663, DOI 10.1111/j.1467-8659.2006.00986.x
   Bjontegaard Gisle, 2001, P 13 M VID EXP GROUP
   Brett King D., 2004, M WERTHEIMER GESTALT
   Cole F, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531334
   COOMBS CH, 1967, J PERS SOC PSYCHOL, V6, P85, DOI 10.1037/h0024522
   de Goes F, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366190
   Deussen O, 2000, COMPUT GRAPH FORUM, V19, pC41, DOI 10.1111/1467-8659.00396
   Deussen O, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130819
   Deussen Oliver, 2009, P 5 INT S COMP AESTH, DOI [10.2312/COMPAESTH/COMPAESTH09/123-128, DOI 10.2312/COMPAESTH/COMPAESTH09/123-128]
   Deussen Oliver, 2013, HALFTONING STIPPLING, P45, DOI [10.1007/978-1-4471-4519-6_3, DOI 10.1007/978-1-4471-4519-6_3]
   Durand F, 2001, SPRING EUROGRAP, P71
   Engelke U, 2015, SIGNAL PROCESS-IMAGE, V39, P386, DOI 10.1016/j.image.2015.03.004
   FLOYD RW, 1976, P SID, V17, P75
   Gatzidis C, 2008, IEEE INT CONF INF VI, P475, DOI 10.1109/IV.2008.75
   Grabli S, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1731047.1731056
   Harrison L, 2014, IEEE T VIS COMPUT GR, V20, P1943, DOI 10.1109/TVCG.2014.2346979
   Hiller S, 2003, COMPUT GRAPH FORUM, V22, P515, DOI 10.1111/1467-8659.00699
   Hodges ElaineR S., 2003, GUILD HDB SCI ILLUST, VSecond
   Hosu V., 2016, PQS 2016 5 ISCADEGA, P117
   Isenberg T., 2006, Non-photorealistic rendering in context: an observational study, P115
   Isenberg Tobias, 2013, EVALUATING VALIDATIN, P311, DOI [10.1007/978-1-4471-4519-6_15, DOI 10.1007/978-1-4471-4519-6_15]
   Jiang M, 2015, PROC CVPR IEEE, P1072, DOI 10.1109/CVPR.2015.7298710
   Jodoin Pierre-Marc., 2002, PROC NPAR, P29
   Johnson MH, 2005, NAT REV NEUROSCI, V6, P766, DOI 10.1038/nrn1766
   Kanwisher N, 1997, J NEUROSCI, V17, P4302
   Kay M, 2016, IEEE T VIS COMPUT GR, V22, P469, DOI 10.1109/TVCG.2015.2467671
   Kim D, 2008, COMPUT GRAPH FORUM, V27, P1209, DOI 10.1111/j.1467-8659.2008.01259.x
   Kopf J, 2006, ACM T GRAPHIC, V25, P509, DOI 10.1145/1141911.1141916
   Li H, 2011, PROCEEDINGS OF THE 4TH CONFERENCE ON SYSTEMS SCIENCE, MANAGEMENT SCIENCE AND SYSTEMS DYNAMICS, SSMSSD10, VOL 2, P127
   Likert R., 1932, TECHNIQUE MEASUREMEN, DOI 1933-01885-001
   LLOYD SP, 1982, IEEE T INFORM THEORY, V28, P129, DOI 10.1109/TIT.1982.1056489
   Maciejewski R, 2008, IEEE COMPUT GRAPH, V28, P62, DOI 10.1109/MCG.2008.35
   Maciejewski Ross, 2007, P EUR C COMP AESTH G, P53, DOI [10.2312/COMPAESTH/COMPAESTH07/053-056, DOI 10.2312/COMPAESTH/COMPAESTH07/053-056]
   Martin D, 2015, P WORKSH NONPH AN RE, P103, DOI DOI 10.2312/EXP.20151183
   Martín D, 2017, COMPUT GRAPH-UK, V67, P24, DOI 10.1016/j.cag.2017.05.001
   Martín D, 2011, COMPUT GRAPH-UK, V35, P160, DOI 10.1016/j.cag.2010.11.006
   Mould David., 2007, P 3 EUROGRAPHICS C C, P45
   Pastor OM, 2003, IEEE COMPUT GRAPH, V23, P62, DOI 10.1109/MCG.2003.1210866
   Reichl P, 2010, IEEE ICC
   Singh Mayank, 2010, P COMP AESTH 10, DOI [10.2312/COMPAESTH/COMPAESTH10/025-032, DOI 10.2312/COMPAESTH/COMPAESTH10/025-032]
   Spicker M., 2017, P S NONPHOTOREALISTI, DOI DOI 10.1145/3092919.3092923
   Thurstone LL, 1927, PSYCHOL REV, V34, P273, DOI 10.1037/h0070288
   Tsao DY, 2006, SCIENCE, V311, P670, DOI 10.1126/science.1119983
   Winkenbach G., 1994, Computer Graphics Proceedings. Annual Conference Series 1994. SIGGRAPH 94 Conference Proceedings, P91, DOI 10.1145/192161.192184
   Woods RL, 2010, PROC SPIE, V7527, DOI 10.1117/12.843858
NR 51
TC 1
Z9 1
U1 0
U2 5
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD FEB
PY 2019
VL 16
IS 1
AR 5
DI 10.1145/3301414
PG 20
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HN7TC
UT WOS:000460393600005
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Meyer, B
   Grogorick, S
   Vollrath, M
   Magnor, M
AF Meyer, Benjamin
   Grogorick, Steve
   Vollrath, Mark
   Magnor, Marcus
TI Simulating Visual Contrast Reduction during Nighttime Glare Situations
   on Conventional Displays
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Eye adaptation; glare simulation; psycho-physical experiments; driving
   simulation
ID TIME
AB Bright glare in nighttime situations strongly decreases human contrast perception. Nighttime simulations therefore require a way to realistically depict contrast perception of the user. Due to the limited luminance of popular as well as specialized high-dynamic range displays, physical adaptation of the human eye cannot yet be replicated in a physically correct manner in a simulation environment. To overcome this limitation, we propose a method to emulate the adaptation in nighttime glare situations using a perception-based model. We implemented a postprocessing tone mapping algorithm that simulates the corresponding contrast reduction effect for a night-driving simulation with glares from oncoming vehicles headlights. During glare, tone mapping reduces image contrast in accordance with the incident veiling luminance. As the glare expires, the contrast starts to normalize smoothly over time. The conversion of glare parameters and elapsed time into image contrast during the readaptation phase is based on extensive user studies carried out first in a controlled laboratory setup. Additional user studies have then been conducted in field tests to ensure validity of the derived time-dependent tone-mapping function and to verify transferability onto real-world traffic scenarios.
C1 [Meyer, Benjamin; Grogorick, Steve; Magnor, Marcus] TU Braunschweig, Comp Graph Lab, D-38106 Braunschweig, Germany.
   [Vollrath, Mark] TU Braunschweig, Dept Traff & Engn Psychol, D-38106 Braunschweig, Germany.
C3 Braunschweig University of Technology; Braunschweig University of
   Technology
RP Meyer, B (corresponding author), TU Braunschweig, Comp Graph Lab, D-38106 Braunschweig, Germany.
EM benjamin.meyer1983@gmail.com; grogorick@cg.cs.tu-bs.de;
   magnor@cg.cs.tu-bs.de; mark.vollrath@tu-bs.de
OI Magnor, Marcus/0000-0003-0579-480X; Grogorick, Steve/0000-0003-2837-2642
FU ERC Grant "Reality CG" [256941]
FX This work was supported in part by ERC Grant 256941 "Reality CG."
CR Adler F. H., 2011, ADLERS PHYSL EYE, P73
   Adrian W., 1991, INT COMM ILL CIE SES, V2
   ANDERSON SJ, 1995, OPHTHAL PHYSL OPT, V15, P545, DOI 10.1016/0275-5408(95)00070-T
   [Anonymous], 1994, Graph. Gems, DOI DOI 10.1016/B978-0-12-336156-1.50054-9
   Ashikhmin M., 2002, Rendering Techniques 2002. Eurographics Workshop Proceedings, P145
   Ashikhmin M., 2006, ACM T APPL PERCEPT, V3, P399
   Baer R., 2006, Beleuchtungstechnik: Grundlagen, V3
   BLACKWELL HR, 1946, J OPT SOC AM, V36, P624, DOI 10.1364/JOSA.36.000624
   BLOMMAERT F J J, 1990, Spatial Vision, V5, P15, DOI 10.1163/156856890X00066
   Deering MF, 2005, ACM T GRAPHIC, V24, P649, DOI 10.1145/1073204.1073243
   Durand F, 2000, SPRING COMP SCI, P219
   Ferwerda J. A., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P249, DOI 10.1145/237170.237262
   Fry G. A., 1955, 13 SESS COMM INT ECL
   Grave J, 2008, ACM T APPL PERCEPT, V5, DOI 10.1145/1279920.1361704
   Gray R, 2007, OPHTHAL PHYSL OPT, V27, P440, DOI 10.1111/j.1475-1313.2007.00503.x
   Hecht S, 1937, PHYSIOL REV, V17, P239, DOI 10.1152/physrev.1937.17.2.239
   Heidrich Wolfgang, ERIK REINHARD
   Hood D., 1986, HDB PERCEPTION HUMAN
   Irawan P., 2005, Rendering Techniques, P231
   Irikura T., 1999, LIGHTING RES TECHNOL, V31, P57
   Jacobs DE, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2714573
   JOHANSSON G, 1964, SCAND J PSYCHOL, V5, P17, DOI 10.1111/j.1467-9450.1964.tb01404.x
   Krawczyk G., 2005, Proceedings of the 21st spring conference on Computer graphics, P195
   Krebs T., 1994, Lighting Research and Technology, V26, P195
   Krebs T., 1994, Lighting Research and Technology, V26, P199
   Ledda P, 2005, ACM T GRAPHIC, V24, P640, DOI 10.1145/1073204.1073242
   Ledda P., 2004, Proceedings of the 3rd international conference on Com- puter graphics, virtual reality, visualisation and interaction in Africa, P151
   Mantiuk R, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360667
   Mueller N., 2013, P INT S AUT LIGHT, V10, P92
   Olson P., 1989, UMTRI8934
   Olson P., 1984, SAE TECHNICAL PAPER
   Pajak D., 2010, IS T SPIE ELECT IMAG
   Pattanaik SN, 2000, COMP GRAPH, P47, DOI 10.1145/344779.344810
   PAULSSON LE, 1980, INVEST OPHTH VIS SCI, V19, P401
   PULLING NH, 1980, HUM FACTORS, V22, P103, DOI 10.1177/001872088002200111
   READING VM, 1968, VISION RES, V8, P207, DOI 10.1016/0042-6989(68)90007-2
   Reeves A. J., 2010, VISUAL LIGHT DARK AD, V1
   Ritschel T, 2009, COMPUT GRAPH FORUM, V28, P183, DOI 10.1111/j.1467-8659.2009.01357.x
   Ritschel T., 2012, COMP GRAPH FORUM, V31
   Rockwell T.H., 1972, Australian Road Research Board (ARRB) Conference, 6th, 1972, Canberra, V6, P316
   Ruxton GD, 2006, BEHAV ECOL, V17, P688, DOI 10.1093/beheco/ark016
   Seetzen H, 2004, ACM T GRAPHIC, V23, P760, DOI 10.1145/1015706.1015797
   Shapley R., 1984, Prog Retin Res, V3, P263, DOI [10.1016/0278-4327(84)90011-7, DOI 10.1016/0278-4327(84)90011~7]
   Theeuwes J, 2002, HUM FACTORS, V44, P95, DOI 10.1518/0018720024494775
   WELCH BL, 1947, BIOMETRIKA, V34, P28, DOI 10.2307/2332510
   Wilhelm H., 2013, KLIN MONATSBL AUGENH, V230, P11
   WINNER H., 2012, HDB FAHRERASSISTENZS
   YOSHIDA A, 2008, P ACM S APPL PERC GR, P83
   Yoshida A, 2007, J ELECTRON IMAGING, V16, DOI 10.1117/1.2711822
NR 49
TC 3
Z9 3
U1 0
U2 11
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD AUG
PY 2016
VL 14
IS 1
AR 4
DI 10.1145/2934684
PG 20
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DV4EB
UT WOS:000382876900004
DA 2024-07-18
ER

PT J
AU Leyrer, M
   Linkenauger, SA
   Bülthoff, HH
   Mohler, BJ
AF Leyrer, Markus
   Linkenauger, Sally A.
   Buelthoff, Heinrich H.
   Mohler, Betty J.
TI Eye Height Manipulations: A Possible Solution to Reduce Underestimation
   of Egocentric Distances in Head-Mounted Displays
SO ACM Transactions on Applied Perception
LA English
DT Article
DE Human Factors; Eye height; distance perception; distance
   underestimation; virtual reality
ID VIRTUAL ENVIRONMENTS; SCALED INFORMATION; VIEWING CONDITIONS; DIRECTED
   ACTION; OPTIC FLOW; PERCEPTION; AFFORDANCES; JUDGMENTS; GRAPHICS;
   QUALITY
AB Virtual reality technology can be considered a multipurpose tool for diverse applications in various domains, for example, training, prototyping, design, entertainment, and research investigating human perception. However, for many of these applications, it is necessary that the designed and computer-generated virtual environments are perceived as a replica of the real world. Many research studies have shown that this is not necessarily the case. Specifically, egocentric distances are underestimated compared to real-world estimates regardless of whether the virtual environment is displayed in a head-mounted display or on an immersive large-screen display. While the main reason for this observed distance underestimation is still unknown, we investigate a potential approach to reduce or even eliminate this distance underestimation. Building up on the angle of declination below the horizon relationship for perceiving egocentric distances, we describe how eye height manipulations in virtual reality should affect perceived distances. In addition, we describe how this relationship could be exploited to reduce distance underestimation for individual users. In a first experiment, we investigate the influence of a manipulated eye height on an action-based measure of egocentric distance perception. We found that eye height manipulations have similar predictable effects on an action-based measure of egocentric distance as we previously observed for a cognitive measure. This might make this approach more useful than other proposed solutions across different scenarios in various domains, for example, for collaborative tasks. In three additional experiments, we investigate the influence of an individualized manipulation of eye height to reduce distance underestimation in a sparse-cue and a rich-cue environment. In these experiments, we demonstrate that a simple eye height manipulation can be used to selectively alter perceived distances on an individual basis, which could be helpful to enable every user to have an experience close to what was intended by the content designer.
C1 [Leyrer, Markus; Buelthoff, Heinrich H.; Mohler, Betty J.] Max Planck Inst Biol Cybernet, D-72076 Tubingen, Germany.
   [Linkenauger, Sally A.] Univ Lancaster, Lancaster LA1 4YW, England.
C3 Max Planck Society; Lancaster University
RP Leyrer, M (corresponding author), Spemannstr 38, D-72076 Tubingen, Germany.
EM markus.leyrer@tuebingen.mpg.de; s.linkenauger@lancaster.ac.uk;
   heinrich.buelthoff@tuebingen.mpg.de; betty.mohler@tuebingen.mpg.de
RI Bülthoff, Heinrich H/J-6579-2012; Bülthoff, Heinrich/AAC-8818-2019
OI Bülthoff, Heinrich H/0000-0003-2568-0607; Linkenauger,
   Sally/0000-0002-6056-0187
FU EC [AAT-285681]; Max Planck Society; Brain Korea 21 PLUS Program through
   the National Research Foundation of Korea - Ministry of Education
FX This research was supported in part by the EC FP7 project VR-HYPERSPACE
   (AAT-285681), by the Max Planck Society, and by the Brain Korea 21 PLUS
   Program through the National Research Foundation of Korea funded by the
   Ministry of Education. Authors' addresses: M. Leyrer, H. H. Bulthoff,
   and B. J. Mohler, Spemannstr. 38, 72076 Tuebingen, Germany; emails:
   {markus. leyrer, heinrich.buelthoff, betty.mohler}@tuebingen.mpg.de; S.
   A. Linkenauger, Fylde College, Bailrigg, Lancaster LA1 4YF, United
   Kingdom; email: s.linkenauger@lancaster.ac.uk.
CR [Anonymous], 2008, P 2008 ACM S VIRTUAL, DOI DOI 10.1145/1450579.1450614
   Creem-Regehr SH, 2005, PERCEPTION, V34, P191, DOI 10.1068/p5144
   D'Cruz M, 2014, 2014 IEEE VIRTUAL REALITY (VR), P167, DOI 10.1109/VR.2014.6802104
   Di Luca M, 2010, PRESENCE-TELEOP VIRT, V19, P569, DOI 10.1162/pres_a_00023
   Franz Gerald, 2005, AMPIRICAL APPROACH E
   Grechkin TY, 2010, ACM T APPL PERCEPT, V7, DOI 10.1145/1823738.1823744
   Jones JB, 2012, PLANT NUTRITION AND SOIL FERTILITY MANUAL, 2ND EDITION, P119
   Knapp JM, 2004, PRESENCE-TELEOP VIRT, V13, P572, DOI 10.1162/1054746042545238
   Kuhl SA, 2009, ACM T APPL PERCEPT, V6, DOI 10.1145/1577755.1577762
   Kuhl ScottA., 2006, P 3 S APPL PERCEPTIO, P15, DOI DOI 10.1145/1140491.1140494
   Kulik Alexander, 2011, P SIGGRAPH AS C SA 1, DOI http://dx.doi.org/10.1145/2024156.2024222
   Kunz BR, 2009, ATTEN PERCEPT PSYCHO, V71, P1284, DOI 10.3758/APP.71.6.1284
   LEE DN, 1980, PHILOS T R SOC B, V290, P169, DOI 10.1098/rstb.1980.0089
   Leyrer M., 2011, P ACM SIGGRAPH S APP, DOI 10.1145/2077451.2077464
   Leyrer Markus, 2015, IMPORTANCE POSTURAL
   Loomis Jack M., 2008, CARN S COGN
   Loomis JM, 2003, VIRTUAL AND ADAPTIVE ENVIRONMENTS: APPLICATIONS, IMPLICATIONS, AND HUMAN PERFORMANCE ISSUES, P21
   Loomis JM, 1998, PERCEPT PSYCHOPHYS, V60, P966, DOI 10.3758/BF03211932
   LOOMIS JM, 1992, J EXP PSYCHOL HUMAN, V18, P906, DOI 10.1037/0096-1523.18.4.906
   MARK LS, 1987, J EXP PSYCHOL HUMAN, V13, P361, DOI 10.1037/0096-1523.13.3.361
   Mohler BJ, 2010, PRESENCE-TELEOP VIRT, V19, P230, DOI 10.1162/pres.19.3.230
   Mohler BJ, 2008, APGV 2008: PROCEEDINGS OF THE SYMPOSIUM ON APPLIED PERCEPTION IN GRAPHICS AND VISUALIZATION, P194
   Mohler BettyJ., 2006, P 3 S APPL PERCEPTIO, P9, DOI [10.1145/1140491.1140493, DOI 10.1145/1140491.1140493]
   Ooi TL, 2001, NATURE, V414, P197, DOI 10.1038/35102562
   Phillips Lane., 2009, P 6 S APPL PERCEPTIO, P11
   Piryankova IV, 2013, DISPLAYS, V34, P153, DOI 10.1016/j.displa.2013.01.001
   Renner RS, 2013, ACM COMPUT SURV, V46, DOI 10.1145/2543581.2543590
   Richardson AR, 2007, HUM FACTORS, V49, P507, DOI 10.1518/001872007X200139
   RIESER JJ, 1995, J EXP PSYCHOL HUMAN, V21, P480, DOI 10.1037/0096-1523.21.3.480
   Sahm C. S., 2005, ACM Trans. Appl. Percept. (TAP), V2, P35, DOI DOI 10.1145/1048687.1048690
   SEDGWICK HA, 1986, HDB PERCEPTION HUMAN, V1
   Sedgwick Harold A., 1973, THESIS PROQUEST INFO
   Thompson WB, 2004, PRESENCE-TELEOP VIRT, V13, P560, DOI 10.1162/1054746042545292
   Waller D, 2008, J EXP PSYCHOL-APPL, V14, P61, DOI 10.1037/1076-898X.14.1.61
   WARREN WH, 1987, J EXP PSYCHOL HUMAN, V13, P371, DOI 10.1037/0096-1523.13.3.371
   Willemsen P, 2008, PRESENCE-TELEOP VIRT, V17, P91, DOI 10.1162/pres.17.1.91
   Willemsen P, 2009, ACM T APPL PERCEPT, V6, DOI 10.1145/1498700.1498702
   Williams B., 2009, Proc. Sixth Symposium on Applied Perception in Graphics and Visualization, P7
   Wraga M, 1999, J EXP PSYCHOL HUMAN, V25, P518, DOI 10.1037/0096-1523.25.2.518
   Wraga M, 1999, PERCEPT PSYCHOPHYS, V61, P490, DOI 10.3758/BF03211968
   Wu J, 2005, PERCEPTION, V34, P1045, DOI 10.1068/p5416
   Zhang RM, 2012, ACM T APPL PERCEPT, V9, DOI 10.1145/2325722.2325727
NR 42
TC 24
Z9 26
U1 0
U2 21
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD MAR
PY 2015
VL 12
IS 1
BP 3
EP 25
DI 10.1145/2699254
PG 23
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CG8AI
UT WOS:000353528600001
DA 2024-07-18
ER

PT J
AU Niewiadomski, R
   Pelachaud, C
AF Niewiadomski, Radoslaw
   Pelachaud, Catherine
TI The Effect of Wrinkles, Presentation Mode, and Intensity on the
   Perception of Facial Actions and Full-Face Expressions of Laughter
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Human Factors; Facial expressions; action units; wrinkles; laughter
ID EMOTION RECOGNITION; VIRTUAL CHARACTERS; SMILES; DYNAMICS; AGE
AB This article focuses on the identification and perception of facial action units displayed alone as well as the meaning decoding and perception of full-face synthesized expressions of laughter. We argue that the adequate representation of single action units is important in the decoding and perception of full-face expressions. In particular, we focus on three factors that may influence the identification and perception of single actions and full-face expressions: their presentation mode (static vs. dynamic), their intensity, and the presence of wrinkles.
   For the purpose of this study, we used a hybrid approach for animation synthesis that combines data-driven and procedural animations with synthesized wrinkles generated using a bump mapping method. Using such animation technique, we created animations of single action units and full-face movements of two virtual characters. Next, we conducted two studies to evaluate the role of presentation mode, intensity, and wrinkles in single actions and full-face context-free expressions. Our evaluation results show that intensity and presentation mode influence (1) the identification of single action units and (2) the perceived quality of the animation. At the same time, wrinkles (3) are useful in the identification of a single action unit and (4) influence the perceived meaning attached to the animation of full-face expressions. Thus, all factors are important for successful communication of expressions displayed by virtual characters.
C1 [Niewiadomski, Radoslaw] Univ Genoa, DIBRIS, I-16126 Genoa, Italy.
   [Pelachaud, Catherine] Telecom ParisTech, CNRS LTCI, Paris, France.
C3 University of Genoa; Centre National de la Recherche Scientifique
   (CNRS); IMT - Institut Mines-Telecom; Institut Polytechnique de Paris;
   Telecom Paris
RP Niewiadomski, R (corresponding author), DIBRIS, Viale Causa 13, I-16145 Genoa, Italy.
EM radoslaw.niewiadomski@dibris.unige.it;
   catherine.pelachaud@telecom-paristech.fr
RI Niewiadomski, Radoslaw/D-9775-2015
OI Niewiadomski, Radoslaw/0000-0002-0476-0803
FU European Union [270780 ILHAIRE]
FX The research leading to these results has received funding from the
   European Union Seventh Framework Programme (FP7/2007-2013) under grant
   agreement n 270780 ILHAIRE. Furthermore, the authors would like to thank
   Bing Qu, Satish Pammi, and Jing Huang (Telecom Paristech) for helping in
   the creation of the animations used in the studies presented in this
   article.
CR Ambadar Z, 2005, PSYCHOL SCI, V16, P403, DOI 10.1111/j.0956-7976.2005.01548.x
   [Anonymous], 2001, Emotions, qualia, and consciousness, DOI [DOI 10.1142/9789812810687_0033, 10.1142/9789812810687_0033]
   [Anonymous], 2013, EUROPEAN J HUMOR RES
   [Anonymous], 2009, CURRENT FUTURE PERSP
   Aylett R, 2009, AFFECTIVE INFORMATION PROCESSING, P75, DOI 10.1007/978-1-84800-306-4_5
   Bartneck C, 2005, INT J HUM-COMPUT ST, V62, P179, DOI 10.1016/j.ijhcs.2004.11.006
   BASSILI JN, 1979, J PERS SOC PSYCHOL, V37, P2049, DOI 10.1037/0022-3514.37.11.2049
   Blinn J. F., 1978, SIGGRAPH 1978, P286
   Courgeon M., 2009, P INT WORKSH AFF AW
   Courgeon M, 2009, LECT NOTES ARTIF INT, V5773, P201, DOI 10.1007/978-3-642-04380-2_24
   Darwin C., 1979, EXPRESS EMOT MAN
   Ekman P., 2002, FACIAL ACTION CODING
   Ekman P., 2003, UNMASKING FACE GUIDE
   Elfenbein HA, 2002, PSYCHOL BULL, V128, P203, DOI 10.1037//0033-2909.128.2.203
   FRANK MG, 1993, HUMOR, V6, P9, DOI 10.1515/humr.1993.6.1.9
   Hess U, 1997, J NONVERBAL BEHAV, V21, P241, DOI 10.1023/A:1024952730333
   Hess U, 2012, J EXP SOC PSYCHOL, V48, P1377, DOI 10.1016/j.jesp.2012.05.018
   Hyniewska S., 2013, THESIS
   Kaiser S., 2001, EMOTION THEORY METHO, P285
   Kätsyri J, 2008, INT J HUM-COMPUT ST, V66, P233, DOI 10.1016/j.ijhcs.2007.10.001
   KELTNER D, 1995, J PERS SOC PSYCHOL, V68, P441, DOI 10.1037/0022-3514.68.3.441
   Krumhuber E, 2009, J NONVERBAL BEHAV, V33, P1, DOI 10.1007/s10919-008-0056-8
   Martin RA, 1999, HUMOR, V12, P355, DOI 10.1515/humr.1999.12.4.355
   Melo C. M., 2012, LECT NOTES COMPUTER, V7502, P53
   Niewiadomski R., 2012, P 4 INT WORKSH CORP
   Niewiadomski R., 2011, P 16 INT C 3D WEB TE, P11
   Niewiadomski R., 2012, P 25 ANN C COMP AN S, P37
   Noel S., 2006, IEEE INT WORKSH HAPT, P99
   Ochs M, 2013, IEEE SIGNAL PROC MAG, V30, P128, DOI 10.1109/MSP.2012.2230541
   Ochs M, 2012, COGN PROCESS, V13, P519, DOI 10.1007/s10339-011-0424-x
   Ostermann J., 2002, MPEG-4 Facial Animation: The Standard, Implementation and Applications, P17
   Qu B., 2012, FAA 3 INT S FAC AN A
   Rossen B, 2008, LECT NOTES COMPUT SC, V5208, P237
   Ruch W., 1993, HDB EMOTIONS, P605, DOI DOI 10.5167/UZH-77841
   Rymarczyk K, 2011, INT J PSYCHOPHYSIOL, V79, P330, DOI 10.1016/j.ijpsycho.2010.11.001
   Saragih JM, 2011, INT J COMPUT VISION, V91, P200, DOI 10.1007/s11263-010-0380-4
   Sato W, 2004, COGNITION EMOTION, V18, P701, DOI 10.1080/02699930341000176
   Scherer K. R., 2001, Appraisal processes in emotion: Theory, methods, research, P92
   Scherer KR, 2004, SOC SCI INFORM, V43, P499, DOI 10.1177/0539018404047701
   Urbain J., 2010, P 7 C INT LANG RES E, P2996
   Weyers P, 2006, PSYCHOPHYSIOLOGY, V43, P450, DOI 10.1111/j.1469-8986.2006.00451.x
   Young AW, 1997, COGNITION, V63, P271, DOI 10.1016/S0010-0277(97)00003-6
NR 42
TC 7
Z9 7
U1 1
U2 12
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD MAR
PY 2015
VL 12
IS 1
BP 27
EP 47
DI 10.1145/2699255
PG 21
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA CG8AI
UT WOS:000353528600002
DA 2024-07-18
ER

PT J
AU Wang, RI
   Pelfrey, B
   Duchowski, AT
   House, DH
AF Wang, Rui I.
   Pelfrey, Brandon
   Duchowski, Andrew T.
   House, Donald H.
TI Online 3D Gaze Localization on Stereoscopic Displays
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Visual Perception; Stereo; Stereoscopic displays; eye tracking; 3D eye
   tracking; vergence
AB This article summarizes our previous work on developing an online system to allow the estimation of 3D gaze depth using eye tracking in a stereoscopic environment. We report on recent extensions allowing us to report the full 3D gaze position. Our system employs a 3D calibration process that determines the parameters of a mapping from a naive depth estimate, based simply on triangulation, to a refined 3D gaze point estimate tuned to a particular user. We show that our system is an improvement on the geometry-based 3D gaze estimation returned by a proprietary algorithm provided with our tracker. We also compare our approach with that of the Parameterized Self-Organizing Map (PSOM) method, due to Essig and colleagues, which also individually calibrates to each user. We argue that our method is superior in speed and ease of calibration, is easier to implement, and does not require an iterative solver to produce a gaze position, thus guaranteeing computation at the rate of tracker acquisition. In addition, we report on a user study that indicates that, compared with PSOM, our method more accurately estimates gaze depth, and is nearly as accurate in estimating horizontal and vertical position. Results are verified on two different 4D eye tracking systems, a high accuracy Wheatstone haploscope and a medium accuracy active stereo display. Thus, it is the recommended method for applications that primarily require gaze depth information, while its ease of use makes it suitable for many applications requiring full 3D gaze position.
C1 [Wang, Rui I.; Pelfrey, Brandon; Duchowski, Andrew T.; House, Donald H.] Clemson Univ, Sch Comp, Clemson, SC 29634 USA.
C3 Clemson University
RP House, DH (corresponding author), Clemson Univ, Sch Comp, 100 McAdams Hall, Clemson, SC 29634 USA.
FU National Science Foundation [IIS-0915085]
FX This material is based upon work supported by the National Science
   Foundation under Grant No. IIS-0915085. Any opinions, findings, and
   conclusions expressed in this material are those of the authors and do
   not necessarily reflect the views of the NSF. Author's address: D.H.
   House, Clemson University, School of Computing, 100 McAdams Hall,
   Clemson, SC 29634.
CR [Anonymous], 1997, The eye and visual optical instruments
   [Anonymous], ETRA 10
   [Anonymous], 1986, Curve and Surface Fitting: An Introduction
   [Anonymous], 2007, Eye tracking methodology: Theory and practice, DOI DOI 10.1007/978-3-319-57883-5
   Backus BT, 1999, VISION RES, V39, P1143, DOI 10.1016/S0042-6989(98)00139-4
   Bair AS, 2006, IEEE T VIS COMPUT GR, V12, P1125, DOI 10.1109/TVCG.2006.183
   Brown R. G., 1983, Introduction to random signal analysis and Kalman filtering
   Buttner-Ennever J. A., 1988, REV OCULOMOTOR RES S, VII
   CAMPBELL FW, 1965, NATURE, V208, P191, DOI 10.1038/208191a0
   Duchowski A. T., 2011, APPL PERCEPTION GRAP
   Duchowski A. T., 2012, 5 HAML S MED ROB LON
   Essig K., 2012, ETRA 12
   Essig K, 2006, INT J PARALLEL EMERG, V21, P79, DOI 10.1080/17445760500354440
   Essig K, 2004, PROCEEDINGS OF THE TWENTY-SIXTH ANNUAL CONFERENCE OF THE COGNITIVE SCIENCE SOCIETY, P357
   FINCHAM EF, 1957, J PHYSIOL-LONDON, V137, P488, DOI 10.1113/jphysiol.1957.sp005829
   Finn J.D., 1974, GEN MODEL MULTIVARIA
   Interrante V, 2006, P IEEE VIRT REAL ANN, P3, DOI 10.1109/VR.2006.52
   Jones J. A., 2008, ACM SIGGRAPH S APPL, P914
   JULESZ B, 1971, FDN CYCLOPIAN PERCEP
   KOHONEN T, 1982, BIOL CYBERN, V43, P59, DOI 10.1007/BF00337288
   Medlin E., 2003, THESIS CLEMSON U CLE
   Oppenheim A. V., 1989, Discrete -Time Signal Processing
   Pfeiffer T., 2012, ETRA 12
   Pfeiffer T., 2010, THESIS BIELEFELD U B
   Pfeiffer T., 2009, J VIRTUAL REALITY BR, V5, P16
   Pomplun M., 1994, ADV ARTIFICIAL INTEL, P63
   SHAKHNOVICH A.R., 1977, BRAIN REGULATION EYE
   Shibata T, 2011, J VISION, V11, DOI 10.1167/11.8.11
   Wang R. I., 2012, 3D IMAGING MODELING
NR 29
TC 19
Z9 23
U1 3
U2 11
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD APR
PY 2014
VL 11
IS 1
AR 3
DI 10.1145/2593689
PG 21
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AG7CF
UT WOS:000335574900003
DA 2024-07-18
ER

PT J
AU Watanabe, J
   Maeda, T
   Ando, H
AF Watanabe, Junji
   Maeda, Taro
   Ando, Hideyuki
TI Gaze-Contingent Visual Presentation Technique with
   Electro-Ocular-Graph-Based Saccade Detection
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Human Factors; Measurement; Visual display; saccade-based display;
   electro-ocular-graph; saccade detection; character recognition
ID EYE-MOVEMENTS; SUPPRESSION; PERCEPTION; THRESHOLD; EOG; SYSTEM
AB When a single column of light sources flashes quickly in a temporal pattern during a horizontal saccade eye movement, two-dimensional images can be perceived in the space neighboring the light source. This perceptual phenomenon has been applied to light devices for visual arts and entertainment. However, a serious drawback in exploiting this perceptual phenomenon for a visual information display is that a two-dimensional image cannot be viewed if there is any discrepancy between the ocular motility and the flicker timing. We overcame this drawback by combining the saccade-based display with an electro-ocular-graph-based sensor for detecting the saccade. The saccade onset is measured with the electro-ocular-graph-based sensor in real time and the saccade-based display is activated instantaneously as the saccade begins. The psychophysical experiments described in this article demonstrates that the method that we used can detect saccades with low latency and allows the saccade-based display to convey visual information more effectively than when the light sources continuously blink regardless of the observer's eye movements.
C1 [Watanabe, Junji] NTT Corp, NTT Commun Sci Labs, Kanagawa, Japan.
   [Maeda, Taro; Ando, Hideyuki] Osaka Univ, Grad Sch Informat Sci & Technol, Osaka, Japan.
C3 Nippon Telegraph & Telephone Corporation; Osaka University
RP Watanabe, J (corresponding author), NTT Corp, NTT Commun Sci Labs, Kanagawa, Japan.
EM junji@junji.org; hide@ist.osaka-u.ac.jp
OI ANDO, HIDEYUKI/0000-0002-8405-3120
CR ANDO H., 2007, P SIGGRAPH 07 EM TEC
   [Anonymous], 2006, CHI 06 EXTENDED ABST, DOI DOI 10.1145/1125451.1125655
   ARUGA R., 2008, P 18 INT C ART REAL, P254
   Barea R, 2002, IEEE T NEUR SYS REH, V10, P209, DOI 10.1109/TNSRE.2002.806829
   BELL B, 1986, LEONARDO, V19, P3, DOI 10.2307/1578294
   Borghetti D, 2007, COMPUT BIOL MED, V37, P1765, DOI 10.1016/j.compbiomed.2007.05.003
   Brown M, 2006, DOC OPHTHALMOL, V113, P205, DOI 10.1007/s10633-006-9030-0
   Bulling A, 2009, J AMB INTEL SMART EN, V1, P157, DOI 10.3233/AIS-2009-0020
   BURR DC, 1994, NATURE, V371, P511, DOI 10.1038/371511a0
   Cowan N, 2001, BEHAV BRAIN SCI, V24, P87, DOI 10.1017/S0140525X01003922
   Duchowski AT, 2007, ACM T MULTIM COMPUT, V3, DOI 10.1145/1314303.1314309
   FISCHER B, 1984, EXP BRAIN RES, V57, P191
   HERSHBERGER W, 1987, PERCEPT PSYCHOPHYS, V41, P35, DOI 10.3758/BF03208211
   Hori J, 2006, LECT NOTES COMPUT SC, V4061, P950
   Komogortsev OV, 2009, J ELECTRON IMAGING, V18, DOI 10.1117/1.3158609
   KUTNER M. H., 2004, APPL LINEAR STAT MOD, P790
   LATOUR PL, 1962, VISION RES, V2, P261, DOI 10.1016/0042-6989(62)90031-7
   Levi DM, 2008, VISION RES, V48, P635, DOI 10.1016/j.visres.2007.12.009
   Loschky LC, 2007, ACM T MULTIM COMPUT, V3, DOI 10.1145/1314303.1314310
   Noritake A, 2005, SPATIAL VISION, V18, P297, DOI 10.1163/1568568054089384
   Pelli DG, 2008, NAT NEUROSCI, V11, P1129, DOI 10.1038/nn.2187
   Roberts D, 2008, PROCEEDINGS OF THE EYE TRACKING RESEARCH AND APPLICATIONS SYMPOSIUM (ETRA 2008), P197, DOI 10.1145/1344471.1344519
   Sogo H, 2001, VISION RES, V41, P935, DOI 10.1016/S0042-6989(00)00318-7
   Triesch J., 2002, Proceedings ETRA 2002. Eye Tracking Research and Applications Symposium, P95, DOI 10.1145/507072.507092
   UCHIKAWA K, 1995, J OPT SOC AM A, V12, P661, DOI 10.1364/JOSAA.12.000661
   VOLKMANN FC, 1968, J OPT SOC AM, V58, P562, DOI 10.1364/JOSA.58.000562
   Watanabe J., 2005, Systems and Computers in Japan, V36, P77, DOI 10.1002/scj.10690
   Watanabe J, 2005, VISION RES, V45, P413, DOI 10.1016/j.visres.2004.09.010
   WATANABE J., 2004, SIGGRAPH 2004 SKETCH
   WATANABE J., 2005, SYST COMPUT JPN, V36, P9
   Watanabe J, 2007, PRESENCE-TELEOP VIRT, V16, P224, DOI 10.1162/pres.16.2.224
   YOUNG LR, 1975, BEHAV RES METH INSTR, V7, P397, DOI 10.3758/BF03201553
   ZUBER BL, 1966, EXP NEUROL, V16, P65, DOI 10.1016/0014-4886(66)90087-2
NR 33
TC 4
Z9 4
U1 0
U2 5
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD JUN
PY 2012
VL 9
IS 2
AR 6
DI 10.1145/2207216.2207217
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 959PR
UT WOS:000305325800001
DA 2024-07-18
ER

PT J
AU Vicentini, M
   Galvan, S
   Botturi, D
   Fiorini, P
AF Vicentini, M.
   Galvan, S.
   Botturi, D.
   Fiorini, P.
TI Evaluation of Force and Torque Magnitude Discrimination Thresholds on
   the Human Hand-Arm System
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Human Factors; Measurement; Force thresholds; perceptual asymmetries;
   perceptual-based signal processing
ID PERCEPTION; FEEDBACK; WEIGHT
AB This article reports on experiments about haptic perception aimed at measuring the force/torque differential thresholds applied to the hand-arm system. The experimental work analyzes how force is sent back to the user by means of a 6 degrees-of-freedom haptic device. Our findings on force perception indicate that the just-noticeable-difference is generally higher than previously reported in the literature and not constant along the stimulus continuum. We found evidence that the thresholds change also among the different directions. Furthermore, asymmetries in force perceptions, which were not described in previous reports, can be evinced for most of the directions. These findings support our claim that human beings perceive forces differently along different directions, thus suggesting that perception can also be enhanced by suitable signal processing, that is, with a manipulation of the force signal before it reaches the haptic device. We think that the improvement of the user perception can have a great impact in many applications and in particular we are focusing on surgical teleoperation scenarios.
C1 [Vicentini, M.; Galvan, S.; Botturi, D.; Fiorini, P.] Univ Verona, Altair Lab, Dept Comp Sci, I-37134 Verona, Italy.
C3 University of Verona
RP Vicentini, M (corresponding author), Univ Verona, Altair Lab, Dept Comp Sci, Ca Vignal 2,Str Le Grazie 15, I-37134 Verona, Italy.
EM vicentini@metropolis.sci.univr.it; galvan@metropolis.sci.univr.it;
   botturi@metropolis.sci.univr.it; fiorini@metropolis.sci.univr.it
RI Fiorini, Paolo/A-2603-2012
OI Vicentini, Marco/0000-0003-3112-2070
FU EU [IST-045201]
FX This work was partially founded by the AccuRobAs project under EU's 6th
   Framework Programme (contract IST-045201).
CR Allin S, 2002, 10TH SYMPOSIUM ON HAPTIC INTERFACES FOR VIRTUAL ENVIRONMENT AND TELEOPERATOR SYSTEMS, PROCEEDINGS, P299, DOI 10.1109/HAPTIC.2002.998972
   [Anonymous], 2008, R LANG ENV STAT COMP
   Barbagli F., 2006, ACM T APPL PERCEPT, V3, P125, DOI [10.1145/1141897.1141901, DOI 10.1145/1141897.1141901]
   Baud-Bovy G, 1998, J NEUROSCI, V18, P1528
   BEJCZY A, 1980, ADV COMPUTER TECHNOL, V1, P197
   Brewer BR, 2005, IEEE T NEUR SYS REH, V13, P1, DOI 10.1109/TNSRE.2005.843443
   BRODIE EE, 1984, PERCEPT PSYCHOPHYS, V36, P477, DOI 10.3758/BF03207502
   Burdea GRIGORE, 1996, Force and touch feedback for virtual reality
   Burnham KP, 2004, SOCIOL METHOD RES, V33, P261, DOI 10.1177/0049124104268644
   Fasse ED, 2000, BIOL CYBERN, V82, P69, DOI 10.1007/PL00007962
   Galvan S, 2006, 2006 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, VOLS 1-12, P971, DOI 10.1109/IROS.2006.281776
   Gescheider G.A., 2002, Stevens' handbook of experimental psychology, V3rd, P91
   Gescheider GA., 1997, PSYCHOPHYSICS FUNDAM
   GREEN DM, 1993, J ACOUST SOC AM, V93, P2096, DOI 10.1121/1.406696
   GU X, 1994, J ACOUST SOC AM, V96, P93, DOI 10.1121/1.410378
   Hale KS, 2004, IEEE COMPUT GRAPH, V24, P33, DOI 10.1109/MCG.2004.1274059
   HINTERSEER P, 2006, HAPTIC INTERFACES VI, P4
   Hurmuzlu Y, 1998, PRESENCE-TELEOP VIRT, V7, P290, DOI 10.1162/105474698565721
   JONES LA, 1993, EXP BRAIN RES, V94, P343
   JONES LA, 1982, EUR J APPL PHYSIOL, V50, P125, DOI 10.1007/BF00952251
   Kaas AL, 2006, EXP BRAIN RES, V170, P403, DOI 10.1007/s00221-005-0223-7
   KAY BA, 1989, ENG MED BIOL SOC, V5, P1522
   Leek MR, 2001, PERCEPT PSYCHOPHYS, V63, P1279, DOI 10.3758/BF03194543
   Luyat M, 2001, PERCEPT PSYCHOPHYS, V63, P541, DOI 10.3758/BF03194419
   Newberry AC, 2007, P I MECH ENG D-J AUT, V221, P405, DOI 10.1243/09544070JAUTO415
   Newport R, 2002, CURR BIOL, V12, P1661, DOI 10.1016/S0960-9822(02)01178-8
   PANG XD, 1991, PERCEPT PSYCHOPHYS, V49, P531, DOI 10.3758/BF03212187
   REES DW, 1960, 60601 WADD WRIGHTPAT
   ROSS HE, 1987, Q J EXP PSYCHOL-A, V39, P77, DOI 10.1080/02724988743000042
   SAMUR E, 2007, P IEEE INT C INT ROB
   Schlicht EJ, 2007, J NEUROPHYSIOL, V97, P4203, DOI 10.1152/jn.00160.2007
   Srinivasan MA, 1997, COMPUT GRAPH-UK, V21, P393, DOI 10.1016/S0097-8493(97)00030-7
   STANNEY K, 1995, VIRTUAL REALITY ANNUAL INTERNATIONAL SYMPOSIUM '95, PROCEEDINGS, P28
   Toffin D, 2003, J NEUROPHYSIOL, V90, P3040, DOI 10.1152/jn.00271.2003
   Wagner CR, 2007, PRESENCE-VIRTUAL AUG, V16, P252, DOI 10.1162/pres.16.3.252
   Wheat HE, 2004, J NEUROSCI, V24, P3394, DOI 10.1523/JNEUROSCI.4822-03.2004
   Wichmann FA, 2001, PERCEPT PSYCHOPHYS, V63, P1293, DOI 10.3758/BF03194544
NR 37
TC 23
Z9 26
U1 0
U2 8
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD OCT
PY 2010
VL 8
IS 1
AR 1
DI 10.1145/1857893.1857894
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 748AJ
UT WOS:000289362100001
DA 2024-07-18
ER

PT J
AU Kuhl, SA
   Thompson, WB
   Creem-Regehr, SH
AF Kuhl, Scott A.
   Thompson, William B.
   Creem-Regehr, Sarah H.
TI HMD Calibration and Its Effects on Distance Judgments
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Experimentation; Human Factors; Immersive virtual environment;
   perception; minification; field of view; pincushion distortion; pitch
ID PERCEPTION
AB Most head-mounted displays (HMDs) suffer from substantial optical distortion, and vendor-supplied specifications for field-of-view often are at variance with reality. Unless corrected, such displays do not present perspective-related visual cues in a geometrically correct manner. Distorted geometry has the potential to affect applications of HMDs, which depend on precise spatial perception. This article provides empirical evidence for the degree to which common geometric distortions affect one type of spatial judgment in virtual environments. We show that minification or magnification in the HMD that would occur from misstated HMD field of view causes significant changes in distance judgments. Incorrectly calibrated pitch and pincushion distortion, however, do not cause statistically significant changes in distance judgments for the degree of distortions examined. While the means for determining the optical distortion of display systems are well known, they are often not used in non-see-through HMDs due to problems in measuring and correcting for distortion. As a result, we also provide practical guidelines for creating geometrically calibrated systems.
C1 [Kuhl, Scott A.] Univ Utah, Sch Comp, Salt Lake City, UT 84112 USA.
C3 Utah System of Higher Education; University of Utah
RP Kuhl, SA (corresponding author), Univ Utah, Sch Comp, 50 S Cent Campus Dr,Room 3190, Salt Lake City, UT 84112 USA.
EM skuhl@cs.utah.edu; thompson@cs.utah.edu; sarah.creem@psych.utah.edu
FU National Science Foundation [IIS-0121084, IIS-0745131]
FX This material is based on work supported by the National Science
   Foundation under grant numbers IIS-0121084 and IIS-0745131.
CR Andre J, 2006, PERCEPT PSYCHOPHYS, V68, P353, DOI 10.3758/BF03193682
   [Anonymous], 2005, ACM Transactions on Applied Perception, DOI [DOI 10.1145/1077399.1077403, DOI 10.1145/1077399.10774032,3,9]
   Azuma R.T., 1994, Proceedings of ACM SIGGRAPH, V94, P197
   BAX MR, 2004, STANFORD ELECT ENG C
   Campos J., 2007, J VISION, V7, p1028a
   ELLIS SR, 1993, P SOC INF DISPL SOC
   Gardner PL, 2001, EXP BRAIN RES, V136, P379, DOI 10.1007/s002210000590
   GENC Y, 2002, P 1 IEEE ACM INT S M
   Gilson SJ, 2008, J NEUROSCI METH, V173, P140, DOI 10.1016/j.jneumeth.2008.05.015
   GRUTZMACHER RP, 1997, P 9 INT C PERC ACT S, P229
   Knapp J. M., 1999, ProQuest Diss. Theses,, P125
   KRAFT RN, 1989, PERCEPT PSYCHOPHYS, V45, P459, DOI 10.3758/BF03210720
   KUHL SA, 2008, P S APPL PERC GRAPH
   Kuhl SA, 2008, ACM T APPL PERCEPT, V5, DOI 10.1145/1402236.1402241
   Kuhl ScottA., 2006, P 3 S APPL PERCEPTIO, P15, DOI DOI 10.1145/1140491.1140494
   Loomis JM, 2003, VIRTUAL AND ADAPTIVE ENVIRONMENTS: APPLICATIONS, IMPLICATIONS, AND HUMAN PERFORMANCE ISSUES, P21
   LUMSDEN EA, 1983, PERCEPT PSYCHOPHYS, V33, P177, DOI 10.3758/BF03202836
   McGarrity E., 1999, Proceedings 2nd IEEE and ACM International Workshop on Augmented Reality (IWAR'99), P75, DOI 10.1109/IWAR.1999.803808
   Mohler BettyJ., 2006, P 3 S APPL PERCEPTIO, P9, DOI [10.1145/1140491.1140493, DOI 10.1145/1140491.1140493]
   Ooi TL, 2001, NATURE, V414, P197, DOI 10.1038/35102562
   Richardson AR, 2005, APPL COGNITIVE PSYCH, V19, P1089, DOI 10.1002/acp.1140
   Rinalducci EJ, 1996, PRESENCE-TELEOP VIRT, V5, P353, DOI 10.1162/pres.1996.5.3.353
   Robinett W., 1992, Presence: Teleoper. Virtual Environ, V1, P45, DOI DOI 10.1162/pres.1992.1.1.45
   Rogers S., 1995, PERCEPTION SPACE MOT, V5, P119
   Sahm C. S., 2005, ACM Trans. Appl. Percept. (TAP), V2, P35, DOI DOI 10.1145/1048687.1048690
   Sedgwick H.A., 1983, HUMAN MACHINE VISION, P425, DOI DOI 10.1016/B978-0-12-084320-6.50020-7
   Smith O.W., 1958, PERCEPT MOTOR SKILL, V8, P79
   SMITH OW, 1958, AM J PSYCHOL, V71, P529, DOI 10.2307/1420248
   Stoper AE, 1999, EMORY S COG, P97
   Thompson WB, 2004, PRESENCE-TELEOP VIRT, V13, P560, DOI 10.1162/1054746042545292
   TSAI RY, 1987, IEEE T ROBOTIC AUTOM, V3, P323, DOI 10.1109/JRA.1987.1087109
   Watson B. A., 1995, Proceedings. Virtual Reality Annual International Symposium '95 (Cat. No.95CH35761), P172, DOI 10.1109/VRAIS.1995.512493
   WENG JY, 1992, IEEE T PATTERN ANAL, V14, P965, DOI 10.1109/34.159901
   Willemsen P, 2009, ACM T APPL PERCEPT, V6, DOI 10.1145/1498700.1498702
   Willett WC, 2008, ASIA PAC J CLIN NUTR, V17, P1
   Witmer BG, 1998, PRESENCE-TELEOP VIRT, V7, P144, DOI 10.1162/105474698565640
NR 36
TC 73
Z9 84
U1 0
U2 17
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD AUG
PY 2009
VL 6
IS 3
AR 19
DI 10.1145/1577755.1577762
PG 20
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 511ZS
UT WOS:000271212000007
DA 2024-07-18
ER

PT J
AU Boucheny, C
   Bonneau, GP
   Droulez, J
   Thibault, G
   Ploix, S
AF Boucheny, Christian
   Bonneau, Georges-Pierre
   Droulez, Jacques
   Thibault, Guillaume
   Ploix, Stephane
TI A Perceptive Evaluation of Volume Rendering Techniques
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article; Proceedings Paper
CT Symposium on Applied Perception in Graphics and Visualization
CY JUL 25-27, 2007
CL Tubingen, GERMANY
DE Experimentation; Human Factors; Direct volume rendering; perception of
   transparency; perspective projection; structure from motion
ID STRUCTURE-FROM-MOTION; ENHANCING DEPTH-PERCEPTION; ACHROMATIC
   TRANSPARENCY; INFORMATION; SURFACES; SHAPE; CONSTANCY; LIGHTNESS
AB The display of space filling data is still a challenge for the community of visualization. Direct volume rendering (DVR) is one of the most important techniques developed to achieve direct perception of such volumetric data. It is based on semitransparent representations, where the data are accumulated in a depth-dependent order. However, it produces images that may be difficult to understand, and thus several techniques have been proposed so as to improve its effectiveness, using for instance lighting models or simpler representations (e.g., maximum intensity projection). In this article, we present three perceptual studies that examine how DVR meets its goals, in either static or dynamic context. We show that a static representation is highly ambiguous, even in simple cases, but this can be counterbalanced by use of dynamic cues (i.e., motion parallax) provided that the rendering parameters are correctly tuned. In addition, perspective projections are demonstrated to provide relevant information to disambiguate depth perception in dynamic displays.
C1 [Boucheny, Christian; Bonneau, Georges-Pierre] Univ Grenoble, Grenoble, France.
   [Boucheny, Christian; Bonneau, Georges-Pierre] CNRS, Lab Jean Kuntzmann, F-75700 Paris, France.
   [Boucheny, Christian; Bonneau, Georges-Pierre] INRIA Grenoble Rhone Alpes, Grenoble, France.
   [Droulez, Jacques] LPPA, CNRS, Paris, France.
   [Droulez, Jacques] LPPA, Coll France, Paris, France.
C3 Communaute Universite Grenoble Alpes; Universite Grenoble Alpes (UGA);
   Communaute Universite Grenoble Alpes; Institut National Polytechnique de
   Grenoble; Universite Grenoble Alpes (UGA); Centre National de la
   Recherche Scientifique (CNRS); Inria; Universite PSL; College de France;
   Centre National de la Recherche Scientifique (CNRS); Universite PSL;
   College de France; Centre National de la Recherche Scientifique (CNRS)
RP Boucheny, C (corresponding author), Univ Grenoble, Grenoble, France.
EM christian.boucheny@inria.fr; georges-pierre.bonneau@inria.fr;
   jacques.droulez@college-de-france.fr; guillaume.thibault@edf.fr;
   stephane.ploix@edf.fr
OI THIBAULT, Guillaume/0000-0002-5294-9184; Ploix,
   Stephane/0000-0002-8554-5901
CR Anderson BL, 1997, PERCEPTION, V26, P419, DOI 10.1068/p260419
   Bair A, 2007, IEEE T VIS COMPUT GR, V13, P1656, DOI 10.1109/TVCG.2007.70559
   Bruckner S, 2007, IEEE T VIS COMPUT GR, V13, P1344, DOI 10.1109/TVCG.2007.70555
   Eagle RA, 1999, VISION RES, V39, P1713, DOI 10.1016/S0042-6989(98)00275-2
   Ebert D, 2000, IEEE VISUAL, P195, DOI 10.1109/VISUAL.2000.885694
   Engel K., 2001, HWWS 01, P9
   Fulvio JM, 2006, J VISION, V6, P760, DOI 10.1167/6.8.1
   GERBINO W, 1990, J EXP PSYCHOL HUMAN, V16, P3, DOI 10.1037/0096-1523.16.1.3
   Giesen J, 2007, IEEE T VIS COMPUT GR, V13, P1664, DOI 10.1109/TVCG.2007.70542
   GREEN BF, 1961, J EXP PSYCHOL, V62, P272, DOI 10.1037/h0045622
   Hibbard B, 2000, COMPUT GRAPHICS-US, V34, P11
   Interrante V, 1997, IEEE T VIS COMPUT GR, V3, P98, DOI 10.1109/2945.597794
   Kajiya J. T., 1984, Computers & Graphics, V18, P165
   Kasrai R, 2001, J OPT SOC AM A, V18, P1, DOI 10.1364/JOSAA.18.000001
   KAUFMAN A, 2004, VISUALIZATION HDB, P127
   KERSTEN D, 1992, NEURAL COMPUT, V4, P573, DOI 10.1162/neco.1992.4.4.573
   Kersten MA, 2006, IEEE T VIS COMPUT GR, V12, P1117, DOI 10.1109/TVCG.2006.139
   LACROUTE P, 1994, SIGGRAPH 94, P451, DOI DOI 10.1145/192161.192283
   LEVOY M, 1988, IEEE COMPUT GRAPH, V8, P29, DOI 10.1109/38.511
   Masin SC, 2006, PERCEPTION, V35, P1611, DOI 10.1068/p5034
   MAX N, 1995, IEEE T VISUALIZATION, V1, P2
   METELLI F, 1974, SCI AM, V230, P91, DOI 10.1038/scientificamerican0474-90
   Mora B, 2004, COMPUT GRAPH FORUM, V23, P489, DOI 10.1111/j.1467-8659.2004.00780.x
   NAWROT M, 1989, SCIENCE, V244, P716, DOI 10.1126/science.2717948
   Perlin K., 1985, Computer Graphics, V19, P287, DOI 10.1145/325165.325247
   Petersik JT, 2004, SPATIAL VISION, V17, P201, DOI 10.1163/1568568041866015
   Pfister H, 2000, IEEE VISUAL, P523
   Purves D, 2004, PSYCHOL REV, V111, P142, DOI 10.1037/0033-295X.111.1.142
   ROGERS S, 1992, PERCEPT PSYCHOPHYS, V52, P446, DOI 10.3758/BF03206704
   ROPINSKI T, 2006, P 6 INT S SMART GRAP, P93
   SABELLA P, 1988, SIGGRAPH 88, P15
   Sereno ME, 1999, J EXP PSYCHOL HUMAN, V25, P1834
   Singh M, 2004, VISION RES, V44, P1827, DOI 10.1016/j.visres.2004.02.010
   Singh M, 2002, PSYCHOL REV, V109, P492, DOI 10.1037//0033-295X.109.3.492
   Singh M, 1998, PSYCHOL SCI, V9, P370, DOI 10.1111/1467-9280.00070
   SPERLING G, 1989, J EXP PSYCHOL HUMAN, V15, P826
   TODD JT, 1985, J EXP PSYCHOL HUMAN, V11, P689, DOI 10.1037/0096-1523.11.6.689
   ULLMAN S, 1979, THESIS MIT
   VANNES FL, 1967, J OPT SOC AM, V57, P401, DOI 10.1364/JOSA.57.000401
   WALLACH H, 1953, J EXP PSYCHOL, V45, P205, DOI 10.1037/h0056880
NR 40
TC 21
Z9 29
U1 0
U2 6
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD JAN
PY 2009
VL 5
IS 4
AR 23
DI 10.1145/1462048.1462054
PG 24
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 450YL
UT WOS:000266437800006
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Bicego, M
   Grosso, E
   Lagorio, A
   Brelstaff, G
   Brodo, L
   Tistarelli, M
AF Bicego, Manuele
   Grosso, Enrico
   Lagorio, Andrea
   Brelstaff, Gavin
   Brodo, Linda
   Tistarelli, Massimo
TI Distinctiveness of Faces: A Computational Approach
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Algorithms; Design; Human factors; Security; Perceptual models; Face
   recognition; log-polar representation; illumination changes; face
   authentication
ID VISUAL-ATTENTION; RECOGNITION; SKETCH; MODEL
AB This paper develops and demonstrates an original approach to face-image analysis based on identifying distinctive areas of each individual's face by its comparison to others in the population. The method differs from most others-that we refer as unary-where salient regions are defined by analyzing only images of the same individual. We extract a set of multiscale patches from each face image before projecting them into a common feature space. The degree of "distinctiveness" of any patch depends on its distance in feature space from patches mapped from other individuals. First a pairwise analysis is developed and then a simple generalization to the multiple-face case is proposed. A perceptual experiment, involving 45 observers, indicates the method to be fairly compatible with how humans mark faces as distinct. A quantitative example of face authentication is also performed in order to show the essential role played by the distinctive information. A comparative analysis shows that performance of our n-ary approach is as good as several contemporary unary, or binary, methods, while tapping a complementary source of information. Furthermore, we show it can also provide a useful degree of illumination invariance.
C1 [Bicego, Manuele; Grosso, Enrico; Lagorio, Andrea] Deir Univ Sassari, I-07100 Sassari, Italy.
   [Brelstaff, Gavin] Polaris, CRS4, Pula, Italy.
   [Brodo, Linda] Dsl Univ Sassari, Sassari, Italy.
   [Tistarelli, Massimo] Dap Univ Sassari, Sassari, Italy.
C3 University of Sassari; University of Sassari; University of Sassari
RP Bicego, M (corresponding author), Deir Univ Sassari, Via Torre Tonda 34, I-07100 Sassari, Italy.
EM bicego@uniss.it
RI Tistarelli, Massimo/AAH-9437-2021
OI Tistarelli, Massimo/0000-0002-3406-3048; LAGORIO,
   Andrea/0000-0001-9113-6103
CR Adini Y, 1997, IEEE T PATTERN ANAL, V19, P721, DOI 10.1109/34.598229
   Agarwal S, 2002, LECT NOTES COMPUT SC, V2353, P113
   Bailly-Bailliére E, 2003, LECT NOTES COMPUT SC, V2688, P625
   Belhumeur PN, 1997, IEEE T PATTERN ANAL, V19, P711, DOI 10.1109/34.598228
   Bengio S., 2004, SPEAKER LANG RECOGNI, P237
   Bicego M, 2005, LECT NOTES COMPUT SC, V3546, P329
   BRACCINI C, 1982, BIOL CYBERN, V44, P47, DOI 10.1007/BF00353955
   BRELSTAFF G, 1990, P BRIT MACH VIS C, P151
   Brelstaff GJ, 2006, PERCEPTION, V35, P209
   BRUCE V, 1994, Q J EXP PSYCHOL-A, V47, P119, DOI 10.1080/14640749408401146
   BRUCE V, 1987, APPL COGNITIVE PSYCH, V1, P109, DOI 10.1002/acp.2350010204
   Campadelli P, 2004, IMAGE VISION COMPUT, V22, P863, DOI 10.1016/j.imavis.2003.07.006
   Collishaw SM, 2000, PERCEPTION, V29, P893, DOI 10.1068/p2949
   CSURKA G, 2004, P WORKSH PATT REC MA
   Dorkó G, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P634
   Fergus R, 2003, PROC CVPR IEEE, P264
   Gauthier I, 1999, NAT NEUROSCI, V2, P568, DOI 10.1038/9224
   Gonzalez R. C., 2006, Digital Image Processing, V3rd
   González-Jiménez D, 2005, LECT NOTES COMPUT SC, V3523, P513
   GOREN CC, 1975, PEDIATRICS, V56, P544
   Gross R, 2003, LECT NOTES COMPUT SC, V2688, P10
   GROSSO E, 2000, P EUR C COMP VIS, V1, P299
   HAITH MM, 1977, SCIENCE, V198, P853, DOI 10.1126/science.918670
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Jojic N, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P34
   Kim J, 2005, IEEE T PATTERN ANAL, V27, P1977, DOI 10.1109/TPAMI.2005.242
   KIRBY M, 1990, IEEE T PATTERN ANAL, V12, P103, DOI 10.1109/34.41390
   KLIN A, 2001, M NICHD COLL PROGR E
   KOCH C, 1985, HUM NEUROBIOL, V4, P219
   Li SZ, 2001, PROC CVPR IEEE, P207
   LINDEBERG T, 1993, INT J COMPUT VISION, V11, P283, DOI 10.1007/BF01469346
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   MAIO D, 2003, HDB FINGERPRINT RECO
   Marr D., 1982, Vision
   MESSER K, 1997, P INT C AUD VID BAS
   MINGHSUAN Y, 2002, IEEE T PATTERN ANAL, V24, P3458
   Nahm FKD, 1997, J COGNITIVE NEUROSCI, V9, P611, DOI 10.1162/jocn.1997.9.5.611
   O'Toole A.J., 2005, HDB FACE RECOGNITION
   O'Toole AJ, 2005, J COGNITIVE NEUROSCI, V17, P580, DOI 10.1162/0898929053467550
   OTOOLE A, 2006, FACE PROCES IN PRESS
   Penev PS, 1996, NETWORK-COMP NEURAL, V7, P477, DOI 10.1088/0954-898X/7/3/002
   Perlibakas V, 2004, PATTERN RECOGN LETT, V25, P711, DOI 10.1016/j.patrec.2004.01.011
   Phillips PJ, 2005, PROC CVPR IEEE, P947
   Salah AA, 2002, IEEE T PATTERN ANAL, V24, P420, DOI 10.1109/34.990146
   Senior Andrew W., 1999, P INT C AUD VID BAS, P154
   TISTARELLI M, 1993, IEEE T PATTERN ANAL, V15, P401, DOI 10.1109/34.206959
   TSOTSOS JK, 1995, ARTIF INTELL, V78, P507, DOI 10.1016/0004-3702(95)00025-9
   TURK M, 1991, J COGNITIVE NEUROSCI, V3, P71, DOI 10.1162/jocn.1991.3.1.71
   Ullman S, 2002, NAT NEUROSCI, V5, P682, DOI 10.1038/nn870
   WALTHER D, 2006, THESIS PASADENA
   Wiskott L, 1997, IEEE T PATTERN ANAL, V19, P775, DOI 10.1109/34.598235
   Yarbus A. L., 1967, Eye Movements and Vision
   Zhao W, 2003, ACM COMPUT SURV, V35, P399, DOI 10.1145/954339.954342
NR 53
TC 13
Z9 13
U1 0
U2 2
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD MAY
PY 2008
VL 5
IS 2
AR 11
DI 10.1145/1279920.1279925
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 450YJ
UT WOS:000266437600005
DA 2024-07-18
ER

PT J
AU Frenz, H
   Lappe, M
   Kolesnik, M
   Bührmann, T
AF Frenz, Harald
   Lappe, Markus
   Kolesnik, Marina
   Buehrmann, Thomas
TI Estimation of Travel Distance from Visual Motion in Virtual Environments
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Human factors; Experimentation; Optic flow; distance estimation; stereo;
   depth; virtual reality
ID OPTIC FLOW; PATH INTEGRATION; PERCEPTION; DEPTH; DISCRIMINATION;
   LOCOMOTION; VISION; HUMANS; FIELD; CUES
AB Distance estimation of visually simulated self-motion is difficult, because one has to know or make assumptions about scene layout to judge ego speed. Discrimination of the travel distances of two sequentially simulated self-motions in the same scene can be performed quite accurately (Bremmer and Lappe 1999; Frenz et al., 2003). However, the indication of the perceived distance of a single movement in terms of a spatial interval results in a depth scaling error: Intervals are correlated with the true travel distance, but underestimate travel distance by about 25% (Frenz and Lappe, 2005). Here we investigated whether the inclusion of further depth cues (disparity/motion parallax/figural cues) in the virtual environment allows more veridical interval adjustment. Experiments were conducted on a large single projection screen and in a fully immersive computer-animated virtual environment (CAVE). Forward movements in simple virtual environments were simulated with distances between 1.5 and 13 m with varying speeds. Subjects indicated the perceived distance of each movement in terms of a depth interval on the virtual ground plane. We found good correlation between simulated and indicated distances, indicative of an internal representation of the perceived distance. The slopes of the fitted regression lines revealed an underestimation of distance by about 25% under all conditions. We conclude that estimation of travel distance from optic flow is subject to scaling when compared to static intervals in the environment, irrespective of additional depth cues.
C1 [Frenz, Harald; Lappe, Markus; Buehrmann, Thomas] Univ Munster, Psychol Inst 2, Munster, Germany.
   [Frenz, Harald; Lappe, Markus] Ruhr Univ Bochum, Dept Zool & Neurobiol, Bochum, Germany.
   [Kolesnik, Marina] Fraunhofer Inst Media Commun IMK, St Augustin, Germany.
C3 University of Munster; Ruhr University Bochum; Fraunhofer Gesellschaft
RP Frenz, H (corresponding author), Univ Munster, Psychol Inst 2, Munster, Germany.
EM mlappe@psy.uni-muenster.de
OI Buhrmann, Thomas/0000-0002-8617-9608
FU German Science Foundation; German Federal Ministry of Education and
   Research; EC
FX ML is supported by the German Science Foundation, the German Federal
   Ministry of Education and Research BioFuture Prize and the EC Project
   Drivsco.
CR [Anonymous], 2005, ACM Transactions on Applied Perception, DOI [DOI 10.1145/1077399.1077403, DOI 10.1145/1077399.10774032,3,9]
   BERTHOZ A, 1995, SCIENCE, V269, P95, DOI 10.1126/science.7604286
   Beusmans JMH, 1998, VISION RES, V38, P1153, DOI 10.1016/S0042-6989(97)00285-X
   Bremmer F, 1999, EXP BRAIN RES, V127, P33, DOI 10.1007/s002210050771
   Bronstein AM, 1997, EXP BRAIN RES, V113, P243, DOI 10.1007/BF02450322
   Cuijpers RH, 2002, PERCEPT PSYCHOPHYS, V64, P392, DOI 10.3758/BF03194712
   Cuijpers RH, 2000, PERCEPTION, V29, P1467, DOI 10.1068/p3041
   Durgin FH, 2005, J EXP PSYCHOL HUMAN, V31, P398, DOI 10.1037/0096-1523.31.3.398
   Durgin FH, 1999, EXP BRAIN RES, V127, P12, DOI 10.1007/s002210050769
   Foley JM, 2004, VISION RES, V44, P147, DOI 10.1016/j.visres.2003.09.004
   FOLEY JM, 1980, PSYCHOL REV, V87, P411, DOI 10.1037/0033-295X.87.5.411
   Frenz H, 2005, VISION RES, V45, P1679, DOI 10.1016/j.visres.2004.12.019
   Frenz H, 2003, VISION RES, V43, P2173, DOI 10.1016/S0042-6989(03)00337-7
   Gibson J.J., 1950, PERCEPTION VISUAL WO
   Grigo A, 1998, VISION RES, V38, P281, DOI 10.1016/S0042-6989(97)00123-5
   Harris LR, 2000, EXP BRAIN RES, V135, P12, DOI 10.1007/s002210000504
   INDOW T, 1991, PSYCHOL REV, V98, P430, DOI 10.1037/0033-295X.98.3.430
   Kearns MJ, 2002, PERCEPTION, V31, P349, DOI 10.1068/p3311
   LEE DN, 1980, PHILOS T R SOC B, V290, P169, DOI 10.1098/rstb.1980.0089
   Li L, 2004, VISION RES, V44, P1879, DOI 10.1016/j.visres.2004.03.008
   LOOMIS JM, 1993, J EXP PSYCHOL GEN, V122, P73, DOI 10.1037/0096-3445.122.1.73
   Palmisano S, 2002, PERCEPTION, V31, P463, DOI 10.1068/p3321
   Pelah A, 1996, NATURE, V381, P283, DOI 10.1038/381283a0
   Peruch P, 1997, PERCEPTION, V26, P301, DOI 10.1068/p260301
   Prokop T, 1997, EXP BRAIN RES, V114, P63, DOI 10.1007/PL00005624
   Redlick FP, 2001, VISION RES, V41, P213, DOI 10.1016/S0042-6989(00)00243-1
   Riecke BE, 2002, PRESENCE-VIRTUAL AUG, V11, P443, DOI 10.1162/105474602320935810
   Rushton SK, 1999, PERCEPTION, V28, P255, DOI 10.1068/p2780
   Sun HJ, 2004, EXP BRAIN RES, V154, P246, DOI 10.1007/s00221-003-1652-9
   Thompson WB, 2005, PROC SPIE, V5666, P481, DOI 10.1117/12.610861
   VANDENBERG AV, 1994, NATURE, V371, P700, DOI 10.1038/371700a0
   VANDENBERG AV, 1994, VISION RES, V34, P2153, DOI 10.1016/0042-6989(94)90324-7
   WAGNER M, 1985, PERCEPT PSYCHOPHYS, V38, P483, DOI 10.3758/BF03207058
   WARREN WH, 1988, J EXP PSYCHOL HUMAN, V14, P646, DOI 10.1037/0096-1523.14.4.646
   WARREN WH, 1990, J OPT SOC AM A, V7, P160, DOI 10.1364/JOSAA.7.000160
   Witmer BG, 1998, PRESENCE-TELEOP VIRT, V7, P144, DOI 10.1162/105474698565640
NR 36
TC 33
Z9 35
U1 1
U2 10
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD JAN
PY 2007
VL 4
IS 1
AR 3
DI 10.1145/1227134.1227137
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA V04IL
UT WOS:000207052000003
DA 2024-07-18
ER

PT J
AU Blissing, B
   Bruzelius, F
   Eriksson, O
AF Blissing, Bjorn
   Bruzelius, Fredrik
   Eriksson, Olle
TI The Effects on Driving Behavior When Using a Head-mounted Display in a
   Dynamic Driving Simulator
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Head-mounted display; driving simulator; driver behavior
AB Driving simulators are established tools used during automotive development and research. Most simulators use either monitors or projectors as their primary display system. However, the emergence of a newgeneration of head-mounted displays has triggered interest in using these as the primary display type. The general benefits and drawbacks of head-mounted displays are well researched, but their effect on driving behavior in a simulator has not been sufficiently quantified.
   This article presents a study of driving behavior differences between projector-based graphics and head-mounted display in a large dynamic driving simulator. This study has selected five specific driving maneuvers suspected of affecting driving behavior differently depending on the choice of display technology. Some of these maneuvers were chosen to reveal changes in lateral and longitudinal driving behavior. Others were picked for their ability to highlight the benefits and drawbacks of head-mounted displays in a driving context.
   The results show minor changes in lateral and longitudinal driver behavior changes when comparing projectors and a head-mounted display. The most noticeable difference in favor of projectors was seen when the display resolution is critical to the driving task. The choice of display type did not affect simulator sickness nor the realism rated by the subjects.
C1 [Blissing, Bjorn; Bruzelius, Fredrik; Eriksson, Olle] Swedish Natl Rd & Transport Res Inst VTI, SE-58195 Linkoping, Sweden.
C3 VTI
RP Blissing, B (corresponding author), Swedish Natl Rd & Transport Res Inst VTI, SE-58195 Linkoping, Sweden.
EM bjorn.blissing@vti.se; fredrik.bruzelius@vti.se; olle.eriksson@vti.se
OI Blissing, Bjorn/0000-0001-5057-4043
FU Vinnova/FFI project Chronos 2 [2017-05501]
FX This work was supported by the Vinnova/FFI project Chronos 2, dnr.
   2017-05501.
CR [Anonymous], 1988, ITALIAN J OPHTHALMOL, VI, P1
   Aykent Baris, 2015, P DRIVING SIMULATION, P235
   Benz TM, 2019, AUTOMOTIVEUI'19: PROCEEDINGS OF THE 11TH ACM INTERNATIONAL CONFERENCE ON AUTOMOTIVE USER INTERFACES AND INTERACTIVE VEHICULAR APPLICATIONS, P379, DOI 10.1145/3342197.3344515
   Berg LP, 2017, VIRTUAL REAL-LONDON, V21, P1, DOI 10.1007/s10055-016-0293-9
   Blissing B., 2018, Driving Simulation Association (Hrsg.): Proceedings of the Driving Simulation Conference. Antibes. S, P163
   Burns Peter C., 1999, P DRIVING SIMULATION, P153
   Carlozzi NE, 2013, DISABIL REHABIL-ASSI, V8, P176, DOI 10.3109/17483107.2012.699990
   Coates Nick, 2002, P DRIVING SIMULATION, P33
   Colombet F., 2016, Proceedings of the Driving Simulation Conference, P201
   Fischer M., 2012, Driving Simulation Conference, P16
   Foxlin E, 2000, P SOC PHOTO-OPT INS, V4021, P133, DOI 10.1117/12.389141
   FRANK LH, 1988, HUM FACTORS, V30, P201, DOI 10.1177/001872088803000207
   Glaser Sebastien, 2013, FUTURE ACTIVE SAFETY
   Grabe V., 2010, P DRIVING SIMULATION, P81
   Harris L. R., 2002, Virtual Reality, V6, P75, DOI 10.1007/s100550200008
   Hartfiel B, 2019, P DSC 2019 VR DSA ST, P25
   Jerald J, 2012, ACM T APPL PERCEPT, V9, DOI 10.1145/2134203.2134207
   Kemeny Andras., 2014, Proceedings of the 2014 Virtual Reality International Conference (VRIC'14), P1
   Kennedy RS, 1993, The international journal of aviation psychology, V3, P203, DOI [DOI 10.1207/S15327108IJAP0303, 10.1207/s15327108ijap0303_3]
   Koulieris GA, 2019, COMPUT GRAPH FORUM, V38, P493, DOI 10.1111/cgf.13654
   Koulieris GA, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073622
   Kreylos Oliver, 2018, DISPLAY RESOLUTION H
   Lassagne Antoine, 2017, P DRIVING SIMULATION, P163
   Mareck Sebastian, 2020, 8 VIRT DIM CTR, DOI [10.6084/m9.figshare.13200149, DOI 10.6084/M9.FIGSHARE.13200149]
   Mareck Sebastian, 2020, 10 VIRT DIM CTR, DOI [10.6084/m9.figshare.13275062, DOI 10.6084/M9.FIGSHARE.13275062]
   Nordmark Staffan, 2004, P DRIVING SIMULATION, P45
   Panerai F., 2001, P DRIVING SIMULATION, P91
   Patterson R, 2006, HUM FACTORS, V48, P555, DOI 10.1518/001872006778606877
   Reason JT., 1975, INT J MAN MACH STUD
   Renner RS, 2013, ACM COMPUT SURV, V46, DOI 10.1145/2543581.2543590
   Runde Christoph, 2020, 12 VIRT DIM CTR, DOI [10.6084/m9.figshare.13352699, DOI 10.6084/M9.FIGSHARE.13352699]
   Schill Volkhard, 2019, PRODUCT SOLUTIONS BO, P67
   Schill Volkhard, 1997, P DRIVING SIMULATION, P35
   The Commission of the European Communities, 2013, OFFICIAL J EUROPEAN
   van Veen HAHC, 1998, FUTURE GENER COMP SY, V14, P231, DOI 10.1016/S0167-739X(98)00027-2
   Weidner F, 2017, P IEEE VIRT REAL ANN, P281, DOI 10.1109/VR.2017.7892286
   Zoller Chris, 2019, P DRIVING SIMULATION, P9
NR 37
TC 4
Z9 4
U1 1
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD JAN
PY 2022
VL 19
IS 1
AR 4
DI 10.1145/3483793
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 9R2WY
UT WOS:000945516600004
DA 2024-07-18
ER

PT J
AU Erickson, A
   Kim, K
   Lambert, A
   Bruder, G
   Browne, MP
   Welch, GF
AF Erickson, Austin
   Kim, Kangsoo
   Lambert, Alexis
   Bruder, Gerd
   Browne, Michael P.
   Welch, Gregory F.
TI An Extended Analysis on the Benefits of Dark Mode User Interfaces in
   Optical See-Through Head-Mounted Displays
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Augmented reality; optical see-through display; dark mode; light mode;
   visual acuity; user experience; vergence accommodation conflict
ID BACKGROUND POLARITY; PUPIL SIZE; COLOR; PERFORMANCE; TEXT; INFORMATION;
   READABILITY; PERCEPTION; DESIGN; REAL
AB Light-on-dark color schemes, so-called "Dark Mode," are becoming more and more popular over a wide range of display technologies and application fields. Many people who have to look at computer screens for hours at a time, such as computer programmers and computer graphics artists, indicate a preference for switching colors on a computer screen from dark text on a light background to light text on a dark background due to perceived advantages related to visual comfort and acuity, specifically when working in low-light environments.
   In this article, we investigate the effects of dark mode color schemes in the field of optical see-through head-mounted displays (OST-HMDs), where the characteristic "additive" light model implies that bright graphics are visible but dark graphics are transparent. We describe two human-subject studies in which we evaluated a normal and inverted color mode in front of different physical backgrounds and different lighting conditions. Our results indicate that dark mode graphics displayed on the HoloLens have significant benefits for visual acuity and usability, while user preferences depend largely on the lighting in the physical environment. We discuss the implications of these effects on user interfaces and applications.
C1 [Erickson, Austin; Kim, Kangsoo; Lambert, Alexis; Bruder, Gerd; Welch, Gregory F.] Univ Cent Florida, 4000 Cent Florida Blvd, Orlando, FL 32816 USA.
   [Browne, Michael P.] SA Photon, 120 Knowles Dr, Los Gatos, CA 95032 USA.
C3 State University System of Florida; University of Central Florida
RP Erickson, A (corresponding author), Univ Cent Florida, 4000 Cent Florida Blvd, Orlando, FL 32816 USA.
EM ericksona@knights.ucf.edu; kangsoo.kim@ucf.edu;
   lamberta2@knights.ucf.edu; bruder@ucf.edu; m.browne@saphotonics.com;
   welch@ucf.edu
RI Erickson, Austin/AAV-9677-2020; Kim, Kangsoo/AAL-9592-2020
OI Kim, Kangsoo/0000-0002-0925-378X
FU Office of Naval Research [N00014-17-1-2927, N00014-18-12964]; National
   Science Foundation [1800961, 1800947, 1800922]; Advent Health Endowed
   Chair in Healthcare Simulation
FX This material includes work supported in part by the Office of Naval
   Research under Award Numbers N00014-17-1-2927 and N00014-18-12964 (Dr.
   Peter Squire, Code 34); the National Science Foundation under
   Collaborative Award Numbers 1800961, 1800947, and 1800922 (Dr. Ephraim
   P. Glinert, IIS) to the University of Central Florida, University of
   Florida, and Stanford University, respectively; the and the Advent
   Health Endowed Chair in Healthcare Simulation (Prof. Welch). Any
   opinions, findings, and conclusions or recommendations expressed in this
   material are those of the author(s) and do not necessarily reflect the
   views of the supporting institutions.
CR [Anonymous], 1990, ERGONOMIC ASPECTS VI
   [Anonymous], 1927, STUDIES OPTICS
   Banks MS, 2013, PROC SPIE, V8735, DOI 10.1117/12.2019866
   Blehm C, 2005, SURV OPHTHALMOL, V50, P253, DOI 10.1016/j.survophthal.2005.02.008
   Bruder G, 2015, P IEEE VIRT REAL ANN, P27, DOI 10.1109/VR.2015.7223320
   Buchner A, 2007, ERGONOMICS, V50, P1036, DOI 10.1080/00140130701306413
   Buchner A, 2009, ERGONOMICS, V52, P882, DOI 10.1080/00140130802641635
   Cadena C, 2016, IEEE T ROBOT, V32, P1309, DOI 10.1109/TRO.2016.2624754
   Cajochen C, 2011, J APPL PHYSIOL, V110, P1432, DOI 10.1152/japplphysiol.00165.2011
   CAMPBELL FW, 1983, OPHTHAL PHYSL OPT, V3, P175, DOI 10.1111/j.1475-1313.1983.tb00596.x
   CAMPBELL FW, 1960, NATURE, V187, P1121, DOI 10.1038/1871121c0
   Chan AHS, 2005, BEHAV INFORM TECHNOL, V24, P81, DOI 10.1080/0144929042000267073
   Cohen J., 1988, STAT POWER ANAL BEHA
   Debernardis S, 2014, IEEE T VIS COMPUT GR, V20, P125, DOI 10.1109/TVCG.2013.86
   Erickson A, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR 2020), P493, DOI [10.1109/VR46266.2020.00-34, 10.1109/VR46266.2020.00070]
   Erickson A, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR 2020), P434, DOI [10.1109/VR46266.2020.1580695145399, 10.1109/VR46266.2020.00-40]
   Erickson A, 2020, IEEE T VIS COMPUT GR, V26, P1934, DOI 10.1109/TVCG.2020.2973054
   Ericson A., 2020, 3 INT S SMALL SCALE, P1, DOI [DOI 10.1109/SIMS49386.2020, 10.1145/3385959.3418445, DOI 10.1145/3385959.3418445]
   Fiorentino M, 2013, PRESENCE-TELEOP VIRT, V22, P171, DOI 10.1162/PRES_a_00146
   Gabbard JL, 2007, IEEE VIRTUAL REALITY 2007, PROCEEDINGS, P35
   Gabbard JL, 2019, IEEE T VIS COMPUT GR, V25, P2228, DOI 10.1109/TVCG.2018.2832633
   Gabbard JL, 2010, P IEEE VIRT REAL ANN, P79, DOI 10.1109/VR.2010.5444808
   Gattullo M, 2015, IEEE T VIS COMPUT GR, V21, P638, DOI 10.1109/TVCG.2014.2385056
   Harris J., 2014, SENSATION PERCEPTION
   Higuchi S, 2003, J APPL PHYSIOL, V94, P1773, DOI 10.1152/japplphysiol.00616.2002
   Holladay JT, 2004, J CATARACT REFR SURG, V30, P287, DOI 10.1016/j.jcrs.2004.01.014
   Huang W., 2012, Human factors in augmented reality environments
   Inoue T, 1997, APPL OPTICS, V36, P4509, DOI 10.1364/AO.36.004509
   International Organization for Standardization (ISO), 85971994 8596 2009 O
   ISO, 1994, 572521994 ISO
   Kim K, 2019, ACM CONFERENCE ON SPATIAL USER INTERACTION (SUI 2019), DOI 10.1145/3357251.3357584
   Kim K, 2018, IEEE T VIS COMPUT GR, V24, P2947, DOI 10.1109/TVCG.2018.2868591
   Kiyokawa K, 2003, SECOND IEEE AND ACM INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY, PROCEEDINGS, P133, DOI 10.1109/ISMAR.2003.1240696
   KNAPP TR, 1990, NURS RES, V39, P121
   Kooi FL, 2004, DISPLAYS, V25, P99, DOI 10.1016/j.displa.2004.07.004
   Krösl K, 2018, VISUAL COMPUT, V34, P911, DOI 10.1007/s00371-018-1517-7
   Kuzon WM, 1996, ANN PLAS SURG, V37, P265, DOI 10.1097/00000637-199609000-00006
   Lambooij M, 2010, J SOC INF DISPLAY, V18, P931, DOI 10.1889/JSID18.11.931
   Lee Y.-H., 2019, Virtual Reality Intelligent Hardware, V1, P10, DOI [10.3724/SP.J.2096-5796.2018.0009, DOI 10.3724/SP.J.2096-5796.2018.0009]
   Löffler D, 2017, LECT NOTES COMPUT SC, V10514, P184, DOI 10.1007/978-3-319-67684-5_12
   MacDonald LW, 1999, IEEE COMPUT GRAPH, V19, P20, DOI 10.1109/38.773961
   Mayr S, 2010, ERGONOMICS, V53, P914, DOI 10.1080/00140139.2010.484508
   Merenda Coleman, 2016, 2016 IEEE VR 2016 Workshop on Perceptual and Cognitive Issues in AR (PERCAR), P1, DOI 10.1109/PERCAR.2016.7562419
   Norouzi N, 2019, ACM CONFERENCE ON SPATIAL USER INTERACTION (SUI 2019), DOI 10.1145/3357251.3357587
   Peillard E, 2020, INT SYM MIX AUGMENT, P80, DOI 10.1109/ISMAR50242.2020.00028
   Piepenbrock C, 2014, ERGONOMICS, V57, P1670, DOI 10.1080/00140139.2014.948496
   Piepenbrock C, 2014, HUM FACTORS, V56, P942, DOI 10.1177/0018720813515509
   Piepenbrock C, 2013, ERGONOMICS, V56, P1116, DOI 10.1080/00140139.2013.790485
   Schiffman RM, 2000, ARCH OPHTHALMOL-CHIC, V118, P615, DOI 10.1001/archopht.118.5.615
   Schmidt S., 2017, ICAT EGVE, P161
   Schrepp M, 2017, INT J INTERACT MULTI, V4, P103, DOI 10.9781/ijimai.2017.09.001
   Soares S., 2019, OPTICAL SCATTER TECH, P1
   Sridharan SrikanthKirshnamachari., 2013, Proceedings of the 19th ACM Symposium on Virtual Reality Software and Technology, P231, DOI [10.1145/2503713.2503716, DOI 10.1145/2503713.2503716]
   Swan JE, 2015, IEEE T VIS COMPUT GR, V21, P1289, DOI 10.1109/TVCG.2015.2459895
   TAPTAGAPORN S, 1990, ERGONOMICS, V33, P201, DOI 10.1080/00140139008927110
   Walker H.K., 1990, CLIN METHODS
   Wang AH, 2003, INT J IND ERGONOM, V32, P93, DOI 10.1016/S0169-8141(03)00041-6
   Weiland C, 2009, LECT NOTES COMPUT SC, V5615, P603, DOI 10.1007/978-3-642-02710-9_67
   Welch G., 2019, ANT WID AUGM REAL IN
   Zhao YH, 2017, PROCEEDINGS OF THE 2017 ACM SIGCHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'17), P4170, DOI 10.1145/3025453.3025949
   Zhao YH, 2015, ASSETS'15: PROCEEDINGS OF THE 17TH INTERNATIONAL ACM SIGACCESS CONFERENCE ON COMPUTERS & ACCESSIBILITY, P239, DOI 10.1145/2700648.2809865
NR 61
TC 10
Z9 10
U1 4
U2 27
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD JUL
PY 2021
VL 18
IS 3
AR 12
DI 10.1145/3456874
PG 22
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA UD5LZ
UT WOS:000687249300003
OA Bronze
DA 2024-07-18
ER

PT J
AU Knopp, B
   Velychko, D
   Dreibrodt, J
   Endres, D
AF Knopp, Benjamin
   Velychko, Dmytro
   Dreibrodt, Johannes
   Endres, Dominik
TI Predicting Perceived Naturalness of Human Animations Based on Generative
   Movement Primitive Models
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article; Proceedings Paper
CT 16th Symposium on Applied Perception (SAP)
CY SEP, 2019
CL Barcelona, SPAIN
DE Human animation; movement primitives; perception; dynamical systems;
   psychophysics; Gaussian process dynamical model; dynamical movement
   primitives
ID BIOLOGICAL MOTION; PERCEPTION; FRAMEWORK; CONSTRUCTION; RECOGNITION
AB We compared the perceptual validity of human avatar walking animations driven by six different representations of human movement using a graphics Turing test. All six representations are based on movement primitives (MPs), which are predictive models of full-body movement that differ in their complexity and prediction mechanism. Assuming that humans are experts at perceiving biological movement from noisy sensory signals, it follows that these percepts should be describable by a suitably constructed Bayesian ideal observer model. We build such models from MPs and investigate if the perceived naturalness of human animations are predictable from approximate Bayesian model scores of the MPs. We found that certain MP-based representations are capable of producing movements that are perceptually indistinguishable from natural movements. Furthermore, approximate Bayesian model scores of these representations can be used to predict perceived naturalness. In particular, we could show that movement dynamics are more important for perceived naturalness of human animations than single frame poses. This indicates that perception of human animations is highly sensitive to their temporal coherence. More generally, our results add evidence for a shared MP-representation of action and perception. Even though the motivation of our work is primarily drawn from neuroscience, we expect that our results will be applicable in virtual and augmented reality settings, when perceptually plausible human avatar movements are required.
C1 [Knopp, Benjamin; Velychko, Dmytro; Dreibrodt, Johannes; Endres, Dominik] Univ Marburg, Dept Psychol, Gutenbergstr 18, D-35039 Marburg, Germany.
C3 Philipps University Marburg
RP Knopp, B (corresponding author), Univ Marburg, Dept Psychol, Gutenbergstr 18, D-35039 Marburg, Germany.
EM benjamin.knopp@uni-marburg.de; dmytro.velychko@uni-marburg.de;
   dreibrod@students.uni-marburg.de; dominik.endres@uni-marburg.de
OI Knopp, Benjamin/0000-0002-1394-5228
FU DFG [IRTG1901, SFB-TRR 135]
FX This work was funded by DFG, IRTG1901 - The brain in action, and SFB-TRR
   135 - Cardinal mechanisms of perception.
CR Beintema JA, 2002, P NATL ACAD SCI USA, V99, P5661, DOI 10.1073/pnas.082483699
   Bernshtein N.A., 1967, The co-ordination and regulation of movements
   BERTENTHAL BI, 1994, PSYCHOL SCI, V5, P221, DOI 10.1111/j.1467-9280.1994.tb00504.x
   Casile A, 2005, J VISION, V5, P348, DOI 10.1167/5.4.6
   Chiovetto E, 2018, J VISION, V18, DOI 10.1167/18.4.13
   Clever D, 2017, IEEE ROBOT AUTOM LET, V2, P977, DOI 10.1109/LRA.2017.2657000
   Clever D, 2016, ROBOT AUTON SYST, V83, P287, DOI 10.1016/j.robot.2016.06.001
   d'Avella A, 2003, NAT NEUROSCI, V6, P300, DOI 10.1038/nn1010
   Dayan E, 2007, P NATL ACAD SCI USA, V104, P20582, DOI 10.1073/pnas.0710033104
   Endres D, 2011, ACM T APPL PERCEPT, V8, DOI 10.1145/2010325.2010326
   Endres DM, 2013, FRONT COMPUT NEUROSC, V7, P1, DOI 10.3389/fncom.2013.00185
   Friston KJ, 2010, NAT REV NEUROSCI, V11, P127, DOI 10.1038/nrn2787
   Giese MA, 2000, INT J COMPUT VISION, V38, P59, DOI 10.1023/A:1008118801668
   Giese MA, 2003, NAT REV NEUROSCI, V4, P179, DOI 10.1038/nrn1057
   Giese Martin A., 2014, OXFORD HDB PERCEPTUA, DOI [10.1093/oxfordhb/9780199686858.013.008, DOI 10.1093/OXFORDHB/9780199686858.013.008]
   GISZTER SF, 1992, ANALYSIS AND MODELING OF NEURAL SYSTEMS, P377
   Giszter SF, 2015, CURR OPIN NEUROBIOL, V33, P156, DOI 10.1016/j.conb.2015.04.004
   Hodgins JK, 1998, IEEE T VIS COMPUT GR, V4, P307, DOI 10.1109/2945.765325
   Hommel B, 2001, BEHAV BRAIN SCI, V24, P849, DOI 10.1017/S0140525X01000103
   Ijspeert AJ, 2013, NEURAL COMPUT, V25, P328, DOI 10.1162/NECO_a_00393
   Ivanenko YP, 2004, J PHYSIOL-LONDON, V556, P267, DOI 10.1113/jphysiol.2003.057174
   JOHANSSON G, 1994, PERCEIVING EVENTS AND OBJECTS, P185
   Jones E., 2001, SciPy: Open source scientific tools for Python
   Knill David C., 2004, TRENDS NEUROSCIENCE, V27
   McGuigan Michael D., 2006, ABSCS0603132 CORR
   Omlor L, 2011, J MACH LEARN RES, V12, P1111
   Peirce JW, 2009, FRONT NEUROINFORM, V2, DOI 10.3389/neuro.11.010.2008
   Polyakov F, 2009, BIOL CYBERN, V100, P159, DOI 10.1007/s00422-008-0287-0
   Prinz W, 1997, EUR J COGN PSYCHOL, V9, P129, DOI 10.1080/713752551
   Roether CL, 2009, J VISION, V9, DOI 10.1167/9.6.15
   Schaal S, 2006, ADAPTIVE MOTION OF ANIMALS AND MACHINES, P261, DOI 10.1007/4-431-31381-8_23
   Schaal S, 1999, TRENDS COGN SCI, V3, P233, DOI 10.1016/S1364-6613(99)01327-3
   Shenoy KV, 2013, ANNU REV NEUROSCI, V36, P337, DOI 10.1146/annurev-neuro-062111-150509
   Shin YK, 2010, PSYCHOL BULL, V136, P943, DOI 10.1037/a0020541
   Sussillo D, 2015, NAT NEUROSCI, V18, P1025, DOI 10.1038/nn.4042
   Taubert N., 2012, Proceedings of the acm symposium on applied perception, P25, DOI DOI 10.1145/2338676.2338682
   Todorov E., 2003, Advances in Neural Information Processing Systems, V15, P27
   Tresch MC, 1999, NAT NEUROSCI, V2, P162, DOI 10.1038/5721
   Troje NF, 2005, PERCEPT PSYCHOPHYS, V67, P667, DOI 10.3758/BF03193523
   Troje NF, 2002, J VISION, V2, P371, DOI 10.1167/2.5.2
   Troje NF, 2013, SOCIAL PERCEPTION: DETECTION AND INTERPRETATION OF ANIMACY, AGENCY, AND INTENTION, P13
   Velychko Dmytro, 2014, Artificial Neural Networks and Machine Learning - ICANN 2014. 24th International Conference on Artificial Neural Networks. Proceedings: LNCS 8681, P603, DOI 10.1007/978-3-319-11179-7_76
   Velychko D, 2018, ENTROPY-SWITZ, V20, DOI 10.3390/e20100724
   Wang JM, 2008, IEEE T PATTERN ANAL, V30, P283, DOI 10.1109/TPAMI.2007.1167
   Wolpert DM, 2003, PHILOS T R SOC B, V358, P593, DOI 10.1098/rstb.2002.1238
NR 45
TC 3
Z9 3
U1 1
U2 8
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD SEP
PY 2019
VL 16
IS 3
SI SI
AR 15
DI 10.1145/3355401
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA JA3KH
UT WOS:000487720000003
OA Bronze
DA 2024-07-18
ER

PT J
AU Kawabe, T
AF Kawabe, Takahiro
TI Shadow-based Illusion of Depth and Transparency in Printed Images
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Shadow; depth; light projection; transparency
ID CAST SHADOWS; PERCEPTION; MOTION
AB A cast shadow is one of the visual features that serve as a perceptual cue to the three-dimensional (3D) layout of objects. Although it is well known that adding cast shadows to an object produces the illusion that the object has a 3D layout, investigations into this illusion have been limited to virtual objects in a display. Using a light-projection technique, we show that it is possible to create a similar 3D layout illusion for real two-dimensional objects. Specifically, we displayed spatial patterns that look like cast shadows in the vicinity of an object depicted as a printed image. The combination of the cast shadow patterns with the printed object made it appear as if the printed object hovered over its original location even though the object was physically two-dimensional. By using this technique, we demonstrated that the shadow-induced layout illusion resulted in printed images having novel perceptual transparency. Vision researchers may find our technique useful if they want to extend their studies on the perception of cast shadows and transparency with real objects.
C1 [Kawabe, Takahiro] NTT Corp, NTT Commun Sci Labs, 3-1 Morinosato Wakamiya, Atsugi, Kanagawa 2430198, Japan.
C3 Nippon Telegraph & Telephone Corporation
RP Kawabe, T (corresponding author), NTT Corp, NTT Commun Sci Labs, 3-1 Morinosato Wakamiya, Atsugi, Kanagawa 2430198, Japan.
EM takkawabe@gmail.com
RI Kawabe, Takahiro/V-3676-2017
OI Kawabe, Takahiro/0000-0002-9888-8866
CR Adelson E., 1990, P NATL C ARTIFICAL I, P77
   Adelson E.H., 1991, Computational Models of Visual Processing, P3
   Aliaga Daniel G., 2008, P ACM SIGGRAPH AS 20, DOI 10.1145/1457515.1409102
   Amano T, 2013, IEEE COMPUT SOC CONF, P918, DOI 10.1109/CVPRW.2013.135
   [Anonymous], 2005, Spatial Augmented Reality: Merging Real and Virtual Worlds
   Audet S., 2007, COMPUTER VISION PATT, P1, DOI DOI 10.1109/CVPR.2007.383470
   Bimber O, 2008, COMPUT GRAPH FORUM, V27, P2219, DOI 10.1111/j.1467-8659.2008.01175.x
   Casati R, 2008, PERCEPTION, V37, P495, DOI 10.1068/p5588
   Dee HM, 2011, SPAT COGN COMPUT, V11, P226, DOI 10.1080/13875868.2011.565396
   Fleming R. W., 2005, ACM Transactions on Applied Perception (TAP), V2, P346, DOI DOI 10.1145/1077399.1077409
   Fukiage T, 2017, J SOC INF DISPLAY, V25, P434, DOI 10.1002/jsid.572
   Grundhöfer A, 2013, IEEE COMPUT SOC CONF, P924, DOI 10.1109/CVPRW.2013.136
   Imura T, 2008, J VISION, V8, DOI 10.1167/8.13.10
   Kawabe T, 2016, ACM T APPL PERCEPT, V13, DOI 10.1145/2874358
   Kersten D, 1996, NATURE, V379, P31, DOI 10.1038/379031a0
   Kersten D, 1997, PERCEPTION, V26, P171, DOI 10.1068/p260171
   Kersten D., 2017, OXFORD COMPENDIUM VI
   Khuu SK, 2014, J VISION, V14, DOI 10.1167/14.3.25
   Law AJ, 2011, COMPUT GRAPH FORUM, V30, P2288, DOI 10.1111/j.1467-8659.2011.02035.x
   Lovell PG, 2009, J VISION, V9, DOI 10.1167/9.1.37
   Madison C, 2001, PERCEPT PSYCHOPHYS, V63, P187, DOI 10.3758/BF03194461
   Mamassian P, 2004, PERCEPTION, V33, P1279, DOI 10.1068/p5280
   Mamassian P, 1998, TRENDS COGN SCI, V2, P288, DOI 10.1016/S1364-6613(98)01204-2
   Marquardt G, 2015, FRONT PSYCHOL, V6, DOI [10.3389/fpsyg.2015.01381, 10.3389/fpg.2015.01381]
   Motoyoshi I, 2007, NATURE, V447, P206, DOI 10.1038/nature05724
   Motoyoshi I, 2010, J VISION, V10, DOI 10.1167/10.9.6
   Ostrovsky Y, 2005, PERCEPTION, V34, P1301, DOI 10.1068/p5418
   Ouhnana M, 2016, I-PERCEPTION, V7, DOI 10.1177/2041669516677843
   Peri H., 2004, P 2004 IEEE COMP SOC, V1, pI, DOI [10.1109/CVPR.2004.1315067, DOI 10.1109/CVPR.2004.1315067]
   POSDAMER JL, 1982, COMPUT VISION GRAPH, V18, P1, DOI 10.1016/0146-664X(82)90096-X
   Price TJ, 1998, PERCEPTION, V27, P591, DOI 10.1068/p270591
   Raskar R, 2001, SPRING EUROGRAP, P89
   Raskar R., 1998, 1 IEEE WORKSHOP AUGM, P11
   Raskar Ramesh, 2002, P INT S NONPH AN REN, P284
   Salvi J, 2004, PATTERN RECOGN, V37, P827, DOI 10.1016/j.patcog.2003.10.002
   Siegl C, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818111
   Taya S, 2010, ATTEN PERCEPT PSYCHO, V72, P1930, DOI 10.3758/APP.72.7.1930
   Witt JK, 2008, PSYCHON B REV, V15, P581, DOI 10.3758/PBR.15.3.581
   Yonas A, 2006, PERCEPT PSYCHOPHYS, V68, P154, DOI 10.3758/BF03193665
NR 39
TC 1
Z9 1
U1 0
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD AUG
PY 2019
VL 16
IS 2
AR 10
DI 10.1145/3342350
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA JC8UL
UT WOS:000489551500004
DA 2024-07-18
ER

PT J
AU Lyu, YQ
   Zhang, X
   Luo, XM
   Hu, ZY
   Zhang, JY
   Shi, YC
AF Lyu, Yongqiang
   Zhang, Xiao
   Luo, Xiaomin
   Hu, Ziyue
   Zhang, Jingyu
   Shi, Yuanchun
TI Non-Invasive Measurement of Cognitive Load and Stress Based on the
   Reflected Stress-Induced Vascular Response Index
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Cognitive load; mental effort; photoplethysmogram; stress; reflected
   stress-induced vascular response index
ID TASK-DIFFICULTY; MENTAL STRESS; WORKLOAD
AB Measuring cognitive load and stress is crucial for ubiquitous human computer interaction applications to dynamically understand and respond to the mental status of users, such as in smart healthcare, smart driving, and robotics. Various quantitative methods have been employed for this purpose, such as physiological and behavioral methods. However, the sensitivity, reliability, and usability are not satisfactory in many of the current methods, so they are not ideal for ubiquitous applications. In this study, we employed a reflected photoplethysmogram-based stress-induced vascular response index, i.e., the reflected sVRI (sVRI-r), to non-invasively measure the cognitive load and stress. This method has high usability as well as good sensitivity and reliability compared with the previously proposed transmitted sVRI (sVRI-t). We developed the basic methodology and detailed algorithm framework to validate the sVRI-r measurements, and it was implemented by employing two light sources, i.e., infrared light and green light. Compared with the simultaneously recorded blood pressure, heart rate variation, and sVRI-t, our findings demonstrated the greater potential of the sVRI-r for use as a sensitive, reliable, and usable parameter, as well as suggesting its potential integration with ubiquitous touch interactions for dynamic cognition and stress-sensing scenarios.
C1 [Lyu, Yongqiang; Zhang, Xiao; Hu, Ziyue; Shi, Yuanchun] Tsinghua Univ, 1-407,FIT Bldg, Beijing 100084, Peoples R China.
   [Luo, Xiaomin] BGI Wuhan Translat Res Ctr, Wuhan 430074, Hubei, Peoples R China.
   [Zhang, Jingyu] Chinese Acad Sci, Inst Psychol, 16 Lincui Rd, Beijing 100101, Peoples R China.
C3 Tsinghua University; Chinese Academy of Sciences; Institute of
   Psychology, CAS
RP Lyu, YQ (corresponding author), Tsinghua Univ, 1-407,FIT Bldg, Beijing 100084, Peoples R China.
EM luyq@tsinghua.edu.cn; luo.xiaomin@139.com; zhangjingyu@psych.ac.cn
RI Lyu, Yongqiang/JUF-0554-2023; luo, xiaomin/HNS-7206-2023
FU ubihealth [FP7-PEOPLE-2012-IRSES]; Chinese National Key Research and
   Development Program [2017YFB0403404]; Tsinghua University Initiative
   Scientific Research Program; Beijing Innovation Center for Future Chip
   [KYJJ2016005]; China Scholarship Council
FX We gratefully acknowledge the supports from ubihealth (Grant No.
   FP7-PEOPLE-2012-IRSES), the Chinese National Key Research and
   Development Program (Grant No. 2017YFB0403404), Tsinghua University
   Initiative Scientific Research Program, the Research Fund from Beijing
   Innovation Center for Future Chip (Grant No. KYJJ2016005), and the China
   Scholarship Council.
CR [Anonymous], 2012, ACM T COMPUTER HUMAN, DOI DOI 10.1145/2395131.2395138]
   [Anonymous], P INT ERG ASS SEOUL
   BERNTSON GG, 1991, PSYCHOL REV, V98, P459, DOI 10.1037/0033-295X.98.4.459
   Bousefsaf F, 2014, COMPUT BIOL MED, V53, P154, DOI 10.1016/j.compbiomed.2014.07.014
   Brouwer AM, 2014, INT J PSYCHOPHYSIOL, V93, P242, DOI 10.1016/j.ijpsycho.2014.05.004
   Brouwer AM, 2012, J NEURAL ENG, V9, DOI 10.1088/1741-2560/9/4/045008
   Brünken R, 2003, EDUC PSYCHOL-US, V38, P53, DOI 10.1207/S15326985EP3801_7
   Chen CC, 2014, J ALTERN COMPLEM MED, V20, P860, DOI 10.1089/acm.2013.0421
   Fink MP, 2014, CRIT CARE, V18, DOI 10.1186/s13054-014-0561-6
   Haim S., 2015, Adjunct Proceedings of the 2015 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2015 ACM International Symposium on Wearable Computers, P733, DOI [10.1145/2800835.2804398, DOI 10.1145/2800835.2804398]
   HART S G, 1988, P139
   Healey J, 2000, INT C PATT RECOG, P218, DOI 10.1109/ICPR.2000.902898
   Hearn Louisa, 2014, Process Engineering, V95, P51
   Hjortskov N, 2004, EUR J APPL PHYSIOL, V92, P84, DOI 10.1007/s00421-004-1055-z
   Iani C, 2004, PSYCHOPHYSIOLOGY, V41, P789, DOI 10.1111/j.1469-8986.2004.00200.x
   Kiecolt-Glaser JK, 2002, J CONSULT CLIN PSYCH, V70, P537, DOI 10.1037//0022-006X.70.3.537
   Kil-sang Yoo, 2011, 2011 IEEE 15th International Symposium on Consumer Electronics, P323, DOI 10.1109/ISCE.2011.5973841
   Kobayashi M., 2011, P 24 ANN ACM S US IN, P499
   Kreibig SD, 2010, BIOL PSYCHOL, V84, P394, DOI 10.1016/j.biopsycho.2010.03.010
   Li Luo, 2012, 2012 International Conference on Biomedical Engineering and Biotechnology (iCBEB), P929, DOI 10.1109/iCBEB.2012.437
   Lu H, 2012, UBICOMP'12: PROCEEDINGS OF THE 2012 ACM INTERNATIONAL CONFERENCE ON UBIQUITOUS COMPUTING, P351
   Lyu YQ, 2015, CHI 2015: PROCEEDINGS OF THE 33RD ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P857, DOI 10.1145/2702123.2702399
   Matthews G, 2002, EMOTION, V2, P315, DOI 10.1037//1528-3542.2.4.315
   Oviatt S., 2006, Proc. 14th Annu. ACM Int. Conf. Multimed.-Multimed. '06, P871, DOI [10.1145/1180639.1180831, DOI 10.1145/1180639.1180831]
   Paas F, 2003, EDUC PSYCHOL, V38, P1, DOI 10.1207/S15326985EP3801_1
   Ritter F.E., 2007, Integrated Models of Cognitive Systems, P254, DOI DOI 10.1093/acprof:oso/9780195189193.003.0018
   Sarker H, 2014, UBICOMP'14: PROCEEDINGS OF THE 2014 ACM INTERNATIONAL JOINT CONFERENCE ON PERVASIVE AND UBIQUITOUS COMPUTING, P909, DOI 10.1145/2632048.2636082
   Tamura T, 2014, ELECTRONICS-SWITZ, V3, P282, DOI 10.3390/electronics3020282
   Verwey WB, 1996, J EXP PSYCHOL-APPL, V2, P270, DOI 10.1037/1076-898X.2.3.270
   Wijshoff RWCGR, 2012, J BIOMED OPT, V17, DOI 10.1117/1.JBO.17.11.117007
   Wijshoff RWCGR, 2011, PROC SPIE, V7894, DOI 10.1117/12.874170
NR 31
TC 3
Z9 3
U1 1
U2 14
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD AUG
PY 2018
VL 15
IS 3
AR 17
DI 10.1145/3185665
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA GY2UV
UT WOS:000448400300003
DA 2024-07-18
ER

PT J
AU Morrison-Smith, S
   Hofmann, M
   Li, Y
   Ruiz, J
AF Morrison-Smith, Sarah
   Hofmann, Megan
   Li, Yang
   Ruiz, Jaime
TI Using Audio Cues to Support Motion Gesture Interaction on Mobile Devices
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Motion gestures; mobile interaction; audio feedback
AB Motion gestures are an underutilized input modality for mobile interaction despite numerous potential advantages. Negulescu et al. found that the lack of feedback on attempted motion gestures made it difficult for participants to diagnose and correct errors, resulting in poor recognition performance and user frustration. In this article, we describe and evaluate a training and feedback technique, Glissando, which uses audio characteristics to provide feedback on the system's interpretation of user input. This technique enables feedback by verbally confirming correct gestures and notifying users of errors in addition to providing continuous feedback by manipulating the pitch of distinct musical notes mapped to each of three dimensional axes in order to provide both spatial and temporal information.
C1 [Morrison-Smith, Sarah; Hofmann, Megan; Ruiz, Jaime] Colorado State Univ, Ft Collins, CO 80523 USA.
   [Li, Yang] Google Res, Mountain View, CA USA.
   [Li, Yang] Google Inc, Mountain View, CA USA.
C3 Colorado State University; Google Incorporated; Google Incorporated
RP Morrison-Smith, S (corresponding author), Colorado State Univ, Ft Collins, CO 80523 USA.
EM sarahms@cs.colostate.edu; mkh@cs.colostate.edu; yangli@acm.org;
   jgruiz@cs.colostate.edu
FU Google Faculty Research Award
FX This work was funded by a Google Faculty Research Award.
CR Andersen TH, 2010, ACM T APPL PERCEPT, V7, DOI 10.1145/1773965.1773968
   [Anonymous], IPHONE US GUID IPHON
   Ashbrook D, 2010, CHI2010: PROCEEDINGS OF THE 28TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1-4, P2159
   Bartlett JF, 2000, IEEE COMPUT GRAPH, V20, P40, DOI 10.1109/38.844371
   Bau O, 2008, UIST 2008: PROCEEDINGS OF THE 21ST ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P37, DOI 10.1145/1449715.1449724
   Bragdon A, 2009, CHI2009: PROCEEDINGS OF THE 27TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1-4, P2269
   Brewster S., 2003, Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, CHI '03, ACM New York, NY, USA, P473, DOI DOI 10.1145/642611.642694
   Cook ND, 2007, MUSIC PERCEPT, V24, P315, DOI 10.1525/MP.2007.24.3.315
   Create Digital Music, 2012, CREAT DIG MUS
   Edwards W. H., 2010, MOTOR LEARNING CONTR
   Eslambolchilar Parisa, 2004, P 17 ANN ACM S US IN
   Eslambolchilar Parisa, 2004, P INT WORKSH INT SON
   Ghez C, 2000, P INT C AUD DISPL IC
   Google Inc, 2013, ANDR OP SOURC PROJ
   Harada S, 2011, 29TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P2779
   Harrison B. L., 1998, CHI 98. Human Factors in Computing Systems. CHI 98 Conference Proceedings, P17, DOI 10.1145/274644.274647
   Hartmann B, 2007, CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1 AND 2, P145
   Hinckley K., 2000, UIST. Proceedings of the 13th Annual ACM Symposium on User Interface Software and Technology, P91, DOI 10.1145/354401.354417
   JONES E, 2010, P SIGCHI C HUM FACT, P2173, DOI DOI 10.1145/1753326.1753655
   Kamal A, 2014, P ACM C INT US INT, P73
   Kane S.K., 2011, Proceedings of the 24th annual ACM symposium on User interface software and technology, P273, DOI DOI 10.1145/2047196.2047232
   Kane Shaun K., 2008, Proceedings of the 10th international ACM SIGACCESS confer- ence on Computers and accessibility, Assets '08, P73, DOI DOI 10.1145/1414471.1414487
   LI FCY, 2009, P UIST 09, P125
   Liu JQ, 2009, GLOB TELECOMM CONF, P6186
   Lumsden J., 2003, Proceedings of the 2003 Conference of the Centre For Advanced Studies on Collaborative Research (Toronto, Ontario, Canada, October 06 - 09, P197
   Marentakis G, 2004, LECT NOTES COMPUT SC, V3160, P180
   Marentakis G., 2005, Proceedings of the 7th International Conference on Human Computer Interaction with Mobile Devices Services, MobileHCI '05, ACM New York, NY, USA, P55
   Marentakis GeorgiosN., 2006, PROC CHI 06, P359, DOI DOI 10.1145/1124772.1124826
   Muller-Tomfelde C., 2001, Proc. ICAD 2001. ICAD, P267
   Negulescu M., 2012, Proceedings of the 14th International Conference on Human-computer Interaction with Mobile Devices and Services, MobileHCI '12, ACM New York, NY, USA, P147
   Negulescu M, 2012, PROCEEDINGS OF THE INTERNATIONAL WORKING CONFERENCE ON ADVANCED VISUAL INTERFACES, P173, DOI 10.1145/2254556.2254589
   Oh Uran, 2013, P ACM SIGACCESS C CO, DOI [10.1145/2513383.2513455, 10.1145/2513383.2513455event-place, DOI 10.1145/2513383.2513455]
   Partridge K., 2002, Proc. of UIST '02, P201, DOI [DOI 10.1145/571985.572013, 10.1145/571985.572013]
   Plimmer B, 2011, ACM T COMPUT-HUM INT, V18, DOI 10.1145/1993060.1993067
   Rekimoto J., 1996, Proc. UIST, P167, DOI [10.1145/237091.237115, DOI 10.1145/237091.237115]
   Ruiz J, 2011, 29TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P2717
   Schmitz G, 2014, 20 INT C AUD DISPL I
   Small D., 1997, Human Factors in Computing Systems. CHI 97 Extended Abstracts, P367
   Sodhi R., 2012, Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, CHI '12, ACM New York, NY, USA, P179, DOI [10.1145/2207676.2207702, DOI 10.1145/2207676.2207702]
   TALBOT M, 2009, P SIGCHI C HUM FACT, P1839
   Wallis I, 2007, P 13 INT C AUD DISPL, P497
   Weberg L., 2001, CHI '01 Extended Abstracts on Human Factors in Computing Systems, CHI EA '01, ACM New York, NY, USA, P435, DOI DOI 10.1145/634067.634320
   Wigdor Daniel., 2003, UIST '03: Proceedings of the 16th annual ACM symposium on User interface software and technology, P81, DOI DOI 10.1145/964696.964705
   Williamson J, 2005, IEEE MULTIMEDIA, V12, P45, DOI 10.1109/MMUL.2005.37
   Williamson John, 2002, TR2002127 DCS U GLAS
NR 45
TC 2
Z9 2
U1 1
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD MAY
PY 2016
VL 13
IS 3
AR 16
DI 10.1145/2897516
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DV4DU
UT WOS:000382876200006
OA Green Published, Bronze
DA 2024-07-18
ER

PT J
AU Kokkinara, E
   Slater, M
   López-Moliner, J
AF Kokkinara, Elena
   Slater, Mel
   Lopez-Moliner, Joan
TI The Effects of Visuomotor Calibration to the Perceived Space and Body,
   through Embodiment in Immersive Virtual Reality
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Experimentation; Human Factors
ID SENSORIMOTOR ADAPTATION; ILLUSORY OWNERSHIP; VISUAL DISTORTIONS; ARM
   MOVEMENTS; END-POINTS; MOTOR; HAND; RECALIBRATION; PERCEPTION; JUDGMENTS
AB We easily adapt to changes in the environment that involve cross-sensory discrepancies (e.g., between vision and proprioception). Adaptation can lead to changes in motor commands so that the experienced sensory consequences are appropriate for the new environment (e.g., we program a movement differently while wearing prisms that shift our visual space). In addition to these motor changes, perceptual judgments of space can also be altered (e.g., how far can I reach with my arm?). However, in previous studies that assessed perceptual judgments of space after visuomotor adaptation, the manipulation was always a planar spatial shift, whereas changes in body perception could not directly be assessed. In this study, we investigated the effects of velocity dependent (spatiotemporal) and spatial scaling distortions of arm movements on space and body perception, taking advantage of immersive virtual reality. Exploiting the perceptual illusion of embodiment in an entire virtual body, we endowed subjects with new spatiotemporal or spatial 3D mappings between motor commands and their sensory consequences. The results imply that spatiotemporal manipulation of 2 and 4 times faster can significantly change participants' proprioceptive judgments of a virtual object's size without affecting the perceived body ownership, although it did affect the agency of the movements. Equivalent spatial manipulations of 11 and 22 degrees of angular offset also had a significant effect on the perceived virtual object's size; however, the mismatched information did not affect either the sense of body ownership or agency. We conclude that adaptation to spatial and spatiotemporal distortion can similarly change our perception of space, although spatiotemporal distortions can more easily be detected.
C1 [Kokkinara, Elena; Slater, Mel] Univ Barcelona, Fac Psicol, Barcelona 08035, Spain.
   [Kokkinara, Elena; Lopez-Moliner, Joan] Univ Barcelona, VISCA Lab, Dept Psicol Basica, Barcelona 08035, Spain.
   [Kokkinara, Elena; Slater, Mel; Lopez-Moliner, Joan] Inst Brain Cognit & Behav IR3C, Barcelona, Spain.
   [Slater, Mel] ICREA, Barcelona, Spain.
C3 University of Barcelona; University of Barcelona; University of
   Barcelona; ICREA
RP Kokkinara, E (corresponding author), Univ Barcelona, Fac Psicol, Barcelona 08035, Spain.
EM elena.kokkinara@ub.edu; melslater@ub.edu; j.lopezmoliner@ub.edu
RI McDonnell, Rachel/HGC-4337-2022; Slater, Mel/M-5210-2014
OI McDonnell, Rachel/0000-0002-1957-2506; Slater, Mel/0000-0002-6223-0050
FU FP7 EU VR-HYPERSPACE under the Aeronautics and Air Transport (AAT) work
   program [AAT-285681]; ICREA Funding Source: Custom
FX This work was funded by the FP7 EU VR-HYPERSPACE (AAT-285681) project
   funded under the Aeronautics and Air Transport (AAT) work program.
CR [Anonymous], 1978, PERCEPTUAL MODIFICAT
   [Anonymous], 2000, R Language Definition
   BAILY JS, 1972, Q J EXP PSYCHOL, V24, P8, DOI 10.1080/14640747208400261
   Banakou D, 2013, P NATL ACAD SCI USA, V110, P12846, DOI 10.1073/pnas.1306779110
   Baraduc P, 2002, J NEUROPHYSIOL, V88, P973, DOI 10.1152/jn.2002.88.2.973
   Bays PM, 2005, EXP BRAIN RES, V163, P400, DOI 10.1007/s00221-005-2299-5
   BEDFORD FL, 1993, J EXP PSYCHOL HUMAN, V19, P517, DOI 10.1037/0096-1523.19.3.517
   BEDFORD FL, 1994, CAH PSYCHOL COGN, V13, P405
   Berniker M, 2008, NAT NEUROSCI, V11, P1454, DOI 10.1038/nn.2229
   Bock O, 2003, EXP BRAIN RES, V151, P557, DOI 10.1007/s00221-003-1553-y
   Bock O, 1997, BEHAV BRAIN RES, V89, P267, DOI 10.1016/S0166-4328(97)00069-7
   Bock O, 2011, HUM MOVEMENT SCI, V30, P415, DOI 10.1016/j.humov.2010.10.007
   Botvinick M, 1998, NATURE, V391, P756, DOI 10.1038/35784
   Bourgeois J, 2012, ATTEN PERCEPT PSYCHO, V74, P1268, DOI 10.3758/s13414-012-0316-x
   CHOE CS, 1974, J EXP PSYCHOL, V102, P1076, DOI 10.1037/h0036325
   Clayton HA, 2014, EXP BRAIN RES, V232, P2073, DOI 10.1007/s00221-014-3896-y
   Clower DM, 2000, J NEUROPHYSIOL, V84, P2703, DOI 10.1152/jn.2000.84.5.2703
   Coello Y, 2008, PLOS ONE, V3, DOI 10.1371/journal.pone.0002862
   Cunningham DW, 2001, PSYCHOL SCI, V12, P532, DOI 10.1111/1467-9280.00398
   de Grave DDJ, 2011, PERCEPTION, V40, P962, DOI 10.1068/p6788
   de la Malla C, 2014, J VISION, V14, DOI 10.1167/14.12.8
   de la Malla C, 2012, J VISION, V12, DOI 10.1167/12.10.4
   Desmurget M, 1997, EXP BRAIN RES, V115, P180, DOI 10.1007/PL00005680
   DESMURGET M, 1995, CAN J PHYSIOL PHARM, V73, P262, DOI 10.1139/y95-037
   EA MCMANUS., 2011, P ACM SIGGRAPH S APP, P37, DOI DOI 10.1145/2077451.2077458
   Ehrsson HH, 2009, PERCEPTION, V38, P310, DOI 10.1068/p6304
   Ehrsson HH, 2005, PLOS BIOL, V3, P2200, DOI 10.1371/journal.pbio.0030412
   ELLIOTT D, 1991, HUM MOVEMENT SCI, V10, P393, DOI 10.1016/0167-9457(91)90013-N
   Farrer C, 2008, BEHAV NEUROL, V19, P53, DOI 10.1155/2008/425267
   Farrer C, 2003, NEUROIMAGE, V18, P324, DOI 10.1016/S1053-8119(02)00041-1
   Ferrel C, 2000, ERGONOMICS, V43, P461, DOI 10.1080/001401300184341
   Fourneret P, 1998, NEUROPSYCHOLOGIA, V36, P1133, DOI 10.1016/S0028-3932(98)00006-2
   Franck N, 2001, AM J PSYCHIAT, V158, P454, DOI 10.1176/appi.ajp.158.3.454
   Ghahramani Z, 1996, J NEUROSCI, V16, P7085
   GOODALE MA, 1992, TRENDS NEUROSCI, V15, P20, DOI 10.1016/0166-2236(92)90344-8
   Goodbody SJ, 1998, J NEUROPHYSIOL, V79, P1825, DOI 10.1152/jn.1998.79.4.1825
   Grechkin TY, 2010, ACM T APPL PERCEPT, V7, DOI 10.1145/1823738.1823744
   HARRIS CS, 1965, PSYCHOL REV, V72, P419, DOI 10.1037/h0022616
   HAY JC, 1971, J EXP PSYCHOL, V91, P11, DOI 10.1037/h0031787
   HAY JC, 1966, J EXP PSYCHOL, V72, P640, DOI 10.1037/h0023737
   Heron J, 2009, PLOS ONE, V4, DOI 10.1371/journal.pone.0007681
   Hothorn T, 2008, BIOMETRICAL J, V50, P346, DOI 10.1002/bimj.200810425
   Izawa J, 2012, J NEUROSCI, V32, P4230, DOI 10.1523/JNEUROSCI.6353-11.2012
   Kennedy JS, 2009, Q J EXP PSYCHOL, V62, P453, DOI 10.1080/17470210801985235
   Kilteni K, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0040867
   Knoblich G, 2004, J EXP PSYCHOL HUMAN, V30, P657, DOI 10.1037/0096-1523.30.4.657
   Kokkinara E, 2014, PERCEPTION, V43, P43, DOI 10.1068/p7545
   Krakauer JW, 2000, J NEUROSCI, V20, P8916, DOI 10.1523/jneurosci.20-23-08916.2000
   LAZAR G, 1968, PERCEPT MOTOR SKILL, V26, P579, DOI 10.2466/pms.1968.26.2.579
   Leube DT, 2003, NEUROIMAGE, V20, P2084, DOI 10.1016/j.neuroimage.2003.07.033
   Leyrer M., 2011, P ACM SIGGRAPH S APP, DOI 10.1145/2077451.2077464
   Llobera J, 2013, J R SOC INTERFACE, V10, DOI 10.1098/rsif.2013.0300
   Mohler BJ, 2010, PRESENCE-TELEOP VIRT, V19, P230, DOI 10.1162/pres.19.3.230
   Mon-Williams M, 2007, J EXP PSYCHOL HUMAN, V33, P645, DOI 10.1037/0096-1523.33.3.645
   Pinheiro J., 2012, R package version, V3, P1
   POLIT A, 1979, J NEUROPHYSIOL, V42, P183, DOI 10.1152/jn.1979.42.1.183
   Posada A, 2007, CR BIOL, V330, P382, DOI 10.1016/j.crvi.2007.02.003
   Redding GM, 1996, J EXP PSYCHOL HUMAN, V22, P379, DOI 10.1037/0096-1523.22.2.379
   REDDING GM, 1993, J MOTOR BEHAV, V25, P75, DOI 10.1080/00222895.1993.9941642
   Rieger M, 2005, EXP BRAIN RES, V163, P487, DOI 10.1007/s00221-004-2203-8
   Rodríguez-Herreros B, 2013, VISION RES, V88, P30, DOI 10.1016/j.visres.2013.06.005
   ROSSETTI Y, 1994, EXP BRAIN RES, V101, P323, DOI 10.1007/BF00228753
   Shadmehr R, 2010, ANNU REV NEUROSCI, V33, P89, DOI 10.1146/annurev-neuro-060909-153135
   Shimada S, 2010, EXP BRAIN RES, V201, P359, DOI 10.1007/s00221-009-2028-6
   Simani MC, 2007, J NEUROPHYSIOL, V98, P2827, DOI 10.1152/jn.00290.2007
   Slachevsky A, 2001, J COGNITIVE NEUROSCI, V13, P332, DOI 10.1162/08989290151137386
   Slater M, 2009, FRONT NEUROSCI-SWITZ, V3, P214, DOI 10.3389/neuro.01.029.2009
   Slater M, 2010, PLOS ONE, V5, DOI 10.1371/journal.pone.0010564
   Slater M, 2008, FRONT HUM NEUROSCI, V2, DOI 10.3389/neuro.09.006.2008
   Stetson C, 2006, NEURON, V51, P651, DOI 10.1016/j.neuron.2006.08.006
   Sutter C., 2008, Mensch computer, P147
   Synofzik M, 2008, CONSCIOUS COGN, V17, P219, DOI 10.1016/j.concog.2007.03.010
   Synofzik M, 2006, J NEUROPHYSIOL, V96, P1592, DOI 10.1152/jn.00104.2006
   Thomas M, 2010, HUM MOVEMENT SCI, V29, P179, DOI 10.1016/j.humov.2010.02.002
   van den Dobbelsteen JJ, 2003, EXP BRAIN RES, V148, P471, DOI 10.1007/s00221-002-1321-4
   van den Dobbelsteen JJ, 2001, EXP BRAIN RES, V138, P279, DOI 10.1007/s002210100689
   Vetter P, 1999, J NEUROPHYSIOL, V81, P935, DOI 10.1152/jn.1999.81.2.935
   von Helmholtz H., 1910, HELMHOLTZS TREATISE, VIII
   Welch R.B., 1986, HDB PERCEPTION HUMAN, V1, p25
   WELCH RB, 1993, PERCEPT PSYCHOPHYS, V54, P195, DOI 10.3758/BF03211756
   Welch RB, 2008, DISPLAYS, V29, P152, DOI 10.1016/j.displa.2007.09.013
   Yuan Y, 2010, P IEEE VIRT REAL ANN, P95, DOI 10.1109/VR.2010.5444807
NR 82
TC 64
Z9 69
U1 0
U2 30
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD DEC
PY 2015
VL 13
IS 1
AR 3
DI 10.1145/2818998
PG 22
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA DD2SV
UT WOS:000369773400003
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Lin, KH
   Zhuang, XD
   Goudeseune, C
   King, S
   Hasegawa-Johnson, M
   Huang, TS
AF Lin, Kai-Hsiang
   Zhuang, Xiaodan
   Goudeseune, Camille
   King, Sarah
   Hasegawa-Johnson, Mark
   Huang, Thomas S.
TI Saliency-Maximized Audio Visualization and Efficient Audio-Visual
   Browsing for Faster-Than-Real-Time Human Acoustic Event Detection
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Human Factors; Algorithms; Experimentation; Visual salience/saliency;
   audio visualization; acoustic event detection
ID VISUAL-ATTENTION
AB Browsing large audio archives is challenging because of the limitations of human audition and attention. However, this task becomes easier with a suitable visualization of the audio signal, such as a spectrogram transformed to make unusual audio events salient. This transformation maximizes the mutual information between an isolated event's spectrogram and an estimate of how salient the event appears in its surrounding context. When such spectrograms are computed and displayed with fluid zooming over many temporal orders of magnitude, sparse events in long audio recordings can be detected more quickly and more easily. In particular, in a 1/10-real-time acoustic event detection task, subjects who were shown saliency-maximized rather than conventional spectrograms performed significantly better. Saliency maximization also improves the mutual information between the ground truth of nonbackground sounds and visual saliency, more than other common enhancements to visualization.
C1 [Lin, Kai-Hsiang; Zhuang, Xiaodan; Goudeseune, Camille; King, Sarah; Hasegawa-Johnson, Mark; Huang, Thomas S.] Univ Illinois, Urbana, IL USA.
C3 University of Illinois System; University of Illinois Urbana-Champaign
RP Lin, KH (corresponding author), Univ Illinois, Beckman Inst, 405 North Mathews Ave, Urbana, IL 61801 USA.
EM klin21@illinois.edu; xzhuang@bbn.com; cog@illinois.edu;
   sborys@illinois.edu; jhasegaw@illinois.edu; t-huang1@illinois.edu
RI yan, shuicheng/A-8531-2014; yan, shuicheng/HCH-9860-2022
OI yan, shuicheng/0000-0001-8906-3777; yan, shuicheng/0000-0003-4527-1018;
   Goudeseune, Camille/0000-0003-3530-8577
FU US National Science Foundation [0807329]; Division of Computing and
   Communication Foundations; Direct For Computer & Info Scie & Enginr
   [0807329] Funding Source: National Science Foundation
FX This work is funded by the US National Science Foundation grant 0807329.
   All results and opinions are those of the authors and are not endorsed
   by the US National Science Foundation.
CR Abouchacra KS, 2007, MIL PSYCHOL, V19, P197, DOI 10.1080/08995600701386341
   Anderson J.R., 2009, COGNITIVE PSYCHOL IT, V7th
   [Anonymous], 2006, Elements of Information Theory
   [Anonymous], 1949, MATH MODEL COMMUNICA
   [Anonymous], P IEEE WORKSH APPL S
   [Anonymous], 1992, SENSATION PERCEPTION
   Arons B., 1997, ACM Transactions on Computer-Human Interaction, V4, P3, DOI 10.1145/244754.244758
   Belopolsky AV, 2008, VIS COGN, V16, P409, DOI 10.1080/13506280701695454
   Berry MW, 2007, COMPUT STAT DATA AN, V52, P155, DOI 10.1016/j.csda.2006.11.006
   Borji A, 2013, IEEE T PATTERN ANAL, V35, P185, DOI 10.1109/TPAMI.2012.89
   Bovik A, 2009, ESSENTIAL GUIDE TO IMAGE PROCESSING, 2ND EDITION, P1
   Carletta J, 2007, LANG RESOUR EVAL, V41, P181, DOI 10.1007/s10579-007-9040-x
   Frintrop S, 2010, ACM T APPL PERCEPT, V7, DOI 10.1145/1658349.1658355
   Gonzalez R.C., 2018, DIGITAL IMAGE PROCES, V4th
   Goudeseune Camille., 2012, P 2 ACM INT WORKSHOP, P35, DOI [10.1145/2390821.2390831, DOI 10.1145/2390821.2390831]
   Hasegawa-Johnson M. A., 2011, P AS PAC SIGN INF PR, P526
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Itti L, 2001, J ELECTRON IMAGING, V10, P161, DOI 10.1117/1.1333677
   Jänicke H, 2010, COMPUT GRAPH FORUM, V29, P1183, DOI 10.1111/j.1467-8659.2009.01667.x
   Lee DD, 1999, NATURE, V401, P788, DOI 10.1038/44565
   Lim J.S., 1990, Two-dimensional Signal and Image Processing
   Lin KH, 2012, INT CONF ACOUST SPEE, P2277, DOI 10.1109/ICASSP.2012.6288368
   MILLER GA, 1956, PSYCHOL REV, V63, P81, DOI 10.1037/h0043158
   OTSU N, 1979, IEEE T SYST MAN CYB, V9, P62, DOI 10.1109/TSMC.1979.4310076
   Rosenholtz R, 2011, ACM T APPL PERCEPT, V8, DOI 10.1145/1870076.1870080
   Smaragdis P, 2004, LECT NOTES COMPUT SC, V3195, P494
   Smith Jaclyn A, 2006, Cough, V2, P6, DOI 10.1186/1745-9974-2-6
   TEMKO A, 2007, CLEAR 2007 AED EV PL
   Temko A., 2006, COUGH, V65, P5
   Van Rijsbergen C. J., 1979, Information Retrieval, V2nd
   Walther D, 2006, NEURAL NETWORKS, V19, P1395, DOI 10.1016/j.neunet.2006.10.001
   Wickens C.D., 1999, Engineering Psychology and Human Performance, V3rd
   Zhou X., 2007, P CLASS EV ACT REL E
NR 33
TC 0
Z9 1
U1 0
U2 6
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD OCT
PY 2013
VL 10
IS 4
AR 26
DI 10.1145/2536764.2536773
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA 281YK
UT WOS:000329136700009
DA 2024-07-18
ER

PT J
AU Rébillat, M
   Boutillon, X
   Corteel, É
   Katz, BFG
AF Rebillat, Marc
   Boutillon, Xavier
   Corteel, Etienne
   Katz, Brian F. G.
TI Audio, Visual, and Audio-Visual Egocentric Distance Perception by Moving
   Subjects in Virtual Environments
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Experimentation; Human Factors; Virtual environments; large-screen
   immersive displays; wave field synthesis; spatialized audio; distance
   estimation; spatial perception
ID DEPTH-PERCEPTION; MOTION; REALITY
AB We present a study on audio, visual, and audio-visual egocentric distance perception by moving subjects in virtual environments. Audio-visual rendering is provided using tracked passive visual stereoscopy and acoustic wave field synthesis (WFS). Distances are estimated using indirect blind-walking (triangulation) under each rendering condition. Experimental results show that distances perceived in the virtual environment are systematically overestimated for rendered distances closer than the position of the audio-visual rendering system and underestimated for farther distances. Interestingly, subjects perceived each virtual object at a modality-independent distance when using the audio modality, the visual modality, or the combination of both. WFS was able to synthesise perceptually meaningful sound fields. Dynamic audio-visual cues were used by subjects when estimating the distances in the virtual world. Moving may have provided subjects with a better visual distance perception of close distances than if they were static. No correlation between the feeling of presence and the visual distance underestimation has been found. To explain the observed perceptual distance compression, it is proposed that, due to conflicting distance cues, the audio-visual rendering system physically anchors the virtual world to the real world. Virtual objects are thus attracted by the physical audio-visual rendering system.
C1 [Rebillat, Marc] Ecole Normale Super, Lab Psychol Percept, F-75231 Paris, France.
   [Boutillon, Xavier] Ecole Polytechn, LMS, Palaiseau, France.
   [Corteel, Etienne] Sonic Emot Labs, F-75015 Paris, France.
   [Katz, Brian F. G.] Univ Paris 11, LIMSI CNRS, F-91403 Orsay, France.
   [Rebillat, Marc] Univ Paris 11, Paris, France.
C3 Universite Paris Cite; Universite PSL; Ecole Normale Superieure (ENS);
   Institut Polytechnique de Paris; Ecole Polytechnique; Universite Paris
   Saclay; Centre National de la Recherche Scientifique (CNRS); Universite
   Paris Saclay
RP Rébillat, M (corresponding author), Ecole Normale Super, Lab Psychol Percept, 24 Rue Lhomond, F-75231 Paris, France.
EM marc.rebillat@polytechnique.edu; boutillon@lms.polytechnique.fr;
   etienne.corteel@sonicemotion.com; brian.katz@limsi.fr
RI Katz, Brian F.G./I-3191-2012
OI Katz, Brian F.G./0000-0001-5118-0943; Rebillat, Marc/0000-0003-0469-8437
CR Alexandrova I.V., 2010, Proceedings of the 7th Symposium on Applied Perception in Graphics and Visualization, APGV'10, P57, DOI DOI 10.1145/1836248.1836258
   Andre J, 2006, PERCEPT PSYCHOPHYS, V68, P353, DOI 10.3758/BF03193682
   [Anonymous], 2012, ACM T APPL PERCEPTIO, V9
   Armbrüster C, 2008, CYBERPSYCHOL BEHAV, V11, P9, DOI 10.1089/cpb.2007.9935
   ASHMEAD DH, 1995, J EXP PSYCHOL HUMAN, V21, P239, DOI 10.1037/0096-1523.21.2.239
   BEALL AC, 1994, INVEST OPHTH VIS SCI, V35, P2111
   BERKHOUT AJ, 1993, J ACOUST SOC AM, V93, P2764, DOI 10.1121/1.405852
   Blauert J., 1999, Spatial Hearing: The Psychophysics of Human Sound Localization
   Bormann K, 2005, PRESENCE-TELEOP VIRT, V14, P278, DOI 10.1162/105474605323384645
   Bronkhorst AW, 1999, NATURE, V397, P517, DOI 10.1038/17374
   Brooks FP, 1999, IEEE COMPUT GRAPH, V19, P16, DOI 10.1109/38.799723
   CORTEEL E., 2007, P 30 INT C AUD ENG S
   CORTEEL E., 2004, THESIS U PARIS 6, P6
   COTE N., 2011, P FOR AC
   Creem-Regehr SH, 2005, PERCEPTION, V34, P191, DOI 10.1068/p5144
   Cutting JE, 1997, BEHAV RES METH INS C, V29, P27, DOI 10.3758/BF03200563
   Faria RRA, 2005, Proceedings of the 2005 IEEE International Conference on Virtual Environments, Human-Computer Interfaces and Measurement Systems, P103
   Fukusima SS, 1997, J EXP PSYCHOL HUMAN, V23, P86, DOI 10.1037/0096-1523.23.1.86
   GERZON MA, 1985, J AUDIO ENG SOC, V33, P859
   Grechkin TY, 2010, ACM T APPL PERCEPT, V7, DOI 10.1145/1823738.1823744
   Hoffman DM, 2008, J VISION, V8, DOI 10.1167/8.3.33
   HOWARTH P.A., 2011, OPHTHAL PHYSL OPT, V31, P111
   Interrante V, 2008, PRESENCE-TELEOP VIRT, V17, P176, DOI 10.1162/pres.17.2.176
   Kearney G, 2012, ACTA ACUST UNITED AC, V98, P61, DOI 10.3813/AAA.918492
   Klein E, 2009, IEEE VIRTUAL REALITY 2009, PROCEEDINGS, P107, DOI 10.1109/VR.2009.4811007
   KOMIYAMA S., 1991, P 9 AUD ENG SOC INT
   Loomis JM, 2003, VIRTUAL AND ADAPTIVE ENVIRONMENTS: APPLICATIONS, IMPLICATIONS, AND HUMAN PERFORMANCE ISSUES, P21
   Loomis JM, 1998, PERCEPT PSYCHOPHYS, V60, P966, DOI 10.3758/BF03211932
   Naceri A, 2009, 2009 COMPUTATION WORLD: FUTURE COMPUTING, SERVICE COMPUTATION, COGNITIVE, ADAPTIVE, CONTENT, PATTERNS, P460, DOI 10.1109/ComputationWorld.2009.91
   Nawrot M, 2009, VISION RES, V49, P1969, DOI 10.1016/j.visres.2009.05.008
   Plumert Jodie., 2005, ACM Transactions on Applied Perception, V2, P216, DOI DOI 10.1145/1077399.1077402
   Pörschmann C, 2009, ACTA ACUST UNITED AC, V95, P696, DOI 10.3813/AAA.918198
   REBILLAT M., 2008, P 125 CONV AUD ENG S
   REBILLAT M., 2009, P IEEE 3D TV C
   Russell MK, 2006, ECOL PSYCHOL, V18, P223, DOI 10.1207/s15326969eco1803_4
   Ryu J, 2005, 2005 International Conference on Cyberworlds, Proceedings, P43
   SANSON J., 2008, P 124 CONV AUD ENG S
   Slater M, 1997, PRESENCE-VIRTUAL AUG, V6, P603, DOI 10.1162/pres.1997.6.6.603
   Speigle J. M., 1993, P IEEE S RES FRONT V, P25
   Springer JR, 2006, P IEEE VIRT REAL ANN, P237, DOI 10.1109/VR.2006.33
   Watt SJ, 2005, J VISION, V5, P834, DOI 10.1167/5.10.7
   WIEST WM, 1985, PSYCHOL BULL, V98, P457, DOI 10.1037/0033-2909.98.3.457
   Zahorik P, 2005, ACTA ACUST UNITED AC, V91, P409
NR 43
TC 21
Z9 22
U1 1
U2 30
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD OCT
PY 2012
VL 9
IS 4
AR 19
DI 10.1145/2355598.2355602
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 025DJ
UT WOS:000310164900004
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Kastanis, I
   Slater, M
AF Kastanis, Iason
   Slater, Mel
TI Reinforcement Learning Utilizes Proxemics: An Avatar Learns to
   Manipulate the Position of People in Immersive Virtual Reality
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Experimentation; Human Factors; Human-computer interaction; proxemics;
   virtual characters; avatars
ID EXPOSURE THERAPY; FEAR; ENVIRONMENTS; ANXIETY; DISTANCE; BEHAVIOR; SENSE
AB A reinforcement learning (RL) method was used to train a virtual character to move participants to a specified location. The virtual environment depicted an alleyway displayed through a wide field-of-view head-tracked stereo head-mounted display. Based on proxemics theory, we predicted that when the character approached within a personal or intimate distance to the participants, they would be inclined to move backwards out of the way. We carried out a between-groups experiment with 30 female participants, with 10 assigned arbitrarily to each of the following three groups: In the Intimate condition the character could approach within 0.38m and in the Social condition no nearer than 1.2m. In the Random condition the actions of the virtual character were chosen randomly from among the same set as in the RL method, and the virtual character could approach within 0.38m. The experiment continued in each case until the participant either reached the target or 7 minutes had elapsed. The distributions of the times taken to reach the target showed significant differences between the three groups, with 9 out of 10 in the Intimate condition reaching the target significantly faster than the 6 out of 10 who reached the target in the Social condition. Only 1 out of 10 in the Random condition reached the target. The experiment is an example of applied presence theory: we rely on the many findings that people tend to respond realistically in immersive virtual environments, and use this to get people to achieve a task of which they had been unaware. This method opens up the door for many such applications where the virtual environment adapts to the responses of the human participants with the aim of achieving particular goals.
C1 [Kastanis, Iason; Slater, Mel] Univ Barcelona, ICREA, E-08007 Barcelona, Spain.
   [Slater, Mel] UCL, London, England.
C3 ICREA; University of Barcelona; University of London; University College
   London
RP Kastanis, I (corresponding author), Univ Barcelona, ICREA, E-08007 Barcelona, Spain.
EM melslater@ub.edu
RI Slater, Mel/M-5210-2014
OI Slater, Mel/0000-0002-6223-0050
FU EU [215756]; ICREA Funding Source: Custom
FX This research was supported by the EU 7th Framework Program Project
   MIMICS, Grant 215756.
CR Abbeel P., 2004, INT C MACH LEARN ICM
   [Anonymous], 1992, PRESENCE-VIRTUAL AUG, DOI [10.1162/pres.1992.1.1.109, DOI 10.1162/PRES.1992.1.1.109]
   Baas JM, 2004, BIOL PSYCHIAT, V55, P1056, DOI 10.1016/j.biopsych.2004.02.024
   Bailenson JN, 2001, PRESENCE-VIRTUAL AUG, V10, P583, DOI 10.1162/105474601753272844
   Bailenson JN, 2003, PERS SOC PSYCHOL B, V29, P819, DOI 10.1177/0146167203029007002
   BARFIELD W, 1993, ADV HUM FACT ERGON, V19, P699
   Barfield W., 1995, J VIRTUAL REALITY SO, V1, P3, DOI DOI 10.1007/BF02009709
   Collins S, 2005, SCIENCE, V307, P1082, DOI 10.1126/science.1107799
   Doya K, 2007, HFSP J, V1, P30, DOI [10.2976/1.2732246/10.2976/1, 10.2976/1.2732246]
   Draper JV, 1998, HUM FACTORS, V40, P354, DOI 10.1518/001872098779591386
   Freeman D, 2008, BRIT J PSYCHIAT, V192, P258, DOI 10.1192/bjp.bp.107.044677
   Friedman D, 2005, LECT NOTES ARTIF INT, V3661, P205
   Friedman D, 2007, LECT NOTES ARTIF INT, V4722, P252
   Garau M, 2008, PRESENCE-TELEOP VIRT, V17, P293, DOI 10.1162/pres.17.3.293
   Gillies M, 2010, PRESENCE-VIRTUAL AUG, V19, P95, DOI 10.1162/pres.19.2.95
   Guye-Vuilleme A., 1999, Virtual Reality, V4, P49, DOI 10.1007/BF01434994
   HALL E. T., 1973, HIDDEN DIMENSION
   Harris SR, 2002, CYBERPSYCHOL BEHAV, V5, P543, DOI 10.1089/109493102321018187
   HODGES LF, 1995, COMPUTER, V28, P27, DOI 10.1109/2.391038
   Hoffman HG, 2004, NEUROREPORT, V15, P1245, DOI 10.1097/01.wnr.0000127826.73576.91
   Hoffman HG, 2003, CYBERPSYCHOL BEHAV, V6, P127, DOI 10.1089/109493103321640310
   IKEMOTO L., 2005, ACM SIGGRAPH SKETCH, P46
   Krijn M, 2004, CLIN PSYCHOL REV, V24, P259, DOI 10.1016/j.cpr.2004.04.001
   Krijn M, 2007, AVIAT SPACE ENVIR MD, V78, P121
   Kuhl SA, 2009, ACM T APPL PERCEPT, V6, DOI 10.1145/1577755.1577762
   Lee SJ, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778859
   Lessiter J, 2001, PRESENCE-TELEOP VIRT, V10, P282, DOI 10.1162/105474601300343612
   Llobera J, 2010, ACM T APPL PERCEPT, V8, DOI 10.1145/1857893.1857896
   Lo Wan-Yen, 2008, Proceedings of the 2008 ACM SIGGRAPH/Eurographics Symposium on Computer Animation, P29
   McCann J, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1276377.1276385, 10.1145/1239451.1239457]
   Meehan M, 2003, P IEEE VIRT REAL ANN, P141, DOI 10.1109/VR.2003.1191132
   Meehan M, 2002, ACM T GRAPHIC, V21, P645, DOI 10.1145/566570.566630
   Mine M. R., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P19, DOI 10.1145/258734.258747
   Mortensen J, 2008, IEEE COMPUT GRAPH, V28, P56, DOI 10.1109/MCG.2008.121
   Pan XN, 2008, LECT NOTES COMPUT SC, V5208, P89
   Razzaque S., 2002, Virtual Environments 2002. Eurographics Workshop Proceedings, P123
   Regenbrecht HT, 1998, INT J HUM-COMPUT INT, V10, P233, DOI 10.1207/s15327590ijhc1003_2
   Rey B., 2008, P 11 ANN INT WORKSHO, P209
   Rizzo A, 2005, CYBERPSYCHOL BEHAV, V8, P352
   Rothbaum BO, 1999, BEHAV MODIF, V23, P507, DOI 10.1177/0145445599234001
   Rothbaum BO, 2000, J CONSULT CLIN PSYCH, V68, P1020, DOI 10.1037/0022-006X.68.6.1020
   Sanchez-Vives MV, 2005, NAT REV NEUROSCI, V6, P332, DOI 10.1038/nrn1651
   Sandro B., 2005, Proceedings of the 2005 ACM SIGCHI International Con- ference on Advances in Computer Entertainment Technology, P270, DOI DOI 10.1145/1178477.1178524
   Schubert T, 2001, PRESENCE-VIRTUAL AUG, V10, P266, DOI 10.1162/105474601300343603
   Sheridan T., 1992, Presence: Teleoperators and Virtual Environments, V1, P120, DOI DOI 10.1162/PRES.1992.1.1.120
   SHUM H. P., 2008, P S INT 3D GRAPH GAM, P131
   Slater M, 1997, PRESENCE-VIRTUAL AUG, V6, P603, DOI 10.1162/pres.1997.6.6.603
   Slater M, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778829
   Slater M, 2009, IEEE COMPUT GRAPH, V29, P76, DOI 10.1109/MCG.2009.55
   Sutton R., 1998, Reinforcement Learning: An Introduction
   Treuille A, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239458
   Usoh M, 1999, COMP GRAPH, P359, DOI 10.1145/311535.311589
   Usoh M, 2000, PRESENCE-TELEOP VIRT, V9, P497, DOI 10.1162/105474600566989
   VIGORITO C. M., 2007, P 6 INT JOINT C AUT, P1, DOI DOI 10.1145/1329125.1329273
   Wallach HS, 2009, BEHAV MODIF, V33, P314, DOI 10.1177/0145445509331926
   Wilcox Laurie M., 2006, ACM Trans. on Perception, V3, P412, DOI [DOI 10.1145/1190036.1190041, 10.1145/1190036.1190041]
   Witmer BG, 2005, PRESENCE-TELEOP VIRT, V14, P298, DOI 10.1162/105474605323384654
   Witmer BG, 1998, PRESENCE-TELEOP VIRT, V7, P225, DOI 10.1162/105474698565686
   Wu Tingfan., 2009, IEEE 8th International Conference on Development and Learning, 2009 (ICDL 2009), P1, DOI [10.1109/DEVLRN.2009.5175536., DOI 10.1109/DEVLRN.2009.5175536]
   Zimmons P, 2003, P IEEE VIRT REAL ANN, P293, DOI 10.1109/VR.2003.1191170
NR 60
TC 23
Z9 26
U1 0
U2 6
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD MAR
PY 2012
VL 9
IS 1
AR 3
DI 10.1145/2134203.2134206
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA 949YM
UT WOS:000304614400003
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Williams, B
   Bailey, S
   Narasimham, G
   Li, MQ
   Bodenheimer, B
AF Williams, Betsy
   Bailey, Stephen
   Narasimham, Gayathri
   Li, Muqun
   Bodenheimer, Bobby
TI Evaluation of Walking in Place on a Wii Balance Board to Explore a
   Virtual Environment
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Experimentation; Human Factors; Virtual Reality (VR); space perception;
   graphics; human-computer interaction
ID ORIENTATION; NAVIGATION; LOCOMOTION; TRAVEL
AB In this work, we present a method of "Walking In Place" (WIP) on the Nintendo Wii Fit Balance Board to explore a virtual environment. We directly compare our method to joystick locomotion and normal walking. The joystick proves inferior to physically walking and to WIP on the Wii Balance Board (WIP-Wii). Interestingly, we find that physically exploring an environment on foot is equivalent in terms of spatial orientation to exploring an environment using our WIP-Wii method. This implies that the WIP-Wii is a good inexpensive alternative to exploring a virtual environment and it may be well-suited for exploring large virtual environments.
C1 [Williams, Betsy] Rhodes Coll, Math & Comp Sci Dept, Memphis, TN 38112 USA.
   [Bailey, Stephen; Narasimham, Gayathri; Li, Muqun; Bodenheimer, Bobby] Vanderbilt Univ, Nashville, TN 37235 USA.
C3 Vanderbilt University
RP Williams, B (corresponding author), Rhodes Coll, Math & Comp Sci Dept, 2000 N Pkwy, Memphis, TN 38112 USA.
EM williamsb@rhodes.eud
FU National Science Foundation [0705863, 0821640]; Rhodes College;
   NIH/NCRR; NIH/NIBIB [2P41EB002025]; Division Of Computer and Network
   Systems; Direct For Computer & Info Scie & Enginr [0821640] Funding
   Source: National Science Foundation; Div Of Information & Intelligent
   Systems; Direct For Computer & Info Scie & Enginr [0705863] Funding
   Source: National Science Foundation
FX This material is based on work supported by the National Science
   Foundation under grants 0705863 and 0821640. Any opinions, findings, and
   conclusions or recommendations expressed in this material are those of
   the authors and do not necessarily reflect the views of the sponsors.
   The authors thank the Rhodes College Fellowship program for also
   supporting Stephen Bailey while he was a student there. The authors also
   thank the CISMM project at the University of North Carolina at Chapel
   Hill, supported by NIH/NCRR and NIH/NIBIB award 2P41EB002025, for the
   use of thier VRPN software.
CR [Anonymous], 2006, Proceedings of the 3rd Symposium on Applied Perception in Graphics and Visualization, APGV'06, DOI 10.1145/1140491.1140495
   Bakker NH, 1999, PRESENCE-TELEOP VIRT, V8, P36, DOI 10.1162/105474699566035
   Bowman DA, 1999, PRESENCE-TELEOP VIRT, V8, P618, DOI 10.1162/105474699566521
   Chance SS, 1998, PRESENCE-TELEOP VIRT, V7, P168, DOI 10.1162/105474698565659
   Darken RP, 1996, INT J HUM-COMPUT INT, V8, P49, DOI 10.1080/10447319609526140
   ENGEL D, 2008, P ACM S VIRT REAL SO, P157
   Feasel J, 2008, 3DUI: IEEE SYMPOSIUM ON 3D USER INTERFACES 2008, PROCEEDINGS, P97
   Fernandes KJ, 2003, COMMUN ACM, V46, P141, DOI 10.1145/903893.903929
   Gramann K, 2005, J EXP PSYCHOL HUMAN, V31, P1199, DOI 10.1037/0096-1523.31.6.1199
   Gramann K, 2010, J COGNITIVE NEUROSCI, V22, P2836, DOI 10.1162/jocn.2009.21369
   Interrante V, 2007, 3DUI: IEEE SYMPOSIUM ON 3D USER INTERFACES 2007, PROCEEDINGS, P167
   Klatzky RL, 1998, PSYCHOL SCI, V9, P293, DOI 10.1111/1467-9280.00058
   Lathrop WB, 2002, PRESENCE-TELEOP VIRT, V11, P19, DOI 10.1162/105474602317343631
   LATHROP WB, 2005, PRESENCE, V17, P249
   May M, 2004, COGNITIVE PSYCHOL, V48, P163, DOI 10.1016/S0010-0285(03)00127-0
   Nitzsche N, 2004, PRESENCE-TELEOP VIRT, V13, P44, DOI 10.1162/105474604774048225
   Pausch R., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P399, DOI 10.1145/218380.218495
   Pausch R., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P13, DOI 10.1145/258734.258744
   PERUCH P, 2000, SPATIAL COGNITION, V2
   Plank M, 2010, LECT NOTES ARTIF INT, V6222, P191, DOI 10.1007/978-3-642-14749-4_18
   Plumert JM, 2004, CHILD DEV, V75, P1243, DOI 10.1111/j.1467-8624.2004.00736.x
   Razzaque Sharif., 2001, EUROGRAPHICS SHORT P
   Riecke BE, 2010, LECT NOTES ARTIF INT, V6222, P234, DOI 10.1007/978-3-642-14749-4_21
   Rieser J.J., 2007, EMERGING SPATIAL MIN, P77, DOI DOI 10.1093/ACPROF:OSO/9780195189223.003.0004
   Ruddle RA, 1999, PRESENCE-TELEOP VIRT, V8, P157, DOI 10.1162/105474699566143
   Ruddle RA, 2006, PSYCHOL SCI, V17, P460, DOI 10.1111/j.1467-9280.2006.01728.x
   Ruddle RA, 2009, ACM T COMPUT-HUM INT, V16, DOI 10.1145/1502800.1502805
   SCHWAIGER M, 2007, P IEEE INT WORKSH HA, P50
   Slater Mel, 1995, ACM Transactions on Computer-Human Interaction, V2, P201, DOI DOI 10.1145/210079.210084
   Steinicke F, 2010, IEEE T VIS COMPUT GR, V16, P17, DOI 10.1109/TVCG.2009.62
   Suma EA, 2007, 3DUI: IEEE SYMPOSIUM ON 3D USER INTERFACES 2007, PROCEEDINGS, P147
   Templeman JN, 1999, PRESENCE-TELEOP VIRT, V8, P598, DOI 10.1162/105474699566512
   Usoh M, 1999, COMP GRAPH, P359, DOI 10.1145/311535.311589
   Waller D, 1998, PRESENCE-TELEOP VIRT, V7, P129, DOI 10.1162/105474698565631
   Wartenberg F., 1998, Spatial Cognition. An Interdisciplinary Approach to Representing and Processing Spatial Knowledge, P469
   Wendt JD, 2010, P IEEE VIRT REAL ANN, P51
   Williams B, 2007, ACM T APPL PERCEPT, V4, DOI 10.1145/1265957.1265961
   XIE X, 2010, P 7 S APPL PERC GRAP, P65
   2011, ACM T APPL PERCEPTIO, V8
NR 39
TC 49
Z9 57
U1 0
U2 11
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD AUG
PY 2011
VL 8
IS 3
AR 19
DI 10.1145/2010325.2010329
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 819HK
UT WOS:000294815800004
DA 2024-07-18
ER

PT J
AU Blank, A
   Okamura, AM
   Kuchenbecker, KJ
AF Blank, Amy
   Okamura, Allison M.
   Kuchenbecker, Katherine J.
TI Identifying the Role of Proprioception in Upper-Limb Prosthesis Control:
   Studies on Targeted Motion
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Design; Experimentation; Human Factors; Human psychophysics; motion
   control; proprioception; prosthetic limb control; vision
ID SENSORY FEEDBACK; MOTOR CONTROL; MOVEMENT
AB Proprioception plays a crucial role in enabling humans to move purposively and interact with their physical surroundings. Current technology in upper-limb prostheses, while beginning to incorporate some haptic feedback, does not provide amputees with proprioceptive information about the state of the limb. Thus, the wearer must visually monitor the limb, which is often inconvenient or even impossible for some tasks. This work seeks to quantify the potential benefits of incorporating proprioceptive motion feedback into upper-limb prosthesis designs. We apply a noninvasive method for controlling the availability of proprioceptive motion feedback in unimpaired individuals in a human subject study to compare the benefits of visual and proprioceptive motion feedback in targeted motion tasks. Combined results of the current study and our previous study using a different task indicate that the addition of proprioceptive motion feedback improves targeting accuracy under nonsighted conditions and, for some tasks, under sighted conditions as well. This work motivates the development of methods for providing artificial proprioceptive feedback to a prosthesis wearer.
C1 [Blank, Amy; Okamura, Allison M.] Johns Hopkins Univ, Lab Computat Sensing & Robot, Baltimore, MD 21218 USA.
   [Kuchenbecker, Katherine J.] Univ Penn, Philadelphia, PA 19104 USA.
C3 Johns Hopkins University; University of Pennsylvania
RP Blank, A (corresponding author), Johns Hopkins Univ, Lab Computat Sensing & Robot, Baltimore, MD 21218 USA.
RI Okamura, Allison M/A-3323-2010
OI Kuchenbecker, Katherine J./0000-0002-5004-0313; Okamura,
   Allison/0000-0002-6912-1666
FU Johns Hopkins University; University of Pennsylvania; NSF; Johns Hopkins
   University Applied Physics Laboratory [N66001-06-C-8005]
FX This work was supported by the Johns Hopkins University, the University
   of Pennsylvania, an NSF Graduate Research Fellowship, and the Johns
   Hopkins University Applied Physics Laboratory under the DARPA
   Revolutionizing Prosthetics program, contract N66001-06-C-8005.
CR [Anonymous], 2002, Proc. Eurohaptics
   Atkins DJ., 1996, JPO: Journal of Prosthetics and Orthotics, V8, P2, DOI DOI 10.1097/00008526-199601000-00003
   Bark K, 2008, SYMPOSIUM ON HAPTICS INTERFACES FOR VIRTUAL ENVIRONMENT AND TELEOPERATOR SYSTEMS 2008, PROCEEDINGS, P71
   Brewer BR, 2005, IEEE T NEUR SYS REH, V13, P1, DOI 10.1109/TNSRE.2005.843443
   Brown LE, 2003, J NEUROPHYSIOL, V90, P3105, DOI 10.1152/jn.00013.2003
   Carrozza MC, 2003, IEEE INT CONF ROBOT, P2230, DOI 10.1109/ROBOT.2003.1241925
   Collins DF, 1996, J PHYSIOL-LONDON, V496, P857, DOI 10.1113/jphysiol.1996.sp021733
   Collins DF, 2005, J NEUROPHYSIOL, V94, P1699, DOI 10.1152/jn.00191.2005
   Dhillon GS, 2005, IEEE T NEUR SYS REH, V13, P468, DOI 10.1109/TNSRE.2005.856072
   Doubler J A, 1984, J Rehabil Res Dev, V21, P19
   Doubler J A, 1984, J Rehabil Res Dev, V21, P5
   Edin BB, 2004, J NEUROPHYSIOL, V92, P3233, DOI 10.1152/jn.00628.2004
   Ephraim PL, 2003, ARCH PHYS MED REHAB, V84, P747, DOI 10.1016/S0003-9993(03)04932-8
   GHEZ C, 1990, COLD SH Q B, V55, P837
   Kawato M, 1999, CURR OPIN NEUROBIOL, V9, P718, DOI 10.1016/S0959-4388(99)00028-8
   KELSO JAS, 1977, J EXP PSYCHOL HUMAN, V3, P529, DOI 10.1037/0096-1523.3.4.529
   Kuchenbecker KJ, 2007, INT C REHAB ROBOT, P513, DOI 10.1109/ICORR.2007.4428474
   LARISH DD, 1984, J MOTOR BEHAV, V16, P76
   Maxwell S E., 1990, Designing experiments and analysing data: A model comparison approach
   MCCLOSKEY DI, 1978, PHYSIOL REV, V58, P763, DOI 10.1152/physrev.1978.58.4.763
   MEEK S G, 1989, Journal of Rehabilitation Research and Development, V26, P53
   Mendenhall W., 1995, Statistics for Engineering and the Sciences, V4th, P652
   PATTERSON PE, 1992, J REHABIL RES DEV, V29, P1, DOI 10.1682/JRRD.1992.01.0001
   PHILLIPS CA, 1988, CRIT REV BIOMED ENG, V16, P105
   Pylatiuk C, 2004, P ANN INT IEEE EMBS, V26, P4260
   Robles-De-La-Torre G, 2006, IEEE MULTIMEDIA, V13, P24, DOI 10.1109/MMUL.2006.69
   SAINBURG RL, 1993, J NEUROPHYSIOL, V70, P2136, DOI 10.1152/jn.1993.70.5.2136
   Siegel Sidney, 1988, Nonparametric statistics for the behavioral sciences
   Simpson D.C., 1974, The Control of Upper-Limb Prostheses and Orthoses, P136
   van Beers RJ, 1998, EXP BRAIN RES, V122, P367, DOI 10.1007/s002210050525
   WANN JP, 1992, EXP BRAIN RES, V91, P162
   WINDEBANK AJ, 1990, NEUROLOGY, V40, P584, DOI 10.1212/WNL.40.4.584
   WING AM, 1983, Q J EXP PSYCHOL-A, V35, P297, DOI 10.1080/14640748308402135
   Ziegler-Graham K, 2008, ARCH PHYS MED REHAB, V89, P422, DOI 10.1016/j.apmr.2007.11.005
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
NR 41
TC 55
Z9 67
U1 2
U2 31
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD JUN
PY 2010
VL 7
IS 3
AR 15
DI 10.1145/1773965.1773966
PG 23
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 618NF
UT WOS:000279361800001
DA 2024-07-18
ER

PT J
AU McDonnell, R
   Ennis, C
   Dobbyn, S
   O'Sullivan, C
AF McDonnell, Rachel
   Ennis, Cathy
   Dobbyn, Simon
   O'Sullivan, Carol
TI Talking Bodies: Sensitivity to Desynchronization of Conversations
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Experimentation; Human Factors; Perception; graphics; motion capture
ID BIOLOGICAL MOTION
AB In this article, we investigate human sensitivity to the coordination and timing of conversational body language for virtual characters. First, we captured the full body motions (excluding faces and hands) of three actors conversing about a range of topics, in either a polite (i.e., one person talking at a time) or debate/argument style. Stimuli were then created by applying the motion-captured conversations from the actors to virtual characters. In a 2AFC experiment, participants viewed paired sequences of synchronized and desynchronized conversations and were asked to guess which was the real one. Detection performance was above chance for both conversation styles but more so for the polite conversations, where desynchronization was more noticeable.
C1 [McDonnell, Rachel; Ennis, Cathy; Dobbyn, Simon; O'Sullivan, Carol] Trinity Coll Dublin, Coll Green, Graph Vis & Visualisat Grp, Dublin 2, Ireland.
C3 Trinity College Dublin
RP McDonnell, R (corresponding author), Trinity Coll Dublin, Coll Green, Graph Vis & Visualisat Grp, Dublin 2, Ireland.
EM ramcdonn@cs.tcd.ie
RI McDonnell, Rachel/HGC-4337-2022
OI McDonnell, Rachel/0000-0002-1957-2506; Ennis, Cathy/0000-0002-1274-5347;
   O'Sullivan, Carol/0000-0003-3772-4961
CR Cassell J., 2001, Proceedings of SIGGRAPH 2001, P477
   CIECHOMSKI PD, 2005, P 6 INT S VIRT REAL, P1
   ENNIS C, 2008, P 5 S APPL PERC GRAP, P75
   Hannah A, 1999, J LANG SOC PSYCHOL, V18, P153, DOI 10.1177/0261927X99018002002
   Heck R, 2006, COMPUT GRAPH FORUM, V25, P459, DOI 10.1111/j.1467-8659.2006.00965.x
   Ikemoto Leslie., 2004, SCA 2004: Proceedings of the 2004 ACM SIGGRAPH/Eurographics Symposium on Computer Animation, P99
   JOHANSSON G, 1973, PERCEPT PSYCHOPHYS, V14, P201, DOI 10.3758/BF03212378
   Maim J., 2007, P 8 INT S VIRT REAL, P26
   McDonnell R, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360625
   MUSSE SR, 2001, IEEE T VISUALIZATION, V7, P2
   O'Sullivan C, 2002, COMPUT GRAPH FORUM, V21, P733, DOI 10.1111/1467-8659.00631
   OSULLIVAN C, 2002, P EUR IR WORKSH, V21, P15
   Paris S, 2007, COMPUT GRAPH FORUM, V26, P665, DOI 10.1111/j.1467-8659.2007.01090.x
   Peters C, 2008, P EUR SHORT PAP, P227
   Peters C, 2009, IEEE COMPUT GRAPH, V29, P54, DOI 10.1109/MCG.2009.69
   Rose D, 2009, PERCEPTION, V38, P153, DOI 10.1068/p6279
   THALMANN D, 2001, LECT NOTES ARTIF INT, V2190, P1
NR 17
TC 9
Z9 9
U1 0
U2 2
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD SEP
PY 2009
VL 6
IS 4
AR 22
DI 10.1145/1609967.1609969
PG 8
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 511ZV
UT WOS:000271212300002
DA 2024-07-18
ER

PT J
AU Harper, S
   Michailidou, E
   Stevens, R
AF Harper, Simon
   Michailidou, Eleni
   Stevens, Robert
TI Toward a Definition of Visual Complexity as an Implicit Measure of
   Cognitive Load
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Measurement; Theory; Design; Human Factors; Web accessibility; semantic
   Web; visual impairment; visual complexity; knowledge elicitation
ID EYE-MOVEMENTS; PICTURES; DIMENSIONS; MEMORY
AB The visual complexity of Web pages is much talked about; "complex Web pages are difficult to use," but often regarded as a subjective decision by the user. This subjective decision is of limited use if we wish to understand the importance of visual complexity, what it means, and how it can be used. We theorize that by understanding a user's visual perception of Web page complexity, we can understand the cognitive effort required for interaction with that page. This is important because by using an easily identifiable measure, such as visual complexity, as an implicit marker of cognitive load, we can design Web pages which are easier to interact with. We have devised an initial empirical experiment, using card sorting and triadic elicitation, to test our theories and assumptions, and have built an initial baseline sequence of 20 Web pages along with a library of qualitative and anecdotal feedback. Using this library, we define visual complexity, ergo perceived interaction complexity, and by taking these pages as "prototypes" and ranking them into a sequence of complexity, we are able to group them into: simple, neutral, and complex. This means we can now work toward a definition of visual complexity as an implicit measure of cognitive load.
C1 [Harper, Simon] Univ Manchester, Sch Comp Sci, Informat Management Grp, Manchester M13 9P, Lancs, England.
C3 University of Manchester
RP Harper, S (corresponding author), Univ Manchester, Sch Comp Sci, Informat Management Grp, Kilburn Bldg,Oxford Rd, Manchester M13 9P, Lancs, England.
EM Simon.harper@manchester.ac.uk
RI Harper, Simon/AAD-4759-2021
OI Harper, Simon/0000-0001-9301-5049
CR AMITAY E, 2003, P 14 ACM C HYP HYP H
   [Anonymous], WORKING PAPERS INFOR
   [Anonymous], WEB ACC INCL DIS PEO
   [Anonymous], GROWTH STRUCTURAL FU
   Asakawa C., 2005, P 2005 INT CROSS DIS, P1
   ASAKAWA C, 1998, P CLOS GAP C
   BRAMBRING M, 1984, ELECT SPATIAL SENSIN, P493
   Cheng LS, 1997, MICROPOROUS MATER, V8, P177, DOI 10.1016/S0927-6513(96)00067-3
   DAVIS B, 1993, P C HUM FACT COMP SY
   Faraday P., 1998, Proceedings ACM Multimedia 98, P29, DOI 10.1145/290747.290752
   FISCHER S, 1996, MULTIMEDIA AUTHORING
   Fodor J, 1996, COGNITION, V58, P253, DOI 10.1016/0010-0277(95)00694-X
   Freedman M., 1984, Proceedings of the International Congress on Technology and Technology Exchange: Technology and the World Around Us - ICTTE '84, and EMC '84 - Management of Technology and its Limitations, P34
   FRIEDMAN A, 1979, J EXP PSYCHOL GEN, V108, P316, DOI 10.1037/0096-3445.108.3.316
   FURUTA R, 1997, P 8 ACM C HYP HYP
   Granka L. A., 2004, Proceedings of Sheffield SIGIR 2004. The Twenty-Seventh Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P478, DOI 10.1145/1008992.1009079
   Harper Simon., 2005, P 7 INT ACM SIGACCES, P90, DOI DOI 10.1145/1090785.1090804
   Heaps C, 1999, J EXP PSYCHOL HUMAN, V25, P299, DOI 10.1037/0096-1523.25.2.299
   Henderson JM, 1999, J EXP PSYCHOL HUMAN, V25, P210, DOI 10.1037/0096-1523.25.1.210
   Hoffman R., 2004, P SAICSIT, P205
   HOFFOS S, 1991, CD 1 DESIGNERS GUIDE
   Horton RN, 2006, BMC MICROBIOL, V6, DOI 10.1186/1471-2180-6-5
   Ivory M.Y., 2000, P 6 C HUM FACT WEB
   Ivory MY, 2005, ACM T INFORM SYST, V23, P463, DOI 10.1145/1095872.1095876
   IVORY MY, 2001, P C HUM FACT COMP SY, V1, P53
   Jacob R. J. K., 1995, VIRTUAL ENV ADV INTE, P258, DOI [DOI 10.1093/OSO/9780195075557.003.0015, 10.5555/216164.216178]
   JACOB RJK, 1991, ACM T INFORM SYST, V9, P152, DOI 10.1145/123078.128728
   JAY C, 2006, P INT CROSS DISC WOR, P113
   JUST MA, 1980, PSYCHOL REV, V87, P329, DOI 10.1037/0033-295X.87.4.329
   Just Marcel., 1987, The Psychology of Reading and Language Comprehension
   KAMP H, 1995, COGNITION, V57, P129, DOI 10.1016/0010-0277(94)00659-9
   KRAUSS K, 2004, P SO AFR COMP LECT A
   Lavie T, 2004, INT J HUM-COMPUT ST, V60, P269, DOI 10.1016/j.ijhcs.2003.09.002
   Lie H.W., 1999, Cascading style sheets, designing for the web
   Lohse GL, 1996, ORGAN BEHAV HUM DEC, V68, P28, DOI 10.1006/obhd.1996.0087
   MCCARTHY JD, 2003, P HCI2003, P8
   McConkie GW, 1996, J EXP PSYCHOL HUMAN, V22, P563, DOI 10.1037/0096-1523.22.3.563
   Muller-Brockmann J., 1981, Grid Systems in Graphic Design
   Oliva A., 2004, P 26 ANN M COGN SCI
   Oliva Aude, 2005, P251, DOI 10.1016/B978-012375731-9/50045-8
   Outing S., 2006, EYETRACK 3 WHAT WE S
   PLESSERS P, 2005, P 14 INT C WORLDWIDE
   POTTER MC, 1976, J EXP PSYCHOL-HUM L, V2, P509, DOI 10.1037/0278-7393.2.5.509
   Rayner K, 1998, PSYCHOL BULL, V124, P372, DOI 10.1037/0033-2909.124.3.372
   *RNIB, 1996, SHORT GUID BLINDN
   RUSSO EJ, 1994, J CONSUM RES, V21
   SANDERS MG, 1979, HUM FACTORS, V21, P369, DOI 10.1177/001872087902100311
   TAYLOR JR, 2001, LINGUISTICS PROTOTYP, P8954
   ZETTL H, 1999, SIGHT SOUND MOTION A
NR 49
TC 75
Z9 80
U1 1
U2 40
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD FEB
PY 2009
VL 6
IS 2
AR 10
DI 10.1145/1498700.1498704
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA 450YN
UT WOS:000266438000004
DA 2024-07-18
ER

PT J
AU Li, L
   Adelstein, BD
   Ellis, SR
AF Li, Li
   Adelstein, Bernard D.
   Ellis, Stephen R.
TI Perception of Image Motion During Head Movement
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Human Factors; Design; Measurement; Performance; Head movement; image
   motion; object motion; motion perception; VE latency
ID SELF-MOTION; SPEED; WALKING
AB We examined human perception of head-referenced image motion during concurrent head movement. The visual stimulus was a checkerboard image in a head mounted display that moved from side-to-side. Observers rated the magnitude of the checkerboard motion while either rotating their head about a vertical axis (yaw), about a horizontal axis (pitch), or holding it still. In Experiment 1, we tested four image oscillation frequencies (0.25, 0.5, 1, and 2 Hz) while holding the head motion frequency constant at 0.5 Hz. In Experiment 2, we tested three head motion frequencies (0.25, 0.5, and 1 Hz) while holding the image oscillation frequency constant at 1 Hz. Across all image and head motion frequencies, perceptual sensitivity to image motion was reduced by about 45% during horizontal head movement. During vertical head movement, perceptual sensitivity was reduced by about 25% when head and image motion were of the same frequency. Compared with when the head was still, horizontal and vertical head movements produced a downward shift of about 10% in overall motion magnitude estimation response. Findings from this study provide virtual environment developers with a quantitative description of the influence of concurrent head movement on the perception of frontoparallel image motion.
C1 [Li, Li] Univ Hong Kong, Dept Psychol, Hong Kong, Hong Kong, Peoples R China.
   [Adelstein, Bernard D.; Ellis, Stephen R.] NASA, Ames Res Ctr, Human Syst Integrat Div, Moffett Field, CA 94035 USA.
C3 University of Hong Kong; National Aeronautics & Space Administration
   (NASA); NASA Ames Research Center
RP Li, L (corresponding author), Univ Hong Kong, Dept Psychol, Pokfulam Rd, Hong Kong, Hong Kong, Peoples R China.
EM lili@hku.hk; Bernard.D.Adelstein@nasa.gov; sellis@mail.arc.nasa.gov
RI Li, Li/D-4924-2009
FU NASA's Space Human Factors Engineering Program; University of Hong Kong
FX This research was supported by NASA's Space Human Factors Engineering
   Program and the University of Hong Kong Seed Funding Program for Basic
   Research.
CR Adelstein B.D., 2005, Proceedings of the Human Factors and Ergonomics Society Annual Meeting, V49, P2221
   Adelstein B. D., 2003, P HUMAN FACTORS ERGO, V47, P2083, DOI [DOI 10.1177/1541931203047020, DOI 10.1177/154193120304702001]
   ADELSTEIN BD, 2006, P 50 ANN M HUM FACT
   [Anonymous], 1962, PHYSL OPTICS
   Azuma R.T., 1994, Proceedings of ACM SIGGRAPH, V94, P197
   Banton T, 2005, PRESENCE-TELEOP VIRT, V14, P394, DOI 10.1162/105474605774785262
   Barlow H.B., 1990, Vision: Coding and effeciency, P363, DOI 10.1017/cbo9780511626197.034
   BERTHOZ A, 1981, ATTENTION PERFORM, V9, P27
   Brooks FP, 1999, IEEE COMPUT GRAPH, V19, P16, DOI 10.1109/38.799723
   CULLEN KE, 1991, EXP BRAIN RES, V83, P237
   DRAZIN DH, 1962, RESEARCH LOND, V15, P275
   Durgin FH, 2005, J EXP PSYCHOL HUMAN, V31, P339, DOI 10.1037/0096-1523.31.2.339
   HEEGER DJ, 1992, VISUAL NEUROSCI, V9, P181, DOI 10.1017/S0952523800009640
   HUDDLEST.JH, 1970, J APPL PSYCHOL, V54, P401, DOI 10.1037/h0029925
   JUNG JY, 2000, P 44 ANN M HUM FACT, P499
   Pelah A, 1996, NATURE, V381, P283, DOI 10.1038/381283a0
   PROBST T, 1982, PERCEPTION, V11, pA33
   PROBST T, 1986, BEHAV BRAIN RES, V22, P1, DOI 10.1016/0166-4328(86)90076-8
   PROBST T, 1984, SCIENCE, V225, P536, DOI 10.1126/science.6740325
   Thurrell A, 2005, P SOC PHOTO-OPT INS, V5666, P434, DOI 10.1117/12.610856
   Thurrell AEI, 1998, PERCEPTION, V27, P147
   WALLACH H, 1974, PERCEPT PSYCHOPHYS, V15, P339, DOI 10.3758/BF03213955
   WALLACH H, 1987, ANNU REV PSYCHOL, V38, P1, DOI 10.1146/annurev.ps.38.020187.000245
NR 23
TC 6
Z9 6
U1 0
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD FEB
PY 2009
VL 6
IS 1
AR 5
DI 10.1145/1462055.1462060
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA 450YM
UT WOS:000266437900005
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Tarr, MJ
   Georghiades, AS
   Jackson, CD
AF Tarr, Michael J.
   Georghiades, Athinodoros S.
   Jackson, Cullen D.
TI Identifying Faces Across Variations in Lighting: Psychophysics and
   Computation
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Experimentation; Performance; Illumination invariance; object
   recognition; human psychophysics; image-based models; face recognition
ID OBJECT RECOGNITION; ILLUMINATION; IMAGES; PERCEPTION; MOTION; SHAPE
AB Humans have the ability to identify objects under varying lighting conditions with extraordinary accuracy. We investigated the behavioral aspects of this ability and compared it to the performance of the illumination cones (IC) model of Belhumeur and Kriegman [1998]. In five experiments, observers learned 10 faces under a small subset of illumination directions. We then tested observers' recognition ability under different illuminations. Across all experiments, recognition performance was found to be dependent on the distance between the trained and tested illumination directions. This effect was modulated by the nature of the trained illumination directions. Generalizations from frontal illuminations were different than generalizations from extreme illuminations. Similarly, the IC model was also sensitive to whether the trained images were near-frontal or extreme. Thus, we find that the nature of the images in the training set affects the accuracy of an object's representation under variable lighting for both humans and the model. Beyond this general correspondence, the microstructure of the generalization patterns for both humans and the IC model were remarkably similar, suggesting that the two systems may employ related algorithms.
C1 [Tarr, Michael J.] Brown Univ, Dept Cognit & Linguist Sci, Providence, RI 02912 USA.
   [Georghiades, Athinodoros S.] Yale Univ, Dept Comp Sci, New Haven, CT 06520 USA.
   [Jackson, Cullen D.] Aptima Inc, Woburn, MA 01801 USA.
C3 Brown University; Yale University; Aptima, Inc.
RP Tarr, MJ (corresponding author), Brown Univ, Dept Cognit & Linguist Sci, 190 Thayer St,Box 1978, Providence, RI 02912 USA.
EM Michael_Tarr@brown.edu; athinodoros.georghiades@yale.edu;
   cjackson@aptima.com
RI Jackson, Cullen D/H-9874-2016; Jackson, Cullen/JAC-5573-2023
OI Jackson, Cullen D/0000-0002-5134-9152; Jackson,
   Cullen/0000-0002-5134-9152
CR [Anonymous], 1991, P 1991 IEEE COMP SOC, DOI DOI 10.1109/CVPR.1991.139758
   [Anonymous], 1971, P IEEE C SYST CONTR
   Atick JJ, 1996, NEURAL COMPUT, V8, P1321, DOI 10.1162/neco.1996.8.6.1321
   Belhumeur PN, 1997, IEEE T PATTERN ANAL, V19, P711, DOI 10.1109/34.598228
   Belhumeur PN, 1998, INT J COMPUT VISION, V28, P245, DOI 10.1023/A:1008005721484
   BIEDERMAN I, 1987, PSYCHOL REV, V94, P115, DOI 10.1037/0033-295X.94.2.115
   BIEDERMAN I, 1993, J EXP PSYCHOL HUMAN, V19, P1162, DOI 10.1037/0096-1523.19.6.1162
   Braje WL, 1998, PSYCHOBIOLOGY, V26, P371
   Braje WL, 2000, PERCEPTION, V29, P383, DOI 10.1068/p3051
   Braje WL, 2003, J VISION, V3, P161, DOI 10.1167/3.2.4
   Fukushima K, 2000, LECT NOTES COMPUT SC, V1811, P623
   Georghiades AS, 1998, PROC CVPR IEEE, P52, DOI 10.1109/CVPR.1998.698587
   Georghiades AS, 2001, IEEE T PATTERN ANAL, V23, P643, DOI 10.1109/34.927464
   Gross R., 2004, HDB FACE RECOGNITION
   Hallinan P., 1999, Two and Three-dimensional Patterns of the Face
   HALLINAN PW, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P995, DOI 10.1109/CVPR.1994.323941
   Hayward WG, 1997, J EXP PSYCHOL HUMAN, V23, P1511, DOI 10.1037/0096-1523.23.5.1511
   Horn B. K. P., 1975, OBTAINING SHAPE SHAD
   JOHNSTON A, 1992, PERCEPTION, V21, P365, DOI 10.1068/p210365
   Kardos L., 1934, Z PSYCHOL, V23
   Kersten D, 1996, NATURE, V379, P31, DOI 10.1038/379031a0
   Kersten D, 2004, ANNU REV PSYCHOL, V55, P271, DOI 10.1146/annurev.psych.55.090902.142005
   Kersten D, 1997, PERCEPTION, V26, P171, DOI 10.1068/p260171
   LADES M, 1993, IEEE T COMPUT, V42, P300, DOI 10.1109/12.210173
   LOGOTHETIS NK, 1995, CEREB CORTEX, V5, P270, DOI 10.1093/cercor/5.3.270
   Lowe DG, 2000, LECT NOTES COMPUT SC, V1811, P20
   MARR D, 1978, PROC R SOC SER B-BIO, V200, P269, DOI 10.1098/rspb.1978.0020
   Marr D., 1982, Visual perception
   Moore C, 1998, COGNITION, V67, P45, DOI 10.1016/S0010-0277(98)00014-6
   Moses Y, 1996, PERCEPTION, V25, P443, DOI 10.1068/p250443
   MOSES Y, 1994, LECT NOTES COMPUTER, V800, P286
   RAMACHANDRAN VS, 1988, NATURE, V331, P163, DOI 10.1038/331163a0
   Riesenhuber M, 2000, LECT NOTES COMPUT SC, V1811, P1
   Sanocki T, 1998, J EXP PSYCHOL HUMAN, V24, P340, DOI 10.1037/0096-1523.24.1.340
   Tarr MJ, 1998, VISION RES, V38, P2259, DOI 10.1016/S0042-6989(98)00041-8
   Tarr MJ, 1998, NAT NEUROSCI, V1, P275, DOI 10.1038/1089
   TARR MJ, 1995, PSYCHON B REV, V2, P55, DOI 10.3758/BF03214412
   Tarr MJ, 1998, COGNITION, V67, P73, DOI 10.1016/S0010-0277(98)00023-7
   TURK M, 1991, J COGNITIVE NEUROSCI, V3, P71, DOI 10.1162/jocn.1991.3.1.71
   Ullman S, 2002, NAT NEUROSCI, V5, P682, DOI 10.1038/nn870
   Ullman S, 2000, LECT NOTES COMPUT SC, V1811, P73
   Vuong QC, 2005, VISION RES, V45, P1213, DOI 10.1016/j.visres.2004.11.015
   Williams P., 2001, RSVP EXPT CONTROL SO
   Zhang L, 2006, IEEE T PATTERN ANAL, V28, P351, DOI 10.1109/TPAMI.2006.53
   Zhou SK, 2007, IEEE T PATTERN ANAL, V29, P230, DOI 10.1109/TPAMI.2007.25
   Zhou SK, 2005, J OPT SOC AM A, V22, P217, DOI 10.1364/JOSAA.22.000217
NR 46
TC 6
Z9 7
U1 0
U2 2
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD MAY
PY 2008
VL 5
IS 2
AR 10
DI 10.1145/1279920.1279924
PG 25
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA 450YJ
UT WOS:000266437600004
DA 2024-07-18
ER

PT J
AU Radun, J
   Leisti, T
   Häkkinen, J
   Ojanen, H
   Olives, JL
   Vuori, T
   Nyman, G
AF Radun, Jenni
   Leisti, Tuomas
   Hakkinen, Jukka
   Ojanen, Harri
   Olives, Jean-Luc
   Vuori, Tero
   Nyman, Gote
TI Content and Quality: Interpretation-Based Estimation of Image Quality
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Human Factors; Experimentation; Measurement; Image quality; subjective
   measurement; qualitative methodology; image contents
AB Test image contents affect subjective image-quality evaluations. Psychometric methods might show that contents have an influence on image quality, but they do not tell what this influence is like, i.e., how the contents influence image quality. To obtain a holistic description of subjective image quality, we have used an interpretation-based quality (IBQ) estimation approach, which combines qualitative and quantitative methodology. The method enables simultaneous examination of psychometric results and the subjective meanings related to the perceived image-quality changes. In this way, the relationship between subjective feature detection, subjective preferences, and interpretations are revealed. We report a study that shows that different impressions are conveyed in five test image contents after similar sharpness variations. Thirty naive observers classified and freely described the images after which magnitude estimation was used to verify that they distinguished the changes in the images. The data suggest that in the case of high image quality, the test image selection is crucial. If subjective evaluation is limited only to technical defects in test images, important subjective information of image-quality experience is lost. The approach described here can be used to examine image quality and it will help image scientists to evaluate their test images.
C1 [Radun, Jenni; Leisti, Tuomas; Nyman, Gote] Univ Helsinki, Dept Psychol, FIN-00014 Helsinki, Finland.
   [Hakkinen, Jukka; Ojanen, Harri; Olives, Jean-Luc; Vuori, Tero] Nokia Electr Ltd, FIN-00045 Helsinki, Finland.
C3 University of Helsinki; Nokia Corporation
RP Radun, J (corresponding author), Univ Helsinki, Dept Psychol, FIN-00014 Helsinki, Finland.
EM Jenni.radun@helsinki.fi
RI Häkkinen, Jukka/D-2271-2009; Radun, Jenni/AAB-3943-2021; Leisti,
   Tuomas/AAF-1709-2020; Häkkinen, Jukka/A-4122-2019
OI Radun, Jenni/0000-0003-3269-2999; Leisti, Tuomas/0000-0002-9234-2854;
   Häkkinen, Jukka/0000-0003-0215-2238
FU Esa Torniainen (M-real)
FX We thank M-real for the support in developing the methodology and Esa
   Torniainen (M-real) for collaboration in the Visual Quality project.
CR [Anonymous], 2002, IEEE INT C AC SPEECH
   [Anonymous], 2000, Psychometric scaling, a toolkit for imaging systems development
   Avadhanam N, 1998, SPIE PROC SER, V3308, P44, DOI 10.1117/12.302437
   Bakeman R., 1986, OBSERVING INTERACTIO
   Barten Peter G. J, 1999, Contrast sensitivity of the human eye and its effects on image quality
   Bech S, 1996, P SOC PHOTO-OPT INS, V2657, P317, DOI 10.1117/12.238728
   BOREMAN G, 2001, MODULATION TRANSF TT, V52
   CICCHETTI DV, 1994, PSYCHOL ASSESSMENT, V6, P284, DOI DOI 10.1037/1040-3590.6.4.284
   Cui CW, 2004, P SOC PHOTO-OPT INS, V5294, P132
   Faye P, 2004, FOOD QUAL PREFER, V15, P781, DOI 10.1016/j.foodqual.2004.04.009
   Fedorovskaya EA, 1997, COLOR RES APPL, V22, P96, DOI 10.1002/(SICI)1520-6378(199704)22:2<96::AID-COL5>3.0.CO;2-Z
   Hair JF, 2010, Multivariate data analysis
   Heynderickx I, 2002, P SOC PHOTO-OPT INS, V4662, P129, DOI 10.1117/12.469509
   *ISO, 1997, 12640 ISO
   Keelan B., 2002, Handbook of Image Quality: Characterization and Prediction
   Keelan BW, 2004, P SOC PHOTO-OPT INS, V5294, P181
   Nijenhuis M, 1997, P SOC PHOTO-OPT INS, V3025, P173, DOI 10.1117/12.270052
   NYMAN G, 2006, P SOC PHOTO-OPT INS, V6059, P1
   NYMAN G, 2002, PPA LOND UK
   NYMAN G, 2005, P 12 INT DISPL WORKS, P1825
   Picard D, 2003, ACTA PSYCHOL, V114, P165, DOI 10.1016/j.actpsy.2003.08.001
   Silverstein DA., 1996, RELATIONSHIP IMAGE F, P881
   Strauss E, 1998, CLIN ORTHOP RELAT R, P2
   Video Quality Experts Group, 2000, Final report from the video quality experts group on the validation of objective models of video quality assessment march 2000
   Yendrikhovskij S, 1999, IMAGING SCI J, V47, P197, DOI 10.1080/13682199.1999.11736360
NR 25
TC 31
Z9 32
U1 1
U2 7
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD JAN
PY 2008
VL 4
IS 4
AR 21
DI 10.1145/1278760.1278762
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Arts &amp; Humanities Citation Index (A&amp;HCI)
SC Computer Science
GA 450YH
UT WOS:000266437400002
DA 2024-07-18
ER

PT J
AU Wallraven, C
   Breidt, M
   Cunningham, DW
   Bülthoff, HH
AF Wallraven, Christian
   Breidt, Martin
   Cunningham, Douglas W.
   Buelthoff, Heinrich H.
TI Evaluating the Perceptual Realism of Animated Facial Expressions
SO ACM TRANSACTIONS ON APPLIED PERCEPTION
LA English
DT Article
DE Experimentation; Evalution of facial animations; 3D-scanning; avatar;
   recognition; psychophysics; perceptually adaptive graphics
ID STATE
AB The human face is capable of producing an astonishing variety of expressions-expressions for which sometimes the smallest difference changes the perceived meaning considerably. Producing realistic-looking facial animations that are able to transmit this degree of complexity continues to be a challenging research topic in computer graphics. One important question that remains to be answered is: When are facial animations good enough? Here we present an integrated framework in which psychophysical experiments are used in a first step to systematically evaluate the perceptual quality of several different computer-generated animations with respect to real-world video sequences. The first experiment provides an evaluation of several animation techniques, exposing specific animation parameters that are important to achieve perceptual fidelity. In a second experiment, we then use these benchmarked animation techniques in the context of perceptual research in order to systematically investigate the spatiotemporal characteristics of expressions. A third and final experiment uses the quality measures that were developed in the first two experiments to examine the perceptual impact of changing facial features to improve the animation techniques. Using such an integrated approach, we are able to provide important insights into facial expressions for both the perceptual and computer graphics community.
C1 [Wallraven, Christian; Breidt, Martin; Cunningham, Douglas W.; Buelthoff, Heinrich H.] Max Planck Inst Biol Cybernet, Tubingen, Germany.
C3 Max Planck Society
RP Wallraven, C (corresponding author), Max Planck Inst Biol Cybernet, Tubingen, Germany.
EM Christian.wallraven@tuebingen.mpg.de
RI Bülthoff, Heinrich/AAC-8818-2019; Bülthoff, Heinrich H/J-6579-2012
OI Bülthoff, Heinrich H/0000-0003-2568-0607
CR Adolphs Ralph, 2002, Behav Cogn Neurosci Rev, V1, P21, DOI 10.1177/1534582302001001003
   [Anonymous], P SIG CHI C HUM FACT
   [Anonymous], 2005, SOCIAL DIALOGUE EMBO, DOI [DOI 10.1007/1-4020-3933-6_2, 10.1007/1-4020-3933-6_2]
   [Anonymous], 2003, SIGGRAPH 03 SKETCHES
   Blanz V, 1999, COMP GRAPH, P187, DOI 10.1145/311535.311556
   Blanz V, 2003, COMPUT GRAPH FORUM, V22, P641, DOI 10.1111/1467-8659.t01-1-00712
   Bull P, 2001, PSYCHOLOGIST, V14, P644
   Calder AJ, 2005, NAT REV NEUROSCI, V6, P641, DOI 10.1038/nrn1724
   COSKER D, 2004, APGV 2004, P151
   CUNNINGHAM D, 2004, APGV 2004 S APPL PER, P143
   Cunningham DW, 2003, COMP ANIM CONF PROC, P23, DOI 10.1109/CASA.2003.1199300
   Ekman P., 1972, UNIVERSALS CULTURAL, P207
   Ezzat T, 2002, ACM T GRAPHIC, V21, P388, DOI 10.1145/566570.566594
   Fleming J, 2004, Employment Relations Record, V4, P1
   Guenin BM, 1998, P IEEE SEMICOND THER, P55, DOI 10.1109/STHERM.1998.660387
   ITTI L, 2004, APPL SCI NEURAL NETW, V6, P64
   Kajiya J. T., 1986, Computer Graphics, V20, P143, DOI 10.1145/15886.15902
   Knappmeyer B, 2003, VISION RES, V43, P1921, DOI 10.1016/S0042-6989(03)00236-0
   KOHLER K, 2002, SCA 02, P55
   Langer MS, 2000, SPRING COMP SCI, P1
   Langton SRH, 2000, TRENDS COGN SCI, V4, P50, DOI 10.1016/S1364-6613(99)01436-9
   Lee W, 2001, DISCEX'01: DARPA INFORMATION SURVIVABILITY CONFERENCE & EXPOSITION II, VOL I, PROCEEDINGS, P89, DOI 10.1109/DISCEX.2001.932195
   Magnenat-Thalmann N., 1988, Visual Computer, V3, P290, DOI 10.1007/BF01914864
   Myszkowski K, 2001, COMP GRAPH, P221, DOI 10.1145/383259.383284
   OSULLIVAN C, 2004, EUROGRAPHICS 04
   PARKS DA, 1982, GASTROENTEROLOGY, V82, P9
   Rizzo AA, 2001, CYBERPSYCHOL BEHAV, V4, P471, DOI 10.1089/109493101750527033
   Stokes WA, 2004, ACM T GRAPHIC, V23, P742, DOI 10.1145/1015706.1015795
   Terzopoulos D., 1990, Journal of Visualization and Computer Animation, V1, P73, DOI 10.1002/vis.4340010208
   Williams L., 1990, Computer Graphics, V24, P235, DOI 10.1145/97880.97906
NR 30
TC 41
Z9 43
U1 0
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1544-3558
EI 1544-3965
J9 ACM T APPL PERCEPT
JI ACM Trans. Appl. Percept.
PD JAN
PY 2008
VL 4
IS 4
AR 23
DI 10.1145/1278760.1278764
PG 20
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 450YH
UT WOS:000266437400004
DA 2024-07-18
ER

EF