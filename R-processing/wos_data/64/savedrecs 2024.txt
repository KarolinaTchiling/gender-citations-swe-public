FN Clarivate Analytics Web of Science
VR 1.0
PT J
AU Gavane, A
   Watson, B
AF Gavane, Ajinkya
   Watson, Benjamin
TI Light Field Display Point Rendering
SO PROCEEDINGS OF THE ACM ON COMPUTER GRAPHICS AND INTERACTIVE TECHNIQUES
LA English
DT Article
DE Point Rendering; Multiview Rendering; Light Field Rendering; Light Field
   Display; Multiview Mipmapping
AB Rendering for light field displays (LFDs) requires rendering of dozens or hundreds of views, which must then be combined into a single image on the display, making real-time LFD rendering extremely difficult. We introduce light field display point rendering (LFDPR), which meets these challenges by improving eye-based point rendering [Gavane and Watson 2023] with texture-based splatting, which avoids oversampling of triangles mapped to only a few texels; and with LFD-biased sampling, which adjusts horizontal and vertical triangle sampling to match the sampling of the LFD itself. To improve image quality, we introduce multiview mipmapping, which reduces texture aliasing even though compute shaders do not support hardware mipmapping. We also introduce angular supersampling and reconstruction to combat LFD view aliasing and crosstalk. The resulting LFDPR is 2-8x times faster than multiview rendering, with similar comparable quality.
C1 [Gavane, Ajinkya; Watson, Benjamin] North Carolina State Univ, Raleigh, NC 27695 USA.
C3 North Carolina State University
RP Gavane, A (corresponding author), North Carolina State Univ, Raleigh, NC 27695 USA.
EM asgavane@ncsu.edu; bwatson@ncsu.edu
FU National Science Foundation [2008590]
FX We would like to extend our heartfelt gratitude to Dr. Hee-Jin Choi for
   generously providing the LFD prototype display crucial for our research.
   This material is based upon work supported by the National Science
   Foundation under Grant No. - 2008590.
CR Adelson E. H., 1991, PLENOPTIC FUNCTION E, V2
   [Anonymous], 2018, REAL TIME RENDERING
   aye3d, 2022, 3D Monitor without Galsses
   Behrens Colin, 2020, Coconut Scene
   Bemana M, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417827
   CROW FC, 1977, COMMUN ACM, V20, P799, DOI 10.1145/359863.359869
   De Sorbier F., 2010, Computer Games, Multimedia and Allied Technology, P7
   dimenco, 2024, Dimenco-Products
   Fink L, 2023, P ACM COMPUT GRAPH, V6, DOI 10.1145/3585498
   Flynn J, 2019, PROC CVPR IEEE, P2362, DOI 10.1109/CVPR.2019.00247
   Gavane A, 2023, P ACM COMPUT GRAPH, V6, DOI 10.1145/3585513
   Gavane Ajinkya, 2022, Improving View Independent Rendering for Multiview Effects
   Gershun A, 1939, J. Math. Phys, V18, P51, DOI [10.1002/sapm193918151, DOI 10.1002/SAPM193918151]
   Gortler S. J., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P43, DOI 10.1145/237170.237200
   Gross M., 2011, Point-based graphics
   Heusser J., 2018, Light field labs: 3d holograms no glasses deep dive
   HUBNER T., 2006, Proceedings Conference on Computer Graphics and Interactive Techniques in Australasia and South-East Asia (GRAPHITE 2006), P285
   Hubner T., 2007, Single-pass multi-view volume rendering
   Ives Frederic E, 1903, US Patent, Patent No. [725,567, 725567]
   Lanman D, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508366
   Levoy M., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P31, DOI 10.1145/237170.237199
   Levoy Marc, 1985, Technical Report TR 85-022
   lookingglassfactory, 2024, Documentation-Looking Glass Factory
   Mantiuk RK, 2023, Arxiv, DOI arXiv:2304.13625
   Marrs A., 2017, Eurographics (Short Papers), P17
   Marwah K, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461914
   Matusik W, 2004, ACM T GRAPHIC, V23, P814, DOI 10.1145/1015706.1015805
   McGuire Morgan, 2017, Computer Graphics Archive
   Mildenhall B, 2022, COMMUN ACM, V65, P99, DOI 10.1145/3503250
   Ng R., 2005, Comput. Sci. Tech. Rep. (CSTR), V2, P1, DOI DOI 10.1145/3097571
   Perez-Ortiz M, 2020, IEEE T IMAGE PROCESS, V29, P1139, DOI 10.1109/TIP.2019.2936103
   Pharr M., 2016, Physically based rendering: From theory to implementation
   Ritschel T, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409082
   Ritschel T, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618478
   Ritschel T, 2011, COMPUT GRAPH FORUM, V30, P2258, DOI 10.1111/j.1467-8659.2011.01998.x
   Schutz M, 2021, Arxiv, DOI arXiv:2104.07526
   Suhail M, 2022, PROC CVPR IEEE, P8259, DOI 10.1109/CVPR52688.2022.00809
   Unterguggenberger J., 2020, EGPGV@ Eurographics/EuroVis, P13
   Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wilburn B, 2005, ACM T GRAPHIC, V24, P765, DOI 10.1145/1073204.1073259
   Wizadwongsa S, 2021, PROC CVPR IEEE, P8530, DOI 10.1109/CVPR46437.2021.00843
   Yu XB, 2014, CHIN OPT LETT, V12, DOI 10.3788/COL201412.060008
NR 43
TC 0
Z9 0
U1 2
U2 2
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
EI 2577-6193
J9 P ACM COMPUT GRAPH
JI P. ACM Comput. Graph. Interact. Tech.
PD MAY
PY 2024
VL 7
IS 1
AR 19
DI 10.1145/3651300
PG 18
WC Computer Science, Software Engineering
WE Emerging Sources Citation Index (ESCI)
SC Computer Science
GA RQ5P8
UT WOS:001229142400019
DA 2024-08-05
ER

PT J
AU Yuksel, C
AF Yuksel, Cem
TI Skill-Based Matchmaking for Competitive Two-Player Games
SO PROCEEDINGS OF THE ACM ON COMPUTER GRAPHICS AND INTERACTIVE TECHNIQUES
LA English
DT Article
DE Matchmaking; competitive games; rating estimation
AB Skill-based matchmaking is a crucial component of competitive multiplayer games and it is directly tied to how the players would enjoy the game. We present a simple matchmaking algorithm that aims to achieve a target win rate for all players, making long win/loss streaks less probable. It is based on the estimated skill levels of players. Therefore, we also present a rating estimation for players that does not require any game-specific information and purely relies on game outcomes. Our evaluation shows that our methods are effective in estimating a player's rating, responding to changes in rating, and achieving a desirable win rate that avoids long win/loss streaks in competitive two-player games.
C1 [Yuksel, Cem] Univ Utah, Salt Lake City, UT 84112 USA.
   [Yuksel, Cem] Roblox, San Mateo, CA 94403 USA.
C3 Utah System of Higher Education; University of Utah
RP Yuksel, C (corresponding author), Univ Utah, Salt Lake City, UT 84112 USA.; Yuksel, C (corresponding author), Roblox, San Mateo, CA 94403 USA.
EM cem@cemyuksel.com
CR [Anonymous], 1995, American Chess Journal
   Chen S, 2016, PROCEEDINGS OF THE NINTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING (WSDM'16), P227, DOI 10.1145/2835776.2835787
   Chen ZX, 2017, Arxiv, DOI arXiv:1702.06253
   Ebtekar A, 2021, PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE 2021 (WWW 2021), P1772, DOI 10.1145/3442381.3450091
   Elo AE, 1978, The rating of chessplayers: Past and present
   Glickman M.E., 1999, Chance, V12, P21, DOI DOI 10.1080/09332480.1999.10542153
   Glickman ME, 1999, J ROY STAT SOC C-APP, V48, P377, DOI 10.1111/1467-9876.00159
   Glickman ME, 2001, J APPL STAT, V28, P673, DOI 10.1080/02664760120059219
   Gong LX, 2020, KDD '20: PROCEEDINGS OF THE 26TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P2300, DOI 10.1145/3394486.3403279
   Herbrich Ralf, 2007, Advances in Neural Information Processing Systems, V19, DOI [10.7551/mitpress/7503, DOI 10.7551/MITPRESS/7503]
   Huang TK, 2008, J MACH LEARN RES, V9, P2187
   Kovalchik S, 2020, INT J FORECASTING, V36, P1329, DOI 10.1016/j.ijforecast.2020.01.006
   Kschischang FR, 2001, IEEE T INFORM THEORY, V47, P498, DOI 10.1109/18.910572
   Li YZ, 2018, ADV NEUR IN, V31
   Menke JE, 2008, NEURAL COMPUT APPL, V17, P175, DOI 10.1007/s00521-006-0080-8
   Minka T., 2018, Trueskill 2: An improved bayesian skill rating system
   Powell B, 2023, J QUANT ANAL SPORTS, V19, P223, DOI 10.1515/jqas-2023-0004
   Rezapour MM, 2024, MULTIMED TOOLS APPL, V83, P31049, DOI 10.1007/s11042-023-15796-x
   Sonas Jeff, 2002, The Sonas rating formula-better than Elo?
   Weng RC, 2011, J MACH LEARN RES, V12, P267
   WEST DHD, 1979, COMMUN ACM, V22, P532, DOI 10.1145/359146.359153
   Zhang Chaoyun, 2022, CIKM '22: Proceedings of the 31st ACM International Conference on Information & Knowledge Management, P3644, DOI 10.1145/3511808.3557070
NR 22
TC 0
Z9 0
U1 0
U2 0
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
EI 2577-6193
J9 P ACM COMPUT GRAPH
JI P. ACM Comput. Graph. Interact. Tech.
PD MAY
PY 2024
VL 7
IS 1
AR 17
DI 10.1145/3651303
PG 19
WC Computer Science, Software Engineering
WE Emerging Sources Citation Index (ESCI)
SC Computer Science
GA RQ5P8
UT WOS:001229142400007
OA hybrid
DA 2024-08-05
ER

PT J
AU Deliot, T
   Heitz, E
   Belcour, L
AF Deliot, Thomas
   Heitz, Eric
   Belcour, Laurent
TI Transforming a Non-Differentiable Rasterizer into a Differentiable One
   with Stochastic Gradient Estimation
SO PROCEEDINGS OF THE ACM ON COMPUTER GRAPHICS AND INTERACTIVE TECHNIQUES
LA English
DT Article
DE Differentiable rendering; rasterization
AB We show how to transform a non-differentiable rasterizer into a differentiable one with minimal engineering efforts and no external dependencies (no Pytorch/Tensorflow). We rely on Stochastic Gradient Estimation, a technique that consists of rasterizing after randomly perturbing the scene's parameters such that their gradient can be stochastically estimated and descended. This method is simple and robust but does not scale in dimensionality (number of scene parameters). Our insight is that the number of parameters contributing to a given rasterized pixel is bounded. Estimating and averaging gradients on a per-pixel basis hence bounds the dimensionality of the underlying optimization problem and makes the method scalable. Furthermore, it is simple to track per-pixel contributing parameters by rasterizing ID- and UV-buffers, which are trivial additions to a rasterization engine if not already available. With these minor modifications, we obtain an in-engine optimizer for 3D assets with millions of geometry and texture parameters.
C1 [Deliot, Thomas; Heitz, Eric; Belcour, Laurent] Intel Corp, Grenoble, France.
C3 Intel Corporation
RP Deliot, T (corresponding author), Intel Corp, Grenoble, France.
EM thomas.deliot@intel.com; eric.heitz@intel.com; laurent.belcour@intel.com
CR Azinovic D, 2019, PROC CVPR IEEE, P2442, DOI 10.1109/CVPR.2019.00255
   Bangaru Sai, 2023, ACM Transactions on Graphics (SIGGRAPH Asia), V42, P1
   Bangaru SP, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417833
   Berthet Quentin, 2020, P 34 INT C NEUR INF
   CATMULL E, 1978, COMPUT AIDED DESIGN, V10, P350, DOI 10.1016/0010-4485(78)90110-0
   Dupuy J, 2021, COMPUT GRAPH FORUM, V40, P57, DOI 10.1111/cgf.14381
   Fischer M, 2023, PROC CVPR IEEE, P4285, DOI 10.1109/CVPR52729.2023.00417
   Fu Michael, 2005, Technical report
   Glasserman P., 1991, Gradient estimation via Perturbation Analysis
   Hasselgren Jon, 2021, EUROGRAPHICS S REND, P85
   Jakob W, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530099
   Jarzynski Mark, 2020, Journal of Computer Graphics Techniques (JCGT), V9, P20
   Kato H, 2019, PROC CVPR IEEE, P9770, DOI 10.1109/CVPR.2019.01001
   Kato H, 2018, PROC CVPR IEEE, P3907, DOI 10.1109/CVPR.2018.00411
   Kerbl B, 2023, ACM T GRAPHIC, V42, DOI 10.1145/3592433
   Kingma D. P., 2014, arXiv
   Laine S, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417861
   Le Lidec Q, 2021, ADV NEUR IN, V34
   Li TM, 2018, SIGGRAPH ASIA'18: SIGGRAPH ASIA 2018 TECHNICAL PAPERS, DOI 10.1145/3272127.3275109
   Liu SC, 2019, IEEE I CONF COMP VIS, P7707, DOI 10.1109/ICCV.2019.00780
   Loper MM, 2014, LECT NOTES COMPUT SC, V8695, P154, DOI 10.1007/978-3-319-10584-0_11
   Loubet G, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356510
   Nimier-David M, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356498
   Patelli E, 2010, INT J NUMER METH ENG, V81, P172, DOI 10.1002/nme.2687
   Rhodin H, 2015, IEEE I CONF COMP VIS, P765, DOI 10.1109/ICCV.2015.94
   Vicini D, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459804
   Wu SZ, 2023, IEEE T PATTERN ANAL, V45, P5268, DOI 10.1109/TPAMI.2021.3076536
   Yan K, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530080
   Zhang C, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392383
NR 29
TC 0
Z9 0
U1 0
U2 0
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
EI 2577-6193
J9 P ACM COMPUT GRAPH
JI P. ACM Comput. Graph. Interact. Tech.
PD MAY
PY 2024
VL 7
IS 1
AR 3
DI 10.1145/3651298
PG 16
WC Computer Science, Software Engineering
WE Emerging Sources Citation Index (ESCI)
SC Computer Science
GA RQ5P8
UT WOS:001229142400003
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Liu, ZT
   Huang, YK
   Liu, LG
AF Liu, Zitan
   Huang, Yikai
   Liu, Ligang
TI ShaderPerFormer: Platform-independent Context-aware Shader Performance
   Predictor
SO PROCEEDINGS OF THE ACM ON COMPUTER GRAPHICS AND INTERACTIVE TECHNIQUES
LA English
DT Article
DE shader performance prediction; performance modeling; GPU
ID ROOFLINE; MODEL
AB The ability to model and predict the execution time of GPU computations is crucial for real-time graphics application development and optimization. While there are many existing methodologies for graphics programmers to provide such estimates, those methods are often vendor-dependent, require the platforms to be tested, or fail to capture the contextual influences among shader instructions. To address this challenge, we propose ShaderPerFormer, a platform-independent, context-aware deep-learning approach to model GPU performance and provide end-to-end performance predictions on a per-shader basis. To provide more accurate predictions, our method contains a separate stage to gather platform-independent shader program trace information. We also provide a dataset consisting of a total of 54,667 fragment shader performance samples on 5 different platforms. Compared to the PILR and SH baseline methods, our approach reduces the average MAPE across five platforms by 8.26% and 25.25%, respectively.
C1 [Liu, Zitan; Huang, Yikai; Liu, Ligang] Univ Sci & Technol China, Hefei, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS
RP Liu, ZT (corresponding author), Univ Sci & Technol China, Hefei, Peoples R China.
EM jauntyliu@mail.ustc.edu.cn; earendil@mail.ustc.edu.cn; lgliu@ustc.edu.cn
FU National Key R&D Program of China [2022YFB3303400]; National Natural
   Science Foundation of China [62025207]
FX We thank the anonymous reviewers for their valuable comments and
   suggestions. We thank authors on Shadertoy website for creating
   beautiful shaders. We also thank Jiyan He (University of Science and
   Technology of China) for providing helpful insights into this problem.
   This work is supported by the National Key R&D Program of China
   (2022YFB3303400) and the National Natural Science Foundation of China
   (62025207).
CR Abel A, 2022, PROCEEDINGS OF THE 36TH ACM INTERNATIONAL CONFERENCE ON SUPERCOMPUTING, ICS 2022, DOI 10.1145/3524059.3532396
   Abel A, 2023, I S WORKL CHAR PROC, P87, DOI 10.1109/IISWC59245.2023.00023
   AMD, 2023, Radeon Graphics Profiler.
   [Anonymous], 2019, Talvos: A dynamic-analysis framework and debugger for Vulkan/SPIR-V programs
   Bakhoda A, 2009, INT SYM PERFORM ANAL, P163, DOI 10.1109/ISPASS.2009.4919648
   Bao YH, 2022, PROCEEDINGS OF THE 2022 31ST INTERNATIONAL CONFERENCE ON PARALLEL ARCHITECTURES AND COMPILATION TECHNIQUES, PACT 2022, P333, DOI 10.1145/3559009.3569666
   Born J, 2023, NAT MACH INTELL, V5, P432, DOI 10.1038/s42256-023-00639-z
   Chen YS, 2019, I S WORKL CHAR PROC, P167, DOI 10.1109/IISWC47752.2019.9042166
   Crawford Lewis, 2019, 2019 28th International Conference on Parallel Architectures and Compilation Techniques (PACT). Proceedings, P272, DOI 10.1109/PACT.2019.00029
   Crawford L, 2018, INT SYM PERFORM ANAL, P219, DOI 10.1109/ISPASS.2018.00035
   Cummins C, 2017, INT CONFER PARA, P219, DOI 10.1109/PACT.2017.24
   Cummins Chris, 2021, P MACHINE LEARNING R, V139, P2244
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Feng ZY, 2020, FINDINGS OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, EMNLP 2020, P1536
   Gubran AA, 2019, PROCEEDINGS OF THE 2019 46TH INTERNATIONAL SYMPOSIUM ON COMPUTER ARCHITECTURE (ISCA '19), P169, DOI 10.1145/3307650.3322221
   Hart JC, 1996, VISUAL COMPUT, V12, P527, DOI 10.1007/s003710050084
   He Y, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818104
   Huo Yuchi, 2022, ACM SIGGRAPH 2022 C, DOI [10.1145/3528233, DOI 10.1145/3528233]
   Khairy M, 2020, ANN I S COM, P473, DOI 10.1109/ISCA45697.2020.00047
   Khronos Group, 2023, SPIR-V Specification.
   Khronos Group, 2023, About us
   Khronos Group, 2023, SPIRV-Tools.
   Kingma D. P., 2014, arXiv
   Konstantinidis E, 2017, J PARALLEL DISTR COM, V107, P37, DOI 10.1016/j.jpdc.2017.04.002
   Kudo T, 2018, CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018): PROCEEDINGS OF SYSTEM DEMONSTRATIONS, P66
   Lemeire J, 2023, J PARALLEL DISTR COM, V173, P32, DOI 10.1016/j.jpdc.2022.11.002
   Liang YZ, 2023, IEEE T VIS COMPUT GR, V29, P4284, DOI 10.1109/TVCG.2022.3188775
   Mendis Charith, 2019, PR MACH LEARN RES, V97
   Niu C., 2022, P 31 INT JOINT C ART, V6, P5546, DOI [DOI 10.24963/IJCAI.2022/775SURVEYTRACK, 10.24963/ijcai.2022/775]
   Niu CA, 2023, Arxiv, DOI arXiv:2309.04828
   NVIDIA, 2023, NVIDIA Nsight Graphics.
   O'Neal K, 2017, ACM T EMBED COMPUT S, V16, DOI 10.1145/3126557
   Quilez Inigo, 2013, Shadertoy BETA.
   Sembrant A, 2017, I S WORKL CHAR PROC, P54
   Sennrich R, 2016, PROCEEDINGS OF THE 54TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P1715
   Sitthi-amorn P, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024186
   Sykora O, 2022, I S WORKL CHAR PROC, P14, DOI 10.1109/IISWC55918.2022.00012
   Turing AM, 1937, P LOND MATH SOC, V42, P230, DOI 10.1112/plms/s2-42.1.230
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang R, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2661229.2661276
   Williams S, 2009, COMMUN ACM, V52, P65, DOI 10.1145/1498765.1498785
   Yang Y, 2022, COMPUT GRAPH FORUM, V41, P41, DOI 10.1111/cgf.14457
   Yuan YZ, 2018, COMPUT GRAPH FORUM, V37, P143, DOI 10.1111/cgf.13482
   Zhai Y, 2023, PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON ARCHITECTURAL SUPPORT FOR PROGRAMMING LANGUAGES AND OPERATING SYSTEMS, VOL 2, ASPLOS 2023, P833, DOI 10.1145/3575693.3575737
   Zhang Z, 2021, INT J MACH LEARN CYB, V12, P1649, DOI 10.1007/s13042-020-01264-7
   Zheng Shuxin, 2021, PMLR, P8476
NR 46
TC 0
Z9 0
U1 0
U2 0
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
EI 2577-6193
J9 P ACM COMPUT GRAPH
JI P. ACM Comput. Graph. Interact. Tech.
PD MAY
PY 2024
VL 7
IS 1
DI 10.1145/3651295
PG 17
WC Computer Science, Software Engineering
WE Emerging Sources Citation Index (ESCI)
SC Computer Science
GA RQ5P8
UT WOS:001229142400002
DA 2024-08-05
ER

PT J
AU Barkevich, K
   Bailey, R
   Diaz, GJ
AF Barkevich, Kevin
   Bailey, Reynold
   Diaz, Gabriel J.
TI Using Deep Learning to Increase Eye-Tracking Robustness, Accuracy, and
   Precision in Virtual Reality
SO PROCEEDINGS OF THE ACM ON COMPUTER GRAPHICS AND INTERACTIVE TECHNIQUES
LA English
DT Article
DE neural networks; eye tracking; gaze estimation; virtual reality
ID PUPIL DETECTION
AB Algorithms for the estimation of gaze direction from mobile and video-based eye trackers typically involve tracking a feature of the eye that moves through the eye camera image in a way that covaries with the shifting gaze direction, such as the center or boundaries of the pupil. Tracking these features using traditional computer vision techniques can be difficult due to partial occlusion and environmental reflections. Although recent efforts to use machine learning (ML) for pupil tracking have demonstrated superior results when evaluated using standard measures of segmentation performance, little is known of how these networks may affect the quality of the final gaze estimate. This work provides an objective assessment of the impact of several contemporary ML-based methods for eye feature tracking when the subsequent gaze estimate is produced using either feature-based or model-based methods. Metrics include the accuracy and precision of the gaze estimate, as well as drop-out rate.
C1 [Barkevich, Kevin; Bailey, Reynold; Diaz, Gabriel J.] Rochester Inst Technol, Rochester, NY 14623 USA.
C3 Rochester Institute of Technology
RP Barkevich, K (corresponding author), Rochester Inst Technol, Rochester, NY 14623 USA.
EM kdb2713@rit.edu; rjbvcs@rit.edu; gjdpci@rit.edu
FU National Eye Institute of the National Institutes of Health
   [1R15EY031090]; National Science Foundation [DGE-2125362]
FX Research reported in this publication was supported by the National Eye
   Institute of the National Institutes of Health under award number
   1R15EY031090. This material is based uponwork supported by the National
   Science Foundation under Award No. DGE-2125362. Any opinions, findings,
   and conclusions or recommendations expressed in this material are those
   of the author(s) and do not necessarily reflect the views of the
   National Science Foundation.
CR [Anonymous], 2006, Proceedings of the 2006 symposium on Eye tracking research applications, DOI DOI 10.1145/1117309.1117349
   Binaee K, 2021, ACM SYMPOSIUM ON EYE TRACKING RESEARCH AND APPLICATIONS (ETRA 2021), DOI 10.1145/3450341.3458490
   Brookes J, 2020, BEHAV RES METHODS, V52, P455, DOI 10.3758/s13428-019-01242-0
   Cai X, 2021, IEEE INT CONF AUTOMA
   Chaudhary AK, 2019, ETRA 2019: 2019 ACM SYMPOSIUM ON EYE TRACKING RESEARCH & APPLICATIONS, DOI 10.1145/3314111.3322872
   Chaudhary AK, 2019, IEEE INT CONF COMP V, P3698, DOI 10.1109/ICCVW.2019.00568
   Dierkes K, 2019, ETRA 2019: 2019 ACM SYMPOSIUM ON EYE TRACKING RESEARCH & APPLICATIONS, DOI 10.1145/3314111.3319819
   Fuhl W, 2016, Arxiv, DOI [arXiv:1601.04902, DOI 10.48550/ARXIV.1601.04902]
   Fuhl W, 2021, IEEE INT CONF COMP V, P3459, DOI 10.1109/ICCVW54120.2021.00386
   Fuhl W, 2016, 2016 ACM SYMPOSIUM ON EYE TRACKING RESEARCH & APPLICATIONS (ETRA 2016), P123, DOI 10.1145/2857491.2857505
   Fuhl W, 2016, MACH VISION APPL, V27, P1275, DOI 10.1007/s00138-016-0776-4
   Fuhl W, 2015, LECT NOTES COMPUT SC, V9256, P39, DOI 10.1007/978-3-319-23192-1_4
   Guestrin ED, 2006, IEEE T BIO-MED ENG, V53, P1124, DOI 10.1109/TBME.2005.863952
   Garbin SJ, 2019, Arxiv, DOI arXiv:1905.03702
   Javadi Amir-Homayoun, 2015, Front Neuroeng, V8, P4, DOI 10.3389/fneng.2015.00004
   Kar A, 2017, IEEE ACCESS, V5, P16495, DOI 10.1109/ACCESS.2017.2735633
   Kassner M, 2014, PROCEEDINGS OF THE 2014 ACM INTERNATIONAL JOINT CONFERENCE ON PERVASIVE AND UBIQUITOUS COMPUTING (UBICOMP'14 ADJUNCT), P1151, DOI 10.1145/2638728.2641695
   Kim J, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300780
   Kothari Rakshit S., 2022, Proceedings of the ACM on Human-Computer Interaction, V6, DOI 10.1145/3530880
   Kothari RS, 2021, IEEE T VIS COMPUT GR, V27, P2757, DOI 10.1109/TVCG.2021.3067765
   Li D., 2005, 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops, P79, DOI DOI 10.1109/CVPR.2005.531
   Macinnes JJ., 2018, bioRxiv, DOI DOI 10.1101/299925
   MACKWORTH NH, 1962, J OPT SOC AM, V52, P713, DOI 10.1364/JOSA.52.000713
   Merchant John., 1967, Technical Report
   Meyer A, 2006, LECT NOTES ARTIF INT, V4021, P208
   Nair N, 2020, ACM SYMPOSIUM ON APPLIED PERCEPTION (SAP 2020), DOI 10.1145/3385955.3407935
   Pupil Labs GmbH, 2022, Pupil core-open source eye tracking platform-pupil labs
   Pupil Labs GmbH, 2022, HTC Vive Add-On
   Santini T, 2018, COMPUT VIS IMAGE UND, V170, P40, DOI 10.1016/j.cviu.2018.02.002
   Santini Thiago, 2018, P 2018 ACM S EYE TRA, P1
   SR Research Ltd, 2023, SR Research Ltd.-EyeTracking Company
   Swirski L., 2013, Proc. PETMEI, P1
   Swirski Lech, 2012, P S EYE TRACK RES AP, P173
   Tonsen M, 2016, 2016 ACM SYMPOSIUM ON EYE TRACKING RESEARCH & APPLICATIONS (ETRA 2016), P139, DOI 10.1145/2857491.2857520
   Wang ZM, 2021, INT SYM MIX AUGMENT, P11, DOI 10.1109/ISMAR52148.2021.00015
   Wood E, 2016, 2016 ACM SYMPOSIUM ON EYE TRACKING RESEARCH & APPLICATIONS (ETRA 2016), P131, DOI 10.1145/2857491.2857492
   Yiu YH, 2019, J NEUROSCI METH, V324, DOI 10.1016/j.jneumeth.2019.05.016
   Zhu ZW, 2005, COMPUT VIS IMAGE UND, V98, P124, DOI 10.1016/j.cviu.2004.07.012
NR 38
TC 0
Z9 0
U1 0
U2 0
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
EI 2577-6193
J9 P ACM COMPUT GRAPH
JI P. ACM Comput. Graph. Interact. Tech.
PD MAY
PY 2024
VL 7
IS 2
AR 27
DI 10.1145/3654705
PG 16
WC Computer Science, Software Engineering
WE Emerging Sources Citation Index (ESCI)
SC Computer Science
GA SL0I6
UT WOS:001234486100005
OA Green Submitted, hybrid
DA 2024-08-05
ER

PT J
AU Schutz, M
   Herzberger, L
   Wimmer, M
AF Schutz, Markus
   Herzberger, Lukas
   Wimmer, Michael
TI SimLOD: Simultaneous LOD Generation and Rendering for Point Clouds
SO PROCEEDINGS OF THE ACM ON COMPUTER GRAPHICS AND INTERACTIVE TECHNIQUES
LA English
DT Article
DE Point Cloud Rendering; LOD; Octree
AB LOD construction is typically implemented as a preprocessing step that requires users to wait before they are able to view the results in real time. We propose an incremental LOD generation approach for point clouds that allows us to simultaneously load points from disk, update an octree-based level-of-detail representation, and render the intermediate results in real time while additional points are still being loaded from disk. LOD construction and rendering are both implemented in CUDA and share the GPU's processing power, but each incremental update is lightweight enough to leave enough time to maintain real-time frame rates.
   Our approach is able to stream points from an SSD and update the octree on the GPU at rates of up to 580 million points per second (similar to 9.3GB/s) on an RTX 4090 and a PCIe 5.0 SSD. Depending on the data set, our approach spends an average of about 1 to 2 ms to incrementally insert 1 million points into the octree, allowing us to insert several million points per frame into the LOD structure and render the intermediate results within the same frame.
C1 [Schutz, Markus; Herzberger, Lukas; Wimmer, Michael] TU Wien, Favoritenstr 9-11 E193-02, A-1040 Vienna, Austria.
C3 Technische Universitat Wien
RP Schutz, M (corresponding author), TU Wien, Favoritenstr 9-11 E193-02, A-1040 Vienna, Austria.
EM mschuetz@cg.tuwien.ac.at; lherzberger@cg.tuwien.ac.at;
   wimmer@cg.tuwien.ac.at
RI Herzberger, Lukas/HOA-6306-2023
OI Herzberger, Lukas/0000-0002-9047-065X; Wimmer,
   Michael/0000-0002-9370-2663
FU WWTF project [ICT22-055]; FFG project LargeClouds2BIM
FX This research has been funded byWWTF project ICT22-055 - Instant
   Visualization and Interaction for Large Point Clouds, and FFG project
   LargeClouds2BIM.
CR AHN, 2002, About us
   AHN2, 2012, AHN2
   [Anonymous], 2023, C++ Standards Comitee Proposal Paper
   Bormann Pascal, 2022, Computer Graphics and Visual Computing (CGVC), DOI [10.2312/cgvc.20221173, DOI 10.2312/CGVC.20221173]
   Bormann Pascal, 2020, SMART TOOLS APPS GRA
   Botsch M., 2005, P EUR IEEE VGTC S PO, P17, DOI [DOI 10.1109/PBG.2005.194059, DOI 10.2312/SPBG/SPBG05/017-024]
   Bruckner S, 2019, COMPUT GRAPH FORUM, V38, P317, DOI 10.1111/cgf.13640
   Bunds M.P., 2020, High Resolution Topography of the Central San Andreas Fault at Dry Lake Valley
   Careil V, 2020, COMPUT GRAPH FORUM, V39, P111, DOI 10.1111/cgf.13916
   Cesium, 2021, Cesium
   Chajdas Matthaus G., 2014, Journal of WSCG, V22, P77
   Coconu Liviu, 2002, EUR WORKSH REND, DOI [10.2312/EGWR/EGWR02/043-052, DOI 10.2312/EGWR/EGWR02/043-052]
   Cohen JD, 2001, IEEE VISUAL, P37, DOI 10.1109/VISUAL.2001.964491
   Crassin C., 2009, P 2009 S INT 3D GRAP, P15, DOI DOI 10.1145/1507149.1507152
   Dachsbacher C, 2003, ACM T GRAPHIC, V22, P657, DOI 10.1145/882262.882321
   Entwine, 2021, Entwine
   Evans A., 2015, Advances in Real-Time Rendering in Games
   Gnther C., 2013, J. WSCG, V21, P153
   Gobbetti E, 2004, COMPUT GRAPH-UK, V28, P815, DOI 10.1016/j.cag.2004.08.010
   Goswami P., 2010, 2010 Pacific Graphics (PG). Proceedings 18th Pacific Conference on Computer Graphics and Applications, P93, DOI 10.1109/PacificGraphics.2010.20
   Gupta K., 2012, Innovative Parallel Computing - Foundations & Applications of GPU, Manycore, and Heterogeneous Systems (INPAR 2012), DOI 10.1109/InPar.2012.6339596
   Harris Mark, 2017, Cooperative Groups: Flexible CUDA Thread Programming
   Herbig U, 2019, INT ARCH PHOTOGRAMM, V42-2, P555, DOI 10.5194/isprs-archives-XLII-2-W15-555-2019
   Iconem, 2024, Northern necropolis-Meroe
   Keller Maik, 2009, VMV 2009 P VIS MOD V, P165
   Kenzel M, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201374
   Kerbl B, 2023, ACM T GRAPHIC, V42, DOI 10.1145/3592433
   Kocon K, 2021, IEEE INT CONF BIG DA, P109, DOI 10.1109/BigData52589.2021.9671659
   KORF RE, 1985, ARTIF INTELL, V27, P97, DOI 10.1016/0004-3702(85)90084-0
   Lai Kang, 2019, 2019 IEEE Fourth International Conference on Data Science in Cyberspace (DSC). Proceedings, P593, DOI 10.1109/DSC.2019.00096
   Martinez-Rubi Oscar, 2015, CAPT REAL FOR 2015 S
   Microsoft, 2022, BC7 Format
   Molenaar M, 2023, COMPUT GRAPH FORUM, V42, P235, DOI 10.1111/cgf.14757
   Ogayar-Anguita CJ, 2023, ISPRS J PHOTOGRAMM, V195, P287, DOI 10.1016/j.isprsjprs.2022.11.018
   Pacific Gas & Electric Company, 2013, PG&E Diablo Canyon Power Plant (DCPP): San Simeon and Cambria Faults, CA, Airborne Lidar survey
   Perry Cory., 2020, Introducing low-level GPU virtual memory management
   Pfister H, 2000, COMP GRAPH, P335, DOI 10.1145/344779.344936
   Rusinkiewicz S, 2000, COMP GRAPH, P343, DOI 10.1145/344779.344940
   Scheiblauer C, 2011, COMPUT GRAPH-UK, V35, P342, DOI 10.1016/j.cag.2011.01.004
   Schütz M, 2022, P ACM COMPUT GRAPH, V5, DOI 10.1145/3543863
   Schütz M, 2020, COMPUT GRAPH FORUM, V39, P155, DOI 10.1111/cgf.14134
   Schütz M, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P103, DOI [10.1109/vr.2019.8798284, 10.1109/VR.2019.8798284]
   Schutz M., 2016, Potree: Rendering large point clouds in web browsers
   Schütz M, 2021, COMPUT GRAPH FORUM, V40, P115, DOI 10.1111/cgf.14345
   Schutz Markus, 2023, GPU-Accelerated LOD Generation for Point Clouds
   The Khronos Group Inc, 2014, ARBsparsebuffer Extension
   USGS, Entwine 2020
   USGS, 3DEP 2020. 3D Elevation Program (3DEP)
   van Oosterom P, 2022, ISPRS J PHOTOGRAMM, V194, P119, DOI 10.1016/j.isprsjprs.2022.10.004
   Wand M, 2008, COMPUT GRAPH-UK, V32, P204, DOI 10.1016/j.cag.2008.01.010
   Weyrich T, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239541
   WIMMER M., 2006, Proceedings of Eurographics Symposium on Point-Based Graphics 2006, P129, DOI DOI 10.2312/SPBG/SPBG06/129-136
   Yang JC, 2010, COMPUT GRAPH FORUM, V29, P1297, DOI 10.1111/j.1467-8659.2010.01725.x
   Yoon Y, 2002, I-SPAN'02: INTERNATIONAL SYMPOSIUM ON PARALLEL ARCHITECTURES, ALGORITHMS AND NETWORKS, PROCEEDINGS, P105, DOI 10.1109/ISPAN.2002.1004268
   Zellmann S., 2022, P 22 EUR S PAR GRAPH, P61
   Zhong Shao, 1994, Proceedings of the 1994 ACM Conference on LISP and Functional Programming, P185, DOI 10.1145/182409.182453
   Zwicker M, 2001, COMP GRAPH, P371, DOI 10.1145/383259.383300
NR 57
TC 0
Z9 0
U1 0
U2 0
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
EI 2577-6193
J9 P ACM COMPUT GRAPH
JI P. ACM Comput. Graph. Interact. Tech.
PD MAY
PY 2024
VL 7
IS 1
DI 10.1145/3651287
PG 20
WC Computer Science, Software Engineering
WE Emerging Sources Citation Index (ESCI)
SC Computer Science
GA RQ5P8
UT WOS:001229142400017
OA hybrid
DA 2024-08-05
ER

PT J
AU Pharr, M
   Wronski, B
   Salvi, M
   Fajardo, M
AF Pharr, Matt
   Wronski, Bartlomiej
   Salvi, Marco
   Fajardo, Marcos
TI Filtering After Shading With Stochastic Texture Filtering
SO PROCEEDINGS OF THE ACM ON COMPUTER GRAPHICS AND INTERACTIVE TECHNIQUES
LA English
DT Article
DE Texture filtering; filtering; stochastic sampling; Monte Carlo
   techniques
AB 2D texture maps and 3D voxel arrays are widely used to add rich detail to the surfaces and volumes of rendered scenes, and filtered texture lookups are integral to producing high-quality imagery. We show that applying the texture filter after evaluating shading generally gives more accurate imagery than filtering textures before BSDF evaluation, as is current practice. These benefits are not merely theoretical, but are apparent in common cases. We demonstrate that practical and efficient filtering after shading is possible through the use of stochastic sampling of texture filters.
   Stochastic texture filtering offers additional benefits, including efficient implementation of high-quality texture filters and efficient filtering of textures stored in compressed and sparse data structures, including neural representations. We demonstrate applications in both real-time and offline rendering and show that the additional error from stochastic filtering is minimal. We find that this error is handled well by either spatiotemporal denoising or moderate pixel sampling rates.
C1 [Pharr, Matt; Wronski, Bartlomiej; Salvi, Marco] NVIDIA, Santa Clara, CA USA.
   [Fajardo, Marcos] Shiokara Engawa Res, Madrid, Spain.
C3 Nvidia Corporation
RP Pharr, M (corresponding author), NVIDIA, Santa Clara, CA USA.
OI Pharr, Matt/0000-0002-0566-8291; Fajardo Orellana,
   Marcos/0009-0001-7707-2761
FU NVIDIA
FX We would like to thank Aaron Lefohn and NVIDIA for supporting this work,
   John Burgess for suggesting the connection to percentage closer
   filtering, Karthik Vaidyanathan for many discussions and suggestions,
   Johannes Deligiannis for finding the problem with nonlinearity
   introduced aliasing, Markus Kettunen for comments about texture
   reconstruction versus low-pass filtering, and Tomas Akenine-Moller for
   helpful comments on a draft of this paper. We thank the wide graphics
   community on social media for discussion and historical references to
   the earliest uses of stochastic texture filtering in video games. We are
   grateful to Walt Disney Animation Studios for making the detailed cloud
   model available and to Lennart Demes, author of the ambientCG website,
   for providing a public-domain PBR material database that we used to
   produce the real-time rendering figures.
CR Akenine-Moller Tomas, 2021, Journal of Computer Graphics Techniques (JCGT), V10, P1
   Barkans Anthony C., 1997, P ACM SIGGRAPH EUROG, P79, DOI [10.1145/258694.258722, DOI 10.1145/258694.258722]
   Barre-Brisebois Colin., 2019, Ray Tracing Gems, P437
   BLINN JF, 1976, COMMUN ACM, V19, P542, DOI [10.1145/360349.360353, 10.1145/965143.563322]
   BOOR CD, 1977, SIAM J NUMER ANAL, V14, P441, DOI 10.1137/0714026
   Cant RJ, 2000, ACM T GRAPHIC, V19, P164, DOI 10.1145/353981.353991
   Catmull E. E., 1974, A subdivision algorithm for computer display of curved surfaces
   CHAO MT, 1982, BIOMETRIKA, V69, P653
   Clarberg Petrik, 2022, HPG 2022
   Enderton Eric., 2010, Proceedings of the 2010 Symposium on Interactive 3D Graphics and Games, P157
   Ernst M, 2006, RT 06: IEEE SYMPOSIUM ON INTERACTIVE RAY TRACING 2006, PROCEEDINGS, P125
   Estevez AC, 2018, P ACM COMPUT GRAPH, V1, DOI 10.1145/3233305
   Georgiev Iliyan, 2016, ACM SIGGRAPH 2016 TA, DOI DOI 10.1145/2897839.2927430
   GREENE N, 1986, IEEE COMPUT GRAPH, V6, P21, DOI 10.1109/MCG.1986.276658
   Gritz Larry, 2022, OpenImageIO 2.4
   Guo Y, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275053
   Hasselgren Jon, 2021, EUROGRAPHICS S RENDE
   Heckbert Paul, 1989, Fundamentals of Texture Mapping and Image Warping
   Heitz Eric., 2016, ACM Transactions on Graphics (Proceedings of SIGGRAPH), V35, p58:1
   Hofmann N, 2021, P ACM COMPUT GRAPH, V4, DOI 10.1145/3451256
   Igehy H, 1999, COMP GRAPH, P179, DOI 10.1145/311535.311555
   Kajiya J. T., 1986, Computer Graphics, V20, P143, DOI 10.1145/15886.15902
   Kallweit Simon, 2022, The Falcor Rendering Framework
   Krivanek Jaroslav, 2010, ACM SIGGRAPH COURSES
   Kutz P, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073665
   Lee M, 2017, HPG '17: PROCEEDINGS OF HIGH PERFORMANCE GRAPHICS, DOI 10.1145/3105762.3105768
   Liu Edward, 2022, GAM DEV C
   McCormack J, 1999, COMP GRAPH, P243, DOI 10.1145/311535.311562
   Microsoft, 2015, Direct3D 11.3 Functional Specification
   Miller B, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3323025
   Museth K, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2487228.2487235
   Nehab Diego, 2011, Tech. Rep. MSR-TR-2011-16
   Novák J, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2661229.2661292
   Ogaki S, 2021, PROCEEDINGS OF SIGGRAPH ASIA 2021 TECHNICAL COMMUNICATIONS, DOI 10.1145/3478512.3488602
   Olano Marc, 2010, Proceedings of the Symposium on Interactive 3D Graphics and Games, P181, DOI 10.1145/1730804.1730834
   Owen A, 2000, J AM STAT ASSOC, V95, P135, DOI 10.2307/2669533
   Pharr M., 2023, Physically Based Rendering: From Theory to Implementation, Vfourth
   Pharr Matt, 2022, pbrt-v4
   Porter T., 1984, Computers & Graphics, V18, P253
   Productions Interplay, 1992, Texture Scaling in Star Trek: 25th Anniversary
   Reeves W.T., 1987, COMPUTER GRAPHICS P, P283
   Ross S.M., 2019, 1 COURSE PROBABILITY
   Shirley P, 1996, ACM T GRAPHIC, V15, P1, DOI 10.1145/226150.226151
   Shirley Peter, 1990, Ph. D. Dissertation
   Stachowiak Tomasz, 2015, Advances in RealTime Rendering in Games, Part I (ACM SIGGRAPH Courses)
   Sweeney Tim, 2000, Texturing As In Unreal
   Szecsi Laszlo, 2003, Journal of the World Society for Computer Graphics (WSCG), V11, P1
   Takeda H, 2007, IEEE T IMAGE PROCESS, V16, P349, DOI 10.1109/TIP.2006.888330
   Vaidyanathan K, 2023, ACM T GRAPHIC, V42, DOI 10.1145/3592407
   Walt Disney Animation Studios, 2017, Cloud Data Set
   Williams L., 1983, Computer Graphics, V17, P1, DOI 10.1145/964967.801126
   Wolfe Alan, 2022, EUROGRAPHICS S RENDE, P117, DOI DOI 10.2312/SR.20221161
   Wronski B, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3323024
   Wronski Bartlomiej, 2021, Practical Gaussian filtering: Binomial filter and small sigma Gaussians
   Wyman Chris., 2017, Proceedings of the 21st ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games (I3D'17), P1, DOI DOI 10.1145/3023368.3023370
   Yang L, 2020, COMPUT GRAPH FORUM, V39, P607, DOI 10.1111/cgf.14018
   Yu JH, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2421636.2421641
NR 57
TC 0
Z9 0
U1 0
U2 0
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
EI 2577-6193
J9 P ACM COMPUT GRAPH
JI P. ACM Comput. Graph. Interact. Tech.
PD MAY
PY 2024
VL 7
IS 1
AR 14
DI 10.1145/3651293
PG 20
WC Computer Science, Software Engineering
WE Emerging Sources Citation Index (ESCI)
SC Computer Science
GA RQ5P8
UT WOS:001229142400014
DA 2024-08-05
ER

PT J
AU Nishidate, Y
   Fujishiro, I
AF Nishidate, Yuki
   Fujishiro, Issei
TI Efficient Particle-Based Fluid Surface Reconstruction Using Mesh Shaders
   and Bidirectional Two-Level Grids
SO PROCEEDINGS OF THE ACM ON COMPUTER GRAPHICS AND INTERACTIVE TECHNIQUES
LA English
DT Article
DE Fluid rendering; Marching Cubes; Rasterization
AB In this paper, we introduce a novel method for particle-based fluid surface reconstruction that incorporates mesh shaders for the first time. This approach eliminates the need to store triangle meshes in a GPU's global memory, resulting in a significant reduction in the amount of memory needed. Furthermore, our method employs a bidirectional two-level uniform grid, which not only accelerates the computationally expensive stage of surface cell detection but also effectively addresses the issue of vertex overflow among the mesh shaders. Experimental results prove that our method outperforms the state-of-the-art method, achieving both acceleration and memory reductions simultaneously, without sacrificing quality. The method is highly practical, with no major limitations other than requiring a GPU that supports the mesh shaders.
C1 [Nishidate, Yuki; Fujishiro, Issei] Keio Univ, Yokohama, Kanagawa, Japan.
C3 Keio University
RP Nishidate, Y (corresponding author), Keio Univ, Yokohama, Kanagawa, Japan.
EM yuki_nishidate@keio.jp; fuji@ics.keio.ac.jp
OI Fujishiro, Issei/0000-0002-8898-730X
FU  [JP21H04916]
FX We have benefited from the valuable comments of anonymous reviewers to
   improve our draft. This work has been partially supported by
   Grant-in-Aid for Scientific Research (A) JP21H04916.
CR ADALSTEINSSON D, 1995, J COMPUT PHYS, V118, P269, DOI 10.1006/jcph.1995.1098
   Akinci G, 2012, COMPUT GRAPH FORUM, V31, P1797, DOI 10.1111/j.1467-8659.2012.02096.x
   Akinci G, 2013, WSCG 2013, FULL PAPERS PROCEEDINGS, P195
   Blender Online Community, 2023, Blender-a 3d modelling and rendering package
   BRACKBILL JU, 1988, COMPUT PHYS COMMUN, V48, P25, DOI 10.1016/0010-4655(88)90020-3
   Chernyaev Evgeni, 1995, P GRAPHICON 95
   DOI A, 1991, IEICE TRANS COMMUN, V74, P214
   Du S., 2014, 22 INT C CENTRAL EUR, P141
   Dyken C, 2008, COMPUT GRAPH FORUM, V27, P2028, DOI 10.1111/j.1467-8659.2008.01182.x
   Elliott Hamish Robert, 2022, Master of Science (MSc) Thesis.
   Engstrom Ludwig Pethrus, 2015, A Modern GPGPU Approach to Marching Cubes
   Henning Neil, 2018, Vulkan Subgroup Tutorial
   Ju T, 2002, ACM T GRAPHIC, V21, P339
   Kobbelt LP, 2001, COMP GRAPH, P57, DOI 10.1145/383259.383265
   Kreskowski A, 2022, COMPUT GRAPH FORUM, V41, P215, DOI 10.1111/cgf.14670
   Kubisch Christoph, 2018, Introduction to Turing Mesh Shaders
   Kubisch Christoph, 2022, Mesh Shading for Vulkan
   Lewiner T., 2003, Journal of Graphics Tools, V8, P1, DOI 10.1080/10867651.2003.10487582
   Liu BQ, 2016, COMPUT GRAPH FORUM, V35, P211, DOI 10.1111/cgf.12897
   Lorensen H. E., 1987, Proc. SIGGRAPH, V21, P163, DOI 10.1145/37401.37422
   Muller M., 2003, ACM SIGGRAPH/Eurographics Symposium on Computer Animation, P154
   Nielson GM, 2004, IEEE VISUALIZATION 2004, PROCEEEDINGS, P489, DOI 10.1109/VISUAL.2004.28
   NIELSON GM, 1991, VISUALIZATION 91, P83
   Schaefer S, 2004, 12TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P70, DOI 10.1109/PCCGA.2004.1348336
   Willems Sascha, 2023, Vulkan Hardware Database
   Yang WC, 2020, VISUAL COMPUT, V36, P2313, DOI 10.1007/s00371-020-01898-2
   Zou L, 2016, INT J COMPUT APPL T, V54, P68, DOI 10.1504/IJCAT.2016.077795
NR 27
TC 0
Z9 0
U1 0
U2 0
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
EI 2577-6193
J9 P ACM COMPUT GRAPH
JI P. ACM Comput. Graph. Interact. Tech.
PD MAY
PY 2024
VL 7
IS 1
AR 1
DI 10.1145/3651285
PG 14
WC Computer Science, Software Engineering
WE Emerging Sources Citation Index (ESCI)
SC Computer Science
GA RQ5P8
UT WOS:001229142400001
DA 2024-08-05
ER

PT J
AU Chen, WB
   Liu, L
AF Chen, Wenbo
   Liu, Ligang
TI Deblur-GS: 3D Gaussian Splatting from Camera Motion Blurred Images
SO PROCEEDINGS OF THE ACM ON COMPUTER GRAPHICS AND INTERACTIVE TECHNIQUES
LA English
DT Article
DE Graphics; Rendering; 3D Gaussian Splatting; Motion Blur; Novel View
   Synthesis
AB Novel view synthesis has undergone a revolution thanks to the radiance field method. The introduction of 3D Gaussian splatting (3DGS) has successfully addressed the issues of prolonged training times and slow rendering speeds associated with the Neural Radiance Field (NeRF), all while preserving the quality of reconstructions. However, 3DGS remains heavily reliant on the quality of input images and their initial camera pose initialization. In cases where input images are blurred, the reconstruction results suffer from blurriness and artifacts. In this paper, we propose the Deblur-GS method for reconstructing 3D Gaussian points to create a sharp radiance field from a camera motion blurred image set. We model the problem of motion blur as a joint optimization challenge involving camera trajectory estimation and time sampling. We cohesively optimize the parameters of the Gaussian points and the camera trajectory during the shutter time. Deblur-GS consistently achieves superior performance and rendering quality when compared to previous methods, as demonstrated in evaluations conducted on both synthetic and real datasets.
C1 [Chen, Wenbo; Liu, Ligang] Univ Sci & Technol China, Hefei, Anhui, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS
RP Chen, WB (corresponding author), Univ Sci & Technol China, Hefei, Anhui, Peoples R China.
EM chaf@mail.ustc.edu.cn; lgliu@ustc.edu.cn
OI Chen, Wenbo/0000-0002-6338-7658
FU National Key R&D Program of China [2022YFB3303400]; National Natural
   Science Foundation of China [62025207]
FX This work is supported by the National Key R&D Program of China
   (2022YFB3303400) and the National Natural Science Foundation of China
   (62025207).
CR Barron JT, 2023, Arxiv, DOI arXiv:2304.06706
   Bian WJ, 2023, PROC CVPR IEEE, P4160, DOI 10.1109/CVPR52729.2023.00405
   Cao YK, 2023, Arxiv, DOI [arXiv:2304.00916, 10.48550/ARXIV.2304.00916]
   Chan ER, 2021, PROC CVPR IEEE, P5795, DOI 10.1109/CVPR46437.2021.00574
   Chen AP, 2022, LECT NOTES COMPUT SC, V13692, P333, DOI 10.1007/978-3-031-19824-3_20
   Chen LY, 2022, LECT NOTES COMPUT SC, V13667, P17, DOI 10.1007/978-3-031-20071-7_2
   Chen Y, 2023, PROC CVPR IEEE, P8264, DOI 10.1109/CVPR52729.2023.00799
   Cho S, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618491
   Fergus R, 2006, ACM T GRAPHIC, V25, P787, DOI 10.1145/1141911.1141956
   Gafni G, 2021, PROC CVPR IEEE, P8645, DOI 10.1109/CVPR46437.2021.00854
   Gu JT, 2021, Arxiv, DOI arXiv:2110.08985
   Höllein L, 2023, Arxiv, DOI arXiv:2303.11989
   Hore Alain, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P2366, DOI 10.1109/ICPR.2010.579
   Kerbl B, 2023, ACM T GRAPHIC, V42, DOI 10.1145/3592433
   Kingma D. P., 2014, arXiv
   Kopanas G, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3550454.3555497
   Krishnan D., 2009, Advances in neural information processing systems, V22
   Kupyn O, 2019, IEEE I CONF COMP VIS, P8877, DOI 10.1109/ICCV.2019.00897
   Lee B, 2024, Arxiv, DOI arXiv:2401.00834
   Lee D., 2023, IEEE CVF ICCV C P, P17639
   Lee D, 2023, PROC CVPR IEEE, P12386, DOI 10.1109/CVPR52729.2023.01192
   Lee D, 2018, LECT NOTES COMPUT SC, V11220, P300, DOI 10.1007/978-3-030-01270-0_18
   Levin A, 2009, PROC CVPR IEEE, P1964, DOI 10.1109/CVPRW.2009.5206815
   Li KJ, 2022, PROC CVPR IEEE, P6156, DOI 10.1109/CVPR52688.2022.00607
   Lin CH, 2023, PROC CVPR IEEE, P300, DOI 10.1109/CVPR52729.2023.00037
   Lin CH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5721, DOI 10.1109/ICCV48922.2021.00569
   Liu LJ, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3478513.3480528
   Ma L, 2022, PROC CVPR IEEE, P12851, DOI 10.1109/CVPR52688.2022.01252
   Mi Zhenxing, 2023, INT C LEARN REPR ICL
   Mildenhall B, 2022, COMMUN ACM, V65, P99, DOI 10.1145/3503250
   Mueggler E, 2018, IEEE T ROBOT, V34, P1425, DOI 10.1109/TRO.2018.2858287
   Müller T, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530127
   Nah S, 2017, PROC CVPR IEEE, P257, DOI 10.1109/CVPR.2017.35
   Park H, 2017, IEEE I CONF COMP VIS, P4623, DOI 10.1109/ICCV.2017.494
   Park K, 2023, ACM T GRAPHIC, V42, DOI 10.1145/3618321
   Peng SD, 2021, PROC CVPR IEEE, P9050, DOI 10.1109/CVPR46437.2021.00894
   Peng SD, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14294, DOI 10.1109/ICCV48922.2021.01405
   Rosinol A, 2023, IEEE INT C INT ROBOT, P3437, DOI 10.1109/IROS55552.2023.10341922
   Rosu Radu Alexandru, 2022, Neural Strands: Learning Hair Geometry and Appearance from Multi-View Images
   Schönberger JL, 2016, PROC CVPR IEEE, P4104, DOI 10.1109/CVPR.2016.445
   Shan Q, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360672
   Su SC, 2017, PROC CVPR IEEE, P237, DOI 10.1109/CVPR.2017.33
   Suhail M, 2022, PROC CVPR IEEE, P8259, DOI 10.1109/CVPR52688.2022.00809
   Sun C, 2022, PROC CVPR IEEE, P5449, DOI 10.1109/CVPR52688.2022.00538
   Sun JX, 2022, PROC CVPR IEEE, P7662, DOI 10.1109/CVPR52688.2022.00752
   Tancik M., 2022, arXiv
   Tao X, 2018, PROC CVPR IEEE, P8174, DOI 10.1109/CVPR.2018.00853
   Verbin D, 2022, PROC CVPR IEEE, P5481, DOI 10.1109/CVPR52688.2022.00541
   Wang P, 2023, PROC CVPR IEEE, P4170, DOI 10.1109/CVPR52729.2023.00406
   Wang P, 2023, PROC CVPR IEEE, P4150, DOI 10.1109/CVPR52729.2023.00404
   Wang YF, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356513
   Wang Zirui, 2021, arXiv
   Weng CY, 2022, PROC CVPR IEEE, P16189, DOI 10.1109/CVPR52688.2022.01573
   Xu L, 2010, LECT NOTES COMPUT SC, V6311, P157
   Xu QG, 2022, PROC CVPR IEEE, P5428, DOI 10.1109/CVPR52688.2022.00536
   Zamir SW, 2022, PROC CVPR IEEE, P5718, DOI 10.1109/CVPR52688.2022.00564
   Zhang Q, 2022, PROCEEDINGS SIGGRAPH ASIA 2022, DOI 10.1145/3550469.3555413
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhu ZH, 2022, PROC CVPR IEEE, P12776, DOI 10.1109/CVPR52688.2022.01245
   Zhuang Yiyu, 2021, arXiv
NR 60
TC 0
Z9 0
U1 4
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
EI 2577-6193
J9 P ACM COMPUT GRAPH
JI P. ACM Comput. Graph. Interact. Tech.
PD MAY
PY 2024
VL 7
IS 1
AR 18
DI 10.1145/3651301
PG 15
WC Computer Science, Software Engineering
WE Emerging Sources Citation Index (ESCI)
SC Computer Science
GA RQ5P8
UT WOS:001229142400018
DA 2024-08-05
ER

PT J
AU Dinkov, M
   Pattanaik, S
   Mcmahan, RP
AF Dinkov, Martin
   Pattanaik, Sumanta
   Mcmahan, Ryan P.
TI Perceptions of Hybrid Lighting for Virtual Reality
SO PROCEEDINGS OF THE ACM ON COMPUTER GRAPHICS AND INTERACTIVE TECHNIQUES
LA English
DT Article
DE Realtime Rendering; Lightmaps; Hybrid Lighting; Virtual Reality
ID CHOICE; ILLUMINATION; ENVIRONMENTS; RESPONSES; IMPACT
AB Virtual reality (VR) applications are computationally demanding due to high frame rate requirements, which precludes large numbers of realtime lights from being used. In this paper, we explore a hybrid lighting approach that combines the benefits of realtime and baked lights based on the importance of each source. First, we demonstrate that the hybrid approach affords better frames per second than realtime and mixed lighting. We then present the results of an online paired-comparison study, in which participants (n = 60) compared videos of the four lighting conditions (baked, mixed, realtime, and hybrid) in terms of preference. Our results indicate that the hybrid lighting approach is better than realtime lighting in terms of graphical performance and also yields better perceptions of quality for scenes with a small to moderately large number of lights.
C1 [Dinkov, Martin; Pattanaik, Sumanta; Mcmahan, Ryan P.] Univ Cent Florida, Orlando, FL 32816 USA.
C3 State University System of Florida; University of Central Florida
RP Dinkov, M (corresponding author), Univ Cent Florida, Orlando, FL 32816 USA.
EM ma492628@ucf.edu; sumant@cs.ucf.edu; rpm@ucf.edu
CR Abd-Alhamid F, 2019, BUILD ENVIRON, V162, DOI 10.1016/j.buildenv.2019.106278
   Adams H, 2022, IEEE T VIS COMPUT GR, V28, P4624, DOI 10.1109/TVCG.2021.3097978
   Akeley Kurt, 2002, ACM SIGGRAPH 2002 C, P86, DOI [10.1145/1242073.1242120, DOI 10.1145/1242073.1242120]
   Bastos R., 1997, Proceedings 1997 Symposium on Interactive 3D Graphics, P71, DOI 10.1145/253284.253309
   Brons I, 2014, TRENDS HEAR, V18, DOI 10.1177/2331216514553924
   Chamilothori K, 2019, BUILD ENVIRON, V150, P144, DOI 10.1016/j.buildenv.2019.01.009
   Chamilothori K, 2019, LEUKOS, V15, P203, DOI 10.1080/15502724.2017.1404918
   Chen Hao, 2008, ACM SIGGRAPH 2008 GA, P1
   Chen Y, 2019, LIGHTING RES TECHNOL, V51, P820, DOI 10.1177/1477153518825387
   Dachsbacher C., 2005, Proc. Symp. Interactive Graph. and Games, P203, DOI DOI 10.1145/1053427.1053460
   Debattista K, 2018, COMPUT GRAPH FORUM, V37, P363, DOI 10.1111/cgf.13302
   Dhar R, 2003, J MARKETING RES, V40, P146, DOI 10.1509/jmkr.40.2.146.19229
   Do TD, 2020, INT SYM MIX AUGMENT, P64, DOI 10.1109/ISMAR50242.2020.00026
   Erickson A, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR 2020), P434, DOI [10.1109/VR46266.2020.1580695145399, 10.1109/VR46266.2020.00-40]
   Felnhofer A, 2015, INT J HUM-COMPUT ST, V82, P48, DOI 10.1016/j.ijhcs.2015.05.004
   Flor JF, 2021, ENERG BUILDINGS, V231, DOI 10.1016/j.enbuild.2020.110554
   Hartigan J. A., 1979, Applied Statistics, V28, P100, DOI 10.2307/2346830
   Hegazy M, 2021, AUTOMAT CONSTR, V131, DOI 10.1016/j.autcon.2021.103898
   Heloir A, 2011, LECT NOTES COMPUT SC, V6974, P117, DOI 10.1007/978-3-642-24600-5_15
   Heydarian A, 2015, AUTOMAT CONSTR, V54, P116, DOI 10.1016/j.autcon.2015.03.020
   illiams Lance, 1978, SIGGRAPH Comput. Graph, P270
   Jensen HenrikWann., 1996, Rendering Techniques '96, P21
   Jones JA, 2013, IEEE T VIS COMPUT GR, V19, P701, DOI 10.1109/TVCG.2013.37
   Kajiya J. T., 1986, Computer Graphics, V20, P143, DOI 10.1145/15886.15902
   Karlsson T, 2023, GAMES CULT, V18, P821, DOI 10.1177/15554120221139229
   Kost RG, 2018, J CLIN TRANSL SCI, V2, P31, DOI 10.1017/cts.2018.18
   Kuller R., 1991, ENV COGNITION ACTION, P111
   Lai CY, 2020, INT SYM MIX AUGMENT, P627, DOI 10.1109/ISMAR50242.2020.00091
   Lambru C, 2021, IEEE ACCESS, V9, P125158, DOI 10.1109/ACCESS.2021.3109663
   LaViola J.J., 2017, 3D user interfaces: theory and practice
   Louis T, 2019, PROCEEDINGS OF THE 2019 ACM INTERNATIONAL CONFERENCE ON INTERACTIVE SURFACES AND SPACES (ISS '19), P5, DOI 10.1145/3343055.3359710
   Higuera-Trujillo JL, 2017, APPL ERGON, V65, P398, DOI 10.1016/j.apergo.2017.05.006
   Luksch C, 2019, ACM SIGGRAPH SYMPOSIUM ON INTERACTIVE 3D GRAPHICS AND GAMES (I3D 2019), DOI 10.1145/3306131.3317015
   Mania K., 2004, ACM Siggraph Int'l Conf. Virtual Reality Continuum and its Applications in Industry, P200, DOI [10.1145/1044588.1044629, DOI 10.1145/1044588.1044629]
   Marzouk M, 2022, BUILD SIMUL-CHINA, V15, P1561, DOI 10.1007/s12273-021-0873-9
   Masullo M, 2023, SUSTAINABILITY-BASEL, V15, DOI 10.3390/su15032069
   Masullo M, 2022, SUSTAINABILITY-BASEL, V14, DOI 10.3390/su14148556
   Mathis F, 2021, ACM T COMPUT-HUM INT, V28, DOI 10.1145/3428121
   Moore AG, 2021, INT SYM MIX AUGMENT, P221, DOI 10.1109/ISMAR52148.2021.00037
   Moscoso C, 2022, LEUKOS, V18, P294, DOI 10.1080/15502724.2020.1854779
   Moscoso C, 2015, ACM T APPL PERCEPT, V11, DOI 10.1145/2665078
   Naz A, 2017, P IEEE VIRT REAL ANN, P3, DOI 10.1109/VR.2017.7892225
   Oehlmann M, 2017, J ENVIRON ECON MANAG, V81, P59, DOI 10.1016/j.jeem.2016.09.002
   Palan S, 2018, J BEHAV EXP FINANC, V17, P22, DOI 10.1016/j.jbef.2017.12.004
   Peck TC, 2020, IEEE T VIS COMPUT GR, V26, P1945, DOI 10.1109/TVCG.2020.2973498
   Peer E, 2017, J EXP SOC PSYCHOL, V70, P153, DOI 10.1016/j.jesp.2017.01.006
   PHONG BT, 1975, COMMUN ACM, V18, P311, DOI 10.1145/360825.360839
   Pohl Brian J., 2017, P ACM SIGGRAPH DIG P
   Reinhard E., 2010, High dynamic range imaging: Acquisition, display, and image-based lighting
   Rockcastle S, 2021, LIGHTING RES TECHNOL, V53, P701, DOI 10.1177/1477153521990039
   Rolfe J, 2009, ECOL ECON, V68, P1140, DOI 10.1016/j.ecolecon.2008.08.007
   Rothe S, 2018, 17TH INTERNATIONAL CONFERENCE ON MOBILE AND UBIQUITOUS MULTIMEDIA (MUM 2018), P127, DOI 10.1145/3282894.3282896
   Rzig DE, 2023, PROCEEDINGS OF THE 32ND ACM SIGSOFT INTERNATIONAL SYMPOSIUM ON SOFTWARE TESTING AND ANALYSIS, ISSTA 2023, P1269, DOI 10.1145/3597926.3598134
   Schmelter L., 2023, 2023 CHI C HUM FACT, P1, DOI [10.1145/3544549.3583888, DOI 10.1145/3544549.3583888]
   Scorpio M, 2023, SUSTAINABILITY-BASEL, V15, DOI 10.3390/su15097491
   Slater M, 2009, IEEE COMPUT GRAPH, V29, P76, DOI 10.1109/MCG.2009.55
   Stauffert JP, 2020, FRONT VIRTUAL REAL, V1, DOI 10.3389/frvir.2020.582204
   Taehoon H, 2019, BUILD ENVIRON, V166, DOI 10.1016/j.buildenv.2019.106409
   Takoordyal Kishan, 2020, GameObjects, Prefabs, Materials, and Components, P59, DOI [10.1007/978-1-4842-6002-9_3, DOI 10.1007/978-1-4842-6002-9_3]
   WATSON D, 1988, J PERS SOC PSYCHOL, V54, P1063, DOI 10.1037/0022-3514.54.6.1063
   Weier M, 2016, COMPUT GRAPH FORUM, V35, P289, DOI 10.1111/cgf.13026
   Wolski K, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530136
   Yang YF, 2024, IEEE T MOBILE COMPUT, V23, P3620, DOI 10.1109/TMC.2023.3277577
   Zielasko D, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES WORKSHOPS (VRW 2020), P281, DOI [10.1109/VRW50115.2020.00059, 10.1109/VRW50115.2020.0-217]
NR 64
TC 0
Z9 0
U1 0
U2 0
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
EI 2577-6193
J9 P ACM COMPUT GRAPH
JI P. ACM Comput. Graph. Interact. Tech.
PD MAY
PY 2024
VL 7
IS 1
AR 5
DI 10.1145/3651292
PG 20
WC Computer Science, Software Engineering
WE Emerging Sources Citation Index (ESCI)
SC Computer Science
GA RQ5P8
UT WOS:001229142400005
DA 2024-08-05
ER

PT J
AU Granizo-Hidalgo, A
   Holzschuch, N
AF Granizo-Hidalgo, Ana
   Holzschuch, Nicolas
TI Interactive Rendering of Caustics using Dimension Reduction for Manifold
   Next-Event Estimation
SO PROCEEDINGS OF THE ACM ON COMPUTER GRAPHICS AND INTERACTIVE TECHNIQUES
LA English
DT Article
DE Interactive ray-tracing; Caustics; Next-Event Estimation
AB Specular surfaces, like water surfaces, create caustics by focusing the light being refracted or reflected. These caustics are very important for scene realism, but also challenging to render: to compute them, we need to find the exact path connecting two points through a specular reflective or refractive surface. This requires finding the roots of a complicated function on the surface. Manifold-Exploration methods find these roots using the Newton-Raphson method, but this involves computing path derivatives at each step, which can be challenging. We show that these roots lie on a curve on the surface, which reduces the dimensionality of the search. This dimension reduction greatly improves the search, allowing for interactive rendering of caustics. It also makes implementation easier, as we do not need to compute path derivatives.
C1 [Granizo-Hidalgo, Ana; Holzschuch, Nicolas] Univ Grenoble Alpes, INRIA, Grenoble INP, LJK,CNRS, Grenoble, France.
C3 Inria; Communaute Universite Grenoble Alpes; Universite Grenoble Alpes
   (UGA); Centre National de la Recherche Scientifique (CNRS); Institut
   National Polytechnique de Grenoble
RP Granizo-Hidalgo, A (corresponding author), Univ Grenoble Alpes, INRIA, Grenoble INP, LJK,CNRS, Grenoble, France.
EM Granizo-Hidalgo@inria.fr; Nicolas.Holzschuch@inria.fr
CR Hanika J, 2015, COMPUT GRAPH FORUM, V34, P87, DOI 10.1111/cgf.12681
   Jakob W, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185554
   Jakob Wenzel, 2010, Mitsuba renderer
   Jensen HenrikWann., 1996, Rendering Techniques '96, P21
   Li H, 2022, PROCEEDINGS SIGGRAPH ASIA 2022, DOI 10.1145/3550469.3555381
   Loubet G, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417811
   MITCHELL D, 1992, COMP GRAPH, V26, P283, DOI 10.1145/142920.134082
   Müller T, 2017, COMPUT GRAPH FORUM, V36, P91, DOI 10.1111/cgf.13227
   Nimier-David M, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356498
   Veach E., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P65, DOI 10.1145/258734.258775
   Walter B, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531398
   Yang Xueqing, 2021, Ray Tracing Gems, VII, P469
   Zeltner T, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392408
NR 13
TC 0
Z9 0
U1 0
U2 0
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
EI 2577-6193
J9 P ACM COMPUT GRAPH
JI P. ACM Comput. Graph. Interact. Tech.
PD MAY
PY 2024
VL 7
IS 1
AR 12
DI 10.1145/3651297
PG 16
WC Computer Science, Software Engineering
WE Emerging Sources Citation Index (ESCI)
SC Computer Science
GA RQ5P8
UT WOS:001229142400012
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Guo, SQ
   Choi, M
   Kao, D
   Mousas, C
AF Guo, Siqi
   Choi, Minsoo
   Kao, Dominic
   Mousas, Christos
TI Collaborating with my Doppelganger: The Effects of Self-similar
   Appearance and Voice of a Virtual Character during a Jigsaw Puzzle
   Co-solving Task
SO PROCEEDINGS OF THE ACM ON COMPUTER GRAPHICS AND INTERACTIVE TECHNIQUES
LA English
DT Article
DE Virtual Reality; Virtual Characters; Collaboration; Doppelganger;
   Self-similar Appearance; Self-similar Voice; Task Co-solving; Jigsaw
   Puzzle
ID SOCIAL PRESENCE; MERE EXPOSURE; RECOGNITION; PERSONALITY; EMBODIMENT;
   RESPONSES; AGENCY; USERS; GAZE; BODY
AB The research community has long been interested in human interaction with embodied virtual characters in virtual reality (VR). At the same time, interaction with self-similar virtual characters, or virtual doppelgangers, has become a prominent topic in both VR and psychology due to the intriguing psychological effects these characters can have on people. However, studies on human interaction with self-similar virtual characters are still limited. To address this research gap, we designed and conducted a 2 (appearance: self-similar vs. non-selfsimilar appearance) x 2 (voice: self-similar vs. non-self-similar voice) within-group study (N = 25) to explore how combinations of appearance and voice factors influence participants' perception of virtual characters. During the study, we asked participants to collaborate with a virtual character in solving a VR jigsaw puzzle. After each experimental condition, we had participants complete a survey about their experiences with the virtual character. Our findings showed that 1) the virtual characters' self-similarity in appearance enhanced the sense of co-presence and perceived intelligence, but it also elicited higher eeriness; 2) the self-similar voices led to higher ratings on the characters' likability and believability; however, they also induced a more eerie sensation; and 3) we observed an interaction effect between appearance and voice factors for ratings on believability, where the virtual characters were considered more believable when their self-similarity in appearance matched that of their voices. This study provided valuable insights and comprehensive guidance for creating novel collaborative experiences with self-similar virtual characters in immersive environments.
C1 [Guo, Siqi; Choi, Minsoo; Kao, Dominic; Mousas, Christos] Purdue Univ, 401 N Grant St, W Lafayette, IN 47907 USA.
C3 Purdue University System; Purdue University
RP Guo, SQ (corresponding author), Purdue Univ, 401 N Grant St, W Lafayette, IN 47907 USA.
EM guo477@purdue.edu; choi714@purdue.edu; kaod@purdue.edu;
   cmousas@purdue.edu
CR Aymerich-Franch L., 2012, Proceedings of the International Society for Presence Research Annual Conference, USA, P24
   Aymerich-Franch L., 2014, Proceedings of the International Society for Presence Research, P173
   Bailenson JN, 2010, HUM-COMPUT INT-SPRIN, P175, DOI 10.1007/978-1-84882-825-4_14
   Bailenson JN, 2005, HUM COMMUN RES, V31, P511, DOI 10.1111/j.1468-2958.2005.tb00881.x
   Biocca F, 2003, PRESENCE-VIRTUAL AUG, V12, P456, DOI 10.1162/105474603322761270
   Biocca F., 2001, 4 ANN INT WORKSH PRE
   Blascovich J, 2002, PSYCHOL INQ, V13, P103, DOI 10.1207/S15327965PLI1302_01
   Burton Tim., 1997, The Melancholy Death of Oyster Boy Other Stories
   BUUNK BP, 1991, PERS SOC PSYCHOL B, V17, P709, DOI 10.1177/0146167291176015
   Cabral JP, 2017, INTERSPEECH, P229, DOI 10.21437/Interspeech.2017-325
   Chérif E, 2019, RECH APPL MARKET-ENG, V34, P28, DOI 10.1177/2051570719829432
   Choi Minsoo, 2023, 2023 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct), P555, DOI 10.1109/ISMAR-Adjunct60411.2023.00118
   Cohen J., 1988, Statistical power analysis for the behavioral sciences
   Craig Scotty D., 2019, Proceedings of the Human Factors and Ergonomics Society Annual Meeting, V63, P2272, DOI 10.1177/1071181319631517
   Cui DX, 2023, COMPUT ANIMAT VIRT W, V34, DOI 10.1002/cav.2189
   Cui DX, 2021, COMPUT ANIMAT VIRT W, V32, DOI 10.1002/cav.2009
   de Rooij A, 2017, PROCEEDINGS OF THE 2017 ACM SIGCHI CONFERENCE ON CREATIVITY AND COGNITION (C&C 2017), P232, DOI 10.1145/3059454.3078856
   Faul F, 2007, BEHAV RES METHODS, V39, P175, DOI 10.3758/BF03193146
   Ferstl Y, 2021, PROCEEDINGS OF THE 21ST ACM INTERNATIONAL CONFERENCE ON INTELLIGENT VIRTUAL AGENTS (IVA), P76, DOI 10.1145/3472306.3478338
   Fisslerl P, 2018, FRONT AGING NEUROSCI, V10, DOI 10.3389/fnagi.2018.00299
   Fox J, 2015, HUM-COMPUT INTERACT, V30, P401, DOI 10.1080/07370024.2014.921494
   Fribourg R, 2018, 25TH 2018 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P273, DOI 10.1109/VR.2018.8448293
   Giuliani Manuel, 2011, SOCIAL ROBOTICS
   Gonzalez-Franco M, 2020, FRONT VIRTUAL REAL, V1, DOI 10.3389/frvir.2020.561558
   Goodman KL, 2023, APPL ERGON, V106, DOI 10.1016/j.apergo.2022.103864
   Gorisse G, 2018, AVI'18: PROCEEDINGS OF THE 2018 INTERNATIONAL CONFERENCE ON ADVANCED VISUAL INTERFACES, DOI 10.1145/3206505.3206525
   Gorisse G, 2019, FRONT ROBOT AI, V6, DOI 10.3389/frobt.2019.00008
   Guadagno RE, 2007, MEDIA PSYCHOL, V10, P1
   Guadagno RE, 2011, COMPUT HUM BEHAV, V27, P2380, DOI 10.1016/j.chb.2011.07.017
   Haring Markus, 2012, Social Robotics. 4th International Conference (ICSR 2012). Proceedings, P378, DOI 10.1007/978-3-642-34103-8_38
   Hatada Y, 2019, PROCEEDINGS OF THE 10TH AUGMENTED HUMAN INTERNATIONAL CONFERENCE 2019 (AH2019), DOI 10.1145/3311823.3311862
   Hegel F, 2008, 2008 17TH IEEE INTERNATIONAL SYMPOSIUM ON ROBOT AND HUMAN INTERACTIVE COMMUNICATION, VOLS 1 AND 2, P574, DOI 10.1109/ROMAN.2008.4600728
   Higgins D, 2022, COMPUT GRAPH-UK, V104, P116, DOI 10.1016/j.cag.2022.03.009
   Holling H, 2005, PERS INDIV DIFFER, V38, P503, DOI 10.1016/j.paid.2004.05.003
   Hooi R, 2014, COMPUT HUM BEHAV, V39, P20, DOI 10.1016/j.chb.2014.06.019
   Jun H, 2020, ADJUNCT PROCEEDINGS OF THE 2020 IEEE INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY (ISMAR-ADJUNCT 2020), P41, DOI 10.1109/ISMAR-Adjunct51615.2020.00026
   Kammler-Sücker KI, 2021, IEEE T NEUR SYS REH, V29, P2173, DOI 10.1109/TNSRE.2021.3120795
   Kao Dominic, 2021, Proceedings of the ACM on Human-Computer Interaction, V5, DOI 10.1145/3474661
   Kao Dominic, 2021, Proceedings of the ACM on Human-Computer Interaction, V5, DOI 10.1145/3474665
   Kao D, 2022, PROCEEDINGS OF THE 2022 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI' 22), DOI 10.1145/3491102.3501848
   Keenan JP, 2001, NATURE, V409, P305, DOI 10.1038/35053167
   Kim G, 2018, LECT NOTES COMPUT SC, V10910, P94, DOI 10.1007/978-3-319-91584-5_8
   Kim Hayeon, 2023, IEEE Transactions on Visualization and Computer Graphics
   Kim K, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR 2020), P529, DOI [10.1109/VR46266.2020.1581084624004, 10.1109/VR46266.2020.00-30]
   Kim K, 2018, INT SYM MIX AUGMENT, P105, DOI 10.1109/ISMAR.2018.00039
   Kleinlogel EP, 2021, PLOS ONE, V16, DOI 10.1371/journal.pone.0245960
   Krogmeier C, 2020, COMPUT ANIMAT VIRT W, V31, DOI 10.1002/cav.1941
   Krogmeier C, 2019, COMPUT ANIMAT VIRT W, V30, DOI 10.1002/cav.1883
   Kruger J, 1999, J PERS SOC PSYCHOL, V77, P221, DOI 10.1037/0022-3514.77.2.221
   Kyrlitsias C, 2022, FRONT VIRTUAL REAL, V2, DOI 10.3389/frvir.2021.786665
   Lam L, 2023, PROCEEDINGS OF THE ACM SYMPOSIUM ON APPLIED PERCEPTION, SAP 2023, DOI 10.1145/3605495.3605791
   Lee JG, 2015, INT J HUM-COMPUT INT, V31, P682, DOI 10.1080/10447318.2015.1070547
   Lee KM, 2005, MEDIA PSYCHOL, V7, P31, DOI 10.1207/S1532785XMEP0701_2
   Lee KM, 2006, J COMMUN, V56, P754, DOI 10.1111/j.1460-2466.2006.00318.x
   Liew TW, 2023, EDUC INF TECHNOL, V28, P1455, DOI 10.1007/s10639-022-11255-6
   Mannisto-Funk Tiina, 2018, Digital Culture & Society, V4, P45, DOI 10.14361/dcs-2018-0105
   Mazumdar A, 2021, COMPUT ANIMAT VIRT W, V32, DOI 10.1002/cav.2004
   McDonnell R, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185587
   Montoya RM, 2017, PSYCHOL BULL, V143, P459, DOI 10.1037/bul0000085
   Mori M., 1970, IEEE Spectrum, V6
   Mousas C, 2021, 2021 IEEE VIRTUAL REALITY AND 3D USER INTERFACES (VR), P40, DOI 10.1109/VR50410.2021.00024
   Mousas C, 2018, COMPUT HUM BEHAV, V86, P99, DOI 10.1016/j.chb.2018.04.036
   Moussawi S, 2019, PROCEEDINGS OF THE 52ND ANNUAL HAWAII INTERNATIONAL CONFERENCE ON SYSTEM SCIENCES, P115
   NASS C, 1994, HUMAN FACTORS IN COMPUTING SYSTEMS, CHI '94 CONFERENCE PROCEEDINGS - CELEBRATING INTERDEPENDENCE, P72, DOI 10.1145/191666.191703
   Nass C., 2000, P SIGCHI C HUMAN FAC, V2, P329, DOI [10.1145/332040.332452, DOI 10.1145/332040.332452]
   Nelson MG, 2023, INT SYM MIX AUGMENT, P930, DOI 10.1109/ISMAR59233.2023.00109
   Nelson MG, 2022, IEEE INT SYMP M AU R, P594, DOI 10.1109/ISMAR-Adjunct57072.2022.00123
   Norouzi N, 2018, 18TH ACM INTERNATIONAL CONFERENCE ON INTELLIGENT VIRTUAL AGENTS (IVA'18), P17, DOI 10.1145/3267851.3267901
   Nowak KL, 2003, PRESENCE-TELEOP VIRT, V12, P481, DOI 10.1162/105474603322761289
   Pan Y, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0189078
   Parmar D, 2022, AUTON AGENT MULTI-AG, V36, DOI 10.1007/s10458-021-09539-1
   Pathi SK, 2019, MULTIMODAL TECHNOLOG, V3, DOI 10.3390/mti3040069
   Praetorius Anna Samira, 2020, P 15 INT C FDN DIG G, P1
   Qu C, 2014, COMPUT HUM BEHAV, V34, P58, DOI 10.1016/j.chb.2014.01.033
   Reysen S, 2005, SOC BEHAV PERSONAL, V33, P201, DOI 10.2224/sbp.2005.33.2.201
   Richards D, 2020, PROCEEDINGS OF THE AUSTRALASIAN COMPUTER SCIENCE WEEK MULTICONFERENCE (ACSW 2020), DOI 10.1145/3373017.3373065
   Ruijten PAM, 2019, INT J SOC ROBOT, V11, P477, DOI 10.1007/s12369-019-00516-z
   Salem M., 2011, 2011 RO-MAN: The 20th IEEE International Symposium on Robot and Human Interactive Communication, P247, DOI 10.1109/ROMAN.2011.6005285
   Schrammel F, 2009, PSYCHOPHYSIOLOGY, V46, P922, DOI 10.1111/j.1469-8986.2009.00831.x
   Schultze U, 2010, J INF TECHNOL-UK, V25, P434, DOI 10.1057/jit.2010.25
   Shih Meng Ting, 2023, P ACM HUMAN COMPUTER, V7, P1
   Short E, 2010, ACMIEEE INT CONF HUM, P219, DOI 10.1109/HRI.2010.5453193
   Siau K., 2018, Cutter Business Technology Journal, V31, P47
   Smith HJ, 2018, PROCEEDINGS OF THE 2018 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2018), DOI 10.1145/3173574.3173863
   Spiel K, 2021, PROCEEDINGS OF THE 2021 ACM DESIGNING INTERACTIVE SYSTEMS CONFERENCE (DIS 2021), P478, DOI 10.1145/3461778.3462033
   Stein JP, 2017, COGNITION, V160, P43, DOI 10.1016/j.cognition.2016.12.010
   Ullman Daniel., 2014, Proceedings of the Annual Meeting of the Cognitive Science Society, V36
   van Vugt HC, 2010, ACM T COMPUT-HUM INT, V17, DOI 10.1145/1746259.1746261
   Volonte M, 2019, PROCEEDINGS OF THE 19TH ACM INTERNATIONAL CONFERENCE ON INTELLIGENT VIRTUAL AGENTS (IVA' 19), P141, DOI 10.1145/3308532.3329461
   Volonte M, 2016, IEEE T VIS COMPUT GR, V22, P1326, DOI 10.1109/TVCG.2016.2518158
   Waltemate T, 2018, IEEE T VIS COMPUT GR, V24, P1643, DOI 10.1109/TVCG.2018.2794629
   Wauck H, 2018, PROCEEDINGS OF THE 2018 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2018), DOI 10.1145/3173574.3174059
   Weitz K, 2019, PROCEEDINGS OF THE 19TH ACM INTERNATIONAL CONFERENCE ON INTELLIGENT VIRTUAL AGENTS (IVA' 19), P7, DOI 10.1145/3308532.3329441
   WILLIAMS EJ, 1949, AUST J SCI RES SER A, V2, P149, DOI 10.1071/CH9490149
   Yamada M, 2013, P NATL ACAD SCI USA, V110, P4363, DOI 10.1073/pnas.1221681110
   Yee N, 2007, HUM COMMUN RES, V33, P271, DOI 10.1111/j.1468-2958.2007.00299.x
   Zajonc RB, 2001, CURR DIR PSYCHOL SCI, V10, P224, DOI 10.1111/1467-8721.00154
   Zibrek K, 2018, IEEE T VIS COMPUT GR, V24, P1681, DOI 10.1109/TVCG.2018.2794638
   Zibrek Katja, 2021, P 14 ACM SIGGRAPH C, P1
NR 99
TC 0
Z9 0
U1 0
U2 0
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
EI 2577-6193
J9 P ACM COMPUT GRAPH
JI P. ACM Comput. Graph. Interact. Tech.
PD MAY
PY 2024
VL 7
IS 1
AR 4
DI 10.1145/3651288
PG 23
WC Computer Science, Software Engineering
WE Emerging Sources Citation Index (ESCI)
SC Computer Science
GA RQ5P8
UT WOS:001229142400004
OA hybrid
DA 2024-08-05
ER

PT J
AU Aranjuelo, N
   Huang, SY
   Arganda-Carreras, I
   Unzueta, L
   Otaegui, O
   Pfister, H
   Wei, DL
AF Aranjuelo, Nerea
   Huang, Siyu
   Arganda-Carreras, Ignacio
   Unzueta, Luis
   Otaegui, Oihana
   Pfister, Hanspeter
   Wei, Donglai
TI Learning Gaze-aware Compositional GAN from Limited Annotations
SO PROCEEDINGS OF THE ACM ON COMPUTER GRAPHICS AND INTERACTIVE TECHNIQUES
LA English
DT Article
DE Gaze estimation; synthetic data; GAN; generative; DNN; domain transfer
AB Gaze-annotated facial data is crucial for training deep neural networks (DNNs) for gaze estimation. However, obtaining these data is labor-intensive and requires specialized equipment due to the challenge of accurately annotating the gaze direction of a subject. In this work, we present a generative framework to create annotated gaze data by leveraging the benefits of labeled and unlabeled data sources. We propose a Gaze-aware Compositional GAN that learns to generate annotated facial images from a limited labeled dataset. Then we transfer this model to an unlabeled data domain to take advantage of the diversity it provides. Experiments demonstrate our approach's effectiveness in generating within-domain image augmentations in the ETH-XGaze dataset and cross-domain augmentations in the CelebAMask-HQ dataset domain for gaze estimation DNN training. We also show additional applications of our work, which include facial image editing and gaze redirection.
C1 [Aranjuelo, Nerea; Unzueta, Luis; Otaegui, Oihana] Fdn Vicomtech, Basque Res & Technol Alliance, San Sebastian, Spain.
   [Huang, Siyu] Clemson Univ, Clemson, SC 29631 USA.
   [Arganda-Carreras, Ignacio] Univ Basque Country, Leioa, Spain.
   [Arganda-Carreras, Ignacio] Ikerbasque, Bilbao, Spain.
   [Arganda-Carreras, Ignacio] Donostia Int Phys Ctr, San Sebastian, Spain.
   [Arganda-Carreras, Ignacio] Biofisika Inst, Leioa, Spain.
   [Pfister, Hanspeter] Harvard John A Paulson Sch Engn & Appl Sci, Boston, MA USA.
   [Wei, Donglai] Boston Coll, Chestnut Hill, MA 02167 USA.
C3 Clemson University; University of Basque Country; Consejo Superior de
   Investigaciones Cientificas (CSIC); University of Basque Country; CSIC -
   UPV EHU - Instituto Biofisika; Boston College
RP Aranjuelo, N (corresponding author), Fdn Vicomtech, Basque Res & Technol Alliance, San Sebastian, Spain.
EM naranjuelo@vicomtech.org; siyuh@clemson.edu; ignacio.arganda@ehu.eus;
   lunzueta@vicomtech.org; ootaegui@vicomtech.org; pfister@g.harvard.edu;
   weidf@bc.edu
RI Arganda-Carreras, Ignacio/L-4605-2014; Unzueta, Luis/L-6867-2014
OI Arganda-Carreras, Ignacio/0000-0003-0229-5722; Unzueta,
   Luis/0000-0001-5648-0910; Huang, Siyu/0000-0002-2929-0115; Pfister,
   Hanspeter/0000-0002-3620-2582
FU Basque Government; University of the Basque Country UPV/EHU [GIU19/027];
   NSF-IIS [2239688]; NSF [III-2107328]
FX This work has been partially funded by the Basque Government under the
   project AutoTrust (Elkartek-2023 program), the University of the Basque
   Country UPV/EHU grant GIU19/027, NSF-IIS grant 2239688 and NSF grant
   III-2107328.
CR Abdelrahman A.A., 2022, arXiv
   Amini A, 2019, AIES '19: PROCEEDINGS OF THE 2019 AAAI/ACM CONFERENCE ON AI, ETHICS, AND SOCIETY, P289, DOI 10.1145/3306618.3314243
   Arar NM, 2017, IEEE T CIRC SYST VID, V27, P2623, DOI 10.1109/TCSVT.2016.2595322
   Bazarevsky V, 2019, Arxiv, DOI [arXiv:1907.05047, 10.48550/arXiv.1907.05047]
   Benaim S, 2018, ADV NEUR IN, V31
   Brock A, 2019, Arxiv, DOI arXiv:1809.11096
   Chen JJ, 2021, IEEE WINT CONF APPL, P3664, DOI 10.1109/WACV48630.2021.00371
   Cheng Y., 2021, arXiv
   Cheng YH, 2022, INT C PATT RECOG, P3341, DOI 10.1109/ICPR56361.2022.9956687
   Creswell A, 2018, IEEE SIGNAL PROC MAG, V35, P53, DOI 10.1109/MSP.2017.2765202
   Deng Y, 2020, PROC CVPR IEEE, P5153, DOI 10.1109/CVPR42600.2020.00520
   Ganin Y, 2016, LECT NOTES COMPUT SC, V9906, P311, DOI 10.1007/978-3-319-46475-6_20
   Harkonen E, 2020, C NEUR INF PROC SYST
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hensel M, 2017, ADV NEUR IN, V30
   Ishikawa T, 2004, Passive driver gaze tracking with active appearance models
   Jahanian A., 2019, arXiv
   Karras T, 2021, ADV NEUR IN, V34
   Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453
   Kartynnik Y, 2019, Arxiv, DOI [arXiv:1907.06724, DOI 10.48550/ARXIV.1907.06724]
   Kaur H, 2020, IEEE WINT CONF APPL, P299, DOI 10.1109/WACV45572.2020.9093433
   Kellnhofer P, 2019, IEEE I CONF COMP VIS, P6911, DOI 10.1109/ICCV.2019.00701
   Kingma D. P., 2014, arXiv
   Lee CH, 2020, PROC CVPR IEEE, P5548, DOI 10.1109/CVPR42600.2020.00559
   Li GY, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530130
   Masko D., 2017, Calibration in Eye Tracking Using Transfer Learning
   Mescheder L, 2018, PR MACH LEARN RES, V80
   Mildenhall B, 2022, COMMUN ACM, V65, P99, DOI 10.1145/3503250
   Mitchell M, 2019, FAT*'19: PROCEEDINGS OF THE 2019 CONFERENCE ON FAIRNESS, ACCOUNTABILITY, AND TRANSPARENCY, P220, DOI 10.1145/3287560.3287596
   Nikolenko S.I., 2021, Synthetic data for deep learning
   Park S, 2019, IEEE I CONF COMP VIS, P9367, DOI 10.1109/ICCV.2019.00946
   Porta S, 2019, IEEE INT CONF COMP V, P3660, DOI 10.1109/ICCVW.2019.00451
   Pushkarna Mahima, 2022, FAccT '22: 2022 ACM Conference on Fairness, Accountability, and Transparency, P1776, DOI 10.1145/3531146.3533231
   Salimans T, 2016, ADV NEUR IN, V29
   Sela M, 2017, Arxiv, DOI arXiv:1711.09767
   Shen Y., 2020, P IEEECVF C COMPUTER, P9243
   Shi YC, 2022, PROC CVPR IEEE, P11244, DOI 10.1109/CVPR52688.2022.01097
   Shrivastava A, 2017, PROC CVPR IEEE, P2242, DOI 10.1109/CVPR.2017.241
   Shu YZ, 2022, IEEE T VIS COMPUT GR, V28, P3376, DOI 10.1109/TVCG.2021.3067201
   Sinha N, 2021, 2021 17TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE (AVSS 2021), DOI 10.1109/AVSS52988.2021.9663816
   Wood E, 2016, 2016 ACM SYMPOSIUM ON EYE TRACKING RESEARCH & APPLICATIONS (ETRA 2016), P131, DOI 10.1145/2857491.2857492
   Xia WH, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1782, DOI 10.1145/3394171.3413868
   Xucong Zhang, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12350), P365, DOI 10.1007/978-3-030-58558-7_22
   Zhang JC, 2019, Arxiv, DOI arXiv:1906.00805
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhang XC, 2015, PROC CVPR IEEE, P4511, DOI 10.1109/CVPR.2015.7299081
NR 46
TC 0
Z9 0
U1 0
U2 0
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
EI 2577-6193
J9 P ACM COMPUT GRAPH
JI P. ACM Comput. Graph. Interact. Tech.
PD MAY
PY 2024
VL 7
IS 2
AR 28
DI 10.1145/3654706
PG 17
WC Computer Science, Software Engineering
WE Emerging Sources Citation Index (ESCI)
SC Computer Science
GA SL0I6
UT WOS:001234486100006
OA Bronze
DA 2024-08-05
ER

PT J
AU Maquiling, V
   Byrne, SA
   Niehorster, DC
   Nystrom, M
   Kasneci, E
AF Maquiling, Virmarie
   Byrne, Sean Anthony
   Niehorster, Diederick C.
   Nystrom, Marcus
   Kasneci, Enkelejda
TI Zero-Shot Segmentation of Eye Features Using the Segment Anything Model
   (SAM)
SO PROCEEDINGS OF THE ACM ON COMPUTER GRAPHICS AND INTERACTIVE TECHNIQUES
LA English
DT Article
DE Eye-tracking; Segmentation; Segment Anything Model; Zero-shot learning;
   Foundational models; Prompt Engineering
ID ROBUST PUPIL DETECTION; TRACKING
AB The advent of foundation models signals a new era in artificial intelligence. The Segment Anything Model (SAM) is the first foundation model for image segmentation. In this study, we evaluate SAM's ability to segment features from eye images recorded in virtual reality setups. The increasing requirement for annotated eye-image datasets presents a significant opportunity for SAM to redefine the landscape of data annotation in gaze estimation. Our investigation centers on SAM's zero-shot learning abilities and the effectiveness of prompts like bounding boxes or point clicks. Our results are consistent with studies in other domains, demonstrating that SAM's segmentation effectiveness can be on-par with specialized models depending on the feature, with prompts improving its performance, evidenced by an IoU of 93.34% for pupil segmentation in one dataset. Foundation models like SAM could revolutionize gaze estimation by enabling quick and easy image segmentation, reducing reliance on specialized models and extensive manual annotation.
C1 [Maquiling, Virmarie; Kasneci, Enkelejda] Tech Univ Munich, Human Ctr Technol Learning, Marsstr 20-22, D-80335 Munich, Germany.
   [Byrne, Sean Anthony] IMT Sch Adv Studies Lucca, MoMiLab, Piazza S Francesco 19, I-55100 Lucca, LU, Italy.
   [Niehorster, Diederick C.; Nystrom, Marcus] Lund Univ, Humanities Lab, Lund, Sweden.
   [Niehorster, Diederick C.] Lund Univ, Dept Psychol, Lund, Sweden.
C3 Technical University of Munich; IMT School for Advanced Studies Lucca;
   Lund University; Lund University
RP Maquiling, V (corresponding author), Tech Univ Munich, Human Ctr Technol Learning, Marsstr 20-22, D-80335 Munich, Germany.
EM virmarie.maquiling@tum.de; sean.byrne@imtlucca.it;
   diederick_c.niehorster@humlab.lu.se; marcus.nystrom@humlab.lu.se;
   enkelejda.kasneci@tum.de
RI ; Niehorster, Diederick Christian/E-9325-2010
OI Maquiling, Virmarie/0009-0005-5580-2304; Niehorster, Diederick
   Christian/0000-0002-4672-8756; Nystrom, Marcus/0000-0002-2089-9012
CR Archit Anwai, 2023, bioRxiv
   Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615
   Bommasani R., 2021, arXiv, DOI DOI 10.48550/ARXIV.2108.07258
   Bommasani Rishi, P ACM COMP GRAPH INT, V7
   Brown T., 2020, ADV NEURAL INFORM PR, V33, P1877, DOI DOI 10.48550/ARXIV.2005.14165
   Byrne SA, 2023, BEHAV RES METHODS, DOI 10.3758/s13428-023-02297-w
   Byrne SA, 2023, Arxiv, DOI arXiv:2309.06129
   Chaudhary AK, 2022, IEEE T VIS COMPUT GR, V28, P3948, DOI 10.1109/TVCG.2022.3203100
   Chugh S, 2021, INT C PATT RECOG, P2210, DOI 10.1109/ICPR48806.2021.9412066
   Devlin J, 2019, Arxiv, DOI arXiv:1810.04805
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Dunn MJ, 2023, BEHAV RES METHODS, DOI 10.3758/s13428-023-02187-1
   Fuhl W, 2016, Arxiv, DOI [arXiv:1601.04902, DOI 10.48550/ARXIV.1601.04902]
   Fuhl W, 2021, INT SYM MIX AUGMENT, P367, DOI 10.1109/ISMAR52148.2021.00053
   Fuhl W, 2020, ETRA 2020 SHORT PAPERS: ACM SYMPOSIUM ON EYE TRACKING RESEARCH & APPLICATIONS, DOI 10.1145/3379156.3391347
   Fuhl W, 2016, MACH VISION APPL, V27, P1275, DOI 10.1007/s00138-016-0776-4
   Fuhl W, 2015, LECT NOTES COMPUT SC, V9256, P39, DOI 10.1007/978-3-319-23192-1_4
   Garbin SJ, 2020, ETRA'20 FULL PAPERS: ACM SYMPOSIUM ON EYE TRACKING RESEARCH AND APPLICATIONS, DOI 10.1145/3379155.3391317
   He KM, 2022, PROC CVPR IEEE, P15979, DOI 10.1109/CVPR52688.2022.01553
   Holmqvist K., 2011, EYE TRACKING COMPREH
   Huang YH, 2024, MED IMAGE ANAL, V92, DOI 10.1016/j.media.2023.103061
   HUTTENLOCHER DP, 1993, IEEE T PATTERN ANAL, V15, P850, DOI 10.1109/34.232073
   Jing YC, 2023, Arxiv, DOI arXiv:2304.11595
   Kim J, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300780
   Kirillov A, 2023, IEEE I CONF COMP VIS, P3992, DOI 10.1109/ICCV51070.2023.00371
   Kothari Rakshit S., 2022, Proceedings of the ACM on Human-Computer Interaction, V6, DOI 10.1145/3530880
   Kothari R, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-020-59251-5
   Kothari RS, 2021, IEEE T VIS COMPUT GR, V27, P2757, DOI 10.1109/TVCG.2021.3067765
   Ma J, 2024, NAT COMMUN, V15, DOI 10.1038/s41467-024-44824-z
   Maquiling V, 2023, PUBLICATION OF THE 25TH ACM INTERNATIONAL CONFERENCE ON MOBILE HUMAN-COMPUTER INTERACTION, MOBILEHCI 2023 ADJUNCT, DOI 10.1145/3565066.3608690
   Mattjie Christian, 2023, 2023 IEEE 23rd International Conference on Bioinformatics and Bioengineering (BIBE), P108, DOI 10.1109/BIBE60311.2023.00025
   Mazurowski MA, 2023, MED IMAGE ANAL, V89, DOI 10.1016/j.media.2023.102918
   Milletari F, 2016, INT CONF 3D VISION, P565, DOI 10.1109/3DV.2016.79
   Nair N, 2020, ACM SYMPOSIUM ON APPLIED PERCEPTION (SAP 2020), DOI 10.1145/3385955.3407935
   Palmero C, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21144769
   Radford A, 2021, PR MACH LEARN RES, V139
   Romera-Paredes B, 2015, PR MACH LEARN RES, V37, P2152
   Santini T, 2018, COMPUT VIS IMAGE UND, V170, P40, DOI 10.1016/j.cviu.2018.02.002
   Shen QH, 2023, Arxiv, DOI arXiv:2304.10261
   Wang B, 2023, Arxiv, DOI arXiv:2304.13844
   Wang D, 2023, Arxiv, DOI arXiv:2305.02034
   Yang JY, 2023, Arxiv, DOI [arXiv:2304.11968, DOI 10.48550/ARXIV.2304.11968]
   Zhou C, 2023, Arxiv, DOI [arXiv:2302.09419, DOI 10.48550/ARXIV.2302.09419]
   Zhou DF, 2019, INT CONF 3D VISION, P85, DOI 10.1109/3DV.2019.00019
   Zhou KY, 2023, IEEE T PATTERN ANAL, V45, P4396, DOI 10.1109/TPAMI.2022.3195549
   Zhou YK, 2023, NATURE, V622, P156, DOI 10.1038/s41586-023-06555-x
NR 46
TC 1
Z9 1
U1 2
U2 2
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
EI 2577-6193
J9 P ACM COMPUT GRAPH
JI P. ACM Comput. Graph. Interact. Tech.
PD MAY
PY 2024
VL 7
IS 2
AR 26
DI 10.1145/3654704
PG 16
WC Computer Science, Software Engineering
WE Emerging Sources Citation Index (ESCI)
SC Computer Science
GA SL0I6
UT WOS:001234486100004
OA Green Submitted, hybrid
DA 2024-08-05
ER

PT J
AU Talukdar, B
   Zhang, YH
   Weiss, T
AF Talukdar, Bilas
   Zhang, Yunhao
   Weiss, Tomer
TI Learning Crowd Motion Dynamics with Crowds
SO PROCEEDINGS OF THE ACM ON COMPUTER GRAPHICS AND INTERACTIVE TECHNIQUES
LA English
DT Article
DE Collision Avoidance; Position Based Dynamics; Policy Optimization
ID NAVIGATION
AB Reinforcement Learning (RL) has become a popular framework for learning desired behaviors for computational agents in graphics and games. In a multi-agent crowd, one major goal is for agents to avoid collisions while navigating in a dynamic environment. Another goal is to simulate natural-looking crowds, which is difficult to define due to the ambiguity as to what is a natural crowd motion. We introduce a novel methodology for simulating crowds, which learns most-preferred crowd simulation behaviors from crowd-sourced votes via Bayesian optimization. Our method uses deep reinforcement learning for simulating crowds, where crowd-sourcing is used to select policy hyper-parameters. Training agents with such parameters results in a crowd simulation that is preferred to users. We demonstrate our method's robustness in multiple scenarios and metrics, where we show it is superior compared to alternate policies and prior work.
C1 [Talukdar, Bilas; Zhang, Yunhao; Weiss, Tomer] New Jersey Inst Technol, Newark, NJ 07102 USA.
C3 New Jersey Institute of Technology
RP Talukdar, B (corresponding author), New Jersey Inst Technol, Newark, NJ 07102 USA.
EM bt26@njit.edu; yz862@njit.edu; tomer.weiss@njit.edu
CR [Anonymous], 2015, TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems
   Benes Bedrich, 2020, S INT 3D GRAPH GAM, P1
   Brochu Eric, 2010, S COMP AN
   Brochu Eric, 2007, ADV NEURAL INF PROCE, P409
   Chu W., 2005, Proceedings of the 22nd international conference on Machine learning, P137
   Ennis C, 2011, ACM T APPL PERCEPT, V8, DOI 10.1145/1870076.1870078
   Fleiss JL, 2013, Statistical methods for rates and proportions, DOI DOI 10.1002/0471445428
   Greenhill S, 2020, IEEE ACCESS, V8, P13937, DOI 10.1109/ACCESS.2020.2966228
   Guy S. J., 2009, P 2009 ACM SIGGRAPH, P177, DOI DOI 10.1145/1599470.1599494
   Guy Stephen J., 2010, S COMP AN
   He X, 2021, KNOWL-BASED SYST, V212, DOI 10.1016/j.knosys.2020.106622
   Helbing D, 2000, NATURE, V407, P487, DOI 10.1038/35035023
   HELBING D, 1995, PHYS REV E, V51, P4282, DOI 10.1103/PhysRevE.51.4282
   HENDERSON LF, 1974, TRANSPORT RES, V8, P509, DOI 10.1016/0041-1647(74)90027-6
   Hu KD, 2023, IEEE T VIS COMPUT GR, V29, P2036, DOI 10.1109/TVCG.2021.3139031
   Hughes RL, 2002, TRANSPORT RES B-METH, V36, P507, DOI 10.1016/S0191-2615(01)00015-7
   Jiang H, 2018, ACM SIGGRAPH SYMPOSIUM ON INTERACTIVE 3D GRAPHICS AND GAMES (I3D 2018), DOI 10.1145/3190834.3190844
   Karamouzas I, 2014, PHYS REV LETT, V113, DOI 10.1103/PhysRevLett.113.238701
   Koyama Y, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073598
   Kwiatkowski A, 2023, COMPUT GRAPH-UK, V110, P28, DOI 10.1016/j.cag.2022.11.007
   Kwiatkowski A, 2022, COMPUT GRAPH FORUM, V41, P613, DOI 10.1111/cgf.14504
   Lee J, 2018, ACM SIGGRAPH CONFERENCE ON MOTION, INTERACTION, AND GAMES (MIG 2018), DOI 10.1145/3274247.3274510
   López A, 2019, COMPUT GRAPH FORUM, V38, P181, DOI 10.1111/cgf.13629
   Marks J., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P389, DOI 10.1145/258734.258887
   Moussaïd M, 2011, P NATL ACAD SCI USA, V108, P6884, DOI 10.1073/pnas.1016507108
   Panayiotou Andreas, 2022, ACM Transactions on Graphics (TOG)
   Ren Z, 2017, COMPUT GRAPH FORUM, V36, P45, DOI 10.1111/cgf.12993
   Schulman J, 2017, Arxiv, DOI [arXiv:1707.06347, DOI 10.48550/ARXIV.1707.06347]
   Sun LB, 2019, IEEE ACCESS, V7, P109544, DOI 10.1109/ACCESS.2019.2933492
   Talukdar B, 2023, PROCEEDINGS OF SIGGRAPH 2023 POSTERS, SIGGRAPH 2023, DOI 10.1145/3588028.3603670
   Terry JK, 2023, Arxiv, DOI arXiv:2005.13625
   Tracy Daniel M, 2008, P 16 ACM SIGSPATIAL, P1
   van den Berg J, 2011, SPRINGER TRAC ADV RO, V70, P3
   van Toll W, 2020, I3D 2020: ACM SIGGRAPH SYMPOSIUM ON INTERACTIVE 3D GRAPHICS AND GAMES, DOI 10.1145/3384382.3384532
   Vinyals O, 2019, NATURE, V575, P350, DOI 10.1038/s41586-019-1724-z
   Wang H, 2016, PROCEEDINGS I3D 2016: 20TH ACM SIGGRAPH SYMPOSIUM ON INTERACTIVE 3D GRAPHICS AND GAMES, P49, DOI 10.1145/2856400.2856410
   Watson B, 2019, P ACM COMPUT GRAPH, V2, DOI 10.1145/3320286
   Weiss T, 2023, P ACM COMPUT GRAPH, V6, DOI 10.1145/3585507
   Weiss T, 2019, COMPUT GRAPH-UK, V78, P12, DOI 10.1016/j.cag.2018.10.008
   Wolinski D, 2014, COMPUT GRAPH FORUM, V33, P303, DOI 10.1111/cgf.12328
   Xu D, 2020, T GIS, V24, P756, DOI 10.1111/tgis.12620
NR 41
TC 0
Z9 0
U1 0
U2 0
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
EI 2577-6193
J9 P ACM COMPUT GRAPH
JI P. ACM Comput. Graph. Interact. Tech.
PD MAY
PY 2024
VL 7
IS 1
AR 9
DI 10.1145/3651302
PG 17
WC Computer Science, Software Engineering
WE Emerging Sources Citation Index (ESCI)
SC Computer Science
GA RQ5P8
UT WOS:001229142400009
OA hybrid
DA 2024-08-05
ER

PT J
AU Wang, B
   Dutt, NS
   Mitra, NJ
AF Wang, Binglun
   Dutt, Niladri Shekhar
   Mitra, Niloy J.
TI ProteusNeRF: Fast Lightweight NeRF Editing using 3D-Aware Image Context
SO PROCEEDINGS OF THE ACM ON COMPUTER GRAPHICS AND INTERACTIVE TECHNIQUES
LA English
DT Article
DE Neural Editing; Interactive 3D Editing; Neural Radiance Field; Stable
   Diffusion Model; Generative AI; ProteusNeRF
AB Neural Radiance Fields (NeRFs) have recently emerged as a popular option for photo-realistic object capture due to their ability to faithfully capture high-fidelity volumetric content even from handheld video input. Although much research has been devoted to efficient optimization leading to real-time training and rendering, options for interactive editing NeRFs remain limited. We present a very simple but effective neural network architecture that is fast and efficient while maintaining a low memory footprint. This architecture can be incrementally guided through user-friendly image-based edits. Our representation allows straightforward object selection via semantic feature distillation at the training stage. More importantly, we propose a local 3D-aware image context to facilitate view-consistent image editing that can then be distilled into fine-tuned NeRFs, via geometric and appearance adjustments. We evaluate our setup on a variety of examples to demonstrate appearance and geometric edits and report 10-30x speedup over concurrent work focusing on text-guided NeRF editing. Video results and code can be found on our project webpage at https://proteusnerf.github.io.
C1 [Wang, Binglun; Dutt, Niladri Shekhar; Mitra, Niloy J.] UCL, London, England.
C3 University of London; University College London
RP Wang, B (corresponding author), UCL, London, England.
EM ucabbw5@ucl.ac.uk; niladri.dutt.22@alumni.ucl.ac.uk; n.mitra@ucl.ac.uk
OI Mitra, Niloy/0000-0002-2597-0914
CR [Anonymous], 2023, Polycam-LiDAR 3D Scanner (3.1.3)
   Bar-Tal O, 2023, Arxiv, DOI arXiv:2302.08113
   Barron JT, 2022, PROC CVPR IEEE, P5460, DOI 10.1109/CVPR52688.2022.00539
   Brooks T, 2023, PROC CVPR IEEE, P18392, DOI 10.1109/CVPR52729.2023.01764
   Caron M, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9630, DOI 10.1109/ICCV48922.2021.00951
   Chan ER, 2022, PROC CVPR IEEE, P16102, DOI 10.1109/CVPR52688.2022.01565
   Chen AP, 2022, LECT NOTES COMPUT SC, V13692, P333, DOI 10.1007/978-3-031-19824-3_20
   Chiang PZ, 2022, IEEE WINT CONF APPL, P215, DOI 10.1109/WACV51458.2022.00029
   Croitoru FA, 2023, IEEE T PATTERN ANAL, V45, P10850, DOI 10.1109/TPAMI.2023.3261988
   Fischer M, 2024, Arxiv, DOI arXiv:2402.08622
   Fridovich-Keil S, 2023, PROC CVPR IEEE, P12479, DOI 10.1109/CVPR52729.2023.01201
   Fridovich-Keil S, 2022, PROC CVPR IEEE, P5491, DOI 10.1109/CVPR52688.2022.00542
   Garbin SJ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14326, DOI 10.1109/ICCV48922.2021.01408
   Gong BC, 2023, Arxiv, DOI arXiv:2301.07958
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Haque A, 2023, Arxiv, DOI arXiv:2303.12789
   Hedman P, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5855, DOI 10.1109/ICCV48922.2021.00582
   Ho J., 2020, Adv. Neural. Inf. Process. Syst, V33, P6840, DOI DOI 10.48550/ARXIV.2006.11239
   Huang H.-P., 2021, IEEE CVF INT C COMP, P13869
   Huang YH, 2022, PROC CVPR IEEE, P18321, DOI 10.1109/CVPR52688.2022.01780
   Jain A, 2022, PROC CVPR IEEE, P857, DOI 10.1109/CVPR52688.2022.00094
   Jambon C, 2023, P ACM COMPUT GRAPH, V6, DOI 10.1145/3585499
   Karnewar Animesh, 2022, SIGGRAPH22 Conference Proceeding: Special Interest Group on Computer Graphics and Interactive Techniques Conference Proceedings, DOI 10.1145/3528233.3530707
   Kingma D. P., 2014, arXiv
   Kobayashi Sosuke, 2022, ADV NEURAL INFORM PR, V35, P23311
   Kuang ZF, 2023, PROC CVPR IEEE, P20691, DOI 10.1109/CVPR52729.2023.01982
   Lee Jae-Hyeok, 2023, P IEEECVF INT C COMP, P3491
   Metzer G, 2023, PROC CVPR IEEE, P12663, DOI 10.1109/CVPR52729.2023.01218
   Mildenhall B, 2022, COMMUN ACM, V65, P99, DOI 10.1145/3503250
   Mildenhall B, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3322980
   Müller T, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530127
   Nguyen-Phuoc Thu, 2022, arXiv
   Pan XG, 2023, PROCEEDINGS OF SIGGRAPH 2023 CONFERENCE PAPERS, SIGGRAPH 2023, DOI 10.1145/3588432.3591500
   POOLE B., 2022, ARXIV
   Porter T., 1984, P SIGGRAPH, P253, DOI [DOI 10.1145/964965.808606, 10.1145/964965.808606]
   Ramesh A., 2022, Hierarchical text-conditional image generation with clip latents, DOI 10.48550/arXiv.2204.06125
   Ranftl R, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12159, DOI 10.1109/ICCV48922.2021.01196
   Ren ZZ, 2022, PROC CVPR IEEE, P6123, DOI 10.1109/CVPR52688.2022.00604
   Rombach R, 2022, PROC CVPR IEEE, P10674, DOI 10.1109/CVPR52688.2022.01042
   Song H, 2023, IEEE I CONF COMP VIS, P14337, DOI 10.1109/ICCV51070.2023.01323
   Sun C, 2022, PROC CVPR IEEE, P5449, DOI 10.1109/CVPR52688.2022.00538
   Tancik M, 2023, PROCEEDINGS OF SIGGRAPH 2023 CONFERENCE PAPERS, SIGGRAPH 2023, DOI 10.1145/3588432.3591516
   Tancik Matthew, 2022, NeRF Tutorial ECCV 2022
   Tsalicoglou C, 2023, Arxiv, DOI arXiv:2304.12439
   Tschernezki V, 2022, INT CONF 3D VISION, P443, DOI 10.1109/3DV57658.2022.00056
   Wang C, 2022, PROC CVPR IEEE, P3825, DOI 10.1109/CVPR52688.2022.00381
   Wang Can, 2022, arXiv
   Wang DQ, 2024, Arxiv, DOI arXiv:2305.15094
   Wang ZW, 2021, ACM COMPUT SURV, V54, DOI 10.1145/3439723
   Xu TH, 2022, LECT NOTES COMPUT SC, V13693, P159, DOI 10.1007/978-3-031-19827-4_10
   Yu A, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5732, DOI 10.1109/ICCV48922.2021.00570
   Zhang L, 2023, Arxiv, DOI [arXiv:2302.05543, 10.48550/ARXIV.2302.05543]
   Zhuang JY, 2023, Arxiv, DOI arXiv:2306.13455
NR 53
TC 0
Z9 0
U1 0
U2 0
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
EI 2577-6193
J9 P ACM COMPUT GRAPH
JI P. ACM Comput. Graph. Interact. Tech.
PD MAY
PY 2024
VL 7
IS 1
AR 22
DI 10.1145/3651290
PG 17
WC Computer Science, Software Engineering
WE Emerging Sources Citation Index (ESCI)
SC Computer Science
GA RQ5P8
UT WOS:001229142400022
OA Green Submitted, hybrid
DA 2024-08-05
ER

PT J
AU Medin, SC
   Li, G
   Du, RF
   Garbin, S
   Davidson, P
   Wornell, GW
   Beeler, T
   Meka, A
AF Medin, Safa C.
   Li, Gengyan
   Du, Ruofei
   Garbin, Stephan
   Davidson, Philip
   Wornell, Gregory W.
   Beeler, Thabo
   Meka, Abhimitra
TI FaceFolds: Meshed Radiance Manifolds for Efficient Volumetric Rendering
   of Dynamic Faces
SO PROCEEDINGS OF THE ACM ON COMPUTER GRAPHICS AND INTERACTIVE TECHNIQUES
LA English
DT Article
DE Volumetric Rendering; Face Modeling; Novel View Synthesis; Neural
   Radiance Fields; Performance Capture
AB 3D rendering of dynamic face captures is a challenging problem, and it demands improvements on several fronts-photorealism, efficiency, compatibility, and configurability. We present a novel representation that enables high-quality volumetric rendering of an actor's dynamic facial performances with minimal compute and memory footprint. It runs natively on commodity graphics soft- and hardware, and allows for a graceful trade-off between quality and efficiency. Our method utilizes recent advances in neural rendering, particularly learning discrete radiance manifolds to sparsely sample the scene to model volumetric effects. We achieve efficient modeling by learning a single set of manifolds for the entire dynamic sequence, while implicitly modeling appearance changes as temporal canonical texture. We export a single layered mesh and view-independent RGBA texture video that is compatible with legacy graphics renderers without additional ML integration. We demonstrate our method by rendering dynamic face captures of real actors in a game engine, at comparable photorealism to state-of-the-art neural rendering techniques at previously unseen frame rates.
C1 [Medin, Safa C.; Wornell, Gregory W.; Meka, Abhimitra] MIT, Cambridge, MA 02139 USA.
   [Medin, Safa C.; Du, Ruofei; Davidson, Philip; Meka, Abhimitra] Google, Mountain View, CA USA.
   [Li, Gengyan] Swiss Fed Inst Technol, Zurich, Switzerland.
   [Li, Gengyan; Beeler, Thabo] Google, Zurich, Switzerland.
   [Garbin, Stephan] Google, London, England.
C3 Massachusetts Institute of Technology (MIT); Google Incorporated; Swiss
   Federal Institutes of Technology Domain; ETH Zurich; Google
   Incorporated; Google Incorporated
RP Medin, SC (corresponding author), MIT, Cambridge, MA 02139 USA.; Medin, SC (corresponding author), Google, Mountain View, CA USA.
EM medin@mit.edu; gengyan.li@inf.ethz.ch; me@duruofei.com;
   stephangarbin@google.com; pdavidson@google.com; gww@mit.edu;
   tbeeler@google.com; abhim@google.com
RI Medin, Safa/KUC-8831-2024
OI Medin, Safa/0000-0001-7944-0305; Meka, Abhimitra/0000-0001-7906-4004;
   Beeler, Thabo/0000-0002-8077-1205; Wornell, Gregory/0000-0001-9166-4758
CR Atzmon M, 2020, PROC CVPR IEEE, P2562, DOI 10.1109/CVPR42600.2020.00264
   Bai ZQ, 2023, PROC CVPR IEEE, P16890, DOI 10.1109/CVPR52729.2023.01620
   Barron JT, 2023, IEEE I CONF COMP VIS, P19640, DOI 10.1109/ICCV51070.2023.01804
   Barron JT, 2022, PROC CVPR IEEE, P5460, DOI 10.1109/CVPR52688.2022.00539
   Bergman AW, 2022, Arxiv, DOI arXiv:2206.14314
   Blanz V, 1999, COMP GRAPH, P187, DOI 10.1145/311535.311556
   Blelloch Guy E, 1990, Prefix Sums and Their Applications, DOI [10.1109/ISTCS.1995.377028, DOI 10.1109/ISTCS.1995.377028]
   Bühler MC, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13870, DOI 10.1109/ICCV48922.2021.01363
   Bühler MC, 2023, IEEE I CONF COMP VIS, P3379, DOI 10.1109/ICCV51070.2023.00315
   Cao C, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530143
   Chen ZQ, 2023, PROC CVPR IEEE, P16569, DOI 10.1109/CVPR52729.2023.01590
   Cheng Z.-Q., 2022, arXiv, DOI [DOI 10.1109/CVPR52688.2022.01902, 10.1109/CVPR52688.2022.00298, DOI 10.1109/CVPR52688.2022.01565]
   Cheng-hsin Wuu, 2024, Proc. ACM Comput. Graph. Interact. Tech., V7
   Collet A, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766945
   Deering M., 1988, Computer Graphics, V22, P21, DOI 10.1145/378456.378468
   Deng Y, 2023, Arxiv, DOI [arXiv:2211.13901, DOI 10.1109/CVPR52729.2023.02017, 10.1109/CVPR52729.2023.02017]
   Deng Y, 2022, PROC CVPR IEEE, P10663, DOI 10.1109/CVPR52688.2022.01041
   Du R., 2019, Journal of Computer Graphics Techniques, V1, P1
   Du RF, 2018, ACM SIGGRAPH SYMPOSIUM ON INTERACTIVE 3D GRAPHICS AND GAMES (I3D 2018), DOI 10.1145/3190834.3190843
   Duan HB, 2023, ACM T GRAPHIC, V42, DOI 10.1145/3618399
   Duckworth D, 2024, Arxiv, DOI arXiv:2312.07541
   Egger B, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3395208
   Fridovich-Keil S, 2022, PROC CVPR IEEE, P5491, DOI 10.1109/CVPR52688.2022.00542
   Fyffe G, 2011, COMPUT GRAPH FORUM, V30, P425, DOI 10.1111/j.1467-8659.2011.01888.x
   Garbin SJ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14326, DOI 10.1109/ICCV48922.2021.01408
   Garbin Stephan J, 2022, arXiv
   Gotardo P, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275073
   Grassal PW, 2022, PROC CVPR IEEE, P18632, DOI 10.1109/CVPR52688.2022.01810
   Gropp A, 2020, Arxiv, DOI arXiv:2002.10099
   Guo KW, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356571
   Hedman P, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5855, DOI 10.1109/ICCV48922.2021.00582
   Kania K, 2023, PROC CVPR IEEE, P404, DOI 10.1109/CVPR52729.2023.00047
   Kazhdan Michael, 2006, P 4 EUROGRAPHICS S G, V7, DOI [10.1145/2487228.2487237x26amp, DOI 10.1145/2487228.2487237X26AMP]
   Kerbl B, 2023, ACM T GRAPHIC, V42, DOI 10.1145/3592433
   Khakhulin T, 2022, LECT NOTES COMPUT SC, V13662, P345, DOI 10.1007/978-3-031-20086-1_20
   Kim H, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201283
   Kingma D. P., 2014, arXiv
   Kirschstein T, 2023, ACM T GRAPHIC, V42, DOI 10.1145/3592455
   Li GY, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530130
   Li TY, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130813
   Li ZS, 2023, PROC CVPR IEEE, P8456, DOI 10.1109/CVPR52729.2023.00817
   Liu L., 2020, Advances in Neural Information Processing Systems, V33, P15651
   Lombardi S, 2021, ACM T GRAPHIC, V40, DOI [10.1145/3476576.3476608, 10.1145/3450626.3459863]
   Lombardi S, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3323020
   Medin SC, 2022, AAAI CONF ARTIF INTE, P1962
   Mildenhall B, 2022, COMMUN ACM, V65, P99, DOI 10.1145/3503250
   Müller T, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530127
   Niemeyer M, 2020, PROC CVPR IEEE, P3501, DOI 10.1109/CVPR42600.2020.00356
   Orts-Escolano S, 2016, UIST 2016: PROCEEDINGS OF THE 29TH ANNUAL SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P741, DOI 10.1145/2984511.2984517
   Park K, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5845, DOI 10.1109/ICCV48922.2021.00581
   Park K, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3478513.3480487
   Qian SH, 2024, Arxiv, DOI arXiv:2312.02069
   Reiser C, 2023, ACM T GRAPHIC, V42, DOI 10.1145/3592426
   Saito S, 2024, Arxiv, DOI arXiv:2312.03704
   Sun JX, 2023, PROC CVPR IEEE, P20991, DOI 10.1109/CVPR52729.2023.02011
   Tancik M., 2020, ADV NEURAL INFORM PR, V33, P7537, DOI DOI 10.48550/ARXIV.2006.10739
   Wang P, 2021, Arxiv, DOI arXiv:2106.10689
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wen X, 2020, IEEE T VIS COMPUT GR, V26, P3457, DOI 10.1109/TVCG.2020.3023573
   Wu Yue, 2022, arXiv
   Wuu CH, 2023, Arxiv, DOI arXiv:2207.11243
   Xiang JF, 2023, IEEE I CONF COMP VIS, P2195, DOI 10.1109/ICCV51070.2023.00209
   Yuan YJ, 2022, Arxiv, DOI [arXiv:2205.04978, DOI 10.48550/ARXIV.2205.04978.2,3]
   Zakharov Egor, 2020, EUR C COMP VIS, DOI [10.1007/978-3-030-58610-31, DOI 10.1007/978-3-030-58610-31]
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zheng YF, 2023, PROC CVPR IEEE, P21057, DOI 10.1109/CVPR52729.2023.02017
   Zheng YF, 2022, PROC CVPR IEEE, P13535, DOI 10.1109/CVPR52688.2022.01318
   Zhou TH, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201323
   Zielonka W, 2023, PROC CVPR IEEE, P4574, DOI 10.1109/CVPR52729.2023.00444
NR 69
TC 0
Z9 0
U1 0
U2 0
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
EI 2577-6193
J9 P ACM COMPUT GRAPH
JI P. ACM Comput. Graph. Interact. Tech.
PD MAY
PY 2024
VL 7
IS 1
AR 23
DI 10.1145/3651304
PG 17
WC Computer Science, Software Engineering
WE Emerging Sources Citation Index (ESCI)
SC Computer Science
GA RQ5P8
UT WOS:001229142400023
OA Green Submitted, hybrid
DA 2024-08-05
ER

PT J
AU Dolp, R
   Hanika, J
   Dachsbacher, C
AF Dolp, Reiner
   Hanika, Johannes
   Dachsbacher, Carsten
TI A Fast GPU Schedule For A-TrousWavelet-Based Denoisers
SO PROCEEDINGS OF THE ACM ON COMPUTER GRAPHICS AND INTERACTIVE TECHNIQUES
LA English
DT Article
DE real-time rendering; denoising; GPU; scheduling; performance;
   optimization
AB Given limitations of contemporary graphics hardware, real-time ray-traced global illumination is only estimated using a few samples per pixel. This consequently causes stochastic noise in the resulting frame sequences which requires wide filter support during denoising for temporally stable estimates. The edge avoiding a-trous wavelet transform amortizes runtime cost by hierarchical filtering using a constant number of increasingly dilated taps in each iteration. While the number of taps stays constant, the runtime of each iteration increases in these usually memory-throughput bound shaders with increasing dilation, because the increasing non-locality negatively impacts cache hit rates. We present a scheduling approach that optimizes usage of the memory subsystem by permutating global invocation indices in such a way that each wavelet filter iteration is applied through undilated taps. In contrast to prior approaches, our method has identical performance characteristics in each iteration, effectively decreasing maintenance cost and improving performance predictability. Furthermore, we are able to leverage on-chip memory and hardware texture interpolation. Our permutation strategy is trivial to integrate into existing wavelet filters as a permutation before and after each level of the wavelet filter. We achieve speedups between 1.3 and 3.8 for usual wavelet configurations in Monte Carlo denoising and computational photography.
C1 [Dolp, Reiner; Hanika, Johannes; Dachsbacher, Carsten] Karlsruhe Inst Technol, Fasanengarten 5, D-76131 Karlsruhe, Germany.
C3 Helmholtz Association; Karlsruhe Institute of Technology
RP Dolp, R (corresponding author), Karlsruhe Inst Technol, Fasanengarten 5, D-76131 Karlsruhe, Germany.
EM reiner.dolp@kit.edu; hanika@kit.edu; dachs-bacher@kit.edu
OI Hanika, Johannes/0000-0002-7648-1782
FU Helmholtz Association (HGF)
FX This work has been supported by the Helmholtz Association (HGF) under
   the joint research school "HIDSS4Health - Helmholtz Information and Data
   Science School for Health" and through the Pilot Program Core
   Informatics.
CR Alber Lucas, 2024, Master Thesis
   Bako S, 2017, ACM T GRAPHIC, V36, DOI [10.1145/3072959.3073703, 10.1145/3072959.3073708]
   Bekkers Jasper, 2019, Hybrid Rendering for Real-Time Ray Tracing, P437, DOI [10.1007/978-1-4842-4427-225, DOI 10.1007/978-1-4842-4427-225]
   Boksansky Jakub., 2019, Ray Traced Shadows: Maintaining Real-Time Frame Rates, P159, DOI [DOI 10.1007/978-1-4842-4427-213, DOI 10.1007/978-1-4842-4427-2]
   Bondhugula U, 2017, IEEE T PARALL DISTR, V28, P1285, DOI 10.1109/TPDS.2016.2615094
   Chaitanya CRA, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073601
   Fattal R, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1477926.1477933
   GROSSER T., 2013, Proceedings of the 6th Workshop on General Purpose Processor Using Graphics Processing Units, P24
   Grosser Tobias, 2011, P 1 INT WORKSH POL C, V2011, P1
   Grosser Tobias, 2014, P ANN IEEE ACM INT S, DOI [10.1145/2544137.2544160, DOI 10.1145/2544137.2544160]
   Hanika J, 2011, COMPUT GRAPH FORUM, V30, P1879, DOI 10.1111/j.1467-8659.2011.02054.x
   Hasselgren J, 2020, COMPUT GRAPH FORUM, V39, P147, DOI 10.1111/cgf.13919
   Hofmann N, 2023, P ACM COMPUT GRAPH, V6, DOI 10.1145/3585497
   Holger D., 2010, P C HIGH PERF GRAPH, P67
   Kalantari NK, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766977
   Kawase Masaki, 2003, Frame Buffer Postprocessing Effects in DOUBLE-S.T.E.A.L (Wreckless)
   Kelly Patrick, 2021, Ray Tracing Gems II: Next Generation Real-Time Rendering with DXR, Vulkan, and OptiX, P791, DOI [10.1007/978-1-4842-7185-848, DOI 10.1007/978-1-4842-7185-848]
   Muchnick Steven S., 1997, Advanced compiler design implementation, P279
   Nvidia Inc, 2023, NVIDIA Real-Time Denoisers
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Schied C, 2018, P ACM COMPUT GRAPH, V1, DOI 10.1145/3233301
   Schied C, 2017, HPG '17: PROCEEDINGS OF HIGH PERFORMANCE GRAPHICS, DOI 10.1145/3105762.3105770
   Sen Pradeep, 2015, ACM SIGGRAPH COURSES, V7, DOI [10.1145/2776880.2792740, DOI 10.1145/2776880.2792740]
   Thomas MM, 2022, P ACM COMPUT GRAPH, V5, DOI 10.1145/3543870
   Thomas MM, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417786
   Vasilache N, 2018, Arxiv, DOI arXiv:1802.04730
   Vogels T, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201388
   Willberger Thomas, 2019, Ray Tracing Gems: High-Quality and Real-Time Rendering with DXR and Other APIs, P475, DOI [10.1007/978-1-4842-4427-226, DOI 10.1007/978-1-4842-4427-226]
   Xu B, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356547
   Zhdan Dmitry, 2021, Ray Tracing Gems II: Next Generation RealTime Rendering with DXR, Vulkan, and OptiX, P823, DOI [10.1007/978-1-4842-7185-8, DOI 10.1007/978-1-4842-7185-8]
   Zwicker M, 2015, COMPUT GRAPH FORUM, V34, P667, DOI 10.1111/cgf.12592
NR 31
TC 0
Z9 0
U1 0
U2 0
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
EI 2577-6193
J9 P ACM COMPUT GRAPH
JI P. ACM Comput. Graph. Interact. Tech.
PD MAY
PY 2024
VL 7
IS 1
AR 15
DI 10.1145/3651299
PG 18
WC Computer Science, Software Engineering
WE Emerging Sources Citation Index (ESCI)
SC Computer Science
GA RQ5P8
UT WOS:001229142400015
OA hybrid
DA 2024-08-05
ER

PT J
AU Roughton, T
   Sloan, PP
   Silvennoinen, A
   Iwanicki, M
   Shirley, P
AF Roughton, Thomas
   Sloan, Peter-Pike
   Silvennoinen, Ari
   Iwanicki, Michal
   Shirley, Peter
TI ZH3: Quadratic Zonal Harmonics
SO PROCEEDINGS OF THE ACM ON COMPUTER GRAPHICS AND INTERACTIVE TECHNIQUES
LA English
DT Article
DE spherical harmonics; global illumination
AB Spherical Harmonics (SH) have been used widely to represent lighting in games and film. While the quadratic (SH3) and higher order spherical harmonics represent irradiance well, they are expensive to store and evaluate, requiring 27 coefficients per sample. Linear SH (SH2), requiring only 12 coefficients, are sometimes used, but they do not represent irradiance signals accurately and can have challenges with negative reconstruction. We introduce a new representation (ZH3) that augments linear SH with just the zonal coefficient of quadratic SH, yielding significant visual improvement with just 15 coefficients, and discuss how solving for a luminance zonal axis can significantly improve reconstruction accuracy and reduce color artifacts. We also discuss how, rather than storing the ZH3 coefficients explicitly, we can hallucinate them from the linear SH, improving reconstruction accuracy over linear SH at minimal extra cost.
C1 [Roughton, Thomas] Activis Publishing, Auckland, New Zealand.
   [Sloan, Peter-Pike; Silvennoinen, Ari; Iwanicki, Michal; Shirley, Peter] Activis Publishing, Santa Monica, CA USA.
RP Roughton, T (corresponding author), Activis Publishing, Auckland, New Zealand.
EM thomas.roughton@activision.com; ppsloan@activision.com;
   ari.silvennoinen@activision.com; michal.iwanicki@activision.com;
   peter.shirley@activision.com
OI Iwanicki, Michal/0009-0004-2418-7401
CR Advanced Micro Devices Inc., 2023, "RDNA3" Instruction Set Architecture Reference Guide
   Debevec Paul, 2001, P SIGGRAPH, V98
   Dubouchet Adrien, 2019, EUROGRAPHICS S RENDE
   Green Paul, 2006, P 2006 S INTERACTIVE, P7
   Iwanicki M., 2013, ACM SIGGRAPH 2013 Talks
   Iwanicki M., 2017, Eurographics Symposium on Rendering - Experimental Ideas Implementations
   Joseph William, 2015, CEDEC
   McTaggart Gary, 2004, GAM DEV C
   Neubelt David, 2015, SIGGRAPH 2015 COURSE
   Ng R, 2003, ACM T GRAPHIC, V22, P376, DOI 10.1145/882262.882280
   Nowrouzezahrai D, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2167076.2167081
   Ramamoorthi R, 2001, COMP GRAPH, P497, DOI 10.1145/383259.383317
   Sloan PP, 2017, IGGRAPH ASIA 2017 TECHNICAL BRIEFS (SA'17), DOI 10.1145/3145749.3149438
   Sloan PP, 2018, SA'18: SIGGRAPH ASIA 2018 TECHNICAL BRIEFS, DOI 10.1145/3283254.3283281
   Sloan Peter-Pike, 2013, Journal of Computer Graphics Techniques (JCGT), V2
   Sloan Peter-Pike, 2020, SIGGRAPH COURSE ADV
   Sloan Peter-Pike, 2008, GAM DEV C
   Sloan PP, 2005, ACM T GRAPHIC, V24, P1216, DOI 10.1145/1073204.1073335
   Snyder John, 1996, Technical Report MSR-TR-96-11
   Soler C, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2797136
   Tsai YT, 2006, ACM T GRAPHIC, V25, P967, DOI 10.1145/1141911.1141981
   Vogl Bernhard, 2010, Light probes
   Wiederien Tyler, 2022, ACM SIGGRAPH S INT 3
   WYMAN DR, 1989, J COMPUT PHYS, V81, P137, DOI 10.1016/0021-9991(89)90067-3
   Yuan Hong, 2012, Journal of Graphics Tools, V16, P1
   Zhao S, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601104
NR 26
TC 0
Z9 0
U1 0
U2 0
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
EI 2577-6193
J9 P ACM COMPUT GRAPH
JI P. ACM Comput. Graph. Interact. Tech.
PD MAY
PY 2024
VL 7
IS 1
AR 11
DI 10.1145/3651294
PG 15
WC Computer Science, Software Engineering
WE Emerging Sources Citation Index (ESCI)
SC Computer Science
GA RQ5P8
UT WOS:001229142400011
DA 2024-08-05
ER

PT J
AU Chen, BY
   Yan, XN
   Hu, XN
   Kao, D
   Liang, HN
AF Chen, Boyuan
   Yan, Xinan
   Hu, Xuning
   Kao, Dominic
   Liang, Hai-Ning
TI Impact of Tutorial Modes with Different Time Flow Rates in Virtual
   Reality Games
SO PROCEEDINGS OF THE ACM ON COMPUTER GRAPHICS AND INTERACTIVE TECHNIQUES
LA English
DT Article
DE Virtual Reality; Games; Game Tutorial; User Study
AB Virtual reality's (VR) unique affordances compared to traditional media have produced innovative interaction modes and tutorial methodologies in VR games. Existing research predominantly focuses on the performance of VR tutorial modes, such as the placement of text and diagrams within tutorial content. However, few studies have delved into other attributes of tutorials. This study categorizes 4 VR game tutorial modes based on time flow: (1) traditional instruction screen, (2) slow motion, (3) bullet time, and (4) context-sensitive mode. This paper evaluates the impact of these 4 VR game tutorial modes with varying time flow rates on controls learnability, engagement-related outcomes, and player performance. We conducted a between-subjects experiment with 59 participants. Results indicated that bullet time significantly enhanced controls learnability, reduced cognitive load, and improved player performance when compared to other tutorial modes. Our research contributes to a more comprehensive understanding of VR game tutorials and offers practical guidance for game designers, underscoring the potential of bullet time to enhance learnability and game experience.
C1 [Chen, Boyuan; Yan, Xinan; Hu, Xuning; Liang, Hai-Ning] Xian Jiaotong Liverpool Univ, Suzhou, Jiangsu, Peoples R China.
   [Kao, Dominic] Purdue Univ, W Lafayette, IN 47907 USA.
C3 Xi'an Jiaotong-Liverpool University; Purdue University System; Purdue
   University
RP Liang, HN (corresponding author), Xian Jiaotong Liverpool Univ, Suzhou, Jiangsu, Peoples R China.
EM Boyuan.Chen22@student.xjtlu.edu.cn; xinan.yan22@student.xjtlu.edu.cn;
   xuning.hu22@student.xjtlu.edu.cn; kaod@purdue.edu;
   haining.liang@xjtlu.edu.cn
OI Liang, Hai-Ning/0000-0003-3600-8955
FU Suzhou Municipal Key Laboratory for Intelligent Virtual Engineering
   [SZS2022004]; National Natural Science Foundation of China [62272396]
FX The authors thank the participants who joined the user studies and the
   reviewers whose insightful comments and suggestions helped improve our
   paper. This work was partially supported by the Suzhou Municipal Key
   Laboratory for Intelligent Virtual Engineering (#SZS2022004) and the
   National Natural Science Foundation of China (#62272396).
CR Alsop Thomas, 2021, Forecast augmented (AR) and virtual reality (VR) market size worldwide from 2020 to 2024
   Andersen E., 2012, P 2012 ACM ANN C HUM, P59, DOI [DOI 10.1145/2207676.2207687, 10.1145/2207676.2207687]
   [Anonymous], 2010, A Casual Revolution
   Baghaei N, 2021, JMIR MENT HEALTH, V8, DOI 10.2196/29681
   Barreteau O, 2007, SIMULAT GAMING, V38, P364, DOI 10.1177/1046878107300668
   Berg LP, 2017, VIRTUAL REAL-LONDON, V21, P1, DOI 10.1007/s10055-016-0293-9
   Bertram J, 2015, COMPUT HUM BEHAV, V43, P284, DOI 10.1016/j.chb.2014.10.032
   Bogon Johanna, 2023, Time and Timing in Video Games: How Video Game and Time Perception Research can benefit each other
   Bordwell David, 2008, Film art: An introduction, V8
   Brown Douglas, 2009, Movie-games and game-movies: Towards an aesthetics of transmediality
   Clark R C., 2023, E-learning and the science of instruction: Proven guidelines for consumers and designers of multimedia learning
   Cubitt Sean., 2005, The Cinema Effect
   Dillman KR, 2018, PROCEEDINGS OF THE 2018 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2018), DOI 10.1145/3173574.3173714
   Frommel J, 2017, CHI PLAY'17: PROCEEDINGS OF THE ANNUAL SYMPOSIUM ON COMPUTER-HUMAN INTERACTION IN PLAY, P367, DOI 10.1145/3116595.3116610
   Guo ZX, 2023, INT J HUM-COMPUT INT, DOI 10.1080/10447318.2023.2241286
   Hart SG., 2006, P HUM FACT ERG SOC A, V50, P904, DOI [10.1177/154193120605000909, DOI 10.1177/154193120605000909]
   Hu Xinyu, 2020, P 2020 ACM S SPAT IN, P1
   Kao Dominic, 2021, Proceedings of the ACM on Human-Computer Interaction, V5, DOI 10.1145/3474661
   Kennedy R.S., 1993, Int. J. Aviat. Psy, P203
   Lawson G, 2016, APPL ERGON, V53, P323, DOI 10.1016/j.apergo.2015.06.024
   Lee H, 2019, INT SYM MIX AUGMENT, P318, DOI 10.1109/ISMAR.2019.00030
   Levin Iris., 1989, TIME HUMAN COGNITION
   Li ZM, 2023, VIRTUAL REAL-LONDON, V27, P2927, DOI 10.1007/s10055-023-00847-3
   Lin LJ, 2011, COMPUT EDUC, V56, P650, DOI 10.1016/j.compedu.2010.10.007
   Lu FY, 2018, INT SYM MIX AUGMENT, P143, DOI 10.1109/ISMAR.2018.00050
   Mayer R. E., 2005, The Cambridge handbook of multimedia learning, DOI DOI 10.1017/CBO9780511816819
   MCAULEY E, 1989, RES Q EXERCISE SPORT, V60, P48, DOI 10.1080/02701367.1989.10607413
   Monteiro D, 2024, UNIVERSAL ACCESS INF, V23, P23, DOI 10.1007/s10209-023-00985-0
   Monteiro D, 2022, UNIVERSAL ACCESS INF, DOI 10.1007/s10209-022-00947-y
   Nitsche Michael, 2007, DIGRA C
   Nuyens FM, 2020, INT J MENT HEALTH AD, V18, P1226, DOI 10.1007/s11469-019-00121-1
   Rose T, 2018, APPL ERGON, V69, P153, DOI 10.1016/j.apergo.2018.01.009
   Ryan RM, 2006, MOTIV EMOTION, V30, P347, DOI 10.1007/s11031-006-9051-8
   Rzayev R, 2019, CHI PLAY'19: PROCEEDINGS OF THE ANNUAL SYMPOSIUM ON COMPUTER-HUMAN INTERACTION IN PLAY, P199, DOI 10.1145/3311350.3347190
   Schott Gareth, 2013, DIGRA C
   SWELLER J, 1988, COGNITIVE SCI, V12, P257, DOI 10.1207/s15516709cog1202_4
   Tan C.T., 2015, P CHI PLAY 15, P253, DOI DOI 10.1145/2793107.2793117
   Thavikulwat P., 1996, Simulation & Gaming, V27, P110, DOI 10.1177/1046878196271008
   Vibeto HA, 2023, GAMES CULT, V18, P283, DOI 10.1177/15554120221090974
   Wachowski L., 1999, The Matrix
   Wang JL, 2022, P ACM COMPUT GRAPH, V5, DOI 10.1145/3522610
   Wang L, 2023, IEEE T VIS COMPUT GR, V29, P2184, DOI 10.1109/TVCG.2022.3142198
   Wauck H, 2017, IUI'17: PROCEEDINGS OF THE 22ND INTERNATIONAL CONFERENCE ON INTELLIGENT USER INTERFACES, P137, DOI 10.1145/3025171.3025224
   Witmer BG, 1998, PRESENCE-TELEOP VIRT, V7, P225, DOI 10.1162/105474698565686
   Xu W., 2021, P 2021 CHI C HUM FAC, DOI [10.1080/14681366.2021.1881995, DOI 10.1145/3411764.3445801]
   Xu WG, 2023, INT J HUM-COMPUT INT, V39, P1134, DOI 10.1080/10447318.2022.2098559
   Xu WG, 2021, JMIR SERIOUS GAMES, V9, DOI 10.2196/29330
   Yu KY, 2023, VIRTUAL REAL-LONDON, V27, P1887, DOI 10.1007/s10055-023-00780-5
NR 48
TC 0
Z9 0
U1 0
U2 0
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
EI 2577-6193
J9 P ACM COMPUT GRAPH
JI P. ACM Comput. Graph. Interact. Tech.
PD MAY
PY 2024
VL 7
IS 1
AR 6
DI 10.1145/3651296
PG 19
WC Computer Science, Software Engineering
WE Emerging Sources Citation Index (ESCI)
SC Computer Science
GA RQ5P8
UT WOS:001229142400006
DA 2024-08-05
ER

PT J
AU Donnelly, W
   Wolfe, A
   Butepage, J
   Valdes, J
AF Donnelly, William
   Wolfe, Alan
   Butepage, Judith
   Valdes, Jon
TI FAST: Filter-Adapted Spatio-Temporal Sampling for Real-Time Rendering
SO PROCEEDINGS OF THE ACM ON COMPUTER GRAPHICS AND INTERACTIVE TECHNIQUES
LA English
DT Article
DE rendering; noise; sampling
AB Stochastic sampling techniques are ubiquitous in real-time rendering, where performance constraints force the use of low sample counts, leading to noisy intermediate results. To remove this noise, the post-processing step of temporal and spatial denoising is an integral part of the real-time graphics pipeline. The main insight presented in this paper is that we can optimize the samples used in stochastic sampling such that the post-processing error is minimized. The core of our method is an analytical loss function which measures post-filtering error for a class of integrands - multidimensional Heaviside functions. These integrands are an approximation of the discontinuous functions commonly found in rendering. Our analysis applies to arbitrary spatial and spatiotemporal filters, scalar and vector sample values, and uniform and non-uniform probability distributions. We show that the spectrum of Monte Carlo noise resulting from our sampling method is adapted to the shape of the filter, resulting in less noisy final images. We demonstrate improvements over state-of-the-art sampling methods in three representative rendering tasks: ambient occlusion, volumetric ray-marching, and color image dithering. Common use noise textures, and noise generation code is available at https://github.com/electronicarts/fastnoise.
C1 [Donnelly, William] SEED Elect Arts, Burnaby, BC, Canada.
   [Wolfe, Alan] SEED Elect Arts, Austin, TX USA.
   [Butepage, Judith] SEED Elect Arts, Stockholm, Sweden.
   [Valdes, Jon] Frostbite Elect Arts, Stockholm, Sweden.
RP Donnelly, W (corresponding author), SEED Elect Arts, Burnaby, BC, Canada.
EM wdonnelly@ea.com; awolfe@ea.com; jbutepage@ea.com; jvaldes@frostbite.com
CR Ahmed AGM, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3550454.3555519
   Ahmed AGM, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417881
   [Anonymous], 1987, P 14 ANN C COMP GRAP, DOI [10.1145/37401.37410, DOI 10.1145/37401.37410, DOI 10.1145/37402.37410]
   Arnold D.N., 2018, arXiv, DOI [DOI 10.1137/1.9781611975543,ARXIV:HTTPS://EPUBS.SIAM.ORG/DOI/PDF/10.1137/1.9781611975543, DOI 10.1137/1.9781611974546, 10.1137/1.9781611971484, DOI 10.1137/1.9781611971484, 10.1137/1.9781611974546]
   Bauer D, 2023, IEEE T VIS COMPUT GR, V29, P515, DOI 10.1109/TVCG.2022.3209498
   Belcour Laurent, 2021, ACM SIGGRAPH 2021 TA, DOI DOI 10.1145/3450623.3464645
   Bryce E. Bayer, 1973, IEEE INT C COMM, V26, P11
   Christou Cameron N., 2008, MSc thesis
   Georgiev Iliyan, 2016, ACM SIGGRAPH 2016 TA, DOI DOI 10.1145/2897839.2927430
   Gjoel Mikkel, 2016, GAM DEV C
   Heitz E, 2019, COMPUT GRAPH FORUM, V38, P149, DOI 10.1111/cgf.13778
   Heitz E, 2019, SIGGRAPH '19 -ACM SIGGRAPH 2019 TALKS, DOI 10.1145/3306307.3328191
   Jimenez Jorge, 2014, SIGGRAPH Advances in RealTime Rendering in Games
   Karis Brian, 2014, SIGGRAPH Advances in RealTime Rendering in Games
   Keller Alexander, 2019, SIGGRAPH Courses
   Korac Misa, 2023, SIGGRAPH AS 2023 C P, DOI [10.1145/3610548.3618146, DOI 10.1145/3610548.3618146]
   Panteleev Alexey, 2019, GAM DEV C
   Peters Christoph, 2017, The problem with 3D blue noise
   Ramamoorthi R, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2231816.2231819
   Reed Nathan, 2013, Quick And Easy GPU Random Numbers In D3D11
   Roberts M., 2018, The Unreasonable Effectiveness of Quasirandom Sequences
   Salaün C, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3550454.3555484
   Schierup CU, 2017, STUD CRIT SOC SCI, V97, P1, DOI 10.1163/9789004329706_002
   Spitzer John, 2021, GAM DEV C
   ULICHNEY R, 1993, P SOC PHOTO-OPT INS, V1913, P332, DOI 10.1117/12.152707
   ULICHNEY RA, 1988, P IEEE, V76, P56, DOI 10.1109/5.3288
   Ulichney Robert, 1987, Digital Halftoning, DOI DOI 10.7551/MITPRESS/2421.001.0001
   Wolfe Alan, 2022, EUROGRAPHICS S RENDE, P117, DOI DOI 10.2312/SR.20221161
   Wronski Bart, 2020, Optimizing"blue noise dithering-backpropagation through Fourier transform and sorting
   Yang L, 2020, COMPUT GRAPH FORUM, V39, P607, DOI 10.1111/cgf.14018
   Yang L, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618481
NR 31
TC 0
Z9 0
U1 0
U2 0
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
EI 2577-6193
J9 P ACM COMPUT GRAPH
JI P. ACM Comput. Graph. Interact. Tech.
PD MAY
PY 2024
VL 7
IS 1
AR 13
DI 10.1145/3651283
PG 16
WC Computer Science, Software Engineering
WE Emerging Sources Citation Index (ESCI)
SC Computer Science
GA RQ5P8
UT WOS:001229142400013
DA 2024-08-05
ER

PT J
AU Melnyk, K
   Friedman, L
   Katrychuk, D
   Komogortsev, O
AF Melnyk, Kateryna
   Friedman, Lee
   Katrychuk, Dmytro
   Komogortsev, Oleg
TI Per-Subject Oculomotor Plant Mathematical Models and the Reliability of
   Their Parameters
SO PROCEEDINGS OF THE ACM ON COMPUTER GRAPHICS AND INTERACTIVE TECHNIQUES
LA English
DT Article
DE Eye movements; Oculomotor Plant Mathematical Model; Saccade simulation;
   Temporal persistence
AB The practical value of oculomotor plant mathematical models (OPMMs) has been demonstrated across various domains, including biometrics and eye movement prediction. To further enhance their utilization, new optimization approaches are commonly developed and introduced within the research community. In this study, we demonstrate a new integration of a previously developed per-subject optimization procedure for an Enderle OPMM and introduce methods to evaluate the reliability of OPMM parameters using the intraclass correlation coefficient (ICC) and Kendall's Coefficient of Concordance (KCC). We evaluated two per-subject OPMM models, Bahill and Enderle, using the 'GazeBase' eye movement dataset. The models differed in accuracy, expressed as the per-sample error in degrees of visual angle, based on the differences between the actual and predicted saccade trajectories. We found that some of the parameter estimates for both models were quite unreliable. We discuss the importance of addressing low-reliability issues and suggest methods to modify the models to enhance reliability and performance.
C1 [Melnyk, Kateryna; Friedman, Lee; Katrychuk, Dmytro; Komogortsev, Oleg] Texas State Univ, San Marcos, TX 78666 USA.
C3 Texas State University System; Texas State University San Marcos
RP Melnyk, K (corresponding author), Texas State Univ, San Marcos, TX 78666 USA.
CR BAHILL A T, 1975, Mathematical Biosciences, V27, P287, DOI 10.1016/0025-5564(75)90107-8
   BAHILL AT, 1980, CRC CR REV BIOM ENG, V4, P311
   BAHILL AT, 1977, ARCH OPHTHALMOL-CHIC, V95, P1258, DOI 10.1001/archopht.1977.04450070156016
   Cicchetti DV, 1994, Psychol Assess, V6, P284, DOI [10.1037/1040-3590.6.4.284, DOI 10.1037/1040-3590.6.4.284]
   COOK G, 1968, ARCH OPHTHALMOL-CHIC, V79, P428, DOI 10.1001/archopht.1968.03850040430012
   Enderle John, 2010, Models of Horizontal Eye Movements, Part I: Early Models of Saccades and Smooth Pursuit, V5, DOI [10.2200/S00263ED1V01Y201003BME034, DOI 10.2200/S00263ED1V01Y201003BME034]
   Enderle John, 2010, Models of Horizontal Eye Movements, Part II: A 3rd Order Linear Saccade Model, V5, DOI [10.2200/S00264ED1V01Y201003BME035, DOI 10.2200/S00264ED1V01Y201003BME035]
   Fenn WO, 1935, J PHYSIOL-LONDON, V85, P277, DOI 10.1113/jphysiol.1935.sp003318
   Friedman L, 2018, BEHAV RES METHODS, V50, P1374, DOI 10.3758/s13428-018-1050-7
   Friedman L, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0178501
   Ghahari Alireza, 2014, Models of Horizontal Eye Movements: Part 3, A Neuron and Muscle Based Linear Saccade Model, V9, DOI [10.2200/S00592ED1V01Y201408BME053, DOI 10.2200/S00592ED1V01Y201408BME053]
   Ghahari Alireza, 2015, Models of Horizontal Eye Movements: Part 4, A Multiscale Neuron and Muscle Fiber-Based Linear Saccade Model, V9, DOI [10.2200/S00627ED1V01Y201501BME055, DOI 10.2200/S00627ED1V01Y201501BME055]
   Griffith H, 2021, SCI DATA, V8, DOI 10.1038/s41597-021-00959-y
   Guenter B, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366183
   Hoerantner R, 2007, INVEST OPHTH VIS SCI, V48, P1133, DOI 10.1167/iovs.06-0769
   HSU FK, 1976, COMPUT PROG BIOMED, V6, P108, DOI 10.1016/0010-468X(76)90032-5
   Karpov A, 2020, Arxiv, DOI arXiv:2007.09884
   Katrychuk D, 2022, 2022 ACM SYMPOSIUM ON EYE TRACKING RESEARCH AND APPLICATIONS, ETRA 2022, DOI 10.1145/3517031.3532523
   Komogortsev O. V., 2012, 2012 5th IAPR International Conference on Biometrics (ICB), P413, DOI 10.1109/ICB.2012.6199786
   Komogortsev O.V., 2010, P 2010 S EYE TRACK R, P57, DOI DOI 10.1145/1743666.1743679
   Komogortsev O, 2013, ACM T APPL PERCEPT, V10, DOI 10.1145/2536764.2536774
   Komogortsev Oleg, 2012, Two-Dimensional Linear Homeomorphic Oculomotor Plant Mathematical Model, DOI [10.13140/RG.2.1.2711.8486, DOI 10.13140/RG.2.1.2711.8486]
   Komogortsev Oleg, 2014, CHI 14 EXTENDED ABST, P1711, DOI [10.1145/2559206.2581150, DOI 10.1145/2559206.2581150]
   Komogortsev OV, 2008, PROCEEDINGS OF THE EYE TRACKING RESEARCH AND APPLICATIONS SYMPOSIUM (ETRA 2008), P229, DOI 10.1145/1344471.1344525
   Komogortsev Oleg V, 2017, US Patent, Patent No. [9,811,730, 9811730]
   LANDIS JR, 1977, BIOMETRICS, V33, P159, DOI 10.2307/2529310
   Levin A, 1927, P R SOC LOND B-CONTA, V101, P218, DOI 10.1098/rspb.1927.0014
   NELDER JA, 1965, COMPUT J, V7, P308, DOI 10.1093/comjnl/7.4.308
   Patney A, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980246
   ROBINSON DA, 1964, J PHYSIOL-LONDON, V174, P245, DOI 10.1113/jphysiol.1964.sp007485
   ROBINSON DA, 1969, J APPL PHYSIOL, V26, P548, DOI 10.1152/jappl.1969.26.5.548
   Wadehn F, 2020, IEEE T BIO-MED ENG, V67, P588, DOI 10.1109/TBME.2019.2918986
   Wadehn F, 2018, IEEE ENG MED BIO, P2619, DOI 10.1109/EMBC.2018.8512758
   Wadehn Federico, 2019, State space methods with applications in biomedical signal processing, V31
   WESTHEIMER G, 1954, AMA ARCH OPHTHALMOL, V52, P710
NR 35
TC 0
Z9 0
U1 0
U2 0
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
EI 2577-6193
J9 P ACM COMPUT GRAPH
JI P. ACM Comput. Graph. Interact. Tech.
PD MAY
PY 2024
VL 7
IS 2
AR 1
DI 10.1145/3654701
PG 20
WC Computer Science, Software Engineering
WE Emerging Sources Citation Index (ESCI)
SC Computer Science
GA SL0I6
UT WOS:001234486100002
OA Bronze
DA 2024-08-05
ER

PT J
AU Tokuyoshi, Y
   Eto, K
AF Tokuyoshi, Yusuke
   Eto, Kenta
TI Bounded VNDF Sampling for the Smith-GGX BRDF
SO PROCEEDINGS OF THE ACM ON COMPUTER GRAPHICS AND INTERACTIVE TECHNIQUES
LA English
DT Article
DE importance sampling; microfacet BRDF; visible normal distribution
AB Sampling according to a visible normal distribution function (VNDF) is often used to sample rays scattered by glossy surfaces, such as the Smith-GGX microfacet model. However, for rough reflections, existing VNDF sampling methods can generate undesirable reflection vectors occluded by the surface. Since these occluded reflection vectors must be rejected, VNDF sampling is inefficient for rough reflections. This paper introduces an unbiased method to reduce the number of rejected samples for Smith-GGX VNDF sampling. Our method limits the sampling range for a state-of-the-art VNDF sampling method that uses a spherical cap-based sampling range. By using our method, we can reduce the variance for highly rough and low-anisotropy surfaces. Since our method only modifies the spherical cap range in the existing sampling routine, it is simple and easy to implement.
C1 [Tokuyoshi, Yusuke; Eto, Kenta] Adv Micro Devices Inc, Marunouchi Trust Tower Main Bldg 10F, Tokyo, Japan.
C3 Advanced Micro Devices
RP Tokuyoshi, Y (corresponding author), Adv Micro Devices Inc, Marunouchi Trust Tower Main Bldg 10F, Tokyo, Japan.
EM yusuke.tokuyoshi@amd.com; Kenta.Eto@amd.com
OI Eto, Kenta/0009-0001-1325-0645
CR AMD, 2023, HIP Ray Tracing
   Andersson Zap, 2023, OpenPBR Surface specification
   Atanasov A, 2022, COMPUT GRAPH FORUM, V41, P105, DOI 10.1111/cgf.14590
   Beckmann Petr, 1987, The Scattering of Electromagnetic Waves From Rough Surfaces
   Bitterli B, 2022, COMPUT GRAPH FORUM, V41, P93, DOI 10.1111/cgf.14589
   Burley Brent, 2012, SIGGRAPH 12 COURS PR, DOI [10.1145/2343483.2343493, DOI 10.1145/2343483.2343493]
   Cook RL, 1982, ACM Trans. Graph., V1, P7, DOI DOI 10.1145/357290.357293
   Cui Yuang, 2023, SA '23: SIGGRAPH Asia 2023 Conference Papers, DOI 10.1145/3610548.3618198
   Deligiannis Johannes, 2019, GDC 19
   Dupuy J, 2023, COMPUT GRAPH FORUM, V42, DOI 10.1111/cgf.14867
   Georgiev Iliyan, 2019, Autodesk standard surface
   Heitz E, 2014, COMPUT GRAPH FORUM, V33, P103, DOI 10.1111/cgf.12417
   Heitz E., 2014, Journal of Computer Graphics Techniques, V3, P32
   Heitz E, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925943
   Heitz Eric, 2018, Journal of Computer Graphics Techniques (JCGT), V7, P1
   Hillaire Sebastien, 2023, SIGGRAPH 23 COURSE A
   Jakob W, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601139
   Jakob Wenzel, 2014, Technical Report
   Kelemen Cs., 2001, Eurographics 2001, Short papers, P25, DOI [10.2312/egs.20011003, DOI 10.2312/EGS.20011003]
   Kulla Christopher, 2017, SIGGRAPH 17 COURSE P, DOI [10.1145/3084873.3084893, DOI 10.1145/3084873.3084893]
   Laine Samuli, 2013, Proceedings of the 5th High-Performance Graphics Conference, P137, DOI [10.1145/2492045.2492060, DOI 10.1145/2492045.2492060]
   McAuley Stephen, 2013, SIGGRAPH 13 COURSES, DOI [10.1145/2504435.2504457, DOI 10.1145/2504435.2504457]
   SMITH BG, 1967, IEEE T ANTENN PROPAG, VAP15, P668, DOI 10.1109/TAP.1967.1138991
   Stachowiak Tomasz, 2015, SIGGRAPH 15 COURSE A, DOI [10.1145/2776880.2787701, DOI 10.1145/2776880.2787701]
   Tokuyoshi Yusuke, 2021, SIGGRAPH 21 TALKS, DOI [10.1145/3450623.3464655, DOI 10.1145/3450623.3464655]
   TORRANCE KE, 1967, J OPT SOC AM, V57, P1105, DOI 10.1364/JOSA.57.001105
   TROWBRIDGE TS, 1975, J OPT SOC AM, V65, P531, DOI 10.1364/JOSA.65.000531
   Turquin Emmanuel, 2019, Technical Report
   Veach E., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P419, DOI 10.1145/218380.218498
   Walter B., 2007, P 18 EUR C REND TECH, P195, DOI [10.2312/EGWR/EGSR07/195-206, DOI 10.2312/EGWR/EGSR07/195-206]
   Wang BB, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530112
NR 31
TC 0
Z9 0
U1 0
U2 0
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
EI 2577-6193
J9 P ACM COMPUT GRAPH
JI P. ACM Comput. Graph. Interact. Tech.
PD MAY
PY 2024
VL 7
IS 1
AR 10
DI 10.1145/3651291
PG 18
WC Computer Science, Software Engineering
WE Emerging Sources Citation Index (ESCI)
SC Computer Science
GA RQ5P8
UT WOS:001229142400010
DA 2024-08-05
ER

PT J
AU Rosset, N
   Duvigneau, R
   Bousseau, A
   Cordonnier, G
AF Rosset, Nicolas
   Duvigneau, Regis
   Bousseau, Adrien
   Cordonnier, Guillaume
TI Windblown sand around obstacles - simulation and validation of
   deposition patterns
SO PROCEEDINGS OF THE ACM ON COMPUTER GRAPHICS AND INTERACTIVE TECHNIQUES
LA English
DT Article
DE Fluid simulation; Sand dunes; Sand deposition; Landscape
ID SNOW-TRANSPORT MODEL; DYNAMICS; SNOWDRIFT; EVOLUTION; DUNES
AB Sand dunes are iconic landmarks of deserts, but can also put human infrastructures at risk, for instance by forming near buildings or roads. We present a simulator of sand erosion and deposition to predict how dunes form around and behind obstacles under wind. Inspired by both computer graphics and geo-sciences, our algorithm couples a fast wind flow simulation with physical laws of sand saltation and avalanching, which suffices to reproduce characteristic patterns of sand deposition. In particular, we validate our approach via a qualitative comparison of the erosion and deposition patterns produced by our simulator against real-world patterns measured by prior work under controlled conditions.
C1 [Rosset, Nicolas; Duvigneau, Regis; Bousseau, Adrien; Cordonnier, Guillaume] Univ Cote Azur, Inria, Nice, France.
C3 Universite Cote d'Azur; Inria
RP Rosset, N (corresponding author), Univ Cote Azur, Inria, Nice, France.
EM nicolas.rosset@inria.fr; regis.duvigneau@inria.fr;
   adrien.bousseau@inria.fr; guillaume.cordonnier@inria.fr
OI Duvigneau, Regis/0000-0002-3369-9554
FU Agence Nationale de la Recherche [ANR-22-CE33-0012-01]
FX We thank Emilie Yu for helping with the figures. We also thank Jingwei
   Tang for implementing the fluid solver. This work was supported by the
   Agence Nationale de la Recherche project Invterra ANR-22-CE33-0012-01
   and research and software donations from Adobe Inc.
CR [Anonymous], 1941, GEOGR J, V98, P109
   Benes Bedrich, 2004, Simulating desert scenery
   Bruno L., 2018, Recent Patents on Engineering, V12, P237, DOI 10.2174/1872212112666180309151818
   Bruno L, 2015, J WIND ENG IND AEROD, V147, P291, DOI 10.1016/j.jweia.2015.07.014
   Cordonnier G, 2018, COMPUT GRAPH FORUM, V37, P497, DOI 10.1111/cgf.13379
   du Pont SC, 2015, CR PHYS, V16, P118, DOI 10.1016/j.crhy.2015.02.002
   Feldman Bryan E., 2002, ACM SIGGRAPH C APPL
   Kochanski Kelly., 2019, The Journal of Open Source Software, V4, P1699
   Kok JF, 2012, REP PROG PHYS, V75, DOI 10.1088/0034-4885/75/10/106901
   Liston GE, 1998, J GLACIOL, V44, P498, DOI 10.3189/S0022143000002021
   Lo Giudice A, 2020, APPL MATH MODEL, V79, P68, DOI 10.1016/j.apm.2019.07.060
   Lo Giudice A, 2019, MATH MECH SOLIDS, V24, P2558, DOI 10.1177/1081286518755230
   Lo Giudice A, 2019, MATH ENG-US, V1, P508, DOI 10.3934/mine.2019.3.508
   Narteau C, 2009, J GEOPHYS RES-EARTH, V114, DOI 10.1029/2008JF001127
   Ng YT, 2009, J COMPUT PHYS, V228, P8807, DOI 10.1016/j.jcp.2009.08.032
   Nichols G., 2009, SEDIMENTOLOGY STRATI, DOI DOI 10.1007/S10533-012-9751-Y
   Onoue K, 2000, EIGHTH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P427, DOI 10.1109/PCCGA.2000.883978
   Paris A, 2019, COMPUT GRAPH FORUM, V38, P47, DOI 10.1111/cgf.13815
   Poppema DW, 2022, AEOLIAN RES, V59, DOI 10.1016/j.aeolia.2022.100840
   Poppema DW, 2022, GEOMORPHOLOGY, V401, DOI 10.1016/j.geomorph.2022.108114
   Raffaele L, 2022, J STRUCT ENG, V148, DOI 10.1061/(ASCE)ST.1943-541X.0003344
   Raffaele L, 2019, ENG STRUCT, V178, P88, DOI 10.1016/j.engstruct.2018.10.017
   Rosset N, 2023, COMPUT GRAPH FORUM, V42, P427, DOI 10.1111/cgf.14772
   Rozier O, 2014, EARTH SURF PROC LAND, V39, P98, DOI 10.1002/esp.3479
   Schneiderbauer S, 2008, ANN GLACIOL, V48, P150, DOI 10.3189/172756408784700789
   Schneiderbauer S, 2011, J GLACIOL, V57, P526, DOI 10.3189/002214311796905677
   Stam J, 1999, COMP GRAPH, P121, DOI 10.1145/311535.311548
   Strypsteen G, 2021, AEOLIAN RES, V52, DOI 10.1016/j.aeolia.2021.100725
   Taylor B, 2023, P ACM COMPUT GRAPH, V6, DOI 10.1145/3585510
   Tominaga Y, 2018, COLD REG SCI TECHNOL, V150, P2, DOI 10.1016/j.coldregions.2017.05.004
   Vionnet V, 2014, CRYOSPHERE, V8, P395, DOI 10.5194/tc-8-395-2014
   Wang N, 2012, J COMPUT SCI TECH-CH, V27, P135, DOI 10.1007/s11390-012-1212-5
   WERNER BT, 1995, GEOLOGY, V23, P1107, DOI 10.1130/0091-7613(1995)023<1107:EDCSAA>2.3.CO;2
   Zhang DG, 2012, NAT GEOSCI, V5, P463, DOI 10.1038/ngeo1503
   Zhou XY, 2023, J WIND ENG IND AEROD, V234, DOI 10.1016/j.jweia.2023.105350
   Zingg A., 1952, PROC 5 HYDRAUL C, P111
NR 36
TC 0
Z9 0
U1 0
U2 0
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
EI 2577-6193
J9 P ACM COMPUT GRAPH
JI P. ACM Comput. Graph. Interact. Tech.
PD MAY
PY 2024
VL 7
IS 1
AR 20
DI 10.1145/3651284
PG 13
WC Computer Science, Software Engineering
WE Emerging Sources Citation Index (ESCI)
SC Computer Science
GA RQ5P8
UT WOS:001229142400020
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Tong, DCW
   Tan, XY
   Chen, AQ
AF Tong, Derek C. W.
   Tan, Xuet Ying
   Chen, Alex Q.
TI Eye Strokes: An Eye-gaze Drawing System for Mandarin Characters
SO PROCEEDINGS OF THE ACM ON COMPUTER GRAPHICS AND INTERACTIVE TECHNIQUES
LA English
DT Article
DE Mandarin character detection; Chinese character detection; Eye strokes;
   Assistive technology
AB Studies showed that 61% of the elderly in Singapore are English illiterate, and it is essential to find alternatives for non-English literate patients who have literacy in the next most common language, Mandarin. Many eye typing solutions use Mandarin hanyu pinyin, a phonetic system similar to eye typing in the English language. However, most Mandarin-speaking elderly in Singapore are not familiar with Mandarin hanyu pinyin despite being able to read and write Mandarin characters, which makes eye typing redundant. We propose Eye Strokes, a technique to capture eye gaze, as an input modality to identify Mandarin characters for motor neurone disease patients to communicate. This method uses the eye gaze as strokes in a Mandarin character to predict and identify the Mandarin word the user intends to communicate. The proof-of-ideation evaluation discussed in the paper shows that our technique is feasible with promising character prediction accuracy for further investigations, although some limitations exist.
C1 [Tong, Derek C. W.] Univ Glasgow, Singapore, Singapore.
   [Tan, Xuet Ying] Tan Tock Seng Hosp, Singapore, Singapore.
   [Chen, Alex Q.] Singapore Inst Technol, Singapore, Singapore.
C3 Tan Tock Seng Hospital; Singapore Institute of Technology
RP Chen, AQ (corresponding author), Singapore Inst Technol, Singapore, Singapore.
EM derektongcw@gmail.com; xuet_ying_tan@ttsh.com.sg;
   Alex.Q.Chen@singaporetech.edu.sg
CR Ashtiani Behrooz., 2010, Proceedings of the 2010 Symposium on Eye-Tracking Research Applications, ETRA 10, P339
   Deng X, 2017, J CLIN NEUROSCI, V39, P137, DOI 10.1016/j.jocn.2016.12.018
   Hornof A., 2004, ASSETS 2004. The Sixth International ACM SIGACCESS Conference on Computers and Accessibility, P86
   Jayech K, 2016, INT ARAB J INF TECHN, V13, P1024
   Liang Z., 2012, P 2012 S EYE TRACKIN, P237, DOI DOI 10.1145/2168556.2168604
   Liu CL, 2011, PROC INT CONF DOC, P37, DOI 10.1109/ICDAR.2011.17
   Liu Y., 2015, Proceedings of the annual meeting of the australian special interest group for computer human interaction, P192, DOI [10.1145/2838739.2838804, DOI 10.1145/2838739.2838804]
   Lopresti D, 2008, LECT NOTES COMPUT SC, V4768, P218, DOI 10.1007/978-3-540-78199-8_13
   Majaranta P., 2002, Proceedings ETRA 2002. Eye Tracking Research and Applications Symposium, P15, DOI 10.1145/507072.507076
   NIELSEN J, 1994, HUMAN FACTORS IN COMPUTING SYSTEMS, CHI '94 CONFERENCE PROCEEDINGS - CELEBRATING INTERDEPENDENCE, P152, DOI 10.1145/191666.191729
   Nih.gov, Motor Neuron Diseases Fact Sheet-National Institute of Neurological Disorders and Stroke
   Suppiah S, 2020, PROC SINGAP HEALTHC, V29, P25, DOI 10.1177/2010105819899126
   TechMediaToday, 2023, TechMediaToday
   Tobii Dynavox US, How far away should my eyes be from the PCEye 5 when using the device or calibrating?
   Urbina Mario H., 2010, P 2010 S EYE TRACK R, DOI DOI 10.1145/1743666.1743738
   Zou J., 2019, Handwritten Chinese Character Recognition by Convolutional Neural Network and Similarity Ranking
NR 16
TC 0
Z9 0
U1 0
U2 0
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
EI 2577-6193
J9 P ACM COMPUT GRAPH
JI P. ACM Comput. Graph. Interact. Tech.
PD MAY
PY 2024
VL 7
IS 2
AR 29
DI 10.1145/3654702
PG 15
WC Computer Science, Software Engineering
WE Emerging Sources Citation Index (ESCI)
SC Computer Science
GA SL0I6
UT WOS:001234486100007
OA hybrid
DA 2024-08-05
ER

PT J
AU Nguyen, VD
   Bailey, R
   Diaz, GJ
   Ma, CY
   Fix, A
   Ororbia, A
AF Viet Dung Nguyen
   Bailey, Reynold
   Diaz, Gabriel J.
   Ma, Chengyi
   Fix, Alexander
   Ororbia, Alexander
TI Deep Domain Adaptation: A Sim2Real Neural Approach for Improving
   Eye-Tracking Systems
SO PROCEEDINGS OF THE ACM ON COMPUTER GRAPHICS AND INTERACTIVE TECHNIQUES
LA English
DT Article
DE Eye-tracking; Domain adaptation; Eye segmentation; Generative modeling;
   Deep learning
AB Eye image segmentation is a critical step in eye tracking that has great influence over the final gaze estimate. Segmentation models trained using supervised machine learning can excel at this task, their effectiveness is determined by the degree of overlap between the narrow distributions of image properties defined by the target dataset and highly specific training datasets, of which there are few. Attempts to broaden the distribution of existing eye image datasets through the inclusion of synthetic eye images have found that a model trained on synthetic images will often fail to generalize back to real-world eye images. In remedy, we use dimensionality-reduction techniques to measure the overlap between the target eye images and synthetic training data, and to prune the training dataset in a manner that maximizes distribution overlap. We demonstrate that our methods result in robust, improved performance when tackling the discrepancy between simulation and real-world data samples.
C1 [Viet Dung Nguyen; Bailey, Reynold; Diaz, Gabriel J.; Ma, Chengyi; Ororbia, Alexander] Rochester Inst Technol, Rochester, NY 14623 USA.
   [Fix, Alexander] Meta Real Labs, Redmond, WA 98052 USA.
C3 Rochester Institute of Technology
RP Nguyen, VD (corresponding author), Rochester Inst Technol, Rochester, NY 14623 USA.
EM vn1747@rit.edu; rjbvcs@rit.edu; Gabriel.Diaz@rit.edu; cxm3593@rit.edu;
   alexander.fix@meta.com; ago@cs.rit.edu
OI Nguyen, Viet Dung/0009-0008-8639-3138; Ororbia,
   Alexander/0000-0002-2590-1310
FU National Science Foundation [DGE-2125362]
FX We would like to thank Meta Reality Labs for providing invaluable
   feedback and supporting this work.<SUP>1</SUP> This material is based
   upon work supported by the National Science Foundation under Award No.
   DGE-2125362. Any opinions, findings, and conclusions or recommendations
   expressed in this material are those of the author(s) and do not
   necessarily reflect the views of the National Science Foundation.
CR CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851
   Chantapakul Watchanan, 2019, P 2019 2 ART INT CLO
   Chaudhary AK, 2019, IEEE INT CONF COMP V, P3698, DOI 10.1109/ICCVW.2019.00568
   Chen C, 2019, IEEE I CONF COMP VIS, P3184, DOI 10.1109/ICCV.2019.00328
   DAUGMAN JG, 1993, IEEE T PATTERN ANAL, V15, P1148, DOI 10.1109/34.244676
   Deng WJ, 2018, Arxiv, DOI [arXiv:1711.07027, DOI 10.48550/ARXIV.1711.07027, 10.48550/ARXIV.1711.07027, 10.48550/arXiv.1711.07027]
   Dong XP, 2018, LECT NOTES COMPUT SC, V11217, P472, DOI 10.1007/978-3-030-01261-8_28
   Ganin Y, 2016, J MACH LEARN RES, V17
   Ghosh S., 2021, arXiv, DOI DOI 10.48550/ARXIV.2108.05479
   Goodfellow J., 2014, arXiv, DOI DOI 10.48550/ARXIV.1406.2661
   Gretton A, 2012, J MACH LEARN RES, V13, P723
   Hadsell R., 2006, Computer vision and pattern recognition, 2006 IEEE computer society conference on, V2, P1735, DOI DOI 10.1109/CVPR.2006.100
   Haykin S., 1998, Neural Networks: A Comprehensive Foundation
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hensel M, 2017, ADV NEUR IN, V30
   Garbin SJ, 2019, Arxiv, DOI arXiv:1905.03702
   Goodfellow IJ, 2015, Arxiv, DOI [arXiv:1412.6572, DOI 10.48550/ARXIV.1412.6572]
   KANOPOULOS N, 1988, IEEE J SOLID-ST CIRC, V23, P358, DOI 10.1109/4.996
   Kaspar M, 2020, IEEE INT C INT ROBOT, P4383, DOI 10.1109/IROS45743.2020.9341260
   Kervadec H, 2021, MED IMAGE ANAL, V67, DOI 10.1016/j.media.2020.101851
   Koch G., 2015, ICML DEEP LEARN WORK, V2, P1
   Kosub S, 2019, PATTERN RECOGN LETT, V120, P36, DOI 10.1016/j.patrec.2018.12.007
   Kothari Rakshit S., 2022, Proceedings of the ACM on Human-Computer Interaction, V6, DOI 10.1145/3530880
   Kothari RS, 2021, IEEE T VIS COMPUT GR, V27, P2757, DOI 10.1109/TVCG.2021.3067765
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   LeCun Y., 1995, The handbook of brain theory and neural networks, DOI 10.5555/303568.303704
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Liu Sizhe, 2022, Journal of Physics: Conference Series, DOI 10.1088/1742-6596/2400/1/012030
   Lu C, 2022, 2022 ACM SYMPOSIUM ON EYE TRACKING RESEARCH AND APPLICATIONS, ETRA 2022, DOI 10.1145/3517031.3532524
   Lucic M, 2018, Arxiv, DOI arXiv:1711.10337
   Melekhov I, 2016, INT C PATT RECOG, P378, DOI 10.1109/ICPR.2016.7899663
   Milletari F, 2016, INT CONF 3D VISION, P565, DOI 10.1109/3DV.2016.79
   Minaee S, 2022, IEEE T PATTERN ANAL, V44, P3523, DOI 10.1109/TPAMI.2021.3059968
   Mustafa WA, 2018, J PHYS CONF SER, V1019, DOI 10.1088/1742-6596/1019/1/012026
   Nair N, 2020, ACM SYMPOSIUM ON APPLIED PERCEPTION (SAP 2020), DOI 10.1145/3385955.3407935
   Nanni L, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21175809
   Pelz Jeff, 2017, International Application, Patent No. [PCT/US2017/034756, 2017034756]
   Pelz Jeff, 2017, Patent No. [WO/2017/205789, 2017205789]
   Perry J, 2019, IEEE INT CONF COMP V, P3671, DOI 10.1109/ICCVW.2019.00453
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Salimans T, 2016, ADV NEUR IN, V29
   Shah S, 2009, IEEE T INF FOREN SEC, V4, P824, DOI 10.1109/TIFS.2009.2033225
   Stember JN, 2019, J DIGIT IMAGING, V32, P597, DOI 10.1007/s10278-019-00220-4
   Sudre CH, 2017, LECT NOTES COMPUT SC, V10553, P240, DOI 10.1007/978-3-319-67558-9_28
   Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278
   Taigman Yaniv, 2017, INT C LEARN REPR
   Tobin Josh, 2017, 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), P23, DOI 10.1109/IROS.2017.8202133
   Tu LP, 2013, 2013 6TH INTERNATIONAL CONGRESS ON IMAGE AND SIGNAL PROCESSING (CISP), VOLS 1-3, P443, DOI 10.1109/CISP.2013.6744035
   Yaras C, 2021, Arxiv, DOI arXiv:2104.14032
   Yiu YH, 2019, J NEUROSCI METH, V324, DOI 10.1016/j.jneumeth.2019.05.016
   Zhang XC, 2015, PROC CVPR IEEE, P4511, DOI 10.1109/CVPR.2015.7299081
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 52
TC 0
Z9 0
U1 0
U2 0
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
EI 2577-6193
J9 P ACM COMPUT GRAPH
JI P. ACM Comput. Graph. Interact. Tech.
PD MAY
PY 2024
VL 7
IS 2
AR 25
DI 10.1145/3654703
PG 17
WC Computer Science, Software Engineering
WE Emerging Sources Citation Index (ESCI)
SC Computer Science
GA SL0I6
UT WOS:001234486100003
OA Green Submitted, hybrid
DA 2024-08-05
ER

PT J
AU Ying, Z
   Edwards, N
   Kutuzov, M
AF Ying, Zhi
   Edwards, Nicholas
   Kutuzov, Mikhail
TI Efficient Visibility Approximation for Game AI using Neural
   Omnidirectional Distance Fields
SO PROCEEDINGS OF THE ACM ON COMPUTER GRAPHICS AND INTERACTIVE TECHNIQUES
LA English
DT Article
DE Visibility for Game AI; Line of Sight; Omnidirectional Distance Fields;
   Neural Implicit Representation; Multi-resolution Grid Encoding
AB Visibility information is critical in game AI applications, but the computational cost of raycasting-based methods poses a challenge for real-time systems. To address this challenge, we propose a novel method that represents a partitioned game scene as neural Omnidirectional Distance Fields (ODFs), allowing scalable and efficient visibility approximation between positions without raycasting. For each position of interest, we map its omnidirectional distance data from the spherical surface onto a UV plane. We then use multi-resolution grids and bilinearly interpolated features to encode directions. This allows us to use a compact multi-layer perceptron (MLP) to reconstruct the high-frequency directional distance data at these positions, ensuring fast inference speed. We demonstrate the effectiveness of our method through offline experiments and in-game evaluation. For in-game evaluation, we conduct a side-by-side comparison with raycasting-based visibility tests in three different scenes. Using a compact MLP (128 neurons and 2 layers), our method achieves an average cold start speedup of 9.35 times and warm start speedup of 4.8 times across these scenes. In addition, unlike the raycasting-based method, whose evaluation time is affected by the characteristics of the scenes, our method's evaluation time remains constant.
C1 [Ying, Zhi] Ubisoft Forge, Shanghai, Peoples R China.
   [Edwards, Nicholas; Kutuzov, Mikhail] Ubisoft, Montreal, PQ, Canada.
C3 Ubisoft Entertainment
RP Ying, Z (corresponding author), Ubisoft Forge, Shanghai, Peoples R China.
EM zhi.ying@ubisoft.com; nicholas.edwards@ubisoft.com;
   mikhail.kutuzov@ubisoft.com
OI Ying, Zhi/0009-0008-8390-3366
FU Ubisoft
FX We would like to thank Georges Nader, Ludovic Denoyer, Jean-Philippe
   Barrette-LaPierre, Alexis Rolland, and Yves Jacquier for their valuable
   discussions and reviews. This work was supported by Ubisoft.
CR [Anonymous], 2004, AI for game developers
   Bittner J, 2003, ENVIRON PLANN B, V30, P729, DOI 10.1068/b2957
   Cao A, 2023, PROC CVPR IEEE, P130, DOI 10.1109/CVPR52729.2023.00021
   Chen ZQ, 2023, PROC CVPR IEEE, P16569, DOI 10.1109/CVPR52729.2023.01590
   Chibane J., 2020, Advances in Neural Information Processing Systems, V33, P21638
   Dill Kevin, 2019, Game AI Pro 360, P125
   Dou YS, 2023, Arxiv, DOI arXiv:2310.08332
   Dutordoir V., 2020, International Conference on Machine Learning, P2793
   Feng BY, 2022, LECT NOTES COMPUT SC, V13663, P138, DOI 10.1007/978-3-031-20062-5_9
   FOLEY T., 2005, HWWS 05, P15, DOI [10.1145/1071866.1071869, DOI 10.1145/1071866.1071869]
   Forbus KD, 2002, IEEE INTELL SYST, V17, P25, DOI 10.1109/MIS.2002.1024748
   Fridovich-Keil S, 2022, PROC CVPR IEEE, P5491, DOI 10.1109/CVPR52688.2022.00542
   Glassner A.S., 1989, An introduction to ray tracing
   González A, 2010, MATH GEOSCI, V42, P49, DOI 10.1007/s11004-009-9257-x
   Górski KM, 2005, ASTROPHYS J, V622, P759, DOI 10.1086/427976
   Hedman P, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5855, DOI 10.1109/ICCV48922.2021.00582
   Houchens T, 2022, Arxiv, DOI arXiv:2206.05837
   Jack Matthew, 2019, Game AI Pro 360, P1
   Kerbl B, 2023, ACM T GRAPHIC, V42, DOI 10.1145/3592433
   Klosowski James T, 1994, Efficient collision detection using bounding volume hierarchies of k-DOPs. IEEE transactions on Visualization and Computer Graphics, V4, P21
   Liu ZM, 2023, Arxiv, DOI [arXiv:2310.19629, 10.48550/arXiv.2310.19629, DOI 10.48550/ARXIV.2310.19629]
   Luiten Jonathon, 2024, 3DV
   Majercik Z., 2019, J COMPUTER GRAPHICS, V8, P1
   McGuire M., 2017, Proceedings of the 21st ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games, I3D'17, p2:1, DOI DOI 10.1145/3023368.3023378
   McIntosh Travis, 2019, Game AI Pro, V360, P13
   Mildenhall B, 2022, COMMUN ACM, V65, P99, DOI 10.1145/3503250
   Mller T., 1997, J. Graph. Tools, V2, P21, DOI DOI 10.1080/10867651.1997.10487468
   Müller T, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530127
   Muller C, 2006, Spherical harmonics, V17
   Ost J, 2021, PROC CVPR IEEE, P2855, DOI 10.1109/CVPR46437.2021.00288
   Powers D., 2008, Journal of Machine Learning Research, V2, DOI DOI 10.9735/2229-3981
   Rahaman N, 2019, PR MACH LEARN RES, V97
   Smits Brian., 2005, ACM SIGGRAPH 2005 CO, P6, DOI 10.1145/1198555.1198745
   Suda R, 2002, MATH COMPUT, V71, P703, DOI 10.1090/S0025-5718-01-01386-2
   Swinbank R, 2006, Q J ROY METEOR SOC, V132, P1769, DOI 10.1256/qj.05.227
   Tancik M., 2020, ADV NEURAL INFORM PR, V33, P7537, DOI DOI 10.48550/ARXIV.2006.10739
   Tancik M, 2022, PROC CVPR IEEE, P8238, DOI 10.1109/CVPR52688.2022.00807
   Turki H, 2022, PROC CVPR IEEE, P12912, DOI 10.1109/CVPR52688.2022.01258
   Ueda Itsuki, 2022, P EUR C COMP VIS
   Wald I, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601199
   Walsh Martin, 2019, Game AI Pro 360, P73
   Welsh Rich, 2013, Game AI Pro: Collected Wisdom of Game AI Professionals, V403, P411
   Yu A, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5732, DOI 10.1109/ICCV48922.2021.00570
NR 43
TC 0
Z9 0
U1 0
U2 0
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
EI 2577-6193
J9 P ACM COMPUT GRAPH
JI P. ACM Comput. Graph. Interact. Tech.
PD MAY
PY 2024
VL 7
IS 1
AR 21
DI 10.1145/3651289
PG 15
WC Computer Science, Software Engineering
WE Emerging Sources Citation Index (ESCI)
SC Computer Science
GA RQ5P8
UT WOS:001229142400021
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Li, TX
   Shi, R
   Li, ZH
   Kanai, T
   Zhu, Q
AF Li, Tianxing
   Shi, Rui
   Li, Zihui
   Kanai, Takashi
   Zhu, Qing
TI Efficient Deformation Learning of Varied Garments with a
   Structure-Preserving Multilevel Framework
SO PROCEEDINGS OF THE ACM ON COMPUTER GRAPHICS AND INTERACTIVE TECHNIQUES
LA English
DT Article
DE Clothing deformation; Nerual network; Graph pooling; Attribution
   analysis
AB Due to the highly nonlinear behavior of clothing, modelling fine-scale garment deformation on arbitrary meshes under varied conditions within a unified network poses a significant challenge. Existing methods often compromise on either model generalization, deformation quality, or runtime speed, making them less suitable for real-world applications. To address it, we propose to incorporate multi-source graph construction and pooling to achieve a novel graph learning scheme. We first introduce methods for extracting cues from different deformation correlations and transform the garment mesh into a comprehensive graph enriched with deformation-related information. To enhance the learning capability and generalizability of the model, we present structure-preserving pooling and unpooling strategies for the mesh deformation task, thereby improving information propagation across the mesh and enhancing the realism of deformation. Lastly, we conduct an attribution analysis and visualize the contribution of various vertices in the graph to the output, providing insights into the deformation behavior. The experimental results demonstrate superior performance against state-of-the-art methods.
C1 [Li, Tianxing; Shi, Rui; Li, Zihui; Zhu, Qing] Beijing Univ Technol, Beijing, Peoples R China.
   [Kanai, Takashi] Univ Tokyo, Tokyo, Japan.
C3 Beijing University of Technology; University of Tokyo
RP Shi, R (corresponding author), Beijing Univ Technol, Beijing, Peoples R China.
EM litianxing@bjut.edu.cn; ruishi@bjut.edu.cn; lizihui@emails.bjut.edu.cn;
   kanait@acm.org; ccgszq@bjut.edu.cn
OI ZHU, Qing/0000-0001-7380-485X; Kanai, Takashi/0000-0002-1635-3818; Li,
   Tianxing/0000-0002-2489-4884
FU Beijing Natural Science Foundation [4244088, 4232017]; JSPS KAKENHI
   [22K12331]
FX This work has been partially supported by Beijing Natural Science
   Foundation (No. 4244088, 4232017) and JSPS KAKENHI (No. 22K12331).
CR Bertiche Hugo, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12365), P344, DOI 10.1007/978-3-030-58565-5_21
   Bertiche H, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3550454.3555491
   Bertiche H, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3478513.3480479
   Carnegie-Mellon, 2010, CMU graphics lab motion capture database
   Chen Z, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3462758
   Choi KJ, 2002, ACM T GRAPHIC, V21, P604, DOI 10.1145/566570.566624
   Cirio G, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2661229.2661279
   Fowlkes C, 2004, IEEE T PATTERN ANAL, V26, P214, DOI 10.1109/TPAMI.2004.1262185
   Gao HY, 2022, IEEE T PATTERN ANAL, V44, P4948, DOI 10.1109/TPAMI.2021.3081010
   Grigorev A, 2023, PROC CVPR IEEE, P16965, DOI 10.1109/CVPR52729.2023.01627
   Gundogdu E, 2022, IEEE T PATTERN ANAL, V44, P181, DOI 10.1109/TPAMI.2020.3010886
   Hahn F, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601160
   Jeong MH, 2015, COMPUT ANIMAT VIRT W, V26, P291, DOI 10.1002/cav.1653
   Jiang CFF, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073623
   Lee J, 2019, PR MACH LEARN RES, V97
   Lewis JP, 2000, COMP GRAPH, P165, DOI 10.1145/344779.344862
   Li J, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201308
   Li T, 2023, COMPUT GRAPH FORUM, V42, P231, DOI 10.1111/cgf.14651
   Li Tianxing, 2023, IEEE Trans Vis Comput Graph, VPP, DOI 10.1109/TVCG.2023.3346055
   Li TX, 2021, COMPUT GRAPH FORUM, V40, P537, DOI 10.1111/cgf.142653
   Li TX, 2020, I3D 2020: ACM SIGGRAPH SYMPOSIUM ON INTERACTIVE 3D GRAPHICS AND GAMES, DOI 10.1145/3384382.3384525
   Lino M, 2022, PHYS FLUIDS, V34, DOI 10.1063/5.0097679
   Lino Mario, 2021, arXiv
   Loper M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818013
   Magnenat-Thalmann N., 1989, P GRAPHICS INTERFACE, P26, DOI DOI 10.5555/102313.102317
   Narain R, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366171
   Nealen A, 2006, COMPUT GRAPH FORUM, V25, P809, DOI 10.1111/j.1467-8659.2006.01000.x
   Pan Xiaoyu, 2022, SIGGRAPH22 Conference Proceeding: Special Interest Group on Computer Graphics and Interactive Techniques Conference Proceedings, DOI 10.1145/3528233.3530709
   Patel Chaitanya, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7363, DOI 10.1109/CVPR42600.2020.00739
   Ranjan Ekagra, 2019, P AAAI C ART INT
   Santesteban I., 2022, Advances in Neural Information Processing Systems, V35, P12110
   Santesteban I, 2022, PROC CVPR IEEE, P8130, DOI 10.1109/CVPR52688.2022.00797
   Santesteban I, 2021, PROC CVPR IEEE, P11758, DOI 10.1109/CVPR46437.2021.01159
   Santesteban I, 2019, COMPUT GRAPH FORUM, V38, P355, DOI 10.1111/cgf.13643
   Shi R, 2022, IMAGE VISION COMPUT, V124, DOI 10.1016/j.imavis.2022.104516
   Sigal L, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766971
   Stoll C, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866161
   Tang M, 2016, COMPUT GRAPH FORUM, V35, P511, DOI 10.1111/cgf.12851
   Tiwari Garvita, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12348), P1, DOI 10.1007/978-3-030-58580-8_1
   Vassilev T, 2001, COMPUT GRAPH FORUM, V20, pC260, DOI 10.1111/1467-8659.00518
   Velickovic P, 2018, INT C LEARN REPR, DOI DOI 10.48550/ARXIV.1710.10903
   Vidaurre R, 2020, COMPUT GRAPH FORUM, V39, P145, DOI 10.1111/cgf.14109
   Wang HM, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459787
   Wang Tuanfeng Y., 2018, ACM Transactions on Graphics, V37, DOI 10.1145/3272127.3275074
   Wu LH, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3430025
   Yang S, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3026479
   Ying R, 2018, ADV NEUR IN, V31
NR 47
TC 0
Z9 0
U1 1
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
EI 2577-6193
J9 P ACM COMPUT GRAPH
JI P. ACM Comput. Graph. Interact. Tech.
PD MAY
PY 2024
VL 7
IS 1
AR 8
DI 10.1145/3651286
PG 19
WC Computer Science, Software Engineering
WE Emerging Sources Citation Index (ESCI)
SC Computer Science
GA RQ5P8
UT WOS:001229142400008
DA 2024-08-05
ER

PT J
AU Papantonakis, P
   Kopanas, G
   Kerbl, B
   Lanvin, A
   Drettakis, G
AF Papantonakis, Panagiotis
   Kopanas, Georgios
   Kerbl, Bernhard
   Lanvin, Alexandre
   Drettakis, George
TI Reducing the Memory Footprint of 3D Gaussian Splatting
SO PROCEEDINGS OF THE ACM ON COMPUTER GRAPHICS AND INTERACTIVE TECHNIQUES
LA English
DT Article
DE novel view synthesis; radiance fields; 3D gaussian splatting; memory
   reduction
ID FIELDS
AB 3D Gaussian splatting provides excellent visual quality for novel view synthesis, with fast training and realtime rendering; unfortunately, the memory requirements of this method for storing and transmission are unreasonably high. We first analyze the reasons for this, identifying three main areas where storage can be reduced: the number of 3D Gaussian primitives used to represent a scene, the number of coefficients for the spherical harmonics used to represent directional radiance, and the precision required to store Gaussian primitive attributes. We present a solution to each of these issues. First, we propose an efficient, resolution-aware primitive pruning approach, reducing the primitive count by half. Second, we introduce an adaptive adjustment method to choose the number of coefficients used to represent directional radiance for each Gaussian primitive, and finally a codebook-based quantization method, together with a half-float representation for further memory reduction. Taken together, these three components result in a x27 reduction in overall size on disk on the standard datasets we tested, along with a x1.7 speedup in rendering speed. We demonstrate our method on standard datasets and show how our solution results in significantly reduced download times when using the method on a mobile device (see Fig. 1).
C1 [Papantonakis, Panagiotis; Kopanas, Georgios; Kerbl, Bernhard; Lanvin, Alexandre; Drettakis, George] Univ Cote Azur, INRIA, Nice, France.
C3 Inria; Universite Cote d'Azur
RP Papantonakis, P (corresponding author), Univ Cote Azur, INRIA, Nice, France.
EM panagiotis.papantonakis@inria.fr; george.kopanas@gmail.com;
   kerbl@cg.tuwien.ac.at; Alexandre.Lanvin@inria.fr;
   George.Drettakis@inria.fr
OI Kopanas, Georgios/0009-0002-5829-2192; Lanvin,
   Alexandre/0009-0003-5343-2528; Papantonakis,
   Panagiotis/0009-0009-5058-1249; Kerbl, Bernhard/0000-0002-5168-8648
FU ERC Advanced grant FUNGRAPH [788065]
FX This research was funded by the ERC Advanced grant FUNGRAPH No 788065
   (https://fungraph.inria.fr).The authors are grateful to Adobe for
   generous donations, NVIDIA for a hardware donation, and the OPAL
   infrastructure from Universite Cote d'Azur.
CR [Anonymous], 2008, Image-Based Rendering
   Barron JT, 2023, Arxiv, DOI arXiv:2304.06706
   Barron JT, 2022, PROC CVPR IEEE, P5460, DOI 10.1109/CVPR52688.2022.00539
   Barron JT, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5835, DOI 10.1109/ICCV48922.2021.00580
   Bonopera Sebastien, 2020, sibr: A System for Image Based Rendering
   Buehler C, 2001, COMP GRAPH, P425, DOI 10.1145/383259.383309
   Chaurasia G, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2487228.2487238
   Chen AP, 2022, LECT NOTES COMPUT SC, V13692, P333, DOI 10.1007/978-3-031-19824-3_20
   Chung J, 2024, Arxiv, DOI arXiv:2311.13398
   Debevec Paul E, 2023, Seminal Graphics Papers: Pushing the Boundaries, V2, P465
   Duckworth D, 2024, Arxiv, DOI arXiv:2312.07541
   Ebert Dylan, 2024, Proc. ACM Comput. Graph. Interact. Tech., V7
   Eisemann M, 2008, COMPUT GRAPH FORUM, V27, P409, DOI 10.1111/j.1467-8659.2008.01138.x
   Fan ZW, 2024, Arxiv, DOI arXiv:2311.17245
   Fridovich-Keil S, 2022, PROC CVPR IEEE, P5491, DOI 10.1109/CVPR52688.2022.00542
   Girish S, 2024, Arxiv, DOI arXiv:2312.04564
   Hedman P, 2018, SIGGRAPH ASIA'18: SIGGRAPH ASIA 2018 TECHNICAL PAPERS, DOI 10.1145/3272127.3275084
   Kerbl B, 2023, ACM T GRAPHIC, V42, DOI 10.1145/3592433
   Knapitsch A, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073599
   Lee JC, 2024, Arxiv, DOI arXiv:2311.13681
   Levoy M., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P31, DOI 10.1145/237170.237199
   Liu L., 2020, Advances in Neural Information Processing Systems, V33, P15651
   Mildenhall B, 2022, COMMUN ACM, V65, P99, DOI 10.1145/3503250
   Müller T, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530127
   Navaneet KL, 2024, Arxiv, DOI arXiv:2311.18159
   Niedermayr S, 2024, Arxiv, DOI arXiv:2401.02436
   Pranckevicius Aras, Making Gaussian Splats smaller
   Reiser C, 2023, ACM T GRAPHIC, V42, DOI 10.1145/3592426
   Riegler Gernot, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12364), P623, DOI 10.1007/978-3-030-58529-7_37
   Sun C, 2022, PROC CVPR IEEE, P5449, DOI 10.1109/CVPR52688.2022.00538
   Tewari A, 2022, COMPUT GRAPH FORUM, V41, P703, DOI 10.1111/cgf.14507
   Turki H, 2024, Arxiv, DOI arXiv:2312.03160
   Wang Z, 2023, Arxiv, DOI arXiv:2311.10091
   Xie YH, 2022, COMPUT GRAPH FORUM, V41, P641, DOI 10.1111/cgf.14505
   Yu A, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5732, DOI 10.1109/ICCV48922.2021.00570
   Yu Zehao, 2024, CVPR
NR 36
TC 0
Z9 0
U1 1
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
EI 2577-6193
J9 P ACM COMPUT GRAPH
JI P. ACM Comput. Graph. Interact. Tech.
PD MAY
PY 2024
VL 7
IS 1
AR 16
DI 10.1145/3651282
PG 17
WC Computer Science, Software Engineering
WE Emerging Sources Citation Index (ESCI)
SC Computer Science
GA RQ5P8
UT WOS:001229142400016
OA Green Submitted
DA 2024-08-05
ER

EF