FN Clarivate Analytics Web of Science
VR 1.0
PT J
AU Lu, Q
   Chai, BJ
   Zhang, HB
AF Lu Qiang
   Chai Bingjie
   Zhang Haibo
TI Storytelling by the StoryCake visualization
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT International Conference on Cyberworlds (CW)
CY SEP 28-30, 2016
CL Chongqing Univ Technol, Chongqing, PEOPLES R CHINA
SP ACM SIGGRAPH, Eurograph Assoc, Int Federat Informat Proc, China Comp Federat, China Soc Image & Graph, Zhongxing Telecommunicat Equipment Corp, Kingdee Software Corp, ACM
HO Chongqing Univ Technol
DE Storyline visualization; Nonlinear narrative; Story tracking; Scenario
   analysis
ID INFORMATION VISUALIZATION; TIME; EVOLUTION; TRACKING
AB On the issue of how to explore the complex relationships of entities in a story, Storyline visualization has proven to be a useful approach. The traditional Storyline visualization technology only applies to a linear continuous story and has the limitation of reflecting the development of the plots in a story. In this paper, we propose a hierarchical plot visualization method called StoryCake to improve this problem. First, according to the story elements, inter-sessions are divided into groups, whereby the layout of the entities will be optimized in individual groups. Then, the hierarchical relationships of entities in polar coordinates can be confirmed. Finally, we use several skills, e.g., interaction, labels and a fan-shaped visualization view, to enable people better understand how story evolves and track the hierarchical relationship more conveniently. Experimental results show that the proposed hierarchical plot visualization method in polar coordinates can reveal discontinuous events and nonlinear stories. The results also show that our algorithm presents the narrative structure and development of the story.
C1 [Lu Qiang; Chai Bingjie; Zhang Haibo] HFUT, Sch Comp & Informat, Hefei, Anhui, Peoples R China.
RP Lu, Q (corresponding author), HFUT, Sch Comp & Informat, Hefei, Anhui, Peoples R China.
EM luqiang@hfut.edu.cn
RI Lu, Qiang/AAU-9248-2020; Zhang, Haibo/HLP-9266-2023
CR Afzal S, 2012, IEEE T VIS COMPUT GR, V18, P2556, DOI 10.1109/TVCG.2012.264
   Aigner W, 2005, NINTH INTERNATIONAL CONFERENCE ON INFORMATION VISUALISATION, PROCEEDINGS, P457, DOI 10.1109/IV.2005.97
   Andre P, 2007, UIST 2007: PROCEEDINGS OF THE 20TH ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P101
   Andrews K., 1998, P IEEE S INF VIS LAT, P9
   Bach B, 2016, IEEE T VIS COMPUT GR, V22, P559, DOI 10.1109/TVCG.2015.2467851
   Bade R., 2004, P C HUM FACT COMP SY, P65
   Bouali F, 2016, VISUAL COMPUT, V32, P15, DOI 10.1007/s00371-014-1052-0
   Buono P., 2015, P INT C COLL TECHN S
   Burch M, 2015, 2015 IEEE 3RD WORKING CONFERENCE ON SOFTWARE VISUALIZATION (VISSOFT), P116, DOI 10.1109/VISSOFT.2015.7332421
   Christian M., 1974, FILM LANGUAGE SEMIOT
   Gotz D, 2014, IEEE T VIS COMPUT GR, V20, P1783, DOI 10.1109/TVCG.2014.2346682
   Kosara R, 2013, COMPUTER, V46, P44, DOI 10.1109/MC.2013.36
   Lee T.Y., 2010, P IEEE PAC VIS S
   Liang H, 2017, VISUAL COMPUT, V33, P517, DOI 10.1007/s00371-016-1272-6
   Liu S., 2009, P 18 ACM C INF KNOWL, P543
   Liu SX, 2014, VISUAL COMPUT, V30, P1373, DOI 10.1007/s00371-013-0892-3
   Liu SX, 2013, IEEE T VIS COMPUT GR, V19, P2436, DOI 10.1109/TVCG.2013.196
   Lou XH, 2008, IEEE PACIFIC VISUALISATION SYMPOSIUM 2008, PROCEEDINGS, P151
   Luo DN, 2012, IEEE T VIS COMPUT GR, V18, P93, DOI 10.1109/TVCG.2010.225
   Munroe R., 2016, MOVIE NARRATIVE CHAR
   Ogawa M, 2010, SOFTVIS 2010: PROCEEDINGS OF THE 2010 INTERNATIONAL SYMPOSIUM ON SOFTWARE VISUALIZATION, P35
   Plaisant C., 1996, Human Factors in Computing Systems. Common Ground. CHI 96 Conference Proceedings, P221, DOI 10.1145/238386.238493
   Reinders F, 2001, VISUAL COMPUT, V17, P55, DOI 10.1007/PL00013399
   Sun GD, 2014, IEEE T VIS COMPUT GR, V20, P1753, DOI 10.1109/TVCG.2014.2346919
   Tanahashi Y, 2012, IEEE T VIS COMPUT GR, V18, P2679, DOI 10.1109/TVCG.2012.212
   Therón R, 2006, LECT NOTES COMPUT SC, V4073, P70
   Tobiasz M, 2009, IEEE T VIS COMPUT GR, V15, P1065, DOI 10.1109/TVCG.2009.162
   Wang TD, 2008, CHI 2008: 26TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS VOLS 1 AND 2, CONFERENCE PROCEEDINGS, P457
   Wongsuphasawat K, 2012, IEEE T VIS COMPUT GR, V18, P2659, DOI 10.1109/TVCG.2012.225
   Yang J, 2002, INFOVIS 2002: IEEE SYMPOSIUM ON INFORMATION VISUALIZATION 2002, P77, DOI 10.1109/INFVIS.2002.1173151
   Zhu YZ, 2016, VISUAL COMPUT, V32, P1133, DOI 10.1007/s00371-016-1213-4
NR 31
TC 8
Z9 11
U1 1
U2 20
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2017
VL 33
IS 10
BP 1241
EP 1252
DI 10.1007/s00371-017-1409-2
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA FF8VN
UT WOS:000409296000003
DA 2024-07-18
ER

PT J
AU Cho, J
   Heo, JP
   Kim, T
   Han, B
   Yoon, SE
AF Cho, Jaehyeong
   Heo, Jae-Pil
   Kim, Taeyoung
   Han, Bohyung
   Yoon, Sung-Eui
TI Rank-based voting with inclusion relationship for accurate image search
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 34th International Conference on Computer Graphics (CGI)
CY JUN 27-30, 2017
CL Yokohama, JAPAN
SP Keio Univ, Fac Sci & Technol
DE Accurate image search; Spatial relationship of image regions; Image
   search-based applications
AB We present a rank-based voting technique utilizing inclusion relationship for high-quality image search. Since images can have multiple regions of interest, we extract representative object regions using a state-of-the-art region proposal method tailored for our search problem. We then extract CNN features locally from those representative regions and identify inclusion relationship between those regions. To identify similar images given a query, we propose a novel similarity measure based on representative regions and their inclusion relationship. Our similarity measure gives a high score to a pair of images that contain similar object regions with similar spatial arrangement. To verify benefits of our method, we test our method in three standard benchmarks and compare it against the state-of-the-art image search methods using CNN features. Our experiment results demonstrate effectiveness and robustness of the proposed algorithm.
C1 [Cho, Jaehyeong; Kim, Taeyoung; Yoon, Sung-Eui] Korea Adv Inst Sci & Technol, Daejeon, South Korea.
   [Heo, Jae-Pil] Sungkyunkwan Univ, Seoul, South Korea.
   [Han, Bohyung] POSTECH, Pohang, South Korea.
C3 Korea Advanced Institute of Science & Technology (KAIST); Sungkyunkwan
   University (SKKU); Pohang University of Science & Technology (POSTECH)
RP Yoon, SE (corresponding author), Korea Adv Inst Sci & Technol, Daejeon, South Korea.
EM dil122001@gmail.com; jaepilheo@gmail.com; retupmoc@kaist.ac.kr;
   bhhan@postech.ac.kr; sungeui@kaist.edu
RI Yoon, Sung-eui/C-1678-2011
OI Kim, Taeyoung/0000-0002-1894-6596
FU MSIP/IITP [R0126-16-1108]; MI/KEIT [10070171]; DAPA/DITC [UC160003D]
FX This work was supported in part by MSIP/IITP R0126-16-1108, MI/KEIT
   10070171 and DAPA/DITC (UC160003D).
CR [Anonymous], 2012, ECCV
   [Anonymous], 2016, INT C LEARNING REPRE
   [Anonymous], EUR C COMP VIS
   [Anonymous], 2017, COMMUN ACM, DOI DOI 10.1145/3065386
   [Anonymous], IEEE T PATTERN ANAL
   [Anonymous], 2007, CVPR
   [Anonymous], 2015, ARXIV151204065
   [Anonymous], IEEE CONFERENCE ON C
   [Anonymous], 2014, COMPUTER VISION PATT
   [Anonymous], 2011, ICCV
   [Anonymous], 2016, COMPUTER VISION PATT
   [Anonymous], CVPR IN PRESS
   [Anonymous], 2010, CVPR
   [Anonymous], CVPR
   [Anonymous], 2003, ICCV
   Babenko A, 2014, LECT NOTES COMPUT SC, V8689, P584, DOI 10.1007/978-3-319-10590-1_38
   Cheng MM, 2014, PROC CVPR IEEE, P3286, DOI 10.1109/CVPR.2014.414
   Gordo A., 2016, ARXIV160401325
   Hays J, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239455
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Jégou H, 2011, IEEE T PATTERN ANAL, V33, P117, DOI 10.1109/TPAMI.2010.57
   Jégou H, 2009, PROC CVPR IEEE, P1169, DOI 10.1109/CVPRW.2009.5206609
   Jia Y., 2014, P 22 ACM INT C MULT, P675
   Kemelmacher-Shlizerman I, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925871
   Krähenbühl P, 2014, LECT NOTES COMPUT SC, V8693, P725, DOI 10.1007/978-3-319-10602-1_47
   Samii A, 2015, COMPUT GRAPH FORUM, V34, P141, DOI 10.1111/cgf.12465
   Simonyan K., 2014, 14091556 ARXIV
   Snavely N, 2006, ACM T GRAPHIC, V25, P835, DOI 10.1145/1141911.1141964
   Szegedy Christian, 2015, IEEE C COMP VIS PATT, DOI [10.1109/cvpr.2015.7298594, DOI 10.1109/CVPR.2015.7298594]
   Xie LX, 2015, ICMR'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P3, DOI 10.1145/2671188.2749289
   Yandex Artem Babenko, 2015, 2015 IEEE International Conference on Computer Vision (ICCV). Proceedings, P1269, DOI 10.1109/ICCV.2015.150
NR 31
TC 2
Z9 2
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2017
VL 33
IS 6-8
BP 1049
EP 1059
DI 10.1007/s00371-017-1371-z
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA EX1EY
UT WOS:000402964800034
DA 2024-07-18
ER

PT J
AU Liu, HC
   Zhang, FL
   Marshall, D
   Shi, LP
   Hu, SM
AF Liu, Han-Chao
   Zhang, Fang-Lue
   Marshall, David
   Shi, Luping
   Hu, Shi-Min
TI High-speed video generation with an event camera
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 34th Conference on Computer Graphics International (CGI)
CY JUN 27-30, 2017
CL Yokohama, JAPAN
SP Keio Univ, Fac Sci & Technol
DE Event camera; Image matching; Video enhancement; Sensor fusion
AB The event camera is a kind of visual sensor that mimics aspects of the human visual system by only recording events when the light intensity on a pixel changes. This allows for an event camera to possess high temporal resolution and makes it able to capture fast motion. However, an event camera lacks information for all pixels within a scene, especially color information. In this paper, we aim to recover a typical scene in which the foreground undergoes high-speed motion which can be approximated by a planar motion, and the background is static. We demonstrate how to use the event camera to generate high-speed videos of 2D motion augmented with foreground and background images taken from a conventional camera. We match an object obtained for a static image to frames formed by the event stream, from the event camera, based on curve saliency, and we build a parametric model of affine motion to create image sequences. In this work, we are able to restore scenes of very fast motion such as falling or rotating objects and string vibration.
C1 [Liu, Han-Chao; Hu, Shi-Min] Tsinghua Univ, Dept Comp Sci & Technol, TNList, Beijing, Peoples R China.
   [Zhang, Fang-Lue] Victoria Univ Wellington, Sch Engn & Comp Sci, Wellington, New Zealand.
   [Marshall, David; Hu, Shi-Min] Cardiff Univ, Sch Comp Sci & Informat, Cardiff, S Glam, Wales.
   [Shi, Luping] Tsinghua Univ, Dept Precis Instrument, CBICR, Beijing, Peoples R China.
C3 Tsinghua University; Victoria University Wellington; Cardiff University;
   Tsinghua University
RP Hu, SM (corresponding author), Tsinghua Univ, Dept Comp Sci & Technol, TNList, Beijing, Peoples R China.; Hu, SM (corresponding author), Cardiff Univ, Sch Comp Sci & Informat, Cardiff, S Glam, Wales.
EM shimin@tsinghua.edu.cn
RI Hu, Shi-Min/AAW-1952-2020
FU Natural Science Foundation of China [61521002]; Joint NSFC-ISF Research
   Program [61561146393]; Beijing Higher Institution Engineering Research
   Center; Tsinghua-Tencent Joint Laboratory for Internet Innovation
   Technology; Beijing Municipal Science and Technology Commission
   [Z15111000090000]; Tsinghua University [20141080934]; SuZhou-Tsinghua
   innovation leading program [2016SZ0102]
FX We thank all the anonymous reviewers for their helpful and constructive
   comments. This work was supported by the Natural Science Foundation of
   China (Project Number 61521002), the Joint NSFC-ISF Research Program
   (project number 61561146393), Research Grant of Beijing Higher
   Institution Engineering Research Center and Tsinghua-Tencent Joint
   Laboratory for Internet Innovation Technology. Luping Shi was supported
   in part by the Beijing Municipal Science and Technology Commission
   (Z15111000090000), the Study of BrainInspired Computing of Tsinghua
   University (20141080934) and SuZhou-Tsinghua innovation leading program
   2016SZ0102.
CR Agarwala A, 2004, ACM T GRAPHIC, V23, P294, DOI 10.1145/1015706.1015718
   Ancuti C, 2008, VISUAL COMPUT, V24, P709, DOI 10.1007/s00371-008-0251-y
   [Anonymous], 2014, VMV
   [Anonymous], 2007, P IEEE C COMP VIS PA
   Baker S, 2004, INT J COMPUT VISION, V56, P221, DOI 10.1023/B:VISI.0000011205.11775.fd
   Bardow P, 2016, PROC CVPR IEEE, P884, DOI 10.1109/CVPR.2016.102
   Barnes C, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531330
   Barth FG, 2012, FRONTIERS IN SENSING: FROM BIOLOGY TO ENGINEERING, P251
   Ben-Ezra M, 2004, IEEE T PATTERN ANAL, V26, P689, DOI 10.1109/TPAMI.2004.1
   Cheng MM, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778820
   Gupta Ankit., 2009, Proc. of International Conference on Computational Photography (ICCP), P1
   Irani M, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P959, DOI 10.1109/ICCV.1998.710832
   Kim H., 2014, P BRIT MACHINE VISIO, DOI DOI 10.5244/C.28.26
   Levin A, 2008, IEEE T PATTERN ANAL, V30, P228, DOI 10.1109/TPAMI.2007.1177
   Lichtsteiner P, 2008, IEEE J SOLID-ST CIRC, V43, P566, DOI 10.1109/JSSC.2007.914337
   Matsushita Y, 2005, PROC CVPR IEEE, P50
   Meister M, 1999, NEURON, V22, P435, DOI 10.1016/S0896-6273(00)80700-X
   Ni ZJ, 2015, NEURAL COMPUT, V27, P925, DOI 10.1162/NECO_a_00720
   Reinbacher C., 2016, P BRIT MACH VIS C BM
   Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720
   Tai YW, 2010, IEEE T PATTERN ANAL, V32, P1012, DOI 10.1109/TPAMI.2009.97
   Veeraraghavan A, 2016, Network Operations and Management Symposium (APNOMS), 2016 18th Asia-Pacific, P1
   Wu HY, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185561
NR 23
TC 13
Z9 14
U1 2
U2 30
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2017
VL 33
IS 6-8
BP 749
EP 759
DI 10.1007/s00371-017-1372-y
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA EX1EY
UT WOS:000402964800007
DA 2024-07-18
ER

PT J
AU Chen, J
   Jiang, S
   Destefano, Z
   Yoon, S
   Gopi, M
AF Chen, Jia
   Jiang, Shan
   Destefano, Zachary
   Yoon, Sungeui
   Gopi, M.
TI Optimally Redundant, Seek-Time Minimizing Data Layout for Interactive
   Rendering
SO VISUAL COMPUTER
LA English
DT Article
DE Data layout problem; Out-of-core rendering; Cache oblivious mesh layout;
   Redundant data layout; Walkthrough application
AB Performance of interactive graphics walkthrough systems depends on the time taken to fetch the required data from the secondary storage to main memory. It has been earlier established that a large fraction of this fetch time is spent on seeking the data on the hard disk. In order to reduce this seek time, redundant data storage has been proposed in the literature, but the redundancy factors of those layouts are prohibitively high. In this paper, we develop a cost model for the seek time of a layout. Based on this cost model, we propose an elegant algorithm that computes a redundant data layout with the redundancy factor that is within the user-specified bounds, while maximizing the performance of the system. By using a set of training access requirements and a set of validation access requirements, our proposed method is able to automatically maximize system performance with an optimal redundancy factor. Experimental results show that the interactive rendering speed of the walkthrough system was improved by a factor of 2-4 by using our data layout method when compared to existing methods with or without redundancy.
C1 [Chen, Jia; Destefano, Zachary] Univ Calif Irvine, Sch Informat & Comp Sci, Irvine, CA 92697 USA.
   [Jiang, Shan] Univ Calif Irvine, Irvine, CA USA.
   [Gopi, M.] Univ Calif Irvine, Dept Comp Sci, Comp Sci, Irvine, CA USA.
   [Yoon, Sungeui] Korea Adv Inst Sci & Technol, Daejeon, South Korea.
C3 University of California System; University of California Irvine;
   University of California System; University of California Irvine;
   University of California System; University of California Irvine; Korea
   Advanced Institute of Science & Technology (KAIST)
RP Chen, J (corresponding author), Univ Calif Irvine, Sch Informat & Comp Sci, Irvine, CA 92697 USA.
EM jiac5@uci.edu
RI Yoon, Sung-eui/C-1678-2011
CR Agrawal Nitin, 2008, P USENIX ANN TECHN C, P57
   Aliaga D., 1999, Proceedings 1999 Symposium on Interactive 3D Graphics, P199, DOI 10.1145/300523.300554
   [Anonymous], 2003, Level of detail for 3D graphics
   [Anonymous], 1988, A case for redundant arrays of inexpensive disks RAID, DOI DOI 10.1145/50202.50214
   Diaz-Gutierrez P, 2005, COMPUTER GRAPHICS INTERNATIONAL 2005, PROCEEDINGS, P115
   Domingo JS, 2014, PC MAGAZINE
   Hoppe H., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P99, DOI 10.1145/237170.237216
   Hoppe H., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P189, DOI 10.1145/258734.258843
   Jiang S., 2013, SIBGRAPI 2013
   Jiang S., 2014, CGI 2014
   Lindstrom P, 1999, IEEE T VIS COMPUT GR, V5, P98, DOI 10.1109/2945.773803
   Peng JL, 2010, COMPUT GRAPH FORUM, V29, P2029, DOI 10.1111/j.1467-8659.2010.01789.x
   RIZVI SS, 2010, P 2 INT C COMP ENG T, V7, pV7, DOI DOI 10.1109/iccet.2010.5485421
   Sajadi B., 2011, Symposium on Interactive 3D Graphics and Games, P175, DOI DOI 10.1145/1944745.1944775
   Saxena M., 2009, P USENIX HOTOS 12
   Shaffer E, 2001, IEEE VISUAL, P127, DOI 10.1109/VISUAL.2001.964503
   Silva C., 2002, IEEE VISUALIZATION C
   Varadhan G, 2002, VIS 2002: IEEE VISUALIZATION 2002, PROCEEDINGS, P69, DOI 10.1109/VISUAL.2002.1183759
   Yoon S., 2005, ACM SIGGGRAPH 2005
   Yoon SE, 2006, IEEE T VIS COMPUT GR, V12, P1213, DOI 10.1109/TVCG.2006.162
NR 20
TC 1
Z9 1
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2017
VL 33
IS 2
BP 139
EP 149
DI 10.1007/s00371-015-1165-0
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA EI2TF
UT WOS:000392340400003
DA 2024-07-18
ER

PT J
AU Yao, JC
   Yu, HM
   Hu, R
AF Yao, Jincao
   Yu, Huimin
   Hu, Roland
TI A new sparse representation-based object segmentation framework
SO VISUAL COMPUTER
LA English
DT Article
DE Shape segmentation; Sparse representation; Independent component; Pose
   alignment
ID IMAGE SEGMENTATION; FEATURE-EXTRACTION; SHAPE PRIOR; ROBUST;
   CLASSIFICATION; ALGORITHMS
AB In this paper, a novel sparse representation-based object segmentation model is proposed. The model follows from a new energy function that combines the level-set-based sparse representation and the independent component-based shape representation within a unified framework. Before the optimization of the proposed energy, a set of training shapes is firstly projected into the shape space spanned by the independent components. For an arbitrary input shape similar to some of the elements in the training set, the minimization of the energy will automatically recover a sparse shape combination according to the neighbors in the projected shape space to guide the variational image segmentation. We test our model on both public datasets and real applications, and the experimental results show the superior segmentation capabilities of the proposed model.
C1 [Yao, Jincao] Zhejiang Univ, Coll Informat Sci & Elect Engn, Signal & Informat Proc, Hangzhou, Zhejiang, Peoples R China.
   [Yu, Huimin; Hu, Roland] Zhejiang Univ, Coll Informat Sci & Elect Engn, Hangzhou, Zhejiang, Peoples R China.
   [Yu, Huimin] Zhejiang Univ, State Key Lab CAD & CG, Hangzhou, Zhejiang, Peoples R China.
C3 Zhejiang University; Zhejiang University; Zhejiang University
RP Yu, HM (corresponding author), Zhejiang Univ, Coll Informat Sci & Elect Engn, Hangzhou, Zhejiang, Peoples R China.; Yu, HM (corresponding author), Zhejiang Univ, State Key Lab CAD & CG, Hangzhou, Zhejiang, Peoples R China.
EM yaojincao@zju.edu.cn; yhm2005@zju.edu.cn; haoji_hu@zju.edu.cn
FU Natural Science Foundation of China (NSFC) [61471321, 61202400];
   National Key Basic Research Project of China (973 Program)
   [2012CB316400]
FX This work is supported by the Natural Science Foundation of China (NSFC
   No. 61471321 and No. 61202400) and National Key Basic Research Project
   of China (973 Program 2012CB316400).
CR Abiantun R, 2014, IEEE T PATTERN ANAL, V36, P2061, DOI 10.1109/TPAMI.2014.2313124
   Ahn JH, 2006, INT C PATT RECOG, P361
   Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014
   Berretti S, 2013, IEEE T INF FOREN SEC, V8, P374, DOI 10.1109/TIFS.2012.2235833
   Boscaini D, 2014, VISUAL COMPUT, V30, P1233, DOI 10.1007/s00371-014-0938-1
   Bredies K, 2008, J FOURIER ANAL APPL, V14, P813, DOI 10.1007/s00041-008-9041-1
   Chan TF, 2001, IEEE T IMAGE PROCESS, V10, P266, DOI 10.1109/83.902291
   Chen F, 2013, PROC CVPR IEEE, P1870, DOI 10.1109/CVPR.2013.244
   Chen F, 2013, IEEE T IMAGE PROCESS, V22, P992, DOI 10.1109/TIP.2012.2226044
   Chen SQ, 2012, IMAGE VISION COMPUT, V30, P1032, DOI 10.1016/j.imavis.2012.09.005
   Chien JT, 2012, IEEE T AUDIO SPEECH, V20, P302, DOI 10.1109/TASL.2011.2161080
   COMON P, 1994, SIGNAL PROCESS, V36, P287, DOI 10.1016/0165-1684(94)90029-9
   COOLEY JW, 1965, MATH COMPUT, V19, P297, DOI 10.2307/2003354
   Cremers D, 2001, IEEE WORKSHOP ON VARIATIONAL AND LEVEL SET METHODS IN COMPUTER VISION, PROCEEDINGS, P137, DOI 10.1109/VLSM.2001.938892
   Cremers D., 2005, IEEE C COMP VIS PATT, V1, P1
   Dong CH, 2013, 2013 NINTH INTERNATIONAL CONFERENCE ON NATURAL COMPUTATION (ICNC), P1377, DOI 10.1109/ICNC.2013.6818194
   DONOHO DL, 1995, IEEE T INFORM THEORY, V41, P613, DOI 10.1109/18.382009
   Gonzalez R, 2013, IEEE IMAGE PROC, P1282, DOI 10.1109/ICIP.2013.6738264
   Hyvärinen A, 1999, IEEE T NEURAL NETWOR, V10, P626, DOI 10.1109/72.761722
   Klodt M., 2011, P IEEE INT C COMP VI
   Lan RY, 2015, VISUAL COMPUT, V31, P35, DOI 10.1007/s00371-013-0902-5
   Lecumberry F, 2010, IEEE T IMAGE PROCESS, V19, P625, DOI 10.1109/TIP.2009.2038759
   Liu X, 2015, IEEE T IMAGE PROCESS, V24, P3060, DOI 10.1109/TIP.2015.2432711
   Lu Z, 2015, IEEE T IMAGE PROCESS, V24, P1261, DOI 10.1109/TIP.2015.2389619
   OSHER S, 1988, J COMPUT PHYS, V79, P12, DOI 10.1016/0021-9991(88)90002-2
   Park C, 2013, IEEE T PATTERN ANAL, V35, P669, DOI 10.1109/TPAMI.2012.163
   Roe E, 2015, VISUAL COMPUT, V31, P627, DOI 10.1007/s00371-014-0988-4
   Spratling MW, 2013, IEEE T IMAGE PROCESS, V22, P1629, DOI 10.1109/TIP.2012.2235850
   Tao ML, 2015, IEEE T GEOSCI REMOTE, V53, P2481, DOI 10.1109/TGRS.2014.2360943
   Tran TT, 2013, MACH VISION APPL, V24, P1075, DOI 10.1007/s00138-013-0504-2
   Ye JT, 2012, IEEE T IMAGE PROCESS, V21, P4735, DOI 10.1109/TIP.2012.2210724
   Yi CC, 2012, IEEE T IMAGE PROCESS, V21, P4256, DOI 10.1109/TIP.2012.2199327
   Zhang ST, 2011, LECT NOTES COMPUT SC, V6892, P451, DOI 10.1007/978-3-642-23629-7_55
   Zhang ST, 2012, MED IMAGE ANAL, V16, P265, DOI 10.1016/j.media.2011.08.004
   Zhong M, 2014, VISUAL COMPUT, V30, P751, DOI 10.1007/s00371-014-0971-0
NR 35
TC 8
Z9 8
U1 0
U2 10
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2017
VL 33
IS 2
BP 179
EP 192
DI 10.1007/s00371-015-1171-2
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA EI2TF
UT WOS:000392340400006
DA 2024-07-18
ER

PT J
AU Du, H
   Jin, XG
   Willis, PJ
AF Du, Hui
   Jin, Xiaogang
   Willis, Philip J.
TI Two-level joint local laplacian texture filtering
SO VISUAL COMPUTER
LA English
DT Article
DE Texture filtering; Structure extraction; Local Laplacian filters;
   Guidance image
AB Extracting the structure component from an image with textures is a challenging problem. This paper presents a novel structure-preserving texture-filtering approach based on the two-level local Laplacian filter. The new texture-filtering method is developed by introducing local Laplacian filters into the joint filtering. Our study shows that local Laplacian filters can also be used for texture smoothing by defining a special remapping function, which is closely related to joint bilateral filtering. This finding leads to a variant of the joint bilateral filter, which produces smooth edges while preserving color variations. Our filter shares similar advantages with the joint bilateral filter, such as being simple to implement and easy to understand. Experiments demonstrate that the new filter can produce satisfactory filtering results with the properties of texture smoothing, smooth edges, and edge shape preserving. We compare our method with the state-of-the-art methods to demonstrate its improvements, and apply this filter to a variety of image-editing applications.
C1 [Du, Hui] Zhejiang Univ Media & Commun, New Media Coll, Hangzhou 310018, Zhejiang, Peoples R China.
   [Jin, Xiaogang] Zhejiang Univ, State Key Lab CAD&CG, Hangzhou 310058, Zhejiang, Peoples R China.
   [Willis, Philip J.] Univ Bath, Dept Comp Sci, Bath BA2 7AY, Avon, England.
C3 Communication University of Zhejiang; Zhejiang University; University of
   Bath
RP Du, H (corresponding author), Zhejiang Univ Media & Commun, New Media Coll, Hangzhou 310018, Zhejiang, Peoples R China.; Jin, XG (corresponding author), Zhejiang Univ, State Key Lab CAD&CG, Hangzhou 310058, Zhejiang, Peoples R China.
EM duhui@zjicm.edu.cn; jin@cad.zju.edu.cn; P.J.Willis@bath.ac.uk
FU National Natural Science Foundation of China [61472351, 61272298]; State
   Key Lab of CAD&CG, Zhejiang University [A1510]
FX Xiaogang Jin was supported by the National Natural Science Foundation of
   China (Grant Nos. 61472351 and 61272298). Hui Du was supported by the
   Open Project Program of the State Key Lab of CAD&CG (Grant No. A1510),
   Zhejiang University.
CR [Anonymous], P IEEE INT C COMP PH
   Aubry Mathieu, 2014, ACM T GRAPH, V33
   Aujol JF, 2006, INT J COMPUT VISION, V67, P111, DOI 10.1007/s11263-006-4331-z
   BABAUD J, 1986, IEEE T PATTERN ANAL, V8, P26, DOI 10.1109/TPAMI.1986.4767749
   Bao LC, 2014, IEEE T IMAGE PROCESS, V23, P555, DOI 10.1109/TIP.2013.2291328
   Buades A, 2010, IEEE T IMAGE PROCESS, V19, P1978, DOI 10.1109/TIP.2010.2046605
   Cho H, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601188
   Criminisi A, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1857907.1857910
   Farbman Z, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360666
   Fattal R, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531328
   Gastal ESL, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964964
   He KM, 2010, LECT NOTES COMPUT SC, V6311, P1
   Karacan L, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508403
   Kass M, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778837
   Kopf J., 2007, ACM T GRAPHIC, V26
   Ma ZY, 2013, IEEE I CONF COMP VIS, P49, DOI 10.1109/ICCV.2013.13
   Paris S, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964963
   PERONA P, 1990, IEEE T PATTERN ANAL, V12, P629, DOI 10.1109/34.56205
   Petschnigg G, 2004, ACM T GRAPHIC, V23, P664, DOI 10.1145/1015706.1015777
   RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F
   Shen JB, 2007, VISUAL COMPUT, V23, P641, DOI 10.1007/s00371-007-0155-2
   Shen JB, 2014, IEEE T CYBERNETICS, V44, P1579, DOI 10.1109/TCYB.2013.2290435
   Su Z, 2013, IEEE T MULTIMEDIA, V15, P535, DOI 10.1109/TMM.2012.2237025
   Subr K., 2009, ACM T GRAPHIC, V28
   Tomasi C, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P839, DOI 10.1109/ICCV.1998.710815
   van de Weijer J, 2001, PROC CVPR IEEE, P428
   Weiss B, 2006, ACM T GRAPHIC, V25, P519, DOI 10.1145/1141911.1141918
   Xu L, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024208
   Yin WT, 2005, LECT NOTES COMPUT SC, V3752, P73
   Zhang Q, 2014, PROC CVPR IEEE, P2830, DOI 10.1109/CVPR.2014.362
   Zhang Q, 2014, LECT NOTES COMPUT SC, V8691, P815, DOI 10.1007/978-3-319-10578-9_53
   Zhu LF, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366146
NR 32
TC 16
Z9 18
U1 0
U2 9
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2016
VL 32
IS 12
BP 1537
EP 1548
DI 10.1007/s00371-015-1138-3
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA EB3NH
UT WOS:000387271200004
DA 2024-07-18
ER

PT J
AU Bouali, F
   Guettala, A
   Venturini, G
AF Bouali, Fatma
   Guettala, Abdelheq
   Venturini, Gilles
TI VizAssist: an interactive user assistant for visual data mining
SO VISUAL COMPUTER
LA English
DT Article
DE Visual data mining; User assistant; Interactive genetic algorithm; D3js;
   On-line visualization tools
ID VISUALIZATION; DESIGN; ALGORITHM; METRICS; SPACE
AB We study in this work how a user can be guided to find a relevant visualization in the context of visual data mining. We present a state of the art on the user assistance in visual and interactive methods. We propose a user assistant called VizAssist, which aims at improving the existing approaches along three directions: it uses simpler computational models of the visualizations and the visual perception guidelines, in order to facilitate the integration of new visualizations and the definition of a mapping heuristic. VizAssist allows the user to provide feedback in a visual and interactive way, with the aim of improving the data to visualization mapping. This step is performed with an interactive genetic algorithm. Finally, VizAssist aims at proposing a free on-line tool () that respects the privacy of the user data. This assistant can be viewed as a global interface between the user and some of the many visualizations that are implemented with D3js.
C1 [Bouali, Fatma] Univ Lille 2, IUT, 25-27 Rue Marechal Foch, F-59100 Roubaix, France.
   [Bouali, Fatma; Guettala, Abdelheq; Venturini, Gilles] Univ Tours, Comp Sci Lab, 64 Ave Jean Portalis, F-37200 Tours, France.
C3 Universite de Lille; Universite de Tours
RP Venturini, G (corresponding author), Univ Tours, Comp Sci Lab, 64 Ave Jean Portalis, F-37200 Tours, France.
EM fatma.bouali@univ-lille2.fr; Abdelheq.Guettala@univ-tours.fr;
   venturini@univ-tours.fr
FU FDTE research project of the Region Centre, France
FX We would like to thank Jean Debarochez, Remy Pradignac and Yann-Thomas
   Le Moigne, the students in our school of Computer Science, for their
   contribution to the development of the online version of VizAssist. We
   are also grateful to the community of D3js developers for sharing their
   work and visualization codes. VizAssist was partially funded by the FDTE
   research project of the Region Centre, France.
CR Albuquerque G., 2011, 2011 IEEE Conference on Visual Analytics Science and Technology, P13, DOI 10.1109/VAST.2011.6102437
   [Anonymous], 2001, ADAP COMP MACH LEARN
   Azzag H., 2005, PROCESSING MANAGING, P318
   Bertini E, 2011, IEEE T VIS COMPUT GR, V17, P2203, DOI 10.1109/TVCG.2011.229
   Bostock M, 2011, IEEE T VIS COMPUT GR, V17, P2301, DOI 10.1109/TVCG.2011.185
   Boudjeloud L, 2005, LECT NOTES ARTIF INT, V3518, P426
   Cancino W., 2012, IEEE P 2012 IEEE C E, P2286
   CASNER SM, 1991, ACM T GRAPHIC, V10, P111, DOI 10.1145/108360.108361
   CLEVELAND WS, 1984, J AM STAT ASSOC, V79, P531, DOI 10.2307/2288400
   Dasgupta A, 2010, IEEE T VIS COMPUT GR, V16, P1017, DOI 10.1109/TVCG.2010.184
   Dawkins Richard, 1986, e blind watchmaker
   Demiralp C, 2014, IEEE COMPUT GRAPH, V34, P10, DOI 10.1109/MCG.2014.18
   Fayyad U, 1996, AI MAG, V17, P37
   Gnanamgari S., 1981, THESIS
   Goldberg D. E., 1991, Complex Systems, V5, P139
   Gotz D, 2009, P 14 INT C INT US IN, P315, DOI [DOI 10.1145/1502650.1502695, 10.1145/1502650.15026951, DOI 10.1145/1502650.15026951]
   Grammel L, 2010, IEEE T VIS COMPUT GR, V16, P943, DOI 10.1109/TVCG.2010.164
   Guettala A. E., 2012, Proceedings of the 2012 16th International Conference on Information Visualisation (IV), P252, DOI 10.1109/IV.2012.50
   Hanrahan P., 2007, VISUAL ANAL EVERYONE, P27
   Healey C.G., 1999, 28 WORKSH ADV IM PAT, P2
   Healey CG, 2008, IEEE T VIS COMPUT GR, V14, P396, DOI 10.1109/TVCG.2007.70436
   Heer J, 2008, LECT NOTES COMPUT SC, V4950, P92, DOI 10.1007/978-3-540-70956-5_5
   Holland J.H., 1992, Adaptation in Natural and Artificial Systems, DOI DOI 10.7551/MITPRESS/1090.001.0001
   Inselberg A, 1985, VISUAL COMPUT, V1, P69, DOI 10.1007/BF01898350
   Key A., 2012, P ACM SIGMOD INT C M, P681
   Kim HS, 2000, ENG APPL ARTIF INTEL, V13, P635, DOI 10.1016/S0952-1976(00)00045-2
   Lange S., 1995, Proceedings of the International Symposium. Scientific Visualization 95, P1
   MACKINLAY J, 1986, ACM T GRAPHIC, V5, P110, DOI 10.1145/22949.22950
   Mackinlay JD, 2007, IEEE T VIS COMPUT GR, V13, P1137, DOI 10.1109/TVCG.2007.70594
   Maguire E, 2012, IEEE T VIS COMPUT GR, V18, P2603, DOI 10.1109/TVCG.2012.271
   Newman D., 1998, UCI REPOSITORY MACHI
   Pineo D, 2012, IEEE T VIS COMPUT GR, V18, P309, DOI 10.1109/TVCG.2011.52
   ROTH SF, 1994, HUMAN FACTORS IN COMPUTING SYSTEMS, CHI '94 CONFERENCE PROCEEDINGS - CELEBRATING INTERDEPENDENCE, P112, DOI 10.1145/191666.191719
   Schneidewind J, 2006, IEEE CONF VIS ANAL, P199
   Schulz HJ, 2013, IEEE T VIS COMPUT GR, V19, P2366, DOI 10.1109/TVCG.2013.120
   SENAY H, 1994, IEEE COMPUT GRAPH, V14, P36, DOI 10.1109/38.329093
   Senay H., 1992, TECH REP
   Shneiderman B, 1996, IEEE SYMPOSIUM ON VISUAL LANGUAGES, PROCEEDINGS, P336, DOI 10.1109/VL.1996.545307
   Siegel Sidney, 1988, Nonparametric statistics for the behavioral sciences
   Smith J.R., 1991, 4 INT C GENETIC ALGO, P535
   SYSWERDA G, 1989, PROCEEDINGS OF THE THIRD INTERNATIONAL CONFERENCE ON GENETIC ALGORITHMS, P2
   Tokui N., 2000, Proceedings of the 3rd international conference on generative art, V17, P215
   Dang TN, 2014, IEEE PAC VIS SYMP, P73, DOI 10.1109/PacificVis.2014.42
   van Wijk JJ, 2009, MATH VIS, P343
   Venturini G., 1987, GENETIC ALGORITHMS, P696
   Verbeke G., 1997, Linear mixed models for longitudinal data
   Voigt M., 2012, 4 INT C INFORM PROCE, P101
   Wright A. H., 1991, FDN GENETIC ALGORITH, P205, DOI DOI 10.1016/B978-0-08-050684-5.50016-1
   Yi JS, 2007, IEEE T VIS COMPUT GR, V13, P1224, DOI 10.1109/TVCG.2007.70515
NR 49
TC 28
Z9 30
U1 0
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2016
VL 32
IS 11
BP 1447
EP 1463
DI 10.1007/s00371-015-1132-9
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA EA2BS
UT WOS:000386397000008
DA 2024-07-18
ER

PT J
AU Farkas, A
   Papathomas, TV
   Silverstein, SM
   Kourtev, H
   Papayanopoulos, JF
AF Farkas, Attila
   Papathomas, Thomas V.
   Silverstein, Steven M.
   Kourtev, Hristiyan
   Papayanopoulos, John F.
TI Dynamic 3-D computer graphics for designing a diagnostic tool for
   patients with schizophrenia
SO VISUAL COMPUTER
LA English
DT Article
DE 3-D computer graphics; Visual illusion; Perception; Schizophrenia;
   Clinical diagnosis
ID PERCEPTUAL ORGANIZATION; OBJECT RECOGNITION; DEPTH INVERSION;
   HOLLOW-MASK; REVERSALS; COGNITION; ILLUSION; DEFICIT
AB We introduce a novel procedure that uses dynamic 3-D computer graphics as a diagnostic tool for assessing disease severity in schizophrenia patients, based on their reduced influence of top-down cognitive processes in interpreting bottom-up sensory input. Our procedure uses the hollow-mask illusion, in which the concave side of the mask is misperceived as convex, because familiarity with convex faces dominates sensory cues signaling a concave mask. It is known that schizophrenia patients resist this illusion and their resistance increases with illness severity. Our method uses virtual masks rendered with two competing textures: (a) realistic features that enhance the illusion; (b) random-dot visual noise that reduces the illusion. We control the relative weights of the two textures to obtain psychometric functions for controls and patients and assess illness severity. The primary novelty is the use of a rotating mask that is easy to implement on a wide variety of portable devices and avoids the use of elaborate stereoscopic devices that have been used in the past. Thus our method, which can also be used to assess the efficacy of treatments, provides clinicians the advantage to bring the test to the patient's own environment, instead of having to bring patients to the clinic.
C1 [Farkas, Attila; Papathomas, Thomas V.] Rutgers State Univ, Ctr Cognit Sci, Lab Vis Res, New Brunswick, NJ 08901 USA.
   [Papathomas, Thomas V.] Rutgers State Univ, Dept Biomed Engn, New Brunswick, NJ USA.
   [Silverstein, Steven M.] Rutgers Univ Behav HealthCare, Div Schizophrenia Res, Piscataway Township, NJ USA.
   [Silverstein, Steven M.] Rutgers Biomed & Hlth Sci, Robert Wood Johnson Med Sch Dept Psychiat, Newark, NJ USA.
   [Kourtev, Hristiyan] Rutgers State Univ, Ctr Cognit Sci, New Brunswick, NJ USA.
   [Papayanopoulos, John F.] Georgia Inst Technol, Coll Engn, Atlanta, GA 30332 USA.
C3 Rutgers University System; Rutgers University New Brunswick; Rutgers
   University System; Rutgers University New Brunswick; Rutgers University
   System; Rutgers University New Brunswick; Rutgers University Biomedical
   & Health Sciences; Rutgers University System; Rutgers University New
   Brunswick; Rutgers University Biomedical & Health Sciences; Rutgers
   University System; Rutgers University New Brunswick; University System
   of Georgia; Georgia Institute of Technology
RP Farkas, A (corresponding author), Rutgers State Univ, Ctr Cognit Sci, Lab Vis Res, New Brunswick, NJ 08901 USA.
EM ajf215@rci.rutgers.edu
FU NIMH NIH HHS [R01 MH093439] Funding Source: Medline
CR Bar M, 2006, P NATL ACAD SCI USA, V103, P449, DOI 10.1073/pnas.0507062103
   Brainard DH, 1997, SPATIAL VISION, V10, P433, DOI 10.1163/156856897X00357
   Burke JW, 2009, VISUAL COMPUT, V25, P1085, DOI 10.1007/s00371-009-0387-4
   Carter CS, 2005, SCHIZOPHRENIA BULL, V31, P810, DOI 10.1093/schbul/sbi046
   CHAPMAN J, 1966, BRIT J PSYCHIAT, V112, P225, DOI 10.1192/bjp.112.484.225
   Crosbie JH, 2006, CYBERPSYCHOL BEHAV, V9, P137, DOI 10.1089/cpb.2006.9.137
   Dima D, 2010, NEUROIMAGE, V52, P824, DOI 10.1016/j.neuroimage.2009.12.086
   Dima D, 2009, NEUROIMAGE, V46, P1180, DOI 10.1016/j.neuroimage.2009.03.033
   Doniger GM, 2002, ARCH GEN PSYCHIAT, V59, P1011, DOI 10.1001/archpsyc.59.11.1011
   Farkas AJ, 2010, INNOVATIONS IN COMPUTING SCIENCES AND SOFTWARE ENGINEERING, P495, DOI 10.1007/978-90-481-9112-3_84
   Foxe JJ, 2005, CEREB CORTEX, V15, P1914, DOI 10.1093/cercor/bhi069
   Frith C.D., 2014, The cognitive neuropsychology of schizophrenia
   GEORGESON MA, 1979, PERCEPTION, V8, P585, DOI 10.1068/p080585
   Gold JM, 2007, SCHIZOPHR RES, V94, P148, DOI 10.1016/j.schres.2007.04.023
   Gregory Richard Langton, 1970, The intelligent eye
   Hill H, 2007, PERCEPTION, V36, P199, DOI 10.1068/p5523
   Jafri R, 2014, VISUAL COMPUT, V30, P1197, DOI 10.1007/s00371-013-0886-1
   Kang KK, 2009, VISUAL COMPUT, V25, P1073, DOI 10.1007/s00371-009-0385-6
   Keane BP, 2013, J ABNORM PSYCHOL, V122, P506, DOI 10.1037/a0032110
   Kleiner M, 2007, PERCEPTION, V36, P14
   KLOPFER DS, 1991, PERCEPT PSYCHOPHYS, V49, P522, DOI 10.3758/BF03212186
   Knight RA, 2001, J ABNORM PSYCHOL, V110, P15, DOI 10.1037//0021-843X.110.1.15
   Koethe D, 2009, EUR ARCH PSY CLIN N, V259, P195, DOI 10.1007/s00406-008-0851-6
   Liarokapis F, 2009, VISUAL COMPUT, V25, P1053, DOI 10.1007/s00371-009-0390-9
   Moya S, 2013, VISUAL COMPUT, V29, P795, DOI 10.1007/s00371-013-0831-3
   NUECHTERLEIN KH, 1994, ACTA PSYCHIAT SCAND, V90, P71, DOI 10.1111/j.1600-0447.1994.tb05894.x
   Papathomas TV, 2008, SPATIAL VISION, V21, P79, DOI 10.1163/156856808782713852
   Papathomas TV, 2004, PERCEPTION, V33, P1129, DOI 10.1068/p5086
   Pelli DG, 1997, SPATIAL VISION, V10, P437, DOI 10.1163/156856897X00366
   Rabinowicz EF, 1996, J ABNORM PSYCHOL, V105, P336, DOI 10.1037/0021-843X.105.3.336
   Schenkel LS, 2005, J PSYCHIATR RES, V39, P499, DOI 10.1016/j.jpsychires.2005.01.001
   Schneider U, 2002, SCHIZOPHR RES, V53, P101, DOI 10.1016/S0920-9964(00)00172-9
   Silverstein S, 2006, SCHIZOPHR RES, V83, P41, DOI 10.1016/j.schres.2006.01.003
   Silverstein SM, 2011, SCHIZOPHRENIA BULL, V37, P681, DOI 10.1093/schbul/sbr053
   Ullman S., 1979, INTERPRETATION VISUA, P229
   VANDENENDEN A, 1989, SCIENCE, V244, P959, DOI 10.1126/science.2727687
   Vuong Q.C., 2010, MORPHDEMO
   WALLACH H, 1953, J EXP PSYCHOL, V45, P205, DOI 10.1037/h0056880
   WELLS DS, 1984, J ABNORM PSYCHOL, V93, P231, DOI 10.1037/0021-843X.93.2.231
   YELLOTT JI, 1979, PERCEPTION, V8, P135, DOI 10.1068/p080135
NR 40
TC 0
Z9 0
U1 0
U2 20
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2016
VL 32
IS 11
BP 1499
EP 1506
DI 10.1007/s00371-015-1152-5
PG 8
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA EA2BS
UT WOS:000386397000011
PM 27990037
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Zhang, L
   Ge, XY
   Tan, JQ
AF Zhang, Li
   Ge, Xianyu
   Tan, Jieqing
TI Least square geometric iterative fitting method for generalized B-spline
   curves with two different kinds of weights
SO VISUAL COMPUTER
LA English
DT Article
DE Generalized B-spline; Core function; Geometric iterative method;
   Weighted least square
ID PROGRESSIVE INTERPOLATION; SUBDIVISION SURFACES; APPROXIMATION; BASES
AB Generalized B-spline bases are generated by monotone increasing and continuous "core" functions; thus generalized B-spline curves and surfaces not only hold almost the same perfect properties which classical B-splines hold but also show more flexibility in practical applications. Geometric iterative method (also known as progressive iterative approximation method) has good adaptability and stability and is popular due to its straight geometric meaning. However, in classical geometric iterative method, the number of control points is the same as that of data points. It is not suitable when large numbers of data points need to be fitted. In order to combine the advantages of generalized B-splines with those of geometric iterative method, a fresh least square geometric iterative fitting method for generalized B-splines is given, and two different kinds of weights are also introduced. The fitting method develops a series of fitting curves by adjusting control points iteratively, and the limit curve is weighted least square fitting result to the given large data points. Detailed discussion about choosing of core functions and two kinds of weights are also given. Plentiful numerical examples are also presented to show the effectiveness of the method.
C1 [Zhang, Li; Ge, Xianyu; Tan, Jieqing] Hefei Univ Technol, Sch Math, Hefei 23009, Peoples R China.
C3 Hefei University of Technology
RP Zhang, L (corresponding author), Hefei Univ Technol, Sch Math, Hefei 23009, Peoples R China.
EM lizhang@hfut.edu.cn; shuxuegxy@126.com; jieqingtan@hfut.edu.cn
RI Tan, Jie/IVV-5250-2023
FU National Natural Science Foundation of China-Joint Foundation; Guangdong
   [U1135003]; National Natural Science Foundation of China [61100126,
   61472466]
FX This work is supported by the National Natural Science Foundation of
   China-Joint Foundation with Guangdong (Key Project) under Grant No.
   U1135003, and National Natural Science Foundation of China under Grant
   No. 61100126 and No. 61472466. The foundation is China Postdoctoral
   Science Foundation (2015M571926).
CR [陈杰 Chen Jie], 2012, [自动化学报, Acta Automatica Sinica], V38, P135
   Chen J, 2011, COMPUT AIDED DESIGN, V43, P889, DOI 10.1016/j.cad.2011.03.012
   Chen ZX, 2008, COMPUT GRAPH FORUM, V27, P1823, DOI 10.1111/j.1467-8659.2008.01328.x
   Cheng FH, 2009, J COMPUT SCI TECH-CH, V24, P39, DOI 10.1007/s11390-009-9199-2
   Cheng F, 2008, GEOMETRIC MODELING & IMAGING: MODERN TECHNIQUES AND APPLICATIONS, P27, DOI 10.1109/GMAI.2008.15
   de Boor C, 1979, ARO report 79-3, P299
   Delgado J, 2007, COMPUT AIDED GEOM D, V24, P10, DOI 10.1016/j.cagd.2006.10.001
   Deng CY, 2014, COMPUT AIDED DESIGN, V47, P32, DOI 10.1016/j.cad.2013.08.012
   Deng CY, 2012, COMPUT AIDED DESIGN, V44, P424, DOI 10.1016/j.cad.2011.12.001
   Deng CY, 2010, VISUAL COMPUT, V26, P137, DOI 10.1007/s00371-009-0393-6
   Juhász I, 2013, COMPUT AIDED GEOM D, V30, P85, DOI 10.1016/j.cagd.2012.06.007
   Lin HW, 2015, COMPUT AIDED DESIGN, V67-68, P107, DOI 10.1016/j.cad.2015.05.004
   Lin HW, 2013, SIAM J SCI COMPUT, V35, pA3052, DOI 10.1137/120888569
   Lin HW, 2011, COMPUT GRAPH-UK, V35, P967, DOI 10.1016/j.cag.2011.07.003
   Lin HW, 2010, COMPUT AIDED GEOM D, V27, P322, DOI 10.1016/j.cagd.2010.01.003
   Lin HW, 2005, COMPUT MATH APPL, V50, P575, DOI 10.1016/j.camwa.2005.01.023
   Lin HW, 2004, SCI CHINA SER F, V47, P315, DOI 10.1360/02yf0529
   Lu Lizheng, 2009, Journal of Computer Aided Design & Computer Graphics, V21, P1689
   Lu LZ, 2010, COMPUT AIDED GEOM D, V27, P129, DOI 10.1016/j.cagd.2009.11.001
   Piegl L., 1997, The Nurbs Book, Vsecond
   Piegl L., 2010, VISUAL COMPUT, V16, P386
   Qi D, 1975, Acta Math Sin, V18, P173
   [史利民 SHI Limin], 2006, [数学研究与评论, Journal of Mathematical Research and Exposition], V26, P735
   Yang XN, 2013, VISUAL COMPUT, V29, P189, DOI 10.1007/s00371-012-0715-y
   Zhang Li, 2014, Journal of Computer Aided Design & Computer Graphics, V26, P1646
   [张莉 Zhang Li], 2014, [中国图象图形学报, Journal of Image and Graphics], V19, P275
   Zhao Yu, 2011, Journal of Computer Aided Design & Computer Graphics, V23, P2013
NR 27
TC 20
Z9 28
U1 2
U2 24
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2016
VL 32
IS 9
BP 1109
EP 1120
DI 10.1007/s00371-015-1170-3
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DU4DE
UT WOS:000382161400005
DA 2024-07-18
ER

PT J
AU Papaefthymiou, M
   Hildenbrand, D
   Papagiannakis, G
AF Papaefthymiou, Margarita
   Hildenbrand, Dietmar
   Papagiannakis, George
TI An inclusive Conformal Geometric Algebra GPU animation interpolation and
   deformation algorithm
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 33rd Conference on Computer Graphics International (CGI)
CY JUN 28-JUL 01, 2016
CL Heraklion, GREECE
SP Fdn Res Technol
DE Geometric Algebra; Conformal model; Virtual Reality; Animation Blending;
   Animation; Skinning; GPU-based skinning; Virtual Character Simulation
AB In the last years, Geometric Algebra with its Euclidean, Homogeneous and Conformal models attracts the research interest in many areas of Computer Science and Engineering and particularly in Computer Graphics as it is shown that they can produce more efficient and smooth results than other algebras. In this paper, we present an all-inclusive algorithm for real-time animation interpolation and GPU-based geometric skinning of animated, deformable virtual characters using the Conformal model of Geometric Algebra (CGA). We compare our method with standard quaternions, linear algebra matrices and dual-quaternions blending and skinning algorithms and we illustrate how our CGA-GPU inclusive skinning algorithm can provide as smooth and more efficient results as state-of-the-art previous methods. Furthermore, the elements of CGA that handle transformations (CGA motors) can support translation, rotation and dilation(uniform scaling) of joints under a single, GPU-supported mathematical framework and avoid conversion between different mathematical representations in contrast to quaternions and dual-quaternions that support only rotation and rotation-translation, respectively. Hence, our main novelty is the replacement of different types of algebras, and their in-between conversions between CPU and GPU, such as linear algebra matrices, quaternions, dual-quaternions and Euler angles for animation interpolation and skinning with a single mathematical representation, the CGA motors which can optimally handle the composition of translation, rotation and scaling joint transformations and interpolations. Employing latest CGA code generators, we provide a sample implementation of our algorithm running natively in a vertex shader program on modern GPUs for typical deformable virtual character simulations.
C1 [Papaefthymiou, Margarita; Papagiannakis, George] Univ Crete, Fdn Res & Technol Hellas, Iraklion, Greece.
   [Hildenbrand, Dietmar] Hsch RheinMain, Wiesbaden, Germany.
C3 University of Crete; Foundation for Research & Technology - Hellas
   (FORTH)
RP Papagiannakis, G (corresponding author), Univ Crete, Fdn Res & Technol Hellas, Iraklion, Greece.
EM papagian@csd.uoc.gr
RI papagiannakis, george/AAI-7973-2020
OI papagiannakis, george/0000-0002-2977-9850
CR Dorst Leo, 2009, GEOMETRIC ALGEBRA CO
   Hestens D., 1984, FUNDAMENTAL THEORIES
   Hildenbrand D., 2013, Foundations of geometric algebra computing, volume 8 of Geometry and Computing, V8
   Hitzer E., 2011, SICE J CONTROL MEAS, V4
   Hitzer E, 2009, ADV APPL CLIFFORD AL, V19, P339, DOI 10.1007/s00006-009-0160-9
   Kanatani K., 2015, Understanding Geometric Algebra: Hamilton, Grassmann, and Clifford for Computer Vision and Graphics, DOI [DOI 10.1201/B18273, 10.1201/b18273]
   Kavan L, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409625.1409627
   Magnenat-Thalmann, 1988, Proceedings of Graphics Interface '88, P26
   Magnenat-Thalmann N, 2004, 2004 INTERNATIONAL CONFERENCE ON CYBERWORLDS, PROCEEDINGS, P201
   Margarita P., 2015, SIGGRAPH ASIA 2015 M
   Papagiannakis G., 2014, INT J HERITAGE DIGIT, V3, P683, DOI [10.1260/2047-4970.3.4.683, DOI 10.1260/2047-4970.3.4.683]
   Papagiannakis George, 2013, SIGGRAPH ASIA 2013 T, P1
   Perwass C, 2009, GEOM COMPUT, V4, P1
   Sommer G., 2001, Geometric computing with Clifford algebras
   Wareham R, 2005, LECT NOTES COMPUT SC, V3519, P329
   Wareham R, 2008, LECT NOTES COMPUT SC, V5098, P122, DOI 10.1007/978-3-540-70517-8_13
NR 16
TC 14
Z9 14
U1 0
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2016
VL 32
IS 6-8
BP 751
EP 759
DI 10.1007/s00371-016-1270-8
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA DP8ET
UT WOS:000378731600008
DA 2024-07-18
ER

PT J
AU Bhardwaj, A
   Raman, S
AF Bhardwaj, Adit
   Raman, Shanmuganathan
TI Robust PCA-based solution to image composition using augmented Lagrange
   multiplier (ALM)
SO VISUAL COMPUTER
LA English
DT Article
DE Robust principal component analysis; HDR imaging; Variable aperture
   photography; Dynamic scene; Deblurring; Deghosting
AB Computational photography relies on specialized image-processing techniques to combine multiple images captured by a camera to generate a desired image of the scene. We first consider the high dynamic range (HDR) imaging problem. We can change either the exposure time or the aperture while capturing multiple images of the scene to generate an HDR image. This paper addresses the HDR imaging problem for static and dynamic scenes captured using a stationary camera under various aperture and exposure settings, when we do not have any knowledge of the camera settings. We have proposed a novel framework based on sparse representation which enables us to process images while getting rid of artifacts due to moving objects and defocus blur. We show that the proposed approach is able to produce significantly good results through dynamic object rejection and deblurring capabilities. We compare the results with other competitive approaches and discuss the relative advantages of the proposed approach.
C1 [Bhardwaj, Adit; Raman, Shanmuganathan] Indian Inst Technol Gandhinagar, Ahmadabad, Gujarat, India.
C3 Indian Institute of Technology System (IIT System); Indian Institute of
   Technology (IIT) - Gandhinagar
RP Bhardwaj, A (corresponding author), Indian Inst Technol Gandhinagar, Ahmadabad, Gujarat, India.
EM adit@btech2010.iitgn.ac.in; shanmuga@iitgn.ac.in
RI Raman, Shanmuganathan/AAV-2186-2020
OI Raman, Shanmuganathan/0000-0003-2718-7891
CR [Anonymous], 2008, P GRAPHICS INTERFACE
   Bhardwaj A, 2014, EUROGRAPHICS 2014, P3
   Bhardwaj A., 2014, SPCOM 2014
   BURT PJ, 1983, IEEE T COMMUN, V31, P532, DOI 10.1109/TCOM.1983.1095851
   Candès EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395
   Candès EJ, 2010, IEEE T INFORM THEORY, V56, P2053, DOI 10.1109/TIT.2010.2044061
   Candès EJ, 2009, FOUND COMPUT MATH, V9, P717, DOI 10.1007/s10208-009-9045-5
   De la Torre F, 2003, INT J COMPUT VISION, V54, P117, DOI 10.1023/A:1023709501986
   De la Torre F, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P362, DOI 10.1109/ICCV.2001.937541
   Debevec Paul E, 1997, ACM SIGGRAPH
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Gallo O, 2009, IEEE ICCP
   Goshtasby AA, 2005, IMAGE VISION COMPUT, V23, P611, DOI 10.1016/j.imavis.2005.02.004
   Granados M, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508410
   Hasinoff S, 2007, LAYER BASED RESTORAT
   Hasinoff SW, 2010, PROC CVPR IEEE, P553, DOI 10.1109/CVPR.2010.5540167
   Hasinoff S, 2009, INT J COMPUT VISION, V81, P82, DOI 10.1007/s11263-008-0164-2
   Heo YS, 2011, LECT NOTES COMPUT SC, V6495, P486, DOI 10.1007/978-3-642-19282-1_39
   Hu J., 2013, HDR DEGHOSTING DEAL
   Jacobs K, 2008, IEEE COMPUT GRAPH, V28, P84, DOI 10.1109/MCG.2008.23
   Jin HL, 2002, LECT NOTES COMPUT SC, V2351, P18
   Kalantari NK, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508402
   Ke QF, 2005, PROC CVPR IEEE, P739
   Khan EA, 2006, IEEE IMAGE PROC, P2005, DOI 10.1109/ICIP.2006.312892
   Lee C., 2014, GHOST FREE HIGH DYNA
   Levin A, 2011, IEEE T PATTERN ANAL, V33, P2354, DOI 10.1109/TPAMI.2011.148
   Lin, 2010, ARXIV10095055
   Mann S., 1994, BEING UNDIGITAL DIGI, P442
   Mertens T., 2007, EXPOSURE FUSION
   Oh T., 2014, IEEE T PATTERN ANAL, V99, P1
   Oh T. H., 2013, IEEE ICCV
   Oh T. H., 2013, IEEE ICIP
   Peterson B., 2010, UNDERSTANDING EXPOSU, V3rd
   Rajagopalan AN, 1999, IEEE T PATTERN ANAL, V21, P577, DOI 10.1109/34.777369
   Raman S., 2007, ICCV '07: Proceedings of the 11th IEEE International Conference on Computer Vision, P1
   Raman S, 2011, VISUAL COMPUT, V27, P1099, DOI 10.1007/s00371-011-0653-0
   Reinhard E., 2010, HIGH DYNAMIC RANGE I, V2
   Sen P, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366222
   Srikantha A, 2012, SIGNAL PROCESS-IMAGE, V27, P650, DOI 10.1016/j.image.2012.02.001
   Ward G., 2003, Journal of Graphics Tools, V8, P17, DOI 10.1080/10867651.2003.10487583
   Wright J, 2009, ADV NEURAL INFORM PR, P2080, DOI DOI 10.1109/NNSP.2000.889420
   Wu SQ, 2010, IEEE IMAGE PROC, P397, DOI 10.1109/ICIP.2010.5654196
   Zhang W, 2012, IEEE T IMAGE PROCESS, V21, P2318, DOI 10.1109/TIP.2011.2170079
   Zimmer H, 2011, COMPUT GRAPH FORUM, V30, P405, DOI 10.1111/j.1467-8659.2011.01870.x
NR 44
TC 20
Z9 26
U1 0
U2 10
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2016
VL 32
IS 5
BP 591
EP 600
DI 10.1007/s00371-015-1075-1
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DK5UI
UT WOS:000374985800005
DA 2024-07-18
ER

PT J
AU Qiao, SY
   Fang, XX
   Sheng, B
   Wu, W
   Wu, EH
AF Qiao, Siyuan
   Fang, Xiaoxin
   Sheng, Bin
   Wu, Wen
   Wu, Enhua
TI Structure-aware QR Code abstraction
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 32nd Computer Graphics International CGI 15 Conference
CY JUN 24-26, 2015
CL INSA Strasbourg Univ Strasbourg, Strasbourg, FRANCE
SP CNRS, iCUBE, Univ Strasbourg, CGS, Springer, Acm Incooperation, IGG, ACMSIGGRAPH, KIST Europe, Region Alsace, Visteon
HO INSA Strasbourg Univ Strasbourg
DE QR Code abstraction; Structure-aware halftoning; Parallel computing
ID ERROR; QUALITY
AB Quick Response Codes (QR Codes) have proved to be a great success in both automotive industry and general commercial use. However, in most cases, QR Codes are not identified as aesthetical, since the original motivation to invent QR Codes is to achieve high speed and high readability in scanning. This paper proposes a solution framework for QR Code abstraction, which produces machine readable QR Code that is visually similar to the input image. Unlike the existing schemes for QR Code abstraction, which produces QR Codes of low resolution, the framework aims to resolve the high computation complexity of producing halftone QR Codes of high resolution with both similarity and readability preserved. The solution framework has been implemented and tested on various input texts and images, and a user study was conducted to evaluate its performance in preserving similarity. The experimental results show that the framework can produce QR Codes of high resolution and high similarity without compromising readability.
C1 [Qiao, Siyuan; Fang, Xiaoxin; Sheng, Bin] Shanghai Jiao Tong Univ, Dept Comp Sci & Engn, Shanghai 200030, Peoples R China.
   [Qiao, Siyuan] Shanghai Jiao Tong Univ, Dept Comp Sci & Engn, Visual Media & Data Management Lab, Shanghai 200030, Peoples R China.
   [Sheng, Bin; Wu, Enhua] Chinese Acad Sci, Inst Software, State Key Lab Comp Sci, Beijing, Peoples R China.
   [Wu, Wen; Wu, Enhua] Univ Macau, Fac Sci & Technol, Dept Comp & Informat Sci, Macau, Peoples R China.
C3 Shanghai Jiao Tong University; Shanghai Jiao Tong University; Chinese
   Academy of Sciences; Institute of Software, CAS; University of Macau
RP Sheng, B (corresponding author), Shanghai Jiao Tong Univ, Dept Comp Sci & Engn, Shanghai 200030, Peoples R China.
EM subluckey@sjtu.edu.cn; fangxx@sjtu.edu.cn; shengbin@sjtu.edu.cn;
   wenwu@umac.mo; ehwu@umac.mo
FU National Natural Science Foundation of China [61202154, 61272326,
   61133009]; National Basic Research Project of China [2011CB302203];
   National Key Technology RD Program [2012BAH55F02]; Shanghai Pujiang
   Program [13PJ1404500]; University of Macau [MYRG2014-00139-FST,
   MYRG202(Y1-L4)-FST11-WEH]; Science and Technology Commission of Shanghai
   Municipality [13511505000]; Shanghai Jiao Tong University [14JCY10];
   State Key Lab of CAD&CG, Zhejiang University [A1401]
FX The authors would like to thank all reviewers for their helpful
   suggestions and constructive comments. The work is supported by the
   National Natural Science Foundation of China (No. 61202154, 61272326,
   61133009), the National Basic Research Project of China (No.
   2011CB302203), National Key Technology R&D Program (No. 2012BAH55F02),
   Shanghai Pujiang Program (No. 13PJ1404500), the Grant of University of
   Macau under Grant No. MYRG2014-00139-FST and MYRG202(Y1-L4)-FST11-WEH,
   the Science and Technology Commission of Shanghai Municipality Program
   (No. 13511505000), the Interdisciplinary Program of Shanghai Jiao Tong
   University (No. 14JCY10), and the Open Project Program of the State Key
   Lab of CAD&CG (No. A1401), Zhejiang University.
CR ANALOUI M, 1992, P SOC PHOTO-OPT INS, V1666, P96, DOI 10.1117/12.135959
   Bayer B.E., 1973, IEEE INT C COMM, P11
   Chang JH, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618508
   Chu HK, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508408
   Cox R., 2012, QArt Codes
   ESCHBACH R, 1991, J OPT SOC AM A, V8, P1844, DOI 10.1364/JOSAA.8.001844
   Floyd R.W., 1974, SID INT SYMP DIG, P36
   Garateguy GJ, 2014, IEEE T IMAGE PROCESS, V23, P2842, DOI 10.1109/TIP.2014.2321501
   GEIST R, 1993, ACM T GRAPHIC, V12, P136, DOI 10.1145/151280.151281
   Hara M., 1998, US Patent, Patent No. 5726435
   Hwang BW, 2004, LECT NOTES COMPUT SC, V3029, P473
   KNUTH DE, 1987, ACM T GRAPHIC, V6, P245, DOI 10.1145/35039.35040
   Kwak NJ, 2006, 2006 International Conference on Hybrid Information Technology, Vol 1, Proceedings, P499
   Lee C, 2010, IEEE T IMAGE PROCESS, V19, P435, DOI 10.1109/TIP.2009.2032941
   Lee HS, 2010, IEEE IMAGE PROC, P525, DOI 10.1109/ICIP.2010.5651243
   Li H, 2010, COMPUT GRAPH FORUM, V29, P273, DOI 10.1111/j.1467-8659.2009.01596.x
   Neuhoff D. L., 1992, ICASSP-92: 1992 IEEE International Conference on Acoustics, Speech and Signal Processing (Cat. No.92CH3103-9), P189, DOI 10.1109/ICASSP.1992.226219
   Ono S, 2008, IEEE C EVOL COMPUTAT, P1068, DOI 10.1109/CEC.2008.4630929
   Ostromoukhov V, 2001, COMP GRAPH, P567, DOI 10.1145/383259.383326
   Pang WM, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360688
   Pappas TN, 2003, IEEE SIGNAL PROC MAG, V20, P14, DOI 10.1109/MSP.2003.1215228
   Pelet U., 2012, VISUALEAD
   Samretwit D., 2011, Proceedings of the 2011 Third International Conference on Intelligent Networking and Collaborative Systems (INCoS 2011), P552, DOI 10.1109/INCoS.2011.117
   ULICHNEY RA, 1988, P IEEE, V76, P56, DOI 10.1109/5.3288
   Wang SD, 2010, VISUAL COMPUT, V26, P445, DOI 10.1007/s00371-010-0436-z
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Zang Y, 2014, VISUAL COMPUT, V30, P969, DOI 10.1007/s00371-013-0881-6
   Zhou BF, 2003, ACM T GRAPHIC, V22, P437, DOI 10.1145/882262.882289
NR 28
TC 8
Z9 14
U1 1
U2 20
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2015
VL 31
IS 6-8
BP 1123
EP 1133
DI 10.1007/s00371-015-1107-x
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA CM2CN
UT WOS:000357487500036
DA 2024-07-18
ER

PT J
AU Ren, WY
   Li, GH
   Sun, BL
   Huang, KH
AF Ren, Weiya
   Li, Guohui
   Sun, Boliang
   Huang, Kuihua
TI Unsupervised kernel learning for abnormal events detection
SO VISUAL COMPUTER
LA English
DT Article
DE Kernel learning; One-class learning; Anomaly detection; Non-negative
   matrix factorization; Support vector data description
ID HISTOGRAMS; FLOW
AB In this paper, we propose a method to detect abnormal events using a novel unsupervised kernel learning algorithm. The key of our method is to learn a suitable feature space and the associated kernel function of the training samples. By considering the self-similarity property of training samples, we assume that the training samples will show the distinctly clustering property in the obtained feature space. Non-negative matrix factorization (NMF) is used to learn the feature space, and the support vector data description (SVDD) method is adopted to measure the clustering degree of instances in the feature space. We append the clustering constraints in the process of learning the feature space and use the bases produced by NMF as the projection matrix to construct the kernel function in SVDD. In other words, we incorporate the minimal enclosing sphere constraints within the NMF formulation. In the process of feature space learning, instances in the obtained feature space will be described better and better by an hypersphere. Our algorithm converges to a local optimal solution by applying an alternating optimization approach. Experimental results on three public datasets and the comparison to the state-of-the-art methods show that our method is effective in detecting and locating unknown abnormal behaviors.
C1 [Ren, Weiya; Li, Guohui; Sun, Boliang; Huang, Kuihua] Natl Univ Def Technol, Coll Informat Syst & Management, Changsha 410072, Hunan, Peoples R China.
C3 National University of Defense Technology - China
RP Ren, WY (corresponding author), Natl Univ Def Technol, Coll Informat Syst & Management, Changsha 410072, Hunan, Peoples R China.
EM weiyren.phd@gmail.com; guohli@nudt.edu.cn; sumboliang@nudt.edu.cn;
   khhuang.nudt@gmail.com
FU College of Information System and Management, National University of
   Defense Technology; National Natural Science Foundation of China
   [61170158]
FX This paper is supported by College of Information System and Management,
   National University of Defense Technology and subsidized by National
   Natural Science Foundation of China(Grant No. 61170158).
CR [Anonymous], 2004, Analyzing microarray gene expression data
   [Anonymous], 2004, SPRINGER TEXTS STAT
   Buciu I, 2004, MACHINE LEARN SIGN P, P539
   Chapelle O., 2005, Proceedings of the tenth international workshop on artificial intelligence and statistics, P57
   Cong Y, 2011, PROC CVPR IEEE, P1807, DOI 10.1109/CVPR.2011.5995434
   Cong Y, 2009, PROC CVPR IEEE, P1093, DOI 10.1109/CVPRW.2009.5206648
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Ding C, 2010, IEEE T PATTERN ANAL, V32, P45, DOI 10.1109/TPAMI.2008.277
   Fan H., 2006, P PAC AS C KNOWL DIS
   Han J., DATA MINING CONCEPTS, P451
   Jaechul Kim, 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2921, DOI 10.1109/CVPRW.2009.5206569
   Jin W., 2006, P PAC AS C KNOWL DIS
   Kratz L, 2009, PROC CVPR IEEE, P1446, DOI 10.1109/CVPRW.2009.5206771
   Kriegel H.-P., 2008, P ACM SIGKDD INT C K
   Kumar BGV, 2012, IMAGE VISION COMPUT, V30, P279, DOI 10.1016/j.imavis.2012.02.010
   Lampert CH, 2008, FOUND TRENDS COMPUT, V4, P193, DOI 10.1561/0600000027
   Lee D.D., 2001, NIPS, V13, P629
   Lee DD, 1999, NATURE, V401, P788, DOI 10.1038/44565
   MAHADEVAN V, 2010, PROC CVPR IEEE, P1975, DOI DOI 10.1109/CVPR.2010.5539872
   Mehran Ramin, 2009, CVPR
   Pascual-Montano A, 2006, IEEE T PATTERN ANAL, V28, P403, DOI 10.1109/TPAMI.2006.60
   Pers J, 2010, PATTERN RECOGN LETT, V31, P1369, DOI 10.1016/j.patrec.2010.03.024
   Ritter G, 1997, PATTERN RECOGN LETT, V18, P525, DOI 10.1016/S0167-8655(97)00049-4
   Scholkopf B., 2001, LEARNING KERNELS SUP
   Singh K., 2012, Int. J. Comput. Sci. Issues, V9, P3
   Tax DMJ, 2004, MACH LEARN, V54, P45, DOI 10.1023/B:MACH.0000008084.60811.49
   Thurau C., 2008, CVPR, P1, DOI DOI 10.1109/CVPR.2008.4587721
   Tropp J., 2003, Literature survey: Nonnegative matrix factorization
   Wu SD, 2010, PROC CVPR IEEE, P2054, DOI 10.1109/CVPR.2010.5539882
   Yuan JS, 2009, PROC CVPR IEEE, P2442, DOI [10.1109/CVPRW.2009.5206671, 10.1109/CVPR.2009.5206671]
   Zhan BB, 2008, MACH VISION APPL, V19, P345, DOI 10.1007/s00138-008-0132-4
   Zhang DQ, 2006, LECT NOTES ARTIF INT, V4099, P404, DOI 10.1007/978-3-540-36668-3_44
NR 32
TC 13
Z9 14
U1 2
U2 21
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2015
VL 31
IS 3
BP 245
EP 255
DI 10.1007/s00371-013-0915-0
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CC3DB
UT WOS:000350223900001
DA 2024-07-18
ER

PT J
AU Lai, YC
   Chou, HT
   Chen, KW
   Fan, SH
AF Lai, Yu-Chi
   Chou, Hsuan-Ting
   Chen, Kuo-Wei
   Fan, Shaohua
TI Robust and efficient adaptive direct lighting estimation
SO VISUAL COMPUTER
LA English
DT Article
DE Ray-tracing; PMC; Monte Carlo; Global illumination
AB Hemispherical integrals are important for the estimation of direct lighting which has a major impact on the results of global illumination. This work proposes the population Monte Carlo hemispherical integral (PMC-HI) sampler to improve the efficiency of direct lighting estimation. The sampler is unbiased and derived from the population Monte Carlo framework which works on a population of samples and learns to be a better sampling function over iterations. Information found in one iteration can be used to guide subsequent iterations by distributing more samples to important sampling techniques to focus more efforts on the sampling sub-domains which have larger contributions to the hemispherical integrals. In addition, a cone sampling strategy is also proposed to enhance the success rate when complex occlusions exist. The images rendered with PMC-HI are compared against those rendered with multiple importance sampling (Veach and Guibas In: SIGGRAPH '95, pp 419-428, 1995), adaptive light sample distributions (Donikian et al. IEEE Trans Vis Comput Graph 12(3):353-364, 2006), and multidimensional hemispherical adaptive sampling (Hachisuka et al. ACM Trans Graph 27(3):33:1-33:10, 2008). Our PMC-HI sampler can improve rendering efficiency.
C1 [Lai, Yu-Chi] Natl Taiwan Univ Sci & Technol, Taipei, Taiwan.
   [Chou, Hsuan-Ting; Chen, Kuo-Wei] Natl Taiwan Univ Sci & Technol, Dept Comp Sci & Informat Engn, Taipei, Taiwan.
   [Fan, Shaohua] Suzhou Univ, China & NYU Courant Inst Math Sci, Suzhou 215006, Peoples R China.
C3 National Taiwan University of Science & Technology; National Taiwan
   University of Science & Technology
RP Fan, SH (corresponding author), Suzhou Univ, China & NYU Courant Inst Math Sci, Suzhou 215006, Peoples R China.
EM yu-chi@mail.ntust.edu.tw; terry60103@gmail.com; chen51202@gmail.com;
   shaohua@cs.wisc.edu
OI Lai, Yu-Chi/0000-0001-8578-3101
FU NSC, Taiwan [99-2218-E-011-005-]
FX Funded by: NSC 99-2218-E-011-005-, Taiwan.
CR Burke David., 2005, PROC EGSR 2005, P147
   Cappé O, 2004, J COMPUT GRAPH STAT, V13, P907, DOI 10.1198/106186004X12803
   Clarberg P, 2005, ACM T GRAPHIC, V24, P1166, DOI 10.1145/1073204.1073328
   Donikian M, 2006, IEEE T VIS COMPUT GR, V12, P353, DOI 10.1109/TVCG.2006.41
   Dutre P, 1995, SPRING COMP SCI, P306
   DUTRE P., 1994, Eurographics Workshop on Rendering, P185
   Fan S, 2006, THESIS U WISCONSIN M
   GHOSH A., 2006, Eurographics Symposium on Rendering, P115
   Hachisuka T, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360632
   Lafortune EP, 1995, SPRING COMP SCI, P11
   Lai Y.-C., 2010, TR1614 U WISC
   Lai YC, 2010, VISUAL COMPUT, V26, P543, DOI 10.1007/s00371-010-0503-5
   Ohbuchi R., 1996, RT0167 IBM TOK RES L
   Pharr M., 2004, Physically Based Rendering: From Theory to Implementation
   Pietrek G., 1999, 7th International Conference in Central Europe on Computer Graphics, Visualization and Interactive Digital Media'99. in co-operation with EUROGRAPHICS and IFIP WG 5.10. WSCG'99. Conference Proceedings, P217
   Robert Christian P., 2004, Springer Texts in Statistics, Vsecond
   Sameer Agarwal, 2003, ACM Transactions on Graphics, V22, P605, DOI 10.1145/882262.882314
   TALBOT JF, 2005, P 16 EUR C REND TECH, P139
   Veach E., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P419, DOI 10.1145/218380.218498
   Veach E., 1998, ROBUST MONTE CARLO M
   Ward Gregory, 1991, P 2 EUR WORKSH REND, P11
NR 21
TC 5
Z9 5
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2015
VL 31
IS 1
BP 83
EP 91
DI 10.1007/s00371-013-0908-z
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AY8ZQ
UT WOS:000347839600007
DA 2024-07-18
ER

PT J
AU Boscaini, D
   Castellani, U
AF Boscaini, Davide
   Castellani, Umberto
TI A sparse coding approach for local-to-global 3D shape description
SO VISUAL COMPUTER
LA English
DT Article
DE 3D object retrieval; Sparse coding; Bag of words; Partial shape matching
ID OBJECT RECOGNITION
AB The definition of reliable shape descriptors is an essential topic for 3D object retrieval. In general, two main approaches are considered: global, and local. Global approaches are effective in describing the whole object, while local ones are more suitable to characterize small parts of the shape. Recently some strategies to combine these two approaches have been proposed which are mainly concentrated to the so-called bag of words paradigm. With this paper we address this problem and propose an alternative strategy that goes beyond the bag of word approach. In particular, a sparse coding technique is exploited for the 3D domain: a set of local shape descriptors are collected from the shape, and then a dictionary is trained as generative model. In this fashion the dictionary is used as global shape descriptor for shape retrieval purposes. Several experiments are performed on standard databases in order to evaluate the proposed method in challenging situations like the case of 'SHREC 2011: robustness benchmark' where strong shape transformations are included, and the case of 'SHREC 2007: partial matching track' where composite models are considered in the query phase. A drastic improvement of the proposed method is observed by showing that sparse coding approach is particularly suitable for local-to-global description and outperforms other approaches such as the bag of words.
C1 [Boscaini, Davide] Univ Lugano, Lugano, Switzerland.
   [Castellani, Umberto] Univ Verona, Dept Comp Sci, I-37100 Verona, Italy.
   [Castellani, Umberto] LASMEA Lab, Clermont Ferrand, France.
C3 Universita della Svizzera Italiana; University of Verona
RP Castellani, U (corresponding author), Univ Verona, Dept Comp Sci, I-37100 Verona, Italy.
EM davide.boscaini@usi.ch; umberto.castellani@univr.it
OI Boscaini, Davide/0000-0003-4887-2038
CR [Anonymous], 2011, EUR WORKSH 3D OBJ RE
   [Anonymous], UUCS2007015 UTR U DE
   [Anonymous], 2011, PROC EUROGRAPHICS 20, P79, DOI DOI 10.2312/3DOR/3DOR11/079-088
   Aubry M., 2011, P ICCV WORKSH DYN SH
   Belongie S, 2002, IEEE T PATTERN ANAL, V24, P509, DOI 10.1109/34.993558
   BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791
   Boscaini D., 2013, EUR WORKSH 3D OBJ RE
   Bronstein AM, 2008, MONOGR COMPUT SCI, P1, DOI 10.1007/978-0-387-73301-2_1
   Bronstein AM, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1899404.1899405
   Bronstein MM, 2010, PROC CVPR IEEE, P1704, DOI 10.1109/CVPR.2010.5539838
   Castellani U., 2012, 3D SHAPE REGISTRATIO
   Castellani U, 2011, LECT NOTES COMPUT SC, V6892, P426, DOI 10.1007/978-3-642-23629-7_52
   Castellani U, 2011, IEEE T PATTERN ANAL, V33, P2555, DOI 10.1109/TPAMI.2011.85
   Darom T, 2012, IEEE T IMAGE PROCESS, V21, P2758, DOI 10.1109/TIP.2012.2183142
   Elad A, 2003, IEEE T PATTERN ANAL, V25, P1285, DOI 10.1109/TPAMI.2003.1233902
   Funkhouser T, 2005, COMMUN ACM, V48, P58, DOI 10.1145/1064830.1064859
   Johnson AE, 1999, IEEE T PATTERN ANAL, V21, P433, DOI 10.1109/34.765655
   Lavoue G., 2012, VISUAL COMPUT, V26, P1257
   Levy B, 2006, IEEE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS 2006, PROCEEDINGS, P66
   Lian Z., 2010, P EUR WORKSH 3D OBJ
   Lian ZH, 2013, PATTERN RECOGN, V46, P449, DOI 10.1016/j.patcog.2012.07.014
   Litman R, 2014, IEEE T PATTERN ANAL, V36, P171, DOI 10.1109/TPAMI.2013.148
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Mairal J., 2009, P 26 ANN INT C MACHI, P689, DOI 10.1145/1553374.1553463
   Mairal J, 2010, J MACH LEARN RES, V11, P19
   Marini S., 2007, 1007 IMATI
   Mitra N J., 2006, Symposium on Geometry Processing, P121
   Ovsjanikov M, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185526
   Peters J, 2017, ADAPT COMPUT MACH LE
   Pokrass J, 2013, COMPUT GRAPH FORUM, V32, P459, DOI 10.1111/cgf.12066
   Reuter M, 2006, COMPUT AIDED DESIGN, V38, P342, DOI 10.1016/j.cad.2005.10.011
   Rustamov Raif M, 2007, P S GEOM PROC, V257, P225
   Sumner RW, 2004, ACM T GRAPHIC, V23, P399, DOI 10.1145/1015706.1015736
   Sun JA, 2009, COMPUT GRAPH FORUM, V28, P1383, DOI 10.1111/j.1467-8659.2009.01515.x
   Tangelder JWH, 2004, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, P145, DOI 10.1109/SMI.2004.1314502
   Tibshirani R, 1996, J ROY STAT SOC B, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x
   Toldo R, 2010, VISUAL COMPUT, V26, P1257, DOI 10.1007/s00371-010-0519-x
   Wuhrer S, 2010, PROC CVPR IEEE, P374, DOI 10.1109/CVPR.2010.5540188
NR 38
TC 4
Z9 4
U1 0
U2 10
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2014
VL 30
IS 11
BP 1233
EP 1245
DI 10.1007/s00371-014-0938-1
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AS3HX
UT WOS:000344169300004
DA 2024-07-18
ER

PT J
AU Lee, DY
   Sull, S
   Kim, CS
AF Lee, Dae-Youn
   Sull, Sanghoon
   Kim, Chang-Su
TI Progressive 3D mesh compression using MOG-based Bayesian entropy coding
   and gradual prediction
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Computational Visual Media Conference
CY 2013
CL Geneva, SWITZERLAND
DE Triangular mesh; 3D mesh compression; Predictive coding; Progressive
   coding; Mixture of Gaussian model; Context-based arithmetic coding
AB A progressive 3D triangular mesh compression algorithm built on the MOG-based Bayesian entropy coding and the gradual prediction scheme is proposed in this work. For connectivity coding, we employ MOG models to estimate the posterior probabilities of topology symbols given vertex geometries. Then, we encode the topology symbols using an arithmetic coder with different contexts, which depend on the posterior probabilities. For geometry coding, we propose the gradual prediction labeling and the dual-ring prediction to divide vertices into groups and predict later groups more efficiently using the information in already encoded groups. Simulation results demonstrate that the proposed algorithm provides significantly better performance than the conventional wavemesh coder, with the average bit rate reduction of about 16.9 %.
C1 [Lee, Dae-Youn; Sull, Sanghoon; Kim, Chang-Su] Korea Univ, Sch Elect Engn, Seoul, South Korea.
C3 Korea University
RP Kim, CS (corresponding author), Korea Univ, Sch Elect Engn, Seoul, South Korea.
EM inomi@korea.ac.kr; sull@mpeg.korea.ac.kr; changsukim@korea.ac.kr
OI Kim, Chang-Su/0000-0002-4276-1831
FU National Research Foundation of Korea (NRF) - Korea government (MEST)
   [2012-011031]; Global Frontier R&D Program on Human-centered Interaction
   for Coexistence - NRF grant by the Korean Government (MEST)
   [NRF-M1AXA003-2011-0031648]
FX This work was supported partly by the National Research Foundation of
   Korea (NRF) grant funded by the Korea government (MEST) (No.
   2012-011031), and partly by the Global Frontier R&D Program on
   Human-centered Interaction for Coexistence, funded by the NRF grant by
   the Korean Government (MEST) (NRF-M1AXA003-2011-0031648).
CR Ahn JK, 2011, VISUAL COMPUT, V27, P769, DOI 10.1007/s00371-011-0565-z
   Ahn JK, 2010, IEEE IMAGE PROC, P3417, DOI 10.1109/ICIP.2010.5653130
   Alliez P, 2001, COMP GRAPH, P195, DOI 10.1145/383259.383281
   Alliez P, 2001, COMPUT GRAPH FORUM, V20, pC480, DOI 10.1111/1467-8659.00541
   Aspert N, 2002, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL I AND II, PROCEEDINGS, P705, DOI 10.1109/ICME.2002.1035879
   CLEARY JG, 1984, IEEE T COMMUN, V32, P396, DOI 10.1109/TCOM.1984.1096090
   Courbet C, 2011, COMPUT GRAPH FORUM, V30, P139, DOI 10.1111/j.1467-8659.2010.01838.x
   Deering M., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P13, DOI 10.1145/218380.218391
   Gandoin PM, 2002, ACM T GRAPHIC, V21, P372, DOI 10.1145/566570.566591
   Hoppe H., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P99, DOI 10.1145/237170.237216
   Karni Z, 2000, COMP GRAPH, P279, DOI 10.1145/344779.344924
   Kim J, 2011, COMPUT GRAPH-UK, V35, P713, DOI 10.1016/j.cag.2011.03.023
   Lee D.Y., 2011, P ICIP, P921
   Lee H., 2009, P VIS MOD VIS WORKSH, P73
   Lee H, 2012, VISUAL COMPUT, V28, P137, DOI 10.1007/s00371-011-0602-y
   Moffat A, 1998, ACM T INFORM SYST, V16, P256, DOI 10.1145/290159.290162
   Pajarola R, 2000, IEEE T VIS COMPUT GR, V6, P79, DOI 10.1109/2945.841122
   Peng JL, 2010, COMPUT GRAPH FORUM, V29, P2029, DOI 10.1111/j.1467-8659.2010.01789.x
   Peng JL, 2005, ACM T GRAPHIC, V24, P609, DOI 10.1145/1073204.1073237
   Rossignac J, 1999, IEEE T VIS COMPUT GR, V5, P47, DOI 10.1109/2945.764870
   Schindler M.A., 1998, P DAT COMPR C
   Stauffer C., 1999, Proceedings. 1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No PR00149), P246, DOI 10.1109/CVPR.1999.784637
   Taubin G, 1998, ACM T GRAPHIC, V17, P84, DOI 10.1145/274363.274365
   Touma C, 1998, GRAPHICS INTERFACE '98 - PROCEEDINGS, P26
   Valette S, 2004, IEEE T VIS COMPUT GR, V10, P113, DOI 10.1109/TVCG.2004.1260763
   Valette S, 2004, IEEE T VIS COMPUT GR, V10, P123, DOI 10.1109/TVCG.2004.1260764
   Valette S, 2003, IEEE IMAGE PROC, P777
   Valette S, 2009, COMPUT GRAPH FORUM, V28, P1301, DOI 10.1111/j.1467-8659.2009.01507.x
NR 28
TC 7
Z9 8
U1 0
U2 14
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2014
VL 30
IS 10
BP 1077
EP 1091
DI 10.1007/s00371-013-0779-3
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA AP6NF
UT WOS:000342193800003
DA 2024-07-18
ER

PT J
AU Santamaría-Ibirika, A
   Cantero, X
   Salazar, M
   Devesa, J
   Santos, I
   Huerta, S
   Bringas, PG
AF Santamaria-Ibirika, Aitor
   Cantero, Xabier
   Salazar, Mikel
   Devesa, Jaime
   Santos, Igor
   Huerta, Sergio
   Bringas, Pablo G.
TI Procedural approach to volumetric terrain generation
SO VISUAL COMPUTER
LA English
DT Article
DE Procedural generation; Virtual worlds; Volumetric terrain; Terrain
   modeling
AB The recent outbreak of indie games has popularized volumetric terrains to a new level, although video games have used them for decades. These terrains contain geological data, such as materials or cave systems. To improve the exploration experience and due to the large amount of data needed to construct volumetric terrains, industry uses procedural methods to generate them. However, they use their own methods, which are focused on their specific problem domains, lacking customization features. Besides, the evaluation of the procedural terrain generators remains an open issue in this field since no standard metrics have been established yet. In this paper, we propose a new approach to procedural volumetric terrains. It generates completely customizable volumetric terrains with layered materials and other features (e.g., mineral veins, underground caves, material mixtures and underground material flow). The method allows the designer to specify the characteristics of the terrain using intuitive parameters. Additionally, it uses a specific representation for the terrain based on stacked material structures, reducing memory requirements. To overcome the problem in the evaluation of the generators, we propose a new set of metrics for the generated content.
C1 [Santamaria-Ibirika, Aitor; Cantero, Xabier; Salazar, Mikel; Devesa, Jaime; Santos, Igor; Huerta, Sergio; Bringas, Pablo G.] Univ Deusto, Bilbao 48014, Bizkaia, Spain.
C3 University of Deusto
RP Santamaría-Ibirika, A (corresponding author), Univ Deusto, Avda Univ 24, Bilbao 48014, Bizkaia, Spain.
EM a.santamaria@deusto.es; xabier.cantero@deusto.es;
   mikel.salazar@deusto.es; jaime.devesa@deusto.es; isantos@deusto.es;
   shuerta@deusto.es; pablo.garcia.bringas@deusto.es
RI Santos, Igor/ABH-5429-2020; Santos, Igor/AAJ-9037-2021; Garcia Bringas,
   Pablo/M-1399-2015
OI Santos, Igor/0000-0002-9511-8612; Garcia Bringas,
   Pablo/0000-0003-3594-9534
CR [Anonymous], 2010, Workshop on Procedural Content Gen. in Games
   [Anonymous], Proceedings of the 4th International Conference on Foundations of Digital Games. FDG'09. 2009, DOI DOI 10.1145/1536513.1536532
   Benes B, 2001, SPRING CONFERENCE ON COMPUTER GRAPHICS, PROCEEDINGS, P80, DOI 10.1109/SCCG.2001.945341
   Doran J, 2010, IEEE T COMP INTEL AI, V2, P111, DOI 10.1109/TCIAIG.2010.2049020
   FOURNIER A, 1982, COMMUN ACM, V25, P371, DOI 10.1145/358523.358553
   Frade M, 2010, LECT NOTES COMPUT SC, V6024, P90, DOI 10.1007/978-3-642-12239-2_10
   Gain J., 2009, P 2009 S INT 3D GRAP, V1, P31, DOI [10.1145/1507149.1507155, DOI 10.1145/1507149.1507155]
   Hnaidi H, 2010, COMPUT GRAPH FORUM, V29, P2179, DOI 10.1111/j.1467-8659.2010.01806.x
   Kamal KR, 2007, GRAPHITE 2007: 5TH INTERNATIONAL CONFERENCE ON COMPUTER GRAPHICS AND INTERACTIVE TECHNIQUES IN AUSTRALASIA AND SOUTHERN ASIA, PROCEEDINGS, P17
   Miller G. S. P., 1986, Computer Graphics, V20, P39, DOI 10.1145/15886.15890
   Musgrave F. K., 1989, Computer Graphics, V23, P41, DOI 10.1145/74334.74337
   Musgrave F. K.:, 1993, THESIS YALE U
   Natali M., 2013, Eurographics (stars), P155
   Olsen J., 2004, REALTIME PROCEDURAL
   Ong TJ, 2005, GECCO 2005: Genetic and Evolutionary Computation Conference, Vols 1 and 2, P1463
   Perlin K., 1985, Computer Graphics, V19, P287, DOI 10.1145/325165.325247
   Santamaría-Ibirika A, 2013, 2013 INTERNATIONAL CONFERENCE ON CYBERWORLDS (CW), P20, DOI 10.1109/CW.2013.31
   Saunders R. L, 2006, THESIS TEXAS A M U
   Shaker N., 2010, Proceedings of AIIDE, P63
   Togelius J, 2007, 2007 IEEE SYMPOSIUM ON COMPUTATIONAL INTELLIGENCE AND GAMES, P252, DOI 10.1109/CIG.2007.368106
   Voss R.F., 1985, NATO ASI Series, V17, DOI [DOI 10.1103/PHYSREVE.93.042213, 10.1007/978-3-642-84574-1_34, DOI 10.1007/978-3-642-84574-1_34]
   Zhou H, 2007, IEEE T VIS COMPUT GR, V13, P834, DOI 10.1109/TVCG.2007.1027
NR 22
TC 4
Z9 6
U1 1
U2 14
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2014
VL 30
IS 9
BP 997
EP 1007
DI 10.1007/s00371-013-0909-y
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AO5GP
UT WOS:000341372200004
DA 2024-07-18
ER

PT J
AU Chang, HH
   Lai, YC
   Yao, CY
   Hua, KL
   Niu, YZ
   Liu, F
AF Chang, Hsu-Huai
   Lai, Yu-Chi
   Yao, Chin-Yuan
   Hua, Kai-Lung
   Niu, Yuzhen
   Liu, Feng
TI Geometry-shader-based real-time voxelization and applications
SO VISUAL COMPUTER
LA English
DT Article
DE Voxelization; Hardware voxelization; Transparent shadow; Volume
   ray-tracing
AB This work proposes a new voxelization algorithm based on newly available GPU functionalities and designs several real-time applications to render complex lighting effects with the voxelization result. The voxelization algorithm can efficiently transform a highly complex scene in a surface-boundary representation into a set of voxels in one GPU pass using the geometry shader. Newly available 3D textures are used to directly record the surficial and volumetric properties of objects such as opaqueness, refraction, and transmittance. In the first, the usage of 3D textures can remove those strenuous efforts required to modify the encoding and decoding scheme when adjusting the voxel resolution. Second, surficial and volumetric properties recorded in 3D textures can be used to interactively compute and render more realistic lighting effects including the shadow of objects with complex occlusion and the refraction and transmittance of transparent objects. The shadow can be rendered with an absorption coefficient which is computed according to the number of surfaces drawing in each voxel during voxelization and used to compute the amount of light passing through partially occluded complex objects. The surface normal, transmittance coefficient and refraction index recorded in each voxel can be used to simulate the refraction and transmittance lighting effects of transparent objects using our multiple-surfaced refraction algorithm. Finally, the results demonstrate that our algorithm can transform a dynamic scene into a set of voxels and render complex lighting effects in real time without any pre-processing.
C1 [Chang, Hsu-Huai; Lai, Yu-Chi; Yao, Chin-Yuan; Hua, Kai-Lung] Natl Taiwan Univ Sci & Technol, Taipei, Taiwan.
   [Niu, Yuzhen] Fuzhou Univ, Coll Math & Comp Sci, Fuzhou 350002, Peoples R China.
   [Liu, Feng] Portland State Univ, Portland, OR 97207 USA.
C3 National Taiwan University of Science & Technology; Fuzhou University;
   Portland State University
RP Lai, YC (corresponding author), Natl Taiwan Univ Sci & Technol, Taipei, Taiwan.
EM td458193@hotmail.com; cheeryuchi@gmail.com; cyuan.yao@gmail.com;
   hua@mail.ntust.edu.tw; yuzhen@gmail.com; fliu@cecs.pdx.edu
OI Hua, Kai-Lung/0000-0002-7735-243X
FU National Science Council, Taiwan [NSC 101-2221-E-011-151,
   101-2221-E-011-153]
FX This work is supported in part by National Science Council (NSC
   101-2221-E-011-151, and 101-2221-E-011-153), Taiwan.
CR Adelson S.J., 2003, THESIS
   [Anonymous], 1998, J GRAPHICS TOOLS
   [Anonymous], 2011, ACM T GRAPH
   [Anonymous], 2008, TECH REP
   Diefenbach P. J., 1997, Proceedings 1997 Symposium on Interactive 3D Graphics, P59, DOI 10.1145/253284.253308
   Diefenbach P.J., 1996, THESIS
   Dong Z, 2004, 12TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P43
   Eisemann Elmar, 2006, P 2006 S INTERACTIVE, P71, DOI DOI 10.1145/1111411.1111424
   Eisemann Elmar., 2008, Proceedings of Graphics Interface, P73
   Everitt Cass, 2001, TECH REP
   Fang S., 2000, Computers and Graphics, V24, P200
   Forest Vincent., 2009, journal of graphics, gpu, and game tools, V14.3, P21
   Gokul Varadhan, 2003, Symposium on Geometry Processing, P116
   GUY S., 2004, ACM T GRAPH
   Haumont D., 2002, J GRAPH TOOLS, V7, P3
   Heidelberger B, 2003, VISION, MODELING, AND VISUALIZATION 2003, P461
   Heidrich W, 1999, SPRING EUROGRAP, P187
   Huang J, 1998, IEEE SYMPOSIUM ON VOLUME VISUALIZATION, P119, DOI 10.1109/SVV.1998.729593
   Ix F.D., 2000, INCREMENTAL TRIANGLE
   Kajiya J. T., 1989, Computer Graphics, V23, P271, DOI 10.1145/74334.74361
   Kalaiah A, 2005, ACM T GRAPHIC, V24, P348, DOI 10.1145/1061347.1061356
   Karabassi E.-A., 1999, Journal of Graphics Tools, V4, P5, DOI 10.1080/10867651.1999.10487510
   Kay D.S., 1979, ACM SIGGRAPH 79, P158, DOI DOI 10.1145/800249.807438
   LAUTERBACH C, 2009, P EUR 09
   Lindholm E, 2001, COMP GRAPH, P149, DOI 10.1145/383259.383274
   Llamas Ignacio., 2007, SIGGRAPH '07: SIGGRAPH Sketches, P18, DOI 10.1145/1278780.1278802
   Lokovic T, 2000, COMP GRAPH, P385, DOI 10.1145/344779.344958
   MAMMEN A, 1989, IEEE COMPUT GRAPH, V9, P43, DOI 10.1109/38.31463
   Ohbuchi E, 2003, COMPUTER GRAPHICS INTERNATIONAL, PROCEEDINGS, P190, DOI 10.1109/CGI.2003.1214465
   Oliveira G., 2000, REFRACTIVE TEXTURE M
   Oliveira MM, 2007, I3D 2007: ACM SIGGRAPH SYMPOSIUM ON INTERACTIVE 3D GRAPHICS AND GAMES, PROCEEDINGS, P89
   Shade J., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P231, DOI 10.1145/280814.280882
   Sigg C., 2003, Proc. VIS '03, P12
   Sramek M, 1999, IEEE T VIS COMPUT GR, V5, P251, DOI 10.1109/2945.795216
   Stolte N., 1997, TECH REP
   Wang S. W., 1993, Proceedings Visualization '93. (Cat. No.93CH3354-8), P78, DOI 10.1109/VISUAL.1993.398854
   Williams L., 1978, P ANN C COMP GRAPH I, P270
   Wyman C, 2005, ACM T GRAPHIC, V24, P1050, DOI 10.1145/1073204.1073310
   Wyman Chris., 2006, I3D 06, P153
   Zhang L, 2007, VISUAL COMPUT, V23, P783, DOI 10.1007/s00371-007-0149-0
   Zhou K, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409079
NR 41
TC 4
Z9 4
U1 1
U2 20
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2014
VL 30
IS 3
BP 327
EP 340
DI 10.1007/s00371-013-0858-5
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AB3LU
UT WOS:000331693100007
DA 2024-07-18
ER

PT J
AU Bae, MS
   Park, IK
AF Bae, Min Soo
   Park, In Kyu
TI Content-based 3D model retrieval using a single depth image from a
   low-cost 3D camera
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Computer Graphics International (CGI) Conference
CY 2013
CL Hanover, GERMANY
DE Content-based 3D model retrieval; 3D camera; Depth image based
   representation; Adaptive view-sampling; Rotation-invariant descriptor
AB In this paper, we propose an efficient method for 3D model retrieval using a single depth image. Unlike existing algorithms that use a complete 3D model or a user sketch as input queries, a single depth image is used as an input query, which can be captured easily with an off-the-shelf lowcost 3D camera, such as a Kinect camera. 3D models in the database are represented by multiple depth images acquired from adaptively sampled viewpoints. The proposed algorithm can retrieve relevant 3D models while considering local 3D geometric characteristics using a rotation-invariant feature descriptor. The proposed method consists of three steps: preprocessing, multiple depth image based representation (M-DIBR), and description of 3D models, and similarity measurement and comparison. Experimental results demonstrate that the proposed algorithm is convenient to use and its performance is comparable to recent algorithms in terms of retrieval accuracy and speed.
C1 [Bae, Min Soo; Park, In Kyu] Inha Univ, Sch Informat & Commun Engn, Inchon 402751, South Korea.
C3 Inha University
RP Park, IK (corresponding author), Inha Univ, Sch Informat & Commun Engn, Inchon 402751, South Korea.
EM minsoo.bae@lignex1.com; pik@inha.ac.kr
RI Park, In Kyu/B-5967-2013
FU MKE (The Ministry of Knowledge Economy); NHN Corp., under IT/SW Creative
   research program [NIPA-2012-(H0505-12-1003)]; Basic Science Research
   Program through the National Research Foundation of Korea; Ministry of
   Education, Science, and Technology [2012R1A1A2009495]
FX This research was supported by the MKE (The Ministry of Knowledge
   Economy), NHN Corp., under IT/SW Creative research program supervised by
   the NIPA (National IT Industry Promotion Agency)
   (NIPA-2012-(H0505-12-1003)). This research was supported by the Basic
   Science Research Program through the National Research Foundation of
   Korea funded by the Ministry of Education, Science, and Technology
   (2012R1A1A2009495).
CR [Anonymous], 2004, P 6 ACM SIGMM INT WO
   [Anonymous], 3D WAR
   [Anonymous], KIN
   [Anonymous], COMP UN DEV ARCH CUD
   Chaouch A., 2007, P IEEE INT C IM PROC, P373
   Daras P, 2010, INT J COMPUT VISION, V89, P229, DOI 10.1007/s11263-009-0277-2
   Eitz Mathias, 2012, ACM T GRAPH, V31, P3
   Funkhouser T, 2003, ACM T GRAPHIC, V22, P83, DOI 10.1145/588272.588279
   Heczko M., 2002, DATENBANK SPEKTRUM, V2
   Hilaga M, 2001, COMP GRAPH, P203, DOI 10.1145/383259.383282
   HORN BKP, 1984, P IEEE, V72, P1671, DOI 10.1109/PROC.1984.13073
   KHOTANZAD A, 1990, IEEE T PATTERN ANAL, V12, P489, DOI 10.1109/34.55109
   Loop C, 1987, THESIS U UTAH
   MASLYUK LL, 2004, IEEE T CIRC SYST VID, V14, P1032
   Ohbuchi R., 2003, P 5 ACM SIGMM INT WO, P39, DOI DOI 10.1145/973264.973272
   Ohbuchi R, 2008, IEEE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS 2008, PROCEEDINGS, P93, DOI 10.1109/SMI.2008.4547955
   Osada R, 2002, ACM T GRAPHIC, V21, P807, DOI 10.1145/571647.571648
   Paquet E, 2000, SIGNAL PROCESS-IMAGE, V16, P103, DOI 10.1016/S0923-5965(00)00020-5
   Petschnigg G, 2004, ACM T GRAPHIC, V23, P664, DOI 10.1145/1015706.1015777
   Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720
   Shih JL, 2007, PATTERN RECOGN, V40, P283, DOI 10.1016/j.patcog.2006.04.034
   Shilane P, 2004, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, P167, DOI 10.1109/smi.2004.1314504
   Sundar H, 2003, SMI 2003: SHAPE MODELING INTERNATIONAL 2003, PROCEEDINGS, P130, DOI 10.1109/smi.2003.1199609
   Tangelder JH, 2008, MULTIMED TOOLS APPL, V39, P441, DOI 10.1007/s11042-007-0181-0
   Tung T, 2004, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, P157, DOI 10.1109/SMI.2004.1314503
NR 25
TC 11
Z9 13
U1 0
U2 20
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2013
VL 29
IS 6-8
BP 555
EP 564
DI 10.1007/s00371-013-0819-z
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 151RR
UT WOS:000319478400010
DA 2024-07-18
ER

PT J
AU Zhang, J
   Zheng, CW
   Hu, XH
AF Zhang, Jie
   Zheng, Changwen
   Hu, Xiaohui
TI Triangle mesh compression along the Hamiltonian cycle
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Computer Graphics International (CGI) Conference
CY 2013
CL Hanover, GERMANY
DE Triangle mesh; Mesh compression; Hamiltonian cycle; Connectivity coding;
   Geometry coding; Arithmetic coding
ID PROGRESSIVE COMPRESSION; CONNECTIVITY
AB This paper proposes a novel and efficient algorithm for single-rate compression of triangle meshes. The input mesh is traversed along its greedy Hamiltonian cycle in O(n) time. Based on the Hamiltonian cycle, the mesh connectivity can be encoded by a face label sequence with low entropy containing only four kinds of labels (HETS) and the transmission delay at the decoding end that frequently occurs in the conventional single-rate approaches is obviously reduced. The mesh geometry is compressed with a global coordinate concentration strategy and a novel local parallelogram error prediction scheme. Experiments on realistic 3D models demonstrate the effectiveness of our approach in terms of compression rates and run time performance compared to the leading single-rate and progressive mesh compression methods.
C1 [Zhang, Jie; Zheng, Changwen; Hu, Xiaohui] Chinese Acad Sci, Inst Software, Natl Key Lab Integrated Informat Syst Technol, Beijing, Peoples R China.
   [Zhang, Jie] Chinese Acad Sci, Grad Univ, Beijing, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Software, CAS; Chinese Academy
   of Sciences; University of Chinese Academy of Sciences, CAS
RP Zhang, J (corresponding author), Chinese Acad Sci, Inst Software, Natl Key Lab Integrated Informat Syst Technol, Beijing, Peoples R China.
EM zhangjie07@iscas.ac.cn
CR Ahn JK, 2011, VISUAL COMPUT, V27, P769, DOI 10.1007/s00371-011-0565-z
   Alliez P, 2005, MATH VIS, P3, DOI 10.1007/3-540-26808-1_1
   Alliez P, 2001, COMP GRAPH, P195, DOI 10.1145/383259.383281
   Alliez P, 2001, COMPUT GRAPH FORUM, V20, pC480, DOI 10.1111/1467-8659.00541
   CHIBA N, 1989, J ALGORITHM, V10, P187, DOI 10.1016/0196-6774(89)90012-6
   Deering M., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P13, DOI 10.1145/218380.218391
   Garey M. R., 1976, SIAM Journal on Computing, V5, P704, DOI 10.1137/0205049
   Gurung T., 2011, ACM T GRAPHIC, V67, P1
   Hoppe H., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P99, DOI 10.1145/237170.237216
   Isenberg M., 2000, Proceedings of the 12th Annual Canadian Conference on Computational Geometry, P247
   Isenburg M, 2000, COMP GRAPH, P263, DOI 10.1145/344779.344919
   Isenburg M, 2006, PROC GRAPH INTERF, P89
   Jong BS, 2005, FOURTH ANNUAL ACIS INTERNATIONAL CONFERENCE ON COMPUTER AND INFORMATION SCIENCE, PROCEEDINGS, P583
   Kalberer F., 2005, EUROGRAPHICS 2005, V24
   KEELER K, 1995, DISCRETE APPL MATH, V58, P239, DOI 10.1016/0166-218X(93)E0150-W
   Khodavosky A, 2002, GRAPH MODELS, V64, P147, DOI 10.1006/gmod.2002.0575
   King Davis., 1999, 11 CANADIAN C COMPUT, P146
   Lee H., 2009, P VIS MOD VIS WORKSH, P73
   Lee H, 2012, VISUAL COMPUT, V28, P137, DOI 10.1007/s00371-011-0602-y
   Lee HY, 2002, COMPUT GRAPH FORUM, V21, P383, DOI 10.1111/1467-8659.t01-1-00598
   Maglo A, 2012, COMPUT GRAPH-UK, V36, P349, DOI 10.1016/j.cag.2012.03.023
   Pajarola R, 2000, IEEE T VIS COMPUT GR, V6, P79, DOI 10.1109/2945.841122
   Pajarola R, 2000, COMPUTER GRAPHICS INTERNATIONAL 2000, PROCEEDINGS, P173, DOI 10.1109/CGI.2000.852332
   Peng JL, 2005, J VIS COMMUN IMAGE R, V16, P688, DOI 10.1016/j.jvcir.2005.03.001
   Peng JL, 2010, COMPUT GRAPH FORUM, V29, P2029, DOI 10.1111/j.1467-8659.2010.01789.x
   Peng JL, 2005, ACM T GRAPHIC, V24, P609, DOI 10.1145/1073204.1073237
   Rossignac J, 1999, IEEE T VIS COMPUT GR, V5, P47, DOI 10.1109/2945.764870
   Rossignac J, 1999, COMP GEOM-THEOR APPL, V14, P119, DOI 10.1016/S0925-7721(99)00028-0
   Touma C, 1998, GRAPHICS INTERFACE '98 - PROCEEDINGS, P26
   TUTTE WT, 1962, CANADIAN J MATH, V14, P21, DOI 10.4153/CJM-1962-002-9
   Valette S, 2004, IEEE T VIS COMPUT GR, V10, P123, DOI 10.1109/TVCG.2004.1260764
   Valette S, 2009, COMPUT GRAPH FORUM, V28, P1301, DOI 10.1111/j.1467-8659.2009.01507.x
   WITTEN IH, 1987, COMMUN ACM, V30, P520, DOI 10.1145/214762.214771
NR 33
TC 5
Z9 7
U1 0
U2 1
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2013
VL 29
IS 6-8
BP 717
EP 727
DI 10.1007/s00371-013-0808-2
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 151RR
UT WOS:000319478400025
DA 2024-07-18
ER

PT J
AU Hu, SM
   Chen, T
   Xu, K
   Cheng, MM
   Martin, RR
AF Hu, Shi-Min
   Chen, Tao
   Xu, Kun
   Cheng, Ming-Ming
   Martin, Ralph R.
TI Internet visual media processing: a survey with graphics and vision
   applications
SO VISUAL COMPUTER
LA English
DT Article
DE Internet visual media; Large databases; Images; Video; Survey
ID PHOTO COLLECTIONS; IMAGE RETRIEVAL; RECOGNITION; SHAPE; ATTENTION;
   SCENE; COLORIZATION; PROPAGATION; WORLD
AB In recent years, the computer graphics and computer vision communities have devoted significant attention to research based on Internet visual media resources. The huge number of images and videos continually being uploaded by millions of people have stimulated a variety of visual media creation and editing applications, while also posing serious challenges of retrieval, organization, and utilization. This article surveys recent research as regards processing of large collections of images and video, including work on analysis, manipulation, and synthesis. It discusses the problems involved, and suggests possible future directions in this emerging research area.
C1 [Hu, Shi-Min; Chen, Tao; Xu, Kun] Tsinghua Univ, Dept Comp Sci & Technol, Beijing 100084, Peoples R China.
   [Cheng, Ming-Ming] Tsinghua Univ, Beijing 100084, Peoples R China.
   [Martin, Ralph R.] Cardiff Univ, Cardiff CF10 3AX, S Glam, Wales.
C3 Tsinghua University; Tsinghua University; Cardiff University
RP Hu, SM (corresponding author), Tsinghua Univ, Dept Comp Sci & Technol, Beijing 100084, Peoples R China.
EM shimin@tsinghua.edu.cn
RI Martin, Ralph R/D-2366-2010; Cheng, Ming-Ming/A-2527-2009; Hu,
   Shi-Min/AAW-1952-2020; Xu, Kun/K-7134-2012
OI Cheng, Ming-Ming/0000-0001-5550-8758; Martin, Ralph/0000-0002-8495-8536
FU National Basic Research Project of China [2011CB302205]; Natural Science
   Foundation of China [61120106007, 61103079]; National High Technology
   Research and Development Program of China [2012AA011802]; EPSRC; EPSRC
   [EP/J009830/1] Funding Source: UKRI
FX This work was supported by the National Basic Research Project of China
   (Project Number 2011CB302205), the Natural Science Foundation of China
   (Project Number 61120106007 and 61103079), and the National High
   Technology Research and Development Program of China (Project Number
   2012AA011802), and an EPSRC Travel Grant.
CR Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   Agarwal S, 2009, IEEE I CONF COMP VIS, P72, DOI 10.1109/ICCV.2009.5459148
   An XB, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360639
   [Anonymous], 2007, CALTECH 256 OBJECT C
   [Anonymous], 2011, IEEE T VIS COMPUT GR, DOI DOI 10.1109/TVCG.2010.266
   [Anonymous], 2009, P 18 INT C WORLD WID
   [Anonymous], ACM T GRAPH
   [Anonymous], 1999, COMPUTATIONAL GEOMET
   [Anonymous], 2007, P IEEE ICCV
   [Anonymous], 120 BROWN U
   [Anonymous], ACM T GRAPH
   [Anonymous], 2012, ACM T GRAPHIC, DOI DOI 10.1145/2185520.2185597
   [Anonymous], 2009, ACM T GRAPHIC, DOI DOI 10.1145/1618452.1618470
   [Anonymous], 2002, Introduction to MPEG-7: Multimedia Content Description Interface
   Bagon S, 2008, LECT NOTES COMPUT SC, V5305, P30, DOI 10.1007/978-3-540-88693-8_3
   Bai XA, 2009, PROC CVPR IEEE, P1335, DOI 10.1109/CVPRW.2009.5206543
   Bai X, 2010, IEEE T PATTERN ANAL, V32, P861, DOI 10.1109/TPAMI.2009.85
   Belongie S, 2002, IEEE T PATTERN ANAL, V24, P509, DOI 10.1109/34.993558
   Berg Tamara L., 2009, 2009 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops (CVPR Workshops), P1, DOI 10.1109/CVPR.2009.5204174
   Bitouk D, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360638
   Boykov Y, 2006, INT J COMPUT VISION, V70, P109, DOI 10.1007/s11263-006-7934-5
   Cao Yang, 2010, P 18 ACM INT C MULT, P1605
   Chen T, 2013, IEEE T VIS COMPUT GR, V19, P824, DOI 10.1109/TVCG.2012.148
   Cheng MM, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778820
   Cheng MM, 2011, IEEE T PATTERN ANAL, V33, P200, DOI 10.1109/TPAMI.2010.138
   Chia AYS, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024190
   Christopoulos C, 2000, IEEE T CONSUM ELECTR, V46, P1103, DOI 10.1109/30.920468
   Cong L, 2011, VISUAL COMPUT, V27, P187, DOI 10.1007/s00371-010-0522-2
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Datta R, 2008, ACM COMPUT SURV, V40, DOI 10.1145/1348246.1348248
   DelBimbo A, 1997, IEEE T PATTERN ANAL, V19, P121, DOI 10.1109/34.574790
   DESIMONE R, 1995, ANNU REV NEUROSCI, V18, P193, DOI 10.1146/annurev-psych-122414-033400
   Diakopoulos N, 2004, LECT NOTES COMPUT SC, V3115, P299
   Ding M, 2010, VISUAL COMPUT, V26, P721, DOI 10.1007/s00371-010-0448-8
   Eitz M, 2011, IEEE COMPUT GRAPH, V31, P56, DOI 10.1109/MCG.2011.67
   Enzweiler M, 2009, IEEE T PATTERN ANAL, V31, P2179, DOI 10.1109/TPAMI.2008.260
   Everingham M., 2006, PASCAL VISUAL OBJECT
   Farbman Z, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531373
   Fattal R, 2009, ACM T GRAPHIC, V29, DOI 10.1145/1640443.1640449
   Feng BL, 2011, VISUAL COMPUT, V27, P21, DOI 10.1007/s00371-010-0510-6
   Fiss J, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024162
   Frahm JM, 2010, LECT NOTES COMPUT SC, V6314, P368, DOI 10.1007/978-3-642-15561-1_27
   Furukawa Y, 2010, PROC CVPR IEEE, P1434, DOI 10.1109/CVPR.2010.5539802
   Georghiades AS, 2001, IEEE T PATTERN ANAL, V23, P643, DOI 10.1109/34.927464
   Goesele M, 2007, IEEE I CONF COMP VIS, P825, DOI 10.1109/iccv.2007.4408933
   Goldberg C, 2012, COMPUT GRAPH FORUM, V31, P265, DOI 10.1111/j.1467-8659.2012.03005.x
   Grady L, 2006, IEEE T PATTERN ANAL, V28, P1768, DOI 10.1109/TPAMI.2006.233
   Haber T, 2009, PROC CVPR IEEE, P627, DOI 10.1109/CVPRW.2009.5206753
   Han DF, 2010, VISUAL COMPUT, V26, P749, DOI 10.1007/s00371-010-0480-8
   Harel J, 2006, Advances in Neural Information Processing Systems, V19
   Hata M, 2012, VISUAL COMPUT, V28, P657, DOI 10.1007/s00371-012-0689-9
   Hays J, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239455
   He JF, 2012, PROC CVPR IEEE, P3005, DOI 10.1109/CVPR.2012.6248030
   Heath K, 2010, PROC CVPR IEEE, P3432, DOI 10.1109/CVPR.2010.5539991
   HIRATA K, 1992, LECT NOTES COMPUT SC, V580, P56, DOI 10.1007/BFb0032423
   Huang H, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024189
   Huang H, 2011, VISUAL COMPUT, V27, P861, DOI 10.1007/s00371-011-0596-5
   Huang H, 2010, VISUAL COMPUT, V26, P933, DOI 10.1007/s00371-010-0498-y
   Huang H, 2010, VISUAL COMPUT, V26, P731, DOI 10.1007/s00371-010-0504-4
   Huang MC, 2010, VISUAL COMPUT, V26, P943, DOI 10.1007/s00371-010-0435-0
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Jin Y, 2010, VISUAL COMPUT, V26, P769, DOI 10.1007/s00371-010-0472-8
   Jing Y, 2008, IEEE T PATTERN ANAL, V30, P1877, DOI 10.1109/TPAMI.2008.121
   Johnson M, 2006, COMPUT GRAPH FORUM, V25, P407, DOI 10.1111/j.1467-8659.2006.00960.x
   Johnson MK, 2011, IEEE T VIS COMPUT GR, V17, P1273, DOI 10.1109/TVCG.2010.233
   Kalogerakis E, 2009, IEEE I CONF COMP VIS, P253, DOI 10.1109/ICCV.2009.5459259
   KASS M, 1987, INT J COMPUT VISION, V1, P321, DOI 10.1007/BF00133570
   Kennedy Lyndon., 2007, Proceedings of the 15th International Conference on Multimedia, P631, DOI DOI 10.1145/1291233.1291384
   KOCH C, 1985, HUM NEUROBIOL, V4, P219
   Kuthirummal S, 2008, LECT NOTES COMPUT SC, V5305, P74, DOI 10.1007/978-3-540-88693-8_6
   Lalonde JF, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1276377.1276381, 10.1145/1239451.1239454]
   Levin A, 2004, ACM T GRAPHIC, V23, P689, DOI 10.1145/1015706.1015780
   Levin A, 2008, IEEE T PATTERN ANAL, V30, P228, DOI 10.1109/TPAMI.2007.1177
   Lew MS, 2006, ACM T MULTIM COMPUT, V2, P1, DOI 10.1145/1126004.1126005
   Li XW, 2008, LECT NOTES COMPUT SC, V5302, P427
   Li Y, 2010, COMPUT GRAPH FORUM, V29, P2049, DOI 10.1111/j.1467-8659.2010.01791.x
   Li YP, 2009, IEEE I CONF COMP VIS, P1957, DOI 10.1109/ICCV.2009.5459432
   Ling Y, 2012, VISUAL COMPUT, V28, P733, DOI 10.1007/s00371-012-0691-2
   Lischinski D, 2006, ACM T GRAPHIC, V25, P646, DOI 10.1145/1141911.1141936
   Liu H, 2012, VISUAL COMPUT, V28, P279, DOI 10.1007/s00371-011-0638-z
   Liu T, 2011, IEEE T PATTERN ANAL, V33, P353, DOI 10.1109/TPAMI.2010.70
   Liu XM, 2008, CATAL COMMUN, V9, P1, DOI 10.1016/j.catcom.2007.05.020
   Lu SP, 2013, IEEE T VIS COMPUT GR, V19, P1218, DOI 10.1109/TVCG.2012.145
   Ma Y.F., 2003, P 11 ACM INT C MULT, P374, DOI DOI 10.1145/957013.957094
   Min Chen, 2011, IEEE INFOCOM 2011 - IEEE Conference on Computer Communications. Workshops, P409, DOI 10.1109/INFCOMW.2011.5928847
   Mortensen E. N., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P191, DOI 10.1145/218380.218442
   Navalpakkam Vidhya, 2006, Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit, V2, P2049, DOI DOI 10.1109/CVPR.2006.54
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   Page L., 1999, PAGERANK CITATION RA
   Pajak D, 2010, VISUAL COMPUT, V26, P739, DOI 10.1007/s00371-010-0485-3
   Pérez P, 2003, ACM T GRAPHIC, V22, P313, DOI 10.1145/882262.882269
   Quack Till., 2008, P 2008 INT C CONTENT, P47
   Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720
   Russell BC, 2008, INT J COMPUT VISION, V77, P157, DOI 10.1007/s11263-007-0090-8
   Rutishauser U, 2004, PROC CVPR IEEE, P37
   Shen L, 2009, PROC CVPR IEEE, P1850, DOI 10.1109/CVPRW.2009.5206732
   Shrivastava A, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024188
   Sinop A. K., 2007, P IEEE ICCV, P1
   Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663
   Snavely Noah, 2011, IPSJ Transactions on Computer Vision and Applications, V3, P44, DOI 10.2197/ipsjtcva.3.44
   Snavely N, 2008, INT J COMPUT VISION, V80, P189, DOI 10.1007/s11263-007-0107-3
   Snavely N, 2006, ACM T GRAPHIC, V25, P835, DOI 10.1145/1141911.1141964
   Snavely N, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360614
   Sorokin A, 2008, PROC CVPR IEEE, P23
   Sun J, 2004, ACM T GRAPHIC, V23, P315, DOI 10.1145/1015706.1015721
   Tao LT, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531374
   Thayananthan A, 2003, PROC CVPR IEEE, P127
   Tompkin J, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185564
   TREISMAN AM, 1980, COGNITIVE PSYCHOL, V12, P97, DOI 10.1016/0010-0285(80)90005-5
   TSOTSOS JK, 1990, BEHAV BRAIN SCI, V13, P423, DOI 10.1017/S0140525X00079577
   Wang BY, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866172
   Wang D, 2011, VISUAL COMPUT, V27, P853, DOI 10.1007/s00371-011-0559-x
   Welsh T, 2002, ACM T GRAPHIC, V21, P277, DOI 10.1145/566570.566576
   Wu JL, 2012, VISUAL COMPUT, V28, P723, DOI 10.1007/s00371-012-0683-2
   Xiao CX, 2012, VISUAL COMPUT, V28, P713, DOI 10.1007/s00371-012-0679-y
   Xie ZF, 2012, VISUAL COMPUT, V28, P1195, DOI 10.1007/s00371-011-0668-6
   Xie ZF, 2010, VISUAL COMPUT, V26, P1123, DOI 10.1007/s00371-010-0466-6
   Xu K, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618464
   Xue S, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185580
   Yang XW, 2009, PROC CVPR IEEE, P357, DOI 10.1109/CVPRW.2009.5206844
   Zhang FL, 2012, IEEE T VIS COMPUT GR, V18, P1849, DOI 10.1109/TVCG.2012.68
   Zhang JW, 2011, VISUAL COMPUT, V27, P749, DOI 10.1007/s00371-011-0569-8
   Zhang Y, 2011, VISUAL COMPUT, V27, P739, DOI 10.1007/s00371-011-0583-x
   Zheng YT, 2009, PROC CVPR IEEE, P1085, DOI 10.1109/CVPRW.2009.5206749
   Zhong F, 2011, VISUAL COMPUT, V27, P707, DOI 10.1007/s00371-011-0588-5
NR 125
TC 75
Z9 82
U1 0
U2 43
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2013
VL 29
IS 5
BP 393
EP 405
DI 10.1007/s00371-013-0792-6
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 127RD
UT WOS:000317715200007
DA 2024-07-18
ER

PT J
AU Feng, W
   Huang, J
   Ju, T
   Bao, HJ
AF Feng, Wei
   Huang, Jin
   Ju, Tao
   Bao, Hujun
TI Feature correspondences using Morse Smale complex
SO VISUAL COMPUTER
LA English
DT Article
DE Point matching; Correspondence; Morse-Smale complex
AB Establishing corresponding features on two non-rigidly deformed 3D surfaces is a challenging and well-studied problem in computer graphics. Unlike previous approaches that constrain the matching between feature pairs using isometry-invariant distance metrics, we constrain the matching using a discrete connectivity graph derived from the Morse-Smale complex of the Auto Diffusion Function. We observed that the graph remains stable even for surfaces differing by topology or by significant deformation. This algorithm is simple to implement and efficient to run. When tested on a range of examples, our algorithm produces comparable results with state-of-art methods on surfaces with strong isometry but with greatly improved efficiency, and often gets better correspondences on surfaces with larger shape variances.
C1 [Feng, Wei; Huang, Jin; Bao, Hujun] Zhejiang Univ, CAD&CG Lab, Hangzhou 310003, Zhejiang, Peoples R China.
   [Ju, Tao] Washington Univ, Dept Comp Sci & Engn, St Louis, MO USA.
C3 Zhejiang University; Washington University (WUSTL)
RP Feng, W (corresponding author), Zhejiang Univ, CAD&CG Lab, Hangzhou 310003, Zhejiang, Peoples R China.
EM fengwei@cad.zju.edu.cn; hj@cad.zju.edu.cn
FU Div Of Information & Intelligent Systems; Direct For Computer & Info
   Scie & Enginr [0846072] Funding Source: National Science Foundation
CR Anguelov Dragomir., 2004, NIPS
   [Anonymous], 1982, COMBINATORIAL OPTIMI
   [Anonymous], 1963, MORSE THEORY AM 51, DOI [10.1515/9781400881802, DOI 10.1515/9781400881802]
   [Anonymous], 2010, P EUR WORKSH 3D OBJ
   Bommes D, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531383
   Bremer PT, 2004, IEEE T VIS COMPUT GR, V10, P385, DOI 10.1109/TVCG.2004.3
   Bronstein AM, 2010, INT J COMPUT VISION, V89, P266, DOI 10.1007/s11263-009-0301-6
   Bronstein MM, 2010, PROC CVPR IEEE, P1704, DOI 10.1109/CVPR.2010.5539838
   Coifman RR, 2006, APPL COMPUT HARMON A, V21, P5, DOI 10.1016/j.acha.2006.04.006
   Dong S, 2006, ACM T GRAPHIC, V25, P1057, DOI 10.1145/1141911.1141993
   Edelsbrunner H., 2001, PROC 17 ANN ACM SYMP, P70, DOI DOI 10.1145/378583.378626
   Funkhouser T., 2010, COMPUT GRAPH FORUM, V29
   Gebal K, 2009, COMPUT GRAPH FORUM, V28, P1405, DOI 10.1111/j.1467-8659.2009.01517.x
   Giorgi Daniela., 2007, SHAPE RETRIEVAL CONT
   Huang QX, 2008, COMPUT GRAPH FORUM, V27, P1449, DOI 10.1111/j.1467-8659.2008.01285.x
   Jain V, 2007, COMPUT AIDED DESIGN, V39, P398, DOI 10.1016/j.cad.2007.02.009
   Kim V., 2010, COMPUT GRAPH FORUM, V29
   Kim V. G., 2011, T GRAPH P SIGGRAPH 2
   Leordeanu M, 2005, IEEE I CONF COMP VIS, P1482
   Lipman Y, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531378
   Ovsjanikov M, 2010, COMPUT GRAPH FORUM, V29, P1555, DOI 10.1111/j.1467-8659.2010.01764.x
   Praun E, 2001, COMP GRAPH, P179, DOI 10.1145/383259.383277
   Reuter M, 2010, INT J COMPUT VISION, V89, P287, DOI 10.1007/s11263-009-0278-1
   Ruggeri MR, 2010, INT J COMPUT VISION, V89, P248, DOI 10.1007/s11263-009-0250-0
   Rustamov Raif M, 2007, P S GEOM PROC, V257, P225
   Sharma A., 2010, Computer Vision and Pattern Recognition Workshops (CVPRW), 2010 IEEE Computer Society Conference on, P29
   Sun JA, 2009, COMPUT GRAPH FORUM, V28, P1383, DOI 10.1111/j.1467-8659.2009.01515.x
   Tevs A, 2011, COMPUT GRAPH FORUM, V30, P543, DOI 10.1111/j.1467-8659.2011.01879.x
   van Kaick Oliver., 2010, Proc. of Eurographics State-of-the-art Report, P1
   Weinkauf T, 2010, COMPUT GRAPH FORUM, V29, P1221, DOI 10.1111/j.1467-8659.2009.01702.x
   Zaharescu A., 2011, P WORKSH 3D OBJ RETR
   Zeng Y., 2010, CVPR
   Zhang H, 2008, COMPUT GRAPH FORUM, V27, P1431, DOI 10.1111/j.1467-8659.2008.01283.x
NR 33
TC 10
Z9 12
U1 0
U2 13
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2013
VL 29
IS 1
BP 53
EP 67
DI 10.1007/s00371-012-0674-3
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 064HB
UT WOS:000313063400005
DA 2024-07-18
ER

PT J
AU Dahan, MJ
   Chen, N
   Shamir, A
   Cohen-Or, D
AF Dahan, Meir Johnathan
   Chen, Nir
   Shamir, Ariel
   Cohen-Or, Daniel
TI Combining color and depth for enhanced image segmentation and
   retargeting
SO VISUAL COMPUTER
LA English
DT Article
DE Image segmentation; Retargeting; Depth maps
ID MOTION SEGMENTATION; VIDEO
AB As depth cameras become more popular, pixel depth information becomes easier to obtain. This information can clearly enhance many image processing applications. However, combining depth and color information is not straightforward as these two signals can have different noise characteristics, differences in resolution, and their boundaries do not generally agree. We present a technique that combines depth and color image information from real devices in synergy. In particular, we focus on combining them to improve image segmentation. We use color information to fill and clean depth and use depth to enhance color image segmentation. We demonstrate the utility of the combined segmentation for extracting layers and present a novel image retargeting algorithm for layered images.
C1 [Dahan, Meir Johnathan; Cohen-Or, Daniel] Tel Aviv Univ, IL-69978 Tel Aviv, Israel.
   [Chen, Nir; Shamir, Ariel] Interdisciplinary Ctr Herzilya, Herzliyya, Israel.
C3 Tel Aviv University; Reichman University
RP Dahan, MJ (corresponding author), Tel Aviv Univ, IL-69978 Tel Aviv, Israel.
EM meirjon@post.tau.ac.il; chen.nir@idc.ac.il; arik@idc.ac.il
CR [Anonymous], P 11 IEEE INT C COMP
   [Anonymous], EUR C COMP VIS ECCV
   [Anonymous], 3DIM2009
   [Anonymous], 1992, R. woods digital image processing
   [Anonymous], INT J COMPUT VIS
   [Anonymous], ICCV 09 KYOT SEP OCT
   Avidan S, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239461
   Boykov Y, 2004, IEEE T PATTERN ANAL, V26, P1124, DOI 10.1109/TPAMI.2004.60
   CARLBOM I, 1978, COMPUT SURV, V10, P465, DOI 10.1145/356744.356750
   Cheng MM, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778820
   Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236
   Crabb Ryan, 2008, 2008 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops (CVPR Workshops), P1, DOI 10.1109/CVPRW.2008.4563170
   Feldman D, 2008, IEEE T PATTERN ANAL, V30, P1171, DOI 10.1109/TPAMI.2007.70766
   Kopf J, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239453
   Krähenbühl P, 2009, ACM T GRAPHIC, V28, DOI [10.1145/1616452.1618472, 10.1145/1618452.1618472]
   Kwatra V, 2003, ACM T GRAPHIC, V22, P277, DOI 10.1145/882262.882264
   Leens J, 2009, LECT NOTES COMPUT SC, V5815, P104, DOI 10.1007/978-3-642-04667-4_11
   Riemens A. K., 2009, Proceedings of the SPIE - The International Society for Optical Engineering, V7257, DOI 10.1117/12.805640
   Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720
   Rubinstein M, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866186
   Rubinstein M, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360615
   Schuon Sebastian, 2008, 2008 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops (CVPR Workshops), P1, DOI 10.1109/CVPRW.2008.4563171
   Smith P, 2004, IEEE T PATTERN ANAL, V26, P479, DOI 10.1109/TPAMI.2004.1265863
   Vezhnevets V., 2005, GrowCut - Interactive Multi- Label N-D Image Segmentation By Cellular
   Wang Y, 2008, CELL POLYM, V27, P1
   Wexler Y, 2004, PROC CVPR IEEE, P120
   Yang Q, 2007, IEEE COMPUTER SOC C, P1
   Zhang GF, 2009, IEEE T VIS COMPUT GR, V15, P828, DOI 10.1109/TVCG.2009.47
   Zhang GF, 2009, IEEE T PATTERN ANAL, V31, P974, DOI 10.1109/TPAMI.2009.52
NR 29
TC 32
Z9 35
U1 0
U2 10
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2012
VL 28
IS 12
BP 1181
EP 1193
DI 10.1007/s00371-011-0667-7
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 029ZZ
UT WOS:000310538700004
DA 2024-07-18
ER

PT J
AU Kapadia, M
   Singh, S
   Hewlett, W
   Reinman, G
   Faloutsos, P
AF Kapadia, Mubbasir
   Singh, Shawn
   Hewlett, William
   Reinman, Glenn
   Faloutsos, Petros
TI Parallelized egocentric fields for autonomous navigation
SO VISUAL COMPUTER
LA English
DT Article
DE Affordance; Egocentric; Steering; Space-time planning
ID SIMULATION; MODEL
AB In this paper, we propose a general framework for local path-planning and steering that can be easily extended to perform high-level behaviors. Our framework is based on the concept of affordances: the possible ways an agent can interact with its environment. Each agent perceives the environment through a set of vector and scalar fields that are represented in the agent's local space. This egocentric property allows us to efficiently compute a local space-time plan and has better parallel scalability than a global fields approach. We then use these perception fields to compute a fitness measure for every possible action, defined as an affordance field. The action that has the optimal value in the affordance field is the agent's steering decision. We propose an extension to a linear space-time prediction model for dynamic collision avoidance and present our parallelization results on multicore systems. We analyze and evaluate our framework using a comprehensive suite of test cases provided in SteerBench and demonstrate autonomous virtual pedestrians that perform steering and path planning in unknown environments along with the emergence of high-level responses to never seen before situations.
C1 [Kapadia, Mubbasir; Singh, Shawn; Hewlett, William; Reinman, Glenn; Faloutsos, Petros] Univ Calif Los Angeles, Dept Comp Sci, Los Angeles, CA 90024 USA.
   [Kapadia, Mubbasir] Univ Penn, Dept Comp Sci, Philadelphia, PA 19104 USA.
   [Faloutsos, Petros] York Univ, Dept Comp Sci & Engn, Toronto, ON M3J 2R7, Canada.
C3 University of California System; University of California Los Angeles;
   University of Pennsylvania; York University - Canada
RP Kapadia, M (corresponding author), Univ Calif Los Angeles, Dept Comp Sci, Los Angeles, CA 90024 USA.
EM mubbasir@cs.ucla.edu; billyh@cs.ucla.edu; reinman@cs.ucla.edu;
   pfal@cse.yorku.ca
FU Intel through a Visual Computing grant
FX The work in this paper was partially supported by Intel through a Visual
   Computing grant, and the donation of a 32-core Emerald Ridge system with
   Xeon processors X7560. In particular, we would like to thank Randi Rost,
   Scott Buck, and Mitchell Lum from Intel for their support.
CR Altun K, 2005, 2005 IEEE INTERNATIONAL WORKSHOP ON ROBOT AND HUMAN INTERACTIVE COMMUNICATION (RO-MAN), P396
   Arkin R. C., 1987, Proceedings of the 1987 IEEE International Conference on Robotics and Automation (Cat. No.87CH2413-3), P264
   Boulic R, 2008, LECT NOTES COMPUT SC, V5277, P176
   Chao G, 1999, IEE CONF PUBL, P144, DOI 10.1049/cp:19991099
   Chenney S., 2004, P ACM SIGGRAPH EG S
   Clements RR, 2004, MATH COMPUT SIMULAT, V64, P259, DOI 10.1016/j.matcom.2003.09.019
   DECHTER R, 1985, J ACM, V32, P505, DOI 10.1145/3828.3830
   Farenc N, 2000, APPL ARTIF INTELL, V14, P69, DOI 10.1080/088395100117160
   Fiorini P, 1998, INT J ROBOT RES, V17, P760, DOI 10.1177/027836499801700706
   Fleming P., 2005, THESIS VANDERBILT U
   Gibson J. J., PERCEIVING ACTING KN
   Goldenstein S., 2001, SCALABLE NONLINEAR D
   GREENO JG, 1994, PSYCHOL REV, V101, P336, DOI 10.1037/0033-295X.101.2.336
   HART PE, 1968, IEEE T SYST SCI CYB, VSSC4, P100, DOI 10.1109/TSSC.1968.300136
   Hart Peter E, 1972, SIGART Bull, V1, P28
   Heck R, 2007, I3D 2007: ACM SIGGRAPH SYMPOSIUM ON INTERACTIVE 3D GRAPHICS AND GAMES, PROCEEDINGS, P129
   Helbing D, 2005, TRANSPORT SCI, V39, P1, DOI 10.1287/trsc.1040.0108
   HELBING D, 1995, PHYS REV E, V51, P4282, DOI 10.1103/PhysRevE.51.4282
   Hoogendoorn SergeP., 2003, 10 INT C TRAVEL BEHA, P507
   Kant K., 1988, Visual Computer, V3, P304, DOI 10.1007/BF01914866
   Kapadia M., 2009, Proceedings of the 2009 symposium on Interactive 3D graphics and games, I3D '09, P215
   LAMARCHE F, 2004, COMPUTER GRAPHICS FO, V23
   Lee KH, 2007, SYMPOSIUM ON COMPUTER ANIMATION 2007: ACM SIGGRAPH/ EUROGRAPHICS SYMPOSIUM PROCEEDINGS, P109
   Lerner A, 2007, COMPUT GRAPH FORUM, V26, P655, DOI 10.1111/j.1467-8659.2007.01089.x
   Li TY, 2003, IEEE INT CONF ROBOT, P3421
   Loscos C, 2003, THEORY AND PRACTICE OF COMPUTER GRAPHICS, PROCEEDINGS, P122
   LOVAS GG, 1994, TRANSPORT RES B-METH, V28, P429, DOI 10.1016/0191-2615(94)90013-2
   Michael D., 2003, EUROGRAPHICS 2003
   Milazzo JS, 1998, TRANSPORT RES REC, P37
   Paris S, 2007, COMPUT GRAPH FORUM, V26, P665, DOI 10.1111/j.1467-8659.2007.01090.x
   Paris S, 2009, LECT NOTES COMPUT SC, V5884, P13, DOI 10.1007/978-3-642-10347-6_2
   Park SI., 2002, Proceedings of the 2002 ACM SIGGRAPH/Eurographics Symposium on Computer Animation. SCA2, P105, DOI DOI 10.1145/545261.545279
   Pelechano N, 2007, SYMPOSIUM ON COMPUTER ANIMATION 2007: ACM SIGGRAPH/ EUROGRAPHICS SYMPOSIUM PROCEEDINGS, P99
   Pelechano N., 2008, SYNTHESIS LECT COMPU
   Quinn M.J., 2003, P 2 INT C PEDESTRIAN, P63
   Reynolds Craig W, 1999, OPENSTEER
   Reynolds CW., 1987, SIGGRAPH Comput. Graph., V21, P25, DOI [10.1145/37402.37406, DOI 10.1145/37402.37406]
   Rodrigues RA, 2009, LECT NOTES ARTIF INT, V5773, P358, DOI 10.1007/978-3-642-04380-2_39
   Rudomín I, 2005, SIMUL MODEL PRACT TH, V13, P741, DOI 10.1016/j.simpat.2005.08.008
   Shao W., 2005, SCA 05 P 2005 ACM SI, P19, DOI DOI 10.1145/1073368.1073371
   Shao W, 2007, GRAPH MODELS, V69, P246, DOI 10.1016/j.gmod.2007.09.001
   Shapiro A, 2007, I3D 2007: ACM SIGGRAPH SYMPOSIUM ON INTERACTIVE 3D GRAPHICS AND GAMES, PROCEEDINGS, P137
   Shimoda S, 2005, IEEE INT CONF ROBOT, P2828
   Singh S., 2010, WORKSH CROWD SIM COM
   Singh S., 2011, P 2011 S INT 3D GRAP
   Singh S, 2009, LECT NOTES COMPUT SC, V5884, P158, DOI 10.1007/978-3-642-10347-6_15
   Singh S, 2009, COMPUT ANIMAT VIRT W, V20, P533, DOI 10.1002/cav.277
   Sud A, 2007, VRST 2007: ACM SYMPOSIUM ON VIRTUAL REALITY SOFTWARE AND TECHNOLOGY, PROCEEDINGS, P99
   Surasmith S., 2002, AIGame ProgrammingWisdom, P161
   Takeuchi R., 1992, Creating and Animating the Virtual World, P163
   Tecchia Franco., 2001, GTEC2001, P17
   Thalmann Daniel., 2007, CROWD SIMULATION
   Torrens PM, 2007, PROCEEDINGS OF THE IEEE/WIC/ACM INTERNATIONAL CONFERENCE ON INTELLIGENT AGENT TECHNOLOGY (IAT 2007), P63, DOI 10.1109/IAT.2007.45
   Treuille A, 2006, ACM T GRAPHIC, V25, P1160, DOI 10.1145/1141911.1142008
   Trovato KI, 2002, IEEE T KNOWL DATA EN, V14, P1218, DOI 10.1109/TKDE.2002.1047763
   Tsubouchi T., 1995, IROS 95 P INT C INTE, V2, P2033
   Turner A, 2002, ENVIRON PLANN B, V29, P473, DOI 10.1068/b12850
   van den Berg J, 2008, IEEE INT CONF ROBOT, P1928, DOI 10.1109/ROBOT.2008.4543489
   van den Berg J, 2008, I3D 2008: SYMPOSIUM ON INTERACTIVE 3D GRAPHICS AND GAMES, PROCEEDINGS, P139
   Warren C. W., 1990, Proceedings 1990 IEEE International Conference on Robotics and Automation (Cat. No.90CH2876-1), P500, DOI 10.1109/ROBOT.1990.126028
   WARREN CW, 1989, PROCEEDINGS - 1989 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION, VOL 1-3, P316, DOI 10.1109/ROBOT.1989.100007
NR 61
TC 16
Z9 22
U1 0
U2 8
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2012
VL 28
IS 12
BP 1209
EP 1227
DI 10.1007/s00371-011-0669-5
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 029ZZ
UT WOS:000310538700006
DA 2024-07-18
ER

PT J
AU Vázquez, PP
   Marco, J
AF Vazquez, Pere-Pau
   Marco, Jordi
TI Using Normalized Compression Distance for image similarity measurement:
   an experimental study
SO VISUAL COMPUTER
LA English
DT Article
DE Image similarity; Normalized Compression Distance; Kolmogorov complexity
AB Similarity metrics are widely used in computer graphics. In this paper, we will concentrate on a new, algorithmic complexity-based metric called Normalized Compression Distance. It is a universal distance used to compare strings. This measure has also been used in computer graphics for image registration or viewpoint selection. However, there is no previous study on how the measure should be used: which compressor and image format are the most suitable. This paper presents a practical study of the Normalized Compression Distance (NCD) applied to color images. The questions we try to answer are: Is NCD a suitable metric for image comparison? How robust is it to rotation, translation, and scaling? Which are the most adequate image formats and compression algorithms? The results of our study show that NCD can be used to address some of the selected image comparison problems, but care must be taken on the compressor and image format selected.
C1 [Vazquez, Pere-Pau; Marco, Jordi] Univ Politecn Cataluna, Dept Llenguatges & Sistemes Informat LSI, Barcelona, Spain.
C3 Universitat Politecnica de Catalunya
RP Vázquez, PP (corresponding author), Univ Politecn Cataluna, Dept Llenguatges & Sistemes Informat LSI, Barcelona, Spain.
EM ppau@lsi.upc.edu
RI Marco, Jordi/AAA-7309-2021; Vázquez, Pere-Pau/L-4697-2014; Vázquez,
   Pere-Pau/HTP-9691-2023
OI Marco, Jordi/0000-0002-0078-7929; Vázquez, Pere-Pau/0000-0003-4638-4065;
   Vázquez, Pere-Pau/0000-0003-4638-4065
FU Spanish Ministry of Science and Innovation [TIN2010-20590-C02-01]
FX This work has been supported by TIN2010-20590-C02-01 of the Spanish
   Ministry of Science and Innovation. The authors want to thank the
   anonymous reviewers that helped improved the paper with a lot of
   interesting suggestions.
CR [Anonymous], 2010, DEV RES TIFF
   Asimakis K., 2010, INSHAME FRONTPAQ
   Bardera A., 2006, P IEEE INT C INF THE
   Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014
   Benedetto D, 2002, PHYS REV LETT, V88, DOI 10.1103/PhysRevLett.88.048702
   Bennett C., 1998, IEEE T INF THEORY, V44
   Bergmans W., 2010, MAXIMUM COMPRESSION
   Bourke P., 2010, BMP IMAGE FORMAT
   Cebrián M, 2007, IEEE T INFORM THEORY, V53, P1895, DOI 10.1109/TIT.2007.894669
   Cilibrasi R, 2004, COMPUT MUSIC J, V28, P49, DOI 10.1162/0148926042728449
   Cilibrasi R, 2005, IEEE T INFORM THEORY, V51, P1523, DOI 10.1109/TIT.2005.844059
   Cleary J. G., 1995, Proceedings. DCC '95 Data Compression Conference (Cat. No.95TH8037), P52, DOI 10.1109/DCC.1995.515495
   Dubnov S, 2003, COMPUTER, V36, P73, DOI 10.1109/MC.2003.1236474
   Lan Y., 2005, P 2 INT C VIS VID GR, P173
   Lee SM, 2005, COLOR RES APPL, V30, P265, DOI 10.1002/col.20122
   Li J, 2003, IEEE T PATTERN ANAL, V25, P1075, DOI 10.1109/TPAMI.2003.1227984
   Li M, 2004, IEEE T INFORM THEORY, V50, P3250, DOI 10.1109/TIT.2004.838101
   Li M, 2001, BIOINFORMATICS, V17, P149, DOI 10.1093/bioinformatics/17.2.149
   Li M., 1997, INTRO KOLMOGOROV COM
   Li M, 2006, LECT NOTES ARTIF INT, V3918, P704
   Macedonas A, 2008, J VIS COMMUN IMAGE R, V19, P464, DOI 10.1016/j.jvcir.2008.06.006
   Mahoney M., 2010, DATA COMPRESSION PRO
   Nelson M., 1996, The data compression book
   Pierre-Emmanuel G., 2010, XNVIEW SOFTWARE FREE
   Rocha J., 2006, ARXIVQBIO0603007V2 C
   Roshal A., 2010, WINRAR ARCH POWERFUL
   Team T.G., 2010, GIMP GNU IM MAN PROG
   Tran N., 2007, P 19 IS T SPIE S EL, P508
   Tran N., 2010, P 22 IS T SPIE S EL
   Vayrynen J.J., 2011, P MT 25 YEA IN PRESS
   Vázquez PP, 2008, LECT NOTES COMPUT SC, V5166, P106, DOI 10.1007/978-3-540-85412-8_10
   Vázquez PP, 2009, VISUAL COMPUT, V25, P441, DOI 10.1007/s00371-009-0326-4
   World Wide Web Consortium, 2010, PORT NETW GRAPH PNG
NR 33
TC 12
Z9 12
U1 0
U2 7
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2012
VL 28
IS 11
BP 1063
EP 1084
DI 10.1007/s00371-011-0651-2
PG 22
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 021JO
UT WOS:000309881300002
DA 2024-07-18
ER

PT J
AU Gilet, G
   Dischler, JM
   Ghazanfarpour, D
AF Gilet, G.
   Dischler, J-M.
   Ghazanfarpour, D.
TI Multiple kernels noise for improved procedural texturing
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Conference on the Computer Graphics International (CGI)
CY 2012
CL Bournemouth, ENGLAND
DE Procedural textures; Rendering; Noise-based texturing
AB Procedural texturing is a well known method to synthesize details onto virtual surfaces directly during rendering. But the creation of such textures is often a long and painstaking task. This paper introduces a new noise function, called multiple kernels noise. It is characterized by an arbitrary energy distribution in spectral domain. Multiple kernels noise is obtained by adaptively decomposing a user-defined power spectral density (PSD) into rectangular regions. These are then associated to kernel functions used to compute noise values by sparse convolution. We show how multiple kernels noise (1) increases the variety of noisy procedural textures that can be modeled and (2) helps creating structured procedural textures by automatic extraction of noise characteristics from user-supplied samples.
C1 [Gilet, G.; Ghazanfarpour, D.] Univ Limoges, XLIM, F-87000 Limoges, France.
   [Dischler, J-M.] Univ Strasbourg, LSIIT, F-67000 Illkirch Grafenstaden, France.
C3 Universite de Limoges; Universites de Strasbourg Etablissements
   Associes; Universite de Strasbourg
RP Gilet, G (corresponding author), Univ Limoges, XLIM, 123 Ave Albert Thomas, F-87000 Limoges, France.
EM Guillaume.Gilet@unilim.fr; dischler@unistra.fr;
   Djamchid.Ghazanfarpour@unilim.fr
CR [Anonymous], EUROGRAPHICS SHORT
   [Anonymous], STATE ART REPORT EUN
   Bourque E, 2004, COMPUT GRAPH FORUM, V23, P461, DOI 10.1111/j.1467-8659.2004.00777.x
   Cook RL, 2005, ACM T GRAPHIC, V24, P803, DOI 10.1145/1073204.1073264
   Dischler JM, 1997, COMPUT GRAPH FORUM, V16, pC129, DOI 10.1111/1467-8659.00149
   Goldberg A., 2008, Annual Retreat, P1
   Heeger D. J., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P229, DOI 10.1145/218380.218446
   Lagae A, 2010, COMPUT GRAPH FORUM, V29, P2579, DOI 10.1111/j.1467-8659.2010.01827.x
   Lagae A., 2009, SIGGRAPH 2009, P1
   Lagae A, 2010, COMPUT GRAPH-UK, V34, P312, DOI 10.1016/j.cag.2010.05.004
   Lewis J. P., 1989, Computer Graphics, V23, P263, DOI 10.1145/74334.74360
   LEWIS JP, 1987, ACM T GRAPHIC, V6, P167, DOI 10.1145/35068.35069
   Navarro R, 1996, P SOC PHOTO-OPT INS, V2657, P86, DOI 10.1117/12.238704
   Perlin K, 2002, ACM T GRAPHIC, V21, P681, DOI 10.1145/566570.566636
   Perlin K., 1985, Computer Graphics, V19, P287, DOI 10.1145/325165.325247
   Perlin Ken., 2001, Real-Time Shading SIGGRAPH Course Notes
   Worley S., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P291, DOI 10.1145/237170.237267
NR 17
TC 11
Z9 12
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2012
VL 28
IS 6-8
BP 679
EP 689
DI 10.1007/s00371-012-0711-2
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 947EW
UT WOS:000304411500016
DA 2024-07-18
ER

PT J
AU Tang, Y
   Shi, XY
   Xiao, TZ
   Fan, J
AF Tang, Ying
   Shi, Xiaoying
   Xiao, Tingzhe
   Fan, Jing
TI An improved image analogy method based on adaptive CUDA-accelerated
   neighborhood matching framework
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Conference on the Computer Graphics International (CGI)
CY 2012
CL Bournemouth, ENGLAND
DE CUDA; Image analogy; Texture synthesis; Parallel optimization; Synthesis
   magnification
ID TEXTURE SYNTHESIS
AB The image analogy framework is especially useful to synthesize appealing images for non-homogeneous input and gives users creative control over the synthesized results. However, the traditional framework did not adaptively employ the searching strategy based on neighborhood's different textural contents. Besides, the synthesis speed is slow due to intensive computation involved in neighborhood matching. In this paper we present a CUDA-based neighborhood matching algorithm for image analogy. Our algorithm adaptively applies the global search of the exact L (2) nearest neighbor and k-coherence search strategies during synthesis according to different textural features of images, which is especially usefully for non-homogeneous textures. To consistently implement the above two search strategies on GPU, we adopt the fast k nearest neighbor searching algorithm based on CUDA. Such an acceleration greatly reduces the time of the pre-process of k-coherence search and the synthesis procedure of the global search, which makes possible the adjustment of important synthesis parameters. We further adopt synthesis magnification to get the final high-resolution synthesis image for running efficiency. Experimental results show that our algorithm is suitable for various applications of the image analogy framework and takes full advantage of GPU's parallel processing capability to improve synthesis speed and get satisfactory synthesis results.
C1 [Tang, Ying] ZheJiang Univ Technol, Sch Comp Sci, Hangzhou, Zhejiang, Peoples R China.
   [Fan, Jing] ZheJiang Univ Technol, Sch Comp Sci & Technol, Hangzhou, Zhejiang, Peoples R China.
   [Fan, Jing] ZheJiang Univ Technol, Inst Comp Software, Hangzhou, Zhejiang, Peoples R China.
C3 Zhejiang University of Technology; Zhejiang University of Technology;
   Zhejiang University of Technology
RP Fan, J (corresponding author), ZheJiang Univ Technol, Sch Comp Sci, Hangzhou, Zhejiang, Peoples R China.
EM fanjing@zjut.edu.cn
CR [Anonymous], 2009, State of the Art in Example-Based Texture Synthesis R
   [Anonymous], 2001, Schooling for Tomorrow
   Barnes C, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531330
   Bonneel N., 2010, P VIS MOD VIS
   Busto PauPanareda., 2010, Eurographics Workshop on Vision, Modeling Visualization 2010, P81
   Efros A. A., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1033, DOI 10.1109/ICCV.1999.790383
   Efros AA, 2001, COMP GRAPH, P341, DOI 10.1145/383259.383296
   Garcia Vincent, 2008, 2008 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops (CVPR Workshops), P1, DOI 10.1109/CVPRW.2008.4563100
   Hertzmann A, 2001, COMP GRAPH, P327, DOI 10.1145/383259.383295
   Kwatra V, 2005, ACM T GRAPHIC, V24, P795, DOI 10.1145/1073204.1073263
   Kwatra V, 2003, ACM T GRAPHIC, V22, P277, DOI 10.1145/882262.882264
   Lefebvre S, 2005, ACM T GRAPHIC, V24, P777, DOI 10.1145/1073204.1073261
   Lefebvre S, 2006, ACM T GRAPHIC, V25, P541, DOI 10.1145/1141911.1141921
   Lu JY, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1189762.1189765, 10.1145/1186644.1186647]
   Ma CY, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964957
   Mount D., 1997, CGC 2 ANN FALL WORKS
   Muja M, 2009, VISAPP 2009: PROCEEDINGS OF THE FOURTH INTERNATIONAL CONFERENCE ON COMPUTER VISION THEORY AND APPLICATIONS, VOL 1, P331
   Ramanarayanan G, 2007, IEEE T VIS COMPUT GR, V13, P167, DOI 10.1109/TVCG.2007.4
   Risser E, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778822
   Rosenberger A, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618453
   Schmid J, 2011, VISUAL COMPUT, V27, P85, DOI 10.1007/s00371-010-0532-0
   Sivaks E, 2011, COMPUT GRAPH FORUM, V30, P127, DOI 10.1111/j.1467-8659.2010.01837.x
   Tong X, 2002, ACM T GRAPHIC, V21, P665, DOI 10.1145/566570.566634
   Wang B, 2004, IEEE T VIS COMPUT GR, V10, P266, DOI 10.1109/TVCG.2004.1272726
   Wei LY, 2000, COMP GRAPH, P479, DOI 10.1145/344779.345009
   Xiao CX, 2011, IEEE T VIS COMPUT GR, V17, P1122, DOI 10.1109/TVCG.2010.226
   Xu K, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618464
   Zhang FL, 2012, IEEE T VIS COMPUT GR, V18, P1849, DOI 10.1109/TVCG.2012.68
NR 28
TC 6
Z9 9
U1 0
U2 11
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2012
VL 28
IS 6-8
BP 743
EP 753
DI 10.1007/s00371-012-0701-4
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 947EW
UT WOS:000304411500022
DA 2024-07-18
ER

PT J
AU Willcocks, CG
   Li, FWB
AF Willcocks, Chris G.
   Li, Frederick W. B.
TI Feature-varying skeletonization Intuitive control over the target
   feature size and output skeleton topology
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Conference on the Computer Graphics International (CGI)
CY 2012
CL Bournemouth, ENGLAND
DE Automatic; Skeletonization; Contraction; Feature abstraction; Surface
   noise; Topology
ID EXTRACTION; SQUARES
AB Current skeletonization algorithms strive to produce a single centered result which is homotopic and insensitive to surface noise. However, this traditional approach may not well capture the main parts of complex models, and may even produce poor results for applications such as animation. Instead, we approximate model topology through a target feature size omega, where undesired features smaller than omega are smoothed, and features larger than omega are retained into groups called bones. This relaxed feature-varying strategy allows applications to generate robust and meaningful results without requiring additional parameter tuning, even for damaged, noisy, complex, or high genus models.
C1 [Willcocks, Chris G.; Li, Frederick W. B.] Univ Durham, Sch Engn & Comp Sci, Durham DH1 3LE, England.
C3 Durham University
RP Willcocks, CG (corresponding author), Univ Durham, Sch Engn & Comp Sci, Durham DH1 3LE, England.
EM c.g.willcocks@durham.ac.uk
RI Willcocks, Chris G/F-9253-2015; Li, Frederick W. B./AAM-6662-2021
OI Willcocks, Chris G/0000-0001-6821-3924; Li, Frederick W.
   B./0000-0002-4283-4228
FU EPSRC [EP/G009635/1] Funding Source: UKRI
CR [Anonymous], 2010, ANN: a library for approximate nearest neighbor searching
   Arcelli C, 2011, IEEE T PATTERN ANAL, V33, P709, DOI 10.1109/TPAMI.2010.140
   Au O.K.C., 2008, ACM T GRAPHIC, P44, DOI 10.1145/1399504.1360643
   Aujay G, 2007, SYMPOSIUM ON COMPUTER ANIMATION 2007: ACM SIGGRAPH/ EUROGRAPHICS SYMPOSIUM PROCEEDINGS, P151
   BARAN I, 2007, ACM SIGGRAPH 2007 PA, DOI DOI 10.1145/1276377.1276467
   Biasotti S, 2008, THEOR COMPUT SCI, V392, P5, DOI 10.1016/j.tcs.2007.10.018
   Biasotti S, 2008, MATH VIS, P145, DOI 10.1007/978-3-540-33265-7_5
   Boissonnat JD, 2005, GRAPH MODELS, V67, P405, DOI 10.1016/j.gmod.2005.01.004
   Brunner D., 2005, SPRING C COMPUTER GR, P119, DOI DOI 10.1145/1090122.1090143
   Cao J., 2010, SHAP MOD INT C SMI 2, P187, DOI [DOI 10.1109/SMI.2010.25, 10.1109/SMI.2010.25]
   Cornea ND, 2005, VISUAL COMPUT, V21, P945, DOI 10.1007/s00371-005-0308-0
   Cornea ND, 2007, IEEE T VIS COMPUT GR, V13, P530, DOI 10.1109/TVCG.2007.1002
   Desbrun M, 1999, COMP GRAPH, P317, DOI 10.1145/311535.311576
   Dey TK, 2006, S GEOMETRY PROCESSIN, P143
   Hasler N., 2010, P ACM SIGGRAPH S INT, P23, DOI DOI 10.1145/1730804.1730809
   Hassouna MS, 2009, IEEE T PATTERN ANAL, V31, P2257, DOI 10.1109/TPAMI.2008.271
   He Y, 2009, GRAPH MODELS, V71, P49, DOI 10.1016/j.gmod.2008.12.008
   Ho CC, 2005, COMPUT GRAPH FORUM, V24, P537, DOI 10.1111/j.1467-8659.2005.00879.x
   Li FWB, 2011, ACM T MULTIM COMPUT, V7, DOI 10.1145/2000486.2000493
   Lindholm E, 2001, COMP GRAPH, P149, DOI 10.1145/383259.383274
   Liu L, 2010, COMPUT GRAPH FORUM, V29, P2253, DOI 10.1111/j.1467-8659.2010.01814.x
   Marinov M, 2005, COMPUT GRAPH FORUM, V24, P479, DOI 10.1111/j.1467-8659.2005.00873.x
   Miklos B., 2010, SIGGRAPH 10, DOI DOI 10.1145/1833349.1778838
   Ning X., 2010, Proceedings of the 9th ACM SIGGRAPH Conference on Virtual-Reality Continuum and Its Applications in Industry, VRCAI '10, P199
   Oda T, 2006, ICAT 2006: 16TH INTERNATIONAL CONFERENCE ON ARTIFICIAL REALITY AND TELEXISTENCE - WORSHOPS, PROCEEDINGS, P275
   Pantuwong N, 2010, SA, DOI [10.1145/1899950.1899956, DOI 10.1145/1899950.1899956]
   TAGLIASACCHI A, 2009, ACM SIGGRAPH 2009 PA, P1, DOI DOI 10.1145/1576246.1531377
   Wang YS, 2008, IEEE T VIS COMPUT GR, V14, P926, DOI 10.1109/TVCG.2008.38
NR 28
TC 7
Z9 8
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2012
VL 28
IS 6-8
BP 775
EP 785
DI 10.1007/s00371-012-0688-x
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 947EW
UT WOS:000304411500025
DA 2024-07-18
ER

PT J
AU Yang, FL
   Li, BM
AF Yang, Fenglei
   Li, Baomin
TI Unsupervised learning of spatial structures shared among images
SO VISUAL COMPUTER
LA English
DT Article
DE Shared structures; Unsupervised learning
ID RECOGNITION
AB Learning from unlabeled images that contain various objects that change in pose, scale, and degree of occlusion is a challenging task in computer vision. Shared structures embody the consistence and coherence of features that repeatedly cooccur at an object class. They can be used as discriminative information to separate the various objects contained in unlabeled images. In this paper, we propose a maximum likelihood algorithm for unsupervised shared structure learning, where shared structures are represented as the strongly connected clusters of consistent pairwise relationships and shared structures of different order are learned through exploring and combining consistent pairwise spatial relationships. Two routines of sampling data, namely densely sampling and sparsely sampling, are also discussed in our work. We test our algorithm on a diverse set of data to verify its merits.
C1 [Yang, Fenglei] Shanghai Univ, Sch Comp Engn & Sci, Shanghai, Peoples R China.
   [Li, Baomin] E China Normal Univ, Distance Educ Coll, Shanghai 200062, Peoples R China.
C3 Shanghai University; East China Normal University
RP Yang, FL (corresponding author), Shanghai Univ, Sch Comp Engn & Sci, Shanghai, Peoples R China.
EM fengleiy@gmail.com; bmli@dec.ecnu.edu.cn
RI Fenglei, Yang/GXF-6946-2022
CR Agarwal S, 2002, LECT NOTES COMPUT SC, V2353, P113
   [Anonymous], IEEE C CVPR
   [Anonymous], THESIS CALTECH PASAD
   Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236
   FERGUS R, 2003, IEEE C CVPR
   GREEN K, 1995, COMPUT VIS IMAGE UND, V62, P177, DOI 10.1006/cviu.1995.1049
   Heisele B, 2003, COMPUT VIS IMAGE UND, V91, P6, DOI 10.1016/S1077-3142(03)00073-0
   Hoiem D., 2006, IEEE C CVPR
   Leordeanu M., 2005, IEEE C CVPR
   Loeff N., 2005, INT C NIPS
   Parikh D., 2009, IEEE C CVPR
   Pechuk M, 2008, COMPUT VIS IMAGE UND, V110, P173, DOI 10.1016/j.cviu.2007.06.002
   Perrotton X., 2010, IEEE C ICIP
   Rabinovich A., 2007, INT C ICCV
   Rivlin GFE, 2007, COMPUT VIS IMAGE UND, V105, P200, DOI 10.1016/j.cviu.2006.10.003
   Sivic J, 2004, PROC CVPR IEEE, P488
   Weber M, 2000, LECT NOTES COMPUT SC, V1842, P18
NR 17
TC 6
Z9 7
U1 1
U2 10
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2012
VL 28
IS 2
BP 175
EP 180
DI 10.1007/s00371-011-0616-5
PG 6
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 881SW
UT WOS:000299510100004
DA 2024-07-18
ER

PT J
AU Oliveira, GN
   Torchelsen, RP
   Comba, JLD
   Walter, M
   Bastos, R
AF Oliveira, Guilherme N.
   Torchelsen, Rafael P.
   Comba, Joao L. D.
   Walter, Marcelo
   Bastos, Rui
TI Geodesic-driven visual effects over complex surfaces
SO VISUAL COMPUTER
LA English
DT Article
DE Procedural texturing; Geodesics; Displacement mapping; Hardware
   tessellation; Real-time
AB Texture mapping is an important technique for adding visual details to geometric models. Image-based texture mapping is the most popular approach, but it relies on pre-computed images which often limit their use to static effects. For adding dynamic effects, procedural-based texturing is more adequate. Since it rely on functions to describe texturing patterns, procedural texturing allows for a more compact representation and control of visual effects by a simple change of parameters. In this work we describe GeoTextures, an approach that uses geodesic distance fields defined from multiple sources at different locations over a model surface to place, advect, and combine procedural visual effects over complex surfaces. The use of geodesics extends the scope of common procedural textures which are usually limited to using spatial 3D coordinates or 2D texture coordinates. We illustrate the flexibility of our real-time approach with a range of visual effects, such as time-based propagation of weathering phenomena, transparency effects, and mesh displacement over surfaces with smooth silhouettes using hardware based tessellation available in current graphics cards.
C1 [Oliveira, Guilherme N.; Comba, Joao L. D.; Walter, Marcelo] Univ Fed Rio Grande do Sul, Inst Informat, Porto Alegre, RS, Brazil.
   [Torchelsen, Rafael P.] UFFS Univ Fed Fronteira Sul, Chapeco, SC, Brazil.
   [Bastos, Rui] NVIDIA Corp, Porto Alegre, RS, Brazil.
C3 Universidade Federal do Rio Grande do Sul; Universidade Federal da
   Fronteira Sul; Nvidia Corporation
RP Oliveira, GN (corresponding author), Univ Fed Rio Grande do Sul, Inst Informat, Porto Alegre, RS, Brazil.
EM guilherme.n.oliveira@gmail.com; rafael.torchelsen@uffs.edu.br;
   comba@inf.ufrgs.br; marcelo.walter@inf.ufrgs.br; rbastos@nvidia.com
RI Comba, Joao L D/F-6220-2014; Walter, Marcelo/A-1964-2013; Walter,
   Marcelo/O-7526-2019
OI Walter, Marcelo/0000-0002-5634-8765; Dihl Comba, Joao
   Luiz/0000-0003-2921-2130
CR BOMMES D, 2007, VMV
   Bommes D, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531383
   BURLEY B, 2008, EUR S REND
   CATMULL E, 1998, RECURSIVELY GENERATE, DOI DOI 10.1145/280811.280992
   COHENSTEINER D, 2004, SIGGRAPH 04
   Ebert David S, 2003, Texturing Modeling: A Procedural Approach
   González F, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618455
   GU X, 2003, SGP 03
   HEGEMAN K, 2006, GPU BASED CONFORMAL
   Hormann K., 2007, SIGGRAPH Course Notes
   KHODAKOVSKY A, 2003, SIGGRAPH 03
   Lu JY, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1189762.1189765, 10.1145/1186644.1186647]
   MATUSIK W, 2005, ACM SIGGRAPH 2005 PA
   OLIVEIRA GN, 2010, SIBGRAPI C GRAPH PAT
   PEACHEY DR, 1985, SIGGRAPH 85
   PERLIN K, 2001, NOISE HARDWARE, pCH2
   Ray N., 2009, Computer Graphics Forum
   Ray N, 2006, ACM T GRAPHIC, V25, P1460, DOI 10.1145/1183287.1183297
   Sheffer A, 2005, ACM T GRAPHIC, V24, P311, DOI 10.1145/1061347.1061354
   Sheffer A, 2006, FOUND TRENDS COMPUT, V2, P1, DOI 10.1561/0600000011
   STAM J, 2003, ACM SIGGRAPH 2003 PA
   SURAZHSKY V, 2005, ACM SIGGRAPH 2005 PA
   Torchelsen RP, 2009, COMPUT GRAPH FORUM, V28, P1781, DOI 10.1111/j.1467-8659.2009.01555.x
   TORCHELSEN RP, 2010, 13D 10
   WALTER M, 2001, SIGGRAPH 01
   Wang CCL, 2008, IEEE T VIS COMPUT GR, V14, P25, DOI 10.1109/TVCG.2007.1060
   Weber O, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409625.1409626
   WORLEY S, 1996, SIGGRAPH 96
   Xu K, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618454
   Yu QZ, 2009, COMPUT GRAPH FORUM, V28, P239, DOI 10.1111/j.1467-8659.2009.01363.x
   Yuksel C, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1731047.1731053
   Zhang E, 2005, ACM T GRAPHIC, V24, P1, DOI 10.1145/1037957.1037958
NR 32
TC 1
Z9 1
U1 0
U2 1
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2011
VL 27
IS 10
SI SI
BP 917
EP 928
DI 10.1007/s00371-011-0615-6
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 831MU
UT WOS:000295736200005
DA 2024-07-18
ER

PT J
AU Wang, Q
   Sourina, O
   Nguyen, MK
AF Wang, Qiang
   Sourina, Olga
   Minh Khoa Nguyen
TI Fractal dimension based neurofeedback in serious games
SO VISUAL COMPUTER
LA English
DT Article
DE EEG; HCI; BCI; Neurofeedback; Fractal dimension; Game design; Medical
   application
ID HUMAN ELECTROENCEPHALOGRAMS; EEG BIOFEEDBACK; TIME-SERIES; ATTENTION;
   CHILDREN; ADHD
AB EEG-based technology is widely used in serious game design since more wireless headsets that meet consumer criteria for wearability, price, portability, and ease-of-use are coming to the market. Originally, such technologies were mostly used in different medical applications, Brain Computer Interfaces (BCI) and neurofeedback games. The algorithms adopted in such applications are mainly based on power spectrum analysis, which may not be fully revealing the nonlinear complexity of the brain activities. In this paper, we first review neurofeedback games, EEG processing methods, and algorithms, and then propose a new nonlinear fractal dimension based approach to neurofeedback implementation targeting EEG-based serious games design. Only one channel is used in the proposed concentration quantification algorithm. The developed method was compared with other methods used for the concentration level recognition in neurofeedback games. The result analysis demonstrated an efficiency of the proposed approach. We designed and implemented new EEG-based 2D and 3D neurofeedback games that make the process of brain training more enjoyable.
C1 [Wang, Qiang; Sourina, Olga; Minh Khoa Nguyen] Nanyang Technol Univ, Inst Media Innovat, Singapore 637553, Singapore.
   [Wang, Qiang; Sourina, Olga; Minh Khoa Nguyen] Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore 637553, Singapore.
C3 Nanyang Technological University; Nanyang Technological University
RP Wang, Q (corresponding author), Nanyang Technol Univ, Inst Media Innovat, Singapore 637553, Singapore.
EM wang0586@ntu.edu.sg
FU National Research Fund of Singapore [NRF2008-IDM-IDM004-020]; Institute
   for Media Innovation
FX This project is supported by grant NRF2008-IDM-IDM004-020 of National
   Research Fund of Singapore and by Institute for Media Innovation.
CR [Anonymous], SIMPLE DIRECTMEDIA L
   [Anonymous], UNREAL ENGINE
   [Anonymous], 2005, Getting Started with Neurofeedback
   [Anonymous], 2006, P 1 IEEE RAS EMBS IN, DOI DOI 10.1109/BIOROB.2006.1639239
   Birbaumer N, 1999, NEUROSCIENTIST, V5, P74, DOI 10.1177/107385849900500211
   BLOCK A, 1990, PHYS REV A, V42, P1869, DOI 10.1103/PhysRevA.42.1869
   Clarke AR, 2001, PSYCHOPHYSIOLOGY, V38, P212, DOI 10.1017/S0048577201981764
   Coben R, 2010, APPL PSYCHOPHYS BIOF, V35, P83, DOI 10.1007/s10484-009-9117-y
   COWAN JD, 1994, BIOFEEDBACK SELF-REG, V19, P287
   Fawcett T, 2006, PATTERN RECOGN LETT, V27, P861, DOI 10.1016/j.patrec.2005.10.010
   Fuchs T, 2003, APPL PSYCHOPHYS BIOF, V28, P1, DOI 10.1023/A:1022353731579
   Gevensleben H, 2009, INT J PSYCHOPHYSIOL, V74, P149, DOI 10.1016/j.ijpsycho.2009.08.005
   Gevensleben H, 2009, J CHILD PSYCHOL PSYC, V50, P780, DOI 10.1111/j.1469-7610.2008.02033.x
   Hammond D.C., 2007, J. Neurother, V10, P25, DOI [DOI 10.1300/J184V10N04_04, 10.1300/J184v10n0404, DOI 10.1300/J184V10N0404]
   Hanslmayr S, 2005, APPL PSYCHOPHYS BIOF, V30, P1, DOI 10.1007/s10484-005-2169-8
   HIGUCHI T, 1988, PHYSICA D, V31, P277, DOI 10.1016/0167-2789(88)90081-4
   HOMAN RW, 1987, ELECTROEN CLIN NEURO, V66, P376, DOI 10.1016/0013-4694(87)90206-9
   Kerson C., 2009, Journal of Neurofeedback, V13, P146, DOI [10.1080/10874200903107405, DOI 10.1080/10874200903107405]
   Kinsner W, 2005, ICCI 2005: FOURTH IEEE INTERNATIONAL CONFERENCE ON COGNITIVE INFORMATICS - PROCEEDINGS, P58
   Kouijzer MEJ, 2010, RES AUTISM SPECT DIS, V4, P386, DOI 10.1016/j.rasd.2009.10.007
   Kulish V, 2006, COMPUT BIOL MED, V36, P291, DOI 10.1016/j.compbiomed.2004.12.003
   Kulish V, 2006, J MECH MED BIOL, V6, P175, DOI 10.1142/S021951940600187X
   Lécuyer A, 2008, COMPUTER, V41, P66, DOI 10.1109/MC.2008.410
   Li ZH, 2008, BRAIN IMAGING BEHAV, V2, P39, DOI 10.1007/s11682-007-9013-0
   Liu Y, 2010, 2010 INTERNATIONAL CONFERENCE ON CYBERWORLDS (CW 2010), P262, DOI 10.1109/CW.2010.37
   LUBAR JF, 1995, BIOFEEDBACK SELF-REG, V20, P83, DOI 10.1007/BF01712768
   Lutsyuk NV, 2006, NEUROPHYSIOLOGY+, V38, P389, DOI 10.1007/s11062-006-0076-0
   Murray SO, 2004, NAT NEUROSCI, V7, P70, DOI 10.1038/nn1161
   Nunez P.L., 2006, Electric Fields of the Brain, Vsecond
   Pop-Jordanov J, 2009, COGN PROCESS, V10, pS71, DOI 10.1007/s10339-008-0229-8
   Rebsamen B, 2007, IEEE INTELL SYST, V22, P18, DOI 10.1109/MIS.2007.26
   SAXBY E, 1995, J CLIN PSYCHOL, V51, P685, DOI 10.1002/1097-4679(199509)51:5<685::AID-JCLP2270510514>3.0.CO;2-K
   Sokhadze TM, 2008, APPL PSYCHOPHYS BIOF, V33, P1, DOI 10.1007/s10484-007-9047-5
   Sourina O, 2009, IFMBE PROC, V23, P411
   Sourina O, 2009, LECT NOTES COMPUT SC, V5496, P380, DOI 10.1007/978-3-642-01811-4_34
   Thompson L, 2010, APPL PSYCHOPHYS BIOF, V35, P63, DOI 10.1007/s10484-009-9120-3
   Vernon D, 2003, INT J PSYCHOPHYSIOL, V47, P75, DOI 10.1016/S0167-8760(02)00091-0
   WANG Q, 2010, P CGI 2010, pSP25
   Wang Q, 2010, 2010 INTERNATIONAL CONFERENCE ON CYBERWORLDS (CW 2010), P270, DOI 10.1109/CW.2010.56
   WARD J, WHAT GAME ENGINE
   PROJECT PERSONALIZED
NR 41
TC 54
Z9 60
U1 2
U2 48
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2011
VL 27
IS 4
SI SI
BP 299
EP 309
DI 10.1007/s00371-011-0551-5
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 739IC
UT WOS:000288707700006
DA 2024-07-18
ER

PT J
AU Park, MJ
AF Park, Min Je
TI Guiding flows for controlling crowds
SO VISUAL COMPUTER
LA English
DT Article
DE Crowd simulation; Key-frame animation; Interactive control
ID MOTION; NAVIGATION; ANIMATION
AB In this paper, we present a novel method for controlling massive crowds by using control particles. Our method differs from previous ones that generate attraction (or repelling) forces around the control particles. Instead of doing this, we create a steady-state, flow-like control field that guides the crowd to move along with the control particles. Our control field can be naturally incorporated into the original simulation by using density-based weighted blending. Although we focus on simulation methods that use dynamic potential functions, our method can also be used to improve the controllability of agent-based simulation methods. Since the control particles can be easily manipulated by traditional key-framing, our method provides animators with an intuitive interface for manipulating the position of crowd over time. We illustrate the effectiveness of our method on several examples.
C1 Macrograph Co, Seoul, South Korea.
RP Park, MJ (corresponding author), Macrograph Co, Seoul, South Korea.
EM mjpark@macrograph.co.kr
CR Chenney Stephen., 2004, Proceedings of the 2004 ACM SIGGRAPH/Euro- graphics symposium on Computer animation, P233, DOI [10.1145/1028523.1028553, DOI 10.1145/1028523.1028553.]
   Courty N, 2007, COMPUT ANIMAT VIRT W, V18, P361, DOI 10.1002/cav.199
   FUNGE J, 1999, ACM T GRAPHIC, V18, P29
   GOLDENSTEIN S, 2001, COMPUT GRAPH, V11, P111
   Hong JM, 2004, COMPUT ANIMAT VIRT W, V15, P147, DOI 10.1002/cav.17
   Hughes RL, 2003, ANNU REV FLUID MECH, V35, P169, DOI 10.1146/annurev.fluid.35.101101.161136
   Jin XG, 2008, IEEE COMPUT GRAPH, V28, P37, DOI 10.1109/MCG.2008.117
   KWON T, 2008, ACM T GRAPHIC, V27, P111
   Lamarche F, 2004, COMPUT GRAPH FORUM, V23, P509, DOI 10.1111/j.1467-8659.2004.00782.x
   LEE KH, 2007, EUR ACM SIGGRAPH S C, P111
   Lerner A., 2009, SCA, ACM, P199, DOI DOI 10.1145/1599470.1599496
   Lerner A, 2007, COMPUT GRAPH FORUM, V26, P655, DOI 10.1111/j.1467-8659.2007.01089.x
   Lin MC, 2008, LECT NOTES COMPUT SC, V5277, P23
   McNamara A, 2004, ACM T GRAPHIC, V23, P449, DOI 10.1145/1015706.1015744
   Musse SR, 2007, COMPUT ANIMAT VIRT W, V18, P83, DOI 10.1002/cav.163
   MUSSE SR, 1997, COMPUTER ANIMATION S, P39
   Park SI, 2004, COMPUT ANIMAT VIRT W, V15, P125, DOI 10.1002/cav.15
   PETTRE J, 2005, 1 INT WORKSH CROWD S
   Press W. H, 2001, NUMERICAL RECIPES C
   Reynolds C. W., 1999, P GAM DEV C, P763
   Reynolds CW., 1987, SIGGRAPH Comput. Graph., V21, P25, DOI [10.1145/37402.37406, DOI 10.1145/37402.37406]
   Shao W, 2007, GRAPH MODELS, V69, P246, DOI 10.1016/j.gmod.2007.09.001
   Shi L, 2005, ACM T GRAPHIC, V24, P140, DOI 10.1145/1037957.1037965
   Sung M, 2004, COMPUT GRAPH FORUM, V23, P519, DOI 10.1111/j.1467-8659.2004.00783.x
   SUNG M., 2005, SCA 05, P291
   TREUILLE A, 2006, ACM T GRAPHIC, V11, P111
   WEJCHERT J, 2004, INT C COMP GRAPH INT, P19
   WOJTAN C, 2006, EUR ACM SIGGRAPH S C, P1
NR 28
TC 9
Z9 10
U1 0
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2010
VL 26
IS 11
BP 1383
EP 1391
DI 10.1007/s00371-009-0415-4
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 653UV
UT WOS:000282123700005
DA 2024-07-18
ER

PT J
AU Böttcher, G
   Allerkamp, D
   Wolter, FE
AF Boettcher, Guido
   Allerkamp, Dennis
   Wolter, Franz-Erich
TI Multi-rate coupling of physical simulations for haptic interaction with
   deformable objects
SO VISUAL COMPUTER
LA English
DT Article
DE Virtual reality; Multi-rate model; Haptic rendering; Physical simulation
ID CONTACT
AB Real-time simulation of deformable objects involves many computational challenges to be solved, particularly in the context of haptic applications, where high update rates are necessary for obtaining a satisfying experience. The required performance can generally be achieved by introducing an intermediate layer responsible for the simulation of the small part of the surface being in contact with the fingers. In this paper, we present an algorithm controlling the run-time of the concurrent simulation threads. It uses information from previous simulation steps to estimate the time spent in the simulation operation considering also changes in the geometry of the intermediate layer. The introduction of such a local contact simulation introduces damping to the overall system. Its effect on the dynamics of the simulation system is experimentally analysed with an interaction test.
C1 [Boettcher, Guido; Allerkamp, Dennis; Wolter, Franz-Erich] Leibniz Univ Hannover, Welfenlab Comp Graph Div, Inst Man Machine Commun, Hannover, Germany.
C3 Leibniz University Hannover
RP Böttcher, G (corresponding author), Leibniz Univ Hannover, Welfenlab Comp Graph Div, Inst Man Machine Commun, Hannover, Germany.
EM boettcher@welfenlab.de; allerkamp@welfenlab.de; few@welfenlab.de
RI Wolter, Franz-Erich/JAC-5956-2023; Wolter, Franz-Erich/AAV-3008-2020;
   Wolter, Franz - Erich/B-1672-2014
OI Wolter, Franz-Erich/0000-0002-2293-5494; Wolter,
   Franz-Erich/0000-0002-2293-5494; 
FU European Union [IST-6549]; Future and Emerging Technologies (FET)
   Programme, which is part of the Information Society Technologies (IST)
   programme
FX The work was partly conducted inside the project "HAPtic sensing of
   virtual TEXtiles" (HAPTEX) which is a research project funded under the
   Sixth Framework Programme (FP6) of the European Union (Contract Nr.
   IST-6549). The funding is provided by the Future and Emerging
   Technologies (FET) Programme, which is part of the Information Society
   Technologies (IST) programme and focuses on novel and emerging
   scientific ideas. Its mission is to promote research that is of a
   long-term nature or involves particularly high risks, compensated by the
   potential of a significant societal or industrial impact.
CR Anderson E., 1999, LAPACK USERSGUIDE, Vthird
   ASTLEY O, 1997, IEEE RSJ INT C INT R
   BALANIUK R, 1999, PUG99
   Barbagli F, 2003, 11TH SYMPOSIUM ON HAPTIC INTERFACES FOR VIRTUAL ENVIRONMENT AND TELEOPERATOR SYSTEMS - HAPTICS 2003, PROCEEDINGS, P109, DOI 10.1109/HAPTIC.2003.1191248
   Böttcher G, 2008, VISUAL COMPUT, V24, P911, DOI 10.1007/s00371-008-0287-z
   BroNielsen M, 1996, COMPUT GRAPH FORUM, V15, pC57
   CAVUSOGLU MC, 2000, ROBOTICS AUTOMATION, V3, P2458, DOI DOI 10.1109/ROBOT.2000.846397
   DURIEZ C, 2004, INT ROB SYST 2004 IR, V4, P3232, DOI DOI 10.1109/IR0S.2004.1389915
   GIERE L, 2007, PRAKONDITIONIERUNG K
   GLOCKNER D, 2008, THESIS LEIBNIZ U HAN
   JAMES DL, 2005, ACM SIGGRAPH 2005 CO, P141
   LEE M, 2003, IEEE INT C SYST MAN, V4
   Magnenat-Thalmann N., 2007, The International Journal of Virtual Reality, V6, P35
   Mahvash M, 2004, IEEE COMPUT GRAPH, V24, P48, DOI 10.1109/MCG.2004.1274061
   MAZZELLA F, 2002, ROBOTICS AUTOMATION, V1, P939, DOI DOI 10.1109/ROBOT.2002.1013477
   RUSPINI DC, 1997, IEEE INT C INT ROBOT, V1, P128, DOI DOI 10.1109/IROS.1997.649024
   VOLINO P, 2005, COMPUT ANIMAT VIRTUA, V16
   VOLINO P, 2005, COMPUT AIDED DESIGN, V2
   Zhuang Y., 2000, Proceedings 2000 ICRA. Millennium Conference. IEEE International Conference on Robotics and Automation. Symposia Proceedings (Cat. No.00CH37065), P2428, DOI 10.1109/ROBOT.2000.846391
   ZILLES CB, 1995, IROS '95 - 1995 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS: HUMAN ROBOT INTERACTION AND COOPERATIVE ROBOTS, PROCEEDINGS, VOL 3, P146, DOI 10.1109/IROS.1995.525876
NR 20
TC 7
Z9 8
U1 1
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2010
VL 26
IS 6-8
BP 903
EP 914
DI 10.1007/s00371-010-0450-1
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 602JQ
UT WOS:000278135800050
DA 2024-07-18
ER

PT J
AU Iacob, R
   Mitrouchev, P
   Léon, JC
AF Iacob, Robert
   Mitrouchev, Peter
   Leon, Jean-Claude
TI Contact identification for assembly-disassembly simulation with a haptic
   device
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 4th INTUITION International Conference and Workshop
CY OCT 04-05, 2007
CL Athens, GREECE
DE Assembly/disassembly simulation; Contacts identification; Haptic device
ID ENVIRONMENT; REALITY
AB Assembly/Disassembly (A/D) simulations using haptic devices are facing difficulties while simulating insertion/extraction operations, such as removing cylinders from holes. In order to address this configuration as well as others, an approach based on contact identification between components is presented in this paper. This approach can efficiently contribute either to a new A/D simulation preparation process relying on two types of shape representations (mesh and CAD NURBS models), or directly to the real time simulation process when it is performed with 6D haptic devices. The model processing pipeline is described and illustrated to show how information can be propagated and used for contact detection. Then, the contact identification process is introduced and illustrated through an example.
C1 [Iacob, Robert; Mitrouchev, Peter; Leon, Jean-Claude] Grenoble INP UJF CNRS, G SCOP Lab, F-38031 Grenoble 1, France.
C3 Communaute Universite Grenoble Alpes; Institut National Polytechnique de
   Grenoble; Universite Grenoble Alpes (UGA); Centre National de la
   Recherche Scientifique (CNRS)
RP Iacob, R (corresponding author), Grenoble INP UJF CNRS, G SCOP Lab, 46 Av Felix Viallet, F-38031 Grenoble 1, France.
EM Robert.Iacob@gmail.com; Peter.Mitrouchev@g-scop.inpg.fr;
   Jean-Claude.Leon@inpg.fr
CR Coma O, 2003, COMPUT AIDED DESIGN, V35, P1193, DOI 10.1016/S0010-4485(03)00026-5
   EDWARDS GW, 2004, VIRTUAL REAL J, V8, P231
   Graf H., 2002, P ASME DES ENG TECH, P249
   HAMRI O, 2006, P VIRT CONC C CANC M, V2
   HOWARD BM, 2007, VIRTUAL REAL J, V10, P33
   IACOB R, 2007, P ASME INT DES ENG T
   Jung B, 2003, LECT NOTES COMPUT SC, V2669, P721
   LIM T, 2007, VIRTUAL REAL J, V10, P135
   Liu ZY, 2007, INT J ADV MANUF TECH, V32, P797, DOI 10.1007/s00170-005-0382-5
   Raghavan V, 1999, IEEE T ROBOTIC AUTOM, V15, P435, DOI 10.1109/70.768177
   Sun H., 2002, Virtual Reality, V6, P11, DOI 10.1007/BF01408565
   WAN H, 2004, P ASME DES ENG TECHN
   Zhong YM, 2005, VISUAL COMPUT, V21, P17, DOI 10.1007/s00371-004-0268-9
NR 13
TC 13
Z9 13
U1 0
U2 5
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2008
VL 24
IS 11
BP 973
EP 979
DI 10.1007/s00371-008-0275-3
PG 7
WC Computer Science, Software Engineering
WE Conference Proceedings Citation Index - Science (CPCI-S); Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 359BQ
UT WOS:000259961600007
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Syllebranque, C
   Boivin, S
AF Syllebranque, Cedric
   Boivin, Samuel
TI Estimation of mechanical parameters of deformable solids from videos
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 4th INTUITION International Conference and Workshop
CY OCT 04-05, 2007
CL Athens, GREECE
DE Identification; Inverse mechanic; Video comparison metrics; Force
   capture device; Soft tissue simulation
ID YOUNGS MODULUS; SOFT-TISSUES; BRAIN-TISSUE; MODELS
AB In this paper, we present a new method to estimate the mechanical parameters of soft bodies directly from videos of solids getting deformed under external user action. Our method requires one standard camera, a deformable solid made of homogeneous material, and a regular light source. We make estimations using an inverse method based on a quasi-static FEM simulation and a visual error metric. The result is a set of two parameters, the Young modulus and the Poisson ratio, that can be used for more complex simulations, or force feedback applications like virtual surgery, for example. We also present a new device for capturing the external forces applied on the deformable solids.
C1 [Syllebranque, Cedric; Boivin, Samuel] ALCOVE Project, INRIA, F-59000 Lille, France.
C3 Inria
RP Syllebranque, C (corresponding author), ALCOVE Project, INRIA, F-59000 Lille, France.
EM syllebra@lifl.fr; samuel.boivin@inria.fr
CR [Anonymous], MOT CAPT SYST
   [Anonymous], ARTOOLKIT
   *ASC TECHN CORP, FLOCK BIRDS
   Bajka, 2001, P MED IM COMP COMP A, V2208, P128
   Baraff D., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P43, DOI 10.1145/280814.280821
   Becker M., 2007, Proc. of Simulation and Visualization, P15
   Belytschko T, 2003, INT J NUMER METH ENG, V56, P609, DOI 10.1002/nme.686
   Benech N, 2005, J ACOUST SOC AM, V117, P3424, DOI 10.1121/1.1920147
   Bhat Kiran S., 2003, Proceedings of the 2003 ACM SIGGRAPH/Eurographics symposium on Computer animation, P37
   Boivin S, 2001, COMP GRAPH, P107, DOI 10.1145/383259.383270
   Bruyns C, 2002, LECT NOTES COMPUT SC, V2488, P282
   Chen EJ, 1996, IEEE T ULTRASON FERR, V43, P191, DOI 10.1109/58.484478
   Delalleau A, 2006, J BIOMECH, V39, P1603, DOI 10.1016/j.jbiomech.2005.05.001
   Duriez C, 2006, IEEE T VIS COMPUT GR, V12, P36, DOI 10.1109/TVCG.2006.13
   FREEMAN WT, 1991, IEEE T PATTERN ANAL, V13, P891, DOI 10.1109/34.93808
   Hall TJ, 1997, IEEE T ULTRASON FERR, V44, P1355, DOI 10.1109/58.656639
   *HAWL PACK, MP 3130 VID PROJ
   *INSTR, UN MAT TEST MACH
   KIRKPATRICK S, 1983, SCIENCE, V220, P671, DOI 10.1126/science.220.4598.671
   Lang JC, 2002, INT J ROBOT RES, V21, P713, DOI 10.1177/027836402761412458
   Liu HF, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P289, DOI 10.1109/ICCV.2003.1238357
   Miller K, 1997, J BIOMECH, V30, P1115, DOI 10.1016/S0021-9290(97)00092-4
   OSTRANDER LE, 1992, ENG MED BIOL SOC, V14, P107
   Pai DK, 2000, LECT NOTES CONTR INF, V250, P391
   REVELL J, 2005, THESIS U BRISTOL
   Samani A, 2001, IEEE T MED IMAGING, V20, P877, DOI 10.1109/42.952726
   Samani A, 2007, PHYS MED BIOL, V52, P1247, DOI 10.1088/0031-9155/52/5/003
   *SCAIME, K1107 FORC SENS
   *SENSABLE TECHN, PHANT SER
   *SMOOTH ON, EC LIQ RUBB
   *SON, HAND DCR TRV950E
   Soza G, 2005, INT J MED ROBOT COMP, V1, P87, DOI 10.1002/rcs.32
   Yu YZ, 1999, COMP GRAPH, P215
   Zhang M, 1997, MED ENG PHYS, V19, P512, DOI 10.1016/S1350-4533(97)00017-9
   *ZWICK, UN HARDN TEST
NR 35
TC 11
Z9 13
U1 0
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2008
VL 24
IS 11
BP 963
EP 972
DI 10.1007/s00371-008-0273-5
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 359BQ
UT WOS:000259961600006
DA 2024-07-18
ER

PT J
AU Jang, H
   Han, J
AF Jang, Hanyoung
   Han, JungHyun
TI Fast collision detection using the A-buffer
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 26th International Conference on Computer Graphics
CY JUN 09-11, 2008
CL Istanbul, TURKEY
DE real-time collision detection; A-buffer; deformable object
AB This paper presents a novel and fast image-space collision detection algorithm with the A-buffer, where the GPU computes the potentially colliding sets (PCSs), and the CPU performs the standard triangle intersection test. When the bounding boxes of two objects intersect, the intersection is passed to the GPU. The object surfaces in the intersection are rendered into the A-buffer. Rendering into the A-buffer is up to eight-times faster than the ordinary approaches. Then, PCSs are computed by comparing the depth values of each texel of the A-buffer. A PCS consists of only two triangles. The PCSs are read back to the CPU, and the CPU computes the intersection points between the triangles. The proposed algorithm runs extremely fast, does not require any preprocessing, can handle dynamic objects including deformable and fracturing models, and can compute self-collisions. Such versatility and performance gain of the proposed algorithm prove its usefulness in real-time applications such as 3D games.
C1 [Jang, Hanyoung; Han, JungHyun] Korea Univ, Game Res Ctr, Seoul, South Korea.
C3 Korea University
RP Han, J (corresponding author), Korea Univ, Game Res Ctr, Seoul, South Korea.
EM jhan@korea.ac.kr
CR [Anonymous], 1997, J GRAPH TOOLS, DOI DOI 10.1080/10867651.1997.10487480
   Baciu G, 1999, J VISUAL COMP ANIMAT, V10, P181, DOI 10.1002/(SICI)1099-1778(199910/12)10:4<181::AID-VIS211>3.0.CO;2-Q
   Bavoil L, 2007, I3D 2007: ACM SIGGRAPH SYMPOSIUM ON INTERACTIVE 3D GRAPHICS AND GAMES, PROCEEDINGS, P97
   Blythe D, 2006, ACM T GRAPHIC, V25, P724, DOI 10.1145/1141911.1141947
   Gottschalk S., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P171, DOI 10.1145/237170.237244
   Govindaraju NK, 2005, ACM T GRAPHIC, V24, P991, DOI 10.1145/1073204.1073301
   Govindaraju NK, 2005, P IEEE VIRT REAL ANN, P59
   GOVINDARAJU NK, 2003, HWWS 03, P25
   GOVINDARAJU NK, 2004, VRST 04 P ACM S VIRT, P2
   GRAND SL, 2007, GPU GEMS, V3, P697
   Harris M., 2007, GPU GEMS, V3, P851
   Heidelberger B, 2003, VISION, MODELING, AND VISUALIZATION 2003, P461
   Heidelberger B., 2004, PROC WSCG, P145
   HOFF K., 2002, TR02004 U N CAR DEP
   Horn D., 2005, GPU GEMS 2, P573
   Hubbard P. M., 1993, Proceedings IEEE 1993 Symposium on Research Frontiers in Virtual Reality (Cat. No.93TH0585-0), P24, DOI 10.1109/VRAIS.1993.378267
   JANG H, 2007, ISVC 07, V4841, P66
   Klosowski JT, 1998, IEEE T VIS COMPUT GR, V4, P21, DOI 10.1109/2945.675649
   LIU B, 2006, MSRTR200681
   Mezger J, 2003, WSCG'2003, VOL 11, NO 2, CONFERENCE PROCEEDINGS, P322
   MYERS K, 2007, SIGGRAPH 07, P21
   MYSZKOWSKI K, 1995, VISUAL COMPUT, V11, P497, DOI 10.1007/BF02439645
   PALMER IJ, 1995, COMPUT GRAPH FORUM, V14, P105, DOI 10.1111/1467-8659.1420105
   Shinya M., 1991, Journal of Visualization and Computer Animation, V2, P132, DOI 10.1002/vis.4340020408
   Taosong He, 1999, Proceedings 1999 Symposium on Interactive 3D Graphics, P55
   Teschner M, 2003, VISION, MODELING, AND VISUALIZATION 2003, P47
   Vassilev T, 2001, COMPUT GRAPH FORUM, V20, pC260, DOI 10.1111/1467-8659.00518
   ZACHMANN G, 1995, P SIVE 95, P104
   Zhang XY, 2007, IEEE T VIS COMPUT GR, V13, P318, DOI 10.1109/TVCG.2007.42
NR 29
TC 8
Z9 8
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2008
VL 24
IS 7-9
BP 659
EP 667
DI 10.1007/s00371-008-0246-8
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 322OI
UT WOS:000257384800021
DA 2024-07-18
ER

PT J
AU Saito, S
   Kani, A
   Chang, Y
   Nakajima, M
AF Saito, Suguru
   Kani, Akane
   Chang, Youngha
   Nakajima, Masayuki
TI Curvature-based stroke rendering
SO VISUAL COMPUTER
LA English
DT Article
DE line stylization; line rendering; curvature; stroke; NPR
AB This paper describes an algorithm that renders lines that have various thicknesses and have sharp tapered ends. This algorithm does not require any special information on each local point of a line. The thickness is determined by curvature and lengths from both ends. Therefore the algorithm is applicable in a variety of line rendering situations, such as 3D rendering engines for high quality cel-animation-like effects, reuse of geometrical data designed by CAD for advertising purposes, edge enhancement in a photo retouching process with edge detection methods and so on. In addition, using the generated varying thicknesses, we have developed algorithms for shading and embossing effects.
C1 Tokyo Inst Technol, Dept Comp Sci, Tokyo 152, Japan.
   Tokyo Inst Technol, Dept Imaging Sci & Engn, Yokohama, Kanagawa 227, Japan.
C3 Tokyo Institute of Technology; Tokyo Institute of Technology
RP Saito, S (corresponding author), Tokyo Inst Technol, Dept Comp Sci, Tokyo 152, Japan.
EM suguru@img.cs.titech.ac.jp; kania@img.cs.titech.ac.jp;
   chang@img.cs.titech.ac.jp; nakajima@img.cs.titech.ac.jp
CR AGUI T, 1982, T IEICE JD, V65, P1243
   Freeman WT, 2003, ACM T GRAPHIC, V22, P33, DOI 10.1145/588272.588277
   GIRSHICK A, 2000, P 1 INT S NONPH AN R, P43, DOI DOI 10.1145/340916.340922
   GOOCH B, 2001, NON PHOTOREALISTIC R
   HERTZMANN A, 2002, P 13 EUR WORKSH REND, P233
   HSU SC, 1994, P SIGGRAPH 94 ACM BO
   NAKAJIMA N, 2001, IPSJ SIGNOTES 2001 C, P33
   PUDET T, 1994, COMPUT GRAPH FORUM, V13, P205
   STRASSMANN S, P SIGGRAPH 86, V225, P86
   Strothotte T., 2002, NON PHOTOREALISTIC C
   STROTHOTTE T, 2002, NON PHOTOREALISTIC S, P94
   Yoshimoto F., 2002, Proceedings of Second IASTED International Conference Visualization, Imaging, and Image Processing, P276
   Zander J, 2004, COMPUT GRAPH FORUM, V23, P421, DOI 10.1111/j.1467-8659.2004.00773.x
   [No title captured]
NR 14
TC 11
Z9 13
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA 233 SPRING STREET, NEW YORK, NY 10013 USA
SN 0178-2789
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2008
VL 24
IS 1
BP 1
EP 11
DI 10.1007/s00371-007-0165-0
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 234GU
UT WOS:000251149500001
DA 2024-07-18
ER

PT J
AU Li, X
   Deng, JS
   Chen, FL
AF Li, Xin
   Deng, Jiansong
   Chen, Falai
TI Surface modeling with polynomial splines over hierarchical T-meshes
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 10th International Conference on Computer-Aided Design and Graphics
CY OCT 15-18, 2007
CL Beijing `, PEOPLES R CHINA
SP China Comp Federat
DE surface modeling; spline; hierarchical T-mesh; stitching
AB Computer graphics and computer-aided design communities prefer piecewise spline patches to represent surfaces. But keeping the smoothness between the adjacent patches is a challenging task. In this paper, we present a method for stitching several surface patches, which is a key step in complicated surface modeling, with polynomial splines over hierarchical T-meshes (PHT-spline for short). The method is simple and can be easily applied to complex surface modeling. With the method, spline surfaces can be constructed efficiently and adaptively to fit genus-zero meshes after their spherical parameterization is obtained, where only small sized linear systems of equations are involved.
C1 Univ Sci & Technol China, Dept Math, Anhua 230026, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS
RP Deng, JS (corresponding author), Univ Sci & Technol China, Dept Math, Anhua 230026, Peoples R China.
EM dengjs@ustc.edu.cn; chenfl@ustc.edu.cn
RI Li, Xin/B-1324-2012; Chen, Falai/C-3846-2013; Deng, Jiansong/F-1869-2010
OI Li, Xin/0000-0003-0477-7098; Deng, Jiansong/0000-0003-4093-373X
CR [Anonymous], 1996, PRIOR ANAL
   [Anonymous], ACM T GRAPH
   DENG J, 2006, IN PRESS GRAPH MODEL
   Deng JS, 2006, J COMPUT APPL MATH, V194, P267, DOI 10.1016/j.cam.2005.07.009
   Eck M, 1995, P 22 ANN C COMP GRAP, P173, DOI DOI 10.1145/218380.218440
   FARIN G, 2002, CURV SURF CAGD PRACT
   Forsey D. R., 1988, Computer Graphics, V22, P205, DOI 10.1145/378456.378512
   Gotsman C, 2003, ACM T GRAPHIC, V22, P358, DOI 10.1145/882262.882276
   LEE SL, 1991, ACM T GRAPHIC, V10, P342, DOI 10.1145/116913.116915
   Sederberg TW, 2004, ACM T GRAPHIC, V23, P276, DOI 10.1145/1015706.1015715
   Wang R-H., 2001, MULTIVARIATE SPLINE
NR 11
TC 54
Z9 60
U1 3
U2 17
PU SPRINGER
PI NEW YORK
PA 233 SPRING STREET, NEW YORK, NY 10013 USA
SN 0178-2789
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2007
VL 23
IS 12
BP 1027
EP 1033
DI 10.1007/s00371-007-0170-3
PG 7
WC Computer Science, Software Engineering
WE Conference Proceedings Citation Index - Science (CPCI-S); Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 232EP
UT WOS:000251001400008
DA 2024-07-18
ER

PT J
AU Tan, YH
   Hua, J
   Dong, M
AF Tan, Yunhao
   Hua, Jing
   Dong, Ming
TI 30 reconstruction from 2D images with hierarchical continuous simplices
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 25th Computer Graphics International Conference (CGI)
CY MAY 30-JUN 02, 2007
CL Petropolis, BRAZIL
DE 3D reconstruction; DMS-splines; hierarchical simplices; data fitting
ID REGISTRATION; TOMOGRAPHY
AB This paper presents an effective framework for the reconstruction of volumetric data from a sequence of 2D images. The 2D images are first aligned to generate an initial 3D volume, followed by the creation of a tetrahedral domain using the Carver algorithm. The resulting tetrahedralization preserves both the geometry and topology of the original dataset. Then a solid model is reconstructed using simplex splines with fitting and faring procedures. The reconstructed heterogenous volumetric model can be quantitatively analyzed and easily visualized. Our experiments demonstrated that our approach can achieve high accuracy in the data reconstruction. The novel techniques and algorithms proposed in this paper can be applied to reconstruct a heterogeneous solid model with complex geometry and topology from other visual data.
C1 Wayne State Univ, Dept Comp Sci, Detroit, MI 48202 USA.
C3 Wayne State University
RP Hua, J (corresponding author), Wayne State Univ, Dept Comp Sci, 5143 Cass Ave,431 State Hall, Detroit, MI 48202 USA.
EM tanyunhao@wayne.edu; jinghua@wayne.edu; mdong@wayne.edu
RI Tan, Yunhao/K-7442-2016
CR Bardinet E, 2002, 2002 IEEE INTERNATIONAL SYMPOSIUM ON BIOMEDICAL IMAGING, PROCEEDINGS, P329, DOI 10.1109/ISBI.2002.1029260
   Chan HM, 2003, LECT NOTES COMPUT SC, V2717, P301
   Choi SM, 1997, IEEE T MED IMAGING, V16, P887
   Cignoni P, 2000, IEEE VISUAL, P85, DOI 10.1109/VISUAL.2000.885680
   CIGNONI P., 1994, P 1994 S VOLUME VISU, P19
   DAHMEN W, 1992, MATH COMPUT, V59, P97, DOI 10.2307/2152982
   Edelsbrunner H., 2001, Geometry and Topology for Mesh Generation
   GREINER G, 1994, IEEE COMPUT GRAPH, V14, P56, DOI 10.1109/38.267471
   Hua J, 2005, J COMPUT INF SCI ENG, V5, P149, DOI 10.1115/1.1881352
   Hua J, 2004, IEEE T VIS COMPUT GR, V10, P574, DOI 10.1109/TVCG.2004.28
   Hua J, 2002, IEEE/ACM SIGGRAPH SYMPOSIUM ON VOLUME VISUALIZATION AND GRAPHICS 2002, PROCEEDINGS, P55, DOI 10.1109/SWG.2002.1226510
   Hua J., 2004, P 9 ACM S SOLID MODE, P47
   LEE S, 2006, P IEEE C COMP VIS PA, V2, P2221
   Lee SC, 2005, P SOC PHOTO-OPT INS, V5747, P170, DOI 10.1117/12.595699
   Looney RJ, 2002, ARTHRITIS RES, V4, P59, DOI 10.1186/ar384
   Martin W., 2001, Proceedings of Seventh ACM Symposium on Solid Modeling and Applications, P234
   Pauly M, 2003, ACM T GRAPHIC, V22, P641, DOI 10.1145/882262.882319
   Pauly M, 2002, VIS 2002: IEEE VISUALIZATION 2002, PROCEEDINGS, P163, DOI 10.1109/VISUAL.2002.1183771
   PFEIFLE R, 1995, GRAPH INTER, P26
   Raviv A., 1999, Proceedings of Fifth ACM Symposium on Solid Modeling and Applications, P246
   Rosa R, 2005, IEEE T NUCL SCI, V52, P400, DOI 10.1109/TNS.2005.843641
   Roxborough T, 2000, IEEE VISUAL, P93, DOI 10.1109/VISUAL.2000.885681
   Schmitt B., 2001, Proceedings of Sixth ACM Symposium on. Solid Modeling and Applications, P321
   TAN Y, 2007, P IEEE INT S BIOM IM
   Trotts IJ, 1999, IEEE T VIS COMPUT GR, V5, P224, DOI 10.1109/2945.795214
   YAO J, 2000, P 3 INT C MED IM COM, P531
NR 26
TC 9
Z9 11
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2007
VL 23
IS 9-11
SI SI
BP 905
EP 914
DI 10.1007/s00371-007-0157-0
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 206UE
UT WOS:000249207600031
DA 2024-07-18
ER

PT J
AU Pronost, N
   Dumont, G
AF Pronost, Nicolas
   Dumont, Georges
TI Dynamics-based analysis and synthesis of human locomotion
SO VISUAL COMPUTER
LA English
DT Article
DE motion analysis and synthesis; kinematical influences; physical realism;
   virtual human
AB One of the best ways to synthesize realistic human motions is to animate characters from captured motion data that inherently respect motion laws. Retargeting and interpolation methods are often used to adapt these motions to different representations of the character and to various environmental constraints but they may introduce physical inaccuracies, although the synthesized motions are natural looking. This paper presents a method for evaluating the physical correctness of retargeted and interpolated locomotions using an inverse dynamics analysis. Furthermore, we propose to improve an initial database with analysed motions that are synthesized again by using a forward dynamics approach.
   The analysis algorithm consists in determining the resulting forces and torques at joints. With this intention, we develop an automatic creation process of the mass/inertia model of the character. Then using support phase recognition, we compute resulting forces and torques by an inverse dynamics method. The retargeting and the interpolation methods change the physics of the motions. This change is evaluated by using the results of our analysis on artificial and real motions and by using literature results and experimental data from force plates. The evaluation relies on the study of several retargeting and interpolation parameters such as the global size of the character or the structure of the model. The output of this evaluation, the resulting forces and torques at joints, are used to produce physically valid motions by using forward dynamics simulation. With this purpose, we introduce forces and torques normalizations, and finally the synthesized motions may improve the initial database.
C1 Univ Rennes 1, IRISA, UMR 6074, F-35014 Rennes, France.
   ENS Cachan, IRISA, UMR 6074, Cachan, France.
C3 Universite de Rennes; Universite Paris Saclay
RP Pronost, N (corresponding author), Univ Rennes 1, IRISA, UMR 6074, F-35014 Rennes, France.
EM Nicolas.Pronost@irisa.fr
RI DUMONT, Georges/K-8173-2013
OI DUMONT, Georges/0000-0002-0709-0921; PRONOST,
   NICOLAS/0000-0003-4499-509X
CR Abe Y., 2004, P 2004 ACM SIGGRAPHE, P173
   [Anonymous], 1999, HUMAN KINETICS
   APKARIAN J, 1989, J BIOMECH, V22, P143, DOI 10.1016/0021-9290(89)90037-7
   ARIKAN O, 2005, ACM SIGGR EUR S COMP, P59
   Arnaldi B., 1989, Visual Computer, V5, P22, DOI 10.1007/BF01901478
   Bindiganavale R, 1998, LECT NOTES ARTIF INT, V1537, P70
   de Leva P, 1996, J BIOMECH, V29, P1223, DOI 10.1016/0021-9290(95)00178-6
   Hodgins JK, 1998, SCI AM, V278, P64, DOI 10.1038/scientificamerican0398-64
   KHALIL W, 1986, ICRA 86, P75
   Ko H, 1996, IEEE COMPUT GRAPH, V16, P50, DOI 10.1109/38.486680
   Popovic Z, 1999, COMP GRAPH, P11, DOI 10.1145/311535.311536
   Pronost N, 2006, VISUAL COMPUT, V22, P4, DOI 10.1007/s00371-005-0350-y
   PRONOST N, 2006, 4 INT C COMP GRAPH I, P65
   Safonova A., 2005, Dans SCA '05, P171
   Tak S, 2005, ACM T GRAPHIC, V24, P98, DOI 10.1145/1037957.1037963
   VANDEPANNE M, 1992, P 3 EUR WORKSH AN SI
   Yamane K, 2003, IEEE T ROBOTIC AUTOM, V19, P421, DOI 10.1109/TRA.2003.810579
   ZHAO JM, 1994, ACM T GRAPHIC, V13, P313, DOI 10.1145/195826.195827
   Zordan VB, 2005, ACM T GRAPHIC, V24, P697, DOI 10.1145/1073204.1073249
   ZORDAN VB, 2002, SCA 02, P89
NR 20
TC 3
Z9 3
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2007
VL 23
IS 7
BP 513
EP 522
DI 10.1007/s00371-007-0120-0
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 182DL
UT WOS:000247485200007
DA 2024-07-18
ER

PT J
AU Afanasiev, V
   Baigozin, D
   Kazanski, I
   Fomin, S
   Klimenko, S
AF Afanasiev, Valery
   Baigozin, Dmitry
   Kazanski, Ilia
   Fomin, Sergey
   Klimenko, Stanislav
TI RTR-trees for space robotics behavior simulation and visualization
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT International Conference on Cyberworlds (CW 2006)
CY NOV 28-29, 2006
CL Lausanne, SWITZERLAND
SP EPFL, VRlab
DE computer animation; motion simulation; robotics
AB New types of trees of structures and linked lists with variable order relations (RTR-trees and RTR-lists) are discussed. Using them enables application of the direct kinematic scheme for simulation of 3D-objects with reorderable structure, making behavior simulation more natural.
C1 Inst Comp Phys & Technol, Protvino 142284, Russia.
RP Klimenko, S (corresponding author), Inst Comp Phys & Technol, 6 Zavodskoy Pr, Protvino 142284, Russia.
EM klimenko@sim.ol.ru
CR ALESHIN VI, 2001, COSMONAUTICS ROCKET, V25
   Carrano F.M., 2002, Data Abstraction and Problem Solving with C++: Walls and Mirrors, V3
   FORD W, 2000, DATA STRUCTURES C
   Komura T, 2000, VISUAL COMPUT, V16, P254, DOI 10.1007/s003719900065
   Lee J, 1999, COMP GRAPH, P39
   Liu Zicheng., 1994, Proceedings of the 21st Annual Conference on Computer Graphics and Interactive Techniques, SIGGRAPH '94, P35
   MAGNENATTHALMAN.N, 1990, COMPUTER ANIMATION T
   MOLET T, 1996, EUR WORKSH COMP AN S, P79
   *WEB3D CONS, 19775 ISOIEC WEB3D C
   *WEB3D CONS, 19774 ISOIEC FCD WEB
NR 10
TC 0
Z9 1
U1 0
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2007
VL 23
IS 5
BP 347
EP 358
DI 10.1007/s00371-007-0112-0
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 154JS
UT WOS:000245503600006
DA 2024-07-18
ER

PT J
AU Ghosh, A
   Heidrich, W
AF Ghosh, Abhijeet
   Heidrich, Wolfgang
TI Correlated visibility sampling for direct illumination
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 14th Pacific Conference on Computer Graphics and Applications
CY OCT 11-13, 2005
CL Taipei, TAIWAN
DE Monte Carlo techniques; ray tracing
AB State-of-the-art importance sampling strategies for direct illumination take into account the importance of the incident illumination, as well as the surface BRDF. Hence, these techniques achieve low variance in unoccluded regions. However, the resulting images still have noise in partially occluded regions as these techniques do not take visibility into account during the sampling process. We introduce the notion of correlated visibility sampling, which considers visibility in partially occluded regions during the sampling process, thereby improving the quality of the shadowed regions. We aim to draw samples in the partially occluded regions according to the triple product of the incident illumination, BRDF and visibility using Monte Carlo sampling followed by Metropolis sampling.
C1 Univ British Columbia, Dept Comp Sci, Vancouver, BC V6T 1Z4, Canada.
C3 University of British Columbia
RP Ghosh, A (corresponding author), Univ British Columbia, Dept Comp Sci, 2366 Main Mall, Vancouver, BC V6T 1Z4, Canada.
EM ghosh@cs.ubc.ca; heidrich@cs.ubc.ca
CR Burke David., 2005, PROC EGSR 2005, P147
   Clarberg P, 2005, ACM T GRAPHIC, V24, P1166, DOI 10.1145/1073204.1073328
   Cline D, 2005, ACM T GRAPHIC, V24, P1186, DOI 10.1145/1073204.1073330
   COHEN J, 2001, LIGHT GEN HDRSHOP PL
   DUTRE P, 2004, SIGGRAPH COURSE NOTE
   FAN S., 2005, RENDERING TECHNIQUES, P127
   GREENE N, 1986, IEEE COMPUT GRAPH, V6, P21, DOI 10.1109/MCG.1986.276658
   Heidrich W, 1999, COMP GRAPH, P171, DOI 10.1145/311535.311554
   Kautz J, 2000, PROC GRAPH INTERF, P119
   KAUTZ J, 2000, EUR REND WORKSH 00, P185
   KIRK J, 1991, P ACM SIGGRAPH 91, P153
   Kollig T., 2003, Eurographics Symposium on Rendering. 14th Eurographics Workshop on Rendering, P45
   Lawrence J, 2004, ACM T GRAPHIC, V23, P496, DOI 10.1145/1015706.1015751
   Lawrence J., 2005, EGSR 2005, P11
   LLOYD SA, 1983, IMAGE VISION COMPUT, V1, P85
   McCool MD, 1997, PROC GRAPH INTERF, P37
   METROPOLIS N, 1953, J CHEM PHYS, V21, P1087, DOI 10.1063/1.1699114
   Ng R, 2003, ACM T GRAPHIC, V22, P376, DOI 10.1145/882262.882280
   Ostromoukhov V, 2004, ACM T GRAPHIC, V23, P488, DOI 10.1145/1015706.1015750
   Ramamoorthi R, 2001, COMP GRAPH, P497, DOI 10.1145/383259.383317
   Ramantoorthi R, 2002, ACM T GRAPHIC, V21, P517, DOI 10.1145/566570.566611
   Sameer Agarwal, 2003, ACM Transactions on Graphics, V22, P605, DOI 10.1145/882262.882314
   SECORD A, 2002, EG REND WORKSH, P215
   Shirley P., 2000, REALISTIC RAY TRACIN
   SLOAN PP, 2002, P ACM SIGGRAPH, P527
   SLUSALLEK P, 1995, P EUROGRAPHICS, V14, P311
   SMITH AFM, 1992, AM STAT, V46, P84, DOI 10.2307/2684170
   STUMPFEL J, 2004, SIGGRAPH 2004
   Szécsi L, 2004, COMPUT GRAPH FORUM, V23, P585, DOI 10.1111/j.1467-8659.2004.00790.x
   TALBOT JF, 2005, P 16 EUR C REND TECH, P139
   Veach E., 1997, Proceedings of the 24th annual conference on Computer graphics and interactive techniques, P65, DOI [DOI 10.1145/258734.258775, 10.1145/258734.258775]
   Veach E., 1995, P 22 ANN C COMP GRAP, P419, DOI [DOI 10.1145/218380.218498, 10.1145/218380.218498]
NR 32
TC 12
Z9 12
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2006
VL 22
IS 9-11
BP 693
EP 701
DI 10.1007/s00371-006-0055-x
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 082IW
UT WOS:000240381000012
DA 2024-07-18
ER

PT J
AU Yoon, SE
   Lauterbach, C
   Manocha, D
AF Yoon, Sung-Eui
   Lauterbach, Christian
   Manocha, Dinesh
TI R-LODs: fast LOD-based ray tracing of massive models
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 14th Pacific Conference on Computer Graphics and Applications
CY OCT 11-13, 2005
CL Taipei, TAIWAN
DE ray tracing; LODs; cache coherence; layouts; kd-trees; massive models
AB We present a novel LOD (level-of-detail) algorithm to accelerate ray tracing of massive models. Our approach computes drastic simplifications of the model and the LODs are well integrated with the kd-tree data structure. We introduce a simple and efficient LOD metric to bound the error for primary and secondary rays. The LOD representation has small runtime overhead and our algorithm can be combined with ray coherence techniques and cache-coherent layouts to improve the performance. In practice, the use of LODs can alleviate aliasing artifacts and improve memory coherence. We implement our algorithm on both 32-bit and 64-bit machines and are able to achieve up to 2-20 times improvement in frame rate of rendering models consisting of tens or hundreds of millions of triangles with little loss in image quality.
C1 Lawrence Livermore Natl Lab, Livermore, CA 94550 USA.
   Univ N Carolina, Chapel Hill, NC USA.
C3 United States Department of Energy (DOE); Lawrence Livermore National
   Laboratory; University of North Carolina; University of North Carolina
   Chapel Hill
RP Yoon, SE (corresponding author), Lawrence Livermore Natl Lab, Livermore, CA 94550 USA.
EM sungeui@llnl.gov; cl@cs.unc.edu; dm@cs.unc.edu
RI Yoon, Sung-eui/C-1678-2011
OI Manocha, Dinesh/0000-0001-7047-9801
CR Agrawala M, 2000, COMP GRAPH, P375, DOI 10.1145/344779.344954
   Amanatides J., 1984, Computers & Graphics, V18, P129
   [Anonymous], 2003, Level of detail for 3D graphics
   [Anonymous], 2004, THESIS SAARLAND U
   [Anonymous], EUR REND WORKSH
   Appel A., 1968, P AFIPS FALL JOINT C, P37, DOI [DOI 10.1145/1468075.1468082, 10.1145/1468075.1468082]
   Chiang YJ, 2003, IEEE VISUALIZATION 2003, PROCEEDINGS, P217, DOI 10.1109/VISUAL.2003.1250375
   Christensen PH, 2003, COMPUT GRAPH FORUM, V22, P543, DOI 10.1111/1467-8659.t01-1-00702
   Cignoni P, 2004, ACM T GRAPHIC, V23, P796, DOI 10.1145/1015706.1015802
   DEMARLE DE, 2004, EGPGV, P93
   Dietrich A., 2005, Proceedings of the 10th International Fall Workshop-Vision, Modeling, and Visualization, P303
   Gobbetti E, 2005, ACM T GRAPHIC, V24, P878, DOI 10.1145/1073204.1073277
   HAVRAN V, 2000, THESIS CZECH TU PRAG
   Heckbert P. S., 1984, Computers & Graphics, V18, P119
   IGEHY H, 1999, ACM SIGGRAPH, P176
   Jolliffe I.T., 1986, PRINCIPLE COMPONENT
   Levin D, 2004, MATH VISUAL, P37
   MacDonald J. D., 1990, Visual Computer, V6, P153, DOI 10.1007/BF01911006
   Neyret F, 1998, IEEE T VIS COMPUT GR, V4, P55, DOI 10.1109/2945.675652
   PARKER S, 1999, S INT 3D GRAPH, P119
   PHARR M, 1997, ANN C SERIES ACM SIG, P101
   Purcell TJ, 2002, ACM T GRAPHIC, V21, P703, DOI 10.1145/566570.566640
   Reshetov A, 2005, ACM T GRAPHIC, V24, P1176, DOI 10.1145/1073204.1073329
   Rusinkiewicz S, 2000, COMP GRAPH, P343, DOI 10.1145/344779.344940
   Schachter P, 2000, HPB Surg, V11, P319
   Schmittler Jorg., 2004, Proc. of the ACM SIGGRAPH/EUROGRAPHICS Conference on Graphics Hardware (HWWS), P95
   SHINYA M, 1987, COMPUT GRAPH, V21, P45
   SHIRLEY P, 2005, SIGGRAPH COURSE NOTE
   STOLL G, 2006, TR0621 U TEX DEP CS
   Sung K., 1992, GRAPHICS GEMS, P271
   Wald I., 2005, Point-Based Graphics 2005 (IEEE Cat. No. 05EX1159), P9, DOI 10.1109/PBG.2005.194058
   Wald I, 2001, SPRING EUROGRAP, P277
   Wald I, 2001, COMPUT GRAPH FORUM, V20, pC153, DOI 10.1111/1467-8659.00508
   Wald I., 2004, P EUR S REND, P81
   Wand M, 2003, PROC GRAPH INTERF, P139
   WHITTED T, 1980, COMMUN ACM, V23, P343, DOI 10.1145/358876.358882
   Wiley C, 1997, PROC GRAPH INTERF, P88
   Woop S, 2005, ACM T GRAPHIC, V24, P434, DOI 10.1145/1073204.1073211
   Yoon SE, 2004, IEEE VISUALIZATION 2004, PROCEEEDINGS, P131, DOI 10.1109/VISUAL.2004.86
   YOON SE, 2005, P ACM SIGGRAPH 05, P886
   YOON SE, 2006, IN PRESS COMPUT GRAP
   YOON SE, UCRLTR220368DRAFT L
NR 42
TC 37
Z9 44
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2006
VL 22
IS 9-11
BP 772
EP 784
DI 10.1007/s00371-006-0062-y
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 082IW
UT WOS:000240381000020
DA 2024-07-18
ER

PT J
AU Yoshida, N
   Saito, T
AF Yoshida, Norimasa
   Saito, Takafumi
TI Interactive aesthetic curve segments
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 14th Pacific Conference on Computer Graphics and Applications
CY OCT 11-13, 2005
CL Taipei, TAIWAN
DE aesthetic curve segment; logarithmic curvature histogram; radius of
   curvature
ID CURVATURE
AB To meet highly aesthetic requirements in industrial design and styling, we propose a new category of aesthetic curve segments. To achieve these aesthetic requirements, we use curves whose logarithmic curvature histograms (LCH) are represented by straight lines. We call such curves aesthetic curves. We identify the overall shapes of aesthetic curves depending on the slope of LCH alpha, by imposing specific constraints to the general formula of aesthetic curves. For interactive control, we propose a novel method for drawing an aesthetic curve segment by specifying two endpoints and their tangent vectors. We clarify several characteristics of aesthetic curve segments.
C1 Nihon Univ, Chiba 2758575, Japan.
   Tokyo Univ Agr & Technol, Tokyo 1848588, Japan.
C3 Nihon University; Tokyo University of Agriculture & Technology
RP Yoshida, N (corresponding author), Nihon Univ, 1-2-1 Izumi Cho, Chiba 2758575, Japan.
EM norimasa@acm.org; txsaito@cc.tuat.ac.jp
RI ; Saito, Takafumi/E-4295-2013
OI Yoshida, Norimasa/0000-0001-8889-0949; Saito,
   Takafumi/0000-0001-5831-596X
CR [Anonymous], 1992, THESIS U CALIFORNIA
   Carmo Do., 1976, DIFFERENTIAL GEOMETR
   FARIN G, 1989, IEEE COMPUT GRAPH, V9, P52, DOI 10.1109/38.19051
   Farin G., 1997, Curves and surfaces for computer-aided geometric design: A practical guide
   GLASS JM, 1966, BIT, V9, P277
   GREINER G, 1994, COMPUT GRAPH FORUM, V13, pC143, DOI 10.1111/1467-8659.1330143
   Harada T., 1999, Proceedings 1999 IEEE Symposium on Visual Languages, P38, DOI 10.1109/VL.1999.795873
   Higashi M, 1996, COMPUT GRAPH FORUM, V15, pC187, DOI 10.1111/1467-8659.1530187
   Meek D. S., 1991, Computer-Aided Geometric Design, V8, P163, DOI 10.1016/0167-8396(91)90042-A
   Melhum Even., 1974, COMPUT AIDED GEOM D, P173
   Miura K.T., 2005, P 8 INT C HUM COMP H, P166
   Miura KT, 2000, COMPUT AIDED GEOM D, V17, P39, DOI 10.1016/S0167-8396(99)00038-2
   MIURA KT, 2006, CAD 06, P457
   Moll M, 2004, IEEE INT CONF ROBOT, P2826, DOI 10.1109/ROBOT.2004.1307489
   Moreton H., 1991, Proceedings. Symposium on Solid Modeling Foundations and CAD/CAM Applications, P291, DOI 10.1145/112515.112553
   Nutbourne A. W., 1972, Computer Aided Design, V4, P176, DOI 10.1016/0010-4485(72)90072-3
   ROULIER J, 1991, APPROXIMATION THEORY, P177
   Wang YL, 2004, COMPUT AIDED GEOM D, V21, P515, DOI 10.1016/j.cagd.2004.04.001
   Yoshimoto F., 2002, Proceedings of Second IASTED International Conference Visualization, Imaging, and Image Processing, P276
NR 19
TC 38
Z9 42
U1 0
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2006
VL 22
IS 9-11
BP 896
EP 905
DI 10.1007/s00371-006-0076-5
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 082IW
UT WOS:000240381000032
DA 2024-07-18
ER

PT J
AU Wesslén, D
   Seipel, S
AF Wesslén, D
   Seipel, S
TI Real-time visualization of animated trees
SO VISUAL COMPUTER
LA English
DT Article
DE forest visualization; point rendering; vertex animation; tree modeling
AB Realistic visualization of plants and trees has recently received increased interest in various fields of applications. Limited computational power and the extreme complexity of botanical structures have called for tradeoffs between interactivity and realism. In this paper we present methods for the creation and real-time visualization of animated trees. In contrast to other previous research, our work is geared toward near-field visualization of highly detailed areas of forestry scenes with animation. We describe methods for rendering and shading of trees by utilizing the programmable hardware of consumer-grade graphics cards. We then describe a straightforward technique for animation of swaying stems and fluttering foliage that can be executed locally on a graphics processor. Our results show that highly detailed tree structures can be visualized at real-time frame rates and that animation of plant structures can be accomplished without sacrificing performance.
C1 Univ Gavle, Gavle, Sweden.
   Uppsala Univ, Uppsala, Sweden.
C3 University of Gavle; Uppsala University
RP Univ Gavle, Gavle, Sweden.
EM dwn@hig.se
OI Seipel, Stefan/0000-0003-0085-5829
CR AITKEN M, 2003, P 1 INT C COMP GRAPH, P37
   [Anonymous], SIGGRAPH, DOI DOI 10.1145/237170.237279
   [Anonymous], P AUSGR 88
   BLOOMENTHAL J, 1985, SIGGRAPH 85, P305
   CODER KD, 2000, U OUTREACH PUBLICATI
   DEREFFYE P, 1988, SIGGRAPH 88 P 15 ANN, P151
   DEUSSEN O, 1998, SIGGRAPH 98, P275
   Marshall D, 1997, PROC GRAPH INTERF, P97
   OPPENHEIMER PE, 1986, SIGGRAPH 86 P 13 ANN, P55
   PRUSINKIEWICZ P, 1988, SIGGRAPH 88, P141
   Prusinkiewicz Przemyslaw., 1994, Proceedings of the 21st annual conference on Computer graphics and interactive techniques. SIGGRAPH'94, P351, DOI DOI 10.1145/192161.192254
   SAKAGUCHI T, 1999, P ACM S VIRT REAL SO, P139
   SMITH AR, 1984, SIGGRAPH 84, P1
   WEBER J, 1995, SIGGRAPH 95, P119, DOI DOI 10.1145/218380.218427
NR 14
TC 6
Z9 8
U1 0
U2 5
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2005
VL 21
IS 6
BP 397
EP 405
DI 10.1007/s00371-005-0295-1
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 952GD
UT WOS:000230991100004
DA 2024-07-18
ER

PT J
AU Goshtasby, AA
AF Goshtasby, AA
TI Plus curves and surfaces
SO VISUAL COMPUTER
LA English
DT Article
DE Bezier curve; Bezier surface; B-spline curve; rational Gaussian (RaG)
   surface; geometric modeling; polygon mesh
AB The formulations for parametric curves and surfaces that are based on control points are revised to use control lines and control planes instead. Curves defined by control lines are called control-line curves or plus curves, and surfaces defined by control planes are called control-plane surfaces or plus surfaces; the plus implies that in addition to the control points, gradient vectors at the control points are used to design curves and surfaces. The new curve and surface formulations provide more flexibility than traditional formulations in geometric design. Properties of plus curves and surfaces are investigated and an application of plus surfaces in smooth parametric representation of polygon meshes is introduced.
C1 Wright State Univ, Dept Comp Sci & Engn, Dayton, OH 45435 USA.
C3 University System of Ohio; Wright State University Dayton
RP Wright State Univ, Dept Comp Sci & Engn, Dayton, OH 45435 USA.
EM ardeshir@cs.wright.edu
CR CATMULL E, 1978, COMPUT AIDED DESIGN, V10, P350, DOI 10.1016/0010-4485(78)90110-0
   Catmull E, 1974, inCom-puter Aided Geometric Design, P317, DOI [DOI 10.1016/B978-0-12-079050-0.50020-5, 10.1016/B978-0-12-079050-0.50020-5]
   CHANG SC, 1997, MED J CMCH, V2, P6
   Chui CK, 2000, COMPUT AIDED GEOM D, V17, P297, DOI 10.1016/S0167-8396(00)00005-4
   CONSTANTINI P, 1996, J COMPUT APPL MATH, V73, P45
   Costantini P, 1999, COMPUT AIDED GEOM D, V16, P385, DOI 10.1016/S0167-8396(99)00007-2
   DOO D, 1978, COMPUT AIDED DESIGN, V10, P356, DOI 10.1016/0010-4485(78)90111-2
   DYN N, 1990, ACM T GRAPHIC, V9, P160, DOI 10.1145/78956.78958
   Farin G., 1985, Computer-Aided Geometric Design, V2, P19, DOI 10.1016/0167-8396(85)90003-2
   Farin G, 1999, NURBS: from projective geometry to practical use
   GOSHTASBY A, 1995, COMPUT AIDED DESIGN, V27, P363, DOI 10.1016/0010-4485(95)96800-2
   HOSCHEK J, 1983, SURFACES COMPUTER AI, P147
   Khodakovsky A, 2003, ACM T GRAPHIC, V22, P350, DOI 10.1145/882262.882275
   KOBBELT L, 1996, P EUR 96 COMP GRAPH, P409
   Loop C, 1987, THESIS U UTAH
   Mortenson ME., 1997, Geometric modeling, V2nd
   PRAUN E, 2003, COMPUTER GRAPHICS P, P340
   Rogers D.F., 1990, Mathematical Elements for Computer Graphics, V2nd
NR 18
TC 2
Z9 3
U1 2
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2005
VL 21
IS 1-2
BP 4
EP 16
DI 10.1007/s00371-004-0267-x
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 911OG
UT WOS:000228013300001
DA 2024-07-18
ER

PT J
AU van Overveld, K
   Wyvill, B
AF van Overveld, K
   Wyvill, B
TI Shrinkwrap: An efficient adaptive algorithm for triangulating an
   iso-surface
SO VISUAL COMPUTER
LA English
DT Article
DE iso-surfaces; implicit surfaces; polygonization
AB An algorithm is presented which generates a triangular mesh to approximate an iso-surface. It starts with a triangulation of a sphere and next applies a series of deformations to this triangulation to transform it into the required surface. These deformations leave the topology invariant, so the final iso-surface should be homeomorphic with a sphere. The algorithm is adaptive in the sense that the lengths of the sides of the triangles in the mesh vary with the local curvature of the underlying surface. A quantitative analysis of the accuracy of the algorithm is given along with an empirical comparison with earlier algorithms.
C1 Eindhoven Univ Technol, Dept Ind Design, NL-5600 MB Eindhoven, Netherlands.
   Univ Calgary, Dept Comp Sci, Calgary, AB T2N 1N4, Canada.
C3 Eindhoven University of Technology; University of Calgary
RP Eindhoven Univ Technol, Dept Ind Design, POB 513, NL-5600 MB Eindhoven, Netherlands.
EM k.van.overveld@wxs.nl; blob@cpsc.ucalgary.ca
CR [Anonymous], 1997, Introduction to Implicit Surfaces
   Blinn J. F., 1982, Computer Graphics, V16, DOI 10.1145/965145.801290
   Bloomenthal J., 1990, Computer Graphics, V24, P109, DOI 10.1145/91394.91427
   Bloomenthal J., 1988, Computer-Aided Geometric Design, V5, P341, DOI 10.1016/0167-8396(88)90013-1
   BOTTINO A, 1995, THESIS EINDHOVEN U T
   Lorensen William E., 1987, COMPUT GRAPH, P163, DOI DOI 10.1145/37402.37422
   MALLET JL, COMPUT AID DES, V24, P178
   NING P, 1993, IEEE COMPUT GRAPH, V13, P33, DOI 10.1109/38.252552
   NISHIMURA H, 1985, J PAPERS GIVEN ELE D, V68
   REQUICHA AAG, 1983, P IEEE, V3, P30
   Sederberg T. W., 1986, Computer Graphics, V20, P151, DOI 10.1145/15886.15903
   TURK G, 1992, COMP GRAPH, V26, P55, DOI 10.1145/142920.134008
   VANOVERVELD K, 1996, P EUR WORKSH IMPL SU
   WILHELMS J, 1991, COMP GRAPH, V25, P275, DOI 10.1145/127719.122758
   WYVILL B, 1988, SIGGRAPH 88 ELECT TH
   Wyvill G., 1986, Visual Computer, V2, P227, DOI 10.1007/BF01900346
NR 16
TC 27
Z9 36
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2004
VL 20
IS 6
BP 362
EP 379
DI 10.1007/s00371-002-0197-4
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 846DU
UT WOS:000223296100002
DA 2024-07-18
ER

PT J
AU Brown, J
   Latombe, JC
   Montgomery, K
AF Brown, J
   Latombe, JC
   Montgomery, K
TI Real-time knot-tying simulation
SO VISUAL COMPUTER
LA English
DT Article
DE rope simulation; knot tying; self-collision detection; collision
   management; surgical suturing
ID FRICTION
AB The real-time simulation of rope, and knot tying in particular, raises difficult issues in contact detection and management. Some practical knots can only be achieved by complicated crossings of the rope, yielding multiple simultaneous contacts, especially when the rope is pulled tight. This paper describes a graphical simulator that allows a user to grasp and smoothly manipulate a virtual rope and to tie arbitrary knots, including knots around other objects, in real time. A first component of the simulator computes the global configuration of the rope based on user interactions. Another component of the simulator precisely detects self-collisions in the rope as well as collisions with other objects. Finally, a third component manages collisions to prevent penetration, while making the rope slide with some friction along itself and other objects, so that knots can be pulled tight in a realistic manner. An additional module uses recent results from knot theory to identify, also in real time, which topological knots have been tied. This work was motivated by surgical suturing, but simulation in other domains, such as sailing and rock climbing, could also benefit from it.
C1 Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA.
   Stanford NASA Natl Biocomputat Ctr, Stanford, CA 94305 USA.
C3 Stanford University; National Aeronautics & Space Administration (NASA)
RP Stanford Univ, Dept Comp Sci, 353 Serra Mall, Stanford, CA 94305 USA.
EM jbrown@cs.stanford.edu
CR Adams C.C., 1994, The Knot Book
   [Anonymous], 1994, KNOTS SURFACES
   [Anonymous], 1963, Introduction to Knot Theory
   Baraff D., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P43, DOI 10.1145/280814.280821
   Barzel R, 1997, IEEE COMPUT GRAPH, V17, P31, DOI 10.1109/38.586016
   Bridson R, 2002, ACM T GRAPHIC, V21, P594, DOI 10.1145/566570.566623
   Brown J, 2002, MED IMAGE ANAL, V6, P289, DOI 10.1016/S1361-8415(02)00086-5
   BROWN J, 2003, THESIS STANFORD U ST
   CAKMAK HK, 2001, THESIS WISSENSCHAFTL
   Dawson Steven L, 2002, Bull Am Coll Surg, V87, P12
   Delingette H, 1998, P IEEE, V86, P512, DOI 10.1109/5.662876
   ERDMANN M, 1994, INT J ROBOT RES, V13, P240, DOI 10.1177/027836499401300306
   Gottschalk S., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P171, DOI 10.1145/237170.237244
   Graydon D., 1992, MOUNTAINEERING FREED, Vfifth
   Guibas L.J., 2002, S COMPUTATIONAL GEOM, P33
   Halperin D, 1998, COMP GEOM-THEOR APPL, V11, P83, DOI 10.1016/S0925-7721(98)00023-6
   Hass J, 1997, ANN IEEE SYMP FOUND, P172, DOI 10.1109/SFCS.1997.646106
   Hoste J, 1998, MATH INTELL, V20, P33, DOI 10.1007/BF03025227
   HOUSE DH, 2000, CLOTH MODELING ANIMA
   Joukhadar A., 1999, Proceedings 1999 IEEE/RSJ International Conference on Intelligent Robots and Systems. Human and Environment Friendly Robots with High Intelligence and Emotional Quotients (Cat. No.99CH36289), P1810, DOI 10.1109/IROS.1999.811741
   Kim TY, 2002, ACM T GRAPHIC, V21, P620
   Kühnapfel U, 2000, COMPUT GRAPH-UK, V24, P671, DOI 10.1016/S0097-8493(00)00070-4
   KUSNER RB, 1998, IDEAL KNOTS, P315
   LADD AM, 2002, P 5 INT WORKSH ALG F, P6
   Larsson A, 2001, STUD HEALTH TECHNOL, V81, P266
   LENOIR J, 2002, P WORKSH MOD SIM COM
   Lin M., 1998, Proc. of IMA Conference on Mathematics of Surfaces, V1, P602
   Lotan Itay., 2002, SCG'02: Proceedings o f the eighteenth annual symposium on Computational geometry, P43
   Mirtich B., 1995, Proceedings 1995 Symposium on Interactive 3D Graphics, P181, DOI 10.1145/199404.199436
   Pai DK, 2002, COMPUT GRAPH FORUM, V21, P347, DOI 10.1111/1467-8659.00594
   PAWSON D, 1998, HDB KNOTS
   Phillips J, 2002, 2002 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION, VOLS I-IV, PROCEEDINGS, P841, DOI 10.1109/ROBOT.2002.1013462
   PROVOT X, 1995, GRAPH INTER, P147
   Provot X, 1997, GRAPHICS INTERFACE, P177, DOI DOI 10.1007/978-3-7091-6874-5_13
   QUINLAN S, 1994, IEEE INT CONF ROBOT, P3324, DOI 10.1109/ROBOT.1994.351059
   Silverstein LH, 1999, Principles of dental suturing: The complete guide to surgical closure
   Smith A., 1995, Proceedings. Virtual Reality Annual International Symposium '95 (Cat. No.95CH35761), P136, DOI 10.1109/VRAIS.1995.512489
   VanderStricht W, 1997, MRS INTERNET J N S R, V2
   VOLINO P, 1995, P 6 EUR WORKSH COMP, P55
   WARD K, 2003, P 16 INT C COMP AN S
   [No title captured]
NR 41
TC 91
Z9 114
U1 2
U2 18
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2004
VL 20
IS 2-3
BP 165
EP 179
DI 10.1007/s00371-003-0226-y
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 818ZV
UT WOS:000221283900006
DA 2024-07-18
ER

PT J
AU Ma, JY
   Cole, R
AF Ma, JY
   Cole, R
TI Animating visible speech and facial expressions
SO VISUAL COMPUTER
LA English
DT Article
DE face animation; visible speech; visual speech; 3D tongue model; animated
   speech
ID MODEL
AB We present four techniques for modeling and animating faces starting from a set of morph targets. The first technique involves obtaining parameters to control individual facial components and learning the mapping from one type of parameter to another through machine learning techniques. The second technique is to fuse visible speech and facial expressions in the lower part of a face. The third technique combines coarticulation rules and kernel smoothing techniques. Finally, a new 3D tongue model with flexible and intuitive skeleton controls is presented. The results of eight animated character models demonstrate that these techniques are powerful and effective.
C1 Univ Colorado, Ctr Spoken Language Res, Boulder, CO 80309 USA.
C3 University of Colorado System; University of Colorado Boulder
RP Univ Colorado, Ctr Spoken Language Res, Campus Box 594, Boulder, CO 80309 USA.
EM jiyong@cslr.colorado.edu; cole@cslr.colorado.edu
CR ALBRECHT I, 2002, VIS COMPUT VISION, V10, P9
   [Anonymous], 1999, Coarticulation: Theory, Data and Techniques
   [Anonymous], 2002, MPEG4 FACIAL ANIMATI
   [Anonymous], 2003, PROCEEDINGS OF IEEE
   BADIN P, 1998, P 5 ICSLP, V2, P417
   Barr A. H., 1981, IEEE COMPUT GRAPH, V1, P1, DOI [10.1109/MCG.1981.1673788, DOI 10.1109/MCG.1981.1673788]
   Bavelas J. B., 1994, Res. Lang. Soc. Interact., V27, P201, DOI DOI 10.1207/S15327973RLSI2703_3
   Brand M, 1999, COMP GRAPH, P21, DOI 10.1145/311535.311537
   BREEN AP, 1996, P INT C SPOK LANG PR, P108
   Bregler C., 1997, P 24 ANN C COMP GRAP, V31, P353, DOI DOI 10.1145/258734.258880
   Cassell J., 2001, Proceedings of SIGGRAPH 2001, P477
   CELNIKER G, 1991, COMP GRAPH, V25, P257, DOI 10.1145/127719.122746
   Cohen M. M., 1993, Models and Techniques in Computer Animation, P139
   COHN JF, 1997, P 7 EUR C FAC EXPR M, P329
   COLE R, 1999, P ESCA SOCRATES WORK, P45
   Ekman P, 1978, FACIAL ACTION CODING
   Engwall O., 2000, ICSLP 2000, V3, P901
   Eubank R.L., 1999, Nonparametric regression and spline smoothing
   Ezzat T, 2002, ACM T GRAPHIC, V21, P388, DOI 10.1145/566570.566594
   FARIN TG, 2002, CURVES SURFACES CAGD, P155
   Guenin BM, 1998, P IEEE SEMICOND THER, P55, DOI 10.1109/STHERM.1998.660387
   JEFFERS J, 1971, SPEECHREADING
   Kent, 1977, Journal of Phonetics, V5, P115, DOI [DOI 10.1006/JPHO.2000.0118, DOI 10.1016/S0095-4470(19)31123-4]
   Kent R., 1997, The Speech Sciences
   King SA, 2001, J VISUAL COMP ANIMAT, V12, P107, DOI 10.1002/vis.249
   KLEISER J, 1989, P ACM SIGGRAPH TUT B, V22, P20
   KOCH R.M., 1996, P SIGGRAPH, P421
   Kouadio C, 1998, COMP ANIM CONF PROC, P128, DOI 10.1109/CA.1998.681917
   Kshirsagar S, 2001, COMPUTER GRAPHICS INTERNATIONAL 2001, PROCEEDINGS, P38, DOI 10.1109/CGI.2001.934656
   Kshirsagar S, 2000, 2000 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, PROCEEDINGS VOLS I-III, P1077, DOI 10.1109/ICME.2000.871547
   Lee Y., 1995, SIGGRAPH, P55, DOI [10.1145/218380.218407, DOI 10.1145/218380.218407]
   LOFQVIST A, 1990, NATO ADV SCI I D-BEH, V55, P289
   MA JY, 2002, P INT C SPOK LANG PR, V1, P197
   MAESTRI G., 1996, Digital Character Animation
   Magnenat-Thalmann N., 1988, Visual Computer, V3, P290, DOI 10.1007/BF01914864
   MASSARO DW, 1996, PERCEIVING TALKING F
   McNeill D., 1992, Hand and Mind: What Gestures Reveal about Thought
   Moccozet L, 1997, COMP ANIM CONF PROC, P93, DOI 10.1109/CA.1997.601047
   Noh JY, 2001, COMP GRAPH, P277, DOI 10.1145/383259.383290
   OHMAN SEG, 1966, J ACOUST SOC AM, V39, P151, DOI 10.1121/1.1909864
   Parke FrederickI., 1972, Proceedings of the ACM annual conference, V1, P451
   PELACHAUD C, 1991, P COMP AN 1 JUN 1991, P15
   Pighin F, 2002, INT J COMPUT VISION, V50, P143, DOI 10.1023/A:1020393915769
   Platt S. M., 1981, Computer Graphics, V15, P245, DOI 10.1145/965161.806812
   Press W. H., 1988, Numerical Recipes
   Sanguineti V, 1997, BIOL CYBERN, V77, P11, DOI 10.1007/s004220050362
   SCLAROFF S, 1995, IEEE T PATTERN ANAL, V17, P545, DOI 10.1109/34.387502
   Small Larry., 1999, Fundamentals of phonetics: A practical guide for students, V3rd
   Stone M, 1996, J ACOUST SOC AM, V99, P3728, DOI 10.1121/1.414969
   Terzopoulos D., 1990, Journal of Visualization and Computer Animation, V1, P73, DOI 10.1002/vis.4340010208
   Vetter T, 1997, IEEE T PATTERN ANAL, V19, P733, DOI 10.1109/34.598230
   WALTHER EF, 1982, LIPREADING
NR 52
TC 10
Z9 12
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2004
VL 20
IS 2-3
BP 86
EP 105
DI 10.1007/s00371-003-0234-y
PG 20
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 818ZV
UT WOS:000221283900002
DA 2024-07-18
ER

PT J
AU Chen, CH
   Lee, CY
AF Chen, CH
   Lee, CY
TI Two-level hierarchical Z-buffer with compression technique for 3D
   graphics hardware
SO VISUAL COMPUTER
LA English
DT Article
DE 3D graphics hardware; hierarchical Z-buffer; hierarchical Z-buffer
   compression
AB The hierarchical Z-buffer is application-invisible and more efficient than the traditional Z-buffer for quickly rejecting hidden geometries. But there are construction and management issues associated with integrating a hierarchical Z-buffer into current graphics hardware. Here we present a two-level hierarchical Z-buffer algorithm, and provide solutions to these issues. Simulation results show that the bandwidth can be reduced by up to 35%. Moreover we propose a dynamic bi-level HZ-buffer compression technique that reduces the buffer size up by to 40%, and for which there is little performance degradation.
C1 Natl Chiao Tung Univ, Dept Elect Engn, Hsinchu 300, Taiwan.
C3 National Yang Ming Chiao Tung University
RP Chen, CH (corresponding author), Natl Chiao Tung Univ, Dept Elect Engn, 1001 Ta Hsueh Rd, Hsinchu 300, Taiwan.
CR BLINN J, 1996, J BLINNS CORNER TRIP, P191
   Catmull E., 1975, Proceedings of the conference on computer graphics, pattern recognition, and data structure, P11
   Fuchs H., 1980, Computer Graphics, V14, P124, DOI 10.1145/965105.807481
   GORDON D, 1991, IEEE COMPUT GRAPH, V11, P79, DOI 10.1109/38.90569
   Greene N., 1993, Proc. SIGGRAPH, P231
   Luebke D., 1995, Proceedings 1995 Symposium on Interactive 3D Graphics, P105, DOI 10.1145/199404.199422
   MOLLER T, 1999, REAL TIME RENDERING, P192
   MOREIN S, 2000, EUR HARDW WORKSH 200
   XIE F, 1999, P 1999 EUR SIGGRAPH, P75
   ZHANG H, 1997, P ACM SIGGRAPH, P77
NR 10
TC 5
Z9 10
U1 0
U2 2
PU SPRINGER-VERLAG
PI NEW YORK
PA 175 FIFTH AVE, NEW YORK, NY 10010 USA
SN 0178-2789
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2003
VL 19
IS 7-8
BP 467
EP 479
DI 10.1007/s00371-003-0212-4
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 749YL
UT WOS:000186957600004
DA 2024-07-18
ER

PT J
AU Park, SC
AF Park, SC
TI Polygonal extrusion
SO VISUAL COMPUTER
LA English
DT Article
DE swept volume; polygonal extrusion; trimming; Voronoi; PWID offset
ID ALGORITHM; OPERATIONS; ENVELOPE; OFFSETS
AB A sweeping operation called polygonal extrusion is defined to improve the modeling power of CSG-based modeling. It is assumed that a 2D cross-sectional polygon (sweeping polygon) moves in space while its containing plane is kept orthogonal to the tangent direction of the trajectory curve, a planar polygonal chain having no self-intersections. The objective of the paper is to compute the boundary of the swept volume of the sweeping polygon as a set of polygons (or triangles). The most significant challenge to accomplishing this objective is the problem of trimming the swept volume. To solve the trimming problem, 2D-curve offsetting methods are employed. Two algorithms are presented for polygonal extrusion that are based on different offsetting methods, the Voronoi diagram and PWID offset. The proposed algorithms have been implemented and tested with various examples.
C1 Cub Technol Res Ctr, KangSeo Ku, Seoul, South Korea.
RP Park, SC (corresponding author), Cub Technol Res Ctr, KangSeo Ku, AceTechno Tower 1101,684-1 DungChon Dong, Seoul, South Korea.
CR Blackmore D, 1999, COMPUT AIDED DESIGN, V31, P215, DOI 10.1016/S0010-4485(99)00017-2
   Blackmore D, 1997, COMPUT AIDED DESIGN, V29, P629, DOI 10.1016/S0010-4485(96)00101-7
   CHIANG CS, 1991, C P CURV SURF COMP V, V2, P76
   Choi BK, 1997, COMPUT AIDED DESIGN, V29, P837, DOI 10.1016/S0010-4485(97)00031-6
   Choi BK, 1999, COMPUT AIDED DESIGN, V31, P735, DOI 10.1016/S0010-4485(99)00060-3
   Elber G, 1997, COMPUT AIDED DESIGN, V29, P441, DOI 10.1016/S0010-4485(96)00085-1
   HANSEN A, 1992, ACM T GRAPHIC, V11, P152, DOI 10.1145/130826.130832
   Hartquist EE, 1999, COMPUT AIDED DESIGN, V31, P175, DOI 10.1016/S0010-4485(99)00014-7
   Held M., 1991, COMPUTATIONAL GEOMET
   Iwata K., 1995, J. CIRP Annals-manufacturing Technology, V44, P399, DOI DOI 10.1016/S0007-8506(07)62350-6
   Lee JH, 2000, VISUAL COMPUT, V16, P208, DOI 10.1007/s003710050209
   LING ZK, 1996, J MECH DESIGN, V118, P221
   Mantyla M., 1998, An Introduction to Solid Modeling
   MARTIN RR, 1990, COMPUT AIDED DESIGN, V22, P223, DOI 10.1016/0010-4485(90)90051-D
   Parker L, 2000, J NUTR EDUC, V32, P1, DOI 10.1016/S0022-3182(00)70502-4
   SUGIHARA K, 1998, P IFIP WG5 2 GEO 6 C, P5
   TILLER W, 1984, IEEE COMPUT GRAPH, V4, P36, DOI 10.1109/MCG.1984.275995
NR 17
TC 3
Z9 7
U1 0
U2 2
PU SPRINGER-VERLAG
PI NEW YORK
PA 175 FIFTH AVE, NEW YORK, NY 10010 USA
SN 0178-2789
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2003
VL 19
IS 1
BP 38
EP 49
DI 10.1007/s00371-002-0170-2
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 671MB
UT WOS:000182467600004
DA 2024-07-18
ER

PT J
AU Indolia, S
   Nigam, S
   Singh, R
AF Indolia, Sakshi
   Nigam, Swati
   Singh, Rajiv
TI A self-attention-based fusion framework for facial expression
   recognition in wavelet domain
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Deep learning; Emotion recognition; Facial expression recognition;
   Wavelet transform; Self-attention
ID FEATURES; CNN; NETWORK
AB Facial expression recognition (FER) plays a vital role for applications based on human-computer interaction. In the past few years, many deep learning models have been proposed for FER, but their performance is limited due to challenges such as variation in head pose, occlusion, illumination, etc. Moreover, existing models consider input image through holistic view without giving attention to features relevant to expressions. In this work, we propose a deep fusion framework for FER which employs self-attention mechanism to resolve this issue. Furthermore, to improve feature representation of the images, the proposed model transforms the input image to wavelet domain through discrete wavelet transform. The framework employs two parallel branches for shallow and deep features which are fused for better feature representation. The proposed model is evaluated on posed and in-the-wild datasets: CK+, JAFFE, MUG, YALE, RAF, and experimental results validate the effectiveness of the model.
C1 [Indolia, Sakshi] NMIMS, SVKMs Narsee Monjee Inst Management Studies, Sch Technol Management & Engn, Dept Comp Engn, Mumbai 410210, Maharashtra, India.
   [Nigam, Swati; Singh, Rajiv] Banasthali Vidyapith, Dept Comp Sci, Tonk 304022, Rajasthan, India.
   [Nigam, Swati; Singh, Rajiv] Banasthali Vidyapith, Ctr Artificial Intelligence, Tonk 304022, Rajasthan, India.
C3 SVKM's NMIMS (Deemed to be University); Banasthali Vidyapith; Banasthali
   Vidyapith
RP Singh, R (corresponding author), Banasthali Vidyapith, Dept Comp Sci, Tonk 304022, Rajasthan, India.; Singh, R (corresponding author), Banasthali Vidyapith, Ctr Artificial Intelligence, Tonk 304022, Rajasthan, India.
EM sakshiindolia95@gmail.com; swatinigam.au@gmail.com;
   jkrajivsingh@gmail.com
RI Singh, Rajiv/H-2377-2014
OI Singh, Rajiv/0000-0003-4022-9945
CR Aghamaleki JA, 2019, MULTIMED TOOLS APPL, V78, P22861, DOI 10.1007/s11042-019-7530-7
   Aifanti N., 2010, P 11 INT WORKSH IM A, DOI DOI 10.1371/JOURNAL.PONE.0009715
   Almanza-Conejo O, 2023, NEURAL COMPUT APPL, V35, P1409, DOI 10.1007/s00521-022-07843-9
   Arora M, 2021, MULTIMED TOOLS APPL, V80, P3039, DOI 10.1007/s11042-020-09726-4
   Belhumeur PN, 1997, IEEE T PATTERN ANAL, V19, P711, DOI 10.1109/34.598228
   Bendjillali RI, 2019, ELECTRONICS-SWITZ, V8, DOI 10.3390/electronics8030324
   Boughida A, 2022, EVOL SYST-GER, V13, P331, DOI 10.1007/s12530-021-09393-2
   Cai J, 2018, IEEE INT CONF AUTOMA, P302, DOI 10.1109/FG.2018.00051
   Ding H, 2017, IEEE INT CONF AUTOMA, P118, DOI 10.1109/FG.2017.23
   Farzaneh AH, 2021, IEEE WINT CONF APPL, P2401, DOI 10.1109/WACV48630.2021.00245
   Franco L, 2001, ISPA 2001: PROCEEDINGS OF THE 2ND INTERNATIONAL SYMPOSIUM ON IMAGE AND SIGNAL PROCESSING AND ANALYSIS, P628, DOI 10.1109/ISPA.2001.938703
   Gonzalez-Lozoya SM, 2020, MULTIMED TOOLS APPL, V79, P13987, DOI 10.1007/s11042-020-08681-4
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He Z., 2022, Multim. Tools Appl, V82, P1
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Jain N, 2018, PATTERN RECOGN LETT, V115, P101, DOI 10.1016/j.patrec.2018.04.010
   Li B., 2021, International Journal of Cognitive Computing in Engineering, V2, P57
   Li J, 2020, NEUROCOMPUTING, V411, P340, DOI 10.1016/j.neucom.2020.06.014
   Li S, 2017, PROC CVPR IEEE, P2584, DOI 10.1109/CVPR.2017.277
   Li SQ, 2021, IEEE T NETW SCI ENG, V8, P2213, DOI 10.1109/TNSE.2021.3083739
   Li THS, 2019, IEEE ACCESS, V7, P93998, DOI 10.1109/ACCESS.2019.2928364
   Liao HB, 2021, MULTIMED TOOLS APPL, V80, P28627, DOI 10.1007/s11042-021-10951-8
   Pham L, 2021, INT C PATT RECOG, P4513, DOI 10.1109/ICPR48806.2021.9411919
   Lucey P., 2010, IEEE COMP SOC C COMP, P94, DOI 10.1109/CVPRW.2010.5543262
   Luo ZM, 2018, INT C PATT RECOG, P3132, DOI 10.1109/ICPR.2018.8545847
   Lyons M, 1998, AUTOMATIC FACE AND GESTURE RECOGNITION - THIRD IEEE INTERNATIONAL CONFERENCE PROCEEDINGS, P200, DOI 10.1109/AFGR.1998.670949
   Ma FY, 2023, IEEE T AFFECT COMPUT, V14, P1236, DOI 10.1109/TAFFC.2021.3122146
   Mahesh VGV, 2021, IEEE ACCESS, V9, P52509, DOI 10.1109/ACCESS.2021.3069881
   Mallat S.A., 1999, WAVELET TOUR SIGNAL
   MALLAT SG, 1989, IEEE T PATTERN ANAL, V11, P674, DOI 10.1109/34.192463
   Minaee S, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21093046
   Navastara Dini Adni, 2021, Advances in Computer, Communication and Computational Sciences. Proceedings of IC4S 2019. Advances in Intelligent Systems and Computing (AISC 1158), P941, DOI 10.1007/978-981-15-4409-5_83
   Nigam S, 2018, MULTIMED TOOLS APPL, V77, P28725, DOI 10.1007/s11042-018-6040-3
   Ou XF, 2019, IEEE ACCESS, V7, P108152, DOI 10.1109/ACCESS.2019.2931922
   Ravi R, 2020, 2020 4 INT C COMP ME
   Reddy AH, 2022, SIGNAL IMAGE VIDEO P, V16, P369, DOI 10.1007/s11760-021-01941-2
   Sadeghi H, 2017, IRAN CONF MACH, P188, DOI 10.1109/IranianMVIP.2017.8342346
   Sen D, 2019, MULTIMED TOOLS APPL, V78, P10287, DOI 10.1007/s11042-018-6537-9
   Shehu HA, 2021, AIP CONF PROC, V2334, DOI 10.1063/5.0042221
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Singh Rajiv, 2013, ScientificWorldJournal, V2013, P521034, DOI 10.1155/2013/521034
   Singh R, 2014, INFORM FUSION, V19, P49, DOI 10.1016/j.inffus.2012.09.005
   Sun X, 2021, NEUROCOMPUTING, V444, P378, DOI 10.1016/j.neucom.2019.11.127
   Viola P, 2001, PROC CVPR IEEE, P511, DOI 10.1109/cvpr.2001.990517
   Wang F, 2017, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2017.683
   Wang K, 2020, IEEE T IMAGE PROCESS, V29, P4057, DOI 10.1109/TIP.2019.2956143
   Xie SY, 2019, IEEE T MULTIMEDIA, V21, P211, DOI 10.1109/TMM.2018.2844085
   Yaddaden Y., 2020, INT C COMP SYST APPL, P14
   Zhang T, 2019, IEEE T CYBERNETICS, V49, P839, DOI 10.1109/TCYB.2017.2788081
   Zhao JF, 2018, VISUAL COMPUT, V34, P1461, DOI 10.1007/s00371-018-1477-y
   Zhao S., 2018, P BMVC, P1
   Zheng C, 2023, Arxiv, DOI arXiv:2204.04083
   Zou W, 2022, APPL INTELL, V52, P2918, DOI 10.1007/s10489-021-02575-0
NR 53
TC 1
Z9 1
U1 1
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2023 DEC 7
PY 2023
DI 10.1007/s00371-023-03168-3
EA DEC 2023
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AA2V2
UT WOS:001115674200004
DA 2024-07-18
ER

PT J
AU Aldayri, A
   Albattah, W
AF Aldayri, Amnah
   Albattah, Waleed
TI A deep learning approach for anomaly detection in large-scale Hajj
   crowds
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Abnormal behavior; Anomaly detection; Human crowd; Mass gathering;
   Surveillance system
ID BEHAVIOR DETECTION; FRAMEWORK
AB Hajj is an annual Islamic event attended by millions of pilgrims every year from around the globe. It is considered to be the biggest religious event that includes large human crowds in the world. Managing such crowds and detecting abnormal behaviors is one of the most significant challenges for the host country, particularly the crowds of pilgrims. Most of the current solutions can only handle small-scale crowd management issues, that involve simple and clear abnormal behaviors. Therefore, there is a need to have a human abnormal behavior detection approach that can deal with large-scale crowd situations. This study aims to propose a computer vision-based framework that automatically analyzes video sequences and detects human abnormal behaviors. The Convolutional LSTM Autoencoder is used for analyzing video scenes and extracting valuable spatial and temporal features. The proposed approach has achieved a good loss reduction of 0.176587 in detecting abnormal pilgrims' behavior. The results demonstrate a promising picture of the effectiveness of computer vision technologies to detect abnormal behavior in large-scale crowds.
C1 [Aldayri, Amnah; Albattah, Waleed] Qassim Univ, Coll Comp, Dept Informat Technol, Buraydah 52571, Saudi Arabia.
C3 Qassim University
RP Albattah, W (corresponding author), Qassim Univ, Coll Comp, Dept Informat Technol, Buraydah 52571, Saudi Arabia.
EM 421200304@qu.edu.sa; w.albattah@qu.edu.sa
OI Albattah, Waleed/0000-0003-0292-7304
FU The researchers would like to thank the Deanship of Scientific Research,
   Qassim University for funding the publication of this project.; Deanship
   of Scientific Research, Qassim University
FX The researchers would like to thank the Deanship of Scientific Research,
   Qassim University for funding the publication of this project.
CR Abi Sen Adnan Ahmed, 2021, Proceedings of the 2021 8th International Conference on Computing for Sustainable Global Development (INDIACom), P385, DOI 10.1109/INDIACom51348.2021.00067
   Al-Dhamari A, 2020, IEEE ACCESS, V8, P61085, DOI 10.1109/ACCESS.2020.2982906
   Alafif T, 2022, Arxiv, DOI arXiv:2207.11931
   Alafif T, 2021, J AMB INTEL HUM COMP, DOI 10.1007/s12652-021-03323-5
   Amrutha C. V., 2020, 2020 2nd International Conference on Innovative Mechanisms for Industry Applications (ICIMIA). Proceedings, P335, DOI 10.1109/ICIMIA48430.2020.9074920
   Aqeel M., 2020, 2020 IEEE 23 INT MUL, P1, DOI [10.1109/INMIC50486.2020.9318140, DOI 10.1109/INMIC50486.2020.9318140]
   Aytekin C, 2018, IEEE IJCNN
   Bao YQ, 2019, STRUCT HEALTH MONIT, V18, P401, DOI 10.1177/1475921718757405
   Basahel Abdullah, 2017, International Journal of Information Technology, V9, P287, DOI 10.1007/s41870-017-0029-4
   Basora L, 2019, AEROSPACE-BASEL, V6, DOI 10.3390/aerospace6110117
   Bhuiyan MR, 2021, Bull. Electr. Eng. Inform, V10, P2598, DOI [DOI 10.11591/EEI.V10I5.2361, 10.11591/eei.v10i5.2361]
   Brownlee J, 2020, How to choose loss functions when Training Deep Learning Neural Networks
   Canizo M, 2019, NEUROCOMPUTING, V363, P246, DOI 10.1016/j.neucom.2019.07.034
   Chow JK, 2020, ADV ENG INFORM, V45, DOI 10.1016/j.aei.2020.101105
   Deepak K, 2021, SIGNAL IMAGE VIDEO P, V15, P215, DOI 10.1007/s11760-020-01740-1
   Direkoglu C, 2020, IEEE ACCESS, V8, P80408, DOI 10.1109/ACCESS.2020.2990355
   Doshi K, 2021, Arxiv, DOI arXiv:2103.11299
   Feng JF, 2022, ISPRS INT J GEO-INF, V11, DOI 10.3390/ijgi11030205
   Gao J, 2021, IEEE INTERNET THINGS, V8, P951, DOI 10.1109/JIOT.2020.3009180
   Gao X, 2021, INT J COMPUT VISION, V129, P1202, DOI 10.1007/s11263-020-01427-7
   General Authority for statistics, 2022, Hajj and Umrah Statistics
   Gong D, 2019, IEEE I CONF COMP VIS, P1705, DOI 10.1109/ICCV.2019.00179
   Guan YP, 2021, MULTIMED TOOLS APPL, V80, P18787, DOI 10.1007/s11042-021-10667-9
   Gudi A., 2022, arXiv
   Habib S, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21248291
   Hassan MM, 2020, INFORM SCIENCES, V513, P386, DOI 10.1016/j.ins.2019.10.069
   Kim D, 2021, APPL SCI-BASEL, V11, DOI 10.3390/app11136153
   Ko KE, 2018, ENG APPL ARTIF INTEL, V67, P226, DOI 10.1016/j.engappai.2017.10.001
   Lee S.-W., 2021, arXiv
   Li A, 2020, PATTERN RECOGN, V108, DOI 10.1016/j.patcog.2020.107355
   Lin W, 2021, NEUROCOMPUTING, V436, P248, DOI 10.1016/j.neucom.2021.01.031
   Liu H., 2023, IEEE Trans. Neural Netw. Learn. Syst.
   Liu HM, 2020, PATTERN RECOGN, V99, DOI 10.1016/j.patcog.2019.107112
   Ma JZ, 2019, 2019 2ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND PATTERN RECOGNITION (AIPR 2019), P10, DOI 10.1145/3357254.3357273
   Ma Y, 2020, Annals of Data Science, V9, P187, DOI DOI 10.1007/S40745-020-00253-5
   Madhuri GS, 2020, ADV INTELL SYST COMP, V1054, P499, DOI 10.1007/978-981-15-0135-7_46
   Mansour RF, 2021, IMAGE VISION COMPUT, V112, DOI 10.1016/j.imavis.2021.104229
   Nguyen D. T., 2019, PR MACH LEARN RES, V97, P4800
   Oh DY, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18051308
   Owaidah A., Review of Modelling and Simulating Crowds at Mass Gathering Events: Hajj as a Case Study
   Park Y, 2023, Arxiv, DOI arXiv:2201.05748
   Park Y, 2021, ETRI J, V43, P511, DOI 10.4218/etrij.2020-0052
   Park Y, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18103573
   Pawar K, 2021, 2021 11TH INTERNATIONAL CONFERENCE ON CLOUD COMPUTING, DATA SCIENCE & ENGINEERING (CONFLUENCE 2021), P506, DOI 10.1109/Confluence51648.2021.9377055
   Sabokrou M, 2018, COMPUT VIS IMAGE UND, V172, P88, DOI 10.1016/j.cviu.2018.02.006
   Saini DK, 2016, SMART INNOV SYST TEC, V51, P11, DOI 10.1007/978-3-319-30927-9_2
   Saudi Press Agency, 2019, Hajj Crowd
   Schlegl T, 2019, MED IMAGE ANAL, V54, P30, DOI 10.1016/j.media.2019.01.010
   Shaukat K., 2021, A review of time-series anomaly detection techniques: a step to future perspectives, P865, DOI [10.1007/978-3-030-73100-7_60, DOI 10.1007/978-3-030-73100-7_60]
   Siddiqui AA, 2012, SAFETY SCI, V50, P478, DOI 10.1016/j.ssci.2011.10.011
   Sikdar A, 2020, NEUROCOMPUTING, V415, P317, DOI 10.1016/j.neucom.2020.07.058
   Song W, 2019, IEEE ACCESS, V7, P39172, DOI 10.1109/ACCESS.2019.2906275
   Sonkar Riddhi, 2020, ITM Web of Conferences, V32, DOI 10.1051/itmconf/20203203040
   Tahura S., 2021, ANOMALY DETECTION EL, P205, DOI [10.1007/978-981-33-4673-4_18, DOI 10.1007/978-981-33-4673-4_18]
   Tay N.C., 2018, COMPUTATIONAL SCI TE, P37
   Traoré A, 2020, IEEE SYS MAN CYBERN, P154, DOI [10.1109/smc42975.2020.9282971, 10.1109/SMC42975.2020.9282971]
   Ullah FUM, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19112472
   Ullah W, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21082811
   Usman I, 2021, INT J COMPUT SCI NET, V21, P131, DOI 10.22937/IJCSNS.2021.21.4.18
   Voican O., 2021, INFORM EC, V25, P70
   Xia LM, 2021, J SUPERCOMPUT, V77, P3223, DOI 10.1007/s11227-020-03391-y
   Yang JX, 2021, NEUROCOMPUTING, V444, P170, DOI 10.1016/j.neucom.2020.08.087
   Yang Jie, 2021, arXiv, DOI DOI 10.48550/ARXIV.2109.13157
   Yin CL, 2017, IEEE ACCESS, V5, P21954, DOI 10.1109/ACCESS.2017.2762418
   Zhang XW, 2021, INFORM SCIENCES, V557, P302, DOI 10.1016/j.ins.2019.05.023
NR 65
TC 1
Z9 1
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2023 NOV 1
PY 2023
DI 10.1007/s00371-023-03124-1
EA NOV 2023
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA W3JS2
UT WOS:001090630400001
DA 2024-07-18
ER

PT J
AU Li, ZX
   Wang, YL
   Han, QL
   Peng, C
AF Li, Zi-Xin
   Wang, Yu-Long
   Han, Qing-Long
   Peng, Chen
TI ZRDNet: zero-reference image defogging by physics-based
   decomposition-reconstruction mechanism and perception fusion
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Unsupervised image defogging; Deep neural networks;
   Decomposition-reconstruction mechanism; Perceptual fusion
ID RESTORATION
AB This paper investigates challenging fully unsupervised defogging problems, i.e., how to remove fog by feeding only foggy images in deep neural networks rather than using paired or unpaired synthetic images, and how to overcome the problems of insufficient structure and detail recovery in existing unsupervised defogging methods. For this purpose, a zero-reference image defogging method (ZRDNet) is proposed to solve these two problems. Specifically, we develop an unsupervised defogging network consisting of a layer decomposition network and a perceptual fusion network, which are separately optimized by joint multiple-loss based on the stage-wise learning. The decomposition network guides the image decomposition-reconstruction process by rationally constructing loss functions. The fusion network further enhances the details and contrast of the defogged images by fusing the decomposition-reconstruction results. The joint multiple-loss optimization strategy based on the stage-wise learning guides decomposition and fusion tasks, which are completed stage-by-stage. Additionally, a non-reference loss is constructed to prevent artifacts and distortion induced by transmission value deviation. Our method is completely unsupervised, and training only relies on fog images and information derived from the fog images themselves. Experiments are conducted to demonstrate that our ZRDNet, which overcomes the problems of insufficient structure and detail recovery, and domain shift induced by using synthetic image, achieves favorable performance.
C1 [Li, Zi-Xin; Wang, Yu-Long; Peng, Chen] Shanghai Univ, Sch Mechatron Engn & Automat, Shanghai 200444, Peoples R China.
   [Li, Zi-Xin; Wang, Yu-Long; Peng, Chen] Shanghai Univ, Shanghai Key Lab Power Stn Automat Technol, Shanghai 200444, Peoples R China.
   [Han, Qing-Long] Swinburne Univ Technol, Sch Sci Comp & Engn Technol, Melbourne, Vic 3122, Australia.
C3 Shanghai University; Shanghai University; Swinburne University of
   Technology
RP Wang, YL (corresponding author), Shanghai Univ, Sch Mechatron Engn & Automat, Shanghai 200444, Peoples R China.; Wang, YL (corresponding author), Shanghai Univ, Shanghai Key Lab Power Stn Automat Technol, Shanghai 200444, Peoples R China.
EM zixinli@shu.edu.cn; yulongwang@shu.edu.cn; qhan@swin.edu.au;
   c.peng@i.shu.edu.cn
RI peng, chen/HHS-8720-2022
OI peng, chen/0000-0003-3652-2233
FU This work was supported in part by the National Science Foundation of
   China (Grant Nos. 52371372, 61833011); and the Project of Science and
   Technology Commission of Shanghai Municipality, China (Grant Nos.
   20ZR1420200, 21SQBS01600, 22JC1401400, 19510750300 [52371372, 61833011];
   National Science Foundation of China [20ZR1420200, 21SQBS01600,
   22JC1401400, 19510750300, 21190780300]; Project of Science and
   Technology Commission of Shanghai Municipality, China
FX This work was supported in part by the National Science Foundation of
   China (Grant Nos. 52371372, 61833011); and the Project of Science and
   Technology Commission of Shanghai Municipality, China (Grant Nos.
   20ZR1420200, 21SQBS01600, 22JC1401400, 19510750300, 21190780300).
CR Blau Y, 2019, LECT NOTES COMPUT SC, V11133, P334, DOI 10.1007/978-3-030-11021-5_21
   Bui TM, 2018, IEEE T IMAGE PROCESS, V27, P999, DOI 10.1109/TIP.2017.2771158
   Chen ZY, 2021, PROC CVPR IEEE, P7176, DOI 10.1109/CVPR46437.2021.00710
   Engin D, 2018, IEEE COMPUT SOC CONF, P938, DOI 10.1109/CVPRW.2018.00127
   Gandelsman Y, 2019, PROC CVPR IEEE, P11018, DOI 10.1109/CVPR.2019.01128
   Golts A, 2020, IEEE T IMAGE PROCESS, V29, P2692, DOI 10.1109/TIP.2019.2952032
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Guo CL, 2020, PROC CVPR IEEE, P1777, DOI 10.1109/CVPR42600.2020.00185
   He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huynh-Thu Q, 2008, ELECTRON LETT, V44, P800, DOI 10.1049/el:20080522
   Jiang QP, 2022, IEEE T INTELL TRANSP, V23, P19440, DOI 10.1109/TITS.2022.3165176
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kingma D. P., 2014, arXiv
   Kuanar S, 2022, VISUAL COMPUT, V38, P1121, DOI 10.1007/s00371-021-02071-z
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Li BY, 2019, IEEE T IMAGE PROCESS, V28, P492, DOI 10.1109/TIP.2018.2867951
   Li BY, 2021, INT J COMPUT VISION, V129, P1754, DOI 10.1007/s11263-021-01431-5
   Li BY, 2020, IEEE T IMAGE PROCESS, V29, P8457, DOI 10.1109/TIP.2020.3016134
   Li CY, 2020, IEEE T MULTIMEDIA, V22, P704, DOI 10.1109/TMM.2019.2933334
   Li JF, 2023, IEEE T MULTIMEDIA, V25, P3587, DOI 10.1109/TMM.2022.3163554
   Li LRH, 2020, IEEE T IMAGE PROCESS, V29, P2766, DOI 10.1109/TIP.2019.2952690
   Li RD, 2018, PROC CVPR IEEE, P8202, DOI 10.1109/CVPR.2018.00856
   Li X, 2019, PROC CVPR IEEE, P510, DOI 10.1109/CVPR.2019.00060
   Li XL, 2023, VISUAL COMPUT, V39, P663, DOI 10.1007/s00371-021-02365-2
   Li ZX, 2023, MULTIMED TOOLS APPL, V82, P21535, DOI 10.1007/s11042-022-14103-4
   Narasimhan SG, 2003, IEEE T PATTERN ANAL, V25, P713, DOI 10.1109/TPAMI.2003.1201821
   Qin X, 2020, AAAI CONF ARTIF INTE, V34, P11908
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song YD, 2023, IEEE T IMAGE PROCESS, V32, P1927, DOI 10.1109/TIP.2023.3256763
   Su SL, 2020, PROC CVPR IEEE, P3664, DOI 10.1109/CVPR42600.2020.00372
   Wang SB, 2024, VISUAL COMPUT, V40, P2807, DOI 10.1007/s00371-023-02987-8
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Xu W, 2022, IEEE T CIRC SYST VID, V32, P5736, DOI 10.1109/TCSVT.2022.3153685
   Yang D, 2018, LECT NOTES COMPUT SC, V11211, P729, DOI 10.1007/978-3-030-01234-2_43
   Yang F, 2022, VISUAL COMPUT, V38, P1579, DOI 10.1007/s00371-021-02089-3
   Yang XT, 2018, AAAI CONF ARTIF INTE, P7485
   Yang Y, 2022, PROC CVPR IEEE, P2027, DOI 10.1109/CVPR52688.2022.00208
   Yi WC, 2024, VISUAL COMPUT, V40, P2293, DOI 10.1007/s00371-023-02917-8
   Yin SB, 2020, PATTERN RECOGN, V102, DOI 10.1016/j.patcog.2020.107255
   Zhang H, 2018, PROC CVPR IEEE, P3194, DOI 10.1109/CVPR.2018.00337
   Zhang SD, 2023, VISUAL COMPUT, V39, P953, DOI 10.1007/s00371-021-02377-y
   Zhang XQ, 2022, IEEE T CIRC SYST VID, V32, P3490, DOI 10.1109/TCSVT.2021.3114601
   Zhao H, 2020, COMPUTER VISION ECCV, P56, DOI DOI 10.1007/978-3-030-67070-23
   Zhao SY, 2021, IEEE T IMAGE PROCESS, V30, P3391, DOI 10.1109/TIP.2021.3060873
   Zhao SY, 2020, IEEE T IMAGE PROCESS, V29, P6947, DOI 10.1109/TIP.2020.2995264
   Zhu QS, 2015, IEEE T IMAGE PROCESS, V24, P3522, DOI 10.1109/TIP.2015.2446191
NR 50
TC 1
Z9 1
U1 3
U2 7
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2023 OCT 9
PY 2023
DI 10.1007/s00371-023-03109-0
EA OCT 2023
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA T8PY5
UT WOS:001080561500002
DA 2024-07-18
ER

PT J
AU Tolie, HF
   Faraji, MR
   Qi, XJ
AF Tolie, Hamidreza Farhadi
   Faraji, Mohammad Reza
   Qi, Xiaojun
TI Blind quality assessment of screen content images via edge histogram
   descriptor and statistical moments
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Image quality assessment (IQA); Screen content image (SCI); Edge
   histogram descriptor (EHD); Image content descriptor; Chrominance
   features
ID COLOR; SIMILARITY
AB With the growth in utilizing desktop sharing and remote control applications in recent years for many purposes like online education and remote working, quality assessment (QA) of screen images has become a hot topic. It could be used to enhance the user's quality experience. Currently, most screen image QA methods require a reference image, and the existing blind/no-reference methods do not consider both the image's content and chrominance degradations. This paper proposes a novel blind quality assessment method for screen content images (SCIs) through block-based content representation, which extracts content- and chromatic-based features on local, semi-global, and global scales. Our proposed edge histogram descriptor- and statistical moment-based (EHDSM) method divides the image into 16 blocks and then describes each block using its local edge and semi-global chrominance features. It also takes the global chrominance features into account to investigate how the image's color information is changed in the presence of chrominance distortions. Local features are extracted using edge histogram descriptor, while the semi-global and global features are measured by computing the statistical moments. Next, the quality assessment is achieved by training a support vector regression (SVR) model. Extensive experiments on three commonly used SCI datasets have verified the superiority of our proposed EHDSM method compared with the state-of-the-art blind screen content image quality assessment methods.
C1 [Tolie, Hamidreza Farhadi; Faraji, Mohammad Reza] Inst Adv Studies Basic Sci IASBS, Dept Comp Sci & Informat Technol, Zanjan 4513766731, Iran.
   [Qi, Xiaojun] Utah State Univ, Dept Comp Sci, Logan, UT 84322 USA.
C3 Institute for Advanced Studies in Basic Sciences (IASBS); Utah System of
   Higher Education; Utah State University
RP Tolie, HF (corresponding author), Inst Adv Studies Basic Sci IASBS, Dept Comp Sci & Informat Technol, Zanjan 4513766731, Iran.
EM h.farhadi@iasbs.ac.ir; m.faraji@iasbs.ac.ir; xiaojun.qi@usu.edu
OI Farhadi Tolie, Hamidreza/0000-0002-5433-0182
FU No funding was received for conducting this study. The authors have no
   relevant financial or non-financial interests to disclose.
FX No funding was received for conducting this study. The authors have no
   relevant financial or non-financial interests to disclose.
CR Aharon M, 2006, IEEE T SIGNAL PROCES, V54, P4311, DOI 10.1109/TSP.2006.881199
   Bai Y., 2020, IEEE Trans. Multimedia, V161, P248
   Bai YQ, 2022, IEEE ACCESS, V10, P13174, DOI 10.1109/ACCESS.2022.3141914
   Bai YQ, 2019, SIGNAL PROCESS, V161, P248, DOI 10.1016/j.sigpro.2019.03.013
   Cai RT, 2023, VISUAL COMPUT, V39, P4639, DOI 10.1007/s00371-022-02614-y
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Chen JN, 2018, IEEE SIGNAL PROC LET, V25, P1685, DOI 10.1109/LSP.2018.2871250
   Fang YM, 2021, IEEE T MULTIMEDIA, V23, P955, DOI 10.1109/TMM.2020.2991528
   Fang YM, 2020, IEEE T CIRC SYST VID, V30, P4050, DOI 10.1109/TCSVT.2019.2951747
   Fang YM, 2018, IEEE T IMAGE PROCESS, V27, P1600, DOI 10.1109/TIP.2017.2781307
   Fu Y, 2018, IEEE T CIRC SYST VID, V28, P2428, DOI 10.1109/TCSVT.2018.2854176
   Gerhard HE, 2013, PLOS COMPUT BIOL, V9, DOI 10.1371/journal.pcbi.1002873
   Gu K, 2017, IEEE T IMAGE PROCESS, V26, P4005, DOI 10.1109/TIP.2017.2711279
   Gu K, 2016, IEEE T MULTIMEDIA, V18, P1098, DOI 10.1109/TMM.2016.2547343
   Gu K, 2016, NEUROCOMPUTING, V196, P140, DOI 10.1016/j.neucom.2015.11.101
   Jain AK, 1996, PATTERN RECOGN, V29, P1233, DOI 10.1016/0031-3203(95)00160-3
   Ji JY, 2023, VISUAL COMPUT, V39, P443, DOI 10.1007/s00371-021-02340-x
   Jiang XH, 2021, SIGNAL PROCESS-IMAGE, V94, DOI 10.1016/j.image.2021.116181
   Kabbai L, 2019, VISUAL COMPUT, V35, P679, DOI 10.1007/s00371-018-1503-0
   Kusumoto R, 2014, INT C PATT RECOG, P1490, DOI 10.1109/ICPR.2014.265
   Loh WT, 2019, IEEE IMAGE PROC, P3025, DOI [10.1109/icip.2019.8803254, 10.1109/ICIP.2019.8803254]
   Lu N, 2018, SIGNAL PROCESS, V145, P225, DOI 10.1016/j.sigpro.2017.12.004
   MARR D, 1980, PROC R SOC SER B-BIO, V207, P187, DOI 10.1098/rspb.1980.0020
   Martini MG, 2012, SIGNAL PROCESS-IMAGE, V27, P875, DOI 10.1016/j.image.2012.01.012
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Ni Z., 2016, International Scientific Publications and Consulting Services, P774
   Ni ZK, 2017, IEEE T IMAGE PROCESS, V26, P4818, DOI 10.1109/TIP.2017.2718185
   Ni ZK, 2016, IEEE SIGNAL PROC LET, V23, P1394, DOI 10.1109/LSP.2016.2599294
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Sadykova D, 2017, 2017 INTERNATIONAL CONFERENCE ON ADVANCES IN COMPUTING, COMMUNICATIONS AND INFORMATICS (ICACCI), P2366, DOI 10.1109/ICACCI.2017.8126200
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P3440, DOI 10.1109/TIP.2006.881959
   Sheikh HR, 2005, LIVE IMAGE QUALITY A, DOI DOI 10.1109/CVPR.2015.7298594
   STRICKER M, 1995, P SOC PHOTO-OPT INS, V2410, P381, DOI 10.1117/12.205308
   SWAIN MJ, 1991, INT J COMPUT VISION, V7, P11, DOI 10.1007/BF00130487
   Tolie HF, 2022, SIGNAL PROCESS-IMAGE, V101, DOI 10.1016/j.image.2021.116562
   van de Weijer J, 2006, LECT NOTES COMPUT SC, V3952, P334
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Video Quality Experts Group, 2000, Final report from the video quality experts group on the validation of objective models of video quality assessment march 2000
   Wang R., 2019, IEEE Access, V7, P4818
   Wang SQ, 2016, IEEE J EM SEL TOP C, V6, P532, DOI 10.1109/JETCAS.2016.2598756
   Wang SQ, 2016, IEEE T CIRC SYST VID, V26, P1595, DOI 10.1109/TCSVT.2015.2461891
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Won CS, 2002, ETRI J, V24, P23, DOI 10.4218/etrij.02.0102.0103
   Wu JJ, 2015, IEEE T IMAGE PROCESS, V24, P4602, DOI 10.1109/TIP.2015.2460467
   Xue WF, 2014, IEEE T IMAGE PROCESS, V23, P4850, DOI 10.1109/TIP.2014.2355716
   Yang H, 2015, IEEE T IMAGE PROCESS, V24, P4408, DOI 10.1109/TIP.2015.2465145
   Zhai GT, 2012, IEEE T IMAGE PROCESS, V21, P41, DOI 10.1109/TIP.2011.2161092
   Zhang L, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2426416
   Zheng L., 2020, IEEE Trans. Cybern, V52, P2798
   Zheng LR, 2019, IEEE T MULTIMEDIA, V21, P2057, DOI 10.1109/TMM.2019.2894939
NR 51
TC 0
Z9 0
U1 4
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2023 OCT 1
PY 2023
DI 10.1007/s00371-023-03108-1
EA OCT 2023
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA T1IK0
UT WOS:001075590000001
DA 2024-07-18
ER

PT J
AU Csoba, I
   Kunkli, R
AF Csoba, Istvan
   Kunkli, Roland
TI Fast rendering of central and peripheral human visual aberrations across
   the entire visual field with interactive personalization
SO VISUAL COMPUTER
LA English
DT Article
DE Human vision simulation; Eye structure estimation; Aberration
   estimation; Neural networks; Convolution; Point-spread function
   interpolation
ID HUMAN-EYE; VIRTUAL-REALITY; ACCOMMODATION; IMAGE; COMPENSATION;
   SIMULATION; MECHANISM; ERRORS; DEPTH; LENS
AB With the recent progress made in areas such as head-mounted displays and vision-correcting devices, there is a growing interest in fast and personalized algorithms for simulating aberrated human vision. Existing vision-simulating approaches are generally hindered by the lack of personalization, computational cost of rendering, and limited types of supported aberrations. This paper presents a fast vision simulation method with interactive personalization capabilities for simulating arbitrary central and peripheral aberrations of the human eye. First, we describe a novel, neural network-based solution for efficiently estimating the physical structure of the simulated eye and calculating the necessary Zernike aberration coefficients for computing the point-spread functions with varying pupil sizes, focus distances, and incidence angles. Our new approach operates in the sub-second regime and produces highly accurate outputs, facilitating the interactive personalization of vision simulation. Next, we present an improved PSF interpolation method for an existing tiled PSF splatting algorithm for rendering. The proposed algorithm significantly improves the computational performance and memory efficiency of the previous approach, allowing the simulation of peripheral vision with arbitrary visual aberrations in low-latency applications. Following the description of our new techniques, we evaluate their performance characteristics and simulation accuracies on several different eye conditions and test scenarios and compare our results to several previous vision simulation algorithms.
C1 [Csoba, Istvan; Kunkli, Roland] Univ Debrecen, Fac Informat, Debrecen, Hungary.
   [Csoba, Istvan] Univ Debrecen, Doctoral Sch Informat, Debrecen, Hungary.
C3 University of Debrecen; University of Debrecen
RP Csoba, I (corresponding author), Univ Debrecen, Fac Informat, Debrecen, Hungary.; Csoba, I (corresponding author), Univ Debrecen, Doctoral Sch Informat, Debrecen, Hungary.
EM csoba.istvan@inf.unideb.hu; kunkli.roland@inf.unideb.hu
RI ; Kunkli, Roland/A-2541-2013
OI Csoba, Istvan/0000-0002-4703-9979; Kunkli, Roland/0000-0003-3947-6586
FU We gratefully acknowledge the support of NVIDIA Corporation with the
   donation of the TITAN Xp GPU used for this research.; NVIDIA Corporation
FX We gratefully acknowledge the support of NVIDIA Corporation with the
   donation of the TITAN Xp GPU used for this research.
CR Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265
   [Anonymous], 2020, MATLAB-Version 9.9.0 (R2020b)
   Antonello J, 2015, J OPT SOC AM A, V32, P1160, DOI 10.1364/JOSAA.32.001160
   Arefin MS, 2021, 2021 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES ABSTRACTS AND WORKSHOPS (VRW 2021), P731, DOI 10.1109/VRW52623.2021.00248
   Ates H.C., 2015, P 9 INT C TANGIBLE E, P225, DOI [DOI 10.1145/2677199.2680551, 10.1145/2677199.2680551]
   Aydindogan G, 2021, BIOMED OPT EXPRESS, V12, P511, DOI 10.1364/BOE.405026
   Ba J. L., 2016, LAYER NORMALIZATION, DOI DOI 10.48550/ARXIV.1607.06450
   Barsky B.A., 2004, APGV 04, P73, DOI DOI 10.1145/1012551.1012564
   Barsky BA, 2008, REC ADV COMPUT ENG, P999
   Barsky BA, 2015, LECT NOTES COMPUT SC, V8927, P524, DOI 10.1007/978-3-319-16199-0_37
   Barsky BA, 2011, COMM COM INF SC, V229, P3
   BHATIA AB, 1954, P CAMB PHILOS SOC, V50, P40, DOI 10.1017/S0305004100029066
   Bickerdt J, 2019, J EYE MOVEMENT RES, V12, DOI 10.16910/jemr.12.3.9
   Camp J. J., 1990, Proceedings of the First Conference on Visualization in Biomedical Computing (Cat. No.90TH0311-1), P278, DOI 10.1109/VBC.1990.109333
   Chen DW, 2020, ENTROPY-SWITZ, V22, DOI 10.3390/e22020193
   Cheng X, 2004, J VISION, V4, P310, DOI 10.1167/4.4.7
   Cholewiak SA, 2018, J VISION, V18, DOI 10.1167/18.9.1
   Cholewiak SA, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130815
   Cook R. L., 1984, Computers & Graphics, V18, P137
   Csoba I., 2022, 2022 IEEE 2 C INF TE, P69, DOI [10.1109/CITDS54976.2022.9914232, DOI 10.1109/CITDS54976.2022.9914232]
   Csoba I, 2023, VIS COMPUT IND BIOME, V6, DOI 10.1186/s42492-023-00132-9
   Csoba I, 2021, COMPUT GRAPH FORUM, V40, P182, DOI 10.1111/cgf.14267
   Dai Guang-ming., 2008, WAVEFRONT OPTICS VIS
   Dias C., 2016, P 37 ANN C EUR ASS C, P61, DOI [10.2312/egsh.20161015, DOI 10.2312/EGSH.20161015]
   Duchowski A.T., 2014, P ACM S APPL PERC, P39, DOI DOI 10.1145/2628257.2628259
   Fink W, 2006, J BIOMED OPT, V11, DOI 10.1117/1.2357734
   Fülep C, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-44160-z
   Gonzalez-Utrera D., 2018, THESIS U ARIZONA
   GREIVENKAMP JE, 1995, AM J OPHTHALMOL, V120, P227, DOI 10.1016/S0002-9394(14)72611-X
   GU YC, 1987, J OPT SOC AM A, V4, P1681, DOI 10.1364/JOSAA.4.001681
   Hermans EA, 2009, INVEST OPHTH VIS SCI, V50, P281, DOI 10.1167/iovs.08-2124
   HERRMANN J, 1980, J OPT SOC AM, V70, P28, DOI 10.1364/JOSA.70.000028
   Huang FC, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601122
   Ioffe S, 2015, P INT C MACH LEARN, V2015, P1
   Iskander M, 2021, ASIA-PAC J OPHTHALMO, V10, P244, DOI 10.1097/APO.0000000000000409
   Itoh Y., 2015, P 6 AUGM HUM INT C A, P1, DOI [10.1145/2735711, DOI 10.1145/2735711]
   Janssen AJEM, 2004, J MOD OPTIC, V51, P687, DOI 10.1080/09500340310001631635
   Jin B, 2005, P ANN INT IEEE EMBS, P5128
   Kanazawa Katsuhisa, 2011, Journal of the Institute of Image Electronics Engineers of Japan, V40, P151
   Keles O, 2019, INT CONF IMAG PROC
   Kessenich J., 2016, OpenGL Programming Guide: The official guide to learning OpenGL
   Kingma D. P., 2014, arXiv
   Kordek D, 2021, SCI REP-UK, V11, DOI 10.1038/s41598-021-82965-z
   Krösl K, 2020, INT SYM MIX AUGMENT, P682, DOI 10.1109/ISMAR50242.2020.00098
   Krösl K, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P655, DOI [10.1109/VR.2019.8798239, 10.1109/vr.2019.8798239]
   Krösl K, 2018, VISUAL COMPUT, V34, P911, DOI 10.1007/s00371-018-1517-7
   Krösl K, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES WORKSHOPS (VRW 2020), P831, DOI [10.1109/VRW50115.2020.000-9, 10.1109/VRW50115.2020.00266]
   Krueger ML, 2016, SIBGRAPI, P64, DOI [10.1109/SIBGRAPI.2016.17, 10.1109/SIBGRAPI.2016.018]
   Li TK, 2021, FRONT MED-LAUSANNE, V8, DOI 10.3389/fmed.2021.733241
   Lian T, 2019, J VISION, V19, DOI 10.1167/19.12.23
   Lima ARC, 2021, VISUAL COMPUT, V37, P2581, DOI 10.1007/s00371-021-02194-3
   Liu L., 2020, 8 INT C LEARN REPR I
   Loos J, 1998, COMPUT GRAPH FORUM, V17, pC255, DOI 10.1111/1467-8659.00272
   Loshchilov I., 2019, DECOUPLED WEIGHT DEC
   Lyu Q, 2021, VISUAL COMPUT, V37, P2341, DOI 10.1007/s00371-020-01990-7
   Mantiuk R, 2011, LECT NOTES COMPUT SC, V6944, P1, DOI 10.1007/978-3-642-23834-5_1
   McGraw T, 2015, VISUAL COMPUT, V31, P601, DOI 10.1007/s00371-014-0986-6
   Misra D., 2019, ARXIV190808681
   Mostafawy S, 1997, IEEE COMPUT GRAPH, V17, P8, DOI 10.1109/38.576849
   Nair V., 2010, P 27 INT C MACHINE L, P807
   NAVARRO R, 1985, J OPT SOC AM A, V2, P1273, DOI 10.1364/JOSAA.2.001273
   NieSSner Matthias., 2012, Proceedings of the 11th ACM SIGGRAPH International Conference on Virtual-Reality Continuum and its Applications in Industry, P195, DOI [10.1145/2407516.2407565, DOI 10.1145/2407516.2407565]
   Odden JL, 2020, INVEST OPHTH VIS SCI, V61, DOI 10.1167/iovs.61.13.23
   Ong CW, 2021, J MED INTERNET RES, V23, DOI 10.2196/24152
   Rodriguez Celaya JA., 2005, ACT 15 C ESP INF GRA, P43
   Rokita P, 1996, IEEE COMPUT GRAPH, V16, P18, DOI 10.1109/38.486676
   Rosenholtz R, 2016, ANNU REV VIS SCI, V2, P437, DOI 10.1146/annurev-vision-082114-035733
   Sauer A., 2021, Advances in Neural Information Processing Systems, V34, P17480
   Schuster K, 2020, P ACM COMPUT GRAPH, V3, DOI 10.1145/3406182
   Seidemann A, 2002, J OPT SOC AM A, V19, P2363, DOI 10.1364/JOSAA.19.002363
   Shen J., 2014, OPHTHALMOLOGY CURREN, P177, DOI [10.5772/58456, DOI 10.5772/58456]
   Sun YL, 2001, VISUAL COMPUT, V17, P429, DOI 10.1007/s003710100116
   Tabernero J, 2007, J OPT SOC AM A, V24, P3274, DOI 10.1364/JOSAA.24.003274
   Tabernero J, 2006, INVEST OPHTH VIS SCI, V47, P4651, DOI 10.1167/iovs.06-0444
   Tabernero J, 2011, J OPT SOC AM A, V28, P1889, DOI 10.1364/JOSAA.28.001889
   Tang N, 2015, 14TH ACM SIGGRAPH INTERNATIONAL CONFERENCE ON VIRTUAL REALITY CONTINUUM AND ITS APPLICATIONS IN INDUSTRY, VRCAI 2015, P39, DOI 10.1145/2817675.2817686
   van Haver S., 2010, THESIS DELFT U TECHN
   Vu CT, 2021, PROC SPIE, V11353, DOI 10.1117/12.2555872
   Wang W., 2020, INTELLIGENT PLANNING, DOI [10.17638/03090577, DOI 10.17638/03090577]
   Watson AB, 2008, J VISION, V8, DOI 10.1167/8.4.17
   Wei Q, 2014, COMPUT METH PROG BIO, V114, P302, DOI 10.1016/j.cmpb.2014.02.003
   Wei X, 2008, OPT EXPRESS, V16, P20490, DOI 10.1364/OE.16.020490
   Wu J., 2011, 32 ANN C EUROPEAN AS, P37, DOI [10.2312/EG2011/short/037-040, DOI 10.2312/EG2011/SHORT/037-040]
   Wu JZ, 2013, VISUAL COMPUT, V29, P41, DOI 10.1007/s00371-012-0673-4
   Xiao L, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275032
   Xiong YZ, 2021, FRONT NEUROSCI-SWITZ, V15, DOI 10.3389/fnins.2021.671121
   Xu F, 2018, 25TH 2018 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P246, DOI 10.1109/VR.2018.8447557
   Yamamoto K, 2021, PROCEEDINGS OF THE AUGMENTED HUMANS CONFERENCE 2021, AHS 2021, P204, DOI 10.1145/3458709.3458955
   Zaman N., 2021, THESIS U NEVADA RENO
   Zhang M. R., 2019, Advances in Neural Information Processing Systems, P1, DOI DOI 10.48550/ARXIV.1907
   Zhao J., 2021, 2021 6 INT C IM VIS, P202, DOI [10.1109/ICIVC52351.2021.9526982, DOI 10.1109/ICIVC52351.2021.9526982]
   Zhu Z, 2019, VISUAL COMPUT, V35, P1053, DOI 10.1007/s00371-019-01689-4
NR 92
TC 0
Z9 0
U1 3
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2024
VL 40
IS 5
BP 3709
EP 3731
DI 10.1007/s00371-023-03060-0
EA SEP 2023
PG 23
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OC3D9
UT WOS:001073086400001
OA hybrid
DA 2024-07-18
ER

PT J
AU Xu, RY
   Jin, Y
   Zhang, HX
   Zhang, Y
   Lai, YK
   Zhu, Z
   Zhang, FL
AF Xu, Rongyan
   Jin, Yao
   Zhang, Huaxiong
   Zhang, Yun
   Lai, Yu-kun
   Zhu, Zhe
   Zhang, Fang-Lue
TI A variational approach for feature-aware B-spline curve design on
   surface meshes
SO VISUAL COMPUTER
LA English
DT Article
DE B-spline curve; Surface meshes; Feature aware; Knot insertion;
   Variational optimization
ID SUBDIVISION
AB Robust curve design on surface meshes with flexible controls is useful in a wide range of applications but remains challenging. Most existing methods fall into one of the two strategies: one is to discretize a curve into a polyline, which is then optimized, and the other is to directly design smooth splines on meshes. While the former approach usually needs a sufficiently dense sampling of curve points, which is computational costly, the latter approach relaxes the sampling requirement but suffers from the lack of user control. To tackle these problems, we proposed a variational method for designing feature-aware B-spline curves on surface meshes. Given the recent advances in shell space construction methods, we could relax the B-spline curve inside a simplified shell mesh and evaluate its distance to the surface using equipped bijective mapping. To effectively minimize the distance between the curve and the surface, with additional controls in the form of both internal and external constraints, we applied the interior point method and adaptively inserted knots of the spline to increase its freedom and adjust the weighting during the iterations. When the curve is close enough to the surface, it can be efficiently sampled at any resolution and robustly projected to the surface. Experiments show that our method is more robust, has higher flexibility, and generates smoother results than existing methods.
C1 [Xu, Rongyan; Jin, Yao; Zhang, Huaxiong] Zhejiang Sci Tech Univ, Sch Comp Sci & Technol, Hangzhou 310018, Peoples R China.
   [Jin, Yao; Zhang, Huaxiong] Zhejiang Prov Innovat Ctr Adv Text Technol, Shaoxing 312000, Peoples R China.
   [Zhang, Yun] Commun Univ Zhejiang, Coll Media Engn, Hangzhou 310018, Peoples R China.
   [Lai, Yu-kun] Cardiff Univ, Sch Comp Sci & Informat, Cardiff CF24 4AG, Wales.
   [Zhu, Zhe] Mathworks, Natick, MA 01760 USA.
   [Zhang, Fang-Lue] Victoria Univ Wellington, Sch Engn & Comp Sci, Wellington 6012, New Zealand.
C3 Zhejiang Sci-Tech University; Communication University of Zhejiang;
   Cardiff University; MathWorks; Victoria University Wellington
RP Zhang, HX (corresponding author), Zhejiang Sci Tech Univ, Sch Comp Sci & Technol, Hangzhou 310018, Peoples R China.; Zhang, HX (corresponding author), Zhejiang Prov Innovat Ctr Adv Text Technol, Shaoxing 312000, Peoples R China.
EM 1536980672@qq.com; zhxhz@zstu.edu.cn
RI Lai, Yu-Kun/D-2343-2010
FU Key R amp;D Programs of Zhejiang Province [2023C01224]; National Natural
   Science Foundation of China [23232106-Y]; Fundamental Research Funds of
   Zhejiang Sci-Tech University [LGG22F020009]; Zhejiang Province Public
   Welfare Technology Application Research [2020E10015]; Key Lab of Film;
   TV Media Technology of Zhejiang Province [jgxm202131]; Teaching Reform
   Project of Communication University of Zhejiang [2022C01220]; 
   [61702458]
FX We would like to thank all the anonymous reviewers for their valuable
   comments. We would also like to thank Wanqiang Shen from Jiannan
   University for her helpful discussions. This work was supported by Key R
   & D Programs of Zhejiang Province (No. 2022C01220, 2023C01224) and
   National Natural Science Foundation of China (No. 61702458) and the
   Fundamental Research Funds of Zhejiang Sci-Tech University (No.
   23232106-Y). Yun Zhang was partially supported by the Zhejiang Province
   Public Welfare Technology Application Research (No. LGG22F020009), Key
   Lab of Film and TV Media Technology of Zhejiang Province (No.
   2020E10015), and Teaching Reform Project of Communication University of
   Zhejiang (No. jgxm202131).
CR Cheng LT, 2002, J COMPUT PHYS, V175, P604, DOI 10.1006/jcph.2001.6960
   Sarlabous JE, 2012, VISUAL COMPUT, V28, P971, DOI 10.1007/s00371-012-0728-6
   Guennebaud G., 2010, Eigen
   Hofer M, 2004, ACM T GRAPHIC, V23, P284, DOI 10.1145/1015706.1015716
   Hofer M., 2007, SER SCCG 07, P27
   Ji ZP, 2006, COMPUT GRAPH FORUM, V25, P283, DOI 10.1111/j.1467-8659.2006.00947.x
   Jiang Z., 2020, ACM T GRAPHIC, V39
   Jin Y, 2019, COMPUT AIDED DESIGN, V113, P24, DOI 10.1016/j.cad.2019.03.001
   Jung M, 2004, 12TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P87
   Kang HM, 2015, COMPUT AIDED DESIGN, V58, P179, DOI 10.1016/j.cad.2014.08.022
   Lawonn K, 2014, COMPUT GRAPH-UK, V40, P22, DOI 10.1016/j.cag.2014.01.004
   Lee Y, 2005, COMPUT AIDED GEOM D, V22, P444, DOI 10.1016/j.cagd.2005.04.002
   Lee YJ, 2002, COMPUT GRAPH FORUM, V21, P229, DOI 10.1111/1467-8659.t01-1-00582
   Liang FS, 2017, MEAS SCI TECHNOL, V28, DOI 10.1088/1361-6501/aa6a05
   Liu Z, 2017, J SCI COMPUT, V70, P631, DOI 10.1007/s10915-016-0260-3
   Livesu M, 2018, COMPUT GRAPH-UK, V71, P124, DOI 10.1016/j.cag.2018.01.004
   Mancinelli C., 2023, IEEE T COMPUT GRAPH
   Mohanty SD, 2021, COMPUTATION STAT, V36, P155, DOI 10.1007/s00180-020-01022-x
   Morera DM, 2008, VISUAL COMPUT, V24, P1025, DOI 10.1007/s00371-008-0298-9
   Panozzo D., 2013, ACM T GRAPHIC, V32, P1
   Park F.C., 1994, INT DES ENG TECHN C, V12846, P15
   Pottmann H, 2005, COMPUT AIDED GEOM D, V22, P693, DOI 10.1016/j.cagd.2005.06.006
   Sharp N, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417839
   Dung VT, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0173857
   Wallner J, 2006, ACM T GRAPHIC, V25, P356, DOI 10.1145/1138450.1138459
   Yeh R, 2020, COMPUT AIDED DESIGN, V128, DOI 10.1016/j.cad.2020.102905
   Zheng WN, 2012, COMPUT AIDED GEOM D, V29, P448, DOI 10.1016/j.cagd.2012.03.004
NR 27
TC 1
Z9 1
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2023
VL 39
IS 8
SI SI
BP 3767
EP 3781
DI 10.1007/s00371-023-03001-x
EA JUL 2023
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA P2DS6
UT WOS:001035510700002
OA Bronze
DA 2024-07-18
ER

PT J
AU Yi, WC
   Dong, LQ
   Liu, M
   Hui, M
   Kong, LQ
   Zhao, YJ
AF Yi, Weichao
   Dong, Liquan
   Liu, Ming
   Hui, Mei
   Kong, Lingqin
   Zhao, Yuejin
TI MFAF-Net: image dehazing with multi-level features and adaptive fusion
SO VISUAL COMPUTER
LA English
DT Article
DE Image dehazing; Multi-level feature; Adaptive fusion; Deep learning
ID VISION
AB Hazy images often cause blurring, detail loss and color distortion, which makes it difficult to address the other visual tasks such as tracking, classification and object detection. In recent years, significant advances have been made in image dehazing task, dominated by convolutional neural networks (CNNs). Most existing CNNs methods tend to estimate the transmission map and atmospheric light and then recover the haze-free image based on atmospheric scattering model. However, its dehazing performance is limited due to inaccurate estimation. To this end, we present a new architecture called multi-level features and adaptive fusion network (MFAF-Net) for single image dehazing, which can obtain the haze-free image in an end-to-end manner. For one thing, we utilize a novel context enhanced module as the core of feature extraction, and it combines multi-scale dilation convolution layers with feature attention module, which enables to acquire much informative contextual information. For another, we present a new fusion approach called adaptive fusion module for both low- and high-level feature fusion, and it can provide more flexibility when handling features of inconsistent semantic and level; thus, our network restores images with more detailed information. Experimental results on both synthetic and real-world datasets demonstrate that MFAF-Net outperforms existing state-of-the-art methods in terms of quantitative and qualitative evaluation metrics. The code will be made publicly available on Github.
C1 [Yi, Weichao; Dong, Liquan; Liu, Ming; Hui, Mei; Kong, Lingqin; Zhao, Yuejin] Beijing Inst Technol, Sch Opt & Photon, Beijing Key Lab Precis Optoelect Measurement Instr, Beijing 100081, Peoples R China.
   [Dong, Liquan; Liu, Ming; Kong, Lingqin; Zhao, Yuejin] Beijing Inst Technol, Yangtze Delta Reg Acad, Jiaxing 314019, Peoples R China.
C3 Beijing Institute of Technology; Beijing Institute of Technology
RP Dong, LQ; Liu, M (corresponding author), Beijing Inst Technol, Sch Opt & Photon, Beijing Key Lab Precis Optoelect Measurement Instr, Beijing 100081, Peoples R China.; Dong, LQ; Liu, M (corresponding author), Beijing Inst Technol, Yangtze Delta Reg Acad, Jiaxing 314019, Peoples R China.
EM kylind@bit.edu.cn; bit411liu@bit.edu.cn
RI yi, weichao/GWQ-6196-2022
FU Beijing Key Laboratory of Precision Photoelectric Measuring Instrument
   and Technology; Winter Olympics Key Project Technology Fund
   [2018YFF0300804]
FX AcknowledgementsThis work was supported by the Beijing Key Laboratory of
   Precision Photoelectric Measuring Instrument and Technology, the Winter
   Olympics Key Project Technology Fund (2018YFF0300804).
CR Ancuti CO, 2021, IEEE COMPUT SOC CONF, P627, DOI 10.1109/CVPRW53098.2021.00074
   Ancuti CO, 2020, IEEE COMPUT SOC CONF, P2029, DOI 10.1109/CVPRW50498.2020.00253
   Ancuti CO, 2018, IEEE COMPUT SOC CONF, P867, DOI 10.1109/CVPRW.2018.00119
   Ancuti C, 2018, LECT NOTES COMPUT SC, V11182, P620, DOI 10.1007/978-3-030-01449-0_52
   Berman D, 2016, PROC CVPR IEEE, P1674, DOI 10.1109/CVPR.2016.185
   Bin Xie, 2010, Proceedings 2010 International Conference on Intelligent System Design and Engineering Application (ISDEA 2010), P848, DOI 10.1109/ISDEA.2010.141
   Blau Y, 2019, LECT NOTES COMPUT SC, V11133, P334, DOI 10.1007/978-3-030-11021-5_21
   Cai BL, 2016, IEEE T IMAGE PROCESS, V25, P5187, DOI 10.1109/TIP.2016.2598681
   Chen C, 2022, IEEE T INTELL TRANSP, V23, P19655, DOI 10.1109/TITS.2021.3128012
   Chen DD, 2019, IEEE WINT CONF APPL, P1375, DOI 10.1109/WACV.2019.00151
   Das DK, 2022, VISUAL COMPUT, V38, P3803, DOI 10.1007/s00371-021-02222-2
   Das SD, 2020, IEEE COMPUT SOC CONF, P1994, DOI 10.1109/CVPRW50498.2020.00249
   Dong Y, 2020, AAAI CONF ARTIF INTE, V34, P10729
   Engin D., 2018, CYCLE DEHAZE ENHANCE
   Fan WS, 2021, APPL INTELL, V51, P5822, DOI 10.1007/s10489-020-02056-w
   Fu J., 2020, ARXIV
   He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu Q, 2023, VISUAL COMPUT, V39, P997, DOI 10.1007/s00371-021-02380-3
   Huang T, 2021, PROC CVPR IEEE, P14776, DOI 10.1109/CVPR46437.2021.01454
   Huynh-Thu Q, 2008, ELECTRON LETT, V44, P800, DOI 10.1049/el:20080522
   Jia ZQ, 2023, VISUAL COMPUT, V39, P1205, DOI 10.1007/s00371-022-02398-1
   Khongkraphan K, 2021, APPL COMPUT INTELL S, V2021, DOI 10.1155/2021/6684345
   Li BY, 2019, IEEE T IMAGE PROCESS, V28, P492, DOI 10.1109/TIP.2018.2867951
   Li BY, 2017, IEEE I CONF COMP VIS, P4780, DOI 10.1109/ICCV.2017.511
   Li J., 2020, IEEE T IMAGE PROCESS, VPP, P1
   Li LRH, 2020, IEEE T IMAGE PROCESS, V29, P2766, DOI 10.1109/TIP.2019.2952690
   Li X, 2018, LECT NOTES COMPUT SC, V11211, P262, DOI 10.1007/978-3-030-01234-2_16
   Li XL, 2023, VISUAL COMPUT, V39, P663, DOI 10.1007/s00371-021-02365-2
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu XH, 2019, IEEE I CONF COMP VIS, P7313, DOI 10.1109/ICCV.2019.00741
   McCartney E. J., 1976, Optics of the atmosphere. Scattering by molecules and particles
   Narasimhan SG, 2002, INT J COMPUT VISION, V48, P233, DOI 10.1023/A:1016328200723
   Narasimhan SG, 2000, PROC CVPR IEEE, P598, DOI 10.1109/CVPR.2000.855874
   Qu YY, 2019, PROC CVPR IEEE, P8152, DOI 10.1109/CVPR.2019.00835
   Redmon J, 2018, Arxiv, DOI [arXiv:1804.02767, DOI 10.1109/CVPR.2017.690, DOI 10.48550/ARXIV.1804.02767]
   Ren WQ, 2018, PROC CVPR IEEE, P3253, DOI 10.1109/CVPR.2018.00343
   Ren WQ, 2016, LECT NOTES COMPUT SC, V9906, P154, DOI 10.1007/978-3-319-46475-6_10
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Shao YJ, 2020, PROC CVPR IEEE, P2805, DOI 10.1109/CVPR42600.2020.00288
   Singh D, 2019, APPL INTELL, V49, P4276, DOI 10.1007/s10489-019-01504-6
   Ullah H, 2021, IEEE T IMAGE PROCESS, V30, P8968, DOI 10.1109/TIP.2021.3116790
   Wang C, 2020, APPL INTELL, V50, P2932, DOI 10.1007/s10489-020-01693-5
   Wang C, 2020, APPL INTELL, V50, P1437, DOI 10.1007/s10489-019-01567-5
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang Z, 2019, FFA NET FEATURE FUSI
   Xie J., 2021, Vis. Comput, V10, P1
   Yang F, 2022, VISUAL COMPUT, V38, P1579, DOI 10.1007/s00371-021-02089-3
   Yang WH, 2021, IEEE T PATTERN ANAL, V43, P4059, DOI 10.1109/TPAMI.2020.2995190
   Yu F., 2015, ARXIV
   Zhang H, 2018, PROC CVPR IEEE, P3194, DOI 10.1109/CVPR.2018.00337
   Zhang SD, 2023, VISUAL COMPUT, V39, P953, DOI 10.1007/s00371-021-02377-y
   Zhang SD, 2020, VISUAL COMPUT, V36, P1797, DOI 10.1007/s00371-019-01774-8
   Zhang XQ, 2020, COMPUT VIS IMAGE UND, V197, DOI 10.1016/j.cviu.2020.103003
   Zhu QS, 2015, IEEE T IMAGE PROCESS, V24, P3522, DOI 10.1109/TIP.2015.2446191
NR 55
TC 4
Z9 4
U1 4
U2 14
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2024
VL 40
IS 4
BP 2293
EP 2307
DI 10.1007/s00371-023-02917-8
EA JUL 2023
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA MZ2U3
UT WOS:001027416100001
DA 2024-07-18
ER

PT J
AU Wang, Y
   Luan, HQ
   Wang, LT
   Song, SZ
   Bian, YL
   Bao, XY
   Liu, R
   Gai, W
   Lv, GR
   Yang, CL
AF Wang, Yu
   Luan, Hongqiu
   Wang, Lutong
   Song, Shengzun
   Bian, Yulong
   Bao, Xiyu
   Liu, Ran
   Gai, Wei
   Lv, Gaorong
   Yang, Chenglei
TI MGP: a monitoring-based VR interactive mode to support guided practice
SO VISUAL COMPUTER
LA English
DT Article
DE Virtual reality; Interactive mode; VR training; Guided practice
AB Currently, VR training systems are primarily designed according to the non-directive mode supporting independent practice without a real teacher. Students must complete all system operations resulting in heavy interaction burden and confusion. In this paper, a monitoring-based VR interactive mode is designed to support guided practice in the presence of teacher resources. The mode mainly realizes the separation of teacher-student interaction tasks, environment and equipment, helping the students focus on their own interactions and tasks. It also provides multi-channel information of the students for teachers to better understand students' training status and make precise guidance. A platform and two applications were developed based on this mode for user study. The results show that the proposed mode can improve learning efficiency and enhance the immersive experience.
C1 [Wang, Yu; Luan, Hongqiu; Wang, Lutong; Bian, Yulong; Bao, Xiyu; Gai, Wei; Lv, Gaorong; Yang, Chenglei] Shandong Univ, Sch Software, Shunhua Rd, Jinan 250000, Shandong, Peoples R China.
   [Song, Shengzun] Natl Police Univ Criminal Justice, Coll Prison Sci, Qiyi Middle Rd, Baoding 071000, Hebei, Peoples R China.
   [Liu, Ran] Shandong Normal Univ, Acad Fine Arts, Wenhuadong Rd, Jinan 250000, Shandong, Peoples R China.
C3 Shandong University; National Police University for Criminal Justice;
   Shandong Normal University
RP Gai, W; Lv, GR (corresponding author), Shandong Univ, Sch Software, Shunhua Rd, Jinan 250000, Shandong, Peoples R China.; Song, SZ (corresponding author), Natl Police Univ Criminal Justice, Coll Prison Sci, Qiyi Middle Rd, Baoding 071000, Hebei, Peoples R China.
EM 16811170@qq.com; luanxiaoqiu@mail.sdu.edu.cn;
   wanglutong1002@mail.sdu.edu.cn; songshenzun@163.com;
   bianyulong@sdu.edu.cn; 201820489@mail.sdu.edu.cn; 386922576@qq.com;
   gw@sdu.edu.cn; 15953173765@163.com; chl_yang@sdu.edu.cn
OI bao, xiyu/0009-0002-5847-0876
FU National Key R amp;D Program of China [2022ZD0118002]; National Natural
   Science Foundation of China [62277035, 62007021, 61972233]
FX AcknowledgementsWe would like to thank all reviewers for their valuable
   comments. This work is supported by National Key R &D Program of China
   (2022ZD0118002) and the National Natural Science Foundation of China
   under Grant (62277035, 62007021 and 61972233).
CR Al Farsi Ghaliya, 2021, International Journal of Interactive Mobile Technologies, V15, P99, DOI [10.3991/ijim.v15i22.25003, DOI 10.3991/IJIM.V15I22.25003]
   Albus P, 2021, COMPUT EDUC, V166, DOI 10.1016/j.compedu.2021.104154
   [Anonymous], 2015, J ONLINE LEARN RES P
   Bastug E, 2017, IEEE COMMUN MAG, V55, P110, DOI 10.1109/MCOM.2017.1601089
   Bian Y., 2018, COMP 3D DISPLAY TECH
   Boxin Wan, 2020, 2020 IEEE International Conference on Power, Intelligent Computing and Systems (ICPICS), P87, DOI 10.1109/ICPICS50287.2020.9202208
   Brough John E., 2007, Virtual Reality, V11, P189, DOI 10.1007/s10055-007-0076-4
   Dunleavy M, 2009, J SCI EDUC TECHNOL, V18, P7, DOI 10.1007/s10956-008-9119-1
   Ghufron MA, 2018, INT J INSTR, V11, P657, DOI 10.12973/iji.2018.11441a
   Han J., 2021, J ADV INF TECHNOL, V12
   Harvey S., 2007, STRATEGIES WORK TEAC, V2n
   Kao Dominic, 2021, Proceedings of the ACM on Human-Computer Interaction, V5, DOI 10.1145/3474661
   Kim YM, 2020, INT J HUM-COMPUT INT, V36, P893, DOI 10.1080/10447318.2019.1699746
   Knierim P, 2020, CHI'20: EXTENDED ABSTRACTS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3334480.3382920
   Lang Zhou, 2020, 2020 IEEE 9th Global Conference on Consumer Electronics (GCCE), P655, DOI 10.1109/GCCE50665.2020.9291758
   LaViola Joseph J., 2017, 3D User interfaces: theory and practice
   Lazorko O., 2020, SYST REV PHARM, V11, P1316
   Liu DJ, 2017, SMART COMPUT INTELL, P105, DOI 10.1007/978-981-10-5490-7_7
   Mäkinen H, 2022, BEHAV INFORM TECHNOL, V41, P1, DOI 10.1080/0144929X.2020.1788162
   Makransky G, 2019, LEARN INSTR, V60, P225, DOI 10.1016/j.learninstruc.2017.12.007
   Ojala S, 2022, MED EDUC ONLINE, V27, DOI 10.1080/10872981.2022.2050345
   Rheinberg F., 2002, P 1 INT POS PSYCH SU
   Salkevicius J, 2019, ELECTRONICS-SWITZ, V8, DOI 10.3390/electronics8091039
   Sandars J., 2020, Twelve tips for rapidly migrating to online learning during the Covid-19 pandemic
   Sasinka C, 2019, ISPRS INT J GEO-INF, V8, DOI 10.3390/ijgi8010003
   Seabrook E, 2020, J MED INTERNET RES, V22, DOI 10.2196/16106
   Serafin G., 2004, SOUND DESIGN ENHANCE
   Sharratt L., 2013, PERSPECTIVES TRANSIT
   Stanica I, 2018, 2018 ZOOMING INNOVATION IN CONSUMER TECHNOLOGIES CONFERENCE (ZINC), P9, DOI 10.1109/ZINC.2018.8448645
   ThoraviKumaravel B., 2019, P 2019 CHI C HUMAN F, P1
   Urueta SH, 2020, ADV INTELL SYST, V1036, P359, DOI 10.1007/978-3-030-29029-0_33
   Voigt-Antons, 2019, INT WORK QUAL MULTIM, P1, DOI DOI 10.1109/qomex.2019.8743273
   Wang C., 2019, COMP NONDIRECTIVE TE
   Wang F, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES WORKSHOPS (VRW 2020), P797, DOI [10.1109/VRW50115.2020.00249, 10.1109/VRW50115.2020.00-26]
   Wang X, 2014, L N INST COMP SCI SO, V100, P79, DOI 10.1007/978-3-319-11564-1_9
   Wingrave CA, 2010, PRESENCE-TELEOP VIRT, V19, P179, DOI 10.1162/pres.19.2.179
   Witmer BG, 1998, PRESENCE-TELEOP VIRT, V7, P225, DOI 10.1162/105474698565686
   Zhang J, 2020, INT J CONTIN ENG EDU, V30, P313, DOI 10.1504/IJCEELL.2020.108543
   Zhao HT, 2020, ROY SOC OPEN SCI, V7, DOI 10.1098/rsos.191523
NR 39
TC 0
Z9 0
U1 4
U2 14
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2023
VL 39
IS 8
SI SI
BP 3387
EP 3401
DI 10.1007/s00371-023-02950-7
EA JUL 2023
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA P2DS6
UT WOS:001017717100003
DA 2024-07-18
ER

PT J
AU Tao, WY
   Hua, XH
   He, XX
   Liu, JB
   Xu, D
AF Tao, Wuyong
   Hua, Xianghong
   He, Xiaoxing
   Liu, Jingbin
   Xu, Dong
TI Automatic multi-view registration of point clouds via a high-quality
   descriptor and a novel 3D transformation estimation technique
SO VISUAL COMPUTER
LA English
DT Article
DE Local shape descriptor; Geometric distance constraint; Correspondence
   removal; Pairwise registration; Multi-view registration
ID OBJECT RECOGNITION; HISTOGRAMS; ALGORITHM; IMAGES
AB Generally, performing multiple scans is necessary to cover entire scanning area, and multiple point clouds are thus obtained. These point clouds need to be registered together, which is the focus of our work. However, influenced by the noise, point density variation, partial overlap and so on, the success rate of registration is still low. For this reason, a pairwise registration method is first proposed. It includes two main components: the triple local coordinate image (TriLCI) descriptor and a new 3D transformation estimation technique. The descriptor has high descriptiveness and strong robustness, by which more right correspondences can be achieved. The 3D transformation estimation technique is employed to calculate the rotation matrix and translation vector under the condition that most of correspondences are false. In this technique, a new geometric distance constraint is developed, and is then applied to eliminate incorrect correspondences. The robust estimation based on the median is introduced to calculate the rotation matrix and translation vector. For multi-view registration, the connected graph algorithm and shape growing based method are applied. A comparative study is presented. The experimental results well demonstrate that the proposed pairwise registration method has high success rate. The comparative study gives the merits and demerits of the two multi-view registration methods.
C1 [Tao, Wuyong] Nanchang Univ, Sch Math & Comp Sci, Nanchang 330031, Peoples R China.
   [Hua, Xianghong] Wuhan Univ, Sch Geomat & Geodesy, Wuhan 430079, Peoples R China.
   [He, Xiaoxing] Jiangxi Univ Sci & Technol, Sch Civil & Surveying & Mapping Engn, Ganzhou 341000, Peoples R China.
   [Liu, Jingbin; Xu, Dong] Wuhan Univ, State Key Lab Informat Engn Surveying Mapping & Re, Wuhan 430079, Peoples R China.
   [Liu, Jingbin] Finnish Geospatial Res Inst, Dept Remote Sensing & Photogrammetry, Masala 02430, Finland.
C3 Nanchang University; Wuhan University; Jiangxi University of Science &
   Technology; Wuhan University; The National Land Survey of Finland;
   Finnish Geospatial Research Institute (FGI)
RP Tao, WY (corresponding author), Nanchang Univ, Sch Math & Comp Sci, Nanchang 330031, Peoples R China.
EM taowuyong@ncu.edu.cn
RI Tao, Wuyong/AAI-1973-2019
OI Tao, Wuyong/0000-0003-0821-644X
FU National Natural Science Foundation of China [41674005, 42104023];
   Jiangxi University of Science and Technology High-level Talent Research
   Startup Project [2021205200100564, 41874031]
FX AcknowledgementsThe authors would like to acknowledge the publishers of
   the UWA3M and ICL-NUIM datasets. This work is supported by the National
   Natural Science Foundation of China (41674005, 42104023), Jiangxi
   University of Science and Technology High-level Talent Research Startup
   Project (2021205200100564) and Spatial Cognition Augmented
   High-usability High-precision Smartphone Indoor Positioning (41874031).
CR Aiger D, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360684
   Albarelli A, 2010, PROC CVPR IEEE, P430, DOI 10.1109/CVPR.2010.5540183
   BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791
   Chen H, 2007, PATTERN RECOGN LETT, V28, P1252, DOI 10.1016/j.patrec.2007.02.009
   Chen XJ, 2020, MEASUREMENT, V160, DOI 10.1016/j.measurement.2020.107852
   Chen XJ, 2018, IEEE T GEOSCI REMOTE, V56, P105, DOI [10.1109/TGRS.2017.2737471, 10.1109/tgrs.2017.2737471]
   Choi S, 2015, PROC CVPR IEEE, P5556, DOI 10.1109/CVPR.2015.7299195
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Frome A, 2004, LECT NOTES COMPUT SC, V3023, P224
   Guo YL, 2015, IEEE T INSTRUM MEAS, V64, P683, DOI 10.1109/TIM.2014.2358131
   Guo YL, 2014, IEEE T MULTIMEDIA, V16, P1377, DOI 10.1109/TMM.2014.2316145
   Guo YL, 2013, INT J COMPUT VISION, V105, P63, DOI 10.1007/s11263-013-0627-y
   Huber DF, 2003, IMAGE VISION COMPUT, V21, P637, DOI 10.1016/S0262-8856(03)00060-X
   Johnson AE, 1999, IEEE T PATTERN ANAL, V21, P433, DOI 10.1109/34.765655
   Kusari A, 2019, IEEE T GEOSCI REMOTE, V57, P3404, DOI 10.1109/TGRS.2018.2884712
   Lim J, 2019, VISUAL COMPUT, V35, P71, DOI 10.1007/s00371-017-1453-y
   Liu JB, 2021, IEEE J-STARS, V14, P5627, DOI 10.1109/JSTARS.2021.3068796
   Malassiotis S, 2007, IEEE T PATTERN ANAL, V29, P1285, DOI 10.1109/TPAMI.2007.1060
   Mian AS, 2006, IEEE T PATTERN ANAL, V28, P1584, DOI 10.1109/TPAMI.2006.213
   Mohamad M, 2015, 2015 INTERNATIONAL CONFERENCE ON 3D VISION, P598, DOI 10.1109/3DV.2015.74
   Newcombe RA, 2011, INT SYM MIX AUGMENT, P127, DOI 10.1109/ISMAR.2011.6092378
   Quan SW, 2018, INFORM SCIENCES, V444, P153, DOI 10.1016/j.ins.2018.02.070
   Rousseeuw PJ, 2018, WIRES DATA MIN KNOWL, V8, DOI 10.1002/widm.1236
   Rusu RB, 2009, IEEE INT CONF ROBOT, P1848
   Steder B, 2011, IEEE INT CONF ROBOT, P2601, DOI 10.1109/ICRA.2011.5980187
   Tang KK, 2017, IEEE ACCESS, V5, P1833, DOI 10.1109/ACCESS.2017.2658681
   Tao WY, 2021, IEEE T GEOSCI REMOTE, V59, P801, DOI 10.1109/TGRS.2020.2998683
   Tao WY, 2021, SURV REV, V53, P454, DOI 10.1080/00396265.2020.1831829
   Tao WY, 2020, REMOTE SENS-BASEL, V12, DOI 10.3390/rs12081283
   Tao WY, 2020, PHOTOGRAMM ENG REM S, V86, P121, DOI 10.14358/PERS.86.2.121
   Tian PJ, 2020, IEEE ACCESS, V8, P30873, DOI 10.1109/ACCESS.2020.2973580
   Tombari F, 2010, LECT NOTES COMPUT SC, V6313, P356, DOI 10.1007/978-3-642-15558-1_26
   Yang JQ, 2023, VISUAL COMPUT, V39, P3073, DOI 10.1007/s00371-022-02514-1
   Yang JQ, 2020, IEEE T IMAGE PROCESS, V29, P2522, DOI 10.1109/TIP.2019.2959236
   Yang JQ, 2021, IEEE T PATTERN ANAL, V43, P1859, DOI 10.1109/TPAMI.2019.2960234
   Yang JQ, 2017, PATTERN RECOGN, V65, P175, DOI 10.1016/j.patcog.2016.11.019
   Zai DW, 2017, ISPRS J PHOTOGRAMM, V134, P15, DOI 10.1016/j.isprsjprs.2017.10.001
   Zhang YH, 2021, PATTERN RECOGN, V111, DOI 10.1016/j.patcog.2020.107691
   Zhao B, 2020, INFORM SCIENCES, V512, P295, DOI 10.1016/j.ins.2019.04.020
   Zhao BF, 2020, IEEE T GEOSCI REMOTE, V58, P7890, DOI 10.1109/TGRS.2020.2984943
   Zhao H, 2020, PATTERN RECOGN, V103, DOI 10.1016/j.patcog.2020.107272
   Zhou W, 2019, VISUAL COMPUT, V35, P489, DOI 10.1007/s00371-018-1478-x
NR 42
TC 3
Z9 3
U1 5
U2 17
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2024
VL 40
IS 4
BP 2615
EP 2630
DI 10.1007/s00371-023-02942-7
EA JUN 2023
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA MZ2U3
UT WOS:001020988900003
DA 2024-07-18
ER

PT J
AU Zhu, TC
   Zhu, SQ
   Zheng, T
   Ding, HL
   Song, W
   Li, CJ
AF Zhu, Tiancheng
   Zhu, Shiqiang
   Zheng, Tao
   Ding, Hongliang
   Song, Wei
   Li, Cunjun
TI HEU-Net: hybrid attention residual block-based network with external
   skip connections for metal corrosion semantic segmentation
SO VISUAL COMPUTER
LA English
DT Article
DE Corrosion detection; Semantic segmentation; Attention; Residual; Skip
   connections
AB Regularly detect the corrosion of metal structures and take countermeasures according to the degree of corrosion, which can reduce the potential safety hazards and avoid unnecessary economic losses. This paper presents a deep learning-based metal corrosion detection method, which is used to accurately segment the corrosion regions. This model incorporates Hybrid Attention Residual Block (HARB) and External Skip Connections (ESC) in the basic architecture of U-Net. HARB can reweight the features so that the network pays more attention to the corrosion regions and ignores other irrelevant regions. Shallow features and deep features are fused using ESC to make the features of network learning more effective and comprehensive, which is conducive to improving the segmentation accuracy. In order to further improve the semantic segmentation performance, the Attention Gate Residual Block (AGRB) is proposed, which can optimize segmentation by strengthening the regions to be segmented and reducing the activation value of the background, while overcoming the semantic and scale inconsistencies between input features. In this paper, comprehensive comparative experiments are conducted on the Metal Corrosion dataset. The experimental results show that the F1 and mIoU of the proposed method are 91.21% and 89.56%, respectively. Compared with U-Net, HEU-Net achieves a 7.91% improvement in F1 and a 10.16% improvement in mIoU. These results show that our method outperforms state-of-the-art models and can achieve better semantic segmentation results.
C1 [Zhu, Tiancheng; Zhu, Shiqiang] Zhejiang Univ, Ocean Coll, Zhoushan, Peoples R China.
   [Zhu, Tiancheng; Zhu, Shiqiang; Zheng, Tao; Song, Wei] Zhejiang Lab, Hangzhou, Peoples R China.
   [Ding, Hongliang] Zhejiang Construct Engn Grp Co Ltd, Hangzhou, Peoples R China.
   [Li, Cunjun] Zhoushan Inst Calibrat & Testing Qualitat & Tech, Zhoushan, Peoples R China.
C3 Zhejiang University; Zhejiang Laboratory
RP Song, W (corresponding author), Zhejiang Lab, Hangzhou, Peoples R China.
EM weisong@zhejianglab.com
RI Zhu, Tiancheng/KLD-6194-2024
OI WEI, SONG/0000-0002-0828-7486
FU open foundation of "Pioneer" and "Leading Goose" R&D Program of Zhejiang
   [2022C01130]
FX Thiswork is supported by open foundation of "Pioneer" and "Leading
   Goose" R&D Program of Zhejiang; 2022C01130.
CR Ashwinkumar S, 2022, MATER TODAY-PROC, V51, P480, DOI 10.1016/j.matpr.2021.05.584
   Bastian BT, 2019, NDT&E INT, V107, DOI 10.1016/j.ndteint.2019.102134
   Boutros F, 2022, IEEE COMPUT SOC CONF, P1577, DOI 10.1109/CVPRW56347.2022.00164
   Cai YF, 2022, IEEE T INTELL TRANSP, V23, P5298, DOI 10.1109/TITS.2021.3052908
   Chen WJ, 2021, VISUAL COMPUT, V37, P805, DOI 10.1007/s00371-020-01831-7
   Fei Z., 2021, INTELLIGENT LIFE SYS, P145, DOI [10.1007/978-981-16-7207-1_15, DOI 10.1007/978-981-16-7207-1_15]
   Fuentes A, 2017, SENSORS-BASEL, V17, DOI 10.3390/s17092022
   Gajjar R, 2022, VISUAL COMPUT, V38, P2923, DOI 10.1007/s00371-021-02164-9
   Gao HB, 2022, APPL SCI-BASEL, V12, DOI 10.3390/app12136569
   Ha H, 2021, APPL SCI-BASEL, V11, DOI 10.3390/app11146378
   Han QH, 2021, J CIV STRUCT HEALTH, V11, P1375, DOI 10.1007/s13349-021-00515-7
   Hansson CM, 2011, METALL MATER TRANS A, V42A, P2952, DOI 10.1007/s11661-011-0703-2
   He K., 2016, PROC CVPR IEEE, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]
   Hou B., 2019, INTRO STUDY CORROSIO, DOI [10.1007/978-981-32-9354-0_1, DOI 10.1007/978-981-32-9354-0_1]
   Hou B., 2019, The Cost of Corrosion in China, P1, DOI [10.1007/978-981-32-9354-01, DOI 10.1007/978-981-32-9354-01]
   Hu JW, 2023, VISUAL COMPUT, V39, P1503, DOI 10.1007/s00371-022-02425-1
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang YB, 2020, VISUAL COMPUT, V36, P85, DOI 10.1007/s00371-018-1588-5
   Katsamenis Iason, 2020, Advances in Visual Computing. 15th International Symposium, ISVC 2020. Proceedings. Lecture Notes in Computer Science (LNCS 12509), P160, DOI 10.1007/978-3-030-64556-4_13
   Khayatazad M, 2020, DEV BUILT ENVIRON, V3, DOI 10.1016/j.dibe.2020.100022
   Liu L., 2019, P 2019 3 HIGH PERF C, P75, DOI [10.1145/3341069.3342966, DOI 10.1145/3341069.3342966]
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Lv ZH, 2022, IEEE T INTELL TRANSP, V23, P16666, DOI 10.1109/TITS.2021.3113779
   Ma YC, 2018, PROC SPIE, V10602, DOI 10.1117/12.2296540
   Medeiros FNS, 2010, EURASIP J ADV SIG PR, DOI 10.1155/2010/817473
   Michailidis N., 2018, CORROSION INT ACAD P, DOI [10.1007/978-3-642-35950-7_16862-1, DOI 10.1007/978-3-642-35950-7_16862-1]
   Nash W., 2019, CORROSION-US
   Nash W, 2022, NPJ MAT DEGRAD, V6, DOI 10.1038/s41529-022-00232-6
   Hoang ND, 2020, MATH PROBL ENG, V2020, DOI 10.1155/2020/6765274
   Oktay O., 2018, ARXIV, DOI DOI 10.48550/ARXIV.1804.03999
   Papamarkou T, 2021, NUCL ENG TECHNOL, V53, P657, DOI 10.1016/j.net.2020.07.020
   Pirie C., 2021, P 22 ENG APPL NEUR N, V3, DOI [10.1007/978-3, DOI 10.1007/978-3]
   Qian C., 2019, EVALUATION DEEP LEAR, DOI [10.25394/PGS.9959090.v1, DOI 10.25394/PGS.9959090.V1]
   Qingcheng Chen, 2019, IOP Conference Series: Materials Science and Engineering, V608, DOI 10.1088/1757-899X/608/1/012020
   Qiu HB, 2022, IEEE T PATTERN ANAL, V44, P6939, DOI 10.1109/TPAMI.2021.3098962
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Shah D, 2022, INFORM PROCESS AGR, V9, P212, DOI 10.1016/j.inpa.2021.06.001
   Shi JY, 2021, APPL SCI-BASEL, V11, DOI 10.3390/app11020518
   Stoean R, 2021, INT SYMP SYMB NUMERI, P246, DOI 10.1109/SYNASC54541.2021.00049
   Su BY, 2022, IEEE T IND ELECTRON, V69, P3161, DOI 10.1109/TIE.2021.3070507
   Syed-Ab-Rahman SF, 2022, APPL INTELL, V52, P927, DOI 10.1007/s10489-021-02452-w
   Torralba A, 2011, PROC CVPR IEEE, P1521, DOI 10.1109/CVPR.2011.5995347
   Torralba A, 2010, P IEEE, V98, P1467, DOI 10.1109/JPROC.2010.2050290
   Trujillo M, 2004, BBA LIB, V5303, P161, DOI 10.1117/12.526838
   Üzen H, 2023, VISUAL COMPUT, V39, P1745, DOI 10.1007/s00371-022-02442-0
   Vallabhajosyula S, 2022, J PLANT DIS PROTECT, V129, P545, DOI 10.1007/s41348-021-00465-8
   Wang Dalei, 2018, Journal of South China University of Technology (Natural Science Edition), V46, P121, DOI 10.3969/j.issn.1000-565X.2018.12.015
   Woo S., 2018, P EUR C COMP VIS ECC, DOI DOI 10.1007/978-3-030-01234-2_1
   Xie EZ, 2021, ADV NEUR IN, V34
   Yan CG, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3472810
   Yang S, 2018, IEEE T PATTERN ANAL, V40, P1845, DOI 10.1109/TPAMI.2017.2738644
   Yang TJ, 2022, VISUAL COMPUT, V38, P2871, DOI 10.1007/s00371-021-02161-y
   Yao Y, 2019, APPL OCEAN RES, V90, DOI 10.1016/j.apor.2019.05.008
   Zhang ZP, 2019, IEEE ACM T COMPUT BI, V16, P407, DOI 10.1109/TCBB.2017.2704587
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zhong Z, 2020, AAAI CONF ARTIF INTE, V34, P13001
   Zhou QF, 2022, SIGNAL IMAGE VIDEO P, V16, P1701, DOI 10.1007/s11760-021-02126-7
   Zhou Zongwei, 2018, Deep Learn Med Image Anal Multimodal Learn Clin Decis Support (2018), V11045, P3, DOI [10.1007/978-3-030-00889-5_1, 10.1007/978-3-030-00689-1_1]
NR 59
TC 3
Z9 3
U1 3
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2024
VL 40
IS 2
BP 1273
EP 1287
DI 10.1007/s00371-023-02846-6
EA JUN 2023
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GE6E8
UT WOS:001014470900001
DA 2024-07-18
ER

PT J
AU Tang, QW
   Yan, P
   Sun, W
AF Tang, Qingwei
   Yan, Pu
   Sun, Wei
TI Visible-infrared person re-identification employing style-supervision
   and content-supervision
SO VISUAL COMPUTER
LA English
DT Article
DE Visible-infrared person re-identification; Cross-modal; Instance
   normalization module; Spatial feature mapping; Image style and content
ID INVARIANT
AB Cross-modal visible-infrared person re-identification (VI-ReID) aims to retrieve images of the same pedestrians captured by visible (VIS) cameras and infrared (IR) cameras and it is a challenging task in intelligent security systems. The differences in imaging principles between visible and infrared images lead to large cross-modal differences and intra-class differences, and such cross-modal image differences can be considered as special image style differences, while several intra-class differences can be considered as differences in the form of content expression between visible and infrared images. Some state-of-the-art methods improve the performance of the VI-ReID model by using additional feature enhancement or feature generation modules, however, these methods also introduce additional parameters and increase the training cost. In this paper, to mitigate the differences in image style and content between VIS and IR images, we design two objective functions based on content and style, which are style loss and content loss for the VI-ReID task, respectively. Our model can effectively mitigate the differences between modes by optimizing the objective function to map VIS and IR features into the same feature space without additional auxiliary modules. After extensive experiments, our model achieves competitive performance on two challenging datasets. Notably, under the visible2infrared setting on the RegDB dataset, our model achieves the state-of-the-art (SOTA) Rank-1/mAP/mINP = 96.13%/91.35%/83.67%.
C1 [Tang, Qingwei; Yan, Pu] Anhui Jianzhu Univ, Anhui Int Joint Res Ctr Intelligent Percept & High, 292, Ziyun Rd, Econ & Technol Dev Zone, Hefei 230601, Anhui, Peoples R China.
   [Tang, Qingwei; Sun, Wei] Hefei Univ Technol, Sch Elect Engn & Automat, 193 Tunxi Rd, Hefei 230009, Anhui, Peoples R China.
C3 Anhui Jianzhu University; Hefei University of Technology
RP Yan, P (corresponding author), Anhui Jianzhu Univ, Anhui Int Joint Res Ctr Intelligent Percept & High, 292, Ziyun Rd, Econ & Technol Dev Zone, Hefei 230601, Anhui, Peoples R China.
EM tqwprivateemail@163.com; yp8188@ahjzu.edu.cn; wsun@hfut.edu.cn
OI Qingwei, Tang/0000-0001-6743-6608
FU Key Research Project of Natural Science in Anhui Province
   [2022AH050249]; Anhui Provincial Graduate Education Quality Project
   Academic Innovation Project [2022xscx116]
FX This work was supported by the Key Research Project of Natural Science
   in Anhui Province under Grant 2022AH050249 and by the Anhui Provincial
   Graduate Education Quality Project Academic Innovation Project under
   Grant 2022xscx116.
CR Chen F., 2021, ARXIV
   Chen YHS, 2021, PROC CVPR IEEE, P587, DOI 10.1109/CVPR46437.2021.00065
   Cheng D, 2016, PROC CVPR IEEE, P1335, DOI 10.1109/CVPR.2016.149
   Dai PY, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P677
   Fan X, 2022, VISUAL COMPUT, V38, P279, DOI 10.1007/s00371-020-02015-z
   Feng ZX, 2020, IEEE T IMAGE PROCESS, V29, P579, DOI 10.1109/TIP.2019.2928126
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Gray D, 2008, LECT NOTES COMPUT SC, V5302, P262, DOI 10.1007/978-3-540-88682-2_21
   Hao Y, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P57, DOI 10.1145/3343031.3351006
   Hao Y, 2019, AAAI CONF ARTIF INTE, P8385
   Hinton G., 2015, arXiv preprint arXiv:1503.02531
   Jing YC, 2020, IEEE T VIS COMPUT GR, V26, P3365, DOI 10.1109/TVCG.2019.2921336
   Li DG, 2020, AAAI CONF ARTIF INTE, V34, P4610
   Liu H., 2022, arXiv
   Liu HJ, 2021, IEEE T MULTIMEDIA, V23, P4414, DOI 10.1109/TMM.2020.3042080
   Luo H, 2020, IEEE T MULTIMEDIA, V22, P2597, DOI 10.1109/TMM.2019.2958756
   Mang Ye, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12362), P229, DOI 10.1007/978-3-030-58520-4_14
   Nguyen DT, 2017, SENSORS-BASEL, V17, DOI 10.3390/s17030605
   Pervaiz N, 2023, VISUAL COMPUT, V39, P4087, DOI 10.1007/s00371-022-02577-0
   Prosser B., 2010, P BRIT MACH VIS C BM, DOI DOI 10.5244/C.24.21
   Pu N, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2149, DOI 10.1145/3394171.3413673
   Seokeon Choi, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10254, DOI 10.1109/CVPR42600.2020.01027
   Su C, 2017, IEEE I CONF COMP VIS, P3980, DOI 10.1109/ICCV.2017.427
   Sun YF, 2018, LECT NOTES COMPUT SC, V11208, P501, DOI 10.1007/978-3-030-01225-0_30
   Ulyanov Dmitry, 2016, arXiv
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wang GA, 2020, AAAI CONF ARTIF INTE, V34, P12144
   Wang GA, 2019, IEEE I CONF COMP VIS, P3622, DOI 10.1109/ICCV.2019.00372
   Wang P., 2022, Vis Comput, P1
   Wang SL, 2018, PROC CVPR IEEE, P2589, DOI 10.1109/CVPR.2018.00274
   Wang ZX, 2019, PROC CVPR IEEE, P618, DOI 10.1109/CVPR.2019.00071
   Wu AC, 2020, INT J COMPUT VISION, V128, P1765, DOI 10.1007/s11263-019-01290-1
   Wu AC, 2017, IEEE I CONF COMP VIS, P5390, DOI 10.1109/ICCV.2017.575
   Wu Q, 2021, PROC CVPR IEEE, P4328, DOI 10.1109/CVPR46437.2021.00431
   Ye M, 2022, IEEE T PATTERN ANAL, V44, P924, DOI 10.1109/TPAMI.2020.3013379
   Ye M, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1092
   Ye M, 2022, IEEE T PATTERN ANAL, V44, P2872, DOI 10.1109/TPAMI.2021.3054775
   Ye M, 2021, IEEE T INF FOREN SEC, V16, P728, DOI 10.1109/TIFS.2020.3001665
   Ye M, 2018, AAAI CONF ARTIF INTE, P7501
   Zhang LY, 2021, IEEE T NEUR NET LEAR, DOI 10.1109/TNNLS.2021.3085978
   Zhang YK, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P788, DOI 10.1145/3474085.3475250
NR 41
TC 1
Z9 1
U1 4
U2 10
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2024
VL 40
IS 4
BP 2443
EP 2456
DI 10.1007/s00371-023-02929-4
EA JUN 2023
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA MZ2U3
UT WOS:001009368800001
DA 2024-07-18
ER

PT J
AU Wang, WM
   Chen, YB
   Wang, DW
   Tie, ZX
   Tao, LB
   Ke, W
AF Wang, Wangmeng
   Chen, Yanbing
   Wang, Dengwen
   Tie, Zhixin
   Tao, Linbing
   Ke, Wei
TI Joint attribute soft-sharing and contextual local: a multi-level
   features learning network for person re-identification
SO VISUAL COMPUTER
LA English
DT Article
DE Multi-level features; Attribute feature; Local feature; Person
   re-identification
AB In person re-identification (re-id), the key to retrieving the correct person image is to extract discriminative features. The features at different levels are considered complementary. In this work, we design a person re-id learning network that can extract mutually multi-level features called ASCLNet. ASCLNet contains three feature branches, and each branch can extract mutually different levels of features. Furthermore, we propose two novel modules and apply them to learning local and attribute features in ASCLNet. One is the contextual local module, which can learn the local feature with context information from the local body part; the other is the attribute soft-sharing module, which enables shared feature representation among attributes. With the support of these two modules, ASCLNet can extract multi-level features that are more discriminative. Moreover, experimental results show that ASCLNet achieves excellent performances on Market-1501 and DukeMTMC-reID datasets with mAP of 88.85% and 80.18%, respectively.
C1 [Wang, Wangmeng; Chen, Yanbing; Wang, Dengwen; Tie, Zhixin; Tao, Linbing] Zhejiang Sci Tech Univ, Sch Comp Sci & Technol, Hangzhou 310018, Zhejiang, Peoples R China.
   [Tie, Zhixin] Zhejiang Sci Tech Univ, KeYi Coll, Shaoxing 312369, Zhejiang, Peoples R China.
   [Ke, Wei] Macao Polytech Univ, Fac Appl Sci, Macau 999078, Peoples R China.
C3 Zhejiang Sci-Tech University; Zhejiang Sci-Tech University; Macao
   Polytechnic University
RP Chen, YB (corresponding author), Zhejiang Sci Tech Univ, Sch Comp Sci & Technol, Hangzhou 310018, Zhejiang, Peoples R China.
EM 202130504167@mails.zstu.edu.cn; cyb@zstu.edu.cn;
   202130504161@mails.zstu.edu.cn; tiezx@zstu.edu.cn; lb_tao@zstu.edu.cn;
   wke@mpu.edu.mo
RI Ke, Wei/JXM-8153-2024
OI chen, yanbing/0000-0002-6818-0437; Ke, Wei/0000-0003-0952-0961; Tie,
   Zhixin/0000-0002-6468-2309; Wang, Dengwen/0000-0002-6741-1328
CR Arandjelovic R, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.92
   Bedagkar-Gala A, 2014, IMAGE VISION COMPUT, V32, P270, DOI 10.1016/j.imavis.2014.02.001
   Chikontwe P, 2018, IEEE ACCESS, V6, P60801, DOI 10.1109/ACCESS.2018.2875783
   Fan X, 2019, J VIS COMMUN IMAGE R, V60, P51, DOI 10.1016/j.jvcir.2019.01.010
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hou RB, 2019, PROC CVPR IEEE, P9309, DOI 10.1109/CVPR.2019.00954
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Jin HY, 2022, IEEE T CIRC SYST VID, V32, P2170, DOI 10.1109/TCSVT.2021.3088446
   Köstinger M, 2012, PROC CVPR IEEE, P2288, DOI 10.1109/CVPR.2012.6247939
   Li HJ, 2021, PROC CVPR IEEE, P6725, DOI 10.1109/CVPR46437.2021.00666
   Li W, 2018, PROC CVPR IEEE, P2285, DOI 10.1109/CVPR.2018.00243
   Li W, 2014, PROC CVPR IEEE, P152, DOI 10.1109/CVPR.2014.27
   Liao SC, 2015, PROC CVPR IEEE, P2197, DOI 10.1109/CVPR.2015.7298832
   Lin YT, 2019, PATTERN RECOGN, V95, P151, DOI 10.1016/j.patcog.2019.06.006
   Liu JW, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P737, DOI 10.1145/3240508.3240585
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Luo H, 2019, IEEE COMPUT SOC CONF, P1487, DOI 10.1109/CVPRW.2019.00190
   Mansouri N, 2021, NEURAL COMPUT APPL, V33, P12827, DOI 10.1007/s00521-021-05936-5
   Pan XG, 2018, LECT NOTES COMPUT SC, V11208, P484, DOI 10.1007/978-3-030-01225-0_29
   Park H, 2020, AAAI CONF ARTIF INTE, V34, P11839
   Pervaiz N, 2023, VISUAL COMPUT, V39, P4087, DOI 10.1007/s00371-022-02577-0
   Qian XL, 2017, IEEE I CONF COMP VIS, P5409, DOI 10.1109/ICCV.2017.577
   Su C, 2018, IEEE T PATTERN ANAL, V40, P1167, DOI 10.1109/TPAMI.2017.2679002
   Sun YF, 2019, PROC CVPR IEEE, P393, DOI 10.1109/CVPR.2019.00048
   Sun YF, 2018, LECT NOTES COMPUT SC, V11208, P501, DOI 10.1007/978-3-030-01225-0_30
   Tay CP, 2021, IEEE IMAGE PROC, P1144, DOI 10.1109/ICIP42928.2021.9506595
   Tay CP, 2019, PROC CVPR IEEE, P7127, DOI 10.1109/CVPR.2019.00730
   Wang C, 2018, LECT NOTES COMPUT SC, V11208, P384, DOI 10.1007/978-3-030-01225-0_23
   Wang FQ, 2016, PROC CVPR IEEE, P1288, DOI 10.1109/CVPR.2016.144
   Wang GS, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P274, DOI 10.1145/3240508.3240552
   Wang P., 2022, Vis Comput, P1
   Wang SL, 2018, PROC CVPR IEEE, P2589, DOI 10.1109/CVPR.2018.00274
   Wang X, 2022, PATTERN RECOGN, V121, DOI 10.1016/j.patcog.2021.108220
   Wang ZQ, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON VISUAL COMMUNICATIONS AND IMAGE PROCESSING (IEEE VCIP)
   Wei LH, 2019, IEEE T MULTIMEDIA, V21, P986, DOI 10.1109/TMM.2018.2870522
   Wei LH, 2018, PROC CVPR IEEE, P79, DOI 10.1109/CVPR.2018.00016
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Wu GL, 2022, PATTERN RECOGN, V121, DOI 10.1016/j.patcog.2021.108239
   Xi JL, 2023, PATTERN RECOGN, V134, DOI 10.1016/j.patcog.2022.109068
   Xie JH, 2022, VISUAL COMPUT, V38, P2515, DOI 10.1007/s00371-021-02127-0
   Yang F, 2019, PATTERN RECOGN, V86, P143, DOI 10.1016/j.patcog.2018.08.015
   Yang J, 2022, NEURAL COMPUT APPL, V34, P8241, DOI 10.1007/s00521-022-06903-4
   Ye M, 2022, IEEE T PATTERN ANAL, V44, P2872, DOI 10.1109/TPAMI.2021.3054775
   Zeng HT, 2020, IEEE INT CON MULTI, DOI 10.1109/icme46284.2020.9102757
   Zhang L, 2022, NEURAL COMPUT APPL, V34, P11817, DOI 10.1007/s00521-022-07071-1
   Zhao HY, 2017, PROC CVPR IEEE, P907, DOI 10.1109/CVPR.2017.103
   Zhao Q, 2021, IET IMAGE PROCESS, V15, P3599, DOI 10.1049/ipr2.12292
   Zheng F, 2019, PROC CVPR IEEE, P8506, DOI 10.1109/CVPR.2019.00871
   Zheng L, 2019, IEEE T IMAGE PROCESS, V28, P4500, DOI 10.1109/TIP.2019.2910414
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zheng M, 2019, PROC CVPR IEEE, P5728, DOI 10.1109/CVPR.2019.00588
   Zheng ZD, 2017, IEEE I CONF COMP VIS, P3774, DOI 10.1109/ICCV.2017.405
   Zhong Z, 2017, PROC CVPR IEEE, P3652, DOI 10.1109/CVPR.2017.389
   Zhou YL, 2022, SENSORS-BASEL, V22, DOI 10.3390/s22166293
   Zhu K., 2020, ECCV, P346, DOI [10.1007/978-3-030-58580-8_21, DOI 10.1007/978-3-030-58580-8_21]
NR 56
TC 1
Z9 1
U1 8
U2 21
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2024
VL 40
IS 4
BP 2251
EP 2264
DI 10.1007/s00371-023-02914-x
EA JUN 2023
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA MZ2U3
UT WOS:001004827000001
DA 2024-07-18
ER

PT J
AU Xiang, Z
   Zhu, CL
   Qian, M
   Shen, YJ
   Shao, YZ
AF Xiang, Zhong
   Zhu, Chenglin
   Qian, Miao
   Shen, Yujia
   Shao, Yizhou
TI FashionSegNet: a model for high-precision semantic segmentation of
   clothing images
SO VISUAL COMPUTER
LA English
DT Article
DE Clothing image segmentation; Deep learning; Spatial pyramid pooling;
   Encoder-decoder structure; Large convolution kernel; Information
   enhancement
AB Clothing image segmentation is a method to predict the clothing category label of each pixel in the input image. We reduced the influence of the variability of image shots, the similarity of clothing categories, and the complexity of boundaries on the segmentation accuracy of clothing images by developing an advanced ResNet50-based semantic segmentation model in this study whose primary structure is the encoder-decoder. An improved spatial pyramid pooling module combined with a global feature extraction branch of a large convolution kernel is developed to achieve multi-scale feature fusion and improve the model's ability to identify clothing and its boundary features in different shots. Furthermore, to balance the clothing shape and category information in the model, a spatial and semantic information enhancement module is proposed, which can enhance the circulation of the information between different stages of the network through cross-stage connection technology. The model was finally trained and tested on the Deepfashion2 dataset. The comparison experiment demonstrates that the proposed model obtained the highest mIoU and Boundary IoU of 74.55% and 57.51%, respectively, compared with the DeepLabv3+, PSPNet, and other networks.
C1 [Xiang, Zhong; Zhu, Chenglin; Qian, Miao; Shen, Yujia; Shao, Yizhou] Zhejiang Sci Tech Univ, Sch Mech Engn, 928-2 Main St,Xiasha Higher Educ Pk, Hangzhou 310018, Zhejiang, Peoples R China.
C3 Zhejiang Sci-Tech University
RP Xiang, Z (corresponding author), Zhejiang Sci Tech Univ, Sch Mech Engn, 928-2 Main St,Xiasha Higher Educ Pk, Hangzhou 310018, Zhejiang, Peoples R China.
EM xz@zstu.edu.cn
RI Shen, Yujia/GRO-1405-2022
CR Al-Amri Salem Saleh, 2010, PREPRINT
   [Anonymous], 2015, Int. J. Smart Home
   Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615
   Cao Y, 2019, IEEE INT CONF COMP V, P1971, DOI 10.1109/ICCVW.2019.00246
   Chen F, 2022, INT J INTELL SYST, V37, P12267, DOI 10.1002/int.23086
   Chen GS, 2019, APPL SCI-BASEL, V9, DOI 10.3390/app9091816
   Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49
   Cheng BW, 2021, PROC CVPR IEEE, P15329, DOI 10.1109/CVPR46437.2021.01508
   Cheng ZM, 2022, VISUAL COMPUT, V38, P749, DOI 10.1007/s00371-021-02075-9
   Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350
   Dang AH, 2020, INT CONF ADV COMMUN, P1248, DOI [10.23919/ICACT48636.2020.9061408, 10.23919/icact48636.2020.9061408]
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Ding XH, 2022, PROC CVPR IEEE, P11953, DOI 10.1109/CVPR52688.2022.01166
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326
   Fu YP, 2022, VISUAL COMPUT, V38, P3243, DOI 10.1007/s00371-022-02559-2
   Gao H, 2022, PROC CVPR IEEE, P9903, DOI 10.1109/CVPR52688.2022.00968
   Ge YY, 2019, PROC CVPR IEEE, P5332, DOI 10.1109/CVPR.2019.00548
   Gong K, 2018, LECT NOTES COMPUT SC, V11208, P805, DOI 10.1007/978-3-030-01225-0_47
   He JJ, 2019, PROC CVPR IEEE, P7511, DOI 10.1109/CVPR.2019.00770
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang ZL, 2019, IEEE I CONF COMP VIS, P603, DOI 10.1109/ICCV.2019.00069
   Ihsan AM, 2020, NEURAL PROCESS LETT, V51, P2245, DOI 10.1007/s11063-019-10173-y
   Ji W, 2020, IEEE T CIRC SYST VID, V30, P4837, DOI 10.1109/TCSVT.2019.2962216
   Ji W, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P764
   Jouanneau W, 2021, IEEE COMPUT SOC CONF, P3946, DOI 10.1109/CVPRW53098.2021.00443
   Jue Wang, 2021, 2021 IEEE 4th International Conference on Electronics and Communication Engineering (ICECE), P49, DOI 10.1109/ICECE54449.2021.9674326
   Kirillov A, 2019, PROC CVPR IEEE, P6392, DOI 10.1109/CVPR.2019.00656
   KRAHENBUHL P, 2011, ADV NEURAL INFORM PR, P109, DOI DOI 10.5555/2986459.2986472
   Li H.C., 2018, PREPRINT
   Li X, 2019, IEEE I CONF COMP VIS, P9166, DOI 10.1109/ICCV.2019.00926
   Liang XD, 2019, IEEE T PATTERN ANAL, V41, P871, DOI 10.1109/TPAMI.2018.2820063
   Liang XD, 2015, IEEE T PATTERN ANAL, V37, P2402, DOI 10.1109/TPAMI.2015.2408360
   Lin GS, 2017, PROC CVPR IEEE, P5168, DOI 10.1109/CVPR.2017.549
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu S, 2014, IEEE T MULTIMEDIA, V16, P253, DOI 10.1109/TMM.2013.2285526
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Luo P, 2013, IEEE I CONF COMP VIS, P2648, DOI 10.1109/ICCV.2013.329
   Mameli M, 2022, IEEE ACCESS, V10, P1545, DOI 10.1109/ACCESS.2021.3137893
   Martinsson J, 2019, IEEE INT CONF COMP V, P3133, DOI 10.1109/ICCVW.2019.00382
   Milletari F, 2016, INT CONF 3D VISION, P565, DOI 10.1109/3DV.2016.79
   Muthukrishnan R., 2011, International Journal of Computer Science & Information Technology, V3, P259, DOI 10.5121/ijcsit.2011.3620
   Peng C, 2017, PROC CVPR IEEE, P1743, DOI 10.1109/CVPR.2017.189
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Tsung-Yi Lin, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P2999, DOI 10.1109/ICCV.2017.324
   Vozáriková G, 2021, VISAPP: PROCEEDINGS OF THE 16TH INTERNATIONAL JOINT CONFERENCE ON COMPUTER VISION, IMAGING AND COMPUTER GRAPHICS THEORY AND APPLICATIONS - VOL. 5: VISAPP, P15, DOI 10.5220/0010177700150024
   Wang F, 2016, IET IMAGE PROCESS, V10, P456, DOI 10.1049/iet-ipr.2015.0507
   Wu Y, 2022, APPL INTELL, V52, P3319, DOI 10.1007/s10489-021-02603-z
   Xia ZY, 2020, APPL SOFT COMPUT, V91, DOI 10.1016/j.asoc.2020.106209
   Yamaguchi K, 2015, IEEE T PATTERN ANAL, V37, P1028, DOI 10.1109/TPAMI.2014.2353624
   Yamaguchi K, 2012, PROC CVPR IEEE, P3570, DOI 10.1109/CVPR.2012.6248101
   Yan L, 2020, IEEE T GEOSCI REMOTE, V58, P3558, DOI 10.1109/TGRS.2019.2958123
   Yang QR, 2021, IEEE ACCESS, V9, P18867, DOI 10.1109/ACCESS.2021.3053316
   Yang W, 2014, PROC CVPR IEEE, P3182, DOI 10.1109/CVPR.2014.407
   Yu CQ, 2021, INT J COMPUT VISION, V129, P3051, DOI 10.1007/s11263-021-01515-2
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zhao LH, 2021, J ENG FIBER FABR, V16, DOI 10.1177/15589250211019023
   Zhao RL, 2020, INFORM PROCESS MANAG, V57, DOI 10.1016/j.ipm.2020.102306
   Zhou BL, 2019, INT J COMPUT VISION, V127, P302, DOI 10.1007/s11263-018-1140-0
NR 59
TC 1
Z9 1
U1 12
U2 25
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2024
VL 40
IS 3
BP 1711
EP 1727
DI 10.1007/s00371-023-02881-3
EA MAY 2023
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HY8O3
UT WOS:000991071000002
DA 2024-07-18
ER

PT J
AU Gao, LA
   Fu, P
   Xu, MZ
   Wang, TT
   Liu, B
AF Gao, Lina
   Fu, Ping
   Xu, Mingzhu
   Wang, Tiantian
   Liu, Bing
TI UMINet: a unified multi-modality interaction network for RGB-D and RGB-T
   salient object detection
SO VISUAL COMPUTER
LA English
DT Article
DE Salient object detection; RGB-D and RGB-T images; Asymmetric network;
   Multi-modality features fusion
AB Multi-modality images with complementary cues can significantly improve the performance of salient object detection (SOD) methods in challenging scenes. However, existing methods are specially designed for RGB-D or RGB-T SOD in general, thus it is necessary to bridge the gap to develop a unified SOD framework for processing various multi-modality images. To address this issue, we propose a unified multi-modality interaction fusion framework for RGB-D and RGB-T SOD, named UMINet. We deeply investigate the differences between appearance maps and complementary images and design an asymmetric backbone to extract appearance features and complementary cues. For the complementary cues branch, a complementary information aware module (CIAM) is proposed to perceive and enhance the weights of complementary modality features. We also propose a multi-modality difference fusion (MDF) block to fuse cross-modality features. This MDF block simultaneously considers the differences and consistency between the appearance features and complementary features. Furthermore, to promote the rich contextual dependencies and integrate cross-level multi-modality features, we design a mutual refinement decoder (MRD) to progressively predict salient results. The MRD consists of three reverse perception blocks (RPB) and five sub-decoders. Extensive experiments are provided to indicate the substantial improvement achieved by the proposed UMINet over the existing state-of-the-art (SOTA) models on six RGB-D SOD datasets and three RGB-T SOD datasets.
C1 [Gao, Lina; Fu, Ping; Wang, Tiantian; Liu, Bing] Harbin Inst Technol, Sch Elect & Informat Engn, Harbin, Peoples R China.
   [Xu, Mingzhu] Shandong Univ, Sch Software, Jinan, Peoples R China.
C3 Harbin Institute of Technology; Shandong University
RP Liu, B (corresponding author), Harbin Inst Technol, Sch Elect & Informat Engn, Harbin, Peoples R China.
EM gaolina@hit.edu.cn; fuping@hit.edu.cn; xumingzhu@sdu.edu.cn;
   TTW4123@163.com; liubing66@hit.edu.cn
OI Liu, Bing/0000-0001-6831-5779
FU National Natural Science Foundation of China (NSFC) [62171156]
FX AcknowledgementsThis work was supported by the National Natural Science
   Foundation of China (NSFC) under Grants 62171156.
CR Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   [Anonymous], 2016, Proceedings of the IEEE conference on computer vision and pattern recognition, DOI DOI 10.1109/CVPR.2016.257
   ARRIDHANA C, 2013, DEPTH VIEW SALIENCY
   Borji Ali, 2019, [Computational Visual Media, 计算可视媒体], V5, P117
   Borji A, 2012, LECT NOTES COMPUT SC, V7573, P414, DOI 10.1007/978-3-642-33709-3_30
   Chen H, 2019, IEEE T IMAGE PROCESS, V28, P2825, DOI 10.1109/TIP.2019.2891104
   Chen Q., 2022, IEEE T PATTERN ANAL
   Chen Q, 2021, PATTERN RECOGN, V112, DOI 10.1016/j.patcog.2020.107740
   Chen S., 2020, ECCV, P520, DOI DOI 10.1007/978-3-030-58598-3_31
   Chen SH, 2018, LECT NOTES COMPUT SC, V11213, P236, DOI 10.1007/978-3-030-01240-3_15
   Chen TY, 2022, NEURAL COMPUT APPL, V34, P7547, DOI 10.1007/s00521-021-06845-3
   Chen YT, 2023, J VIS COMMUN IMAGE R, V91, DOI 10.1016/j.jvcir.2023.103776
   Chen YT, 2024, VISUAL COMPUT, V40, P489, DOI 10.1007/s00371-023-02795-0
   Cheng MM, 2021, INT J COMPUT VISION, V129, P2622, DOI [10.1007/s11263-021-01490-8, 10.1109/ICCV.2017.487]
   Cong RM, 2016, IEEE SIGNAL PROC LET, V23, DOI 10.1109/LSP.2016.2557347
   Deng-Ping Fan, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P275, DOI 10.1007/978-3-030-58610-2_17
   Fan DP, 2020, IEEE T MED IMAGING, V39, P2626, DOI 10.1109/TMI.2020.2996645
   Fan DP, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P698
   Fan DP, 2021, IEEE T NEUR NET LEAR, V32, P2075, DOI 10.1109/TNNLS.2020.2996406
   Feng W, 2019, IEEE T IMAGE PROCESS, V28, P3232, DOI 10.1109/TIP.2019.2895411
   GAO L, 2023, J HIGH ENERGY ASTROP, V518, P507, DOI DOI 10.1016/J.NEUCOM.2022.11.031
   Gao L, 2021, APPL INTELL, V51, P7601, DOI 10.1007/s10489-021-02289-3
   Gao W, 2022, IEEE T CIRC SYST VID, V32, P2091, DOI 10.1109/TCSVT.2021.3082939
   Guo QL, 2021, IEEE SIGNAL PROC LET, V28, P1655, DOI 10.1109/LSP.2021.3102524
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang LM, 2020, IEEE SIGNAL PROC LET, V27, P1585, DOI 10.1109/LSP.2020.3020735
   Huo FS, 2022, IEEE T CIRC SYST VID, V32, P3111, DOI 10.1109/TCSVT.2021.3102268
   Jie Wang, 2021, IEEE T CIRC SYST VID
   Jin WD, 2021, IEEE T IMAGE PROCESS, V30, P3376, DOI 10.1109/TIP.2021.3060167
   Jing Zhang, 2021, INT C COMPUTER VISIO
   Ju R, 2014, IEEE IMAGE PROC, P1115, DOI 10.1109/ICIP.2014.7025222
   Li P, 2022, MATH PROBL ENG, V2022, DOI 10.1155/2022/8508702
   Li TP, 2020, NEUROCOMPUTING, V389, P170, DOI 10.1016/j.neucom.2019.12.109
   Liang Y, 2020, VISUAL COMPUT, V36, P1883, DOI 10.1007/s00371-019-01781-9
   Liu Liyuan, 2019, INT C LEARNING REPRE
   Liu ZY, 2021, VISUAL COMPUT, V37, P529, DOI 10.1007/s00371-020-01821-9
   Lixin An, 2021, 2021 IEEE 21st International Conference on Communication Technology (ICCT), P1103, DOI 10.1109/ICCT52962.2021.9658080
   Lu Y, 2019, VISUAL COMPUT, V35, P1683, DOI 10.1007/s00371-019-01637-2
   Ma C, 2017, IEEE T MULTIMEDIA, V19, P2415, DOI 10.1109/TMM.2017.2694219
   Miao Zhang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12373), P374, DOI 10.1007/978-3-030-58604-1_23
   Nian Liu, 2021, IEEE T PATTERN ANAL
   Paszke A, 2019, ADV NEUR IN, V32
   Peng HW, 2014, LECT NOTES COMPUT SC, V8691, P92, DOI 10.1007/978-3-319-10578-9_7
   Qu LQ, 2017, IEEE T IMAGE PROCESS, V26, P2274, DOI 10.1109/TIP.2017.2682981
   Ryoo MS, 2021, ADV NEUR IN
   Shigematsu R, 2017, IEEE INT CONF COMP V, P2749, DOI 10.1109/ICCVW.2017.323
   Shu XB, 2023, IEEE T PATTERN ANAL, V45, P7559, DOI 10.1109/TPAMI.2022.3222871
   Shu XB, 2022, IEEE T CIRC SYST VID, V32, P5281, DOI 10.1109/TCSVT.2022.3142771
   Sun P, 2021, PROC CVPR IEEE, P1407, DOI 10.1109/CVPR46437.2021.00146
   Tu ZZ, 2021, IEEE T IMAGE PROCESS, V30, P5678, DOI 10.1109/TIP.2021.3087412
   Tu ZZ, 2020, IEEE T MULTIMEDIA, V22, P160, DOI 10.1109/TMM.2019.2924578
   Tu ZZ, 2019, 2019 2ND IEEE CONFERENCE ON MULTIMEDIA INFORMATION PROCESSING AND RETRIEVAL (MIPR 2019), P141, DOI 10.1109/MIPR.2019.00032
   Wang G., 2018, CHINESE C IMAGE GRAP, P359
   Wang J, 2022, VISUAL COMPUT, V38, P1803, DOI 10.1007/s00371-021-02106-5
   Wang XY, 2022, VISUAL COMPUT, V38, P3831, DOI 10.1007/s00371-021-02224-0
   Wei Gao, 2021, IEEE T CIRC SYST VID
   Wei Ji, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12363), P52, DOI 10.1007/978-3-030-58523-5_4
   Wei Ji, 2022, IEEE T IMAGE PROCESS
   Wei J, 2020, AAAI CONF ARTIF INTE, V34, P12321
   Wu YH, 2021, IEEE T IMAGE PROCESS, V30, P3113, DOI 10.1109/TIP.2021.3058783
   Wu Z, 2019, PROC CVPR IEEE, P3902, DOI 10.1109/CVPR.2019.00403
   Wujie Zhou, 2021, IEEE T CIRC SYST VID
   Xia RL, 2022, J KING SAUD UNIV-COM, V34, P6008, DOI 10.1016/j.jksuci.2022.02.004
   Xiaoqi Zhao, 2022, AAAI C ART INT
   Xu B., 2022, IEEE T IMAGE PROCESS
   Xu M., 2021, APPL INTELL
   Xu M, 2015, VISUAL COMPUT, V31, P355, DOI 10.1007/s00371-014-0930-9
   Xu MZ, 2020, IEEE T CIRC SYST VID, V30, P2191, DOI 10.1109/TCSVT.2019.2920652
   Xu MZ, 2019, IEEE T MULTIMEDIA, V21, P2790, DOI 10.1109/TMM.2019.2914889
   Yanhua Liang, 2022, NEUROCOMPUTING
   Yongri Piao, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9057, DOI 10.1109/CVPR42600.2020.00908
   Zhang J., 2021, HUM-CENT COMPUT INFO, V12
   Zhang JM, 2022, APPL SOFT COMPUT, V118, DOI 10.1016/j.asoc.2022.108485
   Zhang M, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P4107, DOI [10.1145/3394171.3413969, 10.1145/33941713413969]
   Zhang M, 2020, PROC CVPR IEEE, P3469, DOI 10.1109/CVPR42600.2020.00353
   Zhang Q, 2021, IEEE T CIRC SYST VID, V31, P1804, DOI 10.1109/TCSVT.2020.3014663
   Zhang YF, 2021, NEUROCOMPUTING, V423, P463, DOI 10.1016/j.neucom.2020.10.079
   Zhengyi Liu, 2021, IEEE T CIRC SYST VID
   Zhengzheng T., 2022, IEEE T MULTIMEDIA
   Zhu XM, 2020, BIOSYST ENG, V189, P116, DOI 10.1016/j.biosystemseng.2019.11.013
NR 80
TC 2
Z9 2
U1 8
U2 24
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2024
VL 40
IS 3
BP 1565
EP 1582
DI 10.1007/s00371-023-02870-6
EA APR 2023
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HY8O3
UT WOS:000976793600002
DA 2024-07-18
ER

PT J
AU Zerari, A
   Meadi, MN
   Babahenini, MC
AF Zerari, Abd El Moumene
   Meadi, Mohamed Nadjib
   Babahenini, Mohamed Chaouki
TI Multisampling-depth soft shadow based on cascaded area light source
SO VISUAL COMPUTER
LA English
DT Article
DE Multisampling depth; Screen space; Correct soft shadows; Cascaded area
   light source
AB Reproducing correct soft shadows in interactive applications is accomplished by gathering multiple samples of the area light source. The size of the penumbra region is unpredictable, so the required sample density should be high to ensure soft shadow transitions in all configurations of the area light size. Therefore, multiple shadow maps must be evaluated for each scene configuration, which leads to performance degradation. To tackle the challenge, we propose a novel technique based on the multisampling depth, which approximates soft shadows for large, fully dynamic scenes at interactive frame rates. Our algorithm can perform multiple depth tests for each pixel in the shadow map. We effectively reduce the number of samples of zone light to avoid other rendering passes. We reuse shadow maps for other light source sampling setups without performing additional scene rendering while maintaining real-time performance. Our approach fits well in existing rendering pipelines and improves the robustness of many different soft shadow techniques.
C1 [Zerari, Abd El Moumene; Meadi, Mohamed Nadjib; Babahenini, Mohamed Chaouki] Dept Comp Sci, LESIA Lab, BP 145, Biskra 07000, Algeria.
   [Zerari, Abd El Moumene; Meadi, Mohamed Nadjib; Babahenini, Mohamed Chaouki] Univ Mohamed Khider, BP 145, Biskra 07000, Algeria.
C3 Universite Mohamed Khider Biskra
RP Zerari, A (corresponding author), Dept Comp Sci, LESIA Lab, BP 145, Biskra 07000, Algeria.; Zerari, A (corresponding author), Univ Mohamed Khider, BP 145, Biskra 07000, Algeria.
EM a.zerari@univ-biskra.dz; mohamed_nadjib.meadi@univ-biskra.dz;
   chaouki.babahenini@univ-biskra.dz
RI BABAHENINI, Mohamed Chaouki/F-1427-2017
OI BABAHENINI, Mohamed Chaouki/0000-0001-7972-8026; Zerari, Abd El
   Moumene/0000-0002-7433-1713
CR Agrawala M, 2000, COMP GRAPH, P375, DOI 10.1145/344779.344954
   Ali HH, 2017, MULTIMED TOOLS APPL, V76, P2591, DOI 10.1007/s11042-016-3254-0
   Annen Thomas., 2007, Proc. Eurographics Symposium on Ren- dering, P51
   [Anonymous], 1978, P 5 ANN C COMPUTER G, P270
   Aszódi B, 2006, EUROGR TECH REP SER, P53
   Atty L, 2006, COMPUT GRAPH FORUM, V25, P725, DOI 10.1111/j.1467-8659.2006.00995.x
   Bartz D., 1998, P ACM SIGGRAPHEUROGR, P97, DOI 10.2312/EGGH/EGGH98/097-103
   Crow F.C., 1977, ACM SIGGRAPH COMPUT, V11, P242, DOI [DOI 10.1145/965141.563901, DOI 10.1145/563858.563901]
   Eisemann E., 2011, Real-time shadows, DOI DOI 10.1201/B11030
   Engel W., 2007, CASCADED SHADOW MAPS, P197
   Fernando Randima., 2005, SIGGRAPH 05, P35
   Guennebaud G, 2007, COMPUT GRAPH FORUM, V26, P525, DOI 10.1111/j.1467-8659.2007.01075.x
   Guennebaud Gael., 2006, Proceedings of the Eurographics Symposium on Rendering, P227
   Hasenfratz JM, 2003, COMPUT GRAPH FORUM, V22, P753, DOI 10.1111/j.1467-8659.2003.00722.x
   Heckbert P.S., 1997, CMUCS97104
   Kajiya J. T., 1986, Computer Graphics, V20, P143, DOI 10.1145/15886.15902
   Liktor G, 2015, COMPUT GRAPH FORUM, V34, P1, DOI 10.1111/cgf.12673
   Buades JM, 2016, VISUAL COMPUT, V32, P167, DOI 10.1007/s00371-015-1062-6
   McGuire M., 2017, Computer Graphics Archive
   Pagot C.A., 2004, P 17 BRAZ S COMP GRA, DOI [10.1109/SIBGRA.2004.1352975, DOI 10.1109/SIBGRA.2004.1352975]
   Pagot CA, 2004, XVII BRAZILIAN SYMPOSIUM ON COMPUTER GRAPHICS AND IMAGE PROCESSING, PROCEEDINGS, P308
   PHONG BT, 1975, COMMUN ACM, V18, P311, DOI 10.1145/360825.360839
   Reeves W. T., 1987, SIGGRAPH Comput. Graph., P283
   Scherzer D, 2009, LECT NOTES COMPUT SC, V5876, P13, DOI 10.1007/978-3-642-10520-3_2
   Schwarz M, 2007, COMPUT GRAPH FORUM, V26, P515, DOI 10.1111/j.1467-8659.2007.01074.x
   Sketchfab Community, 2012, PUBL FIND 3D CONT ON
   Slater M., 1992, Visual Computer, V9, P25, DOI 10.1007/BF01901026
   St-Amour J.-F., 2005, GI 05 P GRAPHICS INT, P105, DOI [10.5555/1089508.1089526, DOI 10.5555/1089508.1089526]
   Stanford University, 1996, STANF 3D SCANN REP
   Veach E., 1997, Robust Monte Carlo Methods for Light Transport Simulation
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   William D., 2006, P 2006 S INT 3D GRAP, P161, DOI DOI 10.1145/1111411.1111440
   Willmott CJ, 2005, CLIMATE RES, V30, P79, DOI 10.3354/cr030079
   Wimmer, P 17 INT WORKSH VIS, P39, DOI [10.2312/PE/VMV/VMV12/039-046, DOI 10.2312/PE/VMV/VMV12/039-046]
   Xu Z, 2015, INT C COMP AID DES C, P97, DOI 10.1109/CADGRAPHICS.2015.35
   Zhang F., 2006, Proceedings of the 2006 ACM international conference on virtual reality continuum and its applications, ACM Press, P311, DOI 10.1145/1128923.1128975
NR 36
TC 0
Z9 0
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2024
VL 40
IS 3
BP 1339
EP 1357
DI 10.1007/s00371-023-02852-8
EA APR 2023
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HY8O3
UT WOS:000974392900001
DA 2024-07-18
ER

PT J
AU Zhang, JZ
   Wang, HL
   Wu, XH
   Zuo, WM
AF Zhang, Jize
   Wang, Haolin
   Wu, Xiaohe
   Zuo, Wangmeng
TI Invertible network for unpaired low-light image enhancement
SO VISUAL COMPUTER
LA English
DT Article
DE Unpaired learning; Invertible network; Reversibility loss; Progressive
   self-guided enhancement
ID DYNAMIC HISTOGRAM EQUALIZATION; CONTRAST
AB Existing unpaired image enhancement approaches prefer to employ the traditional two-way generative adversarial network (GAN) framework, in which two convolutional neural network (CNN) generators are deployed for enhancement and degradation separately. However, such data-driven models ignore the inherent characteristics of transformation between the low-light and normal-light images, leading to unstable training and artifacts. Here, we propose to leverage the invertible neural network to enhance the low-light images in the forward process and degrade the unpaired normal-light photograph inversely. The generated and real images are then fed into discriminators for adversarial learning. In addition to the adversarial loss, we design transformation-consistent loss to ensure the stability of training, detail-preserving loss to preserve more image details, and reversibility loss to alleviate the over-exposure problem. Moreover, we present a progressive self-guided enhancement process in inference and achieve favorable performance against the state-of-the-art methods.
C1 [Zhang, Jize; Wang, Haolin; Wu, Xiaohe; Zuo, Wangmeng] Harbin Inst Technol, Sch Comp Sci & Technol, Harbin, Peoples R China.
C3 Harbin Institute of Technology
RP Wu, XH (corresponding author), Harbin Inst Technol, Sch Comp Sci & Technol, Harbin, Peoples R China.
EM jize.zhang.cs@outlook.com; Why_cs@outlook.com; xhwu.cpsl.hit@gmail.com;
   cswmzuo@gmail.com
RI Zuo, Wangmeng/B-3701-2008
FU National Natural Science Foundation of China [62006064, U19A2073]
FX AcknowledgementsThis work was supported by National Natural Science
   Foundation of China under Grants 62006064 and U19A2073.
CR Abdullah-Al-Wadud M, 2007, IEEE T CONSUM ELECTR, V53, P593, DOI 10.1109/TCE.2007.381734
   Blau Y, 2019, LECT NOTES COMPUT SC, V11133, P334, DOI 10.1007/978-3-030-11021-5_21
   Chen C, 2018, PROC CVPR IEEE, P3291, DOI 10.1109/CVPR.2018.00347
   Chen YS, 2018, PROC CVPR IEEE, P6306, DOI 10.1109/CVPR.2018.00660
   Dinh L., 2014, ARXIV
   Dinh Laurent, 2017, ICLR, P1019
   Feifan L., 2018, BMVC
   Fu XY, 2016, PROC CVPR IEEE, P2782, DOI 10.1109/CVPR.2016.304
   Guo CL, 2020, PROC CVPR IEEE, P1777, DOI 10.1109/CVPR42600.2020.00185
   Guo XJ, 2017, IEEE T IMAGE PROCESS, V26, P982, DOI 10.1109/TIP.2016.2639450
   He KM, 2010, LECT NOTES COMPUT SC, V6311, P1
   Huang SC, 2013, IEEE T IMAGE PROCESS, V22, P1032, DOI 10.1109/TIP.2012.2226047
   Ibrahim H, 2007, IEEE T CONSUM ELECTR, V53, P1752, DOI 10.1109/TCE.2007.4429280
   Jiang YF, 2021, IEEE T IMAGE PROCESS, V30, P2340, DOI 10.1109/TIP.2021.3051462
   Kim H.-U., 2020, ECCV
   Kingma DP, 2018, ADV NEUR IN, V31
   Lee C, 2012, IEEE IMAGE PROC, P965, DOI 10.1109/ICIP.2012.6467022
   Li CY, 2022, IEEE T PATTERN ANAL, V44, P9396, DOI 10.1109/TPAMI.2021.3126387
   Li CY, 2022, IEEE T PATTERN ANAL, V44, P4225, DOI 10.1109/TPAMI.2021.3063604
   Li MD, 2018, IEEE T IMAGE PROCESS, V27, P2828, DOI 10.1109/TIP.2018.2810539
   Lin Z., 2021, ICCV
   Liu FJ, 2022, FRONT NEUROROBOTICS, V16, DOI 10.3389/fnbot.2022.836551
   Liu JY, 2021, INT J COMPUT VISION, V129, P1153, DOI 10.1007/s11263-020-01418-8
   Liu RS, 2021, PROC CVPR IEEE, P10556, DOI 10.1109/CVPR46437.2021.01042
   Lore KG, 2017, PATTERN RECOGN, V61, P650, DOI 10.1016/j.patcog.2016.06.008
   Ma K, 2015, IEEE T IMAGE PROCESS, V24, P3345, DOI 10.1109/TIP.2015.2442920
   Ma L, 2022, PROC CVPR IEEE, P5627, DOI 10.1109/CVPR52688.2022.00555
   Mao XD, 2017, IEEE I CONF COMP VIS, P2813, DOI 10.1109/ICCV.2017.304
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Mukherjee Subhayan, 2018, Smart Multimedia. First International Conference, ICSM 2018. Revised Selected Papers: Lecture Notes in Computer Science (LNCS 11010), P193, DOI 10.1007/978-3-030-04375-9_17
   Ni Z., 2020, ACM MM
   Wang RX, 2019, PROC CVPR IEEE, P6842, DOI 10.1109/CVPR.2019.00701
   Wang SH, 2013, IEEE T IMAGE PROCESS, V22, P3538, DOI 10.1109/TIP.2013.2261309
   Wang Y., 2021, arXiv
   Wei Cui, 2018, 2018 Photonics North (PN), DOI 10.1109/PN.2018.8438843
   Wu WH, 2022, PROC CVPR IEEE, P5891, DOI 10.1109/CVPR52688.2022.00581
   Yang WH, 2020, PROC CVPR IEEE, P3060, DOI 10.1109/CVPR42600.2020.00313
   Zhang YH, 2021, INT J COMPUT VISION, V129, P1013, DOI 10.1007/s11263-020-01407-x
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 39
TC 0
Z9 0
U1 4
U2 10
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2024
VL 40
IS 1
BP 109
EP 120
DI 10.1007/s00371-023-02769-2
EA FEB 2023
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IX5B8
UT WOS:000937621000001
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Gaur, D
   Mehrotra, D
   Singh, K
AF Gaur, Deepak
   Mehrotra, Deepti
   Singh, Karan
TI A new approach to simulate the dynamic behavior of particulate matter
   using a canny edge detector embedded PIV algorithm
SO VISUAL COMPUTER
LA English
DT Article
DE Particulate matter (PM); Canny edge detector; Particle image velocimetry
   (PIV); Particles stock video footage (PSVF) dataset
AB Over the last few years, the study of characteristics of particles present in the environment becomes an interesting research area for scientists. Simulation of physical and dynamic characteristics of particulate matter (PM) is a prominent area for researchers. For the implementation of particle image velocimetry (PIV) for complex particulate matter with overlapping boundaries, it is necessary to remove non-physical measurements. These non-physical measurements such as unsteady surface and inaccurate edge detection lead to the spurious velocity of particles. In this note, a Canny edge detector is employed to identify the edge of particles. For unsteady surfaces, a special process is followed as follows: (a) find the gradient magnitude in the particles image velocimetry frame from the Canny edge detected frame to optimize particle detection, (b) classify a large high-intensity area in view to extract the particles, (c) find out the rough surface area which contains these particles with their reflections, (d) finally eliminate these particle's reflections. Finally, after this, PIV is implemented on these extracted processed frames from the video dataset to measure the motion of the particles. In this paper, a Canny edge detector with particle image velocimetry (PIV) algorithm is proposed to simulate the dynamic behavior of particulate matter present in particles stock video footage (PSVF) dataset of particles. The proposed model is trained to estimate the motion of particles, and the result showed an accuracy of 92.97% for the particles stock video footage dataset over the other existing methods.
C1 [Gaur, Deepak; Mehrotra, Deepti] Amity Univ, ASET, Noida, India.
   [Singh, Karan] JNU, Dept Comp Sci, New Delhi, India.
C3 Amity University Noida
RP Gaur, D (corresponding author), Amity Univ, ASET, Noida, India.
EM dgaur@amity.edu; dmehrotra@amity.edu; karan@mail.jnu.ac.in
RI mehrotra, deepti/E-5333-2013
OI mehrotra, deepti/0000-0001-5752-9800
CR Alam S, 2019, COMPUT MATH ORGAN TH, V25, P319, DOI 10.1007/s10588-018-9266-8
   Burges CJC, 1998, DATA MIN KNOWL DISC, V2, P121, DOI 10.1023/A:1009715923555
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851
   Ding LJ, 2001, PATTERN RECOGN, V34, P721, DOI 10.1016/S0031-3203(00)00023-6
   Gu JX, 2018, PATTERN RECOGN, V77, P354, DOI 10.1016/j.patcog.2017.10.013
   Gu SH, 2020, IEEE T FUZZY SYST, V28, P1369, DOI 10.1109/TFUZZ.2019.2919481
   Gupta D, 2017, BIOMED SIGNAL PROCES, V31, P116, DOI 10.1016/j.bspc.2016.06.012
   Hesamian MH, 2019, J DIGIT IMAGING, V32, P582, DOI 10.1007/s10278-019-00227-x
   Hiner MC, 2016, BMC BIOINFORMATICS, V17, DOI 10.1186/s12859-016-1383-0
   Kalyani R, 2020, ENG SCI TECHNOL, V23, P1327, DOI 10.1016/j.jestch.2020.07.007
   Kim S, 2019, J SENSORS, V2019, DOI 10.1155/2019/9673047
   Kumawat A, 2022, VISUAL COMPUT, V38, P3681, DOI 10.1007/s00371-021-02196-1
   Li BW, 2022, VISUAL COMPUT, V38, P4437, DOI 10.1007/s00371-021-02307-y
   Li XY, 2021, VEHICLE SYST DYN, V59, P1509, DOI 10.1080/00423114.2020.1767795
   Mahdianpari M, 2018, REMOTE SENS ENVIRON, V206, P300, DOI 10.1016/j.rse.2017.11.005
   Medina-Carnicer R, 2011, PATTERN RECOGN, V44, P1201, DOI 10.1016/j.patcog.2010.12.008
   Nam KH, 2012, INT J CARDIOVAS IMAG, V28, P69, DOI 10.1007/s10554-010-9778-x
   Paul A, 2023, VISUAL COMPUT, V39, P297, DOI 10.1007/s00371-021-02330-z
   Scharf HR, 2018, ENVIRONMETRICS, V29, DOI 10.1002/env.2487
   Shekunov BY, 2007, PHARM RES-DORDR, V24, P203, DOI 10.1007/s11095-006-9146-7
   Sigurgeirson H, 2001, J COMPUT PHYS, V172, P766, DOI 10.1006/jcph.2001.6858
   Vázquez PP, 2012, VISUAL COMPUT, V28, P1063, DOI 10.1007/s00371-011-0651-2
   Vlasic I, 2019, COMPUT IND ENG, V137, DOI 10.1016/j.cie.2019.106030
   WHITLEY D, 1994, STAT COMPUT, V4, P65, DOI 10.1007/BF00175354
   [吴成茂 WU Cheng-Mao], 2008, [模式识别与人工智能, Pattern Recognition and Artificial Intelligence], V21, P746
   Yang J, 2015, COMPUT GEOSCI-UK, V79, P94, DOI 10.1016/j.cageo.2015.03.011
   Yang MS, 2018, IEEE T FUZZY SYST, V26, P817, DOI 10.1109/TFUZZ.2017.2692203
   Zhang YC, 2019, STAT PROBABIL LETT, V152, P137, DOI 10.1016/j.spl.2019.04.017
NR 28
TC 1
Z9 1
U1 0
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2023
VL 39
IS 9
BP 4293
EP 4304
DI 10.1007/s00371-022-02591-2
EA JUL 2022
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA Q8PV6
UT WOS:000825226300001
DA 2024-07-18
ER

PT J
AU Yang, Z
   Wang, Y
   Yang, F
   Yin, ZJ
   Zhang, T
AF Yang, Zhen
   Wang, Yang
   Yang, Fan
   Yin, Zhijian
   Zhang, Tao
TI Real-time instance segmentation with assembly parallel task
SO VISUAL COMPUTER
LA English
DT Article
DE Real-time; Instance Segmentation; Assembly Parallel Task
AB Although instance segmentation has made significant progress in recent years, it is still a challenge to develop highly accurate algorithms with real-time performance. In this paper, we propose a real-time framework denoted by APTMask for instance segmentation, which builds on the real-time project YOLACT. In APTMask, we use Swin-Transformer Tiny with PA-FPN as the default feature backbone and a base image size of 544 x 544. We devise a new mask branch, which can more effectively exploit the semantic information of PA-FPN deeper features and the positional information of shallow features for mask representation, compared to the use of implicit parameterized forms. We replace fast NMS with Cluster NMS, which compensates for the performance penalty of fast NMS compiled to standard NMS. CIoU loss is also adopted to fully exploit the scale information of the aspect ratio of the bounding box. Experimental results show that APTMask can achieve 39.7/34.7 box/mask AP on COCO val2017 dataset at 31.8 fps evaluated with a single RTX 2080TI GPU card. Compared to YOLACT, APTMask improves the box AP by about 8.0% and the mask AP by 6.2%, which is encouraging and competitive. Given its simplicity and efficiency, we hope that our APTMask can serve as a simple but strong baseline for a variety of instance-wise prediction tasks.
C1 [Yang, Zhen; Wang, Yang; Yang, Fan; Yin, Zhijian] Jiangxi Sci & Technol Normal Univ, Sch Commun & Elect, Nanchang, Jiangxi, Peoples R China.
   [Yang, Zhen] Guangdong Atv Acad Performing Arts, Dongguan, Peoples R China.
   [Zhang, Tao] Shanghai Jiao Tong Univ, Dept Automat, 800 Dongchuan Rd, Shanghai, Peoples R China.
   [Zhang, Tao] Shanghai Jiao Tong Univ, Shanghai Key Lab Intelligent Sensing & Recognit, 800 Dongchuan Rd, Shanghai 200240, Peoples R China.
C3 Jiangxi Science & Technology Normal University; Shanghai Jiao Tong
   University; Shanghai Jiao Tong University
RP Zhang, T (corresponding author), Shanghai Jiao Tong Univ, Dept Automat, 800 Dongchuan Rd, Shanghai, Peoples R China.; Zhang, T (corresponding author), Shanghai Jiao Tong Univ, Shanghai Key Lab Intelligent Sensing & Recognit, 800 Dongchuan Rd, Shanghai 200240, Peoples R China.
EM sjtu--zt@sjtu.edu.cn
FU National Natural Science Foundation of China [62061019, 61866016];
   Jiangxi Provincial Natural Science Foundation [20202BABL202014,
   20212BAB202013]; Key Project of Jiangxi Education Department [GJJ201107,
   GJJ190587]; Key Laboratory of System Control and Information Processing,
   Ministry of Education [Scip202106]
FX This work was supported by the National Natural Science Foundation of
   China (62061019, 61866016), Jiangxi Provincial Natural Science
   Foundation (20202BABL202014, 20212BAB202013), the Key Project of Jiangxi
   Education Department (GJJ201107, GJJ190587), and the Key Laboratory of
   System Control and Information Processing, Ministry of Education
   (Scip202106).
CR Bolya D, 2019, IEEE I CONF COMP VIS, P9156, DOI 10.1109/ICCV.2019.00925
   Cai LQ, 2020, IEEE ACCESS, V8, P44400, DOI 10.1109/ACCESS.2020.2976432
   Chen Hao, 2020, 2020 IEEE C COMP VIS, DOI [DOI 10.48550/ARXIV.2001.00309, DOI 10.1109/CVPR42600.2020.00860]
   Chiao JY, 2019, MEDICINE, V98, DOI 10.1097/MD.0000000000015200
   Dai J., 2015, ARXIV 1512
   Dai JF, 2016, LECT NOTES COMPUT SC, V9910, P534, DOI 10.1007/978-3-319-46466-4_32
   Devlin J., 2018, BERT PRE TRAINING DE
   Dosovitskiy A, 2021, arXiv, DOI [10.48550/ARXIV.2010.11929, DOI 10.48550/ARXIV.2010.11929]
   Du W., 2021, P IEEE CVF INT C COM, P7314
   Enze Xie, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12190, DOI 10.1109/CVPR42600.2020.01221
   Gao NY, 2019, IEEE I CONF COMP VIS, P642, DOI 10.1109/ICCV.2019.00073
   Ge Z., ARXIV PREPRINT ARXIV
   Hariharan B, 2011, IEEE I CONF COMP VIS, P991, DOI 10.1109/ICCV.2011.6126343
   He K., 2016, PROC CVPR IEEE, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]
   He KM, 2020, IEEE T PATTERN ANAL, V42, P386, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   Huang ZJ, 2019, PROC CVPR IEEE, P6402, DOI 10.1109/CVPR.2019.00657
   Kisantal M., 2019, P 9 INT C ADV COMP I, DOI 10.5121/csit.2019.91713
   Li X., 2021, VISUAL COMPUT, V5, P1
   LI Y, 2017, PROC CVPR IEEE, P4438, DOI [DOI 10.1109/CVPR.2017.472, DOI 10.1109/CVPR.2017.199]
   Liu S, 2018, PROC CVPR IEEE, P8759, DOI 10.1109/CVPR.2018.00913
   Liu TR, 2022, VISUAL COMPUT, V38, P2303, DOI 10.1007/s00371-021-02112-7
   Liu Z., ARXIV PREPRINT ARXIV
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Mozaffari M. Hamed, 2020, Advances in Visual Computing. 15th International Symposium, ISVC 2020. Proceedings. Lecture Notes in Computer Science (LNCS 12510), P421, DOI 10.1007/978-3-030-64559-5_33
   Newell A., 2017, ADV NEUR IN, P2171
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Touvron H, 2021, PR MACH LEARN RES, V139, P7358
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang G., 2022, SIGNAL IMAGE VIDEO P, V2, P1
   Wang XR, 2020, IEEE C EVOL COMPUTAT, DOI 10.1109/cec48606.2020.9185810
   Xinlong Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12363), P649, DOI 10.1007/978-3-030-58523-5_38
   Zhang G, 2021, PROC CVPR IEEE, P6857, DOI 10.1109/CVPR46437.2021.00679
   Zhao QJ, 2019, AAAI CONF ARTIF INTE, P9259
   Zheng Z., 2021, IEEE T CYBERNETICS, V2, P1140
   Zhi Tian, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P282, DOI 10.1007/978-3-030-58452-8_17
   Zhou K, 2016, DESTECH TRANS COMP
NR 36
TC 2
Z9 2
U1 1
U2 22
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2023
VL 39
IS 9
BP 3937
EP 3947
DI 10.1007/s00371-022-02537-8
EA JUN 2022
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA Q8PV6
UT WOS:000811949200002
DA 2024-07-18
ER

PT J
AU Kera, SB
   Tadepalli, A
   Ranjani, JJ
AF Kera, Shreyas Bhat
   Tadepalli, Anand
   Ranjani, J. J.
TI A paced multi-stage block-wise approach for object detection in thermal
   images
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT ACIAT202
CY 2022
CL ELECTR NETWORK
DE Object detection; Thermal images; Pace; Multi-stage; Domain adaptation;
   Transfer learning; EfficientDet
AB The growing advocacy of thermal imagery in applications, such as autonomous vehicles, surveillance, and COVID-19 detection, necessitates accurate object detection frameworks for the thermal domain. Conventional methods could fall short, especially in situations with poor lighting, for instance, detection during night-time. In this paper, we propose a paced multi-stage block-wise framework for effectively detecting objects from thermal images. Our approach utilizes the pre-existing knowledge of deep neural network-based object detectors trained on large-scale natural image data to enhance performance in the thermal domain constructively. The employed, multi-stage approach drives our model to achieve higher accuracies. And the introduction of the pace parameter during domain adaption enables efficient training. Our experimental results demonstrate that the framework outperforms previous benchmarks on the FLIR ADAS dataset on the person, bicycle, and car categories. We have also illustrated further analysis of the framework, such as the effect of its components on accuracy and training efficiency, its generalizability to other thermal datasets, and its superior performance on night-time images in contrast to state-of-the-art RGB object detectors.
C1 [Kera, Shreyas Bhat; Tadepalli, Anand; Ranjani, J. J.] Birla Inst Technol & Sci, Dept Comp Sci & Informat Syst, Pilani Campus, Pilani 333031, Rajasthan, India.
C3 Birla Institute of Technology & Science Pilani (BITS Pilani)
RP Ranjani, JJ (corresponding author), Birla Inst Technol & Sci, Dept Comp Sci & Informat Syst, Pilani Campus, Pilani 333031, Rajasthan, India.
EM f20181119@pilani.bits-pilani.ac.in; f20181117@pilani.bitspilani.ac.in;
   j.jenniferranjani@yahoo.co.in
RI Rajkumar, Jennifer Ranjani John/I-9753-2014
OI Rajkumar, Jennifer Ranjani John/0000-0001-8555-929X
CR [Anonymous], 2015, PROC 12 IEEE INT C A
   [Anonymous], 2019, ARXIV PREPRINT ARXIV
   [Anonymous], 2020, GOVTECH INTEREST THE
   Baek J, 2017, SENSORS-BASEL, V17, DOI 10.3390/s17081850
   Benenson R, 2015, LECT NOTES COMPUT SC, V8926, P613, DOI 10.1007/978-3-319-16181-5_47
   Bochkovskiy A, 2020, ARXIV PREPRINT ARXIV, DOI DOI 10.48550/ARXIV.2004.10934
   Braun M, 2019, IEEE T PATTERN ANAL, V41, P1844, DOI 10.1109/TPAMI.2019.2897684
   Cao Y., 2019, 2019 IEEE 5 INT C CO
   Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350
   Dai XR, 2021, APPL INTELL, V51, P1244, DOI 10.1007/s10489-020-01882-2
   Davis JW, 2005, WACV 2005: SEVENTH IEEE WORKSHOP ON APPLICATIONS OF COMPUTER VISION, PROCEEDINGS, P364
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Deng ZJ, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P684
   Devaguptapu C, 2019, IEEE COMPUT SOC CONF, P1029, DOI 10.1109/CVPRW.2019.00135
   Dollár P, 2009, PROC CVPR IEEE, P304, DOI 10.1109/CVPRW.2009.5206631
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Gaus YFA, 2020, PROC SPIE, V11542, DOI 10.1117/12.2573968
   Geiger A, 2013, INT J ROBOT RES, V32, P1231, DOI 10.1177/0278364913491297
   Ghose D, 2019, IEEE COMPUT SOC CONF, P988, DOI 10.1109/CVPRW.2019.00130
   Group F.A, 2018, FLIR STARTER THERMAL
   Hazan A., 2018, ARXIV PREPRINT ARXIV
   Herrmann C, 2018, PROC SPIE, V10643, DOI 10.1117/12.2304400
   Hwang S, 2015, PROC CVPR IEEE, P1037, DOI 10.1109/CVPR.2015.7298706
   John V, 2015, 2015 14TH IAPR INTERNATIONAL CONFERENCE ON MACHINE VISION APPLICATIONS (MVA), P246, DOI 10.1109/MVA.2015.7153177
   Kieu M, 2019, LECT NOTES COMPUT SC, V11752, P203, DOI 10.1007/978-3-030-30645-8_19
   Li C, 2018, BRIT MACHINE VISION
   Li CY, 2019, PATTERN RECOGN, V85, P161, DOI 10.1016/j.patcog.2018.08.005
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu NA, 2018, PROC CVPR IEEE, P3089, DOI 10.1109/CVPR.2018.00326
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Loshkarev IY, 2019, J PHYS CONF SER, V1333, DOI 10.1088/1742-6596/1333/4/042019
   Miezianko Roland, Terravic Research Infrared Database.
   Munir F., 2020, ARXIV200600821V1
   Kieu M, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3418213
   Redmon J, 2018, Arxiv, DOI [arXiv:1804.02767, DOI 10.1109/CVPR.2017.690, DOI 10.48550/ARXIV.1804.02767]
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Saeidi M, 2022, VISUAL COMPUT, V38, P2223, DOI 10.1007/s00371-021-02280-6
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474
   Schwarz M, 2015, IEEE INT CONF ROBOT, P1329, DOI 10.1109/ICRA.2015.7139363
   Tan MX, 2019, PR MACH LEARN RES, V97
   Wagner J., 2016, EUR S ART NEUR NETW
   Wu Z, 2014, IEEE COMPUT SOC CONF, P201, DOI 10.1109/CVPRW.2014.39
   Xu D, 2017, PROC CVPR IEEE, P4236, DOI 10.1109/CVPR.2017.451
   Zhang GY, 2017, IEEE SIGNAL PROC LET, V24, P1666, DOI 10.1109/LSP.2017.2731952
   Zhang GY, 2018, J VIS COMMUN IMAGE R, V52, P13, DOI 10.1016/j.jvcir.2018.01.013
   Zhang H, 2021, AUTOM CONTROL COMPUT, V55, P202, DOI 10.3103/S0146411621020097
   Zhang XQ, 2021, VISUAL COMPUT, V37, P1089, DOI 10.1007/s00371-020-01854-0
   Zheng Y., 2019, ARXIV190306999
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 49
TC 7
Z9 7
U1 5
U2 31
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2023
VL 39
IS 6
SI SI
BP 2347
EP 2363
DI 10.1007/s00371-022-02445-x
EA APR 2022
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA J1YY4
UT WOS:000779611700001
PM 35411122
OA Green Published, Bronze
DA 2024-07-18
ER

PT J
AU Liu, CJ
   Niu, DM
   Yang, XH
   Zhao, XY
AF Liu, Chuanju
   Niu, Dongmei
   Yang, Xinghai
   Zhao, Xiuyang
TI Graph matching based on feature and spatial location information
SO VISUAL COMPUTER
LA English
DT Article
DE Graph matching; Shape context descriptor; Feature information; Spatial
   location information
ID ALGORITHM
AB Graph matching (GM) is a crucial task in the fields of computer vision. It aims at finding node-to-node correspondences between two graphs. In this paper, we propose a new GM method. We combine feature and spatial location information to construct a mixture dissimilarity matrix and compensate for the deficiency that previous methods consider only feature information. The element of this matrix is used to measure the difference in a pair of nodes. First, feature dissimilarity is measured using a new shape context descriptor. Second, spatial location dissimilarity is obtained by capturing local and global information of nodes. In accordance with the mixture dissimilarity matrix, the initial correspondences are established using the Hungarian method. Lastly, our method solves two objective functions and obtains the matching results. The experimental results for three commonly used datasets verify the efficiency and advantages of our method relative to the current methods.
C1 [Liu, Chuanju; Niu, Dongmei; Zhao, Xiuyang] Univ Jinan, Shandong Prov Key Lab Network Based Intelligent C, Jinan, Peoples R China.
   [Liu, Chuanju; Niu, Dongmei; Zhao, Xiuyang] Univ Jinan, Sch Informat Sci & Engn, Jinan, Peoples R China.
   [Liu, Chuanju] Shandong Lab Vocat & Tech Coll, Jinan, Peoples R China.
   [Yang, Xinghai] Qingdao Univ Sci & Technol, Qingdao, Peoples R China.
C3 University of Jinan; University of Jinan; Qingdao University of Science
   & Technology
RP Zhao, XY (corresponding author), Univ Jinan, Shandong Prov Key Lab Network Based Intelligent C, Jinan, Peoples R China.; Zhao, XY (corresponding author), Univ Jinan, Sch Informat Sci & Engn, Jinan, Peoples R China.
EM chuanjuliu@outlook.com; xiuyangzhao@hotmail.com
FU Natural Science Foundation of Shandong province [ZR2019MF013,
   ZR2019BF026]; Project of Jinan Scientific Research Leader's Laboratory
   [2018GXRC023]
FX This research was supported by Natural Science Foundation of Shandong
   province (Nos. ZR2019MF013, ZR2019BF026), Project of Jinan Scientific
   Research Leader's Laboratory (No. 2018GXRC023).
CR Almasri I, 2014, 2014 IEEE INTERNATIONAL CONFERENCE ON DATA MINING WORKSHOP (ICDMW), P517, DOI 10.1109/ICDMW.2014.65
   [Anonymous], 2014, P IEEE C COMP VIS PA
   [Anonymous], 2017, COMPUTER VISION PATT
   Belongie S, 2002, IEEE T PATTERN ANAL, V24, P509, DOI 10.1109/34.993558
   Carletti V, 2018, IEEE T PATTERN ANAL, V40, P804, DOI 10.1109/TPAMI.2017.2696940
   Cho M, 2010, LECT NOTES COMPUT SC, V6315, P492
   Chui HL, 2003, COMPUT VIS IMAGE UND, V89, P114, DOI 10.1016/S1077-3142(03)00009-2
   Cour T., 2007, Advances in Neural Information Processing Systems, V19, P313
   Duchenne O, 2011, IEEE I CONF COMP VIS, P1792, DOI 10.1109/ICCV.2011.6126445
   Gold S, 1996, IEEE T PATTERN ANAL, V18, P377, DOI 10.1109/34.491619
   Jiang B, 2019, INT J COMPUT VISION, V127, P1345, DOI 10.1007/s11263-019-01185-1
   Jiang B, 2015, AAAI CONF ARTIF INTE, P3790
   Jiang B, 2017, PATTERN RECOGN, V61, P255, DOI 10.1016/j.patcog.2016.07.021
   KOOPMANS TC, 1957, ECONOMETRICA, V25, P53, DOI 10.2307/1907742
   LAWLER EL, 1963, MANAGE SCI, V9, P586, DOI 10.1287/mnsc.9.4.586
   Leordeanu M, 2005, IEEE I CONF COMP VIS, P1482
   Leordeanu M., 2009, NIPS, P1114
   Leordeanu M, 2012, INT J COMPUT VISION, V96, P28, DOI 10.1007/s11263-011-0442-2
   Riesen K, 2010, ADV DATABASE SYST, V40, P217, DOI 10.1007/978-1-4419-6045-0_7
   Shen TW, 2016, LECT NOTES COMPUT SC, V9907, P139, DOI 10.1007/978-3-319-46487-9_9
   Solnon C, 2010, ARTIF INTELL, V174, P850, DOI 10.1016/j.artint.2010.05.002
   ULLMANN JR, 1976, J ACM, V23, P31, DOI 10.1145/321921.321925
   Wang FD, 2020, IEEE T PATTERN ANAL, V42, P2737, DOI 10.1109/TPAMI.2019.2919308
   Wang FD, 2020, PROC CVPR IEEE, P3030, DOI 10.1109/CVPR42600.2020.00310
   Wang T, 2018, IEEE T PATTERN ANAL, V40, P2853, DOI 10.1109/TPAMI.2017.2767591
   Wang ZL, 2011, IEEE COMMUN LETT, V15, P49, DOI 10.1109/LCOMM.2010.111910.101806
   Wu J, 2013, PATTERN RECOGN, V46, P2927, DOI 10.1016/j.patcog.2013.04.008
   Yan JC, 2016, ICMR'16: PROCEEDINGS OF THE 2016 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P167, DOI 10.1145/2911996.2912035
   Yu T., 2018, Advances in neural information processing systems, P853
   Zass R, 2008, PROC CVPR IEEE, P1221
   Zhou F, 2016, IEEE T PATTERN ANAL, V38, P1774, DOI 10.1109/TPAMI.2015.2501802
NR 31
TC 2
Z9 2
U1 0
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2023
VL 39
IS 2
BP 711
EP 722
DI 10.1007/s00371-021-02369-y
EA MAR 2022
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 8T2TV
UT WOS:000762921500001
DA 2024-07-18
ER

PT J
AU Tyagi, S
   Yadav, D
AF Tyagi, Shobhit
   Yadav, Divakar
TI A detailed analysis of image and video forgery detection techniques
SO VISUAL COMPUTER
LA English
DT Article
DE Visual imagery forgery detection; Image and video manipulation and
   forensics; Deep learning
ID COPY-MOVE FORGERY; LOCALIZATION
AB With the recent advancement in modern technology, one can easily manipulate a digital image or video using computer software or a mobile application. The purpose of editing visual media could be as simple as to look good before sharing to the social networking site's or can be as malicious as to defame or hurt one's reputation in the real world through such morphed visual imagery. Identity theft is one of the examples where one's identity get stolen by some impersonator who can access the personal and financial information of an innocent person. To avoid such drastic situations, law enforcement authorities must use some automatic tools and techniques to find out whether a person is innocent or the culprit. One major question that arises here is how and what parts of visual imagery can be manipulated or edited. The answer to this question is important to distinguish the authentic images/videos from the doctored multimedia. This survey provides a detailed analysis of image and video manipulation types, popular visual imagery manipulation methods, and state-of-the-art image and video forgery detection techniques. It also surveys different fake image and video datasets used in tampering. The goal is to develop a sense of privacy and security in the research community. Finally, it focuses to motivate researchers to develop generalized methods to capture artificial visual imagery which is capable of detecting any type of manipulation in given visual imagery.
C1 [Tyagi, Shobhit; Yadav, Divakar] Natl Inst Technol, Dept Comp Sci & Engn, Hamirpur, HP, India.
C3 National Institute of Technology (NIT System); National Institute of
   Technology Hamirpur
RP Yadav, D (corresponding author), Natl Inst Technol, Dept Comp Sci & Engn, Hamirpur, HP, India.
EM shobhit.tya@gmail.com; dsy99@rediffmail.com
RI Yadav, DIVAKAR/AAF-1777-2020; Bueno, Regis Cortez/AAG-3852-2020; tyagi,
   shobhit/IVH-1265-2023
OI Yadav, DIVAKAR/0000-0001-6051-479X; Bueno, Regis
   Cortez/0000-0002-2923-4930; TYAGI, SHOBHIT/0000-0002-6262-0526
CR Abd Warif NB, 2017, J VIS COMMUN IMAGE R, V46, P219, DOI 10.1016/j.jvcir.2017.04.004
   Amerini I., DEEPFAKE VIDEO DETEC
   Amerini I, 2013, SIGNAL PROCESS-IMAGE, V28, P659, DOI 10.1016/j.image.2013.03.006
   Amerini I, 2017, IEEE COMPUT SOC CONF, P1865, DOI 10.1109/CVPRW.2017.233
   Amerini I, 2011, IEEE T INF FOREN SEC, V6, P1099, DOI 10.1109/TIFS.2011.2129512
   [Anonymous], 2004, TR2004518 DARTM COLL
   [Anonymous], 2018, CONVERSATION MEDIA G
   [Anonymous], Deepfakes github
   [Anonymous], IEEE IFS-TC Image Forensics Challenge Dataset
   Ardizzone E, 2015, IEEE T INF FOREN SEC, V10, P2084, DOI 10.1109/TIFS.2015.2445742
   Bahrami K, 2015, IEEE T INF FOREN SEC, V10, P999, DOI 10.1109/TIFS.2015.2394231
   Banerjee A, 2022, VISUAL COMPUT, V38, P321, DOI 10.1007/s00371-020-02017-x
   Bappy JH, 2019, IEEE T IMAGE PROCESS, V28, P3286, DOI 10.1109/TIP.2019.2895466
   Bas Patrick, 2011, Information Hiding. 13th International Conference, IH 2011. Revised Selected Papers, P59, DOI 10.1007/978-3-642-24178-9_5
   Bashar M, 2010, IEEE Trans Image Process, DOI 10.1109/TIP.2010.2046599
   Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014
   Bi XL, 2019, IEEE COMPUT SOC CONF, P30, DOI 10.1109/CVPRW.2019.00010
   Bondi L, 2017, IEEE COMPUT SOC CONF, P1855, DOI 10.1109/CVPRW.2017.232
   Bowling M, 2002, ARTIF INTELL, V136, P215, DOI 10.1016/S0004-3702(02)00121-2
   Bunk J, 2017, IEEE COMPUT SOC CONF, P1881, DOI 10.1109/CVPRW.2017.235
   Carrington, 2020, MANY PHOTOS 2020 DET
   Chen JS, 2015, IEEE SIGNAL PROC LET, V22, P1849, DOI 10.1109/LSP.2015.2438008
   Chen W, 2007, PROC SPIE, V6505, DOI 10.1117/12.704321
   Chouhan SS, 2019, ARCH COMPUT METHOD E, V26, P533, DOI 10.1007/s11831-018-9257-4
   Christlein V, 2012, IEEE T INF FOREN SEC, V7, P1841, DOI 10.1109/TIFS.2012.2218597
   Cozzolino D, 2015, IEEE T INF FOREN SEC, V10, P2284, DOI 10.1109/TIFS.2015.2455334
   Cozzolino D, 2016, IEEE INT WORKS INFOR
   Dang H, 2020, PROC CVPR IEEE, P5780, DOI 10.1109/CVPR42600.2020.00582
   Dolhansky Brian, 2019, ARXIV191008854
   Eveleth Rose, 2012, BBC NEWS 1213
   Farid H., Photo Tampering Throughout History"
   Fridrich J., 2003, P DIG FOR RES WORKSH, V3, P652
   GIMP Development Team, 2019, GIMP
   Gloe T., 2010, ACM S APPL COMP, P1584, DOI DOI 10.1080/15567281.2010.531500
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Google AI Blog, GOOGLE AI BLOG
   Gyncild AC., 2013, ADOBE PHOTOSHOP 200
   He JF, 2006, LECT NOTES COMPUT SC, V3953, P423
   Hossein-Nejad Z., VISUAL COMPUT, V1-17
   Hsu YF, 2006, 2006 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO - ICME 2006, VOLS 1-5, PROCEEDINGS, P549, DOI 10.1109/ICME.2006.262447
   Huang HL, 2008, PACIIA: 2008 PACIFIC-ASIA WORKSHOP ON COMPUTATIONAL INTELLIGENCE AND INDUSTRIAL APPLICATION, VOLS 1-3, PROCEEDINGS, P1241
   Iizuka S, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073659
   Jiang LM, 2020, PROC CVPR IEEE, P2886, DOI 10.1109/CVPR42600.2020.00296
   Jing Dong, 2013, 2013 IEEE China Summit and International Conference on Signal and Information Processing (ChinaSIP), P422, DOI 10.1109/ChinaSIP.2013.6625374
   Johnson M.K., 2005, Proceedings of the 7th Workshop on Multimedia and Security, P1
   Jung T, 2020, IEEE ACCESS, V8, P83144, DOI 10.1109/ACCESS.2020.2988660
   Karras T, 2021, IEEE T PATTERN ANAL, V43, P4217, DOI 10.1109/TPAMI.2020.2970919
   Karras Tero, 2020, IEEE C COMP VIS PATT
   Korshunov P., 2018, arXiv
   Kuznetsov A., 2019, Journal of Physics: Conference Series, V1368, DOI 10.1088/1742-6596/1368/3/032028
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Lee JC, 2015, INFORM SCIENCES, V321, P250, DOI 10.1016/j.ins.2015.03.009
   Li B, 2015, IEEE T INF FOREN SEC, V10, P558, DOI 10.1109/TIFS.2015.2389148
   Li HD, 2017, IEEE T INF FOREN SEC, V12, P1240, DOI 10.1109/TIFS.2017.2656823
   Li Y., 2019, NEW DATASET DEEPFAKE
   Lowe, 1999, P INT C COMP VIS, P1150, DOI DOI 10.1109/ICCV.1999.790410
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   LTD RTP, 2017, FAC SWAP BOOTH PHOT
   Marra F, 2019, IEEE INT WORKS INFOR, DOI 10.1109/wifs47025.2019.9035099
   Marra F, 2020, IEEE ACCESS, V8, P133488, DOI 10.1109/ACCESS.2020.3009877
   Matern F, 2019, IEEE WINT CONF APPL, P83, DOI 10.1109/WACVW.2019.00020
   Muzaffer G, 2019, 2019 SCIENTIFIC MEETING ON ELECTRICAL-ELECTRONICS & BIOMEDICAL ENGINEERING AND COMPUTER SCIENCE (EBBT), DOI 10.1109/ebbt.2019.8741657
   Ng A, 2018, Machine Learning Yearning
   Ng T.T., 2004, ADVENT Technical Report
   Ng TT, 2004, 2004 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS, VOL 5, PROCEEDINGS, P688
   Nguyen H.H., 2019, ARXIV PREPRINT ARXIV
   Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278
   Popescu AC, 2004, LECT NOTES COMPUT SC, V3200, P128
   Rao Y, 2016, IEEE INT WORKS INFOR
   Rathgeb C, 2020, IET BIOMETRICS, V9, P154, DOI 10.1049/iet-bmt.2019.0196
   Rocha L., 2003, A Practical Approach to Microarray Data Analysis, P91, DOI [10.1007/0-306-47815-3, DOI 10.1007/0-306-47815-35, 10.1007/0-306-47815-3_5, DOI 10.1007/0-306-47815-3_5]
   Rossler Andreas, 2018, Faceforensics: a large-scale video dataset for forgery detection in human faces
   Sabir E., 2019, INTERFACES GUI, V3, P80
   Salloum R, 2018, J VIS COMMUN IMAGE R, V51, P201, DOI 10.1016/j.jvcir.2018.01.010
   Schaefer G, 2004, PROC SPIE, V5307, P472, DOI 10.1117/12.525375
   Schetinger V, 2017, COMPUT GRAPH-UK, V68, P142, DOI 10.1016/j.cag.2017.08.010
   Schonfeld E., 2020, 2020 IEEECVF C COMPU, P8204, DOI 10.1109/CVPR42600.2020.00823
   Shi YQ, 2007, MM&SEC'07: PROCEEDINGS OF THE MULTIMEDIA & SECURITY WORKSHOP 2007, P51
   Shlens J., 2014, arXiv
   Singh KK, 2019, PROC CVPR IEEE, P6483, DOI 10.1109/CVPR.2019.00665
   Sneumueller, 2016, AUT FAC SWAP
   Stamm Matthew C., 2016, P 4 ACM WORKSH INF H, P5
   Sun DQ, 2018, PROC CVPR IEEE, P8934, DOI 10.1109/CVPR.2018.00931
   Tang SC, 2020, IEEE ACCESS, V8, P165044, DOI 10.1109/ACCESS.2020.3022820
   Theobalt Chris- tian, 2016, P IEEE C COMPUTER VI, DOI DOI 10.1109/CVPR.2016.262
   Thies J, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3323035
   Thyagharajan KK, 2021, ARCH COMPUT METHOD E, V28, P897, DOI 10.1007/s11831-020-09400-w
   Toews R., 2020, FORBES          0525
   Tolosana Ruben, 2020, Deepfakes evolution: Analysis of facial regions and fake detection performance
   Tralic Dijana, 2013, Proceedings of the 2013 55th International Symposium. ELMAR-2013, P49
   Tuba V, 2017, 2017 5TH INTERNATIONAL SYMPOSIUM ON DIGITAL FORENSIC AND SECURITY (ISDFS)
   Vinolin V, 2021, VISUAL COMPUT, V37, P2369, DOI 10.1007/s00371-020-01992-5
   Wang RZ, 2019, IEEE I CONF COMP VIS, P3056, DOI 10.1109/ICCV.2019.00315
   Wang SY, 2019, IEEE INT CONF COMM, DOI 10.1109/iccw.2019.8757016
   Wang W, 2009, LECT NOTES COMPUT SC, V5703, P308, DOI 10.1007/978-3-642-03688-0_27
   Wang XY, 2019, MATH BIOSCI ENG, V16, P4581, DOI 10.3934/mbe.2019229
   Wen BH, 2016, IEEE IMAGE PROC, P161, DOI 10.1109/ICIP.2016.7532339
   Wu Y, 2018, LECT NOTES COMPUT SC, V11210, P170, DOI 10.1007/978-3-030-01231-1_11
   Xie DR, 2015, IEEE SYS MAN CYBERN, P1821, DOI 10.1109/SMC.2015.319
   Yang C, 2017, PROC CVPR IEEE, P4076, DOI 10.1109/CVPR.2017.434
   Yang X, 2019, INT CONF ACOUST SPEE, P8261, DOI 10.1109/ICASSP.2019.8683164
   Yin T, 2015, COMPUT SECUR, V55, P130, DOI 10.1016/j.cose.2015.09.003
   Yu F., 2017, PROC CVPR IEEE, P472, DOI [DOI 10.1109/CVPR.2017.75, 10.1109/CVPR.2017.75]
   Yu JJ, 2017, LECT NOTES COMPUT SC, V10082, P3, DOI 10.1007/978-3-319-53465-7_1
   Zampoglou M, 2015, IEEE INT CONF MULTI
   Zhang W., 2017, INT J SOCIAL BEHAV E, V11, P231, DOI [10.5281/zenodo.1128985, DOI 10.5281/ZENODO.1128985]
   Zhang X, 2019, IEEE INT WORKS INFOR, DOI 10.1109/wifs47025.2019.9035107
   Zhang Y, 2016, CRYPTOL INF SEC SER, V14, P1, DOI 10.3233/978-1-61499-617-0-1
   Zhao XD, 2015, IEEE T CIRC SYST VID, V25, P185, DOI 10.1109/TCSVT.2014.2347513
   Zheng Y, 2020, IEEE T INF FOREN SEC, V15, P620, DOI 10.1109/TIFS.2019.2926777
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zhu Y, 2016, MULTIMED TOOLS APPL, V75, P3221, DOI 10.1007/s11042-014-2431-2
NR 113
TC 26
Z9 26
U1 2
U2 20
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2023
VL 39
IS 3
BP 813
EP 833
DI 10.1007/s00371-021-02347-4
EA JAN 2022
PG 21
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 9E6QG
UT WOS:000741976400001
DA 2024-07-18
ER

PT J
AU Jiang, Y
   Li, X
   Liu, YH
   Wang, W
   Du, JS
AF Jiang, Yu
   Li, Xiang
   Liu, Yaohua
   Wang, Wei
   Du, Jinsong
TI 3D interest point detection using balance-distortion oriented selection
SO VISUAL COMPUTER
LA English
DT Article
DE 3D interest point detection; Local topological structure;
   Balance-distortion
ID SALIENCY
AB Interest point detection is a challenging problem in 3D objects. Compared to traditional corner detection based on the curvature, this paper proposes a novel method that quantifies the balance and uniformity of local geometric structures based on the distribution of vertex neighborhoods. We first define the neighborhoods of vertices and structure them within the two-ring, instead of constructing the overall mesh, so as to avoid the interference between the neighborhoods of different vertices. Then we introduce the concept "balance-distortion" to describe the geometric features of the local structure. The experimental results show that the proposed algorithm is robust against noise and invariant to geometric transformation. In addition, compared with the corner detection, more feature points that do not satisfy the balance and direction uniformity are detected, and the distribution of interest point is more uniform.
C1 [Jiang, Yu; Li, Xiang; Liu, Yaohua; Wang, Wei; Du, Jinsong] Chinese Acad Sci, Shenyang Inst Automat, Shenyang 110016, Peoples R China.
   [Jiang, Yu; Li, Xiang; Liu, Yaohua; Wang, Wei; Du, Jinsong] Chinese Acad Sci, Inst Robot & Intelligent Mfg, Shenyang 110169, Peoples R China.
   [Jiang, Yu; Liu, Yaohua; Wang, Wei] Univ Chinese Acad Sci, Beijing 100049, Peoples R China.
   [Jiang, Yu; Li, Xiang; Liu, Yaohua; Wang, Wei; Du, Jinsong] Key Lab Intelligent Detect & Equipment Technol Li, Shenyang 100179, Peoples R China.
C3 Chinese Academy of Sciences; Shenyang Institute of Automation, CAS;
   Chinese Academy of Sciences; Chinese Academy of Sciences; University of
   Chinese Academy of Sciences, CAS
RP Li, X (corresponding author), Chinese Acad Sci, Shenyang Inst Automat, Shenyang 110016, Peoples R China.; Li, X (corresponding author), Chinese Acad Sci, Inst Robot & Intelligent Mfg, Shenyang 110169, Peoples R China.; Li, X (corresponding author), Key Lab Intelligent Detect & Equipment Technol Li, Shenyang 100179, Peoples R China.
EM lixiang@sia.cn
RI li, xiang/HCG-9736-2022
FU Strategic Priority Research Program of the Chinese Academy of Sciences
   [XDC04000000]; National Natural Science Foundation of China [62073312];
   Key Research and Development Program of LiaoNing [2020JH2/10100023];
   China Aero-engine Independent Innovation Special Fund Project
   [ZZCX-2018-035]; LiaoNing Revitalization Talents Program; K.C. Wong
   Education Foundation
FX This work was supported by the Strategic Priority Research Program of
   the Chinese Academy of Sciences (Grant No. XDC04000000), the National
   Natural Science Foundation of China (Grant No. 62073312), the Key
   Research and Development Program of LiaoNing (Grant No.
   2020JH2/10100023), China Aero-engine Independent Innovation Special Fund
   Project (Grant No. ZZCX-2018-035), LiaoNing Revitalization Talents
   Program and K.C. Wong Education Foundation.
CR [Anonymous], 2014, EUR C COMP VIS, P159
   [Anonymous], 2010, P EUR WORKSH 3D OBJ
   Azimi S., 2019, PERFORMANCE EVALUTIO
   Castellani U., 2010, COMPUT GRAPH FORUM, V643, P652
   Ding XY, 2019, IEEE T IMAGE PROCESS, V28, P5379, DOI 10.1109/TIP.2019.2918735
   Dutagaci H, 2012, VISUAL COMPUT, V28, P901, DOI 10.1007/s00371-012-0746-4
   Feng K., 2016, INT C DIG HOM IEEE C
   Gelfand N., 2005, P 3 EUR S GEOM PROC, V2, P5
   Gu XF, 2007, IEEE I CONF COMP VIS, P1554
   Guo Y, 2018, VISUAL COMPUT, V34, P1325, DOI 10.1007/s00371-017-1416-3
   Ho HT, 2009, IET COMPUT VIS, V3, P201, DOI 10.1049/iet-cvi.2009.0044
   HOPPE H, 1992, COMP GRAPH, V26, P71, DOI 10.1145/142920.134011
   Lee CH, 2005, ACM T GRAPHIC, V24, P659, DOI 10.1145/1073204.1073244
   Nishino K., 2007, IEEE 11 INT C COMP V, P1
   Niu DM, 2020, VISUAL COMPUT, V36, P767, DOI 10.1007/s00371-019-01658-x
   Rusu RB, 2009, IEEE INT CONF ROBOT, P1848
   Salti S., 2011, INT C 3D IM IEEE COM
   Sipiran I, 2011, VISUAL COMPUT, V27, P963, DOI 10.1007/s00371-011-0610-y
   Steder B., 2010, WORKSH DEF SOLV REAL
   Sun JA, 2009, COMPUT GRAPH FORUM, V28, P1383, DOI 10.1111/j.1467-8659.2009.01515.x
   Teran L, 2014, EUR C COMP VIS
   Tombari F, 2013, INT J COMPUT VISION, V102, P198, DOI 10.1007/s11263-012-0545-4
   Tonioni A, 2018, INT J COMPUT VISION, V126, P1, DOI 10.1007/s11263-017-1037-3
   Zaharescu A, 2012, INT J COMPUT VISION, V100, P78, DOI 10.1007/s11263-012-0528-5
   Zhao HS, 2022, VISUAL COMPUT, V38, P3765, DOI 10.1007/s00371-021-02217-z
   Zou GY, 2009, IEEE T VIS COMPUT GR, V15, P1193, DOI 10.1109/TVCG.2009.159
NR 26
TC 1
Z9 1
U1 0
U2 11
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2023
VL 39
IS 2
BP 733
EP 747
DI 10.1007/s00371-021-02371-4
EA JAN 2022
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 8T2TV
UT WOS:000741634600001
DA 2024-07-18
ER

PT J
AU Tsuchie, S
AF Tsuchie, Shoichi
TI Reconstruction of aesthetically smooth curves
SO VISUAL COMPUTER
LA English
DT Article
DE Curvature convexity; Parabolic curvature graph; Feature line; Underlying
   curve; Reverse engineering
AB We present a new method for reconstructing aesthetically smooth curves with a focus on engineering applications in the automotive industry. The condition of curvature monotonicity is an important design principle. However, its flexibility is insufficient to create curves in automotive industry applications. Therefore, in this study, we reconstruct a curve based on the curvature convexity such that the curve has a curvature graph that is close to a quadratic function. Consequently, the resulting curve can exhibit curvature monotonicity or convexity and can be flexibly represented based on the situation. The experimental results demonstrate the validity of the proposed method by comparing the obtained results with those created by CAD experts.
C1 [Tsuchie, Shoichi] Nihon Unisys Ltd, Koto Ku, 1-1-1 Toyosu, Tokyo 1358560, Japan.
RP Tsuchie, S (corresponding author), Nihon Unisys Ltd, Koto Ku, 1-1-1 Toyosu, Tokyo 1358560, Japan.
EM shoichi.tsuchie@unisys.co.jp
OI Tsuchie, Shoichi/0000-0002-9991-5635
CR [Anonymous], 2005, S GEOMETRY PROCESSIN
   Baran I, 2010, COMPUT GRAPH FORUM, V29, P655, DOI 10.1111/j.1467-8659.2009.01635.x
   Brent R. P., 1973, Algorithms for Minimization without Derivatives
   Burchard H, 1994, Designing fair curves and surfaces, P3
   Cao J, 2008, COMPUT AIDED GEOM D, V25, P523, DOI 10.1016/j.cagd.2007.10.001
   Farin G, 2006, COMPUT AIDED GEOM D, V23, P573, DOI 10.1016/j.cagd.2006.03.004
   Harada T., 1997, FORMA, V12, P55
   Hoschek J., 1993, FUNDAMENTALS COMPUTE
   Kamaike M., 2013, AUTOMOTIVE DESIGN HI
   Li GQ, 2001, COMPUTER GRAPHICS INTERNATIONAL 2001, PROCEEDINGS, P321, DOI 10.1109/CGI.2001.934690
   Li WS, 2005, COMPUT AIDED DESIGN, V37, P791, DOI 10.1016/j.cad.2004.09.008
   McCrae J, 2009, COMPUT GRAPH-UK, V33, P452, DOI 10.1016/j.cag.2009.05.006
   MORETON HP, 1992, COMP GRAPH, V26, P167, DOI 10.1145/142920.134035
   NELDER JA, 1965, COMPUT J, V7, P308, DOI 10.1093/comjnl/7.4.308
   NLopt, 2014, Nonlinear optimization library, 2.4.2)
   Okaniwa S, 2012, IEEE T VIS COMPUT GR, V18, P1474, DOI 10.1109/TVCG.2011.262
   Piegl L., 1997, The NURBS Book, V2, DOI DOI 10.1007/978-3-642-59223-2
   Schneider R., 1999, Curve and Surface Design: Saint-Malo, P371
   Tsuchie S, 2021, ENG COMPUT-GERMANY, V37, P211, DOI 10.1007/s00366-019-00817-x
   Tsuchie S, 2022, VISUAL COMPUT, V38, P493, DOI 10.1007/s00371-020-02030-0
   Tsuchie S, 2017, COMPUT GRAPH-UK, V68, P108, DOI 10.1016/j.cag.2017.08.015
   Tsuchie S, 2017, VISUAL COMPUT, V33, P1197, DOI 10.1007/s00371-016-1282-4
   Tsuchie S, 2016, COMPUT AIDED DESIGN, V71, P39, DOI 10.1016/j.cad.2015.09.004
   Vergne R., 2008, APPARENT RELIEF SHAP, DOI [10.1145/1377980.1377987, DOI 10.1145/1377980.1377987]
   Yamada Y, 1993, Clay modeling: techniques for giving three-dimensional form to idea
   Yang HP, 2004, COMPUT AIDED DESIGN, V36, P639, DOI 10.1016/S0010-4485(03)00140-4
   Yoshida N, 2006, VISUAL COMPUT, V22, P896, DOI 10.1007/s00371-006-0076-5
   Ziatdinov R, 2012, COMPUT AIDED GEOM D, V29, P510, DOI 10.1016/j.cagd.2012.03.006
NR 28
TC 2
Z9 2
U1 0
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2023
VL 39
IS 1
BP 353
EP 365
DI 10.1007/s00371-021-02334-9
EA JAN 2022
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA F6UJ1
UT WOS:000737733100002
DA 2024-07-18
ER

PT J
AU Ozkan, M
   Secil, S
   Turgut, K
   Dutagaci, H
   Uyanik, C
   Parlaktuna, O
AF Ozkan, Metin
   Secil, Sezgin
   Turgut, Kaya
   Dutagaci, Helin
   Uyanik, Cihan
   Parlaktuna, Osman
TI Surface profile-guided scan method for autonomous 3D reconstruction of
   unknown objects using an industrial robot
SO VISUAL COMPUTER
LA English
DT Article
DE 3D reconstruction; 3D modeling; Industrial robotic arm; Laser scanning;
   Point clouds; Sensor planning
ID SYSTEM; DEPLOYMENT; MODEL; TASK
AB This paper presents a novel method called surface profile-guided scan for 3D surface reconstruction of unknown objects. This method benefits from the advantages of two types of sensors: one having a wide field of view but of low resolution (type I) and the other of high resolution but with a narrow field of view (type II) for the autonomous reconstruction of highly accurate 3D models. It employs a range sensor (type II) mounted on an industrial manipulator, a rotary stage, and a color camera (type I). No prior knowledge of the geometry of the object is required. The only information available is that the object is located on a rotary table and is within the field of view of the camera and in the working space of the industrial robot. The camera provides a set of vertical surface profiles around the object, which are used to obtain scan paths for the range sensor. Then, the robot manipulator moves the range sensor along the scan paths. Finally, the 3D surface model is completed by detecting and rescanning holes on the surface. The quality of the surface model obtained from real objects by the proposed 3D reconstruction method proves its effectiveness and versatility.
C1 [Ozkan, Metin] Eskisehir Osmangazi Univ, Dept Comp Engn, Eskisehir, Turkey.
   [Secil, Sezgin; Turgut, Kaya; Dutagaci, Helin; Parlaktuna, Osman] Eskisehir Osmangazi Univ, Dept Elect & Elect Engn, Eskisehir, Turkey.
   [Uyanik, Cihan] Tech Univ Denmark, Dept Hlth Technol, Copenhagen, Denmark.
C3 Eskisehir Osmangazi University; Eskisehir Osmangazi University;
   Technical University of Denmark
RP Ozkan, M (corresponding author), Eskisehir Osmangazi Univ, Dept Comp Engn, Eskisehir, Turkey.
EM meozkan@ogu.edu.tr; ssecil@ogu.edu.tr; kayaturgut@hotmail.com;
   helindutagaci@gmail.com; ciuya@dtu.dk; oparlak@ogu.edu.tr
RI Secil, Sezgin/AGF-6619-2022; Al-obaidi, Abdullah Thair/P-8487-2017;
   turgut, kaya/JJG-1519-2023; Uyanik, Cihan/KDN-8572-2024; Dutagaci,
   Helin/KFR-3162-2024; Ozkan, Metin/JTU-7443-2023
OI Secil, Sezgin/0000-0002-5721-4363; Al-obaidi, Abdullah
   Thair/0000-0002-9971-5895; turgut, kaya/0000-0003-3345-9339; Ozkan,
   Metin/0000-0001-7281-3455; Uyanik, Cihan/0000-0003-3852-9164
FU Scientific and Technological Research Council of Turkey (TUBITAK)
   [115E374]; Scientific Research Project Commission of Eskisehir Osmangazi
   University (ESOGU-BAP) [201515A105]
FX This work was supported by the Scientific and Technological Research
   Council of Turkey (TUBITAK) under project number 115E374 and project
   title "Fully Automated 3D Modeling of Objects by Using an Industrial
   Robot Manipulator." This work was also supported in part by the
   Scientific Research Project Commission of Eskisehir Osmangazi University
   (ESOGU-BAP) under the project number 201515A105 and project title "3D
   reconstruction of unknown objects with a laser sensor and robot
   manipulator."
CR Ahmadabadian AH, 2017, MEASUREMENT, V99, P185, DOI 10.1016/j.measurement.2016.12.026
   Aleotti J., 2014, 2014 IEEE RSJ INT C
   BOCKMAN SF, 1989, AM MATH MON, V96, P131, DOI 10.2307/2323197
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851
   COWAN CK, 1988, IEEE T PATTERN ANAL, V10, P407, DOI 10.1109/34.3905
   Doumanoglou A, 2016, PROC CVPR IEEE, P3583, DOI 10.1109/CVPR.2016.390
   Guo XY, 2018, VISUAL COMPUT, V34, P93, DOI 10.1007/s00371-016-1316-y
   Hartley R, 2003, MULTIPLE VIEW GEOMET, DOI 10.1016/S0143-8166(01)00145-2
   He BW, 2010, ROBOT CIM-INT MANUF, V26, P711, DOI 10.1016/j.rcim.2010.03.011
   Hosseininaveh AA, 2014, ROBOT AUTON SYST, V62, P1197, DOI 10.1016/j.robot.2014.04.001
   Huang YB, 2007, COMPUT AIDED DESIGN, V39, P987, DOI 10.1016/j.cad.2007.06.008
   Karaszewski M, 2012, ROBOT AUTON SYST, V60, P1205, DOI 10.1016/j.robot.2012.05.005
   Khalfaoui S, 2013, COMPUT IND, V64, P1152, DOI 10.1016/j.compind.2013.04.005
   Kriegel S, 2015, J REAL-TIME IMAGE PR, V10, P611, DOI 10.1007/s11554-013-0386-6
   Lanman D, 2009, COMPUT VIS IMAGE UND, V113, P1107, DOI 10.1016/j.cviu.2009.03.016
   Loriot B., 2009, ELCVIA Electron. Lett. Comput. Vis. Image Anal., V7, P67, DOI [10.5565/rev/elcvia.192, DOI 10.5565/REV/ELCVIA.192]
   Marton ZC, 2009, IEEE INT CONF ROBOT, P2829
   Mavrinac A, 2015, IEEE-ASME T MECH, V20, P799, DOI 10.1109/TMECH.2014.2318729
   Mavrinac A, 2014, ACM T SENSOR NETWORK, V10, DOI 10.1145/2530373
   Mavrinac A, 2013, INT J COMPUT VISION, V101, P205, DOI 10.1007/s11263-012-0587-7
   Mendoza M, 2020, PATTERN RECOGN LETT, V133, P224, DOI 10.1016/j.patrec.2020.02.024
   Monica R, 2018, IEEE ROBOT AUTOM LET, V3, P3324, DOI 10.1109/LRA.2018.2852778
   Monica R, 2018, AUTON ROBOT, V42, P443, DOI 10.1007/s10514-017-9618-0
   Quinsat Y, 2015, INT J ADV MANUF TECH, V81, P411, DOI 10.1007/s00170-015-7185-0
   Santosi Z, 2019, MEASUREMENT, V147, DOI 10.1016/j.measurement.2019.106883
   Schwager M, 2011, P IEEE, V99, P1541, DOI 10.1109/JPROC.2011.2158377
   Secil S., 2017, 3 INT C CONTR AUT RO
   Shi JL, 2018, VISUAL COMPUT, V34, P377, DOI 10.1007/s00371-016-1339-4
   Sklansky J, 1982, PATTERN RECOGN LETT, V1, P79, DOI 10.1016/0167-8655(82)90016-2
   TARABANIS KA, 1995, IEEE T ROBOTIC AUTOM, V11, P72, DOI 10.1109/70.345939
   Torabi L, 2012, INT J ROBOT RES, V31, P82, DOI 10.1177/0278364911425836
   Turgut, 2016, THESIS ESKISEHIR OSM, P35
   Turgut K, 2017, IEEE INT CONF AUTON, P260, DOI 10.1109/ICARSC.2017.7964085
   Uyanik Cihan, 2018, Towards Autonomous Robotic Systems. 19th Annual Conference, TAROS 2018 Proceedings: Lecture Notes in Artificial Intelligence (LNAI 10965), P15, DOI 10.1007/978-3-319-96728-8_2
   Uyanik C, 2017, 2017 INTERNATIONAL CONFERENCE ON COMPUTER SCIENCE AND ENGINEERING (UBMK), P124, DOI 10.1109/UBMK.2017.8093580
   Vincenzo N., 2014, ROBOT CIM-INT MANUF, V26, P550
   Wang YM, 2019, IEEE ROBOT AUTOM LET, V4, P3340, DOI 10.1109/LRA.2019.2926676
   Xie ZX, 2007, INT J MACH TOOL MANU, V47, P33, DOI 10.1016/j.ijmachtools.2006.02.015
   Zhang XB, 2018, IEEE-ASME T MECH, V23, P1007, DOI 10.1109/TMECH.2018.2834393
   Zhang Z., 2004, Emerging Topics in Computer Vision
   Zhao W, 2007, VISUAL COMPUT, V23, P987, DOI 10.1007/s00371-007-0167-y
   Zivkovic Z, 2006, PATTERN RECOGN LETT, V27, P773, DOI 10.1016/j.patrec.2005.11.005
   Zivkovic Z, 2004, INT C PATT RECOG, P28, DOI 10.1109/ICPR.2004.1333992
NR 43
TC 3
Z9 3
U1 4
U2 27
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2022
VL 38
IS 11
BP 3953
EP 3977
DI 10.1007/s00371-021-02241-z
EA JUL 2021
PG 25
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 5X0BS
UT WOS:000673882600002
DA 2024-07-18
ER

PT J
AU Bukenberger, DR
   Lensch, HPA
AF Bukenberger, Dennis R.
   Lensch, Hendrik P. A.
TI Be water my friend: mesh assimilation
SO VISUAL COMPUTER
LA English
DT Article
DE Point cloud reconstruction; 3D scan; Remeshing; Shape assimilation;
   Meshing
ID SURFACE RECONSTRUCTION; IMPLEMENTATION; ALGORITHM
AB Inspired by the ability of water to assimilate any shape, if being poured into it, regardless if flat, round, sharp, or pointy, we present a novel, high-quality meshing method. Our algorithm creates a triangulated mesh, which automatically refines where necessary and accurately aligns to any target, given as mesh, point cloud, or volumetric function. Our core optimization iterates over steps for mesh uniformity, point cloud projection, and mesh topology corrections, always guaranteeing mesh integrity and epsilon-close surface reconstructions. In contrast with similar approaches, our simple algorithm operates on an individual vertex basis. This allows for automated and seamless transitions between the optimization phases for rough shape approximation and fine detail reconstruction. Therefore, our proposed algorithm equals established techniques in terms of accuracy and robustness but supersedes them in terms of simplicity and better feature reconstruction, all controlled by a single parameter, the intended edge length. Due to the overall increased versatility of input scenarios and robustness of the assimilation, our technique furthermore generalizes multiple established approaches such as ballooning or shrink wrapping.
C1 [Bukenberger, Dennis R.; Lensch, Hendrik P. A.] Univ Tubingen, Tubingen, Germany.
C3 Eberhard Karls University of Tubingen
RP Bukenberger, DR (corresponding author), Univ Tubingen, Tubingen, Germany.
EM dennis.bukenberger@uni-tuebingen.de
OI Bukenberger, Dennis/0000-0002-0659-7181
FU Projekt DEAL
FX Open Access funding enabled and organized by Projekt DEAL.
CR Alliez P, 2008, MATH VIS, P53, DOI 10.1007/978-3-540-33265-7_2
   Amenta N., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P415, DOI 10.1145/280814.280947
   Amenta N, 1999, DISCRETE COMPUT GEOM, V22, P481, DOI 10.1007/PL00009475
   Amenta Nina, 2001, P 6 ACM S SOL MOD AP
   [Anonymous], 2020, KEENANS 3D MODEL REP
   [Anonymous], 2014, The stanford 3d scanning repository
   Berger M, 2017, COMPUT GRAPH FORUM, V36, P301, DOI 10.1111/cgf.12802
   Bernardini F, 1999, IEEE T VIS COMPUT GR, V5, P349, DOI 10.1109/2945.817351
   Boltcheva D, 2017, COMPUT AIDED DESIGN, V90, P123, DOI 10.1016/j.cad.2017.05.011
   Botsch M., 2004, PROC EUROGRAPHICS AC, P185, DOI [10.1145/1057432.1057457, DOI 10.1145/1057432.1057457]
   BRESENHAM JE, 1965, IBM SYST J, V4, P25, DOI 10.1147/sj.41.0025
   Calakli F., 2012, EXPANDING FRONTIERS, P323, DOI DOI 10.1007/978-1-4471-2804-5_18
   Campos R, 2013, GRAPH MODELS, V75, P346, DOI 10.1016/j.gmod.2013.08.001
   Cohen-Steiner D, 2004, ACM T GRAPHIC, V23, P905, DOI 10.1145/1015706.1015817
   Digne J, 2015, IMAGE PROCESS ON LIN, V5, P282, DOI 10.5201/ipol.2015.102
   Digne J, 2014, IMAGE PROCESS ON LIN, V4, P149, DOI 10.5201/ipol.2014.81
   Digne J, 2011, COMPUT GRAPH FORUM, V30, P1630, DOI 10.1111/j.1467-8659.2011.01848.x
   Dignel J, 2011, IMAGE PROCESS ON LIN, V1, P281, DOI 10.5201/ipol.2011.dalmm_ps
   Duan Ye., 2001, Proceedings of the sixth ACM symposium on Solid modeling and applications, SMA '01, P47
   EDELSBRUNNER H, 1994, ACM T GRAPHIC, V13, P43, DOI 10.1145/174462.156635
   Fleishman S, 2005, ACM T GRAPHIC, V24, P544, DOI 10.1145/1073204.1073227
   Guibas L. J., 2018, CoRR abs/1802.01698
   Hanocka R, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392415
   Helmstaedter M, 2013, NATURE, V500, P168, DOI 10.1038/nature12346
   Jakob W, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818078
   Kazhdan M., 2006, P 4 EUR S GEOM PROC, P61, DOI DOI 10.2312/SGP/SGP06/061-070
   Kazhdan M, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2487228.2487237
   Kazhdan M, 2020, COMPUT GRAPH FORUM, V39, P173, DOI 10.1111/cgf.14077
   Kazhdan M, 2019, COMPUT GRAPH FORUM, V38, P138, DOI 10.1111/cgf.13449
   Kobbelt L, 2000, COMP GRAPH, P103, DOI 10.1145/344779.344835
   Lorensen William E., 1987, COMPUT GRAPH, P163, DOI DOI 10.1145/37402.37422
   Lu WY, 2020, COMPUT AIDED GEOM D, V77, DOI 10.1016/j.cagd.2020.101831
   Mescheder L, 2019, PROC CVPR IEEE, P4455, DOI 10.1109/CVPR.2019.00459
   Park JJ, 2019, PROC CVPR IEEE, P165, DOI 10.1109/CVPR.2019.00025
   Schertler N, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073635
   Sharf A, 2006, COMPUT GRAPH FORUM, V25, P389, DOI 10.1111/j.1467-8659.2006.00958.x
   Stanculescu L, 2011, COMPUT GRAPH-UK, V35, P614, DOI 10.1016/j.cag.2011.03.033
   Vlachos A., 2001, P 2001 S INTERACTIVE, P159, DOI [DOI 10.1145/364338.364387, 10.1145/364338.364387]
   Wu JH, 2004, COMPUT GRAPH FORUM, V23, P643, DOI 10.1111/j.1467-8659.2004.00796.x
   Zhou Qingnan, 2016, Thingi10k: A dataset of 10,000 3d-printing models
NR 40
TC 3
Z9 3
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2021
VL 37
IS 9-11
SI SI
BP 2725
EP 2739
DI 10.1007/s00371-021-02183-6
EA JUL 2021
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA UL2RI
UT WOS:000669180100004
OA hybrid
DA 2024-07-18
ER

PT J
AU Fang, YY
   Jin, ZY
   Cui, ZH
   Yang, QW
   Xie, TY
   Hu, B
AF Fang, Yanyan
   Jin, Zhiyu
   Cui, Zhenhua
   Yang, Qiaowen
   Xie, Tianyi
   Hu, Bo
TI Modeling human-human interaction with attention-based high-order GCN for
   trajectory prediction
SO VISUAL COMPUTER
LA English
DT Article
DE Graph convolutional network; Pedestrian trajectory prediction; Graph
   attention network; High-order GCN
AB This paper presents a novel high-order graph convolutional network (GCN) for pedestrian trajectory prediction. Specifically, the walking state of a target pedestrian depends on both its historical trajectory, which encodes its speed, walking direction and acceleration information, as well as the movement of its neighbors. Thus we propose to leverage GCNs to aggregate the trajectory features of the target pedestrian and its neighbors to predict the movement of the target pedestrian. Considering that the movement of the neighbors' neighbors affects the movement of the target pedestrian's neighbors, thus indirectly affecting the movement of the target pedestrian, we propose to use a high-order GCN for human-human interaction modelling. Such a high-order GCN considers the target pedestrian's neighbors as well as its neighbors' neighbors. Further, a pedestrian avoids collision with others by estimating its locations and its neighbors' upcoming locations, and it slows down or changes direction if it believes a collision may occur, especially in very crowded scenes. In light of this, we propose to model such anticipation-based decision making behavior as attention and combine it with our high-order GCN. Thus we first roughly estimate the future trajectories of all pedestrians with a simple method. By using the coarse predicted future trajectory and GCN outputs, we calculate the attention in our attention-based high-order GCN and predict future trajectory. Extensive experiments validate the effectiveness of our approach. In addition, our model shows a higher data efficiency. On the ETH&UCY dataset, using only 5% of the training data for each training epoch, our model outperforms the state of the art.
C1 [Fang, Yanyan; Cui, Zhenhua; Hu, Bo] Fudan Univ, Sch Informat & Technol, Shanghai, Peoples R China.
   [Jin, Zhiyu; Yang, Qiaowen; Xie, Tianyi] Fudan Univ, Sch Comp Sci, Shanghai, Peoples R China.
C3 Fudan University; Fudan University
RP Hu, B (corresponding author), Fudan Univ, Sch Informat & Technol, Shanghai, Peoples R China.
EM bohu@fudan.edu.cn
RI Alidadi, Mehdi/HJZ-0235-2023
OI Alidadi, Mehdi/0000-0001-5183-7829
FU National Key Research and Development Program of China [213]; Shanghai
   Municipal Natural Science Foundation [19ZR1404700]; Fudan-Zhuhai
   Innovation Institute; Fudan University-CIOMP Joint Fund [FC2019-003]
FX This work was supported by the National Key Research and Development
   Program of China (No. 213), the Shanghai Municipal Natural Science
   Foundation (No. 19ZR1404700), Fudan-Zhuhai Innovation Institute, and
   Fudan University-CIOMP Joint Fund (No. FC2019-003).
CR Abu-El-Haifa S, 2019, PR MACH LEARN RES, V97
   Alahi A, 2016, PROC CVPR IEEE, P961, DOI 10.1109/CVPR.2016.110
   Cancela B, 2014, PROC CVPR IEEE, P2553, DOI 10.1109/CVPR.2014.327
   Dong HR, 2020, IEEE T INTELL TRANSP, V21, P1849, DOI 10.1109/TITS.2019.2915014
   Emonet Remi, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3233, DOI 10.1109/CVPR.2011.5995572
   Fernandez-Ramírez J, 2020, VISUAL COMPUT, V36, P1535, DOI 10.1007/s00371-019-01754-y
   Gori M, 2005, IEEE IJCNN, P729
   Gupta A, 2018, PROC CVPR IEEE, P2255, DOI 10.1109/CVPR.2018.00240
   Hamilton WL, 2017, ADV NEUR IN, V30
   HELBING D, 1995, PHYS REV E, V51, P4282, DOI 10.1103/PhysRevE.51.4282
   Huang YF, 2019, IEEE I CONF COMP VIS, P6281, DOI 10.1109/ICCV.2019.00637
   Kipf TN, 2016, ARXIV
   Kong X., 2018, ARXIV180907697
   Kumar D, 2017, VISUAL COMPUT, V33, P265, DOI 10.1007/s00371-015-1192-x
   LeCun Y., 2013, P INT C LEARN REPR I
   Lerner A, 2007, COMPUT GRAPH FORUM, V26, P655, DOI 10.1111/j.1467-8659.2007.01089.x
   Li J., 2020, SOCIAL WAGDAT INTERA, DOI [10.13140/RG.2.2.25253.04320, DOI 10.13140/RG.2.2.25253.04320]
   Li MS, 2019, PROC CVPR IEEE, P3590, DOI 10.1109/CVPR.2019.00371
   Li YK, 2019, PROC CVPR IEEE, P294, DOI 10.1109/CVPR.2019.00038
   Liang JW, 2019, PROC CVPR IEEE, P5718, DOI 10.1109/CVPR.2019.00587
   Luber M, 2010, IEEE INT CONF ROBOT, P464, DOI 10.1109/ROBOT.2010.5509779
   Mehran R, 2009, PROC CVPR IEEE, P935, DOI 10.1109/CVPRW.2009.5206641
   Mohamed Abduallah, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P14412, DOI 10.1109/CVPR42600.2020.01443
   Pellegrini S, 2009, IEEE I CONF COMP VIS, P261, DOI 10.1109/ICCV.2009.5459260
   Pellegrini S, 2010, LECT NOTES COMPUT SC, V6311, P452, DOI 10.1007/978-3-642-15549-9_33
   Rudenko A, 2020, INT J ROBOT RES, V39, P895, DOI 10.1177/0278364920917446
   Sadeghian A, 2019, PROC CVPR IEEE, P1349, DOI 10.1109/CVPR.2019.00144
   Schöller C, 2020, IEEE ROBOT AUTOM LET, V5, P1696, DOI 10.1109/LRA.2020.2969925
   Si CY, 2019, PROC CVPR IEEE, P1227, DOI 10.1109/CVPR.2019.00132
   Su H, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2772
   Velickovic Petar, 2018, INT C LEARN REPR
   Vemula A, 2018, IEEE INT CONF ROBOT, P4601
   Xu K., 2018, P INT C LEARN REPR
   Xu YY, 2018, PROC CVPR IEEE, P5275, DOI 10.1109/CVPR.2018.00553
   Yagi T, 2018, PROC CVPR IEEE, P7593, DOI 10.1109/CVPR.2018.00792
   Yao Y, 2019, IEEE INT CONF ROBOT, P9711, DOI [10.1109/ICRA.2019.8794474, 10.1109/icra.2019.8794474]
   Yi S, 2016, LECT NOTES COMPUT SC, V9905, P263, DOI 10.1007/978-3-319-46448-0_16
   Yi S, 2016, IEEE T IMAGE PROCESS, V25, P4354, DOI 10.1109/TIP.2016.2590322
   Yi S, 2015, PROC CVPR IEEE, P3488, DOI 10.1109/CVPR.2015.7298971
   Zhang L., 2019, P BRIT MACHINE VISIO
   Zhang P, 2019, PROC CVPR IEEE, P12077, DOI 10.1109/CVPR.2019.01236
   Zhou BL, 2015, INT J COMPUT VISION, V111, P50, DOI 10.1007/s11263-014-0735-3
NR 42
TC 5
Z9 5
U1 3
U2 46
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2022
VL 38
IS 7
BP 2257
EP 2269
DI 10.1007/s00371-021-02109-2
EA JUL 2021
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 2B0WL
UT WOS:000668881000003
DA 2024-07-18
ER

PT J
AU Du, M
   Luo, T
   Xu, HY
   Song, Y
   Wang, CP
AF Du, Meng
   Luo, Ting
   Xu, Haiyong
   Song, Yang
   Wang, Chunpeng
TI Robust HDR video watermarking method based on saliency extraction and
   T-SVD
SO VISUAL COMPUTER
LA English
DT Article
DE HDR video; Saliency extraction; Tensor-singular value decomposition
   (T-SVD); Robust watermarking
ID BLIND IMAGE WATERMARKING; SCHEME; ALGORITHM; DWT; DCT; FACTORIZATION;
   HYBRID
AB In order to protect the copyright of the high dynamic range (HDR) video, a robust HDR video watermarking method based on saliency extraction and tensor-singular value decomposition (T-SVD) is proposed. Since T- SVD can not only represent high-dimension data, but also remain its intrinsic structure, each frame of the HDR video is considered as the third-order tensor to be transformed by using T-SVD for preserving the main characteristics of the frame. Each frame is divided into non-overlapping blocks, and each block is decomposed by using T-SVD to obtain the orthogonal tensor U, which includes three orthogonal matrices and represents main energies of the frame. Since the second matrix has more correlations of the video frame than other two matrices, it is used to embed watermark for robustness. Moreover, to obtain the trade-off between watermarking robustness and the visual quality, the saliency map of each frame is extracted to predict the most relevant and important areas for determining the watermark embedding strength. The saliency map is computed based on fusing the spatial saliency and the temporal saliency, where the spatial saliency is built by calculating color, intensity and orientation features of the HDR video and the temporal saliency is obtained by using the optical flow. Experiment results show that the proposed watermarking method can resist a variety of tone mapping attacks and video attacks, and is more robust than existing watermarking methods.
C1 [Du, Meng; Luo, Ting; Song, Yang] Ningbo Univ, Coll Sci & Technol, Ningbo 315212, Peoples R China.
   [Du, Meng; Luo, Ting; Xu, Haiyong] Ningbo Univ, Fac Informat Sci & Engn, Ningbo 315211, Peoples R China.
   [Wang, Chunpeng] Qilu Univ Technol Sch Acad Sci, Sch Informat, Jinan 250353, Peoples R China.
C3 Ningbo University; Ningbo University
RP Song, Y (corresponding author), Ningbo Univ, Coll Sci & Technol, Ningbo 315212, Peoples R China.
EM songyang@nbu.edu.cn
FU Natural Science Foundation of China [61971247, 61501270]; Zhejiang
   Provincial Natural Science Foundation of China [LY19F020009,
   LQ20F010002]; K. C. Wong Magna Fund in Ningbo University
FX This work was supported by Natural Science Foundation of China under
   Grant No. 61971247 and 61501270, Zhejiang Provincial Natural Science
   Foundation of China under Grant No. LY19F020009 and LQ20F010002. It was
   also sponsored by the K. C. Wong Magna Fund in Ningbo University.
CR Anbarjafari G, 2018, MULTIMED TOOLS APPL, V77, P24521, DOI 10.1007/s11042-018-5759-1
   Ashikhmin M., 2002, Rendering Techniques 2002. Eurographics Workshop Proceedings, P145
   Ahmadi SBB, 2021, VISUAL COMPUT, V37, P385, DOI 10.1007/s00371-020-01808-6
   Bai YQ, 2018, SIGNAL PROCESS-IMAGE, V65, P187, DOI 10.1016/j.image.2018.04.005
   Bakhsh FY, 2018, J INF SECUR APPL, V41, P12, DOI 10.1016/j.jisa.2018.05.003
   Banterle F., 2013, IEEE INT C DIG SIGN, DOI [10.1109/ICDSP.2013.6622687, DOI 10.1109/ICDSP.2013.6622687]
   Bhardwaj A, 2018, MULTIMED TOOLS APPL, V77, P19659, DOI 10.1007/s11042-017-5340-3
   Bremond Roland, 2012, TRENDS TOPICS COMPUT, P118, DOI [10.1145/1620993, DOI 10.1145/1620993]
   Cedillo-Hernandez A, 2018, J VIS COMMUN IMAGE R, V52, P106, DOI 10.1016/j.jvcir.2018.02.007
   Chang CC, 2016, MULTIMED TOOLS APPL, V75, P145, DOI 10.1007/s11042-014-2279-5
   Cheng YM, 2009, IEEE MULTIMEDIA, V16, P70, DOI 10.1109/MMUL.2009.43
   Dong YY, 2016, IEEE T MULTIMEDIA, V18, P549, DOI 10.1109/TMM.2016.2522639
   Eilertsen G, 2017, COMPUT GRAPH FORUM, V36, P565, DOI 10.1111/cgf.13148
   Ernawan F, 2020, VISUAL COMPUT, V36, P19, DOI 10.1007/s00371-018-1567-x
   Esfahani R, 2019, MULTIMED TOOLS APPL, V78, P16159, DOI 10.1007/s11042-018-6892-6
   Fang H, 2019, MULTIMED TOOLS APPL, V78, P8075, DOI 10.1007/s11042-018-6596-y
   Guerrini F, 2011, IEEE T INF FOREN SEC, V6, P283, DOI 10.1109/TIFS.2011.2109383
   Hatchett J, 2018, VISUAL COMPUT, V34, P167, DOI 10.1007/s00371-016-1322-0
   HORN BKP, 1981, ARTIF INTELL, V17, P185, DOI 10.1016/0004-3702(81)90024-2
   Hsu LY, 2019, IEEE ACCESS, V7, P107438, DOI 10.1109/ACCESS.2019.2932077
   Hu HT, 2016, AEU-INT J ELECTRON C, V70, P1374, DOI 10.1016/j.aeue.2016.07.011
   Itti L, 2003, PROC SPIE, V5200, P64, DOI 10.1117/12.512618
   Jia SL, 2017, J APPL SCI ENG, V20, P193, DOI 10.6180/jase.2017.20.2.07
   Joshi AM, 2017, ADV INTELL SYST, V468, P455, DOI 10.1007/978-981-10-1675-2_45
   Kang XB, 2018, MULTIMED TOOLS APPL, V77, P13197, DOI 10.1007/s11042-017-4941-1
   Karr BA, 2021, VISUAL COMPUT, V37, P895, DOI 10.1007/s00371-020-01841-5
   Khwildi R, 2020, VISUAL COMPUT, V36, P1111, DOI 10.1007/s00371-019-01719-1
   Kilmer ME, 2011, LINEAR ALGEBRA APPL, V435, P641, DOI 10.1016/j.laa.2010.09.020
   Kim MH, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531333
   Kuang JT, 2007, J VIS COMMUN IMAGE R, V18, P406, DOI 10.1016/j.jvcir.2007.06.003
   Lai CC, 2011, OPT COMMUN, V284, P938, DOI 10.1016/j.optcom.2010.10.047
   Lee SH, 2014, INFORM SCIENCES, V273, P263, DOI 10.1016/j.ins.2014.03.039
   Li MT, 2011, INT J INNOV COMPUT I, V7, P2021
   Lin YT, 2017, IEEE T MULTIMEDIA, V19, P196, DOI 10.1109/TMM.2016.2605499
   MANTIUK R, 2011, ACM T GRAPHIC, V30, P1, DOI DOI 10.1145/2010324.1964935
   Martin CD, 2013, SIAM J SCI COMPUT, V35, pA474, DOI 10.1137/110841229
   Najafi E, 2019, J INF SECUR APPL, V44, P144, DOI 10.1016/j.jisa.2018.12.002
   Nasiopoulos P, 2018, ICMSP 2014 12 INT C
   Ozcinar C, 2018, J VIS COMMUN IMAGE R, V55, P166, DOI 10.1016/j.jvcir.2018.06.003
   Perez-Daniel KR, 2020, IEEE ACCESS, V8, P156801, DOI 10.1109/ACCESS.2020.3019517
   Qin C, 2017, SIGNAL PROCESS, V138, P280, DOI 10.1016/j.sigpro.2017.03.033
   Rasti P, 2016, J VIS COMMUN IMAGE R, V38, P838, DOI 10.1016/j.jvcir.2016.05.001
   Shao ZH, 2016, SIGNAL PROCESS-IMAGE, V48, P12, DOI 10.1016/j.image.2016.09.001
   Solachidis V, 2013, PROC SPIE, V8655, DOI 10.1117/12.2005240
   Uscanga O. E., 1979, THESIS
   Wang XC, 2021, VISUAL COMPUT, V37, P859, DOI 10.1007/s00371-020-01948-9
   Wang XC, 2020, VISUAL COMPUT, V36, P2201, DOI 10.1007/s00371-020-01909-2
   Xue XW, 2011, IEICE T FUND ELECTR, VE94A, P2334, DOI 10.1587/transfun.E94.A.2334
   Yu CM, 2011, DISPLAYS, V32, P225, DOI 10.1016/j.displa.2011.02.004
   Yu M, 2019, IEEE ACCESS, V7, P113053, DOI 10.1109/ACCESS.2019.2935627
   Yuan ZH, 2021, VISUAL COMPUT, V37, P1867, DOI 10.1007/s00371-020-01945-y
   Zhang Y, 2016, IEEE T CIRC SYST VID, V26, P950, DOI 10.1109/TCSVT.2015.2426552
   Zhou RG, 2018, INT J QUANTUM INF, V16, DOI 10.1142/S0219749918500211
NR 53
TC 3
Z9 3
U1 2
U2 26
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2022
VL 38
IS 11
BP 3775
EP 3789
DI 10.1007/s00371-021-02220-4
EA JUN 2021
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 5X0BS
UT WOS:000667600400003
DA 2024-07-18
ER

PT J
AU Okuno, H
   Iwasaki, K
AF Okuno, Hiroki
   Iwasaki, Kei
TI Binary space partitioning visibility tree for polygonal and environment
   light rendering
SO VISUAL COMPUTER
LA English
DT Article
DE Polygonal light; GGX BRDF; Visibility
ID PRECOMPUTED RADIANCE TRANSFER; FREQUENCY
AB In this paper, we present a geometric approach to render shadows for physically based materials under polygonal light sources. Direct illumination calculation from a polygonal light source involves the triple product integral of the lighting, the bidirectional reflectance distribution function (BRDF), and the visibility function over the polygonal domain, which is computation intensive. To achieve real-time performance, work on polygonal light shading exploits analytical solutions of boundary integrals along the edges of the polygonal light at the cost of lacking shadowing effects. We introduce a hierarchical representation for the precomputed visibility function to retain the merits of closed-form solutions for boundary integrals. Our method subdivides the polygonal light into a set of polygons visible from a point to be shaded. Experimental results show that our method can render complex shadows with a GGX microfacet BRDF from polygonal light sources at interactive frame rates. In addition, our visibility representation can be easily incorporated into environment lighting.
C1 [Okuno, Hiroki; Iwasaki, Kei] Wakayama Univ, Wakayama, Japan.
   [Iwasaki, Kei] Prometech CG Res, Wakayama, Japan.
C3 Wakayama University
RP Iwasaki, K (corresponding author), Wakayama Univ, Wakayama, Japan.; Iwasaki, K (corresponding author), Prometech CG Res, Wakayama, Japan.
EM iwasaki@wakayama-u.ac.jp
RI Iwasaki, Kei/GNH-6504-2022
OI Iwasaki, Kei/0000-0002-5235-536X
FU JSPS KAKENHI [18H03348, 21H03571]; Grants-in-Aid for Scientific Research
   [21H03571, 18H03348, 21H05826] Funding Source: KAKEN
FX We thank the anonymous reviewers for their constructive suggestions and
   comments. This work was partially supported by JSPS KAKENHI Grant
   Numbers 18H03348 and 21H03571.
CR Arvo J., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P335, DOI 10.1145/218380.218467
   Baum D. R., 1989, Computer Graphics, V23, P325, DOI 10.1145/74334.74367
   Belcour L, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3015459
   Dupuy J, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073694
   Heitz E, 2018, ACM SIGGRAPH SYMPOSIUM ON INTERACTIVE 3D GRAPHICS AND GAMES (I3D 2018), DOI 10.1145/3190834.3190852
   Heitz E, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925895
   Ho TY, 2015, IEEE T VIS COMPUT GR, V21, P945, DOI 10.1109/TVCG.2015.2407398
   Iwasaki K, 2014, COMPUT GRAPH FORUM, V33, P333, DOI 10.1111/cgf.12302
   Iwasaki K, 2012, COMPUT GRAPH FORUM, V31, P727, DOI 10.1111/j.1467-8659.2012.03052.x
   Iwasaki Kei, 2007, Pro- ceedings of the Eurographics Symposium on Rendering, P35
   Lecocq P, 2017, IEEE T VIS COMPUT GR, V23, P1428, DOI 10.1109/TVCG.2017.2656889
   Ng R, 2004, ACM T GRAPHIC, V23, P477, DOI 10.1145/1015706.1015749
   Ng R, 2003, ACM T GRAPHIC, V22, P376, DOI 10.1145/882262.882280
   Nowrouzezahrai D, 2014, COMPUT GRAPH FORUM, V33, P105, DOI 10.1111/cgf.12257
   Sloan PP, 2002, ACM T GRAPHIC, V21, P527, DOI 10.1145/566570.566612
   Soler C, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2797136
   Sun B, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1516522.1516525
   Tsai YT, 2006, ACM T GRAPHIC, V25, P967, DOI 10.1145/1141911.1141981
   Wang JP, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618479
   Wang JW, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201291
   Wu LF, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392373
   Xu K, 2008, IEEE T VIS COMPUT GR, V14, P454, DOI 10.1109/TVCG.2007.70442
   Zhou K, 2005, ACM T GRAPHIC, V24, P1196, DOI 10.1145/1073204.1073332
NR 23
TC 0
Z9 0
U1 0
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2021
VL 37
IS 9-11
SI SI
BP 2499
EP 2511
DI 10.1007/s00371-021-02181-8
EA JUN 2021
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA UL2RI
UT WOS:000661442800002
DA 2024-07-18
ER

PT J
AU Bi, FK
   Han, JH
   Tian, YM
   Wang, YP
AF Bi, Fukun
   Han, Jianhong
   Tian, Yumeng
   Wang, Yanping
TI SSGAN: generative adversarial networks for the stroke segmentation of
   calligraphic characters
SO VISUAL COMPUTER
LA English
DT Article
DE Stroke segmentation; Calligraphic characters; Generative adversarial
   networks; Deep neural networks
ID IMAGE
AB At present, the Chinese government is encouraging people to learn calligraphy; however, an automatic evaluation method for the results is not available. Calligraphy evaluation is challenging, because calligraphic characters are complex graphics composed of strokes as basic units. Therefore, it is necessary to consider not only the structure of the whole character but also the details of each stroke. To address this problem, we propose an automatic stroke segmentation method for calligraphic images as the foundation for the subsequent evaluation task. Specifically, we treat the stroke segmentation problem as an image to image translation problem and propose the stroke segmentation generative adversarial network (SSGAN) algorithm. Unlike the existing approaches that do not exhibit a satisfactory performance or are not suitable for use in practical projects, the proposed approach can efficiently obtain accurate results. The SSGAN enables the simultaneous generation of all the strokes of a calligraphic image, based on a multistroke tensor training strategy. Moreover, we specifically design a ResU-Net structure with embedded attention modules to enhance the accuracy of the results. The experimental results demonstrate the superiority of the proposed method over the existing state of the art models.
C1 [Bi, Fukun; Han, Jianhong; Tian, Yumeng; Wang, Yanping] North China Univ Technol, Sch Informat Sci & Technol, Beijing 100144, Peoples R China.
C3 North China University of Technology
RP Han, JH (corresponding author), North China Univ Technol, Sch Informat Sci & Technol, Beijing 100144, Peoples R China.
EM 751410234@qq.com
FU National Natural Science Foundation of China [61971006]; Beijing Natural
   Science Foundation [4192021]
FX This work is supported in part by the National Natural Science
   Foundation of China (Grant No. 61971006) and Beijing Natural Science
   Foundation (Grant No. 4192021).
CR Arjovsky, 2017, ARXIV170104862
   Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615
   Bissoto A., 2019, ARXIV191013076
   Chen X, 2019, MULTIMED TOOLS APPL, V78, P11173, DOI 10.1007/s11042-018-6690-1
   Choi Y, 2018, PROC CVPR IEEE, P8789, DOI 10.1109/CVPR.2018.00916
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Hicsonmez S, 2020, IMAGE VISION COMPUT, V95, DOI 10.1016/j.imavis.2020.103886
   Huang X, 2018, LECT NOTES COMPUT SC, V11207, P179, DOI 10.1007/978-3-030-01219-9_11
   Isola Phillip, 2016, ARXIV161107004, DOI [DOI 10.1109/CVPR.2017.632, 10.1109/CVPR.2017.632]
   Kingma D. P., 2014, arXiv
   Lin GM, 2018, SIGNAL PROCESS-IMAGE, V68, P88, DOI 10.1016/j.image.2018.07.003
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   McGuinness K, 2010, PATTERN RECOGN, V43, P434, DOI 10.1016/j.patcog.2009.03.008
   Mirza M., 2014, ARXIV PREPRINT ARXIV, P2672, DOI 10.48550/arXiv.1411.1784
   Miyato T, 2018, INT C LEARN REPR
   Oktay O., 2018, ARXIV, DOI DOI 10.48550/ARXIV.1804.03999
   Park T, 2019, PROC CVPR IEEE, P2332, DOI 10.1109/CVPR.2019.00244
   Qu YY, 2019, PROC CVPR IEEE, P8152, DOI 10.1109/CVPR.2019.00835
   Radford A., 2015, ARXIV
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Salimans T, 2016, ADV NEUR IN, V29
   Wan, 2020, ACTA AUTOM SIN, DOI [10.16383/j.aas.c190141, DOI 10.16383/J.AAS.C190141]
   Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917
   Xie, 2019, MODERN COMPUT, V29, P3
   Xue Y, 2018, NEUROINFORMATICS, V16, P383, DOI 10.1007/s12021-018-9377-x
   Yu HP, 2018, MULTIMED TOOLS APPL, V77, P24097, DOI 10.1007/s11042-018-5697-y
   Zhang X., 2015, MICROCOMPUT APPL, V15, P51
NR 27
TC 7
Z9 8
U1 4
U2 31
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2022
VL 38
IS 7
BP 2581
EP 2590
DI 10.1007/s00371-021-02133-2
EA APR 2021
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 2B0WL
UT WOS:000642047400001
DA 2024-07-18
ER

PT J
AU Wu, W
   Zhang, SP
   Tian, M
   Tan, DQ
   Wu, XT
   Wan, Y
AF Wu, Wen
   Zhang, Shuping
   Tian, Mi
   Tan, Daoqiang
   Wu, Xiantao
   Wan, Yi
TI Learning to detect soft shadow from limited data
SO VISUAL COMPUTER
LA English
DT Article
DE Deep learning; Image processing; Semi-supervised learning; Domain
   adaptation; Soft shadow detection
AB Soft shadow is more challenging to detect than hard shadow due to its ambiguous boundary. Existing shadow detection methods pay more attention to hard shadow scene since collecting and annotating hard shadow images is more effortless. Motivated by that soft shadow has similar characteristics with hard shadow, and many traditional hard shadow datasets are publicly available, we propose a novel soft shadow detection method (namely Soft-DA) based on adversarial learning and domain adaptation scheme. Specifically, we create a limited soft shadow dataset, containing 1K soft shadow images with various scenes and shapes. Note that we just only need to annotate 0.4K shadow masks for semi-supervised learning. Besides, to tackle obvious domain discrepancy and potential intention difference between different datasets and similar tasks, we first align data distributions between domains by feature adversarial adaptation. And then, we introduce a novel detector separation strategy to tackle the intention difference issue. In this way, Soft-DA can effectively detect soft shadow with only a small number of soft shadow annotations. Extensive experiments demonstrate that our method can achieve superior performance to state of the arts.
C1 [Wu, Wen; Zhang, Shuping; Tian, Mi] Xinjiang Inst Technol, Sch Informat Engn, Aksu 843100, Peoples R China.
   [Tan, Daoqiang] Hubei Univ, Sch Comp & Informat Engn, Wuhan 431400, Peoples R China.
   [Wu, Xiantao] Xinjiang Univ, Coll Informat Sci & Engn, Urumqi 830046, Peoples R China.
   [Wan, Yi] Wenzhou Univ, Sch Elect & Elect Engn, Wenzhou 325035, Peoples R China.
C3 Xinjiang Institute of Technology; Hubei University; Xinjiang University;
   Wenzhou University
RP Wan, Y (corresponding author), Wenzhou Univ, Sch Elect & Elect Engn, Wenzhou 325035, Peoples R China.
EM 1119764335@qq.com; 2714105538@qq.com; 450269537@qq.com;
   787342703@qq.com; 1185777178@qq.com; 19945853@qq.com
RI Wu, Wen/HKW-7234-2023
OI Wu, Wen/0000-0003-0919-3948
FU Natural Science Foundation of Xinjiang Autonomous Region in China
   [2020D01A48]; National Social Science Foundation Western Project
   [20XGL029]
FX This work is supported by the Natural Science Foundation of Xinjiang
   Autonomous Region in China (No. 2020D01A48) and the National Social
   Science Foundation Western Project (No. 20XGL029).
CR Arbel E, 2011, IEEE T PATTERN ANAL, V33, P1202, DOI 10.1109/TPAMI.2010.157
   Bandara R, 2021, VISUAL COMPUT, V37, P275, DOI 10.1007/s00371-020-01798-5
   Ganin Y, 2015, PR MACH LEARN RES, V37, P1180
   Gong H., 2014, P BRIT MACH VIS C BM, P1
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Gryka M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2732407
   Guo RQ, 2011, PROC CVPR IEEE
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hsu HK, 2020, IEEE WINT CONF APPL, P738, DOI [10.1109/WACV45572.2020.9093358, 10.1109/wacv45572.2020.9093358]
   Hu XW, 2018, PROC CVPR IEEE, P7454, DOI 10.1109/CVPR.2018.00778
   Kán P, 2019, VISUAL COMPUT, V35, P873, DOI 10.1007/s00371-019-01666-x
   Karsch K, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024191
   Khan SH, 2016, IEEE T PATTERN ANAL, V38, P431, DOI 10.1109/TPAMI.2015.2462355
   Khodabandeh M, 2019, IEEE I CONF COMP VIS, P480, DOI 10.1109/ICCV.2019.00057
   Lafarge MW, 2017, LECT NOTES COMPUT SC, V10553, P83, DOI 10.1007/978-3-319-67558-9_10
   Lalonde JF, 2009, IEEE I CONF COMP VIS, P183, DOI 10.1109/ICCV.2009.5459163
   Le HE, 2018, LECT NOTES COMPUT SC, V11206, P680, DOI 10.1007/978-3-030-01216-8_41
   Luo YW, 2019, PROC CVPR IEEE, P2502, DOI 10.1109/CVPR.2019.00261
   Luo Z., 2017, P 31 INT C NEUR INF, P165
   Mao XD, 2017, IEEE I CONF COMP VIS, P2813, DOI 10.1109/ICCV.2017.304
   Mohammadi SM, 2018, IEEE ENG MED BIO, P3501, DOI 10.1109/EMBC.2018.8513009
   Motiian S, 2017, IEEE I CONF COMP VIS, P5716, DOI 10.1109/ICCV.2017.609
   Nielsen M, 2007, LECT NOTES COMPUT SC, V4522, P918
   Okabe T, 2009, IEEE I CONF COMP VIS, P1693
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Saito K, 2018, PROC CVPR IEEE, P3723, DOI 10.1109/CVPR.2018.00392
   Sanchez-Matilla R, 2016, LECT NOTES COMPUT SC, V9914, P84, DOI 10.1007/978-3-319-48881-3_7
   Savarese S, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P190, DOI 10.1109/ICCV.2001.937517
   Shen L, 2015, PROC CVPR IEEE, P2067, DOI 10.1109/CVPR.2015.7298818
   Tsung-Yi Lin, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P2999, DOI 10.1109/ICCV.2017.324
   Tzeng E., 2014, ARXIV14123474
   Tzeng E, 2017, PROC CVPR IEEE, P2962, DOI 10.1109/CVPR.2017.316
   Vincente TFY, 2016, LECT NOTES COMPUT SC, V9910, P816, DOI 10.1007/978-3-319-46466-4_49
   Nguyen V, 2017, IEEE I CONF COMP VIS, P4520, DOI 10.1109/ICCV.2017.483
   Wang JF, 2018, PROC CVPR IEEE, P1788, DOI 10.1109/CVPR.2018.00192
   Wang SJ, 2019, LECT NOTES COMPUT SC, V11764, P102, DOI 10.1007/978-3-030-32239-7_12
   Wang TY, 2020, PROC CVPR IEEE, P1877, DOI 10.1109/CVPR42600.2020.00195
   Wu TP, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1243980.1243982
   Zhang YF, 2019, LECT NOTES COMPUT SC, V11764, P360, DOI 10.1007/978-3-030-32239-7_40
   Zheng MJ, 2020, IMAGE VISION COMPUT, V103, DOI 10.1016/j.imavis.2020.103987
   Zheng QL, 2019, PROC CVPR IEEE, P5162, DOI 10.1109/CVPR.2019.00531
   Zhu L, 2018, LECT NOTES COMPUT SC, V11210, P122, DOI 10.1007/978-3-030-01231-1_8
   Zhu X., 2019, VISUAL COMPUT, P1
NR 43
TC 7
Z9 7
U1 0
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2022
VL 38
IS 5
BP 1665
EP 1675
DI 10.1007/s00371-021-02095-5
EA MAR 2021
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 0M9JP
UT WOS:000626807900002
DA 2024-07-18
ER

PT J
AU Mahmud, H
   Islam, R
   Hasan, MK
AF Mahmud, Hasan
   Islam, Robiul
   Hasan, Md. Kamrul
TI On-air English Capital Alphabet (ECA) recognition using depth
   information
SO VISUAL COMPUTER
LA English
DT Article
DE Air-writing; Gesture recognition; Depth information; Time-series data;
   Dynamic time warping; Support vector machine
AB On-air writing can be considered as a time-dependent event where hand gesture is produced in a natural environment through index finger movement. A sequence of such movements containing several time steps in 3D space can be utilized to construct an English Capital Alphabet (ECA). While Previous researches investigated 2D features, we believe that depth information may play a significant role along with other features in recognition of these dynamic gestures. We have captured hand finger motion information using a depth camera and represented them as depth images for each ECA. The hand finger trajectory data were extracted from the depth image, and a combination of depth-based features and non-depth features were generated; depth variation was performed in the depth-based features, and then, all the feature values were converted into time-series data. Dynamic Time Warping distances were determined between a template ECA and a test ECA for each ECA collected from 15 participants. These distance-based features were then fed into a multi-class SVM for training and testing and got the recognition accuracy of 80.77% without depth and 88.21% with depth-based features. To cope with the over-fitting problem, we applied the resampling technique and got the highest recognition accuracy of 96.85%, and at last, we applied some feature selection techniques to analyze the recognition results.
C1 [Mahmud, Hasan; Hasan, Md. Kamrul] Islamic Univ Technol IUT, Syst & Software Lab, Dept Comp Sci & Engn CSE, Dhaka, Bangladesh.
   [Islam, Robiul] Higher Sch Econ, Lab Computat Pragmat Models & Methods, Pokrovsky Blvd 11, Moscow, Russia.
C3 HSE University (National Research University Higher School of Economics)
RP Mahmud, H (corresponding author), Islamic Univ Technol IUT, Syst & Software Lab, Dept Comp Sci & Engn CSE, Dhaka, Bangladesh.
EM hasan@iut-dhaka.edu; rislam@edu.hse.ru; hasank@iut-dhaka.edu
RI Islam, Robiul/AAJ-4785-2021; Hasan, Kamrul/AAY-6295-2020
OI Islam, Robiul/0000-0002-3704-8409; Hasan, Kamrul/0000-0001-9099-5424;
   Mahmud, Hasan/0000-0003-4375-6943
CR Agrawal S., 2011, Proceedings of the 9th International Conference on Mobile Systems, Applications, P15, DOI [10.1145/1999995.1999998, DOI 10.1145/1999995.1999998]
   Amma C, 2014, PERS UBIQUIT COMPUT, V18, P191, DOI 10.1007/s00779-013-0637-3
   [Anonymous], 2013, KNOWLEDGE EURONEWS
   Barrentine D.B., 2015, US Patent, Patent No. [8,977,255, 8977255]
   Hall M., 2009, ACM SIGKDD Explor. Newsl, V11, P18, DOI DOI 10.1145/1656274.1656278
   HUNTER JS, 1986, J QUAL TECHNOL, V18, P203
   Islam R., 2016, 9 INT C ADV COMP HUM, P299
   Jalalian A, 2013, NEUROCOMPUTING, V99, P270, DOI 10.1016/j.neucom.2012.07.006
   Jeong YS, 2011, PATTERN RECOGN, V44, P2231, DOI 10.1016/j.patcog.2010.09.022
   Kim DH, 2006, LECT NOTES COMPUT SC, V4239, P41
   Mahmud H, 2018, ADV HUM-COMPUT INTER, V2018, DOI 10.1155/2018/1069823
   Mohammadi S, 2020, VISUAL COMPUT, V36, P1001, DOI 10.1007/s00371-019-01717-3
   Mullin R., 1985, CAN J STAT, V13, P167
   Priya A, 2016, 2016 INTERNATIONAL CONFERENCE ON COMMUNICATION AND SIGNAL PROCESSING (ICCSP), VOL. 1, P967, DOI 10.1109/ICCSP.2016.7754291
   Ren Z, 2013, IEEE T MULTIMEDIA, V15, P1110, DOI 10.1109/TMM.2013.2246148
   Siena FL, 2018, J MED SYST, V42, DOI 10.1007/s10916-018-0905-x
   Sreekanth NS, 2017, LECT NOTES ELECTR EN, V395, P105, DOI 10.1007/978-81-322-3592-7_11
   Thomas M, 2014, Journal of Japanese Linguistics, V30, P86, DOI [10.1515/jjl-2014-0107, DOI 10.1515/JJL-2014-0107]
   Wexelblatt A., 1995, ACM Trans. on Computer-Human Interaction, V2, P179
   Yin YF, 2019, ACM T SENSOR NETWORK, V15, DOI 10.1145/3343855
NR 20
TC 3
Z9 3
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2022
VL 38
IS 3
BP 1015
EP 1025
DI 10.1007/s00371-021-02065-x
EA JAN 2021
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZQ8YX
UT WOS:000613039600002
DA 2024-07-18
ER

PT J
AU Saranya, P
   Prabakaran, S
   Kumar, R
   Das, E
AF Saranya, P.
   Prabakaran, S.
   Kumar, Rahul
   Das, Eshani
TI Blood vessel segmentation in retinal fundus images for proliferative
   diabetic retinopathy screening using deep learning
SO VISUAL COMPUTER
LA English
DT Article
DE Proliferative diabetic retinopathy; Blood vessels; Neovascularization;
   Vision loss; Pathogenic blood vessels; Dense-net; CNN
AB Diabetic retinopathy (DR) is also called diabetic eye disease, which causes damage to the retina due to diabetes mellitus and that leads to blindness when the disease reaches an extreme stage. The medical tests take a lot of procedure, time, and money to test for the proliferative stage of diabetic retinopathy (PDR). Hence to resolve this problem, this model is proposed to detect and identify the proliferative stages of diabetic retinopathy which is also identified by its hallmark feature that is neovascularization. In the proposed system, the paper aims to correctly identify the presence of neovascularization using color fundus images. The presence of neovascularization in an eye is an indication that the eye is affected with proliferative PDR. Neovascularization is the development of new abnormal blood vessels in the retina. Since the occurrence of neovascularization may lead to partial or complete vision loss, timely and accurate prediction is important. The aim of the paper is to propose a method to detect the presence of neovascularization which involves image processing methods such as resizing, green channel filtering, Gaussian filter, and morphology techniques such as erosion and dilation. For classification, the different layers of CNN have been used and modeled together in a VGG-16 net architecture. The model was trained and tested on 2200 images all together from the Kaggle database. The proposed model was tested using DRIVE and STARE data sets, and the accuracy, specificity, sensitivity, precision, F1 score achieved are 0.96, 0.99, 0.95, 0.99, and 0.97, respectively, on DRIVE and 0.95, 0.99, 0.9375, 0.96, and 0.95, respectively, on STARE.
C1 [Saranya, P.; Prabakaran, S.; Kumar, Rahul; Das, Eshani] SRM Inst Sci & Technol, Dept Comp Sci & Engn, Chennai 603203, Tamil Nadu, India.
C3 SRM Institute of Science & Technology Chennai
RP Saranya, P (corresponding author), SRM Inst Sci & Technol, Dept Comp Sci & Engn, Chennai 603203, Tamil Nadu, India.
EM saranyap@srmist.edu.in
RI P, Saranya/ABY-7095-2022
OI P, Saranya/0000-0003-2333-6968
CR [Anonymous], Structured Analysis of the Retina
   Chang Y, 2018, IEEE ACCESS, V6, P11782, DOI 10.1109/ACCESS.2018.2797872
   Chen L, 2019, VISUAL COMPUT, V35, P1361, DOI 10.1007/s00371-018-01615-0
   Cherukuri V, 2020, IEEE T IMAGE PROCESS, V29, P2552, DOI 10.1109/TIP.2019.2946078
   Dharmawan DA, 2019, IEEE ACCESS, V7, P41885, DOI 10.1109/ACCESS.2019.2906344
   Guo XX, 2019, IEEE ACCESS, V7, P176912, DOI 10.1109/ACCESS.2019.2957776
   Hammad I, 2018, IEEE ACCESS, V6, P60438, DOI 10.1109/ACCESS.2018.2875376
   Javidi M, 2017, COMPUT METH PROG BIO, V139, P93, DOI 10.1016/j.cmpb.2016.10.015
   Kar SS, 2018, IET IMAGE PROCESS, V12, P1956, DOI 10.1049/iet-ipr.2017.1013
   Lagae A, 2011, IEEE T VIS COMPUT GR, V17, P1096, DOI 10.1109/TVCG.2010.238
   Li XJ, 2020, VISUAL COMPUT, V36, P39, DOI 10.1007/s00371-018-1582-y
   Lin Y, 2019, IEEE ACCESS, V7, P57717, DOI 10.1109/ACCESS.2018.2844861
   Liu X, 2018, INT GEOSCI REMOTE SE, P7137, DOI 10.1109/IGARSS.2018.8518078
   Pan JD, 2018, AM J OPHTHALMOL, V192, P146, DOI 10.1016/j.ajo.2018.05.018
   Pan XQ, 2019, IEEE ACCESS, V7, P122634, DOI 10.1109/ACCESS.2019.2935138
   Qummar S, 2019, IEEE ACCESS, V7, P150530, DOI 10.1109/ACCESS.2019.2947484
   Remeseiro B, 2021, VISUAL COMPUT, V37, P1247, DOI 10.1007/s00371-020-01863-z
   Shah SAA, 2019, IEEE ACCESS, V7, P167221, DOI 10.1109/ACCESS.2019.2954314
   Soomro TA, 2019, IEEE ACCESS, V7, P158183, DOI 10.1109/ACCESS.2019.2950228
   Thangaraj S, 2018, IET IMAGE PROCESS, V12, P669, DOI 10.1049/iet-ipr.2017.0284
   VanGinneken, DRIVE DIGITAL RETINA
   Yan ZQ, 2019, IEEE J BIOMED HEALTH, V23, P1427, DOI 10.1109/JBHI.2018.2872813
   Yu S, 2018, IEEE J BIOMED HEALTH, V22, P886, DOI 10.1109/JBHI.2017.2710201
NR 23
TC 29
Z9 29
U1 1
U2 15
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2022
VL 38
IS 3
BP 977
EP 992
DI 10.1007/s00371-021-02062-0
EA JAN 2021
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZQ8YX
UT WOS:000612910500001
DA 2024-07-18
ER

PT J
AU Ballit, A
   Mougharbel, I
   Ghaziri, H
   Dao, TT
AF Ballit, Abbass
   Mougharbel, Imad
   Ghaziri, Hassan
   Dao, Tien-Tuan
TI Computer-aided parametric prosthetic socket design based on real-time
   soft tissue deformation and an inverse approach
SO VISUAL COMPUTER
LA English
DT Article
DE Computer-aided socket design; Real-time soft tissue deformation;
   Mass-spring system; Inverse approach; Lower limb prosthesis
AB The prosthetic socket provides the critical interface between the prosthetic device and the patient's residual limb. Since each stump is unique in terms of morphology and mechanics, each socket should be patient specific. Computer-aided design solutions have been proposed in the literature. However, there is a lack of an efficient solution able to modify local information based on soft tissue deformation feedback to enhance the design process. The objective of the present work was to develop and evaluate a computer-aided design approach with real-time soft tissue deformation feedback and an inverse approach to optimize the stump-socket interaction. A computer-aided parametric socket design workflow was proposed. Soft tissue deformation was performed using a novel formulation of the mass-spring system. An inverse approach was proposed to estimate and optimize the stump-socket interaction. An interactive parametric design tool was also developed and evaluated. The proposed approach was applied on a CT-based dataset. Finally, the obtained design outcomes were compared with FE simulation outcomes for evaluation purpose. As results, a virtual socket prototype of the CT-based stump model was designed and illustrated by an interactive process. The comparison of stump-socket interaction behavior with FE simulation outcomes showed a very good agreement with a pressure absolute deviation error ranging from 1.44 +/- 2.13 to 3.66 +/- 4.56 kPa. Moreover, the contact pressures are below the pain-threshold curves, confirming the comfortability of the designed sockets according to the predefined criteria. The present study proposed a computer-aided socket design solution to locally enhance the socket geometry and mechanics. This opens new avenues to increase the design accuracy, reduce the design cost and give the involved patient a geometrically and mechanically fitted socket device. As perspectives, this process will be integrated with our available visual sensor fusion toward a complete computer-aided socket design system for lower limb prosthetic design and fabrication.
C1 [Ballit, Abbass; Dao, Tien-Tuan] Univ Technol Compiegne, CNRS, Ctr Rech Royallieu, Biomech & Bioengn,CS 60 319, F-60203 Compiegne, France.
   [Mougharbel, Imad] Ecole Technol Super Montreal, Grp Rech Elect Puissance & Commande Ind, Montreal, PQ, Canada.
   [Ballit, Abbass; Ghaziri, Hassan] Beirut Res & Innovat Ctr BRIC, Beirut, Lebanon.
   [Dao, Tien-Tuan] Univ Lille, LaMcube Lab Mecan, Cent Lille, CNRS,UMR 9013,Multiphys,Multiechelle, F-59000 Lille, France.
C3 Centre National de la Recherche Scientifique (CNRS); Universite de
   Technologie de Compiegne; University of Quebec; Ecole de Technologie
   Superieure - Canada; Universite de Lille; Centrale Lille; Centre
   National de la Recherche Scientifique (CNRS)
RP Dao, TT (corresponding author), Univ Technol Compiegne, CNRS, Ctr Rech Royallieu, Biomech & Bioengn,CS 60 319, F-60203 Compiegne, France.; Dao, TT (corresponding author), Univ Lille, LaMcube Lab Mecan, Cent Lille, CNRS,UMR 9013,Multiphys,Multiechelle, F-59000 Lille, France.
EM abbass-khodor.ballit@utc.fr; imadmoug@yahoo.com;
   hassan.ghaziri@lebcsr.org; tien-tuan.dao@centralelille.fr
OI Dao, Tien-Tuan/0000-0002-5088-3433; Ballit, Abbass/0000-0001-5852-8973
FU 'Beirut Research and Innovation Center' (BRIC), Beirut, Lebanon
FX The authors would like to acknowledge the support and funding of 'Beirut
   Research and Innovation Center' (BRIC), Beirut, Lebanon.
CR Allami M., 2018, CAN PROSTHET ORTHOT, DOI [10.33137/cpoj.v1i2.32028, DOI 10.33137/CPOJ.V1I2.32028]
   Ballit A, 2020, IRBM, V41, P276, DOI 10.1016/j.irbm.2020.02.003
   Ballit A, 2020, IEEE SENS J, V20, P15043, DOI 10.1109/JSEN.2020.3011172
   Barbic J, 2007, SYMPOSIUM ON COMPUTER ANIMATION 2007: ACM SIGGRAPH/ EUROGRAPHICS SYMPOSIUM PROCEEDINGS, P171
   Bonacini Daniele., 2007, Optical 3-D measurement techniques, P96
   Caspers C.A., 2010, US Patent, Patent No. [US20100312360A1, 20100312360]
   Choi APC, 2005, MED BIOL ENG COMPUT, V43, P258, DOI 10.1007/BF02345964
   Chuang W.-C., DES MANUF, DOI [10.1115/imece2009-10758, DOI 10.1115/IMECE2009-10758]
   Chuang W-C., 2011, COMPUT AIDED DES APP, V8, P723, DOI [10.3722/cadaps.2011.723-734, DOI 10.3722/CADAPS.2011.723-734]
   Colombo, 2019, VIRTUAL PLATFORM LOW, DOI [10.1016/B978-0-12-816713-7.00057-X, DOI 10.1016/B978-0-12-816713-7.00057-X]
   Colombo G., 2011, Comput Aided Des Appl, V8, P617, DOI [10.3722/CADAPS.2011.617, DOI 10.3722/CADAPS.2011.617]
   Colombo G., 2016, AUTOMATIC BELOW KNEE, P75
   Colombo G., 2016, P ASME 2016 INT DES, DOI [10.1115/DETC2016-60131, DOI 10.1115/DETC2016-60131]
   Colombo G, 2013, INTERFACE FOCUS, V3, DOI 10.1098/rsfs.2012.0082
   Dao TT, 2017, J BIOMECH ENG-T ASME, V139, DOI 10.1115/1.4035483
   Faustimi M.C, 2004, THESIS U TEXAS AUSTI, P122
   Frillici FS, 2008, INTELLIGENT PRODUCTI, P612
   Gao X, 2019, J R SOC INTERFACE, V16, DOI 10.1098/rsif.2019.0259
   Gholizadeh H, 2012, CLIN BIOMECH, V27, P34, DOI 10.1016/j.clinbiomech.2011.07.004
   Golec K, 2020, VISUAL COMPUT, V36, P809, DOI 10.1007/s00371-019-01663-0
   Goulette F, 2015, COMPUT METHOD APPL M, V295, P18, DOI 10.1016/j.cma.2015.06.015
   Grosland NM, 2009, COMPUT METH PROG BIO, V94, P96, DOI 10.1016/j.cmpb.2008.12.003
   Hsu E, 2013, J PAIN RES, V6, P121, DOI 10.2147/JPR.S32299
   Hsu LH, 2010, PROSTHET ORTHOT INT, V34, P37, DOI 10.3109/03093640902911820
   Igarashi Y, 2009, COMPUT GRAPH FORUM, V28, P1965, DOI 10.1111/j.1467-8659.2009.01575.x
   Kahle JT, 2013, J REHABIL RES DEV, V50, P1241, DOI 10.1682/JRRD.2013.01.0003
   Kot BCW, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0044348
   Lee WC, 2005, ARCH PHYS MED REHAB, V86, P641, DOI 10.1016/j.apmr.2004.08.005
   Li SJ, 2019, REV SCI INSTRUM, V90, DOI 10.1063/1.5092743
   Liu TT, 2017, ACM T GRAPHIC, V36, DOI 10.1145/2990496
   Loper M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818013
   Macklin, 2015, NVIDIA PHYSX RES, DOI [10.2312/egt.20151045.t3, DOI 10.2312/EGT.20151045.T3]
   McGimpsey G., 2010, LIMB PROSTHETICS SER
   McNeely WA, 1999, COMP GRAPH, P401, DOI 10.1145/311535.311600
   Megone W, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-018-24671-x
   Nayak C, 2016, BIOMED ENG-APP BAS C, V28, DOI 10.4015/S1016237216500228
   NAYLOR PFD, 1955, BRIT J DERMATOL, V67, P239, DOI 10.1111/j.1365-2133.1955.tb12729.x
   Nguyen TN, 2020, APPL BIONICS BIOMECH, V2020, DOI 10.1155/2020/5039329
   OBERG K, 1989, J PROSTHET ORTHOT, V1, P139
   Ogawa A, 2008, IEEE ENG MED BIO, P330, DOI 10.1109/IEMBS.2008.4649157
   Paternò L, 2018, IEEE T BIO-MED ENG, V65, P1996, DOI 10.1109/TBME.2017.2775100
   Pezzin LE, 2004, ARCH PHYS MED REHAB, V85, P723, DOI 10.1016/j.apmr.2003.06.002
   Puleo DA, 1999, BIOMATERIALS, V20, P2311, DOI 10.1016/S0142-9612(99)00160-X
   Rogers BM., 2008, J Prosthet Orthot, V20, P1, DOI DOI 10.1097/JPO.0B013E31815EA839
   Safari MR, 2015, ASSIST TECHNOL, V27, P24, DOI 10.1080/10400435.2014.949016
   Sanders JE, 2016, MED ENG PHYS, V38, P801, DOI 10.1016/j.medengphy.2016.04.019
   SAUNDERS CG, 1985, PROSTHET ORTHOT INT, V9, P17
   Sengeh David Moinina, 2013, JPO Journal of Prosthetics and Orthotics, V25, P129, DOI [DOI 10.1097/JPO.0B013E31829BE19C, 10.1097/JPO.0b013-31829be19c, 10.1097/jpo.0b013-31829be19c, DOI 10.1097/JPO.0B013-31829BE19C]
   Singh Divya, 2016, Applied Mechanics and Materials, V852, P558, DOI 10.4028/www.scientific.net/AMM.852.558
   Steer JW, 2020, BIOMECH MODEL MECHAN, V19, P1347, DOI 10.1007/s10237-019-01258-7
   Steer JW, 2020, BIOMECH MODEL MECHAN, V19, P1331, DOI 10.1007/s10237-019-01195-5
   Tang JH, 2015, MED ENG PHYS, V37, P1162, DOI 10.1016/j.medengphy.2015.10.004
   Tawhai M, 2009, IEEE ENG MED BIOL, V28, P41, DOI 10.1109/MEMB.2009.932489
   Tzeng MJ, 2015, BIOMED ENG-APP BAS C, V27, DOI 10.4015/S1016237215500441
   Urbanchek MG, 2001, J GERONTOL A-BIOL, V56, pB191, DOI 10.1093/gerona/56.5.B191
   Weber, 2014, COMPUT GRAPHICS-US, DOI [10.1016/j.cag.2014.07.004(2014), DOI 10.1016/J.CAG.2014.07.004(2014)]
   Wernke M., 2014, Quantification of Transhumeral Prosthetic Socket Residual Limb Interface Movement Using Motion Capture and a Slip Detection Sensor
   Whiteside S., 2007, Practice analysis of certified practitioners in the disciplines of orthotics and prosthetics
   Xu L, 2018, ROY SOC OPEN SCI, V5, DOI 10.1098/rsos.171587
   Zhang M, 2006, PROSTHET ORTHOT INT, V30, P25, DOI 10.1080/03093640500468074
NR 60
TC 7
Z9 7
U1 1
U2 7
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2022
VL 38
IS 3
BP 919
EP 937
DI 10.1007/s00371-021-02059-9
EA JAN 2021
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZQ8YX
UT WOS:000612278800001
DA 2024-07-18
ER

PT J
AU Selvi, TM
   Kavitha, V
AF Selvi, T. Mahesh
   Kavitha, V
TI A privacy-aware deep learning framework for health recommendation system
   on analysis of big data
SO VISUAL COMPUTER
LA English
DT Article
DE Health recommendation system; Security; Implicit and explicit data;
   Rating; Convolution
AB In recent technological advancement, the health recommendation system is gaining attention among the public to acquire health care services online. Traditional health recommendations are insecure due to the lack of security constraints caused by the intruders and not suitable to suggest appropriate recommendations. Thus, it creates hesitation in the minds of the people to share sensitive medical information. Hence, it is essential to design a privacy-preserving health recommendation system that should guarantee privacy and also suggest top-N recommendation to the user based on their preferences and earlier feedback. To cope with these issues, we propose a stacked discriminative de-noising convolution auto-encoder-decoder with a two-way recommendation scheme that provides secure and efficient health data to the end-users. In this scheme, privacy is assured to users through the modified blowfish algorithm. For structuring the big data collected from the patient, the Hadoop transform is used. Here, the two-way system analyzes and learns more effective features from the explicit and implicit information of the patient individually, and finally, all the learned features are fused to provide an efficient recommendation. The performance of the proposed system is analyzed with different statistical metrics and compared with recent approaches. From the result analysis, it is evident that the proposed system performs better than the earlier approaches.
C1 [Selvi, T. Mahesh; Kavitha, V] Univ Coll Engn, Dept Comp Sci & Engn, Kancheepuram 631552, Tamil Nadu, India.
RP Selvi, TM (corresponding author), Univ Coll Engn, Dept Comp Sci & Engn, Kancheepuram 631552, Tamil Nadu, India.
EM phd.maheshselvit.007@gmail.com
CR Babkin A, 2020, STAT PROBABIL LETT, V165, DOI 10.1016/j.spl.2020.108847
   Bao J, 2015, GEOINFORMATICA, V19, P525, DOI 10.1007/s10707-014-0220-8
   Bates DW, 2014, HEALTH AFFAIR, V33, P1123, DOI 10.1377/hlthaff.2014.0041
   Batmaz Z, 2019, ARTIF INTELL REV, V52, P1, DOI 10.1007/s10462-018-9654-y
   Chen SL, 2018, KNOWL-BASED SYST, V158, P109, DOI 10.1016/j.knosys.2018.05.040
   Dimitrov DV, 2016, HEALTHC INFORM RES, V22, P156
   Elmisery Ahmed M., 2010, IEEE 34th Annual Computer Software and Applications Conference Workshops (COMPSACW 2010), P140, DOI 10.1109/COMPSACW.2010.33
   Hu SS, 2016, IEEE TRUST BIG, P19, DOI [10.1109/TrustCom.2016.40, 10.1109/TrustCom.2016.0041]
   Huang T, 2015, BIG DATA RES, V2, P2, DOI 10.1016/j.bdr.2015.02.002
   Huh JH, 2019, J SUPERCOMPUT, V75, P1831, DOI 10.1007/s11227-018-2342-5
   Inukollu V.N., 2014, International Journal of Network Security its Applications, V6, P45, DOI DOI 10.5121/IJNSA.2014.6304
   Isinkaye FO, 2015, EGYPT INFORM J, V16, P261, DOI 10.1016/j.eij.2015.06.005
   Iwendi C, 2020, IEEE ACCESS, V8, P28462, DOI 10.1109/ACCESS.2020.2968537
   Jung, 2019, INFORM TECHNOL MANAG, V12, P1
   Kaur H, 2018, FUTURE GENER COMP SY, V86, P297, DOI 10.1016/j.future.2018.03.017
   Li XH, 2019, ISPRS J PHOTOGRAMM, V148, P103, DOI 10.1016/j.isprsjprs.2018.12.013
   Liu KH, 2019, IEEE ACCESS, V7, P2348, DOI 10.1109/ACCESS.2018.2886198
   Lu RX, 2014, IEEE NETWORK, V28, P46, DOI 10.1109/MNET.2014.6863131
   Manogaran G, 2018, FUTURE GENER COMP SY, V82, P375, DOI 10.1016/j.future.2017.10.045
   Manogaran G, 2017, STUD BIG DATA, V23, P133, DOI 10.1007/978-3-319-49736-5_7
   Moore GC, 1991, INFORM SYST RES, V2, P192, DOI 10.1287/isre.2.3.192
   Patil HK, 2014, IEEE INT CONGR BIG, P762, DOI 10.1109/BigData.Congress.2014.112
   Securosis L.L.C, 2012, SECURING BIG DATA SE
   Sengupta Sushanta, 2020, Emerging Technology in Modelling and Graphics. Proceedings of IEM Graph 2018. Advances in Intelligent Systems and Computing (AISC 937), P501, DOI 10.1007/978-981-13-7403-6_44
   Subramaniyaswamy V, 2019, J SUPERCOMPUT, V75, P3184, DOI 10.1007/s11227-018-2331-8
   Valdez AC, 2016, LECT NOTES COMPUT SC, V9605, P391, DOI 10.1007/978-3-319-50478-0_20
   Vijayarajeswari R, 2019, J MED SYST, V43, DOI 10.1007/s10916-019-1311-8
   Wang YC, 2017, J BUS RES, V70, P287, DOI 10.1016/j.jbusres.2016.08.002
   Wu S, 2016, PROC INT CONF DATA, P1218, DOI 10.1109/ICDE.2016.7498326
   Yang Y, 2019, INFORM SCIENCES, V479, P567, DOI 10.1016/j.ins.2018.02.005
   Ye Y, 2019, INT J INFORM MANAGE, V47, P65, DOI 10.1016/j.ijinfomgt.2019.01.005
   Zhang JS, 2013, 2013 10TH INTERNATIONAL CONFERENCE ON FUZZY SYSTEMS AND KNOWLEDGE DISCOVERY (FSKD), P669, DOI 10.1109/FSKD.2013.6816280
   Zhou XK, 2019, IEEE T COMPUT SOC SY, V6, P888, DOI 10.1109/TCSS.2019.2918285
   Zhuang FZ, 2017, NEURAL NETWORKS, V90, P83, DOI 10.1016/j.neunet.2017.03.009
NR 34
TC 10
Z9 10
U1 4
U2 40
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2022
VL 38
IS 2
BP 385
EP 403
DI 10.1007/s00371-020-02021-1
EA JAN 2021
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA ZE7IA
UT WOS:000608955700001
DA 2024-07-18
ER

PT J
AU Nunes, MD
   Nascimento, FM
   Miranda, GF
   Andrade, BT
AF da Silva Nunes, Mislene
   Melo Nascimento, Fernando
   Florencio Miranda Jr., Gastao
   Trinchao Andrade, Beatriz
TI Techniques for BRDF evaluation
SO VISUAL COMPUTER
LA English
DT Article
DE BRDFs; Evaluation techniques; Comparison functions; Rendered images;
   Plots
ID BIDIRECTIONAL REFLECTANCE; REPRESENTATION; ACCURATE; MODEL
AB Bidirectional reflectance distribution functions (BRDFs) describe how light interacts with a point on a surface. To propose a new BRDF formulation or to compare different reflectance representations, it is necessary to confront the results obtained with those functions against previous work or measured data. Despite the importance of using reliable techniques to evaluate a BRDF, there is a lack of works in the literature that gathers and compares those. This paper proposes a compilation of techniques used to evaluate BRDF representations along with their formal definitions. Those techniques were classified into three different groups-comparison functions, rendered images, and plots-and, to illustrate their use, three classical and widely adopted models and one state-of-the-art BRDF representation were evaluated regarding their capacity to preserve the appearance of measured materials. Based on our research regarding comparison functions, a stable and robust BRDF evaluation technique is proposed. It has been observed during both literature review and experiments that each group of techniques provides complementary information about the evaluated BRDFs, which suggests that at least one model from each category should be adopted during the choice of criteria to evaluate a BRDF.
C1 [da Silva Nunes, Mislene; Trinchao Andrade, Beatriz] Univ Fed Sergipe, Dept Comp, Sao Cristovao, Brazil.
   [Melo Nascimento, Fernando] Lumen Games, Aracaju, Brazil.
   [Florencio Miranda Jr., Gastao] Univ Fed Sergipe, Dept Math, Sao Cristovao, Brazil.
C3 Universidade Federal de Sergipe; Universidade Federal de Sergipe
RP Andrade, BT (corresponding author), Univ Fed Sergipe, Dept Comp, Sao Cristovao, Brazil.
EM mislenesn@dcomp.ufs.br; nascimento.fernandom@gmail.com;
   gastao@mat.ufs.br; beatriz@dcomp.ufs.br
OI da Silva Nunes, Mislene/0000-0002-3764-4147
CR Andrade BT, 2012, J CULT HERIT, V13, P210, DOI 10.1016/j.culher.2011.05.003
   [Anonymous], 2013, MATRIX COMPUTATIONS
   [Anonymous], 2012, Int. J. Appl. Eng. Res.
   [Anonymous], 2016, PHYS BASED RENDERING
   Ashikhmin M., 2000, Journal of Graphics Tools, V5, P25, DOI 10.1080/10867651.2000.10487522
   Bagher MM, 2012, COMPUT GRAPH FORUM, V31, P1509, DOI 10.1111/j.1467-8659.2012.03147.x
   Bilgili A, 2011, COMPUT GRAPH FORUM, V30, P2427, DOI 10.1111/j.1467-8659.2011.02072.x
   Blinn JF, 1977, P 4 ANN C COMP GRAPH, P192, DOI [10.1145/563858.563893, DOI 10.1145/563858.563893]
   Burley B., 2012, P ACM SIGGRAPH, V2012, P1
   Chandler DM, 2007, IEEE T IMAGE PROCESS, V16, P2284, DOI 10.1109/TIP.2007.901820
   Cook R. L., 1981, Computer Graphics, V15, P307, DOI 10.1145/965161.806819
   Edwards D, 2006, ACM T GRAPHIC, V25, P1, DOI 10.1145/1122501.1122502
   Ferrero A, 2013, J OPT SOC AM A, V30, P206, DOI 10.1364/JOSAA.30.000206
   Filip J, 2014, COMPUT GRAPH FORUM, V33, P91, DOI 10.1111/cgf.12477
   Filip J., 2017, P 33 SPRING C COMP G, p13:1, DOI 10.1145/3154353.3154370
   Fores A., 2009, CEIG 09
   Fores A, 2012, COLOR IMAG CONF, P142
   Ghosh A, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866163
   Ghosh S, 2007, PR IEEE COMP DESIGN, P13, DOI 10.1109/ICCD.2007.4601874
   Gonzalez R. C., 2006, PEARSON ED INDIA, V3rd
   Guarnera D, 2016, COMPUT GRAPH FORUM, V35, P625, DOI 10.1111/cgf.12867
   Havran V, 2016, COMPUT GRAPH FORUM, V35, P1, DOI 10.1111/cgf.12944
   Hsia J.J., 1976, HIGH RESOLUTION LASE, P189
   International Telecommunication Union, 2011, INT TEL UN STUD ENC
   Kreyszig E., 1991, Introductory functional analysis with applications, V17
   Lagunas M., SIMILARITY MEASURE M
   Lawrence J, 2004, ACM T GRAPHIC, V23, P496, DOI 10.1145/1015706.1015751
   Löw J, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2077341.2077350
   Marschner SR, 2000, APPL OPTICS, V39, P2592, DOI 10.1364/AO.39.002592
   Matusik W., 2003, Eurographics Symposium on Rendering. 14th Eurographics Workshop on Rendering, P241
   Matusik W, 2003, ACM T GRAPHIC, V22, P759, DOI 10.1145/882262.882343
   Miller S.J., 2006, Math. Depart. Brown Univ., V8, P1
   Nascimento FM, 2016, SIBGRAPI, P440, DOI [10.1109/SIBGRAPI.2016.62, 10.1109/SIBGRAPI.2016.067]
   Ngan A., 2005, Eurographics Symposium on Rendering, P117, DOI [DOI 10.2312/EGWR/EGSR05/117-1261,2,8, DOI 10.2312/EGWR/EGSR05/117-126]
   Ngan A., 2005, EXPT ANAL BRDF MOD S
   Ngan W.K.A., 2006, THESIS
   NICODEMUS FE, 1970, APPL OPTICS, V9, P1474, DOI 10.1364/AO.9.001474
   Nielsen JB, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818085
   Nishino K, 2011, J OPT SOC AM A, V28, P8, DOI 10.1364/JOSAA.28.000008
   Nunes MD, 2017, SIBGRAPI, P230, DOI 10.1109/SIBGRAPI.2017.37
   Ozturk A, 2008, COMPUT GRAPH-UK, V32, P149, DOI 10.1016/j.cag.2008.01.004
   Passos WD, 2009, NUMERICAL METHODS AL, V1
   Pharr M., 2010, PHYS BASED RENDERING
   Rusinkiewicz S., 1997, TECH REP
   Serrano A, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980242
   Tongbuasirilai T, 2020, VISUAL COMPUT, V36, P855, DOI 10.1007/s00371-019-01664-z
   Vanderfeesten R, 2019, LECT NOTES COMPUT SC, V11542, P349, DOI 10.1007/978-3-030-22514-8_30
   Wang Z, 2009, IEEE SIGNAL PROC MAG, V26, P98, DOI 10.1109/MSP.2008.930649
   WARD GJ, 1992, COMP GRAPH, V26, P265, DOI 10.1145/142920.134078
   Weyrich T, 2008, FOUND TRENDS COMPUT, V4, P75, DOI 10.1561/0600000022
   White DR, 1998, APPL OPTICS, V37, P3450, DOI 10.1364/AO.37.003450
   Willmott CJ, 2005, CLIMATE RES, V30, P79, DOI 10.3354/cr030079
NR 52
TC 6
Z9 6
U1 10
U2 32
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2022
VL 38
IS 2
BP 573
EP 589
DI 10.1007/s00371-020-02035-9
EA JAN 2021
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZE7IA
UT WOS:000606170200001
DA 2024-07-18
ER

PT J
AU Balavand, A
AF Balavand, Alireza
TI A new feature clustering method based on crocodiles hunting strategy
   optimization algorithm for classification of MRI images
SO VISUAL COMPUTER
LA English
DT Article
DE MRI images; Google-Net; ResNet; Crocodiles hunting strategy optimization
   algorithm (CHS); Feature clustering; Support vector machine
ID FEATURE-SELECTION; SEARCH ALGORITHM; BRAIN; EXPLORATION/EXPLOITATION;
   REDUCTION; EVOLUTION; SVM
AB In complex data with high dimensions, the dimension reduction methods are used to increase accuracy and speed in the classification algorithms. Feature clustering methods have had a good performance in the selection of important features of data due to using clustering methods. The process of selecting important features of data is a challenge in feature clustering methods which has led to the creation of different algorithms with different performances. The combination of the clustering methods and metaheuristic algorithms, especially the kind of population-based algorithms, have had good results in most cases. In this paper, a new feature clustering method is proposed which is used as a dimension reduction in the classification of brain tumors in 900 magnetic resonance images (MRI). The classification algorithm includes three main steps: in the first step, the Google-Net and ResNet-18 methods have been used for feature extraction of MRI images. Due to the creation of many features using the Google-Net and ResNet-18 methods, a new proposed feature clustering is introduced to reduce the feature dimensions in the second step. In designing the feature clustering algorithm, a new metaheuristic algorithm is introduced which is called the crocodiles hunting strategy optimization algorithm (CHS) that simulates crocodiles' behavior in hunting. Also, the feature clustering algorithm introduced the new chromosome encoding for feature clustering which is called feature clustering based on the crocodiles hunting strategy optimization algorithm (FC-CHS). Finally, in the third step, the support vector machine (SVM) algorithm is used for classification. According to the results of classification on the MRI images, the proposed algorithm has achieved high accuracy in Google-Net and ResNet features based on confusion matrices. For comparing the performance of the FC-CHS, this algorithm is compared with five well-known dimension reduction algorithms. Also, real data are used to further investigate the performance of the FC-CHS algorithm. The results show that the combination of the FC-CHS and SVM algorithms have been reached high accuracy in Iris, and Wine data, and in other real data, the proposed algorithm is outperformed compared to other dimension reduction methods in most cases.
C1 [Balavand, Alireza] Islamic Azad Univ, Sci & Res Branch, Dept Ind Engn, Tehran, Iran.
C3 Islamic Azad University
RP Balavand, A (corresponding author), Islamic Azad Univ, Sci & Res Branch, Dept Ind Engn, Tehran, Iran.
EM a.balavand@srbiau.ac.ir
RI Balavand, Alireza/AAO-7844-2021
CR Abpeykar S, 2019, COMPUT STAT DATA AN, V131, P12, DOI 10.1016/j.csda.2018.08.015
   Alba E, 2005, IEEE T EVOLUT COMPUT, V9, P126, DOI 10.1109/TEVC.2005.843751
   [Anonymous], 2005, ADV NEURAL INF PROCE
   Balavand A, 2020, ALGO INTELL SY, P51, DOI 10.1007/978-981-15-0994-0_4
   Balavand A, 2018, INT J COMPUT INT SYS, V11, P1322, DOI 10.2991/ijcis.11.1.98
   Beni G, 1993, Robots and Biological Systems: towards a New Bionics?, P703, DOI [DOI 10.1007/978-3-642-58069-738, 10.1007/978-3-642-58069-7_38]
   Berry MW, 2007, COMPUT STAT DATA AN, V52, P155, DOI 10.1016/j.csda.2006.11.006
   Bolón-Canedo V, 2019, INFORM FUSION, V52, P1, DOI 10.1016/j.inffus.2018.11.008
   Tran CT, 2018, APPL SOFT COMPUT, V73, P848, DOI 10.1016/j.asoc.2018.09.026
   Cheng Jun, 2017, Figshare
   Dai CH, 2010, J SYST ENG ELECTRON, V21, P300, DOI 10.3969/j.issn.1004-4132.2010.02.021
   Das AK, 2017, EXPERT SYST APPL, V88, P81, DOI 10.1016/j.eswa.2017.06.032
   Dash M., 1997, Intelligent Data Analysis, V1
   Daubechies I., 1992, Ten lectures on wavelets, DOI [DOI 10.1137/1.9781611970104, 10.1137/1.9781611970104]
   Dinets V, 2015, ETHOL ECOL EVOL, V27, P244, DOI 10.1080/03949370.2014.915432
   Eberhart R.C., 1995, Proc Int Symp Micro Mach Hum Sci, P39, DOI [DOI 10.1109/MHS.1995.494215, 10.1109/mhs.1995.494215]
   Eusuff M, 2006, ENG OPTIMIZ, V38, P129, DOI 10.1080/03052150500384759
   Ghimatgar H, 2018, KNOWL-BASED SYST, V159, P270, DOI 10.1016/j.knosys.2018.06.025
   GLOVER F, 1986, COMPUT OPER RES, V13, P533, DOI 10.1016/0305-0548(86)90048-1
   Goswami S, 2017, EXPERT SYST APPL, V79, P76, DOI 10.1016/j.eswa.2017.01.044
   Hansen N, 2003, EVOL COMPUT, V11, P1, DOI 10.1162/106365603321828970
   HARALICK RM, 1973, IEEE T SYST MAN CYB, VSMC3, P610, DOI 10.1109/TSMC.1973.4309314
   He K., 2016, PROC CVPR IEEE, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]
   Holland J.H., 1992, Adaptation in Natural and Artificial Systems, DOI DOI 10.7551/MITPRESS/1090.001.0001
   Hu J., P IEEE C COMPUTER VI, P7132
   HUANG G, 2017, PROC CVPR IEEE, P2261, DOI DOI 10.1109/CVPR.2017.243
   Jackson J. E., 2005, USERS GUIDE PRINCIPA
   Jain D, 2018, EGYPT INFORM J, V19, P179, DOI 10.1016/j.eij.2018.03.002
   Karaboga D., 2005, Technical Report-TR06
   KIRKPATRICK S, 1983, SCIENCE, V220, P671, DOI 10.1126/science.220.4598.671
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kumar V., 2014, SMART COMPUTING REV, V4, P211, DOI [10.6029/smartcr.2014.03.007, DOI 10.6029/SMARTCR.2014.03.007, DOI 10.1145/2740070.2626320]
   Lane M.C., 2013, AUSTR JOINT C ART IN, P214, DOI DOI 10.1007/978-3-319-03680-9_
   LeCun Y, 2010, IEEE INT SYMP CIRC S, P253, DOI 10.1109/ISCAS.2010.5537907
   Lin L, 2009, SOFT COMPUT, V13, P157, DOI 10.1007/s00500-008-0303-2
   Lu SY, 2019, J COMPUT SCI-NETH, V30, P41, DOI 10.1016/j.jocs.2018.11.008
   Mirjalili S, 2014, ADV ENG SOFTW, V69, P46, DOI 10.1016/j.advengsoft.2013.12.007
   Mladenovic N, 1997, COMPUT OPER RES, V24, P1097, DOI 10.1016/S0305-0548(97)00031-2
   Mohan G, 2018, BIOMED SIGNAL PROCES, V39, P139, DOI 10.1016/j.bspc.2017.07.007
   Nagi J., 2011, 2011 IEEE International Conference on Signal and Image Processing Applications (ICSIPA 2011), P342, DOI 10.1109/ICSIPA.2011.6144164
   Olorunda O, 2008, IEEE C EVOL COMPUTAT, P1128, DOI 10.1109/CEC.2008.4630938
   Ramakrishnan T, 2017, PATTERN RECOGN LETT, V94, P163, DOI 10.1016/j.patrec.2017.03.026
   Shahana AH, 2016, PROCEEDINGS OF IEEE INTERNATIONAL CONFERENCE ON CIRCUIT, POWER AND COMPUTING TECHNOLOGIES (ICCPCT 2016)
   Shi Y., 1999, Proceedings of the 1999 Congress on Evolutionary Computation-CEC99 (Cat. No. 99TH8406), P1945, DOI 10.1109/CEC.1999.785511
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Singh R, 2021, VISUAL COMPUT, V37, P2157, DOI 10.1007/s00371-020-01977-4
   Socha K, 2008, EUR J OPER RES, V185, P1155, DOI 10.1016/j.ejor.2006.06.046
   Srivastava RK, 2015, ADV NEUR IN, V28
   Storn R, 1997, J GLOBAL OPTIM, V11, P341, DOI 10.1023/A:1008202821328
   Szegedy C, 2014, Arxiv, DOI [arXiv:1312.6199, DOI 10.1109/CVPR.2015.7298594]
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Talo M, 2019, COGN SYST RES, V54, P176, DOI 10.1016/j.cogsys.2018.12.007
   Tipping ME, 1999, J R STAT SOC B, V61, P611, DOI 10.1111/1467-9868.00196
   Van Der Heijden F, 2005, CLASSIFICATION PARAM
   van der Maaten L. J. P., 2008, Journal of Machine Learning Research, V9, P2579, DOI DOI 10.1007/S10479-011-0841-3
   Vogado LHS, 2018, ENG APPL ARTIF INTEL, V72, P415, DOI 10.1016/j.engappai.2018.04.024
   Voudouris C, 1999, EUR J OPER RES, V113, P469, DOI 10.1016/S0377-2217(98)00099-X
   Xi PC, 2020, VISUAL COMPUT, V36, P1869, DOI 10.1007/s00371-019-01775-7
   Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634
   Xu R., 2008, Clustering, DOI DOI 10.1002/9780470382776
   Xu RF, 2015, INFORM SCIENCES, V299, P42, DOI 10.1016/j.ins.2014.12.003
   Yang XS, 2009, WOR CONG NAT BIOL, P210, DOI 10.1109/nabic.2009.5393690
   Yang XS, 2009, LECT NOTES COMPUT SC, V5792, P169, DOI 10.1007/978-3-642-04944-6_14
   Zhang, 2015, ARXIV150601195
   Zhang N, 2011, COMPUT VIS IMAGE UND, V115, P256, DOI 10.1016/j.cviu.2010.09.007
   Zhu PF, 2017, PATTERN RECOGN, V66, P364, DOI 10.1016/j.patcog.2017.01.016
NR 66
TC 4
Z9 4
U1 0
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2022
VL 38
IS 1
BP 149
EP 178
DI 10.1007/s00371-020-02009-x
EA JAN 2021
PG 30
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA YW6DE
UT WOS:000604490300006
DA 2024-07-18
ER

PT J
AU Yang, MW
   Lin, YM
   Huang, DT
   Kong, LK
AF Yang, Mengwei
   Lin, Yanming
   Huang, Detian
   Kong, Lingke
TI Accurate visual tracking via reliable patch
SO VISUAL COMPUTER
LA English
DT Article
DE Patch-based tracking; Particle filter; Correlation filter; Motion
   trajectory; Scale estimation
ID OBJECT TRACKING
AB To tackle the problem that traditional particle-filter- or correlation-filter-based trackers are prone to low tracking accuracy and poor robustness when the target faces challenges such as occlusion, rotation and scale variation in the case of complex scenes, an accurate reliable-patch-based tracker is proposed through exploiting and complementing the advantages of particle filter and correlation filter. Specifically, to cope with the challenge of continuous full occlusion, the target is divided into numerous patches by combining random with hand-crafted partition methods, and then, an effective target position estimation strategy is presented. Subsequently, according to the motion law between the patch and global target in the particle filter framework, two effective resampling rules are designed to remove unreliable particles to avoid tracking drift, and then, the target position can be estimated by the most reliable patches identified. Finally, an effective scale estimation approach is presented, in which the Manhattan distance between the reliable patches is utilized to estimate the target scale, including the target width and height, respectively. Experimental results illustrate that our tracker can not only be robust against the challenges of occlusion, rotation and scale variation, but also outperform state-of-the-art trackers for comparison in overall performance.
C1 [Yang, Mengwei; Lin, Yanming; Huang, Detian; Kong, Lingke] Huaqiao Univ, Coll Engn, Quanzhou 362021, Peoples R China.
C3 Huaqiao University
RP Huang, DT (corresponding author), Huaqiao Univ, Coll Engn, Quanzhou 362021, Peoples R China.
EM huangdetian@hqu.edu.cn
OI Detian, Huang/0000-0002-8542-3728
FU National Natural Science Foundation of China [61901183, 61976098,
   61602191]; Natural Science Foundation of Fujian Province
   [2019J01010561]; Science and Technology Bureau of Quanzhou [2017G046]
FX This work was partially supported by the National Natural Science
   Foundation of China under Grant No. 61901183, 61976098 and 61602191,
   Natural Science Foundation of Fujian Province under Grant No.
   2019J01010561 and Science and Technology Bureau of Quanzhou under Grant
   No. 2017G046.
CR Abbass MY, 2021, VISUAL COMPUT, V37, P831, DOI 10.1007/s00371-020-01833-5
   Akin O, 2016, J VIS COMMUN IMAGE R, V38, P763, DOI 10.1016/j.jvcir.2016.04.018
   [Anonymous], 2016, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2016.465
   Bao CL, 2012, PROC CVPR IEEE, P1830, DOI 10.1109/CVPR.2012.6247881
   Bertinetto L, 2016, PROC CVPR IEEE, P1401, DOI 10.1109/CVPR.2016.156
   Bertinetto L, 2016, LECT NOTES COMPUT SC, V9914, P850, DOI 10.1007/978-3-319-48881-3_56
   Bie Xiude, 2017, Journal of Xidian University, V44, P151, DOI 10.3969/j.issn.1001-2400.2017.02.026
   Bolme DS, 2010, PROC CVPR IEEE, P2544, DOI 10.1109/CVPR.2010.5539960
   Chen BY, 2019, PATTERN RECOGN, V87, P80, DOI 10.1016/j.patcog.2018.10.005
   [种衍文 Chong Yanwen], 2016, [武汉大学学报. 信息科学版, Geomatics and Information Science of Wuhan University], V41, P598
   Dai MN, 2020, NEUROCOMPUTING, V398, P235, DOI 10.1016/j.neucom.2020.02.095
   Danelljan M., 2014, BRIT MACH VIS C, P1, DOI DOI 10.5244/C.28.65
   Danelljan M, 2020, PROC CVPR IEEE, P7181, DOI 10.1109/CVPR42600.2020.00721
   Fan H., 2017, ROBUST VISUAL TRACKI
   Ferri G, 2018, IEEE J OCEANIC ENG, V43, P323, DOI 10.1109/JOE.2018.2797558
   Gao SW, 2018, PROCEEDINGS OF 2018 INTERNATIONAL CONFERENCE ON ELECTRONICS AND ELECTRICAL ENGINEERING TECHNOLOGY (EEET 2018), P84, DOI 10.1145/3277453.3277471
   Guo D., 2020, IEEE C EVOL COMPUTAT, DOI [10.1109/CEC48606.2020.9185699, DOI 10.1109/cec48606.2020.9185699]
   Henriques JF, 2015, IEEE T PATTERN ANAL, V37, P583, DOI 10.1109/TPAMI.2014.2345390
   Huang DT, 2019, IEEE ACCESS, V7, P117206, DOI 10.1109/ACCESS.2019.2936551
   Huang Z., 2017, P IEEE C COMP VIS PA, P4021, DOI DOI 10.1109/CVPR.2017.510
   Ij ZJ, 2019, J VIS COMMUN IMAGE R, V64, DOI 10.1016/j.jvcir.2019.102602
   Jiri M, 2018, EUR C COMP VIS ECCV
   Li B, 2018, PROC CVPR IEEE, P8971, DOI 10.1109/CVPR.2018.00935
   Li Y, 2015, PROC CVPR IEEE, P353, DOI 10.1109/CVPR.2015.7298632
   Li Y, 2015, LECT NOTES COMPUT SC, V8926, P254, DOI 10.1007/978-3-319-16181-5_18
   Liu GG, 2018, IEEE IMAGE PROC, P3029, DOI 10.1109/ICIP.2018.8451425
   Liu HJ, 2020, VISUAL COMPUT, V36, P2105, DOI 10.1007/s00371-020-01913-6
   Liu Q, 2017, KNOWL-BASED SYST, V134, P189, DOI 10.1016/j.knosys.2017.07.032
   Liu S, 2016, PROC CVPR IEEE, P4312, DOI 10.1109/CVPR.2016.467
   Liu T.J., 2018, J ELECT COMPON INF T, V1, P17
   Liu T, 2015, PROC CVPR IEEE, P4902, DOI 10.1109/CVPR.2015.7299124
   Oh S, 2009, IEEE T AUTOMAT CONTR, V54, P481, DOI 10.1109/TAC.2009.2012975
   Qiao Liu, 2021, IEEE Transactions on Multimedia, V23, P2114, DOI 10.1109/TMM.2020.3008028
   Sangale S.P., 2017, INT C INV COMP TECHN
   Sun X, 2017, IEEE I CONF COMP VIS, P5496, DOI 10.1109/ICCV.2017.586
   Tong X, 2019, IEEE CONF COMPU INTE, DOI 10.1109/cig.2019.8848041
   Valmadre J, 2017, PROC CVPR IEEE, P5000, DOI 10.1109/CVPR.2017.531
   Voigtlaender P, 2020, PROC CVPR IEEE, P6577, DOI 10.1109/CVPR42600.2020.00661
   Wan MJ, 2019, IEEE INTERNET THINGS, V6, P9689, DOI 10.1109/JIOT.2019.2930656
   Wang QL, 2017, PROC CVPR IEEE, P6507, DOI 10.1109/CVPR.2017.689
   Wang X, 2019, IEEE T CYBERNETICS, V49, P146, DOI 10.1109/TCYB.2017.2768570
   Wu Y, 2015, IEEE T PATTERN ANAL, V37, P1834, DOI 10.1109/TPAMI.2014.2388226
   Wu Y, 2013, PROC CVPR IEEE, P2411, DOI 10.1109/CVPR.2013.312
   Xu YL, 2016, IEEE SIGNAL PROC LET, V23, P40, DOI 10.1109/LSP.2015.2479360
   Yang YH, 2017, IEEE T CYBERNETICS, V47, P485, DOI 10.1109/TCYB.2016.2519532
   Yuan D, 2020, KNOWL-BASED SYST, V194, DOI 10.1016/j.knosys.2020.105554
   Zhai YY, 2018, IEEE ACCESS, V6, P50752, DOI 10.1109/ACCESS.2018.2869766
   Zhang JM, 2014, LECT NOTES COMPUT SC, V8694, P188, DOI 10.1007/978-3-319-10599-4_13
   Zhang TZ, 2019, IEEE T PATTERN ANAL, V41, P365, DOI 10.1109/TPAMI.2018.2797062
   Zhang TZ, 2015, INT J COMPUT VISION, V111, P171, DOI 10.1007/s11263-014-0738-0
   Zhang TZ, 2012, LECT NOTES COMPUT SC, V7577, P470, DOI 10.1007/978-3-642-33783-3_34
   Zhang W, 2020, VISUAL COMPUT, V36, P2433, DOI 10.1007/s00371-020-01955-w
   Zhou LF, 2019, IEEE T AUTOM SCI ENG, V16, P1085, DOI 10.1109/TASE.2018.2867189
NR 53
TC 3
Z9 3
U1 0
U2 21
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2022
VL 38
IS 2
BP 625
EP 638
DI 10.1007/s00371-020-02038-6
EA JAN 2021
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZE7IA
UT WOS:000604490300008
DA 2024-07-18
ER

PT J
AU Chakraborty, S
   Singh, A
   Thounaojam, DM
AF Chakraborty, Saptarshi
   Singh, Alok
   Thounaojam, Dalton Meitei
TI A novel bifold-stage shot boundary detection algorithm: invariant to
   motion and illumination
SO VISUAL COMPUTER
LA English
DT Article
DE LTP; Shot boundary detection; Abrupt; Adaptive threshold; Illumination
   invariance; Object motion
ID HISTOGRAM
AB Shot boundary detection is mainly considered as a stepping stone in the broad arena of content-based video retrieval. Ample systematic investigation has been carried out in the terrain of shot boundary detection. The attainment of shot boundary detection procedures is greatly hindered due to the presence of unforeseen illumination change and motion effects in a video. This work proposes a novel bifold-stage technique to recognize abrupt transition in videos, invariant to motion, and illumination effects. In the first stage, the local ternary patterns feature is used to extract information from each frame in a video. Then, a set of novel adaptive thresholds such as gamma and beta are used to find the possible transition frames. In the confirmation stage, Lab color difference along with an adaptive threshold delta is used to extract actual transition frames. The experimental result depicts that the motion effect is also scaled down in the initial stage. The Lab color difference passed down in the second stage also handles the illumination and motion effects which are not managed in the initial stage. Experimentation is done using TRECVid 2001 and 2007 standard datasets and palpable that the proposed technique outperforms most of the contemporary shot boundary detection approaches.
C1 [Chakraborty, Saptarshi; Singh, Alok; Thounaojam, Dalton Meitei] Natl Inst Technol Silchar, Comp Vis Lab, Dept Comp Sci & Engn, Silchar, Assam, India.
C3 National Institute of Technology (NIT System); National Institute of
   Technology Silchar
RP Chakraborty, S (corresponding author), Natl Inst Technol Silchar, Comp Vis Lab, Dept Comp Sci & Engn, Silchar, Assam, India.
EM chakraborty0007@gmail.com
RI thounaojam, Dalton/AAN-8432-2020; Thounaojam, Dalton/AAO-8511-2021;
   SINGH, ALOK/AGM-6725-2022
OI Thounaojam, Dalton/0000-0002-2655-3821; SINGH, ALOK/0000-0002-2683-0542;
   Chakraborty, Saptarshi/0000-0002-8213-1615
CR Abdulhussain SH, 2018, ENTROPY-SWITZ, V20, DOI 10.3390/e20040214
   Cai PP, 2016, INT CONF SIGN PROCES, P900, DOI 10.1109/ICSP.2016.7877960
   Chakraborty S, 2019, APPL INTELL, V49, P3207, DOI 10.1007/s10489-019-01444-1
   Chen LH, 2017, CYBERNET SYST, V48, P1, DOI 10.1080/01969722.2016.1243400
   Fu QF, 2013, 2013 9TH INTERNATIONAL CONFERENCE ON COMPUTATIONAL INTELLIGENCE AND SECURITY (CIS), P219, DOI 10.1109/CIS.2013.53
   Hassanien A., 2017, ARXIV170503281 CORR
   Heng WJ, 2001, J VIS COMMUN IMAGE R, V12, P217, DOI 10.1006/jvci.2001.0457
   Heng WJ, 1999, ISCAS '99: PROCEEDINGS OF THE 1999 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS, VOL 4, P439, DOI 10.1109/ISCAS.1999.780036
   KAABNEH K, 2006, INT C INF COMM TECHN, V1, P1530
   Kanungo P., 2015, INT C MAN MACH INT, P1
   Kar T, 2017, SIGNAL IMAGE VIDEO P, V11, P1237, DOI 10.1007/s11760-017-1080-0
   Kar T, 2015, 2015 IEEE POWER, COMMUNICATION AND INFORMATION TECHNOLOGY CONFERENCE (PCITC-2015), P72, DOI 10.1109/PCITC.2015.7438097
   Krishnakumar K, 2019, VISUAL COMPUT, V36, P1
   Lan XY, 2019, IEEE T IND ELECTRON, V66, P9887, DOI 10.1109/TIE.2019.2898618
   Lan XY, 2019, IEEE ACCESS, V7, P67761, DOI 10.1109/ACCESS.2019.2916895
   Lan XY, 2018, IEEE T IMAGE PROCESS, V27, P2022, DOI 10.1109/TIP.2017.2777183
   Li JT, 2017, NEUROCOMPUTING, V266, P66, DOI 10.1016/j.neucom.2017.04.065
   Li YN, 2009, IET IMAGE PROCESS, V3, P121, DOI 10.1049/iet-ipr.2007.0193
   Liu TR, 2014, INT CONF DIGIT SIG, P541, DOI 10.1109/ICDSP.2014.6900724
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Pele O, 2010, LECT NOTES COMPUT SC, V6312, P749, DOI 10.1007/978-3-642-15552-9_54
   Priya GGL, 2014, IEEE T IMAGE PROCESS, V23, P5187, DOI 10.1109/TIP.2014.2362652
   Rashmi BS, 2016, 2016 INTERNATIONAL CONFERENCE ON ADVANCES IN COMPUTING, COMMUNICATIONS AND INFORMATICS (ICACCI), P201, DOI 10.1109/ICACCI.2016.7732047
   Sharma G, 2005, COLOR RES APPL, V30, P21, DOI 10.1002/col.20070
   Singh A, 2020, SOFTWARE PRACT EXPER, V50, P2012, DOI 10.1002/spe.2722
   Srilakshmi B, 2015, 2015 Fifth International Conference on Advances in Computing and Communications (ICACC), P439, DOI 10.1109/ICACC.2015.94
   Tan XY, 2010, IEEE T IMAGE PROCESS, V19, P1635, DOI 10.1109/TIP.2010.2042645
   Tang S., 2018, ARXIV180804234 CORR
   Thounaojam D, 2019, INT ARAB J INF TECHN, V16, P686
   Thounaojam DM, 2016, COMPUT INTEL NEUROSC, V2016, DOI 10.1155/2016/8469428
   Tong WJ, 2015, IEEE INT SYM BROADB
   WAGHMARE MSP, 2014, INT J ADV RES ELECT, V3, P1460
   Warhade K. K., 2008, 5th International Conference on Visual Information Engineering, VIE 2008, P377, DOI 10.1049/cp:20080342
   Warhade KK, 2013, SIGNAL IMAGE VIDEO P, V7, P581, DOI 10.1007/s11760-011-0262-4
   Warhade KK, 2011, SIGNAL IMAGE VIDEO P, V5, P507, DOI 10.1007/s11760-010-0163-y
   XU JW, 2016, VISUAL COMMUNICATION
   Youssef B, 2017, COMPUT VIS IMAGE UND, V161, P20, DOI 10.1016/j.cviu.2017.06.003
   Zhao Huan, 2008, 2008 International Conference on Computer Science and Software Engineering (CSSE 2008), P1124, DOI 10.1109/CSSE.2008.939
NR 38
TC 11
Z9 12
U1 0
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2022
VL 38
IS 2
BP 445
EP 456
DI 10.1007/s00371-020-02027-9
EA JAN 2021
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZE7IA
UT WOS:000604151000002
DA 2024-07-18
ER

PT J
AU Yang, X
   Liu, L
   Zhu, C
   Guo, YQ
   Zhou, DK
AF Yang, Xin
   Liu, Li
   Zhu, Chen
   Guo, Yingqing
   Zhou, Dake
TI An improved anchor neighborhood regression SR method based on low-rank
   constraint
SO VISUAL COMPUTER
LA English
DT Article
DE Super-resolution; Sparse representation; Low-rank constraint; Anchor
   neighborhood regression
ID SINGLE-IMAGE SUPERRESOLUTION; SPARSE REPRESENTATION
AB At present, the image super-resolution (SR) method based on sparse representation has the problem that the reconstruction speed and quality are difficult to be achieved simultaneously. Therefore, this paper proposes an improved anchor neighborhood regression SR algorithm based on low-rank constraint. Firstly, considering the critical role of locality in nonlinear data learning, the locally weighted regularization weight is introduced in the calculation of the projection matrix, which can constrain the projection process according to the correlation between the anchor point and the atoms in the corresponding neighborhood. Then, in the reconstruction phase, based on the assumption of low-rank between similar blocks, further constraints are made on the reconstruction blocks to obtain better reconstruction image quality. Experiments show that our method can not only reconstruct more image details but also achieve better reconstruction speed. Compared with some state-of-the-art sparse representation method, it achieves better reconstruction results in objective evaluation criteria.
C1 [Yang, Xin; Liu, Li; Zhu, Chen; Guo, Yingqing; Zhou, Dake] Nanjing Univ Aeronaut & Astronaut, Coll Automat Engn, Nanjing 210016, Peoples R China.
C3 Nanjing University of Aeronautics & Astronautics
RP Yang, X (corresponding author), Nanjing Univ Aeronaut & Astronaut, Coll Automat Engn, Nanjing 210016, Peoples R China.
EM yangxin@nuaa.edu.cn; dakzhou@nuaa.edu.cn
RI ZHU, CHEN/A-5356-2010
FU National Natural Science Foundation of China [61573182]; Fundamental
   Research Funds for the Central Universities [NS2020025]
FX This study was funded by the National Natural Science Foundation of
   China (61573182), and by the Fundamental Research Funds for the Central
   Universities (NS2020025).
CR [Anonymous], 2010, Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on, IEEE, DOI [DOI 10.1109/CVPR.2010.5540018, 10.1109/CVPR.2010.5540018]
   [Anonymous], 2009, P ADV NEUR INF PROC
   Arbeláez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161
   Bevilacqua M, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.135
   Bruckstein AM, 2009, SIAM REV, V51, P34, DOI 10.1137/060657704
   Chang H, 2004, PROC CVPR IEEE, P275, DOI 10.1109/cvpr.2004.1315043
   Chang K, 2019, SIGNAL PROCESS, V161, P36, DOI 10.1016/j.sigpro.2019.03.011
   Chang K, 2015, J VIS COMMUN IMAGE R, V33, P286, DOI 10.1016/j.jvcir.2015.09.020
   Dong C, 2014, LECT NOTES COMPUT SC, V8692, P184, DOI 10.1007/978-3-319-10593-2_13
   Dong WS, 2013, IEEE T IMAGE PROCESS, V22, P1618, DOI 10.1109/TIP.2012.2235847
   Fang LY, 2018, NEUROCOMPUTING, V273, P171, DOI 10.1016/j.neucom.2017.08.019
   Gu SH, 2014, PROC CVPR IEEE, P2862, DOI 10.1109/CVPR.2014.366
   Huang SY, 2018, IEEE T IMAGE PROCESS, V27, P2650, DOI 10.1109/TIP.2018.2809472
   Huang T.S., 1984, ADV COMPUTER VISION, V1, P317
   Jiang JJ, 2017, IEEE T MULTIMEDIA, V19, P15, DOI 10.1109/TMM.2016.2599145
   Jiang JJ, 2017, IEEE T MULTIMEDIA, V19, P27, DOI 10.1109/TMM.2016.2601020
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Li X, 2001, IEEE T IMAGE PROCESS, V10, P1521, DOI 10.1109/83.951537
   Li XS, 2018, J VIS COMMUN IMAGE R, V55, P319, DOI 10.1016/j.jvcir.2018.06.012
   Lin Z.C., 2010, 100920105055 ARXIV, V1009, P5055, DOI [DOI 10.1016/J.JSB.2012.10.010, 10.1016/j.jsb.2012.10.010]
   Liu N, 2019, OPT LASER TECHNOL, V110, P135, DOI 10.1016/j.optlastec.2018.01.043
   Lu W, 2017, NEUROCOMPUTING, V269, P180, DOI 10.1016/j.neucom.2016.12.100
   Mandal S, 2017, SIGNAL PROCESS, V132, P134, DOI 10.1016/j.sigpro.2016.09.017
   MARROQUIN J, 1987, J AM STAT ASSOC, V82, P76, DOI 10.2307/2289127
   Mathias M, 2013, IEEE IJCNN
   Moustafa MS, 2017, COMPUT ELECTR ENG, V62, P249, DOI 10.1016/j.compeleceng.2017.02.012
   Rubinstein R, 2010, P IEEE, V98, P1045, DOI 10.1109/JPROC.2010.2040551
   Schultz RR, 1996, IEEE T IMAGE PROCESS, V5, P996, DOI 10.1109/83.503915
   Shang L, 2017, NEUROCOMPUTING, V228, P37, DOI 10.1016/j.neucom.2016.09.090
   Shi J., 2016, IEEE INT C IM PROC I
   Timofte R, 2015, LECT NOTES COMPUT SC, V9006, P111, DOI 10.1007/978-3-319-16817-3_8
   Timofte R, 2014, PATTERN RECOGN LETT, V43, P127, DOI 10.1016/j.patrec.2013.08.010
   Wu X, 2018, INT J GYNECOL CANCER, V28, P868
   Yang JQ, 2008, ITESS: 2008 PROCEEDINGS OF INFORMATION TECHNOLOGY AND ENVIRONMENTAL SYSTEM SCIENCES, PT 1, P1, DOI 10.1109/CVPR.2008.4587647
   Yang JC, 2010, IEEE T IMAGE PROCESS, V19, P2861, DOI 10.1109/TIP.2010.2050625
   Yang X, 2015, OPTIK, V126, P5850, DOI 10.1016/j.ijleo.2015.08.260
   Yang X, 2015, NEUROCOMPUTING, V162, P171, DOI 10.1016/j.neucom.2015.03.055
   Zeng K, 2019, APPL INTELL, V49, P292, DOI 10.1007/s10489-018-1270-7
   Zeyde R., 2012, INT C CURV SURF, P711, DOI DOI 10.1007/978-3-642-27413-8_47
   Zhang CP, 2018, SIGNAL PROCESS-IMAGE, V67, P79, DOI 10.1016/j.image.2018.06.001
   Zhao JW, 2017, KNOWL-BASED SYST, V124, P23, DOI 10.1016/j.knosys.2017.02.029
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 42
TC 9
Z9 9
U1 0
U2 12
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2022
VL 38
IS 2
BP 405
EP 418
DI 10.1007/s00371-020-02022-0
EA DEC 2020
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZE7IA
UT WOS:000600824200003
DA 2024-07-18
ER

PT J
AU Munagala, V
   Kodati, SP
AF Munagala, Venkatesh
   Kodati, Satya Prasad
TI Enhanced holoentropy-based encoding via whale optimization for highly
   efficient video coding
SO VISUAL COMPUTER
LA English
DT Article
DE HEVC; Holoentropy; Weighting tansig function; PSNR; Whale optimization
ID SIZE DECISION ALGORITHM; INTRA PREDICTION; HEVC; REDUCTION; SELECTION;
   TEXTURE; DEPTH; MODE
AB High-efficiency video coding (HEVC), a video compression method is considered as the most capable descendant of the extensively deployed advanced VC (AVC). Compared with AVC, HEVC provides about twice the data compression ratio at the similar video quality level or considerably enhanced video quality at an equal bit rate. This paper proposes a novel enhanced holoentropy model for proficient systems for distributed VC (DVC). HEVC standard is considered as an archetypal system. The main contribution of this paper is the accomplishment of the encoding process in the HEVC system by enhanced holoentropy, which is linked with the proposed weighting tansig function. It necessitates considerable development when handling video sequences with high resolution. The pixel deviations under altering frames are grouped based on interest, and the outliers are eliminated with the aid of an enhanced entropy standard known as enhanced holoentropy. Here, the weight of tansig function is optimally tuned by whale optimization algorithm. To next of implementation, the suggested encoding scheme is compared with the conventional schemes concerning the number of compressed bits and computational time. By carrying out the encoding process, it reduces the video size with perceptually improved video quality or PSNR.
C1 [Munagala, Venkatesh; Kodati, Satya Prasad] JNTUK, Kakinada, Andhra Pradesh, India.
C3 Jawaharlal Nehru Technological University - Kakinada
RP Munagala, V (corresponding author), JNTUK, Kakinada, Andhra Pradesh, India.
EM venkatesh4research@gmail.com; prasad_kodati@yahoo.co.in
RI Munagala, Venkatesh/JBS-6377-2023
OI Munagala, Venkatesh/0009-0009-8314-8898
CR Bozorgi SM, 2019, J COMPUT DES ENG, V6, P243
   Choi YJ, 2019, ELECTRONICS-SWITZ, V8, DOI 10.3390/electronics8090993
   Chung B, 2018, INFORM PROCESS LETT, V131, P20, DOI 10.1016/j.ipl.2017.11.005
   Ding HQ, 2016, OPTIK, V127, P7155, DOI 10.1016/j.ijleo.2016.05.061
   Dutta T, 2016, J VIS COMMUN IMAGE R, V38, P29, DOI 10.1016/j.jvcir.2015.12.007
   Fernández DG, 2018, DIGIT SIGNAL PROCESS, V73, P24, DOI 10.1016/j.dsp.2017.11.001
   Fister I, 2013, SWARM EVOL COMPUT, V13, P34, DOI 10.1016/j.swevo.2013.06.001
   Goswami K, 2018, IEEE T IND ELECTRON, V65, P8861, DOI 10.1109/TIE.2018.2815941
   Goswami K, 2016, INFORM SCIENCES, V364, P72, DOI 10.1016/j.ins.2016.05.018
   Guarda AFR, 2017, SIGNAL PROCESS-IMAGE, V59, P96, DOI 10.1016/j.image.2017.02.002
   Guo JF, 2017, J VIS COMMUN IMAGE R, V43, P50, DOI 10.1016/j.jvcir.2016.12.010
   Hatamlou A, 2011, IEEE DATA MINING, P190, DOI 10.1109/DMO.2011.5976526
   Jin ZP, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P1368, DOI 10.1109/ICASSP.2018.8461356
   Kiran MS, 2015, APPL SOFT COMPUT, V26, P454, DOI 10.1016/j.asoc.2014.10.020
   Kuanar S, 2019, CIRC SYST SIGNAL PR, V38, P5081, DOI 10.1007/s00034-019-01110-4
   Kuanar S, 2018, IEEE INT CONF MULTI
   Kuanar S, 2018, PICT COD SYMP, P164, DOI 10.1109/PCS.2018.8456278
   Kumar BSS, 2018, ALEX ENG J, V57, P1, DOI 10.1016/j.aej.2016.09.003
   Lee D, 2018, SIGNAL PROCESS-IMAGE, V62, P33, DOI 10.1016/j.image.2017.12.005
   Lee JH, 2020, IEEE ACCESS, V8, P64099, DOI 10.1109/ACCESS.2020.2984012
   Lee YW, 2018, IEEE ICCE
   Lin JL, 2018, J VIS COMMUN IMAGE R, V50, P83, DOI 10.1016/j.jvcir.2017.11.003
   Lin TL, 2015, J VIS COMMUN IMAGE R, V32, P257, DOI 10.1016/j.jvcir.2015.03.008
   Liu ZY, 2016, J VIS COMMUN IMAGE R, V38, P474, DOI 10.1016/j.jvcir.2016.03.025
   Llamocca D, 2017, J PARALLEL DISTR COM, V109, P178, DOI 10.1016/j.jpdc.2017.05.017
   González-de-Suso JL, 2017, SIGNAL PROCESS-IMAGE, V56, P40, DOI 10.1016/j.image.2017.04.010
   Masera M, 2017, SIGNAL PROCESS-IMAGE, V57, P173, DOI 10.1016/j.image.2017.06.001
   McCall J, 2005, J COMPUT APPL MATH, V184, P205, DOI 10.1016/j.cam.2004.07.034
   Mercat A, 2017, J SYST ARCHITECT, V80, P56, DOI 10.1016/j.sysarc.2017.09.003
   Migallón H, 2016, ADV ENG SOFTW, V101, P37, DOI 10.1016/j.advengsoft.2016.01.020
   Mirjalili S, 2016, ADV ENG SOFTW, V95, P51, DOI 10.1016/j.advengsoft.2016.01.008
   Mohamad ZS, 2011, IEEE SYMP DIFF EVOL, P128
   Pan ZQ, 2016, J VIS COMMUN IMAGE R, V40, P516, DOI 10.1016/j.jvcir.2016.07.018
   Sarangi A, 2015, 2015 INTERNATIONAL CONFERENCE ON MICROWAVE, OPTICAL AND COMMUNICATION ENGINEERING (ICMOCE), P439, DOI 10.1109/ICMOCE.2015.7489787
   Shen LQ, 2015, SIGNAL PROCESS-IMAGE, V32, P121, DOI 10.1016/j.image.2015.01.008
   Sole J, 2012, IEEE T CIRC SYST VID, V22, P1765, DOI 10.1109/TCSVT.2012.2223055
   Sullivan GJ, 2013, IEEE J-STSP, V7, P1001, DOI 10.1109/JSTSP.2013.2283657
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Tong T, 2017, J VIS COMMUN IMAGE R, V48, P254, DOI 10.1016/j.jvcir.2017.06.014
   Wang SS, 2016, J VIS COMMUN IMAGE R, V35, P120, DOI 10.1016/j.jvcir.2015.12.005
   Wei Gu, 2019, 2019 IEEE 4th Advanced Information Technology, Electronic and Automation Control Conference (IAEAC), P1537, DOI 10.1109/IAEAC47372.2019.8997934
   Van XH, 2015, SIGNAL PROCESS-IMAGE, V33, P51, DOI 10.1016/j.image.2015.02.003
   Xu Z, 2018, SIGNAL PROCESS-IMAGE, V60, P211, DOI 10.1016/j.image.2017.09.008
   Yang R, 2017, IEEE INT CON MULTI, P817, DOI 10.1109/ICME.2017.8019299
   Zhang JH, 2017, J SOUND VIB, V389, P153, DOI 10.1016/j.jsv.2016.11.006
   Zhang QW, 2016, OPTIK, V127, P8864, DOI 10.1016/j.ijleo.2016.06.091
   Zhang QW, 2015, DIGIT SIGNAL PROCESS, V44, P37, DOI 10.1016/j.dsp.2015.06.005
NR 47
TC 5
Z9 5
U1 1
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2021
VL 37
IS 8
BP 2173
EP 2194
DI 10.1007/s00371-020-01978-3
EA OCT 2020
PG 22
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TT5FX
UT WOS:000577432700001
DA 2024-07-18
ER

PT J
AU Ladas, N
   Kaimakis, P
   Chrysanthou, Y
AF Ladas, Nikolas
   Kaimakis, Paris
   Chrysanthou, Yiorgos
TI Background segmentation in multicolored illumination environments
SO VISUAL COMPUTER
LA English
DT Article
DE Segmentation; Background modeling; Shadow detection; Visibility
   decomposition
ID SHADOW DETECTION; IMAGE; SALIENCY; VIDEO; MODEL
AB We present an algorithm for the segmentation of images into background and foreground regions. The proposed algorithm utilizes a physically based formulation of scene appearance which explicitly models the formation of shadows originating from color light sources. This formulation enables a probabilistic model to distinguish between shadows and foreground objects in challenging images. A key component of the proposed method is an algorithm for estimating the illumination arriving at the scene. We evaluate our algorithm using synthetic and real-world data and show that the proposed method performs favorably against other commonly used segmentation methods.
C1 [Ladas, Nikolas; Chrysanthou, Yiorgos] Univ Cyprus, Dept Comp Sci, Nicosia, Cyprus.
   [Kaimakis, Paris] Univ Cent Lancanshire Cyprus, Dept Comp, Larnax, Cyprus.
C3 University of Cyprus
RP Ladas, N (corresponding author), Univ Cyprus, Dept Comp Sci, Nicosia, Cyprus.
EM nladas@cs.ucy.ac.cy; pkaimakis@uclan.ac.uk; yiorgos@cs.ucy.ac.cy
CR [Anonymous], 2005, High Dynamic Range Imaging: Acquisition, Display, and Image-Based Lighting (The Morgan Kaufmann Series in Computer Graphics
   [Anonymous], 2013, P INT WORKSH AN RETR
   Barnich O, 2011, IEEE T IMAGE PROCESS, V20, P1709, DOI 10.1109/TIP.2010.2101613
   Brown M, 2007, INT J COMPUT VISION, V74, P59, DOI 10.1007/s11263-006-0002-3
   Chen LL, 2018, LECT NOTES COMPUT SC, V11211, P538, DOI 10.1007/978-3-030-01234-2_32
   Chen Z., 2020, GLOBAL CONTEXT AWARE
   Chia-Chih Chen, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P2407, DOI 10.1109/ICPR.2010.589
   Cong RM, 2019, IEEE T MULTIMEDIA, V21, P1660, DOI 10.1109/TMM.2018.2884481
   Cong RM, 2019, IEEE T IMAGE PROCESS, V28, P4819, DOI 10.1109/TIP.2019.2910377
   Cong RM, 2019, IEEE T CIRC SYST VID, V29, P2941, DOI 10.1109/TCSVT.2018.2870832
   Cong RM, 2019, IEEE T CYBERNETICS, V49, P233, DOI 10.1109/TCYB.2017.2771488
   Damelin S. B., 2018, INT J MATH MATH SCI, V2018, DOI [10.1155/2018/3950312, DOI 10.1155/2018/3950312]
   Duncan K, 2012, IET COMPUT VIS, V6, P514, DOI 10.1049/iet-cvi.2012.0032
   FUNT BV, 1992, LECT NOTES COMPUT SC, V588, P124
   Garces E, 2012, COMPUT GRAPH FORUM, V31, P1415, DOI 10.1111/j.1467-8659.2012.03137.x
   Garcia-Garcia A, 2018, APPL SOFT COMPUT, V70, P41, DOI 10.1016/j.asoc.2018.05.018
   Godbehere A. B., 2014, Controls and art, P181, DOI DOI 10.1007/978-3-319-03904-6
   Guo Lili., 2016, P IEEE C COMP VIS PA, P86
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]
   Hsieh JW, 2003, IMAGE VISION COMPUT, V21, P505, DOI 10.1016/S0262-8856(03)00030-1
   Huerta I, 2013, NEUROCOMPUTING, V100, P183, DOI 10.1016/j.neucom.2011.10.036
   Kannala J, 2006, IEEE T PATTERN ANAL, V28, P1335, DOI 10.1109/TPAMI.2006.153
   Khan SH, 2014, PROC CVPR IEEE, P1939, DOI 10.1109/CVPR.2014.249
   Ladas N, 2017, PROCEEDINGS OF THE 12TH INTERNATIONAL JOINT CONFERENCE ON COMPUTER VISION, IMAGING AND COMPUTER GRAPHICS THEORY AND APPLICATIONS (VISIGRAPP 2017), VOL 4, P517, DOI 10.5220/0006135505170525
   Leone A, 2007, PATTERN RECOGN, V40, P1222, DOI 10.1016/j.patcog.2006.09.017
   Levin A, 2008, IEEE T PATTERN ANAL, V30, P228, DOI 10.1109/TPAMI.2007.1177
   Levoy M., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P31, DOI 10.1145/237170.237199
   Li CY, 2019, IEEE T GEOSCI REMOTE, V57, P9156, DOI 10.1109/TGRS.2019.2925070
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Perazzi F, 2012, PROC CVPR IEEE, P733, DOI 10.1109/CVPR.2012.6247743
   Pharr M., 2016, Physically Based Rendering: From Theory to Implementation, V3rd ed.
   Sanin Andres, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P141, DOI 10.1109/ICPR.2010.43
   Sanin A, 2012, PATTERN RECOGN, V45, P1684, DOI 10.1016/j.patcog.2011.10.001
   Shahrian E, 2013, PROC CVPR IEEE, P636, DOI 10.1109/CVPR.2013.88
   van der Walt S, 2014, PEERJ, V2, DOI 10.7717/peerj.453
   Wang J, 2007, FOUND TRENDS COMPUT, V3, P97, DOI 10.1561/0600000019
   Yan Q, 2013, PROC CVPR IEEE, P1155, DOI 10.1109/CVPR.2013.153
   Zivkovic Z, 2004, INT C PATT RECOG, P28, DOI 10.1109/ICPR.2004.1333992
NR 38
TC 1
Z9 1
U1 0
U2 8
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2021
VL 37
IS 8
BP 2221
EP 2233
DI 10.1007/s00371-020-01981-8
EA OCT 2020
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TT5FX
UT WOS:000575725100001
DA 2024-07-18
ER

PT J
AU Zhang, YW
   Wang, JL
   Long, WF
   Liu, H
   Zhang, CM
   Chen, YZ
AF Zhang, Yu-Wei
   Wang, Jinlei
   Long, Wenfei
   Liu, Hui
   Zhang, Caiming
   Chen, Yanzhao
TI A fast solution for Chinese calligraphy relief modeling from 2D
   handwriting image
SO VISUAL COMPUTER
LA English
DT Article
DE Chinese calligraphy; Relief modeling; 3D font
ID GENERATION
AB Calligraphy occupies a distinguished position in Chinese traditional culture. For long-term preservation, many calligraphy works have been carved on stones or woods in the form of relief. In this paper, we present a novel solution that enables fast modeling of Chinese calligraphy relief from 2D handwriting image, which benefits from the advances of deep learning. We first construct a relief dataset composed of diverse types of calligraphy fonts and then design a convolutional neural network for height predictions. Through the trained network, one can quickly generate homogeneous type, inhomogeneous type or hybrid style of reliefs. The advantage over previous methods is that our method does not require parameter tuning and is fast in generating calligraphy reliefs from different resolution of inputs. A number of experiments and comparisons prove the effectiveness of our method.
C1 [Zhang, Yu-Wei; Wang, Jinlei; Long, Wenfei; Chen, Yanzhao] Qilu Univ Technol, Sch Mech & Automot Engn, Shandong Acad Sci, Jinan, Peoples R China.
   [Liu, Hui] Shandong Univ Finance & Econ, Sch Comp Sci & Technol, Jinan, Peoples R China.
   [Zhang, Caiming] Shandong Univ, Sch Comp Sci & Technol, Jinan, Peoples R China.
C3 Qilu University of Technology; Shandong University of Finance &
   Economics; Shandong University
RP Chen, YZ (corresponding author), Qilu Univ Technol, Sch Mech & Automot Engn, Shandong Acad Sci, Jinan, Peoples R China.
EM zhangyuwei_scott@126.com; 17862988600@163.com; LLLLwenfei@126.com;
   liuh_lh@sdufe.edu.cn; czhang@sdu.edu.cn; chyzh_ql@126.com
RI Cheng, Lin/KFQ-3111-2024; chen, yanzhao/GWC-1464-2022
OI Chen, Yanzhao/0000-0002-5657-413X
FU National Natural Science Foundation of China [61772293]; NSFC-Zhejiang
   Joint Fund of the Integration of Informatization and Industrialization
   [U1609218]
FX YThe authors would like to thank the anonymous reviewers for their
   careful reviews and valuable suggestions. This work was supported in
   part by the National Natural Science Foundation of China (No. 61772293),
   and the NSFC-Zhejiang Joint Fund of the Integration of Informatization
   and Industrialization (U1609218).
CR [Anonymous], 2015, JDSOFT ARTFORM
   Chollet F, 2015, KERAS
   Du H, 2016, VISUAL COMPUT, V32, P1537, DOI 10.1007/s00371-015-1138-3
   Furferi R, 2014, GRAPH MODELS, V76, P706, DOI 10.1016/j.gmod.2014.10.001
   Hudon M, 2019, LECT NOTES COMPUT SC, V11131, P246, DOI 10.1007/978-3-030-11015-4_20
   Jian M, 2019, IEEE T MULTIMED, V99, P1
   Kingma D. P., 2014, arXiv
   Kolomenkin M, 2011, PROC CVPR IEEE, P993, DOI 10.1109/CVPR.2011.5995643
   Li ZW, 2012, IEEE T VIS COMPUT GR, V18, P177, DOI 10.1109/TVCG.2011.26
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Su WC, 2018, P ACM COMPUT GRAPH, V1, DOI 10.1145/3203186
   Sykora D, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2591011
   Wang YN, 2021, VISUAL COMPUT, V37, P1467, DOI 10.1007/s00371-020-01882-w
   Yang H, 2019, VISUAL COMPUT, V35, P1627, DOI 10.1007/s00371-018-1562-2
   Zeng Q, 2014, GRAPH MODELS, V76, P140, DOI 10.1016/j.gmod.2013.10.001
   [张婷 Zhang Ting], 2014, [计算机学报, Chinese Journal of Computers], V37, P2380
   Zhang YW, 2019, COMPUT GRAPH FORUM, V38, P521, DOI 10.1111/cgf.13655
   Zhang YW, 2018, COMPUT GRAPH-UK, V70, P300, DOI 10.1016/j.cag.2017.07.022
NR 18
TC 6
Z9 6
U1 0
U2 17
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2020
VL 36
IS 10-12
BP 2241
EP 2250
DI 10.1007/s00371-020-01917-2
EA JUL 2020
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA NW1CX
UT WOS:000549695200001
DA 2024-07-18
ER

PT J
AU Evangelou, I
   Papaioannou, G
   Vardis, K
   Vasilakis, AA
AF Evangelou, Iordanis
   Papaioannou, Georgios
   Vardis, Konstantinos
   Vasilakis, Andreas A.
TI Rasterisation-based progressive photon mapping
SO VISUAL COMPUTER
LA English
DT Article
DE Photon mapping; Rasterisation; Ray tracing
AB Ray tracing on the GPU has been synergistically operating alongside rasterisation in interactive rendering engines for some time now, in order to accurately capture certain illumination effects. In the same spirit, in this paper, we propose an implementation of progressive photon mapping entirely on the rasterisation pipeline, which is agnostic to the specific GPU architecture, in order to synthesise images at interactive rates. While any GPU ray tracing architecture can be used for photon mapping, performing ray traversal in image space minimises acceleration data structure construction time and supports arbitrarily complex and fully dynamic geometry. Furthermore, this strategy maximises data structure reuse by encompassing rasterisation, ray tracing and photon gathering tasks in a single data structure. Both eye and light paths of arbitrary depth are traced on multi-view deep G-buffers, and photon flux is gathered by a properly adapted multi-view photon splatting. In contrast to previous methods exploiting rasterisation to some extent, due to our novel indirect photon splatting approach, any event combination present in photon mapping is captured. We evaluate our method using typical test scenes and scenarios for photon mapping methods and show how our approach outperforms typical GPU-based progressive photon mapping.
C1 [Evangelou, Iordanis; Papaioannou, Georgios; Vardis, Konstantinos; Vasilakis, Andreas A.] Athens Univ Econ & Business, Dept Informat, Athens, Greece.
C3 Athens University of Economics & Business
RP Evangelou, I (corresponding author), Athens Univ Econ & Business, Dept Informat, Athens, Greece.
EM iordanise@aueb.gr
RI Papaioannou, Georgios/AAH-9642-2021
OI Papaioannou, Georgios/0000-0003-4774-0746; Vasilakis, Andreas
   A./0000-0001-6895-3324; Vardis, Konstantinos/0000-0003-2282-4644
CR [Anonymous], 2014, Journal of Computer Graphics Techniques (JCGT)
   Frisvad JR, 2014, COMPUT GRAPH FORUM, V33, P252, DOI 10.1111/cgf.12347
   Guntury S, 2012, IEEE T VIS COMPUT GR, V18, P5, DOI 10.1109/TVCG.2011.46
   Hachisuka T, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409083
   Hachisuka T, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618487
   Hu W, 2014, VISUAL COMPUT, V30, P697, DOI 10.1007/s00371-014-0968-8
   Igehy H, 1999, COMP GRAPH, P179, DOI 10.1145/311535.311555
   Jensen H. W., 1996, Rendering Techniques '96. Proceedings of the Eurographics Workshop. Eurographics, P21
   Jensen HW., 2001, REALISTIC IMAGE SYNT, DOI [10.1201/9780429294907, DOI 10.1201/9780429294907]
   Kalojanov J, 2011, COMPUT GRAPH FORUM, V30, P307, DOI 10.1111/j.1467-8659.2011.01862.x
   Knaus C, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1966394.1966404
   Li S., 2012, EUR ACM SIGGRAPH S H
   Ma V.C.H., 2002, SIGGRAPH EUR WORKSH
   Mara Michael., 2013, Proc. I3D'13, I3D'13, P71, DOI DOI 10.1145/2448196.2448207
   Mara Michael, 2016, P HIGH PERF GRAPH, P87
   McGuire AL, 2009, AM J BIOETHICS, V9, P77
   McGuire M., 2017, Computer Graphics Archive
   Moreau P., 2016, High Performance Graphics, P75, DOI [10.2312/hpg.20161194, DOI 10.2312/HPG.20161194]
   Muja M, 2009, VISAPP 2009: PROCEEDINGS OF THE FOURTH INTERNATIONAL CONFERENCE ON COMPUTER VISION THEORY AND APPLICATIONS, VOL 1, P331
   Parker SG, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778803
   Purcell T., 2003, SIGGRAPHEUROGRAPHICS, P41
   Sousa T., 2011, ACM SIGGRAPH TALKS
   Sturzlinger W., 1997, Rendering Techniques '97. Proceedings of the Eurographics Workshop. Eurographics, P93
   Uludag Y., 2014, GPU Pro, V5, P149
   Vardis K., 2016, EUR ACM SIGGRAPH S H
   Vardis K, 2016, PROCEEDINGS I3D 2016: 20TH ACM SIGGRAPH SYMPOSIUM ON INTERACTIVE 3D GRAPHICS AND GAMES, P171, DOI 10.1145/2856400.2856401
   Vinkler M, 2016, COMPUT GRAPH FORUM, V35, P68, DOI 10.1111/cgf.12776
   Weiss M, 2012, COMPUT GRAPH FORUM, V31, P719, DOI 10.1111/j.1467-8659.2012.03051.x
   Yao CH, 2010, COMPUT GRAPH FORUM, V29, P1315, DOI 10.1111/j.1467-8659.2010.01727.x
NR 29
TC 1
Z9 1
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2020
VL 36
IS 10-12
BP 1993
EP 2004
DI 10.1007/s00371-020-01897-3
EA JUL 2020
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA NW1CX
UT WOS:000548484700003
DA 2024-07-18
ER

PT J
AU Latif, UK
   Shin, SY
AF Latif, Ummi Khaira
   Shin, Soo Young
TI OP-MR: the implementation of order picking based on mixed reality in a
   smart warehouse
SO VISUAL COMPUTER
LA English
DT Article
DE Order picking; Mixed reality; Route optimization
AB This paper presents a mixed-reality (MR) application called order picking with mixed reality (OP-MR) for the order-picking activities in a smart warehouse. OP-MR is a set of applications operated by an administrator through a computer server and by the staff using the HoloLens MR device. OP-MR is built to reduce the operational time of an order-picking activity by providing the shortest route to the staff. The HoloLens device displays the order-picking instructions through the MR window, renders virtual navigation, and virtually marks the positions of items. For determining the shortest distance for an order picking, the proposed OP-MR method combines two different algorithms, namely the Held-Karp algorithm in the server and A* algorithm in the client. The Held-Karp algorithm sorts the items in the pick-up list based on the nearest position. Next, the A* algorithm determines the shortest route to ensure that a user travels the shortest distance to pick all the items. To show the effectiveness of the proposed OP-MR method, OP-MR is implemented and experiments are performed. The experimental results show that OP-MR outperforms paper-based order-picking from the viewpoint of completing all the order picking.
C1 [Latif, Ummi Khaira; Shin, Soo Young] Kumoh Natl Inst Technol, Dept IT Convergence Engn, Gumi, South Korea.
C3 Kumoh National University Technology
RP Shin, SY (corresponding author), Kumoh Natl Inst Technol, Dept IT Convergence Engn, Gumi, South Korea.
EM wdragon@kumoh.ac.kr
RI Shin, Soo Young/ABG-4608-2021
OI Shin, Soo Young/0000-0002-2526-2395; Latif, Ummi
   Khaira/0000-0003-3676-5323
FU Kumoh National Institute of Technology (KIT), Gumi, Republic of Korea
   [2019-104-139]
FX This work was supported by the Kumoh National Institute of Technology
   (KIT), Gumi, Republic of Korea (No.2019-104-139).
CR [Anonymous], THESIS
   Bartholdi J.J., 2008, WAREHOUSE DISTRIBUTI
   BELLMAN R, 1962, J ACM, V9, P61, DOI 10.1145/321105.321111
   Bowman Doug, 2004, 3D user interfaces: Theory and practice
   Cui X, 2012, INT J COMPUT SCI NET, V12, P48
   Cui X, 2011, INT J COMPUT SCI NET, V11, P125
   de Koster M.B.M., 2012, WAREHOUSING GLOBAL S, P457, DOI DOI 10.1007/978-1-4471-2274-617
   Frazelle EdwardH., 2002, World-class warehousing and material handling
   Funk M, 2015, PROCEEDINGS OF THE 2015 ACM INTERNATIONAL JOINT CONFERENCE ON PERVASIVE AND UBIQUITOUS COMPUTING (UBICOMP 2015), P601, DOI 10.1145/2750858.2804268
   Gerstweiler G, 2018, COMPUTERS, V7, DOI 10.3390/computers7010005
   Gerstweiler G, 2016, SENSORS-BASEL, V16, DOI 10.3390/s16010017
   Goyal A, 2014, IJITE, V2, P13
   Guo AH, 2015, COMPUTER, V48, P16, DOI 10.1109/MC.2015.166
   HART PE, 1968, IEEE T SYST SCI CYB, VSSC4, P100, DOI 10.1109/TSSC.1968.300136
   HELD M, 1962, J SOC IND APPL MATH, V10, P196, DOI 10.1137/0110015
   Ishii Hirotake., 2010, AUGMENTED REALITY FU
   Isler CA, 2016, INT J ADV MANUF TECH, V87, P2327, DOI 10.1007/s00170-016-8625-1
   Johnson R., 2004, Interface, V21, P27
   Reif R, 2009, VISUAL COMPUT, V25, P461, DOI 10.1007/s00371-009-0348-y
   Schwerdtfeger B, 2008, INT SYM MIX AUGMENT, P91, DOI 10.1109/ISMAR.2008.4637331
   Shi-Gang C., 2012, P 16 INT C MECH TECH, P506
   SHMOYS DB, 1990, INFORM PROCESS LETT, V35, P281, DOI 10.1016/0020-0190(90)90028-V
NR 22
TC 7
Z9 7
U1 1
U2 30
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2020
VL 36
IS 7
BP 1491
EP 1500
DI 10.1007/s00371-019-01745-z
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA LO1PW
UT WOS:000533401000014
DA 2024-07-18
ER

PT J
AU He, Z
   Li, Q
   Feng, HJ
   Xu, ZH
AF He, Zhuang
   Li, Qi
   Feng, Huajun
   Xu, Zhihai
TI Fast and sub-pixel precision target tracking algorithm for intelligent
   dual-resolution camera
SO VISUAL COMPUTER
LA English
DT Article
DE Visual tracking; Optical design of instruments; Dual-resolution camera;
   Image processing; Pattern recognition; Sub-pixel precision
ID OBJECT TRACKING
AB The intelligent dual-resolution camera can provide large imaging field and high-resolution for the parts in which we are interested simultaneously. It has important applications in visual tracking. Though many trackers show great robustness on recent benchmarks, few of them make high precision and run in real-time, which is harmful to practical applications. In this paper, we propose a fast and sub-pixel precision tracker. It uses the time-shift property of Fourier transform to convert the displacement of target in space domain into the period of the response in Fourier domain. Then an improved Hough transform is introduced to measure this period, so that the displacement with sub-pixel precision can be calculated. Furthermore, a method obtaining benchmarks utilizing the intelligent dual-resolution camera is proposed to verify the high-precision of our tracker. Many experiments have been done to compare the proposed tracker with some state-of-the-art sub-pixel precision trackers, and the results have shown that the proposed tracker outperform the best tracker by 15.4% in average median distance precision, while feasible for real-time tracking with the speed faster than 80 fps. What's more, the proposed tracker have been evaluated on the dual-resolution camera, the results show that it can make the observation of targets in the high-resolution image more complete.
C1 [He, Zhuang; Li, Qi; Feng, Huajun; Xu, Zhihai] Zhejiang Univ, State Key Lab Modern Opt Instruments, Hangzhou 310027, Zhejiang, Peoples R China.
C3 Zhejiang University
RP Li, Q (corresponding author), Zhejiang Univ, State Key Lab Modern Opt Instruments, Hangzhou 310027, Zhejiang, Peoples R China.
EM liqi@zju.edu.cn
FU Ministry of Education of the People's Republic of China [6141A02022307]
FX This work was supported by joint fund Project 6141A02022307 of Ministry
   of Education of the People's Republic of China.
CR [Anonymous], 2016, ARXIV PREPRINT ARXIV
   [Anonymous], 2002, Computer Science, DOI DOI 10.1007/978-3-642-27733-7299-3
   [Anonymous], 2016, CVPR
   [Anonymous], INT C COMP VIS PATT
   Arulampalam MS, 2002, IEEE T SIGNAL PROCES, V50, P174, DOI 10.1109/78.978374
   Bertinetto L., 2016, P COMP VIS PATT REC
   BERTINETTO L, 2016, P EUR C COMP VIS
   Bousetouane F, 2013, VISUAL COMPUT, V29, P155, DOI 10.1007/s00371-012-0677-0
   Chen K, 2015, OPT REV, V22, P434, DOI 10.1007/s10043-015-0077-6
   Chen K, 2014, OPT REV, V21, P769, DOI 10.1007/s10043-014-0126-6
   Chen Z, 2020, VISUAL COMPUT, V36, P425, DOI 10.1007/s00371-019-01631-8
   Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236
   Danelljan M., 2016, CORRELATION FILTERS
   Danelljan Martin, 2014, BRIT MACH VIS C
   DUDA RO, 1972, COMMUN ACM, V15, P11, DOI 10.1145/361237.361242
   Foroosh H, 2002, IEEE T IMAGE PROCESS, V11, P188, DOI 10.1109/83.988953
   Hare S, 2011, IEEE I CONF COMP VIS, P263, DOI 10.1109/ICCV.2011.6126251
   Harris C., 1988, ALVEY VISION C, P147151
   Henriques JF, 2015, IEEE T PATTERN ANAL, V37, P583, DOI 10.1109/TPAMI.2014.2345390
   Hough P.V., 1962, U.S. Patent, Patent No. 3069654
   Ji H., 2012, P COMP VIS PATT REC
   Kim JB, 2003, PATTERN RECOGN LETT, V24, P113, DOI 10.1016/S0167-8655(02)00194-0
   Kristan M, 2016, The Visual Object Tracking VOT2016 challenge results, P777
   Kuglin C. D., 1975, IEEE INT C CYB SOC
   Kulikowsk C, 2011, P IEEE C COMP VIS PA
   Li WM, 2017, APPL OPTICS, V56, P2368, DOI 10.1364/AO.56.002368
   Li ZY, 2015, VISUAL COMPUT, V31, P1633, DOI 10.1007/s00371-014-1044-0
   Lien JM, 2010, VISUAL COMPUT, V26, P3, DOI 10.1007/s00371-009-0367-8
   Liveira M., 2009, MULTICAMERA ACTIVE P
   Lucas BD, 1981, NUTR CYCL AGROECOSYS, V83, P13
   Ma ZY, 2014, VISUAL COMPUT, V30, P1133, DOI 10.1007/s00371-013-0894-1
   Orteu JJ, 2011, EXP MECH, V51, P625, DOI 10.1007/s11340-010-9436-1
   Possegger H., 2015, P COMP VIS PATT REC
   Ross DA, 2008, INT J COMPUT VISION, V77, P125, DOI 10.1007/s11263-007-0075-7
   Rui C., 2012, P EUR C COMP VIS
   Stone HS, 2001, IEEE T GEOSCI REMOTE, V39, P2235, DOI 10.1109/36.957286
   Sugimura D, 2017, APPL OPTICS, V56, P8687, DOI 10.1364/AO.56.008687
   Ttger T.-B., 2017, P SCAND C IM AN
   Viola P., 2003, P 2001 IEEE COMP SOC
   Wang YW, 2017, VISUAL COMPUT, V33, P235, DOI 10.1007/s00371-015-1189-5
   Wang Y, 2020, VISUAL COMPUT, V36, P683, DOI 10.1007/s00371-019-01646-1
   Wang Z, 2016, VISUAL COMPUT, V32, P307, DOI 10.1007/s00371-015-1067-1
   Wu Y., 2013, P COMP VIS PATT REC
   Wu Y, 2015, IEEE T PATTERN ANAL, V37, P1834, DOI 10.1109/TPAMI.2014.2388226
   Yang M.-H., 2012, P IEEE C COMP VIS PA
   Zhang DJ, 2020, VISUAL COMPUT, V36, P509, DOI 10.1007/s00371-019-01634-5
   Zhang K, 2013, arXiv, P127
NR 47
TC 4
Z9 4
U1 0
U2 15
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2020
VL 36
IS 6
BP 1157
EP 1171
DI 10.1007/s00371-019-01724-4
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA LI0TO
UT WOS:000529199400006
DA 2024-07-18
ER

PT J
AU Nie, XY
   Hu, Y
   Shen, XK
AF Nie, Xiaoying
   Hu, Yong
   Shen, Xukun
TI Physics-preserving fluid reconstruction from monocular video coupling
   with SFS and SPH
SO VISUAL COMPUTER
LA English
DT Article
DE Fluid reconstruction; SPH; Shape from shading; Video-based
   reconstruction; Physically based simulation
ID SHAPE; FORMULATION
AB We propose a joint method to reconstruct dynamic fluid volume sequences from a monocular video. Compared with previous methods, sophisticated equipment or careful experimental setups are not required. In order to recover the surface detail and maintain its physical property, a joint reconstruction method coupled with shape from shading (SFS) and smoothed particle hydrodynamics (SPH) algorithms is proposed. SFS is the first used to recover the height field for each frame of the video, and then the height field of the first frame is further extended to a volume as the initial state of the SPH simulation. Based on the above initial data, the key idea of our method is to optimize a SPH model to simulate the fluid volume sequences conforming to the laws of physical motion, and correct the fluid volumes to refine the surface details conforming to the recovered height field by SFS. Our experimental results compare favorably to the state of the art in terms of global motion features and fluid surface details and demonstrate the performance of our approach.
C1 [Nie, Xiaoying; Hu, Yong; Shen, Xukun] Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing 100191, Peoples R China.
   [Hu, Yong; Shen, Xukun] Beihang Univ, Sch New Media Art & Design, Beijing 100191, Peoples R China.
C3 Beihang University; Beihang University
RP Shen, XK (corresponding author), Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing 100191, Peoples R China.; Shen, XK (corresponding author), Beihang Univ, Sch New Media Art & Design, Beijing 100191, Peoples R China.
EM niexy@buaa.edu.cn; huyong@buaa.edu.cn; xkshen@buaa.edu.cn
CR Akinci N, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508395
   Akinci N, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185558
   Alduán I, 2017, COMPUT GRAPH FORUM, V36, P32, DOI 10.1111/cgf.12992
   [Anonymous], 2005, IMAGE VISION COMPUTI
   [Anonymous], P 10 AS C COMP VIS A
   [Anonymous], 2016 INT C ART INT T
   [Anonymous], 2017, CVPR
   Balschbach G., 1998, Computer Vision - ECCV'98. 5th European Conference on Computer Vision. Proceedings, P170, DOI 10.1007/BFb0054740
   Band Stefan, 2018, P 14THWORKSHOP VIRTU, P55
   Becker M, 2007, SYMPOSIUM ON COMPUTER ANIMATION 2007: ACM SIGGRAPH/ EUROGRAPHICS SYMPOSIUM PROCEEDINGS, P209
   Bender J., 2015, P 14 ACM SIGGRAPH EU, P147, DOI DOI 10.1145/2786784.2786796
   Bender J, 2017, IEEE T VIS COMPUT GR, V23, P1193, DOI 10.1109/TVCG.2016.2578335
   BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791
   Bridson R., 2015, Fluid simulation for computer graphics
   CAMASSA R, 1993, PHYS REV LETT, V71, P1661, DOI 10.1103/PhysRevLett.71.1661
   Chentanez N, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964977
   Chuan Li, 2011, 2011 Conference for Visual Media Production, P109, DOI 10.1109/CVMP.2011.19
   CRYER JE, 1995, PATTERN RECOGN, V28, P1033, DOI 10.1016/0031-3203(94)00183-M
   Eckert ML, 2018, COMPUT GRAPH FORUM, V37, P47, DOI 10.1111/cgf.13511
   Enright D, 2002, ACM T GRAPHIC, V21, P736, DOI [10.1145/566570.566581, 10.1145/566570.566645]
   GINGOLD RA, 1977, MON NOT R ASTRON SOC, V181, P375, DOI 10.1093/mnras/181.3.375
   Gissler C, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3284980
   Gregson J, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601147
   He XW, 2014, ACM T GRAPHIC, V34, DOI 10.1145/2682630
   Ihmsen M., 2014, SPH FLUIDS COMPUTER
   Ihmsen M, 2014, IEEE T VIS COMPUT GR, V20, P426, DOI 10.1109/TVCG.2013.105
   Ihrke N, 2005, IEEE I CONF COMP VIS, P1055
   Kutulakos KN, 2008, INT J COMPUT VISION, V76, P13, DOI 10.1007/s11263-007-0049-9
   Li C, 2013, IEEE T VIS COMPUT GR, V19, P1242, DOI 10.1109/TVCG.2012.302
   Limtrakul S, 2010, ENG J-THAIL, V14, P41, DOI 10.4186/ej.2010.14.2.41
   LUCY LB, 1977, ASTRON J, V82, P1013, DOI 10.1086/112164
   MONAGHAN JJ, 1992, ANNU REV ASTRON ASTR, V30, P543, DOI 10.1146/annurev.aa.30.090192.002551
   Morris NJW, 2011, IEEE T PATTERN ANAL, V33, P1518, DOI 10.1109/TPAMI.2011.24
   Muller M., 2003, ACM SIGGRAPH/Eurographics Symposium on Computer Animation, P154
   MURASE H, 1992, IEEE T PATTERN ANAL, V14, P1045, DOI 10.1109/34.159906
   Okabe M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766958
   Péteri R, 2010, PATTERN RECOGN LETT, V31, P1627, DOI 10.1016/j.patrec.2010.05.009
   Qian YM, 2018, LECT NOTES COMPUT SC, V11207, P776, DOI 10.1007/978-3-030-01219-9_46
   Quan H., 2014, P 13 ACM SIGGRAPH IN, P219
   Quan HY, 2017, VISUAL COMPUT, V33, P85, DOI 10.1007/s00371-015-1154-3
   Quan HY, 2016, INT J MODEL SIMUL SC, V7, DOI 10.1142/S179396231650029X
   Quan HY, 2013, INT J MODEL SIMUL SC, V4, DOI 10.1142/S1793962313420014
   Rusinkiewicz S, 2001, THIRD INTERNATIONAL CONFERENCE ON 3-D DIGITAL IMAGING AND MODELING, PROCEEDINGS, P145, DOI 10.1109/IM.2001.924423
   Schechter H, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185557
   Sinha SN, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409112
   Solenthaler B, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531346
   Takahashi T, 2018, COMPUT GRAPH FORUM, V37, P313, DOI 10.1111/cgf.13292
   Takahashi T, 2015, COMPUT GRAPH FORUM, V34, P493, DOI 10.1111/cgf.12578
   Tan P, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239538
   TSAI PS, 1994, IMAGE VISION COMPUT, V12, P487, DOI 10.1016/0262-8856(94)90002-7
   Van der Laan W.J., 2009, P 2009 S INT 3D GRAP, P91, DOI [DOI 10.1145/1507149.1507164, 10.1145/1507149.1507164]
   Wang C, 2017, VISUAL COMPUT, V33, P1211, DOI 10.1007/s00371-016-1284-2
   Wang HM, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531396
   Wei YC, 2005, ACM T GRAPHIC, V24, P816, DOI 10.1145/1073204.1073267
   White R, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239485
   WILLIAMSON DL, 1992, J COMPUT PHYS, V102, P211, DOI 10.1016/S0021-9991(05)80016-6
   Wu W, 2017, VISUAL COMPUT, V33, P1429, DOI 10.1007/s00371-016-1289-x
   Xu WP, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3181973
   Yan X, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925897
   Yang M, 2017, VISUAL COMPUT, V33, P597, DOI 10.1007/s00371-016-1274-4
   Yu JH, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2421636.2421641
   Yu MQ, 2013, COMPUT ANIMAT VIRT W, V24, P497, DOI 10.1002/cav.1526
   Zhang R, 1999, IEEE T PATTERN ANAL, V21, P690, DOI 10.1109/34.784284
NR 63
TC 6
Z9 8
U1 0
U2 9
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2020
VL 36
IS 6
BP 1247
EP 1257
DI 10.1007/s00371-019-01735-1
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA LI0TO
UT WOS:000529199400012
DA 2024-07-18
ER

PT J
AU Shi, GY
   Gao, CY
   Wang, D
   Su, Z
AF Shi, Guangyuan
   Gao, Chengying
   Wang, Dong
   Su, Zhuo
TI Automatic 3D virtual fitting system based on skeleton driving
SO VISUAL COMPUTER
LA English
DT Article
DE Virtual fitting system; SMPLify; Mesh segmentation; Skeleton driving
   method
ID HUMAN POSE; EXTRACTION; GARMENTS; SHAPE
AB To facilitate the input and improve the practicality of the existing virtual fitting systems, we propose a fully automatic 3D virtual fitting system to fit garment onto human models with various shapes and poses using a 2D full-body image and a 3D garment model. The proposed method constructs the 3D human model from the input 2D full-body image of the user by adopting the SMPLify method. To automatically position garment models onto human models with arbitrary postures, we present a 3D mesh segmentation method based on the discrete Reeb graph to accurately segment the different parts of a garment model, and a skeleton driving method based on mean curvature flow, which automatically adjusts the posture of the garment model according to the skeleton structural difference between the human model and the garment model. In addition, for the purpose of obtaining a more natural dress effect, we further adopt interpenetration removal and physical simulation for the deformed garment model. Compared to existing automatic 3D virtual fitting systems, the experimental results, we obtained based on the Leeds Sports Pose dataset, reveal that the proposed virtual fitting system is stable and effective.
C1 [Shi, Guangyuan; Gao, Chengying; Su, Zhuo] Sun Yat Sen Univ, Sch Data & Comp Sci, Guangzhou, Peoples R China.
   [Wang, Dong] South China Agr Univ, Coll Math & Informat, Guangzhou, Peoples R China.
C3 Sun Yat Sen University; South China Agricultural University
RP Gao, CY (corresponding author), Sun Yat Sen Univ, Sch Data & Comp Sci, Guangzhou, Peoples R China.
EM mcsgcy@mail.sysu.edu.cn
RI Su, Zhuo/AAO-4506-2020
OI Su, Zhuo/0000-0002-6090-0110
FU Natural Science Foundation of Guangdong Province, China
   [2019A1515011075]; National Natural Science Foundation of China
   [61972433, 61872394]; Fundamental Research Funds for the Central
   Universities [19lgjc11]
FX This work was supported by the Natural Science Foundation of Guangdong
   Province, China (Grant No. 2019A1515011075), National Natural Science
   Foundation of China (Grant No. 61972433, 61872394) and Fundamental
   Research Funds for the Central Universities (19lgjc11).
CR Alldieck T., 2017, LECT NOTES COMPUT SC, P347, DOI DOI 10.1007/978-3-319-66709-6_28
   Anguelov D, 2005, ACM T GRAPHIC, V24, P408, DOI 10.1145/1073204.1073207
   Au OKC, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360643
   Baek SY, 2012, COMPUT AIDED DESIGN, V44, P56, DOI 10.1016/j.cad.2010.12.006
   Balan AO, 2007, IEEE I CONF COMP VIS, P1379
   Bogo F, 2016, LECT NOTES COMPUT SC, V9909, P561, DOI 10.1007/978-3-319-46454-1_34
   Brouet R, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185532
   Cai HZ, 2018, LECT NOTES COMPUT SC, V11164, P267, DOI 10.1007/978-3-030-00776-8_25
   Clegg A, 2018, SIGGRAPH ASIA'18: SIGGRAPH ASIA 2018 TECHNICAL PAPERS, DOI 10.1145/3272127.3275048
   Clegg A, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766986
   Cornea ND, 2007, IEEE T VIS COMPUT GR, V13, P530, DOI 10.1109/TVCG.2007.1002
   Dey TK, 2006, S GEOMETRY PROCESSIN, P143
   Eberly D., 2008, Geometric Tools, P2002
   Everingham M., 2010, BMVC, V2, P5
   Fuhrmann A, 2003, COMPUT GRAPH-UK, V27, P71, DOI 10.1016/S0097-8493(02)00245-5
   Grest D., 2005, P INT WORKSHOP VISIO, P665
   Guan P, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185531
   Guan P, 2009, IEEE I CONF COMP VIS, P1381, DOI 10.1109/iccv.2009.5459300
   Jain A, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866174
   Jiang LG, 2019, COMPUT AIDED DESIGN, V106, P30, DOI 10.1016/j.cad.2018.08.002
   Kanazawa A, 2018, PROC CVPR IEEE, P7122, DOI 10.1109/CVPR.2018.00744
   Kolotouros N, 2019, IEEE I CONF COMP VIS, P2252, DOI 10.1109/ICCV.2019.00234
   Kolotouros N, 2019, PROC CVPR IEEE, P4496, DOI 10.1109/CVPR.2019.00463
   Lassner C., 2017, PROC CVPR IEEE, V2, P3, DOI DOI 10.1109/CVPR.2017.500
   Lee Y, 2013, COMPUT GRAPH-UK, V37, P911, DOI 10.1016/j.cag.2013.07.005
   Li JT, 2011, COMPUT IND, V62, P693, DOI 10.1016/j.compind.2011.04.002
   Li JT, 2010, COMPUT GRAPH-UK, V34, P742, DOI 10.1016/j.cag.2010.07.008
   Li Z, 2009, INT C COMP AID DES C, P74, DOI 10.1109/CADCG.2009.5246928
   Loper M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818013
   Metaaphanon N., 2005, Proceeding of ACM SIGGRAPH, P83
   Narita F., 2015, ACM SIGGRAPH 2015 PO, P91, DOI [https://doi.org/10.1145/2787626.2792622, DOI 10.1145/2787626.2792622]
   Narita F., 2014, SIGGRAPH ASIA 2014 P, P12, DOI [https://doi.org/10.1145/2668975.2668989, DOI 10.1145/2668975.2668989]
   Omran M, 2018, INT CONF 3D VISION, P484, DOI 10.1109/3DV.2018.00062
   Pavlakos G, 2018, PROC CVPR IEEE, P459, DOI 10.1109/CVPR.2018.00055
   Pishchulin L, 2016, PROC CVPR IEEE, P4929, DOI 10.1109/CVPR.2016.533
   Pons-Moll G, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073711
   Rong Y, 2019, IEEE I CONF COMP VIS, P5339, DOI 10.1109/ICCV.2019.00544
   Seidel H.-P, 2004, 2 EUROGRAPHICS S GEO, P175, DOI DOI 10.1145/1057432.1057456
   Sigal L., 2008, ADV NEURAL INFORM PR, P1337
   Tagliasacchi A, 2012, COMPUT GRAPH FORUM, V31, P1735, DOI 10.1111/j.1467-8659.2012.03178.x
   Tan Vince, 2017, BMVC
   Tisserand Y, 2017, COMPUT ANIMAT VIRT W, V28, DOI 10.1002/cav.1770
   Varol G, 2018, LECT NOTES COMPUT SC, V11211, P20, DOI 10.1007/978-3-030-01234-2_2
   Volino P, 2005, COMPUT AIDED DESIGN, V37, P593, DOI 10.1016/j.cad.2004.09.003
   von Marcard T, 2017, COMPUT GRAPH FORUM, V36, P349, DOI 10.1111/cgf.13131
   Wang CCL, 2005, COMPUT AIDED DESIGN, V37, P83, DOI 10.1016/j.cad.2004.05.001
   Wang YS, 2008, IEEE T VIS COMPUT GR, V14, P926, DOI 10.1109/TVCG.2008.38
   Weiss A, 2011, IEEE I CONF COMP VIS, P1951, DOI 10.1109/ICCV.2011.6126465
   Werghi N, 2006, IEEE T SYST MAN CY B, V36, P153, DOI 10.1109/TSMCB.2005.854503
   Wu NN, 2018, COMPUT ANIMAT VIRT W, V29, DOI 10.1002/cav.1811
   Xu WP, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3181973
NR 51
TC 5
Z9 7
U1 3
U2 25
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2021
VL 37
IS 5
BP 1075
EP 1088
DI 10.1007/s00371-020-01853-1
EA MAY 2020
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RV2QF
UT WOS:000535381400002
DA 2024-07-18
ER

PT J
AU Lambers, M
AF Lambers, M.
TI Survey of cube mapping methods in interactive computer graphics
SO VISUAL COMPUTER
LA English
DT Article
DE Cube maps; Environment maps; Distortion
AB The standard cube mapping technique implemented in graphics pipelines, while useful in many scenarios, has significant shortcomings for important application areas in interactive computer graphics, e.g., dynamic environment mapping, omnidirectional shadow maps, or planetary-scale terrain rendering. Many alternative mapping methods have been proposed over the years with the purpose of reducing area and/or angular distortions. In this paper, we give an overview of methods suitable for interactive applications and analyze their properties. Furthermore, we evaluate a set of additional transformation functions and identify a simple new method with favorable distortion properties.
C1 [Lambers, M.] Univ Siegen, Comp Graph Grp, Siegen 57076, Germany.
C3 Universitat Siegen
RP Lambers, M (corresponding author), Univ Siegen, Comp Graph Grp, Siegen 57076, Germany.
EM martin.lambers@uni-siegen.de
OI Lambers, Martin/0000-0002-0994-3318
CR [Anonymous], 275 EPRF CSC
   [Anonymous], VARIOUS MAPPING SPHE
   [Anonymous], MAPPING CUBE SPHERE
   [Anonymous], SHADER X4 ADV RENDER
   [Anonymous], PROJECTION REPOSITOR
   [Anonymous], P INT C CENTR EUR CO
   [Anonymous], J COMPUT GRAPH TECH
   [Anonymous], P INT C COMP GRAPH V
   [Anonymous], OUTERRA
   Bitterli B, 2015, COMPUT GRAPH FORUM, V34, P13, DOI 10.1111/cgf.12674
   Calabretta MR, 2007, MON NOT R ASTRON SOC, V381, P865, DOI 10.1111/j.1365-2966.2007.12297.x
   Calabretta MR, 2002, ASTRON ASTROPHYS, V395, P1077, DOI 10.1051/0004-6361:20021327
   Chen M, 2001, SPRING EUROGRAP, P25
   Dimitrijevic AM, 2016, FACTA UNIV-SER MATH, V31, P259
   Gascuel JD, 2008, I3D 2008: SYMPOSIUM ON INTERACTIVE 3D GRAPHICS AND GAMES, PROCEEDINGS, P107
   GREENE N, 1986, IEEE COMPUT GRAPH, V6, P21, DOI 10.1109/MCG.1986.276658
   Grimm Cindy M., 2007, Journal of Graphics Tools, V12, P25
   Harrison E, 2011, 2011 INTERNATIONAL CONFERENCE ON CYBERWORLDS, P136, DOI 10.1109/CW.2011.36
   Ho TY, 2011, IEEE T VIS COMPUT GR, V17, P51, DOI 10.1109/TVCG.2009.205
   Kooima R, 2009, IEEE T VIS COMPUT GR, V15, P719, DOI 10.1109/TVCG.2009.43
   Kuiken M, 2012, HUM FACT ROAD RAIL, P1
   Lambers M., 2016, J COMPUTER GRAPHICS, V5, P1
   Lee L., 1976, Cartographica, V13, P67, DOI [10 .3138 /X687 -1574 -4325 -WM62, DOI 10.3138/X687-1574-4325-WM62]
   O'Neill E.M., 1976, 376 NEPRF CSC
   Rancic M, 1996, Q J ROY METEOR SOC, V122, P959, DOI 10.1256/smsqj.53208
   Ronchi C, 1996, J COMPUT PHYS, V124, P93, DOI 10.1006/jcph.1996.0047
   Rosca D, 2011, J COMPUT APPL MATH, V236, P1033, DOI 10.1016/j.cam.2011.07.009
   Scherzer D, 2011, COMPUT GRAPH FORUM, V30, P169, DOI 10.1111/j.1467-8659.2010.01841.x
   Snyder E, 2018, GENDER, POWER, AND REPRESENTATIONS OF CREE LAW, P28
   Snyder J., 1992, Cartographica, V29, P10, DOI [10.3138/27H7-8K88-4882-1752, DOI 10.3138/27H7-8K88-4882-1752]
   Snyder JP, 1987, Technical Report 1395, DOI DOI 10.3133/PP1395
   Wan L, 2007, IEEE T VIS COMPUT GR, V13, P720, DOI 10.1109/TVCG.2007.1020
NR 32
TC 3
Z9 3
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2020
VL 36
IS 5
BP 1043
EP 1051
DI 10.1007/s00371-019-01708-4
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA LJ3BL
UT WOS:000530043100013
DA 2024-07-18
ER

PT J
AU Zhang, HJ
   Chen, JJ
   Qiang, Y
   Zhao, JJ
   Xu, JY
   Fan, XB
   Yang, YM
   Zhang, XL
AF Zhang, Huijun
   Chen, Junjie
   Qiang, Yan
   Zhao, Juanjuan
   Xu, Jiangyang
   Fan, Xiaobo
   Yang, Yemin
   Zhang, Xiaolong
TI DART: a visual analytics system for understanding dynamic association
   rule mining
SO VISUAL COMPUTER
LA English
DT Article
DE Visual analytics; Dynamic association rule; Sensemaking; Data mining
ID INFORMATION VISUALIZATION; FREQUENT PATTERNS; SUPPORT; DESIGN
AB Dynamic rule mining can discover time-dependent association rules and provide more accurate descriptions about the relationship among items at different time periods and temporal granularities. However, users still face some challenges in analyzing and choosing reliable rules from the rules identified by algorithms, because of the large number of rules, the dynamic nature of rules across different time periods and granularities and the opacity of the relationship between rules and raw data. In this paper, we present our work on the development of DART, a visual analytics system for dynamic association rule mining, to help analysts gain a better understanding of rules and algorithms. DART allows users to explore rules at different time granularities (e.g., per hour, per day, per month, etc.) and with different time periods (e.g., daily, weekly, yearly, etc.), and to examine rules at multiple levels of detail, including investigating temporal patterns of a set of rules, comparing multiple rules, and evaluating a rule with raw data. Two case studies are used to show the functions and features of DART in analyzing business data and public safety data.
C1 [Zhang, Huijun; Chen, Junjie; Qiang, Yan; Zhao, Juanjuan; Xu, Jiangyang; Fan, Xiaobo; Yang, Yemin] Taiyuan Univ Technol, Coll Informat & Comp, Taiyuan 030600, Peoples R China.
   [Zhang, Huijun] Commun Univ Shanxi, Dept Digital Media Technol, Jinzhong 030600, Peoples R China.
   [Zhang, Xiaolong] Penn State Univ, Coll Informat Sci & Technol, University Pk, PA 16802 USA.
C3 Taiyuan University of Technology; Communication University of Shanxi;
   Pennsylvania Commonwealth System of Higher Education (PCSHE);
   Pennsylvania State University; Pennsylvania State University -
   University Park
RP Zhang, XL (corresponding author), Penn State Univ, Coll Informat Sci & Technol, University Pk, PA 16802 USA.
EM zhanghuijunsxcm@gmail.com; lzhang@ist.psu.edu
RI ZHANG, XIAOLONG/IZQ-4553-2023
OI Zhang, Huijun/0000-0002-7775-595X; Zhang, Xiaolong/0000-0002-6828-4930
FU National Natural Science Foundation of China [61572344]
FX This work was supported in part by the National Natural Science
   Foundation of China (No. 61572344).
CR Agrawal R., 1993, SIGMOD Record, V22, P207, DOI 10.1145/170036.170072
   AGRAWAL R, 1995, PROC INT CONF DATA, P3, DOI 10.1109/ICDE.1995.380415
   Ankerst M, 2001, P ACM SIGKDD WORKSH
   [Anonymous], 1975, MATH BIOSCI, DOI 10.1016/0025-5564(75)90047-4
   Appice A, 2005, LECT NOTES ARTIF INT, V3533, P448
   Bertini E., 2009, Proceedings of the ACM SIGKDD Workshop on Visual Analytics and Knowledge Discovery: Integrating Automated Analysis with Interactive Exploration, P12
   Berzal F., 2002, Intelligent Data Analysis, V6, P221
   Bettini C., 1996, Proceedings of the Fifteenth ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems. PODS 1996, P68, DOI 10.1145/237661.237680
   Bettini C, 1998, IEEE T KNOWL DATA EN, V10, P222, DOI 10.1109/69.683754
   Bing Liu, 1999, Methodologies for Knowledge Discovery and Data Mining. Third Pacific-Asia Conference, PAKDD-99. Proceedings, P380
   Brin S., 1997, SIGMOD Record, V26, P255, DOI [10.1145/253262.253327, 10.1145/253262.253325]
   Bruzzese D, 2003, J VISUAL LANG COMPUT, V14, P621, DOI 10.1016/j.jvlc.2003.06.004
   Chakravarthy S., 2003, P 2003 ACM S APPL CO, P922
   Chen W, 2017, J VISUAL LANG COMPUT, V42, P76, DOI 10.1016/j.jvlc.2017.08.007
   de Oliveira MCF, 2003, IEEE T VIS COMPUT GR, V9, P378, DOI 10.1109/TVCG.2003.1207445
   Delgado M, 2010, INT J UNCERTAIN FUZZ, V18, P87, DOI 10.1142/S0218488510006404
   Djenouri Y, 2019, PROC INT CONF DATA, P1646, DOI 10.1109/ICDE.2019.00163
   Djenouri Y, 2017, INFORM SCIENCES, V420, P1, DOI 10.1016/j.ins.2017.08.043
   El-Assady M, 2019, IEEE T VIS COMPUT GR, V25, P374, DOI 10.1109/TVCG.2018.2864769
   Endert A, 2017, COMPUT GRAPH FORUM, V36, P458, DOI 10.1111/cgf.13092
   Faloutsos C, 2012, MOR KAUF D, P243
   Geng LQ, 2006, ACM COMPUT SURV, V38, DOI 10.1145/1132960.1132963
   Guo PH, 2010, IEEE PAC VIS SYMP, P97, DOI 10.1109/PACIFICVIS.2010.5429608
   Han JW, 2004, DATA MIN KNOWL DISC, V8, P53, DOI 10.1023/B:DAMI.0000005258.31418.83
   Hoffman P, 1997, VISUALIZATION '97 - PROCEEDINGS, P437, DOI 10.1109/VISUAL.1997.663916
   Holzinger A, 2014, INTERACTIVE KNOWLEDG
   Keim DA, 2002, IEEE T VIS COMPUT GR, V8, P1, DOI 10.1109/2945.981847
   Keim D, 2008, LECT NOTES COMPUT SC, V4950, P154, DOI 10.1007/978-3-540-70956-5
   Kosters WA, 2003, LECT NOTES ARTIF INT, V2734, P284
   Li W, 1997, P 3 INT C KNOWL DISC
   Liu SX, 2014, VISUAL COMPUT, V30, P1373, DOI 10.1007/s00371-013-0892-3
   Liu Y, 2005, J METASTAB NANOCRYST, V23, P11, DOI 10.4028/www.scientific.net/JMNM.23.11
   Liu Y, 2006, INT J HUM-COMPUT INT, V21, P15, DOI 10.1207/s15327590ijhc2101_2
   Liu Y, 2012, INT CONF SIGN PROCES, P1536, DOI 10.1109/ICoSP.2012.6491866
   Mühlbacher T, 2014, IEEE T VIS COMPUT GR, V20, P1643, DOI 10.1109/TVCG.2014.2346578
   Nath B, 2013, WIRES DATA MIN KNOWL, V3, P157, DOI 10.1002/widm.1086
   ONG KH, 2002, INT WORKSH ACT MIN A
   Ozden B, 1998, PROC INT CONF DATA, P412, DOI 10.1109/ICDE.1998.655804
   Pak Chung Wong, 1999, Proceedings 1999 IEEE Symposium on Information Visualization (InfoVis'99), P120, DOI 10.1109/INFVIS.1999.801866
   Pasquier N, 1999, LECT NOTES COMPUT SC, V1540, P398
   Rong Gang, 2007, Control Theory & Applications, V24, P127
   Sedlmair M, 2013, IEEE T VIS COMPUT GR, V19, P2634, DOI 10.1109/TVCG.2013.153
   Sekhavat Y. A., 2013, INT J INTELL SCI, V3, P34
   Tansel AU, 2007, INTERNATIONAL CONFERENCE ON INFORMATION TECHNOLOGY, PROCEEDINGS, P371
   Thomas JJ, 2006, IEEE COMPUT GRAPH, V26, P10, DOI 10.1109/MCG.2006.5
   Wang YC, 2019, VISUAL COMPUT, V35, P1567, DOI 10.1007/s00371-018-1558-y
   Yang L, 2005, IEEE T KNOWL DATA EN, V17, P60, DOI 10.1109/TKDE.2005.14
   Yang L, 2003, LECT NOTES COMPUT SC, V2667, P21
   Zhao HQ, 2017, J VISUAL LANG COMPUT, V43, P42, DOI 10.1016/j.jvlc.2017.05.004
   Zhao X, 2019, IEEE T VIS COMPUT GR, V25, P407, DOI 10.1109/TVCG.2018.2864475
   Zhonglin Z, 2014, OPEN MECH ENG J, V8, P303, DOI [10.2174/1874155X01408010303, DOI 10.2174/1874155X01408010303]
NR 51
TC 1
Z9 1
U1 1
U2 17
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2021
VL 37
IS 2
BP 341
EP 357
DI 10.1007/s00371-020-01803-x
EA FEB 2020
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA QR4KJ
UT WOS:000515840200001
DA 2024-07-18
ER

PT J
AU Hammouda, G
   Sellami, D
   Hammouda, A
AF Hammouda, Ghassen
   Sellami, Dorra
   Hammouda, Atef
TI Pattern recognition based on compound complex shape-invariant Radon
   transform
SO VISUAL COMPUTER
LA English
DT Article
DE Complex shape; Pattern recognition; Decomposing; Elementary shapes;
   Primitives
ID CLASSIFICATION; 2D; IDENTIFICATION; REPRESENTATION; DESCRIPTORS;
   SIGNATURE; HISTOGRAM; NETWORK; SYSTEM; MATRIX
AB Radon transform based on complex shape detection is one of the most important challenges in the field of pattern recognition. Finding a robust transform for extracting and identifying complex shape is still an ongoing research issue. In this paper, aiming to simplify complex pattern recognition, we propose a new methodology of recognition flows. The proposed method is based on a decomposition of complex objects into elementary shapes and an evaluation of the pertinence of the generated primitives in recognizing the object. Accordingly, a series of the most pertinent primitives are selected. Then, the recognition step starts with a test on the most pertinent primitives. Our approach, called compound complex shape-invariant Radon transform, takes its importance in recognizing object class from a partial elementary scale and orientation-invariant Radon transforms, done on selected primitives. SVM classifier is then used for decision making based on obtained transforms. Validation of the proposed approach is done on the MPEG7 dataset, yielding a recognition rate of 98%.
C1 [Hammouda, Ghassen; Sellami, Dorra] Natl Engn Sch Sfax, Rd Soukra Km 4, Sfax 3038, Tunisia.
   [Hammouda, Atef] Sci Inst Tunis, 20 Rd Tolede, Tunis 2092, Tunisia.
C3 Universite de Sfax; Ecole Nationale dIngenieurs de Sfax (ENIS)
RP Hammouda, G (corresponding author), Natl Engn Sch Sfax, Rd Soukra Km 4, Sfax 3038, Tunisia.
EM gassenhammouda7@hotmail.fr
RI Sellami, Dorra/KSL-6903-2024
CR [Anonymous], 19 INT C PATT REC IC
   [Anonymous], 2007, 2007 IEEE C COMP VIS, DOI 10.1109/CVPR.2007.383018
   Arigbabu OA, 2015, VISUAL COMPUT, V31, P513, DOI 10.1007/s00371-014-0990-x
   Arivazhagan S, 2006, PATTERN RECOGN LETT, V27, P1875, DOI 10.1016/j.patrec.2006.04.013
   Barré P, 2017, ECOL INFORM, V40, P50, DOI 10.1016/j.ecoinf.2017.05.005
   Berretti S, 2011, VISUAL COMPUT, V27, P1021, DOI 10.1007/s00371-011-0611-x
   BEYLKIN G, 1987, IEEE T ACOUST SPEECH, V35, P162, DOI 10.1109/TASSP.1987.1165108
   CHOUDHURY SD, 2018, BIOSYST ENG, V170, P72, DOI DOI 10.1016/j.biosystemseng.2018.04.001
   de Oliveira AB, 2015, PATTERN RECOGN LETT, V63, P43, DOI 10.1016/j.patrec.2015.05.018
   Ebrahim Y, 2009, PATTERN RECOGN LETT, V30, P348, DOI 10.1016/j.patrec.2008.09.013
   Elouedi I, 2013, SIGNAL PROCESS, V93, P345, DOI 10.1016/j.sigpro.2012.07.031
   Escalera S, 2009, PATTERN RECOGN LETT, V30, P1424, DOI 10.1016/j.patrec.2009.08.001
   Fleury Anthony, 2009, 2009 17th European Signal Processing Conference (EUSIPCO 2009), P2047
   Gangopadhyay A, 2013, 2013 IEEE SECOND INTERNATIONAL CONFERENCE ON IMAGE INFORMATION PROCESSING (ICIIP), P635, DOI 10.1109/ICIIP.2013.6707672
   Gopalan R, 2010, LECT NOTES COMPUT SC, V6313, P286
   Hammouda Ghassen, 2017, INTELLIGENT DECISION, P13
   Hasegawa M., 2012, P 27 ANN ACM S APPL, P777
   Hasegawa M, 2016, NEUROCOMPUTING, V173, P24, DOI 10.1016/j.neucom.2015.04.100
   Hasegawa M, 2014, PATTERN RECOGN, V47, P643, DOI 10.1016/j.patcog.2013.07.024
   Hasegawa M, 2011, PROC INT CONF DOC, P182, DOI 10.1109/ICDAR.2011.45
   Hentati Jihen, 2011, Pattern Recognition. Proceedings Third Mexican Conference, MCPR 2011, P136, DOI 10.1007/978-3-642-21587-2_15
   Hong BW, 2015, IEEE T PATTERN ANAL, V37, P151, DOI 10.1109/TPAMI.2014.2342215
   Hu RX, 2012, IEEE T IMAGE PROCESS, V21, P4667, DOI 10.1109/TIP.2012.2207391
   Ines E., 2014, 4 INT C IM PROC THEO, P1
   Jafri R, 2014, VISUAL COMPUT, V30, P1197, DOI 10.1007/s00371-013-0886-1
   Kaothanthong N, 2016, PATTERN RECOGN LETT, V78, P14, DOI 10.1016/j.patrec.2016.03.029
   Kumar N, 2012, LECT NOTES COMPUT SC, V7573, P502, DOI 10.1007/978-3-642-33709-3_36
   Latecki LJ, 2000, PROC CVPR IEEE, P424, DOI 10.1109/CVPR.2000.855850
   Lei YJ, 2014, PATTERN RECOGN, V47, P509, DOI 10.1016/j.patcog.2013.07.018
   Ling HB, 2010, LECT NOTES COMPUT SC, V6313, P411
   Chau AL, 2014, FUTURE GENER COMP SY, V36, P57, DOI 10.1016/j.future.2013.06.021
   Luengo Hendriks CL, 2005, PATTERN RECOGN, V38, P2494, DOI 10.1016/j.patcog.2005.04.018
   Nasreddine K, 2010, PATTERN RECOGN LETT, V31, P1650, DOI 10.1016/j.patrec.2010.05.014
   Nguyen QK, 2013, PROC INT CONF ADV, P404, DOI 10.1109/ATC.2013.6698145
   Ren Z, 2013, IEEE T PATTERN ANAL, V35, P2546, DOI 10.1109/TPAMI.2013.67
   Rong WB, 2014, 2014 IEEE INTERNATIONAL CONFERENCE ON MECHATRONICS AND AUTOMATION (IEEE ICMA 2014), P577, DOI 10.1109/ICMA.2014.6885761
   Roopkumar R, 2006, ROCKY MT J MATH, V36, P1375, DOI 10.1216/rmjm/1181069418
   Sathish D, 2019, VISUAL COMPUT, V35, P57, DOI 10.1007/s00371-017-1447-9
   Shah MP, 2017, IEEE IMAGE PROC, P860, DOI 10.1109/ICIP.2017.8296403
   Shang J, 2017, VISUAL COMPUT, V33, P221, DOI 10.1007/s00371-015-1179-7
   Shekar BH, 2015, PERCEPTION AND MACHINE INTELLIGENCE, 2015, P46, DOI 10.1145/2708463.2709062
   Sirin Y, 2017, MULTIMED TOOLS APPL, V76, P7823, DOI 10.1007/s11042-016-3422-2
   Suarez J, 2017, VISUAL COMPUT, V33, P1319, DOI 10.1007/s00371-016-1222-3
   Toft PA, 1996, INT CONF ACOUST SPEE, P2219, DOI 10.1109/ICASSP.1996.545862
   Vishwakarma S, 2013, VISUAL COMPUT, V29, P983, DOI 10.1007/s00371-012-0752-6
   Wang JW, 2012, PATTERN RECOGN LETT, V33, P134, DOI 10.1016/j.patrec.2011.09.042
   Wang ZZ, 2010, IEEE SIGNAL PROC LET, V17, P803, DOI 10.1109/LSP.2010.2057506
   Wu H, 2017, VISUAL COMPUT, V33, P113, DOI 10.1007/s00371-015-1156-1
   Wu SG, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND INFORMATION TECHNOLOGY, VOLS 1-3, P120
   Xiao B, 2012, PATTERN RECOGN, V45, P314, DOI 10.1016/j.patcog.2011.06.017
   Xu CJ, 2009, IEEE T PATTERN ANAL, V31, P180, DOI 10.1109/TPAMI.2008.199
   Xu Q, 2014, IEEE T IMAGE PROCESS, V23, P2944, DOI 10.1109/TIP.2014.2311656
   Zabulis X, 2018, VISUAL COMPUT, V34, P193, DOI 10.1007/s00371-016-1326-9
   Zhang X, 2014, VISUAL COMPUT, V30, P401, DOI 10.1007/s00371-013-0864-7
   Zhao ZQ, 2015, LECT NOTES COMPUT SC, V9004, P348, DOI 10.1007/978-3-319-16808-1_24
NR 55
TC 4
Z9 4
U1 0
U2 8
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2020
VL 36
IS 2
BP 279
EP 290
DI 10.1007/s00371-018-1604-9
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KJ2TH
UT WOS:000511910300005
DA 2024-07-18
ER

PT J
AU Sanchez, A
   Raya, L
   Mohedano-Munoz, MA
   Rubio-Sánchez, M
AF Sanchez, Alberto
   Raya, Laura
   Mohedano-Munoz, Miguel A.
   Rubio-Sanchez, Manuel
TI Feature selection based on star coordinates plots associated with
   eigenvalue problems
SO VISUAL COMPUTER
LA English
DT Article
DE Feature selection; Eigenvalue problems; Linear projections;
   Multidimensional visualization; Star coordinates; Principal component
   analysis; Linear discriminant analysis
ID RADIAL AXES; CLASSIFICATION; VISUALIZATION; EXPLORATION; REDUCTION
AB Feature selection consists of choosing a smaller number of variables to work with when analyzing high-dimensional data sets. Recently, several visualization tools, techniques, and feature relevance measures have been developed in order to help users carry out the feature selection. Some of these approaches are based on radial axes methods, where analysts perform backward feature elimination by discarding features that have a low impact on the visualizations. Similarly, in this paper, we propose a new feature relevance measure for star coordinates plots associated with the class of linear dimensionality reduction mappings defined through the solutions of eigenvalue problems, such as linear discriminant analysis or principal component analysis. We show that the approach leads to enhanced feature subsets for class separation or variance maximization in the plots for numerous data sets of the UCI repository. Lastly, in practice, the tool allows analysts to decide which features to discard by examining their relevance and by taking into account previous domain knowledge.
C1 [Sanchez, Alberto; Mohedano-Munoz, Miguel A.; Rubio-Sanchez, Manuel] Univ Rey Juan Carlos, Madrid, Spain.
   [Sanchez, Alberto] Res Ctr Computat Simulat, Madrid, Spain.
   [Raya, Laura] U Tad, Madrid, Spain.
C3 Universidad Rey Juan Carlos
RP Sanchez, A (corresponding author), Univ Rey Juan Carlos, Madrid, Spain.; Sanchez, A (corresponding author), Res Ctr Computat Simulat, Madrid, Spain.
EM alberto.sanchez@urjc.es; laura.raya@u-tad.com; miguel.munoz@urjc.es;
   manuel.rubio@urjc.es
RI Rubio-Sánchez, Manuel/F-9856-2015; Sanchez, Alberto/H-3995-2011
OI Rubio-Sánchez, Manuel/0000-0002-8692-2553; Sanchez,
   Alberto/0000-0002-5382-6805; Raya, Laura/0000-0002-1331-790X; Munoz
   Mohedano, Miguel Angel/0000-0001-6114-4460
FU Spanish Ministry of Economy [RTI2018-098694-B-I00]
FX This work has been supported by the Spanish Ministry of Economy (Grant
   RTI2018-098694-B-I00). The authors would like to thank Diego Rojo for
   constructive criticism of the manuscript.
CR Albuquerque G., 2010, 2010 Proceedings of IEEE Symposium on Visual Analytics Science and Technology (VAST 2010), P19, DOI 10.1109/VAST.2010.5652433
   [Anonymous], 2015, EUROVIS WORKSH VIS A
   Baumgartner C, 2004, FOURTH IEEE INTERNATIONAL CONFERENCE ON DATA MINING, PROCEEDINGS, P11, DOI 10.1109/ICDM.2004.10112
   Bertini E, 2011, IEEE T VIS COMPUT GR, V17, P2203, DOI 10.1109/TVCG.2011.229
   Blum AL, 1997, ARTIF INTELL, V97, P245, DOI 10.1016/S0004-3702(97)00063-5
   Chandrashekar G, 2014, COMPUT ELECTR ENG, V40, P16, DOI 10.1016/j.compeleceng.2013.11.024
   Chegini M, 2018, COMPUT GRAPH FORUM, V37, P99, DOI 10.1111/cgf.13404
   Chen Bin, 1997, Journal of Computer Science and Technology (English Language Edition), V12, P145, DOI 10.1007/BF02951333
   Diehl S, 2010, IEEE T VIS COMPUT GR, V16, P935, DOI 10.1109/TVCG.2010.209
   Draper GM, 2009, IEEE T VIS COMPUT GR, V15, P759, DOI 10.1109/TVCG.2009.23
   Dua D., 2017, UCI MACHINE LEARNING
   Guo D., 2003, Information Visualization, V2, P232, DOI 10.1057/palgrave.ivs.9500053
   Guyon Isabelle, 2003, J MACH LEARN RES, V3, P1157, DOI DOI 10.1162/153244303322753616
   He X., 2003, ADV NEURAL INFORM PR, P153
   He XF, 2005, IEEE I CONF COMP VIS, P1208
   HUBER PJ, 1985, ANN STAT, V13, P435, DOI 10.1214/aos/1176349519
   Hyvärinen A, 2001, INDEPENDENT COMPONENT ANALYSIS: PRINCIPLES AND PRACTICE, P71
   Ingram P, 2010, COMPARATIVE ADMINISTRATIVE CHANGE AND REFORM: LESSONS LEARNED, P3
   Jaegul Choo, 2010, 2010 Proceedings of IEEE Symposium on Visual Analytics Science and Technology (VAST 2010), P27, DOI 10.1109/VAST.2010.5652443
   Jänicke H, 2010, COMPUT GRAPH FORUM, V29, P1183, DOI 10.1111/j.1467-8659.2009.01667.x
   Johansson S, 2009, IEEE T VIS COMPUT GR, V15, P993, DOI 10.1109/TVCG.2009.153
   Jolliffe I.T., 1986, Principal component analysis, DOI DOI 10.1016/0169-7439(87)80084-9
   Kandogan E., 2000, P IEEE INFORM VISUAL, P9
   Kandogan E., 2001, P 7 ACM SIGKDD INT C, P107, DOI DOI 10.1145/502512.502530
   Kokiopoulou E, 2011, NUMER LINEAR ALGEBR, V18, P565, DOI 10.1002/nla.743
   Kotsiantis SB, 2006, ARTIF INTELL REV, V26, P159, DOI 10.1007/S10462-007-9052-3
   Krause J, 2014, IEEE T VIS COMPUT GR, V20, P1614, DOI 10.1109/TVCG.2014.2346482
   Lehmann DJ, 2013, IEEE T VIS COMPUT GR, V19, P2615, DOI 10.1109/TVCG.2013.182
   Li JD, 2018, ACM COMPUT SURV, V50, DOI 10.1145/3136625
   Liu J, 2016, 2016 IEEE INTERNATIONAL CONFERENCE ON RFID TECHNOLOGY AND APPLICATIONS (RFID-TA), P166, DOI 10.1109/RFID-TA.2016.7750759
   Markovitch S, 2002, MACH LEARN, V49, P59, DOI 10.1023/A:1014046307775
   May T., 2011, 2011 IEEE Conference on Visual Analytics Science and Technology, P111, DOI 10.1109/VAST.2011.6102448
   Mclachlan GJ., 2005, DISCRIMINANT ANAL ST
   Reris R., 2015, 14th INFORMS Computing Society Conference, P212, DOI DOI 10.1287/ICS.2015.0016
   ROUSSEEUW PJ, 1987, J COMPUT APPL MATH, V20, P53, DOI 10.1016/0377-0427(87)90125-7
   Rubio-Sánchez M, 2017, COMPUT GRAPH FORUM, V36, P389, DOI 10.1111/cgf.13196
   Rubio-Sánchez M, 2016, IEEE T VIS COMPUT GR, V22, P619, DOI 10.1109/TVCG.2015.2467324
   Rubio-Sánchez M, 2014, IEEE T VIS COMPUT GR, V20, P2013, DOI 10.1109/TVCG.2014.2346258
   Sanchez A, 2018, EXPERT SYST APPL, V100, P182, DOI 10.1016/j.eswa.2018.01.054
   Seo J., 2005, Information Visualization, V4, P96, DOI 10.1057/palgrave.ivs.9500091
   Tatu A., 2010, P INT C ADV VISUAL I, P49, DOI DOI 10.1145/1842993.1843002
   Tatu A, 2012, IEEE CONF VIS ANAL, P63, DOI 10.1109/VAST.2012.6400488
   Velloso Eduardo, 2013, P 4 AUGM HUM INT C, P116, DOI DOI 10.1145/2459236.2459256
   Wang YH, 2017, COMPUT GRAPH FORUM, V36, P401, DOI 10.1111/cgf.13197
   Yang J, 2003, INFOVIS 2002: IEEE SYMPOSIUM ON INFORMATION VISUALIZATION 2003, PROCEEDINGS, P105
   Yang J, 2003, COMPUT GRAPH-UK, V27, P265, DOI 10.1016/S0097-8493(02)00283-2
   ZUPAN J, 1994, ANAL CHIM ACTA, V292, P219, DOI 10.1016/0003-2670(94)00085-9
NR 47
TC 4
Z9 4
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2021
VL 37
IS 2
BP 203
EP 216
DI 10.1007/s00371-020-01793-w
EA JAN 2020
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA QR4KJ
UT WOS:000507708600001
DA 2024-07-18
ER

PT J
AU Kumar, N
   Sukavanam, N
AF Kumar, N.
   Sukavanam, N.
TI Weakly supervised deep network for spatiotemporal localization and
   detection of human actions in wild conditions
SO VISUAL COMPUTER
LA English
DT Article
DE Action localization; Batch normalization; Convolutional neural network;
   LSTM; Optical flow
ID ACTION RECOGNITION; ATTENTION
AB Human action localization in any long, untrimmed video can be determined from where and what action takes place in a given video segment. The main hurdles in human action localization are the spatiotemporal randomnesses of their happening in a parallel mode which means the location (a particular set of frames containing action instances) and duration of any particular action in real-life video sequences generally are not fixed. At another end, the uncontrolled conditions such as occlusions, viewpoints and motions at the crisp boundary of the action sequences demand to develop a fast deep network which can be easily trained from unlabeled samples of complex video sequences. Motivated from the facts, we proposed a weakly supervised deep network model for human action localization. The model is trained from unlabeled action samples from UCF50 action benchmark. The five-channel data obtained from the concatenation of RGB (three-channel) and optical flow vectors (two-channel) are fed to the proposed convolutional neural network. LSTM network is used to yield the region of action happening area. The performance of the model is tested on UCF-sports dataset. The observation and comparative results reflect that our model can localize any action from annotation-free data samples captured in uncontrolled conditions.
C1 [Kumar, N.; Sukavanam, N.] IIT, Dept Math, Roorkee 247667, Uttar Pradesh, India.
C3 Indian Institute of Technology System (IIT System); Indian Institute of
   Technology (IIT) - Roorkee
RP Kumar, N (corresponding author), IIT, Dept Math, Roorkee 247667, Uttar Pradesh, India.
EM naresh7iit@gmail.com; nsukvfma@iitr.ac.in
RI Kumar, Naresh/AAL-8323-2021
OI Kumar, Naresh/0000-0001-7802-0026
FU IIT Roorkee; MHRD
FX We are thankful to have joint financial support from our academic
   institute (IIT Roorkee) and MHRD, a body under the ages of Indian
   government for research associations. I would like to acknowledge with
   special thanks to Prof. R.S. Anand (Department of Electrical
   Engineering, IIT Roorkee) for providing me background motivation to
   complete this work.
CR Agahian S., 2018, VISUAL COMPUT, V35, P1
   [Anonymous], 2018, P IEEE CVF C COMP VI
   Chao YW, 2018, PROC CVPR IEEE, P1130, DOI 10.1109/CVPR.2018.00124
   Dai XY, 2017, IEEE I CONF COMP VIS, P5727, DOI 10.1109/ICCV.2017.610
   Das Dawn D, 2016, VISUAL COMPUT, V32, P289, DOI 10.1007/s00371-015-1066-2
   Dong XP, 2019, IEEE T IMAGE PROCESS, V28, P3516, DOI 10.1109/TIP.2019.2898567
   Dong XP, 2018, LECT NOTES COMPUT SC, V11217, P472, DOI 10.1007/978-3-030-01261-8_28
   Dong XW, 2018, IEEE CONF COMPUT
   Duan XH, 2018, IEEE IMAGE PROC, P918, DOI 10.1109/ICIP.2018.8451692
   Gaidon A, 2013, IEEE T PATTERN ANAL, V35, P2782, DOI 10.1109/TPAMI.2013.65
   Greff K, 2017, IEEE T NEUR NET LEAR, V28, P2222, DOI 10.1109/TNNLS.2016.2582924
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Hui TW, 2018, PROC CVPR IEEE, P8981, DOI 10.1109/CVPR.2018.00936
   Ilg E, 2017, PROC CVPR IEEE, P1647, DOI 10.1109/CVPR.2017.179
   Ioffe S, 2015, P INT C MACH LEARN, V2015, P1
   Jain M, 2014, PROC CVPR IEEE, P740, DOI 10.1109/CVPR.2014.100
   Jiang GP, 2012, PROCEEDINGS OF THE 9TH CHINA NATIONAL CONVENTION ON SPORTS SCIENCE, VOL I: SPORTS PSYCHOLOGY AND PHYSICAL HEALTH, P114
   Jiang XB, 2014, VISUAL COMPUT, V30, P1021, DOI 10.1007/s00371-014-0923-8
   Kalogeiton V, 2017, IEEE I CONF COMP VIS, P4415, DOI 10.1109/ICCV.2017.472
   Klaser A., 2010, IEEE EUROPEAN C COMP, V6553, P219
   Lan T, 2011, IEEE I CONF COMP VIS, P2003, DOI 10.1109/ICCV.2011.6126472
   Laptev I, 2007, IEEE I CONF COMP VIS, P2165
   Liu J., 2008, CVPR
   Liu KW, 2018, SIGNAL IMAGE VIDEO P, V12, P255, DOI 10.1007/s11760-017-1153-0
   Ma SG, 2013, IEEE I CONF COMP VIS, P2744, DOI 10.1109/ICCV.2013.341
   Megrhi S, 2016, J VIS COMMUN IMAGE R, V41, P375, DOI 10.1016/j.jvcir.2016.10.016
   Mettes P, 2016, LECT NOTES COMPUT SC, V9909, P437, DOI 10.1007/978-3-319-46454-1_27
   Mosabbeb EA, 2015, LECT NOTES COMPUT SC, V9007, P241, DOI 10.1007/978-3-319-16814-2_16
   Oikonomopoulos A, 2011, IEEE T IMAGE PROCESS, V20, P1126, DOI 10.1109/TIP.2010.2076821
   Oneata D, 2014, PROC CVPR IEEE, P2545, DOI 10.1109/CVPR.2014.326
   Parameswaran V, 2006, INT J COMPUT VISION, V66, P83, DOI 10.1007/s11263-005-3671-4
   Qin Y., 2019, VISUAL COMPUT, P1
   Ranjan A, 2017, PROC CVPR IEEE, P2720, DOI 10.1109/CVPR.2017.291
   Reddy KK, 2013, MACH VISION APPL, V24, P971, DOI 10.1007/s00138-012-0450-4
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Savarese S, 2008, 2008 IEEE WORKSHOP ON MOTION AND VIDEO COMPUTING, P119
   Shah M., 2017, P BMVC, P7
   Shao L, 2014, IEEE T CIRC SYST VID, V24, P504, DOI 10.1109/TCSVT.2013.2276700
   Shen JB, 2018, IEEE T IMAGE PROCESS, V27, P2688, DOI 10.1109/TIP.2018.2795740
   Shou Z, 2017, PROC CVPR IEEE, P1417, DOI 10.1109/CVPR.2017.155
   Shou Z, 2016, PROC CVPR IEEE, P1049, DOI 10.1109/CVPR.2016.119
   Singh B, 2016, PROC CVPR IEEE, P1961, DOI 10.1109/CVPR.2016.216
   Singh G, 2017, IEEE I CONF COMP VIS, P3657, DOI 10.1109/ICCV.2017.393
   Singh Krishna Kumar, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P3544, DOI 10.1109/ICCV.2017.381
   Singh VK, 2011, VISUAL COMPUT, V27, P1115, DOI 10.1007/s00371-011-0656-x
   Sivic J, 2005, IEEE I CONF COMP VIS, P370
   Soomro K., 2014, COMPUTER VISION SPOR, V71, P181, DOI [DOI 10.1007/978-3-319-09396-3_9, DOI 10.1007/978-3-319-09396-39]
   Soomro K, 2019, IEEE T PATTERN ANAL, V41, P459, DOI 10.1109/TPAMI.2018.2797266
   Soomro K, 2017, IEEE I CONF COMP VIS, P696, DOI 10.1109/ICCV.2017.82
   Soomro K, 2016, PROC CVPR IEEE, P2648, DOI 10.1109/CVPR.2016.290
   Stoian A, 2016, IEEE T CIRC SYST VID, V26, P1917, DOI 10.1109/TCSVT.2015.2475835
   Sultani W, 2017, COMPUT VIS IMAGE UND, V161, P77, DOI 10.1016/j.cviu.2017.05.005
   Sultani W, 2016, PROC CVPR IEEE, P1077, DOI 10.1109/CVPR.2016.122
   Tian YC, 2013, PROC CVPR IEEE, P2642, DOI 10.1109/CVPR.2013.341
   Tu ZG, 2018, PATTERN RECOGN, V79, P32, DOI 10.1016/j.patcog.2018.01.020
   van Gemert Jan C., 2015, BMVC, V2, P4
   Vishwakarma S, 2013, VISUAL COMPUT, V29, P983, DOI 10.1007/s00371-012-0752-6
   Wang WG, 2018, IEEE T IMAGE PROCESS, V27, P38, DOI 10.1109/TIP.2017.2754941
   Wang WG, 2019, IEEE T PATTERN ANAL, V41, P1531, DOI 10.1109/TPAMI.2018.2840724
   Weinzaepfel P, 2015, IEEE I CONF COMP VIS, P3164, DOI 10.1109/ICCV.2015.362
   Wenguan Wang, 2018, IEEE Transactions on Image Processing, V27, P2368, DOI 10.1109/TIP.2017.2787612
   Wu JZ, 2014, VISUAL COMPUT, V30, P1395, DOI 10.1007/s00371-013-0899-9
   Yang HT, 2018, PROC CVPR IEEE, P1450, DOI 10.1109/CVPR.2018.00157
   Yi Y, 2018, VISUAL COMPUT, V34, P391, DOI 10.1007/s00371-016-1345-6
   Yuan ZH, 2017, PROC CVPR IEEE, P3215, DOI 10.1109/CVPR.2017.342
NR 65
TC 6
Z9 6
U1 0
U2 7
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2020
VL 36
IS 9
BP 1809
EP 1821
DI 10.1007/s00371-019-01777-5
EA DEC 2019
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA NA2KY
UT WOS:000500459700001
DA 2024-07-18
ER

PT J
AU Fang, LY
   Wang, J
   Lu, GD
   Zhang, DL
   Fu, JH
AF Fang, Liyang
   Wang, Jin
   Lu, Guodong
   Zhang, Dongliang
   Fu, Jianhui
TI Hand-drawn grayscale image colorful colorization based on natural image
SO VISUAL COMPUTER
LA English
DT Article
DE Hand-drawn grayscale image; Colorful colorization; Region mapping; Gabor
   filter; Highlight effect fusion
AB This paper presents a novel colorization technique for hand-drawn grayscale images, such as cartoons and sketches, based on reference natural images and simple user interactions, which can generate colorful and natural results. Firstly, to solve the problem of inputting complex color scribbles by simple interactions, we introduce a region thinning method for generating color scribbles and then map these scribbles to the corresponding regions in the hand-drawn image via region mapping method based on the scanning line. Secondly, to maintain the color fidelity, a luminance downward modification method is proposed. Next, a optimization method for colorization is proposed, where the feature vectors are adjusted by a smooth feature map, thus maintaining smooth color transitions in the smooth areas and controlling the color overflow in the texture areas. Thirdly, to fuse the colorized image with a highlight effect, a luminance upward modification method by weights, which are determined by color distance and boundary distance, is proposed. The experimental results of the proposed algorithm show the smooth color transition in the intensity-continuity areas, color overflow controlling in the texture-continuity areas and natural effect of light and shadow.
C1 [Fang, Liyang; Wang, Jin; Lu, Guodong; Fu, Jianhui] Zhejiang Univ, State Key Lab Fluid Power & Mechatron Syst, Hangzhou, Zhejiang, Peoples R China.
   [Zhang, Dongliang] Zhejiang Univ, Int Design Inst, Hangzhou, Zhejiang, Peoples R China.
C3 Zhejiang University; Zhejiang University
RP Wang, J (corresponding author), Zhejiang Univ, State Key Lab Fluid Power & Mechatron Syst, Hangzhou, Zhejiang, Peoples R China.
EM dwjcom@zju.edu.cn
FU National Natural Science Foundation of China [51775492]; China
   Postdoctoral Science Foundation [2018M630670]
FX This study is funded by the National Natural Science Foundation of China
   (51775492) and China Postdoctoral Science Foundation (2018M630670).
CR [Anonymous], 2016, Colorful image colorization
   [Anonymous], P SIGGRAPH AS 2016 T
   Arbelot B., 2016, 5 JOINT S COMPUTATIO, P21
   Casaca W, 2015, LECT NOTES COMPUT SC, V9257, P675, DOI 10.1007/978-3-319-23117-4_58
   Charpiat G, 2011, ANN STAT, V36, P1171
   Chen T., 2004, P AS C COMP VIS CIT, V6
   Ding XW, 2012, LECT NOTES COMPUT SC, V7131, P103
   Fan X, 2016, VISUAL COMPUT, V32, P1191, DOI 10.1007/s00371-015-1188-6
   Furusawa C, 2017, IGGRAPH ASIA 2017 TECHNICAL BRIEFS (SA'17), DOI 10.1145/3145749.3149430
   Gauge C., 2012, Journal of Intelligent Learning Systems and Applications, V04, P135, DOI [10.4236/jilsa.2012.42013, DOI 10.4236/JILSA.2012.42013]
   Ge YR, 1996, IEEE T PATTERN ANAL, V18, P1055, DOI 10.1109/34.544075
   IIZUKA S, 2016, ACM T GRAPHIC, V35, DOI DOI 10.1145/2897824.2925974
   Ironi R., 2005, RENDERING TECHNIQUES, P201
   Kawulok M, 2010, IEEE IMAGE PROC, P405, DOI 10.1109/ICIP.2010.5653544
   Koo HI, 2011, J ELECTRON IMAGING, V20, DOI 10.1117/1.3565470
   Levin A, 2004, ACM T GRAPHIC, V23, P689, DOI 10.1145/1015706.1015780
   Li B, 2017, IEEE T IMAGE PROCESS, V26, P5188, DOI 10.1109/TIP.2017.2732239
   Liu SG, 2013, J ELECTRON IMAGING, V22, DOI 10.1117/1.JEI.22.1.013011
   Liu SG, 2012, PATTERN RECOGN LETT, V33, P1673, DOI 10.1016/j.patrec.2012.06.001
   Luan Q., 2007, P 18 EUR C CREND TEC, P309
   Qu YG, 2006, ACM T GRAPHIC, V25, P1214, DOI 10.1145/1141911.1142017
   Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720
   Rusu C, 2013, INT SYMP IMAGE SIG, P564
   Sangkloy P, 2017, PROC CVPR IEEE, P6836, DOI 10.1109/CVPR.2017.723
   Schnitman Y, 2006, LECT NOTES COMPUT SC, V3852, P373
   Sheng B, 2011, IEEE COMPUT GRAPH, V31, P24, DOI 10.1109/MCG.2011.18
   Sykora D., 2011, Proceedings of International Symposium on Non-Photorealistic Animation and Rendering, P75
   Sykora D, 2009, COMPUT GRAPH FORUM, V28, P599, DOI 10.1111/j.1467-8659.2009.01400.x
   Wegner S., 1996, Proceedings of the 13th International Conference on Pattern Recognition, P498, DOI 10.1109/ICPR.1996.546997
   Welsh T, 2002, ACM T GRAPHIC, V21, P277, DOI 10.1145/566570.566576
   Wu EH, 2013, VISUAL COMPUT, V29, P311, DOI 10.1007/s00371-012-0734-8
   Yatziv L, 2006, IEEE T IMAGE PROCESS, V15, P1120, DOI 10.1109/TIP.2005.864231
NR 32
TC 5
Z9 5
U1 1
U2 31
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2019
VL 35
IS 11
SI SI
BP 1667
EP 1681
DI 10.1007/s00371-018-1613-8
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA JD5IZ
UT WOS:000490018000013
DA 2024-07-18
ER

PT J
AU Zhang, J
   Wang, CB
   Li, C
   Qin, H
AF Zhang, Jian
   Wang, Changbo
   Li, Chen
   Qin, Hong
TI Example-based rapid generation of vegetation on terrain via CNN-based
   distribution learning
SO VISUAL COMPUTER
LA English
DT Article
DE Vegetation modeling; Style transfer; Natural phenomena
ID MODELS
AB Modeling large-scale vegetation on terrain is an important and challenging task in computer games, movie production and other digital entertainment applications. In this work, we propose a novel example-based method for rapid generation of vegetation in outdoor natural environments. Its central idea is to learn the vegetation distribution on terrain via deep convolution neural networks. We first use a pre-trained deep neural network to extract rich local information from the terrain pertinent to vegetation distribution. Second, we produce the initial features of the target vegetation distribution based on patch matching and further introduce a network that generates a vegetation density map based on the initial features. Third, during the synthesis stage, we propose a procedural method to generate the vegetation distribution data corresponding to the terrain data. Our research work confirms that the image features extracted by the pre-trained deep neural network could be utilized to explore the connection between vegetation and terrain. We validate our new method over various outdoor scenes, including procedural generated scenes and scenes with manual control on tree patterns. The experimental results demonstrate that our method can rapidly produce new realistic scenes for outdoor natural environments, which relies on the mechanism of learning correlationship between vegetation distribution and terrain data.
C1 [Zhang, Jian; Wang, Changbo; Li, Chen] East China Normal Univ, Sch Comp Sci & Software Engn, Shanghai, Peoples R China.
   [Qin, Hong] SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11794 USA.
C3 East China Normal University; State University of New York (SUNY)
   System; State University of New York (SUNY) Stony Brook
RP Wang, CB (corresponding author), East China Normal Univ, Sch Comp Sci & Software Engn, Shanghai, Peoples R China.
EM cbwang@sei.ecnu.edu.cn
RI Li, Kexin/KAO-2519-2024
FU National Natural Science Foundation of China [61532002, 61672237];
   National Science Foundation of USA [IIS-1715985, IIS-1812606]; China
   Scholarship Council [201806140115]
FX This study was funded by National Natural Science Foundation of China
   (Nos. 61532002, 61672237), National Science Foundation of USA
   (IIS-1715985, IIS-1812606) and China Scholarship Council (No.
   201806140115).
CR Abadi Martin, 2016, TENSORFLOW LARGE SCA, V16, P265
   [Anonymous], 2017, ACM T GRAPH
   [Anonymous], 2009, State of the Art in Example-Based Texture Synthesis R
   Argudo O, 2017, VISUAL COMPUT, V33, P1005, DOI 10.1007/s00371-017-1393-6
   Benes B, 2011, COMPUT GRAPH FORUM, V30, P325, DOI 10.1111/j.1467-8659.2011.01886.x
   Benes B., 2016, ACM SIGGRAPH 2016 CO
   Benes Bedrich., 2009, Eurographics Workshop on Natural Phenomena, P9
   Chen TC, 2017, AGEING SOC, V37, P1798, DOI 10.1017/S0144686X16000623
   Chollet F., KERAS
   Cordonnier Guillaume, 2017, ACM Transactions on Graphics, V36, DOI 10.1145/3072959.3073667
   DARABI S, 2012, ACM T GRAPHIC, V31, DOI DOI 10.1145/2185520.2185578
   Deussen O., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P275, DOI 10.1145/280814.280898
   Emilien A, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766975
   Gatys L., 2015, NIPS
   Gatys L., 2016, Journal of Vision, V16, P326, DOI DOI 10.1167/16.12.326
   Guérin E, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130804
   Huang HB, 2017, IEEE T VIS COMPUT GR, V23, P2003, DOI 10.1109/TVCG.2016.2597830
   Huang X., 2017, ARXIV170306868 CORR
   Ioffe S, 2015, P INT C MACH LEARN, V2015, P1
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kingma D. P., 2014, arXiv
   Kohek S, 2018, COMPUT GRAPH FORUM, V37, P389, DOI 10.1111/cgf.13304
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lane B, 2002, PROC GRAPH INTERF, P69
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Martinovic A, 2013, PROC CVPR IEEE, P201, DOI 10.1109/CVPR.2013.33
   Mei X, 2007, PACIFIC GRAPHICS 2007: 15TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, P47, DOI 10.1109/PG.2007.15
   Nair V., 2010, P 27 INT C MACHINE L, P807
   Nishida G, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925951
   Parish YIH, 2001, COMP GRAPH, P301, DOI 10.1145/383259.383292
   Ritchie D., 2016, Advances in neural information processing systems, P622
   Ritchie D, 2018, COMPUT GRAPH FORUM, V37, P401, DOI 10.1111/cgf.13371
   Ritchie D, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766895
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Samavati F, 2016, VISUAL COMPUT, V32, P1293, DOI 10.1007/s00371-016-1227-y
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Smelik R.M., 2009, P CASA WORKSHOP 3D A, V2009, P25
   Stava O, 2014, COMPUT GRAPH FORUM, V33, P118, DOI 10.1111/cgf.12282
   Talton JO, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1944846.1944851
   Vanegas CA, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366187
   Wang K, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201362
   Wang L., 2018, VISUAL COMPUT, P1
   Wu FZ, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601162
   Yumer ME, 2015, UIST'15: PROCEEDINGS OF THE 28TH ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P109, DOI 10.1145/2807442.2807448
   Zeiler MD, 2011, IEEE I CONF COMP VIS, P2018, DOI 10.1109/ICCV.2011.6126474
   Zhu CY, 1997, ACM T MATH SOFTWARE, V23, P550, DOI 10.1145/279232.279236
NR 46
TC 9
Z9 11
U1 1
U2 14
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2019
VL 35
IS 6-8
SI SI
BP 1181
EP 1191
DI 10.1007/s00371-019-01667-w
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IC1IH
UT WOS:000470712200033
OA Bronze
DA 2024-07-18
ER

PT J
AU Zhang, ZM
   Han, C
   He, SF
   Liu, XT
   Zhu, HC
   Hu, XH
   Wong, TT
AF Zhang, Zhuming
   Han, Chu
   He, Shengfeng
   Liu, Xueting
   Zhu, Haichao
   Hu, Xinghong
   Wong, Tien-Tsin
TI Deep binocular tone mapping
SO VISUAL COMPUTER
LA English
DT Article
DE Tone mapping; Binocular tone mapping; Binocular perception;
   Convolutional neural network
ID CONTRAST; REPRODUCTION
AB Binocular tone mapping is studied in the previous works to generate a fusible pair of LDR images in order to convey more visual content than one single LDR image. However, the existing methods are all based on monocular tone mapping operators. It greatly restricts the preservation of local details and global contrast in a binocular LDR pair. In this paper, we proposed the first binocular tone mapping operator to more effectively distribute visual content to an LDR pair, leveraging the great representability and interpretability of deep convolutional neural network. Based on the existing binocular perception models, novel loss functions are also proposed to optimize the output pairs in terms of local details, global contrast, content distribution, and binocular fusibility. Our method is validated with a qualitative and quantitative evaluation, as well as a user study. Statistics show that our method outperforms the state-of-the-art binocular tone mapping frameworks in terms of both visual quality and time performance.
C1 [Zhang, Zhuming; Han, Chu; Hu, Xinghong; Wong, Tien-Tsin] Chinese Univ Hong Kong, Dept Comp Sci & Engn, Hong Kong, Peoples R China.
   [Zhang, Zhuming; Han, Chu; Hu, Xinghong; Wong, Tien-Tsin] Chinese Acad Sci, Shenzhen Inst Adv Technol, Guangdong Prov Key Lab Comp Vis & Virtual Real, Shenzhen, Peoples R China.
   [He, Shengfeng] South China Univ Technol, Sch Comp Sci & Engn, Guangzhou, Guangdong, Peoples R China.
   [Liu, Xueting] Caritas Inst Higher Educ, Sch Comp & Informat Sci, Hong Kong, Peoples R China.
   [Zhu, Haichao] Rokid Corp Ltd, Hangzhou, Zhejiang, Peoples R China.
C3 Chinese University of Hong Kong; Chinese Academy of Sciences; Shenzhen
   Institute of Advanced Technology, CAS; South China University of
   Technology; Saint Francis University Hong Kong
RP Wong, TT (corresponding author), Chinese Univ Hong Kong, Dept Comp Sci & Engn, Hong Kong, Peoples R China.; Wong, TT (corresponding author), Chinese Acad Sci, Shenzhen Inst Adv Technol, Guangdong Prov Key Lab Comp Vis & Virtual Real, Shenzhen, Peoples R China.
EM ttwong@cse.cuhk.edu.hk
RI Liu, Xueting/AAG-9648-2019; He, Shengfeng/E-5682-2016; Han,
   Chu/GWM-9255-2022
OI Liu, Xueting/0000-0002-0868-5353; He, Shengfeng/0000-0002-3802-4644;
   Han, Chu/0000-0001-7557-9131
FU Research Grants Council of the Hong Kong Special Administrative Region
   [CUHK 14201017]; Shenzhen Science and Technology Programs
   [JCYJ20160429190300857, JCYJ20180507182410327, JCYJ20180507182415428]
FX This project is supported by the Research Grants Council of the Hong
   Kong Special Administrative Region, under RGC General Research Fund
   (PrToject No. CUHK 14201017), and Shenzhen Science and Technology
   Programs (No. JCYJ20160429190300857, No. JCYJ20180507182410327, and No.
   JCYJ20180507182415428).
CR [Anonymous], COMPUTER VISION PATT
   [Anonymous], 2017, Advanced high dynamic range imaging
   [Anonymous], 1999, HDB COMPUTER VISION
   [Anonymous], 1994, Graph. Gems, DOI DOI 10.1016/B978-0-12-336156-1.50054-9
   [Anonymous], 2017, P 31 AAAI C ART INT
   [Anonymous], 2017, IEEE C COMP VIS PATT
   [Anonymous], 2015, PROC CVPR IEEE
   [Anonymous], ARXIV171007480
   [Anonymous], 2016, INT C LEARNING REPRE
   [Anonymous], 28 EUR C VIS PERC
   Aubry M, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2629645
   Chua SH, 2015, ACM T COMPUT-HUM INT, V21, DOI 10.1145/2687923
   CURTIS DW, 1978, J EXP PSYCHOL HUMAN, V4, P132, DOI 10.1037/0096-1523.4.1.132
   DALY S, 1992, P SOC PHOTO-OPT INS, V1666, P2, DOI 10.1117/12.135952
   Debevec P. E., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P369, DOI 10.1145/258734.258884
   DEWEERT CMM, 1974, PERCEPT PSYCHOPHYS, V15, P551, DOI 10.3758/BF03199301
   Drago F, 2003, COMPUT GRAPH FORUM, V22, P419, DOI 10.1111/1467-8659.00689
   Durand F, 2002, ACM T GRAPHIC, V21, P257, DOI 10.1145/566570.566574
   Endo Y, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130834
   ENGEL GR, 1969, VISION RES, V9, P1111, DOI 10.1016/0042-6989(69)90051-0
   Fairchild MD, 2007, FIFTEENTH COLOR IMAGING CONFERENCE: COLOR SCIENCE AND ENGINEERING SYSTEMS, TECHNOLOGIES, AND APPLICATIONS, FINAL PROGRAM AND PROCEEDINGS, P233
   Farbman Z, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360666
   Feng MY, 2017, IEEE APP IMG PAT
   Gharbi M, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073592
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Hasinoff SW, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980254
   Heidrich Wolfgang, ERIK REINHARD
   Hong TT, 2016, ANAL CHIM ACTA, V931, P1, DOI 10.1016/j.aca.2016.05.013
   IIZUKA S, 2016, ACM T GRAPHIC, V35, DOI DOI 10.1145/2897824.2925974
   Johnson Justin, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9906, P694, DOI 10.1007/978-3-319-46475-6_43
   King DB, 2015, ACS SYM SER, V1214, P1
   LEGGE GE, 1984, VISION RES, V24, P385, DOI 10.1016/0042-6989(84)90064-6
   LEGGE GE, 1981, PERCEPT PSYCHOPHYS, V30, P49, DOI 10.3758/BF03206136
   LEVELT WJM, 1965, BRIT J PSYCHOL, V56, P1, DOI 10.1111/j.2044-8295.1965.tb00939.x
   Maehara G, 2005, OPT REV, V12, P76, DOI 10.1007/s10043-004-0076-5
   Meese TS, 2006, J VISION, V6, P1224, DOI 10.1167/6.11.7
   Meese TS, 2004, J VISION, V4, P843, DOI 10.1167/4.10.2
   Nemoto H., 2015, 9 INT WORKSHOP VIDEO
   Paris S, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964963
   Paszke A., 2017, AUTOMATIC DIFFERENTI
   Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278
   Reinhard E, 2002, ACM T GRAPHIC, V21, P267, DOI 10.1145/566570.566575
   Sajjadi MSM, 2017, IEEE I CONF COMP VIS, P4501, DOI 10.1109/ICCV.2017.481
   Schlick C., 1995, Photorealistic Rendering Techniques, P7
   Sendik O, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3015461
   Smith K, 2006, COMPUT GRAPH FORUM, V25, P427, DOI 10.1111/j.1467-8659.2006.00962.x
   TUMBLIN J, 1993, IEEE COMPUT GRAPH, V13, P42, DOI 10.1109/38.252554
   Tumblin J, 1999, COMP GRAPH, P83, DOI 10.1145/311535.311544
   von Helmholtz H., 2005, Treatise on Physiological Optics
   Wilson HR, 2017, VISION RES, V140, P89, DOI 10.1016/j.visres.2017.07.016
   Xie SN, 2015, IEEE I CONF COMP VIS, P1395, DOI [10.1109/ICCV.2015.164, 10.1007/s11263-017-1004-z]
   Yang C., 2016, High-Resolution Image Inpainting using Multi-Scale Neural Patch Synthesis
   Zhang L, 2011, IEEE T IMAGE PROCESS, V20, P2378, DOI 10.1109/TIP.2011.2109730
   Zhang ZM, 2018, COMPUT GRAPH FORUM, V37, P433, DOI 10.1111/cgf.13580
NR 54
TC 8
Z9 10
U1 0
U2 7
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2019
VL 35
IS 6-8
SI SI
BP 997
EP 1011
DI 10.1007/s00371-019-01669-8
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IC1IH
UT WOS:000470712200019
DA 2024-07-18
ER

PT J
AU Wang, LL
   Zhang, WH
   Li, N
   Zhang, BN
   Popescu, V
AF Wang, Lili
   Zhang, Wenhao
   Li, Nian
   Zhang, Boning
   Popescu, Voicu
TI Intermediate shadow maps for interactive many-light rendering
SO VISUAL COMPUTER
LA English
DT Article
DE Many lights; Visibility; Shadow mapping
AB We present an efficient method for computing shadows for many light sources (e.g., 1024). Our work is based on the observation that conventional shadow mapping becomes redundant as the number of lights increases. First, we sample the scene with a constant number of depth images (e.g., 10), which we call intermediate shadow maps. Then the shadow map for each light is approximated by rendering triangles reconstructed from the intermediate shadow maps. The cost of rendering these triangles is much smaller than rendering the original geometry of a complex scene. The algorithm supports fully dynamic scenes. Our results show that our method can produce soft shadows comparable to those obtained by conventional shadow mapping for each light source or by ray tracing, but at a higher frame rate.
C1 [Wang, Lili; Zhang, Wenhao; Li, Nian; Zhang, Boning] Beihang Univ, Sch Comp Sci & Engn, State Key Lab Virtual Real Technol & Syst, Beijing, Peoples R China.
   [Popescu, Voicu] Purdue Univ, W Lafayette, IN 47907 USA.
C3 Beihang University; Purdue University System; Purdue University
RP Wang, LL (corresponding author), Beihang Univ, Sch Comp Sci & Engn, State Key Lab Virtual Real Technol & Syst, Beijing, Peoples R China.
EM wanglily@buaa.edu.cn
RI wang, lili/HJP-8047-2023; Zhang, Wenhao/JEO-9872-2023
CR Akerlund O, 2007, PACIFIC GRAPHICS 2007: 15TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, P161, DOI 10.1109/PG.2007.30
   [Anonymous], 2009, VIS MOD VIS WORKSH
   Cheslack-Postava E., 2008, ACM T GRAPHIC, V27, P32
   Dachsbacher C, 2014, COMPUT GRAPH FORUM, V33, P88, DOI 10.1111/cgf.12256
   Davidovic T, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866169
   Feris R, 2004, XVII BRAZILIAN SYMPOSIUM ON COMPUTER GRAPHICS AND IMAGE PROCESSING, PROCEEDINGS, P316, DOI 10.1109/SIBGRA.2004.1352976
   Hasan M, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239477
   Holländer M, 2011, COMPUT GRAPH FORUM, V30, P1233, DOI 10.1111/j.1467-8659.2011.01982.x
   Huo YC, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818120
   Kristensen AW, 2005, ACM T GRAPHIC, V24, P1208, DOI 10.1145/1073204.1073334
   Nichols G, 2010, COMPUT GRAPH FORUM, V29, P1279, DOI 10.1111/j.1467-8659.2010.01723.x
   Oliveira MM, 2000, COMP GRAPH, P359, DOI 10.1145/344779.344947
   Olsson O., 2014, Proceedings of the 18th meeting of the ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games, P87
   Olsson O, 2015, IEEE T VIS COMPUT GR, V21, P701, DOI 10.1109/TVCG.2015.2418772
   Paquette E, 1998, COMPUT GRAPH FORUM, V17, pC63, DOI 10.1111/1467-8659.00254
   Popescu V, 2000, COMP GRAPH, P433, DOI 10.1145/344779.344979
   Ritschel T., 2008, ACM T GRAPHIC, V27, P32
   Ritschel T., 2008, P GRAPH INT 2008
   Ritschel T., 2009, ACM T GRAPHIC, V28, P89
   Ritschel Tobias., 2007, Proceedings of the 18th Eurographics conference on Rendering Techniques, EGSR'07, P61
   Tang Y, 2004, I C CONT AUTOMAT ROB, P386
   Walter B, 2005, ACM T GRAPHIC, V24, P1098, DOI 10.1145/1073204.1073318
   Walter B, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185555
   Wang R, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508413
   Wu YT, 2013, IEEE T VIS COMPUT GR, V19, P1566, DOI 10.1109/TVCG.2013.21
NR 25
TC 1
Z9 1
U1 0
U2 4
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2018
VL 34
IS 10
BP 1415
EP 1426
DI 10.1007/s00371-017-1449-7
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GR0KK
UT WOS:000442204400011
DA 2024-07-18
ER

PT J
AU Spieldenner, T
   Byelozyorov, S
   Guldner, M
   Slusallek, P
AF Spieldenner, Torsten
   Byelozyorov, Sergiy
   Guldner, Michael
   Slusallek, Philipp
TI FiVES: an aspect-oriented approach for shared virtual environments in
   the web
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT International Conference on Cyberworlds (CW)
CY SEP 20-22, 2017
CL Univ Chester, Chester, ENGLAND
SP Eurograph Assoc, Int Federat Informat Proc, ACM SIGGRAPH
HO Univ Chester
DE Virtual environments; Aspect-oriented design; Virtual worlds; Shared
   virtual environments; Distributed virtual environments; Virtual world
   server; Multiuser environments; 3D web
ID SCALABILITY
AB Virtual Environments have become a compelling tool for various applications beyond gaming and Virtual Worlds, for example, for education, collaborative engineering, or simulation and visualization. In the emerging field of smart environments, like Smart Cities and Smart Factories for industry or agriculture, a digital counterpart of a real-world site, driven by hundreds of Internet of Things sensors that emit their data in real-time, is a central part of the application. Maintaining such a large-scale virtual environment and keeping it up-to-date with changing requirements set by connected sensors and services is a challenge. For this, we present FiVES, a server framework that is based on an aspect-oriented architecture to create highly maintainable large-scale virtual environments by avoiding cross-cutting concerns like tangling or scattering of code between modules in resulting applications. We present a fully functional implementation of our system that builds on an extendable and versatile data model, a flexible plugin mechanism, and a transparent yet efficient synchronization layer that is independent of the modules from which applications are assembled.
C1 [Spieldenner, Torsten; Guldner, Michael; Slusallek, Philipp] German Res Ctr Artificial Intelligence DFKI, D-66123 Saarbrucken, Germany.
   [Spieldenner, Torsten] Saarbrucken Grad Sch Comp Sci, D-66123 Saarbrucken, Germany.
   [Guldner, Michael] Univ Appl Sci, D-66117 Saarbrucken, Germany.
   [Byelozyorov, Sergiy; Slusallek, Philipp] Saarland Univ, D-66123 Saarbrucken, Germany.
C3 German Research Center for Artificial Intelligence (DFKI); Saarland
   University
RP Spieldenner, T (corresponding author), German Res Ctr Artificial Intelligence DFKI, D-66123 Saarbrucken, Germany.; Spieldenner, T (corresponding author), Saarbrucken Grad Sch Comp Sci, D-66123 Saarbrucken, Germany.
EM torsten.spieldenner@dfki.de; rryk.ua@gmail.com; michael.guldner@dfki.de;
   philipp.slusallek@dfki.de
OI Slusallek, Philipp/0000-0002-2189-2429
FU European Union project FI-NEXT [732851]; European Union FP7/ICT-FI
   project FI-CORE [632893]; German Ministry of Education and Research
   (BMBF) [01IW14004]; H2020 - Industrial Leadership [732851] Funding
   Source: H2020 - Industrial Leadership
FX Thework presented in this paper received funding from the European
   Union's project FI-NEXT under Grant Agreement No 732851 and FP7/ICT-FI
   project FI-CORE under Grant Agreement No 632893, and from the project
   INVERSIV, funded by the German Ministry of Education and Research (BMBF)
   under Grant Agreement No 01IW14004.
CR Aguilera Marcos K., 2007, Operating Systems Review, V41, P159, DOI 10.1145/1323293.1294278
   Alatalo T, 2011, IEEE INTERNET COMPUT, V15, P30, DOI 10.1109/MIC.2011.82
   [Anonymous], 2017, arXiv
   Behr J., 2009, WEB3D 09 P 14 INT C, P127, DOI [DOI 10.1145/1559764.1559784, 10.1145/1559764.1559784]
   Blom K. J., 2008, IEEE VR 2008 WORKSH
   Byelozyorov S, 2013, 2013 INTERNATIONAL CONFERENCE ON CYBERWORLDS (CW), P94, DOI 10.1109/CW.2013.19
   Cañas C, 2014, ACM/IFIP/USENIX MIDDLEWARE 2014, P241, DOI 10.1145/2663165.2663337
   Castro M, 2002, IEEE J SEL AREA COMM, V20, P1489, DOI 10.1109/JSAC.2002.803069
   Claypool M, 2006, COMMUN ACM, V49, P40, DOI 10.1145/1167838.1167860
   Dahl Toni, 2013, 2013 Seventh International Conference on Next-Generation Mobile Apps, Services and Technologies (NGMAST), P7, DOI 10.1109/NGMAST.2013.11
   Dionisio JDN, 2013, ACM COMPUT SURV, V45, DOI 10.1145/2480741.2480751
   Farooq U, 2017, SIMUL MODEL PRACT TH, V72, P118, DOI 10.1016/j.simpat.2016.12.010
   Gupta N, 2009, PROC INT CONF DATA, P1311, DOI 10.1109/ICDE.2009.228
   Gusella R., 1987, TECHNICAL REPORTS UC
   Hof R., 2006, WEEK
   Kaplan J, 2011, IEEE INTERNET COMPUT, V15, P38, DOI 10.1109/MIC.2011.76
   Kienzle Jorg., 2009, P 4 INT C FDN DIGITA, P308
   Klein F, 2014, WEB3D 2014, P71
   Lake Dan, 2010, P 9 ANN WORKSH NETW
   Lamping John, 1997, P EUR C OBJ OR PROGR
   Lifton J., 2010, DUAL REALITY MERGING, P12
   Liu HY, 2012, ACM COMPUT SURV, V44, DOI 10.1145/2333112.2333116
   Najaran MT, 2013, INT CONF CLOUD COMP, P411, DOI 10.1109/CloudCom.2013.60
   Najaran MahdiTayarani., 2014, ACM Multimedia Systems Conference (MMSys), P127
   Nicol G., 2001, Document Object Model (DOM) level 3 core specification
   Pinto M, 2001, EIGHTH IEEE WORKSHOP ON FUTURE TRENDS OF DISTRIBUTED COMPUTING SYSTEMS, PROCEEDINGS, P9, DOI 10.1109/FTDCS.2001.969615
   Potel M., 1996, MVP MODEL VIEW PRESE
   Sons K, 2010, P 15 INT C WEB 3D TE, P175
   Valadares A, 2016, WINT SIMUL C PROC, P1024, DOI 10.1109/WSC.2016.7822162
   W3C, 2014, RES DESCR FRAM RDF
NR 30
TC 5
Z9 5
U1 1
U2 16
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2018
VL 34
IS 9
BP 1269
EP 1282
DI 10.1007/s00371-018-1564-0
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA GQ5MB
UT WOS:000441727000011
DA 2024-07-18
ER

PT J
AU dos Santos, JDB
   Sen, P
   Oliveira, MM
AF Brito dos Santos, Jonas Deyson
   Sen, Pradeep
   Oliveira, Manuel M.
TI A framework for developing and benchmarking sampling and denoising
   algorithms for Monte Carlo rendering
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 35th Computer Graphics International conference (CGI)
CY JUN 11-14, 2018
CL INDONESIA
SP Comp Graph Soc, Nanyang Technol Univ
DE Monte Carlo rendering; Adaptive sampling and Reconstruction; Denoising;
   Benchmarking
ID RECONSTRUCTION
AB Although many adaptive sampling and reconstruction techniques for Monte Carlo (MC) rendering have been proposed in the last few years, the case for which one should be used for a specific scene is still to be made. Moreover, developing a new technique has required selecting a particular rendering system, which makes the technique tightly coupled to the chosen renderer and limits the amount of scenes it can be tested on to those available for that renderer. In this paper, we propose a renderer-agnostic framework for testing and benchmarking sampling and denoising techniques for MC rendering, which allows an algorithm to be easily deployed to multiple rendering systems and tested on a wide variety of scenes. Our system achieves this by decoupling the techniques from the rendering systems, hiding the renderer details behind an API. This improves productivity and allows for direct comparisons among techniques originally developed for different rendering systems. We demonstrate the effectiveness of our API by using it to instrument four rendering systems and then using them to benchmark several state-of-the-art MC denoising techniques and sampling strategies.
C1 [Brito dos Santos, Jonas Deyson; Oliveira, Manuel M.] Univ Fed Rio Grande do Sul, Inst Informat, Porto Alegre, RS, Brazil.
   [Sen, Pradeep] Univ Calif Santa Barbara, Dept Elect & Comp Engn, Santa Barbara, CA 93106 USA.
C3 Universidade Federal do Rio Grande do Sul; University of California
   System; University of California Santa Barbara
RP dos Santos, JDB (corresponding author), Univ Fed Rio Grande do Sul, Inst Informat, Porto Alegre, RS, Brazil.
EM jdbsantos@inf.ufrgs.br; psen@ece.ucsb.edu; oliveira@inf.ufrgs.br
RI Oliveira, Manuel M/H-1508-2011
OI Sen, Pradeep/0000-0002-8042-924X
FU CAPES-Brazil [306196/2014-0, 423673/2016-5]; CNPq-Brazil [306196/2014-0,
   423673/2016-5]; US National Science Foundation [IIS-1321168,
   IIS-1619376]
FX This work was funded by CAPES and CNPq-Brazil (fellowships and Grants
   306196/2014-0 and 423673/2016-5), and US National Science Foundation
   Grants IIS-1321168 and IIS-1619376.
CR Anderson L, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073704
   [Anonymous], ACM TOG
   [Anonymous], ALPHA MATTING EVALUA
   [Anonymous], MIDDLEBURY FLOW ACCU
   [Anonymous], ACM T GRAPH
   [Anonymous], 2016, PHYS BASED RENDERING
   [Anonymous], ACM SIGGRAPH ASIA 20
   [Anonymous], EECETR110004 U NEW M
   [Anonymous], P SIGGRAPH 84
   Baker S, 2011, INT J COMPUT VISION, V92, P1, DOI 10.1007/s11263-010-0390-2
   Bako S, 2017, ACM T GRAPHIC, V36, DOI [10.1145/3072959.3073703, 10.1145/3072959.3073708]
   BARRON JL, 1994, INT J COMPUT VISION, V12, P43, DOI 10.1007/BF01420984
   Bauszat P, 2015, COMPUT GRAPH FORUM, V34, P597, DOI 10.1111/cgf.12587
   Bitterli B, 2016, COMPUT GRAPH FORUM, V35, P107, DOI 10.1111/cgf.12954
   Buck I, 2004, ACM T GRAPHIC, V23, P777, DOI 10.1145/1015706.1015800
   Chaitanya CRA, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073601
   Delbracio M, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2532708
   Heck D, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2487228.2487233
   Kajiya J. T., 1986, SIGGRAPH, P143, DOI 10.1145/15886.15902
   Kalantari NK, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766977
   Kalantari NK, 2013, COMPUT GRAPH FORUM, V32, P93, DOI 10.1111/cgf.12029
   LEE ME, 1990, IEEE COMPUT GRAPH, V10, P23, DOI 10.1109/38.55149
   Li TM, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366213
   Mark WR, 2003, ACM T GRAPHIC, V22, P896, DOI 10.1145/882262.882362
   Moon B, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2641762
   Mullapudi RT, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925952
   Pharr M., 2010, PHYS BASED RENDERING
   Ragan-Kelley J, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1966394.1966396
   Ren ZM, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2421636.2421637
   Rhemann C, 2009, PROC CVPR IEEE, P1826, DOI 10.1109/CVPRW.2009.5206503
   Rousselle F, 2013, COMPUT GRAPH FORUM, V32, P121, DOI 10.1111/cgf.12219
   Rousselle F, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366214
   Rousselle F, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024193
   Rushmeier H. E., 1994, Computer Graphics Proceedings. Annual Conference Series 1994. SIGGRAPH 94 Conference Proceedings, P131, DOI 10.1145/192161.192189
   Scharstein D, 2002, INT J COMPUT VISION, V47, P7, DOI 10.1023/A:1014573219977
   Scharstein D., 2002, MIDDLEBURY STEREO VI
   STEIN CM, 1981, ANN STAT, V9, P1135, DOI 10.1214/aos/1176345632
   Vatolin Dmitriy S, 2015, BMVC, P99
   Veach E., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P65, DOI 10.1145/258734.258775
   Zwicker M, 2015, COMPUT GRAPH FORUM, V34, P667, DOI 10.1111/cgf.12592
NR 40
TC 2
Z9 2
U1 0
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2018
VL 34
IS 6-8
SI SI
BP 765
EP 778
DI 10.1007/s00371-018-1521-y
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA GH6MC
UT WOS:000433557400003
DA 2024-07-18
ER

PT J
AU Jain, N
   Wydra, A
   Hai, W
   Thalmann, NM
   Thalmann, D
AF Jain, Nisha
   Wydra, Andrzej
   Hai, Wen
   Thalmann, Nadia Magnenat
   Thalmann, Daniel
TI Time-scaled interactive object-driven multi-party VR
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 35th Computer Graphics International conference (CGI)
CY JUN 11-14, 2018
CL INDONESIA
SP Comp Graph Soc, Nanyang Technol Univ
DE Time scaling; Multi-party; Virtual reality; Avatars
AB Object-driven interactions have various applications, especially in team activities and sports like volleyball, football, tennis, etc. We define a multi-party virtual world as an environment which includes multiple avatars and computer agents. Latency is one of the primary issues while building a multi-party setup. Delays due to latency cause inconsistencies and unnecessary halts resulting in unrealistic experience in VR. For interactions using objects, it becomes vital to handle the latencies. We present a time scaling algorithm for latency management and synchronization between multiple avatars, while they interact using objects in the VR environment. We propose two time scaling schemes-source and target based. Both these methodologies are dependent on which user's virtual view (source or target) needs to be uniform. Incorporating the presented time scaling schemes, we develop a comprehensive multi-party virtual reality platform to simulate applications which involves interactions between agents and avatars using objects. We adopt a client-server architecture for building our multiuser VR platform which induces flexibility of adding multiple avatars and/or agents to the application. The modular nature of our proposed system enables extension to different VR applications that encompass communication using objects. We exemplify our framework by developing a multi-player VR volleyball game which employs the features of our proposed scheme. We demonstrate the significance of time scaling in the object-driven interactive multi-party VR framework through a comparative user study. We also evaluate the immersion experience in our multi-player volleyball game by a user survey.
C1 [Jain, Nisha; Wydra, Andrzej; Hai, Wen; Thalmann, Nadia Magnenat] Nanyang Technol Univ, Inst Media Innovat, Singapore, Singapore.
   [Thalmann, Daniel] Ecole Polytech Fed Lausanne, Lausanne, Switzerland.
   [Thalmann, Daniel] Ecole Polytech Fed Lausanne, VRlab, Lausanne, Switzerland.
C3 Nanyang Technological University; Swiss Federal Institutes of Technology
   Domain; Ecole Polytechnique Federale de Lausanne; Swiss Federal
   Institutes of Technology Domain; Ecole Polytechnique Federale de
   Lausanne
RP Jain, N (corresponding author), Nanyang Technol Univ, Inst Media Innovat, Singapore, Singapore.
EM nisha.iitd@gmail.com; wyderxxx@gmail.com; wenhai@ntu.edu.sg;
   nadiathalmann@ntu.edu.sg; daniel.thalmann@epfl.ch
RI Thalmann, Daniel/AAL-1097-2020; Thalmann, Daniel/A-4347-2008; Thalmann,
   Nadia/AAK-5195-2021
OI Thalmann, Daniel/0000-0002-0451-7491; Thalmann,
   Nadia/0000-0002-1459-5960
FU Nanyang Technological University (NTU) Singapore; Institute of Software,
   Chinese Academy of Sciences (ISCAS) in Beijing, China; National Research
   Foundation, Prime Minister's Office, Singapore, under its NRF-NSFC Joint
   Research Grant
FX This research is supported by the program titled Realistic Immersion
   with Virtual Humans, a collaboration between Nanyang Technological
   University (NTU) Singapore and Institute of Software, Chinese Academy of
   Sciences (ISCAS) in Beijing, China. The program is supported by the
   National Research Foundation, Prime Minister's Office, Singapore, under
   its NRF-NSFC Joint Research Grant Call (Data Science).
CR Bernier Y. W., 2001, GAM DEV C, V98033
   Bouzid A, 2017, 2017 18TH INTERNATIONAL CARPATHIAN CONTROL CONFERENCE (ICCC), P134, DOI 10.1109/CarpathianCC.2017.7970385
   Eidenberger H., 2015, P 21 ACM S VIRT REAL, P9, DOI [10.1145/2821592.2821612, DOI 10.1145/2821592.2821612]
   Evangelakos D, 2016, PROCEEDINGS I3D 2016: 20TH ACM SIGGRAPH SYMPOSIUM ON INTERACTIVE 3D GRAPHICS AND GAMES, P193, DOI 10.1145/2856400.2876015
   Herumurti D, 2017, AIP CONF PROC, V1867, DOI 10.1063/1.4994421
   Jaya I, 2016, IEEE ACM DIS SIM, P92, DOI 10.1109/DS-RT.2016.28
   Jennett C, 2008, INT J HUM-COMPUT ST, V66, P641, DOI 10.1016/j.ijhcs.2008.04.004
   Lee D., 2016, International Journal of Applied Engineering Research, V11, P829
   Lugrin J., 2012, Proceedings of the 18th ACM Symposium on Virtual Reality Software and Technology, VRST '12, P137, DOI [DOI 10.1145/2407336.2407363, 10.1145/2407336.2407363]
   Ryge A, 2017, P IEEE VIRT REAL ANN, P365, DOI 10.1109/VR.2017.7892328
   Smed J, 2002, ELECTRON LIBR, V20, P87, DOI 10.1108/02640470210424392
   Tedjokusumo J, 2010, IEEE T SYST MAN CY A, V40, P147, DOI 10.1109/TSMCA.2009.2028432
   Witmer BG, 1998, PRESENCE-TELEOP VIRT, V7, P225, DOI 10.1162/105474698565686
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
NR 18
TC 8
Z9 8
U1 2
U2 12
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2018
VL 34
IS 6-8
SI SI
BP 887
EP 897
DI 10.1007/s00371-018-1539-1
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA GH6MC
UT WOS:000433557400013
DA 2024-07-18
ER

PT J
AU Mura, C
   Wyss, G
   Pajarola, R
AF Mura, Claudio
   Wyss, Gregory
   Pajarola, Renato
TI Robust normal estimation in unstructured 3D point clouds by selective
   normal space exploration
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 35th Computer Graphics International conference (CGI)
CY JUN 11-14, 2018
CL INDONESIA
SP Comp Graph Soc, Nanyang Technol Univ
DE Normal estimation; Point cloud processing; Robust statistics
ID SURFACE
AB We present a fast and practical approach for estimating robust normal vectors in unorganized point clouds. Our proposed technique is robust to noise and outliers and can preserve sharp features in the input model while being significantly faster than the current state-of-the-art alternatives. The key idea to this is a novel strategy for the exploration of the normal space: First, an initial candidate normal vector, optimal under a robust least median norm, is selected from a discrete subregion of this space, chosen conservatively to include the correct normal; then, the final robust normal is computed, using a simple, robust procedure that iteratively refines the candidate normal initially selected. This strategy allows us to reduce the computation time significantly with respect to other methods based on sampling consensus and yet produces very reliable normals even in the presence of noise and outliers as well as along sharp features. The validity of our approach is confirmed by an extensive testing on both synthetic and real-world data and by a comparison against the most relevant state-of-the-art approaches.
C1 [Mura, Claudio; Wyss, Gregory; Pajarola, Renato] Univ Zurich, Dept Informat, Binzmuhlestr 14, CH-8050 Zurich, Switzerland.
C3 University of Zurich
RP Mura, C (corresponding author), Univ Zurich, Dept Informat, Binzmuhlestr 14, CH-8050 Zurich, Switzerland.
EM claudio@ifi.uzh.ch; wyss@ifi.uzh.ch; pajarola@ifi.uzh.ch
OI Pajarola, Renato/0000-0002-6724-526X
FU Swiss National Science Foundation [159225]
FX This work was partially funded by the Swiss National Science Foundation
   (Grant Number 159225).
CR Alexander M, 2001, INTERNETWEEK, P21
   Avron H, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1857907.1857911
   Berger M, 2017, COMPUT GRAPH FORUM, V36, P301, DOI 10.1111/cgf.12802
   Borrmann D, 2011, 3D RES, V2, DOI 10.1007/3DRes.02(2011)3
   Botsch M, 2005, COMPUT GRAPH FORUM, V24, P611, DOI 10.1111/j.1467-8659.2005.00886.x
   Boulch A, 2016, COMPUT GRAPH FORUM, V35, P281, DOI 10.1111/cgf.12983
   Boulch A, 2012, COMPUT GRAPH FORUM, V31, P1765, DOI 10.1111/j.1467-8659.2012.03181.x
   Cazals F, 2005, COMPUT AIDED GEOM D, V22, P121, DOI 10.1016/j.cagd.2004.09.004
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Fleishman S, 2005, ACM T GRAPHIC, V24, P544, DOI 10.1145/1073204.1073227
   Gross M.H., 2007, SERIES COMPUTER GRAP
   Guennebaud G, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239474
   Guerrero P, 2018, COMPUT GRAPH FORUM, V37, P75, DOI 10.1111/cgf.13343
   HOPPE H, 1992, COMP GRAPH, V26, P71, DOI 10.1145/142920.134011
   Huber P., 2009, Wiley Series in Probability and Statistics, V2
   Jones TR, 2004, IEEE COMPUT GRAPH, V24, P53, DOI 10.1109/MCG.2004.14
   Kazhdan M, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2487228.2487237
   Kobbelt L, 2004, COMPUT GRAPH-UK, V28, P801, DOI 10.1016/j.cag.2004.08.009
   Li B, 2010, COMPUT GRAPH-UK, V34, P94, DOI 10.1016/j.cag.2010.01.004
   Liu XP, 2015, COMPUT GRAPH-UK, V51, P106, DOI 10.1016/j.cag.2015.05.024
   Miller JV, 1996, PROC CVPR IEEE, P300, DOI 10.1109/CVPR.1996.517089
   Mitra NJ, 2004, INT J COMPUT GEOM AP, V14, P261, DOI 10.1142/S0218195904001470
   Pauly M, 2003, ACM T GRAPHIC, V22, P641, DOI 10.1145/882262.882319
   SAINZ M., 2004, Proceedings of Eurographics Symposium on Point-Based Graphics 2004, P121
   Schnabel R, 2007, COMPUT GRAPH FORUM, V26, P214, DOI 10.1111/j.1467-8659.2007.01016.x
   Steder B, 2011, IEEE INT CONF ROBOT, P2601, DOI 10.1109/ICRA.2011.5980187
   Wang Y, 2013, COMPUT AIDED DESIGN, V45, P1333, DOI 10.1016/j.cad.2013.06.003
   Yoon M, 2007, COMPUT AIDED DESIGN, V39, P408, DOI 10.1016/j.cad.2007.02.008
   Zhang J, 2013, COMPUT GRAPH-UK, V37, P697, DOI 10.1016/j.cag.2013.05.008
   Zheng YY, 2011, IEEE T VIS COMPUT GR, V17, P1521, DOI 10.1109/TVCG.2010.264
NR 30
TC 12
Z9 15
U1 0
U2 14
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2018
VL 34
IS 6-8
SI SI
BP 961
EP 971
DI 10.1007/s00371-018-1542-6
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA GH6MC
UT WOS:000433557400019
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Xu, JL
   Kang, HM
   Chen, FL
AF Xu, Jinlan
   Kang, Hongmei
   Chen, Falai
TI Content-aware image resizing using quasi-conformal mapping
SO VISUAL COMPUTER
LA English
DT Article
DE Content-aware image resizing; Quasi-conformal mapping; Beltrami
   coefficient
ID LANDMARK; REGISTRATION
AB Content-aware image resizing is resizing an image such that the prominent feature of the image is intact and the homogenous content of the image is distorted as little as possible. There is a lot of research on this topic, and various approaches have been proposed so far. One problem with previous methods is the lack of a theoretical guarantee of a rigorous bijective map between an image and the target image, which may cause artifacts in the target image. In this paper, we present a new approach to solve the image resizing problem based on quasi-conformal mapping. We apply quasi-conformal mapping to set up a bijective map between an image and the target image such that the salient feature of the image is uniformly scaled, while the homogenous content of the image is distorted as little as possible. The distortion is characterized by a function defined by the Beltrami coefficient of the quasi-conformal mapping, which is minimized by solving a nonconvex optimization problem. Solving the optimization problem is reduced to solving two convex optimization problems alternatingly. We implemented our algorithm with many examples and made comparisons with previous approaches. The examples suggest that our method is comparable with previous approaches while guaranteeing that there are no foldovers. Furthermore, our method can easily preserve line features and handle large changes in the aspect ratios of images.
C1 [Xu, Jinlan] Hangzhou Dianzi Univ, Sch Comp Sci & Technol, Hangzhou, Zhejiang, Peoples R China.
   [Kang, Hongmei; Chen, Falai] Univ Sci & Technol China, Sch Math Sci, Hefei, Anhui, Peoples R China.
C3 Hangzhou Dianzi University; Chinese Academy of Sciences; University of
   Science & Technology of China, CAS
RP Xu, JL (corresponding author), Hangzhou Dianzi Univ, Sch Comp Sci & Technol, Hangzhou, Zhejiang, Peoples R China.
EM jlxu@hdu.edu.cn; chenfl@ustc.edu.cn
FU Open Project Program of the State Key Lab of CAD&CG, Zhejiang University
   [A1602]
FX The work is supported by the Open Project Program of the State Key Lab
   of CAD&CG (Grant No. A1602), Zhejiang University.
CR Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   Ahlfors L., 1955, ANN ACAD SCI FENN A1, V206, P1
   Ahlfors LV., 2006, Lectures on Quasiconformal Mappings. University Lecture Series, V2
   [Anonymous], 2013, ACM T GRAPH
   [Anonymous], 2007, ACM T GRAPH
   Chen R.C., 2010, 2010 IEEE International Conference on Fuzzy Systems, P1
   Chien Edward, 2016, ACM Transactions on Graphics, V35, DOI 10.1145/2897824.2925926
   Gal R., 2006, P S REND, V297-303
   Gardiner F.P., 2000, Quasiconformal Teichmuller Theory
   Goferman S, 2012, IEEE T PATTERN ANAL, V34, P1915, DOI 10.1109/TPAMI.2011.272
   Guo YW, 2009, IEEE T MULTIMEDIA, V11, P856, DOI 10.1109/TMM.2009.2021781
   Ho KT, 2016, ADV COMPUT MATH, V42, P279, DOI 10.1007/s10444-015-9424-1
   Jin Y, 2010, VISUAL COMPUT, V26, P769, DOI 10.1007/s00371-010-0472-8
   Karni Z, 2009, COMPUT GRAPH FORUM, V28, P1257, DOI 10.1111/j.1467-8659.2009.01503.x
   Lam KC, 2014, SIAM J IMAGING SCI, V7, P2364, DOI 10.1137/130943406
   Lee YT, 2016, J SCI COMPUT, V67, P926, DOI 10.1007/s10915-015-0113-5
   Levi Z, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925929
   Lin SS, 2013, IEEE T MULTIMEDIA, V15, P359, DOI 10.1109/TMM.2012.2228475
   Lipman Y, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185604
   Lui LM, 2015, J SCI COMPUT, V63, P573, DOI 10.1007/s10915-014-9903-4
   Lui LM, 2014, SIAM J IMAGING SCI, V7, P391, DOI 10.1137/120900186
   Lui LM, 2012, J SCI COMPUT, V50, P557, DOI 10.1007/s10915-011-9506-2
   Lui LM, 2010, LECT NOTES COMPUT SC, V6362, P323
   Nian X., 2016, COMPUTER GR IN PRESS
   Nian XS, 2016, COMPUT METHOD APPL M, V311, P41, DOI 10.1016/j.cma.2016.07.035
   Panozzo D, 2012, COMPUT GRAPH FORUM, V31, P229, DOI 10.1111/j.1467-8659.2012.03001.x
   Shi ML, 2010, LECT NOTES COMPUT SC, V6249, P456
   Wang J, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1330511.1330512
   Weber O, 2012, COMPUT GRAPH FORUM, V31, P1679, DOI 10.1111/j.1467-8659.2012.03173.x
   Yan Q, 2013, PROC CVPR IEEE, P1155, DOI 10.1109/CVPR.2013.153
   Zeng W, 2009, LECT NOTES COMPUT SC, V5654, P391, DOI 10.1007/978-3-642-03596-8_23
   Zeng W, 2011, PROC CVPR IEEE
   Zeng W, 2012, NUMER MATH, V121, P671, DOI 10.1007/s00211-012-0446-z
   Zhang GX, 2009, COMPUT GRAPH FORUM, V28, P1897, DOI 10.1111/j.1467-8659.2009.01568.x
   Zhang L, 2010, ACM T SENSOR NETWORK, V6, DOI 10.1145/1777406.1777414
NR 35
TC 11
Z9 11
U1 0
U2 11
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2018
VL 34
IS 3
BP 431
EP 442
DI 10.1007/s00371-017-1350-4
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FW2CZ
UT WOS:000425110900010
DA 2024-07-18
ER

PT J
AU Kang, H
   Han, J
AF Kang, HyeongYeop
   Han, Junghyun
TI Feature-preserving procedural texture
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 34th Conference on Computer Graphics International (CGI)
CY JUN 27-30, 2017
CL Yokohama, JAPAN
SP Keio Univ, Fac Sci & Technol
DE Procedural texturing; Feature preservation; Texture analysis; Noise by
   example
ID PHASE; NOISE; IMAGES
AB This paper presents how to synthesize a texture in a procedural way that preserves the features of the input exemplar. The exemplar is analyzed in both spatial and frequency domains to be decomposed into feature and non-feature parts. Then, the non-feature parts are reproduced as a procedural noise, whereas the features are independently synthesized. They are combined to output a non-repetitive texture that also preserves the exemplar's features. The proposed method allows the user to control the extent of extracted features and also enables a texture to edited quite effectively.
C1 [Kang, HyeongYeop] Korea Univ, Comp Sci Dept, Seoul, South Korea.
   [Han, Junghyun] Korea Univ, Dept Comp Sci, Interact Media Lab 3D, Seoul, South Korea.
C3 Korea University; Korea University
RP Han, J (corresponding author), Korea Univ, Dept Comp Sci, Interact Media Lab 3D, Seoul, South Korea.
EM siamiz88@gmail.com; jhan@korea.ac.kr
RI Kang, HyeongYeop/AAJ-2471-2020
OI Kang, HyeongYeop/0000-0001-5292-4342
FU National Research Foundation of Korea (NRF) - Korea government (MSIP)
   [NRF-2016R1A2B3014319]; Institute for Information and Communications
   Technology Promotion (IITP) - Korea government (MSIP) [R0115-16-1011]
FX This work was supported by the National Research Foundation of Korea
   (NRF) Grant funded by the Korea government (MSIP) (No.
   NRF-2016R1A2B3014319) and by Institute for Information and
   Communications Technology Promotion (IITP) Grant funded by the Korea
   government (MSIP) (No. R0115-16-1011).
CR Cook RL, 2005, ACM T GRAPHIC, V24, P803, DOI 10.1145/1073204.1073264
   Dischler JM, 1997, COMPUT GRAPH FORUM, V16, pC129, DOI 10.1111/1467-8659.00149
   Efros AA, 2001, COMP GRAPH, P341, DOI 10.1145/383259.383296
   Galerne B, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185569
   Gardner G. Y., 1985, ACM SIGRRAPH COMPUTE, V19, P297
   GHAZANFARPOUR D, 1995, COMPUT GRAPH-UK, V19, P413, DOI 10.1016/0097-8493(95)00011-Z
   Gilet G, 2012, VISUAL COMPUT, V28, P679, DOI 10.1007/s00371-012-0711-2
   Gilet G, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2661229.2661249
   Goldberg A, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360653
   HORNER JL, 1984, APPL OPTICS, V23, P812, DOI 10.1364/AO.23.000812
   Kaspar A, 2015, COMPUT GRAPH FORUM, V34, P349, DOI 10.1111/cgf.12565
   Lagae A, 2010, COMPUT GRAPH FORUM, V29, P2579, DOI 10.1111/j.1467-8659.2010.01827.x
   Lagae A, 2010, COMPUT GRAPH-UK, V34, P312, DOI 10.1016/j.cag.2010.05.004
   Lagae A, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531360
   LEVI A, 1983, J OPT SOC AM, V73, P810, DOI 10.1364/JOSA.73.000810
   Lewis J. P., 1989, Computer Graphics, V23, P263, DOI 10.1145/74334.74360
   Lockerman YD, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925964
   MORRONE MC, 1988, PROC R SOC SER B-BIO, V235, P221, DOI 10.1098/rspb.1988.0073
   Nicoll A, 2005, COMPUT GRAPH FORUM, V24, P569, DOI 10.1111/j.1467-8659.2005.00882.x
   OPPENHEIM AV, 1981, P IEEE, V69, P529, DOI 10.1109/PROC.1981.12022
   Perlin K., 1985, Computer Graphics, V19, P287, DOI 10.1145/325165.325247
   PIOTROWSKI LN, 1982, PERCEPTION, V11, P337, DOI 10.1068/p110337
   Reinhard E, 2001, IEEE COMPUT GRAPH, V21, P34, DOI 10.1109/38.946629
   Ruderman DL, 1998, J OPT SOC AM A, V15, P2036, DOI 10.1364/JOSAA.15.002036
   Wu FZ, 2016, VISUAL COMPUT, V32, P43, DOI 10.1007/s00371-014-1054-y
NR 25
TC 2
Z9 2
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2017
VL 33
IS 6-8
BP 761
EP 768
DI 10.1007/s00371-017-1375-8
PG 8
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA EX1EY
UT WOS:000402964800008
DA 2024-07-18
ER

PT J
AU Kim, T
   Hong, E
   Im, J
   Yang, D
   Kim, Y
   Kim, CH
AF Kim, TaeHyeong
   Hong, Eunki
   Im, Jaeho
   Yang, Dohyeon
   Kim, Youngbin
   Kim, Chang-Hun
TI Visual simulation of fire-flakes synchronized with flame
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 34th International Conference on Computer Graphics (CGI)
CY JUN 27-30, 2017
CL Yokohama, JAPAN
SP Keio Univ, Fac Sci & Technol
DE Fire-flake; Flame; Fire; Visual simulation; Animation
AB We propose a framework for generating, animating, and controlling fire-flakes to correspond with the movements of flame. Not only do fire-flakes themselves display a distinctively complex and elaborate movement, but also they are heavily influenced by where the heat transforms into flame and how the flame moves. It is difficult for designers to generate and simulate synchronous movement of fire-flakes and flame because the movement of flame tends to be chaotic and constantly changing. We provide a novel framework that can improve on the current processes both effectively and efficiently. We first propose a fire-flake motion model that simulates movement harmoniously with the movement of flame. We then introduce automatic generation of fire-flakes by analyzing temperature fields and the velocity fields input from the fire simulation. Lastly, our sample-based control method enables the formation of a target shape by gathering fire-flakes together into one location while maintaining their inherent movement without external forces. We also provide various parameters that the designer can use to control the effects of the forces determining the amount, velocity, and movement of fire-flakes. To the best of our knowledge, this work is the first visual simulation method to generate fire-flakes in coordination with flame, and we believe it will contribute not only to research, but also to the animation industry.
C1 [Kim, TaeHyeong; Yang, Dohyeon; Kim, Youngbin] Korea Univ, Interdisciplinary Program Visual Informat Proc, Seoul, South Korea.
   [Hong, Eunki; Im, Jaeho; Kim, Chang-Hun] Korea Univ, Dept Comp & Radio Commun Engn, Seoul, South Korea.
C3 Korea University; Korea University
RP Kim, CH (corresponding author), Korea Univ, Dept Comp & Radio Commun Engn, Seoul, South Korea.
EM chkim@korea.ac.kr
OI Kim, TaeHyeong/0000-0002-1862-9208
CR [Anonymous], SIGGRAPH 08 ACM SIGG
   Beaudoin P., 2001, P GRAPH INTERFACE, P159
   Bridson R., 2007, P ACM SIGGRAPH 2007, DOI DOI 10.1145/1275808.1276435
   Feldman BE, 2003, ACM T GRAPHIC, V22, P708, DOI 10.1145/882262.882336
   Hong JM, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239498
   Horvath C, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531347
   Kawada G., 2011, P 2011 ACM SIGGRAPHE, P167
   Kim PR, 2012, VISUAL COMPUT, V28, P597, DOI 10.1007/s00371-012-0696-x
   Kim T, 2016, VISUAL COMPUT, V32, P871, DOI 10.1007/s00371-016-1267-3
   Kwatra Nipun., 2010, ACM SIGGRAPH/Eurographics Symp. on Comput. Anim, P207
   Lamorlette A, 2002, ACM T GRAPHIC, V21, P729, DOI 10.1145/566570.566644
   Melek Z, 2002, 10TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P431, DOI 10.1109/PCCGA.2002.1167889
   Nguyen DQ, 2002, ACM T GRAPHIC, V21, P721, DOI 10.1145/566570.566643
   Selle A, 2005, ACM T GRAPHIC, V24, P910, DOI 10.1145/1073204.1073282
   Stam J, 1999, COMP GRAPH, P121, DOI 10.1145/311535.311548
   Stam J., 1993, Computer Graphics Proceedings, P369, DOI 10.1145/166117.166163
   Zhao HL, 2009, COMPUT ANIMAT VIRT W, V20, P185, DOI 10.1002/cav.287
NR 17
TC 5
Z9 5
U1 0
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2017
VL 33
IS 6-8
BP 1029
EP 1038
DI 10.1007/s00371-017-1374-9
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA EX1EY
UT WOS:000402964800032
DA 2024-07-18
ER

PT J
AU Yang, M
   Li, XS
   Liu, YQ
   Yang, G
   Wu, EH
AF Yang, Meng
   Li, Xiaosheng
   Liu, Youquan
   Yang, Gang
   Wu, Enhua
TI A novel surface tension formulation for SPH fluid simulation
SO VISUAL COMPUTER
LA English
DT Article
DE Fluid simulation; Surface tension; Smoothed particle hydrodynamics
AB Surface tension plays a significant role in fluid simulation, especially small-scale fluid. In this paper, we present a novel surface tension formulation for smoothed particle hydrodynamics (SPH) to simulate interfacial fluid flow. The surface tension formulation is decomposed into three main processes: (1) volume-preserved mesh smoothing, (2) surface tension computation and (3) surface tension transfer. Firstly, we exploit a Lagrangian operator to smooth an initial three-dimensional discrete interfacial surface mesh generated from fluid particles; and then the surface mesh is scaled in a volume-preserved way and the center is translated to its original position to get a smoothed mesh. Secondly, surface tension strengths on the vertices of the interfacial surface mesh are computed according to the offsets from the original surface mesh to the smoothed mesh. Finally, we transfer the surface tension strengths from the mesh vertices onto their neighbor fluid particles in a conservative way. The proposed surface tension solver is simple and straightforward to be plugged into a standard SPH solver. Experimental results show that it is effective and efficient to produce realistic fluid simulations, especially for the phenomena with strong surface tension.
C1 [Yang, Meng; Yang, Gang] Beijing Forestry Univ, Sch Informat Sci & Technol, Beijing, Peoples R China.
   [Yang, Meng; Li, Xiaosheng; Wu, Enhua] Chinese Acad Sci, State Key Lab Comp Sci, Inst Software, Beijing, Peoples R China.
   [Wu, Enhua] Univ Macau, Fac Sci & Technol, Dept Comp & Informat Sci, Macau, Peoples R China.
   [Li, Xiaosheng; Wu, Enhua] Univ Chinese Acad Sci, Beijing, Peoples R China.
   [Liu, Youquan] Changan Univ, Sch Informat Engn, Xian, Peoples R China.
C3 Beijing Forestry University; Chinese Academy of Sciences; Institute of
   Software, CAS; University of Macau; Chinese Academy of Sciences;
   University of Chinese Academy of Sciences, CAS; Chang'an University
RP Yang, M (corresponding author), Beijing Forestry Univ, Sch Informat Sci & Technol, Beijing, Peoples R China.; Yang, M (corresponding author), Chinese Acad Sci, State Key Lab Comp Sci, Inst Software, Beijing, Peoples R China.
EM yangmeng@bjfu.edu.cn; lixs@ios.ac.cn; youquan@chd.edu.cn;
   yanggang@bjfu.edu.cn; EHWu@umac.mo
OI YANG, Meng/0000-0001-6439-2873
FU Fundamental Research Funds for the Central Universities [BLX2012049,
   2015ZCQ-XX]; National Natural Science Foundation of China [61402038,
   61100132]; CCF-Tencent Open Fund
FX The authors sincerely thank the anonymous reviewers for their kind
   suggestions. The authors also thank Xuemin Liu and Cheng Wang at the
   School of Information Science and Technology, Beijing Forestry
   University, for their hard work on the final result rendering and to
   Dong Li for his careful grammar revision. The work in this paper has
   been supported by the Fundamental Research Funds for the Central
   Universities (Nos. BLX2012049, 2015ZCQ-XX), National Natural Science
   Foundation of China (Nos. 61402038, 61100132), CCF-Tencent Open Fund in
   2014.
CR Adams B, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1276377.1276437, 10.1145/1239451.1239499]
   Akinci N, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508395
   Becker M., 2007, P S COMP AN, P6372
   BETCHELOR GK, 1967, INTRO FLUID DYNAMICS
   Brochu T, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778784
   Colagrossi A, 2003, J COMPUT PHYS, V191, P448, DOI 10.1016/S0021-9991(03)00324-3
   Desbrun M, 1999, COMP GRAPH, P317, DOI 10.1145/311535.311576
   Erleben K., 2011, P S COMP AN
   FUJIWARA K, 1995, P AM MATH SOC, V123, P2585, DOI 10.2307/2161293
   Hong JM, 2005, ACM T GRAPHIC, V24, P915, DOI 10.1145/1073204.1073283
   Hu XY, 2007, J COMPUT PHYS, V227, P264, DOI 10.1016/j.jcp.2007.07.013
   Hu XY, 2009, J COMPUT PHYS, V228, P2082, DOI 10.1016/j.jcp.2008.11.027
   Hu XY, 2006, J COMPUT PHYS, V213, P844, DOI 10.1016/j.jcp.2005.09.001
   Ihmsen Markus, 2014, P 35 ANN C EUR ASS C, DOI [10.2312/egst.20141034, DOI 10.2312/EGST.20141034]
   Lorensen W. E., 1987, COMPUTER GRAPHICS, V21, P163, DOI 10.1145/37401.37422
   Losasso F, 2004, ACM T GRAPHIC, V23, P457, DOI 10.1145/1015706.1015745
   Misztal M., 2010, P VIRT REAL INT PHYS, p[1120, 2, 3]
   Monaghan JJ, 2005, REP PROG PHYS, V68, P1703, DOI 10.1088/0034-4885/68/8/R01
   MONAGHAN JJ, 1994, J COMPUT PHYS, V110, P399, DOI 10.1006/jcph.1994.1034
   Morris JP, 2000, INT J NUMER METH FL, V33, P333, DOI 10.1002/1097-0363(20000615)33:3<333::AID-FLD11>3.0.CO;2-7
   Muller M., 2003, INL P S COMP AN
   Muller M., 2005, P ACM SIGGRAPH EUR S
   Muller M., 2003, P ACM SIGGRAPH EUR S
   Myungjoo Kang, 2000, Journal of Scientific Computing, V15, P323, DOI 10.1023/A:1011178417620
   Nils T., 2010, ACM T GRAPHIC, V29
   Nugent S, 2000, PHYS REV E, V62, P4968, DOI 10.1103/PhysRevE.62.4968
   Olenthaler B.S., 2008, P ACM SIGGRAPH EUR S
   Rackbill J.U.B., 1992, J COMPUT PHYS, V2
   Sussman M, 2009, SIAM J SCI COMPUT, V31, P2447, DOI 10.1137/080732122
   Tartakovsky A, 2005, PHYS REV E, V72, DOI 10.1103/PhysRevE.72.026301
   Yu JH, 2012, COMPUT GRAPH FORUM, V31, P815, DOI 10.1111/j.1467-8659.2012.03062.x
   Zhang MY, 2012, COMPUT FLUIDS, V59, P61, DOI 10.1016/j.compfluid.2012.02.017
   Zheng W., 2006, Proc. of the ACM Siggraph/Eurographics Symposium on Computer Animation, P325
NR 33
TC 6
Z9 8
U1 0
U2 28
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2017
VL 33
IS 5
BP 597
EP 606
DI 10.1007/s00371-016-1274-4
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ER4KB
UT WOS:000398767300005
DA 2024-07-18
ER

PT J
AU Murtza, I
   Abdullah, D
   Khan, A
   Arif, M
   Mirza, SM
AF Murtza, Iqbal
   Abdullah, Duraid
   Khan, Asifullah
   Arif, Muhammad
   Mirza, Sikandar Majeed
TI Cortex-inspired multilayer hierarchy based object detection system using
   PHOG descriptors and ensemble classification
SO VISUAL COMPUTER
LA English
DT Article
DE Object detection; Brain's cortex; Ensemble classification; Feature
   extraction
ID RECOGNITION
AB In this paper, a hierarchal feature extraction and ensemble classification-based framework for object detection is proposed. The proposed object detection technique is motivated by the hierarchical learning mechanism in primate visual cortex, where each layer processes information differently. Initially, pyramid histogram of oriented gradients (PHOG) based descriptors are selected to generate shift and scale invariant descriptors of an image. PHOG-based feature descriptors are then processed in multi-layered hierarchy, following feed forward models in brain's visual cortex and exploited through ensemble classification techniques. The proposed cortex-inspired ensemble-based object detection (CI-EnsOD) system exploits hierarchical learning mechanism of visual cortex and it is computationally efficient compared to the existing cortex-inspired models. In addition, it reduces feature dimensionality and offers improved object detection performance. The performance of proposed technique is demonstrated using three publically available standard datasets. It is shown experimentally that the prototype selection in the proposed CI-EnsOD can be improved using k-means clustering. The obtained experimental results show that the proposed CI-EnsOD technique is more accurate and efficient than contemporary cortex-inspired object detection techniques. Finally, it is also observed that the proposed technique is capable of providing compact descriptors compared to principle component analysis and independent component analysis.
C1 [Murtza, Iqbal; Abdullah, Duraid; Khan, Asifullah] Pakistan Inst Engn & Appl Sci, Pattern Recognit Lab, Dept Comp & Informat Sci, Islamabad, Pakistan.
   [Abdullah, Duraid; Arif, Muhammad] Pakistan Inst Engn & Appl Sci, Dept Elect Engn, Islamabad, Pakistan.
   [Mirza, Sikandar Majeed] Pakistan Inst Engn & Appl Sci, Dept Phys & Appl Math, Islamabad, Pakistan.
C3 Pakistan Institute of Engineering & Applied Science; Pakistan Institute
   of Engineering & Applied Science; Pakistan Institute of Engineering &
   Applied Science
RP Khan, A (corresponding author), Pakistan Inst Engn & Appl Sci, Pattern Recognit Lab, Dept Comp & Informat Sci, Islamabad, Pakistan.
EM asif@pieas.edu.pk
RI Khan, ASIFULLAH/H-9617-2015
OI Murtza, Iqbal/0000-0001-7541-8581; Khan, Asifullah/0000-0003-2039-5305
CR Abdullah D, 2013, 2013 16TH INTERNATIONAL MULTI TOPIC CONFERENCE (INMIC), P160, DOI 10.1109/INMIC.2013.6731343
   Amit Y, 2003, VISION RES, V43, P2073, DOI 10.1016/S0042-6989(03)00306-7
   [Anonymous], 2000, MIT CBCL PED DAT
   [Anonymous], 2003, CALT MOT SID DAT
   [Anonymous], 2014, ARXIV14123684
   Bar M, 2003, J COGNITIVE NEUROSCI, V15, P600, DOI 10.1162/089892903321662976
   Barla A, 2003, 2003 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL 3, PROCEEDINGS, P513
   Bosch A., 2007, P 6 ACM INT C IMAGE, P401, DOI DOI 10.1145/1282280.1282340
   Cadieu CF, 2014, PLOS COMPUT BIOL, V10, DOI 10.1371/journal.pcbi.1003963
   Chopra S, 2005, PROC CVPR IEEE, P539, DOI 10.1109/cvpr.2005.202
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Dalal N., 2005, INRIA PERSON DATASET
   DiCarlo JJ, 2012, NEURON, V73, P415, DOI 10.1016/j.neuron.2012.01.010
   Gabor D., 1946, Journal of the Institution of Electrical Engineers-part III: radio and communication engineering, V93, P429, DOI [DOI 10.1049/JI-3-2.1946.0074, 10.1049/ji-3-2.1946.0074]
   Haushofer J, 2007, NEURON, V53, P773, DOI 10.1016/j.neuron.2007.03.003
   Heisele B, 2002, ADV NEUR IN, V14, P1239
   Hyvärinen A, 2000, NEURAL NETWORKS, V13, P411, DOI 10.1016/S0893-6080(00)00026-5
   Jafri R, 2014, VISUAL COMPUT, V30, P1197, DOI 10.1007/s00371-013-0886-1
   Jiang XB, 2014, VISUAL COMPUT, V30, P1021, DOI 10.1007/s00371-014-0923-8
   Jolliffe I., 2005, ENCY STAT BEHAV SCI, P1, DOI [10.1002/0470013192.bsa501, DOI 10.1002/0470013192.BSA501]
   Jolliffe I.T., 1986, Principal component analysis, DOI DOI 10.1016/0169-7439(87)80084-9
   JONES JP, 1987, J NEUROPHYSIOL, V58, P1187, DOI 10.1152/jn.1987.58.6.1187
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   LeCun Y, 2004, PROC CVPR IEEE, P97
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   McManus JNJ, 2011, P NATL ACAD SCI USA, V108, P9739, DOI 10.1073/pnas.1105855108
   Panzoli D, 2010, VISUAL COMPUT, V26, P353, DOI 10.1007/s00371-010-0424-3
   PERRETT DI, 1993, IMAGE VISION COMPUT, V11, P317, DOI 10.1016/0262-8856(93)90011-5
   Riesenhuber M, 1999, NAT NEUROSCI, V2, P1019, DOI 10.1038/14819
   Rokach L, 2010, ARTIF INTELL REV, V33, P1, DOI 10.1007/s10462-009-9124-7
   Serre T, 2005, PROC CVPR IEEE, P994
   SERRE T, 2005, 2005036CBCL AI MIT
   Serre T, 2007, IEEE T PATTERN ANAL, V29, P411, DOI 10.1109/TPAMI.2007.56
   Solgi M, 2009, IEEE T AUTON MENT DE, V1, P238, DOI 10.1109/TAMD.2009.2038360
   Stone J.V., 2005, ENCY STAT BEHAV SCI
   SWAIN MJ, 1991, INT J COMPUT VISION, V7, P11, DOI 10.1007/BF00130487
   Thedoridis S., 2009, PATTERN RECOGNITION
   Thorpe S, 2002, LECT NOTES COMPUT SC, V2525, P1
   Tipping ME, 1999, J R STAT SOC B, V61, P611, DOI 10.1111/1467-9868.00196
   van de Sande KEA, 2010, IEEE T PATTERN ANAL, V32, P1582, DOI 10.1109/TPAMI.2009.154
   Vishwakarma S, 2013, VISUAL COMPUT, V29, P983, DOI 10.1007/s00371-012-0752-6
   Watanabe T, 2009, LECT NOTES COMPUT SC, V5414, P37, DOI 10.1007/978-3-540-92957-4_4
   Wersing H, 2003, NEURAL COMPUT, V15, P1559, DOI 10.1162/089976603321891800
   Zhang C, 2012, ADV IND CONTROL, P1, DOI 10.1007/978-1-4471-2224-1
NR 44
TC 9
Z9 9
U1 0
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2017
VL 33
IS 1
BP 99
EP 112
DI 10.1007/s00371-015-1155-2
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA EI2JM
UT WOS:000392313200010
DA 2024-07-18
ER

PT J
AU Catala, A
   Oliver, M
   Molina, JP
   Gonzalez, P
AF Catala, Alejandro
   Oliver, Miguel
   Pascual Molina, Jose
   Gonzalez, Pascual
TI Involving multiple fingers in exploring a haptic surface: an evaluation
   study
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 33rd Conference on Computer Graphics International (CGI)
CY JUN 28-JUL 01, 2016
CL Heraklion, GREECE
SP Fdn Res Technol
DE Human computer interaction; Haptic surface; Haptics; Multi-point
   feedback; Touch; Tactile search
ID SERIAL SEARCH; TACTILE; FEEDBACK; TOUCH
AB In most haptic search tasks, tactile stimuli are usually presented to the fingers to discriminate simulated features and identify patterns. In this paper, we focus on a more complex exploration task in which users have to discriminate different stimuli, move their fingers in a free way to find and locate an object in a wider area of exploration and integrate all the perceived information to determine the position of the object. The study explores how users perform this haptic search task involving one or two fingers on a surface area. In order to carry out this research and overcome limitations of current hardware approaches to multi-point haptic surfaces, we used a setting consisting of a capacitive multi-touch screen and a general-purpose wearable vibrotactile device designed in our laboratory. The results indicate that using one finger in one hand shows to be more effective than using two fingers in either one or two hands in the task under study. Users showed higher confidence, lower exploration times, higher amount of right answers and higher exploration speed. This suggests that great efforts in providing independent multi-point haptic surface hardware could not be a priority for this kind of exploration task.
C1 [Catala, Alejandro; Oliver, Miguel; Pascual Molina, Jose; Gonzalez, Pascual] Univ Castilla La Mancha, LoUISE Res Grp, Comp Syst Dept, Avda Espana S-N, Albacete 02071, Spain.
C3 Universidad de Castilla-La Mancha
RP Gonzalez, P (corresponding author), Univ Castilla La Mancha, LoUISE Res Grp, Comp Syst Dept, Avda Espana S-N, Albacete 02071, Spain.
EM pascual.gonzalez@uclm.es
RI Catala, Alejandro/ABE-6981-2021; González, Pascual/E-3693-2016
OI González, Pascual/0000-0003-3549-5712; Oliver Segovia,
   Miguel/0000-0002-4761-6764; Catala, Alejandro/0000-0002-3677-672X;
   Molina Masso, Jose Pascual/0000-0001-6832-3250
CR [Anonymous], ADDING VIBROTACTILE
   [Anonymous], COMPUTER
   [Anonymous], 2014, HEIDEGGERS SPENGLERR
   Assumpçao L, 2015, ATTEN PERCEPT PSYCHO, V77, P1212, DOI 10.3758/s13414-015-0848-y
   Baldwin R L., 2012, Metagenomics, pb1, DOI [10.4303/mg/235571, DOI 10.4303/MG/235571]
   Bau O., 2010, P 23 ANN ACM S US IN, P283, DOI DOI 10.1145/1866029.1866074
   Burnett GE, 2001, INT J HUM-COMPUT ST, V55, P521, DOI 10.1006/ijhc.2001.0482
   Carter T., 2013, P 26 ANN ACM S US IN, P505
   Cockburn A, 2015, ACM COMPUT SURV, V47, DOI 10.1145/2659796
   Dandekar K, 2003, J BIOMECH ENG-T ASME, V125, P682, DOI 10.1115/1.1613673
   Hart S. G., 2006, P HUM FACT ERG SOC A, V50, P904, DOI DOI 10.1177/154193120605000909
   Hoggan E., 2007, Proc. SIGCHI Human Factors in Computing Systems, P2417
   Hou XY, 2013, VISUAL COMPUT, V29, P1063, DOI 10.1007/s00371-013-0838-9
   Israr A., 2012, P 2012 ACM ANN C EXT, P1571, DOI DOI 10.1145/2212776.2223674
   Jansen Y., 2010, Proc. of ACM Int. Conf. on ITS, P11
   Kim S.-C., 2013, P 26 ANN ACM S US IN, P531, DOI DOI 10.1145/2501988.2502020
   Kuchenbecker KJ, 2006, IEEE T VIS COMPUT GR, V12, P219, DOI 10.1109/TVCG.2006.32
   Leiva LA, 2015, CHI 2015: PROCEEDINGS OF THE 33RD ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P669, DOI 10.1145/2702123.2702388
   MacLean K.E., 2008, REV HUMAN FACTORS ER, V4, P149, DOI [DOI 10.1518/155723408X342826, 10.1518/155723408X342826]
   Martínez J, 2016, IEEE COMPUT GRAPH, V36, P42, DOI 10.1109/MCG.2014.81
   Martínez J, 2014, INT J HUM-COMPUT INT, V30, P855, DOI 10.1080/10447318.2014.941272
   Martínez J, 2013, VISUAL COMPUT, V29, P111, DOI 10.1007/s00371-012-0716-x
   Overvliet KE, 2007, PERCEPT PSYCHOPHYS, V69, P1059, DOI 10.3758/BF03193944
   Overvliet KE, 2007, EXP BRAIN RES, V182, P427, DOI 10.1007/s00221-007-0998-9
   Overvliet KE, 2010, EXP BRAIN RES, V202, P261, DOI 10.1007/s00221-009-2127-4
   Pitts MJ, 2012, DISPLAYS, V33, P7, DOI 10.1016/j.displa.2011.09.002
   Poupyrev I., 2004, CHI 04 HUM FACT COMP, P1309, DOI DOI 10.1145/985921.986051
   Poupyrev I., 2003, UIST 03, P217, DOI DOI 10.1145/964696.964721
   Robles-De-La-Torre G, 2006, IEEE MULTIMEDIA, V13, P24, DOI 10.1109/MMUL.2006.69
   Shin H, 2014, ETRI J, V36, P979, DOI 10.4218/etrij.14.0114.0028
   Ware J, 2014, IEEE T HAPTICS, V7, P545, DOI 10.1109/TOH.2014.2323257
   Xiaowei Dai, 2012, 2012 IEEE Haptics Symposium (HAPTICS), P7, DOI 10.1109/HAPTIC.2012.6183753
NR 32
TC 2
Z9 2
U1 1
U2 10
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2016
VL 32
IS 6-8
BP 921
EP 932
DI 10.1007/s00371-016-1250-z
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA DP8ET
UT WOS:000378731600024
DA 2024-07-18
ER

PT J
AU Iglesias, A
   Gálvez, A
   Avila, A
AF Iglesias, Andres
   Galvez, Akemi
   Avila, Andreina
TI Hybridizing mesh adaptive search algorithm and artificial immune systems
   for discrete rational B,zier curve approximation
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT International Conference on Cyberworlds (CW)
CY OCT 06-08, 2014
CL Santander, SPAIN
SP IEEE Comp Soc, Univ Cantabria, Comp Graph & Geometr Modeling Grp, Toho Univ, Fac Sci, Dept Informat Sci, European Assoc Comp Graph, Int Federat Informat Proc, Workgroup 5 10 Comp Graph & Virtual Worlds, Univ Cantabria, Dept Appl Math & Computatl Sci, Vice Rector Res & Knowledge Transfer, Municipal Santander, Reg Govt Cantabria, Spanish Minist Econ & Competitiveness, Cantabria Campus Int, Int Federat Informat Proc, Tech Comm 5 Informat Technol Applicat
DE Mesh adaptive search algorithm; Artificial immune systems; Hybrid
   optimization methods; Discrete data approximation; Rational Bezier
   curves
ID PARTICLE SWARM OPTIMIZATION; B-SPLINE CURVES; NEURAL-NETWORK; FUNCTIONAL
   NETWORKS; POINT CLOUDS; RECONSTRUCTION; SURFACES; PARAMETERIZATION;
   ADJUSTMENT
AB This paper is an extension of a previous one presented at the conference Cyberworlds 2014. In that work we addressed the problem of obtaining the rational B,zier curve that fits a given set of data points better in the least-squares sense. Our approach was based on the clonal selection theory principles to compute all parameters of the problem, namely, the control points of the approximating curve, their corresponding weights, and a suitable parameterization of data points. Although we were able to obtain results with good accuracy, this scheme can still be significantly improved by hybridizing it with an efficient local search procedure. This is the approach proposed in this paper. In particular, we consider the mesh adaptive search algorithm, a direct search method aimed at improving the local search step to refine the quality of the solution. This hybrid strategy has been applied to six illustrative free-form shapes exhibiting challenging features, including the three examples in previous paper. A comparative analysis of our results with respect to the previous methodology is also reported. Our experimental results show that this hybrid scheme performs extremely well. It also outperforms the previous approach for all instances in our benchmark.
C1 [Iglesias, Andres; Galvez, Akemi; Avila, Andreina] Univ Cantabria, Dept Appl Math & Comp Sci, ETSI Caminos Canales & Puertos, Avda Castros S-N, Santander 39005, Spain.
   [Iglesias, Andres] Toho Univ, Fac Sci, Dept Informat Sci, Narashino Campus,2-2-1 Miyama, Funabashi, Chiba 2748510, Japan.
C3 Universidad de Cantabria; Toho University
RP Iglesias, A (corresponding author), Univ Cantabria, Dept Appl Math & Comp Sci, ETSI Caminos Canales & Puertos, Avda Castros S-N, Santander 39005, Spain.; Iglesias, A (corresponding author), Toho Univ, Fac Sci, Dept Informat Sci, Narashino Campus,2-2-1 Miyama, Funabashi, Chiba 2748510, Japan.
EM iglesias@unican.es
RI Iglesias, Andres/E-6784-2013; Galvez, Akemi/E-6787-2013
OI Iglesias, Andres/0000-0002-5672-8274; Galvez, Akemi/0000-0002-2100-2289
CR Andreina A., 2013, STUDIES COMPUTATIONA, V441, P59
   Audet C, 2006, SIAM J OPTIMIZ, V17, P188, DOI 10.1137/040603371
   Barhak J, 2001, IEEE T VIS COMPUT GR, V7, P1, DOI 10.1109/2945.910817
   Barnhill R.E., 1992, Geometric processing for design and manufacturing
   Castillo E, 1997, ACM T GRAPHIC, V16, P296, DOI 10.1145/256157.256161
   Dasgupta D., 1999, Artificial Immune Systems and their Applications
   de Castro L.N., 1999, 0199 RT DCA
   de Castro LeandroN., 2002, ARTIFICIAL IMMUNE SY
   de Castro LN, 2002, IEEE T EVOLUT COMPUT, V6, P239, DOI 10.1109/TEVC.2002.1011539
   Dierckx P., 1993, Curve and Surface Fitting with Splines
   Echevarría G, 2002, LECT NOTES COMPUT SC, V2330, P305
   Farin G., 2001, Curves and Surfaces for CAGD: A Practical Guide, Vfifth
   Fister I, 2015, CHAOS SOLITON FRACT, V73, P29, DOI 10.1016/j.chaos.2014.12.019
   Galvez A., INT J BIOIN IN PRESS
   Galvez A., 2013, P INT C COMP SCI ICC
   Galvez A., 2013, P INT C COMP SCI APP
   Galvez A, 2008, LECT NOTES COMPUT SC, V5102, P116, DOI 10.1007/978-3-540-69387-1_13
   Gálvez A, 2007, LECT NOTES COMPUT SC, V4706, P680
   Gálvez A, 2014, 2014 INTERNATIONAL CONFERENCE ON CYBERWORLDS (CW), P221, DOI 10.1109/CW.2014.38
   Gálvez A, 2013, MATH PROBL ENG, V2013, DOI 10.1155/2013/528215
   Galvez A, 2015, APPL SOFT COMPUT, V26, P90, DOI 10.1016/j.asoc.2014.09.030
   Gálvez A, 2014, SCI WORLD J, DOI 10.1155/2014/138760
   Gálvez A, 2013, SCI WORLD J, DOI 10.1155/2013/283919
   Gálvez A, 2013, J APPL MATH, DOI 10.1155/2013/237984
   Gálvez A, 2013, APPL SOFT COMPUT, V13, P1491, DOI 10.1016/j.asoc.2012.05.030
   Gálvez A, 2011, COMPUT AIDED DESIGN, V43, P1683, DOI 10.1016/j.cad.2011.07.010
   Gálvez A, 2012, INFORM SCIENCES, V182, P56, DOI 10.1016/j.ins.2010.09.031
   Gálvez A, 2012, INFORM SCIENCES, V192, P174, DOI 10.1016/j.ins.2010.11.007
   GU P, 1995, COMPUT AIDED DESIGN, V27, P59, DOI 10.1016/0010-4485(95)90753-3
   Hoffmann M, 2005, NUMER ALGORITHMS, V39, P175, DOI 10.1007/s11075-004-3628-7
   Iglesias A, 2004, FUTURE GENER COMP SY, V20, P1337, DOI 10.1016/j.future.2004.05.025
   Iglesias A, 2001, LECT NOTES ARTIF INT, V1930, P200
   Iglesias A, 2001, COMPUTER GRAPHICS INTERNATIONAL 2001, PROCEEDINGS, P329, DOI 10.1109/CGI.2001.934692
   Iglesias A., 2008, P INT C CONV INF TEC
   Iglesias A, 2014, MATH PROBL ENG, V2014, DOI 10.1155/2014/351648
   Jing L, 2005, PROCEEDINGS OF THE 2005 INTERNATIONAL CONFERENCE ON NEURAL NETWORKS AND BRAIN, VOLS 1-3, P905
   JUPP DLB, 1978, SIAM J NUMER ANAL, V15, P328, DOI 10.1137/0715022
   Knopf GK, 2001, ENG APPL ARTIF INTEL, V14, P577, DOI 10.1016/S0952-1976(01)00037-9
   Loucera C, 2014, 2014 INTERNATIONAL CONFERENCE ON CYBERWORLDS (CW), P182, DOI 10.1109/CW.2014.33
   LYCHE T, 1988, IMA J NUMER ANAL, V8, P185, DOI 10.1093/imanum/8.2.185
   MA WY, 1995, COMPUT AIDED DESIGN, V27, P663, DOI 10.1016/0010-4485(94)00018-9
   Park H, 2004, COMPUT AIDED GEOM D, V21, P479, DOI 10.1016/j.cagd.2004.03.003
   Park H, 2007, COMPUT AIDED DESIGN, V39, P439, DOI 10.1016/j.cad.2006.12.006
   Patrikalakis N., 2002, Shape Interrogation for Computer Aided Design and Manufacturing
   Piegl L., 1997, The Nurbs Book, Vsecond
   Pottmann H, 2005, COMPUT AIDED DESIGN, V37, P751, DOI 10.1016/j.cad.2004.08.013
   Powell M.J.D., 1970, NUMERICAL APPROXIMAT
   Rice JR, 1969, APPROXIMATION FUNCTI, V2
   Sarfraz M, 2001, FIFTH INTERNATIONAL CONFERENCE ON INFORMATION VISUALISATION, PROCEEDINGS, P738, DOI 10.1109/IV.2001.942138
   Ülker E, 2009, INFORM SCIENCES, V179, P1483, DOI 10.1016/j.ins.2008.11.037
   Varady T., 2002, Handbook of Computer Aided Geometric Design
   Wang WP, 2006, ACM T GRAPHIC, V25, P214, DOI 10.1145/1138450.1138453
   Yang HP, 2004, COMPUT AIDED DESIGN, V36, P639, DOI 10.1016/S0010-4485(03)00140-4
   Yoshimoto F, 2003, COMPUT AIDED DESIGN, V35, P751, DOI 10.1016/S0010-4485(03)00006-X
   Yoshimoto F, 1999, SHAPE MODELING INTERNATIONAL '99 - INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, PROCEEDINGS, P162, DOI 10.1109/SMA.1999.749336
   Zhao XY, 2011, COMPUT AIDED DESIGN, V43, P598, DOI 10.1016/j.cad.2011.01.015
NR 56
TC 5
Z9 5
U1 1
U2 11
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2016
VL 32
IS 3
BP 393
EP 402
DI 10.1007/s00371-015-1181-0
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA DF9FK
UT WOS:000371666200012
DA 2024-07-18
ER

PT J
AU Chen, CB
   Liang, H
   Zhao, SR
   Lyu, ZH
   Sarem, M
AF Chen, Chuanbo
   Liang, Hu
   Zhao, Shengrong
   Lyu, Zehua
   Sarem, Mudar
TI A novel multi-image super-resolution reconstruction method using
   anisotropic fractional order adaptive norm
SO VISUAL COMPUTER
LA English
DT Article
DE Anisotropic fractional order adaptive norm; Multi-image super-resolution
   reconstruction; Gradient descent method
ID IMAGE; REGULARIZATION; SHRINKAGE; ALGORITHM
AB A high-resolution image is obtained by fusing the information derived from blurred, sub-pixel shifted, and noisy low-resolution observations. In this paper, a novel regularization model based on an Anisotropic Fractional Order Adaptive (AFOA) norm is proposed and then we apply the AFOA model into the Super-Resolution Reconstruction technology. Compared with the existing models, the proposed AFOA model can remove the noise and protect the edges adaptively according to the local features of the images. Meanwhile, the proposed AFOA model can avoid the staircase effect effectively in the smooth region. To obtain the solution to the proposed AFOA model, the Gradient Descent Method is used in this paper. Finally, the experimental results show that the proposed method has much improvement than the existing methods in the respect of the Peak Signal-to-Noise Ratio and the visual quality.
C1 [Chen, Chuanbo; Lyu, Zehua; Sarem, Mudar] Huazhong Univ Sci & Technol, Sch Software Engn, Wuhan 430074, Peoples R China.
   [Liang, Hu; Zhao, Shengrong] Huazhong Univ Sci & Technol, Sch Comp Sci & Technol, Wuhan 430074, Peoples R China.
C3 Huazhong University of Science & Technology; Huazhong University of
   Science & Technology
RP Zhao, SR (corresponding author), Huazhong Univ Sci & Technol, Sch Comp Sci & Technol, Wuhan 430074, Peoples R China.
EM Zhaosr2006@126.com
OI Zhao, Shengrong/0000-0003-0965-0918
FU Ph.D. Programs Foundation of Ministry of Education of China
   [20120142120110]; Graduates' Innovation Fund of Huazhong University of
   Science and Technology [HF-11-06-2013]
FX We are thankful to the anonymous reviewers for their constructive
   suggestions which helped us in improving our manuscript. This work is
   partly supported by Ph.D. Programs Foundation of Ministry of Education
   of China (No. 20120142120110) and partly supported by Graduates'
   Innovation Fund of Huazhong University of Science and Technology (No.
   HF-11-06-2013).
CR Akyol A, 2012, PATTERN RECOGN, V45, P4103, DOI 10.1016/j.patcog.2012.05.018
   [Anonymous], J INF COMPUT SCI
   Babacan SD, 2011, IEEE T IMAGE PROCESS, V20, P984, DOI 10.1109/TIP.2010.2080278
   BATTITI R, 1991, INT J COMPUT VISION, V6, P133, DOI 10.1007/BF00128153
   Birkholz H, 2011, J COMPUT APPL MATH, V235, P2502, DOI 10.1016/j.cam.2010.11.003
   Buades A, 2008, INT J COMPUT VISION, V76, P123, DOI 10.1007/s11263-007-0052-1
   Chu N, 2013, J SOUND VIB, V332, P4369, DOI 10.1016/j.jsv.2013.02.037
   Cimrák I, 2012, APPL MATH COMPUT, V218, P11583, DOI 10.1016/j.amc.2012.05.042
   El Hamidi A, 2010, PATTERN RECOGN, V43, P1564, DOI 10.1016/j.patcog.2009.10.011
   Esedoglu S, 2004, COMMUN PUR APPL MATH, V57, P1609, DOI 10.1002/cpa.20045
   Farsiu S, 2004, IEEE T IMAGE PROCESS, V13, P1327, DOI 10.1109/TIP.2004.834669
   Izadpanahi S, 2013, SIGNAL PROCESS, V93, P2076, DOI 10.1016/j.sigpro.2013.01.006
   Kanaev AV, 2013, OPT EXPRESS, V21, P19850, DOI 10.1364/OE.21.019850
   Kindermann S, 2005, MULTISCALE MODEL SIM, V4, P1091, DOI 10.1137/050622249
   Krommweh J, 2010, SIGNAL PROCESS, V90, P2529, DOI 10.1016/j.sigpro.2010.02.022
   Li XL, 2010, SIGNAL PROCESS, V90, P405, DOI 10.1016/j.sigpro.2009.05.028
   Li YY, 1996, IEEE T IMAGE PROCESS, V5, P987, DOI 10.1109/83.503914
   Lucas B., 1981, P INT JOINT C ART IN, P674, DOI DOI 10.1364/J0SAA.19.002142
   Lucka F, 2012, INVERSE PROBL, V28, DOI 10.1088/0266-5611/28/12/125012
   Ma W, 2012, INVERSE PROBL, V28, DOI 10.1088/0266-5611/28/12/125001
   Ng MK, 2007, EURASIP J ADV SIG PR, DOI 10.1155/2007/74585
   Ren ZM, 2013, SIGNAL PROCESS, V93, P2408, DOI 10.1016/j.sigpro.2013.02.015
   RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F
   Shao W. Z., 2008, THESIS NANJING U TEC
   Tang S, 2014, SIGNAL PROCESS, V94, P339, DOI 10.1016/j.sigpro.2013.07.005
   Tian YS, 2012, MULTIMED TOOLS APPL, V60, P519, DOI 10.1007/s11042-011-0821-2
   Tian YS, 2013, IEEE T CIRC SYST VID, V23, P1224, DOI 10.1109/TCSVT.2013.2242593
   Villena S, 2013, DIGIT SIGNAL PROCESS, V23, P530, DOI 10.1016/j.dsp.2012.10.002
   Villena S, 2009, 2009 PROCEEDINGS OF 6TH INTERNATIONAL SYMPOSIUM ON IMAGE AND SIGNAL PROCESSING AND ANALYSIS (ISPA 2009), P156
   Vrigkas M, 2013, SIGNAL PROCESS-IMAGE, V28, P494, DOI 10.1016/j.image.2012.12.008
   Wang LF, 2013, IEEE T CIRC SYST VID, V23, P1289, DOI 10.1109/TCSVT.2013.2240915
   Wang LQ, 2013, SIGNAL PROCESS, V93, P661, DOI 10.1016/j.sigpro.2012.09.004
   Yuan QQ, 2013, IEEE T IMAGE PROCESS, V22, P2327, DOI 10.1109/TIP.2013.2251648
NR 33
TC 8
Z9 10
U1 0
U2 21
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2015
VL 31
IS 9
BP 1217
EP 1231
DI 10.1007/s00371-014-1007-5
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CO8AY
UT WOS:000359388100006
DA 2024-07-18
ER

PT J
AU Eslitzbichler, M
AF Eslitzbichler, Markus
TI Modelling character motions on infinite-dimensional manifolds
SO VISUAL COMPUTER
LA English
DT Article
DE Riemannian shape analysis; Elastic metric; Character animation;
   Parametric motion; Motion capture; Motion retrieval
ID SHAPES; SPACES
AB In this article, we will formulate a mathematical framework that allows us to treat character animations as points on infinite-dimensional Hilbert manifolds. Constructing geodesic paths between animations on those manifolds allows us to derive a distance function to measure similarities of different motions. This approach is derived from the field of geometric shape analysis, where such formalisms have been used to facilitate object recognition tasks. Analogously to the idea of shape spaces, we construct motion spaces consisting of equivalence classes of animations under reparametrizations. Especially cyclic motions can be represented elegantly in this framework. We demonstrate the suitability of this approach in multiple applications in the field of computer animation. First, we show how visual artefacts in cyclic animations can be removed by applying a computationally efficient manifold projection method. We next highlight how geodesic paths can be used to calculate interpolations between various animations in a computationally stable way. Finally, we show how the same mathematical framework can be used to perform cluster analysis on large motion capture databases, which can be used for or as part of motion retrieval problems.
C1 Norwegian Univ Sci & Technol, Dept Math Sci, N-7491 Trondheim, Norway.
C3 Norwegian University of Science & Technology (NTNU)
RP Eslitzbichler, M (corresponding author), Norwegian Univ Sci & Technol, Dept Math Sci, N-7491 Trondheim, Norway.
EM markuses@math.ntnu.no
FU Research Council of Norway; NSF [EIA-0196217]
FX The author would like to thank Elena Celledoni and Markus Grasmair for
   valuable discussions and feedback. This research was supported in part
   by the GeNuIn Applications project grant from the Research Council of
   Norway. The data used in this project was obtained from
   mocap.cs.cmu.edu. The database was created with funding from NSF
   EIA-0196217.
CR Abdelkader MF, 2011, COMPUT VIS IMAGE UND, V115, P439, DOI 10.1016/j.cviu.2010.10.006
   Bauer M, 2014, J MATH IMAGING VIS, V50, P60, DOI 10.1007/s10851-013-0490-z
   Bauer M, 2011, J GEOM MECH, V3, P389, DOI 10.3934/jgm..2011.3.389
   Castro GG, 2010, VISUAL COMPUT, V26, P325, DOI 10.1007/s00371-010-0422-5
   Gleicher M., 2003, P 2003 ACM SIGGRAPH
   Kilian M, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1239451.1239515, 10.1145/1276377.1276457]
   Klassen E, 2004, IEEE T PATTERN ANAL, V26, P372, DOI 10.1109/TPAMI.2004.1262333
   Kovar L, 2002, ACM T GRAPHIC, V21, P473, DOI 10.1145/566570.566605
   Kurtek S, 2012, J AM STAT ASSOC, V107, P1152, DOI 10.1080/01621459.2012.699770
   Kurtek S, 2010, PROC CVPR IEEE, P1625, DOI 10.1109/CVPR.2010.5539778
   Liu W, 2011, THESIS FLORIDA STATE
   Liu W, 2011, PLOS COMPUT BIOL, V7, DOI 10.1371/journal.pcbi.1001075
   Milnor J. W., 1963, MORSE THEORY AM 51, V51
   Ormoneit D, 2005, IMAGE VISION COMPUT, V23, P1264, DOI 10.1016/j.imavis.2005.09.004
   Palais RS., 1963, Topology, V2, P299, DOI [10.1016/0040-9383(63)90013-2, DOI 10.1016/0040-9383(63)90013-2]
   Pejsa T, 2010, COMPUT GRAPH FORUM, V29, P202, DOI 10.1111/j.1467-8659.2009.01591.x
   Shoemaker K., 1985, Computer Graphics, V19, P245, DOI 10.1145/325165.325242
   Srivastava A, 2012, IMAGE VISION COMPUT, V30, P398, DOI 10.1016/j.imavis.2012.03.006
   Srivastava A, 2011, IEEE T PATTERN ANAL, V33, P1415, DOI 10.1109/TPAMI.2010.184
   Williams L., 1995, 22ND ANNUAL CONFEREN
   Younes L, 2012, IMAGE VISION COMPUT, V30, P389, DOI 10.1016/j.imavis.2011.09.009
NR 21
TC 7
Z9 8
U1 0
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2015
VL 31
IS 9
BP 1179
EP 1190
DI 10.1007/s00371-014-1001-y
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CO8AY
UT WOS:000359388100003
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Adhikarla, VK
   Marton, F
   Balogh, T
   Gobbetti, E
AF Adhikarla, Vamsi Kiran
   Marton, Fabio
   Balogh, Tibor
   Gobbetti, Enrico
TI Real-time adaptive content retargeting for live multi-view capture and
   light field display
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 32nd Computer Graphics International CGI 15 Conference
CY JUN 24-26, 2015
CL INSA Strasbourg Univ Strasbourg, Strasbourg, FRANCE
SP CNRS, iCUBE, Univ Strasbourg, CGS, Springer, Acm Incooperation, IGG, ACMSIGGRAPH, KIST Europe, Region Alsace, Visteon
HO INSA Strasbourg Univ Strasbourg
DE On-the-fly depth retargeting; GPU; Multiprojector light field display;
   Visually enhanced live 3D video; Multi-view capture and display
AB The discrete nature of multiprojector light field displays results in aliasing when rendering scene points at depths outside the supported depth of field causing visual discomfort. We propose an efficient on-the-fly content-aware real-time depth retargeting algorithm for live 3D light field video to increase the quality of visual perception on a cluster-driven multiprojector light field display. The proposed algorithm is embedded in an end-to-end real-time system capable of capturing and reconstructing light field from multiple calibrated cameras on a full horizontal parallax light field display. By automatically detecting salient regions of a scene, we solve an optimization to derive a non-linear operator to fit the whole scene within the comfortable viewing range of the light field display. We evaluate the effectiveness of our approach on synthetic and real world scenes.
C1 [Adhikarla, Vamsi Kiran] Pazmany Peter Catholic Univ, H-1083 Budapest, Hungary.
   [Marton, Fabio; Gobbetti, Enrico] CRS4, Pula, Italy.
   [Adhikarla, Vamsi Kiran] Holografika, Budapest, Hungary.
C3 Pazmany Peter Catholic University
RP Adhikarla, VK (corresponding author), Pazmany Peter Catholic Univ, Prater Utca 50-A, H-1083 Budapest, Hungary.
EM adhikarla.vamsi.kiran@itk.ppke.hu
RI Marton, Fabio/KBC-4179-2024; Gobbetti, Enrico/O-2188-2015
OI Gobbetti, Enrico/0000-0003-0831-2458; Balogh, Tibor/0000-0002-5083-851X;
   Marton, Fabio/0000-0001-8611-1921
FU DIVA Marie Curie Action of the People programme of the EU under REA
   [290227];  [TAMOP-4.2.1.B-11/2/KMR-2011-0002]
FX This research has received funding from the DIVA Marie Curie Action of
   the People programme of the EU FP7/2007- 2013/ Program under REA Grant
   agreement 290227. The support of the TAMOP-4.2.1.B-11/2/KMR-2011-0002 is
   kindly acknowledged.
CR Agus M., 2013, P EUR, P1
   Agus M, 2008, COMPUT GRAPH FORUM, V27, P231, DOI 10.1111/j.1467-8659.2008.01120.x
   Balogh T., 2010, SPIE, V7724, P5
   Birklbauer C, 2012, COMPUT GRAPH FORUM, V31, P295, DOI 10.1111/j.1467-8659.2012.03008.x
   Chai JX, 2000, COMP GRAPH, P307, DOI 10.1145/344779.344932
   Didyk P, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964991
   Graf D., 2013, P VMV
   Guitián JAI, 2010, VISUAL COMPUT, V26, P1037, DOI 10.1007/s00371-010-0453-y
   Jones A, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1276377.1276427
   Kim C, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024224
   Kraevoy V, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409064
   Kubota A, 2007, IEEE SIGNAL PROC MAG, V24, P10, DOI 10.1109/MSP.2007.905873
   Kunita Y., 2006, P ACM S VIRT REAL SO, P181
   Lang M, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778812
   Marton F., 2011, 3DTV C TRUE VIS CAPT, P1, DOI DOI 10.1109/3DTV.2011.5877176
   Masia B, 2013, COMPUT GRAPH-UK, V37, P1012, DOI 10.1016/j.cag.2013.10.003
   Masia B, 2013, COMPUT GRAPH-UK, V37, P983, DOI 10.1016/j.cag.2013.06.004
   Matusik W, 2004, ACM T GRAPHIC, V23, P814, DOI 10.1145/1015706.1015805
   Smolic A, 2011, PATTERN RECOGN, V44, P1958, DOI 10.1016/j.patcog.2010.09.005
   Taguchi Yuichi, 2008, 2008 3DTV-Conference: The True Vision - Capture, Transmission and Display of 3D Video, P241, DOI 10.1109/3DTV.2008.4547853
   Taguchi Y, 2009, IEEE T VIS COMPUT GR, V15, P841, DOI 10.1109/TVCG.2009.30
   Yang RG, 2002, 10TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P225, DOI 10.1109/PCCGA.2002.1167864
   Yang R, 2008, IEEE T VIS COMPUT GR, V14, P84, DOI 10.1109/70410
   Zabih R., 1994, Computer Vision - ECCV '94. Third European Conference on Computer Vision. Proceedings. Vol.II, P151, DOI 10.1007/BFb0028345
NR 24
TC 10
Z9 10
U1 0
U2 9
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2015
VL 31
IS 6-8
BP 1023
EP 1032
DI 10.1007/s00371-015-1127-6
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA CM2CN
UT WOS:000357487500027
DA 2024-07-18
ER

PT J
AU Lee, T
   Park, J
   Kwon, T
AF Lee, Taekgu
   Park, Jinho
   Kwon, Taesoo
TI Adaptive locomotion on slopes and stairs using pelvic rotation
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 32nd Computer Graphics International CGI 15 Conference
CY JUN 24-26, 2015
CL INSA Strasbourg Univ Strasbourg, Strasbourg, FRANCE
SP CNRS, iCUBE, Univ Strasbourg, CGS, Springer, Acm Incooperation, IGG, ACMSIGGRAPH, KIST Europe, Region Alsace, Visteon
HO INSA Strasbourg Univ Strasbourg
DE Computer animation; Inverse kinematics; Physics-based
ID INVERSE KINEMATICS; MOTION; ANIMATION
AB In this paper, we introduce a new online motion retargeting technique to generate natural locomotion of walking on slopes and stairs using only a single captured reference motion. An inverse-kinematics solver is developed to generate poses satisfying smooth trajectories of positional and rotational constraints for feet and hands. By considering the rotations of the pelvis and upper body, our technique is able to produce natural poses without knee-popping artifacts.
C1 [Lee, Taekgu; Kwon, Taesoo] Hanyang Univ, Seoul 133791, South Korea.
   [Park, Jinho] Soongsil Univ, Global Sch Media, Seoul, South Korea.
C3 Hanyang University; Soongsil University
RP Kwon, T (corresponding author), Hanyang Univ, Seoul 133791, South Korea.
EM taesoobear@gmail.com
FU National Research Foundation of Korea (NRF) - Ministry of Science, ICT
   and Future Planning [NRF-2014R1A1A1038386]; Technology Innovation
   Program - Ministry of Trade, industry and Energy (MI, Korea) [10047078]
FX Taesoo Kwon and Jinho Park are co-corresponding authors of this paper.
   This work was supported by Basic Science Research Program through the
   National Research Foundation of Korea (NRF) funded by the Ministry of
   Science, ICT and Future Planning (NRF-2014R1A1A1038386) and by the
   Technology Innovation Program (ID: 10047078) funded by the Ministry of
   Trade, industry and Energy (MI, Korea).
CR Arikan O, 2002, ACM T GRAPHIC, V21, P483, DOI 10.1145/566570.566606
   Baerlocher P, 2004, VISUAL COMPUT, V20, P402, DOI 10.1007/s00371-004-0244-4
   Boulic R, 2009, LECT NOTES COMPUT SC, V5884, P231, DOI 10.1007/978-3-642-10347-6_21
   Buss S. R., 2005, Journal of Graphics Tools, V10, P37
   Coros S, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618516
   Coros S, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409066
   de Lasa M, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1781157
   Feng AndrewW., 2012, I3D, P95, DOI [DOI 10.1145/2159616.2159632, 10.1145/2159616.2159632]
   Grochow K, 2004, ACM T GRAPHIC, V23, P522, DOI 10.1145/1015706.1015755
   Harrison J, 2004, ACM T GRAPHIC, V23, P569, DOI 10.1145/1015706.1015761
   Huang WW, 2008, IEEE-RAS INT C HUMAN, P155, DOI 10.1109/ICHR.2008.4755946
   Kallmann M, 2008, COMPUT ANIMAT VIRT W, V19, P79, DOI 10.1002/cav.176
   Kim Y., 2012, Proceedings of ACM SIGGRAPH/Eurographics Symposium on Computer Animation, P165
   Kovar L, 2002, ACM T GRAPHIC, V21, P473, DOI 10.1145/566570.566605
   Kovar Lucas., 2002, SCA 2002: Proceedings of the 2002 ACM SIG-GRAPH/Eurographics Symposium on Computer Animation, P97
   Kwon Taesoo, 2010, P 2010 ACM SIGGRAPHE, P129
   Lau M., 2005, Proceedings of the 2005 ACM SIGGRAPH/Eurographics symposium on Computer animation, SCA '05, P271
   Lay AN, 2006, J BIOMECH, V39, P1621, DOI 10.1016/j.jbiomech.2005.05.005
   Lee J, 1999, COMP GRAPH, P39
   Lee JH, 2002, ACM T GRAPHIC, V21, P491
   Lee Y, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866160
   Lee Yongjoon, 2010, Proc. ACMSIGGRAPH Papers, P1
   Ma W., 2010, P ACM SIGGRAPH EUR S, P21
   McCann J, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1276377.1276385, 10.1145/1239451.1239457]
   Min JY, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366172
   Molla E., 2013, P MOT GAM, V2013, P165, DOI [10.1145/2522628.2522649, DOI 10.1145/2522628.2522649]
   Mordatch I, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778808
   Mukai T, 2005, ACM T GRAPHIC, V24, P1062, DOI 10.1145/1073204.1073313
   Rose C, 1998, IEEE COMPUT GRAPH, V18, P32, DOI 10.1109/38.708559
   Shin HJ, 2001, ACM T GRAPHIC, V20, P67, DOI 10.1145/502122.502123
   Tolani D, 2000, GRAPH MODELS, V62, P353, DOI 10.1006/gmod.2000.0528
   Treuille A, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239458
   Unzueta L, 2008, GRAPH MODELS, V70, P87, DOI 10.1016/j.gmod.2008.03.002
   Wang JG, 2010, MAGNETIC FRINGE FIELDS AND INTERFERENCE IN HIGH INTENSITY ACCELERATORS, P1
   Wangs JM, 2012, ACM T GRAPHIC, V31
   Wei XLK, 2011, IEEE COMPUT GRAPH, V31, P78, DOI 10.1109/MCG.2009.132
   Wu JC, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778809
   Wu XM, 2011, IEEE COMPUT GRAPH, V31, P69, DOI 10.1109/MCG.2009.111
   Xu WW, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531341
   Yamane K, 2004, ACM T GRAPHIC, V23, P532, DOI 10.1145/1015706.1015756
NR 40
TC 1
Z9 1
U1 0
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2015
VL 31
IS 6-8
BP 873
EP 881
DI 10.1007/s00371-015-1103-1
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA CM2CN
UT WOS:000357487500013
DA 2024-07-18
ER

PT J
AU Hung, APL
   Wu, T
   Hunter, P
   Mithraratne, K
AF Hung, Alice Pui Lam
   Wu, Tim
   Hunter, Peter
   Mithraratne, Kumar
TI A framework for generating anatomically detailed subject-specific human
   facial models for biomechanical simulations
SO VISUAL COMPUTER
LA English
DT Article
DE Human facial model; Cubic Hermite elements; Subject-specific
   customization
ID FINITE-ELEMENT MODEL; MAXILLOFACIAL SURGERY; ORBICULARIS OCULI; MUSCLE;
   ANIMATION; SYSTEM; SMAS; FACE
AB Realistic biomechanical simulations of the human face rely on detailed and accurate anatomical models. A recent study involving ultrasound imaging revealed that tissue structures in the human face can be separated into two major strata that move independent of each other. Based on this observation, anatomically accurate finite element models representing soft tissues in both layers of the human face were developed using 3D segmented data derived from the high-resolution US Visible Human cryosection images. The three-dimensional geometry of these tissue structures was described using Cubic Hermite finite elements. The use of Hermite family elements ensures the continuity of displacement gradient across element boundaries and hence maintains the moment balance throughout the computational domain in mechanical simulations. This in turn leads to more accurate predictions of soft tissue deformations. Creating subject-specific detailed model of the face suitable for biomechanical analysis is, however, a time-consuming task. This paper proposes a fast semi-automated framework for generating detailed subject-specific facial models including internal muscles using techniques that involve landmark-based affine transformation, iterative surface-fitting and free-form deformation. Generated models for three individuals are presented to demonstrate the efficacy of the proposed methodology.
C1 [Hung, Alice Pui Lam] Univ Auckland, Auckland 1, New Zealand.
   [Wu, Tim; Hunter, Peter; Mithraratne, Kumar] Univ Auckland, Auckland Bioengn Inst, Auckland 1, New Zealand.
C3 University of Auckland; University of Auckland
RP Hung, APL (corresponding author), Univ Auckland, Auckland 1, New Zealand.
EM ahun047@aucklanduni.ac.nz
RI Wu, Tim/I-2643-2013
OI Wu, Tim/0000-0001-6444-598X; Hunter, Peter/0000-0001-9665-4145;
   Mithraratne, Kumar/0000-0001-8090-8876
FU Ministry of Business, Innovation and Employment of New Zealand
   [UOAX0712]
FX The work presented in this paper was funded by the Ministry of Business,
   Innovation and Employment of New Zealand under the grant number
   UOAX0712. Authors would like to thank Sally Che for her contributions
   towards the development of muscle models.
CR Ackerman MJ, 1998, P IEEE, V86, P504, DOI 10.1109/5.662875
   Aina O., 2011, GENERATING ANATOMICA
   Aston J., 2009, AESTHETIC PLASTIC SU
   Barbarino GG, 2009, J BIOMECH ENG-T ASME, V131, DOI 10.1115/1.3049857
   Beldie L, 2010, INT J MED ROBOT COMP, V6, P422, DOI 10.1002/rcs.352
   BORGES AF, 1984, PLAST RECONSTR SURG, V73, P144, DOI 10.1097/00006534-198401000-00036
   Chabanas M, 2003, MED IMAGE ANAL, V7, P131, DOI 10.1016/S1361-8415(02)00108-1
   Che S., 2009, P WORLD ACAD SCI ENG
   Clemente CD., 1987, ANATOMY REGIONAL ATL
   Desai CS, 2001, INTRO FINITE ELEMENT, P496
   Erian A., 2011, Advanced Surgical Facial Rejuvenation, Art and Clinical Practice
   Fernandez J., 2012, PATIENT SPECIFIC COM, V5, DOI [10.1007/978-94-007-4552-0_2, DOI 10.1007/978-94-007-4552-0_2]
   Fernandez JW, 2004, BIOMECH MODEL MECHAN, V2, P139, DOI [10.1007/s10237-003-0036-1, 10.1007/S10237-003-0036-1]
   Fernandez JW, 2005, BIOMECH MODEL MECHAN, V4, P20, DOI 10.1007/s10237-005-0072-0
   Gladilin E, 2004, MED BIOL ENG COMPUT, V42, P167, DOI 10.1007/BF02344627
   Goodmurphy CW, 1999, CLIN ANAT, V12, P1, DOI 10.1002/(SICI)1098-2353(1999)12:1<1::AID-CA1>3.0.CO;2-J
   Hung A, 2012, THESIS
   Hung A., 2011, P SIGGRAPH ASIA, V2011, P29
   Hussain G, 2004, BRIT J PLAST SURG, V57, P502, DOI 10.1016/j.bjps.2004.04.003
   Kahler K., 2001, GRAPH INTERF
   Keeve E, 1996, IEEE VISUAL, P21, DOI 10.1109/VISUAL.1996.567595
   Kim H, 2010, PROG BIOPHYS MOL BIO, V103, P284, DOI 10.1016/j.pbiomolbio.2010.09.004
   Koch R., 1996, SIGGRAPH 96 P 23 ANN
   Lander T, 1996, INVEST OPHTH VIS SCI, V37, P1732
   LEE Y., 1995, SIGGRAPH 95
   Macchi V, 2010, CELLS TISSUES ORGANS, V191, P47, DOI 10.1159/000226276
   Mendelson B.C., 2008, ADV AESTHETIC SURG
   MITZ V, 1976, PLAST RECONSTR SURG, V58, P80, DOI 10.1097/00006534-197607000-00013
   NICOLAU PJ, 1983, BRIT J PLAST SURG, V36, P141, DOI 10.1016/0007-1226(83)90081-4
   Oberhofer K, 2009, VISUAL COMPUT, V25, P843, DOI 10.1007/s00371-009-0314-8
   Özdemir R, 2002, PLAST RECONSTR SURG, V110, P1134, DOI 10.1097/01.PRS.0000021442.30272.0E
   Platzer W., 2008, Color Atlas of Human Anatomy: Vol. 1
   Puso MA, 2002, INT J NUMER METH ENG, V54, P1161, DOI 10.1002/nme.466
   Savran A., 2008, 1 COST 2101 WORKSH B, P7
   Schuenke M., 2010, Head and Neuroanatomy, V1st
   Sederberg T., 1986, ACM SIGGRAPH COMPUT
   Shim VB, 2008, J BIOMECH ENG-T ASME, V130, DOI 10.1115/1.2960368
   Sifakis E, 2005, ACM T GRAPHIC, V24, P417, DOI 10.1145/1073204.1073208
   Smith SM, 1997, INT J COMPUT VISION, V23, P45, DOI 10.1023/A:1007963824710
   Terzopoulos D., 1990, J VIS COMPUT ANIM, V1
   Wilkinson C., 2008, Forensic Facial Reconstruction
   Wu T., 2010, 6 WORLD C BIOM SING
   Zhang Y, 2004, IEEE T VIS COMPUT GR, V10, P339, DOI 10.1109/TVCG.2004.1272733
   Zhang Y., 2001, COMP GRAPH INT 2011
   Zide BM, 2000, PLAST RECONSTR SURG, V105, P1213, DOI 10.1097/00006534-200003000-00061
NR 45
TC 7
Z9 7
U1 1
U2 11
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2015
VL 31
IS 5
BP 527
EP 539
DI 10.1007/s00371-014-0945-2
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CH2CF
UT WOS:000353831400003
DA 2024-07-18
ER

PT J
AU Wu, ZH
   Zhou, Z
   Tian, DL
   Wu, W
AF Wu, Zhaohui
   Zhou, Zhong
   Tian, Delei
   Wu, Wei
TI Reconstruction of three-dimensional flame with color temperature
SO VISUAL COMPUTER
LA English
DT Article
DE Color distortion; Color temperature; Multiplication reconstruction;
   Visual hull; Three-dimensional reconstruction; Flame/fire
ID LUMINOSITY DISTRIBUTION; COMBUSTION; VISUALIZATION
AB The reconstruction of flame from the captured images is a difficult and computationally expensive problem. Reconstruction from color images will keep the colorful appearance, as is beneficial for visually realistic flame modeling. Most of existing color-image-based methods rebuild three density fields from RGB intensities; however, these methods suffer from the color distortion problem due to the high correlation of RGB intensities. A novel method for 3D flame reconstruction using color temperature is presented in this paper. Color-temperature mapping is calculated to avoid color distortion; this method maps the RGB intensities into the color temperature and its joint intensity. We improve the multiplication reconstruction with visual hull restriction so that the energy distribution is more reasonable, which allows avoidance of the impossible zones. Experimental results indicate that our approach is efficient in the visually plausible 3D flame generation and produces better color restorations.
C1 [Wu, Zhaohui; Zhou, Zhong; Tian, Delei; Wu, Wei] Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing 100191, Peoples R China.
C3 Beihang University
RP Zhou, Z (corresponding author), Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing 100191, Peoples R China.
EM zz@vrlab.buaa.edu.cn
FU National 863 Program of China [2012AA011803]; National Natural Science
   Foundation of China [61300066]
FX This work was supported by the National 863 Program of China (Grant No.
   2012AA011803) and National Natural Science Foundation of China (No.
   61300066). We thank Voicu Popescu of Purdue University for his lectures
   about academic writing.
CR [Anonymous], EUR 2003
   [Anonymous], REELFIRE 2
   Atcheson B, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409085
   Bheemul HC, 2002, MEAS SCI TECHNOL, V13, P1643, DOI 10.1088/0957-0233/13/10/318
   Fu TR, 2010, J HEAT TRANS-T ASME, V132, DOI 10.1115/1.4000467
   Gilabert G, 2005, J PHYS CONF SER, V15, P167, DOI 10.1088/1742-6596/15/1/028
   Gilabert G, 2007, IEEE T INSTRUM MEAS, V56, P1300, DOI 10.1109/TIM.2007.900161
   Gregson J, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185548
   Hasinoff SW, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1184
   Hossain MM, 2012, IEEE IMTC P, P1856
   HUNT RWG, 1985, COLOR RES APPL, V10, P165, DOI 10.1002/col.5080100306
   Ihrke Ivo, 2004, P ACM SIGGRAPHEUROGR, P365, DOI DOI 10.1145/1028523.1028572
   Lightman A. P., 2004, RAD PROCESSES SSTROP
   Luo ZX, 2007, IEEE T INSTRUM MEAS, V56, P1877, DOI 10.1109/TIM.2007.904489
   Ohiwa N., 2001, LEAN COMBUSTION TECH, V2, P1
   Oleari C, 2009, J OPT SOC AM A, V26, P121, DOI 10.1364/JOSAA.26.000121
   Pardo PJ, 2012, J OPT SOC AM A, V29, pA209, DOI 10.1364/JOSAA.29.00A209
   Sezgin M, 2004, J ELECTRON IMAGING, V13, P146, DOI 10.1117/1.1631315
   Shin Y.-S., 2009, MICROELECTRONICS PAC, P1
   SZELISKI R, 1993, CVGIP-IMAG UNDERSTAN, V58, P23, DOI 10.1006/ciun.1993.1029
   Upton TD, 2011, EXP FLUIDS, V50, P125, DOI 10.1007/s00348-010-0900-6
   WALKER J, 2003, COLOUR RENDERING SPE
   Wang X., 2013, OPT ENG, V52
   Wu Zhaohui, 2011, 2011 12th International Conference on Computer-Aided Design and Computer Graphics, P333, DOI 10.1109/CAD/Graphics.2011.26
   Zhang XY, 2011, P COMBUST INST, V33, P2755, DOI 10.1016/j.proci.2010.06.119
   Zhang ZY, 2000, IEEE T PATTERN ANAL, V22, P1330, DOI 10.1109/34.888718
   Zhao H, 2007, OPT LASER TECHNOL, V39, P1351, DOI 10.1016/j.optlastec.2006.11.004
   Zhao QP, 2011, SCI CHINA INFORM SCI, V54, P683, DOI 10.1007/s11432-011-4210-2
NR 28
TC 12
Z9 15
U1 2
U2 20
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2015
VL 31
IS 5
BP 613
EP 625
DI 10.1007/s00371-014-0987-5
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CH2CF
UT WOS:000353831400009
OA hybrid
DA 2024-07-18
ER

PT J
AU Song, XB
   Zhong, F
   Wang, YK
   Qin, XY
AF Song, Xibin
   Zhong, Fan
   Wang, Yanke
   Qin, Xueying
TI Estimation of Kinect depth confidence through self-training
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 31st CGI conference
CY JUN 10-13, 2014
CL Sydney, AUSTRALIA
DE Depth map; Kinect; Confidence prediction; RGB-D; Learning
ID CALIBRATION
AB All depth data captured by Kinect devices are noisy, and sometimes even lost or shifted, especially around the edges of the depth. In this paper, we propose an approach to generate a per-pixel confidence measurement for each depth map captured by Kinect devices in indoor environments through supervised learning. Several distinguishing features from both the color images and depth maps are selected to train depth map estimators using Random Forest regressor. Using this estimator, we can predict a confidence map of any depth map captured by Kinect devices. Usage of other devices, such as an industrial laser scanner, is unnecessary, making the implementation more convenient. The experiments demonstrate precise confidence prediction of the depth.
C1 [Song, Xibin; Zhong, Fan; Wang, Yanke; Qin, Xueying] Shandong Univ, Sch Comp Sci & Technol, Jinan 250100, Peoples R China.
   [Qin, Xueying] Shandong Prov Key Lab Network Based Intelligent C, Jinan, Peoples R China.
C3 Shandong University
RP Zhong, F (corresponding author), Shandong Univ, Sch Comp Sci & Technol, Jinan 250100, Peoples R China.
EM xibins@163.com; zhongfan@sdu.edu.cn; wangyankecn@gmail.com;
   qxy@sdu.edu.cn
RI Qin, Xueying/AAM-8775-2021; Song, Xibin/AAF-8629-2019
OI Qin, Xueying/0000-0003-0057-295X; 
FU NSF of China [U1035004, 61173070, 61202149]; Key Projects in the
   National Science & Technology Pillar Program [2013BAH39F00]
FX The authors gratefully acknowledge the anonymous reviewers for their
   comments to help us to improve our paper, and also thank for their
   enormous help in revising this paper. This work is supported by NSF of
   China (Nos.U1035004, 61173070, 61202149), Key Projects in the National
   Science & Technology Pillar Program (No. 2013BAH39F00).
CR [Anonymous], P 2 INT C IMM TEL IC
   [Anonymous], INT C INT ROB SYST I
   Breiman L., 2001, Machine Learning, V45, P5, DOI 10.1023/A:1010933404324
   Chen WY, 2005, 2005 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO (ICME), VOLS 1 AND 2, P1315
   DAUGMAN JG, 1980, VISION RES, V20, P847, DOI 10.1016/0042-6989(80)90065-6
   Herrera D, 2011, LECT NOTES COMPUT SC, V6855, P437, DOI 10.1007/978-3-642-23678-5_52
   Herrera CD, 2012, IEEE T PATTERN ANAL, V34, P2058, DOI 10.1109/TPAMI.2012.125
   Khoshelham K, 2012, SENSORS-BASEL, V12, P1437, DOI 10.3390/s120201437
   Liu XY, 2011, 2011 INTERNATIONAL CONFERENCE ON ELECTRONICS, COMMUNICATIONS AND CONTROL (ICECC), P2658
   Nguyen CV, 2012, SECOND JOINT 3DIM/3DPVT CONFERENCE: 3D IMAGING, MODELING, PROCESSING, VISUALIZATION & TRANSMISSION (3DIMPVT 2012), P524, DOI 10.1109/3DIMPVT.2012.84
   OJALA T, 1994, INT C PATT RECOG, P582, DOI 10.1109/ICPR.1994.576366
   Reynolds M, 2011, PROC CVPR IEEE, P945, DOI 10.1109/CVPR.2011.5995550
   Scaramuzza D, 2007, 2007 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, VOLS 1-9, P4170, DOI 10.1109/iros.2007.4399276
   Shotton J, 2013, COMMUN ACM, V56, P116, DOI 10.1145/2398356.2398381
   Smisek J., 2011, 2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops), P1154, DOI 10.1109/ICCVW.2011.6130380
   Sung-Yeol Kim, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P2358, DOI 10.1109/ICPR.2010.577
   TAM WJ, 2004, OPTICS E, V5599, P173
   Unnikrishnan R., 2005, Tech. Rep. CMU-RI-TR-05-09
   Zhang Q. L., 2004, 2004 IEEERSJ INT C I, P2301
NR 19
TC 15
Z9 17
U1 1
U2 21
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2014
VL 30
IS 6-8
BP 855
EP 865
DI 10.1007/s00371-014-0965-y
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA AI7GX
UT WOS:000337054700027
DA 2024-07-18
ER

PT J
AU Xie, ZG
   Xiong, YS
   Xu, K
AF Xie, Zhige
   Xiong, Yueshan
   Xu, Kai
TI AB3D: action-based 3D descriptor for shape analysis
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 31st CGI conference
CY JUN 10-13, 2014
CL Sydney, AUSTRALIA
DE Shape analysis; Affordance; Consistent segmentation; Point cloud
   construction
ID CO-SEGMENTATION; SYMMETRY
AB High-level geometry processing has been a hot topic in graphics community. The functionality analysis of 3D models is an essential issue in this area. Existing 3D models often exhibit both large intra-class and inter-class variations in shape geometry and topology, making the consistent analysis of functionality challenging. Traditional 3D shape analysis methods which rely on geometric shape descriptors can not obtain satisfying results on these 3D models. We develop a new 3D shape descriptor based on the interactions between 3D models and virtual human actions, which is called Action-Based 3D Descriptor (AB3D). Due to the implied semantic meanings of virtual human actions, we obtain encouraging results on consistent segmentation based on AB3D. Finally, we present a method for recognition and reconstruction of scanned 3D indoor scenes using our AB3D. Experiments show that AB3D is a promising shape descriptor toward functionality analysis of 3D shapes.
C1 [Xie, Zhige; Xiong, Yueshan; Xu, Kai] Natl Univ Def Technol, Sch Comp Sci, State Key Lab High Performance Comp, Changsha, Hunan, Peoples R China.
C3 National University of Defense Technology - China
RP Xie, ZG (corresponding author), Natl Univ Def Technol, Sch Comp Sci, State Key Lab High Performance Comp, Changsha, Hunan, Peoples R China.
EM zhigexie@gmail.com; ysxiong@nudt.edu.cn; kevin.kai.xu@gmail.com
FU Research Fund for the Doctoral Program of Higher Education of China
   [20104307110003]; National Natural Science Foundation of China
   [61379103, 61202333, 61303185]; China Postdoctoral Science Foundation
   [2012M520392]
FX This work was partially supported by the Research Fund for the Doctoral
   Program of Higher Education of China (No. 20104307110003), The National
   Natural Science Foundation of China (No. 61379103, 61202333, 61303185)
   and China Postdoctoral Science Foundation (No. 2012M520392).
CR [Anonymous], SHAP MOD INT C SMI
   [Anonymous], P 1 WORKSH SEM VIRT
   [Anonymous], 2014, PROC ACM SIGGRAPH CO
   [Anonymous], 2012, ACM T GRAPHIC, DOI DOI 10.1145/2366145.2366156
   [Anonymous], EUROGRAPHICS STATE A
   [Anonymous], 2011, ACM T INTEL SYST TEC, DOI DOI 10.1145/1961189.1961199
   [Anonymous], 2011, P ADV NEURAL INFORM
   Attene M, 2006, COMPUT GRAPH-UK, V30, P323, DOI 10.1016/j.cag.2006.02.007
   Attene M, 2009, COMPUT AIDED DESIGN, V41, P756, DOI 10.1016/j.cad.2009.01.003
   Bar-Aviv E., 2006, Proceedings of the British Machine Vision Conference, BMVC 2006, P307
   Biasotti S, 2000, LECT NOTES COMPUT SC, V1953, P185
   Chen DY, 2003, COMPUT GRAPH FORUM, V22, P223, DOI 10.1111/1467-8659.00669
   Chen XB, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531379
   Cignoni P, 2008, ERCIM NEWS, P45
   Fanti C, 2004, ADV NEUR IN, V16, P1603
   Fu HB, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360641
   FUKUNAGA K, 1975, IEEE T COMPUT, VC 24, P750, DOI 10.1109/T-C.1975.224297
   Golovinskiy A, 2009, COMPUT GRAPH-UK, V33, P262, DOI 10.1016/j.cag.2009.03.010
   Gottschalk S., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P171, DOI 10.1145/237170.237244
   Grabner H, 2011, PROC CVPR IEEE, P1529, DOI 10.1109/CVPR.2011.5995327
   Gupta A., 2007, IEEE C COMPUTER VISI, P1
   Hu RZ, 2012, COMPUT GRAPH FORUM, V31, P1703, DOI 10.1111/j.1467-8659.2012.03175.x
   Jiang Y, 2013, PROC CVPR IEEE, P2993, DOI 10.1109/CVPR.2013.385
   Jones MW, 2006, IEEE T VIS COMPUT GR, V12, P581, DOI 10.1109/TVCG.2006.56
   Kalogerakis E, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778839
   Kazhdan M., 2003, Symposium on Geometry Processing, P156
   Kim VG, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461933
   Reuter M, 2010, INT J COMPUT VISION, V89, P287, DOI 10.1007/s11263-009-0278-1
   Rifkin R, 2004, J MACH LEARN RES, V5, P101
   Shilane P, 2004, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, P167, DOI 10.1109/smi.2004.1314504
   Sidi O, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024160
   Silberman N., 2011, 2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops), P601, DOI 10.1109/ICCVW.2011.6130298
   Veltkamp R.C., 2010, Proc. of the Eurographics/ACM SIGGRAPH Symp. on 3D Object Retrieval, P63
   Vladimir V, 2000, The nature of statistical learning theory
   Wang Y, 2011, COMPUT GRAPH FORUM, V30, P287, DOI 10.1111/j.1467-8659.2011.01885.x
   Xu K, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185553
   Xu K, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618484
   Xu K, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866206
NR 38
TC 3
Z9 3
U1 0
U2 9
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2014
VL 30
IS 6-8
BP 591
EP 601
DI 10.1007/s00371-014-0980-z
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA AI7GX
UT WOS:000337054700004
DA 2024-07-18
ER

PT J
AU Shanmugavadivu, P
   Balasubramanian, K
   Muruganandam, A
AF Shanmugavadivu, P.
   Balasubramanian, K.
   Muruganandam, A.
TI Particle swarm optimized bi-histogram equalization for contrast
   enhancement and brightness preservation of images
SO VISUAL COMPUTER
LA English
DT Article
DE Brightness preservation; Contrast enhancement; Histogram equalization;
   Particle swarm optimization; Entropy; Absolute mean brightness error
AB A novel technique, Optimized Bi-Histogram Equalization (OBHE), is proposed in this paper for preserving brightness and enhancing the contrast of any input image. The central idea of this technique is to first segment the histogram of the input image into two, based on its mean and then weighting constraints are applied to each of the sub-histograms separately. Those two histograms are equalized independently and their union produces a brightness-preserved and contrast-enhanced output image. While formulating the weighting constraints, Particle Swarm Optimization (PSO) is employed to find the optimal constraints in order to maximize the degree of brightness preservation and contrast enhancement. This technique is found to have an edge over the other contemporary methods in terms of Entropy and Absolute Mean Brightness Error.
C1 [Shanmugavadivu, P.] Deemed Univ, Dept Comp Sci & Applicat, Gandhigram Rural Inst, Dindigul, Tamil Nadu, India.
   [Balasubramanian, K.] PSNA Coll Engn & Technol, Dept Comp Applicat, Dindigul, Tamil Nadu, India.
   [Muruganandam, A.] PSNA Coll Engn & Technol, Dept Mech Engn, Dindigul, Tamil Nadu, India.
C3 Gandhigram Rural Institute; PSNA College of Engineering & Technology;
   PSNA College of Engineering & Technology
RP Shanmugavadivu, P (corresponding author), Deemed Univ, Dept Comp Sci & Applicat, Gandhigram Rural Inst, Dindigul, Tamil Nadu, India.
EM psvadivu67@gmail.com; ksbala75@gmail.com; profanandan@gmail.com
RI Krishnasamy, Balasubramanian/AAX-7007-2020; Pichai,
   Shanmugavadivu/H-3888-2019
OI Krishnasamy, Balasubramanian/0000-0002-7850-1759; Pichai,
   Shanmugavadivu/0000-0003-2032-6115
CR Chen SD, 2004, DIGIT SIGNAL PROCESS, V14, P413, DOI 10.1016/j.dsp.2004.04.001
   Chen SD, 2003, IEEE T CONSUM ELECTR, V49, P1301, DOI 10.1109/TCE.2003.1261233
   Chen SD, 2003, IEEE T CONSUM ELECTR, V49, P1310, DOI 10.1109/TCE.2003.1261234
   Gonzalez R. C., 2006, Digital Image Processing, V3rd
   Gorai A, 2009, WOR CONG NAT BIOL, P72, DOI 10.1109/NABIC.2009.5393603
   Hamnandlu M, 2006, IEEE T IMAGE PROCESS, V15, P2956, DOI 10.1109/TIP.2006.877499
   Hashemi S, 2010, PATTERN RECOGN LETT, V31, P1816, DOI 10.1016/j.patrec.2009.12.006
   Ibrahim H, 2009, IEEE T CONSUM ELECTR, V55, P891, DOI 10.1109/TCE.2009.5174471
   Kennedy J, 1995, 1995 IEEE INTERNATIONAL CONFERENCE ON NEURAL NETWORKS PROCEEDINGS, VOLS 1-6, P1942, DOI 10.1109/icnn.1995.488968
   Kim M, 2008, IEEE T CONSUM ELECTR, V54, P1389, DOI 10.1109/TCE.2008.4637632
   Kim YT, 1997, IEEE T CONSUM ELECTR, V43, P1, DOI 10.1109/30.580378
   Kwok NM, 2007, IEEE C EVOL COMPUTAT, P3353, DOI 10.1109/CEC.2007.4424905
   Kwok NM, 2009, IEEE T AUTOM SCI ENG, V6, P145, DOI 10.1109/TASE.2008.917053
   OTSU N, 1979, IEEE T SYST MAN CYB, V9, P62, DOI 10.1109/TSMC.1979.4310076
   Sengee N, 2008, IEEE T CONSUM ELECTR, V54, P1329, DOI 10.1109/TCE.2008.4637624
   Sim KS, 2007, PATTERN RECOGN LETT, V28, P1209, DOI 10.1016/j.patrec.2007.02.003
   Vlachos IK, 2006, FUZZY SET SYST, V157, P1126, DOI 10.1016/j.fss.2005.11.016
   Wang C, 2005, IEEE T CONSUM ELECTR, V51, P1326, DOI 10.1109/TCE.2005.1561863
   Wang Q, 2007, IEEE T CONSUM ELECTR, V53, P757, DOI 10.1109/TCE.2007.381756
   Wang Y, 1999, IEEE T CONSUM ELECTR, V45, P68, DOI 10.1109/30.754419
   Yoon J.H., 2002, IEICE T INF SYST, V1, pE85
   Zhang CJ, 2008, FUZZY OPTIM DECIS MA, V7, P331, DOI 10.1007/s10700-008-9042-1
NR 22
TC 36
Z9 39
U1 0
U2 8
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2014
VL 30
IS 4
BP 387
EP 399
DI 10.1007/s00371-013-0863-8
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AD3SW
UT WOS:000333167500003
DA 2024-07-18
ER

PT J
AU Bini, AA
   Bhat, MS
AF Bini, A. A.
   Bhat, M. S.
TI A nonlinear level set model for image deblurring and denoising
SO VISUAL COMPUTER
LA English
DT Article
DE Image restoration; Deblurring and denoising; Regularization; Level set
   method
ID ILL-POSED PROBLEMS; ACTIVE CONTOURS; NOISE REMOVAL; REACTION-DIFFUSION;
   RESTORATION; SHAPE; DECONVOLUTION; SEGMENTATION; ALGORITHMS;
   RECONSTRUCTION
AB Image deblurring and denoising are fundamental problems in the field of image processing with numerous applications. This paper presents a new nonlinear Partial Differential Equation (PDE) model based on curve evolution via level sets, for recovering images from their blurry and noisy observations. The proposed method integrates an image deconvolution process and a curve evolution based regularizing process to form a reaction-diffusion PDE. The regularization term in the proposed PDE is a combination of a diffusive image smoothing term and a reactive image enhancement term. The diffusive and reactive terms present in the model lead to effective suppression of noise with sharp restoration of image features. We present several numerical results for image restoration, with synthetic and real degradations and compare it to other state-of-the-art image restoration techniques. The experiments confirm the favorable performance of our method, both visually and in terms of Improvement in Signal-to-Noise-Ratio (ISNR) and Pratt's Figure Of Merit (FOM).
C1 [Bini, A. A.; Bhat, M. S.] Natl Inst Technol Karnataka, Dept Elect & Commun Engn, Mangalore 575025, India.
C3 National Institute of Technology (NIT System); National Institute of
   Technology Karnataka
RP Bini, AA (corresponding author), Natl Inst Technol Karnataka, Dept Elect & Commun Engn, Mangalore 575025, India.
EM biniaa@gmail.com
RI A, Bini A/AAB-1841-2019; Bhat, M Shankaranarayana/U-3219-2019
OI Bhat, M Shankaranarayana/0000-0002-6162-7187
CR ACAR R, 1994, INVERSE PROBL, V10, P1217, DOI 10.1088/0266-5611/10/6/003
   Almansa A., 2006, IMA PREPRINT SERIES
   ALVAREZ L, 1992, SIAM J NUMER ANAL, V29, P845, DOI 10.1137/0729052
   ANGENENT S, 1991, ANN MATH, V133, P171, DOI 10.2307/2944327
   [Anonymous], 1977, NEW YORK
   Aubert G., 2006, MATH PROBLEMS IMAGE
   Carasso AS, 1999, SIAM J NUMER ANAL, V36, P1659, DOI 10.1137/S0036142997320413
   Caselles V, 1997, INT J COMPUT VISION, V22, P61, DOI 10.1023/A:1007979827043
   Chan T., 1998, SIAM J SCI COMPUT
   Chan TF, 1998, IEEE T IMAGE PROCESS, V7, P370, DOI 10.1109/83.661187
   Chantas GK, 2006, IEEE T IMAGE PROCESS, V15, P2987, DOI 10.1109/TIP.2006.877520
   Chantas G, 2008, IEEE T IMAGE PROCESS, V17, P1795, DOI 10.1109/TIP.2008.2002828
   Chen YM, 2000, COMPUT MATH APPL, V39, P131, DOI 10.1016/S0898-1221(00)00050-X
   DEMOMENT G, 1989, IEEE T ACOUST SPEECH, V37, P2024, DOI 10.1109/29.45551
   Dobson DC, 1996, SIAM J APPL MATH, V56, P1181, DOI 10.1137/S003613999427560X
   Gao S, 2005, IEEE T IMAGE PROCESS, V14, P1537, DOI 10.1109/TIP.2005.852200
   Hanke M., 1993, Surveys on Mathematics for Industry, V3, P253
   HANSEN PC, 1992, SIAM REV, V34, P561, DOI 10.1137/1034115
   Jidesh P., 2012, J MOD OPT
   KIMIA BB, 1995, INT J COMPUT VISION, V15, P189, DOI 10.1007/BF01451741
   Li YY, 1996, IEEE T IMAGE PROCESS, V5, P987, DOI 10.1109/83.503914
   Malladi R, 1996, J MATH IMAGING VIS, V6, P269, DOI 10.1007/BF00119843
   MALLADI R, 1995, IEEE T PATTERN ANAL, V17, P158, DOI 10.1109/34.368173
   Malladi R, 1996, IEEE T IMAGE PROCESS, V5, P1554, DOI 10.1109/83.541425
   MALLADI R, 1995, P NATL ACAD SCI USA, V92, P7046, DOI 10.1073/pnas.92.15.7046
   Marquina A, 2000, SIAM J SCI COMPUT, V22, P387, DOI 10.1137/S1064827599351751
   Neelamani R, 2004, IEEE T SIGNAL PROCES, V52, P418, DOI 10.1109/TSP.2003.821103
   OSHER S, 1988, J COMPUT PHYS, V79, P12, DOI 10.1016/0021-9991(88)90002-2
   OSHER S, 1990, SIAM J NUMER ANAL, V27, P919, DOI 10.1137/0727053
   Rudin L. I., 1994, Proceedings ICIP-94 (Cat. No.94CH35708), P31, DOI 10.1109/ICIP.1994.413269
   RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F
   Sapiro G, 1996, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, PROCEEDINGS - VOL I, P817, DOI 10.1109/ICIP.1996.559624
   Sethian JA, 2001, J COMPUT PHYS, V169, P503, DOI 10.1006/jcph.2000.6657
   TEK H, 1995, 5 INT C COMP VIS
   Vogel CR, 1998, IEEE T IMAGE PROCESS, V7, P813, DOI 10.1109/83.679423
   Vogel CR, 1996, SIAM J SCI COMPUT, V17, P227, DOI 10.1137/0917016
   Welk M, 2005, LECT NOTES COMPUT SC, V3459, P585
   You YL, 1996, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, PROCEEDINGS - VOL II, P461, DOI 10.1109/ICIP.1996.560885
   Zhang KH, 2013, IEEE T IMAGE PROCESS, V22, P258, DOI 10.1109/TIP.2012.2214046
   Zhang KH, 2010, IMAGE VISION COMPUT, V28, P668, DOI 10.1016/j.imavis.2009.10.009
   Zhang KH, 2010, PATTERN RECOGN, V43, P1199, DOI 10.1016/j.patcog.2009.10.010
NR 41
TC 11
Z9 11
U1 0
U2 24
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2014
VL 30
IS 3
BP 311
EP 325
DI 10.1007/s00371-013-0857-6
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AB3LU
UT WOS:000331693100006
DA 2024-07-18
ER

PT J
AU Altantsetseg, E
   Muraki, Y
   Matsuyama, K
   Konno, K
AF Altantsetseg, Enkhbayar
   Muraki, Yuta
   Matsuyama, Katsutsugu
   Konno, Kouichi
TI Feature line extraction from unorganized noisy point clouds using
   truncated Fourier series
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Computer Graphics International (CGI) Conference
CY 2013
CL Hanover, GERMANY
DE Feature extraction; Fourier series; FFT; Point clouds
AB The detection of feature lines is important for representing and understanding geometric features of 3D models. In this paper, we introduce a new and robust method for extracting feature lines from unorganized point clouds. We use a one-dimensional truncated Fourier series for detecting feature points. Each point and its neighbors are approximated along the principal directions by using the truncated Fourier series, and the curvature of the point is computed from the approximated curves. The Fourier coefficients are computed by Fast Fourier Transform (FFT). We apply low-pass filtering to remove noise and to compute the curvature of the point robustly. For extracting feature points from the detected potential feature points, the potential feature points are thinned using a curvature weighted Laplacian-like smoothing method. The feature lines are constructed through growing extracted points and then projected onto the original point cloud. The efficiency and robustness of our approach is illustrated by several experimental results.
C1 [Altantsetseg, Enkhbayar; Muraki, Yuta; Konno, Kouichi] Iwate Univ, Fac Engn, Morioka, Iwate 020, Japan.
   [Matsuyama, Katsutsugu] Iwate Univ, Morioka, Iwate 020, Japan.
C3 Iwate University; Iwate University
RP Altantsetseg, E (corresponding author), Iwate Univ, Fac Engn, Morioka, Iwate 020, Japan.
EM bayar@lk.cis.iwate-u.ac.jp
FU Grants-in-Aid for Scientific Research [24501253] Funding Source: KAKEN
CR Aaftab MunshiBenedict Gaster., 2011, OpenCL Programming Guide, V1st
   [Anonymous], S GEOM PROC
   [Anonymous], 2001, P IMR 2001 NEWP BEAC
   Barnsley M.F, 1988, The Science of Fractal Images
   Brigham E. O., 1988, FAST FOURIER TRANSFO
   Cazals F, 2005, INT J COMPUT GEOM AP, V15, P511, DOI 10.1142/S0218195905001816
   Co T. B., 2004, SHORT TUTORIAL OBTAI
   Daniels J, 2008, VISUAL COMPUT, V24, P449, DOI 10.1007/s00371-008-0223-2
   Daniels J, 2007, IEEE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS 2007, PROCEEDINGS, P123, DOI 10.1109/SMI.2007.32
   Demarsin K, 2007, COMPUT AIDED DESIGN, V39, P276, DOI 10.1016/j.cad.2006.12.005
   Dong S, 2006, ACM T GRAPHIC, V25, P1057, DOI 10.1145/1141911.1141993
   Flannery B. P., 1992, NUMERICAL RECIPES C, DOI DOI 10.2277/052143064X
   Gökberk B, 2008, IEEE T SYST MAN CY B, V38, P155, DOI 10.1109/TSMCB.2007.908865
   Huang H, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618522
   Huy Tho Ho, 2008, 2008 Digital Image Computing: Techniques and Applications, P16, DOI 10.1109/DICTA.2008.64
   Imtiaz H., 2012, INT J SCI TECHNOL RE, V1, P1
   Kalogerakis E., 2007, P EUROGRAPHICSSIGGRA, P13
   Kalogerakis E, 2009, COMPUT AIDED DESIGN, V41, P282, DOI 10.1016/j.cad.2008.12.004
   Kim SK, 2005, COMPUT AIDED DESIGN, V37, P1533, DOI 10.1016/j.cad.2005.05.002
   Kim SK, 2013, MULTIMED TOOLS APPL, V63, P265, DOI 10.1007/s11042-012-0999-y
   Linsen L, 2001, TECHNICAL REPORT
   Lucchese L, 2002, IEEE T PATTERN ANAL, V24, P1468, DOI 10.1109/TPAMI.2002.1046160
   Mehdi-Souzani C., 2010, COMPUT AIDED DES APP, V7, P863
   Mérigot Q, 2011, IEEE T VIS COMPUT GR, V17, P743, DOI 10.1109/TVCG.2010.261
   Pang X., 2011, International Journal of Information Engineering and Electronic Business, V3, P1
   Park MK, 2012, GRAPH MODELS, V74, P197, DOI 10.1016/j.gmod.2012.04.008
   Pauly M, 2003, COMPUT GRAPH FORUM, V22, P281, DOI 10.1111/1467-8659.00675
   Pauly M, 2001, COMP GRAPH, P379, DOI 10.1145/383259.383301
   Ramli A., 2009, P EG UK THEOR PRACT
   Taubin G., 1995, P 22 ANN C COMP GRAP, P351, DOI DOI 10.1145/218380.218473
   Vais A, 2012, COMPUT GRAPH-UK, V36, P398, DOI 10.1016/j.cag.2012.03.027
   Weber Christopher, 2010, Proceedings of the Shape Modeling International (SMI 2010), P175, DOI 10.1109/SMI.2010.32
   Weber C, 2012, GRAPH MODELS, V74, P335, DOI 10.1016/j.gmod.2012.04.012
   Weisstein E. W., 2013, SPECIAL AFFINE CURVA
   Yang Y.-L., 2006, EUROGRAPHICS S GEOME, P223
NR 35
TC 33
Z9 38
U1 1
U2 43
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2013
VL 29
IS 6-8
BP 617
EP 626
DI 10.1007/s00371-013-0800-x
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 151RR
UT WOS:000319478400016
DA 2024-07-18
ER

PT J
AU Bousetouane, F
   Dib, L
   Snoussi, H
AF Bousetouane, Fouad
   Dib, Lynda
   Snoussi, Hichem
TI Improved mean shift integrating texture and color features for robust
   real time object tracking
SO VISUAL COMPUTER
LA English
DT Article
DE Visual tracking; Mean shift tracker; Color density; Haralick texture
   features; Co-occurrence matrix; Moving object extraction
AB The subject of mean shift algorithm for tracking the location of an object using a color model has recently gained considerable interest. However, the use of a color model to represent the tracked object is very sensitive to clutter interference, illumination changes, and the influence of background. Therefore, the applicability of basic color-based mean shift tracking is limited in many real world complex conditions. In this paper, we present a modified adaptive mean shift tracking algorithm integrating a combination of texture and color features. We first suggest a new texture-based target representation based on spatial dependencies and co-occurrence distribution within interest target region for invariant target description, which is computed through so-scaled Haralick texture features. Then, to improve the tracking further, we propose an extension to the mean shift tracker where a combination of texture and color features are used as the target model. To be consistent to the scale change and complex non-rigid motions of the tracked target, we suggest to adapt the tracking window of the proposed algorithm with the real moving target mask at tracking over time. Many experimental results demonstrate the successful of target tracking using the proposed algorithm in many complex situations, where the basic mean shift tracker obviously fails. The performance of the proposed adaptive mean shift tracker is evaluated using the VISOR video Dataset, thermal infrared-acquired images sequences bench mark, and also some proprietary videos.
C1 [Bousetouane, Fouad; Dib, Lynda] Univ Badji Mokhtar, LASE Embedded Syst Lab, Annaba, Algeria.
   [Snoussi, Hichem] Univ Technol Troyes, ICD LM2S, Troyes, France.
C3 Universite Badji Mokhtar - Annaba; Universite de Technologie de Troyes
RP Bousetouane, F (corresponding author), Univ Badji Mokhtar, LASE Embedded Syst Lab, BP 12, Annaba, Algeria.
EM bousetouane_f@yahoo.fr; diblynda@yahoo.fr; Hichem.Snoussi@utt.fr
CR [Anonymous], P IEEE C COMP VIS PA
   [Anonymous], P IEEE 19 INT C PATT
   Azghani M., 2010, 2010 5th International Symposium on Telecommunications (IST), P806, DOI 10.1109/ISTEL.2010.5734133
   Bekkali A., 2011, 2011 IEEE INT C SIGN, P1, DOI [10.1109/ICSPCC.2011.6061737, DOI 10.1109/ICSPCC.2011.6061737]
   Benz UC, 2004, ISPRS J PHOTOGRAMM, V58, P239, DOI 10.1016/j.isprsjprs.2003.10.002
   Bousetouane F, 2011, PROC SPIE, V8285, DOI 10.1117/12.913034
   Chitrakala S, 2009, 2009 IEEE INTERNATIONAL ADVANCE COMPUTING CONFERENCE, VOLS 1-3, P496, DOI 10.1109/IADCC.2009.4809061
   Comaniciu D, 2003, IEEE T PATTERN ANAL, V25, P564, DOI 10.1109/TPAMI.2003.1195991
   FUKUNAGA K, 1975, IEEE T INFORM THEORY, V21, P32, DOI 10.1109/TIT.1975.1055330
   Guo QC, 2007, 2007 IEEE INTERNATIONAL CONFERENCE ON MECHATRONICS AND AUTOMATION, VOLS I-V, CONFERENCE PROCEEDINGS, P2314, DOI 10.1109/ICMA.2007.4303914
   HARALICK RM, 1973, IEEE T GEOSCI REMOTE, VGE11, P171, DOI 10.1109/TGE.1973.294312
   Jeyakar J., 2007, IEEE INT C IMAGE PRO, P49
   Leichter I, 2010, COMPUT VIS IMAGE UND, V114, P400, DOI 10.1016/j.cviu.2009.12.006
   Ma YH, 2011, IEEE T MAGN, V47, P970, DOI 10.1109/TMAG.2010.2076401
   Mao KZ, 2011, IEEE ACM T COMPUT BI, V8, P266, DOI 10.1109/TCBB.2010.43
   Ming-Yi Ju, 2010, 2010 International Conference on Machine Learning and Cybernetics (ICMLC 2010), P2904, DOI 10.1109/ICMLC.2010.5580780
   Oliva A, 2007, TRENDS COGN SCI, V11, P520, DOI 10.1016/j.tics.2007.09.009
   Pan P, 2009, INT CONF ACOUST SPEE, P3529, DOI 10.1109/ICASSP.2009.4960387
   Peng NS, 2005, PATTERN RECOGN LETT, V26, P605, DOI 10.1016/j.patrec.2004.08.023
   Porebski A., 2008, PROC FIRSTWORKSHOPS, P1
   Posada LF, 2010, IEEE INT C INT ROBOT, P804, DOI 10.1109/IROS.2010.5652869
   Qi GJ, 2010, IEEE T MULTIMEDIA, V12, P278, DOI 10.1109/TMM.2010.2046270
   Schlögl T, 2004, INT C PATT RECOG, P519, DOI 10.1109/ICPR.2004.1333825
   Souza DFL, 2010, IEEE LAT AM T, V8, P714, DOI 10.1109/TLA.2010.5688100
   Wang JQ, 2006, 2006 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND BIOMIMETICS, VOLS 1-3, P1
   Wang LW, 2005, IEEE T PATTERN ANAL, V27, P1334, DOI 10.1109/TPAMI.2005.165
   Wu H, 2010, IEEE T PATTERN ANAL, V32, P1443, DOI 10.1109/TPAMI.2009.135
   Yilmaz A, 2006, ACM COMPUT SURV, V38, DOI 10.1145/1177352.1177355
   Zhang Chi., 2010, IPDPS Workshops, P1, DOI [10.1109/ICMSS.2010.5576858, DOI 10.1109/ICMSS.2010.5576858]
   Zhang XA, 2010, INT C WAVEL ANAL PAT, P38, DOI 10.1109/ICWAPR.2010.5576453
NR 30
TC 35
Z9 44
U1 1
U2 49
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2013
VL 29
IS 3
BP 155
EP 170
DI 10.1007/s00371-012-0677-0
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 115AI
UT WOS:000316783800001
DA 2024-07-18
ER

PT J
AU Alhashim, I
   Zhang, H
   Liu, LG
AF Alhashim, Ibraheem
   Zhang, Hao
   Liu, Ligang
TI Detail-replicating shape stretching
SO VISUAL COMPUTER
LA English
DT Article
DE Detail-replication; Stretching; Geometry synthesis; Mesh editing
ID IMAGE; DEFORMATION; SYMMETRY
AB Mesh deformation has become a powerful tool for creating shape variations. Existing deformation techniques work on preserving surface details under bending and twisting operations. Stretching different parts of a shape is also a useful operation for generating shape variations. However, under stretching, texture-like geometric details should not be preserved but rather replicated. We propose a simple and efficient method that helps create model variations by applying nonuniform stretching on 3D models with organic geometric details. The method replicates the geometric details and synthesizes extensions by adopting texture synthesis techniques on surface details. We work on analyzing and separating the stretching of surface details from the stretching of the base mesh resulting in the appearance of preserved details. The efficiency of our method is attributed to a local parameterization of the surface with the help of curve skeletons. We show a variety of experimental results that demonstrate the usefulness of this intuitive stretching tool in creating shape variations.
C1 [Alhashim, Ibraheem; Zhang, Hao] Simon Fraser Univ, Graph Usabil & Visualizat Lab, Burnaby, BC V5A 1S6, Canada.
   [Liu, Ligang] Zhejiang Univ, Dept Math, Hangzhou 310003, Zhejiang, Peoples R China.
C3 Simon Fraser University; Zhejiang University
RP Alhashim, I (corresponding author), Simon Fraser Univ, Graph Usabil & Visualizat Lab, Burnaby, BC V5A 1S6, Canada.
EM iaa7@sfu.ca; haoz@cs.sfu.ca; ligangliu@zju.edu.cn
RI li, jinsong/HJH-9559-2023; Zhang, Hao/HHM-1940-2022
OI Zhang, Hao/0000-0003-1991-119X; Alhashim, Ibraheem/0000-0002-7856-1735
CR Andersen V., 2009, ISVC 09, P656
   Attene M, 2010, VISUAL COMPUT, V26, P1393, DOI 10.1007/s00371-010-0416-3
   Au OKC, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360643
   BAREQUET G, 1995, COMPUT AIDED GEOM D, V12, P207, DOI 10.1016/0167-8396(94)00011-G
   Barnes C, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531330
   Biermann H, 2002, ACM T GRAPHIC, V21, P312, DOI 10.1145/566570.566583
   Bokeloh M, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778841
   Botsch M, 2008, IEEE T VIS COMPUT GR, V14, P213, DOI 10.1109/TVCG.2007.1054
   Chen L., 2009, P 2009 SIAM ACM JOIN, P289
   Cheng MM, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778820
   Cornea ND, 2007, IEEE T VIS COMPUT GR, V13, P530, DOI 10.1109/TVCG.2007.1002
   Derose T, 2006, 0602 PIX AN STUD
   Desbrun M, 1999, COMP GRAPH, P317, DOI 10.1145/311535.311576
   Fang H, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239463
   Floater MS, 2003, COMPUT AIDED GEOM D, V20, P19, DOI 10.1016/S0167-8396(03)00002-5
   Fu HB, 2004, GEOMETRIC MODELING AND PROCESSING 2004, PROCEEDINGS, P173
   Gal R, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531339
   Gelfand Natasha, 2004, Proceedings of the 2004 Eurographics/ACM SIGGRAPH symposium on Geometry processing, P214
   Hormann K, 2006, ACM T GRAPHIC, V25, P1424, DOI 10.1145/1183287.1183295
   Kraevoy V, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409064
   Kwatra V, 2003, ACM T GRAPHIC, V22, P277, DOI 10.1145/882262.882264
   Lefebvre S, 2005, ACM T GRAPHIC, V24, P777, DOI 10.1145/1073204.1073261
   LIEN SL, 1984, IEEE COMPUT GRAPH, V4, P35, DOI 10.1109/MCG.1984.6429334
   Liepa P., 2003, Symposium on Geometry Processing, P200
   Liu YX, 2004, ACM T GRAPHIC, V23, P368, DOI 10.1145/1015706.1015731
   Liu YX, 2004, IEEE T PATTERN ANAL, V26, P354, DOI 10.1109/TPAMI.2004.1262332
   Müller P, 2006, ACM T GRAPHIC, V25, P614, DOI 10.1145/1141911.1141931
   Parish YIH, 2001, COMP GRAPH, P301, DOI 10.1145/383259.383292
   SABHA M., 2006, Vision, Modeling, and Visualization 2006, P97
   Schmidt R., 2010, ACM T GRAPHIC, V6, P1
   Sharf A, 2006, VISUAL COMPUT, V22, P835, DOI 10.1007/s00371-006-0068-5
   Sorkine O., 2004, P 2004 EUR ACM SIGGR, P179
   Szirmay-Kalos L., 2002, PROC 18 SPRING C COM, P97
   Vaquero D, 2010, PROC SPIE, V7798, DOI 10.1117/12.862419
   Wu HS, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866185
NR 35
TC 5
Z9 5
U1 0
U2 10
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2012
VL 28
IS 12
BP 1153
EP 1166
DI 10.1007/s00371-011-0665-9
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 029ZZ
UT WOS:000310538700002
DA 2024-07-18
ER

PT J
AU Xiao, CX
   Gan, JJ
AF Xiao, Chunxia
   Gan, Jiajia
TI Fast image dehazing using guided joint bilateral filter
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Conference on the Computer Graphics International (CGI)
CY 2012
CL Bournemouth, ENGLAND
DE Image dehazing; Filtering; Image processing; Bilateral filter
AB In this paper, we propose a new fast dehazing method from single image based on filtering. The basic idea is to compute an accurate atmosphere veil that is not only smoother, but also respect with depth information of the underlying image. We firstly obtain an initial atmosphere scattering light through median filtering, then refine it by guided joint bilateral filtering to generate a new atmosphere veil which removes the abundant texture information and recovers the depth edge information. Finally, we solve the scene radiance using the atmosphere attenuation model. Compared with exiting state of the art dehazing methods, our method could get a better dehazing effect at distant scene and places where depth changes abruptly. Our method is fast with linear complexity in the number of pixels of the input image; furthermore, as our method can be performed in parallel, thus it can be further accelerated using GPU, which makes our method applicable for real-time requirement.
C1 [Xiao, Chunxia; Gan, Jiajia] Wuhan Univ, Sch Comp, Wuhan 430072, Peoples R China.
C3 Wuhan University
RP Xiao, CX (corresponding author), Wuhan Univ, Sch Comp, Wuhan 430072, Peoples R China.
EM cxxiao@whu.edu.cn; whdxgjj@whu.edu.cn
RI yang, zhou/KBB-6972-2024
CR [Anonymous], 2001, P IEEE COMP SOC C CO
   [Anonymous], 2003, IEEE WORKSH COL PHOT
   DERICHE R, 1993, 1893 INRIA
   Fattal R, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360671
   He KM, 2010, LECT NOTES COMPUT SC, V6311, P1
   He KM, 2009, PROC CVPR IEEE, P1956, DOI [10.1109/CVPRW.2009.5206515, 10.1109/CVPR.2009.5206515]
   Kopf J, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1276377.1276497, 10.1145/1239451.1239547]
   Kopf J, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409069
   Levin A, 2008, IEEE T PATTERN ANAL, V30, P228, DOI 10.1109/TPAMI.2007.1177
   Narasimhan S., 2008, ACM SIGGRAPH ASIA 20, P69, DOI DOI 10.1109/ICACTE.2008.187
   Narasimhan S G, 2008, ACM SIGGRAPH ASIA 20
   Narasimhan SG, 2000, PROC CVPR IEEE, P598, DOI 10.1109/CVPR.2000.855874
   Paris Sylvain, 2009, BILATERAL FILTERING
   Petschnigg G, 2004, ACM T GRAPHIC, V23, P664, DOI 10.1145/1015706.1015777
   Schechner Y., 2008, ACM SIGGRAPH ASIA 20
   Shwartz S., 2006, 2006 IEEE COMP SOC C, V2, P1984, DOI DOI 10.1109/CVPR.2006.71
   Tan R., 2008, CVPR 2008
   Tarel JP, 2009, IEEE I CONF COMP VIS, P2201, DOI 10.1109/ICCV.2009.5459251
   Tomasi C, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P839, DOI 10.1109/ICCV.1998.710815
   Weiss B, 2006, ACM T GRAPHIC, V25, P519, DOI 10.1145/1141911.1141918
   Xiao CX, 2011, IEEE T VIS COMPUT GR, V17, P1135, DOI 10.1109/TVCG.2010.125
   Xiao CX, 2011, IEEE T VIS COMPUT GR, V17, P1122, DOI 10.1109/TVCG.2010.226
   Xiao CX, 2010, COMPUT GRAPH FORUM, V29, P2065, DOI 10.1111/j.1467-8659.2010.01793.x
   Yang QX, 2009, PROC CVPR IEEE, P557, DOI 10.1109/CVPRW.2009.5206542
NR 24
TC 177
Z9 194
U1 2
U2 48
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2012
VL 28
IS 6-8
BP 713
EP 721
DI 10.1007/s00371-012-0679-y
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 947EW
UT WOS:000304411500019
DA 2024-07-18
ER

PT J
AU Nugjgar, P
   Fujimoto, T
   Chiba, N
AF Nugjgar, Purevtsogt
   Fujimoto, Tadahiro
   Chiba, Norishige
TI Markov-type velocity field for efficiently animating water stream
SO VISUAL COMPUTER
LA English
DT Article
DE Natural phenomena; Stochastic-modeling; Tracer particles; Endless
   animation; Water stream
ID SIMULATION
AB In computer graphics, one of the most challenging tasks is continuously varying phenomena such as waving, swaying, and flowing motions. In this paper, we present a novel hybrid model (physical-stochastic) to create an endless animation in which offline simulation is used to produce an infinitely varying real-time animated result. In this particular case, a water stream model is proposed. Most fully 3D physically based simulation methods for depicting fluid flows are very time and memory consuming. Thus, these methods are still reserved for offline simulations and small-domain real-time simulations, especially in the case of fluid flows with irregularly repeating patterns. The proposed model is based on the tracer particle technique, uses a non-static velocity field, and consists of two main phases. In the first phase, we construct the stochastic velocity field by using the physically based method. The second phase is the main part, in which we create real-time endless animation. Here, we introduce a new type of velocity field which we refer to as a Markov-type velocity field (MTVF). MTVF allows us to animate a water stream endlessly in real-time by avoiding the time-consuming process of solving the corresponding equations for every simulation step.
C1 [Nugjgar, Purevtsogt; Fujimoto, Tadahiro; Chiba, Norishige] Iwate Univ, Grad Sch Engn, Morioka, Iwate, Japan.
C3 Iwate University
RP Nugjgar, P (corresponding author), Iwate Univ, Grad Sch Engn, Morioka, Iwate, Japan.
EM puugii@cg.cis.iwate-u.ac.jp; fujimoto@cis.iwate-u.ac.jp;
   nchiba@cis.iwate-u.ac.jp
RI Nugjgar, Purevtsogt/IUQ-0835-2023
CR Burrell T, 2009, COMPUT ANIMAT VIRT W, V20, P163, DOI 10.1002/cav.288
   Chentanez N, 2010, P ACM SIGGRAPH EUROG
   Enright D, 2002, ACM T GRAPHIC, V21, P736, DOI [10.1145/566570.566581, 10.1145/566570.566645]
   Foster N, 1996, GRAPH MODEL IM PROC, V58, P471, DOI 10.1006/gmip.1996.0039
   Foster N, 2001, COMP GRAPH, P23, DOI 10.1145/383259.383261
   HARLOW F, 1965, J FLUIDS PHYS, V181
   Kipfer P, 2006, PROC GRAPH INTERF, P41
   Koshizuka S, 1996, NUCL SCI ENG, V123, P421, DOI 10.13182/NSE96-A24205
   Koshizuka S., 1995, Computational Fluid Dynamics Journal, V113, P134
   Maes MarceloM., 2006, P 4 INT C COMPUTER G, P107
   MONAGHAN JJ, 1992, ANNU REV ASTRON ASTR, V30, P543, DOI 10.1146/annurev.aa.30.090192.002551
   Premoze S, 2003, COMPUT GRAPH FORUM, V22, P401, DOI 10.1111/1467-8659.00687
   REEVES WT, 1983, ACM T GRAPHIC, V2, P91, DOI 10.1145/964967.801167
   Shi SX, 2007, SIMUL MODEL PRACT TH, V15, P635, DOI 10.1016/j.simpat.2007.01.004
   Stam J, 1999, COMP GRAPH, P121, DOI 10.1145/311535.311548
   Thon S, 2002, ADVANCES IN MODELLING, ANIMATION AND RENDERING, P333
   Yu QZ, 2009, COMPUT GRAPH FORUM, V28, P239, DOI 10.1111/j.1467-8659.2009.01363.x
NR 17
TC 2
Z9 2
U1 0
U2 1
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2012
VL 28
IS 2
BP 219
EP 229
DI 10.1007/s00371-011-0637-0
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 881SW
UT WOS:000299510100008
DA 2024-07-18
ER

PT J
AU Bein, M
   Fellner, DW
   Stork, A
AF Bein, Matthias
   Fellner, Dieter W.
   Stork, Andre
TI Genetic B-Spline approximation on combined B-reps
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Conference on the Computer Graphics International (CGI)
CY JUN 12-15, 2011
CL Ottawa, CANADA
DE Spline; Approximation; Genetic; Parallel; Combined B-reps; Subdivision
AB We present a genetic algorithm for approximating densely sampled curves with uniform cubic B-Splines suitable for Combined B-reps. A feature of this representation is altering the continuity property of the B-Spline at any knot, allowing to combine freeform curves and polygonal parts within one representation. Naturally there is a trade-off between different approximation properties like accuracy and the number of control points needed. Our algorithm creates very accurate B-Splines with few control points, as shown in Fig. 1. Since the approximation problem is highly nonlinear, we approach it with genetic methods, leading to better results compared to classical gradient based methods. Parallelization and adapted evolution strategies are used to create results very fast.
C1 [Bein, Matthias] Tech Univ Darmstadt, D-64283 Darmstadt, Germany.
   Fraunhofer IGD, D-64283 Darmstadt, Germany.
C3 Technical University of Darmstadt
RP Bein, M (corresponding author), Tech Univ Darmstadt, Fraunhoferstr 5, D-64283 Darmstadt, Germany.
EM matthias.bein@gris.tu-darmstadt.de; d.fellner@igd.fraunhofer.de;
   andre.stork@igd.fraunhofer.de
OI Fellner, Dieter W./0000-0001-7756-0901
CR CATMULL E, 1978, COMPUT AIDED DESIGN, V10, P350, DOI 10.1016/0010-4485(78)90110-0
   Cohen E., 2001, Geometric modeling with splines: an introduction, V1st
   De Boor C., 1978, A practical guide to splines
   Farin G., 1993, CURVES SURFACES CAGD, VThird
   GOLDENTHAL R, 2003, INT C EV METH DES EU
   Golub G.H., 2013, Matrix Computations, DOI DOI 10.56021/9781421407944
   HAVEMANN S, 2005, THESIS BRAUNSCHWEIG
   Havemann S, 2008, JOURNAL WSCG, V16, P121
   Hoschek J., 1988, Computer-Aided Geometric Design, V5, P27, DOI 10.1016/0167-8396(88)90017-9
   Juhasz I., 2001, J GEOMETRY GRAPHICS, V5, P111
   LAURENTGENGOUX P, 1993, COMPUT AIDED DESIGN, V25, P699, DOI 10.1016/0010-4485(93)90011-C
   Markus A, 1995, MATHEMATICAL METHODS FOR CURVES AND SURFACES, P343
   Markus A, 1997, 1997 INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, PROCEEDINGS, P47, DOI 10.1109/SMA.1997.634881
   Renner G, 2003, COMPUT AIDED DESIGN, V35, P709, DOI 10.1016/S0010-4485(03)00003-4
   SAPIDIS N, 1990, COMPUT AIDED DESIGN, V22, P121, DOI 10.1016/0010-4485(90)90006-X
   SHENE DCK, 2008, CS3621 INTRO COMPUTI
   Speer T, 1998, COMPUT AIDED GEOM D, V15, P869, DOI 10.1016/S0167-8396(98)00024-7
NR 17
TC 0
Z9 0
U1 0
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2011
VL 27
IS 6-8
BP 485
EP 494
DI 10.1007/s00371-011-0592-9
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 766FY
UT WOS:000290767600008
DA 2024-07-18
ER

PT J
AU Wei, L
   Sourin, A
AF Wei, Lei
   Sourin, Alexei
TI Function-based approach to mixed haptic effects rendering
SO VISUAL COMPUTER
LA English
DT Article
DE Haptic interaction; Collision detection; Function-based modeling
ID COLLISION DETECTION
AB Commonly, surface and solid haptic effects are defined in such a way that they hardly can be rendered together. We propose a method for defining mixed haptic effects including surface, solid, and force fields. These haptic effects can be applied to virtual scenes containing various objects, including polygon meshes, point clouds, impostors, and layered textures, voxel models as well as function-based shapes. Accordingly, we propose a way how to identify location of the haptic tool in such virtual scenes as well as consistently and seamlessly determine haptic effects when the haptic tool moves in the scenes with objects having different sizes, locations, and mutual penetrations. To provide for an efficient and flexible rendering of haptic effects, we propose to concurrently use explicit, implicit and parametric functions, and algorithmic procedures.
C1 [Wei, Lei; Sourin, Alexei] Nanyang Technol Univ, Sch Comp Engn, Singapore, Singapore.
C3 Nanyang Technological University
RP Wei, L (corresponding author), Nanyang Technol Univ, Sch Comp Engn, Singapore, Singapore.
EM lwei@ntu.edu.sg; assourin@ntu.edu.sg
RI Sourin, Alexei/A-3701-2011
OI Wei, Lei/0000-0001-8267-0283; Sourin, Alexei/0000-0003-4051-2927
FU Singapore National Research Foundation [NRF2008IDM-IDM004-002]
FX This project is supported by the Singapore National Research Foundation
   Interactive Digital Media R&D Program, under research Grant
   NRF2008IDM-IDM004-002 "Visual and Haptic Rendering in Co-Space."
CR [Anonymous], 1996, FORCE TOUCH FEEDBACK
   Basdogan C., 1997, Proceedings of the ASME Dynamic Systems and Control Division, P77
   Booth S., 2003, PROC EUROHAPTICS, P374
   El-Far NR, 2007, IEEE ACM DIS SIM, P15, DOI 10.1109/DS-RT.2007.29
   ELFAR NR, 2007, P VIRT ENV HUM COMP, P25
   Gregory A, 2000, IEEE VISUAL, P139, DOI 10.1109/VISUAL.2000.885687
   GREGORY A, 2005, P ACM SIGGRAPH 2005, P38
   Hua J, 2002, IEEE/ACM SIGGRAPH SYMPOSIUM ON VOLUME VISUALIZATION AND GRAPHICS 2002, PROCEEDINGS, P55, DOI 10.1109/SWG.2002.1226510
   Kim L, 2002, 2002 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, VOLS 1-3, PROCEEDINGS, P2943, DOI 10.1109/IRDS.2002.1041719
   Knott D, 2003, PROC GRAPH INTERF, P73
   KOLCAREK P, 2005, P 3 INT C COMP GRAPH, P389
   Lundin K, 2005, WORLD HAPTICS CONFERENCE: FIRST JOINT EUROHAPTICS CONFERENCE AND SYMPOSIUM ON HAPTIC INTERFACES FOR VIRUTUAL ENVIRONMENT AND TELEOPERATOR SYSTEMS, PROCEEDINGS, P557
   LUNDIN K, 2006, P INT WORKSH VOL GRA, P75
   McNeely W. A., 2005, P ACM SIGGRAPH 2005
   Mirtich B, 1998, ACM T GRAPHIC, V17, P177, DOI 10.1145/285857.285860
   Moustakas K, 2007, IEEE T VIS COMPUT GR, V13, P80, DOI 10.1109/TVCG.2007.20
   PASKO A, 1995, VISUAL COMPUT, V11, P429, DOI 10.1007/BF02464333
   Raymaekers C., 2001, P 6 PHANTOM US GROUP, P19
   Ruspini D. C., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P345, DOI 10.1145/258734.258878
   Schomer E., 2002, P 7 ACM S SOL MOD AP, P321
   Sourin A., 2008, P 7 ACM SIGGRAPH INT
   Sourin A, 2009, VIRTUAL REAL-LONDON, V13, P221, DOI 10.1007/s10055-009-0133-2
   Wei L, 2008, VISUAL COMPUT, V24, P871, DOI 10.1007/s00371-008-0285-1
   Wei LL, 2009, PROCEEDINGS OF THE 2009 SECOND PACIFIC-ASIA CONFERENCE ON WEB MINING AND WEB-BASED APPLICATION, P15, DOI 10.1109/WMWA.2009.56
   Weller R., 2009, P 2009 ACM SIGGRAPH, P151, DOI DOI 10.1145/1581073.1581097
NR 25
TC 7
Z9 7
U1 0
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2011
VL 27
IS 4
SI SI
BP 321
EP 332
DI 10.1007/s00371-011-0548-0
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 739IC
UT WOS:000288707700008
DA 2024-07-18
ER

PT J
AU Cong, L
   Tong, RF
   Dong, JX
AF Cong, Lin
   Tong, Ruofeng
   Dong, Jinxiang
TI Selective image abstraction
SO VISUAL COMPUTER
LA English
DT Article
DE Selective abstraction; Nonlinear diffusion; Image generation; Mixed
   reality
ID EDGE-DETECTION; SPACE
AB We present a novel and convenient method for producing selective stylized simplification of images. The user uses a brush to interactively mark certain areas of the input image which are to be left unaltered. Boundaries of these areas are then automatically optimized to underlying object boundaries in the image. Our method then performs stylized simplification of the unmarked areas, while preserving the marked areas. The method ensures a smooth transition between stylized and unaltered regions to leave a mixed reality image which combines the real and the abstract. Stylized simplification is performed using nonlinear diffusion, which can generate sophisticated results. We modify the classic model of nonlinear diffusion to incorporate bilateral filtering; we apply diffusion speed control of each pixel based on the user's input. The level of simplification can be controlled intuitively based on the diffusion time; another parameter controls the abstraction style, giving a simple and intuitive user interface. Our contributions include a simple-to-use method to produce a novel NPR style and a modified nonlinear diffusion model suited to this selective stylized simplification task. Experimental results show that the final mixed reality results are harmonious.
C1 [Cong, Lin; Tong, Ruofeng; Dong, Jinxiang] Zhejiang Univ, Dept Comp Sci & Technol, Hangzhou 310027, Peoples R China.
C3 Zhejiang University
RP Cong, L (corresponding author), Zhejiang Univ, Dept Comp Sci & Technol, Hangzhou 310027, Peoples R China.
EM jeffersoncong@gmail.com
FU National HighTech Research and Development Program of China
   [2009AA01Z330]; National Basic Research Project of China [2006CB303106]
FX This work is supported by the National HighTech Research and Development
   Program of China (Project Number 2009AA01Z330) and the National Basic
   Research Project of China (Project Number 2006CB303106).
CR ALVAREZ L, 1992, SIAM J NUMER ANAL, V29, P845, DOI 10.1137/0729052
   Boykov YY, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P105, DOI 10.1109/ICCV.2001.937505
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851
   CATTE F, 1992, SIAM J NUMER ANAL, V29, P182, DOI 10.1137/0729012
   Collomosse JP, 2005, IEEE T VIS COMPUT GR, V11, P540, DOI 10.1109/TVCG.2005.85
   Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236
   DeCarlo D, 2002, ACM T GRAPHIC, V21, P769, DOI 10.1145/566570.566650
   Fischer J, 2005, P IEEE VIRT REAL ANN, P195
   Kang H, 2007, NPAR 2007: 5TH INTERNATIONAL SYMPOSIUM ON NON-PHOTOREALISTIC ANIMATION AND RENDERING, PROCEEDINGS, P43
   Kang H, 2009, IEEE T VIS COMPUT GR, V15, P62, DOI 10.1109/TVCG.2008.81
   Kang H, 2008, COMPUT GRAPH FORUM, V27, P1773, DOI 10.1111/j.1467-8659.2008.01322.x
   KOENDERINK JJ, 1984, BIOL CYBERN, V50, P363, DOI 10.1007/BF00336961
   Orzan A, 2007, NPAR 2007: 5TH INTERNATIONAL SYMPOSIUM ON NON-PHOTOREALISTIC ANIMATION AND RENDERING, PROCEEDINGS, P103
   Paris S, 2009, INT J COMPUT VISION, V81, P24, DOI 10.1007/s11263-007-0110-8
   PERONA P, 1990, IEEE T PATTERN ANAL, V12, P629, DOI 10.1109/34.56205
   Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720
   SANTELLA A, 2004, VISUAL INTEREST NPR, P71, DOI DOI 10.1145/987657.987669
   Tomasi C, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P839, DOI 10.1109/ICCV.1998.710815
   Wang J, 2004, ACM T GRAPHIC, V23, P574, DOI 10.1145/1015706.1015763
   WEN F, 2006, NPAR 06, P47, DOI DOI 10.1145/1124728.1124737
   Winnemöller H, 2006, ACM T GRAPHIC, V25, P1221, DOI 10.1145/1141911.1142018
   Zhang SH, 2009, IEEE T VIS COMPUT GR, V15, P618, DOI 10.1109/TVCG.2009.9
   Zhang YF, 2008, COMPUT GRAPH FORUM, V27, P1797, DOI 10.1111/j.1467-8659.2008.01325.x
NR 23
TC 10
Z9 11
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2011
VL 27
IS 3
BP 187
EP 198
DI 10.1007/s00371-010-0522-2
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 722QS
UT WOS:000287450000002
DA 2024-07-18
ER

PT J
AU Cui, M
   Hu, JX
   Razdan, A
   Wonka, P
AF Cui, Ming
   Hu, Jiuxiang
   Razdan, Anshuman
   Wonka, Peter
TI Color-to-gray conversion using ISOMAP
SO VISUAL COMPUTER
LA English
DT Article
DE ISOMAP; Color to gray; Color space
ID DIMENSIONALITY REDUCTION; REPRODUCTION; DISPLAY; IMAGES; EIGENMAPS
AB In this paper we present a new algorithm to transform an RGB color image to a grayscale image. We propose using nonlinear dimension reduction techniques to map higher dimensional color vectors to lower dimensional ones. This approach generalizes the gradient domain manipulation for high dimensional images. Our experiments show that the proposed algorithm generates competitive results and reaches a good compromise between quality and speed.
C1 [Cui, Ming; Hu, Jiuxiang; Razdan, Anshuman; Wonka, Peter] Arizona State Univ, Phoenix, AZ USA.
C3 Arizona State University; Arizona State University-Downtown Phoenix
RP Cui, M (corresponding author), Arizona State Univ, Phoenix, AZ USA.
EM ming.cui@asu.edu
CR [Anonymous], 2005, P 22 INT C MACHINE L
   ARTUSI A, 2003, RENDERING TECHNIQUES, P38
   Bachmann CM, 2006, IEEE T GEOSCI REMOTE, V44, P2786, DOI 10.1109/TGRS.2006.881801
   Bachmann CM, 2005, IEEE T GEOSCI REMOTE, V43, P441, DOI 10.1109/TGRS.2004.842292
   Bala R, 2004, P SOC PHOTO-OPT INS, V5293, P196, DOI 10.1117/12.532192
   Belkin M, 2003, NEURAL COMPUT, V15, P1373, DOI 10.1162/089976603321780317
   Borg I., 2005, Modern Multidimensional Scaling: Theory and Applications
   Cadík M, 2008, COMPUT GRAPH FORUM, V27, P1745
   CAI N, 2006, IGARSS AUG, P537
   Cui M, 2009, IEEE T GEOSCI REMOTE, V47, P1673, DOI 10.1109/TGRS.2008.2010129
   de Silva V., 2003, Global versus local methods in nonlinear dimensionality reduction
   Donoho DL, 2003, P NATL ACAD SCI USA, V100, P5591, DOI 10.1073/pnas.1031596100
   Durand F, 2002, ACM T GRAPHIC, V21, P257, DOI 10.1145/566570.566574
   Fairchild M.D., 2005, Color Appearance Models, V2nd
   FATTAL R, 2002, SIGGRAPH 02, P249
   Fattal R, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531328
   GOOCH AA, 2005, SIGGRAPH 2005, P634
   Grundland M, 2007, PATTERN RECOGN, V40, P2891, DOI 10.1016/j.patcog.2006.11.003
   Han T, 2007, INT GEOSCI REMOTE SE, P1556, DOI 10.1109/IGARSS.2007.4423107
   Jacobson NP, 2005, IEEE T GEOSCI REMOTE, V43, P2684, DOI 10.1109/TGRS.2005.857623
   Ledda P, 2005, ACM T GRAPHIC, V24, P640, DOI 10.1145/1073204.1073242
   Li YZ, 2005, ACM T GRAPHIC, V24, P836, DOI 10.1145/1073204.1073271
   Lischinski D, 2006, ACM T GRAPHIC, V25, P646, DOI 10.1145/1141911.1141936
   Mantiuk R., 2006, ACM Transactions on Applied Perception, V3, P286, DOI DOI 10.1145/1166087.1166095
   Nadler B, 2006, Advances in Neural Information Processing Systems, V18, P955
   Park SH, 2007, J VIS COMMUN IMAGE R, V18, P415, DOI 10.1016/j.jvcir.2007.06.008
   Rasche K, 2005, COMPUT GRAPH FORUM, V24, P423, DOI 10.1111/j.1467-8659.2005.00867.x
   Rasche K, 2005, IEEE COMPUT GRAPH, V25, P22, DOI 10.1109/MCG.2005.54
   Reinhard E, 2002, ACM T GRAPHIC, V21, P267, DOI 10.1145/566570.566575
   Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323
   Silva V.D., 2004, Technical Report
   Smith K, 2008, COMPUT GRAPH FORUM, V27, P193, DOI 10.1111/j.1467-8659.2008.01116.x
   SMITH R, 2006, ANAL HYPERSPECTRAL I
   Socolinsky DA, 2002, IEEE T IMAGE PROCESS, V11, P923, DOI 10.1109/TIP.2002.801588
   Tenenbaum JB, 2000, SCIENCE, V290, P2319, DOI 10.1126/science.290.5500.2319
   THOMAS BA, 1997, IEEE INT C IM PROC O, V3, P30
   TUMBLIN J, 1993, IEEE COMPUT GRAPH, V13, P42, DOI 10.1109/38.252554
   Tyo JS, 2003, IEEE T GEOSCI REMOTE, V41, P708, DOI 10.1109/TGRS.2003.808879
   Wang J, 2006, IEEE T GEOSCI REMOTE, V44, P1586, DOI 10.1109/TGRS.2005.863297
NR 39
TC 14
Z9 18
U1 0
U2 7
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2010
VL 26
IS 11
BP 1349
EP 1360
DI 10.1007/s00371-009-0412-7
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 653UV
UT WOS:000282123700002
DA 2024-07-18
ER

PT J
AU Han, DF
   Sonka, M
   Bayouth, J
   Wu, XD
AF Han, Dongfeng
   Sonka, Milan
   Bayouth, John
   Wu, Xiaodong
TI Optimal multiple-seams search for image resizing with smoothness and
   shape prior
SO VISUAL COMPUTER
LA English
DT Article
DE Image resizing; Multiple seams search; Minimum s-t cut; Geometric
   constraints
AB Content-aware image resizing is of increasing relevance to allow high-quality image and video to be displayed on devices with different resolution. We present a novel method to find multiple seams simultaneously with global optimality for image resizing, incorporating both region smoothness and seam shape prior using a 3-D graph-theoretic approach. The globally optimal seams can be simultaneously achieved by solving a maximum flow problem based on an arc-weighted graph representation. Representing the resizing problem in an arc-weighted graph, we can incorporate a wide spectrum of constraints into the formulation, thus improving resizing results. By removing or inserting those multiple seams, the goal of content-aware image resizing is achieved. Due to simultaneous detection of multiple seams, our algorithm exhibits several good features: the ability to handle both crossing and non-crossing-seam cases, the ability to incorporate various feasible geometry constraints, and the ability to incorporate the seams importance, region smoothness and shape prior information. The proposed method was implemented and experimented on a variety of image data and compared with the state of the art in image resizing.
C1 [Han, Dongfeng; Sonka, Milan; Bayouth, John; Wu, Xiaodong] Univ Iowa, Dept Radiat Oncol, Iowa City, IA 52242 USA.
   [Sonka, Milan; Wu, Xiaodong] Univ Iowa, Dept Elect & Comp Engn, Iowa City, IA 52242 USA.
C3 University of Iowa; University of Iowa
RP Wu, XD (corresponding author), Univ Iowa, Dept Radiat Oncol, Iowa City, IA 52242 USA.
EM handongfeng@gmail.com; milan-sonka@uiowa.edu; john-bayouth@uiowa.edu;
   xiaodong-wu@uiowa.edu
RI Sonka, Milan/IZD-9792-2023; Sonka, Milan/F-6227-2017
OI Sonka, Milan/0000-0002-9613-9968; Sonka, Milan/0000-0002-9613-9968
FU NSF [CCF-0830402, CCF-0844765]; NIH [R01 EB004640]; Division of
   Computing and Communication Foundations; Direct For Computer & Info Scie
   & Enginr [0844765] Funding Source: National Science Foundation
FX The authors thank Qi Song for valuable discussions. This research was
   supported in part by the NSF grants CCF-0830402 and CCF-0844765, and the
   NIH grant R01 EB004640.
CR [Anonymous], P 11 IEEE INT C COMP
   [Anonymous], 2006, P 14 ACM INT C MULT
   [Anonymous], 2009, IEEE INT C COMP VIS
   Avidan S, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239461
   Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114
   Boykov Y, 2004, IEEE T PATTERN ANAL, V26, P1124, DOI 10.1109/TPAMI.2004.60
   Gal R., 2006, P 17 EUROGRAPHICS C, P297, DOI DOI 10.2312/EGWR/EGSR06/297-303
   HAN D, 2009, ICCV 09
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Kolmogorov V, 2004, IEEE T PATTERN ANAL, V26, P147, DOI 10.1109/TPAMI.2004.1262177
   Kopf Stephan., 2009, MM 09, P321
   Li K, 2006, IEEE T PATTERN ANAL, V28, P119, DOI 10.1109/TPAMI.2006.19
   Li Z, 2009, IEEE T IMAGE PROCESS, V18, P2572, DOI 10.1109/TIP.2009.2026677
   PRITCH Y, 2009, ICCV 09
   RUBINSTEIN M, 2009, SIGGRAPH 09, P1, DOI DOI 10.1145/1576246.1531329
   Rubinstein M, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360615
   Setlur Vidya., 2005, MUM, V154, P59, DOI DOI 10.1145/1149488.1149499
   Shamir A, 2009, COMMUN ACM, V52, P77, DOI 10.1145/1435417.1435437
   Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb
   Wang J, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1330511.1330512
   Zhang YF, 2008, COMPUT GRAPH FORUM, V27, P1797, DOI 10.1111/j.1467-8659.2008.01325.x
NR 21
TC 13
Z9 16
U1 0
U2 1
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2010
VL 26
IS 6-8
BP 749
EP 759
DI 10.1007/s00371-010-0480-8
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 602JQ
UT WOS:000278135800035
DA 2024-07-18
ER

PT J
AU Wang, ZL
   Nguyen, BP
   Chui, CK
   Qin, J
   Ang, CH
   Ong, SH
AF Wang, Zhenlan
   Nguyen, Binh P.
   Chui, Chee-Kong
   Qin, Jing
   Ang, Chuan-Heng
   Ong, Sim-Heng
TI An efficient clustering method for fast rendering of time-varying
   volumetric medical data
SO VISUAL COMPUTER
LA English
DT Article
DE Time-varying volume rendering; BIRCH; Clustering; 4-D medical images
ID LOSSLESS COMPRESSION; IMAGES
AB Visualization and exploration of time-varying volumetric medical data help clinicians for better diagnosis and treatment. However, it is a challenge to render these data in an interactive manner because of their complexity and large size. We propose an efficient clustering method for fast compression and rendering of these large dynamic data. We divide the volumes into a set of blocks and use a BIRCH (Balanced Iterative Reducing and Clustering using Hierarchies) based algorithm to cluster them, which can usually find a high quality clustering with a single scan of the blocks. In addition, the granularity of clusters can be adaptively adjusted by dynamically configuring threshold values. In each cluster of blocks, a KeyBlock is generated to represent the cluster, and therefore the storage space of the volumes is reduced greatly. In addition, we assign a lifetime to every KeyBlock and implement a dynamic memory management scheme to further reduce the storage space. During the rendering, each KeyBlock is rendered as a KeyImage, which can be reused if the view transformation and transfer function are not changed. This reuse can help to increase the rendering speed significantly. Experimental results showed that the proposed method can achieve good performance in terms of both speed optimization and space reduction.
C1 [Nguyen, Binh P.; Ong, Sim-Heng] Natl Univ Singapore, Fac Engn, Dept Elect & Comp Engn, Singapore 117548, Singapore.
   [Wang, Zhenlan; Chui, Chee-Kong] Natl Univ Singapore, Fac Engn, Dept Mech Engn, Singapore 117548, Singapore.
   [Qin, Jing] Natl Univ Singapore, Yong Loo Lin Sch Med, Dept Diagnost Radiol, Singapore 117548, Singapore.
   [Ang, Chuan-Heng] Natl Univ Singapore, Sch Comp, Dept Comp Sci, Singapore 117548, Singapore.
C3 National University of Singapore; National University of Singapore;
   National University of Singapore; National University of Singapore
RP Nguyen, BP (corresponding author), Natl Univ Singapore, Fac Engn, Dept Elect & Comp Engn, Singapore 117548, Singapore.
EM mpewzl@nus.edu.sg; phubinh@nus.edu.sg; mpecck@nus.edu.sg;
   dnrqj@nus.edu.sg; dcsach@nus.edu.sg; eleongsh@nus.edu.sg
RI Nguyen, Binh P./N-8193-2013; Ong, Sim-Heng/R-9244-2019; Qin,
   Jing/J-9807-2016; Qin, Jing/JMC-1371-2023
OI Nguyen, Binh P./0000-0001-6203-6664; Ong, Sim-Heng/0000-0003-2766-8150;
   Qin, Jing/0000-0002-7059-0929; 
CR Armitage P, 2005, MED IMAGE ANAL, V9, P315, DOI 10.1016/j.media.2005.01.001
   Chrysafis C, 2000, INT CONF ACOUST SPEE, P2035, DOI 10.1109/ICASSP.2000.859233
   Ellsworth D., 2000, P 2000 S VOL VIS, P119
   Kamasak ME, 2005, IEEE T MED IMAGING, V24, P636, DOI 10.1109/TMI.2005.845317
   Kassim AA, 2005, IEEE T INF TECHNOL B, V9, P132, DOI 10.1109/TITB.2004.838376
   Lalgudi H.G., 2005, P ICIP 2005, V2, P746
   LEVOY M, 1990, ACM T GRAPHIC, V9, P245, DOI 10.1145/78964.78965
   Liao SK, 2004, COMPUT GRAPH-UK, V28, P279, DOI 10.1016/j.cag.2003.12.012
   Liao SK, 2003, J VISUAL LANG COMPUT, V14, P233, DOI 10.1016/S1045-926X(03)00020-X
   Liu YC, 2007, PHYS EARTH PLANET IN, V163, P233, DOI 10.1016/j.pepi.2007.02.012
   Porter T., 1984, Computers & Graphics, V18, P253
   Said A, 1996, IEEE T CIRC SYST VID, V6, P243, DOI 10.1109/76.499834
   Sanchez V, 2008, IEEE T INF TECHNOL B, V12, P442, DOI 10.1109/TITB.2007.911307
   Schneider J, 2003, IEEE VISUALIZATION 2003, PROCEEDINGS, P293, DOI 10.1109/VISUAL.2003.1250385
   Schroeder W., 1998, The visualization toolkit an object-oriented approach to 3D graphics
   SHAPIRO JM, 1993, IEEE T SIGNAL PROCES, V41, P3445, DOI 10.1109/78.258085
   Shen H.-W., 1999, Proceedings Visualization '99 (Cat. No.99CB37067), P371, DOI 10.1109/VISUAL.1999.809910
   Shen Han-Wei., 1994, P IEEE VISUALIZATION, P180
   Wang ZL, 2006, INT J IMAGE GRAPH, V6, P155, DOI 10.1142/S0219467806002173
   Wilhelms Jane., 1994, P 1994 S VOLUME VISU, P27
   Zeng L, 2002, IEEE T MED IMAGING, V21, P1179, DOI 10.1109/TMI.2002.804424
   Zhang T., 1996, BIRCH EFFICIENT DATA, V25, P103, DOI [DOI 10.1145/233269.233324, 10.1145/235968.233324]
NR 22
TC 1
Z9 1
U1 0
U2 7
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2010
VL 26
IS 6-8
BP 1061
EP 1070
DI 10.1007/s00371-010-0483-5
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 602JQ
UT WOS:000278135800065
DA 2024-07-18
ER

PT J
AU Jong, BS
   Chiang, CH
   Lee, PF
   Lin, TW
AF Jong, Bin-Shyan
   Chiang, Chien-Hsing
   Lee, Pai-Feng
   Lin, Tsong-Wuu
TI High quality surface remeshing with equilateral triangle grid
SO VISUAL COMPUTER
LA English
DT Article
DE Remeshing; Mesh optimization; Surface segmentation; Shape feature
   preservation
ID MESHES
AB This study proposes a robust and efficient 3D surface remeshing algorithm for mesh quality optimization. Instead of the global mesh relaxation method proposed in the previous study conducted on remeshing, this study proposes an equilateral triangle grid-resampling scheme for achieving mesh optimization more efficiently. In order to improve the feasibility of resampling by directly using an equilateral triangle grid, the surface structure of the original model is correctly extracted by an automatic surface segmentation technique before the resampling step is executed. Results of this study show that the proposed remeshing algorithm can automatically and substantially improve the quality of triangulation, as well as automatically preserve shape features under an acceptable level of measurement error in the shape approximation, which is suitable for a mesh with a specific topology.
C1 [Chiang, Chien-Hsing] Chung Yuan Christian Univ, Dept Elect Engn, Chungli, Taiwan.
   [Jong, Bin-Shyan] Chung Yuan Christian Univ, Dept Informat & Comp Engn, Chungli, Taiwan.
   [Lee, Pai-Feng] Hsing Wu Coll, Dept Informat Management, Taipei 244, Taiwan.
   [Lin, Tsong-Wuu] Soochow Univ, Dept Sci & Informat Management, Taipei 100, Taiwan.
C3 Chung Yuan Christian University; Chung Yuan Christian University;
   Soochow University
RP Chiang, CH (corresponding author), Chung Yuan Christian Univ, Dept Elect Engn, 22 Pu Jen, Chungli, Taiwan.
EM bsjong@ice.cycu.edu.tw; hikki@cg.ice.cycu.edu.tw;
   093094@mail.hwc.edu.tw; twlin@csim.scu.edu.tw
CR Alliez P, 2003, SMI 2003: SHAPE MODELING INTERNATIONAL 2003, PROCEEDINGS, P49
   Alliez P, 2003, ACM T GRAPHIC, V22, P485, DOI 10.1145/882262.882296
   Alliez P, 2001, COMPUT GRAPH FORUM, V20, pC480, DOI 10.1111/1467-8659.00541
   Alliez Pierre., 2005, RECENT ADV REMESHING
   Attene M, 2006, VISUAL COMPUT, V22, P181, DOI 10.1007/s00371-006-0375-x
   Attene M., 2003, Proc. of the First Eurographics Symposium Geometry Processing (SGP'03), P63
   Ciarlet P.G., 1991, HDB NUMERICAL ANAL, VII
   Cignoni P, 1998, COMPUT GRAPH FORUM, V17, P167, DOI 10.1111/1467-8659.00236
   Cohen-Steiner D, 2004, ACM T GRAPHIC, V23, P905, DOI 10.1145/1015706.1015817
   Dong S, 2005, COMPUT AIDED GEOM D, V22, P392, DOI 10.1016/j.cagd.2005.04.004
   Du Q, 1999, SIAM REV, V41, P637, DOI 10.1137/S0036144599352836
   Floater MS, 2005, MATH VIS, P157, DOI 10.1007/3-540-26808-1_9
   FREY P, 1997, INT MESHING ROUNDTAB, P363
   Garland M., 1997, PROC 24 C COMPUTER G, P209, DOI DOI 10.1145/258734.258849
   Gopi M, 2000, COMPUT GRAPH FORUM, V19, pC467, DOI 10.1111/1467-8659.00439
   GOPI M, 2000, HIGH PERFORMANCE COM, V1, P1
   Guskov I, 2000, COMP GRAPH, P95, DOI 10.1145/344779.344831
   IOANA BM, 2004, P 2004 EUR ACM SIGGR, P193
   Jong BS, 2005, CISST '05: PROCEEDINGS OF THE 2005 INTERNATIONAL CONFERENCE ON IMAGING SCIENCE, SYSTEMS, AND TECHNOLOGY: COMPUTER GRAPHICS, P62
   Khodakovsky A, 2003, ACM T GRAPHIC, V22, P350, DOI 10.1145/882262.882275
   Khodakovsky A, 2004, MATH VISUAL, P189
   LAI YK, 2008, P ACM S SOL PHYS MOD
   Lee A. W., 1998, Proceedings of the 25th annual conference on Computer graphics and interactive techniques, P95, DOI DOI 10.1145/280814.280828
   Lévy B, 2002, ACM T GRAPHIC, V21, P362, DOI 10.1145/566570.566590
   Marez C., 2003, P EUR ACM SIGGRAPH S, V30, P20
   Marinov M, 2005, COMPUT GRAPH FORUM, V24, P479, DOI 10.1111/j.1467-8659.2005.00873.x
   Marinov M, 2004, 12TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P207, DOI 10.1109/PCCGA.2004.1348351
   Marinov M, 2006, COMPUT GRAPH FORUM, V25, P537, DOI 10.1111/j.1467-8659.2006.00973.x
   Payan F, 2004, 2004 IEEE 6TH WORKSHOP ON MULTIMEDIA SIGNAL PROCESSING, P327
   Payan F, 2002, FIRST INTERNATIONAL SYMPOSIUM ON 3D DATA PROCESSING VISUALIZATION AND TRANSMISSION, P391, DOI 10.1109/TDPVT.2002.1024089
   Shewchuk J., Triangle: a two-dimensional quality mesh generator and Delaunay triangulator
   SURAZHSKY V., 2003, P 12 INT MESHING ROU, P215
   Szymczak A, 2002, GRAPH MODELS, V64, P183, DOI 10.1006/gmod.2002.0577
   TURK G, 1992, COMP GRAPH, V26, P55, DOI 10.1145/142920.134008
   Valette S, 2008, IEEE T VIS COMPUT GR, V14, P369, DOI 10.1109/TVCG.2007.70430
   Yue W, 2007, P 2007 ACM S SOL PHY, P23
NR 36
TC 5
Z9 5
U1 0
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2010
VL 26
IS 2
BP 121
EP 136
DI 10.1007/s00371-009-0392-7
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 543RE
UT WOS:000273595200004
DA 2024-07-18
ER

PT J
AU Lo, CH
   Chu, CH
   Debattista, K
   Chalmers, A
AF Lo, Cheng-Hung
   Chu, Chih-Hsing
   Debattista, Kurt
   Chalmers, Alan
TI Selective rendering for efficient ray traced stereoscopic images
SO VISUAL COMPUTER
LA English
DT Article
DE Stereoscopic images; Perceptually-guided rendering; Virtual reality
ID COMPRESSION; OBJECTS
AB Depth-related visual effects are a key feature of many virtual environments. In stereo-based systems, the depth effect can be produced by delivering frames of disparate image pairs, while in monocular environments, the viewer has to extract this depth information from a single image by examining details such as perspective and shadows. This paper investigates via a number of psychophysical experiments, whether we can reduce computational effort and still achieve perceptually high-quality rendering for stereo imagery. We examined selectively rendering the image pairs by exploiting the fusing capability and depth perception underlying human stereo vision. In ray-tracing-based global illumination systems, a higher image resolution introduces more computation to the rendering process since many more rays need to be traced. We first investigated whether we could utilise the human binocular fusing ability and significantly reduce the resolution of one of the image pairs and yet retain a high perceptual quality under stereo viewing condition. Secondly, we evaluated subjects' performance on a specific visual task that required accurate depth perception. We found that subjects required far fewer rendered depth cues in the stereo viewing environment to perform the task well. Avoiding rendering these detailed cues saved significant computational time. In fact it was possible to achieve a better task performance in the stereo viewing condition at a combined rendering time for the image pairs less than that required for the single monocular image. The outcome of this study suggests that we can produce more efficient stereo images for depth-related visual tasks by selective rendering and exploiting inherent features of human stereo vision.
C1 [Lo, Cheng-Hung; Chu, Chih-Hsing] Natl Tsing Hua Univ, Dept Ind Engn & Engn Management, Hsinchu 30113, Taiwan.
   [Debattista, Kurt; Chalmers, Alan] Univ Warwick, Warwick Digital Lab, Coventry CV4 7AL, W Midlands, England.
C3 National Tsing Hua University; University of Warwick
RP Lo, CH (corresponding author), Natl Tsing Hua Univ, Dept Ind Engn & Engn Management, Hsinchu 30113, Taiwan.
EM loch@mx.nthu.edu.tw
RI Chu, Chih-Hsing/AAH-7772-2020; Lo, Cheng-Hung/ABG-7599-2020
OI Chu, Chih-Hsing/0000-0002-4673-5825; Lo, Cheng-Hung/0000-0002-7199-9339
CR *3DCOMBINE, STER IM GEN PROGR SH
   ADELSON SJ, 1992, ACM SE, V30, P148
   [Anonymous], SIGGRAPH
   Badt S.  Jr., 1988, Visual Computer, V4, P123, DOI 10.1007/BF01908895
   Barbour C. G., 1992, Visual Computer, V9, P151, DOI 10.1007/BF01902554
   Cater K., 2003, Eurographics Symposium on Rendering. 14th Eurographics Workshop on Rendering, P270
   *EDIMENSIONAL INC, WIR 3D SHUTT GLASS
   Hu HH, 2002, PRESENCE-TELEOP VIRT, V11, P652, DOI 10.1162/105474602321050758
   Hu HH, 2000, IEEE VISUAL, P179, DOI 10.1109/VISUAL.2000.885692
   Hubona G. S., 1999, ACM Transactions on Computer-Human Interaction, V6, P214, DOI 10.1145/329693.329695
   Kajiya J. T., 1986, Computer Graphics, V20, P143, DOI 10.1145/15886.15902
   Kim KT, 1998, P SOC PHOTO-OPT INS, V3295, P76, DOI 10.1117/12.307195
   KJELLDAHL L, 1995, COMPUT GRAPH, V19, P199, DOI 10.1016/0097-8493(94)00143-M
   Larson G.W., 1998, Rendering with radiance: the art and science of lighting visualization
   MCNAMARA A, 2000, P EUR WORKSH REND TE, P207
   MEYER GW, 1986, ACM T GRAPHIC, V5, P30, DOI 10.1145/7529.7920
   Myszkowski K, 2001, COMP GRAPH, P221, DOI 10.1145/383259.383284
   *NEUR SYST INC, PRES HIGH PREC PROGR
   PERKINS MG, 1992, IEEE T COMMUN, V40, P684, DOI 10.1109/26.141424
   SERVOS P, 1992, VISION RES, V32, P1513, DOI 10.1016/0042-6989(92)90207-Y
   Siegel M, 1997, P SOC PHOTO-OPT INS, V3012, P227, DOI 10.1117/12.274461
   WANGER L, 1992, SI3D 92, P39
   WANGER LR, 1992, IEEE COMPUT GRAPH, V12, P44, DOI 10.1109/38.135913
   WATT A, 1999, COMPUTER IMAGES
   Wendt G, 2008, J VISION, V8, DOI 10.1167/8.1.14
   Yee H, 2001, ACM T GRAPHIC, V20, P39, DOI 10.1145/383745.383748
   [No title captured]
NR 27
TC 5
Z9 6
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2010
VL 26
IS 2
BP 97
EP 107
DI 10.1007/s00371-009-0379-4
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 543RE
UT WOS:000273595200002
OA Green Submitted
DA 2024-07-18
ER

PT J
AU He, QZ
   Ip, HHS
   Feng, J
   Cao, XB
AF He, Qizhen
   Ip, Horace H. S.
   Feng, Jun
   Cao, Xianbin
TI Mr-SDM: a novel statistical deformable model for object deformation
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Computer Graphics International Conference 2009
CY MAY 26-29, 2009
CL Victoria, CANADA
DE Object deformation; Statistical deformable model; Finite element
ID SOFT-TISSUE DEFORMATIONS; SURGERY; PREDICTION
AB In this paper, we propose a novel statistical deformable model (SDM) for Material-related object deformation, which we called Mr-SDM. In Mr-SDM, by integrating the prior knowledge of the physical material property into the training of SDM, we are able to achieve both accuracy and computational efficiency in simulating material deformation for various applications. Our Mr-SDM training process takes advantage of the accuracy of Finite Element Method (FEM) to generate a set of deforming samples which enables us to estimate the deformation parameter of an unknown object based on its material knowledge. Our experiments have shown that Mr-SDM is able to give comparable accuracy with respect to FEM while, at the same time, reducing the computation cost from O(n (2)) for FEM-based simulation to O(n) using Mr-SDM.
C1 [He, Qizhen; Ip, Horace H. S.; Feng, Jun] City Univ Hong Kong, Dept Comp Sci, Image Comp Grp, Hong Kong, Hong Kong, Peoples R China.
   [He, Qizhen; Cao, Xianbin] Univ Sci & Technol China, Dept Comp Sci & Technol, Hefei 230026, Peoples R China.
C3 City University of Hong Kong; Chinese Academy of Sciences; University of
   Science & Technology of China, CAS
RP He, QZ (corresponding author), City Univ Hong Kong, Dept Comp Sci, Image Comp Grp, Hong Kong, Hong Kong, Peoples R China.
EM rainy@mail.ustc.edu.cn; cship@cityu.edu.hk; fengjun@nwu.edu.cn;
   xbcao@ustc.edu.cn
OI IP, Ho Shing Horace/0000-0002-1509-9002
CR [Anonymous], 2005, ACMEUROGRAPHICS S CO
   [Anonymous], 2004, P 2004 ACM SIGGRAPH, DOI DOI 10.1145/1028523.1028541
   Botsch M, 2005, COMPUT GRAPH FORUM, V24, P611, DOI 10.1111/j.1467-8659.2005.00886.x
   Bro-Nielsen M, 1998, P IEEE, V86, P490, DOI 10.1109/5.662874
   BroNielsen M, 1996, COMPUT GRAPH FORUM, V15, pC57
   Chen BQ, 2003, WORLD J GASTROENTERO, V9, P44, DOI 10.3748/wjg.v9.i1.44
   COOTES TF, 1995, COMPUT VIS IMAGE UND, V61, P38, DOI 10.1006/cviu.1995.1004
   Cotin S, 2000, VISUAL COMPUT, V16, P437, DOI 10.1007/PL00007215
   GARCIA M, 2007, P 4 WORKSH VIRT REAL, P27
   GELDER A, 1998, J GRAPHICS TOOLS, V3, P21
   He QZ, 2008, LECT NOTES COMPUT SC, V5128, P31
   HIROTA G, 2001, COMPUTER ANIMATIONS
   HUANG J, 2005, P 2005 ACM S SOL PHY, P221
   Keeve E, 1998, Comput Aided Surg, V3, P228
   Lasseter John., 1987, Proceedings of the 14th annual conference on Computer graphics and interactive techniques, P35
   MACIEL A, 2003, P INT S SURG SIM SOF, P74
   Meller S, 2005, LECT NOTES COMPUT SC, V3750, P443, DOI 10.1007/11566489_55
   Mollemans W, 2007, MED IMAGE ANAL, V11, P282, DOI 10.1016/j.media.2007.02.003
   Müller M, 2004, COMPUTER GRAPHICS INTERNATIONAL, PROCEEDINGS, P26, DOI 10.1109/CGI.2004.1309189
   Muller M., 2004, P 2004 ACM SIGGRAPHE, P141, DOI [DOI 10.1145/1028523.1028542, 10.1145/1028523.1028542, 10]
   Nayroles B., 1992, COMPUT MECH, V10, P307, DOI [DOI 10.1007/BF00364252, 10.1007/BF00364252]
   O'Brien JF, 2002, ACM T GRAPHIC, V21, P291, DOI 10.1145/566570.566579
   Pfister H, 2000, COMP GRAPH, P335, DOI 10.1145/344779.344936
   REEVES WT, 1983, ACM T GRAPHIC, V2, P91, DOI 10.1145/964967.801167
   Sadeghipour K, 1996, COMPUT IND, V28, P195, DOI 10.1016/0166-3615(95)00072-0
   Sederberg TN, 2003, ACM T GRAPHIC, V22, P477, DOI 10.1145/882262.882295
   Sifakis E, 2005, ACM T GRAPHIC, V24, P417, DOI 10.1145/1073204.1073208
   Sukumar N., 2003, Proceedings of the Sixth International ESAFORM Conference on Material Forming, P603
   WU X, 2002, COMPUT GRAPH FORUM, V20, P349
   Zhang Y, 2004, IEEE T VIS COMPUT GR, V10, P339, DOI 10.1109/TVCG.2004.1272733
NR 30
TC 1
Z9 1
U1 0
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2009
VL 25
IS 5-7
BP 609
EP 616
DI 10.1007/s00371-009-0333-5
PG 8
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 438ES
UT WOS:000265539300026
DA 2024-07-18
ER

PT J
AU Reif, R
   Günthner, WA
AF Reif, Rupert
   Guenthner, Willibald A.
TI Pick-by-vision: augmented reality supported order picking
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Computer Graphics International Conference 2009
CY MAY 26-29, 2009
CL Victoria, CANADA
DE Augmented reality; Logistics; Order picking; User studies
ID PERFORMANCE
AB Order picking is one of the most important process steps in logistics. Due to their flexibility, human beings cannot be replaced by machines. But if workers in order picking systems are equipped with a head-mounted display, Augmented Reality can improve the information visualization. In this paper the development of such a Pick-by-Vision system is presented. It is evaluated in a user study performed in a real storage environment. Important logistic figures as well as subjective figures were measured. The results show that Pick-by-Vision can improve order picking processes on a big scale.
C1 [Reif, Rupert; Guenthner, Willibald A.] Tech Univ Munich, Dept Mat Handling, D-85748 Garching, Germany.
C3 Technical University of Munich
RP Reif, R (corresponding author), Tech Univ Munich, Dept Mat Handling, Boltzmannstr 15, D-85748 Garching, Germany.
EM reif@fml.mw.tum.de
CR Azuma RT, 1997, PRESENCE-VIRTUAL AUG, V6, P355, DOI 10.1162/pres.1997.6.4.355
   Biocca F., 2006, Conference on Human Factors in Computing Systems. CHI2006, P1115
   BORTZ J, 1995, STAT HUMAN SOZIALWIS
   Bowman D., 2006, 3D user interfaces Theory and practice
   Brynzer H, 1995, INT J PROD ECON, V41, P115, DOI 10.1016/0925-5273(95)00083-6
   DANGELMAIER W, 2006, P 7 INT C PROD ENG L
   DOIL F, 2003, P 7 INT IMM PROJ TEC
   Echtler F, 2004, VIRTUAL AND AUGMENTED REALITY APPLICATIONS IN MANUFACTURING, P333
   Friedrich W., 2004, Arvika-augmented reality in Entwicklung, Produktion und Service
   Gudehus T., 2007, Logistics. A handbook: Principles, strategies, operations
   IBACH P, 2005, P IADIS INT C E COMM
   KLINKER G, 2006, P 16 INT C ART REAL
   Mizell D, 2001, FUNDAMENTALS OF WEARABLE COMPUTERS AND AUGMENTED REALITY, P447
   Reif R, 2008, VISUAL COMPUT, V24, P987, DOI 10.1007/s00371-008-0271-7
   Rolland JP, 2001, FUNDAMENTALS OF WEARABLE COMPUTERS AND AUGMENTED REALITY, P67
   SCHWERDTFEGER B, 2008, P 7 IEEE ACM INT S M
   Tang A, 2004, VIRTUAL AND AUGMENTED REALITY APPLICATIONS IN MANUFACTURING, P311
   TENHOMPEL M, 2004, WAREHOUSE MANAGEMENT
   WALCH D, 2007, P 10 IAESTED INT C A
   Welch G, 2001, PRESENCE-VIRTUAL AUG, V10, P1, DOI 10.1162/105474601750182289
   [No title captured]
NR 21
TC 47
Z9 55
U1 2
U2 39
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2009
VL 25
IS 5-7
BP 461
EP 467
DI 10.1007/s00371-009-0348-y
PG 7
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 438ES
UT WOS:000265539300010
DA 2024-07-18
ER

PT J
AU Chin, TJ
   You, YL
   Coutrix, C
   Lim, JH
   Chevallet, JP
   Nigay, L
AF Chin, Tat-Jun
   You, Yilun
   Coutrix, Celine
   Lim, Joo-Hwee
   Chevallet, Jean-Pierre
   Nigay, Laurence
TI Mobile phone-based mixed reality: the Snap2Play game
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 14th International Multimedia Modeling Conference (MMM 2008)
CY JAN 09-11, 2008
CL Kyoto Univ, Kyoto, JAPAN
HO Kyoto Univ
DE Mixed reality; Scene identification; Memory game; Mobile phone
AB The ubiquity of camera phones provides a convenient platform to develop immersive mixed-reality games. In this paper we introduce such a game which is loosely based on the popular card game "Memory", where players are asked to match a pair of identical cards among a set of overturned cards by revealing only two cards at a time. In our game, the players are asked to match a "digital card", which corresponds to a scene in a virtual world, to a "physical card", which is an image of a scene in the real world. The objective is to convey a mixed-reality sensation. Cards are matched with a scene identification engine which consists of multiple classifiers trained on previously collected images. We present our comprehensive overall game design, as well as implementation details and results. We also describe how we constructed our scene identification engine and its performance. Finally, we present an analysis of player surveys to gauge the potential market acceptance.
C1 [Chin, Tat-Jun; You, Yilun; Lim, Joo-Hwee; Chevallet, Jean-Pierre] Inst Infocomm Res, Image Percept Access & Language Lab, Singapore 119613, Singapore.
   [Coutrix, Celine; Nigay, Laurence] Univ Grenoble 1, Lab Informat Grenoble, F-38041 Grenoble 9, France.
C3 Agency for Science Technology & Research (A*STAR); A*STAR - Institute
   for Infocomm Research (I2R); Communaute Universite Grenoble Alpes;
   Institut National Polytechnique de Grenoble; Universite Grenoble Alpes
   (UGA); Centre National de la Recherche Scientifique (CNRS)
RP Chin, TJ (corresponding author), Inst Infocomm Res, Image Percept Access & Language Lab, 21 Heng Mui Keng Terrace, Singapore 119613, Singapore.
EM tatjun@gmail.com; ylyou@i2r.a-star.edu.sg; celine.coutrix@imag.fr;
   joohwee@i2r.a-star.edu.sg; viscjp@i2r.a-star.edu.sg;
   laurence.nigay@imag.fr
OI Coutrix, Celine/0000-0001-7904-4257
CR [Anonymous], 2006, P 9 EUR C COMP VIS 1
   [Anonymous], COMPUTER VISION PATT
   [Anonymous], COMPUTER VISION PATT
   BALLAGAS RA, 2007, CHI 07 HUM FACT COMP
   COUTRIX C, 2008, C ADV VIS INT
   COUTRIX C, 2006, C ADV VIS INT
   Hartley R., 2003, MULTIPLE VIEW GEOMET
   LIM JH, 2006, INT C PATT REC
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Opelt A, 2006, IEEE T PATTERN ANAL, V28, P416, DOI 10.1109/TPAMI.2006.54
   STRACHAN S, 2007, P ACM SIG COMP HUM I
   VERNIER F, 2000, INT WORKSH DES SPEC
NR 12
TC 2
Z9 2
U1 0
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2009
VL 25
IS 1
BP 25
EP 37
DI 10.1007/s00371-008-0283-3
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 379RC
UT WOS:000261412900004
DA 2024-07-18
ER

PT J
AU Jeong, DH
   Darvish, A
   Najarian, K
   Yang, J
   Ribarsky, W
AF Jeong, Dong Hyun
   Darvish, Alireza
   Najarian, Kayvan
   Yang, Jing
   Ribarsky, William
TI Interactive visual analysis of time-series microarray data
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Conference on SIBGRAPI 2007
CY SEP, 2007
CL Belo Horizonte, BRAZIL
DE Visual analysis; Information visualization; Microarray anaysis;
   Bioinformatics
ID GENE-EXPRESSION DATA; VISUALIZATION; TOOL
AB Estimating dynamic regulatory pathways using DNA microarray time-series can provide invaluable information about the dynamic interactions among genes and result in new methods of rational drug design. Even though several purely computational methods have been introduced for DNA pathway analysis, most of these techniques do not provide a fully interactive method to explore and analyze these dynamic interactions in detail, which is necessary to obtain a full understanding. In this paper, we present a unified modeling and visual approach focusing on visual analysis of gene regulatory pathways over time. As a preliminary step in analyzing the gene interactions, the method applies two different techniques, a clustering algorithm and an auto regressive (AR) model. This approach provides a successful prediction of the dynamic pathways involved in the biological process under study. At this level, these pure computational techniques lack the transparency required for analysis and understanding of the gene interactions. To overcome the limitations, we have designed a visual analysis method that applies several visualization techniques, including pixel-based gene representation, animation, and multi-dimensional scaling (MDS), in a new way. This visual analysis framework allows the user to quickly and thoroughly search for and find the dynamic interactions among genes, highlight interesting gene information, show the detailed annotations of the selected genes, compare regulatory behaviors for different genes, and support gene sequence analysis for the interesting genes. In order to enhance these analysis capabilities, several methods are enabled, providing a simple graph display, a pixel-based gene visualization technique, and a relation-displaying technique among gene expressions and gene regulatory pathways.
C1 [Jeong, Dong Hyun; Yang, Jing; Ribarsky, William] Univ N Carolina, Charlotte Visualizat Ctr, Charlotte, NC 28223 USA.
   [Darvish, Alireza] Univ N Carolina, Dept Comp Sci, Bioinformat & Adv Signal Proc Lab, Charlotte, NC 28223 USA.
   [Najarian, Kayvan] Virginia Commonwealth Univ, Sch Engn, Dept Comp Sci, Biomed Signal & Image Proc Lab, Richmond, VA 23284 USA.
C3 University of North Carolina; University of North Carolina Charlotte;
   University of North Carolina; University of North Carolina Charlotte;
   Virginia Commonwealth University
RP Jeong, DH (corresponding author), Univ N Carolina, Charlotte Visualizat Ctr, Charlotte, NC 28223 USA.
EM dhjeong@uncc.edu; adarvish@uncc.edu; knajarian@vcu.edu;
   jyang13@uncc.edu; wribarsky@uncc.edu
RI Jeong, Dong H/AAC-6736-2019; Najarian, Kayvan/B-2303-2010
OI Jeong, Dong Hyun/0000-0001-5271-293X
CR Agrafiotis DK, 2001, J COMPUT CHEM, V22, P488, DOI 10.1002/1096-987X(20010415)22:5<488::AID-JCC1020>3.0.CO;2-4
   ALTSCHUL SF, 1990, J MOL BIOL, V215, P403, DOI 10.1016/S0022-2836(05)80360-2
   [Anonymous], 1999, HUMAN FACTORS COMPUT
   [Anonymous], 2007, SACCHAROMYCES GENOME
   *AR GEN, PATHWAYSTUDIOTM
   Bederson B. B., 1994, UIST '94. Seventh Annual Symposium on User Interface Software and Technology. Proceedings of the ACM Symposium on User Interface Software and Technology, P17, DOI 10.1145/192426.192435
   Breinholt G, 1998, ACM T MATH SOFTWARE, V24, P184, DOI 10.1145/290200.290219
   Brown J., 2000, PAM 2000, P33
   Buja A., 1991, Proceedings Visualization '91 (Cat. No.91CH3046-0), P156, DOI 10.1109/VISUAL.1991.175794
   Cho RJ, 1998, MOL CELL, V2, P65, DOI 10.1016/S1097-2765(00)80114-8
   CHUAH MC, 1995, INFORM VISUALIZATION, P36
   Craig P, 2005, THIRD INTERNATIONAL CONFERENCE ON COORDINATED & MULTIPLE VIEWS IN EXPLORATORY VISUALIZATION, PROCEEDINGS, P3
   Craig P, 2002, SIXTH INTERNATIONAL CONFERENCE ON INFORMATION VISUALISATION, PROCEEDINGS, P427, DOI 10.1109/IV.2002.1028809
   Dahlquist KD, 2002, NAT GENET, V31, P19, DOI 10.1038/ng0502-19
   Darvish A, 2004, P ANN INT IEEE EMBS, V26, P2941
   DARVISH A, 2004, IEEE S COMP INT BIOI, P76
   de Hoon MJL, 2002, BIOINFORMATICS, V18, P1477, DOI 10.1093/bioinformatics/18.11.1477
   de Waele S, 2003, IEEE T SIGNAL PROCES, V51, P427, DOI 10.1109/TSP.2002.806905
   Eisen MB, 1998, P NATL ACAD SCI USA, V95, P14863, DOI 10.1073/pnas.95.25.14863
   Eisenstein M, 2006, NATURE, V442, P1067, DOI [10.1038/4421067a, 10.1038/4421067b]
   Friedman N, 2000, J COMPUT BIOL, V7, P601, DOI 10.1089/106652700750050961
   Furnas G. W., 1995, Human Factors in Computing Systems. CHI'95 Conference Proceedings, P234
   Hong J., 2005, EUROVIS, P191
   Keim DA, 2000, IEEE T VIS COMPUT GR, V6, P59, DOI 10.1109/2945.841121
   KEIM DA, 1993, IEEE C VIS 93 IEEE S, P158
   Liu Xueli, 2004, J Biopharm Stat, V14, P671, DOI 10.1081/BIP-200025657
   Moser RJ, 2004, J ANIM SCI, V82, P1261
   Nakahara H, 2003, BIOINFORMATICS, V19, P1124, DOI 10.1093/bioinformatics/btg098
   Perlin K., 1993, Computer Graphics Proceedings, P57, DOI 10.1145/166117.166125
   Perrin BE, 2003, BIOINFORMATICS, V19, pII138, DOI 10.1093/bioinformatics/btg1071
   PETERSON B, 2005, WHITE PAPER
   PIROLLI PL, 2001, HUMAN FACTORS COMPUT, P506
   Reichert J, 2000, NUCLEIC ACIDS RES, V28, P246, DOI 10.1093/nar/28.1.246
   ROBERTSON GG, 1993, COMMUN ACM, V36, P57, DOI 10.1145/255950.153577
   ROUCHKA EC, 1997, WUCS9739 DEP COMP SC
   Sales-Pardo M, 2005, PHYS REV E, V71, DOI 10.1103/PhysRevE.71.051902
   SARAIYA P, 2005, J INFORM VIS, V4, P191
   SCHULZEWOLLGAST P, 2005, INT C CENTR EUR COMP, P203
   *SIL GEN, GENESPRINGTM
   Silvescu A., 2001, Complex Systems, V13, P54
   Speed T., 2003, STAT ANAL GENE EXPRE
   Symeonidis A, 2004, LECT NOTES COMPUT SC, V3337, P468
   Tominski C, 2005, NINTH INTERNATIONAL CONFERENCE ON INFORMATION VISUALISATION, PROCEEDINGS, P175, DOI 10.1109/IV.2005.3
   Toyoda T, 2003, BIOINFORMATICS, V19, P433, DOI 10.1093/bioinformatics/btf874
   van Wezel M. C., 2004, Intelligent Data Analysis, V8, P601
   Wolfsberg TG, 1999, GENOME RES, V9, P775
   Wong PC, 2003, IEEE T VIS COMPUT GR, V9, P361, DOI 10.1109/TVCG.2003.1207444
   WRIGHT W, 1995, IEEE INF 95 S P, P19
   YEUNG LK, 2004, AUST COMPUT J, V29, P309
NR 49
TC 8
Z9 8
U1 0
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2008
VL 24
IS 12
BP 1053
EP 1066
DI 10.1007/s00371-007-0205-9
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 363VH
UT WOS:000260294500006
DA 2024-07-18
ER

PT J
AU Dos Passos, VA
   Walter, M
AF Dos Passos, V. A.
   Walter, M.
TI 3D mosaics with variable-sized tiles
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 26th International Conference on Computer Graphics
CY JUN 09-11, 2008
CL Istanbul, TURKEY
DE surface mosaics; 3D mosaics
AB Three dimensional mosaics, or surface mosaics, are a beautiful art form where a sculpture is made from putting together tiles on a given shape. Although 2D mosaics have attracted a lot of attention in graphics research, the same is not true for 3D mosaics. We present in this paper a technique to simulate 3D mosaics where the size of the individual pieces vary according to the local geometry. Previous work on this topic have addressed the problem of mosaics with tiles of the same size. The user specifies the number of tiles and our system covers a given 3D shape with square-shaped tiles with size adjusted according to the local curvature of the surface. The user can also controls the number of tiles, orientation on the surface, and relative size of tiles.
C1 [Dos Passos, V. A.; Walter, M.] UFPE, Ctr Informt, Pelotas, Brazil.
C3 Universidade Federal de Pernambuco
RP Dos Passos, VA (corresponding author), UFPE, Ctr Informt, Pelotas, Brazil.
EM vap2@cin.ufpc.br; marcclow@cin.ufpe.br
RI Walter, Marcelo/A-1964-2013; Walter, Marcelo/O-7526-2019
OI Walter, Marcelo/0000-0002-5634-8765
CR [Anonymous], 2006, P EUR IT CHAPT C 200
   Baumgart B., 1972, WINGED EDGE POLYHEDR
   Di Blasi G, 2005, VISUAL COMPUT, V21, P373, DOI 10.1007/s00371-005-0292-4
   Elber G, 2003, VISUAL COMPUT, V19, P67, DOI 10.1007/s00371-002-0175-x
   Haeberli P., 1990, P 17 ANN C COMP GRAP, P207, DOI [10.1145/97879.97902, DOI 10.1145/97879.97902]
   Hausner A, 2001, COMP GRAPH, P573, DOI 10.1145/383259.383327
   King Sonia., 2003, MOSAIC TECHNIQUES TR
   Lai YK, 2006, VISUAL COMPUT, V22, P604, DOI 10.1007/s00371-006-0047-x
   Schlechtweg S, 2005, COMPUT GRAPH FORUM, V24, P137, DOI 10.1111/j.1467-8659.2005.00838.x
   Surazhsky V, 2005, ACM T GRAPHIC, V24, P553, DOI 10.1145/1073204.1073228
   TURK G, 1992, COMP GRAPH, V26, P55, DOI 10.1145/142920.134008
   Turk G, 2001, COMP GRAPH, P347, DOI 10.1145/383259.383297
   Turk G., 1990, GRAPHICS GEMS GENERA, P24
   Walter M, 2001, COMP GRAPH, P317, DOI 10.1145/383259.383294
NR 14
TC 5
Z9 5
U1 0
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2008
VL 24
IS 7-9
BP 617
EP 623
DI 10.1007/s00371-008-0242-z
PG 7
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 322OI
UT WOS:000257384800017
DA 2024-07-18
ER

PT J
AU Liu, XG
   Dong, Z
   Bao, HJ
   Peng, QS
AF Liu, Xinguo
   Dong, Zhao
   Bao, Hujun
   Peng, Qunsheng
TI Caustic spot light for rendering caustics
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 26th International Conference on Computer Graphics
CY JUN 09-11, 2008
CL Istanbul, TURKEY
DE image synthesis; display algorithm; caustic; spot light
ID REAL-TIME CAUSTICS; SURFACES
AB It is difficult to render caustic patterns at interactive frame rates. This paper introduces new rendering techniques that relax current constraints, allowing scenes with moving, non-rigid scene objects, rigid caustic objects, and rotating directional light sources to be rendered in real-time with GPU hardware acceleration. Because our algorithm estimates the intensity and the direction of caustic light, rendering of non-Lambertian surfaces is supported. Previous caustics algorithms have separated the problem into pre-rendering and rendering phases, storing intermediate results in data structures such as photon maps or radiance transfer functions. Our central idea is to use specially parameterized spot lights, called caustic spot lights (CSLs), as the intermediate representation of a two-phase algorithm. CSLs are flexible enough that a small number can approximate the light leaving a caustic object, yet simple enough that they can be efficiently evaluated by a pixel shader program during accelerated rendering.We extend our approach to support changing lighting direction by further dividing the pre-rendering phase into per-scene and per-frame components: the per-frame phase computes frame-specific CSLs by interpolating between CSLs that were pre-computed with differing light directions.
C1 [Liu, Xinguo; Bao, Hujun; Peng, Qunsheng] Zhejiang Univ, State Key Lab CAD&CG, Hangzhou 310027, Peoples R China.
   [Dong, Zhao] Max Planck Inst Informat, Comp Graph Grp, D-66123 Saarbrucken, Germany.
C3 Zhejiang University; Max Planck Society
RP Liu, XG (corresponding author), Zhejiang Univ, State Key Lab CAD&CG, Hangzhou 310027, Peoples R China.
EM xgliu@cad.zju.edu.cn; dong@mpi-sb.mpg.de; bao@cad.zju.edu.cn;
   peng@cad.zju.edu.cn
RI dong, zhao/JHS-9392-2023
CR [Anonymous], 1992, NUMERICAL RECIPES C
   Brière N, 2000, PROC GRAPH INTERF, P127
   ERNST M, 2005, GI 05, P87
   GUENTEHR J, 2004, EUR S REND, P111
   Heckbert P. S., 1984, Computers & Graphics, V18, P119
   Iwasaki K, 2003, COMPUT GRAPH FORUM, V22, P601, DOI 10.1111/1467-8659.t01-2-00708
   Jensen HW, 1997, COMPUT GRAPH FORUM, V16, P57, DOI 10.1111/1467-8659.116
   Jensen HW., 2001, REALISTIC IMAGE SYNT, DOI [10.1201/9780429294907, DOI 10.1201/9780429294907]
   NISHITA T, 1994, SIGGRAPH 94, P373
   Purcell T., 2003, SIGGRAPHEUROGRAPHICS, P41
   Shah MA, 2007, IEEE T VIS COMPUT GR, V13, P272, DOI 10.1109/TVCG.2007.32
   Shirley P., 2000, REALISTIC RAY TRACIN
   Sloan Peter-Pike., 2002, Siggraph'02: Proceedings of the 29th annual conference on computer graphics and interactive techniques, P527
   STAM J, 1997, R046 ERCIM RES
   STAM J, 1996, ACM SIGGRAPH VIS P, P150
   Wand M, 2003, COMPUT GRAPH FORUM, V22, P611, DOI 10.1111/1467-8659.t01-2-00709
   Wyman C, 2004, 12TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P143, DOI 10.1109/PCCGA.2004.1348344
   WYMAN C, 2006, I3D, P153
   WYMAN C, 2005, ACM SIGGRAPH, P1050
   Yu X, 2007, PACIFIC GRAPHICS 2007: 15TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, P181, DOI 10.1109/PG.2007.35
   Zwicker M, 2002, IEEE T VIS COMPUT GR, V8, P223, DOI 10.1109/TVCG.2002.1021576
NR 21
TC 1
Z9 1
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2008
VL 24
IS 7-9
BP 485
EP 494
DI 10.1007/s00371-008-0229-9
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 322OI
UT WOS:000257384800004
OA hybrid
DA 2024-07-18
ER

PT J
AU Rong, GD
   Cao, Y
   Guo, XH
AF Rong, Guodong
   Cao, Yan
   Guo, Xiaohu
TI Spectral mesh deformation
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 26th International Conference on Computer Graphics
CY JUN 09-11, 2008
CL Istanbul, TURKEY
DE spectral geometry; manifold harmonics; mesh deformation; interactive
   manipulation
ID SURFACES
AB In this paper, we present a novel spectral method for mesh deformation based on manifold harmonics transform. The eigenfunctions of the Laplace-Beltrami operator give orthogonal bases for parameterizing the space of functions defined on the surfaces. The geometry and motion of the original irregular meshes can be compactly encoded using the low-frequency spectrum of the manifold harmonics. Using the spectral method, the size of the linear deformation system can be significantly reduced to achieve interactive computational speed for manipulating large triangle meshes. Our experimental results demonstrate that only a small spectrum is needed to achieve undistinguishable deformations for large triangle meshes. The spectral mesh deformation approach shows great performance improvement on computational speed over its spatial counterparts.
C1 [Rong, Guodong; Guo, Xiaohu] Univ Texas Dallas, Dept Comp Sci, Richardson, TX 75083 USA.
   [Cao, Yan] Univ Texas Dallas, Dept Math Sci, Richardson, TX 75083 USA.
C3 University of Texas System; University of Texas Dallas; University of
   Texas System; University of Texas Dallas
RP Rong, GD (corresponding author), Univ Texas Dallas, Dept Comp Sci, Richardson, TX 75083 USA.
EM guodongrong@utdallas.edu; yan.cao@utdallas.edu; xguo@utdallas.edu
RI Rong, Guodong/A-7851-2009
CR Au OKC, 2006, IEEE T VIS COMPUT GR, V12, P386, DOI 10.1109/TVCG.2006.47
   Boier-Martin I, 2004, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, P119, DOI 10.1109/SMI.2004.1314499
   Botsch M, 2004, ACM T GRAPHIC, V23, P630, DOI 10.1145/1015706.1015772
   Botsch M., 2006, SGP 06, P11
   Botsch M., 2006, P VISION MODELING VI, P357
   Botsch M, 2008, IEEE T VIS COMPUT GR, V14, P213, DOI 10.1109/TVCG.2007.1054
   Flannery B. P., 1992, NUMERICAL RECIPES C, DOI DOI 10.2277/052143064X
   Guo XH, 2006, IEEE T VIS COMPUT GR, V12, P375, DOI 10.1109/TVCG.2006.52
   Guskov I, 1999, COMP GRAPH, P325, DOI 10.1145/311535.311577
   Huang J, 2006, ACM T GRAPHIC, V25, P1126, DOI 10.1145/1141911.1142003
   Karni Z, 2000, COMP GRAPH, P279, DOI 10.1145/344779.344924
   KOBBELT L, 1998, P SIGGRAPH 98, P105, DOI DOI 10.1145/280814.280831
   Lipman Y, 2005, ACM T GRAPHIC, V24, P479, DOI 10.1145/1073204.1073217
   Marinov M., 2007, Journal of Graphics Tools, V12, P27
   Meyer M., 2002, VISUALIZATION MATH, V6, P35, DOI DOI 10.1007/978-3-662-05105-4_2
   Reuter M, 2006, COMPUT AIDED DESIGN, V38, P342, DOI 10.1016/j.cad.2005.10.011
   Seidel H.-P, 2004, 2 EUROGRAPHICS S GEO, P175, DOI DOI 10.1145/1057432.1057456
   Sumner RW, 2004, ACM T GRAPHIC, V23, P399, DOI 10.1145/1015706.1015736
   Taubin G., 1995, P 22 ANN C COMP GRAP, P351, DOI DOI 10.1145/218380.218473
   Terzopoulos D., 1987, COMPUT GRAPH, P205, DOI DOI 10.1145/37402.37427
   Zayer R, 2005, COMPUT GRAPH FORUM, V24, P601, DOI 10.1111/j.1467-8659.2005.00885.x
   ZHANG H., 2007, P EUROGRAPHICS STATE, P1, DOI DOI 10.1109/IPDPS.2007.370248
   Zhou K, 2005, ACM T GRAPHIC, V24, P496, DOI 10.1145/1073204.1073219
   Zhou K, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239542
   ZORIN D, 1997, P SIGGRAPH 97, P259
NR 25
TC 37
Z9 46
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2008
VL 24
IS 7-9
BP 787
EP 796
DI 10.1007/s00371-008-0260-x
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 322OI
UT WOS:000257384800034
DA 2024-07-18
ER

PT J
AU Xu, ZL
   Ma, LZ
   Kimachi, M
   Suwa, M
AF Xu, Zhiliang
   Ma, Lizhuang
   Kimachi, Masatoshi
   Suwa, Masaki
TI Efficient contrast invariant stereo correspondence using dynamic
   programming with vertical constraint
SO VISUAL COMPUTER
LA English
DT Article
DE stereo correspondence; dynamic programming; vertical constraint;
   computer vision
AB In this paper, we propose a dense stereo algorithm based on the census transform and improved dynamic programming (DP). Traditional scanline-based DP algorithms are the most efficient ones among global algorithms, but are well-known to be affected by the streak effect. To solve this problem, we improve the traditional three-state DP algorithm by taking advantage of an extended version of sequential vertical consistency constraint. Using this method, we increase the accuracy of the disparity map greatly. Optimizations have been made so that the computational cost is only increased by about 20%, and the additional memory needed for the improvement is negligible. Experimental results show that our algorithm outperforms many state-of-the-art algorithms with similar efficiency on Middlebury College's stereo Web site. Besides, the algorithm is robust enough for image pairs with utterly different contrasts by using of census transform as the basic match metric.
C1 Shanghai Jiao Tong Univ, Dept Comp Sci & Engn, Shanghai 200030, Peoples R China.
   HQ Corp, Corp R&D, Sensing & Control Technol Lab, Kyoto, Japan.
C3 Shanghai Jiao Tong University
RP Xu, ZL (corresponding author), Shanghai Jiao Tong Univ, Dept Comp Sci & Engn, Shanghai 200030, Peoples R China.
EM xuzhiliang@sjtu.edu.cn
CR [Anonymous], IEEE COMP SOC C COMP
   [Anonymous], P ICCV
   Baker H.H., 1981, Proceedings of the 7th international joint conference on Artificial intelligence - Volume 2, IJCAI'81, V2, P631
   BARNARD ST, 1989, INT J COMPUT VISION, V3, P17, DOI 10.1007/BF00054836
   Bobick AF, 1999, INT J COMPUT VISION, V33, P181, DOI 10.1023/A:1008150329890
   Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114
   Brown MZ, 2003, IEEE T PATTERN ANAL, V25, P993, DOI 10.1109/TPAMI.2003.1217603
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851
   Cox IJ, 1996, COMPUT VIS IMAGE UND, V63, P542, DOI 10.1006/cviu.1996.0040
   GEMAN S, 1984, IEEE T PATTERN ANAL, V6, P721, DOI 10.1109/TPAMI.1984.4767596
   KANADE T, 1994, IEEE T PATTERN ANAL, V16, P920, DOI 10.1109/34.310690
   Mordohai P, 2006, IEEE T PATTERN ANAL, V28, P968, DOI 10.1109/TPAMI.2006.129
   Ogale AS, 2005, IEEE INT CONF ROBOT, P819
   Scharstein D, 2002, INT J COMPUT VISION, V47, P7, DOI 10.1023/A:1014573219977
   Sun J, 2003, IEEE T PATTERN ANAL, V25, P787, DOI 10.1109/TPAMI.2003.1206509
   SZELISKI R, 1990, INT J COMPUT VISION, V5, P271, DOI 10.1007/BF00126502
   Yang Q., 2006, P 17 BRIT MACHINE VI, P989
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
NR 23
TC 5
Z9 6
U1 0
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2008
VL 24
IS 1
BP 45
EP 55
DI 10.1007/s00371-007-0177-9
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 234GU
UT WOS:000251149500004
DA 2024-07-18
ER

PT J
AU Lee, TY
   Wang, YS
   Chen, TG
AF Lee, Tong-Yee
   Wang, Yu-Shuen
   Chen, Tai-Guang
TI Segmenting a deforming mesh into near-rigid components
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 14th Pacific Conference on Computer Graphics and Applications
CY OCT 11-13, 2005
CL Taipei, TAIWAN
DE near-rigid sub-meshes; mesh segmentation; deformation transfer
ID SURFACE DECOMPOSITION; METAMORPHOSIS
AB Given a deforming mesh in an animation, we propose a new method to segment this mesh into several near-rigid sub-meshes. From this deforming mesh over all frames of an animation, we can analyze the degree of deformation between two nearby faces on the mesh. Then, our algorithm partitions the given deforming mesh into near-rigid components where the segmentation boundaries always pass at regions of large deformation. As a result, the mesh segmentation is invariant to all frames of the given animation and the motion of faces in each near-rigid-component can be represented by the same approximate affine transformation. To demonstrate the usefulness of the algorithm, we solve the restriction of deformation transfer for triangle meshes [30] which requires similar reference poses between source mesh and target mesh.
RP Lee, TY (corresponding author), 1 Ta Hsueh Rd, Tainan, Taiwan.
EM tonylee@mail.ncku.edu.tw; braveheart@csie.ncku.edu.tw;
   hf@csie.ncku.edu.tw
OI Wang, Yu-Shuen/0000-0003-2550-2990
CR Attene M, 2006, VISUAL COMPUT, V22, P181, DOI 10.1007/s00371-006-0375-x
   ATTENNE M, 2006, SMI 06
   BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791
   BISCHOFF S, 2005, SNAKES TRIANGLE MESH, P208
   CHUNG FRK, 1997, CBMS REGION C SER MA
   Cormen Thomas H., 2001, INTRO ALGORITHMS
   Funkhouser T, 2004, ACM T GRAPHIC, V23, P652, DOI 10.1145/1015706.1015775
   GARLAND M, 2001, SI3D 01, P49, DOI DOI 10.1145/364338.364345
   Gregory A, 1999, VISUAL COMPUT, V15, P453, DOI 10.1007/s003710050192
   James DL, 2005, ACM T GRAPHIC, V24, P399, DOI 10.1145/1073204.1073206
   Julius D, 2005, COMPUT GRAPH FORUM, V24, P581, DOI 10.1111/j.1467-8659.2005.00883.x
   Kalvin AD, 1996, IEEE COMPUT GRAPH, V16, P64, DOI 10.1109/38.491187
   KARAI Z, 2000, SIGGRAPH 00, P279, DOI DOI 10.1145/344779.344924
   Katz S, 2003, ACM T GRAPHIC, V22, P954, DOI 10.1145/882262.882369
   Lee TY, 2005, COMPUT ANIMAT VIRT W, V16, P519, DOI 10.1002/cav.79
   Lee TY, 2005, IEICE T INF SYST, VE88D, P646, DOI 10.1093/ietisy/e88-d.3.646
   Lee TY, 2003, IEEE T VIS COMPUT GR, V9, P85, DOI 10.1109/TVCG.2003.1175099
   Lévy B, 2002, ACM T GRAPHIC, V21, P362, DOI 10.1145/566570.566590
   Liefman S.K. G., 2005, VISUAL COMPUT, V21, P865
   Lin CH, 2005, IEEE T VIS COMPUT GR, V11, P2
   LLOYD SP, 1982, IEEE T INFORM THEORY, V28, P129, DOI 10.1109/TIT.1982.1056489
   Moakher M, 2002, SIAM J MATRIX ANAL A, V24, P1, DOI 10.1137/S0895479801383877
   MORTARA M, 2004, 9 ACM S SOL MOD APPL, P339
   POPA T, 2006, SMI 2005
   Sander P. V., 2003, Symposium on Geometry Processing, P146
   Sattler Mirko, 2005, P ACM SIGGRAPH EUR S, P209
   SHAMIR A, 2004, 3DPVT 04, P82, DOI DOI 10.1109/3DPVT.2004.13
   Shlafman S, 2002, COMPUT GRAPH FORUM, V21, P219, DOI 10.1111/1467-8659.00581
   Sumner RW, 2004, ACM T GRAPHIC, V23, P399, DOI 10.1145/1015706.1015736
   Zayer R, 2005, COMPUT GRAPH FORUM, V24, P601, DOI 10.1111/j.1467-8659.2005.00885.x
   Zöckler M, 2000, VISUAL COMPUT, V16, P241, DOI 10.1007/PL00013396
   Zuckerberger E, 2002, COMPUT GRAPH-UK, V26, P733, DOI 10.1016/S0097-8493(02)00128-0
NR 32
TC 42
Z9 45
U1 0
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2006
VL 22
IS 9-11
BP 729
EP 739
DI 10.1007/s00371-006-0059-6
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 082IW
UT WOS:000240381000016
DA 2024-07-18
ER

PT J
AU Shen, JB
   Jin, XG
   Mao, XY
   Feng, JQ
AF Shen, Jianbing
   Jin, Xiaogang
   Mao, Xiaoyang
   Feng, Jieqing
TI Completion-based texture design using deformation
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 14th Pacific Conference on Computer Graphics and Applications
CY OCT 11-13, 2005
CL Taipei, TAIWAN
DE texture design; layer completion; cyclic texture; deformation
ID IMAGE; SIMILARITY
AB In this paper, we present a novel approach for designing a variety of large textures from a single small sample texture. Firstly, the original small texture is segmented into layers, each of which contains one particular texture element. Secondly, each layer is deformed using a set of chaotic-based transformation operations. Thirdly, all the deformed layers are added together to form a new texture, which is a natural variation of the original sample texture. Since each layer is deformed independently, adding the deformed layers together usually results in a texture with overlapping regions and holes. We employ the graphcut algorithm and an example-based image inpainting technique to seamlessly patch the overlapping regions and to fill the holes. Moreover, an optimized graphcut synthesis algorithm and a new cyclic texture synthesis technique are also developed for efficiently creating large seamless textures. As a result, our approach shows particular strength in generating a large variety of textures from a single sample texture while avoiding highly repetitive patterns. Our experiments demonstrate that the proposed technique can also be used for other texture synthesis applications, such as texture synthesis from multiple samples.
C1 Zhejiang Univ, State Key Lab CAD & CG, Hangzhou 310027, Peoples R China.
   Univ Yamanashi, Tamaho, Yamanashi, Japan.
C3 Zhejiang University; University of Yamanashi
RP Jin, XG (corresponding author), Zhejiang Univ, State Key Lab CAD & CG, Hangzhou 310027, Peoples R China.
EM shenjianbing@cad.zju.edu.cn; jin@cad.zju.edu.cn; mao@yamanashi.ac.jp;
   jqfeng@cad.zju.edu.cn
RI Shen, Jianbing/U-8796-2019
OI Shen, Jianbing/0000-0002-4109-8353; mao, xiaoyang/0000-0001-9531-3197
CR Agarwala A, 2004, ACM T GRAPHIC, V23, P294, DOI 10.1145/1015706.1015718
   Agrawal A, 2005, ACM T GRAPHIC, V24, P828, DOI 10.1145/1073204.1073269
   Bar-Joseph Z, 2001, IEEE T VIS COMPUT GR, V7, P120, DOI 10.1109/2945.928165
   Bertalmio M, 2000, COMP GRAPH, P417, DOI 10.1145/344779.344972
   Boykov Y, 2004, IEEE T PATTERN ANAL, V26, P1124, DOI 10.1109/TPAMI.2004.60
   Brooks S, 2002, ACM T GRAPHIC, V21, P653, DOI 10.1145/566570.566632
   Chuang YY, 2005, ACM T GRAPHIC, V24, P853, DOI 10.1145/1073204.1073273
   Cohen MF, 2003, ACM T GRAPHIC, V22, P287, DOI 10.1145/882262.882265
   Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236
   Criminisi A, 2004, IEEE T IMAGE PROCESS, V13, P1200, DOI 10.1109/TIP.2004.833105
   Dischler JM, 2002, COMPUT GRAPH FORUM, V21, P401, DOI 10.1111/1467-8659.t01-1-00600
   Dorsey J, 1999, COMP GRAPH, P225, DOI 10.1145/311535.311560
   Drori I, 2003, ACM T GRAPHIC, V22, P303, DOI 10.1145/882262.882267
   Efros AA, 2001, COMP GRAPH, P341, DOI 10.1145/383259.383296
   Fattal R, 2002, ACM T GRAPHIC, V21, P249
   Felzenszwalb PF, 2004, INT J COMPUT VISION, V59, P167, DOI 10.1023/B:VISI.0000022288.19776.77
   Harrison P, 2001, W S C G ' 2001, VOLS I & II, CONFERENCE PROCEEDINGS, P190
   Heaps C, 1999, J EXP PSYCHOL HUMAN, V25, P299, DOI 10.1037/0096-1523.25.2.299
   Hertzmann A, 2001, COMP GRAPH, P327, DOI 10.1145/383259.383295
   Jakimoski G, 2001, IEEE T CIRCUITS-I, V48, P163, DOI 10.1109/81.904880
   Jia JY, 2003, PROC CVPR IEEE, P643
   Kwatra V, 2003, ACM T GRAPHIC, V22, P277, DOI 10.1145/882262.882264
   Liu YX, 2004, ACM T GRAPHIC, V23, P368, DOI 10.1145/1015706.1015731
   Masuda N, 2002, IEEE T CIRCUITS-I, V49, P28, DOI 10.1109/81.974872
   Matusik W, 2005, ACM T GRAPHIC, V24, P787, DOI 10.1145/1073204.1073262
   Pérez P, 2003, ACM T GRAPHIC, V22, P313, DOI 10.1145/882262.882269
   Press W. H., 2002, Numerical Recipes in C: the Art of Scientific Computing, V2nd ed., DOI DOI 10.1119/1.14981
   Schödl A, 2000, COMP GRAPH, P489, DOI 10.1145/344779.345012
   Sedgewick Robert., 2002, ALGORITHMS C PART 5, V3rd
   SHEN J, 2006, IN PRESS COMPUT GRAP, V30
   Sun J, 2005, ACM T GRAPHIC, V24, P861, DOI 10.1145/1073204.1073274
   Walter M, 2001, COMP GRAPH, P317, DOI 10.1145/383259.383294
   Wexler Y, 2004, PROC CVPR IEEE, P120
   WILCZKOWIAK M, 2005, P BRIT MACH VIS C, P492
   Wu Q, 2004, ACM T GRAPHIC, V23, P364, DOI 10.1145/1015706.1015730
   Zhang W, 2002, INT J NONLINEAR SCI, V3, P295, DOI 10.1515/IJNSNS.2002.3.3-4.295
   ZHANG Y, 2004, EUROGRAPHICS
NR 37
TC 11
Z9 15
U1 0
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2006
VL 22
IS 9-11
BP 936
EP 945
DI 10.1007/s00371-006-0079-2
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 082IW
UT WOS:000240381000036
DA 2024-07-18
ER

PT J
AU Kunii, TL
AF Kunii, TL
TI The genesis of "The Visual Computer"
SO VISUAL COMPUTER
LA English
DT Article
DE The Visual Computer; computer graphics; computer vision; visual data
   structures
AB A brief history and the prospects of "The Visual Computer"and "The Visual Computer: An International Journal" are presented solely to foster future research on the visual computer. It is still in its infancy, and the author's view is based on his own limited experiences, and hence is prone to mistakes.
C1 Kanazawa Inst Technol, IT Inst, Shibuya Ku, Tokyo 1500001, Japan.
RP Kanazawa Inst Technol, IT Inst, Shibuya Ku, 1-15-13 Jingumae, Tokyo 1500001, Japan.
EM tosi@kunii.info
CR Kunii T. L., 1975, Computers & Graphics, V1, P297, DOI 10.1016/0097-8493(75)90043-6
   Kunii T. L., 1990, Visual Computer, V6, P326, DOI 10.1007/BF01901019
   KUNII TL, 1999, INT J SHAPE MODE DEC, P123
   KUNII TL, 1974, P 2 INT JOINT C PATT, P310
   1975, P C COMP GRAPH PATT
NR 5
TC 0
Z9 0
U1 4
U2 25
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2005
VL 21
IS 12
BP 958
EP 960
DI 10.1007/s00371-005-0359-2
PG 3
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 996AB
UT WOS:000234145700002
DA 2024-07-18
ER

PT J
AU Lai, SH
   Cheng, FH
AF Lai, SH
   Cheng, FH
TI Texture mapping on surfaces of arbitrary topology using norm
   preserving-based optimization
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 13th Pacific Conference on Computer Graphics and Applications
CY OCT 12-14, 2005
CL Macao, PEOPLES R CHINA
DE texture mapping; optimization; realistic rendering
ID SUBJECT
AB A simple and yet highly efficient, high-quality texture mapping method for surfaces of arbitrary topology is presented. The new method projects the given surface from the 3D object space into the 2D texture space to identify the 2D texture structure that will be used to texture the surface. The object space to texture space projection is optimized to ensure minimum distortion of the texture mapping process. The optimization is achieved through a commonly used norm preserving minimization process on edges of the surface. The main difference here is, by using an initial value approach, the optimization problem can be set up as a quadratic programming problem and, consequently, solved by a linear least squares method. Three methods to choose a good initial value are presented. Test cases show that the new method works well on surfaces of arbitrary topology, with the exception of surfaces with exceptionally abnormal curvature distribution. Other advantages of the new method include uniformity and seamlessness of the texture mapping process. The new method is suitable for applications that do not require precise texture mapping results but demand highly efficient mapping process such as computer animation or video games.
C1 Univ Kentucky, Dept Comp Sci, Graph & Geometr Modeling Lab, Lexington, KY 40506 USA.
C3 University of Kentucky
RP Lai, SH (corresponding author), Univ Kentucky, Dept Comp Sci, Graph & Geometr Modeling Lab, Lexington, KY 40506 USA.
EM slai2@cs.uky.edu; cheng@cs.uky.edu
CR Arad N, 1997, COMPUT GRAPH FORUM, V16, P247, DOI 10.1111/1467-8659.00192
   BENNIS C, 1991, COMP GRAPH, V25, P237, DOI 10.1145/127719.122744
   BIER EA, 1986, IEEE COMPUT GRAPH, V6, P40, DOI 10.1109/MCG.1986.276545
   BLINN JF, 1976, COMMUN ACM, V19, P542, DOI 10.1145/965143.563322
   Catmull Edwin Earl, 1974, A Subdivision Algorithm for Computer Display of Curved Surfaces
   Coleman TF, 1996, SIAM J OPTIMIZ, V6, P418, DOI 10.1137/0806023
   Coleman TF, 1996, SIAM J OPTIMIZ, V6, P1040, DOI 10.1137/S1052623494240456
   Crow F. C., 1984, Computers & Graphics, V18, P207
   Do Carmo M., 1976, Differential Geometry of Curves and Surfaces
   GU X, 2003, P ACM S GEOM PROC AA
   Hanrahan P., 1990, Computer Graphics, V24, P215, DOI 10.1145/97880.97903
   HECKBERT PS, 1986, IEEE COMPUT GRAPH, V6, P56, DOI 10.1109/MCG.1986.276672
   MAGDA S, 2003, P EUR S REND, P82
   MAILLOT J, 1993, P SIGGRAPH 93, P27, DOI DOI 10.1145/166117.166120
   Song de Ma, 1988, EUROGRAPHICS '88. Proceedings of the European Computer Graphics Conference and Exhibition, P421
   SORMANN M, 2003, P 19 SPRING C COMP G, P131
   Tang Y, 2003, COMPUT GRAPH-UK, V27, P415, DOI 10.1016/S0097-8493(03)00036-0
   Wei LY, 2001, COMP GRAPH, P355, DOI 10.1145/383259.383298
   ZHANG J, 2003, P SIGGRAPH, P295
   [No title captured]
NR 20
TC 1
Z9 1
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2005
VL 21
IS 8-10
SI SI
BP 783
EP 790
DI 10.1007/s00371-005-0327-x
PG 8
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 964CX
UT WOS:000231857400031
DA 2024-07-18
ER

PT J
AU McDonnell, KT
   Chang, YS
   Qin, H
AF McDonnell, KT
   Chang, YS
   Qin, H
TI Interpolatory, solid subdivision of unstructured hexahedral meshes
SO VISUAL COMPUTER
LA English
DT Article
DE subdivision algorithms; geometric and topological representations; solid
   modeling; multiresolution models; volumetric meshes
ID SCHEME
AB This paper presents a new, volumetric subdivision scheme for interpolation of arbitrary hexahedral meshes. To date, nearly every existing volumetric subdivision scheme is approximating, i.e., with each application of the subdivision algorithm, the geometry shrinks away from its control mesh. Often, an approximating algorithm is undesirable and inappropriate, producing unsatisfactory results for certain applications in solid modeling and engineering design (e.g., finite element meshing). We address this lack of smooth, interpolatory subdivision algorithms by devising a new scheme founded upon the concept of tri-cubic Lagrange interpolating polynomials. We show that our algorithm is a natural generalization of the butterfly subdivision surface scheme to a tri-variate, volumetric setting.
C1 SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11794 USA.
C3 State University of New York (SUNY) System; State University of New York
   (SUNY) Stony Brook
RP SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11794 USA.
EM ktm@cs.sunysb.edu; yusung@cs.sunysb.edu; qin@cs.sunysb.edu
FU Direct For Computer & Info Scie & Enginr [0830183] Funding Source:
   National Science Foundation; Div Of Information & Intelligent Systems
   [0830183] Funding Source: National Science Foundation
CR [Anonymous], 2001, P 2001 S INT 3D GRAP
   Bajaj C, 2002, VISUAL COMPUT, V18, P343, DOI 10.1007/s003710100150
   Bertram M., 2002, Proceedings of the seventh ACM symposium on Solid modeling and applications, SMA '02, P72
   CATMULL E, 1978, COMPUT AIDED DESIGN, V10, P350, DOI 10.1016/0010-4485(78)90110-0
   Chang YS, 2003, SMI 2003: SHAPE MODELING INTERNATIONAL 2003, PROCEEDINGS, P143
   Chang Yu, 2002, P 7 ACM S SOL MOD AP, P226
   DOO D, 1978, P INT TECHN COMP AID, P157
   Dyn N., 1990, Computer-Aided Geometric Design, V7, P129, DOI 10.1016/0167-8396(90)90025-M
   DYN N, 1990, ACM T GRAPHIC, V9, P160, DOI 10.1145/78956.78958
   Dyn N., 1987, Computer-Aided Geometric Design, V4, P257, DOI 10.1016/0167-8396(87)90001-X
   DYN N, 1993, P WORKSH COMP GEOM, P97
   Linsen L, 2002, 10TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P346, DOI 10.1109/PCCGA.2002.1167878
   MacCracken R., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P181, DOI 10.1145/237170.237247
   MCDONNELL KT, 2001, P 6 ACM S SOL MOD AP, P312
   MUUSS MJ, 1991, STATE OF THE ART IN COMPUTER GRAPHICS, P185
   Pascucci V., 2000, PROC VOLUME VISUALIZ, P33
   REIF U, 1995, COMPUT AIDED GEOM D, V12, P153, DOI 10.1016/0167-8396(94)00007-F
   Warren J., 2001, SUBDIVISION METHODS
   WEILER K, 1986, THESIS RENSSELAER PO
   Zorin D, 2000, CONSTR APPROX, V16, P359, DOI 10.1007/s003659910016
NR 20
TC 8
Z9 9
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2004
VL 20
IS 6
BP 418
EP 436
DI 10.1007/s00371-004-0246-2
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 846DU
UT WOS:000223296100006
DA 2024-07-18
ER

PT J
AU Kohout, J
   Kolingerová, I
AF Kohout, J
   Kolingerová, I
TI Parallel Delaunay triangulation in E<SUP>3</SUP>:: make it simple
SO VISUAL COMPUTER
LA English
DT Article
DE Delaunay triangulation; paralellization; incremental insertion;
   tetrahedronization
ID ALGORITHM
AB The randomized incremental insertion algorithm of Delaunay triangulation in E-3 is very popular due to its simplicity and stability. This paper describes a new parallel algorithm based on this approach. The goals of the proposed parallel solution are not only to make it efficient but also to make it simple. The algorithm is intended for computer architectures with several processors and shared memory. Several versions of the proposed method were tested on workstations with up to eight processors and on datasets of up to 200000 points with favorable results.
C1 Univ W Bohemia, Dept Comp Sci & Engn, Ctr Comp Graph & Data Visualizat, Plzen 30614, Czech Republic.
C3 University of West Bohemia Pilsen
RP Univ W Bohemia, Dept Comp Sci & Engn, Ctr Comp Graph & Data Visualizat, Univerzitni 8, Plzen 30614, Czech Republic.
EM kolinger@kiv.zcu.cz
RI Kohout, Josef/C-7495-2012
OI Kohout, Josef/0000-0002-3231-2573; Kolingerova,
   Ivana/0000-0003-4556-2771
CR AURENHAMMER F, 1991, COMPUT SURV, V23, P345, DOI 10.1145/116873.116880
   CHRISOCHOIDES N, 1996, P 5 INT C NUM GRID
   Chrisochoides N., 1999, Proceedings ofthe Eighth International Meshing Roundtable, P55
   Cignoni P, 1998, COMPUT AIDED DESIGN, V30, P333, DOI 10.1016/S0010-4485(97)00082-1
   Cignoni P., 1993, Computer Graphics Forum, V12, pC129, DOI 10.1111/1467-8659.1230129
   de Berg M., 2000, COMPUTATIONAL GEOMET
   EDELSBRUNNER H, 2001, P 17 ACM S COMP GEOM, P115
   Erickson J., 2001, P 17 ANN ACM S COMP, P96, DOI [10.1145/378583.378636, DOI 10.1145/378583.378636]
   FACELLO MA, 1995, COMPUT AIDED GEOM D, V12, P349, DOI 10.1016/0167-8396(94)00018-N
   FANG TP, 1995, IEEE COMPUT GRAPH, V15, P62
   Golias NA, 1997, FINITE ELEM ANAL DES, V25, P331, DOI 10.1016/S0168-874X(96)00054-6
   GUIBAS LJ, 1992, ALGORITHMICA, V7, P381, DOI 10.1007/BF01758770
   HARDWICK JC, 1997, P 9 ANN S PAR ALG AR, P22
   Joe B., 1991, Computer-Aided Geometric Design, V8, P123, DOI 10.1016/0167-8396(91)90038-D
   JOE B, 1991, INT J NUMER METH ENG, V31, P987, DOI 10.1002/nme.1620310511
   Kolingerová I, 2002, VISUAL COMPUT, V18, P511, DOI 10.1007/s00371-002-0173-z
   KOLINGEROVA I, 2000, P GRAPH 2000 MOSC, P76
   MAUR P, 2002, DELAUNAY TRINGULATIO
   RAJAN VT, 1994, DISCRETE COMPUT GEOM, V12, P189, DOI 10.1007/BF02574375
   Shewchuk J. R., 1996, Proceedings of the Twelfth Annual Symposium on Computational Geometry, FCRC '96, P141, DOI 10.1145/237218.237337
   SHEWCHUK JR, 2000, STABBING DELAUNAY TE
   Siu-Wing Cheng, 1999, Proceedings of the Fifteenth Annual Symposium on Computational Geometry, P1, DOI 10.1145/304893.304894
   Teng Y. A., 1993, Proceedings SUPERCOMPUTING '93, P112, DOI 10.1145/169627.169667
   WATSON DF, 1981, COMPUT J, V24, P167, DOI 10.1093/comjnl/24.2.167
   1999, BORLAND DELPHI V 5 0
NR 25
TC 8
Z9 9
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2003
VL 19
IS 7-8
BP 532
EP 548
DI 10.1007/s00371-003-0219-x
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 749YL
UT WOS:000186957600010
DA 2024-07-18
ER

PT J
AU Kähler, KA
   Haber, J
   Seidel, HP
AF Kähler, KA
   Haber, J
   Seidel, HP
TI Dynamically refining animated triangle meshes for rendering
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 19th Computer Graphics International Conference
CY JUL 03-06, 2001
CL CITY UNIV HONG KONG, KOWLOON, PEOPLES R CHINA
SP Comp Graph Soc, IEEE Hong Kong Comp Chapter, KC Wong Educ Fda, Hong Kong Pei Hua Educ Fdn
HO CITY UNIV HONG KONG
DE triangle meshes; surface deformation; adaptive refinement; real-time
   rendering
AB We present a method to dynamically apply local refinements to an irregular triangle mesh as it deforms in real time. The method increases surface smoothness in regions of high deformation by splitting triangles in a fashion similar to one or two steps of Loop subdivision. The refinement is computed for an arbitrary triangle mesh, and the subdivided triangles are simply passed to the rendering engine, leaving the mesh itself unchanged. The algorithm can thus be easily plugged into existing systems to enhance the visual appearance of animated meshes. The refinement step has very low computational overhead and is easy to implement. We demonstrate the use of the algorithm in a physics-based facial animation system.
C1 Max Planck Inst Informat, D-66123 Saarbrucken, Germany.
C3 Max Planck Society
EM kaehler@mpi-sb.mpg.de; haberj@mpi-sb.mpg.de; hpseidel@mpi-sb.mpg.de
CR [Anonymous], 1996, THESIS U WASHINGTON
   Campbell T, 1998, EXOT PET PRACT, V3, P1
   De Boor C, 1978, A Pratical Guide to Splines, V27
   DeRose T., 1998, P 25 ANN C COMP GRAP
   Farin G., 1993, Curves and Surfaces for Computer Aiaded Geometric Design, V3
   FORSEY DR, P 15 ANN C COMP GRAP
   HABER J, 2001, P 3 ISR KOR BIN C GE
   HUTCHINSON D, 1996, 7 INT WORKSH AN SIM
   KAHLER K, 2001, P GRAPH INT NAT RES
   KOBBELT L, 1998, P 25 ANN C COMP GRAP
   Kobbelt LP, 2000, COMPUT GRAPH FORUM, V19, pC249, DOI 10.1111/1467-8659.00417
   KUMAR S, 1995, 1995 S INT 3D GRAPH
   LEE Y, 1995, P 22 ANN C COMP GRAP
   Loop C, 1987, THESIS U UTAH
   PETERSON JW, 1994, GRAPHICS GEMS, V4
   Piegl L., 1997, The Nurbs Book, Vsecond
   ROCKWOOD A, 1988, P 16 ANN C COMP GRAP
   SEO H, 2000, P IEEE VIRT REAL 200
   TAUBIN G, 1995, P INT C COMP VIS IEE
   Van Gelder A., 1998, Journal of Graphics Tools, V3, P21, DOI 10.1080/10867651.1998.10487490
   VOLINO P, 1998, P 25 ANN C COMP GRAP
   WARREN J, 2001, SUBDIIVISION METHODS
   ZORIN D, 1997, P 24 ANN C COMP GRAP
NR 23
TC 1
Z9 1
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2003
VL 19
IS 5
BP 310
EP 318
DI 10.1007/s00371-002-0185-8
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 726KP
UT WOS:000185598500004
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Xiao, F
   Zhu, YR
   Liu, RY
   Zhang, JH
   Chen, SY
AF Xiao, Feng
   Zhu, Yunrui
   Liu, Ruyu
   Zhang, Jianhua
   Chen, Shengyong
TI KSRB-Net: a continuous sign language recognition deep learning strategy
   based on motion perception mechanism
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Continuous sign language recognition; Computer vision; Sequence
   learning; Perception mechanism
AB Continuous sign language recognition (CSLR) is an intricate task aimed at transcribing sign language sequences from continuous video streams into sentences. Typically, deep learning-based CSLR systems are composed of a visual input encoder for feature extraction and a sequence learning model for the corresponding relationship between the input sequence and output sentence-level labels. The complex nature of sign language, characterized by an extensive vocabulary and many similar gestures and motions, renders CSLR particularly challenging. Additionally, the unsupervised nature of CSLR due to the unavailability of signing glosses for alignment necessitates detailed labeling of each word in a sentence, thereby limiting the amount of training data available. In this paper, we proposes a CSLR framework named KSRB-Net to address these critical problems. The proposed method incorporates a practical module that efficiently captures frame-wise motion information and spatio-temporal context information, which can be embedded into existing feature extraction modules. Additionally, a keyframe extraction algorithm based on the characteristics of the sign language dataset is designed to significantly accelerate the model training and reduce the risk of overfitting. Finally, connectionist temporal classification is employed as the objective function to capture the alignment proposal. The proposed method is validated on three datasets, namely the Chinese TJUT-SLRT, the Chinese USTC-CSL, and the German RWTH-Phoenix-Weather-2014. Experiment results demonstrate that the KSRB-Net achieves 98.40% accuracy and outperforms state-of-the-art methods in terms of efficiency and accuracy.
C1 [Xiao, Feng; Zhu, Yunrui; Zhang, Jianhua; Chen, Shengyong] Tianjin Univ Technol, Sch Integrated Circuit Sci & Engn, Tianjin Key Lab Film Elect & Commun Devices, 391 Bin Shui Xi Dao Rd, Tianjin 300384, Peoples R China.
   [Xiao, Feng; Zhu, Yunrui; Zhang, Jianhua; Chen, Shengyong] Minist Educ, Engn Res Ctr Learning Based Intelligent Syst, Tianjin 300384, Peoples R China.
   [Liu, Ruyu] Hangzhou Normal Univ, Hangzhou 311121, Peoples R China.
C3 Tianjin University of Technology; Hangzhou Normal University
RP Zhang, JH (corresponding author), Tianjin Univ Technol, Sch Integrated Circuit Sci & Engn, Tianjin Key Lab Film Elect & Commun Devices, 391 Bin Shui Xi Dao Rd, Tianjin 300384, Peoples R China.; Zhang, JH (corresponding author), Minist Educ, Engn Res Ctr Learning Based Intelligent Syst, Tianjin 300384, Peoples R China.
EM fengxiao@stud.tjut.edu.cn; yunrui.zhu@outlook.com; lry@hznu.edu.cn;
   zjh@email.tjut.edu.cn; csy@tjut.edu.cn
OI Zhang, Jianhua/0000-0001-7844-6035; Liu, Ruyu/0000-0003-2130-9122
CR [Anonymous], 2017, IEEE INT S CIRC SYST
   Camgoz Necati Cihan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10020, DOI 10.1109/CVPR42600.2020.01004
   Camgoz NC, 2017, IEEE I CONF COMP VIS, P3075, DOI 10.1109/ICCV.2017.332
   Chiu CC, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P4774, DOI 10.1109/ICASSP.2018.8462105
   Cui RP, 2019, IEEE T MULTIMEDIA, V21, P1880, DOI 10.1109/TMM.2018.2889563
   Cui RP, 2017, PROC CVPR IEEE, P1610, DOI 10.1109/CVPR.2017.175
   de Amorim CC, 2019, LECT NOTES COMPUT SC, V11731, P646, DOI 10.1007/978-3-030-30493-5_59
   Feichtenhofer C, 2016, PROC CVPR IEEE, P1933, DOI 10.1109/CVPR.2016.213
   Graves A., 2006, P 23 INT C MACHINE L, P369
   Guo D, 2018, AAAI CONF ARTIF INTE, P6845
   Guo D, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3152121
   Guo D, 2016, IEEE IMAGE PROC, P2876, DOI 10.1109/ICIP.2016.7532885
   Huang J, 2018, AAAI CONF ARTIF INTE, P2257
   Ioffe S, 2015, P INT C MACH LEARN, V2015, P1
   Jiang BY, 2019, IEEE I CONF COMP VIS, P2000, DOI 10.1109/ICCV.2019.00209
   Ka Leong Cheng, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12369), P697, DOI 10.1007/978-3-030-58586-0_41
   Koller O, 2020, IEEE T PATTERN ANAL, V42, P2306, DOI 10.1109/TPAMI.2019.2911077
   Koller O, 2017, PROC CVPR IEEE, P3416, DOI 10.1109/CVPR.2017.364
   Koller O, 2015, COMPUT VIS IMAGE UND, V141, P108, DOI 10.1016/j.cviu.2015.09.013
   Koller Oscar., 2016, P BMVC, P1, DOI 10.5244/C.30.136
   Li C, 2022, IEEE T NEUR NET LEAR, V33, P4800, DOI 10.1109/TNNLS.2021.3061115
   Li Y, 2020, PROC CVPR IEEE, P906, DOI 10.1109/CVPR42600.2020.00099
   Liao MH, 2019, AAAI CONF ARTIF INTE, P8714
   Liu T, 2016, IEEE IMAGE PROC, P2871, DOI 10.1109/ICIP.2016.7532884
   Meng FD, 2019, AAAI CONF ARTIF INTE, P224
   Molchanov P, 2016, PROC CVPR IEEE, P4207, DOI 10.1109/CVPR.2016.456
   Pu JF, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1497, DOI 10.1145/3394171.3413931
   Pu JF, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P885
   Pu JF, 2019, PROC CVPR IEEE, P4160, DOI 10.1109/CVPR.2019.00429
   Slimane FB, 2021, INT C PATT RECOG, P7884, DOI 10.1109/ICPR48806.2021.9412916
   Venugopalan S, 2015, IEEE I CONF COMP VIS, P4534, DOI 10.1109/ICCV.2015.515
   Wu D, 2016, IEEE T PATTERN ANAL, V38, P1583, DOI 10.1109/TPAMI.2016.2537340
   Xiao F., 2021, SMC
   Xiao F, 2022, 2022 10TH INTERNATIONAL CONFERENCE ON AFFECTIVE COMPUTING AND INTELLIGENT INTERACTION WORKSHOPS AND DEMOS, ACIIW, DOI 10.1109/ACIIW57231.2022.10086026
   Yang QM, 2015, IEEE T NEUR NET LEAR, V26, P3278, DOI 10.1109/TNNLS.2015.2470175
   Yin KY, 2020, Arxiv, DOI arXiv:2004.00588
   Yuan T., 2020, TJUT Sign Language Recognition and Translation Dataset
   Zhang JH, 2016, IEEE INT CON MULTI, DOI 10.1109/ICME.2016.7552950
   Zhang L, 2017, IEEE INT CONF COMP V, P3120, DOI 10.1109/ICCVW.2017.369
   Zhou H, 2020, AAAI CONF ARTIF INTE, V34, P13009
NR 40
TC 0
Z9 0
U1 4
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2023 DEC 26
PY 2023
DI 10.1007/s00371-023-03211-3
EA DEC 2023
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DE5P8
UT WOS:001130370100001
DA 2024-07-18
ER

PT J
AU Mohanty, A
   Roy, K
   Sahay, RR
AF Mohanty, Aparna
   Roy, Kankana
   Sahay, Rajiv Ranjan
TI Robust static hand gesture recognition: harnessing sparsity of deeply
   learned features
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Convolutional neural network (CNN); Hand gesture recognition; Dictionary
   learning; Sparsity; K-SVD; Discriminative KSVD; Label consistent KSVD;
   ResNeXt
ID FACE RECOGNITION; K-SVD; POSTURE; SYSTEM; DICTIONARY; TRACKING
AB Apart from verbal communication among humans, non-verbal interactions also play a significant role in conveying meaningful information. Non-verbal cues mainly comprise gestures, body postures, and facial expressions. Hand gestures constitute the preferred mechanism for non-verbal communication, and today, they also find utility in human-computer interaction (HCI), gaming, virtual reality, robotics, sign language, etc. While extensive research has been conducted on utilizing deep learning for hand gesture recognition, there has been a notable scarcity of efforts focused on leveraging the sparse characteristics of deeply acquired features to distinguish hand postures, even in the presence of challenges such as varying hand sizes, diverse spatial positions within images, and background clutter. We demonstrate the effect of data augmentation, transfer learning, and sparsity on the performance of the proposed algorithm using publicly available hand gesture datasets. We also provide a quantitative comparative analysis of the proposed approach with state-of-the-art algorithms for static hand gesture recognition. We illustrate a noteworthy finding wherein dictionary learning through LC-KSVD, when applied to fine-tuned features extracted from a deep architecture, outperforms the results achieved by state-of-the-art architectures in the context of hand gesture classification. We have realized substantial enhancements with our proposed methodology when compared to a baseline convolutional model. For instance, in the case of the EgoGesture dataset, we attained an accuracy of 94.9%, as opposed to the baseline accuracy of 63.3%, through the utilization of sparsity in deep features.
C1 [Mohanty, Aparna] Vellore Inst Technol, Vellore, Tamilnadu, India.
   [Roy, Kankana] Helmholtz Zentrum Hereon, Max Plank Str, Geesthacht, Germany.
   [Mohanty, Aparna; Roy, Kankana; Sahay, Rajiv Ranjan] Indian Inst Technol, Computat Vis Lab, Elect Engn, Kharagpur, West Bengal, India.
C3 Vellore Institute of Technology (VIT); VIT Vellore; Helmholtz
   Association; Helmholtz-Zentrum Hereon; Indian Institute of Technology
   System (IIT System); Indian Institute of Technology (IIT) - Kharagpur
RP Mohanty, A (corresponding author), Vellore Inst Technol, Vellore, Tamilnadu, India.; Mohanty, A (corresponding author), Indian Inst Technol, Computat Vis Lab, Elect Engn, Kharagpur, West Bengal, India.
EM aparnamhnty@gmail.com; kankana.kankana.roy@gmail.com;
   rajivranjansay@gmail.com
CR Aharon M, 2006, IEEE T SIGNAL PROCES, V54, P4311, DOI 10.1109/TSP.2006.881199
   [Anonymous], 2011, Res Lett Inf Math Sci
   Candès EJ, 2008, IEEE SIGNAL PROC MAG, V25, P21, DOI 10.1109/MSP.2007.914731
   Candès EJ, 2006, COMMUN PUR APPL MATH, V59, P1207, DOI 10.1002/cpa.20124
   Chan TH, 2015, IEEE T IMAGE PROCESS, V24, P5017, DOI 10.1109/TIP.2015.2475625
   Chao YW, 2021, PROC CVPR IEEE, P9040, DOI 10.1109/CVPR46437.2021.00893
   Chen DS, 2017, SENSORS-BASEL, V17, DOI 10.3390/s17020253
   CHEN S, 1989, INT J CONTROL, V50, P1873, DOI 10.1080/00207178908953472
   DAVIS G, 1994, OPT ENG, V33, P2183, DOI 10.1117/12.173207
   Davis G, 1997, CONSTR APPROX, V13, P57, DOI 10.1007/BF02678430
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   El-Sawah A, 2008, IEEE T INSTRUM MEAS, V57, P1627, DOI 10.1109/TIM.2008.925725
   Elad M, 2005, APPL COMPUT HARMON A, V19, P340, DOI 10.1016/j.acha.2005.03.005
   Elad M, 2006, IEEE T IMAGE PROCESS, V15, P3736, DOI 10.1109/TIP.2006.881969
   Garcia B., 2016, Convolut. Neural Netw. Vis. Recognit., V2
   Ge C., 2017, MACHINE LEARNING SIG, P1
   Ge SS, 2008, IMAGE VISION COMPUT, V26, P1607, DOI 10.1016/j.imavis.2008.03.004
   Hara K, 2018, PROC CVPR IEEE, P6546, DOI 10.1109/CVPR.2018.00685
   Jaramillo-Yánez A, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20092467
   Jiang ZL, 2013, IEEE T PATTERN ANAL, V35, P2651, DOI 10.1109/TPAMI.2013.88
   Kawulok M, 2013, IEEE INT CONF AUTOMA, DOI 10.1109/FG.2013.6553733
   Kawulok M, 2014, PATTERN RECOGN LETT, V41, P3, DOI 10.1016/j.patrec.2013.08.028
   Kendon A., 1986, The Biological Foundations of Gestures: Motor and Semiotic Aspects
   Keskin C., 2012, Randomized Decision Forests for Static and Dynamic Hand Shape Classification, P31
   Kim SY, 2017, IEEE SENS J, V17, P2975, DOI 10.1109/JSEN.2017.2679220
   Köpüklü O, 2019, IEEE INT CONF AUTOMA, P407
   Krizhevsky A., 2009, LEARNING MULTIPLE LA, DOI DOI 10.1145/3065386
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kumar P.P., 2010, 2010 11 INT C CONTR, P1151
   Kumar PP, 2010, INT J HUM ROBOT, V7, P331, DOI 10.1142/S0219843610002180
   Kviatkovsky I, 2017, IEEE T PATTERN ANAL, V39, P411, DOI 10.1109/TPAMI.2016.2545661
   LADES M, 1993, IEEE T COMPUT, V42, P300, DOI 10.1109/12.210173
   LeCun Y, 2004, PROC CVPR IEEE, P97
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Li G, 2017, IEEE RAD CONF, P928, DOI 10.1109/RADAR.2017.7944336
   Lu W, 2016, IEEE SIGNAL PROC LET, V23, P1188, DOI 10.1109/LSP.2016.2590470
   Mairal J., 2009, P 26 ANN INT C MACHI, P689, DOI 10.1145/1553374.1553463
   MALLAT SG, 1993, IEEE T SIGNAL PROCES, V41, P3397, DOI 10.1109/78.258082
   Marcel S, 1999, LECT NOTES ARTIF INT, V1739, P97
   Materzynska J, 2019, IEEE INT CONF COMP V, P2874, DOI 10.1109/ICCVW.2019.00349
   Min YC, 2020, PROC CVPR IEEE, P5760, DOI 10.1109/CVPR42600.2020.00580
   Mohanty A., 2018, Preservation, DOI 10.1007/978-981-10-7221-5_11
   Mohanty A., 2016, P INT C COMP VIS IM, V2, P449
   Mohanty A, 2016, SIGNAL PROCESS-IMAGE, V47, P529, DOI 10.1016/j.image.2016.05.019
   Molchanov P, 2016, PROC CVPR IEEE, P4207, DOI 10.1109/CVPR.2016.456
   Nalepa J, 2014, ADV INTEL SOFT COMPU, V242, P79, DOI 10.1007/978-3-319-02309-0_8
   Oudah M, 2020, J IMAGING, V6, DOI 10.3390/jimaging6080073
   Padhy S, 2021, IEEE SENS J, V21, P6634, DOI 10.1109/JSEN.2020.3042540
   PATI YC, 1993, CONFERENCE RECORD OF THE TWENTY-SEVENTH ASILOMAR CONFERENCE ON SIGNALS, SYSTEMS & COMPUTERS, VOLS 1 AND 2, P40, DOI 10.1109/ACSSC.1993.342465
   Pisharady PK, 2013, INT J COMPUT VISION, V101, P403, DOI 10.1007/s11263-012-0560-5
   Rastgoo R, 2021, EXPERT SYST APPL, V164, DOI 10.1016/j.eswa.2020.113794
   Roy K, 2017, IEEE INT CONF COMP V, P640, DOI 10.1109/ICCVW.2017.81
   Sang Y, 2018, Arxiv, DOI arXiv:1712.00216
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Teng XL, 2005, J VISUAL LANG COMPUT, V16, P442, DOI 10.1016/j.jvlc.2005.04.003
   Triesch J, 1996, PROCEEDINGS OF THE SECOND INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, P170, DOI 10.1109/AFGR.1996.557260
   Triesch J, 1998, AUTOMATIC FACE AND GESTURE RECOGNITION - THIRD IEEE INTERNATIONAL CONFERENCE PROCEEDINGS, P546, DOI 10.1109/AFGR.1998.671005
   Triesch J, 2001, IEEE T PATTERN ANAL, V23, P1449, DOI 10.1109/34.977568
   Tropp JA, 2004, IEEE T INFORM THEORY, V50, P2231, DOI 10.1109/TIT.2004.834793
   Ueda E, 2003, IEEE T IND ELECTRON, V50, P676, DOI 10.1109/TIE.2003.814758
   Vedaldi A, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P689, DOI 10.1145/2733373.2807412
   Wiskott L, 1997, IEEE T PATTERN ANAL, V19, P775, DOI 10.1109/34.598235
   Wright J, 2009, IEEE T PATTERN ANAL, V31, P210, DOI 10.1109/TPAMI.2008.79
   Xiang Y, 2018, ROBOTICS: SCIENCE AND SYSTEMS XIV
   Xie RQ, 2016, IEEE SENS J, V16, P4537, DOI 10.1109/JSEN.2016.2546942
   Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634
   Yang C, 2016, ELECTRON LETT, V52, P1679, DOI 10.1049/el.2016.0841
   Yang JC, 2010, IEEE T IMAGE PROCESS, V19, P2861, DOI 10.1109/TIP.2010.2050625
   Yin XM, 2003, PATTERN RECOGN, V36, P567, DOI 10.1016/S0031-3203(02)00072-9
   Yu-Ting Li, 2012, Progress in Pattern Recognition, Image Analysis, ComputerVision, and Applications. Proceedings 17th Iberoamerican Congress, CIARP 2012, P308, DOI 10.1007/978-3-642-33275-3_38
   Zhang QA, 2010, PROC CVPR IEEE, P2691, DOI 10.1109/CVPR.2010.5539989
   Zhang YF, 2018, IEEE T MULTIMEDIA, V20, P1038, DOI 10.1109/TMM.2018.2808769
NR 73
TC 0
Z9 0
U1 4
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2023 DEC 14
PY 2023
DI 10.1007/s00371-023-03179-0
EA DEC 2023
PG 25
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CG2L2
UT WOS:001124032100002
DA 2024-07-18
ER

PT J
AU Liu, ZY
   Xue, R
AF Liu, Zhaoyang
   Xue, Ru
TI Visual image encryption based on compressed sensing and Cycle-GAN
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Discrete wavelet transform; Compressed sensing; Cycle generative
   adversarial network; Improved Henon map; Visual image encryption
ID NETWORKS
AB At present, most image encryption schemes directly change plaintext images into ciphertext images without visual significance, and such ciphertext images can be detected by hackers during transmission, and therefore subject to various attacks. To protect the content security and visual safety of images, a learning visual image encryption scheme based on compressed sensing (CS) and cycle generative adversarial network is proposed. First, the secret image is sparse by discrete wavelet transform and compressed by CS. Secondly, the compressed image is permuted and diffused by an improved Henon map to obtain the ciphertext image. Finally, the images are migrated from the ciphertext domain to the plaintext domain by generating an adversarial network to obtain visually meaningful images. We constrain and guide the image generation process by introducing a feature loss function to guarantee the quality of the reconstructed images. Experimental results and security analysis show that the image encryption scheme has sufficient key space, strong key sensitivity, and high reconstruction quality.
C1 [Liu, Zhaoyang; Xue, Ru] Xizang Minzu Univ, Sch Informat Engn, Xianyang 712082, Shaanxi, Peoples R China.
   [Liu, Zhaoyang; Xue, Ru] Key Lab Opt Informat Proc & Visualizat Technol Tib, Xianyang 712082, Shaanxi, Peoples R China.
   [Liu, Zhaoyang; Xue, Ru] Xizang Cyberspace Governance Res Ctr, Xianyang, Peoples R China.
C3 Xizang Minzu University
RP Xue, R (corresponding author), Xizang Minzu Univ, Sch Informat Engn, Xianyang 712082, Shaanxi, Peoples R China.; Xue, R (corresponding author), Key Lab Opt Informat Proc & Visualizat Technol Tib, Xianyang 712082, Shaanxi, Peoples R China.; Xue, R (corresponding author), Xizang Cyberspace Governance Res Ctr, Xianyang, Peoples R China.
EM rxue@xzmu.edu.cn
OI Xue, Ru/0000-0002-2793-1257
FU National Natural Science Foundation of China [62262062]; major programs
   incubation plan of Xizang Minzu University [22MDZ03]; Research Team
   Project for Xizang-related Network Information Content and Data Security
   [324042000709]
FX This project is supported in part by the National Natural Science
   Foundation of China: 62262062, the major programs incubation plan of
   Xizang Minzu University: 22MDZ03, and the Research Team Project for
   Xizang-related Network Information Content and Data Security (No.
   324042000709).
CR Abdal R, 2019, IEEE I CONF COMP VIS, P4431, DOI 10.1109/ICCV.2019.00453
   Bandeira AS, 2013, IEEE T INFORM THEORY, V59, P3448, DOI 10.1109/TIT.2013.2248414
   Bao L, 2015, INFORM SCIENCES, V324, P197, DOI 10.1016/j.ins.2015.06.049
   Bao ZJ, 2021, APPL OPTICS, V60, P5320, DOI 10.1364/AO.428203
   Baraniuk RG, 2007, IEEE SIGNAL PROC MAG, V24, P118, DOI 10.1109/MSP.2007.4286571
   Belazi A, 2016, SIGNAL PROCESS, V128, P155, DOI 10.1016/j.sigpro.2016.03.021
   Ben Farah MA, 2020, NONLINEAR DYNAM, V99, P3041, DOI 10.1007/s11071-019-05413-8
   Bezerra JIM, 2023, CHAOS SOLITON FRACT, V168, DOI 10.1016/j.chaos.2023.113160
   Cao YJ, 2019, IEEE ACCESS, V7, P14985, DOI 10.1109/ACCESS.2018.2886814
   Chai XL, 2017, SIGNAL PROCESS, V134, P35, DOI 10.1016/j.sigpro.2016.11.016
   Chen GR, 2004, CHAOS SOLITON FRACT, V21, P749, DOI 10.1016/j.chaos.2003.12.022
   Ding Y, 2022, IEEE T NEUR NET LEAR, V33, P4915, DOI [10.1109/TNNLS.2021.3062754, 10.1080/00206814.2021.1969525]
   Ding Y, 2021, IEEE INTERNET THINGS, V8, P1504, DOI 10.1109/JIOT.2020.3012452
   Donoho DL, 2012, IEEE T INFORM THEORY, V58, P1094, DOI 10.1109/TIT.2011.2173241
   Donoho DL, 2006, IEEE T INFORM THEORY, V52, P1289, DOI 10.1109/TIT.2006.871582
   Fang W, 2018, CMC-COMPUT MATER CON, V57, P167, DOI 10.32604/cmc.2018.02356
   Gao XH, 2021, PHYS SCRIPTA, V96, DOI 10.1088/1402-4896/abed7d
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Kesavan KK, 2011, 2011 3 INT C ULTR TE, P1
   Krstulovic S., 2006, 2006 IEEE INT C AC S, V3, pIII
   Kumar M. Ratheesh, 2010, 2010 International Congress on Ultra Modern Telecommunications and Control Systems and Workshops (ICUMT 2010), P860, DOI 10.1109/ICUMT.2010.5676474
   Li XL, 2018, EURASIP J IMAGE VIDE, DOI 10.1186/s13640-018-0358-7
   Man ZL, 2019, IEEE ACCESS, V7, P103047, DOI 10.1109/ACCESS.2019.2931732
   Panwar Kirtee, 2023, Procedia Computer Science, P644, DOI 10.1016/j.procs.2023.01.046
   Ping P, 2022, DIGIT SIGNAL PROCESS, V120, DOI 10.1016/j.dsp.2021.103263
   Ren H, 2022, FRACTAL FRACT, V6, DOI 10.3390/fractalfract6060302
   Shamsi Z, 2023, MULTIMED TOOLS APPL, V82, P33063, DOI 10.1007/s11042-023-14735-0
   Shen HF, 2014, IEEE T GEOSCI REMOTE, V52, P894, DOI 10.1109/TGRS.2013.2245509
   Tang ZJ, 2015, MULTIMED TOOLS APPL, V74, P5429, DOI 10.1007/s11042-014-1861-1
   Wang C, 2022, SIGNAL PROCESS, V196, DOI 10.1016/j.sigpro.2022.108536
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Xian YJ, 2023, MULTIMED TOOLS APPL, V82, P407, DOI 10.1007/s11042-022-13280-6
   Xu SC, 2022, CHAOS SOLITON FRACT, V157, DOI 10.1016/j.chaos.2022.111889
   Yabing Zhu, 2019, Natural Language Processing and Chinese Computing. 8th CCF International Conference, NLPCC 2019. Proceedings. Lecture Notes in Artificial Intelligence, Subseries of Lecture Notes in Computer Science (LNAI 11839), P529, DOI 10.1007/978-3-030-32236-6_48
   Yan S, 2023, IEEE T FUZZY SYST, V31, P930, DOI 10.1109/TFUZZ.2022.3193757
   Ye GD, 2021, T EMERG TELECOMMUN T, V32, DOI 10.1002/ett.4071
   Zhang H, 2017, IEEE I CONF COMP VIS, P5908, DOI 10.1109/ICCV.2017.629
   Zhang XB, 2019, IEEE ACCESS, V7, P175554, DOI 10.1109/ACCESS.2019.2955759
   Zhang YB, 2020, IEEE T IMAGE PROCESS, V29, P1101, DOI 10.1109/TIP.2019.2938347
   Zhuang QB, 2022, COMPUT METH PROG BIO, V226, DOI 10.1016/j.cmpb.2022.107096
NR 40
TC 0
Z9 0
U1 16
U2 25
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2023 NOV 15
PY 2023
DI 10.1007/s00371-023-03140-1
EA NOV 2023
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA Y0UG9
UT WOS:001102500100001
DA 2024-07-18
ER

PT J
AU Zhao, LJ
   Zhao, LL
   Cui, FP
   Sun, TT
AF Zhao, Lijiang
   Zhao, Lilong
   Cui, Fenping
   Sun, Tingting
TI Satellite image encryption based on RNA and 7D complex chaotic system
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Satellite image encryption; Complex chaotic system; RNA coding; Arnold
   transform; Information security
AB With the rapid development of remote sensing technology, satellite images have been widely used in various fields. To ensure information security during satellite transmission, it is essential to improve the cryptographic techniques. In this work, a novel satellite image encryption algorithm based on seven-dimensional complex chaotic system and RNA computing is proposed. At first, the complex chaotic system and its dynamics characteristics are briefly introduced. And then eight RNA coding rules and six RNA operations are given in detail. Next, an image encryption algorithm is designed which consists of scrambling process and diffusion process. During the scrambling phase, the satellite image is scrambled by Arnold transform combined with the chaotic sequences. Then during the diffusion process, substitution of the RNA codon, achieved by Arnold transform, is applied to modify the value of the pixel. In the end, security analysis has been discussed in detail. Experimental results show that the proposed algorithm has excellent satisfactory security and tolerance to single event upsets.
C1 [Zhao, Lijiang; Zhao, Lilong; Cui, Fenping; Sun, Tingting] Nanjing Univ Informat Sci & Technol, Sch Phys & Optoelect Engn, Jiangsu Key Lab Optoelect Detect Atmosphere & Ocea, Nanjing 210044, Peoples R China.
C3 Nanjing University of Information Science & Technology
RP Zhao, LL (corresponding author), Nanjing Univ Informat Sci & Technol, Sch Phys & Optoelect Engn, Jiangsu Key Lab Optoelect Detect Atmosphere & Ocea, Nanjing 210044, Peoples R China.
EM 2418533277@qq.com; llzhao@nuist.edu.cn; cuifenping@nuist.edu.cn;
   001966@nuist.edu.cn
RI 赵, 理江/IXN-1003-2023
FU This work was supported in part by the National Key Research and
   Development Program of China under Grants 2018YFB1800901 and
   2018YFB1800905, in part by the National Natural Science Foundation of
   China under Grants 62075097, 62075038, 61975084, 61935005, 6
   [2018YFB1800901, 2018YFB1800905]; National Key Research and Development
   Program of China [62075097, 62075038, 61975084, 61935005, 61835005,
   61822507, 62005125, 61775098, 61875248, 61727817, U2001601, 62035018,
   61720106015, 61935011]; National Natural Science Foundation of China
FX This work was supported in part by the National Key Research and
   Development Program of China under Grants 2018YFB1800901 and
   2018YFB1800905, in part by the National Natural Science Foundation of
   China under Grants 62075097, 62075038, 61975084, 61935005, 61835005,
   61822507, 62005125, 61775098, 61875248, 61727817, U2001601, 62035018,
   61720106015, and 61935011.
CR Adeel M, 2022, IJST-T ELECTR ENG, V46, P847, DOI 10.1007/s40998-022-00503-7
   ADLEMAN LM, 1994, SCIENCE, V266, P1021, DOI 10.1126/science.7973651
   Aygun E., 2019, CUMHUR SCI J, DOI [10.17776/csj.416395, DOI 10.17776/csj.416395]
   Bensikaddour EH, 2020, J KING SAUD UNIV-COM, V32, P50, DOI 10.1016/j.jksuci.2018.05.002
   Chu R, 2022, FRONT PHYS-LAUSANNE, V10, DOI 10.3389/fphy.2022.844966
   Du SM, 2023, ALEX ENG J, V66, P979, DOI 10.1016/j.aej.2022.10.066
   Fang PF, 2023, VISUAL COMPUT, V39, P1975, DOI 10.1007/s00371-022-02459-5
   Fridrich J, 1998, INT J BIFURCAT CHAOS, V8, P1259, DOI 10.1142/S021812749800098X
   Gilbert WV, 2016, SCIENCE, V352, P1408, DOI 10.1126/science.aad8711
   Gopalakrishnan T, 2019, WIRELESS PERS COMMUN, V109, P437, DOI 10.1007/s11277-019-06573-x
   Hosny KM, 2023, VISUAL COMPUT, V39, P1027, DOI 10.1007/s00371-021-02382-1
   Hosny KM, 2022, J AMB INTEL HUM COMP, V13, P973, DOI 10.1007/s12652-021-03675-y
   Hosny KM, 2022, MULTIMED TOOLS APPL, V81, P505, DOI 10.1007/s11042-021-11384-z
   Hua ZY, 2019, IEEE ACCESS, V7, P8660, DOI 10.1109/ACCESS.2018.2890116
   Hua ZY, 2016, INFORM SCIENCES, V339, P237, DOI 10.1016/j.ins.2016.01.017
   Kadir A, 2014, OPTIK, V125, P1671, DOI 10.1016/j.ijleo.2013.09.040
   Kang XJ, 2020, SIGNAL PROCESS-IMAGE, V80, DOI 10.1016/j.image.2019.115670
   Kim Daehak, 2014, [Journal of the Korean Data And Information Science Sociaty, 한국데이터정보과학회지], V25, P1419, DOI 10.7465/jkdi.2014.25.6.1419
   Kou L, 2022, FRONT ENERGY RES, V10, DOI 10.3389/fenrg.2022.980863
   Kumar A, 2021, MULTIMED TOOLS APPL, V80, P27785, DOI 10.1007/s11042-021-10970-5
   Lian SG, 2005, CHAOS SOLITON FRACT, V26, P117, DOI 10.1016/j.chaos.2004.11.096
   Liu LC, 2019, ENTROPY-SWITZ, V21, DOI 10.3390/e21030287
   Lu Q, 2021, SYMMETRY-BASEL, V13, DOI 10.3390/sym13122317
   Lu Y, 2022, OPTIK, V263, DOI 10.1016/j.ijleo.2022.169357
   Ma XJ, 2020, NONLINEAR DYNAM, V100, P2859, DOI 10.1007/s11071-020-05601-x
   Madani M., 2017, J. Electr. Electron. Eng, V10, P29, DOI [10.1007/s00500-023-08747-z, DOI 10.1007/S00500-023-08747-Z]
   Murali P, 2023, VISUAL COMPUT, V39, P1057, DOI 10.1007/s00371-021-02384-z
   Nabil M., 2020, Int. J. Recent Technol. Eng, DOI [10.1007/s10470-021-01959-z, DOI 10.1007/S10470-021-01959-Z]
   Naim M, 2023, J SUPERCOMPUT, V79, P17585, DOI 10.1007/s11227-023-05346-5
   Naim M, 2023, INF SECUR J, V32, P187, DOI 10.1080/19393555.2021.1982082
   Naim M, 2021, ADV SPACE RES, V67, P2077, DOI 10.1016/j.asr.2021.01.018
   Naskar PK, 2021, NONLINEAR DYNAM, V103, P2019, DOI 10.1007/s11071-020-06164-7
   Nguimdo RM, 2012, OPT EXPRESS, V20, P28603, DOI 10.1364/OE.20.028603
   Sakamoto K, 2023, INT J MOL SCI, V24, DOI 10.3390/ijms24010361
   Suman RR, 2022, MULTIMED TOOLS APPL, V81, P27089, DOI 10.1007/s11042-021-11460-4
   Sun M., 2023, IAENG Int. J. Comput. Sci, DOI [10.1016/j.optlastec.2020.106366, DOI 10.1016/J.OPTLASTEC.2020.106366]
   Tahbaz M, 2023, MULTIMED TOOLS APPL, DOI 10.1007/s11042-023-15677-3
   Tahiri MA, 2023, VISUAL COMPUT, V39, P6395, DOI 10.1007/s00371-022-02736-3
   Wang SM, 2022, OPT LASER TECHNOL, V148, DOI 10.1016/j.optlastec.2021.107753
   Wang XY, 2023, VISUAL COMPUT, V39, P43, DOI 10.1007/s00371-021-02311-2
   Wang XY, 2021, MULTIMED TOOLS APPL, V80, P23337, DOI 10.1007/s11042-020-10209-9
   WOLF A, 1985, PHYSICA D, V16, P285, DOI 10.1016/0167-2789(85)90011-9
   Wu YH, 2020, J INTELL FUZZY SYST, V39, P5085, DOI 10.3233/JIFS-179994
   Xu J, 2022, VISUAL COMPUT, V38, P1509, DOI 10.1007/s00371-021-02085-7
   Yan XP, 2021, MULTIMED TOOLS APPL, V80, P10949, DOI 10.1007/s11042-020-10218-8
   Yang CH, 2020, SYMMETRY-BASEL, V12, DOI 10.3390/sym12020189
   Yang QG, 2018, INT J BIFURCAT CHAOS, V28, DOI 10.1142/S0218127418500578
   Zhan K, 2017, J ELECTRON IMAGING, V26, DOI 10.1117/1.JEI.26.1.013021
   Zhang J, 2014, MATH PROBL ENG, V2014, DOI 10.1155/2014/917147
   Zhang QY, 2021, MULTIMED TOOLS APPL, V80, P13841, DOI 10.1007/s11042-020-10437-z
   Zhang S., 2021, Adv. Comput. Signals Syst., DOI [10.23977/ACSS.2021.050115, DOI 10.23977/ACSS.2021.050115]
   Zhang Y, 2021, INFORM SCIENCES, V547, P307, DOI 10.1016/j.ins.2020.07.058
   Zhu SL, 2023, MATHEMATICS-BASEL, V11, DOI 10.3390/math11010231
NR 53
TC 0
Z9 0
U1 8
U2 14
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2023 OCT 20
PY 2023
DI 10.1007/s00371-023-03128-x
EA OCT 2023
PG 21
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA U9HU2
UT WOS:001087848800001
DA 2024-07-18
ER

PT J
AU Karpukhin, I
   Dereka, S
   Kolesnikov, S
AF Karpukhin, Ivan
   Dereka, Stanislav
   Kolesnikov, Sergey
TI Probabilistic embeddings revisited
SO VISUAL COMPUTER
LA English
DT Article
DE Probabilistic metric learning; Metric learning; Computer vision; Image
   retrieval
AB In recent years, deep metric learning and its probabilistic extensions claimed state-of-the-art results in the face verification task. Despite improvements in face verification, probabilistic methods received little attention in the research community and practical applications. Previous metric learning benchmarks avoided comparison with probabilistic methods, and it is still unclear whether they generalize well to image retrieval tasks beyond faces. In this paper, we, for the first time, perform an in-depth analysis and unified comparison of known probabilistic methods in verification and retrieval tasks. We study different design choices and argue that many of them have limited impact, while only a few probabilistic methods surpass modern metric learning approaches. Combining the best ideas of earlier works, we propose a simple modification of the top-performing method, achieving new state-of-the-art results among probabilistic methods. Finally, we study confidence prediction and show that it correlates with data quality, but contains little information about prediction error probability. We thus provide a new confidence evaluation benchmark and establish a baseline for future confidence prediction research. PyTorch implementation is publicly released.
C1 [Karpukhin, Ivan; Dereka, Stanislav; Kolesnikov, Sergey] Tinkoff, Golovinskoye Highway 5a, Moscow 125212, Russia.
RP Karpukhin, I (corresponding author), Tinkoff, Golovinskoye Highway 5a, Moscow 125212, Russia.
EM i.a.karpukhin@tinkoff.ru; s.dereka@tinkoff.ru; s.s.kolesnikov@tinkoff.ru
RI Karpukhin, Ivan/V-5514-2017
OI Karpukhin, Ivan/0000-0001-9694-7810
CR Aziere N, 2019, PROC CVPR IEEE, P7291, DOI 10.1109/CVPR.2019.00747
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Bishop C. M., 1994, NCRG94004 AST U, DOI DOI 10.1007/978-3-322-81570-58
   Chang J, 2020, PROC CVPR IEEE, P5709, DOI 10.1109/CVPR42600.2020.00575
   Cheng D, 2016, PROC CVPR IEEE, P1335, DOI 10.1109/CVPR.2016.149
   Davidson TR, 2018, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P856
   Deng JK, 2019, PROC CVPR IEEE, P4685, DOI 10.1109/CVPR.2019.00482
   El-Yaniv R, 2010, J MACH LEARN RES, V11, P1605
   Hadsell R, 2006, IEEE C COMP VIS PATT, P1735, DOI DOI 10.1109/CVPR.2006.100
   Hasnat M., 2017, ARXIV
   Huang G.B., 2014, LABELED FACES WILD U
   Huang JS, 2015, IEEE I CONF COMP VIS, P1062, DOI 10.1109/ICCV.2015.127
   Jian Wang, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P2612, DOI 10.1109/ICCV.2017.283
   Jun H., 2019, ARXIV
   Jung S., 2021, P IEEE CVF INT C COM, P15425
   Kim S, 2020, PROC CVPR IEEE, P5163, DOI 10.1109/CVPR42600.2020.00521
   Kingma D. P., 2014, arXiv
   Krause J, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P554, DOI 10.1109/ICCVW.2013.77
   Li S, 2021, PROC CVPR IEEE, P15624, DOI 10.1109/CVPR46437.2021.01537
   Lin CW, 2022, VISUAL COMPUT, V38, P1741, DOI 10.1007/s00371-021-02102-9
   Liu LZ, 2022, LECT NOTES COMPUT SC, V13686, P399, DOI 10.1007/978-3-031-19809-0_23
   Liu W, 2017, ADV SOC SCI EDUC HUM, V99, P212
   Liu ZW, 2016, PROC CVPR IEEE, P1096, DOI 10.1109/CVPR.2016.124
   Mena J, 2020, IEEE ACCESS, V8, P101721, DOI 10.1109/ACCESS.2020.2996495
   Meng Q, 2021, PROC CVPR IEEE, P14220, DOI 10.1109/CVPR46437.2021.01400
   Movshovitz-Attias Y, 2017, IEEE I CONF COMP VIS, P360, DOI 10.1109/ICCV.2017.47
   Musgrave Kevin, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12370), P681, DOI 10.1007/978-3-030-58595-2_41
   Oh Seong Joon, 2018, INT C LEARN REPR
   Phillips PJONATHON., 2003, Evaluation report
   Ridgeway, 2019, ARXIV
   Roth K., 2020, INT C MACH LEARN
   Roth Karsten, 2021, ICML, V139, P9095
   Scott T. R., 2021, P IEEE CVF C COMP VI, P10612
   Shi YC, 2019, IEEE I CONF COMP VIS, P6901, DOI 10.1109/ICCV.2019.00700
   Sohn K, 2016, ADV NEUR IN, V29
   Song HO, 2016, PROC CVPR IEEE, P4004, DOI 10.1109/CVPR.2016.434
   Wang F, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1041, DOI 10.1145/3123266.3123359
   Wang F, 2018, IEEE SIGNAL PROC LET, V25, P926, DOI 10.1109/LSP.2018.2822810
   Wang H, 2018, PROC CVPR IEEE, P5265, DOI 10.1109/CVPR.2018.00552
   Wang M., 2020, NEUROCOMPUTING
   Wang X, 2019, PROC CVPR IEEE, P5017, DOI 10.1109/CVPR.2019.00516
   Weinberger KQ, 2009, J MACH LEARN RES, V10, P207
   Welinder P., 2010, Technical Report CNS-TR-2010-001
   Wu CY, 2017, IEEE I CONF COMP VIS, P2859, DOI 10.1109/ICCV.2017.309
   Wu QB, 2018, IEEE T IMAGE PROCESS, V27, P2499, DOI 10.1109/TIP.2018.2799331
   Yu BS, 2019, IEEE I CONF COMP VIS, P6499, DOI 10.1109/ICCV.2019.00659
   Zheng WZ, 2021, PROC CVPR IEEE, P9316, DOI 10.1109/CVPR46437.2021.00920
NR 47
TC 0
Z9 0
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2024
VL 40
IS 6
BP 4373
EP 4386
DI 10.1007/s00371-023-03087-3
EA SEP 2023
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TV2X4
UT WOS:001069495600001
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Wang, S
   Wang, Q
   Min, WD
   Han, Q
   Gai, D
   Luo, HW
AF Wang, Sheng
   Wang, Qi
   Min, Weidong
   Han, Qing
   Gai, Di
   Luo, Haowen
TI Trade-off background joint learning for unsupervised vehicle
   re-identification
SO VISUAL COMPUTER
LA English
DT Article
DE Vehicle re-identification; Label smoothing; Pyramid background-aware
   attention; Joint learning
AB Existing vehicle re-identification (Re-ID) methods either extract valuable background information to enhance the robustness of the vehicle model or segment background interference information to learn vehicle fine-grained information. However, these methods do not consider the background information as a trade-off attribute to unite valuable background and background interference. This work proposes the trade-off background joint learning method for unsupervised vehicle Re-ID, which consists of two branches, to exploit the ambivalence of background information. In the global branch, a background focus of the pyramid global branch module is proposed to optimize the sample feature space. The designed pyramid background-aware attention extracts background-related features from the global image and constructs a two-fold confidence metric based on background-related and identity-related confidence scores to obtain robust clustering results during the clustering. In the local branch, a background filtering of the local branch module is proposed to alleviate the background interference. First, the background of each local region is segmented and weakened. Then, a background adaptive local label smoothing is designed to reduce noise in every local region. Comprehensive experiments on VeRi-776 and VeRi-Wild are conducted to validate the performance of the proposed balanced background information method. Experimental results show that the proposed method outperforms the state-of-the-art.
C1 [Wang, Sheng] Nanchang Univ, Sch Software, Nanchang 330047, Peoples R China.
   [Wang, Qi; Min, Weidong; Han, Qing; Gai, Di] Nanchang Univ, Sch Math & Comp Sci, Nanchang 330031, Peoples R China.
   [Luo, Haowen] Nanchang Univ, Med Big Data Ctr, Affiliated Hosp 2, Nanchang 330000, Peoples R China.
   [Wang, Qi; Min, Weidong; Han, Qing; Gai, Di] Nanchang Univ, Inst Metaverse, Nanchang 330031, Peoples R China.
   [Wang, Qi; Min, Weidong; Han, Qing; Gai, Di] Jiangxi Key Lab Smart City, Nanchang 330031, Peoples R China.
C3 Nanchang University; Nanchang University; Nanchang University; Nanchang
   University
RP Min, WD (corresponding author), Nanchang Univ, Sch Math & Comp Sci, Nanchang 330031, Peoples R China.; Min, WD (corresponding author), Nanchang Univ, Inst Metaverse, Nanchang 330031, Peoples R China.; Min, WD (corresponding author), Jiangxi Key Lab Smart City, Nanchang 330031, Peoples R China.
EM ws@email.ncu.edu.cn; wangqi@ncu.edu.cn; minweidong@ncu.edu.cn;
   hanqing@ncu.edu.cn; gaidi@ncu.edu.cn; ndefy19423@ncu.edu.cn
RI han, qing/KCZ-0174-2024; Han, Qing-Long/B-6635-2013; Min,
   Weidong/D-4585-2017
OI Han, Qing-Long/0000-0002-7207-0716; Wang, Qi/0009-0001-3708-6560; Min,
   Weidong/0000-0003-2526-2181
FU National Natural Science Foundation of China [62076117, 62166026];
   Jiangxi Key Laboratory of Smart City [20192BCD40002]; Jiangxi Provincial
   Natural Science Foundation [20224BAB212]
FX AcknowledgementsThis work was supported by the National Natural Science
   Foundation of China under Grant No. 62076117 and 62166026, the Jiangxi
   Key Laboratory of Smart City under Grant No. 20192BCD40002 and the
   Jiangxi Provincial Natural Science Foundation under Grant No.
   20224BAB212.
CR Chen H, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14940, DOI 10.1109/ICCV48922.2021.01469
   Chen J., 2021, Transunet: transformers make strong encoders for medical image segmentation
   Chen XY, 2021, J INTELL TRANSPORT S, V26, P100, DOI 10.1080/15472450.2020.1797502
   Cho Y., 2022, ARXIV
   Ding YH, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3369393
   Dongkai Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10978, DOI 10.1109/CVPR42600.2020.01099
   Fu Y., 2019, SELF SIMILARITY GROU, P6112
   Ge Y., 2020, Self-paced contrastive learning with hybrid memory for domain adaptive object re-ID. 2020-December
   Ge YX, 2020, Arxiv, DOI [arXiv:2001.01526, 10.48550/arXiv.2001.01526]
   Han X., 2021, RETHINKING SAMPLING
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu Z., 2021, HARD SAMPLE GUIDED H
   Huang Z., 2021, ADV NEURAL INF PROCE, V34, P29406
   Khorramshahi P., 2020, DEVIL IS DETAILS SEL
   Li MK, 2022, IEEE T IMAGE PROCESS, V31, P3606, DOI 10.1109/TIP.2022.3173163
   Liu XC, 2016, LECT NOTES COMPUT SC, V9906, P869, DOI 10.1007/978-3-319-46475-6_53
   Lou YH, 2019, PROC CVPR IEEE, P3230, DOI 10.1109/CVPR.2019.00335
   Lu ZF, 2022, IEEE T INTELL TRANSP, V23, P19001, DOI 10.1109/TITS.2022.3157463
   Munir A., 2021, ORIENTED SPLITS NETW, P1
   Pan XG, 2018, LECT NOTES COMPUT SC, V11208, P484, DOI 10.1007/978-3-030-01225-0_29
   Peng J., 2019, ELIMINATING CROSS CA
   Selvaraju RR, 2017, IEEE I CONF COMP VIS, P618, DOI 10.1109/ICCV.2017.74
   Sun YF, 2018, LECT NOTES COMPUT SC, V11208, P501, DOI 10.1007/978-3-030-01225-0_30
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Wu M., 2020, BACKGROUND SEGMENTAT
   Yang FX, 2021, PROC CVPR IEEE, P4853, DOI 10.1109/CVPR46437.2021.00482
   Yang M., 2022, LEARNING TWIN NOISY, P14308
   Yang QZ, 2019, PROC CVPR IEEE, P3628, DOI 10.1109/CVPR.2019.00375
   Yu J., 2021, CAMERA TRACKLET AWAR
   Yu J., 2021, UNSUPERVISED PERSON
   Yu J, 2021, IEEE INT C INT ROBOT, P3806, DOI 10.1109/IROS51168.2021.9636545
   Zhang X, 2021, PROC CVPR IEEE, P3435, DOI 10.1109/CVPR46437.2021.00344
   Zheng A., 2021, VIEWPOINT AWARE PROG
   Zheng Z., 2020, GOING REAL DATA ROBU
   Zhu WJ, 2022, KNOWL-BASED SYST, V235, DOI 10.1016/j.knosys.2021.107624
   Zhu XY, 2020, IEEE COMPUT SOC CONF, P2566, DOI 10.1109/CVPRW50498.2020.00309
   Zhuge C., 2020, ATTRIBUTE GUIDED FEA
NR 38
TC 2
Z9 2
U1 2
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2023
VL 39
IS 8
SI SI
BP 3823
EP 3835
DI 10.1007/s00371-023-03034-2
EA AUG 2023
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA P2DS6
UT WOS:001039684000001
DA 2024-07-18
ER

PT J
AU Jiang, JC
   Lu, XQ
   Ouyang, WL
   Wang, ML
AF Jiang, Jincen
   Lu, Xuequan
   Ouyang, Wanli
   Wang, Meili
TI Unsupervised contrastive learning with simple transformation for 3D
   point cloud data
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Unsupervised contrastive learning; Point cloud; 3D object
   classification; Semantic segmentation
ID NETWORKS
AB Though a number of point cloud learning methods have been proposed to handle unordered points, most of them are supervised and require labels for training. By contrast, unsupervised learning of point cloud data has received much less attention to date. In this paper, we propose a simple yet effective approach for unsupervised point cloud learning. In particular, we identify a very useful transformation which generates a good contrastive version of an original point cloud. They make up a pair. After going through a shared encoder and a shared head network, the consistency between the output representations are maximized with introducing two variants of contrastive losses to respectively facilitate downstream classification and segmentation. To demonstrate the efficacy of our method, we conduct experiments on three downstream tasks which are 3D object classification (on ModelNet40 and ModelNet10), shape part segmentation (on ShapeNet Part dataset) as well as scene segmentation (on S3DIS). Comprehensive results show that our unsupervised contrastive representation learning enables impressive outcomes in object classification and semantic segmentation. It generally outperforms current unsupervised methods, and even achieves comparable performance to supervised methods.
C1 [Jiang, Jincen; Wang, Meili] Northwest A&F Univ, Coll Informat Engn, Xianyang, Peoples R China.
   [Lu, Xuequan] Deakin Univ, Sch Informat Technol, Geelong, Australia.
   [Ouyang, Wanli] Shanghai Artifcial Intelligence Lab, Shanghai, Peoples R China.
C3 Northwest A&F University - China; Deakin University
RP Wang, ML (corresponding author), Northwest A&F Univ, Coll Informat Engn, Xianyang, Peoples R China.
EM jinec@nwsuaf.edu.cn; xuequan.lu@deakin.edu.au;
   wanli.ouyang@sydney.edu.au; wml@nwsuaf.edu.cn
RI ZHU, JIALI/JNE-3065-2023; Lin, Xiaoqi/KFS-5750-2024; cheng,
   chen/JHS-9462-2023; Wang, Xiaojun/JUU-9683-2023; Chen,
   Chao/JHS-6563-2023; Lu, Lu/JPE-5187-2023; Wei, Wei/JVM-8876-2024; wang,
   jiaqi/JSL-7112-2023; Yang, Min/JPY-3791-2023; li, tao/JVO-9006-2024;
   zhang, jt/JVE-1333-2024; Zhang, Chi/JSK-0744-2023; Chen,
   Fang/JZE-4446-2024; Li, YiXue/JRW-6306-2023
OI Wei, Wei/0000-0002-4109-3878; Jiang, Jincen/0000-0002-0150-4644; Lu,
   Xuequan/0000-0003-0959-408X
CR Achlioptas P., 2018, International Conference on Machine Learning, P40
   Armeni I., 2017, arXiv
   Atzmon M, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201301
   Chen C., 2020, IEEE T MULTIMED
   Chen Ting, 2019, 25 AMERICAS C INFORM
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Du B, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P3133, DOI 10.1145/3474085.3475458
   Feng Z., 2020, arXiv
   Fujiwara K., 2020, P IEEE CVF C COMP VI, P11734
   Gadelha M, 2018, LECT NOTES COMPUT SC, V11211, P105, DOI 10.1007/978-3-030-01234-2_7
   Han ZZ, 2019, IEEE I CONF COMP VIS, P10441, DOI 10.1109/ICCV.2019.01054
   Han ZZ, 2019, AAAI CONF ARTIF INTE, P8376
   Hassani K, 2019, IEEE I CONF COMP VIS, P8159, DOI 10.1109/ICCV.2019.00825
   Huang J., 2020, IEEE T MULTIMED
   Huang QG, 2018, PROC CVPR IEEE, P2626, DOI 10.1109/CVPR.2018.00278
   Jiang L, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6403, DOI 10.1109/ICCV48922.2021.00636
   Klokov R, 2017, IEEE I CONF COMP VIS, P863, DOI 10.1109/ICCV.2017.99
   Komarichev A, 2019, PROC CVPR IEEE, P7413, DOI 10.1109/CVPR.2019.00760
   Li JX, 2018, PROC CVPR IEEE, P9397, DOI 10.1109/CVPR.2018.00979
   Li L, 2020, PROC CVPR IEEE, P1916, DOI 10.1109/CVPR42600.2020.00199
   Li YZ, 2018, ADV NEUR IN, V31
   Lin ZH, 2020, PROC CVPR IEEE, P1797, DOI 10.1109/CVPR42600.2020.00187
   Liu H., 2020, IEEE T MULTIMED
   Liu XH, 2019, AAAI CONF ARTIF INTE, P8778
   Liu YC, 2019, PROC CVPR IEEE, P8887, DOI 10.1109/CVPR.2019.00910
   Lu DN, 2020, COMPUT AIDED DESIGN, V125, DOI 10.1016/j.cad.2020.102860
   Lu XQ, 2022, IEEE T VIS COMPUT GR, V28, P1835, DOI 10.1109/TVCG.2020.3026785
   Lu XQ, 2018, IEEE T VIS COMPUT GR, V24, P2315, DOI 10.1109/TVCG.2017.2725948
   Lyu Y., 2020, P IEEE CVF C COMP VI, P12252, DOI DOI 10.1109/CVPR42600.2020.01227
   Qi C., 2017, NeurIPS, P5105, DOI [10.1109/ CVPR.2017.16, DOI 10.1109/CVPR.2017.16]
   Qiu S., 2021, IEEE Trans. Multimed.
   Rao YM, 2020, PROC CVPR IEEE, P5375, DOI 10.1109/CVPR42600.2020.00542
   Riegler G, 2017, PROC CVPR IEEE, P6620, DOI 10.1109/CVPR.2017.701
   Saining Xie, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12348), P574, DOI 10.1007/978-3-030-58580-8_34
   Shen YR, 2018, PROC CVPR IEEE, P4548, DOI 10.1109/CVPR.2018.00478
   Simonovsky M, 2017, PROC CVPR IEEE, P29, DOI 10.1109/CVPR.2017.11
   Su H, 2018, PROC CVPR IEEE, P2530, DOI 10.1109/CVPR.2018.00268
   Su H, 2015, IEEE I CONF COMP VIS, P945, DOI 10.1109/ICCV.2015.114
   Thomas H, 2019, IEEE I CONF COMP VIS, P6420, DOI 10.1109/ICCV.2019.00651
   van den Oord A, 2019, Arxiv, DOI arXiv:1807.03748
   Wang PS, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073608
   Wang SL, 2018, PROC CVPR IEEE, P2589, DOI 10.1109/CVPR.2018.00274
   Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362
   Wu WX, 2019, PROC CVPR IEEE, P9613, DOI 10.1109/CVPR.2019.00985
   Wu ZR, 2015, PROC CVPR IEEE, P1912, DOI 10.1109/CVPR.2015.7298801
   Chang AX, 2015, Arxiv, DOI arXiv:1512.03012
   Xie SN, 2018, PROC CVPR IEEE, P4606, DOI 10.1109/CVPR.2018.00484
   Xu C, 2020, IEEE T MULTIMEDIA, V22, P2234, DOI 10.1109/TMM.2019.2957933
   Xu YF, 2018, LECT NOTES COMPUT SC, V11212, P90, DOI 10.1007/978-3-030-01237-3_6
   Yan X, 2020, PROC CVPR IEEE, P5588, DOI 10.1109/CVPR42600.2020.00563
   Yang YQ, 2018, PROC CVPR IEEE, P206, DOI 10.1109/CVPR.2018.00029
   Yi L, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980238
   Zhang DB, 2021, IEEE T VIS COMPUT GR, V27, P2015, DOI 10.1109/TVCG.2020.3027069
   Zhang M, 2020, IEEE T MULTIMEDIA, V22, P1744, DOI 10.1109/TMM.2019.2963592
   Zhao HS, 2019, PROC CVPR IEEE, P5550, DOI 10.1109/CVPR.2019.00571
   Zhao YH, 2019, PROC CVPR IEEE, P1009, DOI 10.1109/CVPR.2019.00110
   Zhou HY, 2020, IEEE T MULTIMEDIA, V22, P1496, DOI 10.1109/TMM.2019.2943740
NR 57
TC 3
Z9 3
U1 3
U2 11
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2023 JUL 31
PY 2023
DI 10.1007/s00371-023-02921-y
EA JUL 2023
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA N9LK6
UT WOS:001040141100002
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Ha, I
   Chang, HS
   Son, M
   Yoon, SE
AF Ha, Inwoo
   Chang, Hyun Sung
   Son, Minjung
   Yoon, Sung-eui
TI Learning to disentangle latent physical factors of deformable faces
SO VISUAL COMPUTER
LA English
DT Article
DE 3D face; Canonical space; Intrinsic decomposition; Face modeling;
   Deformable object
ID ILLUMINATION; REFLECTANCE
AB We proposed a monocular image disentanglement framework based on a compositional model. Our model disentangles the input image into its constituent components of albedo, depth, deformation, pose, and illumination. Instead of relying on any handcrafted priors, we trained our deep neural network to understand the physical meaning of each element by mimicking real-world operations, allowing it to reconstruct images in a self-supervised manner. Our model, trained on multi-frame images of each subject, demonstrates a better understanding of the objects without requiring any supervision or strong model assumptions. We utilized a deformation-free canonical space to align multi-frame images in the same space. This approach enables the understanding of information from multi-frame images in the same space. Our experiments showed that our approach accurately disentangled the physical elements of deformable faces from images with wide variations found in the wild.
C1 [Ha, Inwoo; Chang, Hyun Sung; Son, Minjung] SAIT Samsung Adv Inst Technol, Suwon, South Korea.
   [Ha, Inwoo; Yoon, Sung-eui] Korea Adv Inst Sci & Technol, Daejeon, South Korea.
C3 Samsung; Korea Advanced Institute of Science & Technology (KAIST)
RP Yoon, SE (corresponding author), Korea Adv Inst Sci & Technol, Daejeon, South Korea.
EM sungeui@kaist.edu
RI Son, Minjung/JPQ-1876-2023
CR Abrevaya VF, 2020, PROC CVPR IEEE, P4978, DOI 10.1109/CVPR42600.2020.00503
   Agrawal P, 2015, IEEE I CONF COMP VIS, P37, DOI 10.1109/ICCV.2015.13
   [Anonymous], 1975, The psychology of computer vision.
   Barron JT, 2015, IEEE T PATTERN ANAL, V37, P1670, DOI 10.1109/TPAMI.2014.2377712
   Barrow H. G., 1978, Computer Vision Systems, P3
   Bell Sean, 2014, ACM Transactions on Graphics, V33, DOI 10.1145/2601097.2601206
   Blanz V, 1999, COMP GRAPH, P187, DOI 10.1145/311535.311556
   Blanz V, 2003, COMPUT GRAPH FORUM, V22, P641, DOI 10.1111/1467-8659.t01-1-00712
   Burkov E., 2020, IEEE C COMPUTER VISI
   Chung JS, 2018, INTERSPEECH, P1086
   Danecek Radek, 2022, P IEEE CVF C COMP VI
   Fan HQ, 2017, PROC CVPR IEEE, P2463, DOI 10.1109/CVPR.2017.264
   Fan QN, 2018, PROC CVPR IEEE, P8944, DOI 10.1109/CVPR.2018.00932
   Feng Y, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459936
   Geiger A, 2011, IEEE INT VEH SYM, P963, DOI 10.1109/IVS.2011.5940405
   Georgoulis S, 2018, IEEE T PATTERN ANAL, V40, P1932, DOI 10.1109/TPAMI.2017.2742999
   Goel S., 2020, EUROPEAN C COMPUTER
   Henderson P., 2018, ARXIV
   Hyeongwoo Kim, 2018, ACM Transactions on Graphics, V37, DOI [10.1145/3197517.3201283, 10.18022/acfco.2018.37.1.001]
   Insafutdinov E, 2018, ADV NEUR IN, V31
   Kanazawa A, 2018, LECT NOTES COMPUT SC, V11219, P386, DOI 10.1007/978-3-030-01267-0_23
   Kato H, 2018, PROC CVPR IEEE, P3907, DOI 10.1109/CVPR.2018.00411
   Kim H, 2018, PROC CVPR IEEE, P4625, DOI 10.1109/CVPR.2018.00486
   Kovacs B, 2017, PROC CVPR IEEE, P850, DOI 10.1109/CVPR.2017.97
   Liu F, 2022, LECT NOTES COMPUT SC, V13661, P497, DOI 10.1007/978-3-031-19769-7_29
   Liu SC, 2019, IEEE I CONF COMP VIS, P7707, DOI 10.1109/ICCV.2019.00780
   Lombardi S, 2016, IEEE T PATTERN ANAL, V38, P129, DOI 10.1109/TPAMI.2015.2430318
   Tran L, 2018, PROC CVPR IEEE, P7346, DOI 10.1109/CVPR.2018.00767
   Meka A, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3323027
   Meka A, 2018, PROC CVPR IEEE, P6315, DOI 10.1109/CVPR.2018.00661
   Mobahi H., 2014, P IEEE C COMP VIS PA
   Nestmeyer Thomas, 2020, P IEEE CVF C COMP VI
   Novotny D, 2017, IEEE I CONF COMP VIS, P5228, DOI 10.1109/ICCV.2017.558
   Ondrúska P, 2015, IEEE T VIS COMPUT GR, V21, P1251, DOI 10.1109/TVCG.2015.2459902
   Pan Xingang, 2021, INT C LEARNING REPRE
   Paysan P, 2009, AVSS: 2009 6TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE, P296, DOI 10.1109/AVSS.2009.58
   Ramamoorthi R, 2001, COMP GRAPH, P497, DOI 10.1145/383259.383317
   Sengupta S, 2018, PROC CVPR IEEE, P6296, DOI 10.1109/CVPR.2018.00659
   Shang J., 2020, ARXIV
   Shu ZX, 2018, LECT NOTES COMPUT SC, V11214, P664, DOI 10.1007/978-3-030-01249-6_40
   Shu ZX, 2017, PROC CVPR IEEE, P5444, DOI 10.1109/CVPR.2017.578
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sun TC, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3323008
   Tewari A, 2019, PROC CVPR IEEE, P10804, DOI 10.1109/CVPR.2019.01107
   Tewari A, 2017, IEEE INT CONF COMP V, P1274, DOI 10.1109/ICCVW.2017.153
   Tran L, 2021, IEEE T PATTERN ANAL, V43, P157, DOI 10.1109/TPAMI.2019.2927975
   Tulsiani S, 2017, PROC CVPR IEEE, P1466, DOI 10.1109/CVPR.2017.160
   Tulsiani Shubham, 2018, CVPR, DOI DOI 10.1109/CVPR.2018.00306
   Ummenhofer B, 2017, PROC CVPR IEEE, P5622, DOI 10.1109/CVPR.2017.596
   Wang CY, 2018, PROC CVPR IEEE, P2022, DOI 10.1109/CVPR.2018.00216
   Wen Y., 2021, P IEEE CVF INT C COM, p13,289
   Wiles Olivia, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7465, DOI 10.1109/CVPR42600.2020.00749
   WOODHAM RJ, 1980, OPT ENG, V19, P139, DOI 10.1117/12.7972479
   Wu SZ, 2020, PROC CVPR IEEE, P1, DOI 10.1109/CVPR42600.2020.00008
   Yan XC, 2016, ADV NEUR IN, V29
   Zakharov E, 2019, IEEE I CONF COMP VIS, P9458, DOI 10.1109/ICCV.2019.00955
   Zhang R, 1999, IEEE T PATTERN ANAL, V21, P690, DOI 10.1109/34.784284
   Zhang Y, 2016, IEEE T IMAGE PROCESS, V25, DOI 10.1109/TIP.2016.2549360
   Zhang Z., 2022, P IEEE CVF C COMP VI, p20,353
   Zhou H, 2019, IEEE I CONF COMP VIS, P7193, DOI 10.1109/ICCV.2019.00729
   Zhou TH, 2017, PROC CVPR IEEE, P6612, DOI 10.1109/CVPR.2017.700
NR 61
TC 0
Z9 0
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2023
VL 39
IS 8
SI SI
BP 3481
EP 3494
DI 10.1007/s00371-023-02948-1
EA JUL 2023
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA P2DS6
UT WOS:001032577400003
OA Bronze
DA 2024-07-18
ER

PT J
AU Liang, ZQ
   Xiao, G
   Hu, JQ
   Wang, JS
   Ding, CS
AF Liang, Zhenqi
   Xiao, Gang
   Hu, Jianqiu
   Wang, Jingshi
   Ding, Chunshan
TI MotionTrack: rethinking the motion cue for multiple object tracking in
   USV videos
SO VISUAL COMPUTER
LA English
DT Article
DE Multiple object tracking; Unmanned surface vehicle; Motion cue; Deep
   learning
AB Multiple object tracking (MOT) in unmanned surface vehicle (USV) videos has many application scenarios in the military and civilian fields. State-of-the-art MOT methods first extract a set of detections from the video frames, then utilize IoU distance to associate the detections of current frame and tracklets of last frame, and finally adopt linear Kalman filter to estimate the current position of tracklets. However, some problems in USV videos seriously affect the tracking performance, such as low frame rate, wobble of observation platform, nonlinear motion of objects, small objects and ambiguous appearance. In this paper, we fully explore the motion cue in USV videos and propose a simple but effective tracker, named MotionTrack. Equipping with YOLOv7 as object detector, the data association of MotionTrack is mainly composed of cascade matching with Gaussian distance module and observation-centric Kalman filter module. We validate the effectiveness with extensive experiments on the recent Jari-Maritime-Tracking-2022 dataset, achieving new state-of-the-art 46.9 MOTA, 49.2 IDF1 with 35.2 FPS running speed on a single 3090 GPU. The source code, pretrained models with deploy versions are released at .
C1 [Liang, Zhenqi; Xiao, Gang; Wang, Jingshi] Shanghai Jiao Tong Univ, Sch Aeronaut & Astronaut, Shanghai 200240, Peoples R China.
   [Hu, Jianqiu; Wang, Jingshi; Ding, Chunshan] Jiangsu Automat Res Inst, Lianyungang 222061, Peoples R China.
C3 Shanghai Jiao Tong University
RP Xiao, G (corresponding author), Shanghai Jiao Tong Univ, Sch Aeronaut & Astronaut, Shanghai 200240, Peoples R China.
EM xiaogang@sjtu.edu.cn
FU National Program on Key Basic Research Project [2014CB744903]; National
   Natural Science Foundation of China [61673270]; Artificial Intelligence
   Key Laboratory of Sichuan Province [2022RZY02]
FX This work was supported by National Program on Key Basic Research
   Project (2014CB744903), National Natural Science Foundation of China
   (61673270), and Artificial Intelligence Key Laboratory of Sichuan
   Province (2022RZY02). We thank the Jiangsu Automation Research Institute
   for providing the challenging and well-annotated JMT2022 dataset.
CR [Anonymous], 2022, DESIGNING NETWORK DE
   Bar-Shalom Y, 2009, IEEE CONTR SYST MAG, V29, P82, DOI 10.1109/MCS.2009.934469
   Bernardin K, 2008, EURASIP J IMAGE VIDE, DOI 10.1155/2008/246309
   Bewley A, 2016, IEEE IMAGE PROC, P3464, DOI 10.1109/ICIP.2016.7533003
   Bochkovskiy A, 2020, ARXIV PREPRINT ARXIV, DOI DOI 10.48550/ARXIV.2004.10934
   Cao JK, 2023, PROC CVPR IEEE, P9686, DOI 10.1109/CVPR52729.2023.00934
   Chen GC, 2022, VISUAL COMPUT, V38, P1051, DOI 10.1007/s00371-021-02067-9
   Chen LJ, 2018, 2018 3RD INTERNATIONAL CONFERENCE ON SYSTEM RELIABILITY AND SAFETY (ICSRS), P1, DOI [10.1109/ICSRS.2018.8688869, 10.1109/ICSRS.2018.00009]
   Ge Z., 2021, ARXIV, DOI DOI 10.48550/ARXIV.2107.08430
   github.com, About us
   Guo ZB, 2023, VISUAL COMPUT, V39, P4267, DOI 10.1007/s00371-022-02589-w
   Kalman R E., 1960, J BASIC ENG, V82, P35, DOI DOI 10.1115/1.3662552
   Kuhn HW, 2005, NAV RES LOG, V52, P7, DOI 10.1002/nav.20053
   Lehmann EL., 1998, THEORY POINT ESTIMAT, DOI 10.1007/b98854
   Liang C, 2022, AAAI CONF ARTIF INTE, P1546
   Liang C, 2022, IEEE T IMAGE PROCESS, V31, P3182, DOI 10.1109/TIP.2022.3165376
   Liang ZQ, 2022, CHIN OPT LETT, V20, DOI 10.3788/COL202220.081101
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Micikevicius Paulius, 2018, 6 INT C LEARNING REP
   Pang JM, 2021, PROC CVPR IEEE, P164, DOI 10.1109/CVPR46437.2021.00023
   Patel AS, 2023, VISUAL COMPUT, V39, P2127, DOI 10.1007/s00371-022-02469-3
   Ramanan D., 2003, 2003 IEEE COMP SOC C, V2, DOI [10.1007/10.1109/CVPR.2003.1211504, DOI 10.1007/10.1109/CVPR.2003.1211504]
   Redmon J, 2018, Arxiv, DOI [arXiv:1804.02767, DOI 10.1109/CVPR.2017.690, DOI 10.48550/ARXIV.1804.02767]
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Ristani E, 2016, LECT NOTES COMPUT SC, V9914, P17, DOI 10.1007/978-3-319-48881-3_2
   Schuster-Bockler Benjamin, 2007, Curr Protoc Bioinformatics, VAppendix 3, p3A, DOI 10.1002/0471250953.bia03as18
   Voigtlaender Paul, 2019, P IEEE CVF C COMP VI, P7942, DOI DOI 10.1109/CVPR.2019.00813
   Wang CY, 2023, PROC CVPR IEEE, P7464, DOI 10.1109/CVPR52729.2023.00721
   Wang Z., 2020, COMPUTER VISION ECCV, P107, DOI [10.1007/978-3-030-58621-8_7, DOI 10.1007/978-3-030-58621-8_7]
   Wojke N, 2017, IEEE IMAGE PROC, P3645, DOI 10.1109/ICIP.2017.8296962
   Zhang XQ, 2021, VISUAL COMPUT, V37, P1089, DOI 10.1007/s00371-020-01854-0
   Zhang YF, 2022, LECT NOTES COMPUT SC, V13682, P1, DOI 10.1007/978-3-031-20047-2_1
   Zhang YF, 2021, INT J COMPUT VISION, V129, P3069, DOI 10.1007/s11263-021-01513-4
NR 33
TC 3
Z9 3
U1 10
U2 20
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2024
VL 40
IS 4
BP 2761
EP 2773
DI 10.1007/s00371-023-02983-y
EA JUL 2023
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA MZ2U3
UT WOS:001025875300001
DA 2024-07-18
ER

PT J
AU Zhang, C
AF Zhang, Chao
TI ExtremeFormer: a new framework for accurate object tracking by designing
   an efficient head prediction module
SO VISUAL COMPUTER
LA English
DT Article
DE Single-object tracking; Transformer; Edge prediction; ExtremeFormer
ID AUTOMATIC SEGMENTATION; LAYER SEGMENTATION; RETINAL LAYER; OCT IMAGES;
   MACULAR DEGENERATION; COHERENCE; AMD; FEATURES; SET
AB Object tracking has become a crucial area of research in the field of intelligent perception in recent years. Current mainstream single-object trackers utilize the method of point regression and heatmaps to predict the position of the target. However, the performance of these models can be negatively impacted by occlusions of key points. To address this issue, we introduce the ExtremeFormer model, which includes a backbone similar to OSTrack and an ENM (Extreme Net Module) head. Our core idea is to use the ENM module to predict the target's position by regressing the position of the edges instead of the points. Different from traditional center-based regression methods, ENM predicts the left, top, right, and bottom boundaries of the target bounding box through the network and uses an offset branch to compensate for errors caused by resolution reduction. This approach greatly alleviates the problem of tracking failure caused by occlusion of the center point and improves the robustness of the tracking model. In addition, our tracker does not require Hanning windows or penalties to ensure stability during tracking. Our final ExtremeFormer model outperforms existing state-of-the-art trackers on four tracking benchmarks, including LaSOT, TrackingNet, GOT-10k, and UAV123. Specifically, our ExtremeFormer-384 achieves a Precision score of 83.1% on TrackingNet, 74.9% on LaSOT, and an AO of 73.9% on GOT-10k. These results demonstrate the effectiveness of our proposed model, which provides a more robust and accurate approach for single-object tracking in challenging environments.
C1 [Zhang, Chao] Beihang Univ, Comp Sci & Technol, XueYuan Rd 37, Beijing 100191, Peoples R China.
C3 Beihang University
RP Zhang, C (corresponding author), Beihang Univ, Comp Sci & Technol, XueYuan Rd 37, Beijing 100191, Peoples R China.
EM chaozlimex@gmail.com
OI Zhang, Chao/0000-0001-6076-836X
CR Amirkhani A, 2022, VISUAL COMPUT, V38, P1929, DOI 10.1007/s00371-021-02256-6
   An FP, 2022, VISUAL COMPUT, V38, P541, DOI 10.1007/s00371-020-02033-x
   Bertinetto L, 2016, LECT NOTES COMPUT SC, V9914, P850, DOI 10.1007/978-3-319-48881-3_56
   Bhat Goutam, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12368), P205, DOI 10.1007/978-3-030-58592-1_13
   Bhat G, 2019, IEEE I CONF COMP VIS, P6181, DOI 10.1109/ICCV.2019.00628
   Chen GC, 2022, VISUAL COMPUT, V38, P1051, DOI 10.1007/s00371-021-02067-9
   Chen ZD, 2020, PROC CVPR IEEE, P6667, DOI 10.1109/CVPR42600.2020.00670
   Danelljan M, 2019, PROC CVPR IEEE, P4655, DOI 10.1109/CVPR.2019.00479
   Danelljan M, 2017, PROC CVPR IEEE, P6931, DOI 10.1109/CVPR.2017.733
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Dong XP, 2021, IEEE T PATTERN ANAL, V43, P1515, DOI 10.1109/TPAMI.2019.2956703
   Dong XP, 2017, IEEE T MULTIMEDIA, V19, P763, DOI 10.1109/TMM.2016.2631884
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Duan KW, 2019, Arxiv, DOI arXiv:1904.08189
   Fan H, 2019, PROC CVPR IEEE, P5369, DOI 10.1109/CVPR.2019.00552
   Guo DY, 2020, PROC CVPR IEEE, P6268, DOI 10.1109/CVPR42600.2020.00630
   He K., 2021, Masked autoencoders are scalable vision learners
   Huang LH, 2021, IEEE T PATTERN ANAL, V43, P1562, DOI 10.1109/TPAMI.2019.2957464
   Jiang BR, 2018, LECT NOTES COMPUT SC, V11218, P816, DOI 10.1007/978-3-030-01264-9_48
   Law H, 2020, INT J COMPUT VISION, V128, P642, DOI 10.1007/s11263-019-01204-1
   Li B, 2019, PROC CVPR IEEE, P4277, DOI 10.1109/CVPR.2019.00441
   Li B, 2018, PROC CVPR IEEE, P8971, DOI 10.1109/CVPR.2018.00935
   Li ZC, 2023, IEEE T NEUR NET LEAR, DOI 10.1109/TNNLS.2023.3240195
   Lin LT, 2022, Arxiv, DOI arXiv:2112.00995
   Lin M., 2014, C TRACK P
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Loshchilov I., 2019, DECOUPLED WEIGHT DEC
   Ma F, 2022, PROC CVPR IEEE, P8771, DOI 10.1109/CVPR52688.2022.00858
   Mayer C, 2021, arXiv
   Mayer C. etal, 2022, PROC CVPR IEEE, P8731, DOI DOI 10.1109/CVPR52688.2022.00853
   Müller M, 2018, LECT NOTES COMPUT SC, V11205, P310, DOI 10.1007/978-3-030-01246-5_19
   Mueller M, 2016, LECT NOTES COMPUT SC, V9905, P445, DOI 10.1007/978-3-319-46448-0_27
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rezatofighi H, 2019, PROC CVPR IEEE, P658, DOI 10.1109/CVPR.2019.00075
   Tang H, 2022, PATTERN RECOGN, V130, DOI 10.1016/j.patcog.2022.108792
   Tang H, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P610, DOI 10.1145/3394171.3413884
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang D, 2023, INFORM FUSION, V98, DOI 10.1016/j.inffus.2023.101828
   Wang Q, 2019, PROC CVPR IEEE, P1328, DOI 10.1109/CVPR.2019.00142
   Xu YD, 2020, AAAI CONF ARTIF INTE, V34, P12549
   Yan B, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P10428, DOI 10.1109/ICCV48922.2021.01028
   Ye BT, 2022, LECT NOTES COMPUT SC, V13682, P341, DOI 10.1007/978-3-031-20047-2_20
   Yin JB, 2020, PROC CVPR IEEE, P6767, DOI 10.1109/CVPR42600.2020.00680
   Yu J., 2016, P 24 ACM INT C MULT, P516, DOI DOI 10.1145/2964284.2967274
   Zha ZC, 2023, IEEE T CIRC SYST VID, V33, P3947, DOI 10.1109/TCSVT.2023.3236636
   Zhao MJ, 2021, Arxiv, DOI arXiv:2105.03817
   Zheng Z., 2019, IEEE T CYBERN, V52, P8574
   Zheng ZH, 2022, IEEE T CYBERNETICS, V52, P8574, DOI 10.1109/TCYB.2021.3095305
   Zhou XY, 2019, PROC CVPR IEEE, P850, DOI 10.1109/CVPR.2019.00094
NR 50
TC 2
Z9 2
U1 2
U2 8
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2024
VL 40
IS 4
BP 2961
EP 2974
DI 10.1007/s00371-023-02997-6
EA JUL 2023
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA MZ2U3
UT WOS:001025875300003
DA 2024-07-18
ER

PT J
AU Nader, N
   EL-Gamal, FEA
   Elmogy, M
AF Nader, Nermeen
   EL-Gamal, Fatma EL-Zahraa A.
   Elmogy, Mohammed
TI Enhanced kinship verification analysis based on color and texture
   handcrafted techniques
SO VISUAL COMPUTER
LA English
DT Article
DE Kinship verification; Heterogeneous auto-similarities of characteristics
   (HASC); Color correlogram (CC); Dense color histogram (DCH); Ensemble
   classifier; Color models
ID CLASSIFICATION
AB Nowadays, kinship verification is an attractive research area within computer vision. It significantly affects applications in the real world, such as finding missing individuals and forensics. Despite the importance of this research topic, it still faces many challenges, such as low accuracy and illumination variations. Due to the existence of different classes of feature extraction techniques, different types of information can be extracted from the input data. Moreover, the fusion power produces complementary information that can address kinship verification problems. Therefore, this paper proposes a new approach for verifying kinship by fusing features from different perspectives, including color-texture and color features in different color spaces. Besides using promising methods in the field, such as local binary pattern (LBP) and scale-invariant feature transform (SIFT), the paper utilizes other feature extraction methods, which are heterogeneous auto-similarities of characteristics (HASC), color correlogram (CC), and dense color histogram (DCH). As far as we know, these features haven't been employed before in this research area. Accordingly, the proposed approach goes into six stages: preprocessing, feature extraction, feature normalization, feature fusion, feature representation, and kinship verification. The proposed approach was evaluated on the KinFaceW-I and KinFaceW-II field standard datasets, achieving maximum accuracy of 79.54% and 90.65%, respectively. Compared with many state-of-the-art approaches, the results of the proposed approach reflect the promising achievements and encourage the authors to plan for future enhancement.
C1 [Nader, Nermeen; EL-Gamal, Fatma EL-Zahraa A.; Elmogy, Mohammed] Mansoura Univ, Fac Comp & Informat, Informat Technol Dept, Mansoura 35516, Egypt.
C3 Egyptian Knowledge Bank (EKB); Mansoura University
RP Elmogy, M (corresponding author), Mansoura Univ, Fac Comp & Informat, Informat Technol Dept, Mansoura 35516, Egypt.
EM nermeennader@mans.edu.eg; fatma-zahraa@mans.edu.eg; melmogy@mans.edu.eg
RI Elmogy, Mohammed/E-3428-2018
OI Elmogy, Mohammed/0000-0002-2504-6051
FU Science, Technology & Innovation Funding Authority (STDF); Egyptian
   Knowledge Bank (EKB)
FX Open access funding provided by The Science, Technology & Innovation
   Funding Authority (STDF) in cooperation with The Egyptian Knowledge Bank
   (EKB).
CR Alvergne A, 2009, J VISION, V9, DOI 10.1167/9.6.23
   [Anonymous], 2005, Graphicon
   [Anonymous], 2012, Int. J. Comput. Sci. Telecommun
   [Anonymous], 2014, INT J ENG RES TECHNO
   Armi L., 2019, ARXIV
   Ben Fredj H, 2021, VISUAL COMPUT, V37, P217, DOI 10.1007/s00371-020-01794-9
   Bessaoudi M, 2021, APPL INTELL, V51, P3534, DOI 10.1007/s10489-020-02044-0
   Biagio MS, 2013, IEEE I CONF COMP VIS, P809, DOI 10.1109/ICCV.2013.105
   Biswas S., 2011, Proc. IEEE International Workshop on Information Forensics and Security (WIFS), P1, DOI DOI 10.1109/WIFS.2011.6123126
   Chavolla E, 2018, STUD COMPUT INTELL, V730, P3, DOI 10.1007/978-3-319-63754-9_1
   Chen FQ, 2015, 2015 12TH INTERNATIONAL COMPUTER CONFERENCE ON WAVELET ACTIVE MEDIA TECHNOLOGY AND INFORMATION PROCESSING (ICCWAMTIP), P214, DOI 10.1109/ICCWAMTIP.2015.7493978
   Chen XJ, 2017, MULTIMED TOOLS APPL, V76, P4105, DOI 10.1007/s11042-015-2930-9
   Chen YT, 2023, INT J MACH LEARN CYB, V14, P2945, DOI 10.1007/s13042-023-01811-y
   Chen YT, 2023, J VIS COMMUN IMAGE R, V91, DOI 10.1016/j.jvcir.2023.103776
   Chen YT, 2024, VISUAL COMPUT, V40, P489, DOI 10.1007/s00371-023-02795-0
   Chen YT, 2021, APPL INTELL, V51, P4367, DOI 10.1007/s10489-020-02116-1
   Chen YT, 2021, MULTIMED TOOLS APPL, V80, P30839, DOI 10.1007/s11042-020-09969-1
   Chen YT, 2021, J AMB INTEL HUM COMP, DOI 10.1007/s12652-020-02778-2
   Chergui A, 2018, IEEE HAMMAMET TUNISI, P1
   Chua T.-S., 2009, P ACM INT C IM VID R, P1
   Dehshibi MM, 2019, VISUAL COMPUT, V35, P23, DOI 10.1007/s00371-017-1442-1
   Dibeklioglu H, 2012, LECT NOTES COMPUT SC, V7574, P525, DOI 10.1007/978-3-642-33712-3_38
   Dornaika F, 2020, NEURAL COMPUT APPL, V32, P7139, DOI 10.1007/s00521-019-04201-0
   Duan QY, 2017, IEEE INT CONF COMP V, P1590, DOI 10.1109/ICCVW.2017.187
   Fang RG, 2010, IEEE IMAGE PROC, P1577, DOI 10.1109/ICIP.2010.5652590
   Fraz MM, 2012, IEEE T BIO-MED ENG, V59, P2538, DOI 10.1109/TBME.2012.2205687
   Goyal Aarti, 2019, Cognitive Informatics and Soft Computing. Proceeding of CISC 2017. Advances in Intelligent Systems and Computing (AISC 768), P371, DOI 10.1007/978-981-13-0617-4_37
   Guo GD, 2012, IEEE T INSTRUM MEAS, V61, P2322, DOI 10.1109/TIM.2012.2187468
   Guo L, 2012, PHYSCS PROC, V25, P1528, DOI 10.1016/j.phpro.2012.03.272
   Gupta S, 2021, VISUAL COMPUT, V37, P447, DOI 10.1007/s00371-020-01814-8
   Harpending H, 2002, POPUL ENVIRON, V24, P141, DOI 10.1023/A:1020815420693
   Hu JL, 2019, IEEE IMAGE PROC, P1178, DOI [10.1109/ICIP.2019.8803754, 10.1109/icip.2019.8803754]
   Jalal AS, 2023, VISUAL COMPUT, V39, P2609, DOI 10.1007/s00371-022-02482-6
   Joblove G.H., 1978, COMPUT GRAPHICS-US, V12, P20, DOI [DOI 10.1145/965139.807362, 10.1145/800248.807362]
   Arachchilage SPKW, 2020, EURASIP J IMAGE VIDE, V2020, DOI 10.1186/s13640-020-00510-w
   Kabbai L, 2015, 2015 IEEE 12 INT MUL, P1
   Kaya Y, 2014, VISUAL COMPUT, V30, P71, DOI 10.1007/s00371-013-0782-8
   Kou L, 2015, MATH PROBL ENG, V2015, DOI 10.1155/2015/472473
   Laiadi O, 2021, INT J MACH LEARN CYB, V12, P171, DOI 10.1007/s13042-020-01163-x
   Laiadi O, 2020, NEUROCOMPUTING, V377, P286, DOI 10.1016/j.neucom.2019.10.055
   Li YY, 2017, ADV SOC SCI EDUC HUM, V159, P13
   Liang JQ, 2019, IEEE T IMAGE PROCESS, V28, P1149, DOI 10.1109/TIP.2018.2875346
   Ligang Zhang, 2011, Proceedings of the 2011 International Conference on Digital Image Computing: Techniques and Applications (DICTA 2011), P620, DOI 10.1109/DICTA.2011.110
   Liu F, 2022, MATHEMATICS-BASEL, V10, DOI 10.3390/math10030480
   Liu XC, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER AND COMMUNICATIONS (ICCC), P1, DOI 10.1109/CompComm.2015.7387529
   Lopez MB, 2018, MACH VISION APPL, V29, P873, DOI 10.1007/s00138-018-0943-x
   Lu JW, 2014, IEEE T PATTERN ANAL, V36, P331, DOI 10.1109/TPAMI.2013.134
   MUKHERJEE M, 2019, INT CONF COMPUT, pNI460, DOI DOI 10.1109/icccnt45670.2019.8944489
   Nader N, 2021, PEERJ COMPUT SCI, V7, DOI 10.7717/peerj-cs.735
   Nanni L, 2020, J IMAGING, V6, DOI 10.3390/jimaging6120143
   Polikar R., 2006, IEEE Circuits and Systems Magazine, V6, P21, DOI 10.1109/MCAS.2006.1688199
   Qin XQ, 2022, MULTIMED TOOLS APPL, V81, P11049, DOI 10.1007/s11042-022-12032-w
   Rachmadi RF, 2021, 2020 IEEE INTERNATIONAL CONFERENCE ON INTERNET OF THINGS AND INTELLIGENCE SYSTEM (IOTAIS), P123, DOI 10.1109/IoTaIS50849.2021.9359720
   Rasool RA, 2021, APPL COMPUT INTELL S, V2021, DOI 10.1155/2021/6621772
   Rattani A, 2007, 2007 FIRST IEEE INTERNATIONAL CONFERENCE ON BIOMETRICS: THEORY, APPLICATIONS AND SYSTEMS, P85
   Rehman A., 2019, 2019 INT S RECENT AD, V4, P1, DOI DOI 10.1109/RAEE.2019.8886969
   Rehman B, 2020, VISUAL COMPUT, V36, P633, DOI 10.1007/s00371-019-01649-y
   Saravanan G, 2016, 2016 INTERNATIONAL CONFERENCE ON COMMUNICATION AND SIGNAL PROCESSING (ICCSP), VOL. 1, P462, DOI 10.1109/ICCSP.2016.7754179
   Sellam A, 2020, MULTIMED TOOLS APPL, V79, P20861, DOI 10.1007/s11042-020-08906-6
   Shahbahrami A., 2008, P 19 ANN WORKSH CIRC
   Shu XB, 2016, PATTERN RECOGN, V59, P156, DOI 10.1016/j.patcog.2015.12.015
   Song CH, 2020, IEEE INT CON MULTI, DOI 10.1109/icme46284.2020.9102891
   Van TN, 2019, 2019 26TH INTERNATIONAL CONFERENCE ON TELECOMMUNICATIONS (ICT), P376, DOI [10.1109/ict.2019.8798781, 10.1109/ICT.2019.8798781]
   Tuzel O, 2008, IEEE T PATTERN ANAL, V30, P1713, DOI 10.1109/TPAMI.2008.75
   Tuzel O, 2006, LECT NOTES COMPUT SC, V3952, P589
   Vinayak V., 2017, INT J COMPUT APPL, V161, P1, DOI DOI 10.5120/IJCA2017913282
   Wang M, 2018, ARXIV
   Wang MY, 2020, PATTERN RECOGN, V105, DOI 10.1016/j.patog.2020.10732
   Wang SW, 2020, PATTERN RECOGN LETT, V138, P38, DOI 10.1016/j.patrec.2020.06.019
   Wei Wang, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12367), P613, DOI 10.1007/978-3-030-58542-6_37
   Wei ZQ, 2019, IEEE ACCESS, V7, P100029, DOI 10.1109/ACCESS.2019.2929939
   Wu HS, 2021, J VIS COMMUN IMAGE R, V79, DOI 10.1016/j.jvcir.2021.103265
   Wu XT, 2018, 2018 IEEE 3RD INTERNATIONAL CONFERENCE ON SIGNAL AND IMAGE PROCESSING (ICSIP), P187, DOI 10.1109/SIPROCESS.2018.8600423
   Xia RL, 2022, J KING SAUD UNIV-COM, V34, P6008, DOI 10.1016/j.jksuci.2022.02.004
   Xia SY, 2012, IEEE T MULTIMEDIA, V14, P1046, DOI 10.1109/TMM.2012.2187436
   Xiaojun Wu, 2016, 2016 13th International Conference on Service Systems and Service Management (ICSSSM), P1, DOI 10.1109/ICSSSM.2016.7538581
   Xu M, 2016, MATH PROBL ENG, V2016, DOI 10.1155/2016/4072323
   Xu Yunhao, 2022, Biometric Recognition: 16th Chinese Conference, CCBR 2022, Proceedings. Lecture Notes in Computer Science (13628), P197, DOI 10.1007/978-3-031-20233-9_20
   Yan HB, 2021, PATTERN RECOGN, V110, DOI 10.1016/j.patcog.2020.107541
   Yan HB, 2017, IMAGE VISION COMPUT, V60, P91, DOI 10.1016/j.imavis.2016.08.009
   Yan HB, 2015, IEEE T CYBERNETICS, V45, P2535, DOI 10.1109/TCYB.2014.2376934
   Yan HB, 2014, IEEE T INF FOREN SEC, V9, P1169, DOI 10.1109/TIFS.2014.2327757
   Yu LS, 2021, J MICROBIOL IMMUNOL, V54, P963, DOI 10.1016/j.jmii.2020.08.003
   Zekrini Fatima, 2022, Artificial Intelligence and Its Applications: Proceeding of the 2nd International Conference on Artificial Intelligence and Its Applications (2021). Lecture Notes in Networks and Systems (413), P486, DOI 10.1007/978-3-030-96311-8_45
   Zhang1 2.K., 2015, BRIT MACHINE VISION
   Zhao R, 2013, PROC CVPR IEEE, P3586, DOI 10.1109/CVPR.2013.460
   Zhou XZ, 2016, INFORM FUSION, V32, P40, DOI 10.1016/j.inffus.2015.08.006
   Zhou XZ, 2016, NEUROCOMPUTING, V197, P136, DOI 10.1016/j.neucom.2016.02.039
   Zhou Xiuzhuang, 2012, P 20 ACM INT C MULT, P725, DOI [DOI 10.1145/2393347, DOI 10.1145/2393347.2396297]
   Zimei Li, 2020, Advances in Intelligent Information Hiding and Multimedia Signal Processing. Proceedings of the 15th International Conference on IIH-MSP in conjunction with the 12th International Conference on FITAT. Smart Innovation, Systems and Technologies (SIST 157), P333, DOI 10.1007/978-981-13-9710-3_35
NR 90
TC 0
Z9 0
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2024
VL 40
IS 4
BP 2325
EP 2346
DI 10.1007/s00371-023-02919-6
EA JUN 2023
PG 22
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA MZ2U3
UT WOS:001005826400001
OA hybrid, Green Submitted
DA 2024-07-18
ER

PT J
AU Peng, WR
   Chen, HJ
   Li, YF
   Sun, J
AF Peng, Wanru
   Chen, Houjin
   Li, Yanfeng
   Sun, Jia
TI MCFR: multi-confidence contrastive learning with feature refined for
   unsupervised person re-identification
SO VISUAL COMPUTER
LA English
DT Article
DE Person re-identification; Unsupervised learning; Contrastive learning;
   Feature refined block
AB Unsupervised person re-identification (re-ID) aims to identify the same person in unsupervised settings, which is a realistic and challenging problem due to getting rid of the dependence on annotations. Existing pseudo-label-based methods have been proved to be effective by discriminative representation learning and high-quality pseudo labels. However, there are some undesirable similarity structures in images affected by different states (e.g., backgrounds, occlusions, or other target confusions) which may generate noisy labels and lead to model degradation. To tackle the above issues, this paper analyzes the key factors for unsupervised re-ID performance from two perspectives: network structure and loss function. Specifically, we propose a multi-confidence contrastive learning with feature refinement (MCFR) model which can enhance the feature discriminative ability and meanwhile alleviate the impact of noisy data. For network architecture, a feature refined block is embedded into a multi-branch network, constructing FR-net. For loss function, a multi-confidence contrastive loss (MCCL) based on similarity and confidence relationship is developed. MCCL can flexibly update the list of hard negative samples in clusters and outliers, promoting the model by learning from confident samples. Extensive experiments including visualizations and ablation studies validate the effectiveness of each component in the MCFR model. Additionally, compared with the state-of-the-art unsupervised re-ID methods, our method achieves considerable performance on three benchmark datasets.
C1 [Peng, Wanru; Chen, Houjin; Li, Yanfeng; Sun, Jia] Beijing Jiaotong Univ, Sch Elect & Informat Engn, Beijing 100044, Peoples R China.
C3 Beijing Jiaotong University
RP Chen, HJ (corresponding author), Beijing Jiaotong Univ, Sch Elect & Informat Engn, Beijing 100044, Peoples R China.
EM hjchen@bjtu.edu.cn
FU National Natural Science Foundation of China [62172029, 61872030]
FX AcknowledgementsThis work was supported by the National Natural Science
   Foundation of China (No. 62172029 and No. 61872030).
CR Caron M., 2020, 34 C NEURAL INFORM P
   Caron M, 2019, IEEE I CONF COMP VIS, P2959, DOI 10.1109/ICCV.2019.00305
   Caron M, 2018, LECT NOTES COMPUT SC, V11218, P139, DOI 10.1007/978-3-030-01264-9_9
   Chang WG, 2019, PROC CVPR IEEE, P7346, DOI 10.1109/CVPR.2019.00753
   Chen H, 2020, ARXIV
   Chen H, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14940, DOI 10.1109/ICCV48922.2021.01469
   Chen Ting, 2019, 25 AMERICAS C INFORM
   Chen YB, 2019, IEEE I CONF COMP VIS, P232, DOI 10.1109/ICCV.2019.00032
   Cheng D, 2022, IEEE T IMAGE PROCESS, V31, P3334, DOI 10.1109/TIP.2022.3169693
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dongkai Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10978, DOI 10.1109/CVPR42600.2020.01099
   Fan HH, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3243316
   Ge Y., 2020, ADV NEURAL INFORM PR
   Ge Y., 2020, ARXIV
   Guo Y., 2022, IEEE C COMP VIS PATT
   Gutmann Michael, 2010, P MACHINE LEARNING R, P297, DOI DOI 10.1145/3292500.3330651
   Han X. -F., 2021, ARXIV
   Hao C., 2021, IEEE C COMP VIS PATT
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Jianing Li, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12369), P483, DOI 10.1007/978-3-030-58586-0_29
   Junnan L., 2021, INT C LEARNING REPRE
   Kaiwei Zeng, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13654, DOI 10.1109/CVPR42600.2020.01367
   Kalantidis Y., 2020, ARXIV
   Li M., 2021, ARXIV
   Li Y., 2021, C ART INT AAAI
   Lin Shan, 2018, 2018 IEEE 36 VLSI TE
   Lin X.V., 2021, arXiv
   Lin YT, 2020, PROC CVPR IEEE, P3387, DOI 10.1109/CVPR42600.2020.00345
   Lin YT, 2019, AAAI CONF ARTIF INTE, P8738
   Liu XB, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P547, DOI 10.1145/3394171.3413904
   Ristani E, 2016, LECT NOTES COMPUT SC, V9914, P17, DOI 10.1007/978-3-319-48881-3_2
   Robinson Joshua, 2020, INT C LEARN REPR
   Tian Y, 2020, ELECTROPHORESIS, V41, P1491, DOI 10.1002/elps.201900322
   van den Oord Aaron, 2018, Advances in neural information processing systems
   Wang DK, 2022, INT J COMPUT VISION, V130, P2924, DOI 10.1007/s11263-022-01680-y
   Wang F, 2021, PROC CVPR IEEE, P2495, DOI 10.1109/CVPR46437.2021.00252
   Wang XD, 2021, PROC CVPR IEEE, P12581, DOI 10.1109/CVPR46437.2021.01240
   Wang X, 2020, PROC CVPR IEEE, P6387, DOI 10.1109/CVPR42600.2020.00642
   Wu ZR, 2018, PROC CVPR IEEE, P3733, DOI 10.1109/CVPR.2018.00393
   Yang FX, 2021, PROC CVPR IEEE, P4853, DOI 10.1109/CVPR46437.2021.00482
   Yang FX, 2020, AAAI CONF ARTIF INTE, V34, P12597
   Yang QZ, 2019, PROC CVPR IEEE, P3628, DOI 10.1109/CVPR.2019.00375
   Yunpeng Zhai, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9018, DOI 10.1109/CVPR42600.2020.00904
   Zhang X, 2021, PROC CVPR IEEE, P3435, DOI 10.1109/CVPR46437.2021.00344
   Zhang X, 2014, INT J APPROX REASON, V55, P1787, DOI 10.1016/j.ijar.2014.05.007
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zilong Ji, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12373), P20, DOI 10.1007/978-3-030-58604-1_2
NR 47
TC 1
Z9 1
U1 2
U2 7
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2024
VL 40
IS 3
BP 1853
EP 1866
DI 10.1007/s00371-023-02890-2
EA JUN 2023
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HY8O3
UT WOS:001002485500001
DA 2024-07-18
ER

PT J
AU Rudolph, C
   Brunnett, G
   Bretschneider, M
   Meyer, B
   Asbrock, F
AF Rudolph, Carsten
   Brunnett, Guido
   Bretschneider, Maximilian
   Meyer, Bertolt
   Asbrock, Frank
TI TechnoSapiens: merging humans with technology in augmented reality
SO VISUAL COMPUTER
LA English
DT Article
DE Mixed reality; Augmented reality; Human computer interaction; Social
   perception; Stereotyping
ID VALIDATION; EMBODIMENT; ILLUSION; FABRIK; MODEL
AB We present a marker-less AR/DR system that can replace the arm of the user with a virtual bionic prosthesis in real time including finger tracking. For this, we use a mixed reality HMD that provides the user with a stereo image based on video-see-through (VST). We apply chroma-keying to remove the user's arm from each captured image and input reconstructed background information into the removed pixels. Before rendering the prosthesis model into the image, we re-target motion capture data of the user's hand to the kinematic skeleton of the prosthesis to match the current hand pose. This system opens new research possibilities on self- and other-perception of bionic bodies. In a first evaluation study of the system, we propose that users perceive the virtual prosthesis model as a part of their body (i.e., that they experience a sense of ownership). We tested this assumption in a laboratory study with 27 individuals who used the system to perform a series of simple tasks in AR with their prosthesis. We measured body ownership and other measures with self-reports. In support of the hypothesis, users experienced a sense of body ownership. Also, a feeling of self-presence is induced during the task, and participants rated the overall experience as positive.
C1 [Rudolph, Carsten; Brunnett, Guido; Bretschneider, Maximilian; Meyer, Bertolt; Asbrock, Frank] Tech Univ Chemnitz, Chemnitz, Germany.
C3 Technische Universitat Chemnitz
RP Rudolph, C (corresponding author), Tech Univ Chemnitz, Chemnitz, Germany.
EM carsten.rudolph@informatik.tu-chemnitz.de;
   guido.brunnett@informatik.tu-chemnitz.de;
   maximilian.bretschneider@psychologie.tu-chemnitz.de;
   bertolt.meyer@psychologie.tu-chemnitz.de;
   frank.asbrock@psychologie.tu-chemnitz.de
RI Rudolph, Carsten/AAC-5107-2021; Bretschneider, Maximilian/ABW-6187-2022;
   Asbrock, Frank/AAU-2857-2020; Rudolph, Carsten/Y-6261-2018
OI Bretschneider, Maximilian/0000-0002-2270-6501; Rudolph,
   Carsten/0000-0001-6462-0418
FU Projekt DEAL
FX Open Access funding enabled and organized by Projekt DEAL.
CR Adikari SB., 2020, ADV HUMAN COMPUT INT, V1-10, P2020
   Agisoft LLC, 2022, Agisoft Metashape
   Almeida A, 2021, ROBOT AUTON SYST, V145, DOI 10.1016/j.robot.2021.103857
   [Anonymous], 2011, 2011 IEEE INT C MULT
   Araki N., 2008, 2008 3 INT C DIG INF, P33
   Argelaguet F, 2016, P IEEE VIRT REAL ANN, P3, DOI 10.1109/VR.2016.7504682
   Aristidou A, 2016, COMPUT ANIMAT VIRT W, V27, P35, DOI 10.1002/cav.1630
   Aristidou A, 2011, GRAPH MODELS, V73, P243, DOI 10.1016/j.gmod.2011.05.003
   Becker C, 2018, PHOTOGRAMM ENG REM S, V84, P287, DOI 10.14358/PERS.84.5.287
   Bekrater-Bodmann R, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-020-70828-y
   Blender Foundation, 2022, About Blender
   Botvinick M, 1998, NATURE, V391, P756, DOI 10.1038/35784
   BUCHSBAUM G, 1980, J FRANKLIN I, V310, P1, DOI 10.1016/0016-0032(80)90058-7
   Calmon J, 2015, SIBGRAPI, P265, DOI 10.1109/SIBGRAPI.2015.30
   Cosco FI, 2009, INT SYM MIX AUGMENT, P99, DOI 10.1109/ISMAR.2009.5336492
   Elharrouss O, 2020, NEURAL PROCESS LETT, V51, P2007, DOI 10.1007/s11063-019-10163-0
   Faul F, 2007, BEHAV RES METHODS, V39, P175, DOI 10.3758/BF03193146
   Fengxiang Rong, 2021, 2021 International Conference on Networking Systems of AI (INSAI), P129, DOI 10.1109/INSAI54028.2021.00033
   Ferdous H.S., P 2019 CHI C HUM FAC, P1
   Franke T, 2019, INT J HUM-COMPUT INT, V35, P456, DOI 10.1080/10447318.2018.1456150
   Ghanem A., 2020, P 13 ACM INT C PERVA, P1
   Herling J., 2010, 2010 9th IEEE International Symposium on Mixed and Augmented Reality (ISMAR). Science & Technology Papers, P207, DOI 10.1109/ISMAR.2010.5643572
   Herling J, 2014, IEEE T VIS COMPUT GR, V20, P866, DOI 10.1109/TVCG.2014.2298016
   Hervieu A., 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P4101, DOI 10.1109/ICPR.2010.997
   Hoang T, 2017, PROCEEDINGS OF THE 2017 ACM SIGCHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'17), P1419, DOI 10.1145/3025453.3025860
   Hoyet L, 2016, FRONT ROBOT AI, V3, DOI 10.3389/frobt.2016.00027
   Isikdogan F., 2012, CMPE537 Computer Vision Course Project
   Jarque-Bou NJ, 2021, APPL SCI-BASEL, V11, DOI 10.3390/app11073158
   Kilteni K, 2012, PRESENCE-TELEOP VIRT, V21, P373, DOI 10.1162/PRES_a_00124
   Kilteni K, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0040867
   Kjarside K., 2005, P CENTRAL EUROPEAN M
   Lécuyer A, 2017, IEEE COMPUT GRAPH, V37, P20, DOI 10.1109/MCG.2017.14
   Li CX, 2021, MULTIMED TOOLS APPL, V80, P5203, DOI 10.1007/s11042-020-09989-x
   Makransky G, 2017, COMPUT HUM BEHAV, V72, P276, DOI 10.1016/j.chb.2017.02.066
   Martin M., 2014, P 2014 C INT ENT NEW, P1
   McNeish D, 2018, PSYCHOL METHODS, V23, P412, DOI 10.1037/met0000144
   Meerits S, 2015, PROCEEDINGS OF THE 2015 IEEE INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY WORKSHOPS, P53, DOI 10.1109/ISMARW.2015.19
   Meng-Li Shih, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8025, DOI 10.1109/CVPR42600.2020.00805
   Meyer B, 2018, FRONT PSYCHOL, V9, DOI 10.3389/fpsyg.2018.02251
   Mitsuhashi N, 2009, NUCLEIC ACIDS RES, V37, pD782, DOI 10.1093/nar/gkn613
   Mori S., 2017, IPSJ Transactions on Computer Vision and Applications, V9, P17, DOI [10.1186/s41074-017-0028-1, DOI 10.1186/S41074-017-0028-1, 10.3390/electronics10080900]
   Mori S, 2016, ADJUNCT PROCEEDINGS OF THE 2016 IEEE INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY (ISMAR-ADJUNCT), P292, DOI [10.1109/ISMAR-Adjunct.2016.0098, 10.1109/ISMAR-Adjunct.2016.89]
   Mori S, 2015, PROCEEDINGS OF THE 2015 IEEE INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY WORKSHOPS, P32, DOI 10.1109/ISMARW.2015.16
   Mu TJ, 2014, VISUAL COMPUT, V30, P833, DOI 10.1007/s00371-014-0961-2
   Narasimhaswamy S, 2019, IEEE I CONF COMP VIS, P9566, DOI 10.1109/ICCV.2019.00966
   Norman K.L, 2018, GHITALY18 2 WORKSH G
   Perez-Marcos D, 2009, NEUROREPORT, V20, P589, DOI 10.1097/WNR.0b013e32832a0a2a
   Petkova VI, 2011, FRONT PSYCHOL, V2, DOI 10.3389/fpsyg.2011.00035
   Pharr M., 2016, Physically Based Rendering: From Theory to Implementation, V3rd ed.
   Pielli L., 2020, Cognitive Semiotics, V13
   Piryankova IV, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0103428
   Putri G.V.G, 2020, Jurnal Informatika dan Sains, V3, P67
   Romano D, 2021, SCI REP-UK, V11, DOI 10.1038/s41598-021-84595-x
   Schwind V, 2017, CHI PLAY'17: PROCEEDINGS OF THE ANNUAL SYMPOSIUM ON COMPUTER-HUMAN INTERACTION IN PLAY, P507, DOI 10.1145/3116595.3116596
   Simmons J.P., 2012, DIALOGUE OFFICIAL NE, DOI [10.2139/ssrn.2160588, DOI 10.2139/SSRN.2160588]
   Slater M, 2010, PLOS ONE, V5, DOI 10.1371/journal.pone.0010564
   Tsai TH, 2022, NEUROCOMPUTING, V495, P1, DOI 10.1016/j.neucom.2022.04.079
   Tsakiris M, 2005, J EXP PSYCHOL HUMAN, V31, P80, DOI 10.1037/0096-1523.31.1.80
   Unity Technologies, 2022, Unity 3D
   Van Den Bergh F., 1999, South African Computer Journal, P155
   Varjo Technologies, 2022, Varjo XR-3
   Wang L., 2008, 2008 IEEE C COMP VIS, P1
   Zbinden J, 2022, J NEUROENG REHABIL, V19, DOI 10.1186/s12984-022-01006-6
NR 63
TC 3
Z9 3
U1 3
U2 10
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2024
VL 40
IS 2
BP 1021
EP 1036
DI 10.1007/s00371-023-02829-7
EA MAY 2023
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GE6E8
UT WOS:000991071000001
OA hybrid
DA 2024-07-18
ER

PT J
AU Hu, MF
   Liu, ZY
   Liu, JW
AF Hu, Ming-fei
   Liu, Ze-yu
   Liu, Jian-wei
TI mcVAE: disentangling by mean constraint
SO VISUAL COMPUTER
LA English
DT Article
DE Variational autoencoder; Disentanglement; Representation learning
ID REPRESENTATION
AB Disentanglement tends to automatically learn and separate the interpretable factors of variation hidden in the data. Disentangled representations are more transferable and robust for the chosen model, and they are commonly used in image attack detection and anti-fraud, as well as classification and recommendation systems in special situations. As a popular method for learning unsupervised disentanglement, beta-VAE re-weights the KL divergence by an adjustable hyperparameter. However, good disentangled representations always lead to blurry reconstructions and mode collapse on complex datasets. We find that the variance vector of the variational posterior is related to the nature of the dataset and representation space, limiting its value to 1 is not reasonable enough. More importantly, constraining mean variable alone can achieve better disentanglement and reconstruction performance. Therefore, we introduce mean constraint VAE, a simple and effective replacement of the beta-VAE for improving the poor reconstruction and learning a higher degree of disentanglement. In addition, a classifier-free measure of disentanglement called variance proportion metric is proposed. Experiments show that our framework outperforms beta-VAE on several benchmark datasets.
C1 [Hu, Ming-fei; Liu, Ze-yu; Liu, Jian-wei] China Univ Petr, Dept Automat, Coll Informat Sci & Engn, 260 Mailbox, Beijing 102249, Peoples R China.
C3 China University of Petroleum
RP Liu, JW (corresponding author), China Univ Petr, Dept Automat, Coll Informat Sci & Engn, 260 Mailbox, Beijing 102249, Peoples R China.
EM hmfzsy@gmail.com; 2275045480@qq.com; liujw@cup.edu.cn
RI Liu, Jian-Wei/C-5013-2008
CR Achille A, 2018, IEEE T PATTERN ANAL, V40, P2897, DOI 10.1109/TPAMI.2017.2784440
   Alemi Alexander A, 2017, INT C LEARNING REPRE
   Bengio Y., 2012, ARXIV
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Brakel P., 2017, ARXIV
   Chen Ricky T. Q., 2018, Advances in neural information processing systems, V31, DOI DOI 10.5555/3327757.3327764
   Chen Xi, 2016, Advances in Neural Information Processing Systems (NIPS), V29
   Dupont E, 2018, ADV NEUR IN, V31
   Eastwood C., 2018, INT C LEARN REPR
   Gyawali PK, 2019, IEEE DATA MINING, P1078, DOI 10.1109/ICDM.2019.00127
   Higgins I., 2016, BETA VAE LEARNING BA
   Higgins I., 2018, ARXIV
   Hoffman Matthew D, 2016, WORKSH ADV APPR BAYE, V1
   Hu M.F., 2021, ICANN
   Huang X., 2020, BMC PLANT BIOL, V37, P1
   Jeon I., 2021, AAAI CONF ARTIF INTE
   Karaletsos T., 2015, ARXIV
   Kim H, 2018, PR MACH LEARN RES, V80
   Kingma D. P., 2014, arXiv
   Liu XP, 2022, VISUAL COMPUT, V38, P669, DOI 10.1007/s00371-020-02042-w
   Makhzani A., 2017, ADV NEUR IN
   Makhzani A., 2015, ARXIV
   Mathieu E., 2019, P INT C MACH LEARN I, P4402
   Matthey L., 2017, dsprites: Disentanglement testing sprites dataset
   Park S, 2021, AAAI CONF ARTIF INTE, V35, P2403
   Rezende DJ, 2014, PR MACH LEARN RES, V32, P1278
   Sabour S, 2017, ADV NEUR IN, V30
   SCHMIDHUBER J, 1992, NEURAL COMPUT, V4, P863, DOI 10.1162/neco.1992.4.6.863
   Shwartz-Ziv R., 2017, ARXIV
   Tang Y., 2013, INT C MACH LEARN, P163
   Tishby N., 2000, CORR, Vphysics
   Tishby N, 2015, 2015 IEEE INFORMATION THEORY WORKSHOP (ITW)
   Wang GQ, 2020, PROC CVPR IEEE, P6677, DOI 10.1109/CVPR42600.2020.00671
   Wonkwang Lee, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12371), P157, DOI 10.1007/978-3-030-58574-7_10
   Yang HH, 1997, NEURAL COMPUT, V9, P1457, DOI 10.1162/neco.1997.9.7.1457
   Zhang K., 2020, ARXIV PREPRINT ARXIV
   Zhao S., 2017, arXiv
NR 37
TC 1
Z9 1
U1 1
U2 8
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2024
VL 40
IS 2
BP 1229
EP 1243
DI 10.1007/s00371-023-02843-9
EA APR 2023
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GE6E8
UT WOS:000963831000002
DA 2024-07-18
ER

PT J
AU Zhang, ZM
   Deng, XM
   Li, JY
   Lai, YK
   Ma, CX
   Liu, YJ
   Wang, HA
AF Zhang, Zhengming
   Deng, Xiaoming
   Li, Jinyao
   Lai, Yukun
   Ma, Cuixia
   Liu, Yongjin
   Wang, Hongan
TI Stroke-based semantic segmentation for scene-level free-hand sketches
SO VISUAL COMPUTER
LA English
DT Article
DE Sketch dataset; Scene sketch; Free-hand sketch; Semantic segmentation
AB Sketching is a simple and efficient way for humans to express their perceptions of the world. Sketch semantic segmentationplays a key role in sketch understanding and is widely used in sketch recognition, sketch-based image retrieval, or editing.Due to modality difference between images and sketches, existing image segmentation methods may not perform best, whichoverlook the sparse nature and stroke-based representation in sketches. The existing sketch semantic segmentation methodsare mainly designed for single-instance sketches. In this paper, we present a new stroke-based sequential-spatial neuralnetwork (S3NN) for scene-level free-hand sketch semantic segmentation, which leverages a bidirectional LSTM and graphconvolutional network to capture the sequential and spatial features of sketches. In order to address the data lacking issue, wepropose the first scene-level free-hand sketch dataset (SFSD). SFSD is composed of 12K sketch-photo pairs over 40 objectcategories, where the sketches were completely hand-drawn and each contains 7 objects on average. We conduct comparativeand ablative experiments on SFSD to evaluate the effectiveness of our method. The experimental results demonstrate that ourmethod outperforms state-of-the-art methods. The code, models, and dataset will be made public after acceptance.
C1 [Zhang, Zhengming; Deng, Xiaoming; Li, Jinyao; Ma, Cuixia; Wang, Hongan] Chinese Acad Sci, Inst Software, State Key Lab Comp Sci, Beijing, Peoples R China.
   [Zhang, Zhengming; Deng, Xiaoming; Li, Jinyao; Ma, Cuixia; Wang, Hongan] Chinese Acad Sci, Inst Software, Beijing Key Lab Human Comp Interact, Beijing, Peoples R China.
   [Lai, Yukun] Univ Chinese Acad Sci, Beijing, Peoples R China.
   [Lai, Yukun] Cardiff Univ, Cardiff, Wales.
   [Liu, Yongjin] Tsinghua Univ, Beijing, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Software, CAS; Chinese Academy
   of Sciences; Institute of Software, CAS; Chinese Academy of Sciences;
   University of Chinese Academy of Sciences, CAS; Cardiff University;
   Tsinghua University
RP Ma, CX (corresponding author), Chinese Acad Sci, Inst Software, State Key Lab Comp Sci, Beijing, Peoples R China.; Ma, CX (corresponding author), Chinese Acad Sci, Inst Software, Beijing Key Lab Human Comp Interact, Beijing, Peoples R China.
EM zhangzhengming16@mails.ucas.ac.cn; xiaoming@iscas.ac.cn;
   ljyyolia@gmail.com; LaiY4@cardiff.ac.uk; cuixia@iscas.ac.cn;
   liuyongjin@tsinghua.edu.cn; hongan@iscas.ac.cn
RI yang, yue/KCK-7870-2024; chen, si/JPK-4258-2023; Chen,
   Feng/JQW-8742-2023; tian, ye/KGL-6485-2024; huang, shan/JVN-1240-2024;
   feng, chen/JLM-8296-2023; Lai, Yu-Kun/D-2343-2010; Yang,
   Lili/JTT-5215-2023
OI Yang, Lili/0009-0008-2926-484X; ma, cui xia/0000-0003-3999-7429
FU Natural Science Foundation of China [61872346]; Beijing Natural Science
   Foundation [L222008]; 2019 China Prize of Newton Prize Project
   [NP2PB/100047]
FX This work was supported by the Natural Science Foundation of China under
   Grant 61872346, Beijing Natural Science Foundation under Grant L222008,
   and 2019 China Prize of Newton Prize Project under Grant NP2PB/100047.
CR Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49
   Delaye A, 2015, PATTERN RECOGN, V48, P1197, DOI 10.1016/j.patcog.2014.10.022
   Eitz M, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185540
   Gao CY, 2020, PROC CVPR IEEE, P5173, DOI 10.1109/CVPR42600.2020.00522
   Ge C, 2022, IEEE T IMAGE PROCESS, V31, P1447, DOI 10.1109/TIP.2022.3142511
   Gennari L, 2005, COMPUT GRAPH-UK, V29, P547, DOI 10.1016/j.cag.2005.05.007
   Graves A, 2005, NEURAL NETWORKS, V18, P602, DOI 10.1016/j.neunet.2005.06.042
   Ha David, 2018, INT C LEARNING REPRE
   Hahnlein F., 2019, JOURNEES FRANCAISES
   Huang Z, 2014, ACM T GRAPHIC, V33, DOI [10.1145/2661229.2661280, 10.1145/2661228.2661280]
   Kaiyrbekov K, 2020, IEEE COMPUT GRAPH, V40, P112, DOI 10.1109/MCG.2019.2943333
   Kipf TN, 2016, ARXIV
   Kirillov A., 2017, A unified architecture for instance and semantic segmentation," ed
   Li K., 2018, ECCV, P582
   Li K, 2019, IEEE T IMAGE PROCESS, V28, P3219, DOI 10.1109/TIP.2019.2895155
   Li L, 2019, IEEE COMPUT GRAPH, V39, P38, DOI 10.1109/MCG.2018.2884192
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Qi YG, 2019, IEEE ACCESS, V7, P102717, DOI 10.1109/ACCESS.2019.2929804
   Qi YG, 2015, PROC CVPR IEEE, P1856, DOI 10.1109/CVPR.2015.7298795
   Qiu Y, 2010, SCAND J IMMUNOL, V72, P425, DOI 10.1111/j.1365-3083.2010.02456.x
   Sangkloy P, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925954
   Sarvadevabhatla RK, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P10, DOI 10.1145/3123266.3123270
   Schneider RG, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2898351
   Sun ZB, 2012, LECT NOTES COMPUT SC, V7572, P626, DOI 10.1007/978-3-642-33718-5_45
   Wu X., 2018, Policy Capacity and Governance: Assessing Governmental Competences and Capabilities in Theory and Practice, P1, DOI DOI 10.1007/978-3-319-54675-9
   Yang LM, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450284
   Yu Q, 2016, PROC CVPR IEEE, P799, DOI 10.1109/CVPR.2016.93
   Zhu XY, 2020, MULTIMED TOOLS APPL, V79, P1585, DOI 10.1007/s11042-019-08158-z
   Zou CQ, 2018, LECT NOTES COMPUT SC, V11219, P438, DOI 10.1007/978-3-030-01267-0_26
NR 30
TC 1
Z9 1
U1 2
U2 7
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2023
VL 39
IS 12
BP 6309
EP 6321
DI 10.1007/s00371-022-02731-8
EA DEC 2022
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA X7HK1
UT WOS:000895050300002
DA 2024-07-18
ER

PT J
AU Yang, GM
   Xu, K
   Fang, XJ
   Zhang, J
AF Yang, Gaoming
   Xu, Kun
   Fang, Xianjin
   Zhang, Ji
TI Video face forgery detection via facial motion-assisted capturing dense
   optical flow truncation
SO VISUAL COMPUTER
LA English
DT Article
DE Face forgery detection; Deepfakes detection; Dense optical flow; Deep
   learning
ID FORENSICS
AB Deep learning advancements have resulted in breakthroughs in facial forgery techniques. Facial forgery videos are growing increasingly lifelike, making it impossible for people to tell the difference between the real and the fake. The proliferation of facial forgery techniques, as well as the slowness with which they may be detected, could jeopardize personal data security. As a result, it's critical to look at approaches that can be trained on both real and fake videos and then utilized to identify facial forgeries or not in videos. This study found that forged face videos undergo truncation between consecutive frames of optical flow imaging after dense optical flow processing. We present an approach for detecting video face forgery that extracts and analyzes characteristics from real and fake material, then use those features to train classification models on Celeb-DF and FaceForensics++. In addition, we employ a unique facial double-triangle region to assist in the extraction of video inter-frame feature data. Experiments results show that the facial motion features extracted from the double triangle region successfully assist in capturing the dense optical flow truncation. Extensive evaluation suggests that our proposed approach is effective for video face forgery detection.
C1 [Yang, Gaoming; Xu, Kun; Fang, Xianjin] Anhui Univ Sci & Technol, Sch Comp Sci & Engn, Huainan 232001, Peoples R China.
   [Zhang, Ji] Univ Southern Queensland, Dept Math & Comp, Brisbane, Qld, Australia.
C3 Anhui University of Science & Technology; University of Southern
   Queensland
RP Xu, K (corresponding author), Anhui Univ Sci & Technol, Sch Comp Sci & Engn, Huainan 232001, Peoples R China.
EM gmyang@aust.edu.cn; xukun@aust.edu.cn; xjfang@aust.edu.cn;
   Ji.Zhang@usq.edu.au
RI Bueno, Regis Cortez/AAG-3852-2020; Xu, Kun/GXV-6266-2022; yan,
   su/KHT-1728-2024; Wang, Siying/KHX-1894-2024; wang, shuo/KCL-3379-2024
OI Bueno, Regis Cortez/0000-0002-2923-4930; Xu, Kun/0000-0002-1866-4433;
   fang, xian jin/0000-0002-3894-2007; Yang, Gaoming/0000-0002-7666-1038
FU Natural Science Foundation of Anhui Province of China [2008085MF220];
   School Foundation of Anhui University of Science and Technology
   [2021CX2102]
FX This work was supported by the Natural Science Foundation of Anhui
   Province of China underGrant No.2008085MF220, and the School Foundation
   of Anhui University of Science and Technology under Grant No.2021CX2102.
CR Afchar D, 2018, IEEE INT WORKS INFOR
   Agarwal S., 2019, P IEEE C COMP VIS PA, P38, DOI DOI 10.1109/ICCV.2015.425
   Alvarez L, 2000, INT J COMPUT VISION, V39, P41, DOI 10.1023/A:1008170101536
   Amerini I, 2019, IEEE INT CONF COMP V, P1205, DOI 10.1109/ICCVW.2019.00152
   Bau David, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P351, DOI 10.1007/978-3-030-58452-8_21
   Bonettini N, 2021, INT C PATT RECOG, P5012, DOI 10.1109/ICPR48806.2021.9412711
   Caldelli R, 2021, PATTERN RECOGN LETT, V146, P31, DOI 10.1016/j.patrec.2021.03.005
   Chen RW, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2003, DOI 10.1145/3394171.3413630
   Chen ZK, 2021, PROC CVPR IEEE, P9010, DOI 10.1109/CVPR46437.2021.00890
   Chesney B, 2019, CALIF LAW REV, V107, P1753, DOI 10.15779/Z38RV0D15J
   Das Arijit, 2021, P INT C COMP COMM NE, P1, DOI DOI 10.1109/ICCCNT51525.2021.9580160
   Deng JK, 2020, PROC CVPR IEEE, P5202, DOI 10.1109/CVPR42600.2020.00525
   Dolhansky B., 2020, arXiv
   Farnebäck G, 2003, LECT NOTES COMPUT SC, V2749, P363, DOI 10.1007/3-540-45103-x_50
   Foley JD., 1996, COMPUTER GRAPHICS PR
   Guo ZQ, 2021, MULTIMED TOOLS APPL, V80, P7687, DOI 10.1007/s11042-020-10098-y
   Ha S, 2020, AAAI CONF ARTIF INTE, V34, P10893
   HORN BKP, 1981, ARTIF INTELL, V17, P185, DOI 10.1016/0004-3702(81)90024-2
   Huang QD, 2021, AAAI CONF ARTIF INTE, V35, P1619
   Juefei-Xu F, 2022, INT J COMPUT VISION, V130, P1678, DOI 10.1007/s11263-022-01606-8
   Kaur S, 2020, J ELECTRON IMAGING, V29, DOI 10.1117/1.JEI.29.3.033013
   Ki Chan Christopher Chun, 2020, 2020 IEEE/ITU International Conference on Artificial Intelligence for Good (AI4G), P55, DOI 10.1109/AI4G50087.2020.9311067
   Kietzmann J, 2020, BUS HORIZONS, V63, P135, DOI 10.1016/j.bushor.2019.11.006
   Li LZ, 2020, PROC CVPR IEEE, P5000, DOI 10.1109/CVPR42600.2020.00505
   Li Y., 2018, 2018 IEEE INT WORKSH, P1, DOI DOI 10.1109/WIFS.2018.8630787
   Li YZ, 2017, ADV NEUR IN, V30
   Li YZ, 2020, PROC CVPR IEEE, P3204, DOI 10.1109/CVPR42600.2020.00327
   Liu HG, 2021, PROC CVPR IEEE, P772, DOI 10.1109/CVPR46437.2021.00083
   Malik A, 2022, IEEE ACCESS, V10, P18757, DOI 10.1109/ACCESS.2022.3151186
   Masi Iacopo, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12352), P667, DOI 10.1007/978-3-030-58571-6_39
   Matern F, 2019, IEEE WINT CONF APPL, P83, DOI 10.1109/WACVW.2019.00020
   Mirsky Y, 2021, ACM COMPUT SURV, V54, DOI 10.1145/3425780
   Naruniec J, 2020, COMPUT GRAPH FORUM, V39, P173, DOI 10.1111/cgf.14062
   Nguyen HH, 2019, INT CONF BIOMETR THE, DOI 10.1109/btas46853.2019.9185974
   Nguyen HH, 2019, INT CONF ACOUST SPEE, P2307, DOI 10.1109/ICASSP.2019.8682602
   Nirkin Y, 2022, IEEE T PATTERN ANAL, V44, P6111, DOI 10.1109/TPAMI.2021.3093446
   Pant K, 2020, AACL-IJCNLP 2020: THE 1ST CONFERENCE OF THE ASIA-PACIFIC CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 10TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING: PROCEEDINGS OF THE STUDENT RESEARCH WORKSHOP, P37
   Qian Y., 2020, EUROPEAN C COMPUTER, V12357, P86, DOI 10.1007/978-3- 030-58610-2 6
   Raghavendra R, 2017, 2017 IEEE INTERNATIONAL JOINT CONFERENCE ON BIOMETRICS (IJCB), P555, DOI 10.1109/BTAS.2017.8272742
   Rossi A, 2020, IEEE T INTELL TRANSP, V21, P2980, DOI 10.1109/TITS.2019.2922002
   Ruiz N., 2020, EUR C COMP VIS, P236, DOI [10.1007/978-3-030-66823-5_14, DOI 10.1007/978-3-030-66823-5_14]
   Sherstinsky A, 2020, PHYSICA D, V404, DOI 10.1016/j.physd.2019.132306
   Tyagi S, 2023, VISUAL COMPUT, V39, P813, DOI 10.1007/s00371-021-02347-4
   Wang JK, 2022, PROCEEDINGS OF THE 2022 INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, ICMR 2022, P615, DOI 10.1145/3512527.3531415
   Wang M, 2021, NEUROCOMPUTING, V429, P215, DOI 10.1016/j.neucom.2020.10.081
   Wang Y., 2021, arXiv
   Wang YK, 2022, IEEE T INF FOREN SEC, V17, P500, DOI 10.1109/TIFS.2022.3146766
   Wang Z, 2022, IEEE T IMAGE PROCESS, V31, P3541, DOI 10.1109/TIP.2022.3172845
   Xu ZP, 2021, J VIS COMMUN IMAGE R, V77, DOI 10.1016/j.jvcir.2021.103119
   Yu N., 2021, ARXIV
   Yu PP, 2022, IEEE T INF FOREN SEC, V17, P547, DOI 10.1109/TIFS.2022.3146781
   Zhang JN, 2020, PROC CVPR IEEE, P5325, DOI 10.1109/CVPR42600.2020.00537
   Zhang T, 2022, MULTIMED TOOLS APPL, V81, P6259, DOI 10.1007/s11042-021-11733-y
   Zhang WG, 2020, ENTROPY-SWITZ, V22, DOI 10.3390/e22020249
   Zhao HQ, 2021, PROC CVPR IEEE, P2185, DOI 10.1109/CVPR46437.2021.00222
   Zhou P, 2017, IEEE COMPUT SOC CONF, P1831, DOI 10.1109/CVPRW.2017.229
   Zhu BQ, 2020, PROCEEDINGS OF THE 3RD AAAI/ACM CONFERENCE ON AI, ETHICS, AND SOCIETY AIES 2020, P414, DOI 10.1145/3375627.3375849
   Zhu YH, 2021, PROC CVPR IEEE, P4832, DOI 10.1109/CVPR46437.2021.00480
NR 58
TC 5
Z9 5
U1 7
U2 31
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2023
VL 39
IS 11
BP 5589
EP 5608
DI 10.1007/s00371-022-02683-z
EA OCT 2022
PG 20
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA W5SX6
UT WOS:000864990700001
DA 2024-07-18
ER

PT J
AU Chen, JQ
   Wei, DP
   Long, T
   Luo, T
   Wang, HB
AF Chen Jiqing
   Wei Depeng
   Long Teng
   Luo Tian
   Wang Huabin
TI All-weather road drivable area segmentation method based on CycleGAN
SO VISUAL COMPUTER
LA English
DT Article
DE Generative adversarial network; Atrous convolution; Convolutional neural
   network; Image enhancement; Semantic segmentation
ID NETWORK; SUPERPIXELS
AB It is a challenging task to segment drivable area of road in automatic driving system. Convolutional neural network has excellent performance in road segmentation. However, the existing segmentation methods only focus on improving the performance of road segmentation under good road conditions, but pay little attention to the performance of road segmentation under severe weather conditions. In this paper, an image enhancement network (IEC-Net) based on CycleGAN is proposed to enhance the diversified features of input images. Firstly, an unsupervised CycleGAN network is trained to feature enhance road images under severe weather conditions, so as to obtain an enhanced image with rich feature information. Secondly, the enhanced image is input into the most advanced semantic segmentation network, so as to realize the segmentation of the drivable area of the road. The experimental results show that the IEC-Net based on CycleGAN can be directly combined with any advanced semantic segmentation network and can not only realize end-to-end training, but also greatly improve the performance of the original semantic segmentation network for road segmentation under severe weather conditions.
C1 [Chen Jiqing; Wei Depeng; Long Teng; Luo Tian; Wang Huabin] Guangxi Univ, Coll Mech Engn, Nanning 530007, Peoples R China.
C3 Guangxi University
RP Long, T (corresponding author), Guangxi Univ, Coll Mech Engn, Nanning 530007, Peoples R China.
EM 1059343169@qq.com
FU National Nature Science Foundation of China [62163005]; Natural Science
   Foundation of Guangxi Province [2022GXNSFAA035633]
FX This research was funded by the National Nature Science Foundation of
   China (grant number 62163005), Natural Science Foundation of Guangxi
   Province (grant number 2022GXNSFAA035633).
CR Abdollahi A, 2021, IEEE ACCESS, V9, P64381, DOI 10.1109/ACCESS.2021.3075951
   Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615
   Bai L., 2020, ROADNET RT HIGH THRO
   Chen BK, 2019, IEEE T INTELL TRANSP, V20, P137, DOI 10.1109/TITS.2018.2801309
   Chen L.Z, 2017, CORR ABS170605587
   Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Cheng MM, 2018, IEEE T VEH TECHNOL, V67, P10330, DOI 10.1109/TVT.2018.2865836
   Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350
   Defferrard M, 2016, ADV NEUR IN, V29
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dong SJ, 2022, IEEE GEOSCI REMOTE S, V19, DOI 10.1109/LGRS.2021.3137551
   Dong XP, 2015, IEEE T IMAGE PROCESS, V24, P3966, DOI 10.1109/TIP.2015.2456636
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Howard A.G., 2017, MOBILENETS EFFICIENT, DOI DOI 10.48550/ARXIV.1704.04861
   Isola Phillip, 2016, ARXIV161107004, DOI [DOI 10.1109/CVPR.2017.632, 10.1109/CVPR.2017.632]
   Kim T, 2017, PR MACH LEARN RES, V70
   Kingma D., 2014, COMPUT SCI, V6, P66
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li XD, 2019, IEEE T CIRC SYST VID, V29, P2538, DOI 10.1109/TCSVT.2017.2749620
   Li Y, 2019, IEEE GEOSCI REMOTE S, V16, P613, DOI 10.1109/LGRS.2018.2878771
   Liang YH, 2021, NEUROCOMPUTING, V422, P22, DOI 10.1016/j.neucom.2020.09.033
   Liu MY, 2017, ADV NEUR IN, V30
   Long J., 2017, IEEE T PATTERN ANAL, V6, P66
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   López-Cifuentes A, 2020, PATTERN RECOGN, V102, DOI 10.1016/j.patcog.2020.107256
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Lu QQ, 2014, 2014 11TH WORLD CONGRESS ON INTELLIGENT CONTROL AND AUTOMATION (WCICA), P1962, DOI 10.1109/WCICA.2014.7053021
   Maurya R., 2011, P INT C IM INF PROC, P1, DOI [10.1109/ICIIP.2011.6108839, DOI 10.1109/ICIIP.2011.6108839]
   Ouyang N, 2019, IEEE GEOSCI REMOTE S, V16, P457, DOI 10.1109/LGRS.2018.2872359
   Paszke A., 2016, ARXIV160602147
   Peng JT, 2016, IEEE T CYBERNETICS, V46, P1616, DOI 10.1109/TCYB.2015.2453091
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Reyes A, 2017, MEX INT CONF ARTIF I, P46, DOI 10.1109/MICAI-2017.2017.00016
   Romera E, 2019, IEEE INT VEH SYM, P1312, DOI [10.1109/IVS.2019.8813888, 10.1109/ivs.2019.8813888]
   Romera E, 2018, IEEE T INTELL TRANSP, V19, P263, DOI 10.1109/TITS.2017.2750080
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Shen JB, 2014, IEEE T IMAGE PROCESS, V23, P1451, DOI 10.1109/TIP.2014.2302892
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sun L, 2019, PROC SPIE, V11169, DOI 10.1117/12.2532477
   Tan XW, 2021, IEEE GEOSCI REMOTE S, V18, P533, DOI 10.1109/LGRS.2020.2976551
   Tang B, 2015, IEEE COMPUT INTELL M, V10, P52, DOI 10.1109/MCI.2015.2437512
   Uricár M, 2021, IEEE WINT CONF APPL, P766, DOI 10.1109/WACV48630.2021.00081
   Wang R, 2019, CHIN CONTR CONF, P8494, DOI [10.23919/chicc.2019.8865464, 10.23919/ChiCC.2019.8865464]
   Wang WG, 2016, IEEE T MULTIMEDIA, V18, P1011, DOI 10.1109/TMM.2016.2545409
   Wang ZJ, 2021, PATTERN RECOGN, V118, DOI 10.1016/j.patcog.2021.108023
   Xie EZ, 2021, ADV NEUR IN, V34
   [杨飞 Yang Fei], 2018, [机器人, Robot], V40, P803
   Yang KL, 2022, IEEE T INTELL TRANSP, V23, P1184, DOI 10.1109/TITS.2020.3023331
   Yu F., 2015, ARXIV
   Zhang J., 2020, ISSAFE IMPROVING SEM
   Zhang Y., 2021, OPTO ELECT ENG, V48, P210
   Zhang YX, 2018, NEUROCOMPUTING, V314, P316, DOI 10.1016/j.neucom.2018.06.059
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zheng SX, 2021, PROC CVPR IEEE, P6877, DOI 10.1109/CVPR46437.2021.00681
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 57
TC 6
Z9 6
U1 8
U2 42
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2023
VL 39
IS 10
BP 5135
EP 5151
DI 10.1007/s00371-022-02650-8
EA AUG 2022
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA T1GK8
UT WOS:000844427300001
DA 2024-07-18
ER

PT J
AU Guo, ZB
   Shuai, H
   Liu, GC
   Zhu, YS
   Wang, WQ
AF Guo, Zebin
   Shuai, Hui
   Liu, Guangcan
   Zhu, Yisheng
   Wang, Wenqing
TI Multi-level feature fusion pyramid network for object detection
SO VISUAL COMPUTER
LA English
DT Article
DE Object detection; Multi-level fusion; Feature pyramid; Receptive field;
   Object detection dataset
AB Scale variation is one of the challenges in object detection. In this paper, we design a Multi-Level Feature Fusion Pyramid Network (MLFFPN) that can fuse features with different receptive fields so as to produce reliable object representations robust against scale variation. Specifically, we perform feature extraction on the backbone network with convolutional kernels of different sizes, reconstructing the feature pyramids with the various receptive fields by adding top-down paths and lateral connections. Then, the reconstructed feature pyramids are fused. Finally, the bottom-up path enhancement is added for the final prediction. To verify the proposed method, we constructed a large-scale object detection dataset containing in total 225,944 instances and 16,000 images of 30 classes of common objects. In this study, we introduce MLFFPN into the object detection network and conduct a series of experiments on our datasets and MSCOCO datasets. Without bells and whistles, MLFFPN achieves a considerable detection improvement over the baseline network.
C1 [Guo, Zebin; Shuai, Hui; Liu, Guangcan; Zhu, Yisheng; Wang, Wenqing] Nanjing Univ Informat Sci & Technol, Jiangsu Key Lab Big Data Anal Technol B DAT, Automat, Nanjing, Peoples R China.
C3 Nanjing University of Information Science & Technology
RP Guo, ZB (corresponding author), Nanjing Univ Informat Sci & Technol, Jiangsu Key Lab Big Data Anal Technol B DAT, Automat, Nanjing, Peoples R China.
EM binze.zero@gmail.com
RI Zhu, Yisheng/GYA-3445-2022; Liu, Guangcan/J-1391-2014
OI Liu, Guangcan/0000-0002-9428-4387
FU National Natural Science Foundation of China (NSFC) [U21B2027]
FX This work was supported by National Natural Science Foundation of China
   (NSFC) under Grant U21B2027.
CR [Anonymous], CoRR abs/1511.07122
   Bochkovskiy A, 2020, ARXIV PREPRINT ARXIV, DOI DOI 10.48550/ARXIV.2004.10934
   Caesar Holger, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11618, DOI 10.1109/CVPR42600.2020.01164
   Chen K., 2019, arXiv preprint arXiv:1906.07155
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen WJ, 2021, VISUAL COMPUT, V37, P805, DOI 10.1007/s00371-020-01831-7
   Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5
   Ghiasi G, 2019, PROC CVPR IEEE, P7029, DOI 10.1109/CVPR.2019.00720
   Girshick R., 2014, PROC CVPR IEEE, DOI [DOI 10.1109/CVPR.2014.81, 10.1109/CVPR.2014.81]
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   He K., 2016, PROC CVPR IEEE, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]
   He KM, 2020, IEEE T PATTERN ANAL, V42, P386, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   He KM, 2014, LECT NOTES COMPUT SC, V8691, P346, DOI [arXiv:1406.4729, 10.1007/978-3-319-10578-9_23]
   Law H, 2018, LECT NOTES COMPUT SC, V11218, P765, DOI 10.1007/978-3-030-01264-9_45
   Li YH, 2019, IEEE I CONF COMP VIS, P6053, DOI 10.1109/ICCV.2019.00615
   Lin T.Y., 2017, P IEEE C COMPUTER VI, P2117, DOI [10.1109/CVPR.2017.106, DOI 10.1109/CVPR.2017.106]
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu S, 2018, PROC CVPR IEEE, P8759, DOI 10.1109/CVPR.2018.00913
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Masi I, 2018, SIBGRAPI, P471, DOI 10.1109/SIBGRAPI.2018.00067
   Pang JM, 2019, PROC CVPR IEEE, P821, DOI 10.1109/CVPR.2019.00091
   Paszke A, 2019, ADV NEUR IN, V32
   Redmon J., 2016, PROC CVPR IEEE, DOI [10.1109/CVPR.2016.91, DOI 10.1109/CVPR.2016.91]
   Redmon J, 2018, Arxiv, DOI [arXiv:1804.02767, DOI 10.1109/CVPR.2017.690, DOI 10.48550/ARXIV.1804.02767]
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Singh B, 2018, PROC CVPR IEEE, P3578, DOI 10.1109/CVPR.2018.00377
   Sreenu G, 2019, J BIG DATA-GER, V6, DOI 10.1186/s40537-019-0212-5
   Tan MX, 2019, PR MACH LEARN RES, V97
   Tian Z, 2019, IEEE I CONF COMP VIS, P9626, DOI 10.1109/ICCV.2019.00972
   Tiwari Abhishek Sunil, 2022, Artificial Intelligence and Technologies: Select Proceedings of ICRTAC-AIT 2020. Lecture Notes in Electrical Engineering (806), P103, DOI 10.1007/978-981-16-6448-9_12
   Tsung-Yi Lin, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P2999, DOI 10.1109/ICCV.2017.324
   Wei LX, 2021, VISUAL COMPUT, V37, P133, DOI 10.1007/s00371-019-01787-3
   Yin X., 2020, P AS C COMP VIS
   Zhang T., 2022, VISUAL COMPUT, P1
   Zhaowei Cai, 2018, 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition. Proceedings, P6154, DOI 10.1109/CVPR.2018.00644
   Zhou X., 2019, arXiv
   Zhu XZ, 2019, PROC CVPR IEEE, P9300, DOI 10.1109/CVPR.2019.00953
NR 38
TC 10
Z9 10
U1 7
U2 52
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2023
VL 39
IS 9
BP 4267
EP 4277
DI 10.1007/s00371-022-02589-w
EA JUL 2022
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA Q8PV6
UT WOS:000820534400001
DA 2024-07-18
ER

PT J
AU Qiu, ZX
   Zhang, HB
   Deng, WM
   Du, JX
   Lei, Q
   Zhang, GL
AF Qiu, Zhong-Xiang
   Zhang, Hong-Bo
   Deng, Wei-Mo
   Du, Ji-Xiang
   Lei, Qing
   Zhang, Guo-Liang
TI Effective skeleton topology and semantics-guided adaptive graph
   convolution network for action recognition
SO VISUAL COMPUTER
LA English
DT Article
DE Graph convolution network; Skeleton-based action recognition; Skeleton
   topology; Semantics information; Multistream network
ID MULTI-STREAM
AB Most of the existing graph convolutional network-based action recognition methods use an adaptive mechanism to learn action features from a skeleton sequence. Although this mechanism improves the recognition accuracy to some extent, its performance is still limited by the initial skeleton topology, which uses a natural human connection approach to connect skeletal joints. In addition, the semantic information of skeletal joints is naturally informative and discriminative for action recognition tasks, but its inclusion has rarely been investigated in the existing methods. To solve these problems, in this work, we propose a novel multistream-based effective skeleton topology and semantically guided adaptive graph convolution network for action recognition. By comparing several different topological graphs, we design an elbow- and knee-centric topology structure that forms the input to the adaptive graph convolutional network. Moreover, we explicitly embed the high-level semantic skeletal information into this network to enhance the feature representation capabilities. Finally, we study the positional relationships between different joints and the center of gravity in the same frame to generate relative position data. They are combined with the joint data, bone data and their corresponding motion information by a multistream network to further improve the action recognition accuracy. Extensive experiments show that the proposed method achieves state-of-the-art performance levels.
C1 [Qiu, Zhong-Xiang; Zhang, Hong-Bo] Huaqiao Univ, Sch Comp Sci & Technol, Xiamen 361000, Peoples R China.
   [Deng, Wei-Mo; Lei, Qing] Huaqiao Univ, Xiamen Key Lab Comp Vis & Pattern Recognit, Xiamen 361000, Peoples R China.
   [Du, Ji-Xiang; Zhang, Guo-Liang] Huaqiao Univ, Fujian Key Lab Big Data Intelligence & Secur, Xiamen 361000, Peoples R China.
C3 Huaqiao University; Huaqiao University; Huaqiao University
RP Zhang, HB (corresponding author), Huaqiao Univ, Sch Comp Sci & Technol, Xiamen 361000, Peoples R China.
EM qiuzhongxiang@stu.hqu.edu.cn; zhanghongbo@hqu.edu.cn
RI Zhang, Hong-Bo/GWC-9306-2022
FU Natural Science Foundation of China [61871196, 61902330, 62001176];
   National Key Research and Development Program of China [2019YFC1604700];
   Natural Science Foundation of Fujian Province of China [2019J01082,
   2020J01085]; Promotion Program for Young and Middle-aged Teacher in
   Science and Technology Research of Huaqiao University [ZQN-YX601]
FX The authors would like to thank the anonymous reviewers for their
   valuable and insightful comments on an earlier version of this
   manuscript. This work was supported by the Natural Science Foundation of
   China (No. 61871196, 61902330, 62001176); National Key Research and
   Development Program of China (NO.2019YFC1604700); Natural Science
   Foundation of Fujian Province of China (No. 2019J01082 and 2020J01085);
   and the Promotion Program for Young and Middle-aged Teacher in Science
   and Technology Research of Huaqiao University (ZQN-YX601).
CR [Anonymous], 2011, PROC CVPR IEEE, DOI [DOI 10.1109/CVPR.2011.5995316, 10.1109/CVPR.2011.5995316]
   [Anonymous], 2016, CVPR, DOI DOI 10.1109/CVPR.2016.115
   [Anonymous], 2017, Hierarchical representations for efficient architecture search
   Ardianto S, 2018, ASIAPAC SIGN INFO PR, P1601, DOI 10.23919/APSIPA.2018.8659539
   Boski M, 2017, 2017 10TH INTERNATIONAL WORKSHOP ON MULTIDIMENSIONAL (ND) SYSTEMS (NDS)
   Cao Z, 2017, PROC CVPR IEEE, P1302, DOI 10.1109/CVPR.2017.143
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chan WS, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20123499
   Cheng K, 2020, PROC CVPR IEEE, P180, DOI 10.1109/CVPR42600.2020.00026
   Cho Kyunghyun, 2014, SYNTAX SEMANTICS STR, P5, DOI [10.3115/v1/w14-4012, 10.3115 /v1/D14-1179, DOI 10.3115/V1/D14-1179]
   Du Y, 2016, IEEE T IMAGE PROCESS, V25, P3010, DOI 10.1109/TIP.2016.2552404
   Du Y, 2015, PROC CVPR IEEE, P1110, DOI 10.1109/CVPR.2015.7298714
   Fernando B, 2015, PROC CVPR IEEE, P5378, DOI 10.1109/CVPR.2015.7299176
   Gupta P., 2020, ARXIV PREPRINT ARXIV
   Han JG, 2013, IEEE T CYBERNETICS, V43, P1318, DOI 10.1109/TCYB.2013.2265378
   Heidari N., 2020, ARXIV PREPRINT ARXIV
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Hu JF, 2016, LECT NOTES COMPUT SC, V9905, P280, DOI 10.1007/978-3-319-46448-0_17
   Ke Q, 2017, PROC CVPR IEEE, P4570, DOI 10.1109/CVPR.2017.486
   Kim TS, 2017, IEEE COMPUT SOC CONF, P1623, DOI 10.1109/CVPRW.2017.207
   Li B, 2017, IEEE INT C COMPUT, P187, DOI 10.1109/CSE-EUC.2017.217
   Li C.-L., 2018, ARXIV PREPRINT ARXIV
   Li DL, 2022, VISUAL COMPUT, V38, P2971, DOI 10.1007/s00371-021-02167-6
   Li FJ, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20185260
   Li FJ, 2020, IEEE ACCESS, V8, P97757, DOI 10.1109/ACCESS.2020.2996779
   Li S, 2018, PROC CVPR IEEE, P5457, DOI 10.1109/CVPR.2018.00572
   Li W, 2020, IEEE ACCESS, V8, P144529, DOI 10.1109/ACCESS.2020.3014445
   Liu J, 2016, LECT NOTES COMPUT SC, V9907, P816, DOI 10.1007/978-3-319-46487-9_50
   Liu MY, 2017, PATTERN RECOGN, V68, P346, DOI 10.1016/j.patcog.2017.02.030
   Lo Presti L, 2016, PATTERN RECOGN, V53, P130, DOI 10.1016/j.patcog.2015.11.019
   Peng W, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1432, DOI 10.1145/3394171.3413910
   Peng W, 2021, PATTERN RECOGN, V115, DOI 10.1016/j.patcog.2021.107921
   Peng W, 2021, IEEE SIGNAL PROC LET, V28, P244, DOI 10.1109/LSP.2021.3049691
   Peng W, 2020, AAAI CONF ARTIF INTE, V34, P2669
   Peng W, 2019, IEEE IMAGE PROC, P11, DOI [10.1109/icip.2019.8802919, 10.1109/ICIP.2019.8802919]
   Shi H., 2021, ELECT IMAGING
   Shi L, 2020, IEEE T IMAGE PROCESS, V29, P9532, DOI 10.1109/TIP.2020.3028207
   Shi L, 2019, PROC CVPR IEEE, P12018, DOI 10.1109/CVPR.2019.01230
   Si CY, 2019, PROC CVPR IEEE, P1227, DOI 10.1109/CVPR.2019.00132
   Si CY, 2018, LECT NOTES COMPUT SC, V11205, P106, DOI 10.1007/978-3-030-01246-5_7
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song SJ, 2017, AAAI CONF ARTIF INTE, P4263
   Subetha T, 2016, 2016 INTERNATIONAL CONFERENCE ON INFORMATION COMMUNICATION AND EMBEDDED SYSTEMS (ICICES)
   Sun KK, 2021, IEEE T SYST MAN CY-S, V51, P3968, DOI 10.1109/TSMC.2019.2958072
   Tang YS, 2018, PROC CVPR IEEE, P5323, DOI 10.1109/CVPR.2018.00558
   Vaswani A, 2017, ADV NEUR IN, V30
   Vemulapalli R, 2014, PROC CVPR IEEE, P588, DOI 10.1109/CVPR.2014.82
   Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441
   Wang YL, 2018, PROC CVPR IEEE, P5314, DOI 10.1109/CVPR.2018.00557
   Wu ZH, 2021, IEEE T NEUR NET LEAR, V32, P4, DOI 10.1109/TNNLS.2020.2978386
   Xu M, 2017, IEEE INT CON MULTI, P517, DOI 10.1109/ICME.2017.8019351
   Yan SJ, 2018, AAAI CONF ARTIF INTE, P7444
   Zhang PF, 2020, PROC CVPR IEEE, P1109, DOI 10.1109/CVPR42600.2020.00119
   Zhang PF, 2017, IEEE I CONF COMP VIS, P2136, DOI [10.1109/ICCV.2017.233, 10.1109/ICCV.2017.231]
   Zheng H, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P6236
   Zheng W, 2019, IEEE INT CON MULTI, P826, DOI 10.1109/ICME.2019.00147
NR 56
TC 4
Z9 4
U1 3
U2 20
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2023
VL 39
IS 5
BP 2191
EP 2203
DI 10.1007/s00371-022-02473-7
EA APR 2022
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA D7ZK3
UT WOS:000783083500001
DA 2024-07-18
ER

PT J
AU Patel, AS
   Vyas, R
   Vyas, OP
   Ojha, M
   Tiwari, V
AF Patel, Ashish Singh
   Vyas, Ranjana
   Vyas, O. P.
   Ojha, Muneendra
   Tiwari, Vivek
TI Motion-compensated online object tracking for activity detection and
   crowd behavior analysis
SO VISUAL COMPUTER
LA English
DT Article
DE Object tracking; Pedestrian movement; Activity recognition; Loitering;
   Physical distancing
ID MULTIOBJECT TRACKING; RECOGNITION; MULTIPLE; FLOW; SET
AB It is a nontrivial task to manage crowds in public places and recognize unacceptable behavior (such as violating social distancing norms during the COVID-19 pandemic). In such situations, people should avoid loitering (unnecessary moving out in public places without apparent purpose) and maintain a sufficient physical distance. In this study, a multi-object tracking algorithm has been introduced to improve short-term object occlusion, detection errors, and identity switches. The objects are tracked through bounding box detection and with linear velocity estimation of the object using the Kalman filter frame by frame. The predicted tracks are kept alive for some time, handling the missing detections and short-term object occlusion. ID switches (mainly due to crossing trajectories) are managed by explicitly considering the motion direction of the objects in real time. Furthermore, a novel approach to detect unusual behavior of loitering with a severity level is proposed based on the tracking information. An adaptive algorithm is also proposed to detect physical distance violation based on the object dimensions for the entire length of the track. At last, a mathematical approach to calculate actual physical distance is proposed by using the height of a human as a reference object which adheres more specific distancing norms. The proposed approach is evaluated in traffic and pedestrian movement scenarios. The experimental results demonstrate a significant improvement in the results.
C1 [Patel, Ashish Singh; Ojha, Muneendra; Tiwari, Vivek] Int Inst Informat Technol Naya Raipur, Dept Comp Sci & Engn, Atal Nagar, India.
   [Vyas, Ranjana; Vyas, O. P.] Indian Inst Informat Technol Allahabad, Dept Informat Technol, Prayagraj, India.
C3 Indian Institute of Information Technology Allahabad
RP Patel, AS (corresponding author), Int Inst Informat Technol Naya Raipur, Dept Comp Sci & Engn, Atal Nagar, India.
EM ashish@iiitnr.edu.in; ranajana@iiita.ac.in; dropvyas@gmail.com;
   muneendra@iiitnr.edu.in; vivek@iiitnr.edu.in
RI Ojha, Muneendra/GQZ-0811-2022; Patel, Ashish Singh/HIU-0959-2022
OI Ojha, Muneendra/0000-0002-3913-1185; Patel, Ashish
   Singh/0000-0002-2963-1039
CR Andriyenko A, 2012, PROC CVPR IEEE, P1926, DOI 10.1109/CVPR.2012.6247893
   [Anonymous], 2019, CVPR
   Arivazhagan S, 2019, MULTIMED TOOLS APPL, V78, P10933, DOI 10.1007/s11042-018-6618-9
   Arroyo R, 2015, EXPERT SYST APPL, V42, P7991, DOI 10.1016/j.eswa.2015.06.016
   Bae SH, 2014, PROC CVPR IEEE, P1218, DOI 10.1109/CVPR.2014.159
   Basly H, 2022, VISUAL COMPUT, V38, P993, DOI 10.1007/s00371-021-02064-y
   Ben Mabrouk A, 2018, EXPERT SYST APPL, V91, P480, DOI 10.1016/j.eswa.2017.09.029
   Ben Mabrouk A, 2017, PATTERN RECOGN LETT, V92, P62, DOI 10.1016/j.patrec.2017.04.015
   Benfold B., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3457, DOI 10.1109/CVPR.2011.5995667
   Bernardin K, 2008, EURASIP J IMAGE VIDE, DOI 10.1155/2008/246309
   Betke M, 2007, PROC CVPR IEEE, P192
   Bewley A, 2016, IEEE IMAGE PROC, P3464, DOI 10.1109/ICIP.2016.7533003
   Brau E, 2011, PROC CVPR IEEE, P1137, DOI 10.1109/CVPR.2011.5995736
   Collins RT, 2012, PROC CVPR IEEE, P1744, DOI 10.1109/CVPR.2012.6247870
   Dai JF, 2016, ADV NEUR IN, V29
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Das Dawn D, 2016, VISUAL COMPUT, V32, P289, DOI 10.1007/s00371-015-1066-2
   DEHGHAN A, 2015, PROC CVPR IEEE, P4091, DOI DOI 10.1109/CVPR.2015
   Dicle C, 2013, IEEE I CONF COMP VIS, P2304, DOI 10.1109/ICCV.2013.286
   Erhan D, 2014, PROC CVPR IEEE, P2155, DOI 10.1109/CVPR.2014.276
   Fernandez-Ramírez J, 2020, VISUAL COMPUT, V36, P1535, DOI 10.1007/s00371-019-01754-y
   Ferryman, PETS 2006 BENCHMARK
   FORTMANN TE, 1983, IEEE J OCEANIC ENG, V8, P173, DOI 10.1109/JOE.1983.1145560
   Fu C.-Y., 2017, DSSD: Deconvolutional Single Shot Detector
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Gupta Savyasachi, 2020, 2020 12th International Conference on Computational Intelligence and Communication Networks (CICN), P306, DOI 10.1109/CICN49253.2020.9242628
   He KM, 2020, IEEE T PATTERN ANAL, V42, P386, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   He KM, 2014, LECT NOTES COMPUT SC, V8691, P346, DOI [arXiv:1406.4729, 10.1007/978-3-319-10578-9_23]
   Isard M, 1998, INT J COMPUT VISION, V29, P5, DOI 10.1023/A:1008078328650
   Kalman R.E., 1961, Trans. ASME, V83, P95, DOI [10.1115/1.3658902, DOI 10.1115/1.3658902]
   Kim C, 2015, IEEE I CONF COMP VIS, P4696, DOI 10.1109/ICCV.2015.533
   Li Zhang, 2008, 2008 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops (CVPR Workshops), P1, DOI 10.1109/CVPRW.2008.4563097
   Lin T.Y., 2017, P IEEE C COMPUTER VI, P2117, DOI [10.1109/CVPR.2017.106, DOI 10.1109/CVPR.2017.106]
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Luo WH, 2021, ARTIF INTELL-AMST, V293, DOI 10.1016/j.artint.2020.103448
   Mercaldo F, 2021, IEEE IJCNN, DOI 10.1109/IJCNN52387.2021.9534231
   Milan A., 2016, ARXIV160300831
   Morris BT, 2008, IEEE T CIRC SYST VID, V18, P1114, DOI 10.1109/TCSVT.2008.927109
   Najibi M, 2016, PROC CVPR IEEE, P2369, DOI 10.1109/CVPR.2016.260
   Nam Y, 2015, MULTIMED TOOLS APPL, V74, P2939, DOI 10.1007/s11042-013-1763-7
   NCD Risk Factor Collaboration (NCD-RisC), 2016, Elife, V5, DOI 10.7554/eLife.13410
   Patel AS, 2019, 2019 IEEE C INFORM C, P1, DOI [10.1109/CICT48419.2019.9066256, DOI 10.1109/CICT48419.2019.9066256]
   Patel AS, 2021, SEMANT WEB, V12, P467, DOI 10.3233/SW-200393
   Patino L, 2016, IEEE COMPUT SOC CONF, P1240, DOI 10.1109/CVPRW.2016.157
   Rai H, 2015, ADV INTELL SYST, V366, P789, DOI 10.1007/978-3-319-08422-0_114
   Redmon J., 2016, PROC CVPR IEEE, DOI [10.1109/CVPR.2016.91, DOI 10.1109/CVPR.2016.91]
   Redmon J, 2018, Arxiv, DOI [arXiv:1804.02767, DOI 10.1109/CVPR.2017.690, DOI 10.48550/ARXIV.1804.02767]
   REID DB, 1979, IEEE T AUTOMAT CONTR, V24, P843, DOI 10.1109/TAC.1979.1102177
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rezatofighi SH, 2015, IEEE I CONF COMP VIS, P3047, DOI 10.1109/ICCV.2015.349
   Ristani E, 2016, LECT NOTES COMPUT SC, V9914, P17, DOI 10.1007/978-3-319-48881-3_2
   Saponara S, 2021, J REAL-TIME IMAGE PR, V18, P1937, DOI 10.1007/s11554-021-01070-6
   Shen ZQ, 2017, IEEE I CONF COMP VIS, P1937, DOI 10.1109/ICCV.2017.212
   Son J, 2017, PROC CVPR IEEE, P3786, DOI 10.1109/CVPR.2017.403
   Sugianto N, 2024, INFORM TECHNOL PEOPL, V37, P998, DOI 10.1108/ITP-07-2020-0534
   Thomas A, 2007, IEEE I CONF COMP VIS, P23
   Tu ZG, 2019, IEEE T IMAGE PROCESS, V28, P2799, DOI 10.1109/TIP.2018.2890749
   Vishwakarma S, 2013, VISUAL COMPUT, V29, P983, DOI 10.1007/s00371-012-0752-6
   Wojke N, 2017, IEEE IMAGE PROC, P3645, DOI 10.1109/ICIP.2017.8296962
   Wu B, 2007, INT J COMPUT VISION, V75, P247, DOI 10.1007/s11263-006-0027-7
   Wu Z, 2011, PROC CVPR IEEE, P1185, DOI 10.1109/CVPR.2011.5995515
   Xing JL, 2009, PROC CVPR IEEE, P1200, DOI 10.1109/CVPRW.2009.5206745
   Yoo D, 2015, IEEE I CONF COMP VIS, P2659, DOI 10.1109/ICCV.2015.305
   Yuan Li, 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2953, DOI 10.1109/CVPRW.2009.5206735
   Yunpeng Chang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12360), P329, DOI 10.1007/978-3-030-58555-6_20
   Zuo F, 2021, J TRANSP HEALTH, V21, DOI 10.1016/j.jth.2021.101032
NR 67
TC 11
Z9 11
U1 13
U2 80
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2023
VL 39
IS 5
BP 2127
EP 2147
DI 10.1007/s00371-022-02469-3
EA APR 2022
PG 21
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA D7ZK3
UT WOS:000782355800001
PM 35437336
OA hybrid, Green Published
DA 2024-07-18
ER

PT J
AU Aghamaleki, JA
   Ghorbani, A
AF Aghamaleki, Javad Abbasi
   Ghorbani, Alireza
TI Image fusion using dual tree discrete wavelet transform and weights
   optimization
SO VISUAL COMPUTER
LA English
DT Article
DE Image fusion; Infrared image; Dual tree discrete wavelet transform
   (DT-DWT); Multi-scale decomposition (MSD)
ID VISIBLE IMAGES; REGISTRATION; MODEL
AB Image fusion is a useful context in image processing. It goals to produce more informative image using multi-image data with different sensors. In this study, an effective approach in discrete wavelet transform domain for infrared and visible image fusion is proposed. In fact, important parts of thermal images along with details of visual image must be considered in fused images. Therefore, dual tree discrete wavelet transform is used to extract both subjects based on an optimization process. The optimization considers parts of input images with maximum entropy and minimum mean square error in fused image in comparison with both input images. Experimental results on a standard database demonstrate that proposed method can achieve a superior performance compared with other fusion methods in both subjective and objective assessments.
C1 [Aghamaleki, Javad Abbasi] Damghan Univ, Dept Elect Engn, Damghan, Iran.
   [Ghorbani, Alireza] Islamic Azad Univ, Dept Elect Engn, Sari Branch, Sari, Iran.
C3 Damghan University; Islamic Azad University
RP Ghorbani, A (corresponding author), Islamic Azad Univ, Dept Elect Engn, Sari Branch, Sari, Iran.
EM arghorbani@iausari.ac.ir
CR AbouRayan, 2016, REAL TIME IMAGE FUSI
   Adu JH, 2013, INFRARED PHYS TECHN, V61, P94, DOI 10.1016/j.infrared.2013.07.010
   Bhatnagar G, 2015, NEUROCOMPUTING, V157, P143, DOI 10.1016/j.neucom.2015.01.025
   BURT PJ, 1983, IEEE T COMMUN, V31, P532, DOI 10.1109/TCOM.1983.1095851
   Chandana M., 2011, International Journal of Research and Reviews in Computer Science, V2, P948
   Chen C, 2014, PROC CVPR IEEE, P2760, DOI 10.1109/CVPR.2014.347
   Dong LM, 2015, NEUROCOMPUTING, V159, P268, DOI 10.1016/j.neucom.2015.01.050
   El-Khamy SE, 2005, APPL OPTICS, V44, P7349, DOI 10.1364/AO.44.007349
   Gao Y, 2017, IEEE T IMAGE PROCESS, V26, P2545, DOI 10.1109/TIP.2017.2675341
   Ghaneizad M, 2017, J OPT SOC AM A, V34, P856, DOI 10.1364/JOSAA.34.000856
   Heijmans HJAM, 2000, IEEE T IMAGE PROCESS, V9, P1897, DOI 10.1109/83.877211
   Hou RC, 2020, IEEE T COMPUT IMAG, V6, P640, DOI 10.1109/TCI.2020.2965304
   Hou RC, 2019, MULTIMED TOOLS APPL, V78, P28609, DOI 10.1007/s11042-018-6099-x
   Huang Y, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18041169
   Jin X, 2018, INFRARED PHYS TECHN, V88, P1, DOI 10.1016/j.infrared.2017.10.004
   Kingsbury N, 1999, PHILOS T R SOC A, V357, P2543, DOI 10.1098/rsta.1999.0447
   Kong SG, 2007, INT J COMPUT VISION, V71, P215, DOI 10.1007/s11263-006-6655-0
   Lagarias JC, 1998, SIAM J OPTIMIZ, V9, P112, DOI 10.1137/S1052623496303470
   Leung L.W., 22 AS C REM SENS, P9
   LI H, 1995, GRAPH MODEL IM PROC, V57, P235, DOI 10.1006/gmip.1995.1022
   Li H, 2019, INFRARED PHYS TECHN, V102, DOI 10.1016/j.infrared.2019.103039
   Li YS, 2016, IEEE GEOSCI REMOTE S, V13, P157, DOI 10.1109/LGRS.2015.2503142
   Liu CY, 2016, J OPT SOC AM A, V33, P1267, DOI 10.1364/JOSAA.33.001267
   Ma JY, 2019, INT J COMPUT VISION, V127, P512, DOI [10.1109/TMAG.2017.2763198, 10.1007/s11263-018-1117-z]
   Ma JY, 2017, INFORM SCIENCES, V417, P128, DOI 10.1016/j.ins.2017.07.010
   Ma JY, 2015, PATTERN RECOGN, V48, P772, DOI 10.1016/j.patcog.2014.09.005
   Ma JY, 2013, PATTERN RECOGN, V46, P3519, DOI 10.1016/j.patcog.2013.05.017
   Ma JL, 2017, INFRARED PHYS TECHN, V82, P8, DOI 10.1016/j.infrared.2017.02.005
   Meng FJ, 2016, NEUROCOMPUTING, V177, P1, DOI 10.1016/j.neucom.2015.10.080
   Nencini F, 2007, INFORM FUSION, V8, P143, DOI 10.1016/j.inffus.2006.02.001
   Sadjadi F, 2005, 2005 IEEE COMPUTER S, P8
   Shao ZF, 2012, APPL OPTICS, V51, P1910, DOI 10.1364/AO.51.001910
   Sun CQ, 2020, ELECTRONICS-SWITZ, V9, DOI 10.3390/electronics9122162
   Thung KH, 2009, 2009 INTERNATIONAL CONFERENCE FOR TECHNICAL POSTGRADUATES (TECHPOS 2009), P153
   TOET A, 1989, OPT ENG, V28, P789, DOI 10.1117/12.7977034
   Toet A, 1997, DISPLAYS, V18, P85, DOI 10.1016/S0141-9382(97)00014-0
   TOET A, 1989, PATTERN RECOGN LETT, V9, P255, DOI 10.1016/0167-8655(89)90004-4
   TOET A, 1989, PATTERN RECOGN LETT, V9, P245, DOI 10.1016/0167-8655(89)90003-2
   Wang NY, 2014, NEUROCOMPUTING, V130, P44, DOI 10.1016/j.neucom.2012.12.060
   Wei ZQ, 2017, REMOTE SENS-BASEL, V9, DOI 10.3390/rs9090904
   Yang CC, 2015, APPL OPTICS, V54, P2255, DOI 10.1364/AO.54.002255
   Yang K, 2017, REMOTE SENS-BASEL, V9, DOI 10.3390/rs9060581
   Yang Y, 2015, PATTERN RECOGN, V48, P156, DOI 10.1016/j.patcog.2014.06.017
   Zhang Q, 2016, INFRARED PHYS TECHN, V74, P11, DOI 10.1016/j.infrared.2015.11.003
   Zhou C., 2019, IOP C SERIES EARTH E
   Zhou YJ, 2018, IEEE ACCESS, V6, P79039, DOI 10.1109/ACCESS.2018.2870393
   Zhou ZQ, 2016, APPL OPTICS, V55, P6480, DOI 10.1364/AO.55.006480
NR 47
TC 14
Z9 15
U1 7
U2 29
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2023
VL 39
IS 3
BP 1181
EP 1191
DI 10.1007/s00371-021-02396-9
EA MAR 2022
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 9E6QG
UT WOS:000770509700001
DA 2024-07-18
ER

PT J
AU Wang, XH
   Wu, K
   Zhang, Y
   Xiao, Y
   Xu, PF
AF Wang, Xuanhong
   Wu, Kun
   Zhang, Ying
   Xiao, Yun
   Xu, Pengfei
TI A GAN-based Denoising Method for Chinese Stele and Rubbing Calligraphic
   Image
SO VISUAL COMPUTER
LA English
DT Article
DE Chinese calligraphy; Generative adversarial network; Image denoising;
   Calligraphic images
ID ADVERSARIAL NETWORKS; FILTER; CNN
AB Chinese calligraphic images have important artistic and historical values. However, subjected to hundreds of years of natural weathering, corrosion and man-made destruction, Chinese calligraphic images inevitably contain some special noise, such as dotted noise, flake corrosion noise and scratch noise. How to denoise this special noise is a challenge for digital preservation of Chinese calligraphic. In this paper, we propose an end-to-end calligraphic image denoising algorithm based on a well-designed generative adversarial network. The generator contains a recurrent network and a denoising autoencoder. By introducing an attention mechanism, we use a recurrent network with multiple progressive network units to generate a noise attention map. Through the noise attention map, the denoising autoencoder can restore the noisy calligraphic image into a clean image with reduced noise or even no noise. The extensive experiments results show that the results of our method are better than those of other comparison methods in terms of visual effects, PSNR and SSIM.
C1 [Wang, Xuanhong; Wu, Kun] Xian Univ Posts & Telecommun, Sch Telecommun & Informat Engn, Xian 710121, Shaanxi, Peoples R China.
   [Zhang, Ying; Xiao, Yun; Xu, Pengfei] Northwest Univ, Sch Informat Sci & Technol, Xian 710127, Shaanxi, Peoples R China.
C3 Xi'an University of Posts & Telecommunications; Northwest University
   Xi'an
RP Xiao, Y (corresponding author), Northwest Univ, Sch Informat Sci & Technol, Xian 710127, Shaanxi, Peoples R China.
EM yxiao@nwu.edu.cn
FU NSFC [61972315]
FX This study was funded by the NSFC under Grant No.61972315.
CR [Anonymous], CoRR abs/1511.07122
   Buades A, 2011, IMAGE PROCESS ON LIN, V1, P208, DOI 10.5201/ipol.2011.bcm_nlm
   Cao S, 2021, MATH BIOSCI ENG, V18, P1981, DOI 10.3934/mbe.2021103
   Chen JD, 2006, IEEE T AUDIO SPEECH, V14, P1218, DOI 10.1109/TSA.2005.860851
   Chen JW, 2018, PROC CVPR IEEE, P3155, DOI 10.1109/CVPR.2018.00333
   Chen RP, 2013, IEEE GEOSCI REMOTE S, V10, P826, DOI 10.1109/LGRS.2012.2225594
   Chen YT, 2021, VISUAL COMPUT, V37, P1691, DOI 10.1007/s00371-020-01932-3
   Chu J, 2018, IEEE ACCESS, V6, P19959, DOI 10.1109/ACCESS.2018.2815149
   Ding B, 2019, IEEE I CONF COMP VIS, P10212, DOI 10.1109/ICCV.2019.01031
   Friebe Markus, 2005, 2005 13th European Signal Processing Conference, P1
   Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326
   Fu XY, 2020, IEEE T NEUR NET LEAR, V31, P1794, DOI 10.1109/TNNLS.2019.2926481
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   GOU YB, 2020, ADV NEUR IN, V33, pNI734
   Guo H, 2019, PROC CVPR IEEE, P729, DOI 10.1109/CVPR.2019.00082
   Guo S, 2019, PROC CVPR IEEE, P1712, DOI 10.1109/CVPR.2019.00181
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   HUANG TS, 1979, IEEE T ACOUST SPEECH, V27, P13, DOI 10.1109/TASSP.1979.1163188
   Jiang XB, 2021, VISUAL COMPUT, V37, P2419, DOI 10.1007/s00371-020-01996-1
   Kandemir C, 2015, DIGIT SIGNAL PROCESS, V46, P164, DOI 10.1016/j.dsp.2015.08.012
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Li RD, 2018, PROC CVPR IEEE, P8202, DOI 10.1109/CVPR.2018.00856
   Liu Q, 2016, VISUAL COMPUT, V32, P535, DOI 10.1007/s00371-015-1087-x
   Liu XW, 2013, APPL MECH MATER, V411-414, P1348, DOI 10.4028/www.scientific.net/AMM.411-414.1348
   Mao XJ, 2016, ADV NEUR IN, V29
   Qin Y, 2016, DESTECH TRANS ENG
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Shi XJ, 2015, ADV NEUR IN, V28
   Shi ZH, 2017, MULTIMED TOOLS APPL, V76, P14921, DOI 10.1007/s11042-016-4284-3
   Shi ZH, 2016, MULTIMED TOOLS APPL, V75, P12245, DOI 10.1007/s11042-016-3421-3
   Tian CW, 2020, NEURAL NETWORKS, V124, P117, DOI 10.1016/j.neunet.2019.12.024
   Wang CY, 2018, IEEE T IMAGE PROCESS, V27, P4066, DOI 10.1109/TIP.2018.2836316
   Wang QQ, 2020, SCI CHINA INFORM SCI, V63, DOI 10.1007/s11432-019-2713-1
   Wang XP, 2018, IDEAS HIST MOD CHINA, V19, P1, DOI 10.1163/9789004385580_002
   [肖蕾 XIAO Lei], 2009, [激光杂志, Laser Journal], V30, P44
   Xiao TJ, 2015, PROC CVPR IEEE, P842, DOI 10.1109/CVPR.2015.7298685
   Zhang H, 2020, IEEE T CIRC SYST VID, V30, P3943, DOI 10.1109/TCSVT.2019.2920407
   Zhang JL, 2020, MULTIMED TOOLS APPL, V79, P119, DOI 10.1007/s11042-019-08052-8
   Zhang JL, 2019, INT J DOC ANAL RECOG, V22, P177, DOI 10.1007/s10032-019-00324-1
   Zhang Jun-song, 2006, Journal of Zhejiang University (Science), V7, P1178, DOI 10.1631/jzus.2006.A1178
   Zhang K, 2018, IEEE T IMAGE PROCESS, V27, P4608, DOI 10.1109/TIP.2018.2839891
   Zhang K, 2017, IEEE T IMAGE PROCESS, V26, P3142, DOI 10.1109/TIP.2017.2662206
   Zhang L, 2020, AAAI CONF ARTIF INTE, V34, P12829
   Zhang YQ, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20041010
   Zheng X, 2016, MULTIMED TOOLS APPL, V75, P8719, DOI 10.1007/s11042-015-2788-x
NR 45
TC 2
Z9 2
U1 1
U2 26
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2023
VL 39
IS 4
BP 1351
EP 1362
DI 10.1007/s00371-022-02410-8
EA FEB 2022
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA C3OY5
UT WOS:000753220400001
DA 2024-07-18
ER

PT J
AU Chen, LF
   Zhang, Q
AF Chen, Lifang
   Zhang, Qian
TI DDGCN: graph convolution network based on direction and distance for
   point cloud learning
SO VISUAL COMPUTER
LA English
DT Article
DE Point cloud; Deep learning; Similarity matrix; Graph convolutional
   network
AB Point cloud is usually used to construct the surface shape of three-dimensional geometric objects. Due to the disorder and irregularity of the point cloud, it is still a challenge to fully acquire the semantic features of the point cloud. With the development of graph neural network and graph convolution neural network, researchers are integrating point cloud and graph structure to better represent the semantic features of the point cloud. In this paper, we propose a novel graph convolutional neural network that integrates distance and direction (DDGCN), which constructs a dynamic neighborhood graph by obtaining the similarity matrix of the point cloud, and then uses several multi-layer perceptrons to obtain the local features of the point cloud. For the sake of making the intra-classes in the point cloud data more compact and the spacing between classes larger than the intra-class spacing, we propose a new loss function combined with center loss. The proposed DDGCN has been tested on ModelNet40 dataset, ShapeNet Part dataset and S3DIS dataset, and has achieved state-of-the-art performance in both classification and segmentation tasks.
C1 [Chen, Lifang; Zhang, Qian] Jiangnan Univ, Sch Artificial Intelligence & Comp Sci, Wuxi 214122, Jiangsu, Peoples R China.
C3 Jiangnan University
RP Chen, LF (corresponding author), Jiangnan Univ, Sch Artificial Intelligence & Comp Sci, Wuxi 214122, Jiangsu, Peoples R China.
EM may7366@163.com; qianzhang@stu.jiangnan.edu.cn
CR Armeni I, 2016, PROC CVPR IEEE, P1534, DOI 10.1109/CVPR.2016.170
   Chen Can, 2019, CoRR
   Jiang M., 2018, ARXIV180700652
   King DB, 2015, ACS SYM SER, V1214, P1
   Kipf TN, 2016, ARXIV
   Landrieu L, 2018, PROC CVPR IEEE, P4558, DOI 10.1109/CVPR.2018.00479
   Li GH, 2019, IEEE I CONF COMP VIS, P9266, DOI 10.1109/ICCV.2019.00936
   Li HY, 2021, VISUAL COMPUT, V37, P325, DOI 10.1007/s00371-020-01801-z
   Lin ZH, 2020, PROC CVPR IEEE, P1797, DOI 10.1109/CVPR42600.2020.00187
   Lu Y, 2019, VISUAL COMPUT, V35, P1683, DOI 10.1007/s00371-019-01637-2
   Qi CR, 2017, ADV NEUR IN, V30
   Qi CR, 2017, PROC CVPR IEEE, P77, DOI 10.1109/CVPR.2017.16
   Qingyong Hu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11105, DOI 10.1109/CVPR42600.2020.01112
   Scarselli F, 2009, IEEE T NEURAL NETWOR, V20, P61, DOI 10.1109/TNN.2008.2005605
   Sun YL, 2020, VISUAL COMPUT, V36, P2407, DOI 10.1007/s00371-020-01892-8
   Te GS, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P746, DOI 10.1145/3240508.3240621
   Wang C, 2018, LECT NOTES COMPUT SC, V11208, P56, DOI 10.1007/978-3-030-01225-0_4
   Wang L, 2019, PROC CVPR IEEE, P10288, DOI 10.1109/CVPR.2019.01054
   Wang X, 2019, ADV NEUR IN, V32
   Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362
   Wen YD, 2016, LECT NOTES COMPUT SC, V9911, P499, DOI 10.1007/978-3-319-46478-7_31
   Wu WX, 2019, PROC CVPR IEEE, P9613, DOI 10.1109/CVPR.2019.00985
   Wu ZR, 2015, PROC CVPR IEEE, P1912, DOI 10.1109/CVPR.2015.7298801
   Xu MT, 2021, PROC CVPR IEEE, P3172, DOI 10.1109/CVPR46437.2021.00319
   Xu QG, 2020, PROC CVPR IEEE, P5660, DOI 10.1109/CVPR42600.2020.00570
   Yi L, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980238
   Zhang YX, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P6279, DOI 10.1109/ICASSP.2018.8462291
NR 27
TC 12
Z9 14
U1 2
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2023
VL 39
IS 3
BP 863
EP 873
DI 10.1007/s00371-021-02351-8
EA JAN 2022
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 9E6QG
UT WOS:000745362200001
DA 2024-07-18
ER

PT J
AU Abd El-Hameed, HA
   Ramadan, N
   El-Shafai, W
   Khalaf, AAM
   Ahmed, HEH
   Elkhamy, SE
   Abd El-Samie, FE
AF Abd El-Hameed, Hayam A.
   Ramadan, Noha
   El-Shafai, Walid
   Khalaf, Ashraf A. M.
   Ahmed, Hossam Eldin H.
   Elkhamy, Said E.
   Abd El-Samie, Fathi E.
TI Cancelable biometric security system based on advanced chaotic maps
SO VISUAL COMPUTER
LA English
DT Article
DE Cancelable biometric security; Authentication; Chaotic maps; Fingerprint
   recognition
ID FINGERPRINT; EFFICIENT
AB In recent years, the protection of human biometrics has witnessed an exponential growth. Fingerprint recognition has been utilized for cell phone authentication, biometric passports, and airport security. To improve the fingerprint recognition process, different approaches have been proposed. To keep biometrics away from hacking attempts, non-invertible transformations or encryption algorithms have been proposed to provide cancelable biometric templates for biometric protection. This paper presents a scheme that depends on chaos-based image encryption with different chaotic maps. The chaotic maps are used instead of the simple random number generator to overcome the loss of randomness in the case of a large number of images. To preserve the authentication performance, we should convolve the training images with random kernels to build the encrypted biometric templates. We can obtain different templates from the same biometrics by varying the chaotic map used to generate the convolution kernels. A comparative study is introduced between the used chaotic maps to determine the one, which gives the best performance. The simulation experiments reveal that the enhanced quadratic map 3 achieves the lowest error probability of 3.861% in the cancelable fingerprint recognition system. The cancelable fingerprint recognition system based on this chaotic map achieves the largest probability of detection of 96.139%, with an Equal Error Rate (EER) of 0.593.
C1 [Abd El-Hameed, Hayam A.; Khalaf, Ashraf A. M.] Minia Univ, Dept Elect & Elect Commun, Fac Engn, Al Minya, Egypt.
   [Ramadan, Noha] Ahram Canadian Univ, Dept Elect Engn, 6th Of October, Egypt.
   [El-Shafai, Walid] Prince Sultan Univ, Dept Comp Sci, Secur Engn Lab, Riyadh 11586, Saudi Arabia.
   [Ramadan, Noha; El-Shafai, Walid; Ahmed, Hossam Eldin H.; Abd El-Samie, Fathi E.] Menoufia Univ, Dept Elect & Elect Commun, Fac Elect Engn, Menoufia 32952, Egypt.
   [Elkhamy, Said E.] Alexandria Univ, Dept Commun Engn, Alexandria, Egypt.
   [Abd El-Samie, Fathi E.] Princess Nourah Bint Abdulrahman Univ, Dept Informat Technol, Coll Comp & Informat Sci, Riyadh 21974, Saudi Arabia.
C3 Egyptian Knowledge Bank (EKB); Minia University; Prince Sultan
   University; Egyptian Knowledge Bank (EKB); Menofia University; Egyptian
   Knowledge Bank (EKB); Alexandria University; Princess Nourah bint
   Abdulrahman University
RP Abd El-Hameed, HA (corresponding author), Minia Univ, Dept Elect & Elect Commun, Fac Engn, Al Minya, Egypt.
EM hayamabdalmordy@yahoo.com; eng_noharamadan@yahoo.com;
   walid.elshafai@al-eng-menofia.edu.eg; ashkhalaf@yahoo.com;
   hhossamkh@yahoo.com; elkhamy@ieee.org; fathi_sayed@yahoo.com
RI El-Shafai, Walid/AAG-4796-2021; Sayed, Fathi/HRA-4752-2023; Khalaf,
   Ashraf ِA. M./X-8289-2018
OI El-Shafai, Walid/0000-0001-7509-2120; Sayed, Fathi/0000-0001-8749-9518;
   Khalaf, Ashraf ِA. M./0000-0003-3344-5420
CR Abd El-Samie FE, 2021, APPL OPTICS, V60, P3659, DOI 10.1364/AO.415523
   Abou elazm Lamiaa A., 2020, 2020 Fourth International Conference on I-SMAC (IoT in Social, Mobile, Analytics and Cloud) (I-SMAC), P1145, DOI 10.1109/I-SMAC49090.2020.9243390
   Abou Elazm LA, 2020, MULTIMED TOOLS APPL, V79, P14053, DOI 10.1007/s11042-019-08462-8
   Agarwal R, 2021, VISUAL COMPUT, V37, P1357, DOI 10.1007/s00371-020-01870-0
   Alarifi A, 2020, IEEE ACCESS, V8, P221246, DOI 10.1109/ACCESS.2020.3043689
   Algarni AD, 2020, ENTROPY-SWITZ, V22, DOI 10.3390/e22121361
   Anand V, 2020, IEEE SENS J, V20, P9305, DOI 10.1109/JSEN.2020.2987287
   Badr IS, 2021, DIGIT SIGNAL PROCESS, V116, DOI 10.1016/j.dsp.2021.103103
   Cheung KH, 2005, CISST '05: PROCEEDINGS OF THE 2005 INTERNATIONAL CONFERENCE ON IMAGING SCIENCE, SYSTEMS, AND TECHNOLOGY: COMPUTER GRAPHICS, P40
   Dahia G., 2018, ARXIV181106846
   El-Shafai, 2021, IEEE ACCES
   El-Shafai W, 2021, IEEE ACCESS, V9, P35004, DOI 10.1109/ACCESS.2021.3062403
   Faragallah OS, 2021, OPT LASER ENG, V137, DOI 10.1016/j.optlaseng.2020.106333
   Faragallah OS, 2020, IEEE ACCESS, V8, P167069, DOI 10.1109/ACCESS.2020.3019840
   Gupta S, 2021, VISUAL COMPUT, V37, P447, DOI 10.1007/s00371-020-01814-8
   KANTZ H, 1994, PHYS LETT A, V185, P77, DOI 10.1016/0375-9601(94)90991-1
   Minaee S., 2019, Deep-emotion: Facial expression recognition using attentional convolutional network
   Paul PP, 2014, VISUAL COMPUT, V30, P1059, DOI 10.1007/s00371-013-0907-0
   Ramadan N., 2015, INT J COMPUT APPL, V119, P12
   ROSENSTEIN MT, 1993, PHYSICA D, V65, P117, DOI 10.1016/0167-2789(93)90009-P
   Sandhya M, 2017, INT J PATTERN RECOGN, V31, DOI 10.1142/S0218001417560043
   Wang S, 2014, PATTERN RECOGN, V47, P1321, DOI 10.1016/j.patcog.2013.10.003
   Xu YR, 2019, IEEE T CIRC SYST VID, V29, P2927, DOI 10.1109/TCSVT.2018.2875147
   Yang WC, 2018, PATTERN RECOGN, V78, P242, DOI 10.1016/j.patcog.2018.01.026
   Zakaria Y, 2019, MULTIMED TOOLS APPL, V78, P32333, DOI 10.1007/s11042-019-07824-6
NR 25
TC 22
Z9 22
U1 1
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2022
VL 38
IS 6
BP 2171
EP 2187
DI 10.1007/s00371-021-02276-2
EA SEP 2021
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 1C6YN
UT WOS:000700983100001
DA 2024-07-18
ER

PT J
AU Du, CJ
   Ji, ZP
   Dong, ZK
   Wu, H
   Gao, MY
   He, ZW
AF Du, Chenjie
   Ji, Zhongping
   Dong, Zhekang
   Wu, Han
   Gao, Mingyu
   He, Zhiwei
TI A novel kernelized correlation filter by fusing multiple feature
   response maps, enhanced target re-detection, and improved model updating
   for visual tracking
SO VISUAL COMPUTER
LA English
DT Article
DE Unpredictable challenges; Correlation filter; Enhanced target
   re-detection; Improved model updating
ID OBJECT TRACKING
AB Visual target tracking remains a hard research problem due to many unpredictable challenges (e.g., complete occlusion, out-of-view, and fast motion). This paper proposes a novel kernelized correlation filter architecture based on the fusion of the multiple feature response maps (FRMs), enhanced target re-detection, and improved online model updating. First, the multi-feature fusion strategy reveals that complementary feature information paves the way for effectively improving the target recognition ability. Meanwhile, a novel target re-detection module performs superior in relocating the tracked target and mitigating the model drift problem. Moreover, most existing trackers may indiscriminately update the model when encountering inaccurate predictions, eventually induce undesired model drift and tracking performance decay. Inspiring by the average peak-to-correlation energy, we apply an improved model updating scheme to ascertain the relative fluctuated level of FRMs. This novel model updating method can reject deteriorated samples and improves the discriminative power of the model. As evaluated on the four popular tracking benchmark datasets (including OTB-2013, OTB-2015, UAV123, and LaSOT), benefiting from proposed methods, our tracker obtains better effectiveness and efficiency than that of 28 state-of-the-art competitors (including 17 correlation filter-based competitors and 11 deep learning-based competitors). Meanwhile, our proposed tracker can meet the requirements of limited storage space and real-time performance.
C1 [Du, Chenjie; Dong, Zhekang; Wu, Han; Gao, Mingyu; He, Zhiwei] Hangzhou Dianzi Univ, Sch Elect Informat, Hangzhou, Peoples R China.
   [Ji, Zhongping] Hangzhou Dianzi Univ, Sch Comp, Hangzhou, Peoples R China.
   [Du, Chenjie; Ji, Zhongping; Dong, Zhekang; Wu, Han; Gao, Mingyu; He, Zhiwei] Hangzhou Dianzi Univ, Zhejiang Prov Key Lab Equipment Elect, Hangzhou, Peoples R China.
C3 Hangzhou Dianzi University; Hangzhou Dianzi University; Hangzhou Dianzi
   University
RP He, ZW (corresponding author), Hangzhou Dianzi Univ, Sch Elect Informat, Hangzhou, Peoples R China.; He, ZW (corresponding author), Hangzhou Dianzi Univ, Zhejiang Prov Key Lab Equipment Elect, Hangzhou, Peoples R China.
EM zwhe@hdu.edu.cn
RI Dong, Zhekang/AGY-5660-2022
OI Dong, Zhekang/0000-0003-4639-3834; He, Zhiwei/0000-0001-7264-2019
FU National Natural Science Foundation of China [62001149]; Key R&D Program
   of Zhejiang Province [2020C03098]
FX This research was supported by the National Natural Science Foundation
   of China (No. 62001149), and the Key R&D Program of Zhejiang Province
   (No. 2020C03098)
CR [Anonymous], 2016, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2016.465
   Bai B, 2018, NEUROCOMPUTING, V286, P109, DOI 10.1016/j.neucom.2018.01.068
   Bertinetto L, 2016, PROC CVPR IEEE, P1401, DOI 10.1109/CVPR.2016.156
   Bertinetto L, 2016, LECT NOTES COMPUT SC, V9914, P850, DOI 10.1007/978-3-319-48881-3_56
   Bolme DS, 2010, PROC CVPR IEEE, P2544, DOI 10.1109/CVPR.2010.5539960
   Danelljan M., 2014, BRIT MACH VIS C, P1, DOI DOI 10.5244/C.28.65
   Danelljan M, 2017, PROC CVPR IEEE, P6931, DOI 10.1109/CVPR.2017.733
   Danelljan M, 2017, IEEE T PATTERN ANAL, V39, P1561, DOI 10.1109/TPAMI.2016.2609928
   Danelljan M, 2016, PROC CVPR IEEE, P1430, DOI 10.1109/CVPR.2016.159
   Danelljan M, 2016, LECT NOTES COMPUT SC, V9909, P472, DOI 10.1007/978-3-319-46454-1_29
   Danelljan M, 2015, IEEE I CONF COMP VIS, P4310, DOI 10.1109/ICCV.2015.490
   Danelljan M, 2014, PROC CVPR IEEE, P1090, DOI 10.1109/CVPR.2014.143
   Dong EN, 2021, VISUAL COMPUT, V37, P567, DOI 10.1007/s00371-020-01824-6
   Dong XP, 2018, LECT NOTES COMPUT SC, V11217, P472, DOI 10.1007/978-3-030-01261-8_28
   Dong XW, 2018, IEEE CONF COMPUT
   Fan H, 2019, PROC CVPR IEEE, P5369, DOI 10.1109/CVPR.2019.00552
   Fan H, 2017, IEEE I CONF COMP VIS, P5487, DOI 10.1109/ICCV.2017.585
   Fazl-Ersi E, 2019, VISUAL COMPUT, V35, P1447, DOI 10.1007/s00371-018-1510-1
   Feng W, 2019, IEEE T IMAGE PROCESS, V28, P3232, DOI 10.1109/TIP.2019.2895411
   Galoogahi HK, 2017, IEEE I CONF COMP VIS, P1144, DOI [10.1109/ICCV.2017.128, 10.1109/ICCV.2017.129]
   Gao L, 2019, IET COMPUT VIS, V13, P71, DOI 10.1049/iet-cvi.2018.5138
   Guo Q, 2020, IEEE T IMAGE PROCESS, V29, P2999, DOI 10.1109/TIP.2019.2955292
   Guo Q, 2017, IEEE I CONF COMP VIS, P1781, DOI 10.1109/ICCV.2017.196
   Han YM, 2020, PATTERN RECOGN, V97, DOI 10.1016/j.patcog.2019.107027
   Han ZJ, 2020, IEEE T CIRC SYST VID, V30, P155, DOI 10.1109/TCSVT.2018.2888492
   Henriques JF, 2015, IEEE T PATTERN ANAL, V37, P583, DOI 10.1109/TPAMI.2014.2345390
   Henriques JF, 2012, LECT NOTES COMPUT SC, V7575, P702, DOI 10.1007/978-3-642-33765-9_50
   Huang B, 2020, PATTERN RECOGN, V100, DOI 10.1016/j.patcog.2019.107157
   Huang Z., 2017, P IEEE C COMP VIS PA, P4021, DOI DOI 10.1109/CVPR.2017.510
   Li B, 2018, PROC CVPR IEEE, P8971, DOI 10.1109/CVPR.2018.00935
   Li DD, 2018, IEEE SENS J, V18, P7721, DOI 10.1109/JSEN.2018.2861912
   Li Y, 2015, LECT NOTES COMPUT SC, V8926, P254, DOI 10.1007/978-3-319-16181-5_18
   Liu FH, 2018, IEEE T IMAGE PROCESS, V27, P2777, DOI 10.1109/TIP.2018.2813161
   Lukezic A, 2017, PROC CVPR IEEE, P4847, DOI 10.1109/CVPR.2017.515
   Ma C, 2015, IEEE I CONF COMP VIS, P3074, DOI 10.1109/ICCV.2015.352
   Ma C, 2015, PROC CVPR IEEE, P5388, DOI 10.1109/CVPR.2015.7299177
   Mbelwa JT, 2019, VISUAL COMPUT, V35, P371, DOI 10.1007/s00371-018-1470-5
   Mueller M, 2016, LECT NOTES COMPUT SC, V9905, P445, DOI 10.1007/978-3-319-46448-0_27
   Tao R, 2016, PROC CVPR IEEE, P1420, DOI 10.1109/CVPR.2016.158
   Tian SJ, 2020, VISUAL COMPUT, V36, P1219, DOI 10.1007/s00371-019-01730-6
   Valmadre J, 2017, PROC CVPR IEEE, P5000, DOI 10.1109/CVPR.2017.531
   Wang J, 2018, SIGNAL PROCESS-IMAGE, V63, P44, DOI 10.1016/j.image.2018.01.005
   Wang LJ, 2015, IEEE I CONF COMP VIS, P3119, DOI 10.1109/ICCV.2015.357
   Wang X, 2019, IEEE T CYBERNETICS, V49, P146, DOI 10.1109/TCYB.2017.2768570
   Wang Y, 2020, VISUAL COMPUT, V36, P683, DOI 10.1007/s00371-019-01646-1
   Wu Y, 2015, IEEE T PATTERN ANAL, V37, P1834, DOI 10.1109/TPAMI.2014.2388226
   Wu Y, 2013, PROC CVPR IEEE, P2411, DOI 10.1109/CVPR.2013.312
   Xu TY, 2019, IEEE I CONF COMP VIS, P7949, DOI 10.1109/ICCV.2019.00804
   Yan JR, 2021, MULTIMED TOOLS APPL, V80, P2355, DOI 10.1007/s11042-020-09644-5
   Yunhua Zhang, 2018, Computer Vision - ECCV 2018. 15th European Conference. Proceedings: Lecture Notes in Computer Science (LNCS 11213), P355, DOI 10.1007/978-3-030-01240-3_22
   Zhang DJ, 2020, VISUAL COMPUT, V36, P509, DOI 10.1007/s00371-019-01634-5
   Zhang HY, 2018, VISUAL COMPUT, V34, P41, DOI 10.1007/s00371-016-1310-4
   Zhang JM, 2014, LECT NOTES COMPUT SC, V8694, P188, DOI 10.1007/978-3-319-10599-4_13
   Zhang WC, 2021, VISUAL COMPUT, V37, P881, DOI 10.1007/s00371-020-01839-z
   Zhang ZP, 2019, PROC CVPR IEEE, P4586, DOI 10.1109/CVPR.2019.00472
   Zhao LJ, 2017, VISUAL COMPUT, V33, P1169, DOI 10.1007/s00371-016-1279-z
NR 56
TC 1
Z9 1
U1 0
U2 14
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2022
VL 38
IS 6
BP 1883
EP 1900
DI 10.1007/s00371-021-02247-7
EA JUL 2021
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 1C6YN
UT WOS:000679759600003
DA 2024-07-18
ER

PT J
AU Hu, WH
   Wang, T
   Wang, YS
   Chen, ZY
   Huang, GH
AF Hu, Weihua
   Wang, Tao
   Wang, Yangsai
   Chen, ZiYang
   Huang, Guoheng
TI LE-MSFE-DDNet: a defect detection network based on low-light enhancement
   and multi-scale feature extraction
SO VISUAL COMPUTER
LA English
DT Article
DE Surface detection; Deep learning; Low-light enhancement; Complex scenes;
   Multi-scale feature extraction
AB Surface defect detection of industrial products has become a promising area of research. Among the existing defect detection algorithms, most of the CNN-based methods can achieve the task of defect detection under ideal experimental conditions. However, the accuracy of defect detection is easily affected by the different lighting conditions of the environment and the inconsistency of the defect scale. Therefore, general deep learning methods have difficulties in solving the problem of defect detection in complex scenes. In this paper, a defect detection network based on low-light enhancement and multi-scale feature extraction (LE-MSFE-DDNet) is proposed. There are two blocks in the proposed network, including a low-light enhancement block and a SE-FP block. In the low-light enhancement block, the deep network is applied to enhance light adaptation of the deconstructed low-light feature map in our network. The influence of illumination inconsistency is weakened by the introduction of this block. In the SE-FP block, the dependencies between different channels are combined with the multi-scale feature extraction. The defects with different scales are accurately located through the combination of this block and our network. In addition, a Fine Cans Defect dataset based on the surface of fine cans is collected by this paper to verify the feasibility of the proposed network. The proposed model is compared with the state-of-the-art object detection network and the proposed method achieves 94.3% average accuracy on the Fine Cans Defect dataset. The experimental results show that the proposed method outperforms the state-of-the-art method for surface defect detection.
C1 [Hu, Weihua; Wang, Tao; Wang, Yangsai] Guangdong Univ Technol, Sch Automat, Guangzhou, Guangdong, Peoples R China.
   [Chen, ZiYang; Huang, Guoheng] Guangdong Univ Technol, Sch Comp, Guangzhou, Guangdong, Peoples R China.
C3 Guangdong University of Technology; Guangdong University of Technology
RP Wang, T (corresponding author), Guangdong Univ Technol, Sch Automat, Guangzhou, Guangdong, Peoples R China.
EM wangtaosea@qq.com
FU R&D projects in key areas of Guangdong Province [2018B010109007];
   National Natural Science Foundation of Guangdong Joint Funds [U1801263,
   U1701262, U2001201]; Natural Science Foundation of Guangdong Province
   [2020A1515010890]; projects of science and technology plan of Guangdong
   Province [2017B090901019, 2016B010127005]; Guangdong Provincial Key
   Laboratory of Cyber-Physical System [2020B1212060069]
FX This work was supported by the R&D projects in key areas of Guangdong
   Province (2018B010109007), the National Natural Science Foundation of
   Guangdong Joint Funds (U1801263, U1701262, U2001201), the Natural
   Science Foundation of Guangdong Province (2020A1515010890), the projects
   of science and technology plan of Guangdong Province (2017B090901019,
   2016B010127005). Our work is also supported by the Guangdong Provincial
   Key Laboratory of Cyber-Physical System (2020B1212060069).
CR [Anonymous], 2014, Advances in Neural Information Processing Systems
   Bai XL, 2014, IEEE T IND INFORM, V10, P2135, DOI 10.1109/TII.2014.2359416
   Cai JR, 2018, IEEE T IMAGE PROCESS, V27, P2049, DOI 10.1109/TIP.2018.2794218
   Cai ZW, 2021, IEEE T PATTERN ANAL, V43, P1483, DOI 10.1109/TPAMI.2019.2956516
   Dung CV, 2019, AUTOMAT CONSTR, V99, P52, DOI 10.1016/j.autcon.2018.11.028
   Chan CH, 2000, IEEE T IND APPL, V36, P1267, DOI 10.1109/28.871274
   Chen C, 2018, PROC CVPR IEEE, P3291, DOI 10.1109/CVPR.2018.00347
   Cheng HD, 2004, DIGIT SIGNAL PROCESS, V14, P158, DOI 10.1016/j.dsp.2003.07.002
   Du Y, 2018, LECT NOTES COMPUT SC, V11220, P388, DOI 10.1007/978-3-030-01270-0_23
   Elad M, 2006, IEEE T IMAGE PROCESS, V15, P3736, DOI 10.1109/TIP.2006.881969
   Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326
   Guo XJ, 2017, IEEE T IMAGE PROCESS, V26, P982, DOI 10.1109/TIP.2016.2639450
   He KM, 2020, IEEE T PATTERN ANAL, V42, P386, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   Hoang K, 1997, COMPUT IND, V34, P43, DOI 10.1016/S0166-3615(97)00019-5
   Hu BZ, 2021, IEEE T IMAGE PROCESS, V30, P472, DOI 10.1109/TIP.2020.3036770
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang YB, 2020, VISUAL COMPUT, V36, P85, DOI 10.1007/s00371-018-1588-5
   Huang ZL, 2019, IEEE I CONF COMP VIS, P603, DOI 10.1109/ICCV.2019.00069
   Komodakis N, 2017, P ICLR
   Li H., 2018, P BRIT MACH VIS C
   Liu GH, 2022, VISUAL COMPUT, V38, P639, DOI 10.1007/s00371-020-02040-y
   Liu GH, 2021, VISUAL COMPUT, V37, P515, DOI 10.1007/s00371-020-01820-w
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Liu ZF, 2018, PROCEEDINGS OF THE 2ND INTERNATIONAL CONFERENCE ON GRAPHICS AND SIGNAL PROCESSING (ICGSP 2018), P74, DOI 10.1145/3282286.3282300
   Lore KG, 2017, PATTERN RECOGN, V61, P650, DOI 10.1016/j.patcog.2016.06.008
   Ma L., 2018, DESTECH T COMPUT SCI, DOI [10.12783/dtcse/icmsie2017/18645, DOI 10.12783/DTCSE/ICMSIE2017/18645]
   Mehle A, 2016, MACH VISION APPL, V27, P1087, DOI 10.1007/s00138-016-0797-z
   Park J., 2018, P BRIT MACH VIS PATT
   Park S, 2017, IEEE T CONSUM ELECTR, V63, P178, DOI 10.1109/TCE.2017.014847
   Redmon J, 2018, Arxiv, DOI [arXiv:1804.02767, DOI 10.1109/CVPR.2017.690, DOI 10.48550/ARXIV.1804.02767]
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Shafarenko L, 1997, IEEE T IMAGE PROCESS, V6, P1530, DOI 10.1109/83.641413
   Shen L., 2017, ARXIV171102488
   Shi JH, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20164398
   Song YA, 2018, CHIN AUTOM CONGR, P1563, DOI 10.1109/CAC.2018.8623082
   Soukup D, 2014, LECT NOTES COMPUT SC, V8887, P668, DOI 10.1007/978-3-319-14249-4_64
   Szegedy C, 2014, Arxiv, DOI [arXiv:1312.6199, DOI 10.1109/CVPR.2015.7298594]
   Tabernik D, 2020, J INTELL MANUF, V31, P759, DOI 10.1007/s10845-019-01476-x
   Tanaka M, 2019, I SYMP CONSUM ELECTR
   Tao X, 2018, APPL SCI-BASEL, V8, DOI 10.3390/app8091575
   Tian Z, 2019, IEEE I CONF COMP VIS, P9626, DOI 10.1109/ICCV.2019.00972
   TRAHANIAS PE, 1992, 11TH IAPR INTERNATIONAL CONFERENCE ON PATTERN RECOGNITION, PROCEEDINGS, VOL III, P545, DOI 10.1109/ICPR.1992.202045
   Tsai DM, 2019, IEEE T COMP PACK MAN, V9, P163, DOI 10.1109/TCPMT.2018.2873744
   Urbonas A, 2019, APPL SCI-BASEL, V9, DOI 10.3390/app9224898
   Wang F, 2017, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2017.683
   Wang SL, 2018, PROC CVPR IEEE, P2589, DOI 10.1109/CVPR.2018.00274
   Wei Cui, 2018, 2018 Photonics North (PN), DOI 10.1109/PN.2018.8438843
   Woo S., 2018, P EUR C COMP VIS ECC, DOI DOI 10.1007/978-3-030-01234-2_1
   Yang TJ, 2021, VISUAL COMPUT, V37, P1559, DOI 10.1007/s00371-020-01901-w
   Ying Z., 2017, ARXIV PREPRINT ARXIV
NR 51
TC 19
Z9 20
U1 5
U2 58
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2022
VL 38
IS 11
BP 3731
EP 3745
DI 10.1007/s00371-021-02210-6
EA JUN 2021
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 5X0BS
UT WOS:000667441600001
DA 2024-07-18
ER

PT J
AU Mousas, C
   Kao, D
   Koilias, A
   Rekabdar, B
AF Mousas, Christos
   Kao, Dominic
   Koilias, Alexandros
   Rekabdar, Banafsheh
TI Evaluating virtual reality locomotion interfaces on collision avoidance
   task with a virtual character
SO VISUAL COMPUTER
LA English
DT Article
DE Virtual reality; Locomotion interfaces; Avoidance movement; Natural
   walking; Omnidirectional treadmill; Walk-in-place; Joystick
ID WALKING-IN-PLACE; SPATIAL ORIENTATION; TRAVEL; COORDINATION;
   ENVIRONMENTS; DISTANCE; HUMANS; MOTION; STATIONARY; BEHAVIORS
AB We have evaluated four locomotion interfaces, namely natural walking (NW), omnidirectional treadmill (OT), walk-in-place (WiP), and joystick (JS). In this within-group study, an avoidance movement task with a virtual character was performed by all participants for each examined interface. Our study considers that natural walking is the most realistic method for navigating in a virtual environment and explores the differences across the examined locomotion interfaces by collecting avoidance movement measurements (clearance distance, trajectory length, and trajectory curvature) and self-reported subjective ratings (simulation sickness, usefulness, satisfaction, ease of use, and task load). The results suggest that, despite the fact that the avoidance movement measurements of the WiP, JS, and NW interfaces share similarities, they, more often than not, differ from the measurements of the OT interface, which makes the OT interface unable to provide precise avoidance movement data for our participants. Moreover, the OT interface was rated lower by participants in terms of learning, usability, efficacy, satisfaction, physical demand, and effort. Our study shows that NW, OT, WiP, and JS as locomotion interfaces present several benefits and drawbacks concerning their application in avoidance movement behavior tasks with a virtual character.
C1 [Mousas, Christos] Purdue Univ, Dept Comp Graph Technol, W Lafayette, IN 47907 USA.
   [Kao, Dominic] Purdue Univ, Dept Comp & Informat Technol, W Lafayette, IN 47907 USA.
   [Koilias, Alexandros] Univ Aegean, Dept Cultural Technol & Commun, Mitilini 81100, Greece.
   [Rekabdar, Banafsheh] Southern Illinois Univ, Dept Comp Sci, Carbondale, IL 62901 USA.
C3 Purdue University System; Purdue University; Purdue University System;
   Purdue University; University of Aegean; Southern Illinois University
   System; Southern Illinois University
RP Mousas, C (corresponding author), Purdue Univ, Dept Comp Graph Technol, W Lafayette, IN 47907 USA.
EM cmousas@purdue.edu
RI Mousas, Christos/AGV-3533-2022
OI Mousas, Christos/0000-0003-0955-7959
CR Al Zayer M, 2020, IEEE T VIS COMPUT GR, V26, P2315, DOI 10.1109/TVCG.2018.2887379
   Arechavaleta G, 2006, IEEE-RAS INT C HUMAN, P131, DOI 10.1109/ICHR.2006.321374
   Azmandian M, 2016, 2016 IEEE 2ND WORKSHOP ON EVERYDAY VIRTUAL REALITY (WEVR), P9, DOI 10.1109/WEVR.2016.7859537
   Bailenson JN, 2001, PRESENCE-VIRTUAL AUG, V10, P583, DOI 10.1162/105474601753272844
   Bailenson JN, 2003, PERS SOC PSYCHOL B, V29, P819, DOI 10.1177/0146167203029007002
   Bakker NH, 2003, HUM FACTORS, V45, P160, DOI 10.1518/hfes.45.1.160.27234
   Basili P, 2013, GAIT POSTURE, V37, P385, DOI 10.1016/j.gaitpost.2012.08.003
   Bönsch A, 2016, 2016 IEEE SYMPOSIUM ON 3D USER INTERFACES (3DUI), P145, DOI 10.1109/3DUI.2016.7460045
   Boletsis Costas, 2017, Multimodal Technologies and Interaction, V1, DOI 10.3390/mti1040024
   Borchiellini R, 2018, EUROGRAPHICS SHORT P, P53
   Bowman DA, 1999, PRESENCE-TELEOP VIRT, V8, P618, DOI 10.1162/105474699566521
   Bozgeyikli E, 2016, CHI PLAY 2016: PROCEEDINGS OF THE 2016 ANNUAL SYMPOSIUM ON COMPUTER-HUMAN INTERACTION IN PLAY, P205, DOI 10.1145/2967934.2968105
   Brogan DC, 2003, COMP ANIM CONF PROC, P94, DOI 10.1109/CASA.2003.1199309
   Bruder G, 2015, IEEE T VIS COMPUT GR, V21, P539, DOI 10.1109/TVCG.2015.2391864
   Cardoso JCS, 2019, COMPUT GRAPH-UK, V85, P55, DOI 10.1016/j.cag.2019.09.005
   Cerezo E, 1999, VISUAL COMPUT, V15, P124, DOI 10.1007/s003710050167
   Chance SS, 1998, PRESENCE-TELEOP VIRT, V7, P168, DOI 10.1162/105474698565659
   Cinelli ME, 2008, GAIT POSTURE, V28, P596, DOI 10.1016/j.gaitpost.2008.04.006
   Cirio G, 2013, IEEE T VIS COMPUT GR, V19, P671, DOI 10.1109/TVCG.2013.34
   Cirio G, 2012, IEEE T VIS COMPUT GR, V18, P546, DOI 10.1109/TVCG.2012.60
   Cliburn D., 2009, JOINT VIRT REAL C EG, P117
   Cohen J., 1988, STAT POWER ANAL BEHA
   CUTTING JE, 1995, PSYCHOL REV, V102, P627, DOI 10.1037/0033-295X.102.4.627
   Darken R. P., 1997, Proceedings of the ACM Symposium on User Interface Software and Technology. 10th Annual Symposium. UIST '97, P213, DOI 10.1145/263407.263550
   Ducourant T, 2005, NEUROSCI LETT, V389, P6, DOI 10.1016/j.neulet.2005.06.052
   Fajen BR, 2003, J EXP PSYCHOL HUMAN, V29, P343, DOI 10.1037/0096-1523.29.2.343
   Faul F, 2009, BEHAV RES METHODS, V41, P1149, DOI 10.3758/BRM.41.4.1149
   *FED HIGHW ADM, 2003, US MAN UN TRAFF CONT
   Fink PW, 2007, ACM T APPL PERCEPT, V4, DOI 10.1145/1227134.1227136
   Freitag S, 2014, 2014 IEEE SYMPOSIUM ON 3D USER INTERFACES (3DUI), P119, DOI 10.1109/3DUI.2014.6798852
   Frissen I, 2011, EXP BRAIN RES, V212, P163, DOI 10.1007/s00221-011-2717-9
   Gérin-Lajoie M, 2005, MOTOR CONTROL, V9, P242, DOI 10.1123/mcj.9.3.242
   Hale K.S., 2014, Handbook of virtual environments: Design, implementation, and applications
   HALL ET, 1963, AM ANTHROPOL, V65, P1003, DOI 10.1525/aa.1963.65.5.02a00020
   Hart S. G., 2006, P HUM FACT ERG SOC A, V50, P904, DOI DOI 10.1177/154193120605000909
   Hashemian AM, 2017, LECT NOTES COMPUT SC, V10280, P15, DOI 10.1007/978-3-319-57987-0_2
   Hollman JH, 2006, GAIT POSTURE, V23, P441, DOI 10.1016/j.gaitpost.2005.05.005
   Huang JY, 2003, IEEE T MULTIMEDIA, V5, P39, DOI 10.1109/TMM.2003.808822
   Huber M, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0089589
   Hutchinson JC, 2007, PSYCHOL SPORT EXERC, V8, P233, DOI 10.1016/j.psychsport.2006.03.006
   Iachini T, 2016, J ENVIRON PSYCHOL, V45, P154, DOI 10.1016/j.jenvp.2016.01.004
   Iachini T, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0111511
   Iwata H, 2005, IEEE COMPUT GRAPH, V25, P64, DOI 10.1109/MCG.2005.5
   Iwata H, 1999, IEEE COMPUT GRAPH, V19, P30, DOI 10.1109/38.799737
   Iwata H, 1999, PRESENCE-TELEOP VIRT, V8, P587, DOI 10.1162/105474699566503
   Karamouzas I., 2010, Proceedings of the 17th ACM Symposium on Virtual Reality Software and Technology, VRST '10, P183, DOI DOI 10.1145/1889863
   Keedwell AD, 2015, LATIN SQUARES AND THEIR APPLICATIONS, 2ND EDITION, P1
   Kennedy RS, 1993, The international journal of aviation psychology, V3, P203, DOI [DOI 10.1207/S15327108IJAP0303, 10.1207/s15327108ijap0303_3]
   Kitson A, 2017, P IEEE VIRT REAL ANN, P215, DOI 10.1109/VR.2017.7892253
   Koilias A, 2020, COMPUT ANIMAT VIRT W, V31, DOI 10.1002/cav.1963
   Koilias A, 2020, BEHAV SCI-BASEL, V10, DOI 10.3390/bs10090130
   Koilias A, 2020, COMPUT ANIMAT VIRT W, V31, DOI 10.1002/cav.1928
   Koilias A, 2019, INFORMATICS-BASEL, V6, DOI 10.3390/informatics6020018
   Langbehn E, 2018, PROCEEDINGS OF THE VIRTUAL REALITY INTERNATIONAL CONFERENCE - LAVAL VIRTUAL (ACM VRIC 2018), DOI 10.1145/3234253.3234291
   Langbehn E, 2017, IEEE T VIS COMPUT GR, V23, P1349, DOI 10.1109/TVCG.2017.2657220
   Lapointe JF, 2011, COMPUT HUM BEHAV, V27, P2186, DOI 10.1016/j.chb.2011.06.014
   Lee CH, 2009, VISUAL COMPUT, V25, P1009, DOI 10.1007/s00371-009-0356-y
   Lemercier S, 2012, COMPUT GRAPH FORUM, V31, P489, DOI 10.1111/j.1467-8659.2012.03028.x
   Lund A M., 2001, USABILITY INTERFACE, V8, P3, DOI DOI 10.1177/1078087402250360
   Magnenat-Thalmann N, 2005, VISUAL COMPUT, V21, P997, DOI 10.1007/s00371-005-0363-6
   Marcora SM, 2009, J APPL PHYSIOL, V106, P857, DOI 10.1152/japplphysiol.91324.2008
   Matthies D.J., 2014, ARXIV PREPRINT ARXIV
   Medina Eliana, 2008, Proceedings of the Human Factors and Ergonomics Society. 52nd Annual Meeting, P2102, DOI 10.1518/107118108X352490
   Moghadam K, 2020, IEEE T VIS COMPUT GR, V26, P2273, DOI 10.1109/TVCG.2018.2884468
   Mousas C, 2021, 2021 IEEE VIRTUAL REALITY AND 3D USER INTERFACES (VR), P40, DOI 10.1109/VR50410.2021.00024
   Mousas C, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR 2020), P626, DOI [10.1109/VR46266.2020.00-19, 10.1109/VR46266.2020.1581211592060]
   Mousas C, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P726, DOI [10.1109/VR.2019.8798043, 10.1109/vr.2019.8798043]
   Mousas C, 2017, SENSORS-BASEL, V17, DOI 10.3390/s17112589
   Mousas C, 2017, 3D RES, V8, DOI 10.1007/s13319-017-0124-0
   Multon F, 2001, VISUAL COMPUT, V17, P91, DOI 10.1007/s003710000086
   Nabiyouni Mahdi, 2015, 2015 IEEE Symposium on 3D User Interfaces (3DUI), P3, DOI 10.1109/3DUI.2015.7131717
   NCD Risk Factor Collaboration (NCD-RisC), 2016, Elife, V5, DOI 10.7554/eLife.13410
   Neth C., 2010, JOINT VIRT REAL C EU, P1
   Oh K, 2018, GAIT POSTURE, V65, P157, DOI 10.1016/j.gaitpost.2018.07.175
   Olivier Anne-Helene, 2018, IEEE Transactions on Visualization and Computer Graphics, V24, P2251, DOI 10.1109/TVCG.2017.2714665
   Olivier AH, 2013, GAIT POSTURE, V38, P751, DOI 10.1016/j.gaitpost.2013.03.017
   Olivier AH, 2012, GAIT POSTURE, V36, P399, DOI 10.1016/j.gaitpost.2012.03.021
   Park HS, 2013, 2013 13TH INTERNATIONAL CONFERENCE ON CONTROL, AUTOMATION AND SYSTEMS (ICCAS 2013), P1436, DOI 10.1109/ICCAS.2013.6704111
   Patla AE, 1996, J MOTOR BEHAV, V28, P35, DOI 10.1080/00222895.1996.9941731
   Peck TC, 2011, P IEEE VIRT REAL ANN, P55, DOI 10.1109/VR.2011.5759437
   Perrinet J, 2013, P ACM S APPL PERC, P59
   Peters C, 2009, IEEE COMPUT GRAPH, V29, P54, DOI 10.1109/MCG.2009.69
   Pettre J., 2011, SIGGRAPH AS 2011 EM, P1
   Pham QC, 2007, EUR J NEUROSCI, V26, P2391, DOI 10.1111/j.1460-9568.2007.05835.x
   Razzaque S., 2002, Virtual Environments 2002. Eurographics Workshop Proceedings, P123
   Razzaque S., 2005, REDIRECTED WALKING
   Riecke BE, 2010, LECT NOTES ARTIF INT, V6222, P234, DOI 10.1007/978-3-642-14749-4_21
   Ruddle RA, 2006, PSYCHOL SCI, V17, P460, DOI 10.1111/j.1467-9280.2006.01728.x
   Ruddle RA, 2013, ACM T APPL PERCEPT, V10, DOI 10.1145/2465780.2465785
   Ruddle RA, 2011, ACM T COMPUT-HUM INT, V18, DOI 10.1145/1970378.1970384
   Ruddle RA, 2009, ACM T COMPUT-HUM INT, V16, DOI 10.1145/1502800.1502805
   Sanz FA, 2015, P IEEE VIRT REAL ANN, P75, DOI 10.1109/VR.2015.7223327
   Schrammel F, 2009, PSYCHOPHYSIOLOGY, V46, P922, DOI 10.1111/j.1469-8986.2009.00831.x
   Schwaiger M., 2007, Proc. IEEE International Workshop on Haptic Audio Visual Environments and their Applications, P50
   Silva WS, 2018, GAIT POSTURE, V61, P294, DOI 10.1016/j.gaitpost.2018.01.028
   Slater Mel, 1995, ACM Transactions on Computer-Human Interaction, V2, P201, DOI DOI 10.1145/210079.210084
   Souman JL, 2011, ACM T APPL PERCEPT, V8, DOI 10.1145/2043603.2043607
   Steinicke F., 2008, Proceedings of the Virtual Reality International Conference (VRIC), P15
   Steinicke F, 2010, IEEE T VIS COMPUT GR, V16, P17, DOI 10.1109/TVCG.2009.62
   Steinicke F, 2008, PROCEEDINGS OF THE 2008 INTERNATIONAL CONFERENCE ON CYBERWORLDS, P217, DOI 10.1109/CW.2008.53
   Suma EA, 2012, IEEE VIRTUAL REALITY CONFERENCE 2012 PROCEEDINGS, P43, DOI 10.1109/VR.2012.6180877
   Suma EA, 2010, IEEE T VIS COMPUT GR, V16, P690, DOI 10.1109/TVCG.2009.93
   Suryajaya M., 2009, Proceedings of the 16th ACM Symposium on Virtual Reality Software and Technology, P31, DOI DOI 10.1145/1643928.1643938
   Tajima Y, 2002, PHYSICA A, V313, P709, DOI 10.1016/S0378-4371(02)00965-2
   Templeman JN, 1999, PRESENCE-TELEOP VIRT, V8, P598, DOI 10.1162/105474699566512
   Terziman L., 2011, EUROVR EGVE JOINT VI
   Terziman L., 2010, P 17 ACM S VIRT REAL, P27, DOI DOI 10.1145/1889863.1889867
   Tregillus S, 2016, 34TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, CHI 2016, P1250, DOI 10.1145/2858036.2858084
   Usoh M, 1999, COMP GRAPH, P359, DOI 10.1145/311535.311589
   Vallis LA, 2003, EXP BRAIN RES, V152, P409, DOI 10.1007/s00221-003-1558-6
   van Basten BJH, 2009, LECT NOTES COMPUT SC, V5884, P29, DOI 10.1007/978-3-642-10347-6_3
   Waller D., 2013, HDB SPATIAL COGNITIO, P3, DOI [DOI 10.1007/978-1-4419-8432-6_1, DOI 10.1007/978-1-4419-8432-61]
   Warren W. H., 2003, J VISION, V3, P134
   Warren WH, 2001, NAT NEUROSCI, V4, P213, DOI 10.1038/84054
   Wendt JD, 2010, P IEEE VIRT REAL ANN, P51
   Whitton MC, 2005, P IEEE VIRT REAL ANN, P123
   Williams B, 2011, ACM T APPL PERCEPT, V8, DOI 10.1145/2010325.2010329
   Wilson PT, 2016, PROCEEDINGS VRCAI 2016: 15TH ACM SIGGRAPH CONFERENCE ON VIRTUAL-REALITY CONTINUUM AND ITS APPLICATIONS IN INDUSTRY, P243, DOI 10.1145/3013971.3014010
   Zanbaka CA, 2005, IEEE T VIS COMPUT GR, V11, P694, DOI 10.1109/TVCG.2005.92
   Zielasko D, 2016, 2016 IEEE SYMPOSIUM ON 3D USER INTERFACES (3DUI), P113, DOI 10.1109/3DUI.2016.7460040
NR 120
TC 10
Z9 11
U1 0
U2 8
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2021
VL 37
IS 9-11
SI SI
BP 2823
EP 2839
DI 10.1007/s00371-021-02202-6
EA JUN 2021
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA UL2RI
UT WOS:000661027800001
DA 2024-07-18
ER

PT J
AU Fang, ZY
   Zhao, M
   Yu, ZT
   Li, MY
   Yang, Y
AF Fang, Zhengyun
   Zhao, Ming
   Yu, Zhengtao
   Li, Meiyu
   Yang, Yong
TI A guiding teaching and dual adversarial learning framework for a single
   image dehazing
SO VISUAL COMPUTER
LA English
DT Article
DE Image dehazing; Knowledge distillation; Adversarial learning; Teacher
   stream; Student stream
ID FUSION; NETWORK
AB In most existing deep learning-based image dehazing methods, the haze-free source images are only used as the ground truth for the design of the loss function, whereas the guiding role that the source image should play on different feature levels has been ignored. This will result in a sub-optimal dehazing output. To address this issue, inspired by the knowledge distillation, a guiding teaching framework is designed for single image dehazing in an end-to-end manner, where the features of the haze-free source image at different levels are completely used to promoting the restoration of the hazy image. Specifically, the framework consists of a two-stream convolutional neural network termed teacher stream (TS) and student stream (SS), respectively. The input of the former is a haze-free image while the output is the desired image after reconstruction. The input of the latter is the hazy image, and the output is the restored image. Moreover, a dual adversarial strategy is designed to further improve the ability of SS to imitate teacher stream. In this process, the output results of SS are divided into two categories according to their hazy intensity levels. Then a thick light discriminator is introduced and made against the SS pit, such that the images with better dehazing effects can be used to deal with the ones poorly dehazed. A second discriminator termed light clear discriminator (LCD) is further introduced and a minimax game between the LCD and the SS is defined to drive the final result produced by SS closer to the reconstruction result of the TS. Experimental results show that the proposed method outperforms several latest methods applied to both artificial hazy images and the hazy images from the real scene.
C1 [Fang, Zhengyun] Kunming Univ Sci & Technol, Coll Land Resource Engn, Kunming 650500, Yunnan, Peoples R China.
   [Zhao, Ming; Li, Meiyu; Yang, Yong] Elect Power Res Inst Yunnan Power Grid Co Ltd, Kunming 650217, Yunnan, Peoples R China.
   [Yu, Zhengtao] Kunming Univ Sci & Technol, Fac Informat Engn & Automat, Kunming 650500, Yunnan, Peoples R China.
   [Yu, Zhengtao] Kunming Univ Sci & Technol, Yunnan Key Lab Artificial Intelligence, Kunming 650500, Yunnan, Peoples R China.
C3 Kunming University of Science & Technology; Kunming University of
   Science & Technology; Kunming University of Science & Technology
RP Yu, ZT (corresponding author), Kunming Univ Sci & Technol, Fac Informat Engn & Automat, Kunming 650500, Yunnan, Peoples R China.; Yu, ZT (corresponding author), Kunming Univ Sci & Technol, Yunnan Key Lab Artificial Intelligence, Kunming 650500, Yunnan, Peoples R China.
EM ztyu@hotmail.com
FU Science and Technology Project of Yunnan Power Grid Co., Ltd. [YNKJXM
   20190729]; National Key Research and Development Plan Project
   [2018YFC0830105, 2018YFC0830100]
FX This work is partly supported by the Science and Technology Project of
   Yunnan Power Grid Co., Ltd. (No. YNKJXM 20190729) and the National Key
   Research and Development Plan Project (Nos. 2018YFC0830105 and
   2018YFC0830100).
CR Algabri M, 2020, IEEE ACCESS, V8, P54663, DOI 10.1109/ACCESS.2020.2980452
   Ba LJ, 2014, ADV NEUR IN, V27
   Berman D, 2016, PROC CVPR IEEE, P1674, DOI 10.1109/CVPR.2016.185
   Cai BL, 2016, IEEE T IMAGE PROCESS, V25, P5187, DOI 10.1109/TIP.2016.2598681
   Chen DD, 2019, IEEE WINT CONF APPL, P1375, DOI 10.1109/WACV.2019.00151
   Chen JW, 2018, PROC CVPR IEEE, P3155, DOI 10.1109/CVPR.2018.00333
   Chen ZH, 2020, VISUAL COMPUT, V36, P2189, DOI 10.1007/s00371-020-01929-y
   Choi LK, 2015, IEEE T IMAGE PROCESS, V24, P3888, DOI 10.1109/TIP.2015.2456502
   Dean J., 2015, NIPS DEEP LEARNING R
   Dong Y, 2020, AAAI CONF ARTIF INTE, V34, P10729
   Engin D, 2018, IEEE COMPUT SOC CONF, P938, DOI 10.1109/CVPRW.2018.00127
   Feng J, 2019, INT GEOSCI REMOTE SE, P588, DOI [10.1109/igarss.2019.8897819, 10.1109/IGARSS.2019.8897819]
   Ge SM, 2020, IEEE T IMAGE PROCESS, V29, P6898, DOI 10.1109/TIP.2020.2995049
   Ge SM, 2019, IEEE T IMAGE PROCESS, V28, P2051, DOI 10.1109/TIP.2018.2883743
   Guo F, 2020, NEUROCOMPUTING, V378, P9, DOI 10.1016/j.neucom.2019.09.094
   Ha E., 2020, 2020 IEEE INT C CONS, P1
   He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168
   Heo B, 2019, IEEE I CONF COMP VIS, P1921, DOI 10.1109/ICCV.2019.00201
   Hong M, 2020, PROC CVPR IEEE, P3459, DOI 10.1109/CVPR42600.2020.00352
   Ignatov A, 2017, IEEE I CONF COMP VIS, P3297, DOI 10.1109/ICCV.2017.355
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Khmag A, 2018, VISUAL COMPUT, V34, P675, DOI 10.1007/s00371-017-1406-5
   Komodakis N, 2017, P ICLR
   Kupyn O, 2018, PROC CVPR IEEE, P8183, DOI 10.1109/CVPR.2018.00854
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Li B., 2017, ICCV, P4770
   Li BY, 2019, IEEE T IMAGE PROCESS, V28, P492, DOI 10.1109/TIP.2018.2867951
   Li BY, 2017, IEEE I CONF COMP VIS, P4780, DOI 10.1109/ICCV.2017.511
   Li HF, 2020, INFORM SCIENCES, V523, P14, DOI 10.1016/j.ins.2020.03.009
   Li HF, 2020, IEEE T INSTRUM MEAS, V69, P1082, DOI 10.1109/TIM.2019.2912239
   Li HF, 2018, PATTERN RECOGN, V79, P130, DOI 10.1016/j.patcog.2018.02.005
   [李权合 Li Quanhe], 2014, [自动化学报, Acta Automatica Sinica], V40, P744
   Li R., 2018, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2018.00856
   Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151
   Liu RS, 2019, IEEE T NEUR NET LEAR, V30, P2973, DOI 10.1109/TNNLS.2018.2862631
   Liu XH, 2019, IEEE I CONF COMP VIS, P7313, DOI 10.1109/ICCV.2019.00741
   MANNOS JL, 1974, IEEE T INFORM THEORY, V20, P525, DOI 10.1109/TIT.1974.1055250
   Mehta A, 2020, IEEE COMPUT SOC CONF, P846, DOI 10.1109/CVPRW50498.2020.00114
   Meng GF, 2013, IEEE I CONF COMP VIS, P617, DOI 10.1109/ICCV.2013.82
   Minghong Xie, 2019, IEEE Access, V7, P174092, DOI 10.1109/ACCESS.2019.2957165
   Nah S, 2017, PROC CVPR IEEE, P257, DOI 10.1109/CVPR.2017.35
   Pan YT, 2019, NEUROCOMPUTING, V332, P137, DOI 10.1016/j.neucom.2018.12.025
   Qin X, 2020, AAAI CONF ARTIF INTE, V34, P11908
   Ren WQ, 2018, PROC CVPR IEEE, P3253, DOI 10.1109/CVPR.2018.00343
   Ren WQ, 2016, LECT NOTES COMPUT SC, V9906, P154, DOI 10.1007/978-3-319-46475-6_10
   Romero A., 2014, ARXIV14126550
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Shao YJ, 2020, PROC CVPR IEEE, P2805, DOI 10.1109/CVPR42600.2020.00288
   Song YF, 2018, IEEE T MULTIMEDIA, V20, P1548, DOI 10.1109/TMM.2017.2771472
   Swami K, 2018, INT C PATT RECOG, P3061, DOI 10.1109/ICPR.2018.8545522
   Tang GY, 2019, IEEE INT CONF BIG DA, P5062, DOI 10.1109/BigData47090.2019.9006075
   Tong S., 2019, ARXIV190712282
   Wang JB, 2015, NEUROCOMPUTING, V149, P718, DOI 10.1016/j.neucom.2014.08.005
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wu HY, 2020, IEEE COMPUT SOC CONF, P1975, DOI 10.1109/CVPRW50498.2020.00247
   Yang C, 2018, ARXIV180505551
   Yang D, 2018, LECT NOTES COMPUT SC, V11211, P729, DOI 10.1007/978-3-030-01234-2_43
   Yang F, 2022, VISUAL COMPUT, V38, P1579, DOI 10.1007/s00371-021-02089-3
   Yang XT, 2018, AAAI CONF ARTIF INTE, P7485
   Yim J, 2017, PROC CVPR IEEE, P7130, DOI 10.1109/CVPR.2017.754
   Zhang H, 2020, IEEE T CIRC SYST VID, V30, P3943, DOI 10.1109/TCSVT.2019.2920407
   Zhang H, 2018, PROC CVPR IEEE, P3194, DOI 10.1109/CVPR.2018.00337
   Zhang J, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P984, DOI 10.1145/3240508.3240653
   Zhang SD, 2020, NEUROCOMPUTING, V410, P363, DOI 10.1016/j.neucom.2020.06.041
   Zhang SD, 2020, VISUAL COMPUT, V36, P1797, DOI 10.1007/s00371-019-01774-8
   Zhang SD, 2020, VISUAL COMPUT, V36, P305, DOI 10.1007/s00371-018-1612-9
   Zhang WL, 2019, IEEE I CONF COMP VIS, P3096, DOI 10.1109/ICCV.2019.00319
   Zhang YF, 2020, SIGNAL PROCESS, V167, DOI 10.1016/j.sigpro.2019.107327
   Zheng MY, 2020, IEEE SENS J, V20, P8062, DOI 10.1109/JSEN.2020.2981719
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zhu ZQ, 2021, IEEE T INSTRUM MEAS, V70, DOI 10.1109/TIM.2020.3024335
NR 71
TC 7
Z9 7
U1 1
U2 16
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2022
VL 38
IS 11
BP 3563
EP 3575
DI 10.1007/s00371-021-02184-5
EA JUN 2021
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 5X0BS
UT WOS:000660360700001
DA 2024-07-18
ER

PT J
AU Zatout, C
   Larabi, S
AF Zatout, Chayma
   Larabi, Slimane
TI Semantic scene synthesis: application to assistive systems
SO VISUAL COMPUTER
LA English
DT Article
DE Semantic labeling; Depth image; Assistive systems; Point cloud
   classification; Deep learning; Scene synthesis
ID POINT CLOUDS; SEGMENTATION; ENVIRONMENT; NETWORK
AB The aim of this work is to provide a semantic scene synthesis from a single depth image. This is used in assistive aid systems for visually impaired and blind people that allows them to understand their surroundings by the touch sense. The fact that blind people use touch to recognize objects and rely on listening to replace sight motivated us to propose this work. First, the acquired depth image is segmented and each segment is classified in the context of assistive systems using a deep learning network. Second, inspired by the Braille system and the Japanese writing system Kanji, the obtained classes are coded with semantic labels. The scene is then synthesized using these labels and the extracted geometric features. Our system is able to predict more than 17 classes only by understanding the provided illustrative labels. For the remaining objects, their geometric features are transmitted. The labels and the geometric features are mapped on a synthesis area to be sensed by the touch sense. Experiments are conducted on noisy and incomplete data including acquired depth images of indoor scenes and public datasets. The obtained results are reported and discussed.
C1 [Zatout, Chayma; Larabi, Slimane] USTHB Univ, RIIMA Lab, Algiers 16111, Algeria.
C3 University Science & Technology Houari Boumediene
RP Larabi, S (corresponding author), USTHB Univ, RIIMA Lab, Algiers 16111, Algeria.
EM slarabi@usthb.dz
RI Larabi, Slimane/AAD-7871-2020
OI larabi, slimane/0000-0001-8994-5980
CR Bai JQ, 2019, ELECTRONICS-SWITZ, V8, DOI 10.3390/electronics8060697
   Bai JQ, 2017, IEEE T CONSUM ELECTR, V63, P258, DOI 10.1109/TCE.2017.014980
   Blosca JM, 2008, ISPRS J PHOTOGRAMM, V63, P84, DOI 10.1016/j.isprsjprs.2007.07.010
   Cardin S, 2007, VISUAL COMPUT, V23, P109, DOI 10.1007/s00371-006-0032-4
   Cupec R, 2020, PATTERN RECOGN, V102, DOI 10.1016/j.patcog.2020.107199
   Czerniawski T, 2018, AUTOMAT CONSTR, V88, P44, DOI 10.1016/j.autcon.2017.12.029
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Gould, ARXIV191201800
   Hazirbas C, 2017, LECT NOTES COMPUT SC, V10111, P213, DOI 10.1007/978-3-319-54181-5_14
   Hsueh-Cheng Wang, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P6533, DOI 10.1109/ICRA.2017.7989772
   Jafri R, 2014, VISUAL COMPUT, V30, P1197, DOI 10.1007/s00371-013-0886-1
   Jeamwatthanachai Watthanasak., 2017, International Journal of Chaotic Computing, V4, P70, DOI DOI 10.20533/IJCC.2046.3359.2016.0009
   Jie S., 2006, ASPRS 2006 ANN C
   Kuçak RA, 2017, INT ARCH PHOTOGRAMM, V42-1, P595, DOI 10.5194/isprs-archives-XLII-1-W1-595-2017
   Kumawat S, 2019, PROC CVPR IEEE, P4898, DOI 10.1109/CVPR.2019.00504
   Larabi S., 2019, DATASET GROUND DETEC
   Leo M, 2017, COMPUT VIS IMAGE UND, V154, P1, DOI 10.1016/j.cviu.2016.09.001
   Li JX, 2018, PROC CVPR IEEE, P9397, DOI 10.1109/CVPR.2018.00979
   Liu YC, 2019, PROC CVPR IEEE, P8887, DOI 10.1109/CVPR.2019.00910
   Maturana D, 2015, IEEE INT C INT ROBOT, P922, DOI 10.1109/IROS.2015.7353481
   Melzer T, 2007, J APPL GEOD, V1, P159, DOI 10.1515/JAG.2007.018
   Milz S, 2019, LECT NOTES COMPUT SC, V11824, P387, DOI 10.1007/978-3-030-33676-9_27
   Perez-Yus A, 2017, COMPUT VIS IMAGE UND, V154, P192, DOI 10.1016/j.cviu.2016.04.007
   Qi C., 2017, NeurIPS, P5105, DOI [10.1109/ CVPR.2017.16, DOI 10.1109/CVPR.2017.16]
   Rivera-Rubio J, 2016, COMPUT VIS IMAGE UND, V149, P126, DOI 10.1016/j.cviu.2016.02.014
   Szegedy C, 2014, Arxiv, DOI [arXiv:1312.6199, DOI 10.1109/CVPR.2015.7298594]
   Tapu R, 2020, PATTERN RECOGN LETT, V137, P37, DOI 10.1016/j.patrec.2018.10.031
   Wang CX, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19010172
   Wu BC, 2018, IEEE INT CONF ROBOT, P1887
   Wu PW, 2019, IEEE I CONF COMP VIS, P5913, DOI 10.1109/ICCV.2019.00601
   Wu ZR, 2015, PROC CVPR IEEE, P1912, DOI 10.1109/CVPR.2015.7298801
   Zatout C, 2019, IEEE INT CONF COMP V, P4376, DOI 10.1109/ICCVW.2019.00538
   Zhe Wang, 2014, MultiMedia Modeling. 20th Anniversary International Conference, MMM 2014. Proceedings: LNCS 8325, P449, DOI 10.1007/978-3-319-04114-8_38
   Zhu, ARXIV190808854
NR 34
TC 5
Z9 5
U1 2
U2 7
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2022
VL 38
IS 8
BP 2691
EP 2705
DI 10.1007/s00371-021-02147-w
EA MAY 2021
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 3B2OE
UT WOS:000646474900001
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Guo, C
   Yang, DD
   Li, C
   Song, P
AF Guo, Chang
   Yang, Dedong
   Li, Chang
   Song, Peng
TI Dual Siamese network for RGBT tracking via fusing predicted position
   maps
SO VISUAL COMPUTER
LA English
DT Article
DE Visual tracking; RGBT tracking; Deep learning; Response fusion;
   Multimodal
AB Visual object tracking is a basic task in the field of computer vision. Despite the rapid development of visual object tracking, it is not reliable to use only visible light images for object tracking in some cases. Since visible light and thermal infrared images have complementary advantages in imaging, and the use of them as a joint input for tracking becomes more noted, this kind of tracking is RGBT tracking. The existing RGBT tracking can be divided into image-level fusion tracking, feature-level fusion tracking, and response-level fusion tracking. Compared with the first two, response-level fusion tracking can use deeper dual-mode image information, but most of them use traditional tracking methods and introduce weights at inappropriate stages. Based on the above, we propose a response-level fusion tracking algorithm that employed deep learning. And the weight distribution is placed in the feature extraction stage, for which we design the joint modal channel attention module. We adopt the Siamese framework and expand it into a dual Siamese subnetwork. In the meantime, we improve the regional proposal subnetwork and propose the strategy for fusing two modal predicted position maps. To verify the performance of our algorithm, we conducted experiments on two tracking benchmarks. After testing, our algorithm has very good performance and runs at 116 frames per second, which far exceeds the real-time requirement of 25 frames per second.
C1 [Guo, Chang; Yang, Dedong; Li, Chang; Song, Peng] Hebei Univ Technol, Sch Artificial Intelligence, Tianjin 300401, Peoples R China.
C3 Hebei University of Technology
RP Yang, DD (corresponding author), Hebei Univ Technol, Sch Artificial Intelligence, Tianjin 300401, Peoples R China.
EM guoc1209@foxmail.com; dedongyang@hebut.edu.cn
OI Yang, Dedong/0000-0001-7950-6810
FU Natural Science Foundation of Hebei Province [F2017202009]; Hebei
   Province Innovation Capability Improvement Plan [18961604H]
FX This work was supported by the Natural Science Foundation of Hebei
   Province (F2017202009), and Hebei Province Innovation Capability
   Improvement Plan (18961604H).
CR [Anonymous], 2017, P IEEE C COMP VIS PA, DOI DOI 10.1109/CVPR.2017.683
   Bertinetto L, 2016, LECT NOTES COMPUT SC, V9914, P850, DOI 10.1007/978-3-319-48881-3_56
   Chan AL, 2013, OPT ENG, V52, DOI 10.1117/1.OE.52.1.017004
   Chan AL, 2012, PROC SPIE, V8392, DOI 10.1117/12.918373
   Chen ZD, 2020, PROC CVPR IEEE, P6667, DOI 10.1109/CVPR42600.2020.00670
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Danelljan M., 2014, BRIT MACH VIS C, P1, DOI DOI 10.5244/C.28.65
   Danelljan M, 2015, IEEE I CONF COMP VIS, P4310, DOI 10.1109/ICCV.2015.490
   Hare S, 2016, IEEE T PATTERN ANAL, V38, P2096, DOI 10.1109/TPAMI.2015.2509974
   Henriques J, 2012, LNCS, P702, DOI DOI 10.1007/978-3-642-33765-9_50
   Henriques JF, 2015, IEEE T PATTERN ANAL, V37, P583, DOI 10.1109/TPAMI.2014.2345390
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Jaderberg M, 2015, ADV NEUR IN, V28
   Jung I, 2018, LECT NOTES COMPUT SC, V11208, P89, DOI 10.1007/978-3-030-01225-0_6
   Li B, 2019, PROC CVPR IEEE, P4277, DOI 10.1109/CVPR.2019.00441
   Li B, 2018, PROC CVPR IEEE, P8971, DOI 10.1109/CVPR.2018.00935
   Li CL, 2019, PATTERN RECOGN, V96, DOI 10.1016/j.patcog.2019.106977
   Li CL, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1856, DOI 10.1145/3123266.3123289
   Li CL, 2018, NEUROCOMPUTING, V281, P78, DOI 10.1016/j.neucom.2017.11.068
   Li CL, 2016, IEEE T IMAGE PROCESS, V25, P5743, DOI 10.1109/TIP.2016.2614135
   Li Y, 2015, LECT NOTES COMPUT SC, V8926, P254, DOI 10.1007/978-3-319-16181-5_18
   Lindeberg T., 2012, SCHOLARPEDIA, V7, P10491, DOI [10.4249/scholarpedia.10491, DOI 10.4249/SCHOLARPEDIA.10491]
   Luo CW, 2019, INFRARED PHYS TECHN, V99, P265, DOI 10.1016/j.infrared.2019.04.017
   Nam H, 2016, PROC CVPR IEEE, P4293, DOI 10.1109/CVPR.2016.465
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Tian Z, 2019, IEEE I CONF COMP VIS, P9626, DOI 10.1109/ICCV.2019.00972
   Valmadre J, 2017, PROC CVPR IEEE, P5000, DOI 10.1109/CVPR.2017.531
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang Q, 2018, PROC CVPR IEEE, P4854, DOI 10.1109/CVPR.2018.00510
   Wu YF, 2011, PROCEEDINGS OF THE ASME INTERNATIONAL MANUFACTURING SCIENCE AND ENGINEERING CONFERENCE 2011, VOL 1, P1
   Yun X, 2019, MATH PROBL ENG, V2019, DOI 10.1155/2019/2437521
   Zhai SL, 2019, NEUROCOMPUTING, V334, P172, DOI 10.1016/j.neucom.2019.01.022
   Zhang JM, 2014, LECT NOTES COMPUT SC, V8694, P188, DOI 10.1007/978-3-319-10599-4_13
   Zhang XC, 2020, SIGNAL PROCESS-IMAGE, V84, DOI 10.1016/j.image.2019.115756
   Zhang ZP, 2019, PROC CVPR IEEE, P4586, DOI 10.1109/CVPR.2019.00472
   Zhu YB, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P465, DOI 10.1145/3343031.3350928
   Zhu Z., P EUR C COMP VIS ECC
NR 37
TC 16
Z9 16
U1 2
U2 50
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2022
VL 38
IS 7
BP 2555
EP 2567
DI 10.1007/s00371-021-02131-4
EA MAY 2021
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 2B0WL
UT WOS:000646101400001
DA 2024-07-18
ER

PT J
AU Tschiedel, M
   Russold, MF
   Kaniusas, E
   Vincze, M
AF Tschiedel, Michael
   Russold, Michael Friedrich
   Kaniusas, Eugenijus
   Vincze, Markus
TI Real-time limb tracking in single depth images based on circle matching
   and line fitting
SO VISUAL COMPUTER
LA English
DT Article
DE Shank modeling; Body tracking; Depth image; Gait analysis
AB Modern lower limb prostheses neither measure nor incorporate healthy residual leg information for intent recognition or device control. In order to increase robustness and reduce misclassification of devices like these, we propose a vision-based solution for real-time 3D human contralateral limb tracking (CoLiTrack). An inertial measurement unit and a depth camera are placed on the side of the prosthesis. The system is capable of estimating the shank axis of the healthy leg. Initially, the 3D input is transformed into a stabilized coordinate system. By splitting the subsequent shank estimation problem into two less computationally intensive steps, the computation time is significantly reduced: First, an iterative closest point algorithm is applied to fit circular models against 2D projections. Second, the random sample consensus method is used to determine the final shank axis. In our study, three experiments were conducted to validate the static, the dynamic and the real-world performance of our CoLiTrack approach. The shank angle can be tracked at 20 Hz for one sixth of the entire human gait cycle with an angle estimation error below 2.8 +/- 2.1 degrees. Our promising results demonstrate the robustness of the novel CoLiTrack approach to make "next-generation prostheses" more user-friendly, functional and safe.
C1 [Tschiedel, Michael; Russold, Michael Friedrich] Ottobock Healthcare Prod GmbH, Dept Global Res, Vienna, Austria.
   [Tschiedel, Michael; Kaniusas, Eugenijus] TU Wien, Inst Electrodynam Microwave & Circuit Engn, Vienna, Austria.
   [Vincze, Markus] TU Wien, Automat & Control Inst, Vienna, Austria.
C3 Technische Universitat Wien; Technische Universitat Wien
RP Tschiedel, M (corresponding author), Ottobock Healthcare Prod GmbH, Dept Global Res, Vienna, Austria.; Tschiedel, M (corresponding author), TU Wien, Inst Electrodynam Microwave & Circuit Engn, Vienna, Austria.
EM michael@tschiedel.at
RI Kaniušas, Eugenijus/GLV-0557-2022
OI Kaniušas, Eugenijus/0000-0002-1228-3859; Tschiedel,
   Michael/0000-0001-6265-8172
FU Austrian Research Promotion Agency (FFG) program "Industrienahe
   Dissertationen
FX The project was partially funded by the Austrian Research Promotion
   Agency (FFG) program "Industrienahe Dissertationen."
CR Ambrozic L, 2014, IEEE ROBOT AUTOM MAG, V21, P82, DOI 10.1109/MRA.2014.2360278
   Anton David, 2013, 2013 IEEE 15th International Conference on e-Health Networking, Applications and Services (Healthcom 2013), P444, DOI 10.1109/HealthCom.2013.6720717
   Balaji SR, 2017, PROCEEDINGS OF 2017 11TH INTERNATIONAL CONFERENCE ON INTELLIGENT SYSTEMS AND CONTROL (ISCO 2017), P469, DOI 10.1109/ISCO.2017.7856037
   Ballit A, 2022, VISUAL COMPUT, V38, P919, DOI 10.1007/s00371-021-02059-9
   Batten HR, 2019, PROSTHET ORTHOT INT, V43, P196, DOI 10.1177/0309364618792723
   Bergström P, 2014, COMPUT OPTIM APPL, V58, P543, DOI 10.1007/s10589-014-9643-2
   Bernal-Torres MG, 2018, J HEALTHC ENG, V2018, DOI 10.1155/2018/8783642
   Borjian R, 2010, MICROSYST TECHNOL, V16, P257, DOI 10.1007/s00542-009-0853-y
   Chang WC, 2020, VISUAL COMPUT, V36, P593, DOI 10.1007/s00371-019-01642-5
   Colyer SL, 2018, SPORTS MED-OPEN, V4, DOI 10.1186/s40798-018-0139-y
   Diaz JP, 2018, IEEE ENG MED BIO, P1817, DOI 10.1109/EMBC.2018.8512614
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Fluit R, 2020, IEEE T BIO-MED ENG, V67, P277, DOI 10.1109/TBME.2019.2912466
   Gavrilova ML, 2018, IEEE CONSUM ELECTR M, V7, P88, DOI 10.1109/MCE.2017.2755498
   Gorsic M, 2014, SENSORS-BASEL, V14, P2776, DOI 10.3390/s140202776
   GRIMES DL, 1977, J BIOMECH ENG-T ASME, V99, P215, DOI 10.1115/1.3426293
   Grimmer M, 2019, FRONT NEUROROBOTICS, V13, DOI 10.3389/fnbot.2019.00057
   Hargrove LJ, 2009, IEEE ENG MED BIO, P2111, DOI 10.1109/IEMBS.2009.5334303
   Hargrove L.J., 2018, 2018 7 IEEE INT C BI, DOI 10.1109/BIOROB.2018.8487806
   Harms H, 2014, IEEE INT VEH SYM, P730, DOI 10.1109/IVS.2014.6856436
   Hu B, 2018, FRONT ROBOT AI, V5, DOI 10.3389/frobt.2018.00078
   Hu B, 2018, FRONT ROBOT AI, V5, DOI 10.3389/frobt.2018.00014
   Ishikawa T, 2018, IEEE IND ELEC, P5475, DOI 10.1109/IECON.2018.8592894
   JONES A, 1996, J SPORT SCI
   Krausz NE, 2015, IEEE T BIO-MED ENG, V62, P2576, DOI 10.1109/TBME.2015.2448457
   Laschowski B, 2019, INT C REHAB ROBOT, P868, DOI [10.1109/ICORR.2019.8779540, 10.1109/icorr.2019.8779540]
   Latorre J, 2019, J NEUROENG REHABIL, V16, DOI 10.1186/s12984-019-0568-y
   Li QN, 2018, VISUAL COMPUT, V34, P229, DOI 10.1007/s00371-016-1330-0
   Liang D, 2020, J COMPUT SCI TECH-CH, V35, P493, DOI 10.1007/s11390-020-0476-4
   Lien JM, 2016, ACM T GRAPHIC, V35, DOI [10.1145/2897824.2925953, 10.1145/9999997.9999999]
   Liu M, 2016, IEEE T NEUR SYS REH, V24, P434, DOI 10.1109/TNSRE.2015.2420539
   Liu Z, 2015, J VIS COMMUN IMAGE R, V32, P10, DOI 10.1016/j.jvcir.2015.06.013
   Massalin Y, 2018, IEEE T BIO-MED ENG, V65, P1759, DOI 10.1109/TBME.2017.2776157
   McGinley JL, 2009, GAIT POSTURE, V29, P360, DOI 10.1016/j.gaitpost.2008.09.003
   Mendez J, 2020, SCI ROBOT, V5, DOI 10.1126/scirobotics.aba6635
   Murray M P, 1967, Am J Phys Med, V46, P290
   Myronenko A, 2010, IEEE T PATTERN ANAL, V32, P2262, DOI 10.1109/TPAMI.2010.46
   Naeemabadi MR., 2018, BIODEVICES 2018 - 11th Int. Conf. Biomed. Electron. Devices Proc. Part 11th Int. Jt. Conf. Biomed. Eng. Syst. Technol. BIOSTEC, V1, P128, DOI DOI 10.5220/0006578201280135
   Neupane A., 2020, 2020 IEEE 33 INT S C, DOI 10.1109/CBMS49503.2020.00107
   Owen E, 2010, PROSTHET ORTHOT INT, V34, P254, DOI 10.3109/03093646.2010.485597
   Page S, 2015, IEEE INT CONF ROBOT, P4224, DOI 10.1109/ICRA.2015.7139781
   Parri A, 2017, FRONT NEUROROBOTICS, V11, DOI 10.3389/fnbot.2017.00025
   Pasinetti S, 2019, IEEE T INSTRUM MEAS, V68, P4456, DOI 10.1109/TIM.2018.2889233
   Perry J, 2010, GAIT ANALYSIS: NORMAL AND PATHOLOGICAL FUNCTION, SECOND EDITION, P1
   Saini R, 2019, INT J MACH LEARN CYB, V10, P2529, DOI 10.1007/s13042-018-0887-5
   St-Onge N, 2003, EXP BRAIN RES, V148, P139, DOI 10.1007/s00221-002-1212-8
   STIGLER S, 1981, ANN STAT
   Tschiedel M, 2020, J NEUROENG REHABIL, V17, DOI 10.1186/s12984-020-00726-x
   VALLERY H, 2007, INT C REHAB ROBOT
   Varol HA, 2010, IEEE T BIO-MED ENG, V57, P542, DOI 10.1109/TBME.2009.2034734
   Wang KK, 2021, VISUAL COMPUT, V37, P603, DOI 10.1007/s00371-020-01826-4
   Wu JZ, 2020, VISUAL COMPUT, V36, P1401, DOI 10.1007/s00371-019-01740-4
   Yan T, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON PROGNOSTICS AND HEALTH MANAGEMENT (ICPHM)
   Yi SC, 2015, COMPUT ELECTR ENG, V42, P23, DOI 10.1016/j.compeleceng.2015.01.002
   Young AJ, 2014, J NEUROENG REHABIL, V11, DOI 10.1186/1743-0003-11-5
   Zhang KG, 2019, IEEE T NEUR SYS REH, V27, P465, DOI 10.1109/TNSRE.2019.2895221
   Zhang Y, 2021, VISUAL COMPUT, V37, P1045, DOI 10.1007/s00371-020-01851-3
   Zohora FT, 2018, PROC SPIE, V10574, DOI 10.1117/12.2293739
   Zohora FT, 2017, COMM COM INF SC, V709, P391, DOI 10.1007/978-981-10-4859-3_35
NR 59
TC 6
Z9 6
U1 4
U2 23
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2022
VL 38
IS 8
BP 2635
EP 2645
DI 10.1007/s00371-021-02138-x
EA APR 2021
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 3B2OE
UT WOS:000643590800001
OA hybrid
DA 2024-07-18
ER

PT J
AU He, K
   Zhao, Y
   Liu, ZG
   Li, DS
   Ma, XT
AF He, Kai
   Zhao, Yan
   Liu, Zhiguo
   Li, Dashuang
   Ma, Xitao
TI Whole-pixel registration of non-rigid images using correspondences
   interpolation on sparse feature seeds
SO VISUAL COMPUTER
LA English
DT Article
DE Correspondence vector field interpolation; Whole-pixel registration;
   Non-rigid images; Feature matching
AB Whole pixel registration of non-rigid images with high accuracy and efficiency is a challenging problem in computer vision. To address this issue, we propose a correspondence vector field (CVF) Interpolation approach based on sparse matching of feature seeds. First, we detect and match two types of feature seeds to improve the accuracy of the later dense CVF interpolation. The first type of feature seeds is to guarantee the accuracy of the motion boundary, while the second one is to achieve the uniform distribution of seeds, which is helpful to improve the effect of interpolation. Second, we regionally estimate the dense CVF using the proposed interpolation approach on this basis. At last, we realize the whole-pixel registration of non-rigid images to yield the image alignment. Different from the traditional CVF interpolation approaches based on optical flow field, ours is based on the sparse matching of feature seeds. Thus, it is not limited to the large displacements and tends to achieve the accurate matching of certain key points easily, which is critical to the final interpolation result. Qualitative and quantitative experimental results on several internationally used datasets demonstrate that our approach outperforms the state-of-the-art ones.
C1 [He, Kai; Zhao, Yan; Liu, Zhiguo; Li, Dashuang; Ma, Xitao] Tianjin Univ, Sch Elect & Informat Engn, Tianjin, Peoples R China.
C3 Tianjin University
RP He, K (corresponding author), Tianjin Univ, Sch Elect & Informat Engn, Tianjin, Peoples R China.
EM hekai@tju.edu.cn
RI liu, zhiguo/IUQ-7631-2023
CR Alcantarilla PF, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.13
   Alcantarilla PF, 2012, LECT NOTES COMPUT SC, V7577, P214, DOI 10.1007/978-3-642-33783-3_16
   Bailer C, 2015, IEEE I CONF COMP VIS, P4015, DOI 10.1109/ICCV.2015.457
   Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014
   Bian JW, 2017, PROC CVPR IEEE, P2828, DOI 10.1109/CVPR.2017.302
   Brox T, 2004, LECT NOTES COMPUT SC, V2034, P25, DOI 10.1007/978-3-540-24673-2_3
   Brox T, 2011, IEEE T PATTERN ANAL, V33, P500, DOI 10.1109/TPAMI.2010.143
   Cai GR, 2013, SIGNAL PROCESS, V93, P3088, DOI 10.1016/j.sigpro.2013.04.008
   Chen J, 2019, IEEE T CIRC SYST VID, V29, P3595, DOI 10.1109/TCSVT.2018.2885246
   Chen QF, 2016, PROC CVPR IEEE, P4706, DOI 10.1109/CVPR.2016.509
   Collins JA, 2017, IEEE T MED IMAGING, V36, P1502, DOI 10.1109/TMI.2017.2668842
   Dollár P, 2015, IEEE T PATTERN ANAL, V37, P1558, DOI 10.1109/TPAMI.2014.2377715
   Dosovitskiy A, 2015, IEEE I CONF COMP VIS, P2758, DOI 10.1109/ICCV.2015.316
   Geistert J, 2016, PICT COD SYMP
   Gong L, 2019, IEEE J BIOMED HEALTH, V23, P766, DOI 10.1109/JBHI.2018.2836380
   He K, 2017, IEEE T IMAGE PROCESS, V26, P6046, DOI 10.1109/TIP.2017.2751142
   Hu YL, 2017, PROC CVPR IEEE, P4791, DOI 10.1109/CVPR.2017.509
   Hu YL, 2016, PROC CVPR IEEE, P5704, DOI 10.1109/CVPR.2016.615
   Hu YL, 2016, IMAGE VISION COMPUT, V52, P167, DOI 10.1016/j.imavis.2016.06.004
   Hui, 2020, ABS190307414
   Hui TW, 2018, PROC CVPR IEEE, P8981, DOI 10.1109/CVPR.2018.00936
   Hur J., 2020, PROC CVPR IEEE, P5754
   Ke Y, 2004, PROC CVPR IEEE, P506
   Kroeger T, 2016, LECT NOTES COMPUT SC, V9908, P471, DOI 10.1007/978-3-319-46493-0_29
   Leutenegger S, 2011, IEEE I CONF COMP VIS, P2548, DOI 10.1109/ICCV.2011.6126542
   Liu YJ, 2019, VISUAL COMPUT, V35, P667, DOI 10.1007/s00371-018-1502-1
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Melekhov I, 2019, IEEE WINT CONF APPL, P1034, DOI 10.1109/WACV.2019.00115
   Morel JM, 2009, SIAM J IMAGING SCI, V2, P438, DOI 10.1137/080732730
   Mstafa RJ, 2020, IEEE ACCESS, V8, P161825, DOI 10.1109/ACCESS.2020.3021356
   Mur-Artal R, 2015, IEEE T ROBOT, V31, P1147, DOI 10.1109/TRO.2015.2463671
   Niklaus S, 2018, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2018.00183
   Rakêt LL, 2012, LECT NOTES COMPUT SC, V7431, P447, DOI 10.1007/978-3-642-33179-4_43
   Ranjan A, 2017, PROC CVPR IEEE, P2720, DOI 10.1109/CVPR.2017.291
   Redding Nicholas J., 2008, 2008 Digital Image Computing: Techniques and Applications, P343, DOI 10.1109/DICTA.2008.38
   Revaud J, 2015, PROC CVPR IEEE, P1164, DOI 10.1109/CVPR.2015.7298720
   Rublee E, 2011, IEEE I CONF COMP VIS, P2564, DOI 10.1109/ICCV.2011.6126544
   Schuster R, 2018, IEEE IMAGE PROC, P1463, DOI 10.1109/ICIP.2018.8451182
   Sun DQ, 2018, PROC CVPR IEEE, P8934, DOI 10.1109/CVPR.2018.00931
   Sun DQ, 2014, INT J COMPUT VISION, V106, P115, DOI 10.1007/s11263-013-0644-x
   Teed, 2020, ABS200312039
   Truong P, 2020, PROC CVPR IEEE, P6257, DOI 10.1109/CVPR42600.2020.00629
   Wang R, 2018, IEEE SIG PROC MED
   Wasserman L., 2010, ALL STAT CONCISE COU
   Weinzaepfel P, 2013, IEEE I CONF COMP VIS, P1385, DOI 10.1109/ICCV.2013.175
   Wrobel, 2004, MULTIPLE VIEW GEOMET
   Wulff J, 2015, PROC CVPR IEEE, P120, DOI 10.1109/CVPR.2015.7298607
   Xu J, 2017, PROC CVPR IEEE, P5807, DOI 10.1109/CVPR.2017.615
   Xu L, 2012, IEEE T PATTERN ANAL, V34, P1744, DOI 10.1109/TPAMI.2011.236
NR 49
TC 2
Z9 3
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2022
VL 38
IS 5
BP 1815
EP 1832
DI 10.1007/s00371-021-02107-4
EA MAR 2021
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 0M9JP
UT WOS:000634313800001
DA 2024-07-18
ER

PT J
AU Murali, S
   Govindan, VK
   Kalady, S
AF Murali, Saritha
   Govindan, V. K.
   Kalady, Saidalavi
TI Quaternion-based image shadow removal
SO VISUAL COMPUTER
LA English
DT Article
DE Quaternion; Color transfer; Shadow invariant
AB Visual effects such as shadows and highlights appear in photographs due to variations in lighting conditions. Although these effects add more meaning to the images, they pose various problems to specific computer vision algorithms. Hence, the removal of shadows and highlights is often considered as a prerequisite to such algorithms. This paper presents an interactive technique for shadow elimination from images. Our method requires user input in the form of rough strokes on the shadow region and its corresponding non-shadow region in the image. We further use quaternion rotation in the YCbCr color space to derive an image that is invariant to shadows. The actual colors of the image are finally recovered by color transfer from the original shadow image. The proposed method takes less time to generate the shadow-free image and does not necessitate the detection of shadows prior to its removal. Also, unlike the existing shadow-removal techniques, our method generates invariant image with minor texture loss. Experimental findings are reported to demonstrate the performance of our shadow-removal technique.
C1 [Murali, Saritha] Natl Inst Technol Calicut, Dept Comp Sci & Engn, Image Proc, Kozhikode, Kerala, India.
   [Govindan, V. K.; Kalady, Saidalavi] Natl Inst Technol Calicut, Dept Comp Sci & Engn, Kozhikode, Kerala, India.
C3 National Institute of Technology (NIT System); National Institute of
   Technology Calicut; National Institute of Technology (NIT System);
   National Institute of Technology Calicut
RP Murali, S (corresponding author), Natl Inst Technol Calicut, Dept Comp Sci & Engn, Image Proc, Kozhikode, Kerala, India.
EM saritha.mkv@gmail.com; vkg@nitc.ac.in; said@nitc.ac.in
OI MURALI, SARITHA/0000-0002-6778-6138
CR [Anonymous], 2010, SYNTHESIS LECT COMPU
   Chen BJ, 2012, SIGNAL PROCESS, V92, P308, DOI 10.1016/j.sigpro.2011.07.018
   Evans CJ, 2000, IEEE IMAGE PROC, P541, DOI 10.1109/ICIP.2000.901015
   Fan XY, 2020, VISUAL COMPUT, V36, P2175, DOI 10.1007/s00371-020-01916-3
   Finlayson GD, 2006, IEEE T PATTERN ANAL, V28, P59, DOI 10.1109/TPAMI.2006.18
   Finlayson GD, 2004, LECT NOTES COMPUT SC, V3023, P582
   Gong H, 2017, IMAGE VISION COMPUT, V62, P19, DOI 10.1016/j.imavis.2017.04.001
   Gong H, 2016, J OPT SOC AM A, V33, P1798, DOI 10.1364/JOSAA.33.001798
   Gryka M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2732407
   Guo RQ, 2013, IEEE T PATTERN ANAL, V35, P2956, DOI 10.1109/TPAMI.2012.214
   Hiary H, 2018, COMPUT J, V61, P459, DOI 10.1093/comjnl/bxy004
   Murali Saritha, 2016, International Journal of Image, Graphics and Signal Processing, V8, P38, DOI 10.5815/ijigsp.2016.12.05
   Murali S, 2019, COMPUT VIS MEDIA, V5, P311, DOI 10.1007/s41095-019-0148-x
   Murali S, 2013, CYBERN INF TECHNOL, V13, P95, DOI 10.2478/cait-2013-0009
   Qu LQ, 2015, OPT EXPRESS, V23, P2220, DOI 10.1364/OE.23.002220
   Reinhard E, 2001, IEEE COMPUT GRAPH, V21, P34, DOI 10.1109/38.946629
   Sasi RK, 2016, ENG SCI TECHNOL, V19, P1067, DOI 10.1016/j.jestch.2016.01.001
   Shi LL, 2007, COMPUT VIS IMAGE UND, V107, P88, DOI 10.1016/j.cviu.2006.11.014
   Su YF, 2010, IEEE T IMAGE PROCESS, V19, P2749, DOI 10.1109/TIP.2010.2050626
   Subakan ON, 2011, INT J COMPUT VISION, V91, P233, DOI 10.1007/s11263-010-0388-9
   Vincente TFY, 2016, LECT NOTES COMPUT SC, V9910, P816, DOI 10.1007/978-3-319-46466-4_49
   Wang JF, 2018, PROC CVPR IEEE, P1788, DOI 10.1109/CVPR.2018.00192
   Xiao CX, 2013, COMPUT GRAPH FORUM, V32, P421, DOI 10.1111/cgf.12250
   Yang QX, 2012, IEEE T IMAGE PROCESS, V21, P4361, DOI 10.1109/TIP.2012.2208976
   Yu XM, 2017, LECT NOTES COMPUT SC, V10425, P307, DOI 10.1007/978-3-319-64698-5_26
   Zhang L, 2019, VISUAL COMPUT, V35, P1091, DOI 10.1007/s00371-019-01685-8
   Zhang L, 2015, IEEE T IMAGE PROCESS, V24, P4623, DOI 10.1109/TIP.2015.2465159
NR 27
TC 8
Z9 8
U1 1
U2 28
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2022
VL 38
IS 5
BP 1527
EP 1538
DI 10.1007/s00371-021-02086-6
EA MAR 2021
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 0M9JP
UT WOS:000624459000001
DA 2024-07-18
ER

PT J
AU Ong, P
   Chong, TK
   Ong, KM
   Low, ES
AF Ong, Pauline
   Chong, Tang Keat
   Ong, Kok Meng
   Low, Ee Soong
TI Tracking of moving athlete from video sequences using flower pollination
   algorithm
SO VISUAL COMPUTER
LA English
DT Article
DE Athlete tracking; Computer vision; Flower pollination algorithm; Object
   tracking; Sports video
ID PARTICLE SWARM OPTIMIZATION; OBJECT TRACKING; PLAYER DETECTION
AB Performance analysis, as related to sport, is a process underpinned by a systematic analysis of information, to accelerate the performance of athletes through crafted focused practice session based on the obtained analysis. Quantification of athlete performance profile using sports video has thus been put forward, where the athlete tracking in such video-based analysis is one of the critical elements for the success of an object tracking system. In this study, for the first time the flower pollination algorithm (FPA) is utilised to track the motion of the moving athlete from the sports video. Initially, a search window with the attributes of centroid coordinates of the moving athlete, width and length of the search window is used to represent the current position of the athlete. Subsequently, the hue, saturation and value (HSV) histogram of the region within the search window is evaluated. In the consecutive frame, several potential positions of the athlete are identified, and the Bhattacharyya distance between the HSV histogram of the athlete in the previous frame and the potential position in the current frame is calculated. Since the FPA attempts to maximise the similarity of both histograms, intuitively, the current position of the moving athlete should be only slightly different than his previous position. The comparative analysis shows that the FPA is comparable with other competing algorithms in terms of detection rate, tracking accuracy and processing time.
C1 [Ong, Pauline; Chong, Tang Keat; Ong, Kok Meng; Low, Ee Soong] Univ Tun Hussein Onn Malaysia, Fac Mech & Mfg Engn, Batu Pahat 86400, Johor, Malaysia.
C3 University of Tun Hussein Onn Malaysia
RP Ong, P (corresponding author), Univ Tun Hussein Onn Malaysia, Fac Mech & Mfg Engn, Batu Pahat 86400, Johor, Malaysia.
EM ongp@uthm.edu.rny
FU Ministry of Higher Education Malaysia [FRGS/1/2018/ICT02/UTHM/02/2 Vot
   K070]
FX The authors would like to express the deepest appreciation to the
   Ministry of Higher Education Malaysia, for funding this project through
   the Fundamental Research Grant Scheme (FRGS/1/2018/ICT02/UTHM/02/2 Vot
   K070).
CR Abbass MY, 2021, VISUAL COMPUT, V37, P993, DOI 10.1007/s00371-020-01848-y
   Abbass MY, 2021, VISUAL COMPUT, V37, P831, DOI 10.1007/s00371-020-01833-5
   Alyasseri ZAA, 2020, PATTERN RECOGN, V105, DOI 10.1016/j.patcog.2020.107393
   [Anonymous], 2014, J. Inf. Hiding Multimedia Signal Process.
   [Anonymous], 2014, Computer Vision in Sports
   Bae C, 2016, EXPERT SYST APPL, V64, P385, DOI 10.1016/j.eswa.2016.08.027
   Barhoumi W, 2015, SIGNAL IMAGE VIDEO P, V9, P1705, DOI 10.1007/s11760-014-0630-y
   Bertinetto L, 2016, LECT NOTES COMPUT SC, V9914, P850, DOI 10.1007/978-3-319-48881-3_56
   Boroujeni HS, 2012, COMM COM INF SC, V295, P143
   Cust EE, 2019, J SPORT SCI, V37, P568, DOI 10.1080/02640414.2018.1521769
   Dai Z, 2009 INT WORKSH INT, P1
   Dakua SP, 2019, INT J COMPUT ASS RAD, V14, P2165, DOI 10.1007/s11548-019-02030-z
   Dash PP, 2019, MEASUREMENT, V144, P311, DOI 10.1016/j.measurement.2019.05.030
   de Pádua PHC, 2017, MACH VISION APPL, V28, P475, DOI 10.1007/s00138-017-0849-z
   Faichney J, 2002, 2002 6TH INTERNATIONAL CONFERENCE ON SIGNAL PROCESSING PROCEEDINGS, VOLS I AND II, P735, DOI 10.1109/ICOSP.2002.1181161
   Fakhar B, 2019, MULTIMED TOOLS APPL, V78, P16995, DOI 10.1007/s11042-018-7083-1
   Feng Z, 2019, VISUAL COMPUT, P1
   Fohanno V, 2013, COMPUT METHOD BIOMEC, V16, P95, DOI 10.1080/10255842.2013.815906
   Gao ML, 2016, CHIN CONTR CONF, P3866, DOI 10.1109/ChiCC.2016.7553956
   Gao ML, 2016, NEUROCOMPUTING, V177, P612, DOI 10.1016/j.neucom.2015.11.072
   Gao ML, 2013, IET COMPUT VIS, V7, P227, DOI 10.1049/iet-cvi.2012.0207
   Gao ML, 2012, J ELECTRON IMAGING, V21, DOI 10.1117/1.JEI.21.4.043001
   Gao ML, 2018, OPTIK, V156, P522, DOI 10.1016/j.ijleo.2017.11.155
   Ghosh P, 2020, IEEE T MOBILE COMPUT, V19, P1260, DOI 10.1109/TMC.2019.2909020
   Kang K, 2018, APPL SOFT COMPUT, V66, P319, DOI 10.1016/j.asoc.2018.02.037
   Kong YQ, 2018, MULTIMED TOOLS APPL, V77, P13643, DOI 10.1007/s11042-017-4979-0
   Lacambre Jean-Baptiste, 2013, 2013 IEEE Digital Signal Processing and Signal Processing Education Meeting (DSP/SPE), P187, DOI 10.1109/DSP-SPE.2013.6642588
   Li GJ, 2019, EURASIP J IMAGE VIDE, DOI 10.1186/s13640-019-0415-x
   Li WX, 2017, INT J ADV ROBOT SYST, V14, DOI 10.1177/1729881417704544
   Low ES, 2019, ROBOT AUTON SYST, V115, P143, DOI 10.1016/j.robot.2019.02.013
   Lv L, 2018, INT J COMPUT SCI MAT, V9, P219, DOI 10.1504/IJCSM.2018.093158
   Ma M, 2020, MULTIMED TOOLS APPL, V79, P9267, DOI 10.1007/s11042-019-7444-4
   Manafifard M, 2017, MULTIMED TOOLS APPL, V76, P12251, DOI 10.1007/s11042-016-3625-6
   Manafifard M, 2015, SCI IRAN, V22, P1031
   Meng OK, 2019, APPL SOFT COMPUT, V83, DOI 10.1016/j.asoc.2019.105625
   Morimitsu H, 2017, COMPUT VIS IMAGE UND, V159, P89, DOI 10.1016/j.cviu.2016.12.003
   Musa Z, 2016, IEEE T CIRC SYST VID, V26, P1433, DOI 10.1109/TCSVT.2015.2433172
   Nenavath H, 2018, SWARM EVOL COMPUT, V43, P1, DOI 10.1016/j.swevo.2018.02.011
   Nenavath H, 2018, APPL SOFT COMPUT, V62, P1019, DOI 10.1016/j.asoc.2017.09.039
   Niu PF, 2019, NEURAL PROCESS LETT, V49, P737, DOI 10.1007/s11063-018-9854-0
   Rahmad N.A., 2019, Indonesian Journal of Electrical Engineering and Computer Science, V14, P1330, DOI 10.11591/ijeecs.v14.i3.pp1330-1335
   Ridolfi M, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18010168
   Santhosh PK, 2019, CMC-COMPUT MATER CON, V58, P625, DOI 10.32604/cmc.2019.05161
   Thenmozhi T, 2020, MICROPROCESS MICROSY, V76, DOI 10.1016/j.micpro.2020.103084
   Tiwari M., 2017, Int. J. Comput. Intell. Res, V13, P745, DOI DOI 10.1109/CIS.2009.13
   Wang KP, 2019, ROBOT CIM-INT MANUF, V59, P235, DOI 10.1016/j.rcim.2019.04.014
   Wang YW, 2017, VISUAL COMPUT, V33, P235, DOI 10.1007/s00371-015-1189-5
   Wang Y, 2020, VISUAL COMPUT, V36, P683, DOI 10.1007/s00371-019-01646-1
   Xin-She Yang, 2012, Unconventional Computation and Natural Computation. Proceedings of the 11th International Conference, UCNC 2012, P240, DOI 10.1007/978-3-642-32894-7_27
   Xuan SY, 2020, IEEE T GEOSCI REMOTE, V58, P1074, DOI 10.1109/TGRS.2019.2943366
   Yang Y, 2017, J VIS COMMUN IMAGE R, V46, P81, DOI 10.1016/j.jvcir.2017.03.008
   Yoon Y, 2019, IEEE ACCESS, V7, P56564, DOI 10.1109/ACCESS.2019.2913953
   Zhang H., 2018, CHIN C PATT REC COMP, P284
   Zhang WC, 2017, IMAGE VISION COMPUT, V61, P22, DOI 10.1016/j.imavis.2017.02.002
NR 54
TC 8
Z9 9
U1 3
U2 9
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2022
VL 38
IS 3
BP 939
EP 962
DI 10.1007/s00371-021-02060-2
EA JAN 2021
PG 24
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZQ8YX
UT WOS:000609362000001
DA 2024-07-18
ER

PT J
AU Wang, CX
   Zhang, HY
   Liu, LG
AF Wang, Chunxue
   Zhang, Huayan
   Liu, Ligang
TI Total generalized variation-based Retinex image decomposition
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Computer Graphics International (CGI) Conference
CY OCT 20-23, 2020
CL ELECTR NETWORK
DE Retinex theory; Image decomposition; Total generalized variation
   regularization; Alternating minimization scheme
ID MODEL
AB Human visual system (HVS) can perceive color under varying illumination conditions, and Retinex theory is precisely aimed to simulate and explain how the HVS perceives reflectance regardless of different illumination conditions. In this paper, we introduce a reflectance and illumination decomposition model for the Retinex problem via total generalized variation regularization andH1decomposition. The total generalized variation regularization ameliorates the staircasing artifacts that appear in the reflectance component of existing total variation-based models andH1norm guarantees smoother illumination. We analyze the existence and uniqueness of the proposed model and employ an alternating minimization scheme based on split Bregman iteration. We present numerous numerical experiments on both grayscale and color images to make comparisons with several state-of-the-art methods and demonstrate that our method is comparable both quantitatively and qualitatively.
C1 [Wang, Chunxue] Dunhuang Acad, Jiuquan, Gansu, Peoples R China.
   [Zhang, Huayan] Tiangong Univ, Sch Comp Sci & Software Engn, Tianjin, Peoples R China.
   [Liu, Ligang] Univ Sci & Technol China, Sch Math Sci, Hefei, Anhui, Peoples R China.
C3 Tiangong University; Chinese Academy of Sciences; University of Science
   & Technology of China, CAS
RP Wang, CX (corresponding author), Dunhuang Acad, Jiuquan, Gansu, Peoples R China.
EM chunxuewang2019@163.com
RI chen, minghui/KFR-8832-2024
OI Wang, Chunxue/0000-0002-6828-3487
CR Berthold KP, 1974, Comput. Graph. Image Process., V3, P277, DOI [DOI 10.1016/0146-664X(74)90022-7, 10.1016/0146-664X(74)90022-7]
   BLAKE A, 1985, COMPUT VISION GRAPH, V32, P314, DOI 10.1016/0734-189X(85)90054-4
   Bredies K, 2013, J MATH ANAL APPL, V398, P438, DOI 10.1016/j.jmaa.2012.08.053
   Bredies K, 2010, SIAM J IMAGING SCI, V3, P492, DOI 10.1137/090769521
   Chambolle A, 1997, NUMER MATH, V76, P167, DOI 10.1007/s002110050258
   Chang HB, 2015, OPT ENG, V54, DOI 10.1117/1.OE.54.1.013107
   Cheng MH, 2019, APPL MATH MODEL, V66, P305, DOI 10.1016/j.apm.2018.09.022
   Choi D.H., 2008 16 EUROPEAN SIG, P1
   Choi DH, 2007, IEEE INT SYMP CIRC S, P3948, DOI 10.1109/ISCAS.2007.378664
   Chong Frederic T, 2019, ARXIV PREPRINT ARXIV
   Ciurea F., 2004, J ELECTRON IMAGING, V13, P48
   Cooper TJ, 2004, J ELECTRON IMAGING, V13, P85, DOI 10.1117/1.1636182
   Duan JM, 2015, J GLOBAL OPTIM, V62, P853, DOI 10.1007/s10898-015-0290-7
   Duan JM, 2014, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2014-7
   Frankle J. A., 1983, U. S. Patent, Patent No. [4,384,336, 4384336]
   Fu XY, 2016, PROC CVPR IEEE, P2782, DOI 10.1109/CVPR.2016.304
   Funt B, 2000, EIGHTH COLOR IMAGING CONFERENCE: COLOR SCIENCE AND ENGINEERING SYSTEMS, TECHNOLOGIES, APPLICATIONS, P112
   FUNT BV, 1992, LECT NOTES COMPUT SC, V588, P124
   Gao YY, 2018, IEEE T MULTIMEDIA, V20, P335, DOI 10.1109/TMM.2017.2740025
   Goldstein T, 2009, SIAM J IMAGING SCI, V2, P323, DOI 10.1137/080725891
   Gu SH, 2017, IEEE I CONF COMP VIS, P1717, DOI 10.1109/ICCV.2017.189
   Guo XJ, 2017, IEEE T IMAGE PROCESS, V26, P982, DOI 10.1109/TIP.2016.2639450
   Guojia Hou, 2017, IAENG International Journal of Computer Science, V44, P445
   Jiang B, 2015, J REAL-TIME IMAGE PR, V10, P239, DOI 10.1007/s11554-014-0399-9
   Jobson DJ, 1997, IEEE T IMAGE PROCESS, V6, P965, DOI 10.1109/83.597272
   Jobson DJ, 1997, IEEE T IMAGE PROCESS, V6, P451, DOI 10.1109/83.557356
   Juha K., 2017, SOBOLEV SPACES
   Kang M, 2017, J SCI COMPUT, V72, P172, DOI 10.1007/s10915-017-0357-3
   Kimmel R, 2003, INT J COMPUT VISION, V52, P7, DOI 10.1023/A:1022314423998
   Knoll F, 2011, MAGN RESON MED, V65, P480, DOI 10.1002/mrm.22595
   Land Edwin H., 1985, Wenner-gren center international symposium series, P5
   LAND EH, 1971, J OPT SOC AM, V61, P1, DOI 10.1364/JOSA.61.000001
   LAND EH, 1983, P NATL ACAD SCI USA, V80, P5163, DOI 10.1073/pnas.80.16.5163
   LAND EH, 1977, SCI AM, V237, P108, DOI 10.1038/scientificamerican1277-108
   LAND EH, 1986, P NATL ACAD SCI USA, V83, P3078, DOI 10.1073/pnas.83.10.3078
   Lee C, 2012, IEEE IMAGE PROC, P965, DOI 10.1109/ICIP.2012.6467022
   Lei Ling, 2007, Journal of Electronics (China), V24, P696, DOI 10.1007/s11767-006-0222-2
   Li HF, 2012, IEEE T GEOSCI REMOTE, V50, P3053, DOI 10.1109/TGRS.2011.2178075
   Liang JW, 2015, J MATH IMAGING VIS, V52, P345, DOI 10.1007/s10851-015-0568-x
   Lore KG, 2017, PATTERN RECOGN, V61, P650, DOI 10.1016/j.patcog.2016.06.008
   Lu WQ, 2016, MATH METHOD APPL SCI, V39, P4208, DOI 10.1002/mma.3858
   Ma K, 2015, IEEE T IMAGE PROCESS, V24, P3345, DOI 10.1109/TIP.2015.2442920
   Ma WY, 2012, INVERSE PROBL IMAG, V6, P697, DOI 10.3934/ipi.2012.6.697
   Ma WY, 2011, PROC CVPR IEEE, P153, DOI 10.1109/CVPR.2011.5995422
   Ma YP, 2017, LECT NOTES COMPUT SC, V10666, P626, DOI 10.1007/978-3-319-71607-7_55
   Marini D, 2000, IMAGE VISION COMPUT, V18, P1005, DOI 10.1016/S0262-8856(00)00037-8
   Maz'ya V., 2013, SOBOLEV SPACES
   McCann J, 1999, SEVENTH COLOR IMAGING CONFERENCE: COLOR SCIENCE, SYSTEMS AND APPLICATIONS, P1
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Morel J.M., P COL IM 14 DISPL PR, V7241, P724106
   Morel JM, 2010, IEEE T IMAGE PROCESS, V19, P2825, DOI 10.1109/TIP.2010.2049239
   Ng MK, 2011, SIAM J IMAGING SCI, V4, P345, DOI 10.1137/100806588
   Pallara LANFD., 2000, FUNCTIONS BOUNDED VA
   Park S, 2018, IEEE ACCESS, V6, P22084, DOI 10.1109/ACCESS.2018.2812809
   Parthasarathy S., 2012, NATL C COMMUNICATION, P1, DOI DOI 10.1109/NCC.2012.6176791
   Provenzi E, 2005, J OPT SOC AM A, V22, P2613, DOI 10.1364/JOSAA.22.002613
   Provenzi E, 2007, IEEE T IMAGE PROCESS, V16, P162, DOI 10.1109/TIP.2006.884946
   Rahman Z, 1996, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, PROCEEDINGS - VOL III, P1003, DOI 10.1109/ICIP.1996.560995
   Rahman ZU, 2004, J ELECTRON IMAGING, V13, P100, DOI 10.1117/1.1636183
   Shen L., 2017, ARXIV PREPRINT ARXIV
   Shi W, 2016, LECT NOTES COMPUT SC, V9908, P371, DOI 10.1007/978-3-319-46493-0_23
   Wali S, 2019, J VIS COMMUN IMAGE R, V59, P39, DOI 10.1016/j.jvcir.2018.12.047
   Wang LQ, 2014, IEEE T IMAGE PROCESS, V23, P3381, DOI 10.1109/TIP.2014.2324813
   Wang QH, 2016, IEEE IMAGE PROC, P4077, DOI 10.1109/ICIP.2016.7533126
   Wang SH, 2013, IEEE T IMAGE PROCESS, V22, P3538, DOI 10.1109/TIP.2013.2261309
   Wang W, 2015, SIAM J IMAGING SCI, V8, P1955, DOI 10.1137/15M1006908
   Wang W, 2014, NUMER MATH-THEORY ME, V7, P334, DOI 10.4208/nmtma.2014.1326nm
   Wang W, 2008, INT C WAVEL ANAL PAT, P80, DOI 10.1109/ICWAPR.2008.4635754
   Wei Chen, 2018, ARXIV PREPRINT ARXIV
   Zhang X., 1996, 1996 SID International Symposium. Digest of Technical Papers. First Edition, P731
   Zosso D, 2013, PROC SPIE, V8657, DOI 10.1117/12.2008839
NR 71
TC 10
Z9 12
U1 1
U2 23
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2021
VL 37
IS 1
BP 77
EP 93
DI 10.1007/s00371-020-01888-4
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA QE3UO
UT WOS:000616134400007
DA 2024-07-18
ER

PT J
AU Ayadi, W
   Charfi, I
   Elhamzi, W
   Atri, M
AF Ayadi, Wadhah
   Charfi, Imen
   Elhamzi, Wajdi
   Atri, Mohamed
TI Brain tumor classification based on hybrid approach
SO VISUAL COMPUTER
LA English
DT Article
DE MRI; Classification; Brain tumor; DSURF; HoG
ID FEATURE-EXTRACTION; IMAGES; SEGMENTATION; ALGORITHM; RECOGNITION;
   POPULATION; REDUCTION; FEATURES; HOG; ANN
AB Various computer systems have attracted more researchers' attention to arrive at a qualitative diagnosis in a few times. Different brain tumor classification approaches are proposed due to lesion complexity. This complexity makes the early tumor diagnosis using magnetic resonance images (MRI) a hard step. However, the accuracy of these techniques requires a significant amelioration to meet the needs of real-world diagnostic situations. We aim to classify three brain tumor types in this paper. A new technique is suggested which provides excellent results and surpasses the previous schemes. The proposed scheme makes use of the normalization, dense speeded up robust features, and histogram of gradient approaches to ameliorate MRI quality and generate a discriminative feature set. We exploit support vector machine in the classification step. The suggested system is benchmarked on an important dataset. The accuracy achieved based on this scheme is 90.27%. This method surpassed the most recent system according to experimental results. The results were earned through a strict statistical analysis (k-fold cross-validation), which proves the reliability and robustness of the suggested method.
C1 [Ayadi, Wadhah] Univ Monastir, Lab Elect & Microelect, Monastir 5000, Tunisia.
   [Charfi, Imen] Univ Sousse, Higher Sch Sci & Technol Hammam Sousse, Sousse, Tunisia.
   [Elhamzi, Wajdi] Prince Sattam Bin Abdulaziz Univ, Coll Comp Sci & Engn, Al Kharj, Saudi Arabia.
   [Atri, Mohamed] King Khalid Univ, Coll Comp Sci, Abha, Saudi Arabia.
C3 Universite de Monastir; Universite de Sousse; Prince Sattam Bin
   Abdulaziz University; King Khalid University
RP Elhamzi, W (corresponding author), Prince Sattam Bin Abdulaziz Univ, Coll Comp Sci & Engn, Al Kharj, Saudi Arabia.
EM wadhah.ayadi@fsm.rnu.tn; imenecharfi@yahoo.fr; w.elhamzi@essths.rnu.tn;
   matri@kku.edu.sa
RI ATRI, Mohamed/C-4069-2014; Elhamzi, Wajdi/AGR-6299-2022
OI ATRI, Mohamed/0000-0001-8528-5647; Elhamzi, Wajdi/0000-0002-8516-763X;
   ayadi, wadhah/0000-0002-0763-946X
CR Abiwinanda N, 2019, IFMBE PROC, V68, P183, DOI 10.1007/978-981-10-9035-6_33
   Afshar P, 2018, IEEE IMAGE PROC, P3129, DOI 10.1109/ICIP.2018.8451379
   Ain Q, 2014, APPL SOFT COMPUT, V21, P330, DOI 10.1016/j.asoc.2014.03.019
   Al-Shaikhli SDS, 2014, IEEE IMAGE PROC, P2774, DOI 10.1109/ICIP.2014.7025561
   Ayadi W, 2019, BIOMED SIGNAL PROCES, V48, P144, DOI 10.1016/j.bspc.2018.10.010
   Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014
   Caulo M, 2014, RADIOLOGY, V272, P494, DOI 10.1148/radiol.14132040
   Chen JW, 2015, J NEUROL SCI, V356, P148, DOI 10.1016/j.jns.2015.06.036
   Cheng J, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0115339
   CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1007/BF00994018
   Das S, 2013, PROG ELECTROMAGN RES, V137, P1, DOI 10.2528/PIER13010105
   de Robles P, 2015, NEURO-ONCOLOGY, V17, P776, DOI 10.1093/neuonc/nou283
   Deepak S, 2019, COMPUT BIOL MED, V111, DOI 10.1016/j.compbiomed.2019.103345
   Dogra J, 2020, VISUAL COMPUT, V36, P875, DOI 10.1007/s00371-019-01698-3
   Ergin S, 2014, COMPUT BIOL MED, V51, P171, DOI 10.1016/j.compbiomed.2014.05.008
   Fazl-Ersi E, 2019, VISUAL COMPUT, V35, P1447, DOI 10.1007/s00371-018-1510-1
   Feng RT, 2019, ISPRS J PHOTOGRAMM, V151, P15, DOI 10.1016/j.isprsjprs.2019.03.002
   Gordillo N, 2013, MAGN RESON IMAGING, V31, P1426, DOI 10.1016/j.mri.2013.05.002
   Gupta T, 2017, PATTERN RECOGN LETT, V139, P1
   Hang XY, 2009, J BIOMED BIOTECHNOL, DOI 10.1155/2009/403689
   Hemanth DJ, 2014, NEUROCOMPUTING, V130, P98, DOI 10.1016/j.neucom.2011.12.066
   Huang MY, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0102754
   Iqbal S, 2018, BIOMED ENG LETT, V8, P5, DOI 10.1007/s13534-017-0050-3
   Jiang J, 2013, COMPUT MED IMAG GRAP, V37, P512, DOI 10.1016/j.compmedimag.2013.05.007
   Kaashki NN, 2018, J VIS COMMUN IMAGE R, V52, P66, DOI 10.1016/j.jvcir.2018.02.003
   Kalbkhani H, 2013, BIOMED SIGNAL PROCES, V8, P909, DOI 10.1016/j.bspc.2013.09.001
   Kashif M, 2016, COMPUT BIOL MED, V68, P67, DOI 10.1016/j.compbiomed.2015.11.006
   Khosla A, 2020, BIOCYBERN BIOMED ENG, V40, P649, DOI 10.1016/j.bbe.2020.02.002
   Komorkiewicz M., 2012, 2012 22nd International Conference on Field Programmable Logic and Applications (FPL), P711, DOI 10.1109/FPL.2012.6339159
   Korkmaz SA, 2018, J MOL STRUCT, V1156, P255, DOI 10.1016/j.molstruc.2017.11.093
   Li XH, 2019, IEEE GEOSC REM SEN M, V7, P8, DOI 10.1109/MGRS.2019.2921780
   Li XH, 2014, IEEE T GEOSCI REMOTE, V52, P7086, DOI 10.1109/TGRS.2014.2307354
   Lin BJ, 2014, J NEUROSURG, V121, P1201, DOI 10.3171/2014.7.JNS132359
   Liu BZ, 2018, VISUAL COMPUT, V34, P707, DOI 10.1007/s00371-017-1408-3
   Liu YH, 2012, J MED BIOL ENG, V32, P22, DOI 10.5405/jmbe.813
   Nabizadeh N, 2015, COMPUT ELECTR ENG, V45, P286, DOI 10.1016/j.compeleceng.2015.02.007
   Nagpal J, 2015, 2015 INTERNATIONAL CONFERENCE ON SIGNAL PROCESSING AND COMMUNICATION (ICSC), P216, DOI 10.1109/ICSPCom.2015.7150650
   Papageorgiou TS, 2015, PHYS MEDICA, V31, P767, DOI 10.1016/j.ejmp.2015.03.010
   Paul JS, 2017, PROC SPIE, V10137, DOI 10.1117/12.2254195
   Rahim MSM, 2017, BIOMED RES-INDIA, V28, P4660
   Rahim MSM, 2017, BIOMED RES-INDIA, V28, P3641
   Saba T, 2014, NEURAL COMPUT APPL, V25, P1337, DOI 10.1007/s00521-014-1618-9
   Sachdeva J, 2016, APPL SOFT COMPUT, V47, P151, DOI 10.1016/j.asoc.2016.05.020
   Sarhan AM., 2020, J Biomed Sci Eng, V13, P102, DOI DOI 10.4236/JBISE.2020.136010
   Shanker R, 2020, BIOCYBERN BIOMED ENG, V40, P815, DOI 10.1016/j.bbe.2020.03.003
   Shanthakumar P, 2015, COMPUT ELECTR ENG, V45, P302, DOI 10.1016/j.compeleceng.2015.05.011
   Shim JS, 2018, J SUPERCOMPUT, V74, P787, DOI 10.1007/s11227-017-2160-1
   Siegel RL, 2021, CA-CANCER J CLIN, V71, P7, DOI [10.3322/caac.21387, 10.3322/caac.20073, 10.3322/caac.21332, 10.3322/caac.21601, 10.3322/caac.21254, 10.3322/caac.21654, 10.3322/caac.20006, 10.3322/caac.21551]
   Srisamosorn V, 2019, VISUAL COMPUT, V36, P1
   Tahir B, 2019, MICROSC RES TECHNIQ, V82, P803, DOI 10.1002/jemt.23224
   Tamimi AF, 2015, NEUROEPIDEMIOLOGY, V45, P100, DOI 10.1159/000438926
   Tellez ES, 2018, KNOWL-BASED SYST, V149, P110, DOI 10.1016/j.knosys.2018.03.003
   Togaçar M, 2020, BIOCYBERN BIOMED ENG, V40, P23, DOI 10.1016/j.bbe.2019.11.004
   Tsougos I, 2014, PHYS MEDICA, V30, P38, DOI [10.1016/j.ejmp.2014.07.118, DOI 10.1016/J.EJMP.2014.07.118]
   Wang HF, 2018, EUR J OPER RES, V267, P687, DOI 10.1016/j.ejor.2017.12.001
   Wang J, 2010, APPL MECH MATER, V36, P96, DOI [10.4028/www.scientific.net/AMM.36.96, 10.1109/ICICTA.2010.35]
   Wang SH, 2015, INT J IMAG SYST TECH, V25, P153, DOI 10.1002/ima.22132
   Wu Y, 2013, J DIGIT IMAGING, V26, P786, DOI 10.1007/s10278-012-9568-1
   Xi PC, 2020, VISUAL COMPUT, V36, P1869, DOI 10.1007/s00371-019-01775-7
   Zhang YD, 2015, BIOMED SIGNAL PROCES, V21, P58, DOI 10.1016/j.bspc.2015.05.014
   Zhi RC, 2020, VISUAL COMPUT, V36, P1067, DOI 10.1007/s00371-019-01707-5
NR 61
TC 42
Z9 42
U1 1
U2 10
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2022
VL 38
IS 1
BP 107
EP 117
DI 10.1007/s00371-020-02005-1
EA NOV 2020
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA YW6DE
UT WOS:000585765600001
DA 2024-07-18
ER

PT J
AU Amaranageswarao, G
   Deivalakshmi, S
   Ko, SB
AF Amaranageswarao, Gadipudi
   Deivalakshmi, S.
   Ko, Seok-Bum
TI Joint restoration convolutional neural network for low-quality image
   super resolution
SO VISUAL COMPUTER
LA English
DT Article
DE Blocking artifacts; Cross residual connections; Dense residual blocks;
   Ringing; Skip connections
ID SUPERRESOLUTION; DEBLOCKING; ARTIFACTS
AB In this paper, a joint restoration convolutional neural network (JRCNN) is proposed to produce a visually pleasing super resolution (SR) image from a single low-quality (LQ) image. The LQ image is a low resolution (LR) image with ringing, blocking and blurring artifacts arising due to compression. JRCNN consists of three deep dense residual blocks (DRB). Each DRB comprises of parallel convolutional layers with cross residual connections. The representational power of JRCNN is improved by depth-wise concatenation of feature representations from each of the DRBs. Moreover, these connections mitigate the problem of vanishing of gradients. Different from the previous networks, JRCNN exploits the contextual information directly in the LR image space without using any interpolation. This strategy improves the training efficiency of the network. The exhaustive experimentation on different datasets show that the proposed JRCNN produces state-of-the-art performance. Furthermore, ablation experiments are performed to assess the effectiveness of JRCNN. In addition, individual experiments are conducted for SR and compression artifact removal on benchmark datasets.
C1 [Amaranageswarao, Gadipudi; Deivalakshmi, S.] Natl Inst Technol, Dept Elect & Commun Engn, Thiruchirappalli, India.
   [Ko, Seok-Bum] Univ Saskatchewan, Dept Elect & Comp Engn, Saskatoon, SK, Canada.
C3 National Institute of Technology (NIT System); National Institute of
   Technology Tiruchirappalli; University of Saskatchewan
RP Amaranageswarao, G (corresponding author), Natl Inst Technol, Dept Elect & Commun Engn, Thiruchirappalli, India.
EM amar.nag9@gmail.com
RI Ko, Seokbum/H-8366-2012; Deivalakshmi, S/AAM-9082-2021; Amaranageswarao,
   Gadipudi/AAM-8173-2021
OI Ko, Seokbum/0000-0002-9287-317X; Amaranageswarao,
   Gadipudi/0000-0002-8022-1481; Deivalakshmi, S/0000-0002-7019-9807
CR Ahn N, 2018, LECT NOTES COMPUT SC, V11214, P256, DOI 10.1007/978-3-030-01249-6_16
   Amaranageswarao G, 2020, APPL INTELL, V50, P2177, DOI 10.1007/s10489-020-01670-y
   Bevilacqua M, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.135
   Cavigelli L, 2017, IEEE IJCNN, P752, DOI 10.1109/IJCNN.2017.7965927
   Chen HG, 2018, NEUROCOMPUTING, V285, P204, DOI 10.1016/j.neucom.2018.01.043
   Chen YJ, 2017, IEEE T PATTERN ANAL, V39, P1256, DOI 10.1109/TPAMI.2016.2596743
   Dong C, 2016, LECT NOTES COMPUT SC, V9906, P391, DOI 10.1007/978-3-319-46475-6_25
   Dong C, 2015, IEEE I CONF COMP VIS, P576, DOI 10.1109/ICCV.2015.73
   Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281
   Fan YC, 2017, IEEE COMPUT SOC CONF, P1157, DOI 10.1109/CVPRW.2017.154
   Foi A, 2007, IEEE T IMAGE PROCESS, V16, P1395, DOI 10.1109/TIP.2007.891788
   Huang JB, 2015, PROC CVPR IEEE, P5197, DOI 10.1109/CVPR.2015.7299156
   Kang LW, 2015, IEEE T MULTIMEDIA, V17, P921, DOI 10.1109/TMM.2015.2434216
   Kayhan SK, 2016, VISUAL COMPUT, V32, P417, DOI 10.1007/s00371-015-1068-0
   KEYS RG, 1981, IEEE T ACOUST SPEECH, V29, P1153, DOI 10.1109/TASSP.1981.1163711
   Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.182, 10.1109/CVPR.2016.181]
   Kim Y, 2020, IEEE T CIRC SYST VID, V30, P1121, DOI 10.1109/TCSVT.2019.2901919
   Lai WS, 2019, IEEE T PATTERN ANAL, V41, P2599, DOI 10.1109/TPAMI.2018.2865304
   Lan RS, 2021, IEEE T CYBERNETICS, V51, P115, DOI 10.1109/TCYB.2019.2952710
   Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151
   Liu PJ, 2018, IEEE COMPUT SOC CONF, P886, DOI 10.1109/CVPRW.2018.00121
   Ma TS, 2021, VISUAL COMPUT, V37, P925, DOI 10.1007/s00371-020-01843-3
   Mao XJ, 2016, ADV NEUR IN, V29
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Matsui Y, 2017, MULTIMED TOOLS APPL, V76, P21811, DOI 10.1007/s11042-016-4020-z
   Qiu YJ, 2019, IEEE I CONF COMP VIS, P4179, DOI 10.1109/ICCV.2019.00428
   Song Q, 2018, IEEE DATA COMPR CONF, P97, DOI 10.1109/DCC.2018.00018
   Tai Y, 2017, IEEE I CONF COMP VIS, P4549, DOI 10.1109/ICCV.2017.486
   Tai Y, 2017, PROC CVPR IEEE, P2790, DOI 10.1109/CVPR.2017.298
   Timofte R, 2017, IEEE COMPUT SOC CONF, P1110, DOI 10.1109/CVPRW.2017.149
   Timofte R, 2015, LECT NOTES COMPUT SC, V9006, P111, DOI 10.1007/978-3-319-16817-3_8
   Wallace G. K., 1991, Communications of the ACM, V34, P30, DOI 10.1145/103085.103089
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Xiao J, 2012, 2012 5TH INTERNATIONAL CONGRESS ON IMAGE AND SIGNAL PROCESSING (CISP), P607
   Zeyde R., 2012, INT C CURV SURF, P711, DOI DOI 10.1007/978-3-642-27413-8_47
   Zhang J, 2016, IEEE T IMAGE PROCESS, V25, P1246, DOI 10.1109/TIP.2016.2515985
   Zhang K, 2018, PROC CVPR IEEE, P3262, DOI 10.1109/CVPR.2018.00344
   Zhang K, 2017, IEEE T IMAGE PROCESS, V26, P3142, DOI 10.1109/TIP.2017.2662206
   Zhang Y., 2018, COMPUTER VISION ECCV, V294, P310, DOI DOI 10.1007/978-3-030-01234-2_182-S2.0-85055092808
   Zhao C, 2017, IEEE T CIRC SYST VID, V27, P2057, DOI 10.1109/TCSVT.2016.2580399
NR 40
TC 10
Z9 10
U1 1
U2 12
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2022
VL 38
IS 1
BP 31
EP 50
DI 10.1007/s00371-020-01998-z
EA NOV 2020
PG 20
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA YW6DE
UT WOS:000583685000001
DA 2024-07-18
ER

PT J
AU Arulmozhi, P
   Abirami, S
AF Arulmozhi, P.
   Abirami, S.
TI DSHPoolF: deep supervised hashing based on selective pool feature map
   for image retrieval
SO VISUAL COMPUTER
LA English
DT Article
DE Learning-based hashing; Convolutional neural network; Deep supervised
   hashing; Image retrieval
ID NEAREST-NEIGHBOR SEARCH; INFORMATION
AB Deep supervised hashing has turned up to unravel many large-scale image retrieval challenges. Although deep supervised hashing accomplishes good results for image retrieval process, requisite for further improving the retrieval accuracy always remains the primal focus of interest. In Deep hashing methods, feature representation happens at the outset of the fully connected (FC) layers, causing shortage of spatial information owing to its global nature, whereas deeper pooling layers preserve semantically similar information by retaining the images spatial information, which can result in uplifting the retrieval performance. Hereby, for enhancing the image retrieval accuracy through exploring spatial information, a novel way of deep supervised hashing based on Pooled Feature map (DSHPoolF) is proposed to generate compact hash codes that explore the spatial information by weighing the informative Feature maps from the last pooling layer. This is achieved, firstly, by weighing the last pooling layers Feature map in two ways, namely average-max-based pooling and probability-based pooling strategies. Secondly, informative Feature maps are selected with the help of the weights. In addition to this, the informative Feature maps play a key role in optimizing quantization error together with the loss function and classification errors in a single-step, point-wise ranking manner. This proposed DSHPoolF method is assessed using three datasets (CIFAR-10, MNIST and ImageNet) that unveils primitive outcome in comparison with other existing prominent hash-based methods.
C1 [Arulmozhi, P.; Abirami, S.] Anna Univ, Dept Informat Sci & Technol, Chennai, Tamil Nadu, India.
C3 Anna University; Anna University Chennai
RP Arulmozhi, P (corresponding author), Anna Univ, Dept Informat Sci & Technol, Chennai, Tamil Nadu, India.
EM arulmozhikec@gmail.com; abirami@auist.net
RI S, Abirami/AHH-8969-2022; Alsaif, Amal/IUO-9428-2023
OI S, Abirami/0000-0002-2028-5344; Alsaif, Amal/0000-0002-8204-0326
CR Agrawal A, 2020, VISUAL COMPUT, V36, P405, DOI 10.1007/s00371-019-01630-9
   Ahmed KT, 2019, INFORM FUSION, V51, P76, DOI 10.1016/j.inffus.2018.11.004
   Alzu'bi A, 2017, NEUROCOMPUTING, V249, P95, DOI 10.1016/j.neucom.2017.03.072
   Alzu'bi A, 2015, J VIS COMMUN IMAGE R, V32, P20, DOI 10.1016/j.jvcir.2015.07.012
   [Anonymous], 2015, ARXIV150906033
   [Anonymous], 2011, ICML
   [Anonymous], 2009, NIPS
   Arulmozhi P, 2019, ARTIF INTELL REV, V52, P323, DOI 10.1007/s10462-017-9591-1
   Bianco S, 2018, SIGNAL IMAGE VIDEO P, V12, P355, DOI 10.1007/s11760-017-1166-8
   Cao ZJ, 2017, IEEE I CONF COMP VIS, P5609, DOI 10.1109/ICCV.2017.598
   Celik C, 2017, PATTERN RECOGN, V68, P1, DOI 10.1016/j.patcog.2017.03.006
   Chechik G, 2010, J MACH LEARN RES, V11, P1109
   Cheng JD, 2019, OPTIK, V180, P847, DOI 10.1016/j.ijleo.2018.11.145
   Cheng SL, 2019, VISUAL COMPUT, V35, P1255, DOI 10.1007/s00371-018-1583-x
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Esmaeili MM, 2012, IEEE T PATTERN ANAL, V34, P2481, DOI 10.1109/TPAMI.2012.170
   Gong YC, 2013, IEEE T PATTERN ANAL, V35, P2916, DOI 10.1109/TPAMI.2012.193
   Gonzalez-Garcia A, 2018, INT J COMPUT VISION, V126, P476, DOI 10.1007/s11263-017-1048-0
   Gordo A, 2017, INT J COMPUT VISION, V124, P237, DOI 10.1007/s11263-017-1016-8
   Gordo A, 2016, LECT NOTES COMPUT SC, V9910, P241, DOI 10.1007/978-3-319-46466-4_15
   Gorisse D, 2012, IEEE T PATTERN ANAL, V34, P402, DOI 10.1109/TPAMI.2011.193
   Grauman K., 2013, Machine Learning for Computer Vision, V411, P49, DOI 10.1007/978-3-642-28661-2_3
   Guan H, 2018, VISUAL COMPUT, V34, P1701, DOI 10.1007/s00371-017-1445-y
   He T, 2019, J VIS COMMUN IMAGE R, V65, DOI 10.1016/j.jvcir.2019.102654
   Islam SM, 2017, APPL SOFT COMPUT, V57, P102, DOI 10.1016/j.asoc.2017.03.036
   Ji J., 2012, 25 INT C NEUR INF PR, P108
   Jiang QY, 2018, IEEE T IMAGE PROCESS, V27, P5996, DOI 10.1109/TIP.2018.2864894
   Kan SC, 2019, SIGNAL PROCESS-IMAGE, V78, P494, DOI 10.1016/j.image.2019.08.009
   Komorowski M, 2019, APPL SOFT COMPUT, V79, P87, DOI 10.1016/j.asoc.2019.03.031
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kulis B, 2009, IEEE I CONF COMP VIS, P2130, DOI 10.1109/ICCV.2009.5459466
   Lai HJ, 2015, PROC CVPR IEEE, P3270, DOI 10.1109/CVPR.2015.7298947
   LeCun Y., 2010, MNIST HANDWRITTEN DI
   Li JJ, 2019, COMPUT INTEL NEUROSC, V2019, DOI 10.1155/2019/4179397
   Li QR, 2020, MATH PROBL ENG, V2020, DOI 10.1155/2020/7528571
   Li W.-J., 2015, ARXIV151103855
   Li ZC, 2020, INT J COMPUT VISION, V128, P2265, DOI 10.1007/s11263-020-01331-0
   Lin J, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2266
   Lin Kevin, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P27, DOI 10.1109/CVPRW.2015.7301269
   Liong VE, 2015, PROC CVPR IEEE, P2475, DOI 10.1109/CVPR.2015.7298862
   Liu HM, 2016, PROC CVPR IEEE, P2064, DOI 10.1109/CVPR.2016.227
   Liu H, 2019, IEEE T PATTERN ANAL, V41, P941, DOI 10.1109/TPAMI.2018.2819978
   Liu W, 2012, PROC CVPR IEEE, P2074, DOI 10.1109/CVPR.2012.6247912
   Liu WF, 2017, ADV COND MATTER PHYS, V2017, DOI 10.1155/2017/4296243
   Ma Q, 2019, PATTERN RECOGN, V92, P156, DOI 10.1016/j.patcog.2019.03.022
   Ma WC, 2020, PATTERN RECOGN, V100, DOI 10.1016/j.patcog.2019.107149
   Mahendran A, 2016, INT J COMPUT VISION, V120, P233, DOI 10.1007/s11263-016-0911-8
   Rodrigues J, 2020, ARTIF INTELL REV, V53, P5261, DOI 10.1007/s10462-020-09820-x
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Sakr NA, 2016, COMPUT ELECTR ENG, V54, P522, DOI 10.1016/j.compeleceng.2016.04.015
   Salakhutdinov R, 2009, INT J APPROX REASON, V50, P969, DOI 10.1016/j.ijar.2008.11.006
   Shen FM, 2015, PROC CVPR IEEE, P37, DOI 10.1109/CVPR.2015.7298598
   Shen ZY, 2019, IMAGE VISION COMPUT, V85, P14, DOI 10.1016/j.imavis.2019.03.004
   Shengjing T., 2019, VISUAL COMPUT, P1
   Shi ZL, 2016, NEURAL NETWORKS, V83, P21, DOI 10.1016/j.neunet.2016.07.003
   Tang JH, 2018, IEEE T NEUR NET LEAR, V29, P6154, DOI 10.1109/TNNLS.2018.2816743
   Tang JH, 2018, PATTERN RECOGN, V75, P25, DOI 10.1016/j.patcog.2017.03.028
   Wang JD, 2018, IEEE T PATTERN ANAL, V40, P769, DOI 10.1109/TPAMI.2017.2699960
   Wang J, 2012, IEEE T PATTERN ANAL, V34, P2393, DOI 10.1109/TPAMI.2012.48
   Weiss Yair, 2009, Advances in Neural Information Processing Systems, P1753, DOI DOI 10.5555/2981780.2981999
   Xia RK, 2014, AAAI CONF ARTIF INTE, P2156
   Xie WC, 2019, PATTERN RECOGN, V96, DOI 10.1016/j.patcog.2019.106966
   Yang H, 2020, VISUAL COMPUT, V36, P559, DOI 10.1007/s00371-019-01641-6
   Yang HF, 2018, IEEE T PATTERN ANAL, V40, P437, DOI 10.1109/TPAMI.2017.2666812
   Yu JG, 2019, EURASIP J WIREL COMM, DOI 10.1186/s13638-019-1365-9
   Yuan J, 2019, KNOWL-BASED SYST, V172, P86, DOI 10.1016/j.knosys.2019.02.013
   Zhang J, 2019, IEEE T CIRC SYST VID, V29, P212, DOI 10.1109/TCSVT.2017.2771332
   Zhang RM, 2015, IEEE T IMAGE PROCESS, V24, P4766, DOI 10.1109/TIP.2015.2467315
   Zhang WH, 2016, KNOWL-BASED SYST, V97, P40, DOI 10.1016/j.knosys.2016.01.022
   Zhang XN, 2019, PATTERN RECOGN LETT, V125, P677, DOI 10.1016/j.patrec.2019.07.010
   Zhao F, 2015, PROC CVPR IEEE, P1556, DOI 10.1109/CVPR.2015.7298763
   Zhong GQ, 2016, IEEE IJCNN, P2236, DOI 10.1109/IJCNN.2016.7727476
   Zhu H, 2016, AAAI CONF ARTIF INTE, P2415
NR 73
TC 5
Z9 5
U1 0
U2 7
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2021
VL 37
IS 8
BP 2391
EP 2405
DI 10.1007/s00371-020-01993-4
EA OCT 2020
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TT5FX
UT WOS:000582814700001
DA 2024-07-18
ER

PT J
AU Khan, D
   Plopski, A
   Fujimoto, Y
   Kanbara, M
   Cheng, ZL
   Kato, H
AF Khan, Dawar
   Plopski, Alexander
   Fujimoto, Yuichiro
   Kanbara, Masayuki
   Cheng, Zhanglin
   Kato, Hirokazu
TI Valence optimization and angle improvement for molecular surface
   remeshing
SO VISUAL COMPUTER
LA English
DT Article
DE Molecular surface remeshing; Mesh quality; Valence optimization;
   Molecular modeling
ID MESH GENERATION; EFFICIENT
AB Molecular surface mesh generation plays a vital role in molecular modeling and visualization. However, meshes extracted directly from Protein Data Bank files have several issues such as small and large triangles, redundant elements, self-intersections, and irregular vertices. The state-of-the-art mesh improvement methods often fail to deal with these issues. In this paper, we present a novel method for valence optimization and angle improvement. For valence optimization, we remove the bad valence vertices with its neighbor triangle making regular holes in the mesh. The holes are filled in a careful manner to improve their valences as well as angle quality. We also use a segmentation-based surface remeshing which segments the mesh into random segments and then each segment is independently remeshed. In addition, a point insertion scheme is applied to minimize the ratio of obtuse triangles. Experimental results show that our method not only improves the maximal and minimal angles to an angle bound of[30 degrees 120 degrees] but also improves the vertices' regularity, reduces the ratio of obtuse triangles, preserves the area and volume, and always succeeds with downstream applications.
C1 [Khan, Dawar; Fujimoto, Yuichiro; Kanbara, Masayuki; Kato, Hirokazu] Nara Inst Sci & Technol, Interact Media Design Lab, Nara 6300192, Japan.
   [Plopski, Alexander] Univ Otago, Dept Informat Sci, Dunedin, New Zealand.
   [Cheng, Zhanglin] Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen VisuCA Key Lab, Shenzhen 518055, Peoples R China.
C3 Nara Institute of Science & Technology; University of Otago; Chinese
   Academy of Sciences; Shenzhen Institute of Advanced Technology, CAS
RP Khan, D (corresponding author), Nara Inst Sci & Technol, Interact Media Design Lab, Nara 6300192, Japan.
EM dawar@is.naist.jp
RI Fujimoto, Yuichiro/HTS-3801-2023; Kanbara, Masayuki/ABD-7780-2021;
   Cheng, Zhanglin/AAP-1760-2021
OI Cheng, Zhanglin/0000-0002-3360-2679; Khan, Dawar/0000-0001-5864-1888;
   Kato, Hirokazu/0000-0003-3921-2871
FU Japan Society for the Promotion of Science (JSPS) KAKENHI [19K24346];
   National Natural Science Foundation of China [61972388]; Shenzhen Basic
   Research Program [JCYJ20180507182222355]; Grants-in-Aid for Scientific
   Research [19K24346] Funding Source: KAKEN
FX This work is partially funded by the Japan Society for the Promotion of
   Science (JSPS) KAKENHI (19K24346), the National Natural Science
   Foundation of China (61972388), and Shenzhen Basic Research Program
   (JCYJ20180507182222355). We are thankful to the anonymous reviewers for
   their valuable comments.
CR Aghdaii N, 2012, COMPUT GRAPH-UK, V36, P1072, DOI 10.1016/j.cag.2012.09.005
   Bates PW, 2008, J COMPUT CHEM, V29, P380, DOI 10.1002/jcc.20796
   Chan SL, 1998, J COMPUT CHEM, V19, P1268, DOI 10.1002/(SICI)1096-987X(199808)19:11<1268::AID-JCC6>3.0.CO;2-I
   Chen L, 2011, COMPUT METHOD APPL M, V200, P967, DOI 10.1016/j.cma.2010.11.007
   Chen MX, 2012, J MOL GRAPH MODEL, V38, P411, DOI 10.1016/j.jmgm.2012.09.006
   Chen WY, 2010, COMPUT AIDED DESIGN, V42, P267, DOI 10.1016/j.cad.2009.10.003
   Cheng HL, 2009, COMP GEOM-THEOR APPL, V42, P196, DOI 10.1016/j.comgeo.2008.10.001
   Cheng Siu-Wing, 2012, Delaunay mesh generation
   Decherchi S, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0059744
   Dolinsky TJ, 2004, NUCLEIC ACIDS RES, V32, pW665, DOI 10.1093/nar/gkh381
   Dunyach M., 2013, EUROGRAPHICS
   Edelsbrunner H, 1999, DISCRETE COMPUT GEOM, V21, P87, DOI 10.1007/PL00009412
   Fang QQ, 2009, I S BIOMED IMAGING, P1142, DOI 10.1109/ISBI.2009.5193259
   Frey P, 1997, P 6 INT MESH ROUNDT, P363
   Gerstein M., 2000, INT TABLES CRYSTALLO, pf:531
   Gui S, 2018, VIS COMPUT IND BIOME, V1, DOI 10.1186/s42492-018-0007-0
   Hu KM, 2017, IEEE T VIS COMPUT GR, V23, P2560, DOI 10.1109/TVCG.2016.2632720
   Jakob W, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818078
   Khan Dawar, 2018, Computational Visual Media, V4, P113, DOI 10.1007/s41095-018-0107-y
   Khan D, 2018, INT J MOL SCI, V19, DOI 10.3390/ijms19051383
   Khan DW, 2018, COMPUT MATH APPL, V75, P582, DOI 10.1016/j.camwa.2017.09.041
   LEE B, 1971, J MOL BIOL, V55, P379, DOI 10.1016/0022-2836(71)90324-X
   Liu TT, 2018, SIAM J SCI COMPUT, V40, pB507, DOI 10.1137/16M1099704
   Liu TT, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0184206
   Liu TT, 2015, J MOL MODEL, V21, DOI 10.1007/s00894-015-2654-9
   Liu Y, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1559755.1559758
   Liu YJ, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818076
   Ma GH, 2016, COMPUT GRAPH FORUM, V35, P89, DOI 10.1111/cgf.12689
   Men Y., 2018, SIGGRAPH POSTERS, p66:1
   Quan CY, 2017, J MOL GRAPH MODEL, V71, P200, DOI 10.1016/j.jmgm.2016.11.008
   Quan C, 2016, J COMPUT PHYS, V322, P760, DOI 10.1016/j.jcp.2016.07.007
   RICHARDS FM, 1977, ANNU REV BIOPHYS BIO, V6, P151, DOI 10.1146/annurev.bb.06.060177.001055
   Sanner MF, 1996, BIOPOLYMERS, V38, P305, DOI 10.1002/(SICI)1097-0282(199603)38:3<305::AID-BIP4>3.0.CO;2-Y
   Schreiner J, 2006, COMPUT GRAPH FORUM, V25, P527, DOI 10.1111/j.1467-8659.2006.00972.x
   Si H, 2015, ACM T MATH SOFTWARE, V41, DOI 10.1145/2629697
   Takayama Kenshi, 2014, Journal of Computer Graphics Techniques (JCGT), V3, P53
   Taubin G., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P351, DOI 10.1145/218380.218473
   Vidal V, 2015, COMPUT GRAPH-UK, V47, P16, DOI 10.1016/j.cag.2014.10.004
   Wang J, 2009, PROCEEDINGS OF THE 18TH INTERNATIONAL MESHING ROUNDTABLE, P195, DOI 10.1007/978-3-642-04319-2_12
   Wang YQ, 2019, IEEE T VIS COMPUT GR, V25, P2430, DOI 10.1109/TVCG.2018.2837115
   Yan DM, 2016, IEEE T VIS COMPUT GR, V22, P2136, DOI 10.1109/TVCG.2015.2505279
   Yan DM, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2516971.2516973
   Zhang B, 2015, COMPUT PHYS COMMUN, V190, P173, DOI 10.1016/j.cpc.2014.12.022
NR 43
TC 3
Z9 3
U1 1
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2020
VL 36
IS 10-12
BP 2355
EP 2368
DI 10.1007/s00371-020-01967-6
EA SEP 2020
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA NW1CX
UT WOS:000565484100002
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Barlas, G
   Veinidis, C
   Arampatzis, A
AF Barlas, Georgios
   Veinidis, Christos
   Arampatzis, Avi
TI What we see in a photograph: content selection for image captioning
SO VISUAL COMPUTER
LA English
DT Article
DE Image captioning; Computer vision; Image cognition
ID REPRESENTATION; WORDNET; MODELS
AB We propose and experimentally investigate the usefulness of several features for selecting image content (objects) suitable for image captioning. The approach taken explores three broad categories of features, namely geometric, conceptual, and visual. Experiments suggest that widely known geometric 'rules' in art-aesthetics or photography (such as the golden ratio or the rule-of-thirds) and facts about the human visual system (such as its wider horizontal angle than its vertical) provide no useful information for the task. Human captioners seem to prefer large, elongated (but not in the golden ratio) objects, positioned near the image center, irrespective of orientation. Conceptually, the preferred objects are either too specific or too general, and animate things are almost always mentioned; furthermore, some evidence is found for selecting diverse objects in order to achieve maximal image coverage in captions. Visual object features such as saliency, depth, edges, entropy, and contrast, are all found to provide useful information. Beyond evaluating features in isolation, we investigate how well these are combined by performing feature and feature-category ablation studies, leading to an effective set of features which can be proven useful for operational systems. Moreover, we propose alternative ways for feature engineering and evaluation, dealing with the drawbacks of the evaluation methodology proposed in past literature.
C1 [Barlas, Georgios; Veinidis, Christos; Arampatzis, Avi] Democritus Univ Thrace, Dept Elect & Comp Engn, Xanthi 67100, Greece.
C3 Democritus University of Thrace
RP Veinidis, C (corresponding author), Democritus Univ Thrace, Dept Elect & Comp Engn, Xanthi 67100, Greece.
EM gbarlas@ee.duth.gr; cveinidi@ee.duth.gr; avi@ee.duth.gr
OI Barlas, George/0000-0003-4418-262X
CR Anderson P., 2017, ARXIV170707998 CORR
   [Anonymous], 2014, ARXIV14050312 CORR
   [Anonymous], 2015, J PUBLIC MANAGEMENT
   Arampatzis A, 2015, INFORM RETRIEVAL J, V18, P331, DOI 10.1007/s10791-015-9256-0
   Bai S, 2018, NEUROCOMPUTING, V311, P291, DOI 10.1016/j.neucom.2018.05.080
   Barlas G., 2016, CLEF 2016 C LABS EV, P279
   Bejan A., 2009, Int. J. Des. Nat. Ecodyn., V4, P97, DOI DOI 10.2495/DNE-V4-N2-97-104
   Berg TL, 2010, LECT NOTES COMPUT SC, V6311, P663, DOI 10.1007/978-3-642-15549-9_48
   CAMPBELL FW, 1968, J PHYSIOL-LONDON, V197, P551, DOI 10.1113/jphysiol.1968.sp008574
   Chen X, 2015, PROC CVPR IEEE, P2422, DOI 10.1109/CVPR.2015.7298856
   Cheng MM, 2011, PROC CVPR IEEE, P409, DOI 10.1109/CVPR.2011.5995344
   Cristea A., 2016, CLEF 2016 C LABS EV, V1609, P288
   Dagnelie G, 2011, VISUAL PROSTHETICS: PHYSIOLOGY, BIOENGINEERING, REHABILITATION, P1, DOI 10.1007/978-1-4419-0754-7
   Datta R, 2006, LECT NOTES COMPUT SC, V3953, P288, DOI 10.1007/11744078_23
   Ethirajan Manivannan, 2008, P13
   Fang H, 2015, PROC CVPR IEEE, P1473, DOI 10.1109/CVPR.2015.7298754
   Farhadi A, 2010, LECT NOTES COMPUT SC, V6314, P15, DOI 10.1007/978-3-642-15561-1_2
   Farhadi A, 2009, PROC CVPR IEEE, P1778, DOI 10.1109/CVPRW.2009.5206772
   Felzenszwalb PF, 2004, INT J COMPUT VISION, V59, P167, DOI 10.1023/B:VISI.0000022288.19776.77
   Gan Z., 2016, ARXIV161108002 CORR
   Gilbert A., 2016, P CLEF EV PORT 5 8 S, P254
   Hirst G, 1998, LANG SPEECH & COMMUN, P305
   Hodosh M, 2013, J ARTIF INTELL RES, V47, P853, DOI 10.1613/jair.3994
   Jiang J.J., 1997, ARXIVCMPLG9709008 CO
   Klein SA, 1997, P SOC PHOTO-OPT INS, V3016, P13, DOI 10.1117/12.274510
   Kreyszig E., 2000, Advanced Engineering Mathematics: Maple Computer Guide, V8th
   Kulkarni G, 2013, IEEE T PATTERN ANAL, V35, P2891, DOI 10.1109/TPAMI.2012.162
   Leacock C, 1998, COMPUT LINGUIST, V24, P147
   Lin D., 1998, Machine Learning. Proceedings of the Fifteenth International Conference (ICML'98), P296
   Liu A, 2018, P 27 INT JOINT C ART, P821, DOI DOI 10.24963/IJCAI.2018/114
   Liu AA, 2017, COMPUT VIS IMAGE UND, V163, P113, DOI 10.1016/j.cviu.2017.04.013
   Liu FY, 2016, IEEE T PATTERN ANAL, V38, P2024, DOI 10.1109/TPAMI.2015.2505283
   Liu XX, 2019, VISUAL COMPUT, V35, P445, DOI 10.1007/s00371-018-1566-y
   Lu J., 2018, ARXIV180309845 CORR
   LUHN HP, 1957, IBM J RES DEV, V1, P309, DOI 10.1147/rd.14.0309
   Ma L, 2015, IEEE I CONF COMP VIS, P2623, DOI 10.1109/ICCV.2015.301
   MILLER GA, 1995, COMMUN ACM, V38, P39, DOI 10.1145/219717.219748
   Mullner D, 2011, ARXIV11092378 CORR
   Nian FD, 2017, COMPUT VIS IMAGE UND, V163, P126, DOI 10.1016/j.cviu.2017.06.012
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   Plummer B.A., 2015, ARXIV150504870 CORR
   Resnik P, 1995, INT JOINT CONF ARTIF, P448
   SHAPLEY RM, 1973, J PHYSIOL-LONDON, V229, P165, DOI 10.1113/jphysiol.1973.sp010133
   Sobel I., 1968, TALK STANFORD ARTIFI, P271
   Villegas M., 2016, GEN OVERVIEW IMAGECL, P267
   Villegas M, 2016, LECT NOTES COMPUT SC, V9822, P267, DOI 10.1007/978-3-319-44564-9_25
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   Wang JM, 2016, ADVANCES IN ENERGY, ENVIRONMENT AND MATERIALS SCIENCE, P9
   Wu Q., 2015, ARXIV150601144 CORR
   WU ZB, 1994, 32ND ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, P133
   Xu J, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P537, DOI 10.1145/3123266.3123448
   Xu N., 2018, 2018 INT JOINT C NEU, P1
   Yang Y., 2011, P C EMP METH NAT LAN, P444
NR 53
TC 6
Z9 6
U1 0
U2 14
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2021
VL 37
IS 6
BP 1309
EP 1326
DI 10.1007/s00371-020-01867-9
EA JUL 2020
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SO2LW
UT WOS:000547242400002
DA 2024-07-18
ER

PT J
AU Yan, XZ
   Wu, QS
   Liu, GH
   Sun, XY
AF Yan, Xuezhi
   Wu, Qiushuang
   Liu, Guohong
   Sun, Xiaoying
TI Improving the tactile perception of image textures based on adjustable
   amplitude, frequency, and waveform
SO VISUAL COMPUTER
LA English
DT Article
DE Tactile representation; Tactile rendering algorithm; Electrostatic
   tactile display system
ID FRICTION; DISPLAY; SHAPE
AB In this paper, we present a tactile rendering algorithm applied to an electrostatic tactile display that adjusts three parameters of the driving signal (amplitude, frequency, and waveform) to modulate the tangential friction force between a user's finger and a touch screen. The aim of this work is to find an effective electrostatic tactile rendering algorithm to improve the tactile perception of image textures. The key idea is to jointly adjust the three parameters of the driving signal to increase the perceptual difference interval between image textures. We first explore the tactile representation characteristics of amplitude, frequency and waveform through subjective perception experiments. Based on these characteristics, we establish tactile mapping models between the three parameters of the driving signal and image textures. Finally, we use subjective evaluation experiments to verify the effectiveness of the proposed rendering method. The results show that the proposed rendering method can achieve a better tactile experience compared with rendering methods that realize tactile representation by only varying amplitude.
C1 [Yan, Xuezhi; Wu, Qiushuang; Liu, Guohong; Sun, Xiaoying] Jilin Univ, 5372 Nanhu Rd, Changchun 130012, Peoples R China.
C3 Jilin University
RP Yan, XZ (corresponding author), Jilin Univ, 5372 Nanhu Rd, Changchun 130012, Peoples R China.
EM yanxz@jlu.edu.cn; wuqs17@mails.jlu.edu.cn; 18604429927@163.com;
   sunxy@jlu.edu.cn
OI /0000-0002-8380-413X
FU National Natural Science Foundation of China [61631010]
FX This work was partially supported by the National Natural Science
   Foundation of China (61631010).
CR Amberg Michel., 2011, P 24 ANN ACM S ADJUN, P7, DOI [10.1145/2046396.2046401, DOI 10.1145/2046396.2046401]
   Bau O., 2010, P 23 ANN ACM S US IN, P283, DOI DOI 10.1145/1866029.1866074
   Biet M, 2007, IEEE T ULTRASON FERR, V54, P2678, DOI 10.1109/TUFFC.2007.596
   Campbell P., 2008, NATURE, V455, P1, DOI DOI 10.1038/NATURE08077
   Culbertson H, 2017, IEEE T HAPTICS, V10, P63, DOI 10.1109/TOH.2016.2598751
   Hwang I, 2013, IEEE T HAPTICS, V6, P352, DOI 10.1109/TOH.2013.2
   ILKHANI G, 2014, INT C HUM HAPT SENS, V8618, P496, DOI DOI 10.1007/978-3-662-44193-0_62
   Jiao J, 2018, IEEE HAPTICS SYM, P169, DOI 10.1109/HAPTICS.2018.8357171
   Kaboli M, 2019, AUTON ROBOT, V43, P123, DOI 10.1007/s10514-018-9707-8
   KIM JH, 2017, ADV EDUC TECHNOL INS, P1, DOI DOI 10.4018/978-1-5225-2000-9.ch001
   Kim S.-C., 2013, P 26 ANN ACM S US IN, P531, DOI DOI 10.1145/2501988.2502020
   Kim SC, 2018, IEEE T HAPTICS, V11, P140, DOI 10.1109/TOH.2017.2768523
   Koo IM, 2008, IEEE T ROBOT, V24, P549, DOI 10.1109/TRO.2008.921561
   LEVITT H, 1971, J ACOUST SOC AM, V49, P467, DOI 10.1121/1.1912375
   Linjama Jukka, 2009, HAID 2009 4 WORKSH H
   Liu GH, 2018, IEEE T HAPTICS, V11, P51, DOI 10.1109/TOH.2017.2742514
   MALLINCKRODT E, 1953, SCIENCE, V118, P277, DOI 10.1126/science.118.3062.277
   Martínez J, 2013, VISUAL COMPUT, V29, P111, DOI 10.1007/s00371-012-0716-x
   Meyer DJ, 2014, IEEE HAPTICS SYM, P63, DOI 10.1109/HAPTICS.2014.6775434
   Miao YM, 2018, MOBILE NETW APPL, V23, P1645, DOI 10.1007/s11036-018-1110-3
   Minsky M., 1990, Computer Graphics, V24, P235, DOI 10.1145/91394.91451
   Okamoto S, 2013, IEEE T HAPTICS, V6, P81, DOI [10.1109/ToH.2012.32, 10.1109/TOH.2012.32]
   Osgouei RH, 2018, IEEE HAPTICS SYM, P270, DOI 10.1109/HAPTICS.2018.8357187
   Osgouei RH, 2017, IEEE T HAPTICS, V10, P533, DOI 10.1109/TOH.2017.2710314
   Prescher D, 2018, UNIVERSAL ACCESS INF, V17, P391, DOI 10.1007/s10209-017-0538-8
   Robles-De-La-Torre G, 2001, NATURE, V412, P445, DOI 10.1038/35086588
   Saga S., 2012, 2012 IEEE Haptics Symposium (HAPTICS), P15, DOI 10.1109/HAPTIC.2012.6183764
   Sianov A, 2018, IEEE T HAPTICS, V11, P388, DOI 10.1109/TOH.2018.2817483
   STRONG RM, 1970, IEEE T MAN MACHINE, VMM11, P72, DOI 10.1109/TMMS.1970.299965
   Vasudevan Hari., 2006, PROC IEEE INT WORKSH, P130
   Wang TT, 2014, 2014 INTERNATIONAL CONFERENCE ON AUDIO, LANGUAGE AND IMAGE PROCESSING (ICALIP), VOLS 1-2, P775, DOI 10.1109/ICALIP.2014.7009900
   Wiertlewski M, 2011, IEEE T ROBOT, V27, P461, DOI 10.1109/TRO.2011.2132830
   Wu SW, 2017, VISUAL COMPUT, V33, P637, DOI 10.1007/s00371-016-1214-3
   Xia PJ, 2013, VISUAL COMPUT, V29, P433, DOI 10.1007/s00371-012-0748-2
   Xu C., 2011, P 2011 ANN C EXTENDE, P317, DOI DOI 10.1145/1979742.1979705
   Yamamoto A, 2006, IEEE T VIS COMPUT GR, V12, P168, DOI 10.1109/TVCG.2006.28
   Yamamoto A, 2003, 2003 IEEE INTERNATIONAL CONFERENCE ON INDUSTRIAL TECHNOLOGY, VOLS 1 AND 2, PROCEEDINGS, P680
   Yan XZ, 2019, COMPUT J, V62, P1016, DOI 10.1093/comjnl/bxy109
   Yang GH, 2010, IEEE INT C INT ROBOT, P5245, DOI 10.1109/IROS.2010.5651759
   Yatani K, 2009, UIST 2009: PROCEEDINGS OF THE 22ND ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P111
NR 40
TC 0
Z9 0
U1 2
U2 26
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2021
VL 37
IS 6
BP 1297
EP 1308
DI 10.1007/s00371-020-01866-w
EA JUN 2020
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SO2LW
UT WOS:000539941100002
DA 2024-07-18
ER

PT J
AU Oh, I
   Ko, KH
AF Oh, Inyoung
   Ko, Kwnag Hee
TI Automated recognition of 3D pipelines from point clouds
SO VISUAL COMPUTER
LA English
DT Article
DE Recognition; Point cloud; Pipeline detection; Point processing;
   Differential geometry
ID RECONSTRUCTION; REGISTRATION; EXTRACTION; SEGMENTATION; MODELS
AB This study proposes a method for detecting and reconstructing pipelines from a 3D point cloud. First, the method extracts points on cylindrical objects using various properties computed with the principal curvatures. Next, the possible radii of the cylinders in the point cloud are estimated using a histogram constructed with the radii of the curvature at each point. Once the candidate radii are obtained, spheres are estimated using a RANdom SAmple Consensus-based algorithm, whose centroids are processed to find the orientation and centerline of each cylinder. The nearest cylindrical components which are detected are then analyzed to establish connectivity to determine how they are arranged in space. Depending on the type of connectivity, elbows and/or T-junctions are used to connect the cylindrical elements to form pipelines. The proposed method was tested with synthetic and scanned point clouds and demonstrated better performance than that of the existing methods.
C1 [Oh, Inyoung; Ko, Kwnag Hee] Gwangju Inst Sci & Technol, Sch Mech Engn, 1 Oryeongdong, Gwangju 500712, South Korea.
C3 Gwangju Institute of Science & Technology (GIST)
RP Ko, KH (corresponding author), Gwangju Inst Sci & Technol, Sch Mech Engn, 1 Oryeongdong, Gwangju 500712, South Korea.
EM khko@gist.ac.kr
OI Ko, Kwang Hee/0000-0001-7668-5796; Oh, Inyoung/0000-0003-3030-4788
FU Basic Science Research Program through the National Research Foundation
   of Korea (NRF) - Ministry of Science and ICT [2017R1A2B4012124]
FX This research was supported by Basic Science Research Program through
   the National Research Foundation of Korea (NRF) funded by the Ministry
   of Science and ICT (2017R1A2B4012124).
CR [Anonymous], 1996, Computer graphics: principles and practice
   [Anonymous], 2009, THESIS
   Becerik-Gerber B, 2011, AUTOMAT CONSTR, V20, P649, DOI 10.1016/j.autcon.2010.12.008
   Bernardini F, 1999, INT J COMPUT GEOM AP, V9, P327, DOI 10.1142/S0218195999000236
   Brujic D, 2011, INT J ADV MANUF TECH, V54, P691, DOI 10.1007/s00170-010-2947-1
   Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236
   Di Angelo L, 2015, COMPUT AIDED DESIGN, V62, P44, DOI 10.1016/j.cad.2014.09.006
   DOUROS I, 2002, SCANN 2002 P PAR MAY
   Ioannou D, 1999, IMAGE VISION COMPUT, V17, P15, DOI 10.1016/S0262-8856(98)00090-0
   Jolliffe L., 2002, Principal Component Analysis
   Khameneifar F., 2019, Comput.-Aided Des. Appl, V16, P140, DOI DOI 10.14733/CADAPS.2019.140-149
   Li G, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866178
   Li SW, 2018, PROC CVPR IEEE, P2887, DOI 10.1109/CVPR.2018.00305
   Liu YJ, 2013, IEEE T VIS COMPUT GR, V19, P1700, DOI 10.1109/TVCG.2013.74
   MA WY, 1995, COMPUT AIDED DESIGN, V27, P663, DOI 10.1016/0010-4485(94)00018-9
   Patil AK, 2017, AUTOMAT CONSTR, V75, P65, DOI 10.1016/j.autcon.2016.12.002
   Patrikalakis NM, 2009, Shape interrogation for computer aided design and manufacturing
   Pauly M, 2002, VIS 2002: IEEE VISUALIZATION 2002, PROCEEDINGS, P163, DOI 10.1109/VISUAL.2002.1183771
   Petitjean S, 2002, ACM COMPUT SURV, V34, P211, DOI 10.1145/508352.508354
   Piegl LA, 2005, VISUAL COMPUT, V21, P104, DOI 10.1007/s00371-004-0274-y
   Qiu RQ, 2014, LECT NOTES COMPUT SC, V8691, P17, DOI 10.1007/978-3-319-10578-9_2
   RABBANI T., 2006, The International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences, V36, P248
   Rabbani T, 2007, ISPRS J PHOTOGRAMM, V61, P355, DOI 10.1016/j.isprsjprs.2006.09.006
   Schnabel R, 2007, COMPUT GRAPH FORUM, V26, P214, DOI 10.1111/j.1467-8659.2007.01016.x
   Steder B, 2011, IEEE INT CONF ROBOT, P2601, DOI 10.1109/ICRA.2011.5980187
   Sun JY, 2014, J COMPUT DES ENG, V1, P202, DOI 10.7315/JCDE.2014.020
   Tagliasacchi A, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531377
   Tran TT, 2016, VISUAL COMPUT, V32, P1205, DOI 10.1007/s00371-015-1157-0
   Tran TT, 2015, COMPUT GRAPH-UK, V46, P345, DOI 10.1016/j.cag.2014.09.027
   Tsuji T, 2014, IEEE INT C INT ROBOT, P2447, DOI 10.1109/IROS.2014.6942895
   Varady T, 1997, COMPUT AIDED DESIGN, V29, P255, DOI 10.1016/S0010-4485(96)00054-1
   Yan DM, 2012, COMPUT AIDED DESIGN, V44, P1072, DOI 10.1016/j.cad.2012.04.005
   YoungHoon Jin, 2017, Journal of KIISE, V44, P392, DOI 10.5626/JOK.2017.44.4.392
   Yun DH, 2017, GRAPH MODELS, V90, P1, DOI 10.1016/j.gmod.2017.02.001
   Yun D, 2015, ADV ENG INFORM, V29, P930, DOI 10.1016/j.aei.2015.09.008
NR 35
TC 10
Z9 10
U1 7
U2 25
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2021
VL 37
IS 6
BP 1385
EP 1400
DI 10.1007/s00371-020-01872-y
EA JUN 2020
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SO2LW
UT WOS:000539783200001
DA 2024-07-18
ER

PT J
AU Devi, RB
   Chanu, YJ
   Singh, KM
AF Devi, Rajkumari Bidyalakshmi
   Chanu, Yambem Jina
   Singh, Khumanthem Manglem
TI Discriminative object tracking with subspace representation
SO VISUAL COMPUTER
LA English
DT Article
DE Sparse representation; Principal component analysis; Sparse
   discriminative classifier; Visual tracking
ID VISUAL TRACKING; ROBUST
AB Visual object tracking is a core research area in the field of pattern recognition and computer vision. It becomes one of the most significant tasks in computer vision application. But tracking of a visual object is not an easy task as it is always restricted by appearance change, illumination, occlusion and so on. Object tracking based on principal component analysis (PCA) is one of the most effective tracking methods as it can handle the different challenging problems of the tracking algorithm. But in this PCA-based tracking method, the background pixels are also included in the subspace representation of the target object, and so this method cannot overcome all the problems of tracking. In this work, a robust visual object tracking method is proposed by introducing sparse discriminative classifier (SDC) feature selection in PCA subspace representation. The SDC method is utilized to extract the target object from the template image target by removing the background pixels which is unnecessary for tracking task without much computational complexity. The PCA algorithm adequately represents a presentation model of the target object and account of occlusion with trivial template. Qualitative and quantitative analysis of different diverse videos shows that the newly proposed method outperforms the other existing state-of-the-art tracking algorithm.
C1 [Devi, Rajkumari Bidyalakshmi; Chanu, Yambem Jina; Singh, Khumanthem Manglem] NIT, Dept Comp Sci, Imphal, Manipur, India.
C3 National Institute of Technology (NIT System); National Institute of
   Technology Manipur
RP Devi, RB (corresponding author), NIT, Dept Comp Sci, Imphal, Manipur, India.
EM bidrk09mit@gmail.com; jina@nitmanipur.ac.in; manglem@nitmanipur.ac.in
RI Singh, Khumanthem/AFZ-2177-2022
CR [Anonymous], 2006, IEEE C COMPUTER VISI, DOI DOI 10.1109/CVPR.2006.256
   [Anonymous], 2006, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
   [Anonymous], 2012, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2012.6247881
   AVIDAN S, 2001, P 2001 IEEE COMP SOC
   Avidan S, 2007, IEEE T PATTERN ANAL, V29, P261, DOI 10.1109/TPAMI.2007.35
   Babenko B, 2009, PROC CVPR IEEE, P983, DOI 10.1109/CVPRW.2009.5206737
   Bertinetto L, 2016, PROC CVPR IEEE, P1401, DOI 10.1109/CVPR.2016.156
   Chen DB, 2017, OPTOELECTRON LETT, V13, P392, DOI 10.1007/s11801-017-7080-z
   Chen F, 2011, IMAGE VISION COMPUT, V29, P787, DOI 10.1016/j.imavis.2011.08.006
   Choi J, 2016, PROC CVPR IEEE, P4321, DOI 10.1109/CVPR.2016.468
   Comaniciu D, 2003, IEEE T PATTERN ANAL, V25, P564, DOI 10.1109/TPAMI.2003.1195991
   Danelljan M., 2014, BRIT MACH VIS C, P1, DOI DOI 10.5244/C.28.65
   Danelljan M, 2014, PROC CVPR IEEE, P1090, DOI 10.1109/CVPR.2014.143
   Dong Wang, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P1751, DOI 10.1109/ICPR.2010.433
   Fan H, 2019, PROC CVPR IEEE, P5369, DOI 10.1109/CVPR.2019.00552
   Fan H, 2017, IEEE I CONF COMP VIS, P5487, DOI 10.1109/ICCV.2017.585
   Grabner H, 2008, LECT NOTES COMPUT SC, V5302, P234, DOI 10.1007/978-3-540-88682-2_19
   Henriques JF, 2015, IEEE T PATTERN ANAL, V37, P583, DOI 10.1109/TPAMI.2014.2345390
   Henriques JF, 2012, LECT NOTES COMPUT SC, V7575, P702, DOI 10.1007/978-3-642-33765-9_50
   Hu WM, 2011, INT J COMPUT VISION, V91, P303, DOI 10.1007/s11263-010-0399-6
   Jia X, 2012, PROC CVPR IEEE, P1822, DOI 10.1109/CVPR.2012.6247880
   Kalal Z, 2010, PROC CVPR IEEE, P49, DOI 10.1109/CVPR.2010.5540231
   Kwon J, 2010, PROC CVPR IEEE, P1269, DOI 10.1109/CVPR.2010.5539821
   Liu BY, 2011, PROC CVPR IEEE, P1313, DOI 10.1109/CVPR.2011.5995730
   Liu BY, 2010, LECT NOTES COMPUT SC, V6314, P624
   Liu HC, 2015, IEEE T INSTRUM MEAS, V64, P2863, DOI 10.1109/TIM.2015.2437636
   Liwicki S, 2012, IEEE T NEUR NET LEAR, V23, P1624, DOI 10.1109/TNNLS.2012.2208654
   Lu WZ, 2013, INT CONF ACOUST SPEE, P2312, DOI 10.1109/ICASSP.2013.6638067
   Mei X, 2009, IEEE I CONF COMP VIS, P1436, DOI 10.1109/ICCV.2009.5459292
   Mueller M, 2017, PROC CVPR IEEE, P1387, DOI 10.1109/CVPR.2017.152
   Ross DA, 2008, INT J COMPUT VISION, V77, P125, DOI 10.1007/s11263-007-0075-7
   Tang F, 2007, IEEE I CONF COMP VIS, P992
   Wang D, 2013, IEEE T IMAGE PROCESS, V22, P314, DOI 10.1109/TIP.2012.2202677
   Yang GL, 2018, ARAB J SCI ENG, V43, P627, DOI 10.1007/s13369-017-2734-5
   YU G, 2009, ROBUST INCREMENTAL S, P819
   Zhang DJ, 2020, VISUAL COMPUT, V36, P509, DOI 10.1007/s00371-019-01634-5
   Zhang WC, 2021, VISUAL COMPUT, V37, P881, DOI 10.1007/s00371-020-01839-z
   Zhong W, 2014, IEEE T IMAGE PROCESS, V23, P2356, DOI 10.1109/TIP.2014.2313227
NR 38
TC 7
Z9 7
U1 1
U2 11
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2021
VL 37
IS 5
BP 1207
EP 1219
DI 10.1007/s00371-020-01862-0
EA JUN 2020
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RV2QF
UT WOS:000538216100002
DA 2024-07-18
ER

PT J
AU Ali, H
   Faisal, S
   Chen, K
   Rada, L
AF Ali, Haider
   Faisal, Shah
   Chen, Ke
   Rada, Lavdie
TI Image-selective segmentation model for multi-regions within the object
   of interest with application to medical disease
SO VISUAL COMPUTER
LA English
DT Article
DE Selective segmentation; Multi-region segmentation; Variational model;
   Active contours; Edge extraction
ID VARIATIONAL MODEL; ACTIVE CONTOURS; GEOMETRICAL CONDITIONS; CONVEX;
   EFFICIENT; ENERGIES
AB Detection and extraction of an object of interest and accurate boundaries segmentation in a given image has been of interest in the last decades due to its application in different fields. To successfully segment a single object, interactive/selective segmentation techniques has been developed as a supplement to the existing global segmentation techniques. Even though existing interactive/selective segmentation techniques perform well in segmenting the images with prominent edges, those methods are less efficient or even fail in segmenting images having multi-regions of different intensity scale. In this paper, we design a new variational selective segmentation model which incorporates the idea of area-based fitting term along with a signed pressure force function based on a generalized average into a variational energy function. The new model is capable to capture the object of interest which can be single or multi-region within the object of interest. To evaluate the performance of our new model, we compare our results with state of the art models by showing same efficiency and reliability on detecting single-region and an outperforming for multi-region selective segmentation. Comparison tests were carried out on synthetic and real data images.
C1 [Ali, Haider; Faisal, Shah] Univ Peshawar, Dept Math, Peshawar, Pakistan.
   [Chen, Ke] Univ Liverpool, Dept Math Sci, Liverpool, Merseyside, England.
   [Rada, Lavdie] Bahcesehir Univ, Biomed Engn Dept, Istanbul, Turkey.
C3 University of Peshawar; University of Liverpool; Bahcesehir University
RP Rada, L (corresponding author), Bahcesehir Univ, Biomed Engn Dept, Istanbul, Turkey.
EM dr.haider@uop.edu.pk; sfaisaluop@gmail.com; k.chen@liverpool.ac.uk;
   lavdie.rada@eng.bau.edu.tr
RI Chen, Ke/A-1965-2012; Ali, Haider/JBJ-0820-2023; Faisal,
   Shah/F-6795-2017
OI Chen, Ke/0000-0002-6093-6623
CR ADAMS R, 1994, IEEE T PATTERN ANAL, V16, P641, DOI 10.1109/34.295913
   Akram F, 2014, COMPUT MATH METHOD M, V2014, DOI 10.1155/2014/194614
   Ali H, 2017, TURK J ELECTR ENG CO, V25, P2943, DOI 10.3906/elk-1606-260
   Ali H, 2016, PATTERN RECOGN, V51, P27, DOI 10.1016/j.patcog.2015.08.022
   [Anonymous], 2013, Ann. BMVA, DOI [10.1155/2013/325903, DOI 10.1155/2013/325903]
   Aubert G., 2010, Mathematical Problems in Image Processing: Partial Differential Equations and the Calculus of Variations, V2nd
   Badrinarayanan V, 2015, ARXIV151100561 CORR
   Badshah N, 2012, E ASIAN J APPL MATH, V2, P150, DOI 10.4208/eajam.090312.190412a
   Badshah N, 2010, COMMUN COMPUT PHYS, V7, P759, DOI 10.4208/cicp.2009.09.026
   Bai XF, 2007, IEEE IC COMP COM NET, P1
   Brostow GJ, 2009, PATTERN RECOGN LETT, V30, P88, DOI 10.1016/j.patrec.2008.04.005
   Cai XH, 2013, SIAM J IMAGING SCI, V6, P368, DOI 10.1137/120867068
   Caselles V, 1997, INT J COMPUT VISION, V22, P61, DOI 10.1023/A:1007979827043
   Chan T.F., 1998, 9853 UCLA CAM, P98
   Chan TF, 2001, IEEE T IMAGE PROCESS, V10, P266, DOI 10.1109/83.902291
   Chen K, 2005, CAMBRIDGE MONOGRAPHS, DOI [10.1017/CBO9780511543258, DOI 10.1017/CB09780511543258]
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236
   Dong XP, 2016, IEEE T IMAGE PROCESS, V25, P516, DOI 10.1109/TIP.2015.2505184
   Dong XP, 2015, IEEE T IMAGE PROCESS, V24, P3966, DOI 10.1109/TIP.2015.2456636
   Falcao AX, 2000, IEEE T MED IMAGING, V19, P55, DOI 10.1109/42.832960
   GEMAN S, 1984, IEEE T PATTERN ANAL, V6, P721, DOI 10.1109/TPAMI.1984.4767596
   Goldstein T, 2010, J SCI COMPUT, V45, P272, DOI 10.1007/s10915-009-9331-z
   Gout C, 2005, NUMER ALGORITHMS, V39, P155, DOI 10.1007/s11075-004-3627-8
   Grady L, 2006, IEEE T PATTERN ANAL, V28, P1768, DOI 10.1109/TPAMI.2006.233
   KASS M, 1987, INT J COMPUT VISION, V1, P321, DOI 10.1007/BF00133570
   Le Guyader C, 2008, NUMER ALGORITHMS, V48, P105, DOI 10.1007/s11075-008-9174-y
   Li H., 2015, COMPUTATIONAL VISUAL, V1, P183
   Litjens G, 2017, MED IMAGE ANAL, V42, P60, DOI 10.1016/j.media.2017.07.005
   Liu C, 2018, PATTERN RECOGN, V76, P367, DOI 10.1016/j.patcog.2017.11.019
   Mahood L, 2016, PATTERN RECOGN, V55, P87, DOI 10.1016/j.patcog.2016.01.021
   Malik J, 2001, INT J COMPUT VISION, V43, P7, DOI 10.1023/A:1011174803800
   MUMFORD D, 1989, COMMUN PUR APPL MATH, V42, P577, DOI 10.1002/cpa.3160420503
   OSHER S, 1988, J COMPUT PHYS, V79, P12, DOI 10.1016/0021-9991(88)90002-2
   Osher S., 2005, 2003 LECT NOTES COMP, V3708
   Peng JT, 2016, IEEE T CYBERNETICS, V46, P1616, DOI 10.1109/TCYB.2015.2453091
   Rada L, 2013, J ALGORITHMS COMPUT, V7, P509, DOI 10.1260/1748-3018.7.4.509
   Roberts M, 2019, J MATH IMAGING VIS, V61, P1173, DOI 10.1007/s10851-019-00893-0
   Roberts M, 2019, J MATH IMAGING VIS, V61, P482, DOI 10.1007/s10851-018-0857-2
   Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720
   Sen D, 2009, IEEE T IMAGE PROCESS, V18, P879, DOI 10.1109/TIP.2009.2012890
   Shen JB, 2019, IEEE T NEUR NET LEAR, V30, P2637, DOI 10.1109/TNNLS.2018.2885591
   Shen JB, 2017, IEEE T IMAGE PROCESS, V26, P4911, DOI 10.1109/TIP.2017.2722691
   Shen JB, 2016, IEEE T IMAGE PROCESS, V25, P5933, DOI 10.1109/TIP.2016.2616302
   Shen JB, 2014, IEEE T CIRC SYST VID, V24, P1088, DOI 10.1109/TCSVT.2014.2302545
   Spencer J, 2015, COMMUN MATH SCI, V13, P1453, DOI 10.4310/CMS.2015.v13.n6.a5
   Nguyen TNA, 2012, IEEE T IMAGE PROCESS, V21, P3734, DOI 10.1109/TIP.2012.2191566
   VINCENT L, 1991, IEEE T PATTERN ANAL, V13, P583, DOI 10.1109/34.87344
   Wang WG, 2018, IEEE T PATTERN ANAL, V40, P20, DOI 10.1109/TPAMI.2017.2662005
   Wang WG, 2016, IEEE T MULTIMEDIA, V18, P1011, DOI 10.1109/TMM.2016.2545409
   Weickert J, 1998, IEEE T IMAGE PROCESS, V7, P398, DOI 10.1109/83.661190
   Yang WX, 2010, IEEE T IMAGE PROCESS, V19, P2470, DOI 10.1109/TIP.2010.2048611
   Zhang KH, 2010, IMAGE VISION COMPUT, V28, P668, DOI 10.1016/j.imavis.2009.10.009
   Zhang LM, 2014, IEEE T MULTIMEDIA, V16, P470, DOI 10.1109/TMM.2013.2293424
   Zucker S. W., 1976, Computer Graphics and Image Processing, V5, P382, DOI 10.1016/S0146-664X(76)80014-7
NR 55
TC 11
Z9 11
U1 2
U2 20
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2021
VL 37
IS 5
BP 939
EP 955
DI 10.1007/s00371-020-01845-1
EA APR 2020
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RV2QF
UT WOS:000528657100001
DA 2024-07-18
ER

PT J
AU Liu, XW
AF Liu, Xinwu
TI Total generalized variation and wavelet frame-based adaptive image
   restoration algorithm
SO VISUAL COMPUTER
LA English
DT Article
DE Image restoration; Total generalized variation; Wavelet frame;
   Alternating minimization method; Discrepancy principle
ID REGULARIZATION PARAMETER; RECONSTRUCTION; MODELS
AB To achieve superior image reconstruction, this paper investigates a hybrid regularizers model for image denoising and deblurring. This approach closely incorporates the advantages of the total generalized variation and wavelet frame-based methods. Computationally, a highly efficient alternating minimization algorithm containing no inner iterations is introduced in detail, which synchronously restores the degraded image and automatically estimates the regularization parameter based on Morozov's discrepancy principle. Illustrationally, we demonstrate that our proposed strategy significantly outperforms several current state-of-the-art numerical methods and closely matches the performance of human vision in solving the image deconvolution problem, with respect to restoration accuracy, staircase artifacts suppression and features preservation.
C1 [Liu, Xinwu] Hunan Univ Sci & Technol, Sch Math & Computat Sci, Xiangtan 411201, Hunan, Peoples R China.
C3 Hunan University of Science & Technology
RP Liu, XW (corresponding author), Hunan Univ Sci & Technol, Sch Math & Computat Sci, Xiangtan 411201, Hunan, Peoples R China.
EM lxinwu@163.com
OI Liu, Xinwu/0000-0003-1909-3721
FU National Natural Science Foundation of China [61402166]; Hunan
   Provincial Natural Science Foundation of China [14JJ3105]
FX This work was supported by National Natural Science Foundation of China
   (61402166) and Hunan Provincial Natural Science Foundation of China
   (14JJ3105).
CR Aujol JF, 2006, J MATH IMAGING VIS, V26, P217, DOI 10.1007/s10851-006-7801-6
   Babacan SD, 2008, IEEE T IMAGE PROCESS, V17, P326, DOI 10.1109/TIP.2007.916051
   Babacan SD, 2009, IEEE T IMAGE PROCESS, V18, P12, DOI 10.1109/TIP.2008.2007354
   Bertsekas Dimitri, 2015, PARALLEL DISTRIBUTED
   Boyd Stephen, 2010, Foundations and Trends in Machine Learning, V3, P1, DOI 10.1561/2200000016
   Boyd S., 2004, CONVEX OPTIMIZATION
   Bredies K, 2013, LECT NOTES COMPUT SC, P149, DOI DOI 10.1007/978-3-642-38267-3_13
   Bredies K., 2011, P SAMPTA 2011 9 INT
   Bredies K, 2013, INT J COMPUT MATH, V90, P109, DOI 10.1080/00207160.2012.700400
   Bredies K, 2010, SIAM J IMAGING SCI, V3, P492, DOI 10.1137/090769521
   Burger HC, 2012, PROC CVPR IEEE, P2392, DOI 10.1109/CVPR.2012.6247952
   Cai JF, 2012, J AM MATH SOC, V25, P1033, DOI 10.1090/S0894-0347-2012-00740-1
   Cai JF, 2009, MULTISCALE MODEL SIM, V8, P337, DOI 10.1137/090753504
   Chen DQ, 2011, J VIS COMMUN IMAGE R, V22, P643, DOI 10.1016/j.jvcir.2011.07.007
   Galatsanos NP, 1992, IEEE T IMAGE PROCESS, V1, P322, DOI 10.1109/83.148606
   Goldstein T, 2009, SIAM J IMAGING SCI, V2, P323, DOI 10.1137/080725891
   GOLUB GH, 1979, TECHNOMETRICS, V21, P215, DOI 10.1080/00401706.1979.10489751
   Guo WH, 2014, SIAM J IMAGING SCI, V7, P1309, DOI 10.1137/120904263
   Hajiaboli Mohammad Reza, 2010, IPSJ Transactions on Computer Vision and Applications, V2, P94, DOI 10.2197/ipsjtcva.2.94
   HANSEN PC, 1992, SIAM REV, V34, P561, DOI 10.1137/1034115
   He BS, 2002, MATH PROGRAM, V92, P103, DOI 10.1007/s101070100280
   He C, 2014, IEEE T IMAGE PROCESS, V23, DOI 10.1109/TIP.2014.2360133
   He C, 2014, MATH PROBL ENG, V2014, DOI 10.1155/2014/157893
   He L, 2017, VIS COMPUT
   Knoll F, 2011, MAGN RESON MED, V65, P480, DOI 10.1002/mrm.22595
   Liu XW, 2017, MULTIMED TOOLS APPL, V76, P12505, DOI 10.1007/s11042-016-3631-8
   Liu XW, 2016, COMPUT MATH APPL, V71, P1694, DOI 10.1016/j.camwa.2016.03.005
   Ng MK, 2010, SIAM J SCI COMPUT, V32, P2710, DOI 10.1137/090774823
   Ng MK, 1999, SIAM J SCI COMPUT, V21, P851, DOI 10.1137/S1064827598341384
   RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F
   Selesnick IW, 2004, APPL COMPUT HARMON A, V17, P211, DOI 10.1016/j.acha.2004.05.003
   Setzer S, 2011, INT J COMPUT VISION, V92, P265, DOI 10.1007/s11263-010-0357-3
   Valkonen T, 2013, SIAM J IMAGING SCI, V6, P487, DOI 10.1137/120867172
   Wang C, 2017, VIS COMPUT
   Wang Y, 2007, TR0710 CAAM
   Weiss P, 2009, SIAM J SCI COMPUT, V31, P2047, DOI 10.1137/070696143
   Wen YW, 2012, IEEE T IMAGE PROCESS, V21, P1770, DOI 10.1109/TIP.2011.2181401
   Zha ZY, 2018, VISUAL COMPUT, V34, P117, DOI 10.1007/s00371-016-1318-9
   Zhang K, 2017, IEEE T IMAGE PROCESS, V26, P3142, DOI 10.1109/TIP.2017.2662206
   Zhang L, 2011, IEEE T IMAGE PROCESS, V20, P2378, DOI 10.1109/TIP.2011.2109730
   Zhang YS, 2013, IEEE T IMAGE PROCESS, V22, P1373, DOI 10.1109/TIP.2012.2230010
NR 41
TC 20
Z9 20
U1 2
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2019
VL 35
IS 12
BP 1883
EP 1894
DI 10.1007/s00371-018-1581-z
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KI6XQ
UT WOS:000511494300015
DA 2024-07-18
ER

PT J
AU Lhuillier, A
   van Garderen, M
   Weiskopf, D
AF Lhuillier, Antoine
   van Garderen, Mereke
   Weiskopf, Daniel
TI Density-based label placement
SO VISUAL COMPUTER
LA English
DT Article
DE Automated label placement; Image-based information visualization; Kernel
   density estimation
AB We introduce a versatile density-based approach to label placement that aims to put labels in uncluttered areas of an underlying 2D visualization. Our novel, image-space algorithm constructs a density map by applying kernel density estimation to the input features, i.e., the locations of the points to be labeled. In order to find a suitable position for a label where it does not overlap any features or other labels, we move it following the gradient descent of this density map. This guides labels toward nearby areas of low feature density, resulting in a layout where labels are spread around feature-dense areas. The gradient descent trajectory can be used to draw curved leaders that connect the point features to their labels. Additionally, our approach supports prioritized label placement, user-defined label-to-label and label-to-feature margins, obstacle-constrained labeling, and arbitrarily shaped labels. The proposed method is conceptually simple and can easily be implemented using OpenCV and image-processing libraries.
C1 [Lhuillier, Antoine] Univ Stuttgart, Visualizat Res Ctr, Stuttgart, Germany.
   [Weiskopf, Daniel] Univ Stuttgart, Visualizat Res Ctr VISUS, Stuttgart, Germany.
   [van Garderen, Mereke] Univ Konstanz, Algorithm Grp, Constance, Germany.
C3 University of Stuttgart; University of Stuttgart; University of Konstanz
RP van Garderen, M (corresponding author), Univ Konstanz, Algorithm Grp, Constance, Germany.
EM Antoine.Lhuillier@visus.uni-stuttgart.de;
   mereke.van.garderen@uni-konstanz.de;
   Daniel.Weiskopf@visus.uni-stuttgart.de
FU European Research Council (ERC) under the European Union's Seventh
   Framework Programme (FP7/2007-2013) / ERC [319209, NEXUS 1492]
FX MvG was funded by the European Research Council (ERC) under the European
   Union's Seventh Framework Programme (FP7/2007-2013) / ERC Grant
   agreement No. 319209 (project NEXUS 1492).
CR Abramowitz M, 1972, Handbook of Mathematical Functions with Formulas, Graphs, and Mathematical Tables
   [Anonymous], 2014, PROCVCBM, DOI [10.2312/vcbm.20141192, DOI 10.2312/VCBM.20141192]
   Bekos MA, 2007, COMP GEOM-THEOR APPL, V36, P215, DOI 10.1016/j.comgeo.2006.05.003
   Bekos MA, 2011, LECT NOTES COMPUT SC, V6543, P111, DOI 10.1007/978-3-642-18381-2_9
   Benkert Marc., 2009, J GRAPH ALGORITHMS A, V13, P289
   Bradski G., 2008, LEARNING OPENCV COMP, DOI DOI 10.1109/MRA.2009.933612
   Christensen J, 1995, ACM T GRAPHIC, V14, P203, DOI 10.1145/212332.212334
   Dwyer T, 2006, LECT NOTES COMPUT SC, V3843, P153
   EPANECHN.VA, 1969, THEOR PROBAB APPL+, V14, P153, DOI 10.1137/1114019
   Ersoy O, 2011, IEEE T VIS COMPUT GR, V17, P2364, DOI 10.1109/TVCG.2011.233
   Fekete J.-D., 1999, P ACM CHI C HUMAN FA, P512
   Formann M., 1991, P 7 ANN S COMPUTATIO, P281, DOI [DOI 10.1145/109648.109680, https://doi.org/10.1145/109648.109680]
   FREEMAN H, 1988, INFORM SCIENCES, V45, P367, DOI 10.1016/0020-0255(88)90011-4
   Frigo M, 1998, INT CONF ACOUST SPEE, P1381, DOI 10.1109/ICASSP.1998.681704
   Gansner ER, 2009, LECT NOTES COMPUT SC, V5417, P206, DOI 10.1007/978-3-642-00219-9_20
   González R, 2016, PROC IEEE-PES
   Hurter C, 2012, COMPUT GRAPH FORUM, V31, P865, DOI 10.1111/j.1467-8659.2012.03079.x
   Iturriaga C, 2003, J ALGORITHMS, V47, P14, DOI 10.1016/S0196-6774(03)00004-X
   Jones MC, 1996, J AM STAT ASSOC, V91, P401, DOI 10.2307/2291420
   Kallenbach RL, 2012, DAIRY GRAZING GROWTH
   Kaufmann M, 2009, LECT NOTES COMPUT SC, V5760, P290, DOI 10.1007/978-3-642-03456-5_20
   Kindermann P, 2016, ALGORITHMICA, V76, P225, DOI 10.1007/s00453-015-0028-4
   Kouril D, 2019, IEEE T VIS COMPUT GR, V25, P977, DOI 10.1109/TVCG.2018.2864491
   Lampe OD, 2011, IEEE PAC VIS SYMP, P171, DOI 10.1109/PACIFICVIS.2011.5742387
   Lhuillier A, 2017, THESIS
   Lhuillier A, 2017, IEEE PAC VIS SYMP, P190, DOI 10.1109/PACIFICVIS.2017.8031594
   Löffler M, 2015, LECT NOTES COMPUT SC, V9079, P339, DOI 10.1007/978-3-319-18173-8_25
   Luboschik M, 2008, IEEE T VIS COMPUT GR, V14, P1237, DOI 10.1109/TVCG.2008.152
   Meng Y, 2015, IEEE PAC VIS SYMP, P207, DOI 10.1109/PACIFICVIS.2015.7156379
   MISUE K, 1995, J VISUAL LANG COMPUT, V6, P183, DOI 10.1006/jvlc.1995.1010
   Podlozhnyuk Victor., 2007, FFT BASED 2D CONVOLU
   SHEATHER SJ, 1991, J ROY STAT SOC B, V53, P683
   Silverman, 2018, DENSITY ESTIMATION S, DOI 10.1201/9781315140919
   van der Zwan M, 2016, IEEE T VIS COMPUT GR, V22, P2550, DOI 10.1109/TVCG.2016.2515611
   van Garderen M, 2018, PICTURES PASTVISUALI, P155
   van Liere R, 2003, IEEE T VIS COMPUT GR, V9, P206, DOI 10.1109/TVCG.2003.1196007
   Van Wijk J. J., 2002, P VISSYM
   Wagner F, 1998, LECT NOTES COMPUT SC, V1547, P316
   Wolff A., 2009, MAP LABELING BIBLIO
   Zinsmaier M, 2012, IEEE T VIS COMPUT GR, V18, P2486, DOI 10.1109/TVCG.2012.238
   Zoraster S., 1986, Cartographica: The International Journal for Geographic Information and Geovisualization, V23, P16, DOI [https://doi.org/10.3138/9258-63QL-3988-110H, DOI 10.3138/9258-63QL-3988-110H]
NR 41
TC 8
Z9 10
U1 2
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2019
VL 35
IS 6-8
SI SI
BP 1041
EP 1052
DI 10.1007/s00371-019-01686-7
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IC1IH
UT WOS:000470712200022
OA Bronze
DA 2024-07-18
ER

PT J
AU Perrot, R
   Aveneau, L
   Mora, F
   Meneveaux, D
AF Perrot, Romuald
   Aveneau, Lilian
   Mora, Frederic
   Meneveaux, Daniel
TI Photon mapping with visible kernel domains
SO VISUAL COMPUTER
LA English
DT Article
DE Lighting simulation; Photon mapping; Density estimation; Visibility
AB Despite the strong efforts made in the last three decades, lighting simulation systems still remain prone to various types of imprecisions. This paper specifically tackles the problem of biases due to density estimation used in photon mapping approaches. We study the fundamental aspects of density estimation and exhibit the need for handling visibility in the early stage of the kernel domain definition. We show that properly managing visibility in the density estimation process allows to reduce or to remove biases all at once. In practice, we have implemented a 3D product kernel based on a polyhedral domain, with both point-to-point and point-to-surface visibility computation. Our experimental results illustrate the enhancements produced at every stage of density estimation, for direct photon maps visualization and progressive photon mapping.
C1 [Perrot, Romuald; Aveneau, Lilian; Meneveaux, Daniel] Univ Poitiers, UMR CNRS 7252, XLIM ASALI, Poitiers, France.
   [Mora, Frederic] Univ Limoges, UMR CNRS 7252, XLIM ASALI, Limoges, France.
C3 Universite de Poitiers; Centre National de la Recherche Scientifique
   (CNRS); Universite de Limoges
RP Meneveaux, D (corresponding author), Univ Poitiers, UMR CNRS 7252, XLIM ASALI, Poitiers, France.
EM daniel.meneveaux@univ-poitiers.fr
OI Aveneau, Lilian/0000-0003-3129-1149; Meneveaux,
   Daniel/0000-0001-7160-3026
CR [Anonymous], 1997, ROBUST MONTE CARLO M
   [Anonymous], 1998, DENSITY ESTIMATION S, DOI DOI 10.1201/9781315140919
   [Anonymous], 1995, MONOGRAPHS STAT APPL
   [Anonymous], 2006, Advanced Global Illumination
   BEKAERT P., 2003, A custom designed density estimator for light transport
   Chen LH, 2010, VISUAL COMPUT, V26, P217, DOI 10.1007/s00371-009-0397-2
   Dammertz H, 2008, COMPUT GRAPH FORUM, V27, P1225, DOI 10.1111/j.1467-8659.2008.01261.x
   Georgiev I, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366211
   Hachisuka T, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366210
   Hachisuka T, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409083
   Haumont Denis., 2005, Proceedings of the Sixteenth Eurographics Conference on Rendering Techniques, P211
   HAVRAN V., 2005, Eurographics Symposium on Rendering, P43
   Herzog R, 2005, THESIS
   Izenman Alan J., 2008, MODERN MULTIVARIATE
   Jensen H. W., 1996, Rendering Techniques '96. Proceedings of the Eurographics Workshop. Eurographics, P21
   Jensen HW., 2001, REALISTIC IMAGE SYNT, DOI [10.1201/9780429294907, DOI 10.1201/9780429294907]
   JONES MC, 1993, STAT COMPUT, V3, P135, DOI 10.1007/BF00147776
   Kajiya J.T., 1986, ACM SIGGRAPH
   Knaus C, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1966394.1966404
   Lastra M., 2002, EUR WORKSH REND TECH
   Lavignotte F, 2002, WSCG'2002, VOLS I AND II, CONFERENCE PROCEEDINGS, P263
   Lavignotte F., 2003, GRAPHITE 03
   Mora F, 2012, COMPUT GRAPH FORUM, V31, P132, DOI 10.1111/j.1467-8659.2011.02089.x
   Nirenstein S., 2002, EUR WORKSH REND TECH
   Pharr M., 2010, PHYS BASED RENDERING
   QIN H, 2015, PROC SIGGRAPH, V34, P208, DOI DOI 10.1145/2816795.2818119
   Schregle R, 2003, COMPUT GRAPH FORUM, V22, P729, DOI 10.1111/j.1467-8659.2003.00720.x
   Teller S., 1993, ACM SIGGRAPH
   Tobler RF, 2006, WSCG 2006: FULL PAPERS PROCEEDINGS, P257
   Wald I, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601199
NR 30
TC 0
Z9 0
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2019
VL 35
IS 5
BP 707
EP 720
DI 10.1007/s00371-018-1505-y
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HZ0IT
UT WOS:000468524900008
DA 2024-07-18
ER

PT J
AU Wang, JD
   Dai, JJ
   Li, S
   Wang, J
   Wei, MQ
   Pang, MY
AF Wang, Jidong
   Dai, Jiajia
   Li, Kin-Sum
   Wang, Jun
   Wei, Mingqiang
   Pang, Mingyong
TI Cost-effective printing of 3D objects with self-supporting property
SO VISUAL COMPUTER
LA English
DT Article
DE 3D printing; Orientation optimization; Self-supporting property; Support
   structure; Material consumption
ID STRUCTURE GENERATION; PART ORIENTATION; DEPOSITION; ALGORITHM;
   SELECTION; BUILD
AB The fused deposition modeling (FDM) printer is a simple, affordable and widely used device in the 3D printing society. However, the high price of printing materials is one of major restrictive factors for its further application. Based on the self-supporting property of printing materials, we present an optimization method to reduce the total material consumption of 3D printed objects themselves and their support structures for FDM printers in this paper. We first develop an orientation optimization scheme to reduce the outer support volume of a printed model. The volume is evaluated according to the depths of 3D model fragments obtained by the depth peeling technique in an optimization process. We then build a self-supporting frame with a set of scale-adaptive parallelepiped grids to replace the solid interior of the printed model for further reducing the material consumption. In our orientation optimization scheme, the overhanging area detecting function can detect the self-supporting regions of a 3D model in terms of the depths stored in the graphical processing unit memory. The self-supporting frame with grid structures inside printed models does not need to add additional support structures during the printing process. Experimental results indicate that our method is faster and consumes less printing materials than the state-of-the-art algorithms.
C1 [Wang, Jidong; Dai, Jiajia; Pang, Mingyong] Nanjing Normal Univ, Dept Educ Technol, 122 Ninghai Rd, Nanjing, Jiangsu, Peoples R China.
   [Li, Kin-Sum] Hong Kong Baptist Univ, Dept Hist, Hong Kong, Peoples R China.
   [Wang, Jun] Nanjing Univ Aeronaut & Astronaut, Coll Mech & Elect Engn, Nanjing, Jiangsu, Peoples R China.
   [Wei, Mingqiang] Nanjing Univ Aeronaut & Astronaut, Coll Comp Sci & Technol, Nanjing, Jiangsu, Peoples R China.
C3 Nanjing Normal University; Hong Kong Baptist University; Nanjing
   University of Aeronautics & Astronautics; Nanjing University of
   Aeronautics & Astronautics
RP Pang, MY (corresponding author), Nanjing Normal Univ, Dept Educ Technol, 122 Ninghai Rd, Nanjing, Jiangsu, Peoples R China.
EM panion@netease.com
RI wang, dan/JEF-0836-2023; Wang, Jun/AAM-6868-2021
OI Wang, Jun/0000-0001-9223-2615; Wei, Mingqiang/0000-0003-0429-490X;
   Jiajia, Dai/0000-0001-8445-7566
FU National Natural Science Foundation of China [61502137]; China
   Postdoctoral Science Foundation [2016M592047]; Research Grants Council
   of the Hong Kong Special Administrative Region, China [HKBU 12625716,
   12604017]; Chiang Ching-kuo Foundation for International Scholarly
   Exchange [RG025-P-15]; Faculty Research Grant Category II [HKBU]
   [FRG2/15-16/045]; Science Foundation of Chuzhou University [2014GH03]
FX This work was supported by National Natural Science Foundation of China
   [Grant No.61502137], China Postdoctoral Science Foundation [Grant
   No.2016M592047], the Research Grants Council of the Hong Kong Special
   Administrative Region, China [Project No. HKBU 12625716 and 12604017],
   the Chiang Ching-kuo Foundation for International Scholarly Exchange
   [No. RG025-P-15], the Faculty Research Grant Category II [HKBU No.
   FRG2/15-16/045] and Science Foundation of Chuzhou University [Grant
   No.2014GH03].
CR Alexander P, 1998, COMPUT AIDED DESIGN, V30, P343, DOI 10.1016/S0010-4485(97)00083-3
   [Anonymous], 2001, TECHNICAL REPORT
   [Anonymous], INT J ADV MANUFACTUR
   Bermano AH, 2017, COMPUT GRAPH FORUM, V36, P509, DOI 10.1111/cgf.13146
   Dumas J, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601153
   Ezair B, 2015, COMPUT GRAPH-UK, V51, P117, DOI 10.1016/j.cag.2015.05.009
   FRANK D, 1995, J INTELL MANUF, V6, P339, DOI 10.1007/BF00124677
   Gao W, 2015, COMPUT AIDED DESIGN, V69, P65, DOI 10.1016/j.cad.2015.04.001
   Guo X, 2017, COMPUT METHOD APPL M, V323, P27, DOI 10.1016/j.cma.2017.05.003
   Hornus S, 2015, J I TELEV ENG JPN, V28, P1017
   Hornus S., 2017, RR9083 INR NANC GRAN, P14
   Hu KL, 2015, COMPUT AIDED DESIGN, V65, P1, DOI 10.1016/j.cad.2015.03.001
   Huang G, 2015, NEURAL NETWORKS, V61, P32, DOI 10.1016/j.neunet.2014.10.001
   Huang P., 2014, ADV COMPUT INF ENG R, V1, P377
   Huang XM, 2009, INT J ADV MANUF TECH, V42, P1074, DOI 10.1007/s00170-008-1675-2
   Huybrechts S, 1996, COMPOS SCI TECHNOL, V56, P1001, DOI 10.1016/0266-3538(96)00063-2
   KHARDEKAR R., 2006, ASME 2006 INT DES EN, P993
   Lan PT, 1997, COMPUT AIDED DESIGN, V29, P53, DOI 10.1016/S0010-4485(96)00049-8
   Lee J, 2017, INT J ADV MANUF TECH, V89, P2151, DOI 10.1007/s00170-016-9239-3
   Lee M., 2017, SUPPORT FREE HOLLOWI
   Li D, 2015, INT J ADV MANUF TECH, V83, P1
   Li G, 2007, J COMPOS MATER, V41, P2939, DOI 10.1177/0021998307082180
   Livesu M, 2017, COMPUT GRAPH FORUM, V36, P537, DOI 10.1111/cgf.13147
   LU L, 2014, ACM T GRAPHIC, V33, P97
   Lucidi S, 2002, COMPUT OPTIM APPL, V21, P119, DOI 10.1023/A:1013735414984
   MAJHI J, 1939, COMP GEOM-THEOR APPL, V12, P219
   Sá AME, 2015, VISUAL COMPUT, V31, P799, DOI 10.1007/s00371-015-1109-8
   Sá AME, 2014, VISUAL COMPUT, V30, P1321, DOI 10.1007/s00371-013-0883-4
   Mirzendehdel AM, 2016, COMPUT AIDED DESIGN, V81, P1, DOI 10.1016/j.cad.2016.08.006
   Phatak AM, 2012, J MANUF SYST, V31, P395, DOI 10.1016/j.jmsy.2012.07.001
   Stava O, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185544
   Strano G, 2013, INT J ADV MANUF TECH, V66, P1247, DOI 10.1007/s00170-012-4403-x
   Vanek J, 2014, COMPUT GRAPH FORUM, V33, P117, DOI 10.1111/cgf.12437
   Vasiliev VV, 2001, COMPOS STRUCT, V54, P361, DOI 10.1016/S0263-8223(01)00111-8
   Wang W., 2017, IEEE T VIS COMPUT GR
   Wang WM, 2017, COMPUT GRAPH-UK, V66, P154, DOI 10.1016/j.cag.2017.05.022
   Wang WM, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508382
   Wu J, 2016, COMPUT AIDED DESIGN, V80, P32, DOI 10.1016/j.cad.2016.07.006
   Wu J, 2016, IEEE T VIS COMPUT GR, V22, P1195, DOI 10.1109/TVCG.2015.2502588
   Xie Y, 2017, VIS INFORM, V1, P9, DOI 10.1016/j.visinf.2017.01.002
   Xu F, 1999, RAPID PROTOTYPING J, V5, P54, DOI 10.1108/13552549910267344
   Yang Y, 2003, J MANUF SYST, V22, P116, DOI 10.1016/S0278-6125(03)90009-4
   Yang Y, 2017, COMPUT GRAPH
   Zhang XL, 2015, COMPUT AIDED GEOM D, V35-36, P149, DOI 10.1016/j.cagd.2015.03.012
   Zhang XX, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766982
   Zhang YC, 2016, RAPID PROTOTYPING J, V22, P358, DOI 10.1108/RPJ-03-2014-0037
NR 46
TC 8
Z9 10
U1 6
U2 39
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2019
VL 35
IS 5
BP 639
EP 651
DI 10.1007/s00371-018-1493-y
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HZ0IT
UT WOS:000468524900003
DA 2024-07-18
ER

PT J
AU Giraldo-Zuluaga, JH
   Salazar, A
   Gomez, A
   Diaz-Pulido, A
AF Giraldo-Zuluaga, Jhony-Heriberto
   Salazar, Augusto
   Gomez, Alexander
   Diaz-Pulido, Angelica
TI Camera-trap images segmentation using multi-layer robust principal
   component analysis
SO VISUAL COMPUTER
LA English
DT Article
DE Camera-trap images; Multi-layer robust principal component analysis;
   Background subtraction; Image segmentation
AB The segmentation of animals from camera-trap images is a difficult task. To illustrate, there are various challenges due to environmental conditions and hardware limitation in these images. We proposed a multi-layer robust principal component analysis (multi-layer RPCA) approach for background subtraction. Our method computes sparse and low-rank images from a weighted sum of descriptors, using color and texture features as case of study for camera-trap images segmentation. The segmentation algorithm is composed of histogram equalization or Gaussian filtering as pre-processing, and morphological filters with active contour as post-processing. The parameters of our multi-layer RPCA were optimized with an exhaustive search. The database consists of camera-trap images from the Colombian forest taken by the Instituto de Investigacion de Recursos Biologicos Alexander von Humboldt. We analyzed the performance of our method in inherent and therefore challenging situations of camera-trap images. Furthermore, we compared our method with some state-of-the-art algorithms of background subtraction, where our multi-layer RPCA outperformed these other methods. Our multi-layer RPCA reached 76.17 and 69.97% of average fine-grained F-measure for color and infrared sequences, respectively. To our best knowledge, this paper is the first work proposing multi-layer RPCA and using it for camera-trap images segmentation.
C1 [Giraldo-Zuluaga, Jhony-Heriberto; Salazar, Augusto; Gomez, Alexander] Univ Antioquia, Fac Ingn, Grp Invest SISTEMIC, Medellin, Colombia.
   [Diaz-Pulido, Angelica] Inst Invest Recursos Biol Alexander von Humboldt, Bogota, DC, Colombia.
C3 Universidad de Antioquia; Alliance; International Center for Tropical
   Agriculture - CIAT
RP Giraldo-Zuluaga, JH (corresponding author), Univ Antioquia, Fac Ingn, Grp Invest SISTEMIC, Medellin, Colombia.
EM jhonygiraldoz@gmail.com
RI Gomez Villa, Alex/GOE-4941-2022; Giraldo, Jhony H./O-7502-2019; Salazar,
   Augusto/KON-9530-2024
OI Gomez Villa, Alex/0000-0003-0469-3425; Giraldo, Jhony
   H./0000-0002-0039-1270; 
FU Colombian National Fund for Science, Technology and Innovation,
   Francisco Jose de Caldas - COLCIENCIAS (Colombia) [111571451061]
FX This work was supported by the Colombian National Fund for Science,
   Technology and Innovation, Francisco Jose de Caldas - COLCIENCIAS
   (Colombia). Project No. 111571451061.
CR [Anonymous], ARXIV160305875
   [Anonymous], 9 WORKSH VIS COMP WV
   [Anonymous], 2017, COMMUN ACM, DOI DOI 10.1145/3065386
   [Anonymous], 2015, PROC CVPR IEEE
   [Anonymous], COMPUT SCI REV
   [Anonymous], 2012, P IEEE C COMP VIS PA
   [Anonymous], 2011, ARXIV11052126
   [Anonymous], 2009 3 IEEE INT
   [Anonymous], 2014, P IEEE C COMP VIS PA
   [Anonymous], MULTILAYER ROBUST PR
   Bouwmans T, 2014, COMPUT SCI REV, V11-12, P31, DOI 10.1016/j.cosrev.2014.04.001
   Candès EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395
   Caselles V, 1997, INT J COMPUT VISION, V22, P61, DOI 10.1023/A:1007979827043
   Diaz-Pulido Angélica, 2011, Mastozool. neotrop., V18, P63
   Fegraus EH, 2011, ECOL INFORM, V6, P345, DOI 10.1016/j.ecoinf.2011.06.003
   Goldfarb D, 2013, MATH PROGRAM, V141, P349, DOI 10.1007/s10107-012-0530-2
   He J, 2013, IEEE INT CONF AUTOMA
   Heikkilä M, 2006, IEEE T PATTERN ANAL, V28, P657, DOI 10.1109/TPAMI.2006.68
   Javed S, 2016, INT C PATT RECOG, P120, DOI 10.1109/ICPR.2016.7899619
   Javed S, 2015, J ELECTRON IMAGING, V24, DOI 10.1117/1.JEI.24.4.043011
   Javed S, 2015, LECT NOTES COMPUT SC, V9280, P340, DOI 10.1007/978-3-319-23234-8_32
   Lin, 2010, ARXIV10095055
   Maddalena L, 2010, NEURAL COMPUT APPL, V19, P179, DOI 10.1007/s00521-009-0285-8
   Mahadevan V, 2010, IEEE T PATTERN ANAL, V32, P171, DOI 10.1109/TPAMI.2009.112
   O'Connell A, 2011, CAMERA TRAPS IN ANIMAL ECOLOGY: METHODS AND ANALYSES, pV
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Reddy K.P. K., 2012, Communications (NCC), 2012 National Conference on, P1
   Ren XB, 2013, PROC CVPR IEEE, P1947, DOI 10.1109/CVPR.2013.254
   Rodríguez P, 2016, IEEE GLOB CONF SIG, P197, DOI 10.1109/GlobalSIP.2016.7905831
   Rodríguez P, 2015, IEEE IMAGE PROC, P537, DOI 10.1109/ICIP.2015.7350856
   Sobral A., 2015, Robust Low-Rank and Sparse Matrix Decomposition: Applications in Image and Video Processing
   Sobral A, 2015, 2015 12TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE (AVSS)
   Sobral A, 2014, COMPUT VIS IMAGE UND, V122, P4, DOI 10.1016/j.cviu.2013.12.005
   Stauffer C., 1999, Proceedings. 1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No PR00149), P246, DOI 10.1109/CVPR.1999.784637
   Swanson A, 2015, SCI DATA, V2, DOI 10.1038/sdata.2015.26
   Toyama K., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P255, DOI 10.1109/ICCV.1999.791228
   Vishwakarma S, 2013, VISUAL COMPUT, V29, P983, DOI 10.1007/s00371-012-0752-6
   Wang Y., 2014, P IEEE C COMP VIS PA, P387, DOI 10.1109/ICIP40778.2020.9190887
   Yao J., 2007, 2007 IEEE C COMPUTER, P1
   Ye XC, 2015, IEEE T CIRC SYST VID, V25, P1721, DOI 10.1109/TCSVT.2015.2392491
   Zhang Z, 2016, IEEE T MULTIMEDIA, V18, P2079, DOI 10.1109/TMM.2016.2594138
   Zhang Z, 2015, IEEE IMAGE PROC, P2830, DOI 10.1109/ICIP.2015.7351319
   Zivkovic Z, 2004, INT C PATT RECOG, P28, DOI 10.1109/ICPR.2004.1333992
NR 43
TC 12
Z9 12
U1 1
U2 8
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2019
VL 35
IS 3
BP 335
EP 347
DI 10.1007/s00371-017-1463-9
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HP0MR
UT WOS:000461360600004
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Rechy-Ramirez, EJ
   Marin-Hernandez, A
   Rios-Figueroa, HV
AF Janet Rechy-Ramirez, Ericka
   Marin-Hernandez, Antonio
   Vladimir Rios-Figueroa, Homero
TI A human-computer interface for wrist rehabilitation: a pilot study using
   commercial sensors to detect wrist movements
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 6th International Conference on Virtual Reality and Visualization
   (ICVRV)
CY SEP 24-26, 2016
CL Hangzhou, PEOPLES R CHINA
SP China Soc Image & Graph, China Comp Federat, China Syst Simulat Assoc, IEEE Comp Soc, Connected Universal Experiences Labs Inc, China Soc Image & Graph, VR Comm, China Comp Federat, VR & Visualizat Comm, China Syst Simulat Assoc, VR Comm, China Syst Simulat Assoc, Digital Entertainment Comm, China Syst Simulat Assoc, Surgery Simulat Comm
DE Human-computer interface; Leap motion controller; Myo armband; Serious
   game; Wrist movement
ID VIRTUAL-REALITY; HAND; THERAPY; KINECT; SYSTEM
AB Health conditions might cause muscle weakness and immobility in some body parts; hence, physiotherapy exercises play a key role in the rehabilitation. To improve the engagement during the rehabilitation process, we therefore propose a human-computer interface (serious game) in which five wrist movements (extension, flexion, pronation, supination and neutral) are detected via two commercial sensors (Leap motion controller and Myo armband). Leap motion provides data regarding positions of user's finger phalanges through two infrared cameras, while Myo armband facilitates electromyography signal and inertial motion of user's arm through its electrodes and inertial measurement unit. The main aim of this study is to explore the performance of these sensors on wrist movement recognition in terms of accuracy, sensitivity and specificity. Eight healthy participants played 5 times a proposed game with each sensor in one session. Both sensors reported over 85% average recognition accuracy in the five wrist movements. Based on t test and Wilcoxon signed-rank test, early results show that there were significant differences between Leap motion controller and Myo armband recognitions in terms of average sensitivities on extension (p=0.0356), flexion (p=0.0356) and pronation (p=0.0440) movements, and average specificities on extension (p=0.0276) and pronation (p=0.0249) movements.
C1 [Janet Rechy-Ramirez, Ericka; Marin-Hernandez, Antonio; Vladimir Rios-Figueroa, Homero] Univ Veracruzana, Res Ctr Artificial Intelligence, Sebastian Camacho 5, Xalapa 91000, Veracruz, Mexico.
C3 Universidad Veracruzana
RP Rechy-Ramirez, EJ (corresponding author), Univ Veracruzana, Res Ctr Artificial Intelligence, Sebastian Camacho 5, Xalapa 91000, Veracruz, Mexico.
EM erechy@uv.mx; anmarin@uv.mx; hrios@uv.mx
RI Rios-Figueroa, Homero/L-7628-2013; Rechy-Ramírez, Ericka
   J./AAK-4885-2021; Marin-Hernandez, Antonio/G-7398-2012
OI Rios-Figueroa, Homero/0000-0001-8673-8863; Rechy-Ramírez, Ericka
   J./0000-0002-8401-1174; Marin-Hernandez, Antonio/0000-0002-7697-9118
CR Aristidou A, 2018, VISUAL COMPUT, V34, P213, DOI 10.1007/s00371-016-1327-8
   Asadipour A, 2017, VISUAL COMPUT, V33, P401, DOI 10.1007/s00371-016-1275-3
   Bassily D., 2014, ISR ROBOTIK 2014, P1
   Bizzotto N, 2014, SURG INNOV, V21, P655, DOI 10.1177/1553350614528384
   Boian R, 2002, STUD HEALTH TECHNOL, V85, P64
   Burke JW, 2009, PROCEEDINGS OF THE IEEE VIRTUAL WORLDS FOR SERIOUS APPLICATIONS, P103, DOI 10.1109/VS-GAMES.2009.17
   Burke JW, 2009, VISUAL COMPUT, V25, P1085, DOI 10.1007/s00371-009-0387-4
   Chang YJ, 2013, RES DEV DISABIL, V34, P3654, DOI 10.1016/j.ridd.2013.08.021
   Charles D, 2014, J ASSIST TECHNOL, V8, P138, DOI 10.1108/JAT-02-2014-0007
   Chuan CH, 2014, 2014 13TH INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND APPLICATIONS (ICMLA), P541, DOI 10.1109/ICMLA.2014.110
   Da Gama A, 2015, GAMES HEALTH J, V4, P123, DOI 10.1089/g4h.2014.0047
   Alves SFD, 2014, P IEEE RAS-EMBS INT, P970, DOI 10.1109/BIOROB.2014.6913906
   Funasaka M., 2015, Proceedings of the International Conference on Parallel and Distributed Processing Techniques and Applications (PDPTA), P263
   Granic I, 2014, AM PSYCHOL, V69, P66, DOI 10.1037/a0034857
   Grubisic I, 2015, PERIOD BIOL, V117, P139
   Gunasekera W.S. L., 2005, SPR SP SURG SER, P407
   Hettig J., 2015, Eurographics Workshop on Visual Computing for Biology and Medicine
   Khademi M., 2014, Conference on Human Factors in Computing Systems - Proceedings, (February 2015), P1663, DOI [10.1145/2559206.2581203, DOI 10.1145/2559206.2581203]
   Liang H, 2017, VISUAL COMPUT, V33, P517, DOI 10.1007/s00371-016-1272-6
   Ma M, 2008, IEEE SYS MAN CYBERN, P1871
   Marin G, 2014, IEEE IMAGE PROC, P1565, DOI 10.1109/ICIP.2014.7025313
   McCullough Morgan., 2015, Proc. ACM Symp. on Applied Perception, P107
   Mewes A, 2016, INT J COMPUT ASS RAD, V11, P157, DOI 10.1007/s11548-015-1215-0
   Norkin C C., 2017, Measurement of Joint Motion: A Guide to Goniometry, V5th
   O'Brien AV, 2006, RHEUMATOLOGY, V45, P577, DOI 10.1093/rheumatology/kei215
   Pedersoli F, 2014, VISUAL COMPUT, V30, P1107, DOI 10.1007/s00371-014-0921-x
   Qamar A, 2014, P INT CONF INTELL, P215, DOI 10.1109/ISMS.2014.43
   Qamar AM, 2015, ICMR'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P671, DOI 10.1145/2671188.2749412
   Rego P. A., 2010, 5 IBERIAN C INFORM S, P1
   Rondinelli R.D., 2008, GUIDES EVALUATION PE
   Roy Anil K., 2013, 2013 IEEE Global Humanitarian Technology Conference: South Asia Satellite (GHTC-SAS), P298, DOI 10.1109/GHTC-SAS.2013.6629934
   Russoniello CV, 2009, STUD HEALTH TECHNOL, V144, P189, DOI 10.3233/978-1-60750-017-9-189
   Shen JC, 2016, VISUAL COMPUT, V32, P359, DOI 10.1007/s00371-016-1209-0
   Sonntag D., 2015, PERDIS 2015 P 4 ACM, P269, DOI [10.1145/2757710.2776816, DOI 10.1145/2757710.2776816]
   Wakefield AE, 2000, J BONE JOINT SURG BR, V82B, P972, DOI 10.1302/0301-620X.82B7.10377
   Yu NB, 2015, IEEE ANN INT CONF CY, P1923, DOI 10.1109/CYBER.2015.7288241
   Zyda M, 2005, COMPUTER, V38, P25, DOI 10.1109/MC.2005.297
   [No title captured]
NR 38
TC 11
Z9 12
U1 1
U2 15
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2019
VL 35
IS 1
BP 41
EP 55
DI 10.1007/s00371-017-1446-x
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA HL6ZO
UT WOS:000458885900005
DA 2024-07-18
ER

PT J
AU Joshi, P
   Prakash, S
   Rawat, S
AF Joshi, Piyush
   Prakash, Surya
   Rawat, Sonika
TI Continuous wavelet transform-based no-reference quality assessment of
   deblocked images
SO VISUAL COMPUTER
LA English
DT Article
DE Image quality assessment; No-reference image quality assessment;
   Deblocked images; Continuous wavelet transform (CWT)
ID BLOCKING ARTIFACTS; DCT; BLUR; STRENGTH
AB An image when passed through a compression process either gains noise or loses some information, resulting in a degraded image. JPEG is considered to be one of the most commonly used compression standards whose resulting images are found to be subjected to blocking artifacts at low bit rates. There exist a few deblocking algorithms which have been proposed in the literature to reduce the blocking artifacts in compressed images. However, unfortunately these deblocking techniques introduce blur distortion in the images and hence the deblocked images may contain multiple distortions. Existing image quality metrics have limitations in evaluating the quality of deblocked images as they are not designed for multiply distorted images. To overcome this issue, we propose a no-reference quality assessment technique for deblocked images using continuous wavelet transform. We evaluate the proposed technique on DBID database which consists of general deblocked images. Experimental results show that the proposed quality assessment technique outperforms the existing image quality assessment techniques for deblocked images.
C1 [Joshi, Piyush; Prakash, Surya] Indian Inst Technol Indore, Discipline Comp Sci & Engn, Indore 453552, Madhya Pradesh, India.
   [Rawat, Sonika] Amity Univ Rajasthan, Jaipur 303007, Rajasthan, India.
C3 Indian Institute of Technology System (IIT System); Indian Institute of
   Technology (IIT) - Indore
RP Joshi, P (corresponding author), Indian Inst Technol Indore, Discipline Comp Sci & Engn, Indore 453552, Madhya Pradesh, India.
EM phd1301101004@iiti.ac.in; surya@iiti.ac.in; sonika.rawat1993@gmail.com
RI Prakash, Surya/S-6308-2019
OI Prakash, Surya/0000-0001-8039-1280
CR Bahrami K, 2014, IEEE SIGNAL PROC LET, V21, P751, DOI 10.1109/LSP.2014.2314487
   Bovik AC, 2001, INT CONF ACOUST SPEE, P1725, DOI 10.1109/ICASSP.2001.941272
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851
   Chen CH, 2010, LECT NOTES COMPUT SC, V6297, P112, DOI 10.1007/978-3-642-15702-8_11
   Chen MJ, 2011, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2011-3
   Ferzli R, 2009, IEEE T IMAGE PROCESS, V18, P717, DOI 10.1109/TIP.2008.2011760
   Foi A, 2007, IEEE T IMAGE PROCESS, V16, P1395, DOI 10.1109/TIP.2007.891788
   Golestaneh S. Alireza, 2014, IEEE Signal Processing Letters, V21, P155, DOI 10.1109/LSP.2013.2296038
   Golestaneh SA, 2014, J ELECTRON IMAGING, V23, DOI 10.1117/1.JEI.23.1.013018
   GROSSMANN A, 1984, SIAM J MATH ANAL, V15, P723, DOI 10.1137/0515056
   Hassen R, 2013, IEEE T IMAGE PROCESS, V22, P2798, DOI 10.1109/TIP.2013.2251643
   Lee S, 2012, SIGNAL PROCESS-IMAGE, V27, P31, DOI 10.1016/j.image.2011.08.002
   Li LD, 2016, NEUROCOMPUTING, V177, P572, DOI 10.1016/j.neucom.2015.11.063
   Li LD, 2015, J VIS COMMUN IMAGE R, V30, P153, DOI 10.1016/j.jvcir.2015.04.001
   Li LD, 2014, IEEE SIGNAL PROC LET, V21, P122, DOI 10.1109/LSP.2013.2294333
   Liu H, 2009, EURASIP J ADV SIG PR, DOI 10.1155/2009/263540
   Marziliano P, 2004, SIGNAL PROCESS-IMAGE, V19, P163, DOI 10.1016/j.image.2003.08.003
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Moorthy AK, 2011, IEEE T IMAGE PROCESS, V20, P3350, DOI 10.1109/TIP.2011.2147325
   Moorthy AK, 2010, IEEE SIGNAL PROC LET, V17, P513, DOI 10.1109/LSP.2010.2043888
   Narvekar ND, 2011, IEEE T IMAGE PROCESS, V20, P2678, DOI 10.1109/TIP.2011.2131660
   Ong EP, 2003, SEVENTH INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND ITS APPLICATIONS, VOL 1, PROCEEDINGS, P469, DOI 10.1109/ISSPA.2003.1224741
   Pan F, 2007, MULTIDIM SYST SIGN P, V18, P297, DOI 10.1007/s11045-006-0008-6
   Saad MA, 2012, IEEE T IMAGE PROCESS, V21, P3339, DOI 10.1109/TIP.2012.2191563
   Sang QB, 2014, J VIS COMMUN IMAGE R, V25, P1625, DOI 10.1016/j.jvcir.2014.08.002
   Sun DQ, 2007, IEEE T IMAGE PROCESS, V16, P2743, DOI 10.1109/TIP.2007.904969
   Takeda H, 2007, IEEE T IMAGE PROCESS, V16, P349, DOI 10.1109/TIP.2006.888330
   Vu PV, 2012, IEEE SIGNAL PROC LET, V19, P423, DOI 10.1109/LSP.2012.2199980
   Wang Z, 2000, 2000 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL III, PROCEEDINGS, P981, DOI 10.1109/ICIP.2000.899622
   Wu H.R., 2005, DIGITAL VIDEO IMAGE
   Ye P, 2012, PROC CVPR IEEE, P1098, DOI 10.1109/CVPR.2012.6247789
   Yim C, 2011, IEEE T IMAGE PROCESS, V20, P88, DOI 10.1109/TIP.2010.2061859
   Zhai GT, 2008, IEEE T CIRC SYST VID, V18, P122, DOI 10.1109/TCSVT.2007.906942
   Zhang XF, 2013, IEEE T IMAGE PROCESS, V22, P4613, DOI 10.1109/TIP.2013.2274386
   Zhang Y. P., 2013, TSINGHUA U ED RES, V4, P22, DOI DOI 10.1117/1.JEI.22.4.043025
NR 36
TC 10
Z9 10
U1 1
U2 12
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2018
VL 34
IS 12
BP 1739
EP 1748
DI 10.1007/s00371-017-1460-z
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GY3WM
UT WOS:000448487400010
DA 2024-07-18
ER

PT J
AU Pan, JJ
   Chen, LJ
   Yang, YH
   Qin, H
AF Pan, Junjun
   Chen, Lijuan
   Yang, Yuhan
   Qin, Hong
TI Automatic skinning and weight retargeting of articulated characters
   using extended position-based dynamics
SO VISUAL COMPUTER
LA English
DT Article
DE Linear blend skinning; Position-based dynamics; Weight retargeting;
   Bi-harmonic distance
ID SPACE; ANIMATION
AB Animating an articulated character requires the explicit specification of interior skeleton structure and its attachment to skin surface. This task of "rigging" typically involves the manual weight painting and deformation fine-tuning with popular conventional animation methods. Weight painting is unavoidably a time-consuming and laborious process that would need sophisticated skills from animators during animation production. In this paper, using the extended position-based dynamics (PBD), we have articulated a strategy to generate the realistic skin deformation and reuse the painted weights on a new character. For each frame, the skin is deformed by the linear blend skinning method (LBS) at first. To solve the problem of candy-wrapper effect and surface overlapping in LBS, we improve the traditional PBD method by adding energy constraints and employ several geometrically and physically-based constraints to refine the deformed skin automatically. To further reduce the animator's workload from tedious rigging process, we propose the weight retargeting approach using surface matching and interpolation based on the powerful bi-harmonic distance. It could transfer the weights of an existing model to a new character with similar topology. Through numerous experiments, we could demonstrate the visual performance of our new techniques on a variety of articulated characters.
C1 [Pan, Junjun; Chen, Lijuan; Yang, Yuhan] Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing 100191, Peoples R China.
   [Qin, Hong] SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11794 USA.
C3 Beihang University; State University of New York (SUNY) System; State
   University of New York (SUNY) Stony Brook
RP Pan, JJ (corresponding author), Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing 100191, Peoples R China.
EM pan_junjun@hotmail.com; qin@cs.stonybrook.edu
RI Pan, Junjun/A-1316-2013; chen, lijuan/HSE-1019-2023
CR Abu Rumman N, 2015, COMPUT GRAPH FORUM, V34, P240, DOI 10.1111/cgf.12533
   Ali-Hamadi D, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508415
   Allen B, 2003, ACM T GRAPHIC, V22, P587, DOI 10.1145/882262.882311
   [Anonymous], 1968, P 1968 ACM NAT C
   Avril Q, 2016, COMPUT GRAPH FORUM, V35, P115, DOI 10.1111/cgf.12816
   Baran I, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1239451.1239523, 10.1145/1276377.1276467]
   Bender J, 2014, COMPUT GRAPH FORUM, V33, P228, DOI 10.1111/cgf.12346
   Le BH, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925959
   Dionne Olivier., 2013, P 12 ACM SIGGRAPH EU, P173, DOI DOI 10.1145/2485895.2485919
   Forstmann S., 2006, P EG SHORT PAPERS EU, DOI [10.2312/egs.20061014, DOI 10.2312/EGS.20061014]
   Hiebert B., 2006, ACM SIGGRAPH
   Jacobson A, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964973
   James DL, 2005, ACM T GRAPHIC, V24, P399, DOI 10.1145/1073204.1073206
   Ju T., 2008, ACM T GRAPHIC, V27, P32
   Kavan L., 2008, ACM T GRAPHIC, V27, P995
   Kim Y, 2014, COMPUT ANIMAT VIRT W, V25, P323, DOI 10.1002/cav.1604
   Lewis JP, 2000, COMP GRAPH, P165, DOI 10.1145/344779.344862
   Lipman Y., 2010, ACM T GRAPHIC, V29, P483
   Macklin M., ACM T GR, V33, P1
   Magnenat-Thalmann, 1988, Proceedings of Graphics Interface '88, P26
   Merry B, 2006, ACM T GRAPHIC, V25, P1400, DOI 10.1145/1183287.1183294
   Mohr A, 2003, ACM T GRAPHIC, V22, P562, DOI 10.1145/882262.882308
   Müller M, 2007, J VIS COMMUN IMAGE R, V18, P109, DOI 10.1016/j.jvcir.2007.01.005
   Mukai T, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925905
   Muller M., 2011, WORKSH VIRT REAL INT, P83
   Pan JJ, 2015, COMPUT ANIMAT VIRT W, V26, P321, DOI 10.1002/cav.1655
   Pan JJ, 2009, COMPUT ANIMAT VIRT W, V20, P121, DOI 10.1002/cav.284
   Park S. I., 2008, ACM T GRAPHIC, V27, P15
   Seo J, 2010, COMPUT ANIMAT VIRT W, V21, P375, DOI 10.1002/cav.358
   Smith J., 2006, ACM T GRAPHIC, V25, P115
   Vaillant R, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2661229.2661264
   Vaillant R, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461960
   Xu H., 2016, ACM T GRAPHIC, V35, P1
   Yen L, 2007, LECT NOTES COMPUT SC, V4426, P1037
NR 34
TC 4
Z9 5
U1 0
U2 14
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2018
VL 34
IS 10
BP 1285
EP 1297
DI 10.1007/s00371-017-1413-6
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GR0KK
UT WOS:000442204400002
DA 2024-07-18
ER

PT J
AU Henshall, GI
   Teahan, WJ
   Ap Cenydd, L
AF Henshall, Gareth I.
   Teahan, William J.
   Ap Cenydd, Llyr
TI Virtual reality's effect on parameter optimisation for crowd-sourced
   procedural animation
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT International Conference on Cyberworlds (CW)
CY SEP 20-22, 2017
CL Univ Chester, Chester, ENGLAND
SP Eurograph Assoc, Int Federat Informat Proc, ACM SIGGRAPH
HO Univ Chester
DE Virtual reality; Procedural animation; Parameter optimisation; Genetic
   algorithm
AB Procedural animation systems are capable of synthesising life-like organic motion automatically. However, due to extensive parameterisation, tuning these systems can be very difficult. Not only are there potentially hundreds of interlinked parameters, the resultant animation can be very subjective and the process is difficult to automate effectively. In this paper, we describe a crowd-sourced approach to procedural animation parameter optimisation using genetic algorithms. We test our approach by asking users to interactively rate a population of virtual dolphins to a prescribed behavioural criterion. Our results show that within a few generations a group of users can successfully tune the system towards a desired behaviour. Our secondary motivation is to investigate whether there are differences in animation and behavioural preference between observations made using a standard desktop monitor and those made in virtual reality (VR). We describe a study where users tuned two sets of dolphin animation systems in parallel, one using a normal monitor and another using an Oculus Rift. Our results indicate that being immersed in VR leads to some key differences in preferred behaviour.
C1 [Henshall, Gareth I.; Teahan, William J.; Ap Cenydd, Llyr] Bangor Univ, Sch Comp Sci, Bangor, Gwynedd, Wales.
C3 Bangor University
RP Henshall, GI (corresponding author), Bangor Univ, Sch Comp Sci, Bangor, Gwynedd, Wales.
EM g.i.henshall@bangor.ac.uk; w.j.teahan@bangor.ac.uk;
   llyr.ap.cenydd@bangor.ac.uk
CR [Anonymous], 2010, P 2010 ACM SIGGRAPH
   ap Cenydd L, 2013, COMPUT ANIMAT VIRT W, V24, P65, DOI 10.1002/cav.1436
   Bowman C, 2016, LECT NOTES ARTIF INT, V9799, P182, DOI 10.1007/978-3-319-42007-3_16
   Burrell T, 2016, IFAC PAPERSONLINE, V49, P177, DOI 10.1016/j.ifacol.2016.10.541
   Chao QW, 2013, GRAPH MODELS, V75, P305, DOI 10.1016/j.gmod.2013.07.003
   Coros S, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1781156
   Cummings JJ, 2016, MEDIA PSYCHOL, V19, P272, DOI 10.1080/15213269.2015.1015740
   Fang AC, 2003, ACM T GRAPHIC, V22, P417, DOI 10.1145/882262.882286
   Geijtenbeek T, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508399
   Grzeszczuk R., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P63, DOI 10.1145/218380.218411
   Henshall GI, 2017, 2017 INTERNATIONAL CONFERENCE ON CYBERWORLDS (CW), P48, DOI 10.1109/CW.2017.52
   Henshall GI, 2015, 2015 INTERNATIONAL CONFERENCE ON CYBERWORLDS (CW), P379, DOI 10.1109/CW.2015.57
   Herbelin B., 2016, Human Computer Confluence Transforming Human Experience Through Symbiotic Technologies, P80
   Hodgins J. K., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P153, DOI 10.1145/258734.258822
   Holland J.H., 1992, Adaptation in Natural and Artificial Systems, DOI DOI 10.7551/MITPRESS/1090.001.0001
   Hupont Isabelle., 2015, 2015 Seventh International Workshop on Quality of Multimedia Experience (QoMEX), P1, DOI [DOI 10.1109/QOMEX.2015.7148110, 10.1109/QoMEX.2015.7148110]
   Johansen RuneSkovbo., 2009, Automated semi-procedural animation for character locomotion
   Kovar L, 2002, ACM T GRAPHIC, V21, P473, DOI 10.1145/566570.566605
   Kweon S.H., 2017, International Conference on Applied Human Factors and Ergonomics, P194
   Laerd Statistics, 2011, DYN MOT SYNTH
   LEE Y, 2010, ACM T GRAPHIC, V29
   Liu CK, 2005, ACM T GRAPHIC, V24, P1071, DOI 10.1145/1073204.1073314
   Meredith M., CS0406 U SHEFF DEP C
   Mori N, 2005, GECCO 2005: GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE, VOLS 1 AND 2, P1485
   Murias K, 2016, COMPUT HUM BEHAV, V58, P398, DOI 10.1016/j.chb.2016.01.020
   Natural Motion, 2011, DYN MOT SYNTH
   Ren JP, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0155698
   Reynolds C. W., 1999, P GAM DEV C, P763
   Reynolds CW., 1987, SIGGRAPH Comput. Graph., V21, P25, DOI [10.1145/37402.37406, DOI 10.1145/37402.37406]
   Ridsdale G., 1990, Journal of Visualization and Computer Animation, V1, P66, DOI 10.1002/vis.4340010207
   Shahriari B, 2016, P IEEE, V104, P148, DOI 10.1109/JPROC.2015.2494218
   Sims K., 1994, Computer Graphics Proceedings. Annual Conference Series 1994. SIGGRAPH 94 Conference Proceedings, P15, DOI 10.1145/192161.192167
   van Basten B.J. H., 2011, Graphics Interface, P9
   Wang JM, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618514
   WON Y, 2017, WIRELESS PERSONAL CO
   Wu JC, 2003, ACM T GRAPHIC, V22, P888, DOI 10.1145/882262.882360
   Yin KK, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239556
   YIN KK, 2008, ACM T GRAPHIC, V27
   Zhu C, 2006, SOC ART SCI, V5, P1
NR 39
TC 2
Z9 3
U1 1
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2018
VL 34
IS 9
BP 1255
EP 1268
DI 10.1007/s00371-018-1501-2
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA GQ5MB
UT WOS:000441727000010
OA Green Published, hybrid
DA 2024-07-18
ER

PT J
AU Mochizuki, I
   Toyoura, M
   Mao, XY
AF Mochizuki, Issei
   Toyoura, Masahiro
   Mao, Xiaoyang
TI Visual attention prediction for images with leading line structure
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 35th Computer Graphics International conference (CGI)
CY JUN 11-14, 2018
CL INDONESIA
SP Comp Graph Soc, Nanyang Technol Univ
DE Visual attention model; Saliency map; Structure information; Leading
   lines
ID SALIENCY
AB Researchers have proposed a wide variety of visual attention models, ranging from models that use local, low-level image features to recent approaches that incorporate semantic information. However, most models do not account for the visual attention evident in images with certain global structures. We focus specifically on "leading line" structures, in which explicit or implicit lines converge at a point. Through this study, we have conducted the experiments to investigate the visual attentions in images with leading line structure and propose new models that combine the low-level feature of center-surround differences of visual stimuli, the semantic feature of center bias and the structure feature of leading lines. We also create a new data set from 110 natural images containing leading lines and the eye-tracking data for 16 subjects. Our evaluation experiment showed that our models outperform the existing models against common indicators of saliency-map evaluation, underscoring the importance of leading lines in the modeling of visual attention.
C1 [Mochizuki, Issei; Toyoura, Masahiro; Mao, Xiaoyang] Univ Yamanashi, Dept Comp Sci & Engn, Kofu, Yamanashi 4008511, Japan.
C3 University of Yamanashi
RP Mao, XY (corresponding author), Univ Yamanashi, Dept Comp Sci & Engn, Kofu, Yamanashi 4008511, Japan.
EM mao@yamanashi.ac.jp
OI mao, xiaoyang/0000-0001-9531-3197; Toyoura, Masahiro/0000-0002-5897-7573
FU JSPS [17H00738, 16K12459]; Grants-in-Aid for Scientific Research
   [16K12459, 17H00738] Funding Source: KAKEN
FX This study was funded by JSPS Grants-in-Aid for Scientific Research
   (Grant Nos. 17H00738 and 16K12459).
CR [Anonymous], ECCV
   [Anonymous], 2012, CVPR
   [Anonymous], 2008, NIPS
   [Anonymous], P IEEE C COMP VIS PA
   [Anonymous], 2009, ICCV
   [Anonymous], 2014, COMPUT VISUAL MEDIA
   [Anonymous], 2012, NIPS DEEP LEARN UNS
   [Anonymous], 2016, P IEEE C COMP VIS PA
   Borji A, 2016, J VISION, V16, DOI 10.1167/16.14.18
   Carmi R, 2006, VISION RES, V46, P4333, DOI 10.1016/j.visres.2006.08.019
   Harel J, 2006, Advances in Neural Information Processing Systems, V19
   Harel J., 2012, A saliency implementation in MATLAB
   Huang X, 2015, IEEE I CONF COMP VIS, P262, DOI 10.1109/ICCV.2015.38
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Itti L, 2001, J ELECTRON IMAGING, V10, P161, DOI 10.1117/1.1333677
   Judd T., 2012, MITCSAILTR2012001, P545
   Kong H, 2009, PROC CVPR IEEE, P96, DOI 10.1109/CVPRW.2009.5206787
   Liang H., 2017, 2017 IEEE INT C SYST
   Marat S, 2013, COGN COMPUT, V5, P63, DOI 10.1007/s12559-012-9146-3
   Tatler BW, 2007, J VISION, V7, DOI 10.1167/7.14.4
   Wang WG, 2018, IEEE T IMAGE PROCESS, V27, P2368, DOI 10.1109/TIP.2017.2787612
   Wang Wenguan, 2018, IEEE Trans Image Process, V27, P38, DOI 10.1109/TIP.2017.2754941
   Xu J, 2014, J VISION, V14, DOI 10.1167/14.1.28
   Zhao Q., 2012, J VIS, V14, P1
NR 24
TC 8
Z9 8
U1 1
U2 8
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2018
VL 34
IS 6-8
SI SI
BP 1031
EP 1041
DI 10.1007/s00371-018-1518-6
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA GH6MC
UT WOS:000433557400025
DA 2024-07-18
ER

PT J
AU Bista, S
   da Cunha, ILL
   Varshney, A
AF Bista, Sujal
   Leitao da Cunha, Icaro Lins
   Varshney, Amitabh
TI Kinetic depth images: flexible generation of depth perception
SO VISUAL COMPUTER
LA English
DT Article
DE Kinetic depth effect; Depth perception; Aesthetics in visualization;
   Visualization for the masses
ID MOTION PARALLAX; OPTIC FLOW; STEREOPSIS; CONVERSION; DISPARITY; SYSTEM;
   MODEL; 3-D
AB In this paper we present a systematic approach to create smoothly varying images from a pair of photographs to facilitate enhanced awareness of the depth structure of a given scene. Since our system does not rely on sophisticated display technologies such as stereoscopy or auto-stereoscopy for depth awareness, it (a) is inexpensive and widely accessible, (b) does not suffer from vergence - accommodation fatigue, and (c) works entirely with monocular depth cues. Our approach enhances the depth awareness by optimizing across a number of features such as depth perception, optical flow, saliency, centrality, and disocclusion artifacts. We report the results of user studies that examine the relationship between depth perception, relative velocity, spatial perspective effects, and the positioning of the pivot point and use them when generating kinetic-depth images. We also present a novel depth re-mapping method guided by perceptual relationships based on the results of our user study. We validate our system by presenting a user study that compares the output quality of our proposed method against other existing alternatives on a wide range of images.
C1 [Bista, Sujal; Varshney, Amitabh] Univ Maryland, Dept Comp Sci, College Pk, MD 20742 USA.
   [Bista, Sujal; Varshney, Amitabh] Univ Maryland, Inst Adv Comp Studies, College Pk, MD 20742 USA.
   [Leitao da Cunha, Icaro Lins] Univ Fed Rural Pernambuco, Unidade Acad Garanhuns, Garanhuns, PE, Brazil.
C3 University System of Maryland; University of Maryland College Park;
   University System of Maryland; University of Maryland College Park;
   Universidade Federal Rural de Pernambuco (UFRPE)
RP Bista, S (corresponding author), Univ Maryland, Dept Comp Sci, College Pk, MD 20742 USA.; Bista, S (corresponding author), Univ Maryland, Inst Adv Comp Studies, College Pk, MD 20742 USA.
EM sujal@umiacs.umd.edu
OI Varshney, Amitabh/0000-0002-9873-2212
FU NSF [09-59979, 14-29404]; State of Marylands MPower initiative; NVIDIA
   CUDA Center of Excellence
FX This work has been supported in part by the NSF Grants 09-59979 and
   14-29404, the State of Marylands MPower initiative, and the NVIDIA CUDA
   Center of Excellence. Any opinions, findings, conclusions, or
   recommendations expressed in this article are those of the authors and
   do not necessarily reflect the views of the research sponsors.
CR Agarwal S, 2009, IEEE I CONF COMP VIS, P72, DOI 10.1109/ICCV.2009.5459148
   [Anonymous], 2013, PERSP SHIFT
   Buehler C, 2001, COMP GRAPH, P425, DOI 10.1145/383259.383309
   CAELLI T, 1979, BIOL CYBERN, V33, P29, DOI 10.1007/BF00337415
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851
   Chapiro A, 2014, COMPUT GRAPH FORUM, V33, P63, DOI 10.1111/cgf.12291
   Chaurasia G, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2487228.2487238
   Darsa L., 1997, Proceedings 1997 Symposium on Interactive 3D Graphics, P25, DOI 10.1145/253284.253298
   Davidson C, 2012, PIKU PIKU
   Didyk P., 2012, IS T SPIE ELECT IMAG
   Didyk P, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366203
   DOSHER BA, 1989, VISION RES, V29, P1789, DOI 10.1016/0042-6989(89)90161-2
   DURGIN FH, 1995, J EXP PSYCHOL HUMAN, V21, P679, DOI 10.1037/0096-1523.21.3.679
   EPSTEIN W, 1965, AM J PSYCHOL, V78, P301, DOI 10.2307/1420506
   GIBSON EJ, 1959, J EXP PSYCHOL, V58, P40, DOI 10.1037/h0043883
   Gopi M, 2002, SIBGRAPI 2002: XV BRAZILIAN SYMPOSIUM ON COMPUTER GRAPHICS AND IMAGE PROCESSING, PROCEEDINGS, P179, DOI 10.1109/SIBGRA.2002.1167141
   Harel J, 2006, Advances in Neural Information Processing Systems, V19
   Hartley R., 2003, MULTIPLE VIEW GEOMET
   Heineman J., 2012, STEREOGRANIMATOR
   Hoffman DM, 2008, J VISION, V8, DOI 10.1167/8.3.33
   Ip CY, 2011, IEEE T VIS COMPUT GR, V17, P1737, DOI 10.1109/TVCG.2011.231
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Kellnhofer P, 2013, COMPUT GRAPH FORUM, V32, P143, DOI 10.1111/cgf.12160
   Kim Y, 2006, IEEE T VIS COMPUT GR, V12, P925, DOI 10.1109/TVCG.2006.174
   Kim Y, 2010, ACM T APPL PERCEPT, V7, DOI 10.1145/1670671.1670676
   Krähenbühl P, 2009, ACM T GRAPHIC, V28, DOI [10.1145/1616452.1618472, 10.1145/1618452.1618472]
   LANDY MS, 1991, VISION RES, V31, P859, DOI 10.1016/0042-6989(91)90153-V
   Lang M, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778812
   Lee CH, 2005, ACM T GRAPHIC, V24, P659, DOI 10.1145/1073204.1073244
   Lee CH, 2009, IEICE T INF SYST, VE92D, P369, DOI 10.1587/transinf.E92.D.369
   Lee S, 2014, VISUAL COMPUT, V30, P455, DOI 10.1007/s00371-013-0868-3
   Liu W, 2015, VISUAL COMPUT, V31, P55, DOI 10.1007/s00371-013-0904-3
   Mark W. R., 1997, Proceedings 1997 Symposium on Interactive 3D Graphics, P7, DOI 10.1145/253284.253292
   Michailidis GT, 2014, IEEE T CIRC SYST VID, V24, P929, DOI 10.1109/TCSVT.2013.2290575
   Nagata S., 1991, PICTORIAL COMMUNICAT, P527
   NAKAYAMA K, 1981, VISION RES, V21, P427, DOI 10.1016/0042-6989(81)90089-4
   ONO ME, 1986, J EXP PSYCHOL HUMAN, V12, P331, DOI 10.1037/0096-1523.12.3.331
   Patro R, 2011, IEEE COMPUT GRAPH, V31, P74, DOI 10.1109/MCG.2010.107
   Pérez P, 2003, ACM T GRAPHIC, V22, P313, DOI 10.1145/882262.882269
   Policarpo F., 2005, Proceedings of the 2005 symposium on Interactive 3D graphics and games, P155, DOI DOI 10.1145/1053427.1053453
   PROFFITT DR, 1992, J EXP PSYCHOL HUMAN, V18, P3, DOI 10.1037/0096-1523.18.1.3
   ROBINSON DA, 1986, BIOL CYBERN, V55, P43, DOI 10.1007/BF00363977
   ROGERS B, 1982, VISION RES, V22, P261, DOI 10.1016/0042-6989(82)90126-2
   ROGERS B, 1979, PERCEPTION, V8, P125, DOI 10.1068/p080125
   Snavely N, 2006, ACM T GRAPHIC, V25, P835, DOI 10.1145/1141911.1141964
   SPERLING G, 1989, J EXP PSYCHOL HUMAN, V15, P826
   Stereographics, 1997, STER DEV HDB BACKGR
   Sun DQ, 2010, PROC CVPR IEEE, P2432, DOI 10.1109/CVPR.2010.5539939
   Toyoura M., 2012, P 11 ACM SIGGRAPH IN, P39, DOI DOI 10.1145/2407516.2407526
   Ujike H., 2004, ANN INT C IEEE ENG M
   US. Dept. of Labor, 2013, OCC SAF HLTH ADM
   Vishwanath D, 2013, PSYCHOL SCI, V24, P1673, DOI 10.1177/0956797613477867
   WALLACH H, 1953, J EXP PSYCHOL, V45, P205, DOI 10.1037/h0056880
   Weyrich T, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239483
   Wilson M., 2012, SHOOTING CHALLENGE W
   Yoonessi A., 2011, Journal of Vision, V10, P1194
   Yoshimura K, 2010, CHEM RES APPL-NOVA, P173
   Zhang C., 2015, INT C COMP VIS
   Zheng Ke Colin, 2009, P GRAPHICS INTERFACE, P111
   Zhu C, 2013, VISUAL COMPUT, V29, P609, DOI 10.1007/s00371-013-0827-z
   Zitnick CL, 2005, IEEE I CONF COMP VIS, P1308
NR 61
TC 2
Z9 6
U1 0
U2 15
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2017
VL 33
IS 10
BP 1357
EP 1369
DI 10.1007/s00371-016-1231-2
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA FF8VN
UT WOS:000409296000012
DA 2024-07-18
ER

PT J
AU Zhu, SP
   Yan, LN
AF Zhu, Shiping
   Yan, Lina
TI Local stereo matching algorithm with efficient matching cost and
   adaptive guided image filter
SO VISUAL COMPUTER
LA English
DT Article
DE Computer vision; Stereo matching; Census transform; Guided image filter;
   Integral image; Disparity estimation; Illumination
ID WINDOW
AB To make a matching algorithm to satisfy the requirements of high precision and anti-interference, a novel stereo-matching algorithm with efficient matching cost and adaptive guided image filter is proposed. Firstly, we adopt a modified Census transform with a local texture metric to compute the initial cost. It can make full use of the cross-correlation information between pixels. Meanwhile, we incorporate the Census, color and gradient costs as a mixed matching cost algorithm. Then, we aggregate the costs with guided image filter based on adaptive rectangular support window instead of the traditional fixed support window. The variable kernel window is constructed by the local color similarity and spatial distance. In this way, less occluded points will be included in the support region. On this basis, we adopt integral image to further speed up the computation of this step. Finally, the initial disparity of each pixel is selected using winner takes all optimization and the final disparity maps are gained after post-processing. The experimental results demonstrate that the proposed algorithm not only achieves an average error rate of 5.22 % on the Middlebury stereo benchmark data set, but can also overcome the influence of illumination distortion in the matching effectively.
C1 [Zhu, Shiping; Yan, Lina] Beihang Univ, Sch Instrumentat Sci & Optoelect Engn, Dept Measurement Control & Informat Technol, 37 XueYuan Rd, Beijing 100191, Peoples R China.
C3 Beihang University
RP Zhu, SP (corresponding author), Beihang Univ, Sch Instrumentat Sci & Optoelect Engn, Dept Measurement Control & Informat Technol, 37 XueYuan Rd, Beijing 100191, Peoples R China.
EM spzhu@163.com
RI Zhu, Shiping/C-3754-2012
FU National Natural Science Foundation of China (NSFC) [61375025, 61075011,
   60675018]; Scientific Research Foundation for the Returned Overseas
   Chinese Scholars from the State Education Ministry of China
FX This work was funded by the National Natural Science Foundation of China
   (NSFC) under Grant Nos. 61375025, 61075011, and 60675018, and also the
   Scientific Research Foundation for the Returned Overseas Chinese
   Scholars from the State Education Ministry of China.
CR Ambrosch K, 2010, COMPUT VIS IMAGE UND, V114, P1303, DOI 10.1016/j.cviu.2010.07.008
   Besse F, 2014, INT J COMPUT VISION, V110, P2, DOI 10.1007/s11263-013-0653-9
   Bleyer M, 2007, SIGNAL PROCESS-IMAGE, V22, P127, DOI 10.1016/j.image.2006.11.012
   Boykov Y, 1998, IEEE T PATTERN ANAL, V20, P1283, DOI 10.1109/34.735802
   Changming Sun, 1997, DICTA'97 and IVCNZ'97. Proceedings of the First Joint Australia and New Zealand Biennial Conference on: Digital Image and Vision Computing - Techniques and Applications. DICTA'97. Digital Image Computing - Techniques and Applications. IVCNZ'97. Image and Vision Computing New Zealand, P95
   De-Maeztu L, 2011, IEEE I CONF COMP VIS, P1708, DOI 10.1109/ICCV.2011.6126434
   De-Maeztu L, 2011, PATTERN RECOGN LETT, V32, P1643, DOI 10.1016/j.patrec.2011.06.027
   Di Stefano L, 2004, IMAGE VISION COMPUT, V22, P983, DOI 10.1016/j.imavis.2004.03.009
   Fusiello A, 1997, PROC CVPR IEEE, P858, DOI 10.1109/CVPR.1997.609428
   He KM, 2013, IEEE T PATTERN ANAL, V35, P1397, DOI 10.1109/TPAMI.2012.213
   Hirschmüller H, 2009, IEEE T PATTERN ANAL, V31, P1582, DOI 10.1109/TPAMI.2008.221
   Hirschmüller H, 2005, PROC CVPR IEEE, P807, DOI 10.1109/cvpr.2005.56
   Hosni A, 2011, IEEE INT CON MULTI, DOI 10.1109/ICME.2011.6012131
   Hosni A, 2013, IEEE T PATTERN ANAL, V35, P504, DOI 10.1109/TPAMI.2012.156
   Hosni A, 2009, IEEE IMAGE PROC, P2093, DOI 10.1109/ICIP.2009.5414478
   KANADE T, 1994, IEEE T PATTERN ANAL, V16, P920, DOI 10.1109/34.310690
   Kim JC, 2005, PROC CVPR IEEE, P1075
   Kurt M., 2013, TPCG, P85, DOI 10.2312/LocalChapterEvents.TPCG.TPCG13.085-092.URL
   Lee Z, 2013, IEEE T MULTIMEDIA, V15, P1855, DOI 10.1109/TMM.2013.2270456
   Liu J, 2015, VISUAL COMPUT, V31, P1253, DOI 10.1007/s00371-014-1009-3
   Mattoccia Stefano, 2009, Computer Vision - ACCV 2009. 9th Asian Conference on Computer Vision. Revised Selected Papers, P371
   Mei X., 2011, On building an accurate stereo matching system on graphics hardware
   Papadakis N, 2010, J MATH IMAGING VIS, V38, P70, DOI 10.1007/s10851-010-0212-8
   Ren J, 2010, INT CON DISTR COMP S, DOI 10.1109/ICDCS.2010.26
   Scharstein D, 2002, INT J COMPUT VISION, V47, P7, DOI 10.1023/A:1014573219977
   Scharstein D., 2010, MIDDLEBURY STEREO VI
   Tang Y, 2012, VISUAL COMPUT, V28, P743, DOI 10.1007/s00371-012-0701-4
   Tomasi C, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P839, DOI 10.1109/ICCV.1998.710815
   Veksler O, 2003, PROC CVPR IEEE, P556
   Wang L, 2007, THIRD INTERNATIONAL SYMPOSIUM ON 3D DATA PROCESSING, VISUALIZATION, AND TRANSMISSION, PROCEEDINGS, P798
   Xu YF, 2014, APPL OPTICS, V53, P6885, DOI 10.1364/AO.53.006885
   Xu ZL, 2008, VISUAL COMPUT, V24, P45, DOI 10.1007/s00371-007-0177-9
   Xuefeng Chang, 2011, 2011 International Conference on 3D Imaging, Modeling, Processing, Visualization and Transmission (3DIMPVT), P73, DOI 10.1109/3DIMPVT.2011.17
   Yang QQ, 2014, IMAGE VISION COMPUT, V32, P202, DOI 10.1016/j.imavis.2014.01.001
   Yoon KJ, 2006, IEEE T PATTERN ANAL, V28, P650, DOI 10.1109/TPAMI.2006.70
   Zabih R., 1994, Computer Vision - ECCV '94. Third European Conference on Computer Vision. Proceedings. Vol.II, P151, DOI 10.1007/BFb0028345
   Zhang K, 2009, IEEE T CIRC SYST VID, V19, P1073, DOI 10.1109/TCSVT.2009.2020478
NR 37
TC 33
Z9 44
U1 3
U2 43
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2017
VL 33
IS 9
BP 1087
EP 1102
DI 10.1007/s00371-016-1264-6
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FD1CS
UT WOS:000407275600002
DA 2024-07-18
ER

PT J
AU Asadipour, A
   Debattista, K
   Chalmers, A
AF Asadipour, Ali
   Debattista, Kurt
   Chalmers, Alan
TI Visuohaptic augmented feedback for enhancing motor skills acquisition
SO VISUAL COMPUTER
LA English
DT Article
DE Computer based training; Health care education; Augmented feedback;
   Medical palpation; Multi-sensory feedback (visuohaptic); Serious games
   (computing)
ID VIRTUAL-REALITY; GAMES
AB Serious games are accepted as an effective approach to deliver augmented feedback in motor (re-)learning processes. The multi-modal nature of the conventional computer games (e.g. audiovisual representation) plus the ability to interact via haptic-enabled inputs provides a more immersive experience. Thus, particular disciplines such as medical education in which frequent hands on rehearsals play a key role in learning core motor skills (e.g. physical palpations) may benefit from this technique. Challenges such as the impracticality of verbalising palpation experience by tutors and ethical considerations may prevent the medical students from correctly learning core palpation skills. This work presents a new data glove, built from off-the-shelf components which captures pressure sensitivity designed to provide feedback for palpation tasks. In this work the data glove is used to control a serious game adapted from the infinite runner genre to improve motor skill acquisition. A comparative evaluation on usability and effectiveness of the method using multimodal visualisations, as part of a larger study to enhance pressure sensitivity, is presented. Thirty participants divided into a game-playing group () and a control group () were invited to perform a simple palpation task. The game-playing group significantly outperformed the control group in which abstract visualisation of force was provided to the users in a blind-folded transfer test. The game-based training approach was positively described by the game-playing group as enjoyable and engaging.
C1 [Asadipour, Ali; Debattista, Kurt; Chalmers, Alan] Univ Warwick, Visualisat Grp, WMG, Coventry, W Midlands, England.
C3 University of Warwick
RP Asadipour, A (corresponding author), Univ Warwick, Visualisat Grp, WMG, Coventry, W Midlands, England.
EM A.Asadipour@warwick.ac.uk
RI ASADIPOUR, ALI/ABD-7746-2021; Asadipour, Ali/C-9768-2019
OI ASADIPOUR, ALI/0000-0003-0159-3090; 
FU Royal Society Industrial Fellowship
FX Grateful acknowledgement is made to members of staff and students in WMG
   for their participation and support. We would like to thank Unity and
   Creepy Cat for making available the 2D Game Starter Assets used to
   prepare the game used in this publication. Debattista is partially
   supported by a Royal Society Industrial Fellowship.
CR Arnab S, 2013, SERIOUS GAMES FOR HEALTHCARE: APPLICATIONS AND IMPLICATIONS, P1, DOI 10.4018/978-1-4666-1903-6
   Asadipour A., 2005, THESIS
   BENDTSEN L, 1995, CEPHALALGIA, V15, P205, DOI 10.1046/j.1468-2982.1995.015003205.x
   Boulos MNK, 2007, HEALTH INFO LIBR J, V24, P233, DOI 10.1111/J.1471-1842.2007.00733.x
   Brown KE, 2013, SERIOUS GAMES FOR HEALTHCARE: APPLICATIONS AND IMPLICATIONS, P135, DOI 10.4018/978-1-4666-1903-6.ch007
   Burdea G, 1999, IEEE T BIO-MED ENG, V46, P1253, DOI 10.1109/10.790503
   Carmeli E, 2009, 2009 VIRTUAL REHABILITATION INTERNATIONAL CONFERENCE, P220, DOI 10.1109/ICVR.2009.5174258
   Chalmers A, 2009, VISUAL COMPUT, V25, P1101, DOI 10.1007/s00371-009-0389-2
   Cohen J., 1988, STAT POWER ANAL BEHA
   Dunwell I, 2013, SERIOUS GAMES FOR HEALTHCARE: APPLICATIONS AND IMPLICATIONS, P233, DOI 10.4018/978-1-4666-1903-6.ch011
   Duvivier RJ, 2012, ADV HEALTH SCI EDUC, V17, P339, DOI 10.1007/s10459-011-9312-5
   Faul F, 2007, BEHAV RES METHODS, V39, P175, DOI 10.3758/BF03193146
   Florez J.A., 2010, ANDESCON 2010 IEEE, P1, DOI [DOI 10.1109/ANDESCON.2010.5633120, 10.1109/ANDESCON.2010.5633120]
   Futarmal S, 2011, J DENT RES, V90, P918, DOI 10.1177/0022034511402997
   GOODWIN J, 1995, BRIT MED J, V310, P1281, DOI 10.1136/bmj.310.6990.1281
   Gotsis M, 2009, IEEE COMPUT GRAPH, V29, P14, DOI 10.1109/MCG.2009.94
   Graafland M, 2012, BRIT J SURG, V99, P1322, DOI 10.1002/bjs.8819
   Hulusic V, 2012, COMPUT GRAPH FORUM, V31, P102, DOI 10.1111/j.1467-8659.2011.02086.x
   IEE, 2013, CUST INP SENS CIS SO
   JENSEN TR, 1991, J BIOMECH, V24, P851, DOI 10.1016/0021-9290(91)90310-J
   Kato PM, 2008, PEDIATRICS, V122, pE305, DOI 10.1542/peds.2007-3134
   Macleod J., 2009, MACLEODS CLIN EXAMIN
   Patel V., 2011, PRACTICAL PROFESSION
   Saini S., 2012, 2012 Proceedings of International Conference on Computer & Information Science (ICCIS 2012), P55, DOI 10.1109/ICCISci.2012.6297212
   Sauter GmbH, 2015, INSTR MAN FORC GAUG
   Scarle S., 2011, Proceedings of the 2011 3rd International Conference on Games and Virtual Worlds for Serious Applications (VS-GAMES 2011), P178, DOI 10.1109/VS-GAMES.2011.48
   Schonauer C., 2011, Virtual Rehabilitation (ICVR), 2011 International Conference on, P1, DOI DOI 10.1109/ICVR.2011.5971855
   Sliney Aidan, 2008, 2008 First International Conference on Advances in Computer Human Interaction - ACHI '08, P131
   Srinivasan M.A., 1993, Proc. ASME Dynamic Systems and Control Division: Advances in Robotics, Mechatronics, P119
   Susi T., 2007, Serious Games: An Overview
   Williams EM, 2012, J HUM EVOL, V62, P520, DOI 10.1016/j.jhevol.2012.02.005
NR 31
TC 10
Z9 11
U1 1
U2 28
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2017
VL 33
IS 4
BP 401
EP 411
DI 10.1007/s00371-016-1275-3
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA ER4JZ
UT WOS:000398767100002
OA hybrid, Green Accepted, Green Published, Green Submitted
DA 2024-07-18
ER

PT J
AU Shang, J
   Chen, C
   Pei, XB
   Liang, H
   Tang, H
   Sarem, M
AF Shang, Jun
   Chen, Chuanbo
   Pei, Xiaobing
   Liang, Hu
   Tang, He
   Sarem, Mudar
TI A novel local derivative quantized binary pattern for object recognition
SO VISUAL COMPUTER
LA English
DT Article
DE Feature descriptor; Local derivative binary pattern; Object recognition
ID CLASSIFICATION
AB Designing efficient and effective keypoint descriptors for an image plays a vital role in many computer vision tasks. The traditional binary descriptors such as local binary pattern and its variants directly perform a binarization operation on the intensity differences of the local affine covariant regions, thus their performance usually drops a lot because of the limited distinctiveness. In this paper, we propose a novel image keypoint descriptor, namely local derivative quantized binary pattern for object recognition. To incorporate the spatial information, we first divide the local affine covariant region into several subregions according to the intensity orders. For each sub region, we quantize the intensity differences between the central pixels and their neighbors in an adaptive way, and then we order the differences and use a hash function to map the differences into binary codes. The binary codes are histogramed to form the feature of each subregion. Furthermore, we utilize multi-scale support regions and pool the histograms together to represent the features of the image. Our approach does not need prior codebook training and hence it is more flexible and easy to be implemented. Moreover, our descriptor can preserve more local brightness and edge information than the traditional binary descriptors. Also, our descriptor is robust to rotation, illumination variations and other geometric transformations. Finally we conduct extensive experiments on three challenging datasets (i.e., 53 Objects, ZuBuD, and Kentucky) for object recognition and the experimental results show that our descriptor outperforms the existing state-of-the-art descriptors.
C1 [Shang, Jun; Liang, Hu] Huazhong Univ Sci & Technol, Sch Comp Sci & Technol, Wuhan, Peoples R China.
   [Chen, Chuanbo; Pei, Xiaobing; Tang, He; Sarem, Mudar] Huazhong Univ Sci & Technol, Sch Software Engn, Wuhan, Peoples R China.
   [Shang, Jun] Hubei Univ Educ, Hubei Coinnovat Ctr Basic Educ Informat Technol, High Tech Rd 129, Wuhan 430205, Peoples R China.
C3 Huazhong University of Science & Technology; Huazhong University of
   Science & Technology; Hubei University of Education
RP Chen, C (corresponding author), Huazhong Univ Sci & Technol, Sch Software Engn, Wuhan, Peoples R China.
EM shangjunhust@gmail.com; chuanboc@163.com
RI Tang, He/AAN-7118-2020
FU Hubei Provincial Natural Science Foundation of China [2013CFB152]
FX This work is supported partially by Hubei Provincial Natural Science
   Foundation of China (No.2013CFB152).
CR Alahi A, 2012, PROC CVPR IEEE, P510, DOI 10.1109/CVPR.2012.6247715
   Alcantarilla PF, 2012, LECT NOTES COMPUT SC, V7577, P214, DOI 10.1007/978-3-642-33783-3_16
   [Anonymous], 2012, P NEUR INF PROC SYST
   Bay H., 2008, COMPUT VIS IMAGE UND, V10, P346, DOI DOI 10.1016/j.cviu.2007.09.014
   Calonder M, 2010, LECT NOTES COMPUT SC, V6314, P778, DOI 10.1007/978-3-642-15561-1_56
   Cao ZM, 2010, PROC CVPR IEEE, P2707, DOI 10.1109/CVPR.2010.5539992
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Ding SY, 2015, PATTERN RECOGN, V48, P2993, DOI 10.1016/j.patcog.2015.04.005
   Nguyen DT, 2013, PATTERN RECOGN, V46, P1485, DOI 10.1016/j.patcog.2012.10.024
   Nguyen DT, 2010, IEEE IMAGE PROC, P4609, DOI 10.1109/ICIP.2010.5651633
   Guo ZH, 2010, IEEE T IMAGE PROCESS, V19, P1657, DOI 10.1109/TIP.2010.2044957
   Guo ZH, 2010, PATTERN RECOGN, V43, P706, DOI 10.1016/j.patcog.2009.08.017
   Heikkilä M, 2009, PATTERN RECOGN, V42, P425, DOI 10.1016/j.patcog.2008.08.014
   Hu RX, 2014, IEEE T IMAGE PROCESS, V23, DOI 10.1109/TIP.2013.2286330
   Jiang B, 2014, IEEE T IMAGE PROCESS, V23, P5175, DOI 10.1109/TIP.2014.2362614
   Lei Z, 2014, IEEE T PATTERN ANAL, V36, P289, DOI 10.1109/TPAMI.2013.112
   Leutenegger S, 2011, IEEE I CONF COMP VIS, P2548, DOI 10.1109/ICCV.2011.6126542
   Li CC, 2015, VISUAL COMPUT, V31, P1419, DOI 10.1007/s00371-014-1023-5
   Lin L, 2012, PATTERN RECOGN, V45, P231, DOI 10.1016/j.patcog.2011.06.011
   Liu W, 2012, PROC CVPR IEEE, P2074, DOI 10.1109/CVPR.2012.6247912
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Mu Y., 2008, CVPR
   Vu NS, 2012, IEEE T IMAGE PROCESS, V21, P1352, DOI 10.1109/TIP.2011.2166974
   Qi XB, 2014, IEEE T PATTERN ANAL, V36, P2199, DOI 10.1109/TPAMI.2014.2316826
   Rublee E, 2011, IEEE I CONF COMP VIS, P2564, DOI 10.1109/ICCV.2011.6126544
   Satpathy A, 2014, IEEE T IMAGE PROCESS, V24, P1953, DOI 10.1109/TIP.2014.2310123
   Satpathy A, 2014, IEEE T IMAGE PROCESS, V23, P287, DOI 10.1109/TIP.2013.2264677
   Shrivastava N, 2014, VISUAL COMPUT, V30, P1223, DOI 10.1007/s00371-013-0887-0
   Singh C, 2012, VISUAL COMPUT, V28, P1085, DOI 10.1007/s00371-011-0659-7
   Tan XY, 2010, IEEE T IMAGE PROCESS, V19, P1635, DOI 10.1109/TIP.2010.2042645
   Tola E, 2010, IEEE T PATTERN ANAL, V32, P815, DOI 10.1109/TPAMI.2009.77
   Trzcinski T, 2013, PROC CVPR IEEE, P2874, DOI 10.1109/CVPR.2013.370
   Tuytelaars T., LOCAL INVARIANT FEAT
   ul Hussain S, 2012, LECT NOTES COMPUT SC, V7573, P716, DOI 10.1007/978-3-642-33709-3_51
   Wang J, 2010, PROC CVPR IEEE, P3424, DOI 10.1109/CVPR.2010.5539994
   Wu H, 2015, VISUAL COMPUT, V31, P367, DOI 10.1007/s00371-014-0931-8
   Zhang BC, 2010, IEEE T IMAGE PROCESS, V19, P533, DOI 10.1109/TIP.2009.2035882
   Zhang JE, 2011, PROC CVPR IEEE, P1393, DOI 10.1109/CVPR.2011.5995678
   Zhu C, 2013, PATTERN RECOGN, V46, P1949, DOI 10.1016/j.patcog.2013.01.003
NR 39
TC 7
Z9 7
U1 0
U2 16
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2017
VL 33
IS 2
BP 221
EP 233
DI 10.1007/s00371-015-1179-7
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA EI2TF
UT WOS:000392340400009
DA 2024-07-18
ER

PT J
AU Subileau, T
   Mellado, N
   Vanderhaeghe, D
   Paulin, M
AF Subileau, Thomas
   Mellado, Nicolas
   Vanderhaeghe, David
   Paulin, Mathias
TI RayPortals: a light transport editing framework
SO VISUAL COMPUTER
LA English
DT Article
DE Rendering; Global illumination; Editing; Manipulation; Physically based
ID INTERFACE
AB Physically based rendering, using path-space formulation of global illumination, has become a standard technique for high-quality computer-generated imagery. Nonetheless, being able to control and edit the resulting picture so that it corresponds to the artist vision is still a tedious trial-and-error process. We show how the manipulation of light transport translates into the path-space integral formulation of the rendering equation. We introduce portals as a path-space manipulation tool to edit and control renderings and show how our editing tool unifies and extends previous work on lighting editing. Portals allow the artist to precisely control the final aspect of the image without modifying neither scene geometry nor lighting setup. According to the setup of two geometric handles and a simple path selection filter, portals capture specific lightpaths and teleport them through 3D space. We implement portals in major path-based algorithms (Photon Mapping, Progressive Photon Mapping and Bi-directional Path Tracing) and demonstrate the wide range of control this technique allows on various lighting effects, from low-frequency color bleeding to high-frequency caustics as well as view-dependent reflections.
C1 [Subileau, Thomas; Mellado, Nicolas; Vanderhaeghe, David] Univ Toulouse, IRIT, Toulouse, France.
   [Paulin, Mathias] Univ Toulouse, Toulouse, France.
   [Subileau, Thomas; Mellado, Nicolas; Vanderhaeghe, David; Paulin, Mathias] UPS, Toulouse, France.
C3 Universite Federale Toulouse Midi-Pyrenees (ComUE); Universite de
   Toulouse; Institut National Polytechnique de Toulouse; Universite
   Toulouse III - Paul Sabatier; Universite de Toulouse; Universite de
   Toulouse; Universite Toulouse III - Paul Sabatier
RP Subileau, T (corresponding author), Univ Toulouse, IRIT, Toulouse, France.; Subileau, T (corresponding author), UPS, Toulouse, France.
EM subileau@irit.fr
RI Vanderhaeghe, David/AAL-1240-2021; Mellado, Nicolas/J-5411-2014
OI Vanderhaeghe, David/0000-0003-0506-6036; Mellado,
   Nicolas/0000-0003-2180-4318
CR [Anonymous], 1998, THESIS
   BARZEL R., 1997, J GRAPHICS TOOLS, V2, P1
   Birn J., 2005, DIGITAL LIGHTING REN
   Blender Online Community, 2015, BLEND 3D MOD REND PA
   Damez C., 2003, 27 SIGGRAPH, P27
   Hachisuka T, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409083
   Heckbert P. S., 1990, Computer Graphics, V24, P145, DOI 10.1145/97880.97895
   Jensen H W, 2004, ACM SIGGRAPH 2004 CO
   Kerr WB, 2010, COMPUT GRAPH FORUM, V29, P1451, DOI 10.1111/j.1467-8659.2010.01742.x
   Lafortune E. P., 1993, EDUGRAPHICS '93. First International Conference on Graphics Education. COMPUGRAPHICS '93. Third International Conference on Computational Graphics and Visualization Techniques. Combined Proceedings, P145
   Lagae A., 2005, Journal of Graphics Tools, V10, P23
   Mattausch O, 2013, COMPUT GRAPH FORUM, V32, P175, DOI 10.1111/cgf.12037
   Nowrouzezahrai D., 2011, ACM T GRAPHIC, V30
   Obert J., 2010, P 21 EUR C REND EGSR
   Okabe M, 2007, PACIFIC GRAPHICS 2007: 15TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, P171, DOI 10.1109/PG.2007.9
   Pellacini F, 2002, ACM T GRAPHIC, V21, P563, DOI 10.1145/566570.566617
   PELLACINI F., 2007, ACM T GRAPH, V26, P2
   Pellacini F, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778771
   Raymond B, 2014, COMPUT GRAPH FORUM, V33, P313, DOI 10.1111/cgf.12300
   Reiner T, 2012, COMPUT GRAPH FORUM, V31, P711, DOI 10.1111/j.1467-8659.2012.03050.x
   Ritschel T., 2009, ACM T GRAPHIC, V28
   Ritschel T, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778773
   Schmidt TW, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461980
NR 23
TC 3
Z9 3
U1 0
U2 5
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2017
VL 33
IS 2
BP 129
EP 138
DI 10.1007/s00371-015-1163-2
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA EI2TF
UT WOS:000392340400002
DA 2024-07-18
ER

PT J
AU Alemasoom, H
   Samavati, F
   Brosz, J
   Layzell, D
AF Alemasoom, Haleh
   Samavati, Faramarz
   Brosz, John
   Layzell, David
TI EnergyViz: an interactive system for visualization of energy systems
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT International Conference on Cyberworlds (CW)
CY OCT 06-08, 2014
CL Santander, SPAIN
SP IEEE Comp Soc, Univ Cantabria, Comp Graph & Geometr Modeling Grp, Toho Univ, Fac Sci, Dept Informat Sci, European Assoc Comp Graph, Int Federat Informat Proc, Workgroup 5 10 Comp Graph & Virtual Worlds, Univ Cantabria, Dept Appl Math & Computatl Sci, Vice Rector Res & Knowledge Transfer, Municipal Santander, Reg Govt Cantabria, Spanish Minist Econ & Competitiveness, Cantabria Campus Int, Int Federat Informat Proc, Tech Comm 5 Informat Technol Applicat
DE Energy system; Visualization; Flow; Sankey diagram; Time-varying;
   Spatial; Animation
AB Energy systems are under pressure to transform to address concerns about climate change. The modeling and visualization of energy systems can play an important role in communicating the costs, benefits and trade-offs of energy systems choices. We introduce EnergyViz, a visualization system that provides an interface for exploring time-varying, multi-attribute and spatial properties of a particular energy system. EnergyViz integrates several visualization techniques to facilitate exploration of a particular energy system. These techniques include flow diagram representation to show energy flow, 3D interaction with flow diagrams for expanding viewable data attributes such as emissions and an interactive map integrated with flow diagrams for simultaneous exploration of spatial and abstract information. We also perform level-of-detail exploration on flow diagrams and use smooth animation across the visualizations to represent time-varying data. Finally, we include evaluation results of EnergyViz collected from expert and inexperienced participants.
C1 [Alemasoom, Haleh; Samavati, Faramarz] Univ Calgary, Dept Comp Sci, Calgary, AB T2N 1N4, Canada.
   [Brosz, John] Univ Calgary, Lib & Cultural Resources, Calgary, AB, Canada.
   [Layzell, David] Canadian Energy Syst Anal Res CESAR Inst, Calgary, AB, Canada.
C3 University of Calgary; University of Calgary
RP Alemasoom, H (corresponding author), Univ Calgary, Dept Comp Sci, Calgary, AB T2N 1N4, Canada.
EM hsalemas@ucalgary.ca; samavati@ucalgary.ca; jdlbrosz@ucalgary.ca;
   dlayzell@ucalgary.ca
RI Brosz, John/P-8467-2015; Layzell, David B/A-9417-2010
OI Brosz, John/0000-0002-4123-9240; 
CR Aigner W, 2011, HUM-COMPUT INT-SPRIN, P1, DOI 10.1007/978-0-85729-079-3
   Aigner W, 2007, COMPUT GRAPH-UK, V31, P401, DOI 10.1016/j.cag.2007.01.030
   Alemasoom H., 2014, CYB INT C SANT SPAIN
   [Anonymous], 1983, VISUAL DISPLAY QUANT
   Auber D, 2005, NINTH INTERNATIONAL CONFERENCE ON INFORMATION VISUALISATION, PROCEEDINGS, P703, DOI 10.1109/IV.2005.65
   Bendix F, 2005, INFOVIS 05: IEEE SYMPOSIUM ON INFORMATION VISUALIZATION, PROCEEDINGS, P133, DOI 10.1109/INFVIS.2005.1532139
   Bezerianos A, 2010, COMPUT GRAPH FORUM, V29, P863, DOI 10.1111/j.1467-8659.2009.01687.x
   Bondy J.A., 1976, Graph theory with applications, V6
   Brockenauer R., 2001, DRAWING GRAPHS METHO
   Collins C, 2007, IEEE T VIS COMPUT GR, V13, P1192, DOI 10.1109/TVCG.2007.70521
   Erten C, 2004, LECT NOTES COMPUT SC, V2912, P98
   Guo DS, 2006, IEEE T VIS COMPUT GR, V12, P1461, DOI 10.1109/TVCG.2006.84
   Healy P., 2013, Handbook of Graph Drawing and Visualization, V1st
   Kothur P., 2012, Visual-ization of geospatial time series from environmental modeling output, P115
   Müller W, 2003, PROCEEDINGS OF THE 2003 WINTER SIMULATION CONFERENCE, VOLS 1 AND 2, P737, DOI 10.1109/WSC.2003.1261490
   North SC, 2002, LECT NOTES COMPUT SC, V2265, P232
   Phan D, 2005, INFOVIS 05: IEEE SYMPOSIUM ON INFORMATION VISUALIZATION, PROCEEDINGS, P219, DOI 10.1109/INFVIS.2005.1532150
   Riehmann P, 2005, INFOVIS 05: IEEE SYMPOSIUM ON INFORMATION VISUALIZATION, PROCEEDINGS, P233, DOI 10.1109/INFVIS.2005.1532152
   Robertson G, 2008, IEEE T VIS COMPUT GR, V14, P1325, DOI 10.1109/TVCG.2008.125
   Steele J., 2011, Beautiful Visualization, Vfirst
   SUGIYAMA K, 1981, IEEE T SYST MAN CYB, V11, P109, DOI 10.1109/TSMC.1981.4308636
   TILLER W, 1984, IEEE COMPUT GRAPH, V4, P36, DOI 10.1109/MCG.1984.275995
   von Landesberger T, 2012, IEEE CONF VIS ANAL, P183, DOI 10.1109/VAST.2012.6400553
   Wongsuphasawat K, 2012, IEEE T VIS COMPUT GR, V18, P2659, DOI 10.1109/TVCG.2012.225
NR 24
TC 9
Z9 11
U1 0
U2 10
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2016
VL 32
IS 3
BP 403
EP 413
DI 10.1007/s00371-015-1186-8
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA DF9FK
UT WOS:000371666200013
DA 2024-07-18
ER

PT J
AU Zhang, WJ
   Xiong, QY
   Shi, WR
   Chen, SH
AF Zhang, Wenjie
   Xiong, Qingyu
   Shi, Weiren
   Chen, Shuhan
TI Region saliency detection via multi-feature on absorbing Markov chain
SO VISUAL COMPUTER
LA English
DT Article
DE Saliency region; Image contrast; Space relation; Background prior;
   Absorbing Markov chain
ID OBJECT DETECTION; VISUAL SALIENCY; ATTENTION
AB Saliency region detection plays an important role in image pre-processing, and uniformly emphasizing saliency region is still an intractable problem in computer vision. In this paper, we present a data-driven salient region detection method via multi-feature (included contrast, spatial relationship and background prior, etc.) on absorbing Markov chain, which uses super pixel to extract salient regions, and each super-pixel represents a node. In detail, we first construct function to calculate absorption probability of each node on absorbing Markov chain. Second we utilize image contrast and space relation to model the prior salient map which is provided to foreground salient nodes and then calculate the saliency of nodes based on absorption probability. Third, we also exploit background prior to supply the absorbing nodes and compute the saliency of nodes. Finally, we fuse both the saliency of nodes by cosine similarity measurement method and acquire the ultimate saliency map. Our approach is simple and efficient and highlights not only a single object but also multiple objects consistently. We test the proposed method on MSRA-B, iCoSeg and SED databases. Experimental results illustrate that the proposed approach presents better robustness and efficiency against the eleven state-of-the art algorithms.
C1 [Zhang, Wenjie; Shi, Weiren] Chongqing Univ, Coll Automat, Chongqing 400044, Peoples R China.
   [Xiong, Qingyu] MOE, Key Lab Dependable Serv Comp Cyber Phys Soc, Chongqing 400044, Peoples R China.
   [Xiong, Qingyu] Chongqing Univ, Sch Software Engn, Chongqing 400044, Peoples R China.
   [Chen, Shuhan] Yangzhou Univ, Coll Informat Engn, Yangzhou 225009, Jiangsu, Peoples R China.
C3 Chongqing University; Chongqing University; Yangzhou University
RP Xiong, QY (corresponding author), Chongqing Univ, Sch Software Engn, Chongqing 400044, Peoples R China.
EM daaiyiyejian@cqu.edu.cn; cquxqy@163.com; wrs@cqu.edu.cn;
   c.shuhan@gmail.com
RI Zhang, Wenjie/ABB-6712-2020
OI Zhang, Wenjie/0000-0002-5563-0996
CR Achanta R, 2010, IEEE IMAGE PROC, P2653, DOI 10.1109/ICIP.2010.5652636
   Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   Aldous D., Reversible markov chains and random walks on graphs
   Alpert S, 2007, PROC CVPR IEEE, P359
   [Anonymous], 2010, Technical Report
   [Anonymous], 2012, P ACM MULT
   Batra D, 2011, INT J COMPUT VISION, V93, P273, DOI 10.1007/s11263-010-0415-x
   Batra D, 2010, PROC CVPR IEEE, P3169, DOI 10.1109/CVPR.2010.5540080
   Chang KY, 2011, IEEE I CONF COMP VIS, P914, DOI 10.1109/ICCV.2011.6126333
   Du SZ, 2014, IEEE SIGNAL PROC LET, V21, P51, DOI 10.1109/LSP.2013.2290547
   Einhäuser W, 2003, EUR J NEUROSCI, V17, P1089, DOI 10.1046/j.1460-9568.2003.02508.x
   Goferman S, 2012, IEEE T PATTERN ANAL, V34, P1915, DOI 10.1109/TPAMI.2011.272
   Grinstead C. M., 1998, INTRO PROBABILITY, P10
   Harel J, 2006, Advances in Neural Information Processing Systems, V19
   Hou X., 2007, IEEE C COMP VIS PATT, V1, P1, DOI DOI 10.1109/CVPR.2007.383267
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Jiang BW, 2013, IEEE I CONF COMP VIS, P1665, DOI 10.1109/ICCV.2013.209
   Jiang HZ, 2013, PROC CVPR IEEE, P2083, DOI 10.1109/CVPR.2013.271
   Judd T, 2009, IEEE I CONF COMP VIS, P2106, DOI 10.1109/ICCV.2009.5459462
   Ko BC, 2006, J OPT SOC AM A, V23, P2462, DOI 10.1364/JOSAA.23.002462
   Liu T, 2011, IEEE T PATTERN ANAL, V33, P353, DOI 10.1109/TPAMI.2010.70
   Martinez-Gil J, 2013, INFORM SYST FRONT, V15, P399, DOI 10.1007/s10796-012-9404-7
   Min Chen, 2011, IEEE INFOCOM 2011 - IEEE Conference on Computer Communications. Workshops, P409, DOI 10.1109/INFCOMW.2011.5928847
   Norris J. R., 1998, MARKOV CHAINS
   OTSU N, 1979, IEEE T SYST MAN CYB, V9, P62, DOI 10.1109/TSMC.1979.4310076
   Tan P.-N., 2005, Introduction to Data Mining, P500
   Toet A, 2011, IEEE T PATTERN ANAL, V33, P2131, DOI 10.1109/TPAMI.2011.53
   Wang D, 2011, VISUAL COMPUT, V27, P853, DOI 10.1007/s00371-011-0559-x
   Wang XJ, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXP (ICME), VOLS 1-3, P2231, DOI 10.1109/ICME.2004.1394714
   Wei YC, 2012, LECT NOTES COMPUT SC, V7574, P29, DOI 10.1007/978-3-642-33712-3_3
   Yang C, 2013, PROC CVPR IEEE, P3166, DOI 10.1109/CVPR.2013.407
   YANG JM, 2012, PROC CVPR IEEE, P2296, DOI [DOI 10.1109/CVPR.2012.6247940, 10.1109/CVPR.2012.6247940]
   Zhai Y., 2006, P 14 ACM INT C MULT, P815, DOI [DOI 10.1145/1180639.1180824, 10.1145/1180639.1180824]
   Zhang HL, 2016, VISUAL COMPUT, V32, P31, DOI 10.1007/s00371-014-1053-z
NR 34
TC 14
Z9 14
U1 0
U2 19
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2016
VL 32
IS 3
BP 275
EP 287
DI 10.1007/s00371-015-1065-3
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DF9FK
UT WOS:000371666200002
DA 2024-07-18
ER

PT J
AU Danelakis, A
   Theoharis, T
   Pratikakis, I
AF Danelakis, Antonios
   Theoharis, Theoharis
   Pratikakis, Ioannis
TI A robust spatio-temporal scheme for dynamic 3D facial expression
   retrieval
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 3DOR Workshop
CY APR 06, 2014
CL Strasbourg, FRANCE
DE Dynamic 3D mesh sequence; 3D Object retrieval; Facial expressions;
   Wavelet transformation
AB The problem of facial expression recognition in dynamic sequences of 3D face scans has received a significant amount of attention in the recent past whereas the problem of retrieval in this type of data has not. A novel retrieval scheme for such data is introduced in this paper. It is the first spatio-temporal retrieval scheme ever used for retrieval in dynamic sequences of 3D face scans. The proposed scheme automatically detects specific facial landmarks and uses them to create a spatio-temporal descriptor. At first, geometric as well as topological information of the 3D face scans is captured by using the detected landmarks. In the sequel, the aforementioned spatial information is filtered by using wavelet transformation, resulting to our final spatio-temporal descriptor. Our descriptor is invariant to the number of the 3D face scans of a facial expression sequence. The proposed retrieval scheme exploits the Square of Euclidean distance in order to compare descriptors corresponding to different 3D facial sequences. A detailed evaluation of the introduced retrieval scheme is presented showing that it outperforms previous state-of-the-art retrieval schemes. Experiments have been conducted using the six prototypical expressions of the standard data set BU - 4DFE. Finally, a majority voting methodology based on the retrieval results is used to achieve unsupervised dynamic 3D facial expression recognition. The achieved classification accuracy outperforms the state-of-the-art supervised dynamic 3D facial expression recognition techniques.
C1 [Danelakis, Antonios; Theoharis, Theoharis] Univ Athens, Dept Informat & Telecommun, Athens, Greece.
   [Theoharis, Theoharis] Norwegian Univ Sci & Technol, Dept Comp & Informat Sci, N-7034 Trondheim, Norway.
   [Pratikakis, Ioannis] Democritus Univ Thrace, Dept Elect & Comp Engn, GR-67100 Xanthi, Greece.
C3 National & Kapodistrian University of Athens; Norwegian University of
   Science & Technology (NTNU); Democritus University of Thrace
RP Danelakis, A (corresponding author), Univ Athens, Dept Informat & Telecommun, Athens, Greece.
EM a.danelakis@gmail.com; theotheo@di.uoa.gr; ipratika@ee.duth.gr
RI Theoharis, Theoharis/AAN-2555-2020; PRATIKAKIS, IOANNIS/AAD-3387-2019
OI PRATIKAKIS, IOANNIS/0000-0002-4124-3688
NR 0
TC 6
Z9 6
U1 0
U2 1
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2016
VL 32
IS 2
BP 257
EP 269
DI 10.1007/s00371-015-1142-7
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA DF9FU
UT WOS:000371667200009
DA 2024-07-18
ER

PT J
AU Wong, SK
   Cheng, YC
AF Wong, Sai-Keung
   Cheng, Yu-Chun
TI GPU-based radial view-based culling for continuous self-collision
   detection of deformable surfaces
SO VISUAL COMPUTER
LA English
DT Article
DE Continuous collision detection; Radial view-based culling; Deformable
   surfaces
AB We propose a graphics processing unit-based approach to accelerate the radial view-based culling method for continuous self-collision detection of deformable surfaces. The deformable surfaces may have small round-shaped holes and ghost triangles are used to fill the holes. We identify the key processes of the radial view-based culling method, including triangle classification, traversal of bounding volume hierarchies and handling violated triangles (i.e., the triangles intersecting with ghost triangles). We propose efficient parallel processing techniques to perform these key processes on a programmable graphics unit. We have evaluated our proposed approach on several examples. Experimental results show that our approach significantly cuts down the cost of the key processes of the radial-based culling method, compared with the serial implementation on CPU.
C1 [Wong, Sai-Keung; Cheng, Yu-Chun] Natl Chiao Tung Univ, Dept Comp Sci, Hsinchu, Taiwan.
C3 National Yang Ming Chiao Tung University
RP Wong, SK (corresponding author), Natl Chiao Tung Univ, Dept Comp Sci, Hsinchu, Taiwan.
EM cswingo@cs.nctu.edu.tw
FU National Science Council of ROC (Taiwan) [NSC 102-2221-E-009-103-MY2];
   Ministry of Science and Technology of ROC (Taiwan) [MOST
   103-2221-E-009-122-MY3]
FX The authors thank the anonymous referees for their constructive
   comments. This work was supported in part by the National Science
   Council of ROC (Taiwan) under the grant no. NSC 102-2221-E-009-103-MY2
   and the Ministry of Science and Technology of ROC (Taiwan) under the
   grant no. MOST 103-2221-E-009-122-MY3.
CR Allard J, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778819
   [Anonymous], P 18 M ACM SIGGRAPH
   [Anonymous], THESIS HKUST
   [Anonymous], 2009, P 2009 ACM SIGGRAPH, DOI DOI 10.1145/1599470.1599480
   [Anonymous], ACM T GRAPH P SIGGRA
   [Anonymous], ACM S VIRT REAL SOFT
   [Anonymous], ACM SIGGRAPH S INT 3
   [Anonymous], J GR GPU GAME TOOLS
   Baciu George., 2002, P ACM S VIRTUAL REAL, P129
   Gottschalk S., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P171, DOI 10.1145/237170.237244
   Govindaraju N.K., 2003, Proc. of Symp. on Geom. Processing, P25
   Govindaraju NK, 2005, ACM T GRAPHIC, V24, P991, DOI 10.1145/1073204.1073301
   Heidelberger B., 2004, PROC WSCG, P145
   Kim D, 2009, COMPUT GRAPH FORUM, V28, P1791, DOI 10.1111/j.1467-8659.2009.01556.x
   Klosowski JT, 1998, IEEE T VIS COMPUT GR, V4, P21, DOI 10.1109/2945.675649
   Knott D, 2003, PROC GRAPH INTERF, P73
   Larsson T, 2006, COMPUT GRAPH-UK, V30, P450, DOI 10.1016/j.cag.2006.02.011
   Lauterbach C, 2010, COMPUT GRAPH FORUM, V29, P419, DOI 10.1111/j.1467-8659.2009.01611.x
   Liu F, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866180
   Pabst S, 2010, COMPUT GRAPH FORUM, V29, P1605, DOI 10.1111/j.1467-8659.2010.01769.x
   PALMER IJ, 1995, COMPUT GRAPH FORUM, V14, P105, DOI 10.1111/1467-8659.1420105
   Pan J, 2010, SPRINGER TRAC ADV RO, V68, P211
   Provot X., 1997, GRAPHICS INTERFACE, P177
   Tang M., 2011, I3D 11 P 2011 ACM SI, P63
   Tang M, 2008, VISUAL COMPUT, V24, P545, DOI 10.1007/s00371-008-0235-y
   Tang M, 2009, IEEE T VIS COMPUT GR, V15, P544, DOI 10.1109/TVCG.2009.12
   Teschner M, 2005, COMPUT GRAPH FORUM, V24, P61, DOI 10.1111/j.1467-8659.2005.00829.x
   Vassilev T, 2001, COMPUT GRAPH FORUM, V20, pC260, DOI 10.1111/1467-8659.00518
   VOLINO P, 1994, COMPUT GRAPH FORUM, V13, pC155, DOI 10.1111/1467-8659.1330155
   Wong SK, 2015, VISUAL COMPUT, V31, P377, DOI 10.1007/s00371-014-0933-6
   Wong SK, 2014, COMPUT GRAPH FORUM, V33, P143, DOI 10.1111/cgf.12284
   Wong SK, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461951
   Wong W. S. -K., 2006, P ACM VRCIA, P181
   Wong WSK, 2007, COMPUT ANIMAT VIRT W, V18, P179, DOI 10.1002/cav.173
   Wong WSK, 2005, IEEE T VIS COMPUT GR, V11, P329, DOI 10.1109/TVCG.2005.44
   Zhang XY, 2012, IEEE T VIS COMPUT GR, V18, P1146, DOI 10.1109/TVCG.2011.120
NR 36
TC 3
Z9 5
U1 0
U2 8
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2016
VL 32
IS 1
BP 67
EP 81
DI 10.1007/s00371-014-1056-9
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DF9FQ
UT WOS:000371666800007
DA 2024-07-18
ER

PT J
AU Wu, HY
   Wang, JM
AF Wu, Huiyue
   Wang, Jianmin
TI A visual attention-based method to address the midas touch problem
   existing in gesture-based interaction
SO VISUAL COMPUTER
LA English
DT Article
DE Gesture-based interaction; Dynamic gesture; Visual attention; Midas
   Touch problem
ID HAND POSE; MODEL; RECOGNITION; TRACKING
AB The "Midas Touch" problem has long been a difficult problem existing in gesture-based interaction. This paper proposes a visual attention-based method to address this problem from the perspective of cognitive psychology. There are three main contributions in this paper: (1) a visual attention-based parallel perception model is constructed by combining top-down and bottom-up attention, (2) a framework is proposed for dynamic gesture spotting and recognition simultaneously, and (3) a gesture toolkit is created to facilitate gesture design and development. Experimental results show that the proposed method has a good performance for both isolated and continuous gesture recognition tasks. Finally, we highlight the implications of this work for the design and development of all gesture-based applications.
C1 [Wu, Huiyue] Sun Yat Sen Univ, Sch Commun & Design, Guangzhou 510275, Guangdong, Peoples R China.
   [Wang, Jianmin] Tongji Univ, Coll Arts & Media, User Experience Lab, Shanghai 200092, Peoples R China.
C3 Sun Yat Sen University; Tongji University
RP Wu, HY (corresponding author), Sun Yat Sen Univ, Sch Commun & Design, Guangzhou 510275, Guangdong, Peoples R China.
EM wuhuiyue@gmail.com; wangjianmin@tongji.edu.cn
RI wang, jian/GVS-0711-2022; Jiang, Yalin/ITV-2565-2023
OI Jiang, Yalin/0009-0003-3726-8828; wang, jianmin/0000-0001-8703-8973
FU National Natural Science Foundation of China [61202344]; Fundamental
   Research Funds for the Central Universities, Sun Yat-Sen University
   [1209119]; Special Project on the Integration of Industry, Education and
   Research of Guangdong Province [2012B091000062]; Fundamental Research
   Funds for the Central Universities, Tongji University [0600219052,
   0600219053]
FX We thank the financial support from the National Natural Science
   Foundation of China, No. 61202344; the Fundamental Research Funds for
   the Central Universities, Sun Yat-Sen University, No. 1209119; Special
   Project on the Integration of Industry, Education and Research of
   Guangdong Province, No. 2012B091000062; the Fundamental Research Funds
   for the Central Universities, Tongji University, No. 0600219052,
   0600219053. We would like to express our great appreciation to editor
   and reviewers.
CR [Anonymous], 2006, P 19 ANN ACM AYMPOSI
   Betke M, 2002, IEEE T NEUR SYS REH, V10, P1, DOI 10.1109/TNSRE.2002.1021581
   Colaco Andrea., 2013, P 26 ANN ACM S USER, P227
   Cover T. M., 1991, ELEMENTS INFORM THEO
   Elmezain M, 2009, 2009 SECOND INTERNATIONAL CONFERENCE ON MACHINE VISION, PROCEEDINGS, ( ICMV 2009), P128, DOI 10.1109/ICMV.2009.28
   Feng ZQ, 2010, VISUAL COMPUT, V26, P607, DOI 10.1007/s00371-010-0452-z
   Hilliges O, 2009, UIST 2009: PROCEEDINGS OF THE 22ND ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P139
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Itti L., 2000, MODELS BOTTOM UP TOP
   Jacob R. J., 1993, ADV HUMAN COMPUTER I, V4, P151
   JONIDES J, 1983, B PSYCHONOMIC SOC, V21, P247
   Kato H, 2000, IEEE AND ACM INTERNATIONAL SYMPOSIUM ON AUGMENTED REALITY, PROCEEDING, P111, DOI 10.1109/ISAR.2000.880934
   Kjeldsen R, 2004, MACH VISION APPL, V16, P6, DOI 10.1007/s00138-004-0145-6
   KOCH C, 1985, HUM NEUROBIOL, V4, P219
   Kölsch M, 2004, PROCEEDINGS OF MOBIQUITOUS 2004, P86
   Kristensson P.O., 2012, P 2012 ACM INT C INT, P89
   Lee HK, 1999, IEEE T PATTERN ANAL, V21, P961, DOI 10.1109/34.799904
   Liang H, 2013, VISUAL COMPUT, V29, P837, DOI 10.1007/s00371-013-0822-4
   Liu N., 2004, Proceedings of the Ninth International Workshop on Frontiers in Handwriting Recognition. International Workshop in Frontiers of Handwriting Recognition, P100
   Marr D., 1982, Visual perception
   Mo Z.Y., 2005, P 10 INT C INT US IN, P239
   Mujibiya A., 2010, P ACM S US INT SOFTW, P443
   Pan ZG, 2010, P IEEE VIRT REAL ANN, P219, DOI 10.1109/VR.2010.5444787
   Pedersoli F, 2014, VISUAL COMPUT, V30, P1107, DOI 10.1007/s00371-014-0921-x
   Peng B, 2011, IEEE T PATTERN ANAL, V33, P1175, DOI 10.1109/TPAMI.2010.199
   RABINER LR, 1989, P IEEE, V77, P257, DOI 10.1109/5.18626
   Rovelo G, 2014, 32ND ANNUAL ACM CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2014), P4077, DOI 10.1145/2556288.2557113
   Salah AA, 2002, IEEE T PATTERN ANAL, V24, P420, DOI 10.1109/34.990146
   Shen Y, 2011, INT J HUM-COMPUT INT, V27, P523, DOI 10.1080/10447318.2011.555297
   Song Peng., 2012, P 2012 ACM ANN C HUM, P1297, DOI DOI 10.1145/2207676.2208585
   Tian M., 2007, THESIS BEIJING JIAOT
   TREISMAN AM, 1980, COGNITIVE PSYCHOL, V12, P97, DOI 10.1016/0010-0285(80)90005-5
   Ungerleider L.G., 1982, ANAL VISUAL BEHAV, P549, DOI DOI 10.2139/SSRN.1353746
   Vatavu Radu-Daniel, 2012, P 10 EUR C INT TV VI, P45, DOI [10.1145/2325616.2325626, DOI 10.1145/2325616.2325626]
   Walter R., 2013, Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, P841, DOI [DOI 10.1145/2470654.2470774, 10.1145/2470654.2470774]
   Wu Hui-Yue, 2011, Journal of Software, V22, P1067, DOI 10.3724/SP.J.1001.2011.03733
   Yang HD, 2007, IEEE T ROBOT, V23, P256, DOI 10.1109/TRO.2006.889491
   Yang MH, 2002, IEEE T PATTERN ANAL, V24, P1061, DOI 10.1109/TPAMI.2002.1023803
NR 38
TC 13
Z9 15
U1 1
U2 19
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2016
VL 32
IS 1
BP 123
EP 136
DI 10.1007/s00371-014-1060-0
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DF9FQ
UT WOS:000371666800011
DA 2024-07-18
ER

PT J
AU Mykhalchuk, V
   Seo, H
   Cordier, F
AF Mykhalchuk, Vasyl
   Seo, Hyewon
   Cordier, Frederic
TI On spatio-temporal feature point detection for animated meshes
SO VISUAL COMPUTER
LA English
DT Article
DE Feature detection; Animated mesh; Multi-scale representation; Difference
   of Gaussian
AB Although automatic feature detection has been a long-sought subject by researchers in computer graphics and computer vision, feature extraction on deforming models remains a relatively unexplored area. In this paper, we develop a new method for automatic detection of spatio-temporal feature points on animated meshes. Our algorithm consists of three main parts. We first define local deformation characteristics, based on strain and curvature values computed for each point at each frame. Next, we construct multi-resolution space-time Gaussians and difference-of-Gaussian (DoG) pyramids on the deformation characteristics representing the input animated mesh, where each level contains 3D smoothed and subsampled representation of the previous level. Finally, we estimate locations and scales of spatio-temporal feature points by using a scale-normalized differential operator. A new, precise approximation of spatio-temporal scale-normalized Laplacian has been introduced, based on the space-time DoG. We have experimentally verified our algorithm on a number of examples and conclude that our technique allows to detect spatio and temporal feature points in a reliable manner.
C1 [Mykhalchuk, Vasyl; Seo, Hyewon] Univ Strasbourg, Strasbourg, France.
   [Cordier, Frederic] Univ Haute Alsace, Mulhouse, France.
C3 Universites de Strasbourg Etablissements Associes; Universite de
   Strasbourg; Universites de Strasbourg Etablissements Associes;
   Universite de Haute-Alsace (UHA)
RP Seo, H (corresponding author), Univ Strasbourg, Strasbourg, France.
EM seo@unistra.fr
FU French national project SHARED (Shape Analysis and Registration of
   People Using Dynamic Data) [10-CHEX-014-01]
FX We acknowledge Robert W. Sumner for providing triangle correspondences
   of the horse and camel models. We also thank Frederic Larue and Olivier
   Genevaux for their assistance with the facial motion capture. This work
   has been supported by the French national project SHARED (Shape Analysis
   and Registration of People Using Dynamic Data, No. 10-CHEX-014-01).
CR Alliez P, 2003, ACM T GRAPHIC, V22, P485, DOI 10.1145/882262.882296
   Andonie R., 1992, Computers and Artificial Intelligence, V11, P363
   Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014
   Castellani U, 2008, COMPUT GRAPH FORUM, V27, P643, DOI 10.1111/j.1467-8659.2008.01162.x
   Darom T, 2012, IEEE T IMAGE PROCESS, V21, P2758, DOI 10.1109/TIP.2012.2183142
   GOTOH O, 1990, B MATH BIOL, V52, P359, DOI 10.1007/BF02458577
   Harris C., 1988, P 4 ALV VIS C, V15, P10
   Kircher S., 2005, ACM SIGGRAPH EUROGRA, P191
   Laptev I, 2003, LECT NOTES COMPUT SC, V2695, P372
   Lee C.-H., 2005, ACM T GRAPHICS
   Lian ZH, 2013, INT J COMPUT VISION, V102, P221, DOI 10.1007/s11263-012-0548-1
   Lindeberg T, 1998, INT J COMPUT VISION, V30, P79, DOI 10.1023/A:1008045108935
   Lindeberg T., 1994, Journal of AppliedStatistics, V21, P225
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Mikolajczyk K, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P525, DOI 10.1109/ICCV.2001.937561
   Pauly M, 2003, COMPUT GRAPH FORUM, V22, P281, DOI 10.1111/1467-8659.00675
   Pratikakis I., 2010, EUR WORKSH 3D OBJ RE, P7, DOI DOI 10.2312/3DOR/3DOR10/007-014
   Salden AH, 1998, J MATH IMAGING VIS, V9, P103, DOI 10.1023/A:1008300826001
   Scovanner P., 2007, P 15 ACM INT C MULT, P357, DOI DOI 10.1145/1291233.1291311
   Shamir A, 2000, IEEE VISUAL, P423, DOI 10.1109/VISUAL.2000.885724
   Sumner RW, 2005, ACM T GRAPHIC, V24, P488, DOI 10.1145/1073204.1073218
   Sumner RW, 2004, ACM T GRAPHIC, V23, P399, DOI 10.1145/1015706.1015736
   Zaharescu A, 2009, PROC CVPR IEEE, P373, DOI 10.1109/CVPRW.2009.5206748
NR 23
TC 1
Z9 1
U1 0
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2015
VL 31
IS 11
BP 1471
EP 1486
DI 10.1007/s00371-014-1027-1
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CT3BJ
UT WOS:000362680600004
DA 2024-07-18
ER

PT J
AU Radloff, A
   Tominski, C
   Nocke, T
   Schumann, H
AF Radloff, Axel
   Tominski, Christian
   Nocke, Thomas
   Schumann, Heidrun
TI Supporting presentation and discussion of visualization results in smart
   meeting rooms
SO VISUAL COMPUTER
LA English
DT Article
DE Information presentation; Smart meeting room; Visualization; Interaction
ID DESIGN
AB Visualization has become an accepted tool to support the process of gaining insight into data. Current visualization research mainly focuses on exploratory or confirmatory visualization taking place in classic workplace settings. In this paper, we focus on the presentation and discussion of visualization results among domain experts, rather than on the generation of visual representations by visualization experts. We develop a visualization infrastructure for a novel kind of visualization environment labeled smart meeting room, which provides plenty of display space to present the visual information to be discussed. We describe the mechanisms needed to show multiple visualization views on multiple displays and to interact with the views across device boundaries. Our system includes methods to dynamically generate visualization views, to suggest suitable layouts of the views, and to enable interactive fine-tuning to accommodate the dynamically changing needs of the user (e.g., access to details on demand). The benefits for the users are illustrated by an application in the context of climate impact research.
C1 [Radloff, Axel; Tominski, Christian; Schumann, Heidrun] Univ Rostock, Inst Comp Sci, D-18055 Rostock, Germany.
   [Nocke, Thomas] Potsdam Inst Climate Impact Res, Potsdam, Germany.
C3 University of Rostock; Potsdam Institut fur Klimafolgenforschung
RP Tominski, C (corresponding author), Univ Rostock, Inst Comp Sci, D-18055 Rostock, Germany.
EM axel.radloff@uni-rostock.de; christian.tominski@uni-rostock.de;
   nocke@pik-potsdam.de; heidrun.schumann@uni-rostock.de
RI Tominski, Christian/H-6388-2019
OI Tominski, Christian/0000-0001-7704-355X
CR Aarts E.H.L., 2006, TRUE VISIONS EMERGEN
   Ali K, 2008, LECT NOTES COMPUT SC, V5166, P247, DOI 10.1007/978-3-540-85412-8_24
   Andrews C. R., 2004, CLICKY USER CENTRIC
   [Anonymous], 2004, SMART ENV TECHNOLOGY
   [Anonymous], 2010, Proceedings of Graphics Interface 2010
   [Anonymous], VRST 05, DOI DOI 10.1145/1101616.1101637
   [Anonymous], 2013, UB SER 7000
   [Anonymous], 2005, ECSTRIAM05009 U SOUT
   Bader S., 2012, P 9 IEEE WORKSH MAN
   Bader Sebastian, 2010, P 12 ACM INT C ADJ U, P355, DOI DOI 10.1145/1864431.1864433
   Bauer M, 2002, PERS UBIQUIT COMPUT, V6, P322, DOI 10.1007/s007790200036
   Bryden Aaron, 2011, Advances in Visual Computing. Proceedings 7th International Symposium, ISVC 2011, P518
   Chung HY, 2014, PERS UBIQUIT COMPUT, V18, P1169, DOI 10.1007/s00779-013-0727-2
   Forlines C., 2008, Proceedings of the working conference on Advanced visual interfaces, AVI '08, P367, DOI DOI 10.1145/1385569.1385635
   Fukazawa R, 2010, IEEE SYMPOSIUM ON 3D USER INTERFACES (3DUI 2010), P103, DOI 10.1109/3DUI.2010.5444711
   GELERNTER D, 1985, ACM T PROGR LANG SYS, V7, P80, DOI 10.1145/2363.2433
   Haller M, 2006, LECT NOTES COMPUT SC, V4282, P185
   Haller M, 2009, HAGENBERG RESEARCH, P433
   Heider T., 2005, P 2005 JOINT C SMART, P271, DOI [10.1145/1107548.1107614, DOI 10.1145/1107548.1107614]
   Izadi Izadi Shahram Shahram, Proceedings o f UIST (User Interface Software and Technology), P159, DOI [10.1145/964696.964714, DOI 10.1145/964696.964714, 10.1145/964696.964714 10.1145/964696.964714]
   JAIMES A, 2005, P 21 INT C DAT ENG W, P1173, DOI DOI 10.1109/ICDE.2005.202
   Jetter HC, 2012, INT J HUM-COMPUT INT, V28, P737, DOI 10.1080/10447318.2012.715539
   Johanson B., 2002, IEEE Pervasive Computing, V1, P67, DOI 10.1109/MPRV.2002.1012339
   Johanson B., 2000, CS200003 STANF U
   Kurihara K, 2007, LECT NOTES COMPUT SC, V4662, P430
   Lee J. C., 2013, WII PROJECTS
   Marcellin M. W., 2002, JPEG 2000 IMAGE COMP
   Mitchell M., 1998, INTRO GENETIC ALGORI
   Nacenta MiguelA., 2006, P SIGCHI C HUMAN FAC, P289, DOI [10.1145/1124772.1124817, DOI 10.1145/1124772.1124817]
   Nocke T., 2009, INFORM TECHNOLOGY CL, P29
   Peck C.H., 2001, P CHI 01 EXTENDED AB, P461, DOI [10.1145/634067.634333, DOI 10.1145/634067.634333]
   Radloff A., 2012, Proceedings of the Eighth International Conference on Intelligent Environments (IE 2012), P228, DOI 10.1109/IE.2012.34
   Radloff A, 2011, LECT NOTES COMPUT SC, V6815, P1, DOI 10.1007/978-3-642-22571-0_1
   Rosenbaum R., 2006, THESIS U ROSTOCK GER
   Shape F., 2013, SENSFLOOR
   Spindler M., 2010, ACM INT C INTERACTIV, P157, DOI DOI 10.1145/1936652.1936684
   Steinberger M, 2011, IEEE T VIS COMPUT GR, V17, P2249, DOI 10.1109/TVCG.2011.183
   Su S., 2000, Proceedings of the 10th International Conference on Artificial Reality and Telexistence, P112
   Tan D. S., 2004, CHI Extended Abstracts, DOI [10.1145/985921.986106, DOI 10.1145/985921.986106]
   Thiede C, 2009, INFORMATION VISUALIZATION, IV 2009, PROCEEDINGS, P227, DOI 10.1109/IV.2009.54
   Waldner M., 2011, THESIS GRAZ U TECHNO
   Waldner M., 2010, P 6 NORD C HUM COMP, P813
   Waldner M., 2009, P WORKSH COLL VIS IN, P36
   Waldner M, 2011, 29TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P959
   Waldner M, 2010, IUI 2010, P397
   Wigdor D, 2009, CHI2009: PROCEEDINGS OF THE 27TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1-4, P1237
   Wilson A, 2003, P INTERACT
   Wilson A., 2003, P ACM C SIGCHI, V1, P545, DOI DOI 10.1145/642611.642706
   Wrobel M., 2009, P INT S ENV SOFTW SY
   Yost B, 2007, CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1 AND 2, P101
   Youngblood GM, 2005, 19TH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI-05), P1513
NR 51
TC 2
Z9 3
U1 0
U2 7
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2015
VL 31
IS 9
BP 1271
EP 1286
DI 10.1007/s00371-014-1010-x
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CO8AY
UT WOS:000359388100009
DA 2024-07-18
ER

PT J
AU Bohl, E
   Terraz, O
   Ghazanfarpour, D
AF Bohl, Evans
   Terraz, Olivier
   Ghazanfarpour, Djamchid
TI Modeling fruits and their internal structure using parametric 3Gmap
   L-systems
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 32nd Computer Graphics International CGI 15 Conference
CY JUN 24-26, 2015
CL INSA Strasbourg Univ Strasbourg, Strasbourg, FRANCE
SP CNRS, iCUBE, Univ Strasbourg, CGS, Springer, Acm Incooperation, IGG, ACMSIGGRAPH, KIST Europe, Region Alsace, Visteon
HO INSA Strasbourg Univ Strasbourg
DE Fruits; Formal grammar; Procedural modeling; 3Gmap L-systems
AB Modeling a fruit using classic 3D modeling software can be a relatively complicated task. Moreover, modeling every single fruit when we need to generate a large variety of fruits of the same species is not a viable option because it is time consuming. This paper presents an original ad hoc method for modeling a wide range of 3D fruits, using a single formal grammar. Fruits are modeled by parametric 3GmapL-systems that describe their shape and internal structure, thanks to variables and mathematical functions. At the end of our work, our method will eventually be an interesting solution to realistic modeling of fruits and their interior, and to automatic detection and recognition of real fruits.
C1 [Bohl, Evans; Terraz, Olivier; Ghazanfarpour, Djamchid] XLIM, Limoges, France.
RP Bohl, E (corresponding author), XLIM, Limoges, France.
EM evans.bohl@xlim.fr; olivier.terraz@xlim.fr;
   djamchid.ghazanfarpour@xlim.fr
CR [Anonymous], 1996, The Algorithmic Beauty of Plants
   Bao F, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2421636.2421644
   Benes B, 2003, THEORY AND PRACTICE OF COMPUTER GRAPHICS, PROCEEDINGS, P58, DOI 10.1109/TPCG.2003.1206931
   Catmull E., 1998, SEMINAL GRAPHICS
   Cieslak M., 2012, P HORTIMODEL2012
   Cutler B, 2002, ACM T GRAPHIC, V21, P302
   Durikovic R, 1998, COMPUT GRAPH FORUM, V17, pC1, DOI 10.1111/1467-8659.00248
   FOWLER DR, 1992, COMP GRAPH, V26, P361, DOI 10.1145/142920.134093
   Génard M, 2007, J EXP BOT, V58, P917, DOI 10.1093/jxb/erl287
   Génard M, 2010, PLANT J, V62, P344, DOI 10.1111/j.1365-313X.2010.04152.x
   Kider JT, 2011, COMPUT GRAPH FORUM, V30, P257, DOI 10.1111/j.1467-8659.2011.01857.x
   LIENHARDT P, 1991, COMPUT AIDED DESIGN, V23, P59, DOI 10.1016/0010-4485(91)90082-8
   Lienhardt P, 1994, INT J COMPUT GEOM AP, V4, P275, DOI 10.1142/S0218195994000173
   LINDENMAYER A, 1968, J THEOR BIOL, V18, P280, DOI 10.1016/0022-5193(68)90079-9
   Lindenmayer A, 1978, LNCS, V73, P301
   Lintermann B, 1999, IEEE COMPUT GRAPH, V19, P56, DOI 10.1109/38.736469
   Mauseth J.D., 2009, Botany: an introduction to plant biology, V4th
   Müller P, 2006, ACM T GRAPHIC, V25, P614, DOI 10.1145/1141911.1141931
   OECD, 2002, TOM INT STAND FRUIT
   Palubicki W, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531364
   Parish YIH, 2001, COMP GRAPH, P301, DOI 10.1145/383259.383292
   Pasko A, 2001, GRAPH MODELS, V63, P413, DOI 10.1006/gmod.2001.0560
   Petrenko O., 2011, P 21 INT C COMP GRAP, P20
   Peyrat A, 2008, VISUAL COMPUT, V24, P807, DOI 10.1007/s00371-008-0262-8
   Prusinkiewicz P, 1999, INT WORKSH AGTIVE 99, P395
   Runions A, 2005, ACM T GRAPHIC, V24, P702, DOI 10.1145/1073204.1073251
   Runions A., 2007, NPH, P63
   Takayama K, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866202
   Takayama K, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360652
   Terraz O, 2009, VISUAL COMPUT, V25, P165, DOI 10.1007/s00371-008-0212-5
   UNECE, 2012, STANDARD FFV 36 MARK
   Wang LD, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024201
NR 32
TC 9
Z9 11
U1 0
U2 12
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2015
VL 31
IS 6-8
BP 819
EP 829
DI 10.1007/s00371-015-1108-9
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA CM2CN
UT WOS:000357487500008
DA 2024-07-18
ER

PT J
AU Berger, P
   Laier, A
   Velho, L
AF Berger, Pierre
   Laier, Alex
   Velho, Luiz
TI An image-space algorithm for immersive views in 3-manifolds and
   orbifolds
SO VISUAL COMPUTER
LA English
DT Article
DE 3-Manifolds; Hyperbolic geometry
AB We describe the first image-space ray-tracing algorithm to generate interior views of flat or hyperbolic 3-manifolds, orbifolds, and similar polyhedral complexes. The complexity of our algorithm is linear in the number of echoes for a given number of polygons, compared to exponential complexity for the object-space algorithm chosen as standard.
C1 [Berger, Pierre] Univ Paris 13, CNRS, LAGA, F-93430 Villetaneuse, France.
   [Laier, Alex] Univ Fed Fluminense, Dept Geometry, Niteroi, RJ, Brazil.
   [Velho, Luiz] CNPq, Inst Matemat Pura & Aplicada, Rio De Janeiro, Brazil.
C3 Centre National de la Recherche Scientifique (CNRS); Universite Paris
   13; Universidade Federal Fluminense
RP Velho, L (corresponding author), CNPq, Inst Matemat Pura & Aplicada, Rio De Janeiro, Brazil.
EM berger@math.univ-paris13.fr; alex.laier@gmail.com; lvelho@impa.br
CR Amenta  N., 1995, P 11 ANN S COMP GEOM, P412, DOI DOI 10.1145/220279.220327
   Boileau M., 2003, PANORAMAS SYNTHESES, V15
   Burton BA, 2004, EXP MATH, V13, P267, DOI 10.1080/10586458.2004.10504538
   Damour T, 2003, CLASSICAL QUANT GRAV, V20, pR145, DOI 10.1088/0264-9381/20/9/201
   Francis GK, 2003, VISUALIZATION AND MATHEMATICS III, P305
   Gunn C., 1993, Computer Graphics Proceedings, P255, DOI 10.1145/166117.166150
   Haefliger A., 1990, Progr. Math., V83, P203
   HANSON AJ, 1994, COMPUTER, V27, P73, DOI 10.1109/2.299415
   Hudson R., 1995, Proceedings 1995 Symposium on Interactive 3D Graphics, P167, DOI 10.1145/199404.199433
   James Arvo., 1989, INTRO RAY TRACING, P201
   MUNZNER T., 1995, P 1 S VIRT REAL MOD, P33, DOI DOI 10.1145/217306.217311
   Phillips Mark., 1992, Proceedings of the 1992 Symposium on Interactive 3D Graphics I3D'92, P209, DOI DOI 10.1145/147156.147206
   Thurston W.P., 1979, The Geometry and Topology of Three-Manifolds
   von Gagern M, 2009, ELECTRON J COMB, V16
   Weeks J, 2002, IEEE COMPUT GRAPH, V22, P90, DOI 10.1109/MCG.2002.1046633
   Weeks J. R., SNAPPEA
   Weeks JR, 2006, MATH APPL, V581, P287
   WEISSMANN S, 2009, ACM MULTIMEDIA 09, P00927
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
NR 22
TC 5
Z9 5
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2015
VL 31
IS 1
BP 93
EP 104
DI 10.1007/s00371-013-0913-2
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AY8ZQ
UT WOS:000347839600008
DA 2024-07-18
ER

PT J
AU Lan, RY
   Sun, HJ
AF Lan, Rongyi
   Sun, Huaijiang
TI Automated human motion segmentation via motion regularities
SO VISUAL COMPUTER
LA English
DT Article
DE Motion capture; Motion representation; Motion segmentation; Hierarchical
   clustering; Latent Dirichlet allocation; Topic mining
ID CAPTURE DATA; RECOGNITION
AB Analysis and reuse of human motion capture (mocap) data play an important role in animation, games and medical rehabilitation. In various mocap-based animation techniques, motion segmentation is regarded as one of the fundamental functions. Many proposed segmentation methods utilize little or no prior knowledge. However, human motion has its own regularities, so reasonable prior assumptions on these regularities will lead to better performance. In this paper, we focus on the learning of intrinsic regularities of mocap data based on a small set of training data which only contain daily-life motions. By utilizing these learnt motion regularities, we can successfully segment long motion sequences containing motion types that not even include in the training data. First, by assuming that most types of motions can be composed of a small number of typical poses, the motion vocabulary (mo-vocabulary) can be obtained using key pose extraction and clustering analysis, which are regarded as the low-level motion regularity. By replacing each frame with the most similar pose in the mo-vocabulary, mocap data can be transformed into text-like documents. Second, we use latent Dirichlet allocation to capture the patterns of pose combinations that frequently occur in human motions, namely the motion topics (mo-topics), which are regarded as the high-level motion regularities. By representing the target motion as the distribution over the learnt mo-topics, the segmentation task can be naturally turned into a problem of detecting notable changes of this distribution. Finally, we propose local semantic coherence curve to segment motion sequences. Since mo-topics are semantically meaningful and significantly increase the abstraction-level of motion representation, logically correct results can be obtained. The experiments demonstrate that the proposed approach outperforms the available methods on CMU and Bonn mocap database.
C1 [Lan, Rongyi; Sun, Huaijiang] Nanjing Univ Sci & Technol, Sch Comp Sci & Technol, Nanjing 210094, Jiangsu, Peoples R China.
C3 Nanjing University of Science & Technology
RP Sun, HJ (corresponding author), Nanjing Univ Sci & Technol, Sch Comp Sci & Technol, Nanjing 210094, Jiangsu, Peoples R China.
EM lanrongyi@njust.edu.cn; sunhuaijiang@gmail.com
FU Programme of Introducing Talents of Discipline to Universities [B13022];
   NUST Research Funding [2011YBXM79]
FX This research was supported by the Programme of Introducing Talents of
   Discipline to Universities (Grant No. B13022) and NUST Research Funding
   (Grant No. 2011YBXM79). We thank Zexuan Ji, Mingyang Zhu and Liang Zhou
   for their help and valuable suggestions. The mocap data used in this
   work was obtained from CMU Graphics Lab (mocap.cs.cmu.edu) and HDM05,
   Universitat Bonn [21].
CR [Anonymous], 2009, Proceedings of the 2009 Symposium on Interactive 3D Graphics and Games, I3D'09, DOI DOI 10.1145/1507149.1507181
   [Anonymous], 2002, Proceedings of the Eighteenth Conference on Uncertainty in Artificial Intelligence. UAI'02
   Arikan O, 2003, ACM T GRAPHIC, V22, P402, DOI 10.1145/882262.882284
   Barbic J, 2004, PROC GRAPH INTERF, P185
   Beaudoin P., 2008, P 2008 ACM SIGGRAPHE, P117
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Fod A, 2002, AUTON ROBOT, V12, P39, DOI 10.1023/A:1013254724861
   Griffiths TL, 2004, P NATL ACAD SCI USA, V101, P5228, DOI 10.1073/pnas.0307752101
   Heck R, 2007, I3D 2007: ACM SIGGRAPH SYMPOSIUM ON INTERACTIVE 3D GRAPHICS AND GAMES, PROCEEDINGS, P129
   Heidel A., 2007, Proceedings of INTERSPEECH, P2361
   Hu D.J., 2009, P INT C MUSIC INFORM, P441
   Kovar L, 2002, ACM T GRAPHIC, V21, P473, DOI 10.1145/566570.566605
   Kulic D, 2009, IEEE ENG MED BIO, P4011, DOI 10.1109/IEMBS.2009.5333502
   Li C, 2007, MULTIMED TOOLS APPL, V35, P55, DOI 10.1007/s11042-007-0119-6
   Lv F, 2006, LECT NOTES COMPUT SC, V3954, P359
   Min JY, 2009, ACM T GRAPHIC, V29, DOI 10.1145/1640443.1640452
   Misra H., 2008, P 12 C COMPUTATIONAL, P41, DOI 10.3115/1596324.1596332
   Misra H., 2009, P 18 ACM C INF KNOWL, P1553
   Muller M., 2007, Tech. Rep. CG-2007-2
   Rongyi Lan, 2013, Intelligent Science and Intelligent Data Engineering. Third Sino-foreign-interchange Workshop, IScIDE 2012. Revised Selected Papers, P72, DOI 10.1007/978-3-642-36669-7_10
   Shu-Juan Peng, 2010, Proceedings 2010 International Conference on Computational Intelligence and Security (CIS 2010), P223, DOI 10.1109/CIS.2010.54
   Sivic J., 2005, Discovering object categories in image collections
   Souvenir R, 2005, P 10 IEEE INT C COMP
   van Basten Ben JH, 2009, P 4 INT C FDN DIGITA, P199
   Wang TS, 2001, LECT NOTES COMPUT SC, V2195, P174
   Wu S., 2009, P 16 ACM S VIRTUAL R, P207
   Xiao J, 2010, MULTIMED TOOLS APPL, V47, P379, DOI 10.1007/s11042-009-0329-1
   Zhao L., 2009, P 17 ACM INT C MULT, P765
   Zhou F, 2013, IEEE T PATTERN ANAL, V35, P582, DOI 10.1109/TPAMI.2012.137
   Zhu MY, 2012, COMPUT ANIMAT VIRT W, V23, P469, DOI 10.1002/cav.432
NR 30
TC 26
Z9 27
U1 0
U2 15
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2015
VL 31
IS 1
BP 35
EP 53
DI 10.1007/s00371-013-0902-5
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AY8ZQ
UT WOS:000347839600004
DA 2024-07-18
ER

PT J
AU Zhang, P
   Liu, H
   Ding, YH
AF Zhang, Peng
   Liu, Hong
   Ding, Yan-hui
TI Crowd simulation based on constrained and controlled group formation
SO VISUAL COMPUTER
LA English
DT Article
DE Crowd motion; Freestyle formations; Artificial bee colony algorithm; 3D
   simulation
AB Freestyle formations appear widely in animation of groups. Most existing algorithms for generating special formations focus on the visualization performances of target formations, while social dynamics factors in the process of crowd motion are ignored. Thus, disregarding those factors will decrease the bionic features and fidelity of the crowd motion. According to this problem, a method based on bionic intelligence algorithm and self-adaptive evaluation to generate special formations is proposed in this paper. Simulation effect with good fluency and lively interaction is generated by means of user interaction, data analysis and crowd motion. In this method, 3D reconstruction is used to repaint characters, graphics or patterns in the 3D modeling system to build the basic virtual scene. Then, station points are generated through interlacing cross sampling. Based on the concentric circles model of fitness, each individual, self-adaptively, chooses a target station point which matches it aptly. Finally, the Artificial Bee Colony algorithm is used for path planing to generate the optimum route to the destination without collision. Visual simulation experiments are also made on the platforms of ACIS/HOOPS and Maya. The results show that this method can generate the optimum target formation with natural motion features and in accordance with users' input. This method is also insensitive to the scale of crowd, exhibiting good performance when the number of individuals is large.
C1 [Zhang, Peng; Liu, Hong; Ding, Yan-hui] Shandong Normal Univ, Sch Informat Sci & Engn, Jinan 250014, Peoples R China.
   [Zhang, Peng; Liu, Hong; Ding, Yan-hui] Shandong Prov Key Lab Distributed Comp Software N, Jinan 250358, Peoples R China.
C3 Shandong Normal University
RP Zhang, P (corresponding author), Shandong Normal Univ, Sch Informat Sci & Engn, Jinan 250014, Peoples R China.
EM zhpll@126.com
RI Liu, Hong/W-8431-2019
FU Natural Science Foundation of China [61272094, 61202225, 61303007,
   61303157]; Natural Science Foundation of Shandong Province [ZR2010QL01];
   Project of Shandong Province Higher Educational Science and Technology
   Program [J11LG32, J13LN13]
FX This research is supported by: the Natural Science Foundation of China
   under Grant Nos. 61272094, 61202225, 61303007, 61303157; the Natural
   Science Foundation of Shandong Province under Grant No. ZR2010QL01; and
   the Project of Shandong Province Higher Educational Science and
   Technology Program under Grant Nos. J11LG32, J13LN13.
CR Banarjee S., 2005, 7 INT S IEEE SYMB NU
   Bao L, 2009, HIS 2009: 2009 NINTH INTERNATIONAL CONFERENCE ON HYBRID INTELLIGENT SYSTEMS, VOL 1, PROCEEDINGS, P411, DOI 10.1109/HIS.2009.319
   Bo Y, 2007, CIS: 2007 INTERNATIONAL CONFERENCE ON COMPUTATIONAL INTELLIGENCE AND SECURITY, PROCEEDINGS, P296, DOI 10.1109/CIS.2007.209
   Braun A, 2003, COMP ANIM CONF PROC, P143, DOI 10.1109/CASA.2003.1199317
   Chen YP, 2009, APPL SOFT COMPUT, V9, P1170, DOI 10.1016/j.asoc.2009.03.004
   Gu Q., 2011, Graphics Interface 2011, P266, DOI DOI 10.5555/1992917.1992919
   Gu Q, 2013, IEEE COMPUT GRAPH, V33, P20, DOI 10.1109/MCG.2011.87
   Helbing D, 2000, NATURE, V407, P487, DOI 10.1038/35035023
   Ioannidis K, 2011, ROBOT AUTON SYST, V59, P113, DOI 10.1016/j.robot.2010.10.004
   Karaboga D, 2008, APPL SOFT COMPUT, V8, P687, DOI 10.1016/j.asoc.2007.05.007
   Karaboga D., 2005, IDEA BASED HONEY BEE
   Karaboga D, 2007, J GLOBAL OPTIM, V39, P459, DOI 10.1007/s10898-007-9149-x
   Li Jian, 2010, Journal of Computer Aided Design & Computer Graphics, V22, P1158, DOI 10.3724/SP.J.1089.2010.10928
   Li Z., 2008, J SYST SIMUL, V20, P195
   Luo LB, 2008, COMPUT ANIMAT VIRT W, V19, P271, DOI 10.1002/cav.238
   Paris S, 2006, COMPUT ANIMAT VIRT W, V17, P325, DOI 10.1002/cav.136
   Pelechano N, 2007, SYMPOSIUM ON COMPUTER ANIMATION 2007: ACM SIGGRAPH/ EUROGRAPHICS SYMPOSIUM PROCEEDINGS, P99
   Pelechano Nuria., 2005, Crowd Simulation Incorporating Agent Psychological Models, Roles and Communication
   Silveira R, 2008, COMPUT ANIMAT VIRT W, V19, P295, DOI 10.1002/cav.261
   Sun Yu-ling, 2011, Computer Engineering, V37, P131, DOI 10.3969/j.issn.1000-3428.2011.22.042
   Takahashi S, 2009, COMPUT GRAPH FORUM, V28, P639, DOI 10.1111/j.1467-8659.2009.01404.x
   Thalmann Daniel., 2007, CROWD SIMULATION
   Xiangwei Zheng, 2011, Proceedings of the 2011 15th International Conference on Computer Supported Cooperative Work in Design (CSCWD), P168, DOI 10.1109/CSCWD.2011.5960071
   Yang L., 2000, SCI CHINA SER E, V34, P487
   Zhao Shu Ying, 2007, Journal of Chinese Computer Systems, V28, P2220
   Zhao Wei, 2010, Journal of System Simulation, V22, P125
NR 26
TC 28
Z9 30
U1 1
U2 25
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2015
VL 31
IS 1
BP 5
EP 18
DI 10.1007/s00371-013-0900-7
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AY8ZQ
UT WOS:000347839600002
DA 2024-07-18
ER

PT J
AU Zang, Y
   Huang, H
   Li, CF
AF Zang, Yu
   Huang, Hua
   Li, Chen-Feng
TI Artistic preprocessing for painterly rendering and image stylization
SO VISUAL COMPUTER
LA English
DT Article
DE Image processing; Example based painting; Color features learning
ID VIDEO; ABSTRACTION; ATTENTION
AB A practical image enhancing technique is presented as a preprocessing step for painterly rendering and image stylization, which transforms the input image mimicking the vision of artists. The new method contains mainly two parts dealing with artistic enhancement and color adjustment, respectively. First, through feature extraction and simplification, an abstract shadow map is constructed for the input image, which is then taken as a guide for emphasizing the light-shadow contrast and the important shadow lines. Next, to simulate the intense color emotion often subjectively added by the artist, a color adjustment technique is proposed to generate lively colors with sharp contrast imitating the artistic vision. The preprocessing operation is compatible with existing stylization and stroke-based painterly rendering techniques, and it can also produce different types of stylization results independently.
C1 [Zang, Yu] Xi An Jiao Tong Univ, Sch Elect Informat Engn, Xian, Shaanxi, Peoples R China.
   [Huang, Hua] Beijing Inst Technol, Sch Comp Sci & Technol, Beijing Key Lab Intelligent Informat Technol, Beijing 100081, Peoples R China.
   [Li, Chen-Feng] Swansea Univ, Coll Engn, Swansea, W Glam, Wales.
C3 Xi'an Jiaotong University; Beijing Institute of Technology; Swansea
   University
RP Huang, H (corresponding author), Beijing Inst Technol, Sch Comp Sci & Technol, Beijing Key Lab Intelligent Informat Technol, Beijing 100081, Peoples R China.
EM zangyu7@126.com; huanghua_xjtu@hotmail.com; c.f.li@swansea.ac.uk
RI Li, Mengqi/AAG-6804-2021; Li, Chenfeng/D-4034-2014; Li,
   Chenfeng/AFQ-6554-2022; Huang, Hua/M-9684-2013
OI Li, Chenfeng/0000-0003-0441-211X; Huang, Hua/0000-0003-2587-1702
FU Tsinghua-Tencent Joint Laboratory for Internet Innovation Technology
   [2012-01]; National Natural Science Foundation of China [60970068];
   Royal Society of UK [JP100987]
FX This work is supported by Tsinghua-Tencent Joint Laboratory for Internet
   Innovation Technology under Grant No. 2012-01 and the National Natural
   Science Foundation of China (No. 60970068). The authors would also like
   to thank the support from the International Joint Project from the Royal
   Society of UK (No. JP100987.)
CR [Anonymous], AUTOMATICA
   [Anonymous], VIS COMPUT
   Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114
   Cheng MM, 2011, PROC CVPR IEEE, P409, DOI 10.1109/CVPR.2011.5995344
   Collomosse JP, 2005, IEEE T VIS COMPUT GR, V11, P540, DOI 10.1109/TVCG.2005.85
   Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236
   Cong L, 2011, VISUAL COMPUT, V27, P187, DOI 10.1007/s00371-010-0522-2
   DeCarlo D, 2002, ACM T GRAPHIC, V21, P769, DOI 10.1145/566570.566650
   Gooch B, 2004, ACM T GRAPHIC, V23, P27, DOI 10.1145/966131.966133
   Hays J., 2004, PROC NPAR 01, P113
   Hertzmann A., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P453, DOI 10.1145/280814.280951
   Hertzmann A., 2010, Proc. NPAR '10, P147
   Hu SM, 2013, VISUAL COMPUT, V29, P393, DOI 10.1007/s00371-013-0792-6
   Huang H, 2010, COMPUT GRAPH FORUM, V29, P2055, DOI 10.1111/j.1467-8659.2010.01792.x
   Huang YZ, 2009, SCI CHINA SER F, V52, P295, DOI 10.1007/s11432-009-0039-3
   Hulusic V, 2013, VISUAL COMPUT, V29, P1159, DOI 10.1007/s00371-012-0760-6
   Kagaya M, 2011, IEEE T VIS COMPUT GR, V17, P74, DOI 10.1109/TVCG.2010.25
   Kang H, 2007, NPAR 2007: 5TH INTERNATIONAL SYMPOSIUM ON NON-PHOTOREALISTIC ANIMATION AND RENDERING, PROCEEDINGS, P43
   Kang H, 2009, IEEE T VIS COMPUT GR, V15, P62, DOI 10.1109/TVCG.2008.81
   Kyprianidis JE, 2013, IEEE T VIS COMPUT GR, V19, P866, DOI 10.1109/TVCG.2012.160
   Kyprianidis JE, 2011, COMPUT GRAPH FORUM, V30, P593, DOI 10.1111/j.1467-8659.2011.01882.x
   Lee H, 2009, COMPUT GRAPH FORUM, V28, P1207, DOI 10.1111/j.1467-8659.2009.01498.x
   Li P, 2012, SCI CHINA INFORM SCI, V55, P1093, DOI 10.1007/s11432-012-4558-y
   Liao PS, 2001, J INF SCI ENG, V17, P713
   Litwinowicz P., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P407, DOI 10.1145/258734.258893
   Liu LG, 2010, COMPUT GRAPH FORUM, V29, P469, DOI 10.1111/j.1467-8659.2009.01616.x
   Liu YJ, 2011, COMPUT GRAPH FORUM, V30, P583, DOI 10.1111/j.1467-8659.2011.01881.x
   Mould D., 2008, P 6 INT S NONPH AN R, P49, DOI DOI 10.1145/1377980.1377991
   O'Donovan P, 2012, IEEE T VIS COMPUT GR, V18, P475, DOI 10.1109/TVCG.2011.51
   Raskar R, 2004, ACM T GRAPHIC, V23, P679, DOI 10.1145/1015706.1015779
   Sun YR, 2003, ARTIF INTELL, V146, P77, DOI 10.1016/S0004-3702(02)00399-5
   Torralba A, 2006, PSYCHOL REV, V113, P766, DOI 10.1037/0033-295X.113.4.766
   TSAI WH, 1985, COMPUT VISION GRAPH, V29, P377, DOI 10.1016/0734-189X(85)90133-1
   TURNER MR, 1986, BIOL CYBERN, V55, P71
   Wang J, 2010, S CHINA J PREV MED, V36, P26
   Wen F., 2006, PROC INT S NONPHOTOR, P47
   Winnemöller H, 2006, ACM T GRAPHIC, V25, P1221, DOI 10.1145/1141911.1142018
   Wong FJ, 2013, VISUAL COMPUT, V29, P729, DOI 10.1007/s00371-013-0809-1
   Xiao XZ, 2009, COMPUT GRAPH FORUM, V28, P1879, DOI 10.1111/j.1467-8659.2009.01566.x
   Xu Jie., 2008, Proc. NPAR, P39
   Zeng K, 2009, ACM T GRAPHIC, V29, DOI 10.1145/1640443.1640445
   Zhang SH, 2011, IEEE T MULTIMEDIA, V13, P1286, DOI 10.1109/TMM.2011.2165052
   Zhang SH, 2009, SCI CHINA SER F, V52, P162, DOI 10.1007/s11432-009-0035-7
   Zhao M., 2010, PROC INT S NONPHOTOR, P99, DOI DOI 10.1145/1809939.1809951
NR 44
TC 12
Z9 14
U1 2
U2 23
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2014
VL 30
IS 9
BP 969
EP 979
DI 10.1007/s00371-013-0881-6
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AO5GP
UT WOS:000341372200002
DA 2024-07-18
ER

PT J
AU Altantsetseg, E
   Matsuyama, K
   Konno, K
AF Altantsetseg, Enkhbayar
   Matsuyama, Katsutsugu
   Konno, Kouichi
TI Pairwise matching of 3D fragments using fast fourier transform
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 31st CGI conference
CY JUN 10-13, 2014
CL Sydney, AUSTRALIA
DE Pairwise matching; Feature points; FFT; Point clouds
ID RECONSTRUCTION; OBJECTS
AB In this paper, we introduce a new method for pairwise matching of broken fragments from unorganized point clouds. We use a new descriptor that contains not only the cluster of feature points but also curves along the principal directions of the cluster. In our method, feature points are extracted by using the curvature values of points. Curves of the descriptor are approximated using Fourier series. The main idea is motivated by comparing descriptor curves between each cluster of matching faces. For comparing curves, the Fourier coefficients of each curve are computed by using Fast Fourier Transform and total energies of curves are compared.
C1 [Altantsetseg, Enkhbayar; Matsuyama, Katsutsugu; Konno, Kouichi] Iwate Univ, Morioka, Iwate 020, Japan.
C3 Iwate University
RP Altantsetseg, E (corresponding author), Iwate Univ, Morioka, Iwate 020, Japan.
EM bayar@lk.cis.iwate-u.ac.jp
FU JSPS KAKENHI [24501253]; Grants-in-Aid for Scientific Research
   [24501253] Funding Source: KAKEN
FX 3D brick models are courtesy by Vienna University of Technology. This
   work was supported by JSPS KAKENHI Grant Number 24501253.
CR Altantsetseg E, 2013, VISUAL COMPUT, V29, P617, DOI 10.1007/s00371-013-0800-x
   [Anonymous], S GEOM PROC
   [Anonymous], ACM T GRAPH
   Barnsley M.F, 1988, The Science of Fractal Images
   Brigham E. O., 1988, FAST FOURIER TRANSFO
   Brown BJ, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360683
   Cohen F, 2013, COMPUT GRAPH-UK, V37, P41, DOI 10.1016/j.cag.2012.11.001
   Dong S, 2006, ACM T GRAPHIC, V25, P1057, DOI 10.1145/1141911.1141993
   Gal R, 2006, ACM T GRAPHIC, V25, P130, DOI 10.1145/1122501.1122507
   Gelfand N., 2005, P 3 EUR S GEOM PROC, V2, P5
   Gökberk B, 2008, IEEE T SYST MAN CY B, V38, P155, DOI 10.1109/TSMCB.2007.908865
   Huang QX, 2008, COMPUT GRAPH FORUM, V27, P1449, DOI 10.1111/j.1467-8659.2008.01285.x
   Huang QX, 2006, ACM T GRAPHIC, V25, P569, DOI 10.1145/1141911.1141925
   Kang U, 2013, INFORM SCIENCES, V220, P306, DOI 10.1016/j.ins.2012.07.008
   KATCHALSKIKATZIR E, 1992, P NATL ACAD SCI USA, V89, P2195, DOI 10.1073/pnas.89.6.2195
   Koller David, 2006, Bullettino Della Commissione Archaeologica Comunale di Roma, V2, P103
   Kong WX, 2001, PROC CVPR IEEE, P583
   Leitao HCD, 2002, IEEE T PATTERN ANAL, V24, P1239, DOI 10.1109/TPAMI.2002.1033215
   Leordeanu M, 2005, IEEE I CONF COMP VIS, P1482
   Li QH, 2012, INT SYM COMPUT INTEL, P511, DOI 10.1109/ISCID.2012.268
   Lucchese L, 2002, IEEE T PATTERN ANAL, V24, P1468, DOI 10.1109/TPAMI.2002.1046160
   McBride JC, 2003, 2003 C COMP VIS PATT, V1, P1
   Oxholm G, 2013, J CULT HERIT, V14, P51, DOI 10.1016/j.culher.2012.02.017
   Papaioannou G, 2003, IMAGE VISION COMPUT, V21, P401, DOI 10.1016/S0262-8856(03)00008-8
   Papaioannou G, 2002, IEEE T PATTERN ANAL, V24, P114, DOI 10.1109/34.982888
   Papaodysseus C, 2012, COMPUT MATH APPL, V64, P2712, DOI 10.1016/j.camwa.2012.08.003
   Parikh D., 2007, P IEEE WORKSH APPL C
   Pauly M, 2001, COMP GRAPH, P379, DOI 10.1145/383259.383301
   Willis A, 2004, INT C PATT RECOG, P96, DOI 10.1109/ICPR.2004.1333714
   Willis AR, 2008, IEEE SIGNAL PROC MAG, V25, P65, DOI 10.1109/MSP.2008.923101
   Winkelbach S, 2008, INT J COMPUT VISION, V78, P1, DOI 10.1007/s11263-007-0121-5
   Zheng Q, 2010, COMPUT GRAPH FORUM, V29, P635, DOI 10.1111/j.1467-8659.2009.01633.x
NR 32
TC 19
Z9 30
U1 0
U2 21
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2014
VL 30
IS 6-8
BP 929
EP 938
DI 10.1007/s00371-014-0959-9
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA AI7GX
UT WOS:000337054700034
DA 2024-07-18
ER

PT J
AU Pintore, G
   Gobbetti, E
AF Pintore, Giovanni
   Gobbetti, Enrico
TI Effective mobile mapping of multi-room indoor structures
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 31st CGI conference
CY JUN 10-13, 2014
CL Sydney, AUSTRALIA
DE Sensors fusion; Mobile graphics; Mobile mapping; Scene analysis; Indoor
   reconstruction
AB We present a system to easily capture building interiors and automatically generate floor plans scaled to their metric dimensions. The proposed approach is able to manage scenes not necessarily limited to the Manhattan World assumption, exploiting the redundancy of the instruments commonly available on commodity smartphones, such as accelerometer, magnetometer and camera. Without specialized training or equipment, our system can produce a 2D floor plan and a representative 3D model of the scene accurate enough to be used for simulations and interactive applications.
C1 [Pintore, Giovanni; Gobbetti, Enrico] CRS4 Visual Comp, I-09010 Pula, CA, Italy.
RP Gobbetti, E (corresponding author), CRS4 Visual Comp, POLARIS Ed 1, I-09010 Pula, CA, Italy.
EM giovanni.pintore@crs4.it; enrico.gobbetti@crs4.it
RI Gobbetti, Enrico/O-2188-2015; Pintore, Giovanni/AFV-0023-2022
OI Gobbetti, Enrico/0000-0003-0831-2458; Pintore,
   Giovanni/0000-0001-8944-1045
FU EU FP7 grant [607737]
FX This research is partially supported by EU FP7 grant 607737 (VASCO). We
   also acknowledge the contribution of Sardinian Regional Authorities.
CR [Anonymous], P 13 INT C COMP AID
   [Anonymous], 2011, IMAGE PROCESSING LIN
   [Anonymous], 2009, P ICCV
   Arikan M, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2421636.2421642
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851
   Capel D., 2005, Proc. BMVC, P629
   Cornelis N, 2008, INT J COMPUT VISION, V78, P121, DOI 10.1007/s11263-007-0081-9
   Coughlan J. M., 1999, P IEEE INT C COMPUTE, DOI DOI 10.1109/ICCV.1999.790349
   Debevec P. E., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P11, DOI 10.1145/237170.237191
   Di Benedetto M., 2014, COMPUTER GRAPHICS FO, V33
   ElHakim SF, 1997, P SOC PHOTO-OPT INS, V3174, P21, DOI 10.1117/12.279791
   Frueh C, 2005, INT J COMPUT VISION, V61, P159, DOI 10.1023/B:VISI.0000043756.03810.dd
   Guru DS, 2004, PATTERN RECOGN, V37, P165, DOI 10.1016/S0031-3203(03)00234-6
   Kim YM, 2012, IEEE INT CONF ROBOT, P3055, DOI 10.1109/ICRA.2012.6224595
   Matas J, 2000, COMPUT VIS IMAGE UND, V78, P119, DOI 10.1006/cviu.1999.0831
   Müller P, 2006, ACM T GRAPHIC, V25, P614, DOI 10.1145/1141911.1141931
   Mura C., 2014, P EUR EUR ASS
   Pollefeys M, 2008, INT J COMPUT VISION, V78, P143, DOI 10.1007/s11263-007-0086-4
   Sankar A, 2012, UIST'12: PROCEEDINGS OF THE 25TH ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P403
   Seitz S. M., 2006, 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06), V1, P519
   SHI JB, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P593, DOI 10.1109/CVPR.1994.323794
   Shin H, 2012, IEEE T SYST MAN CY C, V42, P889, DOI 10.1109/TSMCC.2011.2169403
   Sinha SN, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409112
   Snavely N, 2006, ACM T GRAPHIC, V25, P835, DOI 10.1145/1141911.1141964
   Stamos I, 2007, THIRD INTERNATIONAL SYMPOSIUM ON 3D DATA PROCESSING, VISUALIZATION, AND TRANSMISSION, PROCEEDINGS, P599
   Xiao JX, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409114
   Zebedin L, 2008, LECT NOTES COMPUT SC, V5305, P873, DOI 10.1007/978-3-540-88693-8_64
NR 27
TC 11
Z9 14
U1 0
U2 11
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2014
VL 30
IS 6-8
BP 707
EP 716
DI 10.1007/s00371-014-0947-0
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA AI7GX
UT WOS:000337054700014
DA 2024-07-18
ER

PT J
AU Taranta, EM
   Pattanaik, SN
AF Taranta, Eugene M., II
   Pattanaik, Sumanta N.
TI Macro 64-regions for uniform grids on GPU
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 31st CGI conference
CY JUN 10-13, 2014
CL Sydney, AUSTRALIA
DE Grid; Accelerator; Ray tracing; GPU
ID SCENES
AB Uniform grids are a spatial subdivision acceleration structure well suited for ray tracing. They are known for their fast build times and ease of use, but suffer from slow traversals in the presence of empty space. To address this issue, we present macro 64-regions, a new GPU based approach for finding and storing empty volumes in an underlying uniform grid. This allows for fast traversals through regions that do not contain scene geometry. Further, unlike previous solutions to this problem, we do not store a hierarchical structure and therefore the traversal steps are simplified. Because macro 64-regions are dependent on an underlying grid, we also introduce an improvement in the grid construction process. Our method does not rely on sorting as previous methods do, but instead uses atomic operators to manage bookkeeping during the build. Using our proposed methods, we show a substantial improvement in build time, trace time, as well as an improvement in the consistency of rendering times for randomly generated views.
C1 [Taranta, Eugene M., II; Pattanaik, Sumanta N.] Univ Cent Florida, Orlando, FL 32816 USA.
C3 State University System of Florida; University of Central Florida
RP Taranta, EM (corresponding author), Univ Cent Florida, Orlando, FL 32816 USA.
EM etaranta@gmail.com; sumant@cs.ucf.edu
FU NSF [IIS-1064427]; Div Of Information & Intelligent Systems; Direct For
   Computer & Info Scie & Enginr [1064427] Funding Source: National Science
   Foundation
FX We acknowledge NSF (Award Number IIS-1064427) for providing partial
   funding for this research. We also wish to thank Anat Grynberg and Greg
   Ward for the Conference Room model, Frank Meinl and Marko Dabrovic for
   the Crytek Sponza model, and Marko Dabrovic, again, for the Sibenik
   Cathedral model.
CR Akenine-Moller Tomas., 2005, ACM SIGGRAPH 2005 Courses, P8, DOI DOI 10.1145/1198555.1198747
   Amanatides John, 1987, P EUROGRAPHICS, P3
   [Anonymous], 2010, Thrust: A parallel template library
   Cleary J. G., 1983, GRAPH INT 83, P33
   Devillers O., 1989, EUROGRAPHICS '89. Proceedings of the European Computer Graphics Conference, P27
   FUJIMOTO A, 1986, IEEE COMPUT GRAPH, V6, P16, DOI 10.1109/MCG.1986.276715
   Garanzha K., 2011, P ACM SIGGRAPH S HIG, P59, DOI DOI 10.1145/2018323.2018333
   Guntury S, 2012, IEEE T VIS COMPUT GR, V18, P5, DOI 10.1109/TVCG.2011.46
   Jevans D., 1989, Proceedings. Graphics Interface'89, P164
   Kalojanov J., 2009, Proceedings of the 1st ACM conference on High Performance Graphics - HPG '09 (New York, New York, USA, 2009), P23
   Kalojanov J, 2011, COMPUT GRAPH FORUM, V30, P307, DOI 10.1111/j.1467-8659.2011.01862.x
   Klimaszewski KS, 1997, IEEE COMPUT GRAPH, V17, P42, DOI 10.1109/38.576857
   Lauterbach C, 2009, COMPUT GRAPH FORUM, V28, P375, DOI 10.1111/j.1467-8659.2009.01377.x
   Liu X., 2013, P 2013 GRAPH INT C G, P165
   Nickolls John, 2008, ACM Queue, V6, DOI 10.1145/1365490.1365500
   Pharr M., 2010, PHYS BASED RENDERING
   Santos A, 2012, INT J PARALLEL PROG, V40, P331, DOI 10.1007/s10766-011-0186-1
   Sengupta S, 2007, GRAPHICS HARDWARE 2007: ACM SIGGRAPH / EUROGRAPHICS SYMPOSIUM PROCEEDINGS, P97
   Snyder JohnM., 1987, P 14 ANN C COMPUTER, P119
   Wald I, 2006, ACM T GRAPHIC, V25, P485, DOI 10.1145/1141911.1141913
   Wald I, 2009, COMPUT GRAPH FORUM, V28, P1691, DOI 10.1111/j.1467-8659.2008.01313.x
   Zhou K, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409079
NR 22
TC 1
Z9 1
U1 0
U2 4
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2014
VL 30
IS 6-8
BP 615
EP 624
DI 10.1007/s00371-014-0974-x
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA AI7GX
UT WOS:000337054700006
DA 2024-07-18
ER

PT J
AU Cheng, MM
   Mitra, NJ
   Huang, XL
   Hu, SM
AF Cheng, Ming-Ming
   Mitra, Niloy J.
   Huang, Xiaolei
   Hu, Shi-Min
TI SalientShape: group saliency in image collections
SO VISUAL COMPUTER
LA English
DT Article
DE Saliency detection; Group saliency; Object of interest segmentation;
   Image retrieval
ID RETRIEVAL; RECOGNITION; EXTRACTION; ATTENTION; COLLAGE
AB Efficiently identifying salient objects in large image collections is essential for many applications including image retrieval, surveillance, image annotation, and object recognition. We propose a simple, fast, and effective algorithm for locating and segmenting salient objects by analysing image collections. As a key novelty, we introduce group saliency to achieve superior unsupervised salient object segmentation by extracting salient objects (in collections of pre-filtered images) that maximize between-image similarities and within-image distinctness. To evaluate our method, we construct a large benchmark dataset consisting of 15 K images across multiple categories with 6000+ pixel-accurate ground truth annotations for salient object regions where applicable. In all our tests, group saliency consistently outperforms state-of-the-art single-image saliency algorithms, resulting in both higher precision and better recall. Our algorithm successfully handles image collections, of an order larger than any existing benchmark datasets, consisting of diverse and heterogeneous images from various internet sources.
C1 [Cheng, Ming-Ming; Hu, Shi-Min] Tsinghua Univ, TNList, Beijing 100084, Peoples R China.
   [Mitra, Niloy J.] UCL, London, England.
   [Huang, Xiaolei] Lehigh Univ, Bethlehem, PA 18015 USA.
C3 Tsinghua University; University of London; University College London;
   Lehigh University
RP Cheng, MM (corresponding author), Tsinghua Univ, TNList, Beijing 100084, Peoples R China.
EM cmm.thu@gmail.com
RI Cheng, Ming-Ming/A-2527-2009; Hu, Shi-Min/AAW-1952-2020
OI Cheng, Ming-Ming/0000-0001-5550-8758; Huang, Sharon
   Xiaolei/0000-0003-2338-6535
FU 973 Program [2011CB302205]; 863 Program [2009AA01Z327]; Key Project of
   ST [2011ZX01042-001-002]; NSFC [U0735001]; Google Ph.D. fellowship; IBM
   Ph.D. fellowship; New Ph.D. Researcher Award (Ministry of Edu., CN);
   EPSRC [EP/I001107/1, EP/I001107/2] Funding Source: UKRI
FX We would like to thank the anonymous reviewers for their constructive
   comments. This research was supported by the 973 Program (2011CB302205),
   the 863 Program (2009AA01Z327), the Key Project of S&T
   (2011ZX01042-001-002), and NSFC (U0735001). Ming-Ming Cheng was funded
   by Google Ph.D. fellowship, IBM Ph.D. fellowship, and New Ph.D.
   Researcher Award (Ministry of Edu., CN).
CR Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   [Anonymous], 2006, 2006 C COMPUTER VISI
   Belongie S, 2002, IEEE T PATTERN ANAL, V24, P509, DOI 10.1109/34.993558
   Biswas S, 2010, IEEE T MULTIMEDIA, V12, P372, DOI 10.1109/TMM.2010.2050735
   Bouet M., 1999, ACM MM, P1
   Cao Y, 2011, PROC CVPR IEEE, P761, DOI 10.1109/CVPR.2011.5995460
   Cao Yang, 2010, P 18 ACM INT C MULT, P1605
   Charpiat G., 2007, P 2007 IEEE COMPUTER, P1
   Chen HT, 2010, IEEE IMAGE PROC, P1117, DOI 10.1109/ICIP.2010.5650014
   Chen T, 2013, IEEE T VIS COMPUT GR, V19, P824, DOI 10.1109/TVCG.2012.148
   Chen TC, 2009, PROC EUR SOLID-STATE, P1
   Cheng M.M., 2011, TPAMI2011100753 TSIN
   Cheng MM, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778820
   Chia AYS, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024190
   Cui J., 2008, MM 08, P729
   Datta R, 2008, ACM COMPUT SURV, V40, DOI 10.1145/1348246.1348248
   DelBimbo A, 1997, IEEE T PATTERN ANAL, V19, P121, DOI 10.1109/34.574790
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Eitz M, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185540
   Eitz M, 2011, IEEE T VIS COMPUT GR, V17, P1624, DOI 10.1109/TVCG.2010.266
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Felzenszwalb PF, 2004, INT J COMPUT VISION, V59, P167, DOI 10.1023/B:VISI.0000022288.19776.77
   Fergus R, 2004, LECT NOTES COMPUT SC, V3021, P242
   FLICKNER M, 1995, COMPUTER, V28, P23, DOI 10.1109/2.410146
   Gao Y, 2012, IEEE T IMAGE PROCESS, V21, P4290, DOI 10.1109/TIP.2012.2199502
   Gao Y, 2011, IEEE T MULTIMEDIA, V13, P1007, DOI 10.1109/TMM.2011.2160619
   Han JW, 2006, IEEE T CIRC SYST VID, V16, P141, DOI 10.1109/TCSVT.2005.859028
   He JF, 2012, PROC CVPR IEEE, P3005, DOI 10.1109/CVPR.2012.6248030
   HIRATA K, 1992, LECT NOTES COMPUT SC, V580, P56, DOI 10.1007/BFb0032423
   Hu SM, 2013, VISUAL COMPUT, V29, P393, DOI 10.1007/s00371-013-0792-6
   Huang H, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024189
   Jones MJ, 2002, INT J COMPUT VISION, V46, P81, DOI 10.1023/A:1013200319198
   Kai-Yueh Chang, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2129, DOI 10.1109/CVPR.2011.5995415
   Ko BC, 2006, J OPT SOC AM A, V23, P2462, DOI 10.1364/JOSAA.23.002462
   Kuettel D, 2012, LECT NOTES COMPUT SC, V7578, P459, DOI 10.1007/978-3-642-33786-4_34
   Kuettel D, 2012, PROC CVPR IEEE, P558, DOI 10.1109/CVPR.2012.6247721
   Li HL, 2011, IEEE T IMAGE PROCESS, V20, P3365, DOI 10.1109/TIP.2011.2156803
   Liu H, 2012, VISUAL COMPUT, V28, P279, DOI 10.1007/s00371-011-0638-z
   Margolin R, 2013, VISUAL COMPUT, V29, P381, DOI 10.1007/s00371-012-0740-x
   Min Chen, 2011, IEEE INFOCOM 2011 - IEEE Conference on Computer Communications. Workshops, P409, DOI 10.1109/INFCOMW.2011.5928847
   Mori G, 2001, PROC CVPR IEEE, P723
   Peter A., 2008, IEEE C COMPUTER VISI, P1
   Popescu A., 2009, ACM MULTIMEDIA, P657
   Rahtu E, 2010, LECT NOTES COMPUT SC, V6315, P366, DOI 10.1007/978-3-642-15555-0_27
   Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720
   Rui Hu, 2011, 2011 18th IEEE International Conference on Image Processing (ICIP 2011), P3661, DOI 10.1109/ICIP.2011.6116513
   Schmid J, 2011, VISUAL COMPUT, V27, P85, DOI 10.1007/s00371-010-0532-0
   Yang XW, 2009, PROC CVPR IEEE, P357, DOI 10.1109/CVPRW.2009.5206844
   Zhang DS, 2004, PATTERN RECOGN, V37, P1, DOI 10.1016/j.patcog.2003.07.008
   Zhang DS, 2002, SIGNAL PROCESS-IMAGE, V17, P825, DOI 10.1016/S0923-5965(02)00084-X
   Zhang GX, 2009, COMPUT GRAPH FORUM, V28, P1897, DOI 10.1111/j.1467-8659.2009.01568.x
   Zhang JY, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2167076.2167079
   Zhang L, 2012, COMPUT GRAPH FORUM, V31, P2173, DOI 10.1111/j.1467-8659.2012.03210.x
NR 53
TC 228
Z9 253
U1 0
U2 31
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2014
VL 30
IS 4
BP 443
EP 453
DI 10.1007/s00371-013-0867-4
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AD3SW
UT WOS:000333167500007
DA 2024-07-18
ER

PT J
AU Olbrich, M
   Graf, H
   Kahn, S
   Engelke, T
   Keil, J
   Riess, P
   Webel, S
   Bockholt, U
   Picinbono, G
AF Olbrich, Manuel
   Graf, Holger
   Kahn, Svenja
   Engelke, Timo
   Keil, Jens
   Riess, Patrick
   Webel, Sabine
   Bockholt, Ulrich
   Picinbono, Guillaume
TI Augmented reality supporting user-centric building information
   management
SO VISUAL COMPUTER
LA English
DT Article
DE Mobile augmented reality; Building information management; Sensor
   fusion; Markerless tracking
ID ARCHITECTURE
AB The rapid development of geo-referenced information changed the way on how we access and interlink data. Smartphones as enabling devices for information access are main driving factor. Thus, the hash key to information is the actual position registered via camera and sensory of the mobile device. A rising technology in this context is Augmented Reality (AR) as its fuses the real world captured with the smartphone camera with geo-referenced data. The technological building blocks analyse the intrinsic sensor data (camera, GPS, inertial) to derive a detailed pose of the smartphone aiming to align geo-referenced information to our real environment. In particular, this is interesting to applications where 3D models are used in planning and organization processes as, e.g., facility management. Here, Building Information Models (BIM) were established in order to hold "as built" information, but also to manage the vast amount of additional information coming with the design, such as building components, properties, maintenance logs, documentation, etc. One challenge is to enable stakeholders involved in the overall building lifecycle to get mobile access to the management system within on-site inspections and to automatise feedback of newly generated information into the BIM. This paper describes a new AR framework that offers on-site access to BIM information and user centric annotation mechanism.
C1 [Olbrich, Manuel; Graf, Holger; Kahn, Svenja; Engelke, Timo; Keil, Jens; Riess, Patrick; Webel, Sabine; Bockholt, Ulrich] Fraunhofer Inst Comp Graph Res IGD, D-64285 Darmstadt, Germany.
   [Picinbono, Guillaume] CSTB, Informat Technol & Knowledge Management Dept, Sophia Antipolis, France.
C3 Fraunhofer Gesellschaft
RP Olbrich, M (corresponding author), Fraunhofer Inst Comp Graph Res IGD, Fraunhoferstr 5, D-64285 Darmstadt, Germany.
EM manuel.olbrich@igd.fraunhofer.de; holger.graf@igd.fraunhofer.de;
   svenja.kahn@igd.fraunhofer.de; timo.engelke@igd.fraunhofer.de;
   jens.keil@igd.fraunhofer.de; patrick.riess@igd.fraunhofer.de;
   sabine.webel@igd.fraunhofer.de; ulrich.bockholt@igd.fraunhofer.de;
   guillaume.picinbono@cstb.fr
OI Graf, Holger/0000-0001-6977-028X
FU Inter Carnot Fraunhofer project LifeBC
FX This work was funded by the Inter Carnot Fraunhofer project LifeBC.
CR [Anonymous], COLLABORATIVE CONSTR
   Arth C, 2009, INT SYM MIX AUGMENT, P73, DOI 10.1109/ISMAR.2009.5336494
   BECKER M, 2007, 3DTV C, P1, DOI DOI 10.1109/3DTV.2007.4379440
   Behzadan AH, 2005, PROCEEDINGS OF THE 2005 WINTER SIMULATION CONFERENCE, VOLS 1-4, P1914, DOI 10.1109/WSC.2005.1574469
   Billinghurst M, 2000, 2000 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, PROCEEDINGS VOLS I-III, P1641, DOI 10.1109/ICME.2000.871085
   Bleser G, 2009, THESIS TU KAISERSLAU
   Bleser Gabriele., 2006, INT S MIXED AUGMENTE, P56
   Dong ZL, 2009, IEEE I CONF COMP VIS, P1538
   Dunston P.S., 2009, Mixed Reality In Architecture, Design And Construction, P157
   Dunston PS, 2005, J CONSTR ENG M, V131, P1301, DOI 10.1061/(ASCE)0733-9364(2005)131:12(1301)
   Eastman C.M., 2008, BIM Handbook: A Guide to Building Information Modeling for Owners, Managers, Designers, Engineers and Contractors
   El-Tawil S., 2006, RAPID RECONNAISSANCE, P1, DOI [10.1061/40878(202)2, DOI 10.1061/40878(202)2]
   Franke T., 2011, Proceedings of the 16th International Conference on 3D Web Technology, P71, DOI DOI 10.1145/2010425.2010439
   Graf H, 2011, SYMPOSIUM ON SIMULATION FOR ARCHITECTURE AND URBAN DESIGN 2011 (SIMAUD 2011) - 2011 SPRING SIMULATION MULTICONFERENCE - BK 8 OF 8, P5
   HARDIN B., 2009, BIM and construction management: proven tools, methods, and workflows
   Irschara A, 2009, PROC CVPR IEEE, P2591, DOI 10.1109/CVPRW.2009.5206587
   Klein G, 2009, INT SYM MIX AUGMENT, P83, DOI 10.1109/ISMAR.2009.5336495
   REITMAYR G, 2007, 6 IEEE ACM INT S MIX, P161, DOI DOI 10.1109/ISMAR.2007.4538842
   ROBERTS G, 2002, FIG 12 INT C
   Shibata F, 2006, LECT NOTES COMPUT SC, V4282, P122
   Shin DH, 2008, AUTOMAT CONSTR, V17, P882, DOI 10.1016/j.autcon.2008.02.012
   Shin DH, 2009, AUTOMAT CONSTR, V18, P118, DOI 10.1016/j.autcon.2008.05.007
   STANGELAND BK, 2012, OPEN BIM COLLABORATI
   Steven A. W., 1996, AUGMENTED REALITY AR
   Taylor S, 2011, INT J COMPUT VISION, V94, P241, DOI 10.1007/s11263-011-0430-6
   WAGNER D, 2009, THESIS GRAZ U TECHNO
   Wang X., 2009, Mixed reality in architecture, design and construction
   Wientapper F., 2011, 2011 International Conference on 3D Imaging, Modeling, Processing, Visualization and Transmission (3DIMPVT), P140, DOI 10.1109/3DIMPVT.2011.25
   Wuest H., 2008, THESIS TU DARMSTADT
   Wuest H, 2007, LECT NOTES COMPUT SC, V4673, P20
NR 30
TC 21
Z9 25
U1 0
U2 60
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2013
VL 29
IS 10
SI SI
BP 1093
EP 1105
DI 10.1007/s00371-013-0840-2
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 227LV
UT WOS:000325115400010
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Xie, ZF
   Lau, RH
   Gui, Y
   Chen, MG
   Ma, LZ
AF Xie, Zhi-Feng
   Lau, RynsonW. H.
   Gui, Yan
   Chen, Min-Gang
   Ma, Li-Zhuang
TI A gradient-domain-based edge-preserving sharpen filter
SO VISUAL COMPUTER
LA English
DT Article
DE Sharpness enhancement; Gradient-domain filter; Image sharpening
ID IMAGE; RANGE
AB As one of the most fundamental operations in computer graphics and computer vision, sharpness enhancement can enhance an image in respect of sharpness characteristics. Unfortunately, the prevalent methods often fail to eliminate image noise, unrealistic details, or incoherent enhancement. In this paper, we propose a new sharpness enhancement approach that can boost the sharpness characteristics of an image effectively with affinity-based edge preserving. Our approach includes three gradient-domain operations: sharpness saliency representation, affinity-based gradient transformation, and gradient-domain image reconstruction. Moreover, we also propose an evaluation method based on sharpness distribution for analyzing all sharpness enhancement approaches in respect of sharpness characteristics. By evaluating the sharpness distribution and comparing the visual appearance, we demonstrate the effectiveness of our sharpness enhancement approach.
C1 [Xie, Zhi-Feng; Gui, Yan; Chen, Min-Gang; Ma, Li-Zhuang] Shanghai Jiao Tong Univ, Shanghai 200030, Peoples R China.
   [Lau, RynsonW. H.] City Univ Hong Kong, Kowloon, Hong Kong, Peoples R China.
C3 Shanghai Jiao Tong University; City University of Hong Kong
RP Xie, ZF (corresponding author), Shanghai Jiao Tong Univ, Shanghai 200030, Peoples R China.
EM zhifeng_xie@sjtu.edu.cn
OI LAU, Rynson W H/0000-0002-8957-8129
FU National Basic Research Project of China [2011CB302203]; National
   Natural Science Foundation of China [61073089, 61133009]; Innovation
   Program of the Science and Technology Commission of Shanghai
   Municipality [10511501200]; City University of Hong Kong [7002664]
FX We would like to thank the anonymous reviewers for their valuable
   comments. This work was supported by the National Basic Research Project
   of China (No. 2011CB302203), the National Natural Science Foundation of
   China (No. 61073089, No. 61133009), the Innovation Program of the
   Science and Technology Commission of Shanghai Municipality (No.
   10511501200), and a SRG grant from City University of Hong Kong (No.
   7002664).
CR Agrawal A, 2005, ACM T GRAPHIC, V24, P828, DOI 10.1145/1073204.1073269
   Agrawal A, 2006, LECT NOTES COMPUT SC, V3951, P578
   [Anonymous], 2006, 2006 IEEE COMP VIS P, DOI DOI 10.1109/CVPR.2006.106
   [Anonymous], 2010, IEEE C COMP VIS PATT
   [Anonymous], 1998, P 6 INT C COMP VIS I
   [Anonymous], PHOT CS 5
   [Anonymous], ACM SIGGRAPH
   Bhat P, 2008, LECT NOTES COMPUT SC, V5303, P114, DOI 10.1007/978-3-540-88688-4_9
   Bhat P, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1731047.1731048
   Boykov YY, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P105, DOI 10.1109/ICCV.2001.937505
   Ding M, 2010, VISUAL COMPUT, V26, P721, DOI 10.1007/s00371-010-0448-8
   Elad M, 2002, IEEE T IMAGE PROCESS, V11, P1141, DOI 10.1109/TIP.2002.801126
   Farbman Z, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360666
   Fattal R, 2002, ACM T GRAPHIC, V21, P249
   Fattal R, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531328
   Fattal R, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1239451.1239546, 10.1145/1276377.1276496]
   Gonzalez R.C., 2018, DIGITAL IMAGE PROCES, V4th
   He KM, 2010, LECT NOTES COMPUT SC, V6311, P1
   He KM, 2009, PROC CVPR IEEE, P1956, DOI [10.1109/CVPRW.2009.5206515, 10.1109/CVPR.2009.5206515]
   Levin A, 2004, ACM T GRAPHIC, V23, P689, DOI 10.1145/1015706.1015780
   Levin A, 2004, LECT NOTES COMPUT SC, V2034, P377
   Levin A, 2008, IEEE T PATTERN ANAL, V30, P228, DOI 10.1109/TPAMI.2007.1177
   Lischinski D, 2006, ACM T GRAPHIC, V25, P646, DOI 10.1145/1141911.1141936
   Orzan A, 2007, NPAR 2007: 5TH INTERNATIONAL SYMPOSIUM ON NON-PHOTOREALISTIC ANIMATION AND RENDERING, PROCEEDINGS, P103
   Pérez P, 2003, ACM T GRAPHIC, V22, P313, DOI 10.1145/882262.882269
   Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720
   Sun J., 2008, IEEE C COMPUTER VISI, P1
   Wang L, 2007, 2007 IEEE SYMPOSIUM ON COMPUTATIONAL INTELLIGENCE IN SECURITY AND DEFENSE APPLICATIONS, P1
   Xie ZF, 2010, VISUAL COMPUT, V26, P1123, DOI 10.1007/s00371-010-0466-6
   Zeng Y, 2006, J COMPUT SCI TECH-CH, V21, P224, DOI 10.1007/s11390-006-0224-4
   Zhang Y, 2011, VISUAL COMPUT, V27, P739, DOI 10.1007/s00371-011-0583-x
NR 31
TC 8
Z9 9
U1 0
U2 20
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2012
VL 28
IS 12
BP 1195
EP 1207
DI 10.1007/s00371-011-0668-6
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 029ZZ
UT WOS:000310538700005
DA 2024-07-18
ER

PT J
AU Marroquim, R
   Pfeiffer, G
   Carvalho, F
   Oliveira, AAF
AF Marroquim, Ricardo
   Pfeiffer, Gustavo
   Carvalho, Felipe
   Oliveira, Antonio A. F.
TI Texturing 3D models from sequential photos
SO VISUAL COMPUTER
LA English
DT Article
DE Texture-to-geometry registration; 3D virtual replica; High-resolution
   texture mapping; Least-squares minimization
ID REGISTRATION
AB This paper proposes a methodology to texture 3D objects with low geometric features from sequentially taken photos. These models pose a challenge to current approaches since they are mainly driven by geometric features-such as contours-that can be extracted from the photographs and uniquely matched with the 3D model. However, when dealing with certain types of objects, such as vases or mechanical equipments, for example, it is not uncommon to find cases where the geometric information is insufficient.
   Our method compensates for the lack of geometric features by using a variation of a contour-based approach that is guided not only by external contours, but also by the internal ones extracted directly from the photos. To align the features a custom-made optimization method is described that avoids common convergence pitfalls encountered in this scenario. In addition, pursuing a fully automatic solution, a linear approach based on feature matching is employed to generate a first guess for the nonlinear optimization. The overall goal is to facilitate an on-site registration process where the photos are taken in a sequential manner and aligned as they are acquired.
RP Marroquim, R (corresponding author), COPPE UFRJ, LCG, Rio De Janeiro, Brazil.
EM marroquim@cos.ufrj.br
RI de Carvalho, Felipe/AAB-5119-2019; Marroquim, Ricardo/M-3791-2019
OI de Carvalho, Felipe/0000-0002-3690-5480; Marroquim,
   Ricardo/0000-0001-8299-7067
FU Rio de Janeiro's research funding agency-FAPERJ
FX This work was supported by Rio de Janeiro's research funding
   agency-FAPERJ.
CR [Anonymous], 2004, Technical Report
   [Anonymous], 1999, Computer Graphics Forum (Eurographics '99)
   [Anonymous], 2004, DISTANCE TRANSFORMS
   Bannai N, 2004, 2ND INTERNATIONAL SYMPOSIUM ON 3D DATA PROCESSING, VISUALIZATION, AND TRANSMISSION, PROCEEDINGS, P558
   Billeter M., 2009, Proceedings of the Conference on High Performance Graphics 2009, HPG '09, P159, DOI [10.2312/EGGH/HPG09/159-166, DOI 10.1145/1572769.1572795, 10. 1145/1572769. 157279]
   Callieri M, 2008, COMPUT GRAPH-UK, V32, P464, DOI 10.1016/j.cag.2008.05.004
   Chuang M, 2009, COMPUT GRAPH FORUM, V28, P1475, DOI 10.1111/j.1467-8659.2009.01524.x
   Corsini M, 2009, COMPUT GRAPH FORUM, V28, P1755, DOI 10.1111/j.1467-8659.2009.01552.x
   DeCarlo D, 2003, ACM T GRAPHIC, V22, P848, DOI 10.1145/882262.882354
   Dellepiane M, 2012, IEEE T VIS COMPUT GR, V18, P463, DOI 10.1109/TVCG.2011.75
   Fiore PD, 2001, IEEE T PATTERN ANAL, V23, P140, DOI 10.1109/34.908965
   Gal R, 2010, COMPUT GRAPH FORUM, V29, P479, DOI 10.1111/j.1467-8659.2009.01617.x
   Guennebaud G., 2010, Eigen
   Harris C., 1988, P 4 ALV VIS C, V15, P10
   Lensch HPA, 2001, GRAPH MODELS, V63, P245, DOI 10.1006/gmod.2001.0554
   Liu L., 2006, IEEE C COMPUTER VISI, V2, P2293, DOI [10.1109/CVPR.2006.204, DOI 10.1109/CVPR.2006.204]
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Neugebauer PJ, 1999, COMPUT GRAPH FORUM, V18, pC245
NR 18
TC 4
Z9 5
U1 0
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2012
VL 28
IS 10
SI SI
BP 983
EP 993
DI 10.1007/s00371-012-0743-7
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 003XE
UT WOS:000308643900004
DA 2024-07-18
ER

PT J
AU Wong, TH
   Leach, G
   Zambetta, F
AF Wong, Tsz Ho
   Leach, Geoff
   Zambetta, Fabio
TI Virtual subdivision for GPU based collision detection of deformable
   objects using a uniform grid
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Conference on the Computer Graphics International (CGI)
CY 2012
CL Bournemouth, ENGLAND
DE Collision detection; Uniform grid; Spatial subdivision; Triangle
   subdivision
AB We present an improved uniform subdivision based discrete and continuous collision detection approach for deformable objects consisting of triangle meshes without any assumption about triangle size. A previously proposed technique using control bits can effectively eliminate redundant object pairs appearing in multiple cells, but this scheme requires the grid cell size adapted to the largest object, and efficiency tends to be severely impaired when object size varies strongly. In this paper, we discuss an approach that virtually subdivides large triangles into a number of child triangles to enable the use of a smaller, better suited cell size, resulting in a considerable decrease in the number of collision tests in the broad phase, with a corresponding reduced memory requirement. The virtual subdivision is used only for the purpose of collision detection and is recomputed each frame, with the original mesh retained for collision response and physical simulation. Our method exploits the benefits of GPU architecture to accelerate the computationally intensive task for improved performance. The results show that the method provides speedups by comparing performance with existing methods.
C1 [Wong, Tsz Ho; Leach, Geoff; Zambetta, Fabio] RMIT Univ, Sch Comp Sci & IT, Melbourne, Vic 3001, Australia.
C3 Royal Melbourne Institute of Technology (RMIT)
RP Wong, TH (corresponding author), RMIT Univ, Sch Comp Sci & IT, GPO Box 2476, Melbourne, Vic 3001, Australia.
EM itszwong@gmail.com
OI Zambetta, Fabio/0000-0003-4133-7913
CR Alcantara DA, 2009, ACM T GRAPHIC, V28, DOI [10.1145/1618452.1610500, 10.1145/1618452.1618500]
   Alliez P, 2003, VISUAL COMPUT, V19, P205, DOI 10.1007/s00371-002-0165-z
   [Anonymous], 1987, SMOOTH SUBDIVISION S
   BANDI S, 1995, COMPUT GRAPH FORUM, V14, pC259
   Bridson R., 2008, P 29 ANN C COMP GRAP, P594
   Curtis S, 2008, I3D 2008: SYMPOSIUM ON INTERACTIVE 3D GRAPHICS AND GAMES, PROCEEDINGS, P61
   Eitz M, 2007, IEEE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS 2007, PROCEEDINGS, P61, DOI 10.1109/SMI.2007.18
   Fan WS, 2011, COMPUT GRAPH FORUM, V30, P1451, DOI 10.1111/j.1467-8659.2011.02019.x
   GRAND SL, 2007, GPU GEMS, V3, P697
   Harris M., 2007, GPU GEMS, V3, P851
   Klosowski JT, 1998, IEEE T VIS COMPUT GR, V4, P21, DOI 10.1109/2945.675649
   Kobbelt L, 2000, COMP GRAPH, P103, DOI 10.1145/344779.344835
   Liu F, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866180
   Mirtich B, 1998, PRACTICAL MOTION PLANNING IN ROBOTICS, P203
   Pabst S, 2010, COMPUT GRAPH FORUM, V29, P1605, DOI 10.1111/j.1467-8659.2010.01769.x
   Provot X., 1997, GRAPHICS INTERFACE, P177
   Sengupta S, 2007, GRAPHICS HARDWARE 2007: ACM SIGGRAPH / EUROGRAPHICS SYMPOSIUM PROCEEDINGS, P97
   Tang M., 2009, 2009 SIAM/ACM Joint Conference on Geometric and Physical Modeling. SPM'09, P355
   Tang M., 2011, I3D 11 P 2011 ACM SI, P63
   Teschner M, 2003, VISION, MODELING, AND VISUALIZATION 2003, P47
   Teschner M., 2004, Eurographics State-of-the-Art Report, P119
   Velho L, 2001, COMPUT AIDED GEOM D, V18, P397, DOI 10.1016/S0167-8396(01)00039-5
   Zhang DL, 2000, EIGHTH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P328, DOI 10.1109/PCCGA.2000.883956
NR 23
TC 7
Z9 10
U1 0
U2 12
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2012
VL 28
IS 6-8
BP 829
EP 838
DI 10.1007/s00371-012-0706-z
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 947EW
UT WOS:000304411500030
DA 2024-07-18
ER

PT J
AU Wu, JL
   Shen, XY
   Liu, LG
AF Wu, Jinliang
   Shen, Xiaoyong
   Liu, Ligang
TI Interactive two-scale color-to-gray
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Conference on the Computer Graphics International (CGI)
CY 2012
CL Bournemouth, ENGLAND
DE Color-to-gray conversion; Contrast enhancement; Perceptually-based
   rendering; Image processing
ID IMAGES
AB Current color-to-gray methods compute the grayscale results by preserving the discriminability among individual pixels. However, human perception tends to firstly group the perceptually similar elements while looking at an image, according to the Gestalt principles. In this paper, we propose a novel two-scale approach for converting color images to grayscale. First, we decompose the input image into multiple soft segments where each segment represents a perceptual group of content. Second, we determine the grayscale of each perceptual group via a global mapping by solving a quadratic optimization. Last, the local details are added into the final result. Our approach is efficient and provides users quick feedback on adjusting the prominent gray tones of the results. As an important aspect of algorithm, our approach offers users an easy, intuitive interactive tool for creating art-like black-and-white images from input color images. Experimental results show that our approach better preserves the overall perception and local details. User studies have been conducted to show the applicability of our approach.
C1 [Wu, Jinliang; Shen, Xiaoyong; Liu, Ligang] Zhejiang Univ, Dept Math, Hangzhou, Zhejiang, Peoples R China.
C3 Zhejiang University
RP Liu, LG (corresponding author), Zhejiang Univ, Dept Math, Hangzhou, Zhejiang, Peoples R China.
EM jinliangwu@zju.edu.cn; shenxiaoyong@zju.edu.cn; ligangliu@zju.edu.cn
RI Li, Shiyue/KFA-3709-2024
CR An XB, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360639
   [Anonymous], 2007, PROC 3 EUR C COMPUTA
   [Anonymous], 2000, WILEY SERIES PURE AP
   Bala R, 2004, 12TH COLOR IMAGING CONFERENCE: COLOR SCIENCE AND ENGINEERING SYSTEMS, TECHNOLOGIES, APPLICATIONS, P82
   Bie XH, 2011, COMPUT GRAPH FORUM, V30, P2041, DOI 10.1111/j.1467-8659.2011.02059.x
   Cadík M, 2008, COMPUT GRAPH FORUM, V27, P1745
   David H.A., 1988, METHOD PAIRED COMPAR
   Farbman Z, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866171
   Gilchrist A, 1999, PSYCHOL REV, V106, P795, DOI 10.1037/0033-295X.106.4.795
   Gooch AA, 2005, ACM T GRAPHIC, V24, P634, DOI 10.1145/1073204.1073241
   Grundland M, 2007, PATTERN RECOGN, V40, P2891, DOI 10.1016/j.patcog.2006.11.003
   Kim Y, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618507
   Kuhn GR, 2008, VISUAL COMPUT, V24, P505, DOI 10.1007/s00371-008-0231-2
   Metzger W., 2006, LAWS OF SEEING
   Rasche K, 2005, COMPUT GRAPH FORUM, V24, P423, DOI 10.1111/j.1467-8659.2005.00867.x
   Rasche K, 2005, IEEE COMPUT GRAPH, V25, P22, DOI 10.1109/MCG.2005.54
   Smith K, 2008, COMPUT GRAPH FORUM, V27, P193, DOI 10.1111/j.1467-8659.2008.01116.x
   Wang BY, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866172
NR 18
TC 17
Z9 24
U1 0
U2 13
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2012
VL 28
IS 6-8
BP 723
EP 731
DI 10.1007/s00371-012-0683-2
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 947EW
UT WOS:000304411500020
DA 2024-07-18
ER

PT J
AU Du, P
   Ip, HHS
   Hua, B
   Feng, J
AF Du, Peng
   Ip, Horace H. S.
   Hua, Bei
   Feng, Jun
TI Using surface variability characteristics for segmentation of deformable
   3D objects with application to piecewise statistical deformable model
SO VISUAL COMPUTER
LA English
DT Article
DE Mesh segmentation; Principal component analysis; Statistical deformable
   model; Surface reconstruction
ID MESH; DECOMPOSITION; COMPRESSION; POINT
AB To cope with the small sample size problem in the construction of Statistical Deformable Models (SDM), this paper proposes two novel measures that quantify the similarity of the variability characteristics among deforming 3D meshes. These measures are used as the basis of our proposed technique for partitioning a 3D mesh for the construction of piecewise SDM in a divide-and-conquer strategy. Specifically, the surface variability information is extracted by performing a global principal component analysis on the set of sample meshes. An iterative face clustering algorithm is developed for segmenting a mesh that favors grouping triangular faces having similar variability characteristics into a same mesh component. We apply the proposed mesh segmentation algorithm to the construction of piecewise SDM and evaluate the representational ability of the resulting piecewise SDM through the reconstruction of unseen meshes. Experimental results show that our approach outperforms several state-of-the-art methods in terms of the representational ability of the resulting piecewise SDM as evaluated by the reconstruction accuracy.
C1 [Du, Peng; Ip, Horace H. S.] City Univ Hong Kong, Dept Comp Sci, Suzhou, Peoples R China.
   [Du, Peng; Ip, Horace H. S.; Hua, Bei] USTC CityU Joint Adv Res Ctr, Suzhou, Peoples R China.
   [Du, Peng; Hua, Bei] Univ Sci & Technol China, Sch Comp Sci & Technol, Suzhou, Peoples R China.
   [Feng, Jun] NW Univ Xian, Sch Informat Technol, Xian 710069, Peoples R China.
   [Ip, Horace H. S.] City Univ Hong Kong, AIMtech Ctr, Suzhou, Peoples R China.
C3 City University of Hong Kong; Chinese Academy of Sciences; University of
   Science & Technology of China, CAS; Northwest University Xi'an; City
   University of Hong Kong
RP Du, P (corresponding author), City Univ Hong Kong, Dept Comp Sci, Suzhou, Peoples R China.
EM pengdu@mail.ustc.edu.cn; cship@cityu.edu.hk; bhua@ustc.edu.cn;
   fengjun@nwu.edu.cn
OI IP, Ho Shing Horace/0000-0002-1509-9002
FU City University of Hong Kong [7008040]; National Natural Science
   Foundation of China [60673173, 60873095]
FX The authors thank Han-Bing Yan for providing us the hand meshes. We
   thank the Computer Science and Artificial Intelligence laboratory of MIT
   for the cat meshes. The work described in this paper was supported by a
   grant from City University of Hong Kong Project No: 7008040, and the
   National Natural Science Foundation of China under Grant Nos. 60673173
   and 60873095.
CR Amjoun R., 2007, P 15 INT C CENTR EUR
   Amjoun R, 2006, LECT NOTES COMPUT SC, V4035, P606
   Attene M, 2006, VISUAL COMPUT, V22, P181, DOI 10.1007/s00371-006-0375-x
   Attene M., 2006, P IEEE INT C SHAP MO
   BORREL P, 1994, ACM T GRAPHIC, V13, P137, DOI 10.1145/176579.176581
   Cohen-Steiner D, 2004, ACM T GRAPHIC, V23, P905, DOI 10.1145/1015706.1015817
   COOTES TF, 1995, COMPUT VIS IMAGE UND, V61, P38, DOI 10.1006/cviu.1995.1004
   Feng J, 2008, LECT NOTES COMPUT SC, V5128, P168, DOI 10.1007/978-3-540-79982-5_19
   Feng J, 2008, IMAGE VISION COMPUT, V26, P761, DOI 10.1016/j.imavis.2007.08.018
   Garland M., 2001, I3D 01, P49, DOI [DOI 10.1145/364338.364345, 10.1145/364338.364345]
   James DL, 2005, ACM T GRAPHIC, V24, P399, DOI 10.1145/1073204.1073206
   Julius D, 2005, COMPUT GRAPH FORUM, V24, P581, DOI 10.1111/j.1467-8659.2005.00883.x
   Kalvin AD, 1996, IEEE COMPUT GRAPH, V16, P64, DOI 10.1109/38.491187
   Karni Z, 2000, COMP GRAPH, P279, DOI 10.1145/344779.344924
   Katz S, 2005, VISUAL COMPUT, V21, P649, DOI 10.1007/s00371-005-0344-9
   Katz S, 2003, ACM T GRAPHIC, V22, P954, DOI 10.1145/882262.882369
   Kervrann C, 1999, IEEE T IMAGE PROCESS, V8, P583, DOI 10.1109/83.753745
   Lanitis A, 1997, IEEE T PATTERN ANAL, V19, P743, DOI 10.1109/34.598231
   Lee TY, 2006, VISUAL COMPUT, V22, P729, DOI 10.1007/s00371-006-0059-6
   Lee TY, 2005, COMPUT ANIMAT VIRT W, V16, P519, DOI 10.1002/cav.79
   Lévy B, 2002, ACM T GRAPHIC, V21, P362, DOI 10.1145/566570.566590
   Lien J.-M., 2004, SCG '04: Proceedings of the twentieth annual symposium on Computational geometry, P457
   Liu R, 2004, 12TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P298
   Okada T, 2007, LECT NOTES COMPUT SC, V4791, P86
   Ruiz-Correa S, 2001, PROC CVPR IEEE, P769
   Sattler M., 2005, P 2005 ACM SIGGRAPH, P209
   Shamir A, 2004, 2ND INTERNATIONAL SYMPOSIUM ON 3D DATA PROCESSING, VISUALIZATION, AND TRANSMISSION, PROCEEDINGS, P82
   Shamir A, 2008, COMPUT GRAPH FORUM, V27, P1539, DOI 10.1111/j.1467-8659.2007.01103.x
   Shlafman S, 2002, COMPUT GRAPH FORUM, V21, P219, DOI 10.1111/1467-8659.00581
   Theobalt C, 2007, SYMPOSIUM ON COMPUTER ANIMATION 2007: ACM SIGGRAPH/ EUROGRAPHICS SYMPOSIUM PROCEEDINGS, P271
   Wuhrer S, 2010, VISUAL COMPUT, V26, P147, DOI 10.1007/s00371-009-0394-5
   Yan HB, 2008, IEEE T VIS COMPUT GR, V14, P693, DOI 10.1109/TVCG.2008.28
   Zhao Z, 2005, LECT NOTES COMPUT SC, V3749, P221
   Zuckerberger E, 2002, COMPUT GRAPH-UK, V26, P733, DOI 10.1016/S0097-8493(02)00128-0
NR 34
TC 3
Z9 3
U1 0
U2 16
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2012
VL 28
IS 5
BP 493
EP 509
DI 10.1007/s00371-011-0646-z
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 926EJ
UT WOS:000302813800006
DA 2024-07-18
ER

PT J
AU Seol, Y
   Seo, J
   Kim, PH
   Lewis, JP
   Noh, J
AF Seol, Yeongho
   Seo, Jaewoo
   Kim, Paul Hyunjin
   Lewis, J. P.
   Noh, Junyong
TI Weighted pose space editing for facial animation
SO VISUAL COMPUTER
LA English
DT Article
DE Pose space; Blendshape; Facial animation; Editing
AB Blendshapes are the most commonly used approach to realistic facial animation in production. A blendshape model typically begins with a relatively small number of blendshape targets reflecting major muscles or expressions. However, the majority of the effort in constructing a production quality model occurs in the subsequent addition of targets needed to reproduce various subtle expressions and correct for the effects of various shapes in combination. To make this subsequent modeling process much more efficient, we present a novel editing method that removes the need for much of the iterative trial-and-error decomposition of an expression into targets. Isolated problematic frames of an animation are re-sculpted as desired and used as training for a nonparametric regression that associates these shapes with the underlying blendshape weights. Using this technique, the artist's correction to a problematic expression is automatically applied to similar expressions in an entire sequence, and indeed to all future sequences. The extent and falloff of editing is controllable and the effect is continuously propagated to all similar expressions. In addition, we present a search scheme that allows effective reuse of pre-sculpted editing examples. Our system greatly reduces time and effort required by animators to create high quality facial animations.
C1 [Noh, Junyong] Korea Adv Inst Sci & Technol, Grad Sch Culture Technol, Taejon 305701, South Korea.
   [Lewis, J. P.] Weta Digital, Wellington, New Zealand.
   [Lewis, J. P.] Victoria Univ Wellington, Wellington, New Zealand.
C3 Korea Advanced Institute of Science & Technology (KAIST); Victoria
   University Wellington
RP Noh, J (corresponding author), Korea Adv Inst Sci & Technol, Grad Sch Culture Technol, Taejon 305701, South Korea.
EM junyongnoh@kaist.ac.kr
RI Noh, Junyong/C-1663-2011; Lewis, J P/GYI-9327-2022; Seol,
   Yeongho/KIE-6801-2024
OI Lewis, J.P./0000-0002-6835-7263
FU KOCCA/MCST [2-10-7602-003-10743-01-007]
FX We wish to thank Younghui Kim for shading/lighting, Jason Osipa for the
   face model, and the reviewers for their time and comments. This work was
   supported by KOCCA/MCST (2-10-7602-003-10743-01-007, Software
   Development for 2D to 3D Stereoscopic Image Conversion).
CR [Anonymous], 2010, ACM SIGGRAPH 2010 Papers. SIGGRAPH'10, DOI [DOI 10.1145/1833349.1778769, DOI 10.1145/1778765.1778769]
   Bickel B., 2008, Proceedings of the 2008 ACM SIGGRAPH/Eurographics Symposium on Computer Animation (SCA '08), P57
   Bishop C. M., 1995, NEURAL NETWORKS PATT
   Cao Y., 2003, ACM SIGGRAPH/Eurographics Symposium on Computer Animation, P225
   Carr JC, 2001, COMP GRAPH, P67, DOI 10.1145/383259.383266
   Choe B, 2001, J VISUAL COMP ANIMAT, V12, P67, DOI 10.1002/vis.246
   Chuang E, 2005, ACM T GRAPHIC, V24, P331, DOI 10.1145/1061347.1061355
   Deng Z., 2006, P 2006 S INT 3D GRAP, P43, DOI DOI 10.1145/1111411.1111419]
   Fornberg B., 2006, GIBBS PHENOMENON VAR
   Joshi P., 2003, ACM SIGGRAPH/Eurographics Symposium on Computer Animation, P187
   Komorowski D., 2010, ACM SIGGRAPH ASI LEA, DOI [10.1145/18999 50.1899992, DOI 10.1145/1899950.1899992]
   Kurihara Tsuneya, 2004, P 2004 ACM SIGGRAPHE, P355
   Lau M, 2009, ACM T GRAPHIC, V29, DOI 10.1145/1640443.1640446
   Lee G., 2009, ACM SIGGRAPH ASIA 20
   Lewis JP, 2010, IEEE COMPUT GRAPH, V30, P42, DOI 10.1109/MCG.2010.41
   Lewis JP, 2005, P 2005 S INT 3D GRAP, P25
   Li Q, 2008, IEEE COMPUT GRAPH, V28, P76, DOI 10.1109/MCG.2008.120
   Ma Xiaohan., 2009, Proceedings of SCA 2009, P123
   Osipa J., 2007, Stop Staring: Facial Modeling and Animation Done Right
   YUILLE AL, 1989, INT J COMPUT VISION, V3, P155, DOI 10.1007/BF00126430
NR 20
TC 6
Z9 9
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2012
VL 28
IS 3
BP 319
EP 327
DI 10.1007/s00371-011-0641-4
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 896QS
UT WOS:000300585400007
DA 2024-07-18
ER

PT J
AU Celes, W
   Abraham, F
AF Celes, Waldemar
   Abraham, Frederico
TI Fast and versatile texture-based wireframe rendering
SO VISUAL COMPUTER
LA English
DT Article
DE Wireframe rendering; Anti-aliased lines; Mipmapping; Geometry shader
AB This paper revisits the problem of wireframe rendering, which, at first, appears to be an easily solved problem. However, the conventional solution is inefficient and does not result in high-quality images. Recently, graphics hardware programming has been employed to achieve high-quality solid line rendering. In this paper, we present a simpler and faster technique for wireframe rendering based on texture mapping. Our technique does not require (but can benefit from) graphics hardware programming and thus can be easily integrated into existing rendering engines, while resulting in fast, accurate, high-quality, anti-aliased, and still versatile, wireframe drawing. For topologically structured meshes, our approach allows the rendering of wireframe decoupled from the underlying mesh, making possible the rendering of original wireframes on top of decimated meshes.
C1 [Celes, Waldemar; Abraham, Frederico] Pontifical Univ Catholic Rio de Janeiro, Tecgraf PUC Rio Comp Sci Dept, Rio De Janeiro, Brazil.
RP Celes, W (corresponding author), Pontifical Univ Catholic Rio de Janeiro, Tecgraf PUC Rio Comp Sci Dept, Rio De Janeiro, Brazil.
EM celes@inf.puc-rio.br; fabraham@tecgraf.puc-rio.br
OI Rodrigues Abraham, Frederico/0000-0002-5143-4971
CR [Anonymous], 2018, Real-Time Rendering
   BAERENTZEN JA, 2008, P SPRING C COMP GRAP
   BAERENTZEN JA, 2006, SIGGRAPH 06, P149
   BAERENTZEN JA, 2006, SINGLE PASS WIREFRAM
   CELES W, 2010, GRAPH PATT IM 23 SIB, P149, DOI DOI 10.1109/SIBGRAPI.2010.28
   GATEAU S, 2007, SOLID WIREFRAME
   Haeberli P., 1993, EGSR 93, P259
   HERRELL R, 1995, IEEE COMPUT GRAPH, V15, P68, DOI 10.1109/38.391494
   Kuschfeldt S, 1998, IEEE COMPUT GRAPH, V18, P60, DOI 10.1109/38.689666
   Rose D, 2003, VISION, MODELING, AND VISUALIZATION 2003, P585
   Tang C, 2010, COMPUT ANIMAT VIRT W, V21, P411, DOI 10.1002/cav.370
   WANG W, 1999, J GRAPHICS TOOLS, V4, P1
NR 12
TC 1
Z9 1
U1 0
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2011
VL 27
IS 10
SI SI
BP 939
EP 948
DI 10.1007/s00371-011-0623-6
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 831MU
UT WOS:000295736200007
DA 2024-07-18
ER

PT J
AU Leblanc, L
   Houle, J
   Poulin, P
AF Leblanc, Luc
   Houle, Jocelyn
   Poulin, Pierre
TI Modeling with blocks
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Conference on the Computer Graphics International (CGI)
CY JUN 12-15, 2011
CL Ottawa, CANADA
DE Subdivision surface; Surface parameterization; Polycube map; z-brush;
   Displacement map; Geometry image; CSG
AB This paper presents a simple and general modeling primitive, called a block, based on a generalized cuboid shape. Blocks are laid out and connected together to constitute the base shape of complex objects, from which is extracted a control mesh that can contain both smooth and sharp edges. The volumetric nature of the blocks allows for easy topology specification, as well as CSG operations between blocks. The surface parameterization inherited from the block faces provides support for texturing and displacement functions to apply surface details. A variety of examples illustrate the generality of our blocks in both interactive and procedural modeling contexts.
C1 [Leblanc, Luc; Houle, Jocelyn; Poulin, Pierre] Univ Montreal, Dept IRO, LIGUM, Montreal, PQ, Canada.
   [Poulin, Pierre] Univ Montreal, Comp Sci & Operat Res Dept, Montreal, PQ, Canada.
C3 Universite de Montreal; Universite de Montreal
RP Leblanc, L (corresponding author), Univ Montreal, Dept IRO, LIGUM, Montreal, PQ, Canada.
EM leblanc@iro.umontreal.ca; houlejo@iro.umontreal.ca;
   poulin@iro.umontreal.ca
CR Andersson L.-E., 2010, INTRO MATH SUBDIVISI
   [Anonymous], 1997, Introduction to Implicit Surfaces
   Bernhardt A, 2010, COMPUT GRAPH FORUM, V29, P367, DOI 10.1111/j.1467-8659.2009.01606.x
   Blinn J. F., 1982, Computer Graphics, V16, DOI 10.1145/965145.801290
   Burley B, 2008, COMPUT GRAPH FORUM, V27, P1155, DOI 10.1111/j.1467-8659.2008.01253.x
   CIGNONI P, 1996, J GRAPHICS TOLS, V1, P1
   DeRose T., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P85, DOI 10.1145/280814.280826
   GU X, 2002, SIGGRAPH 02, P355, DOI DOI 10.1145/566570.566589
   Held M, 2001, ALGORITHMICA, V30, P563, DOI 10.1007/s00453-001-0028-4
   Ji ZP, 2010, COMPUT GRAPH FORUM, V29, P2169, DOI 10.1111/j.1467-8659.2010.01805.x
   LAI S, 2005, CAD CG 05, P125
   LAI S, 2006, LECT NOTES COMPUTER, P595
   LEBLANC L, 2011, GRAPHICS INTERFACE
   NI T, 2008, SMI 08, P3
   PASKO A, 1995, VISUAL COMPUT, V11, P429, DOI 10.1007/BF02464333
   Piegl L, 1995, NURBS BOOK
   *PIXOLOGIC, 2011, ZBRUSH
   Sederberg TN, 2003, ACM T GRAPHIC, V22, P477, DOI 10.1145/882262.882295
   Tarini M, 2004, ACM T GRAPHIC, V23, P853, DOI 10.1145/1015706.1015810
   Wyvill B, 1999, COMPUT GRAPH FORUM, V18, P149, DOI 10.1111/1467-8659.00365
   XIA J, 2011, I3D 11, P151
NR 21
TC 9
Z9 9
U1 0
U2 4
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2011
VL 27
IS 6-8
BP 555
EP 563
DI 10.1007/s00371-011-0589-4
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 766FY
UT WOS:000290767600014
DA 2024-07-18
ER

PT J
AU Wang, SF
   Hou, TB
   Su, ZX
   Qin, H
AF Wang, Shengfa
   Hou, Tingbo
   Su, Zhixun
   Qin, Hong
TI Multi-scale anisotropic heat diffusion based on normal-driven shape
   representation
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Conference on the Computer Graphics International (CGI)
CY JUN 12-15, 2011
CL Ottawa, CANADA
DE Normal-Controlled Coordinates; Normal signature; Anisotropic diffusion;
   Edge-weighted heat kernel; Multi-scale feature extraction
AB Multi-scale geometric processing has been a popular and powerful tool in graphics, which typically employs isotropic diffusion across scales. This paper proposes a novel method of multi-scale anisotropic heat diffusion on manifold, based on the new normal-driven shape representation and Edge-weighted Heat Kernels (EHK). The new shape representation, named as Normal-Controlled Coordinates (NCC), can encode local geometric details of a vertex along its normal direction and rapidly reconstruct surface geometry. Moreover, the inner product of NCC and its corresponding vertex normal, called Normal Signature (NS), defines a scalar/heat field over curved surface. The anisotropic heat diffusion is conducted using the weighted heat kernel convolution governed by local geometry. The convolution is computed iteratively based on the semigroup property of heat kernels toward accelerated performance. This diffusion is an efficient multi-scale procedure that rigorously conserves the total heat. We apply our new method to multi-scale feature detection, scalar field smoothing and mesh denoising, and hierarchical shape decomposition. We conduct various experiments to demonstrate the effectiveness of our method. Our method can be generalized to handle any scalar field defined over manifold.
C1 [Wang, Shengfa; Su, Zhixun] Dalian Univ Technol, Dalian 116024, Peoples R China.
   [Wang, Shengfa; Hou, Tingbo; Qin, Hong] SUNY Stony Brook, Stony Brook, NY 11794 USA.
C3 Dalian University of Technology; State University of New York (SUNY)
   System; State University of New York (SUNY) Stony Brook
RP Wang, SF (corresponding author), Dalian Univ Technol, Dalian 116024, Peoples R China.
EM shengfawang@gmail.com
RI Hou, Tingbo/H-6978-2012
FU Direct For Computer & Info Scie & Enginr; Div Of Information &
   Intelligent Systems [1047715] Funding Source: National Science
   Foundation
CR Alexa M, 2003, VISUAL COMPUT, V19, P105, DOI 10.1007/s00371-002-0180-0
   Bronstein AM, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1899404.1899405
   Clarenz U, 2000, IEEE VISUAL, P397, DOI 10.1109/VISUAL.2000.885721
   Desbrun M, 2000, PROC GRAPH INTERF, P145
   Desbrun M, 1999, COMP GRAPH, P317, DOI 10.1145/311535.311576
   Fleishman S, 2003, ACM T GRAPHIC, V22, P950, DOI 10.1145/882262.882368
   Garland M., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P209, DOI 10.1145/258734.258849
   GEBAL K, 2009, S GEOM PROC, P1405
   Grigor'yan A., 1999, Applicable Analysis, V71, P63, DOI DOI 10.1080/00036819908840705
   Hastie T, 1996, IEEE T PATTERN ANAL, V18, P607, DOI 10.1109/34.506411
   Hildebrandt K, 2004, COMPUT GRAPH FORUM, V23, P391, DOI 10.1111/j.1467-8659.2004.00770.x
   HOPPE H, 1996, SIGGRAPH 96, P99, DOI DOI 10.1145/237170.237216
   Hou TB, 2010, LECT NOTES COMPUT SC, V6313, P384
   Hua J, 2008, IEEE T VIS COMPUT GR, V14, P1643, DOI 10.1109/TVCG.2008.134
   Huang NE, 1998, P ROY SOC A-MATH PHY, V454, P903, DOI 10.1098/rspa.1998.0193
   Huang NE, 2008, REV GEOPHYS, V46, DOI 10.1029/2007RG000228
   Jin SS, 2005, VISUAL COMPUT, V21, P71, DOI 10.1007/s00371-004-0271-1
   Jones TR, 2003, ACM T GRAPHIC, V22, P943, DOI 10.1145/882262.882367
   Ju T, 2005, ACM T GRAPHIC, V24, P561, DOI 10.1145/1073204.1073229
   Lee CH, 2005, ACM T GRAPHIC, V24, P659, DOI 10.1145/1073204.1073244
   Lipman Y, 2005, ACM T GRAPHIC, V24, P479, DOI 10.1145/1073204.1073217
   Lipman Y, 2004, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, P181, DOI 10.1109/SMI.2004.1314505
   MCMOLI F, 2009, NORDIA, P256
   MEYER M., 2002, P VISMATH C, P1
   Ovsjanikov M, 2010, COMPUT GRAPH FORUM, V29, P1555, DOI 10.1111/j.1467-8659.2010.01764.x
   Patane Giuseppe, 2010, Proceedings of the Shape Modeling International (SMI 2010), P113, DOI 10.1109/SMI.2010.27
   Reuter M, 2006, COMPUT AIDED DESIGN, V38, P342, DOI 10.1016/j.cad.2005.10.011
   Seo S, 2010, LECT NOTES COMPUT SC, V6363, P505
   Sheffer A, 2004, 2ND INTERNATIONAL SYMPOSIUM ON 3D DATA PROCESSING, VISUALIZATION, AND TRANSMISSION, PROCEEDINGS, P68
   Su ZX, 2009, SMI 2009: IEEE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, PROCEEDINGS, P1, DOI 10.1109/SMI.2009.5170156
   Sun JA, 2009, COMPUT GRAPH FORUM, V28, P1383, DOI 10.1111/j.1467-8659.2009.01515.x
   TIERNY J, 2007, SMI, P215
   Vaxman A, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778858
   Zaharescu A, 2009, PROC CVPR IEEE, P373, DOI 10.1109/CVPRW.2009.5206748
   Zou GY, 2009, IEEE T VIS COMPUT GR, V15, P1193, DOI 10.1109/TVCG.2009.159
NR 35
TC 10
Z9 15
U1 0
U2 6
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2011
VL 27
IS 6-8
BP 429
EP 439
DI 10.1007/s00371-011-0582-y
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 766FY
UT WOS:000290767600003
DA 2024-07-18
ER

PT J
AU Lin, JC
   Jin, XG
   Wang, CCL
AF Lin, Juncong
   Jin, Xiaogang
   Wang, Charlie C. L.
TI Fusion of disconnected mesh components with branching shapes
SO VISUAL COMPUTER
LA English
DT Article
DE Mesh fusion; Connectivity graph; Branching shape; Interactive modeling
   tool
AB Branching structure is a common feature of many natural objects. Given some mesh components with a branching shape missing, this paper presents a novel approach to fuse the mesh components: connectivity graph of the branching shape is constructed to seamlessly connect the boundaries of given components; then, natural geometry is imposed on the connectivity graph exploiting the information of given boundaries. We present a method to construct a branching connectivity graph to connect arbitrary number of given boundaries. Also, a method to generate natural geometry of the connectivity graph that smoothly fuses the boundaries of mesh components is exploited. Some examples are given to demonstrate that our new scheme can be used in a couple of applications, such as fast tree trunk modeling, mesh composition and shell generation.
C1 [Lin, Juncong; Jin, Xiaogang] Zhejiang Univ, State Key Lab CAD & CG, Hangzhou 310027, Zhejiang, Peoples R China.
   [Wang, Charlie C. L.] Chinese Univ Hong Kong, Dept Mech & Automat Engn, Shatin, Hong Kong, Peoples R China.
C3 Zhejiang University; Chinese University of Hong Kong
RP Jin, XG (corresponding author), Zhejiang Univ, State Key Lab CAD & CG, Hangzhou 310027, Zhejiang, Peoples R China.
EM JuncongLin@gmail.com; jin@cad.zju.edu.cn; cwang@mae.cuhk.edu.hk
RI Wang, Charlie C. L./B-3730-2010
OI Wang, Charlie C. L./0000-0003-4406-8480
FU National Natural Science Foundation of China [60933007, 60833007]; Key
   Technology RD Program [2007BAH11B03]; Science and Technology Plan of
   Zhejiang Province [2009C33001]; CUHK [CUHK/2050341]; Shun Hing Institute
   of Advanced Engineering (SHIAE) [CUHK/8115022]
FX The authors would like to thank Mingdong Zhou for his help in the
   implementation. Xiaogang Jin was supported by the National Natural
   Science Foundation of China (60933007, 60833007), the Key Technology R&D
   Program (2007BAH11B03), and the Science and Technology Plan of Zhejiang
   Province (2009C33001). Juncong Lin and Charlie CL Wang were supported by
   CUHK Direct Research Grant (CUHK/2050341) and Shun Hing Institute of
   Advanced Engineering (SHIAE) Research Grant (CUHK/8115022).
CR [Anonymous], 2004, S GEOM PROC
   [Anonymous], [No title captured], DOI DOI 10.1145/218380.218473
   [Anonymous], ACM T GRAPH
   Bloomenthal J., 1985, Computer Graphics, V19, P305, DOI 10.1145/325165.325249
   BLOOMENTHAL J, 1991, COMP GRAPH, V25, P251, DOI 10.1145/127719.122757
   Bloomenthal J., 1995, THESIS U CALGARY
   BLOOR MIG, 1990, COMPUT AIDED DESIGN, V22, P202, DOI 10.1016/0010-4485(90)90049-I
   Boskamp T, 2005, MEDICAL IMAGING SYSTEMS TECHNOLOGY: METHODS IN CARDIOVASCULAR AND BRAIN SYSTEMS, VOL 5, P1
   DAVIS J, 2001, P 1 INT S 3D DAT PRO, P428
   Desbrun M, 1999, COMP GRAPH, P317, DOI 10.1145/311535.311576
   Felkel P, 2004, COMPUTER GRAPHICS INTERNATIONAL, PROCEEDINGS, P70, DOI 10.1109/CGI.2004.1309194
   Galbraitht C, 2004, COMPUT GRAPH FORUM, V23, P351, DOI 10.1111/j.1467-8659.2004.00766.x
   Isenburg M, 2001, IEEE VISUAL, P135, DOI 10.1109/VISUAL.2001.964504
   Jin XG, 2006, VISUAL COMPUT, V22, P266, DOI 10.1007/s00371-006-0004-8
   Kanai T, 1999, PROC GRAPH INTERF, P148
   KOBBELT L, 1998, P SIGGRAPH 98, P105, DOI DOI 10.1145/280814.280831
   KRAEVOY V, 2005, P EUR S GEOM PROC
   Liepa P., 2003, Symposium on Geometry Processing, P200
   Lin JC, 2008, IEEE T VIS COMPUT GR, V14, P653, DOI 10.1109/TVCG.2007.70632
   Lluch J, 2004, GRAPH MODELS, V66, P89, DOI 10.1016/j.gmod.2004.01.002
   MACMURCHY P, 2004, THESIS U CALGARY
   Meyer M., 2002, VISUALIZATION MATH, V6, P35, DOI DOI 10.1007/978-3-662-05105-4_2
   Nealen A, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1276377.1276429
   Oeltze S., 2004, Proceedings of IEEE/Eurographics Symposium on Visualization (VisSym), P311
   Okabe M, 2005, COMPUT GRAPH FORUM, V24, P487, DOI 10.1111/j.1467-8659.2005.00874.x
   PODOLAK J, 2005, P EUR S GEOM PROC
   Porumbescu SD, 2005, ACM T GRAPHIC, V24, P626, DOI 10.1145/1073204.1073239
   Prusinkiewicz P., 1994, Computer Graphics Proceedings. Annual Conference Series 1994. SIGGRAPH 94 Conference Proceedings, P351, DOI 10.1145/192161.192254
   Sharf A, 2006, VISUAL COMPUT, V22, P835, DOI 10.1007/s00371-006-0068-5
   Sorkine O, 2004, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, P191, DOI 10.1109/SMI.2004.1314506
   Tan P, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239538
   Wang CCL, 2005, J COMPUT INF SCI ENG, V5, P291, DOI 10.1115/1.2052850
   Wardetzky M, 2007, COMPUT AIDED GEOM D, V24, P499, DOI 10.1016/j.cagd.2007.07.006
   WELCH W, 1994, P SIGGRAPH 94, P247
   Xu GL, 2006, COMPUT AIDED GEOM D, V23, P125, DOI 10.1016/j.cagd.2005.05.004
NR 35
TC 4
Z9 8
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2010
VL 26
IS 6-8
BP 1017
EP 1025
DI 10.1007/s00371-010-0460-z
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 602JQ
UT WOS:000278135800061
DA 2024-07-18
ER

PT J
AU Rasmusson, J
   Ström, J
   Akenine-Möller, T
AF Rasmusson, Jim
   Strom, Jacob
   Akenine-Moller, Tomas
TI Error-bounded lossy compression of floating-point color buffers using
   quadtree decomposition
SO VISUAL COMPUTER
LA English
DT Article
DE Color buffer compression; Lossy compression; High dynamic range;
   Real-time; Texture compression; Quadtree
AB In this paper, we present a new color buffer compression algorithm for floating-point buffers. It can operate in either an approximate (lossy) mode or in an exact (lossless) mode. The approximate mode is error-bounded and the amount of introduced accumulated error is controlled via a few parameters. The core of the algorithm lies in an efficient representation and color space transform, followed by a hierarchical quadtree decomposition, and then hierarchical prediction and Golomb-Rice encoding. We believe this is the first lossy compression algorithm for floating-point buffers, and our results indicate significantly reduced color buffer bandwidths and negligible visible artifacts.
C1 [Rasmusson, Jim; Akenine-Moller, Tomas] Lund Univ, Dept Comp Sci, S-22100 Lund, Sweden.
   [Rasmusson, Jim; Strom, Jacob] Ericsson Telecom AB, S-12625 Stockholm, Sweden.
C3 Lund University; Ericsson
RP Rasmusson, J (corresponding author), Lund Univ, Dept Comp Sci, Box 118, S-22100 Lund, Sweden.
EM jim.rasmusson@cs.lth.se; jacob.strom@ericsson.com; tam@cs.lth.se
CR Beers A. C., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P373, DOI 10.1145/237170.237276
   Bogart R., 2003, ACM SIGGRAPH Sketches and Applications
   CAGLAR A, 2008, THESIS LINKOPING U, P11
   GOLOMB SW, 1966, IEEE T INFORM THEORY, V12, P399, DOI 10.1109/TIT.1966.1053907
   Hasselgren J., 2006, P 21 ACM SIGGRAPHEUR, P103
   Knittel G, 1996, COMPUT GRAPH-UK, V20, P475, DOI 10.1016/0097-8493(96)00019-2
   Lehtinen J, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1289603.1289604
   Lindstrom P, 2006, IEEE T VIS COMPUT GR, V12, P1245, DOI 10.1109/TVCG.2006.143
   MALVAR H, 2003, JVT I014R3
   Mantiuk R, 2005, PROC SPIE, V5666, P204, DOI 10.1117/12.586757
   Munkberg J, 2008, COMPUT GRAPH FORUM, V27, P1664, DOI 10.1111/j.1467-8659.2008.01174.x
   Munkberg J, 2006, ACM T GRAPHIC, V25, P698, DOI 10.1145/1141911.1141944
   Rasmusson J, 2007, GRAPHICS HARDWARE 2007: ACM SIGGRAPH / EUROGRAPHICS SYMPOSIUM PROCEEDINGS, P41
   RICE RF, 1979, 22 JET PROP LAB
   Roimela K, 2006, ACM T GRAPHIC, V25, P707, DOI 10.1145/1141911.1141945
   Roimela K, 2008, I3D 2008: SYMPOSIUM ON INTERACTIVE 3D GRAPHICS AND GAMES, PROCEEDINGS, P207
   STROM J., 2008, GRAPHICS HARDWARE, p[96, 2]
   Strom Jacob, 2005, Proceedings of the ACM SIGGRAPH/EUROGRAPHICS conference on Graphics Hardware, P63, DOI DOI 10.1145/1071866.1071877
   Sun Wen., 2008, Proceedings of the 23rd ACM SIGGRAPH/ EUROGRAPHICS Symposium on Graphics Hardware, GH '08, P85
   Teuhola J, 1998, REAL-TIME IMAGING, V4, P299, DOI 10.1006/rtim.1997.9999
   TORBORG J, 1996, P SIGGRAPH, P353
   WANG L, 2007, S INT 3D GRAPH GAM, P17
   WENNERSTEN P, 2009, EUROGRAPHICS 2009 CO, V28, P687
   WILSON R, 1984, IEEE INT C AC SPEECH, V9, P527
NR 24
TC 5
Z9 8
U1 0
U2 1
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2010
VL 26
IS 1
BP 17
EP 30
DI 10.1007/s00371-009-0372-y
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 526WD
UT WOS:000272325300002
DA 2024-07-18
ER

PT J
AU Papakonstantinou, S
   Brujic-Okretic, V
AF Papakonstantinou, Stelios
   Brujic-Okretic, Vesna
TI Framework for context-aware smartphone applications
SO VISUAL COMPUTER
LA English
DT Article
DE Context-awareness; Virtual and augmented reality; Mobile interactions
AB This paper presents research and development of a dedicated system architecture, designed to enable its users to interact with each other as well as to access information on points of interest that exist in their immediate environment. This is accomplished through managing personal preferences and contextual information in a distributed manner and in real time. The advantage of this system is that it uses mobile devices, heterogeneous sensors and a selection of user interface paradigms to produce a sociotechnical framework to enhance the perception of the environment and promote intuitive interactions. Representation of the real-world objects, their spatial relations and other captured features are visualised on scalable interfaces, ranging from 2D to 3D models and from photorealism to stylised clues and symbols. The conceptual design and implementation of our location and orientation based algorithm for mobile augmented reality is presented in detail. The framework is fit for use in unknown environments and therefore suitable for ubiquitous operation. The presented prototype is multifaceted and capable of supporting peer-to-peer exchange of information in a pervasive fashion, usable in various contexts. The modalities of these interactions are explored and laid out particularly in the context of entertainment and urban navigation.
C1 [Papakonstantinou, Stelios; Brujic-Okretic, Vesna] City Univ London, Dept Informat Sci, London EC1V 0HB, England.
C3 City University London
RP Papakonstantinou, S (corresponding author), City Univ London, Dept Informat Sci, London EC1V 0HB, England.
EM stelios@soi.city.ac.uk; vesna@soi.city.ac.uk
CR [Anonymous], 2005, GML C CAMERA CALIBRA
   Benford S, 2004, LECT NOTES COMPUT SC, V3205, P70
   BENFORD S, 2005, WP5 DESIGN EVALUATIO, P54
   Broll Wolfgang., 2006, Proceedings of 5th ACM SIGCOMM workshop on Network and system support for games, page, P28
   Burigat S., 2005, WEB3D 05 P 10 INT C, P57, DOI [DOI 10.1145/1050491.1050499, 10.1145/105041.1050499, DOI 10.1145/105041.1050499]
   CAPRA M, 2005, P 13 ANN ACM INT C M, P89
   CHEOK AD, 2006, COMPUT ENTERTAIN, V4, P20
   EYLES M, 2007, 3 INT C GAM RES DEV
   HALL EL, 1982, COMPUTER, V15, P42
   Hinske S., 2007, Concepts and Technologies for Pervasive Games-A Reader for Pervasive Gaming Research, V1, P1
   Klatzky R. L., 1998, Spatial Cognition. An Interdisciplinary Approach to Representing and Processing Spatial Knowledge, P1
   LIAROKAPIS F, 2006, GRAPP 2006 SPECIAL I, V3, P1
   Lindley C.A., 2004, P 2 ANN INT WORKSHOP, P10
   MAGERKURTH C, 2005, ACM COMPUT ENTERTAIN, V3, P19
   MILGRAM P, 1994, IEICE T INF SYST, V77, P15
   MONTOLA M, 2005, P DIG EXP DES AESTH, P4
   Mostéfaoui GK, 2004, IEEE/ACS INTERNATIONAL CONFERENCE ON PERVASIVE SERVICES, PROCEEDINGS, P39
   Nieuwdorp E., 2007, Computers in Entertainment, V5, P1
   Rashid Omer., 2006, COMPUTERS ENTERTAINM, V4, P4
   REITMAYR G, 2006, 5 IEEE ACM INT S MIX, P109
   Reitmayr G., 2004, P S LOCATION BASED S, P31
   ROECKER C, 2007, CONCEPTS TECHNOLOGIE, V1, P12
   SAHA D, 2003, IEEE COMPUT, V36
   Salvi J, 2002, PATTERN RECOGN, V35, P1617, DOI 10.1016/S0031-3203(01)00126-1
   TSAI RY, 1987, IEEE T ROBOTIC AUTOM, V3, P323, DOI 10.1109/JRA.1987.1087109
   WALTHER BK, 2005, P 2005 ACM SIGCHI IN, P176
   WEISER M, 1993, COMMUN ACM, V36, P75, DOI 10.1145/159544.159617
NR 27
TC 5
Z9 10
U1 0
U2 17
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2009
VL 25
IS 12
BP 1121
EP 1132
DI 10.1007/s00371-009-0391-8
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 510LC
UT WOS:000271089000008
DA 2024-07-18
ER

PT J
AU Diaz-Gutierrez, P
   Bösch, J
   Pajarola, R
   Gopi, M
AF Diaz-Gutierrez, Pablo
   Boesch, Jonas
   Pajarola, Renato
   Gopi, M.
TI Streaming surface sampling using Gaussian <i>ε</i>-nets
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Computer Graphics International Conference 2009
CY MAY 26-29, 2009
CL Victoria, CANADA
DE Normal quantization; Surface sampling; Shape approximation; Epsilon-nets
ID SIMPLIFICATION
AB We propose a robust, feature preserving and user-steerable mesh sampling algorithm, based on the one-to-many mapping of a regular sampling of the Gaussian sphere onto a given manifold surface. Most of the operations are local, and no global information is maintained. For this reason, our algorithm is amenable to a parallel or streaming implementation and is most suitable in situations when it is not possible to hold all the input data in memory at the same time. Using epsilon-nets, we analyze the sampling method and propose solutions to avoid shortcomings inherent to all localized sampling methods. Further, as a byproduct of our sampling algorithm, a shape approximation is produced. Finally, we demonstrate a streaming implementation that handles large meshes with a small memory footprint.
C1 [Diaz-Gutierrez, Pablo; Gopi, M.] Univ Calif Irvine, Irvine, CA 92697 USA.
   [Boesch, Jonas; Pajarola, Renato] Univ Zurich, Dept Informat, CH-8050 Zurich, Switzerland.
C3 University of California System; University of California Irvine;
   University of Zurich
RP Diaz-Gutierrez, P (corresponding author), Univ Calif Irvine, 6210 Donald Bren Hall, Irvine, CA 92697 USA.
EM pablo@ics.uci.edu; boesch@ifi.uzh.ch; pajarola@acm.org; gopi@ics.uci.edu
FU Direct For Computer & Info Scie & Enginr [0811809] Funding Source:
   National Science Foundation; Division of Computing and Communication
   Foundations [0811809] Funding Source: National Science Foundation
CR [Anonymous], 2006, ELEMENTARY DIFFERENT, DOI DOI 10.1090/CHEL/341/01
   [Anonymous], 2000, Journal of Geom. Graphics
   [Anonymous], 2006, Curve and surface reconstruction: algorithms with mathematical analysis
   Bernardini F, 1999, IEEE T VIS COMPUT GR, V5, P349, DOI 10.1109/2945.817351
   Clarkson K.L., 2006, SIGACT S
   Cohen-Steiner D, 2004, ACM T GRAPHIC, V23, P905, DOI 10.1145/1015706.1015817
   DONG W, 2000, P INT C MULT EXP ICM
   Garland M., 1997, PROC 24 C COMPUTER G, P209, DOI DOI 10.1145/258734.258849
   Gopi M, 2000, COMPUT GRAPH FORUM, V19, pC467, DOI 10.1111/1467-8659.00439
   Gruber PM, 2004, ADV MATH, V186, P456, DOI 10.1016/j.aim.2003.07.017
   Heckbert PS., 1997, SURVEY POLYGONAL SUR
   HOPPE H, 1996, SIGGRAPH 96, P99, DOI DOI 10.1145/237170.237216
   Isenburg M, 2005, IEEE Visualization 2005, Proceedings, P231
   Luebke DP, 2001, IEEE COMPUT GRAPH, V21, P24, DOI 10.1109/38.920624
   Nadler E., 1986, APPROXIMATION THEORY, P499
   Pajarola R, 2005, IEEE VISUALIZATION 2005, PROCEEDINGS, P239
   Sheffer A, 2001, COMPUT AIDED DESIGN, V33, P925, DOI 10.1016/S0010-4485(00)00116-0
   Vetterli M, 2002, IEEE T SIGNAL PROCES, V50, P1417, DOI 10.1109/TSP.2002.1003065
   WEILER K, 1985, IEEE COMPUT GRAPH, V5, P21, DOI 10.1109/MCG.1985.276271
   Yamauchi H, 2005, VISUAL COMPUT, V21, P659, DOI 10.1007/s00371-005-0319-x
NR 20
TC 2
Z9 2
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2009
VL 25
IS 5-7
BP 411
EP 421
DI 10.1007/s00371-009-0351-3
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 438ES
UT WOS:000265539300005
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Hou, F
   Qi, Y
   Shen, XK
   Yang, S
   Zhao, QP
AF Hou, Fei
   Qi, Yue
   Shen, Xukun
   Yang, Shen
   Zhao, Qinping
TI Automatic registration of multiple range images based on cycle space
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Computer Graphics International Conference 2009
CY MAY 26-29, 2009
CL Victoria, CANADA
DE Range image; Automatic registration; Graph; Cycle space
AB This paper proposes a method for integrating multiple range images obtained by a range scanner, aiming to get an entire model automatically. We extract texture images for all range images and register any pair of them based on image features of their texture images. To verify the correctness and filter out mismatching pairs of range images, multiview matching process searches a model graph for all the consistent cycles. We prove that the consistent cycle space is a linear subspace and the entire cycle space can be decomposed into the direct sum of a consistent cycle space and its orthogonal complementary subspace. Moreover, we present an efficient algorithm for computing a basis of the consistent cycle space based on adjacency-of-cycle-basis graph, which avoids the exponential complexity of exhaustive search.
C1 [Hou, Fei; Qi, Yue; Shen, Xukun; Yang, Shen; Zhao, Qinping] Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing, Peoples R China.
C3 Beihang University
RP Hou, F (corresponding author), Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing, Peoples R China.
EM houfei@vrlab.buaa.edu.cn
RI yang, shen/GWC-0424-2022; qi, yue/KLE-0386-2024
OI Shen, Xukun/0000-0001-8509-9393
CR Aiger D., 2008, SIGGRAPH 08 ACM SIGG, P1
   [Anonymous], 1997, THESIS CARNEGIE MELL
   Bendels G.H., 2004, VAST, P115
   BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791
   Bondy J. A., 1976, GRAPH THEORY APPL, V290
   CHEN Y, 1992, IMAGE VISION COMPUT, V10, P145, DOI 10.1016/0262-8856(92)90066-C
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Gelfand N., 2005, P 3 EUR S GEOM PROC, V2, P5
   HORN BKP, 1987, J OPT SOC AM A, V4, P629, DOI 10.1364/JOSAA.4.000629
   Huang QX, 2006, ACM T GRAPHIC, V25, P569, DOI 10.1145/1141911.1141925
   Huber DF, 2003, IMAGE VISION COMPUT, V21, P637, DOI 10.1016/S0262-8856(03)00060-X
   HUBER DF, 2002, THESIS CARNEGIE MELL
   LI X, 2005, S GEOM PROC, P217
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Mitra N.J., 2004, Symposium on Geometry Processing, P23
   Rusinkiewicz S, 2001, THIRD INTERNATIONAL CONFERENCE ON 3-D DIGITAL IMAGING AND MODELING, PROCEEDINGS, P145, DOI 10.1109/IM.2001.924423
NR 16
TC 5
Z9 7
U1 0
U2 7
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2009
VL 25
IS 5-7
BP 657
EP 665
DI 10.1007/s00371-009-0324-6
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 438ES
UT WOS:000265539300031
DA 2024-07-18
ER

PT J
AU Kovaleski, RP
   Oliveira, MM
AF Kovaleski, Rafael Pacheco
   Oliveira, Manuel M.
TI High-quality brightness enhancement functions for real-time reverse tone
   mapping
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Computer Graphics International Conference 2009
CY MAY 26-29, 2009
CL Victoria, CANADA
DE Brightness enhancement functions; Reverse tone mapping; Bilateral
   filtering
ID DISPLAYS; VIDEO
AB This paper presents an automatic technique for producing high-quality brightness-enhancement functions for real-time reverse tone mapping of images and videos. Our approach uses a bilateral filter to obtain smooth results while preserving sharp luminance discontinuities, and can be efficiently implemented on GPUs. We demonstrate the effectiveness of our approach by reverse tone mapping several images and videos. Experiments based on HDR visible difference predicator and on an image distortion metric indicate that the results produced by our method are less prone to visible artifacts than the ones obtained with the state-of-the-art technique for real-time automatic computation of brightness enhancement functions.
C1 [Kovaleski, Rafael Pacheco; Oliveira, Manuel M.] Univ Fed Rio Grande do Sul, Inst Informat, Porto Alegre, RS, Brazil.
C3 Universidade Federal do Rio Grande do Sul
RP Oliveira, MM (corresponding author), Univ Fed Rio Grande do Sul, Inst Informat, Av Bento Goncalves 9500,Caixa Postal 15-064, Porto Alegre, RS, Brazil.
EM rpkovaleski@inf.ufrgs.br; oliveira@inf.ufrgs.br
RI Menezes de Oliveira Neto, Manuel/H-1508-2011
OI Menezes de Oliveira Neto, Manuel/0000-0003-4957-9984
CR Akyüz AG, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1276377.1276425
   [Anonymous], 1995, P MUSTERERKENNUNG 19
   Aydin T. O., 2008, DYNAMIC RANGE INDEPE
   AYDIN TO, 2008, COMMUNICATION    OCT
   AYDIN TO, 2008, SIGGRAPH 08 ACM SIGG, P1
   Banterle F., 2006, P 4 INT C COMP GRAPH, P349
   BANTERLE F, 2008, P SPRING C COMP GRAP
   BURT PJ, 1983, IEEE T COMMUN, V31, P532, DOI 10.1109/TCOM.1983.1095851
   CHEN J, BILATERAL GRID SOURC
   Chen J, 2007, ACM T GRAPHIC, V26, DOI [10.1109/SARNOF.2007.4567317, 10.1145/1276377.1276506, 10.1145/1239451.1239554]
   Didyk P, 2008, COMPUT GRAPH FORUM, V27, P1265, DOI 10.1111/j.1467-8659.2008.01265.x
   HEIDRICH W, 2008, COMMUNICATION    APR
   Hoefflinger B, 2007, SPR SER ADV MICROELE, V26, P1, DOI 10.1007/978-3-540-44433-6
   Li YZ, 2005, ACM T GRAPHIC, V24, P836, DOI 10.1145/1073204.1073271
   Mantiuk R, 2004, IEEE SYS MAN CYBERN, P2763
   MANTIUK R, 2008, HDR VISUAL DIFFERENC
   MEYLAN L, 2006, IS T SID 14 COL IM C
   Reinhard E, 2002, ACM T GRAPHIC, V21, P267, DOI 10.1145/566570.566575
   Rempel AG, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239490
   Smith SM, 1997, INT J COMPUT VISION, V23, P45, DOI 10.1023/A:1007963824710
   Tomasi C, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P839, DOI 10.1109/ICCV.1998.710815
   Wang L., 2007, P EUR S REND
   Yoshida A, 2006, COMPUT GRAPH FORUM, V25, P415, DOI 10.1111/j.1467-8659.2006.00961.x
NR 23
TC 41
Z9 51
U1 0
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2009
VL 25
IS 5-7
BP 539
EP 547
DI 10.1007/s00371-009-0327-3
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 438ES
UT WOS:000265539300018
DA 2024-07-18
ER

PT J
AU Liu, X
   Gavrilova, ML
   Rokne, J
AF Liu, Xin
   Gavrilova, Marina L.
   Rokne, Jon
TI Incorporating object-centered sampling and Delaunay tetrahedrization for
   visual hull reconstruction
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Computer Graphics International Conference 2009
CY MAY 26-29, 2009
CL Victoria, CANADA
DE Visual hull; Object-centered sampling; Delaunay tetrahedrization;
   Tetrahedron peeling
AB In this paper we present a novel shape from silhouette algorithm. For an object to be modeled, the algorithm first computes a cloud of points located on a pencil of rays and distributed evenly on the visual hull surface, inside and outside the visual hull. Then Delaunay tetrahedrization is applied to the point cloud to partition its convex hull into a set of tetrahedrons. Finally, outlier tetrahedrons are removed by tetrahedron peeling, and a mesh model of the visual hull is extracted. The algorithm is robust, free from discretization artifacts, and produces a mesh model composed of well-shaped triangles.
C1 [Liu, Xin] Harbin Inst Technol, Sch Comp Sci & Technol, Comp Graph Project, Harbin, Peoples R China.
   [Liu, Xin; Gavrilova, Marina L.; Rokne, Jon] Univ Calgary, Dept Comp Sci, Calgary, AB T2N 1N4, Canada.
   [Liu, Xin] Harbin Inst Technol, Sch Comp Sci & Technol, Comp Vis R&D Project, Harbin, Peoples R China.
C3 Harbin Institute of Technology; University of Calgary; Harbin Institute
   of Technology
RP Liu, X (corresponding author), 2500 Univ Dr NW, Calgary, AB, Canada.
EM liuxin@ucalgary.ca
RI Gavrilova, Marina/AHD-3605-2022
OI Gavrilova, Marina/0000-0002-5338-1834
CR ALLARD J, 2004, MARKERLESS REAL TIME
   [Anonymous], THESIS STANFORD U
   Boyer E, 2003, PROC CVPR IEEE, P695
   BRESENHAM JE, 1965, IBM SYST J, V4, P25, DOI 10.1147/sj.41.0025
   FRANCO J, 2008, IEEE T PATTERN ANAL
   Franco J.-S., 2004, IEEE C COMPUTER VISI, P31
   LAURENTINI A, 1994, IEEE T PATTERN ANAL, V16, P150, DOI 10.1109/34.273735
   Lazebnik S, 2007, INT J COMPUT VISION, V74, P137, DOI 10.1007/s11263-006-0008-x
   Liang C, 2008, VISAPP 2008: PROCEEDINGS OF THE THIRD INTERNATIONAL CONFERENCE ON COMPUTER VISION THEORY AND APPLICATIONS, VOL 2, P597
   Liu X, 2006, INT C PATT RECOG, P9
   Liu X, 2008, GRAPH MODELS, V70, P133, DOI 10.1016/j.gmod.2006.06.003
   MARTIN WN, 1983, IEEE T PATTERN ANAL, V5, P150, DOI 10.1109/TPAMI.1983.4767367
   Matusik W, 2001, SPRING EUROGRAP, P115
   MULAYIM AY, 2000, P VIS MOD VIS NOV, P11
   Szeliski R., 1990, Real-time octree generation from rotating objects
NR 15
TC 6
Z9 6
U1 0
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2009
VL 25
IS 5-7
BP 381
EP 389
DI 10.1007/s00371-009-0329-1
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 438ES
UT WOS:000265539300002
DA 2024-07-18
ER

PT J
AU Paiva, A
   Brazil, EV
   Petronetto, F
   Sousa, MC
AF Paiva, Afonso
   Brazil, Emilio Vital
   Petronetto, Fabiano
   Sousa, Mario Costa
TI Fluid-based hatching for tone mapping in line illustrations
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Computer Graphics International Conference 2009
CY MAY 26-29, 2009
CL Victoria, CANADA
DE Non-photorealistic rendering; Pen and ink hatching; Direction fields;
   Smoothed particles hydrodynamics; Computational fluid dynamics
ID FLOWS
AB This paper presents a novel meshless, physically-based framework for line art rendering of surfaces with complex geometry and arbitrary topology. We apply an inviscid fluid flow simulation using Smoothed Particles Hydrodynamics to compute the global velocity and cross fields over the surface model. These fields guide the automatic placement of strokes while extracting the geometric and topological coherence of the model. Target tones are matched by tonal value maps allowing different hatching and cross-hatching effects. We demonstrate the simplicity and effectiveness of our method with sample renderings obtained for a variety of models.
C1 [Brazil, Emilio Vital] IMPA Inst Pure & Appl Math, Rio De Janeiro, Brazil.
   [Paiva, Afonso] Univ Sao Paulo, ICMC Inst Math & Comp Sci, Sao Carlos, SP, Brazil.
   [Petronetto, Fabiano] Pontificia Univ Catolica Rio de Janeiro, Dept Math, Rio de Janeiro, Brazil.
   [Sousa, Mario Costa] Univ Calgary, Dept Comp Sci, Calgary, AB T2N 1N4, Canada.
C3 Instituto Nacional de Matematica Pura e Aplicada (IMPA); Universidade de
   Sao Paulo; Pontificia Universidade Catolica do Rio de Janeiro;
   University of Calgary
RP Brazil, EV (corresponding author), IMPA Inst Pure & Appl Math, Rio De Janeiro, Brazil.
EM apneto@icmc.usp.br; emilio@impa.br; fbipetro@mat.puc-rio.br;
   mario@cpsc.ucalgary.ca
RI Macc, Inct/K-3440-2013; Petronetto, Fabiano/ABC-6521-2020; Petronetto,
   Fabiano/F-6064-2013; Paiva, Afonso/E-2593-2011
OI Petronetto, Fabiano/0000-0003-1940-5406; Paiva,
   Afonso/0000-0001-8229-3385; Costa Sousa, Mario/0000-0002-4347-0884
CR Elber G, 1998, IEEE T VIS COMPUT GR, V4, P71, DOI 10.1109/2945.675655
   Foster K, 2005, COMPUT GRAPH FORUM, V24, P267, DOI 10.1111/j.1467-8659.2005.00851.x
   GIRSHICK A, 2000, P 1 INT S NONPH AN R, P43, DOI DOI 10.1145/340916.340922
   Hertzmann A, 2000, COMP GRAPH, P517, DOI 10.1145/344779.345074
   Hodges ElaineR S., 2003, GUILD HDB SCI ILLUST, VSecond
   JEPP P, 2006, P THEOR PRACT COMP G, P39
   Karabassi E.-A., 1999, Journal of Graphics Tools, V4, P25, DOI 10.1080/10867651.1999.10487499
   Keiser R., 2005, Point-Based Graphics 2005 (IEEE Cat. No. 05EX1159), P125, DOI 10.1109/PBG.2005.194073
   Liu G.R., 2005, SMOOTHED PARTICLE HY
   MONAGHAN JJ, 1989, J COMPUT PHYS, V82, P1, DOI 10.1016/0021-9991(89)90032-6
   Morris JP, 1997, J COMPUT PHYS, V136, P214, DOI 10.1006/jcph.1997.5776
   Muller M., 2003, Proceedings of the 2003 ACM SIGGRAPH/Eurographics symposium on Computer animation, P154
   OHTAKE Y, 2002, J COMPUT INF SCI ENG, V2, P277
   Paiva A, 2006, SIBGRAPI, P205
   Paiva A, 2006, SIBGRAPI, P78
   PRAUN E, 2001, P SIGGRAPH 2001, P579
   RAWSON Philip -., 1987, Drawing, V3a
   ROSSL C, 2000, P WSCG 00, P168
   SECORD A, 2002, P 13 EUR WORKSH REND, P215, DOI DOI 10.1145/581924.581924
   Stam J, 2003, ACM T GRAPHIC, V22, P724, DOI 10.1145/882262.882338
   Stora D, 1999, PROC GRAPH INTERF, P203
   Turk G, 2001, COMP GRAPH, P347, DOI 10.1145/383259.383297
   Winkenbach G., 1994, Proceedings of the 21st Annual Conference on Computer Graphics and Interactive Techniques, P91
   Zhang E, 2006, ACM T GRAPHIC, V25, P1294, DOI 10.1145/1183287.1183290
NR 24
TC 6
Z9 6
U1 0
U2 1
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2009
VL 25
IS 5-7
BP 519
EP 527
DI 10.1007/s00371-009-0322-8
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 438ES
UT WOS:000265539300016
DA 2024-07-18
ER

PT J
AU Livny, Y
   Kogan, Z
   El-Sana, J
AF Livny, Yotam
   Kogan, Zvi
   El-Sana, Jihad
TI Seamless patches for GPU-based terrain rendering
SO VISUAL COMPUTER
LA English
DT Article
DE Terrain visualization; View-dependent rendering; Level-of-detail
   rendering; Hardware acceleration
ID VISUALIZATION
AB In this paper we present a novel approach for interactive rendering of large terrain datasets. Our approach is based on subdividing a terrain into rectangular patches at different resolutions. Each patch is represented by four triangular tiles that are selected form different resolutions, and four strips which are used to stitch the four tiles in a seamless manner. Such a scheme maintains resolution changes within patches through the stitching strips, and not across patches. At runtime, these patches are used to construct a level-of-detail representation of the input terrain based on view-parameters. A selected level of detail only includes the layout of the patches and their boundary edges resolutions. The layout includes the location and dimension of each patch. Within the graphics hardware, the GPU generates the meshes of the patches by using scaled instances of cached tiles and assigns elevation for each vertex from cached textures. Since adjacent rectangular patches agree on the resolution of the common edges, the resulted mesh does not include cracks or degenerate triangles. Our algorithm manages to achieve quality images at high frame rates while providing seamless transition between different levels of detail.
C1 [Livny, Yotam; Kogan, Zvi; El-Sana, Jihad] Ben Gurion Univ Negev, Dept Comp Sci, IL-84105 Beer Sheva, Israel.
C3 Ben Gurion University
RP Livny, Y (corresponding author), Ben Gurion Univ Negev, Dept Comp Sci, IL-84105 Beer Sheva, Israel.
EM livnyy@cs.bgu.ac.il; koganz@cs.bgu.ac.il; el-sana@cs.bgu.ac.il
FU Lynn and William Frankel Center for Computer Sciences; Tuman fund
FX This work is partially supported by the Lynn and William Frankel Center
   for Computer Sciences and the Tuman fund.
CR Asirvatham A., 2005, GPU GEMS 2, V2, P27
   Bao X., 2004, P VIS MOD VIS VMV, P413
   BOLZ J, 2005, EVALUATION SUB UNPUB
   Cignoni P, 2003, IEEE VISUALIZATION 2003, PROCEEDINGS, P147, DOI 10.1109/VISUAL.2003.1250366
   Cignoni P, 2004, ACM T GRAPHIC, V23, P796, DOI 10.1145/1015706.1015802
   Cignoni P, 2003, COMPUT GRAPH FORUM, V22, P505, DOI 10.1111/1467-8659.00698
   Cignoni P, 1997, VISUAL COMPUT, V13, P199, DOI 10.1007/s003710050099
   CohenOr D, 1996, IEEE VISUAL, P37, DOI 10.1109/VISUAL.1996.567600
   Cook R. L., 1984, Computers & Graphics, V18, P223
   DACHSBACHER C, 2004, EUR S GEOM PROC, P138
   De Floriani L, 1997, VISUALIZATION '97 - PROCEEDINGS, P103, DOI 10.1109/VISUAL.1997.663865
   DOGGETT M, 2000, HWWS 00, P59, DOI DOI 10.1145/346876.348220
   Döllner J, 2000, IEEE VISUAL, P227
   Duchaineau M, 1997, VISUALIZATION '97 - PROCEEDINGS, P81, DOI 10.1109/VISUAL.1997.663860
   El-Sana J, 1999, COMPUT GRAPH FORUM, V18, pC83, DOI 10.1111/1467-8659.00330
   Evans W, 2001, ALGORITHMICA, V30, P264, DOI 10.1007/s00453-001-0006-x
   GUMHOLD S, 1999, HWWS 99, P55, DOI DOI 10.1145/311534.311578
   HITCHNER LE, 1993, P SOC PHOTO-OPT INS, V1913, P622, DOI 10.1117/12.152736
   Hoppe H, 1998, VISUALIZATION '98, PROCEEDINGS, P35, DOI 10.1109/VISUAL.1998.745282
   Hoppe H, 1999, COMP GRAPH, P269, DOI 10.1145/311535.311565
   Hwa LM, 2004, IEEE VISUALIZATION 2004, PROCEEEDINGS, P219, DOI 10.1109/VISUAL.2004.4
   Lario Roberto., 2003, P VIIP 2003, P733
   Larsen BD, 2003, WSCG'2003, VOL 11, NO 2, CONFERENCE PROCEEDINGS, P282
   Levenberg J, 2002, VIS 2002: IEEE VISUALIZATION 2002, PROCEEDINGS, P259, DOI 10.1109/VISUAL.2002.1183783
   Lindstrom P, 2002, IEEE T VIS COMPUT GR, V8, P239, DOI 10.1109/TVCG.2002.1021577
   LINDSTROM Peter., 1996, P ACM SIGGRAPH 96, P109
   Livny Y, 2008, VISUAL COMPUT, V24, P139, DOI 10.1007/s00371-007-0180-1
   Losasso F, 2004, ACM T GRAPHIC, V23, P769, DOI 10.1145/1015706.1015799
   LOSASSO F, 2003, EUR S GEOM PROC, P138
   LUEBKE D, 1997, P SIGGRAPH 97, P199
   Molde K, 2002, PROC GRAPH INTERF, P171
   NVIDIA, 2004, IMPR BATCH US TEXT A
   Pajarola R, 1998, VISUALIZATION '98, PROCEEDINGS, P19, DOI 10.1109/VISUAL.1998.745280
   POMERANZ A., 2000, THESIS UC DAVIS
   Rabinovich B, 1997, VISUALIZATION '97 - PROCEEDINGS, P95, DOI 10.1109/VISUAL.1997.663863
   Schneider J, 2006, JOURNAL WSCG, V14, P49
   Southern R, 2003, COMPUT GRAPH FORUM, V22, P35, DOI 10.1111/1467-8659.t01-1-00644
   TANNER CHRISTOPHERC., 1998, P SIGGRAPH 1998, P151
   WAGNER D, 2004, TERRAIN GEOMORPHING
   Yoon SE, 2005, IEEE T VIS COMPUT GR, V11, P369, DOI 10.1109/TVCG.2005.64
NR 40
TC 36
Z9 48
U1 0
U2 18
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2009
VL 25
IS 3
BP 197
EP 208
DI 10.1007/s00371-008-0214-3
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 403RH
UT WOS:000263099200001
DA 2024-07-18
ER

PT J
AU Matsuoka, H
   Nakashima, Y
   Yoshimura, T
AF Matsuoka, Hosei
   Nakashima, Yusuke
   Yoshimura, Takeshi
TI Acoustic OFDM system and its extension
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 14th International Multimedia Modeling Conference (MMM 2008)
CY JAN 09-11, 2008
CL Kyoto Univ, Kyoto, JAPAN
HO Kyoto Univ
DE Data hiding; Steganography; OFDM
AB This paper presents a method of aerial acoustic communication in which data is modulated using OFDM (Orthogonal Frequency Division Multiplexing) and embedded in regular audio material without significantly degrading the quality of the original sound. It replaces the high frequency band of the audio signal with OFDM carriers, each of which is power-controlled according to the spectrum envelope of the original audio signal. The implemented system enables the transmission of short text messages from loudspeakers to mobile handheld devices at a distance of around 3 m. This paper also provides the subjective assessment results and the transmission performances.
RP Matsuoka, H (corresponding author), 3-5 Hikarinooka Yokosuka, Kanagawa, Japan.
EM matsuoka@nttdocomo.co.jp
CR Boney L, 1996, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON MULTIMEDIA COMPUTING AND SYSTEMS, P473, DOI 10.1109/MMCS.1996.535015
   GERASIMOV V, 2000, IBM SYST J
   Gruhl D., 1996, Information Hiding. First International Workshop Proceedings, P295
   *IRU R, 2001, 1534 ITUR
   JOHNSTON JD, 1988, IEEE J SEL AREA COMM, V6, P314, DOI 10.1109/49.608
   LOPES CV, 2001, IEEE WORKSH APPL SIG
   Matsuoka H, 2008, LECT NOTES COMPUT SC, V4903, P498
   PICKHOLTZ RL, 1982, IEEE T COMMUN, V30, P855, DOI 10.1109/TCOM.1982.1095533
   PLANK J, 1990, SOFTWARE PRACTICE EX, V27, P995
   Speth M, 1999, IEEE T COMMUN, V47, P1668, DOI 10.1109/26.803501
   WEINSTEIN SB, 1971, IEEE T COMMUN TECHN, VCO19, P628, DOI 10.1109/TCOM.1971.1090705
   YARDIMEI Y, 1994, EUROSPEECH97, P1679
NR 12
TC 10
Z9 11
U1 0
U2 1
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2009
VL 25
IS 1
BP 3
EP 12
DI 10.1007/s00371-008-0281-5
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 379RC
UT WOS:000261412900002
DA 2024-07-18
ER

PT J
AU Mei, T
   Yang, B
   Yang, SQ
   Hua, XS
AF Mei, Tao
   Yang, Bo
   Yang, Shi-Qiang
   Hua, Xian-Sheng
TI Video collage: presenting a video sequence using a single image
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 14th International Multimedia Modeling Conference (MMM 2008)
CY JAN 09-11, 2008
CL Kyoto Univ, Kyoto, JAPAN
HO Kyoto Univ
DE Video collage; Energy minimization; Video presentation
AB The explosive growth of video data demands the video presentation technique which supports fast browsing of video content. In this paper, we present an automatic procedure for constructing a compact synthesized collage from a video sequence. The synthesized image, called "Video Collage", is a kind of static video summary-to select the most representative images from video, to extract salient regions of interest (ROIs) from these images, and to seamlessly arrange ROIs on a given canvas with the temporal structure of video content preserved. We formulate the generation of Video Collage as a unified energy minimization problem in which each of above desirability is represented by an energy term. We start from the basic setting of Video Collage in which both the shape of ROIs and collage are fixed as rectangular, and then show how it can support arbitrary shapes of ROIs, as well as a variety of collage templates and region of interest (ROI) arrangement layouts (i.e., book, diagonal, and spiral). The experiments show its effectiveness to present a video in a very compact and visually appealing form while preserving the necessary information to understand the video.
C1 [Mei, Tao; Hua, Xian-Sheng] Microsoft Res Asia, Beijing, Peoples R China.
   [Yang, Bo; Yang, Shi-Qiang] Tsinghua Univ, Dept Comp Sci, Beijing 100084, Peoples R China.
C3 Microsoft Research Asia; Microsoft; Tsinghua University
RP Mei, T (corresponding author), Microsoft Res Asia, Beijing, Peoples R China.
EM tmei@microsoft.com; bo.yang02@gmail.com; yangshq@mail.tsinghua.edu.cn;
   xshua@microsoft.com
RI Mei, Tao/GQZ-0596-2022; yang, shiqiang/AAH-5484-2019
OI Mei, Tao/0000-0002-5990-7307; 
CR AGARWALA A, 2004, P ACM SIGGRAGPH
   [Anonymous], 2006, 2006 IEEE COMP SOC C
   [Anonymous], 2003, P 11 ACM INT C MULTI, DOI DOI 10.1145/957013.957094
   [Anonymous], 1993, Multimedia Systems, DOI DOI 10.1007/BF01210504
   Chen J.-C., 2006, P ACM MULT
   Chiu P, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXP (ICME), VOLS 1-3, P2059, DOI 10.1109/ICME.2004.1394670
   Diakopoulos N., 2005, P UIST, P183
   Girgensohn A, 2001, HUMAN-COMPUTER INTERACTION - INTERACT'01, P464
   GIRGENSOHN A, 2003, P 11 ACM INT C MULT, P92
   Graham J., 2003, P 11 ACM INT C MULTI, P94
   HUA XS, 2005, P ICME
   Irani M, 1998, P IEEE, V86, P905, DOI 10.1109/5.664279
   KANG HW, 2006, P IEEE C COMP VIS PA, P1331
   KAWAI Y, 2007, P CIVR
   Ma YF, 2005, 11TH INTERNATIONAL MULTIMEDIA MODELLING CONFERENCE, PROCEEDINGS, P94
   Ma YF, 2005, IEEE T MULTIMEDIA, V7, P907, DOI 10.1109/TMM.2005.854410
   MEI T, 2006, P ACM MULT
   MEI T, 2005, P IEEE INT C IM PROC, P861
   Mei T, 2007, IEEE T CIRC SYST VID, V17, P699, DOI 10.1109/TCSVT.2007.896640
   ROTHER C, 2006, P ACM SIGGR
   Smith MA, 1997, PROC CVPR IEEE, P775, DOI 10.1109/CVPR.1997.609414
   Taniguchi Y, 1997, ACM MULTIMEDIA 97, PROCEEDINGS, P427
   Uchihashi S, 1999, ACM MULTIMEDIA 99, PROCEEDINGS, P383, DOI 10.1145/319463.319654
   UEDA H, 1993, HUMAN FACTORS IN COMPUTING SYSTEMS, P137
   Wang G., 2006, P IEEE INT C COMP VI
   Wang T, 2007, 2007 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOLS 1-5, P1479
   WHITLEY D, 1994, STAT COMPUT, V4, P65, DOI 10.1007/BF00175354
   YANG B, 2008, INT C MULT MOD KYOT
   Yeung MM, 1997, IEEE T CIRC SYST VID, V7, P771, DOI 10.1109/76.633496
NR 29
TC 46
Z9 60
U1 0
U2 12
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2009
VL 25
IS 1
BP 39
EP 51
DI 10.1007/s00371-008-0282-4
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 379RC
UT WOS:000261412900005
DA 2024-07-18
ER

PT J
AU Zheng, YT
   Neo, SY
   Chua, TS
   Tian, Q
AF Zheng, Yan-Tao
   Neo, Shi-Yong
   Chua, Tat-Seng
   Tian, Qi
TI Toward a higher-level visual representation for object-based image
   retrieval
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 14th International Multimedia Modeling Conference (MMM 2008)
CY JAN 09-11, 2008
CL Kyoto Univ, Kyoto, JAPAN
HO Kyoto Univ
DE Visual representation; Object-based image retrieval; More
AB We propose a higher-level visual representation, visual synset, for object-based image retrieval beyond visual appearances. The proposed visual representation improves the traditional part-based bag-of-words image representation, in two aspects. First, the approach strengthens the discrimination power of visual words by constructing an intermediate descriptor, visual phrase, from frequently co-occurring visual word-set. Second, to bridge the visual appearance difference or to achieve better intra-class invariance power, the approach clusters visual words and phrases into visual synset, based on their class probability distribution. The rationale is that the distribution of visual word or phrase tends to peak around its belonging object classes. The testing on Caltech-256 data set shows that the visual synset can partially bridge visual differences of images of the same class and deliver satisfactory retrieval of relevant images with different visual appearances.
C1 [Zheng, Yan-Tao] Natl Univ Singapore, NUS Grad Sch Integrat Sci & Engn, Singapore 117548, Singapore.
   [Neo, Shi-Yong] Natl Univ Singapore, Dept Comp Sci, Singapore 117548, Singapore.
   [Chua, Tat-Seng] Natl Univ Singapore, Sch Comp, Singapore 117548, Singapore.
   [Tian, Qi] Inst Infocomm Res, Singapore, Singapore.
C3 National University of Singapore; National University of Singapore;
   National University of Singapore; Agency for Science Technology &
   Research (A*STAR); A*STAR - Institute for Infocomm Research (I2R)
RP Zheng, YT (corresponding author), Natl Univ Singapore, NUS Grad Sch Integrat Sci & Engn, Singapore 117548, Singapore.
EM yantaozheng@comp.nus.edu.sg; neoshiy@comp.nus.edu.sg;
   chuats@comp.nus.edu.sg; tian@i2r.a-star.edu.sg
RI Neo, Shiyong/GYD-6063-2022
CR AGARWAL A, 2006, ECCV INT WORKSH STAT
   [Anonymous], 1993, Proceedings of the 31st annual meeting on Association for Computational Linguistics, ACL '93, DOI [10.3115/981574.981598, DOI 10.3115/981574.981598]
   [Anonymous], 2004, P ICPR WORKSH LEARN
   [Anonymous], 2005, THESIS J GUTENBERG U
   Baker L. D., 1998, Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P96, DOI 10.1145/290941.290970
   Bekkerman R., 2003, Journal of Machine Learning Research, V3, P1183, DOI 10.1162/153244303322753625
   Carson C, 2002, IEEE T PATTERN ANAL, V24, P1026, DOI 10.1109/TPAMI.2002.1023800
   Dance C., 2004, P ECCV WORKSH STAT L
   DONOSER M, 2006, P C COMP VIS PATT RE, P553
   Faloutsos C., 1994, Journal of Intelligent Information Systems: Integrating Artificial Intelligence and Database Technologies, V3, P231, DOI 10.1007/BF00962238
   Grauman K, 2005, IEEE I CONF COMP VIS, P1458
   Griffin A., 2007, Caltech-256 object cate-gory dataset
   Gupta A, 1997, COMMUN ACM, V40, P70, DOI 10.1145/253769.253798
   HAN J, 2007, DATA MIN KNOW DISCOV, V14
   JING F, 2003, CIVR, P206
   JURIE F, 2005, P INT C COMP VIS WAS
   Kadir T, 2001, INT J COMPUT VISION, V45, P83, DOI 10.1023/A:1012460413855
   Lazebnik S., 2006, P IEEE CVF C COMP VI, DOI [DOI 10.1109/CVPR.2006.68, 10.1109/CVPR.2006.68]
   LI FF, 2004, P CVPR WORKSH WASH D
   Liu Y, 2007, PATTERN RECOGN, V40, P262, DOI 10.1016/j.patcog.2006.04.045
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Quack Till., 2007, ICCV
   SLONIM N, 2001, ADV NEURAL INFROM PR
   Smith J. R., 1996, Proceedings ACM Multimedia 96, P87, DOI 10.1145/244130.244151
   Squire D., 1999, CONTENT BASED QUERY
   Tishby N., 1999, P 37 ANN ALL C COMM, P368
   Wallraven C, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P257
   Wang JZ, 2001, IEEE T PATTERN ANAL, V23, P947, DOI 10.1109/34.955109
   Witten I.H., 1999, Managing Gigabytes: Compressing and Indexing Documents and Images
   Yuan J, 2007, PROC CVPR IEEE, P1930, DOI 10.1109/CVPR.2007.383222
   Zhang J, 2007, INT J COMPUT VISION, V73, P213, DOI 10.1007/s11263-006-9794-4
   ZHENG QF, 2006, P ACM INT C MULT SAN, P77
   Zheng Y., 2008, P IEEE C COMP VIS PA
NR 33
TC 21
Z9 21
U1 0
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2009
VL 25
IS 1
BP 13
EP 23
DI 10.1007/s00371-008-0294-0
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 379RC
UT WOS:000261412900003
DA 2024-07-18
ER

PT J
AU Van Haevre, W
   Van Laerhoven, T
   Di Fiore, F
   Van Reeth, F
AF Van Haevre, William
   Van Laerhoven, Tom
   Di Fiore, Fabian
   Van Reeth, Frank
TI From dust till drawn - A real-time bidirectional pastel simulation
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 25th Computer Graphics International Conference (CGI)
CY MAY 30-JUN 02, 2007
CL Petropolis, BRAZIL
DE graphics utilities; paint systems; interaction techniques;
   highly-stylized drawn images
AB We present a system for drawing pastel media in realtime as an effective alternative to most existing digital solutions that basically allow for drawing arbitrary strokes in a particular style. Our approach is focused on the simulation of the natural material itself and on its interaction with the drawing surface and the drawing tool. Upon free-form drawing, a bidirectional transfer of pigment takes place. In one direction, the paper surface is dusted with new pigment particles broken off the tip (i.e., the end of the drawing tool). A large part of these particles will be deposited or blended together with previously deposited ones whereas the remainder does not contribute to the drawing and is blown off. On the other hand, a certain amount of previously deposited pigment is scraped off and picked up again soiling the tip. This is noticeable in the next strokes to be drawn. Furthermore, both the tip and the paper surface are subject to weathering depending on the exerted pressure and friction of the drawing tool, and the bumpiness of the paper. As a result, the paper surface becomes slightly damaged, limiting the deposition of new pigment. The tip, on the other hand, becomes blunt making new strokes wider. From a stylistic point of view, similarly to traditional drawings our results convey the artists' characteristics (e.g., the way of wielding the brush, skillfulness, feeling for the medium). Therefore, we believe that our system allows an artist to create realistically looking pastel images without losing his/her personal touch.
C1 Univ Limburg, Hasselt Univ, Expertise Ctr Digital Media Transnatl, BE-3590 Diepenbeek, Belgium.
C3 Hasselt University
RP Van Haevre, W (corresponding author), Univ Limburg, Hasselt Univ, Expertise Ctr Digital Media Transnatl, Wetenschapspk 2, BE-3590 Diepenbeek, Belgium.
EM william.vanhaevre@uhasselt.be
OI VAN REETH, Frank/0000-0002-3705-7807; Di Fiore,
   Fabian/0000-0003-4908-0673
CR Gooch B., 2001, Non-photorealistic rendering
   Hebborn E., 2004, ART FORGERS HDB
   Kubelka P., 1931, Z TECH PHYS, V12
   MUDDE C, 2003, PASTEL CANTECLEER
   Murakami K, 2005, Computer Graphics International 2005, Proceedings, P156
   Murakami K, 2006, VISUAL COMPUT, V22, P415, DOI 10.1007/s00371-006-0021-7
   REYNOLDS C, 2004, STYLIZED DEPICTION C
   Rudolf D, 2005, COMPUT GRAPH FORUM, V24, P27, DOI 10.1111/j.1467-8659.2005.00826.x
   Rudolf D, 2003, 11TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P163, DOI 10.1109/PCCGA.2003.1238258
   Strothotte T, 2002, NONPHOTOREALISTIC CO
   Worley S., 1996, P 23 ANN C COMPUTER, P291, DOI [DOI 10.1145/237170.237267, 10.1145/237170.237267]
NR 11
TC 11
Z9 16
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2007
VL 23
IS 9-11
SI SI
BP 925
EP 934
DI 10.1007/s00371-007-0144-5
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 206UE
UT WOS:000249207600033
DA 2024-07-18
ER

PT J
AU Rolim, C
   Frery, AC
   Almeida, E
   Costa, E
   Gonçalves, LM
AF Rolim, Cledja
   Frery, Alejandro C.
   Almeida, Eliana
   Costa, Evandro
   Goncalves, Luiz Marcos
TI Enhancing the experience of 3D virtual worlds with a cartographic
   generalization approach
SO VISUAL COMPUTER
LA English
DT Article
DE cartographic generalization; virtual worlds; simplification; artificial
   intelligence; image processing
AB In this work we propose a new approach for fast visualization and exploration of virtual worlds based on the use of cartographic concepts and techniques. Versions of cartographic maps with different levels of details can be created by using a set of operations named cartographic generalization. Cartographic generalization employs twelve operators and domain-specific knowledge, being the contribution of this work their transposition to 3D virtual worlds. The architecture of a system for 3D generalization is proposed and the system is implemented. Differently from traditional cartographic processes, we use artificial intelligence for both selecting the key objects and applying the operators. As a case study, we present the simplification of the historical quarter of Recife (Brazil).
C1 Ctr Fed Educ Tecnol Alagoas, Maceio, AL, Brazil.
   Univ Fed Alagoas, CPMAT IC, BR-57072970 Maceio, AL, Brazil.
   Univ Fed Alagoas, IC, BR-57072970 Maceio, AL, Brazil.
   Univ Fed Rio Grande do Norte, DCA CT, BR-59072970 Natal, RN, Brazil.
C3 Universidade Federal de Alagoas; Universidade Federal de Alagoas;
   Universidade Federal do Rio Grande do Norte
RP Rolim, C (corresponding author), Ctr Fed Educ Tecnol Alagoas, Av Barao Atalaia S-N, Maceio, AL, Brazil.
EM cledja@yahoo.com.br; acfrery@pesquisador.cnpq.br;
   eliana.almeida@pesquisador.cnpq.br; ebc@fapeal.br; lmarcos@dca.ufrn.br
RI Gonçalves, Luiz Marcos Garcia/C-3786-2009; Frery, Alejandro/A-8855-2008;
   DE ALMEIDA, ELIANA SILVA/AAR-7966-2020
OI Gonçalves, Luiz Marcos Garcia/0000-0002-7735-5630; Frery,
   Alejandro/0000-0002-8002-5341; 
CR [Anonymous], THESIS U N CAROLINA
   [Anonymous], 2001, ARTIFICIAL INTELLIGE
   Bourdakis V., 1998, Virtual Worlds. First International Conference, VW'98. Proceedings, P345
   BOURDAKIS V, 1997, VIRTUAL REALITY COMM, P45
   BOURDAKIS V, 2001, ARCHITECTURAL INFORM, P404
   COHENOR D, 2000, P EUROGRAPHICS 00 CO
   Constantinescu Z., 2000, NONLINEAR ANAL-MODEL, V5, P39
   *ESRI, 1996, AUT MAP GEN CUTT EDG
   Frery AC, 2002, CYBERPSYCHOL BEHAV, V5, P451, DOI 10.1089/109493102761022878
   FRERY AC, 2004, 20 INT SOC PHOT REM, P200
   GLOVER E, 1999, ICA ACI 1999 P ACT O, P1175
   Guéziec A, 1999, IEEE COMPUT GRAPH, V19, P68, DOI 10.1109/38.749125
   Jain A. K., 1989, Fundamentals of Digital Image Processing
   Joao E.M., 1998, CAUSES CONSEQUENCES
   Lim J.S., 1989, 2 DIMENSIONAL SIGNAL
   LUEBKE D, 2003, M KAUFMANN SERIES CO
   Luebke DP, 2001, IEEE COMPUT GRAPH, V21, P24, DOI 10.1109/38.920624
   McMaster R.B., 1992, GEN DIGITAL CARTOGRA
   MENG L, 1997, AUTOMATIC GEN GEOGRA
   RAMOS AL, 1997, VRML 2 0 SOURCEBOOK
   REDDY M, 1995, VIRTUAL REALITY RES, V1, P85
   RODRIGUEZBACHIL.A, 2003, EXPERT SYSTEMS GEOGR
   ROLIM C, 2003, P SVR 6 S VIRT REAL, P369
   Russell S., 2016, Artificial intelligence a modern approach
   *SUN MICR I, 2006, SOURC JAVA TECHN, V1
   VIEIRA W, 2003, P SIBGRAPI 2003 16 B, P27
NR 26
TC 2
Z9 2
U1 0
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2007
VL 23
IS 6
BP 409
EP 418
DI 10.1007/s00371-007-0099-6
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 165BC
UT WOS:000246277800003
DA 2024-07-18
ER

PT J
AU Hanawa, D
   Yonekura, T
AF Hanawa, Dai
   Yonekura, Tatsuhiro
TI Improvement on the accuracy of the polynomial form extrapolation model
   in distributed virtual environment
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT International Conference on Cyberworlds (CW 2006)
CY NOV 28-29, 2006
CL Lausanne, SWITZERLAND
SP EPFL, VRlab
DE distributed virtual environment; update interval; extrapolation; error
   model; numerical difference
ID CONSISTENCY
AB In this paper, we studied the relationship between the accuracy of the extrapolating data and the update interval in a distributed virtual environment (DVE). Based on the properties of the polynomial models, we proposed the new method to extrapolate the attribute data which arrives at a discrete time period. Theoretical models were formulated and showed that the average error of the proposed method is less than that of current methods. Finally, we confirmed that the proposed method can improve the accuracy in comparison with current methods by conducting experiments with the motion of a pen for a series of letters written by a human.
C1 Tokyo Univ Agr & Technol, Grad Sch Engn, Dept Comp & Informat Sci, Major Field Ubiquitous & Universal Informat Envir, Tokyo 1848588, Japan.
   Ibaraki Univ, Fac Engn, Dept Comp & Informat Sci, Ibaraki 3168511, Japan.
C3 Tokyo University of Agriculture & Technology; Ibaraki University
RP Hanawa, D (corresponding author), Tokyo Univ Agr & Technol, Grad Sch Engn, Dept Comp & Informat Sci, Major Field Ubiquitous & Universal Informat Envir, 2-24-16 Nakao Cho, Tokyo 1848588, Japan.
EM hanawa@cc.tuat.ac.jp; yone@mx.ibaraki.ac.jp
CR BERGLUND EJ, 1985, IEEE SOFTWARE, V2, P30, DOI 10.1109/MS.1985.230698
   Capin TK, 1999, IEEE T CIRC SYST VID, V9, P411, DOI 10.1109/76.754769
   Delaney D, 2006, PRESENCE-TELEOP VIRT, V15, P465, DOI 10.1162/pres.15.4.465
   Ellis S. R., 2002, Proceedings of the Human Factors and Ergonomics Society 46th Annual Meeting, P2149
   Fritsch T, 2005, P 4 ACM SIGCOMM WORK, P1
   FUJINOKI H, 2006, ACM SIGCHI P NETG 20
   Gutwin Carl., 2003, Proceedings of the 2003 International ACM SIGGROUP Conference on Supporting Group Work, P294
   Hagsand O, 1996, IEEE MULTIMEDIA, V3, P30, DOI 10.1109/93.486702
   Hanawa D, 2005, 2005 INTERNATIONAL CONFERENCE ON CYBERWORLDS, PROCEEDINGS, P279
   Hanawa D, 2004, P IEEE VIRT REAL ANN, P227, DOI 10.1109/VR.2004.1310082
   HANAWA D, 2005, ELECT INT J ADV MODE, V7, P85
   Hanawa D, 2006, 2006 INTERNATIONAL CONFERENCE ON CYBERWORLDS, PROCEEDINGS, P107, DOI 10.1109/CW.2006.10
   HIKICHI K, 2002, IEEE P 16 INT WORKSH, P218
   IEEE (Institute of Electrical and Electronics Engineers), 1993, 12781993 ANSIIEEE
   KAMADA M, 2004, ELECT INT J ADV MODE, V6, P37
   Macedonia M.R., 1994, PRESENCE, V3, P265, DOI 10.1162/pres.1994.3.4.265
   Marshall D, 2006, IEEE ACM DIS SIM, P77
   Neumaier Arnold, 2001, Introduction to Numerical Analysis
   Ohlenburg J, 2004, P IEEE VIRT REAL ANN, P83, DOI 10.1109/VR.2004.1310059
   Pozrikidis C., 1998, NUMERICAL COMPUTATIO
   Roberts D, 2005, IEEE ACM DIS SIM, P195, DOI 10.1109/DISTRA.2005.21
   Singhal S., 1999, Networked Virtual Environments
   SINGHAL SK, 1995, PRESENCE-TELEOP VIRT, V4, P169, DOI 10.1162/pres.1995.4.2.169
   SINGHAL SK, 2006, THESIS STANFORD U
   YASUI T, 2005, P 4 ACM SIGCOMM WORK
   YONEKURA T, 2004, I ELECT INFORM COMMU, V87, P2209
   Zhou S., 2004, ACM Transactions on Modeling and Computer Simulation, V14, P31, DOI 10.1145/974734.974736
NR 27
TC 3
Z9 3
U1 0
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2007
VL 23
IS 5
BP 369
EP 379
DI 10.1007/s00371-007-0109-8
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 154JS
UT WOS:000245503600008
DA 2024-07-18
ER

PT J
AU Glardon, P
   Boulic, R
   Thalmann, D
AF Glardon, P
   Boulic, R
   Thalmann, D
TI Robust on-line adaptive footplant detection and enforcement for
   locomotion
SO VISUAL COMPUTER
LA English
DT Article
DE motion anticipation; animation with constraints; human body simulation
ID MOTION; ANIMATION; WALKING
AB A common problem in virtual character computer animation concerns the preservation of the basic foot-floor constraint (or footplant), consisting in detecting it before enforcing it. This paper describes a system capable of generating motion while continuously preserving the footplants for a real-time, dynamically evolving context. This system introduces a constraint detection method that improves classical techniques by adaptively selecting threshold values according to motion type and quality. The footplants are then enforced using a numerical inverse kinematics solver. As opposed to previous approaches, we define the footplant by attaching to it two effectors whose position at the beginning of the constraint can be modified, in order to place the foot on the ground, for example. However, the corrected posture at the constraint beginning is needed before it starts to ensure smoothness between the unconstrained and constrained states. We, therefore, present a new approach based on motion anticipation, which computes animation postures in advance, according to time-evolving motion parameters, such as locomotion speed and type. We illustrate our on-line approach with continuously modified locomotion patterns, and demonstrate its ability to correct motion artifacts, such as foot sliding, to change the constraint position and to modify from a straight to a curved walk motion.
C1 Ecole Polytech Fed Lausanne, Virtual Real Lab, CH-1015 Lausanne, Switzerland.
C3 Swiss Federal Institutes of Technology Domain; Ecole Polytechnique
   Federale de Lausanne
RP Ecole Polytech Fed Lausanne, Virtual Real Lab, CH-1015 Lausanne, Switzerland.
EM pascal.glardon@epfl.ch
RI BOULIC, RONAN/A-9108-2008; Thalmann, Daniel/A-4347-2008; Thalmann,
   Daniel/AAL-1097-2020
OI BOULIC, RONAN/0000-0001-9176-6877; Thalmann, Daniel/0000-0002-0451-7491
CR [Anonymous], P 22 ANN C COMP GRAP
   Baerlocher P, 2004, VISUAL COMPUT, V20, P402, DOI 10.1007/s00371-004-0244-4
   Bindiganavale R, 1998, LECT NOTES ARTIF INT, V1537, P70
   Boulic R., 2004, J. Game Develop., V1, P29
   Bruderlin A, 1996, PROC GRAPH INTERF, P213
   Bruderlin Armin., 1995, Proceedings of the 22nd Annual Conference on Computer Graphics and Interactive Techniques, SIGGRAPH '95, P97, DOI DOI 10.1145/218380.218421
   Butz M.V., 2003, ANTICIPATORY BEHAV A, V2684
   Choi KJ, 2000, J VISUAL COMP ANIMAT, V11, P223, DOI 10.1002/1099-1778(200012)11:5<223::AID-VIS236>3.0.CO;2-5
   Conde T, 2004, COMPUT ANIMAT VIRT W, V15, P311, DOI 10.1002/cav.34
   Girard M., 1987, Proceedings of the 1986 Workshop on Interactive 3D Graphics, P131, DOI 10.1145/319120.319131
   Glardon P., 2004, Proceedings of Computer Animation and Social Agent, P73
   Gleicher M., 1997, Proceedings 1997 Symposium on Interactive 3D Graphics, P139, DOI 10.1145/253284.253321
   Gleicher M, 2001, GRAPH MODELS, V63, P107, DOI 10.1006/gmod.2001.0549
   Hreljac A, 2000, J BIOMECH, V33, P783, DOI 10.1016/S0021-9290(00)00014-2
   Ko H, 1996, IEEE COMPUT GRAPH, V16, P50, DOI 10.1109/38.486680
   KOVAR L, 2003, P ACM SIGGRAPH EUR S, P214
   Kovar Lucas., 2002, SCA 2002: Proceedings of the 2002 ACM SIG-GRAPH/Eurographics Symposium on Computer Animation, P97
   LECALLENNEC B, 2004, P ACM SIGGRAPH EUR S
   Lee J., 2000, COMPUTER GRAPHICS P, DOI [10.1145/311535.311539, DOI 10.1145/311535.311539]
   Multon F, 1999, J VISUAL COMP ANIMAT, V10, P39, DOI 10.1002/(SICI)1099-1778(199901/03)10:1<39::AID-VIS195>3.0.CO;2-2
   PARK SI, 2002, P ACM SIGGRAPH EUR S
   Reynolds C. W., 1999, P GAM DEV C, P763
   Rose C, 1998, IEEE COMPUT GRAPH, V18, P32, DOI 10.1109/38.708559
   Shin HJ, 2001, ACM T GRAPHIC, V20, P67, DOI 10.1145/502122.502123
   SUN HC, 2001, P ACM SIGGRAPH ANN C
   Unuma M., 1995, P 22 ANN C COMPUTER, P91, DOI DOI 10.1145/218380.218419
   vandePanne M, 1997, COMPUT GRAPH FORUM, V16, P211, DOI 10.1111/1467-8659.00181
   WOOTEN WL, 2000, P IEEE INT C ROB AUT
   Yamane K, 2003, IEEE T VIS COMPUT GR, V9, P352, DOI 10.1109/TVCG.2003.1207443
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
NR 43
TC 17
Z9 19
U1 0
U2 1
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2006
VL 22
IS 3
BP 194
EP 209
DI 10.1007/s00371-006-0376-9
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 028UP
UT WOS:000236514600005
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Zhang, L
   Razdan, A
   Farin, G
   Femiani, J
   Bae, M
   Lockwood, C
AF Zhang, L
   Razdan, A
   Farin, G
   Femiani, J
   Bae, M
   Lockwood, C
TI 3D face authentication and recognition based on bilateral symmetry
   analysis
SO VISUAL COMPUTER
LA English
DT Article
DE face authentication; face recognition; symmetry analysis; 3D faces
AB We present a novel and computationally fast method for automatic human face authentication. Taking a 3D triangular facial mesh as input, the approach first automatically extracts the bilateral symmetry plane of the facial surface. The intersection between the symmetry plane and the facial surface, namely the symmetry profile, is then computed. Using both the mean curvature plot of the facial surface and the curvature plot of the symmetry profile curve, three essential points of the nose on the symmetry profile are automatically extracted. The three essential points uniquely determine a Face Intrinsic Coordinate System (FICS). Different faces are aligned based on the FICS. The symmetry profile, together with two transverse profiles, composes a compact representation, called the SFC representation, of a 3D face surface. The face authentication and recognition steps are finally performed by comparing the SFC representations of the faces. The proposed method was tested on 382 face surfaces, which come from 166 individuals and cover a wide ethnic and age variety. The equal error rate (EER) of face authentication on scans with variable facial expressions is 10.8%. For scans with normal expression, the ERR is 0.8%.
C1 Nanjing Univ Aeronaut & Astronaut, Nanjing 210016, Peoples R China.
   Arizona State Univ, PRISM, Tempe, AZ 85287 USA.
   UCL, Dept Anthropol, London WC1E 6BT, England.
C3 Nanjing University of Aeronautics & Astronautics; Arizona State
   University; Arizona State University-Tempe; University of London;
   University College London
RP Nanjing Univ Aeronaut & Astronaut, Nanjing 210016, Peoples R China.
EM zhangly@nuaa.edu.cn; razdan@asu.edu; gerald.farin@asu.edu;
   john.femiani@asu.edu; myungsoo.bae@asu.edu; c.lockwood@ucl.ac.uk
CR [Anonymous], 1989, WORKSH INT 3D SCEN
   Benz M, 2002, VISION MODELING, AND VISUALIZATION 2002, PROCEEDINGS, P43
   BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791
   Beumier C, 2000, IMAGE VISION COMPUT, V18, P315, DOI 10.1016/S0262-8856(99)00052-9
   Beumier C, 2001, PATTERN RECOGN LETT, V22, P1321, DOI 10.1016/S0167-8655(01)00077-0
   Blanz V, 2003, IEEE T PATTERN ANAL, V25, P1063, DOI 10.1109/TPAMI.2003.1227983
   BOWYER K, 2004, SURVEY 3D MULTI MODA
   BRONSTEIN A, 2003, P AUD VID BAS BIOM P, P62
   CHANG K, 2003, P ACM WORKSH MULT US, P25
   Chin-Seng Chua, 2000, Proceedings Fourth IEEE International Conference on Automatic Face and Gesture Recognition (Cat. No. PR00580), P233, DOI 10.1109/AFGR.2000.840640
   HALLINAN P, 1999, 2 THREE DIMENSIONAL
   Hamann B., 1993, GEOMETRIC MODELLING, P139, DOI DOI 10.1007/978-3-7091-6916-2_10
   Kazhdan M, 2004, ALGORITHMICA, V38, P201, DOI 10.1007/s00453-003-1050-5
   Lee MW, 2003, PATTERN RECOGN, V36, P1835, DOI 10.1016/S0031-3203(03)00008-6
   Liu YX, 2003, COMPUT VIS IMAGE UND, V91, P138, DOI 10.1016/S1077-3142(03)00078-X
   MINOVIC P, 1992, 11TH IAPR INTERNATIONAL CONFERENCE ON PATTERN RECOGNITION, PROCEEDINGS, VOL I, P457, DOI 10.1109/ICPR.1992.201599
NR 16
TC 66
Z9 76
U1 0
U2 9
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2006
VL 22
IS 1
BP 43
EP 55
DI 10.1007/s00371-005-0352-9
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 991NT
UT WOS:000233821400005
DA 2024-07-18
ER

PT J
AU Di Blasi, G
   Gallo, G
AF Di Blasi, G
   Gallo, G
TI Artificial mosaics
SO VISUAL COMPUTER
LA English
DT Article
DE mosaic; nonphotorealistic rendering; distance transform; image
   processing and enhancement
AB Art often provides valuable insight that can be applied to technological innovations, especially in the fields of image processing and computer graphics. In this paper we present a method to transform a raster input image into a good-quality mosaic: an "artificial mosaic." The creation of mosaics of artistic quality is challenging because the tiles that compose a mosaic, typically small polygons, must be packed tightly and yet must follow and emphasize orientations chosen by the artist. The proposed method can reproduce the colors of the original image and emphasize relevant boundaries by placing tiles along edge directions. No user intervention is needed to detect the boundaries: they are automatically detected using a simple but effective image processing technique. Several examples reported in the paper show how the right mixture of mathematical tools together with time-tested ideas of mosaicists may lead to impressive results.
C1 Univ Catania, DMI, I-95125 Catania, Italy.
C3 University of Catania
RP Univ Catania, DMI, Viale A Doria 6, I-95125 Catania, Italy.
EM gdiblasi@dmi.unict.it; gallo@dmi.unict.it
OI Gallo, Giovanni/0000-0002-6701-0620
CR Allison W., 2002, P 2 INT S NONPHOTORE, P21
   DIBLASI G, 2003, ARTIFICIAL MOSAIC CR
   DOBASHI J, 2002, P EUR
   Elber G, 2003, VISUAL COMPUT, V19, P67, DOI 10.1007/s00371-002-0175-x
   Gooch B., 2002, P 2 INT S NONPH AN R, P83
   Haeberli P., 1990, P 17 ANN C COMP GRAP, P207, DOI [10.1145/97879.97902, DOI 10.1145/97879.97902]
   Haralick R. M., 1992, COMPUTER ROBOT VISIO, V1
   Hausner A, 2001, COMP GRAPH, P573, DOI 10.1145/383259.383327
   Kaplan CS, 2000, COMP GRAPH, P499, DOI 10.1145/344779.345022
   Kim J, 2002, ACM T GRAPHIC, V21, P657
   King Sonia., 2003, MOSAIC TECHNIQUES TR
   Marr D., 1982, Vision
   Meer P, 2001, IEEE T PATTERN ANAL, V23, P1351, DOI 10.1109/34.977560
   NITTOLO F, 2004, MOSAICO
   Silvers R., 1997, Photomosaics
   TUMMINELLO S, 2003, DESCRIZIONE TECNICA
NR 16
TC 29
Z9 32
U1 2
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2005
VL 21
IS 6
BP 373
EP 383
DI 10.1007/s00371-005-0292-4
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 952GD
UT WOS:000230991100002
DA 2024-07-18
ER

PT J
AU Kubiesa, S
   Ugail, H
   Wilson, M
AF Kubiesa, S
   Ugail, H
   Wilson, M
TI Interactive design using higher order PDEs
SO VISUAL COMPUTER
LA English
DT Article
DE interactive design; PDE surfaces; higher order PDEs
AB This paper extends the PDE method of surface generation. The governing partial differential equation is generalised to sixth order to increase its flexibility. The PDE is solved analytically, even in the case of general boundary conditions, making the method fast. The boundary conditions, which control the surface shape, are specified interactively, allowing intuitive manipulation of generic shapes. A compact user interface is presented which makes use of direct manipulation and other techniques for 3D interaction.
C1 Univ Leeds, Dept Appl Math, Leeds LS2 9JT, W Yorkshire, England.
   Univ Bradford, Dept Elect Imaging & Media Commun, Sch Informat, Bradford BD7 1DP, W Yorkshire, England.
C3 University of Leeds; University of Bradford
RP Univ Leeds, Dept Appl Math, Leeds LS2 9JT, W Yorkshire, England.
CR [Anonymous], 1999, OpenGL programming guide: the official guide to learning OpenGL
   Astley OR, 1998, IEEE INT CONF ROBOT, P989, DOI 10.1109/ROBOT.1998.677216
   Baraff D., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P43, DOI 10.1145/280814.280821
   BIER EA, 1986, WORKSH INT 3D GRAPH, P183
   Bloor MIG, 2000, PHYS REV E, V61, P4218, DOI 10.1103/PhysRevE.61.4218
   Bloor MIG, 1996, COMPUT AIDED DESIGN, V28, P145, DOI 10.1016/0010-4485(95)00060-7
   BLOOR MIG, 2004, IN PRESS COMPUT AIDE
   DEKANSKI CW, 1995, J SHIP RES, V39, P108
   Desbrun M, 1996, COMPUT GRAPH FORUM, V15, P319, DOI 10.1111/1467-8659.1550319
   Du HX, 2000, EIGHTH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P213, DOI 10.1109/PCCGA.2000.883943
   Duffy D., 2015, GREENS FUNCTIONS APP
   FARIN G, 1998, CURVES SURFACES COMP
   Foley JamesD., 1997, COMPUTER GRAPHICS PR
   FOWLER B, 1992, COMPUTER GRAPHICS 19, V25, P101
   HOSCHEK J, 1993, COMPUTER AIDED GEOME
   James DL, 1999, COMP GRAPH, P65, DOI 10.1145/311535.311542
   O'Brien JF, 1999, COMP GRAPH, P137, DOI 10.1145/311535.311550
   Picinbono G, 2003, GRAPH MODELS, V65, P305, DOI 10.1016/S1524-0703(03)00045-6
   RAPPOPORT A, 1994, ACM T GRAPHIC, V13, P156, DOI 10.1145/176579.176582
   RUNCIMAN C, 1986, INT J MAN MACH STUD, V25, P439, DOI 10.1016/S0020-7373(86)80070-0
   Stam J, 1997, COMPUT GRAPH FORUM, V16, pC159, DOI 10.1111/1467-8659.00152
   TERZOPOULOS D, 1994, ACM T GRAPHIC, V13, P103, DOI 10.1145/176579.176580
   Thimbleby H., 1990, USER INTERFACE DESIG
   Ugail H, 1999, COMPUT GRAPH-UK, V23, P525, DOI 10.1016/S0097-8493(99)00071-0
   Ugail H, 1999, ACM T GRAPHIC, V18, P195, DOI 10.1145/318009.318078
   Van Emmerik M.J., 1990, INTERACTIVE DESIGN P
   vanDijk CGC, 1997, COMPUT IND, V34, P125, DOI 10.1016/S0166-3615(96)00073-5
   XIE H, 2001, P INT C SHAP MOD APP
   Yamashina H, 1996, COMPUT INTEGR MANUF, V9, P9, DOI 10.1016/0951-5240(95)00027-5
NR 29
TC 14
Z9 18
U1 0
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2004
VL 20
IS 10
BP 682
EP 693
DI 10.1007/s00371-004-0261-3
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 876TH
UT WOS:000225520000006
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Li, GQ
   Ma, WY
   Bao, HJ
AF Li, GQ
   Ma, WY
   Bao, HJ
TI √2 subdivision for quadrilateral meshes
SO VISUAL COMPUTER
LA English
DT Article
DE subdivision surface; root 2 subdivision; 4-8 subdivision; quadrilateral
   mesh; surface modeling
ID B-SPLINE SURFACES; EXTRAORDINARY POINTS; SCHEMES; SMOOTHNESS; ALGORITHMS
AB This paper presents a root2subdivision scheme for quadrilateral meshes that can be regarded as an extension of a 4-8 subdivision with new subdivision rules and improved capability and performance. The proposed scheme adopts a so-called root2split operator to refine a control mesh such that the face number of the refined mesh generally equals the edge number and is thus about twice the face number of the coarse mesh. Smooth rules are designed in reference to the 4-8 subdivision, while a new set of weights is developed to balance the flatness of surfaces at vertices of different valences. Compared to the 4-8 subdivision, the presented scheme can be naturally generalized for arbitrary control nets and is more efficient in both space and computing time management. Analysis shows that limit surfaces produced by the scheme are C-4 continuous for regular control meshes and G(1) continuous at extraordinary vertices.
C1 City Univ Hong Kong, Dept Mfg Engn & Engn Management, Hong Kong, Peoples R China.
   Zhejiang Univ, State Key Lab CAD&CG, Hangzhou 310027, Peoples R China.
C3 City University of Hong Kong; Zhejiang University
RP City Univ Hong Kong, Dept Mfg Engn & Engn Management, Hong Kong, Peoples R China.
EM gqli@cad.zju.edu.cn; mewma@cityu.edu.hk; bao@cad.zju.edu.cn
RI MA, Weiyin/K-9155-2015
OI MA, Weiyin/0000-0001-9760-7789
CR BALL AA, 1988, ACM T GRAPHIC, V7, P83, DOI 10.1145/42458.42459
   BOLZ J, 2001, P WEB3D 02 TEMP AZ 2, P11
   CATMULL E, 1978, COMPUT AIDED DESIGN, V10, P350, DOI 10.1016/0010-4485(78)90110-0
   DOO D, 1978, COMPUT AIDED DESIGN, V10, P356, DOI 10.1016/0010-4485(78)90111-2
   DYN N, 1990, ACM T GRAPHIC, V9, P160, DOI 10.1145/78956.78958
   Habib A, 1999, COMPUT AIDED GEOM D, V16, P223, DOI 10.1016/S0167-8396(98)00045-4
   HALSTEAD M, 1993, COMPUT GRAPH, V20, P35
   Kobbelt L, 1996, COMPUT GRAPH FORUM, V15, pC409, DOI 10.1111/1467-8659.1530409
   KOBBELT L, 2000, COMPUT GRAPH, V27, P103
   Labsik U, 2000, COMPUT GRAPH FORUM, V19, pC131, DOI 10.1111/1467-8659.00405
   LANE JM, 1980, IEEE T PATTERN ANAL, V2, P35, DOI 10.1109/TPAMI.1980.4766968
   Loop C, 1987, THESIS U UTAH
   Maillot J, 2001, COMPUT GRAPH FORUM, V20, pC471, DOI 10.1111/1467-8659.00540
   Oswald P, 2003, COMPUT AIDED GEOM D, V20, P135, DOI [10.1016/S0167-8396(03)00026-8, 10.1016/s0167-8396(03)00026-8]
   Peters J, 1998, SIAM J NUMER ANAL, V35, P728, DOI 10.1137/S0036142996304346
   Peters J, 1997, ACM T GRAPHIC, V16, P420, DOI 10.1145/263834.263851
   Prautzsch H., 2000, International Journal of Shape Modeling, V6, P21, DOI 10.1142/S0218654300000041
   Prautzsch H, 1998, COMPUTER GRAPHICS INTERNATIONAL, PROCEEDINGS, P626, DOI 10.1109/CGI.1998.694321
   Prautzsch H, 1998, COMP SUPPL, V13, P217
   Prautzsch H, 1998, ADV COMPUT MATH, V9, P377, DOI 10.1023/A:1018945708536
   REIF U, 1995, COMPUT AIDED GEOM D, V12, P153, DOI 10.1016/0167-8396(94)00007-F
   Stam J, 2001, COMPUT AIDED GEOM D, V18, P383, DOI 10.1016/S0167-8396(01)00038-3
   STAM J, 1998, P ACM SIGGRAPH ORL F, V25, P395
   STAM J, 1998, P ACM SIGGRAPH COMP
   Umlauf G, 2000, CONSTR APPROX, V16, P145, DOI 10.1007/s003659910006
   Velho L, 2001, COMPUT AIDED GEOM D, V18, P397, DOI 10.1016/S0167-8396(01)00039-5
   Warren J., 2001, SUBDIVISION METHODS
   Zorin D, 2000, SIAM J NUMER ANAL, V37, P1677, DOI 10.1137/S003614299834263X
   Zorin D, 2001, COMPUT AIDED GEOM D, V18, P429, DOI 10.1016/S0167-8396(01)00040-1
   Zorin D, 2000, CONSTR APPROX, V16, P359, DOI 10.1007/s003659910016
   ZORIN D, 1998, THESIS CALTECH PASAD
NR 31
TC 24
Z9 33
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2004
VL 20
IS 2-3
BP 180
EP 198
DI 10.1007/s00371-003-0238-7
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 818ZV
UT WOS:000221283900007
DA 2024-07-18
ER

PT J
AU Jou, SB
   Tsai, MD
AF Jou, SB
   Tsai, MD
TI A fast 3D seed-filling algorithm
SO VISUAL COMPUTER
LA English
DT Article
DE object fill; seed search; volume graphics
AB The 3D seed-filling algorithm that fills consecutive object voxels at a time has shown higher efficiency than the method of filling only one voxel at a time. However, it searches seeds for filled voxels already containing no seeds. This paper presents a fast 3D seed-filling algorithm that uses a 2D pointer array of linked lists to avoid the redundant seed searches. The linked lists record the spans of filled voxels. Five comparison cases determine the current filling span, and the neighboring spans for searching seeds are either non-overlapping, or completely or partially overlapping. Seed searches are executed only for the non-overlapping span or part (in the case of the partial overlapping span) to minimize the searches. The experimental results show that the proposed algorithm is effective in eliminating the redundant seed searches and achieves high efficiency.
C1 Chung Yuan Christian Univ, Dept Informat & Comp Engn, Chungli 32023, Taiwan.
C3 Chung Yuan Christian University
RP Chung Yuan Christian Univ, Dept Informat & Comp Engn, Chungli 32023, Taiwan.
EM tsai@ice.cycu.edu.tw
CR ALBERT TA, 1995, COMPUT GRAPH, V19, P541, DOI 10.1016/0097-8493(95)00032-8
   Feng L, 1998, COMPUT GRAPH-UK, V22, P641, DOI 10.1016/S0097-8493(98)00073-9
   FISHKIN KP, 1985, GRAPH INT P MONTR 27
   FREUND J, 1997, P IEEE VIS 97 PHOEN
   Frisken-Gibson SF, 1999, IEEE T VIS COMPUT GR, V5, P333, DOI 10.1109/2945.817350
   GONZALEZ E, 1996, IEEE INT C ROB AUT M
   LEVOY M, 1990, ACM T GRAPHIC, V9, P245, DOI 10.1145/78964.78965
   Liu S, 1999, COMPUT AIDED DESIGN, V31, P517, DOI 10.1016/S0010-4485(99)00050-0
   Oikarinen J, 1998, COMPUT GRAPH-UK, V22, P745, DOI 10.1016/S0097-8493(98)00095-8
   Oikarinen JT, 1998, COMPUT NETWORKS ISDN, V30, P2003, DOI 10.1016/S0169-7552(98)00177-9
   SHAREEF N, 1995, P 3 ACM S SOL MOD AP
   SUAREZ A, 1995, P INT VEH 95 S IEEE
   Tsai MD, 2001, COMPUT BIOL MED, V31, P333, DOI 10.1016/S0010-4825(01)00014-2
   TSAI MD, 2001, 9 PAC C COMP GRAPH A
   YU YW, 1999, IEEE INT C SYST MAN
   ZHOU Y, 2000, 8 PAC C COMP GRAPH A
NR 16
TC 12
Z9 12
U1 1
U2 14
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2003
VL 19
IS 4
BP 243
EP 251
DI 10.1007/s00371-003-0192-4
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 713KV
UT WOS:000184857800003
DA 2024-07-18
ER

PT J
AU Salehi, E
   Aghagolzadeh, A
   Hosseini, R
AF Salehi, Erfan
   Aghagolzadeh, Ali
   Hosseini, Reshad
TI Stereo-RSSF: stereo robust sparse scene-flow estimation
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Sparse scene-flow; Inlier detection; Coarse and fine decision; Spatial
   correlation analysis; Disparity; Flow estimation
AB Scene-flow (SF) estimation is considered to be one of the most fundamental problems in scene understanding and autonomous control. The majority of the existing methods adopted for SF estimation suffer lack of robustness in some environments and cannot be easily applied for high-speed applications such as autonomous driving. Although some of the available methods are precise, they include high computational costs or require a GPU. The most serious challenge faced in SF estimation is its inability to strike a balance between speed, precision, robustness, and the computational costs. This paper, therefore, aims at proposing a novel sparse scene-flow (stereo-RSSF) method which is highly distinguished in terms of its faster speed, robustness, and precision using the following: stereo calibrated frames, sparse optical flow such as the LKT algorithm, a new inlier detection module based on spatial correlation analysis, epipolar geometry, and modified circular matching techniques. The comparisons made between stereo-RSSF and several advanced methods indicate that this sparse method has significantly higher accuracy than all the other state-of-the-art methods in the points it estimates. In this paper, the effects of each module and hyper-parameters of stereo-RSSF on the performance and running time are analyzed. Stereo-RSSF has also been evaluated on the KITTI test dataset, and the results have been independently verified by the reference group. The code for our implementation of stereo-RSSF is available at: https://github.com/salehierfan/Stereo-RSSF.
C1 [Salehi, Erfan; Aghagolzadeh, Ali] Babol Noshirvani Univ Technol, Fac Elect & Comp Engn, Babol, Iran.
   [Hosseini, Reshad] Univ Tehran, Coll Engn, Sch ECE, Tehran, Iran.
C3 Babol Noshirvani University of Technology; University of Tehran
RP Aghagolzadeh, A (corresponding author), Babol Noshirvani Univ Technol, Fac Elect & Comp Engn, Babol, Iran.
EM e.salehi@stu.nit.ac.ir; aghagol@nit.ac.ir; reshad.hosseini@ut.ac.ir
RI Hosseini, Reshad/AAD-8561-2021
OI Hosseini, Reshad/0000-0002-3669-760X
FU Babol Noshirvani University of Technology [BNUT/389059/1401]; Babol
   Noshirvani University of Technology
FX This work was supported by Babol Noshirvani University of Technology
   through grant program no. BNUT/389059/1401.
CR Badki A, 2021, PROC CVPR IEEE, P12941, DOI 10.1109/CVPR46437.2021.01275
   Baker S, 2004, INT J COMPUT VISION, V56, P221, DOI 10.1023/B:VISI.0000011205.11775.fd
   Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014
   Cleju I, 2010, VISUAL COMPUT, V26, P1407, DOI 10.1007/s00371-010-0427-0
   Dehne F., 1986, Vis. Comput, V2, P31, DOI [10.1007/BF01890985, DOI 10.1007/BF01890985]
   Feurer M, 2019, SPRING SER CHALLENGE, P3, DOI 10.1007/978-3-030-05318-5_1
   Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074
   Golyanik V, 2017, INT CONF 3D VISION, P273, DOI 10.1109/3DV.2017.00039
   Gong MG, 2002, INT J COMPUT VISION, V47, P63, DOI 10.1023/A:1014529404956
   Jiang HZ, 2019, IEEE I CONF COMP VIS, P3194, DOI 10.1109/ICCV.2019.00329
   Kim W, 2022, J VIS COMMUN IMAGE R, V85, DOI 10.1016/j.jvcir.2022.103523
   Li CC, 2021, INT C PATT RECOG, P3876, DOI 10.1109/ICPR48806.2021.9413289
   Liu Y, 2005, HANDBOOK OF IMAGE AND VIDEO PROCESSING, 2ND EDITION, P297, DOI 10.1016/B978-012119792-6/50081-4
   Lu RR, 2020, VISUAL COMPUT, V36, P253, DOI 10.1007/s00371-018-1605-8
   Ma WC, 2019, PROC CVPR IEEE, P3609, DOI 10.1109/CVPR.2019.00373
   Martull S., 2012, P ICPR WORKSHOP TRAK, V111, P117
   Menze M, 2018, ISPRS J PHOTOGRAMM, V140, P60, DOI 10.1016/j.isprsjprs.2017.09.013
   Menze M, 2015, PROC CVPR IEEE, P3061, DOI 10.1109/CVPR.2015.7298925
   Morel JM, 2009, SIAM J IMAGING SCI, V2, P438, DOI 10.1137/080732730
   Peng XD, 2022, PLOS ONE, V17, DOI 10.1371/journal.pone.0276436
   Rao SN, 2023, VISUAL COMPUT, V39, P835, DOI 10.1007/s00371-021-02349-2
   Revaud J, 2015, PROC CVPR IEEE, P1164, DOI 10.1109/CVPR.2015.7298720
   Rublee E, 2011, IEEE I CONF COMP VIS, P2564, DOI 10.1109/ICCV.2011.6126544
   Schuster R., 2018, Commercial Vehicle Technology, P90
   Schuster R, 2018, IEEE WINT CONF APPL, P1056, DOI 10.1109/WACV.2018.00121
   Schuster R, 2020, INT J COMPUT VISION, V128, P527, DOI 10.1007/s11263-019-01258-1
   Seitz SM, 1999, INT J COMPUT VISION, V35, P151, DOI 10.1023/A:1008176507526
   Song X, 2022, INT J COMPUT VISION, V130, P226, DOI 10.1007/s11263-021-01549-6
   Szeliski R, 2008, IEEE T PATTERN ANAL, V30, P1068, DOI 10.1109/TPAMI.2007.70844
   Teed Z, 2021, PROC CVPR IEEE, P8371, DOI 10.1109/CVPR46437.2021.00827
   Vedula S, 2005, IEEE T PATTERN ANAL, V27, P475, DOI 10.1109/TPAMI.2005.63
   Vogel C, 2015, INT J COMPUT VISION, V115, P1, DOI 10.1007/s11263-015-0806-0
   Wang GM, 2022, IEEE T INSTRUM MEAS, V71, DOI 10.1109/TIM.2022.3166147
   Wang JL, 2021, IEEE IMAGE PROC, P3198, DOI 10.1109/ICIP42928.2021.9506419
   Wang ZR, 2020, IEEE WINT CONF APPL, P91, DOI 10.1109/WACV45572.2020.9093302
   Wannenwetsch AS, 2017, IEEE I CONF COMP VIS, P1182, DOI 10.1109/ICCV.2017.133
   Yamazaki I, 2010, VISUAL COMPUT, V26, P1421, DOI 10.1007/s00371-010-0428-z
   Yang GS, 2021, PROC CVPR IEEE, P1266, DOI 10.1109/CVPR46437.2021.00132
   Zikiou N, 2020, VISUAL COMPUT, V36, P1473, DOI 10.1007/s00371-019-01753-z
NR 39
TC 0
Z9 0
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2023 DEC 11
PY 2023
DI 10.1007/s00371-023-03143-y
EA DEC 2023
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AL3M5
UT WOS:001118581200001
DA 2024-07-18
ER

PT J
AU Vasavi, S
   Pravallika, MS
   Varun, BN
   Sarma, AS
AF Vasavi, S.
   Pravallika, M. Sai
   Varun, B. Naga
   Sarma, A. Sashikant
TI Residual network-based ocean wave modelling from satellite images using
   ensemble Kalman filter
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Satellite images; Data assimilation; Adaptive filter; Internal wave
   modelling; Residual network; Adaptive thresholding
ID DATA ASSIMILATION
AB Nonlinear ocean waves have a significant impact on the functioning of several offshore activities. Predicting the internal ocean waves plays a crucial role on submarine and ship operations. Data assimilation is a mechanism in which data observed is interpreted, processed and adapted. The existing works for estimating the future atmospheric condition are highly dependent on the exact initial state, which mostly differ from the observation. This paper proposes modelling of internal ocean waves using automatic internal wave detection and data assimilation. Ensemble Kalman filtering method is used to model ocean waves. The proposed system is focused on satellite images. The images are pre-processed for speckle noise using adaptive filters. Enhanced residual network is used for edge detection. Unlike the existing edge detection methods that have high complexity, this enhanced residual network works with low complexity and makes a direct mapping between the input wave image and wave edge. Finally, the potential edges of the internal wave are detected and adapted using ensemble Kalman filter. Adaptive thresholding technique is used to determine the appropriate threshold to segregate objects from background. The proposed enhanced edge detection model is compared w.r.t to the parameters weighted cross-entropy loss function, accuracy and root mean squared error with canny edge detection and proved to be better. The detection of internal wave is demonstrated, and the accuracy of the approach is 91% with low RMSE when compared to existing works.
C1 [Vasavi, S.; Pravallika, M. Sai; Varun, B. Naga] Velagapudi Ramakrishna Siddhartha Engn Coll, Dept Comp Sci & Engn, Vijayawada, Andhra Pradesh, India.
   [Sarma, A. Sashikant] ISRO, Space Applicat Ctr, Ahmadabad, India.
C3 Velagapudi Ramakrishna Siddhartha Engineering College; Department of
   Space (DoS), Government of India; Indian Space Research Organisation
   (ISRO); Space Applications Centre (SAC)
RP Vasavi, S (corresponding author), Velagapudi Ramakrishna Siddhartha Engn Coll, Dept Comp Sci & Engn, Vijayawada, Andhra Pradesh, India.
EM vasavi.movva@gmail.com; mynenisp1703@gmail.com;
   nagavarunbatina007@gmail.com; sasharma@sac.isro.gov.in
OI , Dr S.Vasavi/0000-0002-3025-5528
CR Agushaka JO, 2023, NEURAL COMPUT APPL, V35, P4099, DOI 10.1007/s00521-022-07854-6
   Al-Amaren A, 2021, IEEE ACCESS, V9, P74646, DOI 10.1109/ACCESS.2021.3078411
   Aragh S, 2008, INT OFFSHORE POLAR E, P565
   Bocaniov SA, 2014, LIMNOLOGICA, V49, P52, DOI 10.1016/j.limno.2014.08.004
   Caires S, 2018, J WATERW PORT COAST, V144, DOI 10.1061/(ASCE)WW.1943-5460.0000439
   Cheng SB, 2023, IEEE-CAA J AUTOMATIC, V10, P1361, DOI 10.1109/JAS.2023.123537
   Divya C., 2020, 2020 3 INT C ADV EL, P1, DOI [10.1109/ICAECC50550.2020.9339511, DOI 10.1109/ICAECC50550.2020.9339511]
   European Space agency, ERS 1 and ERS2 SAR
   Galanis G, 2009, OCEAN DYNAM, V59, P523, DOI 10.1007/s10236-009-0191-8
   Houghton IA, 2023, OCEAN MODEL, V183, DOI 10.1016/j.ocemod.2023.102200
   Kumar MA, 2020, J INTELL SYST, V29, P1360, DOI 10.1515/jisys-2017-0509
   Ming YS, 2016, IEEE T IMAGE PROCESS, V25, P3597, DOI 10.1109/TIP.2016.2564646
   Mohaghegh F, 2020, Arxiv, DOI arXiv:2007.15250
   Moore AM, 2019, FRONT MAR SCI, V6, DOI 10.3389/fmars.2019.00090
   Murphy KP, 2012, MACHINE LEARNING: A PROBABILISTIC PERSPECTIVE, P1
   Ponte RM, 2019, FRONT MAR SCI, V6, DOI 10.3389/fmars.2019.00437
   Qu K, 2019, ACTA OCEANOL SIN, V38, P183, DOI 10.1007/s13131-019-1422-6
   Reddy Siva, A Study on Global Ocean Analysis from Ocean Data Assimilation System and its Sensitivity to Observations and Forcing Fields, P17, DOI [10.13140/RG.2.1.4459.4326, DOI 10.13140/RG.2.1.4459.4326]
   Rodenas JA, 1998, IEEE T GEOSCI REMOTE, V36, P1494, DOI 10.1109/36.718853
   Roger Labbe K., 2014, Github, P498
   Sai Pravallika M., Lecture Notes in Networks and Systems, V288, DOI [10.1007/978-981-16-5120-5_7, DOI 10.1007/978-981-16-5120-5_7]
   Saulter AN, 2020, FRONT MAR SCI, V7, DOI 10.3389/fmars.2020.579834
   Shao Q, 2021, J GEOPHYS RES-OCEANS, V126, DOI 10.1029/2021JC017515
   Simonin D, 2009, INT J REMOTE SENS, V30, P4581, DOI 10.1080/01431160802621218
   Vasavi S, 2021, IEEE SENS J, V21, P11417, DOI 10.1109/JSEN.2020.3007883
   WienerNorbert J., 1949, Extrapolation, Interpolation, and Smoothing of Stationary Time Series
   Xue QQ, 2023, VISUAL COMPUT, DOI 10.1007/s00371-023-03117-0
   Zagoruyko S, 2017, Arxiv, DOI arXiv:1605.07146
   Zardoua Y, 2023, VISUAL COMPUT, V39, P197, DOI 10.1007/s00371-021-02321-0
   Zhang XD, 2023, ARTIFICIAL INTELLIGENCE OCEANOGRAPHY, P83, DOI 10.1007/978-981-19-6375-9_4
   Zhang XD, 2021, IEEE T GEOSCI REMOTE, V59, P2822, DOI 10.1109/TGRS.2020.3008067
NR 31
TC 0
Z9 0
U1 1
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2023 NOV 30
PY 2023
DI 10.1007/s00371-023-03169-2
EA NOV 2023
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AZ4K2
UT WOS:001122252600002
DA 2024-07-18
ER

PT J
AU Chen, H
   Li, NN
   Chen, R
AF Chen, Hui
   Li, Nannan
   Chen, Rong
TI Ni-DehazeNet: representation learning via bilevel optimized architecture
   search for nighttime dehazing
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Representation learning; Neural architecture search; Nighttime dehazing
ID COLOR TRANSFER
AB Nighttime dehazing is a challenging ill-posed problem due to the severe haze pollution and color attenuation. Since available daytime dehazing approaches cannot be consistently adapted to the nighttime case, this paper specifically designs representation learning to enhance an end-to-end nighttime dehazing network. To explore attentive and problem-driven representations, we propose to realize effective learning via knowledge distillation together with neural architecture search (NAS), which are implemented via two parallel branches of networks, namely, representation learning network (RLN) and guided dehazing network (GDN). The challenges in nighttime dehazing come from distilling the clear elements from the hazy representation and the restoration of real color from ambient illumination or low light condition. To this end, we introduce the subnetwork of RLN to learn the feature of the clear elements of the ground truth. We specifically introduce a NAS for exploring the color subspace of the latent embedding space of both RLN and GDN. With the guidance from the representation learned by RLN, GDN could well learn the clear elements from the hazy image and recover the real color of it effectively. Extensive experimental results on the synthetic and real-world datasets demonstrate the superiority of our method over the state-of-the-arts in nighttime dehazing.
C1 [Chen, Hui; Li, Nannan; Chen, Rong] Dalian Maritime Univ, Coll Informat Sci & Technol, Linghai Rd, Dalian 116026, Liaoning, Peoples R China.
C3 Dalian Maritime University
RP Li, NN; Chen, R (corresponding author), Dalian Maritime Univ, Coll Informat Sci & Technol, Linghai Rd, Dalian 116026, Liaoning, Peoples R China.
EM chenhuidmu@163.com; nannanli@dlmu.edu.cn; rchen@dlmu.edu.cn
FU National Natural Science Foundation of China [62002039, 61672122,
   61802045]; National Natural Science Foundation of China [36330603];
   Fundamental Research Funds for the Central Universities
FX This work is supported by the National Natural Science Foundation of
   China (No. 62002039, No. 61672122, No. 61802045), and the Fundamental
   Research Funds for the Central Universities (No.36330603).
CR Chen H, 2023, VISUAL COMPUT, V39, P5279, DOI 10.1007/s00371-022-02659-z
   Chen TY, 2021, IEEE COMPUT SOC CONF, P487, DOI 10.1109/CVPRW53098.2021.00060
   Codruta AO, 2020, IEEE T IMAGE PROCESS, V29, P2653, DOI 10.1109/TIP.2019.2951304
   He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168
   Hong M, 2020, PROC CVPR IEEE, P3459, DOI 10.1109/CVPR42600.2020.00352
   Jiang B, 2018, MULTIMED TOOLS APPL, V77, P3125, DOI 10.1007/s11042-017-4954-9
   Jin D, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P2401, DOI 10.1145/3474085.3475404
   Kuanar S, 2019, Arxiv, DOI arXiv:1902.00855
   Li RT, 2020, PROC CVPR IEEE, P3172, DOI 10.1109/CVPR42600.2020.00324
   Li XL, 2023, VISUAL COMPUT, V39, P663, DOI 10.1007/s00371-021-02365-2
   Li XP, 2023, NEUROCOMPUTING, V544, DOI 10.1016/j.neucom.2023.126242
   Li Y, 2015, IEEE I CONF COMP VIS, P226, DOI 10.1109/ICCV.2015.34
   Liao YH, 2018, LECT NOTES COMPUT SC, V11164, P469, DOI 10.1007/978-3-030-00776-8_43
   Liu HX, 2019, Arxiv, DOI [arXiv:1806.09055, DOI 10.48550/ARXIV.1806.09055]
   Liu RS, 2021, PROC CVPR IEEE, P10556, DOI 10.1109/CVPR46437.2021.01042
   Liu RS, 2022, IEEE T PATTERN ANAL, V44, P10045, DOI 10.1109/TPAMI.2021.3132674
   Liu XN, 2022, IEEE T MULTIMEDIA, V24, P3934, DOI 10.1109/TMM.2021.3110483
   Liu X, 2019, PROC CVPR IEEE, P7000, DOI 10.1109/CVPR.2019.00717
   Liu Y, 2023, IEEE T CIRC SYST VID, V33, P1643, DOI 10.1109/TCSVT.2022.3214430
   Liu Y, 2021, SIGNAL PROCESS, V183, DOI 10.1016/j.sigpro.2021.107986
   Pei SC, 2012, IEEE IMAGE PROC, P957, DOI 10.1109/ICIP.2012.6467020
   Quan RJ, 2021, PROC CVPR IEEE, P9143, DOI 10.1109/CVPR46437.2021.00903
   Thomas J., 2021, INT C INF PROC, P29
   Wang WH, 2022, IEEE T IMAGE PROCESS, V31, P1349, DOI 10.1109/TIP.2022.3141252
   Wending Yan, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P473, DOI 10.1007/978-3-030-58610-2_28
   Wu HY, 2021, PROC CVPR IEEE, P10546, DOI 10.1109/CVPR46437.2021.01041
   Wu HY, 2020, IEEE COMPUT SOC CONF, P1975, DOI 10.1109/CVPRW50498.2020.00247
   Yang MM, 2018, IEEE T MULTIMEDIA, V20, P3008, DOI 10.1109/TMM.2018.2820327
   Yin JL, 2020, IEEE T CIRC SYST VID, V30, P3957, DOI 10.1109/TCSVT.2019.2917315
   Yu YK, 2021, IEEE COMPUT SOC CONF, P193, DOI 10.1109/CVPRW53098.2021.00028
   Zhang J, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2355, DOI 10.1145/3394171.3413763
   Zhang J, 2017, PROC CVPR IEEE, P7016, DOI 10.1109/CVPR.2017.742
   Zhang J, 2014, IEEE IMAGE PROC, P4557, DOI 10.1109/ICIP.2014.7025924
   Zhang SD, 2020, VISUAL COMPUT, V36, P1797, DOI 10.1007/s00371-019-01774-8
   Zhu QS, 2015, IEEE T IMAGE PROCESS, V24, P3522, DOI 10.1109/TIP.2015.2446191
NR 35
TC 0
Z9 0
U1 6
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2023 NOV 28
PY 2023
DI 10.1007/s00371-023-03159-4
EA NOV 2023
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA Y9DM7
UT WOS:001108193100001
DA 2024-07-18
ER

PT J
AU Joshi, BM
   Bhavsar, H
AF Joshi, Barkha M.
   Bhavsar, Hetal
TI A nightshade crop leaf disease detection using enhance-nightshade-CNN
   for ground truth data
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Background removal methods; Enhance-nightshade-CNN (proposed model);
   Disease identification; Model complexity
ID LINES
AB In the fast-growing agricultural world, early detection of plant diseases is crucial for maintaining crop health and ensuring successful harvests. Advancements in computer vision technology have led to the development of advanced methods for diagnosing plant diseases. However, factors like lighting, weather, and the number of diseases in a single image can make it difficult to detect plant diseases. Traditional deep learning-based algorithms have drawbacks, such as high hardware investment, inference speed, and generalization. This research article aims to raise awareness among farmers about cutting-edge technology for detecting plant leaf disease in nightshade crops. The Enhance-Nightshade-CNN model was used to enhance the quality of nightshade crop leaf disease samples, achieving good accuracy compared to existing algorithms. The model accurately identified healthy and unhealthy leaves in the real environment, with ground truth results showing a 95-100% accuracy rate.
C1 [Joshi, Barkha M.] Sardar Vallabhbhai Patel Inst Technol, Comp Engn Dept, Vasad, India.
   [Bhavsar, Hetal] Maharaja Sayajirao Univ Baroda, Comp Sci & Engn Dept, Vadodara, India.
C3 Sardar Vallabhbhai Patel Institute of Technology; Maharaja Sayajirao
   University Baroda
RP Joshi, BM (corresponding author), Sardar Vallabhbhai Patel Inst Technol, Comp Engn Dept, Vasad, India.
EM barkhajoshi.comp@svitvasad.ac.in; hetal.bhavsar-cse@msubaroda.ac.in
RI Joshi, Barkha M./GPT-3465-2022; Joshi, Barkha/KPY-6755-2024
OI Joshi, Barkha M./0000-0002-6371-0347; Joshi, Barkha/0000-0002-6371-0347
CR Afifi A, 2021, PLANTS-BASEL, V10, DOI 10.3390/plants10010028
   Albahli S, 2022, FRONT PLANT SCI, V13, DOI 10.3389/fpls.2022.1003152
   Amoda N., 2013, Int J Comput Sci Technol, V4, P214
   Arivazhagan S., 2013, Agricultural Engineering International: CIGR Journal, V15, P211
   Ashwinkumar S, 2022, MATER TODAY-PROC, V51, P480, DOI 10.1016/j.matpr.2021.05.584
   Camargo A, 2009, BIOSYST ENG, V102, P9, DOI 10.1016/j.biosystemseng.2008.09.030
   Chai RS, 2021, COMPLEXITY, V2021, DOI 10.1155/2021/5564690
   Chen J, 2011, INT J PATTERN RECOGN, V25, P985, DOI 10.1142/S0218001411008993
   Dhakate M, 2015, NAT CONF COMPUT VIS
   Diaz-Arias A, 2024, VISUAL COMPUT, V40, P2555, DOI 10.1007/s00371-023-02936-5
   Duan GR, 2010, ADV MECH MATH, V23, P1, DOI 10.1007/978-1-4419-6397-0_1
   Ferentinos KP, 2018, COMPUT ELECTRON AGR, V145, P311, DOI 10.1016/j.compag.2018.01.009
   Francis J, 2016, 2016 CONFERENCE ON EMERGING DEVICES AND SMART SYSTEMS (ICEDSS), P161
   Guadagna P, 2023, PRECIS AGRIC, V24, P1547, DOI 10.1007/s11119-023-10006-y
   Hassan SM, 2021, ELECTRONICS-SWITZ, V10, DOI 10.3390/electronics10121388
   Hu X, 2021, KNOWL INF SYST, V63, P2585, DOI 10.1007/s10115-021-01605-0
   Hyvärinen A, 2013, PHILOS T R SOC A, V371, DOI 10.1098/rsta.2011.0534
   Islam M, 2017, CAN CON EL COMP EN
   Jadhav S, 2022, INT J INF SYST MODEL, V13, DOI 10.4018/IJISMD.315126
   Jagiello K, 2016, J NANOPART RES, V18, DOI 10.1007/s11051-016-3564-1
   Joshi B. M., 2022, INT C SUST INN SOL C, P15
   Joshi B. M., 2023, International Journal of Intelligent Systems and Applications in Engineering, V11, P215
   Joshi BM, 2020, J INFORM OPTIM SCI, V41, P475, DOI 10.1080/02522667.2020.1734295
   Li SX, 2022, FRONT PLANT SCI, V13, DOI 10.3389/fpls.2022.972286
   Liu LY, 2020, IEEE ACCESS, V8, P52181, DOI 10.1109/ACCESS.2020.2980310
   Liu L, 2016, PROCEDIA COMPUT SCI, V92, P361, DOI 10.1016/j.procs.2016.07.391
   Lucas G.B., 1992, Introduction to plant disease, identification and management, V2nd
   Lucas GB., 1985, Introduction to plant diseases, P2006
   Mitra D., 2021, Emergingtrends in plant pathology, P1, DOI [10.1007/978-981-15-6275-4_1, DOI 10.1007/978-981-15-6275-4_1]
   Mugithe Pradeep Kumar, 2020, 2020 Proceedings of the International Conference on Communication and Signal Processing (ICCSP), P1603, DOI 10.1109/ICCSP48568.2020.9182065
   Pandian JA, 2022, APPL SCI-BASEL, V12, DOI 10.3390/app12146982
   Pantazi XE, 2017, COMPUT ELECTRON AGR, V137, P130, DOI 10.1016/j.compag.2017.03.017
   Parikshith H, 2020, ADV INTELL SYST COMP, V1108, P304, DOI 10.1007/978-3-030-37218-7_35
   Patil JayamalaK., 2011, Journal of Advanced Bioinformatics Applications and Research, V2, P135
   Pearson K, 1901, PHILOS MAG, V2, P559, DOI 10.1080/14786440109462720
   Prasad R, 2006, KNOWL-BASED SYST, V19, P9, DOI 10.1016/j.knosys.2005.08.001
   Raju D., 2014, Int J Comput Sci Inf Technol, V5, P5716
   Ramcharan A, 2017, FRONT PLANT SCI, V8, DOI 10.3389/fpls.2017.01852
   Rathod ArtiN., 2013, International Journal of Advanced Research in Computer Science and Software Engineering, V3
   Reshma S., 2014, Int J Adv Res Comput Sci Technol, V2, P176
   Samet R, 2012, J VIS COMMUN IMAGE R, V23, P642, DOI 10.1016/j.jvcir.2012.02.005
   Sankaran S, 2010, COMPUT ELECTRON AGR, V72, P1, DOI 10.1016/j.compag.2010.02.007
   Sannakki SS, 2013, 2013 FOURTH INTERNATIONAL CONFERENCE ON COMPUTING, COMMUNICATIONS AND NETWORKING TECHNOLOGIES (ICCCNT), DOI 10.1109/ICCCNT.2013.6726616
   Sardogan M, 2018, 2018 3RD INTERNATIONAL CONFERENCE ON COMPUTER SCIENCE AND ENGINEERING (UBMK), P382, DOI 10.1109/UBMK.2018.8566635
   Sethy PK, 2020, COMPUT ELECTRON AGR, V175, DOI 10.1016/j.compag.2020.105527
   Shirahatti J, 2018, PROCEEDINGS OF THE 3RD INTERNATIONAL CONFERENCE ON COMMUNICATION AND ELECTRONICS SYSTEMS (ICCES 2018), P1171, DOI 10.1109/CESYS.2018.8723881
   Shoaib M, 2023, FRONT PLANT SCI, V14, DOI 10.3389/fpls.2023.1158933
   Singh Vijai, 2017, Information Processing in Agriculture, V4, P41, DOI 10.1016/j.inpa.2016.10.005
   Singh V, 2015, 2015 INTERNATIONAL CONFERENCE ON ADVANCES IN COMPUTER ENGINEERING AND APPLICATIONS (ICACEA), P1028, DOI 10.1109/ICACEA.2015.7164858
   Sladojevic S, 2016, COMPUT INTEL NEUROSC, V2016, DOI 10.1155/2016/3289801
   Sujithra J, 2022, MATH PROBL ENG, V2022, DOI 10.1155/2022/2546873
   Suykens JAK, 1999, NEURAL PROCESS LETT, V9, P293, DOI 10.1023/A:1018628609742
   Tang YC, 2023, PRECIS AGRIC, V24, P1183, DOI 10.1007/s11119-023-10009-9
   Vadivel T, 2022, ACTA AGR SCAND B-S P, V72, P312, DOI 10.1080/09064710.2021.1976266
   Wang JC, 2022, NONLINEAR ENG-MODEL, V11, P347, DOI 10.1515/nleng-2022-0037
   WOEBBECKE DM, 1995, T ASAE, V38, P259, DOI 10.13031/2013.27838
NR 56
TC 0
Z9 0
U1 1
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2023 OCT 27
PY 2023
DI 10.1007/s00371-023-03127-y
EA OCT 2023
PG 20
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA W0BN1
UT WOS:001088370100003
DA 2024-07-18
ER

PT J
AU Sun, SK
   Jia, ZH
   Zhu, YS
   Liu, GC
   Yu, ZT
AF Sun, Shengkun
   Jia, Zihao
   Zhu, Yisheng
   Liu, Guangcan
   Yu, Zhengtao
TI Decoupled spatio-temporal grouping transformer for skeleton-based action
   recognition
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Skeleton-based action recognition; Transformer; Decoupled
ID NETWORK
AB Capturing correlations between joints is crucial in skeleton-based action recognition tasks. Transformer has demonstrated its capability in capturing such correlations. However, conventional Transformer-based approaches model the relationships between joints in a unified spatio-temporal dimension, disregarding the unique semantic information that exists in both the spatial and temporal dimensions of skeleton sequences. To address this issue, we present a novel decoupled spatio-temporal grouping Transformer (DSTGFormer) model. The skeleton sequence is split into multiple spatio-temporal groups, each containing a set of consecutive frames. The spatio-temporal positional encoding (STPE) module assigns identity information to each element in the sequence. The spatio-temporal grouping self-attention (STGA) module captures the spatial and temporal relationships between different joints within a spatio-temporal group. This decoupling of the spatial and temporal dimensions enables the extraction of semantic information with different meanings in each dimension. Additionally, we propose a within-group spatial global regularization mechanism to learn more general spatial attention maps, and an inter-group feature aggregation (IGFA) module to enhance the differentiation between similar actions. Our proposed method outperforms the state-of-the-art methods on two large-scale datasets in terms of both recognition accuracy and computational efficiency.
C1 [Sun, Shengkun; Jia, Zihao; Zhu, Yisheng] Nanjing Univ Sci & Technol, Sch Automat, 219 NingLiu Rd, Nanjing 210000, Jiangsu, Peoples R China.
   [Liu, Guangcan] Southeast Univ, Sch Automat, 2 Southeast Univ Rd, Nanjing 210018, Jiangsu, Peoples R China.
   [Yu, Zhengtao] Kunming Univ Sci & Technol, Fac Informat Engn & Automat, 727 Jingming Rd South, Kunming 650500, Yunnan, Peoples R China.
C3 Nanjing University of Science & Technology; Southeast University -
   China; Kunming University of Science & Technology
RP Sun, SK (corresponding author), Nanjing Univ Sci & Technol, Sch Automat, 219 NingLiu Rd, Nanjing 210000, Jiangsu, Peoples R China.
EM 20211249077@nuist.edu.cn; 1299339316@qq.com; yszhu123@nuist.edu.cn;
   gcliu1982@gmail.com; ztyu@hotmail.com
FU The authors acknowledge the use of the research computing facility at
   Nanjing University of Information Science and Technology.
FX The authors acknowledge the use of the research computing facility at
   Nanjing University of Information Science and Technology.
CR Agahian S, 2019, VISUAL COMPUT, V35, P591, DOI 10.1007/s00371-018-1489-7
   Arnab A, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6816, DOI 10.1109/ICCV48922.2021.00676
   Bertasius G, 2021, PR MACH LEARN RES, V139
   Carion Nicolas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P213, DOI 10.1007/978-3-030-58452-8_13
   Chen Z, 2021, AAAI CONF ARTIF INTE, V35, P1113, DOI 10.1145/3474085.3475574
   Cheng K, 2020, PROC CVPR IEEE, P180, DOI 10.1109/CVPR42600.2020.00026
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Duan HD, 2022, PROC CVPR IEEE, P2959, DOI 10.1109/CVPR52688.2022.00298
   Elmadany NE, 2021, ACM T INTEL SYST TEC, V12, DOI 10.1145/3447686
   Guan SN, 2022, NEUROCOMPUTING, V514, P256, DOI 10.1016/j.neucom.2022.10.016
   Hao XK, 2021, IEEE T IMAGE PROCESS, V30, P2263, DOI 10.1109/TIP.2021.3051495
   Hu JF, 2015, PROC CVPR IEEE, P5344, DOI 10.1109/CVPR.2015.7299172
   Huang LJ, 2020, AAAI CONF ARTIF INTE, V34, P11045
   Ke Cheng, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12369), P536, DOI 10.1007/978-3-030-58586-0_32
   Ke QH, 2018, IEEE T IMAGE PROCESS, V27, P2842, DOI 10.1109/TIP.2018.2812099
   Koniusz P, 2022, IEEE T PATTERN ANAL, V44, P648, DOI 10.1109/TPAMI.2021.3107160
   Li C, 2022, IEEE T NEUR NET LEAR, V33, P4800, DOI 10.1109/TNNLS.2021.3061115
   Li C, 2018, Arxiv, DOI arXiv:1804.06055
   Li C, 2017, IEEE INT CONF MULTI
   Li MS, 2022, IEEE T PATTERN ANAL, V44, P3316, DOI 10.1109/TPAMI.2021.3053765
   Liu J, 2020, IEEE T PATTERN ANAL, V42, P2684, DOI 10.1109/TPAMI.2019.2916873
   Liu J, 2016, LECT NOTES COMPUT SC, V9907, P816, DOI 10.1007/978-3-319-46487-9_50
   Liu ZY, 2020, PROC CVPR IEEE, P140, DOI 10.1109/CVPR42600.2020.00022
   Oreifej O, 2013, PROC CVPR IEEE, P716, DOI 10.1109/CVPR.2013.98
   Peng W, 2021, PATTERN RECOGN, V115, DOI 10.1016/j.patcog.2021.107921
   Plizzari C, 2021, COMPUT VIS IMAGE UND, V208, DOI 10.1016/j.cviu.2021.103219
   Qin Y, 2020, VISUAL COMPUT, V36, P621, DOI 10.1007/s00371-019-01644-3
   Qin ZY, 2024, IEEE T NEUR NET LEAR, V35, P4783, DOI 10.1109/TNNLS.2022.3201518
   Qiu HL, 2023, NEUROCOMPUTING, V518, P30, DOI 10.1016/j.neucom.2022.10.084
   Shahroudy A, 2016, PROC CVPR IEEE, P1010, DOI 10.1109/CVPR.2016.115
   Shi L, 2020, Arxiv, DOI arXiv:2007.03263
   Shi L, 2019, PROC CVPR IEEE, P7904, DOI 10.1109/CVPR.2019.00810
   Shi L, 2019, PROC CVPR IEEE, P12018, DOI 10.1109/CVPR.2019.01230
   Si CY, 2019, PROC CVPR IEEE, P1227, DOI 10.1109/CVPR.2019.00132
   Si CY, 2018, LECT NOTES COMPUT SC, V11205, P106, DOI 10.1007/978-3-030-01246-5_7
   Tang YY, 2019, Arxiv, DOI arXiv:1905.11799
   Vaswani A, 2017, ADV NEUR IN, V30
   Vemulapalli R, 2014, PROC CVPR IEEE, P588, DOI 10.1109/CVPR.2014.82
   Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441
   Wang Lei, 2023, 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), P5620, DOI 10.1109/CVPR52729.2023.00544
   Wang L, 2023, LECT NOTES COMPUT SC, V13844, P307, DOI 10.1007/978-3-031-26316-3_19
   Wang L, 2022, LECT NOTES COMPUT SC, V13681, P176, DOI 10.1007/978-3-031-19803-8_11
   Wang L, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P4324, DOI 10.1145/3474085.3475572
   Wang L, 2019, IEEE I CONF COMP VIS, P8697, DOI 10.1109/ICCV.2019.00879
   Wang L, 2020, IEEE T IMAGE PROCESS, V29, P15, DOI 10.1109/TIP.2019.2925285
   Wang MD, 2022, NEUROCOMPUTING, V501, P822, DOI 10.1016/j.neucom.2022.06.024
   Wei SH, 2017, IEEE IMAGE PROC, P91, DOI 10.1109/ICIP.2017.8296249
   Wu C, 2022, IEEE T CIRC SYST VID, V32, P2120, DOI 10.1109/TCSVT.2021.3085959
   Wu D, 2017, IEEE IJCNN, P2865, DOI 10.1109/IJCNN.2017.7966210
   Wu HP, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P22, DOI 10.1109/ICCV48922.2021.00009
   Yan SJ, 2018, Arxiv, DOI [arXiv:1801.07455, DOI 10.1609/AAAI.V32I1.12328, 10.48550/ARXIV.1801.07455]
   Ye FF, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P55, DOI 10.1145/3394171.3413941
   Zhang J., 2022, The Visual Computer, P1
   Zhang YH, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P3229, DOI 10.1145/3474085.3475473
   Zhao MY, 2022, NEUROCOMPUTING, V501, P640, DOI 10.1016/j.neucom.2022.06.070
   Zheng MH, 2021, Arxiv, DOI [arXiv:2011.09315, 10.48550/arXiv.2011.09315]
   Zhu AC, 2020, NEUROCOMPUTING, V414, P90, DOI 10.1016/j.neucom.2020.07.068
   Zhu YS, 2023, IEEE T IMAGE PROCESS, V32, P496, DOI 10.1109/TIP.2022.3230249
   Zhu YS, 2021, ACM T INTEL SYST TEC, V12, DOI 10.1145/3466181
   Zhu Y, 2015, ACM T INTEL SYST TEC, V6, DOI 10.1145/2629483
NR 60
TC 0
Z9 0
U1 5
U2 12
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2023 OCT 25
PY 2023
DI 10.1007/s00371-023-03132-1
EA OCT 2023
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA U7RA3
UT WOS:001086728700001
DA 2024-07-18
ER

PT J
AU Leng, CJ
   Ding, QC
   Wu, CD
   Chen, AE
   Wang, H
   Wu, H
AF Leng, Chuanjiang
   Ding, Qichuan
   Wu, Chengdong
   Chen, Ange
   Wang, Huan
   Wu, Hao
TI BDNet: a method based on forward and backward convolutional networks for
   action recognition in videos
SO VISUAL COMPUTER
LA English
DT Article
DE Action recognition; Convolutional neural network; Bi-directional
   network; Spatiotemporal features
ID BIDIRECTIONAL LSTM
AB Human action recognition analyzes the behavior in a scene according to the spatiotemporal features carried in image sequences. Existing works suffers from ineffective spatial-temporal feature learning. For short video sequence, the critical challenge is to extract informative spatiotemporal features from a limited-length video. For long video sequences, combining long-range contextual information can improve recognition performance. However, conventional methods primarily consider modeling the action's spatiotemporal features along a single direction, which is difficult to consider context information and ignores the information from the opposite direction. This article proposes a bi-directional network to simulate the bi-directional Long Short-Term Memory (Bi-LSTM) processing of time series data. Specifically, two 3D Convolutional Neural Networks (3D CNNs) extract spatiotemporal features along the forward and backward image sequence of action for each modality individually. After integrating the features of each branch, a dynamic-fusion strategy is applied to obtain a video-level prediction. We conducted comprehensive experiments on the action recognition dataset UCF101 and HMDB51 and achieved 98.0% and 81.4% recognition accuracy, respectively, with a reduction of three quarters of the inputting RGB images.
C1 [Leng, Chuanjiang; Ding, Qichuan; Wu, Chengdong; Chen, Ange; Wang, Huan] Northeastern Univ, Fac Robot Sci & Engn, Shenyang 110169, Peoples R China.
   [Wu, Hao] Swinburne Univ Technol, Sydney, Australia.
C3 Northeastern University - China; Swinburne University of Technology
RP Ding, QC; Wu, CD (corresponding author), Northeastern Univ, Fac Robot Sci & Engn, Shenyang 110169, Peoples R China.
EM dingqichuan@mail.neu.edu.cn; wuchengdong@mail.neu.edu.cn
FU This work was supported in part by the National Natural Science
   Foundation of China under Grant 61973065, Grant U20A20197, and Grant
   61973063, by the Joint fund of Science amp; Technology Department of
   Liaoning Province and State Key Laboratory of Robotic [61973065,
   U20A20197, 61973063]; National Natural Science Foundation of China;
   Joint fund of Science amp; Technology Department of Liaoning Province
   [2020-KF-12-02]; State Key Laboratory of Robotics, China
   [2020JH2/10100040]; Liaoning Key Research and Development Project
   [OEIP-O-202005]; Foundation of National Key Laboratory [N182608004];
   Fundamental Research Funds for the Central Universities
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61973065, Grant U20A20197, and Grant
   61973063, by the Joint fund of Science & Technology Department of
   Liaoning Province and State Key Laboratory of Robotics, China under
   Grant 2020-KF-12-02, by Liaoning Key Research and Development Project
   2020JH2/10100040, by the Foundation of National Key Laboratory
   OEIP-O-202005, and by the Fundamental Research Funds for the Central
   Universities under Grant N182608004.
CR Abdelbaky A, 2021, VISUAL COMPUT, V37, P1821, DOI 10.1007/s00371-020-01940-3
   Bastos ILO, 2020, IEEE IMAGE PROC, P2216, DOI [10.1109/icip40778.2020.9190769, 10.1109/ICIP40778.2020.9190769]
   Berlin SJ, 2022, VISUAL COMPUT, V38, P223, DOI 10.1007/s00371-020-02012-2
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chen J., 2021, SECUR COMMUN NETW, V2021
   Chen J, 2021, IMAGE VISION COMPUT, V112, DOI 10.1016/j.imavis.2021.104214
   Dong WK, 2019, AAAI CONF ARTIF INTE, P8247
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Fang Z, 2022, IET COMPUT VIS, V16, P205, DOI 10.1049/cvi2.12080
   Feichtenhofer C, 2017, PROC CVPR IEEE, P7445, DOI 10.1109/CVPR.2017.787
   Feichtenhofer C, 2016, PROC CVPR IEEE, P1933, DOI 10.1109/CVPR.2016.213
   Girdhar R, 2019, PROC CVPR IEEE, P244, DOI 10.1109/CVPR.2019.00033
   Goyal G, 2022, IMAGE VISION COMPUT, V120, DOI 10.1016/j.imavis.2022.104403
   Han CL, 2022, EXPERT SYST APPL, V198, DOI 10.1016/j.eswa.2022.116764
   He JY, 2021, NEUROCOMPUTING, V444, P319, DOI 10.1016/j.neucom.2020.05.118
   Huang G., 2022, P IEEE CVF WINT C AP, P1341, DOI DOI 10.48550/ARXIV.2103.15584
   Huang WB, 2023, ACM T EMBED COMPUT S, V22, DOI 10.1145/3551486
   Jiang M, 2020, J VIS COMMUN IMAGE R, V71, DOI 10.1016/j.jvcir.2020.102846
   Khan MA, 2020, APPL SOFT COMPUT, V87, DOI 10.1016/j.asoc.2019.105986
   Klaser A., 2008, P BMVC, P275, DOI DOI 10.5244/C.22.99
   Laptev I, 2008, PROC CVPR IEEE, P3222, DOI 10.1109/cvpr.2008.4587756
   Leng CJ, 2021, J VIS COMMUN IMAGE R, V81, DOI 10.1016/j.jvcir.2021.103344
   Li C, 2019, PROC CVPR IEEE, P7864, DOI 10.1109/CVPR.2019.00806
   Li DL, 2022, VISUAL COMPUT, V38, P2971, DOI 10.1007/s00371-021-02167-6
   Li ZY, 2018, COMPUT VIS IMAGE UND, V166, P41, DOI 10.1016/j.cviu.2017.10.011
   Liao ZK, 2020, NEURAL PROCESS LETT, V51, P287, DOI 10.1007/s11063-019-10091-z
   Liu CC, 2021, VISUAL COMPUT, V37, P1327, DOI 10.1007/s00371-020-01868-8
   Liu Z, 2022, PROC CVPR IEEE, P3192, DOI 10.1109/CVPR52688.2022.00320
   Long X, 2018, AAAI CONF ARTIF INTE, P7202
   Ma J, 2020, SCI TOTAL ENVIRON, V705, DOI 10.1016/j.scitotenv.2019.135771
   Mazzia V, 2022, PATTERN RECOGN, V124, DOI 10.1016/j.patcog.2021.108487
   Min XK, 2020, IEEE T IMAGE PROCESS, V29, P6054, DOI 10.1109/TIP.2020.2988148
   Nagrani A., 2020, P C COMP VIS PATT RE, P10317
   Peng XJ, 2016, COMPUT VIS IMAGE UND, V150, P109, DOI 10.1016/j.cviu.2016.03.013
   Nguyen PA, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3429457
   Shah Chirag., 2021, Synthesis Lectures on Information Concepts, Retrieval, and Services, V13, P1
   Shou Z, 2019, PROC CVPR IEEE, P1268, DOI 10.1109/CVPR.2019.00136
   Simonyan K, 2014, ADV NEUR IN, V27
   Singh B, 2016, PROC CVPR IEEE, P1961, DOI 10.1109/CVPR.2016.216
   Sun L, 2015, IEEE I CONF COMP VIS, P4597, DOI 10.1109/ICCV.2015.522
   Sun N, 2021, IMAGE VISION COMPUT, V109, DOI 10.1016/j.imavis.2021.104141
   Tang Y, 2023, IEEE T IND ELECTRON, V70, P2106, DOI 10.1109/TIE.2022.3161812
   Tong Z., 2022, NIPS, V15, P10078
   Ullah A, 2018, IEEE ACCESS, V6, P1155, DOI 10.1109/ACCESS.2017.2778011
   Ullah W, 2021, MULTIMED TOOLS APPL, V80, P16979, DOI 10.1007/s11042-020-09406-3
   Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441
   Wang LM, 2016, LECT NOTES COMPUT SC, V9912, P20, DOI 10.1007/978-3-319-46484-8_2
   Wang PC, 2018, AAAI CONF ARTIF INTE, P7404
   Xianyuan Wang, 2019, IOP Conference Series: Materials Science and Engineering, V569, DOI 10.1088/1757-899X/569/3/032035
   Yan S, 2022, PROC CVPR IEEE, P3323, DOI 10.1109/CVPR52688.2022.00333
   Yao X, 2023, VISUAL COMPUT, V39, P5469, DOI 10.1007/s00371-022-02673-1
   Zach C, 2007, LECT NOTES COMPUT SC, V4713, P214, DOI 10.1007/978-3-540-74936-3_22
   Zarmehi N, 2020, IEEE T CIRC SYST VID, V30, P2046, DOI 10.1109/TCSVT.2019.2923816
   Zhai GT, 2020, SCI CHINA INFORM SCI, V63, DOI 10.1007/s11432-019-2757-1
   Zhang YY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13557, DOI [10.1109/iccv48922.2021.01332, 10.1109/ICCV48922.2021.01332]
NR 55
TC 0
Z9 0
U1 7
U2 13
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2024
VL 40
IS 6
BP 4133
EP 4147
DI 10.1007/s00371-023-03073-9
EA OCT 2023
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TV2X4
UT WOS:001087965600002
DA 2024-07-18
ER

PT J
AU Le, G
   Bao, S
   Yang, DZ
   Duan, KB
AF Le, Gao
   Bao, Shi
   Yang, Dezhi
   Duan, Kaibo
TI IDA: an improved dual attention module for pollen classification
SO VISUAL COMPUTER
LA English
DT Article
DE Deep learning; Pollen image recognition; CNN; Attention mechanism
AB Pollen allergy is a common disease in the department of allergy, and its incidence is increasing year by year, seriously affecting human health. As an important task in palynology, pollen identification is widely used in several fields, such as pollen allergy meteorological index forecasting. Despite its wide range of applications, pollen identification is still, in most cases, carried out manually by professionals, and the whole procedure is tedious and time-consuming. In the last few years, the convolutional neural network based on deep learning has demonstrated excellent performance in visual tasks, such as image recognition. Applying a deep learning solution for recognizing the pollen grains can enhance the pollen identification accuracy and reduce human participation time. In this study, we designed a pollen recognition model on the base of DenseNet combined with an improved dual attention module, which can effectively enhance the pollen identification accuracy by remodeling features in two dimensions: space and channel. Our attention module can be easily integrated into other CNNs and Vision Transformers, and both have a different magnitude of performance improvement. In addition to that, our method has better recognition performance than other most SOTA attention methods. During the experiment, we used a self-built pollen dataset and four public datasets. The proposed method can achieve up to 96.1% classification accuracy, 94.9% recall, 95.2% precision, and 95.6% F1 score on the self-built electron-microscopic pollen dataset. In addition, the same 1-2% performance improvement can be obtained on the public datasets and all experiments were trained from scratch.
C1 [Le, Gao; Bao, Shi; Duan, Kaibo] Inner Mongolia Univ Technol, Sch Informat Engn, Hohhot, Peoples R China.
   [Yang, Dezhi] Inner Mongolia Int Mongolian Hosp, 83 Daxue East Rd, Hohhot, Peoples R China.
C3 Inner Mongolia University of Technology
RP Bao, S (corresponding author), Inner Mongolia Univ Technol, Sch Informat Engn, Hohhot, Peoples R China.
EM 20211800102@imut.edu.cn; kshibao@imut.edu.cn; imudezhi@hotmail.com;
   20201100117@imut.edu.cn
RI Yang, Dezhi/HGB-2668-2022
OI Yang, Dezhi/0000-0003-4653-9856; Bao, Shi/0000-0002-1107-5679
FU National Natural Science Foundation of China [62066035]; Natural Science
   Foundation of Inner Mongolia Autonomous Region [2022LHMS06004]; Inner
   Mongolia Autonomous Region [JY20220089]
FX This work was supported by National Natural Science Foundation of China
   (62066035), Natural Science Foundation of Inner Mongolia Autonomous
   Region (2022LHMS06004), and the basic scientific research business fee
   project of the universities directly under the Inner Mongolia Autonomous
   Region (JY20220089). The authors would like to thank the
   Editor-in-Chief, the anonymous Associate Editor, and reviewers for their
   insightful comments and suggestions that have greatly improved this
   paper.
CR Bahdanau D, 2016, Arxiv, DOI [arXiv:1409.0473, 10.48550/arXiv.1409.0473]
   Cao Y, 2019, IEEE ICC
   Chen MF, 2023, CRIT REV FOOD SCI, DOI 10.1080/10408398.2023.2172547
   CHOLLET F, 2017, PROC CVPR IEEE, P1800, DOI DOI 10.1109/CVPR.2017.195
   Congcong Li, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12358), P481, DOI 10.1007/978-3-030-58601-0_29
   D'Amato G, 1998, ALLERGY, V53, P567, DOI 10.1111/j.1398-9995.1998.tb03932.x
   Dai Z, 2021, ADV NEUR IN, V34
   Daood Amar, 2016, Advances in Visual Computing. 12th International Symposium, ISVC 2016. Proceedings: LNCS 10072, P321, DOI 10.1007/978-3-319-50835-1_30
   de Geus AR, 2019, EUR SIGNAL PR CONF, DOI 10.23919/eusipco.2019.8902735
   Dosovitskiy A, 2021, arXiv, DOI [10.48550/ARXIV.2010.11929, DOI 10.48550/ARXIV.2010.11929]
   Fei Ding, 2020, Medical Image Computing and Computer Assisted Intervention - MICCAI 2020. 23rd International Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12261), P253, DOI 10.1007/978-3-030-59710-8_25
   Goyal A., 2021, ARXIV
   Hassanin Mohammed, 2022, arXiv
   He KM, 2020, IEEE T PATTERN ANAL, V42, P386, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Howard A.G., 2017, MOBILENETS EFFICIENT, DOI DOI 10.48550/ARXIV.1704.04861
   Howard A, 2019, IEEE I CONF COMP VIS, P1314, DOI 10.1109/ICCV.2019.00140
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   HUANG G, 2017, PROC CVPR IEEE, P2261, DOI DOI 10.1109/CVPR.2017.243
   Kingma D. P., 2014, arXiv
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li W, 2018, PROC CVPR IEEE, P2285, DOI 10.1109/CVPR.2018.00243
   Li YH, 2023, IEEE T PATTERN ANAL, V45, P1489, DOI 10.1109/TPAMI.2022.3164083
   Liu H, 2021, PHYS REV LETT, V126, DOI 10.1103/PhysRevLett.126.250502
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Ma ZH, 2022, VISUAL COMPUT, V38, P3163, DOI 10.1007/s00371-022-02535-w
   Park J., 2018, ARXIV
   Radosavovic Ilija, 2020, P IEEE CVF C COMP VI, P10428
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Schaefer J, 2021, SCI TOTAL ENVIRON, V796, DOI 10.1016/j.scitotenv.2021.148932
   Selvaraju RR, 2020, INT J COMPUT VISION, V128, P336, DOI [10.1007/s11263-019-01228-7, 10.1109/ICCV.2017.74]
   Sevillano V, 2018, PLOS ONE, V13, DOI 10.1371/journal.pone.0201807
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Tan M., 2020, INT C MACH LEARN, DOI DOI 10.48550/ARXIV.1905.11946
   Tan MX, 2021, PR MACH LEARN RES, V139, P7102
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang Q, 2020, INT SYM QUAL ELECT, P1, DOI [10.1109/isqed48828.2020.9137057, 10.1109/ISQED48828.2020.9137057, 10.1109/CVPR42600.2020.01155]
   Wang W., 2021, ARXIV
   Woo S., 2018, P EUR C COMP VIS ECC, DOI DOI 10.1007/978-3-030-01234-2_1
   Wu HP, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P22, DOI 10.1109/ICCV48922.2021.00009
   Yu T., 2021, arXiv
   Zhang QL, 2021, 2021 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP 2021), P2235, DOI 10.1109/ICASSP39728.2021.9414568
   Zhao T, 2019, PROC CVPR IEEE, P3080, DOI 10.1109/CVPR.2019.00320
   Ziyi Meng, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12368), P187, DOI 10.1007/978-3-030-58592-1_12
NR 44
TC 1
Z9 1
U1 6
U2 9
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2024
VL 40
IS 6
BP 4167
EP 4181
AR s00371-023-03075-7
DI 10.1007/s00371-023-03075-7
EA SEP 2023
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TV2X4
UT WOS:001064709500001
DA 2024-07-18
ER

PT J
AU Gao, JL
   Wu, J
   Zhao, XM
   Xu, G
AF Gao, Jiongli
   Wu, Jun
   Zhao, Xuemei
   Xu, Gang
TI Integrating TPS, cylindrical projection, and plumb-line constraint for
   natural stitching of multiple images
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Image stitching; Image registration; Image alignment; Seam-line
   generation; Cylindrical projection; TPS
ID WARPS; PANORAMAS; FEATURES
AB This paper presents a novel approach for natural stitching multiple images by integrating thin-plate spline (TPS), cylindrical projection, and plumb-line constraint. Firstly, the homography estimated under plumb-line constraint is used to transform each image to keep the scene in the image upward as much as possible, so as to suppress the accumulation of image projection deformation and make the transformed images approximately available for cylindrical projection. Then, by introducing cylindrical projection into TPS as a global transformation, a multiple image alignment framework called cylindrical projection thin-plate spline (CP-TPS) is established to accurately align the transformed images. In this step, the virtual control points (VCP) are set in the non-overlapping area of images so that the CP-TPS can produce desired deformation in the final stitched image. Finally, a seam-line intersecting the significant structure in the aligned image is automatically generated by combining TPS, dynamic programming matching, and control points triangulation. In this step, the seam-line itself is used to estimate CP-TPS parameter. Experiments were conducted on four public image sets. The results show that the proposed approach can realize the natural stitching of public multiple image sets and has the best performance, compared with Autostitch, APAP, NISwGSP, ELA, and GES-GSP.
C1 [Gao, Jiongli; Wu, Jun; Zhao, Xuemei] Guilin Univ Elect Technol, Sch Elect Engn & Automat, Guilin 541004, Peoples R China.
   [Xu, Gang] Chinese Acad Sci, Ningbo Inst Mat Technol & Engn, Ningbo 315201, Zhejiang, Peoples R China.
C3 Guilin University of Electronic Technology; Chinese Academy of Sciences;
   Ningbo Institute of Materials Technology and Engineering, CAS
RP Wu, J (corresponding author), Guilin Univ Elect Technol, Sch Elect Engn & Automat, Guilin 541004, Peoples R China.
EM gaojiongli@163.com; wujun93161@163.com; zhaoxm@guet.edu.cn;
   xugang@nimte.ac.cn
OI Gao, Jiongli/0000-0001-5093-1805
FU National Natural Science Foundation of China [42361071]; Ningbo Science
   and Technology Innovation Project [2020Z013]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 42361071, and the Ningbo Science and
   Technology Innovation Project under Grant 2020Z013.
CR Bobick AF, 1999, INT J COMPUT VISION, V33, P181, DOI 10.1023/A:1008150329890
   BOOKSTEIN FL, 1989, IEEE T PATTERN ANAL, V11, P567, DOI 10.1109/34.24792
   Brown M., Autostitch
   Brown M, 2007, INT J COMPUT VISION, V74, P59, DOI 10.1007/s11263-006-0002-3
   CAPRILE B, 1990, INT J COMPUT VISION, V4, P127, DOI 10.1007/BF00127813
   Chang CH, 2014, PROC CVPR IEEE, P3254, DOI 10.1109/CVPR.2014.422
   Chen S. E., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P29, DOI 10.1145/218380.218395
   Chen YS, 2016, LECT NOTES COMPUT SC, V9909, P186, DOI 10.1007/978-3-319-46454-1_12
   Du P, 2022, PROC CVPR IEEE, P3678, DOI 10.1109/CVPR52688.2022.00367
   Ehlgen T, 2008, IEEE T INTELL TRANSP, V9, P657, DOI 10.1109/TITS.2008.2006815
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Fu J, 2022, IEEE GEOSCI REMOTE S, V19, DOI 10.1109/LGRS.2021.3111099
   Gandhi T, 2006, IEEE T INTELL TRANSP, V7, P293, DOI 10.1109/TITS.2006.880635
   Gao JH, 2011, PROC CVPR IEEE, P49, DOI 10.1109/CVPR.2011.5995433
   Gu H, 2009, I W IMAG SYST TECHNI, P159, DOI 10.1109/IST.2009.5071624
   Halake K, 2016, J IND ENG CHEM, V35, P1, DOI 10.1016/j.jiec.2016.01.003
   Jia JY, 2008, IEEE T PATTERN ANAL, V30, P617, DOI 10.1109/TPAMI.2007.70729
   Kwatra V, 2003, ACM T GRAPHIC, V22, P277, DOI 10.1145/882262.882264
   Kyu-Yul Lee, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8195, DOI 10.1109/CVPR42600.2020.00822
   Levin A, 2004, LECT NOTES COMPUT SC, V2034, P377
   Li J, 2018, IEEE T MULTIMEDIA, V20, P1672, DOI 10.1109/TMM.2017.2777461
   Li J, 2015, IEEE T CYBERNETICS, V45, P2707, DOI 10.1109/TCYB.2014.2381774
   Li N, 2018, IEEE T MULTIMEDIA, V20, P1365, DOI 10.1109/TMM.2017.2771566
   Lin CC, 2015, PROC CVPR IEEE, P1155, DOI 10.1109/CVPR.2015.7298719
   Lin KM, 2016, LECT NOTES COMPUT SC, V9907, P370, DOI 10.1007/978-3-319-46487-9_23
   Lin WY, 2011, PROC CVPR IEEE, P345, DOI 10.1109/CVPR.2011.5995314
   Liu YJ, 2019, VISUAL COMPUT, V35, P667, DOI 10.1007/s00371-018-1502-1
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Ma QL, 2022, APPL INTELL, V52, P3018, DOI 10.1007/s10489-021-02562-5
   Nie L, 2022, NEUROCOMPUTING, V491, P533, DOI 10.1016/j.neucom.2021.12.032
   Nie L, 2021, IEEE T IMAGE PROCESS, V30, P6184, DOI 10.1109/TIP.2021.3092828
   Rohr K, 2001, IEEE T MED IMAGING, V20, P526, DOI 10.1109/42.929618
   Shum HY, 2005, IEEE T MULTIMEDIA, V7, P85, DOI 10.1109/TMM.2004.840591
   Summa B, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185579
   Szeliski R, 1996, IEEE COMPUT GRAPH, V16, P22, DOI 10.1109/38.486677
   Szeliski R, 2006, FOUND TRENDS COMPUT, V2, P1, DOI 10.1561/0600000009
   Tzavidas S, 2005, IEEE T MULTIMEDIA, V7, P880, DOI 10.1109/TMM.2005.854430
   Wang Z., 1990, Principles of photogrammetry (with Remote Sensing)
   Wu J., 2001, P SPIE INT SOC OPT E
   Zaragoza J, 2014, IEEE T PATTERN ANAL, V36, P1285, DOI 10.1109/TPAMI.2013.247
   Zaragoza J, 2013, PROC CVPR IEEE, P2339, DOI 10.1109/CVPR.2013.303
   Zhang F, 2014, PROC CVPR IEEE, P3262, DOI 10.1109/CVPR.2014.423
   Zhang JC, 2023, VISUAL COMPUT, V39, P4915, DOI 10.1007/s00371-022-02637-5
   Zhang JD, 2024, VISUAL COMPUT, V40, P427, DOI 10.1007/s00371-023-02791-4
   Zhang M., 2023, IEEE Trans. Geosci. Remote Sens, V61, P1
   Zhang MJ, 2024, IEEE T NEUR NET LEAR, V35, P1810, DOI 10.1109/TNNLS.2022.3185529
   Zhang MJ, 2023, IEEE T NEUR NET LEAR, V34, P10538, DOI 10.1109/TNNLS.2022.3168540
   Zhang MJ, 2023, IEEE T CYBERNETICS, V53, P578, DOI 10.1109/TCYB.2022.3163294
   Zhang ZH, 2023, IEEE T MULTIMEDIA, V25, P329, DOI 10.1109/TMM.2021.3126157
   Zhao Q, 2013, IEEE T MULTIMEDIA, V15, P1745, DOI 10.1109/TMM.2013.2280249
   Zheng J, 2019, IEEE T MULTIMEDIA, V21, P2561, DOI 10.1109/TMM.2019.2905692
NR 51
TC 2
Z9 2
U1 2
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2023 AUG 31
PY 2023
AR s00371-023-03065-9
DI 10.1007/s00371-023-03065-9
EA AUG 2023
PG 30
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA R2FT1
UT WOS:001062559600001
DA 2024-07-18
ER

PT J
AU Yang, LJ
   Wu, Z
   Xu, TC
   Du, JX
   Wu, E
AF Yang, Lijie
   Wu, Zhan
   Xu, Tianchen
   Du, Jixiang
   Wu, Enhua
TI Easy recognition of artistic Chinese calligraphic characters
SO VISUAL COMPUTER
LA English
DT Article
DE Character recognition; Convolutional neural network; Recurrent neural
   network; Chinese calligraphy
ID OCR; BRUSH
AB Chinese calligraphy is one of the excellent expressions of Chinese traditional art. But people without domain knowledge of calligraphy can hardly read, appreciate, or learn this art form, due to it contains many brush strokes with unique shapes and complicate structural topological relationship. In this paper, we explore the solution of text sequence recognition of calligraphy, which is a challenging task because traditional algorithms of text recognition can rarely obtain the satisfied results for the varied styles of calligraphy. Therefore, based on a trainable neural network, this paper proposes an easy recognition method, which combines feature sequence extraction based on DenseNet (Dense Convolutional Network) model, sequence modeling and transcription into a consolidated architecture. Compared with previous algorithms for text recognition, it has two distinctive properties: One is to acquire the artistic features on shapes and structures of different styles of calligraphic characters and the other is to handel sequences in arbitrary lengths without character segmentation. Our experimental results prove that in contrast with several common recognition methods, our method of Chinese calligraphic character in diverse styles demonstrates greater robustness, and the recognition accuracy rate reaches 84.70%.
C1 [Yang, Lijie; Wu, Zhan; Du, Jixiang] Huaqiao Univ, Xiamen, Fujian, Peoples R China.
   [Xu, Tianchen; Wu, Enhua] Chinese Acad Sci, Inst Software, State Key Lab CS, Beijing, Peoples R China.
   [Xu, Tianchen; Wu, Enhua] Univ CAS, Beijing, Peoples R China.
   [Wu, Enhua] Univ Macau, FST, Macau, Peoples R China.
C3 Huaqiao University; Chinese Academy of Sciences; Institute of Software,
   CAS; University of Macau
RP Yang, LJ (corresponding author), Huaqiao Univ, Xiamen, Fujian, Peoples R China.
EM muyi6081@sina.com; wuzhan_hqu@163.com; weh@ios.ac.cn
OI Yang, Lijie/0000-0003-0902-401X
FU National Natural Science Foundation of China (NSFC) [62072449];
   Fundamental Research Funds for the Central Universities [ZQN-710]
FX AcknowledgementsThis work was supported by National Natural Science
   Foundation of China (NSFC) (No. 62072449) and the Fundamental Research
   Funds for the Central Universities (No. ZQN-710).
CR Alotaibi F, 2018, IEEE ACCESS, V6, DOI 10.1109/ACCESS.2017.2771621
   [Anonymous], CHINESE CALLIGRAPHIC
   [Anonymous], NEURAL NETWORKS MACH
   Callyope, About us
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851
   Chen JY, 2003, PATTERN RECOGN, V36, P943, DOI 10.1016/S0031-3203(02)00128-0
   Chen Q, 2008, PATTERN RECOGN, V41, P1254, DOI 10.1016/j.patcog.2007.09.007
   Chu NSH, 2002, 10TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P413, DOI 10.1109/PCCGA.2002.1167885
   Ding Xiao-qing, 2002, Acta Electronica Sinica, V30, P1364
   Doermann D, 1996, MACH VISION APPL, V9, P73
   Gao PC, 2015, MULTIMED TOOLS APPL, V74, P7221, DOI 10.1007/s11042-014-1969-3
   Gers FA, 2003, J MACH LEARN RES, V3, P115, DOI 10.1162/153244303768966139
   Graves A., 2006, P 23 INT C MACHINE L, P369
   Graves A, 2009, IEEE T PATTERN ANAL, V31, P855, DOI 10.1109/TPAMI.2008.137
   Hanvon, US
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Hodge V.J., 2008, P 5 IASTED INT C SIG, P81
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Huang G, 2016, LECT NOTES COMPUT SC, V9908, P646, DOI 10.1007/978-3-319-46493-0_39
   [金连文 Jin Lianwen], 2016, [自动化学报, Acta Automatica Sinica], V42, P1125
   Khan NH, 2018, IEEE ACCESS, V6, P46019, DOI 10.1109/ACCESS.2018.2865532
   Lin Y, 2013, ACM-IEEE J CONF DIG, P323
   Liu CL, 2013, PATTERN RECOGN, V46, P155, DOI 10.1016/j.patcog.2012.06.021
   Liu CL, 2001, PATTERN RECOGN, V34, P2339, DOI 10.1016/S0031-3203(00)00165-5
   MORI S, 1992, P IEEE, V80, P1029, DOI 10.1109/5.156468
   Naseer A, 2019, COMPUT MATH ORGAN TH, V25, P165, DOI 10.1007/s10588-018-9265-9
   Park J, 2000, IEEE T PATTERN ANAL, V22, P400, DOI 10.1109/34.845383
   Shi BG, 2017, IEEE T PATTERN ANAL, V39, P2298, DOI 10.1109/TPAMI.2016.2646371
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Srivastava RK., 2015, P 28 INT C NEURAL IN, P2377, DOI DOI 10.48550/ARXIV.1507.06228
   Van Laerhoven T, 2007, VISUAL COMPUT, V23, P763, DOI 10.1007/s0037l-007-0158-z
   Wong HTF, 2000, COMPUT GRAPH-UK, V24, P99, DOI 10.1016/S0097-8493(99)00141-7
   Wong STS, 2008, COMPUT VIS IMAGE UND, V109, P69, DOI 10.1016/j.cviu.2007.03.001
   Xie N., 2010, Proc. NPAR '10, P63
   Xu SH, 2005, IEEE INTELL SYST, V20, P32, DOI 10.1109/MIS.2005.41
   Xu SH, 2012, IEEE INTELL SYST, V27, P63, DOI 10.1109/MIS.2012.46
   Yang L-J, 2014, SIGGRAPH ASIA 2014 T, P1
   Yang LJ, 2009, PROCEEDINGS OF THE 8TH IEEE/ACIS INTERNATIONAL CONFERENCE ON COMPUTER AND INFORMATION SCIENCE, P683, DOI 10.1109/ICIS.2009.183
   Yu K, 2008, LECT NOTES COMPUT SC, V5353, P228
   Zhang X., 2011, P 2011 WORKSHOP HIST, P37
   Zhang XF, 2008, CISP 2008: FIRST INTERNATIONAL CONGRESS ON IMAGE AND SIGNAL PROCESSING, VOL 1, PROCEEDINGS, P355, DOI 10.1109/CISP.2008.268
   Zhiyi Zhang, 2009, 2009 10th International Conference on Document Analysis and Recognition (ICDAR), P763, DOI 10.1109/ICDAR.2009.27
   Zhuang Y., 2007, ACM T ASIAN LANG INF, V6, P8, DOI [10.1145/1282080.1282083, DOI 10.1145/1282080.1282083]
NR 44
TC 0
Z9 0
U1 4
U2 10
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2023
VL 39
IS 8
SI SI
BP 3755
EP 3766
DI 10.1007/s00371-023-03026-2
EA AUG 2023
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA P2DS6
UT WOS:001043657500001
DA 2024-07-18
ER

PT J
AU Ishitobi, A
   Nakayama, M
   Fujishiro, I
AF Ishitobi, Akinori
   Nakayama, Masanori
   Fujishiro, Issei
TI Visual simulation of crack and bend generation in deteriorated films
   coated on metal objects: Combination of static fracture and
   position-based deformation
SO VISUAL COMPUTER
LA English
DT Article
DE Weathering; Coating; Deformation
AB Weathering, an expression of degradation caused by rain and wind, is essential for photorealistic computer graphics. One of the commonest targets of weathering is metal, which is omnipresent in reality. However, for the realistic reproduction of scenes, many of which display degradation, the application of rust-proof paint to metal surfaces cannot be ignored. In our study, we propose a weathering method for coated films on metal objects, which are modeled using a three-dimensional (3D) triangular polygon mesh and deformed by combining two kinds of simulations: static simulation, for determining fractures based on the balance of the internal forces, and position-based bend simulation for moving vertices according to geometric constraints. Our method can digitally reproduce the deterioration of coated films using complex 3D deformation, which is difficult to achieve by material manipulation only.
C1 [Ishitobi, Akinori] Keio Univ, Grad Sch Sci & technol, Yokohama, Japan.
   [Nakayama, Masanori; Fujishiro, Issei] Keio Univ, Fac Sci & Technol, Dept Informat & Comp Sci, Yokohama, Japan.
C3 Keio University; Keio University
RP Ishitobi, A (corresponding author), Keio Univ, Grad Sch Sci & technol, Yokohama, Japan.
EM akinori.ishitobi@fj.ics.keio.ac.jp
OI Ishitobi, Akinori/0000-0003-0975-6456
FU JSPS KAKENHI [21H04916, 21J21729]
FX AcknowledgementsThis work has been supported in part by JSPS KAKENHI
   under the Grant-in-Aid for Scientific Research (A) No. 21H04916 and
   Grant-in-Aid for JSPS Fellows No. 21J21729.
CR Bellini R, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925891
   Bender J, 2014, COMPUT GRAPH-UK, V44, P1, DOI 10.1016/j.cag.2014.07.004
   Chang YX, 2003, VISUAL COMPUT, V19, P50, DOI 10.1007/s00371-002-0172-0
   Chen HY, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201395
   Chen YY, 2005, ACM T GRAPHIC, V24, P1127, DOI 10.1145/1073204.1073321
   Desbenoit B, 2005, VISUAL COMPUT, V21, P717, DOI 10.1007/s00371-005-0317-z
   Dorsey J., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P387, DOI 10.1145/237170.237278
   Gobron S, 2001, NINTH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P338, DOI 10.1109/PCCGA.2001.962890
   Iizuka S, 2016, COMPUT GRAPH FORUM, V35, P501, DOI 10.1111/cgf.12850
   Ishitobi A, 2020, VISUAL COMPUT, V36, P2383, DOI 10.1007/s00371-020-01947-w
   Jain N., 2014, P 2014 IND C COMP VI, p38:1
   Jeong S, 2013, COMPUT GRAPH FORUM, V32, P204, DOI 10.1111/cgf.12009
   Jeong S, 2011, VISUAL COMPUT, V27, P417, DOI 10.1007/s00371-011-0575-x
   Kelager M, 2010, P WORKSH VIRT REAL I, P31, DOI [10.2312/PE/vriphys/vriphys10/031-037, DOI 10.2312/PE/VRIPHYS/VRIPHYS10/031-037]
   Kimmel BW, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2421636.2421646
   Kratt J, 2015, COMPUT GRAPH FORUM, V34, P361, DOI 10.1111/cgf.12566
   Mérillou S, 2008, COMPUT GRAPH-UK, V32, P159, DOI 10.1016/j.cag.2008.01.003
   Merillou S., 2001, P GRAPHICS INTERFACE, P167
   Müller M, 2007, J VIS COMMUN IMAGE R, V18, P109, DOI 10.1016/j.jvcir.2007.01.005
   Muñoz-Pandiella I, 2018, IEEE T VIS COMPUT GR, V24, P3239, DOI 10.1109/TVCG.2018.2794526
   O'Brien JF, 1999, COMP GRAPH, P137, DOI 10.1145/311535.311550
   Paquette E, 2002, PROC GRAPH INTERF, P59
   Perlin K, 2002, ACM T GRAPHIC, V21, P681, DOI 10.1145/566570.566636
   Silling SA, 2000, J MECH PHYS SOLIDS, V48, P175, DOI 10.1016/S0022-5096(99)00029-0
   Stomakhin A, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461948
   Stoney GG, 1909, P R SOC LOND A-CONTA, V82, P172, DOI 10.1098/rspa.1909.0021
   Tanabe R., 2015, P INT WORKSH ADV IM
   TURNER MJ, 1956, J AERONAUT SCI, V23, P805, DOI 10.2514/8.3664
   Wang JP, 2006, ACM T GRAPHIC, V25, P754, DOI 10.1145/1141911.1141951
   Xue S, 2008, COMPUT GRAPH FORUM, V27, P617
   Xue S, 2011, COMPUT GRAPH FORUM, V30, P1189, DOI 10.1111/j.1467-8659.2011.01977.x
NR 31
TC 1
Z9 1
U1 1
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2023
VL 39
IS 8
SI SI
BP 3403
EP 3415
DI 10.1007/s00371-023-03029-z
EA JUL 2023
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA P2DS6
UT WOS:001034388500001
DA 2024-07-18
ER

PT J
AU Evangelou, I
   Papaioannou, G
   Vardis, K
   Gkaravelis, A
AF Evangelou, Iordanis
   Papaioannou, Georgios
   Vardis, Konstantinos
   Gkaravelis, Anastasios
TI A neural builder for spatial subdivision hierarchies
SO VISUAL COMPUTER
LA English
DT Article
DE Rendering; Neural networks; Spatial hierarchies; Ray tracing
AB Spatial data structures, such as k-d trees and bounding volume hierarchies, are extensively used in computer graphics for the acceleration of spatial queries in ray tracing, nearest neighbour searches and other tasks. Typically, the splitting strategy employed during the construction of such structures is based on the greedy evaluation of a predefined objective function, resulting in a less than optimal subdivision scheme. In this work, for the first time, we propose the use of unsupervised deep learning to infer the structure of a fixed-depth k-d tree from a constant, subsampled set of the input primitives, based on the recursive evaluation of the cost function at hand. This results in high-quality upper spatial hierarchy, inferred in constant time and without paying the intractable price of a fully recursive tree optimisation. The resulting fixed-depth tree can then be further expanded, in parallel, into either a full k-d tree or transformed into a bounding volume hierarchy, with any known conventional tree builder. The approach is generic enough to accommodate different cost functions, such as the popular surface area and volume heuristics. We experimentally validate that the resulting hierarchies have competitive traversal performance with respect to established tree builders, while maintaining minimal overhead in construction times.
C1 [Evangelou, Iordanis; Papaioannou, Georgios; Vardis, Konstantinos; Gkaravelis, Anastasios] Athens Univ Econ & Business, Dept Informat, Athens, Greece.
C3 Athens University of Economics & Business
RP Evangelou, I (corresponding author), Athens Univ Econ & Business, Dept Informat, Athens, Greece.
EM iordanise@aueb.gr; gepap@aueb.gr; kvardis@aueb.gr; agkar@aueb.gr
OI Papaioannou, Georgios/0000-0003-4774-0746; Vardis,
   Konstantinos/0000-0003-2282-4644
FU Hellenic Foundation for Research and Innovation (HFRI) [7310]
FX AcknowledgementsThis research was funded by the Hellenic Foundation for
   Research and Innovation (HFRI) under the "3rd Call for H.F.R.I. Research
   Projects to support Post-Doctoral Researchers" (Project No: 7310).
CR BENTLEY JL, 1975, COMMUN ACM, V18, P509, DOI 10.1145/361002.361007
   BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791
   Bitterli B., 2016, RENDERING RESOURCES
   Defferrard M, 2016, ADV NEUR IN, V29
   Domingues Leonardo., 2015, Proceedings of High-Performance Graphics, P13
   Ganestam P., 2015, J COMPUTER GRAPHICS, V4, P23
   GOLDSMITH J, 1987, IEEE COMPUT GRAPH, V7, P14, DOI 10.1109/MCG.1987.276983
   Hanocka R, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392415
   Hendrich J, 2017, COMPUT GRAPH FORUM, V36, P487, DOI 10.1111/cgf.13143
   Hou QM, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360618
   HUBER PJ, 1964, ANN MATH STAT, V35, P73, DOI 10.1214/aoms/1177703732
   Hunt W, 2007, RT07: IEEE/EG SYMPOSIUM ON INTERACTIVE RAY TRACING 2007, P47, DOI 10.1109/RT.2007.4342590
   Jensen H.W., 1996, Proceedings of the Eurographics Workshop on Rendering Techniques, P21
   Kalojanov J, 2011, COMPUT GRAPH FORUM, V30, P307, DOI 10.1111/j.1467-8659.2011.01862.x
   Karras Tero., 2013, Proceedings of the 5th High-Performance Graphics Conference, P89, DOI DOI 10.1145/2492045.2492055
   Kipf TN, 2017, INT C LEARN REPR
   Klokov R, 2017, IEEE I CONF COMP VIS, P863, DOI 10.1109/ICCV.2017.99
   Li RH, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459766
   Lumberyard A., 2017, AMAZON LUMBERYARD BI
   MacDonald J. D., 1990, Visual Computer, V6, P153, DOI 10.1007/BF01911006
   Maturana D, 2015, IEEE INT C INT ROBOT, P922, DOI 10.1109/IROS.2015.7353481
   McGuire M., 2017, Computer Graphics Archive
   Meister D, 2018, COMPUT GRAPH FORUM, V37, P463, DOI 10.1111/cgf.13376
   Meister D, 2021, COMPUT GRAPH FORUM, V40, P683, DOI 10.1111/cgf.142662
   Muja M, 2009, VISAPP 2009: PROCEEDINGS OF THE FOURTH INTERNATIONAL CONFERENCE ON COMPUTER VISION THEORY AND APPLICATIONS, VOL 1, P331
   Pérard-Gayot A, 2017, COMPUT GRAPH FORUM, V36, P477, DOI 10.1111/cgf.13142
   Pharr M., 2016, SCENES PBRT V3
   Qi C., 2017, NeurIPS, P5105, DOI [10.1109/ CVPR.2017.16, DOI 10.1109/CVPR.2017.16]
   Riegler G, 2017, PROC CVPR IEEE, P6620, DOI 10.1109/CVPR.2017.701
   Stich M., 2009, P C HIGH PERF GRAPH, P7, DOI DOI 10.1145/1572769.1572771
   Wald I, 2004, COMPUT GRAPH FORUM, V23, P595, DOI 10.1111/j.1467-8659.2004.00791.x
   Wald I, 2007, RT07: IEEE/EG SYMPOSIUM ON INTERACTIVE RAY TRACING 2007, P33, DOI 10.1109/RT.2007.4342588
   Wald I, 2006, RT 06: IEEE SYMPOSIUM ON INTERACTIVE RAY TRACING 2006, PROCEEDINGS, P61
   Wald I, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601199
   Wang PS, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275050
   Zaheer M, 2017, ADV NEUR IN, V30
NR 36
TC 0
Z9 0
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2023
VL 39
IS 8
SI SI
BP 3797
EP 3809
DI 10.1007/s00371-023-02975-y
EA JUL 2023
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA P2DS6
UT WOS:001035720700006
OA hybrid
DA 2024-07-18
ER

PT J
AU Liu, YH
   Li, CZ
   Liu, XT
   Wu, HS
   Wen, ZK
AF Liu, Yinghua
   Li, Chengze
   Liu, Xueting
   Wu, Huisi
   Wen, Zhenkun
TI AddCR: a data-driven cartoon remastering
SO VISUAL COMPUTER
LA English
DT Article
DE Cartoon remastering; Color enhancement; Denoising; Deep learning;
   Multi-task learning
AB Old cartoon classics have the lasting power to strike the resonance and fantasies of audiences today. However, cartoon animations from earlier years suffered from noise, low resolution, and dull lackluster color due to the improper storage environment of the film materials and limitations in the manufacturing process. In this work, we propose a deep learning-based cartoon remastering application that investigates and integrates noise removal, super-resolution, and color enhancement to improve the presentation of old cartoon animations. We employ multi-task learning methods in the denoising part and color enhancement part individually to guide the model to focus on the structure lines so that the generated image retains the sharpness and color of the structure lines. We evaluate existing super-resolution methods for cartoon inputs and find the best one that can guarantee the sharpness of the structure lines and maintain the texture of images. Moreover, we propose a reference-free color enhancement method that leverages a pre-trained classifier for old and new cartoons to guide color mapping.
C1 [Li, Chengze] Caritas Inst Higher Educ, Sch Comp & Informat Sci, Hong Kong, Peoples R China.
   [Liu, Yinghua; Liu, Xueting; Wu, Huisi; Wen, Zhenkun] Shenzhen Univ, Coll Comp Sci & Software Engn, Shenzhen 518060, Peoples R China.
C3 Saint Francis University Hong Kong; Shenzhen University
RP Li, CZ (corresponding author), Caritas Inst Higher Educ, Sch Comp & Informat Sci, Hong Kong, Peoples R China.; Wu, HS (corresponding author), Shenzhen Univ, Coll Comp Sci & Software Engn, Shenzhen 518060, Peoples R China.
EM czli@cihe.edu.hk; hswu@szu.edu.cn
RI liu, feng/KCL-0778-2024; Zhang, xiaohui/KEE-5747-2024; liu,
   shilong/JZD-8395-2024; Liu, Zhiyuan/KDP-2606-2024; Li,
   Chengze/AAU-7168-2021; Liu, Xueting/AAG-9648-2019; XIN,
   WANG/KGK-5385-2024
OI Li, Chengze/0000-0002-1519-750X; Liu, Xueting/0000-0002-0868-5353; 
FU National Natural Science Foundation of China [61973221, 62002232,
   62273241]; Natural Science Foundation of Guangdong Province, China
   [2019A1515011165]; Major Project of the New Generation of Artificial
   Intelligence [2018AAA0102900]; Research Grants Council of the Hong Kong
   Special Administrative Region, China [UGC/FDS11/E02/21]
FX This work was supported in part by grants from the National Natural
   Science Foundation of China (Nos. 61973221, 62002232 and 62273241), the
   Natural Science Foundation of Guangdong Province, China (No.
   2019A1515011165), the Major Project of the New Generation of Artificial
   Intelligence (No. 2018AAA0102900), the Research Grants Council of the
   Hong Kong Special Administrative Region, China(Project No.
   UGC/FDS11/E02/21).
CR Afifi M, 2021, PROC CVPR IEEE, P7937, DOI 10.1109/CVPR46437.2021.00785
   Chen L., 2022, ARXIV
   Chen SY, 2022, IEEE T VIS COMPUT GR, V28, P1198, DOI 10.1109/TVCG.2020.3009949
   Gharbi M, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073592
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Gu C., 2020, ARXIV
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He MM, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3292482
   He MM, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201365
   Higumax I., 2021, GITHUB
   Higumax I., GITHUB
   Ho MM, 2021, IEEE WINT CONF APPL, P2112, DOI 10.1109/WACV48630.2021.00216
   Hong K, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14589, DOI 10.1109/ICCV48922.2021.01434
   Iizuka S, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356570
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kingma D. P., 2014, arXiv
   Lahitani AR, 2016, 2016 4TH INTERNATIONAL CONFERENCE ON CYBER AND IT SERVICE MANAGEMENT, P205
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Lee J, 2020, VISUAL COMPUT, V36, P2129, DOI 10.1007/s00371-020-01921-6
   Li YJ, 2018, LECT NOTES COMPUT SC, V11207, P468, DOI 10.1007/978-3-030-01219-9_28
   Liang JY, 2021, IEEE INT CONF COMP V, P1833, DOI 10.1109/ICCVW54120.2021.00210
   Luan FJ, 2017, PROC CVPR IEEE, P6997, DOI 10.1109/CVPR.2017.740
   Ma L, 2022, PROC CVPR IEEE, P5627, DOI 10.1109/CVPR52688.2022.00555
   Nagadomi V.I., WAIFU2X
   Nihui I., 2022, REAL CUGAN
   Pitié F, 2005, IEEE I CONF COMP VIS, P1434
   Reinhard E, 2001, IEEE COMPUT GRAPH, V21, P34, DOI 10.1109/38.946629
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Shi M., 2020, ARXIV
   Song Y, 2021, P IEEE CVF INT C COM, P4126
   Tomasi C, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P839, DOI 10.1109/ICCV.1998.710815
   Torrey Lisa, 2010, Handbook of Research on Machine Learning Applications and Trends: Algorithms, Methods, and Techniques, P242
   Wan ZY, 2022, PROC CVPR IEEE, P17673, DOI 10.1109/CVPR52688.2022.01717
   Wan ZY, 2020, PROC CVPR IEEE, P2744, DOI 10.1109/CVPR42600.2020.00282
   Wang XT, 2021, IEEE INT CONF COMP V, P1905, DOI 10.1109/ICCVW54120.2021.00217
   Yu S, 2019, IEEE COMPUT SOC CONF, P2095, DOI 10.1109/CVPRW.2019.00262
   Zhang B, 2019, PROC CVPR IEEE, P8044, DOI 10.1109/CVPR.2019.00824
   Zhang L, 2011, IEEE T IMAGE PROCESS, V20, P2378, DOI 10.1109/TIP.2011.2109730
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhang Y, 2022, IEEE T KNOWL DATA EN, V34, P5586, DOI 10.1109/TKDE.2021.3070203
   Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI 10.1007/978-3-030-01234-2_18
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 42
TC 0
Z9 0
U1 3
U2 7
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2023
VL 39
IS 8
SI SI
BP 3741
EP 3753
DI 10.1007/s00371-023-02962-3
EA JUL 2023
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA P2DS6
UT WOS:001033665800001
DA 2024-07-18
ER

PT J
AU Zhu, YP
   Wang, H
   Chen, SJ
AF Zhu, Yuanping
   Wang, Hui
   Chen, Saijian
TI Joint super-resolution and deblurring for low-resolution text image
   using two-branch neural network
SO VISUAL COMPUTER
LA English
DT Article
DE Super-resolution; Deblurring; Two-branch neural network; Text image;
   Scale-recursion
AB The challenge of image reconstruction from very-low-resolution images is made exceedingly difficult by multiple degradation factors in practical applications. Traditional methods do not consider the interactions between these degradation factors, so the results are often insufficient. To reconstruct low-resolution blurry images, both super-resolution and deblurring processes must be applied. In this paper, we propose a joint super-resolution and a deblurring model with integrated processing of the degradation factors to obtain better image quality. The joint model includes two branches, a super-resolution module and deblurring module, and both of them share the same feature extraction module. The super-resolution module consists of multiple layers of residual blocks. The deblurring module supports the robustness of the super-resolution module through feature feed-back in the learning process, by introducing an image blurring feature description into the feature representation. To create modules with high magnification, the base two-branch model is also used in two stages with scale recursion. A second-stage deblurring module receives the output of the first-stage super-resolution module and improves the deblurring capability when the image is further magnified. The modules enhance each other, significantly improve the quality of very-low-resolution text images, and maintain a low model complexity. A step-by-step training strategy is applied to reduce second-stage training difficulty. Experiments show that our approach significantly outperforms state-of-the-art methods in terms of image quality and optical character recognition accuracy, and with a lower computational cost.
C1 [Zhu, Yuanping; Wang, Hui; Chen, Saijian] Tianjin Normal Univ, Coll Comp & Informat Engn, 393 Binshui Xidao, Tianjin 300387, Peoples R China.
C3 Tianjin Normal University
RP Zhu, YP (corresponding author), Tianjin Normal Univ, Coll Comp & Informat Engn, 393 Binshui Xidao, Tianjin 300387, Peoples R China.
EM zhuyuanping@tjnu.edu.cn
OI Zhu, Yuanping/0000-0002-9812-169X
FU Natural Science Foundation of Tianjin [18JCYBJC85000]
FX & nbsp;This work was supported by the Natural Science Foundation of
   Tianjin (Grant No. 18JCYBJC85000).
CR Chen JY, 2021, PROC CVPR IEEE, P12021, DOI 10.1109/CVPR46437.2021.01185
   Chen YT, 2024, VISUAL COMPUT, V40, P489, DOI 10.1007/s00371-023-02795-0
   Chen YT, 2021, APPL INTELL, V51, P4367, DOI 10.1007/s10489-020-02116-1
   Harmeling Stefan, 2010, P ADV NEUR INF PROC, P829
   He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123
   He KM, 2014, LECT NOTES COMPUT SC, V8691, P346, DOI [arXiv:1406.4729, 10.1007/978-3-319-10578-9_23]
   HRADI M, 2015, BRIT MACH VIS C
   Hu DD, 2023, VISUAL COMPUT, V39, P281, DOI 10.1007/s00371-021-02329-6
   Huang JB, 2015, PROC CVPR IEEE, P5197, DOI 10.1109/CVPR.2015.7299156
   Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.182, 10.1109/CVPR.2016.181]
   Kingma D. P., 2014, arXiv
   Kupyn O, 2018, PROC CVPR IEEE, P8183, DOI 10.1109/CVPR.2018.00854
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Li JA, 2017, PROC CVPR IEEE, P1951, DOI 10.1109/CVPR.2017.211
   Li XS, 2020, SIGNAL PROCESS-IMAGE, V83, DOI 10.1016/j.image.2020.115805
   Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151
   Liu ZW, 2017, IEEE I CONF COMP VIS, P4473, DOI [10.1109/ICCVW.2017.361, 10.1109/ICCV.2017.478]
   Lumentut J., 2020, DEEP NEURAL NETWORK, V95, DOI [10.1117/12.2566962, DOI 10.1117/12.2566962]
   Ma JQ, 2022, PROC CVPR IEEE, P5901, DOI 10.1109/CVPR52688.2022.00582
   Niu WJ, 2021, IEEE T IMAGE PROCESS, V30, P7101, DOI 10.1109/TIP.2021.3101402
   Odena A., 2016, DISTILL, V1, pe3, DOI 10.23915/distill.00003.-URL
   Park H, 2017, IEEE I CONF COMP VIS, P4623, DOI 10.1109/ICCV.2017.494
   Su SC, 2017, PROC CVPR IEEE, P237, DOI 10.1109/CVPR.2017.33
   Tao X, 2018, PROC CVPR IEEE, P8174, DOI 10.1109/CVPR.2018.00853
   Wang XT, 2019, LECT NOTES COMPUT SC, V11133, P63, DOI 10.1007/978-3-030-11021-5_5
   Xing WZ, 2021, PROC CVPR IEEE, P3506, DOI 10.1109/CVPR46437.2021.00351
   Xu L, 2013, PROC CVPR IEEE, P1107, DOI 10.1109/CVPR.2013.147
   Xu XY, 2017, IEEE I CONF COMP VIS, P251, DOI 10.1109/ICCV.2017.36
   Yamaguchi T, 2011, LECT NOTES COMPUT SC, V6495, P127, DOI 10.1007/978-3-642-19282-1_11
   Yang JC, 2010, IEEE T IMAGE PROCESS, V19, P2861, DOI 10.1109/TIP.2010.2050625
   Yoon Y., 2015, IEEE INT C COMPUTER, DOI [10.1109/TPAMI.2015.2439281, DOI 10.1109/TPAMI.2015.2439281]
   Yoon Y, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P57, DOI 10.1109/ICCVW.2015.17
   Zhang HC, 2011, IEEE I CONF COMP VIS, P770, DOI 10.1109/ICCV.2011.6126315
   Zhang X., 2018, ARXIV
   Zhang XY, 2018, PROCEEDINGS OF THE 2ND INTERNATIONAL CONFERENCE ON VISION, IMAGE AND SIGNAL PROCESSING (ICVISP 2018), DOI 10.1145/3271553.3271554
NR 35
TC 0
Z9 0
U1 2
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2024
VL 40
IS 4
BP 2667
EP 2678
DI 10.1007/s00371-023-02970-3
EA JUL 2023
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA MZ2U3
UT WOS:001034268300002
DA 2024-07-18
ER

PT J
AU Ito, T
   Endo, Y
   Kanamori, Y
AF Ito, Taishi
   Endo, Yuki
   Kanamori, Yoshihiro
TI Age-dependent face diversification via latent space analysis
SO VISUAL COMPUTER
LA English
DT Article
DE Age transformation; Image editing; Deep learning; GAN; Multimodal
AB Facial age transformation methods can change facial appearance according to the target age. However, most existing methods do not consider that people get older with different attribute changes (e.g., wrinkles, hair volume, and face shape) depending on their circumstances and environment. Diversifying such age-dependent attributes while preserving a person's identity is crucial to broaden the applications of age transformation. In addition, the accuracy of age transformation to childhood is limited due to dataset bias. To solve these problems, we propose an age transformation method based on latent space analysis of StyleGAN. Our method obtains diverse age-transformed images by randomly manipulating age-dependent attributes in a latent space. To do so, we analyze the latent space and perturb channels affecting age-dependent attributes. We then optimize the perturbed latent code to refine the age and identity of the output image. We also present an unsupervised approach for improving age transformation to childhood. Our approach is based on the assumption that existing methods cannot sufficiently move a latent code toward a desired direction. We extrapolate an estimated latent path and iteratively update the latent code along the extrapolated path until the output image reaches the target age. Quantitative and qualitative comparisons with existing methods show that our method improves output diversity and preserves the target age and identity. We also show that our method can more accurately perform age transformation to childhood.
C1 [Ito, Taishi; Endo, Yuki; Kanamori, Yoshihiro] Univ Tsukuba, Tsukuba, Ibaraki, Japan.
C3 University of Tsukuba
RP Ito, T (corresponding author), Univ Tsukuba, Tsukuba, Ibaraki, Japan.
EM itohlee14@gmail.com; endo@cs.tsukuba.ac.jp; kanamori@cs.tsukuba.ac.jp
CR Abdal R, 2019, IEEE I CONF COMP VIS, P4431, DOI 10.1109/ICCV.2019.00453
   Alaluf Y, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459805
   Deng JK, 2019, PROC CVPR IEEE, P4685, DOI 10.1109/CVPR.2019.00482
   Fang H, 2020, IEEE COMPUT SOC CONF, P3500, DOI 10.1109/CVPRW50498.2020.00410
   Georgopoulos M., 2020, P IEEE CVF C COMP VI, P14
   Gomez-Trenado G, 2022, LECT NOTES COMPUT SC, V13676, P565, DOI 10.1007/978-3-031-19787-1_32
   Hensel M, 2017, ADV NEUR IN, V30
   Inc. M., FAC RES TOOLK
   Isola Phillip, 2016, ARXIV161107004, DOI [DOI 10.1109/CVPR.2017.632, 10.1109/CVPR.2017.632]
   Karras T, 2021, IEEE T PATTERN ANAL, V43, P4217, DOI 10.1109/TPAMI.2020.2970919
   Karras Tero, 2020, IEEE C COMP VIS PATT
   Kingma D. P., 2014, arXiv
   Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425
   Or-El Roy, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12351), P739, DOI 10.1007/978-3-030-58539-6_44
   Parihar R, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P1828, DOI 10.1145/3503161.3547972
   Patashnik O, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P2065, DOI 10.1109/ICCV48922.2021.00209
   Peipei Li, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12348), P86, DOI 10.1007/978-3-030-58580-8_6
   Radford A, 2021, PR MACH LEARN RES, V139
   Richardson E, 2021, PROC CVPR IEEE, P2287, DOI 10.1109/CVPR46437.2021.00232
   Rothe R, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P252, DOI 10.1109/ICCVW.2015.41
   Shen Yujun, 2020, P IEEE CVF C COMP VI, P9243, DOI DOI 10.1109/CVPR42600.2020.00926
   Viazovetskyi Yuri, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12367), P170, DOI 10.1007/978-3-030-58542-6_11
   Wu ZZ, 2021, PROC CVPR IEEE, P12858, DOI 10.1109/CVPR46437.2021.01267
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   zllrunning/face-parsing, PYTORCH US MOD BIS F
NR 25
TC 0
Z9 0
U1 1
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2023
VL 39
IS 8
SI SI
BP 3221
EP 3233
DI 10.1007/s00371-023-03000-y
EA JUL 2023
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA P2DS6
UT WOS:001026397800005
OA Bronze
DA 2024-07-18
ER

PT J
AU Sibrina, D
   Bethapudi, S
   Koulieris, GA
AF Sibrina, David
   Bethapudi, Sarath
   Koulieris, George Alex
TI OrthopedVR: clinical assessment and pre-operative planning of paediatric
   patients with lower limb rotational abnormalities in virtual reality
SO VISUAL COMPUTER
LA English
DT Article
DE Virtual reality; Surgical planning; Virtual collaborative osteotomies;
   Decision-making; VR collaborative environment
ID FEMORAL ANTEVERSION; RADIOGRAPHY; CT
AB Rotational abnormalities in the lower limbs causing patellar mal-tracking negatively affect patients' lives, particularly young patients (10-17 years old). Recent studies suggest that rotational abnormalities can increase degenerative effects on the joints of the lower limbs. Rotational abnormalities are diagnosed using 2D CT imaging and X-rays, and these data are then used by surgeons to make decisions during an operation. However, 3D representation of data is preferable in the examination of 3D structures, such as bones. This correlates with added benefits for medical judgement, pre-operative planning, and clinical training. Virtual reality can enable the transformation of standard clinical imaging examination methods (CT/MRI) into immersive examinations and pre-operative planning in 3D. We present a VR system (OrthopedVR) which allows orthopaedic surgeons to examine patients' specific anatomy of the lower limbs in an immersive three-dimensional environment and to simulate the effect of potential surgical interventions such as corrective osteotomies in VR. In OrthopedVR, surgeons can perform corrective incisions and re-align segments into desired rotational angles. From the system evaluation performed by experienced surgeons we found that OrthopedVR provides a better understanding of lower limb alignment and rotational profiles in comparison with isolated 2D CT scans. In addition, it was demonstrated that using VR software improves pre-operative planning, surgical precision and post-operative outcomes for patients. Our study results indicate that our system can become a stepping stone into simulating corrective surgeries of the lower limbs, and suggest future improvements which will help adopt VR surgical planning into the clinical orthopaedic practice.
C1 [Sibrina, David; Koulieris, George Alex] Univ Durham, Durham, England.
   [Bethapudi, Sarath] Univ Hosp North Durham, Durham, England.
C3 Durham University
RP Sibrina, D (corresponding author), Univ Durham, Durham, England.
EM david.sibrina@durham.ac.uk
RI Sibřina, David/JVZ-5391-2024; Koulieris, George/AAJ-5666-2020
OI Koulieris, George/0000-0003-1610-6240; Sibrina,
   David/0000-0002-1274-0546
FU Engineering and Physical Sciences Research Council (EPSRC) [2456825]
FX Clinical and Pre-operative Assessment of Lower Limb Rotational Profiles
   in Virtual and Augmented Reality. Funder: Engineering and Physical
   Sciences Research Council (EPSRC). Grant Number: 2456825.
CR Alsofy SZ, 2020, J CRANIOFAC SURG, V31, P1865, DOI 10.1097/SCS.0000000000006525
   [Anonymous], 2021, SLICER SLICER WIKI
   [Anonymous], 2021, OC CONF UN SETT
   Ayoub A, 2019, BMC ORAL HEALTH, V19, DOI 10.1186/s12903-019-0937-8
   Baur Wilhelm, 2005, Oper Orthop Traumatol, V17, P326, DOI 10.1007/s00064-005-1136-0
   Chaibi Y, 2012, COMPUT METHOD BIOMEC, V15, P457, DOI 10.1080/10255842.2010.540758
   Chauhan S.K., 2008, ORTHOPAEDIC P, V90
   Chheang V, 2021, COMPUT GRAPH-UK, V99, P234, DOI 10.1016/j.cag.2021.07.009
   Cignoni P., 2008, P EUR IT CHAPT C, P129, DOI [DOI 10.2312/LOCALCHAPTEREVENTS/ITALCHAP/ITALIANCHAPCONF2008/129-136, 10.2312/LocalChapterEvents/ItalChap/ItalianChapConf2008/129-136]
   Dobbe AM, 2017, J PAEDIATR CHILD H, V53, P1077, DOI 10.1111/jpc.13756
   Farr J., 2011, Anterior Knee Pain and Patellar Instability, V2nd, P455
   Garland M., SURFACE SIMPLIFICATI
   Goh GS, 2021, ARCH ORTHOP TRAUM SU, V141, P2303, DOI 10.1007/s00402-021-04037-1
   Goldman LW, 2007, J NUCL MED TECHNOL, V35, P115, DOI 10.2967/jnmt.107.042978
   Izard SG, 2019, J MED SYST, V43, DOI 10.1007/s10916-019-1239-z
   Gruskay JA, 2019, JBJS REV, V7, DOI 10.2106/JBJS.RVW.18.00016
   Guggenberger R, 2014, AM J ROENTGENOL, V202, pW161, DOI 10.2214/AJR.13.10782
   Gustafsson A, 2019, ACTA ORTHOP, V90, P348, DOI 10.1080/17453674.2019.1607111
   Holland CL., 2011, ECR, V2011, P1
   Hospital for Special Surgery, 2021, HIP FEM ANT CAUS SYM
   Hosseinzadeh Pooya, 2016, Iowa Orthop J, V36, P123
   Jibri Z, 2019, INSIGHTS IMAGING, V10, DOI 10.1186/s13244-019-0755-1
   Kaiser P, 2016, ARCH ORTHOP TRAUM SU, V136, P1259, DOI 10.1007/s00402-016-2536-3
   Kim HJ, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10186288
   Kockro RA, 2016, WORLD NEUROSURG, V96, P489, DOI 10.1016/j.wneu.2016.08.124
   Kuo TY, 2003, INVEST RADIOL, V38, P221
   LaViola Joseph J., 2017, 3D User interfaces: theory and practice
   Lohre R, 2020, JAMA NETW OPEN, V3, DOI 10.1001/jamanetworkopen.2020.31217
   MURPHY SB, 1994, ORTHOP CLIN N AM, V25, P477
   Murray R, 2021, JAAOS GLOB RES REV, V5, DOI 10.5435/JAAOSGlobal-D-21-00141
   Palmer H., 2019, UK KNEE OSTEOTOMY RE
   PAPAGIANNAKIS G, 2018, TRANSFORMING MED ED, P1, DOI DOI 10.1145/3283289.3283291
   Pfeiffer M, 2018, INT J COMPUT ASS RAD, V13, P741, DOI 10.1007/s11548-018-1730-x
   Quero G, 2019, SURG ONCOL CLIN N AM, V28, P31, DOI 10.1016/j.soc.2018.08.002
   RUWE PA, 1992, J BONE JOINT SURG AM, V74A, P820, DOI 10.2106/00004623-199274060-00003
   Sadeghi AH, 2020, EUR HEART J, V41, P3279, DOI 10.1093/eurheartj/ehaa518
   Sereno M, 2022, IEEE T VIS COMPUT GR, V28, P2530, DOI 10.1109/TVCG.2020.3032761
   Stadie AT, 2008, J NEUROSURG, V108, P382, DOI 10.3171/JNS/2008/108/2/0382
   Thakrar RR, 2019, J ORTHOP SURG-HONG K, V27, DOI 10.1177/2309499019868148
   Unity Technologies, 2021, UN MAN PROGR LIGHTM
   Unity Technologies, 2021, UN MAN LIGHT MOD SHA
   Verhey JT, 2020, INT J MED ROBOT COMP, V16, DOI 10.1002/rcs.2067
   Vertemati M, 2019, SURG INNOV, V26, P359, DOI 10.1177/1553350618822860
   Wang D., 2017, MEDICINE
   YEN JC, 1995, IEEE T IMAGE PROCESS, V4, P370, DOI 10.1109/83.366472
NR 45
TC 1
Z9 1
U1 4
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2023
VL 39
IS 8
SI SI
BP 3621
EP 3633
DI 10.1007/s00371-023-02949-0
EA JUN 2023
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA P2DS6
UT WOS:001018118800001
OA hybrid, Green Published
DA 2024-07-18
ER

PT J
AU Chen, H
   Zhao, JY
   Zhang, Q
AF Chen, Hao
   Zhao, Jieyu
   Zhang, Qiang
TI Rotation-equivariant spherical vector networks for objects recognition
   with unknown poses
SO VISUAL COMPUTER
LA English
DT Article
DE Equivariance; Vector networks; Routing; Part-whole hierarchy; Spherical
   convolutional neural networks
ID CNNS
AB Analyzing 3D objects without pose priors using neural networks is challenging. In view of the shortcoming that spherical convolutional networks lack the construction of a part-whole hierarchy with rotation equivariance for 3D object recognition with unknown poses, which generates whole rotation-equivariant features that cannot be effectively preserved, a rotation-equivariant part-whole hierarchy spherical vector network is proposed in this paper. In our experiments, we map a 3D object onto the unit sphere, construct an ordered list of vectors from the convolutional layers of the rotation-equivariant spherical convolutional network and then construct a part-whole hierarchy to classify the 3D object using the proposed rotation-equivariant routing algorithm. The experimental results show that the proposed method improves not only the recognition of 3D objects with known poses, but also the recognition of 3D objects with unknown poses compared to previous spherical convolutional neural networks. This finding validates the construction of the rotation-equivariant part-whole hierarchy.
C1 [Chen, Hao; Zhao, Jieyu; Zhang, Qiang] Ningbo Univ, Fac Elect Engn & Comp Sci, Ningbo 315000, Peoples R China.
C3 Ningbo University
RP Zhao, JY (corresponding author), Ningbo Univ, Fac Elect Engn & Comp Sci, Ningbo 315000, Peoples R China.
EM nbu_chenhao@126.com; zhao_jieyu@nbu.edu.cn; qiangzibro@qq.com
FU National Natural Science Foundation of China [62071260, 62006131];
   Natural Science Foundation of Zhejiang Province [LZ22F020001,
   LQ21F020009]
FX This research was supported by the National Natural Science Foundation
   of China (Grant Nos. 62071260 and 62006131) and the Natural Science
   Foundation of Zhejiang Province (Grant Nos.LZ22F020001 and LQ21F020009).
CR Bahadori Mohammad Taha, 2018, INT C LEARN REPR WOR
   Batzner S, 2022, NAT COMMUN, V13, DOI 10.1038/s41467-022-29939-5
   Bengio Y, 2021, COMMUN ACM, V64, P58, DOI 10.1145/3448250
   Bronstein MM, 2010, PROC CVPR IEEE, P1704, DOI 10.1109/CVPR.2010.5539838
   Chen Y, 2022, ENTROPY-SWITZ, V24, DOI 10.3390/e24050678
   Chen Y, 2021, IEEE T MULTIMEDIA, V23, P3098, DOI 10.1109/TMM.2020.3020693
   Chen YT, 2023, J VIS COMMUN IMAGE R, V91, DOI 10.1016/j.jvcir.2023.103776
   Chen YT, 2024, VISUAL COMPUT, V40, P489, DOI 10.1007/s00371-023-02795-0
   Chen YT, 2021, APPL INTELL, V51, P4367, DOI 10.1007/s10489-020-02116-1
   Cohen T.S., 2018, P 6 INT C LEARN REPR, P1
   Cohen TS, 2016, PR MACH LEARN RES, V48
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Esteves C, 2020, INT J COMPUT VISION, V128, P588, DOI 10.1007/s11263-019-01220-1
   Ganea Octavian-Eugen, 2022, INT C LEARN REPR
   Guo YL, 2021, IEEE T PATTERN ANAL, V43, P4338, DOI 10.1109/TPAMI.2020.3005434
   Han JM, 2021, PROC CVPR IEEE, P2785, DOI 10.1109/CVPR46437.2021.00281
   Hinton G.E., 2018, INT C LEARN REPR
   Hinton G, 2023, NEURAL COMPUT, V35, P413, DOI 10.1162/neco_a_01557
   Huang Y., 2022, INT C MACHINE LEARNI, P9280
   Jiang Chiyu, 2019, INT C LEARNING REPRE
   Kanezaki A, 2018, PROC CVPR IEEE, P5010, DOI 10.1109/CVPR.2018.00526
   Kazhdan M, 2012, COMPUT GRAPH FORUM, V31, P1745, DOI 10.1111/j.1467-8659.2012.03179.x
   Kondor R., 2018, ARXIV180609231, V31, P10138
   Kostelec PJ, 2008, J FOURIER ANAL APPL, V14, P145, DOI 10.1007/s00041-008-9013-5
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lafarge MW, 2021, MED IMAGE ANAL, V68, DOI 10.1016/j.media.2020.101849
   Lee Y, 2022, IEEE T PATTERN ANAL, V44, P834, DOI 10.1109/TPAMI.2020.2997045
   Lenc K, 2019, INT J COMPUT VISION, V127, P456, DOI 10.1007/s11263-018-1098-y
   Li DF, 2021, KNOWL-BASED SYST, V211, DOI 10.1016/j.knosys.2020.106522
   Lian YF, 2023, VISUAL COMPUT, V39, P749, DOI 10.1007/s00371-021-02372-3
   Lian Z., 2015, PROC 8 EUROGRAPHICS, P107
   Liu X, 2021, KNOWL-BASED SYST, V212, DOI 10.1016/j.knosys.2020.106611
   McEwen J.D., 2022, INT C LEARNING REPRE
   Patrick MK, 2022, J KING SAUD UNIV-COM, V34, P1295, DOI 10.1016/j.jksuci.2019.09.014
   Perraudin N, 2019, ASTRON COMPUT, V27, P130, DOI 10.1016/j.ascom.2019.03.004
   Qi CR, 2017, ADV NEUR IN, V30
   Rusu RB, 2009, IEEE INT CONF ROBOT, P1848
   Sabour S, 2017, ADV NEUR IN, V30
   Shen ZY, 2021, AAAI CONF ARTIF INTE, V35, P9585
   Spezialetti Riccardo, 2020, ADV NEURAL INFORM PR, V33
   Su YC, 2017, ADV NEUR IN, V30
   Wang D., 2018, P 6 INT C LEARN REPR, DOI DOI 10.14722/NDSS.2018.23142
   Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362
   Weiler M., 2019, Advances in neural information processing systems, V32, P14334
   Wiersma R, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392437
   Xia RL, 2022, J KING SAUD UNIV-COM, V34, P6008, DOI 10.1016/j.jksuci.2022.02.004
   You Y, 2022, IEEE T PATTERN ANAL, V44, P9489, DOI 10.1109/TPAMI.2021.3130590
   You Y, 2020, AAAI CONF ARTIF INTE, V34, P12717
   Zhao Q, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1198
   Zhao Y., 2020, EUR C COMP VIS, P1, DOI DOI 10.1007/978-3-030-58452-81
   Zheng Y, 2021, ALGORITHMS, V14, DOI 10.3390/a14030099
NR 51
TC 1
Z9 1
U1 1
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2024
VL 40
IS 3
BP 2089
EP 2101
DI 10.1007/s00371-023-02904-z
EA JUN 2023
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HY8O3
UT WOS:001014470900003
DA 2024-07-18
ER

PT J
AU Ren, DY
   Li, JW
   Wu, ZY
   Guo, J
   Wei, MQ
   Guo, YW
AF Ren, Dayong
   Li, Jiawei
   Wu, Zhengyi
   Guo, Jie
   Wei, Mingqiang
   Guo, Yanwen
TI MFFNet: multimodal feature fusion network for point cloud semantic
   segmentation
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Feature fusion; Point cloud semantic segmentation
AB We introduce a multimodal feature fusion network (MFFNet) for 3D point cloud semantic segmentation. Unlike previous methods that directly learn from colored point clouds (XYZRGB), MFFNet transforms point clouds to 2D RGB image and frequency image representations for efficient multimodal feature fusion. For each point, MFFNet performs a local projection by automatically learning a weighted orthogonal projection to softly project surrounding points onto 2D images. Regular 2D convolution can thus be applied to these regular grids for efficient semantic feature learning. Then, we fuse 2D semantic features into 3D point cloud features by using a multimodal feature fusion module (MFF). MFF module could employ high-level features from 2D RGB images and frequency images to boost the intrinsic correlation and discriminability of different structure features from the point cloud. In particular, the discriminative descriptions are quantified and leveraged as the local soft attention mask further to enforce the structure feature of the semantic categories. We have evaluated the proposed method on the S3DIS and ScanNet datasets. Experimental results and comparisons with four backbone methods demonstrate that our framework can perform better.
C1 [Ren, Dayong; Li, Jiawei; Wu, Zhengyi; Guo, Jie; Guo, Yanwen] Nanjing Univ, Natl Key Lab Novel Software Technol, Nanjing 210023, Peoples R China.
   [Wei, Mingqiang] Nanjing Univ Aeronaut & Astronaut, Nanjing 211106, Peoples R China.
C3 Nanjing University; Nanjing University of Aeronautics & Astronautics
RP Guo, J; Guo, YW (corresponding author), Nanjing Univ, Natl Key Lab Novel Software Technol, Nanjing 210023, Peoples R China.
EM rdyedu@gmail.com; lijiawei@smail.nju.edu.cn; wuzy@smail.nju.edu.cn;
   guojie@nju.edu.cn; mqwei@nuaa.edu.cn; ywguo@nju.edu.cn
RI Li, Jiawei/AAR-5540-2021; Wang, Jiawei/KHC-8971-2024
OI Li, Jiawei/0000-0002-6661-6946; 
CR Armeni I., 2017, arXiv
   Armeni I, 2016, PROC CVPR IEEE, P1534, DOI 10.1109/CVPR.2016.170
   Baltrusaitis T, 2019, IEEE T PATTERN ANAL, V41, P423, DOI 10.1109/TPAMI.2018.2798607
   Behley J, 2019, IEEE I CONF COMP VIS, P9296, DOI 10.1109/ICCV.2019.00939
   Cao JJ, 2022, IEEE T NEUR NET LEAR, DOI 10.1109/TNNLS.2021.3135655
   Chen XZ, 2017, PROC CVPR IEEE, P6526, DOI 10.1109/CVPR.2017.691
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Dai A, 2017, PROC CVPR IEEE, P2432, DOI 10.1109/CVPR.2017.261
   Howard AG, 2017, Arxiv, DOI arXiv:1704.04861
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Janai J, 2020, FOUND TRENDS COMPUT, V12, P1, DOI 10.1561/0600000079
   Jaritz M, 2019, IEEE INT CONF COMP V, P3995, DOI 10.1109/ICCVW.2019.00494
   Larsson G., 2017, ICLR, P1
   Li Y., Adv. Neural Inform. Process. Syst. (NIPS), P828
   Liu TR, 2022, VISUAL COMPUT, V38, P2303, DOI 10.1007/s00371-021-02112-7
   Meng QH, 2022, IEEE T PATTERN ANAL, V44, P4454, DOI 10.1109/TPAMI.2021.3063611
   Qi C., 2017, NeurIPS, P5105, DOI [10.1109/ CVPR.2017.16, DOI 10.1109/CVPR.2017.16]
   Qian GC, 2022, Arxiv, DOI [arXiv:2206.04670, 10.48550/ARXIV.2206.04670]
   Reddy K.K., 2015, HPEC, P1
   Ren DY, 2022, SCI CHINA INFORM SCI, V65, DOI 10.1007/s11432-021-3387-7
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rizzoli G, 2022, TECHNOLOGIES, V10, DOI 10.3390/technologies10040090
   Shi SS, 2019, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2019.00086
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Su H, 2015, IEEE I CONF COMP VIS, P945, DOI 10.1109/ICCV.2015.114
   Sun YL, 2020, VISUAL COMPUT, V36, P2407, DOI 10.1007/s00371-020-01892-8
   Tosteberg P., 2017, SEMANTIC SEGMENTATIO
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang JD, 2016, Arxiv, DOI arXiv:1605.07716
   Wu WX, 2019, PROC CVPR IEEE, P9613, DOI 10.1109/CVPR.2019.00985
   Wu ZR, 2015, PROC CVPR IEEE, P1912, DOI 10.1109/CVPR.2015.7298801
   Yang F., 2022, IEEE T IMAGE PROCESS, V32, P2889
   Ye M, 2020, IEEE T IMAGE PROCESS, V29, P9387, DOI 10.1109/TIP.2020.2998275
   Yin JB, 2022, LECT NOTES COMPUT SC, V13698, P727, DOI 10.1007/978-3-031-19839-7_42
   Yin JB, 2022, LECT NOTES COMPUT SC, V13699, P17, DOI 10.1007/978-3-031-19842-7_2
   Yin JB, 2023, IEEE T PATTERN ANAL, V45, P9822, DOI 10.1109/TPAMI.2021.3125981
   You HX, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1310, DOI 10.1145/3240508.3240702
   Zhang ZY, 2019, IEEE I CONF COMP VIS, P1607, DOI 10.1109/ICCV.2019.00169
   Zhou T, 2020, IEEE T MED IMAGING, V39, P2772, DOI 10.1109/TMI.2020.2975344
   Zhu XG, 2021, PROC CVPR IEEE, P9934, DOI 10.1109/CVPR46437.2021.00981
NR 40
TC 2
Z9 2
U1 15
U2 37
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2023 JUN 11
PY 2023
DI 10.1007/s00371-023-02907-w
EA JUN 2023
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA I5IO2
UT WOS:001003117800001
DA 2024-07-18
ER

PT J
AU Labrada, A
   Bustos, B
   Sipiran, I
AF Labrada, Arniel
   Bustos, Benjamin
   Sipiran, Ivan
TI A convolutional architecture for 3D model embedding using image views
SO VISUAL COMPUTER
LA English
DT Article
DE 3D model; Deep learning; Convolutional neural network; Embedding
ID CLASSIFICATION
AB During the last years, many advances have been made in tasks like 3D model retrieval, 3D model classification, and 3D model segmentation. The typical 3D representations such as point clouds, voxels, and polygon meshes are mostly suitable for rendering purposes, while their use for cognitive processes (retrieval, classification, segmentation) is limited due to their high redundancy and complexity. We propose a deep learning architecture to handle 3D models represented as sets of image views as input. Our proposed architecture combines other standard architectures, like Convolutional Neural Networks and autoencoders, for computing 3D model embeddings using sets of image views extracted from the 3D models, avoiding the common view pooling layer approach used in these cases. Our goal is to represent a 3D model as a vector with enough information so it can substitute the 3D model for high-level tasks. Since this vector is a learned representation which tries to capture the relevant information of a 3D model, we show that the embedding representation conveys semantic information that helps to deal with the similarity assessment of 3D objects. We compare our proposed embedding technique with state-of-the-art techniques for 3D Model Retrieval using the ShapeNet and ModelNet datasets. We show that the embeddings obtained with our proposed architecture allow us to obtain a high effectiveness score in both normalized and perturbed versions of the ShapeNet dataset while improving the training and inference times compared to the standard state-of-the-art techniques.
C1 [Labrada, Arniel; Bustos, Benjamin; Sipiran, Ivan] Univ Chile, Dept Comp Sci, Santiago, Chile.
C3 Universidad de Chile
RP Labrada, A (corresponding author), Univ Chile, Dept Comp Sci, Santiago, Chile.
EM alabrada@dcc.uchile.cl; bebustos@dcc.uchile.cl; isipiran@dcc.uchile.cl
RI Bustos, Benjamin/G-1170-2010
OI Labrada, Arniel/0000-0002-4843-5711
FU ANID - Millennium Science Initiative Program [ICN17_002]
FX This work was funded by ANID - Millennium Science Initiative
   Program-Code ICN17_002.
CR Achlioptas P., 2018, 6 INT C LEARN REPR I
   Ahmed E., 2018, ARXIV
   Aktar S., 2019, 2019 INT C ELECT COM, P1
   Nguyen A, 2013, PROCEEDINGS OF THE 2013 6TH IEEE CONFERENCE ON ROBOTICS, AUTOMATION AND MECHATRONICS (RAM), P225, DOI 10.1109/RAM.2013.6758588
   [Anonymous], 2015, PROC CVPR IEEE, DOI [DOI 10.1109/CVPR.2015.7298801, 10.1109/CVPR.2015.7298801]
   [Anonymous], 2014, P 2014 C EMP METH NA, DOI DOI 10.3115/V1/D14-1005
   [Anonymous], 2021, 3 NOVEL INTELLIGENT, P405, DOI [10.1109/NILES53778.2021.9600493, DOI 10.1109/NILES53778.2021.9600493]
   Cerri A., EUR WORKSH 3D OBJ RE
   Chang A. X., 2015, ARXIV
   Chen Xuelin, 2020, INT C LEARNING REPRE
   Conneau A., 2017, P 2017 C EMPIRCAL ME, P670, DOI [DOI 10.18653/V1/D17-1070, 10.18653/v1/d17-1070]
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   David OE, 2016, LECT NOTES COMPUT SC, V9887, P20, DOI 10.1007/978-3-319-44781-0_3
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Dumoulin V, 2018, Arxiv, DOI [arXiv:1603.07285, DOI 10.48550/ARXIV.1603.07285]
   Gordo A, 2016, LECT NOTES COMPUT SC, V9910, P241, DOI 10.1007/978-3-319-46466-4_15
   Guo HY, 2016, IEEE T IMAGE PROCESS, V25, P5526, DOI 10.1109/TIP.2016.2609814
   Guo YL, 2021, IEEE T PATTERN ANAL, V43, P4338, DOI 10.1109/TPAMI.2020.3005434
   Han ZZ, 2019, IEEE T IMAGE PROCESS, V28, P3986, DOI 10.1109/TIP.2019.2904460
   Ioannidou A, 2017, ACM COMPUT SURV, V50, DOI 10.1145/3042064
   Kanezaki A, 2021, IEEE T PATTERN ANAL, V43, P269, DOI 10.1109/TPAMI.2019.2922640
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Li B., EUR WORKSH 3D OBJ RE
   Li HS, 2018, IEEE ACCESS, V6, P31854, DOI 10.1109/ACCESS.2018.2845362
   Lin DY, 2022, KNOWL-BASED SYST, V247, DOI 10.1016/j.knosys.2022.108754
   Liu YC, 2019, IEEE I CONF COMP VIS, P5238, DOI 10.1109/ICCV.2019.00534
   Liu YC, 2019, PROC CVPR IEEE, P8887, DOI 10.1109/CVPR.2019.00910
   Liu ZH, 2022, PATTERN RECOGN, V129, DOI 10.1016/j.patcog.2022.108774
   Papadakis P, 2010, INT J COMPUT VISION, V89, P177, DOI 10.1007/s11263-009-0281-6
   Pennington J., 2014, P 2014 C EMP METH NA, P1532
   Qi CR, 2016, PROC CVPR IEEE, P5648, DOI 10.1109/CVPR.2016.609
   Rong X., 2014, Word2vec Parameter Learning Explained, V1, P21
   Sfikas K, 2018, COMPUT GRAPH-UK, V71, P208, DOI 10.1016/j.cag.2017.12.001
   Shamir A, 2008, COMPUT GRAPH FORUM, V27, P1539, DOI 10.1111/j.1467-8659.2007.01103.x
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sitzmann V, 2019, PROC CVPR IEEE, P2432, DOI 10.1109/CVPR.2019.00254
   Su H, 2015, IEEE I CONF COMP VIS, P945, DOI 10.1109/ICCV.2015.114
   Sun K, 2021, IEEE T IMAGE PROCESS, V30, P868, DOI 10.1109/TIP.2020.3039378
   Turchenko V., 2017, ARXIV
   Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362
   Wei X, 2020, PROC CVPR IEEE, P1847, DOI 10.1109/CVPR42600.2020.00192
   Yi KM, 2016, LECT NOTES COMPUT SC, V9910, P467, DOI 10.1007/978-3-319-46466-4_28
   Zhao L, 2015, VISUAL COMPUT, V31, P765, DOI 10.1007/s00371-015-1091-1
   Zuva Keneilwe, 2012, International Journal of Computer Science & Information Technology, V4, P35, DOI 10.5121/ijcsit.2012.4304
NR 45
TC 0
Z9 0
U1 1
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2024
VL 40
IS 3
BP 1601
EP 1615
DI 10.1007/s00371-023-02872-4
EA APR 2023
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HY8O3
UT WOS:000978496000001
DA 2024-07-18
ER

PT J
AU Zhang, HG
   Fang, JS
   Hu, SY
   Zeng, K
AF Zhang, Hongao
   Fang, Jinsheng
   Hu, Siyu
   Zeng, Kun
TI A sparse lightweight attention network for image super-resolution
SO VISUAL COMPUTER
LA English
DT Article
DE Image super-resolution; Lightweight; Attention mechanism; Convolutional
   neural network
ID INTERPOLATION
AB Recently, deep learning methods have been widely applied to single image super-resolution reconstruction tasks and achieved great improvement in both quantitative and qualitative evaluations. However, most of the existing convolutional neural network-based methods commonly reduce the number of layers or channels to obtain lightweight model. These strategies may reduce the representation ability of informative features and degrade the network performance. To address this issue, we propose a sparse lightweight attention network (SLAN), a novel SISR algorithm to keep informative features between layers. Specially, a sparse attention feature fusion module with lightweight attention and sparse extracting modules is developed to expand feature extracting receptive field and enhance the informative feature extracting ability. To take advantage of the multi-level features and keep fewer multi-adds, cross fusion is used and demonstrated usefully. Extensive experimental results on public datasets demonstrate the superior performance of our proposed SLAN. The average PSNR(dB)/SSIM values of SLAN are about 0.04/0.0004, 0.06/0.0009, and 0.09/0.0018 larger than the competitors under scaling factors of x 2, x 3 and x 4, respectively. Our SLAN benefits from its model size and low computation cost and may be deployed on mobile platforms.
C1 [Zhang, Hongao; Fang, Jinsheng; Hu, Siyu] Minnan Normal Univ, Sch Comp Sci & Engn, Zhangzhou 363000, Peoples R China.
   [Zhang, Hongao; Fang, Jinsheng; Hu, Siyu] Key Lab Data Sci & Intelligence Applicat, Zhangzhou 363000, Fujian, Peoples R China.
   [Zeng, Kun] Minjiang Univ, Sch Comp & Control Engn, Fuzhou 350000, Peoples R China.
   [Zeng, Kun] Minjiang Univ, Fujian Prov Key Lab Informat Proc & Intelligent C, Fuzhou 350108, Peoples R China.
C3 MinNan Normal University; Minjiang University; Minjiang University
RP Fang, JS (corresponding author), Minnan Normal Univ, Sch Comp Sci & Engn, Zhangzhou 363000, Peoples R China.; Fang, JS (corresponding author), Key Lab Data Sci & Intelligence Applicat, Zhangzhou 363000, Fujian, Peoples R China.
EM fjs1867@mnnu.edu.cn
OI Fang, Jinsheng/0009-0002-0195-3260
FU Natural Science Foundation of Fujian Province [2020J01824, 2021J011005];
   National Natural Science Foundation of China [61601389]; Open Project of
   the Key Laboratory of Plasma and Magnetic Resonance in Fujian Province,
   Xiamen University [20191201]
FX This work was partly supported by Natural Science Foundation of Fujian
   Province (No.2020J01824, 2021J011005), the National Natural Science
   Foundation of China (No. 61601389) and Open Project of the Key
   Laboratory of Plasma and Magnetic Resonance in Fujian Province, Xiamen
   University (No. 20191201).
CR Ahn N, 2018, LECT NOTES COMPUT SC, V11214, P256, DOI 10.1007/978-3-030-01249-6_16
   Bae H, 2021, STRUCT HEALTH MONIT, V20, P1428, DOI 10.1177/1475921720917227
   Ben Niu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P191, DOI 10.1007/978-3-030-58610-2_12
   Chaudhari AS, 2018, MAGN RESON MED, V80, P2139, DOI 10.1002/mrm.27178
   Dai T, 2019, PROC CVPR IEEE, P11057, DOI 10.1109/CVPR.2019.01132
   Dong C, 2016, LECT NOTES COMPUT SC, V9906, P391, DOI 10.1007/978-3-319-46475-6_25
   Dong C, 2014, LECT NOTES COMPUT SC, V8692, P184, DOI 10.1007/978-3-319-10593-2_13
   Du ZC, 2021, IEEE COMPUT SOC CONF, P2494, DOI 10.1109/CVPRW53098.2021.00283
   Fang XL, 2022, PATTERN RECOGN LETT, V153, P107, DOI 10.1016/j.patrec.2021.11.027
   Fu Z., 2022, IEEE T CYBERNETICS, P1
   Gao DD, 2023, EXPERT SYST APPL, V213, DOI 10.1016/j.eswa.2022.118898
   Guo MH, 2022, COMPUT VIS MEDIA, V8, P331, DOI 10.1007/s41095-022-0271-y
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Hui Z, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2024, DOI 10.1145/3343031.3351084
   Hui Z, 2018, PROC CVPR IEEE, P723, DOI 10.1109/CVPR.2018.00082
   Jiang K, 2019, IEEE T GEOSCI REMOTE, V57, P5799, DOI 10.1109/TGRS.2019.2902431
   Jie Liu, 2020, Proceedings of the 16th European Conference on Computer Vision - ECCV 2020 Workshops. Lecture Notes in Computer Science (LNCS 12537), P41, DOI 10.1007/978-3-030-67070-2_2
   Jing YH, 2022, J HYDROL, V613, DOI 10.1016/j.jhydrol.2022.128388
   KEYS RG, 1981, IEEE T ACOUST SPEECH, V29, P1153, DOI 10.1109/TASSP.1981.1163711
   Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.182, 10.1109/CVPR.2016.181]
   Kim KI, 2010, IEEE T PATTERN ANAL, V32, P1127, DOI 10.1109/TPAMI.2010.25
   Lai WS, 2017, PROC CVPR IEEE, P5835, DOI 10.1109/CVPR.2017.618
   Lan RS, 2021, IEEE T CYBERNETICS, V51, P1443, DOI 10.1109/TCYB.2020.2970104
   Li W., 2020, ADV NEURAL INF PROCE, V33, P20343
   Li X, 2001, IEEE T IMAGE PROCESS, V10, P1521, DOI 10.1109/83.951537
   Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151
   Liu J, 2020, PROC CVPR IEEE, P2356, DOI 10.1109/CVPR42600.2020.00243
   Liu X, 2021, PROC ACM INTERACT MO, V5, DOI 10.1145/3448104
   Liu YQ, 2022, IEEE T CIRC SYST VID, V32, P4927, DOI 10.1109/TCSVT.2021.3138431
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Peng CM, 2022, APPL INTELL, V52, P10045, DOI 10.1007/s10489-021-02891-5
   Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207
   Sun B., 2022, arXiv
   Tai Y, 2017, IEEE I CONF COMP VIS, P4549, DOI 10.1109/ICCV.2017.486
   Tang YC, 2023, ENG STRUCT, V274, DOI 10.1016/j.engstruct.2022.115158
   Tang YC, 2022, STRUCTURES, V37, P426, DOI 10.1016/j.istruc.2021.12.055
   Wang ZH, 2021, IEEE T PATTERN ANAL, V43, P3365, DOI 10.1109/TPAMI.2020.2982166
   Wen RM, 2022, NEUROCOMPUTING, V504, P240, DOI 10.1016/j.neucom.2022.07.050
   Yang J., 2008, 2008 IEEE COMPUTER S
   Yi P, 2020, IEEE T CIRC SYST VID, V30, P2503, DOI 10.1109/TCSVT.2019.2925844
   Zhang KB, 2012, IEEE T IMAGE PROCESS, V21, P4544, DOI 10.1109/TIP.2012.2208977
   Zhang L, 2006, IEEE T IMAGE PROCESS, V15, P2226, DOI 10.1109/TIP.2006.877407
   Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI 10.1007/978-3-030-01234-2_18
   Zhang YL, 2018, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR.2018.00262
   Zhao H., 2020, ARXIV
   Zhao H, 2020, COMPUTER VISION ECCV, P56, DOI DOI 10.1007/978-3-030-67070-23
   Zhu LL, 2019, NEUROCOMPUTING, V345, P58, DOI 10.1016/j.neucom.2018.12.077
NR 47
TC 0
Z9 0
U1 7
U2 22
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2024
VL 40
IS 2
BP 1261
EP 1272
DI 10.1007/s00371-023-02845-7
EA APR 2023
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GE6E8
UT WOS:000976793600003
DA 2024-07-18
ER

PT J
AU Zhang, ZJ
   Zheng, JM
   Thalmann, NM
AF Zhang, Zhijie
   Zheng, Jianmin
   Thalmann, Nadia Magnenat
TI Context-aware personality estimation and emotion recognition in social
   interaction
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Personality estimation; Emotion recognition; Nonverbal signals
   processing; Interaction analysis; Deep learning
ID MODEL
AB Personality and emotion as intrinsic factors often have great influences on the cognition of people's behavior. In computer vision, there is a lot of work done on the recognition of emotions, such as classification of a person's emotions via analyzing facial expressions. Relatively there is less work done on personality estimation. Personality, as a long-term characteristic pattern of behavior, influences the emotion generation of a person. In this paper, we present a new method to analyze and estimate personality and emotions in dyadic and multiparty social interactions. We first propose a context-aware deep learning framework that automatically estimates the personality of a target person based on his/her own and the interlocutor's body behavioral and facial information recorded in the interaction process. Then, we expand this architecture to form a method for jointly estimating personality and recognizing emotions. We conduct a series of experiments on two datasets and the experimental results show that the proposed method has good performance in both personality estimation and emotion recognition.
C1 [Zhang, Zhijie; Zheng, Jianmin] Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore, Singapore.
   [Thalmann, Nadia Magnenat] Univ Geneva, MIRALab, CUI, CH-1227 Geneva, Switzerland.
C3 Nanyang Technological University; University of Geneva
RP Zheng, JM (corresponding author), Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore, Singapore.
EM zhijie002@ntu.edu.sg; asjmzheng@ntu.edu.sg; thalmann@miralab.ch
OI ZHANG, ZHIJIE/0000-0002-7164-2751
FU MOE AcRF Tier 1 Grant of Singapore [RG12/22]
FX We thank the authors of MHHRI [6] and the authors of MUMBAI [8] for
   allowing us to use their datasets in our research under the user
   agreement and licensee. The work is partially supported by MOE AcRF Tier
   1 Grant of Singapore (RG12/22).
CR Arriaga O, 2017, Arxiv, DOI arXiv:1710.07557
   Ashton MC, 2009, J PERS ASSESS, V91, P340, DOI 10.1080/00223890902935878
   Baltrusaitis T, 2018, IEEE INT CONF AUTOMA, P59, DOI 10.1109/FG.2018.00019
   Benitez-Quiroz CF, 2016, PROC CVPR IEEE, P5562, DOI 10.1109/CVPR.2016.600
   Burgoon J.K., 2010, Nonverbal communication
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Celiktutan O, 2019, IEEE T AFFECT COMPUT, V10, P484, DOI 10.1109/TAFFC.2017.2737019
   Curto D., P IEEE CVF INT C COM
   Doyran M, 2021, J MULTIMODAL USER IN, V15, P373, DOI 10.1007/s12193-021-00364-0
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Ekman P., 2022, Universal emotions
   Ekman P., 2013, Emotion in the Human Face: Guidelines for Research and an Integration of Findings, V11
   Eleftheriadis S, 2016, IEEE T IMAGE PROCESS, V25, P5727, DOI 10.1109/TIP.2016.2615288
   Ge Z, 2021, Arxiv, DOI arXiv:2107.08430
   Gross JJ, 2011, EMOT REV, V3, P8, DOI 10.1177/1754073910380974
   Gucluturk Y., 2016, LECT NOTES COMPUT SC
   Gürpinar F, 2016, INT C PATT RECOG, P43, DOI 10.1109/ICPR.2016.7899605
   Hall J.A., 2013, NONVERBAL COMMUNICAT, V8th
   Hara K, 2018, PROC CVPR IEEE, P6546, DOI 10.1109/CVPR.2018.00685
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang GB, 2012, IEEE T SYST MAN CY B, V42, P513, DOI 10.1109/TSMCB.2011.2168604
   Jacques JCS Jr, 2022, IEEE T AFFECT COMPUT, V13, P75, DOI 10.1109/TAFFC.2019.2930058
   Jin HF, 2019, KDD'19: PROCEEDINGS OF THE 25TH ACM SIGKDD INTERNATIONAL CONFERENCCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1946, DOI 10.1145/3292500.3330648
   Kay W, 2017, Arxiv, DOI arXiv:1705.06950
   Kosti R, 2017, PROC CVPR IEEE, P1960, DOI 10.1109/CVPR.2017.212
   Lee J, 2019, IEEE I CONF COMP VIS, P10142, DOI 10.1109/ICCV.2019.01024
   Li Y, 2019, IEEE T IMAGE PROCESS, V28, P2439, DOI 10.1109/TIP.2018.2886767
   MCCRAE RR, 1992, J PERS, V60, P175, DOI 10.1111/j.1467-6494.1992.tb00970.x
   Mehrabian A., 1980, BASIC DIMENSIONS GEN
   Mittal Trisha, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P14222, DOI 10.1109/CVPR42600.2020.01424
   Mollahosseini A, 2019, IEEE T AFFECT COMPUT, V10, P18, DOI 10.1109/TAFFC.2017.2740923
   Nicolaou MA, 2011, IEEE T AFFECT COMPUT, V2, P92, DOI 10.1109/T-AFFC.2011.9
   Palmero C., P IEEE CVF WINT C AP
   PASSINI FT, 1966, J PERS SOC PSYCHOL, V4, P44, DOI 10.1037/h0023519
   Patterson ML, 2019, J NONVERBAL BEHAV, V43, P111, DOI 10.1007/s10919-018-00292-w
   PLUTCHIK R, 1982, SOC SCI INFORM, V21, P529, DOI 10.1177/053901882021004003
   Ponce-Lopez V., 2016, LECT NOTES COMPUT SC
   Rammstedt B, 2007, J RES PERS, V41, P203, DOI 10.1016/j.jrp.2006.02.001
   Romeo M, 2021, ADV ROBOTICS, V35, P1167, DOI 10.1080/01691864.2021.1974941
   RUSSELL JA, 1980, J PERS SOC PSYCHOL, V39, P1161, DOI 10.1037/h0077714
   Salam H, 2017, IEEE ACCESS, V5, P705, DOI 10.1109/ACCESS.2016.2614525
   Schindler K, 2008, NEURAL NETWORKS, V21, P1238, DOI 10.1016/j.neunet.2008.05.003
   Shao ZL, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P357, DOI 10.1145/3474085.3475460
   She JH, 2021, PROC CVPR IEEE, P6244, DOI 10.1109/CVPR46437.2021.00618
   Subramaniam A, 2016, LECT NOTES COMPUT SC, V9915, P337, DOI 10.1007/978-3-319-49409-8_27
   Tellamekala M.K., 2021, IEEE INT CONF AUTOMA
   Vaswani A, 2017, ADV NEUR IN, V30
   Vinciarelli A, 2014, IEEE T AFFECT COMPUT, V5, P273, DOI 10.1109/TAFFC.2014.2330816
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   WATSON D, 1989, J PERS SOC PSYCHOL, V57, P120, DOI 10.1037/0022-3514.57.1.120
   Zhang C.L., 2016, LECT NOTES COMPUT SC
   Zhang L, 2022, IEEE T AFFECT COMPUT, V13, P298, DOI 10.1109/TAFFC.2019.2951656
   Zhang Zhijie, 2022, 2022 International Conference on Cyberworlds (CW), P187, DOI 10.1109/CW55638.2022.00046
NR 53
TC 1
Z9 1
U1 3
U2 9
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2023 APR 24
PY 2023
DI 10.1007/s00371-023-02862-6
EA APR 2023
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA E4KL2
UT WOS:000975246300001
DA 2024-07-18
ER

PT J
AU Guan, HY
   Spratling, M
AF Guan, Haoyan
   Spratling, Michael
TI Query semantic reconstruction for background in few-shot segmentation
SO VISUAL COMPUTER
LA English
DT Article
DE Few-shot learning; Semantic segmentation; Metric learning
AB Few-shot segmentation (FSS) aims to segment unseen classes using a few annotated samples. Typically, a prototype representing the foreground class is extracted from annotated support image(s) and is matched to features representing each pixel in the query image. However, models learnt in this way are insufficiently discriminatory, and often produce false positives: misclassifying background pixels as foreground. Some FSS methods try to address this issue by using the background in the support image(s) to help identify the background in the query image. However, the backgrounds of these images are often quite distinct, and hence, the support image background information is uninformative. This article proposes a method, QSR, that extracts the background from the query image itself, and as a result is better able to discriminate between foreground and background features in the query image. This is achieved by modifying the training process to associate prototypes with class labels including known classes from the training data and latent classes representing unknown background objects. This class information is then used to extract a background prototype from the query image. To successfully associate prototypes with class labels and extract a background prototype that is capable of predicting a mask for the background regions of the image, the machinery for extracting and using foreground prototypes is induced to become more discriminative between different classes. Experiments achieves state-of-the-art results for both 1-shot and 5-shot FSS on the PASCAL-5(i) and COCO-20(i) dataset. As QSR operates only during training, results are produced with no extra computational complexity during testing.
C1 [Guan, Haoyan; Spratling, Michael] Kings Coll London, Dept Informat, London WC2B 4BG, England.
C3 University of London; King's College London
RP Guan, HY (corresponding author), Kings Coll London, Dept Informat, London WC2B 4BG, England.
EM haoyan.guan@kcl.ac.uk; michael.spratling@kcl.ac.uk
RI Spratling, Michael/G-7689-2011
OI Spratling, Michael/0000-0001-9531-2813
FU King's-China Scholarship Council (KCSC)
FX The authors acknowledge the use of the research computing facility at
   King's College London, King's Computational Research, Engineering and
   Technology Environment (CREATE), and the Joint Academic Data science
   Endeavour (JADE) facility. This research was funded by the King's-China
   Scholarship Council (KCSC).
CR Boudiaf M, 2021, PROC CVPR IEEE, P13974, DOI 10.1109/CVPR46437.2021.01376
   Boyu Yang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12353), P763, DOI 10.1007/978-3-030-58598-3_45
   Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen XL, 2021, PROC CVPR IEEE, P15745, DOI 10.1109/CVPR46437.2021.01549
   Chen ZT, 2019, IEEE T IMAGE PROCESS, V28, P4594, DOI 10.1109/TIP.2019.2910052
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Finn C, 2017, PR MACH LEARN RES, V70
   Grant E., 2018, 6 INT C LEARN REPR
   Hariharan B, 2017, IEEE I CONF COMP VIS, P3037, DOI 10.1109/ICCV.2017.328
   Hariharan B, 2014, LECT NOTES COMPUT SC, V8695, P297, DOI 10.1007/978-3-319-10584-0_20
   He JJ, 2019, PROC CVPR IEEE, P7511, DOI 10.1109/CVPR.2019.00770
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Nguyen K, 2019, IEEE I CONF COMP VIS, P622, DOI 10.1109/ICCV.2019.00071
   Li G., 2021, P IEEE C COMP VIS PA
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu BH, 2021, PROC CVPR IEEE, P9742, DOI 10.1109/CVPR46437.2021.00962
   Liu JL, 2020, PROC CVPR IEEE, P2967, DOI 10.1109/CVPR42600.2020.00304
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Lu ZH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P8721, DOI 10.1109/ICCV48922.2021.00862
   Ravi S, 2016, OPTIMIZATION MODEL F
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Shaban A, 2017, ARXIV PREPRINT ARXIV, DOI 10.5244/C.31.167
   Siam M., 2020, IJCAI, V2020, P860
   Siam M, 2019, IEEE I CONF COMP VIS, P5248, DOI 10.1109/ICCV.2019.00535
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Snell J, 2017, ADV NEUR IN, V30
   Tian ZT, 2022, IEEE T PATTERN ANAL, V44, P1050, DOI 10.1109/TPAMI.2020.3013717
   Wang KX, 2019, IEEE I CONF COMP VIS, P9196, DOI 10.1109/ICCV.2019.00929
   Wang YX, 2018, PROC CVPR IEEE, P7278, DOI 10.1109/CVPR.2018.00760
   Wu ZH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P497, DOI 10.1109/ICCV48922.2021.00056
   Yang LH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P8701, DOI 10.1109/ICCV48922.2021.00860
   Zbontar J, 2021, PR MACH LEARN RES, V139
   Zhang BF, 2021, PROC CVPR IEEE, P8308, DOI 10.1109/CVPR46437.2021.00821
   Zhang C, 2019, IEEE I CONF COMP VIS, P9586, DOI 10.1109/ICCV.2019.00968
   Zhang C, 2019, PROC CVPR IEEE, P5212, DOI 10.1109/CVPR.2019.00536
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
NR 38
TC 0
Z9 0
U1 1
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2024
VL 40
IS 2
BP 799
EP 810
DI 10.1007/s00371-023-02817-x
EA MAR 2023
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GE6E8
UT WOS:000953000000001
OA hybrid, Green Submitted
DA 2024-07-18
ER

PT J
AU Tiwari, H
   Subramanian, VK
   Chen, YS
AF Tiwari, Hitika
   Subramanian, Venkatesh K.
   Chen, Yong-Sheng
TI Real-time self-supervised achromatic face colorization
SO VISUAL COMPUTER
LA English
DT Article
DE Self-supervised training; Face colorization; 3D face reconstruction;
   Real time; Dual pipeline; Cooperative learning
ID IMAGE
AB Recent deep learning-based 2D face image colorization techniques demonstrate significant improvement in colorization accuracy and detail preservation. However, the generation of a 3D counterpart is beyond the scope of these methods despite having extensive applications. Moreover, these approaches require a significant amount of inference time, thus posing a challenge for real-time applications. Besides, monocular 3D face reconstruction methods produce skin color consistent with the achromatic 2D face resulting in gray-scale 3D face texture. Therefore, we propose a novel real-time Self-Supervised COoperative COlorizaT ion of Achromatic Faces (COCOTA) framework, which estimates colored 3D faces from both monocular color and achromatic face images without posing additional dependencies. The proposed network contains (1) Chromatic Pipeline to obtain 3D face alignment and geometric details for color face images and (2) Achromatic Pipeline for recovering texture from achromatic images. The proposed dual pipeline feature loss and parameter sharing technique aid in cooperation between COCOTA pipelines for facilitating knowledge transfer between them. We compare color accuracy of our method with several 3D face reconstruction approaches on the challenging CelebA-test and FairFace datasets. COCOTA outperforms the current state-of-the-art method by a large margin (e.g., an improvement of 25.3%, 39.6%, and 17% is obtained on perceptual error, 3D color-based error, and 2D pixel-level error metrics, respectively). Also, we show the improvement in the proposed method's inference time compared to 2D image colorization techniques, demonstrating the effectiveness of the proposed method.
C1 [Tiwari, Hitika; Chen, Yong-Sheng] Natl Yang Ming Chiao Tung Univ, Comp Sci Dept, Hsinchu, Taiwan.
   [Tiwari, Hitika; Subramanian, Venkatesh K.] Indian Inst Technol Kanpur, Elect Engn Dept, Kanpur, India.
C3 National Yang Ming Chiao Tung University; Indian Institute of Technology
   System (IIT System); Indian Institute of Technology (IIT) - Kanpur
RP Tiwari, H (corresponding author), Natl Yang Ming Chiao Tung Univ, Comp Sci Dept, Hsinchu, Taiwan.; Tiwari, H (corresponding author), Indian Inst Technol Kanpur, Elect Engn Dept, Kanpur, India.
EM hitika@iitk.ac.in; venkats@iitk.ac.in; yschen@cs.nycu.edu.tw
CR Antic, 2019, DEOLDIFY DEEP LEARNI
   Blanz V, 1999, COMP GRAPH, P187, DOI 10.1145/311535.311556
   Cao C, 2014, IEEE T VIS COMPUT GR, V20, P413, DOI 10.1109/TVCG.2013.249
   Cao Q, 2018, IEEE INT CONF AUTOMA, P67, DOI 10.1109/FG.2018.00020
   Chen D, 2016, LECT NOTES COMPUT SC, V9909, P122, DOI 10.1007/978-3-319-46454-1_8
   Chen LL, 2021, PROC CVPR IEEE, P13054, DOI 10.1109/CVPR46437.2021.01286
   Deng JK, 2019, PROC CVPR IEEE, P4685, DOI 10.1109/CVPR.2019.00482
   Deng QX, 2022, IEEE T VIS COMPUT GR, V28, P3113, DOI 10.1109/TVCG.2021.3051251
   Deng Y, 2019, IEEE COMPUT SOC CONF, P285, DOI 10.1109/CVPRW.2019.00038
   Dou H, 2020, NEUROCOMPUTING, V415, P114, DOI 10.1016/j.neucom.2020.07.044
   Feng Y, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459936
   Feng Y, 2018, LECT NOTES COMPUT SC, V11218, P557, DOI 10.1007/978-3-030-01264-9_33
   Gecer B, 2019, PROC CVPR IEEE, P1155, DOI 10.1109/CVPR.2019.00125
   Genova K, 2018, PROC CVPR IEEE, P8377, DOI 10.1109/CVPR.2018.00874
   Goodfellow I. J., 2014, ARXIV
   Huang G. B., 2008, WORKSH FAC REAL LIF
   IIZUKA S, 2016, ACM T GRAPHIC, V35, P1, DOI DOI 10.1145/2897824.2925974
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jheng-Wei Su, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7965, DOI 10.1109/CVPR42600.2020.00799
   Jin X., 2021, P 29 ACM INT C MULTI
   Kärkkäinen K, 2021, IEEE WINT CONF APPL, P1547, DOI 10.1109/WACV48630.2021.00159
   King DE, 2009, J MACH LEARN RES, V10, P1755
   Kingma D. P., 2014, arXiv
   Köstinger M, 2011, 2011 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCV WORKSHOPS)
   Larsson G, 2016, LECT NOTES COMPUT SC, V9908, P577, DOI 10.1007/978-3-319-46493-0_35
   Limmer M, 2016, 2016 15TH IEEE INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND APPLICATIONS (ICMLA 2016), P61, DOI [10.1109/ICMLA.2016.0019, 10.1109/ICMLA.2016.114]
   Lin CD, 2023, VISUAL COMPUT, V39, P6005, DOI 10.1007/s00371-022-02708-7
   Lin JK, 2020, PROC CVPR IEEE, P5890, DOI 10.1109/CVPR42600.2020.00593
   Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425
   Ludwiczuk B., 2016, Tech. Rep. CMU-CS-16-118
   Paysan P, 2009, AVSS: 2009 6TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE, P296, DOI 10.1109/AVSS.2009.58
   Richardson E, 2016, INT CONF 3D VISION, P460, DOI 10.1109/3DV.2016.56
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Sela M, 2017, IEEE I CONF COMP VIS, P1585, DOI 10.1109/ICCV.2017.175
   Serengil, 2021, TENSORFLOW 101
   Shu XB, 2018, IEEE T PATTERN ANAL, V40, P905, DOI 10.1109/TPAMI.2017.2705122
   Shu XB, 2015, IEEE I CONF COMP VIS, P3970, DOI 10.1109/ICCV.2015.452
   Tewari A, 2018, PROC CVPR IEEE, P2549, DOI 10.1109/CVPR.2018.00270
   Tewari A, 2017, IEEE I CONF COMP VIS, P3735, DOI 10.1109/ICCV.2017.401
   Tiwari H., 2021, ARXIV
   Tiwari H, 2022, IEEE WINT CONF APPL, P297, DOI 10.1109/WACV51458.2022.00037
   Treneska S, 2022, SENSORS-BASEL, V22, DOI 10.3390/s22041599
   Vitoria P, 2020, IEEE WINT CONF APPL, P2434, DOI [10.1109/WACV45572.2020.9093389, 10.1109/wacv45572.2020.9093389]
   Xiao YX, 2022, INT J INTELL SYST, V37, P1222, DOI 10.1002/int.22667
   Xiao YX, 2019, IEEE IMAGE PROC, P3247, DOI [10.1109/icip.2019.8803677, 10.1109/ICIP.2019.8803677]
   Xu JT, 2021, SIGNAL PROCESS, V183, DOI 10.1016/j.sigpro.2021.108007
   Ye D, 2020, INT J IMAGE GRAPH, V20, DOI 10.1142/S0219467820500035
   Zhang R., 2017, ARXIV
   Zhang R, 2016, LECT NOTES COMPUT SC, V9907, P649, DOI 10.1007/978-3-319-46487-9_40
   Zhong X, 2022, IEEE T CIRC SYST VID, V32, P1418, DOI 10.1109/TCSVT.2021.3072171
   Zhong YY, 2021, IEEE T IMAGE PROCESS, V30, P2587, DOI 10.1109/TIP.2020.3048632
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zhu XY, 2016, PROC CVPR IEEE, P146, DOI 10.1109/CVPR.2016.23
   Zielonka W., 2022, ARXIV
NR 56
TC 1
Z9 1
U1 1
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2023
VL 39
IS 12
BP 6521
EP 6536
DI 10.1007/s00371-022-02746-1
EA DEC 2022
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA X7HK1
UT WOS:000903607000001
DA 2024-07-18
ER

PT J
AU Kong, WH
   Xu, QS
   Su, WJ
   Xu, SY
   Tao, WB
AF Kong, Weihang
   Xu, Qingshan
   Su, Wanjuan
   Xu, Siyuan
   Tao, Wenbing
TI LGP-MVS: combined local and global planar priors guidance for indoor
   multi-view stereo
SO VISUAL COMPUTER
LA English
DT Article
DE MVS; Multi-view stereo; Depth estimation; Point cloud reconstruction
AB Multi-view stereo (MVS) has long been a subject for researchers in the computer vision field. Due to the unreliable photometric consistency in low-textured areas, the existing PatchMatch MVS methods have low accuracy and completeness when recovering the depth information of low-textured areas in indoor environments. To solve the above problem that PatchMatch methods always fail in the textureless regions, we propose a global-local planar priors jointly optimized PatchMatch MVS method. The algorithm constructs global-local planar priors and uses a dynamic texture-related multi-view aggregation cost to balance photometric consistency and planar priors. The validity of the algorithm is verified by quantitative and qualitative analysis of depth maps and 3D reconstruction on multiple real-world scenes from ScanNet Dataset and ETH3D benchmark, also synthetic indoor scenes from ICL-NUIM Dataset. Our method can effectively recover the depth information in the textureless regions, so as to obtain the 3D model with high precision.
C1 [Kong, Weihang; Xu, Qingshan; Su, Wanjuan; Xu, Siyuan; Tao, Wenbing] Huazhong Univ Sci & Technol, Sch Artificial Intelligence & Automation, Natl Key Lab Sci & Technol Multispectral Informat, Wuhan 430074, Hubei, Peoples R China.
C3 Huazhong University of Science & Technology
RP Tao, WB (corresponding author), Huazhong Univ Sci & Technol, Sch Artificial Intelligence & Automation, Natl Key Lab Sci & Technol Multispectral Informat, Wuhan 430074, Hubei, Peoples R China.
EM octavia@hust.edu.cn; qingshanxu@hust.edu.cn; suwanjuan@hust.edu.cn;
   xusy@hust.edu.cn; wenbingtao@hust.edu.cn
RI Xu, Qing/JWP-4901-2024
OI Tao, Wenbing/0000-0003-3284-864X
FU National Natural Science Foundation of China [62176096, 61991412]
FX This work was supported by the National Natural Science Foundation of
   China under Grants 62176096 and 61991412.
CR Barnes C, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531330
   Campbell NDF, 2008, LECT NOTES COMPUT SC, V5302, P766, DOI 10.1007/978-3-540-88682-2_58
   Dai A, 2017, PROC CVPR IEEE, P2432, DOI 10.1109/CVPR.2017.261
   Furukawa Y, 2010, IEEE T PATTERN ANAL, V32, P1362, DOI 10.1109/TPAMI.2009.161
   Furukawa Y, 2013, FOUND TRENDS COMPUT, V9, P1, DOI 10.1561/0600000052
   Galliani S, 2015, IEEE I CONF COMP VIS, P873, DOI 10.1109/ICCV.2015.106
   Gallup D, 2010, PROC CVPR IEEE, P1418, DOI 10.1109/CVPR.2010.5539804
   Gan JB, 2018, VISUAL COMPUT, V34, P1551, DOI 10.1007/s00371-017-1430-5
   Geiger A, 2011, LECT NOTES COMPUT SC, V6492, P25, DOI 10.1007/978-3-642-19315-6_3
   Handa A, 2014, IEEE INT CONF ROBOT, P1524, DOI 10.1109/ICRA.2014.6907054
   Vu HH, 2012, IEEE T PATTERN ANAL, V34, P889, DOI 10.1109/TPAMI.2011.172
   Knapitsch A, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073599
   Li ZX, 2020, IEEE T IMAGE PROCESS, V29, P7176, DOI 10.1109/TIP.2020.2999853
   Liao J, 2019, COMPUT GRAPH FORUM, V38, P335, DOI 10.1111/cgf.13841
   Locher A, 2016, PROC CVPR IEEE, P3244, DOI 10.1109/CVPR.2016.353
   Romanoni A, 2019, IEEE I CONF COMP VIS, P10412, DOI 10.1109/ICCV.2019.01051
   Schönberger JL, 2016, LECT NOTES COMPUT SC, V9907, P501, DOI 10.1007/978-3-319-46487-9_31
   Schops T., ETH3D BENCHMARK
   Schöps T, 2017, PROC CVPR IEEE, P2538, DOI 10.1109/CVPR.2017.272
   Seitz S.M., 2006, 2006 IEEE COMP SOC C, V1, P519, DOI https://doi.org/10.1109/CVPR.2006.19
   Strecha C, 2008, PROC CVPR IEEE, P2838
   Su WJ, 2022, IEEE T CIRC SYST VID, V32, P7796, DOI 10.1109/TCSVT.2022.3183836
   Sun S., 2022, 13 INT C GRAPHICS IM, V2083, P165
   Tan, 2021, PROC IEEECVF INT C C, P4186
   Ulusoy AO, 2015, 2015 INTERNATIONAL CONFERENCE ON 3D VISION, P10, DOI 10.1109/3DV.2015.9
   Vogiatzis G, 2007, IEEE T PATTERN ANAL, V29, P2241, DOI 10.1109/TPAMI.2007.70712
   Wang F., 2022, P IEEE CVF C COMP VI, P8606
   Wang FJ, 2021, PROC CVPR IEEE, P14189, DOI 10.1109/CVPR46437.2021.01397
   Wang W, 2019, VISUAL COMPUT, V35, P625, DOI 10.1007/s00371-018-1492-z
   Wang YS, 2020, PROC CVPR IEEE, P2036, DOI 10.1109/CVPR42600.2020.00211
   Wei MQ, 2019, VISUAL COMPUT, V35, P797, DOI 10.1007/s00371-019-01688-5
   Woodford O, 2009, IEEE T PATTERN ANAL, V31, P2115, DOI 10.1109/TPAMI.2009.131
   Xu QS, 2020, AAAI CONF ARTIF INTE, V34, P12516
   Xu Qingshan, 2019, COMPUTER VISION PATT, P2
   Xu ZY, 2020, PROC CVPR IEEE, P5980, DOI 10.1109/CVPR42600.2020.00602
   Zheng EL, 2014, PROC CVPR IEEE, P1510, DOI 10.1109/CVPR.2014.196
NR 36
TC 0
Z9 0
U1 3
U2 11
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2023
VL 39
IS 12
BP 6421
EP 6433
DI 10.1007/s00371-022-02737-2
EA DEC 2022
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA X7HK1
UT WOS:000903155800001
DA 2024-07-18
ER

PT J
AU Liu, QP
   Bi, JJ
   Zhang, JW
   Bu, XH
   Hanajima, N
AF Liu, Qunpo
   Bi, Junjia
   Zhang, Jingwen
   Bu, Xuhui
   Hanajima, Naohiko
TI B-FPN SSD: an SSD algorithm based on a bidirectional feature fusion
   pyramid
SO VISUAL COMPUTER
LA English
DT Article
DE Target detection; Deep learning; Bidirectional feature fusion;
   Coordinate attention; Huber loss optimization
AB This paper proposes a bidirectional feature fusion pyramid (B-FPN) Single Shot Multiple Frame Detector (SSD) algorithm. First, a bidirectional feature pyramid (B-FPN) structure is constructed, which realizes the bidirectional fusion of the feature layers and improves the accuracy of detection. Second, we introduce coordinate attention (CA) to focus on the important channel features while preserving their location information, thereby increasing the focus on the important information. Finally, optimizing the loss function speeds up the convergence of the model and further improves the detection accuracy of the network. The experimental results show that on the VOC2007 dataset, the mAP of the algorithm in this paper is 76.48%, which is 3.52% higher than that of the SSD algorithm. On the COCO 2017 dataset, the mAP of the proposed algorithm is 3.85% higher than that of the SSD algorithm. Compared with other mainstream target detection algorithms, the algorithm in this paper has certain advantages in detection accuracy, and can also achieve satisfactory results in detection speed. Finally, the accuracy of foreign object recognition in the special environment of iron ore transportation is 98.26%.
C1 [Liu, Qunpo; Bi, Junjia; Zhang, Jingwen; Bu, Xuhui] Henan Polytech Univ, Sch Elect Engn & Automat, Jiaozuo, Peoples R China.
   [Liu, Qunpo; Bu, Xuhui; Hanajima, Naohiko] Henan Int Joint Lab Direct Drive & Control Intelli, Jiaozuo, Henan, Peoples R China.
   [Hanajima, Naohiko] Muroran Inst Technol, Coll Informat & Syst, 27-1 Mizumoto Cho, Muroran, Hokkaido 0508585, Japan.
C3 Henan Polytechnic University; Muroran Institute of Technology
RP Bi, JJ (corresponding author), Henan Polytech Univ, Sch Elect Engn & Automat, Jiaozuo, Peoples R China.
EM lqpny@hpu.edu.cn; 1521383068@qq.com
OI LIU, Qunpo/0000-0002-2157-0278; Hanajima, Naohiko/0000-0002-4707-853X
FU National Natural Science Foundation of China; Innovative Scientists and
   Technicians Team of Henan Provincial High Education; Science and
   Technology Project of Henan Province;  [U1804147];  [20IRTSTHN019]; 
   [212102210508]
FX AcknowledgementsThis work is partially supported by the National Natural
   Science Foundation of China (No. U1804147), Innovative Scientists and
   Technicians Team of Henan Provincial High Education (20IRTSTHN019),
   Science and Technology Project of Henan Province(No. 212102210508) .
CR Bochkovskiy A, 2020, ARXIV PREPRINT ARXIV, DOI DOI 10.48550/ARXIV.2004.10934
   Chen, 2021, COLLOQ MATH-WARSAW, V37, P18
   Du J., 2021, IND MINE AUTOM, V47, P77
   Ge, 2021, IEEE C COMPUTER VISI
   Hao S, 2021, J CHINA COAL SOC, P1
   Hou QB, 2021, PROC CVPR IEEE, P13708, DOI 10.1109/CVPR46437.2021.01350
   Hu J H, 2021, J MINE AUTOMATION, V47, P57
   Huo AQ., 2022, COMPUTER ENG DESIGN, V43, P1981
   JOCHER GLENN, 2021, YOLOV5 EB OL
   Joseph RK, 2016, CRIT POL ECON S ASIA, P1
   [李晖晖 Li Huihui], 2020, [仪器仪表学报, Chinese Journal of Scientific Instrument], V41, P183
   Li JN, 2018, IEEE T MULTIMEDIA, V20, P985, DOI 10.1109/TMM.2017.2759508
   Lin T.Y., 2017, P IEEE C COMPUTER VI, P2117, DOI [10.1109/CVPR.2017.106, DOI 10.1109/CVPR.2017.106]
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Lv, 2020, RES IMAGE RECOGNITIO
   Rasti B, 2020, INFORM FUSION, V64, P121, DOI 10.1016/j.inffus.2020.07.002
   Ren GQ., 2021, IND MINE AUTOMATION, V47, P77
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Tang W, 2022, IEEE T IMAGE PROCESS, V31, P5134, DOI 10.1109/TIP.2022.3193288
   Tang Wei, 2022, IEEE Transactions on Multimedia
   Wang CY, 2021, PROC CVPR IEEE, P13024, DOI 10.1109/CVPR46437.2021.01283
   Wang P., 2022, The Visual Computer, P1
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Wu H, 2019, SEQUENCE LEVEL SEMAN
   Wu S P, 2019, SAF COAL MINES, V50, P127, DOI DOI 10.13347/J.CNKI.MKAQ.2019.12.029
   Xia CX, 2022, VISUAL COMPUT, V38, P3059, DOI 10.1007/s00371-022-02561-8
   Xie F., 2022, Comput Syst Appl, V31, P1
   Xiong CH., 2019, COMMAND INFORM SYSTE, V10, P8
   Yang SD, 2022, J REAL-TIME IMAGE PR, V19, P287, DOI 10.1007/s11554-021-01183-y
   Yuan ZH., 2020, JCHONGQING U TECHNOL, V34, P56
   Zhang, 2020, RES FOREIGN OBJECT I
   Zhang, 2021, RES TRAFFIC TARGET D
   Zhang SF, 2018, LECT NOTES COMPUT SC, V11207, P657, DOI 10.1007/978-3-030-01219-9_39
   [张新钰 Zhang Xinyu], 2018, [清华大学学报. 自然科学版, Journal of Tsinghua University. Science and Technology], V58, P438
   Zhu, 2021, RES IDENTIFICATION F
   [朱梦瑞 Zhu Mengrui], 2022, [北京交通大学学报. 自然科学版, Journal of Beijing Jiaotong University], V46, P37
NR 36
TC 4
Z9 4
U1 4
U2 31
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2023
VL 39
IS 12
BP 6265
EP 6277
DI 10.1007/s00371-022-02727-4
EA DEC 2022
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA X7HK1
UT WOS:000893508800001
DA 2024-07-18
ER

PT J
AU Therrien, O
   Levesque, Y
   Gilet, G
AF Therrien, Olivier
   Levesque, Yannick
   Gilet, Guillaume
TI Screen space indirect lighting with visibility bitmask
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Real-time rendering; Indirect lighting; Ambient occlusion; Visibility
AB Horizon-based indirect illumination efficiently estimates a diffuse light bounce in screen space by analytically integrating the horizon angle difference between samples along a given direction. Like other horizon-based methods, this technique cannot properly simulate light passing behind thin surfaces. We propose the concept of a visibility bitmask that replaces the two horizon angles by a bit field representing the binary state (occluded/un-occluded) of N sectors uniformly distributed around the hemisphere slice. It allows light to pass behind surfaces of constant thickness while keeping the efficiency of horizon-based methods. It can also do more accurate ambient lighting than bent normal by sampling more than one visibility cone. This technique improves the visual quality of ambient occlusion, indirect diffuse, and ambient light compared to previous screen space methods while minimizing noise and keeping a low performance overhead.
C1 [Therrien, Olivier] CDRIN, Matane, PQ, Canada.
   [Levesque, Yannick] Cegep Matane, Matane, PQ, Canada.
   [Gilet, Guillaume] Univ Sherbrooke, Sherbrooke, PQ, Canada.
C3 University of Sherbrooke
RP Gilet, G (corresponding author), Univ Sherbrooke, Sherbrooke, PQ, Canada.
EM Guillaume.Gilet@Usherbrooke.ca
OI Gilet, Guillaume/0000-0002-9973-1772
FU province of Quebec (Canada)
FX Special thanks to Deepti Joshi (CDRIN) and Peter Shirley (NVIDIA) for
   their guidance in the redaction of this paper. We also want to thank our
   other colleagues namely Antoine Fortin (CDRIN), Olivier Leclerc (CDRIN),
   Steven Pigeon (UQAR), and Vahe Vardanyan (CDRIN) who have actively
   supported the current body of work. Most of the models are from the
   Amazon Lumberyard Bistro scene. This research is financed in part by the
   province of Quebec (Canada) via the grant "Programme d'aide a la
   recherche et au transfert (PART)." Data sharing not applicable to this
   article as no datasets were generated or analyzed during the current
   study.
CR Andersson P, 2020, P ACM COMPUT GRAPH, V3, DOI 10.1145/3406183
   [Anonymous], 2009, P S INT 3D GRAPH GAM, DOI 10.1145/1507149.1507161.5,7
   Bavoil L., 2011, NVIDIA DIRECTX, V11
   Dachsbacher C., 2005, Proc. Symp. Interactive Graph. and Games, P203, DOI DOI 10.1145/1053427.1053460
   Dimitrov R., 2008, ACM SIGGRAPH 2008 TA, P1, DOI DOI 10.1145/1401032.1401061
   Hofmann N, 2017, HPG '17: PROCEEDINGS OF HIGH PERFORMANCE GRAPHICS, DOI 10.1145/3105762.3105781
   Jimenez J., 2016, P SIGGRAPH 2016 COUR
   Klehm O., 2011, VMV, P177
   Lumberyard A., 2017, AMAZON LUMBERYARD BI
   Mara Michael, 2016, P HIGH PERF GRAPH, P87
   Mayaux B., 2018, HORIZON BASED INDIRE
   McGuire M., 2011, P ACM SIGGRAPH S HIG, P25, DOI DOI 10.1145/2018323.2018327
   McGuire M., 2012, HIGH PERFORMANCE GRA
   Mittring M., 2007, ACM SIGGRAPH 2007 CO, P97, DOI [DOI 10.1145/1281500.1281671, 10.1145/1281500.1281671]
   Nalbach O., 2014, Proceedings of the ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games, P79
   Schwarz M, 2007, COMPUT GRAPH FORUM, V26, P515, DOI 10.1111/j.1467-8659.2007.01074.x
   Shergin D., 2017, ACM SIGGRAPH 2017 RE, P25
   Silvennoinen A., 2015, ACM SIGGRAPH
   Sousa T., 2011, SECRETS CRYENGINE 3, V1
   Timonen V, 2013, COMPUT GRAPH FORUM, V32, P97, DOI 10.1111/cgf.12155
   Vermeer J, 2021, P ACM COMPUT GRAPH, V4, DOI 10.1145/3451268
   Zhukov S., 1998, Rendering Techniques '98. Proceedings of the Eurographics Workshop, P45
NR 22
TC 1
Z9 1
U1 1
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2022 NOV 11
PY 2022
DI 10.1007/s00371-022-02703-y
EA NOV 2022
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 6C3IV
UT WOS:000881913400001
DA 2024-07-18
ER

PT J
AU Qian, LD
   Huang, HL
   Xia, XY
   Li, Y
   Zhou, X
AF Qian, Ledan
   Huang, Huiling
   Xia, Xiaonyu
   Li, Yi
   Zhou, Xiao
TI Automatic segmentation method using FCN with multi-scale dilated
   convolution for medical ultrasound image
SO VISUAL COMPUTER
LA English
DT Article
DE Fully convolutional network; Medical ultrasound image; Image
   segmentation; Dilated convolution; Laplace correction operator
ID NETWORK
AB Image segmentation plays a critical role in the quantitative and qualitative analysis of medical ultrasound images, directly affecting the follow-up analysis and processing. However, due to the speckle noise, fuzziness, complexity and diversity of medical ultrasound images, the traditional image segmentation algorithms are accessible to leak the boundary at the weak edge of the medical ultrasound image, getting inaccurate results and difficulty extracting the target contour of the ultrasound image. In addition, the non-automatic feature extraction method cannot realize the end-to-end automatic segmentation function. Nevertheless, fully convolutional networks (FCNs) can realize end-to-end automatic semantic segmentation, and are widely used for ultrasound image segmentation. In this paper, we aim at the problems of low segmentation accuracy and long segmentation time in the traditional segmentation method, proposing a novel segmentation method based on an improved FCN with multi-scale dilated convolution for ultrasound image segmentation. The proposed method firstly preprocesses medical ultrasound images through image filtering, normalization and enhancement, and then improves the fully convolutional neural network by constructing four-dilated convolutions with different dilation rates, which can capture multi-scale context feature information and finally postprocesses the segmentation results of the medical ultrasound image by the Laplace correction operator. Our experiments demonstrate that the proposed method achieves better segmentation results than state-of-the-art methods on the breast ultrasound dataset.
C1 [Qian, Ledan; Huang, Huiling; Xia, Xiaonyu] Wenzhou Univ, Coll Math & Phys, Meiquan Big St Chashan, Wenzhou 325035, Zhejiang, Peoples R China.
   [Li, Yi] Wenzhou Univ, Coll Comp Sci & Artificial Intelligence, Meiquan Big St Chashan, Wenzhou 325035, Zhejiang, Peoples R China.
   [Zhou, Xiao] Wenzhou Univ, Informat Technol Ctr, Meiquan Big St Chashan, Wenzhou 325035, Zhejiang, Peoples R China.
C3 Wenzhou University; Wenzhou University; Wenzhou University
RP Zhou, X (corresponding author), Wenzhou Univ, Informat Technol Ctr, Meiquan Big St Chashan, Wenzhou 325035, Zhejiang, Peoples R China.
EM 00802005@wzu.edu.cn; huilin_huang@wzu.edu.cn; xiaonyu.xia@wzu.edu.cn;
   liyi@wzu.edu.cn; 00802002@wzu.edu.cn
RI huang, huiling/JCN-5789-2023
OI Qian, Ledan/0000-0003-2338-6519
FU Project of Science and Technology Plans of Wenzhou [2022G0069]; Wenzhou
   Association For Science and Technology [kjfw39]; Department of Education
   of Zhejiang Province [Y202146494]; Soft Science Key Research Project of
   Zhejiang Province [2022C25033]; National Natural Science Foundation of
   China [12101465]
FX This work was supported in part by the Project of Science and Technology
   Plans of Wenzhou under [Grant No. 2022G0069], in part by Wenzhou
   Association For Science and Technology under [Grant No. kjfw39], in part
   by Department of Education of Zhejiang Province under [Grant No.
   Y202146494], in part by The Soft Science Key Research Project of
   Zhejiang Province under [Grant No. 2022C25033] and in part by The
   National Natural Science Foundation of China under [Grant No. 12101465].
CR ADAMS R, 1994, IEEE T PATTERN ANAL, V16, P641, DOI 10.1109/34.295913
   Agrawal T, 2023, VISUAL COMPUT, V39, P875, DOI 10.1007/s00371-021-02352-7
   Bachmann AH, 2007, OPT EXPRESS, V15, P408, DOI 10.1364/OE.15.000408
   Badrinarayanan V., 2015, SEGNET DEEP CONVOLUT
   Berman M, 2018, PROC CVPR IEEE, P4413, DOI 10.1109/CVPR.2018.00464
   Cao ZH, 2020, VISUAL COMPUT, V36, P1619, DOI 10.1007/s00371-019-01763-x
   Chandale, 2013, INT J COMPUT TECHNOL, V11, P2169, DOI [10.24297/ijct.v11i1.1187, DOI 10.24297/IJCT.V11I1.1187]
   Chen L, 2020, SIGNAL IMAGE VIDEO P, V14, P1043, DOI 10.1007/s11760-020-01637-z
   Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49
   Dormer JD, 2018, PROC SPIE, V10580, DOI 10.1117/12.2293558
   Ghose S, 2012, IEEE ENG MED BIO, P2335, DOI 10.1109/EMBC.2012.6346431
   Han GJ, 2021, ENERGY REP, V7, P210, DOI 10.1016/j.egyr.2021.10.037
   Hao XH, 2000, ULTRASON, P1717, DOI 10.1109/ULTSYM.2000.921653
   Huang QH, 2017, INT J COMPUT ASS RAD, V12, P493, DOI 10.1007/s11548-016-1513-1
   Huang QW, 2021, TSINGHUA SCI TECHNOL, V26, P833, DOI 10.26599/TST.2020.9010042
   Jabbar SI, 2016, IEEE IJCNN, P4619, DOI 10.1109/IJCNN.2016.7727805
   Karani N, 2021, MED IMAGE ANAL, V68, DOI 10.1016/j.media.2020.101907
   Karimi D, 2020, IEEE T MED IMAGING, V39, P499, DOI 10.1109/TMI.2019.2930068
   Kass M., 1987, Proceedings of the First International Conference on Computer Vision (Cat. No.87CH2465-3), P259, DOI 10.1007/BF00133570
   Khan A, 2021, IEEE ACCESS, V9, P55988, DOI 10.1109/ACCESS.2021.3071754
   Kollmann C., 2015, ULTRASOUND MED BIOL, V41, P622, DOI [10.1016/j.ultrasmedbio.2014.09.026, DOI 10.1016/J.ULTRASMEDBIO.2014.09.026]
   Koundal D, 2018, BIOMED SIGNAL PROCES, V40, P117, DOI 10.1016/j.bspc.2017.08.025
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Ma H, 2021, COMPUT METH PROG BIO, V208, DOI 10.1016/j.cmpb.2021.106230
   Meiburger KM, 2018, COMPUT BIOL MED, V92, P210, DOI 10.1016/j.compbiomed.2017.11.018
   Mishra D, 2019, IEEE T BIO-MED ENG, V66, P1637, DOI 10.1109/TBME.2018.2877577
   Moraru L, 2011, AIP CONF PROC, V1387, DOI 10.1063/1.3647090
   Murugappan V, 2019, CLUSTER COMPUT, V22, P10979, DOI 10.1007/s10586-017-1269-6
   Noble JA, 2010, P I MECH ENG H, V224, P307, DOI 10.1243/09544119JEIM604
   Noble JA, 2006, IEEE T MED IMAGING, V25, P987, DOI 10.1109/TMI.2006.877092
   Paszke A., 2016, ARXIV160602147
   Pengli Wei, 2020, Journal of Physics: Conference Series, V1693, DOI 10.1088/1742-6596/1693/1/012155
   Qian L., 2022, ARXIV
   Qian LD, 2020, IEEE ACCESS, V8, P62830, DOI 10.1109/ACCESS.2020.2983774
   Ragesh NK., 2011, Icgst Aiml-11 Conference, Dubai, UAE, V12, P14
   RAMESH N, 1995, IEE P-VIS IMAGE SIGN, V142, P271, DOI 10.1049/ip-vis:19952007
   Ravishankar H, 2016, I S BIOMED IMAGING, P779, DOI 10.1109/ISBI.2016.7493382
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Sahu S., 2012, INT J ADV COMPUT RES, V2, P125
   Shackleton M, 2006, NATURE, V439, P84, DOI 10.1038/nature04372
   Sharifzadeh M, 2022, IEEE T ULTRASON FERR, V69, P1703, DOI 10.1109/TUFFC.2022.3162800
   Shirokikh B, 2021, J IMAGING, V7, DOI 10.3390/jimaging7020035
   Shrivastava N, 2020, INT J IMAGE GRAPH, V20, DOI 10.1142/S0219467820500187
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sun JL, 2020, INT J CONTROL, V93, P1291, DOI 10.1080/00207179.2018.1503724
   Taheri M., 2021, J ELECT COMPUTER ENG, V9, P37
   Talebi M., 2011, J BIOMED SCI ENG, V4, P105, DOI [10.4236/jbise.2011.42015, DOI 10.4236/JBISE.2011.42015]
   THOMAS JG, 1991, IEEE T MED IMAGING, V10, P180, DOI 10.1109/42.79476
   TORRE V, 1986, IEEE T PATTERN ANAL, V8, P147, DOI 10.1109/TPAMI.1986.4767769
   Tran ST, 2021, HEALTHCARE-BASEL, V9, DOI 10.3390/healthcare9010054
   VANVLIET LJ, 1989, COMPUT VISION GRAPH, V45, P167, DOI 10.1016/0734-189X(89)90131-X
   Wang FJ, 2019, FRACTALS, V27, DOI 10.1142/S0218348X19500609
   Wang GT, 2019, IEEE T PATTERN ANAL, V41, P1559, DOI 10.1109/TPAMI.2018.2840695
   Wehrens R, 2004, J CLASSIF, V21, P231, DOI 10.1007/s00357-004-0018-8
   Wijata A, 2021, ULTRASONIC IMAGING, V43, P262, DOI 10.1177/01617346211025267
   Wu LY, 2017, I S BIOMED IMAGING, P663, DOI 10.1109/ISBI.2017.7950607
   Xiao HG, 2023, VISUAL COMPUT, V39, P2291, DOI 10.1007/s00371-022-02414-4
   Xu M, 2021, I S BIOMED IMAGING, P827, DOI 10.1109/ISBI48211.2021.9433899
   Xue C, 2021, MED IMAGE ANAL, V70, DOI 10.1016/j.media.2021.101989
   Yang J, 2019, ULTRASONICS, V96, P24, DOI 10.1016/j.ultras.2019.03.014
   Yujie Yang, 2020, Journal of Physics: Conference Series, V1693, DOI 10.1088/1742-6596/1693/1/012183
   Zhang XS, 2020, ULTRASONIC IMAGING, V42, P191, DOI 10.1177/0161734620928453
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zhou ZW, 2020, IEEE T MED IMAGING, V39, P1856, DOI 10.1109/TMI.2019.2959609
   Zhu J, 2019, ARXIV
   Zuo Q, 2021, SECUR COMMUN NETW, V2021, DOI 10.1155/2021/6625688
   Zyuzin V, 2019, 2019 URAL SYMPOSIUM ON BIOMEDICAL ENGINEERING, RADIOELECTRONICS AND INFORMATION TECHNOLOGY (USBEREIT), P110, DOI [10.1109/usbereit.2019.8736616, 10.1109/USBEREIT.2019.8736616]
NR 67
TC 6
Z9 6
U1 19
U2 62
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2023
VL 39
IS 11
BP 5953
EP 5969
DI 10.1007/s00371-022-02705-w
EA OCT 2022
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA W5SX6
UT WOS:000876238700001
DA 2024-07-18
ER

PT J
AU Su, H
   Han, MY
   Liang, JL
   Liang, J
   Yu, SS
AF Su, Hai
   Han, Meiyin
   Liang, Junle
   Liang, Jun
   Yu, Songsen
TI Deep supervised hashing with hard example pairs optimization for image
   retrieval
SO VISUAL COMPUTER
LA English
DT Article
DE Image retrieval; Deep hashing; Hard example pairs
AB Compared with the traditional hashing methods, deep hashing methods generate hash codes with rich semantic information and greatly improve the performances in the image retrieval field. However, it is unsatisfied for current deep hashing methods to predict the similarity of hard example pairs. There exist two main factors affecting the ability of learning these pairs, which are weak key features extraction and the shortage of hard example pairs. In this paper, we give a novel end-to-end model to extract the key feature and obtain hash code with the accurate semantic information. In addition, we redesign an indicator to assess the hard degree of pairs and update penalty weights of them in the proposed hard pair-wise loss. It effectively alleviates the shortage problem. Experimental results on CIFAR-10 and NUS-WIDE demonstrate that our model outperformances the mainstream hashing-based image retrieval methods.
C1 [Su, Hai; Han, Meiyin; Liang, Jun; Yu, Songsen] South China Normal Univ, Sch Software, Taoyuan East Rd, Foshan 528225, Guangdong, Peoples R China.
   [Liang, Junle] Syracuse Univ, Coll Engn & Comp Sci, South Crouse Ave, Syracuse, NY 13255 USA.
C3 South China Normal University; Syracuse University
RP Liang, J (corresponding author), South China Normal Univ, Sch Software, Taoyuan East Rd, Foshan 528225, Guangdong, Peoples R China.
EM suhai@m.scnu.edu.cn; hanmeiyin@m.scnu.edu.cn; juliang@syr.edu;
   liangjun@m.scnu.edu.cn; yss8109@163.com
RI Liang, Jun/GQO-8542-2022
OI Liang, Jun/0000-0003-0034-2601
FU Guangdong Basic and Applied Basic Research Foundation [2021A1515110673]
FX The research is funded by Guangdong Basic and Applied Basic Research
   Foundation (2021A1515110673).
CR [Anonymous], 2009, NIPS
   Arulmozhi P, 2021, VISUAL COMPUT, V37, P2391, DOI 10.1007/s00371-020-01993-4
   Cai BW, 2017, REMOTE SENS-BASEL, V9, DOI 10.3390/rs9111198
   Cao Y, 2018, PROC CVPR IEEE, P1229, DOI 10.1109/CVPR.2018.00134
   Cao ZJ, 2017, IEEE I CONF COMP VIS, P5609, DOI 10.1109/ICCV.2017.598
   Chen CF, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P347, DOI 10.1109/ICCV48922.2021.00041
   Chen GC, 2022, VISUAL COMPUT, V38, P1051, DOI 10.1007/s00371-021-02067-9
   Chen YX, 2020, NEUROCOMPUTING, V385, P111, DOI 10.1016/j.neucom.2019.12.078
   Chu J, 2018, IEEE ACCESS, V6, P19959, DOI 10.1109/ACCESS.2018.2815149
   Ge J., 2019, ARXIV
   He KM, 2013, PROC CVPR IEEE, P2938, DOI 10.1109/CVPR.2013.378
   Hermans Alexander, 2017, ARXIV170307737
   Ji J., 2012, 25 INT C NEUR INF PR, P108
   Jin ZM, 2014, IEEE T CYBERNETICS, V44, P1362, DOI 10.1109/TCYB.2013.2283497
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kulis B, 2012, IEEE T PATTERN ANAL, V34, P1092, DOI 10.1109/TPAMI.2011.219
   Lai HJ, 2015, PROC CVPR IEEE, P3270, DOI 10.1109/CVPR.2015.7298947
   Li BY, 2019, AAAI CONF ARTIF INTE, P8577
   Li G., arXiv
   Li MN, 2017, COMM COM INF SC, V773, P166, DOI 10.1007/978-981-10-7305-2_15
   Li Q, 2017, ADV NEUR IN, V30
   Li WJ, 2016, IJCAI, P1711
   Li Y., 2019, 30th British Machine Vision Conference 2019
   Lin CT, 2020, IEEE T VEH TECHNOL, V69, P1505, DOI 10.1109/TVT.2019.2961625
   Lin K, 2015, IEEE COMPUT SOC CONF
   Liong VE, 2015, PROC CVPR IEEE, P2475, DOI 10.1109/CVPR.2015.7298862
   Liu HM, 2016, PROC CVPR IEEE, P2064, DOI 10.1109/CVPR.2016.227
   Liu W, 2012, PROC CVPR IEEE, P2074, DOI 10.1109/CVPR.2012.6247912
   Lu XQ, 2020, IEEE T NEUR NET LEAR, V31, P2052, DOI 10.1109/TNNLS.2019.2927868
   Lu XQ, 2018, IEEE T IMAGE PROCESS, V27, P106, DOI 10.1109/TIP.2017.2755766
   Misra D, 2021, IEEE WINT CONF APPL, P3138, DOI 10.1109/WACV48630.2021.00318
   Norouzi M.E., 2011, ICML
   Qi Qian, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12161, DOI 10.1109/CVPR42600.2020.01218
   Selvaraju RR, 2020, INT J COMPUT VISION, V128, P336, DOI [10.1007/s11263-019-01228-7, 10.1109/ICCV.2017.74]
   Shen FM, 2018, IEEE T PATTERN ANAL, V40, P3034, DOI 10.1109/TPAMI.2018.2789887
   Shen FM, 2015, PROC CVPR IEEE, P37, DOI 10.1109/CVPR.2015.7298598
   Shi XS, 2021, IEEE T IMAGE PROCESS, V30, P1130, DOI 10.1109/TIP.2020.3040536
   Shrivastava A, 2016, PROC CVPR IEEE, P761, DOI 10.1109/CVPR.2016.89
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Su S., 2018, THE CNN, P806, DOI [10.5555/3326943.3327018, DOI 10.5555/3326943.3327018]
   Tsung-Yi Lin, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P2999, DOI 10.1109/ICCV.2017.324
   Uijlings JRR, 2013, INT J COMPUT VISION, V104, P154, DOI 10.1007/s11263-013-0620-5
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wang Q, 2020, INT SYM QUAL ELECT, P1, DOI [10.1109/isqed48828.2020.9137057, 10.1109/ISQED48828.2020.9137057, 10.1109/CVPR42600.2020.01155]
   Wang QF, 2014, LECT NOTES COMPUT SC, V8691, P378, DOI 10.1007/978-3-319-10578-9_25
   Wang XL, 2015, IEEE I CONF COMP VIS, P2794, DOI 10.1109/ICCV.2015.320
   Wang YZ, 2019, IEEE IMAGE PROC, P3342, DOI [10.1109/icip.2019.8803418, 10.1109/ICIP.2019.8803418]
   Weiss Y., 2008, NIPS, V1, P4
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Xia RK, 2014, AAAI CONF ARTIF INTE, P2156
   Xiao Q, 2017, ARXIV
   Yan C, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1535, DOI 10.1145/3343031.3350927
   Yang JX, 2020, PATTERN RECOGN, V100, DOI 10.1016/j.patcog.2019.107116
   Zhang BL, 2021, APPL INTELL, V51, P5912, DOI 10.1007/s10489-020-02180-7
   Zhang F, 2016, IEEE T GEOSCI REMOTE, V54, P5553, DOI 10.1109/TGRS.2016.2569141
   Zhang RM, 2015, IEEE T IMAGE PROCESS, V24, P4766, DOI 10.1109/TIP.2015.2467315
   Zhang Z, 2019, IEEE T IMAGE PROCESS, V28, P4803, DOI 10.1109/TIP.2019.2912290
   Zhao F, 2015, PROC CVPR IEEE, P1556, DOI 10.1109/CVPR.2015.7298763
   Zheng XT, 2020, NEUROCOMPUTING, V403, P224, DOI 10.1016/j.neucom.2020.04.037
   Zhu XF, 2017, IEEE T MULTIMEDIA, V19, P2033, DOI 10.1109/TMM.2017.2703636
NR 60
TC 1
Z9 1
U1 1
U2 11
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2023
VL 39
IS 11
BP 5405
EP 5420
DI 10.1007/s00371-022-02668-y
EA SEP 2022
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA W5SX6
UT WOS:000855632900001
DA 2024-07-18
ER

PT J
AU Bhattacharyya, D
   Rao, NT
   Joshua, ESN
   Hu, YC
AF Bhattacharyya, Debnath
   Rao, N. Thirupathi
   Joshua, Eali Stephen Neal
   Hu, Yu-Chen
TI A bi-directional deep learning architecture for lung nodule semantic
   segmentation
SO VISUAL COMPUTER
LA English
DT Article
DE Lung cancer; Deep learning; Computer-aided diagnosis; Bidirectional
   feature extraction; Convolutional neural network
ID IMAGE SEGMENTATION; CANCER
AB Lung nodules are abnormal growths and lesions may exist. Both lungs may have nodules. Most lung nodules are harmless (not cancerous/malignant). Pulmonary nodules are rare in lung cancer. X-rays and CT scans identify the lung nodules. Doctors may term the growth a lung spot, coin lesion, or shadow. It is necessary to obtain properly computed tomography (CT) scans of the lungs to get an accurate diagnosis and a good estimate of the severity of lung cancer. This study aims to design and evaluate a deep learning (DL) algorithm for identifying pulmonary nodules (PNs) using the LUNA-16 dataset and examine the prevalence of PNs using DB-Net. The paper states that a new, resource-efficient deep learning architecture is called for, and it has been given the name of DB-NET. When a physician orders a CT scan, they need to employ an accurate and efficient lung nodule segmentation method because they need to detect lung cancer at an early stage. However, segmentation of lung nodules is a difficult task because of the nodules' characteristics on the CT image as well as the nodules' concealed shape, visual quality, and context. The DB-NET model architecture is presented as a resource-efficient deep learning solution for handling the challenge at hand in this paper. Furthermore, it incorporates the Mish nonlinearity function and the mask class weights to improve segmentation effectiveness. In addition to the LUNA-16 dataset, which contained 1200 lung nodules collected during the LUNA-16 test, the LUNA-16 dataset was extensively used to train and assess the proposed model. The DB-NET architecture surpasses the existing U-NET model by a dice coefficient index of 88.89%, and it also achieves a similar level of accuracy to that of human experts.
C1 [Bhattacharyya, Debnath] Koneru Lakshmaiah Educ Fdn, Dept Comp Sci & Engn, Guntur 522502, Andhra Pradesh, India.
   [Rao, N. Thirupathi; Joshua, Eali Stephen Neal] Vignans Inst Informat Technol A, Dept Comp Sci & Engn, Visakhapatnam 530049, Andhra Pradesh, India.
   [Hu, Yu-Chen] Providence Univ, Dept Comp Sci & Informat Management, 200,Sec 7,Taiwan Blvd, Taichung 43301, Taiwan.
C3 Koneru Lakshmaiah Education Foundation (K L Deemed to be University);
   Providence University - Taiwan
RP Hu, YC (corresponding author), Providence Univ, Dept Comp Sci & Informat Management, 200,Sec 7,Taiwan Blvd, Taichung 43301, Taiwan.
EM debnathb@kluniversity.in; nakkathiru@vignaniit.edu.in;
   stephen.eali@vignaniit.edu.in; ychu@pu.edu.tw
RI Neal Joshua, Eali Stephen/HLX-0331-2023; N, Thirupathi Rao/F-2672-2014;
   Bhattacharyya, Debnath/A-3144-2016
OI Neal Joshua, Eali Stephen/0000-0002-3400-1242; N, Thirupathi
   Rao/0000-0002-8957-8532; Bhattacharyya, Debnath/0000-0003-0140-9644; Hu,
   Yu-Chen/0000-0002-5055-3645
CR Agarwal M, 2021, J MED SYST, V45, DOI 10.1007/s10916-021-01707-w
   Almeida G, 2020, J MED SYST, V44, DOI 10.1007/s10916-020-01641-3
   Amarasinghe KC, 2021, FRONT ONCOL, V11, DOI 10.3389/fonc.2021.580806
   [Anonymous], 2021, Key statistics for lung cancer
   Ayalew Yodit Abebe, 2021, BMC Biomed Eng, V3, P4, DOI 10.1186/s42490-021-00050-y
   Baek S, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-53461-2
   Bouget D, 2019, INT J COMPUT ASS RAD, V14, P977, DOI 10.1007/s11548-019-01948-8
   Chen KB, 2021, COMPUT METH PROG BIO, V207, DOI 10.1016/j.cmpb.2021.106170
   Chiu TW, 2021, SCI REP-UK, V11, DOI 10.1038/s41598-021-90599-4
   Cui L, 2020, BMC BIOINFORMATICS, V21, DOI 10.1186/s12859-020-3431-z
   Dai WT, 2021, VISUAL COMPUT, V37, P3093, DOI 10.1007/s00371-021-02257-5
   Do NT, 2021, DIAGNOSTICS, V11, DOI 10.3390/diagnostics11040691
   Ghazipour A, 2021, PROC SPIE, V11597, DOI 10.1117/12.2581755
   Gianchandani N, 2020, J AMB INTEL HUM COMP, DOI 10.1007/s12652-020-02669-6
   Hossain S, 2019, INT CONF ACOUST SPEE, P1348, DOI 10.1109/ICASSP.2019.8683802
   Hwang JH, 2020, J MED SYST, V44, DOI 10.1007/s10916-020-01652-0
   Jalali Y, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21010268
   Jin JB, 2021, FRONT ONCOL, V10, DOI 10.3389/fonc.2020.614201
   Joshua Eali Stephen Neal, 2020, Revue d'Intelligence Artificielle, V34, P351, DOI 10.18280/ria.340314
   Joshua ESN, 2021, J HEALTHC ENG, V2021, DOI 10.1155/2021/6695518
   Kabir Solaiman, 2021, 2021 International Conference on Advance Computing and Innovative Technologies in Engineering (ICACITE), P373, DOI 10.1109/ICACITE51222.2021.9404584
   Li YY, 2023, VISUAL COMPUT, V39, P2223, DOI 10.1007/s00371-021-02328-7
   Lin L., 2021, INTELLIGENT COMPUTIN, P3, DOI [10.1007/978-981-16-1160-5_1, DOI 10.1007/978-981-16-1160-5_1]
   Lin XF, 2021, CLIN LUNG CANCER, V22, pE756, DOI 10.1016/j.cllc.2021.02.004
   Liu XB, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3465220
   Liu XB, 2021, SUSTAINABILITY-BASEL, V13, DOI 10.3390/su13031224
   Long FX, 2020, BMC BIOINFORMATICS, V21, DOI 10.1186/s12859-019-3332-1
   Mayo Clinic Staff, 2021, TESTS DIAGNOSIS
   Mohammed Kamel K., 2021, Journal of Medical Engineering & Technology, V45, DOI 10.1080/03091902.2021.1905895
   Nemoto T, 2020, J RADIAT RES, V61, P257, DOI 10.1093/jrr/rrz086
   Park J, 2020, J DIGIT IMAGING, V33, P221, DOI 10.1007/s10278-019-00223-1
   Rahman MF, 2021, PROC SPIE, V11595, DOI 10.1117/12.2581882
   Rajagopalan K, 2020, BMC MED INFORM DECIS, V20, DOI 10.1186/s12911-020-01220-z
   Ratnaraj RR., 2021, INFOCOMP J COMPUT SC, V20, P101
   Rocha J, 2020, J MED SYST, V44, DOI 10.1007/s10916-020-1541-9
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Ruikar DD, 2021, J MED SYST, V45, DOI 10.1007/s10916-021-01724-9
   Saood A, 2021, BMC MED IMAGING, V21, DOI 10.1186/s12880-020-00529-5
   Satyanarayana KV, 2022, MULTIDIM SYST SIGN P, V33, P301, DOI 10.1007/s11045-021-00800-0
   Shi LK, 2021, VISUAL COMPUT, V37, P1343, DOI 10.1007/s00371-020-01869-7
   Shorten C, 2019, J BIG DATA-GER, V6, DOI 10.1186/s40537-019-0197-0
   Sori WJ, 2021, FRONT COMPUT SCI-CHI, V15, DOI 10.1007/s11704-020-9050-z
   Su R, 2021, FRONT GENET, V12, DOI 10.3389/fgene.2021.639930
   Tong GF, 2018, OPTIK, V174, P460, DOI 10.1016/j.ijleo.2018.08.086
   Wang Y, 2021, COMPUT MATH METHOD M, V2021, DOI 10.1155/2021/2485934
   Yang JD, 2021, BIOMED SIGNAL PROCES, V68, DOI 10.1016/j.bspc.2021.102643
   Yoo SJ, 2021, KOREAN J RADIOL, V22, P476, DOI 10.3348/kjr.2020.0318
   Zhao C., 2018, PREPRINT, DOI [10.29007/bgkd, DOI 10.29007/BGKD]
   Zhou Zongwei, 2018, Deep Learn Med Image Anal Multimodal Learn Clin Decis Support (2018), V11045, P3, DOI [10.1007/978-3-030-00889-5_1, 10.1007/978-3-030-00689-1_1]
NR 49
TC 10
Z9 10
U1 3
U2 25
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2023
VL 39
IS 11
BP 5245
EP 5261
DI 10.1007/s00371-022-02657-1
EA SEP 2022
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA W5SX6
UT WOS:000852265400001
PM 36097497
OA Bronze, Green Published
DA 2024-07-18
ER

PT J
AU Fan, MZ
   Liu, LL
   Deng, N
   Xin, BJ
   Wang, YL
   He, Y
AF Fan, Mingzhu
   Liu, Lulu
   Deng, Na
   Xin, Binjie
   Wang, Yiliang
   He, Yan
TI Digital 3D system for classifying fabric pilling based on improved
   active contours and neural network
SO VISUAL COMPUTER
LA English
DT Article
DE Pilling assessment; 3D reconstruction; Curve evolution; Neural network;
   Multi-view stereo vision
ID OBJECTIVE EVALUATION; DETECTING PILLS; IMAGE-ANALYSIS
AB In this paper, we proposed an objective 3D evaluation system of pilling. It can be applied to fabrics with intricate patterns and textures. The major features of the pilling evaluation system include a 3D fabric surface reconstruction based on a multi-view stereo vision algorithm, an improved active contours model for pill segmentation, and objective pilling grade calculation utilizing a neural network. To improve the accuracy of pilling grade estimation, the obtained maxima algorithm is used to locate the pilling, which is then used as the seed point for curve evolution until the curve converges to the pilling's edge. In addition, the binarization and adaptive threshold methods can also be utilized to obtain the pilling binary graph. Three feature parameters, such as the pill number, area, and coverage are extracted from the segmented binary graph, which is used to objectively evaluate the pilling grade of fabric. The experimental results show that the objective evaluation system can accurately evaluate the fabric pilling grade, which is highly consistent with the subjective evaluation results.
C1 [Fan, Mingzhu; Liu, Lulu; Deng, Na; Wang, Yiliang; He, Yan] Shanghai Univ Engn Sci, Sch Elect & Elect Engn, Shanghai 201620, Peoples R China.
   [Xin, Binjie] Shanghai Univ Engn Sci, Sch Text & Fash, Shanghai 201620, Peoples R China.
C3 Shanghai University of Engineering Science; Shanghai University of
   Engineering Science
RP Xin, BJ (corresponding author), Shanghai Univ Engn Sci, Sch Text & Fash, Shanghai 201620, Peoples R China.
EM xinbj@sues.edu.cn
RI Liu, Lulu/GXZ-9653-2022
FU ShanghaiNatural Science Foundation of China [18ZR1416600]; National
   Natural Science Foundation of China [61876106]; Shanghai Local
   Capacity-Building Project [19030501200]; Zhihong Scholars Plan of
   Shanghai University of Engineering Science [2018RC032017]
FX This studywas funded by the ShanghaiNatural Science Foundation of China
   (18ZR1416600), National Natural Science Foundation of China (61876106),
   Shanghai Local Capacity-Building Project (No. 19030501200) and Zhihong
   Scholars Plan of Shanghai University of Engineering Science
   (2018RC032017).
CR Abril HC, 1998, OPT ENG, V37, P2937, DOI 10.1117/1.601881
   [Anonymous], 1999, D3512 ASTM INT
   [Anonymous], 2011, 143 AATCC
   Chan TF, 2001, IEEE T IMAGE PROCESS, V10, P266, DOI 10.1109/83.902291
   Chen X, 2009, TEXT RES J, V79, P1389, DOI 10.1177/0040517508099913
   Deng Z, 2011, J TEXT I, V102, P1, DOI 10.1080/00405000903396266
   Deshpande AM, 2020, PROCEDIA MANUF, V48, P1064, DOI 10.1016/j.promfg.2020.05.146
   Furukawa Y, 2010, IEEE T PATTERN ANAL, V32, P1362, DOI 10.1109/TPAMI.2009.161
   Furukawa Y, 2013, FOUND TRENDS COMPUT, V9, P1, DOI 10.1561/0600000052
   Garg M., 2020, Journal of the Institute of Electronics and Computer, V2, P28, DOI [10.33969/JIEC.2020.2100.333, DOI 10.33969/JIEC.2020.21003]
   Jing JF, 2022, TEXT RES J, V92, P30, DOI 10.1177/0040517520928604
   Keith, 2008, SNAVELY SCENE RECONS
   Kim S.C., 2005, IEEE INSTRUMENTATION
   Liu LL, 2021, J TEXT I, V112, P1986, DOI 10.1080/00405000.2020.1862479
   Mendes AO, 2006, PROC SPIE, V6056, DOI 10.1117/12.640438
   Mendes AD., 2020, J TEXT I, V112, P1
   Mendes AD, 2011, TEXT RES J, V81, P892, DOI 10.1177/0040517510397573
   Mendes AD, 2010, TEXT RES J, V80, P1887, DOI 10.1177/0040517510373636
   Nister D., 2008, MACH VISION APPL
   Obuchowski J, 2014, MECH SYST SIGNAL PR, V46, P389, DOI 10.1016/j.ymssp.2014.01.009
   Ouyang WB, 2013, J ELECTRON IMAGING, V22, DOI 10.1117/1.JEI.22.4.043031
   Pyun P., AUTOMATIC DEFECT INS
   Sadeghi BHM, 2000, J MATER PROCESS TECH, V103, P411, DOI 10.1016/S0924-0136(00)00498-2
   Saharkhiz S, 2012, J ENG FIBER FABR, V7, P35
   Snavely N, 2006, ACM T GRAPHIC, V25, P835, DOI 10.1145/1141911.1141964
   Techniková L, 2017, J TEXT I, V108, P123, DOI 10.1080/00405000.2016.1160476
   Telli A, 2020, FIBER POLYM, V21, P1841, DOI 10.1007/s12221-020-9552-1
   Wang YL, 2020, MEASUREMENT, V166, DOI 10.1016/j.measurement.2020.108264
   Wang YL, 2020, OPT LASER TECHNOL, V131, DOI 10.1016/j.optlastec.2020.106415
   Xin BJ, 2002, TEXT RES J, V72, P1057, DOI 10.1177/004051750207201204
   Yang CS, 2019, ELECTRONICS-SWITZ, V8, DOI 10.3390/electronics8050474
   Yang XW, 2013, NEUROCOMPUTING, V113, P1, DOI 10.1016/j.neucom.2012.12.048
   Yurim Kim, 2016, [Textile Science and Engineering, 한국섬유공학회지], V53, P360
   Zhang JM, 2007, TEXT RES J, V77, P929, DOI 10.1177/0040517507083522
   Zhang SC, 2018, IEEE T NEUR NET LEAR, V29, P1774, DOI 10.1109/TNNLS.2017.2673241
NR 35
TC 3
Z9 3
U1 1
U2 11
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2023
VL 39
IS 10
BP 5085
EP 5095
DI 10.1007/s00371-022-02647-3
EA SEP 2022
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA T1GK8
UT WOS:000852265400002
DA 2024-07-18
ER

PT J
AU Liu, JY
   Jiang, ZY
   Wu, GY
   Liu, RS
   Fan, X
AF Liu, Jinyuan
   Jiang, Zhiying
   Wu, Guanyao
   Liu, Risheng
   Fan, Xin
TI A unified image fusion framework with flexible bilevel paradigm
   integration
SO VISUAL COMPUTER
LA English
DT Article
DE Image fusion; Fusion rule; Two-scale decomposition; Bilevel paradigm
ID PEDESTRIAN RECOGNITION; MULTI-FOCUS; ENHANCEMENT; ENSEMBLE
AB Multi-modality image fusion refers to integrating series of images acquired from different sensors and obtaining a fused image which is expected to provide more comprehensive information. It plays a pivotal role in many computer vision tasks and promotes the performance of subsequent applications. Most existing approaches attempted to design appropriate fusion rules for specific image fusion task, which have a bad generalization to different fusion tasks. Apart from that, the texture details in the fused images are common blurred due to undesirable artifacts introduced from the different modalities. In this paper, we propose a generic multimodal image fusion framework by combing the visual saliency model and flexible bilevel paradigm. Specifically, we decompose input images into an intensity layer, representing large-scale intensity variations, and a detail layer, containing small textures changes. Then we fuse the intensity layer through visual saliency map to improve the contrast of an image under consideration, and design a bilevel paradigm for fusing the detail layer to obtain fine details. Furthermore, to make the fused result visual friendly, a deep prior is built in the bilevel paradigm. Besides, an elastic target-guided hyper-parameter is introduced to dominate the proportion of the textural details from the source images, which can be further adjusted in accordance with different fusion tasks. We conducted the experiments on three available datasets to demonstrate the superiority of our framework against the state-of-the-art methods quantitatively and qualitatively in a variety of fusion tasks, including infrared and visible image, near-infrared and visible image fusion and multimodal medical image fusion.
C1 [Liu, Jinyuan; Jiang, Zhiying; Wu, Guanyao; Liu, Risheng; Fan, Xin] Dalian Univ Technol, Int Sch Informat Sci & Engn, Dalian, Peoples R China.
   [Liu, Jinyuan; Jiang, Zhiying; Liu, Risheng; Fan, Xin] Key Lab Ubiquitous Network & Serv Software Liaoni, Dalian, Peoples R China.
C3 Dalian University of Technology
RP Fan, X (corresponding author), Dalian Univ Technol, Int Sch Informat Sci & Engn, Dalian, Peoples R China.; Fan, X (corresponding author), Key Lab Ubiquitous Network & Serv Software Liaoni, Dalian, Peoples R China.
EM xin.fan@dlut.edu.cn
RI , RollingPlain/HJI-2318-2023; Liu, Jinyuan/GNP-2535-2022
OI Liu, Jinyuan/0000-0003-2085-2676
FU Science Foundation of China [61922019, 61733002, 616721255]; LiaoNing
   Revitalization
FX This paper is funded by Science Foundation of China under Grant (Nos.
   61922019, 61733002 and 616721255),LiaoNing Revitalization.
CR Adu JH, 2016, J VIS COMMUN IMAGE R, V40, P218, DOI 10.1016/j.jvcir.2016.06.026
   Adu JH, 2013, INFRARED PHYS TECHN, V61, P94, DOI 10.1016/j.infrared.2013.07.010
   Aslantas V, 2015, AEU-INT J ELECTRON C, V69, P160, DOI 10.1016/j.aeue.2015.09.004
   Bai XZ, 2016, INFRARED PHYS TECHN, V76, P546, DOI 10.1016/j.infrared.2016.04.015
   Barra V, 2001, NEUROIMAGE, V13, P410, DOI 10.1006/nimg.2000.0707
   Bhatnagar G, 2013, IEEE T MULTIMEDIA, V15, P1014, DOI 10.1109/TMM.2013.2244870
   Brown M, 2011, PROC CVPR IEEE, P177, DOI 10.1109/CVPR.2011.5995637
   Chai PF, 2017, IEEE ACCESS, V5, P6724, DOI 10.1109/ACCESS.2017.2685178
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Dikmen M, 2011, LECT NOTES COMPUT SC, V6495, P501, DOI 10.1007/978-3-642-19282-1_40
   Du QL, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18113827
   Duan ZY, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1023, DOI 10.1145/3123266.3123356
   Girshick R., 2014, PROC CVPR IEEE, DOI [DOI 10.1109/CVPR.2014.81, 10.1109/CVPR.2014.81]
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Gray D, 2008, LECT NOTES COMPUT SC, V5302, P262, DOI 10.1007/978-3-540-88682-2_21
   Guo HQ, 2017, J OPT SOC AM A, V34, P1961, DOI 10.1364/JOSAA.34.001961
   Han Y, 2013, INFORM FUSION, V14, P127, DOI 10.1016/j.inffus.2011.08.002
   Hu HM, 2017, IEEE T MULTIMEDIA, V19, P2706, DOI 10.1109/TMM.2017.2711422
   Lahoud F., 2019, ARXIV
   Li HF, 2018, PATTERN RECOGN, V79, P130, DOI 10.1016/j.patcog.2018.02.005
   Li HF, 2016, INFRARED PHYS TECHN, V76, P174, DOI 10.1016/j.infrared.2016.02.005
   Li H, 2019, IEEE T IMAGE PROCESS, V28, P2614, DOI 10.1109/TIP.2018.2887342
   Li ST, 2013, IEEE T IMAGE PROCESS, V22, P2864, DOI 10.1109/TIP.2013.2244222
   Li WS, 2019, IEEE T BIO-MED ENG, V66, P1172, DOI 10.1109/TBME.2018.2869432
   Liang JL, 2012, IEEE T IMAGE PROCESS, V21, P2898, DOI 10.1109/TIP.2012.2183140
   Liu CH, 2017, INFRARED PHYS TECHN, V83, P94, DOI 10.1016/j.infrared.2017.04.018
   Liu JY, 2022, IEEE T CIRC SYST VID, V32, P105, DOI 10.1109/TCSVT.2021.3056725
   Liu JY, 2021, IEEE SIGNAL PROC LET, V28, P1818, DOI 10.1109/LSP.2021.3109818
   Liu K, 2009, CHINESE J AERONAUT, V22, P75, DOI 10.1016/S1000-9361(08)60071-0
   Liu RS, 2021, IEEE T IMAGE PROCESS, V30, P1261, DOI 10.1109/TIP.2020.3043125
   Liu Y, 2015, IET IMAGE PROCESS, V9, P347, DOI 10.1049/iet-ipr.2014.0311
   Liu Y, 2018, INT J WAVELETS MULTI, V16, DOI 10.1142/S0219691318500182
   Liu Y, 2019, IEEE SIGNAL PROC LET, V26, P485, DOI 10.1109/LSP.2019.2895749
   Liu Y, 2017, INFORM FUSION, V36, P191, DOI 10.1016/j.inffus.2016.12.001
   Liu Y, 2016, IEEE SIGNAL PROC LET, V23, P1882, DOI 10.1109/LSP.2016.2618776
   Liu Z, 2001, PATTERN RECOGN LETT, V22, P929, DOI 10.1016/S0167-8655(01)00047-2
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Lu XQ, 2014, INFRARED PHYS TECHN, V67, P397, DOI 10.1016/j.infrared.2014.09.007
   Ma JY, 2019, INFORM FUSION, V48, P11, DOI 10.1016/j.inffus.2018.09.004
   Ma JY, 2016, INFORM FUSION, V31, P100, DOI 10.1016/j.inffus.2016.02.001
   Ma JL, 2017, INFRARED PHYS TECHN, V82, P8, DOI 10.1016/j.infrared.2017.02.005
   Meng FJ, 2017, COMPUT ELECTR ENG, V62, P375, DOI 10.1016/j.compeleceng.2016.09.019
   Nencini F, 2007, INFORM FUSION, V8, P143, DOI 10.1016/j.inffus.2006.02.001
   Nichol A, 2018, ARXIV
   Olshausen BA, 1996, NATURE, V381, P607, DOI 10.1038/381607a0
   Pu MY, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P483, DOI 10.1145/3240508.3240542
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Saeedi J, 2012, APPL SOFT COMPUT, V12, P1041, DOI 10.1016/j.asoc.2011.11.020
   Selesnick IW, 2005, IEEE SIGNAL PROC MAG, V22, P123, DOI 10.1109/MSP.2005.1550194
   Shibata T, 2016, J ELECTRON IMAGING, V25, DOI 10.1117/1.JEI.25.1.013016
   Tomasi C, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P839, DOI 10.1109/ICCV.1998.710815
   Viola P, 2001, PROC CVPR IEEE, P511, DOI 10.1109/cvpr.2001.990517
   Wang AL, 2014, INT CONF INSTR MEAS, P620, DOI 10.1109/IMCCC.2014.132
   Xydeas CS, 2000, ELECTRON LETT, V36, P308, DOI 10.1049/el:20000267
   Yang B, 2010, IEEE T INSTRUM MEAS, V59, P884, DOI 10.1109/TIM.2009.2026612
   Yin HT, 2015, NEUROCOMPUTING, V148, P600, DOI 10.1016/j.neucom.2014.07.003
   Yin M, 2019, IEEE T INSTRUM MEAS, V68, P49, DOI 10.1109/TIM.2018.2838778
   Zhang BH, 2015, INFRARED PHYS TECHN, V73, P286, DOI 10.1016/j.infrared.2015.10.004
   Zhang K, 2017, IEEE T IMAGE PROCESS, V26, P3142, DOI 10.1109/TIP.2017.2662206
   Zhang Q, 2018, INFORM FUSION, V40, P57, DOI 10.1016/j.inffus.2017.05.006
   Zhao WD, 2018, IEEE T MULTIMEDIA, V20, P866, DOI 10.1109/TMM.2017.2760100
   Zhao WD, 2017, IEEE T INSTRUM MEAS, V66, P2283, DOI 10.1109/TIM.2017.2700198
   Zhong JY, 2016, COMM COM INF SC, V663, P78, DOI 10.1007/978-981-10-3005-5_7
   Zhu P, 2017, INFRARED PHYS TECHN, V81, P282, DOI 10.1016/j.infrared.2017.01.013
   Zhu ZQ, 2018, INFORM SCIENCES, V432, P516, DOI 10.1016/j.ins.2017.09.010
NR 65
TC 4
Z9 4
U1 0
U2 7
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2023
VL 39
IS 10
BP 4869
EP 4886
DI 10.1007/s00371-022-02633-9
EA AUG 2022
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA T1GK8
UT WOS:000844461600001
DA 2024-07-18
ER

PT J
AU Wang, XJ
   Hua, Z
   Li, JJ
AF Wang, Xuejiao
   Hua, Zhen
   Li, Jinjiang
TI Cross-UNet: dual-branch infrared and visible image fusion framework
   based on cross-convolution and attention mechanism
SO VISUAL COMPUTER
LA English
DT Article
DE Attention mechanism; Cross-convolution; Image fusion
ID NETWORK; NEST
AB Existing infrared and visible image fusion methods suffer from edge information loss, artifact introduction, and image distortion. Therefore, a dual-branch network model based on the attention mechanism, Cross-UNet, is proposed in this paper for infrared and visible image fusion. First, the encoder part adopts an asymmetric convolution kernel, which can simultaneously obtain local detail information and global structural information of the source image from different directions. Second, in order to fuse the dual-branch image features of different scales, a dual-attention mechanism is added to the fusion block. Finally, the decoder adopts an attention model with a large receptive field to enhance the ability to judge the importance of features, thereby improving the fusion quality. On the public datasets of TNO, RoadScene, and Country, the results are fully compared with nine other advanced fusion methods both qualitatively and quantitatively. The results show that the model in this paper has superior performance and high stability.
C1 [Wang, Xuejiao; Li, Jinjiang] Shandong Technol & Business Univ, Sch Comp Sci & Technol, Inst Network Technol INT, Yantai 264005, Peoples R China.
   [Hua, Zhen] Shandong Technol & Business Univ, Sch Informat & Elect Engn, Yantai 264005, Peoples R China.
C3 Shandong Technology & Business University; Shandong Technology &
   Business University
RP Hua, Z (corresponding author), Shandong Technol & Business Univ, Sch Informat & Elect Engn, Yantai 264005, Peoples R China.
EM huazhen@sdtbu.edu.cn
RI Hua, Zhen/ABG-8734-2021
FU National Natural Science Foundation of China [61772319, 62002200,
   62176140, 12001327]; Shandong Natural Science Foundation of China
   [ZR2021QF134, ZR2021MF068]; Yantai science and technology innovation
   development plan [2022JCYJ031]
FX The authors acknowledge the National Natural Science Foundation of China
   (61772319, 62002200, 62176140, and 12001327), Shandong Natural Science
   Foundation of China (ZR2021QF134 and ZR2021MF068), and Yantai science
   and technology innovation development plan (2022JCYJ031).
CR Aghamaleki JA, 2023, VISUAL COMPUT, V39, P1181, DOI 10.1007/s00371-021-02396-9
   Brown M, 2011, PROC CVPR IEEE, P177, DOI 10.1109/CVPR.2011.5995637
   Cai WC, 2019, INT CONF ACOUST SPEE, P5991, DOI [10.1109/ICASSP.2019.8682386, 10.1109/icassp.2019.8682386]
   Cai WW, 2022, IEEE GEOSCI REMOTE S, V19, DOI 10.1109/LGRS.2020.3026587
   Chen X, 2022, OPT LASER TECHNOL, V149, DOI 10.1016/j.optlastec.2021.107787
   Fu Y, 2021, INT C PATT RECOG, P10675, DOI 10.1109/ICPR48806.2021.9412293
   Galassi A, 2021, IEEE T NEUR NET LEAR, V32, P4291, DOI 10.1109/TNNLS.2020.3019893
   Geng W, 2016, INTERSPEECH, P2944, DOI 10.21437/Interspeech.2016-686
   Guo M.H., ARXIV
   Hou QB, 2020, PROC CVPR IEEE, P4002, DOI 10.1109/CVPR42600.2020.00406
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang ZL, 2019, IEEE I CONF COMP VIS, P603, DOI 10.1109/ICCV.2019.00069
   Li GF, 2021, INFORM FUSION, V71, P109, DOI 10.1016/j.inffus.2021.02.008
   Li H, 2021, INFORM FUSION, V73, P72, DOI 10.1016/j.inffus.2021.02.023
   Li H, 2020, IEEE T INSTRUM MEAS, V69, P9645, DOI 10.1109/TIM.2020.3005230
   Li H, 2020, IEEE T IMAGE PROCESS, V29, P4733, DOI 10.1109/TIP.2020.2975984
   Li H, 2019, IEEE T IMAGE PROCESS, V28, P2614, DOI 10.1109/TIP.2018.2887342
   Li XS, 2021, KNOWL-BASED SYST, V224, DOI 10.1016/j.knosys.2021.107087
   Liu C, 2021, IEEE SENS J, V21, P14950, DOI 10.1109/JSEN.2021.3073568
   Liu Y, arXiv
   Liu YQ, 2022, IEEE T CIRC SYST VID, V32, P4927, DOI 10.1109/TCSVT.2021.3138431
   Liu Z, 2012, IEEE T PATTERN ANAL, V34, P94, DOI 10.1109/TPAMI.2011.109
   Lu R., 2022, VISUAL COMPUT, V1, P1
   Ma JY, 2021, IEEE T INSTRUM MEAS, V70, DOI 10.1109/TIM.2020.3038013
   Ma JY, 2019, INFORM FUSION, V48, P11, DOI 10.1016/j.inffus.2018.09.004
   Nozaripour A, 2023, VISUAL COMPUT, V39, P1731, DOI 10.1007/s00371-022-02441-1
   Park J., 2018, ARXIV
   Qing YH, 2021, REMOTE SENS-BASEL, V13, DOI 10.3390/rs13030335
   Song AY, 2022, NEUROCOMPUTING, V483, P183, DOI 10.1016/j.neucom.2022.02.025
   Szegedy Christian, 2016, P IEEE C COMP VIS PA, DOI DOI 10.1109/CVPR.2016.308
   Tang LF, 2022, INFORM FUSION, V83, P79, DOI 10.1016/j.inffus.2022.03.007
   Toet Alexander, 2014, Figshare
   Wang SL, 2018, PROC CVPR IEEE, P2589, DOI 10.1109/CVPR.2018.00274
   Wang S, 2017, MED IMAGE ANAL, V40, P172, DOI 10.1016/j.media.2017.06.014
   Wang ZS, 2022, IEEE T CIRC SYST VID, V32, P3360, DOI 10.1109/TCSVT.2021.3109895
   Wang ZH, 2019, INFRARED PHYS TECHN, V98, P212, DOI 10.1016/j.infrared.2019.03.030
   Xu H, 2022, COMPUT VIS IMAGE UND, V218, DOI 10.1016/j.cviu.2022.103407
   Xu H, 2022, IEEE T PATTERN ANAL, V44, P502, DOI 10.1109/TPAMI.2020.3012548
   Xue ZX, 2021, IEEE J-STARS, V14, P3566, DOI 10.1109/JSTARS.2021.3065987
   Yang BS, 2021, NEUROCOMPUTING, V458, P157, DOI 10.1016/j.neucom.2021.06.009
   Yang F, 2022, VISUAL COMPUT, V38, P1579, DOI 10.1007/s00371-021-02089-3
   Yin WX, 2022, INFRARED PHYS TECHN, V121, DOI 10.1016/j.infrared.2022.104041
   Yousif AS, 2022, BIOMED SIGNAL PROCES, V72, DOI 10.1016/j.bspc.2021.103357
   Yu Q., ARXIV
   Yu Q, 2021, PATTERN RECOGN, V113, DOI 10.1016/j.patcog.2020.107756
   Yu Q, 2019, IEEE T IMAGE PROCESS, V28, P4060, DOI 10.1109/TIP.2019.2905537
   Zhang H, 2020, AAAI CONF ARTIF INTE, V34, P12797
   Zhang WH, 2021, IEEE SENS J, V21, P19176, DOI 10.1109/JSEN.2021.3090021
   Zhang Y, 2020, INFORM FUSION, V54, P99, DOI 10.1016/j.inffus.2019.07.011
   Zhang ZZ, 2020, PROC CVPR IEEE, P3183, DOI 10.1109/CVPR42600.2020.00325
   Zhu ZQ, 2018, INFORM SCIENCES, V432, P516, DOI 10.1016/j.ins.2017.09.010
NR 51
TC 14
Z9 14
U1 5
U2 43
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2023
VL 39
IS 10
BP 4801
EP 4818
DI 10.1007/s00371-022-02628-6
EA AUG 2022
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA T1GK8
UT WOS:000839465000002
DA 2024-07-18
ER

PT J
AU Wang, ZH
   Li, YJ
   Xu, HX
   Liu, JZ
   Deng, CY
AF Wang, Zhihao
   Li, Yajuan
   Xu, Huixia
   Liu, Jianzhen
   Deng, Chongyang
TI P-spline curves
SO VISUAL COMPUTER
LA English
DT Article
DE P-spline curves; Parametric curves; Mean value coordinates
ID B-SPLINE; BEZIER
AB In this paper, we introduce a new parametric spline curve, named as P-spline curves. Given a control point set associated with a set of knots and parameters, we first define three sets of points using this set of knots and parameters. One set of points lie on a line segment spaced by knots, and two other sets of points lie on two sides of the line segment symmetrically according to the parameters and knots. Then, we compute the mean value coordinates of points on the middle line segment with respect to the two involved quadrilaterals, whose vertices are selected from the three defined point sets. Last, we use these coordinates and blending functions to construct basis functions, which are used to define P-spline curves together with given control point set. There are several desirable features for the P-spline curves. The continuous orders of the resulting curves are determined by the basis functions, and we can adjust the distance between the curves and control points by changing the parameters. Moreover, the construction of P-spline curves is simple, and the relations between the P-spline curves and knots/parameters are intuitive. More importantly, the influences of parameters and control points are local because the four vertices of each quadrilateral only depend on one parameter and three knots. Some numerical examples are used to show that P-spline curves are more local than NURBS (non-uniform rational B-spline) curves, P-curves and P-Bspline curves.
C1 [Wang, Zhihao; Xu, Huixia] Zhejiang Wanli Univ, Inst Math, Ningbo 315100, Peoples R China.
   [Wang, Zhihao; Li, Yajuan; Liu, Jianzhen; Deng, Chongyang] Hangzhou Dianzi Univ, Sch Sci, Hangzhou 310018, Peoples R China.
C3 Zhejiang Wanli University; Hangzhou Dianzi University
RP Xu, HX (corresponding author), Zhejiang Wanli Univ, Inst Math, Ningbo 315100, Peoples R China.
EM xuhx0916@hotmail.com
RI Deng, Chongyang/E-4422-2017
OI Deng, Chongyang/0000-0002-8725-4622; wang, zhihao/0000-0001-6348-7125
FU National Natural Science Foundation ofChina [61872121]; Natural Science
   Foundation of Zhejiang Province [LY16F020020, LQ17A010009]; Project of
   the Science and Technology Plan for Zhejiang Province [LGF21F020022];
   Natural Science Foundation of Ningbo [2019A610033, 202003N4324]
FX This study was part of the research supported by the National Natural
   Science Foundation ofChina (Grant No. 61872121), the Natural Science
   Foundation of Zhejiang Province (Grant Nos. LY16F020020 and
   LQ17A010009), Project of the Science and Technology Plan for Zhejiang
   Province (Grant No. LGF21F020022) and the Natural Science Foundation of
   Ningbo (Grant Nos. 2019A610033 and 202003N4324).
CR Bezier P., 1972, Numerical Control: Mathematics and Applications
   Bezier P., 1986, The mathematical basis of the UNISURF CAD system
   Cao J, 2008, PROG NAT SCI-MATER, V18, P303, DOI 10.1016/j.pnsc.2007.09.005
   Cao J, 2011, J ZHEJIANG U-SCI C, V12, P800, DOI 10.1631/jzus.C1000381
   Coons S., 1964, SURFACES COMPUTER AI
   Cox M. G., 1972, Journal of the Institute of Mathematics and Its Applications, V10, P134
   FARIN G, 1983, COMPUT AIDED DESIGN, V15, P73, DOI 10.1016/0010-4485(83)90171-9
   Farin G., 2001, Curves and Surfaces for CAGD: A Practical Guide, Vfifth
   Floater MS, 2003, COMPUT AIDED GEOM D, V20, P19, DOI 10.1016/S0167-8396(03)00002-5
   GORDON WJ, 1974, COMPUT AIDED GEOM D, P95, DOI DOI 10.1016/B978-0-12-079050-0.50011-4
   Hormann K, 2006, ACM T GRAPHIC, V25, P1424, DOI 10.1145/1183287.1183295
   Kovács I, 2018, COMPUT AIDED GEOM D, V62, P117, DOI 10.1016/j.cagd.2018.03.020
   Kovács I, 2017, COMPUT AIDED DESIGN, V90, P113, DOI 10.1016/j.cad.2017.05.008
   Loe KF, 1996, VISUAL COMPUT, V12, P18
   Miura KT, 2022, VISUAL COMPUT, V38, P2723, DOI 10.1007/s00371-021-02149-8
   PIEGL L, 1991, IEEE COMPUT GRAPH, V11, P55, DOI 10.1109/38.67702
   Piegl L., 1997, The NURBS Book, V2, DOI DOI 10.1007/978-3-642-59223-2
   Versprille Kenneth., 1975, Computer-Aided Design Applications of the Rational BSpline Approximation Forms
NR 18
TC 0
Z9 0
U1 0
U2 7
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2023
VL 39
IS 10
BP 4697
EP 4707
DI 10.1007/s00371-022-02618-8
EA AUG 2022
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA T1GK8
UT WOS:000837917100003
DA 2024-07-18
ER

PT J
AU Liu, PF
   Chao, QW
   Huang, HW
   Wang, QY
   Zhao, ZY
   Peng, Q
   Yip, MK
   Liu, ES
   Jin, XG
AF Liu, Pengfei
   Chao, Qianwen
   Huang, Henwei
   Wang, Qiongyan
   Zhao, Zhongyuan
   Peng, Qi
   Yip, Milo K.
   Liu, Elvis S.
   Jin, Xiaogang
TI Velocity-based dynamic crowd simulation by data-driven optimization
SO VISUAL COMPUTER
LA English
DT Article
DE Crowd animation; Data driven; Motion control; Optimization
ID MODEL
AB A crowd simulator that generates realistic crowds with various movement patterns and environmental adaptability is urgently desired but underdeveloped for the applications of video games, urban visualization, autonomous driving, and robot navigation test. In this work, we present a novel velocity-based framework based on data-driven optimization to build dynamic crowd simulation that allows interactive control of global navigation, local collision avoidance, and group formation. An agent's adaptive decision-making regarding its goals and dynamic local environment is formulated as an optimization problem which is solved by finding an optimal velocity from the real-world crowd velocity dataset. Each component that affects an agent's movement is integrated into a velocity-based crowd energy metric to measure the similarity between the agent's required simulated velocity and a given velocity sample. The proposed model can simulate thousands of agents at interactive rates. In addition, the framework is general and scalable to be integrated with various crowd simulation methods to meet the requirements of various kinds of robot navigation test. We validate our approach through simulation experiments in robot navigation scenarios, as well as comparisons to real-world crowd data and popular crowd simulation methods.
C1 [Liu, Pengfei; Zhao, Zhongyuan; Peng, Qi; Jin, Xiaogang] Zhejiang Univ, State Key Lab CAD CG, Hangzhou 310058, Peoples R China.
   [Chao, Qianwen; Huang, Henwei] Harvard Med Sch, Boston, MA 02115 USA.
   [Wang, Qiongyan] Xidian Univ, Xian, Peoples R China.
   [Yip, Milo K.; Liu, Elvis S.] Tencent, MoreFun Studios, Shenzhen, Peoples R China.
C3 Zhejiang University; Harvard University; Harvard Medical School; Xidian
   University; Tencent
RP Jin, XG (corresponding author), Zhejiang Univ, State Key Lab CAD CG, Hangzhou 310058, Peoples R China.
EM jin@cad.zju.edu.cn
OI Jin, Xiaogang/0000-0001-7339-2920
FU National Natural Science Foundation of China [62036010]; Key Research
   and Development Program of Zhejiang Province [2020C03096]; Ningbo Major
   Special Projects of the "Science and Technology Innovation 2025"
   [2020Z007]
FX Xiaogang Jin was supported by the National Natural Science Foundation of
   China (Grant No. 62036010), the Key Research and Development Program of
   Zhejiang Province (Grant No. 2020C03096), and the Ningbo Major Special
   Projects of the "Science and Technology Innovation 2025" (Grant No.
   2020Z007).
CR Alahi A, 2016, PROC CVPR IEEE, P961, DOI 10.1109/CVPR.2016.110
   [Anonymous], 1997, COMPUTER ANIMATION S, DOI DOI 10.1007/978-3-7091-6874-5_3
   Chao Q., 2021, IEEE Trans. Vis. Comput. Graphics, DOI 10.1109/TVCG.2021.3128286
   Chao QW, 2016, IEEE INT CONF ROBOT, P5230, DOI 10.1109/ICRA.2016.7487731
   Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236
   Dowsland K.A., 2012, HDB NATURAL COMPUTIN, P1623, DOI DOI 10.1007/978-3-540-92910-9_49
   Guy SJ, 2012, PHYS REV E, V85, DOI 10.1103/PhysRevE.85.016110
   He L, 2016, ARXIV PREPRINT ARXIV
   Helbing D, 2000, NATURE, V407, P487, DOI 10.1038/35035023
   HELBING D, 1995, PHYS REV E, V51, P4282, DOI 10.1103/PhysRevE.51.4282
   Ju E, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866162
   Karamouzas I, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275079
   Karamouzas I, 2014, PHYS REV LETT, V113, DOI 10.1103/PhysRevLett.113.238701
   Karamouzas I, 2012, IEEE T VIS COMPUT GR, V18, P394, DOI 10.1109/TVCG.2011.133
   Kim Manmyung, 2012, P ACM SIGGRAPH EUROG, P117
   Kim S, 2016, P IEEE VIRT REAL ANN, P29, DOI 10.1109/VR.2016.7504685
   Lee JC, 2018, PROCEEDINGS OF 2018 2ND INTERNATIONAL CONFERENCE ON SOFTWARE AND E-BUSINESS (ICSEB 2018), P1, DOI 10.1145/3301761.3301762
   Lemercier S, 2012, COMPUT GRAPH FORUM, V31, P489, DOI 10.1111/j.1467-8659.2012.03028.x
   Lerner A, 2007, COMPUT GRAPH FORUM, V26, P655, DOI 10.1111/j.1467-8659.2007.01089.x
   Li Y., 2012, Eurographics Symposium on Computer Animation, P201
   Moussaïd M, 2010, PLOS ONE, V5, DOI 10.1371/journal.pone.0010047
   Musse SR, 2001, IEEE T VIS COMPUT GR, V7, P152, DOI 10.1109/2945.928167
   Narain R, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618468
   Ondrej J, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778860
   Patil S, 2011, IEEE T VIS COMPUT GR, V17, P244, DOI 10.1109/TVCG.2010.33
   Pedica C, 2008, LECT NOTES COMPUT SC, V5208, P104
   Pelechano N, 2007, SYMPOSIUM ON COMPUTER ANIMATION 2007: ACM SIGGRAPH/ EUROGRAPHICS SYMPOSIUM PROCEEDINGS, P99
   Pellegrini S, 2010, LECT NOTES COMPUT SC, V6311, P452, DOI 10.1007/978-3-642-15549-9_33
   Qiu FS, 2010, SIMUL MODEL PRACT TH, V18, P190, DOI 10.1016/j.simpat.2009.10.005
   Ren JP, 2021, IEEE T VIS COMPUT GR, V27, P1953, DOI 10.1109/TVCG.2019.2946769
   Ren Z, 2017, COMPUT GRAPH FORUM, V36, P45, DOI 10.1111/cgf.12993
   Treuille A, 2006, ACM T GRAPHIC, V25, P1160, DOI 10.1145/1141911.1142008
   van den Berg J, 2008, IEEE INT CONF ROBOT, P1928, DOI 10.1109/ROBOT.2008.4543489
   van den Berg J, 2011, SPRINGER TRAC ADV RO, V70, P3
   van Toll Wouter, 2020, S INT 3D GRAPH GAM
   Vemula A, 2018, IEEE INT CONF ROBOT, P4601
   Wolinski D, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2982442
NR 37
TC 5
Z9 5
U1 4
U2 15
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2022
VL 38
IS 9-10
SI SI
BP 3499
EP 3512
DI 10.1007/s00371-022-02556-5
EA JUL 2022
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 4N3NE
UT WOS:000819705300001
OA Bronze
DA 2024-07-18
ER

PT J
AU Shen, C
   Wanat, R
   Yoo, JJ
   Jang, J
   Fairchild, MD
AF Shen, Che
   Wanat, Robert
   Yoo, Jang Jin
   Jang, Junwoo
   Fairchild, Mark D.
TI Measuring and modeling display observer metamerism
SO VISUAL COMPUTER
LA English
DT Article
DE Observer metamerism; Metric; Individual color matching function
AB Observer metamerism refers to a situation in which some observers with normal color vision will see two colors as identical while others will see them mismatch. In recent years, some issues with metameric failure have been exacerbated by displays with more saturated primaries and narrower emission spectra. Classic trichromatic colorimetry is incapable of predicting this effect because of the typical reliance on a single standard observer-a single color matching function (CMF) averaged across the population. In this paper, we present a new experiment aimed at measuring the amount of metameric failure of a single display for real observers without comparisons to other reference displays. We also propose a recommended metric of observer metamerism failure potential that uses simulated observers, represented as individual CMFs that match the distribution of color sensitivities in the general population, and compare with several other procedures.
C1 [Shen, Che] Rochester Inst Technol, Munsell Color Sci Lab, Rochester, NY 14623 USA.
   [Fairchild, Mark D.] Rochester Inst Technol, Rochester, NY 14623 USA.
   [Wanat, Robert] LGE Zenith, Glenview, IL USA.
   [Yoo, Jang Jin] LG Display Co Ltd, Seoul, South Korea.
   [Jang, Junwoo] LG Display Co Ltd, Image Qual Dev Dept, Seoul, South Korea.
C3 Rochester Institute of Technology; Rochester Institute of Technology; LG
   Display; LG Display
RP Shen, C (corresponding author), Rochester Inst Technol, Munsell Color Sci Lab, Rochester, NY 14623 USA.
EM cs7607@g.rit.edu
OI Shen, Che/0000-0003-0339-3260
CR [Anonymous], 1866, Treatise on Physiological Optics
   Asano Y, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0145671
   Berns R.S., 2019, BILLMEYER SALTZMANS, P157, DOI DOI 10.1002/9781119367314
   CIE, 2006, Technical report 170-1
   *CIE, 1989, CIE PUBL
   David A, 2015, OPT EXPRESS, V23, P15888, DOI 10.1364/OE.23.015888
   Day EA, 2004, COLOR RES APPL, V29, P365, DOI 10.1002/col.20046
   Fairchild MD, 2007, FIFTEENTH COLOR IMAGING CONFERENCE: COLOR SCIENCE AND ENGINEERING SYSTEMS, TECHNOLOGIES, AND APPLICATIONS, FINAL PROGRAM AND PROCEEDINGS, P151
   Fairchild MD, 2018, COLOR RES APPL, V43, P804, DOI 10.1002/col.22261
   Fedutina M, 2011, NINETEENTH COLOR AND IMAGING CONFERENCE: COLOR SCIENCE AND ENGINEERING SYSTEMS, TECHNOLOGIES, AND APPLICATIONS, P2
   Hering E., 1964, OUTLINES THEORY LIGH
   Long DavidL., 2014, Journal for Imaging Science and Technology, V58, P14
   NAYATANI Y, 1985, COLOR RES APPL, V10, P147, DOI 10.1002/col.5080100303
   Park Y, 2021, J SOC INF DISPLAY, V29, P704, DOI 10.1002/jsid.1027
   TAKAHAMA K, 1985, COLOR RES APPL, V10, P106, DOI 10.1002/col.5080100209
   Webster MA, 2000, J OPT SOC AM A, V17, P1545, DOI 10.1364/JOSAA.17.001545
   Wuerger SM, 2005, VISION RES, V45, P3210, DOI 10.1016/j.visres.2005.06.016
   Xie H, 2020, J OPT SOC AM A, V37, pA61, DOI 10.1364/JOSAA.382228
NR 18
TC 1
Z9 1
U1 3
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2022
VL 38
IS 9-10
SI SI
BP 3301
EP 3310
DI 10.1007/s00371-022-02546-7
EA JUN 2022
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 4N3NE
UT WOS:000814494400002
DA 2024-07-18
ER

PT J
AU Yang, JQ
   Fan, SC
   Huang, ZQ
   Quan, SW
   Wang, W
   Zhang, YN
AF Yang, Jiaqi
   Fan, Shichao
   Huang, Zhiqiang
   Quan, Siwen
   Wang, Wei
   Zhang, Yanning
TI VOID: 3D object recognition based on voxelization in invariant distance
   space
SO VISUAL COMPUTER
LA English
DT Article
DE 3D point cloud; Object recognition; Feature matching; Local feature
   descriptor; Normal sign ambiguity
ID LOCAL GEOMETRIC FEATURES; MATCHING ALGORITHM; SEGMENTATION;
   REGISTRATION; INFORMATION; HISTOGRAMS; IMAGES
AB Recognizing 3D objects based on local feature descriptors, in point cloud scenes with occlusion and clutter, is a very challenging task. Most existing 3D local feature descriptors rely on normal information to encode local features, however, they ignore the normal-sign-ambiguity issue, which greatly limits their descriptiveness and robustness. This paper proposes a method called VOxelization in Invariant Distance space for 3D object recognition. First, we propose a VOID descriptor that is invariant to normal-sign-ambiguity, and is also rotation-invariant, distinctive, robust, and efficient. Second, a VOID-based 3D object recognition method considering the self-similarity between local features is proposed to enhance the recognition performance. Five standard datasets are employed to validate our proposed method as well as comparison with the state-of-the-arts. The results suggest that: (1) VOID descriptor is invariant to normal-sign-ambiguity, distinctive, and robust; (2) VOID-based 3D object recognition achieves outstanding recognition performance, i.e., 99.47%, 93.07% and 99.18%, on the U3OR, Queen's and Ca' Foscari Venezia datasets, respectively.
C1 [Yang, Jiaqi; Fan, Shichao; Huang, Zhiqiang] Northwestern Polytech Univ, Ningbo Inst, 218 Qingyi Rd, Ningbo 315103, Peoples R China.
   [Yang, Jiaqi; Fan, Shichao; Huang, Zhiqiang; Zhang, Yanning] Northwestern Polytech Univ, Sch Comp Sci, Xian 710129, Peoples R China.
   [Yang, Jiaqi; Fan, Shichao; Huang, Zhiqiang; Zhang, Yanning] Natl Engn Lab Integrated AeroSp Ground Ocean Big, Xian 710129, Peoples R China.
   [Quan, Siwen] Changan Univ, Sch Elect & Control, Xian 710064, Peoples R China.
   [Wang, Wei] State Key Lab Rail Transit Engn Informatizat FSDI, Xian 710043, Peoples R China.
C3 Northwestern Polytechnical University; Northwestern Polytechnical
   University; Chang'an University
RP Wang, W (corresponding author), State Key Lab Rail Transit Engn Informatizat FSDI, Xian 710043, Peoples R China.
EM zdssww@fsdi.com.cn
RI Quan, Siwen/JWQ-3810-2024
OI Quan, Siwen/0000-0001-7579-937X
FU National Natural Science Foundation of China [62002295, 62006025];
   Ningbo Natural Science Foundation [202003N4058]; China Postdoctoral
   Science Foundation [2020M673319]; Natural Science Basic Research Plan in
   Shaanxi Province of China [2021JQ-290, 2020JQ-210]; State Key Laboratory
   of Rail Transit Engineering Informatization (FSDI) [SKLKZ21-02];
   Fundamental Research Funds for the Central Universities [3102019QD1002]
FX This work is supported in part by the National Natural Science
   Foundation of China (No. 62002295 and 62006025), the Ningbo Natural
   Science Foundation (No. 202003N4058), the China Postdoctoral Science
   Foundation (No. 2020M673319), the Natural Science Basic Research Plan in
   Shaanxi Province of China (No. 2021JQ-290 and 2020JQ-210), the State Key
   Laboratory of Rail Transit Engineering Informatization (FSDI) [Contract
   No. SKLKZ21-02], and the Fundamental Research Funds for the Central
   Universities (No. 3102019QD1002).
CR Ao S, 2021, PROC CVPR IEEE, P11748, DOI 10.1109/CVPR46437.2021.01158
   Bariya P., 2010, Proc. of IEEE International Conference on Computer Vision and Pattern Recognition CVPR'10, P1657, DOI DOI 10.1109/CVPR.2010.5539774
   Bariya P, 2012, INT J COMPUT VISION, V99, P232, DOI 10.1007/s11263-012-0526-7
   Bayramoglu Neslihan, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P352, DOI 10.1109/ICPR.2010.95
   BERKMANN J, 1994, IEEE T PATTERN ANAL, V16, P1114, DOI 10.1109/34.334391
   Bu SH, 2014, VISUAL COMPUT, V30, P867, DOI 10.1007/s00371-014-0970-1
   Deng HW, 2018, LECT NOTES COMPUT SC, V11209, P620, DOI 10.1007/978-3-030-01228-1_37
   Deng HW, 2018, PROC CVPR IEEE, P195, DOI 10.1109/CVPR.2018.00028
   Flint A, 2008, IET COMPUT VIS, V2, P208, DOI 10.1049/iet-cvi:20080037
   Frome A, 2004, LECT NOTES COMPUT SC, V3023, P224
   Fu C., ARXIV PREPRINT ARXIV, DOI [10.48550/arXiv.2202.06028, DOI 10.48550/ARXIV.2202.06028]
   Funkhouser T, 2003, ACM T GRAPHIC, V22, P83, DOI 10.1145/588272.588279
   Gojcic Z, 2019, PROC CVPR IEEE, P5540, DOI 10.1109/CVPR.2019.00569
   Guo YL, 2014, IEEE T PATTERN ANAL, V36, P2270, DOI 10.1109/TPAMI.2014.2316828
   Guo YL, 2013, INT J COMPUT VISION, V105, P63, DOI 10.1007/s11263-013-0627-y
   Horn A, 1954, Amer. J. Math., V76, P620, DOI DOI 10.2307/2372705
   Huang SY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6515, DOI 10.1109/ICCV48922.2021.00647
   Johnson AE, 1999, IEEE T PATTERN ANAL, V21, P433, DOI 10.1109/34.765655
   Klasing K, 2009, IEEE INT CONF ROBOT, P2011
   Li LX, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3478513.3480568
   Lim J, 2019, VISUAL COMPUT, V35, P71, DOI 10.1007/s00371-017-1453-y
   Liu HS, 2019, PATTERN RECOGN, V92, P135, DOI 10.1016/j.patcog.2019.03.025
   Liu Z, 2020, COMPUT AIDED DESIGN, V127, DOI 10.1016/j.cad.2020.102857
   Malassiotis S, 2007, IEEE T PATTERN ANAL, V29, P1285, DOI 10.1109/TPAMI.2007.1060
   Mian A, 2010, INT J COMPUT VISION, V89, P348, DOI 10.1007/s11263-009-0296-z
   Mian AS, 2006, IEEE T PATTERN ANAL, V28, P1584, DOI 10.1109/TPAMI.2006.213
   Mian AS, 2006, INT J COMPUT VISION, V66, P19, DOI 10.1007/s11263-005-3221-0
   Novatnack J, 2008, LECT NOTES COMPUT SC, V5304, P440, DOI 10.1007/978-3-540-88690-7_33
   Osada R, 2002, ACM T GRAPHIC, V21, P807, DOI 10.1145/571647.571648
   Paquet E, 2000, SIGNAL PROCESS-IMAGE, V16, P103, DOI 10.1016/S0923-5965(00)00020-5
   Petrelli A, 2011, IEEE I CONF COMP VIS, P2244, DOI 10.1109/ICCV.2011.6126503
   Qi C., 2017, NeurIPS, P5105, DOI [10.1109/ CVPR.2017.16, DOI 10.1109/CVPR.2017.16]
   Qi CR, 2017, ADV NEUR IN, V30
   Que ZZ, 2021, PROC CVPR IEEE, P6038, DOI 10.1109/CVPR46437.2021.00598
   Rusu RB, 2008, 2008 IEEE/RSJ INTERNATIONAL CONFERENCE ON ROBOTS AND INTELLIGENT SYSTEMS, VOLS 1-3, CONFERENCE PROCEEDINGS, P3384, DOI 10.1109/IROS.2008.4650967
   Rusu RB, 2010, IEEE INT C INT ROBOT, P2155, DOI 10.1109/IROS.2010.5651280
   Rusu RB, 2009, IEEE INT CONF ROBOT, P1848
   Shang LM, 2010, INT J COMPUT VISION, V89, P211, DOI 10.1007/s11263-009-0276-3
   Steder B, 2011, IEEE INT CONF ROBOT, P2601, DOI 10.1109/ICRA.2011.5980187
   Taati B, 2007, IEEE I CONF COMP VIS, P15
   Taati B, 2011, COMPUT VIS IMAGE UND, V115, P681, DOI 10.1016/j.cviu.2010.11.021
   Tao WY, 2021, IEEE T GEOSCI REMOTE, V59, P801, DOI 10.1109/TGRS.2020.2998683
   Tombari F, 2013, INT J COMPUT VISION, V102, P198, DOI 10.1007/s11263-012-0545-4
   Tombari F, 2010, LECT NOTES COMPUT SC, V6313, P356, DOI 10.1007/978-3-642-15558-1_26
   Yang JQ, 2020, INFORM FUSION, V61, P24, DOI 10.1016/j.inffus.2020.03.008
   Yang JQ, 2019, IEEE T CIRC SYST VID, V29, P714, DOI 10.1109/TCSVT.2018.2813083
   Yang JQ, 2018, IEEE T IMAGE PROCESS, V27, P3766, DOI 10.1109/TIP.2018.2827330
   Yang JQ, 2017, COMPUT VIS IMAGE UND, V160, P133, DOI 10.1016/j.cviu.2017.02.004
   Yang JQ, 2017, PATTERN RECOGN, V65, P175, DOI 10.1016/j.patcog.2016.11.019
   Yang JQ, 2016, INFORM SCIENCES, V346, P163, DOI 10.1016/j.ins.2016.01.095
   Yang YQ, 2018, PROC CVPR IEEE, P206, DOI 10.1109/CVPR.2018.00029
   Zeng A, 2017, PROC CVPR IEEE, P199, DOI 10.1109/CVPR.2017.29
   Zhao H, 2020, PATTERN RECOGN, V103, DOI 10.1016/j.patcog.2020.107272
   Zhou W, 2019, VISUAL COMPUT, V35, P489, DOI 10.1007/s00371-018-1478-x
NR 54
TC 6
Z9 6
U1 0
U2 29
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2023
VL 39
IS 7
BP 3073
EP 3089
DI 10.1007/s00371-022-02514-1
EA JUN 2022
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA L4HX1
UT WOS:000813592000004
DA 2024-07-18
ER

PT J
AU Xie, YC
   Lin, ZH
   Yang, ZG
   Deng, H
   Wu, XC
   Mao, XD
   Li, Q
   Liu, WY
AF Xie, Yucheng
   Lin, Zehang
   Yang, Zhenguo
   Deng, Huan
   Wu, Xingcai
   Mao, Xudong
   Li, Qing
   Liu, Wenyin
TI Learning semantic alignment from image for text-guided image inpainting
SO VISUAL COMPUTER
LA English
DT Article
DE Text-guided image inpainting; Graph convolution; Generative adversarial
   networks
AB In this paper, we propose a method called LSAI (learning semantic alignment from image) to recover the corrupted image patches for text-guided image inpainting. Firstly, a multimodal preliminary (MP) module is designed to effectively encode global features for images and textual descriptions, where each local image patch and word are taken into account via multi-head self-attention. Secondly, non-Euclidean semantic relations between images and textual descriptions are captured with graph structure by building a semantic relation graph (SRG). The constructed SRG is able to obtain meaningful words describing the image content and alleviate the impact of distracting words, which is achieved by aggregating the semantic relations with graph convolution. In addition, a text-image matching loss is devised to penalize the restored images for diverse textual and visual semantics. Quantitative and qualitative experiments conducted on two public datasets show the outperformance of our proposed LSAI (e.g., FID value is reduced from 30.87 to 16.73 on CUB-200-2011 dataset).
C1 [Xie, Yucheng; Yang, Zhenguo; Deng, Huan; Wu, Xingcai; Liu, Wenyin] Guangdong Univ Technol, Sch Comp Sci, Guangzhou, Peoples R China.
   [Lin, Zehang; Li, Qing] Hong Kong Polytech Univ, Dept Comp, Hong Kong, Peoples R China.
   [Mao, Xudong] Sun Yat Sen Univ, Sch Artificial Intelligence, Guangzhou, Peoples R China.
   [Liu, Wenyin] Peng Cheng Lab, Cyberspace Secur Res Ctr, Shenzhen, Peoples R China.
C3 Guangdong University of Technology; Hong Kong Polytechnic University;
   Sun Yat Sen University; Peng Cheng Laboratory
RP Yang, ZG (corresponding author), Guangdong Univ Technol, Sch Comp Sci, Guangzhou, Peoples R China.
EM yzg@gdut.edu.cn; liuwy@gdut.edu.cn
RI lin, zehang/ABD-1029-2020; WU, XINGCAI/GWC-4186-2022; Yang,
   Zhenguo/X-9205-2019; zheng, yi/JOZ-7204-2023; Li, Qing/JMH-1365-2023
OI Yang, Zhenguo/0000-0001-7674-977X; Li, Qing/0000-0003-3370-471X
CR Chen ZM, 2019, PROC CVPR IEEE, P5172, DOI 10.1109/CVPR.2019.00532
   Cho Kyunghyun, 2014, SYNTAX SEMANTICS STR, P5, DOI [10.3115/v1/w14-4012, 10.3115 /v1/D14-1179, DOI 10.3115/V1/D14-1179]
   Doersch C., 2016, ARXIV PREPRINT ARXIV
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hensel M, 2017, ADV NEUR IN, V30
   Jingyuan Li, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7757, DOI 10.1109/CVPR42600.2020.00778
   Kipf TN, 2016, ARXIV
   Li QZ, 2019, AAAI CONF ARTIF INTE, P8634
   Lin Q, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1094, DOI 10.1145/3394171.3413982
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Mao XD, 2017, IEEE I CONF COMP VIS, P2813, DOI 10.1109/ICCV.2017.304
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Song Y, 2019, PROC CVPR IEEE, P1979, DOI 10.1109/CVPR.2019.00208
   Vaswani A, 2017, ADV NEUR IN, V30
   Wah C., 2011, CALTECH UCSD BIRDS 2
   Wang X, 2020, KDD '20: PROCEEDINGS OF THE 26TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P1243, DOI 10.1145/3394486.3403177
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Xia LH, 2021, AAAI CONF ARTIF INTE, V35, P4486
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Xu T, 2018, PROC CVPR IEEE, P1316, DOI 10.1109/CVPR.2018.00143
   Zhang LS, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1302, DOI 10.1145/3394171.3414017
   Zhao H, 2017, IEEE T COMPUT IMAG, V3, P47, DOI 10.1109/TCI.2016.2644865
   Zhao JA, 2021, AAAI CONF ARTIF INTE, V35, P4697
   Zheng CX, 2019, PROC CVPR IEEE, P1438, DOI 10.1109/CVPR.2019.00153
   Zhu MY, 2021, IEEE T IMAGE PROCESS, V30, P4855, DOI 10.1109/TIP.2021.3076310
NR 25
TC 3
Z9 3
U1 1
U2 22
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2022
VL 38
IS 9-10
SI SI
BP 3149
EP 3161
DI 10.1007/s00371-022-02523-0
EA JUN 2022
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 4N3NE
UT WOS:000805908600001
OA Bronze
DA 2024-07-18
ER

PT J
AU Bhagat, PK
   Choudhary, P
   Singh, KM
AF Bhagat, P. K.
   Choudhary, Prakash
   Singh, Kh Manglem
TI A study on zero-shot learning from semantic viewpoint
SO VISUAL COMPUTER
LA English
DT Article
DE Zero-shot learning; Semantic Space; Machine learning; ZSL datasets;
   Computer vision
ID CLASSIFICATION
AB Recognition of unseen object class by a human being is always based on the relationship between seen and unseen classes, given that human has some background knowledge of the unseen object class. Zero-shot learning is a learning paradigm that tries to develop a recognition model to recognize mutually exclusive training and testing classes. A zero-shot learning model trained on labeled data can also recognize unseen classes when sufficient information about the relationship between seen and unseen classes is given. Semantic space contains semantic information about seen and unseen classes. It is an important part of zero-shot learning and acts as a bridge between seen and unseen classes. In this article, we provide a compact and comprehensive survey on zero-shot learning. First, we explain the different ways to construct semantic space along with its pros and cons. Next, we present a categorization of zero-shot learning methods from the semantic space construction point of view. Furthermore, this paper also presents performance evaluation measures with a relevant and influential zero-shot learning database.
C1 [Bhagat, P. K.; Singh, Kh Manglem] Natl Inst Technol Manipur, Imphal 795001, Manipur, India.
   [Choudhary, Prakash] Natl Inst Technol Hamirpur, Hamirpur 177005, Himachal Prades, India.
C3 National Institute of Technology (NIT System); National Institute of
   Technology Manipur; National Institute of Technology (NIT System);
   National Institute of Technology Hamirpur
RP Bhagat, PK (corresponding author), Natl Inst Technol Manipur, Imphal 795001, Manipur, India.
EM pkbhagat@nitmanipur.ac.in; pc@nith.ac.in; manglem@gmail.com
FU Department of Computer Science and Engineering, National Institute of
   Technology Manipur; National Institute of Technology Hamirpur, India
FX The authors would also like to thank the Department of Computer Science
   and Engineering, National Institute of Technology Manipur, and National
   Institute of Technology Hamirpur, India, to provide the platform and
   equipment for the study so that authors are able to perform this study.
CR Akata Z, 2016, PROC CVPR IEEE, P59, DOI 10.1109/CVPR.2016.14
   Akata Z, 2016, IEEE T PATTERN ANAL, V38, DOI 10.1109/TPAMI.2015.2487986
   Akata Z, 2015, PROC CVPR IEEE, P2927, DOI 10.1109/CVPR.2015.7298911
   Akata Z, 2013, PROC CVPR IEEE, P819, DOI 10.1109/CVPR.2013.111
   Al-Halah Z, 2015, IEEE WINT CONF APPL, P837, DOI 10.1109/WACV.2015.116
   An FP, 2022, VISUAL COMPUT, V38, P541, DOI 10.1007/s00371-020-02033-x
   [Anonymous], 2013, ADV NEURAL INFORM PR
   [Anonymous], 2006, P 21 NAT C ART INT
   [Anonymous], 2011, P C EMP METH NAT LAN
   Ba JL, 2015, IEEE I CONF COMP VIS, P4247, DOI 10.1109/ICCV.2015.483
   Bhagat PK, 2021, J AMB INTEL HUM COMP, V12, P8647, DOI 10.1007/s12652-020-02615-6
   Bhagat PK, 2018, IMAGE VISION COMPUT, V80, P1, DOI 10.1016/j.imavis.2018.09.017
   BRADLEY DR, 1975, NATURE, V257, P582, DOI 10.1038/257582a0
   Bucher M, 2017, IEEE INT CONF COMP V, P2666, DOI 10.1109/ICCVW.2017.308
   Changpinyo S, 2016, PROC CVPR IEEE, P5327, DOI 10.1109/CVPR.2016.575
   Chao WL, 2016, LECT NOTES COMPUT SC, V9906, P52, DOI 10.1007/978-3-319-46475-6_4
   Chen K., 2013, EFFICIENT ESTIMATION, P2
   Cheng H.-T., 2013, P ACM 11 ANN INT C M, P361
   ELMAN JL, 1990, COGNITIVE SCI, V14, P179, DOI 10.1207/s15516709cog1402_1
   Farhadi A, 2009, PROC CVPR IEEE, P1778, DOI 10.1109/CVPRW.2009.5206772
   Feng JS, 2014, PROC CVPR IEEE, P1645, DOI 10.1109/CVPR.2014.213
   Forsyth D. A., 2002, Computer vision: a modern approach, DOI DOI 10.5555/580035
   Frome Andrea, 2013, DEVISE DEEP VISUAL S, P1, DOI DOI 10.5555/2999792.2999849
   Fu YW, 2020, IEEE T PATTERN ANAL, V42, P3136, DOI 10.1109/TPAMI.2019.2922175
   Fu YW, 2018, IEEE SIGNAL PROC MAG, V35, P112, DOI 10.1109/MSP.2017.2763441
   Fu YW, 2012, LECT NOTES COMPUT SC, V7575, P530, DOI 10.1007/978-3-642-33765-9_38
   Fu YW, 2014, IEEE T PATTERN ANAL, V36, P303, DOI 10.1109/TPAMI.2013.128
   Fu ZY, 2018, IEEE T PATTERN ANAL, V40, P2009, DOI 10.1109/TPAMI.2017.2737007
   Gan C, 2016, PROC CVPR IEEE, P87, DOI 10.1109/CVPR.2016.17
   Gao LL, 2015, ICMR'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P487, DOI 10.1145/2671188.2749309
   Gonzalez R. C., 2006, Digital Image Processing, V3rd
   Guo YC, 2016, AAAI CONF ARTIF INTE, P3494
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang S, 2015, PROC CVPR IEEE, P409, DOI 10.1109/CVPR.2015.7298638
   Jayaraman D, 2014, PROC CVPR IEEE, P1629, DOI 10.1109/CVPR.2014.211
   Ji Z, 2021, NEURAL NETWORKS, V143, P88, DOI 10.1016/j.neunet.2021.05.019
   Jia Z, 2020, IEEE T IMAGE PROCESS, V29, P1958, DOI 10.1109/TIP.2019.2947780
   Jiang HJ, 2017, IEEE I CONF COMP VIS, P4233, DOI 10.1109/ICCV.2017.453
   Kodirov E, 2017, PROC CVPR IEEE, P4447, DOI 10.1109/CVPR.2017.473
   Kordumova S, 2016, ICMR'16: PROCEEDINGS OF THE 2016 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P143, DOI 10.1145/2911996.2912007
   Lampert CH, 2014, IEEE T PATTERN ANAL, V36, P453, DOI 10.1109/TPAMI.2013.140
   Lampert CH, 2009, PROC CVPR IEEE, P951, DOI 10.1109/CVPRW.2009.5206594
   Lazaridou A, 2015, PROCEEDINGS OF THE 53RD ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 7TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING, VOL 1, P270
   Li FF, 2006, IEEE T PATTERN ANAL, V28, P594, DOI 10.1109/TPAMI.2006.79
   Li HH, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P1013, DOI 10.1145/2647868.2655023
   Li X, 2015, JMLR WORKSH CONF PRO, V38, P626
   Li XR, 2015, SIGIR 2015: PROCEEDINGS OF THE 38TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P879, DOI 10.1145/2766462.2767773
   Li Y, 2018, AAAI CONF ARTIF INTE, P7049
   Li Y, 2018, PROC CVPR IEEE, P7463, DOI 10.1109/CVPR.2018.00779
   Li YN, 2017, PROC CVPR IEEE, P5207, DOI 10.1109/CVPR.2017.553
   Liang KM, 2015, IEEE I CONF COMP VIS, P2506, DOI 10.1109/ICCV.2015.288
   Long Y, 2017, PROC CVPR IEEE, P6165, DOI 10.1109/CVPR.2017.653
   Mensink T, 2012, LECT NOTES COMPUT SC, V7573, P488, DOI 10.1007/978-3-642-33709-3_35
   Mikolov T, 2013, P 2013 C N AM CHAPT
   Mikolov T, 2009, INT CONF ACOUST SPEE, P4725, DOI 10.1109/ICASSP.2009.4960686
   MILLER GA, 1995, COMMUN ACM, V38, P39, DOI 10.1145/219717.219748
   Morgado P, 2017, PROC CVPR IEEE, P2037, DOI 10.1109/CVPR.2017.220
   Narayan Sanath, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12367), P479, DOI 10.1007/978-3-030-58542-6_29
   Norouzi M., 2014, P INT C LEARN REPR
   OSHERSON DN, 1991, COGNITIVE SCI, V15, P251, DOI 10.1207/s15516709cog1502_3
   Palatucci M, 2009, NEURAL INFORM PROCES, V22
   Pambala AK, 2020, IEEE WINT CONF APPL, P1226, DOI [10.1109/WACV45572.2020.9093625, 10.1109/wacv45572.2020.9093625]
   Paragios N, 2002, INT J COMPUT VISION, V46, P223, DOI 10.1023/A:1014080923068
   Parikh D, 2011, IEEE I CONF COMP VIS, P503, DOI 10.1109/ICCV.2011.6126281
   Parkkonen L, 2008, P NATL ACAD SCI USA, V105, P20500, DOI 10.1073/pnas.0810966105
   Peng PX, 2018, IEEE T PATTERN ANAL, V40, P1625, DOI 10.1109/TPAMI.2017.2723882
   Pennington J., 2014, P 2014 C EMP METH NA, P1532
   Pi T, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2599
   Qian K, 2022, VISUAL COMPUT, V38, P719, DOI 10.1007/s00371-020-02046-6
   Qiao RZ, 2016, PROC CVPR IEEE, P2249, DOI 10.1109/CVPR.2016.247
   Rastegari M, 2012, LECT NOTES COMPUT SC, V7577, P876, DOI 10.1007/978-3-642-33783-3_63
   Ravi S., 2016, INT C LEARNING REPRE
   Reed S, 2016, PROC CVPR IEEE, P49, DOI 10.1109/CVPR.2016.13
   Renault O., 1990, Journal of Visualization and Computer Animation, V1, P18, DOI 10.1002/vis.4340010106
   Rifai S, 2012, LECT NOTES COMPUT SC, V7577, P808, DOI 10.1007/978-3-642-33783-3_58
   Rohrbach M., 2013, Advances in neural information processing systems, P46
   Rohrbach M, 2011, PROC CVPR IEEE, P1641, DOI 10.1109/CVPR.2011.5995627
   Rohrbach M, 2010, PROC CVPR IEEE, P910, DOI 10.1109/CVPR.2010.5540121
   Romera-Paredes B, 2017, ADV COMPUT VIS PATT, P11, DOI 10.1007/978-3-319-50077-5_2
   Rumelhart David E., 1986, Parallel Distributed Processing: Explorations in the Microstructure of Cognition, V1, P318
   Sariyildiz MB, 2019, PROC CVPR IEEE, P2163, DOI 10.1109/CVPR.2019.00227
   Schönfeld E, 2019, PROC CVPR IEEE, P8239, DOI 10.1109/CVPR.2019.00844
   Schüldt C, 2004, INT C PATT RECOG, P32, DOI 10.1109/ICPR.2004.1334462
   Shimojo S, 2001, P NATL ACAD SCI USA, V98, P12340, DOI 10.1073/pnas.221383698
   Singh S, 2012, LECT NOTES COMPUT SC, V7573, P73, DOI 10.1007/978-3-642-33709-3_6
   Soomro Khurram, 2012, UCF101 DATASET 101 H
   Su Y, 2012, INT J COMPUT VISION, V100, P59, DOI 10.1007/s11263-012-0529-4
   Sun XH, 2021, APPL INTELL, V51, P3600, DOI 10.1007/s10489-020-02075-7
   Tsochantaridis I, 2005, J MACH LEARN RES, V6, P1453
   Verma VK, 2017, LECT NOTES ARTIF INT, V10535, P792, DOI 10.1007/978-3-319-71246-8_48
   Wang W, 2019, ACM T INTEL SYST TEC, V10, DOI 10.1145/3293318
   Wang XY, 2013, IEEE I CONF COMP VIS, P2120, DOI 10.1109/ICCV.2013.264
   Welinder P., 2010, Technical Report CNS-TR-2010-001
   Xian YQ, 2018, PROC CVPR IEEE, P5542, DOI 10.1109/CVPR.2018.00581
   Xian YQ, 2019, IEEE T PATTERN ANAL, V41, P2251, DOI 10.1109/TPAMI.2018.2857768
   Xian YQ, 2017, PROC CVPR IEEE, P3077, DOI 10.1109/CVPR.2017.328
   Xian YQ, 2016, PROC CVPR IEEE, P69, DOI 10.1109/CVPR.2016.15
   Xie GS, 2019, PROC CVPR IEEE, P9376, DOI 10.1109/CVPR.2019.00961
   Xu Wenjia, 2020, Attribute Prototype Network for Zero-Shot Learning
   Xu X, 2017, PROC CVPR IEEE, P2007, DOI 10.1109/CVPR.2017.217
   Yu FLX, 2013, PROC CVPR IEEE, P771, DOI 10.1109/CVPR.2013.105
   Yue ZQ, 2021, PROC CVPR IEEE, P15399, DOI 10.1109/CVPR46437.2021.01515
   Zhang L, 2017, PROC CVPR IEEE, P3010, DOI 10.1109/CVPR.2017.321
   Zhang Y, 2010, INT J MACH LEARN CYB, V1, P43, DOI 10.1007/s13042-010-0001-0
   Zhang ZM, 2015, IEEE I CONF COMP VIS, P4166, DOI 10.1109/ICCV.2015.474
   Zhao A., 2018, DOMAIN INVARIANT PRO
   Zhao B, 2017, IEEE INT CONF COMP V, P2616, DOI 10.1109/ICCVW.2017.310
   Zhu Y., 2019, SEMANTIC GUIDED MULT
   Zhu YZ, 2018, PROC CVPR IEEE, P1004, DOI 10.1109/CVPR.2018.00111
NR 109
TC 3
Z9 3
U1 14
U2 48
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2023
VL 39
IS 5
BP 2149
EP 2163
DI 10.1007/s00371-022-02470-w
EA MAY 2022
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA D7ZK3
UT WOS:000802898700003
DA 2024-07-18
ER

PT J
AU Qiao, NZ
   Di, LM
AF Qiao, Nianzu
   Di, Lamei
TI Underwater image enhancement combining low-dimensional and global
   features
SO VISUAL COMPUTER
LA English
DT Article
DE Underwater image enhancement; Low-dimensional; Global; Loss function
AB The physical transformation of light will cause the quality of underwater images to decrease, which impacts the precision of object detection, recognition, and segmentation in underwater circumstances. In this study, we suggest an underwater image enhancement combining low-dimensional and global features (UIELG). This model can availably heighten the texture minutiae and global key features of subaquatic images. Moreover, we advise a new loss function, which can effectively boost the structure and texture similarity of underwater images. Finally, we train and test the model on the synthetic subaquatic images. The experimental outcomes declare that this model is preferable to the existing models in both SSIM and PSNR scores. And the experimental outcomes on the real-world subaquatic image dataset present the generalization and robustness of the suggested model.
C1 [Qiao, Nianzu] Tongji Univ, Coll Elect & Informat Engn, 4800 Caoan Highway, Shanghai 201804, Peoples R China.
   [Di, Lamei] Xi An Jiao Tong Univ, 28 West Xianning Rd, Xian 710049, Shaanxi, Peoples R China.
C3 Tongji University; Xi'an Jiaotong University
RP Qiao, NZ (corresponding author), Tongji Univ, Coll Elect & Informat Engn, 4800 Caoan Highway, Shanghai 201804, Peoples R China.
EM 2011613@tongji.edu.cn
CR Akkaynak D, 2019, PROC CVPR IEEE, P1682, DOI 10.1109/CVPR.2019.00178
   [Anonymous], 1976, Marine Optics
   Bhandari AK, 2020, IEEE T FUZZY SYST, V28, P2009, DOI 10.1109/TFUZZ.2019.2930028
   Chiang JY, 2012, IEEE T IMAGE PROCESS, V21, P1756, DOI 10.1109/TIP.2011.2179666
   Drews PLJ, 2016, IEEE COMPUT GRAPH, V36, P24, DOI 10.1109/MCG.2016.26
   Forrest N. L., 2014, PREPRINTS
   Fu XY, 2020, SIGNAL PROCESS-IMAGE, V86, DOI 10.1016/j.image.2020.115892
   Galdran A, 2015, J VIS COMMUN IMAGE R, V26, P132, DOI 10.1016/j.jvcir.2014.11.006
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Islam MJ, 2020, IEEE ROBOT AUTOM LET, V5, P3227, DOI 10.1109/LRA.2020.2974710
   Lee HS, 2020, SYMMETRY-BASEL, V12, DOI 10.3390/sym12081220
   Li CY, 2016, IEEE T IMAGE PROCESS, V26, P5664, DOI 10.1109/TIP.2016.2612882
   Li CY, 2020, IEEE T IMAGE PROCESS, V29, P4376, DOI 10.1109/TIP.2019.2955241
   Li CY, 2020, PATTERN RECOGN, V98, DOI 10.1016/j.patcog.2019.107038
   Li J, 2018, IEEE ROBOT AUTOM LET, V3, P387, DOI 10.1109/LRA.2017.2730363
   Liu ST, 2018, LECT NOTES COMPUT SC, V11215, P404, DOI 10.1007/978-3-030-01252-6_24
   Peng YT, 2017, IEEE T IMAGE PROCESS, V26, P1579, DOI 10.1109/TIP.2017.2663846
   Pritish, 2019, P IEEECVF C COMPUTER, P18
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54
   Song W, 2020, IEEE T BROADCAST, V66, P153, DOI 10.1109/TBC.2019.2960942
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Yang H.H., 2021, PREPRINTS
   Zhang JW, 2010, VISUAL COMPUT, V26, P761, DOI 10.1007/s00371-010-0444-z
   Zhao H, 2017, IEEE T COMPUT IMAG, V3, P47, DOI 10.1109/TCI.2016.2644865
   Zhou DY, 2022, VISUAL COMPUT, V38, P119, DOI 10.1007/s00371-020-02007-z
NR 26
TC 3
Z9 3
U1 1
U2 22
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2023
VL 39
IS 7
BP 3029
EP 3039
DI 10.1007/s00371-022-02510-5
EA MAY 2022
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA L4HX1
UT WOS:000800836300002
DA 2024-07-18
ER

PT J
AU Wan, CL
   Cao, JZ
   Wei, XH
   Huang, JQ
   Chen, ZM
   Xu, DM
   Qiu, F
AF Wan, Changlin
   Cao, Jianzhong
   Wei, Xiaohui
   Huang, Jingqiu
   Chen, Zhiming
   Xu, Deming
   Qiu, Feng
TI IPCS: An improved corner detector with intensity, pattern, curvature,
   and scale
SO VISUAL COMPUTER
LA English
DT Article
DE Corner detection; First-order derivative; Second-order derivative;
   8-neighbor pixel blocks; Corner pattern; Corner curvature; Scale
   importance factor
ID COMPUTATIONAL APPROACH; ROBUST
AB The corner detection plays an important role in the area of image processing and computer vision. The current corner detection methods often utilize few cues or single model to improve the detection correctness and repeatability. A composite model of both intensity, pattern, curvature, and scale is proposed as a possible solution to these problems. Firstly, a corner measure function that reflects both the intensity, pattern, and curvature difference is formulated based on the 8-neighbor pixel blocks. Secondly, some scale-based global scale importance factors are formulated based on the contour distribution and corner distribution. Thirdly, based on the corner measure and the importance factors, a high-performance corner detector (IPCS) is derived. The experiments based on both the ground truth and the standard image set are conducted to evaluate the correctness and repeatability of the proposed detector. The experiment results come up with that the proposed detector has remarkable performance advantages among the comprising state-of-the-art detectors.
C1 [Wan, Changlin; Cao, Jianzhong; Wei, Xiaohui; Huang, Jingqiu; Chen, Zhiming; Xu, Deming] Huizhou Univ, Sch Elect Informat & Elect Engn, Huizhou 516007, Guangdong, Peoples R China.
   [Qiu, Feng] Jiangxi Inst Comp Technol, Nanchang 330003, Jiangxi, Peoples R China.
C3 Huizhou University
RP Wan, CL (corresponding author), Huizhou Univ, Sch Elect Informat & Elect Engn, Huizhou 516007, Guangdong, Peoples R China.
EM wancl@hzu.edu.cn; cjz@hzu.edu.cn; weixh0509@hzu.edu.cn; hjq@hzu.edu.cn;
   czm@hzu.edu.cn; xdm@hzu.edu.cn; qf@jict.org
FU Program of Huizhou City Science and Technology Plan [2017C0409025]
FX The authors would like to thank the support of the Program of Huizhou
   City Science and Technology Plan No. 2017C0409025.
CR Ardizzone E, 2013, PATTERN RECOGN LETT, V34, P2071, DOI 10.1016/j.patrec.2013.06.018
   Awrangjeb M, 2008, IEEE T IMAGE PROCESS, V17, P2425, DOI 10.1109/TIP.2008.2006441
   Awrangjeb M, 2008, IEEE T MULTIMEDIA, V10, P1059, DOI 10.1109/TMM.2008.2001384
   Biedl T, 2014, COMP GEOM-THEOR APPL, V47, P282, DOI 10.1016/j.comgeo.2013.08.005
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851
   Chen ST, 2016, NEUROCOMPUTING, V173, P434, DOI 10.1016/j.neucom.2015.01.102
   Debelee TG, 2019, COMPUT VIS MEDIA, V5, P347, DOI 10.1007/s41095-019-0151-2
   DERICHE R, 1993, INT J COMPUT VISION, V10, P101, DOI 10.1007/BF01420733
   Gueguen L, 2011, PATTERN RECOGN LETT, V32, P1714, DOI 10.1016/j.patrec.2011.07.021
   Guillon S., 1998, 9 EUROPEAN SIGNAL PR, P14
   Harris C., 1988, P 4 ALV VIS C, V15, P10
   He XC, 2004, INT C PATT RECOG, P791, DOI 10.1109/ICPR.2004.1334377
   Kimia BB, 2019, IEEE T PATTERN ANAL, V41, P1573, DOI 10.1109/TPAMI.2018.2846268
   Laligant O, 2010, IEEE T PATTERN ANAL, V32, P242, DOI 10.1109/TPAMI.2008.282
   Mikolajczyk K, 2004, INT J COMPUT VISION, V60, P63, DOI 10.1023/B:VISI.0000027790.02288.f2
   Mokhtarian F, 1998, IEEE T PATTERN ANAL, V20, P1376, DOI 10.1109/34.735812
   NOBLE JA, 1988, IMAGE VISION COMPUT, V6, P121, DOI 10.1016/0262-8856(88)90007-8
   Pan X, 2021, BIOMED SIGNAL PROCES, V63, DOI 10.1016/j.bspc.2020.102112
   Peng ZY, 2019, MACH VISION APPL, V30, P1029, DOI 10.1007/s00138-019-01035-7
   Rosten E, 2010, IEEE T PATTERN ANAL, V32, P105, DOI 10.1109/TPAMI.2008.275
   Rothfeder J.L., 2003, P C COMPUTER VISION, P30
   Shui PL, 2013, IEEE T IMAGE PROCESS, V22, P3204, DOI 10.1109/TIP.2013.2259834
   Smith SM, 1997, INT J COMPUT VISION, V23, P45, DOI 10.1023/A:1007963824710
   Xu C., 2008, 19 INT C PATT REC IC, P14
   Zhang WC, 2021, IEEE T PATTERN ANAL, V43, P1213, DOI 10.1109/TPAMI.2019.2949302
   Zhang WC, 2019, IEEE T IMAGE PROCESS, V28, P4444, DOI 10.1109/TIP.2019.2910655
   Zhang XH, 2015, IEEE T PATTERN ANAL, V37, P2207, DOI 10.1109/TPAMI.2015.2396074
NR 27
TC 3
Z9 3
U1 2
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2023
VL 39
IS 7
BP 2499
EP 2513
DI 10.1007/s00371-022-02474-6
EA MAY 2022
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA L4HX1
UT WOS:000800836300003
DA 2024-07-18
ER

PT J
AU Freitas, XG
   Diniz, R
   Farias, MCQ
AF Freitas, XPedro Garcia
   Diniz, Rafael
   Farias, Mylene C. Q.
TI Point cloud quality assessment: unifying projection, geometry, and
   texture similarity
SO VISUAL COMPUTER
LA English
DT Article
DE Point cloud; Quality assessment; Local binary pattern; Computer vision;
   Signal processing
ID SALIENCY DETECTION; COLOR
AB Methods for (PC) quality assessment customarily perform local comparisons between corresponding points in the "degraded" and pristine PCs. These methods often compare the geometry of the degraded PC and the geometry of the reference PC. More recently, a few methods that use texture information to assess the PC quality have been proposed. In this work, we propose a full-reference Point Cloud Quality Assessment (PCQA) metric that combines both geometry and texture information to provide an estimate of the PC quality. We use a projection technique that represents PCs as 2D manifolds in the 3D space. This technique maps attributes from the PCs onto the folded 2D grid, generating a pure-texture 2D image (texture maps) that contains PC texture information. Then, we extract statistical features from these texture maps using a multi-scale rotation-invariant texture descriptor named the Dominant Rotated Local Binary Pattern (DRLBP). The texture similarity is computed by measuring the statistical differences between reference and test PCs. The geometrical similarities are computed using geometry-only distances. Finally, the texture and geometrical similarities are fused using a stacked regressor to model the PC visual quality. Experimental results show that the proposed method outperforms several state-of-the-art methods. An implementation of the metric described in this paper can be found at https://gitlab.com/gpds-unb/pc-gats-metric.
C1 [Freitas, XPedro Garcia; Diniz, Rafael] Univ Brasilia UnB, Dept Comp Sci, Brasilia, DF, Brazil.
   [Farias, Mylene C. Q.] Univ Brasilia UnB, Dept Elect Engn, Brasilia, DF, Brazil.
C3 Universidade de Brasilia; Universidade de Brasilia
RP Freitas, XG (corresponding author), Univ Brasilia UnB, Dept Comp Sci, Brasilia, DF, Brazil.
EM pedrogarcia@ieee.org
RI Garcia Freitas, Pedro/T-5287-2018; Farias, Mylene/C-4900-2015
OI Garcia Freitas, Pedro/0000-0003-0866-658X; Farias,
   Mylene/0000-0002-1957-9943
FU Fundacao de Apoio a Pesquisa do Distrito Federal (FAP-DF); Coordenacao
   de Aperfeicoamento de Pessoal de Nivel Superior (CAPES); Conselho
   Nacional de Desenvolvimento Cientifico e Tecnologico (CNPq); University
   of Brasilia (UnB)
FX This work was supported by the FundacAo de Apoio a Pesquisa do Distrito
   Federal (FAP-DF), by the CoordenacAo de Aperfeicoamento de Pessoal de
   Nivel Superior (CAPES), the Conselho Nacional de Desenvolvimento
   Cientifico e Tecnologico (CNPq), and by the University of Brasilia
   (UnB). We are grateful to Prof. Luis Cruz, Evangelos Alexiou, Tiago
   Fonseca, Prof. Ricardo de Queiroz, and Prof. Touradj Ebrahimi for
   sharing the subjective scores and datasets used in this work.
CR Alexander E, 2021, PUBLIC HEALTH NUTR, V24, P1542, DOI 10.1017/S1368980020003961
   Alexiou E, 2018, INT WORK QUAL MULTIM, P132
   Alexiou E, 2019, APSIPA TRANS SIGNAL, V8, DOI 10.1017/ATSIP.2019.20
   Breiman L, 1996, MACH LEARN, V24, P49
   Chang WC, 2020, VISUAL COMPUT, V36, P593, DOI 10.1007/s00371-019-01642-5
   Diniz R., COMPUT GRAPH-UK, V103, P31
   Diniz R., 2021, Electronic Imaging, V2021
   Diniz R, 2021, IEEE SIGNAL PROC LET, V28, P1150, DOI 10.1109/LSP.2021.3088059
   Diniz R, 2020, IEEE IMAGE PROC, P3443, DOI 10.1109/ICIP40778.2020.9190956
   Diniz R, 2020, INT WORK QUAL MULTIM, DOI 10.1109/qomex48832.2020.9123076
   Fernandes FP, 2018, INT J POLYM SCI, V2018, DOI 10.1155/2018/2403802
   Freitas PG, 2018, PROCEEDINGS OF THE 9TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'18), P52, DOI 10.1145/3204949.3204960
   Freitas PG, 2018, SIGNAL PROCESS-IMAGE, V64, P1, DOI 10.1016/j.image.2018.02.010
   Freitas PG, 2016, 2016 EIGHTH INTERNATIONAL CONFERENCE ON QUALITY OF MULTIMEDIA EXPERIENCE (QOMEX)
   Greenwell BM, 2020, R J, V12, P343
   Guo Y, 2018, VISUAL COMPUT, V34, P1325, DOI 10.1007/s00371-017-1416-3
   Hu L, 2020, VISUAL COMPUT, V36, P669, DOI 10.1007/s00371-019-01648-z
   Javaheri A, 2020, INT WORK QUAL MULTIM, DOI 10.1109/qomex48832.2020.9123087
   Javaheri A, 2020, IEEE SIGNAL PROC LET, V27, P1350, DOI 10.1109/LSP.2020.3010128
   Jurman G., 2009, P ADV RANK NIPS 09 W, P22
   Li HT, 2022, VISUAL COMPUT, V38, P1759, DOI 10.1007/s00371-021-02103-8
   Mehta R, 2016, PATTERN RECOGN LETT, V71, P16, DOI 10.1016/j.patrec.2015.11.019
   Meynet G, 2020, INT WORK QUAL MULTIM, DOI 10.1109/qomex48832.2020.9123147
   Meynet G, 2019, INT WORK QUAL MULTIM
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Oyman O., 2019, 1SC29WG11 ISOIEC
   Pedregosa F, 2011, J MACH LEARN RES, V12, P2825
   Peixoto E, 2020, IEEE SIGNAL PROC LET, V27, P246, DOI 10.1109/LSP.2020.2965322
   Quach M, 2020, IEEE IMAGE PROC, P3309, DOI 10.1109/ICIP40778.2020.9191180
   Ramdas A, 2017, ENTROPY-SWITZ, V19, DOI 10.3390/e19020047
   Sammut C., 2011, Leave-One-Out Cross-Validation, DOI DOI 10.1007/978-0-387-30164-8469
   Sandri GP, 2019, IEEE SIGNAL PROC LET, V26, P1369, DOI 10.1109/LSP.2019.2931425
   Schwarz S., 2018, ISO/IEC JTC1/SC29/WG11
   Schwarz S, 2019, IEEE J EM SEL TOP C, V9, P133, DOI 10.1109/JETCAS.2018.2885981
   Shi JF, 2021, VISUAL COMPUT, V37, P1277, DOI 10.1007/s00371-020-01865-x
   Sing SL, 2021, VIRTUAL PHYS PROTOTY, V16, P372, DOI 10.1080/17452759.2021.1944229
   Teng CH, 2018, VISUAL COMPUT, V34, P1507, DOI 10.1007/s00371-017-1425-2
   Torlig EM, 2018, PROC SPIE, V10752, DOI 10.1117/12.2322741
   Viola I, 2020, INT WORK QUAL MULTIM, DOI 10.1109/qomex48832.2020.9123089
   Wong TT, 2017, IEEE T KNOWL DATA EN, V29, P2417, DOI 10.1109/TKDE.2017.2740926
   Xu M, 2015, VISUAL COMPUT, V31, P355, DOI 10.1007/s00371-014-0930-9
   Yang Q, 2021, IEEE T MULTIMEDIA, V23, P3877, DOI 10.1109/TMM.2020.3033117
   Yang Qi, 2022, IEEE Trans Pattern Anal Mach Intell, V44, P3015, DOI 10.1109/TPAMI.2020.3047083
   Zien A, 2009, LECT NOTES ARTIF INT, V5782, P694, DOI 10.1007/978-3-642-04174-7_45
NR 44
TC 3
Z9 3
U1 3
U2 7
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2023
VL 39
IS 5
BP 1907
EP 1914
DI 10.1007/s00371-022-02454-w
EA MAR 2022
PG 8
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA D7ZK3
UT WOS:000774601300001
DA 2024-07-18
ER

PT J
AU Li, CY
   Monno, Y
   Okutomi, M
AF Li, Chunyu
   Monno, Yusuke
   Okutomi, Masatoshi
TI Pro-Cam SSfM: projector-camera system for structure and spectral
   reflectance from motion
SO VISUAL COMPUTER
LA English
DT Article
DE Projector-camera system; Structured light; Structure from motion;
   Spectral reflectance estimation
ID 3D; RECOVERY
AB Image-based reconstruction of an object's 3D shape having the wavelength-by-wavelength spectral reflectance property enables higher-fidelity object 3D modeling compared with typical RGB-based modeling. In this paper, we propose a novel projector-camera system for practical and low-cost acquisition of a dense object 3D model with the spectral reflectance property. Different from existing spectral 3D data acquisition systems that use a dedicated multispectral camera or light, we use a standard RGB camera and an off-the-shelf projector as active illumination for both the 3D reconstruction and the spectral reflectance estimation. We first propose a calibration-free multi-view structured-light method to reconstruct the 3D points while estimating the intrinsic parameters and the poses of both the camera and the projector, which are alternately moved around the object during our image acquisition procedure. We then exploit the projector for multispectral imaging and estimate the spectral reflectance of each 3D point based on a novel spectral reflectance estimation method considering the geometric relationship between the reconstructed 3D points and the estimated projector positions. Experimental results on both synthetic and real data demonstrate that our system can precisely acquire a dense spectral 3D model using off-the-shelf devices.
C1 [Li, Chunyu; Monno, Yusuke; Okutomi, Masatoshi] Tokyo Inst Technol, Sch Engn, Dept Syst & Control Engn, Tokyo, Japan.
C3 Tokyo Institute of Technology
RP Li, CY (corresponding author), Tokyo Inst Technol, Sch Engn, Dept Syst & Control Engn, Tokyo, Japan.
EM lchunyu@ok.ctrl.titech.ac.jp
RI L, Chun/HKW-1738-2023; Monno, Yusuke/W-9098-2019
OI Monno, Yusuke/0000-0001-6733-3406
FU JSPS KAKENHI [17H00744, 21K17762]; Grants-in-Aid for Scientific Research
   [21K17762, 17H00744] Funding Source: KAKEN
FX This work was partly supported by JSPS KAKENHI Grant Number 17H00744 and
   21K17762.
CR Aanæs H, 2016, INT J COMPUT VISION, V120, P153, DOI 10.1007/s11263-016-0902-9
   Aeschbacher J, 2017, IEEE INT CONF COMP V, P471, DOI 10.1109/ICCVW.2017.63
   Agarwal S, Ceres solver
   Aliaga DanielG., 2008, COMPUTER VISION PATT, P1
   Arad B, 2016, LECT NOTES COMPUT SC, V9911, P19, DOI 10.1007/978-3-319-46478-7_2
   Baek SH, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130896
   Behmann J, 2016, MACH VISION APPL, V27, P611, DOI 10.1007/s00138-015-0716-8
   BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791
   Cao X, 2011, IEEE T PATTERN ANAL, V33, P2423, DOI 10.1109/TPAMI.2011.80
   Caspi D, 1998, IEEE T PATTERN ANAL, V20, P470, DOI 10.1109/34.682177
   Chane CS, 2013, IMAGE VISION COMPUT, V31, P91, DOI 10.1016/j.imavis.2012.10.006
   Chi C, 2010, INT J COMPUT VISION, V86, P140, DOI 10.1007/s11263-008-0176-y
   Cignoni P., 2008, EUR IT CHAPT C, DOI [DOI 10.2312/LOCALCHAPTEREVENTS/ITALCHAP/ITALIANCHAPCONF2008/129-136, 10.2312/LocalChapterEvents/ItalChap/ItalianChapConf2008/129-136]
   Devlin K., 2002, Eurographics
   Fu Y, 2018, IEEE T COMPUT IMAG, V4, P382, DOI 10.1109/TCI.2018.2855445
   Furuakwa Ryo, 2009, 2009 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops (CVPR Workshops), P69, DOI 10.1109/CVPR.2009.5204318
   Furukawa R., 2010, 2010 Fourth Pacific-Rim Symposium on Image and Video Technology (PSIVT), P107, DOI 10.1109/PSIVT.2010.25
   Furukawa R, 2019, INT CONF 3D VISION, P453, DOI 10.1109/3DV.2019.00057
   Furukawa Y, 2010, IEEE T PATTERN ANAL, V32, P1362, DOI 10.1109/TPAMI.2009.161
   Garrido-Jurado S, 2016, J VIS COMMUN IMAGE R, V39, P120, DOI 10.1016/j.jvcir.2016.05.014
   Geng J, 2011, ADV OPT PHOTONICS, V3, P128, DOI 10.1364/AOP.3.000128
   Greg T, STANFORD 3D SCANNING
   Han S, 2014, INT J COMPUT VISION, V110, P172, DOI 10.1007/s11263-013-0687-z
   Herakleous K, 2014, ABS14066595 CORR
   Nguyen H, 2015, APPL OPTICS, V54, pA9, DOI 10.1364/AO.54.0000A9
   Hirai K, 2016, LECT NOTES COMPUT SC, V9680, P137, DOI 10.1007/978-3-319-33618-3_15
   Inokuchi S., 1984, Seventh International Conference on Pattern Recognition (Cat. No. 84CH2046-1), P806
   Ito S., 2016, P AS C COMP VIS ACCV, P236
   Jia Y, 2017, IEEE I CONF COMP VIS, P4715, DOI 10.1109/ICCV.2017.504
   Jiang J, 2013, IEEE WORK APP COMP, P168, DOI 10.1109/WACV.2013.6475015
   Kazhdan M, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2487228.2487237
   Kim K, 2016, LECT NOTES COMPUT SC, V9907, P750, DOI 10.1007/978-3-319-46487-9_46
   Kim MH, 2014, ACM J COMPUT CULT HE, V7, DOI 10.1145/2567652
   Kim MH, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185534
   Kitahara Masahiro, 2015, 10th International Conference on Computer Vision Theory and Applications (VISAPP 2015). Proceedings, P303
   Ley A, 2016, LECT NOTES COMPUT SC, V9911, P236, DOI 10.1007/978-3-319-46478-7_15
   Li C, 2021, IEEE INT C COMPUTATI, P1
   Li CY, 2019, IEEE I CONF COMP VIS, P2414, DOI 10.1109/ICCV.2019.00250
   Li CY, 2018, IEEE ICCE
   Liang J, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P172, DOI 10.1109/ICCVW.2013.29
   Lourakis MIA, 2009, ACM T MATH SOFTWARE, V36, DOI 10.1145/1486525.1486527
   Mansouri A, 2007, IEEE MULTIMEDIA, V14, P40, DOI 10.1109/MMUL.2007.22
   Maurer D, 2016, P BRIT MACH VIS C BM
   Mélou J, 2018, J MATH IMAGING VIS, V60, P1527, DOI 10.1007/s10851-018-0809-x
   Monno Y, 2015, IEEE T IMAGE PROCESS, V24, P3048, DOI 10.1109/TIP.2015.2436342
   Nguyen RMH, 2014, LECT NOTES COMPUT SC, V8695, P186, DOI 10.1007/978-3-319-10584-0_13
   Oh SW, 2016, PROC CVPR IEEE, P2461, DOI 10.1109/CVPR.2016.270
   Ozawa K, 2017, J OPT SOC AM A, V34, P384, DOI 10.1364/JOSAA.34.000384
   Park JI, 2007, IEEE I CONF COMP VIS, P2049
   PARKKINEN JPS, 1989, J OPT SOC AM A, V6, P318, DOI 10.1364/JOSAA.6.000318
   Rusu RB, 2009, IEEE INT CONF ROBOT, P1848
   Salvi J, 2010, PATTERN RECOGN, V43, P2666, DOI 10.1016/j.patcog.2010.03.004
   Schönberger JL, 2016, PROC CVPR IEEE, P4104, DOI 10.1109/CVPR.2016.445
   Seitz S.M., 2006, IEEE COMP SOC C COMP, P519
   Shi Z, 2018, IEEE COMPUT SOC CONF, P1052, DOI 10.1109/CVPRW.2018.00139
   Snavely N, 2006, ACM T GRAPHIC, V25, P835, DOI 10.1145/1141911.1141964
   Weinmann M., 2011, 2011 International Conference on 3D Imaging, Modeling, Processing, Visualization and Transmission (3DIMPVT), P397, DOI 10.1109/3DIMPVT.2011.57
   Zia A, 2015, IEEE WINT CONF APPL, P318, DOI 10.1109/WACV.2015.49
NR 58
TC 1
Z9 1
U1 3
U2 7
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2023
VL 39
IS 4
BP 1651
EP 1666
DI 10.1007/s00371-022-02434-0
EA FEB 2022
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA C3OY5
UT WOS:000758114500002
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Fan, ZB
   Zhu, YX
   Markovic, S
   Zhang, K
AF Fan, Zhen-Bao
   Zhu, Yi-Xuan
   Markovic, Slobodan
   Zhang, Kang
TI A comparative study of oil paintings and Chinese ink paintings on
   composition
SO VISUAL COMPUTER
LA English
DT Article
DE Paintings; Composition; Visual Balance; Layout Rules
ID AESTHETIC PREFERENCE; BALANCE; ART; PHOTOGRAPHS; PERCEPTION; RESPONSES;
   SYMMETRY; CULTURE; GESTALT
AB In this study, we compare Western oil paintings and Chinese ink paintings on their composition, by extracting and computing 28 composition features of the paintings, including visual balance and relationships between different regions (segments). Among the extracted segments, we compute average distance and rule-based features based on three layout rules, rule of thirds, golden mean and golden triangle. A total of 2253 paintings including 1138 oil paintings and 1115 Chinese ink paintings are collected. By comparing the results of the features on these paintings, our study investigates the difference and similarity between the two types of paintings on composition. Their composition designs are similar in visual balance and their tendency of composing along two diagonal lines, but are fairly different on many other aspects. For example, oil paintings are inclined to place objects on the bottom horizontal dividing lines of rule of thirds and golden mean. Having discovered the most important features that can differentiate the two types of paintings, we analyze the differences in the features and discuss their possible relationships to the culture and artists' backgrounds.
C1 [Fan, Zhen-Bao] Univ Texas Dallas, Dept Comp Sci, Richardson, TX 75082 USA.
   [Zhu, Yi-Xuan] China Univ Min & Technol, Dept Math, Xuzhou 221116, Jiangsu, Peoples R China.
   [Markovic, Slobodan] Univ Belgrade, Expt Psychol Lab, Belgrade 11000, Serbia.
   [Zhang, Kang] Hong Kong Univ Sci & Technol, Computat Media & Arts, Guangzhou, Guangdong, Peoples R China.
C3 University of Texas System; University of Texas Dallas; China University
   of Mining & Technology; University of Belgrade; Hong Kong University of
   Science & Technology
RP Zhang, K (corresponding author), Hong Kong Univ Sci & Technol, Computat Media & Arts, Guangzhou, Guangdong, Peoples R China.
EM kzhang@utdallas.edu
OI Markovic, Slobodan/0000-0001-8504-1714; Zhang, Kang/0000-0003-3802-7535
CR Amirshahi SA, 2014, ART PERCEPT, V2, P163, DOI 10.1163/22134913-00002024
   [Anonymous], 1962, Uncertainty and structure as psychological concepts
   Arnheim R., 1969, ART VISUAL PERCEPTIO
   Arnheim R., 1986, NEW ESSAYS PSYCHOL A, DOI [10.1525/9780520907843, DOI 10.1525/9780520907843]
   ATTNEAVE F, 1955, Am J Psychol, V68, P209, DOI 10.2307/1418892
   ATTNEAVE F, 1954, PSYCHOL REV, V61, P183, DOI 10.1037/h0054663
   ATTNEAVE F, 1956, PSYCHOL BULL, V53, P452, DOI 10.1037/h0044049
   Bao Y, 2016, FRONT PSYCHOL, V7, DOI 10.3389/fpsyg.2016.01596
   BIRKHOFF GEORGE DAVID, 1933, Aesthetic measure, DOI DOI 10.4159/HARVARD.9780674734470
   Cameron AS., 1993, CHINESE PAINTING TEC
   Datta R, 2006, LECT NOTES COMPUT SC, V3953, P288, DOI 10.1007/11744078_23
   Eysenck HJ, 1942, PSYCHOL REV, V49, P344, DOI 10.1037/h0057013
   EYSENCK HJ, 1968, J GEN PSYCHOL, V79, P3, DOI 10.1080/00221309.1968.9710447
   Fan ZB, 2017, ACM SYMPOSIUM ON APPLIED PERCEPTION (SAP 2017), DOI 10.1145/3119881.3119883
   Fan ZB, 2020, VIS COMPUT IND BIOME, V3, DOI 10.1186/s42492-020-00059-5
   Fillinger MG, 2020, EMPIR STUD ARTS, V38, P172, DOI 10.1177/0276237418805656
   Gombrich E.H., 1980, THE SENSE OF ORDER
   HOCHBERG J, 1960, AM J PSYCHOL, V73, P337, DOI 10.2307/1420172
   Hübner R, 2019, I-PERCEPTION, V10, DOI 10.1177/2041669519856040
   Hübner R, 2016, FRONT PSYCHOL, V7, DOI 10.3389/fpsyg.2016.00335
   Jacobsen T, 2002, PERCEPT MOTOR SKILL, V95, P755, DOI 10.2466/PMS.95.7.755-766
   Jacobsen T.Hofel., 2001, EMPIR STUD ARTS, V19, P177, DOI [10.2190/P7W1-5F1F-NJK9-X05B, DOI 10.2190/P7W1-5F1F-NJK9-X05B, https://doi.org/10.2190/P7W1-5F1F-NJK9-X05B, 10.2190/P7W1-5F-1F-NJK9-X05B]
   Jacobsen T, 2003, COGN AFFECT BEHAV NE, V3, P289, DOI 10.3758/CABN.3.4.289
   Ji LJ, 2000, J PERS SOC PSYCHOL, V78, P943, DOI 10.1037/0022-3514.78.5.943
   Koffka K., 1935, Principles of gestalt psychology
   KRUPINSKI E, 1988, B PSYCHONOMIC SOC, V26, P355, DOI 10.3758/BF03337681
   Kubovy Michael., 1986, The Psychology of Perspective and Renaissance Art
   Latto R, 2000, PERCEPTION, V29, P981, DOI 10.1068/p2352
   Letsch P, 2020, PSYCHOL AESTHET CREA, V14, P186, DOI 10.1037/aca0000209
   Locher PJ, 2003, ACTA PSYCHOL, V114, P147, DOI 10.1016/j.actpsy.2003.07.001
   Locher PJ, 1998, ACTA PSYCHOL, V99, P141, DOI 10.1016/S0001-6918(98)00008-0
   Makin ADJ, 2012, NEUROPSYCHOLOGIA, V50, P3250, DOI 10.1016/j.neuropsychologia.2012.10.003
   Markovic S, 2011, PSIHOLOGIJA, V44, P191, DOI 10.2298/PSI1103191M
   MARTINDALE C, 1990, AM J PSYCHOL, V103, P53, DOI 10.2307/1423259
   Masuda T, 2008, PERS SOC PSYCHOL B, V34, P1260, DOI 10.1177/0146167208320555
   McManus IC, 2011, I-PERCEPTION, V2, P615, DOI 10.1068/i0445aap
   McManus I.C., 1993, EMPIR STUD ARTS, V11, P83, DOI DOI 10.2190/HXR4-VU9A-P5D9-BPQQ
   MCMANUS IC, 1985, BRIT J PSYCHOL, V76, P311, DOI 10.1111/j.2044-8295.1985.tb01955.x
   NICKI RM, 1975, CAN J PSYCHOL, V29, P237, DOI 10.1037/h0082029
   Nisbett R. E., 2003, The geography of thought: How Asians and Westerners think differentlyand why
   Obrador P, 2010, IEEE IMAGE PROC, P3185, DOI 10.1109/ICIP.2010.5654231
   OSBORNE JW, 1970, PSYCHON SCI, V19, P69, DOI 10.3758/BF03337424
   Palmer SE, 2013, ANNU REV PSYCHOL, V64, P77, DOI 10.1146/annurev-psych-120710-100504
   Palmer SE, 2011, PERCEPTION, V40, P1428, DOI 10.1068/p7021
   Poore HenryR., 1903, Pictorial Composition and the Critical Judgement of Pictures: A Handbook for Students and Lovers of Art
   Ross EA., 1907, THEORY PURE DESIGN H
   Stamps AE, 2002, J GEN PSYCHOL, V129, P300, DOI 10.1080/00221300209602100
   Sullivan Michael., 1984, THE ARTS OF CHINA, VThird
   Thömmes K, 2018, FRONT PSYCHOL, V9, DOI 10.3389/fpsyg.2018.01050
   Tinio PPL, 2009, ACTA PSYCHOL, V130, P241, DOI 10.1016/j.actpsy.2009.01.001
   Van Geert E, 2020, PSYCHOL AESTHET CREA, V14, P135, DOI 10.1037/aca0000224
   Wertheimer M., 1938, SOURCE BOOK GESTALT, P12
   Wilson A., 2005, EMPIR STUD ARTS, V23, P165, DOI [DOI 10.2190/B1LR-MVF3-F36X-XR64, 10.2190/B1LR-MVF3-F36X-XR64]
NR 53
TC 2
Z9 2
U1 14
U2 48
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2023
VL 39
IS 4
BP 1323
EP 1334
DI 10.1007/s00371-022-02408-2
EA FEB 2022
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA C3OY5
UT WOS:000757925500001
OA Green Published
DA 2024-07-18
ER

PT J
AU He, D
   He, XP
   Yuan, R
   Li, Y
   Shen, C
AF He, Dan
   He, Xiping
   Yuan, Rui
   Li, Yue
   Shen, Chao
TI Lightweight network-based multi-modal feature fusion for face
   anti-spoofing
SO VISUAL COMPUTER
LA English
DT Article
DE Face anti-spoofing; Lightweight network; Multi-modal; Patch-level
   images; Redundant feature
AB Effectively identifying attacked faces is an urgent problem to be solved in the scenario of face recognition. As deep learning is applied to face anti-spoofing (FAS), many multi-modal approaches have been proven to be more efficient than single-modal, but at the same time, multi-modal approaches require a huge number of parameters and therefore result in high computation. To tackle this problem, a lightweight network-based multi-modal FAS model was proposed, which took patch-level images from multi-modal images (YCbCr, Depth, and IR) as the input to different branches, and designed a lightweight feature extraction module to solve the redundancy of feature maps extracted by the filters. Finally, an attention-based feature fusion module was constructed to fuse and classify the features extracted by each branch network. A great number of comparative experimental results demonstrated that this method greatly reduced parameters at a high accuracy. For example, the accuracy on single-modal datasets (Replay-Attack and CASIA-FASD) is 100%, and that on multi-modal dataset (CASIA-SURF) is 98.1269% (TPR@FPR = 10e-4) and 0.2232%. In addition, the backbone network has only 0.25 M parameters and 0.37 G FLOPs.
C1 [He, Dan; He, Xiping; Yuan, Rui; Li, Yue] Chongqing Technol & Business Univ, Sch Artificial Intelligence, Chongqing 400067, Peoples R China.
   [He, Xiping] Chongqing Technol & Business Univ, Control & Integrated Syst, Chongqing Engn Lab Detect, Chongqing 400067, Peoples R China.
   [Shen, Chao] Chongqing Technol & Business Univ, Sch Management Sci & Engn, Chongqing 400067, Peoples R China.
C3 Chongqing Technology & Business University; Chongqing Technology &
   Business University; Chongqing Technology & Business University
RP He, XP (corresponding author), Chongqing Technol & Business Univ, Sch Artificial Intelligence, Chongqing 400067, Peoples R China.; He, XP (corresponding author), Chongqing Technol & Business Univ, Control & Integrated Syst, Chongqing Engn Lab Detect, Chongqing 400067, Peoples R China.
EM jsjhxp@ctbu.edu.cn
RI He, Dan/KRQ-7817-2024; He, Xiping/V-6881-2019
OI He, Dan/0000-0001-9541-5214; He, Xiping/0000-0003-3922-3319
FU Science and Technology Research Project of Chongqing Education
   Commission [KJQN201900833]; Scientific Research and Innovation
   Foundation of Chongqing, China [CYS21398]
FX This work was supported by the Science and Technology Research Project
   of Chongqing Education Commission (Grant No. KJQN201900833), and the
   Scientific Research and Innovation Foundation of Chongqing, China (Grant
   No. CYS21398).
CR Arora S, 2022, VISUAL COMPUT, V38, P2461, DOI 10.1007/s00371-021-02123-4
   Atoum Y, 2017, 2017 IEEE INTERNATIONAL JOINT CONFERENCE ON BIOMETRICS (IJCB), P319, DOI 10.1109/BTAS.2017.8272713
   Boulkenafet Z, 2017, IEEE SIGNAL PROC LET, V24, P141, DOI 10.1109/LSP.2016.2630740
   Boulkenafet Z, 2016, IEEE T INF FOREN SEC, V11, P1818, DOI 10.1109/TIFS.2016.2555286
   Chen G, 2013, INT CONF BIOMED, P1, DOI 10.1109/BMEI.2013.6746896
   Chingovska I., 2012, 2012 BIOSIG P INT C, P1
   de Freitas Pereira Tiago, 2013, Computer Vision - ACCV 2012 Workshops. ACCV 2012 International Workshops. Revised Selected Papers, P121, DOI 10.1007/978-3-642-37410-4_11
   Deng JK, 2019, PROC CVPR IEEE, P4685, DOI 10.1109/CVPR.2019.00482
   George A, 2021, PROC CVPR IEEE, P7878, DOI 10.1109/CVPR46437.2021.00779
   Han K, 2020, PROC CVPR IEEE, P1577, DOI 10.1109/CVPR42600.2020.00165
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Jiang XR, 2020, NEUROCOMPUTING, V402, P29, DOI 10.1016/j.neucom.2020.03.073
   Jourabloo A, 2018, LECT NOTES COMPUT SC, V11217, P297, DOI 10.1007/978-3-030-01261-8_18
   Khammari M, 2019, IET IMAGE PROCESS, V13, P1880, DOI 10.1049/iet-ipr.2018.5560
   Komulainen J, 2013, INT CONF BIOMETR
   Kuang HF, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P48, DOI 10.1145/3343031.3351001
   Lagorio A, 2013, I W BIOMETRIC FORENS
   Li XB, 2016, INT C PATT RECOG, P4244, DOI 10.1109/ICPR.2016.7900300
   Li XY, 2020, IEEE INT CON MULTI, DOI 10.1109/icme46284.2020.9102948
   Liu A., 2019, ARXIVABS191202340
   Liu AJ, 2021, IEEE WINT CONF APPL, P1178, DOI 10.1109/WACV48630.2021.00122
   Liu ST, 2018, LECT NOTES COMPUT SC, V11215, P404, DOI 10.1007/978-3-030-01252-6_24
   Liu YJ, 2019, PROC CVPR IEEE, P4675, DOI 10.1109/CVPR.2019.00481
   Liu YJ, 2018, PROC CVPR IEEE, P389, DOI 10.1109/CVPR.2018.00048
   Ma NN, 2018, LECT NOTES COMPUT SC, V11218, P122, DOI 10.1007/978-3-030-01264-9_8
   Parkin A, 2019, IEEE COMPUT SOC CONF, P1617, DOI 10.1109/CVPRW.2019.00204
   Peng CL, 2019, IEEE T IMAGE PROCESS, V28, P4553, DOI 10.1109/TIP.2019.2912360
   Peng CL, 2019, PATTERN RECOGN, V90, P161, DOI 10.1016/j.patcog.2019.01.041
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474
   Shen T, 2019, IEEE COMPUT SOC CONF, P1611, DOI 10.1109/CVPRW.2019.00203
   Sun YJ, 2021, VISUAL COMPUT, V37, P1015, DOI 10.1007/s00371-020-01849-x
   Wang Y, 2017, J VIS COMMUN IMAGE R, V49, P332, DOI 10.1016/j.jvcir.2017.09.002
   Wang ZZ, 2020, PROC CVPR IEEE, P5041, DOI 10.1109/CVPR42600.2020.00509
   Wu BC, 2018, PROC CVPR IEEE, P9127, DOI 10.1109/CVPR.2018.00951
   Xin J., 2019, AAAI, DOI [10.1609/aaai.v33i01.33019054, DOI 10.1609/AAAI.V33I01.33019054]
   Xin J., 2020, P AAAI C ART INT, V34
   Xu ZQ, 2015, PROCEEDINGS 3RD IAPR ASIAN CONFERENCE ON PATTERN RECOGNITION ACPR 2015, P141, DOI 10.1109/ACPR.2015.7486482
   Yang J., 2014, ARXIVABS14085601, DOI [10.1007/978-3-319-21963-9_34, DOI 10.1007/978-3-319-21963-9_34]
   Yang X, 2019, PROC CVPR IEEE, P3502, DOI 10.1109/CVPR.2019.00362
   Yu ZT, 2021, IEEE T PATTERN ANAL, V43, P3005, DOI 10.1109/TPAMI.2020.3036338
   Yu ZT, 2020, PROC CVPR IEEE, P5294, DOI 10.1109/CVPR42600.2020.00534
   Yu ZT, 2020, IEEE COMPUT SOC CONF, P2766, DOI 10.1109/CVPRW50498.2020.00333
   Zhang P, 2019, IEEE COMPUT SOC CONF, P1574, DOI 10.1109/CVPRW.2019.00199
   Zhang SF, 2019, PROC CVPR IEEE, P919, DOI 10.1109/CVPR.2019.00101
   Zhao PZ, 2016, IEEE IMTC P, P1
   Zhiwei Zhang, 2012, 2012 5th IAPR International Conference on Biometrics (ICB), P26, DOI 10.1109/ICB.2012.6199754
NR 46
TC 8
Z9 8
U1 0
U2 14
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2023
VL 39
IS 4
BP 1423
EP 1435
DI 10.1007/s00371-022-02420-6
EA FEB 2022
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA C3OY5
UT WOS:000757925500002
DA 2024-07-18
ER

PT J
AU Zhao, XY
   Wang, GL
   He, ZX
   Liang, D
   Zhang, SY
   Tan, JR
AF Zhao, Xinyue
   Wang, Guangli
   He, Zaixing
   Liang, Dong
   Zhang, Shuyou
   Tan, Jianrong
TI Unsupervised inner-point-pairs model for unseen-scene and online moving
   object detection
SO VISUAL COMPUTER
LA English
DT Article
DE Moving object detection; Inner-point-pairs; Dynamic background model;
   Unseen-scene and online detection
ID BACKGROUND SUBTRACTION; NETWORK
AB Moving object detection in complex environments is a challenging task. Recently, many deep learning-based methods have improved the detecting accuracy. However, because of the supervised learning nature, these methods cannot deal with unseen scene, where manually generated groundtruth of training data is unavailable. Furthermore, due to the limitation of generality, their online detection performance is not satisfactory, where the scene changes a lot over time. In this paper, we propose a new unsupervised method for moving object detection, which performs well in both seen and unseen scene, offline and online detection. It can better adapt to changes in environmental lighting and has good tolerance for dynamic backgrounds in the environment. Based on the analysis of the reflections of objects in a scene, the proposed method not only processes images at the pixel level but also takes into account the characteristics at the object level. We distinguish between the ordinary point areas and complex point areas in an image and process them separately to improve the detection accuracy. The proposed method is tested using several challenging datasets, and the results show its effectiveness and applicability in complex situations.
C1 [Zhao, Xinyue; He, Zaixing; Tan, Jianrong] Zhejiang Univ, State Key Lab Fluid Power & Mechatron Syst, Hangzhou 310027, Peoples R China.
   [Wang, Guangli; Zhang, Shuyou] Zhejiang Univ, State Key Lab CAD&CG, Hangzhou 310027, Zhejiang, Peoples R China.
   [Liang, Dong] Nanjing Univ Aeronaut & Astronaut, Coll Comp Sci & Technol, Nanjing 210016, Peoples R China.
C3 Zhejiang University; Zhejiang University; Nanjing University of
   Aeronautics & Astronautics
RP He, ZX (corresponding author), Zhejiang Univ, State Key Lab Fluid Power & Mechatron Syst, Hangzhou 310027, Peoples R China.
EM zaixinghe@zju.edu.cn
RI Liang, Dong/AAO-7160-2020; Zhang, Jun/JPK-7723-2023; Wang,
   Guang/JFS-8374-2023
OI Liang, Dong/0000-0003-2784-3449; 
FU National Natural Science Foundation of China [51775498, 51775497];
   Zhejiang Provincial Natural Science Foundation [LY21E050021]
FX This study was funded by the National Natural Science Foundation of
   China (51775498 and 51775497) and the Zhejiang Provincial Natural
   Science Foundation (LY21E050021).
CR Amraee S, 2018, MULTIMED TOOLS APPL, V77, P14767, DOI 10.1007/s11042-017-5061-7
   Azeez Bayar, 2020, 2020 6th International Engineering Conference, Sustainable Technology and Development (IEC). Proceedings, P185, DOI 10.1109/IEC49899.2020.9122929
   Bao JT, 2015, SENSORS-BASEL, V15, P21054, DOI 10.3390/s150921054
   Barnich O, 2011, IEEE T IMAGE PROCESS, V20, P1709, DOI 10.1109/TIP.2010.2101613
   Bianco S, 2017, IEEE T EVOLUT COMPUT, V21, P914, DOI 10.1109/TEVC.2017.2694160
   Bouwmans T., 2019, Background Subtraction in Real Applications: Challenges, Current Models and Future Directions
   Bouwmans T, 2019, NEURAL NETWORKS, V117, P8, DOI 10.1016/j.neunet.2019.04.024
   Bouwmans T, 2014, COMPUT SCI REV, V11-12, P31, DOI 10.1016/j.cosrev.2014.04.001
   Braham M, 2016, INT CONF SYST SIGNAL, P113
   Cai X, 2020, IEEE ACCESS, V8, P114422, DOI 10.1109/ACCESS.2020.3003724
   Chen Y, 2017, IEEE CUST INTEGR CIR
   Choi YW, 2017, INT CONF UBIQ FUTUR, P203
   El Harrouss O, 2015, I C COMP SYST APPLIC
   Elgammal A, 2002, P IEEE, V90, P1151, DOI 10.1109/JPROC.2002.801448
   Elgammal A, 2000, EUR C COMP VIS, P751, DOI DOI 10.1007/3-540-45053-X_48
   Giraldo J. H., 2020, GRAPHBGS BACKGROUND
   Giraldo JH, 2020, IEEE IMAGE PROC, P3224, DOI [10.1109/icip40778.2020.9190887, 10.1109/ICIP40778.2020.9190887]
   Giraldozuluaga JH, 2022, IEEE T PATTERN ANAL, V44, P2485, DOI 10.1109/TPAMI.2020.3042093
   Giveki D, 2020, OPTIK, V209, DOI 10.1016/j.ijleo.2020.164563
   Haines TSF, 2014, IEEE T PATTERN ANAL, V36, P670, DOI 10.1109/TPAMI.2013.239
   Hu M, 2008, INT C PATT RECOG, P9
   Hu ZH, 2018, IEEE ACCESS, V6, P43450, DOI 10.1109/ACCESS.2018.2861223
   Jiang SQ, 2018, IEEE T CIRC SYST VID, V28, P2105, DOI 10.1109/TCSVT.2017.2711659
   Liang CW, 2015, IEEE T INTELL TRANSP, V16, DOI 10.1109/TITS.2015.2459917
   Liang D, 2015, PATTERN RECOGN, V48, P1374, DOI 10.1016/j.patcog.2014.10.020
   Liao S., 2010, Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on, P1301, DOI DOI 10.1109/CVPR.2010.5539817
   Lim Kyungsun., 2017, Advanced Video and Signal Based Surveillance (AVSS), 2017 14th IEEE International Conference on, P1
   Lim LA, 2020, PATTERN ANAL APPL, V23, P1369, DOI 10.1007/s10044-019-00845-9
   Lim LA, 2018, PATTERN RECOGN LETT, V112, P256, DOI 10.1016/j.patrec.2018.08.002
   Losada C, 2013, ROBOT AUTON SYST, V61, P75, DOI 10.1016/j.robot.2012.11.007
   Ma C, 2015, PROC CVPR IEEE, P5388, DOI 10.1109/CVPR.2015.7299177
   MANDAL M, IEEE T INTELL TRANSP, P2021
   Patil PW, 2021, IEEE SIGNAL PROC LET, V28, P489, DOI 10.1109/LSP.2021.3059195
   Patil PW, 2019, IEEE T INTELL TRANSP, V20, P4066, DOI 10.1109/TITS.2018.2880096
   Sakkos D, 2018, MULTIMED TOOLS APPL, V77, P23023, DOI 10.1007/s11042-017-5460-9
   Sengar SS, 2017, OPTIK, V145, P130, DOI 10.1016/j.ijleo.2017.07.040
   Tezcan MO, 2020, IEEE WINT CONF APPL, P2763, DOI [10.1109/WACV45572.2020.9093464, 10.1109/wacv45572.2020.9093464]
   Tezcan MO, 2021, IEEE ACCESS, V9, P53849, DOI 10.1109/ACCESS.2021.3071163
   Vijayan M, 2018, OPTIK, V168, P963, DOI 10.1016/j.ijleo.2018.05.012
   Wang R, 2014, IEEE COMPUT SOC CONF, P420, DOI 10.1109/CVPRW.2014.68
   Wang Y, 2017, PATTERN RECOGN LETT, V96, P66, DOI 10.1016/j.patrec.2016.09.014
   Wang Y, 2014, IEEE COMPUT SOC CONF, P393, DOI 10.1109/CVPRW.2014.126
   Xu YP, 2020, OPTIK, V207, DOI 10.1016/j.ijleo.2020.164195
   Yeh CH, 2017, IEEE T IND ELECTRON, V64, P4945, DOI 10.1109/TIE.2017.2669881
   Zhao XY, 2011, PATTERN RECOGN, V44, P1296, DOI 10.1016/j.patcog.2010.11.022
   Zhong ZF, 2017, IEEE T INTELL TRANSP, V18, P1109, DOI 10.1109/TITS.2016.2597441
   Zhou WJ, 2018, PROCEEDINGS 2018 12TH FRANCE-JAPAN AND 10TH EUROPE-ASIA CONGRESS ON MECHATRONICS, P77, DOI 10.1109/MECATRONICS.2018.8495871
NR 47
TC 1
Z9 1
U1 0
U2 17
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2023
VL 39
IS 4
BP 1391
EP 1407
DI 10.1007/s00371-022-02417-1
EA FEB 2022
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA C3OY5
UT WOS:000755389200001
DA 2024-07-18
ER

PT J
AU Xiao, HG
   Ran, ZQ
   Mabu, S
   Li, YW
   Li, L
AF Xiao, Hanguang
   Ran, Zhiqiang
   Mabu, Shingo
   Li, Yuewei
   Li, Li
TI SAUNet plus plus : an automatic segmentation model of COVID-19 lesion
   from CT slices
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT ACIAT202
CY 2022
CL ELECTR NETWORK
DE Coronavirus disease 2019 (COVID-19); Image segmentation; Computed
   tomography (CT); Squeeze excitation residual (SER); Atrous spatial
   pyramid pooling (ASPP); Generalized dice loss (GDL)
ID LUNG INFECTION SEGMENTATION; NETWORK; ATTENTION
AB The coronavirus disease 2019 (COVID-19) epidemic has spread worldwide and the healthcare system is in crisis. Accurate, automated and rapid segmentation of COVID-19 lesion in computed tomography (CT) images can help doctors diagnose and provide prognostic information. However, the variety of lesions and small regions of early lesion complicate their segmentation. To solve these problems, we propose a new SAUNet++ model with squeeze excitation residual (SER) module and atrous spatial pyramid pooling (ASPP) module. The SER module can assign more weights to more important channels and mitigate the problem of gradient disappearance; the ASPP module can obtain context information by atrous convolution using various sampling rates. In addition, the generalized dice loss (GDL) can reduce the correlation between lesion size and dice loss, and is introduced to solve the problem of small regions segmentation of COVID-19 lesion. We collected multinational CT scan data from China, Italy and Russia and conducted extensive comparative and ablation studies. The experimental results demonstrated that our method outperforms state-of-the-art models and can effectively improve the accuracy of COVID-19 lesion segmentation on the dice similarity coefficient (our: 87.38% vs. U-Net++: 84.25%), sensitivity (our: 93.28% vs. U-Net++: 89.85%) and Hausdorff distance (our: 19.99 mm vs. U-Net++: 26.79 mm), respectively.
C1 [Xiao, Hanguang; Ran, Zhiqiang; Li, Yuewei; Li, Li] Chongqing Univ Technol, Sch Artificial Intelligence, Chongqing 401135, Peoples R China.
   [Ran, Zhiqiang; Mabu, Shingo] Yamaguchi Univ, Grad Sch Sci & Technol Innovat, Yamaguchi 7558611, Japan.
C3 Chongqing University of Technology; Yamaguchi University
RP Ran, ZQ (corresponding author), Chongqing Univ Technol, Sch Artificial Intelligence, Chongqing 401135, Peoples R China.; Ran, ZQ (corresponding author), Yamaguchi Univ, Grad Sch Sci & Technol Innovat, Yamaguchi 7558611, Japan.
EM zqran@2019.cqut.edu.cn
RI RAN, ZHIQIANG/KHY-2616-2024; xiao, hanguang/GPX-7141-2022
FU National Natural Science Foundation of China [61971078]; Science and
   Technology Foundation of Chongqing Education Commission [CQUT20181124];
   Graduate Student Innovation Program of Chongqing [CYS20351];
   Undergraduate Student Innovation and Entrepreneurship Training Program
   of Chongqing [S202111660048]
FX This work was supported in part by the National Natural Science
   Foundation of China (Grant No. 61971078), Science and Technology
   Foundation of Chongqing Education Commission (Grant No. CQUT20181124),
   Graduate Student Innovation Program of Chongqing (Grant No. CYS20351),
   and Undergraduate Student Innovation and Entrepreneurship Training
   Program of Chongqing (Grant No. S202111660048).
CR Abdel-Basset M, 2021, KNOWL-BASED SYST, V212, DOI 10.1016/j.knosys.2020.106647
   Abraham N, 2019, I S BIOMED IMAGING, P683, DOI 10.1109/ISBI.2019.8759329
   Alhasson HF., 2020, VISUAL COMPUT, V3, P21
   Amyar A, 2020, COMPUT BIOL MED, V126, DOI 10.1016/j.compbiomed.2020.104037
   [Anonymous], 2016, INT C LEARNING REPRE
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen LB, 2017, IEEE INT SYMP NANO, P1, DOI 10.1109/NANOARCH.2017.8053709
   Cicek Ozgun, 2016, Medical Image Computing and Computer-Assisted Intervention - MICCAI 2016. 19th International Conference. Proceedings: LNCS 9901, P424, DOI 10.1007/978-3-319-46723-8_49
   Crum WR, 2006, IEEE T MED IMAGING, V25, P1451, DOI 10.1109/TMI.2006.880587
   Dong JH, 2020, PROC CVPR IEEE, P4022, DOI 10.1109/CVPR42600.2020.00408
   Dong JH, 2019, IEEE I CONF COMP VIS, P10711, DOI 10.1109/ICCV.2019.01081
   Fabian I., 2018, ARXIV PREPRINT ARXIV
   Fan DP, 2020, IEEE T MED IMAGING, V39, P2626, DOI 10.1109/TMI.2020.2996645
   Fang YC, 2020, RADIOLOGY, V296, pE115, DOI 10.1148/radiol.2020200432
   Geng L, 2019, COMPUT ASSIST SURG, V24, P27, DOI 10.1080/24699322.2019.1649071
   Gu J, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11151817
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Jin QG, 2020, FRONT BIOENG BIOTECH, V8, DOI 10.3389/fbioe.2020.605132
   Kaushal C, 2020, CAAI T INTELL TECHNO, V5, P294, DOI 10.1049/trit.2019.0077
   Khanna A, 2020, BIOCYBERN BIOMED ENG, V40, P1314, DOI 10.1016/j.bbe.2020.07.007
   Kumar SP, 2020, IETE J RES, V66, P370, DOI 10.1080/03772063.2018.1494519
   Ma J, 2021, MED PHYS, V48, P1197, DOI 10.1002/mp.14676
   Ma JJ, 2019, NEUROCOMPUTING, V350, P91, DOI 10.1016/j.neucom.2019.03.065
   Morozov S., 2020, ARXIV200506465, DOI 10.1101/2020.05.20.20100362
   Nithila EE, 2019, BIOMED SIGNAL PROCES, V47, P57, DOI 10.1016/j.bspc.2018.08.008
   Oktay O., 2018, ARXIV, DOI DOI 10.48550/ARXIV.1804.03999
   Qiu Y, 2021, AAAI CONF ARTIF INTE, V35, P4846
   Raj ANJ, 2021, PEERJ COMPUT SCI, DOI 10.7717/peerj-cs.349
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Roy AG, 2020, MED IMAGE ANAL, V59, DOI 10.1016/j.media.2019.101587
   Shi LK, 2021, VISUAL COMPUT, V37, P1343, DOI 10.1007/s00371-020-01869-7
   Sudre CH, 2017, LECT NOTES COMPUT SC, V10553, P240, DOI 10.1007/978-3-319-67558-9_28
   Tan JX, 2021, COMPUT MED IMAG GRAP, V87, DOI 10.1016/j.compmedimag.2020.101817
   Tavare AN, 2020, THORAX, V75, P537, DOI 10.1136/thoraxjnl-2020-214916
   Wang GT, 2020, IEEE T MED IMAGING, V39, P2653, DOI 10.1109/TMI.2020.3000314
   Wang PQ, 2018, IEEE WINT CONF APPL, P1451, DOI 10.1109/WACV.2018.00163
   WHO, 2021, Comprehensive Mental Health Action Plan 2012-2030
   Xiao Hanguang, 2021, Sheng Wu Yi Xue Gong Cheng Xue Za Zhi, V38, P379, DOI 10.7507/1001-5515.202008032
   Zhang PY, 2020, DIAGNOSTICS, V10, DOI 10.3390/diagnostics10110901
   Zhang Z, 2020, COMPUT METH PROG BIO, V192, DOI 10.1016/j.cmpb.2020.105395
   Zhao X., 2021, ARXIV PREPRINT ARXIV
   Zheng BB, 2020, IEEE ACCESS, V8, P185786, DOI 10.1109/ACCESS.2020.3027738
   Zheng WB, 2021, INFORM FUSION, V75, P168, DOI 10.1016/j.inffus.2021.05.015
   Zhou TX, 2021, INT J IMAG SYST TECH, V31, P16, DOI 10.1002/ima.22527
   Zhou ZW, 2018, LECT NOTES COMPUT SC, V11045, P3, DOI 10.1007/978-3-030-00889-5_1
   Ziyad SR, 2020, CURR MED IMAGING, V16, P16, DOI 10.2174/1573405615666190206153321
NR 47
TC 24
Z9 24
U1 2
U2 33
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2023
VL 39
IS 6
SI SI
BP 2291
EP 2304
DI 10.1007/s00371-022-02414-4
EA FEB 2022
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA J1YY4
UT WOS:000752807000001
OA Bronze, Green Submitted
DA 2024-07-18
ER

PT J
AU Gao, Y
   Qi, ZY
   Zhao, DX
AF Gao, Ying
   Qi, Zhiyang
   Zhao, Dexin
TI Edge-enhanced instance segmentation by grid regions of interest
SO VISUAL COMPUTER
LA English
DT Article
DE Instance segmentation; Single-stage; Regions of interest; Edge-enhanced
AB This paper focuses on the instance segmentation task. The purpose of instance segmentation is to jointly detect, classify and segment individual instances in images, so it is used to solve a large number of industrial tasks such as novel coronavirus diagnosis and autonomous driving. However, it is not easy for instance models to achieve good results in terms of both efficiency of prediction classes and segmentation results of instance edges. We propose a single-stage instance segmentation model EEMask (edge-enhanced mask), which generates grid ROIs (regions of interest) instead of proposal boxes. EEMask divides the image uniformly according to the grid and then calculates the relevance between the grids based on the distance and grayscale values. Finally, EEMask uses the grid relevance to generate grid ROIs and grid classes. In addition, we design an edge-enhanced layer, which enhances the model's ability to perceive instance edges by increasing the number of channels with higher contrast at the instance edges. There is not any additional convolutional layer overhead, so the whole process is efficient. We evaluate EEMask on a public benchmark. On average, EEMask is 17.8% faster than BlendMask with the same training schedule. EEMask achieves a mask AP score of 39.9 on the MS COCO dataset, which outperforms Mask RCNN by 7.5% and BlendMask by 3.9%.
C1 [Gao, Ying; Qi, Zhiyang; Zhao, Dexin] Tianjin Univ Technol, Tianjin Key Lab Intelligence Comp & Novel Softwar, 391 West Binshui Rd, Tianjin 300384, Peoples R China.
C3 Tianjin University of Technology
RP Zhao, DX (corresponding author), Tianjin Univ Technol, Tianjin Key Lab Intelligence Comp & Novel Softwar, 391 West Binshui Rd, Tianjin 300384, Peoples R China.
EM zhaodexin@email.tjut.edu.cn
CR [Anonymous], 2021, LECT NOTES COMPUTER
   [Anonymous], 2020, CORR
   [Anonymous], 2010, INT J COMPUT VISION, DOI [DOI 10.1007/s11263-009-0275-4, 10.1007/s11263-009-0275-4]
   [Anonymous], 2020, CORR
   [Anonymous], 2020, CORR
   Bai M, 2017, PROC CVPR IEEE, P2858, DOI 10.1109/CVPR.2017.305
   Bolya D, 2019, IEEE I CONF COMP VIS, P9156, DOI 10.1109/ICCV.2019.00925
   Chen XL, 2019, IEEE I CONF COMP VIS, P2061, DOI 10.1109/ICCV.2019.00215
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Enze Xie, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12190, DOI 10.1109/CVPR42600.2020.01221
   Felzenszwalb P, 2008, PROC CVPR IEEE, P1984
   Felzenszwalb PF, 2010, IEEE T PATTERN ANAL, V32, P1627, DOI 10.1109/TPAMI.2009.167
   Felzenszwalb PF, 2010, PROC CVPR IEEE, P2241, DOI 10.1109/CVPR.2010.5539906
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He Kaiming, 2017, P IEEE INT C COMPUTE
   Huang R, 2018, IEEE INT CONF BIG DA, P2503, DOI 10.1109/BigData.2018.8621865
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Leng L, 2017, MULTIMED TOOLS APPL, V76, P333, DOI 10.1007/s11042-015-3058-7
   Leng L, 2013, NEUROCOMPUTING, V108, P1, DOI 10.1016/j.neucom.2012.08.028
   Li JQ, 2019, PROC SPIE, V10972, DOI 10.1117/12.2514437
   Li Y, 2017, PROC CVPR IEEE, P4438, DOI 10.1109/CVPR.2017.472
   Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu C, 2011, IEEE T PATTERN ANAL, V33, P2368, DOI 10.1109/TPAMI.2011.131
   Liu WB, 2017, NEUROCOMPUTING, V234, P11, DOI 10.1016/j.neucom.2016.12.038
   Ren S, 2015, FASTER R CNN REAL TI, P91
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Selvaraju RR, 2020, INT J COMPUT VISION, V128, P336, DOI [10.1007/s11263-019-01228-7, 10.1109/ICCV.2017.74]
   Tian Z, 2019, IEEE I CONF COMP VIS, P9626, DOI 10.1109/ICCV.2019.00972
   Viola P, 2001, PROC CVPR IEEE, P511, DOI 10.1109/cvpr.2001.990517
   Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb
   Xu WJ, 2020, IEEE J-STSP, V14, P506, DOI 10.1109/JSTSP.2020.2987729
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zhang YQ, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20041010
   Zhao DX, 2021, MULTIMED TOOLS APPL, V80, P28201, DOI 10.1007/s11042-021-11094-6
   Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319
   Zhou K, 2016, DESTECH TRANS COMP
NR 40
TC 10
Z9 10
U1 11
U2 40
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2023
VL 39
IS 3
BP 1137
EP 1148
DI 10.1007/s00371-021-02393-y
EA JAN 2022
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 9E6QG
UT WOS:000749149200005
PM 35125577
OA Bronze, Green Published
DA 2024-07-18
ER

PT J
AU Pham, TA
AF The-Anh Pham
TI Effective deep neural networks for license plate detection and
   recognition
SO VISUAL COMPUTER
LA English
DT Article
DE Automatic license plate recognition; License plate detection; Character
   recognition
AB Recently, automatic license plate recognition (ALPR) has drawn much of attention from researchers due to the impressive performance of deep learning (DL) techniques. While a large number of methods for ALPR have been investigated, there are a few attempts emphasizing efficient yet accurate models for facilitating the deployment on traditional CPU boxes or low resource devices. In the present work, we propose lightweight and effective deep convolutional neuron networks to address the problems of license plate detection and recognition. Differing from traditional DL methods, the proposed models discard the use of max-pooling modules, belong to a single-phase object detector, and consist of alternating convolutional layers and Inception residual networks. Different strategies for character prediction are also studied and deeply discussed. To have more insight of convolutional features, we provide a sequence of visual snapshots or slices of feature maps learned by the network at different layers for a given input image. These data are useful to explain the model's behavior for the task of character recognition. Extensive experiments conducted on two public datasets, CCPD and AOLP, showed a promising improvement of LP detection and recognition accuracy. Interestingly, the full system can work on low-resource CPU machines with a real-time speed.
C1 [The-Anh Pham] Hong Duc Univ HDU, Thanh Hoa City, Vietnam.
C3 Hong Duc University
RP Pham, TA (corresponding author), Hong Duc Univ HDU, Thanh Hoa City, Vietnam.
EM phamtheanh@hdu.edu.vn
FU Vietnam National Foundation for Science and Technology Development
   (NAFOSTED) [102.05-2020.02]
FX This research is funded by Vietnam National Foundation for Science and
   Technology Development (NAFOSTED) Under Grant Number 102.05-2020.02.
CR Bochkovskiy A., 2020, PREPRINT
   Bochkovskiy A., 2020, DARKNET OPEN SOURCE
   Dong M., 2017, P BRIT MACH VIS C LO, P1
   Goncalves GR, 2016, J ELECTRON IMAGING, V25, DOI 10.1117/1.JEI.25.6.069801
   Graves A, 2009, IEEE T PATTERN ANAL, V31, P855, DOI 10.1109/TPAMI.2008.137
   Hsu GS, 2013, IEEE T VEH TECHNOL, V62, P552, DOI 10.1109/TVT.2012.2226218
   Izidio DMF, 2020, DES AUTOM EMBED SYST, V24, P23, DOI 10.1007/s10617-019-09230-5
   Laroca R., 2018, P INT JOINT C NEUR N, P1
   Laroca R, 2021, IET INTELL TRANSP SY, V15, P483, DOI 10.1049/itr2.12030
   Li H, 2019, IEEE T INTELL TRANSP, V20, P1126, DOI 10.1109/TITS.2018.2847291
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Montazzolli S, 2017, SIBGRAPI, P55, DOI 10.1109/SIBGRAPI.2017.14
   Nair Vinod, 2010, INT C INT C MACHINE, P807
   Redmon J., 2016, P IEEE C COMP VIS PA, P779, DOI DOI 10.1109/CVPR.2016.91
   Redmon J, 2017, PROC CVPR IEEE, P6517, DOI 10.1109/CVPR.2017.690
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Selmi Z, 2020, PATTERN RECOGN LETT, V129, P213, DOI 10.1016/j.patrec.2019.11.007
   Selmi Z, 2017, PROC INT CONF DOC, P1132, DOI 10.1109/ICDAR.2017.187
   Silva SM, 2020, J VIS COMMUN IMAGE R, V71, DOI 10.1016/j.jvcir.2020.102773
   Silva SM, 2018, LECT NOTES COMPUT SC, V11216, P593, DOI 10.1007/978-3-030-01258-8_36
   Simonyan K., 2014, 14091556 ARXIV
   Spanhel J, 2017, 2017 14TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE (AVSS)
   Su H, 2015, I S BIOMED IMAGING, P55, DOI 10.1109/ISBI.2015.7163815
   Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278
   Tan MX, 2019, PR MACH LEARN RES, V97
   Xu ZB, 2018, LECT NOTES COMPUT SC, V11217, P261, DOI 10.1007/978-3-030-01261-8_16
   Yang Y, 2018, IET INTELL TRANSP SY, V12, P213, DOI 10.1049/iet-its.2017.0136
   Zhang SF, 2019, NEUROCOMPUTING, V364, P297, DOI 10.1016/j.neucom.2019.07.064
   Zhang X, 2019, PLOS ONE, V14, DOI 10.1371/journal.pone.0214133
   Zhuang JF, 2018, LECT NOTES COMPUT SC, V11207, P314, DOI 10.1007/978-3-030-01219-9_19
NR 30
TC 12
Z9 12
U1 7
U2 29
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2023
VL 39
IS 3
BP 927
EP 941
DI 10.1007/s00371-021-02375-0
EA JAN 2022
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 9E6QG
UT WOS:000744789700001
DA 2024-07-18
ER

PT J
AU Agrawal, T
   Choudhary, P
AF Agrawal, Tarun
   Choudhary, Prakash
TI Segmentation and classification on chest radiography: a systematic
   survey
SO VISUAL COMPUTER
LA English
DT Article
DE Deep convolutional neural network; Computer vision; Lung segmentation;
   Multiclass classification; Nodule; TB; COVID-19; Pneumothorax detection;
   GAN
ID COMPUTER-AIDED DIAGNOSIS; LUNG FIELD SEGMENTATION; IMAGE FEATURE
   ANALYSIS; ACTIVE SHAPE MODEL; X-RAY; NEURAL-NETWORKS; DISEASE
   CLASSIFICATION; NODULE DETECTION; TUBERCULOSIS; CANCER
AB Chest radiography (X-ray) is the most common diagnostic method for pulmonary disorders. A trained radiologist is required for interpreting the radiographs. But sometimes, even experienced radiologists can misinterpret the findings. This leads to the need for computer-aided detection diagnosis. For decades, researchers were automatically detecting pulmonary disorders using the traditional computer vision (CV) methods. Now the availability of large annotated datasets and computing hardware has made it possible for deep learning to dominate the area. It is now the modus operandi for feature extraction, segmentation, detection, and classification tasks in medical imaging analysis. This paper focuses on the research conducted using chest X-rays for the lung segmentation and detection/classification of pulmonary disorders on publicly available datasets. The studies performed using the Generative Adversarial Network (GAN) models for segmentation and classification on chest X-rays are also included in this study. GAN has gained the interest of the CV community as it can help with medical data scarcity. In this study, we have also included the research conducted before the popularity of deep learning models to have a clear picture of the field. Many surveys have been published, but none of them is dedicated to chest X-rays. This study will help the readers to know about the existing techniques, approaches, and their significance.
C1 [Agrawal, Tarun; Choudhary, Prakash] Natl Inst Technol Hamirpur, Dept Comp Sci & Engn, Hamirpur 177005, Himachal Prades, India.
C3 National Institute of Technology (NIT System); National Institute of
   Technology Hamirpur
RP Choudhary, P (corresponding author), Natl Inst Technol Hamirpur, Dept Comp Sci & Engn, Hamirpur 177005, Himachal Prades, India.
EM tarunagrawal@nith.ac.in; pc@nith.ac.in
RI Agrawal, Tarun/GRX-2420-2022
OI Agrawal, Tarun/0000-0003-2117-8601; Choudhary,
   Prakash/0000-0003-4337-7273
CR Abedalla A, 2020, IEEE IJCNN, DOI 10.1109/ijcnn48605.2020.9207268
   Agrawal A, 2020, VISUAL COMPUT, V36, P405, DOI 10.1007/s00371-019-01630-9
   Ahmed MN, 2002, IEEE T MED IMAGING, V21, P193, DOI 10.1109/42.996338
   Al Aseri Z, 2009, EMERG RADIOL, V16, P111, DOI 10.1007/s10140-008-0763-9
   Andermatt S, 2016, LECT NOTES COMPUT SC, V10008, P142, DOI 10.1007/978-3-319-46976-8_15
   Annangi P, 2010, I S BIOMED IMAGING, P892, DOI 10.1109/ISBI.2010.5490130
   [Anonymous], 2015, Advances in Neural Information Processing Systems
   [Anonymous], 2019, ARXIV PREPRINT ARXIV
   Antony J, 2016, INT C PATT RECOG, P1195, DOI 10.1109/ICPR.2016.7899799
   Arjovsky M, 2017, PR MACH LEARN RES, V70
   Armato SG, 1998, ACAD RADIOL, V5, P245, DOI 10.1016/S1076-6332(98)80223-7
   AUSTIN JHM, 1992, RADIOLOGY, V182, P115, DOI 10.1148/radiology.182.1.1727272
   Ayaz M, 2021, PHYS ENG SCI MED, V44, P183, DOI 10.1007/s13246-020-00966-0
   Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615
   Baltruschat IM, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-42294-8
   Bianco S, 2018, IEEE ACCESS, V6, P64270, DOI 10.1109/ACCESS.2018.2877890
   Brown MS, 1998, COMPUT MED IMAG GRAP, V22, P463, DOI 10.1016/S0895-6111(98)00051-2
   Cai JZ, 2018, LECT NOTES COMPUT SC, V11071, P589, DOI 10.1007/978-3-030-00934-2_66
   Candemir S, 2014, IEEE T MED IMAGING, V33, P577, DOI 10.1109/TMI.2013.2290491
   Candemir S, 2013, I S BIOMED IMAGING, P1473
   Candemir Sema., 2012, IEEE Healthcare Technology Conference: Translational Engineering in Health Medicine, P31
   CASELLES V, 1993, NUMER MATH, V66, P1, DOI 10.1007/BF01385685
   Chang N, 2019, SCI DATA, V6, DOI 10.1038/s41597-019-0052-3
   Chen C, 2018, LECT NOTES COMPUT SC, V11046, P143, DOI 10.1007/978-3-030-00919-9_17
   Chen J., 2016, Advances in neural information processing systems, P3036
   Chen SC, 2004, IEEE T SYST MAN CY B, V34, P1907, DOI 10.1109/TSMCB.2004.831165
   Chen S, 2020, ARTIF INTELL MED, V107, DOI 10.1016/j.artmed.2020.101881
   Chen S, 2013, IEEE T BIO-MED ENG, V60, P369, DOI 10.1109/TBME.2012.2226583
   Chen S, 2011, MED PHYS, V38, P1844, DOI 10.1118/1.3561504
   Chen WJ, 2006, ACAD RADIOL, V13, P63, DOI 10.1016/j.acra.2005.08.035
   CHOLLET F, 2017, PROC CVPR IEEE, P1800, DOI DOI 10.1109/CVPR.2017.195
   Clevert D., 2016, ARXIV151107289
   COOTES TF, 1995, COMPUT VIS IMAGE UND, V61, P38, DOI 10.1006/cviu.1995.1004
   Coppini G, 2003, IEEE T INF TECHNOL B, V7, P344, DOI 10.1109/TITB.2003.821313
   Dai W, 2018, LECT NOTES COMPUT SC, V11045, P263, DOI 10.1007/978-3-030-00889-5_30
   Demner-Fushman D, 2016, J AM MED INFORM ASSN, V23, P304, DOI 10.1093/jamia/ocv080
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dexin Cheng, 1988, Proceedings of the SPIE - The International Society for Optical Engineering, V1001, P261, DOI 10.1117/12.968961
   Dey N, 2021, PATTERN RECOGN LETT, V143, P67, DOI 10.1016/j.patrec.2020.12.010
   Dosovitskiy A, 2021, arXiv, DOI [10.48550/ARXIV.2010.11929, DOI 10.48550/ARXIV.2010.11929]
   Duong LT, 2021, EXPERT SYST APPL, V184, DOI 10.1016/j.eswa.2021.115519
   DURYEA J, 1995, MED PHYS, V22, P183, DOI 10.1118/1.597539
   Fukushima S., 1982, inCompetition and Cooperation in Neural Nets, P267, DOI 10.1007/978-3-642-46466-9_18
   Gaal G., 2020, CEUR WORKSHOP PROC, DOI DOI 10.48550/ARXIV.2003.10304
   Gabruseva T, 2020, IEEE COMPUT SOC CONF, P1436, DOI 10.1109/CVPRW50498.2020.00183
   Ge Z, 2018, ARXIV PREPRINT ARXIV
   Ghassemi, 2020, ARXIV200611988
   Giger ML, 2008, MED PHYS, V35, P5799, DOI 10.1118/1.3013555
   Gleason S., 2000, 4th IEEE Southwest Symposium on Image Analysis and Interpretation, P93, DOI 10.1109/IAI.2000.839578
   Glorot X., 2010, INT C ARTIFICIAL INT, P249
   Gohagan JK, 2000, CONTROL CLIN TRIALS, V21, p251S, DOI 10.1016/S0197-2456(00)00097-0
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Gozes O, 2019, IEEE ENG MED BIO, P4076, DOI [10.1109/embc.2019.8856729, 10.1109/EMBC.2019.8856729]
   Gu JX, 2018, PATTERN RECOGN, V77, P354, DOI 10.1016/j.patcog.2017.10.013
   Guan QJ, 2020, PATTERN RECOGN LETT, V131, P38, DOI 10.1016/j.patrec.2019.11.040
   Gundel Sebastian, 2019, Progress in Pattern Recognition, Image Analysis, Computer Vision, and Applications. 23rd Iberoamerican Congress, CIARP 2018. Proceedings: Lecture Notes in Computer Science (LNCS 11401), P757, DOI 10.1007/978-3-030-13469-3_88
   HASEGAWA A, 1994, PROC SPIE, V2167, P654
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Henschke CI, 1999, LANCET, V354, P99, DOI 10.1016/S0140-6736(99)06093-6
   Hira S, 2021, APPL INTELL, V51, P2864, DOI 10.1007/s10489-020-02010-w
   Hooda R, 2017, IEEE I C SIGNAL IMAG, P497, DOI 10.1109/ICSIPA.2017.8120663
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Huang XW, 2018, PROCEEDINGS OF 2018 IEEE INTERNATIONAL CONFERENCE ON INTEGRATED CIRCUITS, TECHNOLOGIES AND APPLICATIONS (ICTA 2018), P172, DOI 10.1109/CICTA.2018.8706048
   Huang YP, 2019, ADV NEUR IN, V32
   Huang ZL, 2019, IEEE I CONF COMP VIS, P603, DOI 10.1109/ICCV.2019.00069
   Hwang S, 2017, LECT NOTES COMPUT SC, V10553, P92, DOI 10.1007/978-3-319-67558-9_11
   Hwang S, 2015, PROC SPIE, V9785, DOI 10.1117/12.2216198
   Iandola Forrest N, 2016, SQUEEZENET ALEXNET L
   Imran AA, 2019, LECT NOTES COMPUT SC, V11861, P151, DOI 10.1007/978-3-030-32692-0_18
   Inunganbi S, 2021, VISUAL COMPUT, V37, P291, DOI 10.1007/s00371-020-01799-4
   Ioffe S, 2015, P INT C MACH LEARN, V2015, P1
   Irvin J, 2019, AAAI CONF ARTIF INTE, P590
   Isola Phillip, 2016, ARXIV161107004, DOI [DOI 10.1109/CVPR.2017.632, 10.1109/CVPR.2017.632]
   Jaeger S, 2014, QUANT IMAG MED SURG, V4, P475, DOI 10.3978/j.issn.2223-4292.2014.11.20
   Jaeger S, 2014, IEEE T MED IMAGING, V33, P233, DOI 10.1109/TMI.2013.2284099
   Jaeger S, 2012, IEEE ENG MED BIO, P4978, DOI 10.1109/EMBC.2012.6347110
   Jain R, 2021, APPL INTELL, V51, P1690, DOI 10.1007/s10489-020-01902-1
   Jaiswal AK, 2019, MEASUREMENT, V145, P511, DOI 10.1016/j.measurement.2019.05.076
   Jangam E., 2018, INT C REC TRENDS IM, P303
   Joseph A, 2020, VISUAL COMPUT, V36, P529, DOI 10.1007/s00371-019-01628-3
   Kalinovsky Alexander., 2016, LUNG IMAGE SSGMENTAT
   Kass M., 1987, Proceedings of the First International Conference on Computer Vision (Cat. No.87CH2465-3), P259, DOI 10.1007/BF00133570
   Kermany Daniel, 2018, Mendeley Data, V3
   Kim E, 2016, PROC SPIE, V9789, DOI 10.1117/12.2216468
   Kim M, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21020369
   KIMMEL R, 1995, IEEE T PATTERN ANAL, V17, P635, DOI 10.1109/34.387512
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lakhani P, 2017, RADIOLOGY, V284, P574, DOI 10.1148/radiol.2017162326
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   LeCun Y., 1990, ADV NEURAL INFORM PR, P396
   Leung CC, 2011, INT J TUBERC LUNG D, V15, P1279, DOI 10.5588/ijtld.11.0425
   Li CF, 2018, IEEE ACCESS, V6, P16060, DOI 10.1109/ACCESS.2018.2817023
   Li LH, 2001, ACAD RADIOL, V8, P629, DOI 10.1016/S1076-6332(03)80688-8
   Li XC, 2020, ARTIF INTELL MED, V103, DOI 10.1016/j.artmed.2019.101744
   Lin M, 2014, 2014 INTERNATIONAL CONFERENCE ON MEDICAL BIOMETRICS (ICMB 2014), P1, DOI 10.1109/ICMB.2014.8
   Litjens G, 2017, MED IMAGE ANAL, V42, P60, DOI 10.1016/j.media.2017.07.005
   Liu H, 2019, COMPUT MED IMAG GRAP, V75, P66, DOI 10.1016/j.compmedimag.2019.05.005
   Lo SCB, 1995, IEEE T MED IMAGING, V14, P711, DOI 10.1109/42.476112
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Luo H, 2000, 2000 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL II, PROCEEDINGS, P483
   Luo H, 2000, PROC CVPR IEEE, P452, DOI 10.1109/CVPR.2000.855854
   Ma JC, 2020, FRONT MED-PRC, V14, P450, DOI 10.1007/s11684-019-0726-4
   Madani A, 2018, I S BIOMED IMAGING, P1038, DOI 10.1109/ISBI.2018.8363749
   Mahapatra D, 2018, LECT NOTES COMPUT SC, V11046, P73, DOI 10.1007/978-3-030-00919-9_9
   Marques G, 2020, APPL SOFT COMPUT, V96, DOI 10.1016/j.asoc.2020.106691
   McInerney T, 1996, Med Image Anal, V1, P91, DOI 10.1016/S1361-8415(96)80007-7
   McNitt-Gray M.F., 1993, Pattern classification approach to segmentation of chest radiographs, P160
   Milletari F, 2016, INT CONF 3D VISION, P565, DOI 10.1109/3DV.2016.79
   Mittal A, 2018, WIRELESS PERS COMMUN, V101, P511, DOI 10.1007/s11277-018-5702-9
   Mooney P., 2018, CHEST X RAY IMAGES P
   Mould RF., 1993, CENTURY X RAYS RADIO
   Munadi K, 2020, IEEE ACCESS, V8, P217897, DOI 10.1109/ACCESS.2020.3041867
   Munawar F, 2020, IEEE ACCESS, V8, P153535, DOI 10.1109/ACCESS.2020.3017915
   Nair V., 2010, P 27 INT C MACHINE L, P807
   Neff T, 2017, PROC OAGM ARW JOINT, P140, DOI [10.3217/978-3-85125-524-9-30, 10.3217/978-3-85125- 524-9-30., DOI 10.3217/978-3-85125-524-9-30]
   Nie Dong, 2017, Med Image Comput Comput Assist Interv, V10435, P417, DOI 10.1007/978-3-319-66179-7_48
   Nijiati M., J X-RAY SCI TECHNOL, P1
   Noh H, 2015, IEEE I CONF COMP VIS, P1520, DOI 10.1109/ICCV.2015.178
   Novikov AA, 2018, IEEE T MED IMAGING, V37, P1865, DOI 10.1109/TMI.2018.2806086
   Oakden-Rayner L, 2020, ACAD RADIOL, V27, P106, DOI 10.1016/j.acra.2019.10.006
   Oken MM, 2011, JAMA-J AM MED ASSOC, V306, P1865, DOI 10.1001/jama.2011.1591
   Organization W.H, 2013, GLOBAL TUBERCULOSIS
   Panwar H, 2020, CHAOS SOLITON FRACT, V138, DOI 10.1016/j.chaos.2020.109944
   Park B, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-51832-3
   Park S., 2021, ARXIV PREPRINT ARXIV
   Pasa F, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-42557-4
   Petinaux B, 2011, AM J EMERG MED, V29, P18, DOI 10.1016/j.ajem.2009.07.011
   Pooch Eduardo H. P., 2020, Thoracic Image Analysis. Second International Workshop, TIA 2020. Held in Conjunction with MICCAI 2020. Proceedings. Lecture Notes in Computer Science (LNCS 12502), P74, DOI 10.1007/978-3-030-62469-9_7
   POWELL GF, 1988, MED PHYS, V15, P581, DOI 10.1118/1.596209
   Radford A., 2015, ARXIV
   Rahman T, 2020, IEEE ACCESS, V8, P191586, DOI 10.1109/ACCESS.2020.3031384
   Rahman T, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10093233
   Rajpurkar Pranav, 2017, ARXIV170701836
   Rakshit S, 2019, LECT NOTES ARTIF INT, V11509, P271, DOI 10.1007/978-3-030-20915-5_25
   Redmon J, 2018, Arxiv, DOI [arXiv:1804.02767, DOI 10.1109/CVPR.2017.690, DOI 10.48550/ARXIV.1804.02767]
   Rehman B, 2020, VISUAL COMPUT, V36, P633, DOI 10.1007/s00371-019-01649-y
   ROCHESTER N, 1956, IRE T INFORM THEOR, V2, P80, DOI 10.1109/TIT.1956.1056810
   Romeny BMT, 1999, LECT NOTES COMPUT SC, V1613, P56
   Romero M, 2020, MED PHYS, V47, P6246, DOI 10.1002/mp.14507
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   ROSENBLATT F, 1958, PSYCHOL REV, V65, P386, DOI 10.1037/h0042519
   Rusk N, 2016, NAT METHODS, V13, P35, DOI 10.1038/nmeth.3707
   SAIDY L, 2018, 2018 IEEE INT C CONS, P1
   Salimans T, 2016, ADV NEUR IN, V29
   Schilham AMR, 2006, MED IMAGE ANAL, V10, P247, DOI 10.1016/j.media.2005.09.003
   Sethian J., 1999, LEVEL SET METHODS FA
   Shah PK, 2003, RADIOLOGY, V226, P235, DOI 10.1148/radiol.2261011924
   Shao YQ, 2014, IEEE T MED IMAGING, V33, P1761, DOI 10.1109/TMI.2014.2305691
   Shi YH, 2008, IEEE T MED IMAGING, V27, P481, DOI 10.1109/TMI.2007.908130
   Shiraishi J, 2000, AM J ROENTGENOL, V174, P71, DOI 10.2214/ajr.174.1.1740071
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Smith RA, 2004, CA-CANCER J CLIN, V54, P41, DOI 10.3322/canjclin.54.1.41
   Souid A, 2021, APPL SCI-BASEL, V11, DOI 10.3390/app11062751
   Souza JC, 2019, COMPUT METH PROG BIO, V177, P285, DOI 10.1016/j.cmpb.2019.06.005
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Sze-To A, 2019, LECT NOTES COMPUT SC, V11663, P325, DOI 10.1007/978-3-030-27272-2_28
   Szegedy C, 2014, Arxiv, DOI [arXiv:1312.6199, DOI 10.1109/CVPR.2015.7298594]
   Szegedy Christian, 2016, P IEEE C COMP VIS PA, DOI DOI 10.1109/CVPR.2016.308
   Tajbakhsh N, 2020, MED IMAGE ANAL, V63, DOI 10.1016/j.media.2020.101693
   Tan MX, 2019, PR MACH LEARN RES, V97
   Tang YB, 2019, PR MACH LEARN RES, V102, P457
   Tang YX, 2019, I S BIOMED IMAGING, P1358, DOI [10.1109/ISBI.2019.8759442, 10.1109/isbi.2019.8759442]
   Tang YX, 2019, LECT NOTES COMPUT SC, V11769, P431, DOI 10.1007/978-3-030-32226-7_48
   van Ginneken B, 2006, MED IMAGE ANAL, V10, P19, DOI 10.1016/j.media.2005.02.002
   van Ginneken B, 2002, IEEE T MED IMAGING, V21, P924, DOI 10.1109/TMI.2002.803121
   van Ginneken B, 2002, IEEE T MED IMAGING, V21, P139, DOI 10.1109/42.993132
   van Ginneken B, 2001, IEEE T MED IMAGING, V20, P1228, DOI 10.1109/42.974918
   Vantaggiato E, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21051742
   Vaswani A, 2017, ADV NEUR IN, V30
   Vittitoe NF, 1998, MED PHYS, V25, P976, DOI 10.1118/1.598405
   Waheed A, 2020, IEEE ACCESS, V8, P91916, DOI 10.1109/ACCESS.2020.2994762
   Wang X., 2017, PROC CVPR IEEE, P2097, DOI [DOI 10.1109/CVPR.2017.369, 10.1109/CVPR.2017.369]
   Wang X, 2020, IEEE C EVOL COMPUTAT
   Wong KCL, 2019, PROC SPIE, V10950, DOI 10.1117/12.2513164
   Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634
   Xie Yuanpu, 2016, Med Image Comput Comput Assist Interv, V9901, P185, DOI 10.1007/978-3-319-46723-8_22
   Xu HW, 1996, MED PHYS, V23, P1613, DOI 10.1118/1.597738
   Xu T, 2009, IEEE ENG MED BIO, P3561, DOI 10.1109/IEMBS.2009.5334886
   XU XW, 1995, MED PHYS, V22, P617, DOI 10.1118/1.597549
   Xue Y, 2018, NEUROINFORMATICS, V16, P383, DOI 10.1007/s12021-018-9377-x
   Yan CC, 2018, ACM-BCB'18: PROCEEDINGS OF THE 2018 ACM INTERNATIONAL CONFERENCE ON BIOINFORMATICS, COMPUTATIONAL BIOLOGY, AND HEALTH INFORMATICS, P103, DOI 10.1145/3233547.3233573
   Yeh RA, 2017, PROC CVPR IEEE, P6882, DOI 10.1109/CVPR.2017.728
   Yi X, 2019, MED IMAGE ANAL, V58, DOI 10.1016/j.media.2019.101552
   Yosinski J, 2014, ADV NEUR IN, V27
   Yuan XH, 2007, 2007 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOLS 1-5, P2058
   Yunchao Wei, 2017, 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P6488, DOI 10.1109/CVPR.2017.687
   Zebin T, 2021, APPL INTELL, V51, P1010, DOI 10.1007/s10489-020-01867-1
   Zhang JJ, 2018, BIOMED SIGNAL PROCES, V43, P138, DOI 10.1016/j.bspc.2018.01.011
   Zhe L, 2018, PROC CVPR IEEE, P8290, DOI 10.1109/CVPR.2018.00865
   Zhenghao Shi, 2009, Proceedings of the 2009 Sixth International Conference on Fuzzy Systems and Knowledge Discovery (FSKD 2009), P428, DOI 10.1109/FSKD.2009.811
   Zoph B, 2018, PROC CVPR IEEE, P8697, DOI 10.1109/CVPR.2018.00907
NR 192
TC 24
Z9 25
U1 3
U2 45
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2023
VL 39
IS 3
BP 875
EP 913
DI 10.1007/s00371-021-02352-7
EA JAN 2022
PG 39
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 9E6QG
UT WOS:000740220700001
PM 35035008
OA Green Published, Bronze
DA 2024-07-18
ER

PT J
AU Wang, KK
   Zhang, GF
   Yang, J
AF Wang, Kangkan
   Zhang, Guofeng
   Yang, Jian
TI 3D human pose and shape estimation with dense correspondence from a
   single depth image
SO VISUAL COMPUTER
LA English
DT Article
DE 3D human pose and shape; Dense correspondence; 3D model fitting; Depth
   image; Deep learning
AB We propose a novel approach to estimate the 3D pose and shape of human bodies with dense correspondence from a single depth image. In contrast to most current 3D body model recovery methods from depth images that employ motion information of depth sequences to compute point correspondences, we reconstruct 3D human body models from a single depth image by combining the correspondence learning and the parametric model fitting. Specifically, a novel multi-view coarse-to-fine correspondence network is proposed by projecting a 3D template model into multi-view depth images. The proposed correspondence network can predict 2D flows of the input depth relative to each projected depth in a coarse-to-fine manner. The predicted multi-view flows are then aggregated to establish accurate dense point correspondences between the 3D template and the input depth with the known 3D-to-2D projection. Based on the learnt correspondences, the 3D human pose and shape represented by a parametric 3D body model are recovered through a model fitting method that incorporates an adversarial prior. We conduct extensive experiments on SURREAL, Human3.6M, DFAUST, and real depth data of human bodies. The experimental results demonstrate that the proposed method outperforms the state-of-the-art methods in terms of reconstruction accuracy.
C1 [Wang, Kangkan; Yang, Jian] Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Key Lab Intelligent Percept & Syst High Dimens In, Minist Educ,PCA Lab, Nanjing 210094, Peoples R China.
   [Wang, Kangkan; Yang, Jian] Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Jiangsu Key Lab Image & Video Understanding Socia, Nanjing 210094, Peoples R China.
   [Zhang, Guofeng] Zhejiang Univ, State Key Lab CAD&CG, Zijingang Campus, Hangzhou 310058, Peoples R China.
C3 Nanjing University of Science & Technology; Nanjing University of
   Science & Technology; Zhejiang University
RP Wang, KK (corresponding author), Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Key Lab Intelligent Percept & Syst High Dimens In, Minist Educ,PCA Lab, Nanjing 210094, Peoples R China.; Wang, KK (corresponding author), Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Jiangsu Key Lab Image & Video Understanding Socia, Nanjing 210094, Peoples R China.
EM wangkangkan@njust.edu.cn; zhangguofeng@cad.zju.edu.cn;
   csjyang@njust.edu.cn
FU Natural Science Foundation of China [61602444]; Fundamental Research
   Funds for the Central Universities [NJ2020023]; Open Project Program of
   the State Key Laboratory of Virtual Reality Technology and Systems of
   Beihang University [VRLAB2021C03]; Open Project Program of the State Key
   Laboratory of CAD&CG of Zhejiang University [A2106]; Open Project
   Program of the State Key Laboratory of Novel Software Technology of
   Nanjing University [KFKT2021B19]
FX This work was funded by the Natural Science Foundation of China (No.
   61602444). Thisworkwas also supported in part by the Fundamental
   Research Funds for the Central Universities (NJ2020023), in part by the
   Open Project Program of the State Key Laboratory of Virtual Reality
   Technology and Systems of Beihang University (No. VRLAB2021C03), in part
   by the Open Project Program of the State Key Laboratory of CAD&CG of
   Zhejiang University (Grant No. A2106), and in part by the Open Project
   Program of the State Key Laboratory of Novel Software Technology of
   Nanjing University (No. KFKT2021B19).
CR Anguelov D, 2005, ACM T GRAPHIC, V24, P408, DOI 10.1145/1073204.1073207
   [Anonymous], 2015, PROC CVPR IEEE, DOI [DOI 10.1109/CVPR.2015.7298801, 10.1109/CVPR.2015.7298801]
   Bai S, 2016, PROC CVPR IEEE, P5023, DOI 10.1109/CVPR.2016.543
   Bhatnagar Bharat Lal, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P311, DOI 10.1007/978-3-030-58536-5_19
   Bhatnagar Bharat Lal, 2020, Advances in Neural Information Processing Systems, V33, P12909
   Bogo F, 2017, PROC CVPR IEEE, P5573, DOI 10.1109/CVPR.2017.591
   Bogo F, 2016, LECT NOTES COMPUT SC, V9909, P561, DOI 10.1007/978-3-319-46454-1_34
   Chibane J, 2020, PROC CVPR IEEE, P6968, DOI 10.1109/CVPR42600.2020.00700
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Donati Nicolas, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8589, DOI 10.1109/CVPR42600.2020.00862
   Dosovitskiy A, 2015, IEEE I CONF COMP VIS, P2758, DOI 10.1109/ICCV.2015.316
   Dou MS, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925969
   Dou MS, 2015, PROC CVPR IEEE, P493, DOI 10.1109/CVPR.2015.7298647
   Graham B., 2015, BMVC
   Groueix T, 2018, LECT NOTES COMPUT SC, V11206, P235, DOI 10.1007/978-3-030-01216-8_15
   Güler RA, 2018, PROC CVPR IEEE, P7297, DOI 10.1109/CVPR.2018.00762
   Güler RA, 2017, PROC CVPR IEEE, P2614, DOI 10.1109/CVPR.2017.280
   Guo KW, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3083722
   Guo KW, 2015, IEEE I CONF COMP VIS, P3083, DOI 10.1109/ICCV.2015.353
   Habermann M, 2020, PROC CVPR IEEE, P5051, DOI 10.1109/CVPR42600.2020.00510
   Habermann M, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3311970
   Ilg E, 2017, PROC CVPR IEEE, P1647, DOI 10.1109/CVPR.2017.179
   Innmann M, 2016, LECT NOTES COMPUT SC, V9912, P362, DOI 10.1007/978-3-319-46484-8_22
   Ionescu C, 2014, IEEE T PATTERN ANAL, V36, P1325, DOI 10.1109/TPAMI.2013.248
   Kanazawa A, 2019, PROC CVPR IEEE, P5597, DOI 10.1109/CVPR.2019.00576
   Kanazawa A, 2018, PROC CVPR IEEE, P7122, DOI 10.1109/CVPR.2018.00744
   Kingma D. P., 2014, arXiv
   Klokov R, 2017, IEEE I CONF COMP VIS, P863, DOI 10.1109/ICCV.2017.99
   Kolotouros N, 2019, PROC CVPR IEEE, P4496, DOI 10.1109/CVPR.2019.00463
   Li SY, 2021, SCI CHINA INFORM SCI, V64, DOI 10.1007/s11432-020-3118-7
   Li YY, 2018, ADV NEUR IN, V31
   Liang YQ, 2020, INTEGR COMPUT-AID E, V27, P417, DOI 10.3233/ICA-200641
   Loper M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818013
   Loper M, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2661229.2661273
   NEWCOMBE RA, 2015, PROC CVPR IEEE, P343, DOI DOI 10.1109/CVPR.2015.7298631
   Park JJ, 2019, PROC CVPR IEEE, P165, DOI 10.1109/CVPR.2019.00025
   Pavlakos G, 2018, PROC CVPR IEEE, P459, DOI 10.1109/CVPR.2018.00055
   Peng Sida, 2021, CVPR
   PONSMOLL G, 2017, ACM T GRAPHIC, V36, P1, DOI DOI 10.1145/3072959.3073711
   Qi C., 2017, NeurIPS, P5105, DOI [10.1109/ CVPR.2017.16, DOI 10.1109/CVPR.2017.16]
   Ranjan A., 2018, BMVC
   Ranjan A, 2017, PROC CVPR IEEE, P2720, DOI 10.1109/CVPR.2017.291
   Saito S, 2020, INT CONF GLOBAL SOFT, P81, DOI 10.1145/3372787.3389301
   Saito S, 2019, IEEE I CONF COMP VIS, P2304, DOI 10.1109/ICCV.2019.00239
   Su H, 2015, IEEE I CONF COMP VIS, P945, DOI 10.1109/ICCV.2015.114
   Varol G, 2018, LECT NOTES COMPUT SC, V11211, P20, DOI 10.1007/978-3-030-01234-2_2
   Varol G, 2017, PROC CVPR IEEE, P4627, DOI 10.1109/CVPR.2017.492
   Wang KK, 2020, PROC CVPR IEEE, P7273, DOI 10.1109/CVPR42600.2020.00730
   Wang PS, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073608
   Wei LY, 2016, PROC CVPR IEEE, P1544, DOI 10.1109/CVPR.2016.171
   Weinzaepfel P, 2013, IEEE I CONF COMP VIS, P1385, DOI 10.1109/ICCV.2013.175
   Wu WX, 2019, PROC CVPR IEEE, P9613, DOI 10.1109/CVPR.2019.00985
   Ye GZ, 2012, LECT NOTES COMPUT SC, V7573, P828, DOI 10.1007/978-3-642-33709-3_59
   Ye M, 2014, PROC CVPR IEEE, P2353, DOI 10.1109/CVPR.2014.301
   Yu R, 2017, IEEE I CONF COMP VIS, P4733, DOI 10.1109/ICCV.2017.506
   Yu T, 2018, PROC CVPR IEEE, P7287, DOI 10.1109/CVPR.2018.00761
   Yu T, 2017, IEEE I CONF COMP VIS, P910, DOI 10.1109/ICCV.2017.104
   Yu Tao, 2021, CVPR
   Zeng W, 2020, PROC CVPR IEEE, P7052, DOI 10.1109/CVPR42600.2020.00708
   Zeng Y., CVPR
   Zhang C, 2017, PROC CVPR IEEE, P5484, DOI 10.1109/CVPR.2017.582
   Zhao HS, 2019, PROC CVPR IEEE, P5550, DOI 10.1109/CVPR.2019.00571
   Zheng ZR, 2018, LECT NOTES COMPUT SC, V11213, P389, DOI 10.1007/978-3-030-01240-3_24
   Zheng ZR, 2019, IEEE I CONF COMP VIS, P7738, DOI 10.1109/ICCV.2019.00783
   Zhou K, 2016, IEEE IC COMP COM NET
   Zhu H, 2019, PROC CVPR IEEE, P4486, DOI 10.1109/CVPR.2019.00462
NR 66
TC 4
Z9 4
U1 0
U2 29
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2023
VL 39
IS 1
BP 429
EP 441
DI 10.1007/s00371-021-02339-4
EA JAN 2022
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA F6UJ1
UT WOS:000739816600001
DA 2024-07-18
ER

PT J
AU Zhang, JQ
   Zhao, K
   Dong, B
   Fu, YK
   Wang, YX
   Yang, X
   Yin, BC
AF Zhang, Jiqing
   Zhao, Kai
   Dong, Bo
   Fu, Yingkai
   Wang, Yuxin
   Yang, Xin
   Yin, Baocai
TI Multi-domain collaborative feature representation for robust visual
   object tracking
SO VISUAL COMPUTER
LA English
DT Article
DE Visual object tracking; Event-based camera; Multi-domain; Challenging
   conditions
ID NETWORK; FLOWS
AB Jointly exploiting multiple different yet complementary domain information has been proven to be an effective way to perform robust object tracking. This paper focuses on effectively representing and utilizing complementary features from the frame domain and event domain for boosting object tracking performance in challenge scenarios. Specifically, we propose common features extractor to learn potential common representations from the RGB domain and event domain. For learning the unique features of the two domains, we utilize a unique extractor for event based on Spiking neural networks to extract edge cues in the event domain which may be missed in RGB in some challenging conditions, and a unique extractor for RGB based on deep convolutional neural networks to extract texture and semantic information in RGB domain. Extensive experiments on standard RGB benchmark and real event tracking dataset demonstrate the effectiveness of the proposed approach. We show our approach outperforms all compared state-of-the-art tracking algorithms and verify event-based data is a powerful cue for tracking in challenging scenes.
C1 [Zhang, Jiqing; Fu, Yingkai; Wang, Yuxin; Yang, Xin; Yin, Baocai] Dalian Univ Technol, Dalian, Peoples R China.
   [Zhao, Kai] Peking Univ, Adv Inst Informat Technol, Beijing, Peoples R China.
   [Dong, Bo] SRI Int, 333 Ravenswood Ave, Menlo Pk, CA 94025 USA.
C3 Dalian University of Technology; Peking University; SRI International
RP Yang, X (corresponding author), Dalian Univ Technol, Dalian, Peoples R China.
EM jqz@mail.dlut.edu.cn; kzhao@aiit.org.cn; bo.dong@sri.com;
   yingkaifu@mail.dlut.cdu.cn; wyx@dlut.edu.cn; xinyang@dlut.edu.cn;
   ybc@dlut.edu.cn
RI Wang, Yuxin/ABF-2724-2020
OI Zhao, Kai/0000-0002-5353-7413; Zhang, Jiqing/0000-0002-0061-5465; ,
   Xin/0000-0002-8046-722X
FU National Natural Science Foundation of China [91748104, 61972067];
   Innovation Technology Funding of Dalian [2018J11CY010, 2020JJ26GX036]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 91748104, Grant 61972067, and the
   Innovation Technology Funding of Dalian (Project No. 2018J11CY010,
   2020JJ26GX036).
CR [Anonymous], 2016, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2016.465
   Barranco F, 2018, IEEE INT C INT ROBOT, P5764, DOI 10.1109/IROS.2018.8593380
   Bertinetto L, 2016, LECT NOTES COMPUT SC, V9914, P850, DOI 10.1007/978-3-319-48881-3_56
   Bhat G, 2019, IEEE I CONF COMP VIS, P6181, DOI 10.1109/ICCV.2019.00628
   Bi Y, 2019, IEEE I CONF COMP VIS, P491, DOI 10.1109/ICCV.2019.00058
   Brandli Christian, 2014, IEEE J SOLID STATE C, V1
   Cadena, IEEE T IMAGE PROCESS
   Chen, 2020, P AAAI C ART INT
   Chen HS, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P473, DOI 10.1145/3343031.3350975
   Cohen GK, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00184
   Dai KN, 2019, PROC CVPR IEEE, P4665, DOI 10.1109/CVPR.2019.00480
   Danelljan M, 2019, PROC CVPR IEEE, P4655, DOI 10.1109/CVPR.2019.00479
   Danelljan M, 2017, PROC CVPR IEEE, P6931, DOI 10.1109/CVPR.2017.733
   Danelljan M, 2015, IEEE I CONF COMP VIS, P4310, DOI 10.1109/ICCV.2015.490
   Dong XW, 2018, IEEE CONF COMPUT
   Fan H, 2019, PROC CVPR IEEE, P7944, DOI 10.1109/CVPR.2019.00814
   Gehrig D, 2019, IEEE I CONF COMP VIS, P5632, DOI 10.1109/ICCV.2019.00573
   Gehrig M, 2020, IEEE INT CONF ROBOT, P4195, DOI [10.1109/ICRA40945.2020.9197133, 10.1109/icra40945.2020.9197133]
   GERSTNER W, 1995, PHYS REV E, V51, P738, DOI 10.1103/PhysRevE.51.738
   He AF, 2018, PROC CVPR IEEE, P4834, DOI 10.1109/CVPR.2018.00508
   Henriques JF, 2015, IEEE T PATTERN ANAL, V37, P583, DOI 10.1109/TPAMI.2014.2345390
   Huang LH, 2021, IEEE T PATTERN ANAL, V43, P1562, DOI 10.1109/TPAMI.2019.2957464
   Jung, 2018, P EUR C COMP VIS
   Kalal Z, 2012, IEEE T PATTERN ANAL, V34, P1409, DOI 10.1109/TPAMI.2011.239
   Kart U, 2018, INT C PATT RECOG, P2112, DOI 10.1109/ICPR.2018.8546179
   Kart Ugur, 2018, ECCV WORKSH
   Kepple, 2020, P EUR C COMP VIS
   Lan XY, 2018, AAAI CONF ARTIF INTE, P7008
   Li B, 2019, PROC CVPR IEEE, P4277, DOI 10.1109/CVPR.2019.00441
   Li B, 2018, PROC CVPR IEEE, P8971, DOI 10.1109/CVPR.2018.00935
   Li CL, 2019, IEEE INT CONF COMP V, P2262, DOI 10.1109/ICCVW.2019.00279
   Li CL, 2018, LECT NOTES COMPUT SC, V11217, P831, DOI 10.1007/978-3-030-01261-8_49
   Li PX, 2019, IEEE I CONF COMP VIS, P6161, DOI 10.1109/ICCV.2019.00626
   Li WC, 2022, IEEE T CYBERNETICS, V52, P3519, DOI 10.1109/TCYB.2020.2985398
   Mei HY, 2022, IEEE T CIRC SYST VID, V32, P1378, DOI 10.1109/TCSVT.2021.3069848
   Mei HY, 2020, PROC CVPR IEEE, P3684, DOI 10.1109/CVPR42600.2020.00374
   Mitrolchin A, 2018, IEEE INT C INT ROBOT, P6895, DOI 10.1109/IROS.2018.8593805
   Mostafavi M, 2021, INT J COMPUT VISION, V129, P900, DOI 10.1007/s11263-020-01410-2
   Mostafavi ISM, 2020, PROC CVPR IEEE, P2765, DOI 10.1109/CVPR42600.2020.00284
   Neftci EO, 2019, IEEE SIGNAL PROC MAG, V36, P51, DOI 10.1109/MSP.2019.2931595
   Pan LY, 2020, PROC CVPR IEEE, P1669, DOI 10.1109/CVPR42600.2020.00174
   Piatkowska E., 2012, P IEEE COMP SOC C CO, P35, DOI DOI 10.1109/CVPRW.2012.6238892
   Qiao Y, 2020, COMPUT GRAPH FORUM, V39, P565, DOI 10.1111/cgf.14168
   Ramesh B, 2021, IEEE T CIRC SYST VID, V31, P3996, DOI 10.1109/TCSVT.2020.3044287
   Rebecq H, 2019, PROC CVPR IEEE, P3852, DOI 10.1109/CVPR.2019.00398
   Ren WH, 2021, IEEE T IMAGE PROCESS, V30, P1439, DOI 10.1109/TIP.2020.3044219
   Scaramuzza D., 2018, P MACHINE LEARNING R, P969
   Shrestha S. B., 2018, Advances in Neural Information Processing Systems (NeurIPS), P1412
   Shrestha SB, 2018, IEEE T NEUR NET LEAR, V29, P3126, DOI 10.1109/TNNLS.2017.2713125
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song SR, 2013, IEEE I CONF COMP VIS, P233, DOI 10.1109/ICCV.2013.36
   Stoffregen T, 2019, IEEE I CONF COMP VIS, P7243, DOI 10.1109/ICCV.2019.00734
   Tavanaei A, 2019, NEURAL NETWORKS, V111, P47, DOI 10.1016/j.neunet.2018.12.002
   Tulyakov S, 2019, IEEE I CONF COMP VIS, P1527, DOI 10.1109/ICCV.2019.00161
   Valmadre J, 2017, PROC CVPR IEEE, P5000, DOI 10.1109/CVPR.2017.531
   Wang L, 2019, PROC CVPR IEEE, P10073, DOI 10.1109/CVPR.2019.01032
   Wang XC, 2017, IEEE T IMAGE PROCESS, V26, P4765, DOI 10.1109/TIP.2017.2723239
   Wu Y, 2015, IEEE T PATTERN ANAL, V37, P1834, DOI 10.1109/TPAMI.2014.2388226
   Xiao JJ, 2018, IEEE T CYBERNETICS, V48, P2485, DOI 10.1109/TCYB.2017.2740952
   Xu K, 2020, PROC CVPR IEEE, P2278, DOI 10.1109/CVPR42600.2020.00235
   Yang X, 2019, IEEE I CONF COMP VIS, P8808, DOI 10.1109/ICCV.2019.00890
   Yang X, 2018, ADV NEUR IN, V31
   Yang X, 2019, IEEE T MULTIMEDIA, V21, P328, DOI 10.1109/TMM.2018.2863602
   Yu Qiao, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13673, DOI 10.1109/CVPR42600.2020.01369
   Zenke F, 2018, NEURAL COMPUT, V30, P1514, DOI 10.1162/neco_a_01086
   Zhang, IEEE T CIRC SYST VID
   Zhang, 2020, 2020 IEEE INT C MULT
   Zhang L, 2019, IEEE IC COMP COM NET, DOI 10.1109/icccn.2019.8847081
   Zhang TZ, 2019, IEEE T PATTERN ANAL, V41, P365, DOI 10.1109/TPAMI.2018.2797062
   Zhang TZ, 2018, IEEE T IMAGE PROCESS, V27, P2676, DOI 10.1109/TIP.2017.2781304
   Zhang ZP, 2019, PROC CVPR IEEE, P4586, DOI 10.1109/CVPR.2019.00472
   Zhu QP, 2020, IEEE J EM SEL TOP C, V10, P557, DOI 10.1109/JETCAS.2020.3040329
   Zhu YB, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P465, DOI 10.1145/3343031.3350928
   Zhu Z., 2018, 2018 IEEE International Magnetics Conference (INTERMAG), DOI 10.1109/INTMAG.2018.8508710
NR 74
TC 6
Z9 6
U1 4
U2 34
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2021
VL 37
IS 9-11
SI SI
BP 2671
EP 2683
DI 10.1007/s00371-021-02237-9
EA AUG 2021
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA UL2RI
UT WOS:000683300600001
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Hu, K
   Wang, XC
   Hu, JP
   Wang, HF
   Qin, H
AF Hu, Kun
   Wang, Xiaochao
   Hu, Jianping
   Wang, Hongfei
   Qin, Hong
TI A novel robust zero-watermarking algorithm for medical images
SO VISUAL COMPUTER
LA English
DT Article
DE Medical images; Zero-watermarking; Bi-dimensional empirical mode
   decomposition; Singular value decomposition; Tampering detection
ID EMPIRICAL MODE DECOMPOSITION; MULTI-WATERMARKING; HILBERT SPECTRUM; EMD;
   COMPUTATION; TRANSFORM; DCT
AB A novel robust zero-watermarking algorithm for medical images is presented in this paper. The multi-scale decomposition of bi-dimensional empirical mode decomposition (BEMD) has exhibited many attractive properties that enable the proposed algorithm to robustly detect the tampering regions and protect the copyright of medical images simultaneously. Given a medical image, we first decompose a medical image adaptively into a finite number of intrinsic mode functions (IMFs) and a residue, by taking a full advantage of BEMD. The first IMF starts with the finest scale retaining fragile information and is best suitable for tampering detection, while the residue includes robust information at the coarser scale and is applied to the protection of intellectual property rights of medical images. Next, the feature matrices are extracted from the first IMF and the residue via singular value decomposition, which achieves robust performance subject to most attacks. For a given watermark image, it is encrypted by Arnold transform to enhance the security of the watermark. Then, the feature images are constructed by performing the exclusive-or operation between the encrypted watermark image and the extracted feature matrices. Finally, the feature images are securely stored in the copyright authentication database to be further used for copyright authentication and tampering detection. A large number of experimental results and comparisons with existing watermarking algorithms confirm that the newly proposed watermarking algorithm not only has strong ability on tampering detection, but also has better performance in combating various attacks, including cropping, Gaussian noise, median filtering, image enhancement attacks, etc. The newly developed algorithm also shows great promise in processing natural images.
C1 [Hu, Kun] Chinese Acad Sci, Beijing 100049, Peoples R China.
   [Wang, Xiaochao] Tiangong Univ, Sch Math Sci, Tianjin 300387, Peoples R China.
   [Hu, Jianping] Northeast Elect Power Univ, Sch Sci, Jilin 132012, Jilin, Peoples R China.
   [Wang, Hongfei] Chinese Acad Sci, Technol & Engn Ctr Space Utilizat, Key Lab Space Utilizat, Beijing 100094, Peoples R China.
   [Qin, Hong] SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11794 USA.
C3 Chinese Academy of Sciences; Tiangong University; Northeast Electric
   Power University; Chinese Academy of Sciences; Technology & Engineering
   Center for Space Utilization, CAS; State University of New York (SUNY)
   System; State University of New York (SUNY) Stony Brook
RP Wang, XC (corresponding author), Tiangong Univ, Sch Math Sci, Tianjin 300387, Peoples R China.; Wang, HF (corresponding author), Chinese Acad Sci, Technol & Engn Ctr Space Utilizat, Key Lab Space Utilizat, Beijing 100094, Peoples R China.
EM ucas_hukun@163.com; wangxiaochao18@163.com; neduhjp307@163.com;
   whf@csu.ac.cn; qin@cs.stonybrook.edu
RI Hu, Jianping/IWM-3698-2023; Wang, Hongfei/JCF-2357-2023
OI Hu, Jianping/0000-0001-8675-4564; Wang, Xiaochao/0000-0001-7852-2106
FU National Science Foundation of USA [IIS-1715985, IIS-1812606]; National
   Natural Science Foundation of China [61532002, 61672077, 61672149,
   61802279, 61872347]; Science & Technology Development Fund of Tianjin
   Education Commission for Higher Education [2018 KJ222]; Open Project
   Program of the State Key Lab of CAD&CG, Zhejiang University [A2105]
FX This research is supported in part byNational Science Foundation of USA
   (IIS-1715985 and IIS-1812606), National Natural Science Foundation of
   China (Nos. 61532002, 61672077, 61672149, 61802279, 61872347); The
   Science & Technology Development Fund of Tianjin Education Commission
   for Higher Education (Grant No. 2018 KJ222); The Open Project Program of
   the State Key Lab of CAD&CG (Grant No. A2105), Zhejiang University.
CR Ali H, 2015, EXPERT SYST APPL, V42, P1261, DOI 10.1016/j.eswa.2014.08.049
   Alshanbari HS, 2021, MULTIMED TOOLS APPL, V80, P16549, DOI 10.1007/s11042-020-08814-9
   Cedillo-Hernandez M, 2020, BIOMED SIGNAL PROCES, V56, DOI 10.1016/j.bspc.2019.101695
   Colominas MA, 2014, BIOMED SIGNAL PROCES, V14, P19, DOI 10.1016/j.bspc.2014.06.009
   Di CL, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0104663
   Fakhari P, 2011, DIGIT SIGNAL PROCESS, V21, P433, DOI 10.1016/j.dsp.2011.01.014
   Fengming Qin, 2020, Artificial Intelligence and Security. 6th International Conference (ICAIS 2020). Proceedings. Lecture Notes in Computer Science (LNCS 12240), P179, DOI 10.1007/978-3-030-57881-7_16
   Gong LH, 2021, MULTIMED TOOLS APPL, V80, P439, DOI 10.1007/s11042-020-09677-w
   Hu JP, 2016, COMPUT AIDED GEOM D, V43, P95, DOI 10.1016/j.cagd.2016.02.011
   Hu JP, 2014, GRAPH MODELS, V76, P340, DOI 10.1016/j.gmod.2014.03.006
   Hua ZY, 2018, SIGNAL PROCESS, V144, P134, DOI 10.1016/j.sigpro.2017.10.004
   Huang NE, 1998, P ROY SOC A-MATH PHY, V454, P903, DOI 10.1098/rspa.1998.0193
   Hurrah NN, 2020, MULTIMED TOOLS APPL, V79, P21441, DOI 10.1007/s11042-020-08988-2
   Katzenbeisser S., 2000, DIGITAL WATERMARKING
   Kopsinis Y, 2009, IEEE T SIGNAL PROCES, V57, P1351, DOI 10.1109/TSP.2009.2013885
   Li LJ, 2020, L N INST COMP SCI SO, V336, P439, DOI 10.1007/978-3-030-63095-9_28
   Liu J, 2019, CMC-COMPUT MATER CON, V61, P889, DOI 10.32604/cmc.2019.06034
   Liu J, 2019, APPL SCI-BASEL, V9, DOI 10.3390/app9040700
   Liu RZ, 2002, IEEE T MULTIMEDIA, V4, P121, DOI 10.1109/6046.985560
   LLOYD SP, 1982, IEEE T INFORM THEORY, V28, P129, DOI 10.1109/TIT.1982.1056489
   Nematzadeh H, 2018, OPT LASER ENG, V110, P24, DOI 10.1016/j.optlaseng.2018.05.009
   Niang O, 2012, IEEE T IMAGE PROCESS, V21, P3991, DOI 10.1109/TIP.2012.2199503
   Nyeem H, 2013, J DIGIT IMAGING, V26, P326, DOI 10.1007/s10278-012-9527-x
   Singh AK, 2015, J MED IMAG HEALTH IN, V5, P607, DOI 10.1166/jmihi.2015.1432
   Swaraja K, 2020, BIOMED SIGNAL PROCES, V55, DOI 10.1016/j.bspc.2019.101665
   Thanki R, 2017, ENG SCI TECHNOL, V20, P1366, DOI 10.1016/j.jestch.2017.06.001
   Tutte WT, 2000, CODING THEORY AND CRYPTOGRAPHY, P9
   Wang H, 2012, GRAPH MODELS, V74, P173, DOI 10.1016/j.gmod.2012.04.005
   Wang JW, 2017, MULTIDIM SYST SIGN P, V28, P617, DOI 10.1007/s11045-015-0363-2
   Wang XC, 2020, VISUAL COMPUT, V36, P2201, DOI 10.1007/s00371-020-01909-2
   Wang XC, 2018, COMPUT GRAPH-UK, V70, P118, DOI 10.1016/j.cag.2017.07.024
   Wang XC, 2018, COMPUT AIDED GEOM D, V59, P1, DOI 10.1016/j.cagd.2017.11.002
   Wang XC, 2015, VISUAL COMPUT, V31, P1135, DOI 10.1007/s00371-015-1100-4
   Wang YW, 2002, IEEE T IMAGE PROCESS, V11, P77, DOI 10.1109/83.982816
   Wu XQ, 2019, MULTIMED TOOLS APPL, V78, P8463, DOI 10.1007/s11042-018-6877-5
   Xu GL, 2009, PATTERN RECOGN, V42, P718, DOI 10.1016/j.patcog.2008.09.017
   Yuan XC, 2018, SIGNAL PROCESS, V149, P103, DOI 10.1016/j.sigpro.2018.03.007
   Zang Y, 2014, IEEE T VIS COMPUT GR, V20, P1253, DOI 10.1109/TVCG.2014.2298017
   Zear A, 2018, MULTIMED TOOLS APPL, V77, P4863, DOI 10.1007/s11042-016-3862-8
   Zhang DB, 2017, COMPUT AIDED DESIGN, V87, P1, DOI 10.1016/j.cad.2017.02.003
   Zhou ZL, 2023, IET IMAGE PROCESS, V17, P3660, DOI 10.1049/ipr2.12143
NR 41
TC 16
Z9 17
U1 4
U2 44
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2021
VL 37
IS 9-11
SI SI
BP 2841
EP 2853
DI 10.1007/s00371-021-02168-5
EA JUN 2021
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA UL2RI
UT WOS:000656809400001
DA 2024-07-18
ER

PT J
AU Zhang, A
   Chen, K
   Johan, H
   Erdt, M
AF Zhang, Alex
   Chen, Kan
   Johan, Henry
   Erdt, Marius
TI High-performance adaptive texture streaming and rendering of large 3D
   cities
SO VISUAL COMPUTER
LA English
DT Article
DE Real-time rendering; Texture streaming; Virtual texturing; 3D cities
AB We propose a high-performance texture streaming system for real-time rendering of large 3D cities with millions of textures. Our main contribution is a texture streaming system that automatically adjusts the streaming workload at runtime based on measured frame latencies, specifically addressing the high memory binding costs of hardware virtual texturing which causes frame rate stuttering. Our system streams textures in parallel with prioritization based on GPU computed mesh perceptibility, and these textures are cached in a sparse partially resident image at runtime without the need for a texture preprocessing step. In addition, we improve rendering quality by minimizing texture pop-in artifacts using a color blending scheme based on mipmap levels. We evaluate our texture streaming system using three structurally distinct datasets with many textures and compared it to a baseline, a game engine, and our prior method. Results show an 8X improvement in rendering performance and 7X improvement in rendering quality compared to the baseline.
C1 [Zhang, Alex; Chen, Kan] Fraunhofer Singapore, Singapore, Singapore.
   [Johan, Henry] Nanyang Technol Univ, Fraunhofer IDM NTU, Singapore, Singapore.
   [Erdt, Marius] Nanyang Technol Univ, Fraunhofer Singapore, Singapore, Singapore.
C3 Nanyang Technological University; Nanyang Technological University
RP Zhang, A (corresponding author), Fraunhofer Singapore, Singapore, Singapore.
EM alex.zhang@fraunhofer.sg; chen.kan@fraunhofer.sg; henryjohan@ntu.edu.sg;
   marius.erdt@fraunhofer.sg
OI Zhang, Alex/0000-0002-5440-3994
FU National Research Foundation, Singapore, under its International
   Research Centres in Singapore Funding Initiative
FX This research is supported by the National Research Foundation,
   Singapore, under its International Research Centres in Singapore Funding
   Initiative. Any opinions, findings and conclusions or recommendations
   expressed in this material are those of the authors and do not reflect
   the views of National Research Foundation, Singapore.
CR [Anonymous], 2008, ACM T GRAPHIC
   Berlin Senate Department for Economics, EN PUBL ENT
   Chen K., 2015, GAME DEVELOPERS C, P869
   Cline D, 1998, VISUALIZATION '98, PROCEEDINGS, P343, DOI 10.1109/VISUAL.1998.745322
   Cozzi P, 2011, 3D ENGINE DESIGN VIR, DOI [10.1201/9781439865583, DOI 10.1201/9781439865583]
   Han Charles., 2010, ACM Transactions on Graphics, V29, P1, DOI DOI 10.1145/1882261.1866193
   Hollemeersch Charles., 2010, GPU Pro: Advanced Rendering Techniques, P623
   Huang WM, 2018, INT J DIGIT EARTH, V11, P939, DOI 10.1080/17538947.2017.1365958
   Krämer M, 2015, WEB3D 2015, P188, DOI 10.1145/2775292.2775303
   Lefebvre S, 2004, INRIA
   LOYA A., 2008, COMPUTER GRAPHICS IN
   Mittring Martin., 2008, ACM SIGGRAPH Games, P23
   Mueller JH, 2018, SIGGRAPH ASIA'18: SIGGRAPH ASIA 2018 TECHNICAL PAPERS, DOI 10.1145/3272127.3275087
   Obert Juraj, 2012, ACM SIGGRAPH 2012 CO
   Schmitz P, 2020, IEEE COMPUT GRAPH, V40, P19, DOI 10.1109/MCG.2020.2974064
   service H.R.I.H, 2017, REAL MESH ENT HELS
   Tanner C. C., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P151, DOI 10.1145/280814.280855
   Van Waveren J., 2012, SOFTWARE VIRTUAL TEX
   Widmark M, 2012, TERRAIN BATTLEFIELD
   Zhang A, 2020, 2020 INTERNATIONAL CONFERENCE ON CYBERWORLDS (CW 2020), P17, DOI 10.1109/CW49994.2020.00011
NR 20
TC 4
Z9 6
U1 1
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2022
VL 38
IS 4
SI SI
BP 1245
EP 1262
DI 10.1007/s00371-021-02152-z
EA JUN 2021
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 0D9DX
UT WOS:000656381600001
OA hybrid
DA 2024-07-18
ER

PT J
AU Liu, TR
   Cai, YY
   Zheng, JM
   Thalmann, NM
AF Liu, Tianrui
   Cai, Yiyu
   Zheng, Jianmin
   Thalmann, Nadia Magnenat
TI BEACon: a boundary embedded attentional convolution network for point
   cloud instance segmentation
SO VISUAL COMPUTER
LA English
DT Article
DE 3D point cloud; Instance segmentation; Attentional convolution network
AB Motivated by how humans perceive geometry and color to recognize objects, we propose a boundary embedded attentional convolution (BEACon) network for point cloud instance segmentation. At the core of BEACon, we introduce the attentional weight in the convolution layer to adjust the neighboring features, with the weight being adapted to the relationship between geometry and color changes. As a result, BEACon makes use of both geometry and color information, takes instance boundary as an important feature, and thus learns a more discriminative feature representation in the neighborhood. Experimental results show that BEACon outperforms the state-of-the-art by a large margin. Ablation studies are also provided to prove the large benefit of incorporating both geometry and color into attention weight for instance segmentation.
C1 [Liu, Tianrui] Nanyang Technol Univ, Inst Media Innovat, Interdisciplinary Grad Sch, Singapore, Singapore.
   [Cai, Yiyu] Nanyang Technol Univ, Sch Mech & Aerosp Engn, Singapore, Singapore.
   [Zheng, Jianmin; Thalmann, Nadia Magnenat] Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore, Singapore.
C3 Nanyang Technological University; Nanyang Technological University;
   Nanyang Technological University
RP Liu, TR (corresponding author), Nanyang Technol Univ, Inst Media Innovat, Interdisciplinary Grad Sch, Singapore, Singapore.
EM tianrui001@e.ntu.edu.sg; myycai@ntu.edu.sg; asjmzheng@ntu.edu.sg;
   nadiathalmann@ntu.edu.sg
RI Zheng, Jianmin/A-3717-2011
OI Zheng, Jianmin/0000-0002-5062-6226; Liu, Tianrui/0000-0002-7532-3790;
   Thalmann, Nadia/0000-0002-1459-5960
FU NationalResearch Foundation, Singapore, under its International Research
   Centres in Singapore Funding Initiative
FX This research is supported by the NationalResearch Foundation,
   Singapore, under its International Research Centres in Singapore Funding
   Initiative. Any opinions, findings and conclusions or recommendations
   expressed in this material are those of the author(s) and do not reflect
   the views of National Research Foundation, Singapore.
CR [Anonymous], 2015, PROC CVPR IEEE, DOI [DOI 10.1109/CVPR.2015.7298801, 10.1109/CVPR.2015.7298801]
   [Anonymous], 2016, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, DOI [DOI 10.1109/CVPR.2016.170, 10.1109/CVPR.2016.170]
   Boulch A, 2018, COMPUT GRAPH-UK, V71, P189, DOI 10.1016/j.cag.2017.11.010
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851
   Choy C, 2019, PROC CVPR IEEE, P3070, DOI 10.1109/CVPR.2019.00319
   De Brabandere B, 2017, IEEE COMPUT SOC CONF, P478, DOI 10.1109/CVPRW.2017.66
   Graham B, 2018, PROC CVPR IEEE, P9224, DOI 10.1109/CVPR.2018.00961
   Groh Fabian., 2018, ACCV
   Hou J, 2019, PROC CVPR IEEE, P4416, DOI 10.1109/CVPR.2019.00455
   Jiang L, 2019, IEEE I CONF COMP VIS, P10432, DOI 10.1109/ICCV.2019.01053
   Komarichev A, 2019, PROC CVPR IEEE, P7413, DOI 10.1109/CVPR.2019.00760
   Lahoud J, 2019, IEEE I CONF COMP VIS, P9255, DOI 10.1109/ICCV.2019.00935
   Landrieu L, 2019, PROC CVPR IEEE, P7432, DOI 10.1109/CVPR.2019.00762
   Landrieu L, 2018, PROC CVPR IEEE, P4558, DOI 10.1109/CVPR.2018.00479
   Landrieu L, 2017, SIAM J IMAGING SCI, V10, P1724, DOI 10.1137/17M1113436
   Lei H, 2019, PROC CVPR IEEE, P9623, DOI 10.1109/CVPR.2019.00986
   Li HR, 2020, APPL SOFT COMPUT, V96, DOI 10.1016/j.asoc.2020.106593
   Li HR, 2020, SOFT COMPUT, V24, P6851, DOI 10.1007/s00500-019-04324-5
   Li HY, 2021, VISUAL COMPUT, V37, P325, DOI 10.1007/s00371-020-01801-z
   Li YY, 2018, ADV NEUR IN, V31
   Liu YC, 2019, PROC CVPR IEEE, P8887, DOI 10.1109/CVPR.2019.00910
   Maturana D, 2015, IEEE INT C INT ROBOT, P922, DOI 10.1109/IROS.2015.7353481
   Mo KC, 2019, PROC CVPR IEEE, P909, DOI 10.1109/CVPR.2019.00100
   Qi CR, 2017, ADV NEUR IN, V30
   Qi CR, 2017, PROC CVPR IEEE, P77, DOI 10.1109/CVPR.2017.16
   Pham QH, 2019, PROC CVPR IEEE, P8819, DOI 10.1109/CVPR.2019.00903
   Rethage D, 2018, LECT NOTES COMPUT SC, V11208, P625, DOI 10.1007/978-3-030-01225-0_37
   Sun YL, 2020, VISUAL COMPUT, V36, P2407, DOI 10.1007/s00371-020-01892-8
   Thomas H, 2019, IEEE I CONF COMP VIS, P6420, DOI 10.1109/ICCV.2019.00651
   Thomas H, 2018, INT CONF 3D VISION, P390, DOI 10.1109/3DV.2018.00052
   Wang C, 2018, LECT NOTES COMPUT SC, V11208, P56, DOI 10.1007/978-3-030-01225-0_4
   Wang L, 2019, PROC CVPR IEEE, P10288, DOI 10.1109/CVPR.2019.01054
   Wang WY, 2018, PROC CVPR IEEE, P2569, DOI 10.1109/CVPR.2018.00272
   Wang XL, 2019, PROC CVPR IEEE, P4091, DOI 10.1109/CVPR.2019.00422
   Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362
   Wu WX, 2019, PROC CVPR IEEE, P9613, DOI 10.1109/CVPR.2019.00985
   Xie SN, 2018, PROC CVPR IEEE, P4606, DOI 10.1109/CVPR.2018.00484
   Xu Y, 2018, ADV SOC SCI EDUC HUM, V284, P87
   Yang Brandon, 2019, NEURIPS
   Ye XQ, 2018, LECT NOTES COMPUT SC, V11211, P415, DOI 10.1007/978-3-030-01234-2_25
   Yi L, 2019, PROC CVPR IEEE, P3942, DOI 10.1109/CVPR.2019.00407
   Zhang SD, 2020, VISUAL COMPUT, V36, P1797, DOI 10.1007/s00371-019-01774-8
   Zhao HS, 2019, PROC CVPR IEEE, P5550, DOI 10.1109/CVPR.2019.00571
NR 43
TC 13
Z9 13
U1 0
U2 11
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2022
VL 38
IS 7
BP 2303
EP 2313
DI 10.1007/s00371-021-02112-7
EA APR 2021
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 2B0WL
UT WOS:000638016600001
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Gong, ZT
   Qin, N
   Zhang, GC
AF Gong, Zengtai
   Qin, Na
   Zhang, Guicang
TI Visible watermarking in document images using two-stage fuzzy inference
   system
SO VISUAL COMPUTER
LA English
DT Article
DE Binary removal attack; Document image; Gray-scale uniform distribution;
   Two-stage fuzzy inference system; Visible watermarking
ID NEURAL-NETWORKS; SCHEME
AB The key problem of visible watermarking is how to balance the watermark visibility, the security and the quality of marked image. In this paper, an adaptive visible watermarking scheme in document images using two-stage Mamdani fuzzy inference system (FIS) is presented. Firstly, four attribute parameters of document images including the neighborhood gray-scale value (G), skewness (Sk), entropy value (En) and standard deviation (Std) are defined as visible watermark embedding criteria. Secondly, the FIS1 and FIS2 are designed with different input and output parameters to get the adaptive intensity factors. In order to avoid the visible watermark being removed by the binary removal attack, the gray-scale uniform distribution method is used to remove the peak of the probability logarithmic histogram after the FIS1 stage. Finally, according to the results of FIS2 stage, the change of histogram is not obvious. To evaluate and analyze the performance of this scheme, the proposed scheme is compared with other previous visible watermarking schemes, and experiment results show that the presented one has better visual effect and less distortion.
C1 [Gong, Zengtai; Qin, Na; Zhang, Guicang] Northwest Normal Univ, Coll Math & Stat, Lanzhou, Peoples R China.
C3 Northwest Normal University - China
RP Qin, N (corresponding author), Northwest Normal Univ, Coll Math & Stat, Lanzhou, Peoples R China.
EM zt-gong@163.com; super_qn@126.com; zhanggc2020@126.com
OI Qin, Na/0000-0002-9511-3280
FU National Natural Science Foundation of China [61763044, 61861040]
FX This study was funded by the National Natural Science Foundation of
   China (Grant Nos. 61763044 and 61861040).
CR Aliev R, 2011, INFORM SCIENCES, V181, P1045, DOI 10.1016/j.ins.2010.11.021
   Ahmadi SBB, 2021, VISUAL COMPUT, V37, P385, DOI 10.1007/s00371-020-01808-6
   Chen CC, 2017, MULTIMED TOOLS APPL, V76, P8497, DOI 10.1007/s11042-016-3452-9
   Chetan KR, 2015, J INF SECUR APPL, V24-25, P13, DOI 10.1016/j.jisa.2015.07.002
   Daraee F, 2014, PATTERN RECOGN LETT, V35, P120, DOI 10.1016/j.patrec.2013.04.022
   Das A., 2015, Guide to signals and patterns in image processing: foundations, methods and applications
   Deeba F, 2020, IET IMAGE PROCESS, V14, P1005, DOI 10.1049/iet-ipr.2018.6040
   Ernawan F, 2020, VISUAL COMPUT, V36, P19, DOI 10.1007/s00371-018-1567-x
   Fragoso-Navarro E, 2018, IEEE ACCESS, V6, P75767, DOI 10.1109/ACCESS.2018.2883322
   Ghadi M, 2019, MULTIMED TOOLS APPL, V78, P15705, DOI 10.1007/s11042-018-6851-2
   Horng SJ, 2014, MULTIMED TOOLS APPL, V72, P3085, DOI 10.1007/s11042-013-1579-5
   Hu Y, 2006, IEEE T CIRC SYST VID, V16, P1423, DOI 10.1109/TCSVT.2006.884011
   Jagadeesh B, 2016, SOFT COMPUT, V20, P3679, DOI 10.1007/s00500-015-1729-y
   Kannammal A, 2014, INT J IMAG SYST TECH, V24, P111, DOI 10.1002/ima.22086
   Kapoor A, 2017, VISUAL COMPUT, V33, P665, DOI 10.1007/s00371-016-1216-1
   Khosravanian A, 2021, VISUAL COMPUT, V37, P1185, DOI 10.1007/s00371-020-01861-1
   Li XH, 2014, IEEE T GEOSCI REMOTE, V52, P7086, DOI 10.1109/TGRS.2014.2307354
   Lin YK, 2017, MULTIMED TOOLS APPL, V77, P1
   Liu L, 2020, VISUAL COMPUT, V36, P1521, DOI 10.1007/s00371-019-01746-y
   Liu TY, 2010, IEEE T IMAGE PROCESS, V19, P1224, DOI 10.1109/TIP.2010.2040757
   Loganathan A, 2016, EXPERT SYST APPL, V63, P412, DOI 10.1016/j.eswa.2016.05.019
   Lu HP, 2004, IEEE SIGNAL PROC LET, V11, P228, DOI 10.1109/LSP.2003.821748
   MahmoumGonbadi A, 2019, EXPERT SYST APPL, V131, P240, DOI 10.1016/j.eswa.2019.04.059
   Mohammad N, 2017, MULTIMED TOOLS APPL, V76, P13301, DOI 10.1007/s11042-016-3757-8
   Mortezaei R., 2010, 2010 International Conference on Electronics and Information Engineering (ICEIE 2010), P527, DOI 10.1109/ICEIE.2010.5559689
   Motwani Mukesh, 2009, Proceedings of the 2009 International Conference on Image Processing, Computer Vision, & Pattern Recognition. IPCV 2009, P321
   Oueslati S., 2010, Int J Image Process, V4, P218
   Papakostas GA, 2016, COMPLEX INTELL SYST, V2, P205, DOI 10.1007/s40747-016-0023-7
   Perfilieva I, 2006, FUZZY SET SYST, V157, P993, DOI 10.1016/j.fss.2005.11.012
   Rajpal A, 2019, APPL SOFT COMPUT, V74, P603, DOI 10.1016/j.asoc.2018.10.043
   Sakr N, 2005, 2005 IEEE International Workshop on Haptic Audio Visual Environments and their Applications, P121
   Su QT, 2018, SOFT COMPUT, V22, P91, DOI 10.1007/s00500-017-2489-7
   Tan LN, 2019, MULTIMED TOOLS APPL, V78, P13189, DOI 10.1007/s11042-018-5771-5
   Xu JY, 2015, VISUAL COMPUT, V31, P1653, DOI 10.1007/s00371-014-1045-z
   Yang Y, 2009, IEEE T CIRC SYST VID, V19, P656, DOI 10.1109/TCSVT.2009.2017401
   Yao YZ, 2019, SIGNAL PROCESS, V164, P386, DOI 10.1016/j.sigpro.2019.06.034
   Youssef SM, 2014, MULTIMED TOOLS APPL, V73, P1545, DOI 10.1007/s11042-013-1515-8
   YUAN ZH, 2020, VISUAL COMPUT 0730
NR 38
TC 5
Z9 5
U1 0
U2 14
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2022
VL 38
IS 2
BP 707
EP 718
DI 10.1007/s00371-020-02045-7
EA JAN 2021
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZE7IA
UT WOS:000604151000001
DA 2024-07-18
ER

PT J
AU Mansouri, A
   Wang, XY
AF Mansouri, Ali
   Wang, Xingyuan
TI Image encryption using shuffled Arnold map and multiple values
   manipulations
SO VISUAL COMPUTER
LA English
DT Article
DE Image encryption; Chaos theory; Arnold map; Confusion and diffusion
AB In this paper, we present an image encryption scheme based on improved Arnold map. The improvement in the Arnold map includes a Divide & Rotate operation and pixels shuffling. The obtained shuffled Arnold map shows better results in terms of performance and speed. The proposed encryption scheme applies a preprocessing procedure on the plain image. We use the Shuffled Arnold map in the confusion process for only one round. For the diffusion process, we execute a Forward & Backward process to apply an integer-level values manipulation. The evaluation of the proposed method shows high sensitivity and resistance against common attacks.
C1 [Mansouri, Ali; Wang, Xingyuan] Dalian Univ Technol, Fac Elect Informat & Elect Engn, Dalian 116024, Peoples R China.
   [Wang, Xingyuan] Dalian Maritime Univ, Sch Informat Sci & Technol, Dalian 116026, Peoples R China.
C3 Dalian University of Technology; Dalian Maritime University
RP Mansouri, A (corresponding author), Dalian Univ Technol, Fac Elect Informat & Elect Engn, Dalian 116024, Peoples R China.
EM mansouriali@mail.dlut.edu.cn; wangxy@dlut.edu.cn
RI MANSOURI, Ali/AGE-4266-2022; Wang, Xing-yuan/I-6353-2015
OI MANSOURI, Ali/0000-0002-3418-8166; 
CR Alvarez G, 2006, INT J BIFURCAT CHAOS, V16, P2129, DOI 10.1142/S0218127406015970
   Chen L, 2017, NONLINEAR DYNAM, V87, P1797, DOI 10.1007/s11071-016-3153-y
   Elshamy AM, 2016, OPT QUANT ELECTRON, V48, DOI 10.1007/s11082-016-0461-x
   Hofmann G. R., 1993, Visual Computer, V9, P303, DOI 10.1007/BF01901911
   Hua ZY, 2019, INFORM SCIENCES, V480, P403, DOI 10.1016/j.ins.2018.12.048
   Hua ZY, 2018, SIGNAL PROCESS, V149, P148, DOI 10.1016/j.sigpro.2018.03.010
   Hua ZY, 2016, INFORM SCIENCES, V339, P237, DOI 10.1016/j.ins.2016.01.017
   Hua ZY, 2015, INFORM SCIENCES, V297, P80, DOI 10.1016/j.ins.2014.11.018
   Li CQ, 2014, NONLINEAR DYNAM, V78, P1545, DOI 10.1007/s11071-014-1533-8
   Li GD, 2019, VISUAL COMPUT, V35, P1267, DOI 10.1007/s00371-018-1574-y
   Lin CH, 2010, VISUAL COMPUT, V26, P1101, DOI [10.1007/s00371-010-0461-y, 10.1007/s00371-010-0461]
   Liu LF, 2016, IET SIGNAL PROCESS, V10, P1096, DOI 10.1049/iet-spr.2015.0522
   Liu YS, 2016, NONLINEAR DYNAM, V84, P2241, DOI 10.1007/s11071-016-2642-3
   Luo YQ, 2019, MULTIMED TOOLS APPL, V78, P22023, DOI 10.1007/s11042-019-7453-3
   Mirzaei O, 2012, NONLINEAR DYNAM, V67, P557, DOI 10.1007/s11071-011-0006-6
   Norouzi B, 2014, NONLINEAR DYNAM, V78, P995, DOI 10.1007/s11071-014-1492-0
   Pareschi F, 2010, IEEE T CIRCUITS-I, V57, P3124, DOI 10.1109/TCSI.2010.2052515
   Ramalingam B, 2018, MULTIMED TOOLS APPL, V77, P11669, DOI 10.1007/s11042-017-4811-x
   Schopf H.-G., 1970, ZAMM J APPL MATH MEC, V50, DOI [10.1002/zamm.19700500721, DOI 10.1002/ZAMM.19700500721]
   Seyedzadeh SM, 2015, NONLINEAR DYNAM, V81, P511, DOI 10.1007/s11071-015-2008-2
   Tu SC, 2010, VISUAL COMPUT, V26, P1177, DOI 10.1007/s00371-009-0398-1
   Wang XY, 2020, INFORM SCIENCES, V507, P16, DOI 10.1016/j.ins.2019.08.041
   Wang XY, 2019, INFORM SCIENCES, V486, P340, DOI 10.1016/j.ins.2019.02.049
   Wang Y, 2011, APPL SOFT COMPUT, V11, P514, DOI 10.1016/j.asoc.2009.12.011
   Wells P.N. T., 2001, PHYSIOL MEAS, V22, P263
   Wu Y., 2011, J SELECTED AREAS TEL, V1, P31
   Wu Y, 2013, INFORM SCIENCES, V222, P323, DOI 10.1016/j.ins.2012.07.049
   Wu Y, 2012, J ELECTRON IMAGING, V21, DOI 10.1117/1.JEI.21.1.013014
   Xie EY, 2017, SIGNAL PROCESS, V132, P150, DOI 10.1016/j.sigpro.2016.10.002
   Ye GD, 2012, NONLINEAR DYNAM, V69, P2079, DOI 10.1007/s11071-012-0409-z
   Ye RS, 2011, OPT COMMUN, V284, P5290, DOI 10.1016/j.optcom.2011.07.070
   Zhan K, 2017, J ELECTRON IMAGING, V26, DOI 10.1117/1.JEI.26.1.013021
   Zhang YQ, 2014, NONLINEAR DYNAM, V77, P687, DOI 10.1007/s11071-014-1331-3
   Zhang Y, 2018, MULTIMED TOOLS APPL, V77, P6647, DOI 10.1007/s11042-017-4577-1
   Zhou YC, 2013, SIGNAL PROCESS, V93, P3039, DOI 10.1016/j.sigpro.2013.04.021
NR 35
TC 30
Z9 32
U1 6
U2 56
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2021
VL 37
IS 1
BP 189
EP 200
DI 10.1007/s00371-020-01791-y
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA QE3UO
UT WOS:000616134400015
DA 2024-07-18
ER

PT J
AU Su, TY
   Wang, W
   Liu, HX
   Liu, ZD
   Li, XF
   Jia, Z
   Zhou, L
   Song, ZL
   Ding, M
   Cui, AJ
AF Su, Tianyun
   Wang, Wen
   Liu, Haixing
   Liu, Zhendong
   Li, Xinfang
   Jia, Zhen
   Zhou, Lin
   Song, Zhuanling
   Ding, Ming
   Cui, Aiju
TI An adaptive and rapid 3D Delaunay triangulation for randomly distributed
   point cloud data
SO VISUAL COMPUTER
LA English
DT Article
DE Delaunay triangulation; Point cloud data; Multi-grid; Incremental
   algorithm; Adaptive Hilbert curve
ID ALGORITHM
AB Incremental algorithms are among the most popular approaches for Delaunay triangulation, and the point insertion sequence has a substantial impact on the amount of work needed to construct Delaunay triangulations in incremental algorithm triangulation. In this paper, 2D adaptive Hilbert curve insertion, including the method of dividing 3D multi-grids and adjusting the 3D adaptive Hilbert curve to avoid the "jump" phenomenon, is extended to 3D Delaunay triangulation. In addition, on the basis of adaptive Hilbert curve insertion, we continue to optimize the addition of control points by selecting control points in every order and every grid level. As a result, the number of conflicting elongated tetrahedra that have to be created and deleted multiple times and the number of search steps for positioning inserted points can both be reduced. Lastly, a new comparison method is used in the point location process to solve the precision problem in 3D Delaunay triangulation. As shown by detailed experiments and analysis, compared with previous adaptive Hilbert curve insertion, CGAL, regular grid insertion, multi-grid insertion and random insertion, the proposed 3D Delaunay triangulation is the most efficient for both artificial and real surface sampling point sets.
C1 [Su, Tianyun; Wang, Wen; Liu, Haixing; Liu, Zhendong; Li, Xinfang; Jia, Zhen; Zhou, Lin; Song, Zhuanling; Ding, Ming; Cui, Aiju] MNR, Inst Oceanog 1, Marine Data & Informat Ctr, Qingdao, Peoples R China.
   [Su, Tianyun; Liu, Haixing; Li, Xinfang] Pilot Natl Lab Marine Sci & Technol, Lab Reg Oceanog & Numer Modeling, Qingdao, Peoples R China.
   [Su, Tianyun; Liu, Zhendong] Natl Engn Lab Integrated Aerosp Ground Ocean Big, Xian, Peoples R China.
   [Liu, Haixing] Minist Nat Resources, Key Lab Marine Sci & Numer Modeling, Qingdao, Peoples R China.
C3 Laoshan Laboratory; Ministry of Natural Resources of the People's
   Republic of China
RP Wang, W (corresponding author), MNR, Inst Oceanog 1, Marine Data & Informat Ctr, Qingdao, Peoples R China.
EM wangwen@fio.org.cn
FU National Key Research and Development Project [2016YFA0602200]; National
   Natural Science Foundation of China [U1806205]
FX This study was funded by the National Key Research and Development
   Project (Grant Number 2016YFA0602200) and the National Natural Science
   Foundation of China (Grant Number U1806205).
CR Amenta N., 2003, PROC 19 ANN ACM SYMP, P211, DOI DOI 10.1145/777792.777824
   Barber C.B, 1993, COMPUTATIONAL GEOMET
   Barnsley M.F, 1988, The Science of Fractal Images
   Boissonnat JD, 2009, PROCEEDINGS OF THE TWENTY-FIFTH ANNUAL SYMPOSIUM ON COMPUTATIONAL GEOMETRY (SCG'09), P208, DOI 10.1145/1542362.1542403
   Borouchaki H, 1995, COMPUT METHOD APPL M, V128, P153, DOI 10.1016/0045-7825(95)00854-1
   Borouchaki H, 1996, INT J NUMER METH ENG, V39, P3407, DOI 10.1002/(SICI)1097-0207(19961030)39:20<3407::AID-NME5>3.0.CO;2-C
   BOWYER A, 1981, COMPUT J, V24, P162, DOI 10.1093/comjnl/24.2.162
   Buchin K, 2008, THESIS
   Buchin K, 2009, LECT NOTES COMPUT SC, V5757, P119, DOI 10.1007/978-3-642-04128-0_11
   Cao Thanh-Tung., 2014, P 18 M ACM SIGGRAPH, P47, DOI DOI 10.1145/2556700.2556710
   Navarro JGC, 2017, PROC INT CONF INTELL, P36
   Chao Y, 2015, INT CONF GEOINFORM, DOI 10.1109/GEOINFORMATICS.2015.7378671
   Cignoni P., 1993, Computer Graphics Forum, V12, pC129, DOI 10.1111/1467-8659.1230129
   Delaunay B., 1934, IZV AKAD NAUK SSSR O, V7, P1
   Devillers O, 2003, SIAM PROC S, P37
   Devillers O., 2002, International Journal of Foundations of Computer Science, V13, P163, DOI 10.1142/S0129054102001035
   Devillers O., 2003, 5 WORKSH ALG ENG EXP
   Dwyer R.A., 1989, DISCRETE COMPUT GEOM
   FORTUNE S, 1987, ALGORITHMICA, V2, P153, DOI 10.1007/BF01840357
   Frisken S.F., 2002, J. Graph. Tools, V7, P1, DOI DOI 10.1080/10867651.2002.10487560
   Fu HH, 2016, SCI CHINA INFORM SCI, V59, DOI 10.1007/s11432-016-5588-7
   Funke D., 2017, PARALLEL D D DELAUNA, P207, DOI [10.1137/1.9781611974768.17, DOI 10.1137/1.9781611974768.17]
   GREEN PJ, 1978, COMPUT J, V21, P168, DOI 10.1093/comjnl/21.2.168
   Hilbert David, 1891, MATH ANN, V38, P459, DOI [DOI 10.1007/BF01199431, 10.1007/bf01199431]
   Kolingerová I, 2002, VISUAL COMPUT, V18, P511, DOI 10.1007/s00371-002-0173-z
   Lawson C. L., 1977, MATH SOFTWARE, P161, DOI DOI 10.1016/B978-0-12-587260-7.50011-X
   LEWIS BA, 1978, COMPUT J, V21, P324, DOI 10.1093/comjnl/21.4.324
   Liu JF, 2013, ACTA MECH SINICA-PRC, V29, P99, DOI 10.1007/s10409-013-0001-x
   Liu X, 2009, VISUAL COMPUT, V25, P381, DOI 10.1007/s00371-009-0329-1
   Liu Y, 2005, COMBIN COMPUT GEOM, V52, P56
   [刘岩 Liu Yan], 2012, [中国科学. 物理学, 力学, 天文学, Scientia Sinica Physica, Mechanica & Astronomica], V42, P192
   Lo SH, 2015, FINITE ELEM ANAL DES, V102-103, P65, DOI 10.1016/j.finel.2015.05.003
   Lo SH, 2014, FINITE ELEM ANAL DES, V90, P113, DOI 10.1016/j.finel.2014.07.002
   Lo SH, 2013, FINITE ELEM ANAL DES, V63, P8, DOI 10.1016/j.finel.2012.08.005
   Lo SH, 2012, COMPUT METHOD APPL M, V237, P88, DOI 10.1016/j.cma.2012.05.009
   Marot C., 2017, 26 INT MESHING ROUND
   Marot C, 2019, INT J NUMER METH ENG, V117, P967, DOI 10.1002/nme.5987
   Nave D, 2004, COMP GEOM-THEOR APPL, V28, P191, DOI 10.1016/j.comgeo.2004.03.009
   Peixinho AZ, 2018, SIBGRAPI, P384, DOI 10.1109/SIBGRAPI.2018.00056
   Quinn JA, 2006, LECT NOTES COMPUT SC, V4077, P465
   Remacle JF, 2015, PROCEDIA ENGINEER, V124, P6, DOI 10.1016/j.proeng.2015.10.118
   SLOAN SW, 1987, ADV ENG SOFTW WORKST, V9, P34, DOI 10.1016/0141-1195(87)90043-X
   Su TY, 2016, COMPUT GRAPH-UK, V54, P65, DOI 10.1016/j.cag.2015.07.019
   Teillaud M, 1999, 3 DIMENSIONAL TRIANG
   Tung CT, 2009, COMP NUS ED SG, V19, P736
   Tychonievich LA, 2010, VISUAL COMPUT, V26, P1485, DOI 10.1007/s00371-010-0506-2
   Wang W, 2019, COMPUT GRAPH-UK, V84, P144, DOI 10.1016/j.cag.2019.08.002
   WATSON DF, 1981, COMPUT J, V24, P167, DOI 10.1093/comjnl/24.2.167
   Zhou S, 2005, INFORM PROCESS LETT, V93, P37, DOI 10.1016/j.ipl.2004.09.020
NR 49
TC 10
Z9 12
U1 1
U2 29
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2022
VL 38
IS 1
BP 197
EP 221
DI 10.1007/s00371-020-02011-3
EA NOV 2020
PG 25
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA YW6DE
UT WOS:000591272700001
DA 2024-07-18
ER

PT J
AU Kita, N
   Miyata, K
AF Kita, Naoki
   Miyata, Kazunori
TI Computational design of polyomino puzzles
SO VISUAL COMPUTER
LA English
DT Article
DE Computational design; Graph partitioning; Puzzles
ID 3D OBJECTS
AB People of all ages enjoy solving geometric puzzles. However, finding suitable puzzles, e.g., puzzles with a moderate level of difficulty or puzzles with intellectually stimulating shapes can be difficult. In addition, designing innovative and appealing puzzles requires demanding effort and, typically, involves many trial and error processes. In this paper, we introduce a computational approach for designing geometric puzzles. Existing approaches employ bottom-up, constructive algorithms to generate puzzle pieces; therefore, intervening in the piece generation procedure is difficult. Differing from existing approaches that generate puzzles automatically or semi-automatically, we propose a top-down, partitioning-based approach, that enables us to control and edit piece shapes. With a subtle modification, the proposed algorithm can be easily extended to both 3D polycube and 2D polyomino puzzle design. To generate a variety of piece shapes, the proposed approach involves a capacity-constrained graph partitioning algorithm combined with polyomino tiling. We demonstrate the versatility of the proposed approach through various example designs, including fabricated puzzles, created using the proposed method.
C1 [Kita, Naoki] Tokyo Univ Agr & Technol, Tokyo, Japan.
   [Miyata, Kazunori] Japan Adv Inst Sci & Technol, Nomi, Ishikawa, Japan.
C3 Tokyo University of Agriculture & Technology; Japan Advanced Institute
   of Science & Technology (JAIST)
RP Kita, N (corresponding author), Tokyo Univ Agr & Technol, Tokyo, Japan.
EM nkita@go.tuat.ac.jp
RI Kita, Naoki/G-8880-2019
OI Miyata, Kazunori/0000-0002-1582-0058
FU JSPS KAKENHI [19K24338, 20K19944]; Grants-in-Aid for Scientific Research
   [19K24338, 20K19944] Funding Source: KAKEN
FX We thank all the anonymous reviewers for their helpful comments and
   feedback. This work was supported by JSPS KAKENHI Grant Nos. 19K24338
   and 20K19944.
CR [Anonymous], 2018, GUROBI GUROBI OPTIMI
   [Anonymous], 1994, Polyominoes, puzzles, patterns, problems, and packagings
   [Anonymous], 2000, Millennial Perspectives in Computer Science
   Araújo C, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3323004
   Attene M, 2015, COMPUT GRAPH FORUM, V34, P64, DOI 10.1111/cgf.12608
   Balzer M, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531392
   Chen XL, 2018, SIGGRAPH ASIA'18: SIGGRAPH ASIA 2018 TECHNICAL PAPERS, DOI 10.1145/3272127.3275033
   Chen XL, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818087
   Coffin S., 2006, Geometric Puzzle Design
   Duncan N, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130831
   Fu CW, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766892
   Hu RZ, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2661229.2661244
   Kita N, 2020, COMPUT GRAPH-UK, V90, P21, DOI 10.1016/j.cag.2020.05.005
   Lau C, 2014, P ACM WORKSH NONPH A, P31
   Li SH, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275061
   Lo KY, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618503
   Luo Linjie., 2012, ACM T GRAPHIC
   Peng CH, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601164
   Song P., 2017, ACM Transactions on Graphics (TOG), V36
   Song P, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925876
   Song P, 2015, COMPUT AIDED GEOM D, V35-36, P137, DOI 10.1016/j.cagd.2015.03.020
   Song P, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366147
   Sun T, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766961
   Tang KK, 2019, COMPUT GRAPH FORUM, V38, P291, DOI 10.1111/cgf.13638
   van de Kerkhof M, 2019, COMPUT GRAPH FORUM, V38, P343, DOI 10.1111/cgf.13642
   Vanek J, 2014, COMPUT GRAPH FORUM, V33, P322, DOI 10.1111/cgf.12353
   Xin SQ, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964992
   Yao JX, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3054740
   Yu MJ, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3269979
   Yuan Y, 2018, COMPUT GRAPH FORUM, V37, P103, DOI 10.1111/cgf.13516
   Zhou YH, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601173
   Zhou YJ, 2012, PROCEEDINGS OF THE 2012 SYMPOSIUM ON PIEZOELECTRICITY, ACOUSTIC WAVES AND DEVICE APPLICATIONS (SPAWDA12), P49, DOI 10.1109/SPAWDA.2012.6464033
NR 32
TC 7
Z9 7
U1 1
U2 12
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2021
VL 37
IS 4
SI SI
BP 777
EP 787
DI 10.1007/s00371-020-01968-5
EA SEP 2020
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RL0LI
UT WOS:000566900900002
DA 2024-07-18
ER

PT J
AU Elmahmudi, A
   Ugail, H
AF Elmahmudi, Ali
   Ugail, Hassan
TI A framework for facial age progression and regression using exemplar
   face templates
SO VISUAL COMPUTER
LA English
DT Article
DE Age progression and regression; Average face; Template-based face
   generation; Face similarity
ID RECOGNITION; CLASSIFICATION; GENDER; COLOR; SHAPE
AB Techniques for facial age progression and regression have many applications and a myriad of challenges. As such, automatic aged or de-aged face generation has become an important subject of study in recent times. Over the past decade or so, researchers have been working on developing face processing mechanisms to tackle the challenge of generating realistic aged faces for applications related to smart systems. In this paper, we propose a novel approach to try and address this problem. We use template faces based on the formulation of an average face of a given ethnicity and for a given age. Thus, given a face image, the target aged image for that face is generated by applying it to the relevant template face image. The resulting image is controlled by two parameters corresponding to the texture and the shape of the face. To validate our approach, we compute the similarity between aged images and the corresponding ground truth via face recognition. To do this, we have utilised a pre-trained convolutional neural network based on the VGG-face model for feature extraction, and we then use well-known classifiers to compare the features. We have utilised two datasets, namely the FEI and the Morph II, to test, verify and validate our approach. Our experimental results do suggest that the proposed approach achieves accuracy, efficiency and possess flexibility when it comes to facial age progression or regression.
C1 [Elmahmudi, Ali; Ugail, Hassan] Univ Bradford, Ctr Visual Comp, Fac Engn & Informat, Bradford, W Yorkshire, England.
C3 University of Bradford
RP Ugail, H (corresponding author), Univ Bradford, Ctr Visual Comp, Fac Engn & Informat, Bradford, W Yorkshire, England.
EM a.a.m.elmahmudi@bradford.ac.uk; h.ugail@bradford.ac.uk
CR Abdurrahim SH, 2018, VISUAL COMPUT, V34, P1617, DOI 10.1007/s00371-017-1428-z
   Amarappa S., 2014, International Journal of Electronics and Computer Science Engineering, V3, P435
   Brunet D, 2012, IEEE T IMAGE PROCESS, V21, P1488, DOI 10.1109/TIP.2011.2173206
   Bukar AM, 2015, INT CONF IMAG PROC, P285, DOI 10.1109/IPTA.2015.7367147
   Bukar A.M., 2017, CONVNET FEATURES AGE
   BURT DM, 1995, P ROY SOC B-BIOL SCI, V259, P137, DOI 10.1098/rspb.1995.0021
   Changseok Choi, 1999, FUZZ-IEEE'99. 1999 IEEE International Fuzzy Systems. Conference Proceedings (Cat. No.99CH36315), P1603, DOI 10.1109/FUZZY.1999.790144
   Chen BC, 2014, LECT NOTES COMPUT SC, V8694, P768, DOI 10.1007/978-3-319-10599-4_49
   Chen JC, 2016, IEEE WINT CONF APPL
   Chu YJ, 2019, VISUAL COMPUT, V35, P239, DOI 10.1007/s00371-017-1468-4
   Cootes T., 2008, The fg-net aging database
   Dehshibi MM, 2019, VISUAL COMPUT, V35, P23, DOI 10.1007/s00371-017-1442-1
   Dong P, 2002, 2002 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL III, PROCEEDINGS, P489, DOI 10.1109/ICIP.2002.1039014
   Elmahmudi A, 2019, FUTURE GENER COMP SY, V99, P213, DOI 10.1016/j.future.2019.04.025
   FREDJ HB, VIS COMPUT
   Gao Y, 2017, IEEE T IMAGE PROCESS, V26, P2545, DOI 10.1109/TIP.2017.2675341
   Gogic I, 2020, VISUAL COMPUT, V36, P97, DOI 10.1007/s00371-018-1585-8
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   GOSHTASBY A, 1987, PATTERN RECOGN, V20, P525, DOI 10.1016/0031-3203(87)90079-3
   Grundland M, 2006, COMPUT GRAPH FORUM, V25, P577, DOI 10.1111/j.1467-8659.2006.00977.x
   Jilani SK, 2017, 2017 INTERNATIONAL CONFERENCE ON CYBERWORLDS (CW), P170, DOI 10.1109/CW.2017.27
   Jin Y, 2015, IEEE T INF FOREN SEC, V10, P640, DOI 10.1109/TIFS.2015.2390414
   Kemelmacher-Shlizerman I, 2014, PROC CVPR IEEE, P3334, DOI 10.1109/CVPR.2014.426
   King DE, 2009, J MACH LEARN RES, V10, P1755
   Kwon YH, 1999, COMPUT VIS IMAGE UND, V74, P1, DOI 10.1006/cviu.1997.0549
   Larsen R, 2008, J MATH IMAGING VIS, V31, P189, DOI 10.1007/s10851-008-0077-2
   Liu S, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P82, DOI 10.1145/3123266.3123431
   Liu XQ, 2020, VISUAL COMPUT, V36, P1635, DOI 10.1007/s00371-019-01759-7
   Murthy SK, 1998, DATA MIN KNOWL DISC, V2, P345, DOI 10.1023/A:1009744630224
   Parkhi OM, 2015, DEEP FACE RECOGNITIO
   Pereira TD, 2019, IEEE T INF FOREN SEC, V14, P1803, DOI 10.1109/TIFS.2018.2885284
   Riaz S, 2019, MACH VISION APPL, V30, P91, DOI 10.1007/s00138-018-0975-2
   Ricanek K, 2006, PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION - PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE, P341
   Shewchuk J., 2016, Delaunay Mesh Generation
   Shu XB, 2018, IEEE T PATTERN ANAL, V40, P905, DOI 10.1109/TPAMI.2017.2705122
   Shu XB, 2016, PATTERN RECOGN, V59, P156, DOI 10.1016/j.patcog.2015.12.015
   Shu XB, 2016, NEUROCOMPUTING, V208, P249, DOI 10.1016/j.neucom.2016.01.101
   Shu XB, 2015, IEEE I CONF COMP VIS, P3970, DOI 10.1109/ICCV.2015.452
   Sidorov G, 2014, COMPUT SIST, V18, P491
   Thomaz C.E., 2010, FEI FACE DATABASE
   Wang W, 2019, IEEE T PATTERN ANAL, V41, P654, DOI 10.1109/TPAMI.2018.2803166
   Wang W, 2016, PROC CVPR IEEE, P2378, DOI 10.1109/CVPR.2016.261
   Wang YH, 2012, IEEE T SYST MAN CY B, V42, P1107, DOI 10.1109/TSMCB.2012.2187051
   WEN YD, 2016, PROC CVPR IEEE, P4893, DOI DOI 10.1109/CVPR.2016.529
   Yu GS, 2009, INT CONF ACOUST SPEE, P1597, DOI 10.1109/ICASSP.2009.4959904
   Zhang ZF, 2017, PROC CVPR IEEE, P4352, DOI 10.1109/CVPR.2017.463
   Zhang ZH, 2016, ANN TRANSL MED, V4, DOI 10.3978/j.issn.2305-5839.2015.12.38
   Zhi RC, 2020, VISUAL COMPUT, V36, P1067, DOI 10.1007/s00371-019-01707-5
NR 48
TC 7
Z9 7
U1 0
U2 13
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2021
VL 37
IS 7
BP 2023
EP 2038
DI 10.1007/s00371-020-01960-z
EA AUG 2020
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SZ5DG
UT WOS:000560267300001
OA hybrid
DA 2024-07-18
ER

PT J
AU Liu, XQ
   Zhou, FY
AF Liu, Xiaoqian
   Zhou, Fengyu
TI Improved curriculum learning using SSM for facial expression recognition
SO VISUAL COMPUTER
LA English
DT Article
DE Curriculum learning; Density-distance clustering; Facial expression;
   Recognition
ID FACE
AB Facial expression recognition is an important research issue in the pattern recognition field. However, the generalization of the model still remains a challenging task. In this paper, we apply a strategy of curriculum learning to facial expression recognition during the stage of training. And a novel curriculum design method is proposed. The system first employs the unsupervised density-distance clustering method to determine the clustering center of each category. Then, the dataset is divided into three subsets of various complexity according to the distance from each sample to the clustering center in the feature space. Importantly, we develop a multistage training process where a main model is trained by continuously adding harder samples to training set to increase the complexity. To solve the problem that the model has a poor recognition accuracy for anger, fear and sadness, a self-selection mechanism is introduced in the test stage to make further judgment on the result of the main model. Experiment results indicate that the proposed model can achieve a satisfactory recognition accuracy of 72.11% on FER-2013 and 98.18% on CK+ dataset for 7-class facial expressions, which outperforms the other state-of-the-art methods.
C1 [Liu, Xiaoqian; Zhou, Fengyu] Shandong Univ, Sch Control Sci & Engn, Jinan 250061, Peoples R China.
C3 Shandong University
RP Zhou, FY (corresponding author), Shandong Univ, Sch Control Sci & Engn, Jinan 250061, Peoples R China.
EM zhoufengyu@sdu.edu.cn
CR Agrawal A, 2020, VISUAL COMPUT, V36, P405, DOI 10.1007/s00371-019-01630-9
   An FP, 2020, VISUAL COMPUT, V36, P483, DOI 10.1007/s00371-019-01635-4
   [Anonymous], 2014, REAL TIME EMOTION RE
   [Anonymous], 2013, Challenges in Representation Learning: Facial Expression Recognition Challenge
   [Anonymous], 2015, Curriculum Learning with Deep Convolutional Neural Networks
   [Anonymous], 2013, ARXIV13060239
   [Anonymous], 2018, CURRICULUMNET WEAKLY
   Ayata D, 2018, IEEE T CONSUM ELECTR, V64, P196, DOI 10.1109/TCE.2018.2844736
   Bartlett MS, 2005, PROC CVPR IEEE, P568
   Bengio Y., 2009, P 26 ANN INT C MACH, V60, P6, DOI [DOI 10.1145/1553374.1553380, 10.1145/1553374.1553380]
   Chernykh V., 2017, EMOTION RECOGNITION
   Dehghan A., 2017, DAGER: Deep age, gender and emotion recognition using convolutional neural network
   Dhall A, 2016, ICMI'16: PROCEEDINGS OF THE 18TH ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P427, DOI 10.1145/2993148.2997638
   Dhall A, 2015, ICMI'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P423, DOI 10.1145/2818346.2829994
   EKMAN P, 1971, J PERS SOC PSYCHOL, V17, P124, DOI 10.1037/h0030377
   Ekman P, 1978, FACIAL ACTION CODING
   Gogic I, 2020, VISUAL COMPUT, V36, P97, DOI 10.1007/s00371-018-1585-8
   Goodfellow IJ, 2015, NEURAL NETWORKS, V64, P59, DOI 10.1016/j.neunet.2014.09.005
   Gui LK, 2017, IEEE INT CONF AUTOMA, P505, DOI 10.1109/FG.2017.68
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Ionescu R. T., 2013, WORKSH CHALL REPR LE
   Jain N, 2018, PATTERN RECOGN LETT, V115, P101, DOI 10.1016/j.patrec.2018.04.010
   Jung H, 2015, IEEE I CONF COMP VIS, P2983, DOI 10.1109/ICCV.2015.341
   Khorrami P, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P19, DOI 10.1109/ICCVW.2015.12
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li K, 2020, VISUAL COMPUT, V36, P391, DOI 10.1007/s00371-019-01627-4
   Li S., 2018, Deep facial expression recognition: A survey
   Liang DD, 2020, VISUAL COMPUT, V36, P499, DOI 10.1007/s00371-019-01636-3
   Liu MY, 2015, LECT NOTES COMPUT SC, V9006, P143, DOI 10.1007/978-3-319-16817-3_10
   Liu MY, 2015, NEUROCOMPUTING, V159, P126, DOI 10.1016/j.neucom.2015.02.011
   Liu MY, 2014, PROC CVPR IEEE, P1749, DOI 10.1109/CVPR.2014.226
   Lucey P., 2010, IEEE COMP SOC C COMP, P94, DOI 10.1109/CVPRW.2010.5543262
   Mavadati SM, 2013, IEEE T AFFECT COMPUT, V4, P151, DOI 10.1109/T-AFFC.2013.4
   Mohammadi MR, 2014, J VIS COMMUN IMAGE R, V25, P1082, DOI 10.1016/j.jvcir.2014.03.006
   Pantic M, 2000, IEEE T PATTERN ANAL, V22, P1424, DOI 10.1109/34.895976
   Pentina A, 2015, PROC CVPR IEEE, P5492, DOI 10.1109/CVPR.2015.7299188
   Rodriguez A, 2014, SCIENCE, V344, P1492, DOI 10.1126/science.1242072
   Sariyanidi E, 2015, IEEE T PATTERN ANAL, V37, P1113, DOI 10.1109/TPAMI.2014.2366127
   Shan CF, 2009, IMAGE VISION COMPUT, V27, P803, DOI 10.1016/j.imavis.2008.08.005
   Simonyan K., 2014, 14091556 ARXIV
   Yang B, 2018, IEEE ACCESS, V6, P4630, DOI 10.1109/ACCESS.2017.2784096
   Zeng ZH, 2009, IEEE T PATTERN ANAL, V31, P39, DOI 10.1109/TPAMI.2008.52
   Zhao GY, 2007, IEEE T PATTERN ANAL, V29, P915, DOI 10.1109/TPAMI.2007.1110
NR 43
TC 16
Z9 18
U1 2
U2 16
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2020
VL 36
IS 8
BP 1635
EP 1649
DI 10.1007/s00371-019-01759-7
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ML7DA
UT WOS:000549620700005
DA 2024-07-18
ER

PT J
AU Singh, RP
   Sharma, P
AF Singh, Rimjhim Padam
   Sharma, Poonam
TI Instance-vote-based motion detection using spatially extended hybrid
   feature space
SO VISUAL COMPUTER
LA English
DT Article
DE Motion detection; Background subtraction; Motion analysis
ID BACKGROUND SUBTRACTION; MIXTURE; SEGMENTATION; ALGORITHMS; MODEL; FLOW
AB Motion recognition, a trivial step employed in several video-based applications, is still a challenging task in real-world complex scenarios containing dynamic noise, varying backgrounds, shadows, improper illuminations, camouflages, etc. Numerous pixel-based change detection techniques employing varied combinations of different feature spaces have been proposed to efficiently overcome many real-world challenges. But ideally, handling all the possible real-world challenges simultaneously is yet to be achieved. Hence, this paper proposes a memory-efficient unique combination of multi-colour feature space with a light-weight intensity-based texture descriptor. The proposed spatially enlarged extended centre-symmetric local binary pattern is combined with YCbCr and RGB colour features for robust pixel representation. The proposed feature space is fed to an extended instance-vote technique for pixel classification. The random and time-subsampled update is employed conditionally for model update, followed by a feedback network that continuously optimizes the local threshold and learning rate parameters of the proposed model. The proposed feature space and model have been evaluated on whole 2014 Change Detection dataset, the largest known dataset. The outperforming performance and memory analysis strengthens its acceptability for real-time applications.
C1 [Singh, Rimjhim Padam; Sharma, Poonam] Visvesvaraya Natl Inst Technol, Comp Sci & Engn, Nagpur, Maharashtra, India.
C3 National Institute of Technology (NIT System); Visvesvaraya National
   Institute of Technology, Nagpur
RP Singh, RP (corresponding author), Visvesvaraya Natl Inst Technol, Comp Sci & Engn, Nagpur, Maharashtra, India.
EM rimjhimsingh1012@gmail.com
RI SHARMA, POONAM/HKM-4093-2023; Singh, Rimjhim Padam/ABG-4736-2020;
   Sharma, Poonam/AAC-6147-2021
OI Singh, Rimjhim Padam/0000-0002-5632-7980; sharma,
   poonam/0000-0003-0766-5171
FU Ministry of Human Resource Development, India (MHRD); Ministry of
   Electronics and Information Technology, India (MEITY)
FX This work has been conducted with the research grants received from
   Ministry of Human Resource Development, India (MHRD) and Ministry of
   Electronics and Information Technology, India (MEITY).
CR Allebosch Gianni., 2015, Computer Vision, Imaging and Computer Graphics Theory and Applications, P433
   Alvarez L, 2000, INT J COMPUT VISION, V39, P41, DOI 10.1023/A:1008170101536
   [Anonymous], 2010, AS C COMP VIS ACCV 2
   [Anonymous], 2011, EVID BASED COMPL ALT
   [Anonymous], 2000, P EUR C COMP VIS, DOI DOI 10.1007/3-540-45053-X_48
   [Anonymous], 2010, INT C AC SPEECH SIGN
   Barnich O, 2011, IEEE T IMAGE PROCESS, V20, P1709, DOI 10.1109/TIP.2010.2101613
   Bianco S, 2017, IEEE T EVOLUT COMPUT, V21, P914, DOI 10.1109/TEVC.2017.2694160
   Bilodeau GA, 2013, 2013 INTERNATIONAL CONFERENCE ON COMPUTER AND ROBOT VISION (CRV), P106, DOI 10.1109/CRV.2013.29
   Bouwmans T, 2019, NEURAL NETWORKS, V117, P8, DOI 10.1016/j.neunet.2019.04.024
   Bouwmans T, 2018, COMPUT SCI REV, V28, P26, DOI 10.1016/j.cosrev.2018.01.004
   Braham M, 2016, INT CONF SYST SIGNAL, P113
   Candès EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395
   Chacon-Murguia MI, 2012, IEEE T IND ELECTRON, V59, P3286, DOI 10.1109/TIE.2011.2106093
   Chan YT, 2018, INFORM FUSION, V39, P154, DOI 10.1016/j.inffus.2017.05.001
   Chen ML, 2018, IEEE T PATTERN ANAL, V40, P1518, DOI 10.1109/TPAMI.2017.2717828
   De Gregorio M., 2017, ESANN
   Dong XP, 2019, IEEE T IMAGE PROCESS, V28, P3516, DOI 10.1109/TIP.2019.2898567
   Dong Z, 2018, INT CONF POW ELECTR, P459, DOI 10.23919/IPEC.2018.8507611
   El Baf F, 2008, LECT NOTES COMPUT SC, V5358, P772, DOI 10.1007/978-3-540-89639-5_74
   Elsayed RA, 2015, IEEE I C ELECT CIRC, P33, DOI 10.1109/ICECS.2015.7440242
   Feng Jiashi, 2013, Advances in Neural Information Processing Systems, P404
   Gao Z, 2014, IEEE T PATTERN ANAL, V36, P1975, DOI 10.1109/TPAMI.2014.2314663
   Goyette N., 2012, 2012 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops (CVPR Workshops), DOI 10.1109/CVPRW.2012.6238919
   Hayase Y, 2006, ICPASM 2005: PROCEEDINGS OF THE 8TH INTERNATIONAL CONFERENCE ON PROPERTIES AND APPLICATIONS OF DIELECTRIC MATERIALS, VOLS 1 AND 2, P159
   He J, 2012, PROC CVPR IEEE, P1568, DOI 10.1109/CVPR.2012.6247848
   Heikkilä M, 2006, IEEE T PATTERN ANAL, V28, P657, DOI 10.1109/TPAMI.2006.68
   Heikkilä M, 2009, PATTERN RECOGN, V42, P425, DOI 10.1016/j.patcog.2008.08.014
   Hofmann Martin., 2012, 2012 IEEE COMPUTER S, P38, DOI DOI 10.1109/CVPRW.2012.6238925
   Ilyas A, 2009, AVSS: 2009 6TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE, P454, DOI 10.1109/AVSS.2009.85
   Isik S, 2018, J ELECTRON IMAGING, V27, DOI 10.1117/1.JEI.27.2.023002
   Jiang SQ, 2018, IEEE T CIRC SYST VID, V28, P2105, DOI 10.1109/TCSVT.2017.2711659
   KaewTraKulPong P, 2002, VIDEO-BASED SURVEILLANCE SYSTEMS: COMPUTER VISION AND DISTRIBUTED PROCESSING, P135
   Kalsotra R, 2019, IEEE ACCESS, V7, P59143, DOI 10.1109/ACCESS.2019.2914961
   Kawabata S, 2006, INT C PATT RECOG, P1171
   Kim K, 2005, REAL-TIME IMAGING, V11, P172, DOI 10.1016/j.rti.2004.12.004
   Lai QX, 2020, IEEE T IMAGE PROCESS, V29, P1113, DOI 10.1109/TIP.2019.2936112
   Lee DS, 2005, IEEE T PATTERN ANAL, V27, P827, DOI 10.1109/TPAMI.2005.102
   Lee SH, 2019, SYMMETRY-BASEL, V11, DOI 10.3390/sym11050621
   Li YX, 2019, INT J MOD PHYS C, V30, DOI 10.1142/S0129183119500013
   Liang XZ, 2018, IEEE INT CON MULTI
   Liang ZY, 2020, IEEE T IMAGE PROCESS, V29, P3351, DOI 10.1109/TIP.2019.2959256
   Liao J, 2018, LECT NOTES COMPUT SC, V11164, P524, DOI 10.1007/978-3-030-00776-8_48
   Liao S., 2010, Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on, P1301, DOI DOI 10.1109/CVPR.2010.5539817
   Lin HH, 2002, 2002 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL III, PROCEEDINGS, P893, DOI 10.1109/ICIP.2002.1039116
   Lin HH, 2009, IEEE T SIGNAL PROCES, V57, P1641, DOI 10.1109/TSP.2009.2014810
   Liu C, 2019, IEEE T CYBERNETICS, V49, P3665, DOI 10.1109/TCYB.2018.2846361
   Liu Y, 2017, IEEE T CYBERNETICS, V49, P159
   Lu XQ, 2014, IEEE IMAGE PROC, P3268, DOI 10.1109/ICIP.2014.7025661
   Luo L, 2018, INT CONF MACH LEARN, P91, DOI 10.1109/ICMLC.2018.8527059
   Maddalena L., 2012, 2012 IEEE COMP SOC C, P21, DOI [10.1109/CVPRW.2012.6238922, DOI 10.1109/CVPRW.2012.6238922]
   Noh SeungJong., 2012, AS C COMP VIS, P493
   Oliver NM, 2000, IEEE T PATTERN ANAL, V22, P831, DOI 10.1109/34.868684
   Porikli F., 2003, IEEE INT WORKSHOP PE, P1
   Ramírez-Alonso G, 2016, NEUROCOMPUTING, V175, P990, DOI 10.1016/j.neucom.2015.04.118
   Reinders F, 2001, VISUAL COMPUT, V17, P55, DOI 10.1007/PL00013399
   Rougier C, 2007, 21ST INTERNATIONAL CONFERENCE ON ADVANCED NETWORKING AND APPLICATIONS WORKSHOPS/SYMPOSIA, VOL 2, PROCEEDINGS, P875, DOI 10.1109/ainaw.2007.181
   Sajid H, 2017, IEEE T IMAGE PROCESS, V26, P3249, DOI 10.1109/TIP.2017.2695882
   SEDKY M, 2014, P IEEE C COMP VIS PA, P399
   Shen JB, 2019, IEEE T NEUR NET LEAR, V30, P2637, DOI 10.1109/TNNLS.2018.2885591
   Shen JB, 2018, IEEE T IMAGE PROCESS, V27, P2688, DOI 10.1109/TIP.2018.2795740
   Shen JB, 2017, IEEE T IMAGE PROCESS, V26, P4911, DOI 10.1109/TIP.2017.2722691
   Shi G, 2018, IEEE T IMAGE PROCESS, V27, P4810, DOI 10.1109/TIP.2018.2845123
   Silva C., 2015, INT JOINT C COMP VIS, P2015
   Silva C, 2016, INT C PATT RECOG, P2216, DOI 10.1109/ICPR.2016.7899965
   Singh RP, 2020, INT J PATTERN RECOGN, V34, DOI 10.1142/S0218001420500238
   Singh RP, 2019, IEEE ACCESS, V7, P130180, DOI 10.1109/ACCESS.2019.2937402
   St-Charles PL, 2016, IEEE T IMAGE PROCESS, V25, P4768, DOI 10.1109/TIP.2016.2598691
   St-Charles PL, 2015, IEEE T IMAGE PROCESS, V24, P359, DOI 10.1109/TIP.2014.2378053
   St-Charles PL, 2014, IEEE WINT CONF APPL, P509, DOI 10.1109/WACV.2014.6836059
   Stauffer C., 1999, Proceedings. 1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No PR00149), P246, DOI 10.1109/CVPR.1999.784637
   Sun YP, 2015, IEEE T IMAGE PROCESS, V24, P2515, DOI 10.1109/TIP.2015.2419075
   Tavakkoh A, 2006, INT C PATT RECOG, P315
   Tavakkoli A., 2006, P INT WORKSH STAT ME
   Tavakkoli A, 2008, INT C PATT RECOG, P3958
   Teng C, 2011, ACTA POLYM SIN, P1001, DOI 10.3724/SP.J.1105.2011.11123
   VAN DROOGENBROECK M., 2014, VIBE DISRUPTIVE METH, P7, DOI DOI 10.1201/B17223-10
   Varadarajan S, 2013, 2013 10TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE (AVSS 2013), P63, DOI 10.1109/AVSS.2013.6636617
   Wang B., 2014, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, P395
   Wang FX, 2019, IEEE INFOCOM SER, P910, DOI [10.1109/infocom.2019.8737456, 10.1109/INFOCOM.2019.8737456]
   Wang HZ, 2006, INT C PATT RECOG, P223
   Wang J.X., 2006, OTCBVS, P137
   Wang JX, 2009, MACH VISION APPL, V20, P205, DOI 10.1007/s00138-007-0118-7
   Wang KF, 2018, IEEE ACCESS, V6, P15505, DOI 10.1109/ACCESS.2018.2812880
   Wang Wenguan, 2018, IEEE TPAMI, V41, P985
   Wang Y., 2014, P IEEE C COMP VIS PA, P387, DOI 10.1109/ICIP40778.2020.9190887
   Wu HF, 2014, SIGNAL IMAGE VIDEO P, V8, P665, DOI 10.1007/s11760-013-0576-5
   Xue GJ, 2010, IEEE INT CON MULTI, P1050, DOI 10.1109/ICME.2010.5582601
   Yang SY, 2018, MEMET COMPUT, V10, P53, DOI 10.1007/s12293-017-0225-6
   Zeng DD, 2018, IET IMAGE PROCESS, V12, P1292, DOI 10.1049/iet-ipr.2016.1026
   Zeng J, 2008, PATTERN RECOGN, V41, P3636, DOI 10.1016/j.patcog.2008.06.006
   Zhao ZJ, 2012, COMM COM INF SC, V346, P177
   Zhou XW, 2013, IEEE T PATTERN ANAL, V35, P597, DOI 10.1109/TPAMI.2012.132
   Zivkovic Z, 2006, PATTERN RECOGN LETT, V27, P773, DOI 10.1016/j.patrec.2005.11.005
   Zivkovic Z, 2004, INT C PATT RECOG, P28, DOI 10.1109/ICPR.2004.1333992
NR 95
TC 3
Z9 3
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2021
VL 37
IS 6
BP 1527
EP 1543
DI 10.1007/s00371-020-01890-w
EA JUL 2020
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SO2LW
UT WOS:000557083300002
DA 2024-07-18
ER

PT J
AU Wu, SP
   Zhao, YB
   Parvinzamir, F
   Ersotelos, NT
   Wei, H
   Dong, F
AF Wu, Shaopeng
   Zhao, Youbing
   Parvinzamir, Farzad
   Ersotelos, Nikolaos Th.
   Wei, Hui
   Dong, Feng
TI Literature Explorer: effective retrieval of scientific documents through
   nonparametric thematic topic detection
SO VISUAL COMPUTER
LA English
DT Article
DE Topic explorer; Data visualisation; Topic modelling; Text mining; Web
   application; Scientific documents
AB Scientific researchers are facing a rapidly growing volume of literatures nowadays. While these publications offer rich and valuable information, the scale of the datasets makes it difficult for the researchers to manage and search for desired information efficiently. Literature Explorer is a new interactive visual analytics suite that facilitates the access to desired scientific literatures through mining and interactive visualisation. We propose a novel topic mining method that is able to uncover "thematic topics" from a scientific corpus. These thematic topics have an explicit semantic association to the research themes that are commonly used by human researchers in scientific fields, and hence are human interpretable. They also contribute to effective document retrieval. The visual analytics suite consists of a set of visual components that are closely coupled with the underlying thematic topic detection to support interactive document retrieval. The visual components are adequately integrated under the design rationale and goals. Evaluation results are given in both objective measurements and subjective terms through expert assessments. Comparisons are also made against the outcomes from the traditional topic modelling methods.
C1 [Wu, Shaopeng; Zhao, Youbing; Wei, Hui; Dong, Feng] Univ Bedfordshire, Luton, Beds, England.
   [Parvinzamir, Farzad] InsightZen LLC, Hangzhou, Peoples R China.
   [Dong, Feng] Univ Strathclyde, Dept Comp & Informat Sci, Glasgow, Lanark, Scotland.
   [Ersotelos, Nikolaos Th.] Univ Wolverhampton, Wolverhampton Cyber Res Inst WCRI, Sch Math & Comp Sci, Wulfruna St, Wolverhampton WV1 1LY, W Midlands, England.
   [Parvinzamir, Farzad] Queens Univ Belfast, Sch Elect Elect Engn & Comp Sci, Belfast, Antrim, North Ireland.
   [Zhao, Youbing] Commun Univ Zhejiang, Hangzhou, Peoples R China.
C3 University of Bedfordshire; University of Strathclyde; University of
   Wolverhampton; Queens University Belfast; Communication University of
   Zhejiang
RP Wu, SP (corresponding author), Univ Bedfordshire, Luton, Beds, England.
EM shaopeng.wu@gmail.com; n.ersotelos@wlv.ac.uk
OI Ersotelos, Nikolaos/0000-0001-7699-9998
FU European Commission [611383, 60929]; UK Engineering and Physical
   Sciences Research Council [EP/L023830/1]; EPSRC [EP/L023830/1] Funding
   Source: UKRI
FX This research is supported by the European Commission with project Dr
   Inventor (No 611383), MyHealthAvatar (No 60929), and by the UK
   Engineering and Physical Sciences Research Council with project
   MyLifeHub (EP/L023830/1).
CR ACM, ACM T GRAPHIC
   [Anonymous], 2006, P 12 ACM SIGKDD INT, DOI [10.1145/1150402.1150467, DOI 10.1145/1150402.1150467]
   [Anonymous], 2010, Atlas of Science: Visualizing what We Know
   [Anonymous], 2009, P 21 INT JOINT C ART
   [Anonymous], 2014, P 23 INT C WORLD WID
   [Anonymous], 2010, P 16 ACM SIGKDD INT
   Beck F, 2016, IEEE T VIS COMPUT GR, V22, P180, DOI 10.1109/TVCG.2015.2467757
   Berger M, 2017, IEEE T VIS COMPUT GR, V23, P691, DOI 10.1109/TVCG.2016.2598667
   Blei DM, 2012, COMMUN ACM, V55, P77, DOI 10.1145/2133806.2133826
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Cao N, 2010, IEEE T VIS COMPUT GR, V16, P1172, DOI 10.1109/TVCG.2010.154
   Chen CM, 2006, J AM SOC INF SCI TEC, V57, P359, DOI 10.1002/asi.20317
   Choo J, 2013, IEEE T VIS COMPUT GR, V19, P1992, DOI 10.1109/TVCG.2013.212
   Cui WW, 2011, IEEE T VIS COMPUT GR, V17, P2412, DOI 10.1109/TVCG.2011.239
   Deerwester S, 1990, J AM SOC INF SCI, V41, P407, DOI [10.1002/(SICI)1097-4571(199009)41:6391::AID-ASI13.0.CO;2-9, DOI 10.1002/(SICI)1097-4571(199009)41:6391::AID-ASI13.0.CO;2-9]
   Dou WW, 2013, IEEE T VIS COMPUT GR, V19, P2002, DOI 10.1109/TVCG.2013.162
   Dunne C, 2012, J AM SOC INF SCI TEC, V63, P2351, DOI 10.1002/asi.22652
   El-Arini Khalid, 2011, P 17 ACM SIGKDD INT, P439, DOI 10.1145/2020408.2020479
   Gao Z.J., 2011, 2011 IEEE 11 INT C D, P1056, DOI DOI 10.1109/ICDM.2011.148
   Gretarsson B, 2012, ACM T INTEL SYST TEC, V3, DOI 10.1145/2089094.2089099
   Guo H., 2018, IEEE T VIS COMPUT GR, P1
   Havre S, 2002, IEEE T VIS COMPUT GR, V8, P9, DOI 10.1109/2945.981848
   Heimerl F, 2016, IEEE T VIS COMPUT GR, V22, P190, DOI 10.1109/TVCG.2015.2467621
   Heimerl F, 2012, IEEE T VIS COMPUT GR, V18, P2839, DOI 10.1109/TVCG.2012.277
   Hirsch JE, 2005, P NATL ACAD SCI USA, V102, P16569, DOI 10.1073/pnas.0507655102
   Hofmann T, 1999, SIGIR'99: PROCEEDINGS OF 22ND INTERNATIONAL CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P50, DOI 10.1145/312624.312649
   Hong L., 2010, P 1 WORKSH SOC MED A, P8088
   Kim J., 2008, TECHNICAL REPORT GT
   Lee Bongshin., 2005, CHI 05 EXTENDED ABST
   Lee DD, 1999, NATURE, V401, P788, DOI 10.1038/44565
   Lee H, 2012, COMPUT GRAPH FORUM, V31, P1155, DOI 10.1111/j.1467-8659.2012.03108.x
   Leskovec J, 2009, KDD-09: 15TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P497
   Liu SX, 2014, IEEE CONF VIS ANAL, P183, DOI 10.1109/VAST.2014.7042494
   Mihalcea R.e., 2004, Proceedings of EMNLP 2004
   Mikolov T, 2013, ICLR WORKSHOP POSTER
   Newman D., 2010, AUTOMATIC EVALUATION
   Pang XW, 2016, INT J GRID HIGH PERF, V8, P100, DOI 10.4018/IJGHPC.2016100106
   Pennington J., 2014, 4 C EMPIRICAL METHOD
   Rosner F., 2014, ARXIV14036397V1
   Shahaf D., 2012, P 18 ACM SIGKDD INT
   Sheikh Y.A., 2007, 2007 IEEE 11 INT C C, P1
   Spasojevic N, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P1809, DOI 10.1145/2623330.2623350
   Sun GD, 2014, IEEE T VIS COMPUT GR, V20, P1753, DOI 10.1109/TVCG.2014.2346919
   Teh YW, 2006, J AM STAT ASSOC, V101, P1566, DOI 10.1198/016214506000000302
   Vosecky J., 2013, P 22 ACM INT C CONFE
   Wei F., 2010, P 16 ACM SIGKDD INT
   Wenwen Dou, 2011, 2011 IEEE Conference on Visual Analytics Science and Technology, P231, DOI 10.1109/VAST.2011.6102461
   Wong P.C., 2004, Information Visualization, pr2
   Xu PP, 2013, IEEE T VIS COMPUT GR, V19, P2012, DOI 10.1109/TVCG.2013.221
NR 49
TC 3
Z9 5
U1 2
U2 13
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2020
VL 36
IS 7
BP 1337
EP 1354
DI 10.1007/s00371-019-01721-7
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA LO1PW
UT WOS:000533401000004
OA Green Published, hybrid, Green Accepted
DA 2024-07-18
ER

PT J
AU Wang, SX
   Zhang, Q
   Wang, SX
   Jing, XK
   Gao, MT
AF Wang, Shuxia
   Zhang, Qian
   Wang, Shouxia
   Jing, Xiaoke
   Gao, Mantun
TI Endpoint fusing method of online freehand-sketched polyhedrons
SO VISUAL COMPUTER
LA English
DT Article
DE Endpoint fusing; Online freehand sketch; Polyhedron with trihedral
   vertices; Parallelism correction; Sketch regularization; 3D
   reconstruction
ID RECONSTRUCTION; STROKES; DESIGN; MODELS
AB Endpoint fusing plays a crucial role in the process of sketch regularization and corrects the gestures from freehand-sketched strokes, which have the imprecise connections at the endpoints and inflection point of into some closed loops. This paper proposes a novel endpoint fusing method, based on a feature descriptor called angle distribution histogram, is specialized for online freehand-sketched polyhedrons. The method comprises four main phases. Firstly, parallelism correction is used to revise and recognize user's inputting strokes according to the linearity distribution of gestures in the angle histogram. Then, the adaptive tolerance circles with variable coefficients are applied to determine which the endpoints should be joined together. Nextly settles the order of connecting endpoints in accordance with the coordinate system weight criteria. Finally, the session of endpoints connection is carried out and ends with a clean, crisp line drawing possessing perfect connections. This approach has been tested with various examples and compared with the other existing methods in an interactive prototype system of freehand sketching recognition-endpoint fusion to evaluate the regularization quality. The experimental results show that the proposed approach overcomes these limitations embedded within the existing sketching systems, in which only simply connect the endpoints without considering the contextual constraints in the sketch. The method can perfectly support the conceptual design based on online freehand-sketched polyhedrons and provide a foundation for the further 3D reconstruction of freehand-sketched polyhedrons.
C1 [Wang, Shuxia; Zhang, Qian; Wang, Shouxia; Jing, Xiaoke; Gao, Mantun] Northwestern Polytech Univ, Sch Mech Engn, Xian, Peoples R China.
C3 Northwestern Polytechnical University
RP Wang, SX (corresponding author), Northwestern Polytech Univ, Sch Mech Engn, Xian, Peoples R China.
EM 2008wangshuxia@163.com
FU National Natural Science Foundation of China [51105310]; Natural
   Sci-ence Basic Research Plan in Shaanxi Province of China [2016JM6054];
   Programme of Introducing Talents of Discipline to Universities (111
   Project) of China [B13044]; Open Project Program of the State Key Lab of
   CAD&CG of Zhejiang University [A1615]; Seed Foundation of Innovation and
   Cre-ation for Graduate Students in Northwestern Polytechnical University
   [Z2017106]
FX This work is partly supported by National Natural Science Foundation of
   China (Grant No. 51105310), Natural Sci-ence Basic Research Plan in
   Shaanxi Province of China (Grant No. 2016JM6054), the Programme of
   Introducing Talents of Discipline to Universities (111 Project) of China
   (Grant No. B13044), the Open Project Program of the State Key Lab of
   CAD&CG (Grant No. A1615) of Zhejiang University, and the Seed Foundation
   of Innovation and Cre-ation for Graduate Students in Northwestern
   Polytechnical University (Grant No. Z2017106).
CR [Anonymous], 2010, ARXIV10072442
   [Anonymous], 1998, CHI 98 Cconference Summary on Human Factors in Computing Systems, CHI '98
   Cheon SU, 2008, COMPUT AIDED DESIGN, V40, P975, DOI 10.1016/j.cad.2008.07.006
   Ching F.D., 2014, Architecture: Form, Space, & Order
   [储珺 Chu Jun], 2004, [中国图象图形学报. A, Journal of image and graphics], V9, P972
   Company P, 2015, VISUAL COMPUT, V31, P775, DOI 10.1007/s00371-015-1099-6
   Durgun FB, 1990, ARCHIT SCI REV, V33, P3
   Forsberg Andrew S., 1998, Proceedings of the Second International Immersive Projection Technology Workshop, P11
   Hsu W, 1998, COMPUT AIDED DESIGN, V30, P377, DOI 10.1016/S0010-4485(97)00101-2
   Igarashi T, 1997, HUMAN-COMPUTER INTERACTION - INTERACT '97, P104
   Jun-Wen XU, 2007, SCI TECHNOL ENG, V6, P34
   Kato O., 1982, Proceedings of PRIP 82. IEEE Computer Society Conference on Pattern Recognition and Image Processing, P544
   Ku D.C., 2006, P EUR C SKETCH BAS I, P83
   Ku DC, 2006, WSCG 2006: FULL PAPERS PROCEEDINGS, P263
   Li B, 2014, COMPUT VIS IMAGE UND, V119, P57, DOI 10.1016/j.cviu.2013.11.008
   LLADOS J, 2003, GRAPHICS RECOGNITION
   Meyer J, 1996, CHI 96 C COMP, P195
   Pavlidis T., 1985, Computer Graphics, V19, P225, DOI 10.1145/325165.325240
   Pusch R, 2007, VISUAL COMPUT, V23, P955, DOI 10.1007/s00371-007-0160-5
   Shpitalni M, 1997, J MECH DESIGN, V119, P131, DOI 10.1115/1.2828775
   Song B, 2003, RES COMPUTER SUPPORT
   [孙建勇 Sun Jianyong], 2003, [计算机科学, Computer Science], V30, P172
   Sun S. Q., 1999, CHINA MECH ENG, V6, P697
   Sutherland I. E., 1964, P SHARE DES AUT WORK, DOI DOI 10.1145/800265.810742
   Tian C, 2009, COMPUT AIDED DESIGN, V41, P147, DOI 10.1016/j.cad.2009.02.002
   Tsuchie S, 2017, VISUAL COMPUT, V33, P1197, DOI 10.1007/s00371-016-1282-4
   ULLMAN DG, 1990, COMPUT GRAPH, V14, P263, DOI 10.1016/0097-8493(90)90037-X
   Wang S, 2016, CHIN J EXP OPHTHALMO, V34, P2
   Wang S, 2014, NEW GROUPING FITTING
   Wang S, 2009, J COMPUT AIDED DES C, V21, P81
   Wang S, 2013, ADV MECH ENG, V558, P561
   Wang S., 2009, 2009 IEEE 10 INT C C
   Wang SX, 2007, LECT NOTES COMPUT SC, V4551, P161
   Wang SX, 2009, INT C COMP AID IND D, P586, DOI 10.1109/CAIDCD.2009.5375407
   Wang Y, 2007, SCI TECHNOL ENG, V7, P1482
   [袁浩 Yuan Hao], 2004, [中国图象图形学报. A, Journal of image and graphics], V9, P178
   Zeleznik RobertC., 2007, ACM SIGGRAPH 2007 CO, DOI DOI 10.1145/1281500.1281530
   Zhang SH, 2009, IEEE T VIS COMPUT GR, V15, P618, DOI 10.1109/TVCG.2009.9
   Zou HL, 2007, COMPUT AIDED DESIGN, V39, P1025, DOI 10.1016/j.cad.2007.08.002
NR 39
TC 4
Z9 4
U1 1
U2 12
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2020
VL 36
IS 2
BP 291
EP 303
DI 10.1007/s00371-018-1608-5
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KJ2TH
UT WOS:000511910300006
DA 2024-07-18
ER

PT J
AU Lu, Q
   Huang, J
   Zhang, QY
   Yuan, XH
   Li, J
AF Lu, Qiang
   Huang, Jing
   Zhang, Qingyu
   Yuan, Xiaohui
   Li, Jie
TI Evaluation on visualization methods of dynamic collaborative
   relationships for project management
SO VISUAL COMPUTER
LA English
DT Article
DE Collaboration; Dynamic relationship; Visualization; Evaluation
ID INFORMATION
AB Using visualization methods to describe collaborative relationships can form a more intuitive and conducive graphical representation of these relationships, helping us better understand and analyze complex dynamic collaborative relationships. To explore a suitable visualization form for collaborative relationship analysis, we propose a task classification method to evaluate the two visual methods (node-link and adjacency matrix) which represent the static features and the three methods (animation, small multiples, and timeline) which represent the time characteristics of dynamic graphs. We present an evaluation system and design a task-based user evaluation experiment with the Dutch railway project data. By collecting and analyzing task completion time and error rates, we summarize our findings from the evaluation experiment and list three key recommendations to provide preliminary clues to visual designers: (1) Node-link has a better performance on small-scale project management. (2) Timeline has more advantages in the expression of project time management. (3) Animation will be a good choice when you need to check the status of tasks in the project management for a period of time. These findings can help the designers discover faster and more accurate ways to visualize the characteristics and changes of collaborative relationships, thus promoting the smooth progress of collaborative work.
C1 [Lu, Qiang; Huang, Jing; Zhang, Qingyu; Li, Jie] Hefei Univ Technol, Minist Educ, Key Lab Knowledge Engn Big Data, Hefei, Peoples R China.
   [Lu, Qiang] Hefei Univ Technol, Sch Comp Sci & Informat, Hefei 230601, Peoples R China.
   [Yuan, Xiaohui] Univ North Texas, Denton, TX 76203 USA.
C3 Hefei University of Technology; Hefei University of Technology;
   University of North Texas System; University of North Texas Denton
RP Lu, Q (corresponding author), Hefei Univ Technol, Minist Educ, Key Lab Knowledge Engn Big Data, Hefei, Peoples R China.; Lu, Q (corresponding author), Hefei Univ Technol, Sch Comp Sci & Informat, Hefei 230601, Peoples R China.
EM luqiang@hfut.edu.cn
RI Lu, Qiang/AAU-9248-2020; Wu, Celimuge/P-1232-2019
OI Wu, Celimuge/0000-0001-6853-5878; LI, Jie/0000-0001-8483-6240; Yuan,
   Xiaohui/0000-0001-6897-4563
FU National Natural Science Foundation of China [61972130, 61906061]; Key
   Research and Development Plan of Anhui Province [1704d0802177]; Natural
   Science Foundation of Anhui Province of China [1708085MF158]; Key
   Project of Transformation and Industrialization of Scientific and
   Technological Achievements of Intelligent Manufacturing Technology
   Research Institute of Hefei University of Technology [IMICZ2017010]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant Nos. 61972130 and 61906061, partly
   supported by the Key Research and Development Plan of Anhui Province
   under Grant No. 1704d0802177, partly supported by the Natural Science
   Foundation of Anhui Province of China under Grant No. 1708085MF158, and
   also partly supported by the Key Project of Transformation and
   Industrialization of Scientific and Technological Achievements of
   Intelligent Manufacturing Technology Research Institute of Hefei
   University of Technology under Grant No. IMICZ2017010.
CR Ahn J, 2013, IEEE T VIS COMPUT GR, V20, P1
   ARENDT D. L., 2014, COMPUT SCI
   Bach B, 2015, COMPUT GRAPH FORUM, V34, P31, DOI 10.1111/cgf.12615
   Bach B., 2014, VISUALIZING DYNAMIC, P877
   Bach B, 2014, IEEE T VIS COMPUT GR, V20, P740, DOI 10.1109/TVCG.2013.254
   Beck F, 2017, COMPUT GRAPH FORUM, V36, P133, DOI 10.1111/cgf.12791
   Behrisch M, 2016, COMPUT GRAPH FORUM, V35, P693, DOI 10.1111/cgf.12935
   Borgo R, 2018, COMPUT GRAPH FORUM, V37, P573, DOI 10.1111/cgf.13444
   Caban JJ, 2011, COMPUT GRAPH FORUM, V30, P821, DOI 10.1111/j.1467-8659.2011.01931.x
   Carley KM, 2002, SIMUL MODEL PRACT TH, V10, P253, DOI 10.1016/S1569-190X(02)00119-3
   Chang CL, 2017, PROCEEDINGS OF THE 2017 ACM SIGCHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'17), P1397, DOI 10.1145/3025453.3026024
   Collins C, 2009, IEEE T VIS COMPUT GR, V15, P1009, DOI 10.1109/TVCG.2009.122
   Farrugia M, 2011, EFFECTIVE TEMPORAL G
   Federico P, 2016, EVALUATION TWO INTER
   Fung DCY, 2012, PROTEOMICS, V12, P1669, DOI 10.1002/pmic.201100454
   Ghoniem M, 2004, IEEE SYMPOSIUM ON INFORMATION VISUALIZATION 2004, PROCEEDINGS, P17, DOI 10.1109/INFVIS.2004.1
   Isenberg T, 2013, IEEE T VIS COMPUT GR, V19, P2818, DOI 10.1109/TVCG.2013.126
   Jianu R, 2014, IEEE T VIS COMPUT GR, V20, P1530, DOI 10.1109/TVCG.2014.2315995
   Kerracher N, 2017, CONSTRUCTING EVALUAT
   Kerracher N, 2015, IEEE T VIS COMPUT GR, V21, P1160, DOI 10.1109/TVCG.2015.2424889
   Liu SX, 2013, IEEE T VIS COMPUT GR, V19, P2436, DOI 10.1109/TVCG.2013.196
   Liu Zhen, 2016, Journal of Computer Aided Design & Computer Graphics, V28, P693
   Lu Q, 2014, INT C COMP SUPP COOP, P47, DOI 10.1109/CSCWD.2014.6846815
   Ma K.L, 2008, VISUALIZATION S
   Marriott K, 2012, IEEE T VIS COMPUT GR, V18, P2477, DOI 10.1109/TVCG.2012.245
   Ogawa M, 2010, SOFTVIS 2010: PROCEEDINGS OF THE 2010 INTERNATIONAL SYMPOSIUM ON SOFTWARE VISUALIZATION, P35
   Okoe M, 2015, COMPUT GRAPH FORUM, V34, P451, DOI 10.1111/cgf.12657
   Qiang L, 2017, VISUAL COMPUT, V33, P1
   Ribeiro AV, 2017, MOMENTO, P79
   Rufiange S, 2013, IEEE T VIS COMPUT GR, V19, P2556, DOI 10.1109/TVCG.2013.149
   Shi Y, 2018, IEEE T VIS COMPUT GR, V24, P1918, DOI 10.1109/TVCG.2018.2816203
   Tanahashi Y, 2012, IEEE T VIS COMPUT GR, V18, P2679, DOI 10.1109/TVCG.2012.212
   Wu YH, 2016, IEEE T VIS COMPUT GR, V22, P260, DOI [10.1109/TVCG.2015.2468151, 10.1109/TVCG.2015.2465151]
   Yoghourdjian V, 2018, VIS INFORM, V2, P264, DOI 10.1016/j.visinf.2018.12.006
   Zhao Y, 2019, IEEE T VIS COMPUT GR, V25, P12, DOI 10.1109/TVCG.2018.2865020
NR 35
TC 1
Z9 1
U1 2
U2 11
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2021
VL 37
IS 1
BP 161
EP 174
DI 10.1007/s00371-019-01789-1
EA JAN 2020
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA QE3UO
UT WOS:000574081100002
DA 2024-07-18
ER

PT J
AU Cao, ZH
   Mu, SM
   Dong, MP
AF Cao, Zhihao
   Mu, Shaomin
   Dong, Mengping
TI Two-attribute e-commerce image classification based on a convolutional
   neural network
SO VISUAL COMPUTER
LA English
DT Article
DE Convolutional neural network; Two-attribute image; Multi-task learning;
   Transfer learning
ID MODEL
AB A novel two-task learning method based on an improved convolutional neural network (CNN) using the idea of parameter transfer in transfer learning is proposed, aiming at the problem that a traditional convolutional neural network cannot simultaneously classify two attributes of e-commerce images. The network designed in this method has two channels, and each channel is responsible for learning a unique attribute of the image. First, the network is pre-trained by the channel corresponding to the most important attribute in the image, and the former network parameters are optimized. Then, two channels are used to train the network simultaneously. In the training process, the two learning tasks help each other by sharing parameters, which improves the convergence speed of the network and the generalization ability of the model. Aiming at the problem that there are fewer specific types of e-commerce images in datasets and the problem of class imbalance exists, a method of over-sampling based on the mix-up algorithm is proposed. The relationship between the complexity of the two attributes and the sparse rate of the CNN output feature matrix is studied, and the improved Grad-CAM algorithm is used to visualize and analyze the key areas for classification of two attributes, which improves the interpretability of the network. Experiments show that the proposed CNN method has good classification effect for two-attribute e-commerce images and traditional images.
C1 [Cao, Zhihao; Mu, Shaomin; Dong, Mengping] Shandong Agr Univ, Coll Informat Sci & Engn, Tai An 271018, Shandong, Peoples R China.
C3 Shandong Agricultural University
RP Mu, SM (corresponding author), Shandong Agr Univ, Coll Informat Sci & Engn, Tai An 271018, Shandong, Peoples R China.
EM msm@sdau.edu.cn
FU First Class Discipline Funding of Shandong Agricultural University
FX This work is supported by the First Class Discipline Funding of Shandong
   Agricultural University.
CR Abdulnabi AH, 2015, IEEE T MULTIMEDIA, V17, P1949, DOI 10.1109/TMM.2015.2477680
   Ak KE, 2018, IEEE WINT CONF APPL, P1671, DOI 10.1109/WACV.2018.00186
   BAO QP, 2017, COMPUTER APPL SOFTWA, V34, P255, DOI DOI 10.3969/j.issn.1000-386x.2017.04.043
   Baxter J, 1997, MACH LEARN, V28, P7, DOI 10.1023/A:1007327622663
   Bonilla EV., 2008, ADV NEURAL INFORM PR, P153, DOI DOI 10.5555/2981562.2981582
   Bossard L., 2012, Revised Selected Papers, P321
   Bui G, 2018, VISUAL COMPUT, V34, P829, DOI 10.1007/s00371-018-1550-6
   Cai N, 2017, VISUAL COMPUT, V33, P249, DOI 10.1007/s00371-015-1190-z
   Ching T, 2018, J R SOC INTERFACE, V15, DOI 10.1098/rsif.2017.0387
   Das A, 2017, COMPUT VIS IMAGE UND, V163, P90, DOI 10.1016/j.cviu.2017.10.001
   Eaton-Rosen Z., 2018, INT C MED IM DEEP LE
   Evgeniou T., 2004, P 10 ACM SIGKDD INT, P109, DOI DOI 10.1145/1014052.1014067
   Finkel JennyR., 2009, NAACL '09: Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, P602, DOI DOI 10.3115/1620754.1620842
   Goyal Y, 2017, PROC CVPR IEEE, P6325, DOI 10.1109/CVPR.2017.670
   He K., 2016, PROC CVPR IEEE, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]
   Huang SY, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P2049, DOI 10.1145/3240508.3240588
   Inoue H., 2018, arXiv
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li DW, 2015, PROCEEDINGS 3RD IAPR ASIAN CONFERENCE ON PATTERN RECOGNITION ACPR 2015, P111, DOI 10.1109/ACPR.2015.7486476
   [黎健成 Li Jiancheng], 2016, [计算机科学, Computer Science], V43, P41
   Li XJ, 2020, VISUAL COMPUT, V36, P39, DOI 10.1007/s00371-018-1582-y
   Liu S, 2012, PROC CVPR IEEE, P3330, DOI 10.1109/CVPR.2012.6248071
   Oquab M, 2014, PROC CVPR IEEE, P1717, DOI 10.1109/CVPR.2014.222
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Park JK, 2019, VISUAL COMPUT, V35, P1615, DOI 10.1007/s00371-018-1561-3
   Redmon J., 2016, PROC CVPR IEEE, DOI [10.1109/CVPR.2016.91, DOI 10.1109/CVPR.2016.91]
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Ruder S, 2017, arXiv preprint arXiv:1706.05098
   Selvaraju RR, 2020, INT J COMPUT VISION, V128, P336, DOI [10.1007/s11263-019-01228-7, 10.1109/ICCV.2017.74]
   Shu, 2018, ARXIV181100270
   Shu XB, 2018, IEEE T CIRC SYST VID, V28, P454, DOI 10.1109/TCSVT.2016.2607345
   Shu XB, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P35, DOI 10.1145/2733373.2806216
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220
   VERMA V, 2018, ARXIV180605236
   Wang Yaowei, 2018, Computer Engineering and Applications, V54, P21, DOI 10.3778/j.issn.1002-8331.1801-0170
   Yosinski J., 2014, Adv Neural Inf Process Syst, V2, P3320, DOI DOI 10.48550/ARXIV.1411.1792
   Yu K., 2005, P 22 INT C MACH LEAR, P1012
   Zhang Hongyi, 2017, ARXIV171009412
   Zhao JF, 2018, VISUAL COMPUT, V34, P1461, DOI 10.1007/s00371-018-1477-y
   Zhou B., 2016, P IEEE C COMP VIS PA, DOI DOI 10.1109/CVPR.2016.319
   Zhou F, 2019, VISUAL COMPUT, V35, P1583, DOI 10.1007/s00371-018-1559-x
   Zhou ZH., 2015, MACH LEARN
NR 44
TC 9
Z9 9
U1 0
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2020
VL 36
IS 8
BP 1619
EP 1634
DI 10.1007/s00371-019-01763-x
EA OCT 2019
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA NJ2RH
UT WOS:000544084700001
DA 2024-07-18
ER

PT J
AU An, J
   Park, S
   Ihm, I
AF An, Jaepung
   Park, Sanghun
   Ihm, Insung
TI Construction of a flexible and scalable 4D light field camera array
   using Raspberry Pi clusters
SO VISUAL COMPUTER
LA English
DT Article
DE 4D light field camera; Raspberry Pi; Camera array calibration; Light
   field rendering; GPU acceleration
ID PHOTOGRAPHY
AB The light field is a 4D function that represents the radiance of light traveling in every direction through every point in 3D space. In this paper, we demonstrate how effectively 4D light fields can be sampled from the real world using a custom-built 4D light field camera system constructed based on cheaply and commonly available Raspberry Pi computers. The camera system consisting of a camera array and a dedicated calibration software is flexible and scalable in structure because the system can be reconfigured easily to meet the user's needs. We show the effectiveness of the camera system by interactively visualizing captured 4D light fields using our GPU-accelerated light field renderer supporting a virtual camera model that synthesizes various photographic effects including zooming and panning, refocusing, focus breathing, and depth-of-field control. We believe that the do-it-yourself 4D light field camera will be explored effectively in traditional application fields, such as computer graphics and vision, and in other areas like science education.
C1 [An, Jaepung; Ihm, Insung] Sogang Univ, Dept Comp Sci & Engn, Seoul, South Korea.
   [Park, Sanghun] Dongguk Univ, Dept Multimedia, Seoul, South Korea.
C3 Sogang University; Dongguk University
RP Ihm, I (corresponding author), Sogang Univ, Dept Comp Sci & Engn, Seoul, South Korea.
EM ajp5050@sogang.ac.kr; mshpark@dongguk.edu; ihm@sogang.ac.kr
FU Basic Science Research Program through the National Research Foundation
   of Korea (NRF) - Ministry of Education [NRF-2017R1D1A1B03029625]
FX This research was supported by Basic Science Research Program through
   the National Research Foundation of Korea (NRF) funded by the Ministry
   of Education (No. NRF-2017R1D1A1B03029625).
CR Aaftab Munshi L. H., 2015, OPENCL C SPEC VERS 2
   ADELSON EH, 1992, IEEE T PATTERN ANAL, V14, P99, DOI 10.1109/34.121783
   Adelson EH, 1991, COMPUTATIONAL MODELS
   Adhikarla VK, 2015, VISUAL COMPUT, V31, P1023, DOI 10.1007/s00371-015-1127-6
   [Anonymous], 2005, Comput. Sci. Tech. Rep.
   [Anonymous], 2006, DIGITAL LIGHT FIELD
   [Anonymous], 2001, Robotica, DOI DOI 10.1017/S0263574700223217
   BROWN DC, 1971, PHOTOGRAMM ENG, V37, P855
   Burden R. L., 2015, Cengage Learning
   Donatsch D, 2014, VISUAL COMPUT, V30, P897, DOI 10.1007/s00371-014-0979-5
   Foundation R. P., RASPB PI
   Georgiev T. G., 2006, P 17 EUROGRAPHICS C, V2006
   Gortler S.J., 1996, ACM T GRAPH, V23, P43
   Levoy M., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P31, DOI 10.1145/237170.237199
   Levoy M, 2006, ACM T GRAPHIC, V25, P924, DOI 10.1145/1141911.1141976
   Liang CK, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360654
   Lin X, 2015, BIOMED OPT EXPRESS, V6, P3179, DOI 10.1364/BOE.6.003179
   Shirley P., 1997, Journal of Graphics Tools, V2, P45, DOI 10.1080/10867651.1997.10487479
   Stroebel Leslie., 1986, Photographic materials and processes
   Veeraraghavan A, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239520
   Venkataraman K, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508390
   Wanner S, 2012, PROC CVPR IEEE, P41, DOI 10.1109/CVPR.2012.6247656
   Wilburn B, 2005, ACM T GRAPHIC, V24, P765, DOI 10.1145/1073204.1073259
   Yang J, 2002, SIXTH IEEE WORKSHOP ON APPLICATIONS OF COMPUTER VISION, PROCEEDINGS, P19, DOI 10.1109/ACV.2002.1182149
   Zhang Cha., 2004, Proceedings of Eurographics Symposium on Rendering, P243
NR 25
TC 2
Z9 2
U1 0
U2 18
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2019
VL 35
IS 10
BP 1475
EP 1488
DI 10.1007/s00371-018-1512-z
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IU7PN
UT WOS:000483775900010
DA 2024-07-18
ER

PT J
AU Li, GD
   Wang, LL
AF Li, Guo-dong
   Wang, Le-le
TI Double chaotic image encryption algorithm based on optimal sequence
   solution and fractional transform
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 36th Computer Graphics International Conference (CGI)
CY JUN 17-20, 2019
CL Calgary, CANADA
DE DCT; Fractional Fourier; Image encryption; Backpack problem; Chaotic
   encryption
AB In order to solve the shortcomings of the traditional chaotic encryption and problems of low security, the double chaotic image encryption algorithm based on fractional Fourier transform is proposed. In this algorithm, the optimization algorithm is obtained with the help of Henon mapping and fractional Fourier transforms, then the ciphertext image obtained by the optimization algorithm is taken as input, and according to the sequence of the optimal solution, the logistic chaos is synthesized to obtain the final ciphertext image. This algorithm combines chaotic systems and Fourier transforms, which allows the plaintext to be well hidden, and spatial and frequency domain scrambling is achieved. After experiments, the results show that the improved encryption algorithm achieves better encryption effect. It not only has strong sensitivity and large key space, but also can resist attacks effectively. It has certain application value in image information security.
C1 [Li, Guo-dong; Wang, Le-le] Xinjiang Univ Finance & Econ, Dept Appl Math, Urumqi 830012, Peoples R China.
C3 Xinjiang University of Finance & Economics
RP Li, GD (corresponding author), Xinjiang Univ Finance & Econ, Dept Appl Math, Urumqi 830012, Peoples R China.
EM wangiaomin@sina.com
RI Li, Guodong/JDX-1233-2023
OI Li, Guodong/0000-0003-1275-2982
FU Natural resources fund of Xinjiang Uygur autonomous region [2017D01A24];
   National Natural science foundation of China [11461063]; Xinjiang
   University of finance postgraduate research innovation project
   [2018K041]
FX This work is partially supported by Natural resources fund of Xinjiang
   Uygur autonomous region (2017D01A24); National Natural science
   foundation of China (11461063); Xinjiang University of finance
   postgraduate research innovation project (XJUFE2017K006); Xinjiang
   University of finance postgraduate research innovation project
   (2018K041).
CR ADLEMAN LM, 1994, SCIENCE, V266, P1021, DOI 10.1126/science.7973651
   Cheng PG, 2015, NONLINEAR DYNAM, V79, P2121, DOI 10.1007/s11071-014-1798-y
   Ganesan K, 2014, EUR PHYS J-SPEC TOP, V223, P1611, DOI 10.1140/epjst/e2014-02123-1
   Huang Dong-Mei, 2016, Journal of Software, V27, P1729, DOI 10.13328/j.cnki.jos.005039
   Huang QM, 2017, JIANGXI MIANYANG NOR, V2, P60
   Jianguo Jin, 2013, J CHIN PICT GR, V18, P1567
   Li Pan-chi, 2016, Control and Decision, V31, P1363, DOI 10.13195/j.kzyjc.2015.0782
   Matthews R., 1989, Cryptologia, V13, P29, DOI [10.1080/0161-118991863745, DOI 10.1080/0161-118991863745]
   Mohindru P, 2012, P NATL A SCI INDIA A, V82, P343, DOI 10.1007/s40010-012-0058-0
   SHANNON CE, 1949, BELL SYST TECH J, V28, P656, DOI 10.1002/j.1538-7305.1949.tb00928.x
   [施飞 Shi Fei], 2018, [计算机工程与应用, Computer Engineering and Application], V54, P91
   Wang XY, 2012, INT J MOD PHYS B, V26, DOI 10.1142/S0217979212501755
   Xie GB, 2017, J COMPUT ENG APPL
   Xie GB, 2017, COMPUT ENG APPL
   Yan S., 2014, J XUZHOU I TECHNOL N, P39
   Yang Jiyun, 2018, Computer Engineering, V44, P151, DOI 10.3969/j.issn.1000-3428.2018.02.027
   Yang Xu, 2018, PACKAGE ENG, V7, P180
   Zhang Y, 2015, COMPUT APPL RES, V32, P1770
   Zhao GM, 2015, ACTA SCI NAT U SUNYA, V1, P141
NR 19
TC 26
Z9 26
U1 2
U2 33
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2019
VL 35
IS 9
BP 1267
EP 1277
DI 10.1007/s00371-018-1574-y
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA IQ2JJ
UT WOS:000480574500007
DA 2024-07-18
ER

PT J
AU Wen, JH
   Ma, H
AF Wen, Jinghuan
   Ma, Huimin
TI Real-time smoke simulation based on vorticity preserving lattice
   Boltzmann method
SO VISUAL COMPUTER
LA English
DT Article
DE Smoke simulation; Lattice Boltzmann method; Vorticity confinement; Real
   time
ID VORTEX PARTICLE METHOD; FLOWS; COMPUTATION; PROJECTION; TURBULENCE;
   MODEL; WATER; GPU
AB It is crucial to achieve high efficiency and to preserve fine-grid details in 3D interactive smoke simulation. In this paper, we present a vorticity preserving lattice Boltzmann method (VPLBM) to simulate high-resolution motion of smoke in real time. We design the method based on lattice Boltzmann method (LBM), which is parallelism-friendly, to ensure the efficiency on parallel computing devices such as graphic processing units (GPUs). To resolve the vorticity dissipation issue in LBM, we further propose an LBM-based vorticity transport method which tracks the magnitude of vorticity during the simulation. We apply vorticity confinement according to the tracked vorticity, and therefore, our method can preserve the detailed motion of smoke. Our method is forward-iterative and contains three weakly dependent distribution functions: two for thermal LBM and one for vorticity tracking. Finally, we show an implementation of a parallel smoke simulator integrating VPLBM method on a dual-GPU platform. Using a fast ray marching method, the simulator shows realistic visual effect and achieves 157.3 frames per second on a 64 x 64 x 128 fine grid.
C1 [Wen, Jinghuan] Tsinghua Univ, Beijing, Peoples R China.
   [Ma, Huimin] Tsinghua Univ, Dept Elect Engn, Beijing, Peoples R China.
C3 Tsinghua University; Tsinghua University
RP Ma, H (corresponding author), Tsinghua Univ, Dept Elect Engn, Beijing, Peoples R China.
EM wenjh14@mails.tsinghua.edu.cn; mhmpub@tsinghua.edu.cn
FU National Key Basic Research Program of China [2016YFB0100900]; National
   Natural Science Foundation of China [61171113, 61773231]
FX This work is supported by National Key Basic Research Program of China
   (No. 2016YFB0100900) and National Natural Science Foundation of China
   (No. 61171113 and No. 61773231).
CR Alim U. R, 2007, THESIS
   Alim UR, 2009, IEEE T VIS COMPUT GR, V15, P630, DOI 10.1109/TVCG.2008.201
   Angelidis Alexis., 2006, S COMPUTER ANIMATION, P25
   [Anonymous], 2012, Computer Animation 2012-ACM SIGGRAPH / Eurographics Symposium Proceedings, SCA
   [Anonymous], 2005, Proceedings of the 2005 ACM SIGGRAPH/Eurographics Symposium on Computer Animation, DOI DOI 10.1145/1073368.1073380
   Azevedo VC, 2013, COMPUT GRAPH FORUM, V32, P235, DOI 10.1111/cgf.12231
   Bailey Peter, 2009, Proceedings of the 2009 International Conference on Parallel Processing (ICPP 2009), P550, DOI 10.1109/ICPP.2009.38
   Bender J., 2015, P 14 ACM SIGGRAPH EU, P147, DOI DOI 10.1145/2786784.2786796
   Bernaschi M, 2010, CONCURR COMP-PRACT E, V22, P1, DOI 10.1002/cpe.1466
   Bolz J, 2003, ACM T GRAPHIC, V22, P917, DOI 10.1145/882262.882364
   Chen S, 1998, ANNU REV FLUID MECH, V30, P329, DOI 10.1146/annurev.fluid.30.1.329
   CHEN SY, 1991, PHYS REV LETT, V67, P3776, DOI 10.1103/PhysRevLett.67.3776
   DHUMIERES D, 1994, PROGR ASTRONAUT AERO, V159, P450
   Dixit HN, 2006, INT J HEAT MASS TRAN, V49, P727, DOI 10.1016/j.ijheatmasstransfer.2005.07.046
   Fedkiw R, 2001, COMP GRAPH, P15, DOI 10.1145/383259.383260
   Feichtinger C, 2011, PARALLEL COMPUT, V37, P536, DOI 10.1016/j.parco.2011.03.005
   Guo ZL, 2002, CHINESE PHYS, V11, P366, DOI 10.1088/1009-1963/11/4/310
   He S, 2013, COMPUT GRAPH FORUM, V32, P27, DOI 10.1111/j.1467-8659.2012.03228.x
   He SF, 2011, COMPUT ANIMAT VIRT W, V22, P107, DOI 10.1002/cav.408
   He X, 1998, J COMPUT PHYS, V146, P282, DOI 10.1006/jcph.1998.6057
   Huang RG, 2013, VISUAL COMPUT, V29, P751, DOI 10.1007/s00371-013-0798-0
   Jami M, 2016, INT J THERM SCI, V100, P98, DOI 10.1016/j.ijthermalsci.2015.09.011
   Kim B., 2005, TECH REP
   KOELMAN JMVA, 1991, EUROPHYS LETT, V15, P603, DOI 10.1209/0295-5075/15/6/007
   Kuznik F, 2010, COMPUT MATH APPL, V59, P2380, DOI 10.1016/j.camwa.2009.08.052
   Lallemand P, 2000, PHYS REV E, V61, P6546, DOI 10.1103/PhysRevE.61.6546
   Li W, 2003, VISUAL COMPUT, V19, P444, DOI 10.1007/s00371-003-0210-6
   Li XS, 2014, VISUAL COMPUT, V30, P787, DOI 10.1007/s00371-014-0969-7
   Li XC, 2016, Adv Inform Managemen, P1757, DOI 10.1109/IMCEC.2016.7867520
   Losasso F, 2004, ACM T GRAPHIC, V23, P457, DOI 10.1145/1015706.1015745
   Mei R, 2000, J COMPUT PHYS, V161, P680, DOI 10.1006/jcph.2000.6522
   Meng Z, 2015, IEEE COMPUT GRAPH, V35, P60, DOI 10.1109/MCG.2015.5
   Mezrhab A, 2010, PHYS LETT A, V374, P3499, DOI 10.1016/j.physleta.2010.06.059
   Mohamad AA, 2011, LATTICE BOLTZMANN METHOD: FUNDAMENTALS AND ENGINEERING APPLICATIONS WITH COMPUTER CODES, P1, DOI 10.1007/978-0-85729-455-5
   Mohamad AA, 2010, INT J HEAT MASS TRAN, V53, P990, DOI 10.1016/j.ijheatmasstransfer.2009.11.014
   Muller M., 2003, ACM SIGGRAPH/Eurographics Symposium on Computer Animation, P154
   Obrecht C, 2013, COMPUT MATH APPL, V65, P252, DOI 10.1016/j.camwa.2011.02.020
   Obrecht C, 2011, COMPUT MATH APPL, V61, P3628, DOI 10.1016/j.camwa.2010.01.054
   Pfaff T, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185608
   QIAN YH, 1992, EUROPHYS LETT, V17, P479, DOI 10.1209/0295-5075/17/6/001
   Rasmussen N, 2003, ACM T GRAPHIC, V22, P703, DOI 10.1145/882262.882335
   Rinaldi PR, 2012, SIMUL MODEL PRACT TH, V25, P163, DOI 10.1016/j.simpat.2012.03.004
   Sato S, 2015, VISUAL COMPUT, V31, P959, DOI 10.1007/s00371-015-1122-y
   Schönherr M, 2011, COMPUT MATH APPL, V61, P3730, DOI 10.1016/j.camwa.2011.04.012
   Selle A, 2005, ACM T GRAPHIC, V24, P910, DOI 10.1145/1073204.1073282
   Selle A, 2008, J SCI COMPUT, V35, P350, DOI 10.1007/s10915-007-9166-4
   Setaluri Rajsekhar, 2014, ACM Transactions on Graphics, V33, DOI 10.1145/2661229.2661269
   Solenthaler B, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531346
   Stam J, 1999, COMP GRAPH, P121, DOI 10.1145/311535.311548
   STEINHOFF J, 1994, PHYS FLUIDS, V6, P2738, DOI 10.1063/1.868164
   Thuerey N., 2004, VISION MODELING VISU, P199
   Thürey N, 2009, COMPUT VIS SCI, V12, P247, DOI 10.1007/s00791-008-0090-4
   Tölke J, 2010, COMPUT VIS SCI, V13, P29, DOI 10.1007/s00791-008-0120-2
   Wei XM, 2004, IEEE T VIS COMPUT GR, V10, P719, DOI 10.1109/TVCG.2004.48
   Wei XM, 2004, IEEE T VIS COMPUT GR, V10, P164, DOI 10.1109/TVCG.2004.1260768
   Wu XY, 2013, COMPUT GRAPH FORUM, V32, P389, DOI 10.1111/cgf.12059
   Xian W, 2011, PARALLEL COMPUT, V37, P521, DOI 10.1016/j.parco.2011.02.007
   Yoon JC, 2009, COMPUT GRAPH FORUM, V28, P1853, DOI 10.1111/j.1467-8659.2009.01563.x
   Zhang GJ, 2011, VISUAL COMPUT, V27, P199, DOI 10.1007/s00371-010-0526-y
   Zhang XX, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2661229.2661261
   Zhang XX, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766982
   Zhu YN, 2005, ACM T GRAPHIC, V24, P965, DOI 10.1145/1073204.1073298
   Zuo Q, 2013, VISUAL COMPUT, V29, P883, DOI 10.1007/s00371-013-0848-7
NR 63
TC 6
Z9 9
U1 1
U2 13
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2019
VL 35
IS 9
BP 1279
EP 1292
DI 10.1007/s00371-018-1514-x
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IQ2JJ
UT WOS:000480574500008
DA 2024-07-18
ER

PT J
AU Jiang, T
   Yang, XS
   Zhang, JJ
   Tian, F
   Liu, S
   Xiang, N
   Qian, K
AF Jiang, Tao
   Yang, Xiaosong
   Zhang, Jianjun
   Tian, Feng
   Liu, Shuang
   Xiang, Nan
   Qian, Kun
TI Huber-L1-based non-isometric surface registration
SO VISUAL COMPUTER
LA English
DT Article
DE Surface registration; Huber-L-1; Non-isometric
ID NONRIGID REGISTRATION
AB Non-isometric surface registration is an important task in computer graphics and computer vision. It, however, remains challenging to deal with noise from scanned data and distortion from transformation. In this paper, we propose a Huber-L1-based non-isometric surface registration and solve it by the alternating direction method of multipliers. With a Huber-L1-regularized model constrained on the transformation variation and position difference, our method is robust to noise and produces piecewise smooth results while still preserving fine details on the target. The introduced as-similar-as-possible energy is able to handle different size of shapes with little stretching distortion. Extensive experimental results have demonstrated that our method is more accurate and robust to noise in comparison with the state-of-the-arts.
C1 [Jiang, Tao] Bournemouth Univ, Fac Media & Commun, Bournemouth, Dorset, England.
   [Yang, Xiaosong; Zhang, Jianjun; Liu, Shuang; Xiang, Nan] Bournemouth Univ, Natl Ctr Comp Animat, Bournemouth, Dorset, England.
   [Tian, Feng] Bournemouth Univ, Bournemouth, Dorset, England.
   [Qian, Kun] Kings Coll London, Bioengn Dept, London, England.
C3 Bournemouth University; Bournemouth University; Bournemouth University;
   University of London; King's College London
RP Yang, XS (corresponding author), Bournemouth Univ, Natl Ctr Comp Animat, Bournemouth, Dorset, England.
EM xyang@bournemouth.ac.uk
OI Yang, Xiaosong/0000-0003-3815-0584
FU EU H2020 under the REA Grant Agreement [691215]
FX This study was funded by EU H2020 under the REA Grant Agreement (Grant
   Number 691215)
CR Amberg B, 2007, IEEE I CONF COMP VIS, P1326
   BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791
   Bouaziz S, 2013, COMPUT GRAPH FORUM, V32, P113, DOI 10.1111/cgf.12178
   Boyd Stephen, 2010, Foundations and Trends in Machine Learning, V3, P1, DOI 10.1561/2200000016
   Chambolle A, 2011, J MATH IMAGING VIS, V40, P120, DOI 10.1007/s10851-010-0251-1
   Floater MS, 2005, MATH VIS, P157, DOI 10.1007/3-540-26808-1_9
   Huang QX, 2008, COMPUT GRAPH FORUM, V27, P1449, DOI 10.1111/j.1467-8659.2008.01285.x
   Jiang T, 2017, VISUAL COMPUT, V33, P891, DOI 10.1007/s00371-017-1390-9
   Lee A, 2000, COMP GRAPH, P85, DOI 10.1145/344779.344829
   Li H, 2008, COMPUT GRAPH FORUM, V27, P1421, DOI 10.1111/j.1467-8659.2008.01282.x
   Li K, 2019, IEEE T VIS COMPUT GR, V25, P2255, DOI 10.1109/TVCG.2018.2832136
   Liao M, 2009, IEEE I CONF COMP VIS, P167, DOI 10.1109/ICCV.2009.5459161
   Low K.-L., 2004, Chapel Hill, Univ. North Carolina, V4, P1
   Moenning C., 2003, Technical Report
   Müller M, 2005, ACM T GRAPHIC, V24, P471, DOI 10.1145/1073204.1073216
   Papazov C, 2011, COMPUT GRAPH FORUM, V30, P1493, DOI 10.1111/j.1467-8659.2011.02023.x
   Rouhani Mohammad, 2014, 2014 2nd International Conference on 3D Vision (3DV). Proceedings, P617, DOI 10.1109/3DV.2014.80
   RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F
   Sorkine Olga., 2007, Symposium on Geometry processing, V4, page, P30
   Süssmuth J, 2008, COMPUT GRAPH FORUM, V27, P1469, DOI 10.1111/j.1467-8659.2008.01287.x
   Sumner RW, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239531
   Sumner RW, 2004, ACM T GRAPHIC, V23, P399, DOI 10.1145/1015706.1015736
   Surazhsky V, 2005, ACM T GRAPHIC, V24, P553, DOI 10.1145/1073204.1073228
   Tam GKL, 2013, IEEE T VIS COMPUT GR, V19, P1199, DOI 10.1109/TVCG.2012.310
   Wand M, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1516522.1516526
   WERLBERGER M., 2009, BMVC, V1, P3
   Yang JY, 2015, COMPUT GRAPH FORUM, V34, P89, DOI 10.1111/cgf.12699
   Yeh IC, 2011, IEEE T VIS COMPUT GR, V17, P1178, DOI 10.1109/TVCG.2010.124
   Yoshiyasu Y, 2014, COMPUT GRAPH FORUM, V33, P257, DOI 10.1111/cgf.12451
   Zach C, 2007, LECT NOTES COMPUT SC, V4713, P214, DOI 10.1007/978-3-540-74936-3_22
NR 30
TC 4
Z9 6
U1 0
U2 7
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2019
VL 35
IS 6-8
SI SI
BP 935
EP 948
DI 10.1007/s00371-019-01670-1
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IC1IH
UT WOS:000470712200014
OA hybrid
DA 2024-07-18
ER

PT J
AU Frigo, O
   Sabater, N
   Delon, J
   Hellier, P
AF Frigo, Oriel
   Sabater, Neus
   Delon, Julie
   Hellier, Pierre
TI Video style transfer by consistent adaptive patch sampling
SO VISUAL COMPUTER
LA English
DT Article
DE Style transfer; Texture synthesis; Non-photorealistic rendering; Video
   processing
ID TEXTURE SYNTHESIS
AB This paper addresses the example-based stylization of videos. Style transfer aims at editing an image so that it matches the style of an example. This topic has been recently investigated by several researchers, both in the industry and in academia. The difficulty lies in how to capture the style of an image and correctly transferring it to a video. In this paper, we build on our previous work Split and Match for still pictures, based on adaptive patch synthesis. We address the issue of extending that particular technique to video, ensuring that the solution is spatially and temporally consistent. Results show that our video style transfer is visually plausible, while being very competitive regarding computation time and memory when compared to neural network approaches.
C1 [Frigo, Oriel; Sabater, Neus; Hellier, Pierre] Technicolor R&I, 975 Ave Champs Blancs, F-35576 Cesson Sevigne, France.
   [Delon, Julie] Univ Paris 05, Appl Math, 45 Rue St Peres, Paris, France.
C3 Technicolor SA; Universite Paris Cite
RP Frigo, O (corresponding author), Technicolor R&I, 975 Ave Champs Blancs, F-35576 Cesson Sevigne, France.
EM oriel.frigo@technicolor.com; neus.sabater@technicolor.com;
   julie.delon@parisdescartes.fr; pierre.hellier@technicolor.com
RI Frigo, Oriel/AAW-5674-2020; Delon, Julie/U-5964-2017; Frigo,
   Oriel/I-6485-2017
CR [Anonymous], 2010, LECT NOTES COMPUTER
   [Anonymous], ACM T GRAPH
   [Anonymous], ARXIV150806576
   [Anonymous], 2016, ARXIV E PRINTS
   [Anonymous], ARXIV170104928
   [Anonymous], 2016, ARXIV160903057
   Barnes C., 2010, LECT NOTES COMPUT SC, V6313, P29
   Bénard P, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461929
   Butler DJ, 2012, LECT NOTES COMPUT SC, V7577, P611, DOI 10.1007/978-3-642-33783-3_44
   Criminisi A, 2004, IEEE T IMAGE PROCESS, V13, P1200, DOI 10.1109/TIP.2004.833105
   Dosovitskiy A., 2016, LECT NOTES COMPUTER, V9796
   Dumoulin V., 2017, A Learned Representation for Artistic Style
   Durand F., 2002, NPAR '02: Proceedings of the 2nd international symposium on Non-photorealistic animation and rendering, P111
   Efros A. A., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1033, DOI 10.1109/ICCV.1999.790383
   Efros AA, 2001, COMP GRAPH, P341, DOI 10.1145/383259.383296
   Farbman Z., 2011, ACM T GRAPHICS P ACM, V30
   Fier J., 2016, ACM T GRAPHIC, V35, P3
   Freeman WT, 2000, INT J COMPUT VISION, V40, P25, DOI 10.1023/A:1026501619075
   Freeman WT, 2002, IEEE COMPUT GRAPH, V22, P56, DOI 10.1109/38.988747
   Frigo O., 2014, COMP VIS ACCV 2014 1
   Frigo O, 2016, IEEE T IMAGE PROCESS, V25, P5455, DOI 10.1109/TIP.2016.2601267
   Frigo Oriel, 2016, IEEE C COMP VIS PATT
   Hertzmann A, 2001, COMP GRAPH, P327, DOI 10.1145/383259.383295
   Kyprianidis JE, 2013, IEEE T VIS COMPUT GR, V19, P866, DOI 10.1109/TVCG.2012.160
   Li C, 2016, LECT NOTES COMPUT SC, V9907, P702, DOI 10.1007/978-3-319-46487-9_43
   Liang L, 2001, ACM T GRAPHIC, V20, P127, DOI 10.1145/501786.501787
   Pearl J., 1988, PROBABILISTIC REASON
   Puy G., 2017, ARXIV170207759 CORR
   Reinhard E, 2001, IEEE COMPUT GRAPH, V21, P34, DOI 10.1109/38.946629
   Shih YC, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601137
   Wang XG, 2009, IEEE T PATTERN ANAL, V31, P1955, DOI 10.1109/TPAMI.2008.222
   Weinzaepfel P., 2013, IEEE INT C COMP VIS
   Weiss Y, 1997, TECHNICAL REPORT
   Yi ZL, 2017, VISUAL COMPUT, V33, P1443, DOI 10.1007/s00371-016-1290-4
NR 34
TC 11
Z9 14
U1 0
U2 9
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2019
VL 35
IS 3
BP 429
EP 443
DI 10.1007/s00371-018-1474-1
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HP0MR
UT WOS:000461360600010
DA 2024-07-18
ER

PT J
AU Fan, Q
   Shen, XK
   Hu, Y
AF Fan, Qing
   Shen, Xukun
   Hu, Yong
TI Detail-preserved real-time hand motion regression from depth
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 35th Computer Graphics International conference (CGI)
CY JUN 11-14, 2018
CL INDONESIA
SP Comp Graph Soc, Nanyang Technol Univ
DE Hand tracking; Deep convolutional network; Skeleton alignment; Detail
   preserving; Virtual reality
ID POSE ESTIMATION
AB This paper aims to address the very challenging problem of efficient and accurate hand tracking from depth sequences, meanwhile to deform a high-resolution 3D hand model with geometric details. We propose an integrated regression framework to infer articulated hand pose, and regress high-frequency details from sparse high-resolution 3D hand model examples. Specifically, our proposed method mainly consists of four components: skeleton embedding, hand joint regression, skeleton alignment, and high-resolution details integration. Skeleton embedding is optimized via a wrinkle-based skeleton refinement method for faithful hand models with fine geometric details. Hand joint regression is based on a deep convolutional network, from which 3D hand joint locations are predicted from a single depth map, then a skeleton alignment stage is performed to recover fully articulated hand poses. Deformable fine-scale details are estimated from a nonlinear mapping between the hand joints and per-vertex displacements. Experiments on two challenging datasets show that our proposed approach can achieve accurate, robust, and real-time hand tracking, while preserve most high-frequency details when deforming a virtual hand.
C1 [Fan, Qing; Shen, Xukun] Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing, Peoples R China.
   [Hu, Yong] Beihang Univ, Sch New Media Art & Design, Beijing, Peoples R China.
C3 Beihang University; Beihang University
RP Hu, Y (corresponding author), Beihang Univ, Sch New Media Art & Design, Beijing, Peoples R China.
EM huyong@buaa.edu.cn
FU National Key Technologies R & D Program of China [2017YFB1002702]
FX We would like to thank the anonymous reviewers for their valuable
   suggestions. This paper is supported by National Key Technologies R & D
   Program of China (No. 2017YFB1002702).
CR [Anonymous], COMP VIS PATT REC 20
   [Anonymous], 2011, BMVC
   Baran I, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1239451.1239523, 10.1145/1276377.1276467]
   Erol A, 2007, COMPUT VIS IMAGE UND, V108, P52, DOI 10.1016/j.cviu.2006.10.012
   Fan Q, 2019, PATTERN RECOGN LETT, V119, P205, DOI 10.1016/j.patrec.2017.10.019
   Garland M., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P209, DOI 10.1145/258734.258849
   Ge L, 2016, PROC CVPR IEEE, P3593, DOI 10.1109/CVPR.2016.391
   Guo KW, 2015, IEEE I CONF COMP VIS, P3083, DOI 10.1109/ICCV.2015.353
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Heap T, 1996, PROCEEDINGS OF THE SECOND INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, P140, DOI 10.1109/AFGR.1996.557255
   Huang HD, 2012, IEEE T VIS COMPUT GR, V18, P1215, DOI 10.1109/TVCG.2012.88
   Li H, 2009, AM J PHYS ANTHROPOL, P175
   Oberweger M., 2015, COMP VIS WINT WORKSH, P21
   Oberweger M, 2015, IEEE I CONF COMP VIS, P3316, DOI 10.1109/ICCV.2015.379
   Oikonomidis I, 2012, PROC CVPR IEEE, P1862, DOI 10.1109/CVPR.2012.6247885
   Schröder M, 2014, IEEE INT CONF ROBOT, P5447, DOI 10.1109/ICRA.2014.6907660
   Sridhar S, 2013, IEEE I CONF COMP VIS, P2456, DOI 10.1109/ICCV.2013.305
   Sun X, 2015, PROC CVPR IEEE, P824, DOI 10.1109/CVPR.2015.7298683
   Supancic III JS, 2015, ARXIV150406378
   Tagliasacchi A, 2015, COMPUT GRAPH FORUM, V34, P101, DOI 10.1111/cgf.12700
   Taylor J, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925965
   Taylor J, 2012, PROC CVPR IEEE, P103, DOI 10.1109/CVPR.2012.6247664
   Tkach A, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980226
   Tompson J, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2629500
   Wang R, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531397
   Xu C, 2013, IEEE I CONF COMP VIS, P3456, DOI 10.1109/ICCV.2013.429
   Zhou X., 2016, IJCAI
   Zollhöfer M, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601165
NR 28
TC 5
Z9 6
U1 0
U2 15
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2018
VL 34
IS 9
BP 1145
EP 1154
DI 10.1007/s00371-018-1546-2
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA GQ5MB
UT WOS:000441727000002
DA 2024-07-18
ER

PT J
AU Alderson, T
   Mahdavi-Amiri, A
   Samavati, F
AF Alderson, Troy
   Mahdavi-Amiri, Ali
   Samavati, Faramarz
TI Offsetting spherical curves in vector and raster form
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 35th Computer Graphics International conference (CGI)
CY JUN 11-14, 2018
CL INDONESIA
SP Comp Graph Soc, Nanyang Technol Univ
DE Offset; Multiresolution; Spherical curves
ID ALGORITHM; SURFACES; POINT; POLYGON
AB In this paper, we present techniques for offsetting spherical curves represented in vector or raster form. Such techniques allow us to efficiently determine and visualize the region within a given distance of a spherical curve. Our methods additionally support multiresolution representations of the underlying data, allowing the initial coarse offsets to be provided quickly, which may then be iteratively refined to the correct result. An example application of offsetting is also specifically explored in the form of improving the performance of inside/outside tests in the vector case.
C1 [Alderson, Troy; Samavati, Faramarz] Univ Calgary, Dept Comp Sci, Calgary, AB, Canada.
   [Mahdavi-Amiri, Ali] Simon Fraser Univ, GrUVi Lab, Burnaby, BC, Canada.
C3 University of Calgary; Simon Fraser University
RP Alderson, T (corresponding author), Univ Calgary, Dept Comp Sci, Calgary, AB, Canada.
EM tfalders@ucalgary.ca; amahdavi@sfu.ca; samavati@ucalgary.ca
OI Alderson, Troy/0000-0001-5528-8652
FU Alberta Innovates-Technology Futures (AITF); NSERC CRD; PYXIS innovation
FX This research was funded by Alberta Innovates-Technology Futures (AITF)
   and by an NSERC CRD with our collaborator, the PYXIS innovation.
CR Alderson T, 2016, GRAPH MODELS, V86, P13, DOI 10.1016/j.gmod.2016.05.002
   [Anonymous], DIFFERENTIAL GEOMETR
   [Anonymous], 2000, P 1 INT C DISCR GLOB
   BEVIS M, 1989, MATH GEOL, V21, P811, DOI 10.1007/BF00894449
   BRESENHAM JE, 1965, IBM SYST J, V4, P25, DOI 10.1147/sj.41.0025
   Choi BK, 1999, COMPUT AIDED DESIGN, V31, P735, DOI 10.1016/S0010-4485(99)00060-3
   Gore A., 1998, DIGITAL EARTH UNDERS
   Grohs P, 2012, J FRANKLIN I, V349, P1648, DOI 10.1016/j.jfranklin.2011.02.010
   Kersting Oliver., P 10 ACM INT S ADV G, P107, DOI DOI 10.1145/585147.585170
   Liu XZ, 2007, COMPUT IND, V58, P240, DOI 10.1016/j.compind.2006.06.002
   Maekawa T, 1999, COMPUT AIDED DESIGN, V31, P165, DOI 10.1016/S0010-4485(99)00013-5
   Mahdavi-Amiri A, 2016, COMPUT AIDED DESIGN, V79, P12, DOI 10.1016/j.cad.2016.04.005
   Mahdavi-Amiri A, 2015, COMPUT GRAPH-UK, V53, P95, DOI 10.1016/j.cag.2015.08.005
   Murphy A., 1978, IBM TECH DISCL B, V20, P5358
   Myoung-Jun Kim, 1995, Computer Graphics Proceedings. SIGGRAPH 95, P369
   NationalGeospatial- Intelligence Agency, WORLD GEOD SYST
   PATRIKALAKIS NM, 1989, ENG COMPUT, V5, P39, DOI 10.1007/BF01201996
   PHAM B, 1992, COMPUT AIDED DESIGN, V24, P223, DOI 10.1016/0010-4485(92)90059-J
   Prentice- Hall Inc., 1976, DIFFERENTIAL GEOMETR
   Schneider M., 2005, Proceedings of the 11th International Conference on Virtual Systems and Multimedia (VSMM), October 3-7, Ghent, Belgium, P573
   Schneider M, 2007, JOURNAL WSCG, V15, P59
   SHIMRAT M, 1962, COMMUN ACM, V5, P434, DOI 10.1145/368637.368653
   Shoemaker K., 1985, Computer Graphics, V19, P245, DOI 10.1145/325165.325242
   Snyder J., 1992, Cartographica, V29, P10, DOI [10.3138/27H7-8K88-4882-1752, DOI 10.3138/27H7-8K88-4882-1752]
   Xin SQ, 2011, COMPUT AIDED DESIGN, V43, P1468, DOI 10.1016/j.cad.2011.08.027
   Zhou MY, 2016, INT J DIGIT EARTH, V9, P230, DOI 10.1080/17538947.2015.1016558
NR 26
TC 8
Z9 8
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2018
VL 34
IS 6-8
SI SI
BP 973
EP 984
DI 10.1007/s00371-018-1525-7
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA GH6MC
UT WOS:000433557400020
DA 2024-07-18
ER

PT J
AU Lian, YJ
   Shen, XK
   Hu, Y
AF Lian, Yongjian
   Shen, Xukun
   Hu, Yong
TI Detecting and inferring repetitive elements with accurate locations and
   shapes from facades
SO VISUAL COMPUTER
LA English
DT Article
DE Repetition detection and occlusion inference; Adaptive region
   descriptor; Image content term; Facade context term; Repetitive
   characteristic curve; Bayesian probability network
ID HYBRID BAYESIAN NETWORKS; MIXTURES; INFERENCE
AB The use of repetition detection is an effective approach for increasing the efficiency of urban modeling. In practice, repetition detection can benefit from the apparent regularities and strong contextual relationships in facades. In view of this, we propose a novel algorithm for automatically detecting and inferring repetitive elements with accurate locations and shapes from facades. More specifically, firstly, starting from a rectification of the input facade, we employ the color clustering method to automatically derive candidate templates. Secondly, to detect the non- and partially occluded repetitive elements matching with the derived templates, we construct an adaptive region descriptor and a repetitive characteristic curve. Finally, the fully occluded elements are inferred by utilizing the Bayesian probability network, which can be learned from a database of the selected facades. The accuracy of our detection and inference is tested through a variety of experiments, and all of them justify the robustness of our algorithm to outliers such as appearance variations and occlusions.
C1 [Lian, Yongjian; Shen, Xukun; Hu, Yong] Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing 100191, Peoples R China.
   [Lian, Yongjian] North Univ China, Modern Educ Technol & Informat Ctr, Taiyuan 030051, Shanxi, Peoples R China.
C3 Beihang University; North University of China
RP Hu, Y (corresponding author), Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing 100191, Peoples R China.
EM lianyj@buaa.edu.cn; xkshen@buaa.edu.cn; huyong@buaa.edu.cn
OI Shen, Xukun/0000-0001-8509-9393
FU National High-Tech Research and Development Program of China
   [2013AA013803-1]; National Natural Science Foundation of China
   [61202235]
FX This work was partially supported by the National High-Tech Research and
   Development Program of China (Grant No. 2013AA013803-1) and the National
   Natural Science Foundation of China (Grant No. 61202235).
CR ADAMS R, 1994, IEEE T PATTERN ANAL, V16, P641, DOI 10.1109/34.295913
   [Anonymous], VIS COMPUT
   [Anonymous], 2006, COMPUTER VISION PATT
   [Anonymous], 2015, NATURE, DOI [DOI 10.1038/NATURE14539, 10.1038/nature14539]
   [Anonymous], 2017, COMMUN ACM, DOI DOI 10.1145/3065386
   [Anonymous], ACM T GR
   [Anonymous], PROC CVPR IEEE
   [Anonymous], ACM T GR
   [Anonymous], GERM C PATT REC
   [Anonymous], 2014, INT J COMPUT SCI INF
   [Anonymous], ACM T GRAPHICS
   [Anonymous], DETECTING SYMMETRY S
   [Anonymous], IEEE INT C IM PROC
   [Anonymous], ACM T GR
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Blohm Jan., 2004, International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences, V35, P867
   Cai YL, 2013, IEEE T IMAGE PROCESS, V22, P2343, DOI 10.1109/TIP.2013.2251649
   Changchang Wu, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3113, DOI 10.1109/CVPR.2011.5995551
   Cobb BR, 2007, STUD FUZZ SOFT COMP, V213, P81
   Cobb BR, 2006, INT J APPROX REASON, V41, P257, DOI 10.1016/j.ijar.2005.06.002
   Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236
   Fan LB, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2661229.2661265
   Furukawa Y, 2009, PROC CVPR IEEE, P1422, DOI 10.1109/CVPRW.2009.5206867
   Geng YB, 2015, J VIS COMMUN IMAGE R, V29, P89, DOI 10.1016/j.jvcir.2015.02.001
   Goutte C, 2005, LECT NOTES COMPUT SC, V3408, P345
   He KM, 2012, LECT NOTES COMPUT SC, V7573, P16, DOI 10.1007/978-3-642-33709-3_2
   Heider P, 2012, VISUAL COMPUT, V28, P919, DOI 10.1007/s00371-012-0725-9
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Hou F, 2016, VISUAL COMPUT, V32, P151, DOI 10.1007/s00371-015-1061-7
   Huang RJ, 2015, VISUAL COMPUT, V31, P1683, DOI 10.1007/s00371-014-1049-8
   Jensen F.V., 1996, INTRO BAYESIAN NETWO, V210
   Kazmi Ismail Khalid, 2013, 2013 10th International Conference on Computer Graphics, Imaging and Visualization (CGIV), P1, DOI 10.1109/CGIV.2013.11
   Koller D., 2009, Probabilistic graphical models: principles and techniques
   Korah T, 2007, IEEE T IMAGE PROCESS, V16, P2262, DOI 10.1109/TIP.2007.903263
   Lee S. C., 2004, IEEE COMPUTER SOC C, V2
   Lian YJ, 2015, IEEE IMAGE PROC, P1920, DOI 10.1109/ICIP.2015.7351135
   Lian YJ, 2015, PROC SPIE, V9524, DOI 10.1117/12.2189644
   Liang X, 2016, J COMPUT SCI TECH-CH, V31, P525, DOI 10.1007/s11390-016-1645-3
   Liang X, 2012, LECT NOTES COMPUT SC, V7576, P482, DOI 10.1007/978-3-642-33715-4_35
   Liu J, 2013, IEEE T PATTERN ANAL, V35, P208, DOI 10.1109/TPAMI.2012.39
   Lu J, 2007, VISUAL COMPUT, V23, P445, DOI 10.1007/s00371-007-0116-9
   Martinovic A, 2012, LECT NOTES COMPUT SC, V7578, P416, DOI 10.1007/978-3-642-33786-4_31
   Mu TJ, 2014, VISUAL COMPUT, V30, P833, DOI 10.1007/s00371-014-0961-2
   Müller P, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1276377.1276484, 10.1145/1239451.1239536]
   Musialski P., 2009, Proc. VMV Workshop, P3
   Nan LL, 2015, COMPUT GRAPH FORUM, V34, P217, DOI 10.1111/cgf.12554
   Ortin Diego., 2005, International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences, V36, P7
   Park M, 2009, IEEE T PATTERN ANAL, V31, P1804, DOI 10.1109/TPAMI.2009.73
   Pearl J., 2014, PROBABILISTIC REASON
   Romero V, 2006, INT J APPROX REASON, V42, P54, DOI 10.1016/j.ijar.2005.10.004
   Rumí R, 2006, TEST-SPAIN, V15, P397, DOI 10.1007/BF02607059
   Serra J., 1994, MATH MORPHOLOGY ITS
   Shao H., 2003, ZUBUD ZURICH BUILD I, P6
   Spath H., 1985, The cluster dissection and analysis theory fortran programs examples
   Szegedy Christian, 2015, IEEE C COMP VIS PATT, DOI [10.1109/cvpr.2015.7298594, DOI 10.1109/CVPR.2015.7298594]
   Teboul O., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2273, DOI 10.1109/CVPR.2011.5995319
   Teboul O, 2013, IEEE T PATTERN ANAL, V35, P1744, DOI 10.1109/TPAMI.2012.252
   Theodoridis S, 2009, PATTERN RECOGNITION, 4RTH EDITION, P1
   Torralba A, 2004, PROC CVPR IEEE, P762
   Wang Xiaoguang., 2002, INTERNA- TIONAL ARCHIVES OF PHOTOGRAMMETRY REMOTE SENSING AND SPATIAL IN- FORMATION SCIENCES, V34, P381
   Wu CC, 2010, LECT NOTES COMPUT SC, V6312, P142
   Yang C, 2012, PROC CVPR IEEE, P1720, DOI 10.1109/CVPR.2012.6247867
   Yeh YT, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2421636.2421639
   Zhao P, 2012, PROC CVPR IEEE, P526, DOI 10.1109/CVPR.2012.6247717
   Zhao P, 2011, PROC CVPR IEEE, P1009, DOI 10.1109/CVPR.2011.5995482
   Zitnick CL, 2014, LECT NOTES COMPUT SC, V8693, P391, DOI 10.1007/978-3-319-10602-1_26
NR 67
TC 2
Z9 2
U1 1
U2 17
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2018
VL 34
IS 4
BP 491
EP 506
DI 10.1007/s00371-017-1355-z
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FY6BJ
UT WOS:000426924400004
DA 2024-07-18
ER

PT J
AU Zhang, HJ
   Qu, DZ
   Liu, QL
   Shang, Q
   Hou, YF
   Shen, HW
AF Zhang, Huijie
   Qu, Dezhan
   Liu, Quanle
   Shang, Qi
   Hou, Yafang
   Shen, Han-Wei
TI Uncertainty visualization for variable associations analysis
SO VISUAL COMPUTER
LA English
DT Article
DE Uncertainty visualization; Gaussian mixture model; Uncertainty
   isosurface; Variable associations analysis
ID SCALAR FIELDS; NONPARAMETRIC MODELS; INTERPOLATION; VARIABILITY;
   ANIMATION
AB Uncertainty is inevitable in scientific simulations. As the increase in computing power, ensemble data have been generated for multiple variables. Uncertainty has become a great challenge to the analysis of variable associations for multivariate ensemble data, as the variable associations are very complex and diverse among different ensemble members. In this paper, we propose a novel visualization method to present the uncertain associations between a reference variable and the associated variable for multivariate ensemble data. Considering the huge scale of original ensemble data, Gaussian mixture model (GMM) is exploited to quantify the uncertainty and represent the original data compactly. To reveal the spatial uncertainty of the reference variable, a GMM-based method for extracting uncertainty isosurface is proposed and shows the accuracy advantage over Gaussian-based method. Meanwhile, a data reduction method is proposed to enhance the performance of extracting uncertainty isosurface. By mapping the values of the associated variable onto the uncertainty isosurface of the reference variable, a syncretic rendering method is proposed to show the variable associations intuitively. Besides, the screen space accumulating strategy is introduced to present the uncertainties of the associations. Furthermore, we provide a switchable view for users to obtain the credibility of variable associations. The credible associations can assist users to make reliable decisions. For the regions with not credible associations, the detailed information of the associations in every ensemble member can be explored through animation for further analysis. The effectiveness of our method is demonstrated by synthetic, climate and combustion data sets.
C1 [Zhang, Huijie; Qu, Dezhan; Liu, Quanle; Shang, Qi; Hou, Yafang] Northeast Normal Univ, Sch Comp Sci & Informat Technol, Changchun, Jilin, Peoples R China.
   [Shen, Han-Wei] Ohio State Univ, Dept Comp Sci & Engn, GRAV Grp, Columbus, OH 43210 USA.
C3 Northeast Normal University - China; University System of Ohio; Ohio
   State University
RP Zhang, HJ (corresponding author), Northeast Normal Univ, Sch Comp Sci & Informat Technol, Changchun, Jilin, Peoples R China.
EM zhanghj167@nenu.edu.cn; qudz862@nenu.edu.cn; liuql402@nenu.edu.cn;
   shangq471@nenu.edu.cn; houyf397@nenu.edu.cn; hwshen@cse.ohio-state.edu
RI zhang, hui/GXH-6098-2022; Shen, Han-wei/A-4710-2012; Zhang,
   Hw/HPD-4999-2023; Zhang, Hui/HHN-8494-2022; ZHANG, hui
   jie/HTN-1690-2023; Zhang, Cheng/JAD-2236-2023; Liu, Quanle/AAL-9648-2020
OI Liu, Quanle/0000-0001-7565-8024
FU National Natural Science Foundation of China [41671379, 41101434];
   Natural Science Foundation of Jilin Province [20140101179JC]; Research
   Fund for the Doctoral Program of Higher Education of China
   [20130043110016]
FX This work was supported by the National Natural Science Foundation of
   China under Grant 41671379, in part by the Natural Science Foundation of
   Jilin Province under Grant 20140101179JC, in part by the National
   Natural Science Foundation of China for Young Scholars under Grant
   41101434 and in part by the Research Fund for the Doctoral Program of
   Higher Education of China under Grant 20130043110016.
CR [Anonymous], 2014, Overview and State-of-the-Art of Uncertainty Visualization
   Athawale T, 2016, IEEE T VIS COMPUT GR, V22, P777, DOI 10.1109/TVCG.2015.2467958
   Athawale T, 2013, IEEE T VIS COMPUT GR, V19, P2723, DOI 10.1109/TVCG.2013.208
   Biswas A, 2013, IEEE T VIS COMPUT GR, V19, P2683, DOI 10.1109/TVCG.2013.133
   Brodlie K, 2012, Expanding the Frontiers of Visual Analytics and Visualization, P81, DOI [DOI 10.1007/978-1-4471-2804-5_6, 10.1007/978-1-4471-2804-5_6]
   Dinesha V, 2012, VISUAL COMPUT, V28, P265, DOI 10.1007/s00371-011-0614-7
   Ehlschlaeger CR, 1997, COMPUT GEOSCI, V23, P387, DOI 10.1016/S0098-3004(97)00005-8
   Gosink LJ, 2007, IEEE T VIS COMPUT GR, V13, P1400, DOI 10.1109/TVCG.2007.70519
   Guo HQ, 2012, IEEE T VIS COMPUT GR, V18, P1397, DOI 10.1109/TVCG.2012.80
   Hao LH, 2016, IEEE T VIS COMPUT GR, V22, P787, DOI 10.1109/TVCG.2015.2468093
   Hengl T., 2003, P 7 INT C GEO COMPUT, P8
   Huber D.E., 2005, IEEE Visualization, P67, DOI [10.1109/VIS.2005.125, DOI 10.1109/VIS.2005.125]
   Jänicke H, 2009, IEEE T VIS COMPUT GR, V15, P1375, DOI 10.1109/TVCG.2009.197
   Liu XT, 2016, IEEE T VIS COMPUT GR, V22, P955, DOI 10.1109/TVCG.2015.2467431
   Lundström C, 2007, IEEE T VIS COMPUT GR, V13, P1648, DOI 10.1109/TVCG.2007.70518
   Mihai M, 2014, COMPUT GRAPH-UK, V41, P13, DOI 10.1016/j.cag.2014.01.007
   Pang AT, 1997, VISUAL COMPUT, V13, P370, DOI 10.1007/s003710050111
   Pfaffelmoser T, 2012, COMPUT GRAPH FORUM, V31, P1025, DOI 10.1111/j.1467-8659.2012.03095.x
   Pfaffelmoser T, 2011, COMPUT GRAPH FORUM, V30, P951, DOI 10.1111/j.1467-8659.2011.01944.x
   Pöthkow K, 2013, COMPUT GRAPH FORUM, V32, P131, DOI 10.1111/cgf.12100
   Pöthkow K, 2011, COMPUT GRAPH FORUM, V30, P931, DOI 10.1111/j.1467-8659.2011.01942.x
   Pöthkow K, 2011, IEEE T VIS COMPUT GR, V17, P1393, DOI 10.1109/TVCG.2010.247
   Potter K, 2010, COMPUT GRAPH FORUM, V29, P823, DOI 10.1111/j.1467-8659.2009.01677.x
   Potter K., 2012, UNCERTAINTY QUANTIFI
   Potter K, 2009, INT CONF DAT MIN WOR, P233, DOI 10.1109/ICDMW.2009.55
   Sanyal J, 2010, IEEE T VIS COMPUT GR, V16, P1421, DOI 10.1109/TVCG.2010.181
   Sanyal J, 2009, IEEE T VIS COMPUT GR, V15, P1209, DOI 10.1109/TVCG.2009.114
   Sauber N, 2006, IEEE T VIS COMPUT GR, V12, P917, DOI 10.1109/TVCG.2006.165
   Schlegel S, 2012, IEEE T VIS COMPUT GR, V18, P2305, DOI 10.1109/TVCG.2012.249
   Shusen Liu, 2012, 2012 IEEE Symposium on Large Data Analysis and Visualization (LDAV 2012), P73, DOI 10.1109/LDAV.2012.6378978
   Thompson D., 2011, Proceedings of the IEEE Symposium on Large Data Analysis and Visualization (LDAV 2011), P23, DOI 10.1109/LDAV.2011.6092313
   Wong P. C., 1996, SCI VIS OVERV METHOD, P3
   Zhang HJ, 2016, IEEE ACCESS, V4, DOI 10.1109/ACCESS.2016.2601339
NR 33
TC 6
Z9 6
U1 1
U2 23
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2018
VL 34
IS 4
BP 531
EP 549
DI 10.1007/s00371-017-1359-8
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FY6BJ
UT WOS:000426924400007
DA 2024-07-18
ER

PT J
AU Huang, J
   Fratarcangeli, M
   Ding, Y
   Pelachaud, C
AF Huang, J.
   Fratarcangeli, M.
   Ding, Y.
   Pelachaud, C.
TI Inverse kinematics using dynamic joint parameters: inverse kinematics
   animation synthesis learnt from sub-divided motion micro-segments
SO VISUAL COMPUTER
LA English
DT Article
DE Animation; Inverse kinematics; Octree; Parallel processing
AB In this paper, we describe a novel parallelizable method for the fast computation of inverse kinematics (IK) animation. The existing IK techniques are based on sequential algorithms as they compute a skeletal pose relying only on the previous one; However, for a given trajectory, both the previous posture and the following posture are desired to compute a natural posture of the current frame. Moreover, they do not take into account that the skeletal joint limits vary with temporal spatial skeleton configurations. In this paper, we describe a novel extension of IK model using dynamic joint parameters to overcome the major drawbacks of traditional approaches of IK. Our constraint model relies on motion capture data of human motion. The dynamic joint motion parameters are learned automatically, embedding dynamic joint limit values and feasible poses. The joint information is stored in an octree which clusters and provides fast access to the data. Where the trajectory of the end-effector is provided in the input or the target positions data are sent by data stream, all the computed poses are assembled into a smooth animation sequence using parallel filtering and retargeting passes. The main benefits of our approach are dual: first, the joint constraints are dynamic (as in a real human body), and automatically learnt from real data; second, the processing time is reduced significantly due to the parallel algorithm. After describing our model, we demonstrate its efficiency and robustness, and show that it can generate high visual quality motion and natural looking postures with a significant performance improvement.
C1 [Huang, J.; Ding, Y.; Pelachaud, C.] CNRS, LTCI, Paris, France.
   [Huang, J.; Ding, Y.; Pelachaud, C.] Telecom ParisTech, Paris, France.
   [Fratarcangeli, M.] Chalmers Univ Technol, Gothenburg, Sweden.
C3 Centre National de la Recherche Scientifique (CNRS); IMT - Institut
   Mines-Telecom; Institut Polytechnique de Paris; Telecom Paris; Chalmers
   University of Technology
RP Huang, J (corresponding author), CNRS, LTCI, Paris, France.; Huang, J (corresponding author), Telecom ParisTech, Paris, France.
EM jing.huang@telecom-paristech.fr; marcof@chalmers.se;
   yu.ding@telecom-paristech.fr; catherine.pelachaud@telecom-paristech.fr
RI Fratarcangeli, Marco/H-3967-2011
OI Fratarcangeli, Marco/0000-0002-1156-3760
CR [Anonymous], 1984, P 23 IEEE C DEC CONT
   [Anonymous], 1985, P 1985 IEEE INT C RO
   Aristidou A, 2011, GRAPH MODELS, V73, P243, DOI 10.1016/j.gmod.2011.05.003
   Baerlocher P, 2004, VISUAL COMPUT, V20, P402, DOI 10.1007/s00371-004-0244-4
   Baerlocher P, 1998, 1998 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS - PROCEEDINGS, VOLS 1-3, P323, DOI 10.1109/IROS.1998.724639
   Baerlocher P, 2000, P IFIP TC5 WG5 10 DE, P180
   Blow J, 2002, GAME DEV
   Buss S.R., 2004, SELECTIVELY DAMPED L, P37
   Chiaverini S, 1997, IEEE T ROBOTIC AUTOM, V13, P398, DOI 10.1109/70.585902
   Feng AndrewW., 2012, I3D, P95, DOI [DOI 10.1145/2159616.2159632, 10.1145/2159616.2159632]
   Fourati N, 2014, LREC 2014 - NINTH INTERNATIONAL CONFERENCE ON LANGUAGE RESOURCES AND EVALUATION, P3486
   Grochow K, 2004, ACM T GRAPHIC, V23, P522, DOI 10.1145/1015706.1015755
   Harish P, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2887740
   Huang J, 2011, EUROGRAPHICS 2011 SH, P29
   Jing Huang, 2012, Motion in Games. 5th International Conference (MIG 2012). Proceedings, P278, DOI 10.1007/978-3-642-34710-8_26
   Kallmann M, 2008, COMPUT ANIMAT VIRT W, V19, P79, DOI 10.1002/cav.176
   KLEIN CA, 1983, IEEE T SYST MAN CYB, V13, P245, DOI 10.1109/TSMC.1983.6313123
   Kopp S., 2006, P INT C AFF COMP INT, V4133, P21, DOI 10.1.1.100.378
   Le Callennec B, 2006, GRAPH MODELS, V68, P175, DOI 10.1016/j.gmod.2005.03.001
   LIEGEOIS A, 1977, IEEE T SYST MAN CYB, V7, P868
   NAKAMURA Y, 1986, J DYN SYST-T ASME, V108, P163, DOI 10.1115/1.3143764
   Paris S, 2006, LECT NOTES COMPUT SC, V3954, P568
   SHAO W., 2003, P 2003 ACM S INTERAC, P11
   Tang W, 1998, LECT NOTES ARTIF INT, V1537, P159
   Tolani D, 2000, GRAPH MODELS, V62, P353, DOI 10.1006/gmod.2000.0528
   Tomasi C, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P839, DOI 10.1109/ICCV.1998.710815
   Unzueta L, 2008, GRAPH MODELS, V70, P87, DOI 10.1016/j.gmod.2008.03.002
   WAMPLER CW, 1986, IEEE T SYST MAN CYB, V16, P93, DOI 10.1109/TSMC.1986.289285
   WANG LCT, 1991, IEEE T ROBOTIC AUTOM, V7, P489, DOI 10.1109/70.86079
   Webber B. L., 1993, SIMULATING HUMANS CO, P68
   Wendland H., 1995, Advances in Computational Mathematics, V4, P389, DOI 10.1007/BF02123482
   Wilhelms J., 2001, Journal of Graphics Tools, V6, P27, DOI 10.1080/10867651.2001.10487539
   Yamane K, 2003, IEEE T VIS COMPUT GR, V9, P352, DOI 10.1109/TVCG.2003.1207443
   Yuan J, 2001, ROBOTICA, V19, P79, DOI 10.1017/S0263574700002769
   ZHAO JM, 1994, ACM T GRAPHIC, V13, P313, DOI 10.1145/195826.195827
NR 35
TC 5
Z9 5
U1 0
U2 5
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2017
VL 33
IS 12
BP 1541
EP 1553
DI 10.1007/s00371-016-1297-x
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FK4JE
UT WOS:000413458600005
OA Green Published
DA 2024-07-18
ER

PT J
AU Frerichs, D
   Vidler, A
   Gatzidis, C
AF Frerichs, Dhana
   Vidler, Andrew
   Gatzidis, Christos
TI Biologically inspired simulation of livor mortis
SO VISUAL COMPUTER
LA English
DT Article
DE Appearance modeling; Decomposition; Livor mortis
ID LIGHT INTERACTION; SKIN; MODEL; WRINKLES; COLOR
AB We present a biologically motivated livor mortis simulation that is capable of modelling the colouration changes in skin caused by blood pooling after death. Our approach consists of a simulation of post mortem blood dynamics and a layered skin shader that is controlled by the haemoglobin and oxygen levels in blood. The object is represented by a layered data structure made of a triangle mesh for the skin and a tetrahedral mesh on which the blood dynamics are simulated. This allows us to simulate the skin discolouration caused by livor mortis, including early patchy appearance, fixation of hypostasis and pressure induced blanching. We demonstrate our approach on two different models and scenarios and compare the results to real world livor mortis photographic examples.
C1 [Frerichs, Dhana; Gatzidis, Christos] Bournemouth Univ, Poole, Dorset, England.
   [Frerichs, Dhana; Vidler, Andrew] Ninja Theory Ltd, Cambridge, England.
C3 Bournemouth University
RP Frerichs, D (corresponding author), Bournemouth Univ, Poole, Dorset, England.; Frerichs, D (corresponding author), Ninja Theory Ltd, Cambridge, England.
EM dfrerichs@bournemouth.ac.uk
FU EPSRC, via the doctorate training Centre for Digital Entertainment;
   Ninja Theory Ltd.
FX We would like to thank the reviewers for constructive criticisms and
   suggestions that improved the manuscript. We would also like to thank
   our colleagues from Ninja Theory Ltd. for all their help and support
   with this project. Special thanks goes to Robin Hansson from Ninja
   Theory Ltd. for providing us with the models used to generate early test
   results and some of the examples in this article. The research presented
   in this article is funded by EPSRC, via the doctorate training Centre
   for Digital Entertainment, in conjunction with Ninja Theory Ltd.
CR Beardall Matthew, 2007, P 3 EUROGRAPHICS C N, P7
   Bohnert M, 2000, INT J LEGAL MED, V113, P343, DOI 10.1007/s004149900107
   Boissieux L, 2000, SPRING COMP SCI, P15
   Chang YX, 2003, VISUAL COMPUT, V19, P50, DOI 10.1007/s00371-002-0172-0
   Chen TF, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2701416
   Chen YY, 2005, ACM T GRAPHIC, V24, P1127, DOI 10.1145/1073204.1073321
   d'Eon Eugene., 2007, GPU Gems 3 - Advanced Techniques for Realistic Real-Time Skin Rendering, P293
   Desbenoit B, 2005, VISUAL COMPUT, V21, P717, DOI 10.1007/s00371-005-0317-z
   Dettmeyer R., 2013, Forensic medicine: Fundamentals and perspectives
   Donner C, 2005, ACM T GRAPHIC, V24, P1032, DOI 10.1145/1073204.1073308
   Donner C., 2008, ACM SIGGRAPH ASIA 20
   Dorsey J., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P387, DOI 10.1145/237170.237278
   Dorsey J., 2010, Digital modeling of material appearance
   Frerichs D, 2015, COMPUT GRAPH-UK, V52, P18, DOI 10.1016/j.cag.2015.06.004
   Fujisawa M, 2007, GRAPHITE 2007: 5TH INTERNATIONAL CONFERENCE ON COMPUTER GRAPHICS AND INTERACTIVE TECHNIQUES IN AUSTRALASIA AND SOUTHERN ASIA, PROCEEDINGS, P249
   Ghosh A., 2008, ACM SIGGRAPH ASIA 20
   Gunther T., 2012, GPU ACCELERATED INTE
   Igarashi T, 2007, FOUND TRENDS COMPUT, V3, P1, DOI 10.1561/0600000013
   Iglesias-Guitian JA, 2015, COMPUT GRAPH FORUM, V34, P45, DOI 10.1111/cgf.12540
   Jeong S, 2013, COMPUT GRAPH FORUM, V32, P204, DOI 10.1111/cgf.12009
   Jeong S, 2011, VISUAL COMPUT, V27, P417, DOI 10.1007/s00371-011-0575-x
   Jimenez J., 2010, ACM SIGGRAPH ASIA 20
   JIMENEZ J., 2015, COMPUTER GRAPHICS FO
   Jimenez J, 2009, ACM T APPL PERCEPT, V6, DOI 10.1145/1609967.1609970
   Jones MD, 2010, IEEE T VIS COMPUT GR, V16, P81, DOI 10.1109/TVCG.2009.39
   Kelley A. D., 1988, Computer Graphics, V22, P263, DOI 10.1145/378456.378519
   KELMAN GR, 1966, J APPL PHYSIOL, V21, P1375, DOI 10.1152/jappl.1966.21.4.1375
   Kider JT, 2011, COMPUT GRAPH FORUM, V30, P257, DOI 10.1111/j.1467-8659.2011.01857.x
   Kienle A, 1996, APPL OPTICS, V35, P1151, DOI 10.1364/AO.35.001151
   Krishnaswamy A, 2004, COMPUT GRAPH FORUM, V23, P331, DOI 10.1111/j.1467-8659.2004.00764.x
   Lii SY, 2014, VISUAL COMPUT, V30, P531, DOI 10.1007/s00371-013-0878-1
   Liu YQ, 2012, COMPUT ANIMAT VIRT W, V23, P395, DOI 10.1002/cav.1459
   Losasso F, 2006, IEEE T VIS COMPUT GR, V12, P343, DOI 10.1109/TVCG.2006.51
   Machado C., 2007, BRAIN DEATH REAPPRAI
   Melek Z., 2004, P C VIS 04 VIS 04 WA, P598, DOI 10.1109/VISUAL.2004.71
   Mérillou S, 2008, COMPUT GRAPH-UK, V32, P159, DOI 10.1016/j.cag.2008.01.003
   Merillou S., 2001, P GRAPHICS INTERFACE, P167
   Muguercia L., 2014, COMPUT GRAPH, V45
   Musgrave F. K., 1989, Computer Graphics, V23, P41, DOI 10.1145/74334.74337
   Neyret F, 2002, VISUAL COMPUT, V18, P135, DOI 10.1007/s003710100118
   Prahlow JA, 2011, Atlas of forensic pathology: for police, forensic scientists, attorneys, and death investigators
   Roering JJ, 1999, WATER RESOUR RES, V35, P853, DOI 10.1029/1998WR900090
   Scanlon ValeriaC., 2015, Essentials of Anatomy and Physiology, V7th
   Si H, 2015, ACM T MATH SOFTWARE, V41, DOI 10.1145/2629697
   Tsumura N, 2003, ACM T GRAPHIC, V22, P770, DOI 10.1145/882262.882344
   Tychonievich LA, 2010, VISUAL COMPUT, V26, P1485, DOI 10.1007/s00371-010-0506-2
   Wu Y, 1999, VISUAL COMPUT, V15, P183, DOI 10.1007/s003710050171
   Yim D, 2012, COMPUT GRAPH FORUM, V31, P845, DOI 10.1111/j.1467-8659.2012.03065.x
   Zhao Y, 2003, IEEE VISUALIZATION 2003, PROCEEDINGS, P271
NR 49
TC 0
Z9 0
U1 0
U2 4
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2017
VL 33
IS 11
BP 1453
EP 1466
DI 10.1007/s00371-016-1291-3
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FJ0TP
UT WOS:000412423100008
OA hybrid
DA 2024-07-18
ER

PT J
AU Packer, JF
   Hasan, M
   Samavati, FF
AF Packer, Jeffrey F.
   Hasan, Mahmudul
   Samavati, Faramarz F.
TI Illustrative multilevel focus plus context visualization along snaking
   paths
SO VISUAL COMPUTER
LA English
DT Article
DE Illustrative visualization; Focus plus context visualization; Multilevel
   visualization hierarchy; Volume deformation; Constrained volume
   sculpting; Pathfinding
AB Artistic anatomical illustrations often focus on cross sections of long, layered, cylindrical structures. Such illustrations emphasize structures along transitions between focal points over a snaking path that optimally traverses the span of a limited space. The transitions between focal points form a multilevel visualization hierarchy. In this article, we present an approach to automatically create focus+context visualizations of the described form. First, a method to automatically create a snaking path through space by applying a pathfinding algorithm is presented. A 3D curve is created based on a 2D snaking path. Then we describe a process to deform cylindrical structures in segmented volumetric models along the 3D curve and provide preliminary geometric models as templates for artists to build upon. Our constrained volume sculpting method enables the removal of occluding material to reveal cylindrical structures of interest intended for such deformation. Finally, we present a set of created visualizations that demonstrates the flexibility of our approach and effectively mimics the form of visualization observed in motivating illustrations.
C1 [Packer, Jeffrey F.; Hasan, Mahmudul; Samavati, Faramarz F.] Univ Calgary, Dept Comp Sci, Calgary, AB, Canada.
C3 University of Calgary
RP Hasan, M (corresponding author), Univ Calgary, Dept Comp Sci, Calgary, AB, Canada.
EM mhasan@ucalgary.ca
CR Aggarwal A, 2000, SIAM J COMPUT, V29, P697, DOI 10.1137/S0097539796312721
   [Anonymous], 1968, P 1968 ACM NAT C
   [Anonymous], 1995, TECH REP
   [Anonymous], THESIS
   [Anonymous], 2006, P 8 EUR IEEE VGTC S, DOI [DOI 10.2312/VISSYM/EUROVIS06/347-354, 10.2312/ VisSym/EuroVis06/347-354]
   Bruckner S., Proceedings of the Seventh Joint Eurographics / IEEE VGTC Conference on Visualization, ser. EUROVIS'05. Aire-la-Ville, Switzerland, Switzerland: Eurographics Association, P69, DOI DOI 10.2312/VISSYM/EUROVIS05/069-076
   Cannon JW, 2007, GEOM TOPOL, V11, P1315, DOI 10.2140/gt.2007.11.1315
   Carpendale M. S. T., 2001, 01UIST. Proceedings of the 14th Annual ACM Symposium on User Interface Software and Technology, P61, DOI 10.1145/502348.502358
   Chen HLJ, 2008, VISUAL COMPUT, V24, P689, DOI 10.1007/s00371-008-0249-5
   Chen HX, 2003, J VISUAL COMP ANIMAT, V14, P61, DOI 10.1002/vis.305
   Chuang JH, 2000, IEEE T PATTERN ANAL, V22, P1241
   Cohen M, 2004, THEORY AND PRACTICE OF COMPUTER GRAPHICS 2004, PROCEEDINGS, P32, DOI 10.1109/TPCG.2004.1314450
   Cornea ND, 2005, IEEE VISUALIZATION 2005, PROCEEDINGS, P95
   Cornea ND, 2005, VISUAL COMPUT, V21, P945, DOI 10.1007/s00371-005-0308-0
   Correa CD, 2006, IEEE T VIS COMPUT GR, V12, P1069, DOI 10.1109/TVCG.2006.144
   Correa CarlosD., 2006, Fifth Eurographics / IEEE VGTC Workshop on Volume Graphics, P9, DOI DOI 10.2312/VG/VG06/009-016
   Csébfalvi B, 2001, COMPUT GRAPH FORUM, V20, pC452, DOI 10.1111/1467-8659.00538
   Dijkstra EW., 1959, NUMER MATH, V1, P269, DOI [10.1007/BF01386390, DOI 10.1007/BF01386390]
   Ebert D, 2000, IEEE VISUAL, P195, DOI 10.1109/VISUAL.2000.885694
   Ferley E, 2000, VISUAL COMPUT, V16, P469, DOI 10.1007/PL00007216
   Gagvani N, 1998, IEEE SYMPOSIUM ON VOLUME VISUALIZATION, P47, DOI 10.1109/SVV.1998.729584
   GALYEAN TA, 1991, COMP GRAPH, V25, P267, DOI 10.1145/127719.122747
   Grigorishin T, 1998, PATTERN ANAL APPL, V1, P163, DOI 10.1007/BF01259366
   HART PE, 1968, IEEE T SYST SCI CYB, VSSC4, P100, DOI 10.1109/TSSC.1968.300136
   Hasan M, 2016, VISUAL COMPUT, V32, P323, DOI 10.1007/s00371-015-1180-1
   Hasan M, 2015, GRAPH MODELS, V78, P36, DOI 10.1016/j.gmod.2015.01.001
   Hasan M, 2014, 2014 INTERNATIONAL CONFERENCE ON CYBERWORLDS (CW), P145, DOI 10.1109/CW.2014.28
   Hauser H, 2001, IEEE T VIS COMPUT GR, V7, P242, DOI 10.1109/2945.942692
   Hsu WH, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024165
   Imanishi K, 2010, INT J COMPUT ASS RAD, V5, P461, DOI 10.1007/s11548-010-0413-z
   LEE TC, 1994, CVGIP-GRAPH MODEL IM, V56, P462, DOI 10.1006/cgip.1994.1042
   McGuffin MJ, 2003, IEEE VISUALIZATION 2003, PROCEEDINGS, P401, DOI 10.1109/VISUAL.2003.1250400
   Pietriga E, 2008, CHI 2008: 26TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS VOLS 1 AND 2, CONFERENCE PROCEEDINGS, P1343
   Pindat C, 2012, UIST'12: PROCEEDINGS OF THE 25TH ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P261
   RENKA RJ, 1988, ACM T MATH SOFTWARE, V14, P139, DOI 10.1145/45054.45055
   Rezk-Salama C., 2001, 2001 ACM SIGGRAPHEUR, P17, DOI DOI 10.1145/383507.383517
   Roberts M., 2010, HPG 10, P123
   Ropinski Timo., 2009, Proceedings of EG UK Theory and Practice of Computer Graphics, P17, DOI 10. 2312/LocalChapterEvents/TPCG/TPCG09/017-024 20
   Samavati FF, 2007, SER MACH PERCEPT ART, V67, P65
   Sederberg T. W., 1986, Computer Graphics, V20, P151, DOI 10.1145/15886.15903
   Tiede U, 1998, VISUALIZATION '98, PROCEEDINGS, P255, DOI 10.1109/VISUAL.1998.745311
   van Emmerik M. J. G. M., 1990, Computer Graphics Forum, V9, P355, DOI 10.1111/j.1467-8659.1990.tb00427.x
   Wang LJ, 2005, IEEE VISUALIZATION 2005, PROCEEDINGS, P367
   Wang S. W., 1995, Proceedings 1995 Symposium on Interactive 3D Graphics, P151, DOI 10.1145/199404.199430
   Westermann R, 2001, COMPUT GRAPH FORUM, V20, pC443, DOI 10.1111/1467-8659.00537
   Winter AS, 2002, COMPUT GRAPH FORUM, V21, P441, DOI 10.1111/1467-8659.t01-1-00604
NR 46
TC 3
Z9 3
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2017
VL 33
IS 10
BP 1291
EP 1306
DI 10.1007/s00371-016-1217-0
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FF8VN
UT WOS:000409296000007
DA 2024-07-18
ER

PT J
AU Jiang, T
   Qian, K
   Liu, S
   Wang, J
   Yang, XS
   Zhang, JJ
AF Jiang, Tao
   Qian, Kun
   Liu, Shuang
   Wang, Jing
   Yang, Xiaosong
   Zhang, Jianjun
TI Consistent as-similar-as-possible non-isometric surface registration
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 34th International Conference on Computer Graphics (CGI)
CY JUN 27-30, 2017
CL Yokohama, JAPAN
SP Keio Univ, Fac Sci & Technol
DE Surface Registration; As-Similar-As-Possible; Consistent; Non-Isometric
ID OBJECT RECOGNITION; DEFORMATION
AB Non-isometric surface registration, aiming to align two surfaces with different sizes and details, has been widely used in computer animation industry. Various existing surface registration approaches have been proposed for accurate template fitting; nevertheless, two challenges remain. One is how to avoid the mesh distortion and fold over of surfaces during transformation. The other is how to reduce the amount of landmarks that have to be specified manually. To tackle these challenges simultaneously, we propose a consistent as-similar-as-possible (CASAP) surface registration approach. With a novel defined energy, it not only achieves the consistent discretization for the surfaces to produce accurate result, but also requires a small number of landmarks with little user effort only. Besides, CASAP is constrained as-similar-as-possible so that angles of triangle meshes are preserved and local scales are allowed to change. Extensive experimental results have demonstrated the effectiveness of CASAP in comparison with the state-of-the-art approaches.
C1 [Jiang, Tao] Bournemouth Univ, Fac Media & Commun, Bournemouth, Dorset, England.
   [Qian, Kun; Liu, Shuang; Yang, Xiaosong; Zhang, Jianjun] Bournemouth Univ, Natl Ctr Comp Animat, Bournemouth, Dorset, England.
   [Wang, Jing] Bournemouth Univ, Fac Sci & Technol, Bournemouth, Dorset, England.
C3 Bournemouth University; Bournemouth University; Bournemouth University
RP Yang, XS (corresponding author), Bournemouth Univ, Natl Ctr Comp Animat, Bournemouth, Dorset, England.
EM xyang@bournemouth.ac.uk
OI Yang, Xiaosong/0000-0003-3815-0584
FU Bournemouth University
FX Open access funding provided by Bournemouth University.
CR Allen B, 2003, ACM T GRAPHIC, V22, P587, DOI 10.1145/882262.882311
   Amberg B, 2007, IEEE I CONF COMP VIS, P1326
   [Anonymous], 2013, THESIS ETH ZURICH ZU
   Aubry M, 2011, IEEE I CONF COMP VIS, P1411, DOI 10.1109/ICCV.2011.6126396
   Belongie S, 2002, IEEE T PATTERN ANAL, V24, P509, DOI 10.1109/34.993558
   Botsch M, 2008, IEEE T VIS COMPUT GR, V14, P213, DOI 10.1109/TVCG.2007.1054
   Bronstein MM, 2010, PROC CVPR IEEE, P1704, DOI 10.1109/CVPR.2010.5539838
   Chao I, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778775
   Floater MS, 2005, MATH VIS, P157, DOI 10.1007/3-540-26808-1_9
   Gilles B, 2010, COMPUT GRAPH FORUM, V29, P2340, DOI 10.1111/j.1467-8659.2010.01718.x
   Jacobson A, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964973
   Jacobson A, 2010, COMPUT GRAPH FORUM, V29, P1565, DOI 10.1111/j.1467-8659.2010.01765.x
   Johnson AE, 1999, IEEE T PATTERN ANAL, V21, P433, DOI 10.1109/34.765655
   Levi Z, 2015, IEEE T VIS COMPUT GR, V21, P264, DOI 10.1109/TVCG.2014.2359463
   Li H, 2008, COMPUT GRAPH FORUM, V27, P1421, DOI 10.1111/j.1467-8659.2008.01282.x
   Lipman Y, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185604
   Meyer M., 2002, VISUALIZATION MATH, V6, P35, DOI DOI 10.1007/978-3-662-05105-4_2
   Moenning C., 2003, Technical Report
   Müller M, 2005, ACM T GRAPHIC, V24, P471, DOI 10.1145/1073204.1073216
   Panozzo D., 2010, P COMP GRAPH COMP VI, V1
   Papazov C, 2011, COMPUT GRAPH FORUM, V30, P1493, DOI 10.1111/j.1467-8659.2011.02023.x
   Qixing H., 2008, COMPUTER GRAPHICS FO
   Seidel H.-P, 2004, 2 EUROGRAPHICS S GEO, P175, DOI DOI 10.1145/1057432.1057456
   Sorkine O, 2007, S GEOM PROC, V4, P109, DOI [10.1145/1073204.1073323, DOI 10.1145/1073204.1073323]
   Sumner RW, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239531
   Sumner RW, 2004, ACM T GRAPHIC, V23, P399, DOI 10.1145/1015706.1015736
   Sun JA, 2009, COMPUT GRAPH FORUM, V28, P1383, DOI 10.1111/j.1467-8659.2009.01515.x
   Surazhsky V, 2005, ACM T GRAPHIC, V24, P553, DOI 10.1145/1073204.1073228
   van Kaick O, 2011, COMPUT GRAPH FORUM, V30, P1681, DOI 10.1111/j.1467-8659.2011.01884.x
   Yamazaki S, 2013, 2013 INTERNATIONAL CONFERENCE ON 3D VISION (3DV 2013), P191, DOI 10.1109/3DV.2013.33
   Yeh IC, 2011, IEEE T VIS COMPUT GR, V17, P1178, DOI 10.1109/TVCG.2010.124
   Yoshiyasu Y, 2014, COMPUT GRAPH FORUM, V33, P257, DOI 10.1111/cgf.12451
NR 32
TC 8
Z9 8
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2017
VL 33
IS 6-8
BP 891
EP 901
DI 10.1007/s00371-017-1390-9
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA EX1EY
UT WOS:000402964800020
OA hybrid
DA 2024-07-18
ER

PT J
AU Díaz, J
   Ropinski, T
   Navazo, I
   Gobbetti, E
   Vázquez, PP
AF Diaz, Jose
   Ropinski, Timo
   Navazo, Isabel
   Gobbetti, Enrico
   Vazquez, Pere-Pau
TI An experimental study on the effects of shading in 3D perception of
   volumetric models
SO VISUAL COMPUTER
LA English
DT Article
DE Volume illumination; Volume visualization; Desktop-based stereoscopy;
   Depth perception
ID VIRTUAL ENVIRONMENTS; VISUAL ANALYSIS; ILLUMINATION; DEPTH; SIMULATION;
   IMMERSION; USABILITY; DISPLAY
AB Throughout the years, many shading techniques have been developed to improve the conveying of information in volume visualization. Some of these methods, usually referred to as realistic, are supposed to provide better cues for the understanding of volume data sets. While shading approaches are heavily exploited in traditional monoscopic setups, no previous study has analyzed the effect of these techniques in virtual reality. To further explore the influence of shading on the understanding of volume data in such environments, we carried out a user study in a desktop-based stereoscopic setup. The goals of the study were to investigate the impact of well-known shading approaches and the influence of real illumination on depth perception. Participants had to perform three different perceptual tasks when exposed to static visual stimuli. 45 participants took part in the study, giving us 1152 trials for each task. Results show that advanced shading techniques improve depth perception in stereoscopic volume visualization. As well, external lighting does not affect depth perception when these shading methods are applied. As a result, we derive some guidelines that may help the researchers when selecting illumination models for stereoscopic rendering.
C1 [Diaz, Jose; Gobbetti, Enrico] CRS4, ViC Grp, Pula, Italy.
   [Ropinski, Timo] Univ Ulm, Visual Comp Res Grp, Ulm, Germany.
   [Navazo, Isabel; Vazquez, Pere-Pau] Univ Politecn Cataluna, VIRViG Grp, Barcelona, Spain.
C3 Ulm University; Universitat Politecnica de Catalunya
RP Díaz, J (corresponding author), CRS4, ViC Grp, Pula, Italy.
EM jdiaz@crs4.it; timo.ropinski@uni-ulm.de; isabel@cs.upc.edu;
   gobbetti@crs4.it; ppau@cs.upc.edu
RI Vázquez, Pere-Pau/HTP-9691-2023; Gobbetti, Enrico/O-2188-2015; Díaz,
   Jose/AAB-3095-2021
OI Vázquez, Pere-Pau/0000-0003-4638-4065; Gobbetti,
   Enrico/0000-0003-0831-2458; Díaz, Jose/0000-0002-2777-0430; Ropinski,
   Timo/0000-0002-7857-5512
FU EU FP7 Program under the DIVA project [290277]; Eurostar CAMILIS
   project; Spanish Government [TIN2013-47137-C2-1-P, TIN2014-52211-C2-1-R]
FX The authors thank J. L. Diaz-Barrero and M. Fairen for their valuable
   contributions and the volunteers who took part in the study. This work
   was partially supported by the EU FP7 Program under the DIVA project
   (290277), the Eurostar CAMILIS project, and the TIN2013-47137-C2-1-P and
   TIN2014-52211-C2-1-R projects of the Spanish Government.
CR [Anonymous], 2003, P ACM SIGCHI C HUM F, DOI [DOI 10.1145/642611.642703, 10.1145/642611.642703]
   Ayten H., 2010, 13 INT C HUM COMP, P66
   Bach C, 2010, INT J HUM-COMPUT INT, V26, P786, DOI 10.1080/10447318.2010.487195
   Baer R., 2009, 3D MED VISUALIZATION, P295
   Boucheny C, 2007, APGV 2007: SYMPOSIUM ON APPLIED PERCEPTION IN GRAPHICS AND VISUALIZATION, PROCEEDINGS, P83
   Bowman DA, 2002, PRESENCE-TELEOP VIRT, V11, P404, DOI 10.1162/105474602760204309
   Diaz J., 2015, COMPUTER GRAPHICS IN
   Díaz J, 2010, COMPUT GRAPH-UK, V34, P337, DOI 10.1016/j.cag.2010.03.005
   Germani M., 2009, INT DES ENG TECHN C, P1325
   Grosset AVP, 2013, IEEE PAC VIS SYMP, P81, DOI 10.1109/PacificVis.2013.6596131
   Hernell F, 2010, IEEE T VIS COMPUT GR, V16, P548, DOI 10.1109/TVCG.2009.45
   Ijsselsteijn W, 1998, DISPLAYS, V18, P207, DOI 10.1016/S0141-9382(98)00022-5
   Jönsson D, 2014, COMPUT GRAPH FORUM, V33, P27, DOI 10.1111/cgf.12252
   Kniss J, 2003, IEEE T VIS COMPUT GR, V9, P150, DOI 10.1109/TVCG.2003.1196003
   Laha B., 2012, IMM VIS REV WORKSH I
   Laha B, 2013, IEEE T VIS COMPUT GR, V19, P529, DOI 10.1109/TVCG.2013.43
   Laha B, 2012, IEEE T VIS COMPUT GR, V18, P597, DOI 10.1109/TVCG.2012.42
   Langer MS, 2000, PERCEPTION, V29, P649, DOI 10.1068/p3060
   Lee C, 2013, IEEE T VIS COMPUT GR, V19, P547, DOI 10.1109/TVCG.2013.41
   Leonard C.Wanger, 1992, IEEE COMPUT GRAPH, V12, P54
   LEVOY M, 1988, IEEE COMPUT GRAPH, V8, P29, DOI 10.1109/38.511
   Lindemann F, 2011, IEEE T VIS COMPUT GR, V17, P1922, DOI 10.1109/TVCG.2011.161
   Liu BX, 2004, VISION RES, V44, P2135, DOI 10.1016/j.visres.2004.03.024
   Livatino S., 2007, VIRTUAL ENV HUMAN CO, P1
   Max N., 2010, Dagstuhl Follow-Ups, P259
   McMahan RP, 2012, IEEE T VIS COMPUT GR, V18, P626, DOI 10.1109/TVCG.2012.43
   PHONG BT, 1975, COMMUN ACM, V18, P311, DOI 10.1145/360825.360839
   Pluim JPW, 2000, LECT NOTES COMPUT SC, V1935, P452
   Ropinski T, 2008, COMPUT GRAPH FORUM, V27, P567, DOI 10.1111/j.1467-8659.2008.01154.x
   Ruiz M., 2008, IEEE EG INT S VOL PO, P113
   Schott M., 2009, P EUROVIS 09, P855
   Sielhorst T, 2006, LECT NOTES COMPUT SC, V4190, P364
   Soltészová V, 2012, IEEE T VIS COMPUT GR, V18, P2265, DOI 10.1109/TVCG.2012.188
   Solteszova Veronika., 2011, Proc. of NPAR, P105, DOI DOI 10.1145/2024676.2024694
   Starck JL, 2001, ASTRON ASTROPHYS, V368, P730, DOI 10.1051/0004-6361:20000575
   Stewart AJ, 2003, IEEE VISUALIZATION 2003, PROCEEDINGS, P355, DOI 10.1109/VISUAL.2003.1250394
   Sun J, 1998, NAT NEUROSCI, V1, P183, DOI 10.1038/630
   Sutcliffe A, 2004, INTERACT COMPUT, V16, P831, DOI 10.1016/j.intcom.2004.05.001
   Sutcliffe AG, 2000, BEHAV INFORM TECHNOL, V19, P415, DOI 10.1080/014492900750052679
   Thompson WB, 2004, PRESENCE-TELEOP VIRT, V13, P560, DOI 10.1162/1054746042545292
   Vázquez PP, 2007, COMPUT GRAPH FORUM, V26, P143, DOI 10.1111/j.1467-8659.2007.00944.x
   WANGER LR, 1992, IEEE COMPUT GRAPH, V12, P44, DOI 10.1109/38.135913
NR 42
TC 12
Z9 12
U1 0
U2 16
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2017
VL 33
IS 1
BP 47
EP 61
DI 10.1007/s00371-015-1151-6
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA EI2JM
UT WOS:000392313200006
DA 2024-07-18
ER

PT J
AU Hong, K
   Hong, S
AF Hong, Kan
   Hong, Sheng
TI Real-time stress assessment using thermal imaging
SO VISUAL COMPUTER
LA English
DT Article
DE Stress assessment; Thermal imaging; Image processing
ID AXIS; TEMPERATURE; PERFORMANCE; DECEPTION; FACE
AB Thermal imaging is one of the most promising methods of probing the psychological status of human beings because of its non-invasiveness. This study develops a new method of assessing the stress status in real time. The differential energy between philtrum (located in the maxillary area) and forehead (DEFP) algorithm is developed to amplify and extract stress-induced thermal imprints. The algorithm is then validated against the clinical standard in a controlled lab experiment in which neurophysiologic responses are invoked from subjects via Trier Social Stress Test. The correlation between the extracted thermal imprints and established stress markers (heart beat rate and cortisol level) is significant. Experimental result demonstrates that DEFP has the capacity to classify stress and baseline status. An accuracy rate of over 90 % suggests the feasibility of the real-time assessment of stress, disregarding personal factors.
C1 [Hong, Kan] Jiangxi Sci & Technol Normal Univ, Optoelect & Commun Engn Key Lab, Nanchang, Jiangxi, Peoples R China.
   [Hong, Sheng] Beihang Univ, Sch Reliabil & Syst Engn, Sci & Technol Reliabil & Environm Engn Lab, 37 Xue Yuan Rd, Beijing 100191, Peoples R China.
C3 Jiangxi Science & Technology Normal University; Beihang University
RP Hong, S (corresponding author), Beihang Univ, Sch Reliabil & Syst Engn, Sci & Technol Reliabil & Environm Engn Lab, 37 Xue Yuan Rd, Beijing 100191, Peoples R China.
EM 16451946@qq.com
FU National Natural Science Foundation of China [61304111]; Beijing Natural
   Science Foundation [4153059]; National Program on Key Basic Research
   Program of China [2014CB744904]; Fundamental Research Funds for the
   Central Universities, China [YWF-14-KKX-001]
FX The authors are highly thankful for the financial support of National
   Natural Science Foundation of China (61304111), Beijing Natural Science
   Foundation (4153059), National Program on Key Basic Research Program of
   China under grant 2014CB744904, and Fundamental Research Funds for the
   Central Universities under grant No. YWF-14-KKX-001, China.
CR Anbar M, 2002, ANN NY ACAD SCI, V972, P111, DOI 10.1111/j.1749-6632.2002.tb04560.x
   [Anonymous], 1953, KATHIMERINI
   [Anonymous], 2014, PLOS ONE
   [Anonymous], 2012, ACM Transactions on Graphics (TOG), DOI DOI 10.1145/2185520.2185561
   Apostolakis KC, 2014, VISUAL COMPUT, V30, P1093, DOI 10.1007/s00371-013-0903-4
   Barreto A, 2007, LECT NOTES COMPUT SC, V4796, P29
   Berry J.W., 2011, COUNS PSYCHOL, V48, P447
   Bhatt S, 2009, BRAIN COGNITION, V69, P382, DOI 10.1016/j.bandc.2008.08.033
   Bles M, 2008, NEUROCASE, V14, P82, DOI 10.1080/13554790801992784
   Chekmenev S., 2009, MULTIRESOLUTION APPR
   Condren RM, 2002, PSYCHONEUROENDOCRINO, V27, P693, DOI 10.1016/S0306-4530(01)00070-1
   Daubechies, 1992, 10 LECT WAVELETS
   Di Giacinto A, 2014, NEUROSCIENCE, V266, P216, DOI 10.1016/j.neuroscience.2014.02.009
   Dowdall J, 2007, COMPUT VIS IMAGE UND, V106, P205, DOI 10.1016/j.cviu.2006.08.011
   DRUMMOND PD, 1994, AUST J PSYCHOL, V46, P95, DOI 10.1080/00049539408259479
   DRUMMOND PD, 1987, BRAIN, V110, P793, DOI 10.1093/brain/110.3.793
   Drummond PD, 1997, PSYCHOPHYSIOLOGY, V34, P163, DOI 10.1111/j.1469-8986.1997.tb02127.x
   Ebisch SJ, 2012, BIOL PSYCHOL, V89, P123, DOI 10.1016/j.biopsycho.2011.09.018
   Federenko I.S., 2004, CLIN ENDOCRINOL META, V89, P127
   Fei J, 2005, P ANN INT IEEE EMBS, P700
   Fei J, 2010, IEEE T BIO-MED ENG, V57, P988, DOI 10.1109/TBME.2009.2032415
   Garbey M, 2007, IEEE T BIO-MED ENG, V54, P1418, DOI 10.1109/TBME.2007.891930
   GAULT T.R., 2010, COMPUTER VISION PATT, P1, DOI DOI 10.1109/CVPRW.2010.5544602
   Gunes Hatice, 2011, Proceedings 2011 IEEE International Conference on Automatic Face & Gesture Recognition (FG 2011), P827, DOI 10.1109/FG.2011.5771357
   Hahn AC, 2012, BIOL LETTERS, V8, P864, DOI 10.1098/rsbl.2012.0338
   Healey JA, 2005, IEEE T INTELL TRANSP, V6, P156, DOI 10.1109/TITS.2005.848368
   Hellhammer DH, 2009, PSYCHONEUROENDOCRINO, V34, P163, DOI 10.1016/j.psyneuen.2008.10.026
   Hong S, 2014, DIGIT SIGNAL PROCESS, V27, P159, DOI 10.1016/j.dsp.2013.12.010
   Ioannou S, 2014, PSYCHOPHYSIOLOGY, V51, P951, DOI 10.1111/psyp.12243
   Ioannou S, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0079440
   Ishii H, 2009, ARCH ORAL BIOL, V54, P486, DOI 10.1016/j.archoralbio.2009.01.012
   Jarlier S, 2011, IEEE T AFFECT COMPUT, V2, P2, DOI 10.1109/T-AFFC.2011.3
   Kozel FA, 2009, J FORENSIC SCI, V54, P220, DOI 10.1111/j.1556-4029.2008.00927.x
   Kozel FA, 2005, BIOL PSYCHIAT, V58, P605, DOI 10.1016/j.biopsych.2005.07.040
   Kudielka BM, 2004, PSYCHONEUROENDOCRINO, V29, P983, DOI 10.1016/j.psyneuen.2003.08.009
   Kudielka BM, 2005, BIOL PSYCHOL, V69, P113, DOI 10.1016/j.biopsycho.2004.11.009
   Kuraoka K, 2011, PHYSIOL BEHAV, V102, P347, DOI 10.1016/j.physbeh.2010.11.029
   Levine JA, 2001, LANCET, V357, P1757, DOI 10.1016/S0140-6736(00)04936-9
   Levine JA, 2009, WORK, V34, P359, DOI 10.3233/WOR-2009-0934
   MADRIGAL A, 2009, WIRED SCI       0316
   Nakasone Arturo., 2005, Proc. of the 5th International Workshop on Biosignal Interpretation, P219
   Nakayama K, 2005, PHYSIOL BEHAV, V84, P783, DOI 10.1016/j.physbeh.2005.03.009
   Nhan B., 2010, IEEE T BIOMED ENG, V57, P887
   Nicolaou Mihalis A., 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P3695, DOI 10.1109/ICPR.2010.900
   Nozawa A, 2011, ARTIF LIFE ROBOT, V16, P147, DOI 10.1007/s10015-011-0903-2
   Nozawa A, 2009, J STAT MECH-THEORY E, DOI 10.1088/1742-5468/2009/01/P01007
   Or Calvin K. L., 2007, Occupational Ergonomics, V7, P83
   Pavlidis I, 2007, COMPUT VIS IMAGE UND, V108, P150, DOI 10.1016/j.cviu.2006.11.018
   Pavlidis I, 2012, SCI REP-UK, V2, DOI 10.1038/srep00305
   Pavlidis I, 2002, NATURE, V415, P35, DOI 10.1038/415035a
   Pavlidis I., 2001, IEEE, V2, P315, DOI [10.1109/ICIP.2001.958491, DOI 10.1109/ICIP.2001.958491]
   Pavlidis I, 2003, P 25 ANN INT C IEEE, P17
   Petkie D.T., 2009, P SPIE EUR SECUR DEF, V7, P1
   Petkie DT, 2009, IEEE RAD CONF, P301
   Rashid M, 2013, VISUAL COMPUT, V29, P1269, DOI 10.1007/s00371-012-0768-y
   Read GF., 1985, STEROID BIOCH, V22, P437
   Sauro MD, 2001, J BEHAV MED, V24, P423, DOI 10.1023/A:1012219426415
   Schommer NC, 2003, PSYCHOSOM MED, V65, P450, DOI 10.1097/01.PSY.0000035721.12441.17
   Shastri D, 2012, IEEE T AFFECT COMPUT, V3, P366, DOI 10.1109/T-AFFC.2012.13
   Shastri D, 2009, IEEE T BIO-MED ENG, V56, P477, DOI 10.1109/TBME.2008.2003265
   Shi YuanNguyen., 2010, INT S QUALITY LIFE T, P28
   Stewart M, 2007, PHYSIOL BEHAV, V92, P520, DOI 10.1016/j.physbeh.2007.04.034
   Tsiotsios C, 2013, PATTERN RECOGN, V46, P1369, DOI 10.1016/j.patcog.2012.11.012
   VATNER SF, 1974, J CLIN INVEST, V54, P225, DOI 10.1172/JCI107757
   Vianna DML, 2005, EUR J NEUROSCI, V21, P2505, DOI 10.1111/j.1460-9568.2005.04073.x
   Wang Q, 2011, VISUAL COMPUT, V27, P299, DOI 10.1007/s00371-011-0551-5
   WILLIAMS H, 1984, ARCH DIS CHILD, V59, P553, DOI 10.1136/adc.59.6.553
   WING RR, 1985, PSYCHOSOM MED, V47, P558, DOI 10.1097/00006842-198511000-00005
   Wöllmer M, 2008, INTERSPEECH 2008: 9TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2008, VOLS 1-5, P597
   Zhou Y, 2013, IEEE T BIO-MED ENG, V60, P1280, DOI 10.1109/TBME.2012.2232927
NR 70
TC 14
Z9 14
U1 0
U2 16
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2016
VL 32
IS 11
BP 1369
EP 1377
DI 10.1007/s00371-015-1164-1
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA EA2BS
UT WOS:000386397000002
DA 2024-07-18
ER

PT J
AU Ma, MY
   Peng, SL
   Hu, XY
AF Ma, Mingyang
   Peng, Silong
   Hu, Xiyuan
TI A lighting robust fitting approach of 3D morphable model for face
   reconstruction
SO VISUAL COMPUTER
LA English
DT Article
DE Morphable model; 3D face; Sphere harmonic illumination; Spatially
   varying specular reflectance
ID RECOGNITION; IMAGE; SHAPE; ILLUMINATION; REFLECTANCE
AB Three-dimensional morphable model (3DMM) is a powerful tool for recovering 3D shape and texture from a single facial image. The success of 3DMM relies on two things: an effective optimization strategy and a realistic approach to synthesizing face images. However, most previous methods have focused on developing an optimization strategy under Phong's synthesis approach. In this paper, we adopt a more realistic synthesis technique that fully considers illumination and reflectance in the 3DMM fitting process. Using the sphere harmonic illumination model (SHIM), our new synthesis approach can account for more lighting factors than Phong's model. Spatially varying specular reflectance is also introduced into the synthesis process. Under SHIM, the cost function is nearly linear for all parameters, which simplifies the optimization. We apply our new optimization algorithm to determine the shape and texture parameters simultaneously. The accuracy of the recovered shape and texture can be improved significantly by considering the spatially varying specular reflectance. Hence, our algorithm produces an enhanced shape and texture compared with previous SHIM-based methods that recover shape from feature points. Although we use just a single input image in a profile pose, our approach gives plausible results. Experiments on a well-known image database show that, compared to state-of-the-art methods based on Phong's model, the proposed approach enhances the robustness of the 3DMM fitting results under extreme lighting and profile pose.
C1 [Ma, Mingyang; Peng, Silong; Hu, Xiyuan] Chinese Acad Sci, Insititute Automat, 95 Zhongguancun East Rd, Beijing 100190, Peoples R China.
C3 Chinese Academy of Sciences
RP Hu, XY (corresponding author), Chinese Acad Sci, Insititute Automat, 95 Zhongguancun East Rd, Beijing 100190, Peoples R China.
EM mingyang.ma@ia.ac.cn; silong.peng@ia.ac.cn; xiyuan.hu@ia.ac.cn
RI Silong, Peng/E-4704-2013; Ma, Mingyang/JXM-3330-2024; Hu,
   Xiyuan/AAF-7773-2021
OI Hu, Xiyuan/0000-0002-7095-6986
FU National Natural Science Foundation of China [61201375, 61571438];
   National High Technology R&D Project of China (863 Program)
   [2013AA014602]
FX We appreciate very much the comments from anonymous reviewers. Their
   suggestions have contributed to beneficial improvement on the earlier
   version of this manuscript and futurere-search. This work was supported
   in part by the National Natural Science Foundation of China under Grant
   Nos. 61201375, 61571438 and the National High Technology R&D Project of
   China (863 Program) (2013AA014602).
CR Aldrian O, 2013, IEEE T PATTERN ANAL, V35, P1080, DOI 10.1109/TPAMI.2012.206
   [Anonymous], 2010, COMPUT GRAPHICS-US, DOI DOI 10.1145/1722991.1722996
   Basri R, 2003, IEEE T PATTERN ANAL, V25, P218, DOI 10.1109/TPAMI.2003.1177153
   Basri R, 2007, INT J COMPUT VISION, V72, P239, DOI 10.1007/s11263-006-8815-7
   Biswas S, 2009, IEEE T PATTERN ANAL, V31, P884, DOI [10.1109/TPAMI.2008.135, 10.1109/TPAMI.2007.12.0830]
   Blanz V, 1999, COMP GRAPH, P187, DOI 10.1145/311535.311556
   Blanz V, 2003, IEEE T PATTERN ANAL, V25, P1063, DOI 10.1109/TPAMI.2003.1227983
   Blanz V, 2004, COMPUT GRAPH FORUM, V23, P669, DOI 10.1111/j.1467-8659.2004.00799.x
   Blanz V., 2002, IT+TI Informationstechnik und Technische Informatik, V44, P295, DOI 10.1524/itit.2002.44.6.295
   Burgos-Artizzu XP, 2013, IEEE I CONF COMP VIS, P1513, DOI 10.1109/ICCV.2013.191
   Cao C, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2462012
   Ding L, 2014, VISUAL COMPUT, V30, P189, DOI 10.1007/s00371-013-0795-3
   Doidge IC, 2013, VISUAL COMPUT, V29, P707, DOI 10.1007/s00371-013-0807-3
   Donner C., 2008, ACM T GRAPHIC, V27, P140
   Ersotelos N, 2008, VISUAL COMPUT, V24, P13, DOI 10.1007/s00371-007-0175-y
   Garrido P, 2014, PROC CVPR IEEE, P4217, DOI 10.1109/CVPR.2014.537
   Hu G., 2013, INT C BIOMETRICS, P1
   Hu KX, 2014, VISUAL COMPUT, V30, P685, DOI 10.1007/s00371-014-0962-1
   Kemelmacher-Shlizerman I, 2014, PROC CVPR IEEE, P3334, DOI 10.1109/CVPR.2014.426
   Kemelmacher-Shlizerman I, 2011, IEEE I CONF COMP VIS, P1746, DOI 10.1109/ICCV.2011.6126439
   Kemelmacher-Shlizerman I, 2011, IEEE T PATTERN ANAL, V33, P394, DOI 10.1109/TPAMI.2010.63
   Kontkanen J, 2006, MONTE CARLO AND QUASI-MONTE CARLO METHODS 2004, P259, DOI 10.1007/3-540-31186-6_16
   Li A.H.L., 2008, P 8 IEEE INT C AUT F, P1
   Li GN, 2013, COMPUT GRAPH FORUM, V32, P275, DOI 10.1111/cgf.12047
   Li SX, 2012, LECT NOTES COMPUT SC, V7572, P102, DOI 10.1007/978-3-642-33718-5_8
   Liu SK, 2013, VISUAL COMPUT, V29, P1135, DOI 10.1007/s00371-012-0756-2
   Ma MY, 2014, INT C PATT RECOG, P2101, DOI 10.1109/ICPR.2014.366
   Nocedal J, 2006, SPRINGER SER OPER RE, P135
   Patel A, 2012, PATTERN RECOGN, V45, P1993, DOI 10.1016/j.patcog.2011.11.013
   Patrik H., 2015, INT C IM PROC ICIP, P1
   Paysan P, 2010, THESIS
   Paysan P, 2009, AVSS: 2009 6TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE, P296, DOI 10.1109/AVSS.2009.58
   Phillips PJ, 2005, PROC CVPR IEEE, P947
   Ramamoorthi R, 2004, ACM T GRAPHIC, V23, P1004, DOI 10.1145/1027411.1027416
   Ramamoorthi R, 2001, COMP GRAPH, P497, DOI 10.1145/383259.383317
   Ramamoorthi R, 2002, IEEE T PATTERN ANAL, V24, P1322, DOI 10.1109/TPAMI.2002.1039204
   Romdhani S, 2005, PROC CVPR IEEE, P986
   Romdhani S, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P59
   Romdhani S, 2002, LECT NOTES COMPUT SC, V2353, P3
   Sánchez-Escobedo D, 2013, PATTERN RECOGN LETT, V34, P389, DOI 10.1016/j.patrec.2012.09.007
   Sim T, 2003, IEEE T PATTERN ANAL, V25, P1615, DOI 10.1109/TPAMI.2003.1251154
   Smith WAP, 2008, INT J COMPUT VISION, V76, P71, DOI 10.1007/s11263-007-0074-8
   Sun Y, 2013, PROC CVPR IEEE, P3476, DOI 10.1109/CVPR.2013.446
   Wang Y, 2009, IEEE T PATTERN ANAL, V31, P1968, DOI 10.1109/TPAMI.2008.244
   Wei L.Y., 2014, VISUAL COMPUT, V30, P1
   Weyrich T, 2006, ACM T GRAPHIC, V25, P1013, DOI 10.1145/1141911.1141987
   Wu CL, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508418
   Xiangyu Zhu, 2015, 2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG), P1, DOI 10.1109/FG.2015.7163096
   Zhang L, 2006, IEEE T PATTERN ANAL, V28, P351, DOI 10.1109/TPAMI.2006.53
   Zhao WY, 2001, INT J COMPUT VISION, V45, P55, DOI 10.1023/A:1012369907247
   Zhao WY, 2000, PROC CVPR IEEE, P286, DOI 10.1109/CVPR.2000.855831
NR 51
TC 8
Z9 9
U1 0
U2 11
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2016
VL 32
IS 10
BP 1223
EP 1238
DI 10.1007/s00371-015-1158-z
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA EA2BQ
UT WOS:000386396800002
DA 2024-07-18
ER

PT J
AU Liu, J
   Li, CP
   Fan, XF
   Wang, ZQ
   Shi, M
   Yang, J
AF Liu, Jing
   Li, Chunpeng
   Fan, Xuefeng
   Wang, Zhaoqi
   Shi, Min
   Yang, Jie
TI View synthesis with 3D object segmentation-based asynchronous blending
   and boundary misalignment rectification
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 33rd Conference on Computer Graphics International (CGI)
CY JUN 28-JUL 01, 2016
CL Heraklion, GREECE
SP Fdn Res Technol
DE 3D object segmentation; Trilateral depth filter; Virtual view synthesis;
   Boundary misalignment
ID DEPTH; VIDEO
AB Numerous depth image-based rendering algorithms have been proposed to synthesize the virtual view for the free viewpoint television. However, inaccuracies in the depth map cause visual artifacts in the virtual view. In this paper, we propose a novel virtual view synthesis framework to create the virtual view of the scene. Here, we incorporate a trilateral depth filter with local texture information, spatial proximity, and color similarity to remove the ghost contours by rectifying the misalignment between the depth map and its associated color image. To further enhance the quality of the synthesized virtual views, we partition the scene into different 3D object segments based on the color image and depth map. Each 3D object segment is warped and blended independently to avoid mixing the pixels belonging to different parts of the scene. The evaluation results indicate that the proposed method significantly improves the quality of the synthesized virtual view compared with other methods and are qualitatively very similar to the ground truth. In addition, it also performs well in real-world scenes.
C1 [Liu, Jing; Li, Chunpeng; Fan, Xuefeng; Wang, Zhaoqi] Chinese Acad Sci, Beijing Key Lab Mobile Comp & Pervas Device, Inst Comp Technol, 6 Kexueyuan South Rd, Beijing 100190, Peoples R China.
   [Liu, Jing; Fan, Xuefeng] Univ Chinese Acad Sci, 19A Yuquan Rd, Beijing 100049, Peoples R China.
   [Shi, Min; Yang, Jie] North China Elect Power Univ, Sch Control & Comp Engn, 2 Beinong Rd, Beijing 102206, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Computing Technology, CAS;
   Chinese Academy of Sciences; University of Chinese Academy of Sciences,
   CAS; North China Electric Power University
RP Liu, J (corresponding author), Chinese Acad Sci, Beijing Key Lab Mobile Comp & Pervas Device, Inst Comp Technol, 6 Kexueyuan South Rd, Beijing 100190, Peoples R China.; Liu, J (corresponding author), Univ Chinese Acad Sci, 19A Yuquan Rd, Beijing 100049, Peoples R China.
EM liujing01@ict.ac.cn
RI Liu, Jing/T-6504-2019; Li, Chunpeng/AAE-6134-2019
OI Liu, Jing/0000-0002-2217-0372; 
CR Ahn I, 2013, IEEE T BROADCAST, V59, P614, DOI 10.1109/TBC.2013.2281658
   [Anonymous], 2014 3DTV C TRUE VIS
   Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114
   Dahan MJ, 2012, VISUAL COMPUT, V28, P1181, DOI 10.1007/s00371-011-0667-7
   Liu J, 2015, VISUAL COMPUT, V31, P1253, DOI 10.1007/s00371-014-1009-3
   Liu MY, 2013, PROC CVPR IEEE, P169, DOI 10.1109/CVPR.2013.29
   Loghman M, 2015, MULTIMED TOOLS APPL, V74, P1611, DOI 10.1007/s11042-013-1747-7
   MULLER K, 2008, MULT SIGN PROC 2008, P34
   Solh M, 2012, IEEE J-STSP, V6, P495, DOI 10.1109/JSTSP.2012.2204723
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wolinski David., 2013, Proceedings of the 21st ACM international conference on Multimedia, P669
   Zitnick CL, 2004, ACM T GRAPHIC, V23, P600, DOI 10.1145/1015706.1015766
NR 12
TC 5
Z9 6
U1 0
U2 7
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2016
VL 32
IS 6-8
BP 989
EP 999
DI 10.1007/s00371-016-1228-x
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA DP8ET
UT WOS:000378731600030
DA 2024-07-18
ER

PT J
AU Shen, JC
   Luo, YL
   Wu, ZK
   Tian, Y
   Deng, QQ
AF Shen, Junchen
   Luo, Yanlin
   Wu, Zhongke
   Tian, Yun
   Deng, Qingqiong
TI CUDA-based real-time hand gesture interaction and visualization for CT
   volume dataset using leap motion
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT International Conference on Cyberworlds (CW)
CY OCT 06-08, 2014
CL Santander, SPAIN
SP IEEE Comp Soc, Univ Cantabria, Comp Graph & Geometr Modeling Grp, Toho Univ, Fac Sci, Dept Informat Sci, European Assoc Comp Graph, Int Federat Informat Proc, Workgroup 5 10 Comp Graph & Virtual Worlds, Univ Cantabria, Dept Appl Math & Computatl Sci, Vice Rector Res & Knowledge Transfer, Municipal Santander, Reg Govt Cantabria, Spanish Minist Econ & Competitiveness, Cantabria Campus Int, Int Federat Informat Proc, Tech Comm 5 Informat Technol Applicat
DE Touchless interaction; Hand and finger gesture; Bimanual interaction;
   Transfer function; Volume visualization
AB Touchless interaction has received considerable attention in recent years with benefit of removing barriers of physical contact. Several approaches are available to achieve mid-air interactions. However, most of these techniques cause discomfort when the interaction method is not direct manipulation. In this paper, gestures based on unimanual and bimanual interactions with different tools for exploring CT volume dataset are designed to perform the similar tasks in realistic applications. Focus + context approach based on GPU volume ray casting by trapezoid-shaped transfer function is used for visualization and the level-of-detail technique is adopted for accelerating interactive rendering. Comparing the effectiveness and intuitiveness of interaction approach with others by experiments, ours has a better performance and superiority with less completion time. Moreover, the bimanual interaction with more advantages is timesaving when performing continuous exploration task.
C1 [Shen, Junchen; Luo, Yanlin; Wu, Zhongke; Tian, Yun; Deng, Qingqiong] Beijing Normal Univ, Coll Informat Sci & Technol, Engn Res Ctr Virtual Real & Applicat, Minist Educ, Beijing 100875, Peoples R China.
C3 Beijing Normal University
RP Luo, YL (corresponding author), Beijing Normal Univ, Coll Informat Sci & Technol, Engn Res Ctr Virtual Real & Applicat, Minist Educ, Beijing 100875, Peoples R China.
EM luoyl@bnu.edu.cn
RI Li, Juan/JEO-6872-2023; LI, Wenhui/JCD-9947-2023
CR Andujar C., 2011, SBC J 3D INTERACTIVE, V2, P2
   Balakrishnan R., 2004, P 17 ANN ACM S US IN, P61, DOI DOI 10.1145/1029632.1029644
   Barr A. H., 1981, IEEE Computer Graphics and Applications, V1, P11, DOI 10.1109/MCG.1981.1673799
   Bérard F, 2009, LECT NOTES COMPUT SC, V5727, P400, DOI 10.1007/978-3-642-03658-3_45
   Brandl P., 2008, Proceedings of the working conference on Advanced visual interfaces, AVI '08, P154, DOI [10.1145/1385569.1385595, DOI 10.1145/1385569.1385595]
   Bruckner S, 2007, COMPUT GRAPH FORUM, V26, P715, DOI 10.1111/j.1467-8659.2007.01095.x
   Bruckner S, 2005, IEEE VISUALIZATION 2005, PROCEEDINGS, P671
   BUXTON WAS, 1990, HUMAN-COMPUTER INTERACTION : INTERACT 90, P449
   Cohen M, 2004, THEORY AND PRACTICE OF COMPUTER GRAPHICS 2004, PROCEEDINGS, P32, DOI 10.1109/TPCG.2004.1314450
   Diaz J, 2010, IEEE EG S VOL GRAPH, DOI [10.2312/VG/VG10/093-100, DOI 10.2312/VG/VG10/093-100]
   Gallo L., 2013, SIGGRAPH ASIA 2013 T, P1
   Gobbetti E, 2008, VISUAL COMPUT, V24, P797, DOI 10.1007/s00371-008-0261-9
   Gobbetti E, 2012, COMPUT GRAPH FORUM, V31, P1315, DOI 10.1111/j.1467-8659.2012.03124.x
   GUIARD Y, 1987, J MOTOR BEHAV, V19, P486
   Hanqi G., 2011, SYSTEMS MAN CYBERN A, V17, P2106
   Jacob RJK, 2008, CHI 2008: 26TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS VOLS 1 AND 2, CONFERENCE PROCEEDINGS, P201
   Jalaliniya S., 2013, Proceedings of the 2013 ACM conference on Pervasive and ubiquitous computing adjunct publication - UbiComp'13 Adjunct, P1265, DOI [10.1145/2494091.2497332., DOI 10.1145/2494091.2497332]
   Laha B., 2014, IEEE VR 2014 WORKSH
   Laha B, 2012, IEEE T VIS COMPUT GR, V18, P597, DOI 10.1109/TVCG.2012.42
   Latulipe C., 2005, Proceedings of the UIST '05 Symposium on User Interface Software and Technology, P123
   Laursen L. F., 2013, 5 INT C INT ASS SOC, P1134
   Leganchuk A., 1998, ACM Transactions on Computer-Human Interaction, V5, P326, DOI 10.1145/300520.300522
   LEVOY M, 1988, IEEE COMPUT GRAPH, V8, P29, DOI 10.1109/38.511
   Luo YL, 2012, COMPUT SCI ENG, V14, P63, DOI 10.1109/MCSE.2011.114
   McInerney T, 2006, PROC GRAPH INTERF, P171
   Monclus E., 2009, PROC ACM S VIRTUAL R, P119, DOI [10.1145/1643928.1643955, DOI 10.1145/1643928.1643955]
   O'Hara K, 2014, COMMUN ACM, V57, P70, DOI 10.1145/2541883.2541899
   Owen R., 2005, Proceedings of Graphics Interface 2005, P17
   Ropinski T., 2006, P 5 INT S SMART GRAP, V3638, P218
   Siemens, GAM CONS TECHN OP RO
   Song Peng., 2012, P 2012 ACM ANN C HUM, P1297, DOI DOI 10.1145/2207676.2208585
   Ullrich S., 2011, 2011 IEEE S SING, P39
   Vanacken L., 2007, 3DUI 07 IEEE S
   Wang RY, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531369
   Wigdor D, 2011, BRAVE NUI WORLD: DESIGNING NATURAL USER INTERFACES FOR TOUCH AND GESTURE, P1
   Zhang Q, 2011, VISUAL COMPUT, V27, P3, DOI 10.1007/s00371-010-0509-z
NR 36
TC 11
Z9 12
U1 0
U2 17
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2016
VL 32
IS 3
BP 359
EP 370
DI 10.1007/s00371-016-1209-0
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA DF9FK
UT WOS:000371666200009
DA 2024-07-18
ER

PT J
AU Förger, K
   Takala, T
AF Forger, Klaus
   Takala, Tapio
TI Animating with style: defining expressive semantics of motion
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 3DOR Workshop
CY APR 06, 2014
CL Strasbourg, FRANCE
DE Computer animation; Human motion; Motion style; Motion synthesis; Style
   vector; Feature extraction; Feature selection; Verbal description of
   motion style
AB Actions performed by a virtual character can be controlled with verbal commands such as 'walk five steps forward'. Similar control of the motion style, meaning how the actions are performed, is complicated by the ambiguity of describing individual motions with phrases such as 'aggressive walking'. In this paper, we present a method for controlling motion style with relative commands such as 'do the same, but more sadly'. Based on acted example motions, comparative annotations, and a set of calculated motion features, relative styles can be defined as vectors in the feature space. We present a new method for creating these style vectors by finding out which features are essential for a style to be perceived and eliminating those that show only incidental correlations with the style. We show with a user study that our feature selection procedure is more accurate than earlier methods for creating style vectors, and that the style definitions generalize across different actors and annotators. We also present a tool enabling interactive control of parametric motion synthesis by verbal commands. As the control method is independent from the generation of motion, it can be applied to virtually any parametric synthesis method.
C1 [Forger, Klaus; Takala, Tapio] Aalto Univ, Dept Comp Sci, Otaniementie 17, Espoo 02150, Finland.
C3 Aalto University
RP Förger, K (corresponding author), Aalto Univ, Dept Comp Sci, Otaniementie 17, Espoo 02150, Finland.
EM klaus.forger@aalto.fi; tapio.takala@aalto.fi
RI Takala, Tapio/G-2446-2013; Forger, Klaus/F-5265-2012
OI Takala, Tapio/0000-0002-7704-5800; Forger, Klaus/0000-0002-8826-9981
NR 0
TC 4
Z9 5
U1 1
U2 5
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2016
VL 32
IS 2
BP 191
EP 203
DI 10.1007/s00371-015-1064-4
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA DF9FU
UT WOS:000371667200005
DA 2024-07-18
ER

PT J
AU Sato, S
   Dobashi, Y
   Yue, Y
   Iwasaki, K
   Nishita, T
AF Sato, Syuhei
   Dobashi, Yoshinori
   Yue, Yonghao
   Iwasaki, Kei
   Nishita, Tomoyuki
TI Incompressibility-preserving deformation for fluid flows using vector
   potentials
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 32nd Computer Graphics International CGI 15 Conference
CY JUN 24-26, 2015
CL INSA Strasbourg Univ Strasbourg, Strasbourg, FRANCE
SP CNRS, iCUBE, Univ Strasbourg, CGS, Springer, Acm Incooperation, IGG, ACMSIGGRAPH, KIST Europe, Region Alsace, Visteon
HO INSA Strasbourg Univ Strasbourg
DE Flow deformation; Incompressibility; Vector potential
ID ANIMATION
AB Physically based fluid simulations usually require expensive computation cost for creating realistic animations. We present a technique that allows the user to create various fluid animations from an input fluid animation sequence, without the need for repeatedly performing simulations. Our system allows the user to deform the flow field in order to edit the overall fluid behavior. In order to maintain plausible physical behavior, we ensure the incompressibility to guarantee the mass conservation. We use a vector potential for representing the flowfields to realize such incompressibility-preserving deformations. Our method first computes (time-varying) vector potentials from the input velocity field sequence. Then, the user deforms the vector potential, and the system computes the deformed velocity field by taking the curl operator on the vector potential. The incompressibility is thus obtained by construction. We show various examples to demonstrate the usefulness of our method.
C1 [Sato, Syuhei] UEI Res, Tokyo, Japan.
   [Dobashi, Yoshinori] Hokkaido Univ, UEI Res, JST CREST, Sapporo, Hokkaido, Japan.
   [Yue, Yonghao] Columbia Univ, New York, NY USA.
   [Iwasaki, Kei] Wakayama Univ, UEI Res, Wakayama, Japan.
   [Nishita, Tomoyuki] Hiroshima Shudo Univ, UEI Res, Tokyo, Japan.
C3 Hokkaido University; Columbia University; Wakayama University
RP Sato, S (corresponding author), UEI Res, Tokyo, Japan.
EM syuhei.sato@uei.co.jp
RI Iwasaki, Kei/GNH-6504-2022; Yue, Yonghao/O-5203-2018
OI Iwasaki, Kei/0000-0002-5235-536X; Yue, Yonghao/0000-0002-8252-0522;
   Sato, Syuhei/0009-0007-9270-7335
FU JSPS
FX This work was supported in part by the JSPS Postdoctoral Fellowships for
   Research Abroad.
CR [Anonymous], 2000, Vortex Methods: Theory and Practice
   [Anonymous], 2008, Fluid Simulation for Computer Graphics
   Bhatia H, 2013, IEEE T VIS COMPUT GR, V19, P1386, DOI 10.1109/TVCG.2012.316
   Fattal R, 2004, ACM T GRAPHIC, V23, P441, DOI 10.1145/1015706.1015743
   Fedkiw R, 2001, COMP GRAPH, P15, DOI 10.1145/383259.383260
   Feldman BE, 2003, ACM T GRAPHIC, V22, P708, DOI 10.1145/882262.882336
   Foster N, 2001, COMP GRAPH, P23, DOI 10.1145/383259.383261
   Fuller AR, 2007, I3D 2007: ACM SIGGRAPH SYMPOSIUM ON INTERACTIVE 3D GRAPHICS AND GAMES, PROCEEDINGS, P175
   Gamito M. N., 1995, Computer Animation and Simulation '95. Proceedings of the Eurographics Workshop, P3
   Hong W, 2008, VISUAL COMPUT, V24, P535, DOI 10.1007/s00371-008-0234-z
   Jakob Wenzel, 2010, Mitsuba renderer
   Kim T, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461987
   Kim Y., 2006, P 2006 ACM SIGGRAPHE, P33
   Lamorlette A, 2002, ACM T GRAPHIC, V21, P729, DOI 10.1145/566570.566644
   Nguyen DQ, 2002, ACM T GRAPHIC, V21, P721, DOI 10.1145/566570.566643
   Park S. I., 2005, Computer Animation, Conference Proceedings, P261, DOI [DOI 10.1145/1073368.1073406, 10.1145/1073368.1073406]
   Pighin Frederic., 2004, SCA'04: Proceedings ofthe 2004 ACM SIGGRAPH/Eurographics symposium on Computer animation, P223, DOI 10.1145/1028523.1028552
   Sato S., 2014, P SIGGRAPH AS 2014 T
   Schaefer S, 2006, ACM T GRAPHIC, V25, P533, DOI 10.1145/1141911.1141920
   Shi Lin., 2005, Proceedings of the 2005 ACM SIGGRAPH/Eurographics Symposium on Computer Animation, SCA '05, P229, DOI DOI 10.1145/1073368.1073401
   Stam J, 1999, COMP GRAPH, P121, DOI 10.1145/311535.311548
   Tong YY, 2003, ACM T GRAPHIC, V22, P445, DOI 10.1145/882262.882290
   Treuille A, 2003, ACM T GRAPHIC, V22, P716, DOI 10.1145/882262.882337
   Treuille A, 2006, ACM T GRAPHIC, V25, P826, DOI 10.1145/1141911.1141962
   Wicke M, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531345
   Yaeger L., 1986, Computer Graphics, V20, P85, DOI 10.1145/15886.15895
   Zhang XH, 2014, CHALCOGENIDE LETT, V11, P1
NR 27
TC 13
Z9 14
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2015
VL 31
IS 6-8
BP 959
EP 965
DI 10.1007/s00371-015-1122-y
PG 7
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA CM2CN
UT WOS:000357487500021
DA 2024-07-18
ER

PT J
AU Rao, YB
AF Rao, Yunbo
TI Automatic vehicle recognition in multiple cameras for video surveillance
SO VISUAL COMPUTER
LA English
DT Article
DE Level-based region comparison; License plate recognition;
   Multiple-cameras; Automatic vehicle recognition
ID LICENSE PLATE-RECOGNITION; NEURAL-NETWORK; SYSTEM; ALGORITHM
AB To efficiently locate identical objects in heterogeneous cameras and possibly propagate reliable information between cameras and refine detection, many techniques were used to recognize vehicles. In this paper, we investigate several key problems and present a novel approach for automatic vehicle recognition (AVR) in multiple cameras for video surveillance application. We propose a level-based region comparison algorithm to AVR in multiple cameras. For improving the recognition accuracy, new license plate recognition method is also proposed. Experimental results show that the proposed algorithm is simple and efficient, and the quality of the composed image can be comparable with the results of the state-of-the-art methods.
C1 Univ Elect Sci & Technol China, Sch Informat & Software Engn, Chengdu 610054, Sichuan, Peoples R China.
C3 University of Electronic Science & Technology of China
RP Rao, YB (corresponding author), Univ Elect Sci & Technol China, Sch Informat & Software Engn, Chengdu 610054, Sichuan, Peoples R China.
EM uestc2008@126.com
FU National Science Foundation of China [61300092]
FX The authors would like to thank the anonymous reviewers for their
   helpful comments. This work is partly supported by National Science
   Foundation of China (Grant No.61300092).
CR Abolghasemi V, 2007, LECT NOTES COMPUT SC, V4781, P468
   Agnes EJ, 2012, PHYSICA A, V391, P843, DOI 10.1016/j.physa.2011.08.036
   Anagnostopoulos CNE, 2006, IEEE T INTELL TRANSP, V7, P377, DOI 10.1109/TITS.2006.880641
   Anagnostopoulos CNE, 2008, IEEE T INTELL TRANSP, V9, P377, DOI 10.1109/TITS.2008.922938
   Chang SL, 2004, IEEE T INTELL TRANSP, V5, P42, DOI 10.1109/TITS.2004.825086
   COMELLI P, 1995, IEEE T VEH TECHNOL, V44, P790, DOI 10.1109/25.467963
   Cui YT, 1998, MACH VISION APPL, V10, P308, DOI 10.1007/s001380050081
   Draghici S, 1997, INT J NEURAL SYST, V8, P113, DOI 10.1142/S0129065797000148
   Duong V. H, 2005, CVPR '05, DOI DOI 10.1109/CVPR.2005.61
   Fan B., 2012, IEEE INT C IM SIGN P
   Gonzalez R C, 2008, DIGITAL IMAGE PROCES
   Jia WJ, 2007, J NETW COMPUT APPL, V30, P1324, DOI 10.1016/j.jnca.2006.09.010
   Kim KK, 2000, NEURAL NETWORKS FOR SIGNAL PROCESSING X, VOLS 1 AND 2, PROCEEDINGS, P614, DOI 10.1109/NNSP.2000.890140
   Kim S.K., 1996, P INT C IM PROC, V2, P661, DOI DOI 10.1109/ICIP.1996.560964
   LAND EH, 1971, J OPT SOC AM, V61, P1, DOI 10.1364/JOSA.61.000001
   Lin WY, 2010, IEEE T CIRC SYST VID, V20, P1057, DOI 10.1109/TCSVT.2010.2057013
   Lin WY, 2008, IEEE T CIRC SYST VID, V18, P1128, DOI 10.1109/TCSVT.2008.927111
   Nomura S, 2005, PATTERN RECOGN, V38, P1961, DOI 10.1016/j.patcog.2005.01.026
   Parisi R, 1998, ISCAS '98 - PROCEEDINGS OF THE 1998 INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS, VOLS 1-6, pB195
   Rao Y.B., 2011, VISUAL COMMUNICATION
   Rao YB, 2010, OPT ENG, V49, DOI 10.1117/1.3520553
   Shi XF, 2005, LECT NOTES COMPUT SC, V3483, P1159
   ter Brugge MH, 1998, CNNA 98 - 1998 FIFTH IEEE INTERNATIONAL WORKSHOP ON CELLULAR NEURAL NETWORKS AND THEIR APPLICATIONS - PROCEEDINGS, P212, DOI 10.1109/CNNA.1998.685366
   Yo-Ping Huang, 2004, 2004 IEEE International Conference on Networking, Sensing and Control (IEEE Cat. No.04EX761), P737
   Yoshimori S, 2003, LECT NOTES ARTIF INT, V2773, P585
   Zhang D., 2005, P CVPR
NR 26
TC 12
Z9 13
U1 1
U2 41
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2015
VL 31
IS 3
BP 271
EP 280
DI 10.1007/s00371-013-0917-y
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CC3DB
UT WOS:000350223900003
DA 2024-07-18
ER

PT J
AU Berretti, S
   Werghi, N
   del Bimbo, A
   Pala, P
AF Berretti, Stefano
   Werghi, Naoufel
   del Bimbo, Alberto
   Pala, Pietro
TI Selecting stable keypoints and local descriptors for person
   identification using 3D face scans
SO VISUAL COMPUTER
LA English
DT Article
DE 3D face recognition; 3D Keypoints detection; Stable scale space
   selection; Feature selection
ID RECOGNITION; EXPRESSIONS
AB 3D face identification based on the detection and comparison of keypoints of the face is a promising solution to extend face recognition approaches to the case of 3D scans with occlusions and missing parts. In fact, approaches that perform sparse keypoints matching can naturally allow for partial face comparison. However, such methods typically use a large number of keypoints, locally described by high-dimensional feature vectors: This, combined with the combinatorial number of keypoint comparisons required to match two face scans, results in a high computational cost that does not scale well with large datasets. Motivated by these considerations, in this paper, we present a 3D face recognition approach based on the meshDOG keypoints detector and local GH descriptor, and propose original solutions to improve keypoints stability and select the most effective features from the local descriptors. Experiments have been performed to assess the validity of the proposed optimizations for stable keypoints detection and feature selection. Recognition accuracy has been evaluated on the Bosphorus database, showing competitive results with respect to existing 3D face identification solutions based on 3D keypoints.
C1 [Berretti, Stefano; del Bimbo, Alberto; Pala, Pietro] Univ Florence, Dept Informat Engn, Florence, Italy.
   [Werghi, Naoufel] Khalifa Univ Sci Technol & Res, Dept Elect & Comp Engn, Abu Dhabi, U Arab Emirates.
   [Berretti, Stefano; del Bimbo, Alberto] Univ Florence, Media Integrat & Commun Ctr, Florence, Italy.
   [Werghi, Naoufel] Univ Edinburgh, Div Informat, Edinburgh EH8 9YL, Midlothian, Scotland.
   [Werghi, Naoufel] Univ Glasgow, Dept Comp Sci, Glasgow G12 8QQ, Lanark, Scotland.
   [del Bimbo, Alberto] Univ Florence, Florence, Italy.
   [del Bimbo, Alberto] IEEE ICMCS 99, Florence, Italy.
   [del Bimbo, Alberto] ACM Multimedia 2008, Vancouver, BC, Canada.
   [del Bimbo, Alberto] ACM Multimedia 2010, Florence, Italy.
   [del Bimbo, Alberto] ECCV 2012, Florence, Italy.
C3 University of Florence; Khalifa University of Science & Technology;
   University of Florence; University of Edinburgh; University of Glasgow;
   University of Florence
RP Berretti, S (corresponding author), Univ Florence, Dept Informat Engn, Florence, Italy.
EM stefano.berretti@unifi.it; naoufel.werghi@kustar.ac.ae
RI Werghi, Naoufel/ABA-6280-2020; Berretti, Stefano/U-9004-2019
OI Werghi, Naoufel/0000-0002-5542-448X; Berretti,
   Stefano/0000-0003-1219-4386; DEL BIMBO, ALBERTO/0000-0002-1052-8322;
   PALA, PIETRO/0000-0001-5670-3774
CR Al-Osaimi F, 2009, INT J COMPUT VISION, V81, P302, DOI 10.1007/s11263-008-0174-0
   [Anonymous], 2011, EUR WORKSH 3D OBJ RE
   ASHBROOK AP, 1998, P EUR C COMP VIS FRI, P674
   Berretti S., 2012, P EORKSH NONR SHAP A
   Berretti S, 2013, COMPUT GRAPH-UK, V37, P509, DOI 10.1016/j.cag.2013.04.001
   Berretti S, 2013, IEEE T INF FOREN SEC, V8, P374, DOI 10.1109/TIFS.2012.2235833
   Berretti S, 2010, IEEE T PATTERN ANAL, V32, P2162, DOI 10.1109/TPAMI.2010.43
   Bowyer KW, 2006, COMPUT VIS IMAGE UND, V101, P1, DOI 10.1016/j.cviu.2005.05.005
   DeCarlo D., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P67, DOI 10.1145/280814.280823
   Dorko G, 2006, LECT NOTES COMPUT SC, V3954, P504
   Drira H, 2013, IEEE T PATTERN ANAL, V35, P2270, DOI 10.1109/TPAMI.2013.48
   Erdogmus N., 2013, P IEEE INR C BIOM TH
   Faltemier TC, 2008, IEEE T INF FOREN SEC, V3, P62, DOI 10.1109/TIFS.2007.916287
   Farkas LG, 1994, Anthropometry of Head and Face, Vsecond
   Goswami G, 2013, 2013 IEEE SIXTH INTERNATIONAL CONFERENCE ON BIOMETRICS: THEORY, APPLICATIONS AND SYSTEMS (BTAS)
   Huang D, 2012, IEEE T INF FOREN SEC, V7, P1551, DOI 10.1109/TIFS.2012.2206807
   Huibin Li, 2011, 2011 18th IEEE International Conference on Image Processing (ICIP 2011), P3053, DOI 10.1109/ICIP.2011.6116308
   Huynh T., 2012, P ACCV WORKSH COMP V
   Kakadiaris IA, 2007, IEEE T PATTERN ANAL, V29, P640, DOI 10.1109/TPAMI.2007.1017
   Lei YJ, 2014, PATTERN RECOGN, V47, P509, DOI 10.1016/j.patcog.2013.07.018
   Li BYL, 2013, IEEE WORK APP COMP, P186, DOI 10.1109/WACV.2013.6475017
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Maes C., 2010, BIOMETRICS THEORY AP, DOI [DOI 10.1109/BTAS.2010.5634543, 10.1109/BTAS.2010.5634543]
   Mian AS, 2008, INT J COMPUT VISION, V79, P1, DOI 10.1007/s11263-007-0085-5
   Min R, 2012, INT C PATT RECOG, P1739
   Passalis G, 2011, IEEE T PATTERN ANAL, V33, P1938, DOI 10.1109/TPAMI.2011.49
   Peng HC, 2005, IEEE T PATTERN ANAL, V27, P1226, DOI 10.1109/TPAMI.2005.159
   Perakis P, 2013, IEEE T PATTERN ANAL, V35, P1552, DOI 10.1109/TPAMI.2012.247
   Peyre G, 2009, MATLAB CTR FILE EXCH
   Phillips PJ, 2005, PROC CVPR IEEE, P947
   Savran A., 2008, P COST 2101 WORKSH B
   Smeets D, 2013, COMPUT VIS IMAGE UND, V117, P158, DOI 10.1016/j.cviu.2012.10.002
   Tombari F, 2013, INT J COMPUT VISION, V102, P198, DOI 10.1007/s11263-012-0545-4
   Wang YM, 2010, IEEE T PATTERN ANAL, V32, P1858, DOI 10.1109/TPAMI.2009.200
   Werghi N, 2012, EURASIP J ADV SIG PR, DOI 10.1186/1687-6180-2012-144
   Yin LJ, 2006, PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION - PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE, P211
   Zaharescu A, 2009, PROC CVPR IEEE, P373, DOI 10.1109/CVPRW.2009.5206748
   Zuliani M, 2005, IEEE IMAGE PROC, P2969
NR 38
TC 33
Z9 38
U1 0
U2 15
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2014
VL 30
IS 11
BP 1275
EP 1292
DI 10.1007/s00371-014-0932-7
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AS3HX
UT WOS:000344169300007
DA 2024-07-18
ER

PT J
AU Chi, J
   Tu, CH
   Zhang, CM
AF Chi, Jing
   Tu, Changhe
   Zhang, Caiming
TI Dynamic 3D facial expression modeling using Laplacian smooth and
   multi-scale mesh matching
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 31st CGI conference
CY JUN 10-13, 2014
CL Sydney, AUSTRALIA
DE Expression modeling; Laplacian smooth; Mesh matching; Point clouds
ID CAPTURE; MOTION
AB We propose a novel algorithm for the high-resolution modeling of dynamic 3D facial expressions from a sequence of unstructured face point clouds captured at video rate. The algorithm can reconstruct not only the global facial deformations caused by muscular movements, but also the expressional details generated by local skin deformations. Our algorithm consists of two parts: Extraction of expressional details and Reconstruction of expressions. In the extraction part, we extract the subtle expressional details such as wrinkles and folds from each point cloud with Laplacian smooth operator. In the reconstruction part, we use a multi-scale deformable mesh model to match each point cloud to reconstruct time-varying expressions. In each matching, we first use the low-scale mesh to match the global deformations of point cloud obtained after filtering out the expressional details, and then use the high-scale mesh to match the extracted expressional details. Comparing to many existing non-rigid ICP-based algorithms that match directly the mesh model to the entire point cloud, our algorithm overcomes the probable large errors occurred where the local sharp deformations are matched since it extracts the expressional details for separate matching, therefore, our algorithm can produce a high-resolution dynamic model reflecting time-varying expressions. Additionally, utilization of multi-scale mesh model makes our algorithm achieve high speed because it decreases iterative optimizations in matching. Experiments demonstrate the efficiency of our algorithm.
C1 [Chi, Jing; Zhang, Caiming] Shandong Univ Finance & Econ, Dept Comp Sci & Technol, Jinan, Peoples R China.
   [Chi, Jing; Tu, Changhe; Zhang, Caiming] Shandong Univ, Jinan 250100, Peoples R China.
   [Chi, Jing] Shandong Prov Key Lab Digital Media Technol, Jinan, Peoples R China.
C3 Shandong University of Finance & Economics; Shandong University
RP Chi, J (corresponding author), Shandong Univ Finance & Econ, Dept Comp Sci & Technol, Jinan, Peoples R China.
EM peace_world_cj@hotmail.com
RI Tu, Changhe/H-5162-2013; Zhang, Caiming/AHD-6558-2022
OI Zhang, Caiming/0000-0002-6365-6221; Chi, Jing/0000-0002-0307-0608
FU National Nature Science Foundation of China [61303088, 61020106001,
   61332015, 61272242, U1201258]; Nature Science Foundation of Shandong
   Province [BS2013DX039]; Sci-tech Development Project of Jinan City
   [201303021]
FX We would like to thank the authors of [6] and [25] for sharing the face
   data used in our experiments. The work is supported by National Nature
   Science Foundation of China under Grant 61303088, 61020106001, 61332015,
   61272242, U1201258, Nature Science Foundation of Shandong Province under
   Grant BS2013DX039, Sci-tech Development Project of Jinan City under
   Grant 201303021.
CR Amberg B, 2007, IEEE I CONF COMP VIS, P1326
   [Anonymous], 2004, 2004 C COMPUTER VISI
   [Anonymous], 2005, CUCS02405
   Beeler T, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964970
   Bickel B, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239484
   Bradley D, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778778
   Chi J., 2011, COMPUT AIDED DES APP, V8, P859
   Dellepiane M, 2008, COMPUT GRAPH FORUM, V27, P1719, DOI 10.1111/j.1467-8659.2008.01316.x
   Furukawa Y, 2009, PROC CVPR IEEE, P1674, DOI 10.1109/CVPRW.2009.5206868
   Goldenstein S., 2004, CVPR, P1880
   Huang HY, 2011, ADV MATER RES-SWITZ, V163-167, P74, DOI 10.4028/www.scientific.net/AMR.163-167.74
   Huang HD, 2012, IEEE T VIS COMPUT GR, V18, P1215, DOI 10.1109/TVCG.2012.88
   Huang YH, 2012, IMAGE VISION COMPUT, V30, P750, DOI 10.1016/j.imavis.2011.12.008
   Hyneman W., 2005, ACM SIGGRAPH 2005 CO, P5
   Joshi P., 2003, ACM SIGGRAPH/Eurographics Symposium on Computer Animation, P187
   Le BH, 2013, IEEE T VIS COMPUT GR, V19, P1859, DOI 10.1109/TVCG.2013.84
   Li H, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618521
   Li H, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2077341.2077343
   Minoi JL, 2007, APGV 2007: SYMPOSIUM ON APPLIED PERCEPTION IN GRAPHICS AND VISUALIZATION, PROCEEDINGS, P138
   Oat Christopher, 2007, Proceedings of the ACM SIGGRAPH 2007 courses (New York, NY, USA2007), ACM, V4, P33
   Popa T, 2010, COMPUT GRAPH FORUM, V29, P1633, DOI 10.1111/j.1467-8659.2010.01772.x
   Schneider David C., 2009, 2009 IEEE 12th International Conference on Computer Vision Workshops, ICCV Workshops, P304, DOI 10.1109/ICCVW.2009.5457684
   Sibbing D, 2011, COMPUT VIS IMAGE UND, V115, P668, DOI 10.1016/j.cviu.2010.11.022
   Süssmuth J, 2008, COMPUT GRAPH FORUM, V27, P1469, DOI 10.1111/j.1467-8659.2008.01287.x
   Wand M, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1516522.1516526
   Wang Y, 2008, INT J COMPUT VISION, V76, P283, DOI 10.1007/s11263-007-0063-y
   Zeng Y, 2011, PROC CVPR IEEE, P1225, DOI 10.1109/CVPR.2011.5995513
   Zhang L, 2004, ACM T GRAPHIC, V23, P548, DOI 10.1145/1015706.1015759
NR 28
TC 8
Z9 9
U1 0
U2 18
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2014
VL 30
IS 6-8
BP 649
EP 659
DI 10.1007/s00371-014-0960-3
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA AI7GX
UT WOS:000337054700009
DA 2024-07-18
ER

PT J
AU Knowles, P
   Leach, G
   Zambetta, F
AF Knowles, Pyarelal
   Leach, Geoff
   Zambetta, Fabio
TI Fast sorting for exact OIT of complex scenes
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 31st CGI conference
CY JUN 10-13, 2014
CL Sydney, AUSTRALIA
DE Sorting; OIT; Transparency; Shaders; Performance; Registers;
   Register-based block sort
AB Exact order-independent transparency (OIT) techniques capture all fragments during rasterization. The fragments are then sorted per-pixel by depth and composited in order using alpha transparency. The sorting stage is a bottleneck for high depth complexity scenes, taking 70-95 % of the total time for those investigated. In this paper, we show that typical shader-based sorting speed is impacted by local memory latency and occupancy. We present and discuss the use of both registers and an external merge sort in register-based block sort to better use the memory hierarchy of the GPU for improved OIT rendering performance. This approach builds upon backwards memory allocation, achieving an OIT rendering speed up to 1.7 that of the best previous method and 6.3 that of the common straight forward OIT implementation. In some cases, the sorting stage is reduced to no longer be the dominant OIT component.
C1 [Knowles, Pyarelal; Leach, Geoff; Zambetta, Fabio] RMIT Univ, Melbourne, Vic, Australia.
C3 Royal Melbourne Institute of Technology (RMIT)
RP Knowles, P (corresponding author), RMIT Univ, Melbourne, Vic, Australia.
EM heuristic42@gmail.com
OI Zambetta, Fabio/0000-0003-4133-7913
CR [Anonymous], 2001, TECHNICAL REPORT
   Bauer F., 2013, IEEE INT C COMP AID
   Bavoil L, 2007, I3D 2007: ACM SIGGRAPH SYMPOSIUM ON INTERACTIVE 3D GRAPHICS AND GAMES, PROCEEDINGS, P97
   Furtak T, 2007, SPAA'07: PROCEEDINGS OF THE NINETEENTH ANNUAL SYMPOSIUM ON PARALLELISM IN ALGORITHMS AND ARCHITECTURES, P348
   Hermes Jan., 2010, VMV, P65
   Kauker D., 2013, PROC EUROGRAPHICS S, P33
   Kessenich John., OPENGL SHADING LANGU
   Knowles P., 2013, Proc. of Pacific Graphics 2013, P59
   Knowles P, 2012, OPENGL INSIGHTS, P279
   Lefebvre S., 2013, RR8282 INRIA
   Lipowski J.K., 2011, D BUFFER LETTING 3 D
   Lipowski JK, 2010, LECT NOTES COMPUT SC, V6375, P89, DOI 10.1007/978-3-642-15907-7_12
   Maule M., 2012, 2012 XXV SIBGRAPI - Conference on Graphics, Patterns and Images (SIBGRAPI 2012), P134, DOI 10.1109/SIBGRAPI.2012.27
   Maule Marilena., 2013, I3D, P103, DOI DOI 10.1145/2448196.2448212
   Peeper C., 2008, Patent, Patent No. [2008/0316214, 20080316214]
   Ranade A., 2000, High Performance Computing - HiPC 2000. 7th International Conference. Proceedings (Lecture Notes in Computer Science Vol.1970), P96
   Salvi Marco., 2011, PROC ACM SIGGRAPH S, P119, DOI [10.1145/2018323.2018342, DOI 10.1145/2018323.2018342]
   Szecsi L., 2012, EUROGRAPHICS SHORT P, P93
   Thibieroz N., 2010, GDC 2010 C SESS ADV
   Tokuyoshi Y, 2013, COMPUT GRAPH FORUM, V32, P315, DOI 10.1111/cgf.12239
   WICKREMESINGHE R, 2002, J EXP ALGORITHMICS, V7, P9
   Yang JC, 2010, COMPUT GRAPH FORUM, V29, P1297, DOI 10.1111/j.1467-8659.2010.01725.x
NR 22
TC 8
Z9 9
U1 0
U2 4
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2014
VL 30
IS 6-8
BP 603
EP 613
DI 10.1007/s00371-014-0956-z
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA AI7GX
UT WOS:000337054700005
DA 2024-07-18
ER

PT J
AU Gong, MY
   Sun, LF
   Yang, SQ
   Yang, Y
AF Gong, Mingying
   Sun, Lifeng
   Yang, Shiqiang
   Yang, Yun
TI Find where you are: a new try in place recognition
SO VISUAL COMPUTER
LA English
DT Article
DE Image recognition; Retrieval failure prediction; Geometric verification;
   Relative camera orientation; Salient region detection
ID SCALE
AB LBS-based applications have become trending on mobile phones. It is useful and necessary to locate users location precisely from a digital image. However, gap exists between the query and the data set in scale, viewpoint, and lighting, or the noise existed in the foreground or background, etc. It is challenging for a location recognition or retrieval system to carry out real-time service. To address this problem, we design a place recognition system and a new building data set with ground truth labels. The algorithm not only significantly improves the efficiency, but also gives satisfied accuracy. The main contributions of our work can be concluded by three points: (1) By adding a fast geometric image matching as a filter procedure before applying Random Sample Consensus (RANSAC), we substantially improve the efficiency of spatial verification and recognition accuracy. (2) We apply a camera orientation algorithm to predict a probable retrieval failure. Salient region detection is applied to remove the confusing features in combination with term frequency-inverse document frequency (tf-idf) recovery. This significantly reduces the influence of noise. (3) We establish a new building data set of Tsinghua University to verify retrieval results. Experiments are conducted on several data sets and all achieve state-of-the-art results.
C1 [Gong, Mingying; Sun, Lifeng; Yang, Shiqiang; Yang, Yun] Tsinghua Univ, Beijing Key Lab Networked Multimedia, Beijing 100084, Peoples R China.
C3 Tsinghua University
RP Gong, MY (corresponding author), Tsinghua Univ, Beijing Key Lab Networked Multimedia, Beijing 100084, Peoples R China.
EM gongmingying07@gmail.com
RI yang, shiqiang/AAH-5484-2019
FU 973 Program [2011CB302206]; NSFC [61272231/60833009]; National
   Significant Science and Technology Projects of China
   [2011ZX01042-001-002]
FX This work was supported by 973 Program under Grant No. 2011CB302206,
   NSFC under Grant Nos. 61272231/60833009, and National Significant
   Science and Technology Projects of China under Grant No.
   2011ZX01042-001-002.
CR [Anonymous], IEEE WORKSH APPL COM
   Arandjelovic R, 2012, PROC CVPR IEEE, P2911, DOI 10.1109/CVPR.2012.6248018
   Arya S, 1998, J ACM, V45, P891, DOI 10.1145/293347.293348
   Bower F., 2005, Proceedings of 38th Annual IEEE/ACM International Symposium on Microarchitecture, P1
   Calonder M, 2010, LECT NOTES COMPUT SC, V6314, P778, DOI 10.1007/978-3-642-15561-1_56
   Cheng MM, 2011, PROC CVPR IEEE, P409, DOI 10.1109/CVPR.2011.5995344
   Chum O., 2004, Proc. of the Asian Conference on Computer Vision ACCV, V2, P812
   Chum O, 2007, IEEE I CONF COMP VIS, P496, DOI 10.1109/cvpr.2007.383172
   Chum O, 2011, PROC CVPR IEEE, P889, DOI 10.1109/CVPR.2011.5995601
   DUDA RO, 1972, COMMUN ACM, V15, P11, DOI 10.1145/361237.361242
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Hartley RI, 1997, IEEE T PATTERN ANAL, V19, P580, DOI 10.1109/34.601246
   Huang TJ, 2011, SCI CHINA INFORM SCI, V54, P2461, DOI 10.1007/s11432-011-4487-1
   Jegou H, 2008, LECT NOTES COMPUT SC, V5302, P304, DOI 10.1007/978-3-540-88682-2_24
   Jégou H, 2011, IEEE T PATTERN ANAL, V33, P117, DOI 10.1109/TPAMI.2010.57
   Jégou H, 2009, PROC CVPR IEEE, P1169, DOI 10.1109/CVPRW.2009.5206609
   Knopp J, 2010, LECT NOTES COMPUT SC, V6311, P748, DOI 10.1007/978-3-642-15549-9_54
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Mikolajczyk K, 2004, INT J COMPUT VISION, V60, P63, DOI 10.1023/B:VISI.0000027790.02288.f2
   Mingying Gong, 2012, Computational Visual Media. Proceedings First International Conference, CVM 2012, P210, DOI 10.1007/978-3-642-34263-9_27
   Nister David, 2006, CVPR
   Philbin J., 2008, P CVPR, P1
   Quack Till., 2008, P 2008 INT C CONTENT, P47
   Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663
   Snavely N, 2008, INT J COMPUT VISION, V80, P189, DOI 10.1007/s11263-007-0107-3
   Tsai SS, 2010, IEEE IMAGE PROC, P1029, DOI 10.1109/ICIP.2010.5648942
   Wang Z., 2011, P 1 INT WORKSH MOB L, P53
   Zobel J, 2006, ACM COMPUT SURV, V38, DOI 10.1145/1132956.1132959
NR 28
TC 1
Z9 1
U1 0
U2 22
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2013
VL 29
IS 11
BP 1211
EP 1220
DI 10.1007/s00371-013-0784-6
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 236PL
UT WOS:000325811300009
DA 2024-07-18
ER

PT J
AU Liu, XD
   Zheng, CW
AF Liu, Xiao Dan
   Zheng, Chang Wen
TI Parallel adaptive sampling and reconstruction using multi-scale and
   directional analysis
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Computer Graphics International (CGI) Conference
CY 2013
CL Hanover, GERMANY
DE Contourlet; Adaptive sampling; Anisotropic filter; Multi-scale analysis
AB We propose a novel parallel image space adaptive rendering approach. Contourlet transform which includes Laplacian pyramid and directional filter banks is modified for multi-scale and directional analysis in our algorithm. In sampling stage, the image space is coarsely sampled first. The sampled image is analyzed into coarse and difference values in multi-scale using Laplacian pyramid transform. Based on the analysis, a heuristic method is proposed to repeatedly distribute adaptive Monte Carlo samples. In reconstruction stage, the final image is reconstructed by filtering each pixel using our anisotropic per-pixel filter. The filter size depends on the variance and attenuation values. The filter's anisotropic property is computed by the directional filter banks. Compared to the state-of-the-art image space adaptive rendering methods, the results rendered by our algorithm show improvement in both visual image quality and numerical error while using sparse samples.
C1 [Liu, Xiao Dan; Zheng, Chang Wen] Chinese Acad Sci, Inst Software, Natl Key Lab Integrated Informat Syst Technol, Beijing, Peoples R China.
   [Liu, Xiao Dan] Chinese Acad Sci, Grad Univ, Beijing, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Software, CAS; Chinese Academy
   of Sciences; University of Chinese Academy of Sciences, CAS
RP Liu, XD (corresponding author), Chinese Acad Sci, Inst Software, Natl Key Lab Integrated Informat Syst Technol, Beijing, Peoples R China.
EM lxdfigo@163.com
CR [Anonymous], 1987, P 14 ANN C COMP GRAP, DOI [DOI 10.1145/37401.37410, DOI 10.1145/37402.37410]
   [Anonymous], 1991, ACM SIGGRAPH 91, DOI DOI 10.1145/127719.122736
   Dammertz H., 2010, P C HIGH PERF GRAPH, P67, DOI DOI 10.5555/1921479.1921491
   Do MN, 2005, IEEE T IMAGE PROCESS, V14, P2091, DOI 10.1109/TIP.2005.859376
   Egan K, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1944846.1944849
   Egan K, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531399
   Genetti J, 1998, COMPUT GRAPH FORUM, V17, P29, DOI 10.1111/1467-8659.00214
   Hachisuka T, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360632
   Jin B., 2009, P C HIGH PERFORMANCE, P117
   Lehtinen J, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185547
   Li TM, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366213
   Liu XD, 2012, VISUAL COMPUT, V28, P613, DOI 10.1007/s00371-012-0709-9
   Overbeck RS, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618486
   Park SI, 2004, IEEE T IMAGE PROCESS, V13, P1424, DOI 10.1109/TIP.2004.836186
   Portilla J, 2003, IEEE T IMAGE PROCESS, V12, P1338, DOI 10.1109/TIP.2003.818640
   Ragan-Kelley J, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1966394.1966396
   Rousselle F, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366214
   Rousselle F, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024193
   Sen P., 2012, ACM T GRAPH
   Shirley P., 2011, ACM Symposium on Interactive 3D Graphics, P9
   Skodras A, 2001, IEEE SIGNAL PROC MAG, V18, P36, DOI 10.1109/79.952804
   Soler C, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1516522.1516529
NR 22
TC 4
Z9 4
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2013
VL 29
IS 6-8
BP 501
EP 511
DI 10.1007/s00371-013-0814-4
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 151RR
UT WOS:000319478400005
DA 2024-07-18
ER

PT J
AU Todo, H
   Anjyo, K
   Yokoyama, S
AF Todo, Hideki
   Anjyo, Ken
   Yokoyama, Shun'ichi
TI Lit-Sphere extension for artistic rendering
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Computer Graphics International (CGI) Conference
CY 2013
CL Hanover, GERMANY
DE Non-photorealistic rendering; Stylized rendering; Lit-Sphere
AB The Lit-Sphere model proposed by Sloan et al. (Proceedings of Graphics Interface 2001, pp. 143-150, 2001) is a method for emulating expressive artistic shading styles for 3D scenes. Assuming that artistic shading styles are described by view space normals, this model produces a variety of stylized shading scenes beyond traditional 3D lighting control. However, it is limited to the static lighting case: the shading effect is only dependent on the camera view. In addition, it cannot support small-scale brush stroke styles. In this paper, we propose a scheme to extend the Lit-Sphere model based on light space normals rather than view space normals. Owing to the light space representation, our shading model addresses the issues of the original Lit-Sphere approach, and allows artists to use a light source to obtain dynamic diffuse and specular shading. Then the shading appearance can be refined using stylization effects including highlight shape control, sub-lighting effects, and brush stroke styles. Our algorithms are easy to implement on GPU, so that our system allows interactive shading design.
C1 [Todo, Hideki; Anjyo, Ken] OLM Digital Inc, JST CREST, Setagaya Ku, Tokyo 1540023, Japan.
   [Yokoyama, Shun'ichi] Kyushu Univ, IMI, JST CREST, Fukuoka 812, Japan.
C3 Japan Science & Technology Agency (JST); Kyushu University
RP Todo, H (corresponding author), Univ Tokyo, Bunkyo Ku, 7-3-1 Hongo, Tokyo 1130033, Japan.
EM tody411@gmail.com; anjyo@olm.co.jp; s-yokoyama@imi.kyushu-u.ac.jp
FU Japan Science and Technology Agency, CREST project
FX We would like to thank the anonymous reviewers for their constructive
   comments. We are also grateful to Marc Salvati, Yosuke Katsura, Tatsuo
   Yotsukura and Ayumi Kimura for their valuable discussions, suggestions,
   and assistance for this work. Additional thanks go to AIM@SHAPE Shape
   Repository, Keenan's 3D Model Repository for the 3D models used in this
   paper. This work was supported in part by the Japan Science and
   Technology Agency, CREST project.
CR Anjyo Kenichi., 2006, Symposium on Non-photorealistic Animation and Rendering, NPAR '06, P133
   [Anonymous], 2001, P GRAPHICS INTERFACE
   Barla P., 2006, Proceedings of the 4th international symposium on Non-photorealistic animation and rendering, P127, DOI DOI 10.1145/1124728.1124749
   Gooch B., 2001, Non-photorealistic rendering
   Hogarth B, 1991, DYNAMIC LIGHT SHADE
   Kulla CD, 2003, 11TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P477, DOI 10.1109/PCCGA.2003.1238298
   Mitchell J, 2007, NPAR 2007: 5TH INTERNATIONAL SYMPOSIUM ON NON-PHOTOREALISTIC ANIMATION AND RENDERING, PROCEEDINGS, P71
   Pacanowski R., 2008, P SBIM 2008 ANN FRAN
   RITSCHEL T., 2010, ACM T GRAPH, V29, P4
   Todo H, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1276377.1276399, 10.1145/1239451.1239468]
   Vanderhaeghe D., 2011, Proceedings of the 8th International Symposium on Non-Photorealistic Animation and Rendering, P99, DOI [10.1145/2024676.2024693, DOI 10.1145/2024676.2024693]
   Yan CR, 2008, IEEE T VIS COMPUT GR, V14, P468, DOI 10.1109/TVCG.2007.70440
NR 12
TC 7
Z9 9
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2013
VL 29
IS 6-8
BP 473
EP 480
DI 10.1007/s00371-013-0811-7
PG 8
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 151RR
UT WOS:000319478400002
DA 2024-07-18
ER

PT J
AU Fayolle, PA
   Pasko, A
AF Fayolle, Pierre-Alain
   Pasko, Alexander
TI Segmentation of discrete point clouds using an extensible set of
   templates
SO VISUAL COMPUTER
LA English
DT Article
DE Surface fitting; Segmentation; Surface, solid and object
   representations; Simulated annealing
ID RANDOM-WALKS; MESH; REPRESENTATION; APPROXIMATION; CONSTRUCTION;
   EXTRACTION; CURVES
AB We present an algorithm for segmenting a discrete three-dimensional point-set (i.e., partitioning an input discrete point-set into appropriate subsets). The algorithm consists in the iteration of two main steps which are: fitting the parameters of template primitives from a user-specified list of primitives and extracting the points from the input point-set matching the best fitted primitive. We illustrate the results of applying our algorithm to several examples of three-dimensional point-sets.
C1 [Fayolle, Pierre-Alain] Univ Aizu, Fukushima Ku, Tsuruga, Fukui 9658580, Japan.
   [Pasko, Alexander] Bournemouth Univ, Natl Ctr Comp Animat, Poole BH12 5BB, Dorset, England.
C3 University of Aizu; Bournemouth University
RP Fayolle, PA (corresponding author), Univ Aizu, Fukushima Ku, Ikki Machi, Tsuruga, Fukui 9658580, Japan.
EM fayolle@u-aizu.ac.jp; apasko@bournemouth.ac.uk
RI Fayolle, Pierre-Alain/AAA-6385-2022; Pasko, Alexander/H-9344-2017
OI Pasko, Alexander/0000-0002-4785-7066
CR Ahn SJ, 2002, IEEE T PATTERN ANAL, V24, P620, DOI 10.1109/34.1000237
   [Anonymous], 2011, ACM Trans. Graph.
   Attene M, 2006, VISUAL COMPUT, V22, P181, DOI 10.1007/s00371-006-0375-x
   Barr A. H., 1981, IEEE Computer Graphics and Applications, V1, P11, DOI 10.1109/MCG.1981.1673799
   Benko P, 2002, COMPUT AIDED GEOM D, V19, P173, DOI 10.1016/S0167-8396(01)00085-1
   Benko P, 2001, COMPUT AIDED DESIGN, V33, P839, DOI 10.1016/S0010-4485(01)00100-2
   BENTLEY JL, 1975, COMMUN ACM, V18, P509, DOI 10.1145/361002.361007
   Buchele SF, 2004, COMPUT AIDED DESIGN, V36, P1063, DOI 10.1016/j.cad.2004.01.006
   Chazelle B, 1997, COMP GEOM-THEOR APPL, V7, P327, DOI 10.1016/S0925-7721(96)00024-7
   Cohen-Steiner D, 2004, ACM T GRAPHIC, V23, P905, DOI 10.1145/1015706.1015817
   Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236
   CORANA A, 1987, ACM T MATH SOFTWARE, V13, P262, DOI 10.1145/29380.29864
   Faber P., 2001, Pattern Recognition. 23rd DAGM Symposium. Proceedings (Lecture Notes in Computer Science Vol.2191), P414
   Faber P., 2001, Visual Form 2001. 4th International Workshop on Visual Form IWVF4. Proceedings (Lecture Notes in Computer Science Vol.2059), P165
   Fayolle Pierre-Alain, 2008, Heterogeneous Objects Modelling and Applications, P118, DOI 10.1007/978-3-540-68443-5_5
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Fitzgibbon AW, 1997, COMPUT AIDED DESIGN, V29, P321, DOI 10.1016/S0010-4485(96)00059-0
   Fleishman S, 2003, ACM T GRAPHIC, V22, P950, DOI 10.1145/882262.882368
   Garland M., 2001, I3D 01, P49, DOI [DOI 10.1145/364338.364345, 10.1145/364338.364345]
   Gelfand Natasha, 2004, Proceedings of the 2004 Eurographics/ACM SIGGRAPH symposium on Geometry processing, P214
   Grady L, 2006, IEEE T PATTERN ANAL, V28, P1768, DOI 10.1109/TPAMI.2006.233
   Guillaume L, 2004, COMPUTER GRAPHICS INTERNATIONAL, PROCEEDINGS, P10, DOI 10.1109/CGI.2004.1309187
   HOPPE H, 1992, COMP GRAPH, V26, P71, DOI 10.1145/142920.134011
   INGBER L, 1989, MATH COMPUT MODEL, V12, P967, DOI 10.1016/0895-7177(89)90202-1
   Johnson A, 1997, THESIS CARNEGIO MELL
   Johnson AE, 1999, IEEE T PATTERN ANAL, V21, P433, DOI 10.1109/34.765655
   Jones TR, 2003, ACM T GRAPHIC, V22, P943, DOI 10.1145/882262.882367
   Kazhdan M., 2006, P 4 EUR S GEOM PROC, P61, DOI DOI 10.2312/SGP/SGP06/061-070
   LAI YK, 2006, P 2006 ACM S SOL PHY, P25
   Lai YK, 2007, IEEE T VIS COMPUT GR, V13, P34, DOI 10.1109/TVCG.2007.19
   Lai YK, 2009, COMPUT AIDED GEOM D, V26, P665, DOI 10.1016/j.cagd.2008.09.007
   Lavoué G, 2005, COMPUT AIDED DESIGN, V37, P975, DOI 10.1016/j.cad.2004.09.001
   Leonardis A, 1997, IEEE T PATTERN ANAL, V19, P1289, DOI 10.1109/34.632988
   Levenberg K., 1944, Quarterly of Applied Mathematics, V2, P164, DOI [10.1090/QAM/10666, DOI 10.1090/QAM/10666]
   Li B, 2009, COMPUT GRAPH FORUM, V28, P1985, DOI 10.1111/j.1467-8659.2009.01577.x
   Lorensen William E., 1987, COMPUT GRAPH, P163, DOI DOI 10.1145/37402.37422
   Mangan AP, 1999, IEEE T VIS COMPUT GR, V5, P308, DOI 10.1109/2945.817348
   MARQUARDT DW, 1963, J SOC IND APPL MATH, V11, P431, DOI 10.1137/0111030
   Marshall D, 2001, IEEE T PATTERN ANAL, V23, P304, DOI 10.1109/34.910883
   Mitra NJ, 2004, INT J COMPUT GEOM AP, V14, P261, DOI 10.1142/S0218195904001470
   Ohtake Y, 2003, ACM T GRAPHIC, V22, P463, DOI 10.1145/882262.882293
   PASKO A, 1995, VISUAL COMPUT, V11, P429, DOI 10.1007/BF02464333
   Rvachev V. L., 1982, THEORY R FUNCTIONS S
   Schnabel R, 2007, COMPUT GRAPH FORUM, V26, P214, DOI 10.1111/j.1467-8659.2007.01016.x
   Shamir A, 2008, COMPUT GRAPH FORUM, V27, P1539, DOI 10.1111/j.1467-8659.2007.01103.x
   Shapiro V., 2007, Acta Numerica, V16, P239, DOI 10.1017/S096249290631001X
   SHAPIRO V, 1991, COMPUT AIDED DESIGN, V23, P4, DOI 10.1016/0010-4485(91)90077-A
   SHAPIRO V, 1993, ACM T GRAPHIC, V12, P35, DOI 10.1145/169728.169723
   SHAPIRO V, 1991, J MECH DESIGN, V113, P292, DOI 10.1115/1.2912782
   Sun XF, 2008, COMPUT AIDED GEOM D, V25, P437, DOI 10.1016/j.cagd.2007.12.008
   Sun XF, 2007, IEEE T VIS COMPUT GR, V13, P925, DOI 10.1109/TVCG.2007.1065
   Sun XF, 2009, GRAPH MODELS, V71, P34, DOI 10.1016/j.gmod.2008.12.002
   Tal A, 2007, COMM COM INF SC, V4, P44
   TAUBIN G, 1991, IEEE T PATTERN ANAL, V13, P1115, DOI 10.1109/34.103273
   Varady T, 1997, COMPUT AIDED DESIGN, V29, P255, DOI 10.1016/S0010-4485(96)00054-1
   Várady T, 2007, COMPUT AIDED DESIGN, V39, P379, DOI 10.1016/j.cad.2007.02.011
   Wolfram Research, 2008, MATHEMATICA, V7
   Wu JH, 2005, COMPUT GRAPH FORUM, V24, P277, DOI 10.1111/j.1467-8659.2005.00852.x
   Wu KN, 1997, IEEE T PATTERN ANAL, V19, P1223, DOI 10.1109/34.632982
   Wu KN, 1997, IMAGE VISION COMPUT, V15, P143, DOI 10.1016/S0262-8856(96)01124-9
   Yamauchi H, 2005, INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, PROCEEDINGS, P236, DOI 10.1109/SMI.2005.21
   Yan DM, 2006, LECT NOTES COMPUT SC, V4077, P73
NR 62
TC 9
Z9 9
U1 1
U2 24
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2013
VL 29
IS 5
BP 449
EP 465
DI 10.1007/s00371-012-0749-1
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 127RD
UT WOS:000317715200011
DA 2024-07-18
ER

PT J
AU Xiao, CX
   Gan, JJ
   Hu, XY
AF Xiao, Chunxia
   Gan, Jiajia
   Hu, Xiangyun
TI Fast level set image and video segmentation using new evolution
   indicator operators
SO VISUAL COMPUTER
LA English
DT Article
DE Level set; Segmentation; Gaussian mixture models; Filtering; Tracking
ID ACTIVE CONTOURS
AB We propose an effective level set evolution method for robust object segmentation in real images. We construct an effective region indicator and an multiscale edge indicator, and use these two indicators to adaptively guide the evolution of the level set function. The multiscale edge indicator is defined in the gradient domain of the multiscale feature-preserving filtered image. The region indicator is built on the similarity map between image pixels and user specified interest regions, where the similarity map is computed using Gaussian Mixture Models (GMM). Then we combine these two methods to develop a new mixing edge stop function, which makes the level set method more robust to initial active contour setting, and forces the level set to evolve adaptively based on the image content. Furthermore, we apply an acceleration approach to speed up our evolution process, which yields real time segmentation performance. Finally, we extend the proposed approach to video segmentation for achieving effective target tracking results. As the results show, our approach is effective for image and video segmentation and works well to accurately detect the complex object boundaries in real-time.
C1 [Xiao, Chunxia; Gan, Jiajia] Wuhan Univ, Sch Comp, Wuhan 430072, Peoples R China.
   [Hu, Xiangyun] Wuhan Univ, Sch Remote Sensing & Informat Engn, Wuhan 430072, Peoples R China.
C3 Wuhan University; Wuhan University
RP Xiao, CX (corresponding author), Wuhan Univ, Sch Comp, Wuhan 430072, Peoples R China.
EM cxxiao@whu.edu.cn; whdxgjj@whu.edu.cn; huxy@whu.edu.cn
RI Chen, Xiaojia/JYV-2395-2024
OI Hu, Xiang-Yun/0000-0003-3623-8304
FU National Basic Research Program of China [2012CB725303]; NSFC [60803081,
   61070081]; Open Project Program of the State Key Laboratory for Novel
   Software Technology [kfkt2010B05]; Open Project Program of the State Key
   Lab of CADCG [A1208]; Luojia Outstanding Young Scholar Program of Wuhan
   University
FX This work was partly supported by the National Basic Research Program of
   China (No. 2012CB725303), NSFC (No. 60803081, No. 61070081), Open
   Project Program of the State Key Laboratory for Novel Software
   Technology (kfkt2010B05), the Open Project Program of the State Key Lab
   of CAD&CG (Grant No. A1208), and Luojia Outstanding Young Scholar
   Program of Wuhan University.
CR ADALSTEINSSON D, 1995, J COMPUT PHYS, V118, P269, DOI 10.1006/jcph.1995.1098
   [Anonymous], VIS COMPUT
   [Anonymous], GEODESIC FRAMEWORK F
   Boykov YY, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P105, DOI 10.1109/ICCV.2001.937505
   Caselles V, 1997, INT J COMPUT VISION, V22, P61, DOI 10.1023/A:1007979827043
   CASELLES V, 1993, NUMER MATH, V66, P1, DOI 10.1007/BF01385685
   Chan TF, 2001, IEEE T IMAGE PROCESS, V10, P266, DOI 10.1109/83.902291
   Chen J, 2007, ACM T GRAPHIC, V26, DOI [10.1109/SARNOF.2007.4567317, 10.1145/1276377.1276506, 10.1145/1239451.1239554]
   Cremers D, 2007, INT J COMPUT VISION, V72, P195, DOI 10.1007/s11263-006-8711-1
   Criminisi A., 2008, ECCV 08 P 10 EUR C 1
   Duchenne O., 2008, IEEE COMPUTER VISION, P1, DOI DOI 10.1109/CVPR.2008.4587419
   Farbman Z, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360666
   Fattal R, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1239451.1239502, 10.1145/1276377.1276441]
   Fedkiw R., 2003, LEVEL SET METHODS DY
   KASS M, 1987, INT J COMPUT VISION, V1, P321, DOI 10.1007/BF00133570
   Lefohn AE, 2003, IEEE VISUALIZATION 2003, PROCEEDINGS, P75, DOI 10.1109/VISUAL.2003.1250357
   LI C, 2005, IEEE COMP SOC C COMP, P1063
   Li Y, 2004, ACM T GRAPHIC, V23, P303, DOI 10.1145/1015706.1015719
   OSHER S, 1988, J COMPUT PHYS, V79, P12, DOI 10.1016/0021-9991(88)90002-2
   Paragios N., 1999, International Conference on Computer Vision (ICCV), P688
   Peng DP, 1999, J COMPUT PHYS, V155, P410, DOI 10.1006/jcph.1999.6345
   Pock T., 2009, CONVEX RELAZATION AP
   Qu YG, 2006, ACM T GRAPHIC, V25, P1214, DOI 10.1145/1141911.1142017
   Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720
   Santner J, 2011, LECT NOTES COMPUT SC, V6492, P397, DOI 10.1007/978-3-642-19315-6_31
   Sethian J.A., 2000, Level Set Methods and Fast Marching Methods
   Shi YG, 2005, PROC CVPR IEEE, P34
   Sussman M, 1999, SIAM J SCI COMPUT, V20, P1165, DOI 10.1137/S1064827596298245
   Tomasi C, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P839, DOI 10.1109/ICCV.1998.710815
   Unger M., 2008, BMVC
   Vemuri B, 2003, GEOMETRIC LEVEL SET METHODS IN IMAGING, VISION AND GRAPHICS, P251, DOI 10.1007/0-387-21810-6_14
   Xia T, 2010, VISUAL COMPUT, V26, P157, DOI 10.1007/s00371-009-0359-8
   Xiao C., 2011, PACIFIC GRAPHICS
NR 33
TC 8
Z9 11
U1 0
U2 34
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2013
VL 29
IS 1
BP 27
EP 39
DI 10.1007/s00371-012-0672-5
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 064HB
UT WOS:000313063400003
DA 2024-07-18
ER

PT J
AU Li, Y
   Zhou, G
   Li, CP
   Qiu, XJ
   Wang, ZQ
AF Li, Yang
   Zhou, Guo
   Li, Chunpeng
   Qiu, Xianjie
   Wang, Zhaoqi
TI Adaptive mesh subdivision for efficient light baking
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Conference on the Computer Graphics International (CGI)
CY 2012
CL Bournemouth, ENGLAND
DE Light baking; Adaptive mesh subdivision; Lighting map; Real-time
   rendering
AB Light baking has long been a popular technique for real-time rendering. It usually precomputes and bakes the global lighting effects as vertex attributes or textures. Vertex baking requires less memory but can cause artifacts for large triangles. Texture baking can avoid this and generate a high-quality visual effect in real-time rendering. However, it requires significant memory consumption, which may limit the real-time performance and usage. To address this problem, we propose an adaptive mesh subdivision algorithm for memory-efficient light baking, including a fast triangle subdivision level determination method and an optimized solution to calculate vertex colors. Only the subdivided mesh is required during the real-time rendering. Therefore, memory requirements can be significantly reduced while keeping the visual effect. Besides, the subdivision level is allowed to be intuitively controlled by users with a specified parameter. Our algorithm can be easily implemented on commodity graphics hardware and integrated in existing real-time applications such as online preview systems.
C1 [Li, Yang; Zhou, Guo] Chinese Acad Sci, Inst Comp Technol, Virtual Real Lab, Beijing 100864, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Computing Technology, CAS
RP Li, Y (corresponding author), 6 Kexueyuan S Rd, Beijing 100190, Peoples R China.
EM liyang01@ict.ac.cn
RI Li, Chunpeng/AAE-6134-2019
CR Bosch C., 2008, COMP GRAPH INT C IST, P256
   Coombe G, 2004, PROC GRAPH INTERF, P161
   Greger G, 1998, IEEE COMPUT GRAPH, V18, P32, DOI 10.1109/38.656788
   Hu Y.H., 2008, GDC 08 GAM DEV C 200
   Huang FC, 2010, COMPUT GRAPH FORUM, V29, P1335, DOI 10.1111/j.1467-8659.2010.01729.x
   Kavan L, 2011, COMPUT GRAPH FORUM, V30, P1319, DOI 10.1111/j.1467-8659.2011.01991.x
   Kontkanen Janne, 2005, P 2005 S INT 3D GRAP, P41, DOI 10.1145/1053427.1053434
   Krivanek J., 2004, P 20 SPRING C COMP G, P106
   Larsson D, 2010, SIGGRAPH 10 ACM SIGG
   Ni T., 2009, SIGGRAPH 09 ACM SIGG
   Rasmusson J., 2010, P C HIGH PERF GRAPH, P143
   Schäfer H, 2012, COMPUT GRAPH-UK, V36, P193, DOI 10.1016/j.cag.2011.12.001
   Schafer H., 2012, Symposium on Interactive 3D Graphics and Games (I3D). ACM, P175
   Sloan PP, 2002, ACM T GRAPHIC, V21, P527, DOI 10.1145/566570.566612
   Vedel C., 2008, EUR WORKSH REND
   Wallner G, 2008, WSCG, V16
   Yuksel C, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1731047.1731053
NR 17
TC 0
Z9 0
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2012
VL 28
IS 6-8
BP 635
EP 645
DI 10.1007/s00371-012-0682-3
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 947EW
UT WOS:000304411500012
DA 2024-07-18
ER

PT J
AU Lin, WC
   Yan, ZC
AF Lin, Wen-Chieh
   Yan, Zhi-Cheng
TI Attention-based high dynamic range imaging
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Conference on the Computer Graphics International (CGI)
CY JUN 12-15, 2011
CL Ottawa, CANADA
DE High dynamic range imaging; Visual saliency; Attention and adaptation
ID TONE REPRODUCTION; MODEL
AB Many tone mapping algorithms have been proposed based on the studies in Human Visual System; however, they rarely addressed the effects of attention to contrast response. As attention plays an important role in human visual system, we proposed a local tone mapping method that respects both attention and adaptation effects. We adopt the High Dynamic Range (HDR) saliency map to compute an attention map, which predicts the attentive regions and nonattentative regions in an HDR image. The attention map is then used to locally adjust the contrast of the HDR image according to attention and adaptation models found in psychophysics. We applied our tone mapping approach to HDR images and videos and compared with the results generated by three state-of-the-art tone mapping algorithms. Our experiments show that our approach produces results with better image quality in terms of preserving details and chromaticity of visual saliency.
C1 [Lin, Wen-Chieh; Yan, Zhi-Cheng] Natl Chiao Tung Univ, Dept Comp Sci, Hsinchu, Taiwan.
   [Lin, Wen-Chieh] Natl Chiao Tung Univ, Inst Multimedia Engn, Hsinchu, Taiwan.
C3 National Yang Ming Chiao Tung University; National Yang Ming Chiao Tung
   University
RP Lin, WC (corresponding author), Natl Chiao Tung Univ, Dept Comp Sci, Hsinchu, Taiwan.
EM wclin@cs.nctu.edu.tw; peter12.cs97g@g2.nctu.edu.tw
CR ALBRECHT DG, 1982, J NEUROPHYSIOL, V48, P217, DOI 10.1152/jn.1982.48.1.217
   [Anonymous], 2005, High Dynamic Range Imaging: Acquisition, Display, and Image-Based Lighting (The Morgan Kaufmann Series in Computer Graphics
   [Anonymous], 1994, Graph. Gems, DOI DOI 10.1016/B978-0-12-336156-1.50054-9
   [Anonymous], 1996, P 23 ANN C COMP GRAP, DOI DOI 10.1145/237170.237262
   [Anonymous], 2002, PROC ACM T GRAPH SIG, DOI DOI 10.1145/566570.566574
   BREMOND R, 2010, MED RET WORKSH CONJ
   Cameron EL, 2002, VISION RES, V42, P949, DOI 10.1016/S0042-6989(02)00039-1
   Carrasco M, 2000, VISION RES, V40, P1203, DOI 10.1016/S0042-6989(00)00024-9
   CHEN HT, 2005, CVPR, V2, P369
   CHIU K, 1993, GRAPH INTER, P245
   Fattal R., 2002, ACM Transactions on Graphics, V21, P249, DOI 10.1145/566570.566573
   HENDERSON JM, 1992, CAN J PSYCHOL, V46, P319, DOI 10.1037/h0084325
   Hickey C, 2010, EXP BRAIN RES, V201, P789, DOI 10.1007/s00221-009-2094-9
   Hunt R.W. G., 2004, REPROD COLOUR, V6th
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Itti L, 2003, PROC SPIE, V5200, P64, DOI 10.1117/12.512618
   Larson GW, 1997, IEEE T VIS COMPUT GR, V3, P291, DOI 10.1109/2945.646233
   LIANG CK, 2010, EUROGRAPHICS, P253
   Ling S, 2006, NAT NEUROSCI, V9, P1243, DOI 10.1038/nn1761
   Lischinski D, 2006, ACM T GRAPHIC, V25, P646, DOI 10.1145/1141911.1141936
   Mantiuk R, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360667
   Martínez-Trujillo JC, 2002, NEURON, V35, P365, DOI 10.1016/S0896-6273(02)00778-X
   NAKA KI, 1966, J PHYSIOL-LONDON, V185, P587, DOI 10.1113/jphysiol.1966.sp008003
   Pattanaik SN, 2000, COMP GRAPH, P47, DOI 10.1145/344779.344810
   Pestilli F, 2007, J VISION, V7, DOI 10.1167/7.7.9
   Pestilli F, 2009, VISION RES, V49, P1144, DOI 10.1016/j.visres.2008.09.018
   REICHARDT W, 1987, J COMP PHYSIOL A, V161, P533, DOI 10.1007/BF00603660
   Reinhard E, 2002, ACM T GRAPHIC, V21, P267, DOI 10.1145/566570.566575
   Seetzen H, 2004, ACM T GRAPHIC, V23, P760, DOI 10.1145/1015706.1015797
   TREISMAN AM, 1980, COGNITIVE PSYCHOL, V12, P97, DOI 10.1016/0010-0285(80)90005-5
   TUMBLIN J, 1993, IEEE COMPUT GRAPH, V13, P42, DOI 10.1109/38.252554
   Tumblin J, 1999, COMP GRAPH, P83, DOI 10.1145/311535.311544
   Van Hateren JH, 2006, ACM T GRAPHIC, V25, P1380, DOI 10.1145/1183287.1183293
   Ward G., 2005, P 13 COL IM C
   Xu RF, 2005, IEEE COMPUT GRAPH, V25, P57, DOI 10.1109/MCG.2005.133
NR 35
TC 8
Z9 8
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2011
VL 27
IS 6-8
BP 717
EP 727
DI 10.1007/s00371-011-0578-7
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 766FY
UT WOS:000290767600030
DA 2024-07-18
ER

PT J
AU Shen, Y
   Ma, LZ
   Liu, H
AF Shen, Yang
   Ma, Lizhuang
   Liu, Hai
TI An MLS-based cartoon deformation
SO VISUAL COMPUTER
LA English
DT Article
DE Deformation; MLS; Cartoon
AB We present an image deformation method driven by skeleton; it is based on MLS deformation algorithm (Schaefer et al. in SIGGRAPH, vol. 25, pp. 533-540, 2006). We improve the MLS deformation by defining a new weight function based on skeleton. Being different from the weight function based on control points, our weight function has benefited from the shape information of undeformed object and keeps deformation local, therefore our method can achieve a realistic effect. In cartoon video, we propose a new method to track the skeleton in the video, to build new origin skeleton and new target skeleton on each frame, and to apply our image deformation method to each frame and maintain spatiotemporal consistency. Results demonstrate that our method is able to decrease the effect of squeeze and use less control points.
C1 [Shen, Yang; Ma, Lizhuang; Liu, Hai] Shanghai Jiao Tong Univ, Shanghai 200030, Peoples R China.
C3 Shanghai Jiao Tong University
RP Shen, Y (corresponding author), Shanghai Jiao Tong Univ, Shanghai 200030, Peoples R China.
EM sheny@sjtu.edu.cn
FU National Basic Research Program of China (Program 973) [2006CB303105];
   Fund for National High Technology Research and Development Program of
   China (Program 863) [2009AA01Z334]; Natural Science Foundation of China
   [60873136]
FX This work is supported by National Basic Research Program of China
   (Program 973) under Grant No. 2006CB303105, Fund for National High
   Technology Research and Development Program of China (Program 863 No.
   2009AA01Z334), Natural Science Foundation of China (No. 60873136). We
   would like to thank Scott Schaefer for providing us with the image of
   the wooden guy.
CR BOOKSTEIN FL, 1989, PAMI, V22, P567
   BROX T, 2004, ECCV, V12, P25
   Chen T, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618470
   CUNO A, 2008, 3D AS RIGID AS POSSI
   Igarashi T, 2005, ACM T GRAPHIC, V24, P1134, DOI 10.1145/1073204.1073323
   Joshi P, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239522
   LEE SY, 1995, COMPUT GRAPH, V29, P439
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Mohr A., 2003, P 2003 S INT 3D GRAP, P27, DOI DOI 10.1145/641480.641488
   Price B, 2006, VISUAL COMPUT, V22, P661, DOI 10.1007/s00371-006-0051-1
   RUSTAMOV R., 2009, COMPUTER GRAPHICS FO, V28
   SAND P, 2006, CVPR, V24, P2195
   Schaefer S, 2006, ACM T GRAPHIC, V25, P533, DOI 10.1145/1141911.1141920
   SCHIWIETZ T, 2007, FREE FORM IMAGE, P27
   Sederberg T. W., 1986, Computer Graphics, V20, P151, DOI 10.1145/15886.15903
   Weng YL, 2006, VISUAL COMPUT, V22, P653, DOI 10.1007/s00371-006-0054-y
   Weng YL, 2008, COMPUT GRAPH FORUM, V27, P1789, DOI 10.1111/j.1467-8659.2008.01324.x
   XIAO J, 2006, ECCV, V9, P211
   Xu K, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618464
   Yan HB, 2008, IEEE T VIS COMPUT GR, V14, P693, DOI 10.1109/TVCG.2008.28
   Zhang GX, 2009, COMPUT GRAPH FORUM, V28, P1897, DOI 10.1111/j.1467-8659.2009.01568.x
   ZHANG SH, 2009, TVCG, V99, P1077
NR 22
TC 3
Z9 5
U1 0
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2010
VL 26
IS 9
BP 1229
EP 1239
DI 10.1007/s00371-009-0404-7
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 635JD
UT WOS:000280650500008
DA 2024-07-18
ER

PT J
AU Chen, M
   Tang, K
AF Chen, Ming
   Tang, Kai
TI A fully geometric approach for developable cloth deformation simulation
SO VISUAL COMPUTER
LA English
DT Article
DE Developable surface; Cloth simulation; Collision; Mesh deformation
ID BEZIER; MESHES
AB We present a new method for simulation of inextensible cloth subjected to a conservative force (e.g., the gravity) and collision-free constraint. Traditional algorithms for cloth simulation are all physically-based in which cloth is treated as an elastic material with some stiffness coefficient(s). These algorithms break down ultimately if one tries to set this stiffness coefficient to infinite which corresponds to inextensible cloth. The crux of the method is an algorithm for interpolating a given set of arbitrary points or space curves by a smooth developable mesh surface. We formulate this interpolation problem as a mesh deformation process that transforms an initial developable mesh surface, e.g., a planar figure, to a final mesh surface that interpolates the given points (called anchor points). During the deformation process, all the triangle elements in the intermediate meshes are kept isometric to their initial shapes, while the potential energy due to the conservative force is reduced gradually. The collision problem is resolved by introducing dynamic anchor points owing to the collision during the deformation. Notwithstanding its simplicity, the proposed method has shown some promising efficacy for simulation of inextensible cloth.
C1 [Chen, Ming; Tang, Kai] Hong Kong Univ Sci & Technol, Dept Mech, Hong Kong, Hong Kong, Peoples R China.
C3 Hong Kong University of Science & Technology
RP Tang, K (corresponding author), Hong Kong Univ Sci & Technol, Dept Mech, Hong Kong, Hong Kong, Peoples R China.
EM mecm@ust.hk; mektang@ust.hk
RI Tang, Kai/ABA-9642-2021
OI Tang, Kai/0000-0002-5184-2086
CR AU OKC, 2007, THESIS HONG KONG U S
   Au OKC, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1239451.1239534, 10.1145/1276377.1276481]
   Aumann G, 2004, COMPUT AIDED GEOM D, V21, P661, DOI 10.1016/j.cagd.2004.04.007
   Aumann G, 2003, COMPUT AIDED GEOM D, V20, P601, DOI 10.1016/j.cagd.2003.07.001
   Baraff D., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P43, DOI 10.1145/280814.280821
   Bo P, 2007, COMPUT GRAPH FORUM, V26, P365, DOI 10.1111/j.1467-8659.2007.01059.x
   Breen D. E., 1994, Computer Graphics Proceedings. Annual Conference Series 1994. SIGGRAPH 94 Conference Proceedings, P365, DOI 10.1145/192161.192259
   Bridson R, 2002, ACM T GRAPHIC, V21, P594, DOI 10.1145/566570.566623
   CHOI K, 2002, COMPUTER GRAPHICS P, P604
   Choi KJ, 2005, COMPUT AIDED DESIGN, V37, P585, DOI 10.1016/j.cad.2004.11.002
   Chu CH, 2002, COMPUT AIDED DESIGN, V34, P511, DOI 10.1016/S0010-4485(01)00122-1
   Desbrun M, 1999, PROC GRAPH INTERF, P1
   Do Carmo M., 1976, Differential Geometry of Curves and Surfaces
   English E, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360665
   Frey W. H., 2002, International Journal of Foundations of Computer Science, V13, P285, DOI 10.1142/S0129054102001096
   Frey WH, 2004, COMPUT AIDED DESIGN, V36, P299, DOI 10.1016/S0010-4485(03)00105-2
   Goldenthal R, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239500
   Julius D, 2005, COMPUT GRAPH FORUM, V24, P581, DOI 10.1111/j.1467-8659.2005.00883.x
   Lang J., 1992, Computer-Aided Geometric Design, V9, P291, DOI 10.1016/0167-8396(92)90036-O
   Liu Y, 2006, ACM T GRAPHIC, V25, P681, DOI 10.1145/1141911.1141941
   Liu YJ, 2007, COMPUT AIDED DESIGN, V39, P719, DOI 10.1016/j.cad.2007.02.013
   Mitani J, 2004, ACM T GRAPHIC, V23, P259, DOI 10.1145/1015706.1015711
   POTTMANN H, 1995, COMPUT AIDED GEOM D, V12, P513, DOI 10.1016/0167-8396(94)00031-M
   PROVOT X, 1995, GRAPH INTER, P147
   Provot X, 1997, GRAPHICS INTERFACE, P177, DOI DOI 10.1007/978-3-7091-6874-5_13
   ROSE K, 2007, P 5 EUR S GEOM PROC, V257, P163
   Shatz I, 2006, VISUAL COMPUT, V22, P825, DOI 10.1007/s00371-006-0067-6
   Terzopoulos D., 1987, COMPUT GRAPH, P205, DOI DOI 10.1145/37402.37427
   Volino P, 2001, COMPUTER GRAPHICS INTERNATIONAL 2001, PROCEEDINGS, P265, DOI 10.1109/CGI.2001.934683
   Volino P, 1996, IEEE COMPUT GRAPH, V16, P42, DOI 10.1109/38.536274
   VOLINO P, 1995, P SIGGRAPH 95 COMPUT, V29, P137
   Volino P, 2007, VISUAL COMPUT, V23, P669, DOI 10.1007/s00371-007-0152-5
   Wallner J., 2001, MATH VISUAL
   Wang CCL, 2005, J COMPUT INF SCI ENG, V5, P291, DOI 10.1115/1.2052850
   Wang CCL, 2004, VISUAL COMPUT, V20, P521, DOI 10.1007/s00371-004-0256-0
NR 35
TC 19
Z9 38
U1 1
U2 11
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2010
VL 26
IS 6-8
BP 853
EP 863
DI 10.1007/s00371-010-0467-5
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 602JQ
UT WOS:000278135800045
DA 2024-07-18
ER

PT J
AU Huang, MC
   Liu, F
   Wu, EH
AF Huang, Mengcheng
   Liu, Fang
   Wu, Enhua
TI A GPU-based matting Laplacian solver for high resolution image matting
SO VISUAL COMPUTER
LA English
DT Article
DE Image matting; Matting Laplacian; Conjugate gradient solver; GPU
AB The recently proposed matting Laplacian (Levin et al., IEEE Trans. Pattern Anal. Mach. Intell. 30(2):228-242, 2008) has been proven to be a state-of-the-art method for solving the image matting problem. Using this method, matting is formulated as solving a high-order linear system which is hard-constrained by the input trimap. The main drawback of this method, however, is the high computational cost. As the size of the input image increases, the matting Laplacian becomes expensive to solve in terms of both memory and computational time.
   In this paper we propose a GPU-based matting Laplacian solution which is dramatically faster than a conventional CPU solution, and at the same time largely reduces the memory consumption, making this method practical for the first time for high resolution image matting. To achieve this end, we employ a novel hierarchical windowing scheme to approximate the global optimal solution by solving a serial of local regions at multiple scales. We further employ a GPU-based local solver which can efficiently evaluate local solutions under various boundary conditions. Experimental results show that our system in general is more than two orders of magnitude faster than traditional CPU-based solvers, with about 80% less memory footprint.
C1 [Huang, Mengcheng; Liu, Fang; Wu, Enhua] Chinese Acad Sci, State Key Lab Comp Sci, Inst Software, Beijing, Peoples R China.
   [Huang, Mengcheng; Liu, Fang] Chinese Acad Sci, Grad Univ, Beijing, Peoples R China.
   [Wu, Enhua] Univ Macau, Fac Sci & Technol, Dept Comp & Informat Sci, Macau, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Software, CAS; Chinese Academy
   of Sciences; University of Chinese Academy of Sciences, CAS; University
   of Macau
RP Huang, MC (corresponding author), Chinese Acad Sci, State Key Lab Comp Sci, Inst Software, Beijing, Peoples R China.
EM hmcen@ios.ac.cn; liuf@ios.ac.cn; ehwu@umac.mo
RI Wang, Fang/HPC-5174-2023; huang, mengcheng/KDO-3718-2024
FU National Basic Research Program of China (973 Program) [2009CB320802];
   National 863 High-Tech Project [2008AA01Z301]; National Science
   Foundation of China [60573155]; University of Macau
FX We would like to thank anonymous reviewers for their constructive
   comments and suggestions. This research is supported by National Basic
   Research Program of China (973 Program) (Grant No. 2009CB320802),
   National 863 High-Tech Project (Grant No. 2008AA01Z301), National
   Science Foundation of China (Grant No. 60573155) and University of Macau
   Research Grant.
CR Agarwala A, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1239451.1239545, 10.1145/1276377.1276495]
   [Anonymous], 2008, NVIDIA CUDA COMP UN
   [Anonymous], 2007, Optimizing parallel reductions in CUDA
   Levin A, 2008, IEEE T PATTERN ANAL, V30, P228, DOI 10.1109/TPAMI.2007.1177
   Rhemann C, 2009, PROC CVPR IEEE, P1826, DOI 10.1109/CVPRW.2009.5206503
   Saad Y, 2003, ITERATIVE METHODS SP, DOI DOI 10.1137/1.9780898718003
   Szeliski R, 2006, ACM T GRAPHIC, V25, P1135, DOI 10.1145/1141911.1142005
   WANG J, P IEEE CVPR, P1
   Wang J, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239460
   Wang J, 2007, FOUND TRENDS COMPUT, V3, P97, DOI 10.1561/0600000019
NR 10
TC 9
Z9 12
U1 0
U2 4
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2010
VL 26
IS 6-8
BP 943
EP 950
DI 10.1007/s00371-010-0435-0
PG 8
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 602JQ
UT WOS:000278135800054
DA 2024-07-18
ER

PT J
AU Li, Z
   Levin, D
   Deng, ZJ
   Liu, DY
   Luo, XN
AF Li, Zheng
   Levin, David
   Deng, Zhengjie
   Liu, Dingyuan
   Luo, Xiaonan
TI Cage-free local deformations using green coordinates
SO VISUAL COMPUTER
LA English
DT Article
DE Shape deformations; Green coordinates; Cage; Umbrella shaped cell;
   Handles
ID SPACE
AB Green coordinates require to build an enclosing cage around a shape that is deformed by manipulating the cage. In this paper, we build upon the local differences of Green coordinates and propose a method to deform a shape locally without constructing a cage. Second, we derive linear formulas to directly interpolate points on the shape to desired locations using an umbrella shaped cell. Moreover, we set up an influence region to confine the deformation in a local area. Finally, we suggest a method to automatically construct an umbrella shaped cell and thus update it consecutively while the shape is deforming. Experiments have shown that the suggested approach delivers smooth local deformations and enables shape manipulations at an interactive rate.
C1 [Li, Zheng; Deng, Zhengjie; Liu, Dingyuan; Luo, Xiaonan] Sun Yat Sen Univ, Guangzhou 510006, Guangdong, Peoples R China.
   [Levin, David] Tel Aviv Univ, IL-69978 Tel Aviv, Israel.
C3 Sun Yat Sen University; Tel Aviv University
RP Li, Z (corresponding author), Sun Yat Sen Univ, Guangzhou 510006, Guangdong, Peoples R China.
EM lee_chung_2002@yahoo.com.cn; levin@tau.ac.il; dzhengj@mail2.sysu.edu.cn
RI Deng, Zhengjie/JCO-9393-2023
OI Deng, Zhengjie/0000-0002-1936-0714
CR Allen B, 2003, ACM T GRAPHIC, V22, P587, DOI 10.1145/882262.882311
   [Anonymous], 2007, S GEOMETRY PROCESSIN
   [Anonymous], 2004, P 2004 EUR ACM SIGGR
   Au OKC, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360643
   Ben-Chen M, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531340
   BENCHEN M, 2009, SCA 09, P67
   Botsch M, 2008, IEEE T VIS COMPUT GR, V14, P213, DOI 10.1109/TVCG.2007.1054
   Botsch M, 2007, COMPUT GRAPH FORUM, V26, P339, DOI 10.1111/j.1467-8659.2007.01056.x
   Joshi P, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239522
   Ju T, 2005, ACM T GRAPHIC, V24, P561, DOI 10.1145/1073204.1073229
   LEVIN D, 2008, DERIVATION ANAL GREE
   Lipman Y, 2004, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, P181, DOI 10.1109/SMI.2004.1314505
   Lipman Y, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360677
   Schneider P., 2002, Geometric Tools for Computer Graphics
   Tagliasacchi Andrea, 2009, ACM T GRAPHICS, V28, P3
   Toledo Sivan., 2003, Taucs: A library of sparse linear solvers
   von Funck W, 2006, ACM T GRAPHIC, V25, P1118, DOI 10.1145/1141911.1142002
   Weber O, 2007, COMPUT GRAPH FORUM, V26, P265, DOI 10.1111/j.1467-8659.2007.01048.x
   Weber O, 2009, COMPUT GRAPH FORUM, V28, P587, DOI 10.1111/j.1467-8659.2009.01399.x
   Yu YZ, 2004, ACM T GRAPHIC, V23, P644, DOI 10.1145/1015706.1015774
NR 20
TC 12
Z9 13
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2010
VL 26
IS 6-8
BP 1027
EP 1036
DI 10.1007/s00371-010-0438-x
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 602JQ
UT WOS:000278135800062
DA 2024-07-18
ER

PT J
AU Pintus, R
   Gobbetti, E
   Cignoni, P
   Scopigno, R
AF Pintus, R.
   Gobbetti, E.
   Cignoni, P.
   Scopigno, R.
TI Shape enhancement for rapid prototyping
SO VISUAL COMPUTER
LA English
DT Article
DE Shape enhancement; Rapid prototyping; Unsharp-masking
AB Many applications, for instance, in the reverse engineering and cultural heritage field, require building a physical replica of 3D digital models. Recent 3D printers can easily perform this task in a relatively short time and using color to reproduce object textures. However, the finite resolution of printers and, most of all, some peculiar optical and physical properties of the used materials reduce their perceptual quality. The contribution of this paper is a shape enhancing technique which allows users to increase readability of the tiniest details in physical replicas, without requiring manual post-reproduction interventions.
C1 [Pintus, R.; Gobbetti, E.] CRS4, ViC, I-09010 Pula, CA, Italy.
   [Cignoni, P.; Scopigno, R.] CNR, ISTI, I-56124 Pisa, Italy.
C3 Consiglio Nazionale delle Ricerche (CNR); Istituto di Scienza e
   Tecnologie dell'Informazione "Alessandro Faedo" (ISTI-CNR)
RP Pintus, R (corresponding author), CRS4, ViC, Loc Pixina Manna Edificio 1, I-09010 Pula, CA, Italy.
EM ruggero.pintus@gmail.com
RI Cignoni, Paolo/B-7192-2012; Gobbetti, Enrico/O-2188-2015; scopigno,
   roberto/AAH-7645-2020; Pintus, Ruggero/AAX-5719-2020
OI Cignoni, Paolo/0000-0002-2686-8567; Gobbetti,
   Enrico/0000-0003-0831-2458; Pintus, Ruggero/0000-0003-1786-7068
FU Regione Autonoma della Sardegna [2008-2010]; EG 7FP IP '3D-COFORM'
   project [2008-2012, 231809]; Regione Toscana initiative 'START'
FX This work has been partially funded by Master & Back grant (2008-2010)
   of Regione Autonoma della Sardegna. The research leading to these
   results has also received funding from the EG 7FP IP '3D-COFORM' project
   (2008-2012, no. 231809) and from the Regione Toscana initiative 'START'.
CR CAI W, 1996, SPIE C SERIES, V2644, P267
   CHEN LS, 1985, IEEE COMPUT GRAPH, V5, P33, DOI 10.1109/MCG.1985.276275
   Cignoni P., 2008, 9 INT S VIRT REAL AR
   Decenciere Etienne, 2007, Image Analysis & Stereology, V26, P73, DOI 10.5566/ias.v26.p73-81
   Ebert D, 2000, IEEE VISUAL, P195, DOI 10.1109/VISUAL.2000.885694
   Guskov I, 1999, COMP GRAPH, P325, DOI 10.1145/311535.311577
   Ma RH, 2004, IEEE IMAGE PROC, P1661
   MARCHESIN S, 2007, IEEE EG INT S VOL GR
   Steiner A, 1998, GRAPH MODEL IM PROC, V60, P112, DOI 10.1006/gmip.1998.0461
   Tao YB, 2009, VISUAL COMPUT, V25, P581, DOI 10.1007/s00371-009-0328-2
   Taubin G., 2000, EUROGRAPHICS STATE A
   Westermann R, 1998, SIGGRAPH 98, P169, DOI DOI 10.1145/280814.280860
   Yagou H., 2003, J 3 DIMENSIONAL IMAG, V17, P170
   *Z CORP, 3D PRINT
NR 14
TC 13
Z9 13
U1 0
U2 9
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2010
VL 26
IS 6-8
BP 831
EP 840
DI 10.1007/s00371-010-0488-0
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 602JQ
UT WOS:000278135800043
DA 2024-07-18
ER

PT J
AU Zhang, X
   Chen, W
   Fang, J
   Wang, R
   Peng, QS
AF Zhang, Xin
   Chen, Wei
   Fang, Jing
   Wang, Rui
   Peng, Qunsheng
TI Perceptually-motivated shape exaggeration
SO VISUAL COMPUTER
LA English
DT Article
DE Shape exaggeration; Weighted least square; Adaptive exaggeration
   function
ID PERCEPTION; ILLUMINATION; CURVATURE; LINES
AB Image communication would appear more efficient if the visual cases of the concerned parts can be enhanced. One way to achieve this is by local shape exaggeration in rendering. In this paper, we present an interactive scheme for controllable local shape exaggeration. Our approach achieves local, direct, and consistent appearance enhancement by modifying the surface orientation in an intuitive and globally optimized manner with sparse user-specified constraints. Compared with previous approaches, the main contribution of this paper is the introduction of adaptive exaggeration function (AEF), which is capable of modulating the extent of detail enhancement to obtain a satisfactory shape exaggeration result. The AEF model is derived based on a series of experiments. We complement our new approach with a variety of examples, user studies, and provide comparisons with recent approaches.
C1 [Zhang, Xin; Chen, Wei; Fang, Jing; Wang, Rui; Peng, Qunsheng] Zhejiang Univ, State Key Lab CAD&CG, Hangzhou 310027, Zhejiang, Peoples R China.
C3 Zhejiang University
RP Wang, R (corresponding author), Zhejiang Univ, State Key Lab CAD&CG, Hangzhou 310027, Zhejiang, Peoples R China.
EM rwang@cad.zju.edu.cn
FU National Grand Foundation [2009CB320802]; National Natural Science
   Foundation of China [60970020, 60873123]
FX This research is supported by National Grand Foundation Research 973
   Program of China under Grant No. 2009CB320802, National Natural Science
   Foundation of China Under Grant No. 60970020 and No. 60873123. We would
   like to thank Mr. Pengfei Xu and Mr. Naiyang Lin for their help.
CR [Anonymous], P 4 EUR S GEOM PROC
   [Anonymous], ACM T GRAPH
   BUATOIS L, 2007, HIGH PERF COMP C HPC
   Cignoni P, 2005, COMPUT GRAPH-UK, V29, P125, DOI 10.1016/j.cag.2004.11.012
   Cole F., 2006, EUROGRAPHICS S RENDE, P377
   DeCarlo D, 2003, ACM T GRAPHIC, V22, P848, DOI 10.1145/882262.882354
   DeCarlo D, 2002, ACM T GRAPHIC, V21, P769, DOI 10.1145/566570.566650
   DECARLO D, 2007, P NPAR, P63
   DeCarlo Doug., 2004, P INT S NONPHOTOREAL, P15, DOI DOI 10.1145/987657.987661
   EBERT D, 2006, P ACM SIGGRAPH COURS
   Eigensatz M, 2008, COMPUT GRAPH FORUM, V27, P241, DOI 10.1111/j.1467-8659.2008.01121.x
   Farbman Z, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360666
   Fleishman S, 2003, ACM T GRAPHIC, V22, P950, DOI 10.1145/882262.882368
   Fleming RW, 2004, J VISION, V4, P798, DOI 10.1167/4.9.10
   Fleming RW, 2003, J VISION, V3, P347, DOI 10.1167/3.5.3
   GOOCH A, 1998, P SIGGRAPH 98, V31, P447
   Gooch B., 1999, Proceedings 1999 Symposium on Interactive 3D Graphics, P31, DOI 10.1145/300523.300526
   Gooch B., 2001, Non-photorealistic rendering
   Guskov I, 1999, COMP GRAPH, P325, DOI 10.1145/311535.311577
   Hertzmann A, 2000, COMP GRAPH, P517, DOI 10.1145/344779.345074
   IHRKE M, 2009, 72400R72400R12 SPIE
   INTERRANTE V, 1995, IEEE VISUALIZATION, P52
   Judd T, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239470
   Kalnins RD, 2002, ACM T GRAPHIC, V21, P755, DOI 10.1145/566570.566648
   Kim B, 2005, COMPUT GRAPH FORUM, V24, P295, DOI 10.1111/j.1467-8659.2005.00854.x
   Kim Y, 2008, IEEE T VIS COMPUT GR, V14, P772, DOI 10.1109/TVCG.2007.70624
   Koenderink J.J., 1984, PERCEPTION, V13, p321 330
   Kolomenkin M, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409110
   LAGENDIJK RL, 1988, IEEE T ACOUST SPEECH, V36, P1874, DOI 10.1109/29.9032
   Lee BH, 2007, AGRO FOOD IND HI TEC, V18, P18
   Lee CH, 2006, IEEE T VIS COMPUT GR, V12, P197, DOI 10.1109/TVCG.2006.30
   Levin A, 2004, ACM T GRAPHIC, V23, P689, DOI 10.1145/1015706.1015780
   Levin A, 2008, IEEE T PATTERN ANAL, V30, P228, DOI 10.1109/TPAMI.2007.1177
   Lischinski D, 2006, ACM T GRAPHIC, V25, P646, DOI 10.1145/1141911.1141936
   LIU R, 2007, P NPAR, P111
   Ng R, 2003, ACM T GRAPHIC, V22, P376, DOI 10.1145/882262.882280
   Ohtake Y, 2004, ACM T GRAPHIC, V23, P609, DOI 10.1145/1015706.1015768
   OSHEA JP, 2008, P 5 S APPL PERC GRAP, P135
   Pérez P, 2003, ACM T GRAPHIC, V22, P313, DOI 10.1145/882262.882269
   Ramanarayanan G, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1276377.1276472, 10.1145/1239451.1239527]
   Ramanarayanan G, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360659
   Rusinkiewicz S, 2006, ACM T GRAPHIC, V25, P1199, DOI 10.1145/1141911.1142015
   Strothotte T, 2002, NONPHOTOREALISTIC CO
   Todd JT, 2004, PSYCHOL SCI, V15, P33, DOI 10.1111/j.0963-7214.2004.01501006.x
   TODD JT, 1983, J EXP PSYCHOL HUMAN, V9, P583, DOI 10.1037/0096-1523.9.4.583
   Todo H, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1276377.1276399, 10.1145/1239451.1239468]
   TOLERFRANKLIN C, 2007, COMPUT GRAPH FORUM, V26, P385
   van der Maaten Laurens., 2006, Proceedings of Computer Applications and Quantitative Methods in Archaeology, P112
   Vergne R, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531331
   Vergne Romain, 2010, I3D 10
   Xie X, 2007, IEEE T VIS COMPUT GR, V13, P1328, DOI 10.1109/TVCG.2007.70538
NR 51
TC 6
Z9 8
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2010
VL 26
IS 6-8
BP 985
EP 995
DI 10.1007/s00371-010-0431-4
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 602JQ
UT WOS:000278135800058
DA 2024-07-18
ER

PT J
AU Liarokapis, F
   Macan, L
   Malone, G
   Rebolledo-Mendez, G
   de Freitas, S
AF Liarokapis, Fotis
   Macan, Louis
   Malone, Gary
   Rebolledo-Mendez, Genaro
   de Freitas, Sara
TI Multimodal augmented reality tangible gaming
SO VISUAL COMPUTER
LA English
DT Article
DE Serious games; Pervasive computing; Augmented reality; Multimodal
   interfaces
AB This paper presents tangible augmented reality gaming environment that can be used to enhance entertainment using a multimodal tracking interface. Players can interact using different combinations between a pinch glove, a Wiimote, a six-degrees-of-freedom tracker, through tangible ways as well as through I/O controls. Two tabletop augmented reality games have been designed and implemented including a racing game and a pile game. The goal of the augmented reality racing game is to start the car and move around the track without colliding with either the wall or the objects that exist in the gaming arena. Initial evaluation results showed that multimodal-based interaction games can be beneficial in gaming. Based on these results, an augmented reality pile game was implemented with goal of completing a circuit of pipes (from a starting point to an end point on a grid). Initial evaluation showed that tangible interaction is preferred to keyboard interaction and that tangible games are much more enjoyable.
C1 [Liarokapis, Fotis; Macan, Louis; Malone, Gary] Coventry Univ, Interact Worlds Appl Res Grp, Coventry, W Midlands, England.
   [Rebolledo-Mendez, Genaro; de Freitas, Sara] Coventry Univ, Serious Games Inst, Coventry, W Midlands, England.
C3 Coventry University; Coventry University
RP Liarokapis, F (corresponding author), Coventry Univ, Interact Worlds Appl Res Grp, Coventry, W Midlands, England.
EM F.Liarokapis@coventry.ac.uk; macanl@coventry.ac.uk;
   maloneg@coventry.ac.uk; GRebolledo-Mendez@coventry.ac.uk;
   s.defreitas@coventry.ac.uk
RI Liarokapis, Fotis/B-7464-2014; Liarokapis, Fotis/AAQ-9498-2021;
   Liarokapis, Fotis/AAD-4444-2019; Rebolledo-Mendez, Genaro/D-7842-2019
OI Liarokapis, Fotis/0000-0003-3617-2261; Liarokapis,
   Fotis/0000-0003-3617-2261; Liarokapis, Fotis/0000-0003-3617-2261;
   Rebolledo-Mendez, Genaro/0000-0002-3214-0935
FU Interactive Worlds Applied Research Group (iWARG); Cogent Computing
   Applied Research Centre (Cogent)
FX The authors would like to thank 'Interactive Worlds Applied Research
   Group (iWARG)' as well as 'Cogent Computing Applied Research Centre
   (Cogent)' for their support and inspiration. Videos of the AR racing
   game and pile game can be found at:
   http://www.youtube.com/watch?v=k3r181_GW-o and
   http://www.youtube.com/watch?v=0xPIpinN4r8 respectively.
CR [Anonymous], 3RD NARRATIVE INTERA
   Beigl M., 2004, P 1 INT WORKSH NETW, V4, P153
   Benford S, 2005, COMMUN ACM, V48, P54, DOI 10.1145/1047671.1047704
   Billinghurst M, 2001, COMPUT GRAPH-UK, V25, P745, DOI 10.1016/S0097-8493(01)00117-0
   Butz A., 1999, Proceedings 2nd IEEE and ACM International Workshop on Augmented Reality (IWAR'99), P35, DOI 10.1109/IWAR.1999.803804
   CHEOK AD, 2003, P 2 WORKSH NETW SYST, P106
   Craig S., 2004, J. Educ. Media, V29, P241, DOI [10.1080/135816, DOI 10.1080/1358165042000283101, 10.1080/1358165042000283101]
   Csikszentmihalyi M., 2008, FLOW PSYCHOL OPTIMAL
   DEFREITAS S, 2008, SERIOUS GAMES I COVE
   Dix A., 2004, Human-computer interaction
   Dombroviak KM, 2007, APPLIED COMPUTING 2007, VOL 1 AND 2, P1609, DOI 10.1145/1244002.1244345
   *EMP INT ENT, PIP 2008 NEW 2 CONS
   Goldsmith D, 2008, IEEE INT CONF INF VI, P539, DOI 10.1109/IV.2008.72
   HENRYSSON A, 2006, INT C COMP GRAPH INT
   HUGHES CE, 2005, IEEE COMPUT GRA 1124
   KORRIS J, 2004, 24 ARM SCI C FLOR OR
   Laurel B., 1997, Software agents, P67
   Lester JC, 2000, EMBODIED CONVERSATIONAL AGENTS, P123
   Lester JC, 1997, FR ART INT, V39, P23
   Magerkurth C., 2004, COMPUT ENTERTAIN, V2, P12, DOI [DOI 10.1145/1037851.1037870, 10.1145/1037851.1037870]
   Magerkurth C., 2006, Proceedings of the 2006 ACM SIGCHI international conference on Advances in computer entertainment technology, P15
   Malone Thomas W, 2021, Aptitude, learning, and instruction, P223, DOI DOI 10.1016/S0037-6337(09)70509-1
   Oda O., 2008, Proceedings of the 2nd international conference on INtelligent TEchnologies for interactive enterTAINment, P2
   Reitmayr G., 2005, P WORKSH SEM VIRT EN
   Rekimoto J., 1995, P ACM S USER INTERFA, P29, DOI [10.1145/215585.215639, DOI 10.1145/215585.215639]
   Sandor C, 2005, PERS UBIQUIT COMPUT, V9, P169, DOI 10.1007/S00779-004-0328-1
   Stapleton CB, 2003, SECOND IEEE AND ACM INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY, PROCEEDINGS, P354, DOI 10.1109/ISMAR.2003.1240757
   Starner T, 1997, PRESENCE-VIRTUAL AUG, V6, P386, DOI 10.1162/pres.1997.6.4.386
   TUDGE JRH, 1992, CHILD DEV, V63, P1364, DOI 10.2307/1131562
   WALTHER BK, 2005, P 2005 ACM SIGCHI IN, P176
   YOON SY, 2000, 4 INT C AUT AG BARC
NR 31
TC 10
Z9 17
U1 3
U2 27
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2009
VL 25
IS 12
BP 1109
EP 1120
DI 10.1007/s00371-009-0388-3
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 510LC
UT WOS:000271089000007
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Seong, JK
   Jeong, WK
   Cohen, E
AF Seong, Joon-Kyung
   Jeong, Won-Ki
   Cohen, Elaine
TI Curvature-based anisotropic geodesic distance computation for parametric
   and implicit surfaces
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT IEEE International Conference on Shape Modeling and Applications
CY JUN 04-06, 2008
CL Stony Brook, NY
SP IEEE Comp Soc, VGTC, ACM SIGGRAPH, EUROGRAPHICS, Comp Graph Soc
DE Anisotropy; Parametric and implicit surface; Geodesic; Normal curvature;
   Tensor; H-J equation
ID HAMILTON-JACOBI EQUATIONS; SEGMENTATION
AB Distribution of geometric features varies with direction, including, for example, normal curvature. In this paper, this characteristic of shape is used to define a new anisotropic geodesic (AG) distance for both parametric and implicit surfaces. Local distance (LD) from a point is defined as a function of both the point and a unit tangent plane directions, and a total distance is defined as an integral of that local distance. The AG distance between points on the surface is the minimum total distance between them. The path between the points that attains the minimum is called the anisotropic geodesic path. Minimization of total distance to attain the AG distance is performed by associating the LD function with a tensor speed function that controls wave propagation in the convex Hamilton-Jacobi (H-J) equation solver. We present new distance metrics for both parametric and implicit surfaces based on the curvature tensor. In order to solve for the implicit AG, a bounded 3D H-J equation solver was developed. We present a second metric for the AG distance, a difference curvature tensor, for parametric surfaces. Some properties of both new AG distances are presented, including parameterization invariance. This AG path differs from the usual geodesic in that minimal path, i.e., lowest cost path, roughly speaking, minimizes an integral of curvature along the curve. Then, the effectiveness of the proposed AG distances as shape discriminators is demonstrated in several applications, including surface segmentation and partial shape matching.
C1 [Seong, Joon-Kyung; Jeong, Won-Ki; Cohen, Elaine] Univ Utah, Sch Comp, Salt Lake City, UT 84112 USA.
C3 Utah System of Higher Education; University of Utah
RP Seong, JK (corresponding author), Univ Utah, Sch Comp, Salt Lake City, UT 84112 USA.
EM seong@cs.utah.edu; wkjeong@cs.utah.edu; cohen@cs.utah.edu
RI Jeong, Won-Ki/F-8171-2011
CR Alexander DC, 2000, COMPUT VIS IMAGE UND, V77, P233, DOI 10.1006/cviu.1999.0817
   [Anonymous], P 4 EUR S GEOM PROC
   [Anonymous], 2001, P 2001 S INTERACTIVE
   [Anonymous], SSD 99
   Belongie S, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P454, DOI 10.1109/ICCV.2001.937552
   Belyaev AG, 1998, COMPUTER GRAPHICS INTERNATIONAL, PROCEEDINGS, P530, DOI 10.1109/CGI.1998.694306
   Bornemann F, 2006, COMPUT VIS SCI, V9, P57, DOI 10.1007/s00791-006-0016-y
   BRONSTEIN A.M., 2007, PARALLEL ALGORITHMS
   Bronstein AM, 2007, J COMPUT PHYS, V225, P771, DOI 10.1016/j.jcp.2007.01.009
   Chen DY, 2003, COMPUT GRAPH FORUM, V22, P223, DOI 10.1111/1467-8659.00669
   CHEN J, 1990, INT J COMPUT GEOM AP, V6, P127
   CHENG SW, 2007, SODA 07, P766
   Cohen-Steiner D, 2004, ACM T GRAPHIC, V23, P905, DOI 10.1145/1015706.1015817
   DeCarlo D, 2003, ACM T GRAPHIC, V22, P848, DOI 10.1145/882262.882354
   Du Q, 2005, SIAM J SCI COMPUT, V26, P737, DOI 10.1137/S1064827503428527
   Elad A, 2003, IEEE T PATTERN ANAL, V25, P1285, DOI 10.1109/TPAMI.2003.1233902
   Funkhouser T, 2004, ACM T GRAPHIC, V23, P652, DOI 10.1145/1015706.1015775
   Gal R, 2006, ACM T GRAPHIC, V25, P130, DOI 10.1145/1122501.1122507
   Guillaume L, 2004, COMPUTER GRAPHICS INTERNATIONAL, PROCEEDINGS, P10, DOI 10.1109/CGI.2004.1309187
   Hilaga M, 2001, COMP GRAPH, P203, DOI 10.1145/383259.383282
   Hofer M, 2004, ACM T GRAPHIC, V23, P284, DOI 10.1145/1015706.1015716
   IYANAGA S, 1980, FINSLER SPACES
   Jeong WK, 2007, IEEE T VIS COMPUT GR, V13, P1480, DOI 10.1109/TVCG.2007.70571
   Johnson AE, 1999, IEEE T PATTERN ANAL, V21, P433, DOI 10.1109/34.765655
   KANAI T, 2000, GEOMETRIC MODELING P, P241
   Katz S, 2003, ACM T GRAPHIC, V22, P954, DOI 10.1145/882262.882369
   KAZHDAN M, 2004, SIGGRAPH 04, P623
   Kimmel R, 1998, P NATL ACAD SCI USA, V95, P8431, DOI 10.1073/pnas.95.15.8431
   Kindlmann G, 2003, IEEE VISUALIZATION 2003, PROCEEDINGS, P513, DOI 10.1109/VISUAL.2003.1250414
   Lai Y.K., 2006, PROC ACM S SOLID PHY, P17
   Mangan AP, 1999, IEEE T VIS COMPUT GR, V5, P308, DOI 10.1109/2945.817348
   Mémoli F, 2001, J COMPUT PHYS, V173, P730, DOI 10.1006/jcph.2001.6910
   Meyer M, 2007, IEEE T VIS COMPUT GR, V13, P1704, DOI 10.1109/TVCG.2007.70604
   MITCHELL JSB, 1987, SIAM J COMPUT, V16, P647, DOI 10.1137/0216045
   Page DL, 2003, PROC CVPR IEEE, P27
   Peyré G, 2006, INT J COMPUT VISION, V69, P145, DOI 10.1007/s11263-006-6859-3
   Pichon E, 2005, LECT NOTES COMPUT SC, V3749, P180
   Pottmann H, 2004, LECT NOTES COMPUTER, V3024, P18
   PRADOS E, 2006, CVPR 06, P1076
   Razdan A, 2003, COMPUT AIDED DESIGN, V35, P783, DOI 10.1016/S0010-4485(02)00101-X
   Reif JH, 2004, ALGORITHMICA, V39, P127, DOI 10.1007/s00453-003-1079-5
   SELLEN J, 1995, IEEE INT CONF ROBOT, P1970, DOI 10.1109/ROBOT.1995.525552
   Sethian JA, 1999, GEOPHYSICS, V64, P516, DOI 10.1190/1.1444558
   Sethian JA, 1996, P NATL ACAD SCI USA, V93, P1591, DOI 10.1073/pnas.93.4.1591
   Sethian JA, 2003, SIAM J NUMER ANAL, V41, P325, DOI 10.1137/S0036142901392742
   Sifri O., 2003, IMR, P189
   SKIENA SS, 2000, ALGORITHM DESIGN MAN
   Surazhsky V, 2005, ACM T GRAPHIC, V24, P553, DOI 10.1145/1073204.1073228
   Tsai YHR, 2003, SIAM J NUMER ANAL, V41, P673, DOI 10.1137/S0036142901396533
   Watanabe K, 2001, COMPUT GRAPH FORUM, V20, pC385, DOI 10.1111/1467-8659.00531
   Yamauchi H, 2005, VISUAL COMPUT, V21, P659, DOI 10.1007/s00371-005-0319-x
   Zhang Y, 2002, 2002 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL III, PROCEEDINGS, P273
   ZHOU K, 2004, SGP 04, P45
NR 53
TC 6
Z9 6
U1 2
U2 12
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2009
VL 25
IS 8
BP 743
EP 755
DI 10.1007/s00371-009-0362-0
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 465NI
UT WOS:000267593700003
DA 2024-07-18
ER

PT J
AU Jeong, K
   Kim, D
   Park, SY
   Lee, S
AF Jeong, Kyuman
   Kim, Dongyeon
   Park, Soon-Yong
   Lee, Seungyong
TI Digital shallow depth-of-field adapter for photographs
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 7th Korea-Israel Bi-National Conference on Geometric Modeling and
   Computer Graphics
CY JAN 29-30, 2007
CL Seoul, SOUTH KOREA
DE depth-of-field control; depth estimation; depth from defocus; digital
   refocusing; image detail control
ID EXTENDED DEPTH; DEFOCUS
AB This paper proposes a novel method to synthesize shallow depth-of-field images from two input photographs taken with different aperture values. The basic approach is to estimate the depth map of a given scene using a DFD (depth-from-defocus) algorithm and blur an input image according to the estimated depth map. The depth information estimated by DFD contains much noise and error, while the estimation is rather accurate along the edges of the image. To overcome the limitation, we propose a depth map filling algorithm using a set of initial depth maps and a segmented image. After depth map filling, the depth map can be fine tuned by applying segment clustering and user interaction. Since our method blurs an input image according to the estimated depth information, it generates physically plausible result images with shallow depth-of-field. In addition to depth-of-field control, the proposed method can be utilized for digital refocusing and detail control in image stylization.
C1 [Kim, Dongyeon; Lee, Seungyong] POSTECH, Dept Comp Sci & Engn, Pohang 790784, South Korea.
   [Jeong, Kyuman] Samsung Elect Co Ltd, Div Mobile Commun, Suwon 443742, South Korea.
   [Park, Soon-Yong] Kyungpook Natl Univ, Dept Comp Engn, Taegu 702701, South Korea.
C3 Pohang University of Science & Technology (POSTECH); Samsung; Samsung
   Electronics; Kyungpook National University
RP Lee, S (corresponding author), POSTECH, Dept Comp Sci & Engn, Pohang 790784, South Korea.
EM kyuman.jeong@samsung.com; spiff@postech.ac.kr; sypark@knu.ac.kr;
   leesy@postech.ac.kr
RI Park, Soon-Yong/HGV-2374-2022
CR Agarwala A, 2004, ACM T GRAPHIC, V23, P294, DOI 10.1145/1015706.1015718
   CHO S, 2006, P SOC PHOTO-OPT INS, V6055, P462
   Christoudias CM, 2002, INT C PATT RECOG, P150, DOI 10.1109/ICPR.2002.1047421
   Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236
   COSGROVE R, 2003, DIGIT PHOTOGR BUYER, V89
   Forster B, 2004, 2004 2ND IEEE INTERNATIONAL SYMPOSIUM ON BIOMEDICAL IMAGING: MACRO TO NANO, VOLS 1 and 2, P660
   Forster B, 2004, MICROSC RES TECHNIQ, V65, P33, DOI 10.1002/jemt.20092
   LACEY R, CREATING DEPTH FIELD
   Lin JL, 2004, PATTERN RECOGN LETT, V25, P407, DOI 10.1016/j.patrec.2003.11.003
   London B., 2004, PHOTOGRAPHY
   MERKLINGER HM, INS OUTS FOCUS
   Ng R, 2005, ACM T GRAPHIC, V24, P735, DOI 10.1145/1073204.1073256
   NG R, 2005, LIGH FIELD PHOTOGRAP
   PENTLAND AP, 1987, IEEE T PATTERN ANAL, V9, P523, DOI 10.1109/TPAMI.1987.4767940
   Raskar R., 2006, SIGGRAPH 2006 COURSE
   RASKAR R, 2005, SIGGRAPH 2005 COURSE
   Ray S.F., 1988, Applied Photographic Optics: Imaging Systems for Photography, Film, and Video
   SUBBARAO M, 1994, INT J COMPUT VISION, V13, P271, DOI 10.1007/BF02028349
   SURYA G, 1994, THESIS STATE U NEW Y
   Tucker SC, 1999, OPT EXPRESS, V4, P467, DOI 10.1364/OE.4.000467
   Watanabe M, 1998, INT J COMPUT VISION, V27, P203, DOI 10.1023/A:1007905828438
   Weiss B, 2006, ACM T GRAPHIC, V25, P519, DOI 10.1145/1141911.1141918
   Winnemöller H, 2006, ACM T GRAPHIC, V25, P1221, DOI 10.1145/1141911.1142018
   Ziou D, 2001, COMPUT VIS IMAGE UND, V81, P143, DOI 10.1006/cviu.2000.0899
NR 24
TC 4
Z9 4
U1 0
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2008
VL 24
IS 4
BP 281
EP 294
DI 10.1007/s00371-007-0199-3
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 277UL
UT WOS:000254240100007
DA 2024-07-18
ER

PT J
AU Venetillo, JS
   Celes, W
AF Venetillo, Jeronimo S.
   Celes, Waldemar
TI GPU-based particle simulation with inter-collisions
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 25th Computer Graphics International Conference (CGI)
CY MAY 30-JUN 02, 2007
CL Petropolis, BRAZIL
DE particle simulation; animation; generic programming on GPU
AB In this paper, we present a new GPU-based particle system. The simulation runs entirely on the GPU, thus eliminating data transfer between CPU and GPU. The proposed system is able to simulate particles in confined environments, including support for inter-particle collisions, constraints, and particle-obstacle collisions. On modern graphics cards, the system simulates up to one million particles at interactive rate. We also propose a flexible approach for modeling the obstacles that define the environment, allowing the creation of different scenes without relying on shader re-coding. A set of computational experiments demonstrates the effectiveness of the proposed system.
C1 PUC Rio, Tecgraf Comp Sci Dept, Janeiro, Brazil.
C3 Pontificia Universidade Catolica do Rio de Janeiro
RP Venetillo, JS (corresponding author), PUC Rio, Tecgraf Comp Sci Dept, Rua Maques Sao Vicente 255, Janeiro, Brazil.
EM jeronimo@tec-raf.puc-rio.br; celes@inf.puc-rio.br
CR ALDER BJ, 1959, J CHEM PHYS, V31, P459, DOI 10.1063/1.1730376
   Bagi K, 2005, GRANUL MATTER, V7, P31, DOI 10.1007/s10035-004-0187-5
   Bell N., 2005, P 2005 ACM SIGGRAPH, P77, DOI DOI 10.1145/1073368.1073379
   Clavet S., 2005, SCA '05, P219, DOI DOI 10.1145/1073368.1073400
   Courty N, 2005, COMPUTER GRAPHICS INTERNATIONAL 2005, PROCEEDINGS, P206
   Eberly D.H., 2004, GAME PHYS
   Ericson C., 2005, Real-time collision detection
   GOVINDARAJU NK, 2005, P S INT 3D GRAPH GAM, P49
   Jakobsen T, 2001, P GAM DEV C
   Kipfer P., 2004, HWWS 04, P115, DOI [10.1145/1058129.1058146, DOI 10.1145/1058129.1058146]
   LATTA L, 2004, P GAM DEV C
   Liu GR., 2003, SMOOTHED PARTICLE HY, DOI 10.1142/5340
   Mirtich B, 2000, COMP GRAPH, P193, DOI 10.1145/344779.344866
   Muller M., 2003, ACM SIGGRAPH/Eurographics Symposium on Computer Animation, P154
   Muller M, 2005, P 2005 ACM SIGGRAPH, P237, DOI DOI 10.1145/1073368.1073402
   Press W. H., 2002, Numerical Recipes in C: the Art of Scientific Computing, V2nd ed., DOI DOI 10.1119/1.14981
   Purcell T., 2003, SIGGRAPHEUROGRAPHICS, P41
   Selle A, 2005, ACM T GRAPHIC, V24, P910, DOI 10.1145/1073204.1073282
   Sigurgeirson H, 2001, J COMPUT PHYS, V172, P766, DOI 10.1006/jcph.2001.6858
NR 19
TC 7
Z9 8
U1 0
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2007
VL 23
IS 9-11
SI SI
BP 851
EP 860
DI 10.1007/s00371-007-0151-6
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 206UE
UT WOS:000249207600025
DA 2024-07-18
ER

PT J
AU Noble, P
   Tang, W
AF Noble, Paul
   Tang, Wen
TI Automatic expressive deformations for implying and stylizing motion
SO VISUAL COMPUTER
LA English
DT Article
DE expressive deformations; cartoon animation; non-photorealistic
   rendering; stylizing motion; implied motion
AB Three-dimensional computer animation often struggles to compete with the flexibility and expressiveness commonly found in traditional animation, particularly when rendered non-photorealistically. We present an animation tool that takes skeleton-driven 3D computer animations and generates expressive deformations to the character geometry. The technique is based upon the cartooning and animation concepts of "lines of action" and "lines of motion" and automatically infuses computer animations with some of the expressiveness displayed by traditional animation. Motion and pose-based expressive deformations are generated from the motion data and the character geometry is warped along each limb's individual line of motion. The effect of this subtle, yet significant, warping is twofold: geometric inter-frame consistency is increased which helps create visually smoother animated sequences, and the warped geometry provides a novel solution to the problem of implied motion in non-photorealistic imagery. Object-space and image-space versions of the algorithm have been implemented and are presented.
C1 Univ Teesside, Sch Comp, Middlesbrough, Cleveland, England.
C3 University of Teesside
RP Tang, W (corresponding author), Univ Teesside, Sch Comp, Middlesbrough, Cleveland, England.
EM p.r.noble@tees.ac.uk; w.tang@tees.ac.uk
OI Noble, Paul/0000-0001-8967-4543
CR [Anonymous], 2001, The animator's survival kit
   *AUT INC, 2006, AUT MAYA
   Blair P., 1994, CARTOON ANIMATION
   Bregler C, 2002, ACM T GRAPHIC, V21, P399, DOI 10.1145/566570.566595
   Brooks B., 2001, COMPLETE CARTOONING
   Chang S., 2002, Proceedings of Physical Phenomena at High Magnetic Fields - IV, P133, DOI 10.1142/9789812777805_0032
   HART C, 1994, EVERYTHING YOU EVER
   Hart Christopher., 1997, DRAW ANIMATION
   Kawagishi Y, 2003, COMPUTER GRAPHICS INTERNATIONAL, PROCEEDINGS, P276, DOI 10.1109/CGI.2003.1214482
   Lake A., 2000, Proceedings of the 1st International Symposium on Non-Photorealistic Animation and Rendering New York, NY, USA,, P13
   LASSETER J, 1987, COMPUTER GRAPHICS, V21, P35, DOI DOI 10.1145/37401.37407
   Lee Stan., 1978, DRAW COMICS MARVEL W
   LI Y, 2003, P 2003 ACM SIGGRAPH, P309
   MacCracken Ron., 1996, SIGGRAPH, P181, DOI DOI 10.1145/237170.237247
   Noble P., 2006, Proceedings of the 4th International Conference on Computer Graphics and Interactive Techniques in Australasia and Southeast Asia, P57
   POMERANTZ JR, 1983, PERCEPT PSYCHOPHYS, V33, P365, DOI 10.3758/BF03205883
   Potmesil M., 1983, Computer Graphics, V17, P389, DOI 10.1145/964967.801169
   Rademacher P, 1999, COMP GRAPH, P439, DOI 10.1145/311535.311612
   Sederberg T. W., 1986, Computer Graphics, V20, P151, DOI 10.1145/15886.15903
   Siu Chi Hsu, 1994, Computer Graphics Proceedings. Annual Conference Series 1994. SIGGRAPH 94 Conference Proceedings, P109
   STROTHOTTE T, 1994, COMPUT GRAPH FORUM, V13, pC455, DOI 10.1111/1467-8659.1330455
   Strothotte T, 2002, NONPHOTOREALISTIC CO
   Thomas F., 1981, The Illusion of Life: Disney Animation
   WAWNG J, 2006, ACM SIGGRAPH 2006 PA, P1169
   WHEELER A, 2006, ACM SIGGRAPH 2006 SK, P95
   White T., 1986, The Animator's Workbook
NR 26
TC 1
Z9 2
U1 0
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2007
VL 23
IS 7
BP 523
EP 533
DI 10.1007/s00371-007-0125-8
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 182DL
UT WOS:000247485200008
DA 2024-07-18
ER

PT J
AU Kim, J
   Kim, S
   Ko, H
   Terzopoulos, D
AF Kim, Jinwook
   Kim, Soojae
   Ko, Heedong
   Terzopoulos, Demetri
TI Fast GPU computation of the mass properties of a general shape and its
   application to buoyancy simulation
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 14th Pacific Conference on Computer Graphics and Applications
CY OCT 11-13, 2005
CL Taipei, TAIWAN
DE general-purpose computation on GPUs; mass property computation;
   physics-based animation; rigid-body dynamics; buoyancy simulation
ID ALGORITHMS; FLUID
AB To simulate solid dynamics, we must compute the mass, the center of mass, and the products of inertia about the axes of the body of interest. These mass property computations must be continuously repeated for certain simulations with rigid bodies or as the shape of the body changes. We introduce a GPU-friendly algorithm to approximate the mass properties for an arbitrarily shaped body. Our algorithm converts the necessary volume integrals into surface integrals on a projected plane. It then maps the plane into a framebuffer in order to perform the surface integrals rapidly on the GPU. To deal with non-convex shapes, we use a depth-peeling algorithm. Our approach is image-based; hence, it is not restricted by the mathematical or geometric representation of the body, which means that it can efficiently compute the mass properties of any object that can be rendered on the graphics hardware. We compare the speed and accuracy of our algorithm with an analytic algorithm, and demonstrate it in a hydrostatic buoyancy simulation for real-time applications, such as interactive games.
C1 Korea Inst Sci & Technol, Image Media Res Ctr, Seoul 136791, South Korea.
   Univ Calif Los Angeles, Dept Comp Sci, Los Angeles, CA 90095 USA.
C3 Korea Institute of Science & Technology (KIST); University of California
   System; University of California Los Angeles
RP Kim, J (corresponding author), Korea Inst Sci & Technol, Image Media Res Ctr, 39-1 Hawolgok Dong, Seoul 136791, South Korea.
EM jwkim@imrc.kist.re.kr; lono@imrc.kist.re.kr; ko@imrc.kist.re.kr;
   dt@cs.ucla.edu
CR Bolz J, 2003, ACM T GRAPHIC, V22, P917, DOI 10.1145/882262.882364
   Buck I., 2004, GPU Gems, P621
   Carlson M, 2004, ACM T GRAPHIC, V23, P377, DOI 10.1145/1015706.1015733
   Everitt C, 2001, INTERACTIVE ORDER IN
   Foster N, 1996, GRAPH MODEL IM PROC, V58, P471, DOI 10.1006/gmip.1996.0039
   Gonzalez-Ochoa C, 1998, ACM T GRAPHIC, V17, P143, DOI 10.1145/285857.285858
   Hillesland KE, 2003, ACM T GRAPHIC, V22, P925, DOI 10.1145/882262.882365
   Krüger J, 2003, ACM T GRAPHIC, V22, P908, DOI 10.1145/882262.882363
   KRUGER J, 2003, VIS 03, P38
   Layton AT, 2002, VISUAL COMPUT, V18, P41, DOI 10.1007/s003710100131
   LEE YT, 1982, COMMUN ACM, V25, P635, DOI 10.1145/358628.358643
   MAMMEN A, 1989, IEEE COMPUT GRAPH, V9, P43, DOI 10.1109/38.31463
   Mirtich B., 1996, Journal of Graphics Tools, V1, P31, DOI DOI 10.1080/10867651.1996.10487458
   Moreland Kenneth, 2003, P ACM SIGGRAPH EUROG, V117, P112, DOI DOI 10.2312/EGGH.EGGH03.112-119
   ROST R.J., 2004, OPENGL SHADING LANGU
   Thompson CJ, 2002, INT SYMP MICROARCH, P306, DOI 10.1109/MICRO.2002.1176259
   Wu EH, 2004, COMPUT ANIMAT VIRT W, V15, P139, DOI 10.1002/cav.16
NR 17
TC 6
Z9 8
U1 0
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2006
VL 22
IS 9-11
BP 856
EP 864
DI 10.1007/s00371-006-0071-x
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 082IW
UT WOS:000240381000028
DA 2024-07-18
ER

PT J
AU Lum, EB
   Shearer, J
   Ma, KL
AF Lum, Eric B.
   Shearer, James
   Ma, Kwan-Liu
TI Interactive multi-scale exploration for volume classification
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 14th Pacific Conference on Computer Graphics and Applications
CY OCT 11-13, 2005
CL Taipei, TAIWAN
DE classification; filter banks; transfer functions; scale-space filtering;
   volume rendering; hardware acceleration; interface design; visualization
AB Filter banks are a class of signal processing techniques that can be used to reveal the local energy of a signal at multiple scales. Utilizing such filtering allows us to consider local texture and other data characteristics, and permits volume classification and visualization that cannot be accomplished easily using conventional, transfer function-based methods. Our filter bank approach increases the dimensionality, and thus, the complexity of the classification task. We have therefore developed an interactive user interface for specifying and visualizing these higher dimensional classifiers, which enables volume data exploration and visualization in a filter-bank space. We demonstrate that this technique is particularly effective for the classification of noisy data, and for classifying regions that are difficult to approach using conventional methods.
C1 Univ Calif Davis, Davis, CA 95616 USA.
C3 University of California System; University of California Davis
RP Lum, EB (corresponding author), Univ Calif Davis, 1 Shields Ave, Davis, CA 95616 USA.
EM ericlum101@yahoo.com; jjshearer@ucdavis.edu; klma@ucdavis.edu
CR Andrienko G, 2004, SECOND INTERNATIONAL CONFERENCE ON COORDINATED & MULTIPLE VIEWS IN EXPLORATORY VISUALIZATION, PROCEEDINGS, P93
   [Anonymous], 2002, SURFACES
   [Anonymous], 1994, Multirate Digital Signal Processing: Multirate Systems, Filter Banks, Wavelets
   Bajaj CL, 1997, VISUALIZATION '97 - PROCEEDINGS, P167, DOI 10.1109/VISUAL.1997.663875
   Barlow N, 2004, IEEE INFOR VIS, P725, DOI 10.1109/IV.2004.1320222
   BOVIK AC, 1990, IEEE T PATTERN ANAL, V12, P55, DOI 10.1109/34.41384
   BURT PJ, 1983, IEEE T COMMUN, V31, P532, DOI 10.1109/TCOM.1983.1095851
   Drebin R. A., 1988, Computer Graphics, V22, P65, DOI 10.1145/378456.378484
   DUNN D, 1994, IEEE T PATTERN ANAL, V16, P130, DOI 10.1109/34.273736
   FUA YH, 1999, VIS 99, P43
   Fujishiro I., 1999, Proceedings Visualization '99 (Cat. No.99CB37067), P467, DOI 10.1109/VISUAL.1999.809932
   Georgescu B, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P456
   Hauser H, 2002, INFOVIS 2002: IEEE SYMPOSIUM ON INFORMATION VISUALIZATION 2002, P127, DOI 10.1109/INFVIS.2002.1173157
   He TS, 1996, IEEE VISUAL, P227, DOI 10.1109/VISUAL.1996.568113
   Huang RZ, 2003, IEEE VISUALIZATION 2003, PROCEEDINGS, P547, DOI 10.1109/VISUAL.2003.1250418
   Huang RZ, 2003, 11TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P355
   Inselberg A, 1985, VISUAL COMPUT, V1, P69, DOI 10.1007/BF01898350
   Jain AK, 1996, IEEE T PATTERN ANAL, V18, P195, DOI 10.1109/34.481543
   JAIN AK, 1991, PATTERN RECOGN, V24, P1167, DOI 10.1016/0031-3203(91)90143-S
   Jankun-Kelly TJ, 2001, SPRING EUROGRAP, P51
   Kindlmann G, 1998, IEEE SYMPOSIUM ON VOLUME VISUALIZATION, P79, DOI 10.1109/SVV.1998.729588
   Kniss J, 2001, IEEE VISUAL, P255, DOI 10.1109/VISUAL.2001.964519
   Lefohn AE, 2003, LECT NOTES COMPUT SC, V2878, P564
   LEVOY M, 1988, IEEE COMPUT GRAPH, V8, P29, DOI 10.1109/38.511
   Lum EB, 2004, IEEE VISUALIZATION 2004, PROCEEEDINGS, P289, DOI 10.1109/VISUAL.2004.64
   MARKS J., 1997, INT C COMPUTER GRAPH, P389
   Pfister H, 2001, IEEE COMPUT GRAPH, V21, P16, DOI 10.1109/38.920623
   Randen T, 1999, IEEE T PATTERN ANAL, V21, P291, DOI 10.1109/34.761261
   Tzeng FY, 2003, IEEE VISUALIZATION 2003, PROCEEDINGS, P505, DOI 10.1109/VISUAL.2003.1250413
   UNSER M, 1986, SIGNAL PROCESS, V11, P61, DOI 10.1016/0165-1684(86)90095-2
   Vaidyanathan P. P., 1993, MULTIRATE SYSTEMS FI
   VanGelder A, 1996, 1996 SYMPOSIUM ON VOLUME VISUALIZATION, PROCEEDINGS, P23, DOI 10.1109/SVV.1996.558039
   Vilanova A, 2001, SPRING CONFERENCE ON COMPUTER GRAPHICS, PROCEEDINGS, P241, DOI 10.1109/SCCG.2001.945360
NR 33
TC 13
Z9 18
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2006
VL 22
IS 9-11
BP 622
EP 630
DI 10.1007/s00371-006-0049-8
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 082IW
UT WOS:000240381000005
DA 2024-07-18
ER

PT J
AU Tack, K
   Lafruit, G
   Catthoor, F
   Lauwereins, R
AF Tack, Klaas
   Lafruit, Gauthier
   Catthoor, Francky
   Lauwereins, Rudy
TI Platform independent optimisation of multi-resolution 3D content to
   enable universal media access
SO VISUAL COMPUTER
LA English
DT Article
DE universal media access; multi-resolution 3D content; trade-off
   quality/display time
ID SURFACES
AB Flurries of terminals with large differences in terminal capabilities currently consume information and multi-media content. Their different processing capabilities make it challenging to guarantee satisfactory quality in all possible situations. This paper proposes a systematic methodology for interactive 3D graphics applications to adapt the complexity of the content automatically to the terminal's available resources. Our contribution is an offline/online partitioned optimisation that increases the visual quality with respect to previous work at the same rendering cost, while the overhead of the optimisation is minimal.
C1 IMEC, B-3001 Louvain, Belgium.
   Katholieke Univ Leuven, Louvain, Belgium.
C3 IMEC; KU Leuven
RP Tack, K (corresponding author), IMEC, Kapeldreef 75, B-3001 Louvain, Belgium.
EM klaas.tack@imec.be; gauthier.lafruit@imec.be; francky.catthoor@imec.be;
   rudy.lauwereins@imec.be
CR Aliaga DG, 1999, COMP GRAPH, P307, DOI 10.1145/311535.311574
   [Anonymous], 2003, LEVEL DETAIL 3D GRAP
   [Anonymous], SIGGRAPH 94
   Aspert N, 2002, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL I AND II, PROCEEDINGS, P705, DOI 10.1109/ICME.2002.1035879
   AVILES M, 2005, PCM 05
   BIYD S, 2004, CONVEX OPTIMIZATION
   BLAKE E, 1987, EUR C P, P295
   BONTEMPI G, 2004, MODELES STOCHASTIQUE, V2
   Boyd S., 2004, CONVEX OPTIMIZATION
   Brandt S, 1998, INT WORKSH QUAL SERV, P154, DOI 10.1109/IWQOS.1998.675233
   Burgard AP, 2002, METAB ENG, V4, P111, DOI 10.1006/mben.2001.0222
   BURGOS FM, 1999, IWSNHC3DI 99, P189
   CATMULL E, 1978, COMPUT AIDED DESIGN, V10, P350, DOI 10.1016/0010-4485(78)90110-0
   CHANG C, 2002, IEEE PAC RIM C MULT, P1105
   Diepstraten J, 2004, COMPUTER GRAPHICS INTERNATIONAL, PROCEEDINGS, P454, DOI 10.1109/CGI.2004.1309247
   DOO D, 1978, COMPUT AIDED DESIGN, V10, P356, DOI 10.1016/0010-4485(78)90111-2
   DUGUET F, 2004, IEEE COMPUT GRAPH AP, V24
   Fernando R., 2004, GPU Gems: Programming Techniques, Tips and Tricks for Real-Time Graphics
   FUNKHOUSER TA, 1992, 1992 S INT 3D GRAPH, P11
   FUNKHOUSER TA, 1993, ACM SIGGRAPH, V93, P247
   Gobbetti E, 2000, COMPUT AIDED DESIGN, V32, P785, DOI 10.1016/S0010-4485(00)00068-3
   GOBBETTI E, 1999, IEEE VISUALIZATION, P123
   Guskov I, 2000, COMP GRAPH, P95, DOI 10.1145/344779.344831
   Hoppe H., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P189, DOI 10.1145/258734.258843
   Ibaraki T., 1978, Journal of the Operations Research Society of Japan, V21, P59
   Khodakovsky A, 2000, COMP GRAPH, P271, DOI 10.1145/344779.344922
   Kim HS, 2004, COMPUTER GRAPHICS INTERNATIONAL, PROCEEDINGS, P462
   Lamberti F., 2003, Proceedings of the eighth international conference on 3D Web technology, Web3D '03, P55
   Lee A, 2000, COMP GRAPH, P85, DOI 10.1145/344779.344829
   Lee A.W. F., 1998, COMPUT GRAPHICS-US, V32, P95
   Lindstrom P, 2000, ACM T GRAPHIC, V19, P204, DOI 10.1145/353981.353995
   LUEBKE D, 1997, SIGGRAPH 97, P199
   MACIEL PWC, 1995, S INT 3D GRAPH, P95
   PARETO V, 1996, COURS EC POLITIQUE
   PENG J, 2003, J VISUAL COMMUN IMAG
   ROSSIGNAC J, 1993, C GEOM MOD COMP GRAP, P455
   Said A, 1996, IEEE T CIRC SYST VID, V6, P243, DOI 10.1109/76.499834
   Schachter B., 1983, COMPUTER IMAGE GENER
   Stanford University, STANF 3D SCANN REP
   Tack N., 2005, Proc. 10th Int. Conf. on 3D Web Technol., New York, NY, P19
   Tack Nicolaas., 2004, Proceedings of the ninth international conference on 3D Web technology, P109
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Williams MC, 2003, ELEC SOC S, V2003, P3
   WIMMER M, 2003, P 14 EUR WORKSH REND, P118
   Xia JC, 1996, IEEE VISUAL, P327, DOI 10.1109/VISUAL.1996.568126
   Zach C., 2002, Proceedings of the ACM symposium on Virtual reality software and technology, P1, DOI DOI 10.1145/585740.585742
   CHARACTERIZATION TOP
NR 47
TC 11
Z9 12
U1 0
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2006
VL 22
IS 8
BP 577
EP 590
DI 10.1007/s00371-006-0036-0
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 088TD
UT WOS:000240833100006
DA 2024-07-18
ER

PT J
AU Wu, Y
   He, YJ
   Tian, HS
AF Wu, Y
   He, YJ
   Tian, HS
TI Relaxation of spherical parameterization meshes
SO VISUAL COMPUTER
LA English
DT Article
DE relaxation; spherical mesh; parameterization; patch; distortion metric
AB For closed two-manifold genus-0 meshes, the sphere is the most natural parameterization domain. Like planar parameterization, spherical parameterization without foldover and with low distortion consitutes two basic challenges. However, in order to prevent foldovers, many current studies cannot validly reduce distortion, especially area distortion. In this paper, we present a two-stage relaxation method to reduce the distortion of spherical meshes parameterized by traditional methods. The first stage is an authalic (area-preserving) relaxation algorithm that iteratively relaxes the mesh surface by growing patches around seed triangles. The second one is a relaxation procedure based on SCDM (spherical-domain compositive distortion metric).
C1 Shanghai Jiao Tong Univ, Dept Comp Sci & Technol, Shanghai 200030, Peoples R China.
C3 Shanghai Jiao Tong University
RP Shanghai Jiao Tong Univ, Dept Comp Sci & Technol, Shanghai 200030, Peoples R China.
EM wuyong916@sjtu.edu.cn
CR Alexa M, 2000, VISUAL COMPUT, V16, P26, DOI 10.1007/PL00007211
   BIRKHOLZ H, 2004, WSCG 04 PLZEN CZECH, P57
   Degener P., 2003, P 12 INT MESH ROUNDT, P201
   Floater MS, 2005, MATH VIS, P157, DOI 10.1007/3-540-26808-1_9
   Floater MS, 1997, COMPUT AIDED GEOM D, V14, P231, DOI 10.1016/S0167-8396(96)00031-3
   GOTSMAN C, 2003, P ACM SIGGRAPH 03
   Gu XF, 2004, IEEE T MED IMAGING, V23, P949, DOI 10.1109/TMI.2004.831226
   Haker S, 2000, IEEE T VIS COMPUT GR, V6, P181, DOI 10.1109/2945.856998
   HOPPE H, 1996, P ACM SIGGRAPH 96
   HOPPE H, 1993, P ACM SIGGRAPH 93
   KENT J, 1992, P ACM SIGGRAPH 92
   LEE DT, 1979, J ACM, V26, P415, DOI 10.1145/322139.322142
   LEVY B, 2002, P ACM SIGGRAPH 02
   PARAJOLA R, 2000, IEEE T VISUALIZATION, V6, P79
   SANDER P, 2001, P ACM SIGGRAPH 01
   Sander PedroV., 2002, EGRW 02, P87
   Shapiro A, 1998, VISUAL COMPUT, V14, P429, DOI 10.1007/s003710050153
   Sheffer A, 2001, ENG COMPUT-GERMANY, V17, P326, DOI 10.1007/PL00013391
   SHEFFER A, 2003, 4 ISR KOR BIN C GEOM, P94
NR 19
TC 1
Z9 1
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2005
VL 21
IS 11
BP 897
EP 904
DI 10.1007/s00371-005-0301-7
PG 8
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 974RH
UT WOS:000232608600002
DA 2024-07-18
ER

PT J
AU Cao, XC
   Shen, YP
   Shah, M
   Foroosh, H
AF Cao, XC
   Shen, YP
   Shah, M
   Foroosh, H
TI Single view compositing with shadows
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 13th Pacific Conference on Computer Graphics and Applications
CY OCT 12-14, 2005
CL Macao, PEOPLES R CHINA
DE matting and compositing; image-based rendering; computer vision; shadow
   matte
AB In this paper, we describe how geometrically correct and visually realistic shadows may be computed for objects composited into a single view of a target scene. Compared to traditional single view compositing methods, which either do not deal with the shadow effects or manually create the shadows for the composited objects, our approach efficiently utilizes the geometric and photometric constraints extracted from a single target image to synthesize the shadows consistent with the overall target scene for the inserted objects. In particular, we explore (i) the constraints provided by imaged scene structure, e.g. vanishing points of orthogonal directions, for camera calibration and thus explicit determination of the locations of the camera and the light source; (ii) the relatively weaker geometric constraint, the planar homology, that models the imaged shadow relations when explicit camera calibration is not possible; and (iii) the photometric constraints that are required to match the color characteristics of the synthesized shadows with those of the original scene. For each constraint, we demonstrate the working examples followed by our observations. To show the accuracy and the applications of the proposed method, we present the results for a variety of target scenes, including footage from commercial Hollywood movies and 3D video games.
C1 Univ Cent Florida, Sch Comp Sci, Orlando, FL 32816 USA.
C3 State University System of Florida; University of Central Florida
RP Univ Cent Florida, Sch Comp Sci, Orlando, FL 32816 USA.
EM xccao@cs.ucf.edu; ypshen@cs.ucf.edu; shah@cs.ucf.ed; foroosh@cs.ucf.edu
RI Sahoo, Sarat Kumar/J-8765-2014
OI Sahoo, Sarat Kumar/0000-0001-5734-6844; Shah,
   Mubarak/0000-0001-6172-5572
CR Agarwala A, 2004, ACM T GRAPHIC, V23, P584, DOI 10.1145/1015706.1015764
   [Anonymous], 1978, COMPUTER VISION SYST
   [Anonymous], 2001, Robotica, DOI DOI 10.1017/S0263574700223217
   Apostoloff N, 2004, PROC CVPR IEEE, P407
   Cao XC, 2005, PROC CVPR IEEE, P918
   CAPRILE B, 1990, INT J COMPUT VISION, V4, P127, DOI 10.1007/BF00127813
   Chen Q, 2004, LECT NOTES COMPUT SC, V3023, P521
   Chuang Y.-Y., 2004, THESIS U WASHINGTON
   CROW F, 1977, P SIGGRAPH 77, P242
   Finlayson GD, 2004, LECT NOTES COMPUT SC, V3023, P582
   Hasenfratz JM, 2003, COMPUT GRAPH FORUM, V22, P753, DOI 10.1111/j.1467-8659.2003.00722.x
   Kersten D, 1997, PERCEPTION, V26, P171, DOI 10.1068/p260171
   Li Y, 2004, ACM T GRAPHIC, V23, P303, DOI 10.1145/1015706.1015719
   LI Y, 2003, P IEEE ICCV, P1336
   Liebowitz D., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P293, DOI 10.1109/ICCV.1999.791233
   Nadimi S, 2004, IEEE T PATTERN ANAL, V26, P1079, DOI 10.1109/TPAMI.2004.51
   Petrovic L, 2000, COMP GRAPH, P511, DOI 10.1145/344779.345073
   Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720
   Sato I, 2001, PROC CVPR IEEE, P400
   Smith A., 1996, PROC C COMPUTER GRAP, P259
   SPRINGER CE, 1964, FREE MAN, P23
   Sun J, 2004, ACM T GRAPHIC, V23, P315, DOI 10.1145/1015706.1015721
   TAPPEN MF, 2002, ADV NEURAL INFORMATI
   Weiss Y, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P68, DOI 10.1109/ICCV.2001.937606
   Wright S., 2001, Digital compositing for film and video
NR 25
TC 11
Z9 12
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2005
VL 21
IS 8-10
SI SI
BP 639
EP 648
DI 10.1007/s00371-005-0335-x
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 964CX
UT WOS:000231857400016
DA 2024-07-18
ER

PT J
AU Ju, T
   Warren, J
   Carson, J
   Eichele, G
   Thaller, C
   Chiu, W
   Bello, M
   Kakadiaris, I
AF Ju, T
   Warren, J
   Carson, J
   Eichele, G
   Thaller, C
   Chiu, W
   Bello, M
   Kakadiaris, I
TI Building 3D surface networks from 2D curve networks with application to
   anatomical modeling
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 13th Pacific Conference on Computer Graphics and Applications
CY OCT 12-14, 2005
CL Macao, PEOPLES R CHINA
DE surface network; polygonal modeling; contour interpolation
ID SHAPE RECONSTRUCTION
AB Constructing 3D surfaces that interpolate 2D curves defined on parallel planes is a fundamental problem in computer graphics with wide applications including modeling anatomical structures. Typically the problem is simplified so that the 2D curves partition each plane into only two materials (e.g., air versus tissue). Here we consider the general problem where each plane is partitioned by a curve network into multiple materials (e.g., air, cortex, cerebellum, etc.). We present a novel method that automatically constructs a surface network from curve networks with arbitrary topology and partitions an arbitrary number of materials. The surface network exactly interpolates the curve network on each plane and is guaranteed to be free of gaps or self-intersections. In addition, our method provides a flexible framework for user interaction so that the surface topology can be modified conveniently when necessary. As an application, we applied the method to build a high-resolution 3D model of the mouse brain from 2D anatomical boundaries defined on 350 tissue sections. The surface network accurately models the partitioning of the brain into 17 abutting anatomical regions with complex topology.
C1 Washington Univ, Dept Comp Sci & Engn, St Louis, MO 63130 USA.
   Rice Univ, Houston, TX 77251 USA.
   Baylor Coll Med, Houston, TX 77030 USA.
   Max Planck Inst Expt Endocrinol, D-30625 Hannover, Germany.
   Univ Houston, Houston, TX USA.
C3 Washington University (WUSTL); Rice University; Baylor College of
   Medicine; Max Planck Society; University of Houston System; University
   of Houston
RP Washington Univ, Dept Comp Sci & Engn, St Louis, MO 63130 USA.
EM taoju@cs.wustl.edu; jwarren@rice.edu; james.carson@bcm.tmc.edu;
   geichele@bcm.tmc.edu; cthaller@bcm.tmc.edu; wah@bcm.tmc.edu;
   mbello@uh.edu; ioannisk@uh.edu
RI Carson, James/Q-5693-2019
OI Carson, James/0000-0003-3733-8796; Kakadiaris,
   Ioannis/0000-0002-0591-1079; Carson, James/0000-0001-9009-5645
CR Aichholzer O., 1996, Computing and Combinatorics. Second Annual International Conference. COCOON '96. Proceedings, P117
   Bajaj CL, 1996, GRAPH MODEL IM PROC, V58, P524, DOI 10.1006/gmip.1996.0044
   Barequet G, 1996, COMPUT VIS IMAGE UND, V63, P251, DOI 10.1006/cviu.1996.0018
   Barequet Gill., 2004, Graph. Models, V65, P323
   BLOOR MIG, 1994, COMPUT GRAPH, V18, P161, DOI 10.1016/0097-8493(94)90090-6
   BOISSONNAT JD, 1988, COMPUT VISION GRAPH, V44, P1, DOI 10.1016/S0734-189X(88)80028-8
   Chai JY, 1998, VISUALIZATION '98, PROCEEDINGS, P27, DOI 10.1109/VISUAL.1998.745281
   Cheng S.-W., 1999, PROC ACM S SOLID MOD, P322
   Christiansen H.N., 1978, CONVERSION OF COMPLEX CONTOUR LINE DEFINITIONS INTO POLYGONAL ELEMENT MOSAICS, V12, P187
   Csébfalvi B, 2002, VISION MODELING, AND VISUALIZATION 2002, PROCEEDINGS, P123
   FUCHS H, 1977, COMMUN ACM, V20, P693, DOI 10.1145/359842.359846
   Geiger B., 3 DIMENSIONAL MODELI
   HAGEN H, 1993, FOCUS SCI VISUALIZAT
   HERMAN GT, 1992, IEEE COMPUT GRAPH, V12, P69, DOI 10.1109/38.135915
   Ju T, 2004, ACM T GRAPHIC, V23, P888, DOI 10.1145/1015706.1015815
   Ju T, 2002, ACM T GRAPHIC, V21, P339
   JU T, 2003, P EUR ACM SIGGRAPH S, P166
   KEPPEL E, 1975, IBM J RES DEV, V19, P2, DOI 10.1147/rd.191.0002
   Klein R, 2000, GRAPH MODELS, V62, P429, DOI 10.1006/gmod.2000.0530
   MEYERS D, 1992, ACM T GRAPHIC, V11, P228, DOI 10.1145/130881.131213
   MOODY D, SURFDRIVER PRACTICAL
   Oliva JM, 1996, COMPUT GRAPH FORUM, V15, pC397, DOI 10.1111/1467-8659.1530397
   SLOAN KR, 1987, CHI 87, P1115
   Turk G, 1999, COMP GRAPH, P335, DOI 10.1145/311535.311580
   WEINSTEIN D, 2000, VISUALIZATION 00
   WOLFRAM S, 1993, MATH BOOK
NR 26
TC 24
Z9 27
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2005
VL 21
IS 8-10
SI SI
BP 764
EP 773
DI 10.1007/s00371-005-0321-3
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 964CX
UT WOS:000231857400029
DA 2024-07-18
ER

PT J
AU Katz, S
   Leifman, G
   Tal, A
AF Katz, S
   Leifman, G
   Tal, A
TI Mesh segmentation using feature point and core extraction
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 13th Pacific Conference on Computer Graphics and Applications
CY OCT 12-14, 2005
CL Macao, PEOPLES R CHINA
DE mesh segmentation; mesh decomposition; feature point extraction
ID POLYHEDRAL SURFACE DECOMPOSITION; PARAMETERIZATION
AB Mesh segmentation has become a necessary ingredient in many applications in computer graphics. This paper proposes a novel hierarchical mesh segmentation algorithm, which is based on new methods for prominent feature point and core extraction. The algorithm has several benefits. First, it is invariant both to the pose of the model and to different proportions between the model's components. Second, it produces correct hierarchical segmentations of meshes, both in the coarse levels of the hierarchy and in the fine levels, where tiny segments are extracted. Finally, the boundaries between the segments go along the natural seams of the models.
C1 Technion Israel Inst Technol, Dept Elect Engn, IL-32000 Haifa, Israel.
C3 Technion Israel Institute of Technology
RP Technion Israel Inst Technol, Dept Elect Engn, IL-32000 Haifa, Israel.
EM sagikatz@tx.technion.ac.il; gleifman@tx.technion.ac.il;
   ayelett@ee.technion.ac.il
CR Alexa M, 2000, VISUAL COMPUT, V16, P26, DOI 10.1007/PL00007211
   Barlow RE., 1972, STAT INFERENCE ORDER
   BORG I, 1977, MODERN MULTIDIMENSIO
   Chazelle B, 1997, COMP GEOM-THEOR APPL, V7, P327, DOI 10.1016/S0925-7721(96)00024-7
   Cohen-Steiner D, 2004, ACM T GRAPHIC, V23, P905, DOI 10.1145/1015706.1015817
   Cox M., 1994, MULTIDIMENSIONAL SCA
   Elad A, 2003, IEEE T PATTERN ANAL, V25, P1285, DOI 10.1109/TPAMI.2003.1233902
   Funkhouser T, 2004, ACM T GRAPHIC, V23, P652, DOI 10.1145/1015706.1015775
   Garland M., 1997, PROC 24 C COMPUTER G, P209, DOI DOI 10.1145/258734.258849
   Garland M., 2001, I3D 01, P49, DOI [DOI 10.1145/364338.364345, 10.1145/364338.364345]
   Gregory A, 1999, VISUAL COMPUT, V15, P453, DOI 10.1007/s003710050192
   Hilaga M, 2001, COMP GRAPH, P203, DOI 10.1145/383259.383282
   HOFFMAN DD, 1985, VIS COGN, P65
   Karni Z, 2000, COMP GRAPH, P279, DOI 10.1145/344779.344924
   Katz S, 2003, ACM T GRAPHIC, V22, P954, DOI 10.1145/882262.882369
   Kimmel R, 1998, P NATL ACAD SCI USA, V95, P8431, DOI 10.1073/pnas.95.15.8431
   Kraevoy V, 2004, ACM T GRAPHIC, V23, P861, DOI 10.1145/1015706.1015811
   Kraevoy V, 2003, ACM T GRAPHIC, V22, P326, DOI 10.1145/882262.882271
   Kruskal J. B., 1964, PSYCHOMETRIKA, V29
   Kruskal JB, 1978, MULTIDIMENSIONAL SCA
   Lee Y, 2004, 12TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P279
   Lévy B, 2002, ACM T GRAPHIC, V21, P362, DOI 10.1145/566570.566590
   Li X., 2001, P 2001 S INT 3D GRAP, P35, DOI DOI 10.1145/364338.364343
   Liu R, 2004, 12TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P298
   Mangan AP, 1999, IEEE T VIS COMPUT GR, V5, P308, DOI 10.1109/2945.817348
   Praun E, 2001, COMP GRAPH, P179, DOI 10.1145/383259.383277
   Sander PV, 2001, COMP GRAPH, P409, DOI 10.1145/383259.383307
   Schreiner J, 2004, ACM T GRAPHIC, V23, P870, DOI 10.1145/1015706.1015812
   SHAMIR A, 2004, P 2 INT S 3DPVT
   SHEPARD RN, 1962, PSYCHOMETRIKA, V27
   Shlafman S, 2002, COMPUT GRAPH FORUM, V21, P219, DOI 10.1111/1467-8659.00581
   Sumner RW, 2004, ACM T GRAPHIC, V23, P399, DOI 10.1145/1015706.1015736
   Zhang E, 2005, ACM T GRAPHIC, V24, P1, DOI 10.1145/1037957.1037958
   ZHOU K, 2004, EUR ACM SIGGRAPH S G, P45
   Zhou YN, 2004, 10TH INTERNATIONAL MULTIMEDIA MODELLING CONFERENCE, PROCEEDINGS, P187
   Zöckler M, 2000, VISUAL COMPUT, V16, P241, DOI 10.1007/PL00013396
   Zuckerberger E, 2002, COMPUT GRAPH-UK, V26, P733, DOI 10.1016/S0097-8493(02)00128-0
NR 37
TC 229
Z9 279
U1 0
U2 19
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2005
VL 21
IS 8-10
SI SI
BP 649
EP 658
DI 10.1007/s00371-005-0344-9
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 964CX
UT WOS:000231857400017
DA 2024-07-18
ER

PT J
AU Magnenat-Thalmann, N
   Volino, P
AF Magnenat-Thalmann, N
   Volino, P
TI From early draping to haute couture models: 20 years of research
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 13th Pacific Conference on Computer Graphics and Applications
CY OCT 12-14, 2005
CL Macao, PEOPLES R CHINA
DE cloth simulation; virtual garments; fashion design
AB Simulating the complex fashion garments of haute couture can only be reached through an optimal combination of modeling techniques and numerical methods that combines high computation efficiency with the versatility required for simulating intricate garment designs. Here we describe optimal choices illustrated by their integration into a design and simulation tool that allow interactive prototyping of garments along drape motion and comfortability tests on animated postures. These techniques have been successfully used to bring haute couture garments from early draping of fashion designers, to be simulated and visualized in the virtual world.
C1 Univ Geneva, MIRALab, CH-1211 Geneva, Switzerland.
C3 University of Geneva
RP Univ Geneva, MIRALab, CH-1211 Geneva, Switzerland.
EM thalmann@miralab.unige.ch
RI Thalmann, Nadia/AAK-5195-2021
OI Thalmann, Nadia/0000-0002-1459-5960
CR BARAFF D, 1998, COMPUTER GRAPHICS, V32, P106
   BREEN DE, 1994, COMPUT GRAPH, V32, P365
   CARIGNAN M, 1992, COMP GRAPH, V26, P99, DOI 10.1145/142920.134017
   Choi KJ, 2002, ACM T GRAPHIC, V21, P604, DOI 10.1145/566570.566624
   COLLIER JR, 1991, J TEXT I, V82, P96, DOI 10.1080/00405009108658741
   DEROSE T, 1998, COMPUT GRAPH SIGGRAP, V32, P148
   DESBRUN M, 1999, P GRAPH INT WARR NSW
   Eberhardt B, 2000, SPRING COMP SCI, P137
   EBERHARDT B, 1996, COMPUTER GRAPHICS TE, P52
   EISCHEN JW, 1996, COMPUTER GRAPHICS TE, P71
   GAN L, 1995, TEXT RES J, V65, P660, DOI 10.1177/004051759506501106
   HAUTH M, 2001, P EUR
   KANG YM, 2000, P WSCG, P322
   LAFLEUR B, 1991, P IFIP C MOD COMP GR, P179
   Mezger J, 2003, WSCG'2003, VOL 11, NO 2, CONFERENCE PROCEEDINGS, P322
   OH S, 2005, IN PRESS PACIFIC GRA
   Press W. H., 2002, Numerical Recipes in C: the Art of Scientific Computing, V2nd ed., DOI DOI 10.1119/1.14981
   PROVOT X, 1995, GRAPH INTER, P147
   SAKAGUCHI Y, 1991, T SOC ELECT INF COMM, V54, P25
   Terzopoulos D., 1988, Computer Graphics, V22, P269, DOI 10.1145/378456.378522
   Terzopoulos D., 1987, COMPUT GRAPH, P205, DOI DOI 10.1145/37402.37427
   Volino P, 2005, COMPUT AIDED DESIGN, V37, P593, DOI 10.1016/j.cad.2004.09.003
   Volino P., 2005, Computer-Aided Design and Applications, V2, P645
   Volino P, 1997, INTERNATIONAL CONFERENCE ON VIRTUAL SYSTEMS AND MULTIMEDIA - VSMM'97, PROCEEDINGS, P109, DOI 10.1109/VSMM.1997.622337
   Volino P, 2000, COMPUTER GRAPHICS INTERNATIONAL 2000, PROCEEDINGS, P257, DOI 10.1109/CGI.2000.852341
   VOLINO P, 1995, COMPUT GRAPH, V29, P137
   VOLINO P, 2001, P COMP GRAPH INT
   Weil J., 1986, Computer Graphics, V20, P49, DOI 10.1145/15886.15891
   YANG Y, 1993, COMPUTER GRAPHICS AP, V1, P237
NR 29
TC 52
Z9 64
U1 0
U2 27
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2005
VL 21
IS 8-10
SI SI
BP 506
EP 519
DI 10.1007/s00371-005-0347-6
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 964CX
UT WOS:000231857400002
DA 2024-07-18
ER

PT J
AU Rueda, AJ
   Feito, FR
   Ortega, LM
AF Rueda, AJ
   Feito, FR
   Ortega, LM
TI Layer-based decomposition of solids and its applications
SO VISUAL COMPUTER
LA English
DT Article
DE 3D decompositions; solid modeling; geometric algorithms
ID POLYGONS
AB In this work we describe a decomposition scheme for polyhedra called layer-based decomposition. This decomposition can be computed in a straightforward way for any kind of polyhedron: convex or nonconvex, genus 0 or higher, etc. and presents interesting properties and applications like point-in-polyhedron inclusion test, computation of Boolean operations, or 3D location. Two methods for computing this decomposition and several of its applications are described in detail, including experimental results and comparisons with alternative approaches.
C1 Univ Jaen, Escuela Politecn Super, Dept Informat, Jaen 23071, Spain.
C3 Universidad de Jaen
RP Univ Jaen, Escuela Politecn Super, Dept Informat, Jaen 23071, Spain.
EM ajrueda@ujaen.es; ffeito@ujaen.es; lidia@ujaen.es
RI Rueda-Ruiz, Antonio Jesús/AAY-5298-2021; Ortega Alvarado, Lidia
   M./M-1843-2014; Feito, Francisco/M-1672-2014
OI Rueda-Ruiz, Antonio Jesús/0000-0001-7692-454X; Ortega Alvarado, Lidia
   M./0000-0002-7320-7382; Feito, Francisco/0000-0001-8230-6529
CR AKENNINEMOLLER T, 2002, REAL TIME RENDERING
   [Anonymous], 1996, Computer graphics: principles and practice
   Bern M, 2000, HANDBOOK OF COMPUTATIONAL GEOMETRY, P291, DOI 10.1016/B978-044482537-7/50007-3
   BERN M, 1995, COMPUTING EUCLIDEAN, P189
   FEITO F, 1999, P WSCG 99, P87
   Feito FR, 1997, COMPUT GRAPH, V21, P23, DOI 10.1016/S0097-8493(96)00067-2
   FEITO FR, 1995, THESIS U GRANADA
   Goodman J.E., 1997, Handbook of Discrete and Computational Geometry
   Rivero M, 2000, COMPUT GRAPH-UK, V24, P881, DOI 10.1016/S0097-8493(00)00090-X
   RIVERO ML, 2002, THESIS U GRANADA
   RIVERO ML, 2001, P 11 C ESP INF GRAF, P341
   Rueda AJ, 2002, COMPUT GRAPH-UK, V26, P805, DOI 10.1016/S0097-8493(02)00135-8
   RUEDA AJ, 2004, THESIS U MALAGA
   SCHNEIDER P, 2002, GEOMETRY TOOLS COMPU
NR 14
TC 5
Z9 5
U1 0
U2 1
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2005
VL 21
IS 6
BP 406
EP 417
DI 10.1007/s00371-005-0302-6
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 952GD
UT WOS:000230991100005
DA 2024-07-18
ER

PT J
AU Yamauchi, H
   Lensch, HPA
   Haber, J
   Seidel, HP
AF Yamauchi, H
   Lensch, HPA
   Haber, J
   Seidel, HP
TI Textures revisited
SO VISUAL COMPUTER
LA English
DT Article
DE multiresolution texture synthesis; mesh parameterization; image
   inpainting; image restoration; facial modeling; frequency decomposition
ID IMAGE; PARAMETERIZATIONS; SPACE; MODEL
AB We describe texture generation methods for complex objects. Recently developed 3D scanning devices and high-resolution cameras can capture the complex geometry of an object and yield high-resolution images. However, generating a textured model from this input data is still a difficult problem. This task is divided into three subproblems: parameterization, texture combination, and texture restoration. A low-distortion parameterization method is presented, which minimizes geometry stretch energy. Photographs of the object taken from multiple viewpoints under modestly uncontrolled illumination conditions are merged into a seamless texture using our new texture combination method. We also demonstrate a texture restoration method that can fill in missing pixel information when the input photographs do not provide sufficient information to cover the entire surface due to self-occlusion or registration errors. Our methods are fully automatic, except for the registration process between a 3D model and input photographs. We demonstrate the application of our method to human face models for evaluation. The techniques presented in this paper make a consistent and complete pipeline to generate the texture of a complex object.
C1 MPI Informat, Saarbrucken, Germany.
   Stanford Univ, Stanford, CA 94305 USA.
C3 Max Planck Society; Stanford University
RP MPI Informat, Saarbrucken, Germany.
CR [Anonymous], 2001, Schooling for Tomorrow
   Arya S, 1998, J ACM, V45, P891, DOI 10.1145/293347.293348
   Ballester C, 2001, IEEE T IMAGE PROCESS, V10, P1200, DOI 10.1109/83.935036
   Balmelli L, 2002, COMPUT GRAPH FORUM, V21, P411, DOI 10.1111/1467-8659.t01-1-00601
   Bar-Joseph Z, 2001, IEEE T VIS COMPUT GR, V7, P120, DOI 10.1109/2945.928165
   BENNIS C, 1991, COMP GRAPH, V25, P237, DOI 10.1145/127719.122744
   Bertalmio M, 2000, COMP GRAPH, P417, DOI 10.1145/344779.344972
   Blinn J., 1978, Proceedings of the 5th annual conference on Computer graphics and interactive techniques-SIGGRAPH'78, V12, P286
   BLINN JF, 1976, COMMUN ACM, V19, P542, DOI 10.1145/965143.563322
   BROOKS S, 2002, ACM SIGGRAPH 02 C P, P653
   BURT PJ, 1983, ACM T GRAPHIC, V2, P217, DOI 10.1145/245.247
   Cignoni P, 1999, VISUAL COMPUT, V15, P519, DOI 10.1007/s003710050197
   Cohen MF, 2003, ACM T GRAPHIC, V22, P287, DOI 10.1145/882262.882265
   Debevec P, 2000, COMP GRAPH, P145, DOI 10.1145/344779.344855
   DEBONET JS, 1997, COMPUTER GRAPHICS, P361
   Desbrun M, 2002, COMPUT GRAPH FORUM, V21, P209, DOI 10.1111/1467-8659.00580
   Desbrun M, 1999, COMP GRAPH, P317, DOI 10.1145/311535.311576
   Drori I, 2003, ACM T GRAPHIC, V22, P303, DOI 10.1145/882262.882267
   Duchamp T., 1997, Hierarchical computation of PL harmonic embeddings
   Eck M., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P173, DOI 10.1145/218380.218440
   Efros AA, 2001, COMP GRAPH, P341, DOI 10.1145/383259.383296
   EFROS AA, 1999, TEXTURE SYNTHESIS NO, P1033
   Fleischer K. W., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P239, DOI 10.1145/218380.218447
   Floater MS, 1997, COMPUT AIDED GEOM D, V14, P231, DOI 10.1016/S0167-8396(96)00031-3
   Floater MS, 2003, COMPUT AIDED GEOM D, V20, P19, DOI 10.1016/S0167-8396(03)00002-5
   FLOATER MS, 2004, ADV MULTIRESOLUTION, P259
   Foley J., 1992, Computer Graphics: Principles and Practice
   Garber D., 1981, THESIS U SO CALIFORN
   GREENE N, 1986, IEEE COMPUT GRAPH, V6, P21, DOI 10.1109/MCG.1986.276658
   Guskov I, 2000, COMP GRAPH, P95, DOI 10.1145/344779.344831
   Haker S, 2000, IEEE T VIS COMPUT GR, V6, P181, DOI 10.1109/2945.856998
   Harrison P, 2001, W S C G ' 2001, VOLS I & II, CONFERENCE PROCEEDINGS, P190
   Heeger D. J., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P229, DOI 10.1145/218380.218446
   Hertzmann A, 2001, COMP GRAPH, P327, DOI 10.1145/383259.383295
   Hirani A. N., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P269, DOI 10.1145/237170.237264
   Hoppe H., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P99, DOI 10.1145/237170.237216
   HORMANN K, 1999, CURVE SURFACE DESIGN, P153
   IGARASHI T., 2001, P 2001 S INTERACTIVE, P209
   Igehy H, 1997, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL III, P186, DOI 10.1109/ICIP.1997.632049
   Kahler K., 2002, Eurographics Symp. on Comp. Animation, P55, DOI DOI 10.1145/545261.545271
   Khodakovsky A, 2003, ACM T GRAPHIC, V22, P350, DOI 10.1145/882262.882275
   Lee A. W. F., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P95, DOI 10.1145/280814.280828
   Lee WS, 2000, IMAGE VISION COMPUT, V18, P355, DOI 10.1016/S0262-8856(99)00057-8
   Lévy B, 2002, ACM T GRAPHIC, V21, P362, DOI 10.1145/566570.566590
   Lévy B, 2001, COMP GRAPH, P417, DOI 10.1145/383259.383308
   Liang L, 2001, ACM T GRAPHIC, V20, P127, DOI 10.1145/501786.501787
   Maillot J., 1993, Computer Graphics Proceedings, P27, DOI 10.1145/166117.166120
   MARSCHNER S, 1999, P 10 EUR WORKSH REND, P131
   MIYATA K, 1990, COMPUTER GRAPHICS, P387
   NEALEN A, 2003, P EUR S REND, P97
   NEUGEBAUER PJ, 1999, EUROGRAPHICS, V18, pC245
   Oh BM, 2001, COMP GRAPH, P433
   Oliveira M., 2001, P INT C VIS IM IM PR, P261
   Pérez P, 2003, ACM T GRAPHIC, V22, P313, DOI 10.1145/882262.882269
   PERONA P, 1990, IEEE T PATTERN ANAL, V12, P629, DOI 10.1109/34.56205
   Pighin F., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P75, DOI 10.1145/280814.280825
   Piponi D, 2000, COMP GRAPH, P471, DOI 10.1145/344779.344990
   POPAT K, 1993, P SOC PHOTO-OPT INS, V2094, P756, DOI 10.1117/12.157992
   PRAUN E, 2003, ACM T GRAPH
   Rocchini C., 1999, Proceeding of 10th Eurographics Workshop on Rendering, P127
   SAINTMARC P, 1991, IEEE T PATTERN ANAL, V13, P514, DOI 10.1109/34.87339
   Sander PedroV., 2002, EGRW 02, P87
   Sander PV, 2001, COMP GRAPH, P409, DOI 10.1145/383259.383307
   Shannon C. E., 1948, BELL SYST TECH J, V27, P379, DOI DOI 10.1002/J.1538-7305.1948.TB01338.X
   Sheffer A, 2002, VIS 2002: IEEE VISUALIZATION 2002, PROCEEDINGS, P291, DOI 10.1109/VISUAL.2002.1183787
   Sheffer A, 2001, ENG COMPUT-GERMANY, V17, P326, DOI 10.1007/PL00013391
   Simoncelli EP, 1998, 1998 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL 1, P62, DOI 10.1109/ICIP.1998.723417
   SMITH SM, 1995, TR95SMS1C DEF RES AG
   Soler C, 2002, ACM T GRAPHIC, V21, P673, DOI 10.1145/566570.566635
   Sorkine O, 2002, VIS 2002: IEEE VISUALIZATION 2002, PROCEEDINGS, P355, DOI 10.1109/VISUAL.2002.1183795
   Soucy M, 1996, VISUAL COMPUT, V12, P503, DOI 10.1007/s003710050082
   Szummer M, 1996, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, PROCEEDINGS - VOL III, P823, DOI 10.1109/ICIP.1996.560871
   Tarini M, 2002, PROC GRAPH INTERF, P89
   Tomasi C, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P839, DOI 10.1109/ICCV.1998.710815
   TURK G, 1991, COMP GRAPH, V25, P289, DOI 10.1145/127719.122749
   Turk G, 2001, COMP GRAPH, P347, DOI 10.1145/383259.383297
   Wei LY, 2001, COMP GRAPH, P355, DOI 10.1145/383259.383298
   Wei LY, 2000, COMP GRAPH, P479, DOI 10.1145/344779.345009
   Yamauchi H, 2003, COMPUTER GRAPHICS INTERNATIONAL, PROCEEDINGS, P120, DOI 10.1109/CGI.2003.1214456
   Ying LI, 2001, SPRING EUROGRAP, P301
   ZAYER R, 2004, ADV MULTIRESOLUTION
   ZELINKA S, 2002, P 13 EUR WORKSH REND, P101
   Zhang JD, 2003, ACM T GRAPHIC, V22, P295, DOI 10.1145/882262.882266
NR 83
TC 4
Z9 5
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2005
VL 21
IS 4
BP 217
EP 241
DI 10.1007/s00371-005-0283-5
PG 25
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 928MD
UT WOS:000229274900002
DA 2024-07-18
ER

PT J
AU Jin, SS
   Lewis, RR
   West, D
AF Jin, SS
   Lewis, RR
   West, D
TI A comparison of algorithms for vertex normal computation
SO VISUAL COMPUTER
LA English
DT Article
DE vertex normals; modeling; rendering meshes
AB We investigate current vertex normal computation algorithms and evaluate their effectiveness at approximating analytically computable (and thus comparable) normals for a variety of classes of model. We find that the most accurate algorithm depends on the class and that for some classes, none of the available algorithms is particularly good. We also compare the relative speeds of all algorithms.
C1 Washington State Univ, Sch Elect Engn & Comp Sci, Richland, WA 99354 USA.
C3 Washington State University
RP Washington State Univ, Sch Elect Engn & Comp Sci, 2710 Univ Dr, Richland, WA 99354 USA.
EM bobl@tricity.wsu.edu
CR Desbrun M, 1999, COMP GRAPH, P317, DOI 10.1145/311535.311576
   Ebert D.S., 1998, Texturing Modeling A Procedural Approach, VSecond
   GOURAUD H, 1971, IEEE T COMPUT, VC 20, P623, DOI 10.1109/T-C.1971.223313
   Lorensen William E., 1987, COMPUT GRAPH, P163, DOI DOI 10.1145/37402.37422
   Max N., 1999, Journal of Graphics Tools, V4, P1, DOI 10.1080/10867651.1999.10487501
   MEYER M, 2004, THESIS CALTECH
   OVERBECK P, 1997, FORSCHUNG PRAXIS, V16, P4
   PHILLIPS M, 2000, GEOMVIEW MANUAL
   Thurmer G., 1998, Journal of Graphics Tools, V3, P43, DOI 10.1080/10867651.1998.10487487
   Treece GM, 1999, COMPUT GRAPH-UK, V23, P583, DOI 10.1016/S0097-8493(99)00076-X
   Wyvill G., 1986, Visual Computer, V2, P227, DOI 10.1007/BF01900346
NR 11
TC 73
Z9 92
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2005
VL 21
IS 1-2
BP 71
EP 82
DI 10.1007/s00371-004-0271-1
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 911OG
UT WOS:000228013300005
DA 2024-07-18
ER

PT J
AU Liverani, A
   Morigi, S
AF Liverani, A
   Morigi, S
TI Efficient 6DOF tools for free-form surface modelling
SO VISUAL COMPUTER
LA English
DT Article
DE 3D geometric modelling; non-uniform rational B-spline; real-time
   interaction; virtual reality; human-machine interaction
ID SHAPE
AB In this paper we introduce a virtual integrated surface modeller (VISM) equipped with two advanced tools for 3D surface modelling in virtual reality (VR). VISM has been implemented thinking that the immediate visual feedback greatly improve complex surface modelling and also creativity. The proposed new tools are progressive skinning (PS) and curve-over-surface shaping (COSS), both implemented on a bimanual virtual environment. PS lets the designer generate a new surface by interactively and automatically adding section curves to a non-uniform rational B-spline surface. COSS is based on curve-over-a-surface progressive constraining in order to deform an area of an existent surface. Efficient numerical solutions are proposed to achieve true interactive modelling sessions.
C1 Univ Bologna, DIEM, Bologna, Italy.
   Univ Bologna, Dept Math, Bologna, Italy.
C3 University of Bologna; University of Bologna
RP Univ Bologna, DIEM, Bologna, Italy.
EM alfredo.liverani@unibo.it
CR CHU CC, 1998, EVALUATION VIRTUAL R
   Chu CCP, 1997, COMPUT AIDED DESIGN, V29, P709, DOI 10.1016/S0010-4485(97)00021-3
   Dani TH, 1997, COMPUT AIDED DESIGN, V29, P555, DOI 10.1016/S0010-4485(96)00091-7
   Hu SM, 2001, COMPUT AIDED DESIGN, V33, P903, DOI 10.1016/S0010-4485(00)00115-9
   Hu SM, 2001, VISUAL COMPUT, V17, P370, DOI 10.1007/s003710100114
   HUMMELS C, 1997, ISATA MAGAZINE   OCT, P11
   Kalawsky RoyS., 1994, The Science of Virtual Reality and Virtual Environments
   LAMOUSIN HJ, 1994, IEEE COMPUT GRAPH, V14, P59, DOI 10.1109/38.329096
   Piegl L, 1995, NURBS BOOK
   Saad Y., 1996, Iterative Methods for Sparse Linear Systems
   SACHS E, 1991, IEEE COMPUT GRAPH, V11, P18, DOI 10.1109/38.103389
   STORK A, 2000, P ASME 2000 DES ENG
   VANDIJK CGC, 1993, P ICCG 93 BOMB IND, P271
NR 13
TC 1
Z9 1
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2004
VL 20
IS 8-9
BP 554
EP 564
DI 10.1007/s00371-004-0258-y
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 866DC
UT WOS:000224752600004
DA 2024-07-18
ER

PT J
AU Au, OKC
   Tai, CL
AF Au, OKC
   Tai, CL
TI Sampling-sensitive multiresolution hierarchy for irregular meshes
SO VISUAL COMPUTER
LA English
DT Article
DE meshes; multiresolution; irregular connectivity; sampling sensitive
AB Previous approaches of constructing multiresolution hierarchy for irregular meshes investigated how to overcome the connectivity and topology constraints during the decomposition, but did not consider the effects of sampling information on editing and signal processing operations. We propose a sampling-sensitive downsampling strategy and design a decomposition framework that produces a hierarchy of meshes with decreasing maximum sampling rates and increasingly regular vertex support sizes. The resulting mesh hierarchy has good quality triangles and enables more stable editing. The detail vectors better approximate the frequency spectrum of the mesh, thus making signal filtering more accurate.
C1 Hong Kong Univ Sci & Technol, Dept Comp Sci, Kowloon, Hong Kong, Peoples R China.
C3 Hong Kong University of Science & Technology
RP Au, OKC (corresponding author), Hong Kong Univ Sci & Technol, Dept Comp Sci, Kowloon, Hong Kong, Peoples R China.
EM oscarau@cs.ust.hk; taicl@cs.ust.hk
OI AU, Kin Chung/0000-0002-1401-6201
CR Alliez P, 2002, ACM T GRAPHIC, V21, P347, DOI 10.1145/566570.566588
   DESBRUN M, 1999, P SIGGRAPH 99, V99, P317
   Eck M, 1995, P 22 ANN C COMP GRAP, P173, DOI DOI 10.1145/218380.218440
   Finkelstein A., 1994, P SIGGRAPH, P261, DOI DOI 10.1145/192161.192223
   Forsey D. R., 1988, Computer Graphics, V22, P205, DOI 10.1145/378456.378512
   Garland M., 1997, PROC 24 C COMPUTER G, P209, DOI DOI 10.1145/258734.258849
   Gu XF, 2002, ACM T GRAPHIC, V21, P355
   Guskov I, 2000, COMP GRAPH, P95, DOI 10.1145/344779.344831
   Guskov I, 1999, COMP GRAPH, P325, DOI 10.1145/311535.311577
   Hoppe H., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P99, DOI 10.1145/237170.237216
   Igarashi T, 1999, COMP GRAPH, P409, DOI 10.1145/311535.311602
   Khodakovsky A, 2000, COMP GRAPH, P271, DOI 10.1145/344779.344922
   Kim D, 1999, GRAPH MODEL IM PROC, V61, P363, DOI 10.1006/gmip.1999.0506
   Kobbelt L, 1998, GRAPHICS INTERFACE '98 - PROCEEDINGS, P43
   Kobbelt L, 1999, COMP GEOM-THEOR APPL, V14, P5, DOI 10.1016/S0925-7721(99)00032-2
   KOBBELT L, 1998, P SIGGRAPH 98, P105, DOI DOI 10.1145/280814.280831
   Kobbelt LP, 2000, COMPUT GRAPH FORUM, V19, pC249, DOI 10.1111/1467-8659.00417
   Lee A. W., 1998, Proceedings of the 25th annual conference on Computer graphics and interactive techniques, P95, DOI DOI 10.1145/280814.280828
   Lindstrom P, 1998, VISUALIZATION '98, PROCEEDINGS, P279, DOI 10.1109/VISUAL.1998.745314
   Taubin G., 1995, P 22 ANN C COMP GRAP, P351, DOI DOI 10.1145/218380.218473
   Taubin G., 2000, EUROGRAPHICS
   ZORIN D, 1997, P SIGGRAPH 97, P259
   Zorin D, 2000, SIGGRAPH 2000 COURSE
NR 23
TC 0
Z9 0
U1 0
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2004
VL 20
IS 7
BP 479
EP 493
DI 10.1007/s00371-004-0253-3
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 863IZ
UT WOS:000224556200004
DA 2024-07-18
ER

PT J
AU Bischoff, S
   Kobbelt, LP
AF Bischoff, S
   Kobbelt, LP
TI Parameterization-free active contour models with topology control
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 4th Israel-Korea Bi-National Conference on Geometric Modeling and
   Computer Graphics
CY FEB 12-14, 2003
CL Tel Aviv Univ, Tel Aviv, ISRAEL
HO Tel Aviv Univ
DE active contour model; topology control; implicit parameterization
ID DEFORMABLE SURFACES; 3-D IMAGES; SEGMENTATION; CURVATURE; BALLOONS
AB We present a novel approach for representing and evolving deformable active contours by restricting the movement of the contour vertices to the grid lines of a uniform lattice. This restriction implicitly controls the (re)parameterization of the contour and hence makes it possible to employ parameterization-independent evolution rules. Moreover, the underlying uniform grid makes self-collision detection very efficient. Our contour model is also able to perform topology changes, but - more importantly - it can detect and handle selfcollisions at subpixel precision. In applications where topology changes are not appropriate, we generate contours that touch themselves without any gaps or self-intersections.
C1 Rhein Westfal TH Aachen, Lehrstuhl Informat 8, Comp Graph Grp, D-52056 Aachen, Germany.
C3 RWTH Aachen University
RP Rhein Westfal TH Aachen, Lehrstuhl Informat 8, Comp Graph Grp, D-52056 Aachen, Germany.
EM bischoff@informatik.rwth-aachen.de; kobbelt@informatik.rwth-aachen.de
OI Kobbelt, Leif/0000-0002-7880-9470
CR [Anonymous], 2002, SURFACES
   [Anonymous], P NAT ACAD SCI
   BERGER MO, 1990, LECT NOTES COMPUT SC, V427, P570
   CHOPP DL, 1993, J COMPUT PHYS, V106, P77, DOI 10.1006/jcph.1993.1092
   COHEN I, 1992, CVGIP-IMAG UNDERSTAN, V56, P242, DOI 10.1016/1049-9660(92)90041-Z
   COHEN LD, 1991, CVGIP-IMAG UNDERSTAN, V53, P211, DOI 10.1016/1049-9660(91)90028-N
   COHEN LD, 1993, IEEE T PATTERN ANAL, V15, P1131, DOI 10.1109/34.244675
   Davatzikos C, 1996, IEEE T MED IMAGING, V15, P112, DOI 10.1109/42.481446
   DECARLO D, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P566, DOI 10.1109/CVPR.1994.323883
   Delingette H, 2001, COMPUT VIS IMAGE UND, V83, P140, DOI 10.1006/cviu.2001.0920
   Gage M., 1985, CONT MATH, V51, P51, DOI DOI 10.1090/CONM/051/848933
   GUPTA A, 1994, P IEEE C COMP CARD, P661
   Han X, 2001, PROC CVPR IEEE, P765
   Hug J, 1999, LECT NOTES COMPUT SC, V1679, P106
   KASS M, 1987, INT J COMPUT VISION, V1, P321, DOI 10.1007/BF00133570
   KIMIA BB, 1992, J MATH ANAL APPL, V163, P438, DOI 10.1016/0022-247X(92)90260-K
   Lachaud J O, 1999, Med Image Anal, V3, P187, DOI 10.1016/S1361-8415(99)80012-7
   LOBREGT S, 1995, IEEE T MED IMAGING, V14, P12, DOI 10.1109/42.370398
   LORENSEN WE, 1987, P 14 ANN C COMP GRAP, P163, DOI DOI 10.1145/3740137422
   McInerney T, 1996, Med Image Anal, V1, P91, DOI 10.1016/S1361-8415(96)80007-7
   McInerney T, 2000, MED IMAGE ANAL, V4, P73, DOI 10.1016/S1361-8415(00)00008-6
   McInerney T, 1999, IEEE T MED IMAGING, V18, P840, DOI 10.1109/42.811261
   MCINERNEY T, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P840, DOI 10.1109/ICCV.1995.466850
   MILLER JV, 1991, P SIGGRAPH 91, P217
   OSHER S, 1988, J COMPUT PHYS, V79, P12, DOI 10.1016/0021-9991(88)90002-2
   Rougon N, 1998, J ELECTRON IMAGING, V7, P231, DOI 10.1117/1.482641
   Sethian J., 1999, LEVEL SET METHODS FA
NR 27
TC 12
Z9 22
U1 0
U2 4
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2004
VL 20
IS 4
BP 217
EP 228
DI 10.1007/s00371-003-0228-9
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 834XR
UT WOS:000222444700002
DA 2024-07-18
ER

PT J
AU Kozhekin, N
   Savchenko, V
   Senin, M
   Hagiwara, I
AF Kozhekin, N
   Savchenko, V
   Senin, M
   Hagiwara, I
TI An approach to surface retouching and mesh smoothing
SO VISUAL COMPUTER
LA English
DT Article
DE radial-basis functions; space mapping; surface retouching; mesh
   smoothing
ID INTERPOLATION
AB In this paper, we discuss a novel, fast, practical algorithm for surface modification of geometric objects. A space-mapping technique is used to transform a given or damaged part of a surface into a different shape in a continuous manner. The proposed approach is used for surface-retouching and mesh-smoothing problems. The technique, in fact, is based on a local processing of polygonal data that can be applied to the fairing of 3D meshes. We consider shape transformation as a general type of operation for surface modification and attempt to approach the problem from a single point of view, namely, that of the space-mapping technique based on the implementation of radial-basis functions. Experimental results are included to demonstrate the functionality of our mesh-modeling tool.
C1 Tokyo Inst Technol, Fac Engn, Meguro Ku, Tokyo 1528552, Japan.
   Hosei Univ, Fac Comp & Informat Sci, Koganei, Tokyo 1848584, Japan.
   Moscow Inst Phys & Technol, Moscow 113303, Russia.
C3 Tokyo Institute of Technology; Hosei University; Moscow Institute of
   Physics & Technology
RP Tokyo Inst Technol, Fac Engn, Meguro Ku, 2-12-1 Ookayama, Tokyo 1528552, Japan.
EM karlson@stu.mech.titech.ac.jp; vsavchen@k.hosei.ac.jp; m_senin@mail.ru;
   hagiwara@mech.titech.ac.jp
CR Ahlberg J.H., 1967, The Theory of Splines and Their Applications
   [Anonymous], 2002, NUMERICAL RECIPES C
   [Anonymous], 119 U CANT MATH DEP
   [Anonymous], P EUR REAL TIM 3D DE
   [Anonymous], WAVELETS IMAGES SURF
   [Anonymous], 1981, Computer Solution of Large Sparse Positive Definite Systems
   BAREQUET G, 1993, 27793 TEL AV U DEP C
   Bertalmio M, 2000, COMP GRAPH, P417, DOI 10.1145/344779.344972
   Bloor MIG, 1996, COMPUT AIDED DESIGN, V28, P145, DOI 10.1016/0010-4485(95)00060-7
   BOLLE RM, 1991, IEEE T PATTERN ANAL, V13, P1, DOI 10.1109/34.67626
   Bookstein F., 1997, MORPHOMETRIC TOOLS L
   BOOKSTEIN FL, 1989, IEEE T PATTERN ANAL, V11, P567, DOI 10.1109/34.24792
   Carr JC, 1997, IEEE T MED IMAGING, V16, P96, DOI 10.1109/42.552059
   Carr JC, 2001, COMP GRAPH, P67, DOI 10.1145/383259.383266
   Chen M, 2000, VOLUME GRAPHICS, P97
   Cormen T.H., 1992, INTRO ALGORITHMS
   CRAVEN P, 1979, NUMER MATH, V31, P377, DOI 10.1007/BF01437407
   Davis J., 2002, P 1 INT S 3D DAT PRO
   DESBRUN M, 1999, P SIGGRAPH 99, V33, P317
   DUSHON J, 1976, CONSTRUCTIVE THEORY, P85
   ESEDOGLU S, 2002, IN PRESS EUR J APPL
   Farin G., 1998, CURVES SURFACES CAGD
   GOUSIE MK, 1998, P 8 INT S SPAT DAT H
   GREINER G, 1994, WAVELETS IMAGES SURF, P277
   Hermann T., 1997, Geometric Modeling: Theory and Practice. State of the Art, P14
   HOPPE H, 1992, COMP GRAPH, V26, P71, DOI 10.1145/142920.134011
   JENNINGS A, 1966, COMPUT J, V9, P281, DOI 10.1093/comjnl/9.3.281
   Kobbelt L., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P105, DOI 10.1145/280814.280831
   Kojekine N, 2003, COMPUT GRAPH-UK, V27, P311, DOI 10.1016/S0097-8493(02)00287-X
   Kojekine N., 2002, Proceedings of Second IASTED International Conference Visualization, Imaging, and Image Processing, P613
   Lee S, 1997, IEEE T VIS COMPUT GR, V3, P228, DOI 10.1109/2945.620490
   LEE SY, 1995, P SIGGRAPH 95, P439
   LITWINOWICZ P, 1994, P SIGGRAPH 94, P409
   Morse BS, 2001, INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, PROCEEDING, P89, DOI 10.1109/SMA.2001.923379
   OHTAKE Y, 2003, P SMI 2003 SEOUL S K
   Oliveira M., 2001, P INT C VIS IM IM PR, P261
   SARTI A, 2001, P VIS IM IM PROC IAS, P495
   Savchenko V, 2002, FIRST INTERNATIONAL SYMPOSIUM ON CYBER WORLDS, PROCEEDINGS, P480, DOI 10.1109/CW.2002.1180916
   Savchenko V, 2002, ADVANCES IN MODELLING, ANIMATION AND RENDERING, P139
   SAVCHENKO V, 2001, P VIS IM IM PROC C V, P237
   Savchenko V., 2001, P 6 ACM S SOL MOD AP, P39
   SAVCHENKO VV, 1995, COMPUT GRAPH FORUM, V14, P181, DOI 10.1111/1467-8659.1440181
   SCHNEIDER B, 1998, P 8 S SPAT DAT HANDL
   SETIAN JA, 1996, GEOMETRY FLUID MECH
   Skaria S, 2001, INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, PROCEEDING, P216, DOI 10.1109/SMA.2001.923393
   Taubin G., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P351, DOI 10.1145/218380.218473
   Turk G., 1994, Computer Graphics Proceedings. Annual Conference Series 1994. SIGGRAPH 94 Conference Proceedings, P311, DOI 10.1145/192161.192241
   Turk G, 1999, COMP GRAPH, P335, DOI 10.1145/311535.311580
   Vasilenko V. A., 1983, Spline Functions: Theory, Algorithms, and Programs
   Warren J., 2002, SUBDIVISION METHODS
   Wendland H, 1999, J COMPUT APPL MATH, V101, P177, DOI 10.1016/S0377-0427(98)00218-0
   Wendland H., 1995, Advances in Computational Mathematics, V4, P389, DOI 10.1007/BF02123482
   Whitaker RT, 1998, P 3 INT WORKSH IMPL, P19
   Wyvill B, 1997, 1997 INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, PROCEEDINGS, P205, DOI 10.1109/SMA.1997.634898
   Yagou H, 2003, COMPUTER GRAPHICS INTERNATIONAL, PROCEEDINGS, P28, DOI 10.1109/CGI.2003.1214444
   Yagou H, 2002, FIRST INTERNATIONAL SYMPOSIUM ON CYBER WORLDS, PROCEEDINGS, P488, DOI 10.1109/CW.2002.1180917
   Zhang H, 2002, ADVANCES IN MODELLING, ANIMATION AND RENDERING, P167
NR 57
TC 8
Z9 9
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2003
VL 19
IS 7-8
BP 549
EP 564
DI 10.1007/s00371-003-0218-y
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 749YL
UT WOS:000186957600011
DA 2024-07-18
ER

PT J
AU Su, GD
   Chang, CC
   Lin, CC
   Chang, CC
AF Su, Guo-Dong
   Chang, Ching-Chun
   Lin, Chia-Chen
   Chang, Chin-Chen
TI Towards property-preserving JPEG encryption with structured permutation
   and adaptive group differentiation
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE JPEG; Structured permutation; Adaptive group differentiation; Format
   compatibility; File size preservation
ID JOINT IMAGE COMPRESSION; CHAOTIC SYSTEM; SCHEME
AB The security and privacy of digital images are a major concern in cyberspace. JPEG is the most widely used image compression standard and yet there are problems with format compatibility and file size preservation in most of the modern encryption schemes for JPEG images. To address these problems, we propose a novel properties-preserving encryption scheme that obfuscates JPEG images by scrambling AC and DC coefficients. The key observation is that an innovative structured permutation has a remarkable effect on enciphering the coefficients and simultaneously prevents value overflow as well as size increment. On this basis, we first leverage the row-wise reordering and reversing technique to disorganize the DC coefficients. Afterward, a series of groups of consecutive DC differential values are produced using adaptive group differentiation and each of them is independently permuted inside. Finally, we perform block-wise ACCs permutation and intra-block ACCs shuffling to further confuse the outline of the original image. Moreover, invariant features regarding the coefficients are incorporated in the key generation process to resist various cryptanalytic attacks. Experimental results demonstrate a significant improvement in terms of format compatibility and file size preservation, and verify the security against various attacks.
C1 [Su, Guo-Dong] Fujian Polytech Normal Univ, Sch Big Data & Artificial Intelligence, Fuzhou 350300, Peoples R China.
   [Su, Guo-Dong; Chang, Ching-Chun; Chang, Chin-Chen] Feng Chia Univ, Dept Informat Engn & Comp Sci, Taichung 40724, Taiwan.
   [Su, Guo-Dong] Fujian Prov Univ, Fujian Polytech Normal Univ, Engn Res Ctr ICH Digitalizat & Multisource Informa, Fuzhou, Peoples R China.
   [Lin, Chia-Chen] Natl Chin Yi Univ Technol, Dept Comp Sci & Informat Engn, Taichung 41170, Taiwan.
C3 Fujian Polytechnic Normal University; Feng Chia University; Fujian
   Polytechnic Normal University; Fuzhou University; National Chin-Yi
   University of Technology
RP Lin, CC (corresponding author), Natl Chin Yi Univ Technol, Dept Comp Sci & Informat Engn, Taichung 41170, Taiwan.
EM gdsu@fpnu.edu.cn; ccc@fcu.edu.tw; ally.cclin@ncut.edu.tw;
   alan3c@gmail.com
FU Natural Science Foundation of Fujian Province [62272103]; Natural
   Science Foundation of China [2020J01300, 2022J01971, 2022J01974,
   2022J01975]; Natural Science Foundation of Fujian Province [G3-KF2205];
   Open Fund of Engineering Research Center for ICH Digitalization and
   Multi-source Information Fusion (Fujian Polytechnic Normal University)
FX This work was supported in part by the Natural Science Foundation of
   China under Grant No. 62272103, and in part by the Natural Science
   Foundation of Fujian Province under Grant Nos. 2020J01300, 2022J01971,
   2022J01974, and 2022J01975, and in part by the Open Fund of Engineering
   Research Center for ICH Digitalization and Multi-source Information
   Fusion (Fujian Polytechnic Normal University) under Grant No. G3-KF2205.
CR [Anonymous], DOTA Dataset
   [Anonymous], BOWS-2 Dataset
   [Anonymous], CorelDraw Dataset
   Chang JC, 2017, SIGNAL PROCESS, V133, P135, DOI 10.1016/j.sigpro.2016.11.003
   Chen CH, 2002, VISUAL COMPUT, V18, P29, DOI 10.1007/s003710100130
   Cheng H, 2016, EURASIP J INF SECUR, DOI 10.1186/s13635-015-0028-6
   Cheng H, 2016, J VIS COMMUN IMAGE R, V40, P111, DOI 10.1016/j.jvcir.2016.06.016
   Chuman T, 2019, IEEE T INF FOREN SEC, V14, P1515, DOI 10.1109/TIFS.2018.2881677
   He JH, 2019, IEEE T CIRC SYST VID, V29, P3501, DOI 10.1109/TCSVT.2018.2882850
   He JH, 2018, IEEE T MULTIMEDIA, V20, P2645, DOI 10.1109/TMM.2018.2817065
   Hua ZY, 2019, INFORM SCIENCES, V480, P403, DOI 10.1016/j.ins.2018.12.048
   Ince IF, 2022, VISUAL COMPUT, V38, P1845, DOI 10.1007/s00371-022-02418-0
   Int. Telecommunication Union, 1992, ITU Std. T., V81
   Ji XY, 2015, COMMUN NONLINEAR SCI, V22, P321, DOI 10.1016/j.cnsns.2014.09.011
   Khan NA, 2021, MULTIMED TOOLS APPL, V80, P9639, DOI 10.1007/s11042-020-10110-5
   Li B, 2021, INFORM SCIENCES, V575, P379, DOI 10.1016/j.ins.2021.06.016
   Li CL, 2024, VISUAL COMPUT, V40, P731, DOI 10.1007/s00371-023-02812-2
   Li PY, 2020, IET SIGNAL PROCESS, V14, P475, DOI 10.1049/iet-spr.2019.0276
   Li PY, 2019, J VIS COMMUN IMAGE R, V58, P12, DOI 10.1016/j.jvcir.2018.11.018
   Li PY, 2018, IEEE T MULTIMEDIA, V20, P1960, DOI 10.1109/TMM.2017.2786860
   Li PY, 2017, J VIS COMMUN IMAGE R, V44, P61, DOI 10.1016/j.jvcir.2017.01.021
   Li SS, 2016, KSII T INTERNET INF, V10, P1790, DOI 10.3837/tiis.2016.04.018
   Lian S., 2008, MULTIMEDIA CONTENT E, DOI [10.1201/9781420065282, DOI 10.1201/9781420065282]
   Liang HH, 2019, J VIS COMMUN IMAGE R, V61, P149, DOI 10.1016/j.jvcir.2019.03.021
   Minemura K, 2017, MULTIMED TOOLS APPL, V76, P6709, DOI 10.1007/s11042-016-3338-x
   Minemura K, 2012, IEEE IMAGE PROC, P261, DOI 10.1109/ICIP.2012.6466845
   Niu XA, 2008, 2008 FOURTH INTERNATIONAL CONFERENCE ON INTELLIGENT INFORMATION HIDING AND MULTIMEDIA SIGNAL PROCESSING, PROCEEDINGS, P308, DOI 10.1109/IIH-MSP.2008.207
   Ong S, 2013, IEEE IMAGE PROC, P4574, DOI 10.1109/ICIP.2013.6738942
   Ong SY, 2015, SIGNAL PROCESS-IMAGE, V31, P47, DOI 10.1016/j.image.2014.11.008
   Qian ZX, 2019, IEEE T CIRC SYST VID, V29, P351, DOI 10.1109/TCSVT.2018.2797897
   Qian ZX, 2014, IEEE T MULTIMEDIA, V16, P1486, DOI 10.1109/TMM.2014.2316154
   Qin C, 2023, IEEE T MULTIMEDIA, V25, P2528, DOI 10.1109/TMM.2022.3148591
   Schaefer G, 2004, PROC SPIE, V5307, P472, DOI 10.1117/12.525375
   Shreyamsha Kumar BK, 2010, SIGNAL IMAGE VIDEO P, V4, P419, DOI 10.1007/s11760-009-0131-6
   Su GD, 2023, VISUAL COMPUT, V39, P4623, DOI 10.1007/s00371-022-02613-z
   Su GD, 2020, IEEE ACCESS, V8, P26984, DOI 10.1109/ACCESS.2020.2966234
   Talhaoui MZ, 2021, VISUAL COMPUT, V37, P541, DOI 10.1007/s00371-020-01822-8
   Unterweger A., 2012, P MULT SEC MM SEC NE, P85
   Xu YY, 2014, J VIS COMMUN IMAGE R, V25, P805, DOI 10.1016/j.jvcir.2014.01.005
   Yi S, 2019, IEEE T MULTIMEDIA, V21, P51, DOI 10.1109/TMM.2018.2844679
   Yuan Y., 2021, INT WORKSHOP DIGITAL, P58
   Zhang LY, 2018, IEEE T CYBERNETICS, V48, P1163, DOI 10.1109/TCYB.2017.2682561
   Zhenxing Qian, 2018, IEEE Transactions on Dependable and Secure Computing, V15, P1055, DOI 10.1109/TDSC.2016.2634161
NR 43
TC 0
Z9 0
U1 14
U2 14
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2023 DEC 27
PY 2023
DI 10.1007/s00371-023-03174-5
EA DEC 2023
PG 27
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DK1M4
UT WOS:001131839600001
DA 2024-07-18
ER

PT J
AU Song, XG
   Zhang, DD
   Liang, L
   He, M
   Hei, XH
AF Song, Xiaogang
   Zhang, Dongdong
   Liang, Li
   He, Min
   Hei, Xinhong
TI Local motion feature extraction and spatiotemporal attention mechanism
   for action recognition
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Action recognition; Spatiotemporal attention; Convolution neural
   network; Abnormal behavior
AB Video action recognition faces the important and challenging problem of spatiotemporal relationship modeling. In order to solve this issue, current methods typically rely on 2D or 3D CNN operations to model local spatiotemporal dependencies at fixed scales. However, most of these models fail to emphasize the keyframes and action-sensitive regions of the input video, resulting in poor performance. In this paper, an action recognition network with local motion feature extraction and spatiotemporal attention mechanism is proposed. The proposed network consists of a motion capture (MC) module and a temporal attention (TA) and spatiotemporal attention (STA) module, which capture detailed motion features, and learns the contribution of each frame and each region to the action at the feature level, respectively. To evaluate our network, we construct a concrete water addition violation dataset (CWAVD), which can be used to identify water addition violations by construction site workers and improve construction management efficiency and quality. The proposed network achieves the state-of-the-art performance on three of the most challenging datasets, UCF101 (97.6%), HMDB51 (77.3%) and SSV2 (67.8%).
C1 [Song, Xiaogang; Zhang, Dongdong; Liang, Li; Hei, Xinhong] Xian Univ Technol, Sch Comp Sci & Engn, Xian, Peoples R China.
   [He, Min] Xian Univ Technol, Sch Civil Engn & Architecture, Xian, Peoples R China.
C3 Xi'an University of Technology; Xi'an University of Technology
RP Song, XG (corresponding author), Xian Univ Technol, Sch Comp Sci & Engn, Xian, Peoples R China.
EM songxg@xaut.edu.cn
RI Zhang, Dongdong/AAU-6846-2021
OI Zhang, Dongdong/0000-0002-0091-1803; Song, Xiaogang/0000-0001-9841-9624
FU National Key R&D Program of China;  [2022YFB2602203]
FX This work was supported by the National Key R&D Program of China (No.
   2022YFB2602203).
CR Abdelbaky A, 2021, VISUAL COMPUT, V37, P1821, DOI 10.1007/s00371-020-01940-3
   Bobick AF, 2001, IEEE T PATTERN ANAL, V23, P257, DOI 10.1109/34.910878
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Crasto N, 2019, PROC CVPR IEEE, P7874, DOI 10.1109/CVPR.2019.00807
   Cui Y., 2021, P IEEECVF INT C COMP, P8138
   Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89
   Diba A, 2018, LECT NOTES COMPUT SC, V11208, P299, DOI 10.1007/978-3-030-01225-0_18
   Dong WK, 2022, PATTERN RECOGN, V130, DOI 10.1016/j.patcog.2022.108797
   Dosovitskiy A, 2015, IEEE I CONF COMP VIS, P2758, DOI 10.1109/ICCV.2015.316
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Fei K., 2022, Vis. Comput, V2022, P1
   Fujiyoshi H, 1998, FOURTH IEEE WORKSHOP ON APPLICATIONS OF COMPUTER VISION - WACV'98, PROCEEDINGS, P15, DOI 10.1109/ACV.1998.732852
   Geng TT, 2022, IEEE T IMAGE PROCESS, V31, P5484, DOI 10.1109/TIP.2022.3196175
   Geng Z., 2021, 2021 INT C LEARN REP
   Goyal R, 2017, IEEE I CONF COMP VIS, P5843, DOI 10.1109/ICCV.2017.622
   HORN BKP, 1981, ARTIF INTELL, V17, P185, DOI 10.1016/0004-3702(81)90024-2
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Ilg E, 2017, PROC CVPR IEEE, P1647, DOI 10.1109/CVPR.2017.179
   Imen J., 2020, Forens. Sci. Int. Digit. Investig, V32
   Kim J, 2021, PATTERN RECOGN, V119, DOI 10.1016/j.patcog.2021.108068
   Kuehne H, 2011, IEEE I CONF COMP VIS, P2556, DOI 10.1109/ICCV.2011.6126543
   Laptev I, 2005, INT J COMPUT VISION, V64, P107, DOI 10.1007/s11263-005-1838-7
   Li JP, 2021, NEUROCOMPUTING, V459, P338, DOI 10.1016/j.neucom.2021.06.088
   Li Y, 2020, PROC CVPR IEEE, P906, DOI 10.1109/CVPR42600.2020.00099
   Lin J, 2019, IEEE I CONF COMP VIS, P7082, DOI 10.1109/ICCV.2019.00718
   Liu ZY, 2020, AAAI CONF ARTIF INTE, V34, P11669
   Lu YW, 2023, PROC CVPR IEEE, P18063, DOI 10.1109/CVPR52729.2023.01732
   Piergiovanni AJ, 2019, PROC CVPR IEEE, P9937, DOI 10.1109/CVPR.2019.01018
   Qiu ZX, 2023, VISUAL COMPUT, V39, P2191, DOI 10.1007/s00371-022-02473-7
   Sang HF, 2019, IEEE ACCESS, V7, P118388, DOI 10.1109/ACCESS.2019.2936628
   Simonyan K, 2014, ADV NEUR IN, V27
   Soomro K, 2012, Arxiv, DOI arXiv:1212.0402
   Sun DQ, 2010, PROC CVPR IEEE, P2432, DOI 10.1109/CVPR.2010.5539939
   Wang LM, 2021, PROC CVPR IEEE, P1895, DOI 10.1109/CVPR46437.2021.00193
   Wang LM, 2016, LECT NOTES COMPUT SC, V9912, P20, DOI 10.1007/978-3-319-46484-8_2
   Wang LM, 2018, PROC CVPR IEEE, P1430, DOI 10.1109/CVPR.2018.00155
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Xie Zeke, 2021, INT C LEARN REPR
   Yan L., 2022, 2022 INT JOINT C ART
   Zhang GH, 2023, VISUAL COMPUT, V39, P539, DOI 10.1007/s00371-021-02355-4
   Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319
   Zhu Y, 2019, LECT NOTES COMPUT SC, V11363, P363, DOI 10.1007/978-3-030-20893-6_23
   Zhu YS, 2020, VISUAL COMPUT, V36, P1771, DOI 10.1007/s00371-019-01770-y
   Zolfaghari M, 2018, LECT NOTES COMPUT SC, V11206, P713, DOI 10.1007/978-3-030-01216-8_43
NR 44
TC 0
Z9 0
U1 8
U2 11
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2023 DEC 19
PY 2023
DI 10.1007/s00371-023-03205-1
EA DEC 2023
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CQ6V8
UT WOS:001126758000002
DA 2024-07-18
ER

PT J
AU Wei, L
   Lai, CH
AF Wei, Lin
   Lai, Chenghui
TI Wall segmentation in house plans: fusion of deep learning and
   traditional methods
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE House plan segmentation; Deep learning; Image preprocessing; Smoothing
   optimisation
AB Recognition and extraction of elements from house plans present significant challenges in the construction, decoration and interior design industries. To address this issue, this paper proposes a wall segmentation system for house plans that integrates deep learning and traditional methods. The system comprises several components, such as image preprocessing, main region extraction, wall segmentation and optimisation of wall smoothing. The study combined the rapidity of the traditional method with the robustness of deep learning to enable the extraction of walls from varied image styles and perform smoothing optimisation. The paper demonstrates that the proposed segmentation technique delivers an 89% mean intersection over union, a 94% detection rate and a 96% recognition accuracy. The research surpasses current findings in the same field. Additionally, when combined with the current house map dataset, the system presents a semantic categorisation dataset featuring 6000 images depicting a range of styles, in addition to a recognition dataset including 4000 images.
C1 [Wei, Lin; Lai, Chenghui] Fuzhou Univ, Fuzhou 350108, Fujian, Peoples R China.
C3 Fuzhou University
RP Wei, L (corresponding author), Fuzhou Univ, Fuzhou 350108, Fujian, Peoples R China.
EM 192858520@qq.com; Chenghui271@foxmail.com
OI Wei, Lin/0009-0008-4609-3050
CR Agarwal Vartika, 2022, Sustainable Advanced Computing: Select Proceedings of ICSAC 2021. Lecture Notes in Electrical Engineering (840), P161, DOI 10.1007/978-981-16-9012-9_14
   Agarwal V., 2022, International Journal of Image, Graphics and Signal Processing (IJIGSP), V14, P25, DOI 10.5815/ijigsp.2022.02.03
   Ahmed S., 2012, Proceedings of the 10th IAPR International Workshop on Document Analysis Systems (DAS 2012), P339, DOI 10.1109/DAS.2012.22
   Ahmed S, 2014, PATTERN RECOGN LETT, V35, P91, DOI 10.1016/j.patrec.2013.04.005
   Ahmed S, 2011, PROC INT CONF DOC, P734, DOI 10.1109/ICDAR.2011.153
   Ahti K., 2019, CubiCasa5K: a dataset and an improved multi-task model for floorplan image analysis, DOI [10.1007/978-3-030-20205-7_3, DOI 10.1007/978-3-030-20205-7_3]
   Bochkovskiy A, 2020, Arxiv, DOI arXiv:2004.10934
   Bradski G., 2008, LEARNING OPENCV COMP, DOI DOI 10.1109/MRA.2009.933612
   de las Heras LP, 2014, INT J DOC ANAL RECOG, V17, P221, DOI 10.1007/s10032-013-0215-2
   Dodge S, 2017, PROCEEDINGS OF THE FIFTEENTH IAPR INTERNATIONAL CONFERENCE ON MACHINE VISION APPLICATIONS - MVA2017, P358, DOI 10.23919/MVA.2017.7986875
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Heras L.P.D.L., 2015, 2015 13 INT C DOCUME
   Hu G, 2023, ADV ENG INFORM, V57, DOI 10.1016/j.aei.2023.102004
   Liu C, 2017, IEEE I CONF COMP VIS, P2214, DOI 10.1109/ICCV.2017.241
   Liu CX, 2015, PROC CVPR IEEE, P3413, DOI 10.1109/CVPR.2015.7298963
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Lv X., 2021, Computer Vision and Pattern Recognition IEEE
   Mace S., 2010, P 9 IAPR INT WORKSH, P167, DOI 10.1145/1815330.1815352
   Qian XX, 2023, VISUAL COMPUT, V39, P87, DOI 10.1007/s00371-021-02315-y
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Wang ZY, 2023, IEEE ACCESS, V11, P63667, DOI 10.1109/ACCESS.2023.3288598
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Yang BS, 2022, IEEE J-STARS, V15, P7809, DOI 10.1109/JSTARS.2022.3205746
   Zare M, 2023, J BIONIC ENG, V20, P2359, DOI 10.1007/s42235-023-00386-2
   ZHANG TY, 1984, COMMUN ACM, V27, P236, DOI 10.1145/357994.358023
NR 26
TC 0
Z9 0
U1 1
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2023 NOV 27
PY 2023
DI 10.1007/s00371-023-03150-z
EA NOV 2023
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CB1Q4
UT WOS:001122703200002
DA 2024-07-18
ER

PT J
AU Gao, WH
   Zhang, YJ
   Jian, HC
AF Gao, Weihao
   Zhang, Yongjun
   Jian, Huachun
TI A novel attention-based network for single image dehazing
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Single image dehazing; Multi-spectral attention; Feature enhancement;
   Contrastive learning
ID QUALITY ASSESSMENT
AB As one typical severe weather, haze can exert a bad influence on vision tasks. Therefore, research on the image dehazing task is of great importance. Recent years have witnessed the remarkable success of CNN-based dehazing algorithms, showcasing their powerful capabilities. Nevertheless, despite this progress, these approaches still exhibit limitations in applications, and there remains considerable untapped potential for further refinements. To address this challenge, we develop a novel CNN-based network dubbed (MSSDN), which demonstrates superior dehazing performance and highlights its potential for real-world use cases. Specifically, we design a multi-spectral attention module (MSAM), which shows its superiority in collecting information in the channel dimension. And based on it, a haze capture module (HCM) is designed to eliminate hazy information. Meanwhile, the designed background feature capture module (BFCM) enlarges the receptive field of the proposed network to capture more useful information. Finally, the designed cross stage interaction module (CSIM) promotes the process of information flow, and the contrastive learning is adopted to decouple the haze component from the background part. The evaluation results demonstrate the superiority of the proposed algorithm, and our MSSDN outperforms the SOTA dehazing methods.
C1 [Gao, Weihao; Zhang, Yongjun; Jian, Huachun] Guizhou Univ, Coll Comp Sci & Technol, Guiyang 550025, Peoples R China.
   [Gao, Weihao; Zhang, Yongjun; Jian, Huachun] Guizhou Univ, Inst Artificial Intelligence, Coll Comp Sci & Technol, State Key Lab Publ Big Data, Guiyang 550025, Guizhou, Peoples R China.
C3 Guizhou University; Guizhou University
RP Zhang, YJ (corresponding author), Guizhou Univ, Coll Comp Sci & Technol, Guiyang 550025, Peoples R China.; Zhang, YJ (corresponding author), Guizhou Univ, Inst Artificial Intelligence, Coll Comp Sci & Technol, State Key Lab Publ Big Data, Guiyang 550025, Guizhou, Peoples R China.
EM gs.whgao21@gzu.edu.cn; zyj6667@126.com; gs.hcjian21@gzu.edu.cn
RI Zhang, Yongjun/M-1094-2013
OI Zhang, Yongjun/0000-0002-7534-1219
CR Agrawal SC, 2023, VISUAL COMPUT, V39, P5763, DOI 10.1007/s00371-022-02694-w
   AHMED N, 1974, IEEE T COMPUT, VC 23, P90, DOI 10.1109/T-C.1974.223784
   Ancuti CO, 2020, IEEE COMPUT SOC CONF, P1798, DOI 10.1109/CVPRW50498.2020.00230
   Ancuti CO, 2018, IEEE COMPUT SOC CONF, P867, DOI 10.1109/CVPRW.2018.00119
   Berman D, 2016, PROC CVPR IEEE, P1674, DOI 10.1109/CVPR.2016.185
   Cai BL, 2016, IEEE T IMAGE PROCESS, V25, P5187, DOI 10.1109/TIP.2016.2598681
   Chen DD, 2019, IEEE WINT CONF APPL, P1375, DOI 10.1109/WACV.2019.00151
   Chen ZH, 2020, VISUAL COMPUT, V36, P2189, DOI 10.1007/s00371-020-01929-y
   Ding XH, 2022, PROC CVPR IEEE, P11953, DOI 10.1109/CVPR52688.2022.01166
   Dong H, 2020, PROC CVPR IEEE, P2154, DOI 10.1109/CVPR42600.2020.00223
   Fattal R, 2014, ACM T GRAPHIC, V34, DOI 10.1145/2651362
   Guo CL, 2022, PROC CVPR IEEE, P5802, DOI 10.1109/CVPR52688.2022.00572
   He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168
   Hong M, 2022, AAAI CONF ARTIF INTE, P906
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang ZL, 2021, Arxiv, DOI arXiv:2106.03650
   Jiangxin Dong, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12375), P188, DOI 10.1007/978-3-030-58577-8_12
   Li BY, 2017, Arxiv, DOI arXiv:1707.06543
   Li BY, 2019, IEEE T IMAGE PROCESS, V28, P492, DOI 10.1109/TIP.2018.2867951
   Li KC, 2022, Arxiv, DOI [arXiv:2201.04676, DOI 10.48550/ARXIV.2201.04676]
   Liang YQ, 2022, INTEGR COMPUT-AID E, V29, P23, DOI 10.3233/ICA-210661
   Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151
   Liu LX, 2014, SIGNAL PROCESS-IMAGE, V29, P856, DOI 10.1016/j.image.2014.06.006
   Liu XH, 2019, IEEE I CONF COMP VIS, P7313, DOI 10.1109/ICCV.2019.00741
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Liu Z, 2022, PROC CVPR IEEE, P11966, DOI 10.1109/CVPR52688.2022.01167
   Long CJ, 2023, INT J INTELL SYST, V2023, DOI 10.1155/2023/9953198
   Manu C.M., 2022, VISUAL COMPUT, V38, P1
   McCartney E. J., 1976, Optics of the atmosphere. Scattering by molecules and particles
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Narasimhan SG, 2002, INT J COMPUT VISION, V48, P233, DOI 10.1023/A:1016328200723
   Nayar S. K., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P820, DOI 10.1109/ICCV.1999.790306
   Qili Deng, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12351), P722, DOI 10.1007/978-3-030-58539-6_43
   Qin X, 2020, AAAI CONF ARTIF INTE, V34, P11908
   Ren WQ, 2018, PROC CVPR IEEE, P3253, DOI 10.1109/CVPR.2018.00343
   Ren WQ, 2016, LECT NOTES COMPUT SC, V9906, P154, DOI 10.1007/978-3-319-46475-6_10
   Shu XB, 2023, IEEE T PATTERN ANAL, V45, P7559, DOI 10.1109/TPAMI.2022.3222871
   Si TZ, 2023, IEEE T MULTIMEDIA, V25, P4323, DOI 10.1109/TMM.2022.3174414
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song YD, 2023, IEEE T IMAGE PROCESS, V32, P1927, DOI 10.1109/TIP.2023.3256763
   Tang W, 2022, IEEE T IMAGE PROCESS, V31, P5134, DOI 10.1109/TIP.2022.3193288
   Tu ZZ, 2022, PROC CVPR IEEE, P5759, DOI 10.1109/CVPR52688.2022.00568
   Vaswani A, 2017, ADV NEUR IN, V30
   Wan YC, 2022, KNOWL-BASED SYST, V252, DOI 10.1016/j.knosys.2022.109244
   Wang C, 2021, KNOWL-BASED SYST, V228, DOI 10.1016/j.knosys.2021.107279
   Wang WH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P548, DOI 10.1109/ICCV48922.2021.00061
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wu HY, 2021, PROC CVPR IEEE, P10546, DOI 10.1109/CVPR46437.2021.01041
   Wu YQ, 2018, IEEE T SERV COMPUT, V11, P341, DOI 10.1109/TSC.2015.2501981
   Xiao T., 2021, NEURIPS, V34, P30392
   Xu B., 2023, IEEE T NEUR NET LEAR
   Xu B., 2023, arXiv
   Xu BQ, 2022, IEEE T IMAGE PROCESS, V31, P3852, DOI 10.1109/TIP.2022.3175605
   Yuan L, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P538, DOI 10.1109/ICCV48922.2021.00060
   Zhang H, 2018, PROC CVPR IEEE, P3194, DOI 10.1109/CVPR.2018.00337
   Zhu QS, 2015, IEEE T IMAGE PROCESS, V24, P3522, DOI 10.1109/TIP.2015.2446191
NR 56
TC 0
Z9 0
U1 3
U2 8
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2023 OCT 26
PY 2023
DI 10.1007/s00371-023-03129-w
EA OCT 2023
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA W4JO6
UT WOS:001091305800001
DA 2024-07-18
ER

PT J
AU Fatima, SH
   Munir, A
   Hussain, ST
AF Fatima, S. H.
   Munir, A.
   Hussain, S. T.
TI Image denoising using difference classifier and trimmed global mean
   filter adaptive approach
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Impulse noise; Random ordered impulsive noise; Image denoising; Trimmed
   global mean filter; Difference classifier; Ranked ordered absolute
   difference
ID VALUED IMPULSE NOISE; REMOVAL
AB Inexpensive cameras, different acquisition devices and mobile phones have resulted in the production of digital images in all spheres of life. There is common of these images for extracting useful information in many fields to perform image analysis and interpretation. One of the problems being faced in the use of this information is the presence of random valued impulse noise. Impulse noise removal is a crucial step in computer vision. Many articles have been published to remove or suppress impulse noise from grayscale digital images using different approaches. Edges and details are, however, often lost during the restoration of digital images. In this research, a two-stage approach for edge detection, preservation, and impulse noise removal is proposed. An effective statistics rank ordered absolute difference (ROAD) is used in the first stage to detect noisy pixels. Trimmed global mean (TGM) with adaptive window is employed later for the filtering process. With the combination of these processes in the proposed method, a higher quality image is restored and thus becomes more suitable for extracting the desired information. Comparison with many state-of-the-art methodologies indicates that the proposed method is effective in the detection of corrupted pixels and the restoration of images.
C1 [Fatima, S. H.; Munir, A.] Int Islamic Univ, Dept CS&SE, Islamabad, Pakistan.
   [Hussain, S. T.] Natl Univ Sci & Technol, DBS&H, CEME, Islamabad 44000, Pakistan.
C3 International Islamic University, Pakistan; National University of
   Sciences & Technology - Pakistan
RP Hussain, ST (corresponding author), Natl Univ Sci & Technol, DBS&H, CEME, Islamabad 44000, Pakistan.
EM syedahira016@gmail.com; asim@iiu.edu.pk; sthqau@gmail.com
OI Hussain, Syed Tayyab/0000-0003-4695-7556
CR Aghajarian M, 2022, INT J ADV COMPUT SC, V13
   Cao B, 2020, AAAI CONF ARTIF INTE, V34, P10486
   Cao B, 2019, IEEE T NEUR NET LEAR, V30, P1731, DOI 10.1109/TNNLS.2018.2872675
   Chan RH, 2004, IEEE SIGNAL PROC LET, V11, P921, DOI 10.1109/LSP.2004.838190
   Crnojevic V, 2004, IEEE SIGNAL PROC LET, V11, P589, DOI 10.1109/LSP.2004.830117
   Dong YQ, 2007, IEEE T IMAGE PROCESS, V16, P1112, DOI 10.1109/TIP.2006.891348
   Fan L., 2019, Pdf, V7
   Garnett R, 2005, IEEE T IMAGE PROCESS, V14, P1747, DOI 10.1109/TIP.2005.857261
   Huang JJ, 2022, IEEE T IMAGE PROCESS, V31, P4377, DOI 10.1109/TIP.2022.3184845
   Hussain A, 2017, MULTIMED TOOLS APPL, V76, P22001, DOI 10.1007/s11042-017-4757-z
   Iqbal N, 2019, SYMMETRY-BASEL, V11, DOI 10.3390/sym11030395
   Jana B. R., 2019, INT J INNOVATIVE TEC, V8, P583
   Jin KH, 2018, IEEE T IMAGE PROCESS, V27, P1448, DOI 10.1109/TIP.2017.2771471
   Jin QY, 2020, INVERSE PROBL IMAG, V14, P171, DOI 10.3934/ipi.2020009
   Kaliraj G, 2010, IMAGE VISION COMPUT, V28, P458, DOI 10.1016/j.imavis.2009.07.007
   Kalra GS, 2016, MULTIMED TOOLS APPL, V75, P4467, DOI 10.1007/s11042-015-2484-x
   Kosarevych R, 2022, VISUAL COMPUT, V38, P3719, DOI 10.1007/s00371-021-02207-1
   Lone MR, 2022, J KING SAUD UNIV-COM, V34, P9942, DOI 10.1016/j.jksuci.2021.12.020
   Maity Alenrex., 2018, Comput Vis Graph Image Process, V4, P6
   Molotkov L. A., 2008, J MATH SCI, V148, P753
   Neshatavar R, 2022, PROC CVPR IEEE, P17562, DOI 10.1109/CVPR52688.2022.01706
   Orazaev A, 2023, APPL SCI-BASEL, V13, DOI 10.3390/app13031585
   Prakash A., 2023, e-Prime Adv. Electr. Eng. Electron. Energy, V3
   Qin N, 2023, VISUAL COMPUT, V39, P2051, DOI 10.1007/s00371-022-02464-8
   Rohit V., 2013, Int. J. Adv. Res. Comput. Sci. Softw. Eng, V3, P2277
   Samantaray AK, 2015, PROCEDIA COMPUT SCI, V48, P222, DOI 10.1016/j.procs.2015.04.174
   Shah AW, 2022, J KING SAUD UNIV-COM, V34, P505, DOI 10.1016/j.jksuci.2020.03.007
   Shi KH, 2022, MULTIMED TOOLS APPL, V81, P10529, DOI 10.1007/s11042-022-12255-x
   Shi MW, 2023, VISUAL COMPUT, V39, P2407, DOI 10.1007/s00371-022-02491-5
   Turkmen I, 2013, AEU-INT J ELECTRON C, V67, P771, DOI 10.1016/j.aeue.2013.03.006
   Yang WY, 2024, VISUAL COMPUT, V40, P1947, DOI 10.1007/s00371-023-02894-y
   Zhang CB, 2015, AEU-INT J ELECTRON C, V69, P226, DOI 10.1016/j.aeue.2014.09.006
NR 32
TC 1
Z9 1
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2023 OCT 5
PY 2023
DI 10.1007/s00371-023-03106-3
EA OCT 2023
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA T7QI9
UT WOS:001079888400001
DA 2024-07-18
ER

PT J
AU Pankaj
   Kumar, B
   Bharti, PK
   Vishnoi, VK
   Kumar, K
   Mohan, S
   Singh, KP
AF Pankaj
   Kumar, Brajesh
   Bharti, P. K.
   Vishnoi, Vibhor Kumar
   Kumar, Krishan
   Mohan, Shashank
   Singh, Krishan Pal
TI Paddy yield prediction based on 2D images of rice panicles using
   regression techniques
SO VISUAL COMPUTER
LA English
DT Article
DE Yield prediction; Rice panicle; Image processing; Machine learning;
   Regression
ID RANDOM FOREST CLASSIFIER; GRAIN-YIELD; ALGORITHM
AB Crop yield predictions are important for crop monitoring and agronomic management. The traditional methods for yield predictions are complicated and resource consuming. With the availability of affordable handheld imaging and computing devices, the image processing-based yield prediction methods are gaining popularity. In this work, RGB images of rice panicles are captured using DSLR camera with simple background and processed to determine the panicle area in terms of number of pixels. A machine learning-based model is developed to make predictions for rice yield. The model is trained to make predictions on unseen data. Various machine learning-based regression algorithms including decision tree, random forest, support vector machine, and convolution neural network are tested. The experiments are performed on a publically available dataset from China as well as on a self-acquired dataset in India. The results have shown that image processing and machine learning-based methods can make yield predictions satisfactorily as evident from the coefficient of determination ( R-2) that ranges 0.80-0.97 for different cultivars. The prediction error is determined in terms of root mean square error (RMSE) and mean absolute error (MAE). RMSE for different methods lies between 0.14 and 0.40, whereas MAE varies from 0.11 to 0.30. Among the tested algorithms, linear regression achieved the best precision with R-2 = 0.97, RMSE = 0.14, and MAE = 0.11.
C1 [Pankaj; Kumar, Brajesh] MJP Rohilkhand Univ, Dept Comp Sci & IT, Bareilly 243006, India.
   [Pankaj; Bharti, P. K.] Shri Venkateshwara Univ, Dept Comp Sci & Engn, Gajraula, India.
   [Vishnoi, Vibhor Kumar] Teerthanker Mahaveer Univ, Coll Comp Sci & Informat Technol, Moradabad, India.
   [Kumar, Krishan] Gurukula Kangri, Dept Comp Sci, Haridwar, India.
   [Mohan, Shashank] Michigan State Univ, Dept Biosyst & Agr Engn, E Lansing, MI USA.
   [Kumar, Brajesh; Singh, Krishan Pal] MJP Rohilkhand Univ, Atal Ctr Artificial Intelligence, Bareilly, India.
C3 Mahatma Jyotiba Phule Rohilkhand University; Teerthanker Mahaveer
   University; Gurukul Kangri Vishwavidyalaya; Michigan State University;
   Mahatma Jyotiba Phule Rohilkhand University
RP Kumar, B (corresponding author), MJP Rohilkhand Univ, Dept Comp Sci & IT, Bareilly 243006, India.; Kumar, B (corresponding author), MJP Rohilkhand Univ, Atal Ctr Artificial Intelligence, Bareilly, India.
EM pankaj.kumar@mjpru.ac.in; bkumar@mjpru.ac.in; vc@svu.edu.in;
   vibhor.computers@tmu.ac.in; krishan.kumar@gkv.ac.in; mohansh1@msu.edu;
   kps.biophysics@gmail.com
RI Vishnoi, Vibhor/ABH-1446-2020; Kumar, Brajesh/C-6216-2018
OI Vishnoi, Vibhor/0000-0002-6682-4171; KUMAR, KRISHAN/0000-0002-9296-1655;
   Kumar, Brajesh/0000-0001-8100-7287
FU Department of Higher Education, Government of Uttar Pradesh to support
   this work through research Grant under Center of Excellence scheme.
   [45/2022/- 869/Seventy-4-2022 /001-70-4099-1-2022]
FX We would like to acknowledge Department of Higher Education, Government
   of Uttar Pradesh to support this work through research Grant
   [No.-45/2022/- 869/Seventy-4-2022 /001-70-4099-1-2022] under Center of
   Excellence scheme.
CR Albawi S, 2017, I C ENG TECHNOL
   [Anonymous], 2014, Data Mining with Decision Trees: Theory and Applications
   Azar AT, 2014, COMPUT METH PROG BIO, V113, P465, DOI 10.1016/j.cmpb.2013.11.004
   Basir MS, 2021, J AGR FOOD RES, V5, DOI 10.1016/j.jafr.2021.100186
   Bieniek A, 2000, PATTERN RECOGN, V33, P907, DOI 10.1016/S0031-3203(99)00154-5
   Breiman L., 2001, Machine Learning, V45, P5, DOI 10.1023/A:1010933404324
   Cedric LS, 2022, SMART AGR TECHNOL, V2, DOI 10.1016/j.atech.2022.100049
   Chivasa W, 2017, INT J REMOTE SENS, V38, P6816, DOI 10.1080/01431161.2017.1365390
   Choudhary K, 2022, ADV SPACE RES, V70, P2443, DOI 10.1016/j.asr.2022.06.073
   CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1007/BF00994018
   Desloires J, 2023, COMPUT ELECTRON AGR, V209, DOI 10.1016/j.compag.2023.107807
   Escalante HJ, 2019, INT J REMOTE SENS, V40, P2493, DOI 10.1080/01431161.2019.1577571
   Feng AJ, 2020, BIOSYST ENG, V193, P101, DOI 10.1016/j.biosystemseng.2020.02.014
   Gajjar R, 2022, VISUAL COMPUT, V38, P2923, DOI 10.1007/s00371-021-02164-9
   Geipel J, 2014, REMOTE SENS-BASEL, V6, P10335, DOI 10.3390/rs61110335
   Guo TM, 2017, 2017 IEEE 2ND INTERNATIONAL CONFERENCE ON BIG DATA ANALYSIS (ICBDA), P721, DOI 10.1109/ICBDA.2017.8078730
   He B, 2024, VISUAL COMPUT, V40, P1965, DOI 10.1007/s00371-023-02895-x
   Huang HJ, 2022, NEUROCOMPUTING, V490, P80, DOI 10.1016/j.neucom.2021.10.125
   Junos MH, 2022, VISUAL COMPUT, V38, P2341, DOI 10.1007/s00371-021-02116-3
   Kim K, 2017, PATTERN RECOGN LETT, V98, P39, DOI 10.1016/j.patrec.2017.08.011
   Kumar B, 2017, INT J REMOTE SENS, V38, P5830, DOI 10.1080/01431161.2017.1348636
   Li Q, 2014, I C CONT AUTOMAT ROB, P844, DOI 10.1109/ICARCV.2014.7064414
   Liu Y., 2019, MENDELEY DATA, DOI [10.17632/s9mn5xg4wj1, DOI 10.17632/S9MN5XG4WJ1]
   Liu YX, 2021, FIELD CROP RES, V264, DOI 10.1016/j.fcr.2021.108098
   Ma JC, 2023, INT J APPL EARTH OBS, V118, DOI 10.1016/j.jag.2023.103292
   MarjaniZadeh M.S.J., 2018, MORE PEOPLE MORE FOO
   Mateo-Sagasta Javier, 2018, MORE PEOPLE MORE FOO
   Mateo-Sanchis A, 2019, REMOTE SENS ENVIRON, V234, DOI 10.1016/j.rse.2019.111460
   Nakano H., 2023, CROP ENV, V2, P59, DOI [10.1016/j.crope.2023.03.001, DOI 10.1016/J.CROPE.2023.03.001]
   Nevavuori P, 2019, COMPUT ELECTRON AGR, V163, DOI 10.1016/j.compag.2019.104859
   Ohana-Levi N, 2022, AGR WATER MANAGE, V262, DOI 10.1016/j.agwat.2021.107317
   Olanrewaju S, 2019, INT J REMOTE SENS, V40, P6905, DOI 10.1080/01431161.2019.1597303
   Pal M, 2005, INT J REMOTE SENS, V26, P217, DOI 10.1080/01431160412331269698
   Polikar R, 2012, ENSEMBLE MACHINE LEARNING: METHODS AND APPLICATIONS, P1, DOI 10.1007/978-1-4419-9326-7_1
   Prabakaran G, 2021, MATH COMPUT SIMULAT, V185, P1, DOI 10.1016/j.matcom.2020.12.011
   Prakash AJ, 2023, VISUAL COMPUT, V39, P1765, DOI 10.1007/s00371-022-02443-z
   Rana K, 2022, NAT HAZARD EARTH SYS, V22, P3751, DOI 10.5194/nhess-22-3751-2022
   Rana K, 2021, GEOPHYS RES LETT, V48, DOI 10.1029/2020GL090848
   Roy MH, 2012, J NONPARAMETR STAT, V24, P993, DOI 10.1080/10485252.2012.715161
   Russ Georg, 2009, Advances in Data Mining, Applications and Theoretical Aspects. Proceedings 9th Industrial Confer, ICDM 2009, P24, DOI 10.1007/978-3-642-03067-3_3
   Sagan V, 2021, ISPRS J PHOTOGRAMM, V174, P265, DOI 10.1016/j.isprsjprs.2021.02.008
   Sanches GM, 2018, INT J REMOTE SENS, V39, P5402, DOI 10.1080/01431161.2018.1448484
   Schwalbert RA, 2020, AGR FOREST METEOROL, V284, DOI 10.1016/j.agrformet.2019.107886
   Servia H, 2022, INT J APPL EARTH OBS, V108, DOI 10.1016/j.jag.2022.102725
   Shafiee S, 2021, COMPUT ELECTRON AGR, V183, DOI 10.1016/j.compag.2021.106036
   Sharma V, 2023, VISUAL COMPUT, V39, P6503, DOI 10.1007/s00371-022-02742-5
   Smola AJ, 2004, STAT COMPUT, V14, P199, DOI 10.1023/B:STCO.0000035301.49549.88
   Stephen A, 2024, VISUAL COMPUT, V40, P919, DOI 10.1007/s00371-023-02823-z
   van Wart J, 2013, FIELD CROP RES, V143, P34, DOI 10.1016/j.fcr.2012.11.018
   Velmurugan P., 2023, Materials Today: Proceedings, P112, DOI 10.1016/j.matpr.2021.02.578
   Wan L, 2020, AGR FOREST METEOROL, V291, DOI 10.1016/j.agrformet.2020.108096
   Yang Q, 2019, FIELD CROP RES, V235, P142, DOI 10.1016/j.fcr.2019.02.022
   Zhang MN, 2020, BIOSYST ENG, V189, P24, DOI 10.1016/j.biosystemseng.2019.11.001
   Zhao SQ, 2019, COMPUT ELECTRON AGR, V162, P759, DOI 10.1016/j.compag.2019.05.020
NR 54
TC 0
Z9 0
U1 6
U2 8
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2024
VL 40
IS 6
BP 4457
EP 4471
DI 10.1007/s00371-023-03092-6
EA OCT 2023
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TV2X4
UT WOS:001075822200001
DA 2024-07-18
ER

PT J
AU Liu, WP
   Li, YY
   Huang, D
AF Liu, Wanping
   Li, Yueyue
   Huang, Dong
TI RA-UNet: an improved network model for image denoising
SO VISUAL COMPUTER
LA English
DT Article
DE Attention mechanism; Detail information; Image denoising; Residual
   convolution
ID SUPERRESOLUTION
AB Due to the rapid advancement of GPU computing, deep learning has lately been widely used in image denoising. Most deep learning methods require noise-free images as labels, which are often difficult or impossible to obtain. Therefore, denoising network models have to be trained with a pair of noisy and low-noise images. However, the restored images still face the problem of losing detail information. In this paper, we propose a novel denoising network model based on the concept of Noise2Noise (N2N), where pairs of noisy images are utilized to train a neural network that can learn the noise distribution relationship between them. This newly-proposed model (RA-UNet) draws inspiration from the classical UNet architecture and is designed with a multi-Residual convolutional block with Attention that can adapt different scales to mine the key information of images and recover clearer images. The denoising performance of RA-UNet is comparable and better than that of the conventional CBM3D, while the proposed RA-UNet performs significantly better on both PSNR and SSIM with less 2.5%\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{mathrsfs} \usepackage{upgreek} \setlength{\oddsidemargin}{-69pt} \begin{document}$$2.5\%$$\end{document} FLOPs and less 3%\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{mathrsfs} \usepackage{upgreek} \setlength{\oddsidemargin}{-69pt} \begin{document}$$3\%$$\end{document} running time compared to existing deep learning-based methods, such as DnCNN and ADNet. From the perspective of visual quality, RA-UNet can restore images with higher clarity. Compared with the UNet model, our model improves the average PSNR and SSIM obtained by testing Gaussian noise (& sigma;=50\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{mathrsfs} \usepackage{upgreek} \setlength{\oddsidemargin}{-69pt} \begin{document}$$\sigma =50$$\end{document}) images on four classic datasets by 2.28 dB and 0.0552, respectively.
C1 [Liu, Wanping; Li, Yueyue] Chongqing Univ Technol, Coll Comp Sci & Engn, Chongqing 400054, Peoples R China.
   [Huang, Dong] Guizhou Univ, Key Lab Adv Mfg Technol, Minist Educ, Guiyang 550025, Peoples R China.
C3 Chongqing University of Technology; Guizhou University
RP Liu, WP (corresponding author), Chongqing Univ Technol, Coll Comp Sci & Engn, Chongqing 400054, Peoples R China.
EM wpliu@cqut.edu.cn
RI Li, yueyue/GLR-6313-2022
FU This work was supported by Natural Science Foundation of Chongqing
   (Grant No. cstc2021jcyj-msxmX0594), the Science and Technology Research
   Program of Chongqing Municipal Education Commission (Grant No.
   KJQN201901101) and Chongqing University of Technology
   [cstc2021jcyj-msxmX0594]; Natural Science Foundation of Chongqing
   [KJQN201901101]; Science and Technology Research Program of Chongqing
   Municipal Education Commission [gzlcx20223212]; Chongqing University of
   Technology Innovation Project
FX This work was supported by Natural Science Foundation of Chongqing
   (Grant No. cstc2021jcyj-msxmX0594), the Science and Technology Research
   Program of Chongqing Municipal Education Commission (Grant No.
   KJQN201901101) and Chongqing University of Technology Innovation Project
   (Grant No. gzlcx20223212).
CR Bai J, 2020, VISUAL COMPUT, V36, P2145, DOI 10.1007/s00371-020-01943-0
   Bevilacqua M, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.135
   Chen LC, 2016, PROC CVPR IEEE, P3640, DOI 10.1109/CVPR.2016.396
   Dabov K, 2007, IEEE T IMAGE PROCESS, V16, P2080, DOI 10.1109/TIP.2007.901238
   Dong C, 2014, LECT NOTES COMPUT SC, V8692, P184, DOI 10.1007/978-3-319-10593-2_13
   Dosovitskiy A, 2021, arXiv, DOI [10.48550/ARXIV.2010.11929, DOI 10.48550/ARXIV.2010.11929]
   Gu SH, 2014, PROC CVPR IEEE, P2862, DOI 10.1109/CVPR.2014.366
   Guo MH, 2022, COMPUT VIS MEDIA, V8, P331, DOI 10.1007/s41095-022-0271-y
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang JB, 2015, PROC CVPR IEEE, P5197, DOI 10.1109/CVPR.2015.7299156
   Ibtehaz N, 2020, NEURAL NETWORKS, V121, P74, DOI 10.1016/j.neunet.2019.08.025
   Ioffe S, 2015, P INT C MACH LEARN, V2015, P1
   Jain V., 2009, ADV NEURAL INFORM PR, P769, DOI DOI 10.5555/2981780.2981876
   Lehtinen J, 2018, PR MACH LEARN RES, V80
   Li HY, 2019, CONF REC ASILOMAR C, P2087, DOI [10.1109/IEEECONF44664.2019.9048981, 10.1109/ieeeconf44664.2019.9048981]
   Luo Q, 2021, VISUAL COMPUT, V37, P1899, DOI 10.1007/s00371-020-01951-0
   Mao XJ, 2016, ADV NEUR IN, V29
   Mohan J, 2014, BIOMED SIGNAL PROCES, V9, P56, DOI 10.1016/j.bspc.2013.10.007
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Schmidt U, 2014, PROC CVPR IEEE, P2774, DOI 10.1109/CVPR.2014.349
   Serre T, 2007, IEEE T PATTERN ANAL, V29, P411, DOI 10.1109/TPAMI.2007.56
   Szegedy C, 2014, Arxiv, DOI [arXiv:1312.6199, DOI 10.1109/CVPR.2015.7298594]
   Tian CW, 2020, NEURAL NETWORKS, V124, P117, DOI 10.1016/j.neunet.2019.12.024
   Timofte R, 2015, LECT NOTES COMPUT SC, V9006, P111, DOI 10.1007/978-3-319-16817-3_8
   Timofte R, 2013, IEEE I CONF COMP VIS, P1920, DOI 10.1109/ICCV.2013.241
   Vincent Pascal, 2008, P 25 INT C MACHINE L, DOI DOI 10.1145/1390156.1390294
   Wang F, 2017, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2017.683
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Yang CY, 2013, IEEE I CONF COMP VIS, P561, DOI 10.1109/ICCV.2013.75
   Yue LW, 2016, SIGNAL PROCESS, V128, P389, DOI 10.1016/j.sigpro.2016.05.002
   Zehua Zhou, 2013, Advanced Materials Research, V756-759, P3313, DOI 10.4028/www.scientific.net/AMR.756-759.3313
   Zeyde R., 2012, INT C CURV SURF, P711, DOI DOI 10.1007/978-3-642-27413-8_47
   Zhang J, 2012, J MATH IMAGING VIS, V43, P39, DOI 10.1007/s10851-011-0285-z
   Zhang K, 2018, IEEE T IMAGE PROCESS, V27, P4608, DOI 10.1109/TIP.2018.2839891
   Zhang K, 2017, PROC CVPR IEEE, P2808, DOI 10.1109/CVPR.2017.300
   Zhang K, 2017, IEEE T IMAGE PROCESS, V26, P3142, DOI 10.1109/TIP.2017.2662206
NR 36
TC 0
Z9 0
U1 12
U2 19
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2024
VL 40
IS 6
BP 4319
EP 4335
DI 10.1007/s00371-023-03084-6
EA SEP 2023
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TV2X4
UT WOS:001072285000003
DA 2024-07-18
ER

PT J
AU Wu, J
   Wu, SC
   Sun, BB
AF Wu, Jin
   Wu, Shunchuan
   Sun, Beibei
TI An adaptive methodology for rock mass fracture image enhancement with
   generalized gamma correction
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Rock mass fracture; Image enhancement; Generalized gamma correction;
   Gray histogram
ID BI-HISTOGRAM EQUALIZATION; TRANSFORMATION
AB Rock fractures are an important indicator for assessing the quality of rock masses. The significant differences in the color and texture distributions of rock mass exposures lead to great challenges in the automated extraction of fractures from various low-contrast images. To address these issues, an adaptive generalized gamma correction enhancement algorithm for rock fracture images is proposed. First, the entire gray level is divided into three regions, namely, the fracture region, transition region, and rock mass exposure region, through piecewise linear fitting of the prepeak gray histogram and a designed evaluation function. Subsequently, a generalized gamma transform curve with dynamic nonlinear variation of the gamma factor is proposed, enabling more flexible transitions between compression and expansion effects at different grayscale levels, and its parameters are automatically determined by fitting the histogram partition data. Ten representative rock fracture images and five well-known crack datasets are used for quantitative assessment and visual analysis, with the pixel response intensity ratio introduced as the quantitative metric. The experimental results demonstrate that compared to several traditional and state-of-the-art enhancement algorithms, the proposed method exhibits stronger targeting and adaptability in enhancing fractures while effectively suppressing rock mass exposure noise, with excellent performance.
C1 [Wu, Jin; Wu, Shunchuan; Sun, Beibei] Univ Sci & Technol Beijing, Sch Civil & Resources Engn, Beijing 100083, Peoples R China.
   [Wu, Shunchuan] Kunming Univ Sci & Technol, Fac Land Resources Engn, Kunming 650093, Peoples R China.
   [Wu, Shunchuan] Minist Nat Resources Peoples Republ China, Key Lab Geohazard Forecast & Geoecol Restorat Plat, Kunming 650093, Peoples R China.
   [Wu, Shunchuan] Yunnan Key Lab Geohazard Forecast & Geoecol Restor, Kunming 650093, Peoples R China.
C3 University of Science & Technology Beijing; Kunming University of
   Science & Technology; Ministry of Natural Resources of the People's
   Republic of China
RP Wu, SC (corresponding author), Univ Sci & Technol Beijing, Sch Civil & Resources Engn, Beijing 100083, Peoples R China.; Wu, SC (corresponding author), Kunming Univ Sci & Technol, Fac Land Resources Engn, Kunming 650093, Peoples R China.; Wu, SC (corresponding author), Minist Nat Resources Peoples Republ China, Key Lab Geohazard Forecast & Geoecol Restorat Plat, Kunming 650093, Peoples R China.; Wu, SC (corresponding author), Yunnan Key Lab Geohazard Forecast & Geoecol Restor, Kunming 650093, Peoples R China.
EM b20200006@xs.ustb.edu.cn; wushunchuan@ustb.edu.cn
FU The authors disclosed receipt of the following financial support for the
   research, authorship, and/or publication of this article: This work was
   financially supported by the Natural Science Foundation of China (NSFC;
   Grant Number: 51934003), Yunnan Major S [51934003]; Natural Science
   Foundation of China (NSFC) [202202AG050014]; Yunnan Major Scientific and
   Technological Projects [202105AE160023]; Yunnan Innovation Team
FX The authors disclosed receipt of the following financial support for the
   research, authorship, and/or publication of this article: This work was
   financially supported by the Natural Science Foundation of China (NSFC;
   Grant Number: 51934003), Yunnan Major Scientific and Technological
   Projects (Grant Number: 202202AG050014), and the Yunnan Innovation Team
   (Grant Number: 202105AE160023).
CR Alhasson HF, 2021, VISUAL COMPUT, V37, P2263, DOI 10.1007/s00371-020-01985-4
   Barton N., 1974, Rock Mech, V6, P189, DOI [DOI 10.1007/BF01239496, 10.1007/BF01239496]
   Battulwar R, 2021, J ROCK MECH GEOTECH, V13, P920, DOI 10.1016/j.jrmge.2021.01.008
   Bieniawski ZT., 1973, Civil Eng South Afr, V15, P335
   Byun H, 2021, EARTH SCI INFORM, V14, P1937, DOI 10.1007/s12145-021-00650-1
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851
   Chen JY, 2021, INT J ROCK MECH MIN, V142, DOI 10.1016/j.ijrmms.2021.104745
   Chen SD, 2003, IEEE T CONSUM ELECTR, V49, P1310, DOI 10.1109/TCE.2003.1261234
   Chiu YS, 2011, IEEE SYS MAN CYBERN, P2946, DOI 10.1109/ICSMC.2011.6084119
   Das DK, 2022, VISUAL COMPUT, V38, P3803, DOI 10.1007/s00371-021-02222-2
   Deb D, 2008, COMPUT GEOSCI-UK, V34, P115, DOI 10.1016/j.cageo.2007.03.007
   DUDA RO, 1972, COMMUN ACM, V15, P11, DOI 10.1145/361237.361242
   Eisenbach M, 2017, IEEE IJCNN, P2039, DOI 10.1109/IJCNN.2017.7966101
   Ferrero AM, 2009, ROCK MECH ROCK ENG, V42, P631, DOI 10.1007/s00603-008-0010-4
   Frangi AF, 1998, LECT NOTES COMPUT SC, V1496, P130, DOI 10.1007/BFb0056195
   Fu YP, 2022, VISUAL COMPUT, V38, P3243, DOI 10.1007/s00371-022-02559-2
   Gandhamal A, 2017, COMPUT BIOL MED, V83, P120, DOI 10.1016/j.compbiomed.2017.03.001
   Ge YF, 2018, ENG GEOL, V242, P44, DOI 10.1016/j.enggeo.2018.05.007
   Gigli G, 2011, INT J ROCK MECH MIN, V48, P187, DOI 10.1016/j.ijrmms.2010.11.009
   Gonzalez R.C., 2002, Digital Image Processing, V2nd
   Goodman RE, 1995, GEOTECHNIQUE, V45, P383, DOI 10.1680/geot.1995.45.3.383
   Hoek E, 1997, INT J ROCK MECH MIN, V34, P1165, DOI 10.1016/S1365-1609(97)80069-X
   Hong K, 2017, J ROCK MECH GEOTECH, V9, P702, DOI 10.1016/j.jrmge.2017.05.001
   Huang SC, 2013, IEEE T IMAGE PROCESS, V22, P1032, DOI 10.1109/TIP.2012.2226047
   Jekel C. F., 2019, pwlf: A Python Library for Fitting 1D Continuous Piecewise Linear Functions
   KANOPOULOS N, 1988, IEEE J SOLID-ST CIRC, V23, P358, DOI 10.1109/4.996
   Kim YT, 1997, IEEE T CONSUM ELECTR, V43, P1, DOI 10.1109/30.580378
   Lee N., 2021, arXiv, DOI DOI 10.48550/ARXIV.2104.01459
   Lee YK, 2022, INT J ROCK MECH MIN, V149, DOI 10.1016/j.ijrmms.2021.104981
   Leng B, 2021, TUNN UNDERGR SP TECH, V110, DOI 10.1016/j.tust.2021.103810
   Liu CS, 2019, J ELECTR COMPUT ENG, V2019, DOI 10.1155/2019/9594301
   Liu YH, 2019, NEUROCOMPUTING, V338, P139, DOI 10.1016/j.neucom.2019.01.036
   Ma ZH, 2022, VISUAL COMPUT, V38, P3163, DOI 10.1007/s00371-022-02535-w
   Marques A, 2022, IEEE ACCESS, V10, P20514, DOI 10.1109/ACCESS.2022.3151897
   Ooi CH, 2009, IEEE T CONSUM ELECTR, V55, P2072, DOI 10.1109/TCE.2009.5373771
   OTSU N, 1979, IEEE T SYST MAN CYB, V9, P62, DOI 10.1109/TSMC.1979.4310076
   Paul A, 2023, VISUAL COMPUT, V39, P297, DOI 10.1007/s00371-021-02330-z
   Priest S.D., 1993, Discontinuity Analysis for Rock Engineering
   Rahman S, 2016, EURASIP J IMAGE VIDE, DOI 10.1186/s13640-016-0138-1
   Reid TR, 2000, INT J ROCK MECH MIN, V37, P1073, DOI 10.1016/S1365-1609(00)00041-1
   Roncella R, 2005, P SOC PHOTO-OPT INS, V5665, P17, DOI 10.1117/12.587822
   Shi Y, 2016, IEEE T INTELL TRANSP, V17, P3434, DOI 10.1109/TITS.2016.2552248
   Sim KS, 2007, PATTERN RECOGN LETT, V28, P1209, DOI 10.1016/j.patrec.2007.02.003
   Singh K, 2014, OPTIK, V125, P4646, DOI 10.1016/j.ijleo.2014.04.093
   Slob S., 2004, ENG GEOLOGY INFRASTR, P179
   Tang YD, 2021, INT J ROCK MECH MIN, V142, DOI 10.1016/j.ijrmms.2021.104732
   Vöge M, 2013, ENG GEOL, V164, P155, DOI 10.1016/j.enggeo.2013.07.008
   Wang WX, 2007, ICNC 2007: THIRD INTERNATIONAL CONFERENCE ON NATURAL COMPUTATION, VOL 1, PROCEEDINGS, P632
   Wang WX, 2015, KSII T INTERNET INF, V9, P5073, DOI 10.3837/tiis.2015.12.018
   Wang Y, 1999, IEEE T CONSUM ELECTR, V45, P68, DOI 10.1109/30.754419
   Wu Zhi-guo, 2010, Acta Photonica Sinica, V39, P755, DOI 10.3788/gzxb20103904.0755
   Xu XS, 2023, VISUAL COMPUT, V39, P6097, DOI 10.1007/s00371-022-02715-8
   Yang S, 2021, INT J ROCK MECH MIN, V140, DOI 10.1016/j.ijrmms.2020.104585
   Yin WX, 2023, VISUAL COMPUT, V39, P6723, DOI 10.1007/s00371-022-02759-w
   ZACK GW, 1977, J HISTOCHEM CYTOCHEM, V25, P741, DOI 10.1177/25.7.70454
   Zhang L, 2016, IEEE IMAGE PROC, P3708, DOI 10.1109/ICIP.2016.7533052
   Zhou Y, 2014, J CENT SOUTH UNIV, V21, P1125, DOI 10.1007/s11771-014-2045-x
   Zou Q, 2012, PATTERN RECOGN LETT, V33, P227, DOI 10.1016/j.patrec.2011.11.004
NR 58
TC 0
Z9 0
U1 3
U2 7
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2023 SEP 25
PY 2023
DI 10.1007/s00371-023-03100-9
EA SEP 2023
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA S5MD5
UT WOS:001071597500002
DA 2024-07-18
ER

PT J
AU Kumar, H
   Banerjee, A
   Saurav, S
   Singh, S
AF Kumar, Himanshu
   Banerjee, Abeer
   Saurav, Sumeet
   Singh, Sanjay
TI ParaColorizer-Realistic image colorization using parallel generative
   networks
SO VISUAL COMPUTER
LA English
DT Article
DE Information restoration; Realistic colorization; Parallel GANs; ResUNet;
   Image fusion
ID COLOR
AB Image colorization is a fascinating application of AI for information restoration. The inherently ill-posed nature of the problem increases the challenge since the outputs could be multimodal. Existing learning-based methods produce acceptable results for straightforward cases but usually fail to restore the contextual information without clear figure-ground separation. Also, the images suffer from color bleeding and desaturated backgrounds since a single model trained on full-image features is insufficient for learning the diverse data modes. This work presents a parallel generative adversarial network (GAN)-based colorization framework to address these issues. The proposed framework uses parallel GANs tailored to colorize the foreground (using object-level features) and the background (using full-image features) independently and performs unbalanced GAN training. We develop a DenseFuse-based fusion network to obtain the final colorized image by feature-based fusion of the parallelly generated intermediate outputs. We conduct extensive performance evaluations and ablation studies of our framework with multiple perceptual metrics, including human evaluation. Our approach outperforms most existing learning-based methods and produces results comparable to the state of the art. The runtime analysis experiments revealed an average inference time of 24 milliseconds (ms) per image, and thus the proposed framework can colorize the grayscale images in real time.
C1 [Kumar, Himanshu; Banerjee, Abeer; Saurav, Sumeet; Singh, Sanjay] CSIR Cent Elect Engn Res Inst CSIR CEERI, Pilani 333031, India.
   [Kumar, Himanshu; Banerjee, Abeer; Saurav, Sumeet; Singh, Sanjay] Acad Sci & Innovat Res AcSIR, Ghaziabad 201002, India.
C3 Council of Scientific & Industrial Research (CSIR) - India; CSIR -
   Central Electronics Engineering Research Institute (CEERI); Academy of
   Scientific & Innovative Research (AcSIR)
RP Banerjee, A (corresponding author), CSIR Cent Elect Engn Res Inst CSIR CEERI, Pilani 333031, India.; Banerjee, A (corresponding author), Acad Sci & Innovat Res AcSIR, Ghaziabad 201002, India.
EM himanshu.ceeri20a@acsir.res.in; abeer.ceeri20a@acsir.res.in;
   sumeet@ceeri.res.in; sanjay@ceeri.res.in
OI Kumar, Himanshu/0000-0001-7889-3564; Banerjee, Abeer/0000-0001-7040-7074
FU The authors extend their gratitude to the CSIR-CEERI Director for
   supporting AI-related research and to the volunteers for participating
   in the human evaluation test. All computations were performed using the
   GPU resources provided by the AI Computing Faci; AI Computing Facility,
   CSIR-CEERI
FX The authors extend their gratitude to the CSIR-CEERI Director for
   supporting AI-related research and to the volunteers for participating
   in the human evaluation test. All computations were performed using the
   GPU resources provided by the AI Computing Facility, CSIR-CEERI.
CR An XB, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360639
   Antic J, 2019, JANTIC DEOLDIFY DEEP
   Charpiat G, 2008, LECT NOTES COMPUT SC, V5304, P126, DOI 10.1007/978-3-540-88690-7_10
   Cheng ZZ, 2015, IEEE I CONF COMP VIS, P415, DOI 10.1109/ICCV.2015.55
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Deshpande A, 2017, PROC CVPR IEEE, P2877, DOI 10.1109/CVPR.2017.307
   Freedman D, 2010, PROC CVPR IEEE, P287, DOI 10.1109/CVPR.2010.5540201
   Gatys L., 2016, Journal of Vision, V16, P326, DOI DOI 10.1167/16.12.326
   Guadarrama S., 2017, ARXIV
   Gupta R. K., 2012, P 20 ACM INT C MULT, P369, DOI DOI 10.1145/2393347.2393402
   Ham H.G., 2020, ARXIV
   Hasler D, 2003, P SOC PHOTO-OPT INS, V5007, P87, DOI 10.1117/12.477378
   He KM, 2020, IEEE T PATTERN ANAL, V42, P386, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   He MM, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201365
   Hoang Quan, 2018, INT C LEARN REPR
   IIZUKA S, 2016, ACM T GRAPHIC, V35, P1, DOI DOI 10.1145/2897824.2925974
   Ironi R., 2005, RENDERING TECHNIQUES, P201
   Isola Phillip, 2016, ARXIV161107004, DOI [DOI 10.1109/CVPR.2017.632, 10.1109/CVPR.2017.632]
   Jheng-Wei Su, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7965, DOI 10.1109/CVPR42600.2020.00799
   Kastryulin Sergey, 2022, PYTORCH IMAGE QUALIT
   Kolesnikov A., 2017, P BRIT MACH VIS C BM
   Kumar M., 2021, ARXIV
   Laffont PY, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601101
   Larsson G, 2016, LECT NOTES COMPUT SC, V9908, P577, DOI 10.1007/978-3-319-46493-0_35
   Levin A, 2004, ACM T GRAPHIC, V23, P689, DOI 10.1145/1015706.1015780
   Li H, 2019, IEEE T IMAGE PROCESS, V28, P2614, DOI 10.1109/TIP.2018.2887342
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu XP, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409105
   Luan Q., 2007, Proceedings of the 18th Eurographics conference on Rendering Techniques, P309
   Mao XD, 2017, IEEE I CONF COMP VIS, P2813, DOI 10.1109/ICCV.2017.304
   Messaoud S, 2018, LECT NOTES COMPUT SC, V11210, P603, DOI 10.1007/978-3-030-01231-1_37
   Nazeri Kamyar, 2018, Articulated Motion and Deformable Objects. 10th International Conference, AMDO 2018. Proceedings: LNCS 10945, P85, DOI 10.1007/978-3-319-94544-6_9
   O'Hare N, 2009, IEEE T MULTIMEDIA, V11, P220, DOI 10.1109/TMM.2008.2009679
   Pitié F, 2005, IEEE I CONF COMP VIS, P1434
   Qu YG, 2006, ACM T GRAPHIC, V25, P1214, DOI 10.1145/1141911.1142017
   Seitzer M., 2020, PYTORCH FID FID SCOR
   Shih YC, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601137
   Sun Q, 2022, VISUAL COMPUT, V38, P1283, DOI 10.1007/s00371-021-02219-x
   Sykora D, 2009, COMPUT GRAPH FORUM, V28, P599, DOI 10.1111/j.1467-8659.2009.01400.x
   Vitoria P, 2020, IEEE WINT CONF APPL, P2434, DOI [10.1109/WACV45572.2020.9093389, 10.1109/wacv45572.2020.9093389]
   Welsh T, 2002, ACM T GRAPHIC, V21, P277, DOI 10.1145/566570.566576
   Wu FZ, 2013, COMPUT GRAPH FORUM, V32, P190, DOI 10.1111/cgf.12008
   Wu M, 2021, VISUAL COMPUT, V37, P1707, DOI 10.1007/s00371-020-01933-2
   Wu YZ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14357, DOI 10.1109/ICCV48922.2021.01411
   Xia J, 2021, 2021 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP 2021), P1655, DOI 10.1109/ICASSP39728.2021.9414764
   Xiong K, 2023, VISUAL COMPUT, V39, P6537, DOI 10.1007/s00371-022-02747-0
   Yatziv L, 2006, IEEE T IMAGE PROCESS, V15, P1120, DOI 10.1109/TIP.2005.864231
   Yi-Chin Huang, 2005, 13th Annual ACM International Conference on Multimedia, P351, DOI 10.1145/1101149.1101223
   Zhang B, 2019, PROC CVPR IEEE, P8044, DOI 10.1109/CVPR.2019.00824
   Zhang H, 2019, PR MACH LEARN RES, V97
   Zhang LM, 2017, PROCEEDINGS 2017 4TH IAPR ASIAN CONFERENCE ON PATTERN RECOGNITION (ACPR), P506, DOI 10.1109/ACPR.2017.61
   Zhang R, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073703
   Zhang R, 2016, LECT NOTES COMPUT SC, V9907, P649, DOI 10.1007/978-3-319-46487-9_40
   Zhao J., 2018, PIXEL LEVEL SEMANTIC
   Zhou B, 2016, ARXIV
NR 55
TC 0
Z9 0
U1 4
U2 7
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2024
VL 40
IS 6
BP 4039
EP 4054
DI 10.1007/s00371-023-03067-7
EA SEP 2023
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TV2X4
UT WOS:001066148600001
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Tan, AL
   Guo, TA
   Zhao, Y
   Wang, YX
   Li, XH
AF Tan, Ailing
   Guo, Tianan
   Zhao, Yong
   Wang, Yunxin
   Li, Xiaohang
TI Object detection based on polarization image fusion and grouped
   convolutional attention network
SO VISUAL COMPUTER
LA English
DT Article
DE Object detection; Polarization image fusion; Grouped convolutional
   attention; Network lightweight; Foggy image
AB Objection detection of vehicles and pedestrians in fog is of great significance for intelligent transportation and autonomous driving. Polarization image is beneficial to improve the object detection under adverse weather conditions. This study proposed a polarization image fusion method based on grouped convolutional attention network (GCAnet) to improve the object detection for cars and persons in foggy street scenes. Based on the international available Polar LITIS image dataset, a multi-channel grouped convolution matrix was first constructed to input different types of polarization images. Then, a grouped attention module was added to enhance the features in each type of polarization image, and finally each convolutional matrix was further connected to the detection network in series to perform objection detection. The experimental results prove that three types of polarization image fusion are obviously better than those of any two types of polarization image fusion and one single polarization image; and adding ECA attention module after multi-channel convolution can further enhance the accuracy of I04590 + Pauli + Stokes fused image to the highest value of 76.46%. The improvement of network lightweight shows that the Mobilenet-ECA has increased the speed by 26% with a slightly reduced accuracy. The proposed GCAnet method has significantly surpassed traditional objection detection networks of SSD300, SSD512, Faster R-CNN600, Yolov3, and Yolov4, which has increased the mAP@0.5 by 28.90%, 27.60%, 15.01%, 24.98%, and 16.45%, respectively; and has increased the mAP@0.5 by 9.36% and 6.20% compared to foggy image detection methods of AOD-Net SSD and DeRF-Yolov3-X, respectively. This work demonstrates the potential of GCAnet enabled polarization image fusion technology to be used as an effective foggy objection detection method in the field of intelligent transportation and autonomous driving.
C1 [Tan, Ailing; Guo, Tianan; Wang, Yunxin; Li, Xiaohang] Yanshan Univ, Sch Informat & Sci Engn, Key Lab Special Fiber & Fiber Sensor Hebei Prov, Qinhuangdao 066004, Peoples R China.
   [Zhao, Yong] Yanshan Univ, Sch Elect Engn, Key Lab Measurement Technol & Instrumentat Hebei P, Qinhuangdao 066004, Hebei, Peoples R China.
C3 Yanshan University; Yanshan University
RP Zhao, Y (corresponding author), Yanshan Univ, Sch Elect Engn, Key Lab Measurement Technol & Instrumentat Hebei P, Qinhuangdao 066004, Hebei, Peoples R China.
EM zhaoyong@ysu.edu.cn
FU Hebei Natural Science Foundation [C2020203010]; National Natural Science
   Foundation of China [62073280]
FX AcknowledgementsThe authors would like to acknowledge the support from
   the Hebei Natural Science Foundation under grant No. C2020203010 and the
   National Natural Science Foundation of China under Grant No.62073280.
CR An Q., 2021, INT J VEH INF COMMUN, V6, P121
   Bai RY, 2024, VISUAL COMPUT, V40, P287, DOI 10.1007/s00371-023-02782-5
   Baiju PS, 2022, VISUAL COMPUT, V38, P2357, DOI 10.1007/s00371-021-02117-2
   Bian Y, 2022, COLOR TRANSFER BIOME
   Blin R, 2022, IEEE T INTELL TRANSP, V23, P10753, DOI 10.1109/TITS.2021.3095658
   Cai YH, 2021, NEUROCOMPUTING, V423, P264, DOI 10.1016/j.neucom.2020.10.044
   Chen GC, 2022, VISUAL COMPUT, V38, P1051, DOI 10.1007/s00371-021-02067-9
   Chen YT, 2023, INT J MACH LEARN CYB, V14, P2945, DOI 10.1007/s13042-023-01811-y
   Chen YT, 2023, J VIS COMMUN IMAGE R, V91, DOI 10.1016/j.jvcir.2023.103776
   Chen YT, 2024, VISUAL COMPUT, V40, P489, DOI 10.1007/s00371-023-02795-0
   Das B, 2022, VISUAL COMPUT, V38, P179, DOI 10.1007/s00371-020-02010-4
   Hu Q, 2023, VISUAL COMPUT, V39, P997, DOI 10.1007/s00371-021-02380-3
   Li BY, 2019, IEEE T IMAGE PROCESS, V28, P492, DOI 10.1109/TIP.2018.2867951
   Li XL, 2023, VISUAL COMPUT, V39, P663, DOI 10.1007/s00371-021-02365-2
   Lin CY, 2023, IEEE T MULTIMEDIA, V25, P3089, DOI 10.1109/TMM.2022.3155937
   Liu W, 2022, COMPUTER VISION PATT, V2202
   Raikwar SC, 2020, VISUAL COMPUT, V36, P191, DOI 10.1007/s00371-018-1596-5
   Shit S, 2023, COMPUT ANIMAT VIRT W, V34, DOI 10.1002/cav.2147
   Wang H, 2022, IEEE T INTELL TRANSP, V23, P21405, DOI 10.1109/TITS.2022.3177615
   Wang H, 2022, IEEE T INSTRUM MEAS, V71, DOI 10.1109/TIM.2022.3154827
   Wang HF, 2022, IEEE T INTELL TRANSP, V23, P17873, DOI 10.1109/TITS.2022.3157901
   Wu DH, 2020, COMPUT ELECTRON AGR, V178, DOI 10.1016/j.compag.2020.105742
   Xu XW, 2022, REMOTE SENS-BASEL, V14, DOI 10.3390/rs14205276
   Yang CW, 2019, VISUAL COMPUT, V35, P695, DOI 10.1007/s00371-018-1504-z
   Yang K, 2022, J SENS TECHNOL, P1222
   Yin WX, 2023, VISUAL COMPUT, V39, P6723, DOI 10.1007/s00371-022-02759-w
   Zhang JM, 2022, J AMB INTEL SMART EN, V14, P317, DOI 10.3233/AIS-220038
   Zhang JM, 2022, HUM-CENT COMPUT INFO, V12, DOI 10.22967/HCIS.2022.12.023
   Zhang JM., 2023, MULTIMED TOOLS APPL, DOI [10.1007/s11042-023-1542, DOI 10.1007/S11042-023-1542]
   Zhang JC, 2021, PATTERN RECOGN, V118, DOI 10.1016/j.patcog.2021.108045
   Zhang JC, 2020, OPT LETT, V45, P1507, DOI 10.1364/OL.384189
   Zhang SD, 2020, VISUAL COMPUT, V36, P1797, DOI 10.1007/s00371-019-01774-8
   Zhang XH, 2019, IEEE ACCESS, V7, P141662, DOI 10.1109/ACCESS.2019.2943241
   Zhao YQ, 2008, IEEE T GEOSCI REMOTE, V46, P3337, DOI 10.1109/TGRS.2008.920467
   Zhuo YW, 2022, IEEE J-STARS, V15, P7539, DOI 10.1109/JSTARS.2022.3202866
NR 35
TC 4
Z9 4
U1 21
U2 40
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2024
VL 40
IS 5
BP 3199
EP 3215
DI 10.1007/s00371-023-03022-6
EA JUL 2023
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OC3D9
UT WOS:001039828800002
DA 2024-07-18
ER

PT J
AU Ben-loghfyry, A
   Hakim, A
AF Ben-loghfyry, Anouar
   Hakim, Abdelilah
TI Total variable-order variation as a regularizer applied on multi-frame
   image super-resolution
SO VISUAL COMPUTER
LA English
DT Article
DE Total variable-order variation; Fractional calculus; Multi-frame
   super-resolution; ADMM
ID RECONSTRUCTION; REGISTRATION; NOISE
AB Multi-frame image super-resolution reconstruction focuses on obtaining a high-resolution (HR) image from a low-resolution image. Since the super-resolution problem is ill-posed, it is frequent to use regularization techniques. However, the choice of these regularization terms is not always direct. Generally, every term should contain some prior knowledge over the image, that is why it plays an important role in increasing the quality of the restored HR image. In this paper, we propose a total variable-order variation based-regularization to provide a natural-looking and an effective reconstruction of the desired HR image. We also present some existence and uniqueness results of our proposed model. The alternating direction method of multipliers algorithm is employed to implement the numerical simulations. Experimental results reveal that the proposed approach can reconstruct and recover high-quality results visually and quantitatively.
C1 [Ben-loghfyry, Anouar] Univ Hassan 2, Fac Sci & Technol Mohammedia, Dept Math, Casablanca, Morocco.
   [Hakim, Abdelilah] Univ Cadi Ayyad, Dept Math, LAMAI FST, Marrakech, Morocco.
C3 Hassan II University of Casablanca; Cadi Ayyad University of Marrakech
RP Ben-loghfyry, A (corresponding author), Univ Hassan 2, Fac Sci & Technol Mohammedia, Dept Math, Casablanca, Morocco.
EM anouar.benloghfyry@univh2c.ma; abdelilahhakim@gmail.com
OI Anouar, Ben-loghfyry/0000-0003-3883-6957
CR Baker S, 2002, IEEE T PATTERN ANAL, V24, P1167, DOI 10.1109/TPAMI.2002.1033210
   Ben-loghfyry A., 2023, J MATH MODELING
   Ben-Loghfyry A., 2022, MATH MODELING COMPUT, V9, P351, DOI [10.23939/mmc2022.02.351, DOI 10.23939/MMC2022.02.351]
   Ben-loghfyry A., 2022, INT J APPL COMPUT MA, V8, P177, DOI DOI 10.1007/S40819-022-01380-8
   Ben-Loghfyry A, 2023, J APPL MATH COMPUT, V69, P1431, DOI 10.1007/s12190-022-01798-9
   Ben-loghfyry A, 2022, MATH METHOD APPL SCI, V45, P9719, DOI 10.1002/mma.8331
   Buades A, 2006, IEEE T IMAGE PROCESS, V15, P1499, DOI 10.1109/TIP.2006.871137
   Chen CB, 2015, VISUAL COMPUT, V31, P1217, DOI 10.1007/s00371-014-1007-5
   Farsiu S, 2006, IEEE T IMAGE PROCESS, V15, P141, DOI 10.1109/TIP.2005.860336
   Farsiu S, 2004, IEEE T IMAGE PROCESS, V13, P1327, DOI 10.1109/TIP.2004.834669
   Greenspan H, 2002, MAGN RESON IMAGING, V20, P437, DOI 10.1016/S0730-725X(02)00511-8
   Greenspan H, 2009, COMPUT J, V52, P43, DOI 10.1093/comjnl/bxm075
   Hakim A, 2019, AIMS MATH, V4, P1320, DOI 10.3934/math.2019.5.1320
   He Y, 2006, IEEE IMAGE PROC, P1729, DOI 10.1109/ICIP.2006.312715
   He Y, 2007, IEEE T IMAGE PROCESS, V16, P2830, DOI 10.1109/TIP.2007.908074
   Holland DA, 2006, ISPRS J PHOTOGRAMM, V60, P212, DOI 10.1016/j.isprsjprs.2006.02.002
   Jalalzai K, 2016, J MATH IMAGING VIS, V54, P256, DOI 10.1007/s10851-015-0600-1
   Laghrib A, 2018, SIGNAL PROCESS-IMAGE, V67, P1, DOI 10.1016/j.image.2018.05.011
   Laghrib A, 2015, EURASIP J IMAGE VIDE, DOI 10.1186/s13640-015-0075-4
   Li HF, 2016, NEUROCOMPUTING, V171, P138, DOI 10.1016/j.neucom.2015.06.035
   Li XL, 2010, SIGNAL PROCESS, V90, P405, DOI 10.1016/j.sigpro.2009.05.028
   Lv Z, 2017, SIGNAL PROCESS-IMAGE, V58, P199, DOI 10.1016/j.image.2017.08.006
   Maiseli BJ, 2015, SIGNAL PROCESS-IMAGE, V34, P1, DOI 10.1016/j.image.2015.03.001
   Malesza W, 2015, LECT NOTES ELECTR EN, V320, P71, DOI 10.1007/978-3-319-09900-2_7
   Marquina A, 2008, J SCI COMPUT, V37, P367, DOI 10.1007/s10915-008-9214-8
   Mortazavi M, 2023, VISUAL COMPUT, V39, P3011, DOI 10.1007/s00371-022-02509-y
   Park SC, 2003, IEEE SIGNAL PROC MAG, V20, P21, DOI 10.1109/MSP.2003.1203207
   Patnaik S, 2020, NONLINEAR DYNAM, V100, P561, DOI 10.1007/s11071-020-05488-8
   Podlubny I, 1999, Fractional Differential Equations
   Ren ZM, 2013, SIGNAL PROCESS, V93, P2408, DOI 10.1016/j.sigpro.2013.02.015
   Robinson M. Dirk, 2010, Super-Resolution Imaging
   Shi YY, 2019, IEEE SENS J, V19, P9850, DOI 10.1109/JSEN.2019.2926232
   Shukla AK, 2021, MULTIMED TOOLS APPL, V80, P30213, DOI 10.1007/s11042-020-08968-6
   Sierociuk D, 2014, B POL ACAD SCI-TECH, V62, P809, DOI 10.2478/bpasts-2014-0089
   Sierociuk D, 2015, CIRC SYST SIGNAL PR, V34, P1077, DOI 10.1007/s00034-014-9895-1
   Sierociuk D, 2014, CIRC SYST SIGNAL PR, V33, P3861, DOI 10.1007/s00034-014-9835-0
   Suhua Wei, 2009, 2009 17th European Signal Processing Conference (EUSIPCO 2009), P2579
   Yang XJ, 2017, PHYSICA A, V481, P276, DOI 10.1016/j.physa.2017.04.054
   Yang XM, 2019, VISUAL COMPUT, V35, P1755, DOI 10.1007/s00371-018-1570-2
   Yao WJ, 2020, J SCI COMPUT, V82, DOI 10.1007/s10915-020-01185-1
   Yue LW, 2014, SIGNAL PROCESS, V105, P156, DOI 10.1016/j.sigpro.2014.04.031
   ZEIDLER E., 1990, NONLINEAR FUNCTIONAL
   Zeng WL, 2015, NEUROCOMPUTING, V162, P218, DOI 10.1016/j.neucom.2015.03.049
   Zeng XY, 2013, DIGIT SIGNAL PROCESS, V23, P98, DOI 10.1016/j.dsp.2012.06.013
   Zhang HY, 2012, SIGNAL PROCESS, V92, P2082, DOI 10.1016/j.sigpro.2012.01.020
   Zhang JP, 2015, SIAM J IMAGING SCI, V8, P2487, DOI 10.1137/14097121X
   Zhang YS, 2018, MULTIDIM SYST SIGN P, V29, P999, DOI 10.1007/s11045-017-0482-z
NR 47
TC 4
Z9 4
U1 1
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2024
VL 40
IS 4
BP 2949
EP 2959
DI 10.1007/s00371-023-02996-7
EA JUL 2023
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA MZ2U3
UT WOS:001029318000001
DA 2024-07-18
ER

PT J
AU Meng, DD
   Li, S
   Sheng, B
   Wu, H
   Tian, SQ
   Ma, WJ
   Wang, GP
   Yan, XQ
AF Meng, Dongdong
   Li, Sheng
   Sheng, Bin
   Wu, Hao
   Tian, Suqing
   Ma, Wenjun
   Wang, Guoping
   Yan, Xueqing
TI 3D reconstruction-oriented fully automatic multi-modal tumor
   segmentation by dual attention-guided VNet
SO VISUAL COMPUTER
LA English
DT Article
DE Attention mechanism; Deep learning; Neural network; Segmentation;
   Multi-modality; 3D reconstruction
ID LESION SEGMENTATION; AUTO-SEGMENTATION; NET; IMAGES; HEAD
AB Existing automatic contouring methods for primary nasopharyngeal carcinoma (NPC) and metastatic lymph nodes (MLNs) may suffer from low segmentation accuracy and cannot handle multi-modal images correctly. Furthermore, high inter-patient physiological variations and ineffective multi-modal information fusion pose further difficulties. To address these issues, a 3D reconstruction-oriented fully automatic multi-modal segmentation method has been presented to delineate primary NPC tumors and MLNs via a dual attention-guided VNet. Specifically, we leverage a physiologically-sensitive feature enhancement (PFE) module that emphasizes long-range spatial context information in tumor regions of interest and thereby copes with the variability resulting from inter-patient characteristics. This can help extract the 3D spatial feature and facilitate the high-quality reconstruction of 3D geometry of tumors. Next, we develop a multi-modal feature aggregation (MFA) module to describe multi-scale modality-aware features, exploring the effective information aggregation of multi-modal images. To the best of our knowledge, this is the first fully automatic, highly accurate segmentation framework of the primary NPC tumors and MLNs on combined CT-MR datasets. Experimental results on clinical medical datasets validate the effectiveness of our method, and it outperforms the state-of-the-art methods.
C1 [Meng, Dongdong; Ma, Wenjun; Yan, Xueqing] Peking Univ, Sch Phys, Beijing 100871, Peoples R China.
   [Meng, Dongdong; Li, Sheng; Wang, Guoping] Peking Univ, Sch Comp Sci, Beijing 100871, Peoples R China.
   [Li, Sheng; Wang, Guoping] Peking Univ, Natl Biomed Imaging Ctr, Beijing, Peoples R China.
   [Sheng, Bin] Shanghai Jiao Tong Univ, Dept Comp Sci & Engn, Shanghai 200240, Peoples R China.
   [Wu, Hao] Peking Univ Canc Hosp, Dept Radiotherapy, Beijing 100142, Peoples R China.
   [Tian, Suqing] Peking Univ Third Hosp, Dept Radiat Oncol, Beijing 100191, Peoples R China.
C3 Peking University; Peking University; Peking University; Shanghai Jiao
   Tong University; Peking University
RP Yan, XQ (corresponding author), Peking Univ, Sch Phys, Beijing 100871, Peoples R China.; Li, S (corresponding author), Peking Univ, Sch Comp Sci, Beijing 100871, Peoples R China.; Li, S (corresponding author), Peking Univ, Natl Biomed Imaging Ctr, Beijing, Peoples R China.
EM lisheng@pku.edu.cn; x.yan@pku.edu.cn
RI wang, haoyu/KHY-6295-2024; li, yf/KHX-1148-2024; wang,
   guoping/KQU-3394-2024
OI wang, haoyu/0009-0001-2467-5331; Li, Sheng/0000-0002-8901-2184
FU National Natural Science Foundation of China [11921006]; Beijing
   Outstanding Young Scientists Program; National Grand Instrument Project
   [2019YFF010144003]; Southern Marine Science and Engineering Guangdong
   Laboratory (Zhuhai) [SML2021SP101]
FX This work was supported by the National Natural Science Foundation of
   China (No. 11921006), Beijing Outstanding Young Scientists Program, the
   National Grand Instrument Project (No. 2019YFF01014400), Southern Marine
   Science and Engineering Guangdong Laboratory (Zhuhai) (No.
   SML2021SP101).
CR Azad R., 2022, ARXIV
   Cabezas M, 2011, COMPUT METH PROG BIO, V104, pE158, DOI 10.1016/j.cmpb.2011.07.015
   Chang ET, 2006, CANCER EPIDEM BIOMAR, V15, P1765, DOI 10.1158/1055-9965.EPI-06-0353
   Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49
   cicek Ozgtin, 2016, INT C MED IM COMP CO, P424, DOI DOI 10.1007/978-3-319-46723-8_49
   Daisne JF, 2013, RADIAT ONCOL, V8, DOI 10.1186/1748-717X-8-154
   DenOtter TD., 2022, HOUNSFIELD UNIT
   Dolz J, 2019, LECT NOTES COMPUT SC, V11383, P271, DOI 10.1007/978-3-030-11723-8_27
   Fedorov A, 2012, MAGN RESON IMAGING, V30, P1323, DOI 10.1016/j.mri.2012.05.001
   Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326
   Guo MH, 2022, COMPUT VIS MEDIA, V8, P331, DOI 10.1007/s41095-022-0271-y
   Hatamizadeh A, 2022, IEEE WINT CONF APPL, P1748, DOI 10.1109/WACV51458.2022.00181
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang B, 2018, CONTRAST MEDIA MOL I, DOI 10.1155/2018/8923028
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Jiang HY, 2022, J SUPERCOMPUT, V78, P1807, DOI 10.1007/s11227-021-03901-6
   Joseph Sushitha Susan, 2019, Information Systems Design and Intelligent Applications. Proceedings of Fifth International Conference INDIA 2018. Advances in Intelligent Systems and Computing (AISC 862), P223, DOI 10.1007/978-981-13-3329-3_21
   Kavur AE, 2021, MED IMAGE ANAL, V69, DOI 10.1016/j.media.2020.101950
   Khan S, 2022, ACM COMPUT SURV, V54, DOI 10.1145/3505244
   Kosmin M, 2019, RADIOTHER ONCOL, V135, P130, DOI 10.1016/j.radonc.2019.03.004
   Lachinov D, 2019, LECT NOTES COMPUT SC, V11384, P189, DOI 10.1007/978-3-030-11726-9_17
   Le Moal J, 2018, J THORAC DIS, V10, P196, DOI 10.21037/jtd.2017.11.144
   Lee FKH, 2005, INT J RADIAT ONCOL, V61, P608, DOI 10.1016/j.ijrobp.2004.09.024
   Li Y, 2022, IEEE T MED IMAGING, V41, P1639, DOI 10.1109/TMI.2022.3144274
   Lin L, 2019, RADIOLOGY, V291, P677, DOI 10.1148/radiol.2019182012
   Liu HJ, 2022, NEUROCOMPUTING, V506, P158, DOI 10.1016/j.neucom.2022.07.054
   Liu LL, 2020, NEUROCOMPUTING, V409, P244, DOI 10.1016/j.neucom.2020.05.070
   Lorensen William E., 1987, COMPUT GRAPH, P163, DOI DOI 10.1145/37402.37422
   Ma ZQ, 2019, PHYS MED BIOL, V64, DOI 10.1088/1361-6560/aaf5da
   Ma ZQ, 2018, EXP THER MED, V16, P2511, DOI 10.3892/etm.2018.6478
   Men K, 2017, FRONT ONCOL, V7, DOI 10.3389/fonc.2017.00315
   Milletari F, 2016, INT CONF 3D VISION, P565, DOI 10.1109/3DV.2016.79
   Mohammed MA, 2017, J COMPUT SCI-NETH, V21, P283, DOI 10.1016/j.jocs.2017.03.021
   Qazi AA, 2011, MED PHYS, V38, P6160, DOI 10.1118/1.3654160
   Ren JT, 2021, ACTA ONCOL, V60, P1399, DOI 10.1080/0284186X.2021.1949034
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Sinha A, 2021, IEEE J BIOMED HEALTH, V25, P121, DOI 10.1109/JBHI.2020.2986926
   Stapleford LJ, 2010, INT J RADIAT ONCOL, V77, P959, DOI 10.1016/j.ijrobp.2009.09.023
   Tang P, 2021, NEUROCOMPUTING, V435, P103, DOI 10.1016/j.neucom.2020.12.085
   Teguh DN, 2011, INT J RADIAT ONCOL, V81, P950, DOI 10.1016/j.ijrobp.2010.07.009
   Wang CW, 2022, LECT NOTES COMPUT SC, V13432, P528, DOI 10.1007/978-3-031-16434-7_51
   Wang L, 2022, VOLATILE UPTAKE TRAN, P1, DOI [DOI 10.1016/J.TPLANTS.2021.08.017, 10.1016/j.tplants.2021.08.017]
   Wang PF, 2023, VISUAL COMPUT, V39, P915, DOI 10.1007/s00371-021-02374-1
   Wang SL, 2018, PROC CVPR IEEE, P2589, DOI 10.1109/CVPR.2018.00274
   Wang Y, 2019, IEEE T MED IMAGING, V38, P2768, DOI 10.1109/TMI.2019.2913184
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Xu RT, 2021, LECT NOTES COMPUT SC, V12901, P503, DOI 10.1007/978-3-030-87193-2_48
   Zhang ZP, 2019, IEEE ACM T COMPUT BI, V16, P407, DOI 10.1109/TCBB.2017.2704587
   Zhou J, 2006, I S BIOMED IMAGING, P1364
   Zhou TX, 2019, ARRAY-NY, V3-4, DOI 10.1016/j.array.2019.100004
   Zhou ZW, 2020, IEEE T MED IMAGING, V39, P1856, DOI 10.1109/TMI.2019.2959609
NR 52
TC 2
Z9 3
U1 4
U2 13
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2023
VL 39
IS 8
SI SI
BP 3183
EP 3196
DI 10.1007/s00371-023-02965-0
EA JUL 2023
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA P2DS6
UT WOS:001027416100003
OA Bronze
DA 2024-07-18
ER

PT J
AU Du, XJ
   Wu, HL
AF Du, Xianjun
   Wu, Hailei
TI Gated aggregation network for cloud detection in remote sensing image
SO VISUAL COMPUTER
LA English
DT Article
DE Remote sensing image; Image segmentation; Gated aggregation;
   Interpretability
ID SHADOW; ALGORITHM; SNOW; SEGMENTATION
AB Cloud detection is one of the important tasks in remote sensing image preprocessing, and this paper uses RGB preview images of remote sensing images to extract cloud regions efficiently. The preview images make the detection of cloud regions more challenging due to the lack of resolution and spectral information. The existing remote sensing image cloud detection methods, and feature fusion process, due to unreasonable feature fusion strategy, so that the encoded features cannot be fully utilized, which may introduce noise information, and ultimately lead to the problems of false detection and missing detection. To address these problems, this work designs a gated aggregation network (GANet) for remote sensing image cloud detection. GANet has a novel encoder-decoder architecture, a gated feature aggregation module (GFAM), and a pyramidal attention pooling module (PAPM). GFAM bridges the gap between high resolution with spatial details and low-resolution features with high-level semantics, fully selectively fusing semantic and spatial features to alleviate the semantic divide problem when fusing multi-level features. PAPM extracts multi-scale global contextual features without loss of resolution. The method is validated on three datasets: the publicly available 38-Cloud and SPARCS datasets and the self-built Landsat-8 cloud detection dataset with higher spatial resolution. The experimental results show that the proposed method achieves competitive performance under different evaluation metrics. Codes and datasets can be found at https://github.com/HaiLei-Fly/GANet/.
C1 [Du, Xianjun; Wu, Hailei] Lanzhou Univ Technol, Coll Elect & Informat Engn, Lanzhou 730050, Peoples R China.
C3 Lanzhou University of Technology
RP Du, XJ (corresponding author), Lanzhou Univ Technol, Coll Elect & Informat Engn, Lanzhou 730050, Peoples R China.
EM xdu@lut.edu.cn; wu.hai.lei.fly@gmail.com
FU National Natural Science Foundation of China [62241307]; Scientific and
   Technological Project of Gansu Province [22YF7FA166]; Scientific and
   Technological Project of Lanzhou City [2022-RC-60]; University
   Innovation Fund Project of Gansu Province [2021A-027]
FX This work is supported in part by the National Natural Science
   Foundation of China (62241307), the Scientific and Technological Project
   of Gansu Province (22YF7FA166), the Scientific and Technological Project
   of Lanzhou City (2022-RC-60), and the University Innovation Fund Project
   of Gansu Province (2021A-027).
CR Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   Ackerman SA, 1998, J GEOPHYS RES-ATMOS, V103, P32141, DOI 10.1029/1998JD200032
   Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Cheng MM, 2021, INT J COMPUT VISION, V129, P2622, DOI [10.1007/s11263-021-01490-8, 10.1109/ICCV.2017.487]
   De Boer PT, 2005, ANN OPER RES, V134, P19, DOI 10.1007/s10479-005-5724-z
   Farabet C, 2013, IEEE T PATTERN ANAL, V35, P1915, DOI 10.1109/TPAMI.2012.231
   Fu KR, 2022, IEEE T PATTERN ANAL, V44, P5541, DOI 10.1109/TPAMI.2021.3073689
   GESELL G, 1989, INT J REMOTE SENS, V10, P897, DOI 10.1080/01431168908903929
   Guo HW, 2021, IEEE J-STARS, V14, P9743, DOI 10.1109/JSTARS.2021.3114171
   Guo JH, 2021, IEEE T GEOSCI REMOTE, V59, P700, DOI 10.1109/TGRS.2020.2991398
   Guo YN, 2020, SYMMETRY-BASEL, V12, DOI 10.3390/sym12061056
   He Kaiming, 2016, EUR C COMP VIS ECCV, DOI [DOI 10.1109/CVPR.2016.90, DOI 10.1007/978-3-319-46493-0_38]
   Hu K, 2021, REMOTE SENS-BASEL, V13, DOI 10.3390/rs13224533
   Hughes MJ, 2014, REMOTE SENS-BASEL, V6, P4907, DOI 10.3390/rs6064907
   Islam MA, 2017, PROC CVPR IEEE, P4877, DOI 10.1109/CVPR.2017.518
   Li ZQ, 2016, REMOTE SENS ENVIRON, V173, P59, DOI 10.1016/j.rse.2015.11.019
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Lu C, 2022, NEURAL COMPUT APPL, V34, P6149, DOI 10.1007/s00521-021-06802-0
   Magney TS, 2016, REMOTE SENS ENVIRON, V173, P84, DOI 10.1016/j.rse.2015.11.013
   Margolin R, 2014, PROC CVPR IEEE, P248, DOI 10.1109/CVPR.2014.39
   Mohajerani S., 2018, ARXIV, DOI DOI 10.48550/ARXIV.1810.05782
   Mohajerani S, 2021, IEEE J-STARS, V14, P4254, DOI 10.1109/JSTARS.2021.3070786
   Mohajerani S, 2019, INT GEOSCI REMOTE SE, P1029, DOI [10.1109/IGARSS.2019.8898776, 10.1109/igarss.2019.8898776]
   Peng L, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2021.3095936
   Pu WH, 2022, REMOTE SENS-BASEL, V14, DOI 10.3390/rs14174312
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   ROSSOW WB, 1993, J CLIMATE, V6, P2341, DOI 10.1175/1520-0442(1993)006<2341:CDUSMO>2.0.CO;2
   Rusk N, 2016, NAT METHODS, V13, P35, DOI 10.1038/nmeth.3707
   Stöckli R, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11091052
   Thus G., 2020, SUPPL MAT, DOI DOI 10.1109/WACV45572.2020.9093564
   Wang WG, 2022, IEEE T PATTERN ANAL, V44, P3239, DOI 10.1109/TPAMI.2021.3051099
   Wang WJ, 2022, IEEE GEOSCI REMOTE S, V19, DOI 10.1109/LGRS.2021.3110869
   Xia M, 2021, INT J REMOTE SENS, V42, P2022, DOI 10.1080/01431161.2020.1849852
   Xie FY, 2017, IEEE J-STARS, V10, P3631, DOI 10.1109/JSTARS.2017.2686488
   Yang F, 2017, WATER-SUI, V9, DOI 10.3390/w9020144
   Yang GR, 2016, NAT COMMUN, V7, DOI 10.1038/ncomms12815
   Yang JY, 2019, IEEE T GEOSCI REMOTE, V57, P6195, DOI 10.1109/TGRS.2019.2904868
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zeng XS, 2018, PROCEEDINGS OF THE 2018 2ND INTERNATIONAL CONFERENCE ON BIG DATA RESEARCH (ICBDR 2018), P174, DOI 10.1145/3291801.3291839
   Zhan YJ, 2017, IEEE GEOSCI REMOTE S, V14, P1785, DOI 10.1109/LGRS.2017.2735801
   Zhang L, 2018, PROC CVPR IEEE, P1741, DOI 10.1109/CVPR.2018.00187
   Zhao CH, 2022, IEEE GEOSCI REMOTE S, V19, DOI 10.1109/LGRS.2021.3096526
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zhong B, 2017, IEEE J-STARS, V10, P4898, DOI 10.1109/JSTARS.2017.2734912
   Zhu Z, 2015, REMOTE SENS ENVIRON, V159, P269, DOI 10.1016/j.rse.2014.12.014
NR 46
TC 0
Z9 0
U1 7
U2 11
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2024
VL 40
IS 4
BP 2517
EP 2536
DI 10.1007/s00371-023-02934-7
EA JUN 2023
PG 20
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA MZ2U3
UT WOS:001020988900001
DA 2024-07-18
ER

PT J
AU Bao, YT
   Yu, YB
   Qi, Y
   Wang, ZH
AF Bao, Yongtang
   Yu, Yongbo
   Qi, Yue
   Wang, Zhihui
TI Multiple object tracking with adaptive multi-features fusion and
   improved learnable graph matching
SO VISUAL COMPUTER
LA English
DT Article
DE Graph matching; Graph convolution; Object detection; Feature extraction;
   Target association; Multiple object tracking
ID ATTENTION
AB Multiple object tracking is challenging due to the complex spatiotemporal relationship and the occlusion of different targets. Most existing methods use separate neural networks to generate robust features for data association inside the targets' bounding boxes. Unlike existing methods that just consider each target and the trajectory formed independently while ignoring the context information between tracklet and intra-frame detection, this paper proposes a tracking algorithm that combines multi-channel features with learnable graph matching. In brief, we use global and local salient features to model the appearance of intra-frame targets based on parallel graphs and mining high-order intra-context relationships using a completely undirected graph relationship between trajectories and detections. On this basis, we establish a multi-channel message communication mechanism through the maximum pooling mechanism to transmit the maximum dissimilation registration results to the graph matching layer, thereby forming a more robust allocation result in the data association process. Lastly, the evaluations on MOTChallenge benchmarks verify the effectiveness of the proposed method.
C1 [Bao, Yongtang; Yu, Yongbo; Wang, Zhihui] Shandong Univ Sci & Technol, Coll Comp Sci & Engn, Qingdao 266510, Peoples R China.
   [Qi, Yue] Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing 100191, Peoples R China.
   [Qi, Yue] Beihang Univ, Virtual Real Res Inst, Qingdao Res Inst, Qingdao 266100, Peoples R China.
   [Qi, Yue] Peng Cheng Lab, Shenzhen, Peoples R China.
C3 Shandong University of Science & Technology; Beihang University; Beihang
   University; Peng Cheng Laboratory
RP Wang, ZH (corresponding author), Shandong Univ Sci & Technol, Coll Comp Sci & Engn, Qingdao 266510, Peoples R China.
EM baozi0221@sdust.edu.cn; yuyongbo@sdust.edu.cn; qy@buaa.edu.cn;
   zh_wang@sdust.edu.cn
RI qi, yue/KLE-0386-2024; Wang, Zhihui/D-2915-2015
OI Wang, Zhihui/0000-0001-8140-1882
FU Shandong Provincial Natural Science Foundation [ZR2020MF132]; National
   Natural Science Foundation of China [62072020]
FX AcknowledgementsThis paper is supported by the Shandong Provincial
   Natural Science Foundation (ZR2020MF132), the National Natural Science
   Foundation of China (No. 62072020).
CR Aflalo Y, 2015, P NATL ACAD SCI USA, V112, P2942, DOI 10.1073/pnas.1401651112
   Amos B., 2017, ARXIV
   Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014
   Bergmann P, 2019, IEEE I CONF COMP VIS, P941, DOI 10.1109/ICCV.2019.00103
   Bewley A, 2016, IEEE IMAGE PROC, P3464, DOI 10.1109/ICIP.2016.7533003
   Severino JVB, 2019, EXPERT SYST APPL, V136, P304, DOI 10.1016/j.eswa.2019.06.048
   Chen YT, 2023, J VIS COMMUN IMAGE R, V91, DOI 10.1016/j.jvcir.2023.103776
   Chen YT, 2024, VISUAL COMPUT, V40, P489, DOI 10.1007/s00371-023-02795-0
   Chen YT, 2021, APPL INTELL, V51, P4367, DOI 10.1007/s10489-020-02116-1
   Chu P., 2019, ARXIV
   Dendorfer P., 2020, ARXIV
   Dey S, 2023, MATH PROGRAM, V197, P449, DOI 10.1007/s10107-022-01892-7
   Diamond S, 2016, J MACH LEARN RES, V17
   Gomez-Silva MJ, 2022, EXPERT SYST APPL, V206, DOI 10.1016/j.eswa.2022.117813
   He JW, 2021, PROC CVPR IEEE, P5295, DOI 10.1109/CVPR46437.2021.00526
   Henschel R, 2019, IEEE COMPUT SOC CONF, P770, DOI 10.1109/CVPRW.2019.00105
   Jeong J, 2017, EXPERT SYST APPL, V79, P194, DOI 10.1016/j.eswa.2017.02.043
   Kim S, 2023, APPL INTELL, V53, P930, DOI 10.1007/s10489-022-03473-9
   Kuhn HW, 2005, NAV RES LOG, V52, P7, DOI 10.1002/nav.20053
   LAWLER EL, 1963, MANAGE SCI, V9, P586, DOI 10.1287/mnsc.9.4.586
   Leordeanu M, 2005, IEEE I CONF COMP VIS, P1482
   Li JH, 2020, IEEE WINT CONF APPL, P708, DOI 10.1109/WACV45572.2020.9093347
   Li Zhang, 2008, 2008 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops (CVPR Workshops), P1, DOI 10.1109/CVPRW.2008.4563097
   Li ZY, 2015, VISUAL COMPUT, V31, P1319, DOI 10.1007/s00371-014-1014-6
   Liu QK, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P530
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Luo WH, 2021, ARTIF INTELL-AMST, V293, DOI 10.1016/j.artint.2020.103448
   Micheal AA, 2022, J INDIAN SOC REMOTE, V50, P2543, DOI 10.1007/s12524-022-01615-7
   Min H., 2009, P IEEE INT S MULT, P1
   Nesterov Y., 2004, INTRO LECT CONVEX OP, P1, DOI DOI 10.1007/978-1-4419-8853-9_1
   Papakis I., 2020, ARXIV
   REID DB, 1979, IEEE T AUTOMAT CONTR, V24, P843, DOI 10.1109/TAC.1979.1102177
   Ristani E, 2016, LECT NOTES COMPUT SC, V9914, P17, DOI 10.1007/978-3-319-48881-3_2
   Rodriguez M, 2009, IEEE I CONF COMP VIS, P1389, DOI 10.1109/ICCV.2009.5459301
   Saada M, 2022, COMPUT VIS IMAGE UND, V225, DOI 10.1016/j.cviu.2022.103569
   Singh V.K., 2008, IEEE WORKSH MOT VID
   Soundararajan P, 2006, LECT NOTES COMPUT SC, V4338, P829
   Suljagic H, 2022, NEURAL COMPUT APPL, V34, P18171, DOI 10.1007/s00521-022-07456-2
   Sun ZH, 2021, IEEE T CIRC SYST VID, V31, P1819, DOI 10.1109/TCSVT.2020.3009717
   Tullo D, 2023, RES DEV DISABIL, V133, DOI 10.1016/j.ridd.2022.104402
   Wang Z., 2022, IEEE T INTELL T SYST, V99, P1
   Wojke N, 2017, IEEE IMAGE PROC, P3645, DOI 10.1109/ICIP.2017.8296962
   Xingyi Zhou, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P474, DOI 10.1007/978-3-030-58548-8_28
   Yang JM, 2022, NEURAL PROCESS LETT, V54, P4893, DOI 10.1007/s11063-022-10840-7
   Yoon JH, 2016, PROC CVPR IEEE, P1392, DOI 10.1109/CVPR.2016.155
   YOU S, 2021, IEEE INT INT TECHN C, P1
   Zanfir A, 2018, PROC CVPR IEEE, P2684, DOI 10.1109/CVPR.2018.00284
   Zhang X., 2021, VISUAL COMPUT, V5, P37
   Zhou F, 2016, IEEE T PATTERN ANAL, V38, P1774, DOI 10.1109/TPAMI.2015.2501802
NR 49
TC 2
Z9 2
U1 4
U2 11
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2024
VL 40
IS 4
BP 2279
EP 2292
DI 10.1007/s00371-023-02916-9
EA JUN 2023
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA MZ2U3
UT WOS:001016490300004
DA 2024-07-18
ER

PT J
AU Chen, MM
   Wang, P
   Shang, DH
   Wang, PJ
AF Chen, Miaomiao
   Wang, Pei
   Shang, Dehai
   Wang, Pengjie
TI Cycle-attention-derain: unsupervised rain removal with CycleGAN
SO VISUAL COMPUTER
LA English
DT Article
DE Generative adversarial networks; Rain removal; Unsupervised method;
   Attention
AB Single image deraining is a fundamental task in computer vision, which can greatly improve the performance of subsequent high-level tasks under rainy conditions. Existing data-driven rain removal methods rely heavily on paired training data, which is expensive to collect. In this paper, we propose a new unsupervised method, called Cycle-Attention-Derain, which removes rain from single images in the absence of paired data. Our method is based on the CycleGAN framework with two major novelties. First, since rain removal is highly correlated with analyzing the texture features of an input image, we propose a novel attention fusion module (AFM) with complementary channel attention and spatial attention, which can effectively learn more discriminative features for rain removal. Second, to further improve the generalization ability of our model, we propose a global-local attention discriminator architecture with an attention mechanism to guide the network training, so that the rain removal results are realistic, both globally and locally. Our proposed model is able to remove rain streaks and raindrops of varying degrees without paired training images. Extensive experiments on synthetic and real datasets demonstrate that the proposed method outperforms most of the state-of-the-art unsupervised rain removal methods in terms of both PSNR and SSIM on Rain800 datasets and achieves slightly close results to other popular supervised learning methods.
C1 [Chen, Miaomiao; Wang, Pei; Shang, Dehai; Wang, Pengjie] Dalian Minzu Univ, Sch Comp Sci, 18 Liaohe West Rd, Dalian 116000, Liaoning, Peoples R China.
C3 Dalian Minzu University
RP Wang, PJ (corresponding author), Dalian Minzu Univ, Sch Comp Sci, 18 Liaohe West Rd, Dalian 116000, Liaoning, Peoples R China.
EM chenmmxhj@163.com; peiwang_dlnu@qq.com; dehaishang@qq.com;
   pengjiewang@qq.com
RI Chen, Miaomiao/V-1871-2019; Wang, Pengjie/KHE-3288-2024
OI Chen, Miaomiao/0000-0002-6984-5787; 
FU Natural Science Foundation of Liaoning [2023-MS-133]; Dalian Minzu
   University [110222, 140140, 11200702]
FX AcknowledgementsOur work is supported by Natural Science Foundation of
   Liaoning (Ref.: 2023-MS-133) and Grants from Dalian Minzu University
   (Ref.: 110222, 140140 and 11200702).
CR Chen HT, 2021, PROC CVPR IEEE, P12294, DOI 10.1109/CVPR46437.2021.01212
   Deng LJ, 2018, APPL MATH MODEL, V59, P662, DOI 10.1016/j.apm.2018.03.001
   Fleer S, 2020, PLOS ONE, V15, DOI 10.1371/journal.pone.0226880
   Fu XY, 2017, PROC CVPR IEEE, P1715, DOI 10.1109/CVPR.2017.186
   Fu XY, 2017, IEEE T IMAGE PROCESS, V26, P2944, DOI 10.1109/TIP.2017.2691802
   Goodfellow I. J., 2014, ARXIV
   Gu SH, 2017, IEEE I CONF COMP VIS, P1717, DOI 10.1109/ICCV.2017.189
   Guo ZK, 2022, SIGNAL IMAGE VIDEO P, V16, P185, DOI 10.1007/s11760-021-01972-9
   Huynh-Thu Q, 2008, ELECTRON LETT, V44, P800, DOI 10.1049/el:20080522
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jiang YF, 2021, IEEE T IMAGE PROCESS, V30, P2340, DOI 10.1109/TIP.2021.3051462
   Jiang-Jiang Liu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10093, DOI 10.1109/CVPR42600.2020.01011
   Jin X, 2019, IEEE IMAGE PROC, P2761, DOI [10.1109/ICIP.2019.8803238, 10.1109/icip.2019.8803238]
   Kingma D. P., 2014, arXiv
   Kui Jiang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8343, DOI 10.1109/CVPR42600.2020.00837
   Li X, 2018, LECT NOTES COMPUT SC, V11211, P262, DOI 10.1007/978-3-030-01234-2_16
   Li Y, 2016, PROC CVPR IEEE, P2736, DOI 10.1109/CVPR.2016.299
   Lin CY, 2020, IEEE T IMAGE PROCESS, V29, P9250, DOI 10.1109/TIP.2020.3025402
   Luo Y, 2015, IEEE I CONF COMP VIS, P3397, DOI 10.1109/ICCV.2015.388
   Nair V., 2010, P 27 INT C MACHINE L, P807
   Patil PW, 2022, PATTERN RECOGN, V122, DOI 10.1016/j.patcog.2021.108350
   Qian R, 2018, PROC CVPR IEEE, P2482, DOI 10.1109/CVPR.2018.00263
   Ren DW, 2019, PROC CVPR IEEE, P3932, DOI 10.1109/CVPR.2019.00406
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Sultana M, 2022, PATTERN RECOGN, V129, DOI 10.1016/j.patcog.2022.108719
   Wang B, 2017, CGI'17: PROCEEDINGS OF THE COMPUTER GRAPHICS INTERNATIONAL CONFERENCE, DOI 10.1145/3095140.3095172
   Wang HY, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P1371
   Wang H, 2020, PROC CVPR IEEE, P3100, DOI 10.1109/CVPR42600.2020.00317
   Wang TY, 2019, PROC CVPR IEEE, P12262, DOI 10.1109/CVPR.2019.01255
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wei W, 2019, PROC CVPR IEEE, P3872, DOI 10.1109/CVPR.2019.00400
   Wei YY, 2021, IEEE T IMAGE PROCESS, V30, P4788, DOI 10.1109/TIP.2021.3074804
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Wu ZY, 2022, PATTERN RECOGN, V121, DOI 10.1016/j.patcog.2021.108212
   Yang W., 2017, PROC CVPR IEEE, P1685, DOI DOI 10.1109/CVPR.2017.183
   Yang WH, 2020, IEEE T PATTERN ANAL, V42, P1377, DOI 10.1109/TPAMI.2019.2895793
   Zhang H, 2020, IEEE T CIRC SYST VID, V30, P3943, DOI 10.1109/TCSVT.2019.2920407
   Zhang H, 2018, PROC CVPR IEEE, P695, DOI 10.1109/CVPR.2018.00079
   Zhang H, 2017, IEEE WINT CONF APPL, P1259, DOI 10.1109/WACV.2017.145
   Zhang ZY, 2021, PATTERN RECOGN, V118, DOI 10.1016/j.patcog.2021.108003
   Zhao B, 2017, IEEE T MULTIMEDIA, V19, P1245, DOI 10.1109/TMM.2017.2648498
   Zhu, 2019, PROCEEDING AM ASS AR
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 43
TC 2
Z9 2
U1 12
U2 24
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2023
VL 39
IS 8
SI SI
BP 3727
EP 3739
DI 10.1007/s00371-023-02947-2
EA JUN 2023
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA P2DS6
UT WOS:001019486000002
OA Bronze
DA 2024-07-18
ER

PT J
AU Xin, ZM
   Lu, TW
   Li, YZ
   You, XE
AF Xin, Zhimeng
   Lu, Tongwei
   Li, Yuzhou
   You, Xinge
TI MultiCut-MultiMix: a two-level data augmentation method for detecting
   small and densely distributed objects in large-size images
SO VISUAL COMPUTER
LA English
DT Article
DE SDD object detection; Image cutting; Image fusion; Feature
   representation
AB Detecting small and densely distributed (SDD) objects in large-size images is quite challenging due to the facts that directly inputting such images to the detection network would result in severe geometric deformation and the overcrowded objects would lead to unclear feature expression. In this paper, we propose a two-level data augmentation method, referred to as MultiCut-MultiMix, to solve the problems, in which MultiCut is developed to avoid feature distortion at the physical level and MultiMix is designed to enrich background at the pixel level. Specifically, according to the images size required by the detection network, MultiCut, by appropriately tuning two introduced parameters, cuts large-size images into a series of image chips that are suitable for training, and meanwhile ensuring that the objects at the cutting edge would not lose. Furthermore, to strengthen feature information in these obtained image chips, MultiMix fuses different chips into new ones, in which the chip with most SDD objects will be remained as the major information and the others as the background. The fused chips from MultiMix, together with the original chips from MultiCut, then serve as the new data to train the detection network, by which the dataset is enlarged and thus overfitting can be effectively avoided. Extensive ablation experiments show that, compared with existing approaches, our method usually significantly assists detection networks to identify SDD objects in large-size images. For example, based on the bacterial dataset, 14.69% improvement in the mean average precision is achieved over the classical CutMix on Darknet53.
C1 [Xin, Zhimeng] Huazhong Univ Sci & Technol, Sch Cyber Sci & Engn, Wuhan 430074, Peoples R China.
   [Lu, Tongwei] Wuhan Inst Technol, Sch Comp Sci & Engn, Wuhan 430205, Peoples R China.
   [Li, Yuzhou; You, Xinge] Huazhong Univ Sci & Technol, Sch Elect Informat & Commun, Wuhan 430074, Peoples R China.
C3 Huazhong University of Science & Technology; Wuhan Institute of
   Technology; Huazhong University of Science & Technology
RP Xin, ZM (corresponding author), Huazhong Univ Sci & Technol, Sch Cyber Sci & Engn, Wuhan 430074, Peoples R China.
EM zhimengxin@hust.edu.cn; lutongwei@wit.edu.cn; yuzhouli@hust.edu.cn;
   youxg@hust.edu.cn
FU National Key R &D Program of China [2022YFC3301704]; Cooperation Project
   of Industry, Education, and Research of Zhuhai [ZH22017001210089PWC];
   NSFC [61772220]; Special projects for technological innovation in Hubei
   Province [2018ACA135]; Key R &D Plan of Hubei Province [2020BAB027]
FX AcknowledgementsThis work is partially supported by the National Key R
   &D Program of China under Grant 2022YFC3301704, Cooperation Project of
   Industry, Education, and Research of Zhuhai under Grant
   ZH22017001210089PWC, NSFC under Grant 61772220, Special projects for
   technological innovation in Hubei Province under Grant 2018ACA135, and
   Key R &D Plan of Hubei Province under Grant 2020BAB027.
CR Aftab U, 2018, IEEE INT CONF BIG DA, P2775, DOI 10.1109/BigData.2018.8622182
   Ametefe DS, 2023, VISUAL COMPUT, V39, P1703, DOI 10.1007/s00371-022-02437-x
   Antoniou A., 2017, Data augmentation generative adversarial networks
   Asad M, 2021, VISUAL COMPUT, V37, P1415, DOI 10.1007/s00371-020-01878-6
   Bang S, 2020, AUTOMAT CONSTR, V115, DOI 10.1016/j.autcon.2020.103198
   Ben Fredj H, 2021, VISUAL COMPUT, V37, P217, DOI 10.1007/s00371-020-01794-9
   Bochkovskiy A., 2020, PREPRINT
   Buslaev A, 2020, INFORMATION, V11, DOI 10.3390/info11020125
   Cheng G, 2016, ISPRS J PHOTOGRAMM, V117, P11, DOI 10.1016/j.isprsjprs.2016.03.014
   DeVries T, 2017, PREPRINT
   Hua W, 2012, VISUAL COMPUT, V28, P755, DOI 10.1007/s00371-012-0710-3
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Jaejun Yoo, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8372, DOI 10.1109/CVPR42600.2020.00840
   Khan MJ, 2022, VISUAL COMPUT, V38, P509, DOI 10.1007/s00371-020-02031-z
   Le QV, 2019, Learning data augmentation strategies for object detection
   Li N, 2022, VISUAL COMPUT, V38, P2091, DOI 10.1007/s00371-021-02270-8
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Lorenz KS, 2009, IEEE IMAGE PROC, P4213, DOI 10.1109/ICIP.2009.5413531
   Lu AX, 2019, PLOS COMPUT BIOL, V15, DOI 10.1371/journal.pcbi.1007348
   Misra D., 2019, ARXIV190808681
   Neubeck A, 2006, INT C PATT RECOG, P850, DOI 10.1109/icpr.2006.479
   Redmon J, 2018, Arxiv, DOI [arXiv:1804.02767, DOI 10.1109/CVPR.2017.690, DOI 10.48550/ARXIV.1804.02767]
   Redmon J, 2017, PROC CVPR IEEE, P6517, DOI 10.1109/CVPR.2017.690
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rezatofighi H, 2019, PROC CVPR IEEE, P658, DOI 10.1109/CVPR.2019.00075
   Rohith G, 2021, VISUAL COMPUT, V37, P1965, DOI 10.1007/s00371-020-01957-8
   Sadgal M, 2005, VISUAL COMPUT, V21, P118, DOI 10.1007/S00371-004-0275-x
   Shawky OA, 2020, OPTIK, V221, DOI 10.1016/j.ijleo.2020.165356
   Shin HC, 2020, INT CONF BIG DATA, P463, DOI 10.1109/BigComp48618.2020.00-25
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Sun XD, 2018, NEUROCOMPUTING, V299, P42, DOI 10.1016/j.neucom.2018.03.030
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Van Etten A., 2018, arXiv
   Wei B, 2020, INFORM SCIENCES, V541, P60, DOI 10.1016/j.ins.2020.06.035
   Wu M, 2021, VISUAL COMPUT, V37, P1707, DOI 10.1007/s00371-020-01933-2
   Xi Y, 2018, COGN SYST RES, V52, P144, DOI 10.1016/j.cogsys.2018.06.014
   Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634
   Yu J., 2016, P 24 ACM INT C MULT, P516, DOI DOI 10.1145/2964284.2967274
   Yu XH, 2021, PATTERN RECOGN, V119, DOI 10.1016/j.patcog.2021.108067
   Yun S, 2019, IEEE I CONF COMP VIS, P6022, DOI 10.1109/ICCV.2019.00612
   Zhang H., 2018, P 6 INT C LEARNING R
   Zheng ZH, 2020, AAAI CONF ARTIF INTE, V34, P12993
   Zhou F, 2019, VISUAL COMPUT, V35, P1583, DOI 10.1007/s00371-018-1559-x
   Zhu HG, 2015, IEEE IMAGE PROC, P3735, DOI 10.1109/ICIP.2015.7351502
NR 44
TC 2
Z9 2
U1 1
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2024
VL 40
IS 4
BP 2347
EP 2361
DI 10.1007/s00371-023-02920-z
EA JUN 2023
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA MZ2U3
UT WOS:001008555000001
DA 2024-07-18
ER

PT J
AU Stephen, A
   Punitha, A
   Chandrasekar, A
AF Stephen, Ancy
   Punitha, A.
   Chandrasekar, A.
TI Optimal deep generative adversarial network and convolutional neural
   network for rice leaf disease prediction
SO VISUAL COMPUTER
LA English
DT Article
DE Rice leaf disease detection; Deep learning; Generative adversarial
   network; Improved backtracking search algorithm; 3D fast-learning block
ID BACKTRACKING SEARCH ALGORITHM
AB Rice which is a staple food crop in most Asian countries mainly suffers from higher yield loss due to different factors, and one of the common factors that affect rice yield is rice leaf disease. Many countries do not allow the plantation of new rice varieties unless they are disease-resistant. Hence, disease control serves as an important factor for rice production and this can be effectively accomplished by identifying the rice disease at an early stage which is required for timely pesticide application and disease control. The rice leaf diseases are mainly inspected by the farmers with their naked eye which is error-prone, time-consuming, and often leads to huge yield loss if predicted wrong. This process is effectively handled by different researchers via the use of computer vision and machine learning techniques. To overcome the drawbacks associated with manual processing and low recognition accuracy of different statistical and machine learning techniques, this paper presents a novel concept that uses 3D 2D CNN for feature extraction and an improved backtracking search (IBS) algorithm-optimized deep generative adversarial network (DGAN) for classification. The 3D2D deep convolutional neural network (DCNN) is formed by integrating the 3D fast-learning block with a 2DCNN for extracting the rice disease features such as lesions and shape. The IBS algorithm-optimized GAN architecture is used for rice disease classification and the IBS algorithm is mainly adopted to overcome the instability and overfitting issues that arise in the DGAN. The proposed IBS-optimized GAN architecture offers improved accuracy of 98.7% which is relatively higher than the existing techniques such as extreme gradient boosting (XGBoost), transfer learning techniques, and support vector machine (SVM).
C1 [Stephen, Ancy; Chandrasekar, A.] Annamalai Univ, Fac Engn & Technol, Dept Comp Sci & Engn, Chidambaram, Tamil Nadu, India.
   [Punitha, A.] St Josephs Coll Engn, Dept Comp Sci & Engn, Chennai, Tamil Nadu, India.
C3 Annamalai University; St. Joseph's College of Engineering, Chennai
RP Stephen, A (corresponding author), Annamalai Univ, Fac Engn & Technol, Dept Comp Sci & Engn, Chidambaram, Tamil Nadu, India.
EM ancys@stjosephs.ac.in; 12charuka17@gmail.com; drchandrucse@gmail.com
RI Stephen, ANCY/ACD-8527-2022; A, CHANDRASEKAR/N-7920-2019
OI A, CHANDRASEKAR/0000-0002-3649-0959; Stephen, Ancy/0000-0002-4648-0666
CR Abbas M, 2014, MATH PROBL ENG, V2014, DOI 10.1155/2014/408492
   Azim MA., 2021, Telecommun. Comput. Electronics Control TELKOMNIKA, V19, P463, DOI [10.12928/telkomnika.v19i2.16488, DOI 10.12928/TELKOMNIKA.V19I2.16488]
   Bari BS, 2021, PEERJ COMPUT SCI, DOI 10.7717/peerj-cs.432
   Bashir U, 2013, APPL MATH COMPUT, V219, P10183, DOI 10.1016/j.amc.2013.03.110
   BiBi S, 2020, MATHEMATICS-BASEL, V8, DOI 10.3390/math8060967
   Bibi SMA, 2019, IEEE ACCESS, V7, P165779, DOI 10.1109/ACCESS.2019.2953496
   Chen S, 2021, AGRICULTURE-BASEL, V11, DOI 10.3390/agriculture11050420
   Diakite A, 2021, IET IMAGE PROCESS, V15, P1083, DOI 10.1049/ipr2.12087
   Emami H, 2020, ICT EXPRESS, V6, P258, DOI 10.1016/j.icte.2020.03.001
   Islam MA, 2021, INT J ADV COMPUT SC, V12, P280
   Jena KK, 2021, PROCEEDINGS OF THE 2021 FIFTH INTERNATIONAL CONFERENCE ON I-SMAC (IOT IN SOCIAL, MOBILE, ANALYTICS AND CLOUD) (I-SMAC 2021), P328, DOI 10.1109/I-SMAC52330.2021.9641054
   Jiang F, 2020, COMPUT ELECTRON AGR, V179, DOI [10.1016/j.compeg.2020.105824, 10.1016/j.compag.2020.105824]
   Jiang ZC, 2021, COMPUT ELECTRON AGR, V186, DOI 10.1016/j.compag.2021.106184
   Khosravanian A, 2021, VISUAL COMPUT, V37, P1185, DOI 10.1007/s00371-020-01861-1
   Krishnamoorthy N, 2021, ENVIRON RES, V198, DOI 10.1016/j.envres.2021.111275
   Kumar N, 2012, LECT NOTES COMPUT SC, V7573, P502, DOI 10.1007/978-3-642-33709-3_36
   Liang JJ, 2005, 2005 IEEE SWARM INTELLIGENCE SYMPOSIUM, P124
   Limkar S., INTELLIGENT DATA ENG, P31
   Liu LW, 2021, AGRONOMY-BASEL, V11, DOI 10.3390/agronomy11040771
   Majeed A, 2020, MATHEMATICS-BASEL, V8, DOI 10.3390/math8081246
   Maneesha A., 2021, P INT C ADV COMPUTER, P155
   Maqsood S., 2020, ADV DIFF EQS, V1, P1
   Maqsood S, 2020, MATH PROBL ENG, V2020, DOI 10.1155/2020/4036434
   Mekha Panuwat, 2021, 2021 Joint International Conference on Digital Arts, Media and Technology with ECTI Northern Section Conference on Electrical, Electronics, Computer and Telecommunication Engineering, P165, DOI 10.1109/ECTIDAMTNCON51128.2021.9425696
   Nama S, 2017, APPL SOFT COMPUT, V52, P885, DOI 10.1016/j.asoc.2016.09.037
   Abirami RN, 2021, COMPLEXITY, V2021, DOI 10.1155/2021/5541134
   Nidhis AD, 2019, L N COMPUT VIS BIOME, V31, P281, DOI 10.1007/978-3-030-04061-1_29
   Pallathadka Harikumar, 2023, Materials Today: Proceedings, P2610, DOI 10.1016/j.matpr.2021.06.419
   Pant M, 2011, NEW MATH NAT COMPUT, V7, P363, DOI 10.1142/S1793005711001986
   Patil R.R., 2022, IEEE ACCESS
   Rumy SMSH, 2021, 2021 IEEE INTERNATIONAL IOT, ELECTRONICS AND MECHATRONICS CONFERENCE (IEMTRONICS), P441, DOI 10.1109/IEMTRONICS52119.2021.9422499
   Sethy PK, 2020, J AMB INTEL HUM COMP, V11, P5703, DOI 10.1007/s12652-020-01938-8
   Singh Vijai, 2017, Information Processing in Agriculture, V4, P41, DOI 10.1016/j.inpa.2016.10.005
   Usman M, 2020, J ADV MECH DES SYST, V14, DOI 10.1299/jamdsm.2020jamdsm0048
   Verma T, 2021, MULTIMED TOOLS APPL, V80, P29267, DOI 10.1007/s11042-021-10889-x
   Vimala S, 2021, INT J COMP METH-SING, V18, DOI 10.1142/S0219876221500146
   Wang YB, 2021, EXPERT SYST APPL, V178, DOI 10.1016/j.eswa.2021.114770
   Zhang YY, 2020, ENERG CONVERS MANAGE, V223, DOI 10.1016/j.enconman.2020.113266
NR 38
TC 4
Z9 4
U1 4
U2 21
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2024
VL 40
IS 2
BP 919
EP 936
DI 10.1007/s00371-023-02823-z
EA MAY 2023
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GE6E8
UT WOS:000986475600001
DA 2024-07-18
ER

PT J
AU Shen, ZW
   Kong, B
   Dong, XY
AF Shen, Zhiwei
   Kong, Bin
   Dong, Xiaoyu
TI MAIM: a mixer MLP architecture for image matching
SO VISUAL COMPUTER
LA English
DT Article
DE Detector-free; Global field-of-view; Image-matching; Mixer MLP;
   Real-time
AB Recent advances in multilayer perceptron (MLP) models have provided new effective network architecture designs for computer vision tasks. Compared with convolutional neural networks (CNNs) and visual transformers, MLP-based visual backbones have less induction bias, which can improve the sample utilization efficiency and reduce computational costs. Therefore, we designed the Mixer MLP Architecture for Image-Matching (MAIM), which is a coarse to fine-level detector-free image-matching scheme. Accordingly, we constructed a mixer MLP architecture called Mixer-WMLP, which evenly divides the feature map into non-overlapping windows, spreads each window as a token, achieves the exchange of token information between spatial locations, channels features through a two-layer MLP structure in the coarse-level model, and then feeds the windows with dense fine-level matching, thereby producing the final matches. Furthermore, the implemented global field-of-view mixer MLP framework for image-matching incurs a low computational cost. By conducting experiments with indoor and outdoor relative poses, our MLP architecture is compared with CNN and transformer-based image-matching methods. Our method has significant advantages in terms of real-time performance and largely reduces computational cost, proving its effectiveness in image-matching tasks.
C1 [Shen, Zhiwei; Kong, Bin; Dong, Xiaoyu] Chinese Acad Sci, Inst Intelligent Machines, Hefei 230031, Peoples R China.
   [Shen, Zhiwei; Dong, Xiaoyu] Univ Sci & Technol China, Hefei 230026, Peoples R China.
   [Kong, Bin] Anhui Engn Lab Intelligent Driving Technol & Appli, Hefei 230088, Peoples R China.
   [Kong, Bin] Chinese Acad Sci, Innovat Res Inst Robot & Intelligent Mfg Hefei, Hefei 230088, Peoples R China.
C3 Chinese Academy of Sciences; Hefei Institutes of Physical Science, CAS;
   Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Chinese Academy of Sciences
RP Kong, B (corresponding author), Chinese Acad Sci, Inst Intelligent Machines, Hefei 230031, Peoples R China.; Kong, B (corresponding author), Anhui Engn Lab Intelligent Driving Technol & Appli, Hefei 230088, Peoples R China.; Kong, B (corresponding author), Chinese Acad Sci, Innovat Res Inst Robot & Intelligent Mfg Hefei, Hefei 230088, Peoples R China.
EM henzw@mail.ustc.edu.cn; bkong@iim.ac.cn; johndon@mail.ustc.edu.cn
RI DONG, XIAOYU/AAS-7163-2021
OI Bin, Kong/0000-0002-7678-3123
FU National Key Research and Development Program of China [2020AAA0108103];
   Institute of Robotics and Intelligent Manufacturing Innovation, Chinese
   Academy of Sciences [C2021002]; Innovation Engineering Project for New
   Energy and Intelligent Networked Automobile of Anhui Province, China
FX This work was supported by the National Key Research and Development
   Program of China (Grant Number: 2020AAA0108103). This work was supported
   by the Institute of Robotics and Intelligent Manufacturing Innovation,
   Chinese Academy of Sciences (Grant Number: C2021002). This work was
   supported by the Innovation Engineering Project for New Energy and
   Intelligent Networked Automobile of Anhui Province, China.
CR [Anonymous], 2016, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2016.445
   Bian JW, 2017, PROC CVPR IEEE, P2828, DOI 10.1109/CVPR.2017.302
   Brachmann E, 2019, IEEE I CONF COMP VIS, P4321, DOI 10.1109/ICCV.2019.00442
   Carion N., 2020, EUR C COMP VIS, P213, DOI DOI 10.1007/978-3-030-58452-8_13
   Cavalli Luca, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12364), P770, DOI 10.1007/978-3-030-58529-7_45
   Choy CB, 2016, ADV NEUR IN, V29
   Clark K, 2020, INFORM SYST RES, DOI 10.48550/arXiv.2003.10555
   Dai A, 2017, PROC CVPR IEEE, P2432, DOI 10.1109/CVPR.2017.261
   DeTone D., CVPRW, DOI [10.48550/arXiv.1707.07410, DOI 10.48550/ARXIV.1707.07410]
   DeTone D, 2018, IEEE COMPUT SOC CONF, P337, DOI 10.1109/CVPRW.2018.00060
   Dosovitskiy A, 2021, arXiv, DOI [10.48550/ARXIV.2010.11929, DOI 10.48550/ARXIV.2010.11929]
   Dusmanu M., 2019, ARXIV, DOI DOI 10.48550/ARXIV.1905.03561
   Goodman J, 2001, INT CONF ACOUST SPEE, P561, DOI 10.1109/ICASSP.2001.940893
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huiyu Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P108, DOI 10.1007/978-3-030-58548-8_7
   Jiang W, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6187, DOI 10.1109/ICCV48922.2021.00615
   Jin YH, 2021, INT J COMPUT VISION, V129, P517, DOI 10.1007/s11263-020-01385-0
   Katharopoulos A, 2020, PR MACH LEARN RES, V119
   Lan Z., 2019, INT C LEARNING REPRE
   Li X., 2020, IEEE WINT CONF APPL
   Li ZQ, 2018, PROC CVPR IEEE, P2041, DOI 10.1109/CVPR.2018.00218
   Liu C, 2011, IEEE T PATTERN ANAL, V33, P978, DOI 10.1109/TPAMI.2010.147
   Liu Y, 2019, ADV NEUR IN, V32
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Luo ZX, 2020, PROC CVPR IEEE, P6588, DOI 10.1109/CVPR42600.2020.00662
   Luo ZX, 2019, PROC CVPR IEEE, P2522, DOI 10.1109/CVPR.2019.00263
   Mnih Andriy, 2009, Advances in Neural Information Processing Systems, P1081
   Morin F, 2005, P AISTATS, V5, P246
   Ono Y, 2018, ADV NEUR IN, V31
   Phototourism Challenge, CVPR 2019 IM MATCH W
   Ranftl R, 2018, LECT NOTES COMPUT SC, V11205, P292, DOI 10.1007/978-3-030-01246-5_18
   Revaud J., 2019, P C NEURAL INFORM PR, DOI DOI 10.48550/ARXIV.1906.06195
   Rocco I., 2020, P IEEE EUR C COMP VI, V16, P605
   Rublee E, 2011, IEEE I CONF COMP VIS, P2564, DOI 10.1109/ICCV.2011.6126544
   Sarlin PE, 2020, PROC CVPR IEEE, P4937, DOI 10.1109/CVPR42600.2020.00499
   Schmidt T, 2017, IEEE ROBOT AUTOM LET, V2, P420, DOI 10.1109/LRA.2016.2634089
   Sun JM, 2021, PROC CVPR IEEE, P8918, DOI 10.1109/CVPR46437.2021.00881
   Tan Y., 2021, ARXIV
   Tolstikhin Ilya O., 2021, P 35 C NEURAL INFORM
   Touvron H, 2023, IEEE T PATTERN ANAL, V45, P5314, DOI 10.1109/TPAMI.2022.3206148
   Touvron H, 2021, PR MACH LEARN RES, V139, P7358
   Truong P, ARXIV
   Truong P, 2020, PROC CVPR IEEE, P6257, DOI 10.1109/CVPR42600.2020.00629
   Tu ZZ, 2022, PROC CVPR IEEE, P5759, DOI 10.1109/CVPR52688.2022.00568
   Tyszkiewicz M. J., 2020, Advances in Neural Information Processing Systems
   Ummenhofer B., 2017, C COMP VIS PATT REC, P2
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang Q., 2020, arXiv
   Yi KM, 2018, PROC CVPR IEEE, P2666, DOI 10.1109/CVPR.2018.00282
   Yi KM, 2016, LECT NOTES COMPUT SC, V9910, P467, DOI 10.1007/978-3-319-46466-4_28
   Zafrir Ofir, 2019, 2019 Fifth Workshop on Energy Efficient Machine Learning and Cognitive Computing - NeurIPS Edition (EMC2-NIPS), P36, DOI 10.1109/EMC2-NIPS53020.2019.00016
   Zhang H, 2022, KNOWL-BASED SYST, V258, DOI 10.1016/j.knosys.2022.109792
   Zhang JH, 2019, IEEE I CONF COMP VIS, P5844, DOI 10.1109/ICCV.2019.00594
NR 53
TC 2
Z9 2
U1 4
U2 12
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2024
VL 40
IS 3
BP 1327
EP 1337
DI 10.1007/s00371-023-02851-9
EA APR 2023
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HY8O3
UT WOS:000978496000002
DA 2024-07-18
ER

PT J
AU Lesar, Z
   Alharbi, R
   Bohak, C
   Strnad, O
   Heinzl, C
   Marolt, M
   Viola, I
AF Lesar, Ziga
   Alharbi, Ruwayda
   Bohak, Ciril
   Strnad, Ondrej
   Heinzl, Christoph
   Marolt, Matija
   Viola, Ivan
TI Volume conductor: interactive visibility management for crowded volumes
SO VISUAL COMPUTER
LA English
DT Article
DE Volume visualization; Visibility management; Crowded volumes
ID OPACITY OPTIMIZATION; VISUALIZATION; EXPLORATION
AB We present a novel smart visibility system for visualizing crowded volumetric data containing many object instances. The presented approach allows users to form groups of objects through membership predicates and to individually control the visibility of the instances in each group. Unlike previous smart visibility approaches, our approach controls the visibility on a per-instance basis and decides which instances are displayed or hidden based on the membership predicates and the current view. Thus, cluttered and dense volumes that are notoriously difficult to explore effectively are automatically sparsified so that the essential information is extracted and presented to the user. The proposed system is generic and can be easily integrated into existing volume rendering applications and applied to many different domains. We demonstrate the use of the volume conductor for visualizing fiber-reinforced polymers and intracellular organelle structures.
C1 [Lesar, Ziga; Bohak, Ciril; Marolt, Matija] Univ Ljubljana, Fac Comp & Informat Sci, Vecna Pot 113, Ljubljana 1000, Slovenia.
   [Alharbi, Ruwayda; Bohak, Ciril; Strnad, Ondrej; Viola, Ivan] King Abdullah Univ Sci & Technol KAUST, Visual Comp Ctr, Thuwal 239556900, Saudi Arabia.
   [Heinzl, Christoph] Univ Passau, Innstr 41, D-94032 Passau, Germany.
C3 University of Ljubljana; King Abdullah University of Science &
   Technology; University of Passau
RP Lesar, Z (corresponding author), Univ Ljubljana, Fac Comp & Informat Sci, Vecna Pot 113, Ljubljana 1000, Slovenia.
EM ziga.lesar@fri.uni-lj.si
RI Heinzl, Christoph/D-7272-2016; Viola, Ivan/O-8944-2014
OI Heinzl, Christoph/0000-0002-3173-8871; Strnad,
   Ondrej/0000-0002-8077-4692; Bohak, Ciril/0000-0002-9015-2897; Viola,
   Ivan/0000-0003-4248-6574
FU Slovenian Research Agency; Central Technical Library in Ljubljana; King
   Abdullah University of Science and Technology [BAS/1/1680-01-01];
   Austrian Research Promotion Agency (FFG) [874540]; government of Upper
   Austria
FX Open access publishing supported by the Slovenian Research Agency and
   Central Technical Library in Ljubljana. The research was partially
   supported by the King Abdullah University of Science and
   Technology-award number BAS/1/1680-01-01, partially by funding from the
   Austrian Research Promotion Agency (FFG) within the program line "TAKE
   OFF," FFG Grant No. 874540 "BeyondInspection," and by research subsidies
   granted by the government of Upper Austria during the "X-Pro" project.
CR Ai-Thelaya K, 2021, IEEE T VIS COMPUT GR, V27, P645, DOI 10.1109/TVCG.2020.3030451
   Ament M, 2017, IEEE T VIS COMPUT GR, V23, P1767, DOI 10.1109/TVCG.2016.2569080
   Berge CSZ, 2014, IEEE T VIS COMPUT GR, V20, P2379, DOI 10.1109/TVCG.2014.2346317
   Bohak C., 2019, KAUST CELL VIS SUMM
   Bruckner S, 2006, IEEE T VIS COMPUT GR, V12, P1559, DOI 10.1109/TVCG.2006.96
   Chan MY, 2008, IEEE T VIS COMPUT GR, V14, P1683, DOI 10.1109/TVCG.2008.159
   Correa CD, 2011, IEEE T VIS COMPUT GR, V17, P192, DOI 10.1109/TVCG.2010.35
   Diepstraten J, 2003, COMPUT GRAPH FORUM, V22, P523, DOI 10.1111/1467-8659.t01-3-00700
   Diepstraten J, 2002, COMPUT GRAPH FORUM, V21, P317, DOI 10.1111/1467-8659.t01-1-00591
   Günther T, 2017, COMPUT GRAPH FORUM, V36, P153, DOI 10.1111/cgf.13115
   Günther T, 2014, COMPUT GRAPH FORUM, V33, P11, DOI 10.1111/cgf.12357
   Günther T, 2014, COMPUT GRAPH FORUM, V33, P507, DOI 10.1111/cgf.12336
   Günther T, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461930
   Heinzl C, 2017, COMPUT GRAPH FORUM, V36, P647, DOI 10.1111/cgf.13214
   Kanzler M, 2016, COMPUT GRAPH-UK, V61, P29, DOI 10.1016/j.cag.2016.08.001
   Krüger J, 2006, IEEE T VIS COMPUT GR, V12, P941, DOI 10.1109/TVCG.2006.124
   Kubisch C, 2010, INT J COMPUT ASS RAD, V5, P667, DOI 10.1007/s11548-010-0420-0
   Le Muzic M, 2016, COMPUT GRAPH FORUM, V35, P161, DOI 10.1111/cgf.12892
   Lesar Z, 2018, WEB3D 2018: THE 23RD INTERNATIONAL ACM CONFERENCE ON 3D WEB TECHNOLOGY, DOI 10.1145/3208806.3208814
   Ljung P, 2016, COMPUT GRAPH FORUM, V35, P669, DOI 10.1111/cgf.12934
   Marchesin S, 2010, IEEE T VIS COMPUT GR, V16, P1578, DOI 10.1109/TVCG.2010.212
   Mekuc MZ, 2020, COMPUT BIOL MED, V119, DOI 10.1016/j.compbiomed.2020.103693
   Ropinski T., 2005, P INT FALL WORKSH VI, P273
   Schott M, 2009, COMPUT GRAPH FORUM, V28, P855, DOI 10.1111/j.1467-8659.2009.01464.x
   Schretter C., 2012, J Graph Tools, V16, P95, DOI [DOI 10.1080/2165347X.2012.679555, 10.1080/2165347x.2012.679555]
   Viega John., 1996, UIST 96, P51, DOI [10.1145/237091.237098, DOI 10.1145/237091.237098]
   Viola I., 2005, COMPUTATIONAL AESTHE, P209, DOI [DOI 10.2312/COMPAESTH/COMPAESTH05/209-216, 10.2312/COMPAESTH/COMPAESTH05/209216]
   Wang LJ, 2005, IEEE VISUALIZATION 2005, PROCEEDINGS, P367
   Weissenböck J, 2014, IEEE PAC VIS SYMP, P153, DOI 10.1109/PacificVis.2014.52
   Willett W, 2007, IEEE T VIS COMPUT GR, V13, P1129, DOI 10.1109/TVCG.2007.70589
NR 30
TC 1
Z9 1
U1 1
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2024
VL 40
IS 2
BP 1005
EP 1020
DI 10.1007/s00371-023-02828-8
EA MAR 2023
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GE6E8
UT WOS:000967291000002
OA hybrid, Green Submitted, Green Published
DA 2024-07-18
ER

PT J
AU Liu, MS
   Wang, Y
   Yi, H
   Huang, XH
AF Liu, Mingsheng
   Wang, Yu
   Yi, Hu
   Huang, Xiaohui
TI Vehicle object counting network based on feature pyramid split attention
   mechanism
SO VISUAL COMPUTER
LA English
DT Article
DE Feature pyramid; Vehicle counting; Contextual features; Channel
   attention
AB In recent years, real-time vehicle congestion detection has become a hot research topic in the field of transportation due to the frequent occurrence of highway traffic jams. Vehicle congestion detection generally adopts a vehicle counting algorithm based on object detection, but it is not effective in scenarios with large changes in vehicle scale, dense vehicles, background clutter, and severe occlusion. A vehicle object counting network based on a feature pyramid split attention mechanism is proposed for accurate vehicle counting and the generation of high-quality vehicle density maps in highly congested scenarios. The network extracts rich contextual features by using blocks at different scales, and then obtains a multi-scale feature mapping in the channel direction using kernel convolution of different sizes, and uses the channel attention module at different scales separately to allow the network to focus on features at different scales to obtain an attention vector in the channel direction to reduce mis-estimation of background information. Experiments on the vehicle datasets TRANCOS, CARPK, and HS-Vehicle show that the proposed method outperforms most existing counting methods based on detection or density estimation. The relative improvement in MAE metrics is 90.5% for the CARPK dataset compared to Fast R-CNN and 73.0% for the HSVehicle dataset compared to CSRNet. In addition, the method is also extended to count other objects, such as pedestrians in the ShanghaiTech dataset, and the proposed method effectively reduces the misrecognition rate and achieves higher counting performance compared to the state-of-the-art methods.
C1 [Liu, Mingsheng; Wang, Yu] Jiangxi Prov Traff Monitoring Command Ctr, Nanchang 330036, Peoples R China.
   [Yi, Hu; Huang, Xiaohui] East China Jiaotong Univ, Sch Informat Engn, Nanchang 330013, Peoples R China.
C3 East China Jiaotong University
RP Yi, H (corresponding author), East China Jiaotong Univ, Sch Informat Engn, Nanchang 330013, Peoples R China.
EM yh_hnjx@163.com
RI huang, xiaohui/KRP-2903-2024
OI Yi, Hu/0000-0002-3136-0522
FU Science and Technology Project of the Transportation Department of
   Jiangxi Province, China [2022X0040, 2021X0011]
FX This work was supported in part by the Science and Technology Project of
   the Transportation Department of Jiangxi Province, China (Nos.
   2022X0040, 2021X0011).
CR Avsar E, 2022, MULTIMED TOOLS APPL, V81, P6653, DOI 10.1007/s11042-021-11804-0
   Bai S, 2020, PROC CVPR IEEE, P4593, DOI 10.1109/CVPR42600.2020.00465
   Benjdira B, 2019, 2019 1ST INTERNATIONAL CONFERENCE ON UNMANNED VEHICLE SYSTEMS-OMAN (UVS), DOI 10.1109/uvs.2019.8658300
   Bochkovskiy A, 2020, ARXIV PREPRINT ARXIV, DOI DOI 10.48550/ARXIV.2004.10934
   Cao XK, 2018, LECT NOTES COMPUT SC, V11209, P757, DOI 10.1007/978-3-030-01228-1_45
   Chandrasekar KS, 2020, J VIS COMMUN IMAGE R, V72, DOI 10.1016/j.jvcir.2020.102905
   Chaoxu Guo, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12592, DOI 10.1109/CVPR42600.2020.01261
   Chen WJ, 2021, VISUAL COMPUT, V37, P805, DOI 10.1007/s00371-020-01831-7
   Chen XY, 2019, IEEE WINT CONF APPL, P1941, DOI 10.1109/WACV.2019.00211
   CHOLLET F, 2017, PROC CVPR IEEE, P1800, DOI DOI 10.1109/CVPR.2017.195
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Deng CF, 2022, IEEE T MULTIMEDIA, V24, P1968, DOI 10.1109/TMM.2021.3074273
   Deng P., 2022, SN COMPUT SCI, V3, P329, DOI [10.1007/s42979-022-01229-3, DOI 10.1007/S42979-022-01229-3]
   Ding XH, 2021, IEEE T INTELL TRANSP, V22, P4776, DOI 10.1109/TITS.2020.2983475
   Fiaschi L, 2012, INT C PATT RECOG, P2685
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Guerrero-Gómez-Olmedo R, 2015, LECT NOTES COMPUT SC, V9117, P423, DOI 10.1007/978-3-319-19390-8_48
   Harikrishnan PM, 2021, APPL INTELL, V51, P4714, DOI 10.1007/s10489-020-02127-y
   Horne D, 2016, J RAIL TRANSPORT PLA, V6, P149, DOI 10.1016/j.jrtpm.2016.04.001
   Hsieh MR, 2017, IEEE I CONF COMP VIS, P4165, DOI 10.1109/ICCV.2017.446
   Hu CR, 2020, MULTIMED TOOLS APPL, V79, P15059, DOI 10.1007/s11042-020-08888-5
   Hu JW, 2023, VISUAL COMPUT, V39, P1503, DOI 10.1007/s00371-022-02425-1
   Idrees H, 2018, LECT NOTES COMPUT SC, V11206, P544, DOI 10.1007/978-3-030-01216-8_33
   Idrees H, 2013, PROC CVPR IEEE, P2547, DOI 10.1109/CVPR.2013.329
   Khan SD, 2021, VISUAL COMPUT, V37, P2127, DOI 10.1007/s00371-020-01974-7
   Knaian A.N., 2000, WIRELESS SENSOR NETW
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lempitsky V., 2010, P ADV NEUR INF PROC, V23, P1, DOI DOI 10.5555/2997189.2997337
   Li B, 2023, VISUAL COMPUT, V39, P2671, DOI 10.1007/s00371-022-02485-3
   Li H, 2020, VISUAL COMPUT, V36, P1693, DOI 10.1007/s00371-019-01769-5
   Li P., 2021, SCI PROGRAMMING-NETH, V2021, p5596488:1
   Li YH, 2018, PROC CVPR IEEE, P1091, DOI 10.1109/CVPR.2018.00120
   Li ZX, 2023, VISUAL COMPUT, V39, P1045, DOI 10.1007/s00371-021-02383-0
   Lin H, 2022, PROC CVPR IEEE, P19596, DOI 10.1109/CVPR52688.2022.01901
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu L., 2018, ARXIV
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Liu WZ, 2022, IEEE T PATTERN ANAL, V44, P8151, DOI 10.1109/TPAMI.2021.3102690
   Liu WZ, 2019, PROC CVPR IEEE, P5094, DOI 10.1109/CVPR.2019.00524
   Luo HL, 2018, APPL SCI-BASEL, V8, DOI 10.3390/app8122367
   Manana M, 2017, PROCEEDINGS OF 2017 3RD IEEE INTERNATIONAL CONFERENCE ON COMPUTER AND COMMUNICATIONS (ICCC), P1751, DOI 10.1109/CompComm.2017.8322840
   Mundhenk TN, 2016, LECT NOTES COMPUT SC, V9907, P785, DOI 10.1007/978-3-319-46487-9_48
   Oñoro-Rubio D, 2016, LECT NOTES COMPUT SC, V9911, P615, DOI 10.1007/978-3-319-46478-7_38
   Ranjan V, 2018, LECT NOTES COMPUT SC, V11211, P278, DOI 10.1007/978-3-030-01234-2_17
   Redmon J., 2016, PROC CVPR IEEE, DOI [10.1109/CVPR.2016.91, DOI 10.1109/CVPR.2016.91]
   Redmon J, 2018, Arxiv, DOI [arXiv:1804.02767, DOI 10.1109/CVPR.2017.690, DOI 10.48550/ARXIV.1804.02767]
   Redmon J, 2017, PROC CVPR IEEE, P6517, DOI 10.1109/CVPR.2017.690
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rublee E, 2011, IEEE I CONF COMP VIS, P2564, DOI 10.1109/ICCV.2011.6126544
   Sam DB, 2017, PROC CVPR IEEE, P4031, DOI 10.1109/CVPR.2017.429
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sindagi VA, 2017, IEEE I CONF COMP VIS, P1879, DOI 10.1109/ICCV.2017.206
   Song HS, 2019, EUR TRANSP RES REV, V11, DOI 10.1186/s12544-019-0390-4
   Wang YJ, 2020, MULTIMED TOOLS APPL, V79, P1057, DOI 10.1007/s11042-019-08208-6
   Wojke N, 2017, IEEE IMAGE PROC, P3645, DOI 10.1109/ICIP.2017.8296962
   Yao H.Y., 2017, ARXIV
   Zhang C, 2015, PROC CVPR IEEE, P833, DOI 10.1109/CVPR.2015.7298684
   Zhang SH, 2017, IEEE I CONF COMP VIS, P3687, DOI 10.1109/ICCV.2017.396
   Zhang YY, 2016, PROC CVPR IEEE, P589, DOI 10.1109/CVPR.2016.70
   Zhao JD., 2016, ADV TRANSPORTATION S, V40, P27
NR 60
TC 1
Z9 1
U1 4
U2 18
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2024
VL 40
IS 2
BP 663
EP 680
DI 10.1007/s00371-023-02808-y
EA MAR 2023
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GE6E8
UT WOS:000948848100001
DA 2024-07-18
ER

PT J
AU Huang, Z
   Chen, BA
   Zhu, DK
AF Huang, Zhong
   Chen, Baian
   Zhu, Dingkun
TI ImGeo-VoteNet: image and geometry co-supported VoteNet for RGB-D object
   detection
SO VISUAL COMPUTER
LA English
DT Article
DE ImGeoVoteNet; RGB-D object detection; Image and point cloud fusion; 3D
   vision
ID POINT CLOUD
AB Depth cameras are becoming affordable and widely used to capture depth information in various real-world scenarios. However, depth-based object detectors rarely fully explore the geometry of objects, as well as simply concatenate textures and depths. By fully exploiting images and geometries of objects, we propose a VoteNet-based RGB-D object detection neural network (call ImGeo-VoteNet for short) to address the adverse situation of occluded and similar objects in indoor scenes. First, image votes are generated based on a set of candidate boxes from 2D detectors in RGB images to support the subsequent voting. Second, we transform the depths of any input scene to a point cloud representation for better using its geometry information. Third, we design three modules to capture multi-level contextual information at the point level, the object level and the global scene level, respectively, for alleviating data loss and distinguishing similar objects. Extensive experiments on the benchmark dataset show that ImGeo-VoteNet obtains a better accuracy of 3D object detection in complex indoor scenes than the state-of-the-art methods. For example, ImGeo-VoteNet achieves the improvement of +5.8 mAP over VoteNet. Source code will be available upon publication.
C1 [Huang, Zhong] Anhui Vocat Coll Grain Engn, Dept Mech & Elect Engn, Hefei 231635, Peoples R China.
   [Zhu, Dingkun] Jiangsu Univ Technol, Sch Comp Engn, Changzhou 213100, Peoples R China.
   [Chen, Baian; Zhu, Dingkun] Hong Kong Metropolitan Univ, Sch Sci & Technol, Hong Kong 999077, Peoples R China.
C3 Jiangsu University of Technology; Hong Kong Metropolitan University
RP Zhu, DK (corresponding author), Jiangsu Univ Technol, Sch Comp Engn, Changzhou 213100, Peoples R China.; Zhu, DK (corresponding author), Hong Kong Metropolitan Univ, Sch Sci & Technol, Hong Kong 999077, Peoples R China.
EM huangzhong0012024@126.com; m15262004991@163.com; dingkun_zhu@163.com
OI Zhu, Dingkun/0000-0003-2447-4828
CR Aarthi S, 2017, 2017 INTERNATIONAL CONFERENCE ON COMPUTER, COMMUNICATION AND SIGNAL PROCESSING (ICCCSP), P191
   Chen JT, 2020, PROC CVPR IEEE, P389, DOI 10.1109/CVPR42600.2020.00047
   Chen XZ, 2017, PROC CVPR IEEE, P6526, DOI 10.1109/CVPR.2017.691
   Cheng BW, 2021, PROC CVPR IEEE, P8959, DOI 10.1109/CVPR46437.2021.00885
   Chenhang He, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11870, DOI 10.1109/CVPR42600.2020.01189
   Clouse HS, 2014, IEEE IMAGE PROC, P966, DOI 10.1109/ICIP.2014.7025194
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]
   Jablonski M, 2016, 2016 2ND INTERNATIONAL CONFERENCE ON EVENT-BASED CONTROL, COMMUNICATION, AND SIGNAL PROCESSING (EBCCSP), DOI 10.1109/EBCCSP.2016.7605275
   Koskowich BJ, 2018, INT GEOSCI REMOTE SE, P6416, DOI 10.1109/IGARSS.2018.8518124
   Lahoud J, 2017, IEEE I CONF COMP VIS, P4632, DOI 10.1109/ICCV.2017.495
   Lang AH, 2019, PROC CVPR IEEE, P12689, DOI 10.1109/CVPR.2019.01298
   Liang Du, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13326, DOI 10.1109/CVPR42600.2020.01334
   Liang M, 2018, LECT NOTES COMPUT SC, V11220, P663, DOI 10.1007/978-3-030-01270-0_39
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Liu Y, 2018, PROC CVPR IEEE, P6985, DOI 10.1109/CVPR.2018.00730
   Lowe D. G., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1150, DOI 10.1109/ICCV.1999.790410
   Lu HH, 2019, INT CONF ACOUST SPEE, P1992, DOI 10.1109/ICASSP.2019.8682746
   Mane S, 2018, PROCEEDINGS OF THE 2018 SECOND INTERNATIONAL CONFERENCE ON INTELLIGENT COMPUTING AND CONTROL SYSTEMS (ICICCS), P1809, DOI 10.1109/ICCONS.2018.8662921
   Misra Ishan, 2021, Proceedings of the IEEE/CVF International Conference on Computer Vision, P2906
   Papageorgiou CP, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P555, DOI 10.1109/ICCV.1998.710772
   Qi CR, 2017, ADV NEUR IN, V30
   Qi CR, 2020, PROC CVPR IEEE, P4403, DOI 10.1109/CVPR42600.2020.00446
   Qi CR, 2019, IEEE I CONF COMP VIS, P9276, DOI 10.1109/ICCV.2019.00937
   Qi CR, 2018, PROC CVPR IEEE, P918, DOI 10.1109/CVPR.2018.00102
   Qi CR, 2017, PROC CVPR IEEE, P77, DOI 10.1109/CVPR.2017.16
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Ren Z, 2016, PROC CVPR IEEE, P1525, DOI 10.1109/CVPR.2016.169
   Shi SS, 2019, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2019.00086
   Shi WJ, 2020, PROC CVPR IEEE, P1708, DOI 10.1109/CVPR42600.2020.00178
   Song SR, 2016, PROC CVPR IEEE, P808, DOI 10.1109/CVPR.2016.94
   Song SR, 2015, PROC CVPR IEEE, P567, DOI 10.1109/CVPR.2015.7298655
   Wang Y., 2020, P EUR C COMP VIS ECC, P18
   Wojek C, 2013, IEEE T PATTERN ANAL, V35, P882, DOI 10.1109/TPAMI.2012.174
   Xie Qizhe, 2020, SELF TRAINING NOISY, DOI 10.1109/cvpr42600.2020.01046
   Xu DF, 2018, PROC CVPR IEEE, P244, DOI 10.1109/CVPR.2018.00033
   Yan Y, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18103337
   Yang H, 2022, Arxiv, DOI arXiv:2207.10589
   Yang ZT, 2019, IEEE I CONF COMP VIS, P1951, DOI 10.1109/ICCV.2019.00204
   Yue KY, 2018, ADV NEUR IN, V31
   Zarzar J, 2019, Arxiv, DOI [arXiv:1911.12236, 10.48550/arXiv.1911.12236]
   Zeng MB, 2018, IEEE I C NETW INFRAS, P60, DOI 10.1109/ICNIDC.2018.8525809
   Zeng YM, 2018, IEEE ROBOT AUTOM LET, V3, P3434, DOI 10.1109/LRA.2018.2852843
   Zetong Yang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11037, DOI 10.1109/CVPR42600.2020.01105
   Zhou Y, 2018, PROC CVPR IEEE, P4490, DOI 10.1109/CVPR.2018.00472
   Zotkin D, 2001, IEEE WORKSHOP ON DETECTION AND RECOGNITION OF EVENTS IN VIDEO, PROCEEDINGS, P20, DOI 10.1109/EVENT.2001.938862
NR 49
TC 1
Z9 1
U1 2
U2 9
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2023
VL 39
IS 6
SI SI
BP 2483
EP 2495
DI 10.1007/s00371-023-02815-z
EA MAR 2023
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J1YY4
UT WOS:000946907000002
DA 2024-07-18
ER

PT J
AU Li, XW
   Zhang, G
   Wu, YQ
   Li, XM
   Zhang, YQ
AF Li, Xuewei
   Zhang, Gang
   Wu, YuQuan
   Li, Xueming
   Zhang, YaQing
TI Rule of thirds-aware reinforcement learning for image aesthetic cropping
SO VISUAL COMPUTER
LA English
DT Article
DE Image cropping; Reinforcement learning; Rule of thirds; Aesthetics image
AB Image aesthetic cropping aims at improving the aesthetic quality by adjusting the composition of the image. Most cropping algorithms generate thousands of candidate windows, which is very time-consuming. Motivated by this challenge, we design a rule of thirds-aware reinforcement learning (RoTA-RL) model for image aesthetics cropping. The RoTA-RL model image cropping is a decision-making process of reinforcement learning. Through some simple actions, images with high aesthetic quality can be generated. Firstly, the deep global features and rule of thirds features are extracted. Secondly, the agent can predict the best cropping action by these features. Double deep Q-learning is used to update the parameters of the agent, and dueling deep Q-learning is used to devise the structure of the agent. Finally, a new reward function is proposed based on the aesthetic score and the rule of thirds score predicted by the improved aesthetic assessment network. The RoTA-RL model has been evaluated on public image cropping datasets, and the accuracy of intersection over union is improved by 5.78% on Flickr Cropping Dataset.
C1 [Li, Xuewei; Wu, YuQuan] Chinese Acad Sci, Inst Software, Sci & Technol Integrated Informat Syst Lab, 4 South Fourth St, Beijing 100190, Peoples R China.
   [Zhang, Gang] Unit 95865 PLA, Beijing 102218, Peoples R China.
   [Li, Xueming; Zhang, YaQing] Beijing Univ Posts & Telecommun, Dept Digital Media & Art Design, 10 Xitucheng Rd, Beijing 100876, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Software, CAS; Beijing
   University of Posts & Telecommunications
RP Wu, YQ (corresponding author), Chinese Acad Sci, Inst Software, Sci & Technol Integrated Informat Syst Lab, 4 South Fourth St, Beijing 100190, Peoples R China.; Li, XM (corresponding author), Beijing Univ Posts & Telecommun, Dept Digital Media & Art Design, 10 Xitucheng Rd, Beijing 100876, Peoples R China.
EM lixuewei@iscas.ac.cn; gangzhang1989@126.com; xjhtlxw2022@126.com;
   lixm@bupt.edu.cn; m18810819151@163.com
RI li, xiaomin/KCX-9845-2024; LI, Xiang-Yang/JZE-0275-2024; Zhang,
   Gang/HMC-7360-2023; cheng, cheng/JBR-8359-2023; LIU, HAO/JBI-9623-2023;
   CHEN, AN/KFT-3370-2024
OI Zhang, Yaqing/0000-0002-0079-6171; LI, xueming/0000-0003-1058-2799; Li,
   Xuewei/0000-0001-5336-7234; Zhang, Gang/0000-0002-1803-3180
FU Chinese Academy of Sciences [E1YD5906]
FX This work was supported by Chinese Academy of Sciences (No. E1YD5906).
CR [Anonymous], 2007, P 5 INT C COMP VIS S
   Bari ASMH, 2020, VISUAL COMPUT, V36, P2395, DOI 10.1007/s00371-020-01893-7
   Cai CT, 2021, VISUAL COMPUT, V37, P1917, DOI 10.1007/s00371-020-01952-z
   Caicedo JC, 2015, IEEE I CONF COMP VIS, P2488, DOI 10.1109/ICCV.2015.286
   Chen JS, 2016, PROC CVPR IEEE, P507, DOI 10.1109/CVPR.2016.61
   Chen LQ, 2003, MULTIMEDIA SYST, V9, P353, DOI 10.1007/s00530-003-0105-4
   Chen YL, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P37, DOI 10.1145/3123266.3123274
   Chen YL, 2017, IEEE WINT CONF APPL, P226, DOI 10.1109/WACV.2017.32
   Ciocca G, 2007, IEEE T CONSUM ELECTR, V53, P1622, DOI 10.1109/TCE.2007.4429261
   Datta R, 2006, LECT NOTES COMPUT SC, V3953, P288, DOI 10.1007/11744078_23
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Fang C, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P1105, DOI 10.1145/2647868.2654979
   Greco L, 2013, LECT NOTES COMPUT SC, V8157, P151, DOI 10.1007/978-3-642-41184-7_16
   Hong CY, 2021, PROC CVPR IEEE, P7053, DOI 10.1109/CVPR46437.2021.00698
   Jie ZQ, 2016, ADV NEUR IN, V29
   Kao YY, 2017, INT CONF ACOUST SPEE, P1982, DOI 10.1109/ICASSP.2017.7952503
   Kao YY, 2015, IEEE IMAGE PROC, P1583, DOI 10.1109/ICIP.2015.7351067
   Ke Y., 2006, P IEEE COMP SOC C CO, V1, P419, DOI DOI 10.1109/CVPR.2006.303
   Khan A, 2020, ARTIF INTELL REV, V53, P5455, DOI 10.1007/s10462-020-09825-6
   Kochenderfer MJ, 2015, MIT LINCOLN LAB, P57
   Kong S, 2016, LECT NOTES COMPUT SC, V9905, P662, DOI 10.1007/978-3-319-46448-0_40
   Kong XY, 2017, PROC CVPR IEEE, P7072, DOI 10.1109/CVPR.2017.748
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lapan M., 2019, DEEP REINFORCEMENT L, P119
   Li DB, 2019, IEEE T IMAGE PROCESS, V28, P5105, DOI 10.1109/TIP.2019.2914360
   Li DB, 2018, PROC CVPR IEEE, P8193, DOI 10.1109/CVPR.2018.00855
   Li XW, 2020, IEEE ACCESS, V8, P63043, DOI 10.1109/ACCESS.2020.2983725
   Li ZP, 2019, IEEE INT CON MULTI, P254, DOI 10.1109/ICME.2019.00052
   Lin J, 2017, IEEE PAC RIM CONF CO
   Liu D., 2019, arXiv
   Lu P, 2021, IEEE T MULTIMEDIA, V23, P3618, DOI 10.1109/TMM.2020.3029882
   Lu P, 2019, SIGNAL PROCESS-IMAGE, V77, P1, DOI 10.1016/j.image.2019.05.010
   Lu X, 2015, IEEE I CONF COMP VIS, P990, DOI 10.1109/ICCV.2015.119
   Lu X, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P457, DOI 10.1145/2647868.2654927
   Luo JB, 2007, 2007 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOLS 1-5, P2218
   Ma M, 2004, CCNC 2004: 1ST IEEE CONSUMER COMMUNICATIONS AND NETWORKING CONFERENCE, PROCEEDINGS, P710, DOI 10.1109/CCNC.2004.1286964
   Ma S, 2017, PROC CVPR IEEE, P722, DOI 10.1109/CVPR.2017.84
   Marchesotti L, 2011, IEEE I CONF COMP VIS, P1784, DOI 10.1109/ICCV.2011.6126444
   Mathe S, 2016, PROC CVPR IEEE, P2894, DOI 10.1109/CVPR.2016.316
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   MURRAY N, 2012, PROC CVPR IEEE, P2408, DOI DOI 10.1109/CVPR.2012.6247954
   Nishiyama M., 2009, P 17 ACM INT C MULTI, P669
   Pan Z., 2021, P IEEECVF INT C COMP, P4218
   Rao YM, 2017, IEEE I CONF COMP VIS, P3951, DOI 10.1109/ICCV.2017.424
   Recht B, 2019, ANNU REV CONTR ROBOT, V2, P253, DOI 10.1146/annurev-control-053018-023825
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Suh Bongwon, 2003, P ACM S US INT SOFTW, P95, DOI DOI 10.1145/964696.964707
   Tan MX, 2019, PR MACH LEARN RES, V97
   Tan YL, 2016, NEUROCOMPUTING, V188, P50, DOI 10.1016/j.neucom.2015.04.124
   Tu Y., 2019, ARXIV
   van Hasselt H, 2016, AAAI CONF ARTIF INTE, P2094
   Wang WG, 2017, IEEE I CONF COMP VIS, P2205, DOI 10.1109/ICCV.2017.240
   Wang Z., 2015, ARXIV
   Wei ZJ, 2018, PROC CVPR IEEE, P5437, DOI 10.1109/CVPR.2018.00570
   Wu F., 2017, ARXIV
   Xu YF, 2021, EXPERT SYST APPL, V171, DOI 10.1016/j.eswa.2021.114596
   Yan JZ, 2013, PROC CVPR IEEE, P971, DOI 10.1109/CVPR.2013.130
   Zeng H, 2019, PROC CVPR IEEE, P5942, DOI 10.1109/CVPR.2019.00610
   Zeng H, 2020, IEEE T IMAGE PROCESS, V29, P1548, DOI 10.1109/TIP.2019.2941778
   Zhang LM, 2014, IEEE T IMAGE PROCESS, V23, P2235, DOI 10.1109/TIP.2014.2311658
   Zhang LM, 2014, IEEE T MULTIMEDIA, V16, P94, DOI 10.1109/TMM.2013.2286817
   Zhang LM, 2013, IEEE T IMAGE PROCESS, V22, P802, DOI 10.1109/TIP.2012.2223226
   Zhang MJ, 2005, 2005 IEEE International Conference on Multimedia and Expo (ICME), Vols 1 and 2, P438
   Zhang XY, 2021, IEEE T MULTIMEDIA, V23, P2545, DOI 10.1109/TMM.2020.3013350
   Zhao TY, 2021, IEEE MULTIMEDIA, V28, P117, DOI 10.1109/MMUL.2021.3053774
   Zhong L., 2021, P ACM T GRAPHICS SIG, P2111
NR 66
TC 0
Z9 0
U1 1
U2 12
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2023
VL 39
IS 11
BP 5651
EP 5667
DI 10.1007/s00371-022-02687-9
EA OCT 2022
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA W5SX6
UT WOS:000865700300001
DA 2024-07-18
ER

PT J
AU Yang, GM
   Wang, T
   Fang, XJ
   Zhang, J
AF Yang, Gaoming
   Wang, Tao
   Fang, Xianjin
   Zhang, Ji
TI RSFace: subject agnostic face swapping with expression high fidelity
SO VISUAL COMPUTER
LA English
DT Article
DE Face swapping; Face reenactment; Attribute matching loss; High fidelity
ID IDENTITY
AB Face swapping has shown remarkable progress with the flourishing development of deep learning. In particular, the emergence of subject agnostic methods has broadened the range of applications of face swapping. Furthermore, high fidelity implementation has improved the naturalness of generated faces. However, some high fidelity face swapping methods still suffer from expression distortion at this stage. In this work, we propose an extended Adaptive Embedding Integration Network (AEI-Net) to improve the performance of this network in synthesizing swapped faces on faces in the wild. First, we add a face reenactment module to synchronize the expressions of the input faces and reduce the influence of irrelevant attributes on the synthesis results. Second, we train AEI-Net using a new attribute matching loss to improve the consistency of the generated results and the target face expressions. Finally, extensive experiments on wild faces demonstrate that our method can better restore expression and posture while maintaining identity than previous methods.
C1 [Yang, Gaoming; Wang, Tao; Fang, Xianjin] Anhui Univ Sci & Technol, Sch Comp Sci & Engn, Huainan 232001, Peoples R China.
   [Zhang, Ji] Univ Southern Queensland, Sch Math Phys & Comp, Toowoomba, Qld 4350, Australia.
C3 Anhui University of Science & Technology; University of Southern
   Queensland
RP Wang, T (corresponding author), Anhui Univ Sci & Technol, Sch Comp Sci & Engn, Huainan 232001, Peoples R China.
EM taowang@aust.edu.cn
RI Wang, Tao/IZQ-3814-2023; wang, shuo/KCL-3379-2024
OI Wang, Tao/0000-0002-3038-0097; Yang, Gaoming/0000-0002-7666-1038
FU Natural Science Foundation of Anhui Province of China [2008085MF220]
FX This work was supported by the Natural Science Foundation of Anhui
   Province of China (2008085MF220).
CR Bao JM, 2018, PROC CVPR IEEE, P6713, DOI 10.1109/CVPR.2018.00702
   Bitouk D, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360638
   Burkov E., 2020, P IEEE CVF C COMP VI, P13783, DOI DOI 10.1109/CVPR42600.2020.01380
   Cao M, 2021, IEEE T IMAGE PROCESS, V30, P6107, DOI 10.1109/TIP.2021.3089909
   Chang FJ, 2019, INT J COMPUT VISION, V127, P930, DOI 10.1007/s11263-019-01151-x
   Chen RW, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2003, DOI 10.1145/3394171.3413630
   Dang H, 2020, PROC CVPR IEEE, P5780, DOI 10.1109/CVPR42600.2020.00582
   DeepFakes, DEEPFAKES
   Deng JK, 2019, PROC CVPR IEEE, P4685, DOI 10.1109/CVPR.2019.00482
   Deng Y, 2019, IEEE COMPUT SOC CONF, P285, DOI 10.1109/CVPRW.2019.00038
   FaceSwap, github com deepfakesm faceswap
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Ha S, 2020, AAAI CONF ARTIF INTE, V34, P10893
   Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167
   Isola Phillip, 2016, ARXIV161107004, DOI [DOI 10.1109/CVPR.2017.632, 10.1109/CVPR.2017.632]
   Karras T., 2018, INT C LEARN REPR, P1, Patent No. 171010196
   Karras T, 2021, IEEE T PATTERN ANAL, V43, P4217, DOI 10.1109/TPAMI.2020.2970919
   Karras Tero, 2020, IEEE C COMP VIS PATT
   Langner O, 2010, COGNITION EMOTION, V24, P1377, DOI 10.1080/02699930903485076
   Li LZ, 2020, PROC CVPR IEEE, P5073, DOI 10.1109/CVPR42600.2020.00512
   Luo YC, 2021, PROC CVPR IEEE, P16312, DOI 10.1109/CVPR46437.2021.01605
   Natsume R., 2018, ARXIV180403447, P1, DOI [DOI 10.1145/3230744.3230818, 10.1145/3230744.3230818]
   Nirkin Y, 2019, IEEE I CONF COMP VIS, P7183, DOI 10.1109/ICCV.2019.00728
   Nirkin Y, 2018, IEEE INT CONF AUTOMA, P98, DOI 10.1109/FG.2018.00024
   Park T, 2019, PROC CVPR IEEE, P2332, DOI 10.1109/CVPR.2019.00244
   Parkhi OM, 2015, Proceedings of the British Machine Vision Conference, DOI DOI 10.5244/C.29.41
   Rossi A, 2020, IEEE T INTELL TRANSP, V21, P2980, DOI 10.1109/TITS.2019.2922002
   Ruiz N, 2018, IEEE COMPUT SOC CONF, P2155, DOI 10.1109/CVPRW.2018.00281
   Suwajanakorn S, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073640
   Theobalt Chris- tian, 2016, P IEEE C COMPUTER VI, DOI DOI 10.1109/CVPR.2016.262
   Tyagi S, 2023, VISUAL COMPUT, V39, P813, DOI 10.1007/s00371-021-02347-4
   Wang CR, 2021, PROC CVPR IEEE, P14918, DOI 10.1109/CVPR46437.2021.01468
   Wang H, 2018, PROC CVPR IEEE, P5265, DOI 10.1109/CVPR.2018.00552
   Wang JH, 2020, IEEE T CYBERNETICS, V50, P2971, DOI 10.1109/TCYB.2019.2891265
   Wang Y., 2021, P 30 INT JOINT C ART, VVolume 8, P1136, DOI 10.24963/ijcai.2021/157
   Wiles O, 2018, LECT NOTES COMPUT SC, V11217, P690, DOI 10.1007/978-3-030-01261-8_41
   Zeng XF, 2020, AAAI CONF ARTIF INTE, V34, P12757
   Zhang JN, 2020, PROC CVPR IEEE, P5325, DOI 10.1109/CVPR42600.2020.00537
   Zhang Y, 2016, IEEE T IMAGE PROCESS, V25, DOI 10.1109/TIP.2016.2549360
   Zhu YH, 2021, PROC CVPR IEEE, P4832, DOI 10.1109/CVPR46437.2021.00480
NR 40
TC 0
Z9 0
U1 3
U2 27
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2023
VL 39
IS 11
BP 5497
EP 5511
DI 10.1007/s00371-022-02675-z
EA OCT 2022
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA W5SX6
UT WOS:000864059700002
DA 2024-07-18
ER

PT J
AU Naffouti, SE
   Kricha, A
   Sakly, A
AF Naffouti, Seif Eddine
   Kricha, Anis
   Sakly, Anis
TI A sophisticated and provably grayscale image watermarking system using
   DWT-SVD domain
SO VISUAL COMPUTER
LA English
DT Article
DE Copyright protection; Digital image watermarking; Discrete wavelet
   transform (DWT); Singular value decomposition (SVD); Imperceptibility;
   Robustness
ID SINGULAR-VALUE DECOMPOSITION; DISCRETE WAVELET TRANSFORM; DATA HIDING
   SCHEME; ROBUST WATERMARKING; DIFFERENTIAL EVOLUTION; INVARIANT WAVELET;
   DCT; HYBRID; SECURE
AB Digital watermarking has attracted increasing attentions as it has been the current solution to copyright protection and content authentication in today's digital transformation, which has become an issue to be addressed in multimedia technology. In this paper, we propose an advanced image watermarking system based on the discrete wavelet transform (DWT) in combination with the singular value decomposition (SVD). Firstly, at the sender side, DWT is applied on a grayscale cover image and then eigendecomposition is performed on original HH (high-high) components. Similar operation is done on a grayscale watermark image. Then, two unitary and one diagonal matrices are combined to form a digital watermarked image applying inverse discrete wavelet transform (iDWT). The diagonal component of original image is transmitted through secured channel. At the receiver end, the watermark image is recovered using the watermarked image and diagonal component of the original image. Finally, we compare the original and recovered watermark image and obtained perfect normalized correlation. Simulation consequences indicate that the presented scheme can satisfy the needs of visual imperceptibility and also has high security and strong robustness against many common attacks and signal processing operations. The proposed digital image watermarking system is also compared to state-of-the-art methods to confirm the reliability and supremacy.
C1 [Naffouti, Seif Eddine; Sakly, Anis] Univ Monastir, Natl Engn Sch Monastir ENIM, Lab Automat Elect Syst & Environm LAESE, Skaness Monastir 5019, Tunisia.
   [Kricha, Anis] Natl Engn Sch Sousse, Lab Adv Technol & Intelligent Syst LATIS, Sousse 4023, Tunisia.
   [Sakly, Anis] Natl Engn Sch Monastir ENIM, Skaness 5019, Tunisia.
C3 Universite de Monastir; Universite de Sousse; Universite de Monastir
RP Naffouti, SE (corresponding author), Univ Monastir, Natl Engn Sch Monastir ENIM, Lab Automat Elect Syst & Environm LAESE, Skaness Monastir 5019, Tunisia.
EM seifeddine.naffouti@gmail.com
OI Naffouti, Seif Eddine/0000-0003-1261-8484
CR Agoyi M, 2015, SIGNAL IMAGE VIDEO P, V9, P735, DOI 10.1007/s11760-014-0624-9
   Akhila L., 2022, 2022 4th International Conference on Smart Systems and Inventive Technology (ICSSIT), P1369, DOI 10.1109/ICSSIT53264.2022.9716314
   AL-Mansoori S, 2012, INT J COMPUT SCI NET, V12, P1
   Ali M, 2018, MULTIMED TOOLS APPL, V77, P11751, DOI 10.1007/s11042-017-4815-6
   Ali M, 2016, DISCRETE DYN NAT SOC, V2016, DOI 10.1155/2016/3263587
   Ali M, 2015, INFORM SCIENCES, V301, P44, DOI 10.1016/j.ins.2014.12.042
   Ali M, 2014, OPTIK, V125, P428, DOI 10.1016/j.ijleo.2013.06.082
   ANDREWS HC, 1976, IEEE T COMMUN, V24, P425, DOI 10.1109/TCOM.1976.1093309
   [Anonymous], 2015, 2015 IEEE INT C COMP
   Ansari IA, 2016, ADV INTELL SYST COMP, V437, P411, DOI 10.1007/978-981-10-0451-3_38
   Ansari IA, 2017, PATTERN RECOGN LETT, V94, P228, DOI 10.1016/j.patrec.2016.12.010
   Ahmadi SBB, 2021, VISUAL COMPUT, V37, P385, DOI 10.1007/s00371-020-01808-6
   Begum M., 2021, SN Comput. Sci, V2, P221, DOI [10.1007/s42979-021-00608-6, DOI 10.1007/S42979-021-00608-6]
   Belazi A, 2015, INT WIREL COMMUN, P606, DOI 10.1109/IWCMC.2015.7289152
   Benrhouma O, 2016, MULTIMED TOOLS APPL, V75, P8695, DOI 10.1007/s11042-015-2786-z
   Chung KL, 2007, APPL MATH COMPUT, V188, P54, DOI 10.1016/j.amc.2006.09.117
   Computer vision group, DAT STAND GRAYSC TES
   Cui XC, 2018, PLOS ONE, V13, DOI 10.1371/journal.pone.0196306
   Das C, 2014, AEU-INT J ELECTRON C, V68, P244, DOI 10.1016/j.aeue.2013.08.018
   Ernawan F, 2020, VISUAL COMPUT, V36, P19, DOI 10.1007/s00371-018-1567-x
   Furqan A, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTATIONAL INTELLIGENCE AND COMMUNICATION TECHNOLOGY CICT 2015, P638, DOI 10.1109/CICT.2015.74
   Gong LH, 2021, MULTIMED TOOLS APPL, V80, P439, DOI 10.1007/s11042-020-09677-w
   Hemdan EED, 2013, PROCEEDING 3 INT C A
   Hu K., 2021, VISUAL COMPUT, P1
   Hussain M, 2021, MULTIMED TOOLS APPL, V80, P20381, DOI 10.1007/s11042-021-10652-2
   Hussain M, 2018, SIGNAL PROCESS-IMAGE, V65, P46, DOI 10.1016/j.image.2018.03.012
   Kadian P, 2019, 2019 6TH INTERNATIONAL CONFERENCE ON SIGNAL PROCESSING AND INTEGRATED NETWORKS (SPIN), P987, DOI 10.1109/SPIN.2019.8711681
   Kang XB, 2018, MULTIMED TOOLS APPL, V77, P13197, DOI 10.1007/s11042-017-4941-1
   Kazemivash B, 2018, SOFT COMPUT, V22, P4083, DOI 10.1007/s00500-017-2617-4
   Kazemivash B, 2017, MULTIMED TOOLS APPL, V76, P20499, DOI 10.1007/s11042-016-3962-5
   Khare P, 2021, J INTELL SYST, V30, P297, DOI 10.1515/jisys-2019-0046
   Konstantinides K, 1997, IEEE T IMAGE PROCESS, V6, P479, DOI 10.1109/83.557359
   Kumar S, 2016, 2016 INTERNATIONAL CONFERENCE ON INFORMATION COMMUNICATION AND EMBEDDED SYSTEMS (ICICES)
   Li YM, 2021, INFORM SCIENCES, V551, P205, DOI 10.1016/j.ins.2020.11.020
   Liao X, 2022, IEEE T DEPEND SECURE, V19, P897, DOI 10.1109/TDSC.2020.3004708
   Liao X, 2020, IEEE J-STSP, V14, P955, DOI 10.1109/JSTSP.2020.3002391
   Liao X, 2020, IEEE T CIRC SYST VID, V30, P685, DOI 10.1109/TCSVT.2019.2896270
   Liao X, 2016, SECUR COMMUN NETW, V9, P5756, DOI 10.1002/sec.1734
   Liu HH, 2019, MULTIMED TOOLS APPL, V78, P12157, DOI 10.1007/s11042-018-6766-y
   Liu Y, 2018, EXPERT SYST APPL, V97, P95, DOI 10.1016/j.eswa.2017.12.003
   Lu W, 2010, COMPUT ELECTR ENG, V36, P2, DOI 10.1016/j.compeleceng.2009.04.002
   Makbol NM, 2017, INFORM SCIENCES, V417, P381, DOI 10.1016/j.ins.2017.07.026
   Makbol NM, 2016, IET IMAGE PROCESS, V10, P34, DOI 10.1049/iet-ipr.2014.0965
   MALLAT SG, 1989, IEEE T PATTERN ANAL, V11, P674, DOI 10.1109/34.192463
   Moeinaddini E, 2018, MULTIMED TOOLS APPL, V77, P26083, DOI 10.1007/s11042-018-5838-3
   Moeinaddini E, 2015, 2015 INTERNATIONAL SYMPOSIUM ON ARTIFICIAL INTELLIGENCE AND SIGNAL PROCESSING (AISP), P159, DOI 10.1109/AISP.2015.7123512
   Mohamed M., 2012, INT J COMPUTER SCI I, V9, P394
   Mohammad AA, 2008, SIGNAL PROCESS, V88, P2158, DOI 10.1016/j.sigpro.2008.02.015
   Moran MBH, 2018, IEEE IJCNN
   Murali P, 2023, VISUAL COMPUT, V39, P1057, DOI 10.1007/s00371-021-02384-z
   Ouhsain M, 2009, EXPERT SYST APPL, V36, P2123, DOI 10.1016/j.eswa.2007.12.046
   Peng J, 2019, PROC SPIE, V11179, DOI 10.1117/12.2539753
   Phadikar A, 2011, COMPUT ELECTR ENG, V37, P339, DOI 10.1016/j.compeleceng.2011.02.002
   Ray AK, 2015, ADV INTELL SYST, V332, P79, DOI 10.1007/978-81-322-2196-8_10
   Roy S, 2017, MULTIMED TOOLS APPL, V76, P3577, DOI 10.1007/s11042-016-3902-4
   Sahu AK, 2019, WIRELESS PERS COMMUN, V108, P159, DOI 10.1007/s11277-019-06393-z
   Sahu AK, 2018, INTERNETWORKING INDO, V10, P17
   Salama AS, 2016, 2016 2ND IEEE INTERNATIONAL CONFERENCE ON COMPUTER AND COMMUNICATIONS (ICCC), P557, DOI 10.1109/CompComm.2016.7924763
   Singh RK, 2017, J INFORM OPTIM SCI, V38, P911, DOI 10.1080/02522667.2017.1372137
   Singh S, 2020, MULTIMED TOOLS APPL, V79, P18815, DOI 10.1007/s11042-020-08745-5
   Swain G, 2019, ARAB J SCI ENG, V44, P2995, DOI 10.1007/s13369-018-3372-2
   Swain G, 2016, MULTIMED TOOLS APPL, V75, P13541, DOI 10.1007/s11042-015-2937-2
   Tagesse Takore Tamirat, 2018, 2nd International Conference on Micro-Electronics, Electromagnetics and Telecommunications, ICMEET 2016. Proceedings: LNEE 434, P51, DOI 10.1007/978-981-10-4280-5_6
   Tarhouni N, 2020, CIRC SYST SIGNAL PR, V39, P5059, DOI 10.1007/s00034-020-01401-1
   The waterloo fractal coding and analysis group, IM REP
   Vali MH, 2018, EXPERT SYST APPL, V114, P296, DOI 10.1016/j.eswa.2018.07.004
   Verma VS, 2015, J VIS COMMUN IMAGE R, V31, P75, DOI 10.1016/j.jvcir.2015.06.001
   Wang BW, 2020, MATHEMATICS-BASEL, V8, DOI 10.3390/math8050691
   Wang MS, 2009, COMPUT STAND INTER, V31, P757, DOI 10.1016/j.csi.2008.09.003
   Wang T, 2017, IEEE INT C COMPUT, P724, DOI 10.1109/CSE-EUC.2017.141
   Wang XY, 2022, VISUAL COMPUT, V38, P3831, DOI 10.1007/s00371-021-02224-0
   Wang Z, 2002, IEEE SIGNAL PROC LET, V9, P81, DOI 10.1109/97.995823
   Wu DC, 2003, PATTERN RECOGN LETT, V24, P1613, DOI 10.1016/S0167-8655(02)00402-6
   Wu HC, 2005, IEE P-VIS IMAGE SIGN, V152, P611, DOI 10.1049/ip-vis:20059022
   Wu NI, 2017, IMAGING SCI J, V65, P371, DOI 10.1080/13682199.2017.1355089
   Yasmeen Fauzia, 2021, SN Comput Sci, V2, P82, DOI 10.1007/s42979-021-00478-y
   Zear A, 2018, MULTIMED TOOLS APPL, V77, P4863, DOI 10.1007/s11042-016-3862-8
   Zhang L., 2014, INT J COMMUN IJC, V3, P62
NR 78
TC 9
Z9 9
U1 6
U2 33
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2023
VL 39
IS 9
BP 4227
EP 4247
DI 10.1007/s00371-022-02587-y
EA JUL 2022
PG 21
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA Q8PV6
UT WOS:000819705400004
PM 35791414
OA Green Published, Bronze
DA 2024-07-18
ER

PT J
AU Wang, XY
   Chen, SN
AF Wang, Xingyuan
   Chen, Shengnan
TI An image encryption algorithm based on pixel bit operation and nonlinear
   chaotic system
SO VISUAL COMPUTER
LA English
DT Article
DE Chaos theory; Scrambling and diffusion operations; Chaos image
   encryption; Non-unique diffusion
ID SEMI-TENSOR PRODUCT; MATRIX; COMBINATION; ENTROPY; MAP
AB This paper proposes a new one-dimensional chaotic map-nonlinear coupled Sine-Tent-Logistic chaotic map (1DNCSTL). A series of tests on this map show that the map has the characteristics of randomness and sensitivity to initial values and is suitable for image encryption. Based on this map, the article further proposes pixel bit position scrambling and reorganization operation and dynamic nonunique diffusion operation. Pixel bit position scrambling and reorganization operation is different from the traditional scrambling operation that only changes the position of the pixel value; this operation can achieve the effect of changing the pixel position and pixel value at the same time. The dynamic nonunique diffusion operation is different from the traditional unique-formula diffusion operation. The diffusion formula is not unique, which can ensure the security of the algorithm. Simulation experiment results and various security performance analysis show that the algorithm proposed in this paper has good performance. Compared with other encryption schemes, this algorithm is more suitable for image encryption.
C1 [Wang, Xingyuan; Chen, Shengnan] Dalian Maritime Univ, Sch Informat Sci & Technol, Dalian 116026, Peoples R China.
C3 Dalian Maritime University
RP Wang, XY (corresponding author), Dalian Maritime Univ, Sch Informat Sci & Technol, Dalian 116026, Peoples R China.
EM xywang@dlmu.edu.cn; csn1315319490@163.com
RI Wang, Xing-yuan/I-6353-2015
OI Chen, Shengnan/0000-0003-1455-316X
FU National Natural Science Foundation of China [61672124]; Password Theory
   Project of the 13th Five-Year Plan National Cryptography Development
   Fund [MMJJ20170203]; Liaoning Province Science and Technology Innovation
   Leading Talents Program Project [XLYC1802013]; Key R&D Projects of
   Liaoning Province [2019020105-JH2/103]; Jinan City "20 universities"
   Funding Projects Introducing Innovation Team Program [2019GXRC031]
FX This research is supported by the National Natural Science Foundation of
   China (No: 61672124), the Password Theory Project of the 13th Five-Year
   Plan National Cryptography Development Fund (No: MMJJ20170203), Liaoning
   Province Science and Technology Innovation Leading Talents Program
   Project (No: XLYC1802013), Key R&D Projects of Liaoning Province (No:
   2019020105-JH2/103), Jinan City "20 universities" Funding Projects
   Introducing Innovation Team Program (No: 2019GXRC031).
CR Alvarez G, 2006, INT J BIFURCAT CHAOS, V16, P2129, DOI 10.1142/S0218127406015970
   Brindha M, 2016, APPL SOFT COMPUT, V40, P379, DOI 10.1016/j.asoc.2015.09.055
   Cao C, 2018, SIGNAL PROCESS, V143, P122, DOI 10.1016/j.sigpro.2017.08.020
   Chai XL, 2021, SIGNAL PROCESS, V183, DOI 10.1016/j.sigpro.2021.108041
   Chai XL, 2021, INFORM SCIENCES, V556, P305, DOI 10.1016/j.ins.2020.10.007
   Chai XL, 2020, SIGNAL PROCESS, V176, DOI 10.1016/j.sigpro.2020.107684
   Chai XL, 2020, NEURAL COMPUT APPL, V32, P8065, DOI 10.1007/s00521-019-04312-8
   Chai XL, 2019, SIGNAL PROCESS, V155, P44, DOI 10.1016/j.sigpro.2018.09.029
   Chai XL, 2017, MULTIMED TOOLS APPL, V76, P1159, DOI 10.1007/s11042-015-3088-1
   Chai XL, 2017, OPT LASER ENG, V88, P197, DOI 10.1016/j.optlaseng.2016.08.009
   Hosny KM, 2023, VISUAL COMPUT, V39, P1027, DOI 10.1007/s00371-021-02382-1
   Hua ZY, 2018, SIGNAL PROCESS, V149, P148, DOI 10.1016/j.sigpro.2018.03.010
   Hua ZY, 2018, IEEE T IND ELECTRON, V65, P2557, DOI 10.1109/TIE.2017.2736515
   Hua ZY, 2016, INFORM SCIENCES, V339, P237, DOI 10.1016/j.ins.2016.01.017
   Hua ZY, 2015, INFORM SCIENCES, V297, P80, DOI 10.1016/j.ins.2014.11.018
   Jia M, 2020, INT J OPT, V2020, DOI 10.1155/2020/8678527
   Kang XJ, 2020, SIGNAL PROCESS-IMAGE, V80, DOI 10.1016/j.image.2019.115670
   Li CH, 2017, NONLINEAR DYNAM, V87, P127, DOI 10.1007/s11071-016-3030-8
   Li GD, 2019, VISUAL COMPUT, V35, P1267, DOI 10.1007/s00371-018-1574-y
   Li SL, 2018, ENTROPY-SWITZ, V20, DOI 10.3390/e20060463
   Li XW, 2018, OPT LASER ENG, V100, P200, DOI 10.1016/j.optlaseng.2017.08.018
   [刘艮 Liu Gen], 2013, [计算机工程与科学, Computer Engineering and Science], V35, P106
   Liu HJ, 2014, INT J NONLIN SCI NUM, V15, P1, DOI 10.1515/ijnsns-2011-0001
   Mansouri A, 2021, VISUAL COMPUT, V37, P189, DOI 10.1007/s00371-020-01791-y
   Pak C, 2017, SIGNAL PROCESS, V138, P129, DOI 10.1016/j.sigpro.2017.03.011
   Ramasamy P, 2019, ENTROPY-SWITZ, V21, DOI 10.3390/e21070656
   Richman JS, 2000, AM J PHYSIOL-HEART C, V278, pH2039
   Talhaoui MZ, 2021, VISUAL COMPUT, V37, P541, DOI 10.1007/s00371-020-01822-8
   Talhaoui MZ, 2021, VISUAL COMPUT, V37, P1757, DOI 10.1007/s00371-020-01936-z
   Wang XY, 2019, INT J NONLIN SCI NUM, V20, P167, DOI 10.1515/ijnsns-2018-0027
   Wang XY, 2022, IEEE T CIRCUITS-I, V69, P1291, DOI 10.1109/TCSI.2021.3133318
   Wang XY, 2022, VISUAL COMPUT, V38, P763, DOI 10.1007/s00371-020-02048-4
   Wang XY, 2020, INFORM SCIENCES, V539, P195, DOI 10.1016/j.ins.2020.06.030
   Wang XY, 2019, MULTIMED TOOLS APPL, V78, P34981, DOI 10.1007/s11042-019-08085-z
   Wang XY, 2020, INFORM SCIENCES, V507, P16, DOI 10.1016/j.ins.2019.08.041
   Wang XY, 2019, INFORM SCIENCES, V486, P340, DOI 10.1016/j.ins.2019.02.049
   WOLF A, 1985, PHYSICA D, V16, P285, DOI 10.1016/0167-2789(85)90011-9
   Wu Y, 2011, Cyber J Multidiscip J Sci Technol J Sel Areas Telecommun (JSAT), V1, P31
   Wu Y, 2014, INFORM SCIENCES, V264, P317, DOI 10.1016/j.ins.2013.11.027
   Wu Y, 2013, INFORM SCIENCES, V222, P323, DOI 10.1016/j.ins.2012.07.049
   Wu Y, 2012, J ELECTRON IMAGING, V21, DOI 10.1117/1.JEI.21.1.013014
   Xian YS, 2022, INT J COAL PREP UTIL, V42, P3249, DOI 10.1080/19392699.2021.1949712
   Xu J, 2022, VISUAL COMPUT, V38, P1509, DOI 10.1007/s00371-021-02085-7
   Zheng JY, 2020, IET IMAGE PROCESS, V14, P2310, DOI 10.1049/iet-ipr.2019.1340
   Zhou MJ, 2020, SIGNAL PROCESS, V171, DOI 10.1016/j.sigpro.2020.107484
   Zhou YC, 2013, SIGNAL PROCESS, V93, P3039, DOI 10.1016/j.sigpro.2013.04.021
NR 46
TC 5
Z9 5
U1 2
U2 42
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2023
VL 39
IS 7
BP 3123
EP 3144
DI 10.1007/s00371-022-02517-y
EA MAY 2022
PG 22
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA L4HX1
UT WOS:000803854800001
DA 2024-07-18
ER

PT J
AU Cai, YC
   Li, L
   Wang, D
   Huang, XT
   Liu, XP
AF Cai, Youcheng
   Li, Lin
   Wang, Dong
   Huang, Xintao
   Liu, Xiaoping
TI GlcMatch: global and local constraints for reliable feature matching
SO VISUAL COMPUTER
LA English
DT Article
DE Local feature; Image matching; Correspondence; Outlier rejection
ID IMAGE; CONSENSUS
AB A match is considered as an incorrect match when the matched features in two views do not correspond to the same physical location. It is inevitable that generates mismatches at a local descriptor level. Differentiating true and false matches remains a challenge, especially in the case of ambiguities, wide baselines, and strong illumination variations, which might contain a large number of mismatches (even up to 90%). In this paper, we develop GlcMatch, an outlier rejection method that takes advantage of both global and local constraints to classify putative matches. Specifically, we use vector field consistency to form continuous global smoothness and use triangular mesh constraints to implement the local piecewise smoothness. Evaluation on benchmark datasets demonstrates GlcMatch can obtain large numbers of good quality correspondences and achieve significant performance.
C1 [Cai, Youcheng; Li, Lin; Wang, Dong; Huang, Xintao; Liu, Xiaoping] Hefei Univ Technol, Sch Comp Sci & Informat Engn, Hefei 230009, Peoples R China.
   [Liu, Xiaoping] Minist Educ, Engn Res Ctr Safety Crit Ind Measurement & Contro, Hefei 230009, Peoples R China.
C3 Hefei University of Technology
RP Liu, XP (corresponding author), Hefei Univ Technol, Sch Comp Sci & Informat Engn, Hefei 230009, Peoples R China.; Liu, XP (corresponding author), Minist Educ, Engn Res Ctr Safety Crit Ind Measurement & Contro, Hefei 230009, Peoples R China.
EM liu@hfut.edu.cn
FU National Key Research and Development Program of China [2020YFC1523100];
   Fundamental Research Funds for the Central Universities
   [PA2021GDGP0061]; National Natural Science Foundation of China
   [61877016]
FX This work was supported in part by the National Key Research and
   Development Program of China under Grant 2020YFC1523100, in part by the
   Fundamental Research Funds for the Central Universities under Grant
   PA2021GDGP0061, and in part by the National Natural Science Foundation
   of China under Grant 61877016.
CR [Anonymous], 2016, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2016.445
   ARONSZAJN N, 1950, T AM MATH SOC, V68, P337, DOI 10.1090/s0002-9947-1950-0051437-7
   Avidan S, 2007, IEEE T PATTERN ANAL, V29, P261, DOI 10.1109/TPAMI.2007.35
   Baldassarre L, 2012, MACH LEARN, V87, P259, DOI 10.1007/s10994-012-5282-y
   Baldassarre L, 2010, LECT NOTES ARTIF INT, V6321, P56, DOI 10.1007/978-3-642-15880-3_10
   Balntas V, 2017, PROC CVPR IEEE, P3852, DOI 10.1109/CVPR.2017.410
   Barath D, 2019, PROC CVPR IEEE, P10189, DOI 10.1109/CVPR.2019.01044
   Barath D, 2018, PROC CVPR IEEE, P6733, DOI 10.1109/CVPR.2018.00704
   Bartoli A, 2008, J MATH IMAGING VIS, V31, P133, DOI 10.1007/s10851-007-0062-1
   Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014
   Bentoutou Y, 2005, IEEE T GEOSCI REMOTE, V43, P2127, DOI 10.1109/TGRS.2005.853187
   Bian JW, 2017, PROC CVPR IEEE, P2828, DOI 10.1109/CVPR.2017.302
   Cabral B., 1993, P 20 ANN C COMP GRAP, P263, DOI DOI 10.1145/166117.166151
   Caetano TS, 2009, IEEE T PATTERN ANAL, V31, P1048, DOI 10.1109/TPAMI.2009.28
   Carmeli C, 2006, ANAL APPL, V4, P377, DOI 10.1142/S0219530506000838
   Chum O, 2003, LECT NOTES COMPUT SC, V2781, P236
   Collins T, 2014, LECT NOTES COMPUT SC, V8695, P138, DOI 10.1007/978-3-319-10584-0_10
   DeTone D, 2018, IEEE COMPUT SOC CONF, P337, DOI 10.1109/CVPRW.2018.00060
   Evgeniou T, 2000, ADV COMPUT MATH, V13, P1, DOI 10.1023/A:1018946025316
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Guclu O, 2020, VISUAL COMPUT, V36, P1271, DOI 10.1007/s00371-019-01720-8
   Han JW, 2015, IEEE T CIRC SYST VID, V25, P1309, DOI 10.1109/TCSVT.2014.2381471
   Heinly J, 2012, LECT NOTES COMPUT SC, V7573, P759, DOI 10.1007/978-3-642-33709-3_54
   Kazhdan M., 2006, P 4 EUR S GEOM PROC, P61, DOI DOI 10.2312/SGP/SGP06/061-070
   Leordeanu M, 2005, IEEE I CONF COMP VIS, P1482
   Lin WY, 2018, IEEE T PATTERN ANAL, V40, P34, DOI 10.1109/TPAMI.2017.2652468
   Lin WY, 2016, LECT NOTES COMPUT SC, V9905, P562, DOI 10.1007/978-3-319-46448-0_34
   Lin WYD, 2014, LECT NOTES COMPUT SC, V8692, P341, DOI 10.1007/978-3-319-10593-2_23
   LIU HR, 2010, PROC CVPR IEEE, P1609, DOI DOI 10.1109/CVPR.2010.5539780
   Liu Z, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.16
   Liu ZY, 2020, VISUAL COMPUT, V36, P1823, DOI 10.1007/s00371-019-01778-4
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Ma JY, 2013, PATTERN RECOGN, V46, P3519, DOI 10.1016/j.patcog.2013.05.017
   Micchelli CA, 2005, NEURAL COMPUT, V17, P177, DOI 10.1162/0899766052530802
   Mikolajczyk K, 2005, IEEE T PATTERN ANAL, V27, P1615, DOI 10.1109/TPAMI.2005.188
   Morel JM, 2009, SIAM J IMAGING SCI, V2, P438, DOI 10.1137/080732730
   Mur-Artal R, 2015, IEEE T ROBOT, V31, P1147, DOI 10.1109/TRO.2015.2463671
   Ni K, 2009, IEEE I CONF COMP VIS, P2193, DOI 10.1109/ICCV.2009.5459241
   Olsson C, 2011, LECT NOTES COMPUT SC, V6688, P524, DOI 10.1007/978-3-642-21227-7_49
   Pizarro D, 2012, INT J COMPUT VISION, V97, P54, DOI 10.1007/s11263-011-0452-0
   Ranftl R, 2018, LECT NOTES COMPUT SC, V11205, P292, DOI 10.1007/978-3-030-01246-5_18
   Rublee E, 2011, IEEE I CONF COMP VIS, P2564, DOI 10.1109/ICCV.2011.6126544
   Sarlin PE, 2020, PROC CVPR IEEE, P4937, DOI 10.1109/CVPR42600.2020.00499
   Schönberger JL, 2017, PROC CVPR IEEE, P6959, DOI 10.1109/CVPR.2017.736
   Schönberger JL, 2016, LECT NOTES COMPUT SC, V9907, P501, DOI 10.1007/978-3-319-46487-9_31
   Tian YR, 2019, PROC CVPR IEEE, P11008, DOI 10.1109/CVPR.2019.01127
   Torresani L, 2008, LECT NOTES COMPUT SC, V5303, P596, DOI 10.1007/978-3-540-88688-4_44
   Yi KM, 2018, PROC CVPR IEEE, P2666, DOI 10.1109/CVPR.2018.00282
   Zanfir A, 2018, PROC CVPR IEEE, P2684, DOI 10.1109/CVPR.2018.00284
   Zhang JH, 2019, IEEE I CONF COMP VIS, P5844, DOI 10.1109/ICCV.2019.00594
   Zhao J, 2011, PROC CVPR IEEE, P2977, DOI 10.1109/CVPR.2011.5995336
   Zhao XY, 2023, VISUAL COMPUT, V39, P1391, DOI 10.1007/s00371-022-02417-1
   Zhou F, 2012, PROC CVPR IEEE, P127, DOI 10.1109/CVPR.2012.6247667
NR 53
TC 1
Z9 1
U1 0
U2 13
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2023
VL 39
IS 7
BP 2555
EP 2570
DI 10.1007/s00371-022-02478-2
EA MAY 2022
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA L4HX1
UT WOS:000805468000001
DA 2024-07-18
ER

PT J
AU Zhang, ZC
   Chen, H
   Yin, XQ
   Deng, JS
   Li, WL
AF Zhang, Zhichao
   Chen, Hui
   Yin, Xiaoqing
   Deng, Jinsheng
   Li, Weili
TI Dynamic selection of proper kernels for image deblurring: a
   multistrategy design
SO VISUAL COMPUTER
LA English
DT Article
DE Deep learning; Image deblurring; Blur Kernel selection; Attention
   mechanism; Edge perception; Multifeature fusion
AB The non-blind deblurring approach can adequately deblur single-blur images by applying a suitable mathematical model. In contrast, it cannot satisfactorily deblur images that have multiple blurs. The blind deblurring approach is able to remove various kinds of blurs from an image. However, because the causes of blur in different regions differ, it is difficult to locate and remove all the blurs accurately and also to recover the fine texture details. Considering these weaknesses and strengths of both approaches, we propose a neural network that dynamically selects suitable blur kernels for deblurring. In the proposed method, the most appropriate kernels are extracted by joint training from multiple datasets that contain specific types of blurs to tackle local and global regions in one image. In addition, to further improve the image restoration quality, we designed an edge-attention mechanism to compensate the edges and structures of specific objects. The results of experiments conducted indicate that the dynamic selection of blur kernels combined with the edge attention algorithm not only improves PSNR and SSIM, but also outperforms state-of-the-art methods.
C1 [Zhang, Zhichao; Chen, Hui] Natl Univ Def Technol, Coll Comp, Changsha, Peoples R China.
   [Yin, Xiaoqing; Deng, Jinsheng] Natl Univ Def Technol, Coll Adv Interdisciplinary Studies, Changsha, Peoples R China.
   [Li, Weili] Natl Univ Def Technol, Coll Syst Engn, Changsha, Peoples R China.
C3 National University of Defense Technology - China; National University
   of Defense Technology - China; National University of Defense Technology
   - China
RP Deng, JS (corresponding author), Natl Univ Def Technol, Coll Adv Interdisciplinary Studies, Changsha, Peoples R China.; Li, WL (corresponding author), Natl Univ Def Technol, Coll Syst Engn, Changsha, Peoples R China.
EM jsdeng@nudt.edu.cn; weiwei6563@163.com
OI Zhang, Zhichao/0000-0002-7516-059X
FU Postgraduate Innovation Project of Double First-class Universities;
   National Nature Science Foundation of China [:62001493]
FX This research was supported by the Postgraduate Innovation Project of
   Double First-class Universities. This work is supported by National
   Nature Science Foundation of China (grant number:62001493).
CR Dai JG, 2019, IEEE ACCESS, V7, P173377, DOI 10.1109/ACCESS.2019.2956725
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hirsch M, 2011, IEEE I CONF COMP VIS, P463, DOI 10.1109/ICCV.2011.6126276
   Hore Alain, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P2366, DOI 10.1109/ICPR.2010.579
   Hu Z, 2015, INT J COMPUT VISION, V115, P345, DOI 10.1007/s11263-015-0821-1
   Javaran TA, 2017, COMPUT VIS IMAGE UND, V154, P16, DOI 10.1016/j.cviu.2016.09.013
   Krishnan D, 2011, PROC CVPR IEEE, P233, DOI 10.1109/CVPR.2011.5995521
   Kupyn O, 2019, IEEE I CONF COMP VIS, P8877, DOI 10.1109/ICCV.2019.00897
   Kupyn O, 2018, PROC CVPR IEEE, P8183, DOI 10.1109/CVPR.2018.00854
   Kyrki V, 2011, IEEE ROBOT AUTOM MAG, V18, P121, DOI 10.1109/MRA.2011.941638
   Li J, 2021, VISUAL COMPUT, V37, P619, DOI 10.1007/s00371-020-01828-2
   Lin GS, 2017, PROC CVPR IEEE, P5168, DOI 10.1109/CVPR.2017.549
   Lin M, 2014, 2014 INTERNATIONAL CONFERENCE ON MEDICAL BIOMETRICS (ICMB 2014), P1, DOI 10.1109/ICMB.2014.8
   Liu GC, 2014, IEEE T IMAGE PROCESS, V23, P5047, DOI 10.1109/TIP.2014.2362055
   Mei JH, 2019, MULTIMED TOOLS APPL, V78, P18869, DOI 10.1007/s11042-019-7251-y
   Nah S, 2017, PROC CVPR IEEE, P257, DOI 10.1109/CVPR.2017.35
   Nekrasov V, 2018, P BRIT MACH VIS C BM
   Oquab M, 2015, PROC CVPR IEEE, P685, DOI 10.1109/CVPR.2015.7298668
   Pan JS, 2014, PROC CVPR IEEE, P2901, DOI 10.1109/CVPR.2014.371
   Qi Q, 2020, SIGNAL PROCESS-IMAGE, V88, DOI 10.1016/j.image.2020.115952
   RICHARDSON WH, 1972, J OPT SOC AM, V62, P55, DOI 10.1364/JOSA.62.000055
   Rong WB, 2014, 2014 IEEE INTERNATIONAL CONFERENCE ON MECHATRONICS AND AUTOMATION (IEEE ICMA 2014), P577, DOI 10.1109/ICMA.2014.6885761
   Schelten K, 2015, IEEE WINT CONF APPL, P494, DOI 10.1109/WACV.2015.72
   Schuler CJ, 2013, PROC CVPR IEEE, P1067, DOI 10.1109/CVPR.2013.142
   Shan Q, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360672
   Shi WL, 2021, VISUAL COMPUT, V37, P1569, DOI 10.1007/s00371-020-01903-8
   Tao X, 2018, PROC CVPR IEEE, P8174, DOI 10.1109/CVPR.2018.00853
   Whyte O, 2014, INT J COMPUT VISION, V110, P185, DOI 10.1007/s11263-014-0727-3
   Xie SN, 2015, IEEE I CONF COMP VIS, P1395, DOI [10.1109/ICCV.2015.164, 10.1007/s11263-017-1004-z]
   Xu L, 2014, ADV NEUR IN, V27
   Xu L, 2010, LECT NOTES COMPUT SC, V6311, P157
   Ye MY, 2020, IEEE ACCESS, V8, P18316, DOI 10.1109/ACCESS.2020.2967823
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zhang HG, 2019, PROC CVPR IEEE, P5971, DOI 10.1109/CVPR.2019.00613
   Zhang K., ARXIV PREPRINT ARXIV
   Zheng S, 2019, IEEE SIGNAL PROC LET, V26, P1546, DOI 10.1109/LSP.2019.2939752
   Zhou B., 2014, CORR, V1412, P6856
   Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319
   Zhu PF, 2019, IEEE INT CONF COMP V, P227, DOI 10.1109/ICCVW.2019.00031
NR 39
TC 4
Z9 4
U1 2
U2 22
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2023
VL 39
IS 4
BP 1375
EP 1390
DI 10.1007/s00371-022-02415-3
EA APR 2022
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA C3OY5
UT WOS:000784625300001
DA 2024-07-18
ER

PT J
AU Lai, HL
   Luo, Y
   Zhang, GK
   Shen, X
   Li, B
   Lu, JW
AF Lai, Huilin
   Luo, Ye
   Zhang, Guokai
   Shen, Xiaoang
   Li, Bo
   Lu, Jianwei
TI Toward accurate polyp segmentation with cascade boundary-guided
   attention
SO VISUAL COMPUTER
LA English
DT Article
DE Polyp segmentation; Colonoscopy image; Boundary; Attention; Neural
   network
ID MULTISCALE
AB In clinical practice, accurate polyp segmentation provides important information for the early detection of colorectal cancer. Benefiting from the advancement of deep learning techniques, various neural networks have been developed for polyp segmentation. However, most state-of-the-art methods have suffered from the challenge of precisely segmenting polyps with clear boundaries. To tackle this challenge, in this paper, we propose a novel and effective cascade boundary-guided attention network based on an encoder-decoder framework. Specifically, instead of just using the addition of shallow and deep features, the fine-grained boundary information is explicitly introduced into the skip connection of encoder and decoder layers to achieve accurate polyp segmentation. Moreover, the cascade refinement strategy is utilized into the multi-stage enhancement of boundary features to progressively produce better predictions. Extensive evaluations on five public benchmark datasets show that our method outperforms state-of-the-arts on various polyp segmentation tasks. Further experiments conducted on the cross-dataset (training on one dataset and testing on another dataset) validate the generalization ability of the proposed method.
C1 [Lai, Huilin; Luo, Ye; Shen, Xiaoang; Li, Bo; Lu, Jianwei] Tongji Univ, Sch Software Engn, Shanghai 200092, Peoples R China.
   [Luo, Ye] Chinese Acad Sci, Shenyang Inst Automat, State key Lab Robot, Shenyang 110016, Peoples R China.
   [Zhang, Guokai] Univ Shanghai Sci & Technol, Sch Opt Elect & Comp Engn, Shanghai 200093, Peoples R China.
   [Lu, Jianwei] Shanghai Univ Tradit Chinese Med, Coll Rehabil Sci, Shanghai 201203, Peoples R China.
   [Lu, Jianwei] Minist Educ, Engn Res Ctr Tradit Chinese Med Intelligent Rehab, Shanghai 201203, Peoples R China.
C3 Tongji University; Chinese Academy of Sciences; Shenyang Institute of
   Automation, CAS; University of Shanghai for Science & Technology;
   Shanghai University of Traditional Chinese Medicine
RP Luo, Y; Lu, JW (corresponding author), Tongji Univ, Sch Software Engn, Shanghai 200092, Peoples R China.; Luo, Y (corresponding author), Chinese Acad Sci, Shenyang Inst Automat, State key Lab Robot, Shenyang 110016, Peoples R China.; Lu, JW (corresponding author), Shanghai Univ Tradit Chinese Med, Coll Rehabil Sci, Shanghai 201203, Peoples R China.; Lu, JW (corresponding author), Minist Educ, Engn Res Ctr Tradit Chinese Med Intelligent Rehab, Shanghai 201203, Peoples R China.
EM yeluo@tongji.edu.cn; jwlu33@tongji.edu.cn
RI luo, ye/KQU-4093-2024
OI Lai, Huilin/0000-0003-2057-7512
FU National Natural Science Foundation of China (NSFC) [61806147,
   62102259]; Shanghai Sailing Program [21YF1431600]; StateKey Laboratory
   of Robotics [2019O15]
FX This work was supported by the General Program of National Natural
   Science Foundation of China (NSFC) (Grant No. 61806147), the Shanghai
   Sailing Program (21YF1431600), the General Program of National Natural
   Science Foundation of China (NSFC) (Grant No. 62102259), and the
   StateKey Laboratory ofRobotics (2019O15).
CR Ameling S., 2009, BILDVERARBEITUNG MED, P346
   Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615
   Bernal J, 2012, PATTERN RECOGN, V45, P3166, DOI 10.1016/j.patcog.2012.03.002
   Bernal J, 2015, COMPUT MED IMAG GRAP, V43, P99, DOI 10.1016/j.compmedimag.2015.02.007
   Brandao P, 2017, PROC SPIE, V10134, DOI 10.1117/12.2254361
   Chao P, 2019, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2019.00365
   Chen SH, 2018, LECT NOTES COMPUT SC, V11213, P236, DOI 10.1007/978-3-030-01240-3_15
   Cheng ZM, 2022, VISUAL COMPUT, V38, P749, DOI 10.1007/s00371-021-02075-9
   De Boer PT, 2005, ANN OPER RES, V134, P19, DOI 10.1007/s10479-005-5724-z
   Deng-Ping Fan, 2020, Medical Image Computing and Computer Assisted Intervention - MICCAI 2020. 23rd International Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12266), P263, DOI 10.1007/978-3-030-59725-2_26
   Fang YQ, 2021, IEEE SENS J, V21, P11799, DOI 10.1109/JSEN.2020.3015831
   Fang YQ, 2019, LECT NOTES COMPUT SC, V11764, P302, DOI 10.1007/978-3-030-32239-7_34
   Galdran Adrian, 2021, Pattern Recognition. ICPR 2020 International Workshops and Challenges. Proceedings. Lecture Notes in Computer Science (LNCS 12661), P293, DOI 10.1007/978-3-030-68763-2_22
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Ho Jonathan, 2019, Axial attention in multidimensional transformers
   Huang LY, 2022, VISUAL COMPUT, V38, P135, DOI 10.1007/s00371-020-02008-y
   Huang SW, 2012, ADV MATER RES-SWITZ, V340, P70, DOI 10.4028/www.scientific.net/AMR.340.70
   Hwang S, 2007, IEEE IMAGE PROC, P1029
   Jha D, 2021, IEEE J BIOMED HEALTH, V25, P2029, DOI 10.1109/JBHI.2021.3049304
   Jha D, 2020, LECT NOTES COMPUT SC, V11962, P451, DOI 10.1007/978-3-030-37734-2_37
   Jha D, 2019, IEEE INT SYM MULTIM, P225, DOI 10.1109/ISM46123.2019.00049
   Jiang M, 2022, VISUAL COMPUT, V38, P2473, DOI 10.1007/s00371-021-02124-3
   Karkanis SA, 2003, IEEE T INF TECHNOL B, V7, P141, DOI 10.1109/TITB.2003.813794
   Mahmud T, 2021, COMPUT BIOL MED, V128, DOI 10.1016/j.compbiomed.2020.104119
   Milletari F, 2016, INT CONF 3D VISION, P565, DOI 10.1109/3DV.2016.79
   Murugesan B, 2019, IEEE ENG MED BIO, P7223, DOI [10.1109/EMBC.2019.8857339, 10.1109/embc.2019.8857339]
   Oktay O., 2018, ARXIV, DOI DOI 10.48550/ARXIV.1804.03999
   Pal S, 2019, MULTIDIM SYST SIGN P, V30, P373, DOI 10.1007/s11045-018-0561-9
   Qadir H.A., 2019, 2019 13 INT S MED IN, P1
   Qian X., 2021, Vis Comput, P1
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Ruifei Zhang, 2020, Medical Image Computing and Computer Assisted Intervention - MICCAI 2020. 23rd International Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12266), P253, DOI 10.1007/978-3-030-59725-2_25
   Rutter CM, 2012, CANCER CAUSE CONTROL, V23, P289, DOI 10.1007/s10552-011-9878-5
   Silva J, 2014, INT J COMPUT ASS RAD, V9, P283, DOI 10.1007/s11548-013-0926-3
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Tajbakhsh N, 2016, IEEE T MED IMAGING, V35, P630, DOI 10.1109/TMI.2015.2487997
   Vázquez D, 2017, J HEALTHC ENG, V2017, DOI 10.1155/2017/4037190
   Wang D, 2021, VISUAL COMPUT, V37, P1101, DOI 10.1007/s00371-020-01855-z
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   Wu Z, 2019, PROC CVPR IEEE, P3902, DOI 10.1109/CVPR.2019.00403
   Yeung M, 2021, COMPUT BIOL MED, V137, DOI 10.1016/j.compbiomed.2021.104815
   Yue KY, 2018, ADV NEUR IN, V31
   Yuqian Z., 2006, Proceedings of The Sixth World Congress on Intelligent Control and Automation (WCICA), P9795
   Zhang GK, 2022, IEEE J BIOMED HEALTH, V26, P5298, DOI 10.1109/JBHI.2021.3127688
   Zhang ZX, 2018, IEEE GEOSCI REMOTE S, V15, P749, DOI 10.1109/LGRS.2018.2802944
   Zhou YN, 2019, LECT NOTES COMPUT SC, V11492, P682, DOI 10.1007/978-3-030-20351-1_53
   Zhou ZW, 2018, LECT NOTES COMPUT SC, V11045, P3, DOI 10.1007/978-3-030-00889-5_1
   Zhu L., 2021, P IEEECVF INT C COMP, P12292
NR 50
TC 11
Z9 11
U1 5
U2 28
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2023
VL 39
IS 4
BP 1453
EP 1469
DI 10.1007/s00371-022-02422-4
EA FEB 2022
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA C3OY5
UT WOS:000759383200002
DA 2024-07-18
ER

PT J
AU Wei, D
   Wang, ZY
   Luo, YP
AF Wei, Dan
   Wang, Ziyang
   Luo, Yiping
TI Video person re-identification based on RGB triple pyramid model
SO VISUAL COMPUTER
LA English
DT Article
DE Video-based person re-identification; Triple pyramid model; Fusion;
   Video triplet loss
ID NETWORKS
AB In order to solve the difficult problem of pedestrian motion extraction in video, in this paper, we propose a novel video action information extraction model named RGB triple pyramid model. Firstly, the model extracts the action information of the three parts by R, G, and B channels in the RGB image and integrates the action information of the three parts to obtain the complete action information. Secondly, two fusion stages are set and the fusion methods and functions of the two fusion stage are different. In the fusion I stage, we fuse R, G, and B action information into a complete person motion information. In the fusion II stage, we integrate the action information into the appearance information to include the action information when processing the appearance information, which complements the overall appearance information. Finally, we improve the method of triplet loss training parameters and apply triplet loss training to video pedestrian re-identification. Video triplet loss includes not only intra-video distance metric loss and an inter-video distance metric loss, but also action loss between intra-video and inter-video and the appearance loss within intra-video and inter-video. Extensive experimental results on the large-scale MARS, iLIDS-VID, and PRID-2011 datasets demonstrate that the proposed method achieves the state-of-the-art performance.
C1 [Wei, Dan; Wang, Ziyang; Luo, Yiping] Shanghai Univ Engn Sci, Sch Mech & Automot Engn, 333 Longteng Rd, Shanghai, Peoples R China.
C3 Shanghai University of Engineering Science
RP Wei, D (corresponding author), Shanghai Univ Engn Sci, Sch Mech & Automot Engn, 333 Longteng Rd, Shanghai, Peoples R China.
EM weiweidandan@163.com
OI Wei, Dan/0000-0002-8077-7623
CR [Anonymous], 2014, ABS14053531 CORR
   Brox T, 2004, LECT NOTES COMPUT SC, V2034, P25, DOI 10.1007/978-3-540-24673-2_3
   Chen DP, 2018, PROC CVPR IEEE, pCP1, DOI 10.1109/CVPR.2018.00128
   Feichtenhofer C, 2016, PROC CVPR IEEE, P1933, DOI 10.1109/CVPR.2016.213
   Fu Y, 2019, AAAI CONF ARTIF INTE, P8295
   Gao CX, 2020, INFORM SCIENCES, V527, P176, DOI 10.1016/j.ins.2020.04.007
   He KM, 2014, LECT NOTES COMPUT SC, V8691, P346, DOI [arXiv:1406.4729, 10.1007/978-3-319-10578-9_23]
   He LX, 2019, IEEE I CONF COMP VIS, P8449, DOI 10.1109/ICCV.2019.00854
   Hirzer M, 2011, LECT NOTES COMPUT SC, V6688, P91, DOI 10.1007/978-3-642-21227-7_9
   Hu XQ, 2021, PATTERN RECOGN, V111, DOI 10.1016/j.patcog.2020.107688
   Huang WJ, 2018, AAAI CONF ARTIF INTE, P2273
   Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59
   Jiang XY, 2020, AAAI CONF ARTIF INTE, V34, P11133
   Kviatkovsky Igor, 2013, IEEE Trans Pattern Anal Mach Intell, V35, P1622, DOI 10.1109/TPAMI.2012.246
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Leibe B., 2017, ARXIV170307737CS
   Li JN, 2019, IEEE I CONF COMP VIS, P3957, DOI 10.1109/ICCV.2019.00406
   Li JN, 2019, AAAI CONF ARTIF INTE, P8618
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu H, 2018, IEEE T CIRC SYST VID, V28, P2788, DOI 10.1109/TCSVT.2017.2715499
   Liu Y, 2017, PROC CVPR IEEE, P4694, DOI 10.1109/CVPR.2017.499
   Martinel N, 2019, IEEE COMPUT SOC CONF, P1544, DOI 10.1109/CVPRW.2019.00196
   Martinel N, 2015, LECT NOTES COMPUT SC, V8927, P191, DOI 10.1007/978-3-319-16199-0_14
   Prates R, 2019, J VIS COMMUN IMAGE R, V58, P304, DOI 10.1016/j.jvcir.2018.12.003
   Ranzato MA., 2007, CVPR, DOI [10.1109/cvpr.2007.383157, 10.1109/CVPR.2007.383157]
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Si JL, 2018, PROC CVPR IEEE, P5363, DOI 10.1109/CVPR.2018.00562
   Wang JT, 2018, NEUROCOMPUTING, V316, P166, DOI 10.1016/j.neucom.2018.07.063
   Wang TQ, 2014, LECT NOTES COMPUT SC, V8692, P688, DOI 10.1007/978-3-319-10593-2_45
   Wang ZY, 2020, NEUROCOMPUTING, V388, P309, DOI 10.1016/j.neucom.2019.12.120
   Wei D, 2021, IEEE ACCESS, V9, P34845, DOI 10.1109/ACCESS.2021.3062967
   Xu SJ, 2017, IEEE I CONF COMP VIS, P4743, DOI 10.1109/ICCV.2017.507
   Yan YC, 2020, PROC CVPR IEEE, P2896, DOI 10.1109/CVPR42600.2020.00297
   Ye M, 2019, IEEE T IMAGE PROCESS, V28, P2976, DOI 10.1109/TIP.2019.2893066
   Zhang JF, 2018, PROC CVPR IEEE, P6781, DOI 10.1109/CVPR.2018.00709
   Zhang W, 2019, IEEE T NEUR NET LEAR, V30, P3847, DOI 10.1109/TNNLS.2019.2899588
   Zheng F, 2019, PROC CVPR IEEE, P8506, DOI 10.1109/CVPR.2019.00871
   Zheng L, 2016, LECT NOTES COMPUT SC, V9910, P868, DOI 10.1007/978-3-319-46466-4_52
   Zheng ZX, 2019, NEUROCOMPUTING, V358, P446, DOI 10.1016/j.neucom.2019.05.058
   Zhou Z, 2017, PROC CVPR IEEE, P6776, DOI 10.1109/CVPR.2017.717
   Zhu XK, 2018, IEEE T CIRC SYST VID, V28, P2599, DOI 10.1109/TCSVT.2017.2718036
   Zhu XK, 2018, IEEE T IMAGE PROCESS, V27, P5683, DOI 10.1109/TIP.2018.2861366
NR 42
TC 4
Z9 4
U1 0
U2 13
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2023
VL 39
IS 1
BP 501
EP 517
DI 10.1007/s00371-021-02344-7
EA JAN 2022
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA F6UJ1
UT WOS:000743848900001
DA 2024-07-18
ER

PT J
AU Yang, X
   Zhu, YT
   Guo, YQ
   Zhou, DK
AF Yang, Xin
   Zhu, Yitian
   Guo, Yingqing
   Zhou, Dake
TI An image super-resolution network based on multi-scale convolution
   fusion
SO VISUAL COMPUTER
LA English
DT Article
DE Super-resolution; Deep learning; Convolutional neural network (CNN);
   Multi-branch structure; Information fusion
AB In this paper, we propose a multi-scale convolution adaptive fusion super-resolution reconstruction network. Firstly, the input is passed through three convolution kernels of different sizes, and then the results are added and fused. Then, after pooling and full connection, the output results of the convolution layer with different sizes are weighted and added by the Softmax weighting mechanism to get the fusion feature map. Since the weights of the different branches with different convolution kernel sizes can be adaptively changed with the input information, the SR reconstruction is effectively improved. The detailed comparative experiments on the public datasets show that the SR reconstruction effect of our model is better than that of some state-of-the-art networks in objective criteria PSNR, SSIM, and subjective visual effect.
C1 [Yang, Xin; Zhu, Yitian; Guo, Yingqing; Zhou, Dake] Nanjing Univ Aeronaut & Astronaut, Coll Automat Engn, Nanjing 210016, Peoples R China.
C3 Nanjing University of Aeronautics & Astronautics
RP Yang, X (corresponding author), Nanjing Univ Aeronaut & Astronaut, Coll Automat Engn, Nanjing 210016, Peoples R China.
EM yangxin@nuaa.edu.cn; dakzhou@nuaa.edu.cn
RI zhong, jing/KBP-7800-2024; WANG, YONGJIA/KFQ-4823-2024
OI Yang, Xin/0000-0003-0445-6497
FU National Natural Science Foundation of China [61573182]; Fundamental
   Research Funds for the Central Universities [NS2020025]
FX This research was supported by the National Natural Science Foundation
   of China (61573182), and by the Fundamental Research Funds for the
   Central Universities (NS2020025).
CR Agustsson E, 2017, IEEE COMPUT SOC CONF, P1122, DOI 10.1109/CVPRW.2017.150
   Aly HA, 2005, IEEE T IMAGE PROCESS, V14, P1647, DOI 10.1109/TIP.2005.851684
   Bevilacqua M, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.135
   Blau Y, 2019, LECT NOTES COMPUT SC, V11133, P334, DOI 10.1007/978-3-030-11021-5_21
   Chan TM, 2009, PATTERN RECOGN LETT, V30, P494, DOI 10.1016/j.patrec.2008.11.008
   Cheng Ma, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7766, DOI 10.1109/CVPR42600.2020.00779
   Dai SY, 2007, PROC CVPR IEEE, P445
   Dong C, 2016, LECT NOTES COMPUT SC, V9906, P391, DOI 10.1007/978-3-319-46475-6_25
   Dong C, 2014, LECT NOTES COMPUT SC, V8692, P184, DOI 10.1007/978-3-319-10593-2_13
   Fujimoto A, 2016, PROCEEDINGS OF THE 1ST INTERNATIONAL WORKSHOP ON COMICS ANALYSIS, PROCESSING AND UNDERSTANDING (MANPU 2016), DOI 10.1145/3011549.3011551
   Goodman J., 1968, INTRO FOURIER OPTICS
   Haris M, 2018, PROC CVPR IEEE, P1664, DOI 10.1109/CVPR.2018.00179
   HARRIS JL, 1964, J OPT SOC AM, V54, P931, DOI 10.1364/JOSA.54.000931
   He H, 2011, PROC CVPR IEEE, P449, DOI 10.1109/CVPR.2011.5995713
   Hu Y, 2018, BLOCKCHAIN BASED SMA
   Huang JB, 2015, PROC CVPR IEEE, P5197, DOI 10.1109/CVPR.2015.7299156
   Hui Z, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2024, DOI 10.1145/3343031.3351084
   Hui Z, 2018, PROC CVPR IEEE, P723, DOI 10.1109/CVPR.2018.00082
   Jie Liu, 2020, Proceedings of the 16th European Conference on Computer Vision - ECCV 2020 Workshops. Lecture Notes in Computer Science (LNCS 12537), P41, DOI 10.1007/978-3-030-67070-2_2
   Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.182, 10.1109/CVPR.2016.181]
   Kuanar S, 2022, VISUAL COMPUT, V38, P1121, DOI 10.1007/s00371-021-02071-z
   Kuanar S, 2018, PICT COD SYMP, P164, DOI 10.1109/PCS.2018.8456278
   Lai WS, 2019, IEEE T PATTERN ANAL, V41, P2599, DOI 10.1109/TPAMI.2018.2865304
   Lai WS, 2017, PROC CVPR IEEE, P5835, DOI 10.1109/CVPR.2017.618
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Li X, 2019, PROC CVPR IEEE, P510, DOI 10.1109/CVPR.2019.00060
   Li Z, 2019, PROC CVPR IEEE, P3862, DOI 10.1109/CVPR.2019.00399
   Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Shan Q, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409106
   Tai Y, 2017, IEEE I CONF COMP VIS, P4549, DOI 10.1109/ICCV.2017.486
   Tai Y, 2017, PROC CVPR IEEE, P2790, DOI 10.1109/CVPR.2017.298
   Tsai R.Y., 1984, Proc. Inst Elect Eng, V1, P317
   Yang X, 2021, J VIS COMMUN IMAGE R, V75, DOI 10.1016/j.jvcir.2021.103019
   Yang X, 2021, SIGNAL IMAGE VIDEO P, V15, P1397, DOI 10.1007/s11760-021-01870-0
   Yang X, 2022, VISUAL COMPUT, V38, P405, DOI 10.1007/s00371-020-02022-0
   Zeyde R., 2012, INT C CURV SURF, P711, DOI DOI 10.1007/978-3-642-27413-8_47
   Zhang KB, 2012, IEEE T IMAGE PROCESS, V21, P4544, DOI 10.1109/TIP.2012.2208977
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI 10.1007/978-3-030-01234-2_18
   Zhang YL, 2018, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR.2018.00262
NR 41
TC 10
Z9 10
U1 4
U2 19
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2022
VL 38
IS 12
BP 4307
EP 4317
DI 10.1007/s00371-021-02297-x
EA OCT 2021
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 7B7SH
UT WOS:000702984800001
DA 2024-07-18
ER

PT J
AU Altinsoy, E
   Yang, J
   Tu, EM
AF Altinsoy, Emrecan
   Yang, Jie
   Tu, Enmei
TI An improved denoising of G-banding chromosome images using cascaded CNN
   and binary classification network
SO VISUAL COMPUTER
LA English
DT Article
DE G-banding chromosome image; Denoising; U-net; Convolutional neural
   network; Binary classification
ID SEGMENTATION
AB Background Chromosome analysis plays an important role in detecting genetic disorders. However, it is time-consuming when it is done manually. The first step for an automated solution is removing the background noise in the chromosome images. Denoising is studied by many researchers; however, it is still a challenging task due to contrast issues, blotches, and non-chromosome objects. Methods In this paper, we proposed a cascaded neural network architecture for denoising G-banding chromosomes images. The proposed method consists of two steps. The first step is the initial segmentation network which combines the capabilities of U-net and residual units. The second step is the classification block, which is implemented in order to automate the denoising process and reduce the pixel losses on the chromosomes. Results The results showed that the proposed segmentation network achieves a higher dice score compared to state-of-the-art semantic segmentation neural networks, and the classification block greatly reduces the pixel losses on the chromosomes. We tested the proposed denoising method on 84 G-banding chromosome images and achieved a 98.74% dice score. Conclusion Our automated denoising method outperformed the methods presented in previous studies and state-of-the-art methods. It can help cytogeneticists with repetitive work and provide them more accurate chromosomes for further evaluation.
C1 [Altinsoy, Emrecan; Yang, Jie; Tu, Enmei] Shanghai Jiao Tong Univ, Inst Image Proc & Pattern Recognit, Shanghai, Peoples R China.
   [Yang, Jie] Shanghai Jiao Tong Univ, Inst Med Robot, Shanghai, Peoples R China.
C3 Shanghai Jiao Tong University; Shanghai Jiao Tong University
RP Yang, J; Tu, EM (corresponding author), Shanghai Jiao Tong Univ, Inst Image Proc & Pattern Recognit, Shanghai, Peoples R China.; Yang, J (corresponding author), Shanghai Jiao Tong Univ, Inst Med Robot, Shanghai, Peoples R China.
EM emrecanaltinsoy@yahoo.com.tr; jieyang@sjtu.edu.cn; tuen@sjtu.edu.cn
RI Altinsoy, Emrecan/AHH-9454-2022; Yang, Jie/JCD-9867-2023
OI ALTINSOY, Emrecan/0000-0001-7598-1710
FU Committee of Science and Technology, Shanghai, China [19510711200]
FX The authors thank the Center for Medical Genetics, School of Life
   Sciences, Central South University, and Diagens Hangzhou for providing
   the chromosome images and manual karyotypes. This research was supported
   by the Committee of Science and Technology, Shanghai, China (no.
   19510711200). The authors are grateful to the anonymous reviewers for
   their helpful comments.
CR Altinsoy E, 2020, IET IMAGE PROCESS, V14, P1920, DOI 10.1049/iet-ipr.2019.1104
   Altinsoy E, 2019, LECT NOTES ARTIF INT, V11509, P117, DOI 10.1007/978-3-030-20915-5_11
   Andrade MFS, 2020, MACH VISION APPL, V31, DOI 10.1007/s00138-020-01115-z
   Andrade MFS, 2018, 2018 7TH BRAZILIAN CONFERENCE ON INTELLIGENT SYSTEMS (BRACIS), P290, DOI 10.1109/BRACIS.2018.00057
   Arachchige AS, 2010, IEEE IMAGE PROC, P3613, DOI 10.1109/ICIP.2010.5652017
   Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615
   Bai H, 2020, IEEE ACCESS, V8, P178563, DOI 10.1109/ACCESS.2020.3026483
   Bradski G, 2000, DR DOBBS J, V25, P120
   Cao HB, 2011, I S BIOMED IMAGING, P1442, DOI 10.1109/ISBI.2011.5872671
   Chen L.-C., 2017, ARXIV E PRINTS ARXIV
   Dougherty AW, 2017, IEEE IJCNN, P198, DOI 10.1109/IJCNN.2017.7965855
   Drozdzal M, 2016, LECT NOTES COMPUT SC, V10008, P179, DOI 10.1007/978-3-319-46976-8_19
   Graham J., 1994, AUTOMATIC KARYOTYPE, P141
   Grisan E., 2009, 11th International Congress of the IUPESM. World Congress on Medical Physics and Biomedical Engineering. Image Processing, Biosignal Processing, Modelling and Simulation, Biomechanics, P748, DOI 10.1007/978-3-642-03882-2_199
   Grisan E, 2009, IEEE T INF TECHNOL B, V13, P575, DOI 10.1109/TITB.2009.2014464
   Gu ZW, 2019, IEEE T MED IMAGING, V38, P2281, DOI 10.1109/TMI.2019.2903562
   Guimaraes LV, 2003, P ANN INT IEEE EMBS, V25, P941, DOI 10.1109/IEMBS.2003.1279921
   He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38
   Jahani S, 2011, 2011 7 IR C MACH VIS, P1, DOI DOI 10.1109/IRANIANMVIP.2011.6121574
   JI L, 1994, CYTOMETRY, V17, P196, DOI 10.1002/cyto.990170303
   JI LA, 1989, PATTERN RECOGN, V22, P519, DOI 10.1016/0031-3203(89)90021-6
   KAPUR JN, 1985, COMPUT VISION GRAPH, V29, P273, DOI 10.1016/0734-189X(85)90125-2
   Karvelis P.S., 2005, 3 EUR MED BIOL ENG C, V11, P1727
   Kingma D. P., 2014, arXiv
   Lerner B, 1998, IEEE T SIGNAL PROCES, V46, P2841, DOI 10.1109/78.720391
   Lerner B, 1998, IEEE T SYST MAN CY B, V28, P544, DOI 10.1109/3477.704293
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Mina Shaily, 2014, Psychiatry J, V2014, P897493, DOI [10.1109/SPMB.2015.7405471, 10.1155/2014/897493]
   Mona A.A., 2015, SIMPLE APPROACH SEGM, P3
   Oktay O., 2018, ARXIV, DOI DOI 10.48550/ARXIV.1804.03999
   OTSU N, 1979, IEEE T SYST MAN CYB, V9, P62, DOI 10.1109/TSMC.1979.4310076
   Paszke A, 2019, ADV NEUR IN, V32
   Poletti E, 2012, COMPUT METH PROG BIO, V108, P679, DOI 10.1016/j.cmpb.2011.12.003
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Ruder S., 2016, ARXIV
   Shen X, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-38614-7
   Soumya D., 2013, INT J SCI ENG RES, V4, P937
   Srisang W., 2006, Walailak J Sci Technol, V3, P181
   Stanley RJ, 1998, IEEE T MED IMAGING, V17, P451, DOI 10.1109/42.712134
   SUGAPRIYAA T, 2018, INDIAN J SCI TECHNOL, V11, pNI125, DOI DOI 10.17485/ijst/2018/v11i18/123037
   Tanvi T., 2014, Int J Comput Appl, V95, P29, DOI DOI 10.5120/16560-4861
   Uhlmann V, 2016, I S BIOMED IMAGING, P395, DOI 10.1109/ISBI.2016.7493291
   Wayalun P, 2012, 2012 8TH INTERNATIONAL CONFERENCE ON COMPUTING AND NETWORKING TECHNOLOGY (ICCNT, INC, ICCIS AND ICMIC), P163
   Yan FX, 2005, PATTERN RECOGN LETT, V26, P1183, DOI 10.1016/j.patrec.2004.11.003
   Yilmaz IC, 2018, INT CONF SYST INFORM, P944, DOI 10.1109/ICSAI.2018.8599328
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zhou Zongwei, 2018, Deep Learn Med Image Anal Multimodal Learn Clin Decis Support (2018), V11045, P3, DOI [10.1007/978-3-030-00889-5_1, 10.1007/978-3-030-00689-1_1]
   Zou KH, 2004, ACAD RADIOL, V11, P178, DOI 10.1016/S1076-6332(03)00671-8
NR 48
TC 5
Z9 6
U1 0
U2 11
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2022
VL 38
IS 6
BP 2139
EP 2152
DI 10.1007/s00371-021-02273-5
EA AUG 2021
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 1C6YN
UT WOS:000685399300001
DA 2024-07-18
ER

PT J
AU Chen, H
   Han, Q
   Li, Q
   Tong, XJ
AF Chen, Hao
   Han, Qi
   Li, Qiong
   Tong, Xiaojun
TI Digital image manipulation detection with weak feature stream
SO VISUAL COMPUTER
LA English
DT Article
DE Image forensics; Object detection; Faster R-CNN; Deep residual network
ID STEGANALYSIS; LOCALIZATION; NETWORK
AB With the rapid development of deep neural network, two-stream Faster R-CNN network has been applied to tampering detection field and achieved good detection results. However, the noise stream generation of the two-stream Faster R-CNN needs to be manually selected, and the feature extraction network is the same as RGB stream, so it doesn't maximize the program to play the advantages of deep neural network. In this paper, the hand-crafted features of the Faster R-CNN noise stream generation layer are cancelled and the two-layer convolution network is used to directly fit the weak features generated by image tampering. At the same time, according to the characteristics of image tampering detection, a weak feature extraction network based on multi-scale residual network is established to extract weak feature signals of image tampering. The network can suppress the natural features of the image and preserve the weak feature signals. In the network, the multi-level residual layers are used to extract RoI features, which makes full use of the feature layer informations with higher resolution. The experimental results show that the performance of the Faster R-CNN network with weak feature stream has been improved significantly in F1 score and localization of the tampering region.
C1 [Chen, Hao; Han, Qi; Li, Qiong; Tong, Xiaojun] Harbin Inst Technol, Sch Comp Sci & Technol, Harbin 150001, Heilongjiang, Peoples R China.
C3 Harbin Institute of Technology
RP Han, Q (corresponding author), Harbin Inst Technol, Sch Comp Sci & Technol, Harbin 150001, Heilongjiang, Peoples R China.
EM chenhaoxv@126.com; qi.han@hit.edu.cn; qiong.li@hit.edu.cn;
   tong_xiaojun@163.com
FU National Natural Science Foundation of China [61771168]
FX This work was supported by the National Natural Science Foundation of
   China [Grant Numbers 61771168]. The authors would like to thank the
   Institute of Information Countermeasures Technology providing deep
   learning servers.
CR Bappy JH, 2017, IEEE I CONF COMP VIS, P4980, DOI 10.1109/ICCV.2017.532
   Bayar B, 2018, IEEE T INF FOREN SEC, V13, P2691, DOI 10.1109/TIFS.2018.2825953
   Bianchi T, 2011, INT CONF ACOUST SPEE, P2444
   Boroumand M, 2019, IEEE T INF FOREN SEC, V14, P1181, DOI 10.1109/TIFS.2018.2871749
   Chen JS, 2015, IEEE SIGNAL PROC LET, V22, P1849, DOI 10.1109/LSP.2015.2438008
   Cozzolino D, 2015, IEEE INT WORKS INFOR
   Dong J., 2010, Casia image tampering detection evaluation database
   Ferrara P, 2012, IEEE T INF FOREN SEC, V7, P1566, DOI 10.1109/TIFS.2012.2202227
   Fridrich J, 2012, IEEE T INF FOREN SEC, V7, P868, DOI 10.1109/TIFS.2012.2190402
   Goljan M, 2015, PROC SPIE, V9409, DOI 10.1117/12.2078399
   Guclu O, 2020, VISUAL COMPUT, V36, P1271, DOI 10.1007/s00371-019-01720-8
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Jing Dong, 2013, 2013 IEEE China Summit and International Conference on Signal and Information Processing (ChinaSIP), P422, DOI 10.1109/ChinaSIP.2013.6625374
   Joseph A, 2020, VISUAL COMPUT, V36, P529, DOI 10.1007/s00371-019-01628-3
   Li WH, 2009, SIGNAL PROCESS, V89, P1821, DOI 10.1016/j.sigpro.2009.03.025
   Liao X, 2020, IEEE J-STSP, V14, P955, DOI 10.1109/JSTSP.2020.3002391
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Mahdian B, 2009, IMAGE VISION COMPUT, V27, P1497, DOI 10.1016/j.imavis.2009.02.001
   Margolin R, 2013, VISUAL COMPUT, V29, P381, DOI 10.1007/s00371-012-0740-x
   MARR D, 1980, PROC R SOC SER B-BIO, V207, P187, DOI 10.1098/rspb.1980.0020
   Peng L, 2021, MULTIMEDIA SYST, V27, P363, DOI 10.1007/s00530-020-00697-y
   Prewitt J. M., 1970, Picture processing and psychopictorics, V10, P15
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rota P, 2016, INT C PATT RECOG, P2503, DOI 10.1109/ICPR.2016.7900012
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Salloum R, 2018, J VIS COMMUN IMAGE R, V51, P201, DOI 10.1016/j.jvcir.2018.01.010
   Wei XY, 2019, SYMMETRY-BASEL, V11, DOI 10.3390/sym11101223
   Zhang L, 2019, VISUAL COMPUT, V35, P1091, DOI 10.1007/s00371-019-01685-8
   Zhong Z. Y., 2016, ARXIV160507314
   Zhou P, 2018, PROC CVPR IEEE, P1053, DOI 10.1109/CVPR.2018.00116
NR 30
TC 2
Z9 3
U1 0
U2 14
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2022
VL 38
IS 8
BP 2675
EP 2689
DI 10.1007/s00371-021-02146-x
EA AUG 2021
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 3B2OE
UT WOS:000682625900001
DA 2024-07-18
ER

PT J
AU Li, LD
   Qing, LB
   Wang, YC
   Su, J
   Cheng, YQ
   Peng, YH
AF Li, Lindong
   Qing, Linbo
   Wang, Yuchen
   Su, Jie
   Cheng, Yongqiang
   Peng, Yonghong
TI HF-SRGR: a new hybrid feature-driven social relation graph reasoning
   model
SO VISUAL COMPUTER
LA English
DT Article
DE Social relation recognition (SRR); Hybrid features; Graph neural
   networks (GNNs); Attention mechanism; Feature fusion
AB Social relations and interactions between persons form the foundation of human society. Effective recognition of social relationships has great potential for understanding and improving people's psychology and behaviors, e.g., mental health and activity analysis, and further improving social resilience. Existing work of social relation recognition (SRR) mainly focuses on exploiting two or three types of features to recognize social relations without considering the relations between features. In this paper, we proposed a new framework for extraction and fusion of the hybrid features, namely Social Relation Graph Reasoning model driven by Hybrid-Features (HF-SRGR). For the proposed method, a social relation graph was constructed first using relation and scene features as nodes. An attention mechanism was then designed to incorporate into graph neural networks (GNNs), generating inter-pair features and interactions between relation nodes and the scene node, respectively. Besides, the propagation of scene information further strengthens the rationality of interaction reasoning. Extensive experiments on PISC and PIPA datasets show that our proposed approach achieves better performance over the state-of-the-art methods in terms of accuracy.
C1 [Li, Lindong; Qing, Linbo; Wang, Yuchen; Su, Jie] Sichuan Univ, Coll Elect & Informat Engn, Chengdu 610065, Peoples R China.
   [Cheng, Yongqiang] Univ Hull, Dept Comp Sci & Technol, Kingston Upon Hull HU6 7RX, N Humberside, England.
   [Peng, Yonghong] Manchester Metropolitan Univ, Dept Comp & Math, Manchester M15 6BH, Lancs, England.
C3 Sichuan University; University of Hull; Manchester Metropolitan
   University
RP Qing, LB (corresponding author), Sichuan Univ, Coll Elect & Informat Engn, Chengdu 610065, Peoples R China.
EM qing_lb@scu.edu.cn
RI Peng, Yonghong/ABD-5633-2021; Cheng, Yongqiang/ABF-5994-2021; wang,
   yuchen/AED-4066-2022
OI Cheng, Yongqiang/0000-0001-7282-7638; 
FU National Nature Science Foundation of China [61871278]; Sichuan Science
   and Technology Program [2018HH0143]
FX This work was supported by the National Nature Science Foundation of
   China under Grant 61871278 and the Sichuan Science and Technology
   Program under Grant 2018HH0143.
CR BIDDLE BJ, 1986, ANNU REV SOCIOL, V12, P67, DOI 10.1146/annurev.so.12.080186.000435
   Boonstra TW, 2017, IEEE ENG MED BIO, P287, DOI 10.1109/EMBC.2017.8036818
   Bugental DB, 2000, PSYCHOL BULL, V126, P187, DOI 10.1037/0033-2909.126.2.187
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dibeklioglu H, 2013, IEEE I CONF COMP VIS, P1497, DOI 10.1109/ICCV.2013.189
   Fang RG, 2010, IEEE IMAGE PROC, P1577, DOI 10.1109/ICIP.2010.5652590
   Gao JJ, 2021, NEUROCOMPUTING, V456, P243, DOI 10.1016/j.neucom.2021.05.097
   GORI M, 2005, IEEE IJCNN, P729, DOI DOI 10.1109/IJCNN.2005.1555942
   Hammond DK, 2011, APPL COMPUT HARMON A, V30, P129, DOI 10.1016/j.acha.2010.04.005
   He K., 2016, PROC CVPR IEEE, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]
   He KM, 2020, IEEE T PATTERN ANAL, V42, P386, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   Jia Yangqing, 2014, ARXIV14085093, DOI [10.1145/2647868.2654889, DOI 10.1145/2647868.2654889]
   Jun SH, 2017, ICEC'17: PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON ELECTRONIC COMMERCE, DOI 10.1145/3154943.3154947
   Kawachi I, 2001, J URBAN HEALTH, V78, P458, DOI 10.1093/jurban/78.3.458
   Kipf TN, 2016, ARXIV
   Li JN, 2017, IEEE I CONF COMP VIS, P2669, DOI 10.1109/ICCV.2017.289
   Li WH, 2020, IEEE INT CON MULTI, DOI 10.1109/icme46284.2020.9102823
   Liu XC, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P907, DOI 10.1145/3394171.3413578
   Liu XC, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P338, DOI 10.1145/3343031.3350857
   Liu XC, 2019, PROC CVPR IEEE, P3561, DOI 10.1109/CVPR.2019.00368
   Lu JW, 2014, IEEE T PATTERN ANAL, V36, P331, DOI 10.1109/TPAMI.2013.134
   Morris C, 2019, AAAI CONF ARTIF INTE, P4602
   Platek SM, 2004, EVOL HUM BEHAV, V25, P394, DOI 10.1016/j.evolhumbehav.2004.08.007
   Qin Y, 2020, VISUAL COMPUT, V36, P621, DOI 10.1007/s00371-019-01644-3
   Ramanathan V, 2013, PROC CVPR IEEE, P2475, DOI 10.1109/CVPR.2013.320
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Scarselli F, 2009, IEEE T NEURAL NETWOR, V20, P61, DOI 10.1109/TNN.2008.2005605
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sui YL, 2013, INT SYM CODE GENER, P1
   Sun QR, 2017, PROC CVPR IEEE, P435, DOI 10.1109/CVPR.2017.54
   Velic kovic P., 2018, INT C LEARN REPR ICL
   Wang G, 2010, LECT NOTES COMPUT SC, V6315, P169, DOI 10.1007/978-3-642-15555-0_13
   Wang MY, 2020, PATTERN RECOGN LETT, V138, P410, DOI 10.1016/j.patrec.2020.08.005
   Wang MY, 2020, PATTERN RECOGN, V105, DOI 10.1016/j.patog.2020.10732
   Wang ZX, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1021
   Xia SY, 2012, IEEE T MULTIMEDIA, V14, P1046, DOI 10.1109/TMM.2012.2187436
   Yoo H, 2019, PATTERN RECOGN, V93, P498, DOI 10.1016/j.patcog.2019.05.001
   Zhang M, 2019, IEEE INT CON MULTI, P1618, DOI 10.1109/ICME.2019.00279
   Zhang N, 2015, PROC CVPR IEEE, P4804, DOI 10.1109/CVPR.2015.7299113
   Zhang W, 2020, VISUAL COMPUT, V36, P2433, DOI 10.1007/s00371-020-01955-w
   Zhang ZP, 2015, IEEE I CONF COMP VIS, P3631, DOI 10.1109/ICCV.2015.414
   Zhou BL, 2018, IEEE T PATTERN ANAL, V40, P1452, DOI 10.1109/TPAMI.2017.2723009
   Zhou Xiuzhuang, 2012, P 20 ACM INT C MULT, P725, DOI [DOI 10.1145/2393347, DOI 10.1145/2393347.2396297]
NR 43
TC 7
Z9 7
U1 1
U2 18
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2022
VL 38
IS 11
BP 3979
EP 3992
DI 10.1007/s00371-021-02244-w
EA JUL 2021
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 5X0BS
UT WOS:000674546600001
DA 2024-07-18
ER

PT J
AU Zhu, ZY
   Mao, XY
AF Zhu, Zhenyang
   Mao, Xiaoyang
TI Image recoloring for color vision deficiency compensation: a survey
SO VISUAL COMPUTER
LA English
DT Article
DE Color vision deficiency; Recoloring; Evaluation metrics
ID RED-GREEN DICHROMATS; CONTRAST ENHANCEMENT; NATURALNESS; TRANSFORMATION;
   SIMULATION; QUALITY; SYSTEM
AB People with color vision deficiency (CVD) have a reduced capability to discriminate different colors. This impairment can cause inconveniences in the individuals' daily lives and may even expose them to dangerous situations, such as failing to read traffic signals. CVD affects approximately 200 million people worldwide. In order to compensate for CVD, a significant number of image recoloring studies have been proposed. In this survey, we briefly review the representative existing recoloring methods and categorize them according to their methodological characteristics. Concurrently, we summarize the evaluation metrics, both subjective and quantitative, introduced in the existing studies and compare the state-of-the-art studies using the experimental evaluation results with the quantitative metrics.
C1 [Zhu, Zhenyang] Univ Yamanashi, Grad Sch Engn, Kofu, Yamanashi 4008511, Japan.
   [Mao, Xiaoyang] Univ Yamanashi, Dept Comp Sci & Engn, Kofu, Yamanashi 4008511, Japan.
   [Mao, Xiaoyang] Hangzhou Dianzi Univ, Sch Comp Sci, 1156 2nd St, Hangzhou 310018, Peoples R China.
C3 University of Yamanashi; University of Yamanashi; Hangzhou Dianzi
   University
RP Mao, XY (corresponding author), Univ Yamanashi, Dept Comp Sci & Engn, Kofu, Yamanashi 4008511, Japan.; Mao, XY (corresponding author), Hangzhou Dianzi Univ, Sch Comp Sci, 1156 2nd St, Hangzhou 310018, Peoples R China.
EM zhuyamanashi2016@gmail.com; mao@yamanashi.ac.jp
RI Zhu, Zhenyang/AAZ-6197-2021
OI Zhu, Zhenyang/0000-0003-1023-3193
FU JSPS [17H00738, 20J15406, 20K20408]; Grants-in-Aid for Scientific
   Research [20J15406, 20K20408] Funding Source: KAKEN
FX This work is supported by JSPS Grants-in-Aid for Scientific Research
   (Grant Nos. 17H00738, 20J15406, and 20K20408).
CR Anagnostopoulos C.-N., 2007, P 5 INT C COMP VIS S
   Brettel H, 1997, J OPT SOC AM A, V14, P2647, DOI 10.1364/JOSAA.14.002647
   Chen WF, 2011, IEEE T VIS COMPUT GR, V17, P2144, DOI 10.1109/TVCG.2011.164
   Chen YC, 2011, ETRI J, V33, P71, DOI 10.4218/etrij.11.1510.0009
   Deng YH, 2007, LECT NOTES COMPUT SC, V4681, P1018
   Flatla D.R., 2013, P SIGCHI C HUM FACT, P2069, DOI DOI 10.1145/2470654.2481283
   Flatla D.R., 2012, Proceedings. of Human factors in computing systems, P2297, DOI DOI 10.1145/2207676.2208388
   Gooch AA, 2005, ACM T GRAPHIC, V24, P634, DOI 10.1145/1073204.1073241
   GRAHAM CH, 1959, P NATL ACAD SCI USA, V45, P96, DOI 10.1073/pnas.45.1.96
   Hassan MF, 2019, MULTIDIM SYST SIGN P, V30, P1975, DOI 10.1007/s11045-019-00638-7
   Hassan MF, 2017, SIGNAL PROCESS-IMAGE, V57, P126, DOI 10.1016/j.image.2017.05.011
   Huang CR, 2010, LECT NOTES COMPUT SC, V6298, P637, DOI 10.1007/978-3-642-15696-0_59
   Huang HB, 2007, IEEE SIGNAL PROC LET, V14, P711, DOI 10.1109/LSP.2007.898333
   Huang J.-B., 2008, P WORKSH ENH COL REP
   Huang JB, 2009, INT CONF ACOUST SPEE, P1161, DOI 10.1109/ICASSP.2009.4959795
   Ichikawa M, 2003, LECT NOTES COMPUT SC, V2724, P2134
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jefferson L, 2007, CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1 AND 2, P1535
   Jefferson Luke., 2006, P 8 INT ACM SIGACCES, P40, DOI DOI 10.1145/1168987.1168996
   JUDD DB, 1948, J RES NAT BUR STAND, V41, P247, DOI 10.6028/jres.041.027
   JUDD DB, 1966, P NATL ACAD SCI USA, V55, P1313, DOI 10.1073/pnas.55.6.1313
   Kojima Takanori, 2014, Universal Access in Human-Computer Interaction. Universal Access to Information and Knowledge. 8th International Conference, UAHCI 2014, Held as Part of HCI International 2014. Proceedings: LNCS 8514, P121, DOI 10.1007/978-3-319-07440-5_12
   Kovalev V, 2005, LECT NOTES ARTIF INT, V3587, P456
   Kovalev VA, 2004, INT C PATT RECOG, P943, DOI 10.1109/ICPR.2004.1334414
   Kuhn GR, 2008, IEEE T VIS COMPUT GR, V14, P1747, DOI 10.1109/TVCG.2008.112
   Kuhn GR, 2008, VISUAL COMPUT, V24, P505, DOI 10.1007/s00371-008-0231-2
   Laccarino G., 2006, WWW, P919
   Lai CL, 2009, I SYMP CONSUM ELECTR, P81
   Langlotz T, 2018, PROCEEDINGS OF THE 2018 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2018), DOI 10.1145/3173574.3173964
   Lee J, 2011, INTEGR COMPUT-AID E, V18, P29, DOI 10.3233/ICA-2011-0356
   Li HS, 2020, MULTIMED TOOLS APPL, V79, P27583, DOI 10.1007/s11042-020-09299-2
   Liu QG, 2019, VISUAL COMPUT, V35, P205, DOI 10.1007/s00371-017-1464-8
   Ma Y, 2009, INFORM SCIENCES, V179, P830, DOI 10.1016/j.ins.2008.11.010
   Machado GM, 2010, COMPUT GRAPH FORUM, V29, P933, DOI 10.1111/j.1467-8659.2009.01701.x
   Machado GM, 2009, IEEE T VIS COMPUT GR, V15, P1291, DOI 10.1109/TVCG.2009.113
   Milic N, 2015, J IMAGING SCI TECHN, V59, DOI 10.2352/J.ImagingSci.Technol.2015.59.1.010504
   Mochizuki Rika, 2008, CGIV 2008/MCS'08. 4th European Conference on Colour in Graphics, Imaging and Vision. 10th International Symposium on Multispectral Colour Science, P208
   Mollon J., 2000, CAMBRIDGE COLOUR TES
   Nakauchi S, 2008, COLOR RES APPL, V33, P203, DOI 10.1002/col.20404
   Nuñez JR, 2018, PLOS ONE, V13, DOI 10.1371/journal.pone.0199239
   Poret S, 2009, IEEE TIC-STH 09: 2009 IEEE TORONTO INTERNATIONAL CONFERENCE: SCIENCE AND TECHNOLOGY FOR HUMANITY, P539, DOI 10.1109/TIC-STH.2009.5444442
   Rasche K, 2005, COMPUT GRAPH FORUM, V24, P423, DOI 10.1111/j.1467-8659.2005.00867.x
   Rasche K, 2005, IEEE COMPUT GRAPH, V25, P22, DOI 10.1109/MCG.2005.54
   REITNER A, 1991, NATURE, V352, P798, DOI 10.1038/352798a0
   Ribeiro M, 2019, ACM COMPUT SURV, V52, DOI 10.1145/3329118
   Ruminski J, 2010, C HUM SYST INTERACT, P634, DOI 10.1109/HSI.2010.5514503
   Sharpe L.T., 1999, Color Vision: From Genes to Perception
   Shen WY, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925878
   Shen XM, 2021, IET IMAGE PROCESS, V15, P983, DOI 10.1049/ipr2.12079
   Siew-Li Ching, 2010, 2nd International Conference on Computer Technology and Development (ICCTD 2010), P255, DOI 10.1109/ICCTD.2010.5645874
   Tang Y, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10072381
   Viénot F, 1999, COLOR RES APPL, V24, P243, DOI 10.1002/(SICI)1520-6378(199908)24:4<243::AID-COL5>3.0.CO;2-3
   Wakita Ken., 2005, Assets '05: Proceedings of the 7th international ACM SIGACCESS conference on Computers and accessibility, P158, DOI [10.1145/1090785.1090815, DOI 10.1145/1090785.1090815]
   Wang M., 2009, P 17 ACM INT C MULT
   Wang M, 2010, ACM T INTEL SYST TEC, V1, DOI 10.1145/1858948.1858956
   Wang XY, 2021, COMPUT GRAPH-UK, V98, P19, DOI 10.1016/j.cag.2021.04.027
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wong A, 2008, CAN CON EL COMP EN, P1937
   Woo S, 2018, CURR OPT PHOTONICS, V2, P101, DOI 10.3807/COPP.2018.2.1.101
   Xu L, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024208
   Yang S, 2003, IEEE IMAGE PROC, P453
   Yang S, 2008, EURASIP J IMAGE VIDE, DOI 10.1155/2008/487618
   Zhang L, 2011, IEEE T IMAGE PROCESS, V20, P2378, DOI 10.1109/TIP.2011.2109730
   Zhang XL, 2018, VISUAL COMPUT, V34, P1099, DOI 10.1007/s00371-018-1524-8
   Zhu Z., 2021, IEEE T MULTIMED
   Zhu ZY, 2019, SIGNAL PROCESS-IMAGE, V76, P68, DOI 10.1016/j.image.2019.04.004
   Zhu Z, 2019, VISUAL COMPUT, V35, P1053, DOI 10.1007/s00371-019-01689-4
NR 67
TC 8
Z9 8
U1 3
U2 24
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2021
VL 37
IS 12
SI SI
BP 2999
EP 3018
DI 10.1007/s00371-021-02240-0
EA JUL 2021
PG 20
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP1DP
UT WOS:000673536600004
OA hybrid
DA 2024-07-18
ER

PT J
AU Peichang, OY
   Chung, KW
   Bailey, D
   Nicolas, A
   Gdawiec, K
AF Ouyang, Peichang
   Chung, Kwok Wai
   Bailey, David
   Nicolas, Alain
   Gdawiec, Krzysztof
TI Generation of advanced Escher-like spiral tessellations
SO VISUAL COMPUTER
LA English
DT Article
DE Symmetry group; Spiral; Escher art; Conformal mapping; Wallpaper group
ID AUTOMATIC-GENERATION; CHAOTIC ATTRACTORS; COLOR SYMMETRY; PATTERNS
AB In this paper, using both hand-drawn and computer-drawn graphics, we establish a method to generate advanced Escher-like spiral tessellations. We first give a way to achieve simple spiral tilings of cyclic symmetry. Then, we introduce several conformal mappings to generate three derived spiral tilings. To obtain Escher-like tessellations on the generated tilings, given pre-designed wallpaper motifs, we specify the tessellations' implementation details. Finally, we exhibit a rich gallery of the generated Escher-like tessellations. According to the proposed method, one can produce a great variety of exotic Escher-like tessellations that have both good aesthetic value and commercial potential.
C1 [Ouyang, Peichang] Guangxi Univ Sci & Technol, Sch Sci, Liuzhou, Peoples R China.
   [Ouyang, Peichang] Jinggangshan Univ, Sch Math & Phys, Jian, Jiangxi, Peoples R China.
   [Chung, Kwok Wai] City Univ Hong Kong, Dept Math, Kowloon, Hong Kong, Peoples R China.
   [Bailey, David] 17 Larden Ave, Grimsby DN33 3HU, North East Linc, England.
   [Nicolas, Alain] Nicolas Tessellat, 15 Rue Georges Auric, F-91480 Quincy Sous Senart, France.
   [Gdawiec, Krzysztof] Univ Silesia, Inst Comp Sci, Bedzinska 39, PL-41200 Sosnowiec, Poland.
C3 Guangxi University of Science & Technology; Jinggangshan University;
   City University of Hong Kong; University of Silesia in Katowice
RP Gdawiec, K (corresponding author), Univ Silesia, Inst Comp Sci, Bedzinska 39, PL-41200 Sosnowiec, Poland.
EM g_fcayang@163.com; makchung@cityu.edu.hk; davidbailey500@gmail.com;
   al.nicolas15@gmail.com; krzysztof.gdawiec@us.edu.pl
RI Gdawiec, Krzysztof/O-3587-2015
OI Gdawiec, Krzysztof/0000-0001-9434-9307; CHUNG, Kwok
   Wai/0000-0002-0153-8801; ouyang, peichang/0000-0003-0447-3190
FU Natural Science Foundation of China [11761039, 62062042]; Science and
   Technology Project of the Education Department of Jiangxi Province of
   China [GJJ190542]; Natural Science Foundation of Jiangxi Province of
   China [20202ACBL211001]; Undergraduate Teaching Reform Project of
   Guangxi Higher Education [2021JGA224]
FX This work was supported by the Natural Science Foundation of China (Nos.
   11761039, 62062042), Science and Technology Project of the Education
   Department of Jiangxi Province of China (No. GJJ190542), Natural Science
   Foundation of Jiangxi Province of China (No. 20202ACBL211001), and
   Undergraduate Teaching Reform Project of Guangxi Higher Education (No.
   2021JGA224).
CR [Anonymous], 44 MOST POPULAR ARTI
   [Anonymous], 2003, ISAMA BRIDGES C P
   [Anonymous], GREATEST NETHERLANDE
   [Anonymous], 1951, LIFE, V7, P8
   [Anonymous], 1954, TIME, V65, P68
   [Anonymous], 1951, TIME, V57, P50
   Carter NC, 1998, COMPUT GRAPH-UK, V22, P765, DOI 10.1016/S0097-8493(98)00097-1
   Champagne F., DEFORMING YOUR TESSE
   Chung KW, 1999, COMPUT GRAPH-UK, V23, P439, DOI 10.1016/S0097-8493(99)00050-3
   Crompton A., LIFELIKE TESSELATION
   DIXON R, 1992, LEONARDO, V25, P263, DOI 10.2307/1575848
   Ernst, 1995, MAGIC MIRROR MC ESCH
   Gdawiec, 2021, IN PRESS
   Grunbaum B., 1981, The Mathematical Gardner, P167
   Hardy G. H., 2012, MATHEMATICIANS APOLO, DOI [10.1017/CBO9781107295599, DOI 10.1017/CBO9781107295599]
   Howison Mark, 2009, Computer-Aided Design and Applications, V6, P737
   Kaplan, 2008, BRIDGES LEEUWARDEN M, P39
   Kaplan C.S., ESCHER SPIRAL TILING
   Kaplan CS, 2000, COMP GRAPH, P499, DOI 10.1145/344779.345022
   Klaassen B., 2017, MATH MAG, V90, P26
   Lalvani H., 1989, Visual Computer, V5, P180, DOI 10.1007/BF02153749
   Leys J., ESCHER TILINGS
   Lin SS, 2018, IEEE T VIS COMPUT GR, V24, P1089, DOI 10.1109/TVCG.2017.2660488
   Lu J, 2007, VISUAL COMPUT, V23, P445, DOI 10.1007/s00371-007-0116-9
   Ouyang PC, 2019, APPL MATH COMPUT, V347, P653, DOI 10.1016/j.amc.2018.09.052
   Ouyang PC, 2018, NONLINEAR DYNAM, V94, P261, DOI 10.1007/s11071-018-4357-0
   Pickover C. A., 1992, Visual Computer, V8, P233, DOI 10.1007/BF01900658
   Reiter CA, 1999, VISUAL COMPUT, V15, P211, DOI 10.1007/s003710050173
   Severin MF., 1951, STUDIO, V141, P50
   Shephard, 1979, MATH TEACH, V88, P50
   Sugihara K, 2009, J MATH ARTS, V3, P195, DOI 10.1080/17513470903185626
   Veugen T., TIS VEUGEN ART
   Voderburg, 1936, JAHRESBERICHT DTSCH, V46, P189
   von Gagern M, 2009, ELECTRON J COMB, V16
   Wang XC, 2013, IEEE COMPUT GRAPH, V33, P21, DOI 10.1109/MCG.2013.87
   Yen J., 2001, P 2001 S INT 3D GRAP, P95
NR 36
TC 5
Z9 5
U1 2
U2 10
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2022
VL 38
IS 11
BP 3923
EP 3935
DI 10.1007/s00371-021-02232-0
EA JUL 2021
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 5X0BS
UT WOS:000670881700001
OA hybrid
DA 2024-07-18
ER

PT J
AU Chudasama, V
   Upla, K
   Raja, K
   Ramachandra, R
   Busch, C
AF Chudasama, Vishal
   Upla, Kishor
   Raja, Kiran
   Ramachandra, Raghavendra
   Busch, Christoph
TI Compact and progressive network for enhanced single image
   super-resolution-<i>ComPrESRNet</i>
SO VISUAL COMPUTER
LA English
DT Article
DE Super-Resolution; Convolutional neural networks; Compact Network;
   Progressive Structure
ID SUPERRESOLUTION
AB The use of deep convolutional neural networks (CNNs) for single image super-resolution (SISR) in the recent years has led to numerous vision-based applications. Complementing the growing interest in the computer vision community embracing such networks, there is an unmet demand of reduced computational complexity. Despite being state-of-the-art for SISR tasks, CNN-based models need to be compact and efficient to account for applications running on low-cost deployment devices that have limited computation resources. While it is common to note that many state-of-the-art SISR approaches stack large number of convolutional layers in order to enhance their SR performance, there is proportional increase in the computational complexity. We propose a computationally efficient, compact and enhanced progressive network for SISR task which we hereafter refer as ComPrESRNet. The architectural changes employs a progressive learning strategy with a novel design of Enhanced densely connected parallel residual network (EDPRN) which simultaneously extracts rich features from the low-resolution (LR) observation while reducing the total number of parameters to 2.61M making it compact in nature that is suitable for low computational platform. The novelty of the proposed model stems from the inclusion of (i) densely connected ResBlock to extract rich features in the LR observation, (ii) extended global residual learning approach which stabilizes the training process effectively and also helps network to further improve the SR performance and (iii) progressive upscaling module which can generate an SR image of size x4 and x8 of original LR image. The robustness of the proposed method is further demonstrated on four different benchmark testing datasets consisting of natural scenes and urban landscape to exemplify the different applications. The superior performance over other state-of-the-art methods is also illustrated in this work for an upscaling factor x4 and x8 despite the lower computational complexity.
C1 [Chudasama, Vishal; Upla, Kishor] Sardar Vallabhbhai Natl Inst Technol, Surat, India.
   [Raja, Kiran; Ramachandra, Raghavendra; Busch, Christoph] Norwegian Univ Sci & Technol NTNU, Gjovik, Norway.
C3 National Institute of Technology (NIT System); Sardar Vallabhbhai
   National Institute of Technology; Norwegian University of Science &
   Technology (NTNU)
RP Upla, K (corresponding author), Sardar Vallabhbhai Natl Inst Technol, Surat, India.
EM kishorupla@gmail.com
RI Busch, Christoph/AAF-8176-2019; chudasama, vishal/AAW-7508-2021
OI Busch, Christoph/0000-0002-9159-2923; chudasama,
   vishal/0000-0002-3727-5484; Patel, Heena/0000-0001-7087-6276;
   Ramachandra, Raghavendra/0000-0003-0484-3956
CR Agustsson E, 2017, IEEE COMPUT SOC CONF, P1122, DOI 10.1109/CVPRW.2017.150
   Ahn N, 2018, LECT NOTES COMPUT SC, V11214, P256, DOI 10.1007/978-3-030-01249-6_16
   Anwar S, 2020, ACM COMPUT SURV, V53, DOI 10.1145/3390462
   Barron J. T, 2017, ARXIV PREPRINT ARXIV
   Bevilacqua M, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.135
   Chollet F, 2017, PROC CVPR IEEE, P1800, DOI 10.1109/CVPR.2017.195
   Chudasama V, 2020, COMPUT VIS IMAGE UND, V200, DOI 10.1016/j.cviu.2020.103038
   Dai T, 2019, PROC CVPR IEEE, P11057, DOI 10.1109/CVPR.2019.01132
   Dang-Nguyen D.T., 2015, P ACM MULT SYST C, P219
   Dong C, 2016, LECT NOTES COMPUT SC, V9906, P391, DOI 10.1007/978-3-319-46475-6_25
   Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281
   Fang FM, 2020, IEEE T IMAGE PROCESS, V29, P4656, DOI 10.1109/TIP.2020.2973769
   Freeman I, 2018, IEEE IMAGE PROC, P6, DOI 10.1109/ICIP.2018.8451339
   Freeman WT, 2002, IEEE COMPUT GRAPH, V22, P56, DOI 10.1109/38.988747
   Glasner D, 2009, IEEE I CONF COMP VIS, P349, DOI 10.1109/ICCV.2009.5459271
   Guo Y, 2020, PROC CVPR IEEE, P5406, DOI 10.1109/CVPR42600.2020.00545
   Haris M, 2018, PROC CVPR IEEE, P1664, DOI 10.1109/CVPR.2018.00179
   Hayat K., 2017, ARXIV PREPRINT ARXIV
   He K., 2016, PROC CVPR IEEE, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]
   Hore Alain, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P2366, DOI 10.1109/ICPR.2010.579
   HUANG G, 2017, PROC CVPR IEEE, P2261, DOI DOI 10.1109/CVPR.2017.243
   HUANG JB, 2015, PROC CVPR IEEE, P5197, DOI DOI 10.1109/CVPR.2015.7299156
   Huang T.S., 1984, ADV COMPUTER VISION, V1, P317
   Huang YF, 2021, IEEE T IMAGE PROCESS, V30, P2325, DOI 10.1109/TIP.2021.3050856
   Hui Z, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2024, DOI 10.1145/3343031.3351084
   Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.182, 10.1109/CVPR.2016.181]
   Kim KI, 2010, IEEE T PATTERN ANAL, V32, P1127, DOI 10.1109/TPAMI.2010.25
   Kingma D. P., 2014, arXiv
   Lai WS, 2019, IEEE T PATTERN ANAL, V41, P2599, DOI 10.1109/TPAMI.2018.2865304
   Lan RS, 2021, IEEE T CYBERNETICS, V51, P115, DOI 10.1109/TCYB.2019.2952710
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Li JC, 2018, LECT NOTES COMPUT SC, V11212, P527, DOI 10.1007/978-3-030-01237-3_32
   Li XG, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3282445
   Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151
   Lin ZC, 2004, IEEE T PATTERN ANAL, V26, P83, DOI 10.1109/TPAMI.2004.1261081
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Nasrollahi K, 2014, MACH VISION APPL, V25, P1423, DOI 10.1007/s00138-014-0623-4
   Park SJ, 2018, LECT NOTES COMPUT SC, V11220, P455, DOI 10.1007/978-3-030-01270-0_27
   Peleg T, 2014, IEEE T IMAGE PROCESS, V23, P2569, DOI 10.1109/TIP.2014.2305844
   Prajapati Kalpesh, 2020, Procedia Computer Science, V171, P139, DOI 10.1016/j.procs.2020.04.015
   Sajjadi Mehdi S. M., 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P4501, DOI 10.1109/ICCV.2017.481
   Shi WL, 2021, VISUAL COMPUT, V37, P1569, DOI 10.1007/s00371-020-01903-8
   Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207
   Singh A, 2014, PROC CVPR IEEE, P2846, DOI 10.1109/CVPR.2014.364
   Soh JW, 2019, PROC CVPR IEEE, P8114, DOI 10.1109/CVPR.2019.00831
   Tai Y, 2017, PROC CVPR IEEE, P2790, DOI 10.1109/CVPR.2017.298
   Timofte R, 2017, IEEE COMPUT SOC CONF, P1110, DOI 10.1109/CVPRW.2017.149
   Timofte R, 2013, IEEE I CONF COMP VIS, P1920, DOI 10.1109/ICCV.2013.241
   Tong T, 2017, IEEE I CONF COMP VIS, P4809, DOI 10.1109/ICCV.2017.514
   Trottier L, 2017, 2017 16TH IEEE INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND APPLICATIONS (ICMLA), P207, DOI 10.1109/ICMLA.2017.00038
   Wang C, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3670
   Wang YF, 2018, IEEE COMPUT SOC CONF, P977, DOI 10.1109/CVPRW.2018.00131
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Yang CY, 2014, LECT NOTES COMPUT SC, V8692, P372, DOI 10.1007/978-3-319-10593-2_25
   Yang JC, 2013, PROC CVPR IEEE, P1059, DOI 10.1109/CVPR.2013.141
   Yang WM, 2019, IEEE T MULTIMEDIA, V21, P3106, DOI 10.1109/TMM.2019.2919431
   Zeyde R., 2012, INT C CURV SURF, P711, DOI DOI 10.1007/978-3-642-27413-8_47
   Zhang DY, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3398685
   Zhang W, 2018, IEEE CONF COMPUT
   Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI 10.1007/978-3-030-01234-2_18
NR 60
TC 5
Z9 5
U1 0
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2022
VL 38
IS 11
BP 3643
EP 3665
DI 10.1007/s00371-021-02193-4
EA JUL 2021
PG 23
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 5X0BS
UT WOS:000669781600003
DA 2024-07-18
ER

PT J
AU Nandi, U
AF Nandi, Utpal
TI Fractal image compression using a fast affine transform and hierarchical
   classification scheme
SO VISUAL COMPUTER
LA English
DT Article
DE Fractal image compression; Affine transform; Encoding time; Compression
   ratio; Image quality; Iterated function system
AB Fractal image compression is one of the efficient structure-based methods in applications where images are compressed only once but decoded several times due to its resolution-independent feature and fast reconstruction time. However, it has high computational complexity restricting practical use most of the time. Although several methods have been developed to speed up the compression process, these do not satisfy the compression time or the decoded image quality requirements. The affine transforms of image blocks used in fractal coding require a huge number of multiplications and additions and are very expensive in computation that may also slow down the compression process. This paper presents a novel fractal image compression using a fast affine transform and hierarchical classification scheme. The applied affine transform computation algorithm of image blocks uses relationships among neighboring pixels of transformed image block that significantly reduces the number of multiplication and addition operations. Then, this strategy with hierarchical classification and class-wise domain sorting is applied in fractal coding with quad-tree and horizontal vertical partitioning schemes to reduce compression time. Experimental results show that the quad-tree-based fractal coding with the proposed scheme can significantly speed up the compression process keeping image quality and compression ratio almost unchanged.
C1 [Nandi, Utpal] Vidyasagar Univ, Dept Comp Sci, Midnapore 721102, W Bengal, India.
C3 Vidyasagar University
RP Nandi, U (corresponding author), Vidyasagar Univ, Dept Comp Sci, Midnapore 721102, W Bengal, India.
EM nandi.3utpal@gmail.com
RI Nandi, Utpal/AAW-9041-2021
OI Nandi, Utpal/0000-0002-9638-1906
CR Al Sideiri A, 2020, J REAL-TIME IMAGE PR, V17, P1375, DOI 10.1007/s11554-019-00894-7
   Barnsley M. F., 1988, Proceedings of the SPIE - The International Society for Optical Engineering, V1001, P122, DOI 10.1117/12.968945
   Barnsley MF., 1993, Fractals Everywhere
   Bhattacharya Nilavra, 2015, 10th International Conference on Computer Vision Theory and Applications (VISAPP 2015). Proceedings, P46
   Chen HN, 2010, IMAGE VISION COMPUT, V28, P518, DOI 10.1016/j.imavis.2009.08.007
   Davern P., 1996, Information Hiding. First International Workshop Proceedings, P279
   Distasi R, 2006, IEEE T IMAGE PROCESS, V15, P89, DOI 10.1109/TIP.2005.860334
   Fisher Y., 1995, Fractal image compression: theory and application, P55, DOI [10.1007/978-1-4612-2472-33, DOI 10.1007/978-1-4612-2472-3]
   Ghazel M, 2003, IEEE T IMAGE PROCESS, V12, P1560, DOI 10.1109/TIP.2003.818038
   Gupta R, 2016, IMAGING SCI J, V64, P374, DOI 10.1080/13682199.2016.1219100
   [何传江 HE Chuanjiang], 2005, [计算机仿真, Computer Simulation], V22, P60
   Hore Alain, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P2366, DOI 10.1109/ICPR.2010.579
   Jacquin AE, 1992, IEEE T IMAGE PROCESS, V1, P18, DOI 10.1109/83.128028
   Jeng JH, 2009, IEEE T IMAGE PROCESS, V18, P995, DOI 10.1109/TIP.2009.2013080
   Kovács T, 2008, IMAGE VISION COMPUT, V26, P1129, DOI 10.1016/j.imavis.2007.12.008
   Lai CM, 2003, IEEE T IMAGE PROCESS, V12, P1398, DOI 10.1109/TIP.2003.817246
   Lee S, 2006, LECT NOTES COMPUT SC, V4113, P1180, DOI 10.1007/11816157_147
   Liao X, 2015, J VIS COMMUN IMAGE R, V28, P21, DOI 10.1016/j.jvcir.2014.12.007
   Liao X, 2013, IEICE T FUND ELECTR, VE96A, P2731, DOI 10.1587/transfun.E96.A.2731
   Liao X, 2013, IEICE T FUND ELECTR, VE96A, P2039, DOI 10.1587/transfun.E96.A.2039
   Lin KT, 2012, OPT COMMUN, V285, P2335, DOI 10.1016/j.optcom.2012.01.028
   Lin YL, 2010, COMPUT MATH APPL, V60, P2099, DOI 10.1016/j.camwa.2010.07.051
   Liu S, 2017, FRACTALS, V25, DOI 10.1142/S0218348X17400047
   Liu S, 2016, MULTIMED TOOLS APPL, V75, P15525, DOI 10.1007/s11042-014-2446-8
   Liu SA, 2017, MULTIMED TOOLS APPL, V76, P5787, DOI 10.1007/s11042-014-2408-1
   Lu J, 2013, IEEE T IMAGE PROCESS, V22, P134, DOI 10.1109/TIP.2012.2215619
   Majhi, 2021, INTELLIGENT DATA ENG, P123, DOI [10.1007/978-981-15-5679-1_12, DOI 10.1007/978-981-15-5679-1_12]
   Menassel Rafik, 2020, International Journal of Computers and Applications, V42, P697, DOI 10.1080/1206212X.2019.1638631
   Menassel R, 2018, J EXP THEOR ARTIF IN, V30, P429, DOI 10.1080/0952813X.2017.1409281
   Nandi Utpal, 2021, Advances in Smart Grid and Renewable Energy. Select Proceedings of ETAEERE 2020. Lecture Notes in Electrical Engineering (LNEE 691), P237, DOI 10.1007/978-981-15-7511-2_21
   Nandi Utpal, 2016, International Journal of Computers and Applications, V38, P156, DOI 10.1080/1206212X.2016.1237130
   Nandi U, 2020, MULTIMED TOOLS APPL, V79, P26345, DOI 10.1007/s11042-020-09256-z
   Nandi U, 2019, INNOV SYST SOFTW ENG, V15, P35, DOI 10.1007/s11334-019-00327-5
   Nandi U, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON RESEARCH IN COMPUTATIONAL INTELLIGENCE AND COMMUNICATION NETWORKS (ICRCICN), P56, DOI 10.1109/ICRCICN.2015.7434209
   Papathomas T. V., 1987, Visual Computer, V3, P23, DOI 10.1007/BF02153648
   Roy SK, 2018, CHAOS SOLITON FRACT, V106, P16, DOI 10.1016/j.chaos.2017.11.013
   Schwartz WR, 2011, INT J IMAGE GRAPH, V11, P571, DOI 10.1142/S0219467811004251
   Sinha D, 2018, SOCIAL TRANSFORMATIO, V836, P603
   Svynchuk O, 2021, FRACTAL FRACT, V5, DOI 10.3390/fractalfract5020031
   Tang XT, 2010, BELL LABS TECH J, V15, P209, DOI 10.1002/bltj.20433
   Truong TK, 2004, CHAOS SOLITON FRACT, V22, P1071, DOI 10.1016/j.chaos.2004.03.015
   Wallace G. K., 1991, Communications of the ACM, V34, P30, DOI 10.1145/103085.103089
   Wang JJ, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0184408
   Wang JJ, 2013, IEEE T IMAGE PROCESS, V22, P3690, DOI 10.1109/TIP.2013.2268977
   Wang Q, 2016, MOB INF SYST, V2016, DOI [10.1155/2016/2159703, 10.1155/2016/8765874]
   Wang SS, 2008, PATTERN RECOGN, V41, P701, DOI 10.1016/j.patcog.2007.05.012
   Wang XY, 2014, NONLINEAR DYNAM, V75, P439, DOI 10.1007/s11071-013-1076-4
   Wang XY, 2011, CHINESE PHYS B, V20, DOI 10.1088/1674-1056/20/10/104202
   Wang XY, 2010, IMAGE VISION COMPUT, V28, P1303, DOI 10.1016/j.imavis.2010.01.008
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Weber G, 1993, USC SIPI IMAGE DATAB
   Xing CJ, 2008, CISP 2008: FIRST INTERNATIONAL CONGRESS ON IMAGE AND SIGNAL PROCESSING, VOL 1, PROCEEDINGS, P283, DOI 10.1109/CISP.2008.711
   Zhao Y, 1998, IEEE T CIRC SYST VID, V8, P269, DOI 10.1109/76.678621
   Zhou YM, 2009, CHAOS SOLITON FRACT, V39, P1823, DOI 10.1016/j.chaos.2007.06.089
NR 54
TC 4
Z9 4
U1 0
U2 13
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2022
VL 38
IS 11
BP 3867
EP 3880
DI 10.1007/s00371-021-02226-y
EA JUL 2021
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 5X0BS
UT WOS:000669781600002
DA 2024-07-18
ER

PT J
AU Koh, N
   Jayaraman, PK
   Zheng, JM
AF Koh, Naimin
   Jayaraman, Pradeep Kumar
   Zheng, Jianmin
TI Truncated octree and its applications
SO VISUAL COMPUTER
LA English
DT Article
DE Octree; Point cloud; Spatial partition; Scene query; Compression
ID COLLISION DETECTION; POINT; QUADTREES
AB Octree is a hierarchical data structure with many applications, especially in encoding unstructured point clouds. The depth of an octree is dependent of the scale of the input data and the desired resolution of the smallest voxels in the leaf nodes as well. Thus, it often requires a deep octree to maintain low level of geometric errors for large-scale sparse point clouds, which leads to high memory requirement and low access speed. This paper presents a new structure called truncated octree or T-Octree that truncates the octree by adaptively pruning the top hierarchy and represents the deep octree by a set of shallow sub-octrees. The structure is further extended to support random access of nodes and out-of-core streaming of large data sets. We also propose a variable length addressing scheme to adaptively choose the length of an octree's node address based on the truncation level. As a result, T-Octree provides highly efficient query performance and can save storage without losing the original structure for sparse or clustered models and scenes. We demonstrate the efficacy and efficiency of the new structure on point cloud compression and scene query tasks for sparse or clustered data.
C1 [Koh, Naimin; Jayaraman, Pradeep Kumar; Zheng, Jianmin] Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore, Singapore.
   [Koh, Naimin] Autodesk Inc, Singapore, Singapore.
   [Jayaraman, Pradeep Kumar] Autodesk Inc, Toronto, ON, Canada.
C3 Nanyang Technological University; Autodesk, Inc.; Autodesk, Inc.
RP Zheng, JM (corresponding author), Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore, Singapore.
EM NAIMIN001@e.ntu.edu.sg; pradeep.pyro@gmail.com; asjmzheng@ntu.edu.sg
RI Zheng, Jianmin/A-3717-2011; Jayaraman, Pradeep Kumar/AAV-8965-2021
OI Zheng, Jianmin/0000-0002-5062-6226; Naimin, Koh/0000-0002-4646-4041
CR [Anonymous], 2009, P 8 BRAZ S GAM DIG E
   [Anonymous], 2008, P 1 INT WORKSH MULT, DOI [10.1145/1370082.1370085, DOI 10.1145/1370082.1370085]
   Artale MC, 2017, MON NOT R ASTRON SOC, V470, P1771, DOI 10.1093/mnras/stx1263
   Aulinas J, 2008, FRONT ARTIF INTEL AP, V184, P363, DOI 10.3233/978-1-58603-925-7-363
   Chang JW, 2010, COMPUT AIDED DESIGN, V42, P50, DOI 10.1016/j.cad.2009.04.010
   Chen YW, 2019, INFORM SCIENCES, V472, P145, DOI 10.1016/j.ins.2018.09.012
   Girardeau-Montaut Daniel., 2016, Cloudcompare
   Golla T, 2015, IEEE INT C INT ROBOT, P5087, DOI 10.1109/IROS.2015.7354093
   Hackel T, 2018, PHOTOGRAMM ENG REM S, V84, P297, DOI 10.14358/PERS.84.5.297
   Hornung A, 2013, AUTON ROBOT, V34, P189, DOI 10.1007/s10514-012-9321-0
   Huang Y, 2008, IEEE T VIS COMPUT GR, V14, P440, DOI 10.1109/TVCG.2007.70441
   Kazhdan M, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2487228.2487237
   Koh N, 2020, 2020 INTERNATIONAL CONFERENCE ON CYBERWORLDS (CW 2020), P1, DOI 10.1109/CW49994.2020.00009
   Lindstrom P, 2014, IEEE T VIS COMPUT GR, V20, P2674, DOI 10.1109/TVCG.2014.2346458
   MEAGHER D, 1982, COMPUT VISION GRAPH, V19, P129, DOI 10.1016/0146-664X(82)90104-6
   Merry B, 2006, COMPUT GRAPH FORUM, V25, P709, DOI 10.1111/j.1467-8659.2006.00993.x
   Patel J. M., 1996, SIGMOD Record, V25, P259, DOI 10.1145/235968.233338
   Pollefeys, 2017, ARXIV PREPRINT ARXIV
   Porwal S, 2013, DEFENCE SCI J, V63, P89, DOI 10.14429/dsj.63.3768
   Schauer J, 2015, ADV ENG INFORM, V29, P440, DOI 10.1016/j.aei.2015.03.007
   SCHRACK G, 1992, CVGIP-IMAG UNDERSTAN, V55, P221, DOI 10.1016/1049-9660(92)90022-U
   Segura C., 2013, COMPUT AIDED DESIGN, P24
   STOCCO L, 1995, IEEE PACIF, P426, DOI 10.1109/PACRIM.1995.519560
   Ströter D, 2020, VISUAL COMPUT, V36, P2327, DOI 10.1007/s00371-020-01886-6
   Sundar H, 2008, SIAM J SCI COMPUT, V30, P2675, DOI 10.1137/070681727
   Taketomi T, 2017, IPSJ Trans. Comput. Vis. Appl, V9, P1, DOI [10.1186/s41074-017-0027-2, DOI 10.1186/S41074-017-0027-2]
   Vinkler M., 2014, Proceedings of the 30th Spring Conference on Computer Graphics (SCCG '14) (, P29
   Wang MN, 2019, J MECH MED BIOL, V19, DOI 10.1142/S021951941940044X
   Wu, 2017, ACTA OPT SINICA, V37, P251
   Yi Z, 2016, ISPRS J PHOTOGRAMM, V120, P25, DOI 10.1016/j.isprsjprs.2016.08.002
   Yin H, 2017, IEEE INT VEH SYM, P1364, DOI 10.1109/IVS.2017.7995901
NR 31
TC 1
Z9 1
U1 2
U2 19
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2022
VL 38
IS 4
SI SI
BP 1167
EP 1179
DI 10.1007/s00371-021-02130-5
EA APR 2021
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 0D9DX
UT WOS:000644371000002
DA 2024-07-18
ER

PT J
AU Tsuchie, S
AF Tsuchie, Shoichi
TI Reconstruction of adaptive swept surfaces from scanned data for styling
   design
SO VISUAL COMPUTER
LA English
DT Article
DE Adaptive swept surface; Underlying surface; Difference surface; Reverse
   engineering
AB This study presents a new method for reconstructing an adaptive underlying surface, (S) over tilde, from scanned data for styling design objects. (S) over tilde is usually generated by sweeping a curve that varies its shape gradually while being swept. However, when (S) over tilde is reconstructed from a segmented part of the scanned data, it is generally more difficult to control the gradual variation as the ratio of the segmented part to the area of (S) over tilde becomes smaller. Therefore, this study represents (S) over tilde by the sum of two surfaces, (S) over tilde = S-U + S-Delta. Here, an underlying surface S-U is generated by the standard sweep method and the difference surface S-Delta not only compensates for the error between (S) over tilde and the scanned data but also exhibits monotonous change in the curvatures. Consequently, the gradual change in a curve being swept is represented by S-Delta, which does not encounter the aforementioned problem because it is intended to control the deviation from the "reference" S-U under the constraint of "curvature monotonicity." The experimental results demonstrate the validity of surface reconstruction from real-world scanned data as well as an application of the proposed method.
C1 [Tsuchie, Shoichi] Nihon Unisys Ltd, Koto Ku, 1-1-1 Toyosu, Tokyo 1358560, Japan.
RP Tsuchie, S (corresponding author), Nihon Unisys Ltd, Koto Ku, 1-1-1 Toyosu, Tokyo 1358560, Japan.
EM shoichi.tsuchie@unisys.co.jp
OI Tsuchie, Shoichi/0000-0002-9991-5635
CR [Anonymous], 2005, S GEOMETRY PROCESSIN
   [Anonymous], 2002, Algorithms for minimization without derivatives
   Burchard H, 1994, Designing fair curves and surfaces, P3
   Farin G, 2006, COMPUT AIDED GEOM D, V23, P573, DOI 10.1016/j.cagd.2006.03.004
   Hosaka M, 1992, MODELING CURVES SURF, DOI [10.1007/978-3-642-76598-8, DOI 10.1007/978-3-642-76598-8]
   Hoschek J., 1993, FUNDAMENTALS COMPUTE
   Inoue J., 2009, P INT ASS SOC DES RE, P2513
   Johnson S. G., 2014, NLOPT NONLINEAR OPTI
   Joshi P., 2007, COMPUT AIDED DESIGN, V4, P607, DOI DOI 10.1080/16864360.2007.10738495
   Mehlum E, 1998, ADV COMPUT MATH, V8, P49, DOI 10.1023/A:1018931910836
   Meier H., 1987, Computer-Aided Geometric Design, V4, P297, DOI 10.1016/0167-8396(87)90004-5
   MORETON HP, 1992, COMP GRAPH, V26, P167, DOI 10.1145/142920.134035
   NELDER JA, 1965, COMPUT J, V7, P308, DOI 10.1093/comjnl/7.4.308
   Piegl L.A., 1997, NURBS BOOK, V2nd, P81, DOI [10.1007/978-3-642-59223-2_3, DOI 10.1007/978-3-642-59223-2, DOI 10.1007/978-3-642-59223-2_3]
   Tsuchie S, 2021, ENG COMPUT-GERMANY, V37, P211, DOI 10.1007/s00366-019-00817-x
   Tsuchie S, 2017, COMPUT GRAPH-UK, V68, P108, DOI 10.1016/j.cag.2017.08.015
   Tsuchie S, 2017, VISUAL COMPUT, V33, P1197, DOI 10.1007/s00371-016-1282-4
   Tsuchie S, 2016, COMPUT AIDED DESIGN, V71, P39, DOI 10.1016/j.cad.2015.09.004
   Varady Tamas, 2008, ComputerAided Design and Applications, V5, P577, DOI [DOI 10.3722/CADAPS.2008.577-588, 10.3722/cadaps.2008.577-588arXiv:https://www.tandfonline.com/doi/pdf/10.3722/cadaps.2008.577-588]
   Weiss V, 2002, COMPUT AIDED GEOM D, V19, P19, DOI 10.1016/S0167-8396(01)00086-3
   Westgaard G., 2001, S SOLID MODELING APP, P88, DOI [10.1145/376957.376969, DOI 10.1145/376957.376969]
   Yamada Y, 1993, Clay modeling: techniques for giving three-dimensional form to idea
   Ziatdinov R, 2012, COMPUT AIDED GEOM D, V29, P510, DOI 10.1016/j.cagd.2012.03.006
NR 23
TC 4
Z9 4
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2022
VL 38
IS 2
BP 493
EP 507
DI 10.1007/s00371-020-02030-0
EA JAN 2021
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZE7IA
UT WOS:000604077400001
DA 2024-07-18
ER

PT J
AU Kumar, RJR
   Sundaram, M
   Arumugam, N
AF Kumar, R. Jeen Retna
   Sundaram, M.
   Arumugam, N.
TI Facial emotion recognition using subband selective multilevel stationary
   wavelet gradient transform and fuzzy support vector machine
SO VISUAL COMPUTER
LA English
DT Article
DE Facial emotion recognition; Fuzzy support vector machine; Kernel
   principal component analysis; Stationary biorthogonal wavelet transform;
   Wavelet gradient transform
ID EXPRESSION RECOGNITION; FACE
AB Facial emotion recognition finds a major role in affective computing. Recognizing emotion by facial expression is an extremely important activity to design control oriented and human computer interactive applications especially in cognitive science and neuroscience. For a precise and robust recognition, feature extraction is one of the major challenges in facial expression recognition system. Wavelet transform is one of the major key methods utilized for feature extraction in facial emotion recognition. In this paper, the statistical parameters from the proposed subband selective multilevel stationary wavelet gradient transform are calculated and are utilized as features for efficacious recognition of emotion. The features of the wavelet transform contain both spatial and spectral domain information which is best suited for identifying human emotions through facial expression. The introduction of gradient transform to find the gradient of subband avails to estimate the edges in images for the quality amelioration of subbands. The dimension reduction in the extracted features is done by using Pearson-kernel-principal component analysis method. The classification of emotion using the selected features is done by the proposed Gaussian membership function fuzzy SVM classifier. Experiments were performed on the well-known database for facial expression such as JAFEE database, CK + database and FG Net database and obtained promising emotion classification results.
C1 [Kumar, R. Jeen Retna] Bethlahem Inst Engn, Dept ECE, Karungal 629157, Tamil Nadu, India.
   [Sundaram, M.] VSB Engn Coll, Dept ECE, Karur 639111, Tamil Nadu, India.
   [Arumugam, N.] Natl Engn Coll, Dept ECE, Kovilpatti 628503, Tamil Nadu, India.
C3 National Engineering College - India
RP Kumar, RJR (corresponding author), Bethlahem Inst Engn, Dept ECE, Karungal 629157, Tamil Nadu, India.
EM jejinrsrch@gmail.com; cm.sundaram2011@gmail.com; arms.ece@gmail.com
RI Muniasamy, Sundaram/AAC-5619-2022; KUMAR R, JEEN RETNA/ADL-2829-2022
OI N, Arumugam/0000-0002-5165-6881
CR Ali H, 2015, J MED IMAG HEALTH IN, V5, P1272, DOI 10.1166/jmihi.2015.1527
   [Anonymous], 2010, PROCEEDINGS OF THE T
   Baron-Cohen S, 1998, NATURE, V392, P459, DOI 10.1038/33076
   Bendjillali RI, 2019, ELECTRONICS-SWITZ, V8, DOI 10.3390/electronics8030324
   Bin Iqbal MT, 2020, IEEE T AFFECT COMPUT, V11, P125, DOI 10.1109/TAFFC.2018.2829707
   Cambria E, 2016, IEEE INTELL SYST, V31, P102, DOI 10.1109/MIS.2016.31
   Datta A., 2017, ADV PRINCIPAL COMPON, P19, DOI DOI 10.1007/978-981-10-6704-4_2
   EKMAN P, 1971, J PERS SOC PSYCHOL, V17, P124, DOI 10.1037/h0030377
   Fan XJ, 2019, J VIS COMMUN IMAGE R, V65, DOI 10.1016/j.jvcir.2019.102659
   Gan YL, 2020, IEEE ACCESS, V8, P7383, DOI 10.1109/ACCESS.2020.2963913
   Gavrilescu M, 2015, 2015 23RD TELECOMMUNICATIONS FORUM TELFOR (TELFOR), P720, DOI 10.1109/TELFOR.2015.7377568
   Gogic I, 2020, VISUAL COMPUT, V36, P97, DOI 10.1007/s00371-018-1585-8
   Goh KM, 2020, VISUAL COMPUT, V36, P445, DOI 10.1007/s00371-018-1607-6
   Jamshidnezhad A, 2013, INT J BIO-INSPIR COM, V5, P175, DOI 10.1504/IJBIC.2013.055092
   Joseph A, 2020, VISUAL COMPUT, V36, P529, DOI 10.1007/s00371-019-01628-3
   Khan RA, 2013, PATTERN RECOGN LETT, V34, P1159, DOI 10.1016/j.patrec.2013.03.022
   Li K, 2020, VISUAL COMPUT, V36, P391, DOI 10.1007/s00371-019-01627-4
   Li W, 2007, ICNC 2007: THIRD INTERNATIONAL CONFERENCE ON NATURAL COMPUTATION, VOL 5, PROCEEDINGS, P809
   Lyons M, 1998, AUTOMATIC FACE AND GESTURE RECOGNITION - THIRD IEEE INTERNATIONAL CONFERENCE PROCEEDINGS, P200, DOI 10.1109/AFGR.1998.670949
   Ma JX, 2018, INT J PATTERN RECOGN, V32, DOI 10.1142/S0218001418540186
   Maheswari VU, 2021, J AMB INTEL HUM COMP, V12, P4775, DOI 10.1007/s12652-020-01886-3
   Makhmudkhujaev F, 2019, SIGNAL PROCESS-IMAGE, V74, P1, DOI 10.1016/j.image.2019.01.002
   Meena HK, 2021, IETE J RES, V67, P667, DOI 10.1080/03772063.2019.1565952
   Nezhadarya E., 2012, IEEE 14 INT WORKSH M, DOI [10.1109/mmsp.2012.6343424, DOI 10.1109/MMSP.2012.6343424]
   Pan XZ, 2020, IET IMAGE PROCESS, V14, P176, DOI 10.1049/iet-ipr.2019.0293
   Qayyum H, 2017, MATH PROBL ENG, V2017, DOI 10.1155/2017/9854050
   Qin S, 2020, INT J WAVELETS MULTI, V18, DOI 10.1142/S0219691320500034
   Reddy GV, 2020, COGN SYST RES, V62, P23, DOI 10.1016/j.cogsys.2020.03.002
   Reza AM, 2004, J VLSI SIG PROC SYST, V38, P35, DOI 10.1023/B:VLSI.0000028532.53893.82
   Sevakula RK, 2017, IEEE T FUZZY SYST, V25, P1446, DOI 10.1109/TFUZZ.2017.2722421
   Sun X, 2020, INFORM SCIENCES, V522, P35, DOI 10.1016/j.ins.2020.02.047
   Sun X, 2020, IEEE ACCESS, V8, P7183, DOI 10.1109/ACCESS.2020.2964298
   Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb
   Wallhoff F, 2006, 2006 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO - ICME 2006, VOLS 1-5, PROCEEDINGS, P493, DOI 10.1109/ICME.2006.262433
   Wang SH, 2018, NEUROCOMPUTING, V272, P668, DOI 10.1016/j.neucom.2017.08.015
   Yu MJ, 2020, PATTERN RECOGN LETT, V131, P166, DOI 10.1016/j.patrec.2020.01.016
   Yu NG, 2020, IEEE ACCESS, V8, P4700, DOI 10.1109/ACCESS.2019.2963201
   Zhang HF, 2020, IEEE ACCESS, V8, P37976, DOI 10.1109/ACCESS.2020.2975913
   Zhang YD, 2016, IEEE ACCESS, V4, P8375, DOI 10.1109/ACCESS.2016.2628407
NR 39
TC 18
Z9 18
U1 0
U2 17
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2021
VL 37
IS 8
BP 2315
EP 2329
DI 10.1007/s00371-020-01988-1
EA DEC 2020
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TT5FX
UT WOS:000600824200004
DA 2024-07-18
ER

PT J
AU Huang, X
   Wang, MJ
   Gong, ML
AF Huang, Xin
   Wang, Mingjie
   Gong, Minglun
TI Fine-grained talking face generation with video reinterpretation
SO VISUAL COMPUTER
LA English
DT Article
DE Talking face; Video generation; Multi-purpose discriminators
AB Generating a talking face video from a given audio clip and an arbitrary face image has many applications in areas such as special visual effects and human-computer interactions. This is a challenging task, as it requires disentangling semantic information from both input audio clips and face image, then synthesizing novel animated facial image sequences from the combined semantic features. The desired output video should maintain both video realism and audio-lip motion consistency. To achieve these two objectives, we propose a coarse-to-fine tree-like architecture for synthesizing realistic talking face frames directly from audio clips. This is followed by a video-to-word regeneration module to translate the synthesized talking videos back to the words space, which is enforced to align with the input audios. With multi-level facial landmark attentions, the proposed audio-to-video-to-words framework can generate fine-grained talking face videos that are not only synchronous with the input audios but also maintain visual details from the input face images. Multi-purpose discriminators are also adopted for adversarial learning to further improve both image fidelity and semantic consistency. Extensive experiments on GRID and LRW datasets demonstrate the advantages of our framework over previous methods in terms of video quality and audio-video synchronization.
C1 [Huang, Xin; Wang, Mingjie] Mem Univ Newfoundland, St John, NF, Canada.
   [Gong, Minglun] Univ Guelph, Guelph, ON, Canada.
C3 Memorial University Newfoundland; University of Guelph
RP Huang, X (corresponding author), Mem Univ Newfoundland, St John, NF, Canada.
EM xhuang@mun.ca
RI Gong, Minglun/AAU-3103-2020; wang, ming/HPC-6329-2023
OI Gong, Minglun/0000-0001-5820-5381; Huang, Xin/0000-0001-7113-5066
CR Bregler C., 1997, P 24 ANN C COMP GRAP, V31, P353, DOI DOI 10.1145/258734.258880
   Bulat A, 2017, IEEE I CONF COMP VIS, P1021, DOI 10.1109/ICCV.2017.116
   Chen LL, 2019, PROC CVPR IEEE, P7824, DOI 10.1109/CVPR.2019.00802
   Chen LL, 2018, LECT NOTES COMPUT SC, V11211, P538, DOI 10.1007/978-3-030-01234-2_32
   Chung J., 2017, 5 INT C LEARNING REP
   Chung JS, 2017, LECT NOTES COMPUT SC, V10112, P87, DOI 10.1007/978-3-319-54184-6_6
   Cooke M, 2006, J ACOUST SOC AM, V120, P2421, DOI 10.1121/1.2229005
   Deng Z, 2008, COMPUT GRAPH FORUM, V27, P2096, DOI 10.1111/j.1467-8659.2008.01192.x
   Fan B, 2016, MULTIMED TOOLS APPL, V75, P5287, DOI 10.1007/s11042-015-2944-3
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Huang X, 2019, 2019 16TH CONFERENCE ON COMPUTER AND ROBOT VISION (CRV 2019), P73, DOI 10.1109/CRV.2019.00018
   Karras T, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073658
   Kim Y., 2020, VISUAL COMPUT, P1
   Li YT, 2018, AAAI CONF ARTIF INTE, P7065
   Luong M.-T., 2015, P 2015 C EMPIRICAL M, DOI DOI 10.18653/V1/D15-1166
   Ma S, 2018, PROC CVPR IEEE, P5657, DOI 10.1109/CVPR.2018.00593
   Ma XH, 2012, IEEE T VIS COMPUT GR, V18, P1915, DOI 10.1109/TVCG.2012.67
   Mathieu M., 2015, PROC INT C LEARN REP
   Oh J., 2015, Advances in neural information processing systems, P2863
   Parkhi OM, 2015, DEEP FACE RECOGNITIO
   Petridis S, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P6548, DOI 10.1109/ICASSP.2018.8461326
   Petridis S, 2017, INT CONF ACOUST SPEE, P2592, DOI 10.1109/ICASSP.2017.7952625
   Pumarola A, 2018, LECT NOTES COMPUT SC, V11214, P835, DOI 10.1007/978-3-030-01249-6_50
   Qiao TT, 2019, PROC CVPR IEEE, P1505, DOI 10.1109/CVPR.2019.00160
   Saito M, 2017, IEEE I CONF COMP VIS, P2849, DOI 10.1109/ICCV.2017.308
   Song Y., 2018, ARXIV180404786
   Stafylakis Themos, 2017, ARXIV170304105
   Suwajanakorn S, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073640
   Taylor S, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073699
   Theobalt Chris- tian, 2016, P IEEE C COMPUTER VI, DOI DOI 10.1109/CVPR.2016.262
   Tulyakov S, 2018, PROC CVPR IEEE, P1526, DOI 10.1109/CVPR.2018.00165
   Vondrick C., 2016, ADV NEURAL INFORM PR, P613, DOI DOI 10.13016/M26GIH-TNYZ
   Vougioukas K, 2020, INT J COMPUT VISION, V128, P1398, DOI 10.1007/s11263-019-01251-8
   Wan V, 2013, INTERSPEECH, P2666
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wiles O, 2018, LECT NOTES COMPUT SC, V11217, P690, DOI 10.1007/978-3-030-01261-8_41
   Willmott CJ, 2005, CLIMATE RES, V30, P79, DOI 10.3354/cr030079
   Xie L, 2007, IEEE T MULTIMEDIA, V9, P500, DOI 10.1109/TMM.2006.888009
   Xu T, 2018, PROC CVPR IEEE, P1316, DOI 10.1109/CVPR.2018.00143
   Yehia H, 1998, SPEECH COMMUN, V26, P23, DOI 10.1016/S0167-6393(98)00048-X
   Zhang H., 2018, ARXIV180508318
   Zhang H, 2019, IEEE T PATTERN ANAL, V41, P1947, DOI 10.1109/TPAMI.2018.2856256
   Zhang H, 2017, IEEE I CONF COMP VIS, P5908, DOI 10.1109/ICCV.2017.629
   Zhou H, 2019, AAAI CONF ARTIF INTE, P9299
NR 44
TC 7
Z9 7
U1 4
U2 19
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2021
VL 37
IS 1
BP 95
EP 105
DI 10.1007/s00371-020-01982-7
EA SEP 2020
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA QE3UO
UT WOS:000572037900002
DA 2024-07-18
ER

PT J
AU Karthik, K
   Kamath, SS
AF Karthik, K.
   Kamath, S. Sowmya
TI A deep neural network model for content-based medical image retrieval
   with multi-view classification
SO VISUAL COMPUTER
LA English
DT Article
DE Neural networks; Medical image retrieval; Image classification; View
   classification; Diagnostic image analysis; Similarity computation
AB In medical applications, retrieving similar images from repositories is most essential for supporting diagnostic imaging-based clinical analysis and decision support systems. However, this is a challenging task, due to the multi-modal and multi-dimensional nature of medical images. In practical scenarios, the availability of large and balanced datasets that can be used for developing intelligent systems for efficient medical image management is quite limited. Traditional models often fail to capture the latent characteristics of images and have achieved limited accuracy when applied to medical images. For addressing these issues, a deep neural network-based approach for view classification and content-based image retrieval is proposed and its application for efficient medical image retrieval is demonstrated. We also designed an approach for body part orientation view classification labels, intending to reduce the variance that occurs in different types of scans. The learned features are used first to predict class labels and later used to model the feature space for similarity computation for the retrieval task. The outcome of this approach is measured in terms of error score. When benchmarked against 12 state-of-the-art works, the model achieved the lowest error score of 132.45, with 9.62-63.14% improvement over other works, thus highlighting its suitability for real-world applications.
C1 [Karthik, K.; Kamath, S. Sowmya] Natl Inst Technol Karnataka, Dept Informat Technol, Healthcare Analyt & Language Engn HALE Lab, Surathkal, Mangaluru, India.
C3 National Institute of Technology (NIT System); National Institute of
   Technology Karnataka
RP Karthik, K (corresponding author), Natl Inst Technol Karnataka, Dept Informat Technol, Healthcare Analyt & Language Engn HALE Lab, Surathkal, Mangaluru, India.
EM 2karthik.bhat@gmail.com; sowmyakamath@nitk.edu.in
RI K, Karthik/AGA-6671-2022
OI K, Karthik/0000-0003-0846-2982; Kamath S, Sowmya/0000-0002-0888-7238
FU Science and Engineering Research Board, Department of Science and
   Technology, Government of India [ECR/2017/001056]
FX The authors gratefully acknowledge the Science and Engineering Research
   Board, Department of Science and Technology, Government of India, for
   its financial support through Early Career Research Grant
   (ECR/2017/001056) to second author. We also thank Dr. T.M. Deserno,
   Department of Medical Informatics, RWTH Aachen, Germany, for providing
   the dataset used in our experiments.
CR Aggarwal P., 5 INT C ADV REC TECH, DOI 10.1049/cp.2013.2204
   Ahmad J, 2018, J MED SYST, V42, DOI 10.1007/s10916-017-0875-4
   [Anonymous], 2009, WORKSH CROSS LANG EV
   [Anonymous], 2002, ICVGIP
   Anthimopoulos M, 2016, IEEE T MED IMAGING, V35, P1207, DOI 10.1109/TMI.2016.2535865
   Avni U., 2009, CLEF WORKING NOTES
   Çamlica Z, 2015, INT CONF IMAG PROC, P550, DOI 10.1109/IPTA.2015.7367208
   Getto R, 2015, VISUAL COMPUT, V31, P989, DOI 10.1007/s00371-015-1113-z
   Iakovidis DK, 2009, IEEE T INF TECHNOL B, V13, P442, DOI 10.1109/TITB.2008.923144
   Karthik K, 2018, INT CONF IND INF SYS, P20
   Khatami A, 2018, EXPERT SYST APPL, V100, P224, DOI 10.1016/j.eswa.2018.01.056
   Khatami A, 2018, APPL SOFT COMPUT, V63, P197, DOI 10.1016/j.asoc.2017.11.024
   Lehmann TM, 2003, P SOC PHOTO-OPT INS, V5033, P109, DOI 10.1117/12.481942
   Lehmann TM, 2003, PROC SPIE, V5033, P440, DOI 10.1117/12.480677
   Liu XR, 2016, IEEE IJCNN, P2872, DOI 10.1109/IJCNN.2016.7727562
   Nardelli P, 2018, IEEE T MED IMAGING, V37, P2428, DOI 10.1109/TMI.2018.2833385
   Pourghassem H, 2013, TURK J ELECTR ENG CO, V21, P882, DOI 10.3906/elk-1010-876
   Qayyum A, 2017, NEUROCOMPUTING, V266, P8, DOI 10.1016/j.neucom.2017.05.025
   Seidel K, 2009, NVMTS: 2009 10TH ANNUAL NON-VOLATILE MEMORY TECHNOLOGY SYMPOSIUM, P72, DOI 10.1109/NVMT.2009.5429789
   Sharma S, 2009, 2009 IEEE INTERNATIONAL ADVANCE COMPUTING CONFERENCE, VOLS 1-3, P458, DOI 10.1109/IADCC.2009.4809054
   Shyu CR, 1999, COMPUT VIS IMAGE UND, V75, P111, DOI 10.1006/cviu.1999.0768
   Soundalgekar P, 2018, IEEE INT C SEMANT CO, P369, DOI 10.1109/ICSC.2018.00075
   Srinivas M, 2015, NEUROCOMPUTING, V168, P880, DOI 10.1016/j.neucom.2015.05.036
   Sze-To A, 2016, IEEE IJCNN, P2864, DOI 10.1109/IJCNN.2016.7727561
   Tang LH, 1999, PROC SPIE, V3662, P360, DOI 10.1117/12.352767
   Tizhoosh HR, 2015, IEEE IMAGE PROC, P818, DOI 10.1109/ICIP.2015.7350913
   Vikram M, 2019, PROCEEDINGS OF THE 6TH ACM IKDD CODS AND 24TH COMAD, P44, DOI 10.1145/3297001.3297007
   Wang JZ, 2000, J AM MED INFORM ASSN, P883
   Xi PC, 2020, VISUAL COMPUT, V36, P1869, DOI 10.1007/s00371-019-01775-7
   Xue ZY, 2015, COMP MED SY, P66, DOI 10.1109/CBMS.2015.49
   Zare M. R., 2011, Proceedings of the 2011 3rd International Conference on Computational Intelligence, Communication Systems and Networks (CICSyN 2011), P264, DOI 10.1109/CICSyN.2011.63
   Zare MR, 2013, IET COMPUT VIS, V7, P105, DOI 10.1049/iet-cvi.2012.0291
   Zhu S., 2016, ARXIV160404675
NR 33
TC 24
Z9 24
U1 1
U2 12
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2021
VL 37
IS 7
BP 1837
EP 1850
DI 10.1007/s00371-020-01941-2
EA AUG 2020
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SZ5DG
UT WOS:000557136700001
DA 2024-07-18
ER

PT J
AU Sun, YL
   Miao, YW
   Chen, JZ
   Pajarola, R
AF Sun, Yuliang
   Miao, Yongwei
   Chen, Jiazhou
   Pajarola, Renato
TI PGCNet: patch graph convolutional network for point cloud segmentation
   of indoor scenes
SO VISUAL COMPUTER
LA English
DT Article
DE Point cloud; Scene segmentation; Surface patch; Graph convolutional
   network; Edge convolution; Encoder-decoder
AB Semantic segmentation of 3D point clouds is a crucial task in scene understanding and is also fundamental to indoor scene applications such as indoor navigation, mobile robotics, augmented reality. Recently, deep learning frameworks have been successfully adopted to point clouds but are limited by the size of data. While most existing works focus on individual sampling points, we use surface patches as a more efficient representation and propose a novel indoor scene segmentation framework called patch graph convolution network (PGCNet). This framework treats patches as input graph nodes and subsequently aggregates neighboring node features by dynamic graph U-Net (DGU) module, which consists of dynamic edge convolution operation inside U-shaped encoder-decoder architecture. The DGU module dynamically update graph structures at each level to encode hierarchical edge features. Incorporating PGCNet, we can segment the input scene into two types, i.e., room layout and indoor objects, which is afterward utilized to carry out final rich semantic labeling of various indoor scenes. With considerable speedup training, the proposed framework achieves effective performance equivalent to state-of-the-art for segmenting standard indoor scene dataset.
C1 [Sun, Yuliang; Chen, Jiazhou] Zhejiang Univ Technol, Coll Comp Sci & Technol, Hangzhou, Peoples R China.
   [Miao, Yongwei] Zhejiang Sci Tech Univ, Coll Informat Sci & Technol, Hangzhou, Peoples R China.
   [Pajarola, Renato] Univ Zurich, Dept Informat, CH-8050 Zurich, Switzerland.
C3 Zhejiang University of Technology; Zhejiang Sci-Tech University;
   University of Zurich
RP Miao, YW (corresponding author), Zhejiang Sci Tech Univ, Coll Informat Sci & Technol, Hangzhou, Peoples R China.
EM ywmiao2009@hotmail.com
RI Miao, Yongwei/ABH-1238-2021
OI Miao, Yongwei/0000-0002-5479-9060; Sun, Yuliang/0009-0000-1428-0857
FU National Natural Science Foundation of China [61972458]; Natural Science
   Foundation of Zhejiang Province [LY18F020035]; Science Foundation of
   Zhejiang Sci-Tech University [17032001-Y]
FX The authors declare that they have no conflict of interest. This
   research is supported by the National Natural Science Foundation of
   China under Grant No. 61972458, the Natural Science Foundation of
   Zhejiang Province under Grant No. LY18F020035, and the Science
   Foundation of Zhejiang Sci-Tech University under Grant No. 17032001-Y.
CR [Anonymous], CoRR abs/1511.07122
   [Anonymous], 2016, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, DOI [DOI 10.1109/CVPR.2016.170, 10.1109/CVPR.2016.170]
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Dai A, 2017, PROC CVPR IEEE, P2432, DOI 10.1109/CVPR.2017.261
   Defferrard Michael, 2016, ADV NEURAL INFORM PR, P3837, DOI DOI 10.5555/3157382.3157527
   Gao HY, 2019, PR MACH LEARN RES, V97
   Guo H, 2016, VISUAL COMPUT, V32, P1511, DOI 10.1007/s00371-015-1136-5
   Guo Y, 2018, VISUAL COMPUT, V34, P1325, DOI 10.1007/s00371-017-1416-3
   Guo YL, 2014, IEEE T PATTERN ANAL, V36, P2270, DOI 10.1109/TPAMI.2014.2316828
   Hou J, 2019, PROC CVPR IEEE, P4416, DOI 10.1109/CVPR.2019.00455
   Hu SM, 2020, IEEE T VIS COMPUT GR, V26, P2485, DOI 10.1109/TVCG.2018.2889944
   Huang JW, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130824
   Kipf TN, 2017, INT C LEARN REPR
   Landrieu L, 2019, PROC CVPR IEEE, P7432, DOI 10.1109/CVPR.2019.00762
   Landrieu L, 2018, PROC CVPR IEEE, P4558, DOI 10.1109/CVPR.2018.00479
   Li Y., 2016, P 4 INT C LEARNING R
   Li YY, 2015, COMPUT GRAPH FORUM, V34, P435, DOI 10.1111/cgf.12573
   Li YZ, 2018, ADV NEUR IN, V31
   Mattausch O, 2014, COMPUT GRAPH FORUM, V33, P11, DOI 10.1111/cgf.12286
   Maturana D, 2015, IEEE INT C INT ROBOT, P922, DOI 10.1109/IROS.2015.7353481
   Mura C, 2014, COMPUT GRAPH-UK, V44, P20, DOI 10.1016/j.cag.2014.07.005
   Nan LL, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366156
   Peng Nanyun, 2017, Transactions of the Association for Computational Linguistics, V5, P101, DOI 10.1162/tacl_a_00049
   Qi C., 2017, NeurIPS, P5105, DOI [10.1109/ CVPR.2017.16, DOI 10.1109/CVPR.2017.16]
   Qu BY, 2019, IEEE C EVOL COMPUTAT, P1267, DOI [10.1109/CEC.2019.8790286, 10.1109/cec.2019.8790286]
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Rusu RB, 2008, ROBOT AUTON SYST, V56, P927, DOI 10.1016/j.robot.2008.08.005
   Shi YF, 2018, LECT NOTES COMPUT SC, V11212, P767, DOI 10.1007/978-3-030-01237-3_46
   Shi YF, 2016, COMPUT GRAPH-UK, V55, P55, DOI 10.1016/j.cag.2015.11.003
   Su H, 2015, IEEE I CONF COMP VIS, P945, DOI 10.1109/ICCV.2015.114
   Sutskever I., 2014, ADV NEURAL INFORM PR, V4, P3104, DOI DOI 10.5555/2969033.2969173
   Tchapmi LP, 2017, INT CONF 3D VISION, P537, DOI 10.1109/3DV.2017.00067
   Te GS, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P746, DOI 10.1145/3240508.3240621
   Velickovic Petar, 2017, ARXIV171010903, DOI DOI 10.48550/ARXIV.1710.10903
   Vinyals O., 2015, ICLR
   Wang WY, 2018, PROC CVPR IEEE, P2569, DOI 10.1109/CVPR.2018.00272
   Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362
   Wirth F., 2019, IEEE INT VEH SYM, P1693, DOI [DOI 10.1109/ivs.2019.8814115, 10.1109/IVS.2019.8814115]
   Ying R, 2018, ADV NEUR IN, V31
   Zhang MH, 2018, AAAI CONF ARTIF INTE, P4438
   Zhou J, 2018, ARXIV181208434
   Zhou Y, 2018, PROC CVPR IEEE, P4490, DOI 10.1109/CVPR.2018.00472
   Zhu YK, 2017, INT CONF ACOUST SPEE, P5335, DOI 10.1109/ICASSP.2017.7953175
NR 43
TC 18
Z9 20
U1 0
U2 30
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2020
VL 36
IS 10-12
BP 2407
EP 2418
DI 10.1007/s00371-020-01892-8
EA JUL 2020
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA NW1CX
UT WOS:000548484700002
DA 2024-07-18
ER

PT J
AU Shajini, M
   Ramanan, A
AF Shajini, Majuran
   Ramanan, Amirthalingam
TI An improved landmark-driven and spatial-channel attentive convolutional
   neural network for fashion clothes classification
SO VISUAL COMPUTER
LA English
DT Article
DE Dilated convolutions; Fashion clothes classification; Landmark-driven;
   Spatial-channel attention
AB Fashion clothes classification encompasses spotting and identifying items of clothing in an image. This area of research has involved using deep neural networks to make an impact in the field of social media, e-commerce and fashion world. In this paper, we propose an attention-driven technique for tackling visual fashion clothes analysis in images, aiming to achieve clothing category classification and attribute prediction by producing regularised landmark layouts. For enhancing clothing classification, our fashion model incorporates two attention pipelines: landmark-driven attention and spatial-channel attention. These attention pipelines allow our model to represent multiscale contextual information of landmarks, thus improving the efficiency of classification by identifying the important features and locating where they exist in an input image. We evaluated the proposed network on two large-scale benchmark datasets: DeepFashion-C and fashion landmark detection (FLD). Experimental results show that the proposed architecture involving deep neural network outperforms other recently reported state-of-the-art techniques in the classification of fashion clothes.
C1 [Shajini, Majuran; Ramanan, Amirthalingam] Univ Jaffna, Fac Sci, Dept Comp Sci, Jaffna, Sri Lanka.
C3 University Jaffna
RP Shajini, M (corresponding author), Univ Jaffna, Fac Sci, Dept Comp Sci, Jaffna, Sri Lanka.
EM shayu.kiri@gmail.com; a.ramanan@univ.jfn.ac.lk
RI Ramanan, Amirthalingam/AAO-1674-2020
OI Ramanan, Amirthalingam/0000-0002-7110-1277
CR [Anonymous], CoRR abs/1511.07122
   Cao WC, 2019, APPL POWER ELECT CO, P3151, DOI 10.1109/APEC.2019.8722318
   Cao Z, 2021, IEEE T PATTERN ANAL, V43, P172, DOI 10.1109/TPAMI.2019.2929257
   Chen K., 2015, Abc-cnn: An attention based convolutional neural network for visual question answering
   Chen L, 2017, PROC CVPR IEEE, P6298, DOI 10.1109/CVPR.2017.667
   Chen M., 2019, P IEEE INT C COMP VI
   Chen YL, 2019, FRONT GENET, V10, DOI 10.3389/fgene.2019.01110
   Cho H., 2019, P IEEE INT C COMP VI
   Corbière C, 2017, IEEE INT CONF COMP V, P2268, DOI 10.1109/ICCVW.2017.266
   Fan DP, 2019, PROC CVPR IEEE, P8546, DOI 10.1109/CVPR.2019.00875
   Gao JY, 2019, NEUROCOMPUTING, V363, P1, DOI 10.1016/j.neucom.2019.08.018
   He K., 2016, PROC CVPR IEEE, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang CQ, 2019, IEEE T CYBERNETICS, V49, P3744, DOI 10.1109/TCYB.2018.2850745
   Huang JS, 2015, IEEE I CONF COMP VIS, P1062, DOI 10.1109/ICCV.2015.127
   Huang Y, 2018, IEEE T PATTERN ANAL, V40, P1015, DOI 10.1109/TPAMI.2017.2701380
   Jaderberg Max, 2015, ADV NEURAL INFORM PR, P8
   Kabbai L, 2019, VISUAL COMPUT, V35, P679, DOI 10.1007/s00371-018-1503-0
   Kiapour MH, 2015, IEEE I CONF COMP VIS, P3343, DOI 10.1109/ICCV.2015.382
   Kingma D. P., 2014, arXiv
   Lee S, 2019, ELECTRON LETT, V55, P745, DOI 10.1049/el.2019.0660
   Lee S, 2019, IEEE I CONF COMP VIS, P4412, DOI 10.1109/ICCV.2019.00451
   Li P., 2019, ARXIV190110172
   Li YX, 2019, IEEE INT CON MULTI, P820, DOI 10.1109/ICME.2019.00146
   Liu J., 2018, EUROPEAN C COMPUTER, P30
   Liu ZW, 2016, PROC CVPR IEEE, P1096, DOI 10.1109/CVPR.2016.124
   Liu ZW, 2016, LECT NOTES COMPUT SC, V9906, P229, DOI 10.1007/978-3-319-46475-6_15
   Paszke Adam, 2017, NIPS W
   Qi SY, 2018, LECT NOTES COMPUT SC, V11213, P407, DOI 10.1007/978-3-030-01240-3_25
   Selvaraju RR, 2020, INT J COMPUT VISION, V128, P336, DOI [10.1007/s11263-019-01228-7, 10.1109/ICCV.2017.74]
   Shao JQ, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18093039
   Shen ZY, 2019, IEEE I CONF COMP VIS, P5571, DOI 10.1109/ICCV.2019.00567
   Shih KJ, 2016, PROC CVPR IEEE, P4613, DOI 10.1109/CVPR.2016.499
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Wang WG, 2019, IEEE I CONF COMP VIS, P5702, DOI 10.1109/ICCV.2019.00580
   Wang WG, 2018, PROC CVPR IEEE, P4271, DOI 10.1109/CVPR.2018.00449
   Yan SJ, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P172, DOI 10.1145/3123266.3123276
NR 37
TC 14
Z9 14
U1 1
U2 28
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2021
VL 37
IS 6
BP 1517
EP 1526
DI 10.1007/s00371-020-01885-7
EA JUN 2020
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SO2LW
UT WOS:000544152300001
DA 2024-07-18
ER

PT J
AU El Ouafdi, AF
   El Houari, H
   Ziou, D
AF El Ouafdi, A. F.
   El Houari, H.
   Ziou, D.
TI Adaptive estimation of Hodge star operator on simplicial surfaces
SO VISUAL COMPUTER
LA English
DT Article
DE Discrete Hodge operator; Physical-based modeling; Harmonic form
ID DISCRETIZATION; COMPUTATION; EQUATIONS
AB The Hodge star operator is a fundamental component of the second-order differential operators that bridges constitutive physical laws, as a matter of fact, it plays a central role in geometry processing and physical simulation. To be admissible, the discrete Hodge operators should be regular, symmetric, sparse and positive definite. Unfortunately, the last criteria is rarely met in the literature, which leads to inconsistent physical simulation behavior. In this paper, we exploit the intrinsic relationship between Hodge operator and the physical Fourier's constitutive laws, to construct an adaptive discrete Hodge operator by expressing the Fourier's laws on a surface mesh. As by-product, the new discrete Hodge operator is diagonal, regular and positive definite. Various comparative examples are presented to demonstrate the performance of our approach. The results show that the proposed operator performs better compared with the standard discrete Hodge.
C1 [El Ouafdi, A. F.] Ibn Zohr Univ, Fac Sci Agadir, Dept Comp Sci, LabSIV, Agadir, Morocco.
   [El Houari, H.] CUNY, LaGuardia Community Coll, New York, NY 10021 USA.
   [Ziou, D.] Univ Sherbrooke, Dept Informat, Sherbrooke, PQ, Canada.
C3 Ibn Zohr University of Agadir; City University of New York (CUNY)
   System; University of Sherbrooke
RP El Ouafdi, AF (corresponding author), Ibn Zohr Univ, Fac Sci Agadir, Dept Comp Sci, LabSIV, Agadir, Morocco.
EM elouafdi@gmail.com; helhouari@lagcc.cuny.edu; Djemel.Ziou@USherbrooke.ca
OI El Ouafdi, Ahmed Fouad/0000-0003-1173-2170
CR [Anonymous], 2017, FINITE DIFFERENCE CO
   [Anonymous], 1991, COURSE MATH STUDENTS
   [Anonymous], 1957, GEOMETRIC INTEGRATIO
   Auchmann B, 2006, IEEE T MAGN, V42, P643, DOI 10.1109/TMAG.2006.870932
   Auclair-Fortier MF, 2006, IEEE T IMAGE PROCESS, V15, P2558, DOI 10.1109/TIP.2006.877410
   Bobenko AI, 2007, DISCRETE COMPUT GEOM, V38, P740, DOI 10.1007/s00454-007-9006-1
   Bossavit A, 2000, IEEE T MAGN, V36, P861, DOI 10.1109/20.877580
   Bossavit A., 2000, J. Japan. Soc. Appl. Electromagn. Mech., V8, P203
   Desbrun M, 2006, ACM SIGGRAPH 2006 CO, P39, DOI DOI 10.1145/1185657.1185665
   El Ouafdi AF, 2015, VISUAL COMPUT, V31, P295, DOI 10.1007/s00371-014-0922-9
   Geuzaine C, 2009, INT J NUMER METH ENG, V79, P1309, DOI 10.1002/nme.2579
   Gillette A, 2011, COMPUT AIDED DESIGN, V43, P1213, DOI 10.1016/j.cad.2011.06.017
   Gu X.D., 2008, Computational Conformal Geometry
   He B, 2006, PHYS LETT A, V349, P1, DOI 10.1016/j.physleta.2005.09.002
   Hiptmair R, 2001, J ELECTROMAGNET WAVE, V15, P343, DOI 10.1163/156939301X00508
   Hirani AN, 2013, COMPUT AIDED DESIGN, V45, P540, DOI 10.1016/j.cad.2012.10.038
   Hirani Anil Nirmal, 2003, THESIS
   Mattiussi C, 2002, ADV IMAG ELECT PHYS, V121, P143
   Mohamed MS, 2016, COMPUT AIDED DESIGN, V78, P118, DOI 10.1016/j.cad.2016.05.002
   Mullen P, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964998
   Pellikka M, 2013, SIAM J SCI COMPUT, V35, pB1195, DOI 10.1137/130906556
   Pinkall U., 1993, Exp. Math., V2, P15, DOI 10.1080/10586458.1993.10504266
   Poelke K, 2016, COMPUT AIDED DESIGN, V78, P126, DOI 10.1016/j.cad.2016.05.004
   Polthier K, 2003, VISUALIZATION AND MATHEMATICS III, P113
   Sharp N, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3322979
   Shigeyuki M, 2001, AM MATH SOC, V201
   Tarhasaari T, 1999, IEEE T MAGN, V35, P1494, DOI 10.1109/20.767250
   Teixeira F.L., 2001, Prog. Electromagn. Res, V32, P171, DOI DOI 10.2528/PIER00080107
   Tonti E, 2013, MATH STRUCTURE CLASS
   WEILAND T, 1977, AEU-INT J ELECTRON C, V31, P116
   Weiland T, 1996, INT J NUMER MODEL EL, V9, P295, DOI 10.1002/(SICI)1099-1204(199607)9:4<295::AID-JNM240>3.0.CO;2-8
   Wilson SO, 2007, TOPOL APPL, V154, P1898, DOI 10.1016/j.topol.2007.01.017
   Yan DM, 2016, IEEE T VIS COMPUT GR, V22, P2136, DOI 10.1109/TVCG.2015.2505279
NR 33
TC 2
Z9 2
U1 0
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2021
VL 37
IS 6
BP 1433
EP 1445
DI 10.1007/s00371-020-01879-5
EA JUN 2020
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SO2LW
UT WOS:000542085500001
DA 2024-07-18
ER

PT J
AU Tongbuasirilai, T
   Unger, J
   Kronander, J
   Kurt, M
AF Tongbuasirilai, Tanaboon
   Unger, Jonas
   Kronander, Joel
   Kurt, Murat
TI Compact and intuitive data-driven BRDF models
SO VISUAL COMPUTER
LA English
DT Article
DE Reflectance modeling; Rendering; Computer graphics
ID REPRESENTATION; REFLECTANCE
AB Measured materials are rapidly becoming a core component in the photo-realistic image synthesis pipeline. The reason is that data-driven models can easily capture the underlying, fine details that represent the visual appearance of materials, which can be difficult or even impossible to model by hand. There are, however, a number of key challenges that need to be solved in order to enable efficient capture, representation and interaction with real materials. This paper presents two new data-driven BRDF models specifically designed for 1D separability. The proposed 3D and 2D BRDF representations can be factored into three or two 1D factors, respectively, while accurately representing the underlying BRDF data with only small approximation error. We evaluate the models using different parameterizations with different characteristics and show that both the BRDF data itself and the resulting renderings yield more accurate results in terms of both numerical errors and visual results compared to previous approaches. To demonstrate the benefit of the proposed factored models, we present a new Monte Carlo importance sampling scheme and give examples of how they can be used for efficient BRDF capture and intuitive editing of measured materials.
C1 [Tongbuasirilai, Tanaboon; Unger, Jonas; Kronander, Joel] Linkoping Univ, Linkoping, Sweden.
   [Kurt, Murat] Ege Univ, Int Comp Inst, Izmir, Turkey.
C3 Linkoping University; Ege University
RP Tongbuasirilai, T (corresponding author), Linkoping Univ, Linkoping, Sweden.
EM tanaboon.tongbuasirilai@liu.se; jonas.unger@liu.se;
   murat.kurt@ege.edu.tr
RI Tongbuasirilai, Tanaboon/JDM-2247-2023; Kurt, Murat/AAF-9554-2020
OI Tongbuasirilai, Tanaboon/0000-0002-3239-8581; Kurt,
   Murat/0000-0002-3236-5595
FU Swedish Science Council [VR-2015-05180]; strategic research environment
   ELLIIT; Scientific and Technical Research Council of Turkey [115E203];
   Scientific Research Projects Directorate of Ege University
   [2015/BL/043]; Swedish Research Council [2015-05180] Funding Source:
   Swedish Research Council
FX This work was supported by the Swedish Science Council through grant
   VR-2015-05180, the strategic research environment ELLIIT, the Scientific
   and Technical Research Council of Turkey (Project No: 115E203), and the
   Scientific Research Projects Directorate of Ege University (Project No:
   2015/BL/043).
CR Aittala M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766967
   Andersson CA, 2000, CHEMOMETR INTELL LAB, V52, P1, DOI 10.1016/S0169-7439(00)00071-X
   [Anonymous], 1977, P 4 ANN C COMPUTER G, DOI DOI 10.1145/965141.563893
   [Anonymous], 1982, ACM T GRAPHIC, DOI DOI 10.1145/357290.357293
   Ashikhmin M., 2000, Journal of Graphics Tools, V5, P25, DOI 10.1080/10867651.2000.10487522
   Bagher MM, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2907941
   Barla P., 2015, P 3 WORKSH MAT APP M, P9
   Ben-Artzi A, 2006, ACM T GRAPHIC, V25, P945, DOI 10.1145/1141911.1141979
   Bilgili A, 2011, COMPUT GRAPH FORUM, V30, P2427, DOI 10.1111/j.1467-8659.2011.02072.x
   CARROLL JD, 1970, PSYCHOMETRIKA, V35, P283, DOI 10.1007/BF02310791
   Cichocki A., 2009, NONNEGATIVE MATRIX T, DOI 10.1002/9780470747278
   Edwards D, 2006, ACM T GRAPHIC, V25, P1, DOI 10.1145/1122501.1122502
   Georgoulis S, 2015, IEEE I CONF COMP VIS, P3559, DOI 10.1109/ICCV.2015.406
   Guarnera D, 2016, COMPUT GRAPH FORUM, V35, P625, DOI 10.1111/cgf.12867
   Harshman Richard A., 1970, FDN PARAFAC PROCEDUR
   Holzschuch N, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073621
   Kajiya J. T., 1986, SIGGRAPH, P143, DOI 10.1145/15886.15902
   Kautz J, 1999, SPRING EUROGRAP, P247
   Kurt M., 2013, TPCG, P85, DOI 10.2312/LocalChapterEvents.TPCG.TPCG13.085-092.URL
   Lawrence J, 2004, ACM T GRAPHIC, V23, P496, DOI 10.1145/1015706.1015751
   Lawrence J, 2006, ACM T GRAPHIC, V25, P735, DOI 10.1145/1141911.1141949
   Lee DD, 2001, ADV NEUR IN, V13, P556
   Lensch H.P.A., 2005, ACM SIGGRAPH 2005 CO
   Löw J, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2077341.2077350
   Luksch C, 2014, VISUAL COMPUT, V30, P717, DOI 10.1007/s00371-014-0958-x
   Matusik W, 2003, ACM T GRAPHIC, V22, P759, DOI 10.1145/882262.882343
   McCool MD, 2001, COMP GRAPH, P171, DOI 10.1145/383259.383276
   Ngan A., 2005, Eurographics Symposium on Rendering, P117, DOI [DOI 10.2312/EGWR/EGSR05/117-1261,2,8, DOI 10.2312/EGWR/EGSR05/117-126]
   Ngan A., 2004, ACM SIGGRAPH 2004 SK, P90
   NICODEMUS FE, 1970, APPL OPTICS, V9, P1474, DOI 10.1364/AO.9.001474
   Nielsen JB, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818085
   Pacanowski R, 2012, IEEE T VIS COMPUT GR, V18, P1824, DOI 10.1109/TVCG.2012.73
   Peers P, 2006, ACM T GRAPHIC, V25, P746, DOI 10.1145/1141911.1141950
   Pharr M., 2010, PHYS BASED RENDERING
   PHONG BT, 1975, COMMUN ACM, V18, P311, DOI 10.1145/360825.360839
   Richardson I.E.G., 2002, Video Codec Design: Developing Image and Video Compression Systems
   Romeiro F, 2008, LECT NOTES COMPUT SC, V5305, P859, DOI 10.1007/978-3-540-88693-8_63
   Ruiters R., 2010, CG20101 U BONN
   Rusinkiewicz S. M., 1998, Rendering Techniques '98. Proceedings of the Eurographics Workshop, P11
   Sbert M, 2018, VISUAL COMPUT, V34, P843, DOI 10.1007/s00371-018-1522-x
   Sbert M, 2017, VISUAL COMPUT, V33, P845, DOI 10.1007/s00371-017-1398-1
   Schwenk K, 2010, GRAPP 2010: PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON COMPUTER GRAPHICS THEORY AND APPLICATIONS, P165
   Stark MM, 2005, IEEE T VIS COMPUT GR, V11, P126, DOI 10.1109/TVCG.2005.26
   Sun X, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239478
   TUCKER LR, 1966, PSYCHOMETRIKA, V31, P279, DOI 10.1007/BF02289464
   Veach E.:, 1998, THESIS
   WARD GJ, 1992, COMP GRAPH, V26, P265, DOI 10.1145/142920.134078
NR 47
TC 9
Z9 9
U1 2
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2020
VL 36
IS 4
BP 855
EP 872
DI 10.1007/s00371-019-01664-z
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KW0AL
UT WOS:000520835800015
OA hybrid, Green Published
DA 2024-07-18
ER

PT J
AU Zhang, ZQ
   Zhang, L
   Zhang, M
AF Zhang, Zhengqi
   Zhang, Li
   Zhang, Meng
TI Dissimilarity-based nearest neighbor classifier for single-sample face
   recognition
SO VISUAL COMPUTER
LA English
DT Article
DE Face recognition; Single-sample per person; Dissimilarity; Image
   partitioning
ID ONE TRAINING IMAGE; 2-DIMENSIONAL PCA; REPRESENTATION; ROBUST
AB In single-sample face recognition (SSFR) tasks, the nearest neighbor classifier (NNC) is the most popular method for its simplicity in implementation. However, in complex situations with light, posture, expression, and obscuration, NNC cannot achieve good recognition performance when applying common distance measurements, such as the Euclidean distance. Thus, this paper proposes a novel distance measurement scheme for NNC and applies it to SSFR. The proposed method, called dissimilarity-based nearest neighbor classifier (DNNC), first segments each (training or test) image into non-overlapping patches with a given size and then generates an ordered image patch set. The dissimilarities between the given test image patch set and the training image patch sets are computed and taken as the distance measurement of NNC. The smaller the dissimilarity of image patch sets is, the closer is the distance from the test image to the training image. Therefore, the category of the test image can be determined according to the smallest dissimilarity. Extensive experiments on the AR face database demonstrate the effectiveness of DNNC, especially for the case of obscuration.
C1 [Zhang, Zhengqi; Zhang, Li; Zhang, Meng] Soochow Univ, Sch Comp Sci & Technol, Joint Int Res Lab Machine Learning & Neuromorph C, Suzhou 215006, Jiangsu, Peoples R China.
   [Zhang, Li] Soochow Univ, Prov Key Lab Comp Informat Proc Technol, Suzhou 215006, Jiangsu, Peoples R China.
C3 Soochow University - China; Soochow University - China
RP Zhang, L (corresponding author), Soochow Univ, Sch Comp Sci & Technol, Joint Int Res Lab Machine Learning & Neuromorph C, Suzhou 215006, Jiangsu, Peoples R China.; Zhang, L (corresponding author), Soochow Univ, Prov Key Lab Comp Informat Proc Technol, Suzhou 215006, Jiangsu, Peoples R China.
EM 20174227025@stu.suda.edu.cn; zhangliml@suda.edu.cn;
   20184227025@stu.suda.edu.cn
OI Zhang, Li/0000-0001-7914-0679
FU Natural Science Foundation of the Jiangsu Higher Education Institutions
   of China [19KJA550002]; Six Talent Peak Project of Jiangsu Province of
   China [XYDXX-054]; Priority Academic Program Development of Jiangsu
   Higher Education Institutions; Collaborative Innovation Center of Novel
   Software Technology and Industrialization
FX We would like to thank three anonymous reviewers and Editor
   NadiaMagnenat-Thalmann for their valuable comments and suggestions,
   which have significantly improved this paper. This work was supported in
   part by the Natural Science Foundation of the Jiangsu Higher Education
   Institutions of China under Grant No. 19KJA550002, by the Six Talent
   Peak Project of Jiangsu Province of China under Grant No. XYDXX-054, by
   the Priority Academic Program Development of Jiangsu Higher Education
   Institutions, and by the Collaborative Innovation Center of Novel
   Software Technology and Industrialization.
CR [Anonymous], INFORM TECHNOLOGY
   Benavente R, 1998, 24 COMP VIS CTR
   Chen SC, 2004, PATTERN RECOGN LETT, V25, P1173, DOI 10.1016/j.patrec.2004.03.012
   Chen SC, 2004, PATTERN RECOGN, V37, P1553, DOI 10.1016/j.patcog.2003.12.010
   Cheng Y, 2017, VISUAL COMPUT, V33, P1483, DOI 10.1007/s00371-017-1357-x
   Chu YJ, 2019, VISUAL COMPUT, V35, P239, DOI 10.1007/s00371-017-1468-4
   Deng WH, 2012, IEEE T PATTERN ANAL, V34, P1864, DOI 10.1109/TPAMI.2012.30
   Deng WH, 2010, PATTERN RECOGN, V43, P1748, DOI 10.1016/j.patcog.2009.12.004
   Ding C, 2017, SIGNAL IMAGE VIDEO P, V11, P1213, DOI 10.1007/s11760-017-1077-8
   Ding RX, 2015, J VIS COMMUN IMAGE R, V30, P35, DOI 10.1016/j.jvcir.2015.03.001
   Ding SF, 2018, SOFT COMPUT, V22, P4573, DOI 10.1007/s00500-017-2640-5
   Fakhari F, 2017, IET IMAGE PROCESS, V11, P1041, DOI 10.1049/iet-ipr.2017.0104
   Gottumukkal R, 2004, PATTERN RECOGN LETT, V25, P429, DOI 10.1016/j.patrec.2003.11.005
   Gou JP, 2019, EXPERT SYST APPL, V115, P356, DOI 10.1016/j.eswa.2018.08.021
   He XF, 2005, IEEE T PATTERN ANAL, V27, P328, DOI 10.1109/TPAMI.2005.55
   Ji HK, 2017, PATTERN RECOGN, V62, P125, DOI 10.1016/j.patcog.2016.08.007
   Kano M, 2002, AICHE J, V48, P1231, DOI 10.1002/aic.690480610
   [李俊霞 Li Junxia], 2015, [计算机应用与软件, Computer Applications and Software], V32, P310
   [林静 Lin Jing], 2017, [计算机工程与应用, Computer Engineering and Application], V53, P123
   Liu F, 2015, LECT NOTES COMPUT SC, V9010, P85, DOI 10.1007/978-3-319-16634-6_7
   Lu JW, 2013, IEEE T PATTERN ANAL, V35, P39, DOI 10.1109/TPAMI.2012.70
   Mehrasa N, 2017, J CENT SOUTH UNIV, V24, P2853, DOI 10.1007/s11771-017-3700-9
   Pan J, 2016, IEEE ACCESS, V4, P2873, DOI 10.1109/ACCESS.2016.2574366
   Pei TW, 2017, PATTERN RECOGN, V64, P305, DOI 10.1016/j.patcog.2016.11.016
   Qin J., 2005, INT C MACH LEARN CYB, V8
   Tan XY, 2005, IEEE T NEURAL NETWOR, V16, P875, DOI 10.1109/TNN.2005.849817
   TURK M, 1991, J COGNITIVE NEUROSCI, V3, P71, DOI 10.1162/jocn.1991.3.1.71
   Wang H, 2019, SIGNAL IMAGE VIDEO P, V13, P985, DOI 10.1007/s11760-019-01436-1
   [魏明俊 Wei Mingjun], 2017, [计算机科学与探索, Journal of Frontiers of Computer Science & Technology], V11, P1505
   Wright J, 2009, IEEE T PATTERN ANAL, V31, P210, DOI 10.1109/TPAMI.2008.79
   [吴煌鹏 Wu Huangpeng], 2014, [模式识别与人工智能, Pattern Recognition and Artificial Intelligence], V27, P894
   Wu JX, 2002, PATTERN RECOGN LETT, V23, P1711, DOI 10.1016/S0167-8655(02)00134-4
   Xiang FT, 2018, IET IMAGE PROCESS, V12, P345, DOI 10.1049/iet-ipr.2017.0327
   Yang J, 2004, IEEE T PATTERN ANAL, V26, P131, DOI 10.1109/TPAMI.2004.1261097
   Yang M, 2013, IEEE I CONF COMP VIS, P689, DOI 10.1109/ICCV.2013.91
   [于延 Yu Yan], 2014, [计算机应用与软件, Computer Applications and Software], V31, P173
   Yu YF, 2017, PATTERN RECOGN, V66, P302, DOI 10.1016/j.patcog.2017.01.021
   Zhang DQ, 2005, NEUROCOMPUTING, V69, P224, DOI 10.1016/j.neucom.2005.06.004
   Zhang DQ, 2005, APPL MATH COMPUT, V163, P895, DOI 10.1016/j.amc.2004.04.016
   Zhang L, 2011, IEEE I CONF COMP VIS, P471, DOI 10.1109/ICCV.2011.6126277
   Zhang L, 2011, PATTERN RECOGN, V44, P97, DOI 10.1016/j.patcog.2010.07.021
   Zhang Z., 2019, IEEE T CIRCUITS SYST
   Zhang Z, 2018, IEEE T NEUR NET LEAR, V29, P3798, DOI 10.1109/TNNLS.2017.2740224
   Zhang Z, 2016, IEEE T SIGNAL PROCES, V64, P3790, DOI 10.1109/TSP.2016.2550016
   Zhou B, 2018, LASER OPTOELECTRON P, V55, DOI 10.3788/LOP55.031010
   Zhou SF, 2018, SOFTW GUIDE, V2, P15
   Zhu B, 2016, IEEE INT CONF ELECTR, P34, DOI 10.1109/ICEIEC.2016.7589681
   Zhuang L, 2013, IEEE COMPUT VIS PATT, V114, P3546
NR 48
TC 5
Z9 5
U1 1
U2 12
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2021
VL 37
IS 4
SI SI
BP 673
EP 684
DI 10.1007/s00371-020-01827-3
EA MAR 2020
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RL0LI
UT WOS:000518343100001
DA 2024-07-18
ER

PT J
AU Altantawy, DA
   Saleh, AI
   Kishk, SS
AF Altantawy, Doaa A.
   Saleh, Ahmed I.
   Kishk, Sherif S.
TI Texture-guided depth upsampling using Bregman split: a clustering
   graph-based approach
SO VISUAL COMPUTER
LA English
DT Article
DE Structured sparsity; Low-rank prior; Texture guidance; Joint dictionary
   learning; Depth upsampling; Incoherence self-learning
ID SPARSE; SUPERRESOLUTION; COLOR; RECOVERY
AB Recently, RGB-D sensors have gained significant popularity due to their affordable cost. Compared to their associated high-resolution (HR) color images, their depth maps counterparts are typically with lower resolution. In addition, the quality of those maps is still inadequate for further applications due to the existing holes, noises and artifacts. In this paper, we propose a clustering graph-based framework for depth map super-resolution. This framework uses the guidance of HR textured-intensity layer to support and compel high-frequency details in the depth map recovery process. This textured layer is extracted from the consolidated HR intensity image in a texture-structure separation process via a new relative total variation technique. Furthermore, instead of the standard sparse representation that does not consider the local structural information effectively, we propose a novel clustered-graph sparse representation with a low-rank prior. With this joint representation, any signal can be coded effectively, as the low-rank property reveals the global structure information while the intrinsic information is kept by a novel multiclass incoherence self-learning between classes. At the same time, a grouped coherence within each class dictionary is preserved. We optimize that joint objective function using state-of-the-art split Bregman algorithm. Experimental results on Middleburry 2005, 2007, 2014 and real-world datasets demonstrate that the proposed algorithm is very efficient and outperforms the state-of-the-art approaches in terms of objective and subjective quality.
C1 [Altantawy, Doaa A.; Kishk, Sherif S.] Mansoura Univ, Fac Engn, Elect & Commun Engn Dept, 60 El Gomhoria St, Mansoura, Egypt.
   [Saleh, Ahmed I.] Mansoura Univ, Comp Engn & Syst Dept, Fac Engn, 60 El Gomhoria St, Mansoura, Egypt.
C3 Egyptian Knowledge Bank (EKB); Mansoura University; Egyptian Knowledge
   Bank (EKB); Mansoura University
RP Kishk, SS (corresponding author), Mansoura Univ, Fac Engn, Elect & Commun Engn Dept, 60 El Gomhoria St, Mansoura, Egypt.
EM doaa1adel@mans.edu.eg; aisaleh@yahoo.com; shkishk@mans.edu.eg
RI Altantawy, Doaa A./X-3097-2018
OI Altantawy, Doaa A./0000-0001-6476-2934
CR Al Ismaeil Kassem, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P8, DOI 10.1109/CVPRW.2015.7301389
   [Anonymous], 2012, INT C INT MULT COMP
   Bao LC, 2014, IEEE T IMAGE PROCESS, V23, P555, DOI 10.1109/TIP.2013.2291328
   Boyd S, 2011, TRENDS MACH LEARN, V3, P1, DOI DOI 10.1561/2200000016
   Cai JF, 2010, SIAM J OPTIMIZ, V20, P1956, DOI 10.1137/080738970
   Cai ZY, 2017, MULTIMED TOOLS APPL, V76, P4313, DOI 10.1007/s11042-016-3374-6
   Chen CY, 2015, ACM T INTEL SYST TEC, V6, DOI 10.1145/2700475
   Cho H, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601188
   Dai QQ, 2017, IEEE T IMAGE PROCESS, V26, P765, DOI 10.1109/TIP.2016.2631339
   Ding K, 2014, VISUAL COMPUT, V30, P1311, DOI 10.1007/s00371-013-0888-z
   Eichhardt I, 2017, MACH VISION APPL, V28, P267, DOI 10.1007/s00138-017-0831-9
   Ferstl D, 2013, IEEE I CONF COMP VIS, P993, DOI 10.1109/ICCV.2013.127
   Garcia F, 2015, IMAGE VISION COMPUT, V41, P26, DOI 10.1016/j.imavis.2015.06.008
   Gilboa G, 2002, LECT NOTES COMPUT SC, V2350, P399
   Goldstein T, 2009, SIAM J IMAGING SCI, V2, P323, DOI 10.1137/080725891
   Ham B, 2018, IEEE T PATTERN ANAL, V40, P192, DOI 10.1109/TPAMI.2017.2669034
   Han JG, 2013, IEEE T CYBERNETICS, V43, P1318, DOI 10.1109/TCYB.2013.2265378
   Hui TW, 2016, LECT NOTES COMPUT SC, V9907, P353, DOI 10.1007/978-3-319-46487-9_22
   Jiang ZY, 2018, IEEE T IMAGE PROCESS, V27, P2587, DOI 10.1109/TIP.2018.2806089
   Jung C, 2017, J VIS COMMUN IMAGE R, V42, P132, DOI 10.1016/j.jvcir.2016.11.009
   Kiechle M, 2013, IEEE I CONF COMP VIS, P1545, DOI 10.1109/ICCV.2013.195
   Kopf J, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1276377.1276497, 10.1145/1239451.1239547]
   Kwon H, 2015, PROC CVPR IEEE, P159, DOI 10.1109/CVPR.2015.7298611
   Li Y, 2016, LECT NOTES COMPUT SC, V9907, P717, DOI 10.1007/978-3-319-46487-9_44
   Liu FY, 2016, IEEE T PATTERN ANAL, V38, P2024, DOI 10.1109/TPAMI.2015.2505283
   Liu MY, 2013, PROC CVPR IEEE, P169, DOI 10.1109/CVPR.2013.29
   Liu W, 2017, IEEE T CIRC SYST VID, V27, P2072, DOI 10.1109/TCSVT.2016.2556598
   Liu W, 2017, IEEE T IMAGE PROCESS, V26, DOI 10.1109/TIP.2016.2612826
   Lu X, 2017, TOOLS APPL, V1, P1
   Mac Aodha O, 2012, LECT NOTES COMPUT SC, V7574, P71, DOI 10.1007/978-3-642-33712-3_6
   Min DB, 2014, IEEE T IMAGE PROCESS, V23, P5638, DOI 10.1109/TIP.2014.2366600
   Ning Q, 2013, IEEE SIGNAL PROC LET, V20, P399, DOI 10.1109/LSP.2013.2242198
   Park J, 2011, IEEE I CONF COMP VIS, P1623, DOI 10.1109/ICCV.2011.6126423
   Song XB, 2017, LECT NOTES COMPUT SC, V10114, P360, DOI 10.1007/978-3-319-54190-7_22
   Starck JL, 2005, IEEE T IMAGE PROCESS, V14, P1570, DOI 10.1109/TIP.2005.852206
   Subr K, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618493
   Tropp JA, 2010, P IEEE, V98, P948, DOI 10.1109/JPROC.2010.2044010
   Tseng C.-M., 2016, Proceedings of the Workshop on Electric Vehicle Systems, Data, and Applications p, P1
   Wang YK, 2014, VISUAL COMPUT, V30, P1157, DOI 10.1007/s00371-013-0896-z
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Xie J, 2016, IEEE T IMAGE PROCESS, V25, P428, DOI 10.1109/TIP.2015.2501749
   Xu L, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024208
   Yang H, 2017, LECT NOTES COMPUT SC, V10666, P85, DOI 10.1007/978-3-319-71607-7_8
   Yang JC, 2010, IEEE T IMAGE PROCESS, V19, P2861, DOI 10.1109/TIP.2010.2050625
   Yang JY, 2014, IEEE T IMAGE PROCESS, V23, P3443, DOI 10.1109/TIP.2014.2329776
   Yuan L, 2017, J VIS COMMUN IMAGE R, V46, P280, DOI 10.1016/j.jvcir.2017.04.012
   Zhang HT, 2018, MULTIMED TOOLS APPL, V77, P9003, DOI 10.1007/s11042-017-4791-x
   Zhang HT, 2017, IEEE IMAGE PROC, P4043, DOI 10.1109/ICIP.2017.8297042
   Zhang Q, 2014, LECT NOTES COMPUT SC, V8691, P815, DOI 10.1007/978-3-319-10578-9_53
   Zhang YD, 2017, IEEE IMAGE PROC, P972, DOI 10.1109/ICIP.2017.8296426
   Zhang Z, 2015, IEEE ACCESS, V3, P490, DOI 10.1109/ACCESS.2015.2430359
   Zheng H, 2013, IEEE IMAGE PROC, P957, DOI 10.1109/ICIP.2013.6738198
   Zheng M, 2011, IEEE T IMAGE PROCESS, V20, P1327, DOI 10.1109/TIP.2010.2090535
   Zhu J, 2018, LECT NOTES COMPUT SC, V10704, P93, DOI 10.1007/978-3-319-73603-7_8
   Zhu L, 2016, COMPUT GRAPH FORUM, V35, P217, DOI 10.1111/cgf.13019
   Zhu LF, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366146
NR 56
TC 3
Z9 3
U1 3
U2 16
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2020
VL 36
IS 2
BP 333
EP 359
DI 10.1007/s00371-018-1611-x
PG 27
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KJ2TH
UT WOS:000511910300009
DA 2024-07-18
ER

PT J
AU Chen, Z
   Liu, PZ
   Du, YZ
   Luo, YM
   Guo, JM
AF Chen, Zhi
   Liu, Peizhong
   Du, Yongzhao
   Luo, Yanmin
   Guo, Jing-Ming
TI Long-term correlation tracking via spatial-temporal context
SO VISUAL COMPUTER
LA English
DT Article
DE Visual tracking; Discriminative correlation filters; Spatial-temporal
   context; Long-term tracking
ID ROBUST OBJECT TRACKING
AB In this paper, we mainly deal with the problems of long-term visual tracking while the target objects undergo sophisticated scenarios such as occlusion, out-of-view, and scale changes. We employ two discriminative correlation filters (DCFs) for achieving long-term object tracking, which is performed by learning a spatial-temporal context correlation filter for translation estimation. As for the scale estimation, which is achieved by learning a scale DCF centered on the estimated target position to estimate scale from the best confident results. In addition, we proposed an efficient model update and redetecting activate strategy to avoid unrecoverable drift due to noisy updates, and achieve robust long-term tracking in the case of tracking failure. We evaluate our algorithm carry on OTB benchmark datasets, and the tracking results of both qualitative and quantitative evaluations on challenging sequences demonstrate that the proposed algorithm performs superiorly against several state-of-the-art DCFs methods including some methods which follow deep learning paradigm.
C1 [Chen, Zhi; Liu, Peizhong; Du, Yongzhao] Huaqiao Univ, Coll Engn, Quanzhou, Fujian, Peoples R China.
   [Luo, Yanmin] Huaqiao Univ, Coll Comp Sci & Technol, Xiamen, Fujian, Peoples R China.
   [Liu, Peizhong; Du, Yongzhao] Huaqiao Univ, Res Ctr Appl Stat & Big Data, 668 Jimei Ave, Xiamen 361021, Peoples R China.
   [Guo, Jing-Ming] Natl Taiwan Univ Sci & Technol, Dept Elect Engn, Taipei, Taiwan.
C3 Huaqiao University; Huaqiao University; Huaqiao University; National
   Taiwan University of Science & Technology
RP Liu, PZ (corresponding author), Huaqiao Univ, Coll Engn, Quanzhou, Fujian, Peoples R China.; Liu, PZ (corresponding author), Huaqiao Univ, Res Ctr Appl Stat & Big Data, 668 Jimei Ave, Xiamen 361021, Peoples R China.
EM marico2018@163.com; pzliu@hqu.edu.cn; yongzhaodu@126.com;
   lym@hqu.edu.cn; jmguo@seed.net.tw
OI Liu, Peizhong/0000-0001-8785-0195
FU Promotion Program for Young and Middle-aged Teacher in Science and
   Technology Research of Huaqiao University [ZQN-PY518]; National Natural
   Science Foundation of China [61605048]; Natural Science Foundation of
   Fujian Province, China [2015J01256, 2016J01300]; Fujian Provincial Big
   Data Research Institute of Intelligent Manufacturing; Quanzhou
   scientific and technological planning projects [2018C113R, 2017G024];
   Subsidized Project for Postgraduates' Innovative Fund in Scientific
   Research of Huaqiao University [1611422001]
FX This work was supported by Promotion Program for Young and Middle-aged
   Teacher in Science and Technology Research of Huaqiao University
   (No.ZQN-PY518), and the grants from National Natural Science Foundation
   of China (Grant No.61605048), in part by Natural Science Foundation of
   Fujian Province, China under Grant 2015J01256, and Grant 2016J01300, in
   part by Fujian Provincial Big Data Research Institute of Intelligent
   Manufacturing, in part by the Quanzhou scientific and technological
   planning projects (No.2018C113R and No. 2017G024), and in part by the
   Subsidized Project for Postgraduates' Innovative Fund in Scientific
   Research of Huaqiao University under Grant 1611422001.
CR [Anonymous], 2006, IEEE C COMPUTER VISI, DOI DOI 10.1109/CVPR.2006.256
   Avidan S, 2005, PROC CVPR IEEE, P494
   Avidan S, 2001, PROC CVPR IEEE, P184
   Babenko B, 2011, IEEE T PATTERN ANAL, V33, P1619, DOI 10.1109/TPAMI.2010.226
   Bertinetto L, 2016, PROC CVPR IEEE, P1401, DOI 10.1109/CVPR.2016.156
   Bibi A, 2016, LECT NOTES COMPUT SC, V9910, P419, DOI 10.1007/978-3-319-46466-4_25
   Bolme DS, 2010, PROC CVPR IEEE, P2544, DOI 10.1109/CVPR.2010.5539960
   Cauwenberghs T., 2000, P 13 INT C NEUR INF, P388
   Comaniciu D, 2003, IEEE T PATTERN ANAL, V25, P564, DOI 10.1109/TPAMI.2003.1195991
   Danelljan M., 2014, BRIT MACH VIS C, P1, DOI DOI 10.5244/C.28.65
   Danelljan M, 2016, LECT NOTES COMPUT SC, V9909, P472, DOI 10.1007/978-3-319-46454-1_29
   Danelljan M, 2015, IEEE I CONF COMP VIS, P4310, DOI 10.1109/ICCV.2015.490
   Danelljan M, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P621, DOI 10.1109/ICCVW.2015.84
   Fan H, 2017, IEEE T CIRC SYST VID, V27, P1018, DOI 10.1109/TCSVT.2016.2515738
   Grabner H, 2008, LECT NOTES COMPUT SC, V5302, P234, DOI 10.1007/978-3-540-88682-2_19
   Hare S, 2011, IEEE I CONF COMP VIS, P263, DOI 10.1109/ICCV.2011.6126251
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Henriques JF, 2015, IEEE T PATTERN ANAL, V37, P583, DOI 10.1109/TPAMI.2014.2345390
   Henriques JF, 2012, LECT NOTES COMPUT SC, V7575, P702, DOI 10.1007/978-3-642-33765-9_50
   Hongyi Su, 2015, Intelligent Computing Theories and Methodologies. 11th International Conference, ICIC 2015. Proceedings: LNCS 9225, P1, DOI 10.1007/978-3-319-22180-9_1
   Hua Y, 2014, LECT NOTES COMPUT SC, V8694, P172, DOI 10.1007/978-3-319-10599-4_12
   Huang Z., 2017, P IEEE C COMP VIS PA, P4021, DOI DOI 10.1109/CVPR.2017.510
   Kalal Z, 2012, IEEE T PATTERN ANAL, V34, P1409, DOI 10.1109/TPAMI.2011.239
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li Y, 2015, LECT NOTES COMPUT SC, V8926, P254, DOI 10.1007/978-3-319-16181-5_18
   Li ZY, 2015, VISUAL COMPUT, V31, P1319, DOI 10.1007/s00371-014-1014-6
   Ma C, 2015, PROC CVPR IEEE, P5388, DOI 10.1109/CVPR.2015.7299177
   Mei X, 2009, IEEE I CONF COMP VIS, P1436, DOI 10.1109/ICCV.2009.5459292
   Mueller M, 2017, PROC CVPR IEEE, P1387, DOI 10.1109/CVPR.2017.152
   Ning JF, 2016, PROC CVPR IEEE, P4266, DOI 10.1109/CVPR.2016.462
   Ozuysal M., 2007, CVPR, P1, DOI DOI 10.1109/CVPR.2007.383123
   Quan W, 2015, VISUAL COMPUT, V31, P1307, DOI 10.1007/s00371-014-1012-8
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Smeulders AWM, 2014, IEEE T PATTERN ANAL, V36, P1442, DOI 10.1109/TPAMI.2013.230
   Song YB, 2017, IEEE I CONF COMP VIS, P2574, DOI 10.1109/ICCV.2017.279
   Tang M, 2018, PROC CVPR IEEE, P4874, DOI 10.1109/CVPR.2018.00512
   Valmadre J, 2017, PROC CVPR IEEE, P5000, DOI 10.1109/CVPR.2017.531
   Wang N, 2018, PROC CVPR IEEE, P4844, DOI 10.1109/CVPR.2018.00509
   Wu Y, 2015, IEEE T PATTERN ANAL, V37, P1834, DOI 10.1109/TPAMI.2014.2388226
   Wu Y, 2013, PROC CVPR IEEE, P2411, DOI 10.1109/CVPR.2013.312
   Yilmaz A, 2006, ACM COMPUT SURV, V38, DOI 10.1145/1177352.1177355
   Zhang KH, 2014, LECT NOTES COMPUT SC, V8693, P127, DOI 10.1007/978-3-319-10602-1_9
   Zhang KH, 2012, LECT NOTES COMPUT SC, V7574, P864, DOI 10.1007/978-3-642-33712-3_62
   Zhang TZ, 2012, LECT NOTES COMPUT SC, V7577, P470, DOI 10.1007/978-3-642-33783-3_34
   Zhang TZ, 2012, PROC CVPR IEEE, P2042, DOI 10.1109/CVPR.2012.6247908
   Zhu Z, 2017, IEEE INT CONF COMP V, P1973, DOI 10.1109/ICCVW.2017.231
NR 46
TC 4
Z9 4
U1 0
U2 26
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2020
VL 36
IS 2
BP 425
EP 442
DI 10.1007/s00371-019-01631-8
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KJ2TH
UT WOS:000511910300015
DA 2024-07-18
ER

PT J
AU Joshi, P
   Prakash, S
AF Joshi, Piyush
   Prakash, Surya
TI Image enhancement with naturalness preservation
SO VISUAL COMPUTER
LA English
DT Article
DE Image naturalness; Retinex theory; Image quality assessment; Image
   enhancement
ID ADAPTIVE HISTOGRAM EQUALIZATION; CONTRAST ENHANCEMENT; RETINEX
AB There exist many techniques in the literature to enhance poorly illuminated images. Most of these techniques are based on retinex theory and enhance images considering only the reflectance component of the image and ignore illumination component. However, illumination plays a significant role in achieving pleasing perceptual quality in an image. Also, naturalness of an image cannot be preserved without preserving its illumination. This paper proposes an image enhancement technique to enhance details of an image by keeping good illumination present in the image intact. For images with poor illumination, we propose to add suitable amount of brightness in low illuminated regions in the image to get pleasing perceptual quality and naturalness. There are two major contributions of this work. First, it proposes a bad illumination pass filter (BIPF) to locate badly illuminated areas in the image and subsequently create an enhanced image by highlighting details as well as good illumination areas. Second, it proposes an adaptive logarithm transformation to add the required amount of brightness in poorly illuminated areas in the image. Experimental results show that the proposed technique preserves the natural appearance of the image and produces better perceptual quality in comparison with existing state-of-the-art techniques.
C1 [Joshi, Piyush; Prakash, Surya] Indian Inst Technol Indore, Indore 453552, India.
C3 Indian Institute of Technology System (IIT System); Indian Institute of
   Technology (IIT) - Indore
RP Joshi, P (corresponding author), Indian Inst Technol Indore, Indore 453552, India.
EM piyushjoshi3839@gmail.com; surya@iiti.ac.in
CR Celik T, 2012, IEEE T IMAGE PROCESS, V21, P145, DOI 10.1109/TIP.2011.2162419
   Chen SH, 2009, IEEE IMAGE PROC, P1813, DOI 10.1109/ICIP.2009.5413362
   Chen ZY, 2006, IEEE T IMAGE PROCESS, V15, P2290, DOI 10.1109/TIP.2006.875204
   Deng G, 2011, IEEE T IMAGE PROCESS, V20, P1249, DOI 10.1109/TIP.2010.2092441
   Elad M, 2005, LECT NOTES COMPUT SC, V3459, P217
   Gonzalez R. C., 2006, PEARSON ED INDIA, V3rd
   Huang H, 2010, VISUAL COMPUT, V26, P731, DOI 10.1007/s00371-010-0504-4
   Ibrahim H, 2007, IEEE T CONSUM ELECTR, V53, P1752, DOI 10.1109/TCE.2007.4429280
   Jobson DJ, 1997, IEEE T IMAGE PROCESS, V6, P965, DOI 10.1109/83.597272
   Jobson DJ, 1997, IEEE T IMAGE PROCESS, V6, P451, DOI 10.1109/83.557356
   Kang B, 2011, IMAGE VISION COMPUT, V29, P557, DOI 10.1016/j.imavis.2011.06.001
   Kovaleski RP, 2009, VISUAL COMPUT, V25, P539, DOI 10.1007/s00371-009-0327-3
   Li YC, 2016, OPTIK, V127, P1326, DOI 10.1016/j.ijleo.2015.07.177
   Ling Y, 2012, VISUAL COMPUT, V28, P733, DOI 10.1007/s00371-012-0691-2
   Meylan L, 2006, IEEE T IMAGE PROCESS, V15, P2820, DOI 10.1109/TIP.2006.877312
   PIZER SM, 1987, COMPUT VISION GRAPH, V39, P355, DOI 10.1016/S0734-189X(87)80186-X
   Polesel A, 2000, IEEE T IMAGE PROCESS, V9, P505, DOI 10.1109/83.826787
   Reza AM, 2004, J VLSI SIG PROC SYST, V38, P35, DOI 10.1023/B:VLSI.0000028532.53893.82
   Singh KR, 2013, APPL SOFT COMPUT, V13, P4105, DOI 10.1016/j.asoc.2013.04.012
   Subramanyam B., 2016, 2016 IEEE INT C ID S, P1
   Tao L, 2005, J ELECTRON IMAGING, V14, DOI 10.1117/1.2136903
   Tao YB, 2009, VISUAL COMPUT, V25, P581, DOI 10.1007/s00371-009-0328-2
   Wang C, 2005, IEEE T CONSUM ELECTR, V51, P1326, DOI 10.1109/TCE.2005.1561863
   Wang SH, 2013, IEEE T IMAGE PROCESS, V22, P3538, DOI 10.1109/TIP.2013.2261309
   Wang YF, 2012, OPT ENG, V51, DOI 10.1117/1.OE.51.10.107001
   Yuecheng Li, 2011, 2011 4th International Congress on Image and Signal Processing (CISP 2011), P1788, DOI 10.1109/CISP.2011.6100606
   Zhang H, 2013, OPTIK, V124, P5906, DOI 10.1016/j.ijleo.2013.04.046
NR 27
TC 7
Z9 7
U1 0
U2 13
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2020
VL 36
IS 1
BP 71
EP 83
DI 10.1007/s00371-018-1587-6
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KJ3OJ
UT WOS:000511966800007
DA 2024-07-18
ER

PT J
AU Liu, ZY
   Xiang, Q
   Tang, JT
   Wang, Y
   Zhao, P
AF Liu, Zhengyi
   Xiang, Qian
   Tang, Jiting
   Wang, Yuan
   Zhao, Peng
TI Robust salient object detection for RGB images
SO VISUAL COMPUTER
LA English
DT Article
DE Salient object detection; Salient object existence prediction;
   Non-salient images; Robust; Multi-task learning
ID ATTENTION; NETWORK; FUSION
AB Recent advances in supervised salient object detection modeling have resulted in significant performance improvements on benchmark dataset. However, most of the existing salient object detection models assume that an image contains at least one salient object. Such an assumption that often leads to their effectiveness may be impaired once they are applied to real-world scenes. To solve the problem, salient object existence prediction designed to judge whether the image contains salient objects is introduced into deep network to learn a better salient object detection model. For dense salient object detection task, high-level semantic feature is progressively hybrid upsampled from deep to shallow to remedy the spatial information loss guided by higher layer feature and saliency existence information. Our model is aware of non-salient image that contains no salient objects at all and thus reduces the false-positive rate. Experimental results show that our model wins similar multi-task model and outperforms state-of-the-art models in robustness and accuracy.
C1 [Liu, Zhengyi; Xiang, Qian; Tang, Jiting; Wang, Yuan; Zhao, Peng] Anhui Univ, Key Lab Intelligent Comp & Signal Proc, Minist Educ, Sch Comp Sci & Technol, Hefei, Anhui, Peoples R China.
C3 Anhui University
RP Liu, ZY (corresponding author), Anhui Univ, Key Lab Intelligent Comp & Signal Proc, Minist Educ, Sch Comp Sci & Technol, Hefei, Anhui, Peoples R China.
EM liuzywen@ahu.edu.cn; xiangqianforth@qq.com; 1796340141@qq.com;
   wangyuan.ahu@qq.com; 18868519@qq.com
RI Liu, Zhengyi/AAD-6702-2020; xiang, q/KUD-1041-2024; wang,
   yuan/AAB-6626-2022; Liu, Zhengyi/AAB-6589-2022
OI Liu, Zhengyi/0000-0003-3265-823X
FU NationalNatural Science Foundation of China [61602004]; Natural Science
   Foundation of Anhui Province [1908085MF182]; University Natural Science
   Research Project of Anhui Province [KJ2019A0034]
FX We thank Ming-Ming Cheng from Nankai University for providing JSOD
   dataset. We also thank all anonymous reviewers for their valuable
   comments. This research is supported by NationalNatural Science
   Foundation of China (61602004), Natural Science Foundation of Anhui
   Province (1908085MF182) and University Natural Science Research Project
   of Anhui Province (KJ2019A0034).
CR Abadi Martin, 2016, TENSORFLOW LARGE SCA, V16, P265
   Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   Alpert S, 2007, PROC CVPR IEEE, P359
   [Anonymous], 2018, ARXIV180309860
   Borji A, 2012, LECT NOTES COMPUT SC, V7573, P414, DOI 10.1007/978-3-642-33709-3_30
   Chen H, 2019, IEEE T IMAGE PROCESS, V28, P2825, DOI 10.1109/TIP.2019.2891104
   Chen H, 2019, PATTERN RECOGN, V86, P376, DOI 10.1016/j.patcog.2018.08.007
   Cheng MM, 2021, INT J COMPUT VISION, V129, P2622, DOI [10.1007/s11263-021-01490-8, 10.1109/ICCV.2017.487]
   Cheng MM, 2011, PROC CVPR IEEE, P409, DOI 10.1109/CVPR.2011.5995344
   Cholakkal H, 2016, SIGNAL PROCESS-IMAGE, V45, P24, DOI 10.1016/j.image.2016.04.001
   Deng ZJ, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P684
   Fan D.-P., 2019, ARXIV190706781
   Fan DP, 2018, LECT NOTES COMPUT SC, V11219, P196, DOI 10.1007/978-3-030-01267-0_12
   Fan DP, 2019, PROC CVPR IEEE, P8546, DOI 10.1109/CVPR.2019.00875
   He SF, 2017, IEEE I CONF COMP VIS, P1059, DOI 10.1109/ICCV.2017.120
   Hou QB, 2017, PROC CVPR IEEE, P5300, DOI 10.1109/CVPR.2017.563
   Hou QB, 2019, IEEE T PATTERN ANAL, V41, P815, DOI 10.1109/TPAMI.2018.2815688
   Huang K, 2020, VISUAL COMPUT, V36, P1355, DOI 10.1007/s00371-019-01734-2
   Huang Y, 2018, IEEE T PATTERN ANAL, V40, P1015, DOI 10.1109/TPAMI.2017.2701380
   JIANG H, 2017, FRONT COMPUT SCI, V13, P778
   Kingma D. P., 2014, arXiv
   Li F., 2016, P IJCAI, P3411
   Li GB, 2018, AAAI CONF ARTIF INTE, P7024
   Li GB, 2017, PROC CVPR IEEE, P247, DOI 10.1109/CVPR.2017.34
   Li GB, 2015, PROC CVPR IEEE, P5455, DOI 10.1109/CVPR.2015.7299184
   Li M., 2018, BRIT MACH VIS C
   Li X, 2016, IEEE T IMAGE PROCESS, V25, P3919, DOI 10.1109/TIP.2016.2579306
   Li X, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P439, DOI 10.1145/3123266.3123290
   Li X, 2018, LECT NOTES COMPUT SC, V11219, P370, DOI 10.1007/978-3-030-01267-0_22
   Li Y, 2014, PROC CVPR IEEE, P280, DOI 10.1109/CVPR.2014.43
   Liu FL, 2018, INT CONF SOFTW ENG, P744, DOI 10.1109/ICSESS.2018.8663918
   Liu T, 2011, IEEE T PATTERN ANAL, V33, P353, DOI 10.1109/TPAMI.2010.70
   LIU Z, 2019, VISUAL COMPUT, P1
   Liu ZY, 2019, NEUROCOMPUTING, V363, P46, DOI 10.1016/j.neucom.2019.07.012
   Lu Y, 2019, VISUAL COMPUT, V35, P1683, DOI 10.1007/s00371-019-01637-2
   Martin D., 2001, P ICCV, P416, DOI [DOI 10.1109/ICCV.2001.937655, 10.1109/ICCV.2001.937655]
   Mochizuki I, 2018, VISUAL COMPUT, V34, P1031, DOI 10.1007/s00371-018-1518-6
   Qin XB, 2019, PROC CVPR IEEE, P7471, DOI 10.1109/CVPR.2019.00766
   Shen JB, 2018, IEEE T IMAGE PROCESS, V27, P2688, DOI 10.1109/TIP.2018.2795740
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Tang YL, 2016, VISUAL COMPUT, V32, P111, DOI 10.1007/s00371-014-1059-6
   Wang BY, 2018, VISUAL COMPUT, V34, P645, DOI 10.1007/s00371-017-1404-7
   Wang C, 2019, AAAI CONF ARTIF INTE, P8917
   Wang HS, 2017, PROC CVPR IEEE, P3633, DOI 10.1109/CVPR.2017.387
   Wang NN, 2019, IEEE ACCESS, V7, P55277, DOI 10.1109/ACCESS.2019.2913107
   Wang P, 2012, PROC CVPR IEEE, P3194, DOI 10.1109/CVPR.2012.6248054
   Wang WG, 2018, IEEE T IMAGE PROCESS, V27, P38, DOI 10.1109/TIP.2017.2754941
   Wang WG, 2018, PROC CVPR IEEE, P4894, DOI 10.1109/CVPR.2018.00514
   Wang WG, 2018, IEEE T PATTERN ANAL, V40, P20, DOI 10.1109/TPAMI.2017.2662005
   Wei LN, 2019, IEEE T IMAGE PROCESS, V28, P5052, DOI 10.1109/TIP.2019.2909649
   Wenguan Wang, 2018, IEEE Transactions on Image Processing, V27, P2368, DOI 10.1109/TIP.2017.2787612
   Wu Z, 2019, PROC CVPR IEEE, P3902, DOI 10.1109/CVPR.2019.00403
   Xie SN, 2015, IEEE I CONF COMP VIS, P1395, DOI [10.1109/ICCV.2015.164, 10.1007/s11263-017-1004-z]
   Yan Q, 2013, PROC CVPR IEEE, P1155, DOI 10.1109/CVPR.2013.153
   Yang C, 2013, PROC CVPR IEEE, P3166, DOI 10.1109/CVPR.2013.407
   Zhang JM, 2015, PROC CVPR IEEE, P4045, DOI 10.1109/CVPR.2015.7299031
   Zhang KH, 2019, PROC CVPR IEEE, P3090, DOI 10.1109/CVPR.2019.00321
   Zhang L, 2018, PROC CVPR IEEE, P1741, DOI 10.1109/CVPR.2018.00187
   Zhang Q, 2018, VISUAL COMPUT, V34, P473, DOI 10.1007/s00371-017-1354-0
   Zhao J, 2020, IEEE T FUZZY SYST, V28, P2287, DOI 10.1109/TFUZZ.2019.2930492
   Zhuge YZ, 2018, IEEE SIGNAL PROC LET, V25, P1800, DOI 10.1109/LSP.2018.2875586
NR 61
TC 21
Z9 21
U1 1
U2 38
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2020
VL 36
IS 9
BP 1823
EP 1835
DI 10.1007/s00371-019-01778-4
EA DEC 2019
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA NA2KY
UT WOS:000500855900001
DA 2024-07-18
ER

PT J
AU Jiang, T
   Zhang, ZH
   Yang, YP
AF Jiang, Teng
   Zhang, Zehan
   Yang, Yupu
TI Modeling coverage with semantic embedding for image caption generation
SO VISUAL COMPUTER
LA English
DT Article
DE Coverage model; Semantic embedding; Image caption generation;
   Attention-based model
ID ATTENTION
AB This paper presents a coverage-based image caption generation model. The attention-based encoder-decoder framework has enhanced state-of-the-art image caption generation by learning where to attend of the visual field. However, there exists a problem that in some cases it ignores past attention information, which tends to lead to over-recognition and under-recognition. To solve this problem, a coverage mechanism is incorporated into attention-based image caption generation. A sequential updated coverage vector is applied to preserve the attention historical information. At each time step, the attention model takes the coverage vector as auxiliary input to focus more on unattended features. Besides, to maintain the semantics of an image, we propose semantic embedding as global guidance to coverage and attention model. With semantic embedding, the attention and coverage mechanisms consider more about features relevant to the semantics of an image. Experiments conducted on the three benchmark datasets, namely Flickr8k, Flickr30k and MSCOCO, demonstrate the effectiveness of our proposed approach. In addition to solve the over-recognition and under-recognition problems, it behaves better on long descriptions.
C1 [Jiang, Teng; Zhang, Zehan; Yang, Yupu] Shanghai Jiao Tong Univ, Dept Automat, Shanghai 200240, Peoples R China.
   [Jiang, Teng; Zhang, Zehan; Yang, Yupu] Minist Educ China, Key Lab Syst Control & Informat Proc, Shanghai 200240, Peoples R China.
C3 Shanghai Jiao Tong University
RP Jiang, T (corresponding author), Shanghai Jiao Tong Univ, Dept Automat, Shanghai 200240, Peoples R China.; Jiang, T (corresponding author), Minist Educ China, Key Lab Syst Control & Informat Proc, Shanghai 200240, Peoples R China.
EM jtengyp@163.com
RI yang, yang/GVT-5210-2022; yang, yang/HGT-7999-2022; Lang,
   Ming/HIK-0758-2022
FU National Natural Science Foundation of China [61273161]
FX This work is supported by the National Natural Science Foundation of
   China (Grant No. 61273161).
CR [Anonymous], CVPR
   [Anonymous], EUR C COMP VIS ECCV
   Ba J., 2015, P ICLR, P1, DOI DOI 10.1016/J.JCYT.2014.02.008
   Bahdanau D, 2016, Arxiv, DOI [arXiv:1409.0473, 10.48550/arXiv.1409.0473]
   Bandanau D, 2016, INT CONF ACOUST SPEE, P4945, DOI 10.1109/ICASSP.2016.7472618
   Banerjee S., 2005, P WORKSHOP INTRINSIC, P65
   Chan W, 2016, INT CONF ACOUST SPEE, P4960, DOI 10.1109/ICASSP.2016.7472621
   Cho Kyunghyun, 2014, SYNTAX SEMANTICS STR, P5, DOI [10.3115/v1/w14-4012, 10.3115 /v1/D14-1179, DOI 10.3115/V1/D14-1179]
   Donahue B, 2015, 2015 IEEE AEROSPACE CONFERENCE
   Fu K, 2017, IEEE T PATTERN ANAL, V39, P2321, DOI 10.1109/TPAMI.2016.2642953
   Gers FA, 2000, NEURAL COMPUT, V12, P2451, DOI 10.1162/089976600300015015
   Greff K, 2017, IEEE T NEUR NET LEAR, V28, P2222, DOI 10.1109/TNNLS.2016.2582924
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Hodosh M, 2013, J ARTIF INTELL RES, V47, P853, DOI 10.1613/jair.3994
   Ioffe S, 2015, P INT C MACH LEARN, V2015, P1
   Jia X, 2015, IEEE I CONF COMP VIS, P2407, DOI 10.1109/ICCV.2015.277
   Jingna Mao, 2015, 2015 IEEE Biomedical Circuits and Systems Conference (BioCAS), P1, DOI 10.1109/BioCAS.2015.7348279
   Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932
   Kingma D. P., 2014, arXiv
   Kiros R, 2014, PR MACH LEARN RES, V32, P595
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kulkarni G, 2013, IEEE T PATTERN ANAL, V35, P2891, DOI 10.1109/TPAMI.2012.162
   Kuznetsova P., 2012, Long Papers, P359
   Kuznetsova P., 2013, P ANN M ASS COMPUTAT, P790
   Lin Chin-Yew, 2004, Text summarization branches out, P74, DOI DOI 10.2307/3105454
   Mnih V, 2014, ADV NEUR IN, V27
   Pan YW, 2016, PROC CVPR IEEE, P4594, DOI 10.1109/CVPR.2016.497
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135
   Parikh, 2015, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2015.7299087
   Ren Z, 2017, PROC CVPR IEEE, P1151, DOI 10.1109/CVPR.2017.128
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Tu ZP, 2016, PROCEEDINGS OF THE 54TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P76
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   Wang YM, 2017, VISUAL COMPUT, V33, P971, DOI 10.1007/s00371-017-1378-5
   Wu HY, 2016, VISUAL COMPUT, V32, P123, DOI 10.1007/s00371-014-1060-0
   Wu Y., 2016, GOOGLES NEURAL MACHI
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   You QZ, 2016, PROC CVPR IEEE, P4651, DOI 10.1109/CVPR.2016.503
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
NR 40
TC 12
Z9 12
U1 0
U2 18
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2019
VL 35
IS 11
SI SI
BP 1655
EP 1665
DI 10.1007/s00371-018-1565-z
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA JD5IZ
UT WOS:000490018000012
DA 2024-07-18
ER

PT J
AU Fazl-Ersi, E
   Nooghabi, MK
AF Fazl-Ersi, Ehsan
   Nooghabi, Masoud Kazemi
TI Revisiting correlation-based filters for low-resolution and long-term
   visual tracking
SO VISUAL COMPUTER
LA English
DT Article
DE Visual tracking; Low-resolution; Long-term; Local descriptor
AB In this paper, we revisit the problem of visual tracking by introducing a novel low-dimensional descriptor based on gradient distribution and specifically focus our attention on the problem of low-resolution and long-term visual tracking. We show that our tracking solution empowered by our proposed descriptor can effectively address the existing challenges in low-resolution and long-term visual tracking. Compared to the existing descriptors, the proposed method provides better robustness against local geometric and photometric variations. It adopts a new approach for aggregating information in a local neighborhood such that the sensitivity of the descriptor to noise and unreliable texture information is reduced. Integrating the proposed descriptor into a correlation-based tracking framework results in a robust and fast visual tracker. An extensive set of experiments on a number of large-scale benchmark datasets shows that the proposed method outperforms the state-of-the-art methods on low-resolution and long-term challenges, while achieving state-of-the-art performance in generic tracking.
C1 [Fazl-Ersi, Ehsan] Ferdowsi Univ Mashhad, Fac Comp Engn, Mashhad, Razavi Khorasan, Iran.
   [Nooghabi, Masoud Kazemi] Ferdowsi Univ Mashhad, Mashhad, Razavi Khorasan, Iran.
C3 Ferdowsi University Mashhad; Ferdowsi University Mashhad
RP Fazl-Ersi, E (corresponding author), Ferdowsi Univ Mashhad, Fac Comp Engn, Mashhad, Razavi Khorasan, Iran.
EM fazlersi@um.ac.ir; masoud.kazemi@mail.um.ac.ir
OI Fazl-Ersi, Ehsan/0000-0001-7384-1618
CR [Anonymous], 2016, P EUR C COMP VIS ECC
   [Anonymous], 2016, IEEE C COMP VIS PATT
   [Anonymous], 2016, CVPR
   [Anonymous], 2016, CVPR
   Boddeti VN, 2013, PROC CVPR IEEE, P2291, DOI 10.1109/CVPR.2013.297
   Bolme DS, 2010, PROC CVPR IEEE, P2544, DOI 10.1109/CVPR.2010.5539960
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Dalal N., 2005, PROC IEEE COMPUT SOC, V1, P886
   Danelljan M., 2017, CVPR, P21
   Danelljan M., 2014, Accurate Scale Estimation for Robust Visual Tracking
   Danelljan M, 2016, LECT NOTES COMPUT SC, V9909, P472, DOI 10.1007/978-3-319-46454-1_29
   Danelljan M, 2015, IEEE I CONF COMP VIS, P4310, DOI 10.1109/ICCV.2015.490
   Danelljan M, 2014, PROC CVPR IEEE, P1090, DOI 10.1109/CVPR.2014.143
   Dollar P., PIOTRS COMPUTER VISI
   Galoogahi HK, 2013, IEEE I CONF COMP VIS, P3072, DOI 10.1109/ICCV.2013.381
   Gao J, 2014, LECT NOTES COMPUT SC, V8691, P188, DOI 10.1007/978-3-319-10578-9_13
   Grabner H., 2006, BMVC, P47
   Hare S, 2011, IEEE I CONF COMP VIS, P263, DOI 10.1109/ICCV.2011.6126251
   Held D, 2016, LECT NOTES COMPUT SC, V9905, P749, DOI 10.1007/978-3-319-46448-0_45
   Henriques JF, 2015, IEEE T PATTERN ANAL, V37, P583, DOI 10.1109/TPAMI.2014.2345390
   Henriques JF, 2013, IEEE I CONF COMP VIS, P2760, DOI 10.1109/ICCV.2013.343
   Henriques JF, 2012, LECT NOTES COMPUT SC, V7575, P702, DOI 10.1007/978-3-642-33765-9_50
   Hong ZB, 2015, PROC CVPR IEEE, P749, DOI 10.1109/CVPR.2015.7298675
   Jia X, 2012, PROC CVPR IEEE, P1822, DOI 10.1109/CVPR.2012.6247880
   Kalal Z, 2010, PROC CVPR IEEE, P49, DOI 10.1109/CVPR.2010.5540231
   Kumar B.V. K. Vijaya., 2005, Correlation Pattern Recognition
   Li Y, 2015, LECT NOTES COMPUT SC, V8926, P254, DOI 10.1007/978-3-319-16181-5_18
   Liang PP, 2015, IEEE T IMAGE PROCESS, V24, P5630, DOI 10.1109/TIP.2015.2482905
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Ma C, 2015, IEEE I CONF COMP VIS, P3074, DOI 10.1109/ICCV.2015.352
   Ma C, 2015, PROC CVPR IEEE, P5388, DOI 10.1109/CVPR.2015.7299177
   Possegger H, 2015, PROC CVPR IEEE, P2113, DOI 10.1109/CVPR.2015.7298823
   Ross DA, 2008, INT J COMPUT VISION, V77, P125, DOI 10.1007/s11263-007-0075-7
   Sevilla-Lara L, 2012, PROC CVPR IEEE, P1910, DOI 10.1109/CVPR.2012.6247891
   van de Weijer J, 2009, IEEE T IMAGE PROCESS, V18, P1512, DOI 10.1109/TIP.2009.2019809
   Wu Y, 2015, IEEE T PATTERN ANAL, V37, P1834, DOI 10.1109/TPAMI.2014.2388226
   Zhang JM, 2014, LECT NOTES COMPUT SC, V8694, P188, DOI 10.1007/978-3-319-10599-4_13
   Zhang KH, 2016, IEEE T IMAGE PROCESS, V25, P1779, DOI 10.1109/TIP.2016.2531283
NR 38
TC 7
Z9 7
U1 0
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2019
VL 35
IS 10
BP 1447
EP 1459
DI 10.1007/s00371-018-1510-1
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IU7PN
UT WOS:000483775900008
DA 2024-07-18
ER

PT J
AU Zhou, XN
   Wang, YN
   Zhu, Q
   Xiao, CY
   Lu, X
AF Zhou, Xianen
   Wang, Yaonan
   Zhu, Qing
   Xiao, Changyan
   Lu, Xiao
TI SSG: superpixel segmentation and GrabCut-based salient object
   segmentation
SO VISUAL COMPUTER
LA English
DT Article
DE Salient object segmentation; Superpixel segmentation; GrabCut; Region
   growing; DBSCAN clustering
AB Saliency detection is a popular topic for image processing recently. In this paper, we propose a simple, robust and fast salient object segmentation framework. Firstly, we develop a novel saliency map segmentation strategy, named SSG which consists of superpixel region growing, superpixel Density-Based Spatial Clustering of Applications with Noise (DBSCAN) clustering and iterated graph cuts (GrabCut), where DBSCAN makes similar background regions cluster as a whole, region growing groups similar regions together as much as possible, GrabCut segments salient objects accurately. Then, the proposed SSG is combined with saliency detection to abstract salient objects. Experimental results on three benchmark datasets demonstrate that the proposed method achieves the favorable performance than many recent state-of-the-art methods in terms of precision, recall, F-measure and execution time.
C1 [Zhou, Xianen; Wang, Yaonan; Zhu, Qing; Xiao, Changyan] Hunan Univ, Natl Engn Lab Robot Visual Percept & Control Tech, Changsha 410082, Hunan, Peoples R China.
   [Lu, Xiao] Hunan Normal Univ, Changsha 410082, Hunan, Peoples R China.
C3 Hunan University; Hunan Normal University
RP Zhu, Q (corresponding author), Hunan Univ, Natl Engn Lab Robot Visual Percept & Control Tech, Changsha 410082, Hunan, Peoples R China.
EM zhouxianen@hnu.edu.cn; yaonan@hnu.edu.cn; zhuqing_hnu@163.com;
   c.xiao@hnu.edu.cn; xlu_hnu@163.com
RI lu, xiao/AAS-2542-2020; zhou, xianen/AAD-3173-2019
OI zhou, xianen/0000-0001-6115-1310; Zhu, Qing/0000-0003-1237-2322
FU National Science Foundation of China [61573134, 61703155]; National
   Science and Technology Support Program [2015BAF13B00]; Innovation
   Project of Postgraduate Student in Hunan Province, China [CX2017B108]
FX This work was supported by the National Science Foundation of China
   (61573134, 61703155), the National Science and Technology Support
   Program (2015BAF13B00) and the Innovation Project of Postgraduate
   Student in Hunan Province, China (CX2017B108).
CR Achanta R., 2010, SLIC Superpixels
   Achanta R., 2010, P INT C COMP VIS SYS, V5008, P66
   Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   Alpert S., 2007, IEEE C COMP VIS PATT, P1
   [Anonymous], 2017, VIS COMPUT, DOI DOI 10.1007/s00521-017-2912-0
   [Anonymous], 2008, 2008 19 INT C PATT R
   [Anonymous], 2005, Advances in neural information processing systems, DOI DOI 10.5555/2976248.2976268
   [Anonymous], FASA FAST ACCURATE S
   [Anonymous], 2015, ARXIV150904232
   Arbeláez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161
   Borji Ali, 2019, [Computational Visual Media, 计算可视媒体], V5, P117
   Borji A, 2013, IEEE T PATTERN ANAL, V35, P185, DOI 10.1109/TPAMI.2012.89
   Borji A, 2012, LECT NOTES COMPUT SC, V7573, P414, DOI 10.1007/978-3-642-33709-3_30
   Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236
   Ester M., 1996, KDD 96, P226, DOI DOI 10.5555/3001460.3001507
   Fan HP, 2005, PATTERN RECOGN LETT, V26, P1139, DOI 10.1016/j.patrec.2004.10.010
   Felzenszwalb PF, 2004, INT J COMPUT VISION, V59, P167, DOI 10.1023/B:VISI.0000022288.19776.77
   Feng W, 2016, PROC CVPR IEEE, P4049, DOI 10.1109/CVPR.2016.439
   Fu KR, 2015, IEEE T IMAGE PROCESS, V24, P5671, DOI 10.1109/TIP.2015.2485782
   Hou X., 2007, IEEE C COMP VIS PATT, V1, P1, DOI DOI 10.1109/CVPR.2007.383267
   Huo LN, 2016, PATTERN RECOGN, V49, P162, DOI 10.1016/j.patcog.2015.07.005
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Jiang HZ, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.110
   Jiang HZ, 2013, PROC CVPR IEEE, P2083, DOI 10.1109/CVPR.2013.271
   Kadir T, 2001, INT J COMPUT VISION, V45, P83, DOI 10.1023/A:1012460413855
   Liu T, 2011, IEEE T PATTERN ANAL, V33, P353, DOI 10.1109/TPAMI.2010.70
   Liu Tie, 2007, P IEEE C COMP VIS PA, P1, DOI DOI 10.1109/CVPR.2007.383047
   Ma Y.F., 2003, P 11 ACM INT C MULT, P374, DOI DOI 10.1145/957013.957094
   Min Chen, 2011, IEEE INFOCOM 2011 - IEEE Conference on Computer Communications. Workshops, P409, DOI 10.1109/INFCOMW.2011.5928847
   OJALA T, 1994, INT C PATT RECOG, P582, DOI 10.1109/ICPR.1994.576366
   OTSU N, 1979, IEEE T SYST MAN CYB, V9, P62, DOI 10.1109/TSMC.1979.4310076
   Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720
   Shi JP, 2016, IEEE T PATTERN ANAL, V38, P717, DOI 10.1109/TPAMI.2015.2465960
   Wang JD, 2017, INT J COMPUT VISION, V123, P251, DOI 10.1007/s11263-016-0977-3
   Wei YC, 2012, LECT NOTES COMPUT SC, V7574, P29, DOI 10.1007/978-3-642-33712-3_3
   Yan Q, 2013, PROC CVPR IEEE, P1155, DOI 10.1109/CVPR.2013.153
   Yang C, 2013, PROC CVPR IEEE, P3166, DOI 10.1109/CVPR.2013.407
   Zhai Y., 2006, P 14 ACM INT C MULT, P815, DOI [DOI 10.1145/1180639.1180824, 10.1145/1180639.1180824]
   Zhang HL, 2016, VISUAL COMPUT, V32, P31, DOI 10.1007/s00371-014-1053-z
NR 40
TC 19
Z9 20
U1 0
U2 40
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2019
VL 35
IS 3
BP 385
EP 398
DI 10.1007/s00371-018-1471-4
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HP0MR
UT WOS:000461360600007
DA 2024-07-18
ER

PT J
AU Sadeghi, B
   Jamshidi, K
   Vafaei, A
   Monadjemi, SA
AF Sadeghi, Bahman
   Jamshidi, Kamal
   Vafaei, Abbas
   Monadjemi, S. Amirhassan
TI 2DIGH: a polar invariant local image descriptor based on joint histogram
SO VISUAL COMPUTER
LA English
DT Article
DE Descriptor; Invariant descriptor; Joint histogram; Image matching
AB One of the key challenges of current image matching techniques is how to build a robust local descriptor which is invariant to large variations in scale and rotation. To address this issue, in this work a polar gradient local oriented histogram pattern (PGP) is localized on normalized cropped regions around detected interest points. Then, a new image descriptor named two-dimensional intensity gradient histogram (2DIGH) is introduced using the joint histogram scheme. 2DIGH builds the extracted feature vector by intersecting of gradient and intensity information on subregions of the PGP. The measured distance with K-nearest neighbor represents feature vectors similarity/distance for image matching. The experimental results on Graffiti, Boat, Bark and ZuBud datasets indicate that the performance of the introduced 2DIGH is at least 41% better than other widely applied descriptors.
C1 [Sadeghi, Bahman; Jamshidi, Kamal; Vafaei, Abbas; Monadjemi, S. Amirhassan] Univ Isfahan, Fac Comp Engn, Esfahan, Iran.
C3 University of Isfahan
RP Jamshidi, K (corresponding author), Univ Isfahan, Fac Comp Engn, Esfahan, Iran.
EM B.Sadeghi@eng.ui.ac.ir; Jamshidi@eng.ui.ac.ir;
   Abbas_Vafaei@eng.ui.ac.ir; Monadjemi@eng.ui.ac.ir
CR Alahi A., 2012, IEEE C CVPR
   [Anonymous], 8 INT C COMP VIS
   [Anonymous], 2003, ZUBUD DATASET ZURICH
   [Anonymous], 2011, IEEE INT C COMP VIS
   Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1007/11744023_32
   Calonder M, 2012, IEEE T PATTERN ANAL, V34, P1281, DOI 10.1109/TPAMI.2011.222
   Desai A., 2014, SSIAI
   Ferrari V, 2004, LECT NOTES COMPUT SC, V3021, P40
   Kang TK, 2015, PATTERN RECOGN, V48, P670, DOI 10.1016/j.patcog.2014.06.022
   Ke Y., 2004, P 2004 IEEE COMP SOC
   Lazebnik S, 2005, IEEE T PATTERN ANAL, V27, P1265, DOI 10.1109/TPAMI.2005.151
   Lazebnik S., 2005, COMPUTER VISION PATT
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Matas J, 2004, IMAGE VISION COMPUT, V22, P761, DOI 10.1016/j.imavis.2004.02.006
   Mikolajczyk K, 2005, IEEE T PATTERN ANAL, V27, P1615, DOI 10.1109/TPAMI.2005.188
   Mikolajczyk K, 2004, INT J COMPUT VISION, V60, P63, DOI 10.1023/B:VISI.0000027790.02288.f2
   Rublee E, 2011, IEEE I CONF COMP VIS, P2564, DOI 10.1109/ICCV.2011.6126544
   Sivic J., 2003, 9 INT C COMP VIS
   Smith SM, 1997, INT J COMPUT VISION, V23, P45, DOI 10.1023/A:1007963824710
   Song TC, 2013, IEEE SIGNAL PROC LET, V20, P59, DOI 10.1109/LSP.2012.2229273
   Tamrakar D, 2016, MULTIMED TOOLS APPL, V75, P5777, DOI 10.1007/s11042-015-2541-5
   Tola E, 2010, IEEE T PATTERN ANAL, V32, P815, DOI 10.1109/TPAMI.2009.77
   Tuytelaars T, 2004, INT J COMPUT VISION, V59, P61, DOI 10.1023/B:VISI.0000020671.28016.e8
   Tuytelaars T, 2007, FOUND TRENDS COMPUT, V3, P177, DOI 10.1561/0600000017
   Yu YN, 2012, IEEE T IMAGE PROCESS, V21, P229, DOI 10.1109/TIP.2011.2160271
   Zhang MM, 2014, OPTIK, V125, P1469, DOI 10.1016/j.ijleo.2013.09.007
   Zheng M., 2014, IEEE CHINESE GUIDANC, DOI [10. 1109/CGNCC. 2014. 7007582, DOI 10.1109/CGNCC.2014.7007582]
   Zhou W, 2014, IEEE SIGNAL PROC LET, V21, P339, DOI 10.1109/LSP.2013.2294458
NR 28
TC 1
Z9 1
U1 0
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2018
VL 34
IS 11
BP 1579
EP 1595
DI 10.1007/s00371-017-1433-2
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GV9ZR
UT WOS:000446521700009
DA 2024-07-18
ER

PT J
AU Guo, Y
   Wang, F
   Xin, JM
AF Guo, Yu
   Wang, Fei
   Xin, Jingmin
TI Point-wise saliency detection on 3D point clouds via covariance
   descriptors
SO VISUAL COMPUTER
LA English
DT Article
DE Point clouds saliency; Covariance descriptor; Sigma sets; Principal
   component analysis; Keypoint detection; Smoothing
ID MESH SALIENCY; MODEL
AB In the human visual system, visual saliency perception is a rapid pre-attention processing mechanism, which can benefit myriad visual tasks, such as segmentation, localization, and detection. While most research is devoted to saliency detection on 2D images and 3D meshes, little work has been performed for efficient saliency detection on 3D point clouds. In this paper, we present a novel point clouds saliency detection method by employing principal component analysis (PCA) in a sigma-set feature space. In this method, we first construct local shape descriptors based on covariance matrices for saliency detection, considering that covariance matrices can naturally model nonlinear correlations of different low-level compact and rotational-invariant features. Secondly, we transform these covariance matrices to vector descriptors in Euclidean vector space by applying the sigma-point technique, which keeps the inner statistics of regions of 3D point clouds. Based on our informative descriptors, PCA is employed in the descriptor space for identifying saliency patterns in a point cloud. Our method shows its advantages of being structure-sensitive, capturing geometry information and computationally efficient. Experimental results demonstrate that our method achieves good performance without using any topological information.
C1 [Guo, Yu; Wang, Fei; Xin, Jingmin] Xi An Jiao Tong Univ, Inst Artificial Intelligence & Robot, Xian 710049, Shaanxi, Peoples R China.
   [Guo, Yu; Wang, Fei; Xin, Jingmin] Xi An Jiao Tong Univ, Natl Engn Lab Visual Informat Proc & Applicat, Xian 710049, Shaanxi, Peoples R China.
   [Guo, Yu; Wang, Fei; Xin, Jingmin] Shaanxi Prov Key Lab Digital Technol & Intelligen, Xian 710049, Shaanxi, Peoples R China.
C3 Xi'an Jiaotong University; Xi'an Jiaotong University
RP Wang, F (corresponding author), Xi An Jiao Tong Univ, Inst Artificial Intelligence & Robot, Xian 710049, Shaanxi, Peoples R China.; Wang, F (corresponding author), Xi An Jiao Tong Univ, Natl Engn Lab Visual Informat Proc & Applicat, Xian 710049, Shaanxi, Peoples R China.; Wang, F (corresponding author), Shaanxi Prov Key Lab Digital Technol & Intelligen, Xian 710049, Shaanxi, Peoples R China.
EM wfx@mail.xjtu.edu.cn; jxin@mail.xjtu.edu.cn
RI Wang, Fei/ITV-5151-2023
OI Wang, Fei/0000-0003-3462-8472; Guo, Yu/0000-0002-5489-8288
CR Akman O, 2010, LECT NOTES COMPUT SC, V6474, P290, DOI 10.1007/978-3-642-17688-3_28
   [Anonymous], 2005, ACM Trans. Appl. Percep.
   [Anonymous], 2007, PROC IEEE C COMPUT V, DOI 10.1109/CVPR.2007.383267
   [Anonymous], 3D IMAGE PROCESSING
   [Anonymous], 2009, THESIS
   Castellani U, 2008, COMPUT GRAPH FORUM, V27, P643, DOI 10.1111/j.1467-8659.2008.01162.x
   Chen XW, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366151
   Cheng MM, 2011, PROC CVPR IEEE, P409, DOI 10.1109/CVPR.2011.5995344
   Cirujeda P, 2015, INT J COMPUT VISION, V115, P306, DOI 10.1007/s11263-015-0820-2
   Desbrun M, 1999, COMP GRAPH, P317, DOI 10.1145/311535.311576
   Dutagaci H, 2012, VISUAL COMPUT, V28, P901, DOI 10.1007/s00371-012-0746-4
   Gal R, 2006, ACM T GRAPHIC, V25, P130, DOI 10.1145/1122501.1122507
   Goferman S, 2012, IEEE T PATTERN ANAL, V34, P1915, DOI 10.1109/TPAMI.2011.272
   Guy G., 1995, Proceedings International Symposium on Computer Vision (Cat. No.95TB100006), P599, DOI 10.1109/ISCV.1995.477067
   Hariri W, 2016, PATTERN RECOGN LETT, V78, P1, DOI 10.1016/j.patrec.2016.03.028
   Hong XP, 2009, PROC CVPR IEEE, P1802, DOI 10.1109/CVPRW.2009.5206742
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Le Meur O, 2013, BEHAV RES METHODS, V45, P251, DOI 10.3758/s13428-012-0226-9
   Lee CH, 2005, ACM T GRAPHIC, V24, P659, DOI 10.1145/1073204.1073244
   Leifman G, 2012, PROC CVPR IEEE, P414, DOI 10.1109/CVPR.2012.6247703
   Liu XY, 2016, COMPUT GRAPH-UK, V57, P12, DOI 10.1016/j.cag.2016.03.001
   Liu XP, 2016, VISUAL COMPUT, V32, P1121, DOI 10.1007/s00371-015-1184-x
   Margolin R, 2013, PROC CVPR IEEE, P1139, DOI 10.1109/CVPR.2013.151
   Moonen B., 2003, METRIC COVARIANCE MA
   Nealen A., 2006, P 4 INT C COMP GRAPH, P381, DOI DOI 10.1145/1174429.1174494
   Novatnack J, 2007, IEEE I CONF COMP VIS, P2001
   Pauly M, 2003, COMPUT GRAPH FORUM, V22, P281, DOI 10.1111/1467-8659.00675
   Pauly M, 2002, VIS 2002: IEEE VISUALIZATION 2002, PROCEEDINGS, P163, DOI 10.1109/VISUAL.2002.1183771
   Qi W, 2017, VISUAL COMPUT, V33, P209, DOI 10.1007/s00371-015-1176-x
   Shilane P, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1243980.1243981
   Shtrom E, 2013, IEEE I CONF COMP VIS, P3591, DOI 10.1109/ICCV.2013.446
   Sipiran I, 2011, VISUAL COMPUT, V27, P963, DOI 10.1007/s00371-011-0610-y
   Song R, 2018, VISUAL COMPUT, V34, P323, DOI 10.1007/s00371-016-1334-9
   Song R, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2530691
   Sun JA, 2009, COMPUT GRAPH FORUM, V28, P1383, DOI 10.1111/j.1467-8659.2009.01515.x
   Tabia H, 2015, IEEE T MULTIMEDIA, V17, P1591, DOI 10.1109/TMM.2015.2457676
   Tao PP, 2015, COMPUT GRAPH-UK, V46, P264, DOI 10.1016/j.cag.2014.09.023
   Tasse F. P., 2016, SIGGRAPH ASIA 2016 T
   Tasse FP, 2015, IEEE I CONF COMP VIS, P163, DOI 10.1109/ICCV.2015.27
   Tuzel O, 2008, IEEE T PATTERN ANAL, V30, P1713, DOI 10.1109/TPAMI.2008.75
   Tuzel O, 2006, LECT NOTES COMPUT SC, V3952, P589
   Van Rijsbergen C. J., 1979, Information Retrieval, V2nd
   Wang SF, 2015, COMPUT AIDED GEOM D, V35-36, P206, DOI 10.1016/j.cagd.2015.03.003
   Wu JL, 2013, GRAPH MODELS, V75, P255, DOI 10.1016/j.gmod.2013.05.002
   Yang BL, 2016, VISUAL COMPUT, V32, P1415, DOI 10.1007/s00371-015-1129-4
   Yang C, 2013, PROC CVPR IEEE, P3166, DOI 10.1109/CVPR.2013.407
   Yanlei Wang., 2016, VACUUM, Vxxx, P1
   Zhu WJ, 2014, PROC CVPR IEEE, P2814, DOI 10.1109/CVPR.2014.360
NR 48
TC 21
Z9 22
U1 2
U2 41
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2018
VL 34
IS 10
BP 1325
EP 1338
DI 10.1007/s00371-017-1416-3
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GR0KK
UT WOS:000442204400005
DA 2024-07-18
ER

PT J
AU Bernard, J
   Zeppelzauer, M
   Sedlmair, M
   Aigner, W
AF Bernard, Juergen
   Zeppelzauer, Matthias
   Sedlmair, Michael
   Aigner, Wolfgang
TI VIAL: a unified process for visual interactive labeling
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Euro VA Conference
CY JUN 12-13, 2017
CL Barcelona, SPAIN
DE Information visualization; Visual analytics; Machine learning; Labeling;
   Active learning; Classification; Regression; Similarity search; Visual
   interactive labeling; Labeling
ID ANALYTICS; EXPLORATION; FRAMEWORK; QUALITY; DESIGN; MODELS
AB The assignment of labels to data instances is a fundamental prerequisite for many machine learning tasks. Moreover, labeling is a frequently applied process in visual interactive analysis approaches and visual analytics. However, the strategies for creating labels usually differ between these two fields. This raises the question whether synergies between the different approaches can be attained. In this paper, we study the process of labeling data instances with the user in the loop, from both the machine learning and visual interactive perspective. Based on a review of differences and commonalities, we propose the "visual interactive labeling" (VIAL) process that unifies both approaches. We describe the six major steps of the process and discuss their specific challenges. Additionally, we present two heterogeneous usage scenarios from the novel VIAL perspective, one on metric distance learning and one on object detection in videos. Finally, we discuss general challenges to VIAL and point out necessary work for the realization of future VIAL approaches.
C1 [Bernard, Juergen] Tech Univ Darmstadt, Darmstadt, Germany.
   [Zeppelzauer, Matthias; Aigner, Wolfgang] St Polten Univ Appl Sci, Inst Creat Media Technol, St Polten, Austria.
   [Sedlmair, Michael] Jacobs Univ, Bremen, Germany.
   [Bernard, Juergen] Fraunhofer Inst Comp Graph Res, IGD, Darmstadt, Germany.
   [Aigner, Wolfgang] TU Wien, Vienna, Austria.
C3 Technical University of Darmstadt; St. Polten University of Applied
   Sciences; Jacobs University; Fraunhofer Gesellschaft; Technische
   Universitat Wien
RP Bernard, J (corresponding author), Tech Univ Darmstadt, Darmstadt, Germany.; Bernard, J (corresponding author), Fraunhofer Inst Comp Graph Res, IGD, Darmstadt, Germany.
EM juergen.bernard@gris.tu-darmstadt.de
RI Bernard, Jürgen/M-5499-2019; Bernard, Jürgen/AAK-5732-2021
OI Bernard, Jürgen/0000-0001-8741-9709; Bernard,
   Jürgen/0000-0001-8741-9709; NASSREDDINE, Redhaounia/0000-0002-5436-6484;
   Zeppelzauer, Matthias/0000-0003-0413-4746; Sedlmair,
   Michael/0000-0001-7048-9292
FU Deutsche Forschungsgemeinschaft (DFG) [I 2850]; Lead Agency Verfahren
   (DACH) "Visual Segmentation and Labeling of Multivariate Time Series
   (VIS-SECT)"; Austrian Research Promotion Agency (FFG) [7179681,
   7189193]; Austrian Ministry for Transport, Innovation and Technology
   under the initiative "ICT of the future" via the project "VALiD"
   [845598]; Austrian Research Fund (FWF) via the project "KAVA-Time"
   [P25489-N23]; Austrian Research Fund (FWF) via the project "VisOn-Fire"
   [P27975-NBL]; Lower Austrian Research and Education Company; Provincial
   Government of Lower Austria (NFB), Department of Science and Research
   via the project "IntelliGait" [LSC14-005]
FX This work is an extended version of a previous EuroVA paper [20]
   entitled "A Unified Process for Visual Interactive Labeling." This work
   was supported by the Deutsche Forschungsgemeinschaft (DFG), Project No.
   I 2850, Lead Agency Verfahren (DACH) "Visual Segmentation and Labeling
   of Multivariate Time Series (VIS-SECT)," the Austrian Research Promotion
   Agency (FFG), Project Nos. 7179681, 7189193, the Austrian Ministry for
   Transport, Innovation and Technology under the initiative "ICT of the
   future" via the project "VALiD" (Project No. 845598), the Austrian
   Research Fund (FWF) via the projects "KAVA-Time" (Project No.
   P25489-N23), and "VisOn-Fire" (Project No. P27975-NBL), as well as by
   the Lower Austrian Research and Education Company and the Provincial
   Government of Lower Austria (NFB), Department of Science and Research
   via the project "IntelliGait" (Project No. LSC14-005).
CR Abe N., 1998, Machine Learning. Proceedings of the Fifteenth International Conference (ICML'98), P1
   [Anonymous], 2008, Visual Analytics: Definition, Process, and Challenges, DOI DOI 10.1007/978-3-540-70956-5_7
   [Anonymous], 2012, Proceedings of SIGRAD 2012; Interactive Visual Analysis of Data; November 29-30; 2012; Vaxjo; Sweden
   [Anonymous], 2009, SICS Technical Report
   [Anonymous], 2009, Technical report
   [Anonymous], TREC
   [Anonymous], 2006, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
   [Anonymous], VISUALIZATIONS DEEP
   [Anonymous], 2016, EUROVIS WORKSH VIS A
   Attenberg Josh, 2011, ACM SIGKDD Explorations Newsl, V12, P36, DOI [DOI 10.1145/1964897.1964906, 10.1145/1964897.1964906]
   Baeza-Yates Ricardo, 1999, MODERN INFORM RETRIE, V463
   Behrisch M, 2014, IEEE CONF VIS ANAL, P43, DOI 10.1109/VAST.2014.7042480
   Bellet A., ARXIV13066709 CORR
   Bengio Y., 2012, JMLR WORKSHOP C P, P17
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Bernard J., 2014, IEEE VIS AN SCI TECH, DOI [10.1109/VAST.2014.7042503, DOI 10.1109/VAST.2014.7042503]
   Bernard J, 2012, INT C KNOWLEDGE MANA, P22, DOI [10.1145/2362456.2362485, DOI 10.1145/2362456.2362485]
   Bernard J., 2015, Proceedings of the 2015 Workshop on Visual Analytics in Healthcare - VAHC'15, P1, DOI [10.1145/2836034.2836035, DOI 10.1145/2836034.2836035]
   BERNARD J., 2017, EUROVIS WORKSH VIS A, DOI [10.2312/eurova.20171123, DOI 10.2312/EUROVA.20171123]
   Bernard J., 2017, ELECT IMAG, V2017, P34, DOI DOI 10.2352/ISSN.2470-1173.2017.1.VDA-387
   Bernard J., 2015, THESIS
   Bernard J, 2017, PROCEEDINGS OF THE 12TH INTERNATIONAL JOINT CONFERENCE ON COMPUTER VISION, IMAGING AND COMPUTER GRAPHICS THEORY AND APPLICATIONS (VISIGRAPP 2017), VOL 3, P75, DOI 10.5220/0006116400750087
   Bernard J, 2018, IEEE T VIS COMPUT GR, V24, P298, DOI 10.1109/TVCG.2017.2744818
   Bernard J, 2015, INT J DIGIT LIBRARIE, V16, P37, DOI 10.1007/s00799-014-0134-y
   Bernard Jurgen, 2014, P 22 INT C CENTR EUR, V22, P329
   Blascheck T., 2014, PROC EUROVIS, P1, DOI DOI 10.2312/EUROVISSTAR.20141173
   Brown ET, 2012, IEEE CONF VIS ANAL, P83, DOI 10.1109/VAST.2012.6400486
   Buhrmester M, 2011, PERSPECT PSYCHOL SCI, V6, P3, DOI 10.1177/1745691610393980
   Card S. K., 1991, Human Factors in Computing Systems. Reaching Through Technology. CHI '91. Conference Proceedings, P181, DOI 10.1145/108844.108874
   Card S K., 1999, READINGS INFORM VISU
   Chapelle O., 2006, SEMISUPERVISED LEARN
   Chen M, 2016, IEEE T VIS COMPUT GR, V22, P2619, DOI 10.1109/TVCG.2015.2513410
   Choo J, 2013, IEEE COMPUT GRAPH, V33, P22, DOI 10.1109/MCG.2013.39
   Craik K., 1943, NATURE EXPLANATION
   Dagli CK, 2006, LECT NOTES COMPUT SC, V4071, P123
   Elmqvist N, 2010, IEEE T VIS COMPUT GR, V16, P439, DOI 10.1109/TVCG.2009.84
   Endert A, 2017, COMPUT GRAPH FORUM, V36, P458, DOI 10.1111/cgf.13092
   Endert A., 2011, 2011 IEEE Conference on Visual Analytics Science and Technology, P121, DOI 10.1109/VAST.2011.6102449
   Endert A, 2012, IEEE T VIS COMPUT GR, V18, P2879, DOI 10.1109/TVCG.2012.260
   Endert Alex., 2012, P SIGCHI C HUM FACT, P473, DOI DOI 10.1145/2207676.2207741
   Gleicher M, 2016, BIG DATA-US, V4, P75, DOI 10.1089/big.2016.0007
   Gleicher M, 2011, INFORM VISUAL, V10, P289, DOI 10.1177/1473871611416549
   Gschwandtner T., 2012, TAXONOMY DIRTY TIME, P58, DOI [10.1007/978-3-642-32498-7_5, DOI 10.1007/978-3-642-32498-7_5]
   Heimerl F, 2012, IEEE T VIS COMPUT GR, V18, P2839, DOI 10.1109/TVCG.2012.277
   Höferlin B, 2012, IEEE CONF VIS ANAL, P23, DOI 10.1109/VAST.2012.6400492
   Hoi Steven C. H., 2006, Proceedings of the International Conference on the World Wide Web (WWW-2006), P633, DOI [10.1145/1135777.1135870, DOI 10.1145/1135777.1135870]
   Janetzko H, 2014, IEEE CONF VIS ANAL, P13, DOI 10.1109/VAST.2014.7042477
   Kandel S, 2011, INFORM VISUAL, V10, P271, DOI 10.1177/1473871611415994
   Karpinski M, 1997, J COMPUT SYST SCI, V54, P169, DOI 10.1006/jcss.1997.1477
   Krause J, 2014, IEEE T VIS COMPUT GR, V20, P1614, DOI 10.1109/TVCG.2014.2346482
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Lewis J., 2012, P ANN M COGN SCI SOC, P1870
   Liu ZC, 2010, IEEE T VIS COMPUT GR, V16, P999, DOI 10.1109/TVCG.2010.177
   Losing V, 2018, NEUROCOMPUTING, V275, P1261, DOI 10.1016/j.neucom.2017.06.084
   Mamani GMH, 2013, COMPUT GRAPH FORUM, V32, P291, DOI 10.1111/cgf.12116
   Mitrovic D, 2010, ADV COMPUT, V78, P71, DOI 10.1016/S0065-2458(10)78003-7
   Moehrmann J, 2011, LECT NOTES COMPUT SC, V6761, P618, DOI 10.1007/978-3-642-21602-2_67
   Mühlbacher T, 2014, IEEE T VIS COMPUT GR, V20, P1643, DOI 10.1109/TVCG.2014.2346578
   Mühlbacher T, 2013, IEEE T VIS COMPUT GR, V19, P1962, DOI 10.1109/TVCG.2013.125
   Norman D.A., 2002, DESIGN EVERYDAY THIN
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Qi GJ, 2009, IEEE T PATTERN ANAL, V31, P1880, DOI 10.1109/TPAMI.2008.218
   Rauber PE, 2017, IEEE T VIS COMPUT GR, V23, P101, DOI 10.1109/TVCG.2016.2598838
   Riek LD, 2011, LECT NOTES COMPUT SC, V6974, P277, DOI 10.1007/978-3-642-24600-5_31
   Russell BC, 2008, INT J COMPUT VISION, V77, P157, DOI 10.1007/s11263-007-0090-8
   Sacha D., 2016, ARTIFICIAL NEURAL NE
   Sacha D, 2017, NEUROCOMPUTING, V268, P164, DOI 10.1016/j.neucom.2017.01.105
   Sacha D, 2016, IEEE T VIS COMPUT GR, V22, P240, DOI 10.1109/TVCG.2015.2467591
   Sacha D, 2014, IEEE T VIS COMPUT GR, V20, P1604, DOI 10.1109/TVCG.2014.2346481
   Salton G., 1997, Readings in Information Retrieval, P355
   Sarkar A, 2016, S VIS LANG HUM CEN C, P78, DOI 10.1109/VLHCC.2016.7739668
   Schreck T, 2009, INFORM VISUAL, V8, P14, DOI 10.1057/ivs.2008.29
   Sedlmair M, 2015, COMPUT GRAPH FORUM, V34, P201, DOI 10.1111/cgf.12632
   Sedlmair M, 2014, IEEE T VIS COMPUT GR, V20, P2161, DOI 10.1109/TVCG.2014.2346321
   Sedlmair M, 2012, IEEE T VIS COMPUT GR, V18, P2431, DOI 10.1109/TVCG.2012.213
   Seifert C., 2010, Proceedings 2010 10th IEEE International Conference on Data Mining Workshops (ICDMW 2010), P418, DOI 10.1109/ICDMW.2010.181
   Sessler D., 2014, ADOPTING MENTAL SIMI
   Settles B., 2011, P C EMP METH NAT LAN, P1467
   Settles B, 2008, ADV NEURAL INFORM PR, V21, P1289
   Settles B., 2012, Active Learning, V6, P1, DOI DOI 10.1007/978-3-031-01560-1
   Settles B, 2008, P C EMP METH NAT LAN, P1070, DOI DOI 10.3115/1613715.1613855
   Seung H. S., 1992, Proceedings of the Fifth Annual ACM Workshop on Computational Learning Theory, P287, DOI 10.1145/130385.130417
   Shurkhovetskyy G, 2018, COMPUT GRAPH FORUM, V37, P125, DOI 10.1111/cgf.13237
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Stasko J, 2008, INFORM VISUAL, V7, P118, DOI 10.1057/palgrave.ivs.9500180
   Stolper CD, 2014, IEEE T VIS COMPUT GR, V20, P1653, DOI 10.1109/TVCG.2014.2346574
   Tie-Yan Liu, 2009, Foundations and Trends in Information Retrieval, V3, P225, DOI 10.1561/1500000016
   Tuia D, 2011, IEEE J-STSP, V5, P606, DOI 10.1109/JSTSP.2011.2139193
   Turkay C, 2017, IEEE T VIS COMPUT GR, V23, P131, DOI 10.1109/TVCG.2016.2598470
   van den Elzen S., 2011, 2011 IEEE Conference on Visual Analytics Science and Technology, P151, DOI 10.1109/VAST.2011.6102453
   van der Corput P, 2017, COMPUT GRAPH FORUM, V36, P295, DOI 10.1111/cgf.13188
   van Wijk JJ, 2005, IEEE VISUALIZATION 2005, PROCEEDINGS, P79
   Vapnik V., 1999, NATURE STAT LEARNING
   Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb
   Visentini I., 2008, PATT REC 2008 ICPR 2, P1
   Von Ahn L, 2004, P SIGCHI C HUM FACT, DOI DOI 10.1145/985692.985733
   Wall E, 2018, IEEE T VIS COMPUT GR, V24, P288, DOI 10.1109/TVCG.2017.2745078
   Wang M, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1899412.1899414
   Wenskovitch John, 2017, P 2 WORKSH HUM LOOP, DOI [10.1145/3077257.3077259, DOI 10.1145/3077257.3077259]
   Wongsuphasawat K, 2018, IEEE T VIS COMPUT GR, V24, P1, DOI 10.1109/TVCG.2017.2744878
   Wu Y, 2006, 2006 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO - ICME 2006, VOLS 1-5, PROCEEDINGS, P529, DOI 10.1109/ICME.2006.262442
   Yang L., 2006, Michigan State Universiy
   Yosinski J., 2014, Adv Neural Inf Process Syst, V2, P3320, DOI DOI 10.48550/ARXIV.1411.1792
   Yosinski J., 2015, ARXIV150606579, V2015, P12
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zhu QA, 2010, LECT NOTES COMPUT SC, V6065, P245
NR 106
TC 44
Z9 47
U1 1
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2018
VL 34
IS 9
BP 1189
EP 1207
DI 10.1007/s00371-018-1500-3
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA GQ5MB
UT WOS:000441727000006
DA 2024-07-18
ER

PT J
AU Walch, A
   Luksch, C
   Szabo, A
   Steinlechner, H
   Haaser, G
   Schwärzler, M
   Maierhofer, S
AF Walch, Andreas
   Luksch, Christian
   Szabo, Attila
   Steinlechner, Harald
   Haaser, Georg
   Schwaerzler, Michael
   Maierhofer, Stefan
TI Lens flare prediction based on measurements with real-time visualization
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 35th Computer Graphics International conference (CGI)
CY JUN 11-14, 2018
CL INDONESIA
SP Comp Graph Soc, Nanyang Technol Univ
DE Lens flare; Data-driven workflow; Real time
AB Lens flare is a visual phenomenon caused by interreflection of light within a lens system. This effect is often seen as an undesired artifact, but it also gives rendered images a realistic appearance and is even used for artistic purposes. In the area of computer graphics, several simulation-based approaches have been presented to render lens flare for a given spherical lens system. For physically reliable results, these approaches require an accurate description of that system, which differs from camera to camera. Also, for the lens flares appearance, crucial parameters-especially the anti-reflection coatings-can often only be approximated. In this paper we present a novel workflow for generating physically plausible renderings of lens flare phenomena by analyzing the lens flares captured with a camera. Our method allows predicting the occurrence of lens flares for a given light setup. This is an often requested feature in light planning applications in order to efficiently avoid lens flare-prone light positioning. A model with a tight parameter set and a GPU-based rendering method allows our approach to be used in real-time applications.
C1 [Walch, Andreas; Luksch, Christian; Szabo, Attila] VRVis Res Ctr, Semant Modeling & Acquisit Grp, Vienna, Austria.
   [Steinlechner, Harald; Haaser, Georg; Maierhofer, Stefan] VRVis Res Ctr, Vienna, Austria.
   [Schwaerzler, Michael] VRVis Res Ctr, Res Grp 3, Vienna, Austria.
RP Walch, A (corresponding author), VRVis Res Ctr, Semant Modeling & Acquisit Grp, Vienna, Austria.
EM walch@vrvis.at
OI Walch, Andreas/0000-0002-4567-7942
FU BMVIT; BMWFW; Styria; SFG; Vienna Business Agency in the scope of COMET
   - Competence Centers for Excellent Technologies [854174]
FX VRVis is funded by BMVIT, BMWFW, Styria, SFG and Vienna Business Agency
   in the scope of COMET - Competence Centers for Excellent Technologies
   (854174) which is managed by FFG. Conflict of Interest: The authors
   declare that they have no conflict of interest.
CR Alspach T, 2009, US Patent, Patent No. [7,526,417, 7526417]
   [Anonymous], 2016, GEN MIN MOT CONTR
   Bishop Christopher M, 2006, PATTERN RECOGNITION, DOI DOI 10.18637/JSS.V017.B05
   Chaumond J, 2007, REALISTIC CAMERA LEN
   Franke G., 1966, PHYS OPTICS PHOTOGRA, Vfirst
   Hanika J, 2014, COMPUT GRAPH FORUM, V33, P323, DOI 10.1111/cgf.12301
   Hennessy P., 2015, IMPLEMENTATION NOTES
   Hullin M, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1965003
   Hullin MB, 2012, COMPUT GRAPH FORUM, V31, P1375, DOI 10.1111/j.1467-8659.2012.03132.x
   Joo H, 2016, COMPUT GRAPH FORUM, V35, P99, DOI 10.1111/cgf.12953
   Keshmirian A., 2008, PHYS BASED APPROACH
   Kilgard M. J., 2000, FAST OPENGL RENDERIN
   King Y., 2000, GAME PROGRAMMING GEM, P515
   Lee S, 2013, COMPUT GRAPH FORUM, V32, P1, DOI 10.1111/cgf.12145
   Light I., 2014, MAGIC OPENEXR
   Mchugh S., 2005, UNDERSTANDING CAMERA
   Pixar, 2008, IMP LENS CREAT LOOK
   Sekulic D., 2004, EFFICIENT OCCLUSION, P487
   Steinert B, 2011, COMPUT GRAPH FORUM, V30, P1643, DOI 10.1111/j.1467-8659.2011.01851.x
   TOCCI M, 2007, QUANTIFYING VEILING
   Towell J., 2012, BRIEF HIST MOST OVER
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   WOERNER M, 2009, COMMUNICATION
NR 23
TC 2
Z9 3
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2018
VL 34
IS 9
BP 1155
EP 1164
DI 10.1007/s00371-018-1552-4
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA GQ5MB
UT WOS:000441727000003
DA 2024-07-18
ER

PT J
AU Aristidou, A
AF Aristidou, Andreas
TI Hand tracking with physiological constraints
SO VISUAL COMPUTER
LA English
DT Article
DE Geometric Algebra; Hand Tracking; Inverse Kinematics; Motion Capture;
   Physiological Constraints
ID REAL-TIME; INVERSE KINEMATICS; POSE ESTIMATION; MODEL
AB Articulated hand tracking systems have been commonly used in virtual reality applications, including systems with human-computer interaction or interaction with game consoles; hand pose estimation has various other applications comprising sign language recognition and animation synthesis. The advanced technological achievements in motion capture over the last decade allow data acquisition with high accuracy and low cost. However, due to the high complexity of the human hand, it is still challenging to animate a hand model able to deal in details with the anatomical and physiological constraints of the hand. In this paper, we present a simple and efficient methodology for tracking and reconstructing 3D hand poses. Using an optical motion capture system, where markers are positioned at strategic points, we manage to acquire the movement of the hand and establish its orientation using a minimum number of markers. An Inverse Kinematics solver was then employed to control the postures of the hand, subject to physiological constraints that restrict the allowed movements to a feasible and natural set. The proposed methodology produces smooth and biomechanically correct movements, while the required processing time remains low, enabling an effective real-time hand motion tracking and reconstruction system.
C1 [Aristidou, Andreas] Univ Cyprus, Dept Comp Sci, CY-1678 Nicosia, Cyprus.
C3 University of Cyprus
RP Aristidou, A (corresponding author), Univ Cyprus, Dept Comp Sci, CY-1678 Nicosia, Cyprus.
EM a.aristidou@ieee.org
RI Aristidou, Andreas/AAI-8096-2020
OI Aristidou, Andreas/0000-0001-7754-0791
FU Office of Naval Research (ONR) Global [N62909-13-1-V090]
FX This work has received funding from the Office of Naval Research (ONR)
   Global (N62909-13-1-V090).
CR Albrecht I., 2003, ACM SIGGRAPH/Eurographics Symposium on Computer Animation, P98
   An K., 1992, J BIOMECH, V16, P419
   [Anonymous], 2005, ACM SIGGRAPH EUR S C, DOI DOI 10.1145/1073368.1073413
   [Anonymous], 2013, P GRAPHICS INTERFACE
   [Anonymous], 2011, P BRIT MACH VIS C
   [Anonymous], 2015, P COMP VIS PATT REC
   [Anonymous], 2008, MoCap for Artists: Workflow and Techniques for Motion Capture
   [Anonymous], 2013, P 10 IEEE INT C WORK
   [Anonymous], 2005, P 2005 ACM SIGGRAPHE, DOI DOI 10.1145/1073368.1073414
   Aristidou A., 2010, Proceedings of the 4th International Symposium on Communications, Control and Signal Processing, IEEE-ISCCSP '10, P1, DOI DOI 10.1109/ISCCSP.2010.5463419
   Aristidou A., VISUAL COMP, V29, P7
   Aristidou A., COMP ANIM VIRT WORLD, V27, P3557
   Aristidou A, 2011, GRAPH MODELS, V73, P243, DOI 10.1016/j.gmod.2011.05.003
   Baran I, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1239451.1239523, 10.1145/1276377.1276467]
   Bruser P., 1999, Finger Bone and Joint Injuries
   BUCHHOLZ B, 1992, J BIOMECH, V25, P149, DOI 10.1016/0021-9290(92)90272-3
   Cerveri P, 2007, ANN BIOMED ENG, V35, P1989, DOI 10.1007/s10439-007-9364-0
   Chen WY, 2007, 14TH INTERNATIONAL CONFERENCE ON IMAGE ANALYSIS AND PROCESSING, PROCEEDINGS, P247, DOI 10.1109/ICIAP.2007.4362787
   de La Gorce M, 2011, IEEE T PATTERN ANAL, V33, P1793, DOI 10.1109/TPAMI.2011.33
   Delaney Martin, 2008, Proj Inf Perspect, P1
   Dewaele G, 2006, LECT NOTES COMPUT SC, V3953, P578, DOI 10.1007/11744078_45
   Doran C., 2003, GEOMET ALGEB PHYS
   Feng ZQ, 2013, PATTERN RECOGN, V46, P590, DOI 10.1016/j.patcog.2012.07.019
   Fredriksson J., 2008, P 5 NORDIC C HUMAN C, P133, DOI [10.1145/1463160.1463175, DOI 10.1145/1463160.1463175]
   Fujiki R, 2005, LECT NOTES COMPUT SC, V3617, P850, DOI 10.1007/11553595_104
   Girshick R, 2011, IEEE I CONF COMP VIS, P415, DOI 10.1109/ICCV.2011.6126270
   Guan HY, 2006, PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION - PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE, P263
   Hestens D., 1984, CLIFFORD ALGEBRA GEO
   Hoyet L., 2012, P ACM SIGGRAPH S INT, P79, DOI DOI 10.1145/2159616.2159630.6,17
   Jörg S, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366208
   Kahlesz F, 2004, COMPUTER GRAPHICS INTERNATIONAL, PROCEEDINGS, P403, DOI 10.1109/CGI.2004.1309241
   Kaimakis P., P INT C COMP VIS PAT, P19
   Kaimakis P., P 3 INT S VIS COMP L, V1, P24
   Kerdvibulvech C., 2009, J IMAGE VIDEO PROCES, V2009
   Lasenby A. N., 2004, FINFENGTR483 CAMBR U
   Lasenby J, 1998, INT J COMPUT VISION, V26, P191, DOI 10.1023/A:1007901028047
   Lee C., 1998, P ACM S VIRTUAL REAL, P59
   Lee JH, 2002, ACM T GRAPHIC, V21, P491
   Liang H., 2014, IEEE T CIRC SYST VID, V99, P1
   Liang H, 2014, IEEE T MULTIMEDIA, V16, P1241, DOI 10.1109/TMM.2014.2306177
   Liang H, 2013, VISUAL COMPUT, V29, P837, DOI 10.1007/s00371-013-0822-4
   Lien CC, 1998, IMAGE VISION COMPUT, V16, P121, DOI 10.1016/S0262-8856(97)00041-3
   Lin J, 2000, WORKSHOP ON HUMAN MOTION, PROCEEDINGS, P121, DOI 10.1109/HUMO.2000.897381
   Liu C.K., 2008, ACM SIGGRAPH EUR S C, P163
   LIU CK, 2009, ACM T GRAPHIC, V28
   Ma ZY, 2014, VISUAL COMPUT, V30, P1133, DOI 10.1007/s00371-013-0894-1
   Majkowska A, 2006, P 2006 ACM SIGGRAPH, P309, DOI DOI 10.1145/1218064.1218106
   Mikic I, 2003, INT J COMPUT VISION, V53, P199, DOI 10.1023/A:1023012723347
   Oikonomidis I, 2012, PROC CVPR IEEE, P1862, DOI 10.1109/CVPR.2012.6247885
   Park J, 2006, ICAT 2006: 16TH INTERNATIONAL CONFERENCE ON ARTIFICIAL REALITY AND TELEXISTENCE - WORSHOPS, PROCEEDINGS, P395
   Qian C., 2014, P COMP VIS PATT REC
   Rehg J. M., 1994, Proceedings of the 1994 IEEE Workshop on Motion of Non-Rigid and Articulated Objects (Cat. No.94TH0671-8), P16, DOI 10.1109/MNRAO.1994.346260
   Schlattmann M, 2007, VRST 2007: ACM SYMPOSIUM ON VIRTUAL REALITY SOFTWARE AND TECHNOLOGY, PROCEEDINGS, P39
   Schroder M., 2015, Proceedings of the 8th ACM SIGGRAPH Conference on Motion in Games, P7, DOI 10.1145/2822013.2822026
   Shakhnarovich G, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P750
   Shan C., PATT RECOGN, V40, P1958
   Sharp T, 2015, CHI 2015: PROCEEDINGS OF THE 33RD ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P3633, DOI 10.1145/2702123.2702179
   Shotton J., 2013, T PATT ANAL MACH INT, V35
   Sibille L, 2002, CARS 2002: COMPUTER ASSISTED RADIOLOGY AND SURGERY, PROCEEDINGS, P7
   Stenger B, 2001, PROC CVPR IEEE, P310
   Stenger B, 2006, IEEE T PATTERN ANAL, V28, P1372, DOI 10.1109/TPAMI.2006.189
   Sudderth ErikB., 2004, Advances in Neural Information Processing Systems, P1369
   Tagliasacchi A, 2015, COMPUT GRAPH FORUM, V34, P101, DOI 10.1111/cgf.12700
   Tao S., 2016, COLLISION FREE MOTIO, P1
   Tompson J, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2629500
   Ulrich Neumann ZhenyaoMoand., 2006, IEEE Conference on Computer Vision and Pattern Recognition, P1499, DOI DOI 10.1109/CVPR.2006.237
   Wang RY, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531369
   Wang Y., 2013, PROC 12 ACM SIGGRAPH, P15
   Wheatland N., 2015, COMP GRAPH FORUM, V34
   Xu C, 2013, IEEE I CONF COMP VIS, P3456, DOI 10.1109/ICCV.2013.429
   Ye Y, 2012, CHIN J NAT MEDICINES, V10, P1, DOI [10.3724/SP.J.1009.2012.00001, 10.1016/S1875-5364(12)60001-6]
   Zhao WP, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508412
   Zhao Wenping, 2012, Proceedings of the ACM SIGGRAPH/eurographics symposium on computer animation. Eurographics Association, P33, DOI [10.2312/SCA/SCA12/033-042, DOI 10.2312/SCA/SCA12/033-042]
NR 73
TC 12
Z9 14
U1 0
U2 18
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2018
VL 34
IS 2
BP 213
EP 228
DI 10.1007/s00371-016-1327-8
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FU6ZO
UT WOS:000424001800006
DA 2024-07-18
ER

PT J
AU Zabulis, X
   Lourakis, MIA
   Koutlemanis, P
AF Zabulis, Xenophon
   Lourakis, Manolis I. A.
   Koutlemanis, Panagiotis
TI Correspondence-free pose estimation for 3D objects from noisy depth data
SO VISUAL COMPUTER
LA English
DT Article
DE 3D pose estimation; Object localization; Depth images; RGB-D sensors;
   Particle swarm optimization; GPU computing
ID RECOGNITION; REGISTRATION; IMAGES
AB Estimating the pose of objects from depth data is a problem of considerable practical importance for many vision applications. This paper presents an approach for accurate and efficient 3D pose estimation from noisy 2.5D depth images obtained from a consumer depth sensor. Initialized with a coarsely accurate pose, the proposed approach applies a hypothesize-and-test scheme that combines stochastic optimization and graphics-based rendering to refine the supplied initial pose, so that it accurately accounts for a sensed depth image. Pose refinement employs particle swarm optimization to minimize an objective function that quantifies the misalignment between the acquired depth image and a rendered one that is synthesized from a hypothesized pose with the aid of an object mesh model. No explicit correspondences between the depth data and the model need to be established, whereas pose hypothesis rendering and objective function evaluation are efficiently performed on the GPU. Extensive experimental results demonstrate the superior performance of the proposed approach compared to the ICP algorithm, which is typically used for pose refinement in depth images. Furthermore, the experiments indicate the graceful degradation of its performance to limited computational resources and its robustness to noisy and reduced polygon count models, attesting its suitability for use with automatically scanned object models and common graphics hardware.
C1 [Zabulis, Xenophon; Lourakis, Manolis I. A.; Koutlemanis, Panagiotis] Fdn Res & Technol Hellas, Inst Comp Sci, N Plastira 100, Iraklion 70013, Greece.
C3 Foundation for Research & Technology - Hellas (FORTH)
RP Zabulis, X (corresponding author), Fdn Res & Technol Hellas, Inst Comp Sci, N Plastira 100, Iraklion 70013, Greece.
EM zabulis@ics.forth.gr; lourakis@ics.forth.gr; koutle@ics.forth.gr
RI Zabulis, Xenophon/D-6186-2011
OI Zabulis, Xenophon/0000-0002-1520-4327
FU European Commission [270138]; Foundation for Research and Technology
   Hellas-Institute of Computer Science (FORTH-ICS) internal RTD Programme
   'Ambient Intelligence and Smart Environments'
FX This work was partially supported by the European Commission FP7 DARWIN
   Project, Grant No. 270138 and the Foundation for Research and Technology
   Hellas-Institute of Computer Science (FORTH-ICS) internal RTD Programme
   'Ambient Intelligence and Smart Environments'.
CR [Anonymous], INT C COMP VIS
   [Anonymous], 2009, IEEE INT C ROB AUT
   [Anonymous], 2013, PROC ASIAN C COMPUT
   [Anonymous], 2015, 270139 EUR COMM FP7
   [Anonymous], BMVC
   Badino H., 2011, 2011 IEEE International Conference on Robotics and Automation (ICRA 2011), P3084, DOI 10.1109/ICRA.2011.5980275
   BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791
   Bouaziz S, 2013, COMPUT GRAPH FORUM, V32, P113, DOI 10.1111/cgf.12178
   Brachmann E, 2014, LECT NOTES COMPUT SC, V8690, P536, DOI 10.1007/978-3-319-10605-2_35
   Bratanic B, 2015, COMPUT VIS IMAGE UND, V141, P38, DOI 10.1016/j.cviu.2015.09.002
   Bronstein MM, 2010, PROC CVPR IEEE, P1704, DOI 10.1109/CVPR.2010.5539838
   Cao T.T., 2010, P 2010 ACM SIGGRAPH, P83, DOI 10.1145/1730804.1730818
   Choi C, 2012, IEEE INT C INT ROBOT, P3342, DOI 10.1109/IROS.2012.6386067
   Cignoni P, 2008, ERCIM NEWS, P45
   Collet A, 2011, INT J ROBOT RES, V30, P1284, DOI 10.1177/0278364911401765
   Darom T, 2012, IEEE T IMAGE PROCESS, V21, P2758, DOI 10.1109/TIP.2012.2183142
   Drost B, 2010, PROC CVPR IEEE, P998, DOI 10.1109/CVPR.2010.5540108
   Fischer J, 2013, IEEE INT CONF ROBOT, P2112, DOI 10.1109/ICRA.2013.6630860
   Flöry S, 2010, COMPUT AIDED GEOM D, V27, P60, DOI 10.1016/j.cagd.2009.09.001
   Frome A, 2004, LECT NOTES COMPUT SC, V3023, P224
   Garland M., 24 ANN C COMP GRAPH, P209
   Geiger A., 2011, LIBICP C LIB ITERATI
   Hernandez-Matas C, 2015, IEEE ENG MED BIO, P5650, DOI 10.1109/EMBC.2015.7319674
   Hodan T, 2015, IEEE INT C INT ROBOT, P4421, DOI 10.1109/IROS.2015.7354005
   HORN BKP, 1987, J OPT SOC AM A, V4, P629, DOI 10.1364/JOSAA.4.000629
   Huttenlocher, 2012, THEORY COMPUT, V8, P415, DOI [10.4086/toc.2012.v008a019, DOI 10.4086/TOC.2012.V008A019]
   Ivekovic S, 2008, EVOL COMPUT, V16, P509, DOI 10.1162/evco.2008.16.4.509
   Johnson AE, 1999, IEEE T PATTERN ANAL, V21, P433, DOI 10.1109/34.765655
   Kennedy J., 1995, 1995 IEEE International Conference on Neural Networks Proceedings (Cat. No.95CH35828), P1942, DOI 10.1109/ICNN.1995.488968
   Khoshelham K, 2012, SENSORS-BASEL, V12, P1437, DOI 10.3390/s120201437
   KRULL A, 2015, IEEE I CONF COMP VIS, P954, DOI DOI 10.1109/ICCV.2015.115
   Lourakis Manolis, 2013, Computer Vision Systems. 9th International Conference, ICVS 2013. Proceedings: LNCS 7963, P83, DOI 10.1007/978-3-642-39402-7_9
   Mian A.S., 2005, INT J OFSHAPE MODELI, V11, P253, DOI DOI 10.1142/S0218654305000797
   Nascimento ER, 2012, IEEE INT C INT ROBOT, P1720, DOI 10.1109/IROS.2012.6385693
   Oikonomidis I, 2011, IEEE I CONF COMP VIS, P2088, DOI 10.1109/ICCV.2011.6126483
   Park IK, 2010, MACH VISION APPL, V21, P749, DOI 10.1007/s00138-009-0209-8
   Pauwels K, 2014, IEEE INT C INT ROBOT, P2733, DOI 10.1109/IROS.2014.6942936
   Prankl J, 2015, IEEE INT C INT ROBOT, P96, DOI 10.1109/IROS.2015.7353360
   Rios-Cabrera R, 2013, IEEE I CONF COMP VIS, P2048, DOI 10.1109/ICCV.2013.256
   Rusinkiewicz S, 2001, THIRD INTERNATIONAL CONFERENCE ON 3-D DIGITAL IMAGING AND MODELING, PROCEEDINGS, P145, DOI 10.1109/IM.2001.924423
   Song SR, 2014, LECT NOTES COMPUT SC, V8694, P634, DOI 10.1007/978-3-319-10599-4_41
   Sun M, 2010, LECT NOTES COMPUT SC, V6315, P658, DOI 10.1007/978-3-642-15555-0_48
   Tejani A, 2014, LECT NOTES COMPUT SC, V8694, P462, DOI 10.1007/978-3-319-10599-4_30
   Tombari F, 2011, IEEE IMAGE PROC, P809, DOI 10.1109/ICIP.2011.6116679
   Wang W, 2015, J REAL-TIME IMAGE PR, V10, P667, DOI 10.1007/s11554-013-0380-z
   Wohlhart P, 2015, PROC CVPR IEEE, P3109, DOI 10.1109/CVPR.2015.7298930
   Yuille A, 2006, TRENDS COGN SCI, V10, P301, DOI 10.1016/j.tics.2006.05.002
   Zabulis X, 2015, LECT NOTES COMPUT SC, V9163, P263, DOI 10.1007/978-3-319-20904-3_25
   Zach C, 2015, PROC CVPR IEEE, P196, DOI 10.1109/CVPR.2015.7298615
   Zaharescu A, 2009, PROC CVPR IEEE, P373, DOI 10.1109/CVPRW.2009.5206748
   Zhang X, 2008, PROC CVPR IEEE, P117
   Zhang ZY, 2012, IEEE MULTIMEDIA, V19, P4, DOI 10.1109/MMUL.2012.24
NR 52
TC 9
Z9 9
U1 0
U2 8
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2018
VL 34
IS 2
BP 193
EP 211
DI 10.1007/s00371-016-1326-9
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FU6ZO
UT WOS:000424001800005
DA 2024-07-18
ER

PT J
AU Tsuchie, S
AF Tsuchie, Shoichi
TI Reconstruction of underlying curves with styling radius corners
SO VISUAL COMPUTER
LA English
DT Article
DE Underlying curve; Styling radius corner; Smooth changes in curvature and
   torsion
AB This paper presents a newcurve fitting framework for styling design data. Given a data set that represents a filleted-like curve, underlying curves (U-curves) and styling radius corners (SR-corners) are generated by fitting to low curvature parts and highly curved ones, respectively. A set of U-curves are firstly reconstructed as a unique C-0 composite B-spline curve, and then an SR-corner is reconstructed for each C-0 corner. This approach guarantees that U-curves can be smoothly connected through convex SR-corners while enabling full editing of the smooth corners up to sharp ones. Compared with existing schemes that naively fit a curve to each part, the proposed framework provides a guiding principle for the generation of curves, which is more suitable for styling design. Experimental results demonstrate that highquality curves can be generated even from real-world scanned data.
C1 [Tsuchie, Shoichi] Nihon Unisys Ltd, Koto Ku, 1-1-1 Toyosu, Tokyo 1358560, Japan.
RP Tsuchie, S (corresponding author), Nihon Unisys Ltd, Koto Ku, 1-1-1 Toyosu, Tokyo 1358560, Japan.
EM shoichi.tsuchie@unisys.co.jp
OI Tsuchie, Shoichi/0000-0002-9991-5635
CR [Anonymous], 2002, Algorithms for minimization without derivatives
   ATTNEAVE F, 1954, PSYCHOL REV, V61, P183, DOI 10.1037/h0054663
   Baran I, 2010, COMPUT GRAPH FORUM, V29, P655, DOI 10.1111/j.1467-8659.2009.01635.x
   Burchard H, 1994, Designing fair curves and surfaces, P3
   Cao J, 2008, COMPUT AIDED GEOM D, V25, P523, DOI 10.1016/j.cagd.2007.10.001
   Douglas D.H., 1973, Cartographica: The International Journal for Geographic Information and Geovisualization, V10, P112, DOI [https://doi.org/10.3138/FM57-6770-U75U-7727, DOI 10.1002/9780470669488.CH2]
   Duda R. O., 1973, PATTERN CLASSIFICATI, V3
   Farin G, 2006, COMPUT AIDED GEOM D, V23, P573, DOI 10.1016/j.cagd.2006.03.004
   Hosaka M., 1992, Modeling of Curves and Surfaces in CAD/CAM
   Hoschek J., 1993, FUNDAMENTALS COMPUTE
   Lai YK, 2008, IEEE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS 2008, PROCEEDINGS, P29, DOI 10.1109/SMI.2008.4547943
   Li GQ, 2001, COMPUTER GRAPHICS INTERNATIONAL 2001, PROCEEDINGS, P321, DOI 10.1109/CGI.2001.934690
   Li WS, 2005, COMPUT AIDED DESIGN, V37, P791, DOI 10.1016/j.cad.2004.09.008
   LLOYD SP, 1982, IEEE T INFORM THEORY, V28, P129, DOI 10.1109/TIT.1982.1056489
   McCrae J, 2009, COMPUT GRAPH-UK, V33, P452, DOI 10.1016/j.cag.2009.05.006
   MORETON HP, 1992, COMP GRAPH, V26, P167, DOI 10.1145/142920.134035
   Okaniwa S, 2012, IEEE T VIS COMPUT GR, V18, P1474, DOI 10.1109/TVCG.2011.262
   Piegl L., 1997, The Nurbs Book, Vsecond
   Ramer U., 1972, Comput. Graph. Image Process., V1, P244, DOI DOI 10.1016/S0146-664X(72)80017-0
   Schneider R., 1999, Curve and Surface Design: Saint-Malo, P371
   Tsuchie S, 2016, COMPUT AIDED DESIGN, V71, P39, DOI 10.1016/j.cad.2015.09.004
   Tsuchie S, 2014, COMPUT AIDED DESIGN, V46, P69, DOI 10.1016/j.cad.2013.08.019
   Wallner J, 2007, VISUAL COMPUT, V23, P83, DOI 10.1007/s00371-006-0088-1
   Yang HP, 2004, COMPUT AIDED DESIGN, V36, P639, DOI 10.1016/S0010-4485(03)00140-4
   Yoshida N, 2006, VISUAL COMPUT, V22, P896, DOI 10.1007/s00371-006-0076-5
   Zang Y, 2008, LECT NOTES COMPUT SC, V4975, P304
   Ziatdinov R, 2012, COMPUT AIDED GEOM D, V29, P510, DOI 10.1016/j.cagd.2012.03.006
NR 27
TC 6
Z9 7
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2017
VL 33
IS 9
BP 1197
EP 1210
DI 10.1007/s00371-016-1282-4
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FD1CS
UT WOS:000407275600009
DA 2024-07-18
ER

PT J
AU Hu, PP
   Komura, T
   Holden, D
   Zhong, YQ
AF Hu, Pengpeng
   Komura, Taku
   Holden, Daniel
   Zhong, Yueqi
TI Scanning and animating characters dressed in multiple-layer garments
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 34th Conference on Computer Graphics International (CGI)
CY JUN 27-30, 2017
CL Yokohama, JAPAN
SP Keio Univ, Fac Sci & Technol
DE Cloth animation; 3D scanning; Dressed character
ID REAL-TIME
AB Despite the development of user-friendly interfaces for modeling garments and putting them onto characters, preparing a character dressed in multiple layers of garments can be very time-consuming and tedious. In this paper, we propose a novel scanning-based solution for modeling and animating characters wearing multiple layers of clothes. This is achieved by making use of real clothes and human bodies. We first scan the naked body of a subject by an RGBD camera, and a statistical body model is fit to the scanned data. This results in a skinned articulated model of the subject. The subject is then asked to put on one piece of garment after another, and the articulated body model dressed up to the previous step is fit to the newly scanned data. The new garment is segmented in a semi-automatic fashion and added as an additional layer to the multi-layer garment model. During runtime, the skinned character is controlled based on the motion capture data and the multi-layer garment model is controlled by blending the movements computed by physical simulation and linear blend skinning, such that the cloth preserves its shape while it shows realistic physical motion. We present results where the character is wearing multiple layers of garments including a shirt, coat and a skirt. Our framework can be useful for preparing and animating dressed characters for computer games and films.
C1 [Hu, Pengpeng; Zhong, Yueqi] Donghua Univ, Coll Text, Shanghai, Peoples R China.
   [Komura, Taku] Univ Edinburgh, Inst Percept Act & Behav, Sch Informat, Edinburgh, Midlothian, Scotland.
   [Holden, Daniel] Univ Edinburgh, Sch Informat, Edinburgh, Midlothian, Scotland.
   [Zhong, Yueqi] Minist Educ, Key Lab Text Sci & Technol, Shanghai, Peoples R China.
C3 Donghua University; University of Edinburgh; University of Edinburgh
RP Zhong, YQ (corresponding author), Donghua Univ, Coll Text, Shanghai, Peoples R China.; Zhong, YQ (corresponding author), Minist Educ, Key Lab Text Sci & Technol, Shanghai, Peoples R China.
EM bigpeng.hu@gmail.com; tkomura@ed.ac.uk; s0822954@staffmail.ed.ac.uk;
   zhyq@dhu.edu.cn
RI Hu, Pengpeng/ADG-7735-2022
OI Hu, Pengpeng/0000-0002-2547-1517
FU Natural Science Foundation of China [61572124]; Shanghai Natural Science
   Foundation [14ZR1401100]; China Scholarship Council [201506630055]
FX This work is supported by Natural Science Foundation of China (Grant No.
   61572124), Shanghai Natural Science Foundation (14ZR1401100) and China
   Scholarship Council (File No. 201506630055).
CR Alexiadis DS, 2013, IEEE T MULTIMEDIA, V15, P339, DOI 10.1109/TMM.2012.2229264
   [Anonymous], ACM T GRAPH
   Baran I, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1239451.1239523, 10.1145/1276377.1276467]
   Bouaziz S, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601116
   Brouet R, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185532
   Clegg A, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766986
   Cordier F, 2002, COMPUT GRAPH FORUM, V21, P327, DOI 10.1111/1467-8659.t01-1-00592
   Cordier F, 2003, IEEE COMPUT GRAPH, V23, P38, DOI 10.1109/MCG.2003.1159612
   Cui Y., 2012, ACCV Workshops, P133
   Feng Andrew, 2015, P 8 ACM SIGGRAPH C M, P57
   Han JG, 2013, IEEE T CYBERNETICS, V43, P1318, DOI 10.1109/TCYB.2013.2265378
   Ho ESL, 2009, COMPUT GRAPH FORUM, V28, P299, DOI 10.1111/j.1467-8659.2009.01369.x
   Igarashi T., 2006, P 15 ANN S US INT SO, P91
   Istook C.L., 2001, J FASH MARK MANAG, V5, P120, DOI DOI 10.1108/EUM0000000007283
   Izadi S, 2011, P UIST, P559, DOI DOI 10.1145/2047196.2047270
   Jakob W, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818078
   Ju T, 2005, ACM T GRAPHIC, V24, P561, DOI 10.1145/1073204.1073229
   Oh S, 2005, VISUAL COMPUT, V21, P522, DOI 10.1007/s00371-005-0339-6
   Rocchini C, 2001, COMPUT GRAPH FORUM, V20, pC299
   Stoll C, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866161
   Tagliasacchi A, 2015, COMPUT GRAPH FORUM, V34, P101, DOI 10.1111/cgf.12700
   Tang M, 2016, COMPUT GRAPH FORUM, V35, P511, DOI 10.1111/cgf.12851
   Tong J, 2012, IEEE T VIS COMPUT GR, V18, P643, DOI 10.1109/TVCG.2012.56
   Umetani N, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964985
   Vlasic D., 2009, ACM T GRAPHIC, V28
   Wang H, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2503177
   Wang HM, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818063
   Wu CL, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508418
   Yamazaki S., 2013, P 2 DIG HUM MOD S
   Zeng M, 2013, PROC CVPR IEEE, P145, DOI 10.1109/CVPR.2013.26
   Zhang MM, 2015, MULTIMED TOOLS APPL, V74, P3137, DOI 10.1007/s11042-013-1774-4
   Zhang ZY, 2012, IEEE MULTIMEDIA, V19, P4, DOI 10.1109/MMUL.2012.24
NR 32
TC 9
Z9 10
U1 0
U2 30
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2017
VL 33
IS 6-8
BP 961
EP 969
DI 10.1007/s00371-017-1388-3
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA EX1EY
UT WOS:000402964800026
DA 2024-07-18
ER

PT J
AU Chi, MT
   Liu, WC
   Hsu, SH
AF Chi, Ming-Te
   Liu, Wei-Ching
   Hsu, Shu-Hsuan
TI Image stylization using anisotropic reaction diffusion
SO VISUAL COMPUTER
LA English
DT Article
DE Non-photorealistic rendering; Image stylization; Reaction diffusion;
   Pattern generation
AB Image stylization refers to the process of converting input images to a specific representation that enhances image content using several designed patterns. The critical steps to a successful image stylization are the design of patterns and arrangements. However, only skilled artists master such tasks because these tasks are challenging for most users. In this paper, a novel image stylization system based on anisotropic reaction diffusion is proposed to facilitate pattern generation and stylized image design. The system begins with self-organized patterns generated by reaction diffusion. To extend the style of reaction diffusion, the proposed method involves using a set of modifications of anisotropic diffusion to deform shape and introducing a flow field to guide pattern arrangement. A pattern picker is proposed to facilitate the pattern selection from these modifications. In the post-process step, a new thresholding and color mapping method is introduced to refine the sizes, densities, and colors of patterns. From the experimental results and a user study, several image stylizations, including paper-cut, stylized halftone, and motion illusion, are generated using our method, demonstrating the feasibility and flexibility of the proposed system.
C1 [Chi, Ming-Te; Liu, Wei-Ching; Hsu, Shu-Hsuan] Natl Chengchi Univ, Dept Comp Sci, 64 Chihnan Rd,Sec 2 Wenshan, Taipei 11623, Taiwan.
C3 National Chengchi University
RP Chi, MT (corresponding author), Natl Chengchi Univ, Dept Comp Sci, 64 Chihnan Rd,Sec 2 Wenshan, Taipei 11623, Taiwan.
EM mtchi@cs.nccu.edu.tw
OI Chi, Ming-Te/0000-0002-9014-7561
FU ministry of science and technology, Taiwan [MOST 103-2221-E-004-008]
FX We thank the anonymous reviewers and the editor for their valuable
   comments. We acknowledge Chao-Hung Lin and Shin-Syun Lin for their
   suggestions. We thank Chen-Chi Hu for helping on user study. This work
   is supported by the ministry of science and technology, Taiwan under
   MOST 103-2221-E-004-008.
CR Barla P., 2006, Proceedings of the 4th international symposium on Non-photorealistic animation and rendering, P127, DOI DOI 10.1145/1124728.1124749
   Bousseau A., 2007, ACM SIGGRAPH 2007 SI
   Chi MT, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360661
   Huang H, 2011, VISUAL COMPUT, V27, P861, DOI 10.1007/s00371-011-0596-5
   Huang H, 2010, VISUAL COMPUT, V26, P933, DOI 10.1007/s00371-010-0498-y
   Kang H, 2007, NPAR 2007: 5TH INTERNATIONAL SYMPOSIUM ON NON-PHOTOREALISTIC ANIMATION AND RENDERING, PROCEEDINGS, P43
   Kang H, 2009, IEEE T VIS COMPUT GR, V15, P62, DOI 10.1109/TVCG.2008.81
   Kim T, 2007, COMPUT ANIMAT VIRT W, V18, P329, DOI 10.1002/cav.187
   Kyprianidis J. E., 2008, P EG UK THEOR PRACT, P51
   Kyprianidis JE, 2011, COMPUT GRAPH FORUM, V30, P593, DOI 10.1111/j.1467-8659.2011.01882.x
   Lee H., 2010, Proceedings of the 8th International Symposium on Non-Photorealistic Animation and Rendering, P43
   Li YY, 2011, IEEE T VIS COMPUT GR, V17, P231, DOI 10.1109/TVCG.2010.36
   McGraw T, 2008, COMPUT GRAPH-UK, V32, P82, DOI 10.1016/j.cag.2007.09.002
   PEARSON JE, 1993, SCIENCE, V261, P189, DOI 10.1126/science.261.5118.189
   PIZER SM, 1987, COMPUT VISION GRAPH, V39, P355, DOI 10.1016/S0734-189X(87)80186-X
   Sanderson AR, 2004, IEEE VISUALIZATION 2004, PROCEEEDINGS, P115, DOI 10.1109/VISUAL.2004.25
   Son M, 2011, GRAPH MODELS, V73, P74, DOI 10.1016/j.gmod.2010.12.001
   Steidl G, 2009, LECT NOTES COMPUT SC, V5567, P477, DOI 10.1007/978-3-642-02256-2_40
   TURK G, 1991, COMP GRAPH, V25, P289, DOI 10.1145/127719.122749
   Wan L, 2010, IEEE T VIS COMPUT GR, V16, P287, DOI 10.1109/TVCG.2009.85
   WITKIN A, 1991, COMP GRAPH, V25, P299, DOI 10.1145/127719.122750
   Xu J, 2007, PACIFIC GRAPHICS 2007: 15TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, P343, DOI 10.1109/PG.2007.10
   Xu Jie., 2008, Proc. NPAR, P39
   Zang Y, 2014, VISUAL COMPUT, V30, P969, DOI 10.1007/s00371-013-0881-6
NR 24
TC 8
Z9 9
U1 0
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2016
VL 32
IS 12
BP 1549
EP 1561
DI 10.1007/s00371-015-1139-2
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA EB3NH
UT WOS:000387271200005
DA 2024-07-18
ER

PT J
AU Rasool, S
   Sourin, A
AF Rasool, Shahzad
   Sourin, Alexei
TI Real-time haptic interaction with RGBD video streams
SO VISUAL COMPUTER
LA English
DT Article
DE Image-driven haptic interaction; Tangible images; Tangible video; Kinect
AB Video interaction is a common way of communication in cyberspace. It can become more immersive by incorporating haptic modality. Using commonly available depth sensing controllers like Microsoft Kinect, information about the depth of a scene can be captured in real-time together with the video. In this paper, we present a method for real-time haptic interaction with videos containing depth data. Forces are computed based on the depth information. Spatial and temporal filtering of the depth stream is used to provide stability of force feedback delivered to the haptic device. Fast collision detection ensures the proposed approach to be used in real-time. We present an analysis of various factors that affect algorithm performance. The usefulness of the approach is illustrated by highlighting possible application scenarios.
C1 [Rasool, Shahzad] Nanyang Technol Univ, Fraunhofer IDM NTU, Sch Comp Engn, Singapore, Singapore.
   [Sourin, Alexei] Nanyang Technol Univ, Singapore, Singapore.
C3 Nanyang Technological University; Nanyang Technological University
RP Rasool, S (corresponding author), Nanyang Technol Univ, Fraunhofer IDM NTU, Sch Comp Engn, Singapore, Singapore.
EM shahzadrasool@ntu.edu.sg; assourin@ntu.edu.sg
RI Sourin, Alexei/A-3701-2011; Rasool, Shahzad/AAE-8858-2019
OI Rasool, Shahzad/0000-0001-5504-3727; Sourin, Alexei/0000-0003-4051-2927
FU National Research Foundation, Prime Minister's Office, Singapore under
   its International Research Centers in Singapore Funding Initiative, and
   research grant MOE [T1 RG 17/15]
FX This research was supported by the National Research Foundation, Prime
   Minister's Office, Singapore under its International Research Centers in
   Singapore Funding Initiative, and research grant MOE T1 RG 17/15 "Haptic
   Interaction with Images and Videos".
CR Abramov A., 2012, 2012 IEEE Workshop on Applications of Computer Vision (WACV), P457, DOI 10.1109/WACV.2012.6163000
   Adi W, 2009, LECT NOTES COMPUT SC, V5857, P279, DOI 10.1007/978-3-642-05036-7_27
   Cha J, 2009, ACM T MULTIM COMPUT, V5, DOI 10.1145/1596990.1596993
   Danieau F., 2014, THESIS
   Danieau F, 2014, IEEE MULTIMEDIA, V21, P11, DOI 10.1109/MMUL.2013.64
   Dindar N, 2010, PROC SPIE, V7744, DOI 10.1117/12.863387
   Frati V., 2011, 2011 IEEE World Haptics Conference (WHC 2011), P317, DOI 10.1109/WHC.2011.5945505
   Gaw D, 2006, SYMPOSIUM ON HAPTICS INTERFACES FOR VIRTUAL ENVIRONMENT AND TELEOPERATOR SYSTEMS 2006, PROCEEDINGS, P287
   Ikei Y, 1998, P IEEE VIRT REAL ANN, P51, DOI 10.1109/VRAIS.1998.658422
   Israr A., CHI 12 HUM FACT COMP
   Jayasiri A, 2013, PROCEEDINGS OF 2013 23RD INTERNATIONAL CONFERENCE ON ARTIFICIAL REALITY AND TELEXISTENCE (ICAT 2013), P54
   Kagawa T, 2010, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON COMPLEX, INTELLIGENT AND SOFTWARE INTENSIVE SYSTEMS (CISIS 2010), P1135, DOI 10.1109/CISIS.2010.174
   Kim SB, 2013, J MICROELECTROMECH S, V22, P26, DOI 10.1109/JMEMS.2012.2213069
   Lareau D, 2014, IEEE T INSTRUM MEAS, V63, P35, DOI 10.1109/TIM.2013.2277538
   Li J., P 9 ACM SIGGRAPH C V
   Nguyen CV, 2012, SECOND JOINT 3DIM/3DPVT CONFERENCE: 3D IMAGING, MODELING, PROCESSING, VISUALIZATION & TRANSMISSION (3DIMPVT 2012), P524, DOI 10.1109/3DIMPVT.2012.84
   O'Modhrain S, 2004, 12TH INTERNATIONAL SYMPOSIUM ON HAPTIC INTERFACES FOR VIRTUAL ENVIRONMENT AND TELEOPERATOR SYSTEMS, PROCEEDINGS, P293, DOI 10.1109/HAPTIC.2004.1287211
   Park CH, 2013, 2013 WORLD HAPTICS CONFERENCE (WHC), P229, DOI 10.1109/WHC.2013.6548413
   Rasool S., SIGGRAPH AS 2011 SKE
   Rasool S, 2013, 2013 INTERNATIONAL CONFERENCE ON CYBERWORLDS (CW), P286, DOI 10.1109/CW.2013.28
   Rasool S, 2010, 2010 INTERNATIONAL CONFERENCE ON CYBERWORLDS (CW 2010), P92, DOI 10.1109/CW.2010.19
   Reiner M, 2004, IEEE T CIRC SYST VID, V14, P392, DOI 10.1109/TCSVT.2004.823399
   Ryden F., 2011, 2011 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2011), P2614, DOI 10.1109/IROS.2011.6048646
   Schneider O., 2015, Haptic Interaction: Perception, Devices and Applications, P253
   Sung MY, 2009, 2009 11TH IEEE INTERNATIONAL SYMPOSIUM ON MULTIMEDIA (ISM 2009), P425, DOI 10.1109/ISM.2009.79
   Vasudevan H., P 2008 S HAPT INT VI
   Wu J, 2007, 2007 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND BIOMIMETICS, VOLS 1-5, P1315, DOI 10.1109/ROBIO.2007.4522354
   Xu SP, 2012, 2012 5TH INTERNATIONAL CONGRESS ON IMAGE AND SIGNAL PROCESSING (CISP), P1195
NR 28
TC 9
Z9 9
U1 0
U2 5
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2016
VL 32
IS 10
BP 1311
EP 1321
DI 10.1007/s00371-016-1224-1
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA EA2BQ
UT WOS:000386396800009
DA 2024-07-18
ER

PT J
AU Xing, GY
   Liu, YL
   Zhang, WF
   Ling, HB
AF Xing, Guanyu
   Liu, Yanli
   Zhang, Wanfa
   Ling, Haibin
TI Light mixture intrinsic image decomposition based on a single RGB-D
   image
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 33rd Conference on Computer Graphics International (CGI)
CY JUN 28-JUL 01, 2016
CL Heraklion, GREECE
SP Fdn Res Technol
DE Intrinsic image; Single RGB-D image; Light mixture; Physical-based
   illumination prior
AB We propose a novel intrinsic image decomposition method based on a single RGB-D image. We first separate the shading image into an illumination color component, a distant shading component and a local shading component, inducing a novel intrinsic image model that can encode color and spatial variation of scene illumination. Unlike previous methods, which assume illumination color is white, our light mixture model encodes scene illumination with two different light types, and an automatical strategy is proposed to calculate the color of the two light types. We also adopt physical-based illumination prior to infer the distant shading component. To do so, we firstly recover the illumination distribution of the distant light sources through solving a system of linear equations with sparse and non-negative constraints. Then, the recovered illumination is used to synthesize a coarse distant shading image jointly with the depth map. Later, the synthetic image is employed as an additional constraint of distant shading component. To reduce noise disturbance from the synthetic distant shading image, a novel sampling strategy was proposed. Finally, we consider the similarity of material locally and globally, which gives reliable constraints to the reflectance component. Experimental results demonstrate the validity and flexibility of our approach.
C1 [Xing, Guanyu; Zhang, Wanfa] Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu, Peoples R China.
   [Xing, Guanyu; Zhang, Wanfa] Univ Elect Sci & Technol China, Ctr Robot, Chengdu, Peoples R China.
   [Xing, Guanyu; Ling, Haibin] Temple Univ, Dept Comp & Informat Sci, Ctr Data Analyt & Biomed Informat, Philadelphia, PA 19122 USA.
   [Liu, Yanli] Sichuan Univ, Coll Comp Sci, Chengdu, Peoples R China.
C3 University of Electronic Science & Technology of China; University of
   Electronic Science & Technology of China; Pennsylvania Commonwealth
   System of Higher Education (PCSHE); Temple University; Sichuan
   University
RP Xing, GY (corresponding author), Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu, Peoples R China.; Xing, GY (corresponding author), Univ Elect Sci & Technol China, Ctr Robot, Chengdu, Peoples R China.; Xing, GY (corresponding author), Temple Univ, Dept Comp & Informat Sci, Ctr Data Analyt & Biomed Informat, Philadelphia, PA 19122 USA.
EM xingguanyu@uestc.edu.cn
FU Div Of Information & Intelligent Systems; Direct For Computer & Info
   Scie & Enginr [1350521, 1218156] Funding Source: National Science
   Foundation
CR [Anonymous], 2006, CVPR
   Barron JT, 2013, PROC CVPR IEEE, P17, DOI 10.1109/CVPR.2013.10
   Barron JT, 2012, PROC CVPR IEEE, P334, DOI 10.1109/CVPR.2012.6247693
   Bell M, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P670, DOI 10.1109/ICCV.2001.937585
   Bell S, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601206
   Butler DJ, 2012, LECT NOTES COMPUT SC, V7577, P611, DOI 10.1007/978-3-642-33783-3_44
   Chen QF, 2013, IEEE I CONF COMP VIS, P241, DOI 10.1109/ICCV.2013.37
   Gehler P., 2011, P ADV NEUR INF PROC, P765
   Grosse R, 2009, IEEE I CONF COMP VIS, P2335, DOI 10.1109/ICCV.2009.5459428
   Hsu E., 2008, P SIGGRAPH 2008 LOS
   Izadi S, 2011, P UIST, P559, DOI DOI 10.1145/2047196.2047270
   LAND EH, 1971, J OPT SOC AM, V61, P1, DOI 10.1364/JOSA.61.000001
   Lee DD, 1999, NATURE, V401, P788, DOI 10.1038/44565
   Lee KJ, 2012, LECT NOTES COMPUT SC, V7577, P327, DOI 10.1007/978-3-642-33783-3_24
   Mei X, 2009, IEEE I CONF COMP VIS, P583, DOI 10.1109/ICCV.2009.5459185
   Shen L., 2011, P IEEE C COMP VIS PA, P2904
   Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54
   Sinha P., 1993, [1993] Proceedings Fourth International Conference on Computer Vision, P156, DOI 10.1109/ICCV.1993.378224
   Tappen M., 2006, IEEE COMPUTER VISION, P1992
   Tappen MF, 2005, IEEE T PATTERN ANAL, V27, P1459, DOI 10.1109/TPAMI.2005.185
   Zhai Y., 2006, P 14 ACM INT C MULT, P815, DOI [DOI 10.1145/1180639.1180824, 10.1145/1180639.1180824]
NR 21
TC 3
Z9 4
U1 0
U2 8
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2016
VL 32
IS 6-8
BP 1013
EP 1023
DI 10.1007/s00371-016-1238-8
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA DP8ET
UT WOS:000378731600032
DA 2024-07-18
ER

PT J
AU Ren, B
   Yan, X
   Yang, T
   Li, CF
   Lin, MC
   Hu, SM
AF Ren, Bo
   Yan, Xiao
   Yang, Tao
   Li, Chen-feng
   Lin, Ming C.
   Hu, Shi-min
TI Fast SPH simulation for gaseous fluids
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Computational Visual Media Conference (CVM)
CY APR 16-17, 2015
CL Tsinghua Univ, Beijing, PEOPLES R CHINA
HO Tsinghua Univ
DE Smoothed particle hydrodynamics; Gas simulation; Adaptive particle
   splitting and merging; Phase transition
ID FLOWS
AB This paper presents a fast smoothed particle hydro-dynamics (SPH) simulation approach for gaseous fluids. Unlike previous SPH gas simulators, which solve the transparent air flow in a fixed simulation domain, the proposed approach directly solves the visible gas without involving the transparent air. By compensating the density and force calculation for the visible gas particles, we completely avoid the need of computational cost on ambient air particles in previous approaches. This allows the computational resources to be exclusively focused on the visible gas, leading to significant performance improvement of SPH gas simulation. The proposed approach is at least ten times faster than the standard SPH gas simulation strategy and is able to reduce the total particle number by 25-400 times in large open scenes. The proposed approach also enables fast SPH simulation of complex scenes involving liquid-gas transition, such as boiling and evaporation. A particle splitting and merging scheme is proposed to handle the degraded resolution in liquid-gas phase transition. Various examples are provided to demonstrate the effectiveness and efficiency of the proposed approach.
C1 [Ren, Bo] Nankai Univ, Coll Comp & Control Engn, Tianjin 300071, Peoples R China.
   [Yan, Xiao; Yang, Tao; Hu, Shi-min] Tsinghua Univ, Dept Comp Sci & Technol, Beijing 100084, Peoples R China.
   [Li, Chen-feng] Swansea Univ, Coll Engn, Swansea, W Glam, Wales.
   [Lin, Ming C.] Univ N Carolina, Dept Comp Sci, Chapel Hill, NC USA.
C3 Nankai University; Tsinghua University; Swansea University; University
   of North Carolina; University of North Carolina Chapel Hill
RP Ren, B (corresponding author), Nankai Univ, Coll Comp & Control Engn, Tianjin 300071, Peoples R China.
EM renboeverywhere@gmail.com; shiotoli@gmail.com; yangtao9009@gmail.com;
   c.f.li@swansea.ac.uk; lin@cs.unc.edu; shimin@tsinghua.edu.cn
RI Hu, Shi-Min/AAW-1952-2020; ren, bo/IST-0814-2023; Li,
   Chenfeng/D-4034-2014; Li, Chenfeng/AFQ-6554-2022
OI Li, Chenfeng/0000-0003-0441-211X
FU National Basic Research Project of China [2011CB302205]; Natural Science
   Foundation of China [61120106007]; National High Technology Research and
   Development Program of China [2012AA011503]; National Science Foundation
   [NSF IIS-1320644]; UNC Carolina Development Foundation; Ser Cymru
   National Research Network in Advanced Engineering and Materials
FX The authors would like to thank the anonymous reviewers for their
   constructive comments, and Prof. Jianjun Zhang and Prof. Jian Chang for
   their suggestions. This work is supported by the National Basic Research
   Project of China (Project Number 2011CB302205), the Natural Science
   Foundation of China (Project Number 61120106007) and the National High
   Technology Research and Development Program of China (Project Number
   2012AA011503). The authors would also like to acknowledge the financial
   support provided by the National Science Foundation (NSF IIS-1320644)
   and UNC Carolina Development Foundation, and Ser Cymru National Research
   Network in Advanced Engineering and Materials.
CR Adams B, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1276377.1276437, 10.1145/1239451.1239499]
   Angelidis Alexis., 2006, S COMPUTER ANIMATION, P25
   [Anonymous], 2012, Computer Animation 2012-ACM SIGGRAPH / Eurographics Symposium Proceedings, SCA
   Bender J., 2010, WORKSH VIRT REAL INT
   Boyd L, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2159516.2159522
   Cleary PW, 1998, APPL MATH MODEL, V22, P981, DOI 10.1016/S0307-904X(98)10031-8
   Colagrossi A, 2003, J COMPUT PHYS, V191, P448, DOI 10.1016/S0021-9991(03)00324-3
   Desbrun M., 1999, RR3829 INRIA
   Feldman J, 2007, INT J NUMER METH ENG, V72, P295, DOI 10.1002/nme.2010
   Gao Y, 2013, IEEE T VIS COMPUT GR, V19, P178, DOI 10.1109/TVCG.2012.117
   Gao Y, 2009, COMPUT GRAPH FORUM, V28, P1845, DOI 10.1111/j.1467-8659.2009.01562.x
   Gerszewski D, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508430
   Hong W, 2008, VISUAL COMPUT, V24, P535, DOI 10.1007/s00371-008-0234-z
   Ihmsen M, 2014, IEEE T VIS COMPUT GR, V20, P426, DOI 10.1109/TVCG.2013.105
   Keiser Richard., 2006, Multiresolution particle-based fluids
   Kim ST, 2013, VISUAL COMPUT, V29, P1293, DOI 10.1007/s00371-012-0770-4
   Li XS, 2014, VISUAL COMPUT, V30, P787, DOI 10.1007/s00371-014-0969-7
   Macklin M, 2014, ACM T GRAPHIC, V33, DOI [10.1145/280/109/2601152, 10.1145/2601097.2601152]
   MONAGHAN JJ, 1994, J COMPUT PHYS, V110, P399, DOI 10.1006/jcph.1994.1034
   Müller M, 2004, COMPUT ANIMAT VIRT W, V15, P159, DOI 10.1002/cav.18
   Muller M., 2003, ACM SIGGRAPH/Eurographics Symposium on Computer Animation, P154
   Muller M, 2005, P 2005 ACM SIGGRAPH, P237, DOI DOI 10.1145/1073368.1073402
   Orthmann J, 2012, COMPUT GRAPH FORUM, V31, P2436, DOI 10.1111/j.1467-8659.2012.03186.x
   Pfaff T, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185608
   Schechter H, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185557
   Solenthaler B., 2008, P 2008 ACM SIGGRAPH, P211, DOI 10.2312/SCA/SCA08/211-218
   Solenthaler B, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964976
   Yan H, 2009, COMPUT ANIMAT VIRT W, V20, P417, DOI 10.1002/cav.300
   Zhu B, 2010, COMPUT GRAPH FORUM, V29, P2207, DOI 10.1111/j.1467-8659.2010.01809.x
NR 29
TC 11
Z9 16
U1 5
U2 22
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2016
VL 32
IS 4
BP 523
EP 534
DI 10.1007/s00371-015-1086-y
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA DI6YD
UT WOS:000373645200010
OA Green Published
DA 2024-07-18
ER

PT J
AU Johnsson, B
   Akenine-Möller, T
AF Johnsson, Bjorn
   Akenine-Moller, Tomas
TI A performance and energy evaluation of many-light rendering algorithms
SO VISUAL COMPUTER
LA English
DT Article
DE Energy measurement; Energy analysis; Real-time graphics; Many-light
   rendering
AB Recently, the performance of many-light algorithms, where thousands of light sources are used to compute the lighting in a scene, has improved so much that they have reached the realm of real-time rendering. In general, the algorithm that is considered "best" is the one that is the fastest in terms of time per frame. Given that power efficiency may become or already is one of the most important optimization factors for both hardware and software vendors for graphics, we take a different route and instead measure both energy usage per frame and frame time for a number of popular many-light rendering algorithms on an Intel Iris Pro. We use Pareto frontiers for each configuration to examine the possibilities for trade-offs between rendering time and energy consumption. Furthermore, we examine the optimal algorithms at each configuration, and are able to draw generalized conclusions on when each algorithm is most efficient. We also record several other statistics on the algorithms, e.g., bandwidth, and are able to draw further conclusions with regard to energy consumption.
C1 [Johnsson, Bjorn] Lund Univ, Lund, Sweden.
   [Johnsson, Bjorn; Akenine-Moller, Tomas] Lund Univ, Lund, Sweden.
   [Johnsson, Bjorn; Akenine-Moller, Tomas] Intel Corp, Lund, Sweden.
   [Akenine-Moller, Tomas] Lund Univ, Comp Graph, Lund, Sweden.
C3 Lund University; Lund University; Intel Corporation; Lund University
RP Johnsson, B (corresponding author), Lund Univ, Lund, Sweden.
EM bjorn.johnsson@cs.lth.se
CR Akeley K., 1993, P ACM SIGGRAPH, V1994, P109
   Andersson Johan., 2009, Beyond Programmable Shading (ACM SIGGRAPH 2009 Course), p7:1
   [Anonymous], 2013, KEYNOTE TALK HIGH PE
   Balestra C., 2008, GAM DEV C
   Burns Christopher A., 2013, Journal of Computer Graphics Techniques (JCGT), V2, P55
   Dachsbacher C, 2014, COMPUT GRAPH FORUM, V33, P88, DOI 10.1111/cgf.12256
   Esmaeilzadeh H, 2011, ISCA 2011: PROCEEDINGS OF THE 38TH ANNUAL INTERNATIONAL SYMPOSIUM ON COMPUTER ARCHITECTURE, P365, DOI 10.1145/2024723.2000108
   Fatahalian K, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778804
   Harada Takahiro., 2012, Eurographics (Short Papers), P5
   Johnsson B., 2014, J COMPUT GR TECH JCG, V3, P60
   Johnsson Bjorn., 2012, Proceedings of the Fourth ACM SIGGRAPH/Eurographics conference on High-Performance Graphics, P67
   Keckler SW, 2011, IEEE MICRO, V31, P7, DOI 10.1109/MM.2011.89
   Keller A., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P49, DOI 10.1145/258734.258769
   Koduri R., 2011, PROGRAMMABLE SHADING
   Lauritzen A., 2010, PROGRAMMABLE SHADING
   Lauritzen A., 2012, PROGRAMMABLE SHADING
   Mochocki B, 2006, DES AUT TEST EUROPE, P500
   Olsson O., 2011, Journal of Graphics, GPU, and Game Tools, V15, P235, DOI DOI 10.1080/2151237X.2011.621761
   Saito T., 1990, Computer Graphics, V24, P197, DOI 10.1145/97880.97901
   Vatjus-Anttila J.M., 2013, INT C MAK SENS CONV
   Walter B., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P45, DOI 10.1145/258734.258766
NR 21
TC 1
Z9 1
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2015
VL 31
IS 12
BP 1671
EP 1681
DI 10.1007/s00371-014-1046-y
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CV2ZD
UT WOS:000364126900007
DA 2024-07-18
ER

PT J
AU Quan, W
   Jiang, YQ
   Zhang, JJ
   Chen, JX
AF Quan, Wei
   Jiang, Yongquan
   Zhang, Jianjun
   Chen, Jim X.
TI Robust object tracking with active context learning
SO VISUAL COMPUTER
LA English
DT Article
DE Object tracking; Active context learning; Online model
AB This paper proposes a method to deal with long-term robust object tracking in unconstrained environment. The approach exploits both target and background information on the fly automatically. It builds the structural constraint using active context learning to enhance the adaptability for variation of the target and stability of tracking. An optical-flow-based motion region extraction method is integrated into the context learning framework to address the problem of fast target motion or abrupt camera motion. Experimental results on challenging real-world video sequences demonstrate the effectiveness and robustness of our approach. Comparisons with several state-of-the-art methods are provided.
C1 [Quan, Wei] Southwest Jiaotong Univ, Sch Elect Engn, Chengdu 610031, Sichuan, Peoples R China.
   [Jiang, Yongquan; Zhang, Jianjun; Chen, Jim X.] Southwest Jiaotong Univ, State Key Lab Tract Power, Chengdu 610031, Sichuan, Peoples R China.
   [Zhang, Jianjun] Bournemouth Univ, Natl Ctr Comp Animat, Poole BH12 5BB, Dorset, England.
   [Chen, Jim X.] George Mason Univ, Dept Comp Sci, Fairfax, VA 22030 USA.
C3 Southwest Jiaotong University; Southwest Jiaotong University;
   Bournemouth University; George Mason University
RP Jiang, YQ (corresponding author), Southwest Jiaotong Univ, State Key Lab Tract Power, Chengdu 610031, Sichuan, Peoples R China.
EM wquan@home.swjtu.edu.cn; yqjiang@swjtu.edu.cn; jzhang@bournemouth.ac.uk;
   jchen@gmu.edu
FU Central Universities of China [2682014cx024, SWJTU12BR024]
FX This work was supported by the fundamental research funds for the
   Central Universities of China (Grant No. 2682014cx024 and SWJTU12BR024).
CR Agarwal S., 2004, IEEE T PATTERN ANAL
   [Anonymous], 2006, IEEE C COMPUTER VISI, DOI DOI 10.1109/CVPR.2006.256
   [Anonymous], 2012, IEEE C COMP VIS PATT
   [Anonymous], 2009, IEEE C COMP VIS PATT
   [Anonymous], 2006, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
   Avidan S, 2007, IEEE T PATTERN ANAL, V29, P261, DOI 10.1109/TPAMI.2007.35
   Chapelle O., 2006, SEMISUPERVISED LEARN
   Collins RT, 2005, IEEE T PATTERN ANAL, V27, P1631, DOI 10.1109/TPAMI.2005.205
   Comaniciu D, 2003, IEEE T PATTERN ANAL, V25, P564, DOI 10.1109/TPAMI.2003.1195991
   Dinh TB, 2011, PROC CVPR IEEE, P1177, DOI 10.1109/CVPR.2011.5995733
   Fan JL, 2010, LECT NOTES COMPUT SC, V6311, P480
   Godec M., 2010, 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops, P19
   Grabner H, 2008, EUR C COMP VIS ECCV
   Grabner H, 2010, PROC CVPR IEEE, P1285, DOI 10.1109/CVPR.2010.5539819
   Isard M, 1998, INT J COMPUT VISION, V29, P5, DOI 10.1023/A:1008078328650
   Kalal Z., 2010, IEEE C COMP VIS PATT
   Kalal Z., 2009, IEEE INT C COMP VIS
   Leistner C., 2010, EUR C COMP VIS ECCV
   Leistner C, 2010, LECT NOTES COMPUT SC, V6376, P493
   Leistner C, 2009, IEEE I CONF COMP VIS, P506, DOI 10.1109/ICCV.2009.5459198
   Lim J., 2005, NEURAL INF PROCESS S
   Ozuysal M., 2007, IEEE C COMP VIS PATT
   Q Yu, 2008, EUR C COMP VIS ECCV
   Shakhnarovich G., 2005, NEAREST NEIGHBOR MET
   Stalder S., 2009, IEEE INT C COMP VIS
   Stenger B., 2009, IEEE C COMP VIS PATT
   Viola P, 2001, P 2001 IEEE COMP SOC, pII
   Wang AP, 2009, IEEE IMAGE PROC, P1449, DOI 10.1109/ICIP.2009.5414559
   Yang M, 2009, IEEE T PATTERN ANAL, V31, P1195, DOI 10.1109/TPAMI.2008.146
   Yilmaz A, 2006, ACM COMPUT SURV, V38, DOI 10.1145/1177352.1177355
NR 30
TC 5
Z9 6
U1 0
U2 12
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2015
VL 31
IS 10
BP 1307
EP 1318
DI 10.1007/s00371-014-1012-8
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CQ7CG
UT WOS:000360759800002
DA 2024-07-18
ER

PT J
AU Liu, J
   Li, CP
   Mei, F
   Wang, ZQ
AF Liu, Jing
   Li, Chunpeng
   Mei, Feng
   Wang, Zhaoqi
TI 3D entity-based stereo matching with ground control points and joint
   second-order smoothness prior
SO VISUAL COMPUTER
LA English
DT Article
DE Stereo matching; Joint second-order smoothness prior; GCPs-plane;
   Fusion-move; 3D entity
ID BELIEF PROPAGATION; COST AGGREGATION; GRAPH
AB Disparity estimation for a scene with complex geometric characteristics such as slanted or highly curved surfaces is a basic and important issue in stereo matching. Traditional methods often use first-order smoothness priors that always lead to low-curvature frontal-parallel disparity maps. We propose a stereo framework that views the scene as a set of 3D entities with compact and smooth disparity distributions. The 3D entity-based representation enables some contributions to obtain a precise disparity estimation. A GCPs-plane constraint based on ground control points is used to strengthen the compact distributions of the disparities in each entity by restricting the scope of the disparity variance and reducing matching ambiguities in repetitive or low-texture areas. Furthermore, we have formulated a joint second-order smoothness prior, which combines a geometric weight with the derivative of disparity values. This prior encourages smooth disparity variations inside each entity and means that each entity is biased towards being a 3D planar surface. Segmentation is incorporated as soft constraint by effectively fusing the advantages of the image color gradient and GCPs-plane. This avoids blending of the foreground and background and retains only the disparity discontinuities from geometrically smooth regions with strong texture gradients. Our framework is formulated as a maximum a posteriori probability estimation problem that is optimized using the fusion-move approach. Evaluation results on the Middlebury benchmark show that the proposed method ranks second among the approximately listed algorithms. In addition, it performs well in real-world scenes.
C1 [Liu, Jing; Li, Chunpeng; Mei, Feng; Wang, Zhaoqi] Chinese Acad Sci, Beijing Key Lab Mobile Comp & Pervas Device, Inst Comp Technol, Beijing 100190, Peoples R China.
   [Liu, Jing; Mei, Feng] Univ Chinese Acad Sci, Beijing 100049, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Computing Technology, CAS;
   Chinese Academy of Sciences; University of Chinese Academy of Sciences,
   CAS
RP Liu, J (corresponding author), Chinese Acad Sci, Beijing Key Lab Mobile Comp & Pervas Device, Inst Comp Technol, 6 Kexueyuan South Rd, Beijing 100190, Peoples R China.
EM liujing01@ict.ac.cn
RI Liu, Jing/T-6504-2019; Li, Chunpeng/AAE-6134-2019
OI Liu, Jing/0000-0002-2217-0372; 
FU National Natural Science Foundation of China [61300131]; National Key
   Technology Research and Development Program of China [2013BAK03B07];
   National High Technology Research and Development Program of China (863
   Program) [2013AA013902]
FX This work is supported and funded by the National Natural Science
   Foundation of China (No. 61300131), the National Key Technology Research
   and Development Program of China (No. 2013BAK03B07), the National High
   Technology Research and Development Program of China (863 Program) (No.
   2013AA013902). The authors thank Dr. Meghan Stephens from National
   University of Ireland for her kind assistance on editing the manuscript.
CR [Anonymous], INT C COMP SOFTW MOD
   [Anonymous], IEEE C COMP VIS PATT
   [Anonymous], 2007, CVPR, DOI DOI 10.1109/CVPR.2007.383203
   Birchfield S, 1998, IEEE T PATTERN ANAL, V20, P401, DOI 10.1109/34.677269
   Bleyer M, 2005, ISPRS J PHOTOGRAMM, V59, P128, DOI 10.1016/j.isprsjprs.2005.02.008
   Bleyer M, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.14
   Bleyer M, 2010, PROC CVPR IEEE, P1570, DOI 10.1109/CVPR.2010.5539783
   Boros E., 2006, Technical report, Technical Report RRR 10-2006
   Di Stefano L, 2004, IMAGE VISION COMPUT, V22, P983, DOI 10.1016/j.imavis.2004.03.009
   Felzenszwalb PF, 2004, INT J COMPUT VISION, V59, P167, DOI 10.1023/B:VISI.0000022288.19776.77
   Hirschmüller H, 2007, PROC CVPR IEEE, P2134
   Ishikawa H, 2009, PROC CVPR IEEE, P2985
   Kim J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1033, DOI 10.1109/ICCV.2003.1238463
   Kim JC, 2005, PROC CVPR IEEE, P1075
   Klaus A, 2006, INT C PATT RECOG, P15
   Kolmogorov V, 2002, LECT NOTES COMPUT SC, V2352, P82
   Kolmogorov V, 2004, IEEE T PATTERN ANAL, V26, P147, DOI 10.1109/TPAMI.2004.1262177
   Kolmogorov V, 2007, IEEE T PATTERN ANAL, V29, P1274, DOI 10.1109/TPAMI.2007.1031
   Lee Z, 2013, IEEE T MULTIMEDIA, V15, P1855, DOI 10.1109/TMM.2013.2270456
   Lempitsky V, 2007, IEEE I CONF COMP VIS, P620
   Li G, 2006, LECT NOTES COMPUT SC, V3953, P44
   Li G, 2010, IEEE T PATTERN ANAL, V32, P72, DOI 10.1109/TPAMI.2008.270
   Liang Wang, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3033, DOI 10.1109/CVPR.2011.5995480
   Liu ZB, 2009, ICCSSE 2009: PROCEEDINGS OF 2009 4TH INTERNATIONAL CONFERENCE ON COMPUTER SCIENCE & EDUCATION, P999, DOI 10.1109/ICCSE.2009.5228397
   Mei X, 2013, PROC CVPR IEEE, P313, DOI 10.1109/CVPR.2013.47
   Ogale AS, 2004, PROC CVPR IEEE, P568
   Oh JD, 2010, J VIS COMMUN IMAGE R, V21, P404, DOI 10.1016/j.jvcir.2010.03.003
   Rhemann C, 2011, PROC CVPR IEEE, DOI 10.1109/CVPR.2011.5995372
   Scharstein D, 2003, PROC CVPR IEEE, P195
   Scharstein D, 2001, IEEE WORKSHOP ON STEREO AND MULTI-BASELINE VISION, PROCEEDINGS, P131, DOI 10.1023/A:1014573219977
   Scharstein Daniel., 2007, IEEE Conference on Computer Vision and Pattern Recognition, P1
   Song P, 2010, VISUAL COMPUT, V26, P1435, DOI 10.1007/s00371-010-0429-y
   Sun J, 2005, PROC CVPR IEEE, P399
   Tanimoto M, 2011, IEEE SIGNAL PROC MAG, V28, P67, DOI 10.1109/MSP.2010.939077
   Tao H, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P532, DOI 10.1109/ICCV.2001.937562
   TERZOPOULOS D, 1983, COMPUT VISION GRAPH, V24, P52, DOI 10.1016/0734-189X(83)90020-8
   Wang DL, 2011, J VIS COMMUN IMAGE R, V22, P325, DOI 10.1016/j.jvcir.2011.02.001
   Wang DL, 2010, INT CONF COMP SCI, P410, DOI 10.1109/ICCSIT.2010.5564081
   Woodford O, 2009, IEEE T PATTERN ANAL, V31, P2115, DOI 10.1109/TPAMI.2009.131
   Xu ZL, 2008, VISUAL COMPUT, V24, P45, DOI 10.1007/s00371-007-0177-9
   Xun Sun, 2011, 2011 International Conference on 3D Imaging, Modeling, Processing, Visualization and Transmission (3DIMPVT), P132, DOI 10.1109/3DIMPVT.2011.24
   Yang QX, 2012, PROC CVPR IEEE, P1402, DOI 10.1109/CVPR.2012.6247827
   Yang QX, 2009, IEEE T PATTERN ANAL, V31, P492, DOI 10.1109/TPAMI.2008.99
   Yoon KJ, 2006, IEEE T PATTERN ANAL, V28, P650, DOI 10.1109/TPAMI.2006.70
   Zitnick CL, 2004, ACM T GRAPHIC, V23, P600, DOI 10.1145/1015706.1015766
NR 45
TC 18
Z9 19
U1 0
U2 29
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2015
VL 31
IS 9
BP 1253
EP 1269
DI 10.1007/s00371-014-1009-3
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CO8AY
UT WOS:000359388100008
DA 2024-07-18
ER

PT J
AU Arigbabu, OA
   Ahmad, SMS
   Adnan, WAW
   Yussof, S
AF Arigbabu, Olasimbo Ayodeji
   Ahmad, Sharifah Mumtazah Syed
   Adnan, Wan Azizun Wan
   Yussof, Salman
TI Recent advances in facial soft biometrics
SO VISUAL COMPUTER
LA English
DT Article
DE Face recognition; Soft biometrics; Measured descriptor; Semantic
   descriptor; Image retrieval
ID GENDER CLASSIFICATION; FACE DETECTION; SKIN COLOR; TRAITS; IMAGES;
   IDENTIFICATION; RECOGNITION; EIGENFACES
AB Face as a biometric attribute has been extensively studied over the past few decades. Even though, satisfactory results are already achieved in controlled environments, the practicality of face recognition in realistic scenarios is still limited by several challenges, such as, expression, pose, occlusion, etc. Recently, the research direction is concentrating on the prospects of complementing face recognition systems with facial soft biometric traits. The ease of extracting facial soft biometrics under several varying conditions has mainly resulted in the ability of using the traits to, either improve the performance of traditional face recognition systems, or performing recognition solely based on many facial soft biometrics. This paper presents state-of-the-art techniques in facial soft biometrics research by describing the type of traits, feature extraction methods, and the application domains. It indicates the most recent and valuable results attained, while also highlighting some possible future scientific research directions to be investigated.
C1 [Arigbabu, Olasimbo Ayodeji; Ahmad, Sharifah Mumtazah Syed; Adnan, Wan Azizun Wan] Univ Putra Malaysia, Fac Engn, Serdang 43400, Selangor, Malaysia.
   [Yussof, Salman] Univ Tenaga Nas, Dept Syst & Networking, Selangor, Malaysia.
C3 Universiti Putra Malaysia; Universiti Tenaga Nasional
RP Arigbabu, OA (corresponding author), Univ Putra Malaysia, Fac Engn, Serdang 43400, Selangor, Malaysia.
EM oa.arigbabu@gmail.com; s_mumtazah@upm.edu.my; wawa@upm.edu.my;
   salman@uniten.edu.my
RI arigbabu, olasimbo ayodeji/L-9792-2013; Yussof, Salman/ABC-4516-2020;
   AHMAD, SHARIFAH/JJC-4353-2023
OI Yussof, Salman/0000-0002-2040-4454; 
CR Abreu MCD, 2011, IEEE T SYST MAN CY C, V41, P599, DOI 10.1109/TSMCC.2010.2056920
   Ailisto H, 2006, PATTERN RECOGN LETT, V27, P325, DOI 10.1016/j.patrec.2005.08.018
   [Anonymous], 2011 INT JOINT C BIO
   [Anonymous], 2010, 2010 4 IEEE INT C BI
   [Anonymous], IEEE 3 INT C IM VIS
   [Anonymous], 2012, INT J APPL INF SYST
   [Anonymous], P 17 INT C PATT REC
   [Anonymous], COMPUTER
   [Anonymous], 2012, 2012 IEEE INT C COMP
   [Anonymous], P INT C MULT MM 10
   [Anonymous], 2013, 2013 10 IEEE INT C W
   [Anonymous], LECT NOTES COMPUTER
   [Anonymous], FIDA FACE RECOGNITIO
   [Anonymous], 2011, CVPR 2011 WORKSHOPS
   [Anonymous], P IEEE INT C MULT EX
   [Anonymous], P SPIE
   [Anonymous], 2013, P INT C BIOM
   [Anonymous], 2003, PROC GRAPHICON
   [Anonymous], P 19 INT C PATT REC
   [Anonymous], INT J ARTIF INTELL A
   [Anonymous], INT REV COMPUT SOFTW
   [Anonymous], 2008, HDB BIOMETRICS
   Bekios-Calfa J, 2011, IEEE T PATTERN ANAL, V33, P858, DOI 10.1109/TPAMI.2010.208
   Berbar MA, 2014, VISUAL COMPUT, V30, P19, DOI 10.1007/s00371-013-0774-8
   Boaventura IAG, 2006, IEEE SYS MAN CYBERN, P5071, DOI 10.1109/ICSMC.2006.385112
   Chai D, 1998, AUTOMATIC FACE AND GESTURE RECOGNITION - THIRD IEEE INTERNATIONAL CONFERENCE PROCEEDINGS, P124, DOI 10.1109/AFGR.1998.670936
   Chu WS, 2013, INFORM SCIENCES, V221, P98, DOI 10.1016/j.ins.2012.09.008
   COOTES TF, 1995, COMPUT VIS IMAGE UND, V61, P38, DOI 10.1006/cviu.1995.1004
   Cootes TF, 2001, IEEE T PATTERN ANAL, V23, P681, DOI 10.1109/34.927467
   Dantcheva Antitza, 2011, Proceedings of the 2011 8th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS 2011), P309, DOI 10.1109/AVSS.2011.6027342
   Dantcheva A., 2010, 2010 IEEE 12th International Workshop on Multimedia Signal Processing (MMSP), P511, DOI 10.1109/MMSP.2010.5662074
   Dantcheva A, 2011, MULTIMED TOOLS APPL, V51, P739, DOI 10.1007/s11042-010-0635-7
   Dantcheva Antitza., 2011, IEEE Workshop on Applications of Computer Vision, P227, DOI [DOI 10.1109/WACV.2011.5711507, 10.1109/WACV.2011.5711507]
   DasGupta A, 2005, J STAT PLAN INFER, V130, P377, DOI 10.1016/j.jspi.2003.11.015
   Daugman J, 2004, IEEE T CIRC SYST VID, V14, P21, DOI 10.1109/TCSVT.2003.818350
   Denman S., 2012, DIGITAL IMAGE COMPUT, P1
   El Kissi Ghalleb Asma, 2013, 2013 10th International Multi-Conference on Systems, Signals and Devices (SSD 2013), P1
   Fan Hai Xiang, 2013, International Journal of Information and Electronics Engineering, V3, P172, DOI 10.7763/IJIEE.2013.V3.292
   Farinella G, 2012, 2012 INTERNATIONAL CONFERENCE ON INFORMATICS, ELECTRONICS & VISION (ICIEV), P383, DOI 10.1109/ICIEV.2012.6317383
   Garcia C, 1999, IEEE T MULTIMEDIA, V1, P264, DOI 10.1109/6046.784465
   Golomb B.A., 1990, Advances in Neural Information Processing Systems (NIPS), P572
   Hashem H., 2009, NATL RADIO SCI C, P1
   Hsu RL, 2002, IEEE T PATTERN ANAL, V24, P696, DOI 10.1109/34.1000242
   Ichino M, 2013, 2013 INTERNATIONAL CONFERENCE ON BIOMETRICS AND KANSEI ENGINEERING (ICBAKE), P314, DOI 10.1109/ICBAKE.2013.86
   Jabid Taskeed, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P2162, DOI 10.1109/ICPR.2010.373
   Jain AK, 2004, PROC SPIE, V5404, P561, DOI 10.1117/12.542890
   Jain AK, 2004, LECT NOTES COMPUT SC, V3087, P259
   Jain AK, 2004, IEEE T CIRC SYST VID, V14, P4, DOI 10.1109/TCSVT.2003.818349
   Jain AK, 2009, IEEE IMAGE PROC, P37, DOI 10.1109/ICIP.2009.5413921
   Khan S. A., 2011, 2011 Proceedings of the IEEE 14th International Multitopic Conference (INMIC 2011), P25, DOI 10.1109/INMIC.2011.6151483
   Kim HC, 2006, PATTERN RECOGN LETT, V27, P618, DOI 10.1016/j.patrec.2005.09.027
   Kim MG, 2012, J BIOMED BIOTECHNOL, DOI 10.1155/2012/614146
   Li Lu, 2009, 2009 WRI World Congress on Computer Science and Information Engineering, CSIE, P48, DOI 10.1109/CSIE.2009.871
   Lin D., 2006, COMPUTER VISION PATT, V2, P1355
   Loy G, 2003, IEEE T PATTERN ANAL, V25, P959, DOI 10.1109/TPAMI.2003.1217601
   Lu HC, 2007, ICNC 2007: THIRD INTERNATIONAL CONFERENCE ON NATURAL COMPUTATION, VOL 2, PROCEEDINGS, P646
   Lu XG, 2004, P SOC PHOTO-OPT INS, V5404, P114, DOI 10.1117/12.542847
   Mahoor MH, 2006, PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION - PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE, P144
   Majid A, 2003, MULT C 2003 INMIC 20, P59
   Mäkinen E, 2008, IEEE T PATTERN ANAL, V30, P541, DOI 10.1109/TPAMI.2007.70800
   Maltoni D., 2009, HDB FINGERPRINT RECO
   Marcialis GL, 2009, J VISUAL LANG COMPUT, V20, P101, DOI 10.1016/j.jvlc.2009.01.005
   Martinson E, 2013, ACMIEEE INT CONF HUM, P49, DOI 10.1109/HRI.2013.6483501
   Moghaddam B, 2002, IEEE T PATTERN ANAL, V24, P707, DOI 10.1109/34.1000244
   Nazir M, 2010, ARTIF INT SER WSEAS, P70
   Niinuma K, 2010, IEEE T INF FOREN SEC, V5, P771, DOI 10.1109/TIFS.2010.2075927
   Niu ZH, 2009, IEEE SIGNAL PROC LET, V16, P897, DOI 10.1109/LSP.2009.2026457
   Park U, 2010, IEEE T INF FOREN SEC, V5, P406, DOI 10.1109/TIFS.2010.2049842
   Piernas Juan., 2007, SC 07, P1, DOI DOI 10.1145/1362622.1362660
   Rai P., 2010, 2010 5th International Conference on Industrial and Information Systems (ICIIS 2010), P448, DOI 10.1109/ICIINFS.2010.5578661
   Ramanathan V, 2010, PATTERN RECOGN LETT, V31, P2425, DOI 10.1016/j.patrec.2010.07.011
   Ran Y, 2008, INT C PATT RECOG, P2893
   Reid DA, 2013, HANDB STAT, V31, P327, DOI 10.1016/B978-0-444-53859-8.00013-8
   Reid D.A., 2011, 2011 International Joint Conference On Biometrics (IJCB), P1
   Shan CF, 2012, PATTERN RECOGN LETT, V33, P431, DOI 10.1016/j.patrec.2011.05.016
   Shih HC, 2013, PATTERN RECOGN, V46, P519, DOI 10.1016/j.patcog.2012.08.003
   Sinha P, 2006, P IEEE, V94, P1948, DOI 10.1109/JPROC.2006.884093
   Srinivas N, 2012, IEEE T INF FOREN SEC, V7, P1536, DOI 10.1109/TIFS.2012.2206027
   Tamura S, 1996, PATTERN RECOGN, V29, P331, DOI 10.1016/0031-3203(95)00073-9
   TURK M, 1991, J COGNITIVE NEUROSCI, V3, P71, DOI 10.1162/jocn.1991.3.1.71
   Vijayanandh R., 2011, International Journal of Modeling and Optimization, V1, P236
   Wan KW, 2004, PROCEEDINGS OF THE 2004 INTERNATIONAL SYMPOSIUM ON INTELLIGENT MULTIMEDIA, VIDEO AND SPEECH PROCESSING, P109, DOI 10.1109/ISIMP.2004.1434012
   Wu M, 2012, ELECTRON LETT, V48, P629, DOI 10.1049/el.2012.0834
   Yang MH, 2002, IEEE T PATTERN ANAL, V24, P34, DOI 10.1109/34.982883
   Yang MH, 2000, INT C PATT RECOG, P1115, DOI 10.1109/ICPR.2000.905667
   Yang WK, 2011, LECT NOTES COMPUT SC, V7098, P214, DOI 10.1007/978-3-642-25449-9_27
   Zewail R, 2004, 2004 47TH MIDWEST SYMPOSIUM ON CIRCUITS AND SYSTEMS, VOL I, CONFERENCE PROCEEDINGS, P225
   Zhang Z, 2009, LECT NOTES COMPUT SC, V5558, P424, DOI 10.1007/978-3-642-01793-3_44
   Zhao W, 2003, ACM COMPUT SURV, V35, P399, DOI 10.1145/954339.954342
   Zuo F, 2004, IEEE IMAGE PROC, P1425
NR 90
TC 16
Z9 17
U1 0
U2 13
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2015
VL 31
IS 5
BP 513
EP 525
DI 10.1007/s00371-014-0990-x
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CH2CF
UT WOS:000353831400002
DA 2024-07-18
ER

PT J
AU Ma, ZY
   Wu, EH
AF Ma, Ziyang
   Wu, Enhua
TI Real-time and robust hand tracking with a single depth camera
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Computational Visual Media Conference
CY 2013
CL Geneva, SWITZERLAND
DE Hand tracking; Virtual reality; Motion capture; User interface; 3D
   interaction
AB In this paper, we introduce a novel, real-time and robust hand tracking system, capable of tracking the articulated hand motion in full degrees of freedom (DOF) using a single depth camera. Unlike most previous systems, our system is able to initialize and recover from tracking loss automatically. This is achieved through an efficient two-stage k-nearest neighbor database searching method proposed in the paper. It is effective for searching from a pre-rendered database of small hand depth images, designed to provide good initial guesses for model based tracking. We also propose a robust objective function, and improve the Particle Swarm Optimization algorithm with a resampling based strategy in model based tracking. It provides continuous solutions in full DOF hand motion space more efficiently than previous methods. Our system runs at 40 fps on a GeForce GTX 580 GPU and experimental results show that the system outperforms the state-of-the-art model based hand tracking systems in terms of both speed and accuracy. The work result is of significance to various applications in the field of human-computer-interaction and virtual reality.
C1 [Ma, Ziyang; Wu, Enhua] Chinese Acad Sci, Inst Software, State Key Lab Comp Sci, Beijing, Peoples R China.
   [Ma, Ziyang] Univ Chinese Acad Sci, Beijing, Peoples R China.
   [Wu, Enhua] Univ Macau, Macau, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Software, CAS; Chinese Academy
   of Sciences; University of Chinese Academy of Sciences, CAS; University
   of Macau
RP Ma, ZY (corresponding author), Chinese Acad Sci, Inst Software, State Key Lab Comp Sci, Beijing, Peoples R China.
EM maziyang08@gmail.com
FU National 973 Program of Basic Research on Science and Technology
   [2009CB320800]; NSFC [61272326]; University of Macau
FX The authors would like to thank the anonymous reviewers for their
   valuable comments and suggestions. This research is supported by
   National 973 Program of Basic Research on Science and Technology
   (2009CB320800), NSFC (61272326) and the research grant of University of
   Macau.
CR Albrecht I., 2003, ACM SIGGRAPH/Eurographics Symposium on Computer Animation, P98
   [Anonymous], IEEE C COMP VIS PATT
   [Anonymous], 2011, BRIT MACH VIS C BMVC
   [Anonymous], 2011, IEEE C COMP VIS PATT
   Argyros A.A., 2004, EUR C COMP VIS ECCV
   Athitsos V., 2003, IEEE C COMP VIS PATT
   Cao Y., 2011, IEEE C COMP VIS PATT
   Clerc M, 2002, IEEE T EVOLUT COMPUT, V6, P58, DOI 10.1109/4235.985692
   Criminisil A, 2011, FOUND TRENDS COMPUT, V7, P81, DOI [10.1561/0600000035, 10.1501/0000000035]
   de la Gorce M., 2008, IEEE C COMP VIS PATT
   Dipietro L, 2008, IEEE T SYST MAN CY C, V38, P461, DOI 10.1109/TSMCC.2008.923862
   Erol A, 2007, COMPUT VIS IMAGE UND, V108, P52, DOI 10.1016/j.cviu.2006.10.012
   Huang H., 2011, PROC 2011 ACM SIGGRA, P73, DOI DOI 10.1145/2019406.2019416
   Kennedy J., 2001, SWARM INTELLIGENCE
   Liu YJ, 2012, VISUAL COMPUT, V28, P75, DOI 10.1007/s00371-011-0605-8
   Oikonomidis I., 2010, AS C COMP VIS ACCV
   Oikonomidis I, 2011, IEEE I CONF COMP VIS, P2088, DOI 10.1109/ICCV.2011.6126483
   Pharr M., 2005, GPU Gems 2: Programming techniques for highperformance graphics and general purpose computation
   ROSALES R, 2001, IEEE INT C COMP VIS
   Schlattmann M, 2007, COMPUT GRAPH FORUM, V26, P467, DOI 10.1111/j.1467-8659.2007.01069.x
   Stenger B., 2001, IEEE C COMP VIS PATT
   Stenger B. D. R., 2004, THESIS
   TOMASI C, 2003, IEEE INT C COMP VIS
   Wang RY, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531369
NR 24
TC 17
Z9 17
U1 0
U2 9
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2014
VL 30
IS 10
BP 1133
EP 1144
DI 10.1007/s00371-013-0894-1
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA AP6NF
UT WOS:000342193800007
DA 2024-07-18
ER

PT J
AU Guo, JW
   Yan, DM
   Bao, GB
   Dong, WM
   Zhang, XP
   Wonka, P
AF Guo, Jianwei
   Yan, Dong-Ming
   Bao, Guanbo
   Dong, Weiming
   Zhang, Xiaopeng
   Wonka, Peter
TI Efficient triangulation of Poisson-disk sampled point sets
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 31st CGI conference
CY JUN 10-13, 2014
CL Sydney, AUSTRALIA
DE Triangulation; Poisson-disk sampling; Geometric algorithms
ID DELAUNAY; ALGORITHM
AB In this paper, we present a simple yet efficient algorithm for triangulating a 2D input domain containing a Poisson-disk sampled point set. The proposed algorithm combines a regular grid and a discrete clustering approach to speedup the triangulation. Moreover, our triangulation algorithm is flexible and performs well on more general point sets such as adaptive, non-maximal Poisson-disk sets. The experimental results demonstrate that our algorithm is robust for a wide range of input domains and achieves significant performance improvement compared to the current state-of-the-art approaches.
C1 [Guo, Jianwei; Yan, Dong-Ming; Bao, Guanbo; Dong, Weiming; Zhang, Xiaopeng] Chinese Acad Sci, Inst Automat, LIAMA, NLPR, Beijing, Peoples R China.
   [Yan, Dong-Ming; Wonka, Peter] King Abdullah Univ Sci & Technol, Visual Comp Ctr, Thuwal, Saudi Arabia.
C3 Chinese Academy of Sciences; Institute of Automation, CAS; King Abdullah
   University of Science & Technology
RP Yan, DM (corresponding author), Chinese Acad Sci, Inst Automat, LIAMA, NLPR, Beijing, Peoples R China.
EM jianwei.guo@nlpr.ia.ac.cn; yandongming@gmail.com; guanbo.bao@gmail.com;
   weiming.dong@ia.ac.cn; xiaopeng.zhang@ia.ac.cn; pwonka@gmail.com
RI DONG, Weiming/AAG-7678-2020
OI DONG, Weiming/0000-0001-6502-145X; Yan, Dong-Ming/0000-0003-2209-2404
FU National Natural Science Foundation of China [61372168, 61172104,
   61331018, 61271431]; KAUST Visual Computing Center; National Science
   Foundation
FX This research was partially funded by National Natural Science
   Foundation of China (Nos. 61372168, 61172104, 61331018, and 61271431),
   the KAUST Visual Computing Center, and the National Science Foundation.
CR Barber CB, 1996, ACM T MATH SOFTWARE, V22, P469, DOI 10.1145/235815.235821
   Blelloch GE, 1999, ALGORITHMICA, V24, P243, DOI 10.1007/PL00008262
   Cheng SW, 2008, PROCEEDINGS OF THE 16TH INTERNATIONAL MESHING ROUNDTABLE, P477, DOI 10.1007/978-3-540-75103-8_27
   Cheng Siu-Wing, 2012, Delaunay mesh generation
   Chew LP, 1989, 89983 CORN U DEP COM
   Chrisochoides N, 2003, INT J NUMER METH ENG, V58, P161, DOI 10.1002/nme.765
   Cohen-Steiner D, 2004, ACM T GRAPHIC, V23, P905, DOI 10.1145/1015706.1015817
   COOK RL, 1986, ACM T GRAPHIC, V5, P51, DOI 10.1145/7529.8927
   Dunbar D, 2006, ACM T GRAPHIC, V25, P503, DOI 10.1145/1141911.1141915
   Ebeida MS, 2012, COMPUT GRAPH FORUM, V31, P785, DOI 10.1111/j.1467-8659.2012.03059.x
   Ebeida MS, 2011, COMPUT AIDED DESIGN, V43, P1506, DOI 10.1016/j.cad.2011.08.012
   Ebeida MS, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964944
   Edelsbrunner H., 2001, Geometry and Topology for Mesh Generation
   Gamito MN, 2009, ACM T GRAPHIC, V29, DOI 10.1145/1640443.1640451
   Hoff KE, 1999, COMP GRAPH, P277, DOI 10.1145/311535.311567
   Jones T. R., 2006, Journal of Graphics Tools, V11, P27
   Lagae A, 2008, COMPUT GRAPH FORUM, V27, P114, DOI 10.1111/j.1467-8659.2007.01100.x
   Lu YY, 2012, COMPUT GRAPH-UK, V36, P466, DOI 10.1016/j.cag.2012.03.018
   Qi M, 2013, IEEE T VIS COMPUT GR, V19, P736, DOI 10.1109/TVCG.2012.307
   Rong GD, 2008, I3D 2008: SYMPOSIUM ON INTERACTIVE 3D GRAPHICS AND GAMES, PROCEEDINGS, P89
   Schechter H, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185557
   Shewchuk J. R., 1996, Applied Computational Geometry. Towards Geometric Engineering. FCRC'96 Workshop, WACG'96. Selected Papers, P203, DOI 10.1007/BFb0014497
   Shewchuk JR, 2002, COMP GEOM-THEOR APPL, V22, P21, DOI 10.1016/S0925-7721(01)00047-5
   Wei LY, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778816
   Wei LY, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360619
   White KB, 2007, RT07: IEEE/EG Symposium on Interactive Ray Tracing 2007, P129, DOI 10.1109/RT.2007.4342600
   Yan DM, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2516971.2516973
NR 27
TC 8
Z9 8
U1 0
U2 6
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2014
VL 30
IS 6-8
BP 773
EP 785
DI 10.1007/s00371-014-0948-z
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA AI7GX
UT WOS:000337054700020
DA 2024-07-18
ER

PT J
AU Shu, Y
   Qian, YL
   Sun, HQ
   Chen, YY
AF Shu, Yue
   Qian, Yinling
   Sun, Hanqiu
   Chen, Yanyun
TI Efficient texture synthesis of aggregate solid material
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 31st CGI conference
CY JUN 10-13, 2014
CL Sydney, AUSTRALIA
DE Aggregate material; Solid texture; Cellular texture; Stereology
AB Solid texturing is particularly well suited for many architectural materials as they are intrinsically 3D such as marbles and stones. Directly applying 2D texture to such material results in evident artifact because of the issue of synthesizing solid material or patterns usually very difficult. We propose the simple and effective solid synthesis that efficiently utilizes the procedural texture and exemplar-based synthesis seamlessly in one system. Our method generates warping particles and stores them with a few points based on cellular texture. Stereological technique and spring system are used to automatically guide the synthesis procedure. Using adaptive k-means clustering, we can recover color exemplar with high fidelity. We develop vector-represented results to avoid blurring, and GPU-based rendering for real-time synthesis on a common graphics card. Our experiments showed that the proposed approach can generate realistic texturing of solid material with low memory footprint and efficient aggregate-material synthesis in seconds.
C1 [Shu, Yue; Chen, Yanyun] Chinese Acad Sci, State Key Lab Comp Sci, Inst Software, Beijing 100190, Peoples R China.
   [Shu, Yue] Univ Chinese Acad Sci, Beijing, Peoples R China.
   [Qian, Yinling; Sun, Hanqiu] Chinese Univ Hong Kong, Dept Comp Sci & Engn, Shatin, Hong Kong, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Software, CAS; Chinese Academy
   of Sciences; University of Chinese Academy of Sciences, CAS; Chinese
   University of Hong Kong
RP Shu, Y (corresponding author), Chinese Acad Sci, State Key Lab Comp Sci, Inst Software, Beijing 100190, Peoples R China.
EM shuy@ios.ac.cn; constantine.69@gmail.com
FU RGC [416311, 416212]
FX The work was supported by RGC research grants (ref. 416311, 416212).
CR [Anonymous], 2007, ACM T GRAPHIC, DOI DOI 10.1145/1239451.1239547
   Chen JT, 2010, VISUAL COMPUT, V26, P253, DOI 10.1007/s00371-009-0408-3
   Dischler J.-M., 1998, COMPUT GRAPH FORUM, V17, P87
   Dischler JM, 2001, COMPUT GRAPH-UK, V25, P135, DOI 10.1016/S0097-8493(00)00113-8
   Dong Y, 2008, COMPUT GRAPH FORUM, V27, P1165, DOI 10.1111/j.1467-8659.2008.01254.x
   Du SP, 2013, IEEE T VIS COMPUT GR, V19, P460, DOI 10.1109/TVCG.2012.129
   Ebert David S, 2003, Texturing Modeling: A Procedural Approach
   Efros A. A., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1033, DOI 10.1109/ICCV.1999.790383
   GHAZANFARPOUR D, 1995, COMPUT GRAPH-UK, V19, P413, DOI 10.1016/0097-8493(95)00011-Z
   Ghazanfarpour D, 1996, COMPUT GRAPH FORUM, V15, pC311, DOI 10.1111/1467-8659.1530311
   Heeger D. J., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P229, DOI 10.1145/218380.218446
   Jagnow R, 2004, ACM T GRAPHIC, V23, P329, DOI 10.1145/1015706.1015724
   Kwatra V, 2005, ACM T GRAPHIC, V24, P795, DOI 10.1145/1073204.1073263
   Lagae A, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531360
   Legakis J, 2001, COMP GRAPH, P309, DOI 10.1145/383259.383293
   Ma CY, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461921
   Ma CY, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964957
   Peachey D. R., 1985, Computer Graphics, V19, P279, DOI 10.1145/325165.325246
   Perlin K., 1985, Computer Graphics, V19, P287, DOI 10.1145/325165.325247
   Pietroni N, 2010, IEEE COMPUT GRAPH, V30, P74, DOI 10.1109/MCG.2009.153
   Qin XJ, 2007, IEEE T VIS COMPUT GR, V13, P379, DOI 10.1109/TVCG.2007.31
   Sheng B., 2013, SIGGRAPH ASIA 2013 T, P7
   Takayama K, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360652
   Wang LD, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778823
   Wei LY, 2000, COMP GRAPH, P479, DOI 10.1145/344779.345009
   Worley S., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P291, DOI 10.1145/237170.237267
NR 26
TC 5
Z9 6
U1 0
U2 8
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2014
VL 30
IS 6-8
BP 877
EP 887
DI 10.1007/s00371-014-0951-4
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA AI7GX
UT WOS:000337054700029
DA 2024-07-18
ER

PT J
AU Wong, TH
   Leach, G
   Zambetta, F
AF Wong, Tsz Ho
   Leach, Geoff
   Zambetta, Fabio
TI An adaptive octree grid for GPU-based collision detection of deformable
   objects
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 31st CGI conference
CY JUN 10-13, 2014
CL Sydney, AUSTRALIA
DE Collision detection; Deformable objects; Octree grid; GPU-based
AB In spatial subdivision-based collision detection methods on GPUs, uniform subdivision works well for even triangle spatial distributions, whilst for uneven cases non-uniform subdivision works better. Non-uniform subdivision techniques mainly include hierarchical grids and octrees. Hierarchical grids have been adopted for previous GPU-based approaches, due to their suitability for GPUs. However, octrees offer a better adaptation to distributions. One contribution of this paper is the use of an octree grid that takes a middle path between these two structures, and accelerates collision detection by significantly reducing the number of broad-phase tests which, due to their large quantity, are generally the main bottleneck in performance. Another contribution is to achieve further reduction in the number of tests in the broad phase using a two-stage scheme to improve octree subdivision. The octree grid approach is also able to address the issue of uneven triangle sizes, another common difficulty for spatial subdivision techniques. Compared to the virtual subdivision method which reports the fastest results among existing methods, speedups between 1.0 and 1.5 are observed for most standard benchmarks where triangle sizes and spatial distributions are uneven.
C1 [Wong, Tsz Ho; Leach, Geoff; Zambetta, Fabio] RMIT Univ, Sch Comp Sci & IT, Melbourne, Vic 3001, Australia.
C3 Royal Melbourne Institute of Technology (RMIT)
RP Wong, TH (corresponding author), RMIT Univ, Sch Comp Sci & IT, GPO Box 2476, Melbourne, Vic 3001, Australia.
EM itszwong@gmail.com
OI Zambetta, Fabio/0000-0003-4133-7913
CR Alcantara DA, 2009, ACM T GRAPHIC, V28, DOI [10.1145/1618452.1610500, 10.1145/1618452.1618500]
   [Anonymous], 1997, J GRAPH TOOLS, DOI DOI 10.1080/10867651.1997.10487480
   BRIDSON R, 2005, ACM SIGGRAPH 2005 CO
   Curtis S, 2008, I3D 2008: SYMPOSIUM ON INTERACTIVE 3D GRAPHICS AND GAMES, PROCEEDINGS, P61
   Eitz M, 2007, IEEE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS 2007, PROCEEDINGS, P61, DOI 10.1109/SMI.2007.18
   Fan WS, 2011, COMPUT GRAPH FORUM, V30, P1451, DOI 10.1111/j.1467-8659.2011.02019.x
   Gottschalk S., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P171, DOI 10.1145/237170.237244
   Govindaraju N.K., 2003, Proc. of Symp. on Geom. Processing, P25
   Grand S.L., 2007, GPU GEMS, V3
   Gress A, 2006, COMPUT GRAPH FORUM, V25, P497, DOI 10.1111/j.1467-8659.2006.00969.x
   Harris M., 2007, GPU GEMS, V3
   Heidelberger B, 2003, VISION, MODELING, AND VISUALIZATION 2003, P461
   Heidelberger B., 2004, PROC WSCG, P145
   HILLIS WD, 1986, COMMUN ACM, V29, P1170, DOI 10.1145/7902.7903
   Kim D, 2009, COMPUT GRAPH FORUM, V28, P1791, DOI 10.1111/j.1467-8659.2009.01556.x
   Klosowski JT, 1998, IEEE T VIS COMPUT GR, V4, P21, DOI 10.1109/2945.675649
   Kroiss R.R, 2013, THESIS U COLORADO BO, P45
   Larsson T, 2006, COMPUT GRAPH-UK, V30, P450, DOI 10.1016/j.cag.2006.02.011
   Lauterbach C, 2010, COMPUT GRAPH FORUM, V29, P419, DOI 10.1111/j.1467-8659.2009.01611.x
   Lauterbach C, 2009, COMPUT GRAPH FORUM, V28, P375, DOI 10.1111/j.1467-8659.2009.01377.x
   Mezger J, 2003, WSCG'2003, VOL 11, NO 2, CONFERENCE PROCEEDINGS, P322
   NVIDIA, 2013, CUD SAMPL
   Otaduy MA, 2007, IEEE VIRTUAL REALITY 2007, PROCEEDINGS, P83
   Pabst S, 2010, COMPUT GRAPH FORUM, V29, P1605, DOI 10.1111/j.1467-8659.2010.01769.x
   PALMER IJ, 1995, COMPUT GRAPH FORUM, V14, P105, DOI 10.1111/1467-8659.1420105
   Provot X., 1997, GRAPHICS INTERFACE, P177
   Rodriguez-Navarro X., 2005, P 15 C ESP INF GRAF, P147
   Tang M., 2010, P ACM SIGGRAPH S INT, P7, DOI [10.1145/1730804.1730806, DOI 10.1145/1730804.1730806]
   Tang M., 2011, I3D 11 P 2011 ACM SI, P63
   Tang M, 2010, GRAPH MODELS, V72, P7, DOI 10.1016/j.gmod.2010.01.001
   Tang M, 2008, SPM 2008: PROCEEDINGS OF THE ACM SOLID AND PHYSICAL MODELING SYMPOSIUM, P25
   Teschner M, 2005, COMPUT GRAPH FORUM, V24, P61, DOI 10.1111/j.1467-8659.2005.00829.x
   Teschner M, 2003, VISION, MODELING, AND VISUALIZATION 2003, P47
   Wong TH, 2012, VISUAL COMPUT, V28, P829, DOI 10.1007/s00371-012-0706-z
   Wong W. S. -K., 2006, P ACM VRCIA, P181
   Zhang DL, 2000, EIGHTH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P328, DOI 10.1109/PCCGA.2000.883956
NR 36
TC 26
Z9 31
U1 0
U2 19
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2014
VL 30
IS 6-8
BP 729
EP 738
DI 10.1007/s00371-014-0954-1
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA AI7GX
UT WOS:000337054700016
DA 2024-07-18
ER

PT J
AU Chen, XW
   Li, FM
   Zhao, QP
   Quan, L
AF Chen, Xiaowu
   Li, Faming
   Zhao, Qinping
   Quan, Long
TI Optimizing neighborhood projection with relaxation factor for
   inextensible cloth simulation
SO VISUAL COMPUTER
LA English
DT Article
DE Cloth simulation; Inextensibility; Neighborhood projection; Relaxation
   factor
ID ANIMATION
AB In this paper, we propose a novel method for inextensible cloth simulation. Our method introduces a neighborhood projection optimized with a relaxation factor. The neighborhood projection enforces inextensibility by modifying particle positions with a Jacobi-style iteration, leading to conservation of linear and angular quasi momenta. The relaxation factor is estimated according to the corrections and constraints, and is used to scale the corrections while keeping convergence to a smaller number of iterations. Experimental results demonstrate that our method increases the simulation efficiency, and stably handles inextensible cloth even in overconstrained situations. In addition to the simulation of hanging cloth and draping cloth, the simulated umbrella demonstrates the characters of our method for this type of objects.
C1 [Chen, Xiaowu; Li, Faming; Zhao, Qinping] Beihang Univ, Sch Comp Sci & Engn, State Key Lab Virtual Real Technol & Syst, Beijing, Peoples R China.
   [Quan, Long] Hong Kong Univ Sci & Technol, Dept Comp Sci & Engn, Hong Kong, Hong Kong, Peoples R China.
C3 Beihang University; Hong Kong University of Science & Technology
RP Li, FM (corresponding author), Beihang Univ, Sch Comp Sci & Engn, State Key Lab Virtual Real Technol & Syst, Beijing, Peoples R China.
EM lifm@vrlab.buaa.edu.cn
FU 863 Program [2012AA011504]; NSFC [60933006]; RD Program [2013BAH35F01];
   ITER [2012GB102008]
FX We would like to thank the anonymous reviewers for their help in
   improving the paper. We also thank Marie-Paule Cani for support. This
   work was partially supported by 863 Program (2012AA011504), NSFC
   (60933006), R&D Program (2013BAH35F01), and ITER (2012GB102008).
CR [Anonymous], P GRAPH INT
   [Anonymous], 2001, Game developers conference
   [Anonymous], BETTER CLOTH UNBIASE
   [Anonymous], P ACM SIGGRAPH
   [Anonymous], 2012, Applied Iterative Methods
   [Anonymous], 2007, APPL ITERATIVE METHO
   [Anonymous], VIS COMPUT
   [Anonymous], P S COMP AN
   [Anonymous], ACM T GRAPH
   [Anonymous], COMP GRAPH VIS IADIS
   [Anonymous], 2011, ACM T GRAPHICS PROC
   [Anonymous], 1971, ITERATIVE SOLUTION L
   Ascher UM, 2003, VISUAL COMPUT, V19, P526, DOI 10.1007/s00371-003-0220-4
   Baraff D., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P137, DOI 10.1145/237170.237226
   Baraff D., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P43, DOI 10.1145/280814.280821
   Bender J., 2008, Parallel simulation of inextensible cloth, P47
   BERGOU M, 2006, P 4 EUR S GEOM PROC, P227
   Breen D. E., 1994, Computer Graphics Proceedings. Annual Conference Series 1994. SIGGRAPH 94 Conference Proceedings, P365, DOI 10.1145/192161.192259
   Bridson R., 2003, ACM SIGGRAPH/Eurographics Symposium on Computer Animation, P28
   Chen YJ, 2012, VISUAL COMPUT, V28, P765, DOI 10.1007/s00371-012-0687-y
   Choi KJ, 2002, ACM T GRAPHIC, V21, P604, DOI 10.1145/566570.566624
   Eberhardt B, 2000, SPRING COMP SCI, P137
   English E, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360665
   Goldenthal R, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239500
   Guan P, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185531
   Hong M, 2005, IEEE INT CONF ROBOT, P4520
   Kaldor JM, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778842
   Kaldor JM, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360664
   Macklin M, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461984
   Marsden J. E., 1999, Introduction to Mechanics and Symmetry, V17, pxviii +582
   Meyer M, 2001, J VISUAL COMP ANIMAT, V12, P1, DOI 10.1002/vis.244
   MULLER M., 2006, P VIRTUAL REALITY IN, P71, DOI [10.1007/978-3-319-08234-9_92-1, DOI 10.1007/978-3-319-08234-9_92-1]
   MULLER M., 2008, P VIRT REAL INT PHYS
   Rohmer D, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866183
   Schenk O, 2006, ELECTRON T NUMER ANA, V23, P158
   Terzopoulos D., 1987, COMPUT GRAPH, P205, DOI DOI 10.1145/37402.37427
   Thomaszewski B, 2009, COMPUT GRAPH FORUM, V28, P569, DOI 10.1111/j.1467-8659.2009.01397.x
   Umetani N, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964985
   VERLET L, 1967, PHYS REV, V159, P98, DOI 10.1103/PhysRev.159.98
   Volino P., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P137, DOI 10.1145/218380.218432
   Volino P, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1559755.1559762
   Wang HM, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866182
   Ye JT, 2008, COMPUT GRAPH FORUM, V27, P1901, DOI 10.1111/j.1467-8659.2008.01338.x
NR 43
TC 5
Z9 6
U1 0
U2 11
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2014
VL 30
IS 4
BP 431
EP 442
DI 10.1007/s00371-013-0866-5
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AD3SW
UT WOS:000333167500006
DA 2024-07-18
ER

PT J
AU Zhang, R
   Zhong, F
   Lin, LL
   Xing, GY
   Peng, QS
   Qin, XY
AF Zhang, Rui
   Zhong, Fan
   Lin, Lili
   Xing, Guanyu
   Peng, Qunsheng
   Qin, Xueying
TI Basis image decomposition of outdoor time-lapse videos
SO VISUAL COMPUTER
LA English
DT Article
DE Basis image decomposition; Outdoor scenes; Global illumination;
   Augmented reality
ID AUGMENTED REALITY; ILLUMINATION; SCENES
AB In augmented reality, it is essential that the rendered virtual objects are embedded harmonically into the view of the background scenes and their appearance should reflect the changing lighting condition of the real scene to ensure illumination consistency. In this paper, we propose a novel method to solve for the sunlight and skylight basis images of static outdoor scenes from a time-lapse image sequence. It is proved that the resulted basis images encapsulate the geometry and material reflectivity of the scene, correspond to the global illumination effects of the outdoor scene under a unit intensity of the sunlight and skylight. Our method is fully automatic. Unlike previous methods, it gets rid of the constraints that the reflectance of all objects in scenes should be ideal diffuse, or the weather condition should be overcast or sunshine. During decomposition, we first detect shadowed pixels by analyzing the time-lapse curve of each pixel through k-means clustering, the basis images of sunlight and skylight are then solved by an iterative procedure with the decomposition equation. The basis images are further optimized by exploiting their constraints and priors. Experimental results demonstrate the effectiveness and flexibility of the proposed method. Our method can also be applied in image understanding and compressing.
C1 [Zhang, Rui; Zhong, Fan; Lin, Lili; Qin, Xueying] Shandong Univ, Sch Comp Sci & Technol, Jinan 250100, Peoples R China.
   [Qin, Xueying] Shandong Prov Key Lab Network Based Intelligent C, Jinan, Peoples R China.
   [Zhang, Rui] Shandong Univ Finance & Econ, Jinan, Peoples R China.
   [Xing, Guanyu; Peng, Qunsheng] Zhejiang Univ, State Key Lab CAD&CG, Hangzhou 310003, Zhejiang, Peoples R China.
C3 Shandong University; Shandong University of Finance & Economics;
   Zhejiang University
RP Qin, XY (corresponding author), Shandong Prov Key Lab Network Based Intelligent C, Jinan, Peoples R China.
EM qxy@sdu.edu.cn
RI Qin, Xueying/AAM-8775-2021
OI Qin, Xueying/0000-0003-0057-295X
FU 973 program of China [2009CB320802]; NSF of China [U1035004, 61173070,
   61202149, 61272431]; Natural Science Fund for Distinguished Young
   Scholars of Shandong Province [JQ200920]
FX The authors gratefully acknowledge the anonymous reviewers for their
   comments to help us to improve our paper. This work is supported by 973
   program of China (No. 2009CB320802), NSF of China (No. U1035004, No.
   61173070, No. 61202149, No. 61272431), Natural Science Fund for
   Distinguished Young Scholars of Shandong Province (No. JQ200920).
CR Andersen MS, 2006, INT C PATT RECOG, P91
   [Anonymous], 1978, COMPUTER VISION SYST
   Debevec P., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P189, DOI 10.1145/280814.280864
   Debevec P. E., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P369, DOI 10.1145/258734.258884
   Kajiya J. T., 1986, SIGGRAPH, P143, DOI 10.1145/15886.15902
   Kimmel R, 2003, INT J COMPUT VISION, V52, P7, DOI 10.1023/A:1022314423998
   Lalonde JF, 2012, INT J COMPUT VISION, V98, P123, DOI 10.1007/s11263-011-0501-8
   Li YZ, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1366, DOI 10.1109/ICCV.2003.1238649
   Liu YL, 2012, IEEE T VIS COMPUT GR, V18, P573, DOI 10.1109/TVCG.2012.53
   Liu YL, 2009, VISUAL COMPUT, V25, P637, DOI 10.1007/s00371-009-0342-4
   Liu Y, 2010, STRUCT CHEM, V21, P21, DOI 10.1007/s11224-009-9519-8
   Madsen C., 2005, P 13 DAN C PATT REC
   Matsushita Y, 2004, LECT NOTES COMPUT SC, V3022, P274
   MORENONOGUER F, 2005, P IEE C VIS MED PROD, P201
   Qin X., 2012, P LECT NOTES COMPUTE
   Sato I., 2005, SYST COMPUT JPN, V36, P1864
   Sunkavalli K., 2008, Proc. CVPR, P1
   Sunkavalli K, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1276377.1276504, 10.1145/1239451.1239552]
   Tappen M., 2002, Advances in Neural Information Processing Systems, V15
   Wang Y, 2003, GRAPH MODELS, V65, P185, DOI 10.1016/S1524-0703(03)00043-2
   Xing GY, 2012, COMPUT GRAPH-UK, V36, P857, DOI 10.1016/j.cag.2012.07.005
   Yu YZ, 1999, COMP GRAPH, P215
NR 22
TC 2
Z9 5
U1 0
U2 12
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2013
VL 29
IS 11
BP 1197
EP 1210
DI 10.1007/s00371-013-0776-6
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 236PL
UT WOS:000325811300008
DA 2024-07-18
ER

PT J
AU Collin, C
   Ribardière, M
   Gruson, A
   Cozot, R
   Pattanaik, S
   Bouatouch, K
AF Collin, Charly
   Ribardiere, Mickael
   Gruson, Adrien
   Cozot, Remi
   Pattanaik, Sumanta
   Bouatouch, Kadi
TI Visibility-driven progressive volume photon tracing
SO VISUAL COMPUTER
LA English
DT Article
DE Rendering; Global illumination; Participating media; Markov chain Monte
   Carlo
AB In this paper, we present a novel approach to progressive photon-based volume rendering techniques. By making use of two Kd-trees (built in a preprocessing step) to store view beams (primary rays intersecting the medium) and visible points, our method allows to handle scenes with specular and refractive objects as well as homogeneous and heterogeneous participating media and does not require the storage of photon maps, which solves the memory management issue. These data structures are used to drive the photon shooting process by considering the photon visibility as an importance function (similarly to Hachisuka and Jensen in ACM Trans. Graph. 30(5):114:1-114:11, 2011) for scenes containing participating media. Finally, we demonstrate that our method can be easily combined with the most recent particle tracing approaches such as the one presented in Jarosz et al. (ACM Trans. Graph., vol. 30(6), 2011).
C1 [Collin, Charly; Pattanaik, Sumanta] Univ Cent Florida, Dept Elect Engn & Comp Sci, Orlando, FL 32816 USA.
   [Ribardiere, Mickael; Gruson, Adrien; Cozot, Remi; Bouatouch, Kadi] Univ Rennes 1, IRISA, Rennes, France.
C3 State University System of Florida; University of Central Florida;
   Universite de Rennes
RP Ribardière, M (corresponding author), Univ Rennes 1, IRISA, Rennes, France.
EM charly.collin@bobbyblues.com; mickael.ribardiere@irisa.fr;
   adrien.gruson@irisa.fr; remi.cozot@irisa.fr; sumant@cs.ucf.edu;
   kadi.bouatouch@irisa.fr
RI Gruson, Adrien/AAB-6971-2021
OI Ribardiere, Mickael/0000-0003-2964-2608
FU NSF [IIS-1064427]
FX Charly Collin has been supported in part by NSF grant IIS-1064427.
   Thanks go to the reviewers for their valuable reviews.
CR Chandrasekhar S., 1950, RAD TRANSFER
   Hachisuka T, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2019627.2019633
   Hachisuka T, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360632
   Havran Vlastimil., 2004, ACM SIGGRAPH 2004 Sketches, SIGGRAPH'04, P77
   Jarosz W., 2011, ACM T GRAP, V30
   Jarosz W, 2008, COMPUT GRAPH FORUM, V27, P557, DOI 10.1111/j.1467-8659.2008.01153.x
   Jensen H, 1998, TLS-TIMES LIT SUPPL, P25
   Jensen HW., 2001, REALISTIC IMAGE SYNT, DOI [10.1201/9780429294907, DOI 10.1201/9780429294907]
   Keller A., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P49, DOI 10.1145/258734.258769
   Knaus C, 2011, ACM T GRAPH, V30
   Novak J., 2012, P EGSR 2012 COMP GRA, V31
   Novák J, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185556
NR 12
TC 3
Z9 4
U1 0
U2 8
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2013
VL 29
IS 9
BP 849
EP 859
DI 10.1007/s00371-013-0845-x
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 186CB
UT WOS:000322018100001
DA 2024-07-18
ER

PT J
AU Liu, GD
   Choudhary, S
   Zhang, JZ
   Magenenat-Thalmann, N
AF Liu, Gengdai
   Choudhary, Shantanu
   Zhang, Juzheng
   Magenenat-Thalmann, Nadia
TI Let's keep in touch online: a Facebook aware virtual human interface
SO VISUAL COMPUTER
LA English
DT Article
DE Virtual humans; Social networking sites; Human-virtual human
   interaction; Dialogue
AB A virtual human is an effective interface for interacting with users and plays an important role in carrying out certain tasks. As social networking sites are getting more and more popular, we propose a Facebook aware virtual human. The social networking sites are used to empower virtual humans for interpersonal conversational interaction in this paper. We combine Internet world, physical world and 3D virtual world together to create a new interface for users to interact with an autonomous virtual human which can behave like a real modern human. In order to take advantages of social networking sites, virtual human gathers information of a user from its profile, its likes, dislikes and gauge mood from most recent status update. In two user studies, we investigated whether and how this new interface can enhance human-virtual human interaction. Some positive results concluded from these studies will be guidelines on research and development of future virtual human interfaces.
C1 [Liu, Gengdai; Choudhary, Shantanu; Zhang, Juzheng; Magenenat-Thalmann, Nadia] Nanyang Technol Univ, Inst Media Innovat, Singapore 639798, Singapore.
   [Liu, Gengdai] Xidian Univ, Xian, Peoples R China.
C3 Nanyang Technological University; Xidian University
RP Liu, GD (corresponding author), Nanyang Technol Univ, Inst Media Innovat, Singapore 639798, Singapore.
EM liugengdai@gmail.com; choudhary.shantanu@gmail.com; jzhang19@e.ntu.sg;
   nadiathalmann@ntu.edu.sg
OI Thalmann, Nadia/0000-0002-1459-5960
FU Singapore National Research Foundation under its International Research
   Centre @ Singapore Funding Initiative; National Natural Science
   Foundation of China [61003197]
FX This research, which is carried out at BeingThere Centre, is supported
   by the Singapore National Research Foundation under its International
   Research Centre @ Singapore Funding Initiative and administered by the
   IDM Programme Office. The authors would like to thank all researchers
   and staffs in IMI-NTU for their efforts on development of the virtual
   human platform and discussions about this work. The first author is
   partially supported by National Natural Science Foundation of China
   (Grant No: 61003197).
CR Ahonen T, 2006, IEEE T PATTERN ANAL, V28, P2037, DOI 10.1109/TPAMI.2006.244
   Andrist S., 2012, Proceedings of the International Conference on Human Factors in Computing, CHI '12, P705
   ARON A, 1992, J PERS SOC PSYCHOL, V63, P596, DOI 10.1037/0022-3514.63.4.596
   Babu SV, 2011, IEEE T VIS COMPUT GR, V17, P14, DOI 10.1109/TVCG.2009.211
   Bailenson JN, 2011, IEEE SPECTRUM, V48, P78, DOI 10.1109/MSPEC.2011.5779797
   Bickmore T., 2001, CHI 2001 Conference Proceedings. Conference on Human Factors in Computing Systems, P396, DOI 10.1145/365024.365304
   Daugherty B, 2008, IEEE COMPUT GRAPH, V28, P65, DOI 10.1109/MCG.2008.125
   de Melo C., 2010, 10 INT C INT VIRT AG, P257
   Emeli V., 2010, IEEE RO MAN, P288
   Garau Maia, 2003, P SIGCHI C HUM FACT, P529, DOI DOI 10.1145/642611.642703
   Gilbert E, 2009, CHI2009: PROCEEDINGS OF THE 27TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1-4, P211
   Gobron S, 2010, VISUAL COMPUT, V26, P505, DOI 10.1007/s00371-010-0446-x
   Groenegress C, 2010, VISUAL COMPUT, V26, P649, DOI 10.1007/s00371-010-0471-9
   Isla D., 2008, GAM DEV C
   Jianqiang DS, 2011, 29TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P581
   Kasap Z, 2012, VISUAL COMPUT, V28, P87, DOI 10.1007/s00371-011-0630-7
   Kasap Z, 2009, IEEE COMPUT GRAPH, V29, P20, DOI 10.1109/MCG.2009.26
   Kotranza A, 2009, 3DUI : IEEE SYMPOSIUM ON 3D USER INTERFACES 2009, PROCEEDINGS, P23
   Kramer ADI, 2012, P SIGCHI C HUM FACT, P767, DOI DOI 10.1145/2207676.2207787
   Krumhuber E., 2005, 11 EUR C FAC EXPR ME, P730
   Ma X., 2012, P 10 AS PAC C COMP H, P133, DOI [10.1145/2350046.2350076, DOI 10.1145/2350046.2350076]
   Markowitz Daniel, 2011, Motion in Games. Proceedings 4th International Conference, MIG 2011, P156, DOI 10.1007/978-3-642-25090-3_14
   Mavridis N., 2009, IEEE HUM ROB INT, P273
   Neff M, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1330511.1330516
   Prendinger H, 2011, IEEE T VIS COMPUT GR, V17, P655, DOI 10.1109/TVCG.2010.66
   Raij AB, 2007, IEEE T VIS COMPUT GR, V13, P443, DOI [10.1109/TVCG.2007.1036, 10.1109/TVCG.2007.1030]
   Saerbeck M, 2010, CHI2010: PROCEEDINGS OF THE 28TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1-4, P1613
   Swartout W, 2006, AI MAG, V27, P96
   Szafir D, 2012, P SIGCHI C HUM FACT, P11, DOI DOI 10.1145/2207676.2207679
   Takahashi T, 2005, INT J HUM-COMPUT ST, V62, P193, DOI 10.1016/j.ijhcs.2004.11.005
   Tang J, 2012, IEEE T AFFECT COMPUT, V3, P132, DOI 10.1109/T-AFFC.2011.23
   Toma CL, 2010, CHI2010: PROCEEDINGS OF THE 28TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1-4, P1749
   Wang N, 2010, CHI2010: PROCEEDINGS OF THE 28TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1-4, P1241
   Yamazaki A, 2008, CHI 2008: 26TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS VOLS 1 AND 2, CONFERENCE PROCEEDINGS, P131
   Yonezawa K., 2009, Proceedings of the International Conference on Advances in Computer Enterntainment Technology, P149
   Zanbaka C, 2007, CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1 AND 2, P1561
   Zhao X., 2012, CHI, V2012, P771, DOI [DOI 10.1145/2207676.2207788, 10.1145/2207676.2207788]
NR 37
TC 4
Z9 4
U1 4
U2 50
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2013
VL 29
IS 9
BP 871
EP 881
DI 10.1007/s00371-013-0846-9
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 186CB
UT WOS:000322018100003
DA 2024-07-18
ER

PT J
AU Wang, ML
   Chang, J
   Kerber, J
   Zhang, JJ
AF Wang, Meili
   Chang, Jian
   Kerber, Jens
   Zhang, Jian J.
TI A framework for digital sunken relief generation based on 3D geometric
   models
SO VISUAL COMPUTER
LA English
DT Article
DE Sunken relief; Line drawings; 3D modeling; Digital art
ID RANGE; SHAPE
AB Sunken relief is a special art form of sculpture whereby the depicted shapes are sunk into a given surface. This is traditionally created by laboriously carving materials such as stone. Sunken reliefs often utilize the engraved lines or strokes to strengthen the impressions of a 3D presence and to highlight the features which otherwise are unrevealed. In other types of relief, smooth surfaces and their shadows convey such information in a coherent manner. Existing methods for relief generation are focused on forming a smooth surface with a shallow depth which provides the presence of 3D figures. Such methods unfortunately do not help the art form of sunken reliefs as they omit the presence of feature lines. We propose a framework to produce sunken reliefs from a known 3D geometry, which transforms the 3D objects into three layers of input to incorporate the contour lines seamlessly with the smooth surfaces. The three input layers take the advantages of the geometric information and the visual cues to assist the relief generation. We have modified the existing techniques of line drawings and relief generation, and then combine them organically for this particular purpose.
C1 [Wang, Meili; Chang, Jian; Zhang, Jian J.] Bournemouth Univ, Media Sch, Poole BH12 5BB, Dorset, England.
   [Kerber, Jens] Univ Saarland, MPI Informat, D-6600 Saarbrucken, Germany.
C3 Bournemouth University; Saarland University; Max Planck Society
RP Zhang, JJ (corresponding author), Bournemouth Univ, Media Sch, Poole BH12 5BB, Dorset, England.
EM jzhang@bournemouth.ac.uk
OI Zhang, Jian/0000-0002-7069-5771; Chang, Jian/0000-0003-4118-147X
CR Agrawal A, 2006, LECT NOTES COMPUT SC, V3951, P578
   Alexa M, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778797
   [Anonymous], 2 INT C COMP ENG TEC
   [Anonymous], RELIEF SCULPTURE
   [Anonymous], SIGGRAPH 2008
   [Anonymous], 2010, REPORTS MONOGRAPHS I
   [Anonymous], WEIGHTING LAPLACIAN
   Belhumeur PN, 1999, INT J COMPUT VISION, V35, P33, DOI 10.1023/A:1008154927611
   Cignoni P., 1997, Journal of Graphics Tools, V2, P15, DOI 10.1080/10867651.1997.10487476
   Cole F, 2008, ACM T GRAPHIC, V27, DOI [10.1145/1360612.1360657, 10.1145/1360612.1360687]
   Cole F, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531334
   DeCarlo D, 2003, ACM T GRAPHIC, V22, P848, DOI 10.1145/882262.882354
   DeCarlo Doug., 2004, P INT S NONPHOTOREAL, P15, DOI DOI 10.1145/987657.987661
   Fattal R, 2002, ACM T GRAPHIC, V21, P249
   Flaxman J., 1829, LECT SCULPTURE
   Kalnins RD, 2002, ACM T GRAPHIC, V21, P755, DOI 10.1145/566570.566648
   Kerber J., 2010, COMPUT AIDED DES APP, V7, P465, DOI DOI 10.3722/CADAPS.2010.465-478
   Kerber J, 2007, P 23 SPRING C COMP G, P110
   Kerber J, 2009, SMI 2009: IEEE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, PROCEEDINGS, P148, DOI 10.1109/SMI.2009.5170176
   KOENDERINK JJ, 1984, PERCEPTION, V13, P321, DOI 10.1068/p130321
   Markosian L., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P415, DOI 10.1145/258734.258894
   Oliveira MM, 2000, COMP GRAPH, P359, DOI 10.1145/344779.344947
   Pasko A, 2001, COMPUT AIDED DESIGN, V33, P379, DOI 10.1016/S0010-4485(00)00129-9
   Perry RN, 2001, COMP GRAPH, P47, DOI 10.1145/383259.383264
   Raskar Ramesh., 2001, Proceedings of the ACM SIGGRAPH/EUROGRAPHICS workshop on Graphics hardware, HWWS '01, P41, DOI DOI 10.1145/383507.383525
   Song WH, 2007, IEEE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS 2007, PROCEEDINGS, P211, DOI 10.1109/SMI.2007.9
   Sourin A, 2001, VISUAL COMPUT, V17, P258, DOI 10.1007/s003710100109
   Sousa MC, 2003, COMPUT GRAPH FORUM, V22, P381, DOI 10.1111/1467-8659.00685
   Sun XF, 2009, IEEE T VIS COMPUT GR, V15, P642, DOI 10.1109/TVCG.2009.21
   Weyrich T, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239483
   Wu T. P., 2008, SIGGRAPH ASIA 08, P1
   Zeng G, 2005, PROC CVPR IEEE, P343
   Zhang R, 1999, IEEE T PATTERN ANAL, V21, P690, DOI 10.1109/34.784284
NR 33
TC 6
Z9 8
U1 2
U2 8
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2012
VL 28
IS 11
BP 1127
EP 1137
DI 10.1007/s00371-011-0663-y
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 021JO
UT WOS:000309881300006
DA 2024-07-18
ER

PT J
AU Lever, J
   Komura, T
AF Lever, Jake
   Komura, Taku
TI Real-time controllable fire using textured forces
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Conference on the Computer Graphics International (CGI)
CY 2012
CL Bournemouth, ENGLAND
DE Fire animation; Fluid simulation
ID SMOKE; SIMULATION; ANIMATION
AB Fluid dynamics can produce realistic looking fire effects, which are heavily used in animation and films. However, the parameters of the various underlying physical equations are not intuitive enough to be controlled easily. As a result, animators face problems when editing the fine details of the fire, especially the turbulence and growth at the fire surface. In this paper, we propose a new approach to enable animators to interactively edit such fine details using textured forces. These techniques involve mapping a texture onto the simulation that controls the creation of new forces, growing the fire into specific shape and adding the natural turbulence of fuel ignition. These textures can be edited using an intuitive user interface that allows forces to be painted directly onto the fire. Our system can be integrated into existing GPU fluid solvers to run in real-time. As a result, it is applicable for interactive applications such as 3D computer games.
C1 [Lever, Jake; Komura, Taku] Univ Edinburgh, Sch Informat, Edinburgh, Midlothian, Scotland.
C3 University of Edinburgh
RP Komura, T (corresponding author), Univ Edinburgh, Sch Informat, 10 Crichton St, Edinburgh, Midlothian, Scotland.
EM jake.lever@gmail.com; tkomura@ed.ac.uk
OI Lever, Jake/0000-0001-8198-2939
FU EPSRC [EP/H012338/1] Funding Source: UKRI
CR [Anonymous], P ACM SIGGRAPH EUR S
   [Anonymous], 2004, GPU gems
   Beaudoin P., 2001, P GRAPH INTERFACE, P159
   CHIBA N, 1994, J VISUAL COMP ANIMAT, V5, P37, DOI 10.1002/vis.4340050104
   Crane K., 2007, GPU GEMS, P633
   Ebert David S, 2003, Texturing Modeling: A Procedural Approach
   Fattal R., 2004, ACM T GRAPH, V23
   Fedkiw R., 2001, P SIGGRAPH 01
   Foster N., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P181, DOI 10.1145/258734.258838
   Foster N, 1997, COMPUTER GRAPHICS INTERNATIONAL, PROCEEDINGS, P178, DOI 10.1109/CGI.1997.601299
   Foster N, 2001, COMP GRAPH, P23, DOI 10.1145/383259.383261
   Foundation B., 2009, BLEND 2 48
   Hong J.-M., 2007, P SIGGRAPH 07
   Horvath C, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531347
   IKITS M., 2004, GPU GEMS, P667
   Kim T, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360649
   Kwatra V, 2007, IEEE T VIS COMPUT GR, V13, P939, DOI 10.1109/TVCG.2007.1044
   Lamorlette A, 2002, ACM T GRAPHIC, V21, P729, DOI 10.1145/566570.566644
   Lorensen W. E., 1987, COMPUTER GRAPHICS, V21, P163, DOI 10.1145/37401.37422
   McNamara A, 2004, ACM T GRAPHIC, V23, P449, DOI 10.1145/1015706.1015744
   Narain R, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409119
   Nguyen D.Q., 2002, P SIGGRAPH 02
   NVIDIA, 2009, CUDA GOOD PROGR GUID
   Perry C.H., 1994, P 5 EUROGRAPHICS WOR, P105
   REEVES WT, 1983, ACM T GRAPHIC, V2, P91, DOI 10.1145/964967.801167
   Schechter H., 2008, P 2008 ACM EUR S COM
   Shi L, 2005, ACM T GRAPHIC, V24, P140, DOI 10.1145/1037957.1037965
   Stam J., 1993, Computer Graphics Proceedings, P369, DOI 10.1145/166117.166163
   Stam J., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P129, DOI 10.1145/218380.218430
   Stam J., 1999, P SIGGRAPH 99
   Takahashi JY, 1997, IEICE T INF SYST, VE80D, P1102
   Todo H, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1276377.1276399, 10.1145/1239451.1239468]
   Treuille A, 2003, ACM T GRAPHIC, V22, P716, DOI 10.1145/882262.882337
NR 33
TC 7
Z9 7
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2012
VL 28
IS 6-8
BP 691
EP 700
DI 10.1007/s00371-012-0684-1
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 947EW
UT WOS:000304411500017
DA 2024-07-18
ER

PT J
AU Liang, RH
   Wu, YF
   Dong, F
   Clapworthy, G
AF Liang, Ronghua
   Wu, Yunfei
   Dong, Feng
   Clapworthy, Gordon
TI Accumulation of local maximum intensity for feature enhanced volume
   rendering
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Conference on the Computer Graphics International (CGI)
CY 2012
CL Bournemouth, ENGLAND
DE Feature enhancement; Moving least squares; Local minimum-point
   searching; Local intensity difference accumulation; Tone reduction
ID CLASSIFICATION
AB Maximum Intensity Difference Accumulation (MIDA) combines the advantage of Direct Volume Rendering (DVR) and Maximum Intensity Projection (MIP). However, many features with local maximum intensity are still missing in the final rendering image. This paper presents a novel approach to focus on features with local maximum intensity within the dataset. Moving Least Squares (MLS) is used to smooth each ray profile during the raycasting in order to eliminate noise in the data and to highlight significant transition points on the profile. We then adopt a local minimum-point searching method to analyze the ray profile, and identify the transition points that mark the local maximum intensity points within the dataset. At the rendering stage, we implement a novel local intensity difference accumulation (LIDA) to accumulate the colors and opacity. Surface shading is introduced to improve the spatial cues of the features. We also employ tone-reduction to preserve the original local contrast. Our approach can highlight local features in the dataset without involving the adjustment of transfer functions. The experiments demonstrate high-quality rendering results at an interactive frame rate.
C1 [Liang, Ronghua; Wu, Yunfei] Zhejiang Univ Technol, Coll Comp Sci & Technol, Hangzhou, Zhejiang, Peoples R China.
   [Dong, Feng; Clapworthy, Gordon] Univ Bedfordshire, Dept Comp & Informat Syst, Luton, Beds, England.
   [Clapworthy, Gordon] Univ Bedfordshire, IRAC, Luton, Beds, England.
C3 Zhejiang University of Technology; University of Bedfordshire;
   University of Bedfordshire
RP Wu, YF (corresponding author), Zhejiang Univ Technol, Coll Comp Sci & Technol, Hangzhou, Zhejiang, Peoples R China.
EM ronghualiang2010@gmail.com; yunfei_james@126.com; Fong.Dong@beds.ac.uk;
   Gordon.Clapworthy@beds.ac.uk
RI liang, ronghua/H-4463-2012
CR Bruckner S, 2009, IEEE T VIS COMPUT GR, V15, P1497, DOI 10.1109/TVCG.2009.121
   Bruckner S, 2009, COMPUT GRAPH FORUM, V28, P775, DOI 10.1111/j.1467-8659.2009.01474.x
   Cheng Z.Q., 2008, VGPBG SIGGRAPH, P9
   Correa CD, 2011, IEEE T VIS COMPUT GR, V17, P192, DOI 10.1109/TVCG.2010.35
   Correa CD, 2009, IEEE T VIS COMPUT GR, V15, P1465, DOI 10.1109/TVCG.2009.189
   Correa CD, 2009, IEEE PAC VIS SYMP, P177, DOI 10.1109/PACIFICVIS.2009.4906854
   Diaz J, 2010, IEEE EG S VOL GRAPH, DOI [10.2312/VG/VG10/093-100, DOI 10.2312/VG/VG10/093-100]
   Drago F, 2003, COMPUT GRAPH FORUM, V22, P419, DOI 10.1111/1467-8659.00689
   Heidrich W, 1995, VISUALIZATION '95 - PROCEEDINGS, P11, DOI 10.1109/VISUAL.1995.480790
   Kindlmann G, 1998, IEEE SYMPOSIUM ON VOLUME VISUALIZATION, P79, DOI 10.1109/SVV.1998.729588
   Kniss J, 2002, IEEE T VIS COMPUT GR, V8, P270, DOI 10.1109/TVCG.2002.1021579
   Levoy M., 1992, Proceedings. Graphics Interface '92, P61
   Malik Muhammad Muddassir, 2007, Proceedings Graphics Interface 2007, P273, DOI 10.1145/1268517.1268562
   MALZBENDER T, 1993, ACM T GRAPHIC, V12, P233, DOI 10.1145/169711.169705
   MAX N, 1995, IEEE T VIS COMPUT GR, V1, P99, DOI 10.1109/2945.468400
   Meyer-Spradow J, 2009, IEEE COMPUT GRAPH, V29, P6, DOI 10.1109/MCG.2009.130
   Rezk-Salama C, 2006, COMPUT GRAPH FORUM, V25, P597, DOI 10.1111/j.1467-8659.2006.00979.x
   Sato Y, 1998, J COMPUT ASSIST TOMO, V22, P912, DOI 10.1097/00004728-199811000-00014
   Subramanian N., 2008, P ACM SIGGRAPH POST, P1
   Tzeng FY, 2005, IEEE T VIS COMPUT GR, V11, P273, DOI 10.1109/TVCG.2005.38
   Viola I, 2004, IEEE VISUALIZATION 2004, PROCEEEDINGS, P139, DOI 10.1109/VISUAL.2004.48
   Viola I, 2005, IEEE T VIS COMPUT GR, V11, P408, DOI 10.1109/TVCG.2005.62
   WALLIS JW, 1989, IEEE T MED IMAGING, V8, P297, DOI 10.1109/42.41482
   Wan Y., 2010, 8 IEEE EG INT S VOL, P61
   Zhou ZG, 2011, VISUAL COMPUT, V27, P677, DOI 10.1007/s00371-011-0570-2
NR 25
TC 6
Z9 10
U1 0
U2 10
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2012
VL 28
IS 6-8
BP 625
EP 633
DI 10.1007/s00371-012-0680-5
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 947EW
UT WOS:000304411500011
DA 2024-07-18
ER

PT J
AU Shi, JJ
   Zhu, DM
   Zhang, YP
   Wang, ZQ
AF Shi, Jinjin
   Zhu, Dengming
   Zhang, Yingping
   Wang, Zhaoqi
TI Realistically rendering polluted water
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Conference on the Computer Graphics International (CGI)
CY 2012
CL Bournemouth, ENGLAND
DE Polluted water; Realistic rendering; Bio-optical model; Volumetric
   photon mapping; 3D variable kernel
ID DISSOLVED ORGANIC-MATTER; LAKE; ANIMATION; TAIHU; OCEAN
AB Polluted water is very common in our world. Vividly rendering polluted water can bring people real, different, and fancy feelings. Especially in under water imagery, taking polluted water into consideration will produce more plausible results. Polluted water consists of many kinds of pollutants, which interact with light differently and make water look turbid. The optical properties of polluted water change with the concentrations of pollutants significantly. In this paper, we provide a method to obtain the optical properties of polluted water, which makes a bio-optical model for polluted water and connects the optical parameters with water quality data, i.e., the concentrations of pollutants. Our method can estimate the optical properties of polluted water regardless of the kinds and the concentrations of pollutants in water. Polluted water is inhomogeneous and has multiple scattering effects. We use volumetric photon mapping to render it and provide a 3D weight-varying radiance estimate method for the photon mapping. This radiance estimate method can compute high-frequency effects easily, which can show more details of the pollution process. Experiments demonstrate that our approach can generate polluted water effects unachievable by standard rendering methods.
C1 [Shi, Jinjin; Zhang, Yingping] Chinese Acad Sci, Inst Comp Technol, Virtual Real Lab, Beijing, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Computing Technology, CAS
RP Shi, JJ (corresponding author), Chinese Acad Sci, Inst Comp Technol, Virtual Real Lab, Beijing, Peoples R China.
EM shijinjin@ict.ac.cn
CR Cerezo E, 2002, ADVANCES IN MODELLING, ANIMATION AND RENDERING, P481
   Chen J, 2010, SPECTROSC SPECT ANAL, V30, P137, DOI 10.3964/j.issn.1000-0593(2010)01-0137-05
   Duan Hongtao, 2009, Hupo Kexue, V21, P242
   Enright D, 2002, ACM T GRAPHIC, V21, P736, DOI [10.1145/566570.566581, 10.1145/566570.566645]
   Ferrari GM, 2000, MAR CHEM, V70, P339, DOI 10.1016/S0304-4203(00)00036-0
   Foster N, 1996, GRAPH MODEL IM PROC, V58, P471, DOI 10.1006/gmip.1996.0039
   Fournier A., 1986, Computer Graphics, V20, P75, DOI 10.1145/15886.15894
   GORDON HR, 1992, APPL OPTICS, V31, P2116, DOI 10.1364/AO.31.002116
   Gutierrez D, 2008, COMPUT GRAPH FORUM, V27, P547, DOI 10.1111/j.1467-8659.2008.01152.x
   Hardle W., 2004, NONPARAMETRIC SEMIPA, DOI DOI 10.1007/978-3-642-17146-8
   Iglesias A, 2004, FUTURE GENER COMP SY, V20, P1355, DOI 10.1016/j.future.2004.05.026
   Iwasaki K, 2001, NINTH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P374, DOI 10.1109/PCCGA.2001.962894
   Jakob W, 2011, COMPUT GRAPH FORUM, V30, P1287, DOI 10.1111/j.1467-8659.2011.01988.x
   Jarosz W, 2008, COMPUT GRAPH FORUM, V27, P557, DOI 10.1111/j.1467-8659.2008.01153.x
   Jensen H. W., 1996, Rendering Techniques '96. Proceedings of the Eurographics Workshop. Eurographics, P21
   JENSEN H. W, 1996, THESIS TU DENMARK LY
   Jensen H.W., 2000, SIGGRAPH 2000 COURSE, V8
   JENSEN H.W., 1998, SIGGRAPH 98 Conference Proceedings, Annual Conference Series, P311
   KANEDA K, 1991, SECOND INTERNATIONAL CONFERENCE ON COMPUTER-AIDED DESIGN & COMPUTER GRAPHICS, P25
   Kass M., 1990, Computer Graphics, V24, P49, DOI 10.1145/97880.97884
   [乐成峰 LE Cheng-feng], 2009, [水科学进展, Advances in Water Science], V20, P707
   Ma Ronghua, 2009, Hupo Kexue, V21, P143
   Muller M., 2003, ACM SIGGRAPH/Eurographics Symposium on Computer Animation, P154
   O'Brien J. F., 1995, Proceedings. Computer Animation '95, P198, DOI 10.1109/CA.1995.393532
   Pharr M, 2010, EXTENDED PHOTON MAP
   Pope RM, 1997, APPL OPTICS, V36, P8710, DOI 10.1364/AO.36.008710
   Premoze S, 2001, COMPUT GRAPH FORUM, V20, P189, DOI 10.1111/1467-8659.00548
   Schjoth L, 2007, COMM COM INF SC, V4, P109
   SMITH RC, 1981, APPL OPTICS, V20, P177, DOI 10.1364/AO.20.000177
   [孙德勇 SUN Deyong], 2008, [地理与地理信息科学, Geography and Geo-information Science], V24, P16
   TESSENDORF J, 2001, SIGGRAPH 2001 COURSE, V47
   Zhang YL, 2007, HYDROBIOLOGIA, V581, P43, DOI 10.1007/s10750-006-0520-6
   [张运林 ZHANG Yunlin], 2007, [水科学进展, Advances in Water Science], V18, P415
NR 33
TC 6
Z9 8
U1 1
U2 29
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2012
VL 28
IS 6-8
BP 647
EP 656
DI 10.1007/s00371-012-0685-0
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 947EW
UT WOS:000304411500013
DA 2024-07-18
ER

PT J
AU Marini, S
   Patané, G
   Spagnuolo, M
   Falcidieno, B
AF Marini, S.
   Patane, G.
   Spagnuolo, M.
   Falcidieno, B.
TI Spectral feature selection for shape characterization and classification
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 3rd Eurographics Workshop on 3D Object Retrieval
CY MAY 02, 2010
CL Norrkoping, SWEDEN
DE Shape characterization; Feature selection; Shape classification; Point
   clouds; Laplacian spectrum
ID RETRIEVAL
AB This paper proposes a framework for selecting the Laplacian eigenvalues of 3D shapes that are more relevant for shape characterization and classification. We demonstrate the redundancy of the information coded by the shape spectrum and discuss the shape characterization capability of the selected eigenvalues. The feature selection methods used to demonstrate our claim are the AdaBoost algorithm and Support Vector Machine. The efficacy of the selection is shown by comparing the results of the selected eigenvalues on shape characterization and classification with those related to the first k eigenvalues, by varying k over the cardinality of the spectrum. Our experiments, which have been performed on 3D objects represented either as triangle meshes or point clouds, show that working directly with point clouds provides classification results that are comparable with respect to those related to surface-based representations. Finally, we discuss the stability of the computation of the Laplacian spectrum to matrix perturbations.
C1 [Marini, S.; Patane, G.; Spagnuolo, M.; Falcidieno, B.] CNR IMATI, Genoa, Italy.
C3 Consiglio Nazionale delle Ricerche (CNR); Istituto di Matematica
   Applicata e Tecnologie Informatiche "Enrico Magenes" (IMATI-CNR)
RP Marini, S (corresponding author), CNR IMATI, Via Marini 6, Genoa, Italy.
EM marini@ge.imati.cnr.it; patane@ge.imati.cnr.it;
   spagnuolo@ge.imati.cnr.it; falcidieno@ge.imati.cnr.it
RI Spagnuolo, Michela/F-5068-2013; Spagnuolo, Michela/ABA-1927-2021;
   Marini, Simone/C-3872-2012; Patane', Giuseppe/O-1322-2013
OI Spagnuolo, Michela/0000-0002-5682-6990; Spagnuolo,
   Michela/0000-0002-5682-6990; Marini, Simone/0000-0003-0665-7815;
   Patane', Giuseppe/0000-0002-2276-9553
CR Adamson A., 2003, Symposium on Geometry Processing, P230
   Alexander M, 2001, INTERNETWEEK, P21
   Amenta N, 2004, ACM T GRAPHIC, V23, P264, DOI 10.1145/1015706.1015713
   [Anonymous], IEEE VISUALIZATION
   [Anonymous], 2006, STUDIES FUZZINESS SO
   [Anonymous], ACM SIGGRAPH
   [Anonymous], P COMP LEARN THEOR
   [Anonymous], SIBGRAPI
   [Anonymous], SHAPE RETRIEVAL CONT
   [Anonymous], J COMPUT AIDED DES A
   [Anonymous], WATERTIGHT MODELS TR
   [Anonymous], ACM SIGGRAPH
   [Anonymous], P COMP GRAPH INT TEC
   [Anonymous], EUR WORKSH 3D OBJ RE
   [Anonymous], IEEE SHAPE MODELING
   Arya S, 1998, J ACM, V45, P891, DOI 10.1145/293347.293348
   Attene M, 2010, COMPUT GRAPH FORUM, V29, P1905, DOI 10.1111/j.1467-8659.2010.01658.x
   Belkin M, 2003, NEURAL COMPUT, V15, P1373, DOI 10.1162/089976603321780317
   Belkin M, 2006, J MACH LEARN RES, V7, P2399
   Belkin M, 2009, PROCEEDINGS OF THE TWENTIETH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1031
   Ben-Hur A, 2008, PLOS COMPUT BIOL, V4, DOI 10.1371/journal.pcbi.1000173
   Biasotti Silvia., 2007, Eurographics 2007 tutorial proceedings, P949
   Boser B. E., 1992, Proceedings of the Fifth Annual ACM Workshop on Computational Learning Theory, P144, DOI 10.1145/130385.130401
   Bronstein Alexander M, 2010, P EUROGRAPHICS WORKS
   Bustos B, 2005, ACM COMPUT SURV, V37, P345, DOI 10.1145/1118890.1118893
   Chen DY, 2003, COMPUT GRAPH FORUM, V22, P223, DOI 10.1111/1467-8659.00669
   Chung F. R. K., 1997, Spectral graph theory
   Coifman RR, 2006, APPL COMPUT HARMON A, V21, P5, DOI 10.1016/j.acha.2006.04.006
   Fleishman S, 2003, ACM T GRAPHIC, V22, P997, DOI 10.1145/944020.944023
   Guyon I, 2002, MACH LEARN, V46, P389, DOI 10.1023/A:1012487302797
   Hou S, 2006, LECT NOTES COMPUT SC, V4306, P124
   Jain V, 2007, COMPUT AIDED DESIGN, V39, P398, DOI 10.1016/j.cad.2007.02.009
   Kalaiah A, 2003, IEEE T VIS COMPUT GR, V9, P30, DOI 10.1109/TVCG.2003.1175095
   Lafon S, 2006, IEEE T PATTERN ANAL, V28, P1784, DOI 10.1109/TPAMI.2006.223
   Laga Hamid., 2008, J SOC ART SCI, V7, P124
   Lange C, 2005, COMPUT AIDED GEOM D, V22, P680, DOI 10.1016/j.cagd.2005.06.010
   Levin David., 2003, GEOMETRIC MODELING S, P37
   Mahmoudi M, 2009, GRAPH MODELS, V71, P22, DOI 10.1016/j.gmod.2008.10.002
   Marini S, 2007, IEEE COMPUT GRAPH, V27, P28, DOI 10.1109/MCG.2007.89
   Mateus D., 2007, IEEE INT C COMPUTER, P1
   Mohar Bojan., 1993, Combinatorial and graph-theoretical problems in linear algebra, P107, DOI [DOI 10.1007/978-1-4613-8354-3_5, DOI 10.1007/978-1-4613-8354-3]
   Ohbuchi R., 2006, INT MULTIMEDIA C ACM, P163
   Osada R, 2002, ACM T GRAPHIC, V21, P807, DOI 10.1145/571647.571648
   Pauly M, 2006, ACM T GRAPHIC, V25, P177, DOI 10.1145/1138450.1138451
   Reuter M, 2006, COMPUT AIDED DESIGN, V38, P342, DOI 10.1016/j.cad.2005.10.011
   Reuter M, 2009, COMPUT GRAPH-UK, V33, P381, DOI 10.1016/j.cag.2009.03.005
   Rusinkiewicz S, 2000, COMP GRAPH, P343, DOI 10.1145/344779.344940
   Rustamov Raif M, 2007, P S GEOM PROC, V257, P225
   Schapire RE, 1999, IJCAI-99: PROCEEDINGS OF THE SIXTEENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, VOLS 1 & 2, P1401
   Shilane P, 2006, IEEE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS 2006, PROCEEDINGS, P108
   Tangelder JH, 2008, MULTIMED TOOLS APPL, V39, P441, DOI 10.1007/s11042-007-0181-0
   Tenenbaum JB, 2000, SCIENCE, V290, P2319, DOI 10.1126/science.290.5500.2319
   Tieu K, 2004, INT J COMPUT VISION, V56, P17, DOI 10.1023/B:VISI.0000004830.93820.78
   Vallet B, 2008, COMPUT GRAPH FORUM, V27, P251, DOI 10.1111/j.1467-8659.2008.01122.x
   Xie H, 2003, IEEE VISUALIZATION 2003, PROCEEDINGS, P91, DOI 10.1109/VISUAL.2003.1250359
   Zwicker M, 2002, IEEE T VIS COMPUT GR, V8, P223, DOI 10.1109/TVCG.2002.1021576
NR 56
TC 10
Z9 10
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2011
VL 27
IS 11
SI SI
BP 1005
EP 1019
DI 10.1007/s00371-011-0612-9
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 831MW
UT WOS:000295736400006
DA 2024-07-18
ER

PT J
AU Seo, S
   Yoon, K
AF Seo, SangHyun
   Yoon, KyungHyun
TI Color juxtaposition for pointillism based on an artistic color model and
   a statistical analysis
SO VISUAL COMPUTER
LA English
DT Article
DE Non-photorealistic rendering; Painterly rendering; Pointillism;
   Juxtaposition of color
AB We analyze characteristics and patterns of color juxtaposition and the color theory used by pointillist painters and employ them to create rendering algorithm that generates images in a pointillist style. We determine the distribution of colors in several paintings by Seurat and quantify a number of the theory that he employs. Using an RYB color wheel and a hierarchical point generation technique, we convert an input image into a set of colored dots, which is again converted into brush strokes with appropriate shapes and directions. We present images which illustrate the extent to which we have managed to simulate Seurat's technique.
C1 [Seo, SangHyun; Yoon, KyungHyun] Chung Ang Univ, Seoul 156756, South Korea.
C3 Chung Ang University
RP Yoon, K (corresponding author), Chung Ang Univ, Seoul 156756, South Korea.
EM shseo@cglab.cau.ac.kr; khyoon@cau.ac.kr
FU Korea government (MEST) [2009-0083169]
FX This work was supported by the Korea Science and Engineering Foundation
   (KOSEF) grant funded by the Korea government (MEST) (No. 2009-0083169).
CR [Anonymous], 2002, P 2 INT S NONPH AN R, DOI DOI 10.1145/508535.508537
   Badamchizadeh M. A., 2004, Proceedings. Third International Conference on Image and Graphics, P27
   Blanc Charles., 1874, The Grammar of Painting and Engraving
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851
   Chevreul MichelE., 1987, PRINCIPLES HARMONY C
   Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236
   GAGE J, 1973, COLOR CULTURE
   Gonzalez R. C., 2006, PEARSON ED INDIA, V3rd
   Gossett N, 2004, IEEE SYMPOSIUM ON INFORMATION VISUALIZATION 2004, PROCEEDINGS, P113, DOI 10.1109/INFVIS.2004.52
   Hauptman J., 2007, G SEURAT DRAWINGS
   Hays J., 2004, PROC NPAR 01, P113
   Hertzmann A., 1998, Proceedings of the 25th Annual Conference on Computer Graphics and Interactive Techniques, P453
   HILLER S, 2003, P EUROGRAPHICS, P515
   Hogg R. V., 2006, INTRO MATH STAT
   JING L, 2005, P ISVC2005, P1
   Kang H, 2007, NPAR 2007: 5TH INTERNATIONAL SYMPOSIUM ON NON-PHOTOREALISTIC ANIMATION AND RENDERING, PROCEEDINGS, P43
   Kopf J, 2006, ACM T GRAPHIC, V25, P509, DOI 10.1145/1141911.1141916
   Lagae A, 2008, COMPUT GRAPH FORUM, V27, P114, DOI 10.1111/j.1467-8659.2007.01100.x
   Luong T.-Q., 2005, Proceedings of Graphics Interface 2005, GI'05, P233
   RAHMAN DJZ, 1996, MULTISCALE RETINEX C
   Rood OgdenN., 1973, MODERN CHROMATICS
   Seo S. H., 2009, P COMP AESTH, P9
   SMITH P, 1997, SEURAT AVANT GRADE
   Yang CK, 2008, VISUAL COMPUT, V24, P303, DOI 10.1007/s00371-007-0183-y
NR 24
TC 9
Z9 9
U1 3
U2 15
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2010
VL 26
IS 6-8
BP 421
EP 431
DI 10.1007/s00371-010-0505-3
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Arts &amp; Humanities Citation Index (A&amp;HCI)
SC Computer Science
GA 602JQ
UT WOS:000278135800004
DA 2024-07-18
ER

PT J
AU Chen, JT
   Wang, B
AF Chen, Jiating
   Wang, Bin
TI High quality solid texture synthesis using position and index histogram
   matching
SO VISUAL COMPUTER
LA English
DT Article
DE Solid texture; Texture synthesis; Position histogram matching; Index
   histogram matching
AB The synthesis quality is one of the most important aspects in solid texture synthesis algorithms. In recent years several methods are proposed to generate high quality solid textures. However, these existing methods often suffer from the synthesis artifacts such as blurring, missing texture structures, introducing aberrant voxel colors, and so on. In this paper, we introduce a novel algorithm for synthesizing high quality solid textures from 2D exemplars. We first analyze the relevant factors for further improvements of the synthesis quality, and then adopt an optimization framework with the k-coherence search and the discrete solver for solid texture synthesis. The texture optimization approach is integrated with two new kinds of histogram matching methods, position and index histogram matching, which effectively cause the global statistics of the synthesized solid textures to match those of the exemplars. Experimental results show that our algorithm outperforms or at least is comparable to the previous solid texture synthesis algorithms in terms of the synthesis quality.
C1 [Chen, Jiating; Wang, Bin] Tsinghua Univ, Sch Software, Beijing 100084, Peoples R China.
   [Chen, Jiating] Tsinghua Univ, Dept Comp Sci & Technol, Beijing 100084, Peoples R China.
   [Chen, Jiating; Wang, Bin] Tsinghua Univ, Minist Educ, Key Lab Informat Syst Secur, Beijing 100084, Peoples R China.
   [Chen, Jiating; Wang, Bin] Tsinghua Natl Lab Informat Sci & Technol, Beijing 100084, Peoples R China.
C3 Tsinghua University; Tsinghua University; Tsinghua University; Tsinghua
   University
RP Chen, JT (corresponding author), Tsinghua Univ, Sch Software, Beijing 100084, Peoples R China.
EM chenjt04@gmail.com; wangbins@tsinghua.edu.cn
OI Wang, Bin/0000-0002-5176-9202
FU National Science Foundation of China [90818011, 60773143, 90715043];
   National High-Tech Research & Development Program of China
   [2007AA040401]; National Basic Research Program of China [2004CB719400]
FX We would like to thank Fang Yang, Guidu Chen for help on writing, and
   the anonymous reviewers for their valuable suggestions and comments.
   This work is supported by National Science Foundation of China (Grant
   Nos. 90818011, 60773143 and 90715043), National High-Tech Research &
   Development Program of China (Grant No. 2007AA040401), and National
   Basic Research Program of China (Grant No. 2004CB719400).
CR Dischler JM, 1998, COMPUT GRAPH FORUM, V17, pC87, DOI 10.1111/1467-8659.00256
   Dong Y, 2008, COMPUT GRAPH FORUM, V27, P1165, DOI 10.1111/j.1467-8659.2008.01254.x
   Ebert David S, 2003, Texturing Modeling: A Procedural Approach
   Efros A. A., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1033, DOI 10.1109/ICCV.1999.790383
   Efros AA, 2001, COMP GRAPH, P341, DOI 10.1145/383259.383296
   Han JW, 2006, VISUAL COMPUT, V22, P918, DOI 10.1007/s00371-006-0078-3
   Heeger DJ, 1995, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOLS I-III, pC648
   JAGNOW R, 2004, P ACM SIGGRAPH, P329
   KOPF J, 2007, P SIGGRAPH 2007
   Kwatra V, 2005, ACM T GRAPHIC, V24, P795, DOI 10.1145/1073204.1073263
   Kwatra V, 2003, ACM T GRAPHIC, V22, P277, DOI 10.1145/882262.882264
   Qin XJ, 2005, IEEE I CONF COMP VIS, P128
   Qin XJ, 2007, IEEE T VIS COMPUT GR, V13, P379, DOI 10.1109/TVCG.2007.31
   Tong X, 2002, ACM T GRAPHIC, V21, P665, DOI 10.1145/566570.566634
   Wei L.Y., 2002, THESIS STANFORD U
   Wei LY, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360651
   Wei LY, 2000, COMP GRAPH, P479, DOI 10.1145/344779.345009
NR 17
TC 27
Z9 30
U1 0
U2 20
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2010
VL 26
IS 4
SI SI
BP 253
EP 262
DI 10.1007/s00371-009-0408-3
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 584JA
UT WOS:000276746600003
OA Green Submitted
DA 2024-07-18
ER

PT J
AU He, J
   Chen, X
   Wang, ZY
   Cao, C
   Yan, H
   Peng, QS
AF He, Jian
   Chen, Xi
   Wang, Zhangye
   Cao, Chen
   Yan, He
   Peng, Qunsheng
TI Real-time adaptive fluid simulation with complex boundaries
SO VISUAL COMPUTER
LA English
DT Article
DE Nonsingle resolution SPH; Physical-based simulation; Complex boundary;
   Fluid-boundary interaction
AB In this paper, we present a new adaptive model for real-time fluid simulation with complex boundaries based on Smoothed Particle Hydrodynamics (SPH) framework. Firstly, we introduce an adaptive SPH framework that is based on our character field function composed of four factors: geometrical complexity, boundary condition, physical complexity, and complementary condition in terms of the neighboring particle number. Meanwhile, the rule for particle adaptation is presented. We also present a two-step method to fast detect collision with complex boundary. The first step is voxelization on the complex scene. In the second step, based on the result of voxelization, we propose a three-phase method to fast detect collisions between complex boundaries and particles. By using this method, we avoid most of the useless intersection detection computation and greatly enhance the computation efficiency. In addition, a subdivision of boundary is precomputed before the collision interaction method so that fluid in a scene with complex boundary can still be simulated at relatively high speed and system stability risk is reduced greatly. To further accelerate the simulation, a highly parallel fluid algorithm is presented and implemented using GPU so that we can simulate dynamic fluid with mutual interaction between fluid and complex boundary at a considerably fast speed without compromising realism.
C1 [He, Jian; Chen, Xi; Wang, Zhangye; Cao, Chen; Yan, He; Peng, Qunsheng] Zhejiang Univ, State Key Lab CAD&CG, Hangzhou 310027, Zhejiang, Peoples R China.
C3 Zhejiang University
RP Wang, ZY (corresponding author), Zhejiang Univ, State Key Lab CAD&CG, Hangzhou 310027, Zhejiang, Peoples R China.
EM zywang@cad.zju.edu.cn
RI Zhou, Hong/JKJ-1067-2023
FU National Grand Foundation Research 973 Program of China [2009CB 320802];
   National Natural Science Foundation of China [60833007, 60970075];
   National 863 High Technology Plan Foundation of China [2007AA01Z316];
   Natural Science Foundation of Zhejiang Province, China [R407042]
FX This work was supported by National Grand Foundation Research 973
   Program of China under Grant No. 2009CB 320802, State Key Program of
   National Natural Science Foundation of China under Grant No. 60833007,
   National 863 High Technology Plan Foundation of China under Grand No.
   2007AA01Z316, National Natural Science Foundation of China under Grant
   No. 60970075, and Natural Science Foundation of Zhejiang Province, China
   under Grant No. R407042.
CR Baxter William., 2004, Proceedings of the 3rd international symposium on Non-photorealistic animation and rendering, P45
   BOYLES M, 2000, ACM J GRAPHICS TOOLS, V4, P23
   Dachille F, 2000, PROC GRAPH INTERF, P205
   Desbrun M., 1999, 3829 INRIA
   DONG Z, 2004, P PAC GRAPH 2004 OCT, P73
   HAUMONT D, 2002, ACM J GRAPHICS TOOLS, V7, P27
   Kass M., 1990, Computer Graphics, V24, P49, DOI 10.1145/97880.97884
   KAUFMAN A, 1986, P 1986 WORKSH INT 3D, P45
   KAUFMAN A, 1987, P ACM SIGGRAPH 1987, P171
   Kelager M., 2006, Lagrangian fluid dynamics using smoothed particle hydrodynamics
   Lenaerts T, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360648
   Monaghan JJ, 2005, REP PROG PHYS, V68, P1703, DOI 10.1088/0034-4885/68/8/R01
   MULLER M, 2003, GRAPH EUR S COMP AN, P154
   PHARR M, 2005, GRAPHIC GEMS
   REEVES WT, 1983, COMPUT GRAPH, V17, P359
   Sramek M, 1999, IEEE T VIS COMPUT GR, V5, P251, DOI 10.1109/2945.795216
   Stam Jos., 1995, Proceedings of the 22nd annual conference on Computer graphics and interactive techniques, SIGGRAPH '95, P129
   STOLTE N, 1997, TR970623 STAT U NEW
   YAN H, 2009, P C COMP AN SOC AG 2, P2009
   YUKSEL C, 2007, P SIGGRAPH 07, P8
NR 20
TC 6
Z9 9
U1 0
U2 8
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2010
VL 26
IS 4
SI SI
BP 243
EP 252
DI 10.1007/s00371-010-0426-1
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 584JA
UT WOS:000276746600002
DA 2024-07-18
ER

PT J
AU Deng, CY
   Yang, XN
AF Deng, Chongyang
   Yang, Xunnian
TI A simple method for interpolating meshes of arbitrary topology by
   Catmull-Clark surfaces
SO VISUAL COMPUTER
LA English
DT Article
DE Arbitrary topology mesh; Subdivision surfaces; Interpolation;
   Catmull-Clark subdivision
ID SUBDIVISION SURFACES; SCHEME
AB Interpolating an arbitrary topology mesh by a smooth surface plays important role in geometric modeling and computer graphics. In this paper we present an efficient new algorithm for constructing Catmull-Clark surface that interpolates a given mesh. The control mesh of the interpolating surface is obtained by one Catmull-Clark subdivision of the given mesh with modified geometric rule. Two methods-push-back operation based method and normal-based method-are presented for the new geometric rule. The interpolation method has the following features: (1) Efficiency: we obtain a generalized cubic B-spline surface to interpolate any given mesh in a robust and simple manner. (2) Simplicity: we use only simple geometric rule to construct control mesh for the interpolating subdivision surface. (3) Locality: the perturbation of a given vertex only influences the surface shape near this vertex. (4) Freedom: for each edge and face, there is one degree of freedom to adjust the shape of the limit surface. These features make interpolation using Catmull-Clark surfaces very simple and thus make the method itself suitable for interactive free-form shape design.
C1 [Deng, Chongyang] Hangzhou Dianzi Univ, Inst Appl Math & Engn Computat, Hangzhou 310018, Zhejiang, Peoples R China.
   [Yang, Xunnian] Zhejiang Univ, Dept Math, Hangzhou 310027, Zhejiang, Peoples R China.
C3 Hangzhou Dianzi University; Zhejiang University
RP Deng, CY (corresponding author), Hangzhou Dianzi Univ, Inst Appl Math & Engn Computat, Hangzhou 310018, Zhejiang, Peoples R China.
EM ddd1742@163.com; yxn@zju.edu.cn
RI ; Deng, Chongyang/E-4422-2017
OI Yang, Xunnian/0000-0001-7501-1378; Deng, Chongyang/0000-0002-8725-4622
FU NSFC [60673032]; Scientific Starting Foundation of Hangzhou Dianzi
   University [KYS075608073]
FX This work is supported by NSFC (60673032) and the Scientific Starting
   Foundation of Hangzhou Dianzi University (KYS075608073).
CR Brunet P., 1988, Computer-Aided Geometric Design, V5, P41, DOI 10.1016/0167-8396(88)90019-2
   CATMULL E, 1978, COMPUT AIDED DESIGN, V10, P350, DOI 10.1016/0010-4485(78)90110-0
   Chen B, 2004, J HIGH ENERGY PHYS
   Claes J, 2001, INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, PROCEEDING, P42, DOI 10.1109/SMA.2001.923374
   DYN N, 1990, ACM T GRAPHIC, V9, P160, DOI 10.1145/78956.78958
   Halstead M., 1993, Computer Graphics Proceedings, P35, DOI 10.1145/166117.166121
   HOPPE H, 1994, COMPUTER GRAPHICS, V28, P295
   Kobbelt L, 1998, ACM T GRAPHIC, V17, P209, DOI 10.1145/293145.293146
   Kobbelt L, 1996, COMPUT GRAPH FORUM, V15, pC409, DOI 10.1111/1467-8659.1530409
   LAI S, 2007, P 3 INT S VIS COMP
   Lai SH, 2006, VISUAL COMPUT, V22, P865, DOI 10.1007/s00371-006-0072-9
   Li G, 2007, COMPUT GRAPH FORUM, V26, P185, DOI 10.1111/j.1467-8659.2007.01015.x
   Li G, 2005, COMPUT GRAPH FORUM, V24, P3, DOI 10.1111/j.1467-8659.2005.00824.x
   Lin HW, 2004, SCI CHINA SER F, V47, P315, DOI 10.1360/02yf0529
   Lin SJ, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409099
   Litke N, 2001, IEEE VISUAL, P319, DOI 10.1109/VISUAL.2001.964527
   LOOP C, 1987, THESIS UTAH U US
   Ma WY, 2004, COMPUT AIDED DESIGN, V36, P525, DOI 10.1016/S0010-4485(03)00160-X
   Ma WY, 2002, VISUAL COMPUT, V18, P415, DOI 10.1007/s003710100159
   Maekawa T, 2007, COMPUT AIDED DESIGN, V39, P313, DOI 10.1016/j.cad.2006.12.008
   Maillot J, 2001, COMPUT GRAPH FORUM, V20, pC471, DOI 10.1111/1467-8659.00540
   Marinov M, 2005, GRAPH MODELS, V67, P452, DOI 10.1016/j.gmod.2005.01.003
   NASRI AH, 1987, ACM T GRAPHIC, V6, P29, DOI 10.1145/27625.27628
   Shilane P., 2004, Shape Modeling International
   Stam J, 2003, COMPUT GRAPH FORUM, V22, P79, DOI 10.1111/1467-8659.t01-2-00647
   Suzuki H., 1999, Proceedings. Seventh Pacific Conference on Computer Graphics and Applications (Cat. No.PR00293), P158, DOI 10.1109/PCCGA.1999.803359
   Yang XN, 2005, COMPUT AIDED DESIGN, V37, P497, DOI 10.1016/j.cad.2004.10.008
   Zheng JM, 2006, IEEE T VIS COMPUT GR, V12, P301, DOI 10.1109/TVCG.2006.49
   Zheng JM, 2005, VISUAL COMPUT, V21, P242, DOI 10.1007/s00371-005-0285-3
   ZORIN D, 1996, COMPUTER GRAPHICS, V30, P189
   Zorin D., 2000, Course Notes of SIGGRAPH
NR 31
TC 16
Z9 19
U1 0
U2 5
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2010
VL 26
IS 2
BP 137
EP 146
DI 10.1007/s00371-009-0393-6
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 543RE
UT WOS:000273595200005
DA 2024-07-18
ER

PT J
AU Kainz, B
   Reiter, U
   Reiter, G
   Schmalstieg, D
AF Kainz, Bernhard
   Reiter, Ursula
   Reiter, Gert
   Schmalstieg, Dieter
TI In vivo interactive visualization of four-dimensional blood flow
   patterns
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Workshop on 3D Physiological Human
CY DEC, 2008
CL Zermatt, SWITZERLAND
DE Flow visualization; Cardiovascular diagnostics; Phase contrast magnetic
   resonance imaging (PC-MRI)
ID MAIN PULMONARY-ARTERY; CONTRAST; SIMULATION; DYNAMICS
AB In this paper we give an overview over a series of experiments to visualize and measure flow fields in the human vascular system with respect to their diagnostic capabilities. The experiments utilize a selection of GPU-based sparse and dense flow visualization algorithms to show the diagnostic opportunities for volumetric cardiovascular phase contrast magnetic resonance imaging data sets. Besides classical hardware accelerated particle and line-based approaches, an extensible tublet-based visualization, a four-dimensional volumetric line integral convolution and a new two-dimensional cutting plane tool for three-dimensional velocity data sets have been implemented. To evaluate the results, several hearts of human subjects have been investigated and a flow phantom was built to artificially simulate distinctive flow features. Our results demonstrate that we are able to provide an interactive tool for cardiovascular diagnostics with complementary hardware accelerated visualizations.
C1 [Kainz, Bernhard; Schmalstieg, Dieter] Graz Univ Technol, Inst Comp Graph & Vis, A-8010 Graz, Austria.
   [Reiter, Ursula] Graz Univ, Clin Radiol, A-8010 Graz, Austria.
   [Reiter, Gert] Siemens Healthcare Austria, A-8010 Graz, Austria.
C3 Graz University of Technology; University of Graz; Siemens AG
RP Kainz, B (corresponding author), Graz Univ Technol, Inst Comp Graph & Vis, Inffeldgasse 16A, A-8010 Graz, Austria.
EM kainz@icg.tugraz.at; ursula.reiter@klinikum-gaz.at;
   gert.reiter@siemens.com; schmalstieg@icg.tugraz.at
RI Reiter, Ursula/P-7259-2016; Kainz, Bernhard/H-3416-2016
OI Kainz, Bernhard/0000-0002-7813-5023
CR Angel E., 2005, INTERACTIVE COMPUTER, VFourth
   Cabral B., 1993, P 20 ANN C COMP GRAP, P263, DOI DOI 10.1145/166117.166151
   Cebral JR, 2005, IEEE T MED IMAGING, V24, P457, DOI 10.1109/TMI.2005.844159
   de Leeuw W, 1998, VISUALIZATION '98, PROCEEDINGS, P359, DOI 10.1109/VISUAL.1998.745324
   deLeeuw WC, 1995, VISUALIZATION '95 - PROCEEDINGS, P233, DOI 10.1109/VISUAL.1995.480817
   Edelmann F. T., 2006, COMPREHENSIVE ORGANO, P1
   Gonzalez E, 1996, ANN BIOMED ENG, V24, P48, DOI 10.1007/BF02770994
   Grabner Markus., 2005, P SPRING C COMPUTER, P77
   Helgeland A, 2004, IEEE T VIS COMPUT GR, V10, P673, DOI 10.1109/TVCG.2004.49
   Kolb A., 2004, P ACM SIGGRAPHEUROGR, P123
   Krüger J, 2005, IEEE T VIS COMPUT GR, V11, P744, DOI 10.1109/TVCG.2005.87
   Kvitting JPE, 2004, J THORAC CARDIOV SUR, V127, P1602, DOI 10.1016/j.jtcvs.2003.10.042
   Laramee RS, 2004, COMPUT GRAPH FORUM, V23, P203, DOI 10.1111/j.1467-8659.2004.00753.x
   LATTA L, 2004, GAM DEV C 2004
   Ley S, 2008, INVEST RADIOL, V43, P421, DOI 10.1097/RLI.0b013e318169015d
   Liu ZP, 2005, IEEE T VIS COMPUT GR, V11, P113, DOI 10.1109/TVCG.2005.21
   Markl M, 2003, J MAGN RESON IMAGING, V17, P499, DOI 10.1002/jmri.10272
   Merhof D, 2006, IEEE T VIS COMPUT GR, V12, P1181, DOI 10.1109/TVCG.2006.151
   Morris L, 2005, J BIOMECH ENG-T ASME, V127, P767, DOI 10.1115/1.1992521
   NEREM RM, 1974, CIRC RES, V34, P193, DOI 10.1161/01.RES.34.2.193
   Reiter G, 2007, J CARDIOVASC MAGN R, V9, P237
   Reiter G, 2008, CIRC-CARDIOVASC IMAG, V1, P23, DOI 10.1161/CIRCIMAGING.108.780247
   REZKSALAMA C, 1999, IEEE VISUALIZATION 9, P233
   SCHOEPHOERSTER RT, 1994, J BIOMECH, V27, P125, DOI 10.1016/0021-9290(94)90201-1
   STALLING D, 1995, SIGGRAPH 95 C P, P249
   TAYLOR TW, 1995, BIORHEOLOGY, V32, P61
   VANWIJK JJ, 1991, COMP GRAPH, V25, P309, DOI 10.1145/127719.122751
   Weiskopf D., 2007, GPU BASED INTERACTIV
   Wigström L, 1999, MAGNET RESON MED, V41, P793, DOI 10.1002/(SICI)1522-2594(199904)41:4<793::AID-MRM19>3.0.CO;2-2
NR 29
TC 1
Z9 1
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2009
VL 25
IS 9
BP 853
EP 862
DI 10.1007/s00371-009-0315-7
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 478KA
UT WOS:000268585100005
DA 2024-07-18
ER

PT J
AU Bailer, W
   Lee, F
   Thallinger, G
AF Bailer, Werner
   Lee, Felix
   Thallinger, Georg
TI A distance measure for repeated takes of one scene
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 14th International Multimedia Modeling Conference (MMM 2008)
CY JAN 09-11, 2008
CL Kyoto Univ, Kyoto, JAPAN
HO Kyoto Univ
DE Sequence matching; Video; LCSS; Distance measure
ID VIDEO
AB In applications, such as post-production and archiving of audiovisual material, users are confronted with large amounts of redundant unedited raw material, called rushes. Viewing and organizing this material are crucial but time consuming tasks. Typically, multiple but slightly different takes of the same scene can be found in the rushes video. We propose a method for detecting and clustering takes of one scene shot from the same or very similar camera positions. An important subproblem is to determine the similarity of video segments. We propose a distance measure based on the Longest Common Subsequence (LCSS) model. Two variants of the proposed approach, one with a threshold parameter and one with automatically determined threshold, are compared against the Dynamic Time Warping (DTW) distance measure on six videos from the TRECVID 2007 BBC rushes summarization data set. We also evaluate the influence of the applied temporal segmentation method at the input on the results. Applications of the proposed method to automatic skimming and interactive browsing of rushes video are described.
C1 [Bailer, Werner; Lee, Felix; Thallinger, Georg] JOANNEUM RES Forsch Gesell mbH, Inst Informat Syst & Informat Management, A-8010 Graz, Austria.
RP Bailer, W (corresponding author), JOANNEUM RES Forsch Gesell mbH, Inst Informat Syst & Informat Management, Steyrergasse 17, A-8010 Graz, Austria.
EM werner.bailer@joanneum.at; felix.lee@joanneum.at;
   georg.thallinger@joanneum.at
OI Bailer, Werner/0000-0003-2442-4900
CR Adjeroh DA, 1999, COMPUT VIS IMAGE UND, V75, P25, DOI 10.1006/cviu.1999.0764
   [Anonymous], PRESERVATION DIGITIS
   [Anonymous], Open Source Computer Vision Library - v2.4.9
   [Anonymous], 2004, P 12 ANN ACM INT C M
   [Anonymous], SOX SOUND EXCHANGE
   BAILER W, 2007, P ACM INT WORKSH TRE, P60
   Bailer W, 2008, LECT NOTES COMPUT SC, V4903, P80
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Chang SF, 1997, ACM MULTIMEDIA 97, PROCEEDINGS, P313, DOI 10.1145/266180.266382
   Cormen Thomas H., 2001, INTRO ALGORITHMS
   Covell M, 2006, 2006 IEEE WORKSHOP ON MULTIMEDIA SIGNAL PROCESSING, P461, DOI 10.1109/MMSP.2006.285351
   Duda R., 1973, Pattern Classification and Scene Analysis
   Hampapur A, 2002, P SOC PHOTO-OPT INS, V4676, P194
   Hampapur A., 2001, P IEEE INT C MULT EX, P737, DOI DOI 10.1109/ICME.2001.1237827
   HSU W, 2006, INT C IM PROC ICIP
   Keogh E.J., 2001, P 2001 SIAM INT C DA, P1, DOI [DOI 10.1137/1.9781611972719.1, 10.1137/1.9781611972719.1]
   KLEBAN J, 2007, TVS 07, P84
   *MPEG 7, 2001, 159388 ISOIEC
   *MPEG 7, 2001, 159383 ISOIEC
   MYERS CS, 1981, AT&T TECH J, V60, P1389, DOI 10.1002/j.1538-7305.1981.tb00272.x
   Over P., 2007, TVS '07: Proc. of the International Workshop on TRECVID Video Summarization, P1, DOI DOI 10.1145/1290031.1290032
   SALVADOR S, 2004, P 3 WORKSH MIN TEMP
   Smeaton A., 2006, P TRECVID WORKSH
   TAN YP, 1999, P IEEE INT C IM PROC, V2, P106
   Viola P., 2002, Advances in Neural Information Processing System, V14
   Vlachos M, 2005, MACH LEARN, V58, P301, DOI 10.1007/s10994-005-5830-9
   Vlachos M, 2002, PROC INT CONF DATA, P673, DOI 10.1109/ICDE.2002.994784
   Zhang Z, 2006, INT C PATT RECOG, P1135
   ZHAO L, 2000, MULTIMEDIA 00, P217
   Zhu XQ, 2005, IEEE T MULTIMEDIA, V7, P648, DOI 10.1109/TMM.2005.850977
NR 30
TC 9
Z9 9
U1 0
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2009
VL 25
IS 1
BP 53
EP 68
DI 10.1007/s00371-008-0280-6
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 379RC
UT WOS:000261412900006
DA 2024-07-18
ER

PT J
AU Kuschel, M
   Freyberger, F
   Färber, B
   Buss, M
AF Kuschel, Martin
   Freyberger, Franziska
   Faerber, Berthold
   Buss, Martin
TI Visual-haptic perception of compliant objects in artificially generated
   environments
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT International Conference on Cyberworlds
CY OCT 24-27, 2007
CL Hannover, GERMANY
SP Welfenlab, Gottfried Wilhelm Leibniz Univ, EuroGraphics, ACM SIGWEB, ACM SIGART
DE visual-haptic perception; human system interface; virtual reality;
   telepresence
ID TEXTURE-PERCEPTION; SENSORY MODALITIES; FORM PERCEPTION; VISION; TOUCH;
   CONFLICT; DOMINANCE; PROPRIOCEPTION; INTEGRATION; INFORMATION
AB Perception of compliant objects through a human system interface with visual-haptic feedback was investigated. Participants had to explore virtual cubes at different compliances by squeezing them with their fingers and observing them visually and haptically. The cubes were rendered by admittance control. Perception of compliance was analyzed using an adaptive staircase method. Results showed that visual-haptic perception of compliant environments is less accurate than perception of position and force stimuli. Furthermore, due to the important role of the visual feedback cross-modal comparisons are more difficult than bimodal comparisons.
C1 [Kuschel, Martin; Buss, Martin] Tech Univ Munich, Inst Automat Control Engn LSR, Munich, Germany.
   [Freyberger, Franziska; Faerber, Berthold] Univ Bundeswehr Munich, Human Factors Inst IfA, Munich, Germany.
C3 Technical University of Munich; Bundeswehr University Munich
RP Kuschel, M (corresponding author), Tech Univ Munich, Inst Automat Control Engn LSR, Munich, Germany.
EM martin.kuschel@tum.de; franziska.freyberger@unibw.de;
   berthold.faerber@unibw.de; mb@tum.de
CR [Anonymous], HCI INT
   Brenner E, 2003, SPATIAL VISION, V16, P365, DOI 10.1163/156856803322467581
   BUSS M, 2004, IEEE INT C MECH ROB
   Calvert G., 2004, The Handbook of Multisensory Processes.
   Calvert GA, 1998, TRENDS COGN SCI, V2, P247, DOI 10.1016/S1364-6613(98)01189-9
   Driver J, 2000, CURR BIOL, V10, pR731, DOI 10.1016/S0960-9822(00)00740-5
   Ernst MO, 2002, NATURE, V415, P429, DOI 10.1038/415429a
   Ernst MO, 2004, TRENDS COGN SCI, V8, P162, DOI 10.1016/j.tics.2004.02.002
   Ernst MO, 2000, NAT NEUROSCI, V3, P69, DOI 10.1038/71140
   FISHKIN SM, 1975, PERCEPT MOTOR SKILL, V40, P427, DOI 10.2466/pms.1975.40.2.427
   Friedman M. P., 1974, HDB PERCEPTION, V2, P23, DOI DOI 10.1016/B978-0-12-161902-2.50009-2
   Gescheider G.A., 1976, PSYCHOPHYSICS METHOD
   Guest S, 2003, EXP BRAIN RES, V150, P201, DOI 10.1007/s00221-003-1404-x
   Guest S, 2003, INT J PSYCHOPHYSIOL, V50, P63, DOI 10.1016/S0167-8760(03)00125-9
   HELLER MA, 1983, PERCEPTION, V12, P607, DOI 10.1068/p120607
   HELLER MA, 1992, PERCEPTION, V21, P655, DOI 10.1068/p210655
   Heller MA, 1999, PERCEPT PSYCHOPHYS, V61, P1384, DOI 10.3758/BF03206188
   HELLER MA, 1982, PERCEPT PSYCHOPHYS, V31, P339, DOI 10.3758/BF03202657
   KINNEY JS, 1970, PERCEPT PSYCHOPHYS, V8, P189, DOI 10.3758/BF03210203
   LAWRENCE DA, 1993, IEEE T ROBOTIC AUTOM, V9, P624, DOI 10.1109/70.258054
   LEDERMAN SJ, 1981, J EXP PSYCHOL HUMAN, V7, P902, DOI 10.1037/0096-1523.7.4.902
   LEDERMAN SJ, 1986, J EXP PSYCHOL HUMAN, V12, P169, DOI 10.1037/0096-1523.12.2.169
   Marks L.E., 2004, HDB MULTISENSORY PRO, P85
   MASSARO DW, 1987, PERSPECTIVES PERCEPT, P273
   Misceo GF, 1999, PERCEPT PSYCHOPHYS, V61, P608
   OVER R, 1966, BRIT J PSYCHOL, V57, P335, DOI 10.1111/j.2044-8295.1966.tb01034.x
   PICK HL, 1969, PERCEPT PSYCHOPHYS, V6, P203, DOI 10.3758/BF03207017
   ROCK I, 1967, SCI AM, V216, P96, DOI 10.1038/scientificamerican0567-96
   ROCK I, 1964, SCIENCE, V143, P594, DOI 10.1126/science.143.3606.594
   Sciavicco L., 2003, MODELLING CONTROL RO
   Shimojo S, 2001, CURR OPIN NEUROBIOL, V11, P505, DOI 10.1016/S0959-4388(00)00241-5
   Soto-Faraco S, 2002, COGNITIVE BRAIN RES, V14, P139, DOI 10.1016/S0926-6410(02)00068-X
   Spence C, 2001, PERCEPT PSYCHOPHYS, V63, P330, DOI 10.3758/BF03194473
   Spence C., 2004, Crossmodal space and crossmodal attention
   Srinivasan M. A., 1996, Proceedings of the ASME Dynamic Systems and Control Division, P555
   Stein Barry E., 1993, The Merging of the Senses. The Merging of the Senses. Cognitive Neuroscience
   UBERLE M, 2004, P IEEE RSJ INT C INT
   WARREN DH, 1970, PERCEPT PSYCHOPHYS, V8, P430, DOI 10.3758/BF03207040
   Welch R.B., 1986, HDB PERCEPTION HUMAN, V1, p25
   WITMER BG, 1998, VIRTUAL ENV, V7, P225, DOI DOI 10.1162/105474698565686
   Wu W., 1999, P AM SOC MECH ENG DY, V67, P19
   YOKOKOHJI Y, 1994, IEEE T ROBOTIC AUTOM, V10, P605, DOI 10.1109/70.326566
NR 42
TC 5
Z9 6
U1 0
U2 12
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2008
VL 24
IS 10
BP 923
EP 931
DI 10.1007/s00371-008-0289-x
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 347IQ
UT WOS:000259134200008
DA 2024-07-18
ER

PT J
AU Hong, W
   House, DH
   Keyser, J
AF Hong, Woosuck
   House, Donald H.
   Keyser, John
TI Adaptive particles for incompressible fluid simulation
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 26th International Conference on Computer Graphics
CY JUN 09-11, 2008
CL Istanbul, TURKEY
DE fluid simulation; physically based modeling; natural phenomenon
ID WATER; ANIMATION
AB We propose a particle-based technique for simulating incompressible fluid that includes adaptive refinement of particle sampling. Each particle represents a mass of fluid in its local region. Particles are split into several particles for finer sampling in regions of complex flow. In regions of smooth flow, neighboring particles can be merged. Depth below the surface and Reynolds number are exploited as our criteria for determining whether splitting or merging should take place. For the fluid dynamics calculations, we use the hybrid FLIP method, which is computationally simple and efficient. Since the fluid is incompressible, each particle has a volume proportional to its mass. A kernel function, whose effective range is based on this volume, is used for transferring and updating the particle's physical properties such as mass and velocity. Our adaptive particle-based simulation is demonstrated in several scenarios that show its effectiveness in capturing fine detail of the flow, where needed, while efficiently sampling regions where less detail is required.
C1 [Hong, Woosuck; Keyser, John] Texas A&M Univ, Dept Comp Sci, College Stn, TX 77843 USA.
   [House, Donald H.] Texas A&M Univ, Dept Visualizat, College Stn, TX 77843 USA.
C3 Texas A&M University System; Texas A&M University College Station; Texas
   A&M University System; Texas A&M University College Station
RP Hong, W (corresponding author), Texas A&M Univ, Dept Comp Sci, College Stn, TX 77843 USA.
EM wshong@cs.tamu.edu; house@viz.tamu.edu; keyser@cs.tamu.edu
OI Keyser, John/0000-0002-4829-9975
CR Adalsteinsson D, 1999, J COMPUT PHYS, V148, P2, DOI 10.1006/jcph.1998.6090
   Adams B, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1276377.1276437, 10.1145/1239451.1239499]
   BRACKBILL JU, 1986, J COMPUT PHYS, V65, P314, DOI 10.1016/0021-9991(86)90211-1
   Clavet S., 2005, SCA '05, P219, DOI DOI 10.1145/1073368.1073400
   Desbrun M., 1996, P EUROGRAPHICS WORKS, P61
   Desbrun M., 1999, 3829 INRIA
   Enright D, 2002, ACM T GRAPHIC, V21, P736, DOI [10.1145/566570.566581, 10.1145/566570.566645]
   Foster N, 1996, GRAPH MODEL IM PROC, V58, P471, DOI 10.1006/gmip.1996.0039
   Foster N, 2001, COMP GRAPH, P23, DOI 10.1145/383259.383261
   Greenwood S., 2004, Proceedings of the 2004 ACM SIGGRAPH/Eurographics symposium on Computer animation, P287
   HARLOW F, 1965, J FLUIDS PHYS, V181
   Hong JM, 2005, ACM T GRAPHIC, V24, P915, DOI 10.1145/1073204.1073283
   Irving G, 2006, ACM T GRAPHIC, V25, P805, DOI 10.1145/1141911.1141959
   KIM J, 2006, SCA 06, P335
   KOSHIZUKA S, 1995, COMPUT FLUID DYN, V181, P29
   Lorensen William E., 1987, COMPUT GRAPH, P163, DOI DOI 10.1145/37402.37422
   Losasso F, 2004, ACM T GRAPHIC, V23, P457, DOI 10.1145/1015706.1015745
   LUCY LB, 1977, ASTRON J, V82, P1013, DOI 10.1086/112164
   Muller M., 2003, SCA, P154
   Premoze S., 2003, Particle-based simulation of fluids
   Reynolds O., 1883, Philos. Trans. R. Soc. Lond., V174, P935, DOI [10.1098/rspl.1883.0018, 10.1098/rstl.1883.0029, DOI 10.1098/RSTL.1883.0029]
   Song OY, 2005, ACM T GRAPHIC, V24, P81, DOI 10.1145/1037957.1037962
   Stam J., 1999, The Proceedings of SIGGRAPH, P121, DOI DOI 10.1145/311535.311548
   Zhao HK, 2005, MATH COMPUT, V74, P603
   Zhu YN, 2005, ACM T GRAPHIC, V24, P965, DOI 10.1145/1073204.1073298
NR 25
TC 23
Z9 32
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2008
VL 24
IS 7-9
BP 535
EP 543
DI 10.1007/s00371-008-0234-z
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 322OI
UT WOS:000257384800009
DA 2024-07-18
ER

PT J
AU Mu, YD
   Yan, SC
   Huang, T
   Zhou, BF
AF Mu, Yadong
   Yan, Shuicheng
   Huang, Thomas
   Zhou, Bingfeng
TI Contextual motion field-based distance for video analysis
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 26th International Conference on Computer Graphics
CY JUN 09-11, 2008
CL Istanbul, TURKEY
DE video analysis; motion field; activity classification
AB In this work, we propose a general method for computing distance between video frames or sequences. Unlike conventional appearance-based methods, we first extract motion fields from original videos. To avoid the huge memory requirement demanded by the previous approaches, we utilize the "bag of motion vectors" model, and select Gaussian mixture model as compact representation. Thus, estimating distance between two frames is equivalent to calculating the distance between their corresponding Gaussian mixture models, which is solved via earth mover distance (EMD) in this paper. On the basis of the inter-frame distance, we further develop the distance measures for both full video sequences.
   Our main contribution is four-fold. Firstly, we operate on a tangent vector field of spatio-temporal 2D surface manifold generated by video motions, rather than the intensity gradient space. Here we argue that the former space is more fundamental. Secondly, the correlations between frames are explicitly exploited using a generative model named dynamic conditional random fields (DCRF). Under this framework, motion fields are estimated by Markov volumetric regression, which is more robust and may avoid the rank deficiency problem. Thirdly, our definition for video distance is in accord with human intuition and makes a better tradeoff between frame dissimilarity and chronological ordering. Lastly, our definition for frame distance allows for partial distance.
C1 [Mu, Yadong; Zhou, Bingfeng] Peking Univ, Inst Comp Sci & Technol, Beijing 100871, Peoples R China.
   [Yan, Shuicheng] Natl Univ Singapore, ECE Dept, Singapore 117576, Singapore.
   [Huang, Thomas] Univ Illinois, ECE Dept, Urbana, IL 61801 USA.
C3 Peking University; National University of Singapore; University of
   Illinois System; University of Illinois Urbana-Champaign
RP Mu, YD (corresponding author), Peking Univ, Inst Comp Sci & Technol, Beijing 100871, Peoples R China.
EM muyadong@gmail.com; ccbfzhou@pku.edu.cn
RI Yan, Shuicheng/HCI-1431-2022
CR [Anonymous], ARPA IM UND WORKSH
   [Anonymous], CVPR
   [Anonymous], 2001, COMP SCI W
   Blank M, 2005, IEEE I CONF COMP VIS, P1395
   BOIMAN O, 2007, ADV NEURAL INFORM PR, V19
   Duda R., 1973, Pattern Classification and Scene Analysis
   Efros AA, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P726
   Felzenszwalb PF, 2006, INT J COMPUT VISION, V70, P41, DOI 10.1007/s11263-006-7899-4
   Friedman J., 2001, ELEMENTS STAT LEARNI, V1, DOI DOI 10.1007/978-0-387-84858-7
   Greenspan H, 2004, COMPUT VIS IMAGE UND, V93, P86, DOI 10.1016/j.cviu.2003.08.004
   Greenspan H, 2001, COMPUT VIS IMAGE UND, V84, P384, DOI 10.1006/cviu.2001.0946
   Ke Y, 2005, IEEE I CONF COMP VIS, P166
   Laptev I, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P432, DOI 10.1109/iccv.2003.1238378
   Rubner Y, 2000, INT J COMPUT VISION, V40, P99, DOI 10.1023/A:1026543900054
   SHECHTMAN E, 2005, CVPR, P405
   Sun J, 2005, ACM T GRAPHIC, V24, P861, DOI 10.1145/1073204.1073274
   Trucco E., 1998, Introductory techniques for 3-D computer vision, V201
   Wang Fei, 2006, P 23 INT C MACH LEAR, P985
   WANG Y, 2005, CVPR05, P264, DOI DOI 10.1109/CVPR.2005.26
   Winnemöller H, 2006, ACM T GRAPHIC, V25, P1221, DOI 10.1145/1141911.1142018
   Wu M, 2007, ADV NEURAL INFORM PR, V19
   Yu SX, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P313, DOI 10.1109/iccv.2003.1238361
NR 22
TC 1
Z9 1
U1 0
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2008
VL 24
IS 7-9
BP 595
EP 603
DI 10.1007/s00371-008-0240-1
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 322OI
UT WOS:000257384800015
DA 2024-07-18
ER

PT J
AU Peyrat, A
   Terraz, O
   Merillou, S
   Galin, E
AF Peyrat, Alexandre
   Terraz, Olivier
   Merillou, Stephane
   Galin, Eric
TI Generating vast varieties of realistic leaves with parametric 2Gmap
   L-systems
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 26th International Conference on Computer Graphics
CY JUN 09-11, 2008
CL Istanbul, TURKEY
DE leaves; formal grammar; L-systems; aging
AB Creating realistic plants and trees require the ability to generate thousands of leaves with different shapes and textures for different given species. This paper presents an original method to generate large atlases of leaves with many details from a single formal grammar. Leaves are described by a parameterized 2Gmap L-system that describes their evolution in shape and texture through out their entire life cycle. Our approach automatically synthesizes the deformations as well as the color and texture changes as the leaves age, as well as defects such as holes or cracks produced by insect attacks or accidents.
C1 [Peyrat, Alexandre; Terraz, Olivier; Merillou, Stephane] CNRS, XLIM UMR, Dept Math Informat, F-87060 Limoges, France.
   [Galin, Eric] Univ Lyon 1, F-69622 Villeurbanne, France.
C3 Centre National de la Recherche Scientifique (CNRS); Universite Claude
   Bernard Lyon 1
RP Merillou, S (corresponding author), CNRS, XLIM UMR, Dept Math Informat, Ave Albert Thomas, F-87060 Limoges, France.
EM alexandre.peyrat@xlim.fr; olivier.terraz@xlim.fr;
   stephane.merillou@xlim.fr; eric.galin@liris.cnrs.fr
RI Galin, Eric/X-1938-2019
OI Galin, Eric/0000-0002-5946-4112
CR [Anonymous], 1996, The Algorithmic Beauty of Plants
   Baranoski GVG, 1997, COMPUT GRAPH FORUM, V16, pC141, DOI 10.1111/1467-8659.00150
   Baranoski GVG, 2001, VISUAL COMPUT, V17, P491, DOI 10.1007/s003710100126
   BOUSQUET L, 2006, 2 INT S REC ADV QUAN
   Braitmaier M, 2004, THEORY AND PRACTICE OF COMPUTER GRAPHICS 2004, PROCEEDINGS, P152, DOI 10.1109/TPCG.2004.1314465
   Chiba N, 1996, J VISUAL COMP ANIMAT, V7, P79, DOI 10.1002/(SICI)1099-1778(199604)7:2<79::AID-VIS139>3.0.CO;2-W
   DESBENOIT B, 2006, 26 INT C EUR AIR LA, P107
   Fuhrer M, 2004, 12TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P217, DOI 10.1109/PCCGA.2004.1348352
   HAMMEL MS, 1992, P COMP GRAPH INT TOK, P119
   Lienhardt P, 1994, INT J COMPUT GEOM AP, V4, P275, DOI 10.1142/S0218195994000173
   MECH R, 1998, CPEG VERSION 3 4 USE
   Mochizuki S, 2001, NINTH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P348, DOI 10.1109/PCCGA.2001.962891
   Mündermann L, 2003, COMPUTER GRAPHICS INTERNATIONAL, PROCEEDINGS, P60, DOI 10.1109/CGI.2003.1214448
   Prusinkiewicz P, 2001, COMP GRAPH, P289, DOI 10.1145/383259.383291
   Prusinkiewicz Przemyslaw, 1990, Scientific Visualization and Graphics Simulation, P183
   Quan L, 2006, SIGGRAPH 06, P599
   Rodkaew Y., 2004, 4th International workshop on functional-structural plant models, strani, P391
   Rodkaew Y., 2002, P INT C COMP MATH MO, p73 88
   RUNIONS A, 2005, SIGGRAPH 05, P702, DOI DOI 10.1145/1186822.1073251
   TERRAZ O, 2008, VISUAL COMP IN PRESS
   VanOverveld CWAM, 1996, ACM T GRAPHIC, V15, P72, DOI 10.1145/226150.226154
   Wang LF, 2005, ACM T GRAPHIC, V24, P712, DOI 10.1145/1073204.1073252
NR 22
TC 15
Z9 21
U1 0
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2008
VL 24
IS 7-9
BP 807
EP 816
DI 10.1007/s00371-008-0262-8
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 322OI
UT WOS:000257384800036
DA 2024-07-18
ER

PT J
AU Pajarola, R
   Gobbetti, E
AF Pajarola, Renato
   Gobbetti, Enrico
TI Survey of semi-regular multiresolution models for interactive terrain
   rendering
SO VISUAL COMPUTER
LA English
DT Article
DE terrain rendering; multiresolution triangulation; semi-regular meshes
ID GEOMETRY; DESIGN; MESHES
AB Rendering high quality digital terrains at interactive rates requires carefully crafted algorithms and data structures able to balance the competing requirements of realism and frame rates, while taking into account the memory and speed limitations of the underlying graphics platform. In this survey, we analyze multiresolution approaches that exploit a certain semi-regularity of the data. These approaches have produced some of the most efficient systems to date. After providing a short background and motivation for the methods, we focus on illustrating models based on tiled blocks and nested regular grids, quadtrees and triangle bin-trees triangulations, as well as cluster-based approaches. We then discuss LOD error metrics and system-level data management aspects of interactive terrain visualization, including dynamic scene management, out-of-core data organization and compression, as well as numerical accuracy.
C1 Univ Zurich, Dept Math, Visualizat & MultiMedia Lab, CH-8006 Zurich, Switzerland.
   Ctr Adv Studies Res & Dev Sardinia CRS4, Visual Comp Grp, I-09010 Pula, CA, Italy.
C3 University of Zurich
RP Pajarola, R (corresponding author), Univ Zurich, Dept Math, Visualizat & MultiMedia Lab, CH-8006 Zurich, Switzerland.
EM pajarola@acm.org; gobbetti@crs4.it
RI Gobbetti, Enrico/O-2188-2015
OI Gobbetti, Enrico/0000-0003-0831-2458; Pajarola,
   Renato/0000-0002-6724-526X
CR [Anonymous], 2003, LEVEL DETAIL 3D GRAP
   Asano T, 1997, THEOR COMPUT SCI, V181, P3, DOI 10.1016/S0304-3975(96)00259-9
   Balmelli L, 1998, 1998 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL 2, P914, DOI 10.1109/ICIP.1998.723704
   BALMELLI L, 2001, ELECT P 13 CAN C COM
   Bao X., 2004, P VIS MOD VIS VMV, P413
   BAO XH, 2003, P SPIE 03, P225
   Baumann K, 1999, COMPUTER GRAPHICS INTERNATIONAL, PROCEEDINGS, P85, DOI 10.1109/CGI.1999.777920
   Cignoni P, 2005, IEEE VISUALIZATION 2005, PROCEEDINGS, P207
   Cignoni P, 1998, COMPUT GRAPH-UK, V22, P37, DOI 10.1016/S0097-8493(97)00082-4
   CIGNONI P, 2003, P EUR 2003 SEPT, P505
   De Floriani L, 2005, MATH VIS, P49
   De Floriani L., 1997, Geometric Modeling: Theory and Practice. State of the Art, P302
   DeFloriani L, 1996, VISUAL COMPUT, V12, P317, DOI 10.1007/BF01782231
   DEFLORIANI L, 1997, IEEE VISUALIZATION
   Duchaineau M, 1997, VISUALIZATION '97 - PROCEEDINGS, P81, DOI 10.1109/VISUAL.1997.663860
   Evans W, 2001, ALGORITHMICA, V30, P264, DOI 10.1007/s00453-001-0006-x
   FALBY JS, 1993, COMPUT GRAPH, V17, P65, DOI 10.1016/0097-8493(93)90052-B
   GARLAND M, 1999, EUROGRAPHICS STATE A
   Gerstner T, 2000, IEEE VISUAL, P259, DOI 10.1109/VISUAL.2000.885703
   Gerstner T, 2000, COMPUT GRAPH-UK, V24, P363, DOI 10.1016/S0097-8493(00)00032-7
   GERSTNER T, 1999, 29 I APPL MATH U BON
   GERSTNER T., 2003, TOP DOWN VIEW DEPEND
   GIGNONI P, 2003, P IEEE VISUALIZATION, P147
   Gobbetti E, 2006, COMPUT GRAPH FORUM, V25, P333, DOI 10.1111/j.1467-8659.2006.00952.x
   HEBERT DJ, 1995, P SOC PHOTO-OPT INS, V2569, P381, DOI 10.1117/12.217594
   Heckbert P. S., 1997, SIGGRAPH 97 COURSE N, V25
   HITCHNER LE, 1993, P S ELECT IMAGING, P1
   Hoppe H, 1998, VISUALIZATION '98, PROCEEDINGS, P35, DOI 10.1109/VISUAL.1998.745282
   Hwa LM, 2005, IEEE T VIS COMPUT GR, V11, P355, DOI 10.1109/TVCG.2005.65
   Koller D, 1995, VISUALIZATION '95 - PROCEEDINGS, P94, DOI 10.1109/VISUAL.1995.480800
   Lario Roberto., 2003, P VIIP 2003, P733
   Levenberg J, 2002, VIS 2002: IEEE VISUALIZATION 2002, PROCEEDINGS, P259, DOI 10.1109/VISUAL.2002.1183783
   Lindstrom P, 2002, IEEE T VIS COMPUT GR, V8, P239, DOI 10.1109/TVCG.2002.1021577
   Lindstrom P, 2001, IEEE VISUAL, P363, DOI 10.1109/VISUAL.2001.964533
   LINDSTROM P, 1997, 970 GVU GEORG TECH R
   LINDSTROM P, 1995, 9506 TR GRAPH VIS US
   LINDSTROM Peter., 1996, P ACM SIGGRAPH 96, P109
   Losasso F, 2004, ACM T GRAPHIC, V23, P769, DOI 10.1145/1015706.1015799
   Malvar H. S., 2000, Proceedings DCC 2000. Data Compression Conference, P243, DOI 10.1109/DCC.2000.838164
   Ohlberger M, 1999, IEEE T VIS COMPUT GR, V5, P74, DOI 10.1109/2945.764874
   Pajarola R, 1998, VISUALIZATION '98, PROCEEDINGS, P19, DOI 10.1109/VISUAL.1998.745280
   Pajarola R, 2002, VIS 2002: IEEE VISUALIZATION 2002, PROCEEDINGS, P395, DOI 10.1109/VISUAL.2002.1183800
   Pajarola R, 2001, INT J COMPUT GEOM AP, V11, P1, DOI 10.1142/S0218195901000389
   Pajarola R, 1998, PROC INT CONF DATA, P550, DOI 10.1109/ICDE.1998.655818
   PAJAROLA R, 1998, 292 DEPT COMP SCI
   Pomeranz A.A., 2000, THESIS U CALIFORNIA
   Puppo E, 1998, COMP GEOM-THEOR APPL, V11, P219, DOI 10.1016/S0925-7721(98)00029-7
   Puppo E., 1996, Proceedings of the 8th Canadian Conference on Computational Geometry, P202
   Reddy M, 1999, IEEE COMPUT GRAPH, V19, P30, DOI 10.1109/38.749120
   RIVARA MC, 1993, P 5 CAN C COMP GEOM, P42
   RIVARA MC, 1995, P 4 INT MESH ROUNDT, P335
   Samet H., 1989, APPL SPATIAL DATA ST
   Samet H., 1989, DESIGN ANAL SPATIAL
   Schneider J, 2006, JOURNAL WSCG, V14, P49
   SCHRACK G, 1992, CVGIP-IMAG UNDERSTAN, V55, P221, DOI 10.1016/1049-9660(92)90022-U
   SIVAN R, 1992, PROCEEDINGS : 5TH INTERNATIONAL SYMPOSIUM ON SPATIAL DATA HANDLING, VOLS 1 AND 2, P361
   SIVAN R, 1996, CSTR3609 U MAR COLL
   ULRICH T, 2000, SUPER SIZE IT SCALIN
   Velho L., 2000, Journal of Graphics Tools, V5, P35, DOI 10.1080/10867651.2000.10487526
   Velho L, 2000, COMPUT GRAPH FORUM, V19, P195, DOI 10.1111/1467-8659.00457
   VONHERZEN B, 1987, P ACM SIGGRAPH, P103
   WAHL R, 2004, J WSCG, V12, P521
   WLOKA M, 2004, PROGR GRAPHICS HARDW
NR 63
TC 100
Z9 129
U1 0
U2 10
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2007
VL 23
IS 8
BP 583
EP 605
DI 10.1007/s00371-007-0163-2
PG 23
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 189HE
UT WOS:000247979200005
OA Green Submitted, Green Published
DA 2024-07-18
ER

PT J
AU Muñoz, A
   Gutierrez, D
   Serón, FJ
AF Munoz, Adolfo
   Gutierrez, Diego
   Seron, Francisco J.
TI Optimization techniques for curved path computing
SO VISUAL COMPUTER
LA English
DT Article
DE non-linear media; atmospheric phenomena; optimization
AB Participating media with an inhomogeneous index of refraction make light follow curved paths. Simulating this in a global illumination environment has usually been neglected due to the complexity of the calculations involved, sacrificing accurate physical simulations for efficient visual results.
   This paper aims to simulate non-linear media in a more reasonable time than previous works without losing physical correctness. Accuracy is achieved by solving the Eikonal equation of geometrical optics, which describes the path followed by a light beam that traverses a non-linear medium. This equation is used in the context of a photon mapping extension.
   To improve the efficiency of the method, we study the existing correlation between numerical methods and the description of the non-linear medium, in terms of simulation time and error. Also, by taking advantage of several features of the scenes that include non-linear media, new optimization techniques that can be applied both for ray tracing and photon mapping will be developed. Flight or driving simulators could greatly benefit from this work.
C1 Univ Zaragoza, Grp Informat Grafica Avanzada GIGA, E-50009 Zaragoza, Spain.
C3 University of Zaragoza
RP Serón, FJ (corresponding author), Univ Zaragoza, Grp Informat Grafica Avanzada GIGA, E-50009 Zaragoza, Spain.
EM adolfo@unizar.es; diegog@unizar.es; seron@unizar.es
RI ; Seron Arbeloa, Francisco Jose/L-3146-2014
OI Munoz Orbananos, Adolfo/0000-0002-8160-7159; Seron Arbeloa, Francisco
   Jose/0000-0003-1683-4694
CR BERGER M, 1990, IEEE COMPUT GRAPH, V10, P36, DOI 10.1109/38.55151
   Burden R.L., 1988, Numerical Analysis, VFourth
   Dormand J., 1980, J. Comput. Appl. Math., V6, P19, DOI DOI 10.1016/0771-050X(80)90013-3
   Glassner A. S., 1995, Principles of Digital Image Synthesis
   GROLLER E, 1995, VISUAL COMPUT, V11, P263, DOI 10.1007/BF01901044
   GUTIERREZ D, 2004, SCCG 04 P 20 SPRING, P97
   GUTIERREZ D., 2005, Eurographics Symposium on Rendering, P291
   Gutierrez D, 2006, COMPUT GRAPH-UK, V30, P994, DOI 10.1016/j.cag.2006.05.002
   Hall R., 1989, ILLUMINATION COLOR C
   Jensen HW., 2001, REALISTIC IMAGE SYNT, DOI [10.1201/9780429294907, DOI 10.1201/9780429294907]
   LINU A, 2005, WSCG 2005 FULL PAP C, P79
   MUNOZ A, 2006, GRAPHITE 06 P 4 INT, P97
   MUSGRAVE FK, 1990, IEEE COMPUT GRAPH, V10, P10, DOI 10.1109/38.62692
   STAM J, 1996, P 7 EUR WORKSH REND, P225
   Tawara Takehiro., 2004, SCCG 04, P23
   *USGPC, 1976, US STAND ATM
   Wald I., 2002, Rendering Techniques 2002. Eurographics Workshop Proceedings, P15
NR 17
TC 3
Z9 4
U1 0
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2007
VL 23
IS 7
BP 493
EP 502
DI 10.1007/s00371-007-0122-y
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 182DL
UT WOS:000247485200005
DA 2024-07-18
ER

PT J
AU Cugini, U
   Bordegoni, M
AF Cugini, Umberto
   Bordegoni, Monica
TI Touch and design: novel haptic interfaces for the generation of high
   quality surfaces for industrial design
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 2006 HAPTEX Workshop
CY NOV 21-23, 2006
CL Natl Commun Assoc, Helsinki, FINLAND
HO Natl Commun Assoc
DE haptic interfaces; haptic modelling; virtual shape modelling; 3D
   interaction techniques
AB This paper presents the results of a research project aimed at developing haptic tools for virtual shape modelling resembling real tools like rakes and sandpaper used by modelers and designers in the real workshop. The developed system consists of a CAD (computer aided design) system enhanced with intuitive designer-oriented interaction tools and modalities. The system requirements have been defined on the basis of the observation of designers during their daily work, and translating the way they model shapes using hands and craft tools into specifications for the modelling system based on haptic tools.
C1 Politecn Milan, Dipartimento Meccan, I-20156 Milan, Italy.
C3 Polytechnic University of Milan
RP Cugini, U (corresponding author), Politecn Milan, Dipartimento Meccan, Via Masa 34, I-20156 Milan, Italy.
EM umberto.cugini@polimi.it; monica.bordegoni@polimi.it
OI Bordegoni, Monica/0000-0001-9378-8295
CR [Anonymous], 1996, FORCE TOUCH FEEDBACK
   AVILA RS, 1996, HAPTIC INTERACTION M, P197
   BORDEGONI M, 2006, VIRT REALITY J SPECI, V9
   BORDEGONI M, 2006, P DETC 06 ASME PHIL
   CANI M, 2006, COURS P ACM SIGGRAPH
   CELNIKER G, 1991, COMP GRAPH, V25, P257, DOI 10.1145/127719.122746
   Celniker George., 1992, SI3D 92, P165
   CHEN H, 2002, P ACM S VIRT REAL SO
   Dachille F, 2001, COMPUT AIDED DESIGN, V33, P403, DOI 10.1016/S0010-4485(00)00131-7
   DEWAELE G, 2004, EUROGRAPHICS 04
   Farin G., 1993, Curves and Surfaces for Computer Aiaded Geometric Design, V3
   Ferley E, 2001, GRAPH MODELS, V63, P459, DOI 10.1006/gmod.2001.0558
   GIRAUDO U, 2005, P ICMI 05 TRENT IT
   GROSSMAN T, 2003, P C HUM FACT COMP SY
   Hua J, 2004, IEEE T VIS COMPUT GR, V10, P574, DOI 10.1109/TVCG.2004.28
   IGARASHI T, 1999, SIGGRAPH 99
   Iwata H., 1993, Proceedings IEEE 1993 Symposium on Research Frontiers in Virtual Reality (Cat. No.93TH0585-0), P16, DOI 10.1109/VRAIS.1993.378268
   Jagnow R, 2002, PROC GRAPH INTERF, P125
   KRISHNAMURTHY V, 1996, SIGGRAPH 96
   MCDONNELL KT, 2001, ACM S INT 3D GRAPH
   Nealen A., 2005, EUROGRAPHICS
   Poitou J.-P., 1974, La dissonance cognitive
   SHENG J, 2006, P 4 INT C COMP GRAPH
   SINGH K, 2006, P ACM SIGGRAPH
   TERZOPOULOS D, 1988, COMPUTER GRAPHICS, P269
   WANG S, 1995, P S INT 3D, V3, P151
   WESCHE G, 2001, P ACM S VIRT ENV
   Yamada Yasusato., 1997, Clay Modeling: Techniques for Giving Three-dimensional Form to Idea, volume 93 1/2 of Car Styling
   Yang JZ, 2005, COMPUT AIDED GEOM D, V22, P1, DOI 10.1016/j.cagd.2004.08.002
   ZELEZNIK RC, SIGGRAPH 96
   Zhu WH, 2004, 12TH INTERNATIONAL SYMPOSIUM ON HAPTIC INTERFACES FOR VIRTUAL ENVIRONMENT AND TELEOPERATOR SYSTEMS, PROCEEDINGS, P98
NR 31
TC 7
Z9 7
U1 1
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2007
VL 23
IS 4
BP 233
EP 246
DI 10.1007/s00371-007-0105-z
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 172ZW
UT WOS:000246844300002
DA 2024-07-18
ER

PT J
AU Zadravec, M
   Zalik, B
AF Zadravec, M
   Zalik, B
TI An almost distribution-independent incremental Delaunay triangulation
   algorithm
SO VISUAL COMPUTER
LA English
DT Article
DE Delaunay triangulation; incremental algorithm; computational geometry;
   skip list; hash table
ID IMPROVEMENTS; CONSTRUCTION
AB This paper presents a new incremental insertion algorithm for constructing a Delaunay triangulation. Firstly, the nearest point is found in order to speed up the location of a triangle containing a currently inserted point. A hash table and 1-3 deterministic skip lists, combined with a walking strategy, are used for this task. The obtained algorithm is compared with the most popular Delaunay triangulation algorithms. The algorithm has the following attractive features: it is fast and practically independent of the distribution of input points, it is not memory demanding, and it is numerically stable and easy to implement.
C1 Univ Maribor, Fac Elect Engn & Comp Sci, SLO-2000 Maribor, Slovenia.
C3 University of Maribor
RP Univ Maribor, Fac Elect Engn & Comp Sci, Smetanova 17, SLO-2000 Maribor, Slovenia.
EM mirko.zadravec@uni-mb.si; zalik@uni-mb.si
RI Žalik, Borut/X-1320-2019
CR de Berg M., 2000, COMPUTATIONAL GEOMET
   DWYER RA, 1987, ALGORITHMICA, V2, P137, DOI 10.1007/BF01840356
   EDELSBRUNNER H, 1986, DISCRETE COMPUT GEOM, V1, P25, DOI 10.1007/BF02187681
   FANG TP, 1992, COMPUT AIDED DESIGN, V24, P425, DOI 10.1016/0010-4485(92)90010-8
   FANG TP, 1993, IEEE COMPUT GRAPH, V13, P36, DOI 10.1109/38.210490
   FORTUNE S, 1987, ALGORITHMICA, V2, P153, DOI 10.1007/BF01840357
   GUIBAS L, 1985, ACM T GRAPHIC, V4, P74, DOI 10.1145/282918.282923
   GUIBAS LJ, 1992, ALGORITHMICA, V7, P381, DOI 10.1007/BF01758770
   Huang CW, 1998, COMPUT GEOSCI, V24, P193, DOI 10.1016/S0098-3004(97)00136-2
   Kolingerová I, 2002, COMPUT GRAPH-UK, V26, P477, DOI 10.1016/S0097-8493(02)00090-0
   Lawson C. L., 1977, MATH SOFTWARE, P161, DOI DOI 10.1016/B978-0-12-587260-7.50011-X
   LEE DT, 1980, INT J COMPUT INF SCI, V9, P219, DOI 10.1007/BF00977785
   Mucke E. P., 1996, Proceedings of the Twelfth Annual Symposium on Computational Geometry, FCRC '96, P274, DOI 10.1145/237218.237396
   MUNRO JI, 1992, PROCEEDINGS OF THE THIRD ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P367
   PUGH W, 1990, COMMUN ACM, V33, P668, DOI 10.1145/78973.78977
   Shewchuk J., 1996, First Workshop on Applied Computational Geometry, P124
   SHEWCHUK JR, 2001, 2 DIMENSIONAL QUALIT
   SLOAN SW, 1987, ADV ENG SOFTW WORKST, V9, P34, DOI 10.1016/0141-1195(87)90043-X
   Su P., 1995, Proceedings of the Eleventh Annual Symposium on Computational Geometry, P61, DOI 10.1145/220279.220286
   Zalik B, 2003, INT J GEOGR INF SCI, V17, P119, DOI 10.1080/13658810210157813
NR 20
TC 10
Z9 15
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2005
VL 21
IS 6
BP 384
EP 396
DI 10.1007/s00371-005-0293-3
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 952GD
UT WOS:000230991100003
DA 2024-07-18
ER

PT J
AU Piegl, LA
   Ma, WY
   Tiller, W
AF Piegl, LA
   Ma, WY
   Tiller, W
TI An alternative method of curve interpolation
SO VISUAL COMPUTER
LA English
DT Article
DE shape design; curve interpolation; constrained shaping; NURBS
ID FREE-FORM SURFACES; SPLINE CURVES; PARAMETERIZATION; RECONSTRUCTION;
   APPROXIMATION; CONTINUITY; POINTS
AB Given a point data set that contains several fairly unevenly distributed random points, this paper presents a new paradigm of curve interpolation to fit a curve to the data with end tangent vector constraints. The method uses a base curve, which is subjected to constrained shape manipulations to achieve interpolation, while maintaining end point and end tangent constraints. The algorithm is not sensitive to the distribution or to the randomness of the data, as long as the points represent fairly simple shapes, as in reverse engineering of properly segmented points, or in shape design using simple segments. The method is iterative in nature and allows various forms of adjustments to achieve good results.
C1 City Univ Hong Kong, Dept Mfg Engn & Engn Management, Hong Kong, Hong Kong, Peoples R China.
   GeomWare Inc, Tyler, TX 75703 USA.
C3 City University of Hong Kong
RP City Univ Hong Kong, Dept Mfg Engn & Engn Management, 83 Tat Chee Ave, Hong Kong, Hong Kong, Peoples R China.
EM lap@cadconferences.com; mewma@cityu.edu.hk; geomware@gower.net
RI MA, Weiyin/K-9155-2015
OI MA, Weiyin/0000-0001-9760-7789
CR [Anonymous], 1996, REVERSE ENG
   Benko P, 2001, COMPUT AIDED DESIGN, V33, P839, DOI 10.1016/S0010-4485(01)00100-2
   CHIVATE PN, 1995, COMPUT INTEGR MANUF, V8, P193, DOI 10.1016/0951-5240(95)00012-I
   De Boor C, 1978, A Pratical Guide to Splines, V27
   Dierckx P., 1995, CURVE SURFACE FITTIN
   FANG L, 1992, VISUAL COMPUTING, P535
   Floater MS, 2001, COMPUT AIDED GEOM D, V18, P77, DOI 10.1016/S0167-8396(01)00013-9
   Floater MS, 1997, COMPUT AIDED GEOM D, V14, P231, DOI 10.1016/S0167-8396(96)00031-3
   FOWLER B, 1993, IEEE COMPUT GRAPH, V13, P43, DOI 10.1109/38.232098
   Greiner Gunther., 1997, Surface Fitting and Multiresolution Methods, P163
   Kruth JP, 1998, J MATER PROCESS TECH, V76, P120, DOI 10.1016/S0924-0136(97)00341-5
   LANE JM, 1980, IEEE T PATTERN ANAL, V2, P35, DOI 10.1109/TPAMI.1980.4766968
   Lee IK, 2000, COMPUT AIDED GEOM D, V17, P161, DOI 10.1016/S0167-8396(99)00044-8
   Ma WY, 1995, MATHEMATICAL METHODS FOR CURVES AND SURFACES, P315
   MA WY, 1995, COMPUT AIDED DESIGN, V27, P663, DOI 10.1016/0010-4485(94)00018-9
   MANNING JR, 1974, COMPUT J, V17, P181, DOI 10.1093/comjnl/17.2.181
   MILROY MJ, 1995, COMPUT AIDED DESIGN, V27, P471, DOI 10.1016/0010-4485(95)00020-R
   Piegl L., 1997, The Nurbs Book, Vsecond
   SARKAR B, 1991, COMPUT AIDED DESIGN, V23, P623, DOI 10.1016/0010-4485(91)90038-X
   Sarkar B., 1991, Computer-Aided Geometric Design, V8, P267, DOI 10.1016/0167-8396(91)90016-5
   Varady T, 1997, COMPUT AIDED DESIGN, V29, P255, DOI 10.1016/S0010-4485(96)00054-1
   Weiss V, 2002, COMPUT AIDED GEOM D, V19, P19, DOI 10.1016/S0167-8396(01)00086-3
   Werner A, 1998, J MATER PROCESS TECH, V76, P128, DOI 10.1016/S0924-0136(97)00340-3
NR 23
TC 3
Z9 4
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2005
VL 21
IS 1-2
BP 104
EP 117
DI 10.1007/s00371-004-0274-y
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 911OG
UT WOS:000228013300008
DA 2024-07-18
ER

PT J
AU García, AL
   de Miras, JR
   Feito, FR
AF García, AL
   de Miras, JR
   Feito, FR
TI Point in solid test for free-form solids defined with triangular Bezier
   patches
SO VISUAL COMPUTER
LA English
DT Article
DE free-form solid modeling; extended simplicial chains; triangular Bezier
   patches; point in solid test; computer graphics algorithms
ID SIMPLICIAL CHAINS
AB A new algorithm to study the fundamental point in solid test for free-form solids is presented. The test is performed efficiently and robustly because it does not use trigonometric functions and it makes it unnecessary to deal with complex special cases. The algorithm is based on considering the solid as composed of original tetrahedra and free-form cells, and it computes the test in two stages: evaluation of the point inclusion in these simple elements and merging of the results in a simple sum. The solid boundary is defined as a set of low-degree triangular Bezier patches.
C1 Univ Jaen, Dept Informat, Jaen 23071, Spain.
C3 Universidad de Jaen
RP García, AL (corresponding author), Univ Jaen, Dept Informat, Paraje Lagunilas S-N, Jaen 23071, Spain.
EM algarcia@ujaen.es; demiras@ujaen.es; ffeito@ujaen.es
RI García-Fernández, Ángel-Luis/E-4257-2012; Ruiz de Miras,
   Juan/W-6894-2018; Feito, Francisco/M-1672-2014
OI García-Fernández, Ángel-Luis/0000-0002-8183-7130; Ruiz de Miras,
   Juan/0000-0001-7579-8350; Feito, Francisco/0000-0001-8230-6529
CR [Anonymous], 1985, COMPUTATIONAL GEOMET, DOI DOI 10.1007/978-1-4612-1098-6
   [Anonymous], 1957, GEOMETRIC INTEGRATIO
   [Anonymous], 1984, COMPUT AIDED GEOM D
   de Miras JR, 1999, COMPUT GRAPH-UK, V23, P255, DOI 10.1016/S0097-8493(99)00035-7
   DEMIRAS JR, 1999, P WSCG 99 7 C COMP G, P241
   DEMIRAS JR, 2002, 1 INT S 3D DAT PROC
   DEMIRAS JR, 2001, THESIS U GRANADA GRA
   DEMIRAS JR, 2001, P 11 SPAN C COMP GRA
   DEMIRAS JR, 1998, P 8 SPAN C COMP GRAP
   Do Carmo P. M, 1976, DIFFERENTIAL GEOMETR
   Farin G., 1986, Computer-Aided Geometric Design, V3, P83, DOI 10.1016/0167-8396(86)90016-6
   Farin G., 1993, Curves and Surfaces for Computer Aiaded Geometric Design, V3
   Feito FR, 1998, COMPUT GRAPH, V22, P611, DOI 10.1016/S0097-8493(98)00067-3
   García AL, 2003, COMPUT GRAPH-UK, V27, P27, DOI 10.1016/S0097-8493(02)00222-4
   Hu CY, 1996, COMPUT AIDED DESIGN, V28, P807, DOI 10.1016/0010-4485(96)00013-9
   Hui KC, 1997, COMPUT AIDED DESIGN, V29, P771, DOI 10.1016/S0010-4485(97)00023-7
   Keyser J., 1997, Proceedings. Fourth Symposium on Solid Modeling and Applications, P42, DOI 10.1145/267734.267753
   KOLB A, 1995, TR19952 U ERL
   KRISHNAN S, 1996, P CSG 96, P101
   Piper B.R., 1987, GEOMETRIC MODELING A, P221
   Shirman L. A., 1987, Computer-Aided Geometric Design, V4, P279, DOI 10.1016/0167-8396(87)90003-3
   Vlachos A., 2001, P 2001 S INTERACTIVE, P159, DOI [DOI 10.1145/364338.364387, 10.1145/364338.364387]
   YING JQ, 1991, SYSTEMS COMPUTERS JA, V22, P1823
NR 23
TC 1
Z9 1
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA 233 SPRING STREET, NEW YORK, NY 10013 USA
SN 0178-2789
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2004
VL 20
IS 5
BP 298
EP 313
DI 10.1007/s00371-004-0239-1
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 842GL
UT WOS:000222991600002
DA 2024-07-18
ER

PT J
AU Ascher, UM
   Boxerman, E
AF Ascher, UM
   Boxerman, E
TI On the modified conjugate gradient method in cloth simulation
SO VISUAL COMPUTER
LA English
DT Article
DE cloth simulation; constraints; orthogonal projection; conjugate
   gradients; implicit time stepping
AB The seminal paper on cloth simulation by Baraff and Witkin [4] presents a modified preconditioned conjugate gradient (MPCG) algorithm for solving certain large, sparse systems of linear equations. These arise when employing implicit time integration methods aimed at achieving large step cloth simulation in the presence of constraints. This paper improves the robustness and efficiency of this MPCG algorithm. We prove convergence. For this, we recast the algorithm into a linear algebra setting, identifying its filtering procedure as an orthogonal projection. This leads not only to a convergence proof but also to a correction in the initiation stage of the original algorithm that improves its efficiency. We give an example to illustrate the performance improvement offered by this correction.
C1 Univ British Columbia, Dept Comp Sci, Vancouver, BC V6T 1Z4, Canada.
C3 University of British Columbia
RP Ascher, UM (corresponding author), Univ British Columbia, Dept Comp Sci, Vancouver, BC V6T 1Z4, Canada.
CR Aruliah DA, 2001, MATH MOD METH APPL S, V11, P1, DOI 10.1142/S0218202501000702
   Ascher U. M., 1998, Computer methods for ordinary differential equations and differential-algebraic equations, V61, DOI DOI 10.1137/1.9781611971392
   Axelsson O., 1984, FINITE ELEMENT SOLUT
   Baraff D., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P43, DOI 10.1145/280814.280821
   Barrett R, 1994, TEMPLATES SOLUTION L, DOI DOI 10.1137/1.9781611971538
   BRIDSON R, 2002, P SIGGRAPH SAN ANT T
   CHOI KJ, 2002, P SIGGRAPH SAN ANT T
   CORDIER F, 2002, P EUR SAARBR GERM 2
   COURSHESNES M, 1995, P SIGGRAPH 95 AUG, P137
   Golub G., 1988, MATRIX COMPUTATIONS
   Saad Y., 1996, Iterative Methods for Sparse Linear Systems
NR 11
TC 23
Z9 30
U1 1
U2 1
PU SPRINGER-VERLAG
PI NEW YORK
PA 175 FIFTH AVE, NEW YORK, NY 10010 USA
SN 0178-2789
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2003
VL 19
IS 7-8
BP 526
EP 531
DI 10.1007/s00371-003-0220-4
PG 6
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 749YL
UT WOS:000186957600009
DA 2024-07-18
ER

PT J
AU Jin, HL
   Favaro, P
   Soatto, S
AF Jin, HL
   Favaro, P
   Soatto, S
TI A semi-direct approach to structure from motion
SO VISUAL COMPUTER
LA English
DT Article
DE structure from motion; direct methods; extended Kalman filter;
   observability; tracking
ID SHAPE
AB The problem of structure from motion is often decomposed into two steps: feature correspondence and three-dimensional reconstruction. This separation often causes gross errors when establishing correspondence fails. Therefore, we advocate the necessity to integrate visual information not only in time (i.e. across different views), but also in space, by matching regions - rather than points - using explicit photometric deformation models. We present an algorithm that integrates image-feature tracking and three-dimensional motion estimation into a closed loop, while detecting and rejecting outlier regions that do not fit the model. Due to occlusions and the causal nature of our algorithm, a drift in the estimates accumulates over time. We describe a method to perform global registration of local estimates of motion and structure by matching the appearance of feature regions stored over long time periods. We use image intensities to construct a score function that takes into account changes in brightness and contrast. Our algorithm is recursive and suitable for real-time implementation.
C1 Washington Univ, Dept Elect Engn, St Louis, MO 63130 USA.
   Univ Calif Los Angeles, Dept Comp Sci, Los Angeles, CA 90024 USA.
C3 Washington University (WUSTL); University of California System;
   University of California Los Angeles
RP Washington Univ, Dept Elect Engn, St Louis, MO 63130 USA.
CR ADIV G, 1985, IEEE T PATTERN ANAL, V7, P384, DOI 10.1109/TPAMI.1985.4767678
   ALON J, 2000, IEEE COMPUT VISION P, V2, P550
   [Anonymous], IEEE WORKSH REPR VIS
   AZARBAYEJANI A, 1995, IEEE T PATTERN ANAL, V17, P562, DOI 10.1109/34.387503
   Bartlett M.S., 1956, An Introduction to Stochastic Processes
   BROIDA TJ, 1986, IEEE T PATTERN ANAL, V8, P90, DOI 10.1109/TPAMI.1986.4767755
   Chiuso A, 2000, INT J COMPUT VISION, V39, P195, DOI 10.1023/A:1026563712076
   Dellaert F, 2000, PROC CVPR IEEE, P557, DOI 10.1109/CVPR.2000.854916
   DICKMANNS ED, 1988, MACH VISION APPL, V1, P241
   HANNA KJ, 1991, WORKSH VIS MOT, P156
   JIN H, 2000, P IEEE COMPUT VISION, V2, P778
   Jin HL, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P684, DOI 10.1109/ICCV.2001.937588
   MATTISON DR, 1989, REPROD TOXICOL, V3, P3, DOI 10.1016/0890-6238(89)90032-4
   McLauchlan PF, 1999, IEEE WORKSHOP ON MULTI-VIEW MODELING & ANALYSIS OF VISUAL SCENES (MVIEW'99). PROCEEDINGS, P37, DOI 10.1109/MVIEW.1999.781081
   MCLAUCHLAN PF, 2000, INT C COMP VIS PATT, V2, P738
   Oliensis J, 2000, IEEE T PATTERN ANAL, V22, P685, DOI 10.1109/34.865186
   PHILIP J, 1991, IEEE T PATTERN ANAL, V13, P61, DOI 10.1109/34.67631
   Poelman CJ, 1997, IEEE T PATTERN ANAL, V19, P206, DOI 10.1109/34.584098
   Rahimi A, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P315, DOI 10.1109/ICCV.2001.937535
   SAWHNEY HS, 1994, INT C PATT RECOG, P403, DOI 10.1109/ICPR.1994.576308
   Soatto S, 1997, AUTOMATICA, V33, P1287, DOI 10.1016/S0005-1098(97)00048-4
   Soatto S, 1998, IEEE T PATTERN ANAL, V20, P933, DOI 10.1109/34.713360
   SOATTO S, 1994, OBSERVABILITY IDENTI, P3235
   SPETAKIS ME, 1988, IEEE 2 INT C COMP VI, P449
   STURM PF, 2000, IEEE COMPUT VISION P, V1, P706
   THOMAS JI, 1992, IMAGE UNDERSTANDING WORKSHOP : PROCEEDINGS OF A WORKSHOP, P507
   TOMASI C, 1992, INT J COMPUT VISION, V9, P137, DOI 10.1007/BF00129684
   WENG JY, 1991, IEEE T SIGNAL PROCES, V39, P2691, DOI 10.1109/78.107418
   WENG JY, 1993, IEEE T PATTERN ANAL, V15, P864, DOI 10.1109/34.232074
   Xu G, 2000, PROC CVPR IEEE, P474, DOI 10.1109/CVPR.2000.854886
NR 30
TC 51
Z9 58
U1 1
U2 10
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2003
VL 19
IS 6
BP 377
EP 394
DI 10.1007/s00371-003-0202-6
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 745PC
UT WOS:000186696200003
DA 2024-07-18
ER

PT J
AU Jansson, J
   Vergeest, JSM
AF Jansson, J
   Vergeest, JSM
TI Combining deformable- and rigid-body mechanics simulation
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Computer Animation Conference (CA2001)
CY NOV, 2001
CL SEOUL NATL UNIV, SEOUL, SOUTH KOREA
HO SEOUL NATL UNIV
DE mechanics model; deformation; collision; deformable bodies; geometric
   modeling; conceptual design; rigid bodies
AB We present an interface between a deformable-body mechanics model and a rigid-body mechanics model. What is novel with our approach is that the physical representation in both the models is the same, which ensures behavioral correctness and allows great flexibility. We use a mass-spring representation extended with the concept of volume, and thus contact and collision. All physical interaction occurs between the mass elements only; thus there is no need for the explicit handling of interaction between rigid and deformable bodies or between rigid and rigid bodies. This also means that bodies can be partially rigid and partially deformable. It is also possible to change dynamically whether part of a body should be rigid or not. We present a demonstration example and possible applications in conceptual design engineering, geometric modeling, as well as computer animation.
C1 Uppsala Univ, Fac Informat Technol, Uppsala, Sweden.
   Delft Univ Technol, Fac Design Engn & Prod, NL-2600 AA Delft, Netherlands.
C3 Uppsala University; Delft University of Technology
RP Uppsala Univ, Fac Informat Technol, Uppsala, Sweden.
CR [Anonymous], P 25 ANN C COMP GRAP
   Baraff D., 1997, CMURITR9733
   BARAFF D, 1992, P 19 ANN C COMP GRAP
   BARENBRUG B, 2000, THESIS TU EINDHOVEN
   CHEN Y, 1998, P IEEE COMP AN PHIL
   DONGARRA J, 1993, P 1993 ACM IEEE C SU
   GANOVELLI F, 2000, EUR 2000 INT SWITZ 2
   Goldstein H., 1950, CLASSICAL MECH
   Heath Michael T, 2018, Scientific Computing: An Introductory Survey
   HORVATH I, 1998, P TMCE 98 TOOLS METH
   Jansson J, 2002, COMPUT AIDED DESIGN, V34, P913, DOI 10.1016/S0010-4485(01)00146-4
   JANSSON J, 2001, P 15 EUR SIM MULT 20
   JANSSON J, 2000, P ASME DES ENG TECHN
   Kang HS, 1996, COMPUT AIDED DESIGN, V28, P251, DOI 10.1016/0010-4485(95)00029-1
   KELLER H, 1993, VIRTUAL MECH SIMULAT
   KLEPPNER D, 1978, INTRO MECH
   O'Brien JF, 2000, IEEE COMPUT GRAPH, V20, P86, DOI 10.1109/38.851756
   PROVOT X, 1995, P GRAPHICS INTERFACE
   TERZOPOULOUS D, 1987, P 1J ANN C COMP GRAP
NR 19
TC 12
Z9 17
U1 1
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2003
VL 19
IS 5
BP 280
EP 290
DI 10.1007/s00371-002-0187-6
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 726KP
UT WOS:000185598500002
DA 2024-07-18
ER

PT J
AU Jaszkowski, D
   Rzeszut, J
AF Jaszkowski, D
   Rzeszut, J
TI Interference colours of soap bubbles
SO VISUAL COMPUTER
LA English
DT Article
DE interference colours; thin film; soap bubble; thickness of the film;
   spectral reflectivity of the film
ID FILMS
AB In this article, we present a new versatile method for calculating the interference colour of any configuration of thin, optically isotropic, dielectric films. This method is free from common weakness of previous methods, namely the poor accuracy for large angles of incidence of light. This method gives full control over the accuracy and speed of computations, and allows for adjustments of the parameters according to the physical and chemical properties of the film, in order to present the optimal results. We also present a formula to model the thickness of thin films made up of liquids, such as soap bubbles.
C1 Warsaw Univ Technol, Inst Comp Sci, PL-00665 Warsaw, Poland.
C3 Warsaw University of Technology
RP Warsaw Univ Technol, Inst Comp Sci, Nowowiejska 15-19, PL-00665 Warsaw, Poland.
CR ADAMSON AW, 1960, PHYSICAL CHEM SURFAC
   ALMGREN FJ, 1976, SCI AM, V235, P82, DOI 10.1038/scientificamerican0776-82
   DIAS ML, 1991, IEEE COMPUT GRAPH, V11, P54, DOI 10.1109/38.75591
   DIAS ML, 1994, IEEE COMPUT GRAPH, V14, P17, DOI 10.1109/38.279038
   Durikovic R, 2001, COMPUT GRAPH FORUM, V20, pC67
   Glassner A, 2000, IEEE COMPUT GRAPH, V20, P99, DOI 10.1109/38.888023
   Glassner A, 2000, IEEE COMPUT GRAPH, V20, P76, DOI 10.1109/38.865884
   Hall R., 1989, ILLUMINATION COLOR C
   Hirayama H, 2001, COMPUT GRAPH-UK, V25, P391, DOI 10.1016/S0097-8493(01)00063-2
   Hirayama H, 2001, VISUAL COMPUT, V17, P106, DOI 10.1007/PL00013402
   Icart I, 1999, COMPUT GRAPH-UK, V23, P405, DOI 10.1016/S0097-8493(99)00048-5
   Isenberg C., 1992, The Science of Soap Films and Soap Bubbles
   Jenkins F. A., 1976, FUNDAMENTALS OPTICS
   SULLIVAN J, 1993, VISUALIZATION SOAP B, P79
   Sun YL, 1999, IEEE COMPUT GRAPH, V19, P61, DOI 10.1109/38.773965
   Wyszecki G., 1967, COLOR SCI CONCEPTS M
NR 16
TC 11
Z9 12
U1 2
U2 29
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2003
VL 19
IS 4
BP 252
EP 270
DI 10.1007/s00371-002-0195-6
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 713KV
UT WOS:000184857800004
DA 2024-07-18
ER

PT J
AU Zhou, CY
   Wang, XC
   Wu, ZK
AF Zhou, Ciyang
   Wang, Xingce
   Wu, Zhongke
TI A novel deformable B-spline curve model based on elasticity
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Elasticity; Physically based models; Isogeometric analysis; B-spline
   curve
ID SMOOTHED PARTICLE HYDRODYNAMICS; INTERACTIVE SIMULATION; SURFACES;
   NURBS; RODS
AB The physically based deformable curve models are widely used to simulate thin one-dimensional objects in computer graphics, interactive simulation, and surgery simulation. These models consider objects to be rods described by an adapted frame curve that contains the rod's centerline as well as the orthonormal material frame of each point on the centerline. However, they pose challenges including fine discretization, redundancy in modeling slender rods, and maintaining accuracy and stability. In this paper, we propose a novel physically based deformable B-spline curve model that regards curves as rods consisting of parallel fibers and derive elastic potential energy only from curves' representations. Therefore, our model does not take rotation-based adapted frames into consideration and reduces degree of freedom. Our model divides the curves into infinitesimal elements in parameter space and derives the analytical relationship between elastic potential energy function and curves' representations through the change of total length of infinitesimal elements' fibers. Our model can support material attributes in the real world and maintain the reality and stability of the solution. We employ isogeometric analysis to solve the dynamic equations derived from our deformable model as isogeometric analysis is suitable to solve the dynamic equations of parametric models. We compare the scenarios in the real world, our model's simulation results, and other model's results to demonstrate the reality of our models. The results are in line with expectation. We design several examples to demonstrate our models' applications.
C1 [Zhou, Ciyang; Wang, Xingce; Wu, Zhongke] Beijing Normal Univ, Sch Artificial Intelligence, 19 Xinjiekouwai St, Beijing 100875, Peoples R China.
C3 Beijing Normal University
RP Wu, ZK (corresponding author), Beijing Normal Univ, Sch Artificial Intelligence, 19 Xinjiekouwai St, Beijing 100875, Peoples R China.
EM 201831210008@mail.bnu.edu.cn; wangxingce@bnu.edu.cn; zwu@bnu.edu.cn
RI li, jixiang/JXN-7599-2024; you, li/KHW-2201-2024; chen,
   bin/KBQ-8114-2024; liu, qi/KFA-4047-2024; li, guo/KHU-1749-2024; JIANG,
   Peng/KGL-3427-2024; Han, Yang/JVN-5921-2024; yang, zhou/KBB-6972-2024;
   zhang, hao/KEJ-2291-2024
OI chen, bin/0000-0002-3398-1314; 
FU National Nature Science Foundation of China
FX No Statement Available
CR Barr AH., 1987, READINGS COMPUTER VI, P661
   Belytschko T, 1996, COMPUT METHOD APPL M, V139, P3, DOI 10.1016/S0045-7825(96)01078-X
   Bergou M, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360662
   Bombarde DS, 2022, COMPUT METHOD APPL M, V394, DOI 10.1016/j.cma.2022.114920
   Carrera E, 2017, INT J NUMER METH ENG, V112, P1889, DOI 10.1002/nme.5588
   CATMULL E, 1978, COMPUT AIDED DESIGN, V10, P350, DOI 10.1016/0010-4485(78)90110-0
   Coquillart S., 1990, SIGGRAPH, V90, P187
   Cottrell J.A., 2009, Isogeometric Analysis: Toward Integration of CAD and FEA, DOI [10.1002/9780470749081, DOI 10.1002/9780470749081]
   COVER SA, 1993, IEEE COMPUT GRAPH, V13, P68, DOI 10.1109/38.252559
   Deul C, 2018, COMPUT GRAPH FORUM, V37, P313, DOI 10.1111/cgf.13326
   DOO D, 1978, COMPUT AIDED DESIGN, V10, P356, DOI 10.1016/0010-4485(78)90111-2
   DYN N, 1991, CONSTR APPROX, V7, P127, DOI 10.1007/BF01888150
   GINGOLD RA, 1977, MON NOT R ASTRON SOC, V181, P375, DOI 10.1093/mnras/181.3.375
   Gregoire M., 2006, P 2006 ACM S SOLID P, P95
   Grégoire M, 2007, COMPUT AIDED DESIGN, V39, P694, DOI 10.1016/j.cad.2007.05.005
   Gunakala SR, 2012, Int J Appl Sci Technol, V2, P80
   Hadap S, 2001, COMPUT GRAPH FORUM, V20, pC329, DOI 10.1111/1467-8659.00525
   Hornus S, 2003, VISUAL COMPUT, V19, P94, DOI 10.1007/s00371-002-0179-6
   Hughes TJR, 2005, COMPUT METHOD APPL M, V194, P4135, DOI 10.1016/j.cma.2004.10.008
   Jiang JW, 2020, GRAPH MODELS, V111, DOI 10.1016/j.gmod.2020.101077
   KASS M, 1987, INT J COMPUT VISION, V1, P321, DOI 10.1007/BF00133570
   Korner K, 2021, P ROY SOC A-MATH PHY, V477, DOI 10.1098/rspa.2021.0561
   Langer J, 1996, SIAM REV, V38, P605, DOI 10.1137/S0036144593253290
   Lenoir Julien., 2002, ESAIM P, V12, P102, DOI DOI 10.1051/PROC:2002017
   Li X., 2013, Math. Probl. Eng., V2013
   Monaghan JJ, 2012, ANNU REV FLUID MECH, V44, P323, DOI 10.1146/annurev-fluid-120710-101220
   Pai DK, 2002, COMPUT GRAPH FORUM, V21, P347, DOI 10.1111/1467-8659.00594
   Panneerselvam K, 2020, COMPUT MECH, V65, P269, DOI 10.1007/s00466-019-01768-2
   Poelert S, 2013, P I MECH ENG H, V227, P464, DOI 10.1177/0954411912467884
   Prusinkiewicz P., 2003, International Journal of Shape Modeling, V9, P41, DOI 10.1142/S0218654303000048
   Sano TG, 2022, J MECH PHYS SOLIDS, V160, DOI 10.1016/j.jmps.2021.104739
   Sederberg T. W., 1986, Computer Graphics, V20, P151, DOI 10.1145/15886.15903
   Selle A, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360663
   Soler C, 2018, COMPUT GRAPH FORUM, V37, P137, DOI 10.1111/cgf.13519
   Spetz A., 2017, Geomaterials, V7, P96
   TERZOPOULOS D, 1994, ACM T GRAPHIC, V13, P103, DOI 10.1145/176579.176580
   Terzopoulos D., 1988, Visual Computer, V4, P306, DOI 10.1007/BF01908877
   Terzopoulos D., 1987, COMPUT GRAPH, P205, DOI DOI 10.1145/37402.37427
   Theetten A, 2008, COMPUT AIDED DESIGN, V40, P35, DOI 10.1016/j.cad.2007.05.008
   Timoshenko S., 1941, Advanced theory and problems, V245
   Umetani Nobuyuki., 2014, ACM SIGGRAPH 2014 Talks, SIGGRAPH '14, p47:1
   Nguyen VP, 2015, MATH COMPUT SIMULAT, V117, P89, DOI 10.1016/j.matcom.2015.05.008
   Wang S, 2014, OCEAN ENG, V78, P73, DOI 10.1016/j.oceaneng.2013.12.008
   Weeger O, 2017, COMPUT METHOD APPL M, V316, P100, DOI 10.1016/j.cma.2016.05.009
   Whewell W., 1847, Analytical Statics. A Supplement to the Fourth Edition of an Elementary Treatise on Mechanics
   Yang XN, 2023, COMPUT AIDED GEOM D, V104, DOI 10.1016/j.cagd.2023.102207
   Zeng ZP, 2022, SOIL DYN EARTHQ ENG, V153, DOI 10.1016/j.soildyn.2021.107066
   Zienkiewicz OC, 2005, FINITE ELEMENT METHOD FOR FLUID DYNAMICS, 6TH EDITION, P1
NR 48
TC 0
Z9 0
U1 2
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2023 DEC 2
PY 2023
DI 10.1007/s00371-023-03155-8
EA DEC 2023
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA Z9QB1
UT WOS:001115341900002
DA 2024-07-18
ER

PT J
AU Lin, ZN
   Li, YJ
   Deng, CY
AF Lin, Zinan
   Li, Yajuan
   Deng, Chongyang
TI Interpolating meshes of arbitrary topology by Catmull-Clark surfaces
   with energy constraint
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Subdivision surfaces; Surface energy; Interpolation; Catmull-Clark
   subdivision
ID SUBDIVISION
AB We propose an efficient method with energy constraints for constructing a Catmull-Clark surface that interpolates a given mesh. We approximate the surface energy of Catmull-Clark surfaces near extraordinary points by summing their finite subpatches and then represent the energy of the subpatches as linear combinations of the vertices of control mesh. By minimizing the surface energy as a constraint, we generate a new control mesh whose limit surfaces interpolate a given mesh. Numerous examples and comparisons demonstrate that our method has the following characteristics: (1) The limit surfaces are fairer, reducing unnecessary undulations and having minimal surface energy, and (2) the approximation process is simple and intuitive, requiring only a small number of computational steps and avoiding complex parameterization processes.
C1 [Lin, Zinan; Li, Yajuan; Deng, Chongyang] Hangzhou Dianzi Univ, Sch Sci, Hangzhou 310018, Peoples R China.
C3 Hangzhou Dianzi University
RP Deng, CY (corresponding author), Hangzhou Dianzi Univ, Sch Sci, Hangzhou 310018, Peoples R China.
EM 211070046@hdu.edu.cn; liyajuan@hdu.edu.cn; dcy@hdu.edu.cn
RI Deng, Chongyang/E-4422-2017
OI Deng, Chongyang/0000-0002-8725-4622
FU National Natural Science Foundation of China [61872121]; National
   Natural Science Foundation of China (NSFC)
FX The authors wish to thank all anonymous referees for their valuable
   comments and suggestions. This work was supported by the National
   Natural Science Foundation of China (NSFC) under the project numbers
   61872121.
CR Brunet P., 1988, Computer-Aided Geometric Design, V5, P41, DOI 10.1016/0167-8396(88)90019-2
   CATMULL E, 1978, COMPUT AIDED DESIGN, V10, P350, DOI 10.1016/0010-4485(78)90110-0
   CELNIKER G, 1991, COMP GRAPH, V25, P257, DOI 10.1145/127719.122746
   Chen ZX, 2008, COMPUT GRAPH FORUM, V27, P1823, DOI 10.1111/j.1467-8659.2008.01328.x
   Claes J, 2001, INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, PROCEEDING, P42, DOI 10.1109/SMA.2001.923374
   Deng CY, 2010, VISUAL COMPUT, V26, P137, DOI 10.1007/s00371-009-0393-6
   DOO D, 1978, COMPUT AIDED DESIGN, V10, P356, DOI 10.1016/0010-4485(78)90111-2
   DYN N, 1990, ACM T GRAPHIC, V9, P160, DOI 10.1145/78956.78958
   Farin G., 1993, Pract. Guide, V55, P96
   Hai Z., 2005, J. Southeast Univ, V21, P453
   Halstead M., 1993, Computer Graphics Proceedings, P35, DOI 10.1145/166117.166121
   Kobbelt L, 1996, COMPUT GRAPH FORUM, V15, pC409, DOI 10.1111/1467-8659.1530409
   Lai SH, 2006, VISUAL COMPUT, V22, P865, DOI 10.1007/s00371-006-0072-9
   Li G, 2005, COMPUT GRAPH FORUM, V24, P3, DOI 10.1111/j.1467-8659.2005.00824.x
   Loop C, 1987, THESIS U UTAH
   Maekawa T, 2007, COMPUT AIDED DESIGN, V39, P313, DOI 10.1016/j.cad.2006.12.008
   NASRI AH, 1987, ACM T GRAPHIC, V6, P29, DOI 10.1145/27625.27628
   Stam J., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P395, DOI 10.1145/280814.280945
   Zheng JM, 2006, IEEE T VIS COMPUT GR, V12, P301, DOI 10.1109/TVCG.2006.49
   Zorin D., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P189, DOI 10.1145/237170.237254
   Zorin D., 2000, Course Notes of SIGGRAPH
NR 21
TC 0
Z9 0
U1 1
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2023 NOV 25
PY 2023
DI 10.1007/s00371-023-03154-9
EA NOV 2023
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA Y6UT1
UT WOS:001106597500001
DA 2024-07-18
ER

PT J
AU Feng, QH
   Shao, ZW
   Wang, ZX
AF Feng, Qihan
   Shao, Zhiwen
   Wang, Zhixiao
TI Boundary-aware small object detection with attention and interaction
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Boundary-aware network; Spatial interaction; Attentional feature
   parallel fusion; Small object detection
AB Object detection is a critical technology for the intelligent analytical processing of images captured by drones. The objects usually come in various scales and can be extremely small. Existing detection methods are inherently based on pyramid hierarchy architectures to extract multi-scale features and provide better feature representation for small objects. Nevertheless, they inevitably dilute the representation of details in low-level features during top-down feature fusion and are totally unconcerned with whether the fused feature fits the objects of specific scales within a layer. Moreover, the pyramid can only implicitly fuse the spatial context, which makes the fused features cannot receive fine spatial location information for object localization. In this work, we propose an effective boundary-aware network with attention refinement and spatial interaction to tackle the above challenges. Specifically, we first present a highly effective yet simple boundary-aware detection head (BAH), which directly guides representation learning of object structure semantics in the prediction layer to preserve object-related boundary semantics. Additionally, the attentional feature parallel fusion (AFPF) module offers multi-scale feature encoding capability in a parallel triple fusion fashion and adaptively selects features appropriate for objects of certain scales. Furthermore, we design a spatial interactive module (SIM) to preserve fine spatial detail through cross-spatial feature association. Extensive experiments prove that the proposed network significantly outperforms the state-of-the-art methods, in which we achieve 33.1 mAP and 56.5 AP50 on the VisDrone benchmark, 63.4 mAP and 94 AP50 on the NWPU VHR-10 benchmark. The source code will be released.
C1 [Feng, Qihan; Shao, Zhiwen; Wang, Zhixiao] China Univ Min & Technol, Sch Comp Sci & Technol, Xuzhou 221116, Peoples R China.
   [Shao, Zhiwen; Wang, Zhixiao] Minist Educ Peoples Republ China, Engn Res Ctr Mine Digitizat, Xuzhou 221116, Peoples R China.
   [Shao, Zhiwen] Shanghai Jiao Tong Univ, Dept Comp Sci & Engn, Shanghai 200240, Peoples R China.
C3 China University of Mining & Technology; Shanghai Jiao Tong University
RP Wang, ZX (corresponding author), China Univ Min & Technol, Sch Comp Sci & Technol, Xuzhou 221116, Peoples R China.; Wang, ZX (corresponding author), Minist Educ Peoples Republ China, Engn Res Ctr Mine Digitizat, Xuzhou 221116, Peoples R China.
EM zhxwang@cumt.edu.cn
RI Shao, Zhiwen/N-8985-2018
FU This work was supported by the National Natural Science Foundation of
   China (Nos. 61876186, 62106268), the Xuzhou Science and Technology
   Project (No. KC21300), the China Postdoctoral Science Foundation (No.
   2023M732223), and the Talent Program for Deputy G [61876186, 62106268];
   National Natural Science Foundation of China [KC21300]; Xuzhou Science
   and Technology Project [2023M732223]; China Postdoctoral Science
   Foundation [FZ20220440]; Talent Program for Deputy General Manager of
   Science and Technology of Jiangsu Province [2023WLKXJ178]; Graduate
   Innovation Program of China University of Mining and Technology
   [KYCX23_2727]; Postgraduate Research amp; Practice Innovation Program of
   Jiangsu Province
FX This work was supported by the National Natural Science Foundation of
   China (Nos. 61876186, 62106268), the Xuzhou Science and Technology
   Project (No. KC21300), the China Postdoctoral Science Foundation (No.
   2023M732223), and the Talent Program for Deputy General Manager of
   Science and Technology of Jiangsu Province (No. FZ20220440). It was also
   partially supported by the Graduate Innovation Program of China
   University of Mining and Technology (No. 2023WLKXJ178), and the
   Postgraduate Research & Practice Innovation Program of Jiangsu Province
   (No. KYCX23_2727).
CR Akyon FC, 2022, IEEE IMAGE PROC, P966, DOI 10.1109/ICIP46576.2022.9897990
   Cai ZW, 2018, PROC CVPR IEEE, P6154, DOI 10.1109/CVPR.2018.00644
   Carion Nicolas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P213, DOI 10.1007/978-3-030-58452-8_13
   Chen PY, 2021, IEEE T IMAGE PROCESS, V30, P9099, DOI 10.1109/TIP.2021.3118953
   Chen ZH, 2022, COMPUT ANIMAT VIRT W, V33, DOI 10.1002/cav.2061
   Chen ZH, 2021, VISUAL COMPUT, V37, P2657, DOI 10.1007/s00371-021-02199-y
   Cheng G, 2014, ISPRS J PHOTOGRAMM, V98, P119, DOI 10.1016/j.isprsjprs.2014.10.002
   Colomina I, 2014, ISPRS J PHOTOGRAMM, V92, P79, DOI 10.1016/j.isprsjprs.2014.02.013
   Dong Zhang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12373), P323, DOI 10.1007/978-3-030-58604-1_20
   Du BW, 2023, PROC CVPR IEEE, P13435, DOI 10.1109/CVPR52729.2023.01291
   Duan CZ, 2021, IEEE INT CONF COMP V, P2789, DOI 10.1109/ICCVW54120.2021.00313
   Erdelj M, 2017, IEEE PERVAS COMPUT, V16, P24, DOI 10.1109/MPRV.2017.11
   Ertugrul E, 2020, COMPUT ANIMAT VIRT W, V31, DOI 10.1002/cav.1959
   Ghiasi G, 2019, PROC CVPR IEEE, P7029, DOI 10.1109/CVPR.2019.00720
   Gong YQ, 2021, IEEE WINT CONF APPL, P1159, DOI 10.1109/WACV48630.2021.00120
   He KM, 2015, IEEE T PATTERN ANAL, V37, P1904, DOI 10.1109/TPAMI.2015.2389824
   He X, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2022.3144165
   Hird JN, 2017, REMOTE SENS-BASEL, V9, DOI 10.3390/rs9050413
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang S, 2023, VISUAL COMPUT, V39, P3647, DOI 10.1007/s00371-023-02938-3
   Jocher Glenn, 2021, Zenodo
   Li CL, 2020, IEEE COMPUT SOC CONF, P737, DOI 10.1109/CVPRW50498.2020.00103
   Li JJ, 2022, IEEE T IND INFORM, V18, P163, DOI 10.1109/TII.2021.3085669
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Ling H., 2018, Vision Meets Drones: A Challenge
   Liu S, 2018, PROC CVPR IEEE, P8759, DOI 10.1109/CVPR.2018.00913
   Liu Z, 2022, PROC CVPR IEEE, P11999, DOI 10.1109/CVPR52688.2022.01170
   Mei YQ, 2021, PROC CVPR IEEE, P3516, DOI 10.1109/CVPR46437.2021.00352
   Meng DD, 2023, VISUAL COMPUT, V39, P3183, DOI 10.1007/s00371-023-02965-0
   Nazir A, 2022, IEEE T IMAGE PROCESS, V31, P880, DOI 10.1109/TIP.2021.3136619
   Ping Zhao, 2021, Image and Graphics: 11th International Conference, ICIG 2021, Proceedings. Lecture Notes in Computer Science, Image Processing, Computer Vision, Pattern Recognition, and Graphics (12888), P148, DOI 10.1007/978-3-030-87355-4_13
   Redmon J, 2018, Arxiv, DOI [arXiv:1804.02767, DOI 10.1109/CVPR.2017.690, DOI 10.48550/ARXIV.1804.02767]
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Tan M., 2020, P IEEECVF C COMPUTER, P10781, DOI [10.48550/arXiv.1911.09070, DOI 10.1109/CVPR42600.2020.01079]
   Tsung-Yi Lin, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P2999, DOI 10.1109/ICCV.2017.324
   Varga LA, 2021, IEEE INT CONF COMP V, P2768, DOI 10.1109/ICCVW54120.2021.00311
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang CY, 2023, PROC CVPR IEEE, P7464, DOI 10.1109/CVPR52729.2023.00721
   Wang J., 2021, arXiv
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Yang F, 2019, IEEE I CONF COMP VIS, P8310, DOI 10.1109/ICCV.2019.00840
   Zhang XD, 2019, IEEE INT CONF COMP V, P118, DOI 10.1109/ICCVW.2019.00020
   Zhao Y., 2022, arXiv
   Zhou Y, 2021, COMPUT ANIMAT VIRT W, V32, DOI 10.1002/cav.2011
   Zhu XK, 2021, IEEE INT CONF COMP V, P2778, DOI 10.1109/ICCVW54120.2021.00312
NR 46
TC 2
Z9 2
U1 13
U2 16
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2023 NOV 11
PY 2023
DI 10.1007/s00371-023-03144-x
EA NOV 2023
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA X7KE2
UT WOS:001100186900001
DA 2024-07-18
ER

PT J
AU Bao, H
   Fan, B
AF Bao, Hua
   Fan, Bo
TI Multi-modality information refinement fusion network for RGB-D salient
   object detection
SO VISUAL COMPUTER
LA English
DT Article
DE RGB-D salient object detection; Feature enhancement; Edge detection;
   Cross refinement; Feature aggregation; Edge refinement
AB RGB-D salient object detection (SOD) has gained more and more research interest in recent years. Due to various imaging mechanisms of RGB and depth modalities, RGB-D images contain different information. Thus, how to effectively fuse multi-modality features and aggregate multi-scale features to generate accurate saliency prediction are still the problems. In this article, we present a Multi-Modality Information Refinement Fusion Network (MIRFNet) for RGB-D SOD to solve the problems. Specifically, a Feature-Enhancement and Cross-Refinement Module (FCM) is proposed to reduce redundant features and the gap between cross-modality data to achieve multi-modality feature fusion effectively. In FCM, the Feature-Enhancement step utilizes attention mechanisms to obtain enhanced features which contain less redundant information and more common salient information, and the Cross-Refinement step employs the enhanced features to reduce the gap between cross-modality features and achieve effective feature fusion. Then, we propose an Edge Guidance Module (EGM) to extract edge information from RGB features. Finally, to effectively aggregate multi-level features and achieve accurate saliency prediction, a Feature-Aggregation and Edge-Refinement Module (FEM) is designed, which introduces specific-modality information and edge information to conduct sufficient information interaction. In FEM, the Feature-Aggregation step aggregates multi-scale features with specific-modality information, and the Edge-Refinement step uses edge information to refine the aggregation features. Extensive experiments demonstrate that MIRFNet can achieve comparable performance against the other 12 SOTA methods on five datasets.
C1 [Bao, Hua] Anhui Univ, Sch Artificial Intelligence, Hefei 230601, Peoples R China.
   [Fan, Bo] Anhui Univ, Sch Elect Engn & Automat, Hefei 230601, Peoples R China.
C3 Anhui University; Anhui University
RP Bao, H (corresponding author), Anhui Univ, Sch Artificial Intelligence, Hefei 230601, Peoples R China.
EM baohua@ahu.edu.cn; z21301056@stu.ahu.edu.cn
FU This research did not receive any specific grant from funding agencies
   in the public, commercial, or not-for-profit sectors.
FX This research did not receive any specific grant from funding agencies
   in the public, commercial, or not-for-profit sectors.
CR Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   Borji A, 2012, LECT NOTES COMPUT SC, V7573, P414, DOI 10.1007/978-3-642-33709-3_30
   CAI Y, 2023, VISUAL COMPUT, P1
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen S., 2020, ECCV, P520, DOI DOI 10.1007/978-3-030-58598-3_31
   Chen TY, 2023, NEUROCOMPUTING, V522, P152, DOI 10.1016/j.neucom.2022.12.004
   Cheng J, 2022, IEEE T CIRC SYST VID, V32, P1498, DOI 10.1109/TCSVT.2021.3076165
   Cheng MM, 2021, INT J COMPUT VISION, V129, P2622, DOI [10.1007/s11263-021-01490-8, 10.1109/ICCV.2017.487]
   Cong RM, 2022, IEEE T IMAGE PROCESS, V31, P6800, DOI 10.1109/TIP.2022.3216198
   Deng-Ping Fan, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P275, DOI 10.1007/978-3-030-58610-2_17
   Fan D-P, 2018, ARXIV
   Fan DP, 2021, IEEE T NEUR NET LEAR, V32, P2075, DOI 10.1109/TNNLS.2020.2996406
   Fan X., 2022, VISUAL COMPUT, P1
   Fang X., 2022, ARXIV
   Fu KR, 2020, PROC CVPR IEEE, P3049, DOI 10.1109/CVPR42600.2020.00312
   Gao SH, 2021, IEEE T PATTERN ANAL, V43, P652, DOI 10.1109/TPAMI.2019.2938758
   Gao Y., 2022, VISUAL COMPUT, P1
   Gao Y, 2023, VISUAL COMPUT, V39, P1137, DOI 10.1007/s00371-021-02393-y
   Guo Y., 2022, IEEE T GEOSCI ELECT, V61, P1
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Hu Peng, 2023, IEEE T PATTERN ANAL
   Huang PS, 2018, INT CONF DIGIT SIG
   Ji W, 2021, PROC CVPR IEEE, P9466, DOI 10.1109/CVPR46437.2021.00935
   Jiang B, 2021, IEEE T MULTIMEDIA, V23, P1343, DOI 10.1109/TMM.2020.2997184
   Jiang MJ, 2021, IEEE T INSTRUM MEAS, V70, DOI 10.1109/TIM.2021.3053971
   Ju R, 2014, IEEE IMAGE PROC, P1115, DOI 10.1109/ICIP.2014.7025222
   Kuznietsov Y, 2022, IEEE WINT CONF APPL, P261, DOI 10.1109/WACVW54805.2022.00032
   Lee M, 2022, LECT NOTES COMPUT SC, V13689, P630, DOI 10.1007/978-3-031-19818-2_36
   Li CY, 2021, IEEE T CYBERNETICS, V51, P88, DOI 10.1109/TCYB.2020.2969255
   Li GY, 2021, IEEE T IMAGE PROCESS, V30, P3528, DOI 10.1109/TIP.2021.3062689
   Li GY, 2020, IEEE T IMAGE PROCESS, V29, P4873, DOI 10.1109/TIP.2020.2976689
   Li YM, 2022, IEEE WINT CONF APPL, P241, DOI 10.1109/WACVW54805.2022.00030
   Liu ZY, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P4481, DOI 10.1145/3474085.3475601
   Ma J., 2023, VISUAL COMPUT, P1
   Mikriukov G, 2022, INT CONF ACOUST SPEE, P4463, DOI 10.1109/ICASSP43922.2022.9746251
   Niu YZ, 2020, IEEE T IMAGE PROCESS, V29, P9496, DOI 10.1109/TIP.2020.3028170
   Niu YZ, 2012, PROC CVPR IEEE, P454, DOI 10.1109/CVPR.2012.6247708
   Peng HW, 2014, LECT NOTES COMPUT SC, V8691, P92, DOI 10.1007/978-3-319-10578-9_7
   Piao YR, 2019, IEEE I CONF COMP VIS, P7253, DOI 10.1109/ICCV.2019.00735
   Pradhan K, 2024, VISUAL COMPUT, V40, P505, DOI 10.1007/s00371-023-02796-z
   Qin J, 2022, AAAI CONF ARTIF INTE, P2117
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Song HK, 2017, IEEE T IMAGE PROCESS, V26, P4204, DOI 10.1109/TIP.2017.2711277
   Song Q, 2022, AAAI CONF ARTIF INTE, P2280
   Sun P, 2021, PROC CVPR IEEE, P1407, DOI 10.1109/CVPR46437.2021.00146
   Sun Y., 2023, IEEE T MULTIMEDIA
   Sun Y, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P2097, DOI 10.1145/3503161.3547876
   Tu ZZ, 2021, IEEE T CIRC SYST VID, V31, P582, DOI 10.1109/TCSVT.2020.2980853
   Wang FY, 2022, IEEE T IMAGE PROCESS, V31, P1285, DOI 10.1109/TIP.2022.3140606
   Wang SL, 2018, PROC CVPR IEEE, P2589, DOI 10.1109/CVPR.2018.00274
   Wang Y., 2023, VISUAL COMPUT, P1
   Wei J, 2020, AAAI CONF ARTIF INTE, V34, P12321
   Wei LS, 2023, INFORM SCIENCES, V626, P223, DOI 10.1016/j.ins.2023.01.032
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Wu JY, 2022, EXPERT SYST APPL, V195, DOI 10.1016/j.eswa.2022.116614
   Xia CX, 2022, IEEE SIGNAL PROC LET, V29, P2577, DOI 10.1109/LSP.2022.3229640
   Xiaoqi Zhao, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12367), P646, DOI 10.1007/978-3-030-58542-6_39
   Xu Yuewang, 2022, 2022 IEEE International Conference on Big Data (Big Data), P4201, DOI 10.1109/BigData55660.2022.10020559
   Yang Y, 2022, IEEE T CIRC SYST VID, V32, P5346, DOI 10.1109/TCSVT.2022.3144852
   Ye M, 2023, IEEE T IMAGE PROCESS, V32, P2190, DOI 10.1109/TIP.2023.3261743
   Youwei Pang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12370), P235, DOI 10.1007/978-3-030-58595-2_15
   Zhai HJ, 2021, IEEE T CIRC SYST VID, V31, P742, DOI 10.1109/TCSVT.2020.2991171
   Zhang M, 2020, PROC CVPR IEEE, P3469, DOI 10.1109/CVPR42600.2020.00353
   Zhong X, 2022, IEEE T CIRC SYST VID, V32, P1418, DOI 10.1109/TCSVT.2021.3072171
   Zhou T, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4661, DOI 10.1109/ICCV48922.2021.00464
   Zhou WJ, 2022, IEEE T MULTIMEDIA, V24, P2192, DOI 10.1109/TMM.2021.3077767
   Zhou XF, 2023, IEEE T CYBERNETICS, V53, P539, DOI 10.1109/TCYB.2022.3163152
   Zhu C, 2022, IEEE T CIRC SYST VID, V32, P7061, DOI 10.1109/TCSVT.2022.3169951
   Zhu CB, 2017, IEEE INT CONF COMP V, P1509, DOI 10.1109/ICCVW.2017.178
   Zhu G, 2023, IEEE T NEUR NET LEAR, V34, P6615, DOI 10.1109/TNNLS.2021.3127959
NR 71
TC 1
Z9 1
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2024
VL 40
IS 6
BP 4183
EP 4199
DI 10.1007/s00371-023-03076-6
EA SEP 2023
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TV2X4
UT WOS:001067458800001
DA 2024-07-18
ER

PT J
AU Zhang, HH
   Hu, ZP
   Sun, Z
   Zhao, MY
   Bi, S
   Di, JR
AF Zhang, Hehao
   Hu, Zhengping
   Sun, Zhe
   Zhao, Mengyao
   Bi, Shuai
   Di, Jirui
TI A fused convolutional spatio-temporal progressive approach for 3D human
   pose estimation
SO VISUAL COMPUTER
LA English
DT Article
DE 3D human pose estimation; Spatial convolutional transformer block;
   Temporal Convolutional transformer block; Full sequence loss function
AB The cascaded self-attention module of vision transformer is good at learning the global correlation between joints for 3D human pose estimation. However, it is still a challenging task to capture the local dynamics of body joints. To this end, we present a fused convolutional spatio-temporal progressive approach that combines convolutional operations and self-attention mechanisms to obtain rich representations of human joints. First, we adopt Spatial Convolutional Transformer Block to fuse spatially both local and global representations across body joints within each frame. Then, we employ Temporal Convolutional Transformer Block to aggregate temporal motion information of the same body joints across different frames in a hierarchical progressive fashion. Finally, we also design a novel full sequence loss (FLoss) function to improve temporal smoothness, resulting in smoother and more reliable 3D poses. Experimental results on the Human 3.6M, HumanEva-I, and MPI-INF-3DHP datasets demonstrate the proposed approach achieves satisfying advancements and promising generalization ability compared to some state-of-the-art methods.
C1 [Zhang, Hehao; Hu, Zhengping; Sun, Zhe; Zhao, Mengyao; Bi, Shuai; Di, Jirui] Yanshan Univ, Informat Sci & Engn, Qinhuangdao 066000, Peoples R China.
C3 Yanshan University
RP Hu, ZP (corresponding author), Yanshan Univ, Informat Sci & Engn, Qinhuangdao 066000, Peoples R China.
EM zhanghh@stumail.ysu.edu.cn; hzp_ysu@163.com; sunzhe_ysu@163.com;
   zhaomengyao0826@126.com; xiaoshuai0710@163.com; dijirui123@163.com
OI Zhang, Hehao/0009-0006-4687-3079
FU National Natural Science Foundation of China [61771420, 62001413];
   Natural Science Foundation of Hebei Province [F2020203064]; Science and
   Technology Project of Hebei Education Department [BJK2023117]
FX This work is supported by the National Natural Science Foundation of
   China under Grants 61771420 and 62001413, the Natural Science Foundation
   of Hebei Province under Grants F2020203064, Science and Technology
   Project of Hebei Education Department under Grants BJK2023117.
CR Ailing Zeng, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P507, DOI 10.1007/978-3-030-58568-6_30
   Cai YJ, 2019, IEEE I CONF COMP VIS, P2272, DOI 10.1109/ICCV.2019.00236
   Chen TL, 2022, IEEE T CIRC SYST VID, V32, P198, DOI 10.1109/TCSVT.2021.3057267
   Chen XP, 2019, PROC CVPR IEEE, P10887, DOI 10.1109/CVPR.2019.01115
   Chen YL, 2018, PROC CVPR IEEE, P7103, DOI 10.1109/CVPR.2018.00742
   Cheng Y, 2019, IEEE I CONF COMP VIS, P723, DOI 10.1109/ICCV.2019.00081
   Ci H, 2019, IEEE I CONF COMP VIS, P2262, DOI 10.1109/ICCV.2019.00235
   Dosovitskiy A, 2021, arXiv, DOI [10.48550/ARXIV.2010.11929, DOI 10.48550/ARXIV.2010.11929]
   Gao BK, 2023, APPL INTELL, V53, P1021, DOI 10.1007/s10489-022-03516-1
   Gong KH, 2021, PROC CVPR IEEE, P8571, DOI 10.1109/CVPR46437.2021.00847
   He KM, 2020, IEEE T PATTERN ANAL, V42, P386, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   Hossain MRI, 2018, LECT NOTES COMPUT SC, V11214, P69, DOI 10.1007/978-3-030-01249-6_5
   Hua G., 2022, IEEE T MULTIMEDIA
   Ioffe S, 2015, P INT C MACH LEARN, V2015, P1
   Ionescu C, 2014, IEEE T PATTERN ANAL, V36, P1325, DOI 10.1109/TPAMI.2013.248
   Jingbo Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12358), P764, DOI 10.1007/978-3-030-58601-0_45
   Lee K, 2018, LECT NOTES COMPUT SC, V11211, P123, DOI 10.1007/978-3-030-01234-2_8
   Li C, 2019, PROC CVPR IEEE, P9879, DOI 10.1109/CVPR.2019.01012
   Li WH, 2022, PROC CVPR IEEE, P13137, DOI 10.1109/CVPR52688.2022.01280
   Lin J., 2019, BRIT MACH VIS C
   Lin K, 2021, PROC CVPR IEEE, P1954, DOI 10.1109/CVPR46437.2021.00199
   Liu J, 2020, IEEE T PATTERN ANAL, V42, P494, DOI 10.1109/TPAMI.2019.2894422
   Liu RX, 2020, PROC CVPR IEEE, P5063, DOI 10.1109/CVPR42600.2020.00511
   Luo B, 2022, IEEE-ASME T MECH, V27, P4205, DOI 10.1109/TMECH.2021.3137951
   Ma XX, 2021, PROC CVPR IEEE, P6234, DOI 10.1109/CVPR46437.2021.00617
   Martinez Julieta, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P2659, DOI 10.1109/ICCV.2017.288
   Mehta D, 2017, INT CONF 3D VISION, P506, DOI 10.1109/3DV.2017.00064
   Mofarreh-Bonab M, 2022, VISUAL COMPUT, V38, P2023, DOI 10.1007/s00371-021-02263-7
   Nair V., 2010, P 27 INT C MACHINE L, P807
   Pavlakos G, 2017, PROC CVPR IEEE, P1263, DOI 10.1109/CVPR.2017.139
   Pavllo D, 2019, PROC CVPR IEEE, P7745, DOI 10.1109/CVPR.2019.00794
   Qiu ZX, 2023, VISUAL COMPUT, V39, P2191, DOI 10.1007/s00371-022-02473-7
   Shuai H., 2022, IEEE T PATTERN ANAL
   Sigal L, 2010, INT J COMPUT VISION, V87, P4, DOI 10.1007/s11263-009-0273-6
   Sun KK, 2021, IEEE T SYST MAN CY-S, V51, P3968, DOI 10.1109/TSMC.2019.2958072
   Sun X, 2018, LECT NOTES COMPUT SC, V11210, P536, DOI 10.1007/978-3-030-01231-1_33
   Wang KK, 2023, VISUAL COMPUT, V39, P429, DOI 10.1007/s00371-021-02339-4
   Wu JZ, 2020, VISUAL COMPUT, V36, P1401, DOI 10.1007/s00371-019-01740-4
   Xu TH, 2021, PROC CVPR IEEE, P16100, DOI 10.1109/CVPR46437.2021.01584
   Yang S, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11782, DOI 10.1109/ICCV48922.2021.01159
   Yeh R., 2019, ADV NEURAL INFORM PR, V32, P8163
   Yuan HN, 2023, NEURAL COMPUT APPL, V35, P4243, DOI 10.1007/s00521-022-07083-x
   Zeng AL, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11416, DOI 10.1109/ICCV48922.2021.01124
   Zhang JL, 2022, PROC CVPR IEEE, P13222, DOI 10.1109/CVPR52688.2022.01288
   Zhang JH, 2021, IEEE T IMAGE PROCESS, V30, P7914, DOI 10.1109/TIP.2021.3109517
   Zhao L, 2019, PROC CVPR IEEE, P3420, DOI 10.1109/CVPR.2019.00354
   Zheng C, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11636, DOI 10.1109/ICCV48922.2021.01145
   Zou ZM, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11457, DOI 10.1109/ICCV48922.2021.01128
NR 48
TC 0
Z9 0
U1 14
U2 23
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2024
VL 40
IS 6
BP 4387
EP 4399
DI 10.1007/s00371-023-03088-2
EA SEP 2023
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TV2X4
UT WOS:001067458300001
DA 2024-07-18
ER

PT J
AU Xu, SM
   Luo, LK
   Hong, HC
   Hu, JL
   Yang, B
   Hu, SQ
AF Xu, Simin
   Luo, Lingkun
   Hong, Haichao
   Hu, Jilin
   Yang, Bin
   Hu, Shiqiang
TI Multi-granularity attention in attention for person re-identification in
   aerial images
SO VISUAL COMPUTER
LA English
DT Article
DE Person re-identification; Aerial images; Multi-granularity; Attention
   mechanism
ID NEURAL-NETWORK; CROWD
AB In marrying with Unmanned Aerial Vehicles (UAVs), the person re-identification (re-ID) techniques are further strengthened in terms of mobility. However, the simple hybridization brings unavoidable scale diversity and occlusions caused by the altitude and attitude variations during the flight of UAVs. To harmoniously blend the two techniques, in this research, we argue that the pedestrian should be globally perceived regardless of the scale variation, and the internal occlusions should also be well suppressed. For this purpose, we propose a novel Multi-granularity Attention in Attention (MGAiA) network to satisfy the raised demands for the aerial-based re-ID. Specifically, a novel multi-granularity attention (MGA) module is designed to supply the feature extraction model with a global awareness to explore the discriminative knowledge within scale variations. Subsequently, an Attention in Attention (AiA) mechanism is proposed to generate attention scores for measuring the importance of the different granularity, thereby proactively reducing the negative efforts caused by occlusions. We carry out comprehensive experiments on two large-scale UAV-based datasets including PRAI-1581 and P-DESTRE, as well as the transfer learning from three popular ground-based re-ID datasets CUHK03, Market-1501, and CUHK-SYSU to quantify the effectiveness of the proposed method.
C1 [Xu, Simin; Luo, Lingkun; Hong, Haichao; Hu, Shiqiang] Shanghai Jiao Tong Univ, Sch Aeronaut & Astronaut, Shanghai 200240, Peoples R China.
   [Hu, Jilin; Yang, Bin] Aalborg Univ, Dept Comp Sci, DK-9220 Aalborg, Denmark.
C3 Shanghai Jiao Tong University; Aalborg University
RP Hu, SQ (corresponding author), Shanghai Jiao Tong Univ, Sch Aeronaut & Astronaut, Shanghai 200240, Peoples R China.
EM siminxu0613@sjtu.edu.cn; lolinkun@gmail.com; haichao.hong@sjtu.edu.cn;
   hujilin@cs.aau.dk; byang@cs.aau.dk; sqhu@sjtu.edu.cn
FU National Natural Science Foundation of China [61773262, 62006152]; China
   Aviation Science Foundation [2022Z071057002, 20142057006]
FX This work is supported by the National Natural Science Foundation of
   China (61773262, 62006152), and the China Aviation Science Foundation
   (2022Z071057002, 20142057006).
CR Chen DP, 2018, PROC CVPR IEEE, pCP1, DOI 10.1109/CVPR.2018.00128
   Chen GY, 2019, IEEE T IMAGE PROCESS, V28, P4192, DOI 10.1109/TIP.2019.2908062
   Chen TL, 2019, IEEE I CONF COMP VIS, P8350, DOI 10.1109/ICCV.2019.00844
   Chung D, 2017, IEEE I CONF COMP VIS, P1992, DOI 10.1109/ICCV.2017.218
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Deng WJ, 2018, PROC CVPR IEEE, P994, DOI 10.1109/CVPR.2018.00110
   Ding SY, 2015, PATTERN RECOGN, V48, P2993, DOI 10.1016/j.patcog.2015.04.005
   Fan X., 2022, Vis. Comput., P1
   Felzenszwalb PF, 2010, IEEE T PATTERN ANAL, V32, P1627, DOI 10.1109/TPAMI.2009.167
   Fu H, 2022, IMAGE VISION COMPUT, V118, DOI 10.1016/j.imavis.2021.104356
   Fu Y, 2019, IEEE I CONF COMP VIS, P6111, DOI 10.1109/ICCV.2019.00621
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Gretton A, 2006, Adv Neural Inf Process Syst, V19
   Grigorev A, 2019, EURASIP J ADV SIG PR, V2019, DOI 10.1186/s13634-019-0647-z
   Gu XQ, 2019, IEEE I CONF COMP VIS, P9646, DOI 10.1109/ICCV.2019.00974
   Han CM, 2023, MULTIMED TOOLS APPL, V82, P14755, DOI 10.1007/s11042-022-13833-9
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He LX, 2018, PROC CVPR IEEE, P7073, DOI 10.1109/CVPR.2018.00739
   Hermans A, 2017, Arxiv, DOI [arXiv:1703.07737, DOI 10.48550/ARXIV.1703.07737]
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang YR, 2024, Arxiv, DOI arXiv:1905.10529
   Jia ZQ, 2023, VISUAL COMPUT, V39, P1205, DOI 10.1007/s00371-022-02398-1
   Kumar SVA, 2021, IEEE T INF FOREN SEC, V16, P1696, DOI 10.1109/TIFS.2020.3040881
   Layne R, 2015, LECT NOTES COMPUT SC, V8927, P225, DOI 10.1007/978-3-319-16199-0_16
   Li JN, 2019, IEEE I CONF COMP VIS, P3957, DOI 10.1109/ICCV.2019.00406
   Li S, 2018, PROC CVPR IEEE, P369, DOI 10.1109/CVPR.2018.00046
   Li W, 2018, PROC CVPR IEEE, P2285, DOI 10.1109/CVPR.2018.00243
   Li W, 2014, PROC CVPR IEEE, P152, DOI 10.1109/CVPR.2014.27
   Li YL, 2018, PROC CVPR IEEE, P5997, DOI 10.1109/CVPR.2018.00628
   Liang WQ, 2018, Arxiv, DOI arXiv:1811.03768
   Liu CT, 2019, Arxiv, DOI arXiv:1908.01683
   Liu YH, 2019, AAAI CONF ARTIF INTE, P8786
   Long MS, 2015, PR MACH LEARN RES, V37, P97
   Luo L., 2022, IEEE Trans. Image Process
   Luo LK, 2024, INT J COMPUT VISION, V132, P161, DOI 10.1007/s11263-023-01865-z
   Luo LK, 2020, IEEE T CYBERNETICS, V50, P3914, DOI 10.1109/TCYB.2019.2962000
   Moritz L, 2021, PROC SPIE, V11735, DOI 10.1117/12.2587946
   Mueller M, 2016, LECT NOTES COMPUT SC, V9905, P445, DOI 10.1007/978-3-319-46448-0_27
   Pervaiz N, 2023, VISUAL COMPUT, V39, P4087, DOI 10.1007/s00371-022-02577-0
   Rao SV, 2018, Arxiv, DOI arXiv:1810.11261
   Rubner Y, 2000, INT J COMPUT VISION, V40, P99, DOI 10.1023/A:1026543900054
   Schumann A, 2016, PROC SPIE, V9995, DOI 10.1117/12.2241652
   Schumann A, 2017, PROC SPIE, V10202, DOI 10.1117/12.2262295
   Selvaraju RR, 2017, IEEE I CONF COMP VIS, P618, DOI 10.1109/ICCV.2017.74
   Sommer L., 2021, SPIE, V11729, P207
   Song LC, 2020, PATTERN RECOGN, V102, DOI 10.1016/j.patcog.2019.107173
   Subramaniam A, 2019, IEEE I CONF COMP VIS, P562, DOI 10.1109/ICCV.2019.00065
   Sun YF, 2018, LECT NOTES COMPUT SC, V11208, P501, DOI 10.1007/978-3-030-01225-0_30
   Sun YF, 2017, IEEE I CONF COMP VIS, P3820, DOI 10.1109/ICCV.2017.410
   Ustinova E, 2017, 2017 14TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE (AVSS)
   Vaswani A, 2017, ADV NEUR IN, V30
   Wan WT, 2018, PROC CVPR IEEE, P9117, DOI 10.1109/CVPR.2018.00950
   Wang GC, 2019, AAAI CONF ARTIF INTE, P8933
   Wang GC, 2018, IEEE T CIRC SYST VID, V28, P2777, DOI 10.1109/TCSVT.2017.2748698
   Wang GR, 2021, IEEE T NEUR NET LEAR, V32, P2142, DOI 10.1109/TNNLS.2020.2999517
   Wang GS, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P274, DOI 10.1145/3240508.3240552
   Wang GQ, 2020, PROC CVPR IEEE, P6677, DOI 10.1109/CVPR42600.2020.00671
   Wang P., 2022, The Visual Computer, P1
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Xiang Y, 2014, LECT NOTES COMPUT SC, V8694, P220, DOI 10.1007/978-3-319-10599-4_15
   Xiao Tong, 2016, arXiv preprint arXiv:1604.01850, V2
   Xie J., 2021, Vis. Comput, V10, P1
   Xu SM, 2022, KNOWL-BASED SYST, V252, DOI 10.1016/j.knosys.2022.109354
   Xu SM, 2021, INT C PATT RECOG, P9149, DOI 10.1109/ICPR48806.2021.9413309
   Yan HL, 2017, PROC CVPR IEEE, P945, DOI 10.1109/CVPR.2017.107
   Yang FX, 2020, AAAI CONF ARTIF INTE, V34, P12597
   Zhang M., 2022, IEEE Trans. Neural Netw. Learn. Syst.
   Zhang MJ, 2023, IEEE T CYBERNETICS, V53, P578, DOI 10.1109/TCYB.2022.3163294
   Zhang MJ, 2020, IEEE T NEUR NET LEAR, V31, P2623, DOI 10.1109/TNNLS.2019.2933590
   Zhang MJ, 2019, IEEE T NEUR NET LEAR, V30, P3109, DOI 10.1109/TNNLS.2018.2890017
   Zhang MJ, 2018, IEEE T CYBERNETICS, V48, P904, DOI 10.1109/TCYB.2017.2664499
   Zhang SZ, 2021, IEEE T MULTIMEDIA, V23, P281, DOI 10.1109/TMM.2020.2977528
   Zhang X, 2018, Arxiv, DOI arXiv:1711.08184
   Zhang YQ, 2018, PROC CVPR IEEE, P928, DOI 10.1109/CVPR.2018.00103
   Zhao LM, 2017, IEEE I CONF COMP VIS, P3239, DOI 10.1109/ICCV.2017.349
   Zhedong Zheng, 2017, ACM Transactions on Multimedia Computing, Communications and Applications, V14, DOI 10.1145/3159171
   Zheng L, 2016, Arxiv, DOI arXiv:1610.02984
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zheng ZD, 2017, IEEE I CONF COMP VIS, P3774, DOI 10.1109/ICCV.2017.405
   Zhou KY, 2019, IEEE I CONF COMP VIS, P3701, DOI 10.1109/ICCV.2019.00380
   Zhou P, 2018, PROC CVPR IEEE, P528, DOI 10.1109/CVPR.2018.00062
   Zhou Z, 2017, PROC CVPR IEEE, P6776, DOI 10.1109/CVPR.2017.717
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zhuo JX, 2018, IEEE INT CON MULTI
NR 85
TC 0
Z9 0
U1 0
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2024
VL 40
IS 6
BP 4149
EP 4166
DI 10.1007/s00371-023-03074-8
EA SEP 2023
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TV2X4
UT WOS:001066422900001
DA 2024-07-18
ER

PT J
AU Ma, WZ
   Yin, MX
   Li, GQ
   Yang, F
   Chang, K
AF Ma, Weizhao
   Yin, Mengxiao
   Li, Guiqing
   Yang, Feng
   Chang, Kan
TI PCMG:3D point cloud human motion generation based on self-attention and
   transformer
SO VISUAL COMPUTER
LA English
DT Article
DE Point cloud sequence generation; Conditional human motion generation;
   Transformer; Variational autoencoder
AB Previous methods for human motion generation have predominantly relied on skeleton representations to depict human poses and motion. These methods typically use a series of skeletons to represent the motion of a human. However, they are not directly suitable for handling the 3D point cloud sequences obtained from optical motion capture. To address this limitation, we propose a novel network called point cloud motion generation (PCMG) that can handle both skeleton-based motion representation and point cloud data from the human surface. PCMG is trained on finite point cloud sequences and is capable of generating infinite new point cloud sequences. By providing a predefined action label and shape label as input, PCMG generates a point cloud sequence that captures the semantics associated with these labels. PCMG achieves comparable results to state-of-the-art methods for action-conditional human motion generation, while outperforming previous approaches in terms of generation efficiency. The code for PCMG will be available at https://github.com/gxucg/PCMG
C1 [Ma, Weizhao; Yin, Mengxiao; Yang, Feng; Chang, Kan] Guangxi Univ, Sch Comp Elect & Informat, 100 East Univ Rd, Nanning 530004, Peoples R China.
   [Yin, Mengxiao; Yang, Feng; Chang, Kan] Guangxi Univ, Guangxi Key Lab Multimedia Commun Network Technol, 100 East Univ Rd, Nanning 530004, Peoples R China.
   [Li, Guiqing] South China Univ Technol, Sch Comp Sci & Engn, 381 Wushan Rd, Guangzhou 510006, Guangdong, Peoples R China.
C3 Guangxi University; Guangxi University; South China University of
   Technology
RP Yin, MX (corresponding author), Guangxi Univ, Sch Comp Elect & Informat, 100 East Univ Rd, Nanning 530004, Peoples R China.; Yin, MX (corresponding author), Guangxi Univ, Guangxi Key Lab Multimedia Commun Network Technol, 100 East Univ Rd, Nanning 530004, Peoples R China.
EM ymx@gxu.edu.cn
RI Ma, WeiZhao/JQW-3202-2023; 常, 侃/AGP-4123-2022
OI Ma, WeiZhao/0000-0002-0956-4545; 常, 侃/0000-0002-6587-0360
FU National Natural Science Foundation of China
FX No Statement Available
CR Ahn H, 2018, IEEE INT CONF ROBOT, P5915
   Bhattacharya U, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P2027, DOI 10.1145/3474085.3475223
   Cervantes P, 2022, LECT NOTES COMPUT SC, V13677, P356, DOI 10.1007/978-3-031-19790-1_22
   Chatzitofis A, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19020282
   Chen K, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459681
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Fan HH, 2021, PROC CVPR IEEE, P14199, DOI 10.1109/CVPR46437.2021.01398
   Fang L., 2021, TRANSFORMER BASED CO, P2101
   Ghorbani N, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11097, DOI 10.1109/ICCV48922.2021.01093
   Ghorbani S, 2019, LECT NOTES COMPUT SC, V11542, P167, DOI 10.1007/978-3-030-22514-8_14
   Ginosar S, 2019, PROC CVPR IEEE, P3492, DOI 10.1109/CVPR.2019.00361
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Guo CA, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2021, DOI 10.1145/3394171.3413635
   Guo MH, 2021, COMPUT VIS MEDIA, V7, P187, DOI 10.1007/s41095-021-0229-5
   Henter GE, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417836
   Hodgins J, 2015, CMU graphics lab motion capture database
   Hong F., 2022, AVATARCLIP ZERO SHOT, P2205
   Hui L., 2020, ARXIV
   Ji YL, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1510, DOI 10.1145/3240508.3240675
   Jiang JY, 2020, INT CONF ACOUST SPEE, P516, DOI [10.1109/ICASSP40776.2020.9054554, 10.1109/icassp40776.2020.9054554]
   Kingma DP, 2013, ARXIV
   Kocabas M, 2020, PROC CVPR IEEE, P5252, DOI 10.1109/CVPR42600.2020.00530
   Lee H.-Y., 2019, DANCING MUSIC, V32
   Li HY, 2021, VISUAL COMPUT, V37, P325, DOI 10.1007/s00371-020-01801-z
   Li J, 2020, ARXIV
   Li JF, 2021, PROC CVPR IEEE, P3382, DOI 10.1109/CVPR46437.2021.00339
   Li J, 2022, IEEE T ENG MANAGE, V69, P1902, DOI 10.1109/TEM.2019.2940702
   Li RH, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459766
   Li RL, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13381, DOI 10.1109/ICCV48922.2021.01315
   Li SY, 2022, PROC CVPR IEEE, P11040, DOI 10.1109/CVPR52688.2022.01077
   Lin X., 2018, HUMAN MOTION MODELIN, P1804
   Liu ZJ, 2019, 33 C NEURAL INFORM P, V32
   Loper M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818013
   Luo ST, 2021, PROC CVPR IEEE, P2836, DOI 10.1109/CVPR46437.2021.00286
   Maeda T, 2022, PROC CVPR IEEE, P6417, DOI 10.1109/CVPR52688.2022.00632
   Mahmood N, 2019, IEEE I CONF COMP VIS, P5441, DOI 10.1109/ICCV.2019.00554
   Pavlakos G, 2019, PROC CVPR IEEE, P10967, DOI 10.1109/CVPR.2019.01123
   Petrovich M., 2022, TEMOS GENERATING DIV, P2204
   Petrovich M, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P10965, DOI 10.1109/ICCV48922.2021.01080
   Qi C., 2017, NeurIPS, P5105, DOI [10.1109/ CVPR.2017.16, DOI 10.1109/CVPR.2017.16]
   Sun YL, 2020, VISUAL COMPUT, V36, P2407, DOI 10.1007/s00371-020-01892-8
   Tang KK, 2023, VISUAL COMPUT, V39, P5577, DOI 10.1007/s00371-022-02682-0
   Tang YZ, 2022, PROC CVPR IEEE, P6387, DOI 10.1109/CVPR52688.2022.00629
   Tevet G., 2022, MOTIONCLIP EXPOSING, P2203, DOI [10.48550/arXiv.2203.08063, DOI 10.48550/ARXIV.2203.08063]
   Tevet G., 2022, arXiv
   Vaswani Ashish, 2017, Advances in Neural Information Processing Systems (NeurIPS), V17, P6000, DOI DOI 10.48550/ARXIV.1706.03762
   Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362
   Xin C., 2023, P IEEE CVF C COMP VI
   Yan SJ, 2019, IEEE I CONF COMP VIS, P4393, DOI 10.1109/ICCV.2019.00449
   Yan SJ, 2018, AAAI CONF ARTIF INTE, P7444
   Yan W., 2021, VIDEOGPT VIDEO GENER, P2104
   Yang GD, 2019, IEEE I CONF COMP VIS, P4540, DOI 10.1109/ICCV.2019.00464
   Yang S., 2023, 11 INT C LEARN REPR
   Zhang H, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201366
   Zhang KY, 2022, AAAI CONF ARTIF INTE, P3291
   Zhang Y, 2020, ARXIV
   Zhang Y, 2022, ALGORITHMS, V15, DOI 10.3390/a15040124
   Zhao HS, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P16239, DOI 10.1109/ICCV48922.2021.01595
   Zhao R, 2020, PROC CVPR IEEE, P6224, DOI 10.1109/CVPR42600.2020.00626
   Zhou Y, 2019, PROC CVPR IEEE, P5738, DOI 10.1109/CVPR.2019.00589
   Zou S., 2020, ARXIV
NR 61
TC 0
Z9 0
U1 4
U2 9
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2024
VL 40
IS 5
BP 3765
EP 3780
DI 10.1007/s00371-023-03063-x
EA SEP 2023
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OC3D9
UT WOS:001058970800001
DA 2024-07-18
ER

PT J
AU Nasser, R
   Abouelseoud, Y
   Mikhail, M
AF Nasser, Reem
   Abouelseoud, Yasmine
   Mikhail, Mervat
TI Robust watermark based on Schur decomposition and dynamic weighting
   factors
SO VISUAL COMPUTER
LA English
DT Article
DE Image watermarking; Discrete wavelet transform (DWT); Schur
   decomposition; Singular value decomposition (SVD); Pattern search
   optimization (PS); Friedman's test
ID REVERSIBLE WATERMARKING; IMAGE; SCHEME; SVD; PROTECTION; FRAMEWORK;
   ATTACKS; DOMAIN
AB Since multimedia data are now more widely distributed in digital form and easier to copy and change, copyright protection has become an essential need. One of the most common copyright protection techniques is watermarking. Invisibility and robustness are crucial aspects of watermarking; however, many of the currently used techniques simply focus more on invisibility than how robust they are. Moreover, the trade-off between invisibility and robustness is challenging. To this end, this paper proposes a novel watermark technique that efficiently overcomes the idea of a trade-off between robustness and invisibility, thereby increasing both under most attacks. Schur decomposition and a dynamic weighting factors matrix are added to the embedding process to improve the robustness of the proposed technique. Besides that, the embedding function is improved to simultaneously maximize imperceptibility and robustness. Another key contribution of the proposed approach is its use of a trajectory-based optimization algorithm rather than the more prevalent population-based algorithms to determine the optimal scaling factors. Consequently, the proposed technique rapidly identifies the best scaling factors for the embedding function. Statistical analysis is performed using the Friedman test. Experimental results show that the proposed technique outperforms other existing techniques for different sizes, shapes, and types of watermarks.
C1 [Nasser, Reem; Abouelseoud, Yasmine; Mikhail, Mervat] Alexandria Univ, Fac Engn, Engn Math & Phys Dept, Alexandria 21544, Egypt.
C3 Egyptian Knowledge Bank (EKB); Alexandria University
RP Nasser, R (corresponding author), Alexandria Univ, Fac Engn, Engn Math & Phys Dept, Alexandria 21544, Egypt.
EM reemnasser@alexu.edu.eg
FU Science, Technology amp; Innovation Funding Authority (STDF); Egyptian
   Knowledge Bank (EKB)
FX Open access funding provided by The Science, Technology & Innovation
   Funding Authority (STDF) in cooperation with The Egyptian Knowledge Bank
   (EKB). No funding was received to assist withthe preparation of this
   manuscript
CR Abraham J, 2019, J KING SAUD UNIV-COM, V31, P125, DOI 10.1016/j.jksuci.2016.12.004
   Ahmadi SBB, 2021, APPL INTELL, V51, P1701, DOI 10.1007/s10489-020-01903-0
   Alshoura WH, 2020, IEEE ACCESS, V8, P43391, DOI 10.1109/ACCESS.2020.2978186
   Aslantas V, 2009, OPT COMMUN, V282, P2806, DOI 10.1016/j.optcom.2009.04.034
   Ahmadi SBB, 2021, VISUAL COMPUT, V37, P385, DOI 10.1007/s00371-020-01808-6
   Begum M., 2021, SN Comput. Sci, V2, P221, DOI [10.1007/s42979-021-00608-6, DOI 10.1007/S42979-021-00608-6]
   Bozorg-Haddad O., 2017, Meta-heuristic and Evolutionary Algorithms for Engineering Optimization, DOI DOI 10.1002/9781119387053
   Chen L, 2018, MULTIMED TOOLS APPL, V77, P7187, DOI 10.1007/s11042-017-4628-7
   Chen ST, 2016, MULTIMED TOOLS APPL, V75, P5493, DOI 10.1007/s11042-015-2522-8
   Deeba F, 2022, INF SECUR J, V31, P237, DOI 10.1080/19393555.2021.1919250
   Ernawan F, 2019, IEEE ACCESS, V7, P151985, DOI 10.1109/ACCESS.2019.2948086
   Giri Kaiser J., 2018, International Journal of Information Technology, V10, P139, DOI 10.1007/s41870-017-0075-y
   Hai T, 2014, J APPL RES TECHNOL, V12, P122, DOI 10.1016/S1665-6423(14)71612-8
   Hurrah NN, 2019, FUTURE GENER COMP SY, V94, P654, DOI 10.1016/j.future.2018.12.036
   Kishore RR., 2016, INT J COMPUT INF SCI, V10, P1264
   Kumar C, 2018, MULTIMED TOOLS APPL, V77, P3597, DOI 10.1007/s11042-017-5222-8
   Liu JX, 2019, IEEE ACCESS, V7, P80849, DOI 10.1109/ACCESS.2019.2915596
   Liu XY, 2019, IEEE ACCESS, V7, P76580, DOI 10.1109/ACCESS.2019.2921894
   Liu Y, 2018, EXPERT SYST APPL, V97, P95, DOI 10.1016/j.eswa.2017.12.003
   Mohan B.C., 2011, IJCA P INT C VLSI CO
   Mousavi SM, 2014, J DIGIT IMAGING, V27, P714, DOI 10.1007/s10278-014-9700-5
   Mun SM, 2019, NEUROCOMPUTING, V337, P191, DOI 10.1016/j.neucom.2019.01.067
   Murali P, 2023, VISUAL COMPUT, V39, P1057, DOI 10.1007/s00371-021-02384-z
   Naffouti S., 2013, 14 INT C SCI TECHN A
   Naffouti SE, 2023, VISUAL COMPUT, V39, P4227, DOI 10.1007/s00371-022-02587-y
   Pan J., 2004, Intelligent Watermarking Techniques, chapter Genetic Watermarking on Spatial Domain
   Pan J-S., 2009, INFORM HIDING APPL, DOI [10.1007/978-3-642-02335-4, DOI 10.1007/978-3-642-02335-4]
   Qin C, 2018, IEEE MULTIMEDIA, V25, P36, DOI 10.1109/MMUL.2018.112142509
   Qin C, 2017, MULTIMED TOOLS APPL, V76, P2267, DOI 10.1007/s11042-015-3218-9
   Rahman AU, 2018, J HEALTHC ENG, V2018, DOI 10.1155/2018/8137436
   Riad R, 2017, CONTROL ENG APPL INF, V19, P25
   Sadeghi M, 2019, SIGNAL PROCESS, V163, P213, DOI 10.1016/j.sigpro.2019.05.026
   Singh SP, 2018, J VIS COMMUN IMAGE R, V53, P86, DOI 10.1016/j.jvcir.2018.03.006
   Sodhro AH, 2017, MULTIMED TOOLS APPL, V76, P20001, DOI [10.1007/s11042-017-4452-0, 10.1007/s11042-016-4084-9]
   Su QT, 2019, IEEE ACCESS, V7, P4358, DOI 10.1109/ACCESS.2018.2888857
   Su QT, 2018, SOFT COMPUT, V22, P91, DOI 10.1007/s00500-017-2489-7
   Su QT, 2017, MULTIMED TOOLS APPL, V76, P707, DOI 10.1007/s11042-015-3071-x
   Wang BW, 2022, IEEE T NETW SCI ENG, V9, P2188, DOI 10.1109/TNSE.2022.3157867
   Wang XY, 2022, VISUAL COMPUT, V38, P3831, DOI 10.1007/s00371-021-02224-0
   Xiao B, 2020, INFORM SCIENCES, V516, P545, DOI 10.1016/j.ins.2019.12.044
   Xiong XG, 2019, IEEE ACCESS, V7, P136592, DOI 10.1109/ACCESS.2019.2942449
   Zear A, 2018, MULTIMED TOOLS APPL, V77, P4863, DOI 10.1007/s11042-016-3862-8
   Zewail I., 2017, MENOUFIA J ELECT ENG, V26, P307, DOI [10.21608/mjeer.2017.63501, DOI 10.21608/MJEER.2017.63501]
   Zhang LN, 2019, MULTIMED TOOLS APPL, V78, P28003, DOI 10.1007/s11042-019-07902-9
NR 44
TC 0
Z9 0
U1 3
U2 8
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2024
VL 40
IS 5
BP 3249
EP 3269
DI 10.1007/s00371-023-03028-0
EA AUG 2023
PG 21
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OC3D9
UT WOS:001048063400001
OA hybrid
DA 2024-07-18
ER

PT J
AU Huang, YF
   Shi, P
   He, HT
   He, HD
   Zhao, BW
AF Huang, Yifang
   Shi, Peng
   He, Haitao
   He, Hongdou
   Zhao, Bowen
TI Senet: spatial information enhancement for semantic segmentation neural
   networks
SO VISUAL COMPUTER
LA English
DT Article
DE Real-time semantic segmentation; Attention mechanism; Spatial
   information enhancement
AB Image semantic segmentation is a basic task of computer vision, and plays an important role in automatic driving, robot navigation and many other fields. However, the expensive computing cost limits its deployment on mobile devices. Therefore, the primary object of this study is to balance accuracy and inference speed in the semantic segmentation task. To this end, we propose a real-time semantic segmentation network with Spatial Enhancement (SENet). We propose to strengthen the information association between feature maps of different resolutions by attention mechanism. We design a spatial information branch to retain the high quality spatial features. The segmentation of object edges is improved by enhancing edge information, and the representation of features is improved by correlating high-level semantic information with low-level spatial information. The real-time performance of the model is achieved by using a lightweight feature enhancement module and a backbone network with low computational complexity. We have carried out several sets of experiments to test the validity of our SENet. The effectiveness and efficiency of SENet are evaluated on the PASCAL VOC2012 and the CityScapes dataset. The model achieves 76.37% and 77.23% mIoU segmentation accuracy, respectively, while the speed reaches 193.3 FPS and 30.8 FPS on a NVIDIA RTX 3080 GPU card. The research has resulted in a solution of balancing the accuracy and inference speed.
C1 [Huang, Yifang; Shi, Peng; He, Haitao; He, Hongdou; Zhao, Bowen] Yanshan Univ, Qinhuangdao 066004, Hebei, Peoples R China.
C3 Yanshan University
RP Shi, P (corresponding author), Yanshan Univ, Qinhuangdao 066004, Hebei, Peoples R China.
EM sp@stumail.ysu.edu.cn
OI Shi, Peng/0009-0005-8393-1111; Zhao, Bowen/0009-0002-5362-6384
CR Amiri MM, 2020, IEEE T SIGNAL PROCES, V68, P2155, DOI 10.1109/TSP.2020.2981904
   Araslanov N, 2021, PROC CVPR IEEE, P15379, DOI 10.1109/CVPR46437.2021.01513
   Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615
   Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen LB, 2017, IEEE INT SYMP NANO, P1, DOI 10.1109/NANOARCH.2017.8053709
   Cheng Z., 2022, VISUAL COMPUT, P1
   Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350
   Ding L, 2021, IEEE T GEOSCI REMOTE, V59, P426, DOI 10.1109/TGRS.2020.2994150
   Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5
   Fritsch J, 2013, IEEE INT C INTELL TR, P1693, DOI 10.1109/ITSC.2013.6728473
   Gao R., 2021, PREPRINT
   He JJ, 2019, IEEE I CONF COMP VIS, P3561, DOI 10.1109/ICCV.2019.00366
   Ho Kei Cheng, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8887, DOI 10.1109/CVPR42600.2020.00891
   Hu P., 2022, IEEE T PATTERN ANAL, V45, P3877
   Hu Peng, 2023, IEEE T PATTERN ANAL
   Huang ZL, 2019, IEEE I CONF COMP VIS, P603, DOI 10.1109/ICCV.2019.00069
   Husbands P, 2021, APPL INTELL, V51, P6467, DOI 10.1007/s10489-021-02275-9
   Ibrahim Mostafa S, 2020, P IEEE CVF C COMP VI, P12715
   Ji J, 2021, IEEE ACCESS, V9, P673, DOI 10.1109/ACCESS.2020.3042254
   Jianbo Liu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12371), P1, DOI 10.1007/978-3-030-58574-7_1
   Jiang M, 2022, VISUAL COMPUT, V38, P2473, DOI 10.1007/s00371-021-02124-3
   Li X, 2019, IEEE I CONF COMP VIS, P9166, DOI 10.1109/ICCV.2019.00926
   Lin GS, 2017, PROC CVPR IEEE, P5168, DOI 10.1109/CVPR.2017.549
   Liu J., 2020, ARXIV200408222, P769
   Liu Y., 2018, PREPRINT
   Liu Z, 2017, IEEE I CONF COMP VIS, P2755, DOI 10.1109/ICCV.2017.298
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Ma ZH, 2022, VISUAL COMPUT, V38, P3163, DOI 10.1007/s00371-022-02535-w
   Nirkin Y, 2021, PROC CVPR IEEE, P4060, DOI 10.1109/CVPR46437.2021.00405
   Peng C, 2017, PROC CVPR IEEE, P1743, DOI 10.1109/CVPR.2017.189
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474
   Sun Y., 2023, IEEE T MULTIMED
   Sun Y, 2023, IEEE T IMAGE PROCESS, V32, P1732, DOI 10.1109/TIP.2023.3251025
   Sun Y, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P2097, DOI 10.1145/3503161.3547876
   Sungha Choi, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9370, DOI 10.1109/CVPR42600.2020.00939
   Takikawa T, 2019, IEEE I CONF COMP VIS, P5228, DOI 10.1109/ICCV.2019.00533
   Tian Z, 2019, PROC CVPR IEEE, P3121, DOI 10.1109/CVPR.2019.00324
   Wang K, 2022, VISUAL COMPUT, V38, P2329, DOI 10.1007/s00371-021-02115-4
   Wang Q, 2020, INT SYM QUAL ELECT, P1, DOI [10.1109/isqed48828.2020.9137057, 10.1109/ISQED48828.2020.9137057, 10.1109/CVPR42600.2020.01155]
   Wang WF, 2020, IEEE ACCESS, V8, P36776, DOI 10.1109/ACCESS.2020.2975640
   Wu TY, 2021, IEEE T IMAGE PROCESS, V30, P1169, DOI 10.1109/TIP.2020.3042065
   Wu ZX, 2019, IEEE I CONF COMP VIS, P2121, DOI 10.1109/ICCV.2019.00221
   Xie EZ, 2021, ADV NEUR IN, V34
   Xu C., 2019, MMASIA 19 ACM MULTIM, P11
   Xu CH, 2021, J CENT SOUTH UNIV, V28, P1765, DOI 10.1007/s11771-021-4731-9
   Xu HX, 2021, MACH VISION APPL, V32, DOI 10.1007/s00138-021-01246-x
   Yang ZY, 2023, VISUAL COMPUT, V39, P1409, DOI 10.1007/s00371-022-02419-z
   Yu CQ, 2021, INT J COMPUT VISION, V129, P3051, DOI 10.1007/s11263-021-01515-2
   Yu CQ, 2018, LECT NOTES COMPUT SC, V11217, P334, DOI 10.1007/978-3-030-01261-8_20
   Yuhui Yuan, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12351), P173, DOI 10.1007/978-3-030-58539-6_11
   Zhang DW, 2019, INT J COMPUT VISION, V127, P363, DOI 10.1007/s11263-018-1112-4
   Zhang RY, 2022, IEEE GEOSCI REMOTE S, V19, DOI 10.1109/LGRS.2021.3122875
   Zhao HS, 2018, LECT NOTES COMPUT SC, V11213, P270, DOI 10.1007/978-3-030-01240-3_17
   Zhao HS, 2018, LECT NOTES COMPUT SC, V11207, P418, DOI 10.1007/978-3-030-01219-9_25
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zheng Z, 2020, PROC CVPR IEEE, P4095, DOI 10.1109/CVPR42600.2020.00415
NR 58
TC 2
Z9 2
U1 20
U2 31
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2024
VL 40
IS 5
BP 3427
EP 3440
DI 10.1007/s00371-023-03043-1
EA AUG 2023
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OC3D9
UT WOS:001045698000001
DA 2024-07-18
ER

PT J
AU Tang, CL
   Dong, HY
   Huang, YR
   Han, T
   Fang, MS
   Fu, JH
AF Tang, Chaoli
   Dong, Huiyuan
   Huang, Yourui
   Han, Tao
   Fang, Mingshuai
   Fu, Jiahao
TI Foreign object detection for transmission lines based on Swin
   Transformer V2 and YOLOX
SO VISUAL COMPUTER
LA English
DT Article
DE Transmission lines; Foreign object detection; YOLOX; Swin Transformer
   V2; RepVGGBlock
ID INSPECTION
AB Suspended foreign objects on transmission lines will shorten the discharge distance, easily leading to phase-to-ground or phase-to-phase short circuits, which induces outage accidents. Foreign objects are small and difficult to identify, resulting in low detection accuracy. An improved foreign object detection method based on Swin Transformer V2 and YOLOX (ST2Rep-YOLOX) is proposed. First, the feature extraction layer ST2CSP constructed by Swin Transformer V2 is used in the original backbone network to extract global and local features. Secondly, hybrid spatial pyramid pooling (HSPP) is designed to enlarge the receptive field and retain more feature information. Then, Re-param VGG block (RepVGGBlock) is introduced to replace all 3 x 3 convolutions in the network to deepen the network and improve feature extraction capabilities. Finally, experiments are carried out on the transmission lines foreign object image dataset, which was obtained using unmanned aerial vehicles (UAVs). The experimental results show that the average accuracy of the ST2Rep-YOLOX method can reach 96.7%, which is 4.4% higher than that of YOLOX. The accuracy of the nest, kite, and balloon increased by 9.3%, 15.4%, and 9.6%, and the recall increased by 6.5%, 9.4%, and 2.5%, respectively. This method has high detection accuracy, which provides an important reference for foreign object detection in transmission lines.
C1 [Tang, Chaoli; Dong, Huiyuan; Huang, Yourui; Han, Tao; Fang, Mingshuai; Fu, Jiahao] Anhui Univ Sci & Technol, Sch Elect & Informat Engn, Huainan 232001, Peoples R China.
   [Huang, Yourui] West Anhui Univ, Sch Elect & Elect Engn, Luan 237012, Peoples R China.
C3 Anhui University of Science & Technology; West Anhui University
RP Dong, HY (corresponding author), Anhui Univ Sci & Technol, Sch Elect & Informat Engn, Huainan 232001, Peoples R China.
EM chltang@mail.ustc.edu.cn; 2021200743@aust.edu.cn; hyr628@163.com;
   than@aust.edu.cn; fangms2023@163.com; 2020200703@aust.edu.cn
FU National Natural Science Foundation of China [61772033]; Anhui
   University Collaborative Innovation Project [GXXT-2019-048,
   GXXT-2020-54]
FX This research was funded by the National Natural Science Foundation of
   China, Grant Number 61772033 and Anhui University Collaborative
   Innovation Project, Grant Number GXXT-2019-048, GXXT-2020-54.
CR Alhassan AB, 2020, INT J ELEC POWER, V118, DOI 10.1016/j.ijepes.2020.105862
   [Anonymous], 2018, REMOTE SENS, DOI DOI 10.3390/rs10040613
   [Anonymous], ULTRALYTICS YOLOV5
   Cha YJ, 2016, AUTOMAT CONSTR, V71, P181, DOI 10.1016/j.autcon.2016.06.008
   Chen C, 2022, INT J APPL EARTH OBS, V112, DOI 10.1016/j.jag.2022.102960
   Cui Y., 2021, PROC IEEE INT C COMP, DOI DOI 10.48550/ARXIV.2108.05821
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Ding XH, 2021, PROC CVPR IEEE, P13728, DOI 10.1109/CVPR46437.2021.01352
   DUDA RO, 1972, COMMUN ACM, V15, P11, DOI 10.1145/361237.361242
   Fan JW, 2023, VISUAL COMPUT, V39, P319, DOI 10.1007/s00371-021-02331-y
   Ge Z., 2021, ARXIV, DOI DOI 10.48550/ARXIV.2107.08430
   Hazgui M, 2022, VISUAL COMPUT, V38, P457, DOI 10.1007/s00371-020-02028-8
   He KM, 2014, LECT NOTES COMPUT SC, V8691, P346, DOI [arXiv:1406.4729, 10.1007/978-3-319-10578-9_23]
   Kingma D. P., 2014, arXiv
   Li Cheng, 2019, CLUSTER COMPUT, V22, pS2611, DOI 10.1007/s10586-017-1356-8
   Li H, 2022, DRONES-BASEL, V6, DOI 10.3390/drones6090252
   Li H, 2022, IEEE ACCESS, V10, P45620, DOI 10.1109/ACCESS.2022.3170696
   Liu D., 2021, ARXIV, DOI DOI 10.48550/ARXIV.2103.10284
   Liu XY, 2020, ANNU REV CONTROL, V50, P253, DOI 10.1016/j.arcontrol.2020.09.002
   Liu YC, 2021, VISUAL COMPUT, V37, P1769, DOI 10.1007/s00371-020-01937-y
   Liu Z, 2022, PROC CVPR IEEE, P11999, DOI 10.1109/CVPR52688.2022.01170
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Lu JF, 2018, IEEE ACCESS, V6, P39063, DOI 10.1109/ACCESS.2018.2851588
   Luo YH, 2023, ARTIF INTELL REV, V56, P173, DOI 10.1007/s10462-022-10189-2
   Qiu ZB, 2023, IEEE T POWER DELIVER, V38, P1329, DOI 10.1109/TPWRD.2022.3213598
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Sarkar D, 2022, VISUAL COMPUT, V38, P4457, DOI 10.1007/s00371-021-02308-x
   Shen X, 2024, VISUAL COMPUT, V40, P1415, DOI 10.1007/s00371-023-02858-2
   Smith LN, 2017, IEEE WINT CONF APPL, P464, DOI 10.1109/WACV.2017.58
   Su JY, 2022, KNOWL-BASED SYST, V249, DOI 10.1016/j.knosys.2022.108857
   Tsung-Yi Lin, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P2999, DOI 10.1109/ICCV.2017.324
   Nguyen VN, 2018, INT J ELEC POWER, V99, P107, DOI 10.1016/j.ijepes.2017.12.016
   Wang CY, 2023, PROC CVPR IEEE, P7464, DOI 10.1109/CVPR52729.2023.00721
   Wang JJ, 2017, ENERGIES, V10, DOI 10.3390/en10040415
   Wang SB, 2023, VISUAL COMPUT, V39, P6085, DOI 10.1007/s00371-022-02714-9
   Wang W., 2022, 36 C NEUR INF PROC S, DOI DOI 10.48550/ARXIV.2210.00911
   Wang W., 2023, ARXIV, DOI [10.48550/arXiv.2209.073YOLOv7:Trainable83, DOI 10.48550/ARXIV.2209.073YOLOV7:TRAINABLE83]
   Xu L, 2021, IMAGE VISION COMPUT, V109, DOI 10.1016/j.imavis.2021.104159
   Zhang HX, 2021, VISUAL COMPUT, V37, P2433, DOI 10.1007/s00371-020-01997-0
   Zhang RZ, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11222600
   Zhang XW, 2022, VISUAL COMPUT, V38, P1457, DOI 10.1007/s00371-021-02080-y
   Zhao WQ, 2021, IEEE T INSTRUM MEAS, V70, DOI 10.1109/TIM.2021.3112227
   Zhou W, 2014, IET COMPUT VIS, V8, P579, DOI 10.1049/iet-cvi.2013.0306
NR 43
TC 5
Z9 5
U1 15
U2 41
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2024
VL 40
IS 5
BP 3003
EP 3021
DI 10.1007/s00371-023-03004-8
EA JUL 2023
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OC3D9
UT WOS:001031348200001
DA 2024-07-18
ER

PT J
AU Huang, S
   Liu, XH
   Tan, T
   Hu, MH
   Wei, XE
   Chen, TL
   Sheng, B
AF Huang, Shan
   Liu, Xiaohong
   Tan, Tao
   Hu, Menghan
   Wei, Xiaoer
   Chen, Tingli
   Sheng, Bin
TI TransMRSR: transformer-based self-distilled generative prior for brain
   MRI super-resolution
SO VISUAL COMPUTER
LA English
DT Article
DE Magnetic resonance images; Super-resolution; Generative piror;
   Transformer
AB Magnetic resonance images (MRI) acquired with low through-plane resolution compromise time and cost. The poor resolution in one orientation is insufficient to meet the requirement of high resolution for early diagnosis of brain disease and morphometric study. The common single-image super-resolution (SISR) solutions face two main challenges: (1) local detailed and global anatomical structural information combination; and (2) large-scale restoration when applied for reconstructing thick-slice MRI into high-resolution (HR) isotropic data. To address these problems, we propose a novel two-stage network for brain MRI SR named TransMRSR based on the convolutional blocks to extract local information and transformer blocks to capture long-range dependencies. TransMRSR consists of three modules: the shallow local feature extraction, the deep non-local feature capture, and the HR image reconstruction. We perform a generative task to encapsulate diverse priors into a generative network (GAN), which is the decoder sub-module of the deep non-local feature capture part, in the first stage. The pre-trained GAN is used for the second stage of SR task. We further eliminate the potential latent space shift caused by the two-stage training strategy through the self-distilled truncation trick. The extensive experiments show that our method achieves superior performance to other SSIR methods on both public and private datasets.
C1 [Huang, Shan; Liu, Xiaohong; Sheng, Bin] Shanghai Jiao Tong Univ, Dept Comp Sci & Engn, Shanghai 200240, Peoples R China.
   [Tan, Tao] Macao Polytech Univ, Fac Appl Sci, Taipa 999078, Macao, Peoples R China.
   [Hu, Menghan] East China Normal Univ, Shanghai Key Lab Multidimens Informat Proc, Shanghai 200241, Peoples R China.
   [Wei, Xiaoer] Shanghai Jiao Tong Univ Affiliated Sixth Peoples H, Inst Diagnost & Intervent Radiol, Shanghai 200233, Peoples R China.
   [Chen, Tingli] Huadong Sanat, Dept Ophthalmol, Wuxi 214065, Peoples R China.
C3 Shanghai Jiao Tong University; Macao Polytechnic University; East China
   Normal University; Shanghai Jiao Tong University
RP Sheng, B (corresponding author), Shanghai Jiao Tong Univ, Dept Comp Sci & Engn, Shanghai 200240, Peoples R China.; Wei, XE (corresponding author), Shanghai Jiao Tong Univ Affiliated Sixth Peoples H, Inst Diagnost & Intervent Radiol, Shanghai 200233, Peoples R China.; Chen, TL (corresponding author), Huadong Sanat, Dept Ophthalmol, Wuxi 214065, Peoples R China.
EM hs_christal@sjtu.edu.cn; weixiaoer_2003@163.com; chentingli1028@163.com;
   shengbin@cs.sjtu.edu.cn
RI Hu, Menghan/AAK-7153-2021; Chen, Ting-Li/AAF-5752-2020
OI Hu, Menghan/0000-0002-8557-8930; xie, hui/0000-0002-8490-1695
FU National Key Research and Development Program of China [2022YFC2407000];
   Interdisciplinary Program of Shanghai Jiao Tong University [YG2023LC11,
   YG2022ZD007]; National Natural Science Foundation of China [62272298,
   62077037]; College-level Project Fund of Shanghai Jiao Tong University
   Affiliated Sixth People's Hospital [ynlc201909]; Medical-industrial
   Cross-fund of Shanghai Jiao Tong University [YG2022QN089]; Clinical
   Special Program of Shanghai Municipal Health Commission [2022404];
   Shanghai Pujiang Program [v22PJ1406800]
FX This work was supported by the National Key Research and Development
   Program of China under grant number 2022YFC2407000, the
   Interdisciplinary Program of Shanghai Jiao Tong University under grant
   number YG2023LC11 and YG2022ZD007, National Natural Science Foundation
   of China under grant number 62272298 and 62077037, the College-level
   Project Fund of Shanghai Jiao Tong University Affiliated Sixth People's
   Hospital under grant number ynlc201909, the Medical-industrial
   Cross-fund of Shanghai Jiao Tong University under the grant number
   YG2022QN089, the Clinical Special Program of Shanghai Municipal Health
   Commission under Grant 2022404, and partly supported by the Shanghai
   Pujiang Program under-vGrantv22PJ1406800.
CR Bau David, 2020, ARXIV PREPRINT ARXIV, P9
   Chan KCK, 2021, PROC CVPR IEEE, P14240, DOI 10.1109/CVPR46437.2021.01402
   Chartier S., 2018, INT C MED IM COMP CO, P198
   Chen YH, 2018, LECT NOTES COMPUT SC, V11070, P91, DOI 10.1007/978-3-030-00928-1_11
   Cheng Peng, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7747, DOI 10.1109/CVPR42600.2020.00777
   Chu XJ, 2022, IEEE COMPUT SOC CONF, P1238, DOI 10.1109/CVPRW56347.2022.00130
   Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281
   Du JL, 2020, IEEE ACCESS, V8, P18938, DOI 10.1109/ACCESS.2020.2968395
   Feng CM, 2021, LECT NOTES COMPUT SC, V12906, P140, DOI 10.1007/978-3-030-87231-1_14
   Gatys L., 2016, Journal of Vision, V16, P326, DOI DOI 10.1167/16.12.326
   Goetschalckx L, 2019, IEEE I CONF COMP VIS, P5743, DOI 10.1109/ICCV.2019.00584
   Gu JJ, 2020, PROC CVPR IEEE, P3009, DOI 10.1109/CVPR42600.2020.00308
   Jiapeng Zhu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12362), P592, DOI 10.1007/978-3-030-58520-4_35
   Jin Z., 2022, IEEE J BIOMED HEALTH
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Karras T, 2021, IEEE T PATTERN ANAL, V43, P4217, DOI 10.1109/TPAMI.2020.2970919
   Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.182, 10.1109/CVPR.2016.181]
   Korkmaz Y, 2022, IEEE T MED IMAGING, V41, P1747, DOI 10.1109/TMI.2022.3147426
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Li G., 2022, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, P20636
   Liang JY, 2021, IEEE INT CONF COMP V, P1833, DOI 10.1109/ICCVW54120.2021.00210
   Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151
   Liu GP, 2021, NEUROIMAGE, V245, DOI 10.1016/j.neuroimage.2021.118687
   Liu X, 2020, P IEEE CVF WINT C AP, P2416
   Liu XH, 2021, IEEE T IMAGE PROCESS, V30, P2127, DOI 10.1109/TIP.2021.3049974
   Liu XH, 2018, IEEE T IMAGE PROCESS, V27, P4971, DOI 10.1109/TIP.2018.2848113
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Lyu Q, 2020, IEEE T COMPUT IMAG, V6, P615, DOI 10.1109/TCI.2020.2964201
   Menon S, 2020, PROC CVPR IEEE, P2434, DOI 10.1109/CVPR42600.2020.00251
   Mokady Ron, 2022, SPECIAL INTEREST GRO, P1
   Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207
   Shi ZH, 2022, IEEE T BROADCAST, V68, P345, DOI 10.1109/TBC.2021.3131875
   Wang JC, 2020, IEEE WINT CONF APPL, P3616, DOI [10.1109/WACV45572.2020.9093603, 10.1109/wacv45572.2020.9093603]
   Wang WY, 2021, EURASIP J IMAGE VIDE, V2021, DOI 10.1186/s13640-021-00552-8
   Wang XT, 2021, PROC CVPR IEEE, P9164, DOI 10.1109/CVPR46437.2021.00905
   Xia W., 2022, IEEE T PATTERN ANAL, DOI 10.1109/TPAMI.2022.3181070
   Xia Y, 2021, MED IMAGE ANAL, V71, DOI 10.1016/j.media.2021.102037
   Yuhua Chen, 2018, 2018 IEEE 15th International Symposium on Biomedical Imaging (ISBI 2018), P739, DOI 10.1109/ISBI.2018.8363679
   Zamir SW, 2022, PROC CVPR IEEE, P5718, DOI 10.1109/CVPR52688.2022.00564
   Zhang Bowen, 2022, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, P11304
   Zhang H., 2019, IEEE T MED IMAGING, V38, P167
   Zhang YL, 2021, PROC CVPR IEEE, P13420, DOI 10.1109/CVPR46437.2021.01322
   Zhao C, 2021, IEEE T MED IMAGING, V40, P805, DOI 10.1109/TMI.2020.3037187
NR 43
TC 4
Z9 4
U1 3
U2 13
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2023
VL 39
IS 8
SI SI
BP 3647
EP 3659
DI 10.1007/s00371-023-02938-3
EA JUL 2023
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA P2DS6
UT WOS:001025049000001
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Hong, J
   Li, WH
   Han, JL
   Zheng, JY
   Fang, PF
   Harandi, M
   Petersson, L
AF Hong, Jie
   Li, Weihao
   Han, Junlin
   Zheng, Jiyang
   Fang, Pengfei
   Harandi, Mehrtash
   Petersson, Lars
TI GOSS: towards generalized open-set semantic segmentation
SO VISUAL COMPUTER
LA English
DT Article
DE Open-set semantic segmentation; Generic segmentation; Scene
   understanding
AB In this paper, we extend Open-set Semantic Segmentation (OSS) into a new image segmentation task called Generalized Open-set Semantic Segmentation (GOSS). Previously, with well-known OSS, the intelligent agents only detect unknown regions without further processing, limiting their perception capacity of the environment. It stands to reason that further analysis of the detected unknown pixels would be beneficial for agents' decision-making. Therefore, we propose GOSS, which holistically unifies the abilities of two well-defined segmentation tasks, i.e. OSS and generic segmentation. Specifically, GOSS classifies pixels as belonging to known classes, and clusters (or groups) of pixels of unknown class are labelled as such. We propose a metric that balances the pixel classification and clustering aspects to evaluate this newly expanded task. Moreover, we build benchmark tests on existing datasets and propose neural architectures as baselines. Our experiments on multiple benchmarks demonstrate the effectiveness of our baselines. Code is made available at https://github.com/JHome1/GOSS_Segmentor.
C1 [Hong, Jie; Li, Weihao; Han, Junlin] Australian Natl Univ, Canberra 2601, Australia.
   [Hong, Jie; Han, Junlin; Petersson, Lars] CSIRO, Data61, Canberra 2601, Australia.
   [Harandi, Mehrtash] Monash Univ, Melbourne, Vic 3800, Australia.
   [Zheng, Jiyang] Univ Sydney, Sydney, NSW 2006, Australia.
   [Fang, Pengfei] Southeast Univ, Nanjing 211189, Peoples R China.
C3 Australian National University; Commonwealth Scientific & Industrial
   Research Organisation (CSIRO); Monash University; University of Sydney;
   Southeast University - China
RP Hong, J (corresponding author), Australian Natl Univ, Canberra 2601, Australia.; Hong, J (corresponding author), CSIRO, Data61, Canberra 2601, Australia.
EM jie.hong@anu.edu.au; weihao.li1@anu.edu.au; u6835134@anu.edu.au;
   jiyang.zheng@anu.edu.au; pengfei.fang@anu.edu.au;
   mehrtash.harandi@monash.edu; lars.petersson@data61.csiro.au
RI Li, Weihao/IAR-9142-2023; Harandi, Mehrtash T/D-6586-2018; Petersson,
   Lars/C-2568-2019
OI Li, Weihao/0000-0002-9447-7023; Petersson, Lars/0000-0002-0103-1904
FU CAUL
FX Open Access funding enabled and organized by CAUL andits Member
   Institutions.
CR Arbeláez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161
   Bevandic P, 2022, IMAGE VISION COMPUT, V124, DOI 10.1016/j.imavis.2022.104490
   Bleyer M., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3081, DOI 10.1109/CVPR.2011.5995581
   Caesar H, 2018, PROC CVPR IEEE, P1209, DOI 10.1109/CVPR.2018.00132
   Cen J, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P15313, DOI 10.1109/ICCV48922.2021.01505
   Chan R, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5108, DOI 10.1109/ICCV48922.2021.00508
   Chan Robin, 2021, Neural Information Processing Systems (NeurIPS)
   Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen LB, 2017, IEEE INT SYMP NANO, P1, DOI 10.1109/NANOARCH.2017.8053709
   Cheng B, 2021, ADV NEUR IN, V34
   Cheng BW, 2022, PROC CVPR IEEE, P1280, DOI 10.1109/CVPR52688.2022.00135
   Cheng BW, 2021, PROC CVPR IEEE, P15329, DOI 10.1109/CVPR46437.2021.01508
   Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236
   Corbière C, 2019, ADV NEUR IN, V32
   Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350
   Davis JJ., 2006, PROC INT C MACHINE L, DOI DOI 10.1145/1143844.1143874
   De Brabandere Bert, 2017, Semantic Instance Segmentation with a Discriminative Loss Function
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   DeVries T., 2018, ARXIV
   Dhamija AR, 2020, IEEE WINT CONF APPL, P1010, DOI 10.1109/WACV45572.2020.9093355
   Di Biase G, 2021, PROC CVPR IEEE, P16913, DOI 10.1109/CVPR46437.2021.01664
   Eigen D, 2015, IEEE I CONF COMP VIS, P2650, DOI 10.1109/ICCV.2015.304
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Fawcett T, 2006, PATTERN RECOGN LETT, V27, P861, DOI 10.1016/j.patrec.2005.10.010
   Felzenszwalb PF, 2004, INT J COMPUT VISION, V59, P167, DOI 10.1023/B:VISI.0000022288.19776.77
   Grady L, 2006, IEEE T PATTERN ANAL, V28, P1768, DOI 10.1109/TPAMI.2006.233
   Häne C, 2013, PROC CVPR IEEE, P97, DOI 10.1109/CVPR.2013.20
   Harley AW, 2017, IEEE I CONF COMP VIS, P5048, DOI 10.1109/ICCV.2017.539
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hendrycks D, 2022, PR MACH LEARN RES
   Hendrycks Dan, 2017, INT C LEARNING REPRE
   Hu R, 2018, PROC CVPR IEEE, P4233, DOI 10.1109/CVPR.2018.00445
   Hu YL, 2019, PROC CVPR IEEE, P3380, DOI 10.1109/CVPR.2019.00350
   Hwang J, 2021, PROC CVPR IEEE, P1175, DOI 10.1109/CVPR46437.2021.00123
   Hwang JJ, 2019, IEEE I CONF COMP VIS, P7333, DOI 10.1109/ICCV.2019.00743
   Isaacs O., 2020, P IEEE CVF C COMP VI, P12946
   Jafari Omid Hosseini, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P4620, DOI 10.1109/ICRA.2017.7989537
   Jianqiang Wan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9250, DOI 10.1109/CVPR42600.2020.00927
   Joseph K. J., 2021, 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), P5826, DOI 10.1109/CVPR46437.2021.00577
   Jung S., 2021, P IEEE CVF INT C COM, P15425
   Kim W, 2020, IEEE T IMAGE PROCESS, V29, P8055, DOI 10.1109/TIP.2020.3011269
   Kingma D. P., 2014, arXiv
   Kirillov A, 2019, PROC CVPR IEEE, P6392, DOI 10.1109/CVPR.2019.00656
   Kirillov A, 2019, PROC CVPR IEEE, P9396, DOI 10.1109/CVPR.2019.00963
   Kokkinos I, 2017, PROC CVPR IEEE, P5454, DOI 10.1109/CVPR.2017.579
   Kong S, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P793, DOI 10.1109/ICCV48922.2021.00085
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lis K, 2019, IEEE I CONF COMP VIS, P2152, DOI 10.1109/ICCV.2019.00224
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Liu Y, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P864
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Manning C.D., 1999, FDN STAT NATURAL LAN
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Martin DR, 2004, IEEE T PATTERN ANAL, V26, P530, DOI 10.1109/TPAMI.2004.1273918
   Meila M., 2005, P 22 INT C MACH LEAR, P577, DOI DOI 10.1145/1102351.1102424
   Paszke A, 2019, ADV NEUR IN, V32
   Pinheiro PO, 2015, ADV NEUR IN, V28
   RAND WM, 1971, J AM STAT ASSOC, V66, P846, DOI 10.2307/2284239
   Saito T, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0118432
   Sevilla-Lara L, 2016, PROC CVPR IEEE, P3889, DOI 10.1109/CVPR.2016.422
   Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688
   Shotton J, 2006, LECT NOTES COMPUT SC, V3951, P1
   Sohn K, 2016, ADV NEUR IN, V29
   Sun DQ, 2012, PROC CVPR IEEE, P1768, DOI 10.1109/CVPR.2012.6247873
   Szeliski R., 2022, Computer Vision: Algorithms and Applications
   Vojir Tomas, 2021, P IEEECVF INT C COMP, P15651
   Wang WG, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P7283, DOI 10.1109/ICCV48922.2021.00721
   Xia Yingda, 2020, P EUROPEAN C COMPUTE, P145
   Yamaguchi K, 2014, LECT NOTES COMPUT SC, V8693, P756, DOI 10.1007/978-3-319-10602-1_49
   Yu J, 2020, VISUAL COMPUT, V36, P1457, DOI 10.1007/s00371-019-01751-1
   Zhao S., 2019, arXiv
   Zheng SX, 2021, PROC CVPR IEEE, P6877, DOI 10.1109/CVPR46437.2021.00681
   Zhou BL, 2017, PROC CVPR IEEE, P5122, DOI 10.1109/CVPR.2017.544
NR 74
TC 2
Z9 2
U1 2
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2024
VL 40
IS 4
BP 2391
EP 2404
DI 10.1007/s00371-023-02925-8
EA JUN 2023
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA MZ2U3
UT WOS:001019486000003
OA Green Submitted, hybrid
DA 2024-07-18
ER

PT J
AU Tasyürek, M
AF Tasyurek, Murat
TI ODRP: a new approach for spatial street sign detection from EXIF using
   deep learning-based object detection, distance estimation, rotation and
   projection system
SO VISUAL COMPUTER
LA English
DT Article
DE Geographical information systems; Spatial detection; Street plate;
   Exchangeable image file; Spatial estimation with deep learning
AB Geographical information systems (GIS) are the systems where spatial data are stored and analyzed. The most important raw material in GIS is spatial data. Thus, it is essential to collect and update these data. On the other hand, exchangeable image file (EXIF) format is a special file format that contains camera direction, date-time information and GPS location provided by a digital camera that captures the images. Transferring the objects in EXIF data sets with absolute coordinates on the earth significantly contributes to GIS. In this study, a new hybrid approach, ODPR, which utilizes object detection (O), distance estimation (D), rotation (R) and projection (P) methods, is proposed to detect street sign objects in EXIF with their locations. The performance of the proposed approach has been examined on the natural EXIF data sets obtained from the Kayseri Metropolitan Municipality. In the proposed approach, a deep learning method detects a street sign object in the EXIF. Then, the object's distance is calculated at the point where the photograph is taken. Finally, the spatial location of the detected object on the earth is calculated using distance, direction and GPS data with rotation and projection methods. In the proposed ODRP approach, the performances of convolutional neural network (CNN)-based Faster R-CNN, YOLO V5, YOLO V6 and transformer-based DETR models as deep learning models for object detection are examined. The F1 score metric is widely used to examine the performance of methods in deep learning models. The performances of the proposed approaches are reviewed according to the F1 score values, and ODRP Faster R-CNN, YOLO V5, YOLO V6 and DETR approaches achieved F1 scores of 0.909, 0.956, 0.948 and 0.922, respectively. In addition, to overcome the variability of light and background mixing problems, an improved supervised learning method (ISL) is proposed. Thanks to ISL, ODRP Faster R-CNN, YOLO V5, YOLO V6, and DETR approaches have reached 0.965, 0.985, 0.969 and 0.942 f1 scores, respectively. The proposed ODRP Faster R-CNN, YOLO V5, YOLO V6 and DETR approaches found the location of the street sign object to be 11434.76, 12818.39, 12454.63 and 9843.57ms closer to its position on earth than the classical method, which considers the location of the EXIF, respectively. Regarding time cost, the ODRP Faster R-CNN, YOLO V5, YOLO V6 and DETR analyze EXIF data at an average of 0.99, 0.42, 0.41 and 0.53 s, respectively. The run time of the ODRP YOLO V5 and V6 approaches is almost equal to each other, and it works approximately 2.5 times faster than the ODRP Faster R-CNN method. Consequently, ODRP YOLO V5 outperforms ODRP Faster R-CNN, YOLO V6 and DETR for detecting the spatial location of street sign objects in EXIF and the F1 score.
C1 [Tasyurek, Murat] Kayseri Univ, Dept Comp Engn, Kayseri, Turkiye.
C3 Kayseri University
RP Tasyürek, M (corresponding author), Kayseri Univ, Dept Comp Engn, Kayseri, Turkiye.
EM murattasyurek@kayseri.edu.tr
RI Tasyurek, Murat/AGZ-5319-2022
OI Tasyurek, Murat/0000-0001-5623-8577
CR Abdul-Rahman A., 2007, Spatial Data Modelling for 3D GIS
   Adrakatti A., 2016, Int. J. Libr. Sci., V14, P41
   Agafonkin Vladimir., Leaflet
   Agrawal A, 2020, VISUAL COMPUT, V36, P405, DOI 10.1007/s00371-019-01630-9
   Alburshaid E., 2021, 2021 INT S NETW COMP, P1
   Alom MZ, 2019, ELECTRONICS-SWITZ, V8, DOI 10.3390/electronics8030292
   Alzubaidi L, 2021, J BIG DATA-GER, V8, DOI 10.1186/s40537-021-00444-8
   ArcView G., 1996, Environ. Syst. Res. Inst., V3
   Arslan RS, 2022, CONCURR COMP-PRACT E, V34, DOI 10.1002/cpe.7180
   Batuk F., 2007, JEODEZI JEOINF DERG, V96, P3
   Bayoudh K, 2022, VISUAL COMPUT, V38, P2939, DOI 10.1007/s00371-021-02166-7
   Bochkovskiy A, 2020, Arxiv, DOI arXiv:2004.10934
   Cai G, 2023, VISUAL COMPUT, V39, P2781, DOI 10.1007/s00371-022-02492-4
   Canters F., 2002, SMALL SCALE MAP PROJ
   Carion Nicolas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P213, DOI 10.1007/978-3-030-58452-8_13
   Chang K.-T., 2017, International Encyclopedia of Geography: People, the Earth, Environment and Technology, P1, DOI [10.1002/9781118786352.wbieg0152, DOI 10.1002/9781118786352.WBIEG0152]
   Chen WJ, 2021, VISUAL COMPUT, V37, P805, DOI 10.1007/s00371-020-01831-7
   Chen Y, 2018, PROC CVPR IEEE, P3339, DOI 10.1109/CVPR.2018.00352
   Chun PJ, 2021, APPL SCI-BASEL, V11, DOI 10.3390/app11030892
   Ciaburro G., 2017, Neural networks with R: Smart Models Using CNN, RNN, Deep Learning, and Artificial Intelligence Principles, P183
   Dal A., Yolov4-Detector-and-Distance-Estimator
   Ding Y., LABELIMG
   Dong PL, 2008, COMPUT GEOSCI-UK, V34, P411, DOI 10.1016/j.cageo.2007.04.005
   Folger P., 2010, Geospatial Information and Geographic Information Systems (GIS): Current Issues and FutureChallenges
   Fort S, 2020, Arxiv, DOI [arXiv:1912.02757, DOI 10.48550/ARXIV.1912.02757]
   Fotheringham S., 2013, SPATIAL ANAL GIS, DOI [10.1201/9781482272468, DOI 10.1201/9781482272468]
   Gangwar D., 2018, Int. J. Sci. Res. Comput. Sci. Eng. Inf. Technol. IJSR CSEIT, V3, P335
   Girshick R., 2014, P 2014 IEEE C COMPUT, P580, DOI [DOI 10.1109/CVPR.2014.81, 10.1109/CVPR.2014.81]
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   github, CHILICYY YOLOV6 SING
   Hadlow N, 2018, CLIN ENDOCRINOL, V89, P299, DOI 10.1111/cen.13754
   Han K, 2022, Arxiv, DOI arXiv:2012.12556
   Hussain M, 2019, ADV INTELL SYST, V840, P191, DOI 10.1007/978-3-319-97982-3_16
   Iqbal Z, 2018, COMPUT ELECTRON AGR, V153, P12, DOI 10.1016/j.compag.2018.07.032
   Jain S, 2003, ADDING EPSG 4326 GEO
   Jayalakshmi GS, 2019, 2019 SECOND INTERNATIONAL CONFERENCE ON COMPUTATIONAL INTELLIGENCE IN DATA SCIENCE (ICCIDS 2019), DOI 10.1109/iccids.2019.8862143
   Jiang PY, 2022, PROCEDIA COMPUT SCI, V199, P1066, DOI 10.1016/j.procs.2022.01.135
   Jocher G., YoloV5
   Joseph A, 2020, VISUAL COMPUT, V36, P529, DOI 10.1007/s00371-019-01628-3
   Joseph RK, 2016, CRIT POL ECON S ASIA, P1
   Kalsotra R, 2022, VISUAL COMPUT, V38, P4151, DOI 10.1007/s00371-021-02286-0
   Kamilaris A, 2018, COMPUT ELECTRON AGR, V147, P70, DOI 10.1016/j.compag.2018.02.016
   Kearney SP, 2020, INT J APPL EARTH OBS, V87, DOI 10.1016/j.jag.2019.102031
   Kiefer S., ExifLib.Net: A Fast Exif Data Extractor for.NET 4.5+
   Korotcov A, 2017, MOL PHARMACEUT, V14, P4462, DOI 10.1021/acs.molpharmaceut.7b00578
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   LeCun Y, 1999, LECT NOTES COMPUT SC, V1681, P319, DOI 10.1007/3-540-46805-6_19
   Li Y., 2021, The Visual Computer, P1
   Liu CC, 2021, VISUAL COMPUT, V37, P1327, DOI 10.1007/s00371-020-01868-8
   Maier G., 2014, REGION, V1, P3, DOI [10.18335/region.v1i1.70, DOI 10.18335/REGION.V1I1.70]
   Malaainine M.E.I., 2021, J. Geogr. Inf. Syst., V13, P395, DOI 10.4236/jgis.2021.134022
   Malkauthekar M. D., 2013, P 3 INT C COMP INT I, P503
   Merigó JM, 2011, INT J COMPUT INT SYS, V4, P123
   Nguyen K, 2015, ACM SIGPLAN NOTICES, V50, P675, DOI [10.1145/2694344.2694345, 10.1145/2775054.2694345]
   Nicolai R, 2008, 70 EAGE C EXH INC SP, P40, DOI DOI 10.3997/2214-4609.20147655
   PARKER HD, 1988, PHOTOGRAMM ENG REM S, V54, P1547
   Paul S., 2015, 2015 IEEE workshop on computational intelligence: theories, applications and future directions, P1
   postgis, POSTGIS ST TRANSFORM
   Powell M. J. D., 1977, ACM Transactions on Mathematical Software, V3, P316, DOI 10.1145/355759.355761
   Prasetya RP, 2017, 2017 5TH INTERNATIONAL SYMPOSIUM ON COMPUTATIONAL AND BUSINESS INTELLIGENCE (ISCBI), P37, DOI 10.1109/ISCBI.2017.8053540
   Quan Q, 2021, VISUAL COMPUT, V37, P245, DOI 10.1007/s00371-020-01796-7
   Rath S.R., Custom Object Detection Using PyTorch Faster RCNN
   Redmon J., 2018, arXiv, DOI DOI 10.48550/ARXIV.1804.02767
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Research F., DETR: End-to-End Object Detection with Transformers
   Salt A., ADD SALT PEPPER NOIS
   Santiago A., 2015, The Book of Openlayers 3. Theory and Practice
   Shao Feifei, 2022, Neurocomputing
   Shekhar S., 2008, NEXT GENERATION DATA, P573, DOI [10.1201/9781420085877, DOI 10.1201/9781420085877]
   Si TZ, 2023, IEEE T MULTIMEDIA, V25, P4323, DOI 10.1109/TMM.2022.3174414
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Tang W, 2023, IEEE T MULTIMEDIA, V25, P5413, DOI 10.1109/TMM.2022.3192661
   Tang W, 2022, IEEE T IMAGE PROCESS, V31, P5134, DOI 10.1109/TIP.2022.3193288
   Tarik T., 2008, JEODEZI JEOINF DERG, V99, P13
   TASYUREK M., 2022, J FAC ENG ARCHIT GAZ, V37
   Tasyurek M., EXIF DIRECTION READE
   Tasyurek M., 2021, Gazi Univ Muhendislik Mimar Fak Derg, V36, P715
   Tasyurek M, 2022, NEURAL COMPUT APPL, V34, P14777, DOI 10.1007/s00521-022-07311-4
   Tripathi G, 2019, VISUAL COMPUT, V35, P753, DOI 10.1007/s00371-018-1499-5
   Versloot C., CREATE TRAIN TEST SP
   Wang Guohui., 2012, Proceedings of the first workshop on Hot topics in software defined networks, HotSDN '12, P103
   Yang Q H., 1999, Map Projection Transformation: Principles and Application
   Zhang J., 2022, The Visual Computer, P1
   Zhang Q., 2022, VISUAL COMPUT, P1
   Zope V, 2021, COMM COM INF SC, V1440, P81, DOI 10.1007/978-3-030-81462-5_8
NR 85
TC 8
Z9 8
U1 3
U2 24
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2024
VL 40
IS 2
BP 983
EP 1003
DI 10.1007/s00371-023-02827-9
EA MAR 2023
PG 21
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GE6E8
UT WOS:000952181900002
DA 2024-07-18
ER

PT J
AU Kong, X
   Xia, SF
   Liu, NZ
   Wei, MQ
AF Kong, Xin
   Xia, Shifeng
   Liu, Ningzhong
   Wei, Mingqing
TI GADA-SegNet: gated attentive domain adaptation network for semantic
   segmentation of LiDAR point clouds
SO VISUAL COMPUTER
LA English
DT Article
DE Point cloud semantic segmentation; Unsupervised domain adaptation;
   Adversarial learning
AB We propose GADA-SegNet, a gated attentive domain adaptation network for semantic segmentation of LiDAR point clouds. Unlike most of existing methods that learn fully from point-wise annotations, our GADA-SegNet attempts to learn from labeled data first and then transfer itself smoothly to unlabeled data. We have three key contributions to bridge the domain gap between the labeled data and the unlabeled yet unseen data. First, we design a new gated connection module that can filter out noise and domain-private features from the low-level features, for better high- and low-level feature fusion. Second, we introduce a multi-scale attention module that can ease the large-scale variation of objects and class imbalance in complex scenes to reduce the class-level domain gap. Third, we develop a shared domain discriminator to implement the class-level domain discrimination for large-scale LiDAR point clouds. Experiments on both synthetic-to-real and real-to-real scenarios show clear improvements of our GADA-SegNet over its competitors.
C1 [Kong, Xin; Xia, Shifeng; Liu, Ningzhong; Wei, Mingqing] Nanjing Univ Aeronaut & Astronaut, Coll Comp Sci & Technol, Nanjing 211106, Peoples R China.
   [Kong, Xin; Xia, Shifeng; Liu, Ningzhong; Wei, Mingqing] Collaborat Innovat Ctr Novel Software Technol & In, MIIT Key Lab Pattern Anal & Machine Intelligence, Nanjing 211106, Peoples R China.
C3 Nanjing University of Aeronautics & Astronautics
RP Liu, NZ (corresponding author), Nanjing Univ Aeronaut & Astronaut, Coll Comp Sci & Technol, Nanjing 211106, Peoples R China.; Liu, NZ (corresponding author), Collaborat Innovat Ctr Novel Software Technol & In, MIIT Key Lab Pattern Anal & Machine Intelligence, Nanjing 211106, Peoples R China.
EM lnz_nuaa@163.com
RI Xia, Shifeng/JRX-0996-2023
CR Alonso I., 2010, IEEE ROBOT AUTOM LET, V2020
   Arjovsky M, 2017, PR MACH LEARN RES, V70
   Behley J, 2019, IEEE I CONF COMP VIS, P9296, DOI 10.1109/ICCV.2019.00939
   Chen C, 2020, AAAI CONF ARTIF INTE, V34, P3422
   Chen MH, 2019, IEEE I CONF COMP VIS, P2090, DOI 10.1109/ICCV.2019.00218
   Chen XZ, 2017, PROC CVPR IEEE, P6526, DOI 10.1109/CVPR.2017.691
   Cho KYHY, 2014, Arxiv, DOI [arXiv:1406.1078, DOI 10.3115/V1/D14-1179, 10.48550/ARXIV.1406.1078, DOI 10.48550/ARXIV.1406.1078]
   Choy C, 2019, PROC CVPR IEEE, P3070, DOI 10.1109/CVPR.2019.00319
   Elhadidy Asmaa, 2020, 2020 2nd Novel Intelligent and Leading Emerging Sciences Conference (NILES), P588, DOI 10.1109/NILES50944.2020.9257903
   Ganin Y, 2016, J MACH LEARN RES, V17
   Graham B, 2018, PROC CVPR IEEE, P9224, DOI 10.1109/CVPR.2018.00961
   Guo YL, 2021, IEEE T PATTERN ANAL, V43, P4338, DOI 10.1109/TPAMI.2020.3005434
   Hackel T, 2016, ISPRS ANN PHOTO REM, V3, P177, DOI 10.5194/isprsannals-III-3-177-2016
   Jaritz M., 2020, CVPR, P12605
   Jiang P, 2021, IEEE INT CONF ROBOT, P2457, DOI 10.1109/ICRA48506.2021.9561255
   Landrieu L, 2017, ISPRS J PHOTOGRAMM, V132, P102, DOI 10.1016/j.isprsjprs.2017.08.010
   Lang AH, 2019, PROC CVPR IEEE, P12689, DOI 10.1109/CVPR.2019.01298
   Le T, 2018, PROC CVPR IEEE, P9204, DOI 10.1109/CVPR.2018.00959
   Lee CY, 2019, PROC CVPR IEEE, P10277, DOI 10.1109/CVPR.2019.01053
   Li GH, 2019, IEEE I CONF COMP VIS, P9266, DOI 10.1109/ICCV.2019.00936
   Lin Tsung-Yi, 2020, IEEE Trans Pattern Anal Mach Intell, V42, P318, DOI 10.1109/TPAMI.2018.2858826
   Linderman GC, 2019, NAT METHODS, V16, P243, DOI 10.1038/s41592-018-0308-4
   Liu JX, 2019, IEEE I CONF COMP VIS, P7545, DOI 10.1109/ICCV.2019.00764
   Meng HY, 2019, IEEE I CONF COMP VIS, P8499, DOI 10.1109/ICCV.2019.00859
   Milioto A, 2019, IEEE INT C INT ROBOT, P4213, DOI 10.1109/IROS40897.2019.8967762
   Qi CR, 2017, ADV NEUR IN, V30
   Qingyong Hu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11105, DOI 10.1109/CVPR42600.2020.01112
   Saito K, 2018, PROC CVPR IEEE, P3723, DOI 10.1109/CVPR.2018.00392
   Shan TX, 2020, ROBOT AUTON SYST, V134, DOI 10.1016/j.robot.2020.103647
   Triess LT, 2021, 2021 IEEE INTELLIGENT VEHICLES SYMPOSIUM WORKSHOPS (IV WORKSHOPS), P350, DOI 10.1109/IVWorkshops54471.2021.9669228
   Triess LT, 2019, IEEE INT VEH SYM, P1512, DOI [10.1109/IVS.2019.8813771, 10.1109/ivs.2019.8813771]
   Vu TH, 2019, PROC CVPR IEEE, P2512, DOI 10.1109/CVPR.2019.00262
   Tzeng E, 2014, Arxiv, DOI [arXiv:1412.3474, 10.48550/arXiv.1412.3474, DOI 10.48550/ARXIV.1412.3474]
   Tzeng E, 2017, PROC CVPR IEEE, P2962, DOI 10.1109/CVPR.2017.316
   Wu BC, 2019, IEEE INT CONF ROBOT, P4376, DOI [10.1109/ICRA.2019.8793495, 10.1109/icra.2019.8793495]
   Wu BC, 2018, IEEE INT CONF ROBOT, P1887
   Xiao AR, 2021, Arxiv, DOI arXiv:2107.05399
   Yang B, 2018, PROC CVPR IEEE, P7652, DOI 10.1109/CVPR.2018.00798
   Yi L, 2021, PROC CVPR IEEE, P15358, DOI 10.1109/CVPR46437.2021.01511
   Zhao SC, 2021, Arxiv, DOI arXiv:2009.03456
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 41
TC 2
Z9 2
U1 2
U2 15
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2023
VL 39
IS 6
SI SI
BP 2471
EP 2481
DI 10.1007/s00371-023-02799-w
EA MAR 2023
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J1YY4
UT WOS:000945767400003
DA 2024-07-18
ER

EF