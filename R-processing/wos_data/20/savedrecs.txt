FN Clarivate Analytics Web of Science
VR 1.0
PT J
AU Liu, GJ
   Guo, JH
   Wang, YZ
   Liu, XG
   Ma, WT
   Zhang, R
AF Liu, Guojun
   Guo, Jianhui
   Wang, Yazhen
   Liu, Xiangguo
   Ma, Wentao
   Zhang, Rui
TI Local image segmentation model via Hellinger distance
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Active contour model; Image segmentation; Local region; Hellinger
   distance
ID ACTIVE CONTOURS DRIVEN; LEVEL SET EVOLUTION; ENERGY
AB Highly accurate active contour models are widely used in various image segmentation methods. In this paper, we propose an image segmentation model based on Hellinger distance for local region intensity fitting (HD-LRIF). The method defines two different metrics based on the Hellinger distance and constructs a new data fitting term to segment the image efficiently by minimizing the energy function. In addition, our method is independent of the initial contour and the segmentation results consistently obtain high accuracy. The experimental results show that the HD-LRIF model is far superior to state-of-the-art segmentation methods in terms of accuracy and efficiency. Specifically, it can effectively filter the noise interference, thus enhancing robustness and improving the accuracy of image segmentation in general.
C1 [Liu, Guojun; Guo, Jianhui; Wang, Yazhen; Liu, Xiangguo; Ma, Wentao; Zhang, Rui] Ningxia Univ, Sch Math & Stat, Yinchuan 750021, Peoples R China.
C3 Ningxia University
RP Guo, JH (corresponding author), Ningxia Univ, Sch Math & Stat, Yinchuan 750021, Peoples R China.
EM guojianhui3593@163.com
RI Liu, Guojun/A-9459-2015
FU National Natural Science Foundation of China [62061040, 12162029];
   National Natural Science Foundation of China [NYG2022018]; Scientific
   Research Fund of Ningxia University [2019BEG03056]; Key Research and
   Development Plan in Ningxia District
FX This work was partially supported by the National Natural Science
   Foundation of China (Grant No. 62061040, 12162029), in part by the
   Scientific Research Fund of Ningxia University (Grant No. NYG2022018),
   and in part by the Key Research and Development Plan in Ningxia District
   under Grant 2019BEG03056.
CR [Anonymous], 2014, INT C LEARN REPR
   Arbeláez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161
   Basu A, 2010, STAT PROBABIL LETT, V80, P206, DOI 10.1016/j.spl.2009.10.008
   Bouhlel N., 2020, IEEE Transactions on Geoscience and Remote Sensing, P1
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851
   Caselles V, 1997, INT J COMPUT VISION, V22, P61, DOI 10.1023/A:1007979827043
   Chan TF, 2001, IEEE T IMAGE PROCESS, V10, P266, DOI 10.1109/83.902291
   Chen L.C., 2017, P IEEE C COMP VIS PA, P851
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Ding KY, 2018, PATTERN RECOGN LETT, V104, P29, DOI 10.1016/j.patrec.2018.01.019
   Ding KY, 2017, SIGNAL PROCESS, V134, P224, DOI 10.1016/j.sigpro.2016.12.021
   Fang J., 2019, IEEE Access, DOI [10.1109/ACCESS.2019.2946976, DOI 10.1109/ACCESS.2019.2946976]
   KASS M, 1987, INT J COMPUT VISION, V1, P321, DOI 10.1007/BF00133570
   Li C, 2007, Computer vision and pattern recognition
   Li CM, 2010, IEEE T IMAGE PROCESS, V19, P3243, DOI 10.1109/TIP.2010.2069690
   Li CM, 2005, PATTERN RECOGN, V38, P1947, DOI 10.1016/j.patcog.2004.12.015
   Liu C, 2019, J VIS COMMUN IMAGE R, V59, P89, DOI 10.1016/j.jvcir.2019.01.001
   Liu XG, 2022, FRACTAL FRACT, V6, DOI 10.3390/fractalfract6090462
   Ma C, 2023, VISUAL COMPUT, V39, P4887, DOI 10.1007/s00371-022-02635-7
   Ma DD, 2019, SIGNAL PROCESS-IMAGE, V76, P201, DOI 10.1016/j.image.2019.05.006
   Merabet Y. E., 2017, INT JOINT C COMPUTER
   Min H, 2018, NEUROCOMPUTING, V311, P245, DOI 10.1016/j.neucom.2018.05.070
   MUMFORD D, 1989, COMMUN PUR APPL MATH, V42, P577, DOI 10.1002/cpa.3160420503
   Ouyang S, 2021, REMOTE SENS-BASEL, V13, DOI 10.3390/rs13010119
   Pan HZ, 2019, OPTIK, V182, P257, DOI 10.1016/j.ijleo.2019.01.009
   Pritpal S., 2021, Expert Syst. Appl, V186, P1
   Samad W., 2023, Signal Process, V211, DOI [10.1016/j.sigpro.2023.109105, DOI 10.1016/J.SIGPRO.2023.109105]
   Saman S, 2021, MULTIMED TOOLS APPL, V80, P21925, DOI 10.1007/s11042-021-10738-x
   Shu X, 2021, NEUROCOMPUTING, V453, P438, DOI 10.1016/j.neucom.2021.01.081
   Sobel I.E., 1970, Camera models and machine perception
   Wan MJ, 2023, DISPLAYS, V78, DOI 10.1016/j.displa.2023.102452
   Wang B, 2021, IEEE T IMAGE PROCESS, V30, P5402, DOI 10.1109/TIP.2021.3079798
   Wang D, 2021, VISUAL COMPUT, V37, P1101, DOI 10.1007/s00371-020-01855-z
   Wang L, 2017, INFORM SCIENCES, V418, P61, DOI 10.1016/j.ins.2017.06.042
   Wang L, 2009, SIGNAL PROCESS, V89, P2435, DOI 10.1016/j.sigpro.2009.03.014
   Weng GR, 2021, EXPERT SYST APPL, V185, DOI 10.1016/j.eswa.2021.115633
   Zhang KH, 2015, IEEE T CYBERNETICS, V45, P1426, DOI 10.1109/TCYB.2014.2352343
   Zhang KH, 2010, PATTERN RECOGN, V43, P1199, DOI 10.1016/j.patcog.2009.10.010
   Zhi XH, 2018, PATTERN RECOGN, V80, P241, DOI 10.1016/j.patcog.2018.03.010
NR 39
TC 0
Z9 0
U1 6
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2023 DEC 29
PY 2023
DI 10.1007/s00371-023-03213-1
EA DEC 2023
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DM5K2
UT WOS:001132471200004
DA 2024-07-18
ER

PT J
AU Zhang, YP
   Yang, ZP
   Ma, B
   Wu, JH
   Jin, FS
AF Zhang, Yuping
   Yang, Zepeng
   Ma, Bo
   Wu, Jiahao
   Jin, Fusheng
TI Customizing the feature modulation for visual tracking
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Visual tracking; Deep learning; Template-guided feature modulation
ID OBJECT TRACKING; ATTENTION; NETWORKS; MODEL
AB In visual tracking, the target always undergoes appearance variations due to a variety of challenging situations, such as deformation and rotation. In this paper, we propose the target-guided feature modulation network based on siamese network to extract the more target-relevant features, which is guided by a template to modulate the search features that can be directly used for classification and localization. Specifically, we customize two target-guided feature modulation subnetworks for visual tracking, which are called template-guided spatial-attention modulation subnetwork and template-guided channel-attention modulation subnetwork, to achieve this proposal. The former controls the discriminative region based on the correlation of each search feature position with all template feature positions, whereas the latter readjusts the importance of each channel based on the response value of each channel feature of the template feature and the response value of each channel of the search feature. Extensive experiments on multiple datasets have demonstrated the effectiveness of the proposed approach in this paper.
C1 [Zhang, Yuping; Yang, Zepeng; Ma, Bo; Wu, Jiahao; Jin, Fusheng] Beijing Inst Technol, Sch Comp Sci & Technol, 5 South St Zhongguancun, Beijing 100081, Peoples R China.
C3 Beijing Institute of Technology
RP Ma, B (corresponding author), Beijing Inst Technol, Sch Comp Sci & Technol, 5 South St Zhongguancun, Beijing 100081, Peoples R China.
EM bma000@bit.edu.cn
RI Yuan, Ye/KBC-9835-2024
OI Yuan, Ye/0009-0008-1640-7047
FU Innovative Research Group Project of the National Natural Science
   Foundation of China [62072042]; National Natural Science Foundation of
   China [2020YFC0832502]; National Key Research and Development Program of
   China
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 62072042 and in part by the National Key
   Research and Development Program of China under Grant 2020YFC0832502.
CR Abbass MY, 2021, VISUAL COMPUT, V37, P993, DOI 10.1007/s00371-020-01848-y
   Abbass MY, 2021, VISUAL COMPUT, V37, P831, DOI 10.1007/s00371-020-01833-5
   Bao JH, 2023, Arxiv, DOI arXiv:2303.12304
   Bertinetto L, 2016, PROC CVPR IEEE, P1401, DOI 10.1109/CVPR.2016.156
   Bertinetto L, 2016, LECT NOTES COMPUT SC, V9914, P850, DOI 10.1007/978-3-319-48881-3_56
   Bhat G, 2019, IEEE I CONF COMP VIS, P6181, DOI 10.1109/ICCV.2019.00628
   Bingyan Liao, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12367), P429, DOI 10.1007/978-3-030-58542-6_26
   Blatter P, 2023, IEEE WINT CONF APPL, P1571, DOI 10.1109/WACV56688.2023.00162
   Cao Y, 2022, IEEE T CIRC SYST VID, V32, P674, DOI 10.1109/TCSVT.2021.3063001
   Cao ZA, 2022, PROC CVPR IEEE, P14778, DOI 10.1109/CVPR52688.2022.01438
   Cao ZA, 2021, IEEE INT C INT ROBOT, P3086, DOI 10.1109/IROS51168.2021.9636309
   Cao ZA, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P15437, DOI 10.1109/ICCV48922.2021.01517
   Chang S, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19081858
   Chen BY, 2018, LECT NOTES COMPUT SC, V11211, P328, DOI 10.1007/978-3-030-01234-2_20
   Chen F, 2022, COMPUT VIS IMAGE UND, V222, DOI 10.1016/j.cviu.2022.103508
   Chen X, 2023, PROC CVPR IEEE, P14572, DOI 10.1109/CVPR52729.2023.01400
   Chen X, 2021, PROC CVPR IEEE, P8122, DOI 10.1109/CVPR46437.2021.00803
   Chen ZD, 2020, PROC CVPR IEEE, P6667, DOI 10.1109/CVPR42600.2020.00670
   Cheng SY, 2021, PROC CVPR IEEE, P4419, DOI 10.1109/CVPR46437.2021.00440
   Choi J, 2018, COMPUT VIS IMAGE UND, V171, P10, DOI 10.1016/j.cviu.2018.05.009
   Choi J, 2017, PROC CVPR IEEE, P4828, DOI 10.1109/CVPR.2017.513
   Cui YT, 2022, PROC CVPR IEEE, P13598, DOI 10.1109/CVPR52688.2022.01324
   Cui Z, 2016, PROC CVPR IEEE, P1449, DOI 10.1109/CVPR.2016.161
   Danelljan M, 2019, PROC CVPR IEEE, P4655, DOI 10.1109/CVPR.2019.00479
   Danelljan M, 2017, PROC CVPR IEEE, P6931, DOI 10.1109/CVPR.2017.733
   Danelljan M, 2016, LECT NOTES COMPUT SC, V9909, P472, DOI 10.1007/978-3-319-46454-1_29
   Danelljan M, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P621, DOI 10.1109/ICCVW.2015.84
   Das S, 2019, IEEE I CONF COMP VIS, P833, DOI 10.1109/ICCV.2019.00092
   Das S, 2019, IEEE WINT CONF APPL, P71, DOI 10.1109/WACV.2019.00015
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dong XP, 2018, LECT NOTES COMPUT SC, V11217, P472, DOI 10.1007/978-3-030-01261-8_28
   Fan H, 2019, PROC CVPR IEEE, P7944, DOI 10.1109/CVPR.2019.00814
   Fan H, 2019, PROC CVPR IEEE, P5369, DOI 10.1109/CVPR.2019.00552
   Fan H, 2017, IEEE I CONF COMP VIS, P5487, DOI 10.1109/ICCV.2017.585
   Fan JL, 2010, LECT NOTES COMPUT SC, V6311, P480
   Fu C., 2021, IEEE T MULTIMEDIA
   Fu CH, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2021.3083880
   Fu ZH, 2021, PROC CVPR IEEE, P13769, DOI 10.1109/CVPR46437.2021.01356
   Gao JY, 2019, PROC CVPR IEEE, P4644, DOI 10.1109/CVPR.2019.00478
   Guo DY, 2021, PROC CVPR IEEE, P9538, DOI 10.1109/CVPR46437.2021.00942
   Guo DY, 2020, PROC CVPR IEEE, P6268, DOI 10.1109/CVPR42600.2020.00630
   Guo Q, 2017, IEEE I CONF COMP VIS, P1781, DOI 10.1109/ICCV.2017.196
   Han WC, 2021, PROC CVPR IEEE, P16565, DOI 10.1109/CVPR46437.2021.01630
   He AF, 2018, PROC CVPR IEEE, P4834, DOI 10.1109/CVPR.2018.00508
   Henriques JF, 2015, IEEE T PATTERN ANAL, V37, P583, DOI 10.1109/TPAMI.2014.2345390
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Hu WM, 2023, IEEE T PATTERN ANAL, V45, P3072, DOI 10.1109/TPAMI.2022.3172932
   Huang LH, 2020, AAAI CONF ARTIF INTE, V34, P11037
   Huang LH, 2021, IEEE T PATTERN ANAL, V43, P1562, DOI 10.1109/TPAMI.2019.2957464
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Ke L, 2021, ADV NEUR IN, V34
   Li B, 2019, PROC CVPR IEEE, P4277, DOI 10.1109/CVPR.2019.00441
   Li B, 2018, PROC CVPR IEEE, P8971, DOI 10.1109/CVPR.2018.00935
   Li F, 2018, PROC CVPR IEEE, P4904, DOI 10.1109/CVPR.2018.00515
   Li PX, 2019, IEEE I CONF COMP VIS, P6161, DOI 10.1109/ICCV.2019.00626
   Li SY, 2017, AAAI CONF ARTIF INTE, P4140
   Li X, 2019, PROC CVPR IEEE, P1369, DOI 10.1109/CVPR.2019.00146
   Liang PP, 2015, IEEE T IMAGE PROCESS, V24, P5630, DOI 10.1109/TIP.2015.2482905
   Lin LT, 2022, ADV NEUR IN
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lukezic A, 2020, PROC CVPR IEEE, P7131, DOI 10.1109/CVPR42600.2020.00716
   Lukezic A, 2017, PROC CVPR IEEE, P4847, DOI 10.1109/CVPR.2017.515
   Marvasti-Zadeh SM, 2022, IEEE T INTELL TRANSP, V23, P3943, DOI 10.1109/TITS.2020.3046478
   Mayer C, 2022, PROC CVPR IEEE, P8721, DOI 10.1109/CVPR52688.2022.00853
   Nam H, 2016, PROC CVPR IEEE, P4293, DOI 10.1109/CVPR.2016.465
   Ouerhani N, 2003, LECT NOTES COMPUT SC, V2686, P702
   Qiu J, 2020, COMPUT VIS IMAGE UND, V195, DOI 10.1016/j.cviu.2020.102951
   Roy AG, 2018, LECT NOTES COMPUT SC, V11070, P421, DOI 10.1007/978-3-030-00928-1_48
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Song YB, 2018, PROC CVPR IEEE, P8990, DOI 10.1109/CVPR.2018.00937
   Song ZK, 2022, PROC CVPR IEEE, P8781, DOI 10.1109/CVPR52688.2022.00859
   Tang F, 2022, PROC CVPR IEEE, P8731, DOI 10.1109/CVPR52688.2022.00854
   Tang YY, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P926
   Valmadre J, 2017, PROC CVPR IEEE, P5000, DOI 10.1109/CVPR.2017.531
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441
   Wang L., 2020, P 29 ACM INT C MULT
   Wang L, 2023, PROC CVPR IEEE, P5620, DOI 10.1109/CVPR52729.2023.00544
   Wang L, 2019, IEEE I CONF COMP VIS, P8697, DOI 10.1109/ICCV.2019.00879
   Wang L, 2020, IEEE T IMAGE PROCESS, V29, P15, DOI 10.1109/TIP.2019.2925285
   Wang LJ, 2015, IEEE I CONF COMP VIS, P3119, DOI 10.1109/ICCV.2015.357
   Wang NY, 2015, IEEE I CONF COMP VIS, P3101, DOI 10.1109/ICCV.2015.355
   Wang N, 2021, PROC CVPR IEEE, P1571, DOI 10.1109/CVPR46437.2021.00162
   Wang N, 2018, PROC CVPR IEEE, P4844, DOI 10.1109/CVPR.2018.00509
   Wang Q, 2019, PROC CVPR IEEE, P1328, DOI 10.1109/CVPR.2019.00142
   Wang Q, 2018, PROC CVPR IEEE, P4854, DOI 10.1109/CVPR.2018.00510
   Wang X., 2022, 2022 IEEE INT C MULT, P01
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Xing D., 2022, P IEEE CVF WINT C AP, P2139
   Xingping Dong, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12365), P378, DOI 10.1007/978-3-030-58565-5_23
   Xu TY, 2023, IEEE T IMAGE PROCESS, V32, P1541, DOI 10.1109/TIP.2023.3246800
   Yan B, 2021, PROC CVPR IEEE, P5285, DOI 10.1109/CVPR46437.2021.00525
   Yan B, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P10428, DOI 10.1109/ICCV48922.2021.01028
   Yang M, 2007, PROC CVPR IEEE, P1590, DOI 10.1109/CVPR.2007.383178
   Yang TY, 2018, LECT NOTES COMPUT SC, V11213, P153, DOI 10.1007/978-3-030-01240-3_10
   Ye JJ, 2022, PROC CVPR IEEE, P8886, DOI 10.1109/CVPR52688.2022.00869
   Yiming Li, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11920, DOI 10.1109/CVPR42600.2020.01194
   Yu YC, 2020, PROC CVPR IEEE, P6727, DOI 10.1109/CVPR42600.2020.00676
   Yuan Y., 2022, 2022 IEEE INT C MULT, P1
   Zhang HR, 2022, COMPLEX INTELL SYST, V8, P4251, DOI 10.1007/s40747-022-00712-x
   Zhang JM, 2014, LECT NOTES COMPUT SC, V8694, P188, DOI 10.1007/978-3-319-10599-4_13
   Zhang ZP, 2019, PROC CVPR IEEE, P4586, DOI 10.1109/CVPR.2019.00472
   Zhipeng Zhang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12366), P771, DOI 10.1007/978-3-030-58589-1_46
   Zhu XF, 2022, IEEE T MULTIMEDIA, V24, P301, DOI 10.1109/TMM.2021.3050073
   Zhu Z., 2018, 2018 IEEE International Magnetics Conference (INTERMAG), DOI 10.1109/INTMAG.2018.8508710
NR 106
TC 0
Z9 0
U1 5
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2023 DEC 29
PY 2023
DI 10.1007/s00371-023-03182-5
EA DEC 2023
PG 20
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DM5K2
UT WOS:001132471200001
DA 2024-07-18
ER

PT J
AU Hu, JP
   Dai, MM
   Wang, XC
   Xie, Q
   Zhang, DC
AF Hu, Jianping
   Dai, Minmin
   Wang, Xiaochao
   Xie, Qi
   Zhang, Daochang
TI Robust 3D watermarking with high imperceptibility based on EMD on
   surfaces
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE 3D watermarking; Empirical mode decomposition; Copyright protection;
   Robustness and imperceptibility
ID COMPUTATION; TRANSFORM; SCHEME
AB The rising use of 3D digital products has increased the demand for copyright protection. In this paper, we propose a novel and robust 3D watermarking method with high imperceptibility based on EMD (empirical mode decomposition) on surfaces. It first defines a normalized modulus signal on a 3D host model so as to involve EMD into 3D watermarking effectively. And then, it extracts different scale features of the defined signal by using EMD to locate the proper embedding positions. After this, the watermark signal is embedded repeatedly and cyclically into the 3D host model to enhance the robustness. The embedding strength is optimized according to a predefined fidelity parameter to control the imperceptibility of the watermark. Many experiment results show that the proposed method can obtain good results against various attacks while maintaining high invisibility, such as pseudo-random noise, Laplacian smoothing, simplification, subdivision, cropping, and similarity transformation. Furthermore, it is very competitive with the current state-of-the-art 3D watermarking techniques in terms of robustness and invisibility.
C1 [Hu, Jianping; Dai, Minmin; Xie, Qi; Zhang, Daochang] Northeast Elect Power Univ, Sch Sci, Jilin 132012, Peoples R China.
   [Wang, Xiaochao] Tiangong Univ, Sch Math Sci, Tianjin 300387, Peoples R China.
C3 Northeast Electric Power University; Tiangong University
RP Wang, XC (corresponding author), Tiangong Univ, Sch Math Sci, Tianjin 300387, Peoples R China.
EM neduhjp307@163.com; 13156768514@163.com; wangxiaochao18@163.com;
   xieqi_19820302@126.com; daochangzhang@126.com
OI Wang, Xiaochao/0000-0001-7852-2106; Hu, Jianping/0000-0001-8675-4564
FU Natural Science Foundation of Jilin Province [20210101472JC]; Natural
   Science Foundation of Jilin Province, China
FX This work was supported by the Natural Science Foundation of Jilin
   Province, China (No. 20210101472JC).
CR Abbas NA, 2016, EGYPT INFORM J, V17, P139, DOI 10.1016/j.eij.2015.10.001
   Abbas NH, 2018, MULTIMED TOOLS APPL, V77, P24593, DOI 10.1007/s11042-017-5488-x
   Abulkasim H, 2022, CMC-COMPUT MATER CON, V72, P5969, DOI 10.32604/cmc.2022.027999
   Al-Saadi HS., 2021, Comput. Intell. Neurosci, V2021, P11
   Benedens O., 1999, P MULT SEC WORKSH AC, V99, P95
   Bi N, 2007, IEEE T IMAGE PROCESS, V16, P1956, DOI 10.1109/TIP.2007.901206
   Cho JW, 2007, IEEE T SIGNAL PROCES, V55, P142, DOI 10.1109/TSP.2006.882111
   Cui CC, 2017, LECT NOTES COMPUT SC, V10431, P318, DOI 10.1007/978-3-319-64185-0_24
   Delmotte A, 2021, IEEE T MULTIMEDIA, V23, P3467, DOI 10.1109/TMM.2020.3025660
   Gao Y, 2013, IEEE T VIS COMPUT GR, V19, P178, DOI 10.1109/TVCG.2012.117
   Hilbert D., 1935, Physik Verschiedenes: Nebst Einer Lebensgeschichte, P1
   Hou JU, 2017, IEEE T INF FOREN SEC, V12, P2712, DOI 10.1109/TIFS.2017.2718482
   Hu JP, 2016, COMPUT AIDED GEOM D, V43, P95, DOI 10.1016/j.cagd.2016.02.011
   Hu K, 2022, VISUAL COMPUT, V38, P2153, DOI 10.1007/s00371-021-02275-3
   Hu K, 2021, VISUAL COMPUT, V37, P2841, DOI 10.1007/s00371-021-02168-5
   Huang NE, 1998, P ROY SOC A-MATH PHY, V454, P903, DOI 10.1098/rspa.1998.0193
   Kai Wang, 2010, Proceedings of the Shape Modeling International (SMI 2010), P231, DOI 10.1109/SMI.2010.33
   Kanai S., 1998, IFIP WG, P296
   Kashida N, 2020, 35TH INTERNATIONAL TECHNICAL CONFERENCE ON CIRCUITS/SYSTEMS, COMPUTERS AND COMMUNICATIONS (ITC-CSCC 2020), P363
   Lee JS, 2023, J INF SECUR APPL, V73, DOI 10.1016/j.jisa.2022.103414
   Lee JS, 2021, MULTIMED TOOLS APPL, V80, P25757, DOI 10.1007/s11042-021-10878-0
   Liu G, 2021, NEUROCOMPUTING, V421, P39
   Liu J, 2018, IEEE ACCESS, V6, P56122, DOI 10.1109/ACCESS.2018.2872783
   Liu J, 2017, NEUROCOMPUTING, V237, P304, DOI 10.1016/j.neucom.2016.12.065
   Medimegh N, 2020, MULTIMED TOOLS APPL, V79, P25889, DOI 10.1007/s11042-020-09241-6
   Medimegh N, 2018, MULTIMED TOOLS APPL, V77, P32287, DOI 10.1007/s11042-018-6252-6
   Narendra M, 2022, MULTIMEDIA SYST, V28, P623, DOI 10.1007/s00530-021-00860-z
   Nunes JC, 2003, IMAGE VISION COMPUT, V21, P1019, DOI 10.1016/S0262-8856(03)00094-5
   Ohbuchi R., 1997, Interactive Distributed Multimedia Systems and Telecommunication Services. 4th International Workshop, IDMS '97. Proceedings, P1, DOI 10.1007/BFb0000334
   Ohbuchi R., 2001, Graphics Interface, P9
   Ren B, 2013, IEEE T VIS COMPUT GR, V19, P1708, DOI 10.1109/TVCG.2013.73
   Sayahi I, 2023, MULTIMED TOOLS APPL, V82, P39841, DOI 10.1007/s11042-023-14722-5
   Sharma N, 2020, IET INFORM SECUR, V14, P745, DOI 10.1049/iet-ifs.2019.0601
   Su Cai, 2011, Journal of Multimedia, V6, P83, DOI 10.4304/jmm.6.1.83-90
   Wang H, 2012, GRAPH MODELS, V74, P173, DOI 10.1016/j.gmod.2012.04.005
   Wang K, 2007, 2007 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOLS 1-5, P1235
   Wang XC, 2020, VISUAL COMPUT, V36, P2201, DOI 10.1007/s00371-020-01909-2
   Wang XC, 2015, VISUAL COMPUT, V31, P1135, DOI 10.1007/s00371-015-1100-4
   Wang X, 2019, MULTIMED TOOLS APPL, V78, P27001, DOI 10.1007/s11042-017-4666-1
   Wang YH, 2017, IET IMAGE PROCESS, V11, P822, DOI 10.1049/iet-ipr.2016.0927
   Wang YP, 2009, IEEE T VIS COMPUT GR, V15, P285, DOI 10.1109/TVCG.2008.101
   Xie Q, 2023, DIGIT SIGNAL PROCESS, V133, DOI 10.1016/j.dsp.2022.103891
   Xie Q, 2023, VISUAL COMPUT, V39, P4249, DOI 10.1007/s00371-022-02588-x
   Zhang JW, 2009, I C COMP GRAPH IM VI, P510, DOI 10.1109/CGIV.2009.49
NR 44
TC 0
Z9 0
U1 5
U2 7
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2023 DEC 21
PY 2023
DI 10.1007/s00371-023-03201-5
EA DEC 2023
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CS9L1
UT WOS:001127346100001
DA 2024-07-18
ER

PT J
AU Tu, JF
   Zang, T
   Duan, MR
   Jiang, HR
   Zhao, JH
   Jiang, N
   Liu, LF
AF Tu, Jianfeng
   Zang, Tuo
   Duan, Mengran
   Jiang, Hanrui
   Zhao, Jiahui
   Jiang, Nan
   Liu, Lingfeng
TI MFOGCN: multi-feature-based orthogonal graph convolutional network for
   3D human motion prediction
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Motion prediction; Spectral transform; Attention perception; Graph
   convolutional network; Spatio-temporal
AB Human motion prediction in various motion capture applications, e.g., optical and inertial, is challenging because of the complexity of human motion sequences. Current studies on this issue have insufficient analysis on the latent motion information in a given motion sequence, such as motion trends, transient changes, and temporal evolution. Meanwhile, methods using simple graph convolution networks suffer from over-smoothing, causing the predicted poses staying invariant in long-term prediction. To address these challenges, we propose a multi-feature-based orthogonal graph convolution network (MFOGCN), where the multi-feature extraction consists of two key modules: (1) hybrid spectral transform, which captures local transient features and global motion trends of motion sequences by discrete wavelet transform while considering temporal smoothing between human joints and (2) mask-aware multiple attention, with sliding time windows to extract motion sequence feature representations from historical multiple subsequences, refining the correlation between adjacent poses while obtaining global dependencies between sequences. In addition, we propose orthogonal graph convolution and orthogonal loss for the prediction network, which help to stabilize the feature transformation of the graph convolution to resolve the over-smoothing issue. An extensive evaluation on the Human 3.6M, AMASS and 3DPW datasets has been conducted, showing reliable effectiveness of the proposed MFOGCN that outperforms other approaches.
C1 [Tu, Jianfeng; Zang, Tuo; Duan, Mengran; Jiang, Hanrui; Zhao, Jiahui; Jiang, Nan; Liu, Lingfeng] East China Jiaotong Univ, Sch Informat Engn Dept, Nanchang 330013, Peoples R China.
   [Liu, Lingfeng] Minxuan Intelligent Technol Co Ltd, Nanchang 330029, Peoples R China.
C3 East China Jiaotong University
RP Liu, LF (corresponding author), East China Jiaotong Univ, Sch Informat Engn Dept, Nanchang 330013, Peoples R China.; Liu, LF (corresponding author), Minxuan Intelligent Technol Co Ltd, Nanchang 330029, Peoples R China.
EM jianfeng.tu@ecjtu.edu.cn; tuo.zang@ecjtu.edu.cn; mengran_duan@163.com;
   henryy_jiang@163.com; zhaojiahui@foxmail.com; jiangnan1018@acm.org;
   lingfeng.liu@ecjtu.edu.cn
RI Liu, Lingfeng/W-7547-2018; Zhao, Jiahui/JBJ-5175-2023
OI Zhao, Jiahui/0000-0001-6497-3547
FU Major Discipline Academic and Technical Leaders Training Program of
   Jiangxi Province [20225BCJ22012]; Major Discipline Academic and
   Technical Leaders Training Program of Jiangxi Province [61801180];
   National Natural Science Foundation of China [20202BAB202003]; Natural
   Science Foundation of Jiangxi Province
FX This research was funded by the Major Discipline Academic and Technical
   Leaders Training Program of Jiangxi Province under Grant
   No.20225BCJ22012, the National Natural Science Foundation of China under
   Grant No.61801180, the Natural Science Foundation of Jiangxi Province
   under Grant No.20202BAB202003.
CR Aksan E, 2021, INT CONF 3D VISION, P565, DOI 10.1109/3DV53792.2021.00066
   Aksan E, 2019, IEEE I CONF COMP VIS, P7143, DOI 10.1109/ICCV.2019.00724
   [Anonymous], 2006, Adv. Neural Inf. Process. Syst.
   Cao WM, 2022, NEUROCOMPUTING, V493, P106, DOI 10.1016/j.neucom.2022.04.047
   Chen DL, 2020, AAAI CONF ARTIF INTE, V34, P3438
   Chen M, 2020, INT C MACHINE LEARNI, V119
   Corona E, 2020, PROC CVPR IEEE, P6990, DOI 10.1109/CVPR42600.2020.00702
   Cui QJ, 2020, PROC CVPR IEEE, P6518, DOI 10.1109/CVPR42600.2020.00655
   Cui Qiongjie, 2021, P IEEE CVF C COMP VI, P4801
   Dang LW, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11447, DOI 10.1109/ICCV48922.2021.01127
   Fang YY, 2022, VISUAL COMPUT, V38, P2257, DOI 10.1007/s00371-021-02109-2
   Fragkiadaki K, 2015, IEEE I CONF COMP VIS, P4346, DOI 10.1109/ICCV.2015.494
   Ghosh P, 2017, INT CONF 3D VISION, P458, DOI 10.1109/3DV.2017.00059
   Gopalakrishnan A, 2019, PROC CVPR IEEE, P12108, DOI 10.1109/CVPR.2019.01239
   Gui LY, 2018, LECT NOTES COMPUT SC, V11208, P823, DOI 10.1007/978-3-030-01225-0_48
   Gui LY, 2018, IEEE INT C INT ROBOT, P562, DOI 10.1109/IROS.2018.8594452
   Guo K, 2022, AAAI CONF ARTIF INTE, P3996
   Guo X, 2019, AAAI CONF ARTIF INTE, P2580
   Ionescu C, 2014, IEEE T PATTERN ANAL, V36, P1325, DOI 10.1109/TPAMI.2013.248
   Jain A, 2016, PROC CVPR IEEE, P5308, DOI 10.1109/CVPR.2016.573
   Lebailly T., 2021, Lecture Notes in Computer Science, V12623, P651, DOI [DOI 10.1007/978-3-030-69532-3_39, 10.1007/978-3-030-69532-3_39]
   Lehrmann AM, 2014, PROC CVPR IEEE, P1314, DOI 10.1109/CVPR.2014.171
   Li C, 2018, PROC CVPR IEEE, P5226, DOI 10.1109/CVPR.2018.00548
   Li MS, 2021, IEEE INT CONF COMP V, P854, DOI 10.1109/ICCVW54120.2021.00101
   Li QM, 2018, AAAI CONF ARTIF INTE, P3538
   Li YR, 2019, VISUAL COMPUT, V35, P1143, DOI 10.1007/s00371-019-01692-9
   Liu ZG, 2019, PROC CVPR IEEE, P9996, DOI 10.1109/CVPR.2019.01024
   Ma QL, 2022, APPL INTELL, V52, P3018, DOI 10.1007/s10489-021-02562-5
   Ma TZ, 2022, PROC CVPR IEEE, P6427, DOI 10.1109/CVPR52688.2022.00633
   Mahmood N, 2019, IEEE I CONF COMP VIS, P5441, DOI 10.1109/ICCV.2019.00554
   Mao W, 2021, INT J COMPUT VISION, V129, P2513, DOI 10.1007/s11263-021-01483-7
   Mao W, 2019, IEEE I CONF COMP VIS, P9488, DOI 10.1109/ICCV.2019.00958
   Martinez J, 2017, PROC CVPR IEEE, P4674, DOI 10.1109/CVPR.2017.497
   Medjaouri Omar, 2022, P IEEE CVF C COMP VI, P2540
   Singh D, 2022, APPL INTELL, V52, P12801, DOI 10.1007/s10489-021-03120-9
   Sofianos T, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11189, DOI 10.1109/ICCV48922.2021.01102
   Tang YY, 2018, Arxiv, DOI arXiv:1805.02513
   Vaswani A, 2017, ADV NEUR IN, V30
   von Marcard T, 2018, LECT NOTES COMPUT SC, V11214, P614, DOI 10.1007/978-3-030-01249-6_37
   Wang JM, 2008, IEEE T PATTERN ANAL, V30, P283, DOI 10.1109/TPAMI.2007.1167
   Wei Mao, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P474, DOI 10.1007/978-3-030-58568-6_28
   Yadav GK, 2023, APPL INTELL, V53, P18027, DOI 10.1007/s10489-022-04419-x
   Yan SJ, 2018, AAAI CONF ARTIF INTE, P7444
   Yu Y., 2022, Appl. Intell., P1
   Zhang H., 2023, Vis. Comput., P1
NR 45
TC 0
Z9 0
U1 8
U2 8
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2023 NOV 30
PY 2023
DI 10.1007/s00371-023-03152-x
EA NOV 2023
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AZ4K2
UT WOS:001122252600003
DA 2024-07-18
ER

PT J
AU Yang, Y
   Wu, D
   Tang, L
   Zeng, LL
   Pan, ZG
AF Yang, Yang
   Wu, Dan
   Tang, Ling
   Zeng, Lanling
   Pan, Zhigeng
TI Weighted and truncated L<sub>1</sub> image smoothing based on
   unsupervised learning
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Image smoothing; Optimization model; Unsupervised learning;
   Computational photography
ID BILATERAL FILTER
AB Edge-preserving image smoothing plays a vital role in the field of computational photography. In this paper, we propose a weighted and truncated L-1-regularized optimization model for image smoothing. We show that the weighted and truncated scheme significantly promotes the edge-preserving property. Furthermore, we propose a deep unsupervised learning-based filter based on the loss function defined by the proposed optimization model. The proposed filter leverages a U-Net structure, which fully exploits the spatially varying smoothing scales of the edge-preserving filtering. We have conducted extensive experiments to evaluate the proposed filter. The results suggest that our filter outperforms the state-of-the-art filters in image quality on various tasks, such as image smoothing, detail enhancing, HDR tone mapping, and edge detection. Meanwhile, our filter is extremely efficient. It is able to process 720P images in real-time (more than 16 frames per second) on a modern desktop with an Intel i7-8700K CPU, an NVIDIA GTX 1080 GPU and 16GB memory.
C1 [Yang, Yang; Wu, Dan; Tang, Ling; Zeng, Lanling] Jiangsu Univ, Dept Comp Sci, Zhenjiang 212013, Jiangsu, Peoples R China.
   [Pan, Zhigeng] Nanjing Univ Informat Sci & Technol, Sch Artificial Intelligence, Nanjing 210044, Jiangsu, Peoples R China.
C3 Jiangsu University; Nanjing University of Information Science &
   Technology
RP Yang, Y (corresponding author), Jiangsu Univ, Dept Comp Sci, Zhenjiang 212013, Jiangsu, Peoples R China.
EM yyoung@ujs.edu.cn; 2222008070@stmail.ujs.edu.cn;
   2222008056@stmail.ujs.edu.cn; lanling73@163.com; 003443@nuist.edu.cn
OI , Yang/0000-0001-8782-4819
CR Arbeláez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161
   Bi S, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766946
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851
   Chen QF, 2017, IEEE I CONF COMP VIS, P2516, DOI 10.1109/ICCV.2017.273
   Dai LQ, 2020, IEEE T CIRC SYST VID, V30, P603, DOI 10.1109/TCSVT.2019.2893322
   DeCarlo D, 2002, ACM T GRAPHIC, V21, P769, DOI 10.1145/566570.566650
   Durand F, 2002, ACM T GRAPHIC, V21, P257, DOI 10.1145/566570.566574
   Fan QN, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275081
   Farbman Z, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360666
   Fattal R, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531328
   Feng YD, 2022, IEEE T NEUR NET LEAR, V33, P7223, DOI 10.1109/TNNLS.2021.3084473
   Ferrari V, 2008, IEEE T PATTERN ANAL, V30, P36, DOI 10.1109/TPAMI.2007.1144
   Gastal ESL, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964964
   Gharbi M, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073592
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   He KM, 2013, IEEE T PATTERN ANAL, V35, P1397, DOI 10.1109/TPAMI.2012.213
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Kingma D. P, 2015, INT C LEARNING REPRE, DOI DOI 10.48550/ARXIV.1412.6980
   Liu SF, 2016, LECT NOTES COMPUT SC, V9908, P560, DOI 10.1007/978-3-319-46493-0_34
   Liu W, 2022, IEEE T PATTERN ANAL, V44, P6631, DOI 10.1109/TPAMI.2021.3097891
   Liu W, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3388887
   Liu W, 2020, IEEE T CIRC SYST VID, V30, P23, DOI 10.1109/TCSVT.2018.2890202
   Liu W, 2017, IEEE I CONF COMP VIS, pCP32, DOI 10.1109/ICCV.2017.624
   Ma X, 2021, COMPUT VIS MEDIA, V7, P483, DOI 10.1007/s41095-021-0220-1
   Paris S, 2009, INT J COMPUT VISION, V81, P24, DOI 10.1007/s11263-007-0110-8
   PERONA P, 1990, IEEE T PATTERN ANAL, V12, P629, DOI 10.1109/34.56205
   Rao SN, 2023, VISUAL COMPUT, V39, P835, DOI 10.1007/s00371-021-02349-2
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720
   RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F
   Song XJ, 2023, VISUAL COMPUT, V39, P489, DOI 10.1007/s00371-021-02343-8
   Sun ZG, 2020, IEEE T IMAGE PROCESS, V29, P500, DOI 10.1109/TIP.2019.2928631
   Tan AL, 2023, VISUAL COMPUT, V39, P6491, DOI 10.1007/s00371-022-02741-6
   Tomasi C., 1998, 6 INT C COMP VIS IEE, P839
   Wang CX, 2021, VISUAL COMPUT, V37, P77, DOI 10.1007/s00371-020-01888-4
   Wang H., 2015, COMPUT VIS MEDIA, V1, P27
   Winnemöller H, 2006, ACM T GRAPHIC, V25, P1221, DOI 10.1145/1141911.1142018
   Wu HK, 2018, PROC CVPR IEEE, P1838, DOI 10.1109/CVPR.2018.00197
   Xiao CX, 2012, VISUAL COMPUT, V28, P713, DOI 10.1007/s00371-012-0679-y
   Xie ZF, 2012, VISUAL COMPUT, V28, P1195, DOI 10.1007/s00371-011-0668-6
   Xu J, 2021, IEEE T MULTIMEDIA, V23, P4065, DOI 10.1109/TMM.2020.3037535
   Xu L, 2015, PR MACH LEARN RES, V37, P1669
   Xu L, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366158
   Xu L, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024208
   Yang Y, 2023, IEEE T MULTIMEDIA, V25, P4148, DOI 10.1109/TMM.2022.3171686
   Yang Y, 2022, IEEE T CIRC SYST VID, V32, P4150, DOI 10.1109/TCSVT.2021.3124291
   Yin H, 2019, PROC CVPR IEEE, P8750, DOI 10.1109/CVPR.2019.00896
   Zhang Q, 2014, PROC CVPR IEEE, P2830, DOI 10.1109/CVPR.2014.362
   Zhang Q, 2014, LECT NOTES COMPUT SC, V8691, P815, DOI 10.1007/978-3-319-10578-9_53
   Zhou PC, 2023, VISUAL COMPUT, V39, P1533, DOI 10.1007/s00371-022-02427-z
   Zhu FD, 2019, IEEE T IMAGE PROCESS, V28, P3556, DOI 10.1109/TIP.2019.2908778
NR 51
TC 0
Z9 0
U1 6
U2 8
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2023 NOV 20
PY 2023
DI 10.1007/s00371-023-03141-0
EA NOV 2023
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA Y6GA9
UT WOS:001106212200001
DA 2024-07-18
ER

PT J
AU Li, XY
   Yang, B
   Chen, TD
   Gao, Z
   Li, HJ
AF Li, Xiaoyu
   Yang, Bei
   Chen, Tiandong
   Gao, Zheng
   Li, Huijie
TI Multiple instance learning-based two-stage metric learning network for
   whole slide image classification
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Whole slide image; Metric learning; Multiple instance learning; Cancer
ID DEEP; WSI
AB Cancer is one of the most common diseases around the world. For cancer diagnosis, pathological examination is the most effective method. But the heavy and time-consuming workflow has increased the workload of the pathologists. With the appearance of whole slide image (WSI) scanners, tissues on a glass slide can be saved as a high-definition digital image, which makes it possible to diagnose diseases with computer aid. However, the extreme size and the lack of pixel-level annotations of WSIs make machine learning face a great challenge in pathology image diagnosis. To solve this problem, we propose a metric learning-based two-stage MIL framework (TSMIL) for WSI classification, which combines two stages of supervised clustering and metric-based classification. The training samples (WSIs) are first clustered into different clusters based on their labels in supervised clustering. Then, based on the previous step, we propose four different strategies to measure the distance of the test samples to each class cluster to achieve the test samples classification: MaxS, AvgS, DenS and HybS. Our model is evaluated on three pathology datasets: TCGA-NSCLC, TCGA-RCC and HER2. The average AUC scores can be up to 0.9895 and 0.9988 over TCGA-NSCLC and TCGA-RCC, and 0.9265 on HER2, respectively. The results showed that compared with the state-of-the-art methods, our method outperformed. The excellent performance on different kinds of cancer datasets verifies the feasibility of our method as a general architecture.
C1 [Li, Xiaoyu; Yang, Bei; Gao, Zheng] Zhengzhou Univ, Sch Comp & Artificial Intelligence, Zhengzhou 450001, Henan, Peoples R China.
   [Chen, Tiandong] Zhengzhou Univ, Affiliated Canc Hosp, Dept Pathol, Zhengzhou 450008, Henan, Peoples R China.
   [Chen, Tiandong] Henan Canc Hosp, Zhengzhou 450008, Henan, Peoples R China.
   [Li, Huijie] Henan Univ Technol, Coll Elect Engn, Zhengzhou 450001, Henan, Peoples R China.
C3 Zhengzhou University; Zhengzhou University; Zhengzhou University; Henan
   University of Technology
RP Yang, B (corresponding author), Zhengzhou Univ, Sch Comp & Artificial Intelligence, Zhengzhou 450001, Henan, Peoples R China.
EM 202012172013073@gs.zzu.edu.cn; iebyang@zzu.edu.cn; ctd226@126.com;
   uagaozheng@gs.zzu.edu.cn; 202023010628@stu.haut.edu.cn
FU This work was supported by the Key Research Projects of Universities in
   Henan Province of China under Grant Number 17A520015, the Key Scientific
   and Technological Research Projects in Henan Province of China under
   Grant Numbers 192102310216 and 23210221000 [17A520015]; Key Research
   Projects of Universities in Henan Province of China [192102310216,
   232102210006]; Key Scientific and Technological Research Projects in
   Henan Province of China [221100210600]; Major Research Programs of
   Science and Technology in Henan Province of China [201400210400]; Major
   Science and Technology Project in Henan Province of China
   [LHGJ20220215]; Medical Science and Technology Joint Construction
   Project in Henan Province of China
FX This work was supported by the Key Research Projects of Universities in
   Henan Province of China under Grant Number 17A520015, the Key Scientific
   and Technological Research Projects in Henan Province of China under
   Grant Numbers 192102310216 and 232102210006, the Major Research Programs
   of Science and Technology in Henan Province of China under Grant Number
   221100210600, the Major Science and Technology Project in Henan Province
   of China under Grant Number 201400210400 and the Medical Science and
   Technology Joint Construction Project in Henan Province of China under
   Grant Number LHGJ20220215.
CR Ahmad N, 2022, VISUAL COMPUT, V38, P2751, DOI 10.1007/s00371-021-02153-y
   Amemiya T, 2022, VISUAL COMPUT, V38, P4083, DOI 10.1007/s00371-022-02666-0
   [Anonymous], 2020, Technical report
   Ben Hamida A, 2022, ARTIF INTELL MED, V133, DOI 10.1016/j.artmed.2022.102407
   Campanella G, 2019, NAT MED, V25, P1301, DOI 10.1038/s41591-019-0508-1
   Chen HB, 2019, LECT NOTES COMPUT SC, V11764, P351, DOI 10.1007/978-3-030-32239-7_39
   Chikontwe Philip, 2020, Medical Image Computing and Computer Assisted Intervention - MICCAI 2020. 23rd International Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12265), P519, DOI 10.1007/978-3-030-59722-1_50
   Cornish TC, 2012, ADV ANAT PATHOL, V19, P152, DOI 10.1097/PAP.0b013e318253459e
   COVER TM, 1968, IEEE T INFORM THEORY, V14, P50, DOI 10.1109/TIT.1968.1054098
   Cruz-Roa A, 2014, PROC SPIE, V9041, DOI 10.1117/12.2043872
   Deng JK, 2019, PROC CVPR IEEE, P4685, DOI 10.1109/CVPR.2019.00482
   Duran-Lopez L, 2020, IEEE ACCESS, V8, P128613, DOI 10.1109/ACCESS.2020.3008868
   Farahmand S, 2022, MODERN PATHOL, V35, P44, DOI 10.1038/s41379-021-00911-w
   Gao Y, 2016, PROC SPIE, V9791, DOI 10.1117/12.2216790
   Guan Y., 2022, P IEEECVF C COMPUTER, P18813
   Hartigan J. A., 1979, Applied Statistics, V28, P100, DOI 10.2307/2346830
   He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38
   Hoffer E, 2015, LECT NOTES COMPUT SC, V9370, P84, DOI 10.1007/978-3-319-24261-3_7
   Hsu WW, 2022, SCI REP-UK, V12, DOI 10.1038/s41598-022-09985-1
   Ilse M, 2018, PR MACH LEARN RES, V80
   Kanavati F, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-020-66333-x
   Khan SS, 2024, VISUAL COMPUT, V40, P899, DOI 10.1007/s00371-023-02822-0
   Lerousseau Marvin, 2020, Medical Image Computing and Computer Assisted Intervention - MICCAI 2020. 23rd International Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12265), P470, DOI 10.1007/978-3-030-59722-1_45
   Li B, 2021, PROC CVPR IEEE, P14313, DOI 10.1109/CVPR46437.2021.01409
   Li XT, 2022, ARTIF INTELL REV, V55, P4809, DOI 10.1007/s10462-021-10121-0
   Liu W, 2017, ADV SOC SCI EDUC HUM, V99, P212
   Lu MY, 2021, NATURE, V594, P106, DOI 10.1038/s41586-021-03512-4
   Lu MY, 2021, NAT BIOMED ENG, V5, P555, DOI 10.1038/s41551-020-00682-w
   Meng YD, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P15529, DOI 10.1109/ICCV48922.2021.01526
   OTSU N, 1979, IEEE T SYST MAN CYB, V9, P62, DOI 10.1109/TSMC.1979.4310076
   Pinckaers H, 2022, IEEE T PATTERN ANAL, V44, P1581, DOI 10.1109/TPAMI.2020.3019563
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Shao ZC, 2021, ADV NEUR IN
   Sharma Y, 2021, PR MACH LEARN RES, V143, P682
   Srinidhi CL, 2021, MED IMAGE ANAL, V67, DOI 10.1016/j.media.2020.101813
   Sun YF, 2020, PROC CVPR IEEE, P6397, DOI 10.1109/CVPR42600.2020.00643
   Tian SJ, 2020, VISUAL COMPUT, V36, P1219, DOI 10.1007/s00371-019-01730-6
   Tokunaga H, 2019, PROC CVPR IEEE, P12589, DOI 10.1109/CVPR.2019.01288
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang H, 2018, PROC CVPR IEEE, P5265, DOI 10.1109/CVPR.2018.00552
   Wang XL, 2019, INT CONF MEASURE, P1, DOI [10.1109/TCYB.2019.2935141, 10.1109/ICMIC48233.2019.9068567]
   Wang XG, 2018, PATTERN RECOGN, V74, P15, DOI 10.1016/j.patcog.2017.08.026
   Wang XY, 2022, MED IMAGE ANAL, V81, DOI 10.1016/j.media.2022.102559
   Xie XX, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P3500, DOI 10.1109/ICCV48922.2021.00350
   Lu MY, 2019, Arxiv, DOI arXiv:1910.10825
   Yao JW, 2020, MED IMAGE ANAL, V65, DOI 10.1016/j.media.2020.101789
   Zhang HR, 2022, PROC CVPR IEEE, P18780, DOI 10.1109/CVPR52688.2022.01824
   Zhou CJ, 2021, COMPUT MED IMAG GRAP, V88, DOI 10.1016/j.compmedimag.2021.101861
NR 49
TC 1
Z9 1
U1 3
U2 9
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2023 NOV 1
PY 2023
DI 10.1007/s00371-023-03131-2
EA NOV 2023
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA W5QT6
UT WOS:001092178200002
DA 2024-07-18
ER

PT J
AU Liu, YX
   Fu, XY
   Sun, Z
AF Liu, Yongxu
   Fu, Xiaoyan
   Sun, Zhong
TI Texture-aware re-parameterization to mitigate accuracy drop after
   quantization for 4K/8K image super-resolution
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Super-resolution; Re-parameterization; Model quantization
AB In this paper, we aim to improve super-resolution (SR) imaging quality on 4K/8K images with a negligible increase in computational cost and alleviate the accuracy drop after quantization. Experiments have discovered two phenomena: (1) the re-parameterization (Rep) technique has no apparent advantages in regions with smooth textures, and (2) the accuracy drop after quantization compared with SR methods based on No-Rep because the structure information of the image will be weakened caused by the multi-branch fusion in Rep technique. Inspired by the above phenomenon, we innovatively combine texture classification and Rep techniques to propose a generic TARepSR framework (consisting of TA-Module and VarRepSR-Module) to adjust the branching of Rep blocks texture-awarely. Specifically, the TA-Module is a lightweight classification network to classify textures in different regions. An existing SR network using texture-aware Rep techniques can be used as the VarRepSR-Module to super-resolute images with higher imaging quality without additional computational costs. Moreover, we propose a TC loss to avoid over-fitting caused by an unbalanced degree of the tendency of classification results to classify different textures better. Experiments show that our TARepSR can not only improve the imaging quality of most existing methods (e.g., FSRCNN, CARN, EDSR, XLSR) on 4K/8K images with negligible increase in computational cost but also improve the accuracy after quantization compared with the state-of-the-art Rep methods.
C1 [Liu, Yongxu; Fu, Xiaoyan; Sun, Zhong] Capital Normal Univ, Coll Informat Engn, Beijing 100089, Peoples R China.
C3 Capital Normal University
RP Fu, XY (corresponding author), Capital Normal Univ, Coll Informat Engn, Beijing 100089, Peoples R China.
EM 2211002057@cnu.edu.cn; fuxiaotriumph@163.com; 5689@cnu.edu.cn
RI Sun, Zhong/AHE-9747-2022; Fu, Xiaoyan/C-2573-2012
OI Sun, Zhong/0000-0003-1856-0279; 
FU National Natural Science Foundation of China "Research on Key
   Technologies of Interactive Analysis in Classroom Teaching Based on
   Artificial Intelligence" [61977048]; Research Project of the Digital
   Education Center of Beijing Municipal Education Commission
   [BDEC2023619060]
FX This paper is a stage research result of the National Natural Science
   Foundation of China "Research on Key Technologies of Interactive
   Analysis in Classroom Teaching Based on Artificial Intelligence" (No.
   61977048) and Research Project of the Digital Education Center of
   Beijing Municipal Education Commission "Research on ClassroomTeaching
   inmiddle and primary school Based on Immediately Visualization and
   Intelligent Analysis" (No. BDEC2023619060).
CR Agustsson E, 2017, IEEE COMPUT SOC CONF, P1122, DOI 10.1109/CVPRW.2017.150
   Ahn N, 2018, LECT NOTES COMPUT SC, V11214, P256, DOI 10.1007/978-3-030-01249-6_16
   Ayazoglu M, 2021, IEEE COMPUT SOC CONF, P2472, DOI 10.1109/CVPRW53098.2021.00280
   Bevilacqua M, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.135
   Boski M, 2017, 2017 10TH INTERNATIONAL WORKSHOP ON MULTIDIMENSIONAL (ND) SYSTEMS (NDS)
   Cao YP, 2021, IEEE INT CONF ASAP, P69, DOI 10.1109/ASAP52443.2021.00019
   Chan KCK, 2022, PROC CVPR IEEE, P5962, DOI 10.1109/CVPR52688.2022.00588
   Chen B., 2022, arXiv
   Chen Y., 2023, Vis Comput, P1
   Ding XH, 2022, Arxiv, DOI arXiv:2205.15242
   Ding XH, 2021, PROC CVPR IEEE, P10881, DOI 10.1109/CVPR46437.2021.01074
   Ding XH, 2021, PROC CVPR IEEE, P13728, DOI 10.1109/CVPR46437.2021.01352
   Ding XH, 2019, IEEE I CONF COMP VIS, P1911, DOI 10.1109/ICCV.2019.00200
   Dong C, 2016, LECT NOTES COMPUT SC, V9906, P391, DOI 10.1007/978-3-319-46475-6_25
   Gu SH, 2019, IEEE INT CONF COMP V, P3512, DOI 10.1109/ICCVW.2019.00435
   Hu M, 2022, PROC CVPR IEEE, P558, DOI 10.1109/CVPR52688.2022.00065
   Huang JB, 2015, PROC CVPR IEEE, P5197, DOI 10.1109/CVPR.2015.7299156
   Huang T, 2022, PROC CVPR IEEE, P578, DOI 10.1109/CVPR52688.2022.00067
   Hui Z, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2024, DOI 10.1145/3343031.3351084
   Ignatov A, 2021, IEEE COMPUT SOC CONF, P2525, DOI 10.1109/CVPRW53098.2021.00286
   Ioffe S, 2015, P INT C MACH LEARN, V2015, P1
   Kingma D. P., 2014, arXiv
   Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151
   Liu AQ, 2023, VISUAL COMPUT, V39, P3837, DOI 10.1007/s00371-022-02519-w
   Timofte R, 2017, IEEE COMPUT SOC CONF, P1110, DOI 10.1109/CVPRW.2017.149
   Wang LG, 2021, PROC CVPR IEEE, P4915, DOI 10.1109/CVPR46437.2021.00488
   Wang XT, 2022, Arxiv, DOI arXiv:2205.05671
   Wang Y., 2022, P IEEECVF C COMP VIS, P777
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Yang X, 2022, VISUAL COMPUT, V38, P4307, DOI 10.1007/s00371-021-02297-x
   Zeyde R., 2012, INT C CURV SURF, P711, DOI DOI 10.1007/978-3-642-27413-8_47
   Zhan Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4801, DOI 10.1109/ICCV48922.2021.00478
   Zhang XD, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P4034, DOI 10.1145/3474085.3475291
   Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI 10.1007/978-3-030-01234-2_18
NR 34
TC 1
Z9 1
U1 3
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2023 OCT 17
PY 2023
DI 10.1007/s00371-023-03120-5
EA OCT 2023
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA U9MD4
UT WOS:001087965600001
DA 2024-07-18
ER

PT J
AU Grimmeisen, B
   Chegini, M
   Theissler, A
AF Grimmeisen, Benedikt
   Chegini, Mohammad
   Theissler, Andreas
TI VisGIL: machine learning-based visual guidance for interactive labeling
SO VISUAL COMPUTER
LA English
DT Article
DE Visual-interactive labeling; User guidance; Active learning; Visual
   analytics; Machine learning; Classification
ID HEALTH-CARE; REDUCTION; CLASSIFICATION
AB Labeling of datasets is an essential task for supervised and semi-supervised machine learning. Model-based active learning and user-based interactive labeling are two complementary strategies for this task. We propose VisGIL which, using visual cues, guides the user in the selection of instances to label based on utility measures deduced from an active learning model. We have implemented the approach and conducted a qualitative and quantitative user study and a think-aloud test. The studies reveal that guidance by visual cues improves the trained model's accuracy, reduces the time needed to label the dataset, and increases users' confidence while selecting instances. Furthermore, we gained insights regarding how guidance impacts user behavior and how the individual visual cues contribute to user guidance. A video of the approach is available: https://ml-and-vis.org/visgil/.
C1 [Grimmeisen, Benedikt; Theissler, Andreas] Aalen Univ Appl Sci, D-73430 Aalen, Germany.
   [Grimmeisen, Benedikt] Capgemini, D-70771 Leinfelden Echterdingen, Germany.
   [Chegini, Mohammad] Adidas Runtast, A-1150 Vienna, Austria.
C3 Hochschule Aalen; Capgemini
RP Grimmeisen, B (corresponding author), Aalen Univ Appl Sci, D-73430 Aalen, Germany.; Grimmeisen, B (corresponding author), Capgemini, D-70771 Leinfelden Echterdingen, Germany.
EM benedikt.grimmeisen@hs-aalen.de
RI Theissler, Andreas/AAS-2182-2021
OI Theissler, Andreas/0000-0003-0746-0424
FU Projekt DEAL
FX Open Access funding enabled and organized by Projekt DEAL.
CR Abadi, 2015, TENSORFLOW LARGE SCA
   Ali M, 2019, VISUAL COMPUT, V35, P1013, DOI 10.1007/s00371-019-01673-y
   Amershi S., 2014, Al Mag, V5, P15, DOI 10.160/img.v3514.2513
   [Anonymous], 2009, Technical report
   Baumgart H., 2019, ACEX 2019 P
   Beil D., 2020, Proceedings of the 13th International Symposium on Visual Information Communication and Interaction, P1, DOI DOI 10.1145/3430036.3430060
   Benedikt Grimmeisen, 2020, VINCI 20 P 13 INT S, DOI DOI 10.1145/3430036.3430058
   Bernard J., 2015, Proceedings of the 2015 Workshop on Visual Analytics in Healthcare - VAHC'15, P1, DOI [10.1145/2836034.2836035, DOI 10.1145/2836034.2836035]
   Bernard J, 2018, VISUAL COMPUT, V34, P1189, DOI 10.1007/s00371-018-1500-3
   Bernard J, 2018, COMPUT GRAPH FORUM, V37, P121, DOI 10.1111/cgf.13406
   Bernard J, 2018, IEEE T VIS COMPUT GR, V24, P298, DOI 10.1109/TVCG.2017.2744818
   Blondel M., J. Mach
   Boy J, 2016, IEEE T VIS COMPUT GR, V22, P639, DOI 10.1109/TVCG.2015.2467201
   Breiman L, 2001, MACH LEARN, V45, P5, DOI 10.1023/A:1010933404324
   Cakmak M, 2010, IEEE T AUTON MENT DE, V2, P108, DOI 10.1109/TAMD.2010.2051030
   Ceneda D, 2017, IEEE T VIS COMPUT GR, V23, P111, DOI 10.1109/TVCG.2016.2598468
   Chegini M, 2019, VIS INFORM, V3, P9, DOI 10.1016/j.visinf.2019.03.002
   Chegini M, 2020, FRONT INFORM TECH EL, V21, P524, DOI 10.1631/FITEE.1900549
   Cohen J., 1988, STAT POWER ANAL BEHA
   Collins C, 2018, VIS INFORM, V2, P166, DOI 10.1016/j.visinf.2018.09.003
   Danka T, 2018, Arxiv, DOI arXiv:1805.00979
   Dhillon I. S., 2004, PROC ACM SIGKDD INT, P551, DOI DOI 10.1145/1014052.1014118
   Dy J. G., 2000, Proceedings. KDD-2000. Sixth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, P360, DOI 10.1145/347090.347168
   Fan X, 2019, J VISUAL-JAPAN, V22, P955, DOI 10.1007/s12650-019-00580-7
   Farooq MU, 2022, VISUAL COMPUT, V38, P1553, DOI 10.1007/s00371-021-02088-4
   Fezer E., 2020, P 13 INT S VIS INF C, DOI [10.1145/3430036,3430066, DOI 10.1145/3430036,3430066]
   Fisher B, 2005, Illuminating the Path: An R&D) Agenda for Visual Analytics
   Fu YF, 2013, KNOWL INF SYST, V35, P249, DOI 10.1007/s10115-012-0507-8
   Gillick D., 2006, Berkley, V8
   Han Q., 2018, LEIPZ S VIS APPL 201, P1
   He TX, 2014, SCI WORLD J, DOI 10.1155/2014/827586
   Heimerl F, 2012, IEEE T VIS COMPUT GR, V18, P2839, DOI 10.1109/TVCG.2012.277
   Höferlin B, 2012, IEEE CONF VIS ANAL, P23, DOI 10.1109/VAST.2012.6400492
   Holzinger A, 2019, APPL INTELL, V49, P2401, DOI 10.1007/s10489-018-1361-5
   Hu QW, 2019, BIOCOMPUT-PAC SYM, P362
   INSELBERG A, 1990, PROCEEDINGS OF THE FIRST IEEE CONFERENCE ON VISUALIZATION - VISUALIZATION 90, P361, DOI 10.1109/VISUAL.1990.146402
   Jiang BY, 2017, IUI'17: PROCEEDINGS OF THE 22ND INTERNATIONAL CONFERENCE ON INTELLIGENT USER INTERFACES, P535, DOI 10.1145/3025171.3025172
   Jiang L, 2019, J VISUAL-JAPAN, V22, P401, DOI 10.1007/s12650-018-0531-1
   Jonsson B., 2019, Exquisitor. Interactive learning at large
   Kucher K, 2017, ACM T INTERACT INTEL, V7, DOI 10.1145/3132169
   LeCun Y., 2010, MNIST HANDWRITTEN DI
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Luo LK, 2019, VISUAL COMPUT, V35, P1869, DOI 10.1007/s00371-018-1580-0
   Nadj M, 2020, KUNSTL INTELL, V34, P131, DOI 10.1007/s13218-020-00634-1
   Norman Don, 2013, The design of everyday things
   Plotly Technologies Inc, 2015, Collaborative Data Science
   Prisacaru A, 2018, IEEE T COMP PACK MAN, V8, P750, DOI 10.1109/TCPMT.2018.2816259
   Raschka S, 2020, Arxiv, DOI arXiv:2002.04803
   Rawat W, 2017, NEURAL COMPUT, V29, P2352, DOI [10.1162/NECO_a_00990, 10.1162/neco_a_00990]
   Ritter C., 2018, EUROVIS WORKSH VIS A, DOI [10.2312/eurova20181109, DOI 10.2312/EUROVA20181109]
   Saary MJ, 2008, J CLIN EPIDEMIOL, V61, P311, DOI 10.1016/j.jclinepi.2007.04.021
   Sacha D, 2017, NEUROCOMPUTING, V268, P164, DOI 10.1016/j.neucom.2017.01.105
   Sarikaya A, 2018, IEEE T VIS COMPUT GR, V24, P402, DOI 10.1109/TVCG.2017.2744184
   Sedlmair M, 2013, IEEE T VIS COMPUT GR, V19, P2634, DOI 10.1109/TVCG.2013.153
   Seifert C., 2010, Proceedings 2010 10th IEEE International Conference on Data Mining Workshops (ICDMW 2010), P418, DOI 10.1109/ICDMW.2010.181
   Settles B, 2008, ADV NEURAL INFORM PR, V21, P1289
   Settles B, 2008, P C EMP METH NAT LAN, P1070, DOI DOI 10.3115/1613715.1613855
   Settles Burr, 2011, JMLR WORKSHOP C P, P1
   Seung H. S., 1992, Proceedings of the Fifth Annual ACM Workshop on Computational Learning Theory, P287, DOI 10.1145/130385.130417
   Sharma PK, 2021, VISUAL COMPUT, V37, P2083, DOI 10.1007/s00371-020-01971-w
   Shneiderman B, 1996, IEEE SYMPOSIUM ON VISUAL LANGUAGES, PROCEEDINGS, P336, DOI 10.1109/VL.1996.545307
   Smilkov Daniel, 2016, Embedding Projector: Interactive Visualization and Interpretation of Embeddings
   Stoiber C., 2019, Visualization onboarding: learning how to read and use visualizations, DOI [10.31219/os.io/c38ab, DOI 10.31219/OS.IO/C38AB]
   Ash JT, 2020, Arxiv, DOI [arXiv:1910.08475, 10.48550/arXiv.1910.08475]
   Team R.D, 2018, RAPIDS: Collection of Libraries for End to End GPU Data Science
   Theissler Andreas, 2020, Machine Learning and Knowledge Extraction. 4th IFIP TC 5, TC 12, WG 8.4, WG 8.9, WG 12.9. International Cross-Domain Conference, CD-MAKE 2020. Proceedings. Lecture Notes in Computer Science (LNCS 12279), P281, DOI 10.1007/978-3-030-57321-8_16
   Theissler A., 2020, 4 INT WORKSH INT AD
   Theissler A, 2022, KNOWL-BASED SYST, V247, DOI 10.1016/j.knosys.2022.108651
   Theissler A, 2021, RELIAB ENG SYST SAFE, V215, DOI 10.1016/j.ress.2021.107864
   Tuia D, 2011, IEEE J-STSP, V5, P606, DOI 10.1109/JSTSP.2011.2139193
   Van Someren MW, 1994, AcademicPress
   Vendrig J., 2002, P 11 TEXT RETR C TRE
   Verbraeken J, 2020, ACM COMPUT SURV, V53, DOI 10.1145/3377454
   Vollert S., 2021, IEEE INT C EM TECHN
   Walker JS, 2015, VISUAL COMPUT, V31, P1067, DOI 10.1007/s00371-015-1112-0
   Wallenberg M., 2016, Distill
   Wen ZY, 2020, J MACH LEARN RES, V21
   Wen ZY, 2019, IEEE T PARALL DISTR, V30, P2706, DOI 10.1109/TPDS.2019.2920131
   Wiens J, 2018, CLIN INFECT DIS, V66, P149, DOI 10.1093/cid/cix731
   Wu Y., 2006, 2006 IEEE INT C MULT, DOI [10.1109/ICME2006.262442, DOI 10.1109/ICME2006.262442]
   Xiang SX, 2019, IEEE CONF VIS ANAL, P57, DOI [10.1109/vast47406.2019.8986943, 10.1109/VAST47406.2019.8986943]
   Xiao H., 2017, ARXIV170807747
   Yang LP, 2018, ISPRS INT J GEO-INF, V7, DOI 10.3390/ijgi7020065
   Zahálka J, 2021, IEEE T VIS COMPUT GR, V27, P422, DOI 10.1109/TVCG.2020.3030383
   Zahálka J, 2018, IEEE T MULTIMEDIA, V20, P687, DOI 10.1109/TMM.2017.2755986
   Zhang CL, 2018, INT CON DISTR COMP S, P99, DOI 10.1109/ICDCS.2018.00020
   Zhu Y, 2019, INT J PROD ECON, V211, P22, DOI 10.1016/j.ijpe.2019.01.032
NR 87
TC 1
Z9 1
U1 3
U2 10
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2023
VL 39
IS 10
BP 5097
EP 5119
DI 10.1007/s00371-022-02648-2
PG 23
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA T1GK8
UT WOS:001075538600045
OA hybrid
DA 2024-07-18
ER

PT J
AU Yuan, J
   Bhattacharjee, K
   Islam, AZ
   Dasgupta, A
AF Yuan, Jun
   Bhattacharjee, Kaustav
   Islam, Akm Zahirul
   Dasgupta, Aritra
TI TRIVEA: Transparent Ranking Interpretation using Visual Explanation of
   black-box Algorithmic rankers
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Visual analytics; Learning-to-rank; Explainable ML; ranking
ID VISUALIZATION
AB Ranking schemes drive many real-world decisions, like, where to study, whom to hire, what to buy, etc. Many of these decisions often come with high consequences. For example, a university can be deemed less prestigious if not featured in a top-k list, and consumers might not even explore products that do not get recommended to buyers. At the heart of most of these decisions are opaque ranking schemes, which dictate the ordering of data entities, but their internal logic is inaccessible or proprietary. Drawing inferences about the ranking differences is like a guessing game to the stakeholders, like, the rankees (i.e., the entities who are ranked, like product companies) and the decision-makers (i.e., who use the rankings, like buyers). In this paper, we aim to enable transparency in ranking interpretation by using algorithmic rankers that learn from available data and by enabling human reasoning about the learned ranking differences using explainable AI (XAI) methods. To realize this aim, we leverage the exploration-explanation paradigm of human-data interaction to let human stakeholders explore subsets and groupings of complex multi-attribute ranking data using visual explanations of model fit and attribute influence on rankings. We realize this explanation paradigm for transparent ranking interpretation in TRIVEA, a visual analytic system that is fueled by: (i) visualizations of model fit derived from algorithmic rankers that learn the associations between attributes and rankings from available data and (ii) visual explanations derived from XAI methods that help abstract important patterns, like, the relative influence of attributes in different ranking ranges. Using TRIVEA, end users not trained in data science have the agency to transparently reason about the global and local behavior of the rankings without the need to open black-box ranking models and develop confidence in the resulting attribute-based inferences. We demonstrate the efficacy of TRIVEA using multiple usage scenarios and subjective feedback from researchers with diverse domain expertise.
C1 [Yuan, Jun; Bhattacharjee, Kaustav; Islam, Akm Zahirul; Dasgupta, Aritra] New Jersey Inst Technol, Dept Data Sci, Newark, NJ 07102 USA.
C3 New Jersey Institute of Technology
RP Dasgupta, A (corresponding author), New Jersey Inst Technol, Dept Data Sci, Newark, NJ 07102 USA.
EM jy448@njit.edu; kb526@njit.edu; akm.islam@njit.edu;
   aritra.dasgupta@njit.edu
RI Bhattacharjee, Kaustav/AAF-1907-2021
OI Bhattacharjee, Kaustav/0000-0001-7530-7865; Dasgupta,
   Aritra/0000-0002-5551-5103
FU National Science Foundation (NSF) [2312932]
FX This work was funded in part by the National Science Foundation (NSF)
   grant 2312932. Authors would like to thank Prof. Julia Stoyanovich from
   New York University for her valuable comments on intermediate versions
   of the paper.
CR [Anonymous], State Fiscal Rankings
   Arendt D, 2020, Arxiv, DOI arXiv:2004.07993
   Bauer JM, 2009, HBK PHILOS SCI, V9, P601, DOI 10.1016/B978-0-444-51667-1.50026-4
   Dasgupta A, 2020, IEEE T VIS COMPUT GR, V26, P1043, DOI 10.1109/TVCG.2019.2934540
   github, ICE Feature Impact Python Package
   github, LIME Python Package
   Gleicher M, 2020, COMPUT GRAPH FORUM, V39, P181, DOI 10.1111/cgf.13972
   Gleicher M, 2018, IEEE T VIS COMPUT GR, V24, P413, DOI 10.1109/TVCG.2017.2744199
   Gomez O, 2021, 2021 IEEE VISUALIZATION CONFERENCE - SHORT PAPERS (VIS 2021), P31, DOI 10.1109/VIS49827.2021.9623271
   Gratzl S, 2013, IEEE T VIS COMPUT GR, V19, P2277, DOI 10.1109/TVCG.2013.173
   Havre S, 2000, IEEE SYMPOSIUM ON INFORMATION VISUALIZATION 2000, P115, DOI 10.1109/INFVIS.2000.885098
   Heer J, 2019, P NATL ACAD SCI USA, V116, P1844, DOI 10.1073/pnas.1807184115
   Hong Sungsoo Ray, 2020, Proceedings of the ACM on Human-Computer Interaction, V4, DOI 10.1145/3392878
   Joachims T, 2002, P 8 ACM SIGKDD INT C, P133, DOI [DOI 10.1145/775047.775067, 10.1145/775047.775067]
   Krause J, 2017, IEEE CONF VIS ANAL, P162, DOI 10.1109/VAST.2017.8585720
   Lam H, 2012, IEEE T VIS COMPUT GR, V18, P1520, DOI 10.1109/TVCG.2011.279
   Lipton ZC., 2018, QUEUE, V16, P31, DOI 10.1145/3236386.3241340
   Liu B, 2015, BIOINFORMATICS, V31, P3492, DOI 10.1093/bioinformatics/btv413
   Lundberg SM, 2017, ADV NEUR IN, V30
   Ma YX, 2020, IEEE T VIS COMPUT GR, V26, P1075, DOI 10.1109/TVCG.2019.2934631
   Maack RGC, 2023, VISUAL COMPUT, V39, P6345, DOI 10.1007/s00371-022-02733-6
   Miranda F, 2018, IEEE T VIS COMPUT GR, V24, P1394, DOI 10.1109/TVCG.2017.2671341
   Mohler G, 2020, CRIME SCI, V9, DOI 10.1186/s40163-020-00112-x
   Page L., 1999, PAGERANK CITATION RA
   Pang AT, 1997, VISUAL COMPUT, V13, P370, DOI 10.1007/s003710050111
   Perin C, 2014, 32ND ANNUAL ACM CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2014), P887, DOI 10.1145/2556288.2557379
   Ribeiro MT, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1135, DOI 10.1145/2939672.2939778
   Ribeiro S., 2018, AAAI Conf. Artif. Intell., V32, P1, DOI 10.1609/aaai.v32i1.11491
   Robertson S, 2008, P 31 ANN INT ACM SIG, P689, DOI DOI 10.1145/1390334.1390453
   Seo J., 2005, Information Visualization, V4, P96, DOI 10.1057/palgrave.ivs.9500091
   Seo J, 2006, IEEE T VIS COMPUT GR, V12, P311, DOI 10.1109/TVCG.2006.50
   Shi CL, 2012, IEEE T VIS COMPUT GR, V18, P2669, DOI 10.1109/TVCG.2012.253
   Shneiderman B, 2021, ISSUES SCI TECHNOL, V37, P56
   sourceforge, RankLib Toolkit
   Times World University, Rankings
   Ribeiro MT, 2016, Arxiv, DOI [arXiv:1606.05386, 10.48550/arXiv.1606.05386]
   Valizadegan H., 2009, Advances in Neural Information Processing Systems 22, V22, P1883
   Wall E, 2018, IEEE T VIS COMPUT GR, V24, P288, DOI 10.1109/TVCG.2017.2745078
   Xu XS, 2023, VISUAL COMPUT, V39, P6097, DOI 10.1007/s00371-022-02715-8
   Yeh A, 2021, COMM COM INF SC, V1524, P34, DOI 10.1007/978-3-030-93736-2_4
   Yuan J., 2022, CHI WORKSH HUM CTR D, P2211
   Yuan J, 2022, IEEE COMPUT GRAPH, V42, P24, DOI 10.1109/MCG.2022.3199727
   Zhang JW, 2019, IEEE T VIS COMPUT GR, V25, P364, DOI 10.1109/TVCG.2018.2864499
NR 43
TC 0
Z9 0
U1 2
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2023 SEP 30
PY 2023
DI 10.1007/s00371-023-03055-x
EA SEP 2023
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA MU6X1
UT WOS:001196199100001
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Gao, LQ
   Hu, LY
   Lyu, F
   Zhu, L
   Wan, L
   Pun, CM
   Feng, W
AF Gao, Liqing
   Hu, Lianyu
   Lyu, Fan
   Zhu, Lei
   Wan, Liang
   Pun, Chi-Man
   Feng, Wei
TI Difference-guided multi-scale spatial-temporal representation for sign
   language recognition
SO VISUAL COMPUTER
LA English
DT Article
DE Sign language recognition (SLR); Key spatial-temporal representation;
   Multi-scale sequence alignment
AB Sign language recognition (SLR) is a challenging task, which requires a thorough understanding of spatial-temporal visual features for translating it into comprehensible written or spoken language. However, existing SLR methods ignore the importance of key spatial-temporal representation due to its sparsity and inconsistency in space and time. To solve this problem, we present a difference-guided multi-scale spatial-temporal representation (DMST) learning model for SLR. In DMST, we devise two modules: (1) key spatial-temporal representation, to extract and enhance key spatial-temporal information by a spatial-temporal difference strategy and (2) multi-scale sequence alignment, to perceive and fuse multi-scale spatial-temporal features and achieve sequence mapping. The DMST model outperforms state-of-the-art performance on four public sign language datasets, which demonstrates the superiority of DMST model and the significance of key spatial-temporal representation for SLR.
C1 [Gao, Liqing; Hu, Lianyu; Lyu, Fan; Wan, Liang; Feng, Wei] Tianjin Univ, Tianjin, Peoples R China.
   [Zhu, Lei] Hong Kong Univ Sci & Technol Guangzhou, Guangzhou, Peoples R China.
   [Pun, Chi-Man] Univ Macau, Macau, Peoples R China.
C3 Tianjin University; Hong Kong University of Science & Technology
   (Guangzhou); University of Macau
RP Feng, W (corresponding author), Tianjin Univ, Tianjin, Peoples R China.
EM lqgao@tju.edu.cn; hly2021@tju.edu.cn; fanlyu@tju.edu.cn;
   lwan@tju.edu.cn; leizhu@ust.hk; cmpun@umac.mo; wfeng@ieee.org
RI Zhang, Wenbin/JXX-8070-2024; liu, xingyu/JXW-9444-2024; Wang,
   Guanhua/JXM-6373-2024; Pun, Chi Man/GRJ-3703-2022; zhang,
   ling/JXW-6931-2024; Li, Lei/JPE-6543-2023; Zhu, Lei/GQQ-1130-2022
OI Zhu, Lei/0000-0002-5348-7532; Gao, liqing/0000-0003-4518-2154
FU National Natural Science Foundation of China [62072334]
FX AcknowledgementsThe work is supported by the National Natural Science
   Foundation of China (Grant No. 62072334).
CR Bahdanau D, 2016, Arxiv, DOI [arXiv:1409.0473, 10.48550/arXiv.1409.0473]
   Camgoz Necati Cihan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10020, DOI 10.1109/CVPR42600.2020.01004
   Camgoz NC, 2018, PROC CVPR IEEE, P7784, DOI 10.1109/CVPR.2018.00812
   Camgoz NecatiCihan., 2017, P IEEE INT C COMP VI, P3056
   Chung Junyoung, 2014, ARXIV14123555
   Cui RP, 2019, IEEE T MULTIMEDIA, V21, P1880, DOI 10.1109/TMM.2018.2889563
   Cui RP, 2017, PROC CVPR IEEE, P1610, DOI 10.1109/CVPR.2017.175
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Evangelidis GD, 2015, LECT NOTES COMPUT SC, V8925, P595, DOI 10.1007/978-3-319-16178-5_42
   Gharbi H, 2017, INT CONF ACOUST SPEE, P1502, DOI 10.1109/ICASSP.2017.7952407
   Graves A., 2006, P 23 INT C MACHINE L, P369
   Guo D, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P744
   Guo D, 2018, AAAI CONF ARTIF INTE, P6845
   Hao AM, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11283, DOI 10.1109/ICCV48922.2021.01111
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Huang J, 2018, AAAI CONF ARTIF INTE, P2257
   Iandola F. N., 2016, ARXIV160207360, DOI 10.1007/978-3-319-24553-9
   Ka Leong Cheng, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12369), P697, DOI 10.1007/978-3-030-58586-0_41
   Kar A, 2017, PROC CVPR IEEE, P5699, DOI 10.1109/CVPR.2017.604
   Koller O., 2016, P BRIT MACHINE VISIO
   Koller O, 2020, IEEE T PATTERN ANAL, V42, P2306, DOI 10.1109/TPAMI.2019.2911077
   Koller O, 2017, PROC CVPR IEEE, P3416, DOI 10.1109/CVPR.2017.364
   Koller O, 2015, COMPUT VIS IMAGE UND, V141, P108, DOI 10.1016/j.cviu.2015.09.013
   Koller Oscar., 2016, P BMVC, P1, DOI 10.5244/C.30.136
   Kuncheva LI, 2018, J VIS COMMUN IMAGE R, V52, P118, DOI 10.1016/j.jvcir.2018.02.010
   Li HB, 2020, INT CONF ACOUST SPEE, P2348, DOI [10.1109/icassp40776.2020.9054316, 10.1109/ICASSP40776.2020.9054316]
   Lin J, 2019, IEEE I CONF COMP VIS, P7082, DOI 10.1109/ICCV.2019.00718
   Ma NN, 2018, LECT NOTES COMPUT SC, V11218, P122, DOI 10.1007/978-3-030-01264-9_8
   Min YC, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11522, DOI 10.1109/ICCV48922.2021.01134
   Pan YW, 2016, PROC CVPR IEEE, P4594, DOI 10.1109/CVPR.2016.497
   Pfister T, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.20
   Pu JF, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1497, DOI 10.1145/3394171.3413931
   Pu JF, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P885
   Pu JF, 2019, PROC CVPR IEEE, P4160, DOI 10.1109/CVPR.2019.00429
   Qiu ZF, 2017, IEEE I CONF COMP VIS, P5534, DOI 10.1109/ICCV.2017.590
   Radosavovic Ilija, 2020, P IEEE CVF C COMP VI, P10428
   Simonyan K, 2014, ADV NEUR IN, V27
   Vaswani A, 2017, ADV NEUR IN, V30
   Vazquez-Enriquez M., 2021, P IEEECVF C COMPUTER, P3462
   Venugopalan S, 2015, IEEE I CONF COMP VIS, P4534, DOI 10.1109/ICCV.2015.515
   Wang S, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1483, DOI 10.1145/3240508.3240671
   Wei CC, 2021, IEEE T CIRC SYST VID, V31, P1138, DOI 10.1109/TCSVT.2020.2999384
   Xie P., 2021, IEEE TMM, V24, P3908
   Xie P, 2023, PATTERN RECOGN, V136, DOI 10.1016/j.patcog.2022.109233
   Yang WW, 2016, PATTERN RECOGN LETT, V78, P28, DOI 10.1016/j.patrec.2016.03.030
   Yang Z., 2019, arXiv
   Yao L, 2015, IEEE I CONF COMP VIS, P4507, DOI 10.1109/ICCV.2015.512
   Zhe Niu, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12361), P172, DOI 10.1007/978-3-030-58517-4_11
   Zhou H, 2021, PROC CVPR IEEE, P1316, DOI 10.1109/CVPR46437.2021.00137
   Zhou H, 2022, IEEE T MULTIMEDIA, V24, P768, DOI 10.1109/TMM.2021.3059098
   Zhou H, 2020, AAAI CONF ARTIF INTE, V34, P13009
   Zhou H, 2019, IEEE INT CON MULTI, P1282, DOI 10.1109/ICME.2019.00223
   Zhu QD, 2022, Arxiv, DOI arXiv:2204.03864
   Zhu WJ, 2016, PROC CVPR IEEE, P1991, DOI 10.1109/CVPR.2016.219
NR 55
TC 2
Z9 2
U1 10
U2 16
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2023
VL 39
IS 8
SI SI
BP 3417
EP 3428
DI 10.1007/s00371-023-02979-8
EA JUL 2023
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA P2DS6
UT WOS:001040330500003
DA 2024-07-18
ER

PT J
AU Jiang, ZT
   Shi, DQ
   Zhang, SQ
AF Jiang, Zetao
   Shi, Daoquan
   Zhang, Shaoqin
TI FRSE-Net: low-illumination object detection network based on feature
   representation refinement and semantic-aware enhancement
SO VISUAL COMPUTER
LA English
DT Article
DE Object detection; Low-illumination; Feature representation;
   Semantic-aware; Deep learning
AB Deep learning-based object detection methods have achieved great performance improvement. However, the current mainstream object detectors focus on normal illumination images, while low-illumination object detection is often ignored. It is still a challenging task to detect objects in low-illumination scenes due to insufficient illumination and low visibility. To address this issue, we propose a low-illumination object detection network based on feature representation refinement and semantic-aware enhancement, called FRSE-Net. There are two key components in the proposed network, including a feature capture module (FCM) and a semantic aggregation module (SAM). First, the FCM is designed to enhance the feature representation of the feature map, thus making the object features more discriminative. This is beneficial to capture more effective feature information for subsequent prediction tasks. Furthermore, the SAM is introduced to enhance the semantic-aware ability of the model in low-light images, which makes the detection network focus on the objects of interest to learn rich semantic information. Finally, the experimental results on two low-light image datasets demonstrate the effectiveness and superiority of the proposed network when compared with other advanced low-illumination detection methods.
C1 [Jiang, Zetao; Shi, Daoquan] Guilin Univ Elect Technol, Guangxi Key Lab Image & Graph Intelligent Proc, Guilin 541004, Peoples R China.
   [Zhang, Shaoqin] Nanchang Hangkong Univ, Nanchang 330063, Peoples R China.
C3 Guilin University of Electronic Technology; Nanchang Hangkong University
RP Shi, DQ (corresponding author), Guilin Univ Elect Technol, Guangxi Key Lab Image & Graph Intelligent Proc, Guilin 541004, Peoples R China.
EM zetaojiang@guet.edu.cn; sdaoquan@qq.com; shaoqinzhang@chu.edu.cn
OI Shi, Daoquan/0000-0003-2491-0829
FU Nature Science Foundation of China [62172118]; Nature Science Key
   Foundation of Guangxi [2021GXNSFDA196002]; Guangxi Key Laboratory of
   Image and Graphic Intelligent Processing [GIIP2203, GIIP2204];
   Innovation Project of Guangxi Graduate Education [YCB2021070,
   YCBZ2018052, YCSW2022269]; Innovation Project of GUET Graduate Education
   [2021YCXS071]
FX This work is supported by Nature Science Foundation of China (62172118)
   and Nature Science Key Foundation of Guangxi (2021GXNSFDA196002); in
   part by the Guangxi Key Laboratory of Image and Graphic Intelligent
   Processing under Grants (GIIP2203, GIIP2204); in part by the Innovation
   Project of Guangxi Graduate Education under Grants (YCB2021070,
   YCBZ2018052, YCSW2022269); and in part by the Innovation Project of GUET
   Graduate Education under Grants (2021YCXS071).
CR Arici T, 2009, IEEE T IMAGE PROCESS, V18, P1921, DOI 10.1109/TIP.2009.2021548
   Bochkovskiy A, 2020, Arxiv, DOI arXiv:2004.10934
   Cui Z., 2021, P IEEECVF INT C COMP, P2553
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Guo MH, 2022, COMPUT VIS MEDIA, V8, P331, DOI 10.1007/s41095-022-0271-y
   Guo XJ, 2017, IEEE T IMAGE PROCESS, V26, P982, DOI 10.1109/TIP.2016.2639450
   Guo ZB, 2023, VISUAL COMPUT, V39, P4267, DOI 10.1007/s00371-022-02589-w
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang Y., 2021, PACIFIC RIM INT C AR, P461
   Huang Y, 2022, LECT NOTES COMPUT SC, V13583, P43, DOI 10.1007/978-3-031-21014-3_5
   Huang YS, 2023, Arxiv, DOI [arXiv:2212.12322, DOI 10.48550/ARXIV.2212.12322]
   Huang YS, 2021, IEEE SIGNAL PROC LET, V28, P982, DOI 10.1109/LSP.2021.3077801
   Jiang YF, 2021, IEEE T IMAGE PROCESS, V30, P2340, DOI 10.1109/TIP.2021.3051462
   Kera SB, 2023, VISUAL COMPUT, V39, P2347, DOI 10.1007/s00371-022-02445-x
   Kim S, 2020, VISUAL COMPUT, V36, P1663, DOI 10.1007/s00371-019-01755-x
   Kokufuta K, 2009, I C FIELD PROG LOGIC, P288, DOI 10.1109/FPL.2009.5272284
   Li CY, 2021, Arxiv, DOI arXiv:2104.10729
   Li CY, 2022, IEEE T PATTERN ANAL, V44, P9396, DOI 10.1109/TPAMI.2021.3126387
   Li CY, 2022, IEEE T PATTERN ANAL, V44, P4225, DOI 10.1109/TPAMI.2021.3063604
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu S, 2018, PROC CVPR IEEE, P8759, DOI 10.1109/CVPR.2018.00913
   Liu WY, 2022, AAAI CONF ARTIF INTE, P1792
   Loh YP, 2019, COMPUT VIS IMAGE UND, V178, P30, DOI 10.1016/j.cviu.2018.10.010
   Loshchilov Ilya, 2016, arXiv
   Luo YH, 2022, MULTIMED TOOLS APPL, V81, P30685, DOI 10.1007/s11042-022-11940-1
   Ma L, 2022, PROC CVPR IEEE, P5627, DOI 10.1109/CVPR52688.2022.00555
   Selvaraju RR, 2017, IEEE I CONF COMP VIS, P618, DOI 10.1109/ICCV.2017.74
   Shao MW, 2023, VISUAL COMPUT, V39, P5719, DOI 10.1007/s00371-022-02691-z
   Ulyanov D, 2017, Arxiv, DOI arXiv:1607.08022
   Wang WJ, 2021, PROC CVPR IEEE, P16190, DOI 10.1109/CVPR46437.2021.01593
   Wei LX, 2021, VISUAL COMPUT, V37, P133, DOI 10.1007/s00371-019-01787-3
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Wu WH, 2022, PROC CVPR IEEE, P5891, DOI 10.1109/CVPR52688.2022.00581
   Wu XY, 2021, PROC CVPR IEEE, P15764, DOI 10.1109/CVPR46437.2021.01551
   Zaidi SSA, 2022, DIGIT SIGNAL PROCESS, V126, DOI 10.1016/j.dsp.2022.103514
   Zhang Q, 2019, COMPUT GRAPH FORUM, V38, P243, DOI 10.1111/cgf.13833
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zibo Meng, 2020, Proceedings of the 16th European Conference on Computer Vision - ECCV 2020 Workshops. Lecture Notes in Computer Science (LNCS 12537), P327, DOI 10.1007/978-3-030-67070-2_20
NR 40
TC 0
Z9 0
U1 8
U2 23
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2024
VL 40
IS 5
BP 3233
EP 3247
DI 10.1007/s00371-023-03024-4
EA JUL 2023
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OC3D9
UT WOS:001034268300003
DA 2024-07-18
ER

PT J
AU Zhang, ZQ
   Liu, Y
   Zhu, XC
   Li, FC
   Song, B
AF Zhang, Zhongqi
   Liu, Yong
   Zhu, Xiaochong
   Li, Fuchen
   Song, Bo
TI DSE-FCOS: dilated and SE block-reinforced FCOS for detection of marine
   benthos
SO VISUAL COMPUTER
LA English
DT Article
DE Object detection; Label assignment; Small objects; Marine benthos;
   Feature map
AB The current object detection algorithm suffers from slow detection speed and low object recognition rate when detecting marine benthos due to small objects, severe densities and occlusions. To solve these problems, an improved object detection algorithm based on FCOS, DSE-FCOS (Dilated & SE block reinforced FCOS), is proposed for marine benthos in this paper. In our method, to meet the needs of real-time detection, we only use a single-level feature map for detection, we propose DSE module to solve the problem of insufficient single-scale detection accuracy. The DW (Dual Weighting) label assignment method is introduced and optimized to make it more suitable for the detection of marine benthos, increasing the speed of detection while ensuring accuracy. We performed experiments on the DUO dataset. The results show that our method can improve mAP (mean average precision) by 5.9% on the basis of 13.8% speed improvement compared with FCOS. This improvement reflects that DSE-FCOS is more suitable for the detection of marine benthos.
C1 [Zhang, Zhongqi; Liu, Yong; Zhu, Xiaochong; Li, Fuchen; Song, Bo] Qingdao Univ Sci & Technol, Qingdao 266061, Peoples R China.
C3 Qingdao University of Science & Technology
RP Liu, Y (corresponding author), Qingdao Univ Sci & Technol, Qingdao 266061, Peoples R China.
EM liuyong@qust.edu.cn
RI liu, yong/HSE-8321-2023; song, bo/JQJ-0093-2023; Song, Bo/HJP-1326-2023
OI Liu, Yong/0000-0002-5282-7283
FU Center for Ocean Mega-Science, Chinese Academy of Sciences
   [KEXUE2019GZ04]; GHfund [GHFUNd202107021586]
FX The research was supported by the Center for Ocean Mega-Science, Chinese
   Academy of Sciences (KEXUE2019GZ04) and the GHfund B
   (GHFUNd202107021586).
CR Bochkovskiy A, 2020, ARXIV PREPRINT ARXIV, DOI DOI 10.48550/ARXIV.2004.10934
   Chen K, 2019, PROC CVPR IEEE, P4969, DOI 10.1109/CVPR.2019.00511
   Chen Q, 2021, PROC CVPR IEEE, P13034, DOI 10.1109/CVPR46437.2021.01284
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Ge Z., 2021, ARXIV, DOI DOI 10.48550/ARXIV.2107.08430
   Han FL, 2020, MATH PROBL ENG, V2020, DOI 10.1155/2020/3937580
   Hao YL, 2023, APPL MATH MODEL, V119, P68, DOI 10.1016/j.apm.2023.02.004
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hou GJ, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3578584
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Joseph RK, 2016, CRIT POL ECON S ASIA, P1
   Kong T, 2016, PROC CVPR IEEE, P845, DOI 10.1109/CVPR.2016.98
   Law H, 2018, LECT NOTES COMPUT SC, V11218, P765, DOI 10.1007/978-3-030-01264-9_45
   Li CX, 2021, IEEE T CIRCUITS-II, V68, P1987, DOI 10.1109/TCSII.2020.3034771
   Li MD, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21010313
   Li S, 2022, PROC CVPR IEEE, P9377, DOI 10.1109/CVPR52688.2022.00917
   Li X., 2022, ARXIV, DOI DOI 10.48550/ARXIV.2203.04822
   Li X., 2020, ADV NEURAL INF PROCE, V33, P21002, DOI DOI 10.48550/ARXIV.2006.04388
   Li XB, 2024, VISUAL COMPUT, V40, P1299, DOI 10.1007/s00371-023-02849-3
   Li XJ, 2022, ENG APPL ARTIF INTEL, V111, DOI 10.1016/j.engappai.2022.104759
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Lin WH, 2020, INT CONF ACOUST SPEE, P2588, DOI [10.1109/icassp40776.2020.9053829, 10.1109/ICASSP40776.2020.9053829]
   Liu CW, 2022, IEEE T CIRC SYST VID, V32, P2831, DOI 10.1109/TCSVT.2021.3100059
   Liu C, 2021, 2021 7TH INTERNATIONAL CONFERENCE ON ENGINEERING AND EMERGING TECHNOLOGIES (ICEET 2021), P797, DOI [10.1109/ICEET53442.2021.9659771, 10.1109/ICME51207.2021.9428266]
   Pan X., 2020, P IEEE CVF C COMP VI, P11207
   Pang Y, 2023, VISUAL COMPUT, V39, P1959, DOI 10.1007/s00371-022-02458-6
   Redmon J, 2018, Arxiv, DOI [arXiv:1804.02767, DOI 10.1109/CVPR.2017.690, DOI 10.48550/ARXIV.1804.02767]
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rezatofighi H, 2019, PROC CVPR IEEE, P658, DOI 10.1109/CVPR.2019.00075
   Song PH, 2023, NEUROCOMPUTING, V530, P150, DOI 10.1016/j.neucom.2023.01.088
   Tian Z, 2019, IEEE I CONF COMP VIS, P9626, DOI 10.1109/ICCV.2019.00972
   Tsung-Yi Lin, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P2999, DOI 10.1109/ICCV.2017.324
   Xie J, 2022, IEEE T CIRC SYST VID, V32, P3514, DOI 10.1109/TCSVT.2021.3115791
   Yang X, 2019, IEEE I CONF COMP VIS, P8231, DOI 10.1109/ICCV.2019.00832
   Yu F., 2015, ARXIV
   Yu HF, 2023, APPL INTELL, V53, P2434, DOI 10.1007/s10489-022-03622-0
   Yuan WQ, 2023, VISUAL COMPUT, V39, P5199, DOI 10.1007/s00371-022-02654-4
   Zhang JS, 2020, CHIN AUTOM CONGR, P5928, DOI 10.1109/CAC51589.2020.9326936
   Zhang S., 2020, P IEEECVF C COMPUTER, P9759
   Zhang S, 2023, VISUAL COMPUT, V39, P5375, DOI 10.1007/s00371-022-02665-1
NR 40
TC 0
Z9 0
U1 12
U2 26
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2024
VL 40
IS 4
BP 2679
EP 2693
DI 10.1007/s00371-023-02971-2
EA JUL 2023
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA MZ2U3
UT WOS:001025599200002
DA 2024-07-18
ER

PT J
AU Yang, DZ
   Chen, N
   Tang, QQ
   Zhang, H
   Liu, J
AF Yang, Dezhi
   Chen, Ning
   Tang, Qiqi
   Zhang, Hang
   Liu, Jian
TI Research on defect detection of toy sets based on an improved U-Net
SO VISUAL COMPUTER
LA English
DT Article
DE Toy set; Defect detection; U-Net; Transfer learning
ID HSV
AB In order to address the problem of low efficiency and accuracy of artificial visual inspection for the quality of toy sets, this paper investigates the defect detection of toy sets based on machine vision. Firstly, an improved U-Net (IU-Net) is developed by introducing the Squeeze-and-Excitation (SE) block into the U-Net and modifying its objective function. Then, a convolutional neural network (CNN) is constructed for feature extraction and defect matching. Finally, a defect detection model for toy sets is created by integrating these two models. In addition, when it comes to a new-variety toy set, the IU-Net model is trained through transfer learning with a small number of new-variety data samples. This can reduce the dependence on new-variety data samples and improve the efficiency of model training process. The experimental results show that the defect detection accuracy of the proposed method is 100% and 99.43% for the whole toy set and single component, respectively, which meets the requirements of industrial automation quality inspection of toy sets.
C1 [Yang, Dezhi; Chen, Ning; Tang, Qiqi; Zhang, Hang; Liu, Jian] Hunan Univ, Coll Mech & Vehicle Engn, Changsha 410082, Peoples R China.
C3 Hunan University
RP Chen, N (corresponding author), Hunan Univ, Coll Mech & Vehicle Engn, Changsha 410082, Peoples R China.
EM yangdezhi@hnu.edu.cn; chenning@hnu.edu.cn; 308321878@qq.com;
   zhanghang7@hnu.edu.cn; liujian@hnu.edu.cn
RI Huang, Liping/KIB-4430-2024; Zhao, YuHan/KIE-0813-2024
FU China-Japan Science and Technology Joint Committee of the Ministry of
   Science and Technology of the People's Republic of China
   [2017YFE0128400]; National Natural Science Foundation of China
   [51905162]; Foundation for Innovative Research Groups of the National
   Natural Science Foundation of China [51621004]; Project of Science and
   Technology of Changsha [kq2001015]; Key Research and Development program
   of Hunan Province [2021GK2007]; program of Leading Scientific and
   Technological Innovation in High-tech Industries [2021GK4028]
FX Thiswork is supported by the China-Japan Science and Technology Joint
   Committee of the Ministry of Science and Technology of the People's
   Republic of China (Grant No. 2017YFE0128400), the National Natural
   Science Foundation of China (Grant No. 51905162), the Foundation for
   Innovative Research Groups of the National Natural Science Foundation of
   China (Grant No. 51621004), the Project of Science and Technology of
   Changsha (Grant No. kq2001015), the Key Research and Development program
   of Hunan Province (2021GK2007), and the program of Leading Scientific
   and Technological Innovation in High-tech Industries (2021GK4028).
CR Baktashmotlagh M, 2013, IEEE I CONF COMP VIS, P769, DOI 10.1109/ICCV.2013.100
   Chen WT, 2022, IEEE J-STARS, V15, P1591, DOI 10.1109/JSTARS.2022.3144339
   Chen XY, 2020, IEEE ACCESS, V8, P24006, DOI 10.1109/ACCESS.2020.2970461
   Chen ZY, 2022, VISUAL COMPUT, DOI 10.1007/s00371-077-07554-7
   Cheng ZM, 2022, VISUAL COMPUT, V38, P749, DOI 10.1007/s00371-021-02075-9
   Dahl GE, 2013, INT CONF ACOUST SPEE, P8609, DOI 10.1109/ICASSP.2013.6639346
   Dai WT, 2021, VISUAL COMPUT, V37, P3093, DOI 10.1007/s00371-021-02257-5
   Dai WT, 2022, VISUAL COMPUT, V38, P1181, DOI 10.1007/s00371-021-02137-y
   Heo YS, 2011, IEEE T PATTERN ANAL, V33, P807, DOI 10.1109/TPAMI.2010.136
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Hu WH, 2022, VISUAL COMPUT, V38, P3731, DOI 10.1007/s00371-021-02210-6
   Huang HM, 2020, INT CONF ACOUST SPEE, P1055, DOI [10.1109/ICASSP40776.2020.9053405, 10.1109/icassp40776.2020.9053405]
   Huang YB, 2020, VISUAL COMPUT, V36, P85, DOI 10.1007/s00371-018-1588-5
   Kamal U, 2020, IEEE T INTELL TRANSP, V21, P1467, DOI 10.1109/TITS.2019.2911727
   Kamoona AM, 2021, IEEE ACCESS, V9, P158672, DOI 10.1109/ACCESS.2021.3130261
   Kiran I, 2022, COMPUT BIOL MED, V143, DOI 10.1016/j.compbiomed.2022.105267
   Li WS, 2021, MED PHYS, V48, P329, DOI 10.1002/mp.14617
   Li XM, 2018, IEEE T MED IMAGING, V37, P2663, DOI 10.1109/TMI.2018.2845918
   Lin DY, 2021, KNOWL-BASED SYST, V228, DOI 10.1016/j.knosys.2021.107272
   Liu GH, 2022, VISUAL COMPUT, V38, P639, DOI 10.1007/s00371-020-02040-y
   Liu GH, 2021, VISUAL COMPUT, V37, P515, DOI 10.1007/s00371-020-01820-w
   Liu RQ., 1948, J PHYS C SER, V1
   Ma AL, 2021, ISPRS J PHOTOGRAMM, V172, P171, DOI 10.1016/j.isprsjprs.2020.11.025
   Milletari F, 2016, INT CONF 3D VISION, P565, DOI 10.1109/3DV.2016.79
   Nigam A, 2016, INT J COMPUT APPL T, V54, P61, DOI 10.1504/IJCAT.2016.077797
   Oquab M, 2014, PROC CVPR IEEE, P1717, DOI 10.1109/CVPR.2014.222
   OTSU N, 1979, IEEE T SYST MAN CYB, V9, P62, DOI 10.1109/TSMC.1979.4310076
   Qin XB, 2020, PATTERN RECOGN, V106, DOI 10.1016/j.patcog.2020.107404
   Ren ZH, 2022, INT J PR ENG MAN-GT, V9, P661
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Shaik KB, 2015, PROCEDIA COMPUT SCI, V57, P41, DOI 10.1016/j.procs.2015.07.362
   Shin HC, 2016, IEEE T MED IMAGING, V35, P1285, DOI 10.1109/TMI.2016.2528162
   Smith ML, 2021, COMPUT IND, V130, DOI 10.1016/j.compind.2021.103472
   Suvdaa B., 2012, Int. J. Softw. Eng. Its Appl, V6, P161
   TANNER MA, 1987, J AM STAT ASSOC, V82, P528, DOI 10.2307/2289457
   Tsai DM, 2003, PATTERN RECOGN LETT, V24, P2625, DOI 10.1016/S0167-8655(03)00106-5
   Wang D, 2021, VISUAL COMPUT, V37, P1101, DOI 10.1007/s00371-020-01855-z
   Wang Q, 2022, VISUAL COMPUT, V38, P2591, DOI 10.1007/s00371-021-02134-1
   Wang XD, 2023, VISUAL COMPUT, V39, P5451, DOI 10.1007/s00371-022-02671-3
   Wang XJ, 2023, VISUAL COMPUT, V39, P4801, DOI 10.1007/s00371-022-02628-6
   Wang ZK, 2021, COMPUT BIOL MED, V134, DOI 10.1016/j.compbiomed.2021.104449
   Yang LJ, 2020, IEEE T IND INFORM, V16, P501, DOI 10.1109/TII.2019.2926283
   Yosinski J, 2014, ARXIV
   Zhang Z, 2020, COMPUT METH PROG BIO, V192, DOI 10.1016/j.cmpb.2020.105395
   Zheng XQ, 2021, INT J ADV MANUF TECH, V113, P35, DOI 10.1007/s00170-021-06592-8
   Zhou GDA, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2021.3128033
NR 46
TC 2
Z9 2
U1 5
U2 12
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2024
VL 40
IS 2
BP 1095
EP 1109
DI 10.1007/s00371-023-02834-w
EA MAR 2023
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GE6E8
UT WOS:000958560100001
DA 2024-07-18
ER

PT J
AU Weng, HL
   Zhang, GD
   Sheng, X
   Liu, RY
   Chen, PK
   Wang, LP
AF Weng, Hangli
   Zhang, Guodao
   Sheng, Xin
   Liu, Ruyu
   Chen, Ping-Kuo
   Wang, Liping
TI FeatureB2SENet: point cloud classification of large scenes
SO VISUAL COMPUTER
LA English
DT Article
DE Feature map; Feature fusion; Blueprint separation convolution; Large
   scene point cloud classification
ID LIDAR DATA
AB With the continuous development of 3D data acquisition technology in recent years, it is more and more convenient to obtain the point cloud data of large scenes, which contains a variety of rich information. How to effectively and accurately classify and segment point cloud data of large scenes has become a research hot-spot in the field of computer vision. In this paper, we study the method based on clustering, make full use of the spatial location and context information, and propose a new network framework, FeatureB2SENet. In the 2D and 3D projection feature calculation, we generate a 32 x 32 x 1 feature image for each point and input it into the convolution neural network to process the feature image. Finally, a comprehensive verification analysis is carried out on GML_A, GML_B and Vaihingen data sets, which proves that the classification effect is better.
C1 [Weng, Hangli; Sheng, Xin; Wang, Liping] Zhejiang Univ Technol, Coll Comp Sci & Technol, Hangzhou 310023, Peoples R China.
   [Zhang, Guodao] Hangzhou Dianzi Univ, Dept Digital Media Technol, Hangzhou 310018, Peoples R China.
   [Liu, Ruyu] Hangzhou Normal Univ, Sch Informat Sci & Technol, Hangzhou 310023, Peoples R China.
   [Liu, Ruyu] Chinese Acad Sci, Haixi Inst, Quanzhou Inst Equipment Mfg, Hangzhou 362000, Peoples R China.
   [Chen, Ping-Kuo] Great Bay Univ, Inst Sci Res, Dongguan 523800, Peoples R China.
C3 Zhejiang University of Technology; Hangzhou Dianzi University; Hangzhou
   Normal University; Chinese Academy of Sciences
RP Zhang, GD (corresponding author), Hangzhou Dianzi Univ, Dept Digital Media Technol, Hangzhou 310018, Peoples R China.; Liu, RY (corresponding author), Hangzhou Normal Univ, Sch Informat Sci & Technol, Hangzhou 310023, Peoples R China.; Liu, RY (corresponding author), Chinese Acad Sci, Haixi Inst, Quanzhou Inst Equipment Mfg, Hangzhou 362000, Peoples R China.
EM hlweng@zjut.edu.cn; guodaozhang@hdu.edu.cn; 2111908095@zjut.edu.cn;
   lry@hznu.edu.cn; pgchen@stu.edu.cn; wllp@zjut.edu.cn
RI Yuan, Yu/KBQ-0606-2024; li, yansong/JXL-5023-2024; ZHOU,
   YUE/KCJ-8790-2024; Zhu, Li/JTT-9093-2023; yan, yan/JVN-1800-2024; li,
   jiaxin/JNT-5073-2023; chen, xu/JNT-3068-2023; liu,
   jianyang/JXL-6273-2024; Zhang, Yusi/JNS-2335-2023
OI Liu, Ruyu/0000-0003-2130-9122
FU NationalNatural Science Foundation of China [62202137]; Zhejiang
   Provincial Natural Science Foundation of China [LQ22F030004]; National
   Natural Science Foundation of China (NSFC) [71872131, YJKY220020];
   Research Foundation of Hangzhou Dianzi University [KYS335622091,
   KYH333122029M]
FX This research is supported by the NationalNatural Science Foundation of
   China under Grant 62202137, Zhejiang Provincial Natural Science
   Foundation of China under Grant LQ22F030004, National Natural Science
   Foundation of China (NSFC) under Grant 71872131, Starting Research Fund
   of Great BayUniversity underGrant YJKY220020 and the Research Foundation
   of Hangzhou Dianzi University under Grant KYS335622091 and
   KYH333122029M.
CR Biasutti P., 2019, Riu-net: Embarrassingly simple semantic segmentation of 3d lidar point cloud
   Biasutti P, 2019, IEEE INT CONF COMP V, P942, DOI 10.1109/ICCVW.2019.00123
   Fauzi Fauzi, 2021, 2021 3rd International Conference on Electronics Representation and Algorithm (ICERA), P66, DOI 10.1109/ICERA53111.2021.9538686
   Haase Daniel, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P14588, DOI 10.1109/CVPR42600.2020.01461
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Li M., 2020, Research on point cloud classification based on deep learning
   Li WZ, 2020, ISPRS J PHOTOGRAMM, V164, P26, DOI 10.1016/j.isprsjprs.2020.03.016
   Li XH, 2022, IEEE GEOSCI REMOTE S, V19, DOI 10.1109/LGRS.2021.3098774
   Li Y., 2019, Remote Sens, V11
   Mallet C, 2011, ISPRS J PHOTOGRAMM, V66, pS71, DOI 10.1016/j.isprsjprs.2011.09.008
   Maturana D, 2015, IEEE INT C INT ROBOT, P922, DOI 10.1109/IROS.2015.7353481
   Nezhadarya Ehsan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12953, DOI 10.1109/CVPR42600.2020.01297
   Niemeyer J, 2014, ISPRS J PHOTOGRAMM, V87, P152, DOI 10.1016/j.isprsjprs.2013.11.001
   Qi C.R., 2017, IEEE C COMPUTER VISI
   Quan X., 2019, Remote Sens. Inf., V34, P4
   Rusu RB, 2009, IEEE INT CONF ROBOT, P1848
   Shapovalov R, 2010, INT ARCH PHOTOGRAMM, V38, P103
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sun Jie, 2014, Geomatics and Information Science of Wuhan University, V39, P1310, DOI 10.13203/j.whugis20130206
   Wang L, 2019, IEEE ACCESS, V7, P55649, DOI 10.1109/ACCESS.2019.2909742
   Wang Yuan., 2018, Pointseg: Real-time semantic segmentation based on 3d lidar point cloud
   Weinmann M, 2015, ISPRS J PHOTOGRAMM, V105, P286, DOI 10.1016/j.isprsjprs.2015.01.016
   Wen CC, 2021, ISPRS J PHOTOGRAMM, V173, P181, DOI 10.1016/j.isprsjprs.2021.01.007
   Wu BC, 2018, IEEE INT CONF ROBOT, P1887
   Yasir S., 2014, 3DTV C TRUE VIS CAPT, P1
   Zhang ZX, 2021, IEEE T GEOSCI REMOTE, V59, P1686, DOI 10.1109/TGRS.2020.2997960
   Zhang ZX, 2018, IEEE T GEOSCI REMOTE, V56, P524, DOI 10.1109/TGRS.2017.2751061
   Zhang ZX, 2016, IEEE T GEOSCI REMOTE, V54, P7309, DOI 10.1109/TGRS.2016.2599163
   Zhang ZX, 2016, IEEE T GEOSCI REMOTE, V54, P3309, DOI 10.1109/TGRS.2016.2514508
   Zhu Q, 2017, ISPRS J PHOTOGRAMM, V129, P86, DOI 10.1016/j.isprsjprs.2017.04.022
NR 30
TC 1
Z9 1
U1 10
U2 30
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2024
VL 40
IS 2
BP 1037
EP 1051
DI 10.1007/s00371-023-02830-0
EA MAR 2023
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GE6E8
UT WOS:000957659200001
DA 2024-07-18
ER

PT J
AU Zheng, H
   Guo, ZY
   Liu, C
   Li, X
   Wang, TY
   You, CH
AF Zheng, Hong
   Guo, Zhongyuan
   Liu, Chang
   Li, Xi
   Wang, Tianyu
   You, Changhui
TI Blind deblurring of QR code using intensity and gradient prior of
   positioning patterns
SO VISUAL COMPUTER
LA English
DT Article
DE QR code; Blind deblurring; Positioning patterns; Gradient and intensity
   prior
AB QR codes are widely used in the traceability of commodities, but the QR codes are easy to become blurred during the acquisition process of mobile phones, which affects their normal identification, so it is necessary to deblur them. This paper proposes a sub-regional deblurring method based on prior knowledge of the gradient and intensity of positioning patterns. Firstly, the QR code is divided into four regions according to the positions of three finder positioning patterns and one alignment positioning pattern. The content invariance of the four positioning patterns can avoid the interference of the content to the estimation of the blur kernel, and also take into account the non-uniformity of the QR code blur. Then the gradient and intensity priors are used to estimate the blur kernels of the position patterns in the four regions, and the calculated blur kernels are applied to the corresponding respective regions for deblurring. Finally, the four deblurred regions are stitched together to obtain the entire deblurring image. The experimental results show that the proposed method performs well in terms of deblurring effect and computational time, surpassing similar deblurring methods.
C1 [Zheng, Hong; Guo, Zhongyuan; Liu, Chang; Wang, Tianyu] Wuhan Univ, Sch Elect Informat, Wuhan 430072, Peoples R China.
   [Li, Xi] Nanchang Inst Sci & Technol, Coll Artificial Intelligence, Nanchang 330108, Peoples R China.
   [You, Changhui] Wuhan Univ, Sch Cyber Sci & Engn, Wuhan 430000, Peoples R China.
C3 Wuhan University; Wuhan University
RP Guo, ZY (corresponding author), Wuhan Univ, Sch Elect Informat, Wuhan 430072, Peoples R China.
EM guozhongyuan@whu.edu.cn
RI Guo, Zhongyuan/JLN-1557-2023
FU National Key Research and Development Program of China [2020YFF0304902];
   Science and Technology Research Project of Jiangxi Provincial Department
   of Education [GJJ202511]
FX This research is supported by the National Key Research and Development
   Program of China (Grant No.2020YFF0304902) and the Science and
   Technology Research Project of Jiangxi Provincial Department of
   Education (Grant No.GJJ202511).
CR Conte F, 2013, IEEE T IMAGE PROCESS, V22, P5306, DOI 10.1109/TIP.2013.2284873
   Dey S, 2021, SUSTAINABILITY-BASEL, V13, DOI 10.3390/su13063486
   Focardi R, 2019, J INF SECUR APPL, V48, DOI 10.1016/j.jisa.2019.102369
   Huang Y, 2019, IEEE T MULTIMEDIA, V21, P2447, DOI 10.1109/TMM.2019.2907475
   Kaur M., 2014, J ENG RES APPL   SEP, V4, P12
   Lee Chung-Hee, 2019, [journal of the society of cosmetic scientists of korea, 대한화장품학회지], V45, P277, DOI 10.15230/SCSK.2019.45.3.277
   Li JN, 2022, NEUROCOMPUTING, V493, P351, DOI 10.1016/j.neucom.2022.04.041
   Li JN, 2020, IEEE INT CON AUTO SC, P1269, DOI [10.1109/case48305.2020.9216945, 10.1109/CASE48305.2020.9216945]
   Liu NZ, 2018, PATTERN RECOGN LETT, V111, P117, DOI 10.1016/j.patrec.2018.04.036
   Mishra N., 2020, FOOD TRACEABILITY SY, P33
   Pan JS, 2017, IEEE T PATTERN ANAL, V39, P342, DOI 10.1109/TPAMI.2016.2551244
   Popovic T, 2021, INTERNET THINGS-NETH, V14, DOI 10.1016/j.iot.2021.100375
   Qian JP, 2021, FOOD PACKAGING SHELF, V28, DOI 10.1016/j.fpsl.2021.100638
   Rioux Gabriel., 2019, IEEE Transactions on Pattern Analysis and Machine Intelligence
   Sandhu S., 2018, The international encyclopedia of strategic communication S, P1
   Sara U., 2019, J COMPUT COMMUN, V7, P8, DOI [10.4236/jcc.2019.73002, DOI 10.4236/JCC.2019.73002]
   Setiadi DIM, 2021, MULTIMED TOOLS APPL, V80, P8423, DOI 10.1007/s11042-020-10035-z
   Shi YF, 2020, OPTIK, V219, DOI 10.1016/j.ijleo.2020.164902
   Sörös G, 2015, ISWC 2015: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL SYMPOSIUM ON WEARABLE COMPUTERS, P117, DOI 10.1145/2802083.2808390
   Tiwari S., 2016, 2016 INT C INF TECHN, P39, DOI DOI 10.1109/ICIT.2016.021
   Tu D., 2007, COMPUT ENG SCI, V3
   van Gennip Yves, 2015, IEEE Trans Image Process, V24, P2864, DOI 10.1109/TIP.2015.2432675
   Vijayalakshmi S., 2020, DISS
   Wang M., 2022, INT C ELECT INFORM T, V2254, P589
   Wen F., 2020, IEEE T CIRC SYST VID
   Wu Y., 2018, DEBLURRING BARCODES
   Yan YL, 2021, IEEE INTERNET THINGS, V8, P6789, DOI 10.1109/JIOT.2020.3035697
   YAN YY, 2017, PROC CVPR IEEE, P6978, DOI DOI 10.1109/CVPR.2017.738
   Yang XC, 2020, IET IMAGE PROCESS, V14, P384, DOI 10.1049/iet-ipr.2019.0750
   Yu XY, 2019, IET IMAGE PROCESS, V13, P923, DOI 10.1049/iet-ipr.2018.5792
NR 30
TC 2
Z9 2
U1 1
U2 12
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2024
VL 40
IS 1
BP 441
EP 455
DI 10.1007/s00371-023-02792-3
EA MAR 2023
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IX5B8
UT WOS:000943629800001
DA 2024-07-18
ER

PT J
AU Zhao, CC
   Hu, M
   Ju, F
   Chen, Z
   Li, YQ
   Feng, YJ
AF Zhao, Changchen
   Hu, Meng
   Ju, Feng
   Chen, Zan
   Li, Yongqiang
   Feng, Yuanjing
TI Convolutional neural network with spatio-temporal-channel attention for
   remote heart rate estimation
SO VISUAL COMPUTER
LA English
DT Article
DE Remote photoplethysmography; Spatio-temporal network; Attention; Heart
   rate measurement; Compressed video
ID NONCONTACT
AB Remote photoplethysmography (rPPG), which measures human heart rate without physical contact with the skin, has become active research in recent years. Neural networks have been introduced into rPPG for accurate pulse measurement and have achieved overwhelming results. However, there is a lack of in-depth analysis of key components of neural networks exhibiting a crucial impact on pulse extraction from video. In this paper, we present a network with attention and spatio-temporal convolutional block (ASTNet), exploiting the impact of key factors including different spatio-temporal convolutions, attention mechanism, the number of convolutional layers, and receptive field sizes. The novel attention module named spatio-temporal-channel (STC) attention is designed to jointly learn weights in spatial, temporal, and channel dimensions in a more efficient way. Extensive experiments have been conducted on two uncompressed datasets and one compressed dataset. Results show that ASTNet outperforms state-of-the-art methods in accuracy and computational time. Specifically, networks with larger receptive field sizes and more spatio-temporal blocks generally achieve better performance. Networks with pseudo 3D convolution outperform those with convolutional 3D in static videos, and the opposite is true in motion videos. The results exhibit a similar tendency both on uncompressed and compressed datasets. The proposed method improves the performance of pulse signal compared to PhysNet (the second-best approach in the compared methods), with the signal-to-noise ratio increased by 7.03%, 10.19%, 4.79%, the mean absolute error decreased by 17.95%, 14.17%, 22.76%, and the root-mean-square error decreased by 21.43%, 2.73%, 25.43%, on the PURE, Self-rPPG, and COHFACE datasets, respectively.
C1 [Zhao, Changchen; Hu, Meng; Ju, Feng; Chen, Zan; Li, Yongqiang; Feng, Yuanjing] Zhejiang Univ Technol, Coll Informat Engn, Hangzhou 310023, Peoples R China.
C3 Zhejiang University of Technology
RP Feng, YJ (corresponding author), Zhejiang Univ Technol, Coll Informat Engn, Hangzhou 310023, Peoples R China.
EM fyjing@zjut.edu.cn
RI Zhao, Changchen/AEV-3957-2022; Li, Yongqiang/AAD-1078-2021
OI Zhao, Changchen/0000-0002-0546-6016; 
FU National Natural Science Foundation of China [61903336, 61976190,
   62073294, 62002327]; Key Research and Development Program of Zhejiang
   Province [2020C03070]; Natural Science Foundation of Zhejiang Province
   [LY21F030015, LZ21F030003]
FX This research was supported by National Natural Science Foundation of
   China underGrant No.s 61903336, 61976190, 62073294, 62002327; Key
   Research and Development Program of Zhejiang Province under Grant No.
   2020C03070. Natural Science Foundation of Zhejiang Province under Grant
   No.s LY21F030015, LZ21F030003.
CR Bobbia S, 2019, PATTERN RECOGN LETT, V124, P82, DOI 10.1016/j.patrec.2017.10.017
   Cai JH, 2020, VISUAL COMPUT, V36, P1261, DOI 10.1007/s00371-019-01733-3
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chen WX, 2018, LECT NOTES COMPUT SC, V11206, P356, DOI 10.1007/978-3-030-01216-8_22
   Dawson JA, 2010, ARCH DIS CHILD-FETAL, V95, pF177, DOI 10.1136/adc.2009.169102
   de Haan G, 2014, PHYSIOL MEAS, V35, P1913, DOI 10.1088/0967-3334/35/9/1913
   de Haan G, 2013, IEEE T BIO-MED ENG, V60, P2878, DOI 10.1109/TBME.2013.2266196
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Hu J, 2018, ADV NEUR IN, V31
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Hu M, 2022, VISUAL COMPUT, V38, P2617, DOI 10.1007/s00371-021-02136-z
   Hu M, 2021, IEEE T INSTRUM MEAS, V70, DOI 10.1109/TIM.2021.3058983
   Hu MH, 2019, PROG MOL BIOL TRANSL, V162, P1, DOI 10.1016/bs.pmbts.2018.12.002
   Huang B, 2021, ENG APPL ARTIF INTEL, V106, DOI 10.1016/j.engappai.2021.104447
   Huang B, 2021, BIOMED SIGNAL PROCES, V66, DOI 10.1016/j.bspc.2020.102387
   Klaessens JHGM, 2014, PROC SPIE, V8935, DOI 10.1117/12.2038353
   Lewandowska M., 2011, 2011 Federated Conference on Computer Science and Information Systems (FedCSIS), P405
   Li X, 2019, PROC CVPR IEEE, P510, DOI 10.1109/CVPR.2019.00060
   Liu SQ, 2020, IEEE INT CONF AUTOMA, P481, DOI 10.1109/FG47880.2020.00109
   Liu YJ, 2018, PROC CVPR IEEE, P389, DOI 10.1109/CVPR.2018.00048
   McDuff D, 2018, IEEE COMPUT SOC CONF, P1448, DOI 10.1109/CVPRW.2018.00185
   Niu XS, 2020, IEEE T IMAGE PROCESS, V29, P2409, DOI 10.1109/TIP.2019.2947204
   Niu XS, 2019, IEEE INT CONF AUTOMA, P582, DOI 10.1109/fg.2019.8756554
   Poh MZ, 2010, OPT EXPRESS, V18, P10762, DOI 10.1364/OE.18.010762
   Qiu Y, 2019, IEEE T MULTIMEDIA, V21, P1778, DOI 10.1109/TMM.2018.2883866
   Qiu ZF, 2017, IEEE I CONF COMP VIS, P5534, DOI 10.1109/ICCV.2017.590
   Sharma PK, 2021, VISUAL COMPUT, V37, P2083, DOI 10.1007/s00371-020-01971-w
   Spetlik R., 2018, P BRIT MACH VIS C NE, P3
   Stricker R, 2014, IEEE ROMAN, P1056, DOI 10.1109/ROMAN.2014.6926392
   Tran D, 2018, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2018.00675
   Tsouri GR, 2012, J BIOMED OPT, V17, DOI 10.1117/1.JBO.17.7.077011
   Wang WJ, 2019, IEEE T BIO-MED ENG, V66, P2032, DOI 10.1109/TBME.2018.2882396
   Wang WJ, 2017, IEEE T BIO-MED ENG, V64, P1479, DOI 10.1109/TBME.2016.2609282
   Wang WJ, 2017, PHYSIOL MEAS, V38, P1023, DOI 10.1088/1361-6579/aa6d02
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Wu HY, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185561
   Xi L, 2022, MED ENG PHYS, V105, DOI 10.1016/j.medengphy.2022.103822
   Yu Z., 2019, ARXIV PREPRINT ARXIV, P1
   Yu ZT, 2020, IEEE SIGNAL PROC LET, V27, P1245, DOI 10.1109/LSP.2020.3007086
   Yue ZJ, 2021, IEEE T INSTRUM MEAS, V70, DOI 10.1109/TIM.2021.3109398
   Zhang Q, 2017, MEASUREMENT, V100, P311, DOI 10.1016/j.measurement.2017.01.007
   Zhao CC, 2022, IEEE T INSTRUM MEAS, V71, DOI 10.1109/TIM.2022.3169567
   Zhao CC, 2020, IEEE COMPUT SOC CONF, P1120, DOI 10.1109/CVPRW50498.2020.00147
   Zhao CC, 2018, IEEE COMPUT SOC CONF, P1380, DOI 10.1109/CVPRW.2018.00177
   Zhao HS, 2022, VISUAL COMPUT, V38, P3765, DOI 10.1007/s00371-021-02217-z
   Zhu YS, 2020, VISUAL COMPUT, V36, P1771, DOI 10.1007/s00371-019-01770-y
NR 46
TC 5
Z9 5
U1 3
U2 33
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2023
VL 39
IS 10
BP 4767
EP 4785
DI 10.1007/s00371-022-02624-w
EA AUG 2022
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA T1GK8
UT WOS:000836989900001
DA 2024-07-18
ER

PT J
AU Sun, J
   Yan, SB
   Song, XW
AF Sun, Jie
   Yan, Senbo
   Song, Xiaowen
TI QCNet: query context network for salient object detection of automatic
   surface inspection
SO VISUAL COMPUTER
LA English
DT Article
DE Salient object detection; Fully convolutional network; Encoder-decoder;
   Query context network; Automatic surface inspection
ID DEFECT DETECTION
AB Building upon fully convolutional networks (FCNs), deep learning-based salient object detection (SOD) methods achieve gratifying performance in many vision tasks, including surface defect detection. However, most existing FCN-based methods still suffer from the coarse object edge predictions. The state-of-the-art methods employ intricate feature aggregation techniques to refine boundaries, but they are often too computational cost to deploy in the real application. This paper proposes a semantics guided detection paradigm for salient object detection. Guided atrous pyramid module is first applied on the top feature to segment complete salient semantics. Query context modules are further used to build relation maps between saliency and structural information from the top-down pathway. These two modules allow the semantic features to flow throughout the decoder phase, yielding detail enriched saliency predictions. Experimental results demonstrate that the proposed method performs favorably against the state-of-the-art methods on surface defect detection and SOD benchmarks. In addition, this method can detect at 27 FPS in a fully convolutional fashion without any post-processing, which has the potential for real-time detection.
C1 [Sun, Jie; Song, Xiaowen] Zhejiang Univ, Sch Mech Engn, State Key Lab Fluid Power & Mechatron Syst, Hangzhou 310027, Peoples R China.
   [Sun, Jie; Song, Xiaowen] Zhejiang Univ, Sch Mech Engn, Key Lab Adv Mfg Technol Zhejiang Prov, Hangzhou 310027, Peoples R China.
   [Yan, Senbo] Zhejiang Univ, Coll Comp Sci & Technol, State Key Lab CAD & CG, Hangzhou 310027, Peoples R China.
C3 Zhejiang University; Zhejiang University; Zhejiang University
RP Song, XW (corresponding author), Zhejiang Univ, Sch Mech Engn, State Key Lab Fluid Power & Mechatron Syst, Hangzhou 310027, Peoples R China.; Song, XW (corresponding author), Zhejiang Univ, Sch Mech Engn, Key Lab Adv Mfg Technol Zhejiang Prov, Hangzhou 310027, Peoples R China.
EM songxw@zju.edu.cn
RI song, xiaowen/D-2255-2013; Wang, Fei/KEH-6292-2024
OI song, xiaowen/0000-0001-6386-9836; 
FU Key Program of the National Natural Science Foundation of China
   [91948301]; Key Research and Development Plan of Zhejiang [2020C01039]
FX The work was supported by the Key Program of the National Natural
   Science Foundation of China (Grant No. 91948301) and the Key Research
   and Development Plan of Zhejiang (No. 2020C01039).
CR Bai XL, 2014, IEEE T IND INFORM, V10, P2135, DOI 10.1109/TII.2014.2359416
   Baker A, 1999, COMPOS STRUCT, V47, P431, DOI 10.1016/S0263-8223(00)00011-8
   Borji A., 2012, 2012 IEEE COMPUTER S, P23, DOI 10.1109/CVPRW.2012.6239191
   Borji A, 2012, LECT NOTES COMPUT SC, V7573, P414, DOI 10.1007/978-3-642-33709-3_30
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Cheng MM, 2021, INT J COMPUT VISION, V129, P2622, DOI [10.1007/s11263-021-01490-8, 10.1109/ICCV.2017.487]
   Cheng MM, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778820
   Cheng MM, 2011, PROC CVPR IEEE, P409, DOI 10.1109/CVPR.2011.5995344
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Deng ZJ, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P684
   Dong HW, 2020, IEEE T IND INFORM, V16, P7448, DOI 10.1109/TII.2019.2958826
   Donoser M, 2009, IEEE I CONF COMP VIS, P817, DOI 10.1109/ICCV.2009.5459296
   Fan D-P, 2018, ARXIV
   Feng MY, 2019, PROC CVPR IEEE, P1623, DOI 10.1109/CVPR.2019.00172
   He K., 2016, PROC CVPR IEEE, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]
   He Y, 2020, IEEE T INSTRUM MEAS, V69, P1493, DOI 10.1109/TIM.2019.2915404
   Hongyi Su, 2015, Intelligent Computing Theories and Methodologies. 11th International Conference, ICIC 2015. Proceedings: LNCS 9225, P1, DOI 10.1007/978-3-319-22180-9_1
   Hou Qibin., 2017, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, P3203
   Huang YB, 2020, VISUAL COMPUT, V36, P85, DOI 10.1007/s00371-018-1588-5
   Islam MA, 2017, PROC CVPR IEEE, P4877, DOI 10.1109/CVPR.2017.518
   Jumaat MZ, 2010, INT J PHYS SCI, V5, P619
   Klein DA, 2011, IEEE I CONF COMP VIS, P2214, DOI 10.1109/ICCV.2011.6126499
   Li DP, 2015, PROC CVPR IEEE, P213, DOI 10.1109/CVPR.2015.7298617
   Li GB, 2015, PROC CVPR IEEE, P5455, DOI 10.1109/CVPR.2015.7299184
   Li Y, 2014, PROC CVPR IEEE, P280, DOI 10.1109/CVPR.2014.43
   Liu JJ, 2019, PROC CVPR IEEE, P3912, DOI 10.1109/CVPR.2019.00404
   Liu NA, 2018, PROC CVPR IEEE, P3089, DOI 10.1109/CVPR.2018.00326
   Liu NA, 2016, PROC CVPR IEEE, P678, DOI 10.1109/CVPR.2016.80
   Liu Y, 2015, POLYMERS-BASEL, V7, P2078, DOI 10.3390/polym7101501
   Liu Y, 2021, IEEE T IMAGE PROCESS, V30, P3804, DOI 10.1109/TIP.2021.3065239
   Liu Y, 2021, IEEE T CYBERNETICS, V51, P4439, DOI 10.1109/TCYB.2020.3035613
   Liu ZY, 2020, VISUAL COMPUT, V36, P843, DOI 10.1007/s00371-019-01659-w
   Liu ZY, 2020, VISUAL COMPUT, V36, P1823, DOI 10.1007/s00371-019-01778-4
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Luo ZM, 2017, PROC CVPR IEEE, P6593, DOI 10.1109/CVPR.2017.698
   Mehta S, 2018, LECT NOTES COMPUT SC, V11214, P561, DOI 10.1007/978-3-030-01249-6_34
   Qin XB, 2020, PATTERN RECOGN, V106, DOI 10.1016/j.patcog.2020.107404
   Qin XB, 2019, PROC CVPR IEEE, P7471, DOI 10.1109/CVPR.2019.00766
   Rusk N, 2016, NAT METHODS, V13, P35, DOI 10.1038/nmeth.3707
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Soutis C, 2005, PROG AEROSP SCI, V41, P143, DOI 10.1016/j.paerosci.2005.02.004
   Wang B, 2020, VISUAL COMPUT, V36, P1897, DOI 10.1007/s00371-019-01779-3
   Wang HY, 2019, IEEE T IND INFORM, V15, P2798, DOI 10.1109/TII.2018.2887145
   Wang L, 2018, IEEE IPCCC
   Wang LJ, 2017, PROC CVPR IEEE, P3796, DOI 10.1109/CVPR.2017.404
   Wang PQ, 2018, IEEE WINT CONF APPL, P1451, DOI 10.1109/WACV.2018.00163
   Win M, 2015, IEEE T IND INFORM, V11, P642, DOI 10.1109/TII.2015.2417676
   Wu Z., 2016, ARXIV160506885
   Wu Z, 2019, PROC CVPR IEEE, P3902, DOI 10.1109/CVPR.2019.00403
   Xie SN, 2015, IEEE I CONF COMP VIS, P1395, DOI [10.1109/ICCV.2015.164, 10.1007/s11263-017-1004-z]
   Xie XH, 2005, LECT NOTES COMPUT SC, V3687, P404
   Yan Q, 2013, PROC CVPR IEEE, P1155, DOI 10.1109/CVPR.2013.153
   Yan S., 2020, P ASIAN C COMPUTER V
   Yan S., 2020, MATH PROBL ENG
   Yang C, 2013, PROC CVPR IEEE, P3166, DOI 10.1109/CVPR.2013.407
   Yang F, 2020, IEEE T INTELL TRANSP, V21, P1525, DOI 10.1109/TITS.2019.2910595
   Yang H, 2019, IEEE T AUTOM SCI ENG, V16, P1450, DOI 10.1109/TASE.2018.2886031
   Zhang H, 2018, IEEE T INSTRUM MEAS, V67, P1593, DOI 10.1109/TIM.2018.2803830
   Zhao HL, 2009, VISUAL COMPUT, V25, P973, DOI 10.1007/s00371-008-0308-y
   Zhao JX, 2019, IEEE I CONF COMP VIS, P8778, DOI 10.1109/ICCV.2019.00887
NR 60
TC 6
Z9 6
U1 5
U2 34
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2023
VL 39
IS 10
BP 4391
EP 4403
DI 10.1007/s00371-022-02597-w
EA JUL 2022
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA T1GK8
UT WOS:000829278700001
OA hybrid
DA 2024-07-18
ER

PT J
AU Wang, XC
   Wen, MZ
   Tan, XD
   Zhang, HY
   Hu, JP
   Qin, H
AF Wang, Xiaochao
   Wen, Mingzhu
   Tan, Xiaodong
   Zhang, Huayan
   Hu, Jianping
   Qin, Hong
TI A novel zero-watermarking algorithm based on robust statistical features
   for natural images
SO VISUAL COMPUTER
LA English
DT Article
DE Zero-watermarking; Statistical features; Singular value decomposition;
   Natural images
ID REVERSIBLE WATERMARKING; COPYRIGHT PROTECTION; SCHEME; DIFFERENCE;
   TRANSFORM; DOMAIN
AB In this paper, we develop a robust image zero-watermarking algorithm based on multiple circular statistical features. First of all, we detect stable feature points from the host image, and construct circular region partitions from the identified feature points, which contain the critical information of the host image. Second, the statistics and differences of circular region partitions are calculated, which contain both the local differences and the global information of the host image. Third, we apply discrete cosine transform (DCT) and singular value decomposition (SVD) in statistics and differences in order to construct the feature image. The stability of DCT and SVD further warrants the robustness of the algorithm. Moreover, Arnold transform is used to scramble the watermark image to enhance the security of the algorithm. Finally, we obtain the zero watermark by performing the exclusive-or (XOR) operation between the feature image and the scrambled watermark image, and the zero watermark is stored in the copyright authentication database for copyright authentication. Moreover, to combat geometric attacks, we develop a blind image correction algorithm without using the original image to accurately correct the attacked image. Numerous experiments and comparisons with the state-of-the-art (SOTA) watermarking algorithms confirm that the proposed algorithm affords good robustness against various attacks, such as filtering attacks, noise attacks, geometric attacks, and even a random combination of the above-stated attacks.
C1 [Wang, Xiaochao; Wen, Mingzhu; Tan, Xiaodong] Tiangong Univ, Sch Math Sci, Tianjin 300387, Peoples R China.
   [Zhang, Huayan] Tiangong Univ, Sch Comp Sci & Technol, Tianjin 300387, Peoples R China.
   [Hu, Jianping] Northeast Elect Power Univ, Sch Sci, Jilin 132012, Jilin, Peoples R China.
   [Qin, Hong] SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11794 USA.
C3 Tiangong University; Tiangong University; Northeast Electric Power
   University; State University of New York (SUNY) System; State University
   of New York (SUNY) Stony Brook
RP Hu, JP (corresponding author), Northeast Elect Power Univ, Sch Sci, Jilin 132012, Jilin, Peoples R China.
EM wangxiaochao18@163.com; wenmzz@163.com; txdtxd2021@163.com;
   zhanghy307@163.com; neduhjp307@163.com; qin@cs.stonybrook.edu
RI Hu, Jianping/IWM-3698-2023; chen, xian/KHW-2227-2024; chen,
   minghui/KFR-8832-2024
OI Wang, Xiaochao/0000-0001-7852-2106; Hu, Jianping/0000-0001-8675-4564
FU National Science Foundation of USA [IIS-1812606, IIS-1715985];
   Scientific and Technological Development Plan of Jilin Province
   [20210101472JC]; Open Project Program of the State Key Lab of CADCG
   [A2105]; Zhejiang University
FX We would like to thank the anonymous reviewers for their helpful
   comments. This work is supported in part by National Science Foundation
   of USA (IIS-1812606, IIS-1715985); Scientific and Technological
   Development Plan of Jilin Province (No. 20210101472JC), Open Project
   Program of the State Key Lab of CAD&CG (Grant No.A2105), Zhejiang
   University.
CR Abbas NH, 2018, MULTIMED TOOLS APPL, V77, P24593, DOI 10.1007/s11042-017-5488-x
   Agarwal N, 2019, MULTIMED TOOLS APPL, V78, P8603, DOI 10.1007/s11042-018-7128-5
   Alshoura WH, 2022, ALEX ENG J, V61, P5713, DOI 10.1016/j.aej.2021.10.052
   Alzahrani A, 2021, IEEE ACCESS, V9, P113714, DOI 10.1109/ACCESS.2021.3104985
   Aslantas V, 2008, AEU-INT J ELECTRON C, V62, P386, DOI 10.1016/j.aeue.2007.02.010
   Barni M, 2000, IEEE T IMAGE PROCESS, V9, P1450, DOI 10.1109/83.855442
   Barni M, 1998, SIGNAL PROCESS, V66, P357, DOI 10.1016/S0165-1684(98)00015-2
   Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1007/11744023_32
   Celik MU, 2005, IEEE T IMAGE PROCESS, V14, P253, DOI 10.1109/TIP.2004.840686
   Chang CC, 2002, PATTERN RECOGN LETT, V23, P931, DOI 10.1016/S0167-8655(02)00023-5
   Chang CC, 2008, J SYST SOFTWARE, V81, P1118, DOI 10.1016/j.jss.2007.07.036
   Chu WC, 2003, IEEE T MULTIMEDIA, V5, P34, DOI 10.1109/TMM.2003.808816
   Daoui A, 2021, EXPERT SYST APPL, V177, DOI 10.1016/j.eswa.2021.114978
   Daren H, 2001, IEEE INT C MULT EXP, P429
   Fan TY, 2019, SIGNAL PROCESS-IMAGE, V70, P174, DOI 10.1016/j.image.2018.09.015
   Fares K, 2021, BIOMED SIGNAL PROCES, V66, DOI 10.1016/j.bspc.2020.102403
   Harris C., 1988, P 4 ALV VIS C, V15, P10
   He WG, 2021, IEEE T MULTIMEDIA, V23, P52, DOI 10.1109/TMM.2020.2982042
   Hu K, 2021, VISUAL COMPUT, V37, P2841, DOI 10.1007/s00371-021-02168-5
   Kahlessenane F, 2021, J AMB INTEL HUM COMP, V12, P2931, DOI 10.1007/s12652-020-02450-9
   Kang XB, 2020, MULTIMED TOOLS APPL, V79, P1169, DOI 10.1007/s11042-019-08191-y
   Ko HJ, 2020, INFORM SCIENCES, V517, P128, DOI 10.1016/j.ins.2019.11.005
   Kumar S, 2021, MULTIMED TOOLS APPL, V80, P9315, DOI 10.1007/s11042-020-09943-x
   Lai CC, 2011, DIGIT SIGNAL PROCESS, V21, P522, DOI 10.1016/j.dsp.2011.01.017
   Li CL, 2015, NEUROCOMPUTING, V166, P404, DOI 10.1016/j.neucom.2015.03.039
   Li ZY, 2021, DIGIT SIGNAL PROCESS, V115, DOI 10.1016/j.dsp.2021.103062
   Liu JW, 2019, AIP ADV, V9, DOI 10.1063/1.5094516
   Liu XY, 2021, SIGNAL PROCESS-IMAGE, V92, DOI 10.1016/j.image.2020.116124
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Luo YL, 2021, EXPERT SYST APPL, V168, DOI 10.1016/j.eswa.2020.114272
   Peng F, 2021, IEEE T CIRC SYST VID, V31, P411, DOI 10.1109/TCSVT.2020.2969464
   Sachnev V, 2009, IEEE T CIRC SYST VID, V19, P989, DOI 10.1109/TCSVT.2009.2020257
   Shen YX, 2021, EXPERT SYST APPL, V168, DOI 10.1016/j.eswa.2020.114414
   Singh L., 2021, MULTIMED TOOLS APPL, V2, P1, DOI DOI 10.1007/S11042-021-11246-8
   Wang XC, 2021, J INF SECUR APPL, V59, DOI 10.1016/j.jisa.2021.102820
   Wang XC, 2020, VISUAL COMPUT, V36, P2201, DOI 10.1007/s00371-020-01909-2
   Wang YW, 2002, IEEE T IMAGE PROCESS, V11, P77, DOI 10.1109/83.982816
   Wen Quan, 2003, Acta Electronica Sinica, V31, P214
   Wu XQ, 2019, MULTIMED TOOLS APPL, V78, P8463, DOI 10.1007/s11042-018-6877-5
   Zou BJ, 2018, MULTIMED TOOLS APPL, V77, P28685, DOI 10.1007/s11042-018-5995-4
NR 40
TC 6
Z9 6
U1 4
U2 29
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2022
VL 38
IS 9-10
SI SI
BP 3175
EP 3188
DI 10.1007/s00371-022-02544-9
EA JUN 2022
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 4N3NE
UT WOS:000812611200002
OA Bronze
DA 2024-07-18
ER

PT J
AU Jalal, AS
   Sharma, DK
   Sikander, B
AF Jalal, Anand Singh
   Sharma, Dilip Kumar
   Sikander, Bilal
TI Suspect face retrieval using visual and linguistic information
SO VISUAL COMPUTER
LA English
DT Review
DE Face recognition; Suspect face retrieval; Biometric;
   Sketch-to-photograph retrieval; Linguistic information retrieval
ID SKETCH SYNTHESIS; RECOGNITION; IDENTIFICATION; REPRESENTATION; SALIENCY
AB Faces are the most common biometric used for the identification of a person. Law enforcement agencies use face as a key point to identify the suspect involved in unlawful activities. Forensic sketches are normally developed by the sketch artist based on verbal details provided by an eyewitness about the suspect. In a forensic sketch, the facial description depends on the memory of the eyewitness; therefore, there is uncertainty in facial attributes. In the recent past, lots of sketch-to-photograph retrieval methods are proposed by many researchers; however, they have ignored the uncertainty of facial attributes for suspect face retrieval. Recently, linguistic information is also utilized for suspect face retrieval. In this paper, we have provided an extensive review of the available methods for suspect face retrieval using visual and linguistic information. The review focuses firstly on the traditional methods and their categorization also shows the evolution of suspect face retrieval approaches over the years. We have also shown the summary of the performance of representative state-of-the-art methods.
C1 [Jalal, Anand Singh; Sharma, Dilip Kumar; Sikander, Bilal] GLA Univ, Dept Comp Engn & Applicat, Mathura 281406, UP, India.
C3 GLA University
RP Jalal, AS (corresponding author), GLA Univ, Dept Comp Engn & Applicat, Mathura 281406, UP, India.
EM asjalal@gla.ac.in; dilip.sharma@gla.ac.in; sikander.gla_mt18@gla.ac.in
RI Sharma, Dilip Kumar/GSD-9300-2022
OI Sharma, Dilip Kumar/0000-0002-3860-7997; Jalal,
   Anand/0000-0002-7469-6608
FU Ministry of Electronics and Information Technology, Government of India
   [4(8)/2020-ITEA]
FX The authors highly acknowledge Ministry of Electronics and Information
   Technology, Government of India, for its fund Grant Approval No.
   4(8)/2020-ITEA.
CR Alattab AA, 2013, P INT CONF INTELL, P299, DOI 10.1109/ISMS.2013.87
   [Anonymous], 2013, MULTIFEATURE CANONIC
   [Anonymous], 2015, P IEEE INT C COMPUTE
   [Anonymous], 2012, P 20 ACM INT C MULT
   [Anonymous], 2017, FACE SYNTHESIS VISUA
   [Anonymous], FG NET AGING DATABAS
   Benavente R, 1998, 24 COMP VIS CTR
   BENSON P J, 1991, European Journal of Cognitive Psychology, V3, P105, DOI 10.1080/09541449108406222
   Bhatt HS, 2012, IEEE T INF FOREN SEC, V7, P1522, DOI 10.1109/TIFS.2012.2204252
   Biometrics and identification innovation center, WVU MULT DAT
   Bobulski J, 2012, OPT APPL, V42, P865, DOI 10.5277/oa120418
   BRUCE V, 1992, APPL COGNITIVE PSYCH, V6, P619, DOI 10.1002/acp.2350060705
   Bruce V., 1994, VIS COGN, V1
   Buoncompagni S., 2017, INT C COMP REC SYST
   Chang L, 2011, LECT NOTES COMPUT SC, V6761, P555, DOI 10.1007/978-3-642-21602-2_60
   Cheung Y. M., 2020, LOGICAL OPERATOR ORI
   Chugh T, 2017, IEEE COMPUT SOC CONF, P619, DOI 10.1109/CVPRW.2017.90
   Conilione P, 2012, COMPUT J, V55, P1130, DOI 10.1093/comjnl/bxs041
   Dalal S, 2020, PROCEDIA COMPUT SCI, V167, P562, DOI 10.1016/j.procs.2020.03.318
   DAVIES G, 1978, J APPL PSYCHOL, V63, P180, DOI 10.1037/0021-9010.63.2.180
   Dolecki M, 2016, IEEE IJCNN, P5135, DOI 10.1109/IJCNN.2016.7727877
   FACES 4.0, 2011, IQ BIOMETRIX
   Frowd C. D., 2004, ACM Transactions on Applied Perception, V1939, P19, DOI DOI 10.1145/1008722.1008725
   Galea C., 2016, 2016 INT C BIOM SPEC, P15
   Galea C., 2016, SIGNAL PROCESSING C
   Galea C, 2018, IEEE T INF FOREN SEC, V13, P1421, DOI 10.1109/TIFS.2017.2788002
   Galea C, 2017, IEEE SIGNAL PROC LET, V24, P1586, DOI 10.1109/LSP.2017.2749266
   Galoogahi H. K., 2012, 2012 IEEE International Conference on Multimedia and Expo (ICME), P224, DOI 10.1109/ICME.2012.128
   Galoogahi HK, 2012, IEEE IMAGE PROC, P1837, DOI 10.1109/ICIP.2012.6467240
   Gao XN, 2008, IEEE T CIRC SYST VID, V18, P487, DOI 10.1109/TCSVT.2008.918770
   Gao XB, 2012, IEEE T CIRC SYST VID, V22, P1213, DOI 10.1109/TCSVT.2012.2198090
   Gary B.H., 2012, Advances in Neural Information Processing Systems (NIPS)
   Gibson L., 2008, Forensic art essentials: a manual for law enforcement artists
   Gibson L., 2010, Forensic art essentials: a manual for law enforcement artists
   Goodfellow I., 2014, P 27 INT C NEURAL IN, P2672
   Han H, 2013, IEEE T INF FOREN SEC, V8, P191, DOI 10.1109/TIFS.2012.2228856
   Hu MM, 2020, EURASIP J IMAGE VIDE, V2020, DOI 10.1186/s13640-020-0489-5
   Identi-Kit, 2011, ID KIT SOL
   Iranmanesh SM, 2018, INT CONF BIOMETR THE
   Isola P., 2017, IEEE C COMP VIS PATT
   Jain A. K., 2011, HDB FACE RECOGNITION
   Jiao LC, 2018, PATTERN RECOGN, V76, P125, DOI 10.1016/j.patcog.2017.10.025
   Karczmarek P., 2015, 2015 SIGNAL PROCESSI, P98103
   Karczmarek P, 2018, INT J FUZZY SYST, V20, P2668, DOI 10.1007/s40815-018-0517-0
   Karczmarek P, 2017, SOFT COMPUT, V21, P7503, DOI 10.1007/s00500-016-2305-9
   Karczmarek P, 2014, SOFT COMPUT, V18, P379, DOI 10.1007/s00500-013-1064-0
   Kazemi H, 2018, IEEE WINT CONF APPL, P1, DOI 10.1109/WACVW.2018.00006
   Kazemi H, 2018, IEEE COMPUT SOC CONF, P612, DOI 10.1109/CVPRW.2018.00091
   Khan Mohd Aamir, 2020, 2020 International Conference on Contemporary Computing and Applications (IC3A), P181, DOI 10.1109/IC3A48958.2020.233293
   Khan MA, 2020, EXPERT SYST APPL, V141, DOI 10.1016/j.eswa.2019.112925
   Khan MA, 2019, EXPERT SYST APPL, V134, P138, DOI 10.1016/j.eswa.2019.05.040
   Kiersztyn A, 2016, IEEE INT FUZZY SYST, P1120, DOI 10.1109/FUZZ-IEEE.2016.7737813
   KLARE BF, 2014, 2014 IEEEIAPR INT JO
   Klare B, 2010, PROC SPIE, V7667, DOI 10.1117/12.849821
   Klare BF, 2011, IEEE T PATTERN ANAL, V33, P639, DOI 10.1109/TPAMI.2010.180
   Klum SJ, 2014, IEEE T INF FOREN SEC, V9, P2248, DOI 10.1109/TIFS.2014.2360825
   Kukharev G. A., 2016, Pattern Recognition and Image Analysis, V26, P165, DOI 10.1134/S1054661816010144
   Kukharev G. A., 2015, 2015 18 INT C SOFT C
   Kumar Neeraj, 2011, IEEE Trans Pattern Anal Mach Intell, V33, P1962, DOI 10.1109/TPAMI.2011.48
   Kurach D., 2014, INT C ARTIFICIAL INT
   Lee E, 2004, ERGONOMICS, V47, P719, DOI 10.1080/00140130310001629720
   Li XW, 2012, MATH PROBL ENG, V2012, DOI 10.1155/2012/910719
   Li Y.-H., 2006, P IEEE INT C AC SPEE, V2, P3
   Liang Chang, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P2146, DOI 10.1109/ICPR.2010.526
   Liu DC, 2018, NEUROCOMPUTING, V302, P46, DOI 10.1016/j.neucom.2018.03.042
   Liu QS, 2005, PROC CVPR IEEE, P1005
   Liu W., 2007, IJCAI
   Martinho-Corbishley D., 2016, 2016 IEEE INT C ID S, P16
   Meissner C. A., 2017, HDB EYEWITNESS PSYCH, P134
   Messer K., 1999, 2 INT C AUD VID BAS, V964, P965
   Mittal P., 2015, 2015 INT C BIOMETRIC
   Mittal P, 2017, INFORM FUSION, V33, P86, DOI 10.1016/j.inffus.2016.04.003
   Nagpal S., 2017, P IEEE INT C COMPUTE
   National Institute of Standards and Technology (NIST), 2011, NIST SPECIAL DATABAS
   Ouyang SX, 2016, IMAGE VISION COMPUT, V56, P28, DOI 10.1016/j.imavis.2016.09.001
   Pallavi S, 2018, 2018 INTERNATIONAL CONFERENCE ON ADVANCES IN COMPUTING, COMMUNICATIONS AND INFORMATICS (ICACCI), P460, DOI 10.1109/ICACCI.2018.8554564
   Parkhi OM, 2015, DEEP FACE RECOGNITIO
   Peng CL, 2020, IEEE T INF FOREN SEC, V15, P172, DOI 10.1109/TIFS.2019.2916633
   Peng CL, 2016, IEEE T NEUR NET LEAR, V27, P2201, DOI 10.1109/TNNLS.2015.2464681
   Phillips PJ, 1998, IMAGE VISION COMPUT, V16, P295, DOI 10.1016/S0262-8856(97)00070-X
   Pramanik S., 2012, Proceedings of the 2012 International Conference on Pattern Recognition, Informatics and Medical Engineering (PRIME), P409, DOI 10.1109/ICPRIME.2012.6208381
   Radman A, 2019, IEEE ACCESS, V7, P108838, DOI 10.1109/ACCESS.2019.2933645
   Radman A, 2018, MULTIMED TOOLS APPL, V77, P25311, DOI 10.1007/s11042-018-5786-y
   Rahman A, 2015, INT J MACH LEARN CYB, V6, P597, DOI 10.1007/s13042-014-0256-y
   Rhodes G., 1994, VIS COGNI, V1
   Roy H, 2017, INT J MACH LEARN CYB, V8, P1457, DOI 10.1007/s13042-016-0516-0
   Sahin Yasar Guneri, 2007, Information Technology Journal, V6, P607
   Setumin S, 2019, IEEE ACCESS, V7, P27135, DOI 10.1109/ACCESS.2019.2897599
   Setumin S, 2018, IEEE ACCESS, V6, P39344, DOI 10.1109/ACCESS.2018.2855208
   Singh AK, 2017, J EXP THEOR ARTIF IN, V29, P175, DOI 10.1080/0952813X.2015.1132266
   Singh S, 2016, INT J BIOMETRICS, V8, P134, DOI 10.1504/IJBM.2016.077831
   Song Y., 2019, P EUR C COMP VIS
   Tang X, 2002, IEEE IMAGE PROC, P257
   Tang XO, 2004, IEEE T CIRC SYST VID, V14, P50, DOI 10.1109/TCSVT.2003.818353
   Taylor KT., 2000, FORENSIC ART ILLUSTR, DOI [10.1201/9781420036954, DOI 10.1201/9781420036954]
   Uhl R. G., 1996, P CVPR 96 1996 IEEE
   vanKoppen PJ, 1997, LAW HUMAN BEHAV, V21, P661, DOI 10.1023/A:1024812831576
   Wan W., 2019, INT J SYST ASSURANCE, V16
   Wan W., 2019, ADV SCI TECHNOL ENG, V4, P107, DOI [10.25046/aj040214, DOI 10.25046/AJ040214]
   Wang L., 2018, 13 IEEE INT C AUTOMA, P8390
   Wang N., 2017, PATTERN RECOGN, V76
   Wang NN, 2018, PATTERN RECOGN LETT, V107, P59, DOI 10.1016/j.patrec.2017.06.012
   Wang NN, 2017, NEUROCOMPUTING, V257, P214, DOI 10.1016/j.neucom.2016.07.071
   Wang NN, 2017, IEEE T IMAGE PROCESS, V26, P1264, DOI 10.1109/TIP.2017.2651375
   Wang NN, 2013, IEEE T NEUR NET LEAR, V24, P1364, DOI 10.1109/TNNLS.2013.2258174
   Wang XG, 2009, IEEE T PATTERN ANAL, V31, P1955, DOI 10.1109/TPAMI.2008.222
   Wells GL, 2007, CURR DIR PSYCHOL SCI, V16, P6, DOI 10.1111/j.1467-8721.2007.00465.x
   Wu JK, 1998, INFORM PROCESS MANAG, V34, P513, DOI 10.1016/S0306-4573(98)00017-X
   Wu X, 2017, TSINGHUA SCI TECHNOL, V22, P660, DOI 10.23919/TST.2017.8195348
   Xiao B, 2009, SIGNAL PROCESS, V89, P1576, DOI 10.1016/j.sigpro.2009.02.008
   Xiaoou T., 2003, P 9 IEEE INT C COMPU
   Yu J, 2021, IEEE T CYBERNETICS, V51, P4350, DOI 10.1109/TCYB.2020.2972944
   Yuen PC, 2007, IEEE T SYST MAN CY A, V37, P493, DOI 10.1109/TSMCA.2007.897588
   Zahradnikova B, 2018, ARTIF INTELL REV, V49, P131, DOI 10.1007/s10462-016-9519-1
   Zhang DY, 2017, IEEE T IMAGE PROCESS, V26, DOI 10.1109/TIP.2016.2623485
   Zhang J., 2011, 2011 18 IEEE INT C I
   Zhang L., 2015, P 5 ACM INT C MULTIM
   Zhang M., 2019, IEEE T CYBERN, V114
   Zhang SC, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1163
   Zhang SC, 2019, IEEE T NEUR NET LEAR, V30, P1419, DOI 10.1109/TNNLS.2018.2869574
   Zhang SC, 2015, IEEE T IMAGE PROCESS, V24, P2466, DOI 10.1109/TIP.2015.2422578
   Zhang W., 2010, EUROPEAN C COMPUTER
   Zhang W., 2011, 2011 IEEE C COMPUTER
   Zhang Y., 2008, P INT C BIOM THEOR A, P17
   Zhang Y, 2010, IEEE T SYST MAN CY A, V40, P475, DOI 10.1109/TSMCA.2010.2041654
   Zhao W, 2003, ACM COMPUT SURV, V35, P399, DOI 10.1145/954339.954342
   Zhou H., 2012, IEEE C COMPUTER VISI
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zhu M., 2016, P INT C INTERNET MUL
   Zhu MR, 2019, IEEE T NEUR NET LEAR, V30, P3096, DOI 10.1109/TNNLS.2018.2890018
NR 130
TC 3
Z9 3
U1 2
U2 12
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2023
VL 39
IS 7
BP 2609
EP 2635
DI 10.1007/s00371-022-02482-6
EA APR 2022
PG 27
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA L4HX1
UT WOS:000794955400002
DA 2024-07-18
ER

PT J
AU Garg, A
   Singh, AK
AF Garg, Ankit
   Singh, Anuj Kumar
TI Analysis of seam carving technique: limitations, improvements and
   possible solutions
SO VISUAL COMPUTER
LA English
DT Article
DE Seam carving; Image retargeting; Saliency detection; Multi-operator;
   Energy map; Gradient operators
ID IMAGE; ALGORITHM; PERCEPTION; SALIENCY
AB Nowadays, many efficient content-aware image resizing techniques are being used to safeguard the prominent regions of the image so that aesthetically pleasing retargeting results can be generated. In this paper, firstly various energy map generation methods are analyzed based on the conventional seam carving technique. After objective image quality assessment of the retargeted image obtained from the conventional seam carving technique, it has been found that the percentage of similarity obtained from the gradient energy map generation method is 77.44% which is higher than the other methods. Further, to minimize the deformations on the parameters such as luminance, color, and structure the improved seam carving technique utilizes the gradient energy generation method to obtain the energy map of all kinds of input images during the retargeting operation. To check the efficiency of the improved retargeting technique the obtained results are compared with the conventional seam carving technique based on different properties of the objects present in the image. After objective image quality assessment based on SSIM, it is found that the improved seam carving technique produces 70% similarity between reference and retargeted images which is 10% higher than the conventional seam carving technique. Furthermore, the energy modification operation of the improved seam carving technique also plays its significant contribution to minimize the deformation on the defined parameters. After the subjective and objective image quality assessment, it is found that the high percentage similarity in the retargeted results justifies the efficiency of the improved seam carving technique.
C1 [Garg, Ankit; Singh, Anuj Kumar] Amity Univ, Gurugram 122413, Haryana, India.
RP Garg, A (corresponding author), Amity Univ, Gurugram 122413, Haryana, India.
EM ankitgitm@gmail.com
RI Singh, Dr. Anuj Kr./HGB-0160-2022; Garg, Ankit/ABD-9886-2020; Singh,
   Anuj Kumar/ABF-9762-2021
OI Garg, Dr. Ankit/0000-0002-6466-2738
CR Abhayadev M, 2019, J SCI IND RES INDIA, V78, P193
   Abhayadev M., 2017, INT C INT COMP CONTR, P1
   Arai K, 2019, INT J ADV COMPUT SC, V10, P143
   Avidan S, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239461
   Banerjee A, 2022, VISUAL COMPUT, V38, P321, DOI 10.1007/s00371-020-02017-x
   Boiman O, 2008, PROC CVPR IEEE, P1992, DOI 10.1109/CVPR.2008.4587598
   Bolduc F., 1987, Visual Computer, V3, P51, DOI 10.1007/BF02153650
   Chang CH, 2012, PROC CVPR IEEE, P1075, DOI 10.1109/CVPR.2012.6247786
   Chen LG, 2020, VISUAL COMPUT, V36, P2017, DOI 10.1007/s00371-020-01950-1
   Chen YX, 2015, NEUROCOMPUTING, V151, P645, DOI 10.1016/j.neucom.2014.05.089
   Chen YT, 2021, VISUAL COMPUT, V37, P1691, DOI 10.1007/s00371-020-01932-3
   Choi B, 2021, 2021 INTERNATIONAL CONFERENCE ON ELECTRONICS, INFORMATION, AND COMMUNICATION (ICEIC), DOI 10.1109/ICEIC51217.2021.9369775
   Conge D. D., 2010, Proceedings of the 2010 IEEE Workshop on Signal Processing Systems (SiPS 2010), P345, DOI 10.1109/SIPS.2010.5624813
   Dong WM, 2012, J COMPUT SCI TECH-CH, V27, P121, DOI 10.1007/s11390-012-1211-6
   Fang YM, 2017, IEEE T SYST MAN CY-S, V47, P2956, DOI 10.1109/TSMC.2016.2557225
   Fang YM, 2014, IEEE J EM SEL TOP C, V4, P95, DOI 10.1109/JETCAS.2014.2298919
   Frankovich M, 2011, IEEE SIGNAL PROC LET, V18, P375, DOI 10.1109/LSP.2011.2140396
   Garg A, 2020, IET IMAGE PROCESS, V14, P2965, DOI 10.1049/iet-ipr.2019.1032
   Garg A, 2020, KSII T INTERNET INF, V14, P2997, DOI 10.3837/tiis.2020.07.015
   Garg A, 2021, SIGNAL IMAGE VIDEO P, V15, P185, DOI 10.1007/s11760-020-01736-x
   Gharahbagh AA, 2018, INT J ENG-IRAN, V31, P684, DOI 10.5829/ije.2018.31.05b.01
   Guo TD, 2021, VISUAL COMPUT, V37, P2069, DOI 10.1007/s00371-020-01964-9
   Guo YC, 2018, J ELECTRON INF TECHN, V40, P331, DOI 10.11999/JEIT170501
   Guo Z, 2017, 2 INT C ART INT ENG, P651, DOI [10.12783/dtcse/aiea2017/14995, DOI 10.12783/DTCSE/AIEA2017/14995]
   Han R, 2018, EXPERT SYST APPL, V95, P162, DOI 10.1016/j.eswa.2017.11.023
   Hashemzadeh M, 2019, SIGNAL PROCESS, V155, P233, DOI 10.1016/j.sigpro.2018.09.037
   Kiess J., 2010, INT SOC OPT PHOTON, V7542, P75420
   Kiess J, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3231598
   Kiess J, 2012, PROC SPIE, V8304, DOI 10.1117/12.906386
   Kim HK, 2011, IEEE T CONSUM ELECTR, V57, P1875, DOI 10.1109/TCE.2011.6131166
   Koo HI, 2009, IEEE IMAGE PROC, P209, DOI 10.1109/ICIP.2009.5414470
   Kumar M, 2011, J SIGNAL PROCESS SYS, V65, P159, DOI 10.1007/s11265-011-0613-y
   Lin HH, 2019, INT WORK QUAL MULTIM
   Lin W., 2018, INT C SMART VEH TECH, V86, P282, DOI [10.1007/978-3-319-70730-3_34, DOI 10.1007/978-3-319-70730-3_34]
   Mei Y., 2021, P IEEE INT C MULTIME, P1
   Nie YW, 2013, VISUAL COMPUT, V29, P785, DOI 10.1007/s00371-013-0830-4
   Patel Diptiben, 2018, Proceedings of 2nd International Conference on Computer Vision & Image Processing. CVIP 2017. Advances in Intelligent Systems and Computing (AISC 704), P89, DOI 10.1007/978-981-10-7898-9_8
   Patel D, 2019, PATTERN RECOGN LETT, V125, P179, DOI 10.1016/j.patrec.2019.04.013
   Patel D, 2019, NATL CONF COMMUN, DOI 10.1109/ncc.2019.8732245
   Patel D, 2019, IET IMAGE PROCESS, V13, P885, DOI 10.1049/iet-ipr.2018.5283
   Rubinstein M, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531329
   Senturk Zehra Karapinar, 2014, International Journal of Advanced Studies in Computer Science and Engineering, V3, P5
   Senturk ZK., 2019, 2019 1 INT INFORMATI, P1, DOI [10.1109/UBMYK48245.2019.8965618, DOI 10.1109/UBMYK48245.2019.8965618]
   Shafieyan F, 2017, SIGNAL PROCESS-IMAGE, V50, P34, DOI 10.1016/j.image.2016.10.006
   Shamir A, 2009, COMMUN ACM, V52, P77, DOI 10.1145/1435417.1435437
   Solanki P, 2017, ADV INTELL SYST, V459, P467, DOI 10.1007/978-981-10-2104-6_42
   Song E, 2019, IEEE ACCESS, V7, P284, DOI 10.1109/ACCESS.2018.2885347
   Su Z, 2013, VISUAL COMPUT, V29, P1011, DOI 10.1007/s00371-012-0753-5
   Tang ZH, 2022, MULTIMED TOOLS APPL, V81, P1501, DOI 10.1007/s11042-021-11376-z
   Valdez-Balderas D, 2021, IEEE IMAGE PROC, P1849, DOI 10.1109/ICIP42928.2021.9506584
   Vaquero D, 2010, PROC SPIE, V7798, DOI 10.1117/12.862419
   Venkataramanan AK, 2021, IEEE ACCESS, V9, P28872, DOI 10.1109/ACCESS.2021.3056504
   Wang ZP, 2019, SIGNAL PROCESS-IMAGE, V79, P63, DOI 10.1016/j.image.2019.08.015
   Wei YZ, 2021, J MEAS ENG, V9, P156, DOI 10.21595/jme.2021.22023
   Xin ZH, 2019, VISUAL COMPUT, V35, P1245, DOI 10.1007/s00371-018-1590-y
   Xu JL, 2018, VISUAL COMPUT, V34, P431, DOI 10.1007/s00371-017-1350-4
   Yang YZ, 2022, VISUAL COMPUT, V38, P2647, DOI 10.1007/s00371-021-02143-0
   Yihjia Tsai, 2013, Applied Mechanics and Materials, V385-386, P1453, DOI 10.4028/www.scientific.net/AMM.385-386.1453
   Zhai GT, 2020, SCI CHINA INFORM SCI, V63, DOI 10.1007/s11432-019-2757-1
   Zhang DY, 2017, J VIS COMMUN IMAGE R, V48, P281, DOI 10.1016/j.jvcir.2017.07.006
   Zhang LX, 2017, SOFT COMPUT, V21, P447, DOI 10.1007/s00500-015-1795-1
   Zhang Y, 2017, MULTIMED TOOLS APPL, V76, P8067, DOI 10.1007/s11042-016-3318-1
   Zhang Y, 2019, VISUAL COMPUT, V35, P823, DOI 10.1007/s00371-019-01694-7
   Zhou Y, 2021, IEEE T CIRC SYST VID, V31, P126, DOI 10.1109/TCSVT.2020.2977943
NR 64
TC 4
Z9 4
U1 1
U2 13
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2023
VL 39
IS 7
BP 2683
EP 2709
DI 10.1007/s00371-022-02486-2
EA APR 2022
PG 27
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA L4HX1
UT WOS:000785702100002
DA 2024-07-18
ER

PT J
AU Hu, K
   Wang, XC
   Hu, JP
   Li, DY
   Du, L
   Wang, HF
   Qin, H
AF Hu, Kun
   Wang, Xiaochao
   Hu, Jianping
   Li, Danyang
   Du, Ling
   Wang, Hongfei
   Qin, Hong
TI Robust and efficient image watermarking via EMD and dimensionality
   reduction
SO VISUAL COMPUTER
LA English
DT Article
DE Image watermarking; Empirical mode decomposition; Hilbert curve;
   Dimensionality reduction; Arnold transform; Huffman coding
ID EMPIRICAL MODE DECOMPOSITION; SINGULAR-VALUE DECOMPOSITION; INTEGER
   WAVELET TRANSFORM; HILBERT SPECTRUM; SCHEME; DCT; DOMAIN; SYSTEM;
   COMPUTATION; OWNERSHIP
AB In the past decades, bi-dimensional empirical mode decomposition (BEMD) algorithms have been developed for the image watermarking task with good robustness and imperceptibility; however, the non-negligible algorithm efficiency has received little attention at the same time. In this paper, we devise a robust and efficient image watermarking algorithm based on 1D empirical mode decomposition (EMD) and dimensional reduction via Hilbert curve. Converting the problem of two-dimensional image watermarking into the problem of one-dimensional signal watermarking promises to enhance the efficiency and the robustness of the proposed algorithm. Specifically, host image is first reduced into one-dimensional signal by the Hilbert curve. Our key insight is a dimensionality reduction strategy based on the Hilbert curve that preserves the spatial local relationship to the greatest extent possible. Second, the one-dimensional signal is segmented into several short signal intervals, and each of them is decomposed into several intrinsic mode functions (IMFs) and a residue by 1D EMD, which is much faster than BEMD. Third, the maximum or minimum points of the first IMF are chosen as the watermark embedding positions. For the watermark image, it is first encrypted by Arnold transform, which improves the security of the algorithm. And then, it is also transformed into one-dimensional signal correspondingly. A repeated embedding strategy is used in the embedding process to improve the algorithm's robustness. The final watermarked image can be reconstructed by the inverse Hilbert curve transform after integrating the modified first IMF, the remaining IMFs, and the residual. To improve the security and reduce the length of the key, Arnold transform and Huffman coding are adopted. The watermark extraction is the inverse of the embedding process without using the host image and watermark image. Comprehensive experimental results confirm that our new algorithm exhibits good robustness, efficiency, and high imperceptibility. Compared with the actual watermarking algorithms, the newly proposed watermarking algorithm not only reduces the computational expense, but also shows better performance in combating the cropping attacks, Gaussian noise, median filter, image enhancement attacks, etc.
C1 [Hu, Kun] Univ Chinese Acad Sci, Beijing 100049, Peoples R China.
   [Wang, Xiaochao] Tiangong Univ, Sch Math Sci, Tianjin 300387, Peoples R China.
   [Hu, Jianping] Northeast Elect Power Univ, Sch Sci, Jilin 132012, Jilin, Peoples R China.
   [Li, Danyang; Du, Ling] Tiangong Univ, Sch Comp Sci & Technol, Tianjin 300387, Peoples R China.
   [Wang, Hongfei] Chinese Acad Sci, Technol & Engn Ctr Space Utilizat, Key Lab Space Utilizat, Beijing 100094, Peoples R China.
   [Qin, Hong] SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11794 USA.
C3 Chinese Academy of Sciences; University of Chinese Academy of Sciences,
   CAS; Tiangong University; Northeast Electric Power University; Tiangong
   University; Chinese Academy of Sciences; Technology & Engineering Center
   for Space Utilization, CAS; State University of New York (SUNY) System;
   State University of New York (SUNY) Stony Brook
RP Wang, XC (corresponding author), Tiangong Univ, Sch Math Sci, Tianjin 300387, Peoples R China.
EM ucas_hukun@163.com; wangxiaochao18@163.com; neduhjp307@163.com;
   tjpu_echolee@163.com; duling@tiangong.edu.cn; whf@csu.ac.cn;
   qin@cs.stonybrook.edu
RI Hu, Jianping/IWM-3698-2023; Li, Dan/HJA-0406-2022; li,
   danyang/HHS-3319-2022; Wang, Hongfei/JCF-2357-2023
OI Wang, Xiaochao/0000-0001-7852-2106; Hu, Jianping/0000-0001-8675-4564
FU National Science Foundation of USA [IIS-1812606, IIS-1715985]; National
   Natural Science Foundation of China [61532002, 61672149, 61602341,
   61602344, 61802279]; Science & Technology Development Fund of Tianjin
   Education Commission for Higher Education [2018KJ222]; Open Project
   Program of the State Key Lab of CAD&CG, Zhejiang University [A2105]
FX We would like to thank the anonymous reviewers for their helpful
   comments. This work is supported in part by National Science Foundation
   of USA (IIS-1812606, IIS-1715985); National Natural Science Foundation
   of China (No. 61532002, 61672149, 61602341, 61602344, 61802279); The
   Science & Technology Development Fund of Tianjin Education Commission
   for Higher Education (Grant No.2018KJ222). The Open Project Program of
   the State Key Lab of CAD&CG (Grant No. A2105), Zhejiang University.
CR Abbas NH, 2018, MULTIMED TOOLS APPL, V77, P24593, DOI 10.1007/s11042-017-5488-x
   Ali H, 2015, EXPERT SYST APPL, V42, P1261, DOI 10.1016/j.eswa.2014.08.049
   Ali M, 2015, INFORM SCIENCES, V301, P44, DOI 10.1016/j.ins.2014.12.042
   Alshoura WH, 2020, IEEE ACCESS, V8, P43391, DOI 10.1109/ACCESS.2020.2978186
   Amira-Biad S, 2015, INT ARAB J INF TECHN, V12, P24
   An LL, 2012, IEEE T IMAGE PROCESS, V21, P3598, DOI 10.1109/TIP.2012.2191564
   Ahmadi SBB, 2021, VISUAL COMPUT, V37, P385, DOI 10.1007/s00371-020-01808-6
   Barni M, 1998, SIGNAL PROCESS, V66, P357, DOI 10.1016/S0165-1684(98)00015-2
   Begum M., 2021, SN Comput. Sci, V2, P221, DOI [10.1007/s42979-021-00608-6, DOI 10.1007/S42979-021-00608-6]
   Bi N, 2007, IEEE T IMAGE PROCESS, V16, P1956, DOI 10.1109/TIP.2007.901206
   Boudraa AO, 2007, IEEE T INSTRUM MEAS, V56, P2196, DOI 10.1109/TIM.2007.907967
   Butz A. R., 1969, J COMPUTER SYSTEM SC, V3, P128, DOI [DOI 10.1016/S0022-0000(69)80010-3, 10.1016/S0022-0000(69)80010-3]
   Chang CC, 2006, PATTERN RECOGN LETT, V27, P439, DOI 10.1016/j.patrec.2005.09.006
   Chen BJ, 2014, DIGIT SIGNAL PROCESS, V28, P106, DOI 10.1016/j.dsp.2014.02.010
   Chen ZG, 2018, IEEE T MULTIMEDIA, V20, P1973, DOI 10.1109/TMM.2018.2794985
   Chu WC, 2003, IEEE T MULTIMEDIA, V5, P34, DOI 10.1109/TMM.2003.808816
   Cox IJ, 1997, IEEE T IMAGE PROCESS, V6, P1673, DOI 10.1109/83.650120
   Di CL, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0104663
   Ernawan F, 2020, VISUAL COMPUT, V36, P19, DOI 10.1007/s00371-018-1567-x
   Fallahpour M, 2014, IEEE T INSTRUM MEAS, V63, P1057, DOI 10.1109/TIM.2014.2299371
   Hamidi M, 2018, MULTIMED TOOLS APPL, V77, P27181, DOI 10.1007/s11042-018-5913-9
   Hernández JR, 2000, IEEE T IMAGE PROCESS, V9, P55, DOI 10.1109/83.817598
   Hilbert D, 1935, Dritter Band: Analysis Grundlagen der Mathematik Physik Verschiedenes: Nebst Einer Lebensgeschichte, P1, DOI 10.1007/978-3-662-38452-7_1
   Hu JP, 2016, COMPUT AIDED GEOM D, V43, P95, DOI 10.1016/j.cagd.2016.02.011
   Hu JP, 2014, GRAPH MODELS, V76, P340, DOI 10.1016/j.gmod.2014.03.006
   Huang NE, 1998, P ROY SOC A-MATH PHY, V454, P903, DOI 10.1098/rspa.1998.0193
   Islam M, 2018, MULTIMED TOOLS APPL, V77, P14407, DOI 10.1007/s11042-017-5035-9
   Jagadish HV, 1997, INFORM PROCESS LETT, V62, P17, DOI 10.1016/S0020-0190(97)00014-8
   Kang XB, 2018, MULTIMED TOOLS APPL, V77, P13197, DOI 10.1007/s11042-017-4941-1
   Lee Y, 2009, 2009 11TH IEEE INTERNATIONAL SYMPOSIUM ON MULTIMEDIA (ISM 2009), P583, DOI 10.1109/ISM.2009.48
   Li CL, 2015, NEUROCOMPUTING, V166, P404, DOI 10.1016/j.neucom.2015.03.039
   Li ZY, 2021, DIGIT SIGNAL PROCESS, V115, DOI 10.1016/j.dsp.2021.103062
   Luo YL, 2021, EXPERT SYST APPL, V168, DOI 10.1016/j.eswa.2020.114272
   Makbol NM, 2017, INFORM SCIENCES, V417, P381, DOI 10.1016/j.ins.2017.07.026
   Makbol NM, 2014, DIGIT SIGNAL PROCESS, V33, P134, DOI 10.1016/j.dsp.2014.06.012
   Nazari M, 2021, MULTIMED TOOLS APPL, V80, P10615, DOI 10.1007/s11042-020-10032-2
   Nunes JC, 2003, LECT NOTES COMPUT SC, V2749, P171
   Sayood K, 2017, Introduction to data compression
   Singh SP, 2018, J VIS COMMUN IMAGE R, V53, P86, DOI 10.1016/j.jvcir.2018.03.006
   Solachidis V, 2004, IEEE COMPUT GRAPH, V24, P44, DOI 10.1109/MCG.2004.1297010
   Subr K, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618493
   Taghia J, 2008, CISP 2008: FIRST INTERNATIONAL CONGRESS ON IMAGE AND SIGNAL PROCESSING, VOL 5, PROCEEDINGS, P674, DOI 10.1109/CISP.2008.717
   Tao PN, 2006, CONSUM COMM NETWORK, P1134
   Tsui TK, 2008, IEEE T INF FOREN SEC, V3, P16, DOI 10.1109/TIFS.2007.916275
   Verma VS, 2019, MULTIMED TOOLS APPL, V78, P23203, DOI 10.1007/s11042-019-7599-z
   Wang H, 2012, GRAPH MODELS, V74, P173, DOI 10.1016/j.gmod.2012.04.005
   Wang XC, 2020, VISUAL COMPUT, V36, P2201, DOI 10.1007/s00371-020-01909-2
   Wang XC, 2018, COMPUT GRAPH-UK, V70, P118, DOI 10.1016/j.cag.2017.07.024
   Wang XC, 2018, COMPUT AIDED GEOM D, V59, P1, DOI 10.1016/j.cagd.2017.11.002
   Wang XC, 2015, VISUAL COMPUT, V31, P1135, DOI 10.1007/s00371-015-1100-4
   Wang YW, 2002, IEEE T IMAGE PROCESS, V11, P77, DOI 10.1109/83.982816
   Xie XH, 2014, SIGNAL PROCESS, V103, P250, DOI 10.1016/j.sigpro.2013.11.038
   Xu GL, 2009, PATTERN RECOGN, V42, P718, DOI 10.1016/j.patcog.2008.09.017
   Zang Y, 2014, IEEE T VIS COMPUT GR, V20, P1253, DOI 10.1109/TVCG.2014.2298017
   Zhang DB, 2017, COMPUT AIDED DESIGN, V87, P1, DOI 10.1016/j.cad.2017.02.003
   Zhou NR, 2018, MULTIMED TOOLS APPL, V77, P30251, DOI 10.1007/s11042-018-6128-9
   Zhu X, 2007, SIGNAL PROCESS-IMAGE, V22, P515, DOI 10.1016/j.image.2007.03.004
NR 57
TC 4
Z9 4
U1 1
U2 18
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2022
VL 38
IS 6
BP 2153
EP 2170
DI 10.1007/s00371-021-02275-3
EA AUG 2021
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 1C6YN
UT WOS:000686026700001
DA 2024-07-18
ER

PT J
AU Li, LY
   Tang, JS
   Ye, Z
   Sheng, B
   O, LJ
   Ma, LZ
AF Li, Luying
   Tang, Junshu
   Ye, Zhou
   Sheng, Bin
   O, Lijuan
   Ma, Lizhuang
TI Unsupervised face super-resolution via gradient enhancement and semantic
   guidance
SO VISUAL COMPUTER
LA English
DT Article
DE Unsupervised face super-resolution; Facial semantic priors; Gradient
   enhancement
ID IMAGE; RECOGNITION
AB Face super-resolution aims to recover high-resolution face images with accurate geometric structures. Most of the conventional super-resolution methods are trained on paired data that is difficult to obtain in the real-world setting. Besides, these methods do not fully utilize facial prior knowledge for face super-resolution. To tackle these problems, we propose an end-to-end unsupervised face super-resolution network to super-resolve low-resolution face images. We propose a gradient enhancement branch and a semantic guidance mechanism. Specifically, the gradient enhancement branch reconstructs high-resolution gradient maps, under the restriction of two proposed gradient losses. Then the super-resolution network integrates features in both image and gradient space to super-resolve face images with geometric structure preservation. Moreover, the proposed semantic guidance mechanism, including a semantic-adaptive sharpen module and a semantic-guided discriminator, can reconstruct sharp edges and improve local details in different facial regions adaptively, under the guidance of semantic parsing maps. Qualitative and quantitative experiments demonstrate that our proposed method can reconstruct high-resolution face images with sharp edges and photo-realistic details, outperforming the state-of-the-art methods.
C1 [Li, Luying; Tang, Junshu; Sheng, Bin; Ma, Lizhuang] Shanghai Jiao Tong Univ, Dept Comp Sci & Engn, Shanghai 200240, Peoples R China.
   [Ye, Zhou] Shanghai CLS Fintech Co LTD, Shanghai 200030, Peoples R China.
   [O, Lijuan] Shanghai Univ Sport, Shanghai 200438, Peoples R China.
   [Ma, Lizhuang] East China Normal Univ, Sch Comp Sci & Technol, Shanghai 200062, Peoples R China.
C3 Shanghai Jiao Tong University; Shanghai University of Sport; East China
   Normal University
RP Ma, LZ (corresponding author), Shanghai Jiao Tong Univ, Dept Comp Sci & Engn, Shanghai 200240, Peoples R China.; O, LJ (corresponding author), Shanghai Univ Sport, Shanghai 200438, Peoples R China.; Ma, LZ (corresponding author), East China Normal Univ, Sch Comp Sci & Technol, Shanghai 200062, Peoples R China.
EM liluying@sjtu.edu.cn; tangjs@sjtu.edu.cn; yezhou@cls.cn;
   shengbin@cs.sjtu.edu.cn; maolijuan@sus.edu.cn; ma-lz@cs.sjtu.edu.cn
RI Sun, Peng/KDO-4243-2024
OI Li, Luying/0000-0002-9188-1768
FU National Key Research and Development Program of China [2019YFC1521104];
   National Natural Science Foundation of China [61972157]; Economy and
   Informatization Commission of Shanghai Municipality
   [XX-RGZN-01-19-6348]; Fundamental Research Funds for the Central
   Universities [2021QN1072]
FX This work is supported by the National Key Research and Development
   Program of China (No. 2019YFC1521104), National Natural Science
   Foundation of China (No. 61972157), the Economy and Informatization
   Commission of Shanghai Municipality (No. XX-RGZN-01-19-6348), and
   Fundamental Research Funds for the Central Universities (No.
   2021QN1072).
CR Amaranageswarao G, 2022, VISUAL COMPUT, V38, P31, DOI 10.1007/s00371-020-01998-z
   Anokhin Ivan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7485, DOI 10.1109/CVPR42600.2020.00751
   Biswas S, 2013, IEEE T PATTERN ANAL, V35, P3037, DOI 10.1109/TPAMI.2013.68
   Bulat A, 2018, PROC CVPR IEEE, P109, DOI 10.1109/CVPR.2018.00019
   Bulat A, 2017, IEEE I CONF COMP VIS, P1021, DOI 10.1109/ICCV.2017.116
   Cao G, 2011, IEEE SIGNAL PROC LET, V18, P603, DOI 10.1109/LSP.2011.2164791
   Chang H, 2004, PROC CVPR IEEE, P275, DOI 10.1109/cvpr.2004.1315043
   Chen J, 2020, IEEE SIGNAL PROC LET, V27, P645, DOI 10.1109/LSP.2020.2986942
   Chen Y, 2018, PROC CVPR IEEE, P2492, DOI 10.1109/CVPR.2018.00264
   Cheng Ma, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7766, DOI 10.1109/CVPR42600.2020.00779
   Choudhury A, 2014, IEEE SW SYMP IMAG, P157, DOI 10.1109/SSIAI.2014.6806053
   Damer N, 2019, INT CONF BIOMETR THE, DOI 10.1109/btas46853.2019.9185994
   Dong C, 2014, LECT NOTES COMPUT SC, V8692, P184, DOI 10.1007/978-3-319-10593-2_13
   Fattal R, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1239451.1239546, 10.1145/1276377.1276496]
   Freedman G, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1944846.1944852
   Freeman WT, 2002, IEEE COMPUT GRAPH, V22, P56, DOI 10.1109/38.988747
   Fritsche M, 2019, IEEE INT CONF COMP V, P3599, DOI 10.1109/ICCVW.2019.00445
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Hennings-Yeomans PH, 2008, PROC CVPR IEEE, P3637
   Huang YW, 2017, PROC CVPR IEEE, P5787, DOI 10.1109/CVPR.2017.613
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jain V., 2010, Fddb: A benchmark for face detection in unconstrained settings
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Karras T., 2018, arXiv, DOI [10.48550/arXiv.1710.10196, DOI 10.48550/ARXIV.1710.10196]
   Karras T, 2021, IEEE T PATTERN ANAL, V43, P4217, DOI 10.1109/TPAMI.2020.2970919
   Karras Tero, 2020, IEEE C COMP VIS PATT
   KEYS RG, 1981, IEEE T ACOUST SPEECH, V29, P1153, DOI 10.1109/TASSP.1981.1163711
   Kingma D. P., 2014, arXiv
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Li LY, 2022, VISUAL COMPUT, V38, P3577, DOI 10.1007/s00371-021-02188-1
   Liu W, 2017, ADV SOC SCI EDUC HUM, V99, P212
   Nie YW, 2013, IEEE T VIS COMPUT GR, V19, P1664, DOI 10.1109/TVCG.2012.176
   Peng KS, 2013, IEEE INT SYM MULTIM, P508, DOI 10.1109/ISM.2013.100
   Sajjadi Mehdi S. M., 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P4501, DOI 10.1109/ICCV.2017.481
   Shocher A, 2018, PROC CVPR IEEE, P3118, DOI 10.1109/CVPR.2018.00329
   Wang CY, 2020, INT CONF ACOUST SPEE, P2518, DOI [10.1109/icassp40776.2020.9053398, 10.1109/ICASSP40776.2020.9053398]
   Wang XP, 2018, IDEAS HIST MOD CHINA, V19, P1, DOI 10.1163/9789004385580_002
   Wang ZF, 2014, VISUAL COMPUT, V30, P359, DOI 10.1007/s00371-013-0861-x
   Wen Y, 2019, IEEE T IMAGE PROCESS, V28, P994, DOI 10.1109/TIP.2018.2874285
   Xin JW, 2019, AAAI CONF ARTIF INTE, P9054
   Xiong ZW, 2010, IEEE T IMAGE PROCESS, V19, P2017, DOI 10.1109/TIP.2010.2045707
   Yin Y, 2020, AAAI CONF ARTIF INTE, V34, P12693
   Yu CQ, 2018, LECT NOTES COMPUT SC, V11217, P334, DOI 10.1007/978-3-030-01261-8_20
   Yu X, 2018, LECT NOTES COMPUT SC, V11213, P219, DOI 10.1007/978-3-030-01240-3_14
   Yu X, 2018, PROC CVPR IEEE, P908, DOI 10.1109/CVPR.2018.00101
   Yuan Y, 2018, IEEE COMPUT SOC CONF, P814, DOI 10.1109/CVPRW.2018.00113
   Zhang HC, 2010, LECT NOTES COMPUT SC, V6313, P566
   Zhang LP, 2010, SIGNAL PROCESS, V90, P848, DOI 10.1016/j.sigpro.2009.09.002
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhang YB, 2020, IEEE T IMAGE PROCESS, V29, P1101, DOI 10.1109/TIP.2019.2938347
   Zhao TY, 2020, IEEE INT CON MULTI, DOI 10.1109/icme46284.2020.9102926
   Zhou Y., 2020, P IEEE CVF C COMP VI, P428, DOI DOI 10.1109/YAC51587.2020.9337596
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zou WWW, 2012, IEEE T IMAGE PROCESS, V21, P327, DOI 10.1109/TIP.2011.2162423
NR 54
TC 18
Z9 18
U1 0
U2 12
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2021
VL 37
IS 9-11
SI SI
BP 2855
EP 2867
DI 10.1007/s00371-021-02236-w
EA JUL 2021
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA UL2RI
UT WOS:000676074700001
DA 2024-07-18
ER

PT J
AU Shopon, M
   Bari, ASMH
   Gavrilova, ML
AF Shopon, Md
   Bari, A. S. M. Hossain
   Gavrilova, Marina L.
TI Residual connection-based graph convolutional neural networks for gait
   recognition
SO VISUAL COMPUTER
LA English
DT Article
DE Gait Recognition; Behavioral biometric; Graph Convolutional Neural
   Network; Video Processing
ID REPRESENTATIONS; TECHNOLOGY
AB The walking manner of a person, also known as gait, is a unique behavioral biometric trait. Existing methods for gait recognition predominantly utilize traditional machine learning. However, the performance of gait recognition can deteriorate under challenging conditions including environmental occlusion, bulky clothing, and different viewing angles. To provide an effective solution to gait recognition under these conditions, this paper proposes a novel deep learning architecture using Graph Convolutional Neural Network (GCNN) that incorporates residual connections for gait recognition from videos. The optimized feature map of the proposed GCNN architecture exhibits the invariant property to viewing angle and subject's clothing. The residual connection is used to capture both spatial and temporal features of a gait sequence. The kinematic dependency extracted from shallower network layer is propagated to deeper layer using residual connection-based GCNN architecture. The proposed method is validated on CASIA-B gait dataset and outperforms all recent state-of-the-art methods.
C1 [Shopon, Md] Univ Calgary, Comp Sci, Calgary, AB T2N 1N4, Canada.
   [Bari, A. S. M. Hossain] Univ Calgary, Calgary, AB T2N 1N4, Canada.
   [Gavrilova, Marina L.] Univ Calgary, Dept Comp Sci, Calgary, AB T2N 1N4, Canada.
C3 University of Calgary; University of Calgary; University of Calgary
RP Shopon, M (corresponding author), Univ Calgary, Comp Sci, Calgary, AB T2N 1N4, Canada.
EM md.shopon@ucalgary.ca; asmhossain.bari@ucalgary.ca; mgavrilo@ucalgary.ca
RI Gavrilova, Marina/AHD-3605-2022
OI Gavrilova, Marina/0000-0002-5338-1834; Shopon, Md./0000-0003-0422-9075
FU Natural Sciences and Engineering Research Council (NSERC); NSERC
   Strategic Partnership Grant (SPG); Innovation for Defense Excellence and
   Security Network (IDEaS)
FX The authors acknowledge the Natural Sciences and Engineering Research
   Council (NSERC) Discovery Grant funding, as well as the NSERC Strategic
   Partnership Grant (SPG) and the Innovation for Defense Excellence and
   Security Network (IDEaS) for the partial funding of this project.
CR Ahmed F, 2015, VISUAL COMPUT, V31, P915, DOI 10.1007/s00371-015-1092-0
   Ahmed F, 2020, IEEE ACCESS, V8, P11761, DOI 10.1109/ACCESS.2019.2963113
   [Anonymous], 2016, P 29 INT C COMP AN S
   Bari ASMH, 2019, IEEE ACCESS, V7, P162708, DOI 10.1109/ACCESS.2019.2952065
   Battistone F, 2019, PATTERN RECOGN LETT, V126, P132, DOI 10.1016/j.patrec.2018.05.004
   BenAbdelkader C, 2004, EURASIP J APPL SIG P, V2004, P572, DOI 10.1155/S1110865704309236
   Boulgouris NV, 2005, IEEE SIGNAL PROC MAG, V22, P78, DOI 10.1109/MSP.2005.1550191
   Bronstein MM, 2017, IEEE SIGNAL PROC MAG, V34, P18, DOI 10.1109/MSP.2017.2693418
   Bruna J., 2014, ABS13126203 CORR, P1, DOI [10.48550/arXiv.1312.6203, DOI 10.48550/ARXIV.1312.6203]
   Cao Z, 2021, IEEE T PATTERN ANAL, V43, P172, DOI 10.1109/TPAMI.2019.2929257
   Chao HQ, 2019, AAAI CONF ARTIF INTE, P8126
   Cho CW, 2009, EXPERT SYST APPL, V36, P7033, DOI 10.1016/j.eswa.2008.08.076
   Gavrilova M.L., 2021, Research Anthology on Rehabilitation Practices and Therapy, P653
   Güler RA, 2018, PROC CVPR IEEE, P7297, DOI 10.1109/CVPR.2018.00762
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He YW, 2019, IEEE T INF FOREN SEC, V14, P102, DOI 10.1109/TIFS.2018.2844819
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Huang B., 2019, ARXIV190905214
   Kipf TN, 2017, INT C LEARN REPR
   LeCun Y., 1995, The handbook of brain theory and neural networks, V3361, DOI [10.5555/303568.303704, DOI 10.5555/303568.303704]
   Li N., 2020, ARXIV200508625
   Liao RJ, 2020, PATTERN RECOGN, V98, DOI 10.1016/j.patcog.2019.107069
   Lin BB, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P3054, DOI 10.1145/3394171.3413861
   Mao MG, 2020, IEEE/IAPR INTERNATIONAL JOINT CONFERENCE ON BIOMETRICS (IJCB 2020), DOI 10.1109/ijcb48548.2020.9304916
   Murray M P, 1967, Am J Phys Med, V46, P290
   MURRAY MP, 1964, J BONE JOINT SURG AM, V46, P335, DOI 10.2106/00004623-196446020-00009
   Obaidat MS, 2018, Biometric based physical and cybersecurity systems, P1, DOI [10.1007/978-3-319-98734-7, DOI 10.1007/978-3-319-98734-7]
   Pouyanfar S., 2019, IEEECVF C COMP VIS P
   Rahman MW, 2017, 2017 IEEE 16TH INTERNATIONAL CONFERENCE ON COGNITIVE INFORMATICS & COGNITIVE COMPUTING (ICCI*CC), P423, DOI 10.1109/ICCI-CC.2017.8109783
   Rijun Liao, 2017, Biometric Recognition. 12th Chinese Conference, CCBR 2017. Proceedings: LNCS 10568, P474, DOI 10.1007/978-3-319-69923-3_51
   Seifert AK, 2017, IEEE RAD CONF, P1428, DOI 10.1109/RADAR.2017.7944431
   Shaik, 2020, THESIS NATL COLL IRE
   Shao HK, 2019, ELECTRON LETT, V55, P890, DOI 10.1049/el.2019.1221
   Shen YT, 2018, INT CONF IMAG VIS
   Shi L, 2019, PROC CVPR IEEE, P7904, DOI 10.1109/CVPR.2019.00810
   Teepe T., ARXIV210111228
   Thalmann N. M., 2016, SIGGRAPH ASIA 2016 C, P1
   Tieleman T., 2012, Coursera: Neural networks for machine learning, V4, P26
   Toshev A, 2014, PROC CVPR IEEE, P1653, DOI 10.1109/CVPR.2014.214
   Tu ZG, 2018, PATTERN RECOGN, V79, P32, DOI 10.1016/j.patcog.2018.01.020
   Valsesia D, 2019, IEEE IMAGE PROC, P2399, DOI [10.1109/icip.2019.8803367, 10.1109/ICIP.2019.8803367]
   Vincent P, 2010, J MACH LEARN RES, V11, P3371
   Wang WG, 2019, IEEE I CONF COMP VIS, P9235, DOI 10.1109/ICCV.2019.00933
   Wolf T, 2016, IEEE IMAGE PROC, P4165, DOI 10.1109/ICIP.2016.7533144
   Wu ZF, 2017, IEEE T PATTERN ANAL, V39, P209, DOI 10.1109/TPAMI.2016.2545669
   Xiao QH, 2007, IEEE COMPUT INTELL M, V2, P5, DOI 10.1109/MCI.2007.353415
   Ye FF, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P55, DOI 10.1145/3394171.3413941
   Zhang C, 2016, INT CONF ACOUST SPEE, P2832, DOI 10.1109/ICASSP.2016.7472194
   Zhang J., 2020, SPATIAL ATTENTIVE TE
   Zhang W, 2020, VISUAL COMPUT, V36, P2433, DOI 10.1007/s00371-020-01955-w
   Zhang YZ, 2015, VISUAL COMPUT, V31, P1615, DOI 10.1007/s00371-014-1043-1
   Zhang ZP, 2019, PROC CVPR IEEE, P4586, DOI 10.1109/CVPR.2019.00472
NR 52
TC 6
Z9 6
U1 2
U2 11
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2021
VL 37
IS 9-11
SI SI
BP 2713
EP 2724
DI 10.1007/s00371-021-02245-9
EA JUL 2021
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA UL2RI
UT WOS:000673882700001
DA 2024-07-18
ER

PT J
AU Lima, ARC
   Medeiros, AM
   Marques, VG
   Oliveira, MM
AF Lima, Alex R. Cunha
   Medeiros, Arthur M.
   Marques, Vitor G.
   Oliveira, Manuel M.
TI Real-time simulation of accommodation and low-order aberrations of the
   human eye using light-gathering trees
SO VISUAL COMPUTER
LA English
DT Article
DE Real-time visual simulation; Low-order aberrations; Partial occlusions;
   Ray tracing
ID DEPTH
AB We present a real-time technique for simulating accommodation and low-order aberrations (e.g., myopia, hyperopia, and astigmatism) of the human eye. Our approach models the corresponding point spread function, producing realistic depth-dependent simulations. Real-time performance is achieved with the use of a novel light-gathering tree data structure, which allows us to approximate the contributions of over 300 samples per pixel under 6 ms per frame. For comparison, with the same time budget, an optimized ray tracer exploring specialized hardware acceleration traces two samples per pixel. We demonstrate the effectiveness of our approach through a series of qualitative and quantitative experiments on images with depth from real environments. Our results achieved SSIM values ranging from 0.94 to 0.99 and PSNR ranging from 32.4 to 43.0 in objective evaluations, indicating good agreement with the ground truth.
C1 [Lima, Alex R. Cunha; Medeiros, Arthur M.; Marques, Vitor G.; Oliveira, Manuel M.] Univ Fed Rio Grande do Sul, Inst Informat, Porto Alegre, RS, Brazil.
C3 Universidade Federal do Rio Grande do Sul
RP Oliveira, MM (corresponding author), Univ Fed Rio Grande do Sul, Inst Informat, Porto Alegre, RS, Brazil.
EM arclima@inf.ufrgs.br; ammedeiros@inf.ufrgs.br;
   vitor.marques@inf.ufrgs.br; oliveira@inf.ufrgs.br
RI Menezes de Oliveira Neto, Manuel/H-1508-2011
OI Menezes de Oliveira Neto, Manuel/0000-0003-4957-9984
FU CNPq-Brazil [312975/2018-0, 423673/2016-5, 131288/2016-4]; CAPES [001]
FX This work was funded by CNPq-Brazil (fellowships and Grants
   312975/2018-0, 423673/2016-5 and 131288/2016-4), and CAPES Finance Code
   001.
CR Barsky B., 2002, P 13 EUR WORKSH REND, P1
   Barsky B.A., 2003, P VIS VID GRAPH VVG, P97
   Barsky B.A., 2004, APGV 04, P73, DOI DOI 10.1145/1012551.1012564
   Bedggood P, 2008, J BIOMED OPT, V13, DOI 10.1117/1.2907211
   Cholewiak SA, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130815
   Cook R. L., 1984, Computers & Graphics, V18, P137
   Garcia K, 2017, ACM SIGGRAPH 2017 TALKS, DOI 10.1145/3084363.3085022
   Gilles A, 2016, APPL OPTICS, V55, P5459, DOI 10.1364/AO.55.005459
   He JC, 2000, VISION RES, V40, P41, DOI 10.1016/S0042-6989(99)00156-X
   Hillaire S, 2008, IEEE COMPUT GRAPH, V28, P47, DOI 10.1109/MCG.2008.113
   Kopf J, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392420
   Koulieris GA, 2019, COMPUT GRAPH FORUM, V38, P493, DOI 10.1111/cgf.13654
   Kraus M, 2007, COMPUT GRAPH FORUM, V26, P645, DOI 10.1111/j.1467-8659.2007.01088.x
   Krueger ML, 2016, SIBGRAPI, P64, DOI [10.1109/SIBGRAPI.2016.17, 10.1109/SIBGRAPI.2016.018]
   Lee S, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778802
   Li ZQ, 2018, PROC CVPR IEEE, P2041, DOI 10.1109/CVPR.2018.00218
   Luis A, 2007, EUR J PHYS, V28, P231, DOI 10.1088/0143-0807/28/2/008
   Niemitalo, 2011, CIRCULARLY SYMMETRIC
   Parker SG, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778803
   Policarpo F, 2005, ACM T GRAPHIC, V24, P935, DOI 10.1145/1073204.1073292
   Policarpo F., 2006, Proceedings of the 2006 symposium on Interactive 3D graphics and games, P55
   Polyanskiy M.N., Refractive index database
   Potmesil M., 1982, ACM Transactions on Graphics (TOG), V1, P85
   Ranftl R, 2022, IEEE T PATTERN ANAL, V44, P1623, DOI 10.1109/TPAMI.2020.3019967
   Scharstein D, 2014, LECT NOTES COMPUT SC, V8753, P31, DOI 10.1007/978-3-319-11752-2_3
   Schedl D.C., 2012, J WSCG, V20, P239
   Schwartz S., 2010, VISUAL PERCEPTION CL, VFourth
   Scofield B., 1992, GRAPHICS GEMS, P36
   SHINYA M, 1994, GRAPH INTER, P59
   Thibos LN, 2002, J REFRACT SURG, V18, pS652
   Xiao L, 2018, SIGGRAPH'18: ACM SIGGRAPH 2018 TALKS, DOI 10.1145/3214745.3214769
NR 31
TC 2
Z9 2
U1 0
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2021
VL 37
IS 9-11
SI SI
BP 2581
EP 2593
DI 10.1007/s00371-021-02194-3
EA JUL 2021
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA UL2RI
UT WOS:000670881700002
DA 2024-07-18
ER

PT J
AU Li, HL
   Zhong, ZZ
   Guan, W
   Du, CH
   Yang, Y
   Wei, YX
   Ye, C
AF Li, Haolong
   Zhong, Zizheng
   Guan, Wei
   Du, Chenghao
   Yang, Yu
   Wei, Yuxiang
   Ye, Chen
TI Generative character inpainting guided by structural information
SO VISUAL COMPUTER
LA English
DT Article
DE Character inpainting; Generative adversarial networks; Structural
   information; Style encoder
ID IMAGE; COMPLETION
AB Character inpainting is an attractive and challenging task, especially for Chinese calligraphy characters with complex structures and styles. The diversity of Chinese calligraphy styles has created its unique artistic beauty, but specific style features will lead to obvious differences in style and stroke details while recovering. Most current methods are restricted to recover specific characters already present in the training set and retrain the model when recovering characters of new styles. Moreover, these methods are based on edge recovery, which requires the location of the masked area. In this paper, we propose a novel structure-guided generative framework guided by prototype character, which can not only adapt to multiple style fonts but also overall recover glyph structure and strokes without masked information by inferring the style representation. In this case, our method can recover new style characters, which is the first attempt for character inpainting without parameter retraining. Experimental results demonstrate that our method has generalizability and superiority in most application scenarios, compared with several state-of-the-art character inpainting methods.
C1 [Li, Haolong; Zhong, Zizheng; Guan, Wei; Du, Chenghao; Wei, Yuxiang; Ye, Chen] Tongji Univ, Dept Comp Sci & Technol, Shanghai, Peoples R China.
   [Ye, Chen] Tongji Univ, Key Lab Embedded Syst & Serv Comp, Minist Educ, Shanghai, Peoples R China.
   [Yang, Yu] Tongji Univ, Dept Software Engn, Shanghai, Peoples R China.
C3 Tongji University; Tongji University; Tongji University
RP Ye, C (corresponding author), Tongji Univ, Dept Comp Sci & Technol, Shanghai, Peoples R China.; Ye, C (corresponding author), Tongji Univ, Key Lab Embedded Syst & Serv Comp, Minist Educ, Shanghai, Peoples R China.
EM yechen@tongji.edu.cn
OI Ye, Chen/0000-0002-0048-6974
CR Ballester C, 2001, IEEE T IMAGE PROCESS, V10, P1200, DOI 10.1109/83.935036
   Choi Y, 2018, PROC CVPR IEEE, P8789, DOI 10.1109/CVPR.2018.00916
   Gao YM, 2020, AAAI CONF ARTIF INTE, V34, P646
   Gatys L., 2016, Journal of Vision, V16, P326, DOI DOI 10.1167/16.12.326
   Goodfellow I. J., 2015, 3 INT C LEARNING REP
   He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38
   Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167
   Gulrajani I, 2017, ADV NEUR IN, V30
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jiang Y, 2019, AAAI CONF ARTIF INTE, P4015
   Li JW, 2020, NEURAL COMPUT APPL, V32, P4805, DOI 10.1007/s00521-018-3854-x
   Li ST, 2011, PATTERN RECOGN LETT, V32, P1256, DOI 10.1016/j.patrec.2011.03.015
   Lian Z., 2016, SIGGRAPH ASIA 2016 T, P1, DOI DOI 10.1145/3005358.3005371
   Lian Z, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3213767
   Liu CL, 2011, PROC INT CONF DOC, P37, DOI 10.1109/ICDAR.2011.17
   Liu MY, 2019, IEEE I CONF COMP VIS, P10550, DOI 10.1109/ICCV.2019.01065
   Mao XD, 2017, IEEE I CONF COMP VIS, P2813, DOI 10.1109/ICCV.2017.304
   Nazeri K, 2019, IEEE INT CONF COMP V, P3265, DOI 10.1109/ICCVW.2019.00408
   Song G, 2020, NEUROCOMPUTING, V415, P146, DOI 10.1016/j.neucom.2020.07.046
   Sun J, 2005, ACM T GRAPHIC, V24, P861, DOI 10.1145/1073204.1073274
   Wang XQ, 2012, 8TH INTERNATIONAL CONFERENCE ON SIGNAL IMAGE TECHNOLOGY & INTERNET BASED SYSTEMS (SITIS 2012), P281, DOI 10.1109/SITIS.2012.49
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wei YQ, 2016, SIGNAL IMAGE VIDEO P, V10, P911, DOI 10.1007/s11760-015-0840-y
   Xu S., 2009, 21 IAAI C CIT
   [叶晨 Ye Chen], 2020, [同济大学学报. 自然科学版, Journal of Tongji University. Natural Science], V48, P591
   Yu JH, 2018, PROC CVPR IEEE, P5505, DOI 10.1109/CVPR.2018.00577
   Zhang Y., 2020, IEEE T IMAGE PROCESS
   Zhong Z, 2017, PROCEEDINGS 2017 4TH IAPR ASIAN CONFERENCE ON PATTERN RECOGNITION (ACPR), P804, DOI 10.1109/ACPR.2017.60
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 29
TC 3
Z9 3
U1 1
U2 15
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2021
VL 37
IS 9-11
SI SI
BP 2895
EP 2906
DI 10.1007/s00371-021-02218-y
EA JUN 2021
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA UL2RI
UT WOS:000668413900001
DA 2024-07-18
ER

PT J
AU Shajini, M
   Ramanan, A
AF Shajini, Majuran
   Ramanan, Amirthalingam
TI A knowledge-sharing semi-supervised approach for fashion clothes
   classification and attribute prediction
SO VISUAL COMPUTER
LA English
DT Article
DE DeepFashion; Fashion attribute prediction; Fashion clothes
   classification; Semi-supervised learning; Teacher-student pair model
AB Fashion is a form of self-expression that permits us to manifest our personality and identity with more confidence. Visual fashion clothing analysis has attracted researchers who have sought to pioneer deep learning concepts. In this work, we introduce a semi-supervised multi-task learning approach intending to attain clothing category classification and attribute prediction. For intensifying semi-supervised fashion clothes analysis, we embrace a teacher-student (T-S) pair model that utilises weighted loss minimisation while sharing knowledge between teacher and student. Our focus in this work is on strengthening the feature representation by simultaneous learning of labelled and unlabelled samples that avoids additional training for unlabelled samples. As a result, our approach involves in gaining beneficiary performance by making use of semi-supervised learning in fashion clothing analysis. We evaluated the proposed approach on the large-scale DeepFashion-C dataset and the combined unlabelled dataset obtained from six publicly available datasets. Experimental results show that the proposed paired architecture involving deep neural networks is comparable to state-of-the-art techniques in fashion clothing analysis.
C1 [Shajini, Majuran; Ramanan, Amirthalingam] Univ Jaffna, Dept Comp Sci, Fac Sci, Jaffna, Sri Lanka.
C3 University Jaffna
RP Shajini, M (corresponding author), Univ Jaffna, Dept Comp Sci, Fac Sci, Jaffna, Sri Lanka.
EM shayu.kiri@gmail.com; a.ramanan@univ.jfn.ac.lk
RI Ramanan, Amirthalingam/AAO-1674-2020
OI Ramanan, Amirthalingam/0000-0002-7110-1277; Majuran,
   Shajini/0000-0003-2981-1141
CR Ak KE, 2019, IEEE I CONF COMP VIS, P10540, DOI 10.1109/ICCV.2019.01064
   Bossard L., 2012, Revised Selected Papers, P321
   Chen HZ, 2012, LECT NOTES COMPUT SC, V7574, P609, DOI 10.1007/978-3-642-33712-3_44
   Cheng X., 2020, P IEEECVF C COMPUTER, P12925
   Cho H, 2019, IEEE INT CONF COMP V, P3197, DOI 10.1109/ICCVW.2019.00398
   Corbière C, 2017, IEEE INT CONF COMP V, P2268, DOI 10.1109/ICCVW.2017.266
   Dean J., 2015, NIPS DEEP LEARNING R
   Ferreira B.Q., 2019, ARXIV PREPRINT ARXIV
   Flennerhag S., 2020, ARXIV PREPRINT ARXIV
   Furlanello T., 2018, P MACHINE LEARNING R, V80, P1607, DOI DOI 10.48550/ARXIV.1805.04770
   Gammerman Alex, 2013, ARXIV13017375
   Grandvalet Y., 2005, CAP, V367, P281
   Guo S., 2019, P IEEE INT C COMP VI
   Guodong Xu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12354), P588, DOI 10.1007/978-3-030-58545-7_34
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang CQ, 2019, IEEE T CYBERNETICS, V49, P3744, DOI 10.1109/TCYB.2018.2850745
   Huang JS, 2015, IEEE I CONF COMP VIS, P1062, DOI 10.1109/ICCV.2015.127
   Iscen A, 2019, PROC CVPR IEEE, P5065, DOI 10.1109/CVPR.2019.00521
   Kabbai L, 2019, VISUAL COMPUT, V35, P679, DOI 10.1007/s00371-018-1503-0
   Kendall A, 2018, PROC CVPR IEEE, P7482, DOI 10.1109/CVPR.2018.00781
   Kiapour MH, 2015, IEEE I CONF COMP VIS, P3343, DOI 10.1109/ICCV.2015.382
   Kingma D. P., 2014, arXiv
   Kingma D. P., 2014, AUTOENCODING VARIATI, P3581
   Koch G., 2015, ICML DEEP LEARNING W, V2
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Laine S., 2016, ARXIV161002242
   Lee D.-H., 2013, WORKSHOP CHALLENGES, V3, P896
   Lee S, 2019, ELECTRON LETT, V55, P745, DOI 10.1049/el.2019.0660
   Lee S, 2019, IEEE INT CONF COMP V, P3153, DOI 10.1109/ICCVW.2019.00387
   Li PC, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2909
   Li YH, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2230
   Li YX, 2019, IEEE INT CON MULTI, P820, DOI 10.1109/ICME.2019.00146
   Li ZK, 2019, IEEE INT CON MULTI, P484, DOI 10.1109/ICME.2019.00090
   Liu J., 2018, EUROPEAN C COMPUTER, P30
   Liu ZW, 2016, PROC CVPR IEEE, P1096, DOI 10.1109/CVPR.2016.124
   Loni B, 2014, P 5 ACM MULT SYST C, P41
   Ma JQ, 2019, ADV NEUR IN, V32
   Meng Z, 2019, INT CONF ACOUST SPEE, P6445, DOI [10.1109/ICASSP.2019.8683438, 10.1109/icassp.2019.8683438]
   Miyato T, 2019, IEEE T PATTERN ANAL, V41, P1979, DOI 10.1109/TPAMI.2018.2858821
   Odena Augustus., 2016, Semi-supervised learning with generative adversarial networks
   Oliver A., 2018, Advances in Neural Information Processing Systems, P3239
   Salimans T, 2016, ADV NEUR IN, V29
   Sawant SS, 2020, EGYPT J REMOTE SENS, V23, P243, DOI 10.1016/j.ejrs.2018.11.001
   Shajini M., 2020, IMPROVED LANDMARK DR, P1
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Snell J, 2017, ADV NEUR IN, V30
   Tarvainen Antti, 2017, ADV NEURAL INFORM PR, P2, DOI DOI 10.1137/0330046
   van Engelen JE, 2020, MACH LEARN, V109, P373, DOI 10.1007/s10994-019-05855-6
   Vinyals O, 2016, 30 C NEURAL INFORM P, V29
   Wang WG, 2018, PROC CVPR IEEE, P4271, DOI 10.1109/CVPR.2018.00449
   Wang XH, 2019, PROC CVPR IEEE, P3551, DOI 10.1109/CVPR.2019.00367
   Yalniz Ismet Zeki, 2019, Billion-scale semi-supervised learning for image classification
   Ye Y., 2019, ARXIV PREPRINT ARXIV
   Yim J, 2017, PROC CVPR IEEE, P7130, DOI 10.1109/CVPR.2017.754
   Yuwei Zhang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13535, DOI 10.1109/CVPR42600.2020.01355
   Zhang YL, 2012, COMM COM INF SC, V308, P179
   Zhu MJ, 2019, INT CONF ACOUST SPEE, P3762, DOI [10.1109/ICASSP.2019.8682926, 10.1109/icassp.2019.8682926]
NR 57
TC 14
Z9 14
U1 3
U2 14
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2022
VL 38
IS 11
BP 3551
EP 3561
DI 10.1007/s00371-021-02178-3
EA JUN 2021
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 5X0BS
UT WOS:000659034300003
DA 2024-07-18
ER

PT J
AU Lahaw, ZB
   Seddik, H
AF Bannour Lahaw, Zied
   Seddik, Hassene
TI A new greedy sparse recovery algorithm for fast solving sparse
   representation
SO VISUAL COMPUTER
LA English
DT Article
DE Kernel sparse representation-based classification; Compressive sensing;
   Fast reduced sampling matching pursuit; Sherman-Morrison-Woodbury
   formula; Face recognition
ID L(1)-NORM MINIMIZATION; FACE RECOGNITION; SIGNAL RECOVERY; EIGENFACES;
   PURSUIT
AB Kernel sparse representation-based classification (KSRC) in compressive sensing represents one of the most interesting research areas in pattern recognition and image processing. Nevertheless, KSRC is subjected to some shortcomings. KSRC is greedy in time to achieve an approximate solution of sparse representation based on l(1)-norm minimization. A diversity of greedy recovery algorithms have been tested in order to decrease computational complexity compared to the optimal l(1)-norm minimization while keeping a proved reconstruction accuracy. In this research, we suggest a new greedy recovery algorithm, called the fast reduced sampling matching pursuit (FRSMP). Unlike previous greedy recovery algorithms which applied too many or very few values per iteration, FRSMP selects a sufficient number of elements. Moreover, FRSMP performs the least square minimization iteratively through the Sherman-Morrison-Woodbury formula, to avoid large matrix inversion which results in a significant speedup. Experimental results with both noisy and noiseless data have shown that the proposed FRSMP method achieves a higher reconstruction accuracy at low reconstruction time compared to other greedy pursuit algorithms. Also, experiments on frequent face databases have proven that the KSRC-based FRSMP method shows reliable and higher recognition rate and computation time compared with other advanced sparse representation methods for face recognition.
C1 [Bannour Lahaw, Zied; Seddik, Hassene] Univ Tunis, Lab Robot Intelligente Fiabilite & Traitement Sin, ENSIT, Tunis 1008, Tunisia.
C3 Universite de Tunis
RP Lahaw, ZB (corresponding author), Univ Tunis, Lab Robot Intelligente Fiabilite & Traitement Sin, ENSIT, Tunis 1008, Tunisia.
EM ziedd.bannour@gmail.com; Seddikhassne@gmail.com
OI hassene, seddik/0000-0003-0848-8285
CR Abdel-Sayed MM, 2016, J ADV RES, V7, P851, DOI 10.1016/j.jare.2016.08.005
   Beal Matthew James, 2003, Variational algorithms for approximate Bayesian inference
   Belhumeur PN, 1997, IEEE T PATTERN ANAL, V19, P711, DOI 10.1109/34.598228
   Benavente R, 1998, 24 COMP VIS CTR
   Blumensath T, 2008, IEEE T SIGNAL PROCES, V56, P2370, DOI 10.1109/TSP.2007.916124
   Blumensath T, 2009, IEEE T SIGNAL PROCES, V57, P4333, DOI 10.1109/TSP.2009.2025088
   Candes EJ, 2006, IEEE T INFORM THEORY, V52, P5406, DOI 10.1109/TIT.2006.885507
   Carrasco M, 2015, PATTERN RECOGN, V48, P1598, DOI 10.1016/j.patcog.2014.12.006
   Dai W, 2009, IEEE T INFORM THEORY, V55, P2230, DOI 10.1109/TIT.2009.2016006
   Deng WH, 2012, IEEE T PATTERN ANAL, V34, P1864, DOI 10.1109/TPAMI.2012.30
   Dong Z, 2016, IMAGE VISION COMPUT, V51, P13, DOI 10.1016/j.imavis.2016.03.010
   Donoho DL, 2008, IEEE T INFORM THEORY, V54, P4789, DOI 10.1109/TIT.2008.929958
   Donoho DL, 2006, IEEE T INFORM THEORY, V52, P1289, DOI 10.1109/TIT.2006.871582
   Donoval D, 2006, TRANSISTOR LEVEL MODELING FOR ANALOG/ RF IC DESIGN, P1, DOI 10.1007/1-4020-4556-5_1
   Fan ZZ, 2020, MULTIMED TOOLS APPL, V79, P7319, DOI 10.1007/s11042-019-08211-x
   Fan ZZ, 2018, PATTERN RECOGN, V76, P1, DOI 10.1016/j.patcog.2017.10.001
   Fan ZZ, 2015, NEUROCOMPUTING, V151, P304, DOI 10.1016/j.neucom.2014.09.035
   HAGER WW, 1989, SIAM REV, V31, P221, DOI 10.1137/1031049
   Kim SJ, 2007, IEEE J-STSP, V1, P606, DOI 10.1109/JSTSP.2007.910971
   Lahaw ZB, 2018, 2018 41ST INTERNATIONAL CONFERENCE ON TELECOMMUNICATIONS AND SIGNAL PROCESSING (TSP), P413
   Needell D, 2009, APPL COMPUT HARMON A, V26, P301, DOI 10.1016/j.acha.2008.07.002
   Rosário F, 2016, IEEE SIGNAL PROC LET, V23, P75, DOI 10.1109/LSP.2015.2500682
   Tibshirani R, 1996, J ROY STAT SOC B, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x
   Tropp JA, 2007, IEEE T INFORM THEORY, V53, P4655, DOI 10.1109/TIT.2007.909108
   TURK M, 1991, J COGNITIVE NEUROSCI, V3, P71, DOI 10.1162/jocn.1991.3.1.71
   Wang B, 2017, VISUAL COMPUT, V33, P647, DOI 10.1007/s00371-016-1215-2
   Wright J, 2009, IEEE T PATTERN ANAL, V31, P210, DOI 10.1109/TPAMI.2008.79
   Xu Y, 2016, PATTERN RECOGN, V54, P68, DOI 10.1016/j.patcog.2015.12.017
   Zha ZY, 2018, VISUAL COMPUT, V34, P117, DOI 10.1007/s00371-016-1318-9
   Zhang L, 2012, IEEE T SIGNAL PROCES, V60, P1684, DOI 10.1109/TSP.2011.2179539
NR 30
TC 2
Z9 2
U1 0
U2 15
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2022
VL 38
IS 7
BP 2431
EP 2445
DI 10.1007/s00371-021-02121-6
EA APR 2021
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 2B0WL
UT WOS:000640840600002
DA 2024-07-18
ER

PT J
AU Huang, YP
   Lu, RT
   Li, XF
   Qi, NX
   Yang, XG
AF Huang, Yueping
   Lu, Ruitao
   Li, Xiaofeng
   Qi, Naixin
   Yang, Xiaogang
TI Discriminative correlation tracking based on spatial attention mechanism
   for low-resolution imaging systems
SO VISUAL COMPUTER
LA English
DT Article
DE Visual object tracking; Correlation filter; Spatial attention mechanism;
   Adjustable window function
AB Low-resolution images are characterized by blurring, less texture information, and lack of detail. Visual object tracking for low-resolution imaging systems remains a challenging task. In this paper, we propose a discriminative correlation tracking algorithm based on a spatial attention mechanism for low-resolution imaging systems (LSDCT) to address these challenges. The key innovations of our proposed algorithm include adjustable windows and a spatial attention mechanism. We design a generic adjustable window to mitigate boundary effects and employ the spatial attention mechanism to highlight the target in low-resolution images. We conduct qualitative and quantitative evaluations on three well-known benchmark datasets: OTB100, TC128, and UAV123. Extensive experimental results indicate that the proposed approach is superior to state-of-the-art trackers.
C1 [Huang, Yueping; Lu, Ruitao; Li, Xiaofeng; Qi, Naixin; Yang, Xiaogang] Xian Res Inst Hitech, Xian 710025, Peoples R China.
C3 Rocket Force University of Engineering
RP Huang, YP (corresponding author), Xian Res Inst Hitech, Xian 710025, Peoples R China.
EM hypsc1990@163.com; lrt19880220@163.com; xiaofeng_li2006@126.com;
   qinaixin2015@sina.com; doctoryxg@163.com
RI liang, liang/IAO-8518-2023; li, xiao/HKV-8405-2023; li,
   xiaofeng/GXF-9442-2022; wang, xu/IAN-4886-2023; liu,
   jiaming/IWE-3196-2023; li, xiao/HJP-5134-2023; li, xiao/GSN-6181-2022;
   yuan, liping/JPK-7584-2023
FU Natural Science Foundation of China [61806209]; Key Laboratory of
   Shaanxi Province Open Foundation [SKLIIN-20180103]
FX This research was funded by the Natural Science Foundation of China,
   Grant Number 61806209, and the Key Laboratory of Shaanxi Province Open
   Foundation, Grant Number SKLIIN-20180103.
CR Abbass MY, 2021, VISUAL COMPUT, V37, P993, DOI 10.1007/s00371-020-01848-y
   Abbass MY, 2021, VISUAL COMPUT, V37, P831, DOI 10.1007/s00371-020-01833-5
   [Anonymous], 2017, ARXIV170404057
   Bertinetto L, 2016, PROC CVPR IEEE, P1401, DOI 10.1109/CVPR.2016.156
   Bertinetto L, 2016, LECT NOTES COMPUT SC, V9914, P850, DOI 10.1007/978-3-319-48881-3_56
   Bolme DS, 2010, PROC CVPR IEEE, P2544, DOI 10.1109/CVPR.2010.5539960
   Buch N, 2011, IEEE T INTELL TRANSP, V12, P920, DOI 10.1109/TITS.2011.2119372
   Chatfield K, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.76
   Chen BY, 2019, PATTERN RECOGN, V87, P80, DOI 10.1016/j.patcog.2018.10.005
   Dai KN, 2019, PROC CVPR IEEE, P4665, DOI 10.1109/CVPR.2019.00480
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Danelljan M, 2017, PROC CVPR IEEE, P6931, DOI 10.1109/CVPR.2017.733
   Danelljan M, 2017, IEEE T PATTERN ANAL, V39, P1561, DOI 10.1109/TPAMI.2016.2609928
   Danelljan M, 2016, LECT NOTES COMPUT SC, V9909, P472, DOI 10.1007/978-3-319-46454-1_29
   Danelljan M, 2015, IEEE I CONF COMP VIS, P4310, DOI 10.1109/ICCV.2015.490
   Galoogahi HK, 2017, IEEE I CONF COMP VIS, P1144, DOI [10.1109/ICCV.2017.128, 10.1109/ICCV.2017.129]
   Gundogdu E, 2018, IEEE T IMAGE PROCESS, V27, P2526, DOI 10.1109/TIP.2018.2806280
   Henriques JF, 2015, IEEE T PATTERN ANAL, V37, P583, DOI 10.1109/TPAMI.2014.2345390
   Li F, 2018, PROC CVPR IEEE, P4904, DOI 10.1109/CVPR.2018.00515
   Li WC, 2022, IEEE T CYBERNETICS, V52, P3519, DOI 10.1109/TCYB.2020.2985398
   Li X, 2019, PROC CVPR IEEE, P1369, DOI 10.1109/CVPR.2019.00146
   Li Y, 2015, LECT NOTES COMPUT SC, V8926, P254, DOI 10.1007/978-3-319-16181-5_18
   Liang PP, 2015, IEEE T IMAGE PROCESS, V24, P5630, DOI 10.1109/TIP.2015.2482905
   Lu RT, 2022, IEEE GEOSCI REMOTE S, V19, DOI 10.1109/LGRS.2020.3038784
   Lukezic A, 2017, PROC CVPR IEEE, P4847, DOI 10.1109/CVPR.2017.515
   Ma C, 2015, IEEE I CONF COMP VIS, P3074, DOI 10.1109/ICCV.2015.352
   Mueller M, 2017, PROC CVPR IEEE, P1387, DOI 10.1109/CVPR.2017.152
   Mueller M, 2016, LECT NOTES COMPUT SC, V9905, P445, DOI 10.1007/978-3-319-46448-0_27
   Possegger H, 2015, PROC CVPR IEEE, P2113, DOI 10.1109/CVPR.2015.7298823
   Song YB, 2017, IEEE I CONF COMP VIS, P2574, DOI 10.1109/ICCV.2017.279
   Tian CW, 2020, NEURAL NETWORKS, V124, P117, DOI 10.1016/j.neunet.2019.12.024
   Tian SJ, 2020, VISUAL COMPUT, V36, P1219, DOI 10.1007/s00371-019-01730-6
   Wang N, 2020, IEEE T IMAGE PROCESS, V29, P6123, DOI 10.1109/TIP.2020.2989544
   Wang N, 2019, PROC CVPR IEEE, P1308, DOI 10.1109/CVPR.2019.00140
   Wang XC, 2017, IEEE T IMAGE PROCESS, V26, P4765, DOI 10.1109/TIP.2017.2723239
   Wu Y, 2015, IEEE T PATTERN ANAL, V37, P1834, DOI 10.1109/TPAMI.2014.2388226
   Xiao Y., 2020, IEEE T CYBERNETICS, V99, P1
   Xu, 2019, IEEE INT C UNM SYST
   Xue YX, 2019, IEEE T COGN DEV SYST, V11, P162, DOI 10.1109/TCDS.2018.2800167
   Yiming Li, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11920, DOI 10.1109/CVPR42600.2020.01194
   Zhang DJ, 2020, VISUAL COMPUT, V36, P509, DOI 10.1007/s00371-019-01634-5
   Zhang JM, 2019, IEEE ACCESS, V7, P83873, DOI 10.1109/ACCESS.2019.2924944
   Zhang YS, 2020, IEEE T MULTIMEDIA, V22, P2844, DOI 10.1109/TMM.2020.2966887
   Zhao HS, 2018, LECT NOTES COMPUT SC, V11213, P270, DOI 10.1007/978-3-030-01240-3_17
   Zhu YK, 2017, INT CONF ACOUST SPEE, P5335, DOI 10.1109/ICASSP.2017.7953175
NR 45
TC 2
Z9 2
U1 1
U2 18
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2022
VL 38
IS 4
SI SI
BP 1495
EP 1508
DI 10.1007/s00371-021-02083-9
EA MAR 2021
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 0D9DX
UT WOS:000627014300001
DA 2024-07-18
ER

PT J
AU Jia, XK
   Xiao, J
   Wu, C
AF Jia, Xinkang
   Xiao, Jun
   Wu, Chao
TI TICS: text-image-based semantic CAPTCHA synthesis via multi-condition
   adversarial learning
SO VISUAL COMPUTER
LA English
DT Article
DE Text&#8211; image-based CAPTCHA; security mechanism; Generative
   Adversarial Network; semantic image synthesis
AB CAPTCHA is used to distinguish humans from automated programs and plays an important role in multimedia security mechanisms. Traditional CAPTCHA methods like image-based CAPTCHA and text-based CAPTCHA are usually based on word-level understanding, which can be easily cracked due to the recent success of deep learning techniques. To this end, this paper proposes a text-image-based CAPTCHA based on the cognition process and semantic reasoning and a novel model to generate the CAPTCHA. This method synthesizes three features: sentence, object, and location to generate a multi-conditional CAPTCHA that can resist the attack of the classification of CNN. A quantity of experiments has been conducted, and the result showed that the classification of ResNet-50 on the proposed TIC only achieves 3.38% accuracy.
C1 [Jia, Xinkang] Zhejiang Univ, Coll Software Technol, Hangzhou, Peoples R China.
   [Xiao, Jun] Zhejiang Univ, Coll Comp Sci, Hangzhou, Peoples R China.
   [Wu, Chao] Zhejiang Univ, Sch Publ Affairs, Hangzhou, Peoples R China.
C3 Zhejiang University; Zhejiang University; Zhejiang University
RP Wu, C (corresponding author), Zhejiang Univ, Sch Publ Affairs, Hangzhou, Peoples R China.
EM xk_jia@zju.edu.cn; junx@cs.zju.edu.cn; chao.wu@zju.edu.cn
OI Wu, Chao/0000-0003-0885-6869
FU Fundamental Research Funds for the Central Universities; Artificial
   Intelligence Research Foundation of Baidu Inc., Zhejiang University;
   Cybervein Joint Research Lab, Zhejiang Natural Science Foundation
   [LY19F020051, R19F020009, LZ17F020-001]; National Natural Science
   Foundation of China [61572-431, U19B2042]; Key R&D Program of Zhejiang
   Province [2018C01006]; Program of China Knowledge Center for Engineering
   Sciences and Technology; Program of ZJU and Tongdun Joint Research Lab;
   Program of ZJU and Horizon Robotics Joint Research Lab; Joint Research
   Program of ZJU; Hikvision Research Institute; Major Scientific Research
   Project of Zhejiang Lab [2018EC0ZX01-1]; CAS Earth Science Research
   Project [XDA19020104]
FX This work is supported by Fundamental Research Funds for the Central
   Universities, Artificial Intelligence Research Foundation of Baidu Inc.,
   Zhejiang University and Cybervein Joint Research Lab, Zhejiang Natural
   Science Foundation (LY19F020051, R19F020009, LZ17F020-001), National
   Natural Science Foundation of China (61572-431, U19B2042), Key R&D
   Program of Zhejiang Province (2018C01006), Program of China Knowledge
   Center for Engineering Sciences and Technology, Program of ZJU and
   Tongdun Joint Research Lab, Program of ZJU and Horizon Robotics Joint
   Research Lab, Joint Research Program of ZJU and Hikvision Research
   Institute, and Major Scientific Research Project of Zhejiang Lab (No.
   2018EC0ZX01-1), CAS Earth Science Research Project(XDA19020104).
CR Aleksandrovich P.N., 2012, US Patent App, Patent No. [13/528,373, 13528373]
   Bursztein E, 2011, PROCEEDINGS OF THE 18TH ACM CONFERENCE ON COMPUTER & COMMUNICATIONS SECURITY (CCS 11), P125
   Chen J, 2017, SECUR COMMUN NETW, DOI 10.1155/2017/6898617
   Cheng ZH, 2019, IET INFORM SECUR, V13, P519, DOI 10.1049/iet-ifs.2018.5036
   CHEW M, P INT C INF SEC NOV, P268, DOI [DOI 10.1007/978-3-540-30144-8_23, 10.1007/978-3-540-30144-8_23]
   Datta R., 2005, 13th Annual ACM International Conference on Multimedia, P331, DOI 10.1145/1101149.1101218
   Dong H, 2017, IEEE I CONF COMP VIS, pCP1, DOI 10.1109/ICCV.2017.608
   Gao H., 2013, P 2013 ACM SIGSAC C, P1075, DOI DOI 10.1145/2508859.2516732
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Ince IF, 2008, THIRD 2008 INTERNATIONAL CONFERENCE ON CONVERGENCE AND HYBRID INFORMATION TECHNOLOGY, VOL 2, PROCEEDINGS, P596, DOI 10.1109/ICCIT.2008.195
   Kingma D. P., 2014, arXiv
   Kiros R, 2014, PR MACH LEARN RES, V32, P595
   Kuo-Feng Hwang, 2012, 2012 International Symposium on Biometrics and Security Technologies (ISBAST 2012), P1, DOI 10.1109/ISBAST.2012.17
   Kwon H, 2020, IEICE T INF SYST, VE103D, DOI 10.1587/transinf.2019EDL8194
   Kwon H, 2018, IEICE T INF SYST, VE101D, P543, DOI 10.1587/transinf.2017EDL8175
   Lipton Z. C., 2017, ARXIV170204782
   Liu F, 2018, IEEE INT SYM MULTIM, P192, DOI 10.1109/ISM.2018.000-9
   Makhzani A., 2015, ARXIV
   Malik, 2002, BREAKING VISUAL CAPT
   Mirza M., 2014, ARXIV PREPRINT ARXIV, P2672, DOI 10.48550/arXiv.1411.1784
   Park H., 2018, BRIT MACH VIS C BMVC
   Park K.W., 2019, INT WORKSH INF SEC A, P234
   Radford A., 2015, ARXIV
   Reed S, 2016, PR MACH LEARN RES, V48
   Reed S, 2016, PROC CVPR IEEE, P49, DOI 10.1109/CVPR.2016.13
   Salimans T, 2016, ADV NEUR IN, V29
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sivakorn S, 2016, 1ST IEEE EUROPEAN SYMPOSIUM ON SECURITY AND PRIVACY, P388, DOI 10.1109/EuroSP.2016.37
   Szegedy C, 2014, Arxiv, DOI [arXiv:1312.6199, DOI 10.1109/CVPR.2015.7298594]
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   von Ahn L, 2003, LECT NOTES COMPUT SC, V2656, P294
   Wah C, 2011, CALTECH UCSD BIRDS 2
   Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362
   Ye GX, 2018, PROCEEDINGS OF THE 2018 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY (CCS'18), P332, DOI 10.1145/3243734.3243754
   Zhang H, 2019, IEEE T PATTERN ANAL, V41, P1947, DOI 10.1109/TPAMI.2018.2856256
   Zhang H, 2017, IEEE I CONF COMP VIS, P5908, DOI 10.1109/ICCV.2017.629
   Zhu B, 2010, PROCEEDINGS OF THE 17TH ACM CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY (CCS'10), P187, DOI 10.1145/1866307.1866329
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 39
TC 4
Z9 4
U1 1
U2 20
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2022
VL 38
IS 3
BP 963
EP 975
DI 10.1007/s00371-021-02061-1
EA FEB 2021
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZQ8YX
UT WOS:000616038300001
DA 2024-07-18
ER

PT J
AU Kurt, M
AF Kurt, Murat
TI GenSSS: a genetic algorithm for measured subsurface scattering
   representation
SO VISUAL COMPUTER
LA English
DT Article
DE Subsurface scattering model; Genetic algorithm; Factorization; BSSRDF
   representation; Global illumination; Rendering
ID TRANSLUCENT; SPARSE; BRDFS
AB We present a novel genetic algorithm-based approach for the compact representation of heterogeneous, optically thick, translucent materials. Utilizing genetic optimization, we also find the best transformation to represent measured subsurface scattering data. We employ a factored subsurface scattering representation, based on a singular value decomposition (SVD), separately applying the SVD per-color channel of the transformed profiles. In order to achieve a compact, accurate representation, we perform this iteratively on the model errors. By allowing the number of iterations to be customized, our representation provides a mechanism to trade the visual quality possible against the level of compression achieved through our representation. We validate our approach by analyzing a range of real-world translucent materials, geometries and lighting conditions. For heterogeneous translucent materials, we further demonstrate that for the same level of compression, our method achieves greater visual accuracy than alternative techniques. Finally, we present an application of our factored representation, which can be used to convert heterogeneous materials into homogeneous material representations.
C1 [Kurt, Murat] Ege Univ, Int Comp Inst, Izmir, Turkey.
C3 Ege University
RP Kurt, M (corresponding author), Ege Univ, Int Comp Inst, Izmir, Turkey.
EM murat.kurt@ege.edu.tr
RI Kurt, Murat/AAF-9554-2020
OI Kurt, Murat/0000-0002-3236-5595
CR Arbree A, 2011, IEEE T VIS COMPUT GR, V17, P956, DOI 10.1109/TVCG.2010.117
   Bilgili A, 2011, COMPUT GRAPH FORUM, V30, P2427, DOI 10.1111/j.1467-8659.2011.02072.x
   Bowers J, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866188
   Brady A, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601193
   Chen GJ, 2012, VISUAL COMPUT, V28, P701, DOI 10.1007/s00371-012-0704-1
   d'Eon E, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964951
   dEon E., 2007, P EUR S REND TECHN, P147
   Donner C, 2005, ACM T GRAPHIC, V24, P1032, DOI 10.1145/1073204.1073308
   Fleming R.W., 2004, Proceedings of ACM Symposium on Applied Perception in Graphics and Visualization, P127, DOI DOI 10.1145/1012551.1012575
   Frisvad JR, 2014, ACM T GRAPHIC, V34, DOI 10.1145/2682629
   Goesele M, 2004, ACM T GRAPHIC, V23, P835, DOI 10.1145/1015706.1015807
   Goldberg David E, 1989, GENETIC ALGORITHMS S
   Guarnera D, 2020, IEEE T VIS COMPUT GR, V26, P2258, DOI 10.1109/TVCG.2018.2886877
   Gutierrez D., 2009, P 25 SPRING C COMP G, P39, DOI [10.1145/3230744, DOI 10.1145/3230744]
   Jakob W, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778790
   Jakob Wenzel, 2010, Mitsuba renderer
   Jensen HW, 2001, COMP GRAPH, P511, DOI 10.1145/383259.383319
   Jensen HW, 2002, ACM T GRAPHIC, V21, P576, DOI 10.1145/566570.566619
   Jimenez J, 2015, COMPUT GRAPH FORUM, V34, P188, DOI 10.1111/cgf.12529
   Jimenez J, 2010, IEEE COMPUT GRAPH, V30, P32, DOI 10.1109/MCG.2010.39
   Jimenez J, 2009, ACM T APPL PERCEPT, V6, DOI 10.1145/1609967.1609970
   Kajiya J. T., 1986, SIGGRAPH, P143, DOI 10.1145/15886.15902
   Kautz J, 1999, SPRING EUROGRAP, P247
   Kolda TG, 2009, SIAM REV, V51, P455, DOI 10.1137/07070111X
   Kurt M., 2013, TPCG, P85, DOI 10.2312/LocalChapterEvents.TPCG.TPCG13.085-092.URL
   Lawrence J, 2004, ACM T GRAPHIC, V23, P496, DOI 10.1145/1015706.1015751
   Lawrence J, 2006, ACM T GRAPHIC, V25, P735, DOI 10.1145/1141911.1141949
   McCool MD, 2001, COMP GRAPH, P171, DOI 10.1145/383259.383276
   Mertens T, 2005, COMPUT GRAPH FORUM, V24, P41, DOI 10.1111/j.1467-8659.2005.00827.x
   Mitchell M., 1999, INTRO GENETIC ALGORI, DOI DOI 10.1016/S0898-1221(96)90227-8
   Munoz A., 2009, PROC COMPUT GRAPH VI, P24, DOI [10.1145/3230744.3230761, DOI 10.1145/3230744.3230761]
   Nakamoto K, 2018, P ACM SIGGRAPH 2018
   NICODEMUS FE, 1977, MONOGRAPH NBS US
   Pajarola R, 2013, EG 2013 TUTORIALS, P16
   Peers P, 2006, ACM T GRAPHIC, V25, P746, DOI 10.1145/1141911.1141950
   Richardson IE., 2002, Video codec design: developing image and video compression systems, DOI [10.1002/0470847832, DOI 10.1002/0470847832]
   Ruiters R, 2012, COMPUT GRAPH FORUM, V31, P315, DOI 10.1111/j.1467-8659.2012.03010.x
   Ruiters R, 2009, COMPUT GRAPH FORUM, V28, P1181, DOI 10.1111/j.1467-8659.2009.01495.x
   Seidel H.P, 2005, ACM SIGGRAPH 2005 SK
   Sone H., 2017, EUROGRAPHICS SHORT P, P73
   Song Y., 2013, Automatic Face and Gesture Recognition (FG), 2013 10th IEEE International Conference and Workshops on, P1, DOI DOI 10.1109/APSIPA.2013.6694333
   Song Y, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531337
   Sun X, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239478
   Suter SK, 2011, IEEE T VIS COMPUT GR, V17, P2135, DOI 10.1109/TVCG.2011.214
   Tong X, 2005, ACM T GRAPHIC, V24, P1054, DOI 10.1145/1073204.1073311
   Vasilescu MAO, 2004, ACM T GRAPHIC, V23, P336, DOI 10.1145/1015706.1015725
   Wang HC, 2005, ACM T GRAPHIC, V24, P527, DOI 10.1145/1073204.1073224
   Xu K, 2007, COMPUT GRAPH FORUM, V26, P545, DOI 10.1111/j.1467-8659.2007.01077.x
   Yan LQ, 2012, COMPUT GRAPH FORUM, V31, P2267, DOI 10.1111/j.1467-8659.2012.03220.x
NR 49
TC 3
Z9 3
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2021
VL 37
IS 2
BP 307
EP 323
DI 10.1007/s00371-020-01800-0
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA QR4KJ
UT WOS:000625179800009
DA 2024-07-18
ER

PT J
AU Qian, K
   Wen, XL
   Song, AG
AF Qian, Kui
   Wen, Xiulan
   Song, Aiguo
TI Hybrid neural network model for large-scale heterogeneous classification
   tasks in few-shot learning
SO VISUAL COMPUTER
LA English
DT Article
DE Few-shot learning; Meta-learning model; Siamese GCN; Semi-supervised;
   Large-scale heterogeneous classification
AB How to generalize and unify different few-shot learning tasks using neural network model is a difficult problem in the field of machine learning research. Aiming at the problem that the parameters of existing few-shot learning models cannot adapt with heterogeneous classification tasks, inspired by the human being recognition process, a hybrid neural network (HNN) model for large-scale heterogeneous classification tasks in few-shot learning is proposed. First, a meta-learning model is constructed, which uses a siamese graph convolutional network (SGCN) structure as bone network. The SGCN is trained by semi-supervised way with a small amount of incomplete labeled data. Then, random task slicing by group is performed according to the task size and meta-learning dimensions to ensure that the segmented task size matches the meta-learning model. Combined with the meta-learning model, a task discrimination network and object recognition network are constructed, to perform heterogeneous classification tasks while keeping the scale of HNN network parameters unchanged. Experimental results show that the HNN performs well under different datasets, and is suitable for large-scale heterogeneous tasks in few-shot learning without retraining.
C1 [Qian, Kui; Wen, Xiulan] Nanjing Inst Technol, Sch Automat, Nanjing, Peoples R China.
   [Song, Aiguo] Southeast Univ, Sch Instrument Sci & Engn, Nanjing, Peoples R China.
C3 Nanjing Institute of Technology; Southeast University - China
RP Qian, K (corresponding author), Nanjing Inst Technol, Sch Automat, Nanjing, Peoples R China.
EM kuiqian@njit.edu.cn
FU Nanjing Institute of Technology High-level Scientific Research
   Foundation for the introduction of talent [YKJ201918]; Natural Science
   Foundation of the Jiangsu Higher Education Institutions of China
   [20KJB510049]; National Key R&D Program of China [2017YFB1002802];
   National Natural Science Foundation of China [51675259]
FX This paper was supported by Nanjing Institute of Technology High-level
   Scientific Research Foundation for the introduction of talent (No.
   YKJ201918) and the Natural Science Foundation of the Jiangsu Higher
   Education Institutions of China (No. 20KJB510049), partially supported
   by the National Key R&D Program of China (No.2017YFB1002802) and
   National Natural Science Foundation of China (No.51675259).
CR Al Jallad K, 2019, J BIG DATA-GER, V6, DOI 10.1186/s40537-019-0248-6
   Phan AV, 2018, NEURAL NETWORKS, V108, P533, DOI 10.1016/j.neunet.2018.09.001
   Blaes S, 2017, NEURAL NETWORKS, V94, P159, DOI 10.1016/j.neunet.2017.07.001
   Carneiro G, 2007, IEEE T PATTERN ANAL, V29, P394, DOI 10.1109/TPAMI.2007.61
   Dong XP, 2019, IEEE T IMAGE PROCESS, V28, P3516, DOI 10.1109/TIP.2019.2898567
   Dong XP, 2018, LECT NOTES COMPUT SC, V11217, P472, DOI 10.1007/978-3-030-01261-8_28
   Fan H, 2020, APPL INTELL, V4, P1
   Ji Z, 2020, PATTERN RECOGN LETT, V140, P81, DOI 10.1016/j.patrec.2020.07.015
   Jiang Liubing, 2019, Systems Engineering and Electronics, V41, P1210, DOI 10.3969/j.issn.1001-506X.2019.06.06
   Junbo Yin, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11492, DOI 10.1109/CVPR42600.2020.01151
   Kim J, 2019, PROC CVPR IEEE, P11, DOI 10.1109/CVPR.2019.00010
   Koch G., 2015, ICML DEEP LEARNING W, V2
   Lake B., 2011, P ANN M COGNITIVE SC, V33, P1
   Li XH, 2014, IEEE T GEOSCI REMOTE, V52, P7086, DOI 10.1109/TGRS.2014.2307354
   Liang ZY, 2020, IEEE T IMAGE PROCESS, V29, P3351, DOI 10.1109/TIP.2019.2959256
   Ma YH, 2020, PATTERN RECOGN LETT, V133, P48, DOI 10.1016/j.patrec.2020.02.007
   Makantasis K, 2015, INT GEOSCI REMOTE SE, P4959, DOI 10.1109/IGARSS.2015.7326945
   Mohammadi M, 2018, IEEE COMMUN SURV TUT, V20, P2923, DOI 10.1109/COMST.2018.2844341
   Peng ZM, 2019, IEEE I CONF COMP VIS, P441, DOI 10.1109/ICCV.2019.00053
   Qi SY, 2018, LECT NOTES COMPUT SC, V11213, P407, DOI 10.1007/978-3-030-01240-3_25
   [钱夔 Qian Kui], 2015, [电子学报, Acta Electronica Sinica], V43, P1084
   Rahman S, 2018, IEEE T IMAGE PROCESS, V27, P5652, DOI 10.1109/TIP.2018.2861573
   Rusk N, 2016, NAT METHODS, V13, P35, DOI 10.1038/nmeth.3707
   Santoro A., 2016, INT C MACH LEARN, P1842, DOI DOI 10.5555/3045390.3045585
   Satorras V.G., 2018, ICLR
   Scarselli F, 2009, IEEE T NEURAL NETWOR, V20, P81, DOI 10.1109/TNN.2008.2005141
   Shen JB, 2020, IEEE T CYBERNETICS, V50, P3068, DOI 10.1109/TCYB.2019.2936503
   Snell J, 2017, ADV NEUR IN, V30
   Sun QR, 2019, PROC CVPR IEEE, P403, DOI 10.1109/CVPR.2019.00049
   Sung F, 2018, PROC CVPR IEEE, P1199, DOI 10.1109/CVPR.2018.00131
   Tang H, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P610, DOI 10.1145/3394171.3413884
   Tian SJ, 2020, VISUAL COMPUT, V36, P1219, DOI 10.1007/s00371-019-01730-6
   Vinyals O, 2016, 30 C NEURAL INFORM P, V29
   Wang D, 2019, NEUROCOMPUTING, V349, P202, DOI 10.1016/j.neucom.2019.03.085
   Wang WG, 2019, IEEE I CONF COMP VIS, P9235, DOI 10.1109/ICCV.2019.00933
   Wu H, 2015, VISUAL COMPUT, V31, P367, DOI 10.1007/s00371-014-0931-8
   [余游 Yu You], 2019, [电子学报, Acta Electronica Sinica], V47, P2284
   Zhang LL, 2020, PATTERN RECOGN, V108, DOI 10.1016/j.patcog.2020.107348
NR 38
TC 3
Z9 3
U1 0
U2 26
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2022
VL 38
IS 2
BP 719
EP 728
DI 10.1007/s00371-020-02046-6
EA JAN 2021
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZE7IA
UT WOS:000605494100002
DA 2024-07-18
ER

PT J
AU Sbei, A
   ElBedoui, K
   Barhoumi, W
   Maktouf, C
AF Sbei, Arafet
   ElBedoui, Khaoula
   Barhoumi, Walid
   Maktouf, Chokri
TI Adaptive feature selection in PET scans based on shared information and
   multi-label learning
SO VISUAL COMPUTER
LA English
DT Article
DE Feature selection; Shared information; Treatment outcome prediction;
   Global and local label correlation; Data imbalance
ID OUTCOME PREDICTION; ESOPHAGEAL CANCER; TEXTURAL FEATURES; FDG-PET;
   IMAGES; CLASSIFICATION; SEGMENTATION
AB Feature selection for tumor treatment outcome prediction in PET scans amounts to determine the best predictors in order to classify treatment outcomes for unseen data. Several challenges have to be addressed, notably the dissensus about the most predictive radiomic features and the relatively small-sized and imbalanced training samples. To overcome these issues, we propose a multi-label learning (MLL) problem formulation that we associated to the technique of feature selection with shared information among multiple tasks. In fact, we opt to simultaneously mining the shared knowledge across similar tasks from MRI and fused PET/MRI images in order to sort PET features according to their relevancy. Then, by repeatedly training an SVM model, different feature sets are elected to be informative for each iteration. Thus, by associating these sets with their correspondents in other iterations, the MLL formulation is performed in order to adequately adapt both feature sets and training instances according to their fit with unseen data. The adaptive synthetic sampling technique is adapted to the framework of MLL data balance. After that, the global and local label correlation technique is used for multi-labeling, while taking into consideration the capacity of MLL on recommending the best sets of features and training instances. Finally, an SVM classifier is applied with both selected features and training sub-samples in order to output prediction results. Conducted experiments show that attributing each feature set to a particular set of instances and training the model with balanced subset of the whole training data allows to outperform relevant methods from the state of the art.
C1 [Sbei, Arafet; ElBedoui, Khaoula; Barhoumi, Walid] Univ Tunis El Manar, Res Team Intelligent Syst Imaging & Artificial Vi, Lab Rech Informat Modelisat & Traitement Informat, Inst Super Informat, 2 Rue Bayrouni, Ariana 2080, Tunisia.
   [ElBedoui, Khaoula; Barhoumi, Walid] Univ Carthage, Ecole Natl Ingn Carthage, 45 Rue Entrepreneurs, Tunis 2035, Tunisia.
   [Maktouf, Chokri] Pasteur Inst Tunis, Nucl Med Dept, Tunis, Tunisia.
C3 Universite de Tunis-El-Manar; Universite de Carthage; Pasteur Network;
   Universite de Tunis-El-Manar; Institut Pasteur Tunis
RP Barhoumi, W (corresponding author), Univ Tunis El Manar, Res Team Intelligent Syst Imaging & Artificial Vi, Lab Rech Informat Modelisat & Traitement Informat, Inst Super Informat, 2 Rue Bayrouni, Ariana 2080, Tunisia.; Barhoumi, W (corresponding author), Univ Carthage, Ecole Natl Ingn Carthage, 45 Rue Entrepreneurs, Tunis 2035, Tunisia.
EM walid.barhoumi@enicarthage.rnu.tn
RI Barhoumi, Walid/C-6576-2014
OI Barhoumi, Walid/0000-0003-2123-4992; ElBedoui,
   Khaoula/0000-0001-9687-720X
CR AMADASUN M, 1989, IEEE T SYST MAN CYB, V19, P1264, DOI 10.1109/21.44046
   Amyar A, 2019, IEEE T RADIAT PLASMA, V3, P225, DOI 10.1109/TRPMS.2019.2896399
   Boutell MR, 2004, PATTERN RECOGN, V37, P1757, DOI 10.1016/j.patcog.2004.03.009
   Chawla NV, 2002, J ARTIF INTELL RES, V16, P321, DOI 10.1613/jair.953
   Chen X, 2009, PROCEEDINGS OF THE 3RD INTERNATIONAL CONFERENCE ON ANTI-COUNTERFEITING, SECURITY, AND IDENTIFICATION IN COMMUNICATION, P124, DOI 10.1109/ICASID.2009.5276941
   Clark K, 2013, J DIGIT IMAGING, V26, P1045, DOI 10.1007/s10278-013-9622-7
   Comelli A, 2020, COMM COM INF SC, V1248, P280, DOI 10.1007/978-3-030-52791-4_22
   DARIO G, 2019, CURR PROBL DIAGN RAD
   Desbordes P, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0173208
   El Naqa I, 2009, PATTERN RECOGN, V42, P1162, DOI 10.1016/j.patcog.2008.08.011
   Fürnkranz J, 2008, MACH LEARN, V73, P133, DOI 10.1007/s10994-008-5064-8
   Galloway MM., 1975, COMPUTER GRAPHICS IM, V4, P172, DOI DOI 10.1016/S0146-664X(75)80008-6
   Gillies RJ, 2016, RADIOLOGY, V278, P563, DOI 10.1148/radiol.2015151169
   Guo Z, 2019, IEEE T RADIAT PLASMA, V3, P162, DOI [10.1109/TRPMS.2018.2890359, 10.1109/trpms.2018.2890359]
   HARALICK RM, 1973, IEEE T SYST MAN CYB, VSMC3, P610, DOI 10.1109/TSMC.1973.4309314
   He HB, 2008, IEEE IJCNN, P1322, DOI 10.1109/IJCNN.2008.4633969
   Hermessi H, 2019, EXPERT SYST APPL, V120, P116, DOI 10.1016/j.eswa.2018.11.025
   Huang SM, 2012, INT CONF ACOUST SPEE, P1945, DOI 10.1109/ICASSP.2012.6288286
   Jung Y, 2013, VISUAL COMPUT, V29, P805, DOI 10.1007/s00371-013-0833-1
   KIRA K, 1992, AAAI-92 PROCEEDINGS : TENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE, P129
   Kuanar S, 2019, IEEE IMAGE PROC, P1351, DOI [10.1109/ICIP.2019.8803037, 10.1109/icip.2019.8803037]
   Kuanar S, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P2576, DOI 10.1109/ICASSP.2018.8462243
   Lee PY, 2017, IMAGE VISION COMPUT, V67, P29, DOI 10.1016/j.imavis.2017.09.004
   Li XH, 2014, IEEE T GEOSCI REMOTE, V52, P7086, DOI 10.1109/TGRS.2014.2307354
   Lian CF, 2015, I S BIOMED IMAGING, P63, DOI 10.1109/ISBI.2015.7163817
   Lian CF, 2016, MED IMAGE ANAL, V32, P257, DOI 10.1016/j.media.2016.05.007
   Lian CF, 2015, PATTERN RECOGN, V48, P2318, DOI 10.1016/j.patcog.2015.01.019
   Mi HM, 2015, ARTIF INTELL MED, V64, P195, DOI 10.1016/j.artmed.2015.07.002
   Niedzielski JS, 2016, INT J RADIAT ONCOL, V96, P670, DOI 10.1016/j.ijrobp.2016.07.012
   Obozinski G, 2010, STAT COMPUT, V20, P231, DOI 10.1007/s11222-008-9111-x
   Paul D, 2017, COMPUT MED IMAG GRAP, V60, P42, DOI 10.1016/j.compmedimag.2016.12.002
   PUDIL P, 1994, PATTERN RECOGN LETT, V15, P1119, DOI 10.1016/0167-8655(94)90127-9
   Read J, 2011, MACH LEARN, V85, P333, DOI 10.1007/s10994-011-5256-5
   Sahran S, 2018, ARTIF INTELL MED, V87, P78, DOI 10.1016/j.artmed.2018.04.002
   Sanchez A, 2021, VISUAL COMPUT, V37, P203, DOI 10.1007/s00371-020-01793-w
   Sathish D, 2019, VISUAL COMPUT, V35, P57, DOI 10.1007/s00371-017-1447-9
   Sbei A, 2020, COMPUT BIOL MED, V119, DOI 10.1016/j.compbiomed.2020.103669
   Sbei A, 2017, COMPUT METH PROG BIO, V149, P29, DOI 10.1016/j.cmpb.2017.07.006
   Shafer Glenn, 1976, MATH THEORY EVIDENCE, P2
   Somol P, 2010, IEEE T PATTERN ANAL, V32, P1921, DOI 10.1109/TPAMI.2010.34
   Suk HI, 2016, BRAIN STRUCT FUNCT, V221, P2569, DOI 10.1007/s00429-015-1059-y
   Tan S, 2013, MED PHYS, V40, DOI 10.1118/1.4820445
   Tan S, 2013, INT J RADIAT ONCOL, V85, P1375, DOI 10.1016/j.ijrobp.2012.10.017
   Tibshirani R, 1996, J ROY STAT SOC B, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x
   Tixier F, 2011, J NUCL MED, V52, P369, DOI 10.2967/jnumed.110.082404
   Vallières M, 2015, PHYS MED BIOL, V60, P5471, DOI 10.1088/0031-9155/60/14/5471
   Van de Wiele C, 2013, EUR J NUCL MED MOL I, V40, P290, DOI 10.1007/s00259-012-2280-z
   Wang H, 2011, IEEE I CONF COMP VIS, P557, DOI 10.1109/ICCV.2011.6126288
   Wang S, 2016, SIGNAL PROCESS, V120, P746, DOI 10.1016/j.sigpro.2014.12.012
   WHITNEY AW, 1971, IEEE T COMPUT, VC 20, P1100, DOI 10.1109/T-C.1971.223410
   Wu J, 2019, IEEE T RADIAT PLASMA, V3, P216, DOI 10.1109/TRPMS.2018.2872406
   Yang Y, 2013, IEEE T MULTIMEDIA, V15, P661, DOI 10.1109/TMM.2012.2237023
   Yang Y, 2012, IEEE T IMAGE PROCESS, V21, P1339, DOI 10.1109/TIP.2011.2169269
   Zhang ML, 2007, PATTERN RECOGN, V40, P2038, DOI 10.1016/j.patcog.2006.12.019
   Zhang ML, 2014, IEEE T KNOWL DATA EN, V26, P1819, DOI 10.1109/TKDE.2013.39
   Zhu Y, 2018, IEEE T KNOWL DATA EN, V30, P1081, DOI 10.1109/TKDE.2017.2785795
NR 56
TC 2
Z9 2
U1 2
U2 11
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2022
VL 38
IS 1
BP 257
EP 277
DI 10.1007/s00371-020-02014-0
EA JAN 2021
PG 21
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA YW6DE
UT WOS:000604490300001
DA 2024-07-18
ER

PT J
AU Ghosh, S
   Chatterjee, A
   Singh, PK
   Bhowmik, S
   Sarkar, R
AF Ghosh, Soulib
   Chatterjee, Agneet
   Singh, Pawan Kumar
   Bhowmik, Showmik
   Sarkar, Ram
TI Language-invariant novel feature descriptors for handwritten numeral
   recognition
SO VISUAL COMPUTER
LA English
DT Article
DE Point-Light Source-based Shadow (PLSS); Histogram of Oriented Pixel
   Positions (HOPP); Numeral recognition; CMATERdb; Shape-based feature;
   Handwriting recognition
ID CLASSIFIER COMBINATION; CHARACTER; ONLINE
AB Numeral recognition is treated as a benchmark research problem as this is a basic module for designing a comprehensive optical character recognition system. In this context, unconstrained handwritten numeral recognition is still considered as an open research problem. Most of the feature descriptors found in the literature for the said problem, work well for numeral images written in a particular language. To encounter this shortcoming, in this paper, we have proposed two shape-based feature descriptors, namely Point-Light Source-based Shadow (PLSS) and Histogram of Oriented Pixel Positions (HOPP). We have evaluated the proposed feature descriptors on 10 (9 offline and 1 online) publicly available standard handwritten numeral image datasets written in eight different languages. Besides, to prove the usefulness of the descriptors in real-life scenario, we have considered numeral string images also. We have also shown how the proposed feature descriptors are invariant toward broken, noisy and rotated numeral images. Experimental outcomes soundly prove that the proposed feature descriptors have the ability to estimate the shape of a numeral image almost accurately irrespective of the language in which it is written. Comparison of the proposed feature descriptors with other shape-based as well as texture-based features shows that PLSS and HOPP produce the results which are analogous to state of the art. The code of the proposed feature descriptors can be found at-https://github.com/ghoshsoulib/Numeral-Recognition.
C1 [Ghosh, Soulib; Chatterjee, Agneet; Sarkar, Ram] Jadavpur Univ, Dept Comp Sci & Engn, Kolkata 700032, India.
   [Singh, Pawan Kumar] Jadavpur Univ, Dept Informat Technol, Kolkata 700106, India.
   [Bhowmik, Showmik] Ghani Khan Choudhury Inst Engn & Technol, Dept Comp Sci & Engn, Malda 732141, India.
C3 Jadavpur University; Jadavpur University
RP Bhowmik, S (corresponding author), Ghani Khan Choudhury Inst Engn & Technol, Dept Comp Sci & Engn, Malda 732141, India.
EM ghoshsoulib@gmail.com; agneet257@gmail.com; pawansingh.ju@gmail.com;
   showmik.cse@gmail.com; raamsarkar@gmail.com
RI SINGH, PAWAN KUMAR/E-3408-2013; Sarkar, Ram/AAX-3822-2020; Bhowmik,
   Showmik/M-4248-2017
OI SINGH, PAWAN KUMAR/0000-0002-9598-7981; Sarkar, Ram/0000-0001-8813-4086;
   GHOSH, SOULIB/0000-0001-6458-8681; Bhowmik, Showmik/0000-0003-3971-5807
CR ACHARYA S, 2015, 2015 9 INT C SOFTW, pNI182
   Ahmed M, 2016, INT CONF INFORM COMM, P310, DOI [10.1109/ICT4M.2016.069, 10.1109/ICT4M.2016.62]
   Alani AA, 2017, INFORMATION, V8, DOI 10.3390/info8040142
   Albregtsen F., 2008, STAT TEXTURE MEASURE, P1
   Albregtsen F, 1995, STAT TEXTURE MEASURE
   Alom MZ, 2018, COMPUT INTEL NEUROSC, V2018, DOI 10.1155/2018/6747098
   Amit, 2020, VISUAL COMPUT, V36, P579, DOI 10.1007/s00371-019-01643-4
   [Anonymous], 2018, J U BABYLON
   [Anonymous], 2012, ASIAN HIMAL INT CONF
   Ashiquzzaman A., 2017, 2017 IEEE INT C IMAG, P1
   Aziz T.I., 2018, 20 INT C COMP INF TE, V2018, P1
   Baruah U, 2015, J INF PROCESS SYST, V11, P325, DOI 10.3745/JIPS.02.0008
   Basu S, 2005, LECT NOTES COMPUT SC, V3776, P236
   Basu S., 2005, P 2 IND INT C ART IN, P407
   Basu S, 2009, LECT NOTES COMPUT SC, V5909, P381, DOI 10.1007/978-3-642-11164-8_62
   Bhowmik S., 2017, COMPUTER COMMUNICATI, P15
   Bhowmik S, 2019, NEURAL COMPUT APPL, V31, P5783, DOI 10.1007/s00521-018-3389-1
   Bhowmik S, 2015, INT J APPL PATTERN R, V2, P142, DOI 10.1504/IJAPR.2015.069539
   Bhowmik S, 2014, 2014 6TH INTERNATIONAL CONFERENCE ON COMPUTATIONAL INTELLIGENCE AND COMMUNICATION NETWORKS, P257, DOI 10.1109/CICN.2014.66
   Bhowmik S, 2014, PROC INT CONF EMERG, P193, DOI 10.1109/EAIT.2014.43
   Chatterjee A, 2018, PROC INT CONF EMERG
   Dalal N., 2005, PROC IEEE COMPUT SOC, V1, P886
   Diem M, 2014, INT CONF FRONT HAND, P779, DOI 10.1109/ICFHR.2014.136
   Dongre V. J., 2013, INT J COMPUTER SCI E, V5, P856
   Dongre VJ, 2015, 2015 2ND INTERNATIONAL CONFERENCE ON COMPUTING FOR SUSTAINABLE GLOBAL DEVELOPMENT (INDIACOM), P425
   Dongre VJ, 2012, APPL COMPUT INTELL S, V2012, DOI 10.1155/2012/871834
   Gattal A, 2017, ACM PROCEEDINGS OF INTERNATIONAL CONFERENCE OF COMPUTING FOR ENGINEERING AND SCIENCE (ICCES'17), P13, DOI 10.1145/3129186.3129189
   Gattal A, 2016, PROCEEDINGS OF 12TH IAPR WORKSHOP ON DOCUMENT ANALYSIS SYSTEMS, (DAS 2016), P305, DOI 10.1109/DAS.2016.10
   Ghosh KK, 2020, MED BIOL ENG COMPUT, V58, P1723, DOI 10.1007/s11517-020-02194-w
   Ghosh Manosij, 2018, Intelligent Engineering Informatics. Proceedings of the 6th International Conference on FICTA. Advances in Intelligent Systems and Computing (AISC 695), P471, DOI 10.1007/978-981-10-7566-7_46
   Ghosh S, 2018, J IMAGING, V4, DOI 10.3390/jimaging4040057
   Guha R, 2020, J INTELL SYST, V29, P1453, DOI 10.1515/jisys-2019-0064
   Hamamoto Y, 1998, PATTERN RECOGN, V31, P395, DOI 10.1016/S0031-3203(97)00057-5
   Hammouda G, 2020, VISUAL COMPUT, V36, P279, DOI 10.1007/s00371-018-1604-9
   Hangarge M., 2011, INT J COMPUTER APPL, V26, P11
   Hassan T, 2015, INT CONF ELECTR ENG
   Inunganbi S, 2021, VISUAL COMPUT, V37, P291, DOI 10.1007/s00371-020-01799-4
   Jana P, 2017, 2017 NINTH INTERNATIONAL CONFERENCE ON ADVANCES IN PATTERN RECOGNITION (ICAPR), P332
   Jyothi B., 2015, INDIAN J SCI TECHNOL, V8
   Karthik S, 2015, ADV INTELL SYST, V338, P51, DOI 10.1007/978-3-319-13731-5_7
   Kusetogullari H, 2020, NEURAL COMPUT APPL, V32, P16505, DOI 10.1007/s00521-019-04163-3
   Li CC, 2015, VISUAL COMPUT, V31, P1419, DOI 10.1007/s00371-014-1023-5
   Malakar, 2019, COMMUN COMPUT INF SC, V1020, P27, DOI DOI 10.1007/978-981-13-9361-7_3
   Malakar S, 2020, NEURAL COMPUT APPL, V32, P2533, DOI 10.1007/s00521-018-3937-8
   Mandal A, 2018, INT C EM TECHN SUST
   Mandal B, 2018, PROCEEDINGS OF 2018 IEEE APPLIED SIGNAL PROCESSING CONFERENCE (ASPCON), P304, DOI 10.1109/ASPCON.2018.8748550
   Mollah AF, 2006, P NAT C REC TRENDS I, P200
   Naito T, 2000, IEEE T VEH TECHNOL, V49, P2309, DOI 10.1109/25.901900
   Pal U, 2007, PROC INT CONF DOC, P749
   Plamondon R, 2000, IEEE T PATTERN ANAL, V22, P63, DOI 10.1109/34.824821
   Pusalkar M, 2016, INT J NEUROPSYCHOPH, V19, DOI 10.1093/ijnp/pyw040
   Rajashekararadhya S.V., 2008, IEEE REG 10 ANN INT
   Roy A, 2014, ADV INTELL SYST, V248, P133, DOI 10.1007/978-3-319-03107-1_16
   Saha C., 2019, 2019 INT C ELECT COM, P1
   Sahare P, 2018, IEEE ACCESS, V6, P10603, DOI 10.1109/ACCESS.2018.2795104
   Sarkhel R, 2017, PATTERN RECOGN, V71, P78, DOI 10.1016/j.patcog.2017.05.022
   Sarkhel R, 2016, PATTERN RECOGN, V58, P172, DOI 10.1016/j.patcog.2016.04.010
   Sen S, 2019, COMMUN COMPUT INF SC, V1020, P65
   Sen S, 2018, ACM T ASIAN LOW-RESO, V17, DOI 10.1145/3178457
   Sharma D., 2010, Int J Comput Appl, V4, P9, DOI DOI 10.5120/850-1188
   Singh P, 2016, STUD FUZZ SOFT COMP, V330, P1, DOI 10.1155/2016/2796863
   Singh Pritpal, 2012, International Journal of Modern Education and Computer Science, V4, P34, DOI 10.5815/ijmecs.2012.08.05
   Singh P.K., 2017, 2016 INT C COMP EL C
   Singh PK, 2018, COMPUT INTELL-US, V34, P839, DOI 10.1111/coin.12135
   Singh PK, 2017, ADV INTELL SYST, V458, P459, DOI 10.1007/978-981-10-2035-3_47
   Singh PK, 2016, INT J COMPUT SCI MAT, V7, P410, DOI 10.1504/IJCSM.2016.080073
   Singh PK, 2018, INT J COMPUT VIS ROB, V8, P5
   Singh P, 2015, SADHANA-ACAD P ENG S, V40, P1701, DOI 10.1007/s12046-015-0419-x
   Takruri M., 2015, INT J APPL ENG RES, V10, P1911
   Vamvakas G, 2010, PATTERN RECOGN, V43, P2807, DOI 10.1016/j.patcog.2010.02.018
   Zhan HJ, 2017, LECT NOTES COMPUT SC, V10639, P583, DOI 10.1007/978-3-319-70136-3_62
NR 71
TC 14
Z9 14
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2021
VL 37
IS 7
BP 1781
EP 1803
DI 10.1007/s00371-020-01938-x
EA AUG 2020
PG 23
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SZ5DG
UT WOS:000555755400001
DA 2024-07-18
ER

PT J
AU Zou, QF
   Liu, LG
   Liu, Y
AF Zou, Qian-Fang
   Liu, Ligang
   Liu, Yang
TI Instance-level 3D shape retrieval from a single image by
   hybrid-representation-assisted joint embedding
SO VISUAL COMPUTER
LA English
DT Article
DE Single image 3D shape retrieval; Joint embedding; 3D representation
AB We present a novel and effective joint embedding approach for retrieving the most similar 3D shape for a single image query. Our approach builds upon hybrid 3D representations-the octree-based representation and the multi-view image representation, which characterize shape geometry in different ways. We first pre-train a 3D feature space via jointly embedding 3D shapes with hybrid representations and then introduce a transform layer and an image encoder to map both shape codes and real images into a common space via a second joint embedding. Our pre-training benefits from the hybrid representation of 3D shapes and builds a more discriminative 3D shape space than using any one of 3D representations only. The transform layer helps to mind the gap between the 3D shape space and the real image space. We validate the efficacy of our method on the instance-level single-image 3D retrieval task and achieve significant improvements over existing methods.
C1 [Zou, Qian-Fang] Univ Sci & Technol China, Hefei, Peoples R China.
   [Liu, Ligang] Univ Sci & Technol China, Sch Math Sci, Hefei, Peoples R China.
   [Liu, Yang] Microsoft Res Asia, Beijing, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Chinese Academy of Sciences; University of Science &
   Technology of China, CAS; Microsoft Research Asia; Microsoft
RP Liu, Y (corresponding author), Microsoft Res Asia, Beijing, Peoples R China.
EM zou@mail.ustc.edu.cn; lgliu@ustc.edu.cn; yangliu@microsoft.com
RI Liu, Yang/ABD-2239-2020
OI Liu, Yang/0000-0002-3768-6654
CR Abadi Martin, 2016, TENSORFLOW LARGE SCA, V16, P265
   [Anonymous], 2015, SHAPENET INFORM RICH
   [Anonymous], 2016, ARXIV160705695CSCV
   [Anonymous], 2015, PROC CVPR IEEE, DOI [DOI 10.1109/CVPR.2015.7298801, 10.1109/CVPR.2015.7298801]
   [Anonymous], 2017, CVPR, DOI DOI 10.1109/CVPR.2017.693
   [Anonymous], 2016, NIPS 16 P 30 INT C N, DOI DOI 10.5555/3157096.3157304
   Bai S, 2017, IEEE T MULTIMEDIA, V19, P1257, DOI 10.1109/TMM.2017.2652071
   Bai S, 2016, PROC CVPR IEEE, P5023, DOI 10.1109/CVPR.2016.543
   Bennamoun, 2019, 3D Shape Analysis: Fundamentals, Theory and Application
   Boscaini D, 2016, COMPUT GRAPH FORUM, V35, P431, DOI 10.1111/cgf.12844
   Boscaini D., 2016, ADV NEURAL INFORM PR, P3197
   Bromley J., 1993, International Journal of Pattern Recognition and Artificial Intelligence, V7, P669, DOI 10.1142/S0218001493000339
   Bronstein AM, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1899404.1899405
   Buko B., 2022, ABS151203385 CORR, V22, P8878
   Chen K., 2018, P ASIAN C COMPUTER V, P100
   Chopra S, 2005, PROC CVPR IEEE, P539, DOI 10.1109/cvpr.2005.202
   Eitz M, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185540
   Feng YF, 2018, PROC CVPR IEEE, P264, DOI 10.1109/CVPR.2018.00035
   Grabner A., 2018, COMPUTER VISION PATT
   Gu JX, 2018, PROC CVPR IEEE, P7181, DOI 10.1109/CVPR.2018.00750
   Han Z., 2018, ARXIV181102745CSCV
   He XW, 2018, PROC CVPR IEEE, P1945, DOI 10.1109/CVPR.2018.00208
   Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lee T, 2018, INT CONF 3D VISION, P258, DOI 10.1109/3DV.2018.00038
   Li DW, 2018, NEURAL NETWORKS, V105, P447, DOI 10.1016/j.neunet.2018.06.003
   Li W., 2019, 12 EG WORKSH 3D OBJ
   Li YY, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818071
   Li Y, 2015, ADV SOC BEHAV SCI, V12, P82
   Lin K.Z., 2018, ARXIV181209899CSCV
   Liu HY, 2016, PROC CVPR IEEE, P2167, DOI 10.1109/CVPR.2016.238
   Ma L, 2015, IEEE I CONF COMP VIS, P2623, DOI 10.1109/ICCV.2015.301
   Maturana D, 2015, IEEE INT C INT ROBOT, P922, DOI 10.1109/IROS.2015.7353481
   Muralikrishnan S., 2019, COMPUTER VISION PATT
   Peng YX, 2018, IEEE T MULTIMEDIA, V20, P405, DOI 10.1109/TMM.2017.2742704
   Qi A., 2018, BRIT MACH VIS C BMVC
   Qi C. R., 2017, Advances in neural information processing systems, P5099
   Qi CR, 2016, PROC CVPR IEEE, P5648, DOI 10.1109/CVPR.2016.609
   Qi JW, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P528, DOI 10.1145/3240508.3240558
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Savva M., 2017, EUR WORKSH 3D OBJ
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Shin D., 2018, COMPUTER VISION PATT
   Song HO, 2016, PROC CVPR IEEE, P4004, DOI 10.1109/CVPR.2016.434
   Su H, 2015, IEEE I CONF COMP VIS, P945, DOI 10.1109/ICCV.2015.114
   Su Yu- Ting, 2019, IEEE T CIRCUITS SYST, V2019
   Tangelder JH, 2008, MULTIMED TOOLS APPL, V39, P441, DOI 10.1007/s11042-007-0181-0
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wang F, 2015, PROC CVPR IEEE, P1875, DOI 10.1109/CVPR.2015.7298797
   Wang PS, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073608
   Wang Peng-Shuai, 2018, ACM T GRAPH SIGGRAPH, V37, P6
   Weinberger KQ, 2009, J MACH LEARN RES, V10, P207
   Xiang Y, 2016, LECT NOTES COMPUT SC, V9912, P160, DOI 10.1007/978-3-319-46484-8_10
   Xiang Y, 2014, IEEE WINT CONF APPL, P75, DOI 10.1109/WACV.2014.6836101
   Xiao JX, 2010, PROC CVPR IEEE, P3485, DOI 10.1109/CVPR.2010.5539970
   Xie J, 2017, IEEE T PATTERN ANAL, V39, P1335, DOI 10.1109/TPAMI.2016.2596722
   Zhang Z., 2019, ACM T GRAPH P ACCEPT
   Zhou HY, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1667, DOI 10.1145/3343031.3351011
   Zhu F, 2016, AAAI CONF ARTIF INTE, P3683
NR 59
TC 2
Z9 2
U1 1
U2 13
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2021
VL 37
IS 7
BP 1743
EP 1756
DI 10.1007/s00371-020-01935-0
EA JUL 2020
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SZ5DG
UT WOS:000554347800001
DA 2024-07-18
ER

PT J
AU Wang, XC
   Hu, K
   Hu, JP
   Du, L
   Ho, ATS
   Qin, H
AF Wang, Xiaochao
   Hu, Kun
   Hu, Jianping
   Du, Ling
   Ho, Anthony T. S.
   Qin, Hong
TI Robust and blind image watermarking via circular embedding and
   bidimensional empirical mode decomposition
SO VISUAL COMPUTER
LA English
DT Article
DE Image watermarking; Circular embedding; BEMD; Arnold transform; Hilbert
   curve
ID SINGULAR-VALUE DECOMPOSITION; TRANSFORM; DOMAIN; COMPUTATION; ALGORITHM;
   RECOVERY; SCHEME; SYSTEM; EMD
AB In this paper, a robust and blind image watermarking algorithm via circular embedding and bidimensional empirical mode decomposition (BEMD) is developed. First, the watermark image is scrambled by Arnold transform to increase the security of the algorithm. Second, the Hilbert curve is adopted to reduce the scrambled 2D watermark image to one-dimensional watermark signal. Third, the host image is decomposed by BEMD to obtain the multi-scale representation in the forms of intrinsic mode functions (IMFs) and a residue. Then, the extreme points of the IMFs are extracted as the embedding locations. Finally, the one-dimensional watermark signal is repeatedly and cyclically embedded in the extreme locations of the first IMF according to the texture masking characteristics of the human visual system, which greatly improves the ability of our algorithm against various attacks. The final watermarked image is reconstructed by combining the modified first IMF and the residual. The watermark can be successfully extracted without resorting to the original host image. Furthermore, image correction can be applied before image watermarking extraction if there are geometric attacks in watermarked image. A large number of experimental results and thorough evaluations confirm that our method can obtain higher imperceptibility and robustness under different types of attacks, and achieve better performance than the current state-of-the-art watermarking algorithms, especially in large-scale cropping attack, JPEG compression, Gaussian noise, sharpening, Gamma correction, scaling, histogram equalization, and rotation attacks.
C1 [Wang, Xiaochao; Du, Ling] Tiangong Univ, Sch Math Sci, Tianjin 300387, Peoples R China.
   [Hu, Kun] Univ Chinese Acad Sci, Beijing 100049, Peoples R China.
   [Hu, Jianping] Northeast Elect Power Univ, Sch Sci, Jilin 132012, Jilin, Peoples R China.
   [Ho, Anthony T. S.] Univ Surrey, Dept Comp Sci, Guildford GU2 7XH, Surrey, England.
   [Qin, Hong] SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11794 USA.
C3 Tiangong University; Chinese Academy of Sciences; University of Chinese
   Academy of Sciences, CAS; Northeast Electric Power University;
   University of Surrey; State University of New York (SUNY) System; State
   University of New York (SUNY) Stony Brook
RP Hu, JP (corresponding author), Northeast Elect Power Univ, Sch Sci, Jilin 132012, Jilin, Peoples R China.
EM wangxiaochao18@163.com; ucas_hukun@163.com; neduhjp307@163.com;
   duling@tiangong.edu.cn; a.ho@surrey.ac.uk; qin@cs.stonybrook.edu
RI Hu, Jianping/IWM-3698-2023
FU NationalNatural Science Foundation ofChina [IIS-1715985, IIS-1812606];
   Natural Science Foundation of Tianjin [61672149, 61602341, 61672077,
   61532002, 61602344, 61802279, 61872347]; Science & Technology
   Development Fund of Tianjin Education Commission for Higher Education
   [18JCQNJC00100];  [2018KJ222]
FX This research is supported in part byNational Science Foundation
   ofUSA(IIS-1715985 and IIS-1812606), NationalNatural Science Foundation
   ofChina (No. 61672149, 61602341, 61672077, 61532002, 61602344, 61802279,
   61872347); Natural Science Foundation of Tianjin (18JCQNJC00100). The
   Science & Technology Development Fund of Tianjin Education Commission
   for Higher Education (Grant No. 2018KJ222).
CR Aarab, 2009, 5 INT C SCI EL TECH, P22
   Abbas NH, 2018, MULTIMED TOOLS APPL, V77, P24593, DOI 10.1007/s11042-017-5488-x
   Ali M, 2015, INFORM SCIENCES, V301, P44, DOI 10.1016/j.ins.2014.12.042
   Baluja S, 2020, IEEE T PATTERN ANAL, V42, P1685, DOI 10.1109/TPAMI.2019.2901877
   Barni M, 1998, SIGNAL PROCESS, V66, P357, DOI 10.1016/S0165-1684(98)00015-2
   Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014
   Bi N, 2007, IEEE T IMAGE PROCESS, V16, P1956, DOI 10.1109/TIP.2007.901206
   Celik MU, 2005, IEEE T IMAGE PROCESS, V14, P253, DOI 10.1109/TIP.2004.840686
   Chen BJ, 2014, DIGIT SIGNAL PROCESS, V28, P106, DOI 10.1016/j.dsp.2014.02.010
   Chu WC, 2003, IEEE T MULTIMEDIA, V5, P34, DOI 10.1109/TMM.2003.808816
   Cox IJ, 1997, P SOC PHOTO-OPT INS, V3016, P92, DOI 10.1117/12.274502
   Deepshikha Chopra GSBA Gupta, 2012, IOSR J COMPUTER ENG, V6, P36
   Di CL, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0104663
   Fan MQ, 2008, APPL MATH COMPUT, V203, P926, DOI 10.1016/j.amc.2008.05.003
   Hamidi M, 2018, MULTIMED TOOLS APPL, V77, P27181, DOI 10.1007/s11042-018-5913-9
   HERNANDEZGUZMAN V, 2006, IEEE LATIN AM T, V4, P257, DOI DOI 10.1109/TLA.2006.4472122
   Hilbert David, 1891, MATH ANN, V38, P459, DOI [DOI 10.1007/BF01199431, 10.1007/bf01199431]
   Hu JP, 2016, COMPUT AIDED GEOM D, V43, P95, DOI 10.1016/j.cagd.2016.02.011
   Hu JP, 2014, GRAPH MODELS, V76, P340, DOI 10.1016/j.gmod.2014.03.006
   Huang NE, 1998, P ROY SOC A-MATH PHY, V454, P903, DOI 10.1098/rspa.1998.0193
   Hussein M, 2017, ADV INTELL SYST, V507, P7, DOI 10.1007/978-981-10-2471-9_2
   Lai CC, 2011, DIGIT SIGNAL PROCESS, V21, P522, DOI 10.1016/j.dsp.2011.01.017
   Li CL, 2015, NEUROCOMPUTING, V166, P404, DOI 10.1016/j.neucom.2015.03.039
   Roy S, 2017, MULTIMED TOOLS APPL, V76, P3577, DOI 10.1007/s11042-016-3902-4
   Sagan H, 1993, INT J MATH EDUC SCI, V24, P541, DOI DOI 10.1080/0020739930240405
   Sharma JB, 2013, J OPT-INDIA, V42, P214, DOI 10.1007/s12596-013-0125-1
   Singh SP, 2018, J VIS COMMUN IMAGE R, V53, P86, DOI 10.1016/j.jvcir.2018.03.006
   Soler C, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1516522.1516529
   Soleymani SH, 2019, MULTIMED TOOLS APPL, V78, P19163, DOI 10.1007/s11042-019-7282-4
   Tao PN, 2006, CONSUM COMM NETWORK, P1134
   Torr PHS, 2000, COMPUT VIS IMAGE UND, V78, P138, DOI 10.1006/cviu.1999.0832
   Tsui TK, 2008, IEEE T INF FOREN SEC, V3, P16, DOI 10.1109/TIFS.2007.916275
   Wang H, 2012, GRAPH MODELS, V74, P173, DOI 10.1016/j.gmod.2012.04.005
   Wang XC, 2018, COMPUT GRAPH-UK, V70, P118, DOI 10.1016/j.cag.2017.07.024
   Wang XC, 2018, COMPUT AIDED GEOM D, V59, P1, DOI 10.1016/j.cagd.2017.11.002
   Wang XC, 2015, VISUAL COMPUT, V31, P1135, DOI 10.1007/s00371-015-1100-4
   Wang YW, 2002, IEEE T IMAGE PROCESS, V11, P77, DOI 10.1109/83.982816
   Xie XH, 2014, SIGNAL PROCESS, V103, P250, DOI 10.1016/j.sigpro.2013.11.038
   Zhang DB, 2017, COMPUT AIDED DESIGN, V87, P1, DOI 10.1016/j.cad.2017.02.003
NR 39
TC 16
Z9 17
U1 3
U2 21
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2020
VL 36
IS 10-12
BP 2201
EP 2214
DI 10.1007/s00371-020-01909-2
EA JUL 2020
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA NW1CX
UT WOS:000549317900003
DA 2024-07-18
ER

PT J
AU Yang, TJ
   Zhang, TS
   Huang, L
AF Yang, Tiejun
   Zhang, Tianshu
   Huang, Lin
TI Detection of defects in voltage-dependent resistors using
   stacked-block-based convolutional neural networks
SO VISUAL COMPUTER
LA English
DT Article
DE Neural architecture design; Deep convolutional neural networks; Defect
   detection; Voltage-dependent resistors
ID FEATURE-SELECTION; CLASSIFICATION; MODEL
AB Voltage-dependent resistors (VDRs) are important circuit-protection devices. Their performance is affected by packaging quality. To identify VDR packaging defects more accurately and efficiently, we have proposed a convolutional neural network (CNN)-based VDR appearance quality inspection method that includes four stages: image acquisition, data augmentation, neural architecture design, and CNN training and testing. In designing the neural architecture, we have proposed two VDR-oriented network blocks, which consist of a compressed subnet and a multiscale subnet. Then, a stacking-block-based neural architecture design (BlockNAD) strategy is employed to determine the number of blocks. The last block is connected to a classification layer composed of a global average pooling (GAP) layer and a full connection (FC) layer. Further, using a VDR dataset containing 8058 images, we compared the identification performances of the candidate networks with different structures on 12 categories of VDR defects by adopting a variety of indicators, such as the mean average precision (mAP) and average test time per sample. The experimental results of the proposed method demonstrate competitive results compared to the state-of-the-art methods in identifying VDR defects, with a mAP value of approximately 99.9% and an average test time per sample of approximately 3 ms.
C1 [Yang, Tiejun; Zhang, Tianshu; Huang, Lin] Guilin Univ Technol, Guangxi Key Lab Embedded Technol & Intelligent Sy, Guilin, Guangxi, Peoples R China.
C3 Guilin University of Technology
RP Huang, L (corresponding author), Guilin Univ Technol, Guangxi Key Lab Embedded Technol & Intelligent Sy, Guilin, Guangxi, Peoples R China.
EM hlcucu@qq.com
RI zhang, tian/GZK-6001-2022; Yang, Tiejun/AAJ-8197-2020
OI Yang, Tiejun/0000-0002-8644-4651; Huang, Lin/0000-0002-2678-2085
FU National Natural Science Foundation of China [61941202]; Guangxi Natural
   Science Foundation [2018GXNSFBA281081]; Guangxi Key Laboratory Fund of
   Embedded Technology and Intelligent System [2019-02-01, 2019-01-08]
FX This research was supported in part by the National Natural Science
   Foundation of China (61941202), the Guangxi Natural Science Foundation
   (2018GXNSFBA281081), and the Guangxi Key Laboratory Fund of Embedded
   Technology and Intelligent System (2019-02-01, 2019-01-08).
CR [Anonymous], 2009, Technical report
   Bi L, 2018, VISUAL COMPUT, V34, P1043, DOI 10.1007/s00371-018-1519-5
   Bottou L., 2012, Neural networks: Tricks of the trade, P421, DOI DOI 10.1007/978-3-642-35289-8_25
   Cai NA, 2016, IEEE T COMP PACK MAN, V6, P161, DOI 10.1109/TCPMT.2015.2501284
   Cao ZH, 2020, VISUAL COMPUT, V36, P1619, DOI 10.1007/s00371-019-01763-x
   Cha YJ, 2017, COMPUT-AIDED CIV INF, V32, P361, DOI 10.1111/mice.12263
   [常亮 Chang Liang], 2016, [自动化学报, Acta Automatica Sinica], V42, P1300
   Chen JW, 2018, IEEE T INSTRUM MEAS, V67, P257, DOI 10.1109/TIM.2017.2775345
   Chondronasios A, 2016, INT J ADV MANUF TECH, V83, P33, DOI 10.1007/s00170-015-7514-3
   Dong Z, 2015, IEEE T INTELL TRANSP, V16, P2247, DOI 10.1109/TITS.2015.2402438
   Faghih-Roohi S, 2016, IEEE IJCNN, P2584, DOI 10.1109/IJCNN.2016.7727522
   Feng C, 2017, COMPUTING IN CIVIL ENGINEERING 2017: INFORMATION MODELLING AND DATA ANALYTICS, P298
   He Kaiming, 2016, EUR C COMP VIS ECCV, DOI [DOI 10.1109/CVPR.2016.90, DOI 10.1007/978-3-319-46493-0_38]
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Huang YB, 2020, VISUAL COMPUT, V36, P85, DOI 10.1007/s00371-018-1588-5
   Iandola Forrest N, 2016, SQUEEZENET ALEXNET L
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   LeCun Y., 1990, ADV NEURAL INFORM PR, P396
   Li L, 2021, ASIA PAC J TOUR RES, V26, P428, DOI 10.1080/10941665.2018.1544913
   Li WC, 2012, PATTERN RECOGN, V45, P742, DOI 10.1016/j.patcog.2011.07.025
   Lin M, 2014, 2014 INTERNATIONAL CONFERENCE ON MEDICAL BIOMETRICS (ICMB 2014), P1, DOI 10.1109/ICMB.2014.8
   Liu Z, 2017, INFORMATION SCIENCE AND ELECTRONIC ENGINEERING, P415
   Ng HF, 2006, PATTERN RECOGN LETT, V27, P1644, DOI 10.1016/j.patrec.2006.03.009
   Park JK, 2019, VISUAL COMPUT, V35, P1615, DOI 10.1007/s00371-018-1561-3
   Qingxiang Wang, 2010, 2010 2nd International Conference on Industrial and Information Systems (IIS 2010), P324, DOI 10.1109/INDUSIS.2010.5565716
   Ravikumar S, 2011, EXPERT SYST APPL, V38, P3260, DOI 10.1016/j.eswa.2010.09.012
   Redmon J., 2016, PROC CVPR IEEE, DOI [10.1109/CVPR.2016.91, DOI 10.1109/CVPR.2016.91]
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Soukup D, 2014, LECT NOTES COMPUT SC, V8887, P668, DOI 10.1007/978-3-319-14249-4_64
   Tao X, 2018, APPL SCI-BASEL, V8, DOI 10.3390/app8091575
   Tao X, 2018, IEEE T COMP PACK MAN, V8, P689, DOI 10.1109/TCPMT.2018.2794540
   Wang T, 2018, INT J ADV MANUF TECH, V94, P3465, DOI 10.1007/s00170-017-0882-0
   Wang YC, 2016, INT J COMPUT SCI NET, V16, P21
   Wu H, 2013, IEEE T COMP PACK MAN, V3, P516, DOI 10.1109/TCPMT.2012.2231902
   Zhang X, 2011, J IMAGE GRAPH, V16, P594
   Zhang X, 2018, PROC CVPR IEEE, P6848, DOI 10.1109/CVPR.2018.00716
NR 36
TC 4
Z9 4
U1 0
U2 8
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2021
VL 37
IS 6
BP 1559
EP 1567
DI 10.1007/s00371-020-01901-w
EA JUL 2020
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SO2LW
UT WOS:000545355400001
DA 2024-07-18
ER

PT J
AU Wang, YN
   Wang, HW
   Cao, JZ
AF Wang, Yanan
   Wang, Huawei
   Cao, Jianzhong
TI A contour self-compensated network for salient object detection
SO VISUAL COMPUTER
LA English
DT Article
DE CSCNet; Salient object detection; Contour; Superpixel;
   Multi-classification
ID ATTENTION
AB Given that existing salient object detection methods cannot effectively predict the fine contours of salient objects when extracting local or global contexts and features, we propose a novel contour self-compensated network (CSCNet) to generate a more accurate saliency map with complete contour. Unlike the common binary saliency detection, we reconstruct the salient object detection problem into a multi-classification problem of the background, the salient object, and the salient contour, where the salient contour is used as the third label for ground truth. Meanwhile, the image and its superpixel map are concatenated as the input of our network to add more edge information. Also, a penalty loss is proposed to restrict the spatial relationship between the background, objects, and their contours. Experimentally, we evaluate the proposed CSCNet on six benchmark datasets in both accuracy and efficiency and evaluate the attribute-based performance on the SOC dataset. Compared with 13 state-of-the-art algorithms, our CSCNet can detect salient objects more accurately and completely without adding too many convolutional layers and parameters.
C1 [Wang, Yanan; Wang, Huawei; Cao, Jianzhong] Chinese Acad Sci, Xian Inst Opt & Precis Mech, Xian, Peoples R China.
   [Wang, Yanan] Univ Chinese Acad Sci, Beijing, Peoples R China.
C3 Chinese Academy of Sciences; Xi'an Institute of Optics & Precision
   Mechanics, CAS; Chinese Academy of Sciences; University of Chinese
   Academy of Sciences, CAS
RP Wang, YN (corresponding author), Chinese Acad Sci, Xian Inst Opt & Precis Mech, Xian, Peoples R China.; Wang, YN (corresponding author), Univ Chinese Acad Sci, Beijing, Peoples R China.
EM wangyanan@opt.cn
FU National Natural Science Foundation of China [51905529]; Shaanxi
   Province Natural Science-Based Research Program [2019JQ-295]; UCAS PhD
   scholarship
FX This work is supported by the National Natural Science Foundation of
   China (No. 51905529), Shaanxi Province Natural Science-Based Research
   Program (2019JQ-295), and the UCAS PhD scholarship. Thanks to teacher
   Adams at NTU for providing equipment and to Yew Lee Tan for proofreading
   and modifying this paper.
CR Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   [Anonymous], CVPR
   [Anonymous], 2017, IEEE T PATTERN ANAL
   [Anonymous], 2015, IEEE T IMAGE PROCESS, DOI DOI 10.1109/TIP.2015.2487833
   [Anonymous], 2019, IEEE T PATTERN ANAL
   [Anonymous], 2019, 33 AAAI C ART INT
   Arbeláez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen S., 2018, P EUR C COMP VIS ECC, P234
   Deng ZJ, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P684
   Durrani TS, 2019, SPRINGER NAT HAZARDS, P1, DOI 10.1007/978-981-13-0992-2_1
   Elder, 2010, P IEEE C COMP VIS PA, P49, DOI [10.1109/CVPRW.2010.5543739, DOI 10.1109/CVPRW.2010.5543739]
   Fan D-P, 2019, PROC CVPR IEEE, P8554, DOI DOI 10.1109/CVPR.2019.00875
   Fan DP, 2018, LECT NOTES COMPUT SC, V11219, P196, DOI 10.1007/978-3-030-01267-0_12
   Feng MY, 2019, PROC CVPR IEEE, P1623, DOI 10.1109/CVPR.2019.00172
   Han B, 2018, ADV NEUR IN, V31, DOI 10.5555/3327757.3327944
   Hou Q., 2017, DEEPLY SUPERVISED SA, P3203
   Huang Y, 2018, IEEE T PATTERN ANAL, V40, P1015, DOI 10.1109/TPAMI.2017.2701380
   Jiaxing Zhao, 2018, Computational Visual Media, V4, P333, DOI 10.1007/s41095-018-0123-y
   Lee H, 2018, IEEE WINT CONF APPL, P1170, DOI 10.1109/WACV.2018.00133
   Li GB, 2017, PROC CVPR IEEE, P247, DOI 10.1109/CVPR.2017.34
   Li GB, 2016, PROC CVPR IEEE, P478, DOI 10.1109/CVPR.2016.58
   Li GB, 2015, PROC CVPR IEEE, P5455, DOI 10.1109/CVPR.2015.7299184
   Li RH, 2017, VISUAL COMPUT, V33, P1155, DOI 10.1007/s00371-016-1278-0
   Li XX, 2018, LECT NOTES COMPUT SC, V11207, P93, DOI 10.1007/978-3-030-01219-9_6
   Li Y, 2014, PROC CVPR IEEE, P280, DOI 10.1109/CVPR.2014.43
   Liu J. J., 2019, SIMPLE POOLING BASED
   Liu NA, 2018, PROC CVPR IEEE, P3089, DOI 10.1109/CVPR.2018.00326
   Liu Z, 2020, VISUAL COMPUT, V2020, P1
   Liu ZF, 2019, COMPLEXITY, DOI 10.1155/2019/3089641
   Lu Y, 2019, VISUAL COMPUT, V35, P1683, DOI 10.1007/s00371-019-01637-2
   Mechrez R., 2016, ARXIV161202184 CORR
   Mi JX, 2017, 2017 INTERNATIONAL CONFERENCE ON SECURITY, PATTERN ANALYSIS, AND CYBERNETICS (SPAC), P660, DOI 10.1109/SPAC.2017.8304358
   Niu D, 2019, VISUAL COMPUT, V2019, P1
   Perazzi F, 2012, PROC CVPR IEEE, P733, DOI 10.1109/CVPR.2012.6247743
   Qin XB, 2019, PROC CVPR IEEE, P7471, DOI 10.1109/CVPR.2019.00766
   Qin XB, 2018, IEEE GEOSCI REMOTE S, V15, P1775, DOI 10.1109/LGRS.2018.2857719
   Qin X, 2017, IEEE INT C INT ROBOT, P4284, DOI 10.1109/IROS.2017.8206291
   Shen JB, 2016, IEEE T IMAGE PROCESS, V25, P5933, DOI 10.1109/TIP.2016.2616302
   Shen JB, 2014, IEEE T IMAGE PROCESS, V23, P1451, DOI 10.1109/TIP.2014.2302892
   Singh VK, 2019, VIS COMPUT, V2019, P1
   Srivatsa RS, 2015, IEEE IMAGE PROC, P4481, DOI 10.1109/ICIP.2015.7351654
   Sun J, 2013, INT J COMPUT VISION, V104, P135, DOI 10.1007/s11263-013-0618-z
   Tang YL, 2016, VISUAL COMPUT, V32, P111, DOI 10.1007/s00371-014-1059-6
   Wang LJ, 2017, PROC CVPR IEEE, P3796, DOI 10.1109/CVPR.2017.404
   Wang LJ, 2015, PROC CVPR IEEE, P3183, DOI 10.1109/CVPR.2015.7298938
   Wang LZ, 2016, LECT NOTES COMPUT SC, V9908, P825, DOI 10.1007/978-3-319-46493-0_50
   Wang TT, 2017, IEEE I CONF COMP VIS, P4039, DOI 10.1109/ICCV.2017.433
   Wang W., 2019, ARXIV190409146
   Wang WG, 2018, PROC CVPR IEEE, P1711, DOI 10.1109/CVPR.2018.00184
   Wang WG, 2018, IEEE T IMAGE PROCESS, V27, P2368, DOI 10.1109/TIP.2017.2787612
   Wang Wenguan, 2018, IEEE Trans Image Process, V27, P38, DOI 10.1109/TIP.2017.2754941
   Wang WG, 2017, IEEE T VIS COMPUT GR, V23, P2014, DOI 10.1109/TVCG.2016.2600594
   Wang X, 2016, IEEE IMAGE PROC, P1042
   Wu R., 2019, IEEE C COMP VIS PATT
   Wu ZT, 2019, IEEE INT CONF ELECTR, P726, DOI 10.1109/ICEIEC.2019.8784476
   Yan Q, 2013, PROC CVPR IEEE, P1155, DOI 10.1109/CVPR.2013.153
   Yang C, 2013, PROC CVPR IEEE, P3166, DOI 10.1109/CVPR.2013.407
   Zhang F, 2015, IEEE T GEOSCI REMOTE, V53, P2175, DOI 10.1109/TGRS.2014.2357078
   Zhang J., 2017, ARXIV170804366 CORR
   Zhang L, 2018, PROC CVPR IEEE, P1741, DOI 10.1109/CVPR.2018.00187
   Zhang PP, 2017, IEEE I CONF COMP VIS, P202, DOI 10.1109/ICCV.2017.31
   Zhang PP, 2017, IEEE I CONF COMP VIS, P212, DOI 10.1109/ICCV.2017.32
   Zhao JX, 2019, PROC CVPR IEEE, P3922, DOI 10.1109/CVPR.2019.00405
   Zhao JX, 2019, IEEE I CONF COMP VIS, P8778, DOI 10.1109/ICCV.2019.00887
   Zhao R, 2015, PROC CVPR IEEE, P1265, DOI 10.1109/CVPR.2015.7298731
   Zhu WJ, 2014, PROC CVPR IEEE, P2814, DOI 10.1109/CVPR.2014.360
NR 68
TC 8
Z9 8
U1 2
U2 24
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2021
VL 37
IS 6
BP 1467
EP 1479
DI 10.1007/s00371-020-01882-w
EA JUN 2020
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SO2LW
UT WOS:000543677800001
DA 2024-07-18
ER

PT J
AU Afrasiabi, M
   Khotanlou, H
   Mansoorizadeh, M
AF Afrasiabi, Mahlagha
   Khotanlou, Hassan
   Mansoorizadeh, Muharram
TI DTW-CNN: time series-based human interaction prediction in videos using
   CNN-extracted features
SO VISUAL COMPUTER
LA English
DT Article
DE Interaction prediction; Convolutional neural network; Dynamic time
   warping; Support vector machine; k-Nearest neighbor
ID HUMAN ACTION RECOGNITION; NEURAL-NETWORKS; MODELS
AB Recently, the prediction of interactions in videos has been an active subject in computer vision. Its goal is to deduce interactions in their early stages. Many approaches have been proposed to predict interaction, but it still remains a challenging problem. In the present paper, features are optical flow fields extracted from video frames using convolutional neural networks. This feature, which is extracted from successive frames, constructs a time series. Then, the problem is modeled in the form of a time series prediction. Prediction of the interaction type is based on matching the time series under experiment with the time series available in the training set. Dynamic time warping provides an optimal match between a pair of time-series data by a nonlinear mapping between two data. Finally, the SVM and KNN classification methods with dynamic time warping distance are used to predict the video label. The results showed that the proposed model improved on standard interaction recognition datasets including the TVHI, BIT, and UT interaction.
C1 [Afrasiabi, Mahlagha; Khotanlou, Hassan; Mansoorizadeh, Muharram] Bu Ali Sina Univ, Dept Comp Engn, Hamadan, Hamadan, Iran.
C3 Bu Ali Sina University
RP Khotanlou, H (corresponding author), Bu Ali Sina Univ, Dept Comp Engn, Hamadan, Hamadan, Iran.
EM m.afrasiabi@basu.ac.ir; khotanlou@basu.ac.ir; mansoorm@basu.ac.ir
RI Mansoorizadeh, Muharram/C-4575-2018; afrasiabi, mahlagha/AAC-7717-2021
OI Khotanlou, Hassan/0000-0001-7351-9397; afrasiabi,
   mahlagha/0000-0001-9472-4453
CR Aggarwal, 2010, IEEE International Conference on Pattern Recognition Workshops, P4
   ALTMAN NS, 1992, AM STAT, V46, P175, DOI 10.2307/2685209
   Chaaraoui AA, 2013, PATTERN RECOGN LETT, V34, P1799, DOI 10.1016/j.patrec.2013.01.021
   [Anonymous], 2009, Technical report
   Black M. J., 1993, [1993] Proceedings Fourth International Conference on Computer Vision, P231, DOI 10.1109/ICCV.1993.378214
   Brox T, 2004, LECT NOTES COMPUT SC, V2034, P25, DOI 10.1007/978-3-540-24673-2_3
   Cho NG, 2017, NEUROCOMPUTING, V267, P169, DOI 10.1016/j.neucom.2017.06.009
   Das Dawn D, 2016, VISUAL COMPUT, V32, P289, DOI 10.1007/s00371-015-1066-2
   Demsar J, 2006, J MACH LEARN RES, V7, P1
   Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878
   DUNN OJ, 1961, J AM STAT ASSOC, V56, P52, DOI 10.2307/2282330
   Ess A., 2008, COMPUTER VISION PATT, V2008, P1, DOI [10.1109/CVPR.2008.4587581, DOI 10.1109/CVPR.2008.4587581]
   Farha Y.A., 2018, ARXIV180400892
   Gao CQ, 2016, WORLD WIDE WEB, V19, P265, DOI 10.1007/s11280-015-0348-y
   Gao J., 2017, BRIT MACH VIS C
   Guerrero-Peña FA, 2017, PATTERN RECOGN LETT, V86, P68, DOI 10.1016/j.patrec.2016.12.022
   Herath S, 2017, IMAGE VISION COMPUT, V60, P4, DOI 10.1016/j.imavis.2017.01.010
   Ikizler N, 2009, IMAGE VISION COMPUT, V27, P1515, DOI 10.1016/j.imavis.2009.02.002
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Jin CB, 2015, LECT NOTES COMPUT SC, V9315, P330, DOI 10.1007/978-3-319-24078-7_33
   Kassidas A, 1998, AICHE J, V44, P864, DOI 10.1002/aic.690440412
   Ke Q, 2018, IEEE T MULTIMEDIA, V20, P1712, DOI 10.1109/TMM.2017.2778559
   Ke QH, 2016, LECT NOTES COMPUT SC, V9914, P403, DOI 10.1007/978-3-319-48881-3_28
   Kong Y, 2016, IEEE T PATTERN ANAL, V38, P1844, DOI 10.1109/TPAMI.2015.2491928
   Kong Y, 2014, LECT NOTES COMPUT SC, V8693, P596, DOI 10.1007/978-3-319-10602-1_39
   Kong Y, 2012, LECT NOTES COMPUT SC, V7572, P300, DOI 10.1007/978-3-642-33718-5_22
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lan T, 2014, LECT NOTES COMPUT SC, V8691, P689, DOI 10.1007/978-3-319-10578-9_45
   Lei HS, 2008, SITIS 2007: PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON SIGNAL IMAGE TECHNOLOGIES & INTERNET BASED SYSTEMS, P839, DOI 10.1109/SITIS.2007.112
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Ma Y, 2018, APPL SCI-BASEL, V8, DOI 10.3390/app8040630
   Mo D., 2012, A survey on deep learning: one small step toward AI
   Munoz-Organero M, 2017, SENSORS-BASEL, V17, DOI 10.3390/s17020319
   Oliver NM, 2000, IEEE T PATTERN ANAL, V22, P831, DOI 10.1109/34.868684
   Patron-Perez Alonso., 2010, BMVC, V1, P2
   Pei WJ, 2018, IEEE T NEUR NET LEAR, V29, P920, DOI 10.1109/TNNLS.2017.2651018
   Perronnin F, 2010, LECT NOTES COMPUT SC, V6314, P143, DOI 10.1007/978-3-642-15561-1_11
   Ramanathan M, 2014, IEEE T HUM-MACH SYST, V44, P650, DOI 10.1109/THMS.2014.2325871
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Ryoo MS, 2011, IEEE I CONF COMP VIS, P1036, DOI 10.1109/ICCV.2011.6126349
   Schüldt C, 2004, INT C PATT RECOG, P32, DOI 10.1109/ICPR.2004.1334462
   Shin HC, 2016, IEEE T MED IMAGING, V35, P1285, DOI 10.1109/TMI.2016.2528162
   Simonyan K., 2014, P 27 INT C NEUR INF, P568, DOI DOI 10.1002/14651858.CD001941.PUB3
   Sminchisescu C, 2006, COMPUT VIS IMAGE UND, V104, P210, DOI 10.1016/j.cviu.2006.07.014
   Soomro K, 2019, IEEE T PATTERN ANAL, V41, P459, DOI 10.1109/TPAMI.2018.2797266
   Vishwakarma S, 2013, VISUAL COMPUT, V29, P983, DOI 10.1007/s00371-012-0752-6
   Wang HR, 2017, NEUROCOMPUTING, V225, P139, DOI 10.1016/j.neucom.2016.11.004
   Wang L., 2007, Computer Vision and Pattern Recognition, P1, DOI DOI 10.1109/CVPR.2007.383298
   Wang ZH, 2017, IEEE T CIRC SYST VID, V27, P1647, DOI 10.1109/TCSVT.2016.2539699
   Wongun Choi, 2009, 2009 IEEE 12th International Conference on Computer Vision Workshops, ICCV Workshops, P1282, DOI 10.1109/ICCVW.2009.5457461
   Yamato J., 1992, Proceedings. 1992 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.92CH3168-2), P379, DOI 10.1109/CVPR.1992.223161
NR 51
TC 10
Z9 10
U1 1
U2 22
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2020
VL 36
IS 6
BP 1127
EP 1139
DI 10.1007/s00371-019-01722-6
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA LI0TO
UT WOS:000529199400004
DA 2024-07-18
ER

PT J
AU He, M
   Zhu, CZ
   Huang, Q
   Ren, BS
   Liu, JT
AF He, Ming
   Zhu, Chaozheng
   Huang, Qian
   Ren, Baosen
   Liu, Jintao
TI A review of monocular visual odometry
SO VISUAL COMPUTER
LA English
DT Review
DE Visual odometry; Multi-sensor data fusion; Machine learning; Visual SLAM
ID INERTIAL ODOMETRY; SLAM; NAVIGATION; VERSATILE; ROBUST
AB Monocular visual odometry provides more robust functions on navigation and obstacle avoidance for mobile robots than other visual odometries, such as binocular visual odometry, RGB-D visual odometry and basic odometry. This paper describes the problem of visual odometry and also determines the relationships between visual odometry and visual simultaneous localization and mapping (SLAM). The basic principle of visual odometry is expressed in the form of mathematics, specifically by incrementally solving the pose changes of two series of frames and further improving the odometry through global optimization. After analyzing the three main ways of implementing visual odometry, the state-of-the-art monocular visual odometries, including ORB-SLAM2, DSO and SVO, are also analyzed and compared in detail. The issues of robustness and real-time operations, which are generally of interest in the current visual odometry research, are discussed from the future development of the directions and trends. Furthermore, we present a novel framework for the implementation of next-generation visual odometry based on additional high-dimensional features, which have not been implemented in the relevant applications.
C1 [He, Ming; Zhu, Chaozheng; Liu, Jintao] Army Engn Univ PLA, Coll Command & Control Engn, Nanjing, Peoples R China.
   [Huang, Qian] Hohai Univ, Coll Comp & Informat, Nanjing, Peoples R China.
   [Huang, Qian] Jilin Univ, Key Lab Symbol Computat & Knowledge Engn, Minist Educ, Changchun, Peoples R China.
   [Ren, Baosen] State Grid Shandong Elect Power Maintenance Co, Linyi, Shandong, Peoples R China.
C3 Army Engineering University of PLA; Hohai University; Jilin University
RP Zhu, CZ (corresponding author), Army Engn Univ PLA, Coll Command & Control Engn, Nanjing, Peoples R China.
EM 1091721005@qq.com; 370045744@qq.com; huangqian@hhu.edu.cn;
   18354261031@163.com; top2012@163.com
RI Liu, Jintao/JEF-1290-2023; Huang, Qian/GPX-9181-2022; He,
   Ming/JDN-0835-2023; Huang, Qian/GPX-9488-2022
OI Liu, Jintao/0000-0003-2820-5043; Zhu, Chaozheng/0000-0002-6216-7186;
   Huang, Qian/0000-0001-5625-0402
FU National Key R&D Program of China [2018YFC0806900, 2016YFC0800606,
   2016YFC0800310, 2018YFC0407905]; Natural Science Foundation of Jiangsu
   Province [BK20161469]; Primary Research & Development Plan of Jiangsu
   Province [BE2016904, BE2017616, BE2018754]
FX This work was supported by National Key R&D Program of China Nos.
   2018YFC0806900, 2016YFC0800606, 2016YFC0800310 and 2018YFC0407905;
   Natural Science Foundation of Jiangsu Province under Grants No.
   BK20161469; Primary Research & Development Plan of Jiangsu Province
   underGrant Nos. BE2016904, BE2017616, and BE2018754.
CR Altantsetseg E, 2018, VISUAL COMPUT, V34, P1021, DOI 10.1007/s00371-018-1534-6
   [Anonymous], ARXIV170804814
   [Anonymous], IEEE INT C ROB AUT
   [Anonymous], IEEE RSJ INT C INT R
   [Anonymous], LECT VISUAL SLAM THE
   [Anonymous], 2017, CVPR
   [Anonymous], 2017, P IEEE C COMP VIS PA
   [Anonymous], 2010, EUR C COMP VIS
   [Anonymous], 2018, IEEE T PATTERN ANAL, DOI DOI 10.1109/TPAMI.2017.2716350
   [Anonymous], 2017, THESIS
   [Anonymous], ARXIV170306380
   Baker S, 2004, INT J COMPUT VISION, V56, P221, DOI 10.1023/B:VISI.0000011205.11775.fd
   Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1007/11744023_32
   BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791
   Bloesch M, 2017, INT J ROBOT RES, V36, P1053, DOI 10.1177/0278364917728574
   Bloesch M, 2015, IEEE INT C INT ROBOT, P298, DOI 10.1109/IROS.2015.7353389
   Davison AJ, 2007, IEEE T PATTERN ANAL, V29, P1052, DOI 10.1109/TPAMI.2007.1049
   Quijada SD, 2013, ADV INTELL SYST, V193, P245
   Durrant-Whyte H, 2006, IEEE ROBOT AUTOM MAG, V13, P99, DOI 10.1109/MRA.2006.1638022
   Engel J, 2018, IEEE T PATTERN ANAL, V40, P611, DOI 10.1109/TPAMI.2017.2658577
   Engel J, 2014, LECT NOTES COMPUT SC, V8690, P834, DOI 10.1007/978-3-319-10605-2_54
   Engel J, 2013, IEEE I CONF COMP VIS, P1449, DOI 10.1109/ICCV.2013.183
   Forster C, 2017, IEEE T ROBOT, V33, P249, DOI 10.1109/TRO.2016.2623335
   Forster C, 2014, IEEE INT CONF ROBOT, P15, DOI 10.1109/ICRA.2014.6906584
   Gao X, 2015, ROBOT AUTON SYST, V72, P1, DOI 10.1016/j.robot.2015.03.007
   Gomez-Ojeda R, 2018, IEEE INT CONF ROBOT, P805
   Gui JJ, 2015, ADV ROBOTICS, V29, P1289, DOI 10.1080/01691864.2015.1057616
   Hartley RI, 1997, IEEE T PATTERN ANAL, V19, P580, DOI 10.1109/34.601246
   Izadi S, 2011, P UIST, P559, DOI DOI 10.1145/2047196.2047270
   Jin HL, 2003, VISUAL COMPUT, V19, P377, DOI 10.1007/s00371-003-0202-6
   Kang H, 2017, VISUAL COMPUT, V33, P761, DOI 10.1007/s00371-017-1375-8
   Kerl C, 2013, IEEE INT C INT ROBOT, P2100, DOI 10.1109/IROS.2013.6696650
   Klein George, 2007, P1
   Konolige K, 2010, SPRINGER TRAC ADV RO, V66, P201
   Leutenegger S, 2011, IEEE I CONF COMP VIS, P2548, DOI 10.1109/ICCV.2011.6126542
   Li RH, 2018, IEEE INT CONF ROBOT, P7286, DOI 10.1109/ICRA.2018.8461251
   Li S.-J., 2018, 2018 IEEE INT C ROBO, P1
   Lin S, 2016, VISUAL COMPUT, V32, P681, DOI 10.1007/s00371-016-1245-9
   Lin Y, 2018, J FIELD ROBOT, V35, P23, DOI 10.1002/rob.21732
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Lu FX, 2018, VISUAL COMPUT, V34, P753, DOI 10.1007/s00371-018-1540-8
   Lu RR, 2020, VISUAL COMPUT, V36, P253, DOI 10.1007/s00371-018-1605-8
   Mei C, 2011, INT J COMPUT VISION, V94, P198, DOI 10.1007/s11263-010-0361-7
   Muller P, 2017, IEEE WINT CONF APPL, P624, DOI 10.1109/WACV.2017.75
   Mur-Artal R, 2017, IEEE T ROBOT, V33, P1255, DOI 10.1109/TRO.2017.2705103
   Mur-Artal R, 2015, IEEE T ROBOT, V31, P1147, DOI 10.1109/TRO.2015.2463671
   Newcombe RA, 2011, IEEE I CONF COMP VIS, P2320, DOI 10.1109/ICCV.2011.6126513
   Nister D., 2004, P 2004 IEEE COMPUTER, V1, pI, DOI DOI 10.1109/CVPR.2004.1315094
   Persson M., 2018, P EUROPEAN C COMPUTE, P318
   Pizzoli M, 2014, IEEE INT CONF ROBOT, P2609, DOI 10.1109/ICRA.2014.6907233
   Qin T, 2018, IEEE T ROBOT, V34, P1004, DOI 10.1109/TRO.2018.2853729
   Rosten E, 2006, LECT NOTES COMPUT SC, V3951, P430, DOI 10.1007/11744023_34
   Rublee E, 2011, IEEE I CONF COMP VIS, P2564, DOI 10.1109/ICCV.2011.6126544
   Scaramuzza D, 2011, IEEE ROBOT AUTOM MAG, V18, P80, DOI 10.1109/MRA.2011.943233
   Schneider Thomas, 2018, IEEE Robotics and Automation Letters, V3, P1418, DOI 10.1109/LRA.2018.2800113
   Sharma O, 2018, VISUAL COMPUT, V34, P925, DOI 10.1007/s00371-018-1530-x
   Silveira G, 2008, IEEE T ROBOT, V24, P969, DOI 10.1109/TRO.2008.2004829
   Strasdat H, 2012, IMAGE VISION COMPUT, V30, P65, DOI 10.1016/j.imavis.2012.02.009
   Strasdat H, 2010, IEEE INT CONF ROBOT, P2657, DOI 10.1109/ROBOT.2010.5509636
   Teng CH, 2018, VISUAL COMPUT, V34, P1507, DOI 10.1007/s00371-017-1425-2
   Usenko V, 2016, IEEE INT CONF ROBOT, P1885, DOI 10.1109/ICRA.2016.7487335
   Vogiatzis G, 2011, IMAGE VISION COMPUT, V29, P434, DOI 10.1016/j.imavis.2011.01.006
   Wang K, 2019, MATH PROBL ENG, V2019, DOI 10.1155/2019/2614327
   Wang K, 2018, INT J ELECTROCHEM SC, V13, P10766, DOI 10.20964/2018.11.30
   Wang K, 2018, FRONT CHEM SCI ENG, V12, P376, DOI 10.1007/s11705-018-1705-z
   Weiss S, 2012, IEEE INT CONF ROBOT, P957, DOI 10.1109/ICRA.2012.6225147
   Whelan T, 2016, INT J ROBOT RES, V35, P1697, DOI 10.1177/0278364916669237
   Yang SW, 2017, ROBOT AUTON SYST, V93, P116, DOI 10.1016/j.robot.2017.03.018
   Zeng A, 2017, PROC CVPR IEEE, P199, DOI 10.1109/CVPR.2017.29
   Zhou Y, 2019, VISUAL COMPUT, V35, P123, DOI 10.1007/s00371-017-1435-0
   Zhu AZH, 2017, PROC CVPR IEEE, P5816, DOI 10.1109/CVPR.2017.616
   Zhu Chaozheng, 2018, Computer Engineering and Applications, V54, P20, DOI 10.3778/j.issn.1002-8331.1712-0279
NR 72
TC 48
Z9 51
U1 38
U2 250
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2020
VL 36
IS 5
BP 1053
EP 1065
DI 10.1007/s00371-019-01714-6
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA LJ3BL
UT WOS:000530043100014
DA 2024-07-18
ER

PT J
AU Lamba, S
   Nain, N
AF Lamba, Sonu
   Nain, Neeta
TI Segmentation of crowd flow by trajectory clustering in active contours
SO VISUAL COMPUTER
LA English
DT Article
DE Crowd flow; Trajectory clustering; Active contouring
ID MOTIONS
AB Crowd analysis has become an important topic of research for visual surveillance community. This paper proposes an active contour-based trajectory clustering approach for crowd flow segmentation. To this end, the active contour method is applied to segment the foreground crowd region with an aim to optimize further tracking. From the segmented foreground region, spatiotemporal interest points are detected and tracked to extract crowd trajectories. The trajectories are then parameterized by their shape, location information, flow direction, and neighborhood density. A clustering algorithm is designed to cluster these trajectories, and further flow patterns are segmented by merging trajectory clusters on the basis of their spatial overlapping and distinction in location and in flow direction. Once the flow patterns are segmented, trajectory density of each segment is estimated to analyze crowd flow. Experiments are conducted on three publicly available UCF Web, Collective Motion, and Violent Flows crowd datasets. The proposed work is compared with various state-of-the-art methods and achieves remarkable accuracy while maintaining the lower computational complexity.
C1 [Lamba, Sonu] Malaviya Natl Inst Technol, Jaipur, Rajasthan, India.
   [Nain, Neeta] Malaviya Natl Inst Technol, Dept Comp Sci & Engn, Jaipur, Rajasthan, India.
C3 National Institute of Technology (NIT System); Malaviya National
   Institute of Technology Jaipur; National Institute of Technology (NIT
   System); Malaviya National Institute of Technology Jaipur
RP Lamba, S (corresponding author), Malaviya Natl Inst Technol, Jaipur, Rajasthan, India.
EM lamba.sonu5@gmail.com
OI Nain, Neeta/0000-0002-0550-0376
CR Ali S, 2007, PROC CVPR IEEE, P65
   [Anonymous], 2011, VEGF Expression With San Chong, DOI DOI 10.1109/VCIP.2011.6116003
   [Anonymous], 2013, COMPUTER VISION PATT
   [Anonymous], 2018, MULTIMED TOOLS APPL
   [Anonymous], 2011, VISUAL COMPUT, DOI DOI 10.1007/s00371-011-0610-y
   Biswas S, 2014, IEEE IMAGE PROC, P2319, DOI 10.1109/ICIP.2014.7025470
   Brox T, 2010, LECT NOTES COMPUT SC, V6315, P282, DOI 10.1007/978-3-642-15555-0_21
   Cao LJ, 2015, PATTERN RECOGN, V48, P3016, DOI 10.1016/j.patcog.2015.04.001
   Chan TF, 2001, IEEE T IMAGE PROCESS, V10, P266, DOI 10.1109/83.902291
   Chen L, 2015, IEEE T MULTIMEDIA, V17, P2225, DOI 10.1109/TMM.2015.2481711
   Cheriyadat AM, 2008, IEEE J-STSP, V2, P568, DOI 10.1109/JSTSP.2008.2001306
   Dong XP, 2017, IEEE T MULTIMEDIA, V19, P763, DOI 10.1109/TMM.2016.2631884
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Fradet M, 2009, 2009 CONFERENCE FOR VISUAL MEDIA PRODUCTION: CVMP 2009, P7, DOI 10.1109/CVMP.2009.24
   Kruthiventi SSS, 2015, IEEE IMAGE PROC, P3417, DOI 10.1109/ICIP.2015.7351438
   Kuhn A, 2012, IEEE INT WORKSH MULT, P387, DOI 10.1109/MMSP.2012.6343474
   Lamba DA, 2017, PATIENT-SPECIFIC STEM CELLS, P1, DOI 10.1201/b21629
   Lamba S, 2016, 2016 12TH INTERNATIONAL CONFERENCE ON SIGNAL-IMAGE TECHNOLOGY & INTERNET-BASED SYSTEMS (SITIS), P96, DOI 10.1109/SITIS.2016.24
   Lim MK, 2014, INT C PATT RECOG, P3957, DOI 10.1109/ICPR.2014.678
   Lin WY, 2016, IEEE T IMAGE PROCESS, V25, P1674, DOI 10.1109/TIP.2016.2531281
   Loy CC., 2012, International symposium on communications control and signal processing p, P1
   Lu HJ, 2010, INT CONF SIGN PROCES, P315, DOI 10.1109/ICOSP.2010.5654978
   Magnenat-Thalmann N, 2005, VISUAL COMPUT, V21, P997, DOI 10.1007/s00371-005-0363-6
   Mahadevan V, 2010, PROC CVPR IEEE, P1975, DOI 10.1109/CVPR.2010.5539872
   Mao Y, 2019, VISUAL COMPUT, V35, P1725, DOI 10.1007/s00371-018-1568-9
   Mostafavi S, 2014, IEEE INT CON DIS, P1, DOI 10.1109/ICDCSW.2014.21
   Rao AS, 2015, VISUAL COMPUT, V31, P1533, DOI 10.1007/s00371-014-1032-4
   Rodriguez M, 2011, IEEE I CONF COMP VIS, P1235, DOI 10.1109/ICCV.2011.6126374
   Shao J, 2014, PROC CVPR IEEE, P2227, DOI 10.1109/CVPR.2014.285
   Shen JB, 2018, IEEE T IMAGE PROCESS, V27, P2688, DOI 10.1109/TIP.2018.2795740
   Shen JB, 2018, IEEE T INTELL TRANSP, V19, P162, DOI 10.1109/TITS.2017.2750082
   Shen JB, 2016, IEEE T IMAGE PROCESS, V25, P5933, DOI 10.1109/TIP.2016.2616302
   Si Wu, 2009, Computer Vision - ACCV 2009. 9th Asian Conference on Computer Vision. Revised Selected Papers, P93
   Singh N, 2017, MULTIMED TOOLS APPL, V76, P10521, DOI 10.1007/s11042-016-3676-8
   Tomasi C, 1991, DETECTION TRACKING P
   Tripathi G, 2019, VISUAL COMPUT, V35, P753, DOI 10.1007/s00371-018-1499-5
   Wang WG, 2019, IEEE T PATTERN ANAL, V41, P985, DOI 10.1109/TPAMI.2018.2819173
   Wu S, 2012, IEEE T SYST MAN CY B, V42, P1443, DOI 10.1109/TSMCB.2012.2192267
   Wu S, 2009, LECT NOTES COMPUT SC, V5879, P898
   Wu YW, 2013, COMPUT VIS IMAGE UND, V117, P1421, DOI 10.1016/j.cviu.2013.05.003
   Zhou BL, 2012, LECT NOTES COMPUT SC, V7573, P857, DOI 10.1007/978-3-642-33709-3_61
   Zhou BL, 2013, PROC CVPR IEEE, P3049, DOI 10.1109/CVPR.2013.392
NR 42
TC 5
Z9 5
U1 1
U2 8
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2020
VL 36
IS 5
BP 989
EP 1000
DI 10.1007/s00371-019-01713-7
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA LJ3BL
UT WOS:000530043100009
DA 2024-07-18
ER

PT J
AU Hernández-Bautista, I
   Carrasco-Ochoa, JA
   Martínez-Trinidad, JF
   Carbajal-Hernández, JJ
AF Hernandez-Bautista, Ignacio
   Ariel Carrasco-Ochoa, Jesus
   Francisco Martinez-Trinidad, Jose
   Juan Carbajal-Hernandez, Jose
TI Automatic filter coefficient calculation in lifting scheme wavelet
   transform for lossless image compression
SO VISUAL COMPUTER
LA English
DT Article
DE Image compression; Lifting scheme; Wavelets; Pattern recognition;
   Spectral patterns
ID PREDICTION
AB In this paper, a new method for automatic filter coefficient calculation in lifting scheme wavelet transform for image lossless compression is proposed. Actually, there is no specific rule for setting filter coefficients (a, b). Therefore, this work proposes an automatic method to calculate the filter coefficients depending on the spectral analysis of each image. Also, filter coefficients are determined for five decomposition levels and for each quadrant through applying the discrete wavelet transform in the lossless image compression problem. Spectral patterns are computed and fixed into small length vectors for building different wavelet decomposition levels; these vectors are automatically computed using a 1-NN classifier. Experimental results over standard images show that calculating the wavelet filter coefficients using the proposed method generates higher compression rates (in entropy and bitstream values) against standard wavelet and linear prediction filters.
C1 [Hernandez-Bautista, Ignacio] Catedra CONACyT, Tecnol Nacl Mexico IT Leon, Fracc Av Tecnol S-N, Leon 37290, Guanajuato, Mexico.
   [Ariel Carrasco-Ochoa, Jesus; Francisco Martinez-Trinidad, Jose] Inst Nacl Astrofis Opt & Electr, Dept Ciencias Computac, Luis Enrique Erro 1, Puebla 72840, Mexico.
   [Juan Carbajal-Hernandez, Jose] Inst Politecn Nacl, Ctr Invest Computac, Av Juan de Dios Batiz S-N, Gustavo A Madero 07738, Cdmx, Mexico.
C3 Instituto Nacional de Astrofisica, Optica y Electronica; Instituto
   Politecnico Nacional - Mexico
RP Hernández-Bautista, I (corresponding author), Catedra CONACyT, Tecnol Nacl Mexico IT Leon, Fracc Av Tecnol S-N, Leon 37290, Guanajuato, Mexico.
EM ihernandezb@conacyt.mx; ariel@inaoep.mx; fmartine@inaoep.mx;
   jcarbajalh@cic.ipn.mx
RI Martinez-Trinidad, Jose Francisco/AAS-9657-2021; Hernández, José Juan
   Carbajal/A-4968-2012
OI Martinez-Trinidad, Jose Francisco/0000-0001-7973-9075; Hernandez
   Bautista, Ignacio/0000-0001-5220-3975
FU Catedra-CONACyT; INAOE; National Polytechnic Institute, Mexico
FX The authors of this work want to thank the following organizations for
   their support: Catedra-CONACyT, INAOE, and National Polytechnic
   Institute, Mexico.
CR Ahanonu E, 2018, IEEE DATA COMPR CONF, P395, DOI 10.1109/DCC.2018.00048
   Alberti G, 2019, IEEE T INFORM THEORY, V9448, P1
   ALDROUBI A, 1993, NUMER FUNC ANAL OPT, V14, P417, DOI 10.1080/01630569308816532
   Boulgouris NV, 2001, IEEE T IMAGE PROCESS, V10, P1, DOI 10.1109/83.892438
   Bovik A, 2005, HANDBOOK OF IMAGE AND VIDEO PROCESSING, 2ND EDITION, pV, DOI 10.1016/B978-012119792-6/50062-0
   Calderbank AR, 1997, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL I, P596, DOI 10.1109/ICIP.1997.647983
   Calderbank AR, 1998, APPL COMPUT HARMON A, V5, P332, DOI 10.1006/acha.1997.0238
   California Institute of Technology, COMPUTATIONAL VISION
   Cánovas R, 2014, BIOINFORMATICS, V30, P2130, DOI 10.1093/bioinformatics/btu183
   Celik MU, 2005, IEEE T IMAGE PROCESS, V14, P253, DOI 10.1109/TIP.2004.840686
   Chapa JO, 2000, IEEE T SIGNAL PROCES, V48, P3395, DOI 10.1109/78.887001
   Chen Q., 2012, International Conference on Automatic Control and Artificial Intelligence (ACAI 2012), P359
   Daubechies I, 1998, J FOURIER ANAL APPL, V4, P247, DOI 10.1007/BF02476026
   Deriche M, 2015, INT CONF IMAG PROC, P498, DOI 10.1109/IPTA.2015.7367196
   Duda R. O., 2000, PATTERN CLASSIFICATI
   Grandits Thomas, 2018, Energy Minimization Methods in Computer Vision and Pattern Recognition. 11th International Conference, EMMCVPR 2017. Revised Selected Papers: LNCS 10746, P249, DOI 10.1007/978-3-319-78199-0_17
   Gupta A, 2005, IEEE T SIGNAL PROCES, V53, P1778, DOI 10.1109/TSP.2005.845470
   He LT, 2019, VISUAL COMPUT, V35, P151, DOI 10.1007/s00371-017-1440-3
   He SF, 2015, IEEE T INSTRUM MEAS, V64, P2636, DOI 10.1109/TIM.2015.2416451
   Hernandez Bautista I, 2016, POLIBITS, V53, P23, DOI [10.17562/PB-53-2, DOI 10.17562/PB-53-2]
   Hernández-Cabronero M, 2016, IEEE T MED IMAGING, V35, P654, DOI 10.1109/TMI.2015.2489262
   Kaaniche M, 2011, SIGNAL PROCESS, V91, P2767, DOI 10.1016/j.sigpro.2011.01.003
   Kaaniche M, 2012, EURASIP J ADV SIG PR, DOI 10.1186/1687-6180-2012-10
   Kitanovski V, 2008, INT CONF SYST SIGNAL, P105, DOI 10.1109/IWSSIP.2008.4604378
   Li HL, 2005, IEEE T IMAGE PROCESS, V14, P1831, DOI 10.1109/TIP.2005.854476
   Liu WF, 2009, PHYS REV LETT, V103, DOI 10.1103/PhysRevLett.103.257602
   MacKay D. J. C., 2005, INFORM THEORY INFERE
   MALLAT SG, 1993, IEEE T SIGNAL PROCES, V41, P3397, DOI 10.1109/78.258082
   Martchenko A, 2013, IEEE T IMAGE PROCESS, V22, P5263, DOI 10.1109/TIP.2013.2284067
   Motta G, 2000, P IEEE, V88, P1790, DOI 10.1109/5.892714
   Öktem L, 2000, INT CONF ACOUST SPEE, P2043, DOI 10.1109/ICASSP.2000.859235
   Pogrebnyak O, 2003, PROC SPIE, V5203, P623, DOI 10.1117/12.504092
   Pogrebnyak O, 2014, LECT NOTES COMPUT SC, V8827, P990, DOI 10.1007/978-3-319-12568-8_120
   Pogrebnyak O, 2014, LECT NOTES COMPUT SC, V8495, P221, DOI 10.1007/978-3-319-07491-7_23
   Said A, 1996, IEEE T CIRC SYST VID, V6, P243, DOI 10.1109/76.499834
   Schiopu I, 2020, IEEE T CIRC SYST VID, V30, P1829, DOI 10.1109/TCSVT.2019.2909821
   Seiffert U, 2014, NEUROCOMPUTING, V125, P229, DOI 10.1016/j.neucom.2012.11.051
   Servetto SD, 1999, IEEE T IMAGE PROCESS, V8, P1161, DOI 10.1109/83.784429
   SHAPIRO JM, 1993, IEEE T SIGNAL PROCES, V41, P3445, DOI 10.1109/78.258085
   Strickland RN, 1997, IEEE T IMAGE PROCESS, V6, P724, DOI 10.1109/83.568929
   SWELDENS W, 1995, P SOC PHOTO-OPT INS, V2569, P68, DOI 10.1117/12.217619
   Sweldens W, 1996, APPL COMPUT HARMON A, V3, P186, DOI 10.1006/acha.1996.0015
   Thielemann H, 2001, SCIENCE 80
   Thielemann H, 2004, PAMM, V3
   Thielemann H, 2007, INT C IND APPL MATH, V7, DOI 10.1002/pamm.200700988
   University C.M, VIS AUT SYST
   Wang C, 2018, VISUAL COMPUT, V34, P1357, DOI 10.1007/s00371-017-1418-1
   Wang ST, 2018, INT J SEMANT COMPUT, V12, P425, DOI 10.1142/S1793351X18500022
   Yoo H, 2001, 2001 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL III, PROCEEDINGS, P792, DOI 10.1109/ICIP.2001.958239
NR 49
TC 1
Z9 1
U1 0
U2 8
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2021
VL 37
IS 5
BP 957
EP 972
DI 10.1007/s00371-020-01846-0
EA APR 2020
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RV2QF
UT WOS:000528414900001
DA 2024-07-18
ER

PT J
AU Liu, ZY
   Tang, JT
   Zhao, P
AF Liu, Zhengyi
   Tang, Jiting
   Zhao, Peng
TI Salient object detection via hybrid upsampling and hybrid loss computing
SO VISUAL COMPUTER
LA English
DT Article
DE Salient object detection; Hybrid upsampling; Hybrid loss; CRF; Area
   constraint
ID OPTIMIZATION; NETWORK
AB Salient object detection aims to detect distinct objects which attract human most. It has achieved substantial progress using deep convolutional neural networks in which conventional deconvolution operation is used as recovering the size of image in dense prediction tasks and cross-entropy loss is applied to compute the difference between saliency map and ground truth in pixel level. Different from conventional deconvolution operation, hybrid upsampling block is proposed to retain the detail of object by increasing the receptive field and spatial information when recovering the image size, and hybrid loss which consists of cross-entropy loss and area loss is proposed to train the network optimized by area constraint. At last, an encoder-decoder network based on hybrid upsampling block and hybrid loss is implemented in public benchmark dataset and achieves the best performance against state-of-the-art methods.
C1 [Liu, Zhengyi; Tang, Jiting; Zhao, Peng] Anhui Univ, Key Lab Intelligent Comp Signal Proc, Minist Educ, Hefei, Peoples R China.
   [Liu, Zhengyi; Tang, Jiting; Zhao, Peng] Anhui Univ, Sch Comp Sci & Technol, Hefei, Peoples R China.
C3 Anhui University; Anhui University
RP Liu, ZY (corresponding author), Anhui Univ, Key Lab Intelligent Comp Signal Proc, Minist Educ, Hefei, Peoples R China.; Liu, ZY (corresponding author), Anhui Univ, Sch Comp Sci & Technol, Hefei, Peoples R China.
EM 22927463@qq.com; 1796340141@qq.com; 18868519@qq.com
RI Liu, Zhengyi/AAB-6589-2022
OI Liu, Zhengyi/0000-0003-3265-823X
FU National Natural Science Foundation of China [61602004]
FX We thank Prof. Ming-ming Cheng from Nankai University for providing the
   codes of all evaluation metrics and result saliency maps. We further
   thank all anonymous reviewers for their valuable comments. This research
   is supported by National Natural Science Foundation of China (61602004).
CR Abadi M, 2016, ACM SIGPLAN NOTICES, V51, P1, DOI [10.1145/3022670.2976746, 10.1145/2951913.2976746]
   Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   Borji A, 2012, LECT NOTES COMPUT SC, V7573, P414, DOI 10.1007/978-3-642-33709-3_30
   Chen H, 2019, IEEE T IMAGE PROCESS, V28, P2825, DOI 10.1109/TIP.2019.2891104
   Chen H, 2019, PATTERN RECOGN, V86, P376, DOI 10.1016/j.patcog.2018.08.007
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Cheng MM, 2011, PROC CVPR IEEE, P409, DOI 10.1109/CVPR.2011.5995344
   Cong RM, 2018, IEEE T IMAGE PROCESS, V27, P568, DOI 10.1109/TIP.2017.2763819
   Fan DP, 2017, IEEE I CONF COMP VIS, P4558, DOI 10.1109/ICCV.2017.487
   Fan DP, 2018, LECT NOTES COMPUT SC, V11219, P196, DOI 10.1007/978-3-030-01267-0_12
   Ilg E, 2017, PROC CVPR IEEE, P1647, DOI 10.1109/CVPR.2017.179
   Jiang P, 2013, IEEE I CONF COMP VIS, P1976, DOI 10.1109/ICCV.2013.248
   KRAHENBUHL P, 2011, ADV NEURAL INFORM PR, P109, DOI DOI 10.5555/2986459.2986472
   Lee GY, 2016, PROC CVPR IEEE, P660, DOI 10.1109/CVPR.2016.78
   Li GB, 2016, PROC CVPR IEEE, P478, DOI 10.1109/CVPR.2016.58
   Li GB, 2015, PROC CVPR IEEE, P5455, DOI 10.1109/CVPR.2015.7299184
   Li X, 2016, IEEE T IMAGE PROCESS, V25, P3919, DOI 10.1109/TIP.2016.2579306
   Liu Tie, 2007, P IEEE C COMP VIS PA, P1, DOI DOI 10.1109/CVPR.2007.383047
   Luo ZM, 2017, PROC CVPR IEEE, P6593, DOI 10.1109/CVPR.2017.698
   Mochizuki I, 2018, VISUAL COMPUT, V34, P1031, DOI 10.1007/s00371-018-1518-6
   Pan J., 2017, PROC IEEE C COMPUT V
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Souza MA, 2017, 2017 INTERNATIONAL SYMPOSIUM ON COMPUTER ARCHITECTURE AND HIGH PERFORMANCE COMPUTING WORKSHOPS (SBAC-PADW), P1, DOI 10.1109/SBAC-PADW.2017.9
   Wang H, 2018, NEURAL NETWORKS, V101, P47, DOI 10.1016/j.neunet.2018.02.005
   Wang LJ, 2015, PROC CVPR IEEE, P3183, DOI 10.1109/CVPR.2015.7298938
   Wang TT, 2016, LECT NOTES COMPUT SC, V9912, P450, DOI 10.1007/978-3-319-46484-8_27
   Wang W, 2018, IEEE CLOUD COMPUT, V5, P77, DOI 10.1109/MCC.2018.111122026
   Wang WG, 2018, IEEE T IMAGE PROCESS, V27, P38, DOI 10.1109/TIP.2017.2754941
   Wang WG, 2018, IEEE T CIRC SYST VID, V28, P1727, DOI 10.1109/TCSVT.2017.2701279
   Wang WG, 2018, IEEE T PATTERN ANAL, V40, P20, DOI 10.1109/TPAMI.2017.2662005
   Wang WG, 2016, IEEE T IMAGE PROCESS, V25, P5025, DOI 10.1109/TIP.2016.2601784
   Wang WG, 2015, IEEE T IMAGE PROCESS, V24, P4185, DOI 10.1109/TIP.2015.2460013
   Wei YC, 2012, LECT NOTES COMPUT SC, V7574, P29, DOI 10.1007/978-3-642-33712-3_3
   Yan B, 2017, IEEE IMAGE PROC, P2339, DOI 10.1109/ICIP.2017.8296700
   Yan Q, 2013, PROC CVPR IEEE, P1155, DOI 10.1109/CVPR.2013.153
   Yang C, 2013, PROC CVPR IEEE, P3166, DOI 10.1109/CVPR.2013.407
   Zhang DW, 2017, IEEE T PATTERN ANAL, V39, P865, DOI 10.1109/TPAMI.2016.2567393
   Zhang PP, 2017, IEEE I CONF COMP VIS, P202, DOI 10.1109/ICCV.2017.31
   Zhang PP, 2017, IEEE I CONF COMP VIS, P212, DOI 10.1109/ICCV.2017.32
   Zhu WJ, 2014, PROC CVPR IEEE, P2814, DOI 10.1109/CVPR.2014.360
NR 40
TC 6
Z9 6
U1 2
U2 12
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2020
VL 36
IS 4
BP 843
EP 853
DI 10.1007/s00371-019-01659-w
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KW0AL
UT WOS:000520835800014
DA 2024-07-18
ER

PT J
AU Chen, WJ
   Huang, HB
   Peng, S
   Zhou, CS
   Zhang, CP
AF Chen, Weijun
   Huang, Hongbo
   Peng, Shuai
   Zhou, Changsheng
   Zhang, Cuiping
TI YOLO-face: a real-time face detector
SO VISUAL COMPUTER
LA English
DT Article
DE Face detection; YOLO; Deep learning; Anchor box; Loss function
AB Face detection is one of the important tasks of object detection. Typically detection is the first stage of pattern recognition and identity authentication. In recent years, deep learning-based algorithms in object detection have grown rapidly. These algorithms can be generally divided into two categories, i.e., two-stage detector like Faster R-CNN and one-stage detector like YOLO. Although YOLO and its varieties are not so good as two-stage detectors in terms of accuracy, they outperform the counterparts by a large margin in speed. YOLO performs well when facing normal size objects, but is incapable of detecting small objects. The accuracy decreases notably when dealing with objects that have large-scale changing like faces. Aimed to solve the detection problem of varying face scales, we propose a face detector named YOLO-face based on YOLOv3 to improve the performance for face detection. The present approach includes using anchor boxes more appropriate for face detection and a more precise regression loss function. The improved detector significantly increased accuracy while remaining fast detection speed. Experiments on the WIDER FACE and the FDDB datasets show that our improved algorithm outperforms YOLO and its varieties.
C1 [Chen, Weijun; Huang, Hongbo; Peng, Shuai; Zhou, Changsheng; Zhang, Cuiping] Beijing Informat Sci & Technol Univ, Comp Sch, Beijing 100101, Peoples R China.
   [Huang, Hongbo; Zhou, Changsheng; Zhang, Cuiping] Beijing Informat Sci & Technol Univ, Inst Comp Intelligence, Beijing 100192, Peoples R China.
C3 Beijing Information Science & Technology University; Beijing Information
   Science & Technology University
RP Huang, HB (corresponding author), Beijing Informat Sci & Technol Univ, Comp Sch, Beijing 100101, Peoples R China.; Huang, HB (corresponding author), Beijing Informat Sci & Technol Univ, Inst Comp Intelligence, Beijing 100192, Peoples R China.
EM hhb@bistu.edu.cn
RI Chen, Wei/GZK-7348-2022
OI huang, hongbo/0000-0002-2963-8257; /0009-0006-1337-808X
FU Beijing municipal education committee scientific and technological
   planning Project [KM201811232024, KM201611232022]; Beijing excellent
   talents youth backbone Project [9111524401]
FX This work is supported by the Beijing municipal education committee
   scientific and technological planning Project (KM201811232024,
   KM201611232022) and Beijing excellent talents youth backbone Project
   (9111524401).
CR Burges CJC, 1998, DATA MIN KNOWL DISC, V2, P121, DOI 10.1023/A:1009715923555
   Chen X, 2015, Microsoft coco captions: Data collection and evaluation server
   Dai J, 2016, PROCEEDINGS 2016 IEEE INTERNATIONAL CONFERENCE ON INDUSTRIAL TECHNOLOGY (ICIT), P1796, DOI 10.1109/ICIT.2016.7475036
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He KM, 2014, LECT NOTES COMPUT SC, V8691, P346, DOI [arXiv:1406.4729, 10.1007/978-3-319-10578-9_23]
   Hu P, 2017, PROC CVPR IEEE, P540, DOI 10.1109/CVPR.2017.65
   Huang L., 2015, Comput. Sci.
   Joseph RK, 2016, CRIT POL ECON S ASIA, P1
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Najibi M, 2017, IEEE I CONF COMP VIS, P4885, DOI 10.1109/ICCV.2017.522
   Neubeck A, 2006, INT C PATT RECOG, P850, DOI 10.1109/icpr.2006.479
   Redmon J., 2018, COMPUTER VISION PATT
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rezatofighi H, 2019, PROC CVPR IEEE, P658, DOI 10.1109/CVPR.2019.00075
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Tang X, 2018, LECT NOTES COMPUT SC, V11213, P812, DOI 10.1007/978-3-030-01240-3_49
   Viola P, 2001, PROC CVPR IEEE, P511, DOI 10.1109/cvpr.2001.990517
   Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb
   Wang H., 2017, CoRR
   Wang J., 2017, ARXIV171107246
   Wang Y., 2017, Detecting Faces Using Region-based fully convolutional networks
   Yang S, 2018, IEEE T PATTERN ANAL, V40, P1845, DOI 10.1109/TPAMI.2017.2738644
   Yang S, 2016, PROC CVPR IEEE, P5525, DOI 10.1109/CVPR.2016.596
   ZHANG C, 2018, ARXIV180202142
   Zhang Y, 2016, IEEE T IMAGE PROCESS, V25, DOI 10.1109/TIP.2016.2549360
   Zhaowei Cai, 2018, 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition. Proceedings, P6154, DOI 10.1109/CVPR.2018.00644
NR 32
TC 91
Z9 94
U1 19
U2 100
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2021
VL 37
IS 4
SI SI
BP 805
EP 813
DI 10.1007/s00371-020-01831-7
EA MAR 2020
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RL0LI
UT WOS:000564536400002
DA 2024-07-18
ER

PT J
AU Quan, Q
   He, FZ
   Li, HR
AF Quan, Quan
   He, Fazhi
   Li, Haoran
TI A multi-phase blending method with incremental intensity for training
   detection networks
SO VISUAL COMPUTER
LA English
DT Article
DE Object detection; Data augmentation; Convolutional neural network
ID DEEP; RECOGNITION; GENERATION; TRACKING
AB Object detection is an important topic for visual data processing in the visual computing area. Although a number of approaches have been studied, it still remains a challenge. There is a suitable way to promote image classifiers by blending training with blended images and corresponding blended labels. However, our experiments show that directly moving existing blending methods from classification to object detection will cause the training process become harder and eventually will lead to a bad performance. Inspired by our discovery, this paper presents a multi-phase blending method with incremental blending intensity to improve the accuracy of object detectors and achieve remarkable improvements. Firstly, to adapt blending method to detection task, we propose a smoothly scheduled and incremental blending intensity to control the degree of multi-phase blending. Based on the above dynamic coefficient, we propose an incremental blending method, in which the blending intensity is smoothly increased from zero to full. Therefore, more complex and various data can be created to achieve the goal of regularizing the network. Secondly, we also design an incremental hybrid loss function to replace the original loss function. The blending intensity in our loss function increases smoothly, which is controlled by our scheduled coefficient. Thirdly, we further discard more negative examples in our multi-phase training process than other typical training methods and processes. By doing so, we can regularize the neural network to enhance generalization capability with data diversity and eventually to improve the accuracy in object detection. Another advantage is that there is no negative effect on evaluation because our method is just applied during the training process. Typical experiments show the proposed method improves the generalization of the detection networks. On PASCAL VOC and MS COCO, our method outperforms the state-of-the-art RFBNet of one-stage detectors for real-time processing.
C1 [Quan, Quan; He, Fazhi; Li, Haoran] Wuhan Univ, Sch Comp Sci & Technol, Wuhan, Hubei, Peoples R China.
C3 Wuhan University
RP He, FZ (corresponding author), Wuhan Univ, Sch Comp Sci & Technol, Wuhan, Hubei, Peoples R China.
EM quan_q@whu.edu.cn; fzhe@whu.edu.cn; lhr@whu.edu.cn
RI He, Fazhi/Q-3691-2018; Quan, Quan/GYI-8502-2022; Haoran,
   Li/AAR-8648-2021
OI Haoran, Li/0000-0002-2379-1541
FU NSFC [61472289]
FX This study was funded by NSFC (Grant No. 61472289).
CR Abbasi A, 2019, VISUAL COMPUT, V35, P271, DOI 10.1007/s00371-018-1586-7
   Arashloo SR, 2014, IEEE T MULTIMEDIA, V16, P2099, DOI 10.1109/TMM.2014.2362855
   Bartlett PL, 2006, J AM STAT ASSOC, V101, P138, DOI 10.1198/016214505000000907
   Chapelle O, 2001, ADV NEUR IN, V13, P416
   Chen X, 2019, MULTIMED TOOLS APPL, V78, P11173, DOI 10.1007/s11042-018-6690-1
   Choh Man Teng, 2001, Proceedings of the Fourteenth International Florida Artificial Intelligence Research Society Conference, P269
   Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89
   Dai J, 2016, PROCEEDINGS 2016 IEEE INTERNATIONAL CONFERENCE ON INDUSTRIAL TECHNOLOGY (ICIT), P1796, DOI 10.1109/ICIT.2016.7475036
   Dal Corso A, 2017, VISUAL COMPUT, V33, P371, DOI 10.1007/s00371-016-1207-2
   Dewi C, 2020, MULTIMED TOOLS APPL, V79, P32897, DOI 10.1007/s11042-020-09509-x
   Eren MT, 2018, VISUAL COMPUT, V34, P405, DOI 10.1007/s00371-016-1346-5
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Figueiredo F, 2018, INT C CONTROL DECISI, P440, DOI 10.1109/CoDIT.2018.8394898
   Fu C.-Y., 2017, DSSD: Deconvolutional Single Shot Detector
   Girshick R., 2014, PROC CVPR IEEE, DOI [DOI 10.1109/CVPR.2014.81, 10.1109/CVPR.2014.81]
   Guo H., 2018, ARXIV180902499
   He KM, 2020, IEEE T PATTERN ANAL, V42, P386, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   HOU N, 2019, FRONT COMPUT SCI
   Joseph RK, 2016, CRIT POL ECON S ASIA, P1
   Kán P, 2019, VISUAL COMPUT, V35, P873, DOI 10.1007/s00371-019-01666-x
   LI H, 2019, SOFT COMPUT
   Li HR, 2019, APPL MATH SER B, V34, P1, DOI 10.1007/s11766-019-3706-1
   Li K, 2019, FRONT COMPUT SCI-CHI, V13, P1116, DOI 10.1007/s11704-018-6442-4
   Li K, 2018, J COMPUT SCI TECH-CH, V33, P223, DOI 10.1007/s11390-017-1764-5
   Li K, 2017, APPL MATH SER B, V32, P294, DOI 10.1007/s11766-017-3466-8
   Li YR, 2019, VISUAL COMPUT, V35, P1143, DOI 10.1007/s00371-019-01692-9
   Lin T.Y., 2017, P IEEE C COMPUTER VI, P2117, DOI [10.1109/CVPR.2017.106, DOI 10.1109/CVPR.2017.106]
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Liu XX, 2019, VISUAL COMPUT, V35, P445, DOI 10.1007/s00371-018-1566-y
   Loshchilov I., 2017, P INT C LEARN REPR
   Luciano L, 2019, VISUAL COMPUT, V35, P1171, DOI 10.1007/s00371-019-01668-9
   Luo JK, 2020, INTELL DATA ANAL, V24, P581, DOI 10.3233/IDA-194641
   Oksuz K, 2018, LECT NOTES COMPUT SC, V11211, P521, DOI 10.1007/978-3-030-01234-2_31
   Pan YT, 2020, FRONT COMPUT SCI-CHI, V14, DOI 10.1007/s11704-019-8123-3
   Rasool S, 2016, VISUAL COMPUT, V32, P1311, DOI 10.1007/s00371-016-1224-1
   Redmon J., 2016, PROC CVPR IEEE, DOI [10.1109/CVPR.2016.91, DOI 10.1109/CVPR.2016.91]
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Simard PY, 1998, LECT NOTES COMPUT SC, V1524, P239
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sun J, 2016, APPL MATH SER B, V31, P177, DOI 10.1007/s11766-016-3378-z
   Szegedy Christian, 2016, P IEEE C COMP VIS PA, DOI DOI 10.1109/CVPR.2016.308
   Takahashi R., 2018, ASIAN C MACHINE LEAR, P786
   Tsung-Yi Lin, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P2999, DOI 10.1109/ICCV.2017.324
   Uijlings JRR, 2013, INT J COMPUT VISION, V104, P154, DOI 10.1007/s11263-013-0620-5
   Vapnik V. N., 1998, STAT LEARNING THEORY
   Wu YQ, 2018, IEEE T SERV COMPUT, V11, P341, DOI 10.1109/TSC.2015.2501981
   Xiao T, 2015, PROC CVPR IEEE, P2691, DOI 10.1109/CVPR.2015.7298885
   Yan XH, 2018, INT J COOP INF SYST, V27, DOI 10.1142/S0218843017410015
   Yong JS, 2019, APPL MATH SER B, V34, P480, DOI 10.1007/s11766-019-3714-1
   Yu HP, 2019, MULTIMED TOOLS APPL, V78, P11779, DOI 10.1007/s11042-018-6735-5
   Yu HP, 2018, MULTIMED TOOLS APPL, V77, P24097, DOI 10.1007/s11042-018-5697-y
   Yu ZB, 2018, VISUAL COMPUT, V34, P1691, DOI 10.1007/s00371-017-1443-0
   Zhang C., 2016, UNDERSTANDING DEEP L
   Zhang Hongyi, 2017, ARXIV171009412
   ZHANG J, 2019, MULTIMED TOOLS APPL
   Zhang J, 2019, VISUAL COMPUT, V35, P1181, DOI 10.1007/s00371-019-01667-w
   Zhang SD, 2020, VISUAL COMPUT, V36, P1797, DOI 10.1007/s00371-019-01774-8
   Zhang SD, 2020, VISUAL COMPUT, V36, P305, DOI 10.1007/s00371-018-1612-9
   Zhang SY, 2019, VISUAL COMPUT, V35, P1157, DOI 10.1007/s00371-019-01691-w
   Zhang ZM, 2019, VISUAL COMPUT, V35, P997, DOI 10.1007/s00371-019-01669-8
   Zhong Z, 2020, AAAI CONF ARTIF INTE, V34, P13001
   Zhou Y, 2017, SCI CHINA INFORM SCI, V60, DOI 10.1007/s11432-015-0594-2
   Zhu XQ, 2004, ARTIF INTELL REV, V22, P177, DOI 10.1007/s10462-004-0751-8
NR 65
TC 60
Z9 61
U1 1
U2 17
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2021
VL 37
IS 2
BP 245
EP 259
DI 10.1007/s00371-020-01796-7
EA JAN 2020
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA QR4KJ
UT WOS:000515583300001
DA 2024-07-18
ER

PT J
AU Gogic, I
   Manhart, M
   Pandzic, IS
   Ahlberg, J
AF Gogic, Ivan
   Manhart, Martina
   Pandzic, Igor S.
   Ahlberg, Jorgen
TI Fast facial expression recognition using local binary features and
   shallow neural networks
SO VISUAL COMPUTER
LA English
DT Article
DE Facial expression recognition; Neural networks; Decision tree ensembles;
   Local binary features
ID FUSION; VIEW; FACE
AB Facial expression recognition applications demand accurate and fast algorithms that can run in real time on platforms with limited computational resources. We propose an algorithm that bridges the gap between precise but slow methods and fast but less precise methods. The algorithm combines gentle boost decision trees and neural networks. The gentle boost decision trees are trained to extract highly discriminative feature vectors (local binary features) for each basic facial expression around distinct facial landmark points. These sparse binary features are concatenated and used to jointly optimize facial expression recognition through a shallow neural network architecture. The joint optimization improves the recognition rates of difficult expressions such as fear and sadness. Furthermore, extensive experiments in both within- and cross-database scenarios have been conducted on relevant benchmark data sets for facial expression recognition: CK+, MMI, JAFFE, and SFEW 2.0. The proposed method (LBF-NN) compares favorably with state-of-the-art algorithms while achieving an order of magnitude improvement in execution time.
C1 [Gogic, Ivan; Manhart, Martina; Pandzic, Igor S.] Univ Zagreb, Fac Elect Engn & Comp, Zagreb, Croatia.
   [Ahlberg, Jorgen] Linkoping Univ, Dept Elect Engn, Comp Vis Lab, Linkoping, Sweden.
C3 University of Zagreb; Linkoping University
RP Gogic, I (corresponding author), Univ Zagreb, Fac Elect Engn & Comp, Zagreb, Croatia.
EM ivan.gogic@fer.hr
RI Gogić, Ivan/AAA-4550-2021
OI Gogić, Ivan/0000-0003-1295-017X
CR [Anonymous], 2014, PROCEDIA IEEE COMPUT, DOI DOI 10.1109/CVPR.2014.233
   [Anonymous], 2015, ARXIV150905371
   Blewitt ME, 2008, NAT GENET, V40, P663, DOI 10.1038/ng.142
   Boughrara H, 2016, MULTIMED TOOLS APPL, V75, P709, DOI 10.1007/s11042-014-2322-6
   Bulat A, 2017, IEEE I CONF COMP VIS, P1021, DOI 10.1109/ICCV.2017.116
   BYRD RH, 1995, SIAM J SCI COMPUT, V16, P1190, DOI 10.1137/0916069
   Dhall Abhinav, 2011, Proceedings 2011 IEEE International Conference on Automatic Face & Gesture Recognition (FG 2011), P878, DOI 10.1109/FG.2011.5771366
   Dhall A, 2011, 2011 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCV WORKSHOPS)
   Dhall A, 2015, ICMI'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P423, DOI 10.1145/2818346.2829994
   Ding H, 2017, IEEE INT CONF AUTOMA, P118, DOI 10.1109/FG.2017.23
   EKMAN P, 1971, J PERS SOC PSYCHOL, V17, P124, DOI 10.1037/h0030377
   Ekman P., 1978, Facial action coding system
   Eleftheriadis S, 2015, IEEE T IMAGE PROCESS, V24, P189, DOI 10.1109/TIP.2014.2375634
   Fang H, 2014, PATTERN RECOGN, V47, P1271, DOI 10.1016/j.patcog.2013.09.023
   Friedman J, 2000, ANN STAT, V28, P337, DOI 10.1214/aos/1016218223
   Goodfellow Ian J., 2013, Neural Information Processing. 20th International Conference, ICONIP 2013. Proceedings: LNCS 8228, P117, DOI 10.1007/978-3-642-42051-1_16
   Grites T., 2008, Academic advising: a comprehensive handbook, P1, DOI [10.1109/AFGR.2008.4813379, DOI 10.1109/AFGR.2008.4813379]
   Gu WF, 2012, PATTERN RECOGN, V45, P80, DOI 10.1016/j.patcog.2011.05.006
   Gudi A., 2015, 2015 11 IEEE INT C W, V6, P1
   Guo YM, 2016, IEEE T IMAGE PROCESS, V25, P1977, DOI 10.1109/TIP.2016.2537215
   Happy SL, 2015, IEEE T AFFECT COMPUT, V6, P1, DOI 10.1109/TAFFC.2014.2386334
   Ng HW, 2015, ICMI'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P443, DOI 10.1145/2818346.2830593
   Huang XH, 2014, INT C PATT RECOG, P1734, DOI 10.1109/ICPR.2014.305
   Jaiswal S, 2015, IEEE INT CONF AUTOMA
   Jang YJ, 2016, 2016 IEEE ASIA PACIFIC CONFERENCE ON CIRCUITS AND SYSTEMS (APCCAS), P47, DOI 10.1109/APCCAS.2016.7803892
   Jiang BH, 2014, INT C PATT RECOG, P1776, DOI 10.1109/ICPR.2014.312
   Jiang BH, 2014, IEEE T CYBERNETICS, V44, P161, DOI 10.1109/TCYB.2013.2249063
   Khan RA, 2013, PATTERN RECOGN LETT, V34, P1159, DOI 10.1016/j.patrec.2013.03.022
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee D, 2015, PROC CVPR IEEE, P4204, DOI 10.1109/CVPR.2015.7299048
   Lee SH, 2014, IEEE T AFFECT COMPUT, V5, P340, DOI 10.1109/TAFFC.2014.2346515
   Levi G, 2015, ICMI'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P503, DOI 10.1145/2823327.2823333
   Littlewort G, 2006, IMAGE VISION COMPUT, V24, P615, DOI 10.1016/j.imavis.2005.09.011
   Liu MY, 2014, PROC CVPR IEEE, P1749, DOI 10.1109/CVPR.2014.226
   Long MS, 2015, PR MACH LEARN RES, V37, P97
   Lopes AT, 2017, PATTERN RECOGN, V61, P610, DOI 10.1016/j.patcog.2016.07.026
   Lucey P., 2010, IEEE COMP SOC C COMP, P94, DOI 10.1109/CVPRW.2010.5543262
   Lyons M, 1998, AUTOMATIC FACE AND GESTURE RECOGNITION - THIRD IEEE INTERNATIONAL CONFERENCE PROCEEDINGS, P200, DOI 10.1109/AFGR.1998.670949
   Mehrabian A., 1971, Silent Messages, V8, P30
   Mollahosseini A, 2016, IEEE WINT CONF APPL
   Owusu E, 2014, EXPERT SYST APPL, V41, P3383, DOI 10.1016/j.eswa.2013.11.041
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Pantic M, 2005, 2005 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO (ICME), VOLS 1 AND 2, P317, DOI 10.1109/ICME.2005.1521424
   Poursaberi A, 2012, EURASIP J IMAGE VIDE, P1, DOI 10.1186/1687-5281-2012-17
   Pramerdorfer C, 2016, ARXIV
   Ren SQ, 2016, IEEE T IMAGE PROCESS, V25, P1233, DOI 10.1109/TIP.2016.2518867
   Rivera AR, 2013, IEEE T IMAGE PROCESS, V22, P1740, DOI 10.1109/TIP.2012.2235848
   Rudovic O, 2012, PROC CVPR IEEE, P2634, DOI 10.1109/CVPR.2012.6247983
   Sandbach G, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P738, DOI 10.1109/ICCVW.2013.101
   Shan CF, 2009, IMAGE VISION COMPUT, V27, P803, DOI 10.1016/j.imavis.2008.08.005
   Wan SH, 2014, PATTERN RECOGN, V47, P1859, DOI 10.1016/j.patcog.2013.11.025
   Whitehill J, 2013, SOC EMOT NAT ARTIFAC, V88, P58
   WOLFE P, 1969, SIAM REV, V11, P226, DOI 10.1137/1011036
   Yu ZD, 2015, ICMI'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P435
   Zavaschi THH, 2013, EXPERT SYST APPL, V40, P646, DOI 10.1016/j.eswa.2012.07.074
   Zeng ZH, 2009, IEEE T PATTERN ANAL, V31, P39, DOI 10.1109/TPAMI.2008.52
   Zhai YK, 2017, LECT NOTES COMPUT SC, V10666, P211, DOI 10.1007/978-3-319-71607-7_19
   Zhang LG, 2011, IEEE T AFFECT COMPUT, V2, P219, DOI 10.1109/T-AFFC.2011.13
   Zhang X, 2015, MACH VISION APPL, V26, P467, DOI 10.1007/s00138-015-0677-y
   Zhao GY, 2007, IEEE T PATTERN ANAL, V29, P915, DOI 10.1109/TPAMI.2007.1110
   Zhong L, 2015, IEEE T CYBERNETICS, V45, P1499, DOI 10.1109/TCYB.2014.2354351
   Zhu SZ, 2015, PROC CVPR IEEE, P4998, DOI 10.1109/CVPR.2015.7299134
   Zong Y, 2016, J MULTIMODAL USER IN, V10, P163, DOI 10.1007/s12193-015-0210-7
NR 63
TC 57
Z9 57
U1 1
U2 42
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2020
VL 36
IS 1
BP 97
EP 112
DI 10.1007/s00371-018-1585-8
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KJ3OJ
UT WOS:000511966800009
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Wang, Q
   Artieres, T
   Chen, MK
   Denoyer, L
AF Wang, Qi
   Artieres, Thierry
   Chen, Mickael
   Denoyer, Ludovic
TI Adversarial learning for modeling human motion
SO VISUAL COMPUTER
LA English
DT Article
DE Adversarial learning; generative models; Recurrent neural networks;
   Motion capture data; Motion synthesis; Style transferring
AB We investigate how adversarial learning may be used for various animation tasks related to human motion synthesis. We propose a learning framework that we decline for building various models corresponding to various needs: a random synthesis generator that randomly produces realistic motion capture trajectories; conditional variants that allow controlling the synthesis by providing high-level features that the animation should match; a style transfer model that allows transforming an existing animation in the style of another one. Our work is built on the adversarial learning strategy that has been proposed in the machine learning field very recently (2014) for learning accurate generative models on complex data, and that has been shown to provide impressive results, mainly on image data. We report both objective and subjective evaluation results on motion capture data performed under emotion, the Emilya Dataset. Our results show the potential of our proposals for building models for a variety of motion synthesis tasks.
C1 [Wang, Qi; Artieres, Thierry] Aix Marseille Univ, Univ Toulon, CNRS, LIS, Marseille, France.
   [Wang, Qi; Artieres, Thierry] Ecole Cent Marseille, Marseille, France.
   [Chen, Mickael; Denoyer, Ludovic] Sorbonne Univ, CNRS, Lab Informat Paris 6, LIP6, F-75005 Paris, France.
C3 Centre National de la Recherche Scientifique (CNRS); Aix-Marseille
   Universite; Centre National de la Recherche Scientifique (CNRS);
   Sorbonne Universite
RP Artieres, T (corresponding author), Aix Marseille Univ, Univ Toulon, CNRS, LIS, Marseille, France.; Artieres, T (corresponding author), Ecole Cent Marseille, Marseille, France.
EM thierry.artieres@centrale-marseille.fr
RI wang, qi/AAU-6601-2020; artieres, thierry/E-9155-2019
OI artieres, thierry/0000-0003-3696-0321
CR [Anonymous], ESANN
   [Anonymous], ACM T GRAPHIC
   [Anonymous], ICML JMLR WORKSH C P
   [Anonymous], GENERATING SENTENCES
   [Anonymous], INVERSE KINEMATICS G
   [Anonymous], AUTOCONDITIONED RECU
   [Anonymous], A recurrent latent variable model for sequential data
   [Anonymous], GENERATIVE FACE COMP
   [Anonymous], 2012, ACM Transactions on Graphics (TOG)
   [Anonymous], GENERATIVE MODEL RAW
   [Anonymous], PLUG PLAY GENERATIV
   [Anonymous], AAMAS
   Brand M, 2000, COMP GRAPH, P183, DOI 10.1145/344779.344865
   Chen M., 2018, INT C LEARN REPR
   Chen Mickael, 2016, ABS161102019 CORR
   Chen Xi, 2016, Advances in Neural Information Processing Systems (NIPS), V29
   Cho Kyunghyun, 2014, SYNTAX SEMANTICS STR, P5, DOI [10.3115/v1/w14-4012, 10.3115 /v1/D14-1179, DOI 10.3115/V1/D14-1179]
   Chollet F, 2015, KERAS
   Chung Junyoung, 2014, ARXIV14123555
   Denton, 2017, ABS170510915 CORR
   Denton E.L., 2015, CoRR, P1486
   Fourati N, 2014, LREC 2014 - NINTH INTERNATIONAL CONFERENCE ON LANGUAGE RESOURCES AND EVALUATION, P3486
   Fragkiadaki K, 2015, IEEE I CONF COMP VIS, P4346, DOI 10.1109/ICCV.2015.494
   Gatys L., 2016, Journal of Vision, V16, P326, DOI DOI 10.1167/16.12.326
   Gleicher M., 1997, Proceedings 1997 Symposium on Interactive 3D Graphics, P139, DOI 10.1145/253284.253321
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Grochow K, 2004, ACM T GRAPHIC, V23, P522, DOI 10.1145/1015706.1015755
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Holden D, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073663
   Holden D, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925975
   Hsu E, 2005, ACM T GRAPHIC, V24, P1082, DOI 10.1145/1073204.1073315
   JAIN A, 2016, PROC CVPR IEEE, P5308, DOI DOI 10.1109/CVPR.2016.573
   Kingma D. P., 2013, ARXIV13126114
   Lample Guillaume, 2017, P ANN C NEUR INF PRO, P5967
   Makhzani A., 2015, ARXIV
   Mathieu M., 2016, ABS161103383 CORR
   Mirza M., 2014, ARXIV PREPRINT ARXIV, P2672, DOI 10.48550/arXiv.1411.1784
   Muller M., 2007, Tech. Rep. CG-2007-2
   Murray CE, 2013, RESPONDING TO FAMILY VIOLENCE: A COMPREHENSIVE, RESEARCH-BASED GUIDE FOR THERAPISTS, P1
   Pedregosa F, 2011, J MACH LEARN RES, V12, P2825
   Perarnau G., 2016, arXiv preprint arXiv: 1611.06355
   Radenen M, 2012, INT CONF ACOUST SPEE, P2113, DOI 10.1109/ICASSP.2012.6288328
   Shalev-Shwartz S, 2016, MATH PROGRAM, V155, P105, DOI 10.1007/s10107-014-0839-0
   Shapiro A, 2006, PROC GRAPH INTERF, P33
   Wang JM, 2008, IEEE T PATTERN ANAL, V30, P283, DOI 10.1109/TPAMI.2007.1167
   Wang Q, 2016, MOCO'16: PROCEEDINGS OF THE 3RD INTERNATIONAL SYMPOSIUM ON MOVEMENT AND COMPUTING, DOI 10.1145/2948910.2948958
   Wang Q, 2017, LECT NOTES ARTIF INT, V10498, P467, DOI 10.1007/978-3-319-67401-8_60
   Xia SH, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766999
   Yumer ME, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925955
   Zhu J.Y., Unpaired image-to-image translation using cycle-consistent adversarial networks
NR 50
TC 7
Z9 7
U1 0
U2 10
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2020
VL 36
IS 1
BP 141
EP 160
DI 10.1007/s00371-018-1594-7
PG 20
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KJ3OJ
UT WOS:000511966800012
DA 2024-07-18
ER

PT J
AU Liu, H
   Zhao, QJ
   Mbelwa, JT
   Tang, S
   Zhang, JW
AF Liu, Hao
   Zhao, Qingjie
   Mbelwa, Jimmy T.
   Tang, Song
   Zhang, Jianwei
TI Weighted two-step aggregated VLAD for image retrieval
SO VISUAL COMPUTER
LA English
DT Article
DE Content-based image retrieval; Image representation; VLAD; Local
   descriptors
AB The vector of locally aggregated descriptor (VLAD) has been demonstrated to be efficient and effective in image retrieval and classification tasks. Due to the small-size codebook adopted by the method, the feature space division is coarse and the discriminative power is limited. Toward a discriminative and compact image representation for visual search, we develop a novel aggregating method to build VLAD, called two-step aggregated VLAD. Firstly, we propose the bidirectional quantization from both views of descriptors and visual words, for getting finer division of feature space. Secondly, we impose the probabilistic inverse document frequency to weight the local descriptors, for highlighting the discriminative ones. Experimental results on extensive datasets show that our method yields significant improvement and is competitive with the state-of-the-art methods.
C1 [Liu, Hao; Zhao, Qingjie; Mbelwa, Jimmy T.] Beijing Inst Technol, Sch Comp Sci, Beijing Lab Intelligence Informat Technol, Beijing, Peoples R China.
   [Zhao, Qingjie; Tang, Song; Zhang, Jianwei] Univ Hamburg, Dept Informat, Hamburg, Germany.
C3 Beijing Institute of Technology; University of Hamburg
RP Liu, H (corresponding author), Beijing Inst Technol, Sch Comp Sci, Beijing Lab Intelligence Informat Technol, Beijing, Peoples R China.
EM liuhao3721@gmail.com
RI Zhang, Jianwei/HJA-0011-2022; Tang, Song/ITT-7528-2023
OI Zhang, Jianwei/0000-0002-5491-1745; Tang, Song/0000-0003-2635-1872
FU China Scholarship Council [201706035021]; National Natural Science
   Foundation of China [61175096]; German Research Foundation in Project
   Crossmodal Learning [TRR-169]; Chinese Government Scholarship under
   China Scholarship Council
FX This work was partly supported by the China Scholarship Council
   (201706035021), the National Natural Science Foundation of China
   (61175096), the German Research Foundation in Project Crossmodal
   Learning (TRR-169) and Chinese Government Scholarship under China
   Scholarship Council.
CR [Anonymous], 2010, Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on, IEEE, DOI [DOI 10.1109/CVPR.2010.5540018, 10.1109/CVPR.2010.5540018]
   [Anonymous], 2013, P 21 ACM INT C MULT, DOI 10.1145/2502081.2502171
   Arandjelovic R, 2018, IEEE T PATTERN ANAL, V40, P1437, DOI [10.1109/TPAMI.2017.2711011, 10.1109/CVPR.2016.572]
   Arandjelovic R, 2013, PROC CVPR IEEE, P1578, DOI 10.1109/CVPR.2013.207
   Arandjelovic R, 2012, PROC CVPR IEEE, P2911, DOI 10.1109/CVPR.2012.6248018
   Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1007/11744023_32
   Cai H., 2011, IEEE INT WORKSH MACH, P1, DOI DOI 10.1109/MLSP.2011.6064624
   Chen D, 2011, CONF REC ASILOMAR C, P850, DOI 10.1109/ACSSC.2011.6190128
   Chisholm Erica., 1999, NEW TERM WEIGHTING F
   Cho J, 2017, VISUAL COMPUT, V33, P1049, DOI 10.1007/s00371-017-1371-z
   Chum O, 2007, IEEE I CONF COMP VIS, P496, DOI 10.1109/cvpr.2007.383172
   Chum O, 2010, PROC CVPR IEEE, P3416, DOI 10.1109/CVPR.2010.5539997
   Everingham M, 2012, PASCAL VISUAL OBJECT
   Gong YC, 2014, LECT NOTES COMPUT SC, V8695, P392, DOI 10.1007/978-3-319-10584-0_26
   Jegou H, 2008, LECT NOTES COMPUT SC, V5302, P304, DOI 10.1007/978-3-540-88682-2_24
   Jégou H, 2012, IEEE T PATTERN ANAL, V34, P1704, DOI 10.1109/TPAMI.2011.235
   Jégou H, 2012, LECT NOTES COMPUT SC, V7573, P774, DOI 10.1007/978-3-642-33709-3_55
   Jégou H, 2010, PROC CVPR IEEE, P3304, DOI 10.1109/CVPR.2010.5540039
   Jégou H, 2009, PROC CVPR IEEE, P1169, DOI 10.1109/CVPRW.2009.5206609
   Joe Yue-Hei Ng, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P53, DOI 10.1109/CVPRW.2015.7301272
   Kim TE, 2015, J VIS COMMUN IMAGE R, V31, P237, DOI 10.1016/j.jvcir.2015.07.005
   Lazebnik S., 2006, P IEEE CVF C COMP VI, DOI [DOI 10.1109/CVPR.2006.68, 10.1109/CVPR.2006.68]
   Li HL, 2016, VISUAL COMPUT, V32, P1351, DOI 10.1007/s00371-016-1232-1
   Li Y, 2015, VISUAL COMPUT, V31, P1383, DOI 10.1007/s00371-014-1020-8
   Liu H, 2017, MULTIMED TOOLS APPL, V76, P24435, DOI 10.1007/s11042-016-4176-6
   Liu Z, 2016, IEEE T CIRC SYST VID, V26, P375, DOI 10.1109/TCSVT.2015.2409693
   Liu ZQ, 2016, NEUROCOMPUTING, V173, P1183, DOI 10.1016/j.neucom.2015.08.076
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Nister David, 2006, CVPR
   Paulin M, 2015, IEEE I CONF COMP VIS, P91, DOI 10.1109/ICCV.2015.19
   PERRONNIN F, 2010, PROC CVPR IEEE, P3384, DOI DOI 10.1109/CVPR.2010.5540009
   Perronnin F, 2010, LECT NOTES COMPUT SC, V6314, P143, DOI 10.1007/978-3-642-15561-1_11
   Philipp JB, 2008, 2008 9TH ANNUAL NON-VOLATILE MEMORY TECHNOLOGY SYMPOSIUM, PROCEEDINGS, P12
   Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663
   Spyromitros-Xioufis E, 2014, IEEE T MULTIMEDIA, V16, P1713, DOI 10.1109/TMM.2014.2329648
   Vedaldi A., 2010, P 18 ACM INT C MULT, P1469, DOI DOI 10.1145/1873951.1874249
   Wang D., 2012, Proceedings of the 12th ACM SIGMETRICS/PERFORMANCE joint international conference on Measurement and Modeling of Computer Systems, SIGMETRICS'12, P187, DOI DOI 10.1049/CP.2012.1749
   Zhao WL, 2016, IEEE T MULTIMEDIA, V18, P1843, DOI 10.1109/TMM.2016.2585023
   Zheng L, 2018, IEEE T PATTERN ANAL, V40, P1224, DOI 10.1109/TPAMI.2017.2709749
   Zheng L, 2014, PROC CVPR IEEE, P1947, DOI 10.1109/CVPR.2014.250
   Zhou QZ, 2016, ENTROPY-SWITZ, V18, DOI 10.3390/e18080311
   Zhou W., 2017, ARXIV170606064 CORR
NR 42
TC 3
Z9 3
U1 0
U2 12
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2019
VL 35
IS 12
BP 1783
EP 1795
DI 10.1007/s00371-018-1573-z
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KI6XQ
UT WOS:000511494300008
DA 2024-07-18
ER

PT J
AU Cornelis, J
   Bender, J
   Gissler, C
   Ihmsen, M
   Teschner, M
AF Cornelis, Jens
   Bender, Jan
   Gissler, Christoph
   Ihmsen, Markus
   Teschner, Matthias
TI An optimized source term formulation for incompressible SPH
SO VISUAL COMPUTER
LA English
DT Article
DE Three-dimensional graphics and realism; Animation; Fluid simulation
ID SIMULATION; FLOWS; MODEL
AB Incompressible SPH (ISPH) is a promising concept for the pressure computation in SPH. It works with large timesteps and the underlying pressure Poisson equation (PPE) can be solved very efficiently. Still, various aspects of current ISPH formulations can be optimized. This paper discusses issues of the two standard source terms that are typically employed in PPEs, i.e., density invariance (DI) and velocity divergence (VD). We show that the DI source term suffers from significant artificial viscosity, while the VD source term suffers from particle disorder and volume loss. As a conclusion of these findings, we propose a novel source term handling. A first PPE is solved with the VD source term to compute a divergence-free velocity field with minimized artificial viscosity. To address the resulting volume error and particle disorder, a second PPE is solved to improve the sampling quality. The result of the second PPE is used for a particle shift (PS) only. The divergence-free velocity fieldcomputed from the first PPEis not changed, but only resampled at the updated particle positions. Thus, the proposed source term handling incorporates velocity divergence and particle shift (VD+PS). The proposed VD+PS variant does not only improve the quality of the computed velocity field, but also accelerates the performance of the ISPH pressure computation. This is illustrated for IISPHa recent ISPH implementationwhere a performance gain factor of 1.6 could be achieved.
C1 [Cornelis, Jens; Ihmsen, Markus] FIFTY2 Technol GmbH, Freiburg, Germany.
   [Bender, Jan] Rhein Westfal TH Aachen, Aachen, Germany.
   [Gissler, Christoph; Teschner, Matthias] Univ Freiburg, Freiburg, Germany.
C3 RWTH Aachen University; University of Freiburg
RP Cornelis, J (corresponding author), FIFTY2 Technol GmbH, Freiburg, Germany.
EM mail@jenscornelis.de
CR Adams B, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1276377.1276437, 10.1145/1239451.1239499]
   Akinci N, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508395
   Akinci N, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185558
   Becker M, 2007, SYMPOSIUM ON COMPUTER ANIMATION 2007: ACM SIGGRAPH/ EUROGRAPHICS SYMPOSIUM PROCEEDINGS, P209
   Bender J., 2015, P 2015 ACM SIGGRAPH
   Bender J, 2014, COMPUT GRAPH FORUM, V33, P228, DOI 10.1111/cgf.12346
   Bodin K, 2012, IEEE T VIS COMPUT GR, V18, P516, DOI 10.1109/TVCG.2011.29
   CHORIN AJ, 1968, MATH COMPUT, V22, P745, DOI 10.2307/2004575
   Cline MB, 2003, IEEE INT CONF ROBOT, P3744
   Cornelis J, 2014, COMPUT GRAPH FORUM, V33, P255, DOI 10.1111/cgf.12324
   Cummins SJ, 1999, J COMPUT PHYS, V152, P584, DOI 10.1006/jcph.1999.6246
   F_urstenau J.-P., 2017, P 12 INT SPHER WORKS, P1
   FIFTY2 Technology GmbH: PreonLab, 2017, FIFTY2 TECHNOLOGY GM
   Geier M., 2004, NSTI NAN C TRAD SHOW, P55258
   HE X, 1958, FORUM, V31, P1948, DOI DOI 10.1111/j.1467-8659.2012.03074.x
   Hess B, 2002, J CHEM PHYS, V116, P209, DOI 10.1063/1.1421362
   Hu XY, 2007, J COMPUT PHYS, V227, P264, DOI 10.1016/j.jcp.2007.07.013
   Ihmsen M., 2010, P VRIPHYS, P79
   Ihmsen M, 2014, IEEE T VIS COMPUT GR, V20, P426, DOI 10.1109/TVCG.2013.105
   Ihmsen M, 2011, COMPUT GRAPH FORUM, V30, P99, DOI [10.1111/j.1467-8659.2010.01832.x, 10.1111/j.1467-8659.2010.01834.x]
   Ihmsen Markus, 2014, P 35 ANN C EUR ASS C, DOI [10.2312/egst.20141034, DOI 10.2312/EGST.20141034]
   Kang N, 2014, COMPUT GRAPH FORUM, V33, P219, DOI 10.1111/cgf.12490
   Khayyer A, 2009, APPL OCEAN RES, V31, P111, DOI 10.1016/j.apor.2009.06.003
   Macklin M., 2013, ACM T GRAPHIC, V32
   MONAGHAN JJ, 1992, ANNU REV ASTRON ASTR, V30, P543, DOI 10.1146/annurev.aa.30.090192.002551
   MONAGHAN JJ, 1994, J COMPUT PHYS, V110, P399, DOI 10.1006/jcph.1994.1034
   Morris JP, 1997, J COMPUT PHYS, V136, P214, DOI 10.1006/jcph.1997.5776
   Muller M., 2003, ACM SIGGRAPH/Eurographics Symposium on Computer Animation, P154
   Nestor R, 2008, SPHERIC, P118
   Peer A, 2018, COMPUT GRAPH FORUM, V37, P135, DOI 10.1111/cgf.13317
   Peer A, 2017, IEEE T VIS COMPUT GR, V23, P2656, DOI 10.1109/TVCG.2016.2636144
   Peer A, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766925
   Shao SD, 2003, ADV WATER RESOUR, V26, P787, DOI 10.1016/S0309-1708(03)00030-7
   Skillen A, 2013, COMPUT METHOD APPL M, V265, P163, DOI 10.1016/j.cma.2013.05.017
   Solenthaler B, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531346
   Takahashi T, 2018, COMPUT GRAPH FORUM, V37, P313, DOI 10.1111/cgf.13292
   Tartakovsky AM, 2016, J COMPUT PHYS, V305, P1119, DOI 10.1016/j.jcp.2015.08.037
   Winchenbach R., 2016, P ACM SIGGRAPHEUROGR, P49, DOI [10.2312/sca.20161222, DOI 10.2312/SCA.20161222]
   Winchenbach R, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073713
   Xu R, 2009, J COMPUT PHYS, V228, P6703, DOI 10.1016/j.jcp.2009.05.032
   Yan X, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925897
   Yang T., 2016, EUR ACM SIGGRAPH S C
   Yang T, 2017, IEEE T VIS COMPUT GR, V23, P2235, DOI 10.1109/TVCG.2017.2706289
NR 43
TC 16
Z9 18
U1 3
U2 14
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2019
VL 35
IS 4
BP 579
EP 590
DI 10.1007/s00371-018-1488-8
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HS2EB
UT WOS:000463672800009
DA 2024-07-18
ER

PT J
AU Bui, G
   Le, T
   Morago, B
   Duan, Y
AF Bui, Giang
   Le, Truc
   Morago, Brittany
   Duan, Ye
TI Point-based rendering enhancement via deep learning
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 35th Computer Graphics International conference (CGI)
CY JUN 11-14, 2018
CL INDONESIA
SP Comp Graph Soc, Nanyang Technol Univ
DE Point-based rendering; Deep learning; Super resolution
ID IMAGE SUPERRESOLUTION
AB Current state-of-the-art point rendering techniques such as splat rendering generally require very high-resolution point clouds in order to create high-quality photo realistic renderings. These can be very time consuming to acquire and oftentimes also require high-end expensive scanners. This paper proposes a novel deep learning-based approach that can generate high-resolution photo realistic point renderings from low-resolution point clouds. More specifically, we propose to use co-registered high-quality photographs as the ground truth data to train the deep neural network for point-based rendering. The proposed method can generate high-quality point rendering images very efficiently and can be used for interactive navigation of large-scale 3D scenes as well as image-based localization. Extensive quantitative evaluations on both synthetic and real datasets show that the proposed method outperforms state-of-the-art methods.
C1 [Bui, Giang; Le, Truc; Duan, Ye] Univ Missouri, Columbia, MO 65211 USA.
   [Morago, Brittany] Univ North Carolina Wilmington, Dept Comp Sci, Wilmington, NC USA.
C3 University of Missouri System; University of Missouri Columbia;
   University of North Carolina; University of North Carolina Wilmington
RP Duan, Y (corresponding author), Univ Missouri, Columbia, MO 65211 USA.
EM duanye@missouri.edu
OI Duan, Ye/0000-0002-1166-7703
CR Abadi, 2015, TENSORFLOW LARGE SCA
   [Anonymous], 2015, ARXIV151104587
   Botsch Mario, 2005, P EUROGRAPHICSIEEE V, P17, DOI [DOI 10.2312/SPBG/SPBG05/017-024, 10.1109/PBG.2005.194059.6]
   Brown M, 2005, FIFTH INTERNATIONAL CONFERENCE ON 3-D DIGITAL IMAGING AND MODELING, PROCEEDINGS, P56, DOI 10.1109/3DIM.2005.81
   Chang H, 2004, PROC CVPR IEEE, P275, DOI 10.1109/cvpr.2004.1315043
   Cui Z, 2014, LECT NOTES COMPUT SC, V8693, P49, DOI 10.1007/978-3-319-10602-1_4
   Dai D, 2015, COMPUT GRAPH FORUM, V34, P95, DOI 10.1111/cgf.12544
   Denton E, 2015, ADV NEUR IN, V28
   Dong C, 2016, LECT NOTES COMPUT SC, V9906, P391, DOI 10.1007/978-3-319-46475-6_25
   Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281
   Freedman G, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1944846.1944852
   Glasner D, 2009, IEEE I CONF COMP VIS, P349, DOI 10.1109/ICCV.2009.5459271
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Hartley R, 2003, MULTIPLE VIEW GEOMET, DOI 10.1016/S0143-8166(01)00145-2
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   HUANG JB, 2015, PROC CVPR IEEE, P5197, DOI DOI 10.1109/CVPR.2015.7299156
   Irschara A, 2009, PROC CVPR IEEE, P2591, DOI 10.1109/CVPRW.2009.5206587
   Jia K, 2013, IEEE T PATTERN ANAL, V35, P367, DOI 10.1109/TPAMI.2012.95
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kim J., 2015, ARXIV151104491
   Kim KI, 2010, IEEE T PATTERN ANAL, V32, P1127, DOI 10.1109/TPAMI.2010.25
   Kingma D. P., 2014, arXiv
   Kobbelt L, 2004, COMPUT GRAPH-UK, V28, P801, DOI 10.1016/j.cag.2004.08.009
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Lee CY, 2015, JMLR WORKSH CONF PRO, V38, P562
   Li YP, 2010, LECT NOTES COMPUT SC, V6312, P791
   Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151
   Lipponer S., 2015, SURFACE SPLATTING
   Liu Y, 2008, COMPUT AIDED DESIGN, V40, P576, DOI 10.1016/j.cad.2008.02.004
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Mathieu M., 2015, PROC INT C LEARN REP
   Nistér D, 2004, IEEE T PATTERN ANAL, V26, P756, DOI 10.1109/TPAMI.2004.17
   Qi C. R., 2017, IEEE P COMPUT VIS PA, V1, P4, DOI DOI 10.1109/CVPR.2017.16
   Radford A., 2015, ARXIV
   Savva M., 2016, P EUR WORKSH 3D OBJ, P89, DOI DOI 10.2312/3DOR.20161092
   Schulter S, 2015, PROC CVPR IEEE, P3791, DOI 10.1109/CVPR.2015.7299003
   Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207
   Sibbing D, 2013, 2013 INTERNATIONAL CONFERENCE ON 3D VISION (3DV 2013), P56, DOI 10.1109/3DV.2013.16
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Snavely N., ACM T GRAPH TOG, V25, P835
   Timofte R, 2017, IEEE COMPUT SOC CONF, P1110, DOI 10.1109/CVPRW.2017.149
   Timofte R, 2016, PROC CVPR IEEE, P1865, DOI 10.1109/CVPR.2016.206
   Timofte R, 2015, LECT NOTES COMPUT SC, V9006, P111, DOI 10.1007/978-3-319-16817-3_8
   Timofte R, 2013, IEEE I CONF COMP VIS, P1920, DOI 10.1109/ICCV.2013.241
   Vinyals O., 2015, ICLR
   Xie SN, 2015, IEEE I CONF COMP VIS, P1395, DOI [10.1109/ICCV.2015.164, 10.1007/s11263-017-1004-z]
   Yang CY, 2014, LECT NOTES COMPUT SC, V8692, P372, DOI 10.1007/978-3-319-10593-2_25
   Yang CY, 2011, LECT NOTES COMPUT SC, V6494, P497, DOI 10.1007/978-3-642-19318-7_39
   Yang JQ, 2008, ITESS: 2008 PROCEEDINGS OF INFORMATION TECHNOLOGY AND ENVIRONMENTAL SYSTEM SCIENCES, PT 1, P1, DOI 10.1109/CVPR.2008.4587647
   Yang JC, 2012, IEEE T IMAGE PROCESS, V21, P3467, DOI 10.1109/TIP.2012.2192127
   Yang JC, 2010, IEEE T IMAGE PROCESS, V19, P2861, DOI 10.1109/TIP.2010.2050625
   Zwicker M, 2001, COMP GRAPH, P371, DOI 10.1145/383259.383300
NR 52
TC 17
Z9 22
U1 0
U2 14
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2018
VL 34
IS 6-8
SI SI
BP 829
EP 841
DI 10.1007/s00371-018-1550-6
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA GH6MC
UT WOS:000433557400008
DA 2024-07-18
ER

PT J
AU Hua, H
   Jia, TL
AF Hua, Hao
   Jia, Tingli
TI Wire cut of double-sided minimal surfaces
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 35th Computer Graphics International conference (CGI)
CY JUN 11-14, 2018
CL INDONESIA
SP Comp Graph Soc, Nanyang Technol Univ
DE Minimal surface; Weierstrass parameterization; Wire cut;
   Quadrangulation; CNC
AB We present a systematic method for producing double-sided minimal surfaces by wire-cut machines. A link between minimal surfaces and ruled surfaces is pursued through wire cutting. Weierstrass parameterization is employed to define minimal surfaces () over a complex plane (). Our method consists of three components. First, the orthogonal double-sided cuts match a pair of orthonormal tangent vectors on the surface. Second, A closed-form expression for the principal directions facilitates the global quadrangulation of minimal surfaces. Third, the CNC machine's toolpath results from the surface's analytic characterization. Asymptotic cutting and principal cutting are compared in terms of collisions and cutting error. We employed a general-purpose language (Java) to create machine instructions from the Weierstrass representation of minimal surfaces. Thus, the entire workflow from mathematical modeling to production involves no 3D models or CAD/CAM software. Both a 5-axis wire cutter and a customized robotic system were tested.
C1 [Hua, Hao; Jia, Tingli] Southeast Univ, Key Lab Urban & Architectural Heritage Conservat, Minist Educ, Nanjing, Jiangsu, Peoples R China.
   [Hua, Hao; Jia, Tingli] Southeast Univ, Sch Architecture, 2 Sipailou, Nanjing 210096, Jiangsu, Peoples R China.
C3 Southeast University - China; Southeast University - China
RP Hua, H (corresponding author), Southeast Univ, Key Lab Urban & Architectural Heritage Conservat, Minist Educ, Nanjing, Jiangsu, Peoples R China.; Hua, H (corresponding author), Southeast Univ, Sch Architecture, 2 Sipailou, Nanjing 210096, Jiangsu, Peoples R China.
EM whitegreen@163.com; jtl9855008@163.com
OI Hua, Hao/0000-0001-5988-7767
FU National Natural Science Foundation of China [51778118, 51478116,
   51538006, 51578123]; Ministry of Housing and Urban-Rural Development of
   China [UDC2017020212]
FX This research is supported by National Natural Science Foundation of
   China (51778118, 51478116, 51538006, 51578123) and by Ministry of
   Housing and Urban-Rural Development of China (UDC2017020212).
CR Alliez P, 2003, ACM T GRAPHIC, V22, P485, DOI 10.1145/882262.882296
   ANDERSSON S, 1988, CHEM REV, V88, P221, DOI 10.1021/cr00083a011
   [Anonymous], NASATND5541
   Bo P, 2016, COMPUT AIDED DESIGN, V79, P1, DOI 10.1016/j.cad.2016.04.004
   Brander D, 2016, ADVANCES IN ARCHITECTURAL GEOMETRY 2016, P306
   Carberry E., 2005, LECT NOTES MINIMAL S
   Chen W., 2008, COMPUT AIDED DESIGN, V5, P508, DOI DOI 10.3722/CADAPS.2008.508-518
   Crane K, 2010, COMPUT GRAPH FORUM, V29, P1525, DOI 10.1111/j.1467-8659.2010.01761.x
   Dierkes U., 2010, MINIMAL SURFACES, P91
   Eigensatz M, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778782
   Evans ME, 2013, ACTA CRYSTALLOGR A, V69, P241, DOI 10.1107/S0108767313001670
   Feringa J., 2014, FABRICATE NEGOTIATIN, P77
   Flory S., 2010, P 30 ANN C ASS COMPU, P103
   Fogden A, 1999, EUR PHYS J B, V7, P91, DOI 10.1007/s100510050592
   Gandy P. J., CHEM PHYS LETT, V322, P579
   Gandy PJF, 2000, CHEM PHYS LETT, V321, P363, DOI 10.1016/S0009-2614(00)00373-0
   Graig J. J., 2005, INTRO ROBOTICS MECH
   Gramazio F., 2014, Fabricate: negotiating design making
   Gray A., 2006, STUDIES ADV MATH, Vthird
   Harik RF, 2013, COMPUT AIDED DESIGN, V45, P796, DOI 10.1016/j.cad.2012.08.004
   Hua H., 2016, JAVAKUKA OPEN SOURCE
   Hua H., 2017, ROBOTIC WIRE CUT
   Hyde ST, 2015, INTERFACE FOCUS, V5, DOI 10.1098/rsfs.2015.0027
   Knöppel F, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2462005
   Kreyszig E., 1991, Differential Geometry. Differential Geometry
   Lawden D.F, 2013, ELLIPTIC FUNCTIONS A, V80
   McGee W., 2012, ROB ARCH 2012 ROBOTI, P62
   Meeks WH, 2011, B AM MATH SOC, V48, P325, DOI 10.1090/S0273-0979-2011-01334-9
   Nitsche J. C. C., 1989, LECT MINIMAL SURFACE, V1
   Osserman R., 2002, SURVEY MINIMAL SURFA
   Otto F., 1996, FINDING FORM ARCHITE
   Pottmann H, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360675
   Sharma R., 2012, ARXIV12085689
   Steenstrup KH, 2016, ADVANCES IN ARCHITECTURAL GEOMETRY 2016, P328
   Taubin G., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P351, DOI 10.1145/218380.218473
   Terrones H., 1990, J PHYS C SOLID STATE, V51, P345, DOI DOI 10.1051/JPHYSCOL:1990735
NR 36
TC 3
Z9 3
U1 0
U2 10
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2018
VL 34
IS 6-8
SI SI
BP 985
EP 995
DI 10.1007/s00371-018-1548-0
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA GH6MC
UT WOS:000433557400021
DA 2024-07-18
ER

PT J
AU Shi, JL
   Sun, ZX
   Bai, SQ
AF Shi, Jinlong
   Sun, Zhengxing
   Bai, Suqin
TI 3D reconstruction framework via combining one 3D scanner and multiple
   stereo trackers
SO VISUAL COMPUTER
LA English
DT Article
DE Large; 3D; Reconstruction; Stereo; Tracker
ID SCALE 3-DIMENSIONAL MEASUREMENT; TRACKING
AB This paper presents a novel 3D reconstruction framework of large objects, where we adopt one 3D scanner to reconstruct partial sections of large objects, and employ multiple stereo trackers to extend reconstruction range. Both the 3D scanner and stereo trackers are fitted with infrared light-emitting diode (LED) lights. During reconstruction, the stereo trackers are placed one after another, their poses are estimated according to the LED lights, the 3D scanner is moved to reconstruct partial sections of a large object, and the LED lights on the 3D scanner are tracked by the stereo trackers to compute the poses of the 3D scanner for partial alignment. The experimental results show that this proposed method can accurately and effectively reconstruct large objects, and has its advantages for long-range reconstruction compared with similar existing methods.
C1 [Shi, Jinlong; Sun, Zhengxing] Nanjing Univ, State Key Lab Novel Software Technol, Nanjing, Jiangsu, Peoples R China.
   [Shi, Jinlong; Bai, Suqin] Jiangsu Univ Sci & Technol, Sch Comp Sci & Engn, Zhenjiang, Peoples R China.
C3 Nanjing University; Jiangsu University of Science & Technology
RP Sun, ZX (corresponding author), Nanjing Univ, State Key Lab Novel Software Technol, Nanjing, Jiangsu, Peoples R China.
EM jlshifudan@gmail.com; szx@nju.edu.cn; 1067694928@qq.com
RI shi, jin/JCD-8826-2023; Shi, JIn/JYP-1805-2024; Sun,
   Zhengxing/A-7411-2011; shi, jin/KDO-7906-2024
FU China Postdoctoral Science Foundation [2014M560417]; National Natural
   Science Foundation of China [61272219, 61100110, 61321491]; National
   High Technology Research and Development Program of China
   [2007AA01Z334]; Key Projects Innovation Fund of State Key Laboratory
   [ZZKT2013A12]; Program for New Century Excellent Talents in University
   of China [NCET-04-04605]; Graduate Training Innovative Projects
   Foundation of Jiangsu Province [CXLX13 050]; Science and Technology
   Program of Jiangsu Province [BE2010072, BE2011058, BY2012190]
FX This work is supported by General Financial Grant from the China
   Postdoctoral Science Foundation No. 2014M560417; the National Natural
   Science Foundation of China Nos. 61272219, 61100110, 61321491; the
   National High Technology Research and Development Program of China No.
   2007AA01Z334; the Key Projects Innovation Fund of State Key Laboratory
   No. ZZKT2013A12; the Program for New Century Excellent Talents in
   University of China No. NCET-04-04605; the Graduate Training Innovative
   Projects Foundation of Jiangsu Province No. CXLX13 050; the Science and
   Technology Program of Jiangsu Province Nos. BE2010072, BE2011058,
   BY2012190.
CR Agarwal S, 2011, COMMUN ACM, V54, P105, DOI 10.1145/2001269.2001293
   Barone S, 2012, MACH VISION APPL, V23, P217, DOI 10.1007/s00138-011-0340-1
   Barone S, 2012, OPT LASER ENG, V50, P380, DOI 10.1016/j.optlaseng.2011.10.019
   Bylow E., 2013, ROBOTICS, V9
   Chen J., ACM T GRAPH TOG, V32, P113
   Cui Y, 2010, PROC CVPR IEEE, P1173, DOI 10.1109/CVPR.2010.5540082
   Furukawa Y, 2010, PROC CVPR IEEE, P1434, DOI 10.1109/CVPR.2010.5539802
   Hartley R., 2003, MULTIPLE VIEW GEOMET
   Henry P, 2012, INT J ROBOT RES, V31, P647, DOI 10.1177/0278364911434148
   Iddan GJ, 2001, P SOC PHOTO-OPT INS, V4298, P48, DOI 10.1117/12.424913
   Jeon J, 2016, VISUAL COMPUT, V32, P955, DOI 10.1007/s00371-016-1249-5
   Komodakis N, 2009, VISUAL COMPUT, V25, P117, DOI 10.1007/s00371-008-0209-0
   Kurazume R, 2007, 3DIM 2007: SIXTH INTERNATIONAL CONFERENCE ON 3-D DIGITAL IMAGING AND MODELING, PROCEEDINGS, P91
   Lucas B.D., 1981, IJCAI, V81, P674
   Newcombe RA, 2011, INT SYM MIX AUGMENT, P127, DOI 10.1109/ISMAR.2011.6092378
   Paoli A, 2012, ROBOT CIM-INT MANUF, V28, P592, DOI 10.1016/j.rcim.2012.02.010
   Schuon S, 2009, PROC CVPR IEEE, P343, DOI 10.1109/CVPRW.2009.5206804
   Shan Q, 2013, 2013 INTERNATIONAL CONFERENCE ON 3D VISION (3DV 2013), P25, DOI 10.1109/3DV.2013.12
   Shi JL, 2016, VISUAL COMPUT, V32, P179, DOI 10.1007/s00371-015-1063-5
   Shi JL, 2015, APPL OPTICS, V54, P2814, DOI 10.1364/AO.54.002814
   Shi JL, 2013, OPT ENG, V52, DOI 10.1117/1.OE.52.12.123104
   Shim H, 2012, VISUAL COMPUT, V28, P1139, DOI 10.1007/s00371-011-0664-x
   Song X., VIS COMPUT, V30, P855
   Stringa E, 2000, IEEE T IMAGE PROCESS, V9, P69, DOI 10.1109/83.817599
   Tomasi Carlo, 1991, Image Rochester NY, DOI DOI 10.1016/S0031-3203(03)00234-6
   Xiao JX, 2012, LECT NOTES COMPUT SC, V7572, P668, DOI [10.1007/s11263-014-0711-y, 10.1007/978-3-642-33718-5_48]
   Yahav G, 2007, IEEE ICCE, P417
   Zhang ZY, 2000, IEEE T PATTERN ANAL, V22, P1330, DOI 10.1109/34.888718
   Zhu C, 2013, VISUAL COMPUT, V29, P609, DOI 10.1007/s00371-013-0827-z
NR 29
TC 16
Z9 18
U1 2
U2 25
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2018
VL 34
IS 3
BP 377
EP 389
DI 10.1007/s00371-016-1339-4
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FW2CZ
UT WOS:000425110900006
DA 2024-07-18
ER

PT J
AU Wu, W
   Li, HP
   Su, TY
   Liu, HX
   Lv, ZH
AF Wu, Wei
   Li, Hongping
   Su, Tianyun
   Liu, Haixing
   Lv, Zhihan
TI GPU-accelerated SPH fluids surface reconstruction using two-level
   spatial uniform grids
SO VISUAL COMPUTER
LA English
DT Article
DE Smoothed particle hydrodynamics; Fluids simulation; Surface
   reconstruction; Cuckoo hashing
ID PARTICLE-BASED SIMULATION; MODEL
AB An efficient two-level spatial uniform grid structure-based high-quality surface reconstruction method with Marching Cubes (MC) for smoothed particle hydrodynamics (SPH) fluids was presented in this paper. Compared with the traditional way that dividing the simulation domain with uniform grid directly, an enhanced narrow-band approach using the parallel cuckoo hashing method was taken to index the coarse-level surface vertices, hence decrease the memory consumption. Moreover, a two-level spatial uniform grid structure was employed with a scheme of arranging the fine surface vertices, which could preserve the spatial locality property to facilitate the coalesced memory access on the GPU. Our algorithm was designed for parallel architectures, based on which a parallel version of the optimized surface reconstruction was performed on the CUDA platform. In the experiment of comparison to traditional approaches, the results indicated that our surface reconstruction method was more efficient at the same level of quality of the reconstructed surfaces.
C1 [Wu, Wei; Li, Hongping] Ocean Univ China, Coll Informat Sci & Engn, Qingdao 266100, Peoples R China.
   [Su, Tianyun; Liu, Haixing] SOA, Inst Oceanog 1, Qingdao 266061, Peoples R China.
   [Lv, Zhihan] Chinese Acad Sci, Shenzhen Res Inst Adv Technol, Shenzhen 518055, Peoples R China.
C3 Ocean University of China; First Institute of Oceanography, Ministry of
   Natural Resources; Chinese Academy of Sciences; Shenzhen Institute of
   Advanced Technology, CAS
RP Li, HP (corresponding author), Ocean Univ China, Coll Informat Sci & Engn, Qingdao 266100, Peoples R China.
EM oucws2011@163.com; lhp@ouc.edu.cn; sutiany@fio.org.cn
RI Lv, Zhihan/GLR-6000-2022; Lyu, Zhihan/I-3187-2014
OI Lv, Zhihan/0000-0003-2525-3074; Lyu, Zhihan/0000-0003-2525-3074
FU National Natural Science Foundation of China [41275013]; National
   High-Tech Research and Development Program (863) [2013AA09A506-4]
FX This work was jointly supported by the National Natural Science
   Foundation of China (Grant No. 41275013) and the National High-Tech
   Research and Development Program (863) (Grant No. 2013AA09A506-4).
CR Adams B, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1276377.1276437, 10.1145/1239451.1239499]
   Akinci G, 2012, COMPUT GRAPH FORUM, V31, P1797, DOI 10.1111/j.1467-8659.2012.02096.x
   Akinci G, 2013, WSCG 2013, FULL PAPERS PROCEEDINGS, P195
   Alcantara DA, 2009, ACM T GRAPHIC, V28, DOI [10.1145/1618452.1610500, 10.1145/1618452.1618500]
   [Anonymous], P SIGGRAPH 2004 C SK
   [Anonymous], CUDA C PROGRAMMING G
   [Anonymous], 2012, WORKSH VIRT REAL INT, DOI DOI 10.2312/PE/VRIPHYS/VRIPHYS12/061-068
   [Anonymous], P VOL GRAPH
   Bagar F, 2010, COMPUT GRAPH FORUM, V29, P1383, DOI 10.1111/j.1467-8659.2010.01734.x
   Blinn J. F., 1982, Computer Graphics, V16, DOI 10.1145/965145.801290
   Bridson R.E., 2003, COMPUTATIONAL ASPECT
   Du S., 2014, 22 INT C CENTRAL EUR, P141
   Goswami P., 2010, P 2010 ACM SIGGRAPHE, P55
   Gribble CP, 2007, IEEE T VIS COMPUT GR, V13, P758, DOI 10.1109/TVCG.2007.1059
   Ihmsen M., 2012, VISUAL COMPUT, V30, P99
   Ihmsen M, 2011, COMPUT GRAPH FORUM, V30, P99, DOI [10.1111/j.1467-8659.2010.01832.x, 10.1111/j.1467-8659.2010.01834.x]
   Ju T., 2006, P 14 PAC C COMP GRAP
   Kanamori Y, 2008, COMPUT GRAPH FORUM, V27, P351, DOI 10.1111/j.1467-8659.2008.01132.x
   Lee H., 2004, REAL TIME MARCHING C
   Lorensen W. E., 1987, COMPUTER GRAPHICS, V21, P163, DOI 10.1145/37401.37422
   Manson J, 2010, COMPUT GRAPH FORUM, V29, P377, DOI 10.1111/j.1467-8659.2009.01607.x
   Müller M, 2007, J VIS COMMUN IMAGE R, V18, P109, DOI 10.1016/j.jvcir.2007.01.005
   Muller M., 2003, ACM SIGGRAPH/Eurographics Symposium on Computer Animation, P154
   Nielsen MB, 2006, J SCI COMPUT, V26, P261, DOI 10.1007/s10915-005-9062-8
   Nielsen MB, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1289603.1289607
   Onderik J., 2013, P 27 SPRING C COMP G, P29, DOI [10.1145/2461217.2461224, DOI 10.1145/2461217.2461224]
   Orthmann J., 2010, Vision, Modeling, and Visualization, P147, DOI [10.2312/PE/VMV/VMV10, DOI 10.2312/PE/VMV/VMV10]
   Premoze S, 2003, COMPUT GRAPH FORUM, V22, P401, DOI 10.1111/1467-8659.00687
   Solenthaler B, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531346
   Solenthaler B, 2007, COMPUT ANIMAT VIRT W, V18, P69, DOI 10.1002/cav.162
   Velasco F., 2001, P VIS MOD VIS C, P151
   Yu J., 2012, EUROGRAPHICS, V2012, P41
   Yu JH, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2421636.2421641
   Zhang Yanci., 2008, Proceedings of the Fifth Euro- graphics / IEEE VGTC Conference on Point-Based Graphics, SPBG'08, P137
   Zhou K, 2011, IEEE T VIS COMPUT GR, V17, P669, DOI 10.1109/TVCG.2010.75
   Zhu YN, 2005, ACM T GRAPHIC, V24, P965, DOI 10.1145/1073204.1073298
NR 36
TC 11
Z9 13
U1 0
U2 13
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2017
VL 33
IS 11
BP 1429
EP 1442
DI 10.1007/s00371-016-1289-x
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FJ0TP
UT WOS:000412423100006
DA 2024-07-18
ER

PT J
AU Brich, J
   Rogers, K
   Frommel, J
   Weidhaas, M
   Brückner, A
   Mirabile, S
   Dorn, T
   Riemer, V
   Schrader, C
   Weber, M
AF Brich, Julia
   Rogers, Katja
   Frommel, Julian
   Weidhaas, Martin
   Brueckner, Adrian
   Mirabile, Sarah
   Dorn, Tamara
   Riemer, Valentin
   Schrader, Claudia
   Weber, Michael
TI LiverDefense: how to employ a tower defense game as a customisable
   research tool
SO VISUAL COMPUTER
LA English
DT Article
DE Educational games; Game design; Bio-medical education; Questionnaire
   integration; Customisable games
ID VIDEO GAMES; SIMULATION; EDUCATION; HEALTH; COMPUTER; SURGERY; DESIGN;
   REAL
AB In game-related research, it is often necessary to create different versions of a game prototype and gather information about players. To make this possible even for non-programmers, we present LiverDefense, an educational Tower Defense game about the basic functions of the human liver, which can be used as a customisable research tool. LiverDefense can be customised via human-readable XML files both in its degree of difficulty and the content of Likert scale questionnaires to be presented to the player. As a proof of concept, LiverDefense has been successfully employed in a psychological study focused on exploring the effect of perceived control over gameplay on players' emotions. We report on the analysis of this study with regard to enjoyment and frustration and the resulting insights on using LiverDefense as a customisable research tool.
C1 [Brich, Julia; Rogers, Katja; Frommel, Julian; Weber, Michael] Ulm Univ, Inst Media Informat, Ulm, Germany.
   [Riemer, Valentin; Schrader, Claudia] Ulm Univ, Inst Psychol & Educ, Serious Games Grp, Ulm, Germany.
   [Weidhaas, Martin] Ulm Univ, Media Informat, Ulm, Germany.
   [Brueckner, Adrian; Mirabile, Sarah; Dorn, Tamara] Ulm Univ, Ulm, Germany.
C3 Ulm University; Ulm University; Ulm University; Ulm University
RP Brich, J (corresponding author), Ulm Univ, Inst Media Informat, Ulm, Germany.
EM julia.brich@uni-ulm.de; katja.rogers@uni-ulm.de;
   julian.frommel@uni-ulm.de; martin.weidhaas@uni-ulm.de;
   adrian.brueckner@uni-ulm.de; mirabile.sarah@live.it; tamara-dorn@gmx.de;
   valentin.riemer@uni-ulm.de; claudia.schrader@uni-ulm.de;
   michael.weber@uni-ulm.de
RI Riemer, Valentin/J-4161-2017; Weber, Michael/F-2558-2013; Frommel,
   Julian/JVM-8038-2024
OI Frommel, Julian/0000-0001-8783-7783; Weber, Michael/0000-0002-2692-5568;
   Rogers, Katja/0000-0002-5958-3576
FU Carl Zeiss Foundation - BMBF (German Federal Ministry of Education and
   Research) within the program "Aufstieg durch Bildung: Offene
   Hochschulen" [FKZ: 16OH21032]
FX This work was conducted within the project "Serious Games-Skill
   Advancement Through Adaptive Systems" being funded by the Carl Zeiss
   Foundation and as part of the project "EffIS-Efficient and Interactive
   Studying" (FKZ: 16OH21032) funded since 2014 by the BMBF (German Federal
   Ministry of Education and Research) within the program "Aufstieg durch
   Bildung: Offene Hochschulen".
CR Avery P, 2011, IEEE C EVOL COMPUTAT, P1084
   Baranowski T, 2008, AM J PREV MED, V34, P74, DOI 10.1016/j.amepre.2007.09.027
   Bassilious E., 2011, 2011 IEEE International Games Innovation Conference (IGIC 2011), P124, DOI 10.1109/IGIC.2011.6115113
   Bassilious E., 2013, THESIS
   Bode C., 2004, PROTEKTIVE WIRKUNGEN, P516
   Botha CP, 2014, SCI VISUALIZATION, P265, DOI DOI 10.1007/978-1-4471-6497-5_23
   Brich J., 2015, GAM VIRT WORLDS SER, P1, DOI [10.1109/VS-GAMES.2015.7295779, DOI 10.1109/VS-GAMES.2015.7295779]
   Bro-Nielsen M, 1998, P IEEE, V86, P490, DOI 10.1109/5.662874
   Charles D., 2004, P INT C COMPUTER GAM, P29
   Clark C Abt, 1987, Serious Games
   Clements P., 2009, P 47 ANN SE REG C, P21
   Csikszentmihalyi M., 1991, FLOW PSYCHOL OPTIMAL, V41
   Dickey MD, 2005, ETR&D-EDUC TECH RES, V53, P67, DOI 10.1007/BF02504866
   Dipietro M., 2007, J ED MULTIMEDIA HYPE, V16, P225
   Falah J., 2014, INTELLIGENT SYSTEMS, V591, P369, DOI [10.1007/978-3-319-14654-6_23, DOI 10.1007/978-3-319-14654-6_23]
   Friese K, 2013, INFEKTIONSERKRANKUNG
   Frommel J., 2015, Proceedings of the 2015 Annual Symposium on Computer-Human Interaction in Play, P359, DOI DOI 10.1145/2793107.2793130
   Gilleade K.M., 2004, Proceedings of the 2004 ACM SIGCHI International Conference on Advances in Computer Entertainment Technology, P228, DOI [10.1145/1067343.1067372, DOI 10.1145/1067343.1067372]
   Grodal T, 2000, LEA COMMUN SER, P197
   Hung APL, 2015, VISUAL COMPUT, V31, P527, DOI 10.1007/s00371-014-0945-2
   Johnson D, 2015, CHI 2015: PROCEEDINGS OF THE 33RD ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P2265, DOI 10.1145/2702123.2702447
   Li MC, 2013, J SCI EDUC TECHNOL, V22, P877, DOI 10.1007/s10956-013-9436-x
   Liehr H, 2002, BAUCHSPEICHELDRUSE W
   Likert R, 1932, ARCH PSYHOL
   Liu SY, 2009, LECT NOTES COMPUT SC, V5670, P141, DOI 10.1007/978-3-642-03364-3_18
   Luboz V, 2014, VISUAL COMPUT, V30, P341, DOI 10.1007/s00371-013-0859-4
   Lucas K, 2004, COMMUN RES, V31, P499, DOI 10.1177/0093650204267930
   Malone Thomas W, 2021, Aptitude, learning, and instruction, P223, DOI DOI 10.1016/S0037-6337(09)70509-1
   MALONE TW, 1981, COGNITIVE SCI, V5, P333, DOI 10.1207/s15516709cog0504_2
   Mangold K, 2007, NURS EDUC, V32, P21, DOI 10.1097/00006223-200701000-00007
   McCallum Simon, 2012, Stud Health Technol Inform, V177, P85
   Meyer DK, 2006, EDUC PSYCHOL REV, V18, P377, DOI 10.1007/s10648-006-9032-1
   Michael D.R., 2005, Serious games: Games that educate, train, and inform
   Microsoft Corporation, MICR EXC CREAT ORD
   Mitgutsch K., 2012, FDG 12 P 7 INT C FDN, P121, DOI [https://doi.org/10.1145/2282338.2282364, DOI 10.1145/2282338.2282364]
   Nacke L., 2010, INT J COMPUTER SCI S, V9, P1
   Okuda Y, 2009, MT SINAI J MED, V76, P330, DOI 10.1002/msj.20127
   Pan JJ, 2015, VISUAL COMPUT, V31, P947, DOI 10.1007/s00371-015-1106-y
   Papastergiou M, 2009, COMPUT EDUC, V53, P603, DOI 10.1016/j.compedu.2009.04.001
   Paulus CJ, 2015, VISUAL COMPUT, V31, P831, DOI 10.1007/s00371-015-1123-x
   Pekrun R, 2006, EDUC PSYCHOL REV, V18, P315, DOI 10.1007/s10648-006-9029-9
   Prensky M., 2005, Handbook of Computer Game Studies
   Rauterberg M., 1995, Information System Concepts. Towards a Consolidation of Views. Proceedings of the IFIP International Working Conference on Information System Concepts, 1995, P54
   Riemann J, 2007, GASTROENTEROLOGIE KL
   Sawyer B, 2008, IEEE COMPUT GRAPH, V28, P83, DOI 10.1109/MCG.2008.114
   Schrader C., 2016, SERIOUS GAMES EDUTAI, V2
   Schutz P.A., 2007, EMOTION ED, P3, DOI DOI 10.1016/B978-012372545-5/50002-2
   Steiner-Welz S, 2005, WICHTIGSTEN KORPERFU
   Thompson J, 2010, TECH REP
   Ullrich S, 2012, IEEE T VIS COMPUT GR, V18, P617, DOI 10.1109/TVCG.2012.46
   Um E, 2012, J EDUC PSYCHOL, V104, P485, DOI 10.1037/a0026609
   van Lankveld G, 2010, LECT NOTES COMPUT SC, V6048, P208
   Vorderer P, 2004, COMMUN THEOR, V14, P388, DOI 10.1093/ct/14.4.388
   Wattanasoontorn V, 2013, ENTERTAIN COMPUT, V4, P231, DOI 10.1016/j.entcom.2013.09.002
   Wouters P., 2009, Games-Based Learning Advancements for Multi-Sensory Human Computer Interfaces: Techniques and Effective Practices, P232, DOI [DOI 10.4018/978-1-60566-360-9.CH014, https://doi.org/10.4018/978-1-60566-360-9.ch014]
   Wu J., 2015, COMPUTER GRAPHICS FO
   Xia PJ, 2013, VISUAL COMPUT, V29, P433, DOI 10.1007/s00371-012-0748-2
NR 57
TC 2
Z9 2
U1 0
U2 5
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2017
VL 33
IS 4
BP 429
EP 442
DI 10.1007/s00371-016-1314-0
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA ER4JZ
UT WOS:000398767100004
DA 2024-07-18
ER

PT J
AU Guo, SH
   Xu, HX
   Thalmann, NM
   Yao, JF
AF Guo, Shihui
   Xu, Hanxiang
   Thalmann, Nadia Magnenat
   Yao, Junfeng
TI Customization and fabrication of the appearance for humanoid robot
SO VISUAL COMPUTER
LA English
DT Article
ID MOMENT
AB Designing a robot's appearance is a challenging task because the design should be both aesthetically appealing and physically functional. Therefore, this task was previously limited to experts with professional knowledge and experiences. Given the increasing popularity of consumer-level robots, non-professional users are expecting tools that allow them to customize their robot appearance. We address this challenge with the technology of additive manufacturing and propose an end-to-end solution to customize and fabricate the robot appearance for non-professional users. The input to our solution is a triangular character mesh (commonly used in feature animations and video games) and the output is a set of 3D-printing-ready shell parts. The complete solution includes matching the shape of the character mesh with the robot endoskeleton, optimizing the shape design to maximally avoid collisions and adjusting the motion trajectories to adapt to new shell design. This approach requires no professional background in engineering design and efficiently produces accurate prototypes of robot shells. Both virtual and physically printed designs are demonstrated on a consumer level humanoid robot to validate the feasibility of our method.
C1 [Guo, Shihui; Xu, Hanxiang; Thalmann, Nadia Magnenat] Nanyang Technol Univ, Inst Media Innovat, Singapore, Singapore.
   [Guo, Shihui; Xu, Hanxiang; Yao, Junfeng] Xiamen Univ, Sch Software, Xiamen, Peoples R China.
C3 Nanyang Technological University; Xiamen University
RP Guo, SH (corresponding author), Nanyang Technol Univ, Inst Media Innovat, Singapore, Singapore.; Guo, SH (corresponding author), Xiamen Univ, Sch Software, Xiamen, Peoples R China.
EM guo7805@gmail.com; nadiathalmann@ntu.edu.sg
RI Thalmann, Nadia/AAK-5195-2021
OI Thalmann, Nadia/0000-0002-1459-5960
FU Singapore National Research Foundation (NRF) under its International
   Research Centre @ Singapore Funding Initiative
FX This research, which is carried out at BeingThere Centre, collaboration
   among IMI of Nanyang Technological University (NTU) Singapore, ETH
   Zurich, and UNC Chapel Hill, is supported by the Singapore National
   Research Foundation (NRF) under its International Research Centre @
   Singapore Funding Initiative and administered by the Interactive Digital
   Media Programme Office (IDMPO).
CR Bächer M, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601157
   Bächer M, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185543
   Byrd RH, 2006, NONCONVEX OPTIM, V83, P35
   Calì J, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366149
   Coros S, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461953
   Coumans E., BULLET PHYS ENGINE
   Fu CW, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766892
   Golovinskiy A, 2009, COMPUT GRAPH-UK, V33, P262, DOI 10.1016/j.cag.2009.03.010
   Hao JB, 2011, RAPID PROTOTYPING J, V17, P116, DOI 10.1108/13552541111113862
   Haring K.S., 2013, 2013 8 ACM IEEE INT
   Hildebrand K, 2013, COMPUT GRAPH-UK, V37, P669, DOI 10.1016/j.cag.2013.05.011
   Hu R., 2014, ACM T GRAPHIC, V33, P1, DOI [10.1145/2661229.2661244, DOI 10.1145/2661229.2661244]
   Kajita S, 2003, IEEE INT CONF ROBOT, P1620, DOI 10.1109/robot.2003.1241826
   Koo B., 2014, ACM T GRAPHICS SPECI
   Megaro V., 2015, ACM T GRAPHIC, V34, P1
   Megaro Vittorio., 2014, Proceedings of the ACM SIGGRAPH/Eurographics Symposium on Computer Animation, P123
   Mehta AM, 2014, IEEE INT C INT ROBOT, P2892, DOI 10.1109/IROS.2014.6942960
   Mehta AM, 2014, IEEE INT CONF ROBOT, P1460, DOI 10.1109/ICRA.2014.6907044
   Préost R, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461957
   Robotics T., 2016, HR OS1 HUMANOID ENDO
   Shapira L, 2010, INT J COMPUT VISION, V89, P309, DOI 10.1007/s11263-009-0279-0
   Shapira L, 2008, VISUAL COMPUT, V24, P249, DOI 10.1007/s00371-007-0197-5
   Skouras M, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461979
   Slyper R., 2012, 2012 IEEERO MAN 21 I
   Song P, 2015, COMPUT AIDED GEOM D, V35-36, P137, DOI 10.1016/j.cagd.2015.03.020
   Syrdal D.S., 2007, AAAI SPRING S MULTID, P86
   The CGAL Project, 2016, CGAL USER REFERENCE
   Thomaszewski Bernhard., 2014, ACM Transactions on Graphics (TOG), V33, P1
   van Kaick O, 2011, COMPUT GRAPH FORUM, V30, P553, DOI 10.1111/j.1467-8659.2011.01893.x
NR 29
TC 10
Z9 10
U1 0
U2 18
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2017
VL 33
IS 1
BP 63
EP 74
DI 10.1007/s00371-016-1329-6
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA EI2JM
UT WOS:000392313200007
DA 2024-07-18
ER

PT J
AU Amirolad, A
   Arashloo, SR
   Amirani, MC
AF Amirolad, Ahmad
   Arashloo, Shervin Rahimzadeh
   Amirani, Mehdi Chehel
TI Multi-layer local energy patterns for texture representation and
   classification
SO VISUAL COMPUTER
LA English
DT Article
DE Deep learning; Convolutional neural networks; Texture representation;
   Multi-layer local energy patterns
ID FEATURES; SCALE; RECOGNITION; DESCRIPTOR; IMAGES; LBP
AB Motivated by the recent success of deep networks in providing effective and abstract image representations, in this paper, a multi-layer architecture called the multi-layer local energy patterns (ML-LEP) is proposed for texture representation and classification. The proposed approach follows a multi-layer convolutional neural network paradigm and is built upon the single-layer local energy pattern (LEP) approach, a statistical histogram-based method for texture representation. An important aspect of the proposed multi-layer method compared to other deep convolutional architectures is bypassing the computationally expensive learning stage using fixed filters. As such, the proposed training-free network circumvents the need for large data for learning system parameters. An extensive investigation is carried out to determine the merits of different nonlinear operators in the proposed architecture. For this purpose, different nonlinearities including an energy-based nonlinearity, the absolute operator as well as the rectifier functions are extensively investigated and compared against each other. Extensive experiments conducted on three challenging databases of KTH-TIPS, KTH-TIPS2-a and the UIUC indicate that the extension of the LEP method to the multi-layer LEP is effective and leads to better performance. Moreover, the proposed ML-LEP approach is compared to several other well-known descriptors in the field, achieving the best reported performance on the KTH-TIPS and the KTH-TIPS2-a databases despite being training-free.
C1 [Amirolad, Ahmad; Amirani, Mehdi Chehel] Urmia Univ, Dept Elect Engn, Fac Engn, Orumiyeh, Iran.
   [Arashloo, Shervin Rahimzadeh] Tarbiat Modares Univ, Dept Med Informat, Fac Med Sci, Tehran, Iran.
C3 Urmia University; Tarbiat Modares University
RP Arashloo, SR (corresponding author), Tarbiat Modares Univ, Dept Med Informat, Fac Med Sci, Tehran, Iran.
EM amirolad.ahmad@gmail.com; S.Rahimzadeh@modares.ac.ir
RI Arashloo, Shervin Rahimzadeh/A-6381-2019; Chehel Amirani,
   Mehdi/AGL-1681-2022
OI Rahimzadeh Arashloo, Shervin/0000-0003-0189-4774; Chehel Amirani,
   Mehdi/0000-0002-5179-9831
CR [Anonymous], 2007, Computer Vision
   [Anonymous], 2011, Proceedings of the 19th ACM international conference on Multimedia, DOI [DOI 10.1145/2072298.2072344, 10.1145/2072298.2072344.URL, DOI 10.1145/2072298.2072344.URL]
   [Anonymous], 2015, Deep learning
   [Anonymous], 2014, ARXIV14043606
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Caputo B, 2005, IEEE I CONF COMP VIS, P1597, DOI 10.1109/iccv.2005.54
   Chen J, 2010, IEEE T PATTERN ANAL, V32, P1705, DOI 10.1109/TPAMI.2009.155
   Ciresan D, 2012, PROC CVPR IEEE, P3642, DOI 10.1109/CVPR.2012.6248110
   Crosier M, 2010, INT J COMPUT VISION, V88, P447, DOI 10.1007/s11263-009-0315-0
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   dos Santos JA, 2012, IEEE T GEOSCI REMOTE, V50, P3764, DOI 10.1109/TGRS.2012.2186582
   FREEMAN WT, 1991, IEEE T PATTERN ANAL, V13, P891, DOI 10.1109/34.93808
   Guo ZH, 2010, IEEE IMAGE PROC, P285, DOI 10.1109/ICIP.2010.5652209
   Guo ZH, 2010, IEEE T IMAGE PROCESS, V19, P1657, DOI 10.1109/TIP.2010.2044957
   Guo ZH, 2010, PATTERN RECOGN, V43, P706, DOI 10.1016/j.patcog.2009.08.017
   Hanbay K, 2014, APPL SOFT COMPUT, V21, P433, DOI 10.1016/j.asoc.2014.04.008
   HARALICK RM, 1973, IEEE T SYST MAN CYB, VSMC3, P610, DOI 10.1109/TSMC.1973.4309314
   Hayman E, 2004, LECT NOTES COMPUT SC, V2034, P253
   Hong XP, 2014, IEEE T IMAGE PROCESS, V23, P2557, DOI 10.1109/TIP.2014.2316640
   Huang CR, 2008, PATTERN RECOGN, V41, P3071, DOI 10.1016/j.patcog.2008.03.013
   Ji H, 2013, IEEE T IMAGE PROCESS, V22, P286, DOI 10.1109/TIP.2012.2214040
   Ji Q, 2000, IEEE T MED IMAGING, V19, P1144, DOI 10.1109/42.896790
   Lazebnik S, 2005, IEEE T PATTERN ANAL, V27, P1265, DOI 10.1109/TPAMI.2005.151
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Manjunath BS, 1996, IEEE T PATTERN ANAL, V18, P837, DOI 10.1109/34.531803
   Ng Andrew Y, 2012, P 26 ANN C NEUR PROC, P665, DOI DOI 10.1002/2014GB005021
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Quan YH, 2014, PROC CVPR IEEE, P160, DOI 10.1109/CVPR.2014.28
   Quan YH, 2014, IMAGE VISION COMPUT, V32, P250, DOI 10.1016/j.imavis.2014.02.004
   Schmidhuber J, 2015, NEURAL NETWORKS, V61, P85, DOI 10.1016/j.neunet.2014.09.003
   Shrivastava N, 2014, VISUAL COMPUT, V30, P1223, DOI 10.1007/s00371-013-0887-0
   Tan XY, 2010, IEEE T IMAGE PROCESS, V19, P1635, DOI 10.1109/TIP.2010.2042645
   Tsai DM, 2003, IMAGE VISION COMPUT, V21, P307, DOI 10.1016/S0262-8856(03)00007-6
   Varma M, 2005, INT J COMPUT VISION, V62, P61, DOI 10.1007/s11263-005-4635-4
   Varma M, 2009, IEEE T PATTERN ANAL, V31, P2032, DOI 10.1109/TPAMI.2008.182
   Wang XF, 2008, APPL MATH COMPUT, V205, P916, DOI 10.1016/j.amc.2008.05.108
   Wu XS, 2017, VISUAL COMPUT, V33, P317, DOI 10.1007/s00371-015-1202-z
   Xu Y, 2009, INT J COMPUT VISION, V83, P85, DOI 10.1007/s11263-009-0220-6
   Zhang J, 2007, INT J COMPUT VISION, V73, P213, DOI 10.1007/s11263-006-9794-4
   Zhang J, 2013, IEEE T IMAGE PROCESS, V22, P31, DOI 10.1109/TIP.2012.2214045
   Zhang J, 2013, COMPUT VIS IMAGE UND, V117, P56, DOI 10.1016/j.cviu.2012.10.004
   Zhou SS, 2013, NEUROCOMPUTING, V120, P536, DOI 10.1016/j.neucom.2013.04.017
   Zhu Y, 2001, IEEE T PATTERN ANAL, V23, P1192, DOI 10.1109/34.954608
NR 43
TC 8
Z9 9
U1 1
U2 20
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2016
VL 32
IS 12
BP 1633
EP 1644
DI 10.1007/s00371-016-1220-5
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA EB3NH
UT WOS:000387271200011
DA 2024-07-18
ER

PT J
AU Heo, YS
   Lee, S
   Jung, HY
AF Heo, Yong Seok
   Lee, Soochahn
   Jung, Ho Yub
TI Consistent color and detail transfer from multiple source images for
   video and images
SO VISUAL COMPUTER
LA English
DT Article
DE Color transfer; Detail transfer; Video segmentation; Gaussian mixture
   model; Multiple source images
ID SEGMENTATION; ALIGNMENT
AB In this paper, we propose amethod to jointly transfer the color and detail of multiple source images to a target video or image. Our method is based on a probabilistic segmentation scheme using Gaussian mixture model (GMM) to divide each source image as well as the target video frames or image into soft regions and determine the relevant source regions for each target region. For detail transfer, we first decompose each image as well as the target video frames or image into base and detail components. Then histogram matching is performed for detail components to transfer the detail of matching regions from source images to the target. We propose a unified framework to perform both color and detail transforms in an integrated manner. We also propose a method to maintain consistency for video targets, by enforcing consistent region segmentations for consecutive video frames using GMM-based parameter propagation and adaptive scene change detection. Experimental results demonstrate that our method automatically produces consistent color and detail transferred videos and images from a set of source images.
C1 [Heo, Yong Seok] Ajou Univ, Dept Elect & Comp Engn, Worldcupro 206, Suwon 443749, South Korea.
   [Lee, Soochahn] Soonchunhyang Univ, Dept Elect Engn, Asan 336745, Chungcheongnam, South Korea.
   [Jung, Ho Yub] Hankuk Univ Foreign Studies, Div Comp & Elect Syst Engn, Yongin 449745, Gyeonggi Do, South Korea.
C3 Ajou University; Soonchunhyang University; Hankuk University Foreign
   Studies
RP Jung, HY (corresponding author), Hankuk Univ Foreign Studies, Div Comp & Elect Syst Engn, Yongin 449745, Gyeonggi Do, South Korea.
EM ysheo@ajou.ac.kr; sclsch@sch.ac.kr; jung.ho.yub@gmail.com
RI Heo, Yong Seok/AAD-8816-2021; Lee, Soochahn/AAE-8471-2020
OI Lee, Soochahn/0000-0002-2975-2519
FU new faculty research fund of Ajou University; Hankuk University of
   Foreign Studies Research Fund; Soonchunhyang university research fund
FX This work was partially supported by the new faculty research fund of
   Ajou University, and partially supported by Hankuk University of Foreign
   Studies Research Fund of 2015, and also partially supported by the
   Soonchunhyang university research fund.
CR [Anonymous], 2001, P SIGGRAPH
   Bae S., 2006, P SIGGRAPH
   Belongie S, 2002, IEEE T PATTERN ANAL, V24, P509, DOI 10.1109/34.993558
   Brown M, 2007, INT J COMPUT VISION, V74, P59, DOI 10.1007/s11263-006-0002-3
   Demetriou M., 2012, P EUR C COMP VIS 201
   Fecker U, 2008, IEEE T CIRC SYST VID, V18, P1258, DOI 10.1109/TCSVT.2008.926997
   Galasso F., 2013, P IEEE INT C COMP VI
   Gonzalez RC., Digital image processing third edition Pearson international edition prepared by Pearson Education
   Gould S, 2008, INT J COMPUT VISION, V80, P300, DOI 10.1007/s11263-008-0140-x
   Huang H, 2010, VISUAL COMPUT, V26, P933, DOI 10.1007/s00371-010-0498-y
   Hwang Y., 2014, P IEEE C COMP VIS PA
   Jia JY, 2005, IEEE T PATTERN ANAL, V27, P36, DOI 10.1109/TPAMI.2005.20
   Kim SJ, 2008, IEEE T PATTERN ANAL, V30, P562, DOI 10.1109/TPAMI.2007.70732
   Li BB, 2015, VISUAL COMPUT, V31, P257, DOI 10.1007/s00371-013-0916-z
   Li K., 2008, P VIS COMM IM PROC V
   Musialski P, 2013, VISUAL COMPUT, V29, P1173, DOI 10.1007/s00371-012-0761-5
   Nguyen BP, 2012, VISUAL COMPUT, V28, P181, DOI 10.1007/s00371-011-0634-3
   Pitié F, 2007, COMPUT VIS IMAGE UND, V107, P123, DOI 10.1016/j.cviu.2006.11.011
   Reinhard E, 2001, IEEE COMPUT GRAPH, V21, P34, DOI 10.1109/38.946629
   Shih Y., 2014, P SIGGRAPH
   Su Z, 2013, VISUAL COMPUT, V29, P1011, DOI 10.1007/s00371-012-0753-5
   Tai YW, 2007, IEEE T PATTERN ANAL, V29, P1520, DOI 10.1109/TPAMI.2007.1168
   Xiang Y, 2009, PATTERN RECOGN LETT, V30, P682, DOI 10.1016/j.patrec.2009.01.004
   Xiao X., 2006, P ACM INT C VIRT REA
   Xu W., 2010, P IEEE C COMP VIS PA
   Yang Q., 2009, P IEEE C COMP VIS PA
   Zang Y, 2014, VISUAL COMPUT, V30, P969, DOI 10.1007/s00371-013-0881-6
   Zhang MJ, 2004, REAL-TIME IMAGING, V10, P23, DOI 10.1016/j.rti.2003.11.001
NR 28
TC 0
Z9 0
U1 1
U2 4
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2016
VL 32
IS 10
BP 1273
EP 1289
DI 10.1007/s00371-015-1162-3
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA EA2BQ
UT WOS:000386396800006
DA 2024-07-18
ER

PT J
AU Tran, TT
   Cao, VT
   Laurendeau, D
AF Trung-Thien Tran
   Van-Toan Cao
   Laurendeau, Denis
TI eSphere: extracting spheres from unorganized point clouds How to extract
   multiple spheres accurately and simultaneously
SO VISUAL COMPUTER
LA English
DT Article
DE Sphere fitting; Sphere extraction; Sphere validation; Mean Shift
   clustering
ID HOUGH TRANSFORM; LEAST-SQUARES; CALIBRATION; SURFACES; MESHES; MODELS
AB Spheres are popular geometric primitives found in many manufactured objects. However, sphere fitting and extraction have not been investigated in depth. In this paper, a robust method is proposed to extract multiple spheres accurately and simultaneously from unorganized point clouds. Moreover, a novel validation step is presented to assess the quality of the detected spheres, which help remove the confusion between perfect spheres and sphere-like shapes such as ellipsoids and paraboloids. A novel sampling strategy is introduced to reduce computational burden for sphere extraction. Experiments on both synthetic and scanned point clouds with different levels of noise and outliers are conducted and the results compared to state-of-the-art methods. These experiments demonstrate the efficiency and robustness of the proposed sphere extraction method.
C1 [Trung-Thien Tran; Van-Toan Cao; Laurendeau, Denis] Univ Laval, Dept Elect & Comp Engn, Quebec City, PQ G1V 0A6, Canada.
C3 Laval University
RP Tran, TT (corresponding author), Univ Laval, Dept Elect & Comp Engn, Quebec City, PQ G1V 0A6, Canada.
EM trung-thien.tran.1@ulaval.ca; van-toan.cao.1@ulaval.ca;
   denis.laurendeau@gel.ulaval.ca
FU NSERC/Creaform Industrial Research Chair on 3-D Scanning
FX This research project was supported by the NSERC/Creaform Industrial
   Research Chair on 3-D Scanning. The authors express their gratitude to
   Kean Walmsley at Autodesk for providing the Sphere Packing model, to 3D
   Warehouse for making the Carbon Nano Tube and ADN models available (Fig.
   19) and to eGrab-CAD for the bracelet model (Fig. 16). We are grateful
   to our colleagues, Jean-Francois Lalonde and to the anonymous reviewers
   for fruitful suggestions and to Annette Schwerdteger for proofreading
   the manuscript.
CR Abuzaina Anas, 2013, Computer Analysis of Images and Patterns. 15th International Conference, CAIP 2013. Proceedings: LNCS 8048, P290, DOI 10.1007/978-3-642-40246-3_36
   Agrawal M, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P782
   [Anonymous], 2008, Probability and statistics for engineering and the sciences
   Attene M, 2006, VISUAL COMPUT, V22, P181, DOI 10.1007/s00371-006-0375-x
   Attene M, 2010, COMPUT GRAPH FORUM, V29, P1905, DOI 10.1111/j.1467-8659.2010.01658.x
   Bénière R, 2013, COMPUT AIDED DESIGN, V45, P1382, DOI 10.1016/j.cad.2013.06.004
   Benko P, 2004, COMPUT AIDED DESIGN, V36, P511, DOI 10.1016/S0010-4485(03)00159-3
   Bolles R.C., 1981, IJCAI, P637
   Borrmann D, 2011, 3D RES, V2, DOI 10.1007/3DRes.02(2011)3
   Boulch A, 2012, COMPUT GRAPH FORUM, V31, P1765, DOI 10.1111/j.1467-8659.2012.03181.x
   Camurri M, 2014, MACH VISION APPL, V25, P1877, DOI 10.1007/s00138-014-0640-3
   Cao MY, 2006, PATTERN RECOGN LETT, V27, P980, DOI 10.1016/j.patrec.2005.11.019
   Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236
   Comaniciu D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P438, DOI 10.1109/ICCV.2001.937550
   Deschaud J. -E, 2010, P 3DPVT, P44
   Dias P., 2014, 18 ANN ROBOCUP INT S
   DUDA RO, 1972, COMMUN ACM, V15, P11, DOI 10.1145/361237.361242
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Forbes A.B., 1989, LEAST SQUARES BEST F, V140, P30
   Franaszek M, 2009, IEEE T INSTRUM MEAS, V58, P3544, DOI 10.1109/TIM.2009.2018011
   GANDER W, 1994, BIT, V34, P558, DOI 10.1007/BF01934268
   Holz Dirk, 2012, RoboCup 2011: Robot Soccer World Cup XV: LNCS 7416, P306, DOI 10.1007/978-3-642-32060-6_26
   HOPPE H, 1992, COMP GRAPH, V26, P71, DOI 10.1145/142920.134011
   Kalogerakis E, 2009, COMPUT AIDED DESIGN, V41, P282, DOI 10.1016/j.cad.2008.12.004
   Lamiroy B, 2007, PROC INT CONF DOC, P526
   Levin D, 1998, MATH COMPUT, V67, P1517, DOI 10.1090/S0025-5718-98-00974-0
   Liu YJ, 2013, IEEE T VIS COMPUT GR, V19, P1700, DOI 10.1109/TVCG.2013.74
   Lukacs G., 1998, Computer Vision - ECCV'98. 5th European Conference on Computer Vision. Proceedings, P671, DOI 10.1007/BFb0055697
   Magid E, 2007, COMPUT VIS IMAGE UND, V107, P139, DOI 10.1016/j.cviu.2006.09.007
   Öztireli AC, 2009, COMPUT GRAPH FORUM, V28, P493, DOI 10.1111/j.1467-8659.2009.01388.x
   Ogundana OO, 2007, OPT ENG, V46, DOI 10.1117/1.2739011
   Pratt V., 1987, COMPUTER GRAPHICS AC, V21, P145, DOI DOI 10.1145/37402.37420
   Ruan M., 2014, P INT C 3D VIS
   Schnabel R, 2007, COMPUT GRAPH FORUM, V26, P214, DOI 10.1111/j.1467-8659.2007.01016.x
   Staranowicz AN, 2015, COMPUT VIS IMAGE UND, V137, P102, DOI 10.1016/j.cviu.2015.03.013
   Tran TT, 2015, COMPUT GRAPH-UK, V46, P345, DOI 10.1016/j.cag.2014.09.027
   van der Glas M, 2002, P SOC PHOTO-OPT INS, V4684, P1571, DOI 10.1117/12.467126
   Cao VT, 2014, 2014 PROCEEDINGS OF THE 9TH INTERNATIONAL CONFERENCE ON COMPUTER GRAPHICS THEORY AND APPLICATIONS (GRAPP 2014), P43
   Varady T, 1997, COMPUT AIDED DESIGN, V29, P255, DOI 10.1016/S0010-4485(96)00054-1
   Wang Y, 2014, J APPL REMOTE SENS, V8, DOI 10.1117/1.JRS.8.083603
   Wong KYK, 2011, IEEE T IMAGE PROCESS, V20, P305, DOI 10.1109/TIP.2010.2063035
   YANG P., 2007, Proceedings of Eurographics Symposium on Point-Based Graphics 2007, P29
   Zhang J, 2013, COMPUT GRAPH-UK, V37, P697, DOI 10.1016/j.cag.2013.05.008
NR 43
TC 8
Z9 8
U1 0
U2 6
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2016
VL 32
IS 10
BP 1205
EP 1222
DI 10.1007/s00371-015-1157-0
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA EA2BQ
UT WOS:000386396800001
DA 2024-07-18
ER

PT J
AU Liu, XP
   Tao, PP
   Cao, JJ
   Chen, H
   Zou, CQ
AF Liu, Xiuping
   Tao, Pingping
   Cao, Junjie
   Chen, He
   Zou, Changqing
TI Mesh saliency detection via double absorbing Markov chain in feature
   space
SO VISUAL COMPUTER
LA English
DT Article
DE Mesh saliency; Absorbing Markov chain; Feature space; Foreground cues
ID VISUAL-ATTENTION; SPARSE
AB We propose a mesh saliency detection approach using absorbing Markov chain. Unlike most of the existing methods based on some center-surround operator, our method employs feature variance to obtain insignificant regions and considers both background and foreground cues. Firstly, we partition an input mesh into a set of segments using Ncuts algorithm and then each segment is over segmented into patches based on Zernike coefficients. Afterwards, some background patches are selected by computing feature variance within the segments. Secondly, the absorbed time of each node is calculated via absorbing Markov chain with the background patches as absorbing nodes, which gives a preliminary saliency measure. Thirdly, a refined saliency result is generated in a similar way but with foreground nodes extracted from the preliminary saliency map as absorbing nodes, which inhibits the background and efficiently enhances salient foreground regions. Finally, a Laplacian-based smoothing procedure is utilized to spread the patch saliency to each vertex. Experimental results demonstrate that our scheme performs competitively against the state-of-the-art approaches.
C1 [Liu, Xiuping; Tao, Pingping; Cao, Junjie; Chen, He] Dalian Univ Technol, Sch Math Sci, Dalian 116024, Peoples R China.
   [Zou, Changqing] Hengyang Normal Univ, Coll Comp Sci & Technol, Hengyang 421000, Peoples R China.
   [Zou, Changqing] Simon Fraser Univ, Sch Comp Sci, Burnaby, BC V5A 1S6, Canada.
C3 Dalian University of Technology; Hengyang Normal University; Simon
   Fraser University
RP Cao, JJ (corresponding author), Dalian Univ Technol, Sch Math Sci, Dalian 116024, Peoples R China.
EM jjcao1231@gmail.com
RI Liu, Xiu/IYJ-9134-2023; Zou, Changqing/AAK-8510-2020; Liu,
   Xiufang/I-8003-2015
FU NSFC [61173102, 61370143, 61363048, 61262050, 61502153]
FX The authors sincerely thank reviewers for their valuable comments.
   Xiuping Liu is supported by the NSFC (Nos. 61173102, 61370143). Junjie
   Cao is supported by the NSFC (Nos. 61363048, 61262050). Changqing Zou is
   supported by the NSFC (No. 61502153).
CR [Anonymous], 2007, PROC IEEE C COMPUT V, DOI 10.1109/CVPR.2007.383267
   Castellani U, 2008, COMPUT GRAPH FORUM, V27, P643, DOI 10.1111/j.1467-8659.2008.01162.x
   Chen XB, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531379
   Chen Xinguang, 2012, J Biom Biostat, VSuppl 1
   Cheng MM, 2011, PROC CVPR IEEE, P409, DOI 10.1109/CVPR.2011.5995344
   Feixas M, 2009, ACM T APPL PERCEPT, V6, DOI 10.1145/1462055.1462056
   Frintrop S, 2005, LECT NOTES COMPUT SC, V3368, P168
   Golovinskiy A, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409098
   Gopalakrishnan V, 2010, IEEE T IMAGE PROCESS, V19, P3232, DOI 10.1109/TIP.2010.2053940
   Guy G, 1997, IEEE T PATTERN ANAL, V19, P1265, DOI 10.1109/34.632985
   Harel J, 2006, Advances in Neural Information Processing Systems, V19
   Hu JX, 2009, VISUAL COMPUT, V25, P667, DOI 10.1007/s00371-009-0340-6
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Jiang BW, 2013, IEEE I CONF COMP VIS, P1665, DOI 10.1109/ICCV.2013.209
   Lee CH, 2005, ACM T GRAPHIC, V24, P659, DOI 10.1145/1073204.1073244
   Leifman G, 2012, PROC CVPR IEEE, P414, DOI 10.1109/CVPR.2012.6247703
   Li Z, 2009, VISUAL COMPUT, V25, P139, DOI 10.1007/s00371-008-0210-7
   Liu RS, 2014, PROC CVPR IEEE, P3866, DOI 10.1109/CVPR.2014.494
   Mantiuk R., 2003, PROCS 19 SPRING C CO, P239
   Maximo A, 2011, GRAPH MODELS, V73, P231, DOI 10.1016/j.gmod.2011.05.002
   Ren XF, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P10
   Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688
   Shilane P, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1243980.1243981
   Song R, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2530691
   Song R, 2013, VISUAL COMPUT, V29, P695, DOI 10.1007/s00371-013-0806-4
   Tao PP, 2015, COMPUT GRAPH-UK, V46, P264, DOI 10.1016/j.cag.2014.09.023
   Wu JL, 2013, GRAPH MODELS, V75, P255, DOI 10.1016/j.gmod.2013.05.002
   Yang B., 2015, VISUAL COMPUT, P1
   Yang C, 2013, PROC CVPR IEEE, P3166, DOI 10.1109/CVPR.2013.407
   Yee H, 2001, ACM T GRAPHIC, V20, P39, DOI 10.1145/383745.383748
   Zhang W., 2015, VIS COMPUT
   Zhao S., 2015, VIS COMPUT
NR 32
TC 7
Z9 10
U1 0
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2016
VL 32
IS 9
BP 1121
EP 1132
DI 10.1007/s00371-015-1184-x
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DU4DE
UT WOS:000382161400006
DA 2024-07-18
ER

PT J
AU Lee, RR
   Lo, Y
   Chu, HK
   Chang, CF
AF Lee, Ruen-Rone
   Lo, Yi
   Chu, Hung-Kuo
   Chang, Chun-Fa
TI A simulation on grass swaying with dynamic wind force
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 33rd Conference on Computer Graphics International (CGI)
CY JUN 28-JUL 01, 2016
CL Heraklion, GREECE
SP Fdn Res Technol
DE Grass swaying; Real time; Simulation; Grid-based fluid dynamics
ID FLUID
AB Grass swaying simulation with respect to wind force plays an important role in the outdoor scene design of video games and animations. However, the complex and dynamic interactions between grass and wind largely hinder the existing approaches from generating physically plausible simulation in real-time performance. Therefore, common approaches compromise by either rendering still meadow or simply adopting a procedural method for simulating the grass motion. In this work, we present a simple yet effective grass model that enables the real-time simulation of grass swaying mimicking real-world grass motions under dynamic wind force. We characterize each individual grass using a simple polyline model with four vertices derived from the control knots of a cubic Bezier curve describing the real grass shape. The grass dynamics is modeled by applying a combination of swinging, bending and twisting motions to the polyline model in response to the input wind force. The deformed grass model is then passed to the shader pipeline to synthesize grass blades for the rendering. Experimental results show that our system not only achieves real-time performance in simulation and rendering, but also scales well to large grass field such as a meadow.
C1 [Lee, Ruen-Rone] Ind Technol Res Inst, Informat & Commun Res Labs, Hsinchu, Taiwan.
   [Lo, Yi] MediaTek Inc, Hsinchu, Taiwan.
   [Chu, Hung-Kuo] Natl Tsing Hua Univ, Dept Comp Sci, Hsinchu, Taiwan.
   [Chang, Chun-Fa] Natl Taiwan Normal Univ, Dept Comp Sci, Taipei, Taiwan.
C3 Industrial Technology Research Institute - Taiwan; Mediatek
   Incorporated; National Tsing Hua University; National Taiwan Normal
   University
RP Lee, RR (corresponding author), Ind Technol Res Inst, Informat & Commun Res Labs, Hsinchu, Taiwan.
EM rrlee@itri.org.tw
CR Akagi Y., 2012, J WSCG, V20, P21
   [Anonymous], 1965, ALGEBRAIC EIGENVALUE
   [Anonymous], 2008, Fluid Simulation for Computer Graphics
   [Anonymous], 2004, GPU gems
   Bakay Brook., 2002, EUROGRAPHICS 2002
   Belyaev S., 2011, P GRAPHICON2011, P1
   Bergou M, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360662
   Bertails F, 2009, COMPUT GRAPH FORUM, V28, P417, DOI 10.1111/j.1467-8659.2009.01381.x
   Fan ZZ, 2015, PROCEEDINGS - I3D 2015, P55, DOI 10.1145/2699276.2699283
   Foster N, 1996, GRAPH MODEL IM PROC, V58, P471, DOI 10.1006/gmip.1996.0039
   GINGOLD RA, 1977, MON NOT R ASTRON SOC, V181, P375, DOI 10.1093/mnras/181.3.375
   Gkioulekas I, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2516971.2516972
   Hang Qiu, 2013, WSEAS Transactions on Computers, V12, P277
   Hang Qiu, 2012, Journal of Software, V7, P431, DOI 10.4304/jsw.7.2.431-439
   HARLOW FH, 1965, PHYS FLUIDS, V8, P2182, DOI 10.1063/1.1761178
   Jens Orthmann, 2009, Journal of WSCG, V17, P65
   K: PELZER., 2004, GPU GEMS PROGRAMMING, P107
   LUCY LB, 1977, ASTRON J, V82, P1013, DOI 10.1086/112164
   Perbet F., 2001, I3D 01, P103
   Pirk S., 2012, ACM T GRAPHIC, V31, DOI DOI 10.1145/2185520.2185546
   Pirk S, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2661229.2661252
   Reeves W. T., 1985, Computer Graphics, V19, P313, DOI 10.1145/325165.325250
   Selino A, 2013, COMPUT GRAPH FORUM, V32, P75, DOI 10.1111/j.1467-8659.2012.03232.x
   Shenglian Lu, 2011, International Journal of Virtual Reality, V10, P33
   Stam J, 1997, COMPUT GRAPH FORUM, V16, pC159, DOI 10.1111/1467-8659.00152
   Stam J, 1999, COMP GRAPH, P121, DOI 10.1145/311535.311548
   STAM J, 2003, P GAM DEV C, V4, P76
   Wang CB, 2005, COMPUT ANIMAT VIRT W, V16, P377, DOI 10.1002/cav.91
   Ward K, 2007, IEEE T VIS COMPUT GR, V13, P213, DOI 10.1109/TVCG.2007.30
   Zhao XK, 2009, 2009 INTERNATIONAL CONFERENCE ON INFORMATION TECHNOLOGY AND COMPUTER SCIENCE, VOL 1, PROCEEDINGS, P601, DOI 10.1109/ITCS.2009.131
NR 30
TC 0
Z9 0
U1 1
U2 9
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2016
VL 32
IS 6-8
BP 891
EP 900
DI 10.1007/s00371-016-1263-7
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA DP8ET
UT WOS:000378731600021
DA 2024-07-18
ER

PT J
AU Lim, JG
   Kim, BJ
   Hong, JM
AF Lim, Jae-Gwang
   Kim, Bong-Jun
   Hong, Jeong-Mo
TI Water simulation using a responsive surface tracking for flow-type
   changes
SO VISUAL COMPUTER
LA English
DT Article
DE Fluid modeling; Water simulation; Fluid-implicit-particle method;
   Surface tracking; Material point method
AB The realistic simulation of fluids largely depends on a temporally coherent surface tracking method that can deal effectively with transitions between different types of flows. We model these transitions by constructing a very smooth fluid surface and a much rougher, splashy surface separately, and then blending them together in proportions that depend on the flow speed. This allows creative control of the behavior of the fluids as well as the visual results of the simulation. We overcome the well-known difficulty of obtaining smooth surfaces from Lagrangian particles by allowing them to carry normal vectors as well as signed distances from the level set surface and by introducing a new surface construction algorithm inspired by the moving least-squares method. We also implemented an adaptive form of the fluid-implicit-particle method that only places particles near visually interesting regions, which improves performance. Additionally, we introduce a novel subgrid solver based on the material point method to increase the amount of detail produced by the FLIP method. We present several examples that show visually convincing water flows.
C1 [Lim, Jae-Gwang; Kim, Bong-Jun; Hong, Jeong-Mo] Dongguk Univ, Seoul, South Korea.
C3 Dongguk University
RP Hong, JM (corresponding author), Dongguk Univ, Seoul, South Korea.
EM jmhong@dongguk.edu
FU Dongguk University; National Research Foundation of Korea
   [NRF-2011-0023134]; Korea Creative Content Agency (KOCCA) in the Culture
   Technology (CT) Research & Development Program [RST201100017]
FX This work was supported by the research program of Dongguk University,
   2015, the National Research Foundation of Korea (NRF-2011-0023134), and
   the Korea Creative Content Agency (KOCCA) in the Culture Technology (CT)
   Research & Development Program 2012 (RST201100017).
CR Adams B, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1276377.1276437, 10.1145/1239451.1239499]
   Ando R., 2011, Proceedings-SCA 2011: ACM SIGGRAPH / Eurographics Symposium on Computer Animation, P7, DOI DOI 10.1145/2019406.2019408
   Ando R, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461982
   Ando R, 2012, IEEE T VIS COMPUT GR, V18, P1202, DOI 10.1109/TVCG.2012.87
   [Anonymous], 2010, ACM T GRAPH TOG
   [Anonymous], 2002, SURFACES
   [Anonymous], EG UK THEORY PRACTIC
   Bhatacharya Haimasree., 2011, P 2011 ACM SIGGRAPH, P17
   Boyd L, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2159516.2159522
   BRACKBILL JU, 1986, J COMPUT PHYS, V65, P314, DOI 10.1016/0021-9991(86)90211-1
   Enright D, 2002, ACM T GRAPHIC, V21, P736, DOI [10.1145/566570.566581, 10.1145/566570.566645]
   Enright D, 2002, J COMPUT PHYS, V183, P83, DOI 10.1006/jcph.2002.7166
   Guo YJ, 2006, CMES-COMP MODEL ENG, V16, P141
   Hong JM, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360647
   Hong JM, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239498
   Hong JM, 2005, ACM T GRAPHIC, V24, P915, DOI 10.1145/1073204.1073283
   Ianniello S, 2010, J COMPUT PHYS, V229, P1353, DOI 10.1016/j.jcp.2009.10.034
   Jung HR, 2013, COMPUT ANIMAT VIRT W, V24, P185, DOI 10.1002/cav.1498
   Kim B, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239549
   Losasso F, 2008, IEEE T VIS COMPUT GR, V14, P797, DOI 10.1109/TVCG.2008.37
   MONAGHAN JJ, 1992, ANNU REV ASTRON ASTR, V30, P543, DOI 10.1146/annurev.aa.30.090192.002551
   Muller M., 2003, ACM SIGGRAPH/Eurographics Symposium on Computer Animation, P154
   Nielsen MB, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461918
   Shabana AA, 2008, COMPUTATIONAL CONTINUUM MECHANICS, P1, DOI 10.1017/CBO9780511611469
   Shen C, 2004, ACM T GRAPHIC, V23, P896, DOI 10.1145/1015706.1015816
   Solenthaler B, 2007, COMPUT ANIMAT VIRT W, V18, P69, DOI 10.1002/cav.162
   Stam J, 1999, COMP GRAPH, P121, DOI 10.1145/311535.311548
   Steffen M, 2008, INT J NUMER METH ENG, V76, P922, DOI 10.1002/nme.2360
   Stomakhin A, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461948
   SULSKY D, 1995, COMPUT PHYS COMMUN, V87, P236, DOI 10.1016/0010-4655(94)00170-7
   Sulsky D, 2007, J GEOPHYS RES-OCEANS, V112, DOI 10.1029/2005JC003329
   Wojtan C, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778787
   Yu JH, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2421636.2421641
   Zhang DZ, 2008, J COMPUT PHYS, V227, P3159, DOI 10.1016/j.jcp.2007.11.021
   Zhao HK, 2005, MATH COMPUT, V74, P603
   Zhu YN, 2005, ACM T GRAPHIC, V24, P965, DOI 10.1145/1073204.1073298
NR 36
TC 3
Z9 3
U1 0
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2016
VL 32
IS 5
BP 641
EP 651
DI 10.1007/s00371-015-1080-4
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DK5UI
UT WOS:000374985800009
DA 2024-07-18
ER

PT J
AU Wu, JZ
   Hu, DW
   Chen, FL
AF Wu, Jianzhai
   Hu, Dewen
   Chen, Fanglin
TI Action recognition by hidden temporal models
SO VISUAL COMPUTER
LA English
DT Article
DE Human action recognition; Temporal pyramid model (TPM); Multi-model
   representation; Latent SVM
AB We focus on the recognition of human actions in uncontrolled videos that may contain complex temporal structures. It is a difficult problem because of the large intra-class variations in viewpoint, video length, motion pattern, etc. To address these difficulties, we propose a novel system in this paper that represents each action class by hidden temporal models. In this system, we represent the crucial action event per category by a video segment that covers a fixed number of frames and can move temporally within the sequences. To capture the temporal structures, the video segment is described by a temporal pyramid model. To capture large intra-class variations, multiple models are combined using Or operation to represent alternative structures. The index ofmodel and the start frame of segment are both treated as hidden variables. We implement a learning procedure based on the latent SVM method. The proposed approach is tested on two difficult benchmarks: the Olympic Sports and HMDB51 data sets. The experimental results reveal that our system is comparable to the state-of-the-art methods in the literature.
C1 [Wu, Jianzhai; Hu, Dewen; Chen, Fanglin] Natl Univ Def Technol, Coll Mechatron & Automat, Dept Automat Control, Changsha 410073, Hunan, Peoples R China.
C3 National University of Defense Technology - China
RP Hu, DW (corresponding author), Natl Univ Def Technol, Coll Mechatron & Automat, Dept Automat Control, Changsha 410073, Hunan, Peoples R China.
EM wjz_gfkd@163.com; dwhu@nudt.edu.cn; fanglinchen@nudt.edu.cn
RI Hu, Dewen/AAN-8511-2020; Cataldi, Antonio/AAM-7411-2021; Hu,
   Dewen/D-1978-2015; Chen, Fanglin/I-1527-2013
OI Chen, Fanglin/0000-0002-9193-5412
FU National Basic Research Program of China [2013CB329401]; Natural Science
   Foundation of China [61375034, 61203263]; NUDT Open Project of National
   Key Lab of High Performance Computing
FX This work is supported by the National Basic Research Program of China
   (2013CB329401), the Natural Science Foundation of China (61375034,
   61203263) and the NUDT Open Project of National Key Lab of High
   Performance Computing.
CR [Anonymous], 2007, ICCV
   [Anonymous], 2008, P CVPR
   [Anonymous], CVPR
   [Anonymous], ICML
   [Anonymous], 2012, CVPR
   [Anonymous], 2011, CVPR
   [Anonymous], 2006, IEEECOMPUT SOC C COM
   [Anonymous], 2004, INT C MACH LEARN
   [Anonymous], 2006, ECCV
   [Anonymous], 2012, CVPR
   [Anonymous], 2007, ICCV
   [Anonymous], 2010, ECCV
   [Anonymous], 2011, ICCV
   [Anonymous], 2012, ECCV
   Bobick AF, 2001, IEEE T PATTERN ANAL, V23, P257, DOI 10.1109/34.910878
   Boykov Y, 2004, IEEE T PATTERN ANAL, V26, P1124, DOI 10.1109/TPAMI.2004.60
   Brendel W., 2011, ECCV
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Tran D, 2011, PROC CVPR IEEE, DOI 10.1109/CVPR.2011.5995416
   Fan R.-E., 2008, J MACH LEARN RES
   Fathi A., 2008, CVPR
   Felzenszwalb PF, 2010, IEEE T PATTERN ANAL, V32, P1627, DOI 10.1109/TPAMI.2009.167
   Ikizler N, 2008, INT J COMPUT VISION, V80, P337, DOI 10.1007/s11263-008-0142-8
   Jiang YG, 2012, LECT NOTES COMPUT SC, V7576, P425, DOI 10.1007/978-3-642-33715-4_31
   Laxton Benjamin., 2007, CVPR
   Liu J., 2008, P CVPR
   Natarajan Pradeep., 2008, CVPR
   Ogale AbhijitS., 2004, In VACE
   Satkin S., 2010, ECCV
   Schindler K., 2008, CVPR, P1
   Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688
   Tang K., CVPR
   Tran D., 2012, NIPS
   Wang H., 2011, CVPR
   Wang H, 2013, INT J COMPUT VISION, V103, P60, DOI 10.1007/s11263-012-0594-8
   Wang Y, 2011, IEEE T PATTERN ANAL, V33, P1310, DOI 10.1109/TPAMI.2010.214
   Weinland D, 2011, COMPUT VIS IMAGE UND, V115, P224, DOI 10.1016/j.cviu.2010.10.002
   Xiang T, 2006, INT J COMPUT VISION, V67, P21, DOI 10.1007/s11263-006-4329-6
   Yao BP, 2012, IEEE T PATTERN ANAL, V34, P1691, DOI 10.1109/TPAMI.2012.67
   Yuan JS, 2011, IEEE T PATTERN ANAL, V33, P1728, DOI 10.1109/TPAMI.2011.38
   Yuille A. L., 2001, Advances in Neural Information Processing Systems, P1033
NR 41
TC 14
Z9 14
U1 0
U2 17
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2014
VL 30
IS 12
BP 1395
EP 1404
DI 10.1007/s00371-013-0899-9
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AS8DX
UT WOS:000344481400007
DA 2024-07-18
ER

PT J
AU Zhang, L
   Zhao, Y
   Zhu, ZF
AF Zhang, Lei
   Zhao, Yao
   Zhu, Zhenfeng
TI Extracting shared subspace incrementally for multi-label image
   classification
SO VISUAL COMPUTER
LA English
DT Article
DE Multi-label image classification; Incremental learning; Shared subspace;
   Singular value decomposition; Lossless matrix factorization
AB With the popularity of internet technology, thousands of new images with multiple labels appear on the web every day. For a large number of images updated daily on the websites, it is of ever-increasing importance to classify these new multi-label images online in real time. Accordingly, this paper presents an incremental shared subspace learning method for multi-label image classification. With the incremental lossless matrix factorization, the proposed algorithm can be incrementally performed without using original existing input data, thus high computational complexity involved in extracting the shared subspace can be avoided. Several publicly availablemulti-label image datasets are used to evaluate the proposed method. Experimental results demonstrate that the proposed approach is much more efficient than the non-incremental methods without decreasing the classification performance.
C1 [Zhang, Lei; Zhao, Yao; Zhu, Zhenfeng] Beijing Jiaotong Univ, Inst Informat Sci, Beijing, Peoples R China.
   [Zhang, Lei; Zhao, Yao; Zhu, Zhenfeng] Beijing Key Lab Adv Informat Sci & Network Techno, Beijing 100044, Peoples R China.
C3 Beijing Jiaotong University; Beijing Jiaotong University
RP Zhao, Y (corresponding author), Beijing Key Lab Adv Informat Sci & Network Techno, Beijing 100044, Peoples R China.
EM 10112061@bjtu.edu.cn; yzhao@bjtu.edu.cn
FU 973 Program [2012CB316400]; National Natural Science Foundation of China
   [61025013, 61172129]; PCSIRT [201206]; NCET [13-0661, 4112043];
   Fundamental Research Funds for the Central Universities [2012JBZ012]
FX This work was supported in part by 973 Program (No. 2012CB316400),
   National Natural Science Foundation of China (No. 61025013, No.
   61172129), PCSIRT (No. 201206), NCET (No. 13-0661, No. 4112043) and
   Fundamental Research Funds for the Central Universities (No.
   2012JBZ012).
CR Ando RK, 2005, J MACH LEARN RES, V6, P1817
   [Anonymous], ECCV
   [Anonymous], ADV NEURAL INFORM PR
   [Anonymous], IEEE INT C MULTIMEDI
   Arenas-Garcia J., 2007, ADV NEURAL INFORM PR, V19, P33
   Boutell MR, 2004, PATTERN RECOGN, V37, P1757, DOI 10.1016/j.patcog.2004.03.009
   Burges CJC, 1998, DATA MIN KNOWL DISC, V2, P121, DOI 10.1023/A:1009715923555
   Chen GN, 2012, IEEE T VIS COMPUT GR, V18, P767, DOI 10.1109/TVCG.2011.107
   Chen XY, 2011, IEEE I CONF COMP VIS, P834, DOI 10.1109/ICCV.2011.6126323
   Everingham Mark, 2007, PASCAL VISUAL OBJECT
   Golub G.H., 1989, MATRIX COMPUTATIONS
   HOERL AE, 1970, TECHNOMETRICS, V12, P55, DOI 10.1080/00401706.1970.10488634
   Hoerl AE, 1962, CHEM ENG PROGR, V58, P54, DOI DOI 10.1002/SIM.4780030311
   Hotelling H, 1936, BIOMETRIKA, V28, P321, DOI 10.1093/biomet/28.3-4.321
   Hsu CW, 2002, IEEE T NEURAL NETWOR, V13, P415, DOI 10.1109/72.991427
   Hu SM, 2013, VISUAL COMPUT, V29, P393, DOI 10.1007/s00371-013-0792-6
   Huiskes Mark J., 2008, Proceedings of the 1st ACM international conference on Multimedia information retrieval, P39, DOI DOI 10.1145/1460096.1460104
   Ji S., 2008, P 14 ACM SIGKDD INT, P381, DOI [DOI 10.1145/1401890.1401939, 10.1145/1401890.1401939]
   Ji SW, 2010, ACM T KNOWL DISCOV D, V4, DOI 10.1145/1754428.1754431
   Joachims T, 1999, MACHINE LEARNING, PROCEEDINGS, P200
   Kaplan A. M., 2004, Understanding Statis, V3, P283, DOI 10.1207/s15328031us03044
   Lei Zhang, 2012, Computational Visual Media. Proceedings First International Conference, CVM 2012, P138, DOI 10.1007/978-3-642-34263-9_18
   Mardia K. V., 1979, MULTIVARIATE ANAL, P457
   Pang YW, 2011, IEEE IMAGE PROC, P1797, DOI 10.1109/ICIP.2011.6115811
   Tsoumakas G., 2007, INT J DATA WAREHOUS, V3, P1, DOI DOI 10.4018/JDWM.2007070101
   Vázquez PP, 2012, VISUAL COMPUT, V28, P1063, DOI 10.1007/s00371-011-0651-2
   Yang FL, 2012, VISUAL COMPUT, V28, P175, DOI 10.1007/s00371-011-0616-5
   Zha HY, 1999, SIAM J SCI COMPUT, V21, P782, DOI 10.1137/S1064827597329266
   Zha Z.-J., 2008, 2008 IEEE C COMPUTER, DOI DOI 10.1109/CVPR.2008.4587384
   Zhao HT, 2006, IEEE T SYST MAN CY B, V36, P873, DOI 10.1109/TSMCB.2006.870645
NR 30
TC 4
Z9 5
U1 1
U2 29
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2014
VL 30
IS 12
BP 1359
EP 1371
DI 10.1007/s00371-013-0891-4
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AS8DX
UT WOS:000344481400005
DA 2024-07-18
ER

PT J
AU Barra, V
   Biasotti, S
AF Barra, Vincent
   Biasotti, Silvia
TI 3D shape retrieval and classification using multiple kernel learning on
   extended Reeb graphs
SO VISUAL COMPUTER
LA English
DT Article
DE 3D Object retrieval; Classification; Extended Reeb graph; Kernel
   learning
ID SEARCH
AB We propose in this article a new 3D shape classification and retrieval method, based on a supervised selection of the most significant features in a space of attributed extended Reeb graphs encoding different shape characteristics. The similarity between pairs of graphs is addressed through both their representation as set of bags of shortest paths, and the definition of kernels adapted to these descriptions. A multiple kernel learning algorithm is used on this set of kernels to find an optimal linear combination of kernels for classification and retrieval purposes. Results on classical data sets are comparable with the best results of the literature, and the modularity and flexibility of the kernel learning ensure its applicability to a large set of methods.
C1 [Barra, Vincent] Univ Blaise Pascal, Clermont Univ, LIMOS, F-63000 Clermont Ferrand, France.
   [Barra, Vincent] CNRS, LIMOS, UMR 6158, F-63173 Aubiere, France.
   [Biasotti, Silvia] CNR, Ist Matemat Applicata & Tecnol Informat E Magenes, Genoa, Italy.
   [Barra, Vincent] Univ Blaise Pascal, Clermont Univ, F-63000 Clermont Ferrand, France.
   [Biasotti, Silvia] IMATI CNR, Pavia, Italy.
C3 Universite Clermont Auvergne (UCA); Centre National de la Recherche
   Scientifique (CNRS); Centre National de la Recherche Scientifique
   (CNRS); Universite Clermont Auvergne (UCA); Consiglio Nazionale delle
   Ricerche (CNR); Istituto di Matematica Applicata e Tecnologie
   Informatiche "Enrico Magenes" (IMATI-CNR); Universite Clermont Auvergne
   (UCA); Centre National de la Recherche Scientifique (CNRS); Consiglio
   Nazionale delle Ricerche (CNR); Istituto di Matematica Applicata e
   Tecnologie Informatiche "Enrico Magenes" (IMATI-CNR)
RP Barra, V (corresponding author), Univ Blaise Pascal, Clermont Univ, LIMOS, BP 10448, F-63000 Clermont Ferrand, France.
EM vincent.barra@isima.fr; silvia@ge.imati.cnr.it
RI Biasotti, Silvia/G-8602-2012; Barra, Vincent/AAS-8453-2021; Barra,
   Vincent/KGL-8172-2024
OI Biasotti, Silvia/0000-0002-9992-825X; Barra,
   Vincent/0000-0002-8975-222X; 
FU EU project VISIONAIR (EU) [FP7-INFRASTRUCTURES-262044]; EU project
   IQmulus (EU) [FP7-ICT-2011-318787]
FX This work has been partially developed in the CNR research activity
   ICT.P10.009 and the EU projects VISIONAIR (EU
   FP7-INFRASTRUCTURES-262044) and IQmulus (EU FP7-ICT-2011-318787).
CR Akgül CB, 2010, INT J COMPUT VISION, V89, P392, DOI 10.1007/s11263-009-0294-1
   Akgül CB, 2009, IEEE T PATTERN ANAL, V31, P1117, DOI 10.1109/TPAMI.2009.25
   [Anonymous], P EUR WORKSH 3D OBJ
   [Anonymous], 09 IMATI
   [Anonymous], P ACM WORKSH 3D OBJ
   [Anonymous], 2010, P 3 EUR WORKSH 3D OB
   [Anonymous], 2003, ICML
   [Anonymous], IEEE T PATTERN ANAL
   [Anonymous], S COMP GEOM
   Assfalg J., 2006, P 15 ACM INT C INFOR, P198
   Baeza-Yates Ricardo, 1999, MODERN INFORM RETRIE, V463
   Barra V, 2013, PATTERN RECOGN, V46, P2985, DOI 10.1016/j.patcog.2013.03.019
   Biasotti S, 2008, J MATH IMAGING VIS, V32, P161, DOI 10.1007/s10851-008-0096-z
   Biasotti S, 2008, THEOR COMPUT SCI, V392, P5, DOI 10.1016/j.tcs.2007.10.018
   Biasotti S, 2008, ACM COMPUT SURV, V40, DOI 10.1145/1391729.1391731
   BIASOTTI S, 2005, COMPUT GRAPH GEOM, V7, P31
   Biasotti S, 2013, COMPUT GRAPH-UK, V37, P608, DOI 10.1016/j.cag.2013.05.007
   Borgwardt KM, 2005, Fifth IEEE International Conference on Data Mining, Proceedings, P74, DOI 10.1109/ICDM.2005.132
   Bronstein AM, 2006, SIAM J SCI COMPUT, V28, P1812, DOI 10.1137/050639296
   Bustos B, 2005, ACM COMPUT SURV, V37, P345, DOI 10.1145/1118890.1118893
   Chaouch M., 2007, P SHREC 07 SHAP RETR, P31
   Daras P., 2007, P SHREC 07 SHAP RETR, P38
   Del Bimbo A, 2006, ACM T MULTIM COMPUT, V2, P20
   Fisher M, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964929
   Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504
   Funkhouser T, 2003, ACM T GRAPHIC, V22, P83, DOI 10.1145/588272.588279
   Gärtner T, 2003, LECT NOTES ARTIF INT, V2777, P129, DOI 10.1007/978-3-540-45167-9_11
   Giorgi D, 2010, VISUAL COMPUT, V26, P1321, DOI 10.1007/s00371-010-0524-0
   Giorgi D, 2008, IEEE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS 2008, PROCEEDINGS, P219
   Goldfeder C. F. H., 2008, SHAPE MODELING
   Gönen M, 2011, J MACH LEARN RES, V12, P2211
   Haasdonk B, 2004, LECT NOTES COMPUT SC, V3175, P220
   Hilaga M, 2001, COMP GRAPH, P203, DOI 10.1145/383259.383282
   Kondor R., 2003, INT C MACH LEARN AAA
   Laga H, 2008, LECT NOTES ARTIF INT, V4938, P210, DOI 10.1007/978-3-540-78159-2_20
   Laga H, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2516971.2516975
   Laga H, 2007, GRAPHITE 2007: 5TH INTERNATIONAL CONFERENCE ON COMPUTER GRAPHICS AND INTERACTIVE TECHNIQUES IN AUSTRALASIA AND SOUTHERN ASIA, PROCEEDINGS, P227
   Lanckriet GRG, 2004, J MACH LEARN RES, V5, P27
   Marini S, 2011, VISUAL COMPUT, V27, P1005, DOI 10.1007/s00371-011-0612-9
   Mateus D, 2007, IEEE I CONF COMP VIS, P39
   Napoleon T., 2007, P SHREC 07 SHAP RETR, P33
   Nguyen GP, 2007, IEEE T MULTIMEDIA, V9, P1404, DOI 10.1109/TMM.2007.906586
   Ohbuchi R., 2006, INT MULTIMEDIA C ACM, P163
   Ovsjanikov M, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185526
   Pokrass J, 2013, COMPUT GRAPH FORUM, V32, P459, DOI 10.1111/cgf.12066
   Rakotomamonjy A, 2008, J MACH LEARN RES, V9, P2491
   REEB G, 1946, CR HEBD ACAD SCI, V222, P847
   Rustamov RM, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461959
   Shilane P, 2006, IEEE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS 2006, PROCEEDINGS, P108
   Sonnenburg S, 2006, J MACH LEARN RES, V7, P1531
   Suard F., 2007, Esann, P355
   Suyu Hou, 2005, Computer-Aided Design and Applications, V2, P155
   Tabia H, 2013, ACM T INTEL SYST TEC, V4, DOI 10.1145/2438653.2438668
   Tangelder JH, 2008, MULTIMED TOOLS APPL, V39, P441, DOI 10.1007/s11042-007-0181-0
   Tierny J, 2009, COMPUT GRAPH FORUM, V28, P41, DOI 10.1111/j.1467-8659.2008.01190.x
   Tieu K, 2004, INT J COMPUT VISION, V56, P17, DOI 10.1023/B:VISI.0000004830.93820.78
   Tung T., 2007, P IEEE INT C SHAP MO, P229
   Typke R, 2006, 2006 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO - ICME 2006, VOLS 1-5, PROCEEDINGS, P1793, DOI 10.1109/ICME.2006.262900
   Vishwanathan SVN, 2010, J MACH LEARN RES, V11, P1201
NR 59
TC 14
Z9 17
U1 0
U2 23
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2014
VL 30
IS 11
BP 1247
EP 1259
DI 10.1007/s00371-014-0926-5
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AS3HX
UT WOS:000344169300005
DA 2024-07-18
ER

PT J
AU Pueyo, O
   Patow, G
AF Pueyo, Oriol
   Patow, Gustavo
TI Structuring urban data
SO VISUAL COMPUTER
LA English
DT Article
DE Urban data processing; Urban modeling; GIS data
ID 3D BUILDING MODELS
AB Geometric city modeling is an open problem without standard solutions. Within this problem, there appear several sub-problems that must be faced, like the accurate modeling of streets, buildings and other architectonic structures. One important source of geographical information is (measured) cadastral urban data. However, this information is not always well structured, and sometimes it is even simply corrupted GIS data. In this paper we present a robust and generic solution for the generation of block and building layouts based on a repairing process applied when this data is not correct. Our input data is a top projection map of a city which usually has been created by a mixture of photogrammetric restitution and, in a second stage, hand-drawn using any GIS application. Moreover, these maps are under continuous modifications, like in the case of public administrations. This process sometimes results in the introduction of mistakes and anomalies, which are hard to correct without the appropriate tools. Our solution is based on a novel semi-automatic 2D restructuring algorithm, which uniformly corrects errors and ambiguities that are commonly present in corrupted cadastral data. This problem is complex because it is necessary to identify not just simple elements from the input file, but also their connectivity and structure in the real world. The output of our algorithm is the urban data restructured into a hierarchy of blocks and buildings, from which we can get a realistic 3D model by extruding each building using the floor number for each building within the cadastral data.
C1 [Patow, Gustavo] Comp Sci & Appl Math Dept, Girona, Spain.
RP Patow, G (corresponding author), Edifici P IV,Campus Montilivi, Girona 17071, Spain.
EM oriol@ima.udg.edu; dagush@ima.udg.edu
RI Patow, Gustavo A./L-2943-2015
OI Patow, Gustavo A./0000-0002-1977-9101
FU Ministerio de Ciencia e Innovacion, Spain [TIN2010-20590-C02-02]
FX This work was funded with grant TIN2010-20590-C02-02 from the Ministerio
   de Ciencia e Innovacion, Spain.
CR AGARWAL S., 2009, BUILDING ROME DAY
   Aliaga D.G., 2008, P ACM SIGGRAPH AS 20, DOI [10.1145/1457515.1409113, DOI 10.1145/1457515.1409113, DOI 10.1145/1409060.1409113]
   [Anonymous], 2009, COMPUT GRAPH FORUM
   Autodesk, 2012, AUT MAP 3D
   BAREQUET G, 1995, COMPUT AIDED GEOM D, V12, P207, DOI 10.1016/0167-8396(94)00011-G
   Bischoff S, 2005, ACM T GRAPHIC, V24, P1332, DOI 10.1145/1095878.1095883
   Borodin P, 2002, ADVANCES IN MODELLING, ANIMATION AND RENDERING, P201
   Chen GN, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360702
   Delafontaine M, 2009, INT J GEOGR INF SCI, V23, P719, DOI 10.1080/13658810701694838
   Douglas D.H., 1973, Cartographica: The International Journal for Geographic Information and Geovisualization, V10, P112, DOI [https://doi.org/10.3138/FM57-6770-U75U-7727, DOI 10.1002/9780470669488.CH2]
   Esri, 2012, CIT
   European community, 2007, INFR SPAT INF INSP
   Fabritius G., 2008, VIRTUELLE ERWEITERTE
   Feuchtwanger M., 1989, P AUTO CARTO BALTIMO, P599
   Flamanc D., 2005, PIA05
   Ghawana T., 2010, GEOSPATIAL WORLD
   Hu JH, 2003, IEEE COMPUT GRAPH, V23, P62, DOI 10.1109/MCG.2003.1242383
   Kluckner S., 2011, THESIS GRAZ U TECHNO
   LAURINI R, 1994, COMPUT GRAPH, V18, P803, DOI 10.1016/0097-8493(94)90006-X
   Lefebvre S, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778821
   Lewis R, 1998, COMPUT AIDED DESIGN, V30, P765, DOI 10.1016/S0010-4485(98)00031-1
   Love K.R., 2007, THESIS VIRGINIA POLY
   Maras SS, 2010, INT J PHYS SCI, V5, P476
   Müller P, 2006, ACM T GRAPHIC, V25, P614, DOI 10.1145/1141911.1141931
   Parish YIH, 2001, COMP GRAPH, P301, DOI 10.1145/383259.383292
   Plumer L., 1997, GeoInformatica, V1, P345, DOI 10.1023/A:1009706411129
   Rock S.J., 1992, P S SOLID FREEFORM F, P86
   Rodríguez A, 2004, LECT NOTES COMPUT SC, V3300, P237
   Sae-Jung J., 2008, 21 ISPRS C TECHN COM, P825
   Schwarzkopf O.., 2000, Computational Geometry: Algorithms and Applications, V2nd
   Sudduth KA, 2007, AGRON J, V99, P1471, DOI 10.2134/agronj2006.0326
   Ubeda T, 1997, LECT NOTES COMPUT SC, V1262, P283
   Vanegas CA, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618457
   Watson B, 2008, IEEE COMPUT GRAPH, V28, P18, DOI 10.1109/MCG.2008.58
   Weber B, 2009, COMPUT GRAPH FORUM, V28, P481, DOI 10.1111/j.1467-8659.2009.01387.x
   Yin XT, 2009, IEEE COMPUT GRAPH, V29, P20, DOI 10.1109/MCG.2009.9
NR 36
TC 3
Z9 4
U1 0
U2 16
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2014
VL 30
IS 2
BP 159
EP 172
DI 10.1007/s00371-013-0791-7
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AA9DL
UT WOS:000331393900003
DA 2024-07-18
ER

PT J
AU Li, T
   Ye, M
   Ding, J
AF Li, Tao
   Ye, Mao
   Ding, Jian
TI Discriminative Hough context model for object detection
SO VISUAL COMPUTER
LA English
DT Article
DE Hough context; Feature correlation; ISM model
ID RECOGNITION
AB This paper presents a novel Hough context model for detecting and localizing instances of a certain object class. In particular, our approach detects the object in parameter space by applying Hough voting in context. First, in Hough context, the features around object center or a point in parameter space are sampled and organized in context sense respectively for both training images and test images. Then a parametric discriminant function is constructed based on the Hough context. Such function scores the existence likelihood of object centers in parameter space. Finally, we formulate the training process of this discriminative model as a structure learning problem, which has already been well solved. Compared with the ISM-related methods that the voting is performed by the features independently, the core contribution of our method is that the voting based on context information is available. The experiments on several popular and challenging object datasets (i.e., ETHZ Shape, UIUC Cars, and PASCAL VOC 2007) demonstrate that the detection accuracy can be improved and the voting speed are impressively accelerated via using Hough context.
C1 [Li, Tao; Ye, Mao; Ding, Jian] Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 611731, Peoples R China.
   [Li, Tao] HeNan Radio & Televis Univ, Dept Informat Engn, Zhengzhou 450046, Peoples R China.
C3 University of Electronic Science & Technology of China
RP Li, T (corresponding author), Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 611731, Peoples R China.
EM cvlab.uestc@gmail.com; yem_mei29@hotmail.com; dingjianhl@gmail.com
RI Ye, Mao/K-3012-2019; LIU, JIALIN/JXN-8034-2024
OI Ye, Mao/0000-0001-9253-1332; Ye, Mao/0000-0003-4760-8702
FU 973 National Basic Research Program of China [2010CB732501]; Foundation
   of Sichuan Excellent Young Talents [09ZQ026035]; Fundamental Research
   Funds for the Central University
FX This work was supported in part by the 973 National Basic Research
   Program of China (2010CB732501), Foundation of Sichuan Excellent Young
   Talents (09ZQ026035) and Fundamental Research Funds for the Central
   University.
CR Agarwal S, 2004, IEEE T PATTERN ANAL, V26, P1475, DOI 10.1109/TPAMI.2004.108
   Amit Y, 1997, NEURAL COMPUT, V9, P1545, DOI 10.1162/neco.1997.9.7.1545
   An SJ, 2010, PROC CVPR IEEE, P926, DOI 10.1109/CVPR.2010.5540119
   An SJ, 2009, PROC CVPR IEEE, P264, DOI 10.1109/CVPRW.2009.5206822
   [Anonymous], 2011, CVPR
   [Anonymous], CVPR
   [Anonymous], CVPR
   [Anonymous], CVPR
   [Anonymous], 2008, CVPR
   [Anonymous], BMVC
   [Anonymous], CVPR
   Belongie S, 2002, IEEE T PATTERN ANAL, V24, P509, DOI 10.1109/34.993558
   Blaschko MB, 2008, PROC CVPR IEEE, P93, DOI 10.1109/CVPR.2008.4587353
   Breiman L., 2001, Machine Learning, V45, P5, DOI 10.1023/A:1010933404324
   Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236
   Csurka G., 2004, Workshop on Statistical Learning in Computer Vision, ECCV, P59
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Desai C., 2009, CVPR
   Do Trinh-Minh-Tri., 2009, ICML
   Felzenszwalb PF, 2005, INT J COMPUT VISION, V61, P55, DOI 10.1023/B:VISI.0000042934.15159.49
   Fergus R, 2007, INT J COMPUT VISION, V71, P273, DOI 10.1007/s11263-006-8707-x
   Fergus R, 2003, PROC CVPR IEEE, P264
   Ferrari V, 2010, INT J COMPUT VISION, V87, P284, DOI 10.1007/s11263-009-0270-9
   Fidler S., 2010, ECCV
   FISCHLER MA, 1973, IEEE T COMPUT, VC 22, P67, DOI 10.1109/T-C.1973.223602
   Gall J, 2009, PROC CVPR IEEE, P1022, DOI 10.1109/CVPRW.2009.5206740
   Joachims T, 2009, MACH LEARN, V77, P27, DOI [10.1007/S10994-009-5108-8, 10.1007/s10994-009-5108-8]
   Lazebnik S., 2006, P IEEE CVF C COMP VI, DOI [DOI 10.1109/CVPR.2006.68, 10.1109/CVPR.2006.68]
   Lazebnik S, 2009, OBJECT CATEGORIZATIO
   Lehmann A, 2011, INT J COMPUT VISION, V94, P175, DOI 10.1007/s11263-010-0342-x
   Leibe B, 2008, INT J COMPUT VISION, V77, P259, DOI 10.1007/s11263-007-0095-3
   Leibe Bastian, 2004, WORKSH STAT LEARN CO
   Ma T., 2011, CVPR
   Maji S., 2009, CVPR
   Opelt A., 2006, CVPR
   Opelt Andreas., 2006, ECCV
   Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb
   Wallraven C., 2003, ICCV
   Wang X., 2010, CVPR
   Yarlagadda P., 2010, ECCV
   Zhang J, 2007, INT J COMPUT VISION, V73, P213, DOI 10.1007/s11263-006-9794-4
   Zhang S., 2010, ACM Multimedia
   Zhang Yimeng., 2009, CVPR
   Zhu L, 2010, PROC CVPR IEEE, P1919
NR 44
TC 11
Z9 12
U1 0
U2 7
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2014
VL 30
IS 1
BP 59
EP 69
DI 10.1007/s00371-013-0780-x
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 291AR
UT WOS:000329800500005
DA 2024-07-18
ER

PT J
AU Ye, JW
   Yu, JY
AF Ye, Jinwei
   Yu, Jingyi
TI Ray geometry in non-pinhole cameras: a survey
SO VISUAL COMPUTER
LA English
DT Article
DE Camera models; Ray geometry; Thin lens; Catadioptric imaging; Computer
   vision; Computer graphics; Computational photography
ID STEREO; PERSPECTIVE
AB A pinhole camera collects rays passing through a common 3D point and its image resembles what would be seen by human eyes. In contrast, a non-pinhole (multi-perspective) camera combines rays collected by different viewpoints. Despite their incongruity of view, their images are able to preserve spatial coherence and can depict, within a single context, details of a scene that are simultaneously inaccessible from a single view, yet easily interpretable by a viewer. In this paper, we thoroughly discuss the design, modeling, and implementation of a broad class of non-pinhole cameras and their applications in computer graphics and vision. These include mathematical (conceptual) camera models such as the General Linear Cameras and real non-pinhole cameras such as catadioptric cameras and projectors. A unique component of this paper is a ray geometry analysis that uniformly models these non-pinhole cameras as manifolds of rays and ray constraints. We also model the thin lens as a ray transform and study how ray geometry is changed by the thin lens for studying distortions and defocusing. We hope to provide mathematical fundamentals to satisfy computer vision researchers as well as tools and algorithms to aid computer graphics and optical engineering researchers.
C1 [Ye, Jinwei; Yu, Jingyi] Univ Delaware, Newark, DE 19716 USA.
C3 University of Delaware
RP Ye, JW (corresponding author), Univ Delaware, Newark, DE 19716 USA.
EM jye@cis.udel.edu
FU Direct For Computer & Info Scie & Enginr; Div Of Information &
   Intelligent Systems [0845268] Funding Source: National Science
   Foundation
CR Adelson E.H., 1991, Computational Models of Visual Processing, P3
   ADELSON EH, 1992, IEEE T PATTERN ANAL, V14, P99, DOI 10.1109/34.121783
   Agarwala A, 2006, ACM T GRAPHIC, V25, P853, DOI 10.1145/1141911.1141966
   Agrawal A, 2010, LECT NOTES COMPUT SC, V6313, P129
   [Anonymous], 2003, PROC 19 ANN S COMPUT, DOI DOI 10.1145/777792.777839
   [Anonymous], CTUCMP200202
   [Anonymous], PIN
   [Anonymous], 2009, ACM SIGGRAPH ASIA
   [Anonymous], P 10 IEEE INT C COMP
   [Anonymous], P SIGGRAPH AS C
   [Anonymous], P 9 IEEE INT C COMP
   [Anonymous], COMPUTATIONAL PHOTOG
   [Anonymous], 2009, INT C COMP PHOT
   [Anonymous], COMPUTER VISION PATT
   [Anonymous], 2007, ACM INT C PROCEEDING
   [Anonymous], P BRIT MACH VIS C SE
   [Anonymous], P 13 IEEE INT C COMP
   [Anonymous], COMPUTER VISION PATT
   [Anonymous], P REND TECHN EUR S R
   Baker S, 1999, INT J COMPUT VISION, V35, P175, DOI 10.1023/A:1008128724364
   Buehler C, 2001, COMP GRAPH, P425, DOI 10.1145/383259.383309
   Ding YY, 2009, IEEE I CONF COMP VIS, P1827, DOI 10.1109/ICCV.2009.5459406
   Ding YY, 2009, PROC CVPR IEEE, P2326, DOI 10.1109/CVPRW.2009.5206624
   Gortler S. J., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P43, DOI 10.1145/237170.237200
   Gupta R, 1997, IEEE T PATTERN ANAL, V19, P963, DOI 10.1109/34.615446
   Hartley R., 2003, MULTIPLE VIEW GEOMET
   Isaksen A, 2000, COMP GRAPH, P297, DOI 10.1145/344779.344929
   Kim C, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024224
   Kuthirummal S, 2006, ACM T GRAPHIC, V25, P916, DOI 10.1145/1141911.1141975
   Levoy M., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P31, DOI 10.1145/237170.237199
   McMillan L., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P39, DOI 10.1145/218380.218398
   Mei CH, 2005, COMPUT GRAPH FORUM, V24, P335, DOI 10.1111/j.1467-8659.2005.00858.x
   Meyer M., 2002, VISUALIZATION MATH, V6, P35, DOI DOI 10.1007/978-3-662-05105-4_2
   Mohan A., 2009, ACM SIGGRAPH
   Nayar SK, 1997, PROC CVPR IEEE, P482, DOI 10.1109/CVPR.1997.609369
   Ng R, 2005, ACM T GRAPHIC, V24, P735, DOI 10.1145/1073204.1073256
   Pajdla T, 2001, IEEE WORKSHOP ON STEREO AND MULTI-BASELINE VISION, PROCEEDINGS, P85, DOI 10.1109/SMBV.2001.988766
   Peleg S, 2000, IEEE T PATTERN ANAL, V22, P1144, DOI 10.1109/34.879794
   Peleg S., 1999, Computer Vision and Pattern Recognition
   Ramalingam S, 2006, LECT NOTES COMPUT SC, V3851, P704
   Rucker R., 1984, THE 4 DIMENSION
   Seitz SM, 2002, INT J COMPUT VISION, V48, P21, DOI 10.1023/A:1014851111084
   Shum HY, 2002, INT J COMPUT VISION, V48, P151, DOI 10.1023/A:1016051024520
   Soler C, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1516522.1516529
   Swaminathan R, 2006, INT J COMPUT VISION, V66, P211, DOI 10.1007/s11263-005-3220-1
   Swaminathan R, 2003, PROC CVPR IEEE, P594
   Swaminathan R, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P2, DOI 10.1109/ICCV.2001.937581
   Taguchi Y., 2010, ACM SIGGRAPH ASIA
   Unger J., 2003, Eurographics Symposium on Rendering. 14th Eurographics Workshop on Rendering, P141
   VEERARAGHAVAN A, 2007, ACM SIGGRAPH
   Wetzstein Gordon, 2011, Computational Photography (ICCP), 2011 IEEE International Conference on, P1
   Wilburn B, 2005, ACM T GRAPHIC, V24, P765, DOI 10.1145/1073204.1073259
   Wood D. N., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P243, DOI 10.1145/258734.258859
   Ye Jinwei., 2012, Computer Vision and Pattern Recognition
   Yu J, 2010, COMPUT GRAPH FORUM, V29, P227, DOI 10.1111/j.1467-8659.2009.01587.x
   Yu J., 2004, ECCV
   Yu JY, 2005, PROC CVPR IEEE, P117
   Yuanyuan Ding, 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2528, DOI 10.1109/CVPRW.2009.5206622
   Zomet A, 2003, IEEE T PATTERN ANAL, V25, P741, DOI 10.1109/TPAMI.2003.1201823
NR 59
TC 12
Z9 15
U1 4
U2 21
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2014
VL 30
IS 1
BP 93
EP 112
DI 10.1007/s00371-013-0786-4
PG 20
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 291AR
UT WOS:000329800500008
DA 2024-07-18
ER

PT J
AU Liu, SK
   Wang, JJ
   Zhang, MM
   Wang, ZY
AF Liu, Shoukuai
   Wang, Jiajun
   Zhang, Mingmin
   Wang, Zhiyong
TI Three-dimensional cartoon facial animation based on art rules
SO VISUAL COMPUTER
LA English
DT Article
DE Cartoon facial animation; Caricature; Art rules; Model exaggeration;
   Non-photorealistic rendering
AB Three-dimensional (3D) cartoon facial animation is one step further than the challenging 3D caricaturing which generates 3D still caricatures only. In this paper, a 3D cartoon facial animation system is developed for a subject given only a single frontal face image of a neutral expression. The system is composed of three steps consisting of 3D cartoon face exaggeration, texture processing, and 3D cartoon facial animation. By following caricaturing rules of artists, instead of mathematical formulations, 3D cartoon face exaggeration is accomplished at both global and local levels. As a result, the final exaggeration is capable of depicting the characteristics of an input face while achieving artistic deformations. In the texture processing step, texture coordinates of the vertices of the cartoon face model are obtained by mapping the parameterized grid of the standard face model to a cartoon face template and aligning the input face to the face template.
   Finally, 3D cartoon facial animation is implemented in the MPEG-4 animation framework. In order to avoid time-consuming construction of a face animation table, we propose to utilize the tables of existing models through model mapping. Experimental results demonstrate the effectiveness and efficiency of our proposed system.
C1 [Liu, Shoukuai; Wang, Jiajun] Soochow Univ, Sch Elect & Informat Engn, Suzhou 215006, Peoples R China.
   [Zhang, Mingmin] Zhejiang Univ, State Key Lab CAD&CG, Hangzhou 310058, Zhejiang, Peoples R China.
   [Wang, Zhiyong] Univ Sydney, Sch Informat Technol, Sydney, NSW 2006, Australia.
C3 Soochow University - China; Zhejiang University; University of Sydney
RP Wang, JJ (corresponding author), Soochow Univ, Sch Elect & Informat Engn, Suzhou 215006, Peoples R China.
EM jjwang@suda.edu.cn
RI Zhang, Miao/JXY-8985-2024; zhang, mm/IWV-4201-2023
FU National Natural Science Foundation of China [60871086, 60970076,
   61173124]; Natural Science Foundation of Jiangsu Province China
   [BK2008159]; ARC (Australian Research Council)
FX This research is supported by the National Natural Science Foundation of
   China, Nos. 60871086, 60970076, 61173124, the Natural Science Foundation
   of Jiangsu Province China No. BK2008159, and ARC (Australian Research
   Council) grants.
CR Abrantes GA, 1999, IEEE T CIRC SYST VID, V9, P290, DOI 10.1109/76.752096
   Akleman E., 2004, ACM ANN C COMP GRAPH
   BIER EA, 1986, IEEE COMPUT GRAPH, V6, P40, DOI 10.1109/MCG.1986.276545
   Chiang P.Y., 2004, AS C COMP VIS JEJ KO
   Clarke L, 2011, IEEE T VIS COMPUT GR, V17, P808, DOI 10.1109/TVCG.2010.76
   COOTES TF, 1995, COMPUT VIS IMAGE UND, V61, P38, DOI 10.1006/cviu.1995.1004
   Erol F, 2001, WSCG '2001: SHORT COMMUNICATIONS AND POSTERS, pP5
   Ersotelos N, 2008, VISUAL COMPUT, V24, P13, DOI 10.1007/s00371-007-0175-y
   Fu GH, 2008, 2008 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOLS 1-4, P965, DOI 10.1109/ICME.2008.4607597
   Fujiwara T, 2002, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL I AND II, PROCEEDINGS, pA137
   Gao W, 2003, IEEE T CIRC SYST VID, V13, P1119, DOI 10.1109/TCSVT.2003.817629
   Jin Xiaogang, 1997, Journal of Software, V8, P241
   [蓝娟 Lan Juan], 2004, [计算机工程与应用, Computer Engineering and Application], V40, P60
   Li PF, 2008, 2008 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOLS 1-4, P941, DOI 10.1109/ICME.2008.4607591
   Liang L, 2002, 10TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P386, DOI 10.1109/PCCGA.2002.1167882
   Liu J., 2009, COMPUT GRAPH FORUM, V1, P1
   Luan Q., 2009, COMPUT GRAPH, V21, P1733
   Mo Z., 2004, INT C COMP GRAPH INT
   Noh J.Y., 1998, TR199899705 U SO CAL
   Noh Y., 2000, ACM VIRT S REAL SOFT
   Obaid M, 2010, 7 INT C COMP GRAPH I
   Redman L., 1984, DRAW CARICATURES
   Sederberg T. W., 1986, Computer Graphics, V20, P151, DOI 10.1145/15886.15903
   Song ML, 2007, IEEE T MULTIMEDIA, V9, P1384, DOI 10.1109/TMM.2007.906591
   Sun J, 2004, ACM T GRAPHIC, V23, P315, DOI 10.1145/1015706.1015721
   Tong Jing, 2007, Journal of Computer Applications, V27, P1013
   Xie J., 2009, Proceedings of the ACM International Conference on Multimedia, DOI 10.1145/1631272.1631403
   Zhang M., 2010, INT C VIRT REAL CONT
   Zhou Renqin, 2006, Journal of Computer Aided Design & Computer Graphics, V18, P1362
NR 29
TC 6
Z9 9
U1 1
U2 23
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2013
VL 29
IS 11
BP 1135
EP 1149
DI 10.1007/s00371-012-0756-2
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 236PL
UT WOS:000325811300003
DA 2024-07-18
ER

PT J
AU Hu, JX
   Hua, J
AF Hu, Jiaxi
   Hua, Jing
TI Pose analysis using spectral geometry
SO VISUAL COMPUTER
LA English
DT Article
DE Semantic skeleton extraction; Spectral geometry; Joint detection
ID SKELETON; DEFORMATION; COMPUTATION
AB We propose a novel method to analyze a set of poses of 3D models that are represented with triangle meshes and unregistered. Different shapes of poses are transformed from the 3D spatial domain to a geometry spectrum domain that is defined by Laplace-Beltrami operator. During this space-spectrum transform, all near-isometric deformations, mesh triangulations and Euclidean transformations are filtered away. The different spatial poses from a 3D model are represented with near-isometric deformations; therefore, they have similar behaviors in the spectral domain. Semantic parts of that model are then determined based on the computed geometric properties of all the mapped vertices in the geometry spectrum domain. Semantic skeleton can be automatically built with joints detected as well. The Laplace-Beltrami operator is proved to be invariant to isometric deformations and Euclidean transformations such as translation and rotation. It also can be invariant to scaling with normalization. The discrete implementation also makes the Laplace-Beltrami operator straightforward to be applied on triangle meshes despite triangulations. Our method turns a rather difficult spatial problem into a spectral problem that is much easier to solve. The applications show that our 3D pose analysis method leads to a registration-free pose analysis and a high-level semantic part understanding of 3D shapes.
C1 [Hu, Jiaxi; Hua, Jing] Wayne State Univ, Dept Comp Sci, Detroit, MI 48098 USA.
C3 Wayne State University
RP Hua, J (corresponding author), Wayne State Univ, Dept Comp Sci, 5057 Woodward Ave,STE 3010, Detroit, MI 48098 USA.
EM jinghua@cs.wayne.edu
FU Direct For Computer & Info Scie & Enginr; Div Of Information &
   Intelligent Systems [0915933] Funding Source: National Science
   Foundation
CR Au OKC, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1239451.1239534, 10.1145/1276377.1276481]
   Chu H.-K., 2009, IEEE T VIS COMPUT GR
   Cornea ND, 2007, IEEE T VIS COMPUT GR, V13, P530, DOI 10.1109/TVCG.2007.1002
   de Aguiar E, 2008, COMPUT GRAPH FORUM, V27, P389, DOI 10.1111/j.1467-8659.2008.01136.x
   Dey TK, 2010, PROC APPL MATH, V135, P650
   He Y, 2009, GRAPH MODELS, V71, P49, DOI 10.1016/j.gmod.2008.12.008
   Hu JX, 2009, VISUAL COMPUT, V25, P667, DOI 10.1007/s00371-009-0340-6
   Huang J, 2006, ACM T GRAPHIC, V25, P1126, DOI 10.1145/1141911.1142003
   James D.L, 2005, ACM T GRAPH, V24
   Karni Z, 2000, COMP GRAPH, P279, DOI 10.1145/344779.344924
   Kilian M, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1239451.1239515, 10.1145/1276377.1276457]
   Levy B, 2006, IEEE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS 2006, PROCEEDINGS, P66
   Lewis JP, 2000, COMP GRAPH, P165, DOI 10.1145/344779.344862
   Meyer M., 2002, VISMATH
   Pascucci V, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239509
   Patanè G, 2009, IEEE T VIS COMPUT GR, V15, P583, DOI 10.1109/TVCG.2009.22
   Reuter M, 2006, COMPUT AIDED DESIGN, V38, P342, DOI 10.1016/j.cad.2005.10.011
   Reuter M, 2010, INT J COMPUT VISION, V89, P287, DOI 10.1007/s11263-009-0278-1
   Reuter M, 2009, COMPUT AIDED DESIGN, V41, P739, DOI 10.1016/j.cad.2009.02.007
   Rustamov R.M, 2007, SGP 07
   SHINAGAWA Y, 1991, IEEE COMPUT GRAPH, V11, P44, DOI 10.1109/38.103393
   Sumner RW, 2004, ACM T GRAPHIC, V23, P399, DOI 10.1145/1015706.1015736
   Sundar H, 2003, SMI 2003: SHAPE MODELING INTERNATIONAL 2003, PROCEEDINGS, P130, DOI 10.1109/smi.2003.1199609
   VALLET B., 2008, COMPUT GRAPH FORUM
   Weber O, 2007, COMPUT GRAPH FORUM, V26, P265, DOI 10.1111/j.1467-8659.2007.01048.x
   Xu GL, 2006, INT J COMPUT GEOM AP, V16, P75, DOI 10.1142/S0218195906001938
   Yan HB, 2008, IEEE T VIS COMPUT GR, V14, P693, DOI 10.1109/TVCG.2008.28
   Zou GY, 2007, LECT NOTES COMPUT SC, V4791, P367
NR 28
TC 4
Z9 4
U1 0
U2 12
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2013
VL 29
IS 9
BP 949
EP 958
DI 10.1007/s00371-013-0850-0
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 186CB
UT WOS:000322018100010
DA 2024-07-18
ER

PT J
AU Wang, YB
   Dubey, R
   Magnenat-Thalmann, N
   Thalmann, D
AF Wang, Yanbin
   Dubey, Rohit
   Magnenat-Thalmann, Nadia
   Thalmann, Daniel
TI An immersive multi-agent system for interactive applications
SO VISUAL COMPUTER
LA English
DT Article
DE Multi-agent; Crowd simulation; Path planning; Interaction design
ID CROWD; NAVIGATION; SIMULATIONS; MODEL
AB This paper presents an interactive multi-agent system based on a fully immersive virtual environment. A user can interact with the virtual characters in real time via an avatar by changing their moving behavior. Moreover, the user is allowed to select any character as the avatar to be controlled. A path planning algorithm is proposed to address the problem of dynamic navigation of individual and groups of characters in the multi-agent system. A natural interface is designed for the interaction between the user and the virtual characters, as well as the virtual environment, based on gesture recognition. To evaluate the efficiency of the dynamic navigation method, performance results are provided. The presented system has the potential to be used in the training and evaluation of emergency evacuation and other real-time applications of crowd simulation with interaction.
C1 [Wang, Yanbin; Dubey, Rohit; Magnenat-Thalmann, Nadia; Thalmann, Daniel] Nanyang Technol Univ, Inst Media Innovat, Singapore 639798, Singapore.
   [Magnenat-Thalmann, Nadia] Univ Geneva, Res Lab MIRALab, Geneva, Switzerland.
C3 Nanyang Technological University; University of Geneva
RP Wang, YB (corresponding author), Nanyang Technol Univ, Inst Media Innovat, Singapore 639798, Singapore.
EM wangyb@ntu.edu.sg; rohitdubey@ntu.edu.sg; nadiathalmann@ntu.edu.sg;
   danielthalmann@ntu.edu.sg
RI Thalmann, Daniel/AAL-1097-2020; Alidadi, Mehdi/HJZ-0235-2023; Thalmann,
   Daniel/A-4347-2008; Thalmann, Nadia/AAK-5195-2021
OI Thalmann, Daniel/0000-0002-0451-7491; Alidadi,
   Mehdi/0000-0001-5183-7829; Thalmann, Nadia/0000-0002-1459-5960; Dubey,
   Rohit Kumar/0000-0003-0273-7850
CR [Anonymous], 2009, P 4 INT C FDN DIGITA, DOI DOI 10.1145/1536513.1536540
   [Anonymous], 1999, P GAM DEV C
   Arikan O, 2001, SPRING EUROGRAP, P151
   FIORINI P, 1993, PROCEEDINGS : IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION, VOLS 1-3, P560, DOI 10.1109/ROBOT.1993.292038
   FORTUNE S, 1987, ALGORITHMICA, V2, P153, DOI 10.1007/BF01840357
   Geraerts R, 2007, COMPUT ANIMAT VIRT W, V18, P107, DOI 10.1002/cav.166
   Gerald CF., 2004, Applied numerical analysis, V7
   Goldenstein S, 2001, COMPUT GRAPH-UK, V25, P983, DOI 10.1016/S0097-8493(01)00153-4
   Helbing D, 2005, TRANSPORT SCI, V39, P1, DOI 10.1287/trsc.1040.0108
   HELBING D, 1995, PHYS REV E, V51, P4282, DOI 10.1103/PhysRevE.51.4282
   Hughes RL, 2002, TRANSPORT RES B-METH, V36, P507, DOI 10.1016/S0191-2615(01)00015-7
   Kamphuis A., 2004, SCA '04: Proceedings of the 2004 ACM SIGGRAPH/Eurographics symposium on Computer animation, P19
   Kapadia M., 2009, Proceedings of the 2009 symposium on Interactive 3D graphics and games, I3D '09, P215
   Karamouzas I, 2009, LECT NOTES COMPUT SC, V5884, P41, DOI 10.1007/978-3-642-10347-6_4
   Kavraki LE, 1996, IEEE T ROBOTIC AUTOM, V12, P566, DOI 10.1109/70.508439
   Lamarche F, 2004, COMPUT GRAPH FORUM, V23, P509, DOI 10.1111/j.1467-8659.2004.00782.x
   Morini F, 2007, 2007 INTERNATIONAL CONFERENCE ON CYBERWORLDS, PROCEEDINGS, P144, DOI 10.1109/CW.2007.23
   Patil S, 2011, IEEE T VIS COMPUT GR, V17, P244, DOI 10.1109/TVCG.2010.33
   Pelechano N, 2007, SYMPOSIUM ON COMPUTER ANIMATION 2007: ACM SIGGRAPH/ EUROGRAPHICS SYMPOSIUM PROCEEDINGS, P99
   Pettré J, 2007, IEEE INT CONF ROBOT, P3062, DOI 10.1109/ROBOT.2007.363937
   Pettré J, 2006, COMPUT ANIMAT VIRT W, V17, P445, DOI 10.1002/cav.147
   Rao YB, 2011, MULTIMED TOOLS APPL, V54, P397, DOI 10.1007/s11042-010-0542-y
   Rodriguez S, 2011, IEEE INT CONF ROBOT, P1738
   Salomon B., 2003, Proceedings of the 2003 symposium on Interactive 3D graphics, P41
   Sud A, 2008, IEEE T VIS COMPUT GR, V14, P526, DOI 10.1109/TVCG.2008.27
   Takahashi S, 2009, COMPUT GRAPH FORUM, V28, P639, DOI 10.1111/j.1467-8659.2009.01404.x
   Treuille A, 2006, ACM T GRAPHIC, V25, P1160, DOI 10.1145/1141911.1142008
   van den Berg J, 2008, I3D 2008: SYMPOSIUM ON INTERACTIVE 3D GRAPHICS AND GAMES, PROCEEDINGS, P139
   Yersin B, 2008, VISUAL COMPUT, V24, P859, DOI 10.1007/s00371-008-0286-0
NR 29
TC 9
Z9 9
U1 2
U2 33
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2013
VL 29
IS 5
BP 323
EP 332
DI 10.1007/s00371-012-0735-7
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 127RD
UT WOS:000317715200001
DA 2024-07-18
ER

PT J
AU Lavoué, G
AF Lavoue, Guillaume
TI Combination of bag-of-words descriptors for robust partial shape
   retrieval
SO VISUAL COMPUTER
LA English
DT Article
DE Shape retrieval; 3D model; Bag of words
ID SIGNATURE; MODEL
AB This paper presents a 3D shape retrieval algorithm based on the Bag of Words (BoW) paradigm. For a given 3D shape, the proposed approach considers a set of feature points uniformly sampled on the surface and associated with local Fourier descriptors. This descriptor is computed in the neighborhood of each feature point by projecting the geometry onto the eigenvectors of the Laplace-Beltrami operator; it is very informative, robust to connectivity and geometry changes, and also fast to compute. In a preliminary step, a visual dictionary is built by clustering a large set of feature descriptors, then each 3D shape is described by an histogram of occurrences of these visual words, hence discarding any spatial information. A spatially-sensitive algorithm is also presented where the 3D shape is described by an histogram of pairs of visual words. We show that these two approaches are complementary and can be combined to improve the performance and the robustness of the retrieval. The performances have been compared against very recent state-of-the-art methods on several different datasets. For global shape retrieval, our combined approach is comparable to these recent works, however, it clearly outperforms them in the case of partial shape retrieval.
C1 Univ Lyon, CNRS, INSA Lyon, LIRIS,UMR5205, Lyon, France.
C3 Institut National des Sciences Appliquees de Lyon - INSA Lyon; Centre
   National de la Recherche Scientifique (CNRS)
RP Lavoué, G (corresponding author), Univ Lyon, CNRS, INSA Lyon, LIRIS,UMR5205, Lyon, France.
EM glavoue@liris.cnrs.fr
CR Agathos A., 2009, EUR WORKSH 3D OBJ RE
   [Anonymous], 2004, ECCV INT WORKSH STAT
   [Anonymous], EUR WORKSH 3D OBJ RE
   [Anonymous], 2008, EUR WORKSH 3D OBJ RE
   BENCHEN M, 2008, EUR WORKSH 3D OBJ RE
   Bowers J., 2010, SIGGRAPH ASIA
   Bronstein AM, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1899404.1899405
   Bronstein AM, 2010, INT J COMPUT VISION, V89, P266, DOI 10.1007/s11263-009-0301-6
   Chen DY, 2003, COMPUT GRAPH FORUM, V22, P223, DOI 10.1111/1467-8659.00669
   Cornea N., 2007, SHREC2007 3D SHAP RE, P50
   Dey TK, 2010, COMPUT GRAPH FORUM, V29, P1545, DOI 10.1111/j.1467-8659.2010.01763.x
   Fei-Fei L, 2005, PROC CVPR IEEE, P524
   Ferreira A, 2010, INT J COMPUT VISION, V89, P327, DOI 10.1007/s11263-009-0257-6
   Fu Y, 2009, COMPUT AIDED GEOM D, V26, P711, DOI 10.1016/j.cagd.2009.03.007
   Funkhouser T, 2003, ACM T GRAPHIC, V22, P83, DOI 10.1145/588272.588279
   Funkhouser T., 2006, P 4 EUR S GEOM PROC, P142
   Gal R, 2007, IEEE T VIS COMPUT GR, V13, P261, DOI 10.1109/TVCG.2007.45
   Itskovich A, 2011, COMPUT GRAPH-UK, V35, P334, DOI 10.1016/j.cag.2010.11.010
   Jain V, 2007, COMPUT AIDED DESIGN, V39, P398, DOI 10.1016/j.cad.2007.02.009
   Järvelin K, 2002, ACM T INFORM SYST, V20, P422, DOI 10.1145/582415.582418
   Lavoue G., 2011, EUR WORKSH 3D OBJ RE
   Levy B., 2010, SIGGRAPH 2010 COURS
   Li XY, 2009, MODELLING SIMULATION, P437, DOI 10.1109/ICIP.2009.5414415
   Lian Z., 2010, SHAPE MODELING INT
   Liu Y., 2006, Computer Vision and Pattern Recognition, P2025, DOI DOI 10.1109/CVPR.2006.278
   LLOYD SP, 1982, IEEE T INFORM THEORY, V28, P129, DOI 10.1109/TIT.1982.1056489
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Marini S., 2007, SHREC2007 3D SHAP RE, P53
   Mikolajczyk K, 2005, IEEE T PATTERN ANAL, V27, P1615, DOI 10.1109/TPAMI.2005.188
   Ohbuchi R., 2008, SHAPE MODELING INT
   Reuter M, 2006, COMPUT AIDED DESIGN, V38, P342, DOI 10.1016/j.cad.2005.10.011
   Ruggeri MR, 2010, INT J COMPUT VISION, V89, P248, DOI 10.1007/s11263-009-0250-0
   Rustamov Raif M, 2007, P S GEOM PROC, V257, P225
   Shilane P, 2004, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, P167, DOI 10.1109/smi.2004.1314504
   Sipiran I, 2011, VISUAL COMPUT, V27, P963, DOI 10.1007/s00371-011-0610-y
   Sun JA, 2010, COMPUT GRAPH FORUM, V29, P1535, DOI 10.1111/j.1467-8659.2010.01762.x
   Sun JA, 2009, COMPUT GRAPH FORUM, V28, P1383, DOI 10.1111/j.1467-8659.2009.01515.x
   Tabia H., 2010, IEEE INT C PATT REC
   Tierny J, 2009, COMPUT GRAPH FORUM, V28, P41, DOI 10.1111/j.1467-8659.2008.01190.x
   Toldo R., 2009, EUR WORKSH 3D OBJ RE
   Vallet B, 2008, COMPUT GRAPH FORUM, V27, P251, DOI 10.1111/j.1467-8659.2008.01122.x
   Wang K., 2009, THESIS I NATL SCI AP
NR 42
TC 81
Z9 89
U1 0
U2 8
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2012
VL 28
IS 9
BP 931
EP 942
DI 10.1007/s00371-012-0724-x
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 985QX
UT WOS:000307284700004
DA 2024-07-18
ER

PT J
AU Chen, GJ
   Peers, P
   Zhang, JW
   Tong, X
AF Chen, Guojun
   Peers, Pieter
   Zhang, Jiawan
   Tong, Xin
TI Real-time rendering of deformable heterogeneous translucent objects
   using multiresolution splatting
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Conference on the Computer Graphics International (CGI)
CY 2012
CL Bournemouth, ENGLAND
DE Translucency; Real-time rendering; Image-space splatting; Heterogeneous;
   Deformable
AB In this paper, we present a novel real-time rendering algorithm for heterogenous translucent objects with deformable geometry. The proposed method starts by rendering the surface geometry in two separate geometry buffers-the irradiance buffer and the splatting buffer-with corresponding mipmaps from the lighting and viewing directions, respectively. Irradiance samples are selected from the irradiance buffer according to geometric and material properties using a novel and fast selection algorithm. Next, we gather the irradiance per visible surface point by splatting the irradiance samples to the splatting buffer. To compute the appearance of long-distance low-frequency subsurface scattering, as well as short-range detailed scattering, a fast novel multiresolution GPU algorithm is developed that computes everything on the fly and which does not require any precomputations. We illustrate the effectiveness of our method on several deformable geometries with measured heterogeneous translucent materials.
C1 [Zhang, Jiawan] Tianjin Univ, Sch Comp Software, Tianjin 300072, Peoples R China.
   [Peers, Pieter] Coll William & Mary, Dept Comp Sci, Williamsburg, VA 23185 USA.
   [Tong, Xin] Microsoft Res Asia, Internet Graph Grp, Beijing, Peoples R China.
C3 Tianjin University; William & Mary; Microsoft; Microsoft Research Asia
RP Zhang, JW (corresponding author), Tianjin Univ, Sch Comp Software, Tianjin 300072, Peoples R China.
EM jwZhang@tju.edu.cn
OI Tong, Xin/0000-0001-8788-2453; Zhang, Jiawan/0000-0002-0667-6744
CR CARR N.A., 2003, HWWS 03, P51
   Chang CW, 2008, COMPUT GRAPH FORUM, V27, P517, DOI 10.1111/j.1467-8659.2008.01149.x
   Dachsbacher C., 2003, Eurographics Symposium on Rendering. 14th Eurographics Workshop on Rendering, P197
   Dachsbacher C., 2005, Proc. Symp. Interactive Graph. and Games, P203, DOI DOI 10.1145/1053427.1053460
   Dachsbacher C., 2006, Proc. Symp. Interactive 3D Graph. and Games, Redwood City, P93, DOI DOI 10.1145/1111411.1111428
   dEon E., 2007, P EUR S REND TECHN, P147
   Haber T., 2005, GI 05 P GRAPHICS INT, P79
   Hao XJ, 2004, ACM T GRAPHIC, V23, P120, DOI 10.1145/990002.990004
   Jensen HW, 2001, COMP GRAPH, P511, DOI 10.1145/383259.383319
   Lensch HPA, 2002, 10TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P214, DOI 10.1109/PCCGA.2002.1167862
   Mertens T., 2003, Eurographics Symposium on Rendering. 14th Eurographics Workshop on Rendering, P130
   Nichols G., 2009, P 2009 S INTERACTIVE, P83, DOI DOI 10.1145/1507149.1507162
   Nichols G, 2010, IEEE T VIS COMPUT GR, V16, P729, DOI 10.1109/TVCG.2009.97
   Nichols G, 2009, COMPUT GRAPH FORUM, V28, P1141, DOI 10.1111/j.1467-8659.2009.01491.x
   Nicodemus F. E., 1992, Geometrical Considerations and Nomenclature for Reflectance, P94
   Peers P, 2006, ACM T GRAPHIC, V25, P746, DOI 10.1145/1141911.1141950
   Shah MA, 2009, IEEE COMPUT GRAPH, V29, P66, DOI 10.1109/MCG.2009.11
   Song Y., 2009, ACM SIGGRAPH 2009 PA, P31
   Stam J, 1995, SPRING COMP SCI, P41
   Wang JP, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360640
   Wang R, 2005, ACM T GRAPHIC, V24, P1202, DOI 10.1145/1073204.1073333
   Wang R, 2008, VISUAL COMPUT, V24, P565, DOI 10.1007/s00371-008-0237-9
   Wang YJ, 2010, COMPUT GRAPH FORUM, V29, P497, DOI 10.1111/j.1467-8659.2009.01619.x
   Xu K, 2007, COMPUT GRAPH FORUM, V26, P545, DOI 10.1111/j.1467-8659.2007.01077.x
NR 24
TC 9
Z9 10
U1 0
U2 8
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2012
VL 28
IS 6-8
BP 701
EP 711
DI 10.1007/s00371-012-0704-1
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 947EW
UT WOS:000304411500018
DA 2024-07-18
ER

PT J
AU Doidge, IC
   Jones, M
   Mora, B
AF Doidge, Ian C.
   Jones, Markw.
   Mora, Benjamin
TI Mixing Monte Carlo and progressive rendering for improved global
   illumination
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Conference on the Computer Graphics International (CGI)
CY 2012
CL Bournemouth, ENGLAND
DE Global illumination; Monte Carlo integration; Path tracing; Photon
   mapping
AB In this paper, we seek to eliminate the noise caused by caustic paths during progressive Monte Carlo path tracing. We employ a filtering strategy over path space, handling each subspace using specialized derivations of path tracing and progressive photon mapping. Evaluating diffuse paths with path tracing allows the use of sample stratification over both pixels and the image as a whole, whilst sharp detailed caustics are produced using progressive photon mapping. This is an efficient, low noise progressive algorithm with vanishing bias combining the advantages of both Monte Carlo methods, and particle tracing.
C1 [Doidge, Ian C.; Jones, Markw.; Mora, Benjamin] Swansea Univ, Dept Comp Sci, Swansea SA2 8PP, W Glam, Wales.
C3 Swansea University
RP Doidge, IC (corresponding author), Swansea Univ, Dept Comp Sci, Swansea SA2 8PP, W Glam, Wales.
EM csiand@swansea.ac.uk; m.w.jones@swansea.ac.uk; b.mora@swansea.ac.uk
RI ; Jones, Mark W./F-1114-2015
OI Mora, Benjamin/0000-0002-2945-3519; Jones, Mark W./0000-0001-8991-1190
FU EPSRC [EP/I005870/1, EP/I031243/1] Funding Source: UKRI
CR Budge BC, 2008, COMPUT GRAPH FORUM, V27, P1963, DOI 10.1111/j.1467-8659.2008.01345.x
   Chen JT, 2011, COMPUT GRAPH FORUM, V30, P1205, DOI 10.1111/j.1467-8659.2011.01979.x
   Cline D, 2005, ACM T GRAPHIC, V24, P1186, DOI 10.1145/1073204.1073330
   Dammertz H, 2010, COMPUT GRAPH FORUM, V29, P2504, DOI 10.1111/J.1467-8659.2010.01786.X
   DeCoro C, 2010, COMPUT GRAPH FORUM, V29, P2119, DOI 10.1111/j.1467-8659.2010.01799.x
   Donikian M, 2006, IEEE T VIS COMPUT GR, V12, P353, DOI 10.1109/TVCG.2006.41
   DUTRE P., 1993, Proceedings of Compugraphics '93, P128
   FAN S., 2005, RENDERING TECHNIQUES, P127
   Hachisuka T, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2019627.2019633
   Hachisuka T, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409083
   Hachisuka T, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618487
   Heckbert P. S., 1990, Computer Graphics, V24, P145, DOI 10.1145/97880.97895
   Jensen H. W., 1996, Rendering Techniques '96. Proceedings of the Eurographics Workshop. Eurographics, P21
   Kajiya J. T., 1986, SIGGRAPH, P143, DOI 10.1145/15886.15902
   Kollig T., 2004, MONTE CARLO QUASI MO
   Lafortune E. P., 1993, EDUGRAPHICS '93. First International Conference on Graphics Education. COMPUGRAPHICS '93. Third International Conference on Computational Graphics and Visualization Techniques. Combined Proceedings, P145
   Spencer B, 2009, COMPUT GRAPH FORUM, V28, P319, DOI 10.1111/j.1467-8659.2009.01371.x
   Spencer B, 2009, IEEE T VIS COMPUT GR, V15, P49, DOI 10.1109/TVCG.2008.67
   Veach E., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P419, DOI 10.1145/218380.218498
   Veach E., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P65, DOI 10.1145/258734.258775
   Veach Eric, 1994, P EUR REND WORKSH, P147, DOI DOI 10.1007/978-3-642-87825-1_
NR 21
TC 5
Z9 6
U1 0
U2 8
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2012
VL 28
IS 6-8
BP 603
EP 612
DI 10.1007/s00371-012-0703-2
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 947EW
UT WOS:000304411500009
DA 2024-07-18
ER

PT J
AU Attene, M
AF Attene, Marco
TI A lightweight approach to repairing digitized polygon meshes
SO VISUAL COMPUTER
LA English
DT Article
DE 3D scanning; Self-intersection; Degeneracy; Manifold
AB When designing novel algorithms for geometric processing and analysis, researchers often assume that the input conforms to several requirements. On the other hand, polygon meshes obtained from acquisition of real-world objects typically exhibit several defects, and thus are not appropriate for a widespread exploitation.
   In this paper, an algorithm is presented that strives to convert a low-quality digitized polygon mesh to a single manifold and watertight triangle mesh without degenerate or intersecting elements. Differently from most existing approaches that globally resample the model to produce a fixed version, the algorithm presented here attempts to modify the input mesh only locally within the neighborhood of undesired configurations.
   After having converted the input to a single combinatorial manifold, the algorithm proceeds iteratively by removing growing neighborhoods of undesired elements and by patching the resulting surface gaps until all the "defects" are removed. Though this heuristic approach is not guaranteed to converge, it was tested on more than 400 low-quality models and always succeeded. Furthermore, with respect to similar existing algorithms, it proved to be computationally efficient and produced more accurate results while using fewer triangles.
C1 CNR, IMATI GE, I-16149 Genoa, Italy.
C3 Consiglio Nazionale delle Ricerche (CNR); Istituto di Matematica
   Applicata e Tecnologie Informatiche "Enrico Magenes" (IMATI-CNR)
RP Attene, M (corresponding author), CNR, IMATI GE, Via De Marini 6, I-16149 Genoa, Italy.
EM marco.attene@cnr.it
FU FP7 FOCUS K3D Coordination Action
FX This work has been partially supported by the FP7 FOCUS K3D Coordination
   Action. Special thanks are due to Prof. Leif Kobbelt for the time spent
   to adapt his algorithm, for having provided results for comparison, and
   for the valuable information provided about his work. Also, I would like
   to thank Stephen Bischoff and his co-authors for having provided an
   implementation of the repairing method by Nooruddin and Turk that could
   be used to further compare the results presented here against the state
   of the art. Last but not least, I am grateful to Bianca Falcidieno,
   Michela Spagnuolo and all the members of the Shape Modeling Group at
   IMATI-CNR for helpful discussions.
CR ALBERTONI R, 2006, P 1 INT WORKSH SHAP
   Attene M, 2008, COMPUT GRAPH FORUM, V27, P1323, DOI 10.1111/j.1467-8659.2008.01271.x
   Attene M, 2006, IEEE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS 2006, PROCEEDINGS, P271
   BAREQUET G, 1995, COMPUT AIDED GEOM D, V12, P207, DOI 10.1016/0167-8396(94)00011-G
   BIASOTTI S, 2008, SHAPE MODELING APPL
   Bischoff S, 2005, COMPUT GRAPH FORUM, V24, P527, DOI 10.1111/j.1467-8659.2005.00878.x
   Bischoff S, 2005, ACM T GRAPHIC, V24, P1332, DOI 10.1145/1095878.1095883
   Borodin P, 2002, ADVANCES IN MODELLING, ANIMATION AND RENDERING, P201
   Botsch M., 2001, Vision, Modeling, and Visualization 2001. Proceedings, P283
   BOTSCH M, 2007, SIGGRAPH COURSE NOTE
   Branch J, 2007, THIRD INTERNATIONAL SYMPOSIUM ON 3D DATA PROCESSING, VISUALIZATION, AND TRANSMISSION, PROCEEDINGS, P727
   Cignoni P, 1998, COMPUT GRAPH FORUM, V17, P167, DOI 10.1111/1467-8659.00236
   Davis J, 2002, FIRST INTERNATIONAL SYMPOSIUM ON 3D DATA PROCESSING VISUALIZATION AND TRANSMISSION, P428, DOI 10.1109/TDPVT.2002.1024098
   DEY TK, 1999, PUBL I MATH-BEOGRAD, V20, P23
   FLORIANI LD, 2003, ACM SOLID MODELING, P304
   *GEOM INC, GEOM STUD
   Gottschalk S., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P171, DOI 10.1145/237170.237244
   Guéziec A, 2001, IEEE T VIS COMPUT GR, V7, P136, DOI 10.1109/2945.928166
   GUIBAS LJ, 1989, ACM S COMP GEOM, P208
   GUSKOV I, 2001, P GRAPH INT, P19
   *INNOVMETRIC SOFTW, POLYW
   Ju T, 2004, ACM T GRAPHIC, V23, P888, DOI 10.1145/1015706.1015815
   Liepa P., 2003, Symposium on Geometry Processing, P200
   Murali T. M., 1997, Proceedings 1997 Symposium on Interactive 3D Graphics, P155, DOI 10.1145/253284.253326
   Nooruddin FS, 2003, IEEE T VIS COMPUT GR, V9, P191, DOI 10.1109/TVCG.2003.1196006
   PODOLAK J, 2005, EUR S GEOM PROC, P33
   Rocchini C, 2004, VISUAL COMPUT, V20, P149, DOI 10.1007/s00371-003-0237-8
   Rossignac J., 1999, P 5 ACM S SOL MOD AP, P31
   Rourke C. P., 1972, Ergebnisse der Mathematik und ihrer Grenzgebiete, V69
   SAMET H, 1989, SPATIAL DATA STRUCTU
   Shewchuk JR, 2002, COMP GEOM-THEOR APPL, V22, P21, DOI 10.1016/S0925-7721(01)00047-5
   Si H, 2005, PROCEEDINGS OF THE 14TH INTERNATIONAL MESHING ROUNDTABLE, P147, DOI 10.1007/3-540-29090-7_9
   2004, AIM SHAPE SHAPE REPO
NR 33
TC 145
Z9 150
U1 2
U2 27
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2010
VL 26
IS 11
BP 1393
EP 1406
DI 10.1007/s00371-010-0416-3
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 653UV
UT WOS:000282123700006
DA 2024-07-18
ER

PT J
AU Huang, H
   Xiao, XZ
AF Huang, Hua
   Xiao, Xuezhong
TI Example-based contrast enhancement by gradient mapping
SO VISUAL COMPUTER
LA English
DT Article
DE Contrast enhancement; Gradient mapping; Example-based method
ID IMAGE; RETINEX; FRAMEWORK
AB Contrast enhancement is a very important problem in image processing. The key issue is how to assign correct enhancement levels for the local regions in an image, which makes previous methods incur much artifacts, e.g., over-enhancement, halo.
   In this paper, an example-based contrast enhancement algorithm is proposed, which works in the gradient domain. We utilize GMM model to describe the gradient distribution of an image. Then a GMM-based gradient mapping method is proposed to transfer the gradient of a reference image to the source image. The enhanced image is obtained by solving a Poisson equation defined by the altered gradient. Experimental results show the effectiveness and robustness of our method.
C1 [Huang, Hua; Xiao, Xuezhong] Xi An Jiao Tong Univ, Sch Elect & Informat Engn, Xian 710049, Peoples R China.
C3 Xi'an Jiaotong University
RP Huang, H (corresponding author), Xi An Jiao Tong Univ, Sch Elect & Informat Engn, 28 Xianning W Rd, Xian 710049, Peoples R China.
EM huanghua@xjtu.edu.cn; xzxiao@gmail.com
RI Huang, Hua/M-9684-2013
OI Huang, Hua/0000-0003-2587-1702
FU National Natural Science Foundation of China [60970068]; Chinese
   Ministry of Education [109142]; MOE [MOE-INTEL-09-07]
FX This work is partly supported by the National Natural Science Foundation
   of China (Grant No. 60970068), the Key Project of Chinese Ministry of
   Education (Grant No. 109142), and the MOE-Intel Joint Research Fund
   (Grant No. MOE-INTEL-09-07). We would like to thank Chih-Tsung Shen and
   Wen-Liang Hwang who helped generate the corresponding results of their
   retinex-based algorithm [24] (Fig. 2).
CR Agrawal A., 2007, ICCV 2007 COURS
   Arici T, 2009, IEEE T IMAGE PROCESS, V18, P1921, DOI 10.1109/TIP.2009.2021548
   Bishop C.M., 2006, J ELECTRON IMAGING, V16, P049901, DOI DOI 10.1117/1.2819119
   BOCKSTEIN IM, 1986, J OPT SOC AM A, V3, P735, DOI 10.1364/JOSAA.3.000735
   Cheng HD, 2010, SIGNAL PROCESS, V90, P1279, DOI 10.1016/j.sigpro.2009.10.013
   FARBMAN Z, 2008, SIGGRAPH 08
   Fattal R, 2002, ACM T GRAPHIC, V21, P249
   Gonzalez R. C., 2006, PEARSON ED INDIA, V3rd
   HALL EL, 1974, IEEE T COMPUT, VC 23, P207, DOI 10.1109/T-C.1974.223892
   Hsieh CT, 2003, PATTERN RECOGN, V36, P303, DOI 10.1016/S0031-3203(02)00032-8
   HURLBERT A, 1986, J OPT SOC AM A, V3, P1684, DOI 10.1364/JOSAA.3.001684
   Jobson DJ, 1997, IEEE T IMAGE PROCESS, V6, P965, DOI 10.1109/83.597272
   Jobson DJ, 1997, IEEE T IMAGE PROCESS, V6, P451, DOI 10.1109/83.557356
   Kim JY, 2001, IEEE T CIRC SYST VID, V11, P475, DOI 10.1109/76.915354
   Kimmel R, 2003, INT J COMPUT VISION, V52, P7, DOI 10.1023/A:1022314423998
   LAND EH, 1986, P NATL ACAD SCI USA, V83, P3078, DOI 10.1073/pnas.83.10.3078
   Lischinski D, 2006, ACM T GRAPHIC, V25, P646, DOI 10.1145/1141911.1141936
   Mantiuk R., 2006, ACM Transactions on Applied Perception, V3, P286, DOI DOI 10.1145/1166087.1166095
   MCCANN J, 2008, SIGGRAPH 08
   Meylan L, 2006, IEEE T IMAGE PROCESS, V15, P2820, DOI 10.1109/TIP.2006.877312
   Pratt W.K., 1977, DIGITAL IMAGE PROCES
   Rahman Z, 1996, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, PROCEEDINGS - VOL III, P1003, DOI 10.1109/ICIP.1996.560995
   Sakellaropoulos P, 2003, PHYS MED BIOL, V48, P787, DOI 10.1088/0031-9155/48/6/307
   Shen Chih-Tsung, 2009, P INT C IM PROC ICIP
   Vosoughi A, 2009, INT CONF ACOUST SPEE, P1173, DOI 10.1109/ICASSP.2009.4959798
   Xiao XZ, 2009, COMPUT GRAPH FORUM, V28, P1879, DOI 10.1111/j.1467-8659.2009.01566.x
   Xu K, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618464
   Zhu H, 1999, COMPUT VIS IMAGE UND, V73, P281, DOI 10.1006/cviu.1998.0723
NR 28
TC 10
Z9 11
U1 0
U2 11
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2010
VL 26
IS 6-8
BP 731
EP 738
DI 10.1007/s00371-010-0504-4
PG 8
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 602JQ
UT WOS:000278135800033
DA 2024-07-18
ER

PT J
AU Lin, CH
   Chao, MW
   Liang, CY
   Lee, TY
AF Lin, Chao-Hung
   Chao, Min-Wen
   Liang, Chan-Yu
   Lee, Tong-Yee
TI A novel semi-blind-and-semi-reversible robust watermarking scheme for 3D
   polygonal models
SO VISUAL COMPUTER
LA English
DT Article
DE Watermarking; Copyright protection
ID FRAGILE WATERMARKING
AB We introduce a novel semi-blind-and-semi- reversible robust watermarking scheme for three-dimensional (3D) polygonal models. The proposed approach embeds watermarks in the significant features of 3D models in a spread-spectrum manner. This novel scheme is robust against a wide variety of attacks including rotation, translation, scaling, noise addition, smoothing, mesh simplifications, vertex reordering, cropping, and even pose deformation of meshes. To the best of our knowledge, the existing approaches including blind, semi-blind, and non-blind detection schemes cannot withstand the attack of pose editing, which is a very common routine in 3D animation. In addition, the watermarked models can be semi-reversed (i.e., the peak signal-to-noise ratio (PSNR) of the recovered models is greater than 90 dB in all experiments) in semi-blind detection scheme. Experimental results show that this novel approach has many significant advantages in terms of robustness and invisibility over other state-of-the-art approaches.
C1 [Chao, Min-Wen; Liang, Chan-Yu; Lee, Tong-Yee] Natl Cheng Kung Univ, Dept Comp Sci & Informat Engn, Visual Syst Lab, Comp Graph Grp, Tainan 701, Taiwan.
   [Lin, Chao-Hung] Natl Cheng Kung Univ, Dept Geomat, Tainan 701, Taiwan.
C3 National Cheng Kung University; National Cheng Kung University
RP Lee, TY (corresponding author), Natl Cheng Kung Univ, Dept Comp Sci & Informat Engn, Visual Syst Lab, Comp Graph Grp, 1 Ta Hsueh Rd, Tainan 701, Taiwan.
EM cvivians@hotmail.com; tonylee@mail.ncku.edu.tw
FU National Science Council of Taiwan, R.O.C. [NSC-98-2221-E-006-179,
   NSC-97-2628-E-006-125-MY3, NSC-96-2628-E-006-200-MY3]; National Cheng
   Kung University, Taiwan [B0008, C0038]
FX We thank the anonymous reviewers for their insightful comments that
   helped us improve the paper. This work was supported by the National
   Science Council of Taiwan, R.O.C. under contract Nos.
   NSC-98-2221-E-006-179, NSC-97-2628-E-006125-MY3 and
   NSC-96-2628-E-006-200-MY3, and also supported by the Landmark Project of
   National Cheng Kung University, Taiwan under contract Nos. B0008 and
   C0038.
CR ALLCHIN D, 2001, PERSPECT SCI, P38
   Alliez P, 2003, ACM T GRAPHIC, V22, P485, DOI 10.1145/882262.882296
   [Anonymous], 2004, P 2004 EUR ACM SIGGR
   Ashourian M, 2004, COMPUTER GRAPHICS INTERNATIONAL, PROCEEDINGS, P590, DOI 10.1109/CGI.2004.1309270
   Benedens O, 1999, IEEE COMPUT GRAPH, V19, P46, DOI 10.1109/38.736468
   Benedens O, 2000, COMPUT GRAPH FORUM, V19, pC199, DOI 10.1111/1467-8659.00412
   Chao MW, 2009, IEEE T VIS COMPUT GR, V15, P274, DOI 10.1109/TVCG.2008.94
   Chou CM, 2006, COMPUT AIDED DESIGN, V38, P1154, DOI 10.1016/j.cad.2006.06.009
   Cotting D, 2004, SMI 04, P233
   Cox IJ, 1997, IEEE T IMAGE PROCESS, V6, P1673, DOI 10.1109/83.650120
   Garcia E, 2003, IEEE T CIRC SYST VID, V13, P853, DOI 10.1109/TCSVT.2003.815963
   Garland M., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P209, DOI 10.1145/258734.258849
   HOPPE H, 1996, SIGGRAPH 96, P99, DOI DOI 10.1145/237170.237216
   Kalivas A, 2003, 2003 INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL I, PROCEEDINGS, P637
   KANAI S, 1998, P 6 IFIP WG 5 2 GEO, P296
   Lee JJ, 2004, EURASIP J APPL SIG P, V2004, P2142, DOI 10.1155/S1110865704407148
   Lee SH, 2008, MULTIMEDIA SYST, V13, P323, DOI 10.1007/s00530-007-0095-8
   Lee TY, 2006, COMPUT ANIMAT VIRT W, V17, P433, DOI 10.1002/cav.146
   LEE TY, 1998, COMPUT GRAPH FORUM, P15
   Lin HYS, 2005, IEEE T MULTIMEDIA, V7, P997, DOI 10.1109/TMM.2005.858412
   MEYER M, 2002, P VIS MATH
   NI Y, 2007, CGIV 2007 P COMP GRA, P335
   Ohbuchi R, 2002, COMPUT GRAPH FORUM, V21, P373, DOI 10.1111/1467-8659.t01-1-00597
   Ohbuchi R., 2001, Graphics Interface, P9
   Podilchuk CI, 2001, IEEE SIGNAL PROC MAG, V18, P33, DOI 10.1109/79.939835
   Praun E, 1999, COMP GRAPH, P49, DOI 10.1145/311535.311540
   Song HS, 2008, IEICE T INF SYST, VE91D, P1512, DOI 10.1093/ietisy/e91-d.5.1512
   TAUBIN G, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P902, DOI 10.1109/ICCV.1995.466840
   TAUBIN G, 1995, SIGGRAPH 95 C P, P351, DOI [DOI 10.1145/218380.218473, 10.1145/218380.218473]
   UCCHEDDU F, 2004, P 2004 MULT SEC WORK, P143
   Wang YS, 2008, IEEE T VIS COMPUT GR, V14, P926, DOI 10.1109/TVCG.2008.38
   Yao CY, 2008, IEEE T VIS COMPUT GR, V14, P948, DOI 10.1109/TVCG.2008.39
   Yeo BL, 1999, IEEE COMPUT GRAPH, V19, P36, DOI 10.1109/38.736467
   Yin KK, 2001, COMPUT GRAPH-UK, V25, P409, DOI 10.1016/S0097-8493(01)00065-6
   Zafeiriou S, 2005, IEEE T VIS COMPUT GR, V11, P596, DOI 10.1109/TVCG.2005.71
NR 35
TC 13
Z9 13
U1 0
U2 7
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2010
VL 26
IS 6-8
BP 1101
EP 1111
DI 10.1007/s00371-010-0461-y
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 602JQ
UT WOS:000278135800069
DA 2024-07-18
ER

PT J
AU Tsuda, Y
   Yue, YH
   Dobashi, Y
   Nishita, T
AF Tsuda, Yusuke
   Yue, Yonghao
   Dobashi, Yoshinori
   Nishita, Tomoyuki
TI Visual simulation of mixed-motion avalanches with interactions between
   snow layers
SO VISUAL COMPUTER
LA English
DT Article
DE Simulation; Fluids; Avalanche; Generation of snow smoke; Adhesion of
   snow smoke; Snow entrainment
AB In the field of computer graphics, simulation of fluids, including avalanches, is an important research topic. In this paper, we propose a method to simulate a kind of avalanche, mixed-motion avalanche, which is usually large and travels down the slope fast, often resulting in impressive visual effects. The mixed-motion avalanche consists of snow smokes and liquefied snow which form an upper suspension layer and a lower dense-flow layer, respectively. The mixed-motion avalanche travels down the surface of the snow-covered mountain, which is called accumulated snow layer. We simulate a mixed-motion avalanche taking into account these three snow layers. We simulate the suspension layer using a grid-based approach, the dense-flow and accumulated snow layer using a particle-based approach. An important contribution of our method is an interaction model between these snow layers that enables us to obtain the characteristic motions of avalanches, such as the generation of the snow smoke from the head of the avalanche.
C1 [Tsuda, Yusuke; Yue, Yonghao; Nishita, Tomoyuki] Univ Tokyo, Tokyo, Japan.
   [Dobashi, Yoshinori] Hokkaido Univ, Sapporo, Hokkaido, Japan.
C3 University of Tokyo; Hokkaido University
RP Tsuda, Y (corresponding author), Univ Tokyo, Tokyo, Japan.
EM tsuday@nis-lab.is.s.u-tokyo.ac.jp; yonghao@nis-lab.is.s.u-tokyo.ac.jp;
   doba@ime.ist.hokudai.ac.jp; nis@nis-lab.is.s.u-tokyo.ac.jp
RI Yue, Yonghao/O-5203-2018
OI Yue, Yonghao/0000-0002-8252-0522
FU JSPS [20.7968]
FX We gratefully acknowledge helpful comments and thoughtful suggestions
   from the anonymous reviewers. We would also like to thank Dina Systems
   Co., Ltd. for providing the terrain data of Mt. Myoko for 3D simulation.
   The second author is supported by Grant-in-Aid for JSPS Fellows
   (20.7968).
CR COURANT R, 1967, IBM J RES DEV, V11, P215, DOI 10.1147/rd.112.0215
   Étienne J, 2006, CR MECANIQUE, V334, P545, DOI 10.1016/j.crme.2006.07.010
   FEDKIW R, 2001, P SIGGRAPH 2001, P390
   GAO Y, 2009, PACIFIC GRAPH, V28, P1845
   GINGOLD RA, MON NOT R ASTRON SOC, V181, P375
   HONG JM, 2005, P ACM SIGGRAPH 2005, P915
   HUTTER K, 1992, THEORY ACTA MECH, V100, P37
   KAPLER A, 2003, SIGGRAPH 2003 SKETCH
   Koshizuka S, 1996, NUCL SCI ENG, V123, P421, DOI 10.13182/NSE96-A24205
   MAENO N, 2000, AVALANCHE SNOWSTORM
   Magnaudet J, 2000, ANNU REV FLUID MECH, V32, P659, DOI 10.1146/annurev.fluid.32.1.659
   Muller M., 2003, Proceedings of the 2003 ACM SIGGRAPH/Eurographics symposium on Computer animation, P154
   PERLA A, 1980, DYN SNOW ICE MASSES, P397
   SALM B, 1985, ANN GLACIOL, V6, P26, DOI 10.3189/1985AoG6-1-26-34
   Sovilla B, 2007, COLD REG SCI TECHNOL, V47, P69, DOI 10.1016/j.coldregions.2006.08.012
   Stam J., 2003, P GAM DEV C
   TAKEUCHI M, 1980, J GLACIOL, V26, P481, DOI 10.3189/S0022143000010996
   TAKEUCHI M, 1983, I LOW TEMP SCI, P1
   TURK G, 1991, P SIGGRAPH 91, P289
NR 19
TC 4
Z9 4
U1 2
U2 12
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2010
VL 26
IS 6-8
BP 883
EP 891
DI 10.1007/s00371-010-0491-5
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 602JQ
UT WOS:000278135800048
DA 2024-07-18
ER

PT J
AU Gois, JP
   Polizelli-Junior, V
   Etiene, T
   Tejada, E
   Castelo, A
   Nonato, LG
   Ertl, T
AF Gois, J. P.
   Polizelli-Junior, V.
   Etiene, T.
   Tejada, E.
   Castelo, A.
   Nonato, L. G.
   Ertl, T.
TI Twofold adaptive partition of unity implicits
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Conference on SIBGRAPI 2007
CY SEP, 2007
CL Belo Horizonte, BRAZIL
DE Algebraic triangulation; Partition of unity implicits; Orthogonal
   polynomials
AB Partition of Unity Implicits (PUI) has been recently introduced for surface reconstruction from point clouds. In this work, we propose a PUI method that employs a set of well-observed solutions in order to produce geometrically pleasant results without requiring time consuming or mathematically overloaded computations. One feature of our technique is the use of multivariate orthogonal polynomials in the least-squares approximation, which allows the recursive refinement of the local fittings in terms of the degree of the polynomial. However, since the use of high-order approximations based only on the number of available points is not reliable, we introduce the concept of coverage domain. In addition, the method relies on the use of an algebraically defined triangulation to handle two important tasks in PUI: the spatial decomposition and an adaptive polygonization. As the spatial subdivision is based on tetrahedra, the generated mesh may present poorly-shaped triangles that are improved in this work by means a specific vertex displacement technique. Furthermore, we also address sharp features and raw data treatment. A further contribution is based on the PUI locality property that leads to an intuitive scheme for improving or repairing the surface by means of editing local functions.
C1 [Gois, J. P.; Polizelli-Junior, V.; Etiene, T.; Castelo, A.; Nonato, L. G.] Univ Sao Paulo, Sao Carlos, SP, Brazil.
   [Tejada, E.; Ertl, T.] Univ Stuttgart, Stuttgart, Germany.
C3 Universidade de Sao Paulo; University of Stuttgart
RP Gois, JP (corresponding author), Univ Sao Paulo, Sao Carlos, SP, Brazil.
EM jpgois@icmc.usp.br
RI Castelo, Antonio/E-6808-2011; Nonato, Luis Gustavo/D-5782-2011; Gois,
   Joao Paulo/D-1182-2010
OI Castelo, Antonio/0000-0001-8009-4577; Gois, Joao
   Paulo/0000-0002-9437-6943; Ertl, Thomas/0000-0003-4019-2505
CR AMENTA N, 2004, EUR S POINT BAS GRAP, V1, P139
   [Anonymous], 2005, P 3 EUR S GEOM PROC
   BARTELS RH, 1985, ACM T MATH SOFTWARE, V11, P201, DOI 10.1145/214408.214410
   Bloomenthal J., 1988, Computer-Aided Geometric Design, V5, P341, DOI 10.1016/0167-8396(88)90013-1
   BLOOMENTHAL J, 1994, GRAPHICS GEMS, V4, P324
   Bolitho M., 2007, Symposium on Geometry Processing (SGP), P69
   Castelo A, 2006, COMPUT GRAPH-UK, V30, P737, DOI 10.1016/j.cag.2006.07.025
   Chen YL, 2007, IEEE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS 2007, PROCEEDINGS, P147, DOI 10.1109/SMI.2007.3
   DEFIGUEIREDO LH, 1992, C GRAPH INT, P250
   Gois JP, 2007, SIBGRAPI, P95, DOI 10.1109/SIBGRAPI.2007.8
   HALL M, 1990, IEEE COMPUT GRAPH, V10, P33, DOI 10.1109/38.62694
   Kazhdan M., 2006, P 4 EUR S GEOM PROC, P61, DOI DOI 10.2312/SGP/SGP06/061-070
   KAZHDAN M, 2007, EUR S GEOM PROC, P125
   Kobbelt LP, 2001, COMP GRAPH, P57, DOI 10.1145/383259.383265
   MEDEROS B, 2006, TR0406 IMPA
   Ohtake Y, 2006, GRAPH MODELS, V68, P15, DOI 10.1016/j.gmod.2005.08.001
   Ohtake Y, 2003, ACM T GRAPHIC, V22, P463, DOI 10.1145/882262.882293
   Paiva A, 2006, SIBGRAPI, P205
   *POVRAY, 2007, POVRAY PERS VIS
   Schreiner J, 2006, IEEE T VIS COMPUT GR, V12, P1205, DOI 10.1109/TVCG.2006.149
   Tobor I, 2006, GRAPH MODELS, V68, P25, DOI 10.1016/j.gmod.2005.09.003
   XIA Q, 2006, C GEOM MOD IM, P28
NR 22
TC 8
Z9 9
U1 0
U2 1
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2008
VL 24
IS 12
BP 1013
EP 1023
DI 10.1007/s00371-008-0297-x
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 363VH
UT WOS:000260294500003
DA 2024-07-18
ER

PT J
AU Henze, N
   Reiners, R
   Righetti, X
   Rukzio, E
   Boll, S
AF Henze, Niels
   Reiners, Rene
   Righetti, Xavier
   Rukzio, Enrico
   Boll, Susanne
TI Services surround you - Physical-virtual linkage with contextual
   bookmarks
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 26th International Conference on Computer Graphics
CY JUN 09-11, 2008
CL Istanbul, TURKEY
DE mobile interaction; physical-virtual linkage; physical interaction;
   contextual bookmark; content analysis
AB Our daily life is pervaded by digital information and devices, not least the common mobile phone. However, a seamless connection between our physical world, such as a movie trailer on a screen in the main rail station and its digital counterparts, such as an online ticket service, remains difficult. In this paper, we present contextual bookmarks that enable users to capture information of interest with a mobile camera phone. Depending on the user's context, the snapshot is mapped to a digital service such as ordering tickets for a movie theater close by or a link to the upcoming movie's Web page.
C1 [Henze, Niels] OFFIS Inst Informat Technol, Oldenburg, Germany.
   [Righetti, Xavier] EPFL, VRLab, Zurich, Switzerland.
   [Rukzio, Enrico] Univ Lancaster, Dept Comp, Lancaster LA1 4YW, England.
   [Boll, Susanne] Carl von Ossietzky Univ Oldenburg, D-2900 Oldenburg, Germany.
C3 Swiss Federal Institutes of Technology Domain; Ecole Polytechnique
   Federale de Lausanne; Lancaster University; Carl von Ossietzky
   Universitat Oldenburg
RP Henze, N (corresponding author), OFFIS Inst Informat Technol, Oldenburg, Germany.
EM henze@offis.de; rene.reiners@fit.fraunhofer.de; xavier.righetti@epfl.ch;
   rukzio@comp.lancs.ac.uk; susanne.boll@informatik.uni-oldenburg.de
CR AHLERS D, 2007, GEOSPATIAL WEB GEO B
   Ailisto H., 2003, P WORKSH REAL WORLD, P38
   Ballagas R, 2006, IEEE PERVAS COMPUT, V5, P70, DOI 10.1109/MPRV.2006.18
   Beis JS, 1997, PROC CVPR IEEE, P1000, DOI 10.1109/CVPR.1997.609451
   FITZMAURICE GW, 1993, COMMUN ACM, V36, P39, DOI 10.1145/159544.159566
   Fockler P., 2005, Proceedings of the 4th international conference on Mobile and ubiquitous multimedia, P3
   Fowler G.A., 2005, Wall Street J
   FRITZ G, 2004, MOBILE LEARNING ANYT, P77
   Liu Q., 2006, P 14 ACM INT C MULTI, P791
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   *NOK CORP, 2007, NOW JUST ISNT SOON E
   Rekimoto J., 1995, P ACM S USER INTERFA, P29, DOI [10.1145/215585.215639, DOI 10.1145/215585.215639]
   RUKZIO R, 2007, P EUR C AMB INT BERL, P1
   *SEM ORG, SEM HYP YOUR WORLD
   SIMON R, 2007, P 9 INT C UB COMP BE, P3
   VALKKYNEN P, 2003, P PHYS INT WORKSH RE, P31
NR 16
TC 0
Z9 0
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2008
VL 24
IS 7-9
BP 847
EP 855
DI 10.1007/s00371-008-0266-4
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 322OI
UT WOS:000257384800040
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Magnenat-Thalmann, N
   Peternier, A
   Righetti, X
   Lim, MY
   Papagiannakis, G
   Fragopoulos, T
   Lambropoulou, K
   Barsocchi, P
   Thalmann, D
AF Magnenat-Thalmann, Nadia
   Peternier, Achille
   Righetti, Xavier
   Lim, Mingyu
   Papagiannakis, George
   Fragopoulos, Tasos
   Lambropoulou, Kyriaki
   Barsocchi, Paolo
   Thalmann, Daniel
TI A virtual 3D mobile guide in the INTERMEDIA project
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 26th International Conference on Computer Graphics
CY JUN 09-11, 2008
CL Istanbul, TURKEY
DE interactive media; dynamic networks; personalized and wearable
   interface; content management; mobile mixed reality; virtual humans;
   geolocalization
ID WIRELESS SENSOR NETWORKS
AB In this paper, we introduce a European research project, interactive media with personal networked devices (INTERMEDIA) in which we seek to progress beyond home and device-centric convergence toward truly user-centric convergence of multimedia. Our vision is to make the user the multimedia center: the user as the point at which multimedia services and the means for interacting with them converge. This paper proposes the main research goals in providing users with a personalized interface and content independent of physical networked devices, and space and time. As a case study, we describe an indoors, mobile mixed reality guide system: Chloe@University. With a see-through head-mounted display (HMD) connected to a small wearable computing device, Chloe@University provides users with an efficient way to guide someone in a building. A 3D virtual character in front of the user guides him/her to the required destination.
C1 [Magnenat-Thalmann, Nadia; Lim, Mingyu; Papagiannakis, George] Univ Geneva, MIRALab, CH-1211 Geneva 4, Switzerland.
   [Peternier, Achille; Righetti, Xavier; Thalmann, Daniel] Ecole Polytech Fed Lausanne, CH-1015 Lausanne, Switzerland.
   [Fragopoulos, Tasos] ISI, Platani Patras 26500, Greece.
   [Lambropoulou, Kyriaki] INTRACOM, Peania 19002, Attica, Greece.
   [Barsocchi, Paolo] CNR, I-00185 Rome, Italy.
C3 University of Geneva; Swiss Federal Institutes of Technology Domain;
   Ecole Polytechnique Federale de Lausanne; Consiglio Nazionale delle
   Ricerche (CNR)
RP Lim, MY (corresponding author), Univ Geneva, MIRALab, CH-1211 Geneva 4, Switzerland.
EM thalmann@miralab.unige.ch; achille.peternier@epfl.ch;
   xavier.righetti@epfl.ch; lim@miralab.unige.ch;
   papagiannakis@miralab.unige.ch; afragop@ece.upatras.gr;
   klam@intracom.gr; paolo.barsocchi@isti.cnr.it; daniel.thalmann@epfl.ch
RI Thalmann, Nadia/AAK-5195-2021; Thalmann, Daniel/AAL-1097-2020;
   Peternier, Achille/HSF-7742-2023; Barsocchi, Paolo/AAY-2364-2020; Lim,
   Mingyu/D-3819-2011; Thalmann, Daniel/A-4347-2008; papagiannakis,
   george/AAI-7973-2020
OI Thalmann, Nadia/0000-0002-1459-5960; Thalmann,
   Daniel/0000-0002-0451-7491; Peternier, Achille/0000-0001-9794-2392;
   Barsocchi, Paolo/0000-0002-6862-7593; Lim, Mingyu/0000-0002-3749-1902;
   papagiannakis, george/0000-0002-2977-9850
CR [Anonymous], COMP VIS WINT WORKSH
   Baronti P, 2007, COMPUT COMMUN, V30, P1655, DOI 10.1016/j.comcom.2006.12.020
   Beaudoin SM, 2006, THEMES WORLD HIST, P15
   Daemen J., AES PROPOSAL RIJNDAE
   HART PE, 1968, IEEE T SYST SCI CYB, VSSC4, P100, DOI 10.1109/TSSC.1968.300136
   Ladd AM, 2004, IEEE T ROBOTIC AUTOM, V20, P555, DOI 10.1109/TRA.2004.824948
   MAKRI A, 2005, P 11 INT C VIRT SYST, P681
   Papagiannakis G, 2008, COMPUT ANIMAT VIRT W, V19, P3, DOI 10.1002/cav.221
   Patwari N, 2003, IEEE T SIGNAL PROCES, V51, P2137, DOI 10.1109/TSP.2003.814469
   PETERNIER A, 2006, EUROGRAPHICS S VIRTU, V6, P35
   Peternier A, 2006, LECT NOTES COMPUT SC, V3942, P223, DOI 10.1007/11736639_31
   Savvides A., 2001, P 7 ANN INT C MOB CO, P166, DOI DOI 10.1145/381677.381693
NR 12
TC 5
Z9 5
U1 0
U2 1
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2008
VL 24
IS 7-9
BP 827
EP 836
DI 10.1007/s00371-008-0264-6
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 322OI
UT WOS:000257384800038
OA Green Submitted, Green Published
DA 2024-07-18
ER

PT J
AU Wang, YS
   Lee, TY
AF Wang, Yu-Shuen
   Lee, Tong-Yee
TI Example-driven animation synthesis
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 26th International Conference on Computer Graphics
CY JUN 09-11, 2008
CL Istanbul, TURKEY
DE animation synthesis; warping; intelligent scribbling
ID DEFORMATION; MESH
AB We introduce an easy and intuitive approach to create animations by assembling existing animations. Using our system, the user needs only to simply scribble regions of interest and select the example animations that he/she wants to apply. Our system will then synthesize a transformation for each triangle and solve an optimization problem to compute the new animation for this target mesh. Like playing a jigsaw puzzle game, even a novice can explore his/her creativity by using our system without learning complicated routines, but just using a few simple operations to achieve the goal.
C1 [Wang, Yu-Shuen; Lee, Tong-Yee] Natl Cheng Kung Univ, Comp Graph Grp, Visual Syst Lab, Dept Comp Sci & Informat Engn, Tainan 701, Taiwan.
C3 National Cheng Kung University
RP Lee, TY (corresponding author), Natl Cheng Kung Univ, Comp Graph Grp, Visual Syst Lab, Dept Comp Sci & Informat Engn, 1 Ta Hsueh Rd, Tainan 701, Taiwan.
EM tonylee@mail.ncku.edu.tw
OI Wang, Yu-Shuen/0000-0003-2550-2990
CR Allen B, 2002, ACM T GRAPHIC, V21, P612, DOI 10.1145/566570.566626
   Anguelov D, 2005, ACM T GRAPHIC, V24, P408, DOI 10.1145/1073204.1073207
   Der KG, 2006, ACM T GRAPHIC, V25, P1174, DOI 10.1145/1141911.1142011
   Guenin BM, 1998, P IEEE SEMICOND THER, P55, DOI 10.1109/STHERM.1998.660387
   Guo Z, 2005, COMPUT GRAPH FORUM, V24, P373, DOI 10.1111/j.1467-8659.2005.00862.x
   Herda L, 2001, HUM MOVEMENT SCI, V20, P313, DOI 10.1016/S0167-9457(01)00050-1
   Huang J, 2006, ACM T GRAPHIC, V25, P1126, DOI 10.1145/1141911.1142003
   Hyun DE, 2005, VISUAL COMPUT, V21, P542, DOI 10.1007/s00371-005-0343-x
   Igarashi T, 2005, ACM T GRAPHIC, V24, P1134, DOI 10.1145/1073204.1073323
   Lee TY, 2008, IEEE T CIRC SYST VID, V18, P478, DOI 10.1109/TCSVT.2008.918456
   Lee TY, 2006, VISUAL COMPUT, V22, P729, DOI 10.1007/s00371-006-0059-6
   Lee TY, 2003, IEEE T VIS COMPUT GR, V9, P85, DOI 10.1109/TVCG.2003.1175099
   Lewis JP, 2000, COMP GRAPH, P165, DOI 10.1145/344779.344862
   Lin CH, 2005, IEEE T VIS COMPUT GR, V11, P2
   Mohr A, 2003, ACM T GRAPHIC, V22, P562, DOI 10.1145/882262.882308
   Nealen A, 2005, ACM T GRAPHIC, V24, P1142, DOI 10.1145/1073204.1073324
   Noh JY, 2001, COMP GRAPH, P277, DOI 10.1145/383259.383290
   Park SI, 2006, ACM T GRAPHIC, V25, P881, DOI 10.1145/1141911.1141970
   Shoemake K., 1992, Proceedings. Graphics Interface '92, P258
   Sorkine O, 2004, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, P191, DOI 10.1109/SMI.2004.1314506
   Sorkine O., 2004, P 2004 EUR ACM SIGGR, P179
   Sumner RW, 2005, ACM T GRAPHIC, V24, P488, DOI 10.1145/1073204.1073218
   Sumner RW, 2004, ACM T GRAPHIC, V23, P399, DOI 10.1145/1015706.1015736
   von Funck W, 2006, ACM T GRAPHIC, V25, P1118, DOI 10.1145/1141911.1142002
   Weng YL, 2006, VISUAL COMPUT, V22, P653, DOI 10.1007/s00371-006-0054-y
   Yu YZ, 2004, ACM T GRAPHIC, V23, P644, DOI 10.1145/1015706.1015774
   Zhou K, 2005, ACM T GRAPHIC, V24, P496, DOI 10.1145/1073204.1073219
NR 27
TC 6
Z9 7
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2008
VL 24
IS 7-9
BP 765
EP 773
DI 10.1007/s00371-008-0258-4
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 322OI
UT WOS:000257384800032
DA 2024-07-18
ER

PT J
AU Tierny, J
   Vandeborre, JP
   Daoudi, M
AF Tierny, Julien
   Vandeborre, Jean-Philippe
   Daoudi, Mohamed
TI Enhancing 3D mesh topological skeletons with discrete contour
   constrictions
SO VISUAL COMPUTER
LA English
DT Article
DE shape abstraction; topological skeletons; feature points; constrictions;
   topology driven segmentation
ID SHAPE
AB This paper describes a unified and fully automatic algorithm for Reeb graph construction and simplification as well as constriction approximation on triangulated surfaces.
   The key idea of the algorithm is that discrete contours - curves carried by the edges of the mesh and approximating the continuous contours of a mapping function - encode both topological and geometrical shape characteristics. Therefore, a new concise shape representation, enhanced topological skeletons, is proposed, encoding the contours' topological and geometrical evolution.
   First, mesh feature points are computed. Then they are used as geodesic origins for the computation of an invariant mapping function that reveals the shape most significant features. Next, for each vertex in the mesh, its discrete contour is computed. As the set of discrete contours recovers the whole surface, each of them can be analyzed, both to detect topological changes and constrictions. Constriction approximations enable Reeb graphs refinement into more visually meaningful skeletons, which we refer to as enhanced topological skeletons.
   Extensive experiments showed that, without any preprocessing stage, proposed algorithms are fast in practice, affine-invariant and robust to a variety of surface degradations (surface noise, mesh sampling and model pose variations). These properties make enhanced topological skeletons interesting shape abstractions for many computer graphics applications.
C1 [Tierny, Julien] Univ Lille, CNRS, USTL, UMR 8022, Lille, France.
   [Vandeborre, Jean-Philippe; Daoudi, Mohamed] Univ Lille, CNRS, LIFL USTL UMR 8022, GET TELECOM Lille 1, Lille, France.
C3 Centre National de la Recherche Scientifique (CNRS); Universite de
   Lille; Universite de Lille; Centre National de la Recherche Scientifique
   (CNRS)
RP Tierny, J (corresponding author), Univ Lille, CNRS, USTL, UMR 8022, Lille, France.
EM julien.tierny@lifl.fr; jean-philippe.vandeborre@lifl.fr;
   mohamed.daoudi@lifl.fr
RI Daoudi, Mohammed/H-5935-2013
OI Daoudi, Mohammed/0000-0003-4219-7860
CR Attene M, 2006, IEEE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS 2006, PROCEEDINGS, P14
   Attene M, 2003, VISUAL COMPUT, V19, P127, DOI 10.1007/s00371-002-0182-y
   Berretti S, 2006, INT C PATT RECOG, P19
   Biasotti S, 2003, SMI 2003: SHAPE MODELING INTERNATIONAL 2003, PROCEEDINGS, P245
   BIEDERMAN I, 1987, PSYCHOL REV, V94, P115, DOI 10.1037/0033-295X.94.2.115
   BLUM H, 1978, PATTERN RECOGN, V10, P167, DOI 10.1016/0031-3203(78)90025-0
   Bremer PT, 2004, IEEE T VIS COMPUT GR, V10, P385, DOI 10.1109/TVCG.2004.3
   Capell S, 2002, ACM T GRAPHIC, V21, P586, DOI 10.1145/566570.566622
   Carr H, 2004, IEEE VISUALIZATION 2004, PROCEEEDINGS, P497, DOI 10.1109/VISUAL.2004.96
   EDELSBRUNNER H, 1990, ACM T GRAPHIC, V9, P66, DOI 10.1145/77635.77639
   Funkhouser T, 2004, ACM T GRAPHIC, V23, P652, DOI 10.1145/1015706.1015775
   Hétroy F, 2003, 11TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P394, DOI 10.1109/PCCGA.2003.1238282
   HETROY F, 2005, EUROGRAPHICS, P1
   Hilaga M, 2001, COMP GRAPH, P203, DOI 10.1145/383259.383282
   Katz S, 2003, ACM T GRAPHIC, V22, P954, DOI 10.1145/882262.882369
   KOENDERINK JJ, 1992, IMAGE VISION COMPUT, V10, P557, DOI 10.1016/0262-8856(92)90076-F
   LAZARUS F, 1999, 3546 INRIA
   Liefman S.K. G., 2005, VISUAL COMPUT, V21, P865
   McLaughlin CK, 2003, P 19 ANN SYM COMP GE, P344, DOI [10.1145/777792.777844, DOI 10.1145/777792.777844]
   MEYER M, 2002, VISUALIZATION MATH, P33
   Morse M, 1925, T AM MATH SOC, V27, P345, DOI 10.2307/1989110
   Mortara M, 2002, SHAPE MODELING AND APPLICATIONS, PROCEEDINGS, P245, DOI 10.1109/SMI.2002.1003552
   Ni XL, 2004, ACM T GRAPHIC, V23, P613, DOI 10.1145/1015706.1015769
   Nieda T, 2006, VISUAL COMPUT, V22, P346, DOI 10.1007/s00371-006-0011-9
   Ogniewicz R., 1992, Proceedings. 1992 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.92CH3168-2), P63, DOI 10.1109/CVPR.1992.223226
   Podolak J, 2006, ACM T GRAPHIC, V25, P549, DOI 10.1145/1141911.1141923
   REEB G, 1946, CR HEBD ACAD SCI, V222, P847
   Shilane P, 2004, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, P167, DOI 10.1109/smi.2004.1314504
   SHINAGAWA Y, 1991, IEEE COMPUT GRAPH, V11, P66, DOI 10.1109/38.90568
   TAKAHASHI S, 1995, COMPUT GRAPH FORUM, V14, pC181, DOI 10.1111/j.1467-8659.1995.cgf143_0181.x
   Tierny J., 2006, PACIFIC GRAPHICS, V2006, P85
   Wu FC, 2006, VISUAL COMPUT, V22, P117, DOI 10.1007/s00371-005-0357-4
   Yamauchi H, 2005, VISUAL COMPUT, V21, P659, DOI 10.1007/s00371-005-0319-x
   Zhang E, 2005, ACM T GRAPHIC, V24, P1, DOI 10.1145/1037957.1037958
NR 34
TC 25
Z9 27
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2008
VL 24
IS 3
BP 155
EP 172
DI 10.1007/s00371-007-0181-0
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 255ZA
UT WOS:000252695900001
DA 2024-07-18
ER

PT J
AU Gong, Y
   Chen, W
   Zhang, L
   Zeng, Y
   Peng, QS
AF Gong, Yi
   Chen, Wei
   Zhang, Long
   Zeng, Yun
   Peng, Qunsheng
TI GPU-based rendering for deformable translucent objects
SO VISUAL COMPUTER
LA English
DT Article
DE sub-surface scattering; BSSRDF; translucency; real-time rendering
AB In this paper we introduce an approximate image-space approach for real-time rendering of deformable translucent models by flattening the geometry and lighting information of objects into textures to calculate multi-scattering in texture spaces. We decompose the process into two stages, called the gathering and scattering corresponding to the computations for incident and exident irradiance respectively. We derive a simplified illumination model for the gathering of the incident irradiance, which is amenable for deformable models using two auxiliary textures. In the scattering stage, we adopt two modes for efficient accomplishment of the view-dependent scattering. Our approach is implemented by fully exploiting the capabilities of graphics processing units (GPUs). It achieves visually plausible results and real-time frame rates for deformable models on commodity desktop PCs.
C1 [Gong, Yi; Chen, Wei; Zhang, Long; Zeng, Yun; Peng, Qunsheng] Zhejiang Univ, State Key Lab CAD & CG, Hangzhou 310027, Peoples R China.
C3 Zhejiang University
RP Chen, W (corresponding author), Zhejiang Univ, State Key Lab CAD & CG, Hangzhou 310027, Peoples R China.
EM ygong@cad.zju.edu.cn; chenwei@cad.zju.edu.cn; lzhang@cad.zju.edu.cn;
   yzeng@cad.zju.edu.cn; peng@cad.zju.edu.cn
RI Chen, Wei/AAR-9817-2020
CR [Anonymous], 1978, P 5 ANN C COMPUTER G, P270
   Blasi P., 1993, Computer Graphics Forum, V12, pC201, DOI 10.1111/1467-8659.1230201
   DACHSBACHER C, 2003, P 14 EUR WORKSH REND, P197
   Donner C, 2005, ACM T GRAPHIC, V24, P1032, DOI 10.1145/1073204.1073308
   DONNER C, 2006, P EUR S REND 2006
   Goesele M, 2004, ACM T GRAPHIC, V23, P835, DOI 10.1145/1015706.1015807
   Hanrahan P., 1993, P ACM SIGGRAPH, P165
   Hao XJ, 2004, ACM T GRAPHIC, V23, P120, DOI 10.1145/990002.990004
   Jensen HW, 2001, COMP GRAPH, P511, DOI 10.1145/383259.383319
   Jensen HW, 2002, ACM T GRAPHIC, V21, P576, DOI 10.1145/566570.566619
   JENSEN HW, 1998, P SIGGRAPH 98, P311, DOI DOI 10.1145/280814.280925
   Lafortune E. P., 1996, Rendering Techniques '96. Proceedings of the Eurographics Workshop. Eurographics, P91
   Lensch HPA, 2002, 10TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P214, DOI 10.1109/PCCGA.2002.1167862
   Li Hongsong., 2005, Proceedings of EGSR 2005, P283, DOI DOI 10.2312/EGWR/EGSR05/283-290
   MAILLOT J, 1993, P SIGGRAPH 93, P27, DOI DOI 10.1145/166117.166120
   Mertens T, 2003, 11TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P51, DOI 10.1109/PCCGA.2003.1238246
   MERTENS T, 2003, P 14 EUR WORKSH REND, P130
   *NVIDIA, 2007, GPU PROGR GUID
   Peers P, 2006, ACM T GRAPHIC, V25, P746, DOI 10.1145/1141911.1141950
   RUSHMEIER HE, 1990, ACM T GRAPHIC, V9, P1, DOI 10.1145/77635.77636
   SILLION FX, 1995, IEEE T VIS COMPUT GR, V1, P240, DOI 10.1109/2945.466719
   Sloan PP, 2005, ACM T GRAPHIC, V24, P1216, DOI 10.1145/1073204.1073335
   SLOAN PP, 2002, P ACM SIGGRAPH 2002, P257
   Stam J, 1995, SPRING COMP SCI, P41
   SURAZHSKY V, 2003, SGP 03 P 2003 EUR AC
   TONG X, 2005, P ACM SIGGRAPH 2005, P1054
   Wang R, 2005, ACM T GRAPHIC, V24, P1202, DOI 10.1145/1073204.1073333
NR 27
TC 3
Z9 3
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2008
VL 24
IS 2
BP 95
EP 103
DI 10.1007/s00371-007-0188-6
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 252ZH
UT WOS:000252486200003
DA 2024-07-18
ER

PT J
AU Miao, YW
   Feng, JQ
   Xiao, CX
   Peng, QS
AF Miao, Yongwei
   Feng, Jieqing
   Xiao, Chunxia
   Peng, Qunsheng
TI High frequency geometric detail manipulation and editing for
   point-sampled surfaces
SO VISUAL COMPUTER
LA English
DT Article
DE point-sampled surfaces; shape editing; geometric detail; simplification
   sample point; deformation field; mean shift
ID MEAN SHIFT; ANIMATION
AB In this paper, based on the new definition of high frequency geometric detail for point-sampled surfaces, a new approach for detail manipulation and a detail-preserving editing framework are proposed. Geometric detail scaling and enhancement can always produce fantastic effects by directly manipulating the geometric details of the underlying geometry. Detail-preserving editing is capable of preserving geometric details during the shape deformation of point-sampled model. For efficient editing, the point set of the model is first clustered by a mean shift scheme, according to its anisotropic geometric features and each cluster is abstracted as a simplification sample point (SSP). Our editing operation is implemented by manipulating the SSP first and then diffusing the deformation to all sample points on the underlying geometry. As a post-processing step, a new up-sampling and relaxation procedure is proposed to refine the deformed model. The effectiveness of the proposed method is demonstrated by several examples.
C1 [Miao, Yongwei; Feng, Jieqing; Peng, Qunsheng] Zhejiang Univ, State Key Lab CAD & CG, Hangzhou 310027, Peoples R China.
   [Miao, Yongwei] Zhejiang Univ Technol, Coll Sci, Hangzhou, Peoples R China.
   [Xiao, Chunxia] Wuhan Univ, Sch Sci, Wuhan 430072, Peoples R China.
C3 Zhejiang University; Zhejiang University of Technology; Wuhan University
RP Miao, YW (corresponding author), Zhejiang Univ, State Key Lab CAD & CG, Hangzhou 310027, Peoples R China.
EM ywmiao@zjut.edu.cn
RI Miao, Yongwei/ABH-1238-2021
OI Miao, Yongwei/0000-0002-5479-9060
CR Alexa M, 2003, VISUAL COMPUT, V19, P105, DOI 10.1007/s00371-002-0180-0
   Au OKC, 2006, IEEE T VIS COMPUT GR, V12, P386, DOI 10.1109/TVCG.2006.47
   Bao YF, 2005, COMPUT ANIMAT VIRT W, V16, P509, DOI 10.1002/cav.100
   CHENG YZ, 1995, IEEE T PATTERN ANAL, V17, P790, DOI 10.1109/34.400568
   Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236
   Comaniciu D., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1197, DOI 10.1109/ICCV.1999.790416
   Guo XH, 2006, IEEE T VIS COMPUT GR, V12, P375, DOI 10.1109/TVCG.2006.52
   Guo XH, 2005, COMPUT ANIMAT VIRT W, V16, P189, DOI 10.1002/cav.98
   Guo XH, 2003, 11TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P123
   Guskov I, 1999, COMP GRAPH, P325, DOI 10.1145/311535.311577
   Huang J, 2006, ACM T GRAPHIC, V25, P1126, DOI 10.1145/1141911.1142003
   KOBBELT L, 1998, P SIGGRAPH 98, P105, DOI DOI 10.1145/280814.280831
   Lipman Y, 2005, ACM T GRAPHIC, V24, P479, DOI 10.1145/1073204.1073217
   Lipman Y, 2004, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, P181, DOI 10.1109/SMI.2004.1314505
   Miao YW, 2006, LECT NOTES COMPUT SC, V4035, P673
   Müller M, 2005, ACM T GRAPHIC, V24, P471, DOI 10.1145/1073204.1073216
   MULLER M, 2004, EUR ACM SIGGRAPH S C, P141
   Nealen A, 2005, ACM T GRAPHIC, V24, P1142, DOI 10.1145/1073204.1073324
   Pauly M, 2005, ACM T GRAPHIC, V24, P957, DOI 10.1145/1073204.1073296
   Pauly M, 2003, ACM T GRAPHIC, V22, P641, DOI 10.1145/882262.882319
   Pauly M, 2002, VIS 2002: IEEE VISUALIZATION 2002, PROCEEDINGS, P163, DOI 10.1109/VISUAL.2002.1183771
   Pauly M, 2001, COMP GRAPH, P379, DOI 10.1145/383259.383301
   Pauly M, 2006, ACM T GRAPHIC, V25, P177, DOI 10.1145/1138450.1138451
   SORKINE O, 2005, P EUR, P53
   Sorkine O., 2004, P 2004 EUR ACM SIGGR, P179
   Taubin G., 1995, P 22 ANN C COMP GRAP, P351, DOI DOI 10.1145/218380.218473
   Wicke M, 2005, COMPUT GRAPH FORUM, V24, P667, DOI 10.1111/j.1467-8659.2005.00891.x
   Xiao CX, 2007, VISUAL COMPUT, V23, P433, DOI 10.1007/s00371-007-0115-x
   Xiao CX, 2006, VISUAL COMPUT, V22, P210, DOI 10.1007/s00371-006-0377-8
   Yu YZ, 2004, ACM T GRAPHIC, V23, P644, DOI 10.1145/1015706.1015774
   Zayer R, 2005, COMPUT GRAPH FORUM, V24, P601, DOI 10.1111/j.1467-8659.2005.00885.x
   Zhou K, 2005, ACM T GRAPHIC, V24, P496, DOI 10.1145/1073204.1073219
   ZORIN D, 1997, P SIGGRAPH 97, P259
   Zwicker M, 2002, ACM T GRAPHIC, V21, P322, DOI 10.1145/566570.566584
NR 34
TC 2
Z9 6
U1 0
U2 9
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2008
VL 24
IS 2
BP 125
EP 138
DI 10.1007/s00371-007-0178-8
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 252ZH
UT WOS:000252486200005
DA 2024-07-18
ER

PT J
AU Yang, HP
   Jüttler, B
AF Yang, Huaiping
   Juettler, Bert
TI 3D shape metamorphosis based on T-spline level sets
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 10th International Conference on Computer-Aided Design and Computer
   Graphics
CY OCT 15-18, 2007
CL Peking Univ, Beijing, PEOPLES R CHINA
SP China Comp Federat, IEEE Beijing Sect, Peking Univ, Inst Comp Sci & Technol, Peking Univ, Sch EECS, Natl Nat Sci Fdn China, Microsoft Res Asia, Peking Univ, Natl Lab Machine Percept, Key Lab High Confidence Software Technologies, Minist Educ
HO Peking Univ
DE computer animation; morphing; T-spline; level sets
AB We propose a new method for 3D shape metamorphosis, where the in-between objects are constructed by using T-spline scalar functions. The use of T-spline level sets offers several advantages: First, it is convenient to handle complex topology changes without the need of model parameterization. Second, the constructed objects are smooth (C-2 in our case). Third, high quality meshes can be easily obtained by using the marching triangulation method. Fourth, the distribution of the degrees of freedom can be adapted to the geometry of the object.
   Given one source object and one target object, we firstly find a global coordinate transformation to approximately align the two objects. The T-spline control grid is adaptively generated according to the geometry of the aligned objects, and the initial T-spline level set is found by approximating the signed distance function of the source object. Then we use an evolution process, which is governed by a combination of the signed distance function of the target object and a curvature-dependent speed function, to deform the T-spline level set until it converges to the target shape. Additional intermediate objects are inserted at the beginning/end of the sequence of generated T-spline level sets, by gradually projecting the source/target object to the initial/final T-spline level set. A fully automatic algorithm is developed for the above procedures. Experimental results are presented to demonstrate the effectiveness of our method.
C1 Johannes Kepler Univ Linz, Inst Appl Geometry, A-4040 Linz, Austria.
C3 Johannes Kepler University Linz
RP Yang, HP (corresponding author), Johannes Kepler Univ Linz, Inst Appl Geometry, A-4040 Linz, Austria.
EM yang.huaiping@jku.at; bert.juettler@jku.at
OI Juttler, Bert/0000-0002-5518-7795
CR Alexa M, 2002, COMPUT GRAPH FORUM, V21, P173, DOI 10.1111/1467-8659.00575
   Bao HJ, 1998, COMPUT GRAPH FORUM, V17, pC23, DOI 10.1111/1467-8659.00250
   Bao YF, 2005, COMPUT ANIMAT VIRT W, V16, P509, DOI 10.1002/cav.100
   BEIER T, 1992, COMP GRAPH, V26, P35, DOI 10.1145/142920.134003
   BOTSCH M, 2005, LECT NOTES COMPUTER, V3604, P62
   Breen DE, 2001, IEEE T VIS COMPUT GR, V7, P173, DOI 10.1109/2945.928169
   Chen M, 1996, COMPUT GRAPH-UK, V20, P567, DOI 10.1016/0097-8493(96)00027-1
   Cohen-Or D, 1998, ACM T GRAPHIC, V17, P116, DOI 10.1145/274363.274366
   Galin E, 1996, COMPUT GRAPH FORUM, V15, pC143, DOI 10.1111/1467-8659.1530143
   Hartmann K, 1998, Eur J Med Res, V3, P95
   HE T, 1994, P VIS 94, P85
   HUGHES JF, 1992, P SIGGRAPH 92 NEW YO
   Jin XG, 2005, COMPUT ANIMAT VIRT W, V16, P391, DOI 10.1002/cav.84
   Kanai T, 2000, IEEE COMPUT GRAPH, V20, P62, DOI 10.1109/38.824544
   Kaul A., 1991, EUROGRAPHICS '91. Proceedings of the European Computer Graphics Conference and Exhibition, P493
   Kraevoy V, 2004, ACM T GRAPHIC, V23, P861, DOI 10.1145/1015706.1015811
   Lazarus F, 1998, VISUAL COMPUT, V14, P373, DOI 10.1007/s003710050149
   Lee TY, 2006, COMPUT ANIMAT VIRT W, V17, P433, DOI 10.1002/cav.146
   Lerios A., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P449, DOI 10.1145/218380.218502
   Nieda T, 2006, VISUAL COMPUT, V22, P346, DOI 10.1007/s00371-006-0011-9
   PASKO A, 1995, VISUAL COMPUT, V11, P429, DOI 10.1007/BF02464333
   PAYNE BA, 1992, IEEE COMPUT GRAPH, V12, P65, DOI 10.1109/38.135885
   ROSSIGNAC J, 1994, P EUROGRAPHICS 94, P179
   Sederberg TN, 2003, ACM T GRAPHIC, V22, P477, DOI 10.1145/882262.882295
   Shoemake Ken, 1985, P 12 ANN C COMP GRAP, P245, DOI [DOI 10.1145/325165.325242, DOI 10.1145/325334.325242]
   Turk G, 1999, COMP GRAPH, P335, DOI 10.1145/311535.311580
   Yan HB, 2007, J COMPUT SCI TECH-CH, V22, P147, DOI 10.1007/s11390-007-9020-z
   YANG H, EVOLUTION TSPLINE LE
   Yang HP, 2006, IEEE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS 2006, PROCEEDINGS, P247
NR 29
TC 8
Z9 19
U1 0
U2 8
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2007
VL 23
IS 12
BP 1015
EP 1025
DI 10.1007/s00371-007-0168-x
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 232EP
UT WOS:000251001400007
DA 2024-07-18
ER

PT J
AU Bailey, RJ
   Grimm, CM
AF Bailey, Reynold J.
   Grimm, Cindy M.
TI Perceptually meaningful image editing - Manipulating perceived depth and
   creating the illusion of motion in 2D images
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 25th Computer Graphics International Conference (CGI)
CY MAY 30-JUN 02, 2007
CL Petropolis, BRAZIL
DE depth; motion luminance; warm-cool; image-based
AB We introduce a novel concept called perceptually meaningful image editing and present techniques for manipulating the apparent depth of objects and creating the illusion of motion in 2D images. Our techniques combine principles of human visual perception with approaches developed by traditional artists. For our depth manipulation technique, the user loads an image, selects an object and specifies whether the object should appear closer or further away. The system automatically determines luminance or color temperature target values for the object and/or background that achieve the desired depth change. Our approach for creating the illusion of motion exploits the differences between our peripheral vision and our foveal vision by introducing spatial imprecision to the image.
C1 Washington Univ, Dept Comp Sci & Engn, St Louis, MO 63130 USA.
C3 Washington University (WUSTL)
RP Bailey, RJ (corresponding author), Washington Univ, Dept Comp Sci & Engn, St Louis, MO 63130 USA.
EM rjbl@cse.wustl.edu; cmg@cse.wustl.edu
OI Grimm, Cindy/0000-0002-1711-7112
CR Back BB, 2005, J PHYS CONF SER, V5, P1, DOI 10.1088/1742-6596/5/1/001
   Bailey R., 2006, APGV '06: Proceedings of the 3rd symposium on Applied perception in graphics and visualization, P161
   Bertalmio M, 2000, COMP GRAPH, P417, DOI 10.1145/344779.344972
   Chuang YY, 2005, ACM T GRAPHIC, V24, P853, DOI 10.1145/1073204.1073273
   Comaniciu D, 1997, PROC CVPR IEEE, P750, DOI 10.1109/CVPR.1997.609410
   Efros AA, 2001, COMP GRAPH, P341, DOI 10.1145/383259.383296
   Foley J., 1996, SYSTEMS PROGRAMMING
   FREEMAN WT, 1991, COMP GRAPH, V25, P27, DOI 10.1145/127719.122721
   Freixenet J, 2002, LECT NOTES COMPUT SC, V2352, P408, DOI 10.1007/3-540-47977-5_27
   Gooch Amy A., 2004, Proceedings of the 1st Symposium on Applied Perception in Graphics and Visualization, page, P168
   HECKBERT PS, 1994, GRAPH INTER, P43
   Kawagishi Y, 2003, COMPUTER GRAPHICS INTERNATIONAL, PROCEEDINGS, P276, DOI 10.1109/CGI.2003.1214482
   KUFFLER SW, 1953, J NEUROPHYSIOL, V16, P37, DOI 10.1152/jn.1953.16.1.37
   Li Y, 2004, ACM T GRAPHIC, V23, P303, DOI 10.1145/1015706.1015719
   Livingstone M., 2002, Vision and art: The biology of seeing
   Masuch M., 1999, ACM SIGGRAPH 99 Conference abstracts and applications, SIGGRAPH '99, P277
   Moriya T., 2006, SIGGRAPH'06: Research posters, page, P114
   OGDEN TE, 1966, VISION RES, V6, P485, DOI 10.1016/0042-6989(66)90001-0
   Pfautz J. D., 2000, THESIS U CAMBRIDGE C
   Rogers Douglas., 2000, Implementing Fog in DirectSD
   SHOUP RG, 1979, SIGGRAPH 79 P ANN C, P8
   SUNDET JM, 1978, SCAND J PSYCHOL, V19, P133, DOI 10.1111/j.1467-9450.1978.tb00313.x
   ZIMMERMAN GL, 1992, P SOC PHOTO-OPT INS, V1825, P401, DOI 10.1117/12.131549
NR 23
TC 0
Z9 0
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2007
VL 23
IS 9-11
SI SI
BP 813
EP 821
DI 10.1007/s00371-007-0131-x
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 206UE
UT WOS:000249207600021
DA 2024-07-18
ER

PT J
AU Van Laerhoven, T
   Van Reeth, F
AF Van Laerhoven, Tom
   Van Reeth, Frank
TI Brush up your painting skills - Realistic brush design for interactive
   painting applications
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 25th Computer Graphics International Conference (CGI)
CY MAY 30-JUN 02, 2007
CL Petropolis, BRAZIL
DE paint systems; physically-based modeling; constrained optimization
AB Most present-day interactive paint applications lack the means of adequately capturing a user's gestures and translating them into realistic and predictable strokes, despite the importance of such a mechanism. We present a novel brush design that adopts constrained energy optimization to deform the brush tuft according to the user's input movement. It incorporates bidirectional paint transfer and an anisotropic friction model. The main advantage of our method is its ability to handle a wide range of brush tuft shapes that are animated using a freeform deformation lattice, which is associated with the tuft's geometry. This way, almost no conditions or limitations are placed upon the appearance of the brush. Examples range from round brushes modeled as polygon meshes, to flat brushes with individual bristles. Less common deformable tools that are used to apply or remove paint on the canvas, like sponges, can be created as well. The model is integrated in our interactive painting system for creating images with watery paint.
C1 Hasselt Univ, Univ Limburg, Expertise Ctr Digital Media Transnat, BE-3590 Diepenbeek, Belgium.
C3 Hasselt University
RP Van Laerhoven, T (corresponding author), Hasselt Univ, Univ Limburg, Expertise Ctr Digital Media Transnat, Wetenschaps Pk 2, BE-3590 Diepenbeek, Belgium.
EM tom.vaniaerhoven@uhasselt.be; frank.vanreeth@uhasselt.be
OI VAN REETH, Frank/0000-0002-3705-7807
CR Baxter B, 2001, COMP GRAPH, P461, DOI 10.1145/383259.383313
   BAXTER W, 2004, THESIS U N CAROLINA
   Baxter WV, 2004, 12TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P319
   BAXTER WV, 2004, NOTES BRUSH SIMULATI
   *BLEND FDN, 2006, BLEND V2 41 SOFTW PA
   Chu NSH, 2004, IEEE COMPUT GRAPH, V24, P76, DOI 10.1109/MCG.2004.37
   Chu NSH, 2002, 10TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P413, DOI 10.1109/PCCGA.2002.1167885
   COLOMOSSE JP, 2004, THESIS U BATH
   *COREL, 2006, COREL PAIN X SOFTW P
   CRAIG J, 1989, ROBOTICS
   Gooch A. A., 2001, NONPHOTOREALISTIC RE
   GREEN TS, 1985, P 11 S FUS ENG AUST, P103
   LEE J, 1997, P 5 INT C COMP GRAPH, P1571
   NOCEDAL J, 1999, NUMERICAL OPTIMIZAIO
   *NVIDIA, 2006, CG TOOLK US MAN V1 4
   Parent Rick., 2002, COMPUTER ANIMATION
   SAITO S, 1999, COMPUTER GRAPHICS, P226
   SAITO S, 2000, JYOUHOUSHORI GAKKAI, V41, P608
   Smith AR, 2001, IEEE ANN HIST COMPUT, V23, P4, DOI 10.1109/85.929908
   SMITH S, 1998, COMPLETE WATERCOLOUR
   SPELLUCCI P, 2004, DONLP2 USERS GUIDE
   STRASSMANN S, 1986, P 13 ANN C COMP GRAP, P225
   Van Laerhoven T, 2005, COMPUT ANIMAT VIRT W, V16, P429, DOI 10.1002/cav.95
   WENZDENISE S, 2001, INVALUABLE PAINTBRUS
   Xu SH, 2002, COMPUT GRAPH FORUM, V21, P299, DOI 10.1111/1467-8659.00589
NR 25
TC 6
Z9 7
U1 0
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2007
VL 23
IS 9-11
SI SI
BP 763
EP 771
DI 10.1007/s00371-007-0158-z
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 206UE
UT WOS:000249207600016
DA 2024-07-18
ER

PT J
AU Sela, G
   Elber, G
AF Sela, Guy
   Elber, Gershon
TI Generation of view dependent models using free form deformation
SO VISUAL COMPUTER
LA English
DT Article
DE free form deformation; 3D modeling; non-photorealistic rendering;
   computer graphics art
ID INTERPOLATION; SPLINE; ENERGY
AB We present a scheme which, given two 3D geometric models, creates a third, synergetic model with resemblance to one input model from one viewing direction and the other input model from another, orthogonal, viewing direction. Our scheme automatically calculates the necessary constraints needed to deform the first model's silhouette into the second model's in 2D, and creates a 3D deformation function based on these constraints while minimizing the object's distortion in all areas but the silhouette. The motivation of this work stems from the artwork of conceptual artists such as Shigeo Fukuda [9] and Markus Raetz [19].
C1 Technion Israel Inst Technol, IL-32000 Haifa, Israel.
C3 Technion Israel Institute of Technology
RP Sela, G (corresponding author), Technion Israel Inst Technol, IL-32000 Haifa, Israel.
EM guysela@cs.technion.ac.il; gershon@cs.technion.ac.i
CR BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791
   BULTHOFF HH, 1992, P NATL ACAD SCI USA, V89, P60, DOI 10.1073/pnas.89.1.60
   Castleman K. R., 1996, Digital Image Processing
   Cohen EJ, 1997, CONFIGURATIONS, V5, P369, DOI 10.1353/con.1997.0013
   De Boor C, 1978, A Pratical Guide to Splines, V27
   ELBER G, 2005, IRIT SOLID MODELER
   ELBER G, 2005, ESCHER REAL PROJECT
   FUKUDA S, 2005, WORKS
   GREINER G, 1994, COMPUT GRAPH FORUM, V13, pC143, DOI 10.1111/1467-8659.1330143
   HADENFELD J, 1995, P 1 C MATH METH CURV, P203
   Hahmann S, 1998, COMPUT AIDED DESIGN, V30, P131, DOI 10.1016/S0010-4485(97)00078-X
   HSU WM, 1992, COMP GRAPH, V26, P177, DOI 10.1145/142920.134036
   Hu SM, 2001, VISUAL COMPUT, V17, P370, DOI 10.1007/s003710100114
   HUA J, 2003, SOL MOD C SEATTL WAS, P328
   Igarashi T, 1999, COMP GRAPH, P409, DOI 10.1145/311535.311602
   Johan H, 2000, EIGHTH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P348, DOI 10.1109/PCCGA.2000.883958
   MORETON HP, 1992, COMP GRAPH, V26, P167, DOI 10.1145/142920.134035
   RAETZ M, 2005, M RAETZ METAMORPHOSE
   Raviv A, 2000, COMPUT AIDED DESIGN, V32, P513, DOI 10.1016/S0010-4485(00)00039-7
   RODRIGUES W, 2001, P 3 INT S INT MAN SY
   Sederberg T. W., 1986, Computer Graphics, V20, P151, DOI 10.1145/15886.15903
   Vassilev TI, 1996, COMPUT AIDED DESIGN, V28, P753, DOI 10.1016/0010-4485(95)00087-9
   YOSHIZAWA S, 2003, SM 03 P 8 ACM S SOL, P247
   Zhang CM, 2001, COMPUT AIDED DESIGN, V33, P913, DOI 10.1016/S0010-4485(00)00114-7
   2005, WIKIPEDIA CLAIRAUTS
NR 25
TC 12
Z9 12
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2007
VL 23
IS 3
BP 219
EP 229
DI 10.1007/s00371-006-0095-2
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 134PM
UT WOS:000244095300005
DA 2024-07-18
ER

PT J
AU Wang, CB
   Wang, ZY
   Xia, T
   Peng, QS
AF Wang, CB
   Wang, ZY
   Xia, T
   Peng, QS
TI Real-time snowing simulation
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Computer Animation and Social Agent Conference (CASA 2004)
CY JUL, 2004
CL Univ Geneva, Geneva, SWITZERLAND
HO Univ Geneva
DE dynamic snowing scene; real-time simulation; Boltzmann equation; 3D wind
   field
AB A snowing scene has a unique fascination for people due to its incomparable beauty. However, little work has been presented on the real-time generation of a dynamic snowing scene, partially due to the difficulty that the simulation of a dynamic snowing process involves the complex modeling of the wind field and the interaction between wind and snow. In this paper, by fully considering the physical characteristics of wind and snow, we construct a three-dimensional wind field based on the discrete form of the Boltzmann equation. According to the interaction laws between wind and snow, we simulate the falling of snow, deposition and erosion in 3D space. Experimental results show that realistic wind-driven snow scenes under different speeds of wind with different amounts of snowfall can be rendered in real-time.
C1 Zhejiang Univ, State Key Lab CAD & CG, Hangzhou, Peoples R China.
   Tongji Univ, Dept Bldg Engn, Shanghai 200092, Peoples R China.
C3 Zhejiang University; Tongji University
RP Zhejiang Univ, State Key Lab CAD & CG, Hangzhou, Peoples R China.
RI Zhou, Hong/JKJ-1067-2023
CR BANG B, 1994, ENERG BUILDINGS, V21, P235, DOI 10.1016/0378-7788(94)90039-6
   Chapman S., 1970, The Mathematical Theory of Non-uniform Gases
   Chen S, 1998, ANNU REV FLUID MECH, V30, P329, DOI 10.1146/annurev.fluid.30.1.329
   Chen Yan-Yun, 2002, Chinese Journal of Computers, V25, P916
   Corripio JG, 2004, COLD REG SCI TECHNOL, V39, P93, DOI 10.1016/j.coldregions.2004.03.007
   Fearing P, 2000, COMP GRAPH, P37, DOI 10.1145/344779.344809
   FELDMAN BE, 2002, P ACM SIGGRAPH 2002, P9
   Haglund H., 2002, P SIGRAD, P11
   LANGER MS, 2003, P ACM SIGGRAPH 2003, P58
   MASSELET A, 1995, COMPUTATIONS PHYS CH, P429
   Mei R, 2000, J COMPUT PHYS, V161, P680, DOI 10.1006/jcph.2000.6522
   Nishita T, 1997, COMPUT GRAPH FORUM, V16, pC357, DOI 10.1111/1467-8659.00173
   OHLSSON P, 2004, SIGRAD 2004 SPECIAL, P25
   REEVES WT, 1983, ACM T GRAPHIC, V2, P91, DOI 10.1145/964967.801167
   Sumner RW, 1999, COMPUT GRAPH FORUM, V18, P17, DOI 10.1111/1467-8659.00299
   Thiis TK, 2003, J WIND ENG IND AEROD, V91, P829, DOI 10.1016/S0167-6105(02)00474-9
   WEI XM, 2003, P EUR SIGGRAPH S COM
   WEJCHERT J, 1991, COMP GRAPH, V25, P19, DOI 10.1145/127719.122719
NR 18
TC 26
Z9 34
U1 1
U2 10
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2006
VL 22
IS 5
BP 315
EP 323
DI 10.1007/s00371-006-0012-8
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 041PQ
UT WOS:000237468500004
DA 2024-07-18
ER

PT J
AU Choi, YJ
   Kim, YJ
   Kim, MH
AF Choi, YJ
   Kim, YJ
   Kim, MH
TI Rapid pairwise intersection tests using programmable GPUs
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 5th Israel-Korea Bi-National Conference on Geometric Modeling and
   Computer Graphics
CY OCT 11-12, 2004
CL Seoul Natl Univ, Seoul, SOUTH KOREA
HO Seoul Natl Univ
DE collision detection; geometric modeling; programmable shaders;
   deformable body simulation; computer animation
AB Detecting self-intersections within a triangular mesh model is fundamentally a quadratic problem in terms of its computational complexity, since in principle all triangles must be compared with all others. We reflect the 2D nature of this process by storing the triangles as multiple 1D textures in texture memory, and then exploit the massive parallelism of graphics processing units (GPUs) to perform pairwise comparisons, using a pixel shader. This approach avoids the creation and maintenance of auxiliary geometric structures, such as a bounding volume hierarchy (BVH); but nevertheless we can plug in auxiliary culling schemes, and use stencils to indicate triangle pairs that do not need to be compared. To overcome the readback bottleneck between GPU and CPU, we use a hierarchical encoding scheme. We have applied our technique to detecting self-intersections in extensively deformed models, and we achieve an order of magnitude increase in performance over CPU-based techniques such as [17].
C1 Ewha Womans Univ, Dept Comp Sci & Engn, Seoul, South Korea.
C3 Ewha Womans University
RP Choi, YJ (corresponding author), Ewha Womans Univ, Dept Comp Sci & Engn, Seoul, South Korea.
EM choirina@ewhain.net; kimy@ewha.ac.kr; mhkim@ewha.ac.kr
OI Kim, Young J./0000-0003-2159-4832
CR [Anonymous], 1997, J GRAPH TOOLS, DOI DOI 10.1080/10867651.1997.10487480
   [Anonymous], P ACM SIGGRAPH EUROG
   Baciu G, 1999, J VISUAL COMP ANIMAT, V10, P181, DOI 10.1002/(SICI)1099-1778(199910/12)10:4<181::AID-VIS211>3.0.CO;2-Q
   Bolz J, 2003, ACM T GRAPHIC, V22, P917, DOI 10.1145/882262.882364
   Cotin S, 2000, VISUAL COMPUT, V16, P437, DOI 10.1007/PL00007215
   Elber G, 1999, COMPUT AIDED DESIGN, V31, P163, DOI 10.1016/S0010-4485(99)00012-3
   Fernando R., 2003, CG TUTORIAL
   Govindaraju NK, 2005, P IEEE VIRT REAL ANN, P59
   GOVINDARAJU NK, 2003, TR03044 U N CAR CHAP
   Halperin D., 1997, Handbook of discrete and computational geometry, P389
   Hoff KennethE., 2001, I3D 01, P145
   Hughes M., 1996, Proceedings. Computer Animation '96, P155, DOI 10.1109/CA.1996.540498
   Krüger J, 2003, ACM T GRAPHIC, V22, P908, DOI 10.1145/882262.882363
   Lau RWH, 2002, P ACM S VIRT REAL TE, P113
   Lin M.C., 2004, HDB DISCRETE COMPUTA, Vsecond, P787
   Lombardo JC, 1999, COMP ANIM CONF PROC, P82, DOI 10.1109/CA.1999.781201
   Moeller T., 1997, J GRAPHICS TOOLS, V2, P25
   PELLACINI F, 2004, GPU GEMS
   RAMKUMAR GD, 1998, THESIS STANFORD
   VOLINO P, 1994, P EUROGRAPHICS, P155
   [No title captured]
NR 21
TC 3
Z9 4
U1 0
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2006
VL 22
IS 2
BP 80
EP 89
DI 10.1007/s00371-006-0368-9
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 018GC
UT WOS:000235751200003
DA 2024-07-18
ER

PT J
AU Oh, S
   Ahn, J
   Wohn, K
AF Oh, S
   Ahn, J
   Wohn, K
TI Low damped cloth simulation
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 5th Israel-Korea Bi-National Conference on Geometric Modeling and
   Computer Graphics
CY OCT 11-12, 2004
CL Seoul Natl Univ, Seoul, SOUTH KOREA
HO Seoul Natl Univ
DE computer animation; cloth simulation; implicit integration; low damped
   cloth modeling
AB This paper proposes a new implicit integration technique that reproduces a stable cloth without introducing excessive damping forces. Semi-implicit integration methods have been widely used in cloth simulations because of their high stability and speed. Artificial internal damping forces are generated during the linearization process of the semi-implicit integration. The simulations become extremely stable due to the artificial forces; however, the forces significantly degrade the realism of cloth simulations, since they are generated with respect to rotational rigid motions, as well as internal deformations. Hence, we propose a new method to decrease the damping artifacts. The artificial internal damping forces are computed solely for pure internal deformations, and are stably incorporated into the dynamical system. Experiments show that our simulator can reproduce various cloth materials without excessive damping artifacts even in real-time.
C1 Korea Adv Inst Sci & Technol, EECS Dept, Taejon, South Korea.
C3 Korea Advanced Institute of Science & Technology (KAIST)
RP Korea Adv Inst Sci & Technol, EECS Dept, Taejon, South Korea.
EM redmong@vr.kaist.ac.kr; chocchoggi@vr.kaist.ac.kr; wohn@vr.kaist.ac.kr
RI Wohn, Kwangyun/C-2013-2011
CR Baraff D., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P43, DOI 10.1145/280814.280821
   Breen D. E., 1994, Computer Graphics Proceedings. Annual Conference Series 1994. SIGGRAPH 94 Conference Proceedings, P365, DOI 10.1145/192161.192259
   Bridson R, 2002, ACM T GRAPHIC, V21, P594, DOI 10.1145/566570.566623
   Bridson R., 2003, Simulation of clothing with folds and wrinkles, P28
   Choi KJ, 2002, ACM T GRAPHIC, V21, P604, DOI 10.1145/566570.566624
   Choi MG, 2005, IEEE T VIS COMPUT GR, V11, P91
   Cordier F, 2003, IEEE COMPUT GRAPH, V23, P38, DOI 10.1109/MCG.2003.1159612
   Desbrun M, 1999, PROC GRAPH INTERF, P1
   Eberhardt B, 2000, SPRING COMP SCI, P137
   Eberhardt B, 1996, IEEE COMPUT GRAPH, V16, P52, DOI 10.1109/38.536275
   Etzmuss O, 2003, 11TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P244, DOI 10.1109/PCCGA.2003.1238266
   Grinspun E., 2003, P 2003 ACM SIGGRAPH, P62
   Hauth M, 2003, VISUAL COMPUT, V19, P581, DOI 10.1007/s00371-003-0206-2
   HAUTH M, 2001, P EUR MANCH, P319
   HOUSE DH, 2000, CLOTH MODELING ANIMA
   Müller M, 2004, PROC GRAPH INTERF, P239
   Muller M., 2002, P 2002 ACM SIGGRAPHE, P49, DOI DOI 10.1145/545261.545269
   Oh S. W., 2002, P VSMM 2002 KYUNGJ, P239
   PARKS D, 2002, EUROGRAPHICS SHORT P
   PROVOT X, 1995, GRAPH INTER, P147
   VASSILEV T, 2001, EUROGRAPHICS, V20, P260
   Volino P, 2001, COMPUTER GRAPHICS INTERNATIONAL 2001, PROCEEDINGS, P265, DOI 10.1109/CGI.2001.934683
   Volino P, 2000, COMPUTER GRAPHICS INTERNATIONAL 2000, PROCEEDINGS, P257, DOI 10.1109/CGI.2000.852341
   [No title captured]
NR 24
TC 13
Z9 17
U1 0
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2006
VL 22
IS 2
BP 70
EP 79
DI 10.1007/s00371-006-0367-x
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 018GC
UT WOS:000235751200002
DA 2024-07-18
ER

PT J
AU Meseth, J
   Guthe, M
   Klein, R
AF Meseth, J
   Guthe, M
   Klein, R
TI Interactive fragment tracing
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 13th Pacific Conference on Computer Graphics and Applications
CY OCT 12-14, 2005
CL Macao, PEOPLES R CHINA
DE interactive reflections; GPU-based rendering; ray tracing
AB One of the main challenges in real-time rendering is to enable more and more effects that were previously available in offline rendering only. An important effect among these is physically correct reflections of arbitrary objects in curved reflectors like windshields.
   In this paper we propose fragment tracing on the GPU as a solution to interactively realizing this effect for large scenes as employed in industrial applications. For each rasterized fragment, a ray is traced through an octree representing the original geometry and surface material. By introducing a GPU implementation of an octree traversal, for the first time hierarchical data structures can efficiently be used on the GPU. As a result, the approach allows both handling of large geometries such as those employed in virtual prototyping and accurate rendering. Several examples show the generality and achievable rendering quality of our method.
C1 Univ Bonn, Inst Informat 2, Comp Graph Grp, D-53117 Bonn, Germany.
C3 University of Bonn
RP Univ Bonn, Inst Informat 2, Comp Graph Grp, Romerstr 164, D-53117 Bonn, Germany.
EM meseth@cs.uni-bonn.de; guthe@cs.uni-bonn.de; rk@cs.uni-bonn.de
CR Amanatides J., 1987, EUROGRAPHICS, V87, P3
   [Anonymous], 2005, GPU GEMS
   BASTOS R, 1998, TR98026 CS U N CAR
   BROWN P, NV FRAGMENT PROGRAM
   CARR NA, 2002, P GRAPH HARDW
   CHEN M, 1999, CSTR9905 CALTECH
   CHRISTEN M, 2005, THESIS U APPL SCI BA
   ERNST M, 2004, P VIS MOD VIS, P255
   Fender J, 2003, 2003 IEEE INTERNATIONAL CONFERENCE ON FIELD-PROGRAMMABLE TECHNOLOGY (FPT), PROCEEDINGS, P188, DOI 10.1109/FPT.2003.1275747
   FOLEY T., 2005, HWWS 05, P15
   GOBBETTI E, 2005, P SIGGRAPH, P878
   GREENE N, 1986, IEEE COMPUT GRAPH, V6, P21, DOI 10.1109/MCG.1986.276658
   HACHISUKA T, 2005, GPU GEMS 2, P615
   HEIDRICH W, 1999, 10 EUR WORKSH REND
   HEIDRICH W, 1998, GRAPHCIS HARDWARE, P39
   Kilgard MJ., 1999, PERFECT REFLECTIONS
   LARSEN BD, 2004, P EUR S REND, P124
   MITCHELL D, 1992, COMP GRAPH, V26, P283, DOI 10.1145/142920.134082
   Müller G, 2005, COMPUT GRAPH FORUM, V24, P83, DOI 10.1111/j.1467-8659.2005.00830.x
   Müller G, 2003, VISION, MODELING, AND VISUALIZATION 2003, P271
   Nielsen KH, 2002, WSCG'2002 SHORT COMMUNICATION PAPERS, CONFERENCE PROCEEDINGS, P91
   OFEK E, 1998, P 25 ANN C COMP GRAP, P333
   Parker S, 1998, VISUALIZATION '98, PROCEEDINGS, P233, DOI 10.1109/VISUAL.1998.745713
   PARKER S, 1999, P INT 3D COMP GRAPH
   Purcell T., 2003, SIGGRAPHEUROGRAPHICS, P41
   Purcell TJ, 2002, ACM T GRAPHIC, V21, P703, DOI 10.1145/566570.566640
   SCHLICK C, 1994, COMPUT GRAPH FORUM, V13, pC233, DOI 10.1111/1467-8659.1330233
   SCHMITTLER J., 2002, HWWS 02, P27
   ULLMANN T, 2001, EUROPGRAPHICS SHORT
   Wald I, 2001, COMPUT GRAPH FORUM, V20, pC153, DOI 10.1111/1467-8659.00508
   Wand M, 2003, PROC GRAPH INTERF, P139
   Weiskopf D, 2004, COMPUT GRAPH FORUM, V23, P625, DOI 10.1111/j.1467-8659.2004.00794.x
   WOOD A, 2004, P IM VIS COMP NZ, P327
   Woop S, 2005, ACM T GRAPHIC, V24, P434, DOI 10.1145/1073204.1073211
   Yu Jingyi., 2005, Proceedings of Symposium on Interactive 3D Graphics and Games (I3D), P133
   [No title captured]
NR 36
TC 1
Z9 2
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2005
VL 21
IS 8-10
SI SI
BP 591
EP 600
DI 10.1007/s00371-005-0322-2
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 964CX
UT WOS:000231857400011
DA 2024-07-18
ER

PT J
AU Wang, T
   Yang, XD
AF Wang, T
   Yang, XD
TI A sweeping algorithm for cluster hair rendering
SO VISUAL COMPUTER
LA English
DT Article
DE cluster hair model; hair-rendering algorithm; multi-resolution model;
   projection-based sweeping algorithm
ID FRAME; MODEL
AB Rendering realistic human hair is a very challenging and important issue in computer graphics. The major difficulty comes from the large number of hair strands to be modeled, the fine geometries of individual hair, and the complex illumination interactions among the hair. A new method, called the projection-based sweeping algorithm, is developed to render hair images using the cluster model. Compared with a previous ray-tracing algorithm, the new algorithm offers several significant improvements: (1) it avoids the complex and expensive computation of the ray-cluster intersection; (2) it eliminates the time-consuming density-value evaluations at the sample points along a ray during ray tracing; (3) it allows more flexible variation of the density map along the trajectory to enhance the natural appearance of hair; and (4) it allows easier coarse/fine control over the density map for multi-resolution rendering. As a result, the new algorithm significantly improves rendering efficiency and hair image quality. Compared with the existing hardware-supported, texture-mapping projection methods, the new method allows more sophisticated shading models, accurate shadow calculation, and uniform handling of arbitrary slice orientations with accurate opacity accumulation.
C1 Univ Regina, Dept Comp Sci, Regina, SK S4S 0A2, Canada.
C3 University of Regina
RP Yang, XD (corresponding author), Univ Regina, Dept Comp Sci, Regina, SK S4S 0A2, Canada.
CR AGUI T, 1991, IPSJ, V32, P749
   BISHOP RL, 1975, AM MATH MON, V82, P246, DOI 10.2307/2319846
   Bronsvoort W. F., 1992, Visual Computer, V8, P162, DOI 10.1007/BF01902136
   CARPENTER L, 1984, P 11 ANN C COMP GRAP
   Chen LH, 1999, VISUAL COMPUT, V15, P159, DOI 10.1007/s003710050169
   COOK R, 1987, P 14 ANN C COMP GRAP
   CSURI C, 1979, P 6 ANN C COMP GRAPH
   GELDER A, 1997, GRAPHICS INTERFACE
   GOLDMAN D, 1997, P 24 C COMP GRAPH IN
   HADAP S, 2000, COMPUTER ANIMATION S
   HANSON AJ, 1995, IEEE T VIS COMPUT GR, V1, P164, DOI 10.1109/2945.468403
   Kajiya J. T., 1989, P 16 ANN C COMP GRAP
   LAUR D, 1991, P 18 ANN C COMP GRAP
   LeBlanc A. M., 1991, Journal of Visualization and Computer Animation, V2, P92, DOI 10.1002/vis.4340020305
   LOKOVIC T, 2000, P 27 ANN C COMP GRAP
   MILLER G, 1988, GRAPHICS INTERFACE
   Neyret F, 1998, IEEE T VIS COMPUT GR, V4, P55, DOI 10.1109/2945.675652
   Perlin K., 1989, P 16 ANN C COMP GRAP
   Reeves W., 1983, P 10 ANN C COMP GRAP
   Wang W. P., 1986, IEEE Computer Graphics and Applications, V6, P8, DOI 10.1109/MCG.1986.276586
   Watanabe Y., 1991, Visual Computer, V7, P97, DOI 10.1007/BF01901180
   WESTOVER L, 1989, P 1989 CHAP HILL WOR
   Westover L, 1990, P 17 ANN C COMP GRAP
   WILHELMS J, 1991, P 18 ANN C COMP GRAP
   XU YQ, 2001, P 28 ANN C COMP GRAP
   Yang XD, 2000, GRAPH MODELS, V62, P85, DOI 10.1006/gmod.1999.0518
   YANG XD, 1996, P PAC GRAPH HSINCH T
   YANG XD, 2000, TR200009 U REG DEP C
NR 28
TC 0
Z9 0
U1 0
U2 0
PU SPRINGER-VERLAG
PI NEW YORK
PA 175 FIFTH AVE, NEW YORK, NY 10010 USA
SN 0178-2789
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2003
VL 19
IS 2-3
BP 187
EP 204
DI 10.1007/s00371-003-0193-3
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 691AU
UT WOS:000183583300011
DA 2024-07-18
ER

PT J
AU Zheng, L
   Bai, J
   Bai, SJ
   Li, WJ
   Peng, B
   Zhou, T
AF Zheng, Liang
   Bai, Jing
   Bai, Shaojin
   Li, Wenjing
   Peng, Bin
   Zhou, Tao
TI V<SUP>2</SUP>MLP: an accurate and simple multi-view MLP networkfor
   fine-grained 3D shape recognition
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE 3D shape recognition; Fine-grained recognition; MLP; Multi-view;
   Cross-view
AB Fine-grained 3D shape recognition (FGSR) is crucial for real-world applications. Existing methods face challenges in achieving high accuracy for FGSR due to high similarity within sub-categories and low dissimilarity between them, especially in the absence of part location or attribute annotations. In this paper, we propose (VMLP)-M-2, a multi-view representation-oriented MLP network dedicated to FGSR, using only class labels as supervision. (VMLP)-M-2 comprises two key modules: the cross-view interaction MLP (CVI-MLP) and the cross-view fusion MLP (CVF-MLP). The CVI-MLP module captures contextual information, including local and global contexts through cross-view interactions, to extract discriminative view features that reinforce subtle differences between sub-categories. Meanwhile, the CVF-MLP module performs cross-view aggregation from space and view dimensions to obtain the final 3D shape features, minimizing information loss during the view feature fusion process. Extensive experiments on three categories from the FG3D dataset demonstrate the effectiveness of V2MLP in learning discriminative features for 3D shapes, achieving state-of-the-art accuracy for FGSR. Additionally, V2MLP performs competitively for meta-category recognition on the ModelNet40 dataset
C1 [Zheng, Liang; Bai, Jing; Bai, Shaojin; Li, Wenjing; Peng, Bin; Zhou, Tao] North Minzu Univ, Sch Comp Sci & Engn, Yinchuan 750021, Peoples R China.
   [Bai, Jing; Zhou, Tao] North Minzu Univ, Key Lab Images Proc, Commiss IPPRLab, Yinchuan 750021, Peoples R China.
   [Bai, Jing; Zhou, Tao] North Minzu Univ, Pattern Lab, Commiss IPPRLab, Yinchuan 750021, Peoples R China.
C3 North Minzu University; North Minzu University; North Minzu University
RP Bai, J (corresponding author), North Minzu Univ, Sch Comp Sci & Engn, Yinchuan 750021, Peoples R China.; Bai, J (corresponding author), North Minzu Univ, Key Lab Images Proc, Commiss IPPRLab, Yinchuan 750021, Peoples R China.; Bai, J (corresponding author), North Minzu Univ, Pattern Lab, Commiss IPPRLab, Yinchuan 750021, Peoples R China.
EM 673344391@qq.com; baijing@nun.edu.cn; 19995281956@163.com;
   1273365496@qq.com; 1099207098@qq.com; zhoutaonxmu@126.com
RI Bai, Jing/HGD-3571-2022
OI Bai, Jing/0000-0003-4247-6210
FU National Natural Science Foundation of China
FX No Statement Available
CR Chen SL, 2019, IEEE T VIS COMPUT GR, V25, P3244, DOI 10.1109/TVCG.2018.2866793
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Dai GX, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P670
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Fang Liu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12364), P718, DOI 10.1007/978-3-030-58529-7_42
   Feng YF, 2018, PROC CVPR IEEE, P264, DOI 10.1109/CVPR.2018.00035
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Guo MH, 2021, COMPUT VIS MEDIA, V7, P187, DOI 10.1007/s41095-021-0229-5
   Han ZZ, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P758
   Han ZZ, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P766
   Han ZZ, 2019, IEEE T IMAGE PROCESS, V28, P3986, DOI 10.1109/TIP.2019.2904460
   Han ZZ, 2019, IEEE T IMAGE PROCESS, V28, P658, DOI 10.1109/TIP.2018.2868426
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Johns E, 2016, PROC CVPR IEEE, P3813, DOI 10.1109/CVPR.2016.414
   Kanezaki A, 2018, PROC CVPR IEEE, P5010, DOI 10.1109/CVPR.2018.00526
   Li M, 2022, VISUAL COMPUT, V38, P811, DOI 10.1007/s00371-020-02052-8
   Lin TY, 2015, IEEE I CONF COMP VIS, P1449, DOI 10.1109/ICCV.2015.170
   Liu Hong, 2021, Advances in Neural Information Processing Systems, V34
   Liu XH, 2021, IEEE T IMAGE PROCESS, V30, P1744, DOI 10.1109/TIP.2020.3048623
   Lyu CZ, 2022, VISUAL COMPUT, V38, P345, DOI 10.1007/s00371-020-02018-w
   Ma C, 2019, IEEE T MULTIMEDIA, V21, P1169, DOI 10.1109/TMM.2018.2875512
   Ma Xiaojian, 2022, INT C LEARN REPR
   Maturana D, 2015, IEEE INT C INT ROBOT, P922, DOI 10.1109/IROS.2015.7353481
   Savva M., 2016, P EUR WORKSH 3D OBJ, P89, DOI DOI 10.2312/3DOR.20161092
   Shao HH, 2023, PATTERN RECOGN, V139, DOI 10.1016/j.patcog.2023.109509
   Shilane P, 2004, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, P167, DOI 10.1109/smi.2004.1314504
   Su H, 2015, IEEE I CONF COMP VIS, P945, DOI 10.1109/ICCV.2015.114
   Tang YH, 2022, PROC CVPR IEEE, P10925, DOI 10.1109/CVPR52688.2022.01066
   Tolstikhin I, 2021, ADV NEUR IN, V34
   Wei X, 2020, PROC CVPR IEEE, P1847, DOI 10.1109/CVPR42600.2020.00192
   Wu RS, 2024, VISUAL COMPUT, V40, P781, DOI 10.1007/s00371-023-02816-y
   Xiao TJ, 2015, PROC CVPR IEEE, P842, DOI 10.1109/CVPR.2015.7298685
   Xiong S., 2023, IEEE RSJ INT C INT R
   Yang Z, 2019, IEEE I CONF COMP VIS, P7504, DOI 10.1109/ICCV.2019.00760
   Yu T, 2018, PROC CVPR IEEE, P186, DOI 10.1109/CVPR.2018.00027
   Zhang H, 2016, PROC CVPR IEEE, P1143, DOI 10.1109/CVPR.2016.129
   Zhang N, 2014, LECT NOTES COMPUT SC, V8689, P834, DOI 10.1007/978-3-319-10590-1_54
   Zhao B, 2017, IEEE T MULTIMEDIA, V19, P1245, DOI 10.1109/TMM.2017.2648498
   Zhao HS, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P16239, DOI 10.1109/ICCV48922.2021.01595
   Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319
   Zhu YS, 2020, VISUAL COMPUT, V36, P1771, DOI 10.1007/s00371-019-01770-y
NR 41
TC 0
Z9 0
U1 1
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2023 DEC 21
PY 2023
DI 10.1007/s00371-023-03191-4
EA DEC 2023
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CW7N9
UT WOS:001128338300001
DA 2024-07-18
ER

PT J
AU Fang, ZY
   Chen, NN
   Jiang, Y
   Fan, Y
AF Fang, Zhaoyan
   Chen, Niannian
   Jiang, Yong
   Fan, Yong
TI Decouple and align classification and regression in one-stage object
   detection
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Deep learning; Object detection; Decouple; Alignment; Label assignment
AB Current one-stage object detection methods use dense prediction to generate classification and regression results at the same point on the feature map. Due to the different task attributes, classification and regression are typically trained using separate detection heads, which may result in different feature areas being focused on. However, they ultimately act on the same object, especially in the post-processing stage, where we hope they have the same performance. This inherent contradiction can seriously affect the performance of the detector. To solve this problem, we propose a flexible and effective decouple and align classification and regression one-stage object detector (DAOD), based on different aspects to decouple and align the two subtasks. Specifically, we first propose a regression subtask spatial decouple module to solve the regression spatial sensitivity problem by efficiently sampling the information of the regression result map to strengthen localization. Then, we propose a dynamic aligned label assignment strategy for sample selection, guiding the network to focus on more aligned features during training. Finally, we introduce harmonic supervision to align results while ensuring the independence of the respective task. With the negligible additional overhead, extensive experiments on the COCO dataset demonstrate the effectiveness of our DAOD. Notably, DAOD with ResNeXt-101-64x4d-DCN backbone achieves 50.0 AP at single-model single-scale testing on MS-COCO test-dev.
C1 [Fang, Zhaoyan; Chen, Niannian; Jiang, Yong; Fan, Yong] SouthWest Univ Sci & Technol, Sch Comp Sci & Technol, Mianyang 621010, Peoples R China.
C3 Southwest University of Science & Technology - China
RP Fan, Y (corresponding author), SouthWest Univ Sci & Technol, Sch Comp Sci & Technol, Mianyang 621010, Peoples R China.
EM 2788321369@qq.com; chenniannian@swust.edu.cn; jiang_yong@swust.edu.cn;
   3210343848@qq.com
CR Bottou L, 2010, COMPSTAT'2010: 19TH INTERNATIONAL CONFERENCE ON COMPUTATIONAL STATISTICS, P177, DOI 10.1007/978-3-7908-2604-3_16
   Cai ZW, 2018, PROC CVPR IEEE, P6154, DOI 10.1109/CVPR.2018.00644
   Chen K, 2019, Arxiv, DOI arXiv:1906.07155
   Chen Y., 2020, Proc. Adv. Neural Inf. Process. Syst., NeurIPS, V33, P5621
   Chen YT, 2019, Arxiv, DOI arXiv:1908.01570
   Chen ZH, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P4939, DOI 10.1145/3474085.3475351
   Chenchen Zhu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12354), P91, DOI 10.1007/978-3-030-58545-7_6
   Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89
   Duan KW, 2019, IEEE I CONF COMP VIS, P6568, DOI 10.1109/ICCV.2019.00667
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Feng CJ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P3490, DOI 10.1109/ICCV48922.2021.00349
   Gao ZT, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P3621, DOI 10.1109/ICCV48922.2021.00362
   Ge Z, 2021, Arxiv, DOI arXiv:2107.08430
   Ge Z, 2021, PROC CVPR IEEE, P303, DOI 10.1109/CVPR46437.2021.00037
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Guanglu Song, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11560, DOI 10.1109/CVPR42600.2020.01158
   Han Qiu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P549, DOI 10.1007/978-3-030-58452-8_32
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hengduo Li, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10585, DOI 10.1109/CVPR42600.2020.01060
   Jiang BR, 2018, LECT NOTES COMPUT SC, V11218, P816, DOI 10.1007/978-3-030-01264-9_48
   Jiang Z., 2023, Vis. Comput., P1
   Kang Kim, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12370), P355, DOI 10.1007/978-3-030-58595-2_22
   Kong T, 2020, IEEE T IMAGE PROCESS, V29, P7389, DOI 10.1109/TIP.2020.3002345
   Lang AH, 2019, PROC CVPR IEEE, P12689, DOI 10.1109/CVPR.2019.01298
   Law H, 2018, LECT NOTES COMPUT SC, V11218, P765, DOI 10.1007/978-3-030-01264-9_45
   Li X, 2020, Arxiv, DOI arXiv:2006.04388
   Li X, 2021, PROC CVPR IEEE, P11627, DOI 10.1109/CVPR46437.2021.01146
   Lin Tsung-Yi, 2020, IEEE Trans Pattern Anal Mach Intell, V42, P318, DOI 10.1109/TPAMI.2018.2858826
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu J, 2021, PROC CVPR IEEE, P264, DOI 10.1109/CVPR46437.2021.00033
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Ma YC, 2021, PROC CVPR IEEE, P1717, DOI 10.1109/CVPR46437.2021.00176
   Pang Y, 2023, VISUAL COMPUT, V39, P1959, DOI 10.1007/s00371-022-02458-6
   Redmon J., 2018, arXiv, DOI DOI 10.48550/ARXIV.1804.02767
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rezatofighi H, 2019, PROC CVPR IEEE, P658, DOI 10.1109/CVPR.2019.00075
   Shao MW, 2023, VISUAL COMPUT, V39, P5719, DOI 10.1007/s00371-022-02691-z
   Tian Z, 2019, IEEE I CONF COMP VIS, P9626, DOI 10.1109/ICCV.2019.00972
   Wang C., 2020, FairMOT: On the fairness of detection and re-identification in multiple object tracking
   Wang JQ, 2019, PROC CVPR IEEE, P2960, DOI 10.1109/CVPR.2019.00308
   Wang KY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P3611, DOI 10.1109/ICCV48922.2021.00361
   Wei Ke, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10203, DOI 10.1109/CVPR42600.2020.01022
   Xiao JR, 2022, NEUROCOMPUTING, V514, P58, DOI 10.1016/j.neucom.2022.09.132
   Yang L, 2022, IEEE T IMAGE PROCESS, V31, P5121, DOI 10.1109/TIP.2022.3193223
   Yang Y., 2022, MACHINE LEARNING KNO, P427
   Yang Z, 2019, IEEE I CONF COMP VIS, P9656, DOI 10.1109/ICCV.2019.00975
   Yue Wu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10183, DOI 10.1109/CVPR42600.2020.01020
   Yuhang Cao, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11580, DOI 10.1109/CVPR42600.2020.01160
   Zhang HY, 2021, PROC CVPR IEEE, P8510, DOI 10.1109/CVPR46437.2021.00841
   Zhang S., 2020, P IEEECVF C COMPUTER, P9759
   Zhang S, 2018, PROC CVPR IEEE, P4203, DOI 10.1109/CVPR.2018.00442
   Zhang TS, 2023, VISUAL COMPUT, V39, P569, DOI 10.1007/s00371-021-02357-2
   Zhang XS, 2022, IEEE T PATTERN ANAL, V44, P3096, DOI 10.1109/TPAMI.2021.3050494
   Zhu BJ, 2020, Arxiv, DOI arXiv:2007.03496
   Zhu CC, 2019, PROC CVPR IEEE, P840, DOI 10.1109/CVPR.2019.00093
   Zhu XZ, 2019, PROC CVPR IEEE, P9300, DOI 10.1109/CVPR.2019.00953
NR 60
TC 0
Z9 0
U1 16
U2 16
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2023 DEC 18
PY 2023
DI 10.1007/s00371-023-03207-z
EA DEC 2023
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CP2R3
UT WOS:001126389100001
DA 2024-07-18
ER

PT J
AU Wang, WJ
   Pan, W
   Dai, CF
   Dazeley, R
   Wei, L
   Rolfe, B
   Lu, XQ
AF Wang, Weijia
   Pan, Wei
   Dai, Chaofan
   Dazeley, Richard
   Wei, Lei
   Rolfe, Bernard
   Lu, Xuequan
TI Segmentation-driven feature-preserving mesh denoising
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Mesh denoising; 3D Vision; 3D Processing
ID SHIFT
AB Feature-preserving mesh denoising has received noticeable attention in visual media, with the aim of recovering high-fidelity, clean mesh shapes from the ones that are contaminated by noise. Existing denoising methods often design smaller weights for anisotropic surfaces and larger weights for isotropic surfaces in order to preserve sharp features, such as edges or corners, on the mesh shapes. However, they often disregard the fact that such small weights on anisotropic surfaces still pose negative impacts on the denoising outcomes and detail preservation results on the shapes. In this paper, we propose a novel segmentation-driven mesh denoising method which performs region-wise denoising, and thus avoids the disturbance of anisotropic neighbour faces for better feature preservation results. Also, our backbone can be easily embedded into commonly used mesh denoising frameworks. Extensive experiments have demonstrated that our method can enhance the denoising results on a wide range of synthetic and real mesh models, both quantitatively and visually.
C1 [Wang, Weijia; Dazeley, Richard; Wei, Lei; Rolfe, Bernard] Deakin Univ, Geelong, Vic, Australia.
   [Pan, Wei] OPT Machine Vis Tech Co Ltd, Tokyo, Japan.
   [Dai, Chaofan] OPT Machine Vis Tech Co Ltd, Dongguan, Peoples R China.
   [Lu, Xuequan] La Trobe Univ, Bundoora, Vic, Australia.
C3 Deakin University; La Trobe University
RP Wang, WJ (corresponding author), Deakin Univ, Geelong, Vic, Australia.; Pan, W (corresponding author), OPT Machine Vis Tech Co Ltd, Tokyo, Japan.; Lu, XQ (corresponding author), La Trobe Univ, Bundoora, Vic, Australia.
EM wangweijia@deakin.edu.au; vpan@foxmail.com; daichaofan@optmv.com;
   richard.dazeley@deakin.edu.au; lei.wei@deakin.edu.au;
   bernard.rolfe@deakin.edu.au; b.lu@latrobe.edu.au
RI Lin, Xiaoqi/KFS-5750-2024; cheng, chen/JHS-9462-2023
OI Lu, Xuequan/0000-0003-0959-408X
CR Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   [Anonymous], 2010, ACM SIGGRAPH 2010 papers
   Attene M, 2006, VISUAL COMPUT, V22, P181, DOI 10.1007/s00371-006-0375-x
   Bian Z, 2011, COMPUT AIDED GEOM D, V28, P50, DOI 10.1016/j.cagd.2010.10.001
   Chen S., 2022, arXiv
   Cohen-Steiner D, 2004, ACM T GRAPHIC, V23, P905, DOI 10.1145/1015706.1015817
   Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236
   Fan HQ, 2010, IEEE T VIS COMPUT GR, V16, P312, DOI 10.1109/TVCG.2009.70
   FIELD DA, 1988, COMMUN APPL NUMER M, V4, P709, DOI 10.1002/cnm.1630040603
   Garcia-Garcia A, 2018, APPL SOFT COMPUT, V70, P41, DOI 10.1016/j.asoc.2018.05.018
   Golovinskiy A, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409098
   Goswami P, 2021, VISUAL COMPUT, V37, P1931, DOI 10.1007/s00371-020-01953-y
   Grzeczkowicz G, 2022, ISPRS ANN PHOTO REM, V5-2, P177, DOI 10.5194/isprs-annals-V-2-2022-177-2022
   He L, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461965
   Hildebrandt K, 2004, COMPUT GRAPH FORUM, V23, P391, DOI 10.1111/j.1467-8659.2004.00770.x
   HU ZY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P15468, DOI 10.1109/ICCV48922.2021.01520
   Huang H, 2008, SIAM J SCI COMPUT, V31, P74, DOI 10.1137/060676684
   Jia SY, 2023, VISUAL COMPUT, V39, P765, DOI 10.1007/s00371-021-02373-2
   Katz S, 2003, ACM T GRAPHIC, V22, P954, DOI 10.1145/882262.882369
   Kim B, 2005, COMPUT GRAPH FORUM, V24, P295, DOI 10.1111/j.1467-8659.2005.00854.x
   Lai YK, 2008, SPM 2008: PROCEEDINGS OF THE ACM SOLID AND PHYSICAL MODELING SYMPOSIUM, P183
   Lee KW, 2005, INT C COMP AID DES C, P275
   Lee Y, 2005, COMPUT AIDED GEOM D, V22, P444, DOI 10.1016/j.cagd.2005.04.002
   Legrand H, 2019, COMPUT GRAPH FORUM, V38, P663, DOI 10.1111/cgf.13597
   Li XZ, 2018, COMPUT GRAPH FORUM, V37, P155, DOI 10.1111/cgf.13556
   Lian Jiguang, 2022, Proceedings of 2021 Chinese Intelligent Systems Conference. Lecture Notes in Electrical Engineering (805), P193, DOI 10.1007/978-981-16-6320-8_20
   Liu CM, 2023, VISUAL COMPUT, V39, P1783, DOI 10.1007/s00371-022-02444-y
   Liu XG, 2002, GRAPH MODELS, V64, P169, DOI 10.1006/gmod.2002.0576
   Liu Z, 2022, IEEE T VIS COMPUT GR, V28, P4418, DOI 10.1109/TVCG.2021.3088118
   Lu XQ, 2022, IEEE T VIS COMPUT GR, V28, P1835, DOI 10.1109/TVCG.2020.3026785
   Lu XQ, 2017, COMPUT AIDED GEOM D, V54, P49, DOI 10.1016/j.cagd.2017.02.011
   Lu XQ, 2017, OPT LASER ENG, V90, P186, DOI 10.1016/j.optlaseng.2016.09.003
   Lu XQ, 2016, IEEE T VIS COMPUT GR, V22, P1181, DOI 10.1109/TVCG.2015.2500222
   Nealen A., 2006, P 4 INT C COMP GRAPH, P381, DOI DOI 10.1145/1174429.1174494
   Nehab D, 2005, ACM T GRAPHIC, V24, P536, DOI 10.1145/1073204.1073226
   Pan W, 2020, COMPUT AIDED DESIGN, V121, DOI 10.1016/j.cad.2019.102807
   Shapira L, 2008, VISUAL COMPUT, V24, P249, DOI 10.1007/s00371-007-0197-5
   Sheikh YA, 2007, IEEE I CONF COMP VIS, P1175
   Simari P, 2014, COMPUT GRAPH FORUM, V33, P181, DOI 10.1111/cgf.12486
   Ströter D, 2022, VISUAL COMPUT, V38, P3419, DOI 10.1007/s00371-022-02547-6
   Su ZX, 2009, SMI 2009: IEEE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, PROCEEDINGS, P1, DOI 10.1109/SMI.2009.5170156
   Sun XF, 2007, IEEE T VIS COMPUT GR, V13, P925, DOI 10.1109/TVCG.2007.1065
   Vedaldi A, 2008, LECT NOTES COMPUT SC, V5305, P705, DOI 10.1007/978-3-540-88693-8_52
   Vieira M, 2005, COMPUT AIDED GEOM D, V22, P771, DOI 10.1016/j.cagd.2005.03.006
   Vollmer J, 1999, COMPUT GRAPH FORUM, V18, pC131, DOI 10.1111/1467-8659.00334
   Wang CX, 2022, COMPUT GRAPH-UK, V106, P222, DOI 10.1016/j.cag.2022.06.006
   Wang J, 2012, COMPUT AIDED DESIGN, V44, P597, DOI 10.1016/j.cad.2012.03.001
   Wang J, 2009, PROCEEDINGS OF THE 18TH INTERNATIONAL MESHING ROUNDTABLE, P195, DOI 10.1007/978-3-642-04319-2_12
   Wei MQ, 2017, IEEE T AUTOM SCI ENG, V14, P931, DOI 10.1109/TASE.2016.2553449
   Wei MQ, 2015, IEEE T VIS COMPUT GR, V21, P43, DOI 10.1109/TVCG.2014.2326872
   Yaz I.O., 2022, CGAL User and Reference Manual
   Zhang WY, 2015, COMPUT GRAPH FORUM, V34, P23, DOI 10.1111/cgf.12742
   Zhao Y, 2018, COMPUT AIDED DESIGN, V101, P82, DOI 10.1016/j.cad.2018.04.001
   Zheng YY, 2012, IEEE T VIS COMPUT GR, V18, P1304, DOI 10.1109/TVCG.2011.140
   Zheng YY, 2011, IEEE T VIS COMPUT GR, V17, P1521, DOI 10.1109/TVCG.2010.264
   Zhu L, 2013, COMPUT GRAPH FORUM, V32, P371, DOI 10.1111/cgf.12245
NR 56
TC 0
Z9 0
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2023 NOV 29
PY 2023
DI 10.1007/s00371-023-03161-w
EA NOV 2023
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA Z2OF8
UT WOS:001110515400001
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Zhao, TM
   Zeng, H
   Zhang, BQ
   Fan, B
   Li, C
AF Zhao, Tianmeng
   Zeng, Hui
   Zhang, Baoqing
   Fan, Bin
   Li, Chen
TI Point-voxel dual stream transformer for 3d point cloud learning
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Cross-attention; Multi-representation fusion; Point cloud learning;
   Transformer; Voxel convolution; Local attention
AB Recently, the success of Transformer in natural language processing and image processing inspires researchers to apply Transformer in point cloud processing. However, existing point cloud Transformer methods have problems with massive parameters, heavy computation, and lack of local features due to the use of global self-attention. To solve these problems, this paper presents a novel point-voxel dual stream Transformer (PVDST) network, which combines the voxel-based convolution and point-based local attention, extracting the local and contextual features of point clouds simultaneously. To reduce the parameters and computation of self-attention and make the contextual features contain more position information, we design the local-aware attention module with explicit position encoding and neighbor embedding, which conducts the attention calculation locally. Based on our local-aware attention module and the cross-attention mechanism, we design a unique way to adaptively fuse the local and contextual features. Extensive experiments on shape classification, object part segmentation, and semantic segmentation tasks demonstrate that PVDST achieves competitive performance compared with other methods.
C1 [Zhao, Tianmeng; Zeng, Hui; Fan, Bin] Univ Sci & Technol Beijing, Sch Automat & Elect Engn, Beijing 100083, Peoples R China.
   [Zeng, Hui] Univ Sci & Technol Beijing, Shunde Grad Sch, Foshan 528399, Peoples R China.
   [Zhang, Baoqing] Beijing Inst Elect Syst Engn, Beijing 100854, Peoples R China.
   [Li, Chen] North China Univ Technol, Sch Informat Sci & Technol, Beijing 100144, Peoples R China.
C3 University of Science & Technology Beijing; University of Science &
   Technology Beijing; North China University of Technology
RP Zeng, H (corresponding author), Univ Sci & Technol Beijing, Sch Automat & Elect Engn, Beijing 100083, Peoples R China.; Zeng, H (corresponding author), Univ Sci & Technol Beijing, Shunde Grad Sch, Foshan 528399, Peoples R China.
EM hzeng@ustb.edu.cn
FU National Natural Science Foundation of China [61973029, 62273034,
   62076026]; Scientific and Technological Innovation Foundation of Foshan
   [BK21BF004]; Research Project of the Beijing Young Topnotch Talents
   Cultivation Program [CITTCD201904009]
FX This work was supported by the National Natural Science Foundation of
   China (Grant No. 61973029, 62273034, 62076026), Scientific and
   Technological Innovation Foundation of Foshan (BK21BF004), and Research
   Project of the Beijing Young Topnotch Talents Cultivation Program (Grand
   No: CIT&TCD201904009).
CR Armeni I, 2016, PROC CVPR IEEE, P1534, DOI 10.1109/CVPR.2016.170
   Atzmon M, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201301
   Chen LF, 2023, VISUAL COMPUT, V39, P863, DOI 10.1007/s00371-021-02351-8
   Choy C, 2019, PROC CVPR IEEE, P3070, DOI 10.1109/CVPR.2019.00319
   Cicek Ozgun, 2016, Medical Image Computing and Computer-Assisted Intervention - MICCAI 2016. 19th International Conference. Proceedings: LNCS 9901, P424, DOI 10.1007/978-3-319-46723-8_49
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Dosovitskiy Alexey, 2021, 2021 INT C LEARN REP
   Feng YF, 2018, PROC CVPR IEEE, P264, DOI 10.1109/CVPR.2018.00035
   Guo MH, 2021, COMPUT VIS MEDIA, V7, P187, DOI 10.1007/s41095-021-0229-5
   Hamdi A, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1, DOI 10.1109/ICCV48922.2021.00007
   He YQ, 2023, VISUAL COMPUT, V39, P5669, DOI 10.1007/s00371-022-02688-8
   Hu H, 2019, IEEE I CONF COMP VIS, P3463, DOI 10.1109/ICCV.2019.00356
   Jiang JW, 2019, AAAI CONF ARTIF INTE, P8513
   Jiang L, 2019, IEEE I CONF COMP VIS, P10432, DOI 10.1109/ICCV.2019.01053
   Kanezaki A, 2018, PROC CVPR IEEE, P5010, DOI 10.1109/CVPR.2018.00526
   Klokov R, 2017, IEEE I CONF COMP VIS, P863, DOI 10.1109/ICCV.2017.99
   Landrieu L, 2018, PROC CVPR IEEE, P4558, DOI 10.1109/CVPR.2018.00479
   Le T, 2018, PROC CVPR IEEE, P9204, DOI 10.1109/CVPR.2018.00959
   Lee J, 2019, PR MACH LEARN RES, V97
   Li GH, 2019, IEEE I CONF COMP VIS, P9266, DOI 10.1109/ICCV.2019.00936
   Li YZ, 2018, ADV NEUR IN, V31
   Liu TR, 2022, VISUAL COMPUT, V38, P2303, DOI 10.1007/s00371-021-02112-7
   Liu XH, 2019, AAAI CONF ARTIF INTE, P8778
   Liu YC, 2019, PROC CVPR IEEE, P8887, DOI 10.1109/CVPR.2019.00910
   Liu ZJ, 2019, ADV NEUR IN, V32
   Maturana D, 2015, IEEE INT C INT ROBOT, P922, DOI 10.1109/IROS.2015.7353481
   Noh J, 2021, PROC CVPR IEEE, P14600, DOI 10.1109/CVPR46437.2021.01437
   Qi C., 2017, NeurIPS, P5105, DOI [10.1109/ CVPR.2017.16, DOI 10.1109/CVPR.2017.16]
   Riegler G, 2017, PROC CVPR IEEE, P6620, DOI 10.1109/CVPR.2017.701
   Shaoshuai Shi, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10526, DOI 10.1109/CVPR42600.2020.01054
   Su H, 2015, IEEE I CONF COMP VIS, P945, DOI 10.1109/ICCV.2015.114
   Sun YL, 2020, VISUAL COMPUT, V36, P2407, DOI 10.1007/s00371-020-01892-8
   Tang Haotian, 2020, EUR C COMP VIS, P685, DOI DOI 10.1007/978-3-030-58604-1_41
   Tchapmi LP, 2017, INT CONF 3D VISION, P537, DOI 10.1109/3DV.2017.00067
   Thomas H, 2019, IEEE I CONF COMP VIS, P6420, DOI 10.1109/ICCV.2019.00651
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362
   Wu ZR, 2015, PROC CVPR IEEE, P1912, DOI 10.1109/CVPR.2015.7298801
   Xu JY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P16004, DOI 10.1109/ICCV48922.2021.01572
   Xu YF, 2018, LECT NOTES COMPUT SC, V11212, P90, DOI 10.1007/978-3-030-01237-3_6
   Yan X, 2020, PROC CVPR IEEE, P5588, DOI 10.1109/CVPR42600.2020.00563
   Yi L, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980238
   You HX, 2019, AAAI CONF ARTIF INTE, P9119
   You HX, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1310, DOI 10.1145/3240508.3240702
   Zhao HS, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P16239, DOI 10.1109/ICCV48922.2021.01595
NR 45
TC 0
Z9 0
U1 3
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2023 OCT 9
PY 2023
DI 10.1007/s00371-023-03107-2
EA OCT 2023
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA T8PY5
UT WOS:001080561500003
DA 2024-07-18
ER

PT J
AU Mohammed, A
   Samundiswary, P
AF Mohammed, Ajmal
   Samundiswary, P.
TI SecMISS: Secured Medical Image Secret Sharingmechanism for smart health
   applications
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Visual Secret Sharing (VSS); Security; Super-resolution; Medical images;
   Smart health
ID GRAY-SCALE; PRIVACY; SCHEME; ISSUES
AB Sustainable smart city initiatives significantly improve the living standards of citizens and bring significant changes in economic, environmental, and social well-being. Sustainable and remote medical services are vital attributes of modern smart health infrastructure. Smart remote health infrastructure uses interconnected devices to gather and exchange health data. In smart health infrastructure, massive quantities of health data are gathered as Personal Health Records (PHR). Clinical images form a major part of the accumulated PHR. As medical images are confidential and sensitive, secure storage and transmission are significant. Hence, different cryptographic techniques are used to enhance medical image security. The conventional cryptographic schemes are complex and generate a single cipher image corresponding to a plain image. The cipher images are stored in a single database. Such storage makes the cryptosystem weak against single-point attacks. This paper discusses a simple, keyless, and distributed secure storage mechanism for medical images called SecMISS. SecMISS uses Random Grid-based Visual Secret Sharing (RGVSS) along with super-resolution for secure encryption with distributed storage and better reconstruction. The security parameters such as SSIM (approximate to 0), correlation of adjacent pixels (approximate to 0), PSNR (<= 8 dB), and entropy (= 1) between share and initial image showthat SecMISS provides highest level of security. Similarly, different Human Visual System (HVS) parameters between the initial and reconstructed images show an improvement in the reconstruction. Experimental results show that SecMISS outperforms the existing technique in terms of security and reconstruction. Thereby SecMISS can be efficiently used for securing medical image in sustainable smart hospitals and health infrastructures.
C1 [Mohammed, Ajmal; Samundiswary, P.] Pondicherry Univ, Dept Elect Engn, Pondicherry, India.
C3 Pondicherry University
RP Mohammed, A (corresponding author), Pondicherry Univ, Dept Elect Engn, Pondicherry, India.
EM ajmalvm@pondiuni.ac.in; sam.dee@pondiuni.ac.in
RI Mohammed, Ajmal/JFK-9031-2023
OI Mohammed, Ajmal/0000-0001-9510-8310
CR Afandi T.M.K., 2021, Medical images compression and encryption using DCT, arithmetic encoding and chaos-based encryption
   Ahmad S, 2021, MULTIMED TOOLS APPL, V80, P32071, DOI 10.1007/s11042-021-11152-z
   Ajmal Mohammed V. M., 2022, 2022 Third International Conference on Intelligent Computing Instrumentation and Control Technologies (ICICICT), P887, DOI 10.1109/ICICICT54557.2022.9917608
   [Anonymous], 1994, WORKSH THEOR APPL CR
   Aouissaoui I, 2021, IET IMAGE PROCESS, V15, P2770, DOI 10.1049/ipr2.12261
   Camara C, 2015, J BIOMED INFORM, V55, P272, DOI 10.1016/j.jbi.2015.04.007
   Dagher GG, 2018, SUSTAIN CITIES SOC, V39, P283, DOI 10.1016/j.scs.2018.02.014
   Ding Y, 2023, VISUAL COMPUT, V39, P1517, DOI 10.1007/s00371-022-02426-0
   El-Shafai W, 2021, J AMB INTEL HUM COMP, V12, P9007, DOI 10.1007/s12652-020-02597-5
   Elad M, 1997, IEEE T IMAGE PROCESS, V6, P1646, DOI 10.1109/83.650118
   Fang PF, 2023, VISUAL COMPUT, V39, P1975, DOI 10.1007/s00371-022-02459-5
   Geetha BT, 2022, COMPUT INTEL NEUROSC, V2022, DOI 10.1155/2022/2243827
   Girma A., 2018, Information Technology-New Generations
   Hatzivasilis G, 2019, IEEE INT CONF DISTR, P457, DOI 10.1109/DCOSS.2019.00091
   Janani T, 2021, J INF SECUR APPL, V59, DOI 10.1016/j.jisa.2021.102832
   John S., 2023, Meas., Sensors, V25, P100676, DOI [10.1016/j.measen.2023.100676, DOI 10.1016/J.MEASEN.2023.100676]
   Kamal ST, 2021, IEEE ACCESS, V9, P37855, DOI 10.1109/ACCESS.2021.3063237
   Kiran P, 2022, MICROPROCESS MICROSY, V91, DOI 10.1016/j.micpro.2022.104546
   Kumar S, 2019, MED BIOL ENG COMPUT, V57, P2517, DOI 10.1007/s11517-019-02037-3
   Lin CH, 2021, IEEE ACCESS, V9, P118624, DOI 10.1109/ACCESS.2021.3107608
   Mamdouh Moustafa, 2020, Proceedings of the International Conference on Artificial Intelligence and Computer Vision (AICV2020). Advances in Intelligent Systems and Computing (AISC 1153), P721, DOI 10.1007/978-3-030-44289-7_67
   Mhala NC, 2021, VISUAL COMPUT, V37, P2097, DOI 10.1007/s00371-020-01972-9
   Mhala NC, 2019, SIGNAL PROCESS, V162, P253, DOI 10.1016/j.sigpro.2019.04.023
   Mhala NC, 2018, IET IMAGE PROCESS, V12, P422, DOI 10.1049/iet-ipr.2017.0759
   Mishra P, 2021, J AMB INTEL HUM COMP, DOI 10.1007/s12652-021-03410-7
   Muhammed A, 2021, MULTIMED TOOLS APPL, V80, P10255, DOI 10.1007/s11042-020-10095-1
   Mukhopadhyay S, 2021, MULTIMED TOOLS APPL, V80, P14495, DOI 10.1007/s11042-020-10424-4
   Murali P, 2023, VISUAL COMPUT, V39, P1057, DOI 10.1007/s00371-021-02384-z
   Nazir S, 2020, IEEE ACCESS, V8, P95714, DOI 10.1109/ACCESS.2020.2995572
   Panwar A, 2022, COMPUT INTEL NEUROSC, V2022, DOI 10.1155/2022/3045107
   Rathore MM, 2018, SUSTAIN CITIES SOC, V40, P600, DOI 10.1016/j.scs.2017.12.022
   Samaila MG, 2018, SECUR PRIVACY, V1, DOI 10.1002/spy2.20
   Sarosh P, 2021, SUSTAIN CITIES SOC, V74, DOI 10.1016/j.scs.2021.103129
   Sheng QX, 2024, VISUAL COMPUT, V40, P1643, DOI 10.1007/s00371-023-02876-0
   Sliwa J, 2018, INTELL DAT CENT SYST, P121, DOI 10.1016/B978-0-12-812130-6.00007-X
   Srividhya S, 2016, J VIS COMMUN IMAGE R, V38, P284, DOI 10.1016/j.jvcir.2016.03.012
   Thien CC, 2003, IEEE T CIRC SYST VID, V13, P1161, DOI 10.1109/TCSVT.2003.819176
   Tiwari A., 2023, Multimedia Tools and Applications, P1
   Wang X., 2022, Secur. Commun. Netw, V2022, P1, DOI [10.1155/2022/8288855, DOI 10.1155/2022/8288855]
   Yan XH, 2019, J INF SECUR APPL, V47, P208, DOI 10.1016/j.jisa.2019.05.008
   Yan XH, 2018, J REAL-TIME IMAGE PR, V14, P61, DOI 10.1007/s11554-015-0540-4
   Yangxiu Fang, 2021, Innovation in Medicine and Healthcare. Proceedings of 9th KES-InMed 2021. Smart Innovation, Systems and Technologies (SIST 242), P61, DOI 10.1007/978-981-16-3013-2_6
   Yin S., 2020, Int. J. Netw. Secur, V22, P419
   Yousef R, 2022, MULTIMEDIA SYST, V28, P881, DOI 10.1007/s00530-021-00884-5
NR 44
TC 0
Z9 0
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2023 SEP 27
PY 2023
DI 10.1007/s00371-023-03080-w
EA SEP 2023
PG 21
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA S8XB5
UT WOS:001073931700002
DA 2024-07-18
ER

PT J
AU Chen, ZY
   Huang, GH
   Wang, Y
   Qiu, JH
   Yang, F
   Yu, ZW
   Pun, CM
   Ling, WK
AF Chen, Ziyang
   Huang, Guoheng
   Wang, Ying
   Qiu, Junhao
   Yang, Fan
   Yu, Zhiwen
   Pun, Chi-Man
   Ling, Wing-Kuen
TI Bi-deformation-UNet: recombination of differential channels for printed
   surface defect detection
SO VISUAL COMPUTER
LA English
DT Article
DE Subtle defects; Object detection; Edge detection; Metric learning;
   Class-imbalance
ID SALIENCY DETECTION; MULTI-FEATURE; CLASSIFICATION
AB Deep learning is frequently recommended for standard defect detection because of its ace accuracy and robustness. Unfortunately, current deep learning methods exist several challenges in detecting printed surface defects with multi-scale textures. Firstly, the existing methods only highlight the texture of defects, but concealed the color information of defects. Secondly, since the subtle defects of printed contained with weak semantic, it is difficult for current multi-scale network to locate the defects. Finally, current metric methods cannot measure the similarity between each of defect under class-imbalanced precisely. Therefore, Bi-Deformation-UNet (Bi-DUNet) is designed for automatic printed surface defect detection. In Bi-DUNet, the template-defect image pairs are first enhanced by our proposed pre-processing module Recombination of the Differential Channels. This module can highlight the texture and maintain the color information simultaneously. Then, the preprocessed image pairs are fed into the Dual-fusion Module (DM) and generated the output features with edge information and contextual information. The DM consists of two branches: the Template Branch and the Defect Branch. The two branches are identical in structure and Multi-channel Edge Attention Module. Besides, an Automatic Dual-margin Metric Loss is proposed to alleviate the situation of class-imbalance when measuring similarity of output features. Moreover, a 2020 Assembly Line Defective Product dataset (ALDP2020) is proposed, which contains 4000 images with different environment styles. Finally, our proposed Bi-DUNet achieves 3.97% higher than the state-of-the-arts in ALDP2020 in mAP50. The code is available at https://github.com/MRziyang/DefectDetection.git.
C1 [Chen, Ziyang; Huang, Guoheng; Wang, Ying] Guangdong Univ Technol, Sch Comp, Guangzhou 510006, Peoples R China.
   [Qiu, Junhao] Guangdong Univ Technol, Sch Electromech Engn, Guangzhou 510006, Peoples R China.
   [Yang, Fan] Guangdong Univ Technol, Sch Phys & Optoelect Engn, Guangzhou 510006, Peoples R China.
   [Yu, Zhiwen] Univ Macau, Dept Comp & Informat Sci, Macau 999078, Peoples R China.
   [Pun, Chi-Man] South China Univ Technol, Sch Comp Sci & Engn, Guangzhou 510006, Peoples R China.
   [Ling, Wing-Kuen] Guangdong Univ Technol, Sch Informat Engn, Guangzhou 510006, Peoples R China.
C3 Guangdong University of Technology; Guangdong University of Technology;
   Guangdong University of Technology; University of Macau; South China
   University of Technology; Guangdong University of Technology
RP Huang, GH (corresponding author), Guangdong Univ Technol, Sch Comp, Guangzhou 510006, Peoples R China.
EM kevinwong@gdut.edu.cn
RI Pun, Chi Man/GRJ-3703-2022
OI Huang, Guoheng/0000-0002-3640-3229
FU R&D projects in key areas of Guangdong Province [2019B010153002];
   Science and technology research in key areas in Foshan [2020001006832];
   Guangdong Key Areas R&D Program Project [2018B010109007]; Guangdong
   Provincial Key Laboratory of Cyber-Physical System [2020B1212060069];
   National Natural Science Foundation of China Guangdong Joint Fund
   [U1801263, U2001201]; Guangzhou R&D Programme in Key Areas of Science
   and Technology Projects [202007040006]; Program of Marine Economy
   Development (Six Marine Industries) Special Foundation of Department of
   Natural Resources of Guangdong Province [GDNGC [2020]056]
FX This work was supported in part by the R&D projects in key areas of
   Guangdong Province under Grant 2019B010153002, the Science and
   technology research in key areas in Foshan under Grant 2020001006832,
   the Guangdong Key Areas R&D Program Project under Grant 2018B010109007,
   the Guangdong Provincial Key Laboratory of Cyber-Physical System under
   Grant 2020B1212060069, the National Natural Science Foundation of China
   Guangdong Joint Fund under Grant U1801263 and U2001201, the Guangzhou
   R&D Programme in Key Areas of Science and Technology Projects under
   Grant 202007040006, the Program of Marine Economy Development (Six
   Marine Industries) Special Foundation of Department of Natural Resources
   of Guangdong Province under Grant GDNGC [2020]056.
CR Amiri SA., 2012, International Journal of Computer Applications, V12, P38
   Bai XL, 2014, IEEE T IND INFORM, V10, P2135, DOI 10.1109/TII.2014.2359416
   Bin Li, 2018, IEEE Signal Processing Letters, V25, P650, DOI 10.1109/LSP.2018.2816569
   Borwankar R., 2018, IEEE T INSTRUM MEASU, P1
   Cheng X, 2021, IEEE T INSTRUM MEAS, V70, DOI 10.1109/TIM.2020.3040485
   Cui LS, 2021, IEEE T INSTRUM MEAS, V70, DOI 10.1109/TIM.2021.3056744
   Du XY, 2020, IEEE ACCESS, V8, P152161, DOI 10.1109/ACCESS.2020.3017691
   Feng X., APPL INTELL, V51, P1
   Haddad B, 2016, IEEE IMAGE PROC, P754, DOI 10.1109/ICIP.2016.7532458
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He Y, 2020, IEEE T INSTRUM MEAS, V69, P1493, DOI 10.1109/TIM.2019.2915404
   Hu GH, 2015, OPTIK, V126, P1331, DOI 10.1016/j.ijleo.2015.04.017
   Hu WH, 2022, VISUAL COMPUT, V38, P3731, DOI 10.1007/s00371-021-02210-6
   Jia L, 2020, INFORM SCIENCES, V512, P964, DOI 10.1016/j.ins.2019.10.032
   Jing JF, 2017, COLOR TECHNOL, V133, P26, DOI 10.1111/cote.12239
   Kim J, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21154968
   Li X, 2016, IEEE SENSOR
   Lin H, 2019, J INTELL MANUF, V30, P2525, DOI 10.1007/s10845-018-1415-x
   Lin TY, 2020, IEEE T PATTERN ANAL, V42, P318, DOI 10.1109/TPAMI.2018.2858826
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Lin W.Y., 2018, Advances in physical ergonomics and human factors
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Liu ZF, 2018, PROCEEDINGS OF THE 2ND INTERNATIONAL CONFERENCE ON GRAPHICS AND SIGNAL PROCESSING (ICGSP 2018), P74, DOI 10.1145/3282286.3282300
   Liu ZF, 2017, PROCEEDINGS 2017 4TH IAPR ASIAN CONFERENCE ON PATTERN RECOGNITION (ACPR), P465, DOI 10.1109/ACPR.2017.34
   Lu SJ, 2019, APPL INTELL, V49, P2301, DOI 10.1007/s10489-018-1377-x
   Luan CH, 2020, IEEE IMAGE PROC, P778, DOI 10.1109/ICIP40778.2020.9191128
   Park T, 2020, Arxiv, DOI arXiv:2007.00653
   Protopapadakis E, 2019, APPL INTELL, V49, P2793, DOI 10.1007/s10489-018-01396-y
   Qiu YH, 2020, IEEE ACCESS, V8, P190663, DOI 10.1109/ACCESS.2020.3032108
   Redmon J., 2018, arXiv, DOI DOI 10.48550/ARXIV.1804.02767
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Shankar K, 2020, IEEE ACCESS, V8, P118164, DOI 10.1109/ACCESS.2020.3005152
   Shuanghui Chen, 2021, 2021 International Conference on Computer Communication and Artificial Intelligence (CCAI), P36, DOI 10.1109/CCAI50917.2021.9447538
   Sohn K, 2016, ADV NEUR IN, V29
   Sun SJ, 2022, APPL INTELL, V52, P5739, DOI 10.1007/s10489-021-02697-5
   Thambusamy V., 2018, IntJPureApplMath, V118, P3681, DOI DOI 10.3390/CANCERS14092132
   Tian Z, 2019, IEEE I CONF COMP VIS, P9626, DOI 10.1109/ICCV.2019.00972
   Wakaf Z., 2016, J KING SAUD UNIV-COM, V30
   Wang B, 2020, VISUAL COMPUT, V36, P1897, DOI 10.1007/s00371-019-01779-3
   Wang H., 2013, Opt. Photonics J, V3, P720, DOI DOI 10.4236/OPJ.2013.32B025
   Wang MZ, 2021, AUTOMAT CONSTR, V121, DOI 10.1016/j.autcon.2020.103438
   Wang N, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2805
   Wang WG, 2018, IEEE T IMAGE PROCESS, V27, P2368, DOI 10.1109/TIP.2017.2787612
   Wei B, 2020, NEURAL NETWORKS, V130, P100, DOI 10.1016/j.neunet.2020.06.019
   Win M, 2015, IEEE T IND INFORM, V11, P642, DOI 10.1109/TII.2015.2417676
   Wu SL, 2019, MULTIMED TOOLS APPL, V78, P34627, DOI 10.1007/s11042-019-08042-w
   Xie Q, 2019, IEEE T AUTOM SCI ENG, V16, P1836, DOI 10.1109/TASE.2019.2900170
   Yuan XC, 2015, APPL SURF SCI, V349, P472, DOI 10.1016/j.apsusc.2015.05.033
   Zhang WJ, 2016, VISUAL COMPUT, V32, P275, DOI 10.1007/s00371-015-1065-3
   Zhao D., 2021, APPL INTELL, P1
   Zhiqiang Feng, 2021, Proceedings of the 2021 IEEE 10th Data Driven Control and Learning Systems Conference (DDCLS), P979, DOI 10.1109/DDCLS52934.2021.9455519
NR 52
TC 2
Z9 2
U1 15
U2 20
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2023
VL 39
IS 9
BP 3995
EP 4013
DI 10.1007/s00371-022-02554-7
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA Q8PV6
UT WOS:001060095400012
DA 2024-07-18
ER

PT J
AU Hao, WL
   Wang, P
   Ni, C
   Zhang, GY
   Huangfu, WJ
AF Hao, Weilong
   Wang, Peng
   Ni, Cui
   Zhang, Guangyuan
   Huangfu, Wenjun
TI SuperGlue-based accurate feature matching via outlier filtering
SO VISUAL COMPUTER
LA English
DT Article
DE Feature matching; Outlier filtering; Superpixel segmentation;
   Exponential moving average
AB The feature matching algorithm based on deep learning has achieved superior performance compared to traditional algorithms in terms of both matching quantity and accuracy, but there are still some high-error matching results in complex scenes, which adversely affects the subsequent work. Based on SuperGlue, we propose an accurate feature matching algorithm via outlier filtering. Firstly, DBSCAN real-time superpixel segmentation (RTSS-DBSCAN) is used to divide the image into regions, and then the outlier filtering module is designed according to the local similarity principle of feature matching. On the premise of not affecting the correct matching results, the matching results with high errors will be filtered to improve the matching accuracy. Meanwhile, due to the lag of traditional Exponential Moving Average algorithm (EMA), an adaptive EMA is designed and integrated into the SuperGlue training process to further improve the training speed and matching accuracy. We evaluate the overall performance of the matching method using the AUC of pose error at the thresholds (5 & DEG;, 10 & DEG;, 20 & DEG;), a common evaluation metric, to provide a more detailed and intuitive evaluation of the matching effectiveness using precision and recall. The experimental results show that the method in this paper can effectively filter the matching results with large errors and has high accuracy and robustness. The AUC of pose error at thresholds (5 & DEG;, 10 & DEG;, 20 & DEG;) reaches 36.53, 56.23, and 73.68, and the precision and recall reach 80.07 and 91.52, respectively, which have better matching results compared with other algorithms.
C1 [Hao, Weilong; Wang, Peng; Ni, Cui; Zhang, Guangyuan; Huangfu, Wenjun] Shan Dong Jiao Tong Univ, Sch Informat Sci & Elect Engn, Jinan 250357, Peoples R China.
   [Wang, Peng] Shandong Acad Sci, Inst Automat, Jinan 250013, Peoples R China.
C3 Qilu University of Technology
RP Wang, P (corresponding author), Shan Dong Jiao Tong Univ, Sch Informat Sci & Elect Engn, Jinan 250357, Peoples R China.; Wang, P (corresponding author), Shandong Acad Sci, Inst Automat, Jinan 250013, Peoples R China.
EM 2174698790@qq.com; knightwp@126.com; emilync@126.com; xdzhanggy@163.com;
   huangfuwenjun1998@163.com
FU China Postdoctoral Science Foundation [2021M702030]; Shandong
   ProvincialTransportation Science and Technology Project [2021B120]
FX This work was partially supported by China Postdoctoral Science
   Foundation (Grant No. 2021M702030) and Shandong ProvincialTransportation
   Science and Technology Project (Grant No. 2021B120).
CR Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   Balntas V., 2016, Bmvc, DOI DOI 10.5244/C.30.119
   Bian JW, 2017, PROC CVPR IEEE, P2828, DOI 10.1109/CVPR.2017.302
   Cai YC, 2023, VISUAL COMPUT, V39, P2555, DOI 10.1007/s00371-022-02478-2
   Campos Carlos, 2021, IEEE Transactions on Robotics, V37, P1874, DOI 10.1109/TRO.2021.3075644
   Carrasco M, 2019, CURR OPIN PSYCHOL, V29, P56, DOI 10.1016/j.copsyc.2018.10.010
   Chen C., 2020, ARXIV, DOI DOI 10.48550/ARXIV.2006.12567
   Chen HK, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6281, DOI 10.1109/ICCV48922.2021.00624
   Chizat L., 2020, Advances in Neural Information Processing Systems, V33, P2257
   Corso MP, 2021, COMPUTERS, V10, DOI 10.3390/computers10090112
   Demarche C, 2020, J ECOLE POLYTECH-MAT, V7, P831, DOI 10.5802/jep.129
   DeTone D, 2018, IEEE COMPUT SOC CONF, P337, DOI 10.1109/CVPRW.2018.00060
   Dingsheng Deng, 2020, 2020 7th International Forum on Electrical Engineering and Automation (IFEEA), P949, DOI 10.1109/IFEEA51475.2020.00199
   Fu YP, 2020, VISUAL COMPUT, V36, P2215, DOI 10.1007/s00371-020-01899-1
   Gadipudi N, 2023, VISUAL COMPUT, V39, P5897, DOI 10.1007/s00371-022-02702-z
   Georgiou T, 2020, INT J MULTIMED INF R, V9, P135, DOI 10.1007/s13735-019-00183-w
   He M, 2020, VISUAL COMPUT, V36, P1053, DOI 10.1007/s00371-019-01714-6
   Li ZQ, 2015, PROC CVPR IEEE, P1356, DOI 10.1109/CVPR.2015.7298741
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Luo HS, 2021, KNOWL-BASED SYST, V219, DOI 10.1016/j.knosys.2021.106904
   Ma JY, 2021, INT J COMPUT VISION, V129, DOI 10.1007/s11263-020-01359-2
   Ma SH, 2021, INFORM SCIENCES, V562, P452, DOI 10.1016/j.ins.2021.03.023
   Ming-Yu Liu, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2097, DOI 10.1109/CVPR.2011.5995323
   Ngo D, 2020, REMOTE SENS-BASEL, V12, DOI 10.3390/rs12142233
   Rahman Miftahur, 2019, 2019 IEEE 4th International Conference on Image, Vision and Computing (ICIVC), P463, DOI 10.1109/ICIVC47709.2019.8981025
   Sarlin PE, 2020, PROC CVPR IEEE, P4937, DOI 10.1109/CVPR42600.2020.00499
   Shen JB, 2016, IEEE T IMAGE PROCESS, V25, P5933, DOI 10.1109/TIP.2016.2616302
   Shen JB, 2014, IEEE T IMAGE PROCESS, V23, P1451, DOI 10.1109/TIP.2014.2302892
   Shi WJ, 2020, PROC CVPR IEEE, P1708, DOI 10.1109/CVPR42600.2020.00178
   Shi Y, 2022, PROC CVPR IEEE, P12507, DOI 10.1109/CVPR52688.2022.01219
   Sinaga KP, 2020, IEEE ACCESS, V8, P80716, DOI 10.1109/ACCESS.2020.2988796
   Toft C, 2022, IEEE T PATTERN ANAL, V44, P2074, DOI 10.1109/TPAMI.2020.3032010
   Van den Bergh M, 2015, INT J COMPUT VISION, V111, P298, DOI 10.1007/s11263-014-0744-2
   Vaswani A, 2017, ADV NEUR IN, V30
   Viniavskyi O., 2022, ARXIV, DOI DOI 10.48550/ARXIV.2204.08870
   Xu Q, 2020, IEEE J BIOMED HEALTH, V24, P2481, DOI 10.1109/JBHI.2020.2986376
   Zhang H, 2019, PR MACH LEARN RES, V97
   Zhang YH, 2011, IEEE I CONF COMP VIS, P1387, DOI 10.1109/ICCV.2011.6126393
NR 38
TC 1
Z9 1
U1 3
U2 8
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2024
VL 40
IS 5
BP 3137
EP 3150
DI 10.1007/s00371-023-03015-5
EA JUL 2023
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OC3D9
UT WOS:001035720700005
DA 2024-07-18
ER

PT J
AU Jiang, LX
   Li, XJ
   Wang, YD
AF Jiang, Lixi
   Li, Xujie
   Wang, Yandan
TI Iterative unsupervised deep bilateral texture filtering
SO VISUAL COMPUTER
LA English
DT Article
DE Bilateral texture filtering; Image smoothing; Fully convolution neural
   network; Unsupervised learning
AB Texture filtering attempts to retain salient structures and remove insignificant textures. In this paper, we propose a highly effective iterative unsupervised deep bilateral texture filtering neural network for texture smoothing. The bilateral texture loss function is introduced to train the model without the ground truth smoothing images for guidance. The proposed model inherits well-known advantages of the bilateral texture filter to capture the texture information effectively. The model is trained solely using the training data, then the predicted outputs are generated iteratively through multiple forward passes. Extensive experiments demonstrate that our proposed iterative unsupervised deep bilateral texture filtering neural network outperforms existing methods in effectively removing textures while preserving the main structures of the image. The results showcase the superior performance of our approach and its ability to achieve high-quality texture smoothing without sacrificing important image features.
C1 [Jiang, Lixi] Zhejiang Coll Secur Technol, Wenzhou 325016, Peoples R China.
   [Li, Xujie; Wang, Yandan] Wenzhou Univ, Intelligent Informat Syst Inst, Wenzhou 325035, Peoples R China.
C3 Wenzhou University
RP Jiang, LX (corresponding author), Zhejiang Coll Secur Technol, Wenzhou 325016, Peoples R China.
EM jianglixi125@sina.com
FU Zhejiang Provincial Natural Science Foundation of China [LY18 F020022,
   LQ17F020002]
FX AcknowledgementsThis work was supported by the Zhejiang Provincial
   Natural Science Foundation of China (Grant Nos. LY18 F020022 and
   LQ17F020002).
CR Avidan S, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239461
   Cho H, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601188
   Fan QN, 2021, IEEE T PATTERN ANAL, V43, P33, DOI 10.1109/TPAMI.2019.2925793
   Fan QN, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275081
   Fang XN, 2019, COMPUT GRAPH FORUM, V38, P181, DOI 10.1111/cgf.13827
   Farbman Z, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360666
   Gavaskar RG, 2019, IEEE T IMAGE PROCESS, V28, P779, DOI 10.1109/TIP.2018.2871597
   He KM, 2013, IEEE T PATTERN ANAL, V35, P1397, DOI 10.1109/TPAMI.2012.213
   Jeon J, 2016, COMPUT GRAPH FORUM, V35, P77, DOI 10.1111/cgf.13005
   Li YJ, 2019, IEEE T PATTERN ANAL, V41, P1909, DOI 10.1109/TPAMI.2018.2890623
   Li YJ, 2016, LECT NOTES COMPUT SC, V9908, P154, DOI 10.1007/978-3-319-46493-0_10
   Lin TH, 2016, COMPUT GRAPH FORUM, V35, P57, DOI 10.1111/cgf.13003
   Liu QG, 2016, MULTIMED TOOLS APPL, V75, P7909, DOI 10.1007/s11042-015-2709-z
   Liu W, 2022, IEEE T PATTERN ANAL, V44, P6631, DOI 10.1109/TPAMI.2021.3097891
   Liu W, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3388887
   Liu W, 2017, IEEE I CONF COMP VIS, pCP32, DOI 10.1109/ICCV.2017.624
   Lu KY, 2018, LECT NOTES COMPUT SC, V11208, P229, DOI 10.1007/978-3-030-01225-0_14
   Min DB, 2014, IEEE T IMAGE PROCESS, V23, P5638, DOI 10.1109/TIP.2014.2366600
   Paszke A, 2019, ADV NEUR IN, V32
   Riya, 2022, DIGIT SIGNAL PROCESS, V123, DOI 10.1016/j.dsp.2022.103386
   Ruhela R, 2023, VISUAL COMPUT, V39, P2035, DOI 10.1007/s00371-022-02462-w
   Song CF, 2018, J VISUAL LANG COMPUT, V45, P17, DOI 10.1016/j.jvlc.2018.02.002
   Wang C., 2022, VIS COMP, P1
   Xu J, 2021, IEEE T MULTIMEDIA, V23, P4065, DOI 10.1109/TMM.2020.3037535
   Xu L, 2015, PR MACH LEARN RES, V37, P1669
   Xu L, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024208
   Xu PP, 2018, IEEE T IMAGE PROCESS, V27, P3621, DOI 10.1109/TIP.2018.2820427
   Yu F., 2017, PROC CVPR IEEE, P472, DOI [DOI 10.1109/CVPR.2017.75, 10.1109/CVPR.2017.75]
   Zhang FH, 2015, IEEE I CONF COMP VIS, P361, DOI 10.1109/ICCV.2015.49
   Zhang Q, 2014, LECT NOTES COMPUT SC, V8691, P815, DOI 10.1007/978-3-319-10578-9_53
   Zhang ZY, 2021, SIGNAL PROCESS-IMAGE, V96, DOI 10.1016/j.image.2021.116308
   Zhao HL, 2018, VISUAL COMPUT, V34, P83, DOI 10.1007/s00371-016-1315-z
   Zhou F, 2020, IEEE T IMAGE PROCESS, V29, P3458, DOI 10.1109/TIP.2019.2961232
   Zhou PC, 2023, VISUAL COMPUT, V39, P1533, DOI 10.1007/s00371-022-02427-z
   Zhu D., 2023, VISUAL COMPUT
   Zhu FD, 2019, IEEE T IMAGE PROCESS, V28, P3556, DOI 10.1109/TIP.2019.2908778
   Zhu LF, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366146
NR 37
TC 1
Z9 1
U1 5
U2 11
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2024
VL 40
IS 5
BP 3055
EP 3067
DI 10.1007/s00371-023-03010-w
EA JUL 2023
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OC3D9
UT WOS:001034268300001
DA 2024-07-18
ER

PT J
AU Chen, XD
   He, R
   Mao, XY
AF Chen, Xiao-Diao
   He, Rui
   Mao, Xiaoyang
TI SLIM: A transparent structurized self-learning interpolation method for
   super-resolution images
SO VISUAL COMPUTER
LA English
DT Article
DE Image interpolation; Self-learning; Classification; Structure features;
   Decision tree
ID NETWORK
AB Image super-resolution (SR) is a classic problem of image processing. This paper proposes a self-learning interpolation method (SLIM) based on a single image by combining grid feature mapping with binary decision tree, which is not only transparent as the interpolation-based methods, but also achieves comparable performance as the learning-based methods. Firstly, it downsamples the given image ILR to obtain its low-low-resolution image I-LLR, which is used to obtain sample data for the self-learning interpolation algorithm for enlarging I-LLR to get I-LR. Secondly, it provides a structural feature classification method to divide all of the samples into several groups, such that each class of I-LLR is mapped to a matrix of coefficients for calculating the values of the pixels of I-LR. The image I-LR is approximated by executing the decision tree to refine the corresponding mapping matrix. Finally, the resulting high-resolution image I-HR is obtained from the given image I-LR by using the mapping matrixes. Experimental results show that SLIM achieves more smooth edges and better details on subjective vision than prevailing SR methods, and it is a transparent one but achieves comparable performances on PSNR and SSIM with the learning-based methods, while it outperforms the interpolation-based methods. It means that SLIM is both transparent and efficient and has much better subjective vision than other SR methods.
C1 [Chen, Xiao-Diao; He, Rui] Hangzhou Dianzi Univ, Hangzhou, Peoples R China.
   [Mao, Xiaoyang] Univ Yamanashi, Dept Comp Sci & Engn, Kofu, Japan.
C3 Hangzhou Dianzi University; University of Yamanashi
RP He, R (corresponding author), Hangzhou Dianzi Univ, Hangzhou, Peoples R China.
EM xiaodiao@hdu.edu.cn; 924559627@qq.com; mao@yamanashi.ac.jp
RI Mao, Xiaoyang/AAG-1294-2020
OI Mao, Xiaoyang/0000-0001-5010-6952
FU National Natural Science Foundation of China [61972120]
FX AcknowledgementsThe work was supported by the National Natural Science
   Foundation of China (61972120).
CR Amaranageswarao G, 2022, VISUAL COMPUT, V38, P31, DOI 10.1007/s00371-020-01998-z
   Bao QQ, 2022, IEEE T IMAGE PROCESS, V31, P6455, DOI 10.1109/TIP.2022.3212311
   Chen R, 2022, PATTERN RECOGN, V122, DOI 10.1016/j.patcog.2021.108349
   Chen Y., 2023, Int. J. Mach. Learn. Cybern., P1
   Chen Y., 2023, Multimedia Tools and Applications, P1
   Chen YT, 2023, J VIS COMMUN IMAGE R, V91, DOI 10.1016/j.jvcir.2023.103776
   Chen YT, 2024, VISUAL COMPUT, V40, P489, DOI 10.1007/s00371-023-02795-0
   Dong C, 2014, LECT NOTES COMPUT SC, V8692, P184, DOI 10.1007/978-3-319-10593-2_13
   Dong WS, 2013, IEEE T IMAGE PROCESS, V22, P1382, DOI 10.1109/TIP.2012.2231086
   Freedman G, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1944846.1944852
   Giachetti A, 2011, IEEE T IMAGE PROCESS, V20, P2760, DOI 10.1109/TIP.2011.2136352
   Glasner D, 2009, IEEE I CONF COMP VIS, P349, DOI 10.1109/ICCV.2009.5459271
   Goodfellow I, 2014, ADV NEURAL INFORM PR, P2672
   Gu JJ, 2019, PROC CVPR IEEE, P1604, DOI 10.1109/CVPR.2019.00170
   HOU HS, 1978, IEEE T ACOUST SPEECH, V26, P508
   Huang JB, 2015, PROC CVPR IEEE, P5197, DOI 10.1109/CVPR.2015.7299156
   Huang JJ, 2015, INT CONF ACOUST SPEE, P1221, DOI 10.1109/ICASSP.2015.7178164
   Jing GM, 2014, IEEE IMAGE PROC, P1822, DOI 10.1109/ICIP.2014.7025365
   KEYS RG, 1981, IEEE T ACOUST SPEECH, V29, P1153, DOI 10.1109/TASSP.1981.1163711
   Kim KI, 2008, LECT NOTES COMPUT SC, V5096, P456
   Kim KI, 2010, IEEE T PATTERN ANAL, V32, P1127, DOI 10.1109/TPAMI.2010.25
   Li K, 2016, PATTERN RECOGN, V51, P59, DOI 10.1016/j.patcog.2015.08.008
   Li X, 2001, IEEE T IMAGE PROCESS, V10, P1521, DOI 10.1109/83.951537
   Li YC, 2014, PATTERN RECOGN, V47, P1261, DOI 10.1016/j.patcog.2013.09.012
   Li Z, 2019, PROC CVPR IEEE, P3862, DOI 10.1109/CVPR.2019.00399
   Liang JY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4076, DOI 10.1109/ICCV48922.2021.00406
   Liu AR, 2023, IEEE T PATTERN ANAL, V45, P5461, DOI 10.1109/TPAMI.2022.3203009
   Lu X., 2023, VIS COMP, P1
   Luo ZW, 2022, PROC CVPR IEEE, P17621, DOI 10.1109/CVPR52688.2022.01712
   Ma JQ, 2023, IEEE T IMAGE PROCESS, V32, P1341, DOI 10.1109/TIP.2023.3237002
   Ma JQ, 2022, PROC CVPR IEEE, P5901, DOI 10.1109/CVPR52688.2022.00582
   Salvador J, 2015, IEEE I CONF COMP VIS, P325, DOI 10.1109/ICCV.2015.45
   Shi WL, 2021, VISUAL COMPUT, V37, P1569, DOI 10.1007/s00371-020-01903-8
   Shocher A, 2018, PROC CVPR IEEE, P3118, DOI 10.1109/CVPR.2018.00329
   Sun L., 2022, P ANN C NEURAL INFOR, P1
   Sun W, 2022, PATTERN RECOGN, V126, DOI 10.1016/j.patcog.2022.108577
   Tang Y, 2013, J VIS COMMUN IMAGE R, V24, P148, DOI 10.1016/j.jvcir.2012.02.003
   Timofte R, 2015, LECT NOTES COMPUT SC, V9006, P111, DOI 10.1007/978-3-319-16817-3_8
   Timofte R, 2013, IEEE I CONF COMP VIS, P1920, DOI 10.1109/ICCV.2013.241
   Wang L, 2022, PATTERN RECOGN, V121, DOI 10.1016/j.patcog.2021.108206
   Wang ZH, 2021, IEEE T PATTERN ANAL, V43, P3365, DOI 10.1109/TPAMI.2020.2982166
   Wu W, 2022, VISUAL COMPUT, V38, P1665, DOI 10.1007/s00371-021-02095-5
   Wu W, 2022, VISUAL COMPUT, V38, P1677, DOI 10.1007/s00371-021-02096-4
   Xia RL, 2022, J KING SAUD UNIV-COM, V34, P6008, DOI 10.1016/j.jksuci.2022.02.004
   Xiong ZW, 2010, IEEE T IMAGE PROCESS, V19, P2017, DOI 10.1109/TIP.2010.2045707
   Yang CY, 2013, IEEE I CONF COMP VIS, P561, DOI 10.1109/ICCV.2013.75
   Yang JC, 2010, IEEE T IMAGE PROCESS, V19, P2861, DOI 10.1109/TIP.2010.2050625
   Yang Y, 2021, PATTERN RECOGN, V112, DOI 10.1016/j.patcog.2020.107798
   Yao XX, 2019, IMAGE VISION COMPUT, V82, P39, DOI 10.1016/j.imavis.2019.02.002
   Yuantao C., 2023, J KING SAUD U COMPUT, V35
   Zhang L, 2006, IEEE T IMAGE PROCESS, V15, P2226, DOI 10.1109/TIP.2006.877407
   Zhang YL, 2021, IEEE T PATTERN ANAL, V43, P2480, DOI 10.1109/TPAMI.2020.2968521
   Zhang YF, 2018, IEEE T IMAGE PROCESS, V27, P3782, DOI 10.1109/TIP.2018.2826139
   Zhao LJ, 2019, PATTERN RECOGN, V88, P356, DOI 10.1016/j.patcog.2018.11.028
   Zhou D, 2012, IET IMAGE PROCESS, V6, P627, DOI 10.1049/iet-ipr.2011.0534
NR 55
TC 0
Z9 0
U1 2
U2 11
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2024
VL 40
IS 4
BP 2857
EP 2872
DI 10.1007/s00371-023-02990-z
EA JUL 2023
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA MZ2U3
UT WOS:001029107600001
DA 2024-07-18
ER

PT J
AU Chen, YK
   Lu, YF
   Zhang, XH
   Xie, NE
AF Chen, Yuankang
   Lu, Yifan
   Zhang, Xiaohua
   Xie, Nine
TI Interactive neural cascade denoising for 1-spp Monte Carlo images
SO VISUAL COMPUTER
LA English
DT Article
DE Rendering; Monte Carlo denoising; Neural networks; Ray tracing
AB Monte Carlo (MC) path tracing is known for its high fidelity and heavy computational cost.With the development of neural networks, the kernel-based post-processing method has succeeded in denoising noisy images under low sampling rates, but the complex network structure impedes its deployment in interactive applications. In this paper, we propose a lightweight cascaded network which progressively denoises 1-spp Monte Carlo images through both pixel and kernel prediction methods. A primary denoised image is generated by the pixel prediction network at the first stage, which is then fed to the kernel prediction network to obtain multi-resolution kernels. In addition, to take full advantage of the auxiliary buffers, we introduce a bilateral method during image reconstruction. Experimental results show that our approach achieves state-of-the-art denoising qualities for 1-spp images at an interactive frame speed.
C1 [Chen, Yuankang; Lu, Yifan; Xie, Nine] Univ Elect Sci & Technol China UESTC, Ctr Future Media, Dept Comp Sci & Engn, Chengdu 611731, Peoples R China.
   [Zhang, Xiaohua] Hiroshima Inst Technol, Hiroshima, Japan.
C3 University of Electronic Science & Technology of China; Hiroshima
   Institute of Technology
RP Xie, NE (corresponding author), Univ Elect Sci & Technol China UESTC, Ctr Future Media, Dept Comp Sci & Engn, Chengdu 611731, Peoples R China.
EM ykchen2022@outlook.com; luyifan0821@gmail.com;
   zhxh@cc.it-hiroshima.ac.jp; seanxiening@gmail.com
FU Chengdu Science and Technology Project [2019-YF08-00285-GX]; Development
   and Application Demonstrations of Digitalized Governance of Local
   Society for Future Cities Research Program [2021-JB00-00033-GX];
   National Natural Science Foundation of China [61976156]; Sichuan Science
   and Technology Program [2022NSFSC0640]; Medical Science and Technology
   Project of Sichuan Health Committee [21PJ119]
FX This work is part of the research supported by Chengdu Science and
   Technology Project (2019-YF08-00285-GX), Development and Application
   Demonstrations of Digitalized Governance of Local Society for Future
   Cities Research Program (2021-JB00-00033-GX), the National Natural
   Science Foundation of China under Grant NO. 61976156, Sichuan Science
   and Technology Program(2022NSFSC0640) and Medical Science and Technology
   Project of Sichuan Health Committee (21PJ119).
CR Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265
   Back J, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417847
   Bako S, 2017, ACM T GRAPHIC, V36, DOI [10.1145/3072959.3073703, 10.1145/3072959.3073708]
   Bitterli B., 2016, RENDERING RESOURCES
   Boski M, 2017, 2017 10TH INTERNATIONAL WORKSHOP ON MULTIDIMENSIONAL (ND) SYSTEMS (NDS)
   Chaitanya CRA, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073601
   Fan HM, 2021, COMPUT GRAPH FORUM, V40, P15, DOI 10.1111/cgf.14338
   Gharbi M, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3322954
   Guo J, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3478513.3480531
   Hasselgren J, 2020, COMPUT GRAPH FORUM, V39, P147, DOI 10.1111/cgf.13919
   Huo Y, 2021, COMPUT VIS MEDIA, V7, P169, DOI 10.1007/s41095-021-0209-9
   Isik M, 2021, ACM T GRAPHIC, V40, DOI [10.1145/3476576.3476580, 10.1145/3450626.3459793]
   Kalantari NK, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766977
   Kettunen M, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3323038
   Koskela M, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3269978
   Kuznetsov A, 2018, COMPUT GRAPH FORUM, V37, P35, DOI 10.1111/cgf.13473
   Lin WH, 2021, COMPUT GRAPH FORUM, V40, P369, DOI 10.1111/cgf.14194
   Lu Y., 2020, SIGGRAPH ASIA 2020 T, P1, DOI DOI 10.1145/3410700
   Lu Y., 2021, VISUAL COMPUT
   Meng X., 2020, PROC EGSR, P13
   Müller T, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459812
   Munkberg J, 2020, COMPUT GRAPH FORUM, V39, P1, DOI 10.1111/cgf.14049
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Schied C, 2018, P ACM COMPUT GRAPH, V1, DOI 10.1145/3233301
   Schied C, 2017, HPG '17: PROCEEDINGS OF HIGH PERFORMANCE GRAPHICS, DOI 10.1145/3105762.3105770
   Thomas MM, 2022, P ACM COMPUT GRAPH, V5, DOI 10.1145/3543870
   Vogels T, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201388
   Xiao L, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392376
   Xu B, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356547
   Yu JQ, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3478513.3480565
   Zheng SK, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3478513.3480510
   Zwicker M, 2015, COMPUT GRAPH FORUM, V34, P667, DOI 10.1111/cgf.12592
NR 32
TC 0
Z9 0
U1 5
U2 10
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2023
VL 39
IS 8
SI SI
BP 3197
EP 3210
DI 10.1007/s00371-023-02951-6
EA JUL 2023
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA P2DS6
UT WOS:001028675100002
OA Bronze
DA 2024-07-18
ER

PT J
AU Li, CL
   Song, S
   Wang, XY
   Liu, Y
   Bi, LH
AF Li, Canlin
   Song, Shun
   Wang, Xinyue
   Liu, Yan
   Bi, Lihua
TI RCFNC: a resolution and contrast fusion network with ConvLSTM for
   low-light image enhancement
SO VISUAL COMPUTER
LA English
DT Article
DE Contrast enhancement; ConvLSTM; Low-light image enhancement; Resolution
   enhancement
ID QUALITY ASSESSMENT; COLOR
AB Low-light image enhancement based on deep learning has achieved breakthroughs recently. However, the current methods based on deep learning have problems with inadequate resolution enhancement or inadequate contrast. To address these problems, this paper proposes a resolution and contrast fusion network with ConvLSTM (RCFNC) for low-light image enhancement. The network is mainly constructed by four parts, including resolution enhancement branch, contrast enhancement branch, multi-scale feature fusion block (MFFB), and convolution long short-time memory block (ConvLSTM). Specifically, to improve the resolution of the low-light image, a resolution enhancement branch consisting of multi-scale differential feature blocks is proposed, using residual features at different scales to enhance the spatial details of image. To enhance the contrast of the image, a contrast enhancement branch consisting of adaptive convolution residual blocks is introduced to learn the mapping relationship between global and local features in the image. In addition, a weighted fusion is performed using MFFB to better balance the resolution and contrast features obtained from the above branches. Finally, to improve the learning capability of the model, ConvLSTM is added to filter redundant information. Experiments on the LOL, MIT5K, and five benchmark datasets show that RCFNC outperforms current state-of-the-art methods.
C1 [Li, Canlin; Song, Shun; Wang, Xinyue; Liu, Yan; Bi, Lihua] Zhengzhou Univ Light Ind, Sch Comp & Commun Engn, Zhengzhou, Peoples R China.
C3 Zhengzhou University of Light Industry
RP Li, CL (corresponding author), Zhengzhou Univ Light Ind, Sch Comp & Commun Engn, Zhengzhou, Peoples R China.
EM lcl_zju@aliyun.com
RI feng, yue/KHV-4687-2024; Yan, Lu/KHW-7015-2024
FU Science and Technology Planning Project of Henan Province [212102210097]
FX AcknowledgementsThe work was supported in part by the Science and
   Technology Planning Project of Henan Province under Grant 212102210097.
CR Bychkovsky V, 2011, PROC CVPR IEEE, P97
   Cai BL, 2017, IEEE I CONF COMP VIS, P4020, DOI 10.1109/ICCV.2017.431
   Cai JR, 2018, IEEE T IMAGE PROCESS, V27, P2049, DOI 10.1109/TIP.2018.2794218
   Fu XY, 2016, PROC CVPR IEEE, P2782, DOI 10.1109/CVPR.2016.304
   Fu XY, 2016, SIGNAL PROCESS, V129, P82, DOI 10.1016/j.sigpro.2016.05.031
   Fu Y, 2022, KNOWL-BASED SYST, V240, DOI 10.1016/j.knosys.2021.108010
   Gao YY, 2018, IEEE T MULTIMEDIA, V20, P335, DOI 10.1109/TMM.2017.2740025
   Gong MG, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2021.3139077
   Guo CL, 2020, PROC CVPR IEEE, P1777, DOI 10.1109/CVPR42600.2020.00185
   Guo S, 2023, VISUAL COMPUT, V39, P1363, DOI 10.1007/s00371-022-02412-6
   Guo XJ, 2017, IEEE T IMAGE PROCESS, V26, P982, DOI 10.1109/TIP.2016.2639450
   Hao SJ, 2020, IEEE T MULTIMEDIA, V22, P3025, DOI 10.1109/TMM.2020.2969790
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Horiuchi T, 2002, INT C PATT RECOG, P867, DOI 10.1109/ICPR.2002.1048165
   Jiang YF, 2021, IEEE T IMAGE PROCESS, V30, P2340, DOI 10.1109/TIP.2021.3051462
   Johnson Justin, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kim H., 2021, P IEEE CVF INT C COM, P4459
   Kim YT, 1997, IEEE T CONSUM ELECTR, V43, P1, DOI 10.1109/30.580378
   Lee C, 2013, IEEE T IMAGE PROCESS, V22, P5372, DOI 10.1109/TIP.2013.2284059
   Li JQ, 2021, IEEE T MULTIMEDIA, V23, P3153, DOI 10.1109/TMM.2020.3021243
   Li MD, 2018, IEEE T IMAGE PROCESS, V27, P2828, DOI 10.1109/TIP.2018.2810539
   Liang D, 2022, AAAI CONF ARTIF INTE, P1555
   Liu RS, 2021, PROC CVPR IEEE, P10556, DOI 10.1109/CVPR46437.2021.01042
   Liu YJ, 2021, 2021 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP 2021), P1840, DOI 10.1109/ICASSP39728.2021.9413433
   Lore KG, 2017, PATTERN RECOGN, V61, P650, DOI 10.1016/j.patcog.2016.06.008
   Ma K, 2015, IEEE T IMAGE PROCESS, V24, P3345, DOI 10.1109/TIP.2015.2442920
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Mustafa WA, 2018, J PHYS CONF SER, V1019, DOI 10.1088/1742-6596/1019/1/012026
   Nezhad ZH, 2016, IEEE J-STARS, V9, P2377, DOI 10.1109/JSTARS.2016.2528339
   Ren XT, 2020, IEEE T IMAGE PROCESS, V29, P5862, DOI 10.1109/TIP.2020.2984098
   Reza AM, 2004, J VLSI SIG PROC SYST, V38, P35, DOI 10.1023/B:VLSI.0000028532.53893.82
   Vonikakis V, 2008, IET IMAGE PROCESS, V2, P19, DOI 10.1049/iet-ipr:20070012
   Wang P, 2021, MULTIMED TOOLS APPL, V80, P17705, DOI 10.1007/s11042-021-10607-7
   Wang SH, 2013, IEEE T IMAGE PROCESS, V22, P3538, DOI 10.1109/TIP.2013.2261309
   Wang XH, 2022, MULTIMED TOOLS APPL, DOI 10.1007/s11042-022-13335-8
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wei C, 2018, ARXIV
   Wu WH, 2022, PROC CVPR IEEE, P5891, DOI 10.1109/CVPR52688.2022.00581
   Xie YQ, 2021, IEEE IPCCC, DOI 10.1109/IPCCC51483.2021.9679370
   Xu X., 2022, IEEECVF C COMPUT VIS, P17714
   Yu NN, 2023, VISUAL COMPUT, V39, P1251, DOI 10.1007/s00371-022-02402-8
   YU X, 2022, VISUAL COMPUT, P1
   Yuanmin Xie, 2019, Journal of Physics: Conference Series, V1314, DOI 10.1088/1742-6596/1314/1/012161
   Yukun Huang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P14072, DOI 10.1109/CVPR42600.2020.01409
   Zhang YH, 2021, INT J COMPUT VISION, V129, P1013, DOI 10.1007/s11263-020-01407-x
   Zhang YH, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1632, DOI 10.1145/3343031.3350926
   Zhu MF, 2020, AAAI CONF ARTIF INTE, V34, P13106
NR 47
TC 0
Z9 0
U1 8
U2 17
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2024
VL 40
IS 4
BP 2793
EP 2806
DI 10.1007/s00371-023-02986-9
EA JUL 2023
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA MZ2U3
UT WOS:001024240700002
DA 2024-07-18
ER

PT J
AU Choudhary, R
   Sharma, M
   Uma, TV
   Anil, R
AF Choudhary, Rohit
   Sharma, Mansi
   Uma, T. V.
   Anil, Rithvik
TI MEStereo-Du2CNN: a dual-channel CNN for learning robust depth estimates
   from multi-exposure stereo images for HDR 3D applications
SO VISUAL COMPUTER
LA English
DT Article
DE 3D TV; Depth estimation; Dual convolution neural network; High dynamic
   range; Multi-exposure stereo; Stereo matching; Transfer learning;
   Virtual reality
ID SUPERRESOLUTION
AB Display technologies have evolved over the years. It is critical to develop practical HDR capturing, processing, and display solutions to bring 3D technologies to the next level. Depth estimation of multi-exposure stereo image sequences is an essential task in the development of cost-effective 3D HDR video content. In this paper, we develop a deep architecture for multi-exposure stereo depth estimation. The proposed architecture has two novel components. First, the stereo matching technique used in traditional stereo depth estimation is revamped. For the stereo depth estimation component of our architecture, a mono-to-stereo transfer learning approach is deployed. The proposed formulation circumvents the cost volume construction requirement, which is replaced by a dual-encoder single-decoder CNN with different weights for feature fusion. EfficientNet-based blocks are used to learn the disparity. Secondly, we combine disparity maps obtained from the stereo images at different exposure levels using a robust disparity feature fusion approach. The disparity maps obtained at different exposures are merged using weight maps calculated for different quality measures. The final predicted disparity map obtained is more robust and retains best features that preserve the depth discontinuities. The proposed CNN offers flexibility to train using standard dynamic range stereo data or with multi-exposure low dynamic range stereo sequences. In terms of performance, the proposed model surpasses state-of-the-art monocular and stereo depth estimation methods, both quantitatively and qualitatively, on challenging Scene flow and differently exposed Middlebury stereo datasets. The architecture performs exceedingly well on complex natural scenes, demonstrating its usefulness for diverse 3D HDR applications.
C1 [Sharma, Mansi] Amrita Vishwa Vidyapeetham, Amrita Sch Comp, Dept Comp Sci & Engn, Coimbatore, India.
   [Choudhary, Rohit; Sharma, Mansi] Indian Inst Technol Madras, Dept Elect Engn, Chennai 600036, Tamil Nadu, India.
   [Uma, T. V.; Anil, Rithvik] Indian Inst Technol Madras, Dept Mech Engn, Chennai 600036, Tamil Nadu, India.
C3 Amrita Vishwa Vidyapeetham; Amrita Vishwa Vidyapeetham Coimbatore;
   Indian Institute of Technology System (IIT System); Indian Institute of
   Technology (IIT) - Madras; Indian Institute of Technology System (IIT
   System); Indian Institute of Technology (IIT) - Madras
RP Sharma, M (corresponding author), Amrita Vishwa Vidyapeetham, Amrita Sch Comp, Dept Comp Sci & Engn, Coimbatore, India.; Sharma, M (corresponding author), Indian Inst Technol Madras, Dept Elect Engn, Chennai 600036, Tamil Nadu, India.
EM ee20s002@smail.iitm.ac.in; s_mansi@cb.amrita.edu; uma.tv1699@gmail.com;
   rithvik.anil@gmail.com
RI Sharma, Mansi/S-1187-2017
OI Sharma, Mansi/0000-0003-3243-3321; Anil, Rithvik/0000-0002-4596-0000
CR Akhavan T., 2013, P 1 INT C SME WORKSH, P1
   Akhavan T., 2015, EURASIP JIVP, V1-12, P2015
   Anil R, 2021, IEEE I C VI COM I PR, DOI 10.1109/VCIP53242.2021.9675391
   [Anonymous], 2018, arXiv
   [Anonymous], 2005, Adv. Neural Inf. Process. Syst.
   Bhat SF, 2021, PROC CVPR IEEE, P4008, DOI 10.1109/CVPR46437.2021.00400
   BURT PJ, 1983, IEEE T COMMUN, V31, P532, DOI 10.1109/TCOM.1983.1095851
   Cantrell KJ, 2020, PROCEEDINGS OF THE 6TH INTERNATIONAL CONFERENCE ON VEHICLE TECHNOLOGY AND INTELLIGENT TRANSPORT SYSTEMS (VEHITS), P406, DOI 10.5220/0009781804060414
   Chang JR, 2018, PROC CVPR IEEE, P5410, DOI 10.1109/CVPR.2018.00567
   Chari P., 2020, ARXIV
   Choudhary R., 2022, ARXIV
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Duggal S, 2019, IEEE I CONF COMP VIS, P4383, DOI 10.1109/ICCV.2019.00448
   Eigen D, 2014, ADV NEUR IN, V27
   Eilertsen G, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130816
   Ferstl D, 2015, IEEE I CONF COMP VIS, P513, DOI 10.1109/ICCV.2015.66
   Garcia F, 2012, IEEE J-STSP, V6, P425, DOI 10.1109/JSTSP.2012.2207090
   Hao ZX, 2018, INT CONF 3D VISION, P304, DOI 10.1109/3DV.2018.00043
   Hasinoff SW, 2010, PROC CVPR IEEE, P553, DOI 10.1109/CVPR.2010.5540167
   Hirschmüller H, 2008, IEEE T PATTERN ANAL, V30, P328, DOI 10.1109/TPAMl.2007.1166
   Hirschmüller H, 2007, PROC CVPR IEEE, P2134
   Hu JJ, 2019, IEEE WINT CONF APPL, P1043, DOI 10.1109/WACV.2019.00116
   Hui TW, 2016, LECT NOTES COMPUT SC, V9907, P353, DOI 10.1007/978-3-319-46487-9_22
   Im S, 2019, IEEE T IMAGE PROCESS, V28, P2451, DOI 10.1109/TIP.2018.2886777
   Jeong Y, 2022, SENSORS-BASEL, V22, DOI 10.3390/s22197388
   Kalantari NK, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073609
   Kendall Alex, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P66, DOI 10.1109/ICCV.2017.17
   Kingma D. P., 2014, arXiv
   Kopf J, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1276377.1276497, 10.1145/1239451.1239547]
   Kwon H, 2015, PROC CVPR IEEE, P159, DOI 10.1109/CVPR.2015.7298611
   Laina I, 2016, INT CONF 3D VISION, P239, DOI 10.1109/3DV.2016.32
   Li YZ, 2021, VISUAL COMPUT, V37, P2567, DOI 10.1007/s00371-021-02206-2
   Li Z., 2020, ARXIV
   Liang ZF, 2018, PROC CVPR IEEE, P2811, DOI 10.1109/CVPR.2018.00297
   Lin GS, 2017, PROC CVPR IEEE, P5168, DOI 10.1109/CVPR.2017.549
   Lin HY, 2016, LECT NOTES COMPUT SC, V9431, P605, DOI 10.1007/978-3-319-29451-3_48
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu YL, 2020, PROC CVPR IEEE, P1648, DOI 10.1109/CVPR42600.2020.00172
   MALIK J, 1990, J OPT SOC AM A, V7, P923, DOI 10.1364/JOSAA.7.000923
   Mayer N, 2016, PROC CVPR IEEE, P4040, DOI 10.1109/CVPR.2016.438
   Mertens T, 2007, PACIFIC GRAPHICS 2007: 15TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, P382, DOI 10.1109/PG.2007.17
   Min DB, 2012, IEEE T IMAGE PROCESS, V21, P1176, DOI 10.1109/TIP.2011.2163164
   Mozerov MG, 2015, IEEE T IMAGE PROCESS, V24, P1153, DOI 10.1109/TIP.2015.2395820
   Nayana A., 2015, INT J IMAGE PROCESS, V9, P198
   Newcombe RA, 2011, INT SYM MIX AUGMENT, P127, DOI 10.1109/ISMAR.2011.6092378
   Ning SY, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P1383, DOI 10.1109/ICASSP.2018.8462444
   OHTA Y, 1985, IEEE T PATTERN ANAL, V7, P139, DOI 10.1109/TPAMI.1985.4767639
   Ranftl R, 2022, IEEE T PATTERN ANAL, V44, P1623, DOI 10.1109/TPAMI.2020.3019967
   Riegler G., 2016, PREPRINT
   Scharstein D, 2001, IEEE WORKSHOP ON STEREO AND MULTI-BASELINE VISION, PROCEEDINGS, P131, DOI 10.1023/A:1014573219977
   Scharstein D, 2014, LECT NOTES COMPUT SC, V8753, P31, DOI 10.1007/978-3-319-11752-2_3
   Scharstein D, 2007, PROC CVPR IEEE, P1688
   Schuon S, 2009, PROC CVPR IEEE, P343, DOI 10.1109/CVPRW.2009.5206804
   Sharma M, 2021, IETE TECH REV, V38, P429, DOI 10.1080/02564602.2020.1758226
   Tan M., 2019, arXiv
   tinyurl, SUPPLEMENTARY INFORM
   Wadaskar A., 2019, IC3D, P1
   Wang L., 2021, IEEE T PATTERN ANAL, V1
   Watson J, 2019, IEEE I CONF COMP VIS, P2162, DOI 10.1109/ICCV.2019.00225
   Xian K, 2018, PROC CVPR IEEE, P311, DOI 10.1109/CVPR.2018.00040
   Xu D, 2018, PROC CVPR IEEE, P3917, DOI 10.1109/CVPR.2018.00412
   Xu D, 2017, PROC CVPR IEEE, P161, DOI 10.1109/CVPR.2017.25
   Yan JX, 2021, INT CONF 3D VISION, P464, DOI 10.1109/3DV53792.2021.00056
   Yan QS, 2020, IEEE T IMAGE PROCESS, V29, P4308, DOI 10.1109/TIP.2020.2971346
   Yang GS, 2019, PROC CVPR IEEE, P5510, DOI 10.1109/CVPR.2019.00566
   Yang JC, 2010, IEEE T IMAGE PROCESS, V19, P2861, DOI 10.1109/TIP.2010.2050625
   Yang JY, 2014, IEEE T IMAGE PROCESS, V23, P3443, DOI 10.1109/TIP.2014.2329776
   Zhao T, 2022, VISUAL COMPUT, V38, P1619, DOI 10.1007/s00371-021-02092-8
NR 68
TC 0
Z9 0
U1 6
U2 10
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2024
VL 40
IS 3
BP 2219
EP 2233
DI 10.1007/s00371-023-02912-z
EA JUL 2023
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HY8O3
UT WOS:001023646600001
DA 2024-07-18
ER

PT J
AU Ai, LM
   Xie, ZY
   Yao, RX
   Yang, MY
AF Ai, Lingmei
   Xie, Zhuoyu
   Yao, Ruoxia
   Yang, Mengyao
TI MVTr: multi-feature voxel transformer for 3D object detection
SO VISUAL COMPUTER
LA English
DT Article
DE 3D object detection; Point clouds; RGB image fusion; Segmentation;
   Transformer
AB Convolutional neural networks have become a powerful tool for partial 3D object detection. However, their power has not been fully realized for focusing on global information, which is crucial for object detection. In this paper, we resolve the problem with a multi-feature voxel transformer (MVTr), an architecture that extracts long-range relationship features through self-attention between multi-feature voxels. In general, converting a point cloud to a voxel representation can reduce a lot of computation, but it would take a long process for the attention network to pay attention to the car voxels in a huge 3D real scene. To this end, we propose a semantic voxel module which takes semantic voxels as input and cooperates with a sparse and a non-empty voxel module to extract features. And the semantic voxels are generated from image segmentation and point cloud projection, which only retains a large number of car voxels. To further enlarge the attention range while maintaining a favorable computational, we propose two attention mechanisms for multi-head attention: local attention and stumpy attention. Finally, we propose the fusion attention module, which can add channel attention and spatial attention to the 2D backbone network. MVTr combines the semantic information of the image and the 3D information of the point cloud and can be applied to most 3D object detection tasks. Experimental results on KITTI dataset show that our method is effective, and the precision has significant advantages compared to other similar feature fusion-based methods.
C1 [Ai, Lingmei; Xie, Zhuoyu; Yao, Ruoxia; Yang, Mengyao] Shaanxi Normal Univ, Sch Comp Sci, Xian 710119, Peoples R China.
C3 Shaanxi Normal University
RP Ai, LM (corresponding author), Shaanxi Normal Univ, Sch Comp Sci, Xian 710119, Peoples R China.
EM almsac@163.com
RI Zhang, Yusi/JNS-2335-2023; wang, xi/JNT-5162-2023; liu,
   kaiyuan/JHU-0258-2023; li, wl/JJC-0768-2023; wang, mengyi/KEI-9461-2024;
   LI, LIXIN/KFS-0074-2024; Wang, Yue/JRY-8962-2023; liu,
   jingwen/JQW-9270-2023; Liu, Yuan/JFB-4766-2023; zhao, lin/JPK-8436-2023;
   WANG, YANG/JFA-8821-2023; Liu, Jingyi/JWP-6326-2024; Sun,
   Xinyu/JXX-2281-2024; Chen, Chao/JHS-6563-2023; LI, Xiang/JBJ-8387-2023;
   chen, xu/JNT-3068-2023; Wang, Han/JJF-2614-2023; li,
   jiaxin/JNT-5073-2023; li, tao/JVO-9006-2024; Li, Shuyao/JRY-8603-2023
OI Wang, Yue/0000-0001-8673-6358; 
FU National Natural Science Foundation of China [62271297, 12271324]; Major
   Project of the Science and Technology Ministry in China
   [2020YFB0204500]; Natural Science Basic Research Program of Shaanxi
   Province [2021JZ-21]
FX AcknowledgementsThis work was supported by National Natural Science
   Foundation of China under Grant 62271297, 12271324, and in part by the
   Major Project of the Science and Technology Ministry in China under
   Grant 2020YFB0204500, and in part by the Natural Science Basic Research
   Program of Shaanxi Province under Grant 2021JZ-21.
CR Beltrán J, 2018, IEEE INT C INTELL TR, P3517, DOI 10.1109/ITSC.2018.8569311
   Benedek C, 2021, DIGIT SIGNAL PROCESS, V119, DOI 10.1016/j.dsp.2021.103193
   Carion N., 2020, EUR C COMP VIS, P213, DOI DOI 10.1007/978-3-030-58452-8_13
   Chen LF, 2023, VISUAL COMPUT, V39, P5229, DOI 10.1007/s00371-022-02656-2
   Chen XZ, 2017, PROC CVPR IEEE, P6526, DOI 10.1109/CVPR.2017.691
   Chenhang He, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11870, DOI 10.1109/CVPR42600.2020.01189
   Cui YL, 2021, DIGIT SIGNAL PROCESS, V117, DOI 10.1016/j.dsp.2021.103138
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Deng JJ, 2021, AAAI CONF ARTIF INTE, V35, P1201
   Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074
   Gu WC, 2022, IMAGE VISION COMPUT, V120, DOI 10.1016/j.imavis.2022.104401
   Hasan SMK, 2019, IEEE ENG MED BIO, P7205, DOI [10.1109/EMBC.2019.8856791, 10.1109/embc.2019.8856791]
   He KM, 2020, IEEE T PATTERN ANAL, V42, P386, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   He YQ, 2021, NEUROCOMPUTING, V459, P201, DOI 10.1016/j.neucom.2021.06.046
   Ji CF, 2023, VISUAL COMPUT, V39, P4543, DOI 10.1007/s00371-022-02607-x
   Kirillov A, 2019, PROC CVPR IEEE, P9396, DOI 10.1109/CVPR.2019.00963
   Ku J, 2018, IEEE INT C INT ROBOT, P5750, DOI 10.1109/IROS.2018.8594049
   Lan M, 2023, PATTERN RECOGN, V136, DOI 10.1016/j.patcog.2022.109214
   Lang AH, 2019, PROC CVPR IEEE, P12689, DOI 10.1109/CVPR.2019.01298
   Li XY, 2022, DIGIT SIGNAL PROCESS, V120, DOI 10.1016/j.dsp.2021.103283
   Liang M, 2019, PROC CVPR IEEE, P7337, DOI 10.1109/CVPR.2019.00752
   Liang ZD, 2021, PROC CVPR IEEE, P7136, DOI 10.1109/CVPR46437.2021.00706
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu Z, 2020, AAAI CONF ARTIF INTE, V34, P11677
   Mao JG, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P3144, DOI 10.1109/ICCV48922.2021.00315
   Pan XR, 2021, PROC CVPR IEEE, P7459, DOI 10.1109/CVPR46437.2021.00738
   Pant G, 2020, ALGAL RES, V48, DOI 10.1016/j.algal.2020.101932
   Qamar S, 2019, 2019 IEEE 5TH INTERNATIONAL CONFERENCE FOR CONVERGENCE IN TECHNOLOGY (I2CT), DOI 10.1109/i2ct45611.2019.9033802
   Qi CR, 2017, ADV NEUR IN, V30
   Qi CR, 2018, PROC CVPR IEEE, P918, DOI 10.1109/CVPR.2018.00102
   Shaoshuai Shi, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10526, DOI 10.1109/CVPR42600.2020.01054
   Shi SS, 2019, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2019.00086
   Shi SS, 2021, IEEE T PATTERN ANAL, V43, P2647, DOI 10.1109/TPAMI.2020.2977026
   Wang G., 2020, ARXIV, DOI DOI 10.48550/ARXIV.2011.00652
   Wang HY, 2021, PROC CVPR IEEE, P5459, DOI 10.1109/CVPR46437.2021.00542
   Wang L., 2021, ARXIV, DOI DOI 10.48550/ARXIV.2103.08439
   Wenya Liu, 2019, 2019 IEEE 4th International Conference on Image, Vision and Computing (ICIVC), P311, DOI 10.1109/ICIVC47709.2019.8981007
   Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634
   Yan Y, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18103337
   Yang ZT, 2019, IEEE I CONF COMP VIS, P1951, DOI 10.1109/ICCV.2019.00204
   Ye MS, 2020, PROC CVPR IEEE, P1628, DOI 10.1109/CVPR42600.2020.00170
   Ye YY, 2020, NEUROCOMPUTING, V379, P53, DOI 10.1016/j.neucom.2019.09.086
   Zaheer M., 2020, ADV NEURAL INFORM PR, P17283, DOI DOI 10.5555/3495724.3497174
   Zetong Yang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11037, DOI 10.1109/CVPR42600.2020.01105
   Zhang H, 2022, IEEE COMPUT SOC CONF, P2735, DOI 10.1109/CVPRW56347.2022.00309
   ZHANG Q, 2022, ISME J, P1, DOI DOI 10.1007/S00371-022-02611-1
   Zhao HS, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P16239, DOI 10.1109/ICCV48922.2021.01595
   Zhao X, 2019, AAAI CONF ARTIF INTE, P9267
   Zheng SX, 2021, PROC CVPR IEEE, P6877, DOI 10.1109/CVPR46437.2021.00681
   Zhou XN, 2019, VISUAL COMPUT, V35, P385, DOI 10.1007/s00371-018-1471-4
   Zhou Y, 2018, PROC CVPR IEEE, P4490, DOI 10.1109/CVPR.2018.00472
NR 51
TC 3
Z9 3
U1 7
U2 21
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2024
VL 40
IS 3
BP 1453
EP 1466
DI 10.1007/s00371-023-02860-8
EA MAY 2023
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HY8O3
UT WOS:000978788900001
DA 2024-07-18
ER

PT J
AU Gan, YS
   Lien, SE
   Chiang, YC
   Liong, ST
AF Gan, Y. S.
   Lien, Sung-En
   Chiang, Yi-Chen
   Liong, Sze-Teng
TI LAENet for micro-expression recognition
SO VISUAL COMPUTER
LA English
DT Article
DE Micro-expressions; Eye masking; Apex; Optical strain; Recognition
ID INFORMATION; FACE
AB Micro-expression is an expression that reveals one's true feelings and can be potentially applied in various domains such as healthcare, safety interrogation, and business negotiation. Themicro-expression recognition is thus far being judged manually by psychologists and trained experts, which consumes a lot of human effort and time. Recently, the development of the deep learning network has proven promising performance in many computer vision-related tasks. Amongst, micro-expression recognition adopts the deep learning methodology to improve the feature learning capability and model generalization. This paper introduces a lightweight apex-based enhanced network that improves by extending one of the state-of-the-art, shallow triple stream three-dimensional CNN. Concretely, the network is first pre-trained with a macro-expression dataset to encounter the small data problem. The features were extracted from CASME II, SMIC, and SAMM datasets. Moreover, thorough recognition results comparison of the datasets are the optical flow-guided features. Besides, an eye masking technique is introduced to reduce noise interference such as eye blinking and glasses reflection issues. The results obtained have an accuracy of 79.19% and an F1-score of 75.9%. Comprehensive experimentation had been conducted on the composite dataset that consists of is provided by comparing it with recent methods. Detailed qualitative and quantitative results are reported and discussed.
C1 [Gan, Y. S.] Feng Chia Univ, Sch Architecture, Taichung 40724, Taiwan.
   [Lien, Sung-En; Chiang, Yi-Chen] Purdue Univ, Sch Elect & Comp Engn, W Lafayette, IN 47906 USA.
   [Liong, Sze-Teng] Feng Chia Univ, Dept Elect Engn, Taichung 40724, Taiwan.
C3 Feng Chia University; Purdue University System; Purdue University; Feng
   Chia University
RP Liong, ST (corresponding author), Feng Chia Univ, Dept Elect Engn, Taichung 40724, Taiwan.
EM ysgan@fcu.edu.tw; lien1@purdue.edu; chiang90@purdue.edu;
   stliong@fcu.edu.tw
RI Liong, Christy/N-3434-2018
FU Ministry of Science and Technology (MOST) [MOST 111-2221-E-035-059-MY3,
   MOST 110-2221-E-035-052-]
FX This work was funded by Ministry of Science and Technology (MOST) (Grant
   Numbers: MOST 111-2221-E-035-059-MY3 and MOST 110-2221-E-035-052-)
CR [Anonymous], 1966, Methods of research in psychotherapy, DOI [DOI 10.1007/978-1-4684-6045-2_14, 10.1007/978-1-4684-6045-2_14]
   [Anonymous], 2013, 2013 10th IEEE international conference and workshops on automatic face and gesture recognition (FG), DOI [DOI 10.1109/FG.2013.6553799, DOI 10.1109/FG.2013.6553799.IEEE]
   Asthana A, 2013, PROC CVPR IEEE, P3444, DOI 10.1109/CVPR.2013.442
   Davison AK, 2018, IEEE T AFFECT COMPUT, V9, P116, DOI 10.1109/TAFFC.2016.2573832
   EKMAN P, 1991, AM PSYCHOL, V46, P913, DOI 10.1037/0003-066X.46.9.913
   Ekman P, 2003, ANN NY ACAD SCI, V1000, P205, DOI 10.1196/annals.1280.010
   EKMAN P, 1971, J PERS SOC PSYCHOL, V17, P124, DOI 10.1037/h0030377
   EKMAN P, 1993, AM PSYCHOL, V48, P384, DOI 10.1037/0003-066X.48.4.384
   Esposito A, 2007, LECT NOTES COMPUT SC, V4391, P249
   Fangbing Qu, 2018, IEEE Transactions on Affective Computing, V9, P424, DOI 10.1109/TAFFC.2017.2654440
   Gan YS, 2019, SIGNAL PROCESS-IMAGE, V74, P129, DOI 10.1016/j.image.2019.02.005
   Goh KM, 2020, VISUAL COMPUT, V36, P445, DOI 10.1007/s00371-018-1607-6
   Gu K, 2021, IEEE T NEUR NET LEAR, V32, P4278, DOI 10.1109/TNNLS.2021.3105394
   Gu K, 2021, IEEE T IND INFORM, V17, P2261, DOI 10.1109/TII.2020.2991208
   Gu K, 2020, IEEE T MULTIMEDIA, V22, P311, DOI 10.1109/TMM.2019.2929009
   Gu K, 2015, IEEE T CIRC SYST VID, V25, P1480, DOI 10.1109/TCSVT.2014.2372392
   Gu K, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2439035
   Happy S. L., 2017, IEEE Transactions on Affective Computing
   Huang XH, 2019, IEEE T AFFECT COMPUT, V10, P32, DOI 10.1109/TAFFC.2017.2713359
   Huang XH, 2017, 2017 INTERNATIONAL CONFERENCE ON THE FRONTIERS AND ADVANCES IN DATA SCIENCE (FADS), P198, DOI 10.1109/FADS.2017.8253219
   Huang XH, 2016, NEUROCOMPUTING, V175, P564, DOI 10.1016/j.neucom.2015.10.096
   Husak P., 2017, P 22 COMP VIS WINT W, P1
   Kharat GU, 2009, ADV INTEL SOFT COMPU, V60, P207
   Khor HQ, 2019, IEEE IMAGE PROC, P36, DOI [10.1109/icip.2019.8802965, 10.1109/ICIP.2019.8802965]
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li J, 2019, PATTERN ANAL APPL, V22, P1331, DOI 10.1007/s10044-018-0757-5
   Li XB, 2013, IEEE INT CONF AUTOMA, DOI 10.1109/FG.2013.6553717
   Li YT, 2018, IEEE IMAGE PROC, P3094, DOI 10.1109/ICIP.2018.8451376
   Liong S.-T., 2016, AS C COMP VIS, P345
   Liong ST, 2018, Arxiv, DOI arXiv:1805.08699
   Liong ST, 2020, J SIGNAL PROCESS SYS, V92, P705, DOI 10.1007/s11265-020-01523-4
   Liong ST, 2019, IEEE INT CONF AUTOMA, P658, DOI 10.1109/fg.2019.8756567
   Liong ST, 2018, SIGNAL PROCESS-IMAGE, V62, P82, DOI 10.1016/j.image.2017.11.006
   Liong ST, 2016, SIGNAL PROCESS-IMAGE, V47, P170, DOI 10.1016/j.image.2016.06.004
   Liong ST, 2014, I S INTELL SIG PROC, P180, DOI 10.1109/ISPACS.2014.7024448
   Liong ST, 2015, PROCEEDINGS 3RD IAPR ASIAN CONFERENCE ON PATTERN RECOGNITION ACPR 2015, P665, DOI 10.1109/ACPR.2015.7486586
   Liong ST, 2015, LECT NOTES COMPUT SC, V9009, P644, DOI 10.1007/978-3-319-16631-5_47
   Liu KH, 2021, SIGNAL PROCESS-IMAGE, V93, DOI 10.1016/j.image.2021.116153
   Liu Y., 2019, 2019 14 IEEE INT C A, P1
   Liu YJ, 2016, IEEE T AFFECT COMPUT, V7, P299, DOI 10.1109/TAFFC.2015.2485205
   Lu H, 2018, SIGNAL PROCESS-IMAGE, V67, P108, DOI 10.1016/j.image.2018.05.014
   Lucey P., 2010, IEEE COMP SOC C COMP, P94, DOI 10.1109/CVPRW.2010.5543262
   Iandola FN, 2016, Arxiv, DOI arXiv:1602.07360
   Ojala T, 1996, PATTERN RECOGN, V29, P51, DOI 10.1016/0031-3203(95)00067-4
   Patel D, 2016, INT C PATT RECOG, P2258, DOI 10.1109/ICPR.2016.7899972
   Rivera AR, 2013, IEEE T IMAGE PROCESS, V22, P1740, DOI 10.1109/TIP.2012.2235848
   See J, 2019, IEEE INT CONF AUTOMA, P647
   Shreve Matthew, 2011, Proceedings 2011 IEEE International Conference on Automatic Face & Gesture Recognition (FG 2011), P51, DOI 10.1109/FG.2011.5771451
   Shreve M, 2014, IMAGE VISION COMPUT, V32, P476, DOI 10.1016/j.imavis.2014.04.010
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Szegedy Christian, 2015, IEEE C COMP VIS PATT, DOI [10.1109/cvpr.2015.7298594, DOI 10.1109/CVPR.2015.7298594]
   Van Quang N., 2019, IEEE INT CONF AUTOMA, DOI [10.1109/FG.2019.8756544, DOI 10.1109/fg.2019.8756544]
   Wang SJ, 2018, NEUROCOMPUTING, V312, P251, DOI 10.1016/j.neucom.2018.05.107
   Wang YD, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0124674
   Yan WJ, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0086041
   Zhao GY, 2007, IEEE T PATTERN ANAL, V29, P915, DOI 10.1109/TPAMI.2007.1110
   Zhao SR, 2021, NEUROCOMPUTING, V448, P276, DOI 10.1016/j.neucom.2021.03.058
   Zhou L, 2019, IEEE INT CONF AUTOMA, P642
NR 58
TC 5
Z9 5
U1 12
U2 36
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2024
VL 40
IS 2
BP 585
EP 599
DI 10.1007/s00371-023-02803-3
EA MAR 2023
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GE6E8
UT WOS:000957508600002
DA 2024-07-18
ER

PT J
AU Zhang, JH
   Yang, JF
   Ma, JQ
AF Zhang, Jiahui
   Yang, Jinfu
   Ma, Jiaqi
TI Monocular visual-inertial odometry leveraging point-line features with
   structural constraints
SO VISUAL COMPUTER
LA English
DT Article
DE Structural constraints; Visual-inertial odometry; Vanishing point;
   Structural line
ID STRUCTURE-FROM-MOTION; ROBUST; SLAM; VERSATILE
AB Structural geometry constraints, such as perpendicularity, parallelism and coplanarity, are widely existing inman-made scene, especially in Manhattan scene. By fully exploiting these structural properties, we propose amonocular visual-inertial odometry (VIO) using point and line features with structural constraints. First, a coarse-to-fine vanishing points estimation method with line segment consistency verification is presented to classify lines into structural and non-structural lines accurately with less computation cost. Then, to get precise estimation of camera pose and the position of 3D landmarks, a cost function which combines structural line constraints with feature reprojection residual and inertial measurement unit residual is minimized under a sliding window framework. For geometric representation of lines, Plucker coordinates and orthonormal representation are utilized for 3D line transformation and non-linear optimization respectively. Sufficient evaluations are conducted using two public datasets to verify that the proposed system can effectively enhance the localization accuracy and robustness than other existing state-of-the-art VIO systems with acceptable time consumption.
C1 [Zhang, Jiahui; Yang, Jinfu; Ma, Jiaqi] Beijing Univ Technol, Fac Informat, Beijing 100124, Peoples R China.
   [Yang, Jinfu] Beijing Univ Technol, Beijing Key Lab Computat Intelligence & Intellige, Beijing 100124, Peoples R China.
C3 Beijing University of Technology; Beijing University of Technology
RP Yang, JF (corresponding author), Beijing Univ Technol, Fac Informat, Beijing 100124, Peoples R China.; Yang, JF (corresponding author), Beijing Univ Technol, Beijing Key Lab Computat Intelligence & Intellige, Beijing 100124, Peoples R China.
EM zhangjiahui_2021@163.com; jfyang@bjut.edu.cn; majiaqi@emails.bjut.edu.cn
RI Zhang, Jiahui/JUF-3670-2023; Ma, Jiaqi/AAU-6069-2020
OI Zhang, Jiahui/0000-0001-7005-1629; 
FU National Natural Science Foundation of China [61973009]
FX This work is partly supported by the National Natural Science Foundation
   of China under Grant No.61973009.
CR Agarwal S., 2012, Google Inc, V2, P8
   Almalioglu Y, 2022, NEURAL NETWORKS, V150, P119, DOI 10.1016/j.neunet.2022.03.005
   Bartoli A, 2005, COMPUT VIS IMAGE UND, V100, P416, DOI 10.1016/j.cviu.2005.06.001
   Bloesch M, 2015, IEEE INT C INT ROBOT, P298, DOI 10.1109/IROS.2015.7353389
   Bouguet, 2001, INTEL CORP, V5, P4, DOI DOI 10.1109/HPDC.2004.1323531
   Burri M, 2016, INT J ROBOT RES, V35, P1157, DOI 10.1177/0278364915620033
   Cui HN, 2022, IEEE T IMAGE PROCESS, V31, P2449, DOI 10.1109/TIP.2022.3156375
   Davison AJ, 2007, IEEE T PATTERN ANAL, V29, P1052, DOI 10.1109/TPAMI.2007.1049
   Forster C, 2017, IEEE T ROBOT, V33, P1, DOI 10.1109/TRO.2016.2597321
   Fu Q., 2020, arXiv
   Greene WN, 2020, IEEE INT CONF ROBOT, P43, DOI [10.1109/icra40945.2020.9196900, 10.1109/ICRA40945.2020.9196900]
   Guclu O, 2020, VISUAL COMPUT, V36, P1271, DOI 10.1007/s00371-019-01720-8
   He M, 2020, VISUAL COMPUT, V36, P1053, DOI 10.1007/s00371-019-01714-6
   He YJ, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18041159
   Huang GQ, 2019, IEEE INT CONF ROBOT, P9572, DOI [10.1109/ICRA.2019.8793604, 10.1109/icra.2019.8793604]
   Hughes C, 2010, IEEE T PATTERN ANAL, V32, P2289, DOI 10.1109/TPAMI.2010.159
   Jones ES, 2011, INT J ROBOT RES, V30, P407, DOI 10.1177/0278364910388963
   Joo K, 2022, IEEE T PATTERN ANAL, V44, P8403, DOI 10.1109/TPAMI.2021.3106820
   Kim P, 2018, IEEE INT CONF ROBOT, P7247
   Kneip L., 2011, 2011 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2011), P2235, DOI 10.1109/IROS.2011.6048267
   Lee J, 2021, IEEE ROBOT AUTOM LET, V6, P7033, DOI 10.1109/LRA.2021.3095518
   Li HA, 2019, IEEE INT CONF ROBOT, P2412, DOI [10.1109/icra.2019.8793716, 10.1109/ICRA.2019.8793716]
   Li N, 2022, VISUAL COMPUT, V38, P2091, DOI 10.1007/s00371-021-02270-8
   Li YY, 2020, IEEE ROBOT AUTOM LET, V5, P6583, DOI 10.1109/LRA.2020.3015456
   Lim H, 2022, IEEE ROBOT AUTOM LET, V7, P1518, DOI 10.1109/LRA.2022.3140816
   Lin Y, 2018, J FIELD ROBOT, V35, P23, DOI 10.1002/rob.21732
   Lu XH, 2017, IEEE WINT CONF APPL, P345, DOI 10.1109/WACV.2017.45
   Lu Y, 2015, IEEE I CONF COMP VIS, P3934, DOI 10.1109/ICCV.2015.448
   Lucas B., 1981, P INT JOINT C ART IN, P674, DOI DOI 10.1364/J0SAA.19.002142
   Miao RH, 2022, VISUAL COMPUT, V38, P2207, DOI 10.1007/s00371-021-02278-0
   Mur-Artal R, 2015, IEEE T ROBOT, V31, P1147, DOI 10.1109/TRO.2015.2463671
   Peng X., 2021 IEEERSJ INT C I, P5418
   Pumarola A., 2017, P 2017 IEEE INT C RO, P4503, DOI DOI 10.1109/ICRA.2017.7989522
   Qin T, 2018, IEEE T ROBOT, V34, P1004, DOI 10.1109/TRO.2018.2853729
   Schubert D, 2018, IEEE INT C INT ROBOT, P1680, DOI 10.1109/IROS.2018.8593419
   SHI JB, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P593, DOI 10.1109/CVPR.1994.323794
   Toldo R, 2008, LECT NOTES COMPUT SC, V5302, P537, DOI 10.1007/978-3-540-88682-2_41
   von Gioi RG, 2010, IEEE T PATTERN ANAL, V32, P722, DOI 10.1109/TPAMI.2008.300
   Weiss S, 2012, IEEE INT CONF ROBOT, P957, DOI 10.1109/ICRA.2012.6225147
   Xu B, 2022, IEEE ROBOT AUTOM LET, V7, P3483, DOI 10.1109/LRA.2022.3146893
   Yunus R, 2021, IEEE INT CONF ROBOT, P6687, DOI 10.1109/ICRA48506.2021.9562030
   Zhang LL, 2013, J VIS COMMUN IMAGE R, V24, P794, DOI 10.1016/j.jvcir.2013.05.006
   Zhou HZ, 2015, IEEE T VEH TECHNOL, V64, P1364, DOI 10.1109/TVT.2015.2388780
   Zhou Y, 2019, VISUAL COMPUT, V35, P123, DOI 10.1007/s00371-017-1435-0
   Zou DP, 2019, IEEE T ROBOT, V35, P999, DOI 10.1109/TRO.2019.2915140
NR 45
TC 0
Z9 0
U1 7
U2 31
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2024
VL 40
IS 2
BP 647
EP 661
DI 10.1007/s00371-023-02807-z
EA MAR 2023
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GE6E8
UT WOS:000946907000001
DA 2024-07-18
ER

PT J
AU Wang, S
   Sun, ZX
   Li, Q
AF Wang, Shuang
   Sun, Zhengxing
   Li, Qian
TI High-to-low-level feature matching and complementary information fusion
   for reference-based image super-resolution
SO VISUAL COMPUTER
LA English
DT Article
DE Reference-based image super-resolution; Feature matching; Complementary
   information fusion; Feature fusion
ID NETWORKS
AB The aim of the reference-based image super-resolution (RefSR) is to reconstruct high-resolution (HR) when a reference (Ref) image with similar content as that of the low-resolution (LR) input is given. In the task, the quality of existing approaches degrades severely when there are several similar objects but different contents. Besides, not all similar information in the reference image is useful for the input image. Therefore, we propose high-to-low-level feature matching and complementary information fusion (HMCF) network for RefSR. The matching strategy adopts high-level to low-level feature matching to distinguish similar objects but different contents according to high-level semantics. The complementary information fusion module utilizes the channel and spatial attention to select the complement information for LR image and keeps the pixel consistency of input and Ref image. We perform extensive experiments to demonstrate that our proposed HMCF obtains the SOTA performance on the RefSR benchmarks and presents a high visual quality.
C1 [Wang, Shuang; Sun, Zhengxing; Li, Qian] Nanjing Univ, State Key Lab Novel Software Technol, Nanjing 210023, Peoples R China.
   [Wang, Shuang] Jiangsu Vocat Inst Commerce, Nanjing 211168, Peoples R China.
   [Li, Qian] Natl Univ Def Technol, Coll Meteorol & Oceanog, Changsha, Peoples R China.
C3 Nanjing University; Jiangsu Vocational Institute of Commerce; National
   University of Defense Technology - China
RP Sun, ZX (corresponding author), Nanjing Univ, State Key Lab Novel Software Technol, Nanjing 210023, Peoples R China.
EM szx@nju.edu.cn
RI Sun, Zhengxing/A-7411-2011
OI Qian, Li/0000-0002-9530-4925
FU National Natural Science Foundation of China [42075139, 42077232];
   Science and technology program of Jiangsu Province [BE2020082,
   BE2022063]; Innovation Fund of State Key Laboratory for Novel Software
   Technology [ZZKT2022A18]
FX The paper is supported by The National Natural Science Foundation of
   China Nos. 42075139, 42077232. The Science and technology program of
   Jiangsu Province Nos. BE2020082 and BE2022063. The Innovation Fund of
   State Key Laboratory for Novel Software Technology No. ZZKT2022A18.
CR Chen YT, 2021, APPL INTELL, V51, P4367, DOI 10.1007/s10489-020-02116-1
   Demirel H, 2011, IEEE T GEOSCI REMOTE, V49, P1997, DOI 10.1109/TGRS.2010.2100401
   Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281
   Du, ARXIV
   Hu YT, 2020, IEEE T CIRC SYST VID, V30, P3911, DOI 10.1109/TCSVT.2019.2915238
   HUANG JB, 2015, PROC CVPR IEEE, P5197, DOI DOI 10.1109/CVPR.2015.7299156
   Jiang YM, 2021, PROC CVPR IEEE, P2103, DOI 10.1109/CVPR46437.2021.00214
   Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.182, 10.1109/CVPR.2016.181]
   Kim J, 2018, TENCON IEEE REGION, P0090, DOI 10.1109/TENCON.2018.8650166
   Kingma D. P., 2014, arXiv
   Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151
   Lu LY, 2021, PROC CVPR IEEE, P6364, DOI 10.1109/CVPR46437.2021.00630
   Mahapatra D, 2019, COMPUT MED IMAG GRAP, V71, P30, DOI 10.1016/j.compmedimag.2018.10.005
   Qiu DF, 2021, FUTURE GENER COMP SY, V116, P200, DOI 10.1016/j.future.2020.11.001
   Simonyan K., 2014, 14091556 ARXIV
   Xia B., ARXIV
   Xia R., J KING SAUD U COMPUT
   Yang FZ, 2020, PROC CVPR IEEE, P5790, DOI 10.1109/CVPR42600.2020.00583
   Zhang JM, 2022, APPL SOFT COMPUT, V118, DOI 10.1016/j.asoc.2022.108485
   Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI 10.1007/978-3-030-01234-2_18
   Zhang ZF, 2019, PROC CVPR IEEE, P7974, DOI 10.1109/CVPR.2019.00817
   Zheng H., 2018, P IEEE EUR C COMP VI, P88
NR 22
TC 1
Z9 1
U1 5
U2 9
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2024
VL 40
IS 1
BP 99
EP 108
DI 10.1007/s00371-023-02768-3
EA FEB 2023
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IX5B8
UT WOS:000939344800002
DA 2024-07-18
ER

PT J
AU Cheng, XJ
   Huang, SL
   Liao, BY
   Wang, YY
   Luo, X
AF Cheng, Xiji
   Huang, Shiliang
   Liao, Bingyan
   Wang, Yayun
   Luo, Xiao
TI BG-Net: boundary-guidance network for object consistency maintaining in
   semantic segmentation
SO VISUAL COMPUTER
LA English
DT Article
DE Semantic segmentation; Boundary-guidance; Low-level feature; Object
   consistency maintaining
AB Semantic segmentation suffers from boundary shift and shape deformation problems due to the neglect of overall guidance information. Motivated by that the object boundaries have a stronger representation for overall information of target objects, we propose a simple yet effective network, boundary-guidance network (BG-Net), for object consistency maintaining in semantic segmentation. In our work, we explore the pixels both near and far from the boundary, characterizing each pixel by utilizing pixel-boundary association. The boundary feature is integrated into the segmentation feature to mitigate boundary shift and shape deformation problem. We explicitly supervise the angle and distance information of pixels pointing to the nearest object boundary. Then the association can be learned by geometric modelling. Meanwhile, the low-level feature emphasized up-sampling (LFEU) module is designed to supplement the detail representation in high-level feature without direct interference. Finally, we evaluate our method on Cityscapes and CamVid datasets. The experimental results demonstrate the superiority of our BG-Net.
C1 [Huang, Shiliang; Liao, Bingyan; Wang, Yayun] ZheJiang Dahua Technol CO LTD, Hangzhou 310000, Peoples R China.
   [Cheng, Xiji; Luo, Xiao] Hangzhou Yulian Technol CO LTD, Hangzhou 310000, Peoples R China.
RP Liao, BY (corresponding author), ZheJiang Dahua Technol CO LTD, Hangzhou 310000, Peoples R China.
EM chengxiji@vastchain.ltd; huangsl8@mail.ustc.edu.cn;
   bingyanliao@outlook.com; wang_yayun1@dahuatech.com; hslvien@163.com
FU Zhejiang Provincial Key Laboratory of Harmonized Application of Vision &
   Transmission, No. 1199, Bin'an Road, Binjiang District, Hangzhou, China;
   Binjiang District, Hangzhou, China
FX AcknowledgementsThis study was funded by Zhejiang Provincial Key
   Laboratory of Harmonized Application of Vision & Transmission, No. 1199,
   Bin'an Road, Binjiang District, Hangzhou, China.
CR Abu Alhaija H, 2018, INT J COMPUT VISION, V126, P961, DOI 10.1007/s11263-018-1070-x
   Acuna D, 2019, PROC CVPR IEEE, P11067, DOI 10.1109/CVPR.2019.01133
   Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615
   Bilinski P, 2018, PROC CVPR IEEE, P6596, DOI 10.1109/CVPR.2018.00690
   Bowen Cheng, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12472, DOI 10.1109/CVPR42600.2020.01249
   Brostow GJ, 2008, LECT NOTES COMPUT SC, V5302, P44, DOI 10.1007/978-3-540-88682-2_5
   Chandra S, 2018, PROC CVPR IEEE, P8915, DOI 10.1109/CVPR.2018.00929
   Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen LB, 2017, IEEE INT SYMP NANO, P1, DOI 10.1109/NANOARCH.2017.8053709
   Chen XK, 2021, PROC CVPR IEEE, P2613, DOI 10.1109/CVPR46437.2021.00264
   Chen X, 2019, PROC CVPR IEEE, P11624, DOI 10.1109/CVPR.2019.01190
   Chen Y, 2018, ADV NEUR IN, V31
   Cheng ZM, 2022, VISUAL COMPUT, V38, P749, DOI 10.1007/s00371-021-02075-9
   Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350
   Dias P.A., 2018, P AS C COMP VIS, P131
   Ding HH, 2019, IEEE I CONF COMP VIS, P6818, DOI 10.1109/ICCV.2019.00692
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Fan MY, 2021, PROC CVPR IEEE, P9711, DOI 10.1109/CVPR46437.2021.00959
   Fieraru M, 2018, IEEE COMPUT SOC CONF, P318, DOI 10.1109/CVPRW.2018.00058
   Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326
   Fu YP, 2022, VISUAL COMPUT, V38, P3243, DOI 10.1007/s00371-022-02559-2
   He JJ, 2019, PROC CVPR IEEE, P7511, DOI 10.1109/CVPR.2019.00770
   Huang ZL, 2019, IEEE I CONF COMP VIS, P603, DOI 10.1109/ICCV.2019.00069
   Islam MA, 2017, PROC CVPR IEEE, P4877, DOI 10.1109/CVPR.2017.518
   Jaderberg M, 2015, ADV NEUR IN, V28
   Jiang M, 2022, VISUAL COMPUT, V38, P2473, DOI 10.1007/s00371-021-02124-3
   Ke TW, 2018, LECT NOTES COMPUT SC, V11205, P605, DOI 10.1007/978-3-030-01246-5_36
   Kimmel R, 1996, J MATH IMAGING VIS, V6, P223, DOI 10.1007/BF00119840
   Kirillov A., 2020, P IEEECVF C COMPUTER, P9799, DOI DOI 10.48550/ARXIV.1912.08193
   Kirillov A, 2019, PROC CVPR IEEE, P6392, DOI 10.1109/CVPR.2019.00656
   KRAHENBUHL P, 2011, ADV NEURAL INFORM PR, P109, DOI DOI 10.5555/2986459.2986472
   Kundu A, 2016, PROC CVPR IEEE, P3168, DOI 10.1109/CVPR.2016.345
   Kuo WC, 2019, IEEE I CONF COMP VIS, P9206, DOI 10.1109/ICCV.2019.00930
   Li X, 2019, IEEE I CONF COMP VIS, P9166, DOI 10.1109/ICCV.2019.00926
   Li XL, 2020, IEEE WIREL COMMUN, V27, P116, DOI 10.1109/MWC.001.2000076
   Li ZC, 2022, IEEE T PATTERN ANAL, V44, P9904, DOI 10.1109/TPAMI.2021.3132068
   Liang-Chieh Chen, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12354), P695, DOI 10.1007/978-3-030-58545-7_40
   Liao X, 2022, IEEE T DEPEND SECURE, V19, P897, DOI 10.1109/TDSC.2020.3004708
   Liao X, 2020, IEEE J-STSP, V14, P955, DOI 10.1109/JSTSP.2020.3002391
   Liao X, 2020, IEEE T CIRC SYST VID, V30, P685, DOI 10.1109/TCSVT.2019.2896270
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu S, 2021, NEURAL PROCESS LETT, V53, P4257, DOI 10.1007/s11063-021-10592-w
   Liu TR, 2022, VISUAL COMPUT, V38, P2303, DOI 10.1007/s00371-021-02112-7
   Liu Y, 2019, IEEE T PATTERN ANAL, V41, P1939, DOI 10.1109/TPAMI.2018.2878849
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Lyu CZ, 2022, VISUAL COMPUT, V38, P345, DOI 10.1007/s00371-020-02018-w
   Ma ZH, 2022, VISUAL COMPUT, V38, P3163, DOI 10.1007/s00371-022-02535-w
   Mohan R., 2020, ARXIV
   Mottaghi R, 2014, PROC CVPR IEEE, P891, DOI 10.1109/CVPR.2014.119
   Neuhold G, 2017, IEEE I CONF COMP VIS, P5000, DOI 10.1109/ICCV.2017.534
   Noh H, 2015, IEEE I CONF COMP VIS, P1520, DOI 10.1109/ICCV.2015.178
   Peng G, 2021, NEURAL PROCESS LETT, V53, P4177, DOI 10.1007/s11063-021-10587-7
   Peng X, 2016, LECT NOTES COMPUT SC, V9905, P38, DOI 10.1007/978-3-319-46448-0_3
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Roy K, 2022, VISUAL COMPUT, V38, P2801, DOI 10.1007/s00371-021-02157-8
   Sun K., 2019, ARXIV
   Sun Y., 2021, ARXIV
   Sun Yutao, 2022, arXiv
   Takikawa T, 2019, IEEE I CONF COMP VIS, P5228, DOI 10.1109/ICCV.2019.00533
   Tao A., 2020, Arxiv
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang SL, 2018, PROC CVPR IEEE, P2589, DOI 10.1109/CVPR.2018.00274
   Wu H., 2019, ARXIV
   Xiong YW, 2019, PROC CVPR IEEE, P8810, DOI 10.1109/CVPR.2019.00902
   Yang MK, 2018, PROC CVPR IEEE, P3684, DOI 10.1109/CVPR.2018.00388
   Yu F., 2015, ARXIV
   Yuan Y., 2019, arXiv
   Yuhui Yuan, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P489, DOI 10.1007/978-3-030-58610-2_29
   Zagoruyko S., 2016, ARXIV
   Zhang C, 2019, PROC CVPR IEEE, P5212, DOI 10.1109/CVPR.2019.00536
   Zhang D, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6933, DOI 10.1109/ICCV48922.2021.00687
   Zhang DW., 2020, arXiv
   Zhang H, 2019, PROC CVPR IEEE, P548, DOI 10.1109/CVPR.2019.00064
   Zhang H, 2018, PROC CVPR IEEE, P7151, DOI 10.1109/CVPR.2018.00747
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zheng SX, 2021, PROC CVPR IEEE, P6877, DOI 10.1109/CVPR46437.2021.00681
   Zhu Y, 2019, PROC CVPR IEEE, P8848, DOI 10.1109/CVPR.2019.00906
   Zhu Z, 2019, IEEE I CONF COMP VIS, P593, DOI 10.1109/ICCV.2019.00068
NR 79
TC 0
Z9 0
U1 2
U2 14
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2024
VL 40
IS 1
BP 373
EP 391
DI 10.1007/s00371-023-02787-0
EA FEB 2023
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IX5B8
UT WOS:000933559800002
DA 2024-07-18
ER

PT J
AU Hijam, D
   Saharia, S
AF Hijam, Deena
   Saharia, Sarat
TI A multilevel recognition of Meitei Mayek handwritten characters using
   fusion of features strategy
SO VISUAL COMPUTER
LA English
DT Article
DE Fused feature; Convolutional neural network; Meitei Mayek; Multilevel;
   Handwritten character recognition
ID DEEP; CLASSIFICATION; EXTRACTION; NETWORKS
AB Handwritten character recognition (HCR) is a challenging task because of high intra-class dissimilarity and high inter-class similarity among the character images. Therefore, the features that are used to represent the character images should be such that they maximize the inter-class variations and minimize intra-class variations. In this paper, a multilevel approach for recognition of Meitei Mayek handwritten characters using fusion of features strategy is presented. The proposed methodology has two levels of recognition. The first level employs a simple CNN. A method to identify the structurally similar character classes based on the probability values of the softmax function is also proposed. The fused feature set is employed in the second-level recognition to distinguish between these character classes, thereby enhancing the discriminative power of the fused feature set by reducing the search space in the second level. The fused feature set is a combination of traditional handcrafted feature descriptors and deep features learned by CNN. The fused feature set is fed to an SVM for second-level recognition. The experimental results show a superior performance of the proposed methodology in identifying the structurally similar character classes. This achieves an overall recognition accuracy of 97.08%, which sets the benchmark on the concerned Meitei Mayek handwritten character dataset. The methodology also shows significant results on MNIST, DIDA and CArDIS datasets.
C1 [Hijam, Deena; Saharia, Sarat] Tezpur Univ, Dept CSE, Tezpur, Assam, India.
C3 Tezpur University
RP Hijam, D (corresponding author), Tezpur Univ, Dept CSE, Tezpur, Assam, India.
EM deenahij@gmail.com; sarat@tezu.ernet.in
OI Hijam, Deena/0000-0002-0552-9857
CR Alkhawaldeh RS, 2022, NEURAL COMPUT APPL, V34, P705, DOI 10.1007/s00521-021-06423-7
   Alkhawaldeh RS, 2021, SOFT COMPUT, V25, P3131, DOI 10.1007/s00500-020-05368-8
   Arica N, 2001, IEEE T SYST MAN CY C, V31, P216, DOI 10.1109/5326.941845
   Athiwaratkun B., 2015, ARXIV
   Baker N, 2020, VISION RES, V172, P46, DOI 10.1016/j.visres.2020.04.003
   Barat C, 2016, PATTERN RECOGN, V54, P104, DOI 10.1016/j.patcog.2016.01.007
   Bishop C. M., 1995, NEURAL NETWORKS PATT
   Chahla C, 2020, INT J PATTERN RECOGN, V34, DOI 10.1142/S0218001420550095
   Cimpoi M., 2014, ARXIV
   Daoud MI, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20236838
   Filali Y, 2020, MULTIMED TOOLS APPL, V79, P31219, DOI 10.1007/s11042-020-09637-4
   Fujieda S., 2018, ARXIV
   Golrizkhatami Z, 2018, EXPERT SYST APPL, V114, P54, DOI 10.1016/j.eswa.2018.07.030
   Gong YC, 2014, LECT NOTES COMPUT SC, V8695, P392, DOI 10.1007/978-3-319-10584-0_26
   Hagerty JR, 2019, IEEE J BIOMED HEALTH, V23, P1385, DOI 10.1109/JBHI.2019.2891049
   He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38
   He KM, 2014, LECT NOTES COMPUT SC, V8691, P346, DOI [arXiv:1406.4729, 10.1007/978-3-319-10578-9_23]
   Hijam D., 2018, 2018 4 INT C COMP CO, P1
   Hijam D, 2022, VISUAL COMPUT, V38, P525, DOI 10.1007/s00371-020-02032-y
   HUANG G, 2017, PROC CVPR IEEE, P2261, DOI DOI 10.1109/CVPR.2017.243
   Hussain, 2011, CONCISE INTRO WAVELE
   Kusetogullari H, 2021, BIG DATA RES, V23, DOI 10.1016/j.bdr.2020.100182
   Kutlu H, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19091992
   Lai ZF, 2018, COMPUT INTEL NEUROSC, V2018, DOI 10.1155/2018/2061516
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Li QF, 2020, PROC CVPR IEEE, P7243, DOI 10.1109/CVPR42600.2020.00727
   Li SL, 2019, PHYS MED BIOL, V64, DOI 10.1088/1361-6560/ab326a
   Mangai UG, 2010, IETE TECH REV, V27, P293, DOI 10.4103/0256-4602.64604
   Nanni L, 2017, PATTERN RECOGN, V71, P158, DOI 10.1016/j.patcog.2017.05.025
   Nongmeikapam K, 2019, ACM T ASIAN LOW-RESO, V18, DOI 10.1145/3309497
   Ptucha R, 2019, PATTERN RECOGN, V88, P604, DOI 10.1016/j.patcog.2018.12.017
   Qu XW, 2018, PATTERN RECOGN LETT, V111, P9, DOI 10.1016/j.patrec.2018.04.001
   Rajinikanth V, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10103429
   Ravichandran D., 2016, INT J ADV COMPUTER E, V5, P1
   Razavian AS, 2014, IEEE COMPUT SOC CONF, P512, DOI 10.1109/CVPRW.2014.131
   Roy S, 2017, PATTERN RECOGN LETT, V90, P15, DOI 10.1016/j.patrec.2017.03.004
   Saba T, 2019, J MED SYST, V43, DOI 10.1007/s10916-019-1455-6
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474
   Sarkhel R, 2017, PATTERN RECOGN, V71, P78, DOI 10.1016/j.patcog.2017.05.022
   Sharif M, 2020, PATTERN RECOGN LETT, V129, P150, DOI 10.1016/j.patrec.2019.11.017
   Sharif SMA, 2016, INT C COMP ELEC ENG, P463, DOI 10.1109/ICECE.2016.7853957
   Singh A, 2020, SENS IMAGING, V21, DOI 10.1007/s11220-020-00288-1
   Sulaiman A, 2019, IEEE ACCESS, V7, P91772, DOI 10.1109/ACCESS.2019.2927286
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Tan MX, 2019, PR MACH LEARN RES, V97
   Tang YB, 2018, IEEE T MULTIMEDIA, V20, P2276, DOI 10.1109/TMM.2018.2802644
   Tripathi S, 2020, MULTIMED TOOLS APPL, V79, P34931, DOI 10.1007/s11042-020-08891-w
   UFL M.I.L, 2004, INTR DISCR WAV TRANS
   Vorugunti CS, 2020, NEUROCOMPUTING, V409, P157, DOI 10.1016/j.neucom.2020.05.072
   Wang ZQ, 2019, IEEE ACCESS, V7, P105146, DOI 10.1109/ACCESS.2019.2892795
   Williams T., 2018, J Softw Eng Appl, V11, P69, DOI [DOI 10.4236/JSEA.2018.112004, 10.4236/jsea.2018.112004]
   Xiao XF, 2017, PATTERN RECOGN, V72, P72, DOI 10.1016/j.patcog.2017.06.032
   Yang B, 2015, IEEE I CONF COMP VIS, P82, DOI 10.1109/ICCV.2015.18
   Yang WX, 2016, PATTERN RECOGN, V58, P190, DOI 10.1016/j.patcog.2016.04.007
   Yavariabdi A, 2022, IEEE ACCESS, V10, P55338, DOI 10.1109/ACCESS.2022.3175197
   Zhang TL, 2016, INT J PATTERN RECOGN, V30, DOI 10.1142/S0218001416550041
   Zhang XY, 2017, PATTERN RECOGN, V61, P348, DOI 10.1016/j.patcog.2016.08.005
NR 57
TC 0
Z9 0
U1 1
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2024
VL 40
IS 1
BP 211
EP 225
DI 10.1007/s00371-023-02776-3
EA JAN 2023
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IX5B8
UT WOS:000922469400002
DA 2024-07-18
ER

PT J
AU Wang, SB
   Gao, ZD
   Liu, D
AF Wang, Shibin
   Gao, Zidiao
   Liu, Dong
TI Swin-GAN: generative adversarial network based on shifted windows
   transformer architecture for image generation
SO VISUAL COMPUTER
LA English
DT Article
DE GAN; Transformer; Self-attention; Image generation
AB It is well known that every successful generative adversarial network (GAN) relies on the convolutional neural networks (CNN)-based generators and discriminators. However, CNN cannot process the long-range dependencies because its convolution operator has a local receptive field, which can bring some issues to GAN, such as the optimization, the loss of feature resolution and the fine details. To meet the problem of long-term dependence, we propose a GAN model based on shifted windows Transformer architecture, called Swin-GAN, in which the CNN architecture is replaced by Transformer. In our model, we build a memory-friendly generator based on the shifted window attention mechanism to gradually increase the resolution of feature maps at each stage. Another, we build a multi-scale discriminator to split the image into patches of different sizes as the input at different stages, which can achieve the balance between capturing global contextual semantic information and local detailed features. To further improve the fidelity and stability, we use the techniques such as data enhancement, layer normalization and relative position coding in our model. Compared with the current schemes, the experimental results show that our scheme has better performance, fewer parameters and lower computational cost. Specifically, Params value of Swin-GAN model is 30.254M, and Floating-Point Operations Per Second (FLOPs) value is 4.086G. Inception Score (IS) is 9.04 and Frechet Inception Distance (FID) is 9.23 in CIFAR-10.
C1 [Wang, Shibin; Gao, Zidiao; Liu, Dong] Henan Normal Univ, Sch Comp & Informat Engn, Xinxiang 453007, Henan, Peoples R China.
C3 Henan Normal University
RP Gao, ZD (corresponding author), Henan Normal Univ, Sch Comp & Informat Engn, Xinxiang 453007, Henan, Peoples R China.
EM wangshibin@htu.edu.cn; 2008283060@stu.htu.edu.cn; liudong@htu.edu.cn
RI Wang, Shibin/AAH-1793-2019
FU Key Scientific Research Project of Colleges and Universities in Henan
   Province [20A520021]; National Natural Science Foundation of China
   [62072160]; Science Foundation of Distinguished Young Scholars of Henan
   Normal University
FX This work was supported by the Key Scientific Research Project of
   Colleges and Universities in Henan Province in 2020 (Grant No.
   20A520021), the National Natural Science Foundation of China (Grant No.
   62072160) and the Science Foundation of Distinguished Young Scholars of
   Henan Normal University.
CR Aila T, 2020, P ADV NEUR INF PROC, P12104, DOI DOI 10.48550/ARXIV.2006.06676
   Al-Rfou R, 2019, AAAI CONF ARTIF INTE, P3159
   [Anonymous], 2011, P 4 INT C ART INT ST
   Arjovsky M, 2017, PR MACH LEARN RES, V70
   Ba J. L., 2016, LAYER NORMALIZATION, DOI DOI 10.48550/ARXIV.1607.06450
   Che T, 2016, ARXIV
   Denton E, 2015, ADV NEUR IN, V28
   Dosovitskiy A, 2021, arXiv, DOI [10.48550/ARXIV.2010.11929, DOI 10.48550/ARXIV.2010.11929]
   Gong XY, 2019, IEEE I CONF COMP VIS, P3223, DOI 10.1109/ICCV.2019.00332
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Hensel M, 2017, ADV NEUR IN, V30
   Ioffe S., 2015, P INT C MACH LEARN, VVolume 1, P448, DOI DOI 10.48550/ARXIV.1502.03167
   Gulrajani I, 2017, ADV NEUR IN, V30
   Jiang YF, 2021, ADV NEUR IN
   Jolicoeur-Martineau A., 2018, The relativistic discriminator: A key element missing from standard GAN
   Karnewar Animesh, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7796, DOI 10.1109/CVPR42600.2020.00782
   Karras T., 2018, arXiv, DOI [10.48550/arXiv.1710.10196, DOI 10.48550/ARXIV.1710.10196]
   Karras T, 2021, IEEE T PATTERN ANAL, V43, P4217, DOI 10.1109/TPAMI.2020.2970919
   Kurach K, 2019, PR MACH LEARN RES, V97
   Li CL, 2017, ADV NEUR IN, V30
   Li XH, 2021, ISPRS J PHOTOGRAMM, V179, P14, DOI 10.1016/j.isprsjprs.2021.07.007
   Li Y., 2020, ARXIV
   Li YJ, 2019, IEEE I CONF COMP VIS, P7918, DOI 10.1109/ICCV.2019.00801
   Lin M, 2014, 2014 INTERNATIONAL CONFERENCE ON MEDICAL BIOMETRICS (ICMB 2014), P1, DOI 10.1109/ICMB.2014.8
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Mao XD, 2017, IEEE I CONF COMP VIS, P2813, DOI 10.1109/ICCV.2017.304
   Mescheder L, 2018, PR MACH LEARN RES, V80
   Miyato T., 2018, ARXIV
   Molchanov P., 2016, arXiv
   Nozawa N, 2022, VISUAL COMPUT, V38, P1317, DOI 10.1007/s00371-020-02024-y
   Radford A., 2015, ARXIV
   Rao J, 2022, VIRTUAL PHYS PROTOTY, V17, P1047, DOI 10.1080/17452759.2022.2086142
   Rawassizadeh R, 2016, IEEE T KNOWL DATA EN, V28, P3098, DOI 10.1109/TKDE.2016.2592527
   Roth K, 2017, ADV NEUR IN, V30
   Hussin SHS, 2021, IEEE ACCESS, V9, P13857, DOI 10.1109/ACCESS.2021.3051723
   Salimans T, 2016, ADV NEUR IN, V29
   Santurkar S, 2018, ADV NEUR IN, V31
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Tasar O, 2020, IEEE T GEOSCI REMOTE, V58, P7178, DOI 10.1109/TGRS.2020.2980417
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang XD, 2023, VISUAL COMPUT, V39, P5451, DOI 10.1007/s00371-022-02671-3
   Wang Y, 2022, VISUAL COMPUT, V38, P1915, DOI 10.1007/s00371-021-02254-8
   Zhang H, 2019, PR MACH LEARN RES, V97
   Zhang SY, 2019, VISUAL COMPUT, V35, P1157, DOI 10.1007/s00371-019-01691-w
   Zhang Y., 2022, VISUAL COMPUT, P1
NR 45
TC 2
Z9 2
U1 4
U2 23
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2023
VL 39
IS 12
BP 6085
EP 6095
DI 10.1007/s00371-022-02714-9
EA NOV 2022
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA X7HK1
UT WOS:000880493200001
DA 2024-07-18
ER

PT J
AU Wen, XJ
   Feng, X
   Li, P
   Chen, WF
AF Wen, Xiongjun
   Feng, Xin
   Li, Ping
   Chen, Wenfang
TI Cross-modality collaborative learning identified pedestrian
SO VISUAL COMPUTER
LA English
DT Article
DE Pedestrian re-identification; Cross modality; Collaborative learning;
   Refined features; Multi-source features; Information fusion
ID PERSON REIDENTIFICATION
AB Cross-modal pedestrian re-identification is a key technology to realize all-weather intelligent video surveillance system. This technology is designed to match the visible light image and infrared image of a pedestrian with a specific identity in a non-overlapping camera scene, so it faces huge intra-class changes and modal differences. Existing methods are difficult to solve these two difficulties, which is largely due to the lack of effective mining of feature discrimination and the full use of multi-source heterogeneous information. In view of the above shortcomings, a refined multi-source feature collaborative network is designed by the collaborative learning method, and multiple complementary features are extracted for information fusion, the learning ability of the network is improved. Multi-scale and multi-level features are extracted from the backbone convolutional network, and the refined feature collaborative learning is realized; the discriminative ability of features is enhanced to deal with intra-class changes. A modal sharing and unique feature collaboration module and a cross-modal human semantic self-supervision module are designed to achieve the purpose of multi-source feature collaborative learning, so as to improve the utilization of multi-source heterogeneous image information, and then modal differences are resolved. The validity and advancement of this method are verified on the SYSU-MM01 and RegDB data sets.
C1 [Wen, Xiongjun; Feng, Xin; Li, Ping; Chen, Wenfang] Hunan Int Econ Univ, Sch Informat & Mech Engn, Changsha 410205, Hunan, Peoples R China.
RP Feng, X (corresponding author), Hunan Int Econ Univ, Sch Informat & Mech Engn, Changsha 410205, Hunan, Peoples R China.
EM 2801597781@qq.com
RI Lin, Kuan-Yu/JXM-6653-2024; zhou, chen/KBC-4023-2024; Xi,
   Yang/KEH-5204-2024; xu, lingzhi/JVZ-8748-2024; ying, liu/KEI-0478-2024
FU First-class course in Hunan Province project [[2021] 322, 167]; Hunan
   University Student Innovation and Entrepreneurship Training Program:
   Teaching Reform Research Proiec: Xiangwalingvuan [2022] [[2022] 174,
   4531 [2021] 197, 3281, 64]
FX This work was supported by First-class course in Hunan Province project
   ([2021] 322, No.167); Hunan University Student Innovation and
   Entrepreneurship Training Program: ([2022] 174, No. 4531 [2021] 197. No.
   3281): Teaching Reform Research Proiec: Xiangwalingvuan [2022] No. 64.
CR Bai S, 2019, PROC CVPR IEEE, P740, DOI 10.1109/CVPR.2019.00083
   Baxter J., 1995, Proceedings of the Eighth Annual Conference on Computational Learning Theory, P311, DOI 10.1145/225298.225336
   [贲晛烨 Ben Xianye], 2012, [模式识别与人工智能, Pattern Recognition and Artificial Intelligence], V25, P71
   Caruana R, 1997, MACH LEARN, V28, P41, DOI 10.1023/A:1007379606734
   Chen DP, 2018, LECT NOTES COMPUT SC, V11220, P56, DOI 10.1007/978-3-030-01270-0_4
   Dai J, 2019, IEEE T IMAGE PROCESS, V28, P1366, DOI 10.1109/TIP.2018.2878505
   Dai PY, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P677
   Dean J., 2015, NIPS DEEP LEARNING R
   Fan Caixia, 2016, Computer Engineering and Applications, V52, P156, DOI 10.3778/j.issn.1002-8331.1504-0307
   Feng ZX, 2020, IEEE T IMAGE PROCESS, V29, P579, DOI 10.1109/TIP.2019.2928126
   Hao Y, 2019, AAAI CONF ARTIF INTE, P8385
   He K., 2016, PROC CVPR IEEE, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]
   He L., 2020, arXiv
   Hou RB, 2019, PROC CVPR IEEE, P9309, DOI 10.1109/CVPR.2019.00954
   Jia M., 2020, ARXIV
   Li DG, 2020, AAAI CONF ARTIF INTE, V34, P4610
   Liao SC, 2015, IEEE I CONF COMP VIS, P3685, DOI 10.1109/ICCV.2015.420
   Liao SC, 2015, PROC CVPR IEEE, P2197, DOI 10.1109/CVPR.2015.7298832
   Liu HJ, 2020, NEUROCOMPUTING, V398, P11, DOI 10.1016/j.neucom.2020.01.089
   Luo H, 2020, IEEE T MULTIMEDIA, V22, P2597, DOI 10.1109/TMM.2019.2958756
   Nguyen DT, 2017, SENSORS-BASEL, V17, DOI 10.3390/s17030605
   Seokeon Choi, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10254, DOI 10.1109/CVPR42600.2020.01027
   Song CF, 2018, PROC CVPR IEEE, P1179, DOI 10.1109/CVPR.2018.00129
   Song GC, 2018, ADV NEUR IN, V31
   Song Lili, 2020, Computer Engineering and Applications, V56, P158, DOI 10.3778/j.issn.1002-8331.1904-0235
   Sun YF, 2019, PROC CVPR IEEE, P393, DOI 10.1109/CVPR.2019.00048
   Sun YF, 2018, LECT NOTES COMPUT SC, V11208, P501, DOI 10.1007/978-3-030-01225-0_30
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Wang GA, 2019, IEEE I CONF COMP VIS, P3622, DOI 10.1109/ICCV.2019.00372
   Wang GS, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P274, DOI 10.1145/3240508.3240552
   Wang JB, 2020, PROCEEDINGS OF 2020 5TH INTERNATIONAL CONFERENCE ON MULTIMEDIA AND IMAGE PROCESSING (ICMIP 2020), P28, DOI 10.1145/3381271.3381285
   Wang ZX, 2019, PROC CVPR IEEE, P618, DOI 10.1109/CVPR.2019.00071
   Wu AC, 2017, IEEE I CONF COMP VIS, P5390, DOI 10.1109/ICCV.2017.575
   Wu Y, 2019, IEEE T IMAGE PROCESS, V28, P2872, DOI 10.1109/TIP.2019.2891895
   Yan Lu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13376, DOI 10.1109/CVPR42600.2020.01339
   Yang WJ, 2019, PROC CVPR IEEE, P1389, DOI 10.1109/CVPR.2019.00148
   Ye M, 2020, ARXIV
   Ye M, 2021, IEEE T INF FOREN SEC, V16, P728, DOI 10.1109/TIFS.2020.3001665
   Ye M, 2020, IEEE T IMAGE PROCESS, V29, P9387, DOI 10.1109/TIP.2020.2998275
   Ye M, 2020, IEEE T INF FOREN SEC, V15, P407, DOI 10.1109/TIFS.2019.2921454
   Ye M, 2018, AAAI CONF ARTIF INTE, P7501
   Zhu YX, 2020, NEUROCOMPUTING, V386, P97, DOI 10.1016/j.neucom.2019.12.100
   Zhang X, 2017, ARXIV
   Zhao YB, 2019, IET IMAGE PROCESS, V13, P2897, DOI 10.1049/iet-ipr.2019.0699
   Zheng L, 2016, LECT NOTES COMPUT SC, V9910, P868, DOI 10.1007/978-3-319-46466-4_52
   Zheng Liang, 2016, ARXIV
   Zhu XT, 2018, IEEE T IMAGE PROCESS, V27, P2286, DOI 10.1109/TIP.2017.2740564
NR 47
TC 3
Z9 3
U1 0
U2 20
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2023
VL 39
IS 9
BP 4117
EP 4132
DI 10.1007/s00371-022-02579-y
EA SEP 2022
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA Q8PV6
UT WOS:000859622400001
DA 2024-07-18
ER

PT J
AU Grimmeisen, B
   Chegini, M
   Theissler, A
AF Grimmeisen, Benedikt
   Chegini, Mohammad
   Theissler, Andreas
TI VisGIL: machine learning-based visual guidance for interactive labeling
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Visual-interactive labeling; User guidance; Active learning; Visual
   analytics; Machine learning; Classification
ID HEALTH-CARE; REDUCTION; CLASSIFICATION; POWER
AB Labeling of datasets is an essential task for supervised and semi-supervised machine learning. Model-based active learning and user-based interactive labeling are two complementary strategies for this task. We propose VisGIL which, using visual cues, guides the user in the selection of instances to label based on utility measures deduced from an active learning model. We have implemented the approach and conducted a qualitative and quantitative user study and a think-aloud test. The studies reveal that guidance by visual cues improves the trained model's accuracy, reduces the time needed to label the dataset, and increases users' confidence while selecting instances. Furthermore, we gained insights regarding how guidance impacts user behavior and how the individual visual cues contribute to user guidance. A video of the approach is available: https://ml-and-vis.org/visgil/.
C1 [Grimmeisen, Benedikt; Theissler, Andreas] Aalen Univ Appl Sci, D-73430 Aalen, Germany.
   [Grimmeisen, Benedikt] Capgemini, D-70771 Leinfelden Echterdingen, Germany.
   [Chegini, Mohammad] Adidas Runtast, A-1150 Vienna, Austria.
C3 Hochschule Aalen; Capgemini
RP Grimmeisen, B (corresponding author), Aalen Univ Appl Sci, D-73430 Aalen, Germany.; Grimmeisen, B (corresponding author), Capgemini, D-70771 Leinfelden Echterdingen, Germany.
EM benedikt.grimmeisen@hs-aalen.de
RI Theissler, Andreas/AAS-2182-2021
OI Theissler, Andreas/0000-0003-0746-0424
FU Projekt DEAL
FX Open Access funding enabled and organized by Projekt DEAL.
CR Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265
   Ali M, 2019, VISUAL COMPUT, V35, P1013, DOI 10.1007/s00371-019-01673-y
   Amershi S, 2014, AI MAG, V35, P105, DOI 10.1609/aimag.v35i4.2513
   [Anonymous], 2009, Technical report
   Ash J.T., 2019, WARM STARTING NEURAL
   Baumgartl H, 2019, ACEX 2019 P
   Beil D., 2020, Proceedings of the 13th International Symposium on Visual Information Communication and Interaction, P1, DOI DOI 10.1145/3430036.3430060
   Benedikt Grimmeisen, 2020, VINCI 20 P 13 INT S, DOI DOI 10.1145/3430036.3430058
   Bernard J., 2015, P 2015 WORKSH VIS AN, DOI [10.1145/2836034.2836035, DOI 10.1145/2836034.2836035]
   Bernard J, 2018, VISUAL COMPUT, V34, P1189, DOI 10.1007/s00371-018-1500-3
   Bernard J, 2018, COMPUT GRAPH FORUM, V37, P121, DOI 10.1111/cgf.13406
   Bernard J, 2018, IEEE T VIS COMPUT GR, V24, P298, DOI 10.1109/TVCG.2017.2744818
   Boy J, 2016, IEEE T VIS COMPUT GR, V22, P639, DOI 10.1109/TVCG.2015.2467201
   Breiman L, 2001, MACH LEARN, V45, P5, DOI 10.1023/A:1010933404324
   Cakmak M, 2010, IEEE T AUTON MENT DE, V2, P108, DOI 10.1109/TAMD.2010.2051030
   Ceneda D, 2017, IEEE T VIS COMPUT GR, V23, P111, DOI 10.1109/TVCG.2016.2598468
   Chegini M, 2019, VIS INFORM, V3, P9, DOI 10.1016/j.visinf.2019.03.002
   Chegini M, 2020, FRONT INFORM TECH EL, V21, P524, DOI 10.1631/FITEE.1900549
   Cohen J., 1988, STAT POWER ANAL BEHA
   Collins C, 2018, VIS INFORM, V2, P166, DOI 10.1016/j.visinf.2018.09.003
   Danka T, 2018, Arxiv, DOI arXiv:1805.00979
   Dhillon I. S., 2004, PROC ACM SIGKDD INT, P551, DOI DOI 10.1145/1014052.1014118
   Dy J. G., 2000, Proceedings. KDD-2000. Sixth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, P360, DOI 10.1145/347090.347168
   Fan X, 2019, J VISUAL-JAPAN, V22, P955, DOI 10.1007/s12650-019-00580-7
   Farooq MU, 2022, VISUAL COMPUT, V38, P1553, DOI 10.1007/s00371-021-02088-4
   Fezer E., 2020, P 13 INT S VISUAL IN, DOI [10.1145/3430036.3430066, DOI 10.1145/3430036.3430066]
   Fisher B, 2005, Illuminating the Path: An R&D) Agenda for Visual Analytics
   Fu YF, 2013, KNOWL INF SYST, V35, P249, DOI 10.1007/s10115-012-0507-8
   Gillick D., 2006, Berkley, V8
   Han Q., 2018, LEIPZ S VIS APPL, P1
   He TX, 2014, SCI WORLD J, DOI 10.1155/2014/827586
   Heimerl F, 2012, IEEE T VIS COMPUT GR, V18, P2839, DOI 10.1109/TVCG.2012.277
   Höferlin B, 2012, IEEE CONF VIS ANAL, P23, DOI 10.1109/VAST.2012.6400492
   Holzinger A, 2019, APPL INTELL, V49, P2401, DOI 10.1007/s10489-018-1361-5
   Hu QW, 2019, BIOCOMPUT-PAC SYM, P362
   INSELBERG A, 1990, PROCEEDINGS OF THE FIRST IEEE CONFERENCE ON VISUALIZATION - VISUALIZATION 90, P361, DOI 10.1109/VISUAL.1990.146402
   Jiang BY, 2017, IUI'17: PROCEEDINGS OF THE 22ND INTERNATIONAL CONFERENCE ON INTELLIGENT USER INTERFACES, P535, DOI 10.1145/3025171.3025172
   Jiang L, 2019, J VISUAL-JAPAN, V22, P401, DOI 10.1007/s12650-018-0531-1
   Jonsson B., 2019, Exquisitor. Interactive learning at large
   Kucher K, 2017, ACM T INTERACT INTEL, V7, DOI 10.1145/3132169
   LeCun Y., 2010, MNIST HANDWRITTEN DI
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Luo LK, 2019, VISUAL COMPUT, V35, P1869, DOI 10.1007/s00371-018-1580-0
   Nadj M, 2020, KUNSTL INTELL, V34, P131, DOI 10.1007/s13218-020-00634-1
   Norman Don, 2013, The design of everyday things
   Pedregosa F, 2011, J MACH LEARN RES, V12, P2825
   Plotly Technologies Inc, 2015, Collaborative Data Science
   Prisacaru A, 2018, IEEE T COMP PACK MAN, V8, P750, DOI 10.1109/TCPMT.2018.2816259
   Raschka S, 2020, Arxiv, DOI arXiv:2002.04803
   Rawat W, 2017, NEURAL COMPUT, V29, P2352, DOI [10.1162/NECO_a_00990, 10.1162/neco_a_00990]
   Ritter C., 2018, EUROVA EUROVIS, DOI 10.2312/eurova.20181109
   Saary MJ, 2008, J CLIN EPIDEMIOL, V61, P311, DOI 10.1016/j.jclinepi.2007.04.021
   Sacha D, 2017, NEUROCOMPUTING, V268, P164, DOI 10.1016/j.neucom.2017.01.105
   Sarikaya A, 2018, IEEE T VIS COMPUT GR, V24, P402, DOI 10.1109/TVCG.2017.2744184
   Sedlmair M, 2013, IEEE T VIS COMPUT GR, V19, P2634, DOI 10.1109/TVCG.2013.153
   Seifert C., 2010, Proceedings 2010 10th IEEE International Conference on Data Mining Workshops (ICDMW 2010), P418, DOI 10.1109/ICDMW.2010.181
   Settles B, 2008, ADV NEURAL INFORM PR, V21, P1289
   Settles B, 2008, P C EMP METH NAT LAN, P1070, DOI DOI 10.3115/1613715.1613855
   Settles Burr, 2011, JMLR WORKSHOP C P, P1
   Seung H. S., 1992, Proceedings of the Fifth Annual ACM Workshop on Computational Learning Theory, P287, DOI 10.1145/130385.130417
   Sharma PK, 2021, VISUAL COMPUT, V37, P2083, DOI 10.1007/s00371-020-01971-w
   Shneiderman B, 1996, IEEE SYMPOSIUM ON VISUAL LANGUAGES, PROCEEDINGS, P336, DOI 10.1109/VL.1996.545307
   Smilkov Daniel, 2016, Embedding Projector: Interactive Visualization and Interpretation of Embeddings
   Stoiber C., 2019, VISUALIZATION ONBOAR, DOI DOI 10.31219/OSF.IO/C38AB
   Team R.D., 2018, RAPIDS: Collection of Libraries for End to End GPU Data Science
   Theissler Andreas, 2020, Machine Learning and Knowledge Extraction. 4th IFIP TC 5, TC 12, WG 8.4, WG 8.9, WG 12.9. International Cross-Domain Conference, CD-MAKE 2020. Proceedings. Lecture Notes in Computer Science (LNCS 12279), P281, DOI 10.1007/978-3-030-57321-8_16
   Theissler A., 2020, 4 INT WORKSH INT AD
   Theissler A, 2022, KNOWL-BASED SYST, V247, DOI 10.1016/j.knosys.2022.108651
   Theissler A, 2021, RELIAB ENG SYST SAFE, V215, DOI 10.1016/j.ress.2021.107864
   Tuia D, 2011, IEEE J-STSP, V5, P606, DOI 10.1109/JSTSP.2011.2139193
   Van Someren MW, 1994, AcademicPress
   Vendrig J., 2002, P 11 TEXT RETR C TRE
   Verbraeken J, 2020, ACM COMPUT SURV, V53, DOI 10.1145/3377454
   Vollert Simon, 2021, 2021 26th IEEE International Conference on Emerging Technologies and Factory Automation (ETFA ), DOI 10.1109/ETFA45728.2021.9613467
   Walker JS, 2015, VISUAL COMPUT, V31, P1067, DOI 10.1007/s00371-015-1112-0
   Wattenberg M., 2016, Distill, V1, DOI DOI 10.23915/DISTILL.00002
   Wen ZY, 2020, J MACH LEARN RES, V21
   Wen ZY, 2019, IEEE T PARALL DISTR, V30, P2706, DOI 10.1109/TPDS.2019.2920131
   Wiens J, 2018, CLIN INFECT DIS, V66, P149, DOI 10.1093/cid/cix731
   Wu Y, 2006, 2006 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO - ICME 2006, VOLS 1-5, PROCEEDINGS, P529, DOI 10.1109/ICME.2006.262442
   Xiang SX, 2019, IEEE CONF VIS ANAL, P57, DOI [10.1109/vast47406.2019.8986943, 10.1109/VAST47406.2019.8986943]
   Xiao H., 2017, CoRR abs/1708.07747
   Yang LP, 2018, ISPRS INT J GEO-INF, V7, DOI 10.3390/ijgi7020065
   Zahálka J, 2021, IEEE T VIS COMPUT GR, V27, P422, DOI 10.1109/TVCG.2020.3030383
   Zahálka J, 2018, IEEE T MULTIMEDIA, V20, P687, DOI 10.1109/TMM.2017.2755986
   Zhang CL, 2018, INT CON DISTR COMP S, P99, DOI 10.1109/ICDCS.2018.00020
   Zhu Y, 2019, INT J PROD ECON, V211, P22, DOI 10.1016/j.ijpe.2019.01.032
NR 87
TC 0
Z9 0
U1 1
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2022 SEP 25
PY 2022
DI 10.1007/s00371-077-07648-7
EA SEP 2022
PG 23
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 4U2LA
UT WOS:000858630600001
DA 2024-07-18
ER

PT J
AU Yao, X
   Zhang, J
   Chen, RX
   Zhang, D
   Zeng, YF
AF Yao, Xiao
   Zhang, Jia
   Chen, Ruixuan
   Zhang, Dan
   Zeng, Yifeng
TI Weakly supervised graph learning for action recognition in untrimmed
   video
SO VISUAL COMPUTER
LA English
DT Article
DE Action recognition; Weakly supervised; Proposal relations; GCNs
AB Action recognition in real-world scenarios is a challenging task which involves the action localization and classification for untrimmed video. Since the untrimmed video in real scenarios lacks fine annotation, existing supervised learning methods have limited effectiveness and robustness in performance. Moreover, state-of-the-art methods discuss each action proposal individually, ignoring the exploration of semantic relationship between different proposals from continuity of video. To address these issues, we propose a weakly supervised approach to explore the proposal relations using Graph Convolutional Networks (GCNs). Specifically, the method introduces action similarity edges and temporal similarity edges to represent the context semantic relationship between different proposals for graph constructing, and the similarity of action features is used to weakly supervise the spatial semantic relationship between labeled and unlabeled samples to achieve the effective recognition of actions in the video. We validate the effectiveness of the proposed method on public benchmarks for untrimmed video (THUMOS14 and ActivityNet). The experimental results demonstrate that the proposed method in this paper has achieved state-of-the-art results, and achieves better robustness and generalization performance.
C1 [Yao, Xiao; Zhang, Jia; Chen, Ruixuan; Zeng, Yifeng] Hohai Univ, Coll IoT Engn, Nanjing, Peoples R China.
   [Zhang, Dan] Inner Mongolia Normal Univ, Coll Foreign Languages, Hohhot, Peoples R China.
C3 Hohai University; Inner Mongolia Normal University
RP Zhang, J (corresponding author), Hohai Univ, Coll IoT Engn, Nanjing, Peoples R China.
EM 191320010035@hhu.edu.cn
RI Zeng, Yifeng/Z-3331-2019
OI Zeng, Yifeng/0000-0003-2676-8088
FU Fundamental Research Funds for the Central Universities [B200202205,
   2018B47114]; Key Research and Development Program of Jiangsu
   [BK20192004, BE2018004-04]; Guangdong Forestry Science and Technology
   Innovation Project [2020KJCX005]; International Cooperation and
   Exchanges of Changzhou [CZ20200035]; State Key Laboratory of Integrated
   Management of Pest Insects and Rodents [IPM1914]
FX This work was supported by the Fundamental Research Funds for the
   Central Universities B200202205 and 2018B47114, the Key Research and
   Development Program of Jiangsu under grants BK20192004, BE2018004-04,
   Guangdong Forestry Science and Technology Innovation Project under grant
   2020KJCX005, International Cooperation and Exchanges of Changzhou under
   grant CZ20200035, and by the State Key Laboratory of Integrated
   Management of Pest Insects and Rodents under grant IPM1914.
CR Alwassel H, 2018, LECT NOTES COMPUT SC, V11213, P253, DOI 10.1007/978-3-030-01240-3_16
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chao YW, 2018, PROC CVPR IEEE, P1130, DOI 10.1109/CVPR.2018.00124
   Chen J., 2018, ARXIV
   Chuang Gan, 2018, 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition. Proceedings, P5589, DOI 10.1109/CVPR.2018.00586
   Dai XY, 2017, IEEE I CONF COMP VIS, P5727, DOI 10.1109/ICCV.2017.610
   Duan X, 2018, ARXIV
   Fan LJ, 2018, PROC CVPR IEEE, P6016, DOI 10.1109/CVPR.2018.00630
   Gao J., 2017, arXiv
   Gao JY, 2017, IEEE I CONF COMP VIS, P3648, DOI 10.1109/ICCV.2017.392
   Ghanem B., 2018, ARXIV
   Hamilton WL, 2017, ADV NEUR IN, V30
   Heilbron FC, 2016, PROC CVPR IEEE, P1914, DOI 10.1109/CVPR.2016.211
   Hu H, 2018, PROC CVPR IEEE, P3588, DOI 10.1109/CVPR.2018.00378
   Huang W., 2018, ARXIV
   Jiang Y.-G., 2014, THUMOS challenge: Action recognition with a large number of classes
   Kipf TN, 2016, ARXIV
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lee P, 2020, AAAI CONF ARTIF INTE, V34, P11320
   Lin TW, 2018, LECT NOTES COMPUT SC, V11208, P3, DOI 10.1007/978-3-030-01225-0_1
   Lin TW, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P988, DOI 10.1145/3123266.3123343
   Liu DC, 2019, PROC CVPR IEEE, P1298, DOI 10.1109/CVPR.2019.00139
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Min Kyle, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P283, DOI 10.1007/978-3-030-58568-6_17
   Narayan S, 2019, IEEE I CONF COMP VIS, P8678, DOI 10.1109/ICCV.2019.00877
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Shen YT, 2018, LECT NOTES COMPUT SC, V11219, P508, DOI 10.1007/978-3-030-01267-0_30
   Shi BF, 2020, PROC CVPR IEEE, P1006, DOI 10.1109/CVPR42600.2020.00109
   Shou Z, 2018, LECT NOTES COMPUT SC, V11220, P162, DOI 10.1007/978-3-030-01270-0_10
   Shou Z, 2017, PROC CVPR IEEE, P1417, DOI 10.1109/CVPR.2017.155
   Shou Z, 2016, PROC CVPR IEEE, P1049, DOI 10.1109/CVPR.2016.119
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Tan MK, 2015, PROC CVPR IEEE, P4100, DOI 10.1109/CVPR.2015.7299037
   Vaswani A, 2017, ADV NEUR IN, V30
   Vishwakarma S, 2013, VISUAL COMPUT, V29, P983, DOI 10.1007/s00371-012-0752-6
   Wang LM, 2016, LECT NOTES COMPUT SC, V9912, P20, DOI 10.1007/978-3-319-46484-8_2
   Wang LM, 2017, PROC CVPR IEEE, P6402, DOI 10.1109/CVPR.2017.678
   Wang XL, 2018, LECT NOTES COMPUT SC, V11209, P413, DOI 10.1007/978-3-030-01228-1_25
   Xiong Y., 2017, ARXIV
   Xu HJ, 2017, IEEE I CONF COMP VIS, P5794, DOI 10.1109/ICCV.2017.617
   Xu YL, 2019, AAAI CONF ARTIF INTE, P9070
   Yan SJ, 2018, AAAI CONF ARTIF INTE, P7444
   Yang K, 2018, AAAI CONF ARTIF INTE, P7477
   Yu T, 2019, IEEE I CONF COMP VIS, P5521, DOI 10.1109/ICCV.2019.00562
   Yuan Y., 2019, arXiv
   Zeng RH, 2019, IEEE I CONF COMP VIS, P7093, DOI 10.1109/ICCV.2019.00719
   Zhang C, 2021, PROC CVPR IEEE, P16005, DOI 10.1109/CVPR46437.2021.01575
   Zhao Y, 2017, IEEE I CONF COMP VIS, P2933, DOI 10.1109/ICCV.2017.317
   Zhekun Luo, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12374), P729, DOI 10.1007/978-3-030-58526-6_43
NR 49
TC 2
Z9 2
U1 0
U2 10
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2023
VL 39
IS 11
BP 5469
EP 5483
DI 10.1007/s00371-022-02673-1
EA SEP 2022
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA W5SX6
UT WOS:000859612200001
DA 2024-07-18
ER

PT J
AU Chen, H
   Chen, R
   Ma, L
   Li, NN
AF Chen, Hui
   Chen, Rong
   Ma, Long
   Li, Nannan
TI Single-image dehazing via depth-guided deep retinex decomposition
SO VISUAL COMPUTER
LA English
DT Article
DE Image dehazing; Retinex; Deep learning; Image restoration
ID NETWORK
AB In this paper, we explore the problem of single-image haze removal based on retinex model and new deep retinex decomposition architecture. Reformulating dehazing as reverse retinex, we propose a depth-guided retinex decomposition network, which consists of the Decom-Net, with two-branches for the retinex decomposition on the reversed hazy image, and the Guide-Net, with depth information for guiding the estimation of ideal illumination. To promote the accuracy of retinex decomposition, we develop an effective boosted decoder with a fusion attention mechanism to optimize the illumination iteratively, giving rise to a refined reflectance. Additionally, due to the reversible relationship between haze and low-light images, our network could effectively realize dehazing in nighttime. Through sets of experiments on a variety of synthetic and natural images, we validate the effectiveness of the proposed model in haze removal, competitively in terms of visual appearance and metrics, considering both daytime and nighttime cases.
C1 [Chen, Hui; Chen, Rong; Li, Nannan] Dalian Maritime Univ, Coll Informat Sci & Technol, Linghai Rd, Dalian 116026, Liaoning, Peoples R China.
   [Ma, Long] Dalian Univ Technol, DUT RU Int Sch Informat Sci & Engn, Linghai Rd, Dalian 116026, Liaoning, Peoples R China.
C3 Dalian Maritime University; Dalian University of Technology
RP Chen, R; Li, NN (corresponding author), Dalian Maritime Univ, Coll Informat Sci & Technol, Linghai Rd, Dalian 116026, Liaoning, Peoples R China.
EM chenhuidmu@163.com; rchen@dlmu.edu.cn; malone94319@gmail.com;
   nannanli@dlmu.edu.cn
RI ma, long/JHU-2289-2023
FU National Natural Science Foundation of China [61672122]; Natural Science
   Foundation of China [61802045]
FX This work was supported in part by the National Natural Science
   Foundation of China (No. 61672122) and Natural Science Foundation of
   China (61802045).
CR Berman D, 2016, PROC CVPR IEEE, P1674, DOI 10.1109/CVPR.2016.185
   Cai BL, 2016, IEEE T IMAGE PROCESS, V25, P5187, DOI 10.1109/TIP.2016.2598681
   Chen C, 2018, LECT NOTES COMPUT SC, V11215, P3, DOI 10.1007/978-3-030-01252-6_1
   Choi LK, 2015, IEEE T IMAGE PROCESS, V24, P3888, DOI 10.1109/TIP.2015.2456502
   Dong H, 2020, PROC CVPR IEEE, P2154, DOI 10.1109/CVPR42600.2020.00223
   Galdran A, 2018, PROC CVPR IEEE, P8212, DOI 10.1109/CVPR.2018.00857
   Guo XJ, 2017, IEEE T IMAGE PROCESS, V26, P982, DOI 10.1109/TIP.2016.2639450
   He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168
   Jobson DJ, 1997, IEEE T IMAGE PROCESS, V6, P965, DOI 10.1109/83.597272
   Jobson DJ, 1997, IEEE T IMAGE PROCESS, V6, P451, DOI 10.1109/83.557356
   Ju MY, 2021, IEEE T IMAGE PROCESS, V30, P2180, DOI 10.1109/TIP.2021.3050643
   Kim SE, 2020, IEEE T IMAGE PROCESS, V29, P1985, DOI 10.1109/TIP.2019.2948279
   Li BY, 2018, AAAI CONF ARTIF INTE, P7016
   Li BY, 2019, IEEE T IMAGE PROCESS, V28, P492, DOI 10.1109/TIP.2018.2867951
   Li BY, 2017, IEEE I CONF COMP VIS, P4780, DOI 10.1109/ICCV.2017.511
   Li L, 2020, IEEE T IMAGE PROCESS, V29, P5273, DOI 10.1109/TIP.2020.2980173
   Li PY, 2021, IEEE T IMAGE PROCESS, V30, P1100, DOI 10.1109/TIP.2020.3040075
   Li XR, 2023, VISUAL COMPUT, V39, P1307, DOI 10.1007/s00371-022-02407-3
   Li Y, 2015, IEEE I CONF COMP VIS, P226, DOI 10.1109/ICCV.2015.34
   Liu XH, 2019, IEEE I CONF COMP VIS, P7313, DOI 10.1109/ICCV.2019.00741
   Qin X, 2020, AAAI CONF ARTIF INTE, V34, P11908
   Qu YY, 2019, PROC CVPR IEEE, P8152, DOI 10.1109/CVPR.2019.00835
   Rahman ZU, 2004, J ELECTRON IMAGING, V13, P100, DOI 10.1117/1.1636183
   Ren WQ, 2018, PROC CVPR IEEE, P3253, DOI 10.1109/CVPR.2018.00343
   Romano Y, 2015, SIAM J IMAGING SCI, V8, P1187, DOI 10.1137/140990978
   Sakaridis C, 2018, INT J COMPUT VISION, V126, P973, DOI 10.1007/s11263-018-1072-8
   Tan RT, 2008, PROC CVPR IEEE, P2347, DOI 10.1109/cvpr.2008.4587643
   Tarel JP, 2009, IEEE I CONF COMP VIS, P2201, DOI 10.1109/ICCV.2009.5459251
   Zhang H, 2018, PROC CVPR IEEE, P3194, DOI 10.1109/CVPR.2018.00337
   Zhang J, 2017, PROC CVPR IEEE, P5226, DOI 10.1109/CVPR.2017.555
   Zhang JW, 2011, VISUAL COMPUT, V27, P749, DOI 10.1007/s00371-011-0569-8
   Zhang J, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2355, DOI 10.1145/3394171.3413763
   Zhang J, 2014, IEEE IMAGE PROC, P4557, DOI 10.1109/ICIP.2014.7025924
   Zhang SD, 2020, VISUAL COMPUT, V36, P1797, DOI 10.1007/s00371-019-01774-8
   Zhu QS, 2015, IEEE T IMAGE PROCESS, V24, P3522, DOI 10.1109/TIP.2015.2446191
NR 35
TC 4
Z9 4
U1 15
U2 47
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2023
VL 39
IS 11
BP 5279
EP 5291
DI 10.1007/s00371-022-02659-z
EA SEP 2022
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA W5SX6
UT WOS:000852356100002
DA 2024-07-18
ER

PT J
AU Wang, PF
   Wang, ML
   He, DZ
AF Wang, Pengfei
   Wang, Minglian
   He, Dongzhi
TI Multi-scale feature pyramid and multi-branch neural network for person
   re-identification
SO VISUAL COMPUTER
LA English
DT Article
DE Person re-identification; Feature pyramid network; Multi-scale feature;
   Multi-branch network
ID ALIGNMENT; GAN
AB The key to person re-identification (Re-ID) is how to extract a representative and robust depth feature of the person, which requires the model to pay attention to both global contour information and local detailed features. To extract more representative features, an effective method is to build a multi-branch deep model by duplicating the backbone structure. This method usually severely increases the computational cost, and continuous convolution and pooling operations cause the loss of detailed information. This paper proposes a lightweight multi-scale feature pyramid structure, which extracts features from network layers of different scales and aggregates them to supplement spatial detail information. Meanwhile, this paper adopts a pair of complementary attention modules, which pay attention to the discriminative areas of person features by focusing on channel aggregation and position perception, respectively. In addition, this paper proposes a multi-level orthogonal regularization method to further enhance the diversity of features. The experimental results show that the mAP of this method on the Market1501 dataset reaches 91.6%. The proposed method outperforms state-of-the-art methods and along with lower complexity.
C1 [Wang, Pengfei; Wang, Minglian; He, Dongzhi] Beijing Univ Technol, Fac Informat Technol, Beijing, Peoples R China.
C3 Beijing University of Technology
RP He, DZ (corresponding author), Beijing Univ Technol, Fac Informat Technol, Beijing, Peoples R China.
EM victor@bjut.edu.cn
RI Wang, Minglian/D-3159-2014; Wang, Pengfei/Y-4426-2018
FU National key research and development plan project [2016YFB1200602-37]
FX National key research and development plan project, 2016YFB1200602-37,
   Minglian Wang.
CR Cai H., 2020, 2019 IEEECVF C COMPU
   Chen BH, 2019, IEEE I CONF COMP VIS, P371, DOI 10.1109/ICCV.2019.00046
   Chen JX, 2014, INT C PATT RECOG, P1657, DOI 10.1109/ICPR.2014.292
   Chen TL, 2019, IEEE I CONF COMP VIS, P8350, DOI 10.1109/ICCV.2019.00844
   Chen ZC, 2021, VISUAL COMPUT, V37, P685, DOI 10.1007/s00371-020-01880-y
   Chi S., 2017, 2017 IEEE INT C COMP
   Dai ZZ, 2019, IEEE I CONF COMP VIS, P3690, DOI 10.1109/ICCV.2019.00379
   Gou M., 2014, COMPUTER VISION ECCV
   Guillaumin M, 2009, IEEE I CONF COMP VIS, P498, DOI 10.1109/ICCV.2009.5459197
   Guo JY, 2019, IEEE I CONF COMP VIS, P3641, DOI 10.1109/ICCV.2019.00374
   He K., 2016, PROC CVPR IEEE, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]
   Hou RB, 2019, PROC CVPR IEEE, P7176, DOI 10.1109/CVPR.2019.00735
   Huang HJ, 2020, IEEE T IMAGE PROCESS, V29, P7468, DOI 10.1109/TIP.2020.3003442
   Huang HJ, 2018, PROC CVPR IEEE, P5098, DOI 10.1109/CVPR.2018.00535
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li H., COMBINED DEPTH SPACE
   Li W, 2018, PROC CVPR IEEE, P2285, DOI 10.1109/CVPR.2018.00243
   Li W, 2014, PROC CVPR IEEE, P152, DOI 10.1109/CVPR.2014.27
   Li YL, 2021, PROC CVPR IEEE, P2897, DOI 10.1109/CVPR46437.2021.00292
   Liu H, 2017, IEEE T IMAGE PROCESS, V26, P3492, DOI 10.1109/TIP.2017.2700762
   Liu XP, 2019, NEUROCOMPUTING, V364, P108, DOI 10.1016/j.neucom.2019.07.063
   Luo H, 2019, IEEE COMPUT SOC CONF, P1487, DOI 10.1109/CVPRW.2019.00190
   Ni X., 2021, 2020 25 INT C PATTER
   Ozay M., 2016, OPTIMIZATION SUBMANI
   Quan RJ, 2019, IEEE I CONF COMP VIS, P3749, DOI 10.1109/ICCV.2019.00385
   Ristani E, 2016, LECT NOTES COMPUT SC, V9914, P17, DOI 10.1007/978-3-319-48881-3_2
   Song CF, 2018, PROC CVPR IEEE, P1179, DOI 10.1109/CVPR.2018.00129
   Sun YF, 2018, LECT NOTES COMPUT SC, V11208, P501, DOI 10.1007/978-3-030-01225-0_30
   Szegedy Christian, 2015, IEEE C COMP VIS PATT, DOI [10.1109/cvpr.2015.7298594, DOI 10.1109/CVPR.2015.7298594]
   van der Walt S, 2011, COMPUT SCI ENG, V13, P22, DOI 10.1109/MCSE.2011.37
   Wang C, 2020, NEUROCOMPUTING, V382, P64, DOI 10.1016/j.neucom.2019.11.062
   Wang C, 2018, LECT NOTES COMPUT SC, V11208, P384, DOI 10.1007/978-3-030-01225-0_23
   Wang GS, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P274, DOI 10.1145/3240508.3240552
   Wang PY, 2021, IEEE T IMAGE PROCESS, V30, P2908, DOI 10.1109/TIP.2021.3055952
   Wei LH, 2019, IEEE T MULTIMEDIA, V21, P986, DOI 10.1109/TMM.2018.2870522
   Wei LH, 2018, PROC CVPR IEEE, P79, DOI 10.1109/CVPR.2018.00016
   Wen YD, 2016, LECT NOTES COMPUT SC, V9911, P499, DOI 10.1007/978-3-319-46478-7_31
   Wu D, 2021, IEEE T EM TOP COMP I, V5, P70, DOI 10.1109/TETCI.2020.3034606
   Xia B, 2019, IEEE I CONF COMP VIS, P3759, DOI 10.1109/ICCV.2019.00386
   Xie JH, 2022, VISUAL COMPUT, V38, P2515, DOI 10.1007/s00371-021-02127-0
   Xuan Z., 2017, ARXIV171108184
   Yang WJ, 2019, PROC CVPR IEEE, P1389, DOI 10.1109/CVPR.2019.00148
   Ye M, 2022, IEEE T PATTERN ANAL, V44, P2872, DOI 10.1109/TPAMI.2021.3054775
   Zhang L., 2020, BRANCH COOPERATIVE O
   Zhang SF, 2020, IEEE SIGNAL PROC LET, V27, P850, DOI 10.1109/LSP.2020.2994815
   Zhang ZZ, 2020, PROC CVPR IEEE, P3183, DOI 10.1109/CVPR42600.2020.00325
   Zhao HY, 2017, PROC CVPR IEEE, P907, DOI 10.1109/CVPR.2017.103
   Zhao R, 2013, IEEE I CONF COMP VIS, P2528, DOI 10.1109/ICCV.2013.314
   Zheng F., 2019, 2019 IEEECVF C COMPU
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zheng Liang., 2016, Person re-identification: Past, present and future
   Zheng M, 2019, PROC CVPR IEEE, P5728, DOI 10.1109/CVPR.2019.00588
   Zheng ZD, 2019, PROC CVPR IEEE, P2133, DOI 10.1109/CVPR.2019.00224
   Zhou KY, 2019, IEEE I CONF COMP VIS, P3701, DOI 10.1109/ICCV.2019.00380
NR 54
TC 1
Z9 1
U1 1
U2 31
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2023
VL 39
IS 10
BP 5185
EP 5197
DI 10.1007/s00371-022-02653-5
EA SEP 2022
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA T1GK8
UT WOS:000849158400001
DA 2024-07-18
ER

PT J
AU Tang, JN
   Zhang, F
   Ni, H
AF Tang, Jianeng
   Zhang, Feng
   Ni, Hui
TI A novel fast image encryption scheme based on a new one-dimensional
   compound sine chaotic system
SO VISUAL COMPUTER
LA English
DT Article
DE Compound chaotic system; Chaotic maps; Image encryption; SHA-256
   function; Security analysis
ID SEMI-TENSOR PRODUCT; APPROXIMATE ENTROPY; ALGORITHM; NETWORKS; MATRIX;
   PERMUTATION
AB In the paper, a new one-dimensional (1-D) compound Sine chaotic system (CSCS) is first proposed. Then new chaotic maps are generated by the CSCS. And four novel generated maps are used for the illustration about the CSCS. Secondly, the results of performance analysis show that the four maps have large Lyapunov exponents and high complexity. Thirdly, a high-efficiency image encryption scheme is proposed by employing two of the four new produced chaotic maps. In the proposed encryption scheme, the simplest diffusion operation is used. And we use a variety of scrambling operations, such as Zigzag transform, Magic confusion and the row confusion. In addition, to increase key space and in order to improve the ability to resist two kinds of attacks, namely the known plaintext attack and the selected plaintext attack, the control parameters and the initial values of the two new chaotic systems are generated based on the SHA-256 function. Finally, compared to other schemes, simulation tests show that our scheme not only has higher security but also faster encryption speed.
C1 [Tang, Jianeng] Huaqiao Univ, Coll Engn, Quanzhou 362021, Peoples R China.
   [Zhang, Feng; Ni, Hui] Fujian MM Elect Co Ltd, Quanzhou 362000, Peoples R China.
C3 Huaqiao University
RP Tang, JN (corresponding author), Huaqiao Univ, Coll Engn, Quanzhou 362021, Peoples R China.
EM tangjianeng@sina.com; fzhang_mm@163.com; hni1234mm@88.com
RI Tang, Jianeng/JVD-6235-2023
OI Tang, Jianeng/0000-0003-4822-8437
FU National Natural Science Foundation of China [61573004]; Pilot Project
   of Fujian Province [2022H0017]; Quanzhou City Science & Technology
   Program of China [2018C106R]
FX This research was funded by National Natural Science Foundation of China
   under Grant No. 61573004, Pilot Project of Fujian Province under Grant
   No. 2022H0017 and Quanzhou City Science & Technology Program of China
   under Grant No. 2018C106R.
CR Abd El-Latif AA, 2013, AEU-INT J ELECTRON C, V67, P136, DOI 10.1016/j.aeue.2012.07.004
   Alawida M, 2019, SIGNAL PROCESS, V164, P249, DOI 10.1016/j.sigpro.2019.06.013
   Alawida M, 2019, SIGNAL PROCESS, V160, P45, DOI 10.1016/j.sigpro.2019.02.016
   Alvarez G, 2006, INT J BIFURCAT CHAOS, V16, P2129, DOI 10.1142/S0218127406015970
   Bassham III LE, 2010, Sp 800-22 rev. 1a. a statistical test suite for random and pseudorandom number generators for cryptographic applications, DOI DOI 10.6028/NIST.SP.800-22R1A
   Cao C, 2018, SIGNAL PROCESS, V143, P122, DOI 10.1016/j.sigpro.2017.08.020
   Chai XL, 2017, OPT LASER ENG, V88, P197, DOI 10.1016/j.optlaseng.2016.08.009
   Chen GR, 2004, CHAOS SOLITON FRACT, V21, P749, DOI 10.1016/j.chaos.2003.12.022
   Chen Y, 2022, ENTROPY-SWITZ, V24, DOI 10.3390/e24020287
   Chen ZH, 2016, IEEE T CIRCUITS-I, V63, P1464, DOI 10.1109/TCSI.2016.2573283
   Diaconu AV, 2016, INFORM SCIENCES, V355, P314, DOI 10.1016/j.ins.2015.10.027
   Eckmann J.-P., 1985, Reviews of Modern Physics, V57, P617, DOI 10.1103/RevModPhys.57.617
   Fang PF, 2023, VISUAL COMPUT, V39, P1975, DOI 10.1007/s00371-022-02459-5
   Gan ZH, 2019, NEURAL COMPUT APPL, V31, P7111, DOI 10.1007/s00521-018-3541-y
   Gong LH, 2018, OPT LASER TECHNOL, V103, P48, DOI 10.1016/j.optlastec.2018.01.007
   GRASSBERGER P, 1983, PHYS REV A, V28, P2591, DOI 10.1103/PhysRevA.28.2591
   Hua ZY, 2019, INFORM SCIENCES, V480, P403, DOI 10.1016/j.ins.2018.12.048
   Hua ZY, 2016, INFORM SCIENCES, V339, P237, DOI 10.1016/j.ins.2016.01.017
   Khan JS, 2019, J INTELL FUZZY SYST, V37, P2549, DOI 10.3233/JIFS-182778
   Lidong L, 2020, IEEE ACCESS, V8, P210382, DOI 10.1109/ACCESS.2020.3039891
   Liu HJ, 2015, OPT COMMUN, V338, P340, DOI 10.1016/j.optcom.2014.10.021
   Liu HJ, 2012, APPL SOFT COMPUT, V12, P1457, DOI 10.1016/j.asoc.2012.01.016
   Liu HJ, 2011, OPT COMMUN, V284, P3895, DOI 10.1016/j.optcom.2011.04.001
   Liu HJ, 2010, COMPUT MATH APPL, V59, P3320, DOI 10.1016/j.camwa.2010.03.017
   Liu S, 2022, IEEE MULTIMEDIA, V29, P74, DOI 10.1109/MMUL.2021.3114589
   Liu X, 2019, IEEE INTERNET THINGS, V6, P5962, DOI 10.1109/JIOT.2018.2847731
   Lu Q, 2020, IEEE ACCESS, V8, P25664, DOI 10.1109/ACCESS.2020.2970806
   Mansouri A, 2021, VISUAL COMPUT, V37, P189, DOI 10.1007/s00371-020-01791-y
   Mansouri A, 2020, INFORM SCIENCES, V520, P46, DOI 10.1016/j.ins.2020.02.008
   Pak C, 2017, SIGNAL PROCESS, V138, P129, DOI 10.1016/j.sigpro.2017.03.011
   PINCUS S, 1995, CHAOS, V5, P110, DOI 10.1063/1.166092
   PINCUS SM, 1991, P NATL ACAD SCI USA, V88, P2297, DOI 10.1073/pnas.88.6.2297
   Ping P, 2018, NEUROCOMPUTING, V283, P53, DOI 10.1016/j.neucom.2017.12.048
   Richman JS, 2000, AM J PHYSIOL-HEART C, V278, pH2039
   Shen CW, 2014, IEEE T CIRCUITS-I, V61, P2380, DOI 10.1109/TCSI.2014.2304655
   Stockmann H.-J., 2007, QUANTUM CHAOS INTRO
   Talhaoui MZ, 2021, VISUAL COMPUT, V37, P1757, DOI 10.1007/s00371-020-01936-z
   Wang GY, 2013, ACTA PHYS SIN-CH ED, V62, DOI 10.7498/aps.62.020506
   Wang XY, 2015, OPT LASER ENG, V73, P53, DOI 10.1016/j.optlaseng.2015.03.022
   Wang XY, 2010, NONLINEAR DYNAM, V62, P615, DOI 10.1007/s11071-010-9749-8
   Wang XY, 2022, IEEE T CIRCUITS-I, V69, P1291, DOI 10.1109/TCSI.2021.3133318
   Wang XY, 2021, INFORM SCIENCES, V574, P505, DOI 10.1016/j.ins.2021.06.032
   Wang XY, 2021, INFORM SCIENCES, V569, P217, DOI 10.1016/j.ins.2021.04.013
   Wang XY, 2022, VISUAL COMPUT, V38, P763, DOI 10.1007/s00371-020-02048-4
   Wang XY, 2020, IEEE ACCESS, V8, P174463, DOI 10.1109/ACCESS.2020.3024869
   Wang XY, 2020, INFORM SCIENCES, V539, P195, DOI 10.1016/j.ins.2020.06.030
   Wang XY, 2020, INFORM SCIENCES, V507, P16, DOI 10.1016/j.ins.2019.08.041
   Wang XY, 2019, OPT LASER TECHNOL, V119, DOI 10.1016/j.optlastec.2019.105581
   Wang XY, 2019, INFORM SCIENCES, V486, P340, DOI 10.1016/j.ins.2019.02.049
   Wang XY, 2015, OPT LASER ENG, V66, P10, DOI 10.1016/j.optlaseng.2014.08.005
   Wang XY, 2012, SIGNAL PROCESS, V92, P1101, DOI 10.1016/j.sigpro.2011.10.023
   Wu Y, 2011, Cyber J Multidiscip J Sci Technol J Sel Areas Telecommun (JSAT), V1, P31
   Wu YP, 2018, IEEE J SEL AREA COMM, V36, P679, DOI 10.1109/JSAC.2018.2825560
   Wu Y, 2013, INFORM SCIENCES, V222, P323, DOI 10.1016/j.ins.2012.07.049
   Xian YJ, 2022, IEEE T CIRCUITS-I, V69, P3320, DOI 10.1109/TCSI.2022.3172116
   Xian YJ, 2022, IEEE T CIRC SYST VID, V32, P4028, DOI 10.1109/TCSVT.2021.3108767
   Xian YJ, 2021, INFORM SCIENCES, V547, P1154, DOI 10.1016/j.ins.2020.09.055
   Xie EY, 2017, SIGNAL PROCESS, V132, P150, DOI 10.1016/j.sigpro.2016.10.002
   Xu J, 2022, VISUAL COMPUT, V38, P1509, DOI 10.1007/s00371-021-02085-7
   Xu L, 2016, OPT LASER ENG, V78, P17, DOI 10.1016/j.optlaseng.2015.09.007
   Yap WS, 2016, J VIS COMMUN IMAGE R, V40, P51, DOI 10.1016/j.jvcir.2016.06.005
   Zeraoulia E., 2011, ROBUST CHAOS ITS APP, DOI [10.1142/8296, DOI 10.1142/8296]
   Zhang YQ, 2015, APPL SOFT COMPUT, V26, P10, DOI 10.1016/j.asoc.2014.09.039
   Zhang YQ, 2014, INFORM SCIENCES, V273, P329, DOI 10.1016/j.ins.2014.02.156
   Zhou YC, 2013, SIGNAL PROCESS, V93, P3039, DOI 10.1016/j.sigpro.2013.04.021
   Zhu HG, 2021, MATH COMPUT SIMULAT, V185, P754, DOI 10.1016/j.matcom.2021.02.009
   Zhu HG, 2019, IEEE ACCESS, V7, P14081, DOI 10.1109/ACCESS.2019.2893538
   Zhu HG, 2017, NONLINEAR DYNAM, V89, P61, DOI 10.1007/s11071-017-3436-y
   Zhu LY, 2022, SIGNAL PROCESS, V195, DOI 10.1016/j.sigpro.2022.108489
   Zhu SQ, 2019, IEEE ACCESS, V7, P147106, DOI 10.1109/ACCESS.2019.2946208
NR 70
TC 3
Z9 4
U1 1
U2 27
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2023
VL 39
IS 10
BP 4955
EP 4983
DI 10.1007/s00371-022-02640-w
EA AUG 2022
PG 29
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA T1GK8
UT WOS:000847036400001
OA hybrid
DA 2024-07-18
ER

PT J
AU Ji, CF
   Liu, GZ
   Zhao, D
AF Ji, Chaofeng
   Liu, Guizhong
   Zhao, Dan
TI Stereo 3D object detection via instance depth prior guidance and
   adaptive spatial feature aggregation
SO VISUAL COMPUTER
LA English
DT Article
DE 3D object detection; Instance depth estimation; Deep learning; Stereo
   images; Autonomous driving
AB We present a novel and high-performance framework for 3D object detection using stereo vision. This framework incorporates direct instance depth estimation efficiently, improving the accuracy of the final 3D object detection. Instead of detecting objects separately in the left and right images of a stereo display, we exploit a modified 2D object detector that takes only the left image as input to generate union 2D bounding boxes for both images, and to predict the depth of the 3D box center for each object. Using the union 2D boxes, we propose a direct instance-level depth estimation network, taking the estimated depth as guidance, to predict the desired depths for pixels belonging to an object from a small search range. This approach greatly improves the efficiency and accuracy of 3D detection. Moreover, we design an adaptive spatial feature aggregation module that can weaken the effect of background points and automatically integrate important instance features to achieve accurate 3D object localization. Our method outperforms current state-of-the-art stereo-based 3D detection methods on the KITTI benchmark dataset, and it can efficiently employ a shared model for 3D multi-class detection. Code will be available at https://github.com/xjtuwh/iDepNet/trree/master.
C1 [Ji, Chaofeng; Zhao, Dan] Xi An Jiao Tong Univ, Xian 710049, Peoples R China.
   [Liu, Guizhong] Xi An Jiao Tong Univ, Sch Elect & Informat Engn, Xian 710049, Peoples R China.
C3 Xi'an Jiaotong University; Xi'an Jiaotong University
RP Liu, GZ (corresponding author), Xi An Jiao Tong Univ, Sch Elect & Informat Engn, Xian 710049, Peoples R China.
EM liugz@xjtu.edu.cn
FU Shanxi KeyResearch and Development Program Grant [2018ZDCXL-GY-04-03-02]
FX This work was supported by Shanxi KeyResearch and Development Program
   Grant 2018ZDCXL-GY-04-03-02.
CR Brazil G, 2019, IEEE I CONF COMP VIS, P9286, DOI 10.1109/ICCV.2019.00938
   Chang JR, 2018, PROC CVPR IEEE, P5410, DOI 10.1109/CVPR.2018.00567
   Chen XZ, 2016, PROC CVPR IEEE, P2147, DOI 10.1109/CVPR.2016.236
   Chen Y., 2020, CVPR, P10337
   Garg D., 2020, ADV NEURAL INFORM PR
   Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074
   Huang T., 2020, ARXIV PREPRINT
   Jiaming Sun, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10545, DOI 10.1109/CVPR42600.2020.01056
   Ku J, 2018, IEEE INT C INT ROBOT, P5750, DOI 10.1109/IROS.2018.8594049
   Ku J, 2019, PROC CVPR IEEE, P11859, DOI 10.1109/CVPR.2019.01214
   Ku J, 2018, 2018 15TH CONFERENCE ON COMPUTER AND ROBOT VISION (CRV), P16, DOI 10.1109/CRV.2018.00013
   Law H, 2018, LECT NOTES COMPUT SC, V11218, P765, DOI 10.1007/978-3-030-01264-9_45
   Li BY, 2019, PROC CVPR IEEE, P1019, DOI 10.1109/CVPR.2019.00111
   Li CY, 2020, IEEE INT C INT ROBOT, P5776, DOI 10.1109/IROS45743.2020.9341188
   Li PL, 2019, PROC CVPR IEEE, P7636, DOI 10.1109/CVPR.2019.00783
   Li Peixuan, 2020, EUROPEAN C COMPUTER, P644
   Li X, 2022, VISUAL COMPUT, V38, P3881, DOI 10.1007/s00371-021-02228-w
   Liu ZC, 2020, IEEE COMPUT SOC CONF, P4289, DOI 10.1109/CVPRW50498.2020.00506
   Ma X., 2020, arXiv
   Ma XZ, 2019, IEEE I CONF COMP VIS, P6850, DOI 10.1109/ICCV.2019.00695
   Mousavian A, 2017, PROC CVPR IEEE, P5632, DOI 10.1109/CVPR.2017.597
   Pon AD, 2020, IEEE INT CONF ROBOT, P8383, DOI [10.1109/ICRA40945.2020.9196660, 10.1109/icra40945.2020.9196660]
   Qi CR, 2019, IEEE I CONF COMP VIS, P9276, DOI 10.1109/ICCV.2019.00937
   Qi CR, 2018, PROC CVPR IEEE, P918, DOI 10.1109/CVPR.2018.00102
   Qian R, 2020, PROC CVPR IEEE, P5880, DOI 10.1109/CVPR42600.2020.00592
   Qin ZY, 2019, PROC CVPR IEEE, P7607, DOI 10.1109/CVPR.2019.00780
   Qin ZY, 2019, AAAI CONF ARTIF INTE, P8851
   Shaoshuai Shi, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10526, DOI 10.1109/CVPR42600.2020.01054
   Shi SS, 2019, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2019.00086
   Shi WJ, 2020, PROC CVPR IEEE, P1708, DOI 10.1109/CVPR42600.2020.00178
   Tsung-Yi Lin, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P2999, DOI 10.1109/ICCV.2017.324
   Wang Y, 2019, PROC CVPR IEEE, P8063, DOI 10.1109/CVPR.2019.00826
   Wanli Peng, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13012, DOI 10.1109/CVPR42600.2020.01303
   Xu B, 2018, PROC CVPR IEEE, P2345, DOI 10.1109/CVPR.2018.00249
   Xu ZB, 2020, AAAI CONF ARTIF INTE, V34, P12557
   Yilun Chen, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12533, DOI 10.1109/CVPR42600.2020.01255
   You Y., 2019, ARXIV, p1904.00962
   Yu F, 2018, PROC CVPR IEEE, P2403, DOI 10.1109/CVPR.2018.00255
   Zhou X., 2019, arXiv
NR 39
TC 5
Z9 5
U1 4
U2 22
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2023
VL 39
IS 10
BP 4543
EP 4554
DI 10.1007/s00371-022-02607-x
EA JUL 2022
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA T1GK8
UT WOS:000828925700001
DA 2024-07-18
ER

PT J
AU Yoshikawa, T
   Endo, Y
   Kanamori, Y
AF Yoshikawa, Takato
   Endo, Yuki
   Kanamori, Yoshihiro
TI Diversifying detail and appearance in sketch-based face image synthesis
SO VISUAL COMPUTER
LA English
DT Article
DE Sketch-based image synthesis; Deep learning; GAN; Multimodal
AB Sketch-based face image synthesis has gained greater attention with the increasing realism of its output images. However, existing studies have overlooked the significance of output diversity: because sketches are inherently ambiguous, it would be desirable to have various output candidates for a single-input sketch. In this paper, we explore synthesis of diverse face images from a single sketch by using a three-stage framework consisting of sketch refinement, detail enhancement, and appearance synthesis. Each stage uses supervised learning with neural networks. With this three-stage framework, we can separately control the detail (e.g., wrinkles and hair structures) and appearance (e.g., skin and hair colors) of output face images separately by using multiple latent codes. Quantitative and quantitative evaluations demonstrate that our method offers greater diversity in its output images than the state-of-the-art methods, while retaining the output realism.
C1 [Yoshikawa, Takato] Univ Tsukuba, Dept Engn, Tsukuba, Ibaraki, Japan.
   [Endo, Yuki; Kanamori, Yoshihiro] Univ Tsukuba, Tsukuba, Ibaraki, Japan.
C3 University of Tsukuba; University of Tsukuba
RP Yoshikawa, T (corresponding author), Univ Tsukuba, Dept Engn, Tsukuba, Ibaraki, Japan.
EM tenten0727@icloud.com; endo@cs.tsukuba.ac.jp; kanamori@cs.tsukuba.ac.jp
OI Yoshikawa, Takato/0000-0001-5043-8367
FU Grants-in-Aid for Scientific Research [20K19816] Funding Source: KAKEN
CR [Anonymous], 2018, P INT C LEARN REPR I
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851
   Chen SY, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459760
   Chen SY, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392386
   Chen T, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618470
   Chen WL, 2018, PROC CVPR IEEE, P9416, DOI 10.1109/CVPR.2018.00981
   Choi Y, 2018, PROC CVPR IEEE, P8789, DOI 10.1109/CVPR.2018.00916
   Endo Y, 2020, COMPUT GRAPH FORUM, V39, P519, DOI 10.1111/cgf.14164
   Ghosh A, 2018, PROC CVPR IEEE, P8513, DOI 10.1109/CVPR.2018.00888
   Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167
   Hung-Yu Tseng, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12363), P158, DOI 10.1007/978-3-030-58523-5_10
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Karras T., 2018, INT C LEARN REPR INT C LEARN REPR INT C LEARN REPR INT C LEARN REPR INT C LEARN REPR INT C LEARN REPR
   Kingma DP, 2013, ARXIV
   Larsen ABL, 2016, PR MACH LEARN RES, V48
   Lee J, 2020, PROC CVPR IEEE, P5800, DOI 10.1109/CVPR42600.2020.00584
   Li K, 2019, IEEE I CONF COMP VIS, P4219, DOI 10.1109/ICCV.2019.00432
   Li YH, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P991, DOI 10.1145/3394171.3413684
   Liu MY, 2019, IEEE I CONF COMP VIS, P10550, DOI 10.1109/ICCV.2019.01065
   Liu MY, 2017, ADV NEUR IN, V30
   Liu X., 2019, Advances in Neural Information Processing Systems, P570
   Lu YY, 2018, LECT NOTES COMPUT SC, V11220, P213, DOI 10.1007/978-3-030-01270-0_13
   Mirza M., 2014, CoRR arXiv:1411.1784
   Park T, 2019, PROC CVPR IEEE, P2332, DOI 10.1109/CVPR.2019.00244
   Richardson E, 2021, PROC CVPR IEEE, P2287, DOI 10.1109/CVPR46437.2021.00232
   Sangkloy P, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925954
   Shuai Yang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12360), P601, DOI 10.1007/978-3-030-58555-6_36
   Simonyan K., 2014, 14091556 ARXIV
   Wang S.Y., 2021, ICCV 2021
   Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917
   Wang XG, 2009, IEEE T PATTERN ANAL, V31, P1955, DOI 10.1109/TPAMI.2008.222
   Winnemoeller H, 2012, COMPUT GRAPH-UK, V36, P740, DOI 10.1016/j.cag.2012.03.004
   Xie SN, 2015, IEEE I CONF COMP VIS, P1395, DOI [10.1109/ICCV.2015.164, 10.1007/s11263-017-1004-z]
   Yang Y., 2020, CORR ARXIV201114785
   Yu Q, 2016, PROC CVPR IEEE, P799, DOI 10.1109/CVPR.2016.93
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhu JY, 2017, ADV NEUR IN, V30
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zhu PH, 2020, PROC CVPR IEEE, P5103, DOI 10.1109/CVPR42600.2020.00515
NR 40
TC 5
Z9 5
U1 0
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2022
VL 38
IS 9-10
SI SI
BP 3121
EP 3133
DI 10.1007/s00371-022-02538-7
EA JUN 2022
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 4N3NE
UT WOS:000814494400001
DA 2024-07-18
ER

PT J
AU Shao, ZW
   Zhou, Y
   Liu, B
   Zhu, HC
   Du, WL
   Zhao, JQ
AF Shao, Zhiwen
   Zhou, Yong
   Liu, Bing
   Zhu, Hancheng
   Du, Wen-Liang
   Zhao, Jiaqi
TI Facial action unit detection via hybrid relational reasoning
SO VISUAL COMPUTER
LA English
DT Article
DE Facial AU detection; Hybrid relational reasoning; Pixel-level
   correlations; AU-level correlations
ID SEMANTIC RELATIONSHIPS
AB Correlations in facial action units (AUs) convey significant information for AU detection yet have not been thoroughly exploited. Most existing methods learn the regional correlation distribution of each AU, or reason the dependencies among AUs. However, these methods typically either predefine the correlations based on prior knowledge, which often ignores useful information, or directly learn the correlations guided by AU detection, which often includes irrelevant information. To resolve these limitations, we propose a novel hybrid relational reasoning framework for AU detection. In particular, we propose to adaptively reason pixel-level correlations of each AU, under the constraint of predefined regional correlations by facial landmarks, as well as the supervision of AU detection. Moreover, we propose to adaptively reason AU-level correlations using a graph convolutional network, by considering both predefined AU relationships and learnable relationship weights. Our framework is beneficial for integrating the advantages of correlation predefinition and correlation learning. Extensive experiments demonstrate that our approach (i) soundly outperforms the state-of-the-art AU detection methods on the challenging BP4D, DISFA, and GFT benchmarks, and (ii) can precisely reason the regional correlation distribution of each AU.
C1 [Shao, Zhiwen; Zhou, Yong; Liu, Bing; Zhu, Hancheng; Du, Wen-Liang; Zhao, Jiaqi] China Univ Min & Technol, Sch Comp Sci & Technol, Xuzhou 221116, Jiangsu, Peoples R China.
   [Shao, Zhiwen; Zhou, Yong; Liu, Bing; Zhu, Hancheng; Du, Wen-Liang; Zhao, Jiaqi] Minist Educ Peoples Republ China, Engn Res Ctr Mine Digitizat, Xuzhou 221116, Jiangsu, Peoples R China.
C3 China University of Mining & Technology
RP Zhou, Y (corresponding author), China Univ Min & Technol, Sch Comp Sci & Technol, Xuzhou 221116, Jiangsu, Peoples R China.; Zhou, Y (corresponding author), Minist Educ Peoples Republ China, Engn Res Ctr Mine Digitizat, Xuzhou 221116, Jiangsu, Peoples R China.
EM yzhou@cumt.edu.cn
RI Shao, Zhiwen/N-8985-2018
OI Shao, Zhiwen/0000-0002-9383-8384
FU National Natural Science Foundation of China [62101555, 62002360];
   High-Level Talent Program for Innovation and Entrepreneurship
   (ShuangChuang Doctor) of Jiangsu Province [JSSCBS20211220]; Natural
   Science Foundation of Jiangsu Province [BK20201346, BK20210488];
   Fundamental Research Funds for the Central Universities [2021QN+1072]
FX This work was supported by the National Natural Science Foundation of
   China (No. 62106268), and the High-Level Talent Program for Innovation
   and Entrepreneurship (ShuangChuang Doctor) of Jiangsu Province (No.
   JSSCBS20211220). It was also partially supported by the National Natural
   Science Foundation of China (No. 62101555 and No. 62002360), the Natural
   Science Foundation of Jiangsu Province (No. BK20201346 and No.
   BK20210488), and the Fundamental Research Funds for the Central
   Universities (No. 2021QN+1072).
CR [Anonymous], 2014, 2 INT C LEARN REPR I
   Blanz V, 1999, COMP GRAPH, P187, DOI 10.1145/311535.311556
   Cao Q, 2018, IEEE INT CONF AUTOMA, P67, DOI 10.1109/FG.2018.00020
   Chen YD, 2022, PATTERN RECOGN, V122, DOI 10.1016/j.patcog.2021.108355
   Chu WS, 2017, IEEE INT CONF AUTOMA, P25, DOI 10.1109/FG.2017.13
   Cootes TF, 2001, IEEE T PATTERN ANAL, V23, P681, DOI 10.1109/34.927467
   Corneanu C, 2018, LECT NOTES COMPUT SC, V11216, P309, DOI 10.1007/978-3-030-01258-8_19
   Defferrard M, 2016, ADV NEUR IN, V29
   Ekman P., 2002, FACIAL ACTION CODING
   Ertugrul Itir Onal, 2020, IEEE Trans Biom Behav Identity Sci, V2, P158, DOI [10.1109/tbiom.2020.2977225, 10.1109/TBIOM.2020.2977225]
   Fan RE, 2008, J MACH LEARN RES, V9, P1871
   Friesen E., 1978, Environmental Psychology & Nonverbal Behavior, V3, P5, DOI 10.1037/t27734-000
   Girard JM, 2017, IEEE INT CONF AUTOMA, P581, DOI 10.1109/FG.2017.144
   Guo YD, 2019, IEEE T PATTERN ANAL, V41, P1294, DOI 10.1109/TPAMI.2018.2837742
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Jeni LA, 2017, IMAGE VISION COMPUT, V58, P13, DOI 10.1016/j.imavis.2016.05.009
   Kingma DP., 2014, ADAM METHOD STOCHAST
   Kipf T.N., 2017, P INT C LEARN REPR S
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   KULLBACK S, 1951, ANN MATH STAT, V22, P79, DOI 10.1214/aoms/1177729694
   Li GB, 2019, AAAI CONF ARTIF INTE, P8594
   Li W, 2018, IEEE T PATTERN ANAL, V40, P2583, DOI 10.1109/TPAMI.2018.2791608
   Li Y, 2019, PROC CVPR IEEE, P10916, DOI 10.1109/CVPR.2019.01118
   Li YQ, 2013, IEEE T IMAGE PROCESS, V22, P2559, DOI 10.1109/TIP.2013.2253477
   Liu ZL, 2020, LECT NOTES COMPUT SC, V11962, P489, DOI 10.1007/978-3-030-37734-2_40
   Lowe D. G., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1150, DOI 10.1109/ICCV.1999.790410
   Ma C, 2019, NEUROCOMPUTING, V355, P35, DOI 10.1016/j.neucom.2019.03.082
   Mavadati SM, 2013, IEEE T AFFECT COMPUT, V4, P151, DOI 10.1109/T-AFFC.2013.4
   Niu XS, 2019, PROC CVPR IEEE, P11909, DOI 10.1109/CVPR.2019.01219
   Paszke A, 2019, ADV NEUR IN, V32
   Pearl J., 1988, PROBABILISTIC REASON
   Sankaran N, 2019, IEEE INT CONF AUTOMA, P107
   Shao ZW, 2022, IEEE T AFFECT COMPUT, V13, P1274, DOI 10.1109/TAFFC.2019.2948635
   Shao ZW, 2021, INT J COMPUT VISION, V129, P321, DOI 10.1007/s11263-020-01378-z
   Simonyan K, 2015, IEEE INT C ICLR
   Song TF, 2021, PROC CVPR IEEE, P6263, DOI 10.1109/CVPR46437.2021.00620
   Song TF, 2021, AAAI CONF ARTIF INTE, V35, P5993
   Sutskever Ilya, 2013, P 30 INT C MACHINE L
   Tong Y., 2008, CVPR
   Wang ZH, 2013, IEEE I CONF COMP VIS, P3304, DOI 10.1109/ICCV.2013.410
   Xiong XH, 2013, PROC CVPR IEEE, P532, DOI 10.1109/CVPR.2013.75
   Zhang X, 2014, IMAGE VISION COMPUT, V32, P692, DOI 10.1016/j.imavis.2014.06.002
   Zhao KL, 2016, PROC CVPR IEEE, P3391, DOI 10.1109/CVPR.2016.369
   Zhao KL, 2016, IEEE T IMAGE PROCESS, V25, P3931, DOI 10.1109/TIP.2016.2570550
NR 44
TC 1
Z9 1
U1 1
U2 21
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2022
VL 38
IS 9-10
SI SI
BP 3045
EP 3057
DI 10.1007/s00371-022-02527-w
EA JUN 2022
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 4N3NE
UT WOS:000810352600001
OA Bronze
DA 2024-07-18
ER

PT J
AU Amin, MS
   Wang, CB
   Jabeen, S
AF Amin, Muhammad Shoib
   Wang, Changbo
   Jabeen, Summaira
TI Fashion sub-categories and attributes prediction model using deep
   learning
SO VISUAL COMPUTER
LA English
DT Article
DE Fashion; Deep learning; Segmentation; Sub-categories; Attributes
   classification
AB The fashion clothing and items classification is challenging to incorporate category/sub-category classification and attributes prediction for numerous fashion items into a compact multitask learning infrastructure. The main motive of this research is to improve the fashion items categorization and their attributes prediction from extracted visual features. We proposed a novel fashion sub-categories and attributes prediction (FS(C)AP) model using deep learning techniques. In this proposed model, YOLO and DeepSORT architectures are used for person detection and tracking, Faster-RCNN architecture is used for sub-categories classification, and Custom-EfficientNet-B3 architecture is designed for attributes prediction. Twenty-four distinct modules are designed to increase the attributes classification accuracy for detected fashion items again each sub-category. The performance of the proposed model is evaluated on a customized fully annotated FashionItem dataset. The experimental results clearly show that the proposed model outperforms the recent baseline methods in fashion sub-categories and attributes prediction.
C1 [Amin, Muhammad Shoib] East China Normal Univ, Sch Software Engn, Shanghai, Peoples R China.
   [Wang, Changbo] East China Normal Univ, Sch Comp Sci & Technol, Shanghai, Peoples R China.
   [Jabeen, Summaira] Zhejiang Univ, Dept Comp Sci, Hangzhou, Peoples R China.
C3 East China Normal University; East China Normal University; Zhejiang
   University
RP Wang, CB (corresponding author), East China Normal Univ, Sch Comp Sci & Technol, Shanghai, Peoples R China.
EM 52184501030@stu.ecnu.edu.cn; cbwang@cs.ecnu.edu.cn;
   11821129@mail.zju.edu.cn
CR Ay B., 2021, GENERATIVE ADVERSARI, P185
   Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615
   Bochkovskiy A, 2020, ARXIV PREPRINT ARXIV, DOI DOI 10.48550/ARXIV.2004.10934
   Cho H, 2019, IEEE INT CONF COMP V, P3197, DOI 10.1109/ICCVW.2019.00398
   Dutta Abhishek, 2019, MM '19: Proceedings of the 27th ACM International Conference on Multimedia, P2276, DOI 10.1145/3343031.3350535
   Feng F, 2018, SYST SCI CONTROL ENG, V6, P456, DOI 10.1080/21642583.2018.1536899
   Garg D., 2018, 2018 IEEE PUN, P1
   Geng L, 2019, COMPUT ASSIST SURG, V24, P27, DOI 10.1080/24699322.2019.1649071
   He KM, 2020, IEEE T PATTERN ANAL, V42, P386, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   Hu LB, 2018, PROC SPIE, V10828, DOI 10.1117/12.2501773
   Hu PP, 2017, VISUAL COMPUT, V33, P961, DOI 10.1007/s00371-017-1388-3
   Jain Priyal, 2019, Information Systems Design and Intelligent Applications. Proceedings of Fifth International Conference INDIA 2018. Advances in Intelligent Systems and Computing (AISC 862), P169, DOI 10.1007/978-981-13-3329-3_16
   Jeon Y, 2021, PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE 2021 (WWW 2021), P2367, DOI 10.1145/3442381.3449833
   Jia M., 2018, ARXIV PREPRINT ARXIV
   Kayed M, 2020, PROCEEDINGS OF 2020 INTERNATIONAL CONFERENCE ON INNOVATIVE TRENDS IN COMMUNICATION AND COMPUTER ENGINEERING (ITCE), P238, DOI [10.1109/ITCE48509.2020.9047776, 10.1109/itce48509.2020.9047776]
   Kellman M, 2019, IEEE INT CONF COMPUT, DOI 10.1109/iccphot.2019.8747339
   Kim HJ, 2021, IEEE ACCESS, V9, P11694, DOI 10.1109/ACCESS.2021.3051424
   Lee CH, 2021, APPL SCI-BASEL, V11, DOI 10.3390/app11093782
   Lee S, 2019, IEEE I CONF COMP VIS, P4412, DOI 10.1109/ICCV.2019.00451
   Li HL, 2016, VISUAL COMPUT, V32, P1351, DOI 10.1007/s00371-016-1232-1
   Li Jiwei, 2017, P 2017 C EMP METH NA, P2157, DOI DOI 10.18653/V1/D17-1230
   Li PZ, 2019, IEEE IMAGE PROC, P3038, DOI [10.1109/icip.2019.8803394, 10.1109/ICIP.2019.8803394]
   Lian GY, 2020, VISUAL COMPUT, V36, P799, DOI 10.1007/s00371-019-01661-2
   Liu ZW, 2016, PROC CVPR IEEE, P1096, DOI 10.1109/CVPR.2016.124
   Liu ZW, 2016, LECT NOTES COMPUT SC, V9906, P229, DOI 10.1007/978-3-319-46475-6_15
   Mallavarapu T., 2020, ARXIV PREPRINT ARXIV
   Manfredi M, 2014, MACH VISION APPL, V25, P955, DOI 10.1007/s00138-013-0580-3
   Martinsson J, 2019, IEEE INT CONF COMP V, P3133, DOI 10.1109/ICCVW.2019.00382
   Meshkini K., 2019, INT C INTELLIGENT IN, P85
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Rubio A, 2017, IEEE INT CONF COMP V, P2236, DOI 10.1109/ICCVW.2017.261
   Seo Y, 2019, EXPERT SYST APPL, V116, P328, DOI 10.1016/j.eswa.2018.09.022
   Shajini M, 2022, VISUAL COMPUT, V38, P3551, DOI 10.1007/s00371-021-02178-3
   Shajini M, 2021, VISUAL COMPUT, V37, P1517, DOI 10.1007/s00371-020-01885-7
   Shi Min, 2020, ARXIV PREPRINT ARXIV
   Tan MX, 2019, PR MACH LEARN RES, V97
   Wang HQ, 2018, 2018 14TH IEEE/ASME INTERNATIONAL CONFERENCE ON MECHATRONIC AND EMBEDDED SYSTEMS AND APPLICATIONS (MESA), DOI 10.1145/3204493.3204584
   Wang WY, 2018, PROC CVPR IEEE, P2569, DOI 10.1109/CVPR.2018.00272
   Wojke N, 2017, IEEE IMAGE PROC, P3645, DOI 10.1109/ICIP.2017.8296962
   Ye JW, 2018, IEEE INT CON MULTI
   Zhang HM, 2018, IEEE IMAGE PROC, P2640, DOI 10.1109/ICIP.2018.8451125
NR 43
TC 4
Z9 4
U1 3
U2 22
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2023
VL 39
IS 9
BP 3851
EP 3864
DI 10.1007/s00371-022-02520-3
EA JUN 2022
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA Q8PV6
UT WOS:000807352400001
DA 2024-07-18
ER

PT J
AU Santhiya, P
   Chitrakala, S
AF Santhiya, P.
   Chitrakala, S.
TI PTCERE: personality-trait mapping using cognitive-based emotion
   recognition from electroencephalogram signals
SO VISUAL COMPUTER
LA English
DT Article
DE Cognitive computing; Cognitive personality trait; Emotion recognition;
   Electroencephalogram (EEG); Human emotion; Machine learning
ID FEATURE-EXTRACTION; USER IDENTIFICATION; NEURAL-NETWORKS; EEG;
   CLASSIFICATION; FRAMEWORK; FEATURES
AB Human emotion recognition is a technique for identifying human emotions with respect to various aspects of human life, such as in decision-making, detecting lies, assessing social behaviour, measuring brain-related activity and identifying the personality of a person. In general, this paper aims to propose an efficient EEG-based cognitive personality trait detection based on the human emotion recognition system. Two different algorithms were proposed for this system, namely the normalised window-short time Fourier transform (NW-STFT) algorithm and the cognitive mapping-based Hebbian learning (CM-HL) algorithm. The NW-STFT algorithm dynamically fixes the window size for feature extraction using EEG-based emotion recognition, and the CM-HL algorithm is proposed for mapping the top five big personality traits with the help of recognised primary emotions. The pre-processing method reduces the contaminated artefacts from the benchmark dataset, as well as the brain signal's band power frequency range analysis time. The proposed emotion recognition method has shown a significant improvement in accuracy, leading to greater enhancement in personality-trait mapping. The outcome shows that the proposed two algorithms are effective in finding emotional and personality trait mapping. query Please check the edit made in the article title.
C1 [Santhiya, P.; Chitrakala, S.] Anna Univ, Coll Engn, Dept Comp Sci & Engn, Chennai, Tamil Nadu, India.
C3 Anna University; Anna University Chennai
RP Santhiya, P (corresponding author), Anna Univ, Coll Engn, Dept Comp Sci & Engn, Chennai, Tamil Nadu, India.
EM santhiyasasitha16@gmail.com; chitrakala.au@gmail.com
RI S, C/JLK-9983-2023; S, Chitrakala/T-9631-2019; P, Santhiya/GZK-4830-2022
OI P, Santhiya/0000-0002-4818-3075
CR Bajaj V, 2018, HEALTH INF SCI SYST, V6, DOI 10.1007/s13755-018-0048-y
   Bhatti AM, 2016, COMPUT HUM BEHAV, V65, P267, DOI 10.1016/j.chb.2016.08.029
   Carella T, 2018, IEEE ENG MED BIO, P223, DOI 10.1109/EMBC.2018.8512228
   Chettupuzhakkaran P., 2018, 2018 INT C EM TRENDS, P15
   Cui H, 2020, KNOWL-BASED SYST, V205, DOI 10.1016/j.knosys.2020.106243
   Dar MN, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20164551
   Degirmenci M, 2018, 2018 MEDICAL TECHNOLOGIES NATIONAL CONGRESS (TIPTEKNO)
   Gonzalez HA, 2019, IEEE ENG MED BIO, P694, DOI [10.1109/embc.2019.8857248, 10.1109/EMBC.2019.8857248]
   Gupta V, 2019, IEEE SENS J, V19, P2266, DOI 10.1109/JSEN.2018.2883497
   Katsigiannis S, 2018, IEEE J BIOMED HEALTH, V22, P98, DOI 10.1109/JBHI.2017.2688239
   Kaur B, 2017, MULTIMED TOOLS APPL, V76, P25581, DOI 10.1007/s11042-016-4232-2
   Khurana V, 2018, COGN SYST RES, V49, P33, DOI 10.1016/j.cogsys.2017.11.003
   Krishna AH, 2019, IET SCI MEAS TECHNOL, V13, P375, DOI 10.1049/iet-smt.2018.5237
   Krishna NM, 2019, IEEE ACCESS, V7, P77905, DOI 10.1109/ACCESS.2019.2922047
   KUMAR P, 2017, IEEE REGION 10 SYMP
   Kumar P, 2017, J NETW COMPUT APPL, V89, P62, DOI 10.1016/j.jnca.2017.02.011
   Lan ZR, 2019, IEEE T COGN DEV SYST, V11, P85, DOI 10.1109/TCDS.2018.2826840
   Lan ZR, 2016, VISUAL COMPUT, V32, P347, DOI 10.1007/s00371-015-1183-y
   Li JP, 2018, COGN COMPUT, V10, P368, DOI 10.1007/s12559-017-9533-x
   Liu W, 2016, LECT NOTES COMPUT SC, V9948, P521, DOI 10.1007/978-3-319-46672-9_58
   Liu WJ, 2020, AAAI CONF ARTIF INTE, V34, P2901
   Liu Y, 2020, COMPUT BIOL MED, V123, DOI 10.1016/j.compbiomed.2020.103927
   Matilda S, 2015, International Journal of Advanced Research in Computer Science, V3, P14
   Mehmood RM, 2016, COMPUT ELECTR ENG, V53, P444, DOI 10.1016/j.compeleceng.2016.04.009
   Menezes MLR, 2017, PERS UBIQUIT COMPUT, V21, P1003, DOI 10.1007/s00779-017-1072-7
   Mert A, 2018, PATTERN ANAL APPL, V21, P81, DOI 10.1007/s10044-016-0567-6
   Pane ES, 2019, COGN PROCESS, V20, P405, DOI 10.1007/s10339-019-00924-z
   Rahman MA, 2020, EGYPT INFORM J, V21, P23, DOI 10.1016/j.eij.2019.10.002
   RUSSELL JA, 1980, J PERS SOC PSYCHOL, V39, P1161, DOI 10.1037/h0077714
   Saini R, 2018, INFORM SCIENCES, V430, P163, DOI 10.1016/j.ins.2017.11.045
   Santhiya P., 2019, 2019 INT C VIS EM TR, P16
   Siddharth, 2022, IEEE T AFFECT COMPUT, V13, P96, DOI 10.1109/TAFFC.2019.2916015
   Soleymani M, 2016, IEEE T AFFECT COMPUT, V7, P17, DOI 10.1109/TAFFC.2015.2436926
   Song TF, 2020, IEEE T AFFECT COMPUT, V11, P532, DOI 10.1109/TAFFC.2018.2817622
   Soundarya S, 2019, J EMERG TECHNOL INNO, V5, P744
   Vergini E.S., 2016, 2016 7 INT C INF INT, P16
   Wang F, 2018, LECT NOTES COMPUT SC, V10705, P82, DOI 10.1007/978-3-319-73600-6_8
   Wang SF, 2015, IEEE T AUTON MENT DE, V7, P189, DOI 10.1109/TAMD.2015.2463113
   Wenming Zheng, 2018, IEEE Transactions on Affective Computing, V9, P21, DOI 10.1109/TAFFC.2016.2563432
   Widrow B, 2015, IEEE COMPUT INTELL M, V10, P37, DOI 10.1109/MCI.2015.2471216
   Yadava M, 2017, MULTIMED TOOLS APPL, V76, P19087, DOI 10.1007/s11042-017-4580-6
   Yang YM, 2018, IEEE T COGN DEV SYST, V10, P408, DOI 10.1109/TCDS.2017.2685338
   Yu D, 2020, INFORMATION, V11, DOI 10.3390/info11040212
   Zhang T, 2019, INT J COMPUT MATH, V96, P594, DOI 10.1080/00207160.2018.1455092
   Zhang Y, 2018, MULTIMED TOOLS APPL, V77, P26697, DOI 10.1007/s11042-018-5885-9
   Zhang Y, 2016, NEUROSCI LETT, V633, P152, DOI 10.1016/j.neulet.2016.09.037
   Zhao GZ, 2018, IEEE T AFFECT COMPUT, V9, P362, DOI 10.1109/TAFFC.2017.2786207
   Zheng WM, 2017, IEEE T COGN DEV SYST, V9, P281, DOI 10.1109/TCDS.2016.2587290
NR 48
TC 3
Z9 3
U1 2
U2 8
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2023
VL 39
IS 7
BP 2953
EP 2967
DI 10.1007/s00371-022-02502-5
EA MAY 2022
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA L4HX1
UT WOS:000792533800001
DA 2024-07-18
ER

PT J
AU Li, DY
   Peng, LX
   Peng, SH
   Xiao, HX
   Zhang, YF
AF Li, Dongyuan
   Peng, Lingxi
   Peng, Shaohu
   Xiao, Hongxin
   Zhang, Yifan
TI Retinal vessel segmentation by using AFNet
SO VISUAL COMPUTER
LA English
DT Article
DE Retinal vascular segmentation; Positional attention; Multi-scale feature
   fusion; Semantic aggregation module; AFNet
ID NETWORK
AB Retinal vessel segmentation can obtain rich ocular information which is important for the diagnosis of fundus diseases. To address the problems of existing segmentation methods such as poor capillary segmentation and incorrect segmentation of pathological information, an AFNet vessel segmentation network combining location attention, semantic aggregation module and multi-scale feature fusion module is proposed. Add positional attention to the feature codec block of the network for modeling global dependencies and reducing intra-class inconsistencies, the multiscale feature fusion module is used in the last layer of the coding part to extract multiscale feature information to solve the difficult problem of large variation of retinal vessel width and size, and the designed semantic aggregation module can fully utilize the contextual semantic information to improve the segmentation accuracy of capillaries. Extensive experiments are conducted on three publicly available fundus image databases DRIVE, STARE and CHASE_DB1, and the results show that AFNet can effectively improve the accuracy of retinal vessel segmentation and achieve better comprehensive performance compared with other methods.
C1 [Li, Dongyuan; Peng, Shaohu; Xiao, Hongxin; Zhang, Yifan] Guangzhou Univ, Sch Elect & Commun Engn, Guangzhou 511400, Peoples R China.
   [Peng, Lingxi] Guangzhou Univ, Sch Mech & Elect Engn, Guangzhou 511400, Peoples R China.
C3 Guangzhou University; Guangzhou University
RP Peng, SH (corresponding author), Guangzhou Univ, Sch Elect & Commun Engn, Guangzhou 511400, Peoples R China.
EM 1798265227@qq.com; manplx@163.com; pengsh@gzhu.edu.cn; 934144154@qq.com;
   630588390@qq.com
RI zhang, yifan/GLS-2285-2022
OI zhang, yifan/0000-0002-2086-4864
FU National Natural Science Foundation of China [61100150]
FX This work was supported by the National Natural Science Foundation of
   China (No. 12171114) and the National Natural Science Foundation of
   China (No. 61100150).
CR Akil H, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0170476
   Alom M.Z., 2018, ARXIV PREPRINT
   Chen L.-C., 2016, P IEEE C COMPUTER VI
   Cheung CYL, 2017, PROG RETIN EYE RES, V57, P89, DOI 10.1016/j.preteyeres.2017.01.001
   Feng ST, 2020, NEUROCOMPUTING, V392, P268, DOI 10.1016/j.neucom.2018.10.098
   Fraser-Bell S, 2017, CLIN EXP OPHTHALMOL, V45, P45, DOI 10.1111/ceo.12905
   Fraz MM, 2012, IEEE T BIO-MED ENG, V59, P2538, DOI 10.1109/TBME.2012.2205687
   Gu ZW, 2019, IEEE T MED IMAGING, V38, P2281, DOI 10.1109/TMI.2019.2903562
   Guo S, 2019, INT J MED INFORM, V126, P105, DOI 10.1016/j.ijmedinf.2019.03.015
   Hou Q., 2021, P IEEECVF C COMPUTER
   Jie H., 2018, CVPR
   Jin Q., 2018, DUNET DEFORMABLE NET
   Jin QG, 2019, KNOWL-BASED SYST, V178, P149, DOI 10.1016/j.knosys.2019.04.025
   Jongchan P., 2018, ARXIV
   Karn PK, 2019, IET IMAGE PROCESS, V13, P440, DOI 10.1049/iet-ipr.2018.5413
   Lee H, 2018, RETINA-J RET VIT DIS, V38, P976, DOI 10.1097/IAE.0000000000001618
   Li L., 2020, P IEEECVF WINTER C A
   Li X, 2017, INT J CIV ENG, V2017, P1, DOI DOI 10.16815/J.CNKI.11-5436/S.2017.10.001
   Li X, 2019, PROC CVPR IEEE, P510, DOI 10.1109/CVPR.2019.00060
   Liu J. J., 2019, SIMPLE POOLING BASED
   Lyu CZ, 2022, VISUAL COMPUT, V38, P345, DOI 10.1007/s00371-020-02018-w
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Singh NP, 2016, COMPUT METH PROG BIO, V129, P40, DOI 10.1016/j.cmpb.2016.03.001
   Sun W., 2019, ARXIV
   Tuba E., 2017, 27 INT C RAD RADIOEL
   Uslu F, 2019, PATTERN RECOGN, V87, P157, DOI 10.1016/j.patcog.2018.10.017
   Wang B, 2019, LECT NOTES COMPUT SC, V11764, P84, DOI 10.1007/978-3-030-32239-7_10
   Wang C, 2019, ENTROPY-SWITZ, V21, DOI 10.3390/e21020168
   Wang W, 2020, INT C MED IM COMP CO
   Wang WH, 2018, J MED IMAG HEALTH IN, V8, P262, DOI 10.1166/jmihi.2018.2288
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Wu Y., 2019, MEDICAL IMAGE COMPUT, V11764
   Xianjing M., 2020, COMPUT ENG DES, V41, P3148
   Yang TJ, 2020, J DIGIT IMAGING, V33, P946, DOI 10.1007/s10278-020-00339-9
   Yu C, 2018, P IEEE C COMPUTER VI
   Zhang S., 2019, LECT NOTES COMPUTER, V11764
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
NR 37
TC 6
Z9 6
U1 9
U2 74
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2023
VL 39
IS 5
BP 1929
EP 1941
DI 10.1007/s00371-022-02456-8
EA MAY 2022
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA D7ZK3
UT WOS:000792074600001
DA 2024-07-18
ER

PT J
AU Chen, DY
   Song, YY
   Liang, FZ
   Ma, T
   Zhu, XM
   Jia, T
AF Chen, Dongyue
   Song, Yuanyuan
   Liang, Fangzheng
   Ma, Teng
   Zhu, Xiaoming
   Jia, Tong
TI 3D human body reconstruction based on SMPL model
SO VISUAL COMPUTER
LA English
DT Article
DE 3D human reconstruction; SMPL; Pose estimation
ID POSE
AB Recovering 3D human pose and body shape from a monocular image is a challenging task in computer vision. In this paper, we present an optimization-based algorithm and an innovative framework to reconstruct 3D human body from a single monocular image. All the evaluation tasks are established on the basis of the classic parametric 3D body model SMPL. Firstly, a new combined objective function of SMPL parameters is proposed to involve four loss terms on 2D joints, 3D joints, facial landmarks and pose priori, respectively, which increase the reliability of the evaluation results dramatically. Furthermore, we use the estimation results given by an end-to-end regression network as the initial values of the parameters, which has been proved to speed up the optimization process. The experiments on benchmark datasets Human 3.6 M, LSP and a wild dataset demonstrate that our model achieves an accurate and robust estimation of the 3D human body and outperforms the popular competing algorithms in precision and robustness.
C1 [Chen, Dongyue; Song, Yuanyuan; Liang, Fangzheng; Ma, Teng; Zhu, Xiaoming; Jia, Tong] Northeastern Univ, Artificial Intelligence Lab, Shenyang 110000, Peoples R China.
C3 Northeastern University - China
RP Jia, T (corresponding author), Northeastern Univ, Artificial Intelligence Lab, Shenyang 110000, Peoples R China.
EM chendongyue@ise.neu.edu.cn; 2992046483@qq.com; lfz_neucvlab@163.com;
   jiatong@ise.neu.edu.cn
FU National Natural Science Foundation of China [62173083]; Fundamental
   Research Funds for the Central Universities [N2104027]; Innovation Fund
   of Chinese Universities Industry-University-Research [2020HYA06003];
   Guangdong Basic and Applied Basic Research Foundation [2021B1515120064]
FX This research was funded by the National Natural Science Foundation of
   China (62173083), the Fundamental Research Funds for the Central
   Universities (N2104027), the Innovation Fund of Chinese Universities
   Industry-University-Research (2020HYA06003), and the Guangdong Basic and
   Applied Basic Research Foundation (2021B1515120064).
CR Alldieck T, 2018, PROC CVPR IEEE, P8387, DOI 10.1109/CVPR.2018.00875
   Anguelov D, 2005, ACM T GRAPHIC, V24, P408, DOI 10.1145/1073204.1073207
   Arnab A, 2019, PROC CVPR IEEE, P3390, DOI 10.1109/CVPR.2019.00351
   Bogo F, 2016, LECT NOTES COMPUT SC, V9909, P561, DOI 10.1007/978-3-319-46454-1_34
   Bogo F, 2015, IEEE I CONF COMP VIS, P2300, DOI 10.1109/ICCV.2015.265
   Bulat A, 2018, PROC CVPR IEEE, P109, DOI 10.1109/CVPR.2018.00019
   Dibra E, 2016, INT CONF 3D VISION, P108, DOI 10.1109/3DV.2016.19
   Dollár P, 2010, PROC CVPR IEEE, P1078, DOI 10.1109/CVPR.2010.5540094
   Dong XY, 2018, PROC CVPR IEEE, P360, DOI 10.1109/CVPR.2018.00045
   Everingham M., 2010, BMVC, V2, P5
   Fang HS, 2017, IEEE I CONF COMP VIS, P2353, DOI 10.1109/ICCV.2017.256
   GOWER JC, 1975, PSYCHOMETRIKA, V40, P33, DOI 10.1007/BF02291478
   Güler RA, 2019, PROC CVPR IEEE, P10876, DOI 10.1109/CVPR.2019.01114
   Guo KW, 2015, IEEE I CONF COMP VIS, P3083, DOI 10.1109/ICCV.2015.353
   Ionescu C, 2014, IEEE T PATTERN ANAL, V36, P1325, DOI 10.1109/TPAMI.2013.248
   Jackson AS, 2019, LECT NOTES COMPUT SC, V11132, P64, DOI 10.1007/978-3-030-11018-5_6
   Kanazawa A, 2019, PROC CVPR IEEE, P5597, DOI 10.1109/CVPR.2019.00576
   Kanazawa A, 2018, PROC CVPR IEEE, P7122, DOI 10.1109/CVPR.2018.00744
   KAZEMI V, 2014, PROC CVPR IEEE, P1867, DOI [DOI 10.1109/CVPR.2014.241, 10.1109/CVPR.2014.241]
   Kolotouros N, 2019, IEEE I CONF COMP VIS, P2252, DOI 10.1109/ICCV.2019.00234
   Kowalski M, 2017, IEEE COMPUT SOC CONF, P2034, DOI 10.1109/CVPRW.2017.254
   Lassner C., 2017, PROC CVPR IEEE, V2, P3, DOI DOI 10.1109/CVPR.2017.500
   Li H, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618521
   Li Zhong guo, 2019, SCIA
   Loper M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818013
   Martinez J, 2017, IEEE I CONF COMP VIS, P2659, DOI 10.1109/ICCV.2017.288
   Mehta D, 2017, INT CONF 3D VISION, P506, DOI 10.1109/3DV.2017.00064
   Mehta D, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073596
   Merget D, 2018, PROC CVPR IEEE, P781, DOI 10.1109/CVPR.2018.00088
   Newcombe RA, 2015, PROC CVPR IEEE, P343, DOI 10.1109/CVPR.2015.7298631
   Newcombe RA, 2011, INT SYM MIX AUGMENT, P127, DOI 10.1109/ISMAR.2011.6092378
   Newell A, 2016, PROCEEDINGS OF THE ELEVENTH EUROPEAN CONFERENCE ON COMPUTER SYSTEMS, (EUROSYS 2016), DOI 10.1145/2901318.2901343
   Nocedal J, 2006, SPRINGER SER OPER RE, P1, DOI 10.1007/978-0-387-40065-5
   Omran M, 2018, INT CONF 3D VISION, P484, DOI 10.1109/3DV.2018.00062
   Pavlakos G, 2019, PROC CVPR IEEE, P10967, DOI 10.1109/CVPR.2019.01123
   Pavlakos G, 2018, PROC CVPR IEEE, P459, DOI 10.1109/CVPR.2018.00055
   Ren SQ, 2014, PROC CVPR IEEE, P1685, DOI 10.1109/CVPR.2014.218
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Sun Y, 2013, PROC CVPR IEEE, P3476, DOI 10.1109/CVPR.2013.446
   Tan Vince, 2017, BMVC
   Tung HYF, 2017, ADV NEUR IN, V30
   Varol G, 2018, LECT NOTES COMPUT SC, V11211, P20, DOI 10.1007/978-3-030-01234-2_2
   Wei SE, 2016, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2016.511
   Weiss A, 2011, IEEE I CONF COMP VIS, P1951, DOI 10.1109/ICCV.2011.6126465
   Wu WY, 2018, PROC CVPR IEEE, P2129, DOI 10.1109/CVPR.2018.00227
   Zanfir A, 2018, PROC CVPR IEEE, P2148, DOI 10.1109/CVPR.2018.00229
   Zhang F, 2019, PROC CVPR IEEE, P3512, DOI 10.1109/CVPR.2019.00363
   Zhang Q, 2014, PROC CVPR IEEE, P676, DOI 10.1109/CVPR.2014.92
   Zhang Y, 2016, IEEE T IMAGE PROCESS, V25, DOI 10.1109/TIP.2016.2549360
   Zheng GH, 2020, NEUROCOMPUTING, V404, P186, DOI 10.1016/j.neucom.2020.04.108
   Zheng ZR, 2019, IEEE I CONF COMP VIS, P7738, DOI 10.1109/ICCV.2019.00783
   Zhou EJ, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P386, DOI 10.1109/ICCVW.2013.58
NR 52
TC 7
Z9 7
U1 3
U2 39
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2023
VL 39
IS 5
BP 1893
EP 1906
DI 10.1007/s00371-022-02453-x
EA APR 2022
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA D7ZK3
UT WOS:000784625300002
DA 2024-07-18
ER

PT J
AU Chen, QY
   Feng, DZ
   Hu, HS
AF Chen, Qing-Yan
   Feng, Da-Zheng
   Hu, Hao-Shuang
TI A robust non-rigid point set registration algorithm using both local and
   global constraints
SO VISUAL COMPUTER
LA English
DT Article
DE Non-rigid registration; Gaussian mixture models; Local structure;
   Hausdorff distance; Expectation maximization
ID GAUSSIAN MIXTURE MODEL; HAUSDORFF DISTANCE
AB The goal of non-rigid point set registration is to estimate the optimal correspondence between points, and then recover the non-rigid deformation between point sets in a specific way, typically by using a set of complex interpolation functions. Many non-rigid matching algorithms have been studied, but only a few algorithms fully exploit the local structure between point sets. To improve the accuracy of point set registration, this paper proposes a new non-rigid registration algorithm that uses both the global structure and the stable local structure of a non-rigid shape to constrain the registration. Specifically, we consider the point set registration problem as a probability assignment problem, with the probability determined by the Gaussian mixture model and the local structure of the point set. In particular, the Hausdorff distance can effectively measure the similarity of the local structure of the point set in the proposed algorithm. The transformation between the two-point sets is determined by the reproducing kernel Hilbert space based on the motion coherence theory once the correspondence is determined. A significant number of experiments show that the proposed technique has higher registration accuracy than several other state-of-the-art algorithms when dealing with non-rigid registration problems, especially when the point set contains outliers and severely missing points.
C1 [Chen, Qing-Yan; Feng, Da-Zheng; Hu, Hao-Shuang] Xidian Univ, Natl Lab Radar Signal Proc, Xian 710071, Peoples R China.
C3 Xidian University
RP Chen, QY (corresponding author), Xidian Univ, Natl Lab Radar Signal Proc, Xian 710071, Peoples R China.
EM qychen_1@stu.xidian.edu.cn
OI chen, qingyan/0000-0003-1725-6494
CR Belongie S, 2002, IEEE T PATTERN ANAL, V24, P509, DOI 10.1109/34.993558
   BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791
   Chen J, 2015, SIGNAL PROCESS, V106, P62, DOI 10.1016/j.sigpro.2014.07.004
   Chetverikov D, 2005, IMAGE VISION COMPUT, V23, P299, DOI 10.1016/j.imavis.2004.05.007
   Choi J, 2020, VISUAL COMPUT, V36, P2039, DOI 10.1007/s00371-020-01902-9
   Chui HL, 2003, COMPUT VIS IMAGE UND, V89, P114, DOI 10.1016/S1077-3142(03)00009-2
   Chui HL, 2000, IEEE WORKSHOP ON MATHEMATICAL METHODS IN BIOMEDICAL IMAGE ANALYSIS, PROCEEDINGS, P190, DOI 10.1109/MMBIA.2000.852377
   Deng HW, 2018, PROC CVPR IEEE, P195, DOI 10.1109/CVPR.2018.00028
   Fischer A, 2015, PATTERN RECOGN, V48, P331, DOI 10.1016/j.patcog.2014.07.015
   Fitzgibbon AW, 2003, IMAGE VISION COMPUT, V21, P1145, DOI 10.1016/j.imavis.2003.09.004
   Ge S, 2019, MACH VISION APPL, V30, P717, DOI 10.1007/s00138-019-01024-w
   Hirose O, 2021, IEEE T PATTERN ANAL, V43, P2269, DOI 10.1109/TPAMI.2020.2971687
   Hu L, 2020, VISUAL COMPUT, V36, P669, DOI 10.1007/s00371-019-01648-z
   Hui KC, 1998, COMPUT AIDED DESIGN, V30, P823, DOI 10.1016/S0010-4485(98)00040-2
   Iglesias Jose Pedro, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8284, DOI 10.1109/CVPR42600.2020.00831
   Jain A, 2015, PATTERN RECOGN LETT, V68, P351, DOI 10.1016/j.patrec.2015.07.004
   Jian B, 2011, IEEE T PATTERN ANAL, V33, P1633, DOI 10.1109/TPAMI.2010.223
   Jiayuan Li, 2021, IEEE Transactions on Geoscience and Remote Sensing, V59, P9716, DOI 10.1109/TGRS.2020.3045456
   Kán P, 2019, VISUAL COMPUT, V35, P873, DOI 10.1007/s00371-019-01666-x
   Khoo Y, 2016, IEEE T IMAGE PROCESS, V25, P2956, DOI 10.1109/TIP.2016.2540810
   Krishnakumar K, 2020, VISUAL COMPUT, V36, P1837, DOI 10.1007/s00371-019-01780-w
   Liao QF, 2021, IEEE T PATTERN ANAL, V43, P3229, DOI 10.1109/TPAMI.2020.2978477
   Liu BH, 2008, INFORM PROCESS LETT, V106, P52, DOI 10.1016/j.ipl.2007.10.003
   Lowe DG., 1994, INT J COMPUT VISION, V8, P122
   Ma JY, 2021, INT J COMPUT VISION, V129, DOI 10.1007/s11263-020-01359-2
   Ma JY, 2019, PATTERN RECOGN, V92, P231, DOI 10.1016/j.patcog.2019.04.001
   Ma JY, 2019, IEEE T IMAGE PROCESS, V28, P4045, DOI 10.1109/TIP.2019.2906490
   Ma JY, 2019, INT J COMPUT VISION, V127, P512, DOI [10.1109/TMAG.2017.2763198, 10.1007/s11263-018-1117-z]
   Ma JY, 2017, INFORM SCIENCES, V417, P128, DOI 10.1016/j.ins.2017.07.010
   Ma JY, 2016, IEEE T IMAGE PROCESS, V25, P53, DOI 10.1109/TIP.2015.2467217
   Ma JY, 2015, IEEE T GEOSCI REMOTE, V53, P6469, DOI 10.1109/TGRS.2015.2441954
   Ma JY, 2015, IEEE T SIGNAL PROCES, V63, P1115, DOI 10.1109/TSP.2014.2388434
   Ma YX, 2017, IMAGE VISION COMPUT, V67, P52, DOI 10.1016/j.imavis.2017.09.003
   Maiseli B, 2017, J VIS COMMUN IMAGE R, V46, P95, DOI 10.1016/j.jvcir.2017.03.012
   Meng FY, 2015, IEEE T IMAGE PROCESS, V24, P4160, DOI 10.1109/TIP.2015.2456633
   Myronenko A, 2010, IEEE T PATTERN ANAL, V32, P2262, DOI 10.1109/TPAMI.2010.46
   ROTE G, 1991, INFORM PROCESS LETT, V38, P123, DOI 10.1016/0020-0190(91)90233-8
   Rusinkiewicz S, 2001, THIRD INTERNATIONAL CONFERENCE ON 3-D DIGITAL IMAGING AND MODELING, PROCEEDINGS, P145, DOI 10.1109/IM.2001.924423
   Rusu RB, 2009, IEEE INT CONF ROBOT, P1848
   Sim DG, 1999, IEEE T IMAGE PROCESS, V8, P425, DOI 10.1109/83.748897
   Tangelder JH, 2008, MULTIMED TOOLS APPL, V39, P441, DOI 10.1007/s11042-007-0181-0
   Tsin Y, 2004, LECT NOTES COMPUT SC, V3023, P558
   Vedaldi A., 2010, P 18 ACM INT C MULT, P1469, DOI DOI 10.1145/1873951.1874249
   Vongkulbhisal J, 2018, PROC CVPR IEEE, P2993, DOI 10.1109/CVPR.2018.00316
   Wang G, 2021, IEEE T NEUR NET LEAR, V32, P203, DOI 10.1109/TNNLS.2020.2978031
   Wang Y., 2001, VISUAL COMPUT, V24, P185
   Yang GQ, 2022, VISUAL COMPUT, V38, P603, DOI 10.1007/s00371-020-02037-7
   Yang JL, 2016, IEEE T PATTERN ANAL, V38, P2241, DOI 10.1109/TPAMI.2015.2513405
   YUILLE AL, 1989, INT J COMPUT VISION, V3, P155, DOI 10.1007/BF00126430
   Zhang MF, 2016, INT SYMP PARA DISTR, P382, DOI 10.1109/ISPDC.2016.64
   Zheng YF, 2006, IEEE T PATTERN ANAL, V28, P643, DOI 10.1109/TPAMI.2006.81
NR 51
TC 5
Z9 5
U1 1
U2 23
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2023
VL 39
IS 3
BP 1217
EP 1234
DI 10.1007/s00371-022-02400-w
EA FEB 2022
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 9E6QG
UT WOS:000752196300002
DA 2024-07-18
ER

PT J
AU Zhang, YB
   Han, S
   Zhang, ZX
   Wang, JY
   Bi, HB
AF Zhang, Yubo
   Han, Shuang
   Zhang, Zhongxin
   Wang, Jianyang
   Bi, Hongbo
TI CF-GAN: cross-domain feature fusion generative adversarial network for
   text-to-image synthesis
SO VISUAL COMPUTER
LA English
DT Article
DE Text-to-image; Generative adversarial networks; Residual structure; Deep
   learning
AB In recent years, generative adversarial networks have successfully synthesized images through text descriptions. However, there are still problems that the generated image cannot be deeply embedded in the text description semantics, the target object of the generated image is incomplete, and the texture structure of the target object is not rich enough. Consequently, we propose a network framework, cross-domain feature fusion generative adversarial network (CF-GAN), which includes two modules, feature fusion-enhanced response module (FFERM) and multi-branch residual module (MBRM), to fine-grain the generated images with the way of deep fusion. FFERM can integrate both the word-level vector features and image features deeply. MBRM is a relatively simple and innovative residual network structure instead of the traditional residual module to extract features fully. We conducted experiments on the CUB and COCO datasets, and the results reveal that the Inception Score has improved from 4.36 to 4.83 (increased by 10.78%) on the CUB dataset, compared with AttnGAN. Compared with DM-GAN, the Inception Score has increased from 30.49 to 31.13 (increased by 2.06%) on the COCO dataset. Extensive experiments and ablation studies demonstrate the proposed CF-GAN's superiority compared to other methods.
C1 [Zhang, Yubo; Han, Shuang; Zhang, Zhongxin; Wang, Jianyang; Bi, Hongbo] Northeast Petr Univ, Sch Elect Informat Engn, Daqing 163000, Peoples R China.
C3 Northeast Petroleum University
RP Bi, HB (corresponding author), Northeast Petr Univ, Sch Elect Informat Engn, Daqing 163000, Peoples R China.
EM zhangyubo@nepu.edu.cn; hanshuang970411@163.com;
   zhangzhongxin2021@126.com; wjy971013@163.com; bhbdq@126.com
RI Wang, Jing/IQW-3496-2023; chen, wang/KGK-5932-2024; wang,
   xu/IAN-4886-2023; wang, jing/HJA-5384-2022; Wu, Jing/GZK-5063-2022;
   jing, wang/KCZ-2144-2024; wang, juan/IUO-6218-2023; zhang,
   yubo/I-4301-2016; Wang, Jin/GYA-2019-2022; Wang, Jinyang/JXN-8650-2024;
   wang, jie/HTQ-4920-2023; wang, jian/HRB-9588-2023
OI Wang, Jing/0000-0002-8296-2961; Bi, Hongbo/0000-0003-2442-330X
CR Nguyen A, 2017, PROC CVPR IEEE, P3510, DOI 10.1109/CVPR.2017.374
   [Anonymous], 2017, arXiv preprint arXiv:1703.06412
   [Anonymous], 2013, MODERN APPL STAT S P
   Bishop C. M., 1995, NEURAL NETWORKS PATT
   Chen Xiang, 2019, ARXIV190405729
   Dong H, 2017, IEEE I CONF COMP VIS, pCP1, DOI 10.1109/ICCV.2017.608
   Goodfellow I. J., 2015, 3 INT C LEARNING REP
   Han K, 2020, PROC CVPR IEEE, P1577, DOI 10.1109/CVPR42600.2020.00165
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   King DB, 2015, ACS SYM SER, V1214, P1
   Li B, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P1991
   Li WH, 2019, PROC CVPR IEEE, P4998, DOI 10.1109/CVPR.2019.00514
   Li YT, 2019, PROC CVPR IEEE, P6322, DOI 10.1109/CVPR.2019.00649
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Ni JC, 2020, IEEE ACCESS, V8, P37697, DOI 10.1109/ACCESS.2020.2975841
   Qiao TT, 2019, PROC CVPR IEEE, P1505, DOI 10.1109/CVPR.2019.00160
   Radford A., 2015, ARXIV151106434
   Reed S. E., 2016, ADV NEURAL INFORM PR, V29, P217
   Reed S, 2016, PR MACH LEARN RES, V48
   Ripley BD, 1996, PATTERN RECOGNITION
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Salimans T, 2016, ADV NEUR IN, V29
   Shi C, 2019, INFORM SCIENCES, V490, P1, DOI 10.1016/j.ins.2019.03.055
   Simonyan K., 2014, CORR
   Tan FW, 2019, PROC CVPR IEEE, P6703, DOI 10.1109/CVPR.2019.00687
   Tan HC, 2019, IEEE I CONF COMP VIS, P10500, DOI 10.1109/ICCV.2019.01060
   Tao Ming, 2020, ARXIV200805865
   Wah C., 2011, CALTECH UCSD BIRDS 2
   Xu T, 2018, PROC CVPR IEEE, P1316, DOI 10.1109/CVPR.2018.00143
   Yang YH, 2021, IEEE T IMAGE PROCESS, V30, P2798, DOI 10.1109/TIP.2021.3055062
   Yin GJ, 2019, PROC CVPR IEEE, P2322, DOI 10.1109/CVPR.2019.00243
   Zhang H, 2019, IEEE T PATTERN ANAL, V41, P1947, DOI 10.1109/TPAMI.2018.2856256
   Zhang H, 2017, IEEE I CONF COMP VIS, P5908, DOI 10.1109/ICCV.2017.629
   Zhang ZQ, 2021, 2021 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP 2021), P4195, DOI 10.1109/ICASSP39728.2021.9414166
   Zhang ZZ, 2018, PROC CVPR IEEE, P6199, DOI 10.1109/CVPR.2018.00649
   Zhu MF, 2019, PROC CVPR IEEE, P5795, DOI 10.1109/CVPR.2019.00595
NR 36
TC 11
Z9 11
U1 3
U2 19
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2023
VL 39
IS 4
BP 1283
EP 1293
DI 10.1007/s00371-022-02404-6
EA FEB 2022
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA C3OY5
UT WOS:000750684900002
DA 2024-07-18
ER

PT J
AU Marvasti-Zadeh, SM
   Ghanei-Yakhdan, H
   Kasaei, S
   Nasrollahi, K
   Moeslund, TB
AF Marvasti-Zadeh, Seyed Mojtaba
   Ghanei-Yakhdan, Hossein
   Kasaei, Shohreh
   Nasrollahi, Kamal
   Moeslund, Thomas B.
TI Effective fusion of deep multitasking representations for robust visual
   tracking
SO VISUAL COMPUTER
LA English
DT Article
DE Appearance modeling; Discriminative correlation filters; Deep
   convolutional neural networks; Robust visual tracking
ID CORRELATION FILTER TRACKER; ONLINE OBJECT TRACKING
AB Visual object tracking remains an active research field in computer vision due to persisting challenges with various problem-specific factors in real-world scenes. Many existing tracking methods based on discriminative correlation filters (DCFs) employ feature extraction networks (FENs) to model the target appearance during the learning process. However, using deep feature maps extracted from FENs based on different residual neural networks (ResNets) has not previously been investigated. This paper aims to evaluate the performance of 12 state-of-the-art ResNet-based FENs in a DCF-based framework to determine the best for visual tracking purposes. First, it ranks their best feature maps and explores the generalized adoption of the best ResNet-based FEN into another DCF-based method. Then, the proposed method extracts deep semantic information from a fully convolutional FEN and fuses it with the best ResNet-based feature maps to strengthen the target representation in the learning process of continuous convolution filters. Finally, it introduces a new and efficient semantic weighting method (using semantic segmentation feature maps on each video frame) to reduce the drift problem. Extensive experimental results on the well-known OTB-2013, OTB-2015, TC-128, UAV-123 and VOT-2018 visual tracking datasets demonstrate that the proposed method effectively outperforms state-of-the-art methods in terms of precision and robustness of visual tracking.
C1 [Marvasti-Zadeh, Seyed Mojtaba; Ghanei-Yakhdan, Hossein] Yazd Univ, Dept Elect Engn, Digital Image & Video Proc Lab Di, Yazd, Iran.
   [Kasaei, Shohreh] Sharif Univ Technol, Dept Comp Engn, Image Proc Lab IPL, Tehran, Iran.
   [Nasrollahi, Kamal; Moeslund, Thomas B.] Aalborg Univ, Dept Architecture Design & Media Technol, Visual Anal & Percept Lab VAP, Aalborg, Denmark.
C3 University of Yazd; Sharif University of Technology; Aalborg University
RP Ghanei-Yakhdan, H (corresponding author), Yazd Univ, Dept Elect Engn, Digital Image & Video Proc Lab Di, Yazd, Iran.
EM mojtaba.marvasti@stu.yazd.ac.ir; hghaneiy@yazd.ac.ir; kasaei@sharif.edu;
   kn@create.aau.dk; tbm@create.aau.dk
RI Ghanei-Yakhdan, Hossein/D-7382-2018; Marvasti-Zadeh, Seyed
   Mojtaba/H-7445-2017
OI Marvasti-Zadeh, Seyed Mojtaba/0000-0003-0536-0796; Moeslund, Thomas
   B./0000-0001-7584-5209; Ghanei-Yakhdan, Hosein/0000-0003-4575-1062
FU Iran National Science Foundation (INSF) [96013046]
FX This work was partly supported by a grant (No. 96013046) from Iran
   National Science Foundation (INSF).
CR [Anonymous], 2016, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2016.465
   Bertinetto L, 2016, PROC CVPR IEEE, P1401, DOI 10.1109/CVPR.2016.156
   Bertinetto L, 2016, LECT NOTES COMPUT SC, V9914, P850, DOI 10.1007/978-3-319-48881-3_56
   Boyd S., 2011, FOUND TRENDS MACH LE, V3, P1, DOI DOI 10.1561/2200000016
   Cehovin L, 2017, NEUROCOMPUTING, V260, P5, DOI 10.1016/j.neucom.2017.02.036
   Chatfield K, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.76
   Che MQ, 2019, LECT NOTES COMPUT SC, V11129, P70, DOI 10.1007/978-3-030-11009-3_3
   Chen Z, 2020, VISUAL COMPUT, V36, P425, DOI 10.1007/s00371-019-01631-8
   Chi ZZ, 2017, IEEE T IMAGE PROCESS, V26, P2005, DOI 10.1109/TIP.2017.2669880
   Choi J, 2018, PROC CVPR IEEE, P479, DOI 10.1109/CVPR.2018.00057
   Dai KN, 2019, PROC CVPR IEEE, P4665, DOI 10.1109/CVPR.2019.00480
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Danelljan M, 2017, PROC CVPR IEEE, P6931, DOI 10.1109/CVPR.2017.733
   Danelljan M, 2017, IEEE T PATTERN ANAL, V39, P1561, DOI 10.1109/TPAMI.2016.2609928
   Danelljan M, 2016, PROC CVPR IEEE, P1430, DOI 10.1109/CVPR.2016.159
   Danelljan M, 2016, LECT NOTES COMPUT SC, V9909, P472, DOI 10.1007/978-3-319-46454-1_29
   Danelljan M, 2015, IEEE I CONF COMP VIS, P4310, DOI 10.1109/ICCV.2015.490
   Danelljan M, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P621, DOI 10.1109/ICCVW.2015.84
   Du F, 2018, SIGNAL PROCESS-IMAGE, V67, P58, DOI 10.1016/j.image.2018.05.013
   Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5
   Fan H., 2018, ARXIV181206148
   Fan H, 2019, IEEE T IMAGE PROCESS, V28, P4130, DOI 10.1109/TIP.2019.2904789
   Fan H, 2017, IEEE I CONF COMP VIS, P5487, DOI 10.1109/ICCV.2017.585
   Galoogahi HK, 2017, IEEE I CONF COMP VIS, P1144, DOI [10.1109/ICCV.2017.128, 10.1109/ICCV.2017.129]
   Gao JY, 2019, PROC CVPR IEEE, P4644, DOI 10.1109/CVPR.2019.00478
   Girshick R., 2014, PROC CVPR IEEE, DOI [DOI 10.1109/CVPR.2014.81, 10.1109/CVPR.2014.81]
   Gladh S, 2016, INT C PATT RECOG, P1243, DOI 10.1109/ICPR.2016.7899807
   Gundogdu E, 2018, IEEE T IMAGE PROCESS, V27, P2526, DOI 10.1109/TIP.2018.2806280
   Guo Q, 2017, IEEE I CONF COMP VIS, P1781, DOI 10.1109/ICCV.2017.196
   Hare S, 2016, IEEE T PATTERN ANAL, V38, P2096, DOI 10.1109/TPAMI.2015.2509974
   He AF, 2018, PROC CVPR IEEE, P4834, DOI 10.1109/CVPR.2018.00508
   He K., 2016, PROC CVPR IEEE, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]
   He ZQ, 2017, IEEE INT CONF COMP V, P1992, DOI 10.1109/ICCVW.2017.233
   Henriques JF, 2015, IEEE T PATTERN ANAL, V37, P583, DOI 10.1109/TPAMI.2014.2345390
   Henriques JF, 2012, LECT NOTES COMPUT SC, V7575, P702, DOI 10.1007/978-3-642-33765-9_50
   Hong ZB, 2015, PROC CVPR IEEE, P749, DOI 10.1109/CVPR.2015.7298675
   Hongyi Su, 2015, Intelligent Computing Theories and Methodologies. 11th International Conference, ICIC 2015. Proceedings: LNCS 9225, P1, DOI 10.1007/978-3-319-22180-9_1
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Huang YH, 2020, MULTIMED TOOLS APPL, V79, P35017, DOI 10.1007/s11042-019-08308-3
   Huang Z., 2017, P IEEE C COMP VIS PA, P4021, DOI DOI 10.1109/CVPR.2017.510
   Kalal Z, 2012, IEEE T PATTERN ANAL, V34, P1409, DOI 10.1109/TPAMI.2011.239
   Kristan M, 2019, LECT NOTES COMPUT SC, V11129, P3, DOI 10.1007/978-3-030-11009-3_1
   Kuai YL, 2018, J VIS COMMUN IMAGE R, V51, P104, DOI 10.1016/j.jvcir.2018.01.008
   LEE H, 2019, P ECCVW, P100, DOI DOI 10.1007/978-3-030-11009-3_5
   Li B, 2018, PROC CVPR IEEE, P8971, DOI 10.1109/CVPR.2018.00935
   Li DD, 2019, J VIS COMMUN IMAGE R, V58, P149, DOI 10.1016/j.jvcir.2018.11.036
   Li F, 2018, PROC CVPR IEEE, P4904, DOI 10.1109/CVPR.2018.00515
   Li F, 2017, IEEE INT CONF COMP V, P2001, DOI 10.1109/ICCVW.2017.234
   Li PX, 2018, PATTERN RECOGN, V76, P323, DOI 10.1016/j.patcog.2017.11.007
   Li S., 2018, IEEE T CIRC SYST VID
   Li X, 2019, PROC CVPR IEEE, P1369, DOI 10.1109/CVPR.2019.00146
   Li Y, 2015, LECT NOTES COMPUT SC, V8926, P254, DOI 10.1007/978-3-319-16181-5_18
   Liang PP, 2015, IEEE T IMAGE PROCESS, V24, P5630, DOI 10.1109/TIP.2015.2482905
   Liang Y, 2019, MULTIMED TOOLS APPL, V78, P14195, DOI 10.1007/s11042-018-6760-4
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lin ZG, 2018, IEEE IMAGE PROC, P4103, DOI 10.1109/ICIP.2018.8451826
   Liu J, 2020, VISUAL COMPUT, V36, P1909, DOI 10.1007/s00371-019-01776-6
   Liu M, 2018, IET IMAGE PROCESS, V12, P2023, DOI 10.1049/iet-ipr.2018.5454
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Lugmayr Andreas, 2020, CVPR WORKSH
   Lukezic A, 2018, INT J COMPUT VISION, V126, P671, DOI 10.1007/s11263-017-1061-3
   Ma C, 2019, IEEE T PATTERN ANAL, V41, P2709, DOI [10.1109/TPAMI.2018.2865311, 10.1109/INTMAG.2018.8508195]
   Ma C, 2018, INT J COMPUT VISION, V126, P771, DOI 10.1007/s11263-018-1076-4
   Ma C, 2016, IEEE SIGNAL PROC LET, V23, P1454, DOI 10.1109/LSP.2016.2601691
   Ma C, 2015, IEEE I CONF COMP VIS, P3074, DOI 10.1109/ICCV.2015.352
   Marvasti-Zadeh S.M., 2021, COMPUTER VISION ACCV, V2623, DOI [10.1007/978-3-030-69532-3_36, DOI 10.1007/978-3-030-69532-3_36]
   Marvasti-Zadeh SM, 2022, IEEE T INTELL TRANSP, V23, P3943, DOI 10.1109/TITS.2020.3046478
   Marvasti-Zadeh SM, 2019, IRAN CONF ELECTR ENG, P1272, DOI [10.1109/IranianCEE.2019.8786548, 10.1109/iraniancee.2019.8786548]
   Mozhdehi RJ, 2018, IEEE IMAGE PROC, P798, DOI 10.1109/ICIP.2018.8451069
   Mozhdehi RJ, 2017, IEEE IMAGE PROC, P3650, DOI 10.1109/ICIP.2017.8296963
   Mueller M, 2016, LECT NOTES COMPUT SC, V9905, P445, DOI 10.1007/978-3-319-46448-0_27
   Nah S., 2020, P IEEE CVPRW
   Pu S., 2018, ADV NEURAL INFORM PR, V31, P1931, DOI DOI 10.1016/J.PATCOG.2018.10.005
   Qi YK, 2016, PROC CVPR IEEE, P4303, DOI 10.1109/CVPR.2016.466
   Rout L, 2019, LECT NOTES COMPUT SC, V11129, P83, DOI 10.1007/978-3-030-11009-3_4
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Sang HF, 2020, MULTIMED TOOLS APPL, V79, P5529, DOI 10.1007/s11042-019-08269-7
   Shu X, 2020, MULTIMED TOOLS APPL, V79, P23617, DOI 10.1007/s11042-020-09018-x
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song YB, 2017, IEEE I CONF COMP VIS, P2574, DOI 10.1109/ICCV.2017.279
   Sun C, 2018, PROC CVPR IEEE, P489, DOI 10.1109/CVPR.2018.00058
   Sun C, 2018, PROC CVPR IEEE, P8962, DOI 10.1109/CVPR.2018.00934
   Sun YX, 2019, PROC CVPR IEEE, P5776, DOI 10.1109/CVPR.2019.00593
   Tang FH, 2019, NEUROCOMPUTING, V333, P29, DOI 10.1016/j.neucom.2018.12.035
   Tong K, 2020, IMAGE VISION COMPUT, V97, DOI 10.1016/j.imavis.2020.103910
   Valmadre J, 2017, PROC CVPR IEEE, P5000, DOI 10.1109/CVPR.2017.531
   van de Weijer J, 2007, PROC CVPR IEEE, P1898
   Wang LJ, 2015, IEEE I CONF COMP VIS, P3119, DOI 10.1109/ICCV.2015.357
   Wang N, 2018, PROC CVPR IEEE, P4844, DOI 10.1109/CVPR.2018.00509
   Wang Q., 2017, arXiv preprint arXiv:1704.04057
   Wang X., 2018, P ICIP, P660
   Wang Y, 2019, MULTIMED TOOLS APPL, V78, P31633, DOI 10.1007/s11042-019-07851-3
   Wu Y, 2015, IEEE T PATTERN ANAL, V37, P1834, DOI 10.1109/TPAMI.2014.2388226
   Wu Y, 2013, PROC CVPR IEEE, P2411, DOI 10.1109/CVPR.2013.312
   Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634
   Yang X, 2020, VISUAL COMPUT, V36, P1783, DOI 10.1007/s00371-019-01772-w
   Yi Y, 2019, MULTIMED TOOLS APPL, V78, P12333, DOI 10.1007/s11042-018-6787-6
   Zhang JM, 2014, LECT NOTES COMPUT SC, V8694, P188, DOI 10.1007/978-3-319-10599-4_13
   Zhang P, 2017, NEUROCOMPUTING, V257, P115, DOI 10.1016/j.neucom.2016.10.073
   Zhang TZ, 2017, PROC CVPR IEEE, P4819, DOI [10.1109/CVPR.2017.512, 10.1109/ICCV.2017.469]
   Zhang ZP, 2019, PROC CVPR IEEE, P4586, DOI 10.1109/CVPR.2019.00472
   Zheng S, 2015, IEEE I CONF COMP VIS, P1529, DOI 10.1109/ICCV.2015.179
   Zhu Z, 2017, IEEE INT CONF COMP V, P1973, DOI 10.1109/ICCVW.2017.231
NR 104
TC 1
Z9 1
U1 1
U2 18
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2022
VL 38
IS 12
BP 4397
EP 4417
DI 10.1007/s00371-021-02304-1
EA OCT 2021
PG 21
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 7B7SH
UT WOS:000708789800001
DA 2024-07-18
ER

PT J
AU Liu, X
   Wang, MH
   Wang, AZ
   Hua, XY
   Liu, SS
AF Liu, Xia
   Wang, Minghui
   Wang, Anzhi
   Hua, Xiyao
   Liu, Shanshan
TI Depth-guided learning light field angular super-resolution with
   edge-aware inpainting
SO VISUAL COMPUTER
LA English
DT Article
DE Light field; Angular super-resolution; Reconstruction; Convolutional
   neural network
AB High angular resolution light field (LF) enables exciting applications such as depth estimation, virtual reality, and augmented reality. Although many light field angular super-resolution methods have been proposed, the reconstruction problem of LF with a wide-baseline is far from being solved. In this paper, we propose an end-to-end learning-based approach to achieve angular super-resolution of the light field with a wide-baseline. Our model consists of three components. We first train a convolutional neural network to predict the depth map for each sub-aperture view. Then the estimated depth maps are used to warp the input views. In the final component, we first use a convolutional neural network to fuse the initial warped light fields, and then we propose an edge-aware inpainting network to modify the inaccurate pixels in the near-edge regions. Accordingly, we design an EdgePyramid structure that contains multi-scale edges to perform the inpainting of near-edge pixels. Moreover, we introduce a novel loss function to reduce the artifacts and further estimate the similarity in near-edge regions. Experimental results on various light field datasets including large-baseline light field images show that our method outperforms the state-of-the-art light field angular super-resolution methods, especially in the terms of visual performance near edges.
C1 [Liu, Xia; Wang, Minghui; Hua, Xiyao; Liu, Shanshan] Sichuan Univ, Coll Comp Sci, Chengdu, Sichuan, Peoples R China.
   [Wang, Anzhi] Guizhou Normal Univ, Sch Big Data & Comp Sci, Guiyang, Guizhou, Peoples R China.
C3 Sichuan University; Guizhou Normal University
RP Wang, MH (corresponding author), Sichuan Univ, Coll Comp Sci, Chengdu, Sichuan, Peoples R China.
EM wangminghui@scu.edu.cn
FU Science and Technology Plan Project of Sichuan Province [2021YFG0350];
   National Key Research and Development Program of China [2016YFB0700802];
   Innovative Youth Fund Program of the State Oceanic Administration of
   China [2015001]
FX This work was supported in part by the Science and Technology Plan
   Project of Sichuan Province under Grant 2021YFG0350, in part by the
   National Key Research and Development Program of China under Grant
   2016YFB0700802, and in part by the Innovative Youth Fund Program of the
   State Oceanic Administration of China under Grant 2015001.
CR Adams, 2018, NEW STANFORD LIGHT F
   Chai T., 2019, ARXIV190206221
   Chaurasia G, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2487228.2487238
   Chaurasia G, 2011, COMPUT GRAPH FORUM, V30, P1223, DOI 10.1111/j.1467-8659.2011.01981.x
   Couillaud J, 2020, VISUAL COMPUT, V36, P237, DOI 10.1007/s00371-018-1599-2
   Dong C, 2014, LECT NOTES COMPUT SC, V8692, P184, DOI 10.1007/978-3-319-10593-2_13
   Habtegebrial T, 2019, PROCEEDINGS OF THE 14TH INTERNATIONAL JOINT CONFERENCE ON COMPUTER VISION, IMAGING AND COMPUTER GRAPHICS THEORY AND APPLICATIONS (VISAPP), VOL 5, P792, DOI 10.5220/0007360107920799
   Honauer K, 2017, LECT NOTES COMPUT SC, V10113, P19, DOI 10.1007/978-3-319-54187-7_2
   HUANG G, 2017, PROC CVPR IEEE, P2261, DOI DOI 10.1109/CVPR.2017.243
   Jin J, 2020, AAAI CONF ARTIF INTE, V34, P11141
   Kalantari NK, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980251
   Kauvar I, 2015, ACM T GRAPHIC, V34, DOI [10.1145/2682631, 10.1145/2816795.2818070]
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Levoy M., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P31, DOI 10.1145/237170.237199
   Liu YB, 2006, 2006 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO - ICME 2006, VOLS 1-5, PROCEEDINGS, P2173, DOI 10.1109/ICME.2006.262686
   Min XK, 2020, IEEE T IMAGE PROCESS, V29, P3790, DOI 10.1109/TIP.2020.2966081
   Pearson J, 2013, IEEE T IMAGE PROCESS, V22, P3405, DOI 10.1109/TIP.2013.2268939
   Schilling H, 2018, PROC CVPR IEEE, P4530, DOI 10.1109/CVPR.2018.00476
   Shi JL, 2019, IEEE T IMAGE PROCESS, V28, P5867, DOI 10.1109/TIP.2019.2923323
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Srinivasan PP, 2017, IEEE I CONF COMP VIS, P2262, DOI 10.1109/ICCV.2017.246
   Vadathya A.K., 2018, COMP CAM DISPL CCD W
   Vagharshakyan S, 2018, IEEE T PATTERN ANAL, V40, P133, DOI 10.1109/TPAMI.2017.2653101
   Veeraraghavan, 2012, COMP VIS PATT REC WO
   Wang TC, 2015, IEEE I CONF COMP VIS, P3487, DOI 10.1109/ICCV.2015.398
   Wang YL, 2020, IEEE T COMPUT IMAG, V6, P830, DOI 10.1109/TCI.2020.2986092
   Wang YL, 2018, LECT NOTES COMPUT SC, V11206, P340, DOI 10.1007/978-3-030-01216-8_21
   Wanner S., 2013, DATASETS BENCHMARKS
   Wanner S, 2014, IEEE T PATTERN ANAL, V36, P606, DOI 10.1109/TPAMI.2013.147
   Wetzstein, 2013, SYNTHETIC LIGHT FIEL
   Wilburn B, 2005, ACM T GRAPHIC, V24, P765, DOI 10.1145/1073204.1073259
   Wu YC, 2020, J VIS COMMUN IMAGE R, V72, DOI 10.1016/j.jvcir.2020.102878
   Yoon Y, 2017, IEEE SIGNAL PROC LET, V24, P848, DOI 10.1109/LSP.2017.2669333
   Yoon Y, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P57, DOI 10.1109/ICCVW.2015.17
   Zhang ZT, 2015, PROC CVPR IEEE, P3800, DOI 10.1109/CVPR.2015.7299004
   Zheng HT, 2018, LECT NOTES COMPUT SC, V11210, P87, DOI 10.1007/978-3-030-01231-1_6
NR 36
TC 5
Z9 5
U1 1
U2 20
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2022
VL 38
IS 8
BP 2839
EP 2851
DI 10.1007/s00371-021-02159-6
EA MAY 2021
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 3B2OE
UT WOS:000651332600001
DA 2024-07-18
ER

PT J
AU Wang, Q
   Meng, XY
   Sun, T
   Zhang, XD
AF Wang, Qi
   Meng, Xiangyue
   Sun, Ting
   Zhang, Xiangde
TI A light iris segmentation network
SO VISUAL COMPUTER
LA English
DT Article
DE Iris recognition; Iris segmentation; Fully convolutional network;
   Generative adversarial network
ID NEURAL-NETWORK
AB Iris segmentation plays a vital role in the iris recognition system. However, it faces many challenges in non-ideal situations. To improve the iris segmentation performance for possible mobile devices, this paper presents a light iris segmentation method based on fully convolutional network. Firstly, a lightweight fully convolutional iris segmentation network is developed. Secondly, we adopt weighted loss, multi-level feature dense fusion module, multi-supervised training of multi-scale image and generative adversarial network to improve the segmentation performance. The final model is 6.21 M. Experiments show that the proposed method achieves 99.30% PA, 95.35% mIoU on UBIRIS.v2 and 99.66% PA, 96.75% mIoU on CASIA-Iris-Thousand database, which is relatively encouraging for a light iris segmentation network. It takes 41.56 ms and 63.03 ms to segment an image of UBIRIS.v2 and CASIA-Iris-Thousand databases, respectively.
C1 [Wang, Qi; Meng, Xiangyue; Sun, Ting; Zhang, Xiangde] Northeastern Univ, Dept Math, Coll Sci, Shenyang 110819, Peoples R China.
   [Wang, Qi] Northeastern Univ, Minist Educ, Key Lab Data Analyt & Optimizat Smart Ind, Shenyang, Peoples R China.
C3 Northeastern University - China; Northeastern University - China
RP Zhang, XD (corresponding author), Northeastern Univ, Dept Math, Coll Sci, Shenyang 110819, Peoples R China.
EM wangqimath@mail.neu.edu.cn; 1550719374@qq.com; 13624054135@qq.com;
   zhangxiangde@mail.neu.edu.cn
CR Agrawal A, 2020, VISUAL COMPUT, V36, P405, DOI 10.1007/s00371-019-01630-9
   [Anonymous], 2019, ARXIV190310140
   Arsalan M, 2019, EXPERT SYST APPL, V122, P217, DOI 10.1016/j.eswa.2019.01.010
   Arsalan M, 2017, SYMMETRY-BASEL, V9, DOI 10.3390/sym9110263
   Barnes H, 2018, 2018 IEEE SYMPOSIUM ON ELECTROMAGNETIC COMPATIBILITY, SIGNAL INTEGRITY AND POWER INTEGRITY (EMC, SI & PI)
   Bazrafkan S, 2018, NEURAL NETWORKS, V106, P79, DOI 10.1016/j.neunet.2018.06.011
   Bowyer KW, 2008, COMPUT VIS IMAGE UND, V110, P281, DOI 10.1016/j.cviu.2007.08.005
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Daugman, 2002, EUR CONV SEC DET
   Daugman J, 2007, IEEE T SYST MAN CY B, V37, P1167, DOI 10.1109/TSMCB.2007.903540
   Drozdzal M, 2016, LECT NOTES COMPUT SC, V10008, P179, DOI 10.1007/978-3-319-46976-8_19
   FUKUSHIMA K, 1988, NEURAL NETWORKS, V1, P119, DOI 10.1016/0893-6080(88)90014-7
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Jha RR, 2020, IET BIOMETRICS, V9, P11, DOI 10.1049/iet-bmt.2019.0025
   King DB, 2015, ACS SYM SER, V1214, P1
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541
   Li CC, 2015, VISUAL COMPUT, V31, P1419, DOI 10.1007/s00371-014-1023-5
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Lyu, 2020, VISUAL COMPUT, V8, P22
   Mcbride, 2002, IEEE WORKSH APPL COM
   Naqvi RA, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18020456
   Nianfeng Liu, 2016, 2016 International Conference on Biometrics (ICB), DOI 10.1109/ICB.2016.7550055
   Poudel R.P.K., 2019, ARXIV190204502, P289
   PROCENCA H, 2010, IEEE T PAMI, V32, P1529
   Rahulkar AD, 2012, NEUROCOMPUTING, V81, P12, DOI 10.1016/j.neucom.2011.09.025
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Umer S, 2019, VISUAL COMPUT, V35, P1327, DOI 10.1007/s00371-018-1544-4
   Wang, 2017, J AUTOM, V17, P598
   Wang B, 2020, VISUAL COMPUT, V36, P1897, DOI 10.1007/s00371-019-01779-3
   Wildes RP, 1996, MACH VISION APPL, V9, P1, DOI 10.1007/BF01246633
   Yu CJ, 2018, LECT NOTES COMPUT SC, V11220, P595, DOI 10.1007/978-3-030-01270-0_35
   Yu F., 2015, ARXIV
   Zeiler MD, 2011, IEEE I CONF COMP VIS, P2018, DOI 10.1109/ICCV.2011.6126474
   Zhao ZJ, 2015, IEEE I CONF COMP VIS, P3828, DOI 10.1109/ICCV.2015.436
NR 35
TC 11
Z9 12
U1 3
U2 32
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2022
VL 38
IS 7
BP 2591
EP 2601
DI 10.1007/s00371-021-02134-1
EA MAY 2021
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 2B0WL
UT WOS:000651015200001
DA 2024-07-18
ER

PT J
AU Wang, XY
   Liu, PB
AF Wang, Xingyuan
   Liu, Pengbo
TI Image encryption based on roulette cascaded chaotic system and alienated
   image library
SO VISUAL COMPUTER
LA English
DT Article
DE Image encryption; Roulette wheel selection; Alienated image library;
   Chaotic system
ID SEMI-TENSOR PRODUCT; WHEEL SELECTION; ALGORITHM; MATRIX; CIRCUIT; DESIGN
AB This paper proposes an image encryption algorithm based on roulette jump selection chaotic system and alienated image library transformation. The algorithm obtains the fitness of each system from the plain image. Run the roulette algorithm to choose different chaotic systems according to their fitness, and the roulette algorithm is also used for parameter control, thus effectively reduces the dynamic degradation of chaotic systems. This paper first proposes an alienated image library transformation, which operates the plain image and multiple images in the agreed image library, thereby greatly improving the security of encryption. In the scrambling diffusion, synchronous scrambling diffusion is used. The pseudo-random sequence is sorted as a row and column index, and the image is synchronously scrambled and diffused by a pair of row and column mapping arrays. Through the experimental results and security analysis, the algorithm has very good security and can resist various attacks.
C1 [Wang, Xingyuan; Liu, Pengbo] Dalian Maritime Univ, Sch Informat Sci & Technol, Dalian 116026, Peoples R China.
C3 Dalian Maritime University
RP Wang, XY; Liu, PB (corresponding author), Dalian Maritime Univ, Sch Informat Sci & Technol, Dalian 116026, Peoples R China.
EM xywang@dlmu.edu.cn; 18637190097@163.com
RI Wang, Xing-yuan/I-6353-2015
OI Liu, Pengbo/0000-0003-2344-2584
FU National Natural Science Foundation of China [61672124]; Password Theory
   Project of the 13th Five-Year Plan National Cryptography Development
   Fund [MMJJ20170203]; Liaoning Province Science and Technology Innovation
   Leading Talents Program Project [XLYC1802013]; Key R&D Projects of
   Liaoning Province [2019020105-JH2/103]; Jinan City '20 universities'
   Funding Projects Introducing Innovation Team Program [2019GXRC031]
FX This research is supported by the National Natural Science Foundation of
   China (No: 61672124), the Password Theory Project of the 13th Five-Year
   Plan National Cryptography Development Fund (No: MMJJ20170203), Liaoning
   Province Science and Technology Innovation Leading Talents Program
   Project (No: XLYC1802013), Key R&D Projects of Liaoning Province (No:
   2019020105-JH2/103), Jinan City '20 universities' Funding Projects
   Introducing Innovation Team Program (No: 2019GXRC031).
CR Abd EL-Latif AA, 2020, PHYSICA A, V547, DOI 10.1016/j.physa.2019.123869
   Abd El-Latif AA, 2020, IEEE T NETW SERV MAN, V17, P118, DOI 10.1109/TNSM.2020.2969863
   Abd El-Latif AA, 2018, IEEE ACCESS, V6, P1073, DOI 10.1109/ACCESS.2017.2777869
   Abd El-Latif AA, 2013, SIGNAL PROCESS, V93, P2986, DOI 10.1016/j.sigpro.2013.03.031
   Abd-El-Atty B, 2019, QUANTUM INF PROCESS, V18, DOI 10.1007/s11128-019-2386-3
   Alawida M, 2019, SIGNAL PROCESS, V160, P45, DOI 10.1016/j.sigpro.2019.02.016
   Aslam MN, 2019, IEEE ACCESS, V7, P66395, DOI 10.1109/ACCESS.2019.2911559
   Bechikh R, 2015, SIGNAL PROCESS-IMAGE, V39, P151, DOI 10.1016/j.image.2015.09.006
   Belazi A, 2017, NONLINEAR DYNAM, V87, P337, DOI 10.1007/s11071-016-3046-0
   Belazi A, 2017, OPT LASER ENG, V88, P37, DOI 10.1016/j.optlaseng.2016.07.010
   Benrhouma O, 2015, SIGNAL IMAGE VIDEO P, V9, P1281, DOI 10.1007/s11760-013-0570-y
   Bisht A, 2019, J AMB INTEL HUM COMP, V10, P3519, DOI 10.1007/s12652-018-1072-0
   Cao GH, 2019, MULTIMED TOOLS APPL, V78, P10625, DOI 10.1007/s11042-018-6635-8
   Chai XL, 2017, MULTIMED TOOLS APPL, V76, P1159, DOI 10.1007/s11042-015-3088-1
   Chai XL, 2017, OPT LASER ENG, V88, P197, DOI 10.1016/j.optlaseng.2016.08.009
   Chen X, 2017, SAUDI J BIOL SCI, V24, P1821, DOI 10.1016/j.sjbs.2017.11.023
   Enayatifar R, 2019, OPT LASER ENG, V115, P131, DOI 10.1016/j.optlaseng.2018.11.017
   Enayatifar R, 2014, OPT LASER ENG, V56, P83, DOI 10.1016/j.optlaseng.2013.12.003
   Li GD, 2019, VISUAL COMPUT, V35, P1267, DOI 10.1007/s00371-018-1574-y
   Li SJ, 2005, INT J BIFURCAT CHAOS, V15, P3119, DOI 10.1142/S0218127405014052
   Lipowski A, 2012, PHYSICA A, V391, P2193, DOI 10.1016/j.physa.2011.12.004
   Mansouri A, 2021, VISUAL COMPUT, V37, P189, DOI 10.1007/s00371-020-01791-y
   Pareek NK, 2006, IMAGE VISION COMPUT, V24, P926, DOI 10.1016/j.imavis.2006.02.021
   Sambas A, 2020, IEEE ACCESS, V8, P137116, DOI 10.1109/ACCESS.2020.3011724
   Song XH, 2020, PHYSICA A, V537, DOI 10.1016/j.physa.2019.122660
   Talhaoui MZ, 2021, VISUAL COMPUT, V37, P541, DOI 10.1007/s00371-020-01822-8
   Talhaoui MZ, 2021, VISUAL COMPUT, V37, P1757, DOI 10.1007/s00371-020-01936-z
   Tsafack N, 2020, IEEE ACCESS, V8, P137731, DOI 10.1109/ACCESS.2020.3010794
   Tsafack N, 2020, INFORM SCIENCES, V515, P191, DOI 10.1016/j.ins.2019.10.070
   Wang XY, 2020, INFORM SCIENCES, V539, P195, DOI 10.1016/j.ins.2020.06.030
   Wang XY, 2020, INFORM SCIENCES, V507, P16, DOI 10.1016/j.ins.2019.08.041
   Wang XY, 2019, INFORM SCIENCES, V486, P340, DOI 10.1016/j.ins.2019.02.049
   Weber A. G., 1997, USC SIPI IMAGE DATAB, V315
   Xian YJ, 2021, INFORM SCIENCES, V547, P1154, DOI 10.1016/j.ins.2020.09.055
   Zhang TJ, 2015, INT J SECUR APPL, V9, P217, DOI 10.14257/ijsia.2015.9.7.19
   Zhang Y, 2018, MULTIMED TOOLS APPL, V77, P21589, DOI 10.1007/s11042-017-5585-x
   Zhang Y, 2018, MULTIMED TOOLS APPL, V77, P6647, DOI 10.1007/s11042-017-4577-1
NR 37
TC 14
Z9 16
U1 2
U2 35
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2022
VL 38
IS 3
BP 763
EP 779
DI 10.1007/s00371-020-02048-4
EA JAN 2021
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZQ8YX
UT WOS:000604490300002
DA 2024-07-18
ER

PT J
AU Ishitobi, A
   Nakayama, M
   Fujishiro, I
AF Ishitobi, Akinori
   Nakayama, Masanori
   Fujishiro, Issei
TI Visual simulation of weathering coated metallic objects
SO VISUAL COMPUTER
LA English
DT Article
DE Weathering; Coating; Deformation
AB Weathering, referring to techniques for depicting outdoor objects as having suffered rain and wind damage, has a long history of research, and many methods can be found in the literature. Metal is a representative target of weathering, as metallic surface corrosion is a widespread deterioration phenomenon in our daily life. However, few weathering methods consider the effects of rust-preventive paint, while real metal is usually treated with a preservative. This paper presents a novel procedural method for weathering coated metal objects. In our method, a coated surface is imposed on a 3D triangular mesh. We enable the mechanical deterioration of coating film, and we deform the mesh model to express cracking and peeling. As further defacing elements on the coating film, runoff rust and darkening are reproduced by changing the color and reflectance of the surface. Besides, by locally adjusting the control parameters of the designated areas on the surface, the process of deterioration can be arbitrarily directed. Several visual simulation results are shown to prove empirically the faithfulness of the proposed methods.
C1 [Ishitobi, Akinori; Nakayama, Masanori; Fujishiro, Issei] Keio Univ, Yokohama, Kanagawa, Japan.
   [Fujishiro, Issei] Hangzhou Dianzi Univ, Hangzhou, Peoples R China.
C3 Keio University; Hangzhou Dianzi University
RP Fujishiro, I (corresponding author), Keio Univ, Yokohama, Kanagawa, Japan.; Fujishiro, I (corresponding author), Hangzhou Dianzi Univ, Hangzhou, Peoples R China.
EM fuji@ics.keio.ac.jp
RI Ishitobi, Akinori/GZK-8028-2022
OI Ishitobi, Akinori/0000-0003-0975-6456
FU JSPS KAKENHI [17H00737]; Grants-in-Aid for Scientific Research
   [17H00737] Funding Source: KAKEN
FX We thank the anonymous reviewers for their valuable comments and
   suggestions on our early manuscript. This work has been supported in
   part by JSPS KAKENHI under the Grant-in-Aid for Scientific Research (A)
   No. 17H00737.
CR Angulu Raphael, 2018, EURASIP J IMAGE VIDE, V42, P1
   Bellini R, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925891
   Chang YX, 2003, VISUAL COMPUT, V19, P50, DOI 10.1007/s00371-002-0172-0
   Chen YY, 2005, ACM T GRAPHIC, V24, P1127, DOI 10.1145/1073204.1073321
   Desbenoit B, 2005, VISUAL COMPUT, V21, P717, DOI 10.1007/s00371-005-0317-z
   Dorsey J., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P387, DOI 10.1145/237170.237278
   Gobron S, 2001, NINTH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P338, DOI 10.1109/PCCGA.2001.962890
   Gunther T., 2012, VISION MODELING VISU, P63
   Iizuka S, 2016, COMPUT GRAPH FORUM, V35, P501, DOI 10.1111/cgf.12850
   Ishitobi A, 2019, 2019 INTERNATIONAL CONFERENCE ON CYBERWORLDS (CW), P360, DOI 10.1109/CW.2019.00066
   Jain N., P 2014 IND C COMP VI
   Janssen GCAM, 2009, THIN SOLID FILMS, V517, P1858, DOI 10.1016/j.tsf.2008.07.014
   Jeong S, 2013, COMPUT GRAPH FORUM, V32, P204, DOI 10.1111/cgf.12009
   Kimmel BW, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2421636.2421646
   Kratt J, 2015, COMPUT GRAPH FORUM, V34, P361, DOI 10.1111/cgf.12566
   Merillou S., 2001, P GRAPHICS INTERFACE, P167
   Paquette E, 2002, PROC GRAPH INTERF, P59
   Paquette Eric., 2001, GRAPHICS INTERFACE, P175
   Perlin K, 2002, ACM T GRAPHIC, V21, P681, DOI 10.1145/566570.566636
   Pfaff T, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601132
   Shinozaki S, 2019, IIEEJ T IMAGE ELECT, V7, P106
   Stoney GG, 1909, P R SOC LOND A-CONTA, V82, P172, DOI 10.1098/rspa.1909.0021
   Tanabe R, 2015, IEEE C EVOL COMPUTAT, P2018, DOI 10.1109/CEC.2015.7257133
   THORNTON JA, 1977, J VAC SCI TECHNOL, V14, P164, DOI 10.1116/1.569113
   Tien-Tsin Wong, 1997, Rendering Techniques '97. Proceedings of the Eurographics Workshop. Eurographics, P139
   Wang JP, 2006, ACM T GRAPHIC, V25, P754, DOI 10.1145/1141911.1141951
   Xue S, 2008, COMPUT GRAPH FORUM, V27, P617
   Xue S, 2011, COMPUT GRAPH FORUM, V30, P1189, DOI 10.1111/j.1467-8659.2011.01977.x
   YELLOTT JI, 1983, SCIENCE, V221, P382, DOI 10.1126/science.6867716
NR 29
TC 8
Z9 8
U1 0
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2020
VL 36
IS 10-12
BP 2383
EP 2393
DI 10.1007/s00371-020-01947-w
EA AUG 2020
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA NW1CX
UT WOS:000557344700001
DA 2024-07-18
ER

PT J
AU Noh, ST
   Takahashi, K
   Adachi, M
   Igarashi, T
AF Noh, Seung-Tak
   Takahashi, Kenichi
   Adachi, Masahiko
   Igarashi, Takeo
TI Parametric fur from an image
SO VISUAL COMPUTER
LA English
DT Article
ID HAIR
AB Parametric fur is a powerful tool for content creation in computer graphics. However, setting parameters to realize the desired result is difficult. To address this problem, we propose a method to automatically estimate appropriate parameters from an image. We formulate the process as an optimization problem wherein the system searches for parameters such that the appearance of the rendered parametric fur is as similar as possible to the appearance of the real fur. In each optimization step, we render an image using an off-the-shelf fur renderer and measure image similarity using a pre-trained deep convolutional neural network model. We demonstrate that the proposed method can estimate fur parameters appropriately for a wide range of fur types.
C1 [Noh, Seung-Tak; Igarashi, Takeo] Univ Tokyo, Bunkyo Ku, Tokyo, Japan.
   [Takahashi, Kenichi; Adachi, Masahiko] Kabuku Inc, Shinjuku Ku, Tokyo, Japan.
C3 University of Tokyo
RP Noh, ST (corresponding author), Univ Tokyo, Bunkyo Ku, Tokyo, Japan.
EM seungtak.noh@gmail.com; kenichi.takahashi@kabuku.co.jp;
   masahiko.adachi@kabuku.co.jp; takeo@acm.org
RI Igarashi, Takeo/ITT-5921-2023
OI Noh, Seung-Tak/0000-0002-7823-0864
FU JSPS KAKENHI [JP17H00752, 19J13492]; JSPS Research Fellowships for Young
   Scientists; Grants-in-Aid for Scientific Research [19J13492] Funding
   Source: KAKEN
FX This work was partially supported by JSPS KAKENHI (Grant Number
   JP17H00752 and 19J13492). Seung-Tak Noh is funded by JSPS Research
   Fellowships for Young Scientists.
CR Andersen TG, 2016, VISUAL COMPUT, V32, P739, DOI 10.1007/s00371-016-1252-x
   Autodesk Inc, 1998, AUT MAYA
   Beeler T, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185613
   Chollet F, 2015, KERAS
   Eberhart R.C., 1995, Proc Int Symp Micro Mach Hum Sci, P39, DOI [DOI 10.1109/MHS.1995.494215, 10.1109/mhs.1995.494215]
   Gatys L., 2015, NIPS
   Gatys L., 2016, Journal of Vision, V16, P326, DOI DOI 10.1167/16.12.326
   Hansen N., 2016, The CMA evolution strategy: A tutorial
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Lengyel J., 2001, P 2001 S INTERACTIVE, P227, DOI [10.1145/364338.364407, DOI 10.1145/364338.364407]
   Loper MM, 2014, LECT NOTES COMPUT SC, V8695, P154, DOI 10.1007/978-3-319-10584-0_11
   Luo LJ, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2462026
   Michler F, 2019, IEEE RADIO WIRELESS, P161, DOI 10.1109/rws.2019.8714521
   Paris S, 2004, ACM T GRAPHIC, V23, P712, DOI 10.1145/1015706.1015784
   Paris S, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360629
   Radford A., 2015, ARXIV
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Shahriari B, 2016, P IEEE, V104, P148, DOI 10.1109/JPROC.2015.2494218
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Wei YC, 2005, ACM T GRAPHIC, V24, P816, DOI 10.1145/1073204.1073267
   Zhang ZY, 2000, IEEE T PATTERN ANAL, V22, P1330, DOI 10.1109/34.888718
   Zhao S, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964939
NR 22
TC 0
Z9 0
U1 0
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2021
VL 37
IS 5
BP 1129
EP 1138
DI 10.1007/s00371-020-01857-x
EA JUN 2020
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RV2QF
UT WOS:000538960600001
DA 2024-07-18
ER

PT J
AU Rahman, Z
   Pu, YF
   Aamir, M
   Wali, S
AF Rahman, Ziaur
   Pu, Yi-Fei
   Aamir, Muhammad
   Wali, Samad
TI Structure revealing of low-light images using wavelet transform based on
   fractional-order denoising and multiscale decomposition
SO VISUAL COMPUTER
LA English
DT Article
DE Image enhancement; DT-CWT; Fractional-order denoising; Multiscale
   decomposition; White balance
ID BI-HISTOGRAM EQUALIZATION; CONTRAST ENHANCEMENT; QUALITY ASSESSMENT;
   EDGE-DETECTION; ILLUMINATION; INFORMATION; DIFFUSION; RETINEX; ENTROPY;
   MODEL
AB Images captured in low-light environment often lower its quality due to low illumination and high noise. Hence, the low visibility of images notably degrades the overall performance of multimedia and vision systems that are typically designed for high-quality inputs. To resolve this problem, numerous algorithms have been proposed in extant literature to improve the visual quality of low-light images. However, existing approaches are not good at improving overexposed portions and produce unnecessary distortion, which leads to poor visibility in images. Therefore, in this paper, a new model is proposed to prevent overenhancement, handle uneven illumination, and suppress noise in underexposed images. Firstly, the input image is converted into HSV color space. Then, the obtained V component is decomposed into high- and low-frequency subbands using the dual-tree complex wavelet transform. Secondly, a denoised model based on fractional-order anisotropic diffusion is applied on high-pass subbands. Thirdly, multiscale decomposition is used to extract more details from low-pass subbands, and inverse transformation is performed to compute final V. Next, sigmoid function and tone mapping are used on V-channel to prevent data loss and achieve robust results. Finally, the image is reconstructed and converted to RGB color space to achieve enhanced performance. Comparative experimental statistics show that the proposed method achieves high efficacy and outperforms the traditional approaches in terms of overall performance.
C1 [Rahman, Ziaur; Pu, Yi-Fei; Aamir, Muhammad] Sichuan Univ, Coll Comp Sci & Technol, Chengdu 610065, Sichuan, Peoples R China.
   [Wali, Samad] Univ Elect Sci & Technol China, Sch Informat & Commun Engn, Chengdu 611731, Peoples R China.
C3 Sichuan University; University of Electronic Science & Technology of
   China
RP Pu, YF (corresponding author), Sichuan Univ, Coll Comp Sci & Technol, Chengdu 610065, Sichuan, Peoples R China.
EM ziaurrahman167@yahoo.com; puyifei_007@hotmail.com
RI Aamir, Muhammad/AAJ-4676-2021; Rahman, Ziaur/AEH-8842-2022
OI Aamir, Muhammad/0000-0002-4999-7740; Rahman, Ziaur/0000-0001-8233-567X;
   Wali, Samad/0000-0002-8633-4209
FU National Key Research and Development Program Foundation of China
   [2018YFC0830300]; National Natural Science Foundation of China
   [61571312]
FX The work was supported by the National Key Research and Development
   Program Foundation of China under Grants 2018YFC0830300 and the National
   Natural Science Foundation of China under Grants 61571312.
CR Aamir M, 2019, INT J ADV MANUF TECH, V105, P2289, DOI 10.1007/s00170-019-04348-z
   Aamir M, 2019, SYMMETRY-BASEL, V11, DOI 10.3390/sym11010003
   [Anonymous], 2006, THEORY APPL DIFFEREN
   Arici T, 2009, IEEE T IMAGE PROCESS, V18, P1921, DOI 10.1109/TIP.2009.2021548
   Atta R, 2015, OPTIK, V126, P799, DOI 10.1016/j.ijleo.2015.02.025
   BEGHDADI A, 1989, COMPUT VISION GRAPH, V46, P162, DOI 10.1016/0734-189X(89)90166-7
   BIDWAI P, 2016, P IEEE INT C INF PRO
   BYCHKOVSKY V, 2011, P IEEE COMP SOC C CO
   Cai BL, 2017, IEEE I CONF COMP VIS, P4020, DOI 10.1109/ICCV.2017.431
   CATTE F, 1992, SIAM J NUMER ANAL, V29, P182, DOI 10.1137/0729012
   Celik T, 2011, IEEE T IMAGE PROCESS, V20, P3431, DOI 10.1109/TIP.2011.2157513
   CHAKRABARTI A, 2012, EMPIRICAL CAMERA MOD
   Chen SD, 2003, IEEE T CONSUM ELECTR, V49, P1310, DOI 10.1109/TCE.2003.1261234
   Cheng DL, 2014, J OPT SOC AM A, V31, P1049, DOI 10.1364/JOSAA.31.001049
   Demirel H, 2010, IEEE GEOSCI REMOTE S, V7, P333, DOI 10.1109/LGRS.2009.2034873
   Dong XC, 2011, INT C PAR DISTRIB SY, P9, DOI 10.1109/ICPADS.2011.115
   Easley G, 2008, APPL COMPUT HARMON A, V25, P25, DOI 10.1016/j.acha.2007.09.003
   Easley GR, 2009, IEEE T IMAGE PROCESS, V18, P260, DOI 10.1109/TIP.2008.2008070
   Farhangi N, 2018, EURASIP J IMAGE VIDE, DOI 10.1186/s13640-018-0244-3
   Fu XY, 2016, PROC CVPR IEEE, P2782, DOI 10.1109/CVPR.2016.304
   Fu XY, 2016, SIGNAL PROCESS, V129, P82, DOI 10.1016/j.sigpro.2016.05.031
   Fu XY, 2015, IEEE T IMAGE PROCESS, V24, P4965, DOI 10.1109/TIP.2015.2474701
   Gonzalez L, 2008, J BIOMED OPT, V13, DOI 10.1117/1.2960621
   Gonzalez RC., 2011, DIGITAL IMAGE PROCES
   Gu K, 2016, IEEE T MULTIMEDIA, V18, P432, DOI 10.1109/TMM.2016.2518868
   Guo XJ, 2017, IEEE T IMAGE PROCESS, V26, P982, DOI 10.1109/TIP.2016.2639450
   Guo YH, 2019, IEEE ACCESS, V7, P13737, DOI 10.1109/ACCESS.2019.2891957
   Hautiere Nicolas, 2008, Image Analysis & Stereology, V27, P87, DOI 10.5566/ias.v27.p87-95
   He KM, 2013, IEEE T PATTERN ANAL, V35, P1397, DOI 10.1109/TPAMI.2012.213
   Huang H, 2010, VISUAL COMPUT, V26, P731, DOI 10.1007/s00371-010-0504-4
   Ibrahim H, 2007, IEEE T CONSUM ELECTR, V53, P1752, DOI 10.1109/TCE.2007.4429280
   Jafari S, 2016, J APPL REMOTE SENS, V10, DOI 10.1117/1.JRS.10.015002
   Jobson DJ, 1997, IEEE T IMAGE PROCESS, V6, P965, DOI 10.1109/83.597272
   JOSHI P, 2018, VISUAL COMPUT, V36, P1
   KARUMURI R, 2018, P 2 INT C COMM EL SY
   Kim K, 2018, IET IMAGE PROCESS, V12, P465, DOI 10.1049/iet-ipr.2016.0819
   Kim YT, 1997, IEEE T CONSUM ELECTR, V43, P1, DOI 10.1109/30.580378
   Li CY, 2018, PATTERN RECOGN LETT, V104, P15, DOI 10.1016/j.patrec.2018.01.010
   Lien CM, 2010, IEEE ICC
   Loza A, 2013, DIGIT SIGNAL PROCESS, V23, P1856, DOI 10.1016/j.dsp.2013.06.002
   Lv F., 2018, P BMVC, V220, P4
   Lynch SE, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P899, DOI 10.1109/ICCVW.2013.123
   Ma K, 2015, IEEE T IMAGE PROCESS, V24, P3345, DOI 10.1109/TIP.2015.2442920
   MALM H, 2007, P IEEE INT C COMP VI
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Moorthy AK, 2011, IEEE T IMAGE PROCESS, V20, P3350, DOI 10.1109/TIP.2011.2147325
   Pajak D, 2010, VISUAL COMPUT, V26, P739, DOI 10.1007/s00371-010-0485-3
   Park S, 2017, IEEE T CONSUM ELECTR, V63, P178, DOI 10.1109/TCE.2017.014847
   PELI E, 1990, J OPT SOC AM A, V7, P2032, DOI 10.1364/JOSAA.7.002032
   PERONA P, 1990, IEEE T PATTERN ANAL, V12, P629, DOI 10.1109/34.56205
   Petrol AB, 2014, IMAGE PROCESS ON LIN, V4, P71, DOI 10.5201/ipol.2014.107
   Qiao NS, 2013, OPTIK, V124, P1889, DOI 10.1016/j.ijleo.2012.05.034
   Sen P, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366222
   Shan Q, 2010, IEEE T VIS COMPUT GR, V16, P663, DOI 10.1109/TVCG.2009.92
   Shanmugavadivu P, 2014, VISUAL COMPUT, V30, P387, DOI 10.1007/s00371-013-0863-8
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P430, DOI 10.1109/TIP.2005.859378
   Toh RT, 2016, INT EL DEVICES MEET
   Tsai DY, 2008, J DIGIT IMAGING, V21, P338, DOI 10.1007/s10278-007-9044-5
   Wang C, 2005, IEEE T CONSUM ELECTR, V51, P1326, DOI 10.1109/TCE.2005.1561863
   Wang LQ, 2014, IEEE T IMAGE PROCESS, V23, P3381, DOI 10.1109/TIP.2014.2324813
   Wang RX, 2019, PROC CVPR IEEE, P6842, DOI 10.1109/CVPR.2019.00701
   Wang SH, 2013, IEEE T IMAGE PROCESS, V22, P3538, DOI 10.1109/TIP.2013.2261309
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wu XL, 2011, IEEE T IMAGE PROCESS, V20, P1262, DOI 10.1109/TIP.2010.2092438
   Yang CW, 2019, VISUAL COMPUT, V35, P695, DOI 10.1007/s00371-018-1504-z
   Yang MX, 2018, OPTOELECTRON LETT, V14, P470, DOI 10.1007/s11801-018-8046-5
   YIN W, 2011, P 2011 3 INT C AW SC, P2011
   Ying Z., 2017, ARXIV PREPRINT ARXIV
   Zhang YH, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1632, DOI 10.1145/3343031.3350926
   Zi XM, 2018, 2018 IEEE INTERNATIONAL MAGNETIC CONFERENCE (INTERMAG)
NR 70
TC 23
Z9 24
U1 0
U2 35
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2021
VL 37
IS 5
BP 865
EP 880
DI 10.1007/s00371-020-01838-0
EA APR 2020
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RV2QF
UT WOS:000523091400001
DA 2024-07-18
ER

PT J
AU Liu, Y
   Pan, JS
   Su, ZX
   Tang, KW
AF Liu, Yang
   Pan, Jinshan
   Su, Zhixun
   Tang, Kewei
TI Robust dense correspondence using deep convolutional features
SO VISUAL COMPUTER
LA English
DT Article
DE Deep convolutional neural network; Dense correspondence; Image matching;
   Optical flow; Handcrafted feature
ID FLOW; MOTION
AB Image matching is a challenging problem as different views often undergo significant appearance changes caused by illumination changes, scale variations, large displacement, and deformation. Most state-of-the-art algorithms, however, still cannot perform well enough in handling challenging real-world cases, especially in different objects and scenes. In this paper, we explore deep features extracted from pretrained convolutional neural networks to help the estimation of image matching so that dense pixel correspondence can be built. As the deep features are able to describe the image structures and details hierarchically, the matching method based on these features is able to match different scenes and object appearances effectively. We analyze the deep features and compare them with other robust features, e.g., SIFT. Extensive experiments on benchmark datasets demonstrate that the proposed algorithm performs favorably against the state-of-the-art methods in terms of visually matching quality and accuracy.
C1 [Liu, Yang; Su, Zhixun] Dalian Univ Technol, Sch Math Sci, Dalian, Peoples R China.
   [Pan, Jinshan] Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Nanjing, Peoples R China.
   [Tang, Kewei] Liaoning Normal Univ, Sch Math, Dalian, Peoples R China.
C3 Dalian University of Technology; Nanjing University of Science &
   Technology; Liaoning Normal University
RP Pan, JS (corresponding author), Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Nanjing, Peoples R China.
EM lewisyangliu@gmail.com; sdluran@gmail.com; zxsu@dlut.edu.cn;
   tkwliaoning@gmail.com
RI Pan, Jinshan/AAO-2258-2021
OI Pan, Jinshan/0000-0003-0304-9507; Tang, Kewei/0000-0003-4846-2231
FU National Natural Science Foundation of China [61872421, 61572099,
   61702243]; National Science andTechnology Major Project [ZX20140419,
   2014ZX04001011]; Natural Science Foundation of Jiangsu Province
   [BK20180471]
FX This work is partially supported by the National Natural Science
   Foundation of China (Nos. 61872421, 61572099, and 61702243), theNational
   Science andTechnology Major Project (Nos. ZX20140419, 2014ZX04001011),
   and the Natural Science Foundation of Jiangsu Province (No. BK20180471).
CR [Anonymous], 2014, NIPS
   [Anonymous], IEEE T PATTERN ANAL
   [Anonymous], 2015, ICCV
   [Anonymous], ICIP
   [Anonymous], CVPR
   [Anonymous], 2016, NIPS
   [Anonymous], 2014, EUR C COMP VIS
   [Anonymous], 2015, PROC CVPR IEEE
   [Anonymous], 2016, ARXIV PREPRINT ARXIV
   Bakewell S, 2011, NY TIMES BK REV, P1
   Barnes C, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531330
   Black MJ, 1996, COMPUT VIS IMAGE UND, V63, P75, DOI 10.1006/cviu.1996.0006
   Brox T, 2004, LECT NOTES COMPUT SC, V2034, P25, DOI 10.1007/978-3-540-24673-2_3
   Brox T, 2011, IEEE T PATTERN ANAL, V33, P500, DOI 10.1109/TPAMI.2010.143
   Brox T, 2009, PROC CVPR IEEE, P41, DOI 10.1109/CVPRW.2009.5206697
   Datta R, 2008, ACM COMPUT SURV, V40, DOI 10.1145/1348246.1348248
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Franke U, 2005, LECT NOTES COMPUT SC, V3663, P216
   Han K, 2017, IEEE I CONF COMP VIS, P1849, DOI 10.1109/ICCV.2017.203
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   HORN BKP, 1981, ARTIF INTELL, V17, P185, DOI 10.1016/0004-3702(81)90024-2
   Hur J, 2015, PROC CVPR IEEE, P1392, DOI 10.1109/CVPR.2015.7298745
   Ilg E, 2017, PROC CVPR IEEE, P1647, DOI 10.1109/CVPR.2017.179
   Kim J, 2013, PROC CVPR IEEE, P2307, DOI 10.1109/CVPR.2013.299
   Kim S, 2017, PROC CVPR IEEE, P616, DOI 10.1109/CVPR.2017.73
   Lester H, 1999, PATTERN RECOGN, V32, P129, DOI 10.1016/S0031-3203(98)00095-8
   Li C, 2016, PROC CVPR IEEE, P2479, DOI 10.1109/CVPR.2016.272
   Li FF, 2007, COMPUT VIS IMAGE UND, V106, P59, DOI 10.1016/j.cviu.2005.09.012
   Li WB, 2013, PROC CVPR IEEE, P2435, DOI 10.1109/CVPR.2013.315
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Ma C, 2015, IEEE I CONF COMP VIS, P3074, DOI 10.1109/ICCV.2015.352
   Nag Sayan, 2017, ARXIV171207540
   Revaud J, 2015, PROC CVPR IEEE, P1164, DOI 10.1109/CVPR.2015.7298720
   Russell BC, 2008, INT J COMPUT VISION, V77, P157, DOI 10.1007/s11263-007-0090-8
   Shen XY, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980249
   Simonyan K., 2015, P 3 INT C LEARN REPR
   Sun DQ, 2010, PROC CVPR IEEE, P2432, DOI 10.1109/CVPR.2010.5539939
   Tau M, 2016, IEEE T PATTERN ANAL, V38, P875, DOI 10.1109/TPAMI.2015.2474356
   VANDENELSEN PA, 1993, IEEE ENG MED BIOL, V12, P26, DOI 10.1109/51.195938
   Weinzaepfel P, 2013, IEEE I CONF COMP VIS, P1385, DOI 10.1109/ICCV.2013.175
   Xu L, 2012, IEEE T PATTERN ANAL, V34, P1744, DOI 10.1109/TPAMI.2011.236
   Yang HS, 2014, PROC CVPR IEEE, P3406, DOI 10.1109/CVPR.2014.435
   Zhou TH, 2016, PROC CVPR IEEE, P117, DOI 10.1109/CVPR.2016.20
NR 43
TC 4
Z9 4
U1 0
U2 9
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2020
VL 36
IS 4
BP 827
EP 841
DI 10.1007/s00371-019-01656-z
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KW0AL
UT WOS:000520835800013
DA 2024-07-18
ER

PT J
AU Liu, GH
   Zheng, XT
AF Liu, Guohua
   Zheng, Xiangtong
TI Fabric defect detection based on information entropy and frequency
   domain saliency
SO VISUAL COMPUTER
LA English
DT Article
DE Fabric defect detection; Information entropy; Visual attention;
   Hypercomplex Fourier transform
ID CLASSIFICATION; INSPECTION
AB The automatic detection of defects is an important part of the fabric production process. However, existing methods of detecting defects in fabrics with periodic patterns lack adaptability and perform poorly in detection. In this paper, we propose an unsupervised fabric defect detection method based on the human visual attention mechanism. The method introduces two-dimensional entropy which can reflect the spatial distribution characteristics of images based on one-dimensional entropy, according to the relationship between information entropy and image texture. The image is reconstructed into a quaternion matrix by combining two-dimensional entropy and three feature maps that characterize the opponent color space representation of the input image. The hypercomplex Fourier transform is then used to transform the quaternion image matrix into the frequency domain. We propose a new method for local tuning of amplitude spectrum, thereby suppressing the background pattern while retaining the defect region. Finally, the inverse transform is performed to obtain a saliency map. Through experimental comparisons and a series of numerical evaluations, we demonstrate that the proposed method has a better detection effect compared to state-of-the-art methods in fabric defect detection.
C1 [Liu, Guohua; Zheng, Xiangtong] Tiangong Univ, Sch Mech Engn, Tianjin 300387, Peoples R China.
   [Liu, Guohua] Tiangong Univ, Adv Mechatron Equipment Technol Tianjin Major Lab, Tianjin 300387, Peoples R China.
C3 Tiangong University; Tiangong University
RP Liu, GH (corresponding author), Tiangong Univ, Sch Mech Engn, Tianjin 300387, Peoples R China.; Liu, GH (corresponding author), Tiangong Univ, Adv Mechatron Equipment Technol Tianjin Major Lab, Tianjin 300387, Peoples R China.
EM guohualiumail@163.com
CR Behravan M., 2011, IRANIAN J ELECT COMP, V10, P57
   Bissi L, 2013, J VIS COMMUN IMAGE R, V24, P838, DOI 10.1016/j.jvcir.2013.05.011
   Cao JJ, 2017, MULTIMED TOOLS APPL, V76, P4141, DOI 10.1007/s11042-015-3041-3
   Cao JJ, 2016, INT J CLOTH SCI TECH, V28, P516, DOI 10.1108/IJCST-10-2015-0117
   Chen DY, 2016, SIGNAL PROCESS-IMAGE, V44, P57, DOI 10.1016/j.image.2016.03.003
   Deng HW, 2004, IEEE T PATTERN ANAL, V26, P951, DOI 10.1109/TPAMI.2004.30
   Doyle L, 2019, VISUAL COMPUT, V35, P1489, DOI 10.1007/s00371-018-1513-y
   Ell TA, 2007, IEEE T IMAGE PROCESS, V16, P22, DOI 10.1109/TIP.2006.884955
   Gui Y, 2010, VISUAL COMPUT, V26, P951, DOI 10.1007/s00371-010-0470-x
   Guo CL, 2008, PROC CVPR IEEE, P2908
   Hanbay K, 2016, OPTIK, V127, P11960, DOI 10.1016/j.ijleo.2016.09.110
   Harel J, 2006, Advances in Neural Information Processing Systems, V19
   HOU T, 2015, DIFF WAV DES SHAP, P1
   Huang YB, 2018, IEEE INT CON AUTO SC, P612, DOI 10.1109/COASE.2018.8560423
   Huangpeng QZ, 2018, IEEE ACCESS, V6, P37965, DOI 10.1109/ACCESS.2018.2852663
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Jung C, 2012, IEEE T IMAGE PROCESS, V21, P1272, DOI 10.1109/TIP.2011.2164420
   Li CL, 2018, IEEE ACCESS, V6, P27659, DOI 10.1109/ACCESS.2018.2841055
   Li J, 2015, IEEE T PATTERN ANAL, V37, P2428, DOI 10.1109/TPAMI.2015.2424870
   Li J, 2013, IEEE T PATTERN ANAL, V35, P996, DOI 10.1109/TPAMI.2012.147
   Li YD, 2017, IEEE T AUTOM SCI ENG, V14, P1256, DOI 10.1109/TASE.2016.2520955
   Ma XL, 2015, J VIS COMMUN IMAGE R, V32, P95, DOI 10.1016/j.jvcir.2015.08.003
   Makaremi M, 2018, SIGNAL IMAGE VIDEO P, V12, P1395, DOI 10.1007/s11760-018-1294-9
   Mei S, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18041064
   Ng MK, 2014, IEEE T AUTOM SCI ENG, V11, P943, DOI 10.1109/TASE.2014.2314240
   Ngan HYT, 2011, IMAGE VISION COMPUT, V29, P442, DOI 10.1016/j.imavis.2011.02.002
   OTSU N, 1979, IEEE T SYST MAN CYB, V9, P62, DOI 10.1109/TSMC.1979.4310076
   Park Y, 2016, IEEE T IND INFORM, V12, P597, DOI 10.1109/TII.2016.2522191
   Ren RX, 2018, IEEE T CYBERNETICS, V48, P929, DOI 10.1109/TCYB.2017.2668395
   Seker A, 2016, 2016 24TH SIGNAL PROCESSING AND COMMUNICATION APPLICATION CONFERENCE (SIU), P1437, DOI 10.1109/SIU.2016.7496020
   Shannon C. E., 1948, BELL SYST TECH J, V27, P379, DOI DOI 10.1002/J.1538-7305.1948.TB01338.X
   Tsang CSC, 2016, PATTERN RECOGN, V51, P378, DOI 10.1016/j.patcog.2015.09.022
   Yapi D, 2018, IEEE T AUTOM SCI ENG, V15, P1014, DOI 10.1109/TASE.2017.2696748
   Yu Y, 2009, 2009 IEEE 8TH INTERNATIONAL CONFERENCE ON DEVELOPMENT AND LEARNING, P41
   Zhang KB, 2018, IEEE ACCESS, V6, P49170, DOI 10.1109/ACCESS.2018.2868059
   Zhang Y, 2010, PATTERN RECOGN LETT, V31, P2033, DOI 10.1016/j.patrec.2010.05.030
NR 36
TC 31
Z9 33
U1 5
U2 45
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2021
VL 37
IS 3
BP 515
EP 528
DI 10.1007/s00371-020-01820-w
EA FEB 2020
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA QY5QT
UT WOS:000516099300001
DA 2024-07-18
ER

PT J
AU Couillaud, J
   Ziou, D
AF Couillaud, Julien
   Ziou, Djemel
TI Light field variational estimation using a light field formation model
SO VISUAL COMPUTER
LA English
DT Article
DE Computational photography; Light field; Image formation; Variational
   estimation; Light ray irradiance
AB Many light field acquisition methods have been developed thanks to special optical devices, such as microlens array and coded aperture. However, these methods produce light fields with resolutions limited by the employed optical device. In this paper, we propose an alternative method to acquire light fields with resolutions that users can select. This method relies on the variational estimation of a light field from a single image and a depth map taken with a digital still camera. The light field is obtained by solving an inverse problem built from a light field formation model, based on optical geometry and light ray radiometry. The effectiveness of this estimation method is demonstrated on synthetic and real data. The experimental results show that it is possible to estimate a light field without using special optical devices. In addition, realistic images can be recreated by the exploitation of estimated light fields in image formation.
C1 [Couillaud, Julien; Ziou, Djemel] Univ Sherbrooke, MOIVRE, 2500 Blvd Univ Sherbrooke, Sherbrooke, PQ J1K 2R1, Canada.
C3 University of Sherbrooke
RP Couillaud, J (corresponding author), Univ Sherbrooke, MOIVRE, 2500 Blvd Univ Sherbrooke, Sherbrooke, PQ J1K 2R1, Canada.
EM julien.couillaud@usherbrooke.ca
CR Adelson E.H., 1991, Computational Models of Visual Processing, P3
   An J., 2018, VISUAL COMPUT
   [Anonymous], 2014, IEEE INT CONF COMPUT
   Bishop TE, 2012, IEEE T PATTERN ANAL, V34, P972, DOI 10.1109/TPAMI.2011.168
   Dansereau D, 2004, 2004 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS, VOL 3, PROCEEDINGS, P549
   Flynn B, 2013, CRIT EXPLOR CONT POL, P23
   Georgiev Todor G., 2009, Signal Recovery and Synthesis, page, pSTuA6
   Gortler S. J., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P43, DOI 10.1145/237170.237200
   Hanika J, 2014, COMPUT GRAPH FORUM, V33, P323, DOI 10.1111/cgf.12301
   Honauer K, 2017, LECT NOTES COMPUT SC, V10113, P19, DOI 10.1007/978-3-319-54187-7_2
   Hore Alain, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P2366, DOI 10.1109/ICPR.2010.579
   Horn B.K.P, 1986, Robot Vision
   Horstmeyer R., 2009, Computational Photography (ICCP), 2009 IEEE International Conference on, P1, DOI DOI 10.1109/ICCPHOT.2009.5559016
   Hung M., 2013, SIGN INF PROC ASS AN, P1, DOI DOI 10.2139/SSRN.2206877
   Kingslake R, 2010, LENS DESIGN FUNDAMENTALS, 2ND EDITION, P1, DOI 10.1016/B978-0-12-374301-5.00005-X
   Klein V, 2010, PROCEEDINGS OF THE XVIII INTERNATIONAL CONFERENCE ON COMPUTATIONAL METHODS IN WATER RESOURCES (CMWR 2010), P154
   Kubota A, 2007, IEEE T IMAGE PROCESS, V16, P269, DOI 10.1109/TIP.2006.884938
   Levin A, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531403
   Levin J, 2008, TORT WARS, P88
   Liang CK, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2665075
   Liang CK, 2011, IEEE T IMAGE PROCESS, V20, P446, DOI 10.1109/TIP.2010.2063036
   Lippmann G., 1908, J. Phys. Theor. Appl, V7, P821, DOI [DOI 10.1051/JPHYSTAP:019080070082100, 10.1051/jphystap:019080070082100]
   Marwah K, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461914
   Mokrzycki W. S., 2011, Machine Graphics & Vision, V20, P383
   Ng R, 2005, ACM T GRAPHIC, V24, P735, DOI 10.1145/1073204.1073256
   Ng R., 2005, 2 COMP SCI TECHN
   Ray S., 2002, Applied Photographic Optics
   Smisek J., 2011, 2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops), P1154, DOI 10.1109/ICCVW.2011.6130380
   Smith W. J., 1966, Modern Optical Engineering; the Design of Optical Systems
   Steinert B, 2011, COMPUT GRAPH FORUM, V30, P1643, DOI 10.1111/j.1467-8659.2011.01851.x
   Veeraraghavan A, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239520
   Venkataraman K, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508390
   Wang X, 2008, CISP 2008: FIRST INTERNATIONAL CONGRESS ON IMAGE AND SIGNAL PROCESSING, VOL 1, PROCEEDINGS, P467, DOI 10.1109/CISP.2008.371
   Wang Y., 2016, IEEE T CYBERNETICS, V99, P1
   Wanner S, 2014, IEEE T PATTERN ANAL, V36, P606, DOI 10.1109/TPAMI.2013.147
   Wilburn B, 2005, ACM T GRAPHIC, V24, P765, DOI 10.1145/1073204.1073259
   Wu GC, 2017, IEEE J-STSP, V11, P926, DOI 10.1109/JSTSP.2017.2747126
   Yang RG, 2002, 10TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P225, DOI 10.1109/PCCGA.2002.1167864
   Zhuo SJ, 2011, PATTERN RECOGN, V44, P1852, DOI 10.1016/j.patcog.2011.03.009
   Ziou D, 2001, COMPUT VIS IMAGE UND, V81, P143, DOI 10.1006/cviu.2000.0899
NR 40
TC 5
Z9 5
U1 0
U2 12
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2020
VL 36
IS 2
BP 237
EP 251
DI 10.1007/s00371-018-1599-2
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KJ2TH
UT WOS:000511910300002
DA 2024-07-18
ER

PT J
AU Gaisbauer, F
   Lampen, E
   Agethen, P
   Rukzio, E
AF Gaisbauer, Felix
   Lampen, Eva
   Agethen, Philipp
   Rukzio, Enrico
TI Combining heterogeneous digital human simulations: presenting a novel
   co-simulation approach for incorporating different character animation
   technologies
SO VISUAL COMPUTER
LA English
DT Article
DE Modular character animation; Motion synthesis; Co-simulation; Motion
   model unit; Motion model interface; MMI; MMU; Digital human simulation
ID CONTROLLERS
AB Digital human simulation is important for various domains such as the entertainment, health care and production industries. A variety of simulation techniques and tools are available, ranging from motion-capture-based animation systems and deep learning to physics-based motion synthesis. Each technology has its advantages and disadvantages and is suited for particular use cases. Therefore, a combination of multiple technologies would result in more sophisticated simulations, which can address heterogeneous aspects. However, the different approaches are mostly tightly coupled with the development environment, thus inducing high porting efforts if being incorporated into different platforms. A combination of separately developed simulation systems either for benchmarking or comprehensive simulation is not possible yet. For the domain of plant simulation, the Functional Mock-up Interface (FMI) standard has already solved this problem. Initially being tailored to industrial needs, the standards allow exchanging dynamic simulation approaches such as solvers for mechatronic components. Inspired by the FMI standard, we present a novel framework to incorporate multiple digital human simulation approaches from multiple domains. In particular, the paper introduces the overall concept of the so-called Motion Model Units, as well as its underlying technical architecture. As main contribution, a novel co-simulation for the orchestration of multiple digital human simulation approaches is presented. The overall applicability is approved based on a quantitative evaluation using motion capture data and a user study.
C1 [Gaisbauer, Felix; Lampen, Eva; Agethen, Philipp] Daimler AG, Wilhelm Runge Str 11, D-89081 Ulm, Germany.
   [Rukzio, Enrico] Univ Ulm, D-89081 Ulm, Germany.
C3 Daimler AG; Ulm University
RP Gaisbauer, F (corresponding author), Daimler AG, Wilhelm Runge Str 11, D-89081 Ulm, Germany.
EM felix.gaisbauer@daimler.com
OI Binder geb. Lampen, Eva/0000-0002-1299-5821
FU Federal Ministry of Education and Research of Germany within the MOSIM
   Project [01IS18060A-H]
FX The authors acknowledge the financial support by the Federal Ministry of
   Education and Research of Germany within the MOSIM Project [19] (Grant
   No. 01IS18060A-H).
CR Aristidou A, 2011, GRAPH MODELS, V73, P243, DOI 10.1016/j.gmod.2011.05.003
   Bastian Jens., 2011, Em, P115, DOI DOI 10.3384/ECP11063115
   Blockwitz T., 2012, 9 INT MOD C, P173, DOI DOI 10.3384/ECP12076173
   Buss S. R., 2004, IEEE J ROBOTIC AUTOM, V17, P16
   Cerekovic A, 2009, LECT NOTES ARTIF INT, V5773, P486, DOI 10.1007/978-3-642-04380-2_55
   Clavet S., 2016, Motion matching and the road to nextgen animation. In Proc of GDC 2016
   CMU, 2019, CMU GRAPHICS LAB MOT
   Delp SL, 2007, IEEE T BIO-MED ENG, V54, P1940, DOI 10.1109/TBME.2007.901024
   Faloutsos P, 2001, COMP GRAPH, P251, DOI 10.1145/383259.383287
   Feng Andrew, 2012, Motion in Games. 5th International Conference (MIG 2012). Proceedings, P232, DOI 10.1007/978-3-642-34710-8_22
   Feng AndrewW., 2012, I3D, P95, DOI [DOI 10.1145/2159616.2159632, 10.1145/2159616.2159632]
   Gaisbauer F., 2018, EG 2018 POSTERS
   Gaisbauer F, 2019, MOSIM RES PROJECT DE
   Gaisbauer F, 2019, PROCEEDINGS OF THE 14TH INTERNATIONAL JOINT CONFERENCE ON COMPUTER VISION, IMAGING AND COMPUTER GRAPHICS THEORY AND APPLICATIONS (GRAPP), VOL 1, P65, DOI 10.5220/0007356400650076
   Gaisbauer F, 2018, PROC CIRP, V72, P768, DOI 10.1016/j.procir.2018.03.281
   Hanson Lars., 2014, 3 INT SUMM HUM SIM I
   Holden D, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073663
   ITEA, 2018, 17028 ITEA MOSIM
   Kallmann M, 2005, LECT NOTES ARTIF INT, V3661, P253
   Kovar Lucas., 2008, ACM SIGGRAPH 2008 classes, page, P51
   Lee Y, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866160
   Li Zimo, 2017, ARXIV170705363
   Min JY, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366172
   Muller B, 2018, HDB HUMAN MOTION
   Reed M.P., 2006, 2006 DIG HUM MOD DES, DOI [https://doi.org/10.4271/2006-01-2365, DOI 10.4271/2006-01-2365]
   Shapiro A., 2011, LECT NOTES COMPUTER, P98, DOI DOI 10.1007/978-3-642-25090-3_9
   Shoulson A, 2014, IEEE T VIS COMPUT GR, V20, P1035, DOI 10.1109/TVCG.2013.251
   Thiebaux Marcus., 2008, P AAMAS 08, P151
   Tsai YY, 2010, IEEE T VIS COMPUT GR, V16, P325, DOI 10.1109/TVCG.2009.76
   Van Acker B., 2015, P S THEOR MOD SIM DE, P205
   Wang BB, 2013, IEEE ACM DIS SIM, P33, DOI 10.1109/DS-RT.2013.12
   Welbergen H., 2010, BML REALIZER CONTINU
NR 32
TC 6
Z9 6
U1 1
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2021
VL 37
IS 4
SI SI
BP 717
EP 734
DI 10.1007/s00371-020-01792-x
EA JAN 2020
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RL0LI
UT WOS:000508690200002
DA 2024-07-18
ER

PT J
AU Prieto, SA
   Adan, A
   Quintana, B
AF Prieto, Samuel A.
   Adan, Antonio
   Quintana, Blanca
TI Preparation and enhancement of 3D laser scanner data for realistic
   coloured BIM models
SO VISUAL COMPUTER
LA English
DT Article
DE 3D scanning; Reflection analysis; Colour merging; Specularity
ID IMAGE REGISTRATION METHODS; RECONSTRUCTION; FUSION; RANGE
AB Realistic point cloud models are frequently required in order to create efficient 3D data processing algorithms. In a building information modelling context, for example, the segmentation and object recognition of point clouds become extremely difficult tasks when the inputs of the algorithms are not sufficiently good. This paper proposes a method with which to efficiently solve two of the most important issues during the creation of realistic 3D data with laser scanners: the treatment of specularity in coloured point clouds and the view merging process. We particularly deal with wide-range laser scanners with the objective of generating realistic orthoimages of the main structural elements of the insides of buildings (i.e. walls, ceilings and floors). The new algorithm gathers the coloured points belonging to structural elements and delimits the specular regions originating from non-controlled or directional lighting sources (e.g. flashes). The restoration of these specular regions is solved in a subsequent stage in which several partial views of a structural element are integrated into a unique orthoimage. Finally, the quality of the resulting orthoimage is evaluated by comparing it with the corresponding ground truth image. This method has been tested on a public database, yielding encouraging results.
C1 [Prieto, Samuel A.; Adan, Antonio; Quintana, Blanca] Univ Castilla La Mancha, 3D Visual Comp & Robot Lab, Paseo Univ 4, E-13071 Ciudad Real, Spain.
C3 Universidad de Castilla-La Mancha
RP Prieto, SA (corresponding author), Univ Castilla La Mancha, 3D Visual Comp & Robot Lab, Paseo Univ 4, E-13071 Ciudad Real, Spain.
EM Samuel.Prieto@uclm.es; Antonio.Adan@uclm.es; Blanca.Quintana@uclm.es
RI Quintana, Blanca/AAC-1949-2022; Adán, Antonio/A-1153-2012
OI Quintana, Blanca/0000-0001-6462-4531; Adán, Antonio/0000-0002-0370-9651;
   Prieto Ayllon, Samuel A./0000-0001-8341-2630
CR Adán A, 2005, 2ND CANADIAN CONFERENCE ON COMPUTER AND ROBOT VISION, PROCEEDINGS, P168, DOI 10.1109/CRV.2005.1
   [Anonymous], J COMPUTING CULTURAL
   [Anonymous], VIS COMPUT
   [Anonymous], REMOVING SHADE SPECU
   Bannai N, 2007, PATTERN RECOGN LETT, V28, P748, DOI 10.1016/j.patrec.2006.07.013
   BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791
   Bueno G, 2004, LECT NOTES COMPUT SC, V3212, P33
   Callieri M, 2008, COMPUT GRAPH-UK, V32, P464, DOI 10.1016/j.cag.2008.05.004
   Curless B., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P303, DOI 10.1145/237170.237269
   Debevec P, 2000, COMP GRAPH, P145, DOI 10.1145/344779.344855
   Debevec P., 2004, USCICT Technical Report ICT-TR-06, P1
   Di Angelo L, 2011, COMPUT AIDED DESIGN, V43, P639, DOI 10.1016/j.cad.2011.02.012
   Drauschke M, 2015, ISPRS ANN PHOTO REM, V2-3, P33, DOI 10.5194/isprsannals-II-3-W4-33-2015
   Goldluecke B, 2009, IEEE I CONF COMP VIS, P1677, DOI 10.1109/ICCV.2009.5459378
   Lensch HPA, 2003, ACM T GRAPHIC, V22, P234, DOI 10.1145/636886.636891
   Liu XM, 2012, APPL OPTICS, V51, P1304, DOI 10.1364/AO.51.001304
   Merchán P, 2012, SENSORS-BASEL, V12, P6893, DOI 10.3390/s120606893
   Osadchy M, 2008, COMPUT VIS IMAGE UND, V111, P275, DOI 10.1016/j.cviu.2007.12.004
   Roth S, 2005, PROC CVPR IEEE, P860
   Salvi J, 2007, IMAGE VISION COMPUT, V25, P578, DOI 10.1016/j.imavis.2006.05.012
   Santamaría J, 2011, COMPUT VIS IMAGE UND, V115, P1340, DOI 10.1016/j.cviu.2011.05.006
   Sun YY, 2000, PROC SPIE, V4051, P110, DOI 10.1117/12.381624
   Tan RT, 2005, IEEE T PATTERN ANAL, V27, P178, DOI 10.1109/TPAMI.2005.36
   Troccoli A, 2008, INT J COMPUT VISION, V78, P261, DOI 10.1007/s11263-007-0100-x
   Troccoli A, 2007, THIRD INTERNATIONAL SYMPOSIUM ON 3D DATA PROCESSING, VISUALIZATION, AND TRANSMISSION, PROCEEDINGS, P655
   Turk G., 1994, Computer Graphics Proceedings. Annual Conference Series 1994. SIGGRAPH 94 Conference Proceedings, P311, DOI 10.1145/192161.192241
   Wang LF, 2001, PROC CVPR IEEE, P347
   Wyngaerd JV, 2003, FOURTH INTERNATIONAL CONFERENCE ON 3-D DIGITAL IMAGING AND MODELING, PROCEEDINGS, P179, DOI 10.1109/IM.2003.1240248
   Xiao G, 2005, COMPUT VIS IMAGE UND, V98, P271, DOI 10.1016/j.cviu.2004.10.001
   ZHANG ZY, 1994, INT J COMPUT VISION, V13, P119, DOI 10.1007/BF01427149
NR 30
TC 5
Z9 5
U1 1
U2 16
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2020
VL 36
IS 1
BP 113
EP 126
DI 10.1007/s00371-018-1584-9
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KJ3OJ
UT WOS:000511966800010
DA 2024-07-18
ER

PT J
AU Liu, J
   Luo, ZQ
   Xiong, XZ
AF Liu, Jun
   Luo, Zhongqiang
   Xiong, Xingzhong
TI An improved correlation filter tracking method with occlusion and drift
   handling
SO VISUAL COMPUTER
LA English
DT Article
DE Visual tracking; Circulant matrices; Correlation filter; Kernel methods;
   Occlusion
ID OBJECT TRACKING; VISUAL TRACKING
AB Despite remarkable progress, visual object tracking is still a challenging task as objects usually suffer from significant appearance changes, fast motion, and serious occlusion. In this paper, we propose a correlation filter-based tracking method with reliability evaluation and re-detection mechanism (CF-RERM) to deal with drift and occlusion problems. We first propose a criterion that uses the fluctuation trend of the response values, the displacement difference of the object, and the peak-to-sidelobe ratio to comprehensively evaluate the reliability of the tracking process. Then, a re-detection mechanism with a two-stage screening strategy is proposed for implementing the re-detection task when the criterion is triggered. Experimental results show that our method has achieved considerable performance in terms of accuracy and success rate on widely used OTB-50, OTB-100 and Temple-Color-128 tracking benchmark dataset. In addition, CF-RERM is able to achieve real-time tracking speed.
C1 [Liu, Jun; Luo, Zhongqiang; Xiong, Xingzhong] Sichuan Univ Sci & Engn, Artificial Intelligence Key Lab Sichuan Prov, Yibin 644000, Peoples R China.
   [Liu, Jun; Luo, Zhongqiang; Xiong, Xingzhong] Sichuan Univ Sci & Engn, Sch Automat & Informat Engn, Yibin 644000, Peoples R China.
   [Luo, Zhongqiang] Sichuan Univ Sci & Engn, Key Lab Higher Educ Sichuan Prov Enterprise Infor, Yibin 644000, Peoples R China.
C3 Sichuan University of Science & Engineering; Sichuan University of
   Science & Engineering; Sichuan University of Science & Engineering
RP Luo, ZQ (corresponding author), Sichuan Univ Sci & Engn, Artificial Intelligence Key Lab Sichuan Prov, Yibin 644000, Peoples R China.; Luo, ZQ (corresponding author), Sichuan Univ Sci & Engn, Sch Automat & Informat Engn, Yibin 644000, Peoples R China.; Luo, ZQ (corresponding author), Sichuan Univ Sci & Engn, Key Lab Higher Educ Sichuan Prov Enterprise Infor, Yibin 644000, Peoples R China.
EM liujun100911@163.com; zhongqiangluo@gmail.com; xzxiong@suse.edu.cn
RI Luo, Zhongqiang/Y-4348-2019; Liu, Jun/AAS-9772-2020
OI Luo, Zhongqiang/0000-0003-1767-1831; 
FU National Natural Science Foundation of China [61801319]; Sichuan
   University of Science and Engineering Talent Introduction Project
   [2017RCL11]; Opening Project of Key Laboratory of Higher Education of
   Sichuan Province for Enterprise Informationalization and Internet of
   Things [2017WZJ01]; Major Frontier Project of Science and Technology
   Plan of Sichuan Province [2018JY0512]; Education Agency Project of
   Sichuan Province [18ZB0419]; Sichuan Institute of Technology Graduate
   Innovation Foundation [D10-501128]
FX This work is supported by National Natural Science Foundation of China
   (No. 61801319), Sichuan University of Science and Engineering Talent
   Introduction Project (No. 2017RCL11), the Opening Project of Key
   Laboratory of Higher Education of Sichuan Province for Enterprise
   Informationalization and Internet of Things (No. 2017WZJ01), the Major
   Frontier Project of Science and Technology Plan of Sichuan Province (No.
   2018JY0512), the Education Agency Project of Sichuan Province
   (No.18ZB0419), and the Sichuan Institute of Technology Graduate
   Innovation Foundation (No. D10-501128).
CR [Anonymous], 2006, IEEE C COMPUTER VISI, DOI DOI 10.1109/CVPR.2006.256
   [Anonymous], 2012, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2012.6247881
   Avidan S, 2001, PROC CVPR IEEE, P184
   Babenko B, 2011, IEEE T PATTERN ANAL, V33, P1619, DOI 10.1109/TPAMI.2010.226
   Babenko B, 2009, PROC CVPR IEEE, P983, DOI 10.1109/CVPRW.2009.5206737
   Bai BD, 2015, APPL OPTICS, V54, P2897, DOI 10.1364/AO.54.002897
   Bertinetto L, 2016, PROC CVPR IEEE, P1401, DOI 10.1109/CVPR.2016.156
   Bhat G, 2018, LECT NOTES COMPUT SC, V11206, P493, DOI 10.1007/978-3-030-01216-8_30
   Bolme DS, 2010, PROC CVPR IEEE, P2544, DOI 10.1109/CVPR.2010.5539960
   Chen DP, 2013, IEEE I CONF COMP VIS, P1113, DOI 10.1109/ICCV.2013.142
   Choi J, 2017, PROC CVPR IEEE, P4828, DOI 10.1109/CVPR.2017.513
   Comaniciu D, 2003, IEEE T PATTERN ANAL, V25, P564, DOI 10.1109/TPAMI.2003.1195991
   Dai KN, 2019, PROC CVPR IEEE, P4665, DOI 10.1109/CVPR.2019.00480
   DANELLJAN M, 2014, P BRIT MACH VIS C, P98
   Dollár P, 2015, IEEE T PATTERN ANAL, V37, P1558, DOI 10.1109/TPAMI.2014.2377715
   Dong XP, 2017, IEEE T MULTIMEDIA, V19, P763, DOI 10.1109/TMM.2016.2631884
   GRAY RM, 2006, NOW PUBLISHERS, V77, P125
   Hare S, 2016, IEEE T PATTERN ANAL, V38, P2096, DOI 10.1109/TPAMI.2015.2509974
   Henriques JF, 2015, IEEE T PATTERN ANAL, V37, P583, DOI 10.1109/TPAMI.2014.2345390
   Henriques JF, 2012, LECT NOTES COMPUT SC, V7575, P702, DOI 10.1007/978-3-642-33765-9_50
   HESTER CF, 1980, APPL OPTICS, V19, P1758, DOI 10.1364/AO.19.001758
   Holzer S, 2013, IEEE T PATTERN ANAL, V35, P105, DOI 10.1109/TPAMI.2012.86
   Huang Z., 2017, P IEEE C COMP VIS PA, P4021, DOI DOI 10.1109/CVPR.2017.510
   Jia X, 2012, PROC CVPR IEEE, P1822, DOI 10.1109/CVPR.2012.6247880
   Kalal Z, 2012, IEEE T PATTERN ANAL, V34, P1409, DOI 10.1109/TPAMI.2011.239
   Kalal Z, 2010, PROC CVPR IEEE, P49, DOI 10.1109/CVPR.2010.5540231
   Kristan M, 2016, IEEE T PATTERN ANAL, V38, P2137, DOI 10.1109/TPAMI.2016.2516982
   Kwon J, 2010, PROC CVPR IEEE, P1269, DOI 10.1109/CVPR.2010.5539821
   Li Y, 2015, LECT NOTES COMPUT SC, V8926, P254, DOI 10.1007/978-3-319-16181-5_18
   Liang PP, 2015, IEEE T IMAGE PROCESS, V24, P5630, DOI 10.1109/TIP.2015.2482905
   Liu W, 2016, APPL OPTICS, V55, P6960, DOI 10.1364/AO.55.006960
   Lukezic A, 2019, LECT NOTES COMPUT SC, V11362, P595, DOI 10.1007/978-3-030-20890-5_38
   Ma C, 2015, PROC CVPR IEEE, P5388, DOI 10.1109/CVPR.2015.7299177
   Mei X, 2011, IEEE T PATTERN ANAL, V33, P2259, DOI 10.1109/TPAMI.2011.66
   Mei X, 2009, IEEE I CONF COMP VIS, P1436, DOI 10.1109/ICCV.2009.5459292
   Mueller M, 2016, LECT NOTES COMPUT SC, V9905, P445, DOI 10.1007/978-3-319-46448-0_27
   Nebehay G, 2015, PROC CVPR IEEE, P2784, DOI 10.1109/CVPR.2015.7298895
   Ning JF, 2016, PROC CVPR IEEE, P4266, DOI 10.1109/CVPR.2016.462
   Ross DA, 2008, INT J COMPUT VISION, V77, P125, DOI 10.1007/s11263-007-0075-7
   Sun C, 2018, PROC CVPR IEEE, P489, DOI 10.1109/CVPR.2018.00058
   Wu Y, 2015, IEEE T PATTERN ANAL, V37, P1834, DOI 10.1109/TPAMI.2014.2388226
   Wu Y, 2013, PROC CVPR IEEE, P2411, DOI 10.1109/CVPR.2013.312
   Zhang BC, 2017, IEEE T SYST MAN CY-S, V47, P693, DOI 10.1109/TSMC.2016.2629509
   Zhang JM, 2014, LECT NOTES COMPUT SC, V8694, P188, DOI 10.1007/978-3-319-10599-4_13
   Zhang KH, 2014, IEEE T PATTERN ANAL, V36, P2002, DOI 10.1109/TPAMI.2014.2315808
   Zhang KH, 2012, LECT NOTES COMPUT SC, V7574, P864, DOI 10.1007/978-3-642-33712-3_62
   ZHANG Y, 2018, P IEEE COMP VIS PATT
   Zhao JH, 2019, CHIN OPT LETT, V17, DOI 10.3788/COL201917.031001
   Zhong W, 2014, IEEE T IMAGE PROCESS, V23, P2356, DOI 10.1109/TIP.2014.2313227
   Zhu Z., 2018, 2018 IEEE International Magnetics Conference (INTERMAG), DOI 10.1109/INTMAG.2018.8508710
   Zitnick CL, 2014, LECT NOTES COMPUT SC, V8693, P391, DOI 10.1007/978-3-319-10602-1_26
NR 51
TC 5
Z9 5
U1 1
U2 12
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2020
VL 36
IS 9
BP 1909
EP 1926
DI 10.1007/s00371-019-01776-6
EA NOV 2019
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA NA2KY
UT WOS:000499416600001
DA 2024-07-18
ER

PT J
AU Gao, Q
   Shen, C
   Zhang, K
AF Gao, Qian
   Shen, Chong
   Zhang, Kun
TI The novel part-based cascaded regression algorithm research combining
   with pose estimation
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 36th Computer Graphics International Conference (CGI)
CY JUN 17-20, 2019
CL Calgary, CANADA
SP Biometric Technologies Lab, Univ Calgary, VPR Off, Fac Sci, Comp Sci Dept, Alberta Ingenu, CGS
DE Pose estimation; Feature points localization; Sampling a priori; Shape
   constraint; Kernel regression
ID FACE ALIGNMENT
AB Facial feature location is the key procedure in the process of face image analysis. To increase the localization accuracy and success rate of complex facial images due to lack of shape constraint in robust cascaded pose regression algorithm, the paper had proposed the novel part-based cascaded regression algorithm combining pose estimation. The facial landmarks are divided into several areas and pose estimation has used as shape constraint. To enhance the algorithm's performance, a priori knowledge has been exerted during sampling shape-indexed features to constrain the distance between pixel's pair, and the number of regression has reduced. The experimental results of proposed algorithm and others have demonstrated that the proposed algorithm has outperformed in localization accuracy and achieved better robustness for occlusion and complex facial images, the better localization success rate has been obtained, and the proposed method can realize real-time processing standard.
C1 [Gao, Qian; Shen, Chong; Zhang, Kun] Hainan Univ, State Key Lab Marine Resources Utilizat South Chi, Haikou 570228, Hainan, Peoples R China.
   [Gao, Qian; Shen, Chong; Zhang, Kun] Hainan Univ, Coll Informat Sci & Technol, Haikou 570228, Hainan, Peoples R China.
   [Zhang, Kun] Hainan Trop Ocean Univ, Coll Ocean Informat Engn, Sanya 572022, Hainan, Peoples R China.
C3 Hainan University; Hainan University; Hainan Tropical Ocean University
RP Shen, C (corresponding author), Hainan Univ, State Key Lab Marine Resources Utilizat South Chi, Haikou 570228, Hainan, Peoples R China.; Shen, C (corresponding author), Hainan Univ, Coll Informat Sci & Technol, Haikou 570228, Hainan, Peoples R China.
EM sc_hainu@163.com
FU National Natural Science Foundation of China [61461017]; Hainan Province
   Natural Science Foundation of Innovation Team Project [2017CXTD004];
   Innovative Research Project of Postgraduates in Hainan Province
   [Hyb2017-04]
FX This work was supported in part by National Natural Science Foundation
   of China (61461017), Hainan Province Natural Science Foundation of
   Innovation Team Project (2017CXTD004), Innovative Research Project of
   Postgraduates in Hainan Province (Hyb2017-04).
CR Burgos-Artizzu XP, 2013, IEEE I CONF COMP VIS, P1513, DOI 10.1109/ICCV.2013.191
   Cao C, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601204
   Cao XD, 2014, INT J COMPUT VISION, V107, P177, DOI 10.1007/s11263-013-0667-3
   Charalabopoulos A, 2012, PATTERN RECOGN, V45, P1275
   Chen YT, 2019, CLUSTER COMPUT, V22, pS7435, DOI 10.1007/s10586-018-1772-4
   Felzenszwalb PF, 2016, PATTERN ANAL MACH IN, V38, P1627
   Huang Fei, 2015, Computer Engineering and Applications, V51, P204, DOI 10.3778/j.issn.1002-8331.1308-0412
   Lee D, 2015, PROC CVPR IEEE, P4204, DOI 10.1109/CVPR.2015.7299048
   Li Y, 2016, VISUAL COMPUT, V32, P1525, DOI 10.1007/s00371-015-1137-4
   Martinez A, 2012, J MACH LEARN RES, V13, P1589
   Mustafa O, 2015, IEEE T PATTERN ANAL, V37, P448
   Ren SQ, 2014, PROC CVPR IEEE, P1685, DOI 10.1109/CVPR.2014.218
   Xiong XH, 2013, PROC CVPR IEEE, P532, DOI 10.1109/CVPR.2013.75
   Zhao J, 2018, IEEE PHOTONICS J, V10, DOI [10.1109/JPHOT.2017.2778218, 10.1109/JPHOT.2018.2792444]
   Zhong RZ, 2016, J AIR FORCE ENG U NA, V17, P60
   Zhong S, 2015, CARBON, V85, P51, DOI 10.1016/j.carbon.2014.12.064
NR 16
TC 0
Z9 0
U1 0
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2019
VL 35
IS 9
BP 1237
EP 1244
DI 10.1007/s00371-018-1610-y
PG 8
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA IQ2JJ
UT WOS:000480574500004
DA 2024-07-18
ER

PT J
AU Zhao, JW
   Chen, Y
   Zhang, HT
   Xia, H
   Wang, ZY
   Peng, QS
AF Zhao, Jianwang
   Chen, Yi
   Zhang, Haitong
   Xia, Hao
   Wang, Zhangye
   Peng, Qunsheng
TI Physically based modeling and animation of landslides with MPM
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 36th Computer Graphics International Conference (CGI)
CY JUN 17-20, 2019
CL Calgary, CANADA
SP Biometric Technologies Lab, Univ Calgary, VPR Off, Fac Sci, Comp Sci Dept, Alberta Ingenu, CGS
DE Natural phenomena simulation; Landslide; Modified material point method;
   Soil dynamics; Two-way fluid-solid coupling
AB Landslide is a disaster which may cause huge losses of human life and block the traffic on hilly area. In this paper, we present a new physically based model to simulate the dynamic flow of landslides, under a modified MPM (material point method) framework. To realistically simulate the characteristics of fracture and flow of soil medium in landslide, we introduce the modified Cambridge clay model (MCCM) from soil dynamics into the yield surface criterion to model the dynamic process of landslides. The interaction between soil and rock in the landslide is simulated by a level-set-based two-way fluid-solid coupling algorithm. Meanwhile, we propose a GPU-based optimization to calculate the signed distance function in level set to improve the efficiency of collision detection. We also simplify the hardening and softening parameter calculation algorithm of MCCM to reduce the calculation involved in landslide simulation. By choosing different values of the material yield surface parameters, various kinds of landslide disaster scenes with different cover area are successfully generated, including rocks rolling from hill, soil and rock collapsing, landslide flowing, and covering the road and cars. Experimental results demonstrate the potential of our method.
C1 [Zhao, Jianwang; Zhang, Haitong; Xia, Hao; Wang, Zhangye; Peng, Qunsheng] Zhejiang Univ, State Key Lab CAD & CG, Hangzhou 310058, Zhejiang, Peoples R China.
   [Chen, Yi] Commun Univ Zhejiang, Hangzhou 310018, Zhejiang, Peoples R China.
C3 Zhejiang University; Communication University of Zhejiang
RP Wang, ZY (corresponding author), Zhejiang Univ, State Key Lab CAD & CG, Hangzhou 310058, Zhejiang, Peoples R China.
EM zjwzjw369@zju.edu.cn; chenyi@timeaxis.com.cn; haitong@zju.edu.cn;
   21721084@zju.edu.cn; zywang@cad.zju.edu.cn; peng@cad.zju.edu.cn
FU National Key R&D Program of China [2017YFB1002703]; Natural Science
   Foundation of China [U1736109]; 863 Program of China [2015AA016404]
FX This research work was supported partially by National Key R&D Program
   of China under Grant No. 2017YFB1002703, Natural Science Foundation of
   China under Grant No. U1736109 and 863 Program of China under Grant No.
   2015AA016404. The authors thank Wei Li and Yuan Chen from Timeaxis
   Digital Studios Co., Ltd., for their help with 3D modeling of scenes,
   rendering, and video production. Many thanks are also to the reviewers
   for their helpful comments. There are no conflicts of interest with
   other people or entities.
CR Alduan I., 2011, P 2011 ACM SIGGRAPH, P25, DOI DOI 10.1145/2019406.2019410
   Andersen S, 2010, COMPUTAT GEOSCI, V14, P137, DOI 10.1007/s10596-009-9137-y
   [Anonymous], 2016 INT C COMP ENG
   [Anonymous], COMPUT ANIMAT VIRTUA
   Bonet J., 1997, Nonlinear continuum mechanics for finite element analysis
   BORJA RI, 1990, COMPUT METHOD APPL M, V78, P49, DOI 10.1016/0045-7825(90)90152-C
   Carlson M, 2004, ACM T GRAPHIC, V23, P377, DOI 10.1145/1015706.1015733
   Daviet G, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925877
   Hu YM, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201293
   Ihmsen M, 2013, COMPUT GRAPH-UK, V37, P800, DOI 10.1016/j.cag.2013.04.010
   Jiang C., 2015, ACM T GRAPHIC, V34
   Klár G, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925906
   Koschier D., 2017, P ACM SIGGRAPHEUROGR, P1
   Lenaerts T, 2009, COMPUT GRAPH FORUM, V28, P213, DOI 10.1111/j.1467-8659.2009.01360.x
   Macklin M, 2014, ACM T GRAPHIC, V33, DOI [10.1145/280/109/2601152, 10.1145/2601097.2601152]
   Macklin M, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461984
   Mast CM, 2011, SPRINGER SER GEOMECH, P169
   MILLER G, 1989, COMPUT GRAPH-UK, V13, P305, DOI 10.1016/0097-8493(89)90078-2
   Ming Gao, 2018, ACM Transactions on Graphics, V37, DOI 10.1145/3272127.3275044
   Narain R, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866195
   Ram D., 2015, P 14 ACM SIGGRAPHEUR, P157, DOI DOI 10.1145/2786784.2786798
   Soga K, 2018, GEOTECHNIQUE, V68, P457, DOI 10.1680/jgeot.16.D.004
   Stomakhin A, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461948
   Stomakhin A, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601176
   Tampubolon Andre Pradhana, 2017, ACM Transactions on Graphics, V36, DOI 10.1145/3072959.3073651
   Wang CB, 2015, COMPUT ANIMAT VIRT W, V26, P3, DOI 10.1002/cav.1542
   Yue YH, 2018, SIGGRAPH ASIA'18: SIGGRAPH ASIA 2018 TECHNICAL PAPERS, DOI 10.1145/3272127.3275095
   Zhang SF, 2017, COMPUT ANIMAT VIRT W, V28, DOI 10.1002/cav.1758
   Zhu YN, 2005, ACM T GRAPHIC, V24, P965, DOI 10.1145/1073204.1073298
NR 29
TC 4
Z9 5
U1 0
U2 26
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2019
VL 35
IS 9
BP 1223
EP 1235
DI 10.1007/s00371-019-01709-3
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA IQ2JJ
UT WOS:000480574500003
DA 2024-07-18
ER

PT J
AU Hu, JB
   Wang, SF
   Wang, Y
   Li, FQ
   Luo, ZX
AF Hu, Jiangbei
   Wang, Shengfa
   Wang, Yi
   Li, Fengqi
   Luo, Zhongxuan
TI A lightweight methodology of 3D printed objects utilizing multi-scale
   porous structures
SO VISUAL COMPUTER
LA English
DT Article
DE Lightweight; Porous structures; 3D Printing
ID SCAFFOLD DESIGN; OPTIMIZATION; FABRICATION
AB Lightweight modeling is one of the most important research subjects in modern fabrication manufacturing, and it provides not only a low-cost solution but also functional applications, especially for the fabrication using 3D printing. This approach presents a multi-scale porous structure-based lightweight framework to reduce the weight of 3D printed objects while meeting the specified requirements. Specifically, the triply periodic minimal surface (TPMS) is exploited to design a multi-scale porous structure, which can achieve high mechanical behaviors with lightweight. The multi-scale porous structure is constructed using compactly supported radial basis functions, and it inherits the good properties of TPMS, such as smoothness, full connectivity (no closed hollows) and quasi-self-supporting (free of extra supports in most cases). Then, the lightweight problem utilizing the porous structures is formulated into a constrained optimization. Finally, a strength-to-weight optimization method is proposed to obtain the lightweight models. It is also worth noting that the proposed porous structures can be perfectly fabricated by common 3D printing technologies on account of the leftover material, such as the liquid in SLA, which can be removed through the fully connected void channel.
C1 [Hu, Jiangbei] Dalian Univ Technol, Sch Math Sci, Dalian, Peoples R China.
   [Wang, Shengfa; Wang, Yi] Dalian Univ Technol, DUT RU Int Sch Informat & Software Engn, Dalian, Peoples R China.
   [Wang, Shengfa; Wang, Yi] Dalian Univ Technol, Key Lab Ubiquitous Network & Serv Software Liaoni, Dalian, Peoples R China.
   [Li, Fengqi; Luo, Zhongxuan] Dalian Univ Technol, Sch Software, Dalian, Peoples R China.
   [Luo, Zhongxuan] Guilin Univ Elect Technol, Inst Artificial Intelligence, Guilin, Peoples R China.
C3 Dalian University of Technology; Dalian University of Technology; Dalian
   University of Technology; Dalian University of Technology; Guilin
   University of Electronic Technology
RP Wang, SF (corresponding author), Dalian Univ Technol, DUT RU Int Sch Informat & Software Engn, Dalian, Peoples R China.; Wang, SF (corresponding author), Dalian Univ Technol, Key Lab Ubiquitous Network & Serv Software Liaoni, Dalian, Peoples R China.
EM sfwang@dlut.edu.cn; zxluo@dlut.edu.cn
RI Hu, Jiangbei/JPK-7431-2023; Li, FengQi/HLQ-1543-2023
OI Hu, Jiangbei/0000-0002-6774-6267; Li, FengQi/0000-0003-4056-548X
FU National Natural Science Foundation of China [61772104, 61432003,
   2017YFB1103700, 2016YFB1101100, 61720106005, DUT2017TB02]
FX ;This research is supported by the National Natural Science Foundation
   of China Grant (61772104, 61432003, 2017YFB1103700, 2016YFB1101100,
   61720106005 and DUT2017TB02).
CR [Anonymous], 2001, P 18 INT C MACH LEAR
   Attene Marco, 2018, Synthesis Lectures on Visual Computing, V10, P1
   Bendse M P., 1989, Struct. Optim., V1, P193, DOI DOI 10.1007/BF01650949
   Bickel B, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778800
   Chai SM, 2018, GRAPH MODELS, V97, P80, DOI 10.1016/j.gmod.2018.04.002
   Coros S, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185565
   Cvijovi D, 1992, J PHYS, VI, P137
   Feng JW, 2018, VIS COMPUT IND BIOME, V1, DOI 10.1186/s42492-018-0004-3
   Fryazinov O, 2013, COMPUT AIDED DESIGN, V45, P26, DOI 10.1016/j.cad.2011.09.007
   Guo X, 2014, J APPL MECH, V34, P255
   Hollister SJ, 2005, NAT MATER, V4, P518, DOI 10.1038/nmat1421
   Houck C. R., 1995, NCSU IE T, V95, P1
   Hu KL, 2015, COMPUT AIDED DESIGN, V65, P1, DOI 10.1016/j.cad.2015.03.001
   Hutmacher DW, 2000, BIOMATERIALS, V21, P2529, DOI 10.1016/S0142-9612(00)00121-6
   Li DW, 2018, COMPUT AIDED DESIGN, V104, P87, DOI 10.1016/j.cad.2018.06.003
   Li DW, 2016, INT J ADV MANUF TECH, V83, P1627, DOI 10.1007/s00170-015-7704-z
   LI DJ, 2019, J MECH DES, V7
   Lin L, 2014, ACM T GRAPHIC, V33
   Martínez J, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073638
   Melchels FPW, 2010, BIOMATERIALS, V31, P6909, DOI 10.1016/j.biomaterials.2010.05.068
   Patzák B, 2012, ADV ENG SOFTW, V47, P35, DOI 10.1016/j.advengsoft.2011.12.008
   Prevost R., 2013, ACM T GRAPH, V32, P81
   Rajagopalan S, 2005, LECT NOTES COMPUT SC, V3749, P794
   RESTREPO S, 2017, J PHYS CONF SER, V935, P1
   ROSE DB, 2006, RAP MAN C, V18, P1
   Ruprecht D, 1993, GEOM MODELL, V281, P267
   Savio G, 2019, PROG ADDIT MANUF
   Schroeder C, 2005, COMPUT AIDED DESIGN, V37, P339, DOI 10.1016/j.cad.2004.03.008
   Shao TJ, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366155
   SHAPEWAYS, TUTORIAL HOW TO HOLL
   Skouras M, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461979
   STAVA O, 2001, ACM T GRAPHIC, V31
   Wang HQ, 2005, PROCEEDINGS OF THE ASME INTERNATIONAL DESIGN ENGINEERING TECHNICAL CONFERENCES AND COMPUTERS AND INFORMATION IN ENGINEERING CONFERENCE 2005, VOL 3, PTS A AND B, P421
   Wang L., 2016, Computer Graphics Forum, V35, P49, DOI 10.1111/cgf.12810
   Wang MY, 2003, COMPUT METHOD APPL M, V192, P227, DOI 10.1016/S0045-7825(02)00559-5
   Wang T. Y., 2016, J COMPUTER GRAPHICS, V5, P18
   WANG W, 2001, ACM T GRAPHIC, V32
   Wang WM, 2018, IEEE T VIS COMPUT GR, V24, P2787, DOI 10.1109/TVCG.2017.2764462
   Wang WM, 2017, COMPUT GRAPH-UK, V66, P154, DOI 10.1016/j.cag.2017.05.022
   Wang WM, 2017, VISUAL COMPUT, V33, P949, DOI 10.1007/s00371-017-1386-5
   Wang Y, 2007, COMPUT AIDED DESIGN, V39, P179, DOI 10.1016/j.cad.2006.09.005
   Wendland H., 1995, Advances in Computational Mathematics, V4, P389, DOI 10.1007/BF02123482
   Wu J, 2016, IEEE T VIS COMPUT GR, V22, P1195, DOI 10.1109/TVCG.2015.2502588
   XIE YM, 1993, COMPUT STRUCT, V49, P885, DOI 10.1016/0045-7949(93)90035-C
   Xie Y, 2017, VIS INFORM, V1, P9, DOI 10.1016/j.visinf.2017.01.002
   Yang N, 2014, MAT SCI ENG C-MATER, V43, P502, DOI 10.1016/j.msec.2014.07.052
   Yoo DJ, 2014, INT J PRECIS ENG MAN, V15, P2117, DOI 10.1007/s12541-014-0571-y
   Yoo D, 2012, MED ENG PHYS, V34, P625, DOI 10.1016/j.medengphy.2012.03.009
   Zhang XL, 2015, COMPUT AIDED GEOM D, V35-36, P149, DOI 10.1016/j.cagd.2015.03.012
NR 49
TC 30
Z9 32
U1 11
U2 79
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2019
VL 35
IS 6-8
SI SI
BP 949
EP 959
DI 10.1007/s00371-019-01672-z
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IC1IH
UT WOS:000470712200015
OA Bronze
DA 2024-07-18
ER

PT J
AU He, LT
   Wang, YL
   Xiang, ZY
AF He, Liangtian
   Wang, Yilun
   Xiang, Zhaoyin
TI Wavelet frame-based image restoration using sparsity, nonlocal, and
   support prior of frame coefficients
SO VISUAL COMPUTER
LA English
DT Article
DE Image restoration; Wavelet frame; Support detection; Nonlocal estimation
ID SPLIT BREGMAN METHOD; SIMULTANEOUS CARTOON; NOISE REMOVAL; ALGORITHM;
   MINIMIZATION; REPRESENTATIONS; RECOVERY
AB Over the past decade, wavelet frames have been widely investigated in the field of image restoration. The success of them is largely attributed to their ability of sparsely representing piecewise smooth functions such as natural images. Classical wavelet frame models mostly are based on the sparsity prior of frame coefficients, e.g., the 1 norm or 0 norm regularizer term is commonly employed. The sparsity-promoting regularization has became so prevailing that it is desirable to explore more prior knowledge of the underlying image to achieve better recovery performance, besides the conventional sparsity prior. In this paper, we formulate a new wavelet frame-based truncated 0-2 model which simultaneously combines sparsity, nonlocal and support prior of the frame coefficients. Specifically, we focus on investigating the role of these priors play in the regularization model for image restoration problems. Extensive deblurring and denoising experiments are reported to demonstrate the effectiveness of our proposed method, not only in terms of objective PSNR and SSIM improvements over the state-of-the-art algorithms, but also subjectively producing more pleasing recovery output.
C1 [He, Liangtian; Wang, Yilun; Xiang, Zhaoyin] Univ Elect Sci & Technol China, Sch Math Sci, Chengdu 611731, Sichuan, Peoples R China.
C3 University of Electronic Science & Technology of China
RP He, LT; Wang, YL (corresponding author), Univ Elect Sci & Technol China, Sch Math Sci, Chengdu 611731, Sichuan, Peoples R China.
EM 970754953@qq.com; yilun.wang@gmail.com; zxiangmath@163.com
RI He, Liang/CAF-0477-2022; xiang, zhao/HGB-8063-2022; WANG,
   YILUN/KFB-0627-2024; Xiang, Zhaoyin/O-8802-2018
OI Xiang, Zhaoyin/0000-0002-6494-5164
FU National Basic Research Program (973 Program) [2015CB856000]; Natural
   Science Foundation of China [11201054, 91330201]; Fundamental Research
   Funds for the Central Universities [ZYGX2013Z005]
FX Funding was provided by National Basic Research Program (973 Program)
   (Grant No. 2015CB856000), Natural Science Foundation of China (Grant
   Nos. 11201054, 91330201) and Fundamental Research Funds for the Central
   Universities (Grant No. ZYGX2013Z005).
CR Aharon M, 2006, IEEE T SIGNAL PROCES, V54, P4311, DOI 10.1109/TSP.2006.881199
   Benning M, 2013, J SCI COMPUT, V54, P269, DOI 10.1007/s10915-012-9650-3
   Boyd Stephen, 2010, Foundations and Trends in Machine Learning, V3, P1, DOI 10.1561/2200000016
   Bredies K, 2010, SIAM J IMAGING SCI, V3, P492, DOI 10.1137/090769521
   Buades A, 2005, PROC CVPR IEEE, P60, DOI 10.1109/cvpr.2005.38
   Cai JF, 2016, APPL COMPUT HARMON A, V41, P94, DOI 10.1016/j.acha.2015.06.009
   Cai JF, 2014, APPL COMPUT HARMON A, V37, P89, DOI 10.1016/j.acha.2013.10.001
   Cai JF, 2012, J AM MATH SOC, V25, P1033, DOI 10.1090/S0894-0347-2012-00740-1
   Cai JF, 2010, INVERSE PROBL IMAG, V4, P379, DOI 10.3934/ipi.2010.4.379
   Cai JF, 2009, MULTISCALE MODEL SIM, V8, P337, DOI 10.1137/090753504
   Cai JF, 2009, ADV COMPUT MATH, V31, P87, DOI 10.1007/s10444-008-9084-5
   Candès EJ, 2008, IEEE SIGNAL PROC MAG, V25, P21, DOI 10.1109/MSP.2007.914731
   Chambolle A, 1997, NUMER MATH, V76, P167, DOI 10.1007/s002110050258
   Chan RH, 2003, SIAM J SCI COMPUT, V24, P1408, DOI 10.1137/S1064827500383123
   Chan T, 2000, SIAM J SCI COMPUT, V22, P503, DOI 10.1137/S1064827598344169
   Chen DQ, 2016, J SCI COMPUT, V69, P461, DOI 10.1007/s10915-016-0205-x
   Chen DQ, 2016, J SCI COMPUT, V66, P196, DOI 10.1007/s10915-015-0018-3
   Dabov K., 2009, SPARS 09 SIGNAL PROC
   Dabov K, 2007, IEEE T IMAGE PROCESS, V16, P2080, DOI 10.1109/TIP.2007.901238
   Danielyan A, 2012, IEEE T IMAGE PROCESS, V21, P1715, DOI 10.1109/TIP.2011.2176954
   Daubechies I, 2003, APPL COMPUT HARMON A, V14, P1, DOI 10.1016/S1063-5203(02)00511-0
   Dong B., 2010, IAS Lecture Notes Series, Summer Program on "The Mathematics of Image Processing, V19
   Dong B, 2013, J SCI COMPUT, V54, P350, DOI 10.1007/s10915-012-9597-4
   Dong WS, 2013, IEEE T IMAGE PROCESS, V22, P1618, DOI 10.1109/TIP.2012.2235847
   Dong WS, 2011, PROC CVPR IEEE, P457, DOI 10.1109/CVPR.2011.5995478
   Donoho DL, 2006, IEEE T INFORM THEORY, V52, P1289, DOI 10.1109/TIT.2006.871582
   Elad M, 2005, APPL COMPUT HARMON A, V19, P340, DOI 10.1016/j.acha.2005.03.005
   Elad M, 2006, IEEE T IMAGE PROCESS, V15, P3736, DOI 10.1109/TIP.2006.881969
   Figueiredo MAT, 2003, IEEE T IMAGE PROCESS, V12, P906, DOI 10.1109/TIP.2003.814255
   Figuriredo M., 2005, IEEE INT C IM PROC, V2, pfi
   Goldstein T, 2009, SIAM J IMAGING SCI, V2, P323, DOI 10.1137/080725891
   Gu SH, 2014, PROC CVPR IEEE, P2862, DOI 10.1109/CVPR.2014.366
   He L., 2016, ARXIV160308108
   He LT, 2014, IEEE T IMAGE PROCESS, V23, P5470, DOI 10.1109/TIP.2014.2362051
   Ji H, 2017, J FOURIER ANAL APPL, V23, P729, DOI 10.1007/s00041-016-9487-5
   Ji H, 2016, APPL COMPUT HARMON A, V41, P75, DOI 10.1016/j.acha.2015.08.012
   Khmag A, 2017, VISUAL COMPUT, V33, P1141, DOI 10.1007/s00371-016-1273-5
   Kindermann S, 2005, MULTISCALE MODEL SIM, V4, P1091, DOI 10.1137/050622249
   Mairal J, 2009, IEEE I CONF COMP VIS, P2272, DOI 10.1109/ICCV.2009.5459452
   Quan YH, 2015, J SCI COMPUT, V63, P307, DOI 10.1007/s10915-014-9893-2
   Ron A, 1997, J FUNCT ANAL, V148, P408, DOI 10.1006/jfan.1996.3079
   RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F
   Shen ZW, 2011, SIAM J IMAGING SCI, V4, P573, DOI 10.1137/090779437
   Starck JL, 2005, IEEE T IMAGE PROCESS, V14, P1570, DOI 10.1109/TIP.2005.852206
   Tao M, 2012, SIAM J OPTIMIZ, V22, P1431, DOI 10.1137/110847639
   Wang C, 2018, VISUAL COMPUT, V34, P1357, DOI 10.1007/s00371-017-1418-1
   Wang YL, 2010, SIAM J IMAGING SCI, V3, P462, DOI 10.1137/090772447
   Wang YL, 2008, SIAM J IMAGING SCI, V1, P248, DOI 10.1137/080724265
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Zhang J, 2014, IEEE T IMAGE PROCESS, V23, P3336, DOI 10.1109/TIP.2014.2323127
   Zhang XQ, 2010, SIAM J IMAGING SCI, V3, P253, DOI 10.1137/090746379
   Zhang Y, 2013, MATH COMPUT, V82, P995
NR 52
TC 10
Z9 10
U1 0
U2 11
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2019
VL 35
IS 2
BP 151
EP 174
DI 10.1007/s00371-017-1440-3
PG 24
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HN3DS
UT WOS:000460064600002
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Chen, JZ
   Du, MQ
   Qin, XJ
   Miao, YW
AF Chen, Jiazhou
   Du, Mengqi
   Qin, Xujia
   Miao, Yongwei
TI An improved topology extraction approach for vectorization of sketchy
   line drawings
SO VISUAL COMPUTER
LA English
DT Article
DE Line drawing; Sketch; Vectorization; Topology extraction
AB Vectorization converts raster scans of line drawings into vector graphics; it breaks the barrier between line drawing generation and postprocessing. Prior work on line drawing vectorization considerably succeeded in revealing artists' drawing intention driven by structural topologies. However, none of them is able to extract simplified topologies for sketchy line drawings consisted by many unwanted lines. In this paper, we propose an improved topology extraction approach based on artists' sketching customs. Redundant regions and open curves are discriminated from artists' deliberate ones and further removed progressively through an iterative optimization mechanism. We demonstrate that our improved topology benefits our vectorization method as well as existing topology-driven ones and allows them to vectorize rough sketchy line drawings robustly and efficiently.
C1 [Chen, Jiazhou; Du, Mengqi; Qin, Xujia] Zhejiang Univ Technol, Coll Comp Sci & Technol, Hangzhou, Zhejiang, Peoples R China.
   [Miao, Yongwei] Zhejiang Sci Tech Univ, Sch Informat Sci & Technol, Hangzhou, Zhejiang, Peoples R China.
C3 Zhejiang University of Technology; Zhejiang Sci-Tech University
RP Chen, JZ; Qin, XJ (corresponding author), Zhejiang Univ Technol, Coll Comp Sci & Technol, Hangzhou, Zhejiang, Peoples R China.
EM cjz@zjut.edu.cn; qxj@zjut.edu.cn
RI Miao, Yongwei/ABH-1238-2021; Du, Mengqi/KFQ-9892-2024
OI Miao, Yongwei/0000-0002-5479-9060; Chen, Jiazhou/0000-0003-2780-6146
CR Bao B, 2012, IEEE IMAGE PROC, P805, DOI 10.1109/ICIP.2012.6466982
   Barla Pascal., 2005, Proc. EGSR, P183
   Bartolo A, 2007, SKETCH-BASED INTERFACES AND MODELING 2007, P123
   Bartolo A, 2008, 2008 3RD INTERNATIONAL SYMPOSIUM ON COMMUNICATIONS, CONTROL AND SIGNAL PROCESSING, VOLS 1-3, P554, DOI 10.1109/ISCCSP.2008.4537287
   Bo PB, 2016, J COMPUT DES ENG, V3, P14, DOI 10.1016/j.jcde.2015.05.001
   Bonnici A., 2013, P INT S SKETCH BAS I, P69, DOI [10.1145/2487381.2487386, DOI 10.1145/2487381.2487386]
   Bonnici A, 2009, 2009 THIRD INTERNATIONAL CONFERENCE ON ADVANCED ENGINEERING COMPUTING AND APPLICATIONS IN SCIENCES (ADVCOMP 2009), P89, DOI 10.1109/ADVCOMP.2009.20
   Chen JZ, 2015, SCI CHINA INFORM SCI, V58, DOI 10.1007/s11432-014-5246-x
   Chen JZ, 2013, COMPUT GRAPH FORUM, V32, P98, DOI 10.1111/cgf.12164
   Cornea ND, 2007, IEEE T VIS COMPUT GR, V13, P530, DOI 10.1109/TVCG.2007.1002
   Favreau JD, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925946
   Grabli S, 2004, 12TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P309, DOI 10.1109/PCCGA.2004.1348362
   Hilaire X, 2006, IEEE T PATTERN ANAL, V28, P890, DOI 10.1109/TPAMI.2006.127
   Hilaire X, 2002, LECT NOTES COMPUT SC, V2390, P273
   Huang H, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461913
   Kyprianidis JE, 2011, COMPUT GRAPH FORUM, V30, P593, DOI 10.1111/j.1467-8659.2011.01882.x
   Liu XT, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818067
   Noris G, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2421636.2421640
   Preim B., 1995, WSCG 95 (Winter School of Computer Graphics and Visualisation 95). Third International Conference in Central Europe on Computer Graphics and Visualisation 95, P228
   Saha PK, 2016, PATTERN RECOGN LETT, V76, P3, DOI 10.1016/j.patrec.2015.04.006
   Sasaki K, 2017, PROC CVPR IEEE, P5768, DOI 10.1109/CVPR.2017.611
   Simo-Serra E, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925972
   Sun J, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239462
   Van Nieuwenhuizen P.R., WILEY ONLINE LIB, V13, P349, DOI [DOI 10.1111/1467-8659.1330349, 10.1111/1467-8659.1330349]
   Wang CA, 2017, IEEE T IMAGE PROCESS, V26, P1833, DOI 10.1109/TIP.2017.2666742
   Whited B., 2009, Pattern Recognition and Image Analysis, V19, P277, DOI 10.1134/S1054661809020102
   Wilson Brett., 2004, NPAR 04, P129
   Xia T., 2009, ACM SIGGRAPH AS 2009, DOI 10.1145/1661412.1618461
   Xie GF, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2661229.2661275
   Zhang SH, 2009, IEEE T VIS COMPUT GR, V15, P618, DOI 10.1109/TVCG.2009.9
   ZHANG TY, 1984, COMMUN ACM, V27, P236, DOI 10.1145/357994.358023
NR 31
TC 12
Z9 14
U1 0
U2 13
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2018
VL 34
IS 12
BP 1633
EP 1644
DI 10.1007/s00371-018-1549-z
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GY3WM
UT WOS:000448487400002
DA 2024-07-18
ER

PT J
AU Cambra, AB
   Murillo, AC
   Muñoz, A
AF Cambra, Ana B.
   Murillo, Ana C.
   Munoz, Adolfo
TI A generic tool for interactive complex image editing
SO VISUAL COMPUTER
LA English
DT Article
DE User interaction; Image processing; Computer vision; Dense label
   propagation
ID ENERGY MINIMIZATION; DEPTH; STEREO
AB Plenty of complex image editing techniques require certain per-pixel property or magnitude to be known, e.g., simulating depth of field effects requires a depth map. This work presents an efficient interaction paradigm that approximates any per-pixel magnitude from a few user strokes by propagating the sparse user input to each pixel of the image. The propagation scheme is based on a linear least-squares system of equations which represents local and neighboring restrictions over superpixels. After each user input, the system responds immediately, propagating the values and applying the corresponding filter. Our interaction paradigm is generic, enabling image editing applications to run at interactive rates by changing just the image processing algorithm, but keeping our proposed propagation scheme. We illustrate this through three interactive applications: depth of field simulation, dehazing and tone mapping.
C1 [Cambra, Ana B.; Murillo, Ana C.; Munoz, Adolfo] Univ Zaragoza, Inst Invest Ingn Aragon, E-50009 Zaragoza, Spain.
C3 University of Zaragoza
RP Cambra, AB (corresponding author), Univ Zaragoza, Inst Invest Ingn Aragon, E-50009 Zaragoza, Spain.
EM acambra@unizar.es; acm@unizar.es; adolfo@unizar.es
OI Munoz Orbananos, Adolfo/0000-0002-8160-7159; Murillo Arnal, Ana
   Cristina/0000-0002-7580-9037
CR Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   An XB, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024181
   An XB, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360639
   Andrews S, 2010, LECT NOTES COMPUT SC, V6363, P9
   Ao H., 2015, PAC RIM C MULT
   Berman D, 2016, PROC CVPR IEEE, P1674, DOI 10.1109/CVPR.2016.185
   BESAG J, 1986, J R STAT SOC B, V48, P259
   Bonneel N, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2661229.2661253
   Bousseau A, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618476
   Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114
   Cambra A. B., 2016, BRIT MACH VIS C
   Chaurasia G, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2487228.2487238
   Chen QF, 2014, PROC CVPR IEEE, P3914, DOI 10.1109/CVPR.2014.500
   Chen XW, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366151
   Di Renzo F, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2661229.2661282
   Fattal R, 2014, ACM T GRAPHIC, V34, DOI 10.1145/2651362
   Freedman D, 2012, SIGNAL IMAGE VIDEO P, V6, P533, DOI 10.1007/s11760-010-0181-9
   Garces E, 2012, COMPUT GRAPH FORUM, V31, P1415, DOI 10.1111/j.1467-8659.2012.03137.x
   Grady L, 2006, IEEE T PATTERN ANAL, V28, P1768, DOI 10.1109/TPAMI.2006.233
   Iizuka S, 2014, COMPUT GRAPH FORUM, V33, P279, DOI 10.1111/cgf.12496
   Jarabo A, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2661229.2661251
   Kolmogorov V, 2006, IEEE T PATTERN ANAL, V28, P1568, DOI 10.1109/TPAMI.2006.200
   Kraus M, 2007, COMPUT GRAPH FORUM, V26, P645, DOI 10.1111/j.1467-8659.2007.01088.x
   Levin A, 2004, ACM T GRAPHIC, V23, P689, DOI 10.1145/1015706.1015780
   Lischinski D, 2006, ACM T GRAPHIC, V25, P646, DOI 10.1145/1141911.1141936
   Liu MM, 2014, PROC CVPR IEEE, P716, DOI 10.1109/CVPR.2014.97
   Lopez A, 2014, PREV CHRONIC DIS, P11
   Luan Q., 2007, P 18 EUR C CREND TEC, P309
   Mantiuk R, 2008, COMPUT GRAPH FORUM, V27, P699, DOI 10.1111/j.1467-8659.2008.01168.x
   Micusík B, 2010, INT J COMPUT VISION, V89, P106, DOI 10.1007/s11263-010-0327-9
   Ning JF, 2010, PATTERN RECOGN, V43, P445, DOI 10.1016/j.patcog.2009.03.004
   Phan R, 2014, IEEE T MULTIMEDIA, V16, P122, DOI 10.1109/TMM.2013.2283451
   Reinhard E, 2002, ACM T GRAPHIC, V21, P267, DOI 10.1145/566570.566575
   Ren XF, 2012, PROC CVPR IEEE, P2759, DOI 10.1109/CVPR.2012.6247999
   Scharstein D, 2002, INT J COMPUT VISION, V47, P7, DOI 10.1023/A:1014573219977
   Scharstein D, 2003, PROC CVPR IEEE, P195
   Singaraju D, 2009, PROC CVPR IEEE, P1303, DOI 10.1109/CVPRW.2009.5206669
   Stutz D, 2018, COMPUT VIS IMAGE UND, V166, P1, DOI 10.1016/j.cviu.2017.03.007
   Szeliski R, 2008, IEEE T PATTERN ANAL, V30, P1068, DOI 10.1109/TPAMI.2007.70844
   Tappen MF, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P900
   Tighe J, 2013, INT J COMPUT VISION, V101, P329, DOI 10.1007/s11263-012-0574-z
   Wang OL, 2011, PITUITARY, 3RD EDITION, P47, DOI 10.1016/B978-0-12-380926-1.10003-3
   WEINSHALL D, 1990, COMPUT VISION GRAPH, V49, P222, DOI 10.1016/0734-189X(90)90138-L
   Xu K, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618464
   Yamaguchi K, 2012, LECT NOTES COMPUT SC, V7576, P45, DOI 10.1007/978-3-642-33715-4_4
   Yu F, 2014, PROC CVPR IEEE, P3986, DOI 10.1109/CVPR.2014.509
   Yucer K., 2013, P S VIS MOD VIS VMV
   Yücer K, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366195
NR 48
TC 3
Z9 3
U1 0
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2018
VL 34
IS 11
BP 1493
EP 1505
DI 10.1007/s00371-017-1422-5
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GV9ZR
UT WOS:000446521700003
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Choi, C
   Lee, H
   Yi, J
AF Choi, Changwon
   Lee, Hyungkeun
   Yi, Joonhwan
TI An interpolation method for strong barrel lens distortion
SO VISUAL COMPUTER
LA English
DT Article
DE Fisheye lens; Barrel distortion; Interpolation; Look-up table
AB We propose an interpolation method considering strong barrel distortion of a fisheye lens using nearest pixels on a corrected image. The correction of barrel distortion comprises coordinate transformation and interpolation, and this paper focuses on interpolation. The proposed interpolation method uses nearest coordinates on a corrected image rather than on a distorted image, unlike existing techniques. The increased computational complexity of the proposed interpolation method is alleviated by using look-up table (LUT)-based optimization. Experimental results show that both subjective and objective image qualities are improved with marginal execution time.
C1 [Choi, Changwon; Lee, Hyungkeun; Yi, Joonhwan] Kwangwoon Univ, Dept Comp Engn, 20 Gwangun Ro, Seoul, South Korea.
C3 Kwangwoon University
RP Yi, J (corresponding author), Kwangwoon Univ, Dept Comp Engn, 20 Gwangun Ro, Seoul, South Korea.
EM changwonchoi@kw.ac.kr; hklee@kw.ac.kr; joonhwan.yi@kw.ac.kr
FU Industrial Strategic Technology Development Program - Ministry of Trade,
   industry & Energy (MI, Korea) [10047664]; IDEC(EDA Tool, MPW)
FX The present Research has been conducted by the Research Grant of
   Kwangwoon University in 2016. This work was supported by the Industrial
   Strategic Technology Development Program (10047664, Automatic power
   model generation software development for low power designs with more
   than 300 times faster power analysis speed and less than 20% error rate
   on average with respect to the gate-level power models) funded by the
   Ministry of Trade, industry & Energy (MI, Korea). This work was
   supported by IDEC(EDA Tool, MPW).
CR [Anonymous], 2011, THESIS
   Chen SL, 2011, IEEE T CIRC SYST VID, V21, P1612, DOI 10.1109/TCSVT.2011.2129850
   Chen YS, 2006, VISUAL COMPUT, V22, P445, DOI 10.1007/s00371-006-0014-6
   Daloukas Konstantis, 2010, 2010 IEEE INT S PAR, P1
   Foley JD, 2013, COMPUTER GRAPHICS
   Franzen R., 2013, Kodak lossless true color image suite
   Han J., 2010, THESIS
   Hughes C, 2009, IET INTELL TRANSP SY, V3, P19, DOI 10.1049/iet-its:20080017
   Hui G., 2014, 2014 IEEE WORKSH ADV
   Jacobson R, 2000, MANUAL PHOTOGRAPHY M
   Johnson K. B., 1998, INTELLIGENT ROBOTS C
   Kanatani K, 2013, IEEE T PATTERN ANAL, V35, P813, DOI 10.1109/TPAMI.2012.146
   KEYS RG, 1981, IEEE T ACOUST SPEECH, V29, P1153, DOI 10.1109/TASSP.1981.1163711
   Kim H, 2015, I SYMP CONSUM ELECTR, P505, DOI 10.1109/ICCE.2015.7066501
   Lehmann TM, 1999, IEEE T MED IMAGING, V18, P1049, DOI 10.1109/42.816070
   Rao AS, 2015, VISUAL COMPUT, V31, P1533, DOI 10.1007/s00371-014-1032-4
   Scaramuzza D, 2006, 2006 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, VOLS 1-12, P5695, DOI 10.1109/IROS.2006.282372
   Schneider D, 2009, ISPRS J PHOTOGRAMM, V64, P259, DOI 10.1016/j.isprsjprs.2009.01.001
   Shih SE, 2014, IEEE T VEH TECHNOL, V63, P2521, DOI 10.1109/TVT.2013.2297331
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wei J, 2012, IEEE T VIS COMPUT GR, V18, P1771, DOI 10.1109/TVCG.2011.130
   Zhang B., 2015, 2015 INT C IND INF C
   Zhu HJ, 2013, IET COMPUT VIS, V7, P362, DOI 10.1049/iet-cvi.2013.0013
   최창원, 2013, [Journal of the Institute of Electronics and Information Engineers, 전자공학회논문지], V50, P181
NR 24
TC 3
Z9 3
U1 0
U2 21
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2018
VL 34
IS 11
BP 1479
EP 1491
DI 10.1007/s00371-017-1414-5
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GV9ZR
UT WOS:000446521700002
DA 2024-07-18
ER

PT J
AU Ben-Ezra, S
   Cohen-Or, D
AF Ben-Ezra, Shahar
   Cohen-Or, Daniel
TI Space-time image layout
SO VISUAL COMPUTER
LA English
DT Article
DE Image organization; Spatial ordering; Temporal ordering; Self-organizing
   maps
ID SHAPE; MAP; CLASSIFICATION; ALGORITHM
AB Cameras are now ubiquitous in our lives. A given activity is often captured by multiple people from different viewpoints resulting in a sizable collection of photograph footage. We present a method that effectively organizes this spatiotemporal content. Given an unorganized collection of photographs taken by a number of photographers, capturing some dynamic event at a number of time steps, we would like to organize the collection into a space-time table. The organization is an embedding of the photographs into clusters that preserve the viewpoint and time order. Our method relies on a self-organizing map (SOM), which is a neural network that embeds the training data (the set of images) into a discrete domain. We introduce BiSOM, which is a variation of SOM that considers two features (space and time) rather than a single one, to layout the given photograph collection into a table. We demonstrate our method on several challenging datasets, using different space and time descriptors.
C1 [Ben-Ezra, Shahar] Tel Aviv Univ, Elect Engn Sch, Tel Aviv, Israel.
   [Cohen-Or, Daniel] Tel Aviv Univ, Comp Sci Sch, Tel Aviv, Israel.
C3 Tel Aviv University; Tel Aviv University
RP Ben-Ezra, S (corresponding author), Tel Aviv Univ, Elect Engn Sch, Tel Aviv, Israel.
EM shaharb@mail.tau.ac.il; dcor@tau.ac.il
CR Aoki T, 2007, NEURAL COMPUT, V19, P2515, DOI 10.1162/neco.2007.19.9.2515
   Averbuch-Elor H, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2735628
   Bashyal S, 2008, ENG APPL ARTIF INTEL, V21, P1056, DOI 10.1016/j.engappai.2007.11.010
   Brahmachari A. S., GRAPH PATT IM SIBGR, P157
   Caspi Y, 2002, IEEE T PATTERN ANAL, V24, P1409, DOI 10.1109/TPAMI.2002.1046148
   Chen LP, 2014, NEURAL COMPUT APPL, V24, P1759, DOI 10.1007/s00521-013-1416-9
   Cormode G, 2007, ACM T ALGORITHMS, V3, DOI 10.1145/1186810.1186812
   Dekel T, 2014, INT J COMPUT VISION, V110, P275, DOI 10.1007/s11263-014-0712-x
   Dekel T, 2013, IEEE I CONF COMP VIS, P977, DOI 10.1109/ICCV.2013.125
   Dexter E, 2009, BR MACH VIS C, P1
   Endo M, 2002, J VLSI SIG PROC SYST, V32, P105, DOI 10.1023/A:1016371519687
   Fried O, 2015, COMPUT GRAPH FORUM, V34, P155, DOI 10.1111/cgf.12549
   Furukawa Y, 2010, PROC CVPR IEEE, P1434, DOI 10.1109/CVPR.2010.5539802
   HUBERT L, 1985, J CLASSIF, V2, P193, DOI 10.1007/BF01908075
   Kiang MY, 2001, COMPUT STAT DATA AN, V38, P161, DOI 10.1016/S0167-9473(01)00040-8
   KOHONEN T, 1990, P IEEE, V78, P1464, DOI 10.1109/5.58325
   Lee JA, 2002, NEURAL NETWORKS, V15, P993, DOI 10.1016/S0893-6080(02)00073-4
   Lefebvre G, 2006, INT C PATT RECOG, P728
   Ling HB, 2007, IEEE T PATTERN ANAL, V29, P286, DOI 10.1109/TPAMI.2007.41
   Mauro M., 2013, BMVC, V1, P3
   Moehrmann J, 2011, LECT NOTES COMPUT SC, V6761, P618, DOI 10.1007/978-3-642-21602-2_67
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   Ong SH, 2002, IMAGE VISION COMPUT, V20, P279, DOI 10.1016/S0262-8856(02)00021-5
   Quadrianto Novi, 2009, Advances in Neural Information Processing Systems (NIPS), P1289
   Reinert B, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508409
   Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720
   ROUSSEEUW PJ, 1987, J COMPUT APPL MATH, V20, P53, DOI 10.1016/0377-0427(87)90125-7
   Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688
   Snavely N, 2006, ACM T GRAPHIC, V25, P835, DOI 10.1145/1141911.1141964
   Strong G, 2014, IEEE T MULTIMEDIA, V16, P1045, DOI 10.1109/TMM.2014.2306183
   TOMASI C, 1992, INT J COMPUT VISION, V9, P137, DOI 10.1007/BF00129684
   Zhou HY, 2009, COMPUT VIS IMAGE UND, V113, P345, DOI 10.1016/j.cviu.2008.08.006
NR 32
TC 0
Z9 0
U1 0
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2018
VL 34
IS 3
BP 417
EP 430
DI 10.1007/s00371-016-1347-4
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FW2CZ
UT WOS:000425110900009
DA 2024-07-18
ER

PT J
AU Yi, Y
   Wang, HL
AF Yi, Yun
   Wang, Hanli
TI Motion keypoint trajectory and covariance descriptor for human action
   recognition
SO VISUAL COMPUTER
LA English
DT Article
DE Human action recognition; Motion keypoint trajectory; Optical flow
   rectification; Trajectory-based covariance descriptor
ID REGION COVARIANCE; HISTOGRAMS; DENSE
AB Human action recognition from videos is a challenging task in computer vision. In recent years, histogram-based descriptors that are calculated along dense trajectories have shown promising results for human action recognition, but they usually ignore motion information of the tracking points, and the relationship between different motion variables is not well utilized. To address this issue, we propose a motion keypoint trajectory (MKT) approach and a trajectory-based covariance (TBC) descriptor, which is calculated along the motion keypoint trajectories. The proposed MKT approach tracks motion keypoints at multiple spatial scales and employs an optical flow rectification algorithm to reduce the influence of camera motions and thus achieves better performance than the improved dense trajectory (IDT) approach well known in the literature. In particular, MKT is faster than IDT, because MKT does not need to use human detection and extracts fewer trajectories than IDT. Furthermore, the TBC descriptor outperforms the classical histogram-based descriptors, such as the Histogram of Oriented Gradient, Histogram of Optical Flow and Motion Boundary Histogram. Experimental results on three challenging datasets (i.e., Olympic Sports, HMDB51 and UCF50) demonstrate that our approach is able to achieve better recognition performances than a number of state-of-the-art approaches.
C1 [Yi, Yun; Wang, Hanli] Tongji Univ, Dept Comp Sci & Technol, Shanghai 201804, Peoples R China.
   [Yi, Yun; Wang, Hanli] Tongji Univ, Key Lab Embedded Syst & Serv Comp, Minist Educ, Shanghai 200092, Peoples R China.
   [Yi, Yun] Gannan Normal Univ, Dept Math & Comp Sci, Ganzhou 341000, Peoples R China.
C3 Tongji University; Tongji University; Gannan Normal University
RP Wang, HL (corresponding author), Tongji Univ, Dept Comp Sci & Technol, Shanghai 201804, Peoples R China.; Wang, HL (corresponding author), Tongji Univ, Key Lab Embedded Syst & Serv Comp, Minist Educ, Shanghai 200092, Peoples R China.
EM hanliwang@tongji.edu.cn
RI Wang, Hanli/G-5111-2014; Wang, Hanli/K-5717-2019; Yi, Yun/O-8432-2018
OI Wang, Hanli/0000-0002-9999-4871; Wang, Hanli/0000-0002-9999-4871; Yi,
   Yun/0000-0002-5644-8002
FU National Natural Science Foundation of China [61472281, 61622115];
   Program for Professor of Special Appointment (Eastern Scholar) at
   Shanghai Institutions of Higher Learning [GZ2015005]; NSF of Jiangxi
   Province [20161BAB202069]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61472281 and 61622115, the Program for
   Professor of Special Appointment (Eastern Scholar) at Shanghai
   Institutions of Higher Learning (No. GZ2015005), and the NSF of Jiangxi
   Province under Grant 20161BAB202069.
CR Arsigny V, 2006, MAGN RESON MED, V56, P411, DOI 10.1002/mrm.20965
   Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014
   Bilinski P, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P2140
   Borges PVK, 2013, IEEE T CIRC SYST VID, V23, P1993, DOI 10.1109/TCSVT.2013.2270402
   Brendel W, 2011, IEEE I CONF COMP VIS, P778, DOI 10.1109/ICCV.2011.6126316
   Cheng GC, 2015, INT J INTELL SYST, V30, P99, DOI 10.1002/int.21690
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Dalal N, 2006, LECT NOTES COMPUT SC, V3952, P428, DOI 10.1007/11744047_33
   Das Dawn D, 2016, VISUAL COMPUT, V32, P289, DOI 10.1007/s00371-015-1066-2
   Fan RE, 2008, J MACH LEARN RES, V9, P1871
   Farnebäck G, 2003, LECT NOTES COMPUT SC, V2749, P363, DOI 10.1007/3-540-45103-x_50
   Forstner W, 2003, Geodesy-the Challenge of the 3rd Millennium, P299, DOI [10.1007/978-3-662-05296-9_31, DOI 10.1007/978-3-662-05296-9_31]
   Guo K, 2013, IEEE T IMAGE PROCESS, V22, P2479, DOI 10.1109/TIP.2013.2252622
   Hoai M., 2014, Asian Conference on Computer Vision, P3, DOI DOI 10.1007/978-3-319-16814-21
   Jain M, 2013, PROC CVPR IEEE, P2555, DOI 10.1109/CVPR.2013.330
   Junejo IN, 2014, VISUAL COMPUT, V30, P259, DOI 10.1007/s00371-013-0842-0
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kuehne H, 2011, IEEE I CONF COMP VIS, P2556, DOI 10.1109/ICCV.2011.6126543
   Laptev I, 2005, INT J COMPUT VISION, V64, P107, DOI 10.1007/s11263-005-1838-7
   Laptev I, 2008, PROC CVPR IEEE, P3222, DOI 10.1109/cvpr.2008.4587756
   Lazebnik S., 2006, P IEEE CVF C COMP VI, DOI [DOI 10.1109/CVPR.2006.68, 10.1109/CVPR.2006.68]
   Li Y, 2015, VISUAL COMPUT, V31, P1383, DOI 10.1007/s00371-014-1020-8
   Ma JY, 2014, IEEE T IMAGE PROCESS, V23, P1706, DOI 10.1109/TIP.2014.2307478
   Messing R, 2009, IEEE I CONF COMP VIS, P104, DOI 10.1109/ICCV.2009.5459154
   Niebles JC, 2010, LECT NOTES COMPUT SC, V6312, P392, DOI 10.1007/978-3-642-15552-9_29
   Oneata D, 2013, IEEE I CONF COMP VIS, P1817, DOI 10.1109/ICCV.2013.228
   Pang YW, 2008, IEEE T CIRC SYST VID, V18, P989, DOI 10.1109/TCSVT.2008.924108
   Reddy KK, 2013, MACH VISION APPL, V24, P971, DOI 10.1007/s00138-012-0450-4
   Sánchez J, 2013, INT J COMPUT VISION, V105, P222, DOI 10.1007/s11263-013-0636-x
   SHI JB, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P593, DOI 10.1109/CVPR.1994.323794
   Sun J, 2010, IEEE INT CON MULTI, P322, DOI 10.1109/ICME.2010.5583046
   Truong A., VIS COMPUT, V32, P83
   Tuzel O, 2006, LECT NOTES COMPUT SC, V3952, P589
   Vishwakarma S, 2013, VISUAL COMPUT, V29, P983, DOI 10.1007/s00371-012-0752-6
   Wang H, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P1175, DOI 10.1145/2733373.2806310
   Wang H, 2016, INT J COMPUT VISION, V119, P219, DOI 10.1007/s11263-015-0846-5
   Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441
   Wang H, 2013, INT J COMPUT VISION, V103, P60, DOI 10.1007/s11263-012-0594-8
   Willems G, 2008, LECT NOTES COMPUT SC, V5303, P650, DOI 10.1007/978-3-540-88688-4_48
   Wu JZ, 2014, VISUAL COMPUT, V30, P1395, DOI 10.1007/s00371-013-0899-9
   Zhou LY, 2014, VISUAL COMPUT, V30, P845, DOI 10.1007/s00371-014-0957-y
NR 41
TC 23
Z9 23
U1 0
U2 8
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2018
VL 34
IS 3
BP 391
EP 403
DI 10.1007/s00371-016-1345-6
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FW2CZ
UT WOS:000425110900007
DA 2024-07-18
ER

PT J
AU Shen, C
   Wang, D
   Tang, SM
   Cao, HL
   Liu, J
AF Shen, Chong
   Wang, Ding
   Tang, Shuming
   Cao, Huiliang
   Liu, Jun
TI Hybrid image noise reduction algorithm based on genetic ant colony and
   PCNN
SO VISUAL COMPUTER
LA English
DT Article
DE Image denoising; PCNN; Genetic algorithm; Ant colony algorithm
ID COUPLED NEURAL-NETWORK; SELECTION
AB Pulse Coupled Neural Network (PCNN) has gained widespread attention as a nonlinear filtering technology in reducing the noise while keeping the details of images well, but how to determine the proper parameters for PCNN is a big challenge. In this paper, a method that can optimize the parameters of PCNN by combining the genetic algorithm (GA) and ant colony algorithm is proposed, which named as GACA, and the optimized procedure is named as GACA-PCNN. Firstly, the noisy image is filtered by median filter in the proposed GACA-PCNN method; then, the noisy image is filtered by GACA-PCNN constantly and the median filtering image is used as a reference image; finally, a set of parameters of PCNN can be automatically estimated by GACA, and the pretty effective denoising image will be obtained. Experimental results indicate that GACA-PCNN has a better performance on PSNR (peak signal noise rate) and a stronger capacity of preserving the details than previous denoising techniques.
C1 [Shen, Chong; Wang, Ding; Cao, Huiliang; Liu, Jun] North Univ China, Sch Instrument & Elect, 3 Xueyuan Rd, Taiyuan 030051, Shanxi, Peoples R China.
   [Shen, Chong; Cao, Huiliang; Liu, Jun] North Univ China, Natl Key Lab Elect Measurement Technol, Key Lab Instrumentat Sci & Dynam Measurement, Minist Educ,Sch Instrument & Elect, 3 Xueyuan Rd, Taiyuan 030051, Shanxi, Peoples R China.
   [Tang, Shuming] Univ Chinese Acad Sci, Inst Automat, High Tech Innovat Engn Ctr, 95 Zhongguancun East Rd, Beijing 100190, Peoples R China.
C3 North University of China; North University of China; Chinese Academy of
   Sciences; University of Chinese Academy of Sciences, CAS
RP Liu, J (corresponding author), North Univ China, Sch Instrument & Elect, 3 Xueyuan Rd, Taiyuan 030051, Shanxi, Peoples R China.; Liu, J (corresponding author), North Univ China, Natl Key Lab Elect Measurement Technol, Key Lab Instrumentat Sci & Dynam Measurement, Minist Educ,Sch Instrument & Elect, 3 Xueyuan Rd, Taiyuan 030051, Shanxi, Peoples R China.
EM liuj@nuc.edu.cn
FU National Natural Science Foundation [61603353]; Research Project -
   Shanxi Scholarship Council of China [2015-082]; College Funding of North
   University of China [110246]
FX This work was supported in part by the National Natural Science
   Foundation (61603353), the Research Project Supported by Shanxi
   Scholarship Council of China (2015-082), the College Funding of North
   University of China (110246).
CR [Anonymous], PRENTICE HALL INT
   Bergen S, 2012, VISUAL COMPUT, V28, P35, DOI 10.1007/s00371-011-0597-4
   Bhadouria VS, 2014, SIGNAL IMAGE VIDEO P, V8, P71, DOI 10.1007/s11760-013-0487-5
   BURGESS A, 1985, J OPT SOC AM A, V2, P1424, DOI 10.1364/JOSAA.2.001424
   Cao CH, 2006, PROCEEDINGS OF THE FIFTH IEEE INTERNATIONAL CONFERENCE ON COGNITIVE INFORMATICS, VOLS 1 AND 2, P101
   Chaari I., 2012, 2012 IEEE C EVOLUTIO, P1
   Chao K, 2008, 2008 INTERNATIONAL CONFERENCE ON MICROWAVE AND MILLIMETER WAVE TECHNOLOGY PROCEEDINGS, VOLS 1-4, P597
   Garnett R, 2005, IEEE T IMAGE PROCESS, V14, P1747, DOI 10.1109/TIP.2005.857261
   Halder A, 2013, PATTERN RECOGN, V46, P2239, DOI 10.1016/j.patcog.2013.01.002
   Hong Y, 2007, IEEE C EVOL COMPUTAT, P445, DOI 10.1109/CEC.2007.4424505
   Hosemann P, 2011, EXP MECH, V51, P1095, DOI 10.1007/s11340-010-9419-2
   Ji LP, 2008, NEUROCOMPUTING, V71, P2986, DOI 10.1016/j.neucom.2007.04.015
   Ji LP, 2008, NEURAL COMPUT APPL, V17, P255, DOI 10.1007/s00521-007-0119-5
   Johnson JL, 1999, IEEE T NEURAL NETWOR, V10, P480, DOI 10.1109/72.761706
   KOZEK T, 1993, IEEE T CIRCUITS-I, V40, P392, DOI 10.1109/81.238343
   Li HJ, 2016, PATTERN RECOGN, V49, P237, DOI 10.1016/j.patcog.2015.05.028
   Lu CT, 2012, PATTERN RECOGN LETT, V33, P1287, DOI 10.1016/j.patrec.2012.03.025
   Ma YD, 2003, PROCEEDINGS OF 2003 INTERNATIONAL CONFERENCE ON NEURAL NETWORKS & SIGNAL PROCESSING, PROCEEDINGS, VOLS 1 AND 2, P149
   Ma Y, 2007, ICSPC: 2007 IEEE INTERNATIONAL CONFERENCE ON SIGNAL PROCESSING AND COMMUNICATIONS, VOLS 1-3, PROCEEDINGS, P1499
   Ma Yide, 2004, Sheng Wu Yi Xue Gong Cheng Xue Za Zhi, V21, P1019
   Mi N, 2015, INT J GEOGR INF SCI, V29, P2174, DOI 10.1080/13658816.2015.1070411
   Pang SC, 2015, J COMPUT THEOR NANOS, V12, P1440, DOI 10.1166/jctn.2015.3910
   Ritenour E., 1984, IEEE International Conference on Acoustics, Speech, and Signal Processing, ICASSP'84, V9, P251
   Ryu J, 2010, IEICE T INF SYST, VE93D, P359, DOI 10.1587/transinf.E93.D.359
   Tabakhi S, 2015, NEUROCOMPUTING, V168, P1024, DOI 10.1016/j.neucom.2015.05.022
   Tabakhi S, 2015, PATTERN RECOGN, V48, P2798, DOI 10.1016/j.patcog.2015.03.020
   Wang J, 2015, PATTERN RECOGN, V48, P3135, DOI 10.1016/j.patcog.2015.01.024
   Wang P, 2016, INFORM SCIENCES, V326, P172, DOI 10.1016/j.ins.2015.07.006
   Yayun C, 2014, J CHIN AGR MECH, V34, P246
   Zhang D, 2011, IEEJ T ELECTR ELECTR, V6, P474, DOI 10.1002/tee.20684
   Zhang D, 2010, MATH COMPUT MODEL, V52, P2085, DOI 10.1016/j.mcm.2010.06.016
   Zhu Z, 2015, J ELECTRON IMAGING, V24, DOI 10.1117/1.JEI.24.5.053008
NR 32
TC 16
Z9 19
U1 1
U2 62
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2017
VL 33
IS 11
BP 1373
EP 1384
DI 10.1007/s00371-016-1325-x
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FJ0TP
UT WOS:000412423100002
DA 2024-07-18
ER

PT J
AU Yi, ZL
   Li, Y
   Ji, SY
   Gong, ML
AF Yi, Zili
   Li, Yang
   Ji, Songyuan
   Gong, Minglun
TI Artistic stylization of face photos based on a single exemplar
SO VISUAL COMPUTER
LA English
DT Article
DE Face stylization; Non-photorealistic rendering; Texture synthesis
ID TEXTURE SYNTHESIS; IMAGE
AB In this paper, we propose a unified framework for fully automatic face photo stylization based on a single style exemplar. Constrained by the "single-exemplar" condition, where the numbers and varieties of patch samples are limited, we introduce flexibility in sample selection while preserving identity and content of the input photo. Based on the observation that many styles are characterized by unique color selections and texture patterns, we employ a two-phase procedure. The first phase searches a dense and semantic-aware correspondence between the input and the exemplar images, so that colors in the exemplar can be transferred to the input. The second phase conducts edge-preserving texture transfer, which preserves edges and contours of the input and mimics the textures of the exemplar at multiple scales. Experimental results demonstrate compelling visual effects and notable improvements over other state-of-the-art methods which are adapted for the same task.
C1 [Yi, Zili; Li, Yang; Ji, Songyuan; Gong, Minglun] Mem Univ Newfoundland, Dept Comp Sci, St John, NF A1B 3X5, Canada.
C3 Memorial University Newfoundland
RP Gong, ML (corresponding author), Mem Univ Newfoundland, Dept Comp Sci, St John, NF A1B 3X5, Canada.
EM gong@mun.ca
RI Zili, Yi/AAS-7855-2020; Gong, Minglun/AAU-3103-2020
OI Zili, Yi/0000-0003-4854-2725; Gong, Minglun/0000-0001-5820-5381
CR [Anonymous], 2005, Computational Aesthetics in Graphics, Visualization and Imaging, DOI [DOI 10.2312/COMPAESTH/COMPAESTH05/111-122, 10.2312/COMPAESTH/COMPAESTH05/111-122]
   Barnes C., 2010, LECT NOTES COMPUT SC, V6313, P29
   BEIER T, 1992, COMP GRAPH, V26, P35, DOI 10.1145/142920.134003
   Choi H.C., 2015, COMPUT INTEL NEUROSC, V501
   Cohen-Or D, 2006, ACM T GRAPHIC, V25, P624, DOI 10.1145/1141911.1141933
   Efros A. A., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1033, DOI 10.1109/ICCV.1999.790383
   Efros AA, 2001, COMP GRAPH, P341, DOI 10.1145/383259.383296
   Gooch B., 2002, 2 INT S NONPHOTOREAL, P83
   HaCohen Y, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461997
   Henstock PV, 1996, IEEE T IMAGE PROCESS, V5, P784, DOI 10.1109/83.499917
   Kagaya M, 2011, IEEE T VIS COMPUT GR, V17, P74, DOI 10.1109/TVCG.2010.25
   Kaspar A, 2015, COMPUT GRAPH FORUM, V34, P349, DOI 10.1111/cgf.12565
   Kwatra V, 2003, ACM T GRAPHIC, V22, P277, DOI 10.1145/882262.882264
   Lewis J.-P., 1984, Computers & Graphics, V18, P245
   Li H, 2011, P ACM SIGGRAPH EUR S, V2011, P125
   Li H, 2011, PROCEEDINGS OF THE 4TH CONFERENCE ON SYSTEMS SCIENCE, MANAGEMENT SCIENCE AND SYSTEMS DYNAMICS, SSMSSD10, VOL 2, P127
   Li HL, 2011, IEEE T MULTIMEDIA, V13, P1230, DOI 10.1109/TMM.2011.2168814
   Li H, 2011, GRAPP 2011: PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON COMPUTER GRAPHICS THEORY AND APPLICATIONS, P166
   Li H, 2010, COMPUT GRAPH FORUM, V29, P273, DOI 10.1111/j.1467-8659.2009.01596.x
   Liang L, 2001, ACM T GRAPHIC, V20, P127, DOI 10.1145/501786.501787
   Lu Cewu., 2012, Proc. NPAR, P65
   Pérez P, 2003, ACM T GRAPHIC, V22, P313, DOI 10.1145/882262.882269
   Sen P, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2167076.2167083
   Shih YC, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601137
   Wang XG, 2009, IEEE T PATTERN ANAL, V31, P1955, DOI 10.1109/TPAMI.2008.222
   Wexler Y, 2007, IEEE T PATTERN ANAL, V29, P463, DOI 10.1109/TPAMI.2007.60
   Zhang W, 2010, LECT NOTES COMPUT SC, V6316, P420, DOI 10.1007/978-3-642-15567-3_31
   Zhang Y., 2014, DATA DRIVEN FACE CAR
   Zhu XX, 2012, PROC CVPR IEEE, P2879, DOI 10.1109/CVPR.2012.6248014
NR 29
TC 3
Z9 3
U1 0
U2 5
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2017
VL 33
IS 11
BP 1443
EP 1452
DI 10.1007/s00371-016-1290-4
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FJ0TP
UT WOS:000412423100007
DA 2024-07-18
ER

PT J
AU Kita, N
   Miyata, K
AF Kita, Naoki
   Miyata, Kazunori
TI Multi-class anisotropic blue noise sampling for discrete element pattern
   generation
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 33rd Conference on Computer Graphics International (CGI)
CY JUN 28-JUL 01, 2016
CL Heraklion, GREECE
SP Fdn Res Technol
DE Blue noise sampling; Discrete element texture; Element distribution;
   Pattern generation
AB We present an element placement method for generating patterns containing "discrete elements". By extending various blue noise sampling methods, we propose a visually uniform distribution of multi-class elements. Our method also supports tileable aperiodic distribution. Instead of actual elements, for fast calculation, we use a circular or elliptic disk as a proxy of an element when checking conflicts with nearby elements during the distribution process. The nature of our results is comparable to swatches in books, which shows that our method is capable of generating visually appealing swatches for a set of elements. The user study showed that our method outperformed state-of-the-art discrete element texture synthesis approaches in terms of pattern visual quality.
C1 [Kita, Naoki; Miyata, Kazunori] Japan Adv Inst Sci & Technol, Nomi, Japan.
C3 Japan Advanced Institute of Science & Technology (JAIST)
RP Kita, N (corresponding author), Japan Adv Inst Sci & Technol, Nomi, Japan.
EM naoki-kt@jaist.ac.jp; miyata@jaist.ac.jp
RI Kita, Naoki/G-8880-2019
OI Miyata, Kazunori/0000-0002-1582-0058
CR AlMeraj Zainab., 2013, Proceedings of the Symposium on Computational Aesthetics, CAE'13, P15
   [Anonymous], 1987, P 14 ANN C COMP GRAP, DOI [DOI 10.1145/37401.37410, DOI 10.1145/37402.37410]
   [Anonymous], 2009, Proceedings of the 7th International Symposium on Non-Photorealistic Animation and Rendering (NPAR), DOI DOI 10.1145/1572614.1572623
   [Anonymous], 2010, ACM T GRAPHIC, DOI DOI 10.1145/1778765.1778816
   [Anonymous], 2007, BNN PATTERN BOOK SER
   Barla P., 2006, COMP FOR P EUR 2006, V25
   Bridson R., 2007, Fast Poisson disk sampling in arbitrary dimensions, V10, P1, DOI DOI 10.1145/1278780.1278807
   Chen G., 2008, ACM SIGGRAPH 2008 PA, p103:1, DOI DOI 10.1145/1399504.1360702
   Chen ZG, 2012, IEEE T VIS COMPUT GR, V18, P1784, DOI 10.1109/TVCG.2012.94
   COOK RL, 1986, ACM T GRAPHIC, V5, P51, DOI 10.1145/7529.8927
   Dalal K., 2006, Proceedings of the 4th international symposium on Nonphotorealistic animation and rendering, P71, DOI 10.1145/1124728.1124741
   Hiller S, 2003, COMPUT GRAPH FORUM, V22, P515, DOI 10.1111/1467-8659.00699
   Ijiri T, 2008, COMPUT GRAPH FORUM, V27, P429, DOI 10.1111/j.1467-8659.2008.01140.x
   Jiang M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818102
   Kita N., 2011, SIGGRAPH ASIA 2011 S, DOI [10.1145/2077378.2077422, DOI 10.1145/2077378.2077422]
   Labelle Francois., 2003, Symp. Comp. Geom, P191, DOI DOI 10.1145/777792.777822
   Lagae A, 2005, ACM T GRAPHIC, V24, P1442, DOI 10.1145/1095878.1095888
   Lagae A, 2008, COMPUT GRAPH FORUM, V27, P114, DOI 10.1111/j.1467-8659.2007.01100.x
   Landes P.E., 2013, COMPUT GR FORUM P EG, V32
   Li HW, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866189
   Ma CY, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964957
   Reinert B, 2016, COMPUT GRAPH FORUM, V35, P285, DOI 10.1111/cgf.12725
   Reinert B, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508409
   Sakurai K., 2012, NICOGRAPH INT, P68
   Schmaltz C, 2012, COMPUT GRAPH FORUM, V31, P1924, DOI 10.1111/j.1467-8659.2012.03072.x
   Wei L.Y., 2009, EUROGRAPHICS 09 STAT
   Yan DM, 2015, J COMPUT SCI TECH-CH, V30, P439, DOI 10.1007/s11390-015-1535-0
   Zhang E, 2006, ACM T GRAPHIC, V25, P1294, DOI 10.1145/1183287.1183290
NR 28
TC 1
Z9 1
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2016
VL 32
IS 6-8
BP 1035
EP 1044
DI 10.1007/s00371-016-1248-6
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA DP8ET
UT WOS:000378731600034
DA 2024-07-18
ER

PT J
AU Morgenroth, D
   Weiskopf, D
   Eberhardt, B
AF Morgenroth, D.
   Weiskopf, D.
   Eberhardt, B.
TI Direct raytracing of a closed-form fluid meniscus
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 33rd Conference on Computer Graphics International (CGI)
CY JUN 28-JUL 01, 2016
CL Heraklion, GREECE
SP Fdn Res Technol
DE Raytracing; Implicit surfaces; Fluid meniscus; Smoothed particle
   hydrodynamics; Displacement mapping
ID DROPS; MODEL
AB We present a direct raytracing method for implicitly described fluid surfaces that takes into account the effects of capillary solid coupling at the boundaries. The method is independent of the underlying fluid simulation method and solely based on distance fields. We make use of the closed-form solution of the meniscus shape at the fluid interface to achieve the effect of surface tension exerted by the solid object. The shape of the liquid at these boundaries is influenced by various physical properties such as the force of gravity and the affinity between the liquid and the solid material. We generate contact angles at the boundaries without the need for computationally intensive small-scale simulation. At render time, we combine the closed-form solution for a small-scale effect with the numerical solution of a large-scale simulation. Our method is applicable for any implicit representation of the fluid surface and does not require an explicit extraction of the surface geometry. Therefore, it is especially useful for particle-based simulations. Furthermore, the solution is guaranteed to yield the correct contact angle and, for certain scenarios, it delivers the entirely correct solution throughout the interface; even in general scenarios, it yields plausible results. As for an example, we implemented and tested the proposed method in the setting of a smoothed particle hydrodynamics (SPH) fluid simulation.
C1 [Morgenroth, D.] Stuttgart Media Univ, Kooperat Promotionskolleg Digital Media, Stuttgart, Germany.
   [Eberhardt, B.] Stuttgart Media Univ, Stuttgart, Germany.
   [Weiskopf, D.] Univ Stuttgart, VISUS, Stuttgart, Germany.
C3 University of Stuttgart; University of Stuttgart; University of
   Stuttgart
RP Morgenroth, D (corresponding author), Stuttgart Media Univ, Kooperat Promotionskolleg Digital Media, Stuttgart, Germany.
EM dieter.morgenroth@web.de
OI Eberhardt, Bernhard/0000-0001-6428-4610
CR Adams B, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1276377.1276437, 10.1145/1239451.1239499]
   [Anonymous], 2005, Physics of Continuous Matter
   [Anonymous], 2013, ACM T GRAPHICS TOG, DOI DOI 10.1145/2421636.2421641
   Baerentzen J.A., 2002, IMMTR200221 TU DENM
   Bourque E., 2006, P 2 EUR C NAT PHEN, P33
   Clausen P, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2451236.2451243
   Deserno M., 2004, SHAPE STRAIGHT FLUID
   Fournier P, 1998, GRAPHICS INTERFACE '98 - PROCEEDINGS, P133
   Fraedrich R, 2010, IEEE T VIS COMPUT GR, V16, P1533, DOI 10.1109/TVCG.2010.148
   Gourmel O, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2451236.2451238
   Gourmel O, 2010, COMPUT GRAPH FORUM, V29, P281, DOI 10.1111/j.1467-8659.2009.01597.x
   Hart J.C., 1993, Siggraph 93 Course Notes: Design, Visualization and Animation of Implicit Surfaces, P1
   Huber M, 2015, COMPUT GRAPH FORUM, V34, P14, DOI 10.1111/cgf.12455
   Jung Y., 2009, Proceedings of the 14th international conference on 3D web technology, P51, DOI [10.1145/1559764.1559772, DOI 10.1145/1559764.1559772]
   Kaneda K, 1999, J VISUAL COMP ANIMAT, V10, P15, DOI 10.1002/(SICI)1099-1778(199901/03)10:1<15::AID-VIS192>3.0.CO;2-P
   Kaneda K., 1993, Models and Techniques in Computer Animation, P177
   Liu YQ, 2005, VISUAL COMPUT, V21, P727, DOI 10.1007/s00371-005-0314-2
   Lock JA, 2003, APPL OPTICS, V42, P418, DOI 10.1364/AO.42.000418
   MONAGHAN JJ, 1992, ANNU REV ASTRON ASTR, V30, P543, DOI 10.1146/annurev.aa.30.090192.002551
   Museth K, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2487228.2487235
   Nakata N, 2012, WSCG'2012, CONFERENCE PROCEEDINGS, PTS I & II, P95
   Singh JM, 2010, IEEE T VIS COMPUT GR, V16, P261, DOI 10.1109/TVCG.2009.41
   Solenthaler B, 2007, COMPUT ANIMAT VIRT W, V18, P69, DOI 10.1002/cav.162
   Spivak M, 1999, COMPREHENSIVE INTRO, V4
   Stuppacher I, 2007, CENTR EUR SEM COMP G
   Szirmay-Kalos L, 2008, COMPUT GRAPH FORUM, V27, P1567, DOI 10.1111/j.1467-8659.2007.01108.x
   TAKENAKA S., 2008, The 23rd International Techmcal Conference on Circuits/Systems, Computers and Communications (ITC-CSCC), P13
   Wang HM, 2005, ACM T GRAPHIC, V24, P921, DOI 10.1145/1073204.1073284
   Young T., 1805, Trans Roy. Soc, V95, P65, DOI [10.1098/rstl.1805.0005, DOI 10.1098/RSTL.1805.0005]
   Yu YJ, 1999, COMPUT GRAPH-UK, V23, P213, DOI 10.1016/S0097-8493(99)00031-X
   Yuan Y., 2013, SURFACE SCI TECHNIQU, P3, DOI [DOI 10.1007/978-3-642-34243-1_1, 10.1007/978-3-642-34243-1]
   Zhang YZ, 2012, IEEE T VIS COMPUT GR, V18, P1281, DOI 10.1109/TVCG.2011.141
   Zhao HK, 1998, J COMPUT PHYS, V143, P495, DOI 10.1006/jcph.1997.5810
   Zhu YN, 2005, ACM T GRAPHIC, V24, P965, DOI 10.1145/1073204.1073298
NR 34
TC 3
Z9 3
U1 1
U2 8
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2016
VL 32
IS 6-8
BP 791
EP 800
DI 10.1007/s00371-016-1258-4
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA DP8ET
UT WOS:000378731600012
DA 2024-07-18
ER

PT J
AU Zhou, JY
   Tong, X
   Liu, ZC
   Guo, BN
AF Zhou, Jingyong
   Tong, Xin
   Liu, Zicheng
   Guo, Baining
TI 3D cartoon face generation by local deformation mapping
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 33rd Conference on Computer Graphics International (CGI)
CY JUN 28-JUL 01, 2016
CL Heraklion, GREECE
SP Fdn Res Technol
DE 3D cartoon generation; Local deformation mapping; Data; driven method
ID CARICATURE
AB We present a data-driven method for automatically generating a 3D cartoon of a real 3D face. Given a sparse set of 3D real faces and their corresponding cartoon faces modeled by an artist, our method models the face in each subspace as the deformation of its nearby exemplars and learn a mapping between the deformations defined by the real faces and their cartoon counterparts. To reduce the exemplars needed for learning, we regress a collection of linear mappings defined locally in both face geometry and identity spaces and develop a progressive scheme for users to gradually add new exemplars for training. At runtime, our method first finds the nearby exemplars of an input real face and then constructs the result cartoon face from the corresponding cartoon faces of the nearby real face exemplars and the local deformations mapped from the real face subspace. Our method greatly simplifies the cartoon generation process by learning artistic styles from a sparse set of exemplars. We validate the efficiency and effectiveness of our method by applying it to faces of different facial features. Results demonstrate that our method not only preserves the artistic style of the exemplars, but also keeps the unique facial geometric features of different identities.
C1 [Zhou, Jingyong] Tsinghua Univ, Comp Sci & Technol Inst Adv Study, Beijing, Peoples R China.
   [Guo, Baining] Tsinghua Univ, Beijing, Peoples R China.
   [Tong, Xin] Microsoft Res, Internet Graph Grp, Beijing, Peoples R China.
   [Guo, Baining] Microsoft Res, Beijing, Peoples R China.
   [Liu, Zicheng] Microsoft Res, Redmond, WA USA.
C3 Tsinghua University; Tsinghua University; Microsoft; Microsoft;
   Microsoft
RP Zhou, JY (corresponding author), Tsinghua Univ, Comp Sci & Technol Inst Adv Study, Beijing, Peoples R China.
EM zh-y10@mails.tsinghua.edu.cn; xtong@microsoft.com; zliu@microsoft.com;
   bainguo@microsoft.com
OI Tong, Xin/0000-0001-8788-2453; Zhou, Jingyong/0000-0002-3756-1851
CR Akleman E., 1997, ACM SIGGRAPH, DOI DOI 10.1145/259081.259231
   Akleman E., 2004, ACM SIGGRAPH 2004 SK, DOI 10.1145/1186223.1186299
   Baran I, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531342
   Bradley D., 2010, ACM T GRAPHIC, V29, P3
   Cao C, 2014, IEEE T VIS COMPUT GR, V20, P413, DOI 10.1109/TVCG.2013.249
   Chen H., 2001, COMP VIS 2001 ICCV 2, V2, P433
   Clarke L, 2011, IEEE T VIS COMPUT GR, V17, P808, DOI 10.1109/TVCG.2010.76
   Fujiwara T., 1999, Second International Conference on 3-D Digital Imaging and Modeling (Cat. No.PR00062), P490, DOI 10.1109/IM.1999.805381
   Fujiwara T, 2001, THIRD INTERNATIONAL CONFERENCE ON 3-D DIGITAL IMAGING AND MODELING, PROCEEDINGS, P385, DOI 10.1109/IM.2001.924483
   KAMADA T, 1989, INFORM PROCESS LETT, V31, P7, DOI 10.1016/0020-0190(89)90102-6
   Kondo T, 1997, INTERNATIONAL CONFERENCE ON RECENT ADVANCES IN 3-D DIGITAL IMAGING AND MODELING, PROCEEDINGS, P283, DOI 10.1109/IM.1997.603877
   Li PF, 2008, 2008 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOLS 1-4, P941, DOI 10.1109/ICME.2008.4607591
   Liang L, 2002, 10TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P386, DOI 10.1109/PCCGA.2002.1167882
   Liu JF, 2009, COMPUT GRAPH FORUM, V28, P2104, DOI 10.1111/j.1467-8659.2009.01418.x
   Liu SK, 2013, VISUAL COMPUT, V29, P1135, DOI 10.1007/s00371-012-0756-2
   Mo Z., 2004, ACM SIGGRAPH 2004 SK, P57
   Sadimon SB, 2010, 2010 INTERNATIONAL CONFERENCE ON CYBERWORLDS (CW 2010), P383, DOI 10.1109/CW.2010.33
   Sucontphunt Tanasai, 2014, Smart Graphics. 12th International Symposium (SG 2014). Proceedings: LNCS 8698, P154, DOI 10.1007/978-3-319-11650-1_14
   Sucontphunt T., 2014, INT J COMPUT GAMES T, V2014, P7
   Sumner RW, 2004, ACM T GRAPHIC, V23, P399, DOI 10.1145/1015706.1015736
   Wang SF, 2010, COMPUT GRAPH FORUM, V29, P2161, DOI 10.1111/j.1467-8659.2010.01804.x
   Xie J., 2009, Proceedings of the ACM International Conference on Multimedia, DOI 10.1145/1631272.1631403
   Chen YL, 2006, IEEE SYS MAN CYBERN, P866, DOI 10.1109/ICSMC.2006.384498
   Zhang Mingming., 2010, P 9 ACM SIGGRAPH C V, P191
NR 24
TC 6
Z9 8
U1 0
U2 7
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2016
VL 32
IS 6-8
BP 717
EP 727
DI 10.1007/s00371-016-1265-5
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA DP8ET
UT WOS:000378731600005
DA 2024-07-18
ER

PT J
AU Gai, M
   Wang, GP
AF Gai, Meng
   Wang, Guoping
TI Artistic Low Poly rendering for images
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Computational Visual Media Conference (CVM)
CY APR 16-17, 2015
CL Tsinghua Univ, Beijing, PEOPLES R CHINA
HO Tsinghua Univ
DE Low poly; Non-photorealistic rendering; Image stylization; Image
   decomposition
ID SIMPLIFICATION; EDGE
AB This paper presents an automatic approach for generating low poly rendering of images, which is particularly popular in the recent art design community. Distinguishing from the traditional image triangulation methods for the sake of compression or vectorization, we propose some critical principles of such Low Poly rendering problem, and simulate the artists creation procedures straightforwardly. To produce the visual effects with clear boundaries, we constrain the vertices along the feature edges extracted from the input image. By employing the Voronoi diagram iteration guided by a feature flow field, the vertices in the result image well reflect the feature structure of the local shape. Moreover, with the salient region detection, we can achieve different mesh densities between the front object and the background. Some special color processing techniques are employed to make our result more artistic. Our method works well on a wide variety of images, no matter raster photographs or artificial images. Experiments show that our approach is able to generate satisfying results similar to the artwork created by professional artists.
C1 [Gai, Meng; Wang, Guoping] Peking Univ, Beijing Engn Res Ctr Virtual Simulat & Visualizat, Beijing 100871, Peoples R China.
   [Gai, Meng; Wang, Guoping] State Key Lab Math Engn & Adv Comp, Wuxi, Peoples R China.
C3 Peking University
RP Gai, M (corresponding author), Peking Univ, Beijing Engn Res Ctr Virtual Simulat & Visualizat, Beijing 100871, Peoples R China.; Gai, M (corresponding author), State Key Lab Math Engn & Adv Comp, Wuxi, Peoples R China.
EM gaimeng@pku.edu.cn; wgp@pku.edu.cn
RI wang, guoping/KQU-3394-2024
FU National Natural Science Foundation of China [61421062, 61170205,
   61232014, 61472010]; National Key Technology Research and Development
   Program of China [2012AA011503]
FX This research was supported by Grant Nos. 61421062, 61170205, 61232014,
   61472010 from National Natural Science Foundation of China. Also was
   supported by Grant No. 2012AA011503 from The National Key Technology
   Research and Development Program of China.
CR Adams MD, 2011, IEEE T IMAGE PROCESS, V20, P2414, DOI 10.1109/TIP.2011.2128336
   Arbeláez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161
   Chen Z., 2014, GRAPHICAL MODELS
   de Goes F, 2011, COMPUT GRAPH FORUM, V30, P1593, DOI 10.1111/j.1467-8659.2011.02033.x
   Demaret L, 2005, MATH VIS, P319, DOI 10.1007/3-540-26808-1_18
   Demaret L, 2006, SIGNAL PROCESS, V86, P1604, DOI 10.1016/j.sigpro.2005.09.003
   Faustino GM, 2005, SIBGRAPI 2005: XVIII BRAZILIAN SYMPOSIUM ON COMPUTER GRAPHICS AND IMAGE PROCESSING, CONFERENCE PROCEEDINGS, P315
   Garland M, 2005, ACM T GRAPHIC, V24, P209, DOI 10.1145/1061347.1061350
   Hausner A, 2001, COMP GRAPH, P573, DOI 10.1145/383259.383327
   Hershberger J., 1994, P 10 ANN S COMP GEOM, P383
   Huang SS, 2014, VISUAL COMPUT, V30, P673, DOI 10.1007/s00371-014-0973-y
   Kim D, 2008, COMPUT GRAPH FORUM, V27, P1209, DOI 10.1111/j.1467-8659.2008.01259.x
   Kyprianidis J., 2012, TAXONOMY ARTISTIC ST
   Lai YK, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531391
   Lecot G., 2006, P EUR S REND EGSR, P349
   Li XY, 2013, IEEE T IMAGE PROCESS, V22, P1915, DOI 10.1109/TIP.2013.2237922
   Liao ZC, 2012, IEEE T VIS COMPUT GR, V18, P1858, DOI 10.1109/TVCG.2012.76
   Rong G., 2006, P 2006 S INTERACTIVE, P109, DOI DOI 10.1145/1111411.1111431
   Sun J, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239462
   Swaminarayan Sriram, 2006, AIPR 06, P28
   Topal C, 2012, J VIS COMMUN IMAGE R, V23, P862, DOI 10.1016/j.jvcir.2012.05.004
   Yang YY, 2003, IEEE T IMAGE PROCESS, V12, P866, DOI 10.1109/TIP.2003.812757
   Zang Y., 2013, VIS COMP, P1
NR 23
TC 6
Z9 7
U1 0
U2 16
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2016
VL 32
IS 4
BP 491
EP 500
DI 10.1007/s00371-015-1082-2
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA DI6YD
UT WOS:000373645200007
DA 2024-07-18
ER

PT J
AU Blache, L
   Loscos, C
   Lucas, L
AF Blache, L.
   Loscos, C.
   Lucas, L.
TI Robust motion flow for mesh tracking of freely moving actors
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 3DOR Workshop
CY APR 06, 2014
CL Strasbourg, FRANCE
DE Multi-view reconstruction; Motion flow; Dynamic mesh; Voxel matching;
   Mesh animation
ID CAPTURE
AB 4D multi-view reconstruction of moving actors has many applications in the entertainment industry and although studios providing such services become more accessible, efforts have to be done in order to improve the underlying technology to produce high-quality 4D contents. In this paper, we present a method to derive a time-evolving surface representation from a sequence of binary volumetric data representing an arbitrary motion in order to introduce coherence in the data. The context is provided by an indoor multi-camera system which performs synchronized video captures from multiple viewpoints in a chroma-key studio. Our input is given by a volumetric silhouette-based reconstruction algorithm that generates a visual hull at each frame of the video sequence. These 3D volumetric models lack temporal coherence, in terms of structure and topology, as each frame is generated independently. This prevents an easy post-production editing with 3D animation tools. Our goal is to transform this input sequence of independent 3D volumes into a single dynamic structure, directly usable in post-production. Our approach is based on a motion estimation procedure. An unsigned distance function on the volumes is used as the main shape descriptor and a 3D surface matching algorithm minimizes the interference between unrelated surface regions. Experimental results, tested on our multi-view datasets, show that our method outperforms other approaches based on optical flow when considering robustness over several frames.
C1 [Blache, L.; Loscos, C.; Lucas, L.] Univ Reims, CReSTIC SIC, Reims, France.
C3 Universite de Reims Champagne-Ardenne
RP Blache, L (corresponding author), Univ Reims, CReSTIC SIC, Reims, France.
EM ludovic.blache@univ-reims.fr
RI Loscos, Celine/F-5973-2017
OI Loscos, Celine/0000-0002-0520-6249
CR Allain B, 2014, LECT NOTES COMPUT SC, V8690, P284, DOI 10.1007/978-3-319-10605-2_19
   [Anonymous], IEEE C COMP VIS PATT
   [Anonymous], INT WORKSH VIS MOD V
   [Anonymous], 2007, Symposium on Geometry Processing
   ANUAR N, 2004, INT WORKSHOP VISION, P63
   BARRON J, 2004, 2004012 TINA MEMO
   Cagniart C, 2010, LECT NOTES COMPUT SC, V6314, P326, DOI 10.1007/978-3-642-15561-1_24
   de Aguiar E, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360697
   Gall J, 2009, PROC CVPR IEEE, P1746, DOI 10.1109/CVPRW.2009.5206755
   Hilton A, 2004, 2ND INTERNATIONAL SYMPOSIUM ON 3D DATA PROCESSING, VISUALIZATION, AND TRANSMISSION, PROCEEDINGS, P357, DOI 10.1109/TDPVT.2004.1335229
   HORN BKP, 1981, ARTIF INTELL, V17, P185, DOI 10.1016/0004-3702(81)90024-2
   LAURENTINI A, 1994, IEEE T PATTERN ANAL, V16, P150, DOI 10.1109/34.273735
   Letouzey A, 2012, PROC CVPR IEEE, P190, DOI 10.1109/CVPR.2012.6247675
   Li H, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618521
   Li H, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2077341.2077343
   Liu YB, 2011, PROC CVPR IEEE, P1249, DOI 10.1109/CVPR.2011.5995424
   Lucas B. D., 1981, P 7 INT JOINT C ART, V81, P674, DOI DOI 10.5555/1623264.1623280
   LUCAS L, 2013, 4 INT C 3D BODY SCAN, P219
   Nobuhara S, 2004, 2ND INTERNATIONAL SYMPOSIUM ON 3D DATA PROCESSING, VISUALIZATION, AND TRANSMISSION, PROCEEDINGS, P566, DOI 10.1109/TDPVT.2004.1335289
   SAITO T, 1994, PATTERN RECOGN, V27, P1551, DOI 10.1016/0031-3203(94)90133-3
   Sorkine O, 2007, S GEOM PROC, V4, P109, DOI [10.1145/1073204.1073323, DOI 10.1145/1073204.1073323]
   Starck J, 2007, IEEE COMPUT GRAPH, V27, P21, DOI 10.1109/MCG.2007.68
   Stark JD, 2007, OCEANS 2007 - EUROPE, VOLS 1-3, P331
   Tevs A, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2159516.2159517
   Tung T, 2010, PROC CVPR IEEE, P1402, DOI 10.1109/CVPR.2010.5539806
   Varanasi K, 2008, LECT NOTES COMPUT SC, V5303, P30, DOI 10.1007/978-3-540-88688-4_3
   Vedula S., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P722, DOI 10.1109/ICCV.1999.790293
   Vlasic D, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360696
   Zheng Q, 2010, COMPUT GRAPH FORUM, V29, P635, DOI 10.1111/j.1467-8659.2009.01633.x
NR 29
TC 2
Z9 2
U1 0
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2016
VL 32
IS 2
BP 205
EP 216
DI 10.1007/s00371-015-1191-y
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA DF9FU
UT WOS:000371667200006
DA 2024-07-18
ER

PT J
AU Fan, X
   Wang, Y
   Gao, RJ
   Luo, ZX
AF Fan, Xin
   Wang, Yi
   Gao, Renjie
   Luo, Zhongxuan
TI Haze editing with natural transmission
SO VISUAL COMPUTER
LA English
DT Article
DE Haze editing; Transmission model; Aerial perspective; Color correction;
   Sky compensation
ID VISUAL SIMULATION; COLOR; VISION
AB Significant efforts have been devoted to haze removal of outdoor scenic images and haze simulation of virtual scenes. However, fewworks focus on editing (increasing and decreasing) haze effects, which are common outdoor photography on real world images. In this paper, we present a dark channel prior-based transmission model that can explicitly formulates aerial perspective implying human perception on natural haze. We introduce maximum visibility as a parameter into the transmission model, so that we are able to naturally edit the amount of haze in an image by tuning this parameter with a physical interpretation. Additionally, we derive color correction and sky compensation from the transmission model, which improves the image quality for haze editing. Experimental results demonstrate the ability of the proposed method to generate images with various amounts of haze in a natural and efficient manner. Comparisons with the traditional algorithms on haze removal show the performance of the proposed algorithm in terms of two objective metrics that evaluate the visibility and fidelity of the restored images.
C1 [Fan, Xin; Wang, Yi; Gao, Renjie; Luo, Zhongxuan] Dalian Univ Technol, Sch Software, Dalian, Peoples R China.
   [Luo, Zhongxuan] Dalian Univ Technol, Sch Math Sci, Dalian, Peoples R China.
C3 Dalian University of Technology; Dalian University of Technology
RP Fan, X (corresponding author), Dalian Univ Technol, Sch Software, Dalian, Peoples R China.
EM xin.fan@ieee.org
FU Natural Science Foundation of China (NSFC) [61272371, 61402072,
   61033012, 61328206]; program for New Century Excellent Talents
   [NCET-11-0048]
FX Xin Fan and Renjie Gao are supported by the Natural Science Foundation
   of China (NSFC) under grant Nos. 61272371 and the program for New
   Century Excellent Talents (NCET-11-0048). Yi Wang is supported by the
   NSFC under grant Nos. 61402072. Zhongxuan Luo is supported by the NSFC
   under grant Nos. 61033012 and 61328206. A short version of this paper is
   previously published at IEEE International Conference on Image
   Processing 2012 (ICIP' 12).
CR [Anonymous], 2006, CVPR
   [Anonymous], IEEE T PATTERN ANAL
   Choi LK, 2014, PROC SPIE, V9014, DOI 10.1117/12.2036477
   Fattal R., 2008, ACM T GRAPHIC, V27, P7
   Fedkiw R, 2001, COMP GRAPH, P15, DOI 10.1145/383259.383260
   Ferzli R., 2009, JNB SHARPNESS METRIC
   Ferzli R, 2009, IEEE T IMAGE PROCESS, V18, P717, DOI 10.1109/TIP.2008.2011760
   Gibson KB, 2012, IEEE T IMAGE PROCESS, V21, P662, DOI 10.1109/TIP.2011.2166968
   Goldiez B, 1999, IEEE COMPUT GRAPH, V19, P11, DOI 10.1109/38.736463
   Hautiere Nicolas, 2008, Image Analysis & Stereology, V27, P87, DOI 10.5566/ias.v27.p87-95
   Hautière N, 2008, IEEE T INSTRUM MEAS, V57, P2218, DOI 10.1109/TIM.2008.922096
   He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168
   Huang H, 2011, VISUAL COMPUT, V27, P861, DOI 10.1007/s00371-011-0596-5
   Kil TH, 2013, IEEE IMAGE PROC, P882, DOI 10.1109/ICIP.2013.6738182
   Kopf J, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409069
   Mather G., 2011, SENSATION PERCEPTION
   Moorthy A. K, 2010, BIQI SOFTWARE RELEAS
   Moorthy AK, 2010, IEEE SIGNAL PROC LET, V17, P513, DOI 10.1109/LSP.2010.2043888
   Narasimhan S. G., 2003, Interactive (de) weathering of an image using physical models, V6, P1
   Narasimhan SG, 2003, IEEE T PATTERN ANAL, V25, P713, DOI 10.1109/TPAMI.2003.1201821
   Narasimhan SG, 2002, INT J COMPUT VISION, V48, P233, DOI 10.1023/A:1016328200723
   Oakley JP, 2007, IEEE T IMAGE PROCESS, V16, P511, DOI 10.1109/TIP.2006.887736
   Oakley JP, 1998, IEEE T IMAGE PROCESS, V7, P167, DOI 10.1109/83.660994
   Pei SC, 2012, IEEE IMAGE PROC, P957, DOI 10.1109/ICIP.2012.6467020
   Polatkan G, 2015, IEEE T PATTERN ANAL, V37, P346, DOI 10.1109/TPAMI.2014.2321404
   Preetham A. J., 2003, ACM SIGGRAPH 2003 CO
   Rubinstein M, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866186
   Shwartz S., 2006, 2006 IEEE COMP SOC C, V2, P1984, DOI DOI 10.1109/CVPR.2006.71
   Smith GS, 2005, AM J PHYS, V73, P590, DOI 10.1119/1.1858479
   Tan RT, 2008, PROC CVPR IEEE, P2347, DOI 10.1109/cvpr.2008.4587643
   Tarel JP, 2009, IEEE I CONF COMP VIS, P2201, DOI 10.1109/ICCV.2009.5459251
   Wang BY, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866172
   Xiao CX, 2012, VISUAL COMPUT, V28, P713, DOI 10.1007/s00371-012-0679-y
   Xingyong Lv, 2010, 2010 Pacific Graphics (PG). Proceedings 18th Pacific Conference on Computer Graphics and Applications, P62, DOI 10.1109/PacificGraphics.2010.16
   Zhou K, 2007, PACIFIC GRAPHICS 2007: 15TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, P116, DOI 10.1109/PG.2007.48
NR 35
TC 7
Z9 8
U1 0
U2 6
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2016
VL 32
IS 1
BP 137
EP 147
DI 10.1007/s00371-015-1083-1
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DF9FQ
UT WOS:000371666800012
DA 2024-07-18
ER

PT J
AU Zhang, HL
   Xu, M
   Zhuo, LY
   Havyarimana, V
AF Zhang, Hanling
   Xu, Min
   Zhuo, Liyuan
   Havyarimana, Vincent
TI A novel optimization framework for salient object detection
SO VISUAL COMPUTER
LA English
DT Article
DE Visual saliency; Boundary contrast; Smoothness prior; Optimization
   framework
AB Visual saliency aims to locate the noticeable regions or objects in an image. In this paper, a coarse-to-fine measure is developed to model visual saliency. In the proposed approach, we firstly use the contrast and center bias to generate an initial prior map. Then, we weight the initial prior map with boundary contrast to obtain the coarse saliency map. Finally, a novel optimization framework that combines the coarse saliency map, the boundary contrast and the smoothness prior is introduced with the intention of refining the map. Experiments on three public datasets demonstrate the effectiveness of the proposed method.
C1 [Zhang, Hanling; Xu, Min; Zhuo, Liyuan; Havyarimana, Vincent] Hunan Univ, Coll Informat Sci & Engn, Changsha 410000, Hunan, Peoples R China.
C3 Hunan University
RP Xu, M (corresponding author), Hunan Univ, Coll Informat Sci & Engn, Changsha 410000, Hunan, Peoples R China.
EM jt_hlzhang@hnu.edu.cn; xumin19891030@163.com; charleer@foxmail.com;
   havincent14@hnu.edu.cn
RI Havyarimana, Vincent/ABH-1701-2020
FU Key Science and Technology Planning Project of Hunan province, China
   [2014GK2007]; Natural Science Foundation of Hunan Province, China
   [2015JJ4014]
FX This work was supported by the Key Science and Technology Planning
   Project of Hunan province, China (Grant No. 2014GK2007) and the Natural
   Science Foundation of Hunan Province, China (Grant No. 2015JJ4014).
CR Achanta R., 2010, EPFL Technical Report 149300, V6, P15
   Achanta R, 2008, LECT NOTES COMPUT SC, V5008, P66
   Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   Alexe B, 2012, IEEE T PATTERN ANAL, V34, P2189, DOI 10.1109/TPAMI.2012.28
   Alpert S., 2007, CVPR
   [Anonymous], IEEE TIP
   Borji A, 2012, LECT NOTES COMPUT SC, V7573, P414, DOI 10.1007/978-3-642-33709-3_30
   Chen TC, 2009, PROC EUR SOLID-STATE, P1
   Cheng MM, 2014, PROC CVPR IEEE, P3286, DOI 10.1109/CVPR.2014.414
   Cheng MM, 2013, IEEE I CONF COMP VIS, P1529, DOI 10.1109/ICCV.2013.193
   Goferman S, 2012, IEEE T PATTERN ANAL, V34, P1915, DOI 10.1109/TPAMI.2011.272
   Harel J, 2006, Advances in Neural Information Processing Systems, V19
   Itti L, 2004, IEEE T IMAGE PROCESS, V13, P1304, DOI 10.1109/TIP.2004.834657
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Jiang HZ, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.110
   Levin A, 2008, IEEE T PATTERN ANAL, V30, P228, DOI 10.1109/TPAMI.2007.1177
   Li J, 2013, IEEE SIGNAL PROC LET, V20, P845, DOI 10.1109/LSP.2013.2268868
   Liu T, 2011, IEEE T PATTERN ANAL, V33, P353, DOI 10.1109/TPAMI.2010.70
   Ma Y.F., 2003, P 11 ACM INT C MULT, P374, DOI DOI 10.1145/957013.957094
   Min Chen, 2011, IEEE INFOCOM 2011 - IEEE Conference on Computer Communications. Workshops, P409, DOI 10.1109/INFCOMW.2011.5928847
   Perazzi F, 2012, PROC CVPR IEEE, P733, DOI 10.1109/CVPR.2012.6247743
   Rutishauser U., 2004, P IEEE COMP SOC C CO, DOI DOI 10.1109/CVPR.2004.1315142
   TREISMAN AM, 1980, COGNITIVE PSYCHOL, V12, P97, DOI 10.1016/0010-0285(80)90005-5
   Tsotsos JK, 2008, INT C DEVEL LEARN, P55, DOI 10.1109/DEVLRN.2008.4640805
   Wang D, 2011, VISUAL COMPUT, V27, P853, DOI 10.1007/s00371-011-0559-x
   Wei YC, 2012, LECT NOTES COMPUT SC, V7574, P29, DOI 10.1007/978-3-642-33712-3_3
   Yan Q, 2013, PROC CVPR IEEE, P1155, DOI 10.1109/CVPR.2013.153
   Yang C, 2013, IEEE SIGNAL PROC LET, V20, P637, DOI 10.1109/LSP.2013.2260737
   Zhai Y., 2006, P 14 ACM INT C MULT, P815, DOI [DOI 10.1145/1180639.1180824, 10.1145/1180639.1180824]
NR 29
TC 12
Z9 12
U1 0
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2016
VL 32
IS 1
BP 31
EP 41
DI 10.1007/s00371-014-1053-z
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DF9FQ
UT WOS:000371666800004
DA 2024-07-18
ER

PT J
AU Li, ZY
   Yu, XP
   Li, PF
   Hashem, M
AF Li, Zhiyong
   Yu, Xiaoping
   Li, Pengfei
   Hashem, Mervat
TI Moving object tracking based on multi-independent features distribution
   fields with comprehensive spatial feature similarity
SO VISUAL COMPUTER
LA English
DT Article
DE Object tracking; Distribution fields; Multi-independent features;
   Spatial feature similarity
ID ROBUST
AB Obtaining the exact spatial information of moving objects is a crucial and difficult task during visual object tracking. In this study, a comprehensive spatial feature similarity (CSFS) strategy is proposed to compute the confidence level of target features. This strategy is used to determine the current position of the target among candidates during the tracking process. Given that the spatial information and appearance feature of an object should be considered simultaneously, the CSFS strategy offers the benefit of reliable tracking position decisions. Moreover, we propose an appearance-based multi-independent features distribution fields (MIFDFs) object representation model, which represents targets using spatial distribution fields with multiple features independently. This representation model can preserve a large amount of original spatial and feature data synthetically. Various experimental results show that the proposed method exhibits significant improvement in terms of tracking drift in complex scenes. In particular, the proposed approach outperforms other techniques in tracking robustness and accuracy in some challenging situations.
C1 [Li, Zhiyong; Yu, Xiaoping; Li, Pengfei; Hashem, Mervat] Hunan Univ, Coll Comp Sci & Elect Engn, Changsha 410082, Hunan, Peoples R China.
C3 Hunan University
RP Li, ZY (corresponding author), Hunan Univ, Coll Comp Sci & Elect Engn, Changsha 410082, Hunan, Peoples R China.
EM zhiyong.li@hnu.edu.cn; yuxiaoping555@163.com
RI li, zy/HZM-1892-2023; yu, xiao/KFT-1725-2024
FU National Natural Science Foundation of China [61173107, 91320103];
   National High Technology Research And Development Program (863)
   [2012AA01A301-01]; Research Foundation of Industry-Education-Research
   Cooperation among Guangdong Province; Ministry of Science & Technology,
   China [2011A091000027]; Research Foundation of
   Industry-Education-Research Cooperation of Huizhou, Guangdong
   [2012C050012012]; Ministry of Education
FX This work was partially supported by the National Natural Science
   Foundation of China (Grant no. 61173107, 91320103), National High
   Technology Research And Development Program (863) (Grant no.
   2012AA01A301-01), the Research Foundation of Industry-Education-Research
   Cooperation among Guangdong Province, Ministry of Education and Ministry
   of Science & Technology, China (Grant No. 2011A091000027) and the
   Research Foundation of Industry-Education-Research Cooperation of
   Huizhou, Guangdong (Grant No. 2012C050012012).
CR [Anonymous], 2006, IEEE C COMPUTER VISI, DOI DOI 10.1109/CVPR.2006.256
   [Anonymous], 2006, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
   Arulampalam MS, 2002, IEEE T SIGNAL PROCES, V50, P174, DOI 10.1109/78.978374
   Babenko B, 2011, IEEE T PATTERN ANAL, V33, P1619, DOI 10.1109/TPAMI.2010.226
   Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1007/11744023_32
   Birchfield ST, 2005, PROC CVPR IEEE, P1158
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851
   Chen JY, 2012, IEEE T GEOSCI REMOTE, V50, P4513, DOI 10.1109/TGRS.2012.2194502
   Collins R., 2000, CMURITR0012 VSAM
   Collins RT, 2003, PROC CVPR IEEE, P234
   Comaniciu D, 2003, IEEE T PATTERN ANAL, V25, P564, DOI 10.1109/TPAMI.2003.1195991
   Comaniciu D, 2000, PROC CVPR IEEE, P142, DOI 10.1109/CVPR.2000.854761
   Fröba B, 2002, FIFTH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, PROCEEDINGS, P342, DOI 10.1109/AFGR.2002.1004177
   Hotrakool W., 2010, Proc. International Conference on Electrical Engineering/Electronics Computer Telecommunications and Information Technology (ECTI-CON), P492
   Junqiu W., 2008, P IEEE C COMP VIS PA, V17
   Ke Y, 2004, PROC CVPR IEEE, P506
   Khan ZH, 2010, IEEE T INF FOREN SEC, V5, P591, DOI 10.1109/TIFS.2010.2050312
   Kwon J, 2010, PROC CVPR IEEE, P1269, DOI 10.1109/CVPR.2010.5539821
   Kwon J, 2009, PROC CVPR IEEE, P1208, DOI 10.1109/CVPRW.2009.5206502
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Mei X, 2011, IEEE T PATTERN ANAL, V33, P2259, DOI 10.1109/TPAMI.2011.66
   Moeslund TB, 2001, COMPUT VIS IMAGE UND, V81, P231, DOI 10.1006/cviu.2000.0897
   Nguyen THD, 2005, IEEE T VIS COMPUT GR, V11, P706, DOI 10.1109/TVCG.2005.105
   Sevilla-Lara L, 2012, PROC CVPR IEEE, P1910, DOI 10.1109/CVPR.2012.6247891
   Song S., 2013, TRACKING REVISITED U
   Stauffer C, 2000, IEEE T PATTERN ANAL, V22, P747, DOI 10.1109/34.868677
   Sun L, 2011, IEEE T CIRC SYST VID, V21, P408, DOI 10.1109/TCSVT.2010.2087815
   Sun L, 2009, PROC CVPR IEEE, P9, DOI 10.1109/CVPR.2009.5204358
   Suryanto, 2011, IMAGE VISION COMPUT, V29, P850, DOI 10.1016/j.imavis.2011.09.008
   Tian YL, 2005, PROC CVPR IEEE, P1182
   Xiong HK, 2013, IEEE T CIRC SYST VID, V23, P534, DOI 10.1109/TCSVT.2012.2210801
   Zhang KH, 2012, LECT NOTES COMPUT SC, V7574, P864, DOI 10.1007/978-3-642-33712-3_62
   Zhang KH, 2013, PATTERN RECOGN, V46, P397, DOI 10.1016/j.patcog.2012.07.013
   Zhang L, 2013, PROC CVPR IEEE, P1838, DOI 10.1109/CVPR.2013.240
NR 34
TC 7
Z9 7
U1 0
U2 11
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2015
VL 31
IS 12
BP 1633
EP 1651
DI 10.1007/s00371-014-1044-0
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CV2ZD
UT WOS:000364126900005
DA 2024-07-18
ER

PT J
AU Liu, XL
   Hou, F
   Hao, AM
   Qin, H
AF Liu, Xinglong
   Hou, Fei
   Hao, Aimin
   Qin, Hong
TI A parallelized 4D reconstruction algorithm for vascular structures and
   motions based on energy optimization
SO VISUAL COMPUTER
LA English
DT Article
DE X-ray angiograms; 3D reconstruction; Motion tracking; Belief propagation
ID EFFICIENT BELIEF PROPAGATION; CORONARY-ARTERIES; 3-DIMENSIONAL
   RECONSTRUCTION; QUANTITATIVE-ANALYSIS; 3-D RECONSTRUCTION; VESSEL
   EXTRACTION; MATCHED-FILTER; TRACKING; MINIMIZATION; VALIDATION
AB In this paper, we present a parallel 4D vessel reconstruction algorithm that simultaneously recovers 3D structure, shape, and motion based on multiple views of X-ray angiograms. The fundamental goal is to assist the analysis and diagnosis of interventional surgery in the most efficient way towards interactive and accurate performance. We start with a fully parallelized algorithm to extract vessels as well as their skeletons and topologies from dynamic image sequences. Then, instead of resorting to registration, we present an algorithm to formulate the reconstruction problem as an energy minimization problem with color, coherence, and topology constraints to reconstruct the 3D vessel initially, which is robust to combat noise and incomplete information in images. Next, we incorporate temporal information into our energy optimization framework to track and reconstruct 4D kinematics of the dynamic vessels, which is also capable of recovering previous incomplete and misleading shapes acquired from static images otherwise. We demonstrate our system in coronary arteries reconstruction and movement tracking for percutaneous coronary intervention surgery to help medical practitioners learn about the 3D shapes and their motions of the coronary arteries of specific patient. We envision that our system would be of high assistance for diagnosis and therapy to treat vessel-related diseases in a clinical setting in the near future.
C1 [Liu, Xinglong; Hou, Fei; Hao, Aimin] Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing 100191, Peoples R China.
   [Qin, Hong] SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11794 USA.
C3 Beihang University; State University of New York (SUNY) System; State
   University of New York (SUNY) Stony Brook
RP Hou, F (corresponding author), Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing 100191, Peoples R China.
EM houfei@vrlab.buaa.edu.cn
FU National Natural Science Foundation of China [61190120, 61190121,
   61190125, 61300068, 61300067]; National Science Foundation of USA
   [IIS-0949467, IIS-1047715, IIS-1049448]; National High Technology
   Research and Development Program (863 Program) of China [012AA011503];
   Postdoctoral Science Foundation of China [2013M530512]
FX This work was supported in part by National Natural Science Foundation
   of China (Grant No. 61190120, 61190121, 61190125, 61300068, 61300067),
   National Science Foundation of USA (Grant No. IIS-0949467, IIS-1047715,
   and IIS-1049448), the National High Technology Research and Development
   Program (863 Program) of China (Grant No. 012AA011503), Postdoctoral
   Science Foundation of China (Grant No. 2013M530512).
CR Aylward SR, 2002, IEEE T MED IMAGING, V21, P61, DOI 10.1109/42.993126
   Blondel C, 2006, IEEE T MED IMAGING, V25, P653, DOI 10.1109/TMI.2006.873224
   Bouattour S, 2005, LECT NOTES COMPUT SC, V3691, P724
   Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114
   Boykov YY, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P105, DOI 10.1109/ICCV.2001.937505
   Brunton A., 2006, Proceedings of the 3rd Canadian Conference on Computer and Robot Vision (CRV06), P76
   Cañero C, 2002, IEEE T MED IMAGING, V21, P1188, DOI 10.1109/TMI.2002.804421
   Chen SYJ, 1996, P SOC PHOTO-OPT INS, V2710, P103, DOI 10.1117/12.237914
   Chen SYJ, 2003, IEEE T MED IMAGING, V22, P710, DOI 10.1109/TMI.2003.814788
   Chen SYJ, 1997, MED PHYS, V24, P633, DOI 10.1118/1.598129
   ConduracheAP AachT, 2005, P 9 IAPR C MACH VIS, P269
   Coughlan J, 2007, COMPUT VIS IMAGE UND, V106, P47, DOI 10.1016/j.cviu.2005.09.008
   Dumay A C, 1994, Int J Card Imaging, V10, P205, DOI 10.1007/BF01137902
   Felzenszwalb PF, 2006, INT J COMPUT VISION, V70, P41, DOI 10.1007/s11263-006-7899-4
   FESSLER JA, 1991, IEEE T MED IMAGING, V10, P25, DOI 10.1109/42.75608
   Frangi AF, 1998, LECT NOTES COMPUT SC, V1496, P130, DOI 10.1007/BFb0056195
   GEMAN S, 1984, IEEE T PATTERN ANAL, V6, P721, DOI 10.1109/TPAMI.1984.4767596
   Gollapudi RR, 2007, CATHETER CARDIO INTE, V69, P479, DOI 10.1002/ccd.20955
   Hansis E, 2008, PHYS MED BIOL, V53, P3807, DOI 10.1088/0031-9155/53/14/007
   Hassouna MS, 2007, IEEE T PATTERN ANAL, V29, P1563, DOI 10.1109/TPAMI.2007.1154
   Hoover A, 2000, IEEE T MED IMAGING, V19, P203, DOI 10.1109/42.845178
   Ingrassia C., 1999, Proceedings of the First Joint BMES/EMBS Conference. 1999 IEEE Engineering in Medicine and Biology 21st Annual Conference and the 1999 Annual Fall Meeting of the Biomedical Engineering Society (Cat. No.99CH37015), DOI 10.1109/IEMBS.1999.802251
   Kirbas C, 2004, ACM COMPUT SURV, V36, P81, DOI 10.1145/1031120.1031121
   Kolmogorov V, 2004, IEEE T PATTERN ANAL, V26, P147, DOI 10.1109/TPAMI.2004.1262177
   Lafferty John, 2001, INT C MACH LEARN ICM
   Li Q, 2012, EXPERT SYST APPL, V39, P7600, DOI 10.1016/j.eswa.2011.12.046
   LIU IH, 1992, OPT ENG, V31, P2197, DOI 10.1117/12.59977
   Meltzer T, 2005, IEEE I CONF COMP VIS, P428
   Messenger JC, 2000, INT J CARDIAC IMAG, V16, P413, DOI 10.1023/A:1010643426720
   Movassaghi B, 2004, IEEE T MED IMAGING, V23, P1517, DOI 10.1109/TMI.2004.837340
   Murphy KP, 1999, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, PROCEEDINGS, P467
   Naegel B, 2007, PATTERN RECOGN, V40, P635, DOI 10.1016/j.patcog.2006.06.004
   NGUYEN TV, 1994, IEEE T MED IMAGING, V13, P61, DOI 10.1109/42.276145
   Pearl J., 1982, P NAT C ART INT, P133, DOI DOI 10.1038/4580
   Potetz B, 2008, COMPUT VIS IMAGE UND, V112, P39, DOI 10.1016/j.cviu.2008.05.007
   Puentes J, 1998, IEEE T MED IMAGING, V17, P857, DOI 10.1109/42.746619
   Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720
   RUAN S, 1994, IMAGE VISION COMPUT, V12, P683, DOI 10.1016/0262-8856(94)90043-4
   Sarry L, 2001, IEEE T MED IMAGING, V20, P1341, DOI 10.1109/42.974929
   Schoonenberg G, 2009, MED IMAGE ANAL, V13, P785, DOI 10.1016/j.media.2009.07.005
   Sen A, 1999, MED PHYS, V26, P698, DOI 10.1118/1.598575
   Shechter G, 2006, IEEE T MED IMAGING, V25, P369, DOI 10.1109/TMI.2005.862752
   Shechter G, 2003, IEEE T MED IMAGING, V22, P493, DOI 10.1109/TMI.2003.809090
   Sprague K, 2006, MED PHYS, V33, P707, DOI 10.1118/1.2143352
   Szeliski R, 2008, IEEE T PATTERN ANAL, V30, P1068, DOI 10.1109/TPAMI.2007.70844
   Tappen MF, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P900
   Van Uitert R, 2007, MED PHYS, V34, P627, DOI 10.1118/1.2409238
   Vezhnevets V., 2005, Proc. Graphicon, V1, P150
   Wellnhofer E, 1999, INT J CARDIAC IMAG, V15, P339, DOI 10.1023/A:1006322609072
   WENG JY, 1993, IEEE T PATTERN ANAL, V15, P864, DOI 10.1109/34.232074
   Zhang B, 2010, COMPUT BIOL MED, V40, P438, DOI 10.1016/j.compbiomed.2010.02.008
   ZHANG TY, 1984, COMMUN ACM, V27, P236, DOI 10.1145/357994.358023
NR 52
TC 8
Z9 9
U1 0
U2 13
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2015
VL 31
IS 11
BP 1431
EP 1446
DI 10.1007/s00371-014-1024-4
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CT3BJ
UT WOS:000362680600001
DA 2024-07-18
ER

PT J
AU Wei, LY
   Shi, KL
   Yong, JH
AF Wei, Ling-Yu
   Shi, Kan-Le
   Yong, Jun-Hai
TI Rendering chamfering structures of sharp edges
SO VISUAL COMPUTER
LA English
DT Article
DE Real-time rendering; Chamfering structure; Edge highlighting
AB In most realtime applications such as 3D games, in order to reduce the complexity of the scene being rendered, objects are often made by simple and large primitives. Thus, the phenomenon of edge highlighting, which would require chamfering structures made by lots of small patches at the seaming, is absent and is often faked by "highlights" drawn on the texture. We proposed a realistic realtime rendering procedure for highlighting chamfering structures, or rounded edges, by considering specified edges as thin cylinders and obtained the intensity via integration. We derived a brief approximated formula generalized from Blinn's shadow model, and used a precomputed integration table to accelerate the render speed and reduce resources needed. The algorithm is implemented with shader language, and can be considered as a post-process on original result. Evaluation shows that the effect on rendering speed is limited even for scenes with large scale of vertices.
C1 [Wei, Ling-Yu] Tsinghua Univ, Inst Interdisciplinary Informat Sci, ITCS, Beijing 100084, Peoples R China.
   [Wei, Ling-Yu] Univ So Calif, Dept Comp Sci, Geometr Capture Lab, Los Angeles, CA 90089 USA.
   [Shi, Kan-Le; Yong, Jun-Hai] Tsinghua Univ, Sch Software, Beijing 100084, Peoples R China.
C3 Tsinghua University; University of Southern California; Tsinghua
   University
RP Wei, LY (corresponding author), Tsinghua Univ, Inst Interdisciplinary Informat Sci, ITCS, Beijing 100084, Peoples R China.
EM cosimo.dw@gmail.com; shikanle@gmail.com; yongjh@tsinghua.edu.cn
RI Wei, Lingyu/N-4040-2018
OI Wei, Lingyu/0000-0001-7278-4228
FU National Basic Research Program of China [2011CBA00300, 2011CBA00301];
   NSFC [61033001, 61361136003, 61035002, 61272235, 91315302, 61173077];
   Chinese 973 Program [2010CB328001]
FX Thanks for the first anonymous reviewer for this article who pointed out
   a mistake in the derivation of Eq. 3. And because of this I found out a
   deeper and more serious bug, making me over-write almost the whole Sect.
   3.1. The first author was supported by the National Basic Research
   Program of China Grant 2011CBA00300, 2011CBA00301, the NSFC (61033001,
   61361136003). The second author was supported by the Chinese 973 Program
   (2010CB328001) and the NSFC (61035002, 61272235). The third author was
   supported by the NSFC (91315302, 61173077).
CR [Anonymous], SIGGRAPH 77 P 4 ANN
   Cook R. L., 1982, ACM T GRAPHIC, V1, P7, DOI DOI 10.1145/357290.357293
   Microsoft Corporation, 2009, TES OV DIRECTX11
   PHONG BT, 1975, COMMUN ACM, V18, P311, DOI 10.1145/360825.360839
   Saito T., 1989, New Advances in Computer Graphics. Proceedings of CG International '89, P613
   Sloan P.P., 2012, GPU TECHN C
   Tanaka T., 1991, Scientific Visualization of Physical Phenomena, P283
   Tanaka T., 1992, Visual Computer, V8, P315, DOI 10.1007/BF01897118
NR 8
TC 2
Z9 2
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2015
VL 31
IS 11
BP 1511
EP 1519
DI 10.1007/s00371-014-1030-6
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CT3BJ
UT WOS:000362680600007
DA 2024-07-18
ER

PT J
AU Kim, B
   Tsiotras, P
   Hong, JM
   Song, OY
AF Kim, Byungmoon
   Tsiotras, Panagiotis
   Hong, Jeong-Mo
   Song, Oh-young
TI Interpolation and parallel adjustment of center-sampled trees with new
   balancing constraints
SO VISUAL COMPUTER
LA English
DT Article
DE Octree; Quadtree; Balanced tree; Interpolation; Parallelization; Smoke
   simulation; Segmentation
ID LEVEL SET METHOD; SEGMENTATION; FRAMEWORK; EQUATIONS; SHAPE
AB We present a novel tree balancing constraint that is slightly stronger than the well-known 2-to-1 balancing constraint used in octree data structures (Tu and O'hallaron, Balanced refinement of massive linear octrees. Tech. Rep. CMU-CS-04-129. Carnegie Mellon School of Computer Science, Pennsylvania, 2004). The new balancing produces a limited number of local cell connectivity types (stencils): 5 for a quadtree and 21 for an octree. Using this constraint, we interpolate the data sampled at cell centers using weights pre-computed by interpolation or by generating interpolation codes for each stencil. In addition, we develop a parallel tree adjustment algorithm, and show that the imposed balancing constraint is satisfied even when the tree is adjusted in parallel. We also show that the adjustment has high parallelization performance. We finally apply the new balancing scheme to level set image segmentation and smoke simulation problems.
C1 [Kim, Byungmoon] Adobe Syst, San Jose, CA USA.
   [Tsiotras, Panagiotis] Georgia Inst Technol, Atlanta, GA 30332 USA.
   [Hong, Jeong-Mo] Dongguk Univ, Seoul, South Korea.
   [Song, Oh-young] Sejong Univ, Seoul, South Korea.
C3 Adobe Systems Inc.; University System of Georgia; Georgia Institute of
   Technology; Dongguk University; Sejong University
RP Song, OY (corresponding author), Sejong Univ, Seoul, South Korea.
EM oysong@sejong.ac.kr
RI Tsiotras, Panagiotis/L-4776-2019
OI Tsiotras, Panagiotis/0000-0001-7563-4129
FU US National Science Foundation [CMMI-0856565]; National Research
   Foundation of Korea (NRF) [NRF-2011-0023134]
FX The second author was supported by the US National Science Foundation,
   award CMMI-0856565. The third author was supported by National Research
   Foundation of Korea (NRF) (Grant NRF-2011-0023134).
CR ADALSTEINSSON D, 1995, J COMPUT PHYS, V118, P269, DOI 10.1006/jcph.1995.1098
   [Anonymous], 2002, SURFACES
   Bai Y, 2007, LECT NOTES COMPUT SC, V4584, P556
   Benson D, 2002, ACM T GRAPHIC, V21, P785, DOI 10.1145/566570.566652
   Brox T, 2006, IEEE T IMAGE PROCESS, V15, P3213, DOI 10.1109/TIP.2006.877481
   CASELLES V, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P694, DOI 10.1109/ICCV.1995.466871
   Chan TE, 2000, J VIS COMMUN IMAGE R, V11, P130, DOI 10.1006/jvci.1999.0442
   Chan TF, 2001, IEEE T IMAGE PROCESS, V10, P266, DOI 10.1109/83.902291
   Chen H, 2007, J SCI COMPUT, V31, P19, DOI 10.1007/s10915-006-9122-8
   Cremers D, 2003, PROC CVPR IEEE, P53
   DeBry D, 2002, ACM T GRAPHIC, V21, P763, DOI 10.1145/566570.566649
   Foster N., 2001, ACM COMPUTER GRAPHIC, V18, P15
   Frisken SF, 2000, COMP GRAPH, P249, DOI 10.1145/344779.344899
   Gibou F, 2007, INT SER NUMER MATH, V154, P199
   Ju T, 2002, ACM T GRAPHIC, V21, P339
   Kim B., 2009, P SPIE ELECT IMAGING, VVI, P265
   Losasso F, 2004, ACM T GRAPHIC, V23, P457, DOI 10.1145/1015706.1015745
   Malladi R., 1994, LECT NOTES COMPUTER, P1
   MILNE B, 1995, THESIS U CALIFORNIA
   Min C.-H., 2006, J COMPUT PHYS, V202, P577
   Min C, 2007, J COMPUT PHYS, V225, P300, DOI 10.1016/j.jcp.2006.11.034
   Min CH, 2006, J COMPUT PHYS, V219, P912, DOI 10.1016/j.jcp.2006.07.019
   OSHER S, 1988, J COMPUT PHYS, V79, P12, DOI 10.1016/0021-9991(88)90002-2
   Parashar M., 1995, P INT C HIGH PERF CO
   PLEWA T, 2003, LECT NOTES COMPUTATI, V41
   Sagan H., 1994, SPACE FILLING CURVES
   Schaefer S, 2004, 12TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P70, DOI 10.1109/PCCGA.2004.1348336
   Sethian J., 1999, LEVEL SET METHODS FA
   Stam J, 1999, COMP GRAPH, P121, DOI 10.1145/311535.311548
   Strain J, 1999, J COMPUT PHYS, V152, P664, DOI 10.1006/jcph.1999.6259
   Tu T., 2004, CMUCS04129
   Vese LA, 2002, INT J COMPUT VISION, V50, P271, DOI 10.1023/A:1020874308076
   Westermann R, 1999, VISUAL COMPUT, V15, P100, DOI 10.1007/s003710050165
   WILHELMS J, 1992, ACM T GRAPHIC, V11, P201, DOI 10.1145/130881.130882
   YERRY MA, 1984, INT J NUMER METH ENG, V20, P1965, DOI 10.1002/nme.1620201103
   Zhou K, 2011, IEEE T VIS COMPUT GR, V17, P669, DOI 10.1109/TVCG.2010.75
NR 36
TC 1
Z9 1
U1 0
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2015
VL 31
IS 10
BP 1351
EP 1363
DI 10.1007/s00371-014-1018-2
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CQ7CG
UT WOS:000360759800005
DA 2024-07-18
ER

PT J
AU Li, CC
   Zhou, WD
   Yuan, SS
AF Li, Chengcheng
   Zhou, Weidong
   Yuan, Shasha
TI Iris recognition based on a novel variation of local binary pattern
SO VISUAL COMPUTER
LA English
DT Article
DE Iris recognition; Average local binary pattern; NN classifier; SVM
   classifier
ID CLASSIFICATION
AB In this paper, an efficient method based on a novel variation of local binary pattern (LBP), average local binary pattern (ALBP), is proposed for iris recognition, which is less sensitive to histogram equalization and parameters' selection and has low computation complexity. Center pixel and its neighborhood are the crucial elements involved in basic LBP. ALBP places high value on the significance of center pixel, while nearly all other variations of LBP have been focusing on the selection of neighborhood. Four candidates for the modification of the center pixel are elected and validated, respectively. In the proposed framework, the valid iris region firstly is localized and then normalized into a uniform rectangular. Then the normalized iris is chopped into several sub-images, and ALBP operator is applied to each sub-image to obtain individual histogram feature. Every histogram feature is then concatenated to form a global iris feature vector. Nearest neighbor classifier and support vector machine are employed to validate the recognition performance. Experimental results on CASIA-IrisV4 (including CASIA-Iris-Interval and CASIA-Iris-Thousand) and UBIRIS.V1 database show that our method achieves competitive recognition performance (optimal recognition rate is ) compared with other methods using the same databases.
C1 [Li, Chengcheng; Zhou, Weidong; Yuan, Shasha] Shandong Univ, Sch Informat Sci & Engn, Jinan 250100, Peoples R China.
C3 Shandong University
RP Li, CC (corresponding author), Shandong Univ, Sch Informat Sci & Engn, Jinan 250100, Peoples R China.
EM 714241422@qq.com; wdzhou@sdu.edu.cn; jiayouyss@sina.cn
RI zhou, wei/HTO-6935-2023
CR Ahonen T, 2006, IEEE T PATTERN ANAL, V28, P2037, DOI 10.1109/TPAMI.2006.244
   [Anonymous], IEEE T PAMI
   [Anonymous], IEEE C EV COMP
   Arya KV, 2014, 2014 INTERNATIONAL CONFERENCE ON SIGNAL PROCESSING AND INTEGRATED NETWORKS (SPIN), P744, DOI 10.1109/SPIN.2014.6777053
   Bowyer KW, 2008, COMPUT VIS IMAGE UND, V110, P281, DOI 10.1016/j.cviu.2007.08.005
   Feng XY, 2005, LECT NOTES ARTIF INT, V3789, P328
   Huang D, 2011, IEEE T SYST MAN CY C, V41, P765, DOI 10.1109/TSMCC.2011.2118750
   Hui Zhang, 2012, Proceedings of the 2012 IEEE 3rd International Conference on Software Engineering and Service Science (ICSESS), P131, DOI 10.1109/ICSESS.2012.6269422
   Jain A. K., 1999, BIOMETRICS PERSONAL, P43
   Knerr S., 1990, Neurocomputing, Algorithms, Architectures and Applications. Proceedings of the NATO Advanced Research Workshop, P41
   Liao S, 2009, IEEE T IMAGE PROCESS, V18, P1107, DOI 10.1109/TIP.2009.2015682
   Ma L, 2004, IEEE T IMAGE PROCESS, V13, P739, DOI 10.1109/TIP.2004.827237
   Monro DM, 2007, IEEE T PATTERN ANAL, V29, P586, DOI 10.1109/TPAMI.2007.1002
   Ojala T, 2000, LECT NOTES COMPUT SC, V1842, P404
   Proença H, 2005, LECT NOTES COMPUT SC, V3617, P970, DOI 10.1007/11553595_119
   Rahulkar AD, 2014, PATTERN ANAL APPL, V17, P529, DOI 10.1007/s10044-013-0334-x
   Ren JF, 2013, IEEE T IMAGE PROCESS, V22, P4049, DOI 10.1109/TIP.2013.2268976
   Roy Kaushik, 2007, 2007 10th International Conference on Computer and Information Technology (ICCIT 2007), P1, DOI 10.1109/ICCITECHN.2007.4579426
   Sun Z., BIOMETRIC AUTHENTICA, V3087, P270
   Tan T. N., 2009, IRIS IMAGE DATABASE
   Tze Weng Ng, 2010, Proceedings of the 2010 2nd International Conference on Signal Processing Systems (ICSPS 2010), P820, DOI 10.1109/ICSPS.2010.5555246
   Wang Y, 2004, LECT NOTES COMPUT SC, V3173, P622
   Wildes RP, 1997, P IEEE, V85, P1348, DOI 10.1109/5.628669
   Yuan WQ, 2005, LECT NOTES ARTIF INT, V3614, P306
   Zhang L, 2007, LECT NOTES COMPUT SC, V4642, P11
   Zhao Y, 2012, IEEE T IMAGE PROCESS, V21, P4492, DOI 10.1109/TIP.2012.2204271
NR 26
TC 37
Z9 37
U1 2
U2 25
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2015
VL 31
IS 10
BP 1419
EP 1429
DI 10.1007/s00371-014-1023-5
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CQ7CG
UT WOS:000360759800010
DA 2024-07-18
ER

PT J
AU Ahmed, F
   Paul, PP
   Gavrilova, ML
AF Ahmed, Faisal
   Paul, Padma Polash
   Gavrilova, Marina L.
TI DTW-based kernel and rank-level fusion for 3D gait recognition using
   Kinect
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 32nd Computer Graphics International CGI 15 Conference
CY JUN 24-26, 2015
CL INSA Strasbourg Univ Strasbourg, Strasbourg, FRANCE
SP CNRS, iCUBE, Univ Strasbourg, CGS, Springer, Acm Incooperation, IGG, ACMSIGGRAPH, KIST Europe, Region Alsace, Visteon
HO INSA Strasbourg Univ Strasbourg
DE Gait recognition; Kinect v2 sensor; Joint relative distance; Joint
   relative angle; DTW-kernel; 3D skeleton
ID RETRIEVAL
AB This paper presents a new 3D gait recognition method that utilizes the kinect skeleton data for representing the gait signature. We propose to use two new features, namely joint relative distance (JRD) and joint relative angle (JRA), which are robust against view and pose variations. The relevance of each JRD and JRA sequence in representing human gait is evaluated using a genetic algorithm. We also introduce a dynamic time warping-based kernel that takes a collection of JRD or JRA sequences as parameters and computes a dissimilarity measure between the training and the unknown sample. The proposed kernel can effectively handle variable walking speed without any need of extra pre-processing. In addition, we propose a rank-level fusion of JRD and JRA features that can boost the overall recognition performance greatly. The effectiveness of the proposed method is evaluated using a 3D skeletal gait database captured with a Kinect v2 sensor. In our experiments, rank level fusion of joint relative distance (JRD) and joint relative angle (JRA) achieves promising results, as compared against only JRD and only JRA-based gait recognition.
C1 [Ahmed, Faisal; Paul, Padma Polash; Gavrilova, Marina L.] Univ Calgary, Dept Comp Sci, Calgary, AB T2N 1N4, Canada.
C3 University of Calgary
RP Ahmed, F (corresponding author), Univ Calgary, Dept Comp Sci, Calgary, AB T2N 1N4, Canada.
EM faahmed@ucalgary.ca; pppaul@ucalgary.ca; mgavrilo@ucalgary.ca
RI Gavrilova, Marina/AHD-3605-2022
OI Gavrilova, Marina/0000-0002-5338-1834
FU NSERC; URGC; NSERC ENGAGE; AITF; SMART Technologies ULC, Canada
FX The authors would like to thank NSERC DISCOVERY program, URGC, NSERC
   ENGAGE, AITF, and SMART Technologies ULC, Canada for partial support.
CR [Anonymous], 2011, PROC CVPR IEEE, DOI [DOI 10.1109/CVPR.2011.5995316, 10.1109/CVPR.2011.5995316]
   Bae MS, 2013, VISUAL COMPUT, V29, P555, DOI 10.1007/s00371-013-0819-z
   Ball A, 2012, ACMIEEE INT CONF HUM, P225
   BenAbdelkader C, 2002, FIFTH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, PROCEEDINGS, P372, DOI 10.1109/AFGR.2002.1004182
   Bobick AF, 2001, IEEE T PATTERN ANAL, V23, P257, DOI 10.1109/34.910878
   Chang YJ, 2011, RES DEV DISABIL, V32, P2566, DOI 10.1016/j.ridd.2011.07.002
   Chen CH, 2009, PATTERN RECOGN LETT, V30, P977, DOI 10.1016/j.patrec.2009.04.012
   CHOUDHURY SD, 2014, P INT WORK BIOM FOR, P1
   Deutschmann I, 2013, IT PROF, V15, P12, DOI 10.1109/MITP.2013.50
   Gabel Moshe, 2012, Annu Int Conf IEEE Eng Med Biol Soc, V2012, P1964, DOI 10.1109/EMBC.2012.6346340
   Han J, 2006, IEEE T PATTERN ANAL, V28, P316, DOI 10.1109/TPAMI.2006.38
   Han J, 2004, PROC CVPR IEEE, P842
   Holland J.H., 1992, Adaptation in Natural and Artificial Systems, DOI DOI 10.7551/MITPRESS/1090.001.0001
   Kale A, 2004, IEEE T IMAGE PROCESS, V13, P1163, DOI 10.1109/TIP.2004.832865
   Kumar A, 2010, IEEE IMAGE PROC, P3121, DOI 10.1109/ICIP.2010.5653214
   Kumar R, 2013, 2013 THIRD INTERNATIONAL CONFERENCE ON ADVANCES IN COMPUTING AND COMMUNICATIONS (ICACC 2013), P102, DOI 10.1109/ICACC.2013.26
   Li XX, 2013, MYCOTAXON, V126, P121, DOI 10.5248/126.121
   Munsell BC, 2012, LECT NOTES COMPUT SC, V7585, P91, DOI 10.1007/978-3-642-33885-4_10
   Popa M, 2012, COMM COM INF SC, V277, P91
   Prabhakar S., 2003, IEEE Security & Privacy, V1, P33, DOI 10.1109/MSECP.2003.1193209
   Preis J., 2012, P WORK KIN PERV COMP
   SANKOFF D, 1983, TIME WARPS STRING ED
   Sarkar S, 2005, IEEE T PATTERN ANAL, V27, P162, DOI 10.1109/TPAMI.2005.39
   Shanker AP, 2007, PATTERN RECOGN LETT, V28, P1407, DOI 10.1016/j.patrec.2007.02.016
   Stone E. E., 2011, 2011 5th International Conference on Pervasive Computing Technologies for Healthcare (PervasiveHealth 2011), P71, DOI 10.4108/icst.pervasivehealth.2011.246034
   Tang JKT, 2008, COMPUT ANIMAT VIRT W, V19, P211, DOI 10.1002/cav.260
   Tang JKT, 2012, PATTERN RECOGN LETT, V33, P420, DOI 10.1016/j.patrec.2011.06.005
   Urtasun R, 2004, SIXTH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, PROCEEDINGS, P17, DOI 10.1109/AFGR.2004.1301503
   Wang L, 2003, IEEE T PATTERN ANAL, V25, P1505, DOI 10.1109/TPAMI.2003.1251144
   Wang N, 2010, PROCEEDINGS OF THE 2ND (2010) INTERNATIONAL CONFERENCE ON FINANCIAL RISK AND CORPORATE FINANCE MANAGEMENT, P320
   Yam CY, 2004, PATTERN RECOGN, V37, P1057, DOI 10.1016/j.patcog.2003.09.012
   Zhang YH, 2014, J NANOPART RES, V16, DOI 10.1007/s11051-014-2375-5
   Zhon XL, 2007, IEEE T SYST MAN CY B, V37, P1119, DOI 10.1109/TSMCB.2006.889612
   Zhou LY, 2014, VISUAL COMPUT, V30, P845, DOI 10.1007/s00371-014-0957-y
NR 34
TC 57
Z9 60
U1 0
U2 30
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2015
VL 31
IS 6-8
BP 915
EP 924
DI 10.1007/s00371-015-1092-0
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA CM2CN
UT WOS:000357487500017
DA 2024-07-18
ER

PT J
AU Liu, LY
   Chen, W
   Zheng, WT
   Geng, WD
AF Liu, Lingyue
   Chen, Wei
   Zheng, Wenting
   Geng, Weidong
TI Structure-aware error-diffusion approach using entropy-constrained
   threshold modulation
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Computational Visual Media Conference
CY 2013
CL Geneva, SWITZERLAND
DE Error diffusion; Entropy; Threshold modulation; MSSIM
ID ALGORITHM; QUALITY
AB Error diffusion is known as a commonly used digital halftoning technique. We present a novel and efficient error-diffusion algorithm which is capable of preserving appreciable structures and tones with blue-noise property. According to the theoretical analysis of threshold modulation, the extraction of the high-frequency image contents is helpful to preserve human vision-sensitive textures. The pixel intensity's influence on the structural distortion is observed based on a key statistic phenomenon. This effect leads to the non-uniform conservation of diversiform detail contents. To alleviate this influence, an entropy is introduced to measure the intensity's impact and adaptively constrain the threshold-modulation strength. Compared with the existing edge-enhancement halftoning, our entropy-based method does not suffer from the failure to detect weak edges or improper emphasis of details. On the other hand, this structural improvement enables the modification of error-diffusion coefficients to eliminate visually harmful tonal artifacts, which results in the seamless integration with the best tone-aware techniques (Ostromoukhov in Proceedings of ACM SIGGRAPH, SIGGRAPH '01, pp 567-572, 2001, Zhou and Fang in ACM Trans Graph (TOG) 22(3):437-444, 2003). Comparisons with the state-of-the-art structure-preserving error diffusions (Chang et al. in ACM Trans Graph (TOG) 28(5): 162:1-162:8, 2009, Li and Mould in Forum 29(2):273-280, 2010) indicate that our methods can achieve better structural similarity with better tone consistency. Our performance is one order of magnitude faster than (Chang et al. in ACM Trans Graph (TOG) 28(5): 162:1-162:8, 2009, Li and Mould in Forum 29(2): 273-280, 2010) while ensuring higher visual quality on typical images. Due to low computational overhead and high halftone quality, the proposed methods in this paper can be widely applicable in many practical applications.
C1 [Liu, Lingyue; Chen, Wei; Zheng, Wenting; Geng, Weidong] Zhejiang Univ, State Key Lab CAD&CG, Hangzhou 310003, Zhejiang, Peoples R China.
C3 Zhejiang University
RP Zheng, WT (corresponding author), Zhejiang Univ, State Key Lab CAD&CG, Hangzhou 310003, Zhejiang, Peoples R China.
EM liulingyue@zju.edu.cn; chenwei@cad.zju.edu.cn; wtzheng@cad.zju.edu.cn;
   gengwd@zju.edu.cn
RI zhang, jt/JVE-1333-2024; Chen, Wei/AAR-9817-2020
FU National Basic Research Program of China [2009CB320803]; 863 Program of
   China [2012AA12090]; NSFC [61232012]
FX This work was supported in part by National Basic Research Program of
   China (Grant No. 2009CB320803), and 863 Program of China (2012AA12090),
   NSFC (61232012).
CR ANALOUI M, 1992, P SOC PHOTO-OPT INS, V1666, P96, DOI 10.1117/12.135959
   Asano T, 1996, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, PROCEEDINGS - VOL I, P545, DOI 10.1109/ICIP.1996.559554
   Balian R, 2004, PROG MATH PHYS, V38, P119
   Bayer BE., 1973, 20OPTIMUM METHOD 2 L
   Chang JH, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618508
   ESCHBACH R, 1991, J OPT SOC AM A, V8, P1844, DOI 10.1364/JOSAA.8.001844
   FLOYD RW, 1976, P SID, V17, P75
   Hwang BW, 2004, LECT NOTES COMPUT SC, V3029, P473
   KAPUR JN, 1985, COMPUT VISION GRAPH, V29, P273, DOI 10.1016/0734-189X(85)90125-2
   KHELLAF A, 1991, IEEE T MED IMAGING, V10, P589, DOI 10.1109/42.108593
   Knox K. T., 1993, Journal of Electronic Imaging, V2, P185, DOI 10.1117/12.148736
   Kwak NJ, 2006, 2006 International Conference on Hybrid Information Technology, Vol 1, Proceedings, P499
   Lee HS, 2010, IEEE IMAGE PROC, P525, DOI 10.1109/ICIP.2010.5651243
   Li H, 2010, COMPUT GRAPH FORUM, V29, P273, DOI 10.1111/j.1467-8659.2009.01596.x
   Li X, 2006, IEEE SIGNAL PROC LET, V13, P688, DOI 10.1109/LSP.2006.879465
   MITSA T, 1992, J OPT SOC AM A, V9, P1920, DOI 10.1364/JOSAA.9.001920
   Neuhoff D.L., 1997, P ICASSP, V14, P1997
   Ostromoukhov V, 2001, COMP GRAPH, P567, DOI 10.1145/383259.383326
   Pang WM, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360688
   PUN T, 1981, COMPUT VISION GRAPH, V16, P210, DOI 10.1016/0146-664X(81)90038-1
   Ulichney R., 1987, DIGITAL HALFTONING
   ULICHNEY RA, 1988, P IEEE, V76, P56, DOI 10.1109/5.3288
   Velho Luiz, 1991, P 18 ANN C COMP GRAP, P81, DOI 10.1145/122718.122727
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Zhang Y., 1993, Computer Graphics Proceedings, P305, DOI 10.1145/166117.166156
   Zhou BF, 2003, ACM T GRAPHIC, V22, P437, DOI 10.1145/882262.882289
NR 26
TC 6
Z9 9
U1 0
U2 10
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2014
VL 30
IS 10
BP 1145
EP 1156
DI 10.1007/s00371-013-0895-0
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA AP6NF
UT WOS:000342193800008
DA 2024-07-18
ER

PT J
AU Luksch, C
   Tobler, RF
   Mühlbacher, T
   Schwärzler, M
   Wimmer, M
AF Luksch, Christian
   Tobler, Robert F.
   Muehlbacher, Thomas
   Schwaerzler, Michael
   Wimmer, Michael
TI Real-time rendering of glossy materials with regular sampling
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 31st CGI conference
CY JUN 10-13, 2014
CL Sydney, AUSTRALIA
DE Real-time rendering; BRDF materials; Sampling; Environment maps
ID PRECOMPUTED RADIANCE TRANSFER; FREQUENCY; REFLECTION
AB Rendering view-dependent, glossy surfaces to increase the realism in real-time applications is a computationally complex task, that can only be performed by applying some approximations-especially when immediate changes in the scene in terms of material settings and object placement are a necessity. The use of environment maps is a common approach to this problem, but implicates performance problems due to costly pre-filtering steps or expensive sampling. We, therefore, introduce a regular sampling scheme for environment maps that relies on an efficient MIP-map-based filtering step, and minimizes the number of necessary samples for creating a convincing real-time rendering of glossy BRDF materials.
C1 [Luksch, Christian; Tobler, Robert F.; Muehlbacher, Thomas; Schwaerzler, Michael] VRVis Res Ctr, A-1220 Vienna, Austria.
   [Wimmer, Michael] Vienna Univ Technol, Inst Comp Graph & Algorithms, A-1040 Vienna, Austria.
C3 Technische Universitat Wien
RP Luksch, C (corresponding author), VRVis Res Ctr, Donau City Str 1, A-1220 Vienna, Austria.
EM luksch@vrvis.at; rft@vrvis.at; muehlbacher@vrvis.at;
   schwaerzler@vrvis.at; wimmer@cg.tuwien.ac.at
OI Wimmer, Michael/0000-0002-9370-2663
FU BMVIT; BMWFJ; City of Vienna (ZIT) within the scope of COMET -
   Competence Centers for Excellent Technologies
FX The competence center VRVis is funded by BMVIT, BMWFJ, and City of
   Vienna (ZIT) within the scope of COMET - Competence Centers for
   Excellent Technologies. The program COMET is managed by FFG.
CR Adelson E. H., 1981, IEEE Computer Society Conference on Pattern Recognition and Image Processing, P218
   [Anonymous], 1982, ACM T GRAPHIC, DOI DOI 10.1145/357290.357293
   Ben-Artzi A, 2006, ACM T GRAPHIC, V25, P945, DOI 10.1145/1141911.1141979
   BLINN JF, 1976, COMMUN ACM, V19, P542, DOI 10.1145/965143.563322
   Colbert M., 2007, GPU GEMS, V3, P459
   Diefenbach P. J., 1997, Proceedings 1997 Symposium on Interactive 3D Graphics, P59, DOI 10.1145/253284.253308
   Fuhrmann A.L., 2004, P 3 INT C COMP GRAPH, P87
   HE XD, 1991, COMP GRAPH, V25, P175, DOI 10.1145/127719.122738
   Kautz J., 2002, Rendering Techniques 2002. Eurographics Workshop Proceedings, P291
   Kautz J., 2000, P EG WORKSH REND, V6
   Krivánek J, 2008, COMPUT GRAPH FORUM, V27, P1147, DOI 10.1111/j.1467-8659.2008.01252.x
   Kurt M, 2010, SIGGRAPH COMPUTER GR, V44, P1, DOI [10.1145/1722991.1722996, DOI 10.1145/1722991.1722996]
   Liu X., 2004, P EUR S REND, V1
   McAllister DavidK., 2002, PROC ACM GRAPH HARDW, P79
   Ng R, 2004, ACM T GRAPHIC, V23, P477, DOI 10.1145/1015706.1015749
   PHONG BT, 1975, COMMUN ACM, V18, P311, DOI 10.1145/360825.360839
   Ramamoorthi R, 2001, COMP GRAPH, P497, DOI 10.1145/383259.383317
   Ramantoorthi R, 2002, ACM T GRAPHIC, V21, P517, DOI 10.1145/566570.566611
   Ritschel T, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618478
   Robison Austin., 2009, HPG 09 P C HIGH PERF, P91
   Scherzer D, 2012, COMPUT GRAPH FORUM, V31, P1391, DOI 10.1111/j.1467-8659.2012.03134.x
   Schlick C., 2003, INEXPENSIVE BRDF MOD, V13, P233
   Sloan PP, 2002, ACM T GRAPHIC, V21, P527, DOI 10.1145/566570.566612
   Sun X, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239478
   Szirmay-Kalos L., 2008, Monte-Carlo Methods in Global Illumination - Photo-realistic Rendering with Randomization
   TORRANCE KE, 1967, J OPT SOC AM, V57, P1105, DOI 10.1364/JOSA.57.001105
   Tsai YT, 2006, ACM T GRAPHIC, V25, P967, DOI 10.1145/1141911.1141981
   Walter B., 2005, PCG0506
   Wang Rui., 2004, Proceedings of the Fifteenth Eurographics Conference on Rendering Techniques, EGSR'04, P345
   WARD GJ, 1992, COMP GRAPH, V26, P265, DOI 10.1145/142920.134078
NR 30
TC 5
Z9 5
U1 0
U2 5
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2014
VL 30
IS 6-8
BP 717
EP 727
DI 10.1007/s00371-014-0958-x
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA AI7GX
UT WOS:000337054700015
DA 2024-07-18
ER

PT J
AU Paulano, F
   Jiménez, JJ
   Pulido, R
AF Paulano, Felix
   Jimenez, Juan J.
   Pulido, Ruben
TI 3D segmentation and labeling of fractured bone from CT images
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 31st CGI conference
CY JUN 10-13, 2014
CL Sydney, AUSTRALIA
DE Fractured bone identification; Segmentation; Labeling; Split bone
   fragments
ID GRAPH CUTS; REGISTRATION
AB The segmentation of fractured bone from computed tomographies (CT images) is an important process in medical visualization and simulation, because it enables such applications to use data of a specific patient. On the other hand, the labeling of fractured bone usually requires the participation of an expert. Moreover, close fragment can be joined after the segmentation because of their proximity and the resolution of the CT image. Classical methods perform well in the segmentation of healthy bone, but they are not able to identify bone fragments separately. In this paper, we propose a method to segment and label bone fragments from CT images. Labeling involves the identification of bone fragments separately. The method is based on 2D region growing and requires minimal user interaction. In addition, the presented method is able to separate wrongly joined fragments during the segmentation process.
C1 [Paulano, Felix; Jimenez, Juan J.; Pulido, Ruben] Univ Jaen, Jaen 23071, Spain.
C3 Universidad de Jaen
RP Paulano, F (corresponding author), Univ Jaen, Campus Lagunillas S-N A3-103, Jaen 23071, Spain.
EM fpaulano@ujaen.es
RI Paulano, Félix/L-7254-2014; Jiménez-Delgado, Juan José/O-3213-2018
OI Jiménez-Delgado, Juan José/0000-0003-3014-0496; Paulano Godino,
   Felix/0000-0001-9712-3952
FU Ministerio de Economia y Competitividad; European Union [TIN2011-25259]
FX This work has been partially supported by the Ministerio de Economia y
   Competitividad and the European Union (via ERDF funds) through the
   research project TIN2011-25259.
CR Boykov Y, 2006, INT J COMPUT VISION, V70, P109, DOI 10.1007/s11263-006-7934-5
   Descoteaux M, 2005, LECT NOTES COMPUT SC, V3749, P9
   Egol K., 2010, HANDBOOK OF FRACTURE
   Fan HP, 2005, PATTERN RECOGN LETT, V26, P1139, DOI 10.1016/j.patrec.2004.10.010
   Fornaro J, 2010, LECT NOTES COMPUT SC, V5958, P82, DOI 10.1007/978-3-642-11615-5_9
   Justice RK, 1997, PROC SPIE, V3034, P900, DOI 10.1117/12.274179
   Lee PY, 2012, BIOMED ENG-APP BAS C, V24, P245, DOI 10.4015/S101623721250007X
   Malan DF, 2013, INT J COMPUT ASS RAD, V8, P63, DOI 10.1007/s11548-012-0671-z
   Neubauer A, 2005, INT CONGR SER, V1281, P684, DOI 10.1016/j.ics.2005.03.254
   Pettersson J, 2006, IEEE IMAGE PROC, P1185, DOI 10.1109/ICIP.2006.312695
   Sezgin M, 2004, J ELECTRON IMAGING, V13, P146, DOI 10.1117/1.1631315
   Tassani S, 2012, J BIOMECH, V45, P2035, DOI 10.1016/j.jbiomech.2012.05.019
   Tomazevic M., 2010, 12 MEDITERRANEAN C M, V2010, P430, DOI [10.1007/978-3-642-13039-7_108, DOI 10.1007/978-3-642-13039-7_108]
   Zhang J, 2010, COMPUT BIOL MED, V40, P231, DOI 10.1016/j.compbiomed.2009.11.020
NR 14
TC 37
Z9 37
U1 4
U2 30
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2014
VL 30
IS 6-8
BP 939
EP 948
DI 10.1007/s00371-014-0963-0
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA AI7GX
UT WOS:000337054700035
DA 2024-07-18
ER

PT J
AU You, LH
   Ugail, H
   Tang, BP
   Jin, XG
   You, XY
   Zhang, JJ
AF You, L. H.
   Ugail, H.
   Tang, B. P.
   Jin, Xiaogang
   You, X. Y.
   Zhang, J. J.
TI Blending using ODE swept surfaces with shape control and
   <i>C</i><SUP>1</SUP> continuity
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 31st CGI conference
CY JUN 10-13, 2014
CL Sydney, AUSTRALIA
DE Surface blending; C-1 continuity; Shape control; Fourth order ordinary
   differential equations; Analytical solution; Swept surfaces
ID FINITE-ELEMENT METHODS; N-SIDED HOLES; ROLLING BALL BLENDS; PARAMETRIC
   SURFACES; DIFFERENTIAL-EQUATIONS; SUBDIVISION SURFACES; PATCHES; PDE;
   GENERATION
AB Surface blending with tangential continuity is most widely applied in computer-aided design, manufacturing systems, and geometric modeling. In this paper, we propose a new blending method to effectively control the shape of blending surfaces, which can also satisfy the blending constraints of tangent continuity exactly. This new blending method is based on the concept of swept surfaces controlled by a vector-valued fourth order ordinary differential equation (ODE). It creates blending surfaces by sweeping a generator along two trimlines and making the generator exactly satisfy the tangential constraints at the trimlines. The shape of blending surfaces is controlled by manipulating the generator with the solution to a vector-valued fourth order ODE. This new blending methods have the following advantages: (1) exact satisfaction of continuous blending boundary constraints, (2) effective shape control of blending surfaces, (3) high computing efficiency due to explicit mathematical representation of blending surfaces, and (4) ability to blend multiple (more than two) primary surfaces.
C1 [You, L. H.; Zhang, J. J.] Bournemouth Univ, Natl Ctr Comp Animat, Poole BH12 5BB, Dorset, England.
   [Zhang, J. J.] Southwest Jiaotong Univ, Tract Power State Key Lab, Chengdu, Peoples R China.
   [Ugail, H.] Univ Bardford, Ctr Visual Comp, Bardford, W Yorkshire, England.
   [Tang, B. P.] Chongqing Univ, Coll Mech Engn, Chongqing 630044, Peoples R China.
   [Jin, Xiaogang] Zhejiang Univ, State Key Lab CAD & CG, Hangzhou 310058, Zhejiang, Peoples R China.
   [You, X. Y.] Coventry Univ, Fac Engn & Comp, Coventry, W Midlands, England.
C3 Bournemouth University; Southwest Jiaotong University; Chongqing
   University; Zhejiang University; Coventry University
RP Zhang, JJ (corresponding author), Southwest Jiaotong Univ, Tract Power State Key Lab, Chengdu, Peoples R China.
EM lyou@bournemouth.ac.uk; H.Ugail@Bradford.ac.uk; bptang@cqu.edu.cn;
   jin@cad.zju.edu.cn; lawrenceyoux@googlemail.com;
   jzhang@bournemouth.ac.uk
RI 汤, 宝平/HJP-3934-2023
FU UK Royal Society International Exchanges Scheme [IE131367]; National
   Natural Science Foundation of China [61272298, 61328204]
FX This research is supported by the grant of 2013 UK Royal Society
   International Exchanges Scheme(Grant No. IE131367). Xiaogang Jin was
   supported by the National Natural Science Foundation of China (Grant No.
   61272298), and the Joint Research Fund for Overseas Chinese, Hong Kong
   and Macao Young Scientists of the National Natural Science Foundation of
   China (Grant No. 61328204).
CR Barnhill R.E., 1993, COMPUTING S, V8, P1
   Bloor M.I.G., 1993, COMPUTING S, V8, P21
   Bloor MIG, 2005, COMPUT AIDED GEOM D, V22, P203, DOI 10.1016/j.cagd.2004.08.005
   BLOOR MIG, 1989, COMPUT AIDED DESIGN, V21, P165, DOI 10.1016/0010-4485(89)90071-7
   Bloor MIG, 2000, MATH COMPUT MODEL, V31, P1, DOI 10.1016/S0895-7177(99)00212-5
   Chaudhry E, 2013, COMPUT GRAPH-UK, V37, P638, DOI 10.1016/j.cag.2013.06.001
   CHOI BK, 1989, COMPUT AIDED DESIGN, V21, P213, DOI 10.1016/0010-4485(89)90046-8
   Chuang JH, 1995, VISUAL COMPUT, V11, P513, DOI 10.1007/BF02434038
   Chuang JH, 1997, VISUAL COMPUT, V13, P316, DOI 10.1007/s003710050106
   Farouki RAMT, 1996, COMPUT AIDED DESIGN, V28, P871, DOI 10.1016/0010-4485(96)00008-5
   Hsu KL, 1998, IEEE COMPUT GRAPH, V18, P72, DOI 10.1109/38.637308
   Hwang WC, 2003, J INF SCI ENG, V19, P857
   Koparkar P., 1991, Proceedings. Symposium on Solid Modeling Foundations and CAD/CAM Applications, P317, DOI 10.1145/112515.112557
   Kós G, 2000, COMPUT AIDED GEOM D, V17, P127, DOI 10.1016/S0167-8396(99)00043-6
   Krasauskas R, 2008, COMPUT AIDED GEOM D, V25, P332, DOI 10.1016/j.cagd.2007.11.005
   Li GQ, 2002, J COMPUT SCI TECHNOL, V17, P498, DOI 10.1007/BF02943290
   Li ZC, 1998, J COMPUT MATH, V16, P457
   Li ZC, 1999, J COMPUT APPL MATH, V110, P241, DOI 10.1016/S0377-0427(99)00231-9
   Li ZC, 1999, J COMPUT APPL MATH, V110, P155, DOI 10.1016/S0377-0427(99)00208-3
   Lukacs G, 1998, COMPUT AIDED GEOM D, V15, P585, DOI 10.1016/S0167-8396(98)00006-5
   Piegl LA, 1999, VISUAL COMPUT, V15, P77, DOI 10.1007/s003710050163
   Rossignac J. R., 1984, Computers in Mechanical Engineering, V3, P65
   SCHICHTEL M, 1993, IEEE COMPUT GRAPH, V13, P68, DOI 10.1109/38.232100
   Shi KL, 2010, VISUAL COMPUT, V26, P791, DOI 10.1007/s00371-010-0468-4
   VIDA J, 1994, COMPUT AIDED DESIGN, V26, P341, DOI 10.1016/0010-4485(94)90023-X
   Whited B, 2009, COMPUT AIDED DESIGN, V41, P456, DOI 10.1016/j.cad.2009.02.008
   Yang YJ, 2006, COMPUT AIDED DESIGN, V38, P1166, DOI 10.1016/j.cad.2006.07.001
   You LH, 2008, COMPUT ANIMAT VIRT W, V19, P433, DOI 10.1002/cav.235
   You LH, 2011, COMPUT AIDED DESIGN, V43, P720, DOI 10.1016/j.cad.2011.01.021
   You LH, 2010, COMPUT ANIMAT VIRT W, V21, P297, DOI 10.1002/cav.352
   You LH, 2004, COMPUT GRAPH-UK, V28, P895, DOI 10.1016/j.cag.2004.08.003
   You LH, 2004, VISUAL COMPUT, V20, P199, DOI 10.1007/s00371-003-0241-7
   Zhou P, 2010, COMPUT AIDED GEOM D, V27, P233, DOI 10.1016/j.cagd.2010.01.001
   Zhou P, 2009, COMPUT AIDED DESIGN, V41, P812, DOI 10.1016/j.cad.2009.02.020
NR 34
TC 11
Z9 11
U1 0
U2 16
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2014
VL 30
IS 6-8
BP 625
EP 636
DI 10.1007/s00371-014-0950-5
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA AI7GX
UT WOS:000337054700007
DA 2024-07-18
ER

PT J
AU Su, Z
   Luo, XN
   Artusi, A
AF Su, Zhuo
   Luo, Xiaonan
   Artusi, Alessandro
TI A novel image decomposition approach and its applications
SO VISUAL COMPUTER
LA English
DT Article
DE Edge-preserving; Texture smoothing; Oscillation; Joint bilateral filter;
   Structure-texture decomposition
ID PHOTOGRAPHY; ENHANCEMENT; FLASH; TONE
AB The current state-of-the-art edge-preserving decomposition techniques may not be able to fully separate textures while preserving edges. This may generate artifacts in some applications, e.g., edge detection, texture transfer, etc. To solve this problem, a novel image decomposition approach based on explicit texture separation from large scale components of an image is presented. We first apply a Gaussian structure-texture decomposition, to separate the majority of textures out of the input image. However, residual textures are still visible around the strong edges. To remove these residuals, an asymmetric sampling operator is proposed and followed by a joint bilateral correction to remove an excessive blur effect. We demonstrate that our approach is well suited for the tasks such as texture transfer, edge detection, non-photorealistic rendering, and tone mapping. The results show our approach outperforms existing state-of-the-art image decomposition approaches.
C1 [Su, Zhuo; Luo, Xiaonan] Sun Yat Sen Univ, Natl Engn Res Ctr Digital Life, State Prov Joint Lab Digital Home Interact Applic, Sch Informat Sci & Technol, Guangzhou 510006, Guangdong, Peoples R China.
   [Artusi, Alessandro] Univ Girona, Graph & Imaging Lab, Girona 17071, Spain.
C3 Sun Yat Sen University; Universitat de Girona
RP Su, Z (corresponding author), Sun Yat Sen Univ, Natl Engn Res Ctr Digital Life, State Prov Joint Lab Digital Home Interact Applic, Sch Informat Sci & Technol, Guangzhou 510006, Guangdong, Peoples R China.
EM suzhuoi@gmail.com; lnslxn@mail.sysu.edu.cn;
   artusialessandro4@googlemail.com
RI Su, Zhuo/AAO-4506-2020; Artusi, Alessandro/H-4102-2019
OI Su, Zhuo/0000-0002-6090-0110; Artusi, Alessandro/0000-0002-4502-663X
FU NSFC-Guang-dong Joint Fund [U0935004, U1135003]; National Key Technology
   RD Program [2011BAH27B01]; National Science Fund of China [61262050,
   61202293]; Ministry of Science and Inovation Subprograme Ramon Y Cajal
   [RYC-2011-09372]
FX This research is supported by NSFC-Guang-dong Joint Fund (U0935004,
   U1135003), the National Key Technology R&D Program (2011BAH27B01), the
   National Science Fund of China (61262050, 61202293), and Ministry of
   Science and Inovation Subprograme Ramon Y Cajal RYC-2011-09372. Thanks
   to Antoni Buades, Zeev Farbman, Kaiming He, Michael Kass, Sylvain Paris,
   Michael Rubinstein, and Kartic Subr for providing their experiments and
   data in this work.
CR Alliney S, 1997, IEEE T SIGNAL PROCES, V45, P913, DOI 10.1109/78.564179
   [Anonymous], IEEE C COMP VIS PATT
   [Anonymous], DIGITAL IMAGE PROCES
   Aujol JF, 2006, INT J COMPUT VISION, V67, P111, DOI 10.1007/s11263-006-4331-z
   Aujol JF, 2005, INT J COMPUT VISION, V63, P85, DOI 10.1007/s11263-005-4948-3
   Bae SM, 2006, ACM T GRAPHIC, V25, P637, DOI 10.1145/1141911.1141935
   Baek J, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866191
   Bhat P, 2008, LECT NOTES COMPUT SC, V5303, P114, DOI 10.1007/978-3-540-88688-4_9
   Bhat P, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1731047.1731048
   Buades A, 2010, IEEE T IMAGE PROCESS, V19, P1978, DOI 10.1109/TIP.2010.2046605
   Chen J., ACM T GRAPH, V26
   Choudhury P., 2003, Eurographics Symposium on Rendering. 14th Eurographics Workshop on Rendering, P186
   Durand F, 2002, ACM T GRAPHIC, V21, P257, DOI 10.1145/566570.566574
   Eisemann E, 2004, ACM T GRAPHIC, V23, P673, DOI 10.1145/1015706.1015778
   Farbman Z, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866171
   Farbman Z, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360666
   Fattal R., ACM T GRAPH, V26
   Fattal R, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531328
   He KM, 2010, LECT NOTES COMPUT SC, V6311, P1
   Kass M, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778837
   LEV A, 1977, IEEE T SYST MAN CYB, V7, P435, DOI 10.1109/TSMC.1977.4309740
   Meyer Y., 2001, Memoirs of the American Mathematical Society
   Nikolova M, 2004, J MATH IMAGING VIS, V20, P99, DOI 10.1023/B:JMIV.0000011920.58935.9c
   Osher S, 2003, MULTISCALE MODEL SIM, V1, P349, DOI 10.1137/S1540345902416247
   Paris S., 2010, ACM T GRAPH P ACM SI
   Paris S., 2008, COMPUTER GRAPHICS VI
   Paris S, 2009, INT J COMPUT VISION, V81, P24, DOI 10.1007/s11263-007-0110-8
   Petschnigg G, 2004, ACM T GRAPHIC, V23, P664, DOI 10.1145/1015706.1015777
   Peyre G, 2010, NUMERICAL TOUR SIGNA
   Rubinstein M, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866186
   RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F
   Subr K, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618493
   Tomasi C, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P839, DOI 10.1109/ICCV.1998.710815
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Winnemöller H, 2006, ACM T GRAPHIC, V25, P1221, DOI 10.1145/1141911.1142018
   Xie Z., 2012, VISUAL COMPUT, P1
   Xu L., 2011, SIGGRAPH AS 2011, P2011
   Yang QX, 2009, PROC CVPR IEEE, P557, DOI 10.1109/CVPRW.2009.5206542
   Zhao HL, 2008, VISUAL COMPUT, V24, P727, DOI 10.1007/s00371-008-0254-8
NR 39
TC 12
Z9 14
U1 0
U2 15
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2013
VL 29
IS 10
SI SI
BP 1011
EP 1023
DI 10.1007/s00371-012-0753-5
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 227LV
UT WOS:000325115400003
DA 2024-07-18
ER

PT J
AU Wong, FJ
   Takahashi, S
AF Wong, Fernando J.
   Takahashi, Shigeo
TI Abstracting images into continuous-line artistic styles
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Computer Graphics International (CGI) Conference
CY 2013
CL Hanover, GERMANY
DE Continuous-line art; Line drawing; Image abstraction; Non-photorealistic
   rendering
AB This paper focuses on the problem of designing and generating illustrations that portray a given scene with a single non-intersecting line. In this approach, users partition an image into regions, assigning a type or style to each of them. Next, a grid is generated over the drawing space, based on the parameters specified for each region. The illustration is then obtained in the form of a path that covers most areas of the grid. Contrary to previous works, our approach allows users to control the overall flow of the line throughout any given region, by providing the means to define tensor fields per region which directly influence the line orientation. We also extend this work for generating continuous-line paintings, a similar style consisting of a single line that varies in color and thickness while covering the entire drawing space. This is achieved by transforming drawings obtained with the above-mentioned approach through a Voronoi-based strategy.
C1 [Takahashi, Shigeo] Univ Tokyo, Grad Sch Frontier Sci, Tokyo, Japan.
   [Wong, Fernando J.] Univ Tokyo, Tokyo, Japan.
C3 University of Tokyo; University of Tokyo
RP Takahashi, S (corresponding author), Univ Tokyo, Grad Sch Frontier Sci, Tokyo, Japan.
EM fjwong@visual.k.u-tokyo.ac.jp; takahashis@acm.org
OI Takahashi, Shigeo/0000-0002-4673-577X
FU Japan Society for the Promotion of Science [21300033, 23650042];
   Grants-in-Aid for Scientific Research [21300033, 23650042] Funding
   Source: KAKEN
FX We thank all the participants in our preliminary user study for helping
   us in this algorithm formulation. This research has been partially
   supported by Japan Society for the Promotion of Science under
   Grants-in-Aid for Scientific Research (B) No. 21300033, and Challenging
   Exploratory Research No. 23650042.
CR Alliez P, 2003, ACM T GRAPHIC, V22, P485, DOI 10.1145/882262.882296
   [Anonymous], 2007, Princeton Series in Applied Mathematics
   Applegate D., 2011, Concorde TSP solver
   Bosch R, 2004, OPER RES LETT, V32, P302, DOI 10.1016/j.orl.2003.10.001
   Chi MT, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360661
   Cohen-Or D, 2006, ACM T GRAPHIC, V25, P624, DOI 10.1145/1141911.1141933
   Cox IJ, 2008, MKS MULTIMED INFORM, P425, DOI 10.1016/B978-012372585-1.50015-2
   Douglas D.H., 1973, Cartographica: The International Journal for Geographic Information and Geovisualization, V10, P112, DOI [https://doi.org/10.3138/FM57-6770-U75U-7727, DOI 10.1002/9780470669488.CH2]
   Garey M. R., 1979, Computers and intractability. A guide to the theory of NP-completeness
   Granger G, 2009, EOL B SPLINE LIB
   Huang L, 2003, PROC INT CONF DOC, P780
   Jobard Bruno., 1997, VISUALIZATION SCI CO, P43, DOI DOI 10.1007/978-3-7091-6876-9_5
   Kang H, 2009, IEEE T VIS COMPUT GR, V15, P62, DOI 10.1109/TVCG.2008.81
   Karplan C. S., 2005, Renaissance Banff: Bridges 2005: Mathematical Connections in Art, Music, and Science, P301
   Li Y, 2004, ACM T GRAPHIC, V23, P303, DOI 10.1145/1015706.1015719
   Maharik R, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964995
   O'Donovan P, 2012, IEEE T VIS COMPUT GR, V18, P475, DOI 10.1109/TVCG.2011.51
   Okamoto Y., 2009, CCCG, P137
   Orzan A, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360691
   Pedersen Hans., 2006, P 4 INT S NONPHOTORE, P79
   Pullen W. D, 2008, THINK LABYRINTH
   Ramer U., 1972, Comput. Graph. Image Process., V1, P244, DOI DOI 10.1016/S0146-664X(72)80017-0
   Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720
   Slater G, 2001, G SLATER KILTED SING
   Wan L, 2010, IEEE T VIS COMPUT GR, V16, P287, DOI 10.1109/TVCG.2009.85
   Wong FJ, 2011, COMPUT GRAPH FORUM, V30, P1931, DOI 10.1111/j.1467-8659.2011.02040.x
   Wong FJ, 2009, COMPUT GRAPH FORUM, V28, P1975, DOI 10.1111/j.1467-8659.2009.01576.x
   Xu J, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239480
   Zhang E, 2007, IEEE T VIS COMPUT GR, V13, P94, DOI 10.1109/TVCG.2007.16
NR 29
TC 10
Z9 12
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2013
VL 29
IS 6-8
BP 729
EP 738
DI 10.1007/s00371-013-0809-1
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Arts &amp; Humanities Citation Index (A&amp;HCI); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 151RR
UT WOS:000319478400026
DA 2024-07-18
ER

PT J
AU Ihmsen, M
   Akinci, N
   Akinci, G
   Teschner, M
AF Ihmsen, Markus
   Akinci, Nadir
   Akinci, Gizem
   Teschner, Matthias
TI Unified spray, foam and air bubbles for particle-based fluids
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Conference on the Computer Graphics International (CGI)
CY 2012
CL Bournemouth, ENGLAND
DE Animation; Fluids; Particles
ID ANIMATION; SMOKE; WATER; SPH
AB We present a new model for diffuse material, i.e. water-air mixtures, that can be combined with particle-based fluids. Diffuse material is uniformly represented with particles which are classified into spray, foam and air bubbles. Physically motivated rules are employed to generate, advect and dissipate diffuse material. The approach is realized as a post-processing step which enables efficient processing and versatile handling. As interparticle forces and the influence of diffuse material onto the fluid are neglected, large numbers of diffuse particles are efficiently processed to realize highly detailed small-scale effects. The presented results show that our approach can significantly improve the visual realism of large-scale fluid simulations.
C1 [Ihmsen, Markus; Akinci, Nadir; Akinci, Gizem; Teschner, Matthias] Univ Freiburg, D-79110 Freiburg, Germany.
C3 University of Freiburg
RP Ihmsen, M (corresponding author), Univ Freiburg, Georges Koehler Allee 052, D-79110 Freiburg, Germany.
EM ihmsen@informatik.uni-freiburg.de
RI Kayar, Gizem/GRF-6167-2022
OI Kayar, Gizem/0000-0002-7811-9357
CR [Anonymous], REALFL 2012 HYBR WHI
   [Anonymous], 453 CW KATH U
   [Anonymous], SIGGRAPH 2006 SKETCH
   [Anonymous], SIGGRAPH PANEL
   Bagar F, 2010, COMPUT GRAPH FORUM, V29, P1383, DOI 10.1111/j.1467-8659.2010.01734.x
   Bredow Rob., 2007, ACM SIGGRAPH 2007 courses, SIGGRAPH'07, P1
   Chentanez N., 2011, ACM T GRAPHIC, V30
   Chentanez Nuttapong., 2010, P 2010 ACM SIGGRAPH, P197
   Foster N, 2001, COMP GRAPH, P23, DOI 10.1145/383259.383261
   Greenwood S., 2004, Proceedings of the 2004 ACM SIGGRAPH/Eurographics symposium on Computer animation, P287
   Guendelman E, 2005, ACM T GRAPHIC, V24, P973, DOI 10.1145/1073204.1073299
   Hong JM, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360647
   Ihmsen M., 2010, P VRIPHYS, P79
   Ihmsen M, 2011, GRAPP 2011: PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON COMPUTER GRAPHICS THEORY AND APPLICATIONS, P225
   Ihmsen M, 2011, COMPUT GRAPH FORUM, V30, P99, DOI [10.1111/j.1467-8659.2010.01832.x, 10.1111/j.1467-8659.2010.01834.x]
   Irving G, 2006, ACM T GRAPHIC, V25, P805, DOI 10.1145/1141911.1141959
   Kim J., 2006, Proc ACM SIGGRAPH/Eurograph Symp Comp Anim, SCA '06, P335
   Klingner BM, 2006, ACM T GRAPHIC, V25, P820, DOI 10.1145/1141911.1141961
   Losasso F, 2004, ACM T GRAPHIC, V23, P457, DOI 10.1145/1015706.1015745
   Losasso F, 2008, IEEE T VIS COMPUT GR, V14, P797, DOI 10.1109/TVCG.2008.37
   Mihalef V, 2007, COMPUT GRAPH FORUM, V26, P457, DOI 10.1111/j.1467-8659.2007.01068.x
   Mihalef V, 2009, COMPUT GRAPH FORUM, V28, P229, DOI 10.1111/j.1467-8659.2009.01362.x
   MONAGHAN JJ, 1992, ANNU REV ASTRON ASTR, V30, P543, DOI 10.1146/annurev.aa.30.090192.002551
   Muller M., 2003, ACM SIGGRAPH/Eurographics Symposium on Computer Animation, P154
   Muller M, 2005, P 2005 ACM SIGGRAPH, P237, DOI DOI 10.1145/1073368.1073402
   Raveendran K., 2011, P 2011 ACM SIGGRAPHE, P33
   Sirignano W.A., 1999, FLUID DYNAMICS TRANS, DOI 10.1017/CBO9780511529566
   Solenthaler B, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531346
   Solenthaler B, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964976
   Takahashi T, 2003, COMPUT GRAPH FORUM, V22, P391, DOI 10.1111/1467-8659.00686
   Thürey N, 2007, PACIFIC GRAPHICS 2007: 15TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, P39, DOI 10.1109/PG.2007.33
   Thurey Nils, 2006, P 2006 ACM SIGGRAPHE, P157
   Zhu YN, 2005, ACM T GRAPHIC, V24, P965, DOI 10.1145/1073204.1073298
NR 33
TC 58
Z9 70
U1 0
U2 12
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2012
VL 28
IS 6-8
BP 669
EP 677
DI 10.1007/s00371-012-0697-9
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 947EW
UT WOS:000304411500015
DA 2024-07-18
ER

PT J
AU Nguyen, BP
   Tay, WL
   Chui, CK
   Ong, SH
AF Nguyen, Binh P.
   Tay, Wei-Liang
   Chui, Chee-Kong
   Ong, Sim-Heng
TI A clustering-based system to automate transfer function design for
   medical image visualization
SO VISUAL COMPUTER
LA English
DT Article
DE Transfer function design; Volume rendering; LH histogram; Clustering
ID MULTIDIMENSIONAL TRANSFER-FUNCTIONS; VOLUME EXPLORATION; CLASSIFICATION;
   SIZE
AB Finding good transfer functions for rendering medical volumes is difficult, non-intuitive, and time-consuming. We introduce a clustering-based framework for the automatic generation of transfer functions for volumetric data. The system first applies mean shift clustering to oversegment the volume boundaries according to their low-high (LH) values and their spatial coordinates, and then uses hierarchical clustering to group similar voxels. A transfer function is then automatically generated for each cluster such that the number of occlusions is reduced. The framework also allows for semi-automatic operation, where the user can vary the hierarchical clustering results or the transfer functions generated. The system improves the efficiency and effectiveness of visualizing medical images and is suitable for medical imaging applications.
C1 [Nguyen, Binh P.; Tay, Wei-Liang; Ong, Sim-Heng] Natl Univ Singapore, Dept Elect & Comp Engn, Singapore 117548, Singapore.
   [Chui, Chee-Kong] Natl Univ Singapore, Dept Mech Engn, Singapore 117548, Singapore.
   [Ong, Sim-Heng] Natl Univ Singapore, Div Bioengn, Singapore 117548, Singapore.
C3 National University of Singapore; National University of Singapore;
   National University of Singapore
RP Nguyen, BP (corresponding author), Natl Univ Singapore, Dept Elect & Comp Engn, Singapore 117548, Singapore.
EM phubinh@nus.edu.sg; tayweiliang@nus.edu.sg; mpecck@nus.edu.sg;
   eleongsh@nus.edu.sg
RI Ong, Sim-Heng/R-9244-2019; Nguyen, Binh P./N-8193-2013
OI Ong, Sim-Heng/0000-0003-2766-8150; Nguyen, Binh P./0000-0001-6203-6664
CR Bajaj CL, 1997, VISUALIZATION '97 - PROCEEDINGS, P167, DOI 10.1109/VISUAL.1997.663875
   Chan MY, 2009, IEEE T VIS COMPUT GR, V15, P1283, DOI 10.1109/TVCG.2009.172
   Correa C. D., 2010, IEEE T VISUALIZATION, V17, P1077
   Correa CD, 2008, IEEE T VIS COMPUT GR, V14, P1380, DOI 10.1109/TVCG.2008.162
   Correa CD, 2009, IEEE T VIS COMPUT GR, V15, P1465, DOI 10.1109/TVCG.2009.189
   Correa CD, 2009, IEEE PAC VIS SYMP, P177, DOI 10.1109/PACIFICVIS.2009.4906854
   Duda R. O., 2000, PATTERN CLASSIFICATI
   FUKUNAGA K, 1975, IEEE T INFORM THEORY, V21, P32, DOI 10.1109/TIT.1975.1055330
   Hadwiger M, 2008, IEEE T VIS COMPUT GR, V14, P1507, DOI 10.1109/TVCG.2008.147
   Hladuvka J., 2000, SPRING C COMPUTER GR, V16, P58
   Hong DH, 2003, J ELECTRON IMAGING, V12, P470, DOI 10.1117/1.1579698
   Huang RZ, 2003, 11TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P355
   Kindlmann G, 2003, IEEE VISUALIZATION 2003, PROCEEDINGS, P513, DOI 10.1109/VISUAL.2003.1250414
   Kindlmann G, 1998, IEEE SYMPOSIUM ON VOLUME VISUALIZATION, P79, DOI 10.1109/SVV.1998.729588
   KINDLMANN G, 2002, ACM SIGGRAPH
   Kniss J, 2002, IEEE T VIS COMPUT GR, V8, P270, DOI 10.1109/TVCG.2002.1021579
   Kniss J, 2001, IEEE VISUAL, P255, DOI 10.1109/VISUAL.2001.964519
   Lum EB, 2004, IEEE VISUALIZATION 2004, PROCEEEDINGS, P289, DOI 10.1109/VISUAL.2004.64
   Lundstrom C., 2005, Eurographics Conference on Visualization (EuroVis), P263, DOI DOI 10.2312/VISSYM/EUROVIS05/263-270
   Lundstrom C., 2006, Proc. EuroVis, P227
   Lundström C, 2006, IEEE T VIS COMPUT GR, V12, P1570, DOI 10.1109/TVCG.2006.100
   Maciejewski R, 2009, IEEE T VIS COMPUT GR, V15, P1473, DOI 10.1109/TVCG.2009.185
   Nguyen B.P., 2011, P COMP GRAPH INT
   Pekar V, 2001, IEEE VISUAL, P223, DOI 10.1109/VISUAL.2001.964515
   Pinto Franciscode Moura., 2007, Eurographics IEEE-VGTC Symposium on Visualization, P131, DOI DOI 10.2312/VISSYM/EUROVIS07/131-1382
   Prani J. S., 2009, P 14 INT FALL WORKSH, P285
   Roettger S., 2005, Eurographics IEEE VGTC Symposium on Visualization, P271, DOI DOI 10.2312/VISSYM/EUROVIS05/271-278
   Sereda P, 2006, IEEE T VIS COMPUT GR, V12, P208, DOI 10.1109/TVCG.2006.39
   Sereda P., 2006, Proceedings of Eurographics/IEEE VGTC Symp on Visualization, P243
   Tappenbeck Andreas., 2006, SimVis, P259
   Teistler M, 2007, INT J COMPUT ASS RAD, V2, P55, DOI 10.1007/s11548-007-0079-3
   Tzeng F.-Y., 2004, S DATA VISUALISATION, P17, DOI DOI 10.2312/VISSYM/VISSYM04/017-024
   Tzeng FY, 2005, IEEE T VIS COMPUT GR, V11, P273, DOI 10.1109/TVCG.2005.38
   Tzeng FY, 2003, IEEE VISUALIZATION 2003, PROCEEDINGS, P505, DOI 10.1109/VISUAL.2003.1250413
   Wesarg S, 2010, INT J COMPUT ASS RAD, V5, P655, DOI 10.1007/s11548-010-0480-1
   Wesarg S., 2009, BILDVERARBEITUNG MED, P16
   Zhou FF, 2010, LECT NOTES COMPUT SC, V6357, P67, DOI 10.1007/978-3-642-15948-0_9
NR 37
TC 24
Z9 26
U1 0
U2 7
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2012
VL 28
IS 2
BP 181
EP 191
DI 10.1007/s00371-011-0634-3
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 881SW
UT WOS:000299510100005
DA 2024-07-18
ER

PT J
AU Bergen, S
   Ross, BJ
AF Bergen, Steven
   Ross, Brian J.
TI Automatic and interactive evolution of vector graphics images with
   genetic algorithms
SO VISUAL COMPUTER
LA English
DT Article
DE Genetic algorithm; Vector graphics; Evolutionary art
ID ART
AB Vector graphics are popular in illustration and graphic design. Images are composed of discrete geometric shapes, such as circles, squares, and lines. The generation of vector images by evolutionary computation techniques, however, has been given little attention. JNetic is an implementation of a comprehensive evolutionary vector graphics tool. Vector primitives available range from simple geometric shapes (circles, polygons) to spline-based paint strokes. JNetic supports automatic and user-guided evolution, chromosome editing, and high-detail masks. Automatic evolution involves measuring the pixel-by-pixel colour distance between a candidate and target image. Masks can be painted over areas of the target image, which help reproduce the high-detail features within those areas. By creative selection of primitives and colour schemes, stylized interpretations of target images are produced. The system has been successfully used by the authors as a creative tool.
C1 [Bergen, Steven; Ross, Brian J.] Brock Univ, Dept Comp Sci, St Catharines, ON L2S 3A1, Canada.
C3 Brock University
RP Ross, BJ (corresponding author), Brock Univ, Dept Comp Sci, 500 Glenridge Ave, St Catharines, ON L2S 3A1, Canada.
EM sb04qv@brocku.ca; bross@brocku.ca
FU NSERC USRA; NSERC [138467]
FX Supported by NSERC USRA and NSERC Operating Grant 138467.
CR [Anonymous], 1994, TEXTURING MODELING P
   [Anonymous], P 14 BRAZ S AI BERL
   BACHELIER G, 2008, ART ARTIFICIAL EVOLU
   BARILE P, 2009, P GECCO 2009
   Bentley P., 2002, Creative evolutionary systems
   Bergen S., 2010, GENETIC PROGRAMMING, VVIII
   BOUDREAU T., 2002, NetBeans: the definitive guide
   BUCKLEY R, 1995, GRAPHICS GEMS, P65
   Burton AR, 1999, COMPUT MUSIC J, V23, P59, DOI 10.1162/014892699560001
   Dawkins R., 1996, The Blind Watchmaker, Vthird
   Dorin A, 2005, LECT NOTES COMPUT SC, V3449, P448
   DORIN A, 2001, LECT NOTES ARTIF INT, P659
   EIBEN AE, 2008, ART ARTIFICIAL EVOLU
   ELLIOT J, 2002, JAVA SWING
   FROWD CD, 2008, ART ARTIFICIAL EVOLU
   Gervautz M., 1990, GLASSNER GRAPHICS GE, P287, DOI [10.1016/B978-0-08-050753-8.50061-9, DOI 10.1016/B978-0-08-050753-8.50061-9]
   Goldberg David E, 1989, GENETIC ALGORITHMS S
   GRAF J, 1995, COM ADAP SY, P53
   Greenfeld GR, 2003, IEEE C EVOL COMPUTAT, P1903
   Greenfield GR, 2000, LEONARDO, V33, P93, DOI 10.1162/0024094000552333
   Holland I.H., 1975, ADAPTATION NATURAL A
   IBRAHIM AEM, 1998, THESIS TEXAS A M U
   Jackson H., 2002, Creative Evolutionary Systems, P299
   Klawonn F., 2008, Introduction to Computer Graphics
   LEWIS M, 2000, P GEN ART 2000
   Neufeld C., 2008, ART ARTIFICIAL EVOLU
   O'Neill Michael., 2009, Proceedings of the 11th Conference on Genetic and Evolutionary Computation, P1035, DOI DOI 10.1145/1569901.1570041
   Romero Juan J, 2008, The art of artificial evolution: A handbook on evolutionary art and music
   ROOKE S, 2002, CREATIVE EVOLUTIONAR, P330
   ROSS BJ, 2006, CEC 2006
   Sims K., 1993, Visual Computer, V9, P466, DOI 10.1007/BF01888721
   SUN H, 2007, ACM T GRAPH, V26
   Svangård N, 2004, LECT NOTES COMPUT SC, V3005, P447
   SWAMINARAYAN S, 2006, P 35 APPL IM PATT RE, P28
   Todd S., 1992, Evolutionary art and computers
   WELLER C, 2002, GENERATION VECTOR BA
   WHITELAW M, 2002, CREATIVE EVOLUTIONAR, P129
   Wiens AL, 2002, COMPUT GRAPH-UK, V26, P75, DOI 10.1016/S0097-8493(01)00159-5
   WIJESINGHE G, 2008, CEC 2008, P2739
   WILKENS S, 2005, RENDERING NONP UNPUB
   Xia T, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618461
NR 41
TC 12
Z9 13
U1 0
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2012
VL 28
IS 1
BP 35
EP 45
DI 10.1007/s00371-011-0597-4
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Arts &amp; Humanities Citation Index (A&amp;HCI)
SC Computer Science
GA 874YQ
UT WOS:000298995000004
DA 2024-07-18
ER

PT J
AU Friese, KI
   Blanke, P
   Wolter, FE
AF Friese, Karl-Ingo
   Blanke, Philipp
   Wolter, Franz-Erich
TI YaDiV-an open platform for 3D visualization and 3D segmentation of
   medical data
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 3DAH Conference
CY MAY 23-24, 2010
CL Chania, GREECE
DE Medical visualization; Medical segmentation; Virtual reality; Haptics
AB In this work, we present the concept, design and implementation of a new software to visualize and segment 3-dimensional medical data. The main goal was to create a platform that would allow trying out new approaches and ideas while staying independent from hardware and operating system, being especially useful for interdisciplinary research groups. A special focus will be given on fast and interactive volume visualization, and a survey on the use of Virtual Reality (VR) and especially haptic/force feedback in medical applications will be provided.
   The software will be published as Open Source and therefore be available as a rapid prototyping platform for own ideas and plugins for all members of the scientific community.
C1 [Friese, Karl-Ingo] Leibniz Univ Hannover, Dept Comp Graph, Inst Man Machine Commun, Hannover, Germany.
C3 Leibniz University Hannover
RP Friese, KI (corresponding author), Leibniz Univ Hannover, Dept Comp Graph, Inst Man Machine Commun, Welfengarten 1, Hannover, Germany.
EM kif@welfenlab.de; blanke@welfenlab.de; few@welfenlab.de
RI Wolter, Franz-Erich/AAV-3008-2020; Wolter, Franz-Erich/JAC-5956-2023;
   Wolter, Franz - Erich/B-1672-2014
OI Wolter, Franz-Erich/0000-0002-2293-5494; Wolter,
   Franz-Erich/0000-0002-2293-5494; Friese, Karl-Ingo/0000-0002-9286-5460
CR Aljabar P, 2009, NEUROIMAGE, V46, P726, DOI 10.1016/j.neuroimage.2009.02.018
   [Anonymous], 2002, Hacker's Delight
   [Anonymous], 2001, P GRAPHICS INTERFACE
   Bruckner S, 2007, COMPUT GRAPH FORUM, V26, P715, DOI 10.1111/j.1467-8659.2007.01095.x
   Chan TF, 2001, IEEE T IMAGE PROCESS, V10, P266, DOI 10.1109/83.902291
   FRIESE KI, 2010, THESIS LEIBNIZ U HAN
   Lorensen William E., 1987, COMPUT GRAPH, P163, DOI DOI 10.1145/37402.37422
   MCCORMICK BH, 1987, ACM COMPUT GRAPH, V21
   OSHER S, 1988, J COMPUT PHYS, V79, P12, DOI 10.1016/0021-9991(88)90002-2
   Rohlfing T, 2005, HDB MED IMAGE ANAL, V3, P435
   Schroeder W., 1997, VISUALIZATION TOOLKI
   van der Meijden OAJ, 2009, SURG ENDOSC, V23, P1180, DOI 10.1007/s00464-008-0298-x
NR 12
TC 21
Z9 23
U1 0
U2 7
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2011
VL 27
IS 2
SI SI
BP 129
EP 139
DI 10.1007/s00371-010-0539-6
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 722QR
UT WOS:000287449900006
DA 2024-07-18
ER

PT J
AU Luo, JL
   Qin, KH
   Zhou, YX
   Mao, MA
   Li, RR
AF Luo, Jianli
   Qin, Kaihuai
   Zhou, Yanxia
   Mao, Miao
   Li, Ruirui
TI GPU rendering for tiled multi-projector autostereoscopic display based
   on chromium
SO VISUAL COMPUTER
LA English
DT Article
DE Projector; Autostereoscopic display; Real-time rendering; GPU (Graphics
   Processing Unit); Non-invasive
AB In this paper, a GPU-based high-resolution multiview rendering approach (HRMVRA) is presented and incorporated into Chromium, and then a tiled multi-projector autostereoscopic display system (TMPADS) based on HRMVRA is constructed to provide an immersing 3D perception and a compelling sense of presence without the need of glasses for viewers. HRMVRA renders the multiview images in real time in only one pass, though the traditional multiview rendering approaches based on Chromium render the multiviews in multiple passes. The hardware of the autostereoscopic display system consists of a front-projection screen that covers an area of 360x160 square centimeters, twenty four projectors and thirteen computers connected with the gigabit Ethernet. TMPADS is well scalable since both the resolution and the number of the rendered views are configurable. It is shown by the experiments that HRMVRA has more than five times performance of the traditional high-resolution multiview parallax rendering based on Chromium. Most existing single-view OpenGL applications (e.g., some games like Quake III) can run directly on TMPADS without any source-code modification or re-compiling.
C1 [Luo, Jianli; Qin, Kaihuai; Zhou, Yanxia; Mao, Miao; Li, Ruirui] Tsinghua Univ, Dept Comp Sci, Beijing 100084, Peoples R China.
C3 Tsinghua University
RP Qin, KH (corresponding author), Tsinghua Univ, Dept Comp Sci, Beijing 100084, Peoples R China.
EM qkh-dcs@tsinghua.edu.cn
FU National High Technology Research and Development Program ("863"
   Program) of China [2006AA01Z326]
FX This work is supported by the National High Technology Research and
   Development Program ("863" Program) of China (No. 2006AA01Z326).
CR ANNEN T, 2006, P SPIE STEREOSCOPIC
   Czernuszenko M., 1997, Computer Graphics, V31, P46, DOI 10.1145/271283.271303
   de Sorbier F., 2008, 3DPVT 08 4 INT S 3D, P1
   Dodgson NA, 2005, COMPUTER, V38, P31, DOI 10.1109/MC.2005.252
   ES A, 2007, COMPUTER INFORM SCI, P1
   HINES SP, 2010, Patent No. 20100002193
   Humphreys G, 2002, ACM T GRAPHIC, V21, P693, DOI 10.1145/566570.566639
   Kim II, 1996, P SOC PHOTO-OPT INS, V2650, P274, DOI 10.1117/12.237014
   KNOCKE F, 2005, P SOC PHOTO-OPT INS, V5962, P1
   Kooima RL, 2007, IEEE VIRTUAL REALITY 2007, PROCEEDINGS, P131
   MAJUMDER A, 2002, VRST, P147
   Matusik W, 2004, ACM T GRAPHIC, V23, P814, DOI 10.1145/1015706.1015805
   Okoshi T., 1976, 3 DIMENSIONAL IMAGIN
   Raskar R., 1999, Proceedings Visualization '99 (Cat. No.99CB37067), P161, DOI 10.1109/VISUAL.1999.809883
   Son JY, 2003, OPT ENG, V42, P3326, DOI 10.1117/1.1615259
   Taguchi Y, 2009, IEEE T VIS COMPUT GR, V15, P841, DOI 10.1109/TVCG.2009.30
   YUAN G, 2007, P IPT EGVE S WEIM GE, P77
NR 17
TC 3
Z9 11
U1 0
U2 11
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2010
VL 26
IS 6-8
BP 457
EP 465
DI 10.1007/s00371-010-0479-1
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 602JQ
UT WOS:000278135800007
DA 2024-07-18
ER

PT J
AU Martinez, D
   Kieffer, S
   Martinez, J
   Molina, JP
   Macq, B
   Gonzalez, P
AF Martinez, Diego
   Kieffer, Suzanne
   Martinez, Jonatan
   Molina, Jose P.
   Macq, Benoit
   Gonzalez, Pascual
TI Usability evaluation of virtual reality interaction techniques for
   positioning and manoeuvring in reduced, manipulation-oriented
   environments
SO VISUAL COMPUTER
LA English
DT Article
DE Virtual reality; Interaction techniques; Usability evaluation
AB This paper introduces some novel interaction techniques based on the concepts of composite positioning and composite manoeuvring (described in the paper). In contrast with other previous proposals, these techniques have been designed and evaluated in the context of a user centred process. The results of this evaluation and some relevant findings for the field of human computer interaction are also described.
C1 [Martinez, Diego; Martinez, Jonatan; Molina, Jose P.; Gonzalez, Pascual] Univ Castilla La Mancha, Inst Invest Informat Albacete I3A, La Mancha, Spain.
   [Kieffer, Suzanne; Macq, Benoit] Catholic Univ Louvain, Lab Teldetect & Telecommun TELE, B-1348 Louvain, Belgium.
C3 Universidad de Castilla-La Mancha; Universite Catholique Louvain
RP Martinez, D (corresponding author), Univ Castilla La Mancha, Inst Invest Informat Albacete I3A, La Mancha, Spain.
EM diegomp1982@dsi.uclm.es; suzanne.kieffer@uclouvain.be;
   jonatan@dsi.uclm.es; jpmolina@dsi.uclm.es; benoit.macq@uclouvain.be;
   pgonzalez@dsi.uclm.es
RI González, Pascual/E-3693-2016
OI González, Pascual/0000-0003-3549-5712; Martinez Plasencia,
   Diego/0000-0002-5815-8236; Molina Masso, Jose
   Pascual/0000-0001-6832-3250; Kieffer, Suzanne/0000-0002-5519-8814
FU Ministerio de Ciencia e Innovacion of Spain [TIN2008-06596-C0201]; Junta
   de Comunidades de Castilla-LaMancha [PEII09-0054-9581]
FX This work has been partially supported by the Ministerio de Ciencia e
   Innovacion of Spain (TIN2008-06596-C0201), and by the Junta de
   Comunidades de Castilla-LaMancha (PEII09-0054-9581)
CR [Anonymous], 1993, Usability Engineering
   [Anonymous], 1998, 9241 ISO
   Barrilleaux J., 2001, 3D USER INTERFACES J, V1st
   BOWMAN D, 2002, PRESENCE-TELEOP VIRT, V11, P435
   Bowman DA, 2001, PRESENCE-TELEOP VIRT, V10, P96, DOI 10.1162/105474601750182342
   Bowman Doug, 2004, 3D user interfaces: Theory and practice
   Boyd D., 1999, US CTR DES IMPL VIRT
   Gabbard JL, 1999, IEEE COMPUT GRAPH, V19, P51, DOI 10.1109/38.799740
   García AS, 2007, LECT NOTES COMPUT SC, V4563, P224
   HAKKINEN J, 2002, IEEE INT C SYST MAN, V1, P147
   Heldal I., 2005, HCI INT
   Herndon K. P., 1994, ACM SIGCHI Bulletin, V26, P36, DOI [DOI 10.1145/191642.191652, https://doi.org/10.1145/191642.191652]
   Johansson RS, 2001, J NEUROSCI, V21, P6917, DOI 10.1523/JNEUROSCI.21-17-06917.2001
   KIYOKAWA K, 1996, P ACM VIRT REAL SOFT, P27
   Lawson JYL, 2009, EICS'09: PROCEEDINGS OF THE ACM SIGCHI SYMPOSIUM ON ENGINEERING INTERACTIVE COMPUTING SYSTEMS, P245
   Martínez D, 2008, J UNIVERS COMPUT SCI, V14, P3071
   MOSS J, 2008, HUM FACT ERG SOC ANN, V52, P1631
   PITTARELLO F, 2001, P HCITALY S
   Poupyrev I, 1998, COMPUT GRAPH FORUM, V17, pC41
   Poupyrev I., 1996, P 9 ANN ACM S USER I, P79, DOI [DOI 10.1145/237091.237102, 10.1145/237091.237102]
   Roberts D, 2003, PRESENCE-TELEOP VIRT, V12, P644, DOI 10.1162/105474603322955932
   Sutcliffe Alistair., 2003, Multimedia and virtual reality: designing multisensory user interfaces
NR 22
TC 6
Z9 6
U1 0
U2 12
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2010
VL 26
IS 6-8
BP 619
EP 628
DI 10.1007/s00371-010-0499-x
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 602JQ
UT WOS:000278135800022
DA 2024-07-18
ER

PT J
AU Panzoli, D
   de Freitas, S
   Duthen, Y
   Luga, H
AF Panzoli, David
   de Freitas, Sara
   Duthen, Yves
   Luga, Herve
TI The Cortexionist architecture: behavioural intelligence of artificial
   creatures
SO VISUAL COMPUTER
LA English
DT Article
DE Computer animation; Autonomous adaptive agents; Cognitive modelling;
   Human memory
AB Traditionally, producing intelligent behaviours for artificial creatures involves modelling their cognitive abilities. This approach raises two problems. On the one hand, defining manually the agent's knowledge is a heavy and error-prone task that implies the intervention of the animator. On the other hand, the relationship between cognition and intelligence has not been theoretically nor experimentally proven so far. The ecological approaches provide a solution for these problems, by exploring the links between the creature, its body and its environment. Using an artificial life approach, we propose an original model of memory based on the synthesis of several neuroscience theories. The Cortexionist controller integrates cortex-like structure into a connectionist architecture in order to enhance the agent's adaptation in a dynamic environment, ultimately leading to the emergence of intelligent behaviour. Initial experiments presented in this paper prove the validity of the model.
C1 [Panzoli, David; Duthen, Yves; Luga, Herve] Univ Toulouse, IRIT CNRS, UMR 5505, Toulouse, France.
   [de Freitas, Sara] Coventry Univ, Serious Games Inst, Coventry, W Midlands, England.
C3 Universite de Toulouse; Universite Federale Toulouse Midi-Pyrenees
   (ComUE); Universite Toulouse III - Paul Sabatier; Institut National
   Polytechnique de Toulouse; Universite Toulouse 1 Capitole; Universite de
   Toulouse - Jean Jaures; Centre National de la Recherche Scientifique
   (CNRS); Coventry University
RP Panzoli, D (corresponding author), Univ Toulouse, IRIT CNRS, UMR 5505, Toulouse, France.
EM David.Panzoli@irit.fr; S.deFreitas@coventry.ac.uk; Yves.Duthen@irit.fr;
   Herve.Luga@irit.fr
RI Luga, Herve/ABH-6446-2020
OI Luga, Herve/0000-0001-8675-197X
CR [Anonymous], 1950, MIND, DOI 10.1093/mind/LIX.236.433
   [Anonymous], COSYNE COMPUTATIONAL
   [Anonymous], P ISR2004 35 INT S R
   [Anonymous], 1983, The architecture of cognition
   [Anonymous], ANIMALS ANIMATS
   [Anonymous], P INT JOINT C NEUR N
   Arbib MichaelA., 2002, HDB BRAIN THEORY NEU, V2nd
   Atkinson R. C., 1968, Psychology of learning and motivation, V2, P89, DOI [10.1016/S0079-7421(08)60422-3, DOI 10.1016/S0079-7421(08)60422-3, DOI 10.1017/CBO9781316422250.025]
   Baddeley A.D., 1974, RECENT ADV LEARNING, V8
   BLUMBERG BM, 1996, P 4 INT C SIM AD BEH
   BROOKS RA, 1986, IEEE T ROBOTIC AUTOM, V2, P14, DOI 10.1109/JRA.1986.1087032
   COHEN NJ, 1980, SCIENCE, V210, P207, DOI 10.1126/science.7414331
   Conde T, 2006, COMPUT ANIMAT VIRT W, V17, P457, DOI 10.1002/cav.148
   CRAIK FIM, 1972, J VERB LEARN VERB BE, V11, P671, DOI 10.1016/S0022-5371(72)80001-X
   DONIKIAN S, 2001, P 5 INT C AUT AG MON
   Filliat D, 2000, FROM ANIM ANIMAT, P246
   Funge J, 1999, COMP GRAPH, P29, DOI 10.1145/311535.311538
   Gibbs RaymondW., 2007, Pragmatics Cognition, V15, P610
   Girard Benoit, 2003, J Integr Neurosci, V2, P179, DOI 10.1142/S0219635203000299
   Hall Edward T., 1966, The Hidden Dimension
   HARNAD S, 1990, PHYSICA D, V42, P335, DOI 10.1016/0167-2789(90)90087-6
   Hawkins J., 2004, On intelligence
   Hebb D.O., 1949, ORG BEHAV NEUROPSYCH
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Kallmann M, 1999, COMP ANIM CONF PROC, P138, DOI 10.1109/CA.1999.781207
   Lassabe N., 2007, IEEE ALIFE 2007, P243
   McLean P., 1990, The triune brain in evolution Role in paleocelebral functions
   Minsky M., 1974, P PSYCHOL COMPUTER V, P211
   Panzoli D, 2009, 2009 INTERNATIONAL CONFERENCE ON CYBERWORLDS, P63, DOI 10.1109/CW.2009.50
   PELACHAUD C, 2005, MULTIMEDIA 05, P683
   Pfeifer R., 2006, How the body shapes the way we think: a new view of intelligence
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   SANCHEZ S, 2004, LNCS, P68
   Shao W, 2007, GRAPH MODELS, V69, P246, DOI 10.1016/j.gmod.2007.09.001
   Stanley KO, 2002, EVOL COMPUT, V10, P99, DOI 10.1162/106365602320169811
   TERZOPOULOS D, 1994, SIGGRAPH 94 COMPUTER, P42
NR 36
TC 1
Z9 1
U1 0
U2 7
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2010
VL 26
IS 5
BP 353
EP 366
DI 10.1007/s00371-010-0424-3
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 587GS
UT WOS:000276978800006
DA 2024-07-18
ER

PT J
AU Acar, R
AF Acar, Rueyam
TI Simulation of interface dynamics: a diffuse-interface model
SO VISUAL COMPUTER
LA English
DT Article
DE Dynamics of interfaces; Cahn-Hilliard equation
ID WATER; ANIMATION; SURFACES; FLUIDS; FLOWS
AB Interface modeling involves surface tension effects and topological transitions such as breakup and coalescence. In order to simulate the dynamics of interfaces as well as phase transitions, we introduce diffuse-interface modeling. This model describes the interface evolution with a high-order diffusion equation based on the van der Waals-Cahn-Hilliard theory of interfaces. Thus, unlike other methods proposed in graphics, phase dynamics is derived from interface physics regardless of the numerical solution. Small-scale interfacial events and phase transitions develop naturally as a result of the Cahn-Hilliard equation. With the fast, stable and conservative spectral method we used and also due to the characteristics of the diffuse-interface description, the numerical method does not suffer from oscillations or other difficulties caused by interface singularities, which could be very problematic in other interface methods, and numerical diffusion and mass loss are minimized. Besides, the implementation is simple and practical. This model also allows the control of interface features, such as thickness and mobility. The Cahn-Hilliard equation can be coupled with the Navier-Stokes equations to model the surface tension force in fluid equations at the macroscopic level. More importantly, the interface dynamics that uses a free-energy function provides the flexibility and versatility of modeling complex material behaviors simply by designing an appropriate free-energy functional. In this way, we modeled certain elastic behaviors and multicomponent phase evolutions, which could be computationally complex in other methods.
EM ruyam_acar@yahoo.com
RI Acar, Ruyam/KGL-4508-2024
CR ACAR R, 2008, COMPUT FLUI IN PRESS
   ACAR R, 2007, ACM T GRAPH, V26
   [Anonymous], ELECT J DIFFERENTIAL
   BRACKBILL JU, 1992, J COMPUT PHYS, V100, P335, DOI 10.1016/0021-9991(92)90240-Y
   CAHN JW, 1958, J CHEM PHYS, V28, P258, DOI 10.1063/1.1744102
   CHEN CK, 2000, ADV MATH SCI APPL, V10, P821
   Enright D, 2005, COMPUT STRUCT, V83, P479, DOI 10.1016/j.compstruc.2004.04.024
   Enright D, 2002, ACM T GRAPHIC, V21, P736, DOI [10.1145/566570.566581, 10.1145/566570.566645]
   Eyre D., 1997, UNCONDITIONALLY STAB
   Eyre DJ, 1998, MATER RES SOC SYMP P, V529, P39, DOI 10.1557/PROC-529-39
   FOSTER N, 2001, SIGGRAPH 2001 ANN C, P15
   Fratzl P, 1999, J STAT PHYS, V95, P1429, DOI 10.1023/A:1004587425006
   Garcke H, 2001, IEEE T VIS COMPUT GR, V7, P230, DOI 10.1109/2945.942691
   Gurtin ME, 1996, MATH MOD METH APPL S, V6, P815, DOI 10.1142/S0218202596000341
   HIRT CW, 1981, J COMPUT PHYS, V39, P201, DOI 10.1016/0021-9991(81)90145-5
   Hong JM, 2005, ACM T GRAPHIC, V24, P915, DOI 10.1145/1073204.1073283
   Hong JM, 2003, COMPUT GRAPH FORUM, V22, P253, DOI 10.1111/1467-8659.00672
   IERLEY GR, 2002, SMOOTHING WRINKLES N
   Jacqmin D, 1999, J COMPUT PHYS, V155, P96, DOI 10.1006/jcph.1999.6332
   JACQMIN D, 1996, P 34 AER SCI M EXH R
   Leo PH, 1998, ACTA MATER, V46, P2113, DOI 10.1016/S1359-6454(97)00377-7
   Losasso F, 2004, ACM T GRAPHIC, V23, P457, DOI 10.1145/1015706.1015745
   Losasso F, 2006, ACM T GRAPHIC, V25, P812, DOI 10.1145/1141911.1141960
   MAURERMATHISON D, 1999, ULTIMATE MARBLING HD, P52
   Palffy-Muhoray P., 1997, PHYS REV E, V55, P3844
   Pismen LM, 2001, PHYS REV E, V64, DOI 10.1103/PhysRevE.64.021603
   Song OY, 2005, ACM T GRAPHIC, V24, P81, DOI 10.1145/1037957.1037962
   Sussman M, 1999, J COMPUT PHYS, V148, P81, DOI 10.1006/jcph.1998.6106
   Sussman M, 1999, SIAM J SCI COMPUT, V20, P1165, DOI 10.1137/S1064827596298245
   Unser M, 1999, IEEE SIGNAL PROC MAG, V16, P22, DOI 10.1109/79.799930
   van der Waals J, 1893, J STAT PHYS, V20, P197, DOI DOI 10.1007/BF01011514
   Wang HM, 2005, ACM T GRAPHIC, V24, P921, DOI 10.1145/1073204.1073284
NR 32
TC 7
Z9 8
U1 0
U2 8
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2009
VL 25
IS 2
BP 101
EP 115
DI 10.1007/s00371-008-0208-1
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 394XE
UT WOS:000262485700002
DA 2024-07-18
ER

PT J
AU Zagalo, N
   Torres, A
AF Zagalo, Nelson
   Torres, Ana
TI Character emotion experience in virtual environments
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 4th INTUITION International Conference and Workshop
CY OCT 04-05, 2007
CL Athens, GREECE
DE Authoring tools; Emotion; Virtual environments; Virtual storytelling
AB The present paper presents an emotion module from an authoring tool of interactive storytelling being developed within the European Project-INSCAPE. The Atmosphere Editor (AE) is an INSCAPE software plug-in. Its aim is to help authors to easily create virtual interactive scenes that are recognized as emotional in order to contribute to higher coherence of their content and simultaneously to emphasize their communication purposes. It works through the attribution of emotional meaning to virtual environments and characters classes that act on the virtual story-world. Therefore, it is designed to produce a semantic intervention in the story but does not intend to transcend the storyteller work. AE presents then a taxonomy capable of sustaining the communicational optimization of the interactive narratives at an emotional level. The AE intervention develops in addition a possible pedagogical virtue permitting the learning by the story authors about potential emotional uses of specific virtual parameters. It permits also the INSCAPE user to understand the emotional semantics canons of the interactive virtual stories.
C1 [Zagalo, Nelson] Univ Minho, Dept Commun Sci, P-4710057 Braga, Portugal.
   [Torres, Ana] Univ Aveiro, Dept Commun & Art, P-3810193 Aveiro, Portugal.
C3 Universidade do Minho; Universidade de Aveiro
RP Zagalo, N (corresponding author), Univ Minho, Dept Commun Sci, P-4710057 Braga, Portugal.
EM nzagalo@ics.uminho.pt; atorres@ua.pt
RI Zagalo, Nelson/C-9716-2009; Torres, Ana/AAC-8019-2020
OI Zagalo, Nelson/0000-0002-5478-0650; Torres, Ana/0000-0002-8919-0378
CR [Anonymous], 1999, PASSIONATE VIEWS FIL
   Bolter JayDavid Richard Grusin., 1999, REMEDIATION UNDERSTA
   ELADHARI M, 2003, DIG GAM RES C 2003 4
   Frijda N., 1986, EMOTIONS
   HARLOW HF, 1958, AM PSYCHOL, V13, P573
   KLESEN M, 2005, APPL ARTIFICIAL INTE
   MAO W, 2005, INT C INT VIRT AG KO
   SAMYN M, 2006, TALES OF TALES
   SANDERS AF, 1983, ACTA PSYCHOL, V53, P61, DOI 10.1016/0001-6918(83)90016-1
   ZAGALO N, 2007, THESIS U AVEIRO PORT
   ZAGALO N, 2006, LECT NOTES COMPUTER, V4161
NR 11
TC 8
Z9 8
U1 0
U2 11
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2008
VL 24
IS 11
BP 981
EP 986
DI 10.1007/s00371-008-0272-6
PG 6
WC Computer Science, Software Engineering
WE Conference Proceedings Citation Index - Science (CPCI-S); Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 359BQ
UT WOS:000259961600008
DA 2024-07-18
ER

PT J
AU Kuhn, GR
   Oliveira, MM
   Fernandes, LAF
AF Kuhn, Giovane R.
   Oliveira, Manuel M.
   Fernandes, Leandro A. F.
TI An improved contrast enhancing approach for color-to-grayscale mappings
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 26th International Conference on Computer Graphics
CY JUN 09-11, 2008
CL Istanbul, TURKEY
DE color reduction; color-to-grayscale; image processing; error metric
ID GPU
AB Despite the widespread availability of color sensors for image capture, the printing of documents and books are still primarily done in black-and-white for economic reasons. In this case, the included illustrations and photographs are printed in grayscale, with the potential loss of important information encoded in the chrominance channels of these images. We present an efficient contrast enhancement algorithm for color-to-grayscale image conversion that uses both luminance and chrominance information. Our algorithm is about three orders of magnitude faster than previous optimization-based methods, while providing some guarantees on important image properties. More specifically, our approach preserves gray values present in the color image, ensures global consistency, and locally enforces luminance consistency. Our algorithm is completely automatic, scales well with the number of pixels in the image, and can be efficiently implemented on modern GPUs. We also introduce an error metric for evaluating the quality of color-to-grayscale transformations.
C1 [Kuhn, Giovane R.; Oliveira, Manuel M.; Fernandes, Leandro A. F.] Univ Fed Rio Grande do Sul, Inst Informat, Porto Alegre, RS, Brazil.
C3 Universidade Federal do Rio Grande do Sul
RP Fernandes, LAF (corresponding author), Univ Fed Rio Grande do Sul, Inst Informat, Av Bento Goncalves 9500,Caixa Postal 15-064, Porto Alegre, RS, Brazil.
EM grkuhn@inf.ufrgs.br; oliveira@inf.ufrgs.br; laffernandes@inf.ufrgs.br
RI ; Menezes de Oliveira Neto, Manuel/H-1508-2011
OI Fernandes, Leandro/0000-0001-8491-793X; Menezes de Oliveira Neto,
   Manuel/0000-0003-4957-9984
CR [Anonymous], 1989, Principal Components Analysis
   [Anonymous], 2007, PROC 3 EUR C COMPUTA
   BROWN R, PHOTOSHOP CONVERTING
   DIETRICH CA, 2006, STORING ACCESSING TO, P565
   Georgii J, 2005, SIMUL MODEL PRACT TH, V13, P693, DOI 10.1016/j.simpat.2005.08.004
   Gooch A. A., 2005, COLOR2GRAY SALIENCE
   Gooch AA, 2005, ACM T GRAPHIC, V24, P634, DOI 10.1145/1073204.1073241
   Grundland M, 2007, PATTERN RECOGN, V40, P2891, DOI 10.1016/j.patcog.2006.11.003
   JESCHKE ER, 2002, GIMP CONVERTING COLO
   NEMCSICS A, 1980, COLOR RES APPL, V5, P113, DOI 10.1002/col.5080050214
   Rasche K, 2005, COMPUT GRAPH FORUM, V24, P423, DOI 10.1111/j.1467-8659.2005.00867.x
   RASCHE K, 2005, DETAIL PRESERVING CO
   Shepard D., 1968, P 1968 23 ACM NAT C, P517, DOI DOI 10.1145/800186.810616
   Tejada E, 2005, SIMUL MODEL PRACT TH, V13, P703, DOI 10.1016/j.simpat.2005.08.005
   VERLET L, 1967, PHYS REV, V159, P98, DOI 10.1103/PhysRev.159.98
NR 15
TC 32
Z9 36
U1 0
U2 8
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2008
VL 24
IS 7-9
BP 505
EP 514
DI 10.1007/s00371-008-0231-2
PG 10
WC Computer Science, Software Engineering
WE Conference Proceedings Citation Index - Science (CPCI-S); Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 322OI
UT WOS:000257384800006
DA 2024-07-18
ER

PT J
AU Gamito, MN
   Maddock, SC
AF Gamito, Manuel N.
   Maddock, Steve C.
TI Topological correction of hypertextured implicit surfaces for ray
   casting
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 9th International Conference on Shape Modeling and Applications
CY JUN 13-15, 2007
CL Lyon, FRANCE
SP ACM SIGRAPT, CNRS, Groupement Rech Informat Mathemat, Reg Rhone Alpes, Univ Claude Bernard Lyon 1
DE Morse theory; implicit surface; hypertexturing; ray casting
AB Hypertextures are a useful modelling tool in that they can add three-dimensional detail to the surface of otherwise smooth objects. Hypertextures can be rendered as implicit surfaces, resulting in objects with a complex but well defined boundary. However, representing a hypertexture as an implicit surface often results in many small parts being detached from the main surface, turning an object into a disconnected set. Depending on the context, this can detract from the realism in a scene, where one usually does not expect a solid object to have clouds of smaller objects floating around it. We present a topology correction technique, integrated in a ray casting algorithm for hypertextured implicit surfaces, that detects and removes all the surface components that have become disconnected from the main surface. Our method works with implicit surfaces that are C-2 stop continuous and uses Morse theory to find the critical points of the surface. The method follows the separatrix lines joining the critical points to isolate disconnected components.
C1 [Gamito, Manuel N.; Maddock, Steve C.] Univ Sheffield, Dept Comp Sci, Sheffield S1 4DP, S Yorkshire, England.
C3 University of Sheffield
RP Gamito, MN (corresponding author), Univ Sheffield, Dept Comp Sci, 211 Portobello, Sheffield S1 4DP, S Yorkshire, England.
EM M.Gamito@dcs.shef.ac.uk; S.Maddock@dcs.shef.ac.uk
RI Maddock, Steve/J-1849-2016
OI Maddock, Steve/0000-0003-3179-0263
CR [Anonymous], 1966, Interval Arithmetic
   [Anonymous], 1983, IMACS T SCI COMPUTAT
   Cook R. L., 1984, Computers & Graphics, V18, P223
   de Figueiredo LH, 2004, NUMER ALGORITHMS, V37, P147, DOI 10.1023/B:NUMA.0000049462.70970.b6
   Friedman J. H., 1977, ACM Transactions on Mathematical Software, V3, P209, DOI 10.1145/355744.355745
   Gamito MN, 2007, VISUAL COMPUT, V23, P155, DOI 10.1007/s00371-006-0090-7
   Hart J.C., 1998, Mathematical Visualization, P257
   HART J.C., 1999, P IMPLICIT SURFACES, P107
   Hart J.C., 1998, Proceeding of SIGGRAPH on Implicit Surfaces, P69
   Hart JC, 1996, VISUAL COMPUT, V12, P527, DOI 10.1007/s003710050084
   Heidrich W, 1998, GRAPHICS INTERFACE '98 - PROCEEDINGS, P8
   HELMAN JL, 1991, IEEE COMPUT GRAPH, V11, P36, DOI 10.1109/38.79452
   Lewis J. P., 1989, Computer Graphics, V23, P263, DOI 10.1145/74334.74360
   MILNOR J, 1963, THEORY ANN MATH STUD, V51
   MUSGRAVE FK, 2003, TEXTURING MODELING P, P565
   Perlin K, 2002, ACM T GRAPHIC, V21, P681, DOI 10.1145/566570.566636
   Perlin K., 1985, Computer Graphics, V19, P287, DOI 10.1145/325165.325247
   Perlin K.H., 1989, 16 ANN C COMP GRAPH, P253, DOI 10.1145/74333.74359
   Saupe D., 1989, Visualisierung in Mathematik und Naturwissenschaften: Bremer Computergraphik-Tage, V1988, P114
   SCLAROFF S, 1991, COMP GRAPH, V25, P247, DOI 10.1145/127719.122745
   SHINAGAWA Y, 1991, IEEE COMPUT GRAPH, V11, P66, DOI 10.1109/38.90568
   SNYDER JM, 1992, COMP GRAPH, V26, P121, DOI 10.1145/142920.134024
   SPROULL RF, 1991, ALGORITHMICA, V6, P579, DOI 10.1007/BF01759061
   Stander B. T., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P279, DOI 10.1145/258734.258868
   WU ST, 1999, P IMPL SURF 99, P73
NR 25
TC 2
Z9 2
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2008
VL 24
IS 6
BP 397
EP 409
DI 10.1007/s00371-008-0221-4
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 299FP
UT WOS:000255741800003
OA Green Submitted, Green Accepted
DA 2024-07-18
ER

PT J
AU Jang, WS
   Lee, WK
   Lee, IK
   Lee, J
AF Jang, Won-Seob
   Lee, Won-Kyu
   Lee, In-Kwon
   Lee, Jehee
TI Enriching a motion database by analogous combination of partial human
   motions
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 7th Korea-Israel Bi-National Conference on Geometric Modeling and
   Computer Graphics
CY JAN 29-30, 2007
CL Seoul, SOUTH KOREA
DE character animation; motion synthesis
AB We have synthesized new human body motions from existing motion data, by dividing the body of an animated character into several parts, such as upper and lower body, and partitioning the motion of the character into corresponding partial motions. By combining different partial motions, we can generate new motion sequences. We select the most natural-looking combinations by analyzing the similarity of partial motions, using techniques such as motion segmentation, dimensionality reduction, and clustering. These new combinations can dramatically increase the size of a motion database, allowing more score in selecting motions to meet constraints, such as collision avoidance. We verify the naturalness and physical plausibility of the new motions using an SVM learning model and by analysis of static and dynamic balance.
C1 [Jang, Won-Seob; Lee, Won-Kyu; Lee, In-Kwon] Yonsei Univ, Dept Comp Sci, Seoul 120749, South Korea.
   [Lee, Jehee] Seoul Natl Univ, Sch Comp Sci & Engn, Seoul 151744, South Korea.
C3 Yonsei University; Seoul National University (SNU)
RP Lee, IK (corresponding author), Yonsei Univ, Dept Comp Sci, Seoul 120749, South Korea.
EM iklee@yonsei.ac.kr; jehee@cse.snu.ac.kr
RI Lee, In-Kwon/AGP-6124-2022; Lee, Jehee/V-7545-2019
OI Lee, In-Kwon/0000-0002-1534-1882
CR [Anonymous], 1986, PRINCIPAL COMPONENT, DOI DOI 10.1007/978-1-4757-1904-87
   ARIKAN O, 1996, P ACM SIGGRAPH 02
   ARIKAN O, 2003, P ACM SIGGRAPH, P402
   Barbic J, 2004, PROC GRAPH INTERF, P185
   BOROVAC B, 1990, DYNAMICS STABILITY C
   Brand M, 2000, COMP GRAPH, P183, DOI 10.1145/344779.344865
   Bruderlin Armin., 1995, Proceedings of the 22nd Annual Conference on Computer Graphics and Interactive Techniques, SIGGRAPH '95, P97, DOI DOI 10.1145/218380.218421
   Burges CJC, 1998, DATA MIN KNOWL DISC, V2, P121, DOI 10.1023/A:1009715923555
   Featherstone R, 1987, Robot Dynamics Algorithms, P65, DOI DOI 10.1007/978-0-387-74315-8
   Gleicher M, 1997, P 1997 S INT 3D GRAP
   GLEICHER M, 1998, P ACM SIGGRAPH 98, P105
   GLEICHER M, 2002, P 2001 S INT 3D GRAP, P195
   Gose E., 1996, PATTERN RECOGNITION
   Grochow K, 2004, ACM T GRAPHIC, V23, P522, DOI 10.1145/1015706.1015755
   HSU E, 2005, P 32 ANN C COMP GRAP, P1082
   Ikemoto Leslie., 2004, SCA 2004: Proceedings of the 2004 ACM SIGGRAPH/Eurographics Symposium on Computer Animation, P99
   Kim TH, 2003, ACM T GRAPHIC, V22, P392, DOI 10.1145/882262.882283
   Kovar L, 2002, ACM T GRAPHIC, V21, P473, DOI 10.1145/566570.566605
   KOVAR L, 2002, P ACM SIGGRAPH 04, P559
   Kovar Lucas., 2002, SCA 2002: Proceedings of the 2002 ACM SIG-GRAPH/Eurographics Symposium on Computer Animation, P97
   Kwong A, 2005, AATCC REV, V5, P29
   Lee J, 1999, COMP GRAPH, P39
   Lee Jehee., 2002, Proceedings of the 29th annual conference on Computer graphics and interactive techniques, P491, DOI DOI 10.1145/566570.566607
   LEE KH, 2006, P ACM SIGGRAPH 06, P898
   Li Y, 2002, ACM T GRAPHIC, V21, P465
   Moradoff S, 2004, VISUAL COMPUT, V20, P253, DOI 10.1007/s00371-003-0231-1
   PULLEN K, 2002, P 29 ANN C COMP GRAP, P501
   REITSMA PSA, 2004, P 2004 ACM SIGGRAPH, P89
   Rose C, 1998, IEEE COMPUT GRAPH, V18, P32, DOI 10.1109/38.708559
   Safonova A., 2005, Dans SCA '05, P171
   SAFONOVA A, 2004, P SIGGRAPH, P514
   SCHODL A, 2000, P ACM SIGGRAPH 00, P195
   Shoemake Ken, 1985, P 12 ANN C COMP GRAP, P245, DOI [DOI 10.1145/325165.325242, DOI 10.1145/325334.325242]
   Tak S, 2005, ACM T GRAPHIC, V24, P98, DOI 10.1145/1037957.1037963
   Unuma M., 1995, P 22 ANN C COMPUTER, P91, DOI DOI 10.1145/218380.218419
   WTTKIN A, 1995, P ACM SIGGRAPH 95, P105
   Zelnik-Manor L., 2001, P IEEE C COMP VIS PA
   ZELNIKMANOR L, IN PRESS IEEE T PATT
NR 38
TC 15
Z9 19
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2008
VL 24
IS 4
BP 271
EP 280
DI 10.1007/s00371-007-0200-1
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 277UL
UT WOS:000254240100006
DA 2024-07-18
ER

PT J
AU Nunes, RF
   Vidal, CA
   Cavalcante-Neto, JB
AF Nunes, Rubens Fernandes
   Vidal, Creto Augusto
   Cavalcante-Neto, Joaquirn Bento
TI Sensory state machines for physically-based animation
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 25th Computer Graphics International Conference (CGI)
CY MAY 30-JUN 02, 2007
CL Petropolis, BRAZIL
DE physically-based animation; motion control
AB In this paper, we propose a flexible representation of controllers that uses high-level sensors and possesses a general and intuitive structure that offers several types of parameters, which can be modified either automatically or with the animator's intervention. This structure, with the feedback signals provided by its sensors, allows several state machines to act simultaneously on the model, or in a subset of its actuators. The sensors can be optimized, facilitating their definition and use. The representation also permits the animator to define procedures with general instructions that can be automatically executed by the controller during the dynamics simulation.
C1 Univ Fed Ceara, Dept Comp, BR-60455760 Fortaleza, Ceara, Brazil.
C3 Universidade Federal do Ceara
RP Nunes, RF (corresponding author), Univ Fed Ceara, Dept Comp, BR-60455760 Fortaleza, Ceara, Brazil.
RI Vidal, Creto/AAK-7042-2020
CR [Anonymous], P 2005 S INT 3D GRAP
   [Anonymous], 1993, P SIGGRAPH 93, P335
   Armstrong W. W., 1985, Proceedings of Graphics Interface '85, P407
   Auslander J, 1995, ACM T GRAPHIC, V14, P311, DOI 10.1145/225294.225295
   Faloutsos P, 2001, COMP GRAPH, P251, DOI 10.1145/383259.383287
   Hodgins JessicaK., 1995, Proceedings of the 22nd Annual Conference on Computer Graphics and Interactive Techniques, SIGGRAPH 1995, Los Angeles, CA, USA, August 6-11, P71, DOI DOI 10.1145/218380.218414
   HODGINS JK, 1997, P SIGGRAPH 97, P153
   Laszlo J, 2000, COMP GRAPH, P201, DOI 10.1145/344779.344876
   LASZLO JF, 1996, THESIS U TORONTO
   Oshita M, 2001, COMPUT GRAPH FORUM, V20, pC192, DOI 10.1111/1467-8659.00512
   RAIBERT MH, 1991, COMP GRAPH, V25, P349
   Sims K., 1994, Computer Graphics Proceedings. Annual Conference Series 1994. SIGGRAPH 94 Conference Proceedings, P15, DOI 10.1145/192161.192167
   Smith Russell L., Open dynamics engine
   vandePanne M, 1996, IEEE COMPUT GRAPH, V16, P40, DOI 10.1109/38.486679
   VANDEPANNE M, 1990, P ACM SIGGRAPH 90 DA, P225
   VANDEPANNE M, 1995, P EUR WORKSH COMP AN, P165
   Wilhelms J., 1985, Proceedings of Graphics Interface '85, P97
   Witkin A., 1988, Computer Graphics, V22, P159, DOI 10.1145/378456.378507
NR 18
TC 0
Z9 0
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2007
VL 23
IS 9-11
SI SI
BP 823
EP 832
DI 10.1007/s00371-007-0161-4
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 206UE
UT WOS:000249207600022
DA 2024-07-18
ER

PT J
AU Pusch, R
   Samavati, F
   Nasri, A
   Wyvill, B
AF Pusch, Richard
   Samavati, Fararnarz
   Nasri, Ahmad
   Wyvill, Brian
TI Improving the sketch-based interface - Forming curves from many small
   strokes
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 25th Computer Graphics International Conference (CGI)
CY MAY 30-JUN 02, 2007
CL Petropolis, BRAZIL
DE curve fitting; sketch-based interface; small strokes; PCA; reverse
   subdivision
AB Sketch-based interfaces are becoming a useful methodology for interaction with a wide range of applications. Drawing is a natural and simple paradigm for designers. One of the problems in most of the current generation of such interfaces is that designers are forced to use single strokes where they may prefer to use many strokes while drawing with traditional tools such as a pencil. In this work we have addressed this problem by analyzing multiple strokes and replacing them with a single stroke that makes a reasonable estimate of the designer's intention. Our solution recursively subdivides space stopping where either there is only a single stroke, or several strokes that have a proper ordering using principal component analysis. The subspaces are then reconnected, and the orderings are joined to create the control points of a single B-spline curve. The resulting curve is very noisy due to the multitude of strokes. A multi-resolution technique that makes use of reverse subdivision has been used to fit a smooth B-spline curve.
C1 Univ Calgary, Calgary, AB T2N 1N4, Canada.
   Amer Univ Beirut, Beirut, Lebanon.
   Univ Victoria, Victoria, BC V8W 2Y2, Canada.
C3 University of Calgary; American University of Beirut; University of
   Victoria
RP Pusch, R (corresponding author), Univ Calgary, Calgary, AB T2N 1N4, Canada.
EM rapusch@ucalgary.ca; samavati@cpsc.ucalgary.ca; anasri@aub.edu.lb;
   blob@cs.uvic.ca
OI Nasri, Ahmad/0000-0002-2047-6693
CR [Anonymous], P 21 SPRING C COMP G
   Bartels RH, 2000, J COMPUT APPL MATH, V119, P29, DOI 10.1016/S0377-0427(00)00370-8
   BAUDEL T, 1990, P 7 ANN ACM S US INT, P185
   Igarashi T, 1999, COMP GRAPH, P409, DOI 10.1145/311535.311602
   KARA LB, 2006, EUR WORKSH SKETCH BA, P59
   Kégl B, 2002, IEEE T PATTERN ANAL, V24, P59, DOI 10.1109/34.982884
   Michalik Paul., 2002, SMA 02, P297
   Piegl L., 1997, The Nurbs Book, Vsecond
   Samavati FF, 2007, SER MACH PERCEPT ART, V67, P65
   Samet H., 2005, Foundations of Multidimensional and Metric Data Structures (The Morgan Kaufmann Series in Computer Graphics and Geometric Modeling)
   Schmidt R., 2005, EUROGRAPHICS WORKSH
   Zheng JM, 1998, COMPUT GRAPH FORUM, V17, pC327, DOI 10.1111/1467-8659.00279
NR 12
TC 24
Z9 29
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2007
VL 23
IS 9-11
SI SI
BP 955
EP 962
DI 10.1007/s00371-007-0160-5
PG 8
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 206UE
UT WOS:000249207600036
DA 2024-07-18
ER

PT J
AU Zhang, L
   Chen, W
   Ebert, DS
   Peng, QS
AF Zhang, Long
   Chen, Wei
   Ebert, David S.
   Peng, Qunsheng
TI Conservative voxelization
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 25th Computer Graphics International Conference (CGI)
CY MAY 30-JUN 02, 2007
CL Petropolis, BRAZIL
DE voxelization; GPU; conservative correctness
AB We propose a novel hardware-accelerated voxelization algorithm for polygonal models. Compared with previous approaches, our algorithm has a major advantage that it guarantees the conservative correctness in voxelization: every voxel intersecting the input model is correctly recognized. This property is crucial for applications like collision detection, occlusion culling and visibility processing. We also present an efficient and robust implementation of the algorithm in the GPU. Experiments show that our algorithm has a lower memory consumption than previous approaches and is more efficient when the volume resolution is high. In addition, our algorithm requires no preprocessing and is suitable for voxelizing deformable models.
C1 Zhejiang Univ, State Key Lab CAD&CG, Hangzhou 310027, Peoples R China.
   Purdue Univ, Sch Elect & Comp Engn, W Lafayette, IN 47907 USA.
C3 Zhejiang University; Purdue University System; Purdue University
RP Chen, W (corresponding author), Zhejiang Univ, State Key Lab CAD&CG, Hangzhou 310027, Peoples R China.
EM chenwei@cad.zju.edu.cn
RI Chen, Wei/AAR-9817-2020
OI Ebert, David/0000-0001-6177-1296
CR AKENINEMOLLER T, 2005, ACM J GRAPH TOOLS, V10, P1
   Beckhaus S., 2002, Proceedings of the Fifth IASTED International Conference Computer Graphics and Imaging, P15
   BOYLES M, 2000, ACM J GRAPHICS TOOLS, V4, P23
   Chen CH, 1999, MOL PSYCHIATR, V4, P33, DOI 10.1038/sj.mp.4000484
   Dong Z, 2004, 12TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P43
   Eisemann Elmar., 2006, P 2006 S INT 3D GRAP, P71
   Everitt C, 2001, INTERACTIVE ORDER IN
   Fang SF, 2000, COMPUT GRAPH-UK, V24, P433, DOI 10.1016/S0097-8493(00)00038-8
   Gagvani Nikhil., 2000, Proceedings of the 2000 IEEE Symposium on Volume Visualization, P57
   HARRIS M, 2005, GPU GEMS 2, V34, P547
   Hasselgren J, 2005, GPU GEMS 2, V42, P677
   He TS, 1997, VISUALIZATION '97 - PROCEEDINGS, P27, DOI 10.1109/VISUAL.1997.663851
   HEIDELBERGER B, 2003, 395 EI SCI COMP ETH
   KARABASSI EA, 1999, ACM J GRAPHICS TOOLS, V4, P5
   KAUFMAN A, 1986, P 1986 WORKSH INT 3D, P45
   Kreeger K. A., 1999, Proceedings Visualization '99 (Cat. No.99CB37067), P191, DOI 10.1109/VISUAL.1999.809887
   LI W, 2005, GPU GEMS 2, V47, P677
   McNeely WA, 1999, COMP GRAPH, P401, DOI 10.1145/311535.311600
   *NVIDIA CORP, 2006, CG SPEC
   Press W. H., 2002, Numerical Recipes in C: the Art of Scientific Computing, V2nd ed., DOI DOI 10.1119/1.14981
   Wang S. W., 1993, Proceedings Visualization '93. (Cat. No.93CH3354-8), P78, DOI 10.1109/VISUAL.1993.398854
   WANG SW, 1994, IEEE COMPUT GRAPH, V14, P26, DOI 10.1109/38.310721
NR 22
TC 34
Z9 44
U1 1
U2 11
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2007
VL 23
IS 9-11
SI SI
BP 783
EP 792
DI 10.1007/s00371-007-0149-0
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 206UE
UT WOS:000249207600018
DA 2024-07-18
ER

PT J
AU Sharp, R
   Machiraju, R
AF Sharp, Richard
   Machiraju, Raghu
TI Accelerating subsurface scattering using Cholesky factorization
SO VISUAL COMPUTER
LA English
DT Article
DE subsurface scattering; picture/image generation; three-dimensional
   graphics and realism
AB In this paper we present a simplified subsurface scattering model that exploits a diffusion mechanism to provide a simpler solution to the transport equation. Our model is based on numerical analysis techniques that are amenable to Cholesky factorization. We treat the factorization as a precomputed scattering quantity which can be used to significantly speed up multiple scattering calculations as the global light source changes. On low resolution meshes, we have been able to achieve real-time solutions of the subsurface scattering while still maintaining good visual quality of the solution.
C1 Dreese Labs 395, Columbus, OH 43210 USA.
RP Sharp, R (corresponding author), Dreese Labs 395, 2015 Neil Ave, Columbus, OH 43210 USA.
EM sharpr@cse.ohio-state.edu; raghu@cse.ohio-state.edu
RI Machiraju, Raghu K/D-5963-2012
CR BARANOSKI G, 1998, 8 INT C COMP GRAPH V, P154
   Blinn J. F., 1982, Computer Graphics, V16, P21, DOI 10.1145/965145.801255
   CARR NA, 2003, HWWS 03, P51
   Chandrasekhar S., 1950, RAD TRANSFER
   DACHSBACHER C, 2003, P 14 EUR WORKSH REND, P197
   DORSEY J, 1996, P SIGGRAPH 96, P387
   Gorodnichev E. E., 1995, Journal of Experimental and Theoretical Physics, V80, P112
   HABER T, 2005, GRAPHICS INTERFACE
   Hanrahan P., 1993, P ACM SIGGRAPH, P165
   Holden A, 1997, COMPUTATIONAL BIOL H
   Jensen HW, 2001, COMP GRAPH, P511, DOI 10.1145/383259.383319
   Jensen HW., 2001, REALISTIC IMAGE SYNT, DOI [10.1201/9780429294907, DOI 10.1201/9780429294907]
   JENSEN HW, 2005, SIGGRAPH, V21, P576
   JOHNSON CC, 1970, IEEE T BIO-MED ENG, VBM17, P129, DOI 10.1109/TBME.1970.4502711
   Kajiya J. T., 1984, Computers & Graphics, V18, P165
   Kajiya J. T., 1986, Computer Graphics, V20, P143, DOI 10.1145/15886.15902
   Karypis G, 1998, SIAM J SCI COMPUT, V20, P359, DOI 10.1137/S1064827595287997
   Karypis G., METIS FAMILY MULTILE
   Kniss J, 2003, IEEE T VIS COMPUT GR, V9, P150, DOI 10.1109/TVCG.2003.1196003
   Kniss J, 2002, VIS 2002: IEEE VISUALIZATION 2002, PROCEEDINGS, P109, DOI 10.1109/VISUAL.2002.1183764
   MAX NL, 1994, 5 EUR WORKSH REND, P87
   MERTENS T, 2003, P 14 EUR WORKSH REND, P130
   Prahl S.A., 1988, LIGHT TRANSPORT TISS
   Riley K, 2003, IEEE VISUALIZATION 2003, PROCEEDINGS, P279, DOI 10.1109/VISUAL.2003.1250383
   Rushmeier H.E., 1987, P SIGGRAPH, P293, DOI 10.1145/37402.37436
   Sharp R, 2005, VOLUME GRAPHICS 2005, P63
   SLOAN PP, 2002, SIGGRAPH
   STAM J, 1995, P 6 EUR WORKSH REND, P51
   Toledo S., TAUCS LIB SPARSE LIN
NR 29
TC 0
Z9 0
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2006
VL 22
IS 8
BP 541
EP 549
DI 10.1007/s00371-006-0029-z
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 088TD
UT WOS:000240833100003
DA 2024-07-18
ER

PT J
AU Skala, V
AF Skala, V
TI A new approach to line and line segment clipping in homogeneous
   coordinates
SO VISUAL COMPUTER
LA English
DT Article
DE clipping; homogeneous coordinates; projective space; duality; computer
   graphics
ID EFFICIENT ALGORITHM; POLYGON; CONVEX; E(2)
AB The clipping operation is still the bottleneck of the graphics pipeline in spite of the latest developments in graphical hardware and a significant increase in performance. Algorithms for line and line segment clipping have been studied for a long time and many research papers have been published so far. This paper presents a new robust approach to line and line segment clipping using a rectangular window. A simple extension for the case of convex polygon clipping is presented as well. The presented approach does not require a division operation and uses homogeneous coordinates for input and output point representation. The proposed algorithms can take advantage of operations supported by vector-vector hardware. The main contribution of this paper is a new approach to intersection computations applied to line and line segment clipping. This approach leads to algorithms that are simpler, robust, and easy to implement.
C1 Univ W Bohemia, Dept Comp Sci & Engn, Plzen, Czech Republic.
C3 University of West Bohemia Pilsen
RP Skala, V (corresponding author), Univ W Bohemia, Dept Comp Sci & Engn, Plzen, Czech Republic.
EM skala@kiv.zcu.cz
RI Skala, Vaclav/F-9141-2011
OI Skala, Vaclav/0000-0001-8886-4281
CR Bui DH, 1998, VISUAL COMPUT, V14, P31, DOI 10.1007/s003710050121
   BUI DH, 1999, THESIS U W BOHEMIA
   Coxeter H. S. M., 1961, INTRO GEOMETRY
   CYRUS M, 1978, COMPUT GRAPH, V3, P23, DOI 10.1016/0097-8493(78)90021-3
   Foley D., 1990, Computer Graphics: Principles and Practice. the Systems Programming Series
   Hartley Richard., 2000, MULTIVIEW GEOMETRY C
   Hill F.S., 2001, COMPUTER GRAPHICS US
   Huang YQ, 2002, COMPUT GRAPH FORUM, V21, P683, DOI 10.1111/1467-8659.00626
   JOHNSON M, 1996, MATH TODAY       NOV, P171
   LIANG YD, 1984, ACM T GRAPHIC, V3, P1, DOI 10.1145/357332.357333
   MAILLOT PG, 1992, ACM T GRAPHIC, V11, P276, DOI 10.1145/130881.130894
   Nicholl T.M., 1987, ACM SIGGRAPH Comput. Graph, V21, P253, DOI [10.1145/37402.37432, DOI 10.1145/37402.37432]
   Rappoport A., 1991, Visual Computer, V7, P19, DOI 10.1007/BF01994114
   SKALA V, 1993, COMPUT GRAPH-UK, V17, P417, DOI 10.1016/0097-8493(93)90030-D
   SKALA V, 1994, COMPUT GRAPH, V18, P517, DOI 10.1016/0097-8493(94)90064-7
   Skala V, 1996, COMPUT GRAPH, V20, P523, DOI 10.1016/0097-8493(96)00024-6
   Skala V, 1996, COMPUT GRAPH FORUM, V15, P61, DOI 10.1111/1467-8659.1510061
   Skala V, 2001, VISUAL COMPUT, V17, P236, DOI 10.1007/s003710000094
   SKALA V, 1989, P COMP GRAPH INT 89, P121
   Stolfi J., 2001, ORIENTED PROJECTIVE
   ZHANG M, 2002, P ACM S APPL COMP, P796
NR 21
TC 26
Z9 27
U1 0
U2 5
PU SPRINGER
PI NEW YORK
PA 233 SPRING STREET, NEW YORK, NY 10013 USA
SN 0178-2789
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2005
VL 21
IS 11
BP 905
EP 914
DI 10.1007/s00371-005-0305-3
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 974RH
UT WOS:000232608600003
DA 2024-07-18
ER

PT J
AU Yamauchi, T
   Yoshida, N
   Doi, J
   Yamaguchi, F
AF Yamauchi, T
   Yoshida, N
   Doi, J
   Yamaguchi, F
TI Efficient method of adaptive sign detection for 4x4 determinants using a
   standard arithmetic processing unit
SO VISUAL COMPUTER
LA English
DT Article
DE solid modeling; Boolean set operation; exact integer arithmetic;
   geometric algorithm
AB We propose an efficient and exact method for the adaptive sign detection of 4x4 determinants using a standard arithmetic unit. The entities of determinants are variable length integers (integers of arbitrary bit length). The integers are expressed in 16-bit data units, and the sign detection is reduced to the computation of 4x4 determinants of 16-bit integers. To accelerate the computation, the calculation is performed by using a standard arithmetic unit. We have implemented our method and confirmed that it significantly improves the computation time of 4x4 determinants. The method can be applicable to many geometric algorithms that need the exact sign evaluation of 4x4 determinants, especially to construct robust geometric algorithms.
C1 Waseda Univ, Grad Sch Sci & Engn, Shinjuku Ku, Tokyo 1698555, Japan.
   Nihon Univ, Dept Ind Engn & Management, Narashino, Chiba 2758575, Japan.
   IBM Japan Ltd, Tokyo Res Lab, Yamato, Kanagawa 2428502, Japan.
   Waseda Univ, Sch Sci & Engn, Dept Mech Engn, Shinjuku Ku, Tokyo 1698555, Japan.
C3 Waseda University; Nihon University; International Business Machines
   (IBM); Waseda University
RP Yamauchi, T (corresponding author), Waseda Univ, Grad Sch Sci & Engn, Shinjuku Ku, 3-4-1 Ohkubo, Tokyo 1698555, Japan.
OI Yoshida, Norimasa/0000-0001-8889-0949
CR [Anonymous], 1968, Introduction to Combinatorial Mathematics
   [Anonymous], 1991, Oriented Projective Geometry: A Framework for Geometric Computations
   EDELSBRUNNER H, 1990, ACM T GRAPHIC, V9, P66, DOI 10.1145/77635.77639
   Forrest A. R., 1987, Techniques for Computer Graphics, P23
   Fortune S., 1995, Proceedings. Third Symposium on Solid Modeling and Applications, P225, DOI 10.1145/218013.218065
   Hanamitsu H., 1997, Journal of the Japan Society of Precision Engineering, V63, P657, DOI 10.2493/jjspe.63.657
   Hoffmann C., 1989, Geometric and Solid Modeling: An Introduction
   Segal M., 1990, Computer Graphics, V24, P105, DOI 10.1145/97880.97891
   SUGIHARA K, 1992, LECT NOTES COMPUT SC, V650, P209
   SUGIHARA K, 1992, P IEEE, V80, P1471, DOI 10.1109/5.163412
   Sugihara K., 1989, Journal of Information Processing, V12, P380
   Yamaguchi F, 1998, VISUAL COMPUT, V14, P315, DOI 10.1007/s003710050143
   Yamaguchi F, 1997, VISUAL COMPUT, V13, P29, DOI 10.1007/s003710050087
   YOSHIDA N, 1995, J ADV AUTOMAT TECHNO, V7, P128
   YOSHIDA N, 1994, COMPUT GRAPH FORUM, V13, P55
NR 15
TC 0
Z9 0
U1 0
U2 0
PU SPRINGER-VERLAG
PI NEW YORK
PA 175 FIFTH AVE, NEW YORK, NY 10010 USA
SN 0178-2789
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2004
VL 20
IS 1
BP 37
EP 46
DI 10.1007/s00371-003-0224-0
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 809SA
UT WOS:000220655200003
DA 2024-07-18
ER

PT J
AU Bechmann, D
   Gerber, D
AF Bechmann, D
   Gerber, D
TI Arbitrary shaped deformations with DOGME
SO VISUAL COMPUTER
LA English
DT Article
DE free-form deformations; DOGME; constrained deformation; curvilinear
   constraints; polyhedral volume of influence
ID SPACE DEFORMATION
AB In the field of 3D computer design, free-form deformations, also known as FFD, play an important role. There are mainly two trends in FFD: with a lattice or using constraints. The present paper is based upon the latter family of FFDs, and more specifically the deformation model DOGME. We will present a way of using curvilinear constraints, with either a point or a curve as the constrained element. We will also present two means of using the volume defined by an arbitrary closed polyhedral mesh as the volume of influence around the constraints.
C1 LSIIT ULP, CNRS UMR 7005, Illkirch Graffenstaden, France.
C3 Universites de Strasbourg Etablissements Associes; Universite de
   Strasbourg
RP Bechmann, D (corresponding author), LSIIT ULP, CNRS UMR 7005, Blvd Sebastien Brandt, Illkirch Graffenstaden, France.
CR [Anonymous], 1986, P 13 ANN C COMP GRAP, DOI DOI 10.1145/15922.15903
   [Anonymous], GEN INVERSE MATRICES
   Aubert F, 1997, COMPUT GRAPH, V21, P625, DOI 10.1016/S0097-8493(97)00040-X
   BECHMANN D, 1994, COMPUT GRAPH-UK, V18, P571, DOI 10.1016/0097-8493(94)90071-X
   BORREL P, 1994, ACM T GRAPHIC, V13, P137, DOI 10.1145/176579.176581
   Borrel P, 1991, INT J COMPUT GEOM AP, V1, P427, DOI 10.1142/S0218195991000281
   FUCHS H, 1980, P 7 ANN C COMP GRAPH
   GAIN JE, 1999, P 17 ANN C EUR UK CH
   GRIESSMAIR J, 1989, EUR 89 HAMB 4 8 SEPT
   HSU WM, 1992, COMP GRAPH, V26, P177, DOI 10.1145/142920.134036
   Klok F., 1986, Computer-Aided Geometric Design, V3, P217, DOI 10.1016/0167-8396(86)90039-7
   LAMOUSIN HJ, 1994, IEEE COMPUT GRAPH, V14, P59, DOI 10.1109/38.329096
   MACCRAKEN R, 1996, P 23 ANN C COMP GRAP
   Raffin R, 2000, VISUAL COMPUT, V16, P38, DOI 10.1007/s003710050005
   RAFFIN R, 1999, ACT JOURN COURB SURF
   Singh Karan, 1998, P 25 ANN C COMP GRAP
NR 16
TC 10
Z9 12
U1 0
U2 1
PU SPRINGER-VERLAG
PI NEW YORK
PA 175 FIFTH AVE, NEW YORK, NY 10010 USA
SN 0178-2789
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2003
VL 19
IS 2-3
BP 175
EP 186
DI 10.1007/s00371-002-0191-x
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 691AU
UT WOS:000183583300010
DA 2024-07-18
ER

PT J
AU Elber, G
   Wolberg, G
AF Elber, G
   Wolberg, G
TI Rendering traditional mosaics
SO VISUAL COMPUTER
LA English
DT Article
DE mosaics; nonphotorealistic rendering; offset curves; Voronoi diagrams
AB This paper discusses the principles of traditional mosaics and describes a technique for implementing a digital mosaicing system. The goal of this work is to transform digital images into traditional mosaic-like renderings. We achieve this effect by recovering free-form feature curves from the image and laying rows of tiles along these curves. Composition rules are applied to merge these tiles into an intricate jigsaw that conforms to classical mosaic styles. Mosaic rendering offers the user flexibility over every aspect of this craft, including tile arrangement, shapes, and colors. The result is a system that makes this wonderful craft more flexible and widely accessible than previously possible.
C1 Technion Israel Inst Technol, Dept Comp Sci, IL-32000 Haifa, Israel.
   CUNY City Coll, Dept Comp Sci, New York, NY 10031 USA.
C3 Technion Israel Institute of Technology; City University of New York
   (CUNY) System; City College of New York (CUNY)
RP Technion Israel Inst Technol, Dept Comp Sci, IL-32000 Haifa, Israel.
EM gershon@cs.technion.ac.il; wolberg@cs.ccny.cuny.edu
CR [Anonymous], P COMP GRAPH SIGGRAP, DOI DOI 10.1145/218380.218442
   [Anonymous], 1996, P 23 ANN C COMP GRAP
   [Anonymous], 1999, OpenGL programming guide: the official guide to learning OpenGL
   BERTELLI C, 1993, MOSAIQUES
   CHAVARRIA J, 1999, ART MOSAICS
   Curtis C. J., 1997, Proceedings of the 24th annual conference on Computer graphics and interactive techniques, P421
   DIERKS L, 1997, MAKING MOSAICS STERL
   DOCARMO MP, 1990, DIFFERENTIAL GEOMETR
   ELBER G, 1995, IEEE T VIS COMPUT GR, V1, P231, DOI 10.1109/2945.466718
   Elber G, 1998, IEEE T VIS COMPUT GR, V4, P71, DOI 10.1109/2945.675655
   Elber G, 1997, IEEE COMPUT GRAPH, V17, P62, DOI 10.1109/38.586019
   Elber G, 1991, INT J COMPUT GEOM AP, V1, P67, DOI 10.1142/S0218195991000062
   Glassner A, 1999, IEEE COMPUT GRAPH, V19, P78, DOI 10.1109/38.788804
   Haeberli P., 1990, Computer Graphics, V24, P207, DOI 10.1145/97880.97902
   Hausner A, 2001, COMP GRAPH, P573, DOI 10.1145/383259.383327
   Hoff KE, 1999, COMP GRAPH, P277, DOI 10.1145/311535.311567
   Hoffmann C. M., 1992, Computer Graphics and Mathematics, P229
   LANSDOWN J, 1995, IEEE COMPUT GRAPH, V15, P29, DOI 10.1109/38.376610
   LEE SY, 1995, P SIGGRAPH 95, P439
   Ling Robert., 1998, ANCIENT MOSAICS
   MANTANARI U, 1968, J COMPUT MACH, V16, P534
   Miyata K., 1990, Computer Graphics, V24, P387, DOI 10.1145/97880.97921
   Ostromoukhov V, 1999, COMP GRAPH, P417, DOI 10.1145/311535.311604
   Pnueli Y., 1994, Visual Computer, V10, P277, DOI 10.1007/BF01901584
   SALISBURY MP, 1997, P SIGGRAPH 97, P401
   Silvers R., 1997, Photomosaics
   Stockman George, 2001, Computer Vision
   VANCE P, 1994, MOSAIC BOOK
   WINKENBACH G, 1996, P SIGGRAPH 96, P469
NR 29
TC 43
Z9 45
U1 0
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2003
VL 19
IS 1
BP 67
EP 78
DI 10.1007/s00371-002-0175-x
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Arts &amp; Humanities Citation Index (A&amp;HCI)
SC Computer Science
GA 671MB
UT WOS:000182467600006
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Wu, CM
   Zhao, JT
AF Wu, Chengmao
   Zhao, Jingtian
TI Robust superpixel-based fuzzy possibilistic clustering method
   incorporating local information for image segmentation
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Image segmentation; Superpixel; Fuzzy possibilistic C-means; Local
   information; Robustness
AB In recent years, several superpixel-segmentation methods have been developed to efficiently segment noisy images. However, these methods still face challenges such as high computational complexity and poor adaptability. Therefore, this paper develops a novel superpixel-based robust segmentation model including two modules: superpixel generation and superpixel-based image segmentation. In the superpixel generation module, a fuzzy factor containing local spatial information of pixels is introduced into fuzzy possibilistic clustering algorithm with local search. In the superpixel-based segmentation module, a superpixel-based fuzzy C-means algorithm with local spatial information of superpixels is proposed, which nonlinearly combines the membership of superpixels with the membership of their neighboring superpixels. The experimental results demonstrate that the proposed method outperforms existing state-of-the-art segmentation algorithms in terms of evaluation indexes and visual effects.
C1 [Wu, Chengmao; Zhao, Jingtian] Xian Univ Posts & Telecommun, Sch Elect Engn, Xian 710121, Peoples R China.
C3 Xi'an University of Posts & Telecommunications
RP Zhao, JT (corresponding author), Xian Univ Posts & Telecommun, Sch Elect Engn, Xian 710121, Peoples R China.
EM 943501015@qq.com
OI Wu, Chengmao/0000-0002-5881-4723
FU National Natural Science Foundation of China [62071378]; Shaanxi Natural
   Science Foundation of China [2022JM-370]; School of Electronic
   Engineering, Xi'an University of Posts & Telecommunications, Xi'an,
   China
FX This work was supported by the National Natural Science Foundation of
   China (62071378), and the Shaanxi Natural Science Foundation of China
   (2022JM-370). The authors would like to thank the anonymous reviewers
   for their constructive suggestions to improve the overall quality of the
   paper. Besides, the authors would like to thank the School of Electronic
   Engineering, Xi'an University of Posts & Telecommunications, Xi'an,
   China for financially support.
CR Achanta R, 2017, PROC CVPR IEEE, P4895, DOI 10.1109/CVPR.2017.520
   Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   Arbeláez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161
   Barbato MP, 2022, REMOTE SENS APPL, V28, DOI 10.1016/j.rsase.2022.100823
   Chen GZ, 2022, IEEE GEOSCI REMOTE S, V19, DOI 10.1109/LGRS.2022.3198065
   Chen JS, 2017, IEEE T IMAGE PROCESS, V26, P3317, DOI 10.1109/TIP.2017.2651389
   Chen L, 2022, ENG APPL ARTIF INTEL, V116, DOI 10.1016/j.engappai.2022.105335
   Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5
   github, about us
   Gong MG, 2013, IEEE T IMAGE PROCESS, V22, P573, DOI 10.1109/TIP.2012.2219547
   Guo YW, 2018, IEEE T FUZZY SYST, V26, P2846, DOI 10.1109/TFUZZ.2018.2814591
   Hill PR, 2003, IEEE T IMAGE PROCESS, V12, P1618, DOI 10.1109/TIP.2003.819311
   Ji B, 2022, OPTIK, V260, DOI 10.1016/j.ijleo.2022.169039
   Ji XR, 2022, REMOTE SENS-BASEL, V14, DOI 10.3390/rs14143490
   Jia XH, 2020, IEEE ACCESS, V8, P211526, DOI 10.1109/ACCESS.2020.3039742
   Kanezaki A, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P1543, DOI 10.1109/ICASSP.2018.8462533
   Krinidis S, 2010, IEEE T IMAGE PROCESS, V19, P1328, DOI 10.1109/TIP.2010.2040763
   Lei T, 2019, IEEE T IMAGE PROCESS, V28, P5510, DOI 10.1109/TIP.2019.2920514
   Lei T, 2019, IEEE T FUZZY SYST, V27, P1753, DOI 10.1109/TFUZZ.2018.2889018
   Lei T, 2018, IEEE T FUZZY SYST, V26, P3027, DOI 10.1109/TFUZZ.2018.2796074
   Levinshtein A, 2009, IEEE T PATTERN ANAL, V31, P2290, DOI 10.1109/TPAMI.2009.96
   Liu KH, 2021, IEEE-CAA J AUTOMATIC, V8, P1428, DOI 10.1109/JAS.2021.1004057
   Ma F, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2021.3108585
   Ma K, 2015, IEEE T IMAGE PROCESS, V24, P3345, DOI 10.1109/TIP.2015.2442920
   Ming-Yu Liu, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2097, DOI 10.1109/CVPR.2011.5995323
   Mookiah S, 2022, SOFT COMPUT, V26, P13193, DOI 10.1007/s00500-022-07133-5
   Mujica-Vargas D, 2020, INT J FUZZY SYST, V22, P901, DOI 10.1007/s40815-020-00824-x
   Ng TC, 2023, PATTERN RECOGN, V134, DOI 10.1016/j.patcog.2022.109045
   Ng TC, 2022, IEEE T FUZZY SYST, V30, P14, DOI 10.1109/TFUZZ.2020.3029939
   Pal NR, 2005, IEEE T FUZZY SYST, V13, P517, DOI 10.1109/TFUZZ.2004.840099
   Qin GF, 2019, MULTIMED TOOLS APPL, V78, P5181, DOI 10.1007/s11042-017-4683-0
   Ren XF, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P10
   Shen JB, 2016, IEEE T IMAGE PROCESS, V25, P5933, DOI 10.1109/TIP.2016.2616302
   Shen JB, 2014, IEEE T IMAGE PROCESS, V23, P1451, DOI 10.1109/TIP.2014.2302892
   Shen MY, 2018, MULTIMED TOOLS APPL, V77, P25109, DOI 10.1007/s11042-018-5770-6
   Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688
   Shi LJ, 2022, SENSORS-BASEL, V22, DOI 10.3390/s22176663
   Shi P, 2022, MATERIALS, V15, DOI 10.3390/ma15134417
   Singh NK, 2020, IET IMAGE PROCESS, V14, P487, DOI 10.1049/iet-ipr.2019.0255
   Song SS, 2023, IEEE T FUZZY SYST, V31, P2153, DOI 10.1109/TFUZZ.2022.3220925
   Stutz D, 2018, COMPUT VIS IMAGE UND, V166, P1, DOI 10.1016/j.cviu.2017.03.007
   Szilágyi L, 2011, LECT NOTES ARTIF INT, V6820, P150, DOI 10.1007/978-3-642-22589-5_15
   Tian XL, 2015, J VIS COMMUN IMAGE R, V26, P146, DOI 10.1016/j.jvcir.2014.11.005
   Van den Bergh M, 2015, INT J COMPUT VISION, V111, P298, DOI 10.1007/s11263-014-0744-2
   Vargas-Muñoz JE, 2019, IEEE T IMAGE PROCESS, V28, P3477, DOI 10.1109/TIP.2019.2897941
   Wang H, 2020, IEEE T CIRC SYST VID, V30, P822, DOI 10.1109/TCSVT.2019.2896438
   Wang MR, 2017, SIGNAL PROCESS-IMAGE, V56, P28, DOI 10.1016/j.image.2017.04.007
   Wang PY, 2020, IEEE ACCESS, V8, P31198, DOI 10.1109/ACCESS.2020.2973286
   Wang QS, 2020, APPL SOFT COMPUT, V92, DOI 10.1016/j.asoc.2020.106318
   Wang SF, 2023, DISPLAYS, V76, DOI 10.1016/j.displa.2023.102369
   Winn J, 2005, IEEE I CONF COMP VIS, P1800
   Wu C, 2021, IEEE T CIRC SYST VID, V31, P2114, DOI 10.1109/TCSVT.2020.3019109
   Wu C, 2019, IEEE IMAGE PROC, P1455, DOI [10.1109/icip.2019.8803039, 10.1109/ICIP.2019.8803039]
   Wu GM, 2015, BIOMED ENG ONLINE, V14, DOI 10.1186/s12938-015-0020-x
   Xiao XL, 2018, IEEE T IMAGE PROCESS, V27, P2883, DOI 10.1109/TIP.2018.2810541
   Xie XL, 2019, IEEE ACCESS, V7, P10999, DOI 10.1109/ACCESS.2019.2891941
   Yang H, 2019, IEEE T IMAGE PROCESS, V28, P3061, DOI 10.1109/TIP.2019.2893743
   Zhan J, 2021, COGN COMPUT, V13, P821, DOI 10.1007/s12559-019-09662-y
   Zhou T., 2023, P INT C MACH LEAR, P20787
NR 59
TC 0
Z9 0
U1 8
U2 8
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2023 DEC 30
PY 2023
DI 10.1007/s00371-023-03218-w
EA DEC 2023
PG 40
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DM6X5
UT WOS:001132511100001
DA 2024-07-18
ER

PT J
AU Zheng, ZY
   Chen, ZX
   Wang, SQ
   Wang, WP
AF Zheng, Ziyang
   Chen, Zhixiang
   Wang, Shuqi
   Wang, Wenpeng
TI Dual-attention U-Net and multi-convolution network for single-image rain
   removal
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Image processing; Dual-attention mechanism; U-Net; Single-image de-rain;
   Feature extraction; Convolutional neural networks
AB Images taken on rainy days have rain streaks of varying degrees of intensity, which seriously affect the visibility of the background scene. Aiming at the above problems, we propose a rain mark removal algorithm based on the combination of dual-attention mechanism U-Net and multi-convolution. First, we add a double attention mechanism to the encoder of U-Net. It can give different weights to the rain mark features that need to be extracted in different channels and spaces so that sufficient rain mark features can be obtained. With different dilation factors, we can obtain rain mark characteristics of different depths. Secondly, the multi-convolutional channel integrates the characteristics of rain streaks and prepares sufficient rain mark information for the task of clearing rain streaks. By introducing a cyclic rain streaks detection and removal mechanism into the network architecture, it can achieve gradual removal of rain streaks. Even in the case of heavy rain, our algorithm can get good results. Finally, we tested on both synthetic and real datasets to obtain subjective results and objective evaluations. Experimental results show that for the rainy day image de-rain task with different intensities of rain streaks, our algorithm is more robust. Moreover, the ability of our algorithm to remove rain streaks is better than that of the other five different classical algorithms. The de-raining images produced by our algorithm are visually sharper, and its visibility enhancements are effective for computer vision applications (Google Vision API).
C1 [Zheng, Ziyang; Chen, Zhixiang; Wang, Wenpeng] Minnan Normal Univ, Sch Phys & Informat Engn, Zhangzhou 363000, Fujian, Peoples R China.
   [Wang, Shuqi] Minnan Normal Univ, Sch Comp Sci, Zhangzhou 363000, Fujian, Peoples R China.
   [Wang, Shuqi] Minnan Normal Univ, Key Lab Data Sci & Intelligence Applicat, Zhangzhou 363000, Fujian, Peoples R China.
C3 MinNan Normal University; MinNan Normal University; MinNan Normal
   University
RP Chen, ZX (corresponding author), Minnan Normal Univ, Sch Phys & Informat Engn, Zhangzhou 363000, Fujian, Peoples R China.
EM eva.0311@foxmail.com; zxchenphd@163.com; 347423159@qq.com;
   wwp_9527@163.com
RI WANG, Shuqi/JSL-5038-2023
FU The Natural Science Foundation of Zhangzhou [ZZ2020J33]; Natural Science
   Foundation of Zhangzhou [2023J01924]; Natural Science Foundation of
   Fujian
FX This work was supported by the Natural Science Foundation of Zhangzhou
   under Grant ZZ2020J33 and the Natural Science Foundation of Fujian under
   Grant 2023J01924.
CR Chen CH, 2021, PROC CVPR IEEE, P7738, DOI 10.1109/CVPR46437.2021.00765
   Collins R.T., 2000, VSAM final report, P1
   Collobert R, 2011, BIGLEARN NIPS WORKSH, P1
   Dollár P, 2012, IEEE T PATTERN ANAL, V34, P743, DOI 10.1109/TPAMI.2011.155
   Fan LW, 2019, VIS COMPUT IND BIOME, V2, DOI 10.1186/s42492-019-0016-7
   Fu XY, 2020, IEEE T NEUR NET LEAR, V31, P1794, DOI 10.1109/TNNLS.2019.2926481
   Fu XY, 2017, PROC CVPR IEEE, P1715, DOI 10.1109/CVPR.2017.186
   Fu XY, 2017, IEEE T IMAGE PROCESS, V26, P2944, DOI 10.1109/TIP.2017.2691802
   Garg K., 2004, P 2004 IEEE COMPUTER, V1
   Garg K, 2006, ACM T GRAPHIC, V25, P996, DOI 10.1145/1141911.1141985
   He XG, 2022, COMMUN INF SYST, V22, P79
   Jayaraman T, 2022, INT J AP MAT COM-POL, V32, P111, DOI 10.34768/amcs-2022-0009
   Kim JH, 2013, IEEE IMAGE PROC, P914, DOI 10.1109/ICIP.2013.6738189
   Kingma D. P., 2014, arXiv
   Li G, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1056, DOI 10.1145/3240508.3240636
   Li X, 2018, LECT NOTES COMPUT SC, V11211, P262, DOI 10.1007/978-3-030-01234-2_16
   Li Y, 2016, PROC CVPR IEEE, P2736, DOI 10.1109/CVPR.2016.299
   Luo Y, 2022, VISUAL COMPUT, V38, P3109, DOI 10.1007/s00371-022-02567-2
   Luo Y, 2015, IEEE I CONF COMP VIS, P3397, DOI 10.1109/ICCV.2015.388
   Pan JS, 2018, PROC CVPR IEEE, P3070, DOI 10.1109/CVPR.2018.00324
   Ran W, 2020, IEEE INT CON MULTI, DOI 10.1109/icme46284.2020.9102800
   Ren DW, 2019, PROC CVPR IEEE, P3932, DOI 10.1109/CVPR.2019.00406
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Wang TY, 2019, PROC CVPR IEEE, P12262, DOI 10.1109/CVPR.2019.01255
   Wang YL, 2017, IEEE T IMAGE PROCESS, V26, P3936, DOI 10.1109/TIP.2017.2708502
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wei W, 2017, IEEE I CONF COMP VIS, P2535, DOI 10.1109/ICCV.2017.275
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Yadav S, 2021, MULTIMED TOOLS APPL, V80, P36491, DOI 10.1007/s11042-021-11442-6
   Yang H, 2023, VISUAL COMPUT, V39, P3887, DOI 10.1007/s00371-022-02533-y
   Yang WH, 2020, IEEE T PATTERN ANAL, V42, P1377, DOI 10.1109/TPAMI.2019.2895793
   Yu H, 2018, NEUROCOMPUTING, V295, P108, DOI 10.1016/j.neucom.2018.03.021
   Zhang H, 2018, PROC CVPR IEEE, P695, DOI 10.1109/CVPR.2018.00079
   Zhang H, 2017, IEEE WINT CONF APPL, P1259, DOI 10.1109/WACV.2017.145
   Zhang L, 2019, APPL SOFT COMPUT, V83, DOI 10.1016/j.asoc.2019.105603
   Zheng ZY, 2023, COMPUT VIS IMAGE UND, V235, DOI 10.1016/j.cviu.2023.103766
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zhu L, 2017, IEEE I CONF COMP VIS, P2545, DOI 10.1109/ICCV.2017.276
NR 38
TC 0
Z9 0
U1 2
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2023 DEC 21
PY 2023
DI 10.1007/s00371-023-03198-x
EA DEC 2023
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CW7N9
UT WOS:001128338300002
DA 2024-07-18
ER

PT J
AU Wang, TF
   Wang, J
   Wang, C
   Lei, Y
   Cao, R
   Wang, L
AF Wang, Taofang
   Wang, Jun
   Wang, Chao
   Lei, Yi
   Cao, Rui
   Wang, Li
TI Improving YOLOX network for multi-scale fire detection
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Convolutional neural network; Fire detection; Multi-scale; Feature
   pyramid; Data augmentation
ID FLAME DETECTION
AB Forest fire is a severe natural disaster, which leads to the destruction of forest ecology. At present, fire detection technology represented by convolutional neural network is widely used in forest resource protection, which can realize rapid analysis. However, in forest flame and smoke detection tasks, due to continuous expansion of the target range, a better detection effect cannot be achieved. This paper proposes an improved YOLOX method for multi-scale forest fire detection. This method proposes a novel feature pyramid model to reduce the information loss of high-level forest fire feature maps and enhance the representation ability of feature pyramids. Moreover, the method applies a small object data augmentation strategy to enrich the forest fire dataset, making it more suitable for the actual forest fire scene. According to the experimental results, the mAP of the model proposed in this paper reaches 79.64%, which is about 4.89% higher than the baseline network YOLOX. The method improves the accuracy of forest fire detection, reduces false alarms, and is suitable for real scenarios of forest fires.
C1 [Wang, Taofang; Wang, Li] Taiyuan Univ Technol, Coll Data Sci, Taiyuan 030024, Peoples R China.
   [Wang, Jun; Wang, Chao; Lei, Yi; Cao, Rui] Taiyuan Univ Technol, Coll Software, Taiyuan 030024, Peoples R China.
C3 Taiyuan University of Technology; Taiyuan University of Technology
RP Wang, L (corresponding author), Taiyuan Univ Technol, Coll Data Sci, Taiyuan 030024, Peoples R China.; Cao, R (corresponding author), Taiyuan Univ Technol, Coll Software, Taiyuan 030024, Peoples R China.
EM caorui@tyut.edu.cn; wangli@tyut.edu.cn
FU National Natural Science Foundation of China
FX No Statement Available
CR Barmpoutis P, 2019, INT CONF ACOUST SPEE, P8301, DOI [10.1109/ICASSP.2019.8682647, 10.1109/icassp.2019.8682647]
   Bochkovskiy A, 2020, Arxiv, DOI arXiv:2004.10934
   Cao Guimei., 2018, PROC 9 INT C GRAPHIC, V10615, P381
   Cao LL, 2021, Arxiv, DOI arXiv:2105.10104
   Chaoxu Guo, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12592, DOI 10.1109/CVPR42600.2020.01261
   Cui YM, 2022, Arxiv, DOI arXiv:2207.05252
   DeVries T, 2017, Arxiv, DOI [arXiv:1708.04552, DOI 10.48550/ARXIV.1708.04552]
   Di Lascio R, 2014, LECT NOTES COMPUT SC, V8814, P477, DOI 10.1007/978-3-319-11758-4_52
   Ertugrul M, 2021, ENVIRON MONIT ASSESS, V193, DOI 10.1007/s10661-020-08800-6
   Lucas-Borja ME, 2018, ECOL ENG, V122, P27, DOI 10.1016/j.ecoleng.2018.07.018
   Ge Z, 2021, Arxiv, DOI arXiv:2107.08430
   Habiboglu YH, 2012, MACH VISION APPL, V23, P1103, DOI 10.1007/s00138-011-0369-1
   Hou WQ, 2024, VISUAL COMPUT, V40, P459, DOI 10.1007/s00371-023-02793-2
   Jiao ZT, 2019, 2019 1ST INTERNATIONAL CONFERENCE ON INDUSTRIAL ARTIFICIAL INTELLIGENCE (IAI 2019), DOI 10.1109/iciai.2019.8850815
   Kim B, 2019, APPL SCI-BASEL, V9, DOI 10.3390/app9142862
   Li P, 2020, CASE STUD THERM ENG, V19, DOI 10.1016/j.csite.2020.100625
   Li ZL, 2018, IEEE T IND INFORM, V14, P1146, DOI 10.1109/TII.2017.2768530
   Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu DF, 2021, AAAI CONF ARTIF INTE, V35, P6101
   Liu QP, 2023, VISUAL COMPUT, V39, P6265, DOI 10.1007/s00371-022-02727-4
   Liu S, 2018, PROC CVPR IEEE, P8759, DOI 10.1109/CVPR.2018.00913
   Liu ST, 2019, Arxiv, DOI arXiv:1911.09516
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Lucas-Borja ME, 2019, SCI TOTAL ENVIRON, V654, P441, DOI 10.1016/j.scitotenv.2018.11.161
   Luo YH, 2022, MULTIMED TOOLS APPL, V81, P30685, DOI 10.1007/s11042-022-11940-1
   Majid S, 2022, EXPERT SYST APPL, V189, DOI 10.1016/j.eswa.2021.116114
   Nair Vinod, 2010, INT C INT C MACHINE, P807
   Redmon J., 2018, arXiv, DOI DOI 10.48550/ARXIV.1804.02767
   Redmon J, 2017, PROC CVPR IEEE, P6517, DOI 10.1109/CVPR.2017.690
   Rezatofighi H, 2019, PROC CVPR IEEE, P658, DOI 10.1109/CVPR.2019.00075
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Singh B, 2018, PROC CVPR IEEE, P3578, DOI 10.1109/CVPR.2018.00377
   Tan M., 2020, P IEEECVF C COMPUTER, P10781, DOI [10.48550/arXiv.1911.09070, DOI 10.1109/CVPR42600.2020.01079]
   Taylor L, 2018, 2018 IEEE SYMPOSIUM SERIES ON COMPUTATIONAL INTELLIGENCE (IEEE SSCI), P1542, DOI 10.1109/SSCI.2018.8628742
   Wang GX, 2022, MEASUREMENT, V200, DOI 10.1016/j.measurement.2022.111675
   Wang YB, 2019, J ALGORITHMS COMPUT, V13, DOI 10.1177/1748302619887689
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Wu ZS, 2022, FIRE TECHNOL, V58, P2377, DOI 10.1007/s10694-022-01260-z
   Yun S, 2019, IEEE I CONF COMP VIS, P6022, DOI 10.1109/ICCV.2019.00612
   Zhan JL, 2022, COMPUT ELECTRON AGR, V196, DOI 10.1016/j.compag.2022.106874
   Zhang H, 2011, IEEE IMAGE PROC, P797, DOI 10.1109/ICIP.2011.6116676
   Zhao ET, 2021, ELECTRONICS-SWITZ, V10, DOI 10.3390/electronics10050566
   Zheng ZH, 2020, AAAI CONF ARTIF INTE, V34, P12993
   Zhong Z, 2020, AAAI CONF ARTIF INTE, V34, P13001
   Zhou XY, 2019, Arxiv, DOI [arXiv:1904.07850, 10.48550/arXiv.1904.07850]
NR 46
TC 1
Z9 1
U1 25
U2 25
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2023 NOV 30
PY 2023
DI 10.1007/s00371-023-03178-1
EA NOV 2023
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AZ4K2
UT WOS:001122252600005
DA 2024-07-18
ER

PT J
AU Kong, FQ
   Ren, GL
   Hu, YF
   Li, D
   Hu, KD
AF Kong, Fanqiang
   Ren, Guanglong
   Hu, Yunfang
   Li, Dan
   Hu, Kedi
TI Mixture autoregressive and spectral attention network for multispectral
   image compression based on variational autoencoder
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Hierarchical prior model; Learnt multispectral image compression;
   Spectral attention; Variational autoencoder
AB Multispectral images, with their unique three-dimensional characteristics, require specialized spatial-spectral feature extraction modules to achieve superior compression results. Current end-to-end compression frameworks underperform compared to advanced coding algorithms, primarily due to insufficient spectral feature extraction at high bit rates and challenges in guiding entropy coding. To address these issues, this paper proposes the Mixture Autoregressive Spectral Attention Network (MARSA-Net), featuring two attention mechanisms: Coor-Spec and LD-CAM, and an autoregressive component. Our evaluation on real datasets from satellites demonstrates MARSA-Net's superiority over traditional algorithms, including H.266/VVC, underlining its potential in multispectral image compression. This research contributes to improved compression methods and extends our understanding of spectral feature extraction in multispectral imagery.
C1 [Kong, Fanqiang; Ren, Guanglong; Li, Dan; Hu, Kedi] Nanjing Univ Aeronaut & Astronaut, Coll Astronaut, Nanjing 210016, Jiangsu, Peoples R China.
   [Hu, Yunfang] Jiaozuo Baili United Pigment Co LTD, Jiaozuo 454450, Henan, Peoples R China.
C3 Nanjing University of Aeronautics & Astronautics
RP Ren, GL (corresponding author), Nanjing Univ Aeronaut & Astronaut, Coll Astronaut, Nanjing 210016, Jiangsu, Peoples R China.
EM kongfq@nuaa.edu.cn; rgl19981212@nuaa.edu.cn; hyf861471@163.com;
   danli@nuaa.edu.cn; kedi_hu@nuaa.edu.cn
FU Innovative Research Group Project of the National Natural Science
   Foundation of China
FX No Statement Available
CR Artuger F, 2023, VISUAL COMPUT, V39, P5609, DOI 10.1007/s00371-022-02684-y
   Balle J, 2017, 5 INT C LEARN REPR I
   Ball‚ J, 2018, Arxiv, DOI arXiv:1802.01436
   Ballé J, 2016, PICT COD SYMP, DOI 10.1109/pcs.2016.7906310
   Bellard F, 2015, BPG IMAGE FORMAT
   Dragotti PL, 2000, IEEE T GEOSCI REMOTE, V38, P416, DOI 10.1109/36.823937
   Dua Y, 2022, VISUAL COMPUT, V38, P65, DOI 10.1007/s00371-020-02000-6
   Gao Ge, 2021, P IEEE CVF INT C COM, P14677
   Gregor K, 2014, PR MACH LEARN RES, V32, P1242
   Hagag A, 2017, OPTIK, V131, P1023, DOI 10.1016/j.ijleo.2016.11.172
   Hagag A, 2013, J APPL REMOTE SENS, V7, DOI 10.1117/1.JRS.7.073511
   Hatchett J, 2018, VISUAL COMPUT, V34, P167, DOI 10.1007/s00371-016-1322-0
   He DL, 2022, PROC CVPR IEEE, P5708, DOI 10.1109/CVPR52688.2022.00563
   He DL, 2021, PROC CVPR IEEE, P14766, DOI 10.1109/CVPR46437.2021.01453
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Hu YY, 2022, IEEE T PATTERN ANAL, V44, P4194, DOI 10.1109/TPAMI.2021.3065339
   Ince IF, 2022, VISUAL COMPUT, V38, P1845, DOI 10.1007/s00371-022-02418-0
   Karami A, 2012, IEEE J-STARS, V5, P444, DOI 10.1109/JSTARS.2012.2189200
   Kim BJ, 1997, IEEE DATA COMPR CONF, P251, DOI 10.1109/DCC.1997.582048
   Kong FQ, 2022, SIGNAL PROCESS, V198, DOI 10.1016/j.sigpro.2022.108589
   Kong FQ, 2021, REMOTE SENS-BASEL, V13, DOI 10.3390/rs13010009
   Li D, 2021, PROC CVPR IEEE, P12316, DOI 10.1109/CVPR46437.2021.01214
   Li J, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11070759
   Li X, 2019, IEEE I CONF COMP VIS, P9166, DOI 10.1109/ICCV.2019.00926
   Liang W, 2016, J ELECTRON IMAGING, V25, DOI 10.1117/1.JEI.25.4.043026
   Lu RY, 2020, SIGNAL PROCESS, V177, DOI 10.1016/j.sigpro.2020.107737
   Minnen D, 2020, IEEE IMAGE PROC, P3339, DOI [10.1109/icip40778.2020.9190935, 10.1109/ICIP40778.2020.9190935]
   Minnen D, 2018, ADV NEUR IN, V31
   Mishra D, 2022, SIGNAL PROCESS, V191, DOI 10.1016/j.sigpro.2021.108346
   Mnih V, 2014, ADV NEUR IN, V27
   Ohm J.R., 2018, PICT COD S
   Prion S, 2014, CLIN SIMUL NURS, V10, P587, DOI 10.1016/j.ecns.2014.07.010
   Qian Y., 2010, arXiv e-prints, V2020
   Rabbani M, 2002, J ELECTRON IMAGING, V11, P286, DOI 10.1117/1.1469618.00000
   Shah B., 2023, Vis. Comput., P1
   Tang X, 2021, IEEE T GEOSCI REMOTE, V59, P2430, DOI 10.1109/TGRS.2020.3005431
   Todeschini G, 2017, INVENTIONS-BASEL, V2, DOI 10.3390/inventions2030014
   Vaswani A, 2017, ADV NEUR IN, V30
   Wallace G. K., 1991, Communications of the ACM, V34, P30, DOI 10.1145/103085.103089
   Wang Q, 2020, INT SYM QUAL ELECT, P1, DOI [10.1109/isqed48828.2020.9137057, 10.1109/ISQED48828.2020.9137057, 10.1109/CVPR42600.2020.01155]
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wang XH, 2023, VISUAL COMPUT, V39, P1351, DOI 10.1007/s00371-022-02410-8
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   WITTEN IH, 1987, COMMUN ACM, V30, P520, DOI 10.1145/214762.214771
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Xing CD, 2020, SIGNAL PROCESS, V173, DOI 10.1016/j.sigpro.2020.107585
   Yamano Hiroya, 2013, P51
   Yang LX, 2021, PR MACH LEARN RES, V139
   Yu FS, 2016, Arxiv, DOI [arXiv:1511.07122, DOI 10.48550/ARXIV.1511.07122]
   Zhan K, 2017, J ELECTRON IMAGING, V26, DOI 10.1117/1.JEI.26.6.063004
   Zhengxue Cheng, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7936, DOI 10.1109/CVPR42600.2020.00796
   Zikiou N, 2020, VISUAL COMPUT, V36, P1473, DOI 10.1007/s00371-019-01753-z
NR 52
TC 0
Z9 0
U1 2
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2023 NOV 27
PY 2023
DI 10.1007/s00371-023-03166-5
EA NOV 2023
PG 24
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CB1Q4
UT WOS:001122703200001
DA 2024-07-18
ER

PT J
AU Sun, SH
   Song, KC
   Man, Y
   Dong, HW
   Yan, YH
AF Sun, Shenghui
   Song, Kechen
   Man, Yi
   Dong, Hongwen
   Yan, Yunhui
TI DCBFusion: an infrared and visible image fusion method through detail
   enhancement, contrast reserve and brightness balance
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Image fusion; Feature extraction; Feature enhancement; Complementary
   information fusion; Deep learning
ID FRAMEWORK; NETWORK; NEST
AB Due to the complementary nature of visible and infrared images, they are widely used in image fusion to generate fused images containing more comprehensive information. Although existing fusion methods have achieved good results, there are some problems. In some cases, the features of an image are affected by a shot from another modality, which leads to the problem of background contamination and missing information. To solve these problems, we designed a visible and infrared image fusion network starting from three key factors that affect structural similarity. Our fusion network can avoid these problems through detail enhancement, contrast preservation, and luminance balancing. Through the cross-stage feature extraction and multi-scale feature enhancement modules achieve detail enhancement. The complementary information fusion module finds and fuses complementary information from different images to achieve contrast preservation. The loss function performs luminance balancing. Comparison and generalization experiments on several other public datasets show that our network effectively avoids background contamination and information loss and achieves outstanding results in both quantitative and qualitative aspects.
C1 [Sun, Shenghui; Man, Yi; Yan, Yunhui] Northeastern Univ, Sch Mech Engn & Automat, Shenyang, Liaoning, Peoples R China.
   [Sun, Shenghui; Song, Kechen; Man, Yi; Yan, Yunhui] Northeastern Univ, Natl Frontiers Sci Ctr Ind Intelligence & Syst Opt, Shenyang 110819, Peoples R China.
   [Sun, Shenghui; Song, Kechen; Man, Yi; Yan, Yunhui] Northeastern Univ, Key Lab Data Analyt & Optimizat Smart Ind, Minist Educ, Shenyang, Peoples R China.
   [Dong, Hongwen] Shanghai Radio Equipment Res Inst, Shanghai 200090, Peoples R China.
C3 Northeastern University - China; Northeastern University - China;
   Northeastern University - China
RP Yan, YH (corresponding author), Northeastern Univ, Sch Mech Engn & Automat, Shenyang, Liaoning, Peoples R China.; Song, KC; Yan, YH (corresponding author), Northeastern Univ, Natl Frontiers Sci Ctr Ind Intelligence & Syst Opt, Shenyang 110819, Peoples R China.; Song, KC; Yan, YH (corresponding author), Northeastern Univ, Key Lab Data Analyt & Optimizat Smart Ind, Minist Educ, Shenyang, Peoples R China.
EM songkc@me.neu.edu.cn; yanyh@mail.neu.edu.cn
RI Song, Kechen/T-1896-2019
OI Song, Kechen/0000-0002-7636-3460
FU This work was supported by the National Natural Science Foundation of
   China (51805078), the Fundamental Research Funds for the Central
   Universities (N2103011), the Central Guidance on Local Science and
   Technology Development Fund (2022JH6/100100023), and t [51805078];
   National Natural Science Foundation of China [N2103011]; Fundamental
   Research Funds for the Central Universities [2022JH6/100100023]; Central
   Guidance on Local Science and Technology Development Fund [B16009]; 111
   Project
FX This work was supported by the National Natural Science Foundation of
   China (51805078), the Fundamental Research Funds for the Central
   Universities (N2103011), the Central Guidance on Local Science and
   Technology Development Fund (2022JH6/100100023), and the 111 Project
   (B16009).
CR Chen LC, 2017, Arxiv, DOI arXiv:1606.00915
   Chen YT, 2023, J KING SAUD UNIV-COM, V35, DOI 10.1016/j.jksuci.2023.101567
   Chen YT, 2023, INT J MACH LEARN CYB, V14, P2945, DOI 10.1007/s13042-023-01811-y
   Chen YT, 2023, J VIS COMMUN IMAGE R, V91, DOI 10.1016/j.jvcir.2023.103776
   Chen YT, 2024, VISUAL COMPUT, V40, P489, DOI 10.1007/s00371-023-02795-0
   Cheng CY, 2023, INFORM FUSION, V92, P80, DOI 10.1016/j.inffus.2022.11.010
   Cui YM, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P8118, DOI 10.1109/ICCV48922.2021.00803
   Guo C, 2022, VISUAL COMPUT, V38, P2555, DOI 10.1007/s00371-021-02131-4
   Ha Q, 2017, IEEE INT C INT ROBOT, P5108, DOI 10.1109/IROS.2017.8206396
   Kim Y, 2015, IEEE IMAGE PROC, P1404, DOI 10.1109/ICIP.2015.7351031
   Li H, 2022, Arxiv, DOI arXiv:1804.08992
   Li H, 2021, INFORM FUSION, V73, P72, DOI 10.1016/j.inffus.2021.02.023
   Li H, 2020, IEEE T INSTRUM MEAS, V69, P9645, DOI 10.1109/TIM.2020.3005230
   Li H, 2019, IEEE T IMAGE PROCESS, V28, P2614, DOI 10.1109/TIP.2018.2887342
   Li Y, 2020, MULTIMED TOOLS APPL, V79, P34685, DOI 10.1007/s11042-020-09301-x
   Liu D., 2021, Proc. AAAI Conf. Artif. Intell, V35, P7, DOI [10.1609/aaai.v35i7.16760, DOI 10.1609/AAAI.V35I7.16760]
   Liu GT, 2022, INFRARED PHYS TECHN, V120, DOI 10.1016/j.infrared.2021.103938
   Liu JY, 2022, PROC CVPR IEEE, P5792, DOI 10.1109/CVPR52688.2022.00571
   Liu L, 2019, INFRARED PHYS TECHN, V100, P99, DOI 10.1016/j.infrared.2019.05.019
   Liu Z, 2022, PROC CVPR IEEE, P11966, DOI 10.1109/CVPR52688.2022.01167
   Lu RT, 2023, VISUAL COMPUT, V39, P2321, DOI 10.1007/s00371-022-02438-w
   Lu Yawen, 2023, 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), P18063, DOI 10.1109/CVPR52729.2023.01732
   Luo H., 2019, P 2019 INT C ART INT, P583, DOI DOI 10.1145/3349341.3349472
   Luo X, 2023, IEEE T CIRC SYST VID, V33, P7354, DOI 10.1109/TCSVT.2023.3281462
   Ma JY, 2021, IEEE T INSTRUM MEAS, V70, DOI 10.1109/TIM.2020.3038013
   Ma JY, 2021, IEEE T INSTRUM MEAS, V70, DOI 10.1109/TIM.2021.3075747
   Ma JY, 2020, IEEE T IMAGE PROCESS, V29, P4980, DOI 10.1109/TIP.2020.2977573
   Ma JY, 2019, INFORM FUSION, V48, P11, DOI 10.1016/j.inffus.2018.09.004
   Mustafa HT, 2020, OPTIK, V224, DOI 10.1016/j.ijleo.2020.165409
   Nie C., 2021, SSRN Electron. J, DOI [10.2139/ssrn.3982278, DOI 10.2139/SSRN.3982278]
   Rao Dongyu, 2023, IEEE Trans Image Process, VPP, DOI 10.1109/TIP.2023.3273451
   Rasti B, 2020, INFORM FUSION, V64, P121, DOI 10.1016/j.inffus.2020.07.002
   Ren L, 2021, SIGNAL PROCESS, V186, DOI 10.1016/j.sigpro.2021.108108
   Tang LF, 2022, INFORM FUSION, V83, P79, DOI 10.1016/j.inffus.2022.03.007
   Tang LF, 2022, INFORM FUSION, V82, P28, DOI 10.1016/j.inffus.2021.12.004
   Tang QW, 2024, VISUAL COMPUT, V40, P2443, DOI 10.1007/s00371-023-02929-4
   Wang BL, 2024, VISUAL COMPUT, V40, P1997, DOI 10.1007/s00371-023-02898-8
   Wang CY, 2020, IEEE COMPUT SOC CONF, P1571, DOI 10.1109/CVPRW50498.2020.00203
   Wang D, 2022, Arxiv, DOI [arXiv:2205.11876, DOI 10.48550/ARXIV.2205.11876]
   Wang L, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P4324, DOI 10.1145/3474085.3475572
   Wang L, 2019, IEEE I CONF COMP VIS, P8697, DOI 10.1109/ICCV.2019.00879
   Wang L, 2019, IEEE IMAGE PROC, P974, DOI [10.1109/ICIP.2019.8803051, 10.1109/icip.2019.8803051]
   Wang WG, 2022, Arxiv, DOI arXiv:2209.07383
   Wang ZS, 2022, IEEE T INSTRUM MEAS, V71, DOI 10.1109/TIM.2021.3139654
   Xu DD, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10020554
   Xu H, 2021, IEEE T COMPUT IMAG, V7, P824, DOI 10.1109/TCI.2021.3100986
   Xu H, 2022, IEEE T PATTERN ANAL, V44, P502, DOI 10.1109/TPAMI.2020.3012548
   Yan L., 2022, 2022 IJCAI, P2769
   Yan Lu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13376, DOI 10.1109/CVPR42600.2020.01339
   Yang Y, 2021, IEEE T INSTRUM MEAS, V70, DOI 10.1109/TIM.2020.3011766
   Yubo Wu, 2020, 2020 8th International Conference on Digital Home (ICDH), P87, DOI 10.1109/ICDH51081.2020.00023
   Zhang CF, 2022, ARAB J SCI ENG, V47, P10295, DOI 10.1007/s13369-021-06380-2
   Zhang H, 2021, INFORM FUSION, V76, P323, DOI 10.1016/j.inffus.2021.06.008
   Zhang JM, 2023, MULTIMED TOOLS APPL, DOI 10.1007/s11042-023-15429-3
   Zhang JM, 2022, J AMB INTEL SMART EN, V14, P317, DOI 10.3233/AIS-220038
   Zhang JM, 2022, HUM-CENT COMPUT INFO, V12, DOI 10.22967/HCIS.2022.12.023
   Zhang Y, 2020, INFORM FUSION, V54, P99, DOI 10.1016/j.inffus.2019.07.011
   Zhang YH, 2024, VISUAL COMPUT, V40, P1805, DOI 10.1007/s00371-023-02887-x
   Zhao ZX, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P970
   Zhou L, 2023, J VIS COMMUN IMAGE R, V90, DOI 10.1016/j.jvcir.2022.103725
NR 60
TC 0
Z9 0
U1 5
U2 14
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2023 NOV 2
PY 2023
DI 10.1007/s00371-023-03134-z
EA NOV 2023
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA W5MV3
UT WOS:001092075400001
DA 2024-07-18
ER

PT J
AU Zhao, XY
   Li, QZ
   Chao, Y
   Wang, QY
   He, ZX
   Liang, D
AF Zhao, Xinyue
   Li, Quanzhi
   Chao, Yue
   Wang, Quanyou
   He, Zaixing
   Liang, Dong
TI RT-less: a multi-scene RGB dataset for 6D pose estimation of reflective
   texture-less objects
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Pose estimation; Pose measurement; Object detection; Instance
   segmentation; Reflective; Texture-less; Machine vision
AB The 6D (6 Degree of freedom) pose estimation (or pose measurement) of machined reflective texture-less objects, which are common in industry, is a significant but challenging technique. It has attracted increasing attention in academia and industry. However, it is difficult to obtain suitable public datasets of such objects, which makes relevant studies inconvenient. Thus, we proposed the Reflective Texture-Less (RT-Less) object dataset, which is a new public dataset of reflective texture-less metal parts for pose estimation research. The dataset contains 38 machined texture-less reflective metal parts in total. Different parts demonstrate the symmetry and similarity of shape and size. The dataset contains 289 K RGB images and the same number of masks, including 25,080 real images, 250,800 synthetic images in the training set, and 13,312 real images captured in 32 different scenes in the test set. The dataset also provides accurate ground truth poses, bounding-box annotations and masks for these images, which makes RT-Less suitable for object detection and instance segmentation. To improve the accuracy of the ground truth, an iterative pose optimization method using only RGB images is proposed. Baselines of the state-of-the-art pose estimation methods are provided for further comparative studies. The dataset and results of baselines are available at: http://www.zju-rtl.cn/RT-Less/.
C1 [Zhao, Xinyue; Li, Quanzhi; Chao, Yue; Wang, Quanyou; He, Zaixing] Zhejiang Univ, Sch Mech Engn, State Key Lab Fluid Power & Mechatron Syst, Hangzhou 310027, Peoples R China.
   [Liang, Dong] Nanjing Univ Aeronaut & Astronaut, Coll Comp Sci & Technol, Nanjing 210016, Peoples R China.
C3 Zhejiang University; Nanjing University of Aeronautics & Astronautics
RP He, ZX (corresponding author), Zhejiang Univ, Sch Mech Engn, State Key Lab Fluid Power & Mechatron Syst, Hangzhou 310027, Peoples R China.
EM zaixinghe@zju.edu.cn
FU This work was supported in part by the National Natural Science
   Foundation of China under Grants 52275514 and 52275547, and in part by
   the Zhejiang Provincial Natural Science Foundation of China under Grant
   LY21E050021. [52275514, 52275547]; National Natural Science Foundation
   of China [LY21E050021]; Zhejiang Provincial Natural Science Foundation
   of China
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 52275514 and 52275547, and in part by
   the Zhejiang Provincial Natural Science Foundation of China under Grant
   LY21E050021.
CR Brachmann E, 2014, LECT NOTES COMPUT SC, V8690, P536, DOI 10.1007/978-3-319-10605-2_35
   Brégier R, 2017, IEEE INT CONF COMP V, P2209, DOI 10.1109/ICCVW.2017.258
   Calli B, 2015, PROCEEDINGS OF THE 17TH INTERNATIONAL CONFERENCE ON ADVANCED ROBOTICS (ICAR), P510, DOI 10.1109/ICAR.2015.7251504
   Crivellaro A, 2015, IEEE I CONF COMP VIS, P4391, DOI 10.1109/ICCV.2015.499
   Doumanoglou A, 2016, PROC CVPR IEEE, P3583, DOI 10.1109/CVPR.2016.390
   Drost B, 2017, IEEE INT CONF COMP V, P2200, DOI 10.1109/ICCVW.2017.257
   Dwibedi D, 2017, IEEE I CONF COMP VIS, P1310, DOI 10.1109/ICCV.2017.146
   Eppner C., 2016, ROBOTICS SCI SYSTEMS, P36, DOI DOI 10.15607/RSS.2016.XII.036
   Feng Y., 2022, IEEE INT C ROBOT AUT, P405, DOI 10.1109/ICRA46639.2022.9811728
   Garrido-Jurado S, 2014, PATTERN RECOGN, V47, P2280, DOI 10.1016/j.patcog.2014.01.005
   Haugaard RL, 2022, PROC CVPR IEEE, P6739, DOI 10.1109/CVPR52688.2022.00663
   He Z., 2023, IEEE Trans. Robot, V5, P69
   He Z., 2023, IEEE-ASME Trans. Mechatron, V5, P96
   He ZX, 2022, IEEE-ASME T MECH, V27, P3198, DOI 10.1109/TMECH.2021.3109344
   Hinterstoisser V., 2012, P COMP VIS ACCV 2012, P548
   Hodan T, 2018, LECT NOTES COMPUT SC, V11214, P19, DOI 10.1007/978-3-030-01249-6_2
   Hodan T, 2017, IEEE WINT CONF APPL, P880, DOI 10.1109/WACV.2017.103
   Kaskman R, 2019, IEEE INT CONF COMP V, P2767, DOI 10.1109/ICCVW.2019.00338
   Kleeberger K, 2019, IEEE INT C INT ROBOT, P2573, DOI [10.1109/IROS40897.2019.8967594, 10.1109/iros40897.2019.8967594]
   Li S., 2022, Vis. Comput, V5, P1
   Li XZ, 2022, IEEE ROBOT AUTOM LET, V7, P3961, DOI 10.1109/LRA.2022.3149026
   Liang D, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2021.3136350
   Liang D, 2021, INT C PATT RECOG, P3807, DOI 10.1109/ICPR48806.2021.9412814
   Liu D., 2022, Vis. Comput, V5, P1
   Muoz E, 2016, IEEE INT CONF ROBOT, P5623, DOI 10.1109/ICRA.2016.7487781
   Rennie C, 2016, IEEE ROBOT AUTOM LET, V1, P1179, DOI 10.1109/LRA.2016.2532924
   Richter-Kluge J, 2019, IEEE INT C INT ROBOT, P893, DOI 10.1109/IROS40897.2019.8967937
   Sun H, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20123606
   Tejani A, 2018, IEEE T PATTERN ANAL, V40, P119, DOI 10.1109/TPAMI.2017.2665623
   Tremblay J, 2018, IEEE COMPUT SOC CONF, P2119, DOI 10.1109/CVPRW.2018.00275
   Wei ZQ, 2022, IEEE J-STARS, V15, P2721, DOI 10.1109/JSTARS.2022.3158903
   Wu CR, 2022, IEEE T IND ELECTRON, V69, P2718, DOI 10.1109/TIE.2021.3070501
   Xiang Yu., 2018, PoseCNN: a convolutional neural network for 6D object pose estimation in cluttered scenes
   Xiao JX, 2010, PROC CVPR IEEE, P3485, DOI 10.1109/CVPR.2010.5539970
   Yang J, 2021, IEEE INT C INT ROBOT, P9788, DOI 10.1109/IROS51168.2021.9635871
   Yang J, 2021, IEEE ROBOT AUTOM LET, V6, P4472, DOI 10.1109/LRA.2021.3068706
   Yuan HL, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21041299
   Zabulis X, 2018, VISUAL COMPUT, V34, P193, DOI 10.1007/s00371-016-1326-9
   Zhang ZY, 2000, IEEE T PATTERN ANAL, V22, P1330, DOI 10.1109/34.888718
NR 39
TC 1
Z9 1
U1 6
U2 16
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2023 OCT 18
PY 2023
DI 10.1007/s00371-023-03097-1
EA OCT 2023
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA U8RQ0
UT WOS:001087422400001
DA 2024-07-18
ER

PT J
AU Paul, S
   Patterson, Z
   Bouguila, N
AF Paul, Sneha
   Patterson, Zachary
   Bouguila, Nizar
TI DualMLP: a two-stream fusion model for 3D point cloud classification
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Point cloud classification; 3D computer vision; Supervised learning
ID NEURAL-NETWORKS
AB In this paper, we present DualMLP, a novel 3D model that introduces the idea of a two-stream network for existing 3D models to handle the trade-off between the number of points and the computational overhead. Existing works on point clouds use a small subset of points sampled from the entire 3D object as input. Although increasing the number of input points can enhance scene understanding, it also incurs a higher computational cost for existing networks. To tackle this challenge, we propose a novel architecture called DualMLP, which effectively mitigates the linear increase in computational expense as the number of input points grows. While we evaluate this concept on PointMLP and demonstrate its effectiveness, the idea can be applied to other existing models with minimal adjustments. DualMLP consists of two branches: DenseNet and SparseNet. The SparseNet, a relatively larger network, samples a small number of points from the complete point cloud, while the DenseNet, a lightweight network, takes in a larger number of points as input. Extensive experiments on the ScanObjectNN and ModelNet40 datasets demonstrate the effectiveness of the proposed model, achieving a 1.00% and 0.81% improvement over PointMLP for ScanObjectNN and ModelNet40 while being computationally efficient than the original PointMLP. To ensure the reproducibility of our experimental results, the code for this work is publicly available at https://github.com/snehaputul/DualMLP.
C1 [Paul, Sneha; Patterson, Zachary; Bouguila, Nizar] Concordia Univ, Concordia Inst Informat Syst Engn CIISE, Montreal, PQ, Canada.
C3 Concordia University - Canada
RP Paul, S (corresponding author), Concordia Univ, Concordia Inst Informat Syst Engn CIISE, Montreal, PQ, Canada.
EM sneha.paul@mail.concordia.ca; zachary.patterson@concordia.ca;
   nizar.bouguila@concordia.ca
OI Paul, Sneha/0000-0001-7731-4196
CR Phan AV, 2018, NEURAL NETWORKS, V108, P533, DOI 10.1016/j.neunet.2018.09.001
   Bruna J, 2014, Arxiv, DOI [arXiv:1312.6203, DOI 10.48550/ARXIV.1312.6203]
   Choe J, 2022, LECT NOTES COMPUT SC, V13687, P620, DOI 10.1007/978-3-031-19812-0_36
   Cui YM, 2023, Arxiv, DOI arXiv:2305.07814
   Cui YM, 2021, NEUROCOMPUTING, V432, P300, DOI 10.1016/j.neucom.2020.12.067
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Deng-Ping Fan, 2020, Medical Image Computing and Computer Assisted Intervention - MICCAI 2020. 23rd International Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12266), P263, DOI 10.1007/978-3-030-59725-2_26
   Fang Y, 2015, PROC CVPR IEEE, P2319, DOI 10.1109/CVPR.2015.7298845
   Feichtenhofer C, 2019, IEEE I CONF COMP VIS, P6201, DOI 10.1109/ICCV.2019.00630
   Goyal P, 2018, Arxiv, DOI arXiv:1706.02677
   Guo MH, 2021, COMPUT VIS MEDIA, V7, P187, DOI 10.1007/s41095-021-0229-5
   Hamdi A, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1, DOI 10.1109/ICCV48922.2021.00007
   Han XF, 2021, Arxiv, DOI arXiv:2104.13636
   He YQ, 2023, J VIS COMMUN IMAGE R, V91, DOI 10.1016/j.jvcir.2023.103769
   Li Y., 2018, Adv. Neural Inf. Process. Syst., V31
   Li YY, 2016, ADV NEUR IN, V29
   Liu YC, 2019, IEEE I CONF COMP VIS, P5238, DOI 10.1109/ICCV.2019.00534
   Liu YC, 2019, PROC CVPR IEEE, P8887, DOI 10.1109/CVPR.2019.00910
   Ma X, 2022, Arxiv, DOI arXiv:2202.07123
   Paul S., 2020, J Eng Sci, V11, P83, DOI [10.3329/jes.v11i1.49549, DOI 10.3329/JES.V11I1.49549]
   Paul S, 2023, 2023 20TH CONFERENCE ON ROBOTS AND VISION, CRV, P273, DOI 10.1109/CRV60082.2023.00042
   Paul S, 2022, LECT NOTES COMPUT SC, V13813, P253, DOI 10.1007/978-3-031-23028-8_26
   Qi C., 2017, NeurIPS, P5105, DOI [10.1109/ CVPR.2017.16, DOI 10.1109/CVPR.2017.16]
   Qi CR, 2016, PROC CVPR IEEE, P5648, DOI 10.1109/CVPR.2016.609
   Qiu S, 2021, IEEE WINT CONF APPL, P3812, DOI 10.1109/WACV48630.2021.00386
   Roy S., 2019, 2019 1 INT C ADV SCI, P1
   Rusu RB, 2009, IEEE INT CONF ROBOT, P1848
   Savva M., 2016, P EUR WORKSH 3D OBJ, P89, DOI DOI 10.2312/3DOR.20161092
   Su H, 2015, IEEE I CONF COMP VIS, P945, DOI 10.1109/ICCV.2015.114
   Thomas H, 2019, IEEE I CONF COMP VIS, P6420, DOI 10.1109/ICCV.2019.00651
   Uy MA, 2019, IEEE I CONF COMP VIS, P1588, DOI 10.1109/ICCV.2019.00167
   Vinyals O, 2016, Arxiv, DOI arXiv:1511.06391
   Wang HF, 2022, PATTERN RECOGN, V121, DOI 10.1016/j.patcog.2021.108251
   Wu WX, 2019, PROC CVPR IEEE, P9613, DOI 10.1109/CVPR.2019.00985
   Wu ZR, 2015, PROC CVPR IEEE, P1912, DOI 10.1109/CVPR.2015.7298801
   Xu Xiuwei, 2023, 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), P5313, DOI 10.1109/CVPR52729.2023.00514
   Xu YF, 2018, LECT NOTES COMPUT SC, V11212, P90, DOI 10.1007/978-3-030-01237-3_6
   Yan X, 2020, PROC CVPR IEEE, P5588, DOI 10.1109/CVPR42600.2020.00563
   Zamorski M, 2023, COMPUT VIS IMAGE UND, V228, DOI 10.1016/j.cviu.2023.103621
   Ze Liu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12368), P326, DOI 10.1007/978-3-030-58592-1_20
   Zhao C, 2022, PATTERN RECOGN, V127, DOI 10.1016/j.patcog.2022.108626
NR 41
TC 2
Z9 2
U1 3
U2 7
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2023 OCT 10
PY 2023
DI 10.1007/s00371-023-03114-3
EA OCT 2023
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA T9LD5
UT WOS:001081117300002
DA 2024-07-18
ER

PT J
AU Lu, N
   Chen, YZ
AF Lu, Ning
   Chen, Yizhou
TI Multi-category domain-dependent feature-based medical image translation
SO VISUAL COMPUTER
LA English
DT Article
DE Medical image; Cross-domain image translation; Feature consistency loss;
   Multi-view learning
ID NEURAL-NETWORK
AB The challenge of inaccurate information containment in synthetic images during the process of cross-domain medical image translation could be resolved by using a common strategy of integrating the loss of the feature consistency of the real/synthetic image as a penalty factor into the loss function of the translator. However, the existing methods are capable of using only the "domain-independent" feature of the image when the aligned images are scarcity, which results in the under-utilization of the image information. In the present study, a novel feature consistency loss computing and integration method based on the "domain-dependent" features was proposed, and a multi-category feature consistency-cross-domain image translation (MFC-CIT) model was constructed. The present study is the first to utilize the image feature information related to image domain in the process of cross-domain medical image translation. In the proposed method, the MFC module was first trained on the basis of supervised learning on a limited number of paired real images. Next, cross-domain image translation training based on unsupervised learning was performed on unpaired datasets by the CIT module, and this process was constrained by the loss of feature consistency of the real/synthetic image obtained in the MFC module. The experimental results on two datasets demonstrate that the proposed method effectively improves the translation accuracy of synthetic images.
C1 [Lu, Ning] Guangdong Polytech Sci & Technol, Zhuhai, Guangdong, Peoples R China.
   [Chen, Yizhou] Zhejiang Univ Technol, Coll Comp Sci & Technol, Hangzhou, Peoples R China.
C3 Guangdong Polytechnic of Science & Technology; Zhejiang University of
   Technology
RP Chen, YZ (corresponding author), Zhejiang Univ Technol, Coll Comp Sci & Technol, Hangzhou, Peoples R China.
EM sam@mail.gdit.edu.cn; yizhou@zjut.edu.cn
FU The research would like to thank the financial supports of the Key
   Project for University of Department of Education of Guangdong Province
   of China Funds (Natural) under Grant 2019GZDXM005. [2019GZDXM005]; Key
   Project for University of Department of Education of Guangdong Province
   of China Funds (Natural)
FX The research would like to thank the financial supports of the Key
   Project for University of Department of Education of Guangdong Province
   of China Funds (Natural) under Grant 2019GZDXM005.
CR Andrew G., 2013, ICML, P1247
   Armanious K, 2020, COMPUT MED IMAG GRAP, V79, DOI 10.1016/j.compmedimag.2019.101684
   Bai J, 2020, VISUAL COMPUT, V36, P2145, DOI 10.1007/s00371-020-01943-0
   Bayoudh K, 2022, VISUAL COMPUT, V38, P2939, DOI 10.1007/s00371-021-02166-7
   BenTaieb A, 2018, IEEE T MED IMAGING, V37, P792, DOI 10.1109/TMI.2017.2781228
   Boni KNDB, 2020, PHYS MED BIOL, V65, DOI 10.1088/1361-6560/ab7633
   Burgos N, 2017, PHYS MED BIOL, V62, P4237, DOI 10.1088/1361-6560/aa66bf
   Cai JZ, 2019, MED IMAGE ANAL, V52, P174, DOI 10.1016/j.media.2018.12.002
   Chartsias A, 2018, IEEE T MED IMAGING, V37, P803, DOI 10.1109/TMI.2017.2764326
   Chen Y., 2022, COMPUT METHODS PROG
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Guo LH, 2017, IEEE ENG MED BIO, P1748, DOI 10.1109/EMBC.2017.8037181
   Guo XQ, 2022, IEEE T MED IMAGING, V41, P434, DOI 10.1109/TMI.2021.3114329
   Hardoon DR, 2004, NEURAL COMPUT, V16, P2639, DOI 10.1162/0899766042321814
   Hensel M, 2017, ADV NEUR IN, V30
   Hiasa Y, 2018, LECT NOTES COMPUT SC, V11037, P31, DOI 10.1007/978-3-030-00536-8_4
   Hussein S, 2018, I S BIOMED IMAGING, P800, DOI 10.1109/ISBI.2018.8363693
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jiang GF, 2019, LECT NOTES COMPUT SC, V11769, P801, DOI 10.1007/978-3-030-32226-7_89
   Jiao J., 2019, LECT NOTES COMPUTER, V11861
   Joyce T., 2019, LECT NOTES COMPUTER, V11827
   Karageorgiou E., 2012, PHYS MED BIOL, V9
   Karthik K, 2021, VISUAL COMPUT, V37, P1837, DOI 10.1007/s00371-020-01941-2
   Lartaud PJ, 2022, MED PHYS, V49, P1108, DOI 10.1002/mp.15310
   Le An, 2015, Patch-Based Techniques in Medical Imaging. First International Workshop, Patch-MI 2015, held in conjunction with MICCAI 2015. Revised Selected Papers: LNCS 9467, P1, DOI 10.1007/978-3-319-28194-0_1
   Liang X, 2019, PHYS MED BIOL, V64, DOI 10.1088/1361-6560/ab22f9
   Liu G., 2022, VISUAL COMPUT
   Liu J, 2022, IEEE T CIRC SYST VID, V32, P2773, DOI 10.1109/TCSVT.2021.3098707
   Liu MY, 2017, ADV NEUR IN, V30
   Nie D, 2018, IEEE T BIO-MED ENG, V65, P2720, DOI 10.1109/TBME.2018.2814538
   Roy Snehashis, 2017, Simulation and Synthesis in Medical Imaging. Second International Workshop, SASHIMI 2017. Held in Conjunction with MICCAI 2017. Proceedings: LNCS 10557, P24, DOI 10.1007/978-3-319-68127-6_3
   Sheikh HR, 2005, IEEE T IMAGE PROCESS, V14, P2117, DOI 10.1109/TIP.2005.859389
   Sun SL, 2013, NEURAL COMPUT APPL, V23, P2031, DOI 10.1007/s00521-013-1362-6
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   van Engelen A, 2012, PHYS MED BIOL, V57, P241, DOI 10.1088/0031-9155/57/1/241
   Wang CJ, 2019, LECT NOTES COMPUT SC, V11905, P245, DOI 10.1007/978-3-030-33843-5_23
   Wang L., 2022, VISUAL COMPUT
   Wang MN, 2019, J MED BIOL ENG, V39, P1, DOI 10.1007/s40846-018-0390-1
   Welander Per, 2018, arXiv
   Wolterink J.M., 2017, LECT NOTES COMPUTER, V557
   Xiang L, 2018, MED IMAGE ANAL, V47, P31, DOI 10.1016/j.media.2018.03.011
   Xing FY, 2019, LECT NOTES COMPUT SC, V11764, P740, DOI 10.1007/978-3-030-32239-7_82
   Yang HR, 2018, LECT NOTES COMPUT SC, V11045, P174, DOI 10.1007/978-3-030-00889-5_20
   Yang Lei, 2019, Artificial Intelligence in Radiation Therapy. First International Workshop, AIRT 2019. Held in Conjunction with MICCAI 2019. Proceedings. Lecture Notes in Computer Science (LNCS 11850), P154, DOI 10.1007/978-3-030-32486-5_19
   Yu F., 2020, IEEE T INSTRUM MEAS, V70, P1
   Yurt M, 2021, MED IMAGE ANAL, V70, DOI 10.1016/j.media.2020.101944
   Zhang YQ, 2022, IEEE J BIOMED HEALTH, V26, P774, DOI 10.1109/JBHI.2021.3094187
   Zhao FQ, 2019, LECT NOTES COMPUT SC, V11767, P475, DOI 10.1007/978-3-030-32251-9_52
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 49
TC 1
Z9 1
U1 4
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2024
VL 40
IS 6
BP 4519
EP 4538
DI 10.1007/s00371-023-03096-2
EA SEP 2023
PG 20
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TV2X4
UT WOS:001073086400002
DA 2024-07-18
ER

PT J
AU Zhou, QD
   Shen, SY
   Yu, SS
   Duan, DL
   Yuan, YB
   Lv, HJ
   Lin, HJ
AF Zhou, Qidong
   Shen, Shuyuan
   Yu, Songsen
   Duan, Delin
   Yuan, Yibo
   Lv, Haojie
   Lin, Huanjie
TI A novel robust image watermarking algorithm based on polar decomposition
   and image geometric correction
SO VISUAL COMPUTER
LA English
DT Article
DE Image watermarking; Lifting wavelet transform; Polar decomposition;
   Hadamard transform; Image geometric correction
ID SCHEME; DCT; SVD
AB Nowadays, many image watermarking algorithms based on matrix decomposition have been proposed. In this paper, the properties of polar decomposition in the field of image watermarking is found, that is, embedding the watermark information into diagonal elements of the semi-positive definite matrix can improve the invisibility of the watermark. However, the diagonal elements have poor robustness. Based on this, lifting wavelet transform, which is the second generation wavelet transform, and Hadamard transform are used to improve the robustness of watermarking. In addition, image geometric correction algorithms are also combined to resist geometric attacks. The robustness and comparison experiments show that the proposed algorithm is superior to some state-of-the-art algorithms in some attacks.
C1 [Zhou, Qidong; Shen, Shuyuan; Yu, Songsen; Duan, Delin; Yuan, Yibo; Lv, Haojie; Lin, Huanjie] South China Normal Univ, Sch Software, Foshan 528225, Peoples R China.
C3 South China Normal University
RP Shen, SY (corresponding author), South China Normal Univ, Sch Software, Foshan 528225, Peoples R China.
EM 2022024254@m.scnu.edu.cn; ssyuan16@m.scnu.edu.cn; hjloo97@m.scnu.edu.cn
OI Zhou, Qidong/0009-0007-4957-7272
FU Guangdong basic and applied basic research fund [2021A1515011171,
   2020B1515120089, 2023A1515011472]; Guangzhou basic research plan, basic
   and applied basic research Project [20210208028, 202102080410]
FX AcknowledgementsWe sincerely thank the editors and reviewers. This work
   is supported by the Guangdong basic and applied basic research fund
   (No.2021A1515011171, No.2020B1515120089, No.2023A1515011472), and the
   Guangzhou basic research plan, basic and applied basic research Project
   (No.20210208028, No.202102080410).
CR Ahmadi M, 2020, EXPERT SYST APPL, V146, DOI 10.1016/j.eswa.2019.113157
   Andalibi M, 2015, IEEE T IMAGE PROCESS, V24, P5060, DOI 10.1109/TIP.2015.2476961
   Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014
   Begum M, 2022, J KING SAUD UNIV-COM, V34, P5856, DOI 10.1016/j.jksuci.2021.07.012
   Cao HJ, 2022, OPTIK, V262, DOI 10.1016/j.ijleo.2022.169319
   Castleman K. R., 1996, Digital Image Processing
   Chan CK, 2004, PATTERN RECOGN, V37, P469, DOI 10.1016/j.patcog.2003.08.007
   Chen SY, 2022, OPTIK, V249, DOI 10.1016/j.ijleo.2021.168231
   Dhar PK, 2020, SYMMETRY-BASEL, V12, DOI 10.3390/sym12030486
   Ernawan F, 2018, IEEE ACCESS, V6, P20464, DOI 10.1109/ACCESS.2018.2819424
   Fares K, 2020, OPTIK, V208, DOI 10.1016/j.ijleo.2020.164562
   Gul E, 2023, EXPERT SYST APPL, V212, DOI 10.1016/j.eswa.2022.118730
   Gul E, 2022, CONCURR COMP-PRACT E, V34, DOI 10.1002/cpe.6884
   Han SC, 2022, OPTIK, V268, DOI 10.1016/j.ijleo.2022.169832
   HIGHAM NJ, 1986, SIAM J SCI STAT COMP, V7, P1160, DOI 10.1137/0907079
   Horasan F, 2022, OPTIK, V259, DOI 10.1016/j.ijleo.2022.168958
   Hsu CS, 2020, MULTIMED TOOLS APPL, V79, P11297, DOI 10.1007/s11042-019-08367-6
   Hu FX, 2023, VISUAL COMPUT, V39, P4573, DOI 10.1007/s00371-022-02610-2
   Kahlessenane F, 2021, J AMB INTEL HUM COMP, V12, P2931, DOI 10.1007/s12652-020-02450-9
   Khanam T, 2020, SYMMETRY-BASEL, V12, DOI 10.3390/sym12010052
   Khare P, 2021, MULTIDIM SYST SIGN P, V32, P131, DOI 10.1007/s11045-020-00732-1
   Lai CC, 2011, OPT COMMUN, V284, P938, DOI 10.1016/j.optcom.2010.10.047
   Li FY, 2022, IEEE GEOSCI REMOTE S, V19, DOI 10.1109/LGRS.2020.3048110
   Liu DR, 2022, VEHICLE SYST DYN, V60, P433, DOI 10.1080/00423114.2020.1817508
   Liu GD, 2023, INFORM PROCESS MANAG, V60, DOI 10.1016/j.ipm.2022.103122
   Liu JX, 2019, IEEE ACCESS, V7, P80849, DOI 10.1109/ACCESS.2019.2915596
   Lu J., 2022, ARXIV, DOI DOI 10.48550/ARXIV.2201.00145
   Luo YL, 2022, CIRC SYST SIGNAL PR, V41, P6931, DOI 10.1007/s00034-022-02099-z
   Luo YL, 2021, EXPERT SYST APPL, V168, DOI 10.1016/j.eswa.2020.114272
   Maity SP, 2010, AEU-INT J ELECTRON C, V64, P243, DOI 10.1016/j.aeue.2008.10.004
   Mallat S., 1998, WAVELET TOUR SIGNAL
   Meenakshi K, 2014, 2014 INTERNATIONAL CONFERENCE ON INFORMATION TECHNOLOGY (ICIT), P167, DOI 10.1109/ICIT.2014.53
   Mohashin AHM, 2019, INT CONF KNOWL SMART, P78, DOI [10.1109/KST.2019.8687512, 10.1109/kst.2019.8687512]
   Moosazadeh M, 2017, OPTIK, V140, P975, DOI 10.1016/j.ijleo.2017.05.011
   NHA PT, 2021, 2021 RIVF INT C COMP, P1
   Paar C., 2009, UNDERSTANDING CRYPTO
   Soleymani SH, 2019, MULTIMED TOOLS APPL, V78, P19163, DOI 10.1007/s11042-019-7282-4
   Su QT, 2022, INT J INTELL SYST, V37, P7548, DOI 10.1002/int.22893
   Su QT, 2022, INT J INTELL SYST, V37, P4747, DOI 10.1002/int.22738
   Su QT, 2018, MULTIDIM SYST SIGN P, V29, P1055, DOI 10.1007/s11045-017-0487-7
   Su QT, 2017, MULTIMED TOOLS APPL, V76, P24221, DOI 10.1007/s11042-016-4164-x
   Su QT, 2017, AEU-INT J ELECTRON C, V78, P64, DOI 10.1016/j.aeue.2017.05.025
   Su QT, 2017, MULTIMED TOOLS APPL, V76, P707, DOI 10.1007/s11042-015-3071-x
   Su QT, 2016, IET IMAGE PROCESS, V10, P817, DOI 10.1049/iet-ipr.2016.0048
   Su QT, 2014, SIGNAL PROCESS, V94, P219, DOI 10.1016/j.sigpro.2013.06.025
   Sweldens W, 1998, SIAM J MATH ANAL, V29, P511, DOI 10.1137/S0036141095289051
   Tang M, 2022, ARRAY-NY, V15, DOI 10.1016/j.array.2022.100230
   Torr PHS, 2000, COMPUT VIS IMAGE UND, V78, P138, DOI 10.1006/cviu.1999.0832
   Vaidya SP, 2023, VISUAL COMPUT, V39, P2245, DOI 10.1007/s00371-022-02406-4
   Wan WB, 2022, NEUROCOMPUTING, V488, P226, DOI 10.1016/j.neucom.2022.02.083
   Wang HY, 2023, OPTIK, V274, DOI 10.1016/j.ijleo.2023.170585
   Yuan ZH, 2021, VISUAL COMPUT, V37, P1867, DOI 10.1007/s00371-020-01945-y
   Zear A, 2022, MULTIMED TOOLS APPL, V81, P26721, DOI 10.1007/s11042-020-10472-w
   Zhang H, 2022, J FRANKLIN I, V359, P1755, DOI 10.1016/j.jfranklin.2021.11.027
   Zhang XT, 2020, OPTIK, V219, DOI 10.1016/j.ijleo.2020.165272
NR 55
TC 1
Z9 1
U1 3
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2024
VL 40
IS 5
BP 3303
EP 3330
DI 10.1007/s00371-023-03033-3
EA JUL 2023
PG 28
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OC3D9
UT WOS:001040141100001
DA 2024-07-18
ER

PT J
AU Yao, DZ
   Shao, YX
AF Yao, Dazhi
   Shao, Yunxue
TI A data efficient transformer based on Swin Transformer
SO VISUAL COMPUTER
LA English
DT Article
DE Computer vision; Transformer; Classification; Data efficient
AB Almost all Vision Transformer-based models need to pre-train on the massive datasets and costly computation. Suppose researchers do not have enough data to train a Vision Transformer-based model or do not have powerful GPUs to implement computation for millions of labeled data. In that case, Vision Transformer-based models have no advantages over CNNs. Swin Transformer is brought forward to figure out these problems by applying the shifted window-based self-attention, which has linear computational complexity. Although Swin Transformer significantly reduces computing costs and works well on mid-size datasets, it still performs not well when it trains on a small-size dataset. In this paper, we propose a hierarchical and data-efficient Transformer based on Swin Transformer, which we call ESwin Transformer. We mainly redesigned the patch embedding module and patch merging module of Swin Transformer. We merely applied some unsophisticated convolutional components to these modules, which significantly improved performance when we trained our model on a small dataset. Our empirical results show that ESwin Transformer trained on CIFAR10/CIFAR100 with no extra data for 300 epochs achieves 97.17%/83.78% accuracy and performs better than Swin Transformer and DeiT in the same training time.
C1 [Yao, Dazhi; Shao, Yunxue] Nanjing Tech Univ, Sch Comp Sci & Technol, Nanjing, Peoples R China.
C3 Nanjing Tech University
RP Shao, YX (corresponding author), Nanjing Tech Univ, Sch Comp Sci & Technol, Nanjing, Peoples R China.
EM csydz@njtech.edu.cn; csshyx@njtech.edu.cn
OI Yao, Dazhi/0000-0002-7434-8099
CR Ba J. L., 2016, LAYER NORMALIZATION, DOI DOI 10.48550/ARXIV.1607.06450
   Beltagy Iz., 2020, ARXIV
   Cubuk ED, 2020, IEEE COMPUT SOC CONF, P3008, DOI 10.1109/CVPRW50498.2020.00359
   Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dosovitskiy Alexey, 2021, ICLR
   Han K., 2021, Adv. Neural Inf. Process. Syst., V34, P15908, DOI DOI 10.48550/ARXIV.2103.00112
   Hassani A., 2021, ARXIV
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hendrycks D., 2016, ARXIV
   Heo B, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11916, DOI 10.1109/ICCV48922.2021.01172
   Howard A.G., 2017, MOBILENETS EFFICIENT, DOI DOI 10.48550/ARXIV.1704.04861
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Huang G, 2016, LECT NOTES COMPUT SC, V9908, P646, DOI 10.1007/978-3-319-46493-0_39
   Ioffe S, 2015, P INT C MACH LEARN, V2015, P1
   Kingma D. P., 2014, arXiv
   Krizhevsky A., 2009, LEARNING MULTIPLE LA, DOI DOI 10.1145/3065386
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Le Ya, 2015, CS 231N, P3
   LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Nair V., 2010, P 27 INT C MACHINE L, P807
   Paszke A, 2019, ADV NEUR IN, V32
   Rao YM, 2021, 35 C NEURAL INFORM P, V34
   Sermanet P., 2013, ARXIV
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sun C, 2017, IEEE I CONF COMP VIS, P843, DOI 10.1109/ICCV.2017.97
   Szegedy C, 2014, Arxiv, DOI [arXiv:1312.6199, DOI 10.1109/CVPR.2015.7298594]
   Tan MX, 2019, PR MACH LEARN RES, V97
   Tolstikhin I.O., 2021, Advances in neural information processing systems, V34, P24261, DOI DOI 10.48550/ARXIV.2105.01601
   Touvron H, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P32, DOI 10.1109/ICCV48922.2021.00010
   Touvron H, 2021, PR MACH LEARN RES, V139, P7358
   Vaswani Ashish, 2017, Advances in Neural Information Processing Systems (NeurIPS), V17, P6000, DOI DOI 10.48550/ARXIV.1706.03762
   Wang WH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P548, DOI 10.1109/ICCV48922.2021.00061
   Wu H., 2021, ARXIV
   Xiao T., 2021, ARXIV
   Yuan K, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P559, DOI 10.1109/ICCV48922.2021.00062
   Yuan L, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P538, DOI 10.1109/ICCV48922.2021.00060
   Yun S, 2019, IEEE I CONF COMP VIS, P6022, DOI 10.1109/ICCV.2019.00612
   Zhang H, 2019, IEEE T PATTERN ANAL, V41, P1947, DOI 10.1109/TPAMI.2018.2856256
   Zhang QL, 2021, ADV NEUR IN, V34
   Zhang Z., 2021, arXiv
   Zhong Z, 2020, AAAI CONF ARTIF INTE, V34, P13001
NR 45
TC 3
Z9 3
U1 44
U2 85
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2024
VL 40
IS 4
BP 2589
EP 2598
DI 10.1007/s00371-023-02939-2
EA JUL 2023
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA MZ2U3
UT WOS:001040330500001
DA 2024-07-18
ER

PT J
AU Rashid, MM
   Wu, SH
   Nie, YW
   Li, GQ
AF Rashid, Muhammad Mamunur
   Wu, Shihao
   Nie, Yongwei
   Li, Guiqing
TI High-fidelity facial expression transfer using part-based local-global
   conditional gans
SO VISUAL COMPUTER
LA English
DT Article
DE Facial expression transfer; Local-global conditional GANs; Face
   synthesis; Face reenactment
ID FRAMEWORK
AB We propose a GAN-based facial expression transfer method. It can transfer the facial expression of a reference subject to the source subject while preserving the source identity attributes, such as shape, appearance, and illumination. Our method consists of two modules based on GAN: Parts Generation Networks (PGNs), and Parts Fusion Network (PFN). Instead of training the model on the entire image globally, our key idea is to train different PGNs for different local facial parts independently and then fuse the generated parts together using PFN. To encode the facial expression faithfully, we use a pre-trained parametric 3D head model (called photometric FLAME) to reconstruct realistic head models from both source and reference images. We also extract 3D facial feature points of the reference image to handle extreme poses and occlusions. Based on the extracted contextual information, we use PGNs to generate different parts of the head independently. Finally, PFN is used to fuse all the generated parts together to form the final image. Experiments show that the proposed model outperforms state-of-the-art approaches in faithfully transferring facial expressions, especially when the reference image has a different head pose to the source image. Ablation studies demonstrate the power of using PGNs.
C1 [Rashid, Muhammad Mamunur; Nie, Yongwei; Li, Guiqing] South China Univ Technol, Sch Comp Sci & Engn, Guangzhou, Peoples R China.
   [Wu, Shihao] Capskin AG, Zurich, Switzerland.
C3 South China University of Technology
RP Li, GQ (corresponding author), South China Univ Technol, Sch Comp Sci & Engn, Guangzhou, Peoples R China.
EM ligq@scut.edu.cn
OI Rashid, Muhammad Mamunur/0000-0002-5600-4874
FU NSFC [61972160, 62072191]; NSF of Guangdong Province [2021A1515012301]
FX AcknowledgementsThis research is partially funded by NSFC (61972160,
   62072191), and NSF of Guangdong Province (2021A1515012301).
CR Abdal R., 2020, P IEEECVF C COMPUTER, P8296
   Abdal R, 2019, IEEE I CONF COMP VIS, P4431, DOI 10.1109/ICCV.2019.00453
   Blanz V, 1999, COMP GRAPH, P187, DOI 10.1145/311535.311556
   Bounareli S., 2022, ARXIV
   Chang Jia-Ren, 2021, P IEEE CVF INT C COM, P9680
   Chen AP, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3470848
   Deng JK, 2019, PROC CVPR IEEE, P4685, DOI 10.1109/CVPR.2019.00482
   Doukas Michail Christos, 2021, IEEE Transactions on Biometrics, Behavior, and Identity Science, V3, P31, DOI 10.1109/TBIOM.2021.3049576
   Doukas MC, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14378, DOI 10.1109/ICCV48922.2021.01413
   Feng H., PHOTOMETRIC FLAME FI
   Feng Y, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459936
   Ha S, 2020, AAAI CONF ARTIF INTE, V34, P10893
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hensel M, 2017, ADV NEUR IN, V30
   Hong Y, 2022, PROC CVPR IEEE, P20342, DOI 10.1109/CVPR52688.2022.01973
   Hsu GS, 2022, PROC CVPR IEEE, P632, DOI 10.1109/CVPR52688.2022.00072
   Hsu GSJ, 2021, IEEE IMAGE PROC, P2458, DOI 10.1109/ICIP42928.2021.9506315
   Hyeongwoo Kim, 2018, ACM Transactions on Graphics, V37, DOI [10.1145/3197517.3201283, 10.18022/acfco.2018.37.1.001]
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jiang JF, 2021, GRAPH MODELS, V115, DOI 10.1016/j.gmod.2021.101107
   Jourabloo A., 2022, P IEEE CVF C COMP VI, P20323
   Kafri O, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3527168
   Karras T, 2021, IEEE T PATTERN ANAL, V43, P4217, DOI 10.1109/TPAMI.2020.2970919
   Lee CH, 2020, PROC CVPR IEEE, P5548, DOI 10.1109/CVPR42600.2020.00559
   Liang BR, 2022, PROC CVPR IEEE, P3377, DOI 10.1109/CVPR52688.2022.00338
   Lin YM, 2021, IMAGE VISION COMPUT, V112, DOI 10.1016/j.imavis.2021.104190
   Liu BY, 2021, 2021 IFIP NETWORKING CONFERENCE AND WORKSHOPS (IFIP NETWORKING), DOI 10.23919/IFIPNETWORKING52078.2021.9472798
   Liu Z., 2022, ARXIV
   Loper M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818013
   Ma TZ, 2020, IEEE T VIS COMPUT GR, V26, P3163, DOI 10.1109/TVCG.2019.2923196
   Nie YW, 2018, IEEE T IMAGE PROCESS, V27, P164, DOI 10.1109/TIP.2017.2736603
   Nie YW, 2014, IEEE T VIS COMPUT GR, V20, P1303, DOI 10.1109/TVCG.2013.2297931
   Nirkin Y, 2019, IEEE I CONF COMP VIS, P7183, DOI 10.1109/ICCV.2019.00728
   Peng B, 2022, IEEE T CIRC SYST VID, V32, P3673, DOI 10.1109/TCSVT.2021.3106047
   Shu C., 2022, P IEEECVF C COMPUTER, P10789
   Siarohin A, 2019, ADV NEUR IN, V32
   Thies J, 2016, PROC CVPR IEEE, P2387, DOI 10.1109/CVPR.2016.262
   Tianye Li, 2017, ACM Transactions on Graphics, V36, DOI 10.1145/3130800.3130813
   Wang QL, 2021, INT CONF 3D VISION, P679, DOI 10.1109/3DV53792.2021.00077
   Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917
   WANG X, 2021, P 16 IEEE INT C AUT, P1
   Wu RL, 2020, PROC CVPR IEEE, P5020, DOI 10.1109/CVPR42600.2020.00507
   Xu Chao, 2022, P IEEECVF C COMPUTER, P7632
   Yao G., 2021, ARXIV
   Yao GM, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1773, DOI 10.1145/3394171.3413865
   Yashima, 2022, ARXIV
   Zakharov Egor, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P524, DOI 10.1007/978-3-030-58610-2_31
   Zakharov E, 2019, IEEE I CONF COMP VIS, P9458, DOI 10.1109/ICCV.2019.00955
   Zeno B, 2020, PATTERN RECOGN LETT, V138, P527, DOI 10.1016/j.patrec.2020.08.026
   Zhang JN, 2020, PROC CVPR IEEE, P5325, DOI 10.1109/CVPR42600.2020.00537
   Zhu PH, 2020, PROC CVPR IEEE, P5103, DOI 10.1109/CVPR42600.2020.00515
NR 51
TC 0
Z9 0
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2023
VL 39
IS 8
SI SI
BP 3635
EP 3646
DI 10.1007/s00371-023-03035-1
EA JUL 2023
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA P2DS6
UT WOS:001037161700002
DA 2024-07-18
ER

PT J
AU Dou, Y
   Liu, XL
   Zhou, M
   Wang, JJ
AF Dou, Yi
   Liu, Xinling
   Zhou, Min
   Wang, Jianjun
TI Robust principal component analysis via weighted nuclear norm with
   modified second-order total variation regularization
SO VISUAL COMPUTER
LA English
DT Article
DE Low-rankness; Local smoothness; Weighted nuclear norm; Modified
   second-order total variation; WMSTV-RPCA; ADMM
ID MATRIX COMPLETION; SPARSE; MINIMIZATION; ALGORITHM; NONCONVEX; RECOVERY;
   NOISE
AB The traditional robust principal component analysis (RPCA) model aims to decompose the original matrix into low-rank and sparse components and uses the nuclear norm to describe the low-rank prior information of the natural image. In addition to low-rankness, it has been found in many recent studies that local smoothness is also crucial prior in low-level vision. In this paper, we propose a new RPCA model based on weight nuclear norm and modified second-order total variation regularization (WMSTV-RPCA for short), which exploits both the global low-rankness and local smoothness of the matrix. Extensive experimental results show, both qualitatively and quantitatively, that the proposed WMSTV-RPCA can more effectively remove noise, and model dynamic scenes compared with the competing methods.
C1 [Dou, Yi] Southwest Univ, Coll Comp & Informat Sci, Chongqing, Peoples R China.
   [Liu, Xinling; Wang, Jianjun] Southwest Univ, Sch Math & Stat, Chongqing, Peoples R China.
   [Zhou, Min] Southwest Univ, Informat Construct Off, Chongqing, Peoples R China.
C3 Southwest University - China; Southwest University - China; Southwest
   University - China
RP Wang, JJ (corresponding author), Southwest Univ, Sch Math & Stat, Chongqing, Peoples R China.
EM wjj@swu.edu.cn
CR Abdi H, 2010, WIRES COMPUT STAT, V2, P433, DOI 10.1002/wics.101
   Benner P, 2009, J COMPUT APPL MATH, V233, P1035, DOI 10.1016/j.cam.2009.08.108
   Boyd S., 2011, FOUND TRENDS MACH LE, V3, P1, DOI DOI 10.1561/2200000016
   Cai JF, 2010, SIAM J OPTIMIZ, V20, P1956, DOI 10.1137/080738970
   Candès EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395
   Candès EJ, 2008, J FOURIER ANAL APPL, V14, P877, DOI 10.1007/s00041-008-9045-x
   Chambolle A, 2004, J MATH IMAGING VIS, V20, P89
   Chambolle A, 2010, Theor Found Numer Methods Sparse Recovery, V9, DOI DOI 10.1515/9783110226157
   Chen W, 2018, IEEE T SIGNAL PROCES, V66, P5313, DOI 10.1109/TSP.2018.2867995
   Croux C, 2000, BIOMETRIKA, V87, P603, DOI 10.1093/biomet/87.3.603
   Diwakar M, 2018, BIOMED SIGNAL PROCES, V42, P73, DOI 10.1016/j.bspc.2018.01.010
   Dong WS, 2013, IEEE T IMAGE PROCESS, V22, P1618, DOI 10.1109/TIP.2012.2235847
   DONOHO DL, 1995, IEEE T INFORM THEORY, V41, P613, DOI 10.1109/18.382009
   Gu SH, 2017, INT J COMPUT VISION, V121, P183, DOI 10.1007/s11263-016-0930-5
   Gu SH, 2014, PROC CVPR IEEE, P2862, DOI 10.1109/CVPR.2014.366
   Han X, 2014, ABSTR APPL ANAL, DOI 10.1155/2014/765782
   Hong MY, 2016, SIAM J OPTIMIZ, V26, P337, DOI 10.1137/140990309
   Hu Y, 2013, IEEE T PATTERN ANAL, V35, P2117, DOI 10.1109/TPAMI.2012.271
   Li G., 2020, IEEE Trans. Knowl. Data Eng., V1, P394
   Li LY, 2004, IEEE T IMAGE PROCESS, V13, P1459, DOI 10.1109/TIP.2004.836169
   Lin, 2010, ARXIV10095055
   Lu CY, 2020, IEEE T PATTERN ANAL, V42, P925, DOI 10.1109/TPAMI.2019.2891760
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Meng DY, 2013, IEEE I CONF COMP VIS, P1337, DOI 10.1109/ICCV.2013.169
   Peng JJ, 2023, IEEE T PATTERN ANAL, V45, P5766, DOI 10.1109/TPAMI.2022.3204203
   Recht B, 2010, SIAM REV, V52, P471, DOI 10.1137/070697835
   RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F
   Sun J, 2008, PROC CVPR IEEE, P2471, DOI 10.1109/CVPR.2008.4587659
   Tian DY, 2016, IEEE T IMAGE PROCESS, V25, P961, DOI 10.1109/TIP.2015.2509418
   Wang NY, 2012, LECT NOTES COMPUT SC, V7578, P126, DOI 10.1007/978-3-642-33786-4_10
   Wang WD, 2021, APPL COMPUT HARMON A, V54, P1, DOI 10.1016/j.acha.2021.03.001
   Wang WD, 2018, DISCRETE DYN NAT SOC, V2018, DOI 10.1155/2018/2598160
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wen F, 2020, IEEE T CIRC SYST VID, V30, P1497, DOI 10.1109/TCSVT.2019.2908833
   Wright J, 2009, ADV NEURAL INFORM PR, P2080, DOI DOI 10.1109/NNSP.2000.889420
   Xie Y, 2016, IEEE T IMAGE PROCESS, V25, P4842, DOI 10.1109/TIP.2016.2599290
   Zhang DB, 2012, PROC CVPR IEEE, P2192, DOI 10.1109/CVPR.2012.6247927
   Zhang F, 2021, IEEE T PATTERN ANAL, V43, P3492, DOI 10.1109/TPAMI.2020.2986773
   Zheng YQ, 2012, PROC CVPR IEEE, P1410, DOI 10.1109/CVPR.2012.6247828
NR 39
TC 0
Z9 0
U1 6
U2 10
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2023
VL 39
IS 8
SI SI
BP 3495
EP 3505
DI 10.1007/s00371-023-02960-5
EA JUL 2023
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA P2DS6
UT WOS:001029107600002
OA Bronze
DA 2024-07-18
ER

PT J
AU Jahani, S
   Shoaran, M
   Khosroshahi, GK
AF Jahani, Sima
   Shoaran, Maryam
   Khosroshahi, Ghader Karimian
TI AQPnP: an accurate and quaternion-based solution for the
   Perspective-n-Point problem
SO VISUAL COMPUTER
LA English
DT Article
DE Camera pose estimation; PnP problem; Quaternion; Uncertainty; Augmented
   reality
ID POSE ESTIMATION; CAMERA
AB In this work, we present an accurate and quaternion-based solution for the Perspective-n-Point method for determining the position and orientation of a calibrated camera from a set of n 3D points and their corresponding 2D projections on the image plane referred to as AQPnP. Most of the previous PnP methods have not handled the uncertainty of image feature points directly. In the proposed method, uncertainty of image points is integrated into PnP method. This is achieved by mapping the covariance (uncertainty) matrices of the image points to PnP method. It then uses the alternating direction method of multipliers (ADMM) method to minimize the modeled cost function, which has the ability to find the desired solution in real time. The ADMM method solves the system of equations in a manner that is convergent to the global optimal. The ADMM, a widely used operator splitting technique, has drawn significant attention due to its excellent performance in various applications. Quaternion-based rotation parameterization allows AQPnP to be highly accurate in estimating the rotation matrix. We compared AQPnP method with the state-of-the-art PnP methods, and pose estimation results show that our proposed method outperforms better than other methods.
C1 [Jahani, Sima; Shoaran, Maryam; Khosroshahi, Ghader Karimian] Univ Tabriz, Fac Elect & Comp Engn, Tabriz 5166616471, Iran.
C3 University of Tabriz
RP Khosroshahi, GK (corresponding author), Univ Tabriz, Fac Elect & Comp Engn, Tabriz 5166616471, Iran.
EM sjahani@tabrizu.ac.ir; mshoaran@tabrizu.ac.ir; karimian@tabrizu.ac.ir
RI Karimian, Ghader/IYJ-4021-2023
OI Karimian, Ghader/0000-0001-5030-6970; Shoaran,
   Maryam/0000-0002-4105-0835
CR Banjac G, 2019, J OPTIMIZ THEORY APP, V183, P490, DOI 10.1007/s10957-019-01575-y
   Boyd Stephen, 2010, Foundations and Trends in Machine Learning, V3, P1, DOI 10.1561/2200000016
   Chen H., 2022, IEEE C COMP VIS PATT
   Adli SE, 2020, VISUAL COMPUT, V36, P1549, DOI 10.1007/s00371-019-01747-x
   Ferraz L., 2014, Proceedings of the 28th EnviroInfo 2014 Conference, DOI DOI 10.5244/C.28.83
   Ferraz L, 2014, PROC CVPR IEEE, P501, DOI 10.1109/CVPR.2014.71
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Garro V, 2012, SECOND JOINT 3DIM/3DPVT CONFERENCE: 3D IMAGING, MODELING, PROCESSING, VISUALIZATION & TRANSMISSION (3DIMPVT 2012), P262, DOI 10.1109/3DIMPVT.2012.40
   Hesch JA, 2011, IEEE I CONF COMP VIS, P383, DOI 10.1109/ICCV.2011.6126266
   Jiachen Shi, 2020, 2020 IEEE 5th International Conference on Signal and Image Processing (ICSIP), P407, DOI 10.1109/ICSIP49896.2020.9339274
   Lepetit V, 2009, INT J COMPUT VISION, V81, P155, DOI 10.1007/s11263-008-0152-6
   Li SQ, 2012, IEEE T PATTERN ANAL, V34, P1444, DOI 10.1109/TPAMI.2012.41
   Lu CP, 2000, IEEE T PATTERN ANAL, V22, P610, DOI 10.1109/34.862199
   Pitchandi N, 2019, J INTELL ROBOT SYST, V96, P65, DOI 10.1007/s10846-019-00985-4
   Schmidt J., 2001, Vision, Modeling, and Visualization 2001. Proceedings, P399
   Stellato B, 2020, MATH PROGRAM COMPUT, V12, P637, DOI 10.1007/s12532-020-00179-2
   Terzakis G., 2014, J ENG SCI TECHNOL RE, V6, P15, DOI [10.25103/jestr.065.03, DOI 10.25103/JESTR.065.03]
   Urban S, 2016, ISPRS ANN PHOTO REM, V3, P131, DOI 10.5194/isprsannals-III-3-131-2016
   Vakhitov A, 2021, PROC CVPR IEEE, P4657, DOI 10.1109/CVPR46437.2021.00463
   Zheng YQ, 2013, IEEE I CONF COMP VIS, P2344, DOI 10.1109/ICCV.2013.291
   Zheng YQ, 2013, IEICE T INF SYST, VE96D, P1525, DOI 10.1587/transinf.E96.D.1525
   Zhou BY, 2020, IEEE ACCESS, V8, P162838, DOI 10.1109/ACCESS.2020.3021313
NR 22
TC 0
Z9 0
U1 3
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2024
VL 40
IS 4
BP 2631
EP 2639
DI 10.1007/s00371-023-02952-5
EA JUL 2023
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA MZ2U3
UT WOS:001026397800008
DA 2024-07-18
ER

PT J
AU Zeng, S
   Yang, WZ
   Jiao, YY
   Geng, L
   Chen, XT
AF Zeng, Shuang
   Yang, Wenzhu
   Jiao, Yanyan
   Geng, Lei
   Chen, Xinting
TI SCA-YOLO: a new small object detection model for UAV images
SO VISUAL COMPUTER
LA English
DT Article
DE Small object detection; UAV images; Spatial and coordinate attention
   enhancement; Feature fusion
ID REMOTE-SENSING IMAGES; ENSEMBLE
AB Object detection from UAV (unmanned aerial vehicle) images is a crucial and challenging task in the field of computer vision. The task suffers from the difficulties of small dense objects, low pixel occupation of objects, and features that are not easily extracted in images. In this paper, we proposed a multilayer feature fusion algorithm named SCA-YOLO (spatial and coordinate attention enhancement YOLO) for small object detection with hybrid attention mechanisms. It uses the single-stage detection algorithm YOLOv5 as the base framework. Firstly, a hybrid attention module with associated coordinate attention is designed to enhance the feature extraction of small objects. Secondly, to address the problem that small objects are vulnerable to being disturbed by the complex background information on UAV images, an improved SEB (simple and efficient bottleneck) module is designed to further distinguish foreground and background features. Thirdly, a multilayer feature fusion structure is built to perform channel stitching of shallow and deep feature maps, as well as to enrich the semantic information of shallow features by adding horizontal jump connections. Finally, experiments are conducted on the VisDrone2020 dataset, which involves a large number of small objects photographed by drones. In addition, we also conduct extended experiments on the DOTA dataset and PASCAL VOC dataset. Comparative experimental results indicate that the proposed method considerably improves the accuracy of small object detection on multiple benchmark datasets.
C1 [Zeng, Shuang; Yang, Wenzhu; Jiao, Yanyan; Geng, Lei; Chen, Xinting] Hebei Univ, Sch Cyber Secur & Comp, Baoding 071002, Peoples R China.
   [Yang, Wenzhu] Hebei Univ, Machine Vis Engn Res Ctr, Baoding 071002, Peoples R China.
C3 Hebei University; Hebei University
RP Yang, WZ (corresponding author), Hebei Univ, Sch Cyber Secur & Comp, Baoding 071002, Peoples R China.; Yang, WZ (corresponding author), Hebei Univ, Machine Vis Engn Res Ctr, Baoding 071002, Peoples R China.
EM wenzhuyang@hbu.edu.cn
RI geng, lei/KEZ-8801-2024
FU Hebei Province important research project [22370301D]; Postgraduate
   Research and Innovation Project of Hebei Province [HBU2023SS009]
FX The authors thank the editors and reviewers for their work on this
   manuscript. The authors also thank the Hebei Province important research
   project (22370301D) and the Postgraduate Research and Innovation Project
   of Hebei Province (HBU2023SS009) for their financial support and the
   support of the High-Performance Computing Center of Hebei University.
CR Albaba BM, 2021, INT C PATT RECOG, P10227, DOI 10.1109/ICPR48806.2021.9412847
   ALI S, 2021, 2021 INNOVATIONS POW, P1, DOI DOI 10.1109/I-PACT52855.2021.9696617
   An FP, 2022, VISUAL COMPUT, V38, P541, DOI 10.1007/s00371-020-02033-x
   Bayoudh K, 2022, VISUAL COMPUT, V38, P2939, DOI 10.1007/s00371-021-02166-7
   Bochkovskiy A, 2020, ARXIV PREPRINT ARXIV, DOI DOI 10.48550/ARXIV.2004.10934
   Cao YR, 2021, IEEE INT CONF COMP V, P2847, DOI 10.1109/ICCVW54120.2021.00319
   Carion N., 2020, EUR C COMP VIS, P213, DOI DOI 10.1007/978-3-030-58452-8_13
   Chandana R., 2022, REAL TIME OBJECT DET
   Cui Y. F., 2022, ARXIV
   Cui YM, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P8118, DOI 10.1109/ICCV48922.2021.00803
   Dai J., 2021, ICLR
   Du DW, 2019, IEEE INT CONF COMP V, P213, DOI 10.1109/ICCVW.2019.00030
   Fu C.-Y., 2017, DSSD: Deconvolutional Single Shot Detector
   Ghiasi G, 2019, PROC CVPR IEEE, P7029, DOI 10.1109/CVPR.2019.00720
   Gromada K, 2022, SENSORS-BASEL, V22, DOI 10.3390/s22052068
   He KM, 2020, IEEE T PATTERN ANAL, V42, P386, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hou QB, 2021, PROC CVPR IEEE, P13708, DOI 10.1109/CVPR46437.2021.01350
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Jiang B, 2022, J CIRCUIT SYST COMP, V31, DOI 10.1142/S021812662250147X
   Joseph RK, 2016, CRIT POL ECON S ASIA, P1
   Liu DF, 2021, PROC CVPR IEEE, P9811, DOI 10.1109/CVPR46437.2021.00969
   Liu G, 2021, IMAGE VISION COMPUT, V111, DOI 10.1016/j.imavis.2021.104197
   Liu JY, 2022, IEEE T CIRC SYST VID, V32, P105, DOI 10.1109/TCSVT.2021.3056725
   Liu S, 2018, PROC CVPR IEEE, P8759, DOI 10.1109/CVPR.2018.00913
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Mao G.T., 2022, J AERONAUT, V43, P326738
   Mozaffari M. H., 2022, P CAN C ART INT, DOI [10.21428/594757db.7c1cd4-1, DOI 10.21428/594757DB.7C1CD4-1]
   Qi GQ, 2022, REMOTE SENS-BASEL, V14, DOI 10.3390/rs14020420
   Qu Z., 2022, IET IMAGE PROCESS, V16
   Redmon J, 2018, Arxiv, DOI [arXiv:1804.02767, DOI 10.1109/CVPR.2017.690, DOI 10.48550/ARXIV.1804.02767]
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Shi LK, 2021, INT J REMOTE SENS, V42, P4241, DOI 10.1080/01431161.2021.1892858
   Sun Peize, 2021, CVPR
   Tan M., 2020, P IEEECVF C COMPUTER, P10781, DOI [10.48550/arXiv.1911.09070, DOI 10.1109/CVPR42600.2020.01079]
   Tsung-Yi Lin, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P2999, DOI 10.1109/ICCV.2017.324
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang CY, 2020, IEEE COMPUT SOC CONF, P1571, DOI 10.1109/CVPRW50498.2020.00203
   Wang K, 2022, INT J REMOTE SENS, V43, P1323, DOI 10.1080/01431161.2022.2038396
   Wang Q., 2021, SMALL OBJECT DETECTI
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634
   Yan L, 2022, ARXIV
   Yang H, 2023, VISUAL COMPUT, V39, P6711, DOI 10.1007/s00371-022-02758-x
   Yang LX, 2021, PR MACH LEARN RES, V139
   Yang X, 2018, REMOTE SENS-BASEL, V10, DOI 10.3390/rs10010132
   Yin QJ, 2021, SIGNAL PROCESS-IMAGE, V98, DOI 10.1016/j.image.2021.116402
   Yu WP, 2021, IEEE WINT CONF APPL, P3257, DOI 10.1109/WACV48630.2021.00330
   Zhang LG, 2022, INT J REMOTE SENS, V43, P3280, DOI 10.1080/01431161.2022.2089539
   Zhang Qi, 2022, 2022 5th International Conference on Pattern Recognition and Artificial Intelligence (PRAI), P370, DOI 10.1109/PRAI55851.2022.9904251
   Zhang TS, 2023, VISUAL COMPUT, V39, P569, DOI 10.1007/s00371-021-02357-2
   Zhao HP, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20071861
   Zhao XL, 2021, INT J REMOTE SENS, V42, P5754, DOI 10.1080/01431161.2021.1931537
NR 55
TC 10
Z9 10
U1 35
U2 115
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2024
VL 40
IS 3
BP 1787
EP 1803
DI 10.1007/s00371-023-02886-y
EA MAY 2023
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HY8O3
UT WOS:000994739900003
DA 2024-07-18
ER

PT J
AU Zhang, PW
   Zhou, WB
   Fan, LY
AF Zhang, Pingwei
   Zhou, Wenbiao
   Fan, Luyao
TI Channel and spatial attention-guided network for deep high dynamic range
   imaging with large motions
SO VISUAL COMPUTER
LA English
DT Article
DE High dynamic range imaging; Convolutional neural network; Attention
   mechanism; Multi-exposure fusion; Ghosting artifacts
ID TONE REPRODUCTION; GHOST REMOVAL; IMAGES; RECONSTRUCTION
AB Multi-exposure fusion (MEF) is widely researched and applied to high dynamic range (HDR) imaging, where one of the most challenging problems is the artifacts caused by the motion of objects between input images. Recently, deep learning methods have been widely applied for HDR imaging with excellent results, showing significant advantages. However, many methods cannot avoid artifacts due to inaccurate alignment before merging HDR images. In this paper, we propose an end-to-end network (C-ED-GMNET) with a channel and spatial attention network, an encoder-decoder network and a gradual merging network for generating artifact-free HDR images in dynamic scenes. The attention module consists of two submodules to identify useful features and exclude harmful components in inputs from both channel and spatial dimensions, respectively. The attention-guided feature maps are sent to the encoders for further feature extraction, and then, the outputs are sent to a gradual merging module consisting of two steps to generate deep features progressively. Besides, the differences between the merged image and the original images are identified by applying global residual learning with all the inputs, and the merged image feature is recovered by the decoder to obtain the final HDR image. Quantitative and qualitative experiments on two public datasets show that the proposed C-ED-GMNET produces better results than existing state-of-the-art methods and significantly reduces the runtime due to the encoders which reduce the amount of computation.
C1 [Zhang, Pingwei; Zhou, Wenbiao; Fan, Luyao] Beijing Inst Technol, Sch Integrated Circuits & Elect, Beijing 100081, Peoples R China.
C3 Beijing Institute of Technology
RP Zhou, WB (corresponding author), Beijing Inst Technol, Sch Integrated Circuits & Elect, Beijing 100081, Peoples R China.
EM 3120200726@bit.edu.cn; zhouwenbiao@bit.edu.cn; 3220210579@bit.edu.cn
CR Bogoni L, 2000, INT C PATT RECOG, P7, DOI 10.1109/ICPR.2000.903475
   Duan J, 2010, PATTERN RECOGN, V43, P1847, DOI 10.1016/j.patcog.2009.12.006
   Eilertsen G, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130816
   Endo Y, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130834
   Fan HQ, 2018, PROC CVPR IEEE, P1072, DOI 10.1109/CVPR.2018.00118
   Gallo O., 2009, P IEEE INT C COMP PH, P1
   Glorot X., 2010, INT C ARTIFICIAL INT, P249
   Hanji Param, 2022, SIGGRAPH22 Conference Proceeding: Special Interest Group on Computer Graphics and Interactive Techniques Conference Proceedings, DOI 10.1145/3528233.3530729
   Heide F, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2661229.2661260
   Heo YS, 2011, LECT NOTES COMPUT SC, V6495, P486, DOI 10.1007/978-3-642-19282-1_39
   Hu J, 2013, PROC CVPR IEEE, P1163, DOI 10.1109/CVPR.2013.154
   Jacobs K, 2008, IEEE COMPUT GRAPH, V28, P84, DOI 10.1109/MCG.2008.23
   Kalantari N. K., 2017, ACM Trans. Graph., V36, DOI DOI 10.1145/3072959.3073609
   Kang SB, 2003, ACM T GRAPHIC, V22, P319, DOI 10.1145/882262.882270
   Khan EA, 2006, IEEE IMAGE PROC, P2005, DOI 10.1109/ICIP.2006.312892
   Khwildi R, 2020, VISUAL COMPUT, V36, P1111, DOI 10.1007/s00371-019-01719-1
   Kingma D. P., 2014, arXiv
   Kuo P.H., 2014, IEEE 16 INT WORKSH M, P1, DOI [10.1109/MMSP.2014.6958828, DOI 10.1109/MMSP.2014.6958828]
   Larson GW, 1997, IEEE T VIS COMPUT GR, V3, P291, DOI 10.1109/2945.646233
   Lu JS, 2017, PROC CVPR IEEE, P3242, DOI 10.1109/CVPR.2017.345
   MANTIUK R, 2011, ACM T GRAPHIC, V30, P1, DOI DOI 10.1145/2010324.1964935
   Marín-Vega J, 2022, IEEE COMPUT SOC CONF, P843, DOI 10.1109/CVPRW56347.2022.00100
   Masia B, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618506
   Mohammadi P, 2021, IEEE T CIRC SYST VID, V31, P1711, DOI 10.1109/TCSVT.2020.3014679
   Nayar SK, 2000, PROC CVPR IEEE, P472, DOI 10.1109/CVPR.2000.855857
   Niu YZ, 2021, IEEE T IMAGE PROCESS, V30, P3885, DOI 10.1109/TIP.2021.3064433
   Oh TH, 2015, IEEE T PATTERN ANAL, V37, P1219, DOI 10.1109/TPAMI.2014.2361338
   Pece F., 2010, Proceedings 2010 Conference on Visual Media Production (CVMP 2010). 7th European Conference on Visual Media Production, P1, DOI 10.1109/CVMP.2010.8
   Qiu G, 2007, PATTERN RECOGN, V40, P2641, DOI 10.1016/j.patcog.2007.02.012
   Raman S, 2011, VISUAL COMPUT, V27, P1099, DOI 10.1007/s00371-011-0653-0
   Reinhard E, 2002, ACM T GRAPHIC, V21, P267, DOI 10.1145/566570.566575
   Sen P, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366222
   Serrano A, 2016, COMPUT GRAPH FORUM, V35, P153, DOI 10.1111/cgf.12819
   Shen JB, 2007, VISUAL COMPUT, V23, P641, DOI 10.1007/s00371-007-0155-2
   Tumblin J, 2005, PROC CVPR IEEE, P103
   Vien AG, 2022, IEEE COMPUT SOC CONF, P1182, DOI 10.1109/CVPRW56347.2022.00125
   Wang TH, 2015, IEEE T MULTIMEDIA, V17, P470, DOI 10.1109/TMM.2015.2403612
   Woo S., 2018, P EUR C COMP VIS ECC, DOI DOI 10.1007/978-3-030-01234-2_1
   Wu GT, 2022, PATTERN RECOGN, V127, DOI 10.1016/j.patcog.2022.108620
   Wu SZ, 2018, LECT NOTES COMPUT SC, V11206, P120, DOI 10.1007/978-3-030-01216-8_8
   XIA S, 2022, ADV COMPOS HYBRID MA, DOI DOI 10.1007/S00371-022-02475-5
   Yan QS, 2022, PATTERN RECOGN, V122, DOI 10.1016/j.patcog.2021.108342
   Yan QS, 2021, NEUROCOMPUTING, V428, P79, DOI 10.1016/j.neucom.2020.11.056
   Yan QS, 2020, COMPUT VIS IMAGE UND, V201, DOI 10.1016/j.cviu.2020.103079
   Yan QS, 2020, IEEE T IMAGE PROCESS, V29, P4308, DOI 10.1109/TIP.2020.2971346
   Yan QS, 2019, PROC CVPR IEEE, P1751, DOI 10.1109/CVPR.2019.00185
   Yan QS, 2019, MULTIMED TOOLS APPL, V78, P11487, DOI 10.1007/s11042-018-6625-x
   Yan QS, 2019, IEEE WINT CONF APPL, P41, DOI 10.1109/WACV.2019.00012
   Yan QS, 2017, NEUROCOMPUTING, V269, P160, DOI 10.1016/j.neucom.2017.03.083
   Zhang W, 2012, IEEE T IMAGE PROCESS, V21, P2318, DOI 10.1109/TIP.2011.2170079
   Zhang YF, 2023, VISUAL COMPUT, V39, P413, DOI 10.1007/s00371-021-02338-5
   Zheng JH, 2013, IEEE T IMAGE PROCESS, V22, P5190, DOI 10.1109/TIP.2013.2283401
   Zimmer H, 2011, COMPUT GRAPH FORUM, V30, P405, DOI 10.1111/j.1467-8659.2011.01870.x
NR 53
TC 0
Z9 0
U1 3
U2 8
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2024
VL 40
IS 3
BP 1583
EP 1599
DI 10.1007/s00371-023-02871-5
EA APR 2023
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HY8O3
UT WOS:000979483600003
DA 2024-07-18
ER

PT J
AU Wang, FB
   Geng, SL
   Zhang, D
   Zhou, MQ
AF Wang, Fubo
   Geng, Shengling
   Zhang, Dan
   Zhou, Mingquan
TI Automatic colorization for Thangka sketch-based paintings
SO VISUAL COMPUTER
LA English
DT Article
DE Thangka colorization; Deep learning; Semantic matching; Auxiliary
   coloring; Color calibration
ID IMAGE
AB Thangka is a kind of ancient painting that originated from Tibetan culture. Considering its value in cultural and historical fields, digital protection and restoration of Thangka images are important. In this paper, we propose an automatic colorization framework for a Thangka sketch. First, we propose a network structure with a feedback generator to restore the true color of a Thangka painting based on semantic matching. Second, we design a color gradient algorithm for Thangka coloring. Finally, we construct an automatic coloring framework for Thangka paintings that supports user customization. The experimental results show that the algorithm has a highly accurate response to a user's selection process. Compared with traditional methods, the proposed method improves the image quality by 22.3% on average. The data for our approach are publicly available at https://github.com/wangfubo123/SMAC-CGAN.
C1 Qinghai Normal Univ, Sch Comp Sci, Xining 810008, Qinghai, Peoples R China.
   [Wang, Fubo; Geng, Shengling; Zhang, Dan; Zhou, Mingquan] Peoples Govt Qinghai Prov, Acad Plateau Sci & Sustainabil, Xining 810004, Qinghai, Peoples R China.
   [Wang, Fubo; Geng, Shengling; Zhang, Dan; Zhou, Mingquan] Beijing Normal Univ, Xining 810004, Qinghai, Peoples R China.
   [Wang, Fubo; Geng, Shengling; Zhang, Dan; Zhou, Mingquan] Qinghai Normal Univ, State Key Lab Tibetan Intelligent Informat Proc &, Xining 810008, Qinghai, Peoples R China.
C3 Qinghai Normal University; Beijing Normal University; Qinghai Normal
   University
RP Zhang, D (corresponding author), Peoples Govt Qinghai Prov, Acad Plateau Sci & Sustainabil, Xining 810004, Qinghai, Peoples R China.; Zhang, D (corresponding author), Beijing Normal Univ, Xining 810004, Qinghai, Peoples R China.; Zhang, D (corresponding author), Qinghai Normal Univ, State Key Lab Tibetan Intelligent Informat Proc &, Xining 810008, Qinghai, Peoples R China.
EM wfb18753852115@163.com; geng_sl@126.com; danz@mail.bnu.edu.cn;
   qhsf411@163.com
RI Zhang, Dan/AFA-2608-2022; ZHOU, MING/JVP-2920-2024
OI Zhang, Dan/0000-0002-7295-4837; Dan, Zhang/0000-0001-5676-0656
FU National Key RD plan [2020YFC1523305]; Natural Science Youth Foundation
   of Qinghai Province [2023-ZJ-947Q]; National Nature Science Foundation
   of China [62102213, 62262056]; Key R&D and transformation plan of
   Qinghai Province [2022-QY-203]; Independent project fund of State Key
   Lab of Tibetan Intelligent Information Processing and Application
   [2022-SKL-014]
FX Thisworkwas partially supported by the National Key R&D plan
   (No.2020YFC1523305); Natural Science Youth Foundation of Qinghai
   Province (No.2023-ZJ-947Q); National Nature Science Foundation ofChina
   (No. 62102213, No.62262056); Key R&D and transformation plan of Qinghai
   Province (No. 2022-QY-203); and Independent project fund of State Key
   Lab of Tibetan Intelligent Information Processing and Application (Co
   established by province and ministry) (Nos.2022-SKL-014).
CR Bhattacharyya D, 2023, VISUAL COMPUT, V39, P5245, DOI 10.1007/s00371-022-02657-1
   Brock A, 2019, Arxiv, DOI [arXiv:1809.11096, DOI 10.48550/ARXIV.1809.11096]
   Chen DD, 2017, PROC CVPR IEEE, P2770, DOI 10.1109/CVPR.2017.296
   Cho K., 2014, ARXIV14061078
   Ci YZ, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1536, DOI 10.1145/3240508.3240661
   Cohen N, 2022, COMPUT GRAPH FORUM, V41, P261, DOI 10.1111/cgf.14473
   Deshpande A, 2017, PROC CVPR IEEE, P2877, DOI 10.1109/CVPR.2017.307
   Furusawa C, 2017, Arxiv, DOI arXiv:1706.06759
   Gatys LA, 2016, PROC CVPR IEEE, P2414, DOI 10.1109/CVPR.2016.265
   Gulrajani I, 2017, Arxiv, DOI arXiv:1704.00028
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang X, 2018, LECT NOTES COMPUT SC, V11207, P179, DOI 10.1007/978-3-030-01219-9_11
   Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jheng-Wei Su, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7965, DOI 10.1109/CVPR42600.2020.00799
   Kera SB, 2023, VISUAL COMPUT, V39, P2347, DOI 10.1007/s00371-022-02445-x
   Kingma D. P., 2014, arXiv
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lee J, 2020, PROC CVPR IEEE, P5800, DOI 10.1109/CVPR42600.2020.00584
   Levin A, 2004, ACM T GRAPHIC, V23, P689, DOI 10.1145/1015706.1015780
   Li J., 2019, 2019 IEEE 4 INT C IM
   [李媛 Li Yuan], 2022, [计算机研究与发展, Journal of Computer Research and Development], V59, P1271
   Lin S, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461988
   Lin ZK, 2023, VISUAL COMPUT, V39, P597, DOI 10.1007/s00371-021-02360-7
   Liu YC, 2018, ISPRS J PHOTOGRAMM, V145, P78, DOI 10.1016/j.isprsjprs.2017.12.007
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Messaoud S, 2018, LECT NOTES COMPUT SC, V11210, P603, DOI 10.1007/978-3-030-01231-1_37
   Mirza M, 2014, Arxiv, DOI [arXiv:1411.1784, DOI 10.48550/ARXIV.1411.1784]
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Ruan T, 2019, AAAI CONF ARTIF INTE, P4814
   Sangkloy P, 2017, PROC CVPR IEEE, P6836, DOI 10.1109/CVPR.2017.723
   Seah HS, 2000, VISUAL COMPUT, V16, P289, DOI 10.1007/s003719900068
   Sun Q, 2022, VISUAL COMPUT, V38, P1283, DOI 10.1007/s00371-021-02219-x
   Wang P, 2021, PROC CVPR IEEE, P124, DOI 10.1109/CVPR46437.2021.00019
   Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917
   Xian WQ, 2018, PROC CVPR IEEE, P8456, DOI 10.1109/CVPR.2018.00882
   Yang BY, 2022, IEEE T NEUR NET LEAR, V33, P7141, DOI 10.1109/TNNLS.2021.3084252
   Yatziv L, 2006, IEEE T IMAGE PROCESS, V15, P1120, DOI 10.1109/TIP.2005.864231
   Yin C., 2021, IEEE Transactions on Multimedia, V2021, P4183
   Zhang LM, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275090
   Zheng Q., 2020, P IEEECVF C COMPUTER, P7436
   Zhou Q, 2022, IEEE T INTELL TRANSP, V23, P25259, DOI 10.1109/TITS.2022.3194213
   Zhou Q, 2022, PATTERN RECOGN, V122, DOI 10.1016/j.patcog.2021.108290
NR 43
TC 2
Z9 2
U1 7
U2 21
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2024
VL 40
IS 2
BP 761
EP 779
DI 10.1007/s00371-023-02814-0
EA MAR 2023
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GE6E8
UT WOS:000957075300001
DA 2024-07-18
ER

PT J
AU Maack, RGC
   Scheuermann, G
   Hagen, H
   Penaloza, JTH
   Gillmann, C
AF Maack, Robin G. C.
   Scheuermann, Gerik
   Hagen, Hans
   Penaloza, Jose Tiberio Hernandez
   Gillmann, Christina
TI Uncertainty-aware visual analytics: scope, opportunities, and challenges
SO VISUAL COMPUTER
LA English
DT Article
DE Visual analytics; Uncertainty analysis; Uncertainty-aware visualization
ID VISUALIZATION; APPROXIMATION; PROVENANCE; FRAMEWORK; ERROR; MODEL
AB In many applications, visual analytics (VA) has developed into a standard tool to ease data access and knowledge generation. VA describes a holistic cycle transforming data into hypothesis and visualization to generate insights that enhance the data. Unfortunately, many data sources used in the VA process are affected by uncertainty. In addition, the VA cycle itself can introduce uncertainty to the knowledge generation process but does not provide a mechanism to handle these sources of uncertainty. In this manuscript, we aim to provide an extended VA cycle that is capable of handling uncertainty by quantification, propagation, and visualization, defined as uncertainty-aware visual analytics (UAVA). Here, a recap of uncertainty definition and description is used as a starting point to insert novel components in the visual analytics cycle. These components assist in capturing uncertainty throughout the VA cycle. Further, different data types, hypothesis generation approaches, and uncertainty-aware visualization approaches are discussed that fit in the defined UAVA cycle. In addition, application scenarios that can be handled by such a cycle, examples, and a list of open challenges in the area of UAVA are provided.
C1 [Maack, Robin G. C.; Hagen, Hans] Univ Kaiserslautern, Comp G & HCI Grp, Erwin Schrodinger Str 52, D-67663 Kaiserslautern, Rhineland Palat, Germany.
   [Scheuermann, Gerik; Gillmann, Christina] Univ Leipzig, Image & Signal Proc Grp, Augustuspl10, D-04109 Leipzig, Saxony, Germany.
   [Penaloza, Jose Tiberio Hernandez] Univ Andes, IMAGINE Grp, Cra 1 18A 12, Bogota, Cundinamarca, Colombia.
C3 University of Kaiserslautern; Leipzig University; Universidad de los
   Andes (Colombia)
RP Maack, RGC (corresponding author), Univ Kaiserslautern, Comp G & HCI Grp, Erwin Schrodinger Str 52, D-67663 Kaiserslautern, Rhineland Palat, Germany.
EM maack@rhrk.uni-kl.de; scheuermann@informatik.uni-leipzig.de;
   hagen@cs.uni-kl.de; jhernand@uniandes.edu.co;
   gillmann@informatik.uni-leipzig.de
FU Federal Ministry of Education and Research of Germany; Saechsische
   Staatsministerium fuer Wissenschaft Kultur und Tourismus; program Center
   of Excellence for AI-research "Center for Scalable Data Analytics and
   Artificial Intelligence Dresden/Leipzig [ScaDS.AI]
FX The authors acknowledge the financial support by the Federal Ministry of
   Education and Research of Germany and by the Saechsische
   Staatsministerium fuer Wissenschaft Kultur und Tourismus in the program
   Center of Excellence for AI-research "Center for Scalable Data Analytics
   and Artificial Intelligence Dresden/Leipzig," Project Identification
   Number: ScaDS.AI.
CR Aggarwal CC, 2014, CH CRC DATA MIN KNOW, P1
   Andrienko N, 2018, COMPUT GRAPH FORUM, V37, P275, DOI 10.1111/cgf.13324
   Angelini M, 2015, 8TH INTERNATIONAL SYMPOSIUM ON VISUAL INFORMATION COMMUNICATION AND INTERACTION (VINCI 2015), P83
   [Anonymous], 2014, Overview and State-of-the-Art of Uncertainty Visualization
   Arriola L., 2009, Mathematical and Statistical Estimation Approaches in Epidemiology, P195, DOI [DOI 10.1007/978-90-481-2313-1_10, DOI 10.1007/978-90-481-2313-110]
   Bassil S, 2001, PROG COMPREHEN, P7, DOI 10.1109/WPC.2001.921708
   Beck, 2014, ISR ROB 2014 41 INT, P83, DOI DOI 10.2312/EUROVISSTAR.20141174
   Belforte G., 1987, Measurement, V5, P167, DOI 10.1016/0263-2241(87)90036-4
   Bhatt U, 2021, AIES '21: PROCEEDINGS OF THE 2021 AAAI/ACM CONFERENCE ON AI, ETHICS, AND SOCIETY, P401, DOI 10.1145/3461702.3462571
   Bors C., 2019, EUROVIS WORKSH VIS A
   Boumans M., 2015, ERROR UNCERTAINTY SC
   Boyat A.K., 2015, ARXIV
   Brodbeck D, 2009, LECT NOTES COMPUT SC, V5440, P27, DOI 10.1007/978-3-642-00437-7_2
   Brodlie K, 2012, Expanding the Frontiers of Visual Analytics and Visualization, P81, DOI [DOI 10.1007/978-1-4471-2804-5_6, 10.1007/978-1-4471-2804-5_6]
   Cai GW, 2018, ASCE-ASME J RISK U A, V4, DOI 10.1061/AJRUA6.0000949
   Cai SM, 2019, COMPUTING, V101, P1397, DOI 10.1007/s00607-018-0679-5
   Chen P, 2015, J COMPUT PHYS, V298, P176, DOI 10.1016/j.jcp.2015.06.006
   Cheng R, 2014, PROC INT CONF DATA, P1302, DOI 10.1109/ICDE.2014.6816766
   Correa Carlos D., 2009, Proceedings of the 2009 IEEE Symposium on Visual Analytics Science and Technology. VAST 2009. Held co-jointly with VisWeek 2009, P51, DOI 10.1109/VAST.2009.5332611
   Coutinho MVC, 2015, MEM COGNITION, V43, P990, DOI 10.3758/s13421-015-0527-1
   Cui WQ, 2019, IEEE ACCESS, V7, P81555, DOI 10.1109/ACCESS.2019.2923736
   D' Argens M., 2018, SOME REMARKS THEOLOG, V04
   Dasgupta A., 2010, WORKSHOP ROLE THEORY
   DeVore J. L., 2011, PROBABILITY STAT ENG, P508
   Diamond ME, 2019, PLOS BIOL, V17, DOI 10.1371/journal.pbio.3000430
   Dogan G, 2017, INTERNATIONAL CONFERENCE ON BIG DATA AND INTERNET OF THINGS (BDIOT 2017), P200, DOI 10.1145/3175684.3175692
   Engel D.W., 2015, UQ METHODS HPDA CYBE
   Enke B., 2019, COGNITIVE UNCERTAINT
   Enke B., 2019, COGNITIVE UNCERTAINI
   Federico P, 2016, BEYOND TIME AND ERRORS: NOVEL EVALUATION METHODS FOR VISUALIZATION, BELIV 2016, P104, DOI 10.1145/2993901.2993915
   Fodor I. K, 2002, SURVEY DIMENSION RED
   Frank AU, 2007, NATO SCI PEACE SECUR, P107, DOI 10.1007/978-1-4020-6438-8_7
   Gal Yarin, 2016, Uncertainty in deep learning
   Gerrits T, 2019, COMPUT GRAPH FORUM, V38, P325, DOI 10.1111/cgf.13692
   Gillmann C., 2016, Uncertainty-Awareness in Open Source Visualization Solutions
   Gillmann C, 2021, IEEE COMPUT GRAPH, V41, P90, DOI 10.1109/MCG.2021.3099881
   Gillmann C, 2021, IEEE COMPUT GRAPH, V41, P7, DOI 10.1109/MCG.2021.3094858
   Gillmann C, 2021, COMPUT GRAPH FORUM, V40, P665, DOI 10.1111/cgf.14333
   Gillmann C, 2018, J IMAGING, V4, DOI 10.3390/jimaging4090109
   Giunta A.A., 2004, 9 ASCE SPEC C PROB M, P26
   Görtler J, 2020, IEEE T VIS COMPUT GR, V26, P822, DOI 10.1109/TVCG.2019.2934812
   Griethe H., 2006, SIMVIS, P143
   Guell, 2017, APPROACH URBAN COMPL
   Guo SN, 2019, INT C POWER ELECT DR, DOI [10.1109/peds44367.2019.8998889, 10.1145/3290605.3300803]
   Hansen CD., 2014, SCI VISUALIZATION UN
   Hasinoff SW, 2010, PROC CVPR IEEE, P553, DOI 10.1109/CVPR.2010.5540167
   Hegel TM, 2010, SPATIAL COMPLEXITY, INFORMATICS, AND WILDLIFE CONSERVATION, P273, DOI 10.1007/978-4-431-87771-4_16
   Heinrich J., 2013, EUROGRAPHICS 2013 ST, DOI DOI 10.2312/CONF/EG2013/STARS/095-116
   Herschel M, 2017, VLDB J, V26, P881, DOI 10.1007/s00778-017-0486-1
   Höferlin M, 2011, J SPAT INT SCI, P87, DOI 10.5311/JOSIS.2010.2.1
   Hoffman PatrickE., 2001, Information Visualization in Data Mining and Knowledge Discovery, P47
   Hu Z., 2015, INT DESIGN ENG TECHN, V57083
   Hullman J, 2020, IEEE T VIS COMPUT GR, V26, P130, DOI 10.1109/TVCG.2019.2934287
   Hullman J, 2019, IEEE T VIS COMPUT GR, V25, P903, DOI 10.1109/TVCG.2018.2864889
   Jaenicke S, 2016, IEEE T VIS COMPUT GR, V22, P200, DOI 10.1109/TVCG.2015.2467620
   Janicke Stefan, 2014, 5th International Conference on Information Visualization Theory and Applications (IVAPP 2014). Proceedings, P59
   Jena A, 2020, IEEE PAC VIS SYMP, P201, DOI 10.1109/PacificVis48177.2020.1014
   Jiao FX, 2010, LECT NOTES COMPUT SC, V6326, P179
   Kamal A, 2021, J VISUAL-JAPAN, V24, P861, DOI 10.1007/s12650-021-00755-1
   Karami A., 2015, 2015 International Workshop on Artificial Intelligence and Cognition (AIC), P146
   Kassiano V, 2017, LECT NOTES COMPUT SC, V10230, P87, DOI 10.1007/978-3-319-57045-7_6
   Kaur J., 2015, Int. J. Hospit. Inf. Technol., V8, P239, DOI [10.14257/ijhit.2015.8.7.22, DOI 10.14257/IJHIT.2015.8.7.22]
   Keim D, 2008, LECT NOTES COMPUT SC, V4950, P154, DOI 10.1007/978-3-540-70956-5
   Keim Daniel., 2011, Proceedings of the 11th International Conference on Knowledge Management and Knowledge Technologies, P1, DOI DOI 10.1145/2024288.2024290
   Keim DA, 2008, LECT NOTES COMPUT SC, V4404, P76, DOI 10.1007/978-3-540-71080-6_6
   Kerdjoudj F., 2015, EVALUATING UNCERTAIN
   Khulusi R, 2020, COMPUT GRAPH FORUM, V39, P82, DOI 10.1111/cgf.13905
   Kniss JM, 2008, I S BIOMED IMAGING, P832, DOI 10.1109/ISBI.2008.4541125
   Kohlhammer J, 2009, NATO SCI PEACE SECUR, P299, DOI 10.1007/978-90-481-2899-0_23
   Kretzschmar V., 2020, VISION MODELING VISU, P57
   KU HH, 1966, J RES NBS C ENG INST, VC 70, P263, DOI 10.6028/jres.070C.025
   Lee G, 2019, STRUCT MULTIDISCIP O, V60, P1619, DOI 10.1007/s00158-019-02270-2
   Leffrang D, 2021, 2021 IEEE WORKSHOP ON TRUST AND EXPERTISE IN VISUAL ANALYTICS (TREX 2021), P20, DOI 10.1109/TREX53765.2021.00009
   Lewandowsky S., 2015, UNCERTAINTY KNOWLEDG
   Li LN, 2018, COMPREHENSIVE GEOGRAPHIC INFORMATION SYSTEMS, VOL 1: GIS METHODS AND TECHNIQUES, P313
   Lin G., 2012, SURVEY EVALUATE UNCE
   Lipsa DR, 2012, COMPUT GRAPH FORUM, V31, P2317, DOI 10.1111/j.1467-8659.2012.03184.x
   Liu SX, 2018, VIS INFORM, V2, P191, DOI 10.1016/j.visinf.2018.12.001
   Loucks D.P., 2017, Water Resources Planning and Management An Introduction to Methods, Models, and Application, DOI DOI 10.1007/978-3-319-44234-1_1
   Maack RGC, 2021, COMPUT GRAPH-UK, V98, P293, DOI 10.1016/j.cag.2021.05.011
   MacEachren A. M., 2005, CARTOGR GOEGR INFOR, V32, P139, DOI [10.1559/1523040054738936, DOI 10.1559/1523040054738936]
   MacEachren A.M., 2015, "Visual analytics and uncertainty: Its not about the data"
   Maier H., 2008, SENSITIVITY UNCERTAI
   Mastrandrea MD, 2011, CLIMATIC CHANGE, V108, P675, DOI 10.1007/s10584-011-0178-6
   Olston C, 2002, INFOVIS 2002: IEEE SYMPOSIUM ON INFORMATION VISUALIZATION 2002, P37, DOI 10.1109/INFVIS.2002.1173145
   Pfeiffer JJ, 2002, LECT NOTES ARTIF INT, V2317, P279
   Plaisant C., 2004, Proceedings of the Working Conference on Advanced Visual Interfaces, AVI'04, page, P109, DOI [10.1145/989863.9898802, DOI 10.1145/989863.9898802, 10.1145/989863.989880, DOI 10.1145/989863.989880]
   Potter Kristin, 2012, IFIP Adv Inf Commun Technol, V377, P226
   Preston A, 2019, IEEE COMPUT GRAPH, V39, P72, DOI 10.1109/MCG.2019.2918158
   Ragan ED, 2016, IEEE T VIS COMPUT GR, V22, P31, DOI 10.1109/TVCG.2015.2467551
   Raith F., 2021, P WORKSHOP VISUALISA
   Ranftl S., 2021, 40 INT WORKSHOP BAYE, V6, DOI DOI 10.3390/PSF2021003006
   Rebba R, 2008, RELIAB ENG SYST SAFE, V93, P1197, DOI 10.1016/j.ress.2007.08.001
   Ristovski G, 2014, COMPUT GRAPH-UK, V39, P60, DOI 10.1016/j.cag.2013.10.015
   Sacha D, 2016, IEEE T VIS COMPUT GR, V22, P240, DOI 10.1109/TVCG.2015.2467591
   Sacha D, 2014, IEEE T VIS COMPUT GR, V20, P1604, DOI 10.1109/TVCG.2014.2346481
   Saltelli A., 2004, SENSITIVITY ANAL PRA, DOI [10.1002/0470870958, DOI 10.1002/0470870958]
   Schulz C, 2017, IEEE T VIS COMPUT GR, V23, P531, DOI 10.1109/TVCG.2016.2598919
   Senaratne H.V., 2017, THESIS U KONSTANZ KO
   Shyu Mei-Ling., 1998, Computers and Industrial Engineering, V34, P3
   Sodergren T, 2017, 2017 IEEE VISUALIZATION IN DATA SCIENCE (VDS), P52, DOI 10.1109/VDS.2017.8573448
   Sorzano C. O. S., 2014, ARXIV
   Souza RR, 2019, INFORMATICS-BASEL, V6, DOI 10.3390/informatics6030034
   Su X., 2013, VISUALIZATION ENG, V1, P1, DOI DOI 10.1186/2213-7459-1-2
   Szafir D. A., 2018, Interactions, V25, P26, DOI [DOI 10.1145/3231772, DOI 10.1145/32317721, 10.1145/3231772]
   Taylor P.J., 2015, World city network: a global urban analysis
   Therón R, 2006, LECT NOTES COMPUT SC, V4224, P191
   Sánchez RT, 2019, INFORMATICS-BASEL, V6, DOI 10.3390/informatics6030031
   Thomas J.J., 2005, ILLUMINATING APTH RE
   Timpf Sabine., 2012, Advances in spatial data handling: geospatial dynamics, geosimulation and exploratory visualization
   Varga M., 2016, BUILDING TRUST INFOR, P141
   Vehlow C, 2013, BMC BIOINFORMATICS, V14, DOI 10.1186/1471-2105-14-S19-S2
   Vosough Z., 2017, P 10 INT S VISUAL IN, P1, DOI [10.1145/3105971.3105972, DOI 10.1145/3105971.3105972]
   Wall E., 2018, COGNITIVE BIASES VIS, P29, DOI [DOI 10.1007/978-3-319-95831-63, 10.1007/978-3-319-95831-6_3, DOI 10.1007/978-3-319-95831-6_3, 10.1007/978-3-319-95831-63]
   Wallace Michele., 2015, BLACK MACHO MYTH SUP
   Wang H, 2020, ACM COMPUT SURV, V53, DOI 10.1145/3409383
   Wang J., 2018, IEEE T VIS COMPUT GR, P1
   Ward M., 2015, Interactive data visualization: Foundations, techniques, and applications
   Watanabe N, 2010, COMPUT MECH, V45, P263, DOI 10.1007/s00466-009-0445-9
   WILSON R, 1984, IEEE T PATTERN ANAL, V6, P758, DOI 10.1109/TPAMI.1984.4767599
   Wu DTY, 2019, J AM MED INFORM ASSN, V26, P314, DOI 10.1093/jamia/ocy190
   Xu K, 2020, COMPUT GRAPH FORUM, V39, P757, DOI 10.1111/cgf.14035
   Xu R, 2005, IEEE T NEURAL NETWOR, V16, P645, DOI 10.1109/TNN.2005.845141
   Yan L, 2020, IEEE T VIS COMPUT GR, V26, P832, DOI 10.1109/TVCG.2019.2934242
   YANG MS, 1993, MATH COMPUT MODEL, V18, P1, DOI 10.1016/0895-7177(93)90202-A
   Zhou X., 2021, Neurocomputing
NR 126
TC 4
Z9 4
U1 1
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2023
VL 39
IS 12
BP 6345
EP 6366
DI 10.1007/s00371-022-02733-6
EA DEC 2022
PG 22
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA X7HK1
UT WOS:000903397500001
OA hybrid
DA 2024-07-18
ER

PT J
AU Mou, T
   Li, XB
AF Mou, Tong
   Li, Xiaobin
TI Adaptive arc area inpainting and image enhancement method based on
   AI-DLC model
SO VISUAL COMPUTER
LA English
DT Article
DE Complex industrial scene; Adaptive method; Arc area inpainting; Image
   enhancement
ID CONTRAST ENHANCEMENT; HISTOGRAM EQUALIZATION
AB Complex industrial scenarios are accompanied by many disturbances, especially when arcs are used as a industrial technique method in the process flow. Arc interference can cause disturbances such as contrast reduction, color deviation, instantaneous overexposure, low illumination, and loss of detail to the captured images, as hinders the development of industry toward the intelligent direction industrial intelligence. Due to the particularity of arc interference, none of the existing studies can be applied to the inpainting of such images. In this study, we constructed the Arc Interference-Distance, Light intensity, and Color (AI-DLC) model by analyzing the characteristics of arc light and its mechanism of interference to the image, which was used to measure the local interference of arc light. Based on this model, we propose the adaptive arc area inpainting and image enhancement method. This method, firstly, splits the original image into several equal-sized patches. Secondly, it classifies them according to the model values. Finally, the patches are processed by each adaptive module. Through experiments in real industrial scenes, compared with commonly used image restoration methods, this method can effectively repair the arc area, enhance image information, and improve image quality.
C1 [Mou, Tong; Li, Xiaobin] Shanghai Inst Technol, Dept Control Sci & Engn, 100 Haiquan Rd, Shanghai 201418, Peoples R China.
C3 Shanghai Institute of Technology
RP Li, XB (corresponding author), Shanghai Inst Technol, Dept Control Sci & Engn, 100 Haiquan Rd, Shanghai 201418, Peoples R China.
EM 939209649@qq.com; lixiaobinauto@163.com
FU Baosteel Technology Research and Development Fund [2021310014000358];
   Shanghai Collaborative Innovation Technology Fund [XTCX2022-29]
FX We would like to thank the reviewers for their help in improving the
   paper. This work was supported by the Baosteel Technology Research and
   Development Fund (Grant No. 2021310014000358). This work was supported
   by Shanghai Collaborative Innovation Technology Fund (Grant No.
   XTCX2022-29).
CR Bilcu RC, 2011, IEEE SIGNAL PROC LET, V18, P165, DOI 10.1109/LSP.2011.2105476
   Bulut F, 2022, VISUAL COMPUT, V38, P2239, DOI 10.1007/s00371-021-02281-5
   Cai BL, 2016, IEEE T IMAGE PROCESS, V25, P5187, DOI 10.1109/TIP.2016.2598681
   Cai JR, 2019, IEEE I CONF COMP VIS, P3086, DOI 10.1109/ICCV.2019.00318
   Celik T, 2012, PATTERN RECOGN, V45, P3810, DOI 10.1016/j.patcog.2012.03.019
   Chen ZH, 2020, VISUAL COMPUT, V36, P2189, DOI 10.1007/s00371-020-01929-y
   Fan X, 2019, VISUAL COMPUT, V35, P565, DOI 10.1007/s00371-018-1485-y
   Galdran A, 2018, PROC CVPR IEEE, P8212, DOI 10.1109/CVPR.2018.00857
   Galdran A, 2015, SIAM J IMAGING SCI, V8, P1519, DOI 10.1137/15M1008889
   Gibson KB, 2013, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2013-37
   He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168
   Hodges C, 2019, PATTERN RECOGN LETT, V128, P70, DOI 10.1016/j.patrec.2019.08.013
   Hussin WMSB, 2019, MULTIMED TOOLS APPL, V78, P10401, DOI 10.1007/s11042-018-6566-4
   Jang CY, 2012, INT SOC DESIGN CONF, P37, DOI 10.1109/ISOCC.2012.6406919
   Kim TK, 1998, IEEE T CONSUM ELECTR, V44, P82, DOI 10.1109/30.663733
   Lee S, 2010, IEEE T CONSUM ELECTR, V56, P2636, DOI 10.1109/TCE.2010.5681151
   Li BY, 2017, IEEE I CONF COMP VIS, P4780, DOI 10.1109/ICCV.2017.511
   Li M, 2022, VISUAL COMPUT, V38, P4203, DOI 10.1007/s00371-021-02289-x
   Li ZG, 2015, IEEE T IMAGE PROCESS, V24, P5432, DOI 10.1109/TIP.2015.2482903
   Manju RA, 2019, PROCEDIA COMPUT SCI, V165, P391, DOI 10.1016/j.procs.2020.01.033
   NARENDRA PM, 1981, IEEE T PATTERN ANAL, V3, P655, DOI 10.1109/TPAMI.1981.4767166
   Nishino K, 2012, INT J COMPUT VISION, V98, P263, DOI 10.1007/s11263-011-0508-1
   Shanmugavadivu P, 2014, VISUAL COMPUT, V30, P387, DOI 10.1007/s00371-013-0863-8
   Shao WB, 2020, J MOD OPTIC, V67, P1190, DOI 10.1080/09500340.2020.1823502
   Stark JA, 2000, IEEE T IMAGE PROCESS, V9, P889, DOI 10.1109/83.841534
   Thai B, 2017, SIGNAL IMAGE VIDEO P, V11, P525, DOI 10.1007/s11760-016-0990-6
   Wang AN, 2019, IEEE T IMAGE PROCESS, V28, P381, DOI 10.1109/TIP.2018.2868567
   Wang GM, 2021, PROC CVPR IEEE, P15905, DOI 10.1109/CVPR46437.2021.01565
   Wang JB, 2018, IEEE T CIRC SYST VID, V28, P2190, DOI 10.1109/TCSVT.2017.2728822
   Wang XT, 2021, IEEE INT CONF COMP V, P1905, DOI 10.1109/ICCVW54120.2021.00217
   Wang YF, 2016, NEUROCOMPUTING, V177, P373, DOI 10.1016/j.neucom.2015.10.124
   Yang CW, 2019, VISUAL COMPUT, V35, P695, DOI 10.1007/s00371-018-1504-z
   Yang X, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2021.3122330
   Yang Y, 2020, MULTIDIM SYST SIGN P, V31, P619, DOI 10.1007/s11045-019-00678-z
   Yu NN, 2023, VISUAL COMPUT, V39, P1251, DOI 10.1007/s00371-022-02402-8
   Yu SY, 2016, J MOD OPTIC, V63, P2121, DOI 10.1080/09500340.2016.1184340
   Zhang SD, 2020, VISUAL COMPUT, V36, P1797, DOI 10.1007/s00371-019-01774-8
   [智宁 Zhi Ning], 2016, [中国图象图形学报, Journal of Image and Graphics], V21, P1585
   Zhu QS, 2015, IEEE T IMAGE PROCESS, V24, P3522, DOI 10.1109/TIP.2015.2446191
   Zotin Alexander, 2018, Procedia Computer Science, V131, P6, DOI 10.1016/j.procs.2018.04.179
NR 40
TC 1
Z9 1
U1 3
U2 8
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2023
VL 39
IS 12
BP 6151
EP 6165
DI 10.1007/s00371-022-02718-5
EA NOV 2022
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA X7HK1
UT WOS:000881556300001
DA 2024-07-18
ER

PT J
AU Liu, XQ
   Li, JT
   Lu, GD
AF Liu, Xinqi
   Li, Jituo
   Lu, Guodong
TI Improving RGB-D-based 3D reconstruction by combining voxels and points
SO VISUAL COMPUTER
LA English
DT Article
DE RGB-D reconstruction; Voxels and points; Noisy depth image; Accuracy
   improvement
ID SCENE RECONSTRUCTION; SUPERRESOLUTION
AB We propose a flexible 3D reconstruction method based on the RGB-D data stream. Compared to previous methods using pure voxels or pure points as representations, our works propose a new representation combining voxels and points to improve the reconstruction accuracy. A key insight is that points can store additional depth data that are not sampled by regular voxels. Thus, by integrating points and voxels, the 3D reconstruction process can be accelerated due to higher data utilization. Furthermore, depth information stored in points is used to refine the noisy depth image through a depth image refinement method, consequently improving the reconstructed shape quality. Extensive comparative experiments are performed including different representations (pure voxels/points) and various methods (fusion-based/learning-based and online/offline) to illustrate the effectiveness of our work. Experimental results demonstrate that our method can achieve real-time performance, effectively avoid artifacts, and reach state-of-the-art accuracy levels. More importantly, we provide a novel idea to balance the conflict between memory overhead and reconstruction accuracy.
C1 [Liu, Xinqi; Li, Jituo; Lu, Guodong] Zhejiang Univ, Inst Design Engn, Sch Mech Engn, Hangzhou 310027, Peoples R China.
C3 Zhejiang University
RP Li, JT (corresponding author), Zhejiang Univ, Inst Design Engn, Sch Mech Engn, Hangzhou 310027, Peoples R China.
EM jituo_li@zju.edu.cn
RI Gao, Zihao/KIL-4959-2024
FU National Key Research and Development Program of China [2018YFB1700700];
   National Natural Science Foundation of China [61732015, 61972340];
   Research Funding of Zhejiang University Robotics Institute
FX This work was supported in part by the National Key Research and
   Development Program of China (2018YFB1700700), the National Natural
   Science Foundation of China (61732015, 61972340), and the Research
   Funding of Zhejiang University Robotics Institute.
CR Alldieck T, 2019, IEEE I CONF COMP VIS, P2293, DOI 10.1109/ICCV.2019.00238
   Altantawy DA, 2020, VISUAL COMPUT, V36, P333, DOI 10.1007/s00371-018-1611-x
   Arikan M, 2014, IEEE T VIS COMPUT GR, V20, P1280, DOI 10.1109/TVCG.2014.2312011
   Cao YP, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3182157
   Chabra Rohan, 2020, P EUR C COMP VIS
   Chang A. X., 2015, ARXIV
   Chen JW, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461940
   Chibane J, 2020, PROC CVPR IEEE, P6968, DOI 10.1109/CVPR42600.2020.00700
   Dai A., 2017, ACM Transactions on Graphics (ToG), V36, DOI DOI 10.1145/3054739
   Deng B., 2020, P EUR C COMP VIS, P612
   Dou MS, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925969
   Fu YP, 2020, VISUAL COMPUT, V36, P2215, DOI 10.1007/s00371-020-01899-1
   Fuhrmann S, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024182
   Gao W., 2018, ROBOTICS SCI SYSTEMS, V14
   Haefner B, 2020, IEEE T PATTERN ANAL, V42, P2453, DOI 10.1109/TPAMI.2019.2923621
   He T., 2020, ADV NEURAL INFORM PR
   Huang Jiahui, 2021, CVPR
   Huang JW, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130824
   Jiang CY, 2020, PROC CVPR IEEE, P6000, DOI 10.1109/CVPR42600.2020.00604
   Jiang ZY, 2021, SIGNAL PROCESS-IMAGE, V90, DOI 10.1016/j.image.2020.116040
   Kazhdan M, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2487228.2487237
   Kellert M., 2013, 2013 Conference on Lasers & Electro-Optics. Europe & International Quantum Electronics Conference (CLEO EUROPE/IQEC), DOI 10.1109/CLEOE-IQEC.2013.6800663
   Lan ZQ, 2019, PROC CVPR IEEE, P9682, DOI 10.1109/CVPR.2019.00992
   Lefloch D, 2015, 2015 18TH INTERNATIONAL CONFERENCE ON INFORMATION FUSION (FUSION), P2121
   Li K., 2018, COMPUTER VISION
   Liu XM, 2019, IEEE T IMAGE PROCESS, V28, P1636, DOI 10.1109/TIP.2018.2875506
   Liu XQ, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20154330
   Lu FX, 2018, VISUAL COMPUT, V34, P753, DOI 10.1007/s00371-018-1540-8
   Mac Aodha O, 2012, LECT NOTES COMPUT SC, V7574, P71, DOI 10.1007/978-3-642-33712-3_6
   Maier R, 2017, IEEE I CONF COMP VIS, P3133, DOI 10.1109/ICCV.2017.338
   Marquina A, 2008, J SCI COMPUT, V37, P367, DOI 10.1007/s10915-008-9214-8
   Monica R, 2020, IEEE T VIS COMPUT GR, V26, P2683, DOI 10.1109/TVCG.2020.2990315
   Mostegel C, 2017, PROC CVPR IEEE, P2501, DOI 10.1109/CVPR.2017.268
   Natsume R, 2019, PROC CVPR IEEE, P4475, DOI 10.1109/CVPR.2019.00461
   NEWCOMBE RA, 2015, PROC CVPR IEEE, P343, DOI DOI 10.1109/CVPR.2015.7298631
   Newcombe RA, 2011, INT SYM MIX AUGMENT, P127, DOI 10.1109/ISMAR.2011.6092378
   Niessner M., 2013, ACM T GRAPHIC, V32, DOI DOI 10.1145/2508363.2508374
   Park J, 2011, IEEE I CONF COMP VIS, P1623, DOI 10.1109/ICCV.2011.6126423
   Park JJ, 2019, PROC CVPR IEEE, P165, DOI 10.1109/CVPR.2019.00025
   Peng Songyou, 2020, Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part III 16, P523
   Saito S, 2020, PROC CVPR IEEE, P81, DOI 10.1109/CVPR42600.2020.00016
   Saito S, 2019, IEEE I CONF COMP VIS, P2304, DOI 10.1109/ICCV.2019.00239
   Schöps T, 2020, IEEE T PATTERN ANAL, V42, P2494, DOI 10.1109/TPAMI.2019.2947048
   Song X., 2020, 2020 IEEECVF C COMPU
   Song XB, 2017, LECT NOTES COMPUT SC, V10114, P360, DOI 10.1007/978-3-319-54190-7_22
   Sumner RW, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239531
   Wang KK, 2021, VISUAL COMPUT, V37, P603, DOI 10.1007/s00371-020-01826-4
   Weder S, 2021, PROC CVPR IEEE, P3161, DOI 10.1109/CVPR46437.2021.00318
   Weder S, 2020, PROC CVPR IEEE, P4886, DOI 10.1109/CVPR42600.2020.00494
   Wen Y, 2019, IEEE T IMAGE PROCESS, V28, P994, DOI 10.1109/TIP.2018.2874285
   Whelan T, 2015, ROBOTICS: SCIENCE AND SYSTEMS XI
   Wong YS, 2021, COMPUT GRAPH FORUM, V40, P511, DOI 10.1111/cgf.142651
   Wu HZ, 2016, IEEE T VIS COMPUT GR, V22, P2012, DOI 10.1109/TVCG.2015.2498617
   Xia ZY, 2018, IEEE INT C INT ROBOT, P8449, DOI 10.1109/IROS.2018.8594061
   Yang H, 2020, VISUAL COMPUT, V36, P1411, DOI 10.1007/s00371-019-01748-w
   Yang SY, 2022, VISUAL COMPUT, V38, P883, DOI 10.1007/s00371-021-02057-x
   Yang Y, 2019, IEEE INT CONF ROBOT, P5238, DOI [10.1109/ICRA.2019.8794355, 10.1109/icra.2019.8794355]
   Ye XC, 2020, IEEE T IMAGE PROCESS, V29, P7427, DOI 10.1109/TIP.2020.3002664
   Yu LF, 2013, PROC CVPR IEEE, P1415, DOI 10.1109/CVPR.2013.186
   Zhang JZ, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459676
   Zhao LJ, 2019, PATTERN RECOGN, V88, P356, DOI 10.1016/j.patcog.2018.11.028
   Zheng Zhedong, 2020, CORR
   Zhenpei Yang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11115, DOI 10.1109/CVPR42600.2020.01113
   Zhou QY, 2013, IEEE I CONF COMP VIS, P473, DOI 10.1109/ICCV.2013.65
   Zhou QY, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601134
   Zollhöfer M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766887
NR 66
TC 0
Z9 0
U1 6
U2 27
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2023
VL 39
IS 11
BP 5309
EP 5325
DI 10.1007/s00371-022-02661-5
EA OCT 2022
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA W5SX6
UT WOS:000864610600001
DA 2024-07-18
ER

PT J
AU Fu, Q
   Bai, X
   Zheng, YF
   Du, RS
   Wang, DQ
   Zhang, TY
AF Fu, Qian
   Bai, Xue
   Zheng, Yafeng
   Du, Runsheng
   Wang, Dongqing
   Zhang, Tianyi
TI VisOJ: real-time visual learning analytics dashboard for online
   programming judge
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT ACIAT202
CY 2022
CL ELECTR NETWORK
DE Learning analytics dashboard; Online judge; Visual analytics;
   Programming study
ID STUDENTS
AB Online Judge (OJ) is an important aid for programming learning that can help students evaluate learning effects in real-time, while teachers can adjust practice tasks in time according to the records of the tool. With these advantages, OJ shows great value for promoting teaching and learning in programming. The existing OJ system usually only provides information such as problem status list and recent rank list. However, it is unable to provide teachers with more fine-grained analysis information, such as the distribution of students' incorrect responses and level of knowledge mastery. And it also cannot provide students with effective comparative information on their learning status. This research developed a visual learning analytics dashboard named VisOJ for the OJ system, which includes two types of user interfaces: teacher and student. The teacher interface presents students' learning status and ranking trends, which help teachers monitor and give feedback on their learning activities. The student interface provides views such as error type analysis and evaluation, which promote students' self-reflection and self-regulation. Preliminary case studies and expert interviews prove the usability of the dashboard. In the end, we summarize our main work and suggest future research directions.
C1 [Fu, Qian] Beijing Normal Univ, Fac Educ, Sch Educ Technol, Beijing, Peoples R China.
   [Bai, Xue] Henan Univ Econ & Law, Sch Math & Informat Sci, Zhengzhou, Peoples R China.
   [Zheng, Yafeng] Beijing Normal Univ Zhuhai, Inst Adv Studies Humanities & Social Sci, Zhuhai, Peoples R China.
   [Du, Runsheng; Wang, Dongqing] Henan Univ Econ & Law, Sch Comp & Informat Engn, Zhengzhou, Peoples R China.
   [Zhang, Tianyi] Univ Penn, Sch Engn & Appl Sci, Philadelphia, PA 19104 USA.
C3 Beijing Normal University; Henan University of Economics & Law; Beijing
   Normal University; Beijing Normal University Zhuhai; Henan University of
   Economics & Law; University of Pennsylvania
RP Zheng, YF (corresponding author), Beijing Normal Univ Zhuhai, Inst Adv Studies Humanities & Social Sci, Zhuhai, Peoples R China.
EM zlzyf@126.com
RI Zhang, Tianyi/AAJ-6909-2020
OI Zhang, Tianyi/0000-0002-9266-6082
FU National Natural Science Foundation of China [NSFC: 61907011, 62077005]
FX This research was funded by the National Natural Science Foundation of
   China (NSFC: 61907011, 62077005).
CR [Anonymous], 2021, STAT CHARTS
   Baker M., 2021, J ARTIF INTELL TECHN, DOI [10.37965/jait.2021.0003, DOI 10.37965/JAIT.2021.0003]
   Bez JL, 2014, INT CONF COMP SCI ED, P149, DOI 10.1109/ICCSE.2014.6926445
   Brooke J, 1996, USABILITY EVALUATION, V189, P4
   Charleer S, 2018, IEEE T LEARN TECHNOL, V11, P389, DOI 10.1109/TLT.2017.2720670
   Charleer S, 2016, LECT NOTES COMPUT SC, V9891, P42, DOI 10.1007/978-3-319-45153-4_4
   Chen Q, 2020, IEEE T VIS COMPUT GR, V26, P1622, DOI 10.1109/TVCG.2018.2872961
   Echart, 2021, US
   Emmons SR, 2017, J ASSOC INF SCI TECH, V68, P2350, DOI 10.1002/asi.23852
   Ghosh S, 2020, CAAI T INTELL TECHNO, V5, P55, DOI 10.1049/trit.2019.0051
   Guerra J, 2019, LECT NOTES COMPUT SC, V11722, P765, DOI 10.1007/978-3-030-29736-7_84
   He H, 2018, IEEE CONF VIS ANAL, P25, DOI 10.1109/VAST.2018.8802383
   Hillaire G., 2016, Journal of Learning Analytics, V3, P115, DOI [DOI 10.18608/JLA.2016.33.7, 10.18608/jla.2016.33, DOI 10.18608/JLA.2016.33]
   Ji LE, 2021, VISUAL COMPUT, V37, P477, DOI 10.1007/s00371-020-01818-4
   Jivet I, 2018, PROCEEDINGS OF THE 8TH INTERNATIONAL CONFERENCE ON LEARNING ANALYTICS & KNOWLEDGE (LAK'18): TOWARDS USER-CENTRED LEARNING ANALYTICS, P31, DOI 10.1145/3170358.3170421
   Kokoç M, 2021, BEHAV INFORM TECHNOL, V40, P161, DOI 10.1080/0144929X.2019.1680731
   Krumm A.E., 2014, Learning analytics: From research to practice, P103, DOI [DOI 10.1007/978-1-4614-3305-7_6, 10.1007/978-1-4614-3305-7_6, DOI 10.1007/978-1-4614-3305-76]
   Law Check-Yee., 2017, Proceedings of the Nineteenth Australasian Computing Education Conference, P55
   Lee LK, 2020, J COMPUT EDUC, V7, P1, DOI 10.1007/s40692-020-00155-8
   Lu XD, 2017, 2017 IEEE INTERNATIONAL CONFERENCE ON INTERNET OF THINGS (ITHINGS) AND IEEE GREEN COMPUTING AND COMMUNICATIONS (GREENCOM) AND IEEE CYBER, PHYSICAL AND SOCIAL COMPUTING (CPSCOM) AND IEEE SMART DATA (SMARTDATA), P573, DOI 10.1109/iThings-GreenCom-CPSCom-SmartData.2017.90
   Matcha W, 2020, IEEE T LEARN TECHNOL, V13, P226, DOI 10.1109/TLT.2019.2916802
   Michele Selivon J.L.B.N, 23 WEI WORKSHOP EDUC
   SPOJ, 2021, US
   Tica IT, 2021, SMART INNOV SYST TEC, V197, P245, DOI 10.1007/978-981-15-7383-5_21
   URAL, US
   Urban Vineyard Association (UVA), About us
   USACO, US
   Verbert K, 2020, LAK20: THE TENTH INTERNATIONAL CONFERENCE ON LEARNING ANALYTICS & KNOWLEDGE, P35, DOI 10.1145/3375462.3375504
   Vieira C, 2018, COMPUT EDUC, V122, P119, DOI 10.1016/j.compedu.2018.03.018
   Wang M, 2021, TSINGHUA SCI TECHNOL, V26, P548, DOI 10.26599/TST.2020.9010016
   Wang R, 2016, VISUAL COMPUT, V32, P1379, DOI 10.1007/s00371-015-1206-8
   Wu HT, 2016, 2016 INTERNATIONAL SYMPOSIUM ON EDUCATIONAL TECHNOLOGY (ISET), P57, DOI 10.1109/ISET.2016.14
   Wu TS, 2016, IEEE PAC VIS SYMP, P194, DOI 10.1109/PACIFICVIS.2016.7465269
   Xia M, 2020, COMPUT GRAPH FORUM, V39, P511, DOI 10.1111/cgf.13998
   Xia M, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300864
   Xu B, 2020, SYMMETRY-BASEL, V12, DOI 10.3390/sym12040601
   Zhao WX, 2018, ACM T INFORM SYST, V36, DOI 10.1145/3158670
   Zhu XL, 2020, VISUAL COMPUT, V36, P743, DOI 10.1007/s00371-019-01660-3
NR 38
TC 1
Z9 1
U1 5
U2 34
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2023
VL 39
IS 6
SI SI
BP 2393
EP 2405
DI 10.1007/s00371-022-02586-z
EA AUG 2022
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA J1YY4
UT WOS:000837485500001
DA 2024-07-18
ER

PT J
AU Chen, ZY
   Huang, GH
   Wang, Y
   Qiu, JH
   Yang, F
   Yu, ZW
   Pun, CM
   Ling, WK
AF Chen, Ziyang
   Huang, Guoheng
   Wang, Ying
   Qiu, Junhao
   Yang, Fan
   Yu, Zhiwen
   Pun, Chi-Man
   Ling, Wing-Kuen
TI Bi-deformation-UNet: recombination of differential channels for printed
   surface defect detection
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Subtle defects; Object detection; Edge detection; Metric learning;
   Class-imbalance
ID INSPECTION; CLASSIFICATION
AB Deep learning is frequently recommended for standard defect detection because of its ace accuracy and robustness. Unfortunately, current deep learning methods exist several challenges in detecting printed surface defects with multi-scale textures. Firstly, the existing methods only highlight the texture of defects, but concealed the color information of defects. Secondly, since the subtle defects of printed contained with weak semantic, it is difficult for current multi-scale network to locate the defects. Finally, current metric methods cannot measure the similarity between each of defect under class-imbalanced precisely. Therefore, Bi-Deformation-UNet (Bi-DUNet) is designed for automatic printed surface defect detection. In Bi-DUNet, the template-defect image pairs are first enhanced by our proposed pre-processing module Recombination of the Differential Channels. This module can highlight the texture and maintain the color information simultaneously. Then, the preprocessed image pairs are fed into the Dual-fusion Module (DM) and generated the output features with edge information and contextual information. The DM consists of two branches: the Template Branch and the Defect Branch. The two branches are identical in structure and Multi-channel Edge Attention Module. Besides, an Automatic Dual-margin Metric Loss is proposed to alleviate the situation of class-imbalance when measuring similarity of output features. Moreover, a 2020 Assembly Line Defective Product dataset (ALDP2020) is proposed, which contains 4000 images with different environment styles. Finally, our proposed Bi-DUNet achieves 3.97% higher than the state-of-the-arts in ALDP2020 in mAP50. The code is available at https://github.com/MRziyang/DefectDetection.git.
C1 [Chen, Ziyang; Huang, Guoheng; Wang, Ying] Guangdong Univ Technol, Sch Comp, Guangzhou 510006, Peoples R China.
   [Qiu, Junhao] Guangdong Univ Technol, Sch Electromech Engn, Guangzhou 510006, Peoples R China.
   [Yang, Fan] Guangdong Univ Technol, Sch Phys & Optoelect Engn, Guangzhou 510006, Peoples R China.
   [Yu, Zhiwen] Univ Macau, Dept Comp & Informat Sci, Taipa 999078, Macao, Peoples R China.
   [Pun, Chi-Man] South China Univ Technol, Sch Comp Sci & Engn, Guangzhou 510006, Peoples R China.
   [Ling, Wing-Kuen] Guangdong Univ Technol, Sch Informat Engn, Guangzhou 510006, Peoples R China.
C3 Guangdong University of Technology; Guangdong University of Technology;
   Guangdong University of Technology; University of Macau; South China
   University of Technology; Guangdong University of Technology
RP Huang, GH (corresponding author), Guangdong Univ Technol, Sch Comp, Guangzhou 510006, Peoples R China.
EM kevinwong@gdut.edu.cn
RI Pun, Chi Man/GRJ-3703-2022
FU R &D projects in key areas of Guangdong Province [2019B010153002];
   Science and technology research in key areas in Foshan [2020001006832];
   Guangdong Key Areas R &D Program Project [2018B010109007]; Guangdong
   Provincial Key Laboratory of Cyber-Physical System [2020B1212060069];
   National Natural Science Foundation of China Guangdong Joint Fund
   [U1801263, U2001201]; Guangzhou R &D Programme in Key Areas of Science
   and Technology Projects [202007040006]; Program of Marine Economy
   Development (Six Marine Industries) Special Foundation of Department of
   Natural Resources of Guangdong Province [GDNGC [2020]056]
FX This work was supported in part by the R &D projects in key areas of
   Guangdong Province under Grant 2019B010153002 , the Science and
   technology research in key areas in Foshan under Grant 2020001006832,
   the Guangdong Key Areas R &D Program Project under Grant 2018B010109007,
   the Guangdong Provincial Key Laboratory of Cyber-Physical System under
   Grant 2020B1212060069, the National Natural Science Foundation of China
   Guangdong Joint Fund under Grant U1801263 and U2001201, the Guangzhou R
   &D Programme in Key Areas of Science and Technology Projects under Grant
   202007040006, the Program of Marine Economy Development (Six Marine
   Industries) Special Foundation of Department of Natural Resources of
   Guangdong Province under Grant GDNGC [2020]056.
CR Amiri SA., 2012, International Journal of Computer Applications, V12, P38
   [Anonymous], 2018, J KING SAUD UNIV-COM, DOI [10.1016/j.jksuci.2016.11.001, DOI 10.1016/j.jksuci.2016.11.001]
   Bai XL, 2014, IEEE T IND INFORM, V10, P2135, DOI 10.1109/TII.2014.2359416
   Bin Li, 2018, IEEE Signal Processing Letters, V25, P650, DOI 10.1109/LSP.2018.2816569
   Borwankar R, 2018, IEEE T INSTRUM MEAS, V67, P690, DOI 10.1109/TIM.2017.2783098
   Cheng X, 2021, IEEE T INSTRUM MEAS, V70, DOI 10.1109/TIM.2020.3040485
   Cui LS, 2021, IEEE T INSTRUM MEAS, V70, DOI 10.1109/TIM.2021.3056744
   Du XY, 2020, IEEE ACCESS, V8, P152161, DOI 10.1109/ACCESS.2020.3017691
   Feng XM, 2021, APPL INTELL, V51, P5111, DOI 10.1007/s10489-020-02119-y
   Haddad B, 2016, IEEE IMAGE PROC, P754, DOI 10.1109/ICIP.2016.7532458
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He Y, 2020, IEEE T INSTRUM MEAS, V69, P1493, DOI 10.1109/TIM.2019.2915404
   Hu GH, 2015, OPTIK, V126, P1331, DOI 10.1016/j.ijleo.2015.04.017
   Hu WH, 2022, VISUAL COMPUT, V38, P3731, DOI 10.1007/s00371-021-02210-6
   Jia L, 2020, INFORM SCIENCES, V512, P964, DOI 10.1016/j.ins.2019.10.032
   Jing JF, 2017, COLOR TECHNOL, V133, P26, DOI 10.1111/cote.12239
   Kim J, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21154968
   Li XQ, 2017, IEEE SENS J, V17, P412, DOI 10.1109/JSEN.2016.2625815
   Lin H, 2019, J INTELL MANUF, V30, P2525, DOI 10.1007/s10845-018-1415-x
   Lin TY, 2020, IEEE T PATTERN ANAL, V42, P318, DOI 10.1109/TPAMI.2018.2858826
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Lin W.Y., 2018, Advances in physical ergonomics and human factors
   Liu ZF, 2018, PROCEEDINGS OF THE 2ND INTERNATIONAL CONFERENCE ON GRAPHICS AND SIGNAL PROCESSING (ICGSP 2018), P74, DOI 10.1145/3282286.3282300
   Liu ZF, 2017, PROCEEDINGS 2017 4TH IAPR ASIAN CONFERENCE ON PATTERN RECOGNITION (ACPR), P465, DOI 10.1109/ACPR.2017.34
   Lu SJ, 2019, APPL INTELL, V49, P2301, DOI 10.1007/s10489-018-1377-x
   Luan CH, 2020, IEEE IMAGE PROC, P778, DOI 10.1109/ICIP40778.2020.9191128
   Park T., 2020, Advances on Neural Information Processing Systems (NeurIPS)
   Protopapadakis E, 2019, APPL INTELL, V49, P2793, DOI 10.1007/s10489-018-01396-y
   Qiu YH, 2020, IEEE ACCESS, V8, P190663, DOI 10.1109/ACCESS.2020.3032108
   Redmon J, 2018, Arxiv, DOI [arXiv:1804.02767, DOI 10.1109/CVPR.2017.690, DOI 10.48550/ARXIV.1804.02767]
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Shankar K, 2020, IEEE ACCESS, V8, P118164, DOI 10.1109/ACCESS.2020.3005152
   Shuanghui Chen, 2021, 2021 International Conference on Computer Communication and Artificial Intelligence (CCAI), P36, DOI 10.1109/CCAI50917.2021.9447538
   Sohn K, 2016, ADV NEUR IN, V29
   Sun S, 2021, SELF ATT ENH CNNS AV
   Thambusamy V., 2018, IntJPureApplMath, V118, P3681, DOI DOI 10.3390/CANCERS14092132
   Tian Z, 2019, IEEE I CONF COMP VIS, P9626, DOI 10.1109/ICCV.2019.00972
   Wang B, 2020, VISUAL COMPUT, V36, P1897, DOI 10.1007/s00371-019-01779-3
   Wang H., 2013, Opt. Photonics J, V3, P720, DOI DOI 10.4236/OPJ.2013.32B025
   Wang MZ, 2021, AUTOMAT CONSTR, V121, DOI 10.1016/j.autcon.2020.103438
   Wang N, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2805
   Wang WG, 2018, IEEE T IMAGE PROCESS, V27, P2368, DOI 10.1109/TIP.2017.2787612
   Wei B, 2020, NEURAL NETWORKS, V130, P100, DOI 10.1016/j.neunet.2020.06.019
   Win M, 2015, IEEE T IND INFORM, V11, P642, DOI 10.1109/TII.2015.2417676
   Wu SL, 2019, MULTIMED TOOLS APPL, V78, P34627, DOI 10.1007/s11042-019-08042-w
   Xie Q, 2019, IEEE T AUTOM SCI ENG, V16, P1836, DOI 10.1109/TASE.2019.2900170
   Yuan XC, 2015, APPL SURF SCI, V349, P472, DOI 10.1016/j.apsusc.2015.05.033
   Zhang WJ, 2016, VISUAL COMPUT, V32, P275, DOI 10.1007/s00371-015-1065-3
   Zhao D., 2021, APPL INTELL, P1
   Zhiqiang Feng, 2021, Proceedings of the 2021 IEEE 10th Data Driven Control and Learning Systems Conference (DDCLS), P979, DOI 10.1109/DDCLS52934.2021.9455519
   Zhou K, 2016, DESTECH TRANS COMP
NR 52
TC 1
Z9 1
U1 5
U2 30
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2022 JUN 17
PY 2022
DI 10.1007/s00371-077-07554-7
EA JUN 2022
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 2E8DW
UT WOS:000812454600001
DA 2024-07-18
ER

PT J
AU Yan, F
   Silamu, W
   Li, YB
   Chai, YC
AF Yan, Feng
   Silamu, Wushouer
   Li, Yanbin
   Chai, Yachuang
TI SPCA-Net: a based on spatial position relationship co-attention network
   for visual question answering
SO VISUAL COMPUTER
LA English
DT Article
DE BERT; Guided-attention; Self-attention; Faster R-CNN; Spatial position
   relationship
AB Recently, the latest method of VQA (visual question answering) mainly relies on the co-attention to link each visual object with the text object, which can achieve a rough interaction between multiple models. However, VQA models tend to focus on the association between visual and language features without considering the spatial relationship between image region features extracted by Faster R-CNN. This paper proposes an effective deep co-attention network to solve this problem. As a first step, BERT was introduced in order to better capture the relationship between words and make the extracted text feature more robust; secondly, a multimodal co-attention based on spatial location relationship was proposed in order to realize fine-grained interactions between question and image. It consists of three basic components: the text self-attention unit, the image self-attention unit, and the question-guided-attention unit. The self-attention mechanism of image visual features integrates information about the spatial position and width/height of the image area after obtaining attention so that each image area is aware of the relative location and size of other areas. Our experiment results indicate that our model is significantly better than other existing models.
C1 [Yan, Feng; Li, Yanbin; Chai, Yachuang] Xinjiang Univ, Sch Informat Sci & Engn, Urumqi, Peoples R China.
   [Silamu, Wushouer] Lab Multilingual Informat Technol Xinjiang, Urumqi, Peoples R China.
C3 Xinjiang University
RP Yan, F (corresponding author), Xinjiang Univ, Sch Informat Sci & Engn, Urumqi, Peoples R China.
EM yanfeng@stu.xju.edu.cn
RI chen, qiang/JXY-6982-2024
OI Yan, Feng/0000-0002-8615-139X
FU National Natural Science Foundation of China [U1911401]; Ministry of
   Science and Technology of China [ZDI135-96]
FX This work was supported in part by National Natural Science Foundation
   of China under Grant U1911401 and Key Project of Science and Technology
   Innovation 2030 supported by the Ministry of Science and Technology of
   China under Grant ZDI135-96.
CR Anderson P, 2018, PROC CVPR IEEE, P3674, DOI 10.1109/CVPR.2018.00387
   Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279
   Cadene R, 2019, PROC CVPR IEEE, P1989, DOI 10.1109/CVPR.2019.00209
   Chowdhury MIH, 2018, IEEE IMAGE PROC, P599, DOI 10.1109/ICIP.2018.8451103
   Devlin J., 2018, ARXIV E PRINTS ARXIV
   Donahue J, 2017, IEEE T PATTERN ANAL, V39, P677, DOI 10.1109/TPAMI.2016.2599174
   Nguyen DK, 2018, PROC CVPR IEEE, P6087, DOI 10.1109/CVPR.2018.00637
   Fukui Akira, 2016, P C EMP METH NAT LAN
   Gao P, 2019, IEEE I CONF COMP VIS, P5824, DOI 10.1109/ICCV.2019.00592
   Gao P, 2019, PROC CVPR IEEE, P6632, DOI 10.1109/CVPR.2019.00680
   Gao Y, 2016, PROC CVPR IEEE, P317, DOI 10.1109/CVPR.2016.41
   Han K, 2020, PROC CVPR IEEE, P1577, DOI 10.1109/CVPR42600.2020.00165
   He K., 2016, PROC CVPR IEEE, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]
   Hu RH, 2019, IEEE I CONF COMP VIS, P10293, DOI 10.1109/ICCV.2019.01039
   Hudson DA, 2019, PROC CVPR IEEE, P6693, DOI 10.1109/CVPR.2019.00686
   Kim JH, 2016, ADV NEUR IN, V29
   Kim JH, 2018, ADV NEUR IN, V31
   Kingma D. P., 2014, arXiv
   Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7
   Li LJ, 2019, IEEE I CONF COMP VIS, P10312, DOI 10.1109/ICCV.2019.01041
   Lu JS, 2019, ADV NEUR IN, V32
   Nam H, 2017, PROC CVPR IEEE, P2156, DOI 10.1109/CVPR.2017.232
   Peng L, 2022, IEEE T PATTERN ANAL, V44, P318, DOI 10.1109/TPAMI.2020.3004830
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Sun Q, 2019, ICMR'19: PROCEEDINGS OF THE 2019 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P207, DOI 10.1145/3323873.3325044
   Sun S., 2018, Advances in Neural Information Processing Systems, P760
   Tan H, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P5100
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang P, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1290
   Wang P, 2018, IEEE T PATTERN ANAL, V40, P2413, DOI 10.1109/TPAMI.2017.2754246
   Wu Q, 2018, IEEE T PATTERN ANAL, V40, P1367, DOI 10.1109/TPAMI.2017.2708709
   Wu Q, 2016, PROC CVPR IEEE, P4622, DOI 10.1109/CVPR.2016.500
   Xu HJ, 2016, LECT NOTES COMPUT SC, V9911, P451, DOI 10.1007/978-3-319-46478-7_28
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Yan F, 2022, SENSORS-BASEL, V22, DOI 10.3390/s22031045
   Yu DF, 2017, PROC CVPR IEEE, P4187, DOI 10.1109/CVPR.2017.446
   Yu Z., 2019, MULTIMODAL UNIFIED A
   Yu Z, 2018, IEEE T NEUR NET LEAR, V29, P5947, DOI 10.1109/TNNLS.2018.2817340
   Yu Z, 2019, PROC CVPR IEEE, P6274, DOI 10.1109/CVPR.2019.00644
   Yu Z, 2017, IEEE I CONF COMP VIS, P1839, DOI 10.1109/ICCV.2017.202
   Zhang P., 2022, SCI PROGRAMMING-NETH, P1
   Zhang WF, 2021, KNOWL-BASED SYST, V212, DOI 10.1016/j.knosys.2020.106639
   Zhou LW, 2020, AAAI CONF ARTIF INTE, V34, P13041
NR 43
TC 12
Z9 12
U1 2
U2 11
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2022
VL 38
IS 9-10
SI SI
BP 3097
EP 3108
DI 10.1007/s00371-022-02524-z
EA JUN 2022
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 4N3NE
UT WOS:000811949200001
OA Bronze
DA 2024-07-18
ER

PT J
AU Wang, CH
   Tsai, KY
   Hwang, YS
AF Wang, Chi Hung
   Tsai, Kuo Yu
   Hwang, Yuh Shyan
TI Use improved KINECT to recognize whole-body images to enhance accuracy
SO VISUAL COMPUTER
LA English
DT Article
DE Holographic optical element; Skeleton information; MTCNN; FaceNet;
   Kinect
ID FACE DETECTION
AB Epidemics are severe around the world, and people must wear face masks and authenticate in and out of public places to reduce the risk of virus transmission. However, wearing a face mask cannot detect facial features and affect the accuracy of the recognition system, so limitations need to be overcome. Considering that identity recognition does not rely solely on facial information, we use KINECT to get a wider range of skeleton information for judgment. At the same time, it is proposed to replace the diffractive optical elements of the KINECT sensor with a holographic optical element. The problem of uneven intensity and distortion of the light field can be improved to improve the accuracy of skeleton detection. Next, clear scattering information and images are provided to MTCNN and FaceNet models for whole-body imaging identification and pre-training. Besides, we only keep the skeleton features to represent the standing posture and ignore the difference in appearance, which can reduce the changing appearance features. The experimental results show that the proposed method and model with the improved KINECT can solve the problem that is difficult to identify with the mask, and its accuracy is 99.5%.
C1 [Wang, Chi Hung; Hwang, Yuh Shyan] Taipei Univ Technol, Dept Elect Engn, Taipei, Taiwan.
   [Tsai, Kuo Yu] Feng Chia Univ, Dept Informat Engn & Comp Sci, Taichung, Taiwan.
C3 National Taipei University of Technology; Feng Chia University
RP Tsai, KY (corresponding author), Feng Chia Univ, Dept Informat Engn & Comp Sci, Taichung, Taiwan.
EM wang073000@yahoo.com.tw; kytsai@fcu.edu.tw; yshwang@mail.ntut.edu.tw
RI Wang, Chi-Hung/AFR-6875-2022
OI wang, qi hong/0000-0003-3336-8461
CR [Anonymous], CVPR 2011 WORKSHOPS, DOI DOI 10.1109/CVPRW.2011.5981811
   Aqeelanwar, 2021, MASKTHEFACE
   Banerjee A, 2021, IEEE T CIRC SYST VID, V31, P2206, DOI 10.1109/TCSVT.2020.3019293
   Cahyono F., 2020, 2020 4 INT C VOC ED
   Damer N, 2020, LECT NOTE INFORM, VP-306
   De Marsico M, 2010, IEEE T SYST MAN CY A, V40, P121, DOI 10.1109/TSMCA.2009.2033031
   Domingues A, 2016, TECHNOL HEALTH CARE, V24, P251, DOI 10.3233/THC-151116
   Ferrara M, 2014, J AMB INTEL SMART EN, V6, P435, DOI 10.3233/AIS-140267
   Garcia J., 2008, RANGE MAPPING USING
   Gil R, 2021, J ALZHEIMERS DIS, V79, P9, DOI 10.3233/JAD-201233
   Goldmann L, 2007, IEEE T INF FOREN SEC, V2, P559, DOI 10.1109/TIFS.2007.902019
   Hecht E., 2001, Optics, V4th
   Huang S., 2020, 2020 INT C COMM INF
   Ibrahim R., 2011, COMM SOFTW NETW C
   Khoshelham K., 2011, ISPRS workshop laser scanning, V38, pW12
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Liu J, 2018, IEEE T PATTERN ANAL, V40, P3007, DOI 10.1109/TPAMI.2017.2771306
   Mohanraj V, 2018, J INTELL FUZZY SYST, V34, P1411, DOI 10.3233/JIFS-169436
   Nair S.P., 2021, J INTELL FUZZY SYST, V1
   Rehman B, 2020, VISUAL COMPUT, V36, P633, DOI 10.1007/s00371-019-01649-y
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Shpunt A., 2009, OPTICAL DESIGNS ZERO
   Shpunt A., 2013, OPTICAL PATTERN PROJ
   Shu XB, 2022, IEEE T CIRC SYST VID, V32, P5281, DOI 10.1109/TCSVT.2022.3142771
   Shu XB, 2022, IEEE T PATTERN ANAL, V44, P3300, DOI 10.1109/TPAMI.2021.3050918
   Wang HS, 2017, PROC CVPR IEEE, P3633, DOI 10.1109/CVPR.2017.387
   Wang L., 2020, 2020 CHIN AUT C CAC
   Wu CM, 2021, AUTOM CONTROL COMPUT, V55, P102, DOI 10.3103/S0146411621010090
   Yang Z., 2020, 2020 2 INT C ART INT
   Zhang N., 2007, Tech. Rep. 07-49, P7
   Zhang Y, 2016, IEEE T IMAGE PROCESS, V25, DOI 10.1109/TIP.2016.2549360
NR 31
TC 0
Z9 0
U1 0
U2 7
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2023
VL 39
IS 5
BP 2021
EP 2033
DI 10.1007/s00371-022-02461-x
EA APR 2022
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA D7ZK3
UT WOS:000784106300001
DA 2024-07-18
ER

PT J
AU Wang, YS
   Sun, JG
   Gao, X
   Ye, HM
AF Wang, Yousheng
   Sun, Jinge
   Gao, Xue
   Ye, Hongmei
TI Segmentation of intravascular ultrasound images based on convex-concave
   adjustment in extreme regions
SO VISUAL COMPUTER
LA English
DT Article
DE Intravascular ultrasound; Image segmentation; Edge extraction; Extreme
   region; Contour fitting
ID AUTOMATIC SEGMENTATION; VESSEL; LUMEN; BORDERS
AB The extraction of intima and adventitia from intravascular ultrasound (IVUS) images is of great significance for the diagnosis and treatment of coronary artery disease. However, traditional IVUS image segmentation methods have complex modeling, poor robustness, and need to design different algorithms for internal and external boundaries. This article proposes a method of simultaneous intima and adventitia. We firstly detect extreme regions of the pre-processed image, and then design a screening vector to extract two extreme regions representing the lumen and media. After that, the convex-concave boundaries of the two regions are adjusted by the opening operation with a variable radius of the structural element and the superposition of a circle. Finally, the contours are fitted with ellipse to complete the segmentation. In order to evaluate the performance of the method, we first qualitatively display the extreme region contours and the final contours, and compare the results with those drawn manually by clinical experts. Then, we use Jaccard measure, Dice coefficient, percentage of area difference and Hausdorff distance to test the robust performance and generalization performance, and the index values of inner and outer borders are 0.92 +/- 0.03, 0.96 +/- 0.02, 0.06 +/- 0.04, 0.26 +/- 0.07 mm, and 0.92 +/- 0.04, 0.95 +/- 0.02, 0.06 +/- 0.05, 0.29 +/- 0.08 mm, respectively. Besides, we make a quantitative comparison with the relevant studies. Experiment results show that the proposed automatic methods not only have high accuracy, but also have good robustness.
C1 [Wang, Yousheng; Sun, Jinge; Gao, Xue; Ye, Hongmei] Beijing Univ Technol, 100 Pingyuan, Beijing, Peoples R China.
C3 Beijing University of Technology
RP Sun, JG (corresponding author), Beijing Univ Technol, 100 Pingyuan, Beijing, Peoples R China.
EM sunjqa1@emails.bjut.edu.cn
FU 17th Graduate Science and Technology Foundation of Beijing University of
   Technology [ykj-2018-00615]
FX This research is funded by the 17th Graduate Science and Technology
   Foundation of Beijing University of Technology (key project), the
   Project Number is ykj-2018-00615. In addition, we would like to thank
   Professor Balocco Simone of the University of Barcelona for his help in
   providing datasets containing standard intima and adventitia
   coordinates.
CR Ahonen T, 2004, LECT NOTES COMPUT SC, V3021, P469
   [Anonymous], 2008, 2008 IEEE C COMP VIS
   Antico M, 2020, ULTRASOUND MED BIOL, V46, P422, DOI 10.1016/j.ultrasmedbio.2019.10.015
   Balocco S, 2014, COMPUT MED IMAG GRAP, V38, P70, DOI 10.1016/j.compmedimag.2013.07.001
   Cardinal MHR, 2003, LECT NOTES COMPUT SC, V2879, P432
   Cheng ZM, 2022, VISUAL COMPUT, V38, P749, DOI 10.1007/s00371-021-02075-9
   Destrempes F, 2014, COMPUT MED IMAG GRAP, V38, P91, DOI 10.1016/j.compmedimag.2013.09.004
   Du HY, 2022, COMPUT METH PROG BIO, V215, DOI 10.1016/j.cmpb.2021.106599
   Faraji M, 2018, ULTRASONICS, V84, P356, DOI 10.1016/j.ultras.2017.11.020
   Faraji M, 2015, IEEE T IMAGE PROCESS, V24, P5401, DOI 10.1109/TIP.2015.2477215
   Frostegård J, 2005, J INTERN MED, V257, P485, DOI 10.1111/j.1365-2796.2005.01502.x
   Gao ZF, 2015, ULTRASOUND MED BIOL, V41, P2001, DOI 10.1016/j.ultrasmedbio.2015.03.022
   Hammouche A, 2018, 2018 IEEE LIFE SCIENCES CONFERENCE (LSC), P37, DOI 10.1109/LSC.2018.8572073
   Hossain MM, 2013, PROC SPIE, V8669, DOI 10.1117/12.2007030
   Huang SC, 2013, IEEE T IMAGE PROCESS, V22, P1032, DOI 10.1109/TIP.2012.2226047
   Jodas DS, 2017, MED IMAGE ANAL, V40, P60, DOI 10.1016/j.media.2017.06.006
   Katouzian A, 2012, IEEE T INF TECHNOL B, V16, P823, DOI 10.1109/TITB.2012.2189408
   Klein DA, 2011, IEEE I CONF COMP VIS, P2214, DOI 10.1109/ICCV.2011.6126499
   Lo Vercio L, 2016, INT J COMPUT ASS RAD, V11, P1397, DOI 10.1007/s11548-015-1345-4
   Ma Y., 2021, CHINA DIGIT MED, V16, P98
   Majtey AP, 2005, PHYS REV A, V72, DOI 10.1103/PhysRevA.72.052310
   Mendizabal-Ruiz EG, 2013, MED IMAGE ANAL, V17, P649, DOI 10.1016/j.media.2013.02.003
   Moraes MC, 2010, COMPUT CARDIOL CONF, V37, P389
   Noble JA, 2006, IEEE T MED IMAGING, V25, P987, DOI 10.1109/TMI.2006.877092
   OJALA T, 1994, INT C PATT RECOG, P582, DOI 10.1109/ICPR.1994.576366
   PAPADOGIORGAKI M, 2006, INT C SYST SIGN IM P, V8, P461
   Park JE, 2021, J AM COLL CARDIOL, V77, pS27
   Sofian H, 2015, 2015 IEEE INTERNATIONAL WIE CONFERENCE ON ELECTRICAL AND COMPUTER ENGINEERING (WIECON-ECE), P143, DOI 10.1109/WIECON-ECE.2015.7443882
   Su SR, 2017, I S BIOMED IMAGING, P1120, DOI 10.1109/ISBI.2017.7950713
   Taki A, 2008, INT J COMPUT ASS RAD, V3, P347, DOI 10.1007/s11548-008-0235-4
   Vard A, 2012, AUSTRALAS PHYS ENG S, V35, P135, DOI 10.1007/s13246-012-0131-7
   Wang Y., 2021, OPTIK, V241, P167
   Zhang, 2020, MED DIET HLTH, V18, P185
NR 33
TC 1
Z9 1
U1 1
U2 13
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2023
VL 39
IS 4
BP 1617
EP 1627
DI 10.1007/s00371-022-02432-2
EA MAR 2022
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA C3OY5
UT WOS:000767903700002
DA 2024-07-18
ER

PT J
AU Yu, NN
   Li, JJ
   Hua, Z
AF Yu, Nana
   Li, Jinjiang
   Hua, Zhen
TI FLA-Net: multi-stage modular network for low-light image enhancement
SO VISUAL COMPUTER
LA English
DT Article
DE Image enhancement; Local binary pattern; Feature aggregation module;
   Attention mechanism
ID LOCAL BINARY PATTERN; QUALITY ASSESSMENT; HISTOGRAM EQUALIZATION;
   CLASSIFICATION
AB Under the condition of low illumination, the image is easy to appear unclear, the contrast is not enough, and the details can not be fully displayed, which will inevitably bring huge obstacles to the computer vision task. However, low-light image enhancement is a challenging task. Therefore, we design a multi-stage modular network FLA-Net (F: Feature Aggregation Module, L: LBP Module, A: Attention Module) for low-light image enhancement. In this study, we divide the task into three stages: the first stage is the feature extraction stage (FE stage), in which LBP module is added to the feature extraction stage to help the network better recover the image texture details and other information; the second stage is the feature aggregation stage (FA stage), in which the feature aggregation module is added, which can help the network integrate deeper contrast information and color information; the third stage is the image enhancement stage (IE stage), in which the channel attention module is added to the image enhancement stage to make the network pay more attention to the dark areas of the image, which is conducive to the enhancement and recovery of low-light images. Our FLA-Net method solves a series of problems existing in the existing low-light enhancement methods. In addition, we have verified the effectiveness of FLA-Net on multiple public datasets and compared with multiple existing methods for image enhancement. A large number of experimental results show that the enhanced image generated by our method has better subjective visual quality and is better than the most advanced low-light enhancement method in several objective evaluation indicators.
C1 [Yu, Nana] Shandong Technol & Business Univ ICT Yantai, Sch Informat & Elect Engn, Inst Network Technol, Yantai, Peoples R China.
   [Li, Jinjiang; Hua, Zhen] Shandong Technol & Business Univ, Sch Comp Sci & Technol, Yantai, Peoples R China.
C3 Shandong Technology & Business University
RP Li, JJ (corresponding author), Shandong Technol & Business Univ, Sch Comp Sci & Technol, Yantai, Peoples R China.
EM lijinjiang@gmail.com
RI Hua, Zhen/AGN-6068-2022
FU National Natural Science Foundation of China [61772319, 62002200,
   61976125, 61976124]; Shandong Natural Science Foundation of China
   [ZR2020QF012, ZR2021MF068]
FX This research was supported by the National Natural Science Foundation
   of China (61772319, 62002200, 61976125, 61976124), Shandong Natural
   Science Foundation of China (ZR2020QF012, ZR2021MF068).
CR Al-Ameen Z, 2019, IET IMAGE PROCESS, V13, P1314, DOI 10.1049/iet-ipr.2018.6585
   Alpaslan N, 2020, IEEE SIGNAL PROC LET, V27, P660, DOI 10.1109/LSP.2020.2987474
   Aosiman Abudurusuli, 2020, CSSE '20: Proceedings of the 3rd International Conference on Computer Science and Software Engineering, P181, DOI 10.1145/3403746.3403925
   BRADLEY RA, 1952, BIOMETRIKA, V39, P324, DOI 10.1093/biomet/39.3-4.324
   Cai JR, 2018, IEEE T IMAGE PROCESS, V27, P2049, DOI 10.1109/TIP.2018.2794218
   Capra A., 2006, 2006 Digest of Technical Papers. International Conference on Consumer Electronics, P309, DOI 10.1109/ICCE.2006.1598434
   Chin H, 2019, IEEE I C SIGNAL IMAG, P23, DOI [10.1109/ICSIPA45851.2019.8977790, 10.1109/icsipa45851.2019.8977790]
   Cloude SR, 2021, IEEE GEOSCI REMOTE S, V18, P717, DOI 10.1109/LGRS.2020.2982230
   Dai Q, 2019, SYMMETRY-BASEL, V11, DOI 10.3390/sym11040574
   Dong XC, 2011, INT C PAR DISTRIB SY, P9, DOI 10.1109/ICPADS.2011.115
   Fu XY, 2020, SIGNAL PROCESS-IMAGE, V86, DOI 10.1016/j.image.2020.115892
   Gu ZH, 2020, IEEE T IMAGE PROCESS, V29, P3239, DOI 10.1109/TIP.2019.2958144
   Guo CL, 2020, PROC CVPR IEEE, P1777, DOI 10.1109/CVPR42600.2020.00185
   Guo XJ, 2017, IEEE T IMAGE PROCESS, V26, P982, DOI 10.1109/TIP.2016.2639450
   Gupta B, 2017, COMPUT ELECTR ENG, V62, P360, DOI 10.1016/j.compeleceng.2017.01.010
   Hao SJ, 2020, IEEE T MULTIMEDIA, V22, P3025, DOI 10.1109/TMM.2020.2969790
   Jiang YF, 2021, IEEE T IMAGE PROCESS, V30, P2340, DOI 10.1109/TIP.2021.3051462
   Jobson DJ, 1997, IEEE T IMAGE PROCESS, V6, P965, DOI 10.1109/83.597272
   King DB, 2015, ACS SYM SER, V1214, P1
   Kou QQ, 2019, IEEE SIGNAL PROC LET, V26, P129, DOI 10.1109/LSP.2018.2881544
   Lee C, 2012, IEEE IMAGE PROC, P965, DOI 10.1109/ICIP.2012.6467022
   Lee C, 2012, IEEE T IMAGE PROCESS, V21, P80, DOI 10.1109/TIP.2011.2159387
   Li MD, 2018, IEEE T IMAGE PROCESS, V27, P2828, DOI 10.1109/TIP.2018.2810539
   Loh YP, 2019, COMPUT VIS IMAGE UND, V178, P30, DOI 10.1016/j.cviu.2018.10.010
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Lore KG, 2017, PATTERN RECOGN, V61, P650, DOI 10.1016/j.patcog.2016.06.008
   Low CY, 2021, IEEE SIGNAL PROC LET, V28, P354, DOI 10.1109/LSP.2021.3053480
   Lucknavalai Karen, 2020, Advances in Visual Computing. 15th International Symposium, ISVC 2020. Proceedings. Lecture Notes in Computer Science (LNCS 12509), P224, DOI 10.1007/978-3-030-64556-4_18
   Lv F., 2018, P BMVC, V220, P4
   Ma K, 2015, IEEE T IMAGE PROCESS, V24, P3345, DOI 10.1109/TIP.2015.2442920
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Ojala T, 1996, PATTERN RECOGN, V29, P51, DOI 10.1016/0031-3203(95)00067-4
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Peyret R, 2018, NEUROCOMPUTING, V275, P83, DOI 10.1016/j.neucom.2017.05.010
   Raheja P., 2020, 2020 IEEE INT C MULT, P1
   Ramos, 2017, INT C BIG DAT TECHN, P38
   Ren WQ, 2019, IEEE T IMAGE PROCESS, V28, P4364, DOI 10.1109/TIP.2019.2910412
   Ren XT, 2020, IEEE T IMAGE PROCESS, V29, P5862, DOI 10.1109/TIP.2020.2984098
   Ren XT, 2018, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS.2018.8351427
   RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F
   Singh H, 2018, COMPUT ELECTR ENG, V70, P462, DOI 10.1016/j.compeleceng.2017.06.029
   Singh K, 2015, OPTIK, V126, P2619, DOI 10.1016/j.ijleo.2015.06.060
   Sun Feng, 2020, 2020 5th International Conference on Automation, Control and Robotics Engineering (CACRE), P570, DOI 10.1109/CACRE50138.2020.9229994
   Vaswani A, 2017, ADV NEUR IN, V30
   Vonikakis V., 2012, Proceedings of the 2012 IEEE International Conference on Imaging Systems and Techniques (IST), P158, DOI 10.1109/IST.2012.6295482
   Vonikakis V, 2013, MEAS SCI TECHNOL, V24, DOI 10.1088/0957-0233/24/7/074024
   Wang JY, 2019, IEEE INT CON MULTI, P1186, DOI 10.1109/ICME.2019.00207
   Wang SH, 2013, IEEE T IMAGE PROCESS, V22, P3538, DOI 10.1109/TIP.2013.2261309
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wei Cui, 2018, 2018 Photonics North (PN), DOI 10.1109/PN.2018.8438843
   Xu L, 2021, NEUROCOMPUTING, V421, P115
   Yadav G, 2014, 2014 INTERNATIONAL CONFERENCE ON ADVANCES IN COMPUTING, COMMUNICATIONS AND INFORMATICS (ICACCI), P2392, DOI 10.1109/ICACCI.2014.6968381
   Yang SC, 2021, IEEE INTERNET THINGS, V8, P5607, DOI 10.1109/JIOT.2020.3031390
   Yue HJ, 2017, IEEE T IMAGE PROCESS, V26, P3981, DOI 10.1109/TIP.2017.2703078
   Zhang Q, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P582, DOI 10.1145/3240508.3240595
   Zhang YH, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1632, DOI 10.1145/3343031.3350926
NR 57
TC 12
Z9 12
U1 1
U2 22
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2023
VL 39
IS 4
BP 1251
EP 1270
DI 10.1007/s00371-022-02402-8
EA FEB 2022
PG 20
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA C3OY5
UT WOS:000756283200001
DA 2024-07-18
ER

PT J
AU Yang, LJ
   Yang, Y
   Wang, CP
   Li, FX
AF Yang, Lijuan
   Yang, Ying
   Wang, Changpeng
   Li, Fuxiao
TI Rotation robust non-rigid point set registration with Bayesian student's
   t mixture model
SO VISUAL COMPUTER
LA English
DT Article
DE Non-rigid; Point set registration; Student's t mixture model (SMM);
   Robust; Bayesian; Variational inference
AB Aiming to improve the performance of non-rigid point set registration, this paper proposes a probabilistic method with student's t mixture model (SMM) under the Bayesian inference framework. In the proposed method, non-rigid point set registration is formulated as a probabilistic density estimation problem with SMM in Bayesian manner. In order to improve the robustness to rotation degradation, we consider the rotation transformation in modeling non-rigid displacement. Then, the hierarchical Bayesian model of non-rigid point set registration is constructed, and approximate posteriors of model parameters are derived by the variational Bayesian Expectation Maximization update rules, which can provide the uncertainty measurements of parameters. For those parameters without priors imposed, the updating formulae are obtained by directly maximizing variational lower bound. Finally, an empirical coarse-to-fine algorithm is designed to perform non-rigid point set registration process. The experimental results demonstrate that the proposed method can achieve higher matching performance compared with other several state-of-the-art registration methods.
C1 [Yang, Lijuan; Yang, Ying; Wang, Changpeng] Changan Univ, Coll Sci, Xian 710064, Shaanxi, Peoples R China.
   [Li, Fuxiao] Xian Univ Technol, Faulty Sci, Xian 710048, Shaanxi, Peoples R China.
C3 Chang'an University; Xi'an University of Technology
RP Yang, LJ (corresponding author), Changan Univ, Coll Sci, Xian 710064, Shaanxi, Peoples R China.
EM yanglijuan1987@163.com
FU Fundamental Research Funds for the Central Universities, CHD
   [300102129108, 300102129107]; National Nature Science Foundation of
   China [11801438, 12001057]; Natural Science Foundation of Shaanxi
   Province of China [2021JQ-218]
FX This work was supported by the Fundamental Research Funds for the
   Central Universities, CHD (300102129108, 300102129107), National Nature
   Science Foundation of China (11801438, 12001057) and Natural Science
   Foundation of Shaanxi Province of China (2021JQ-218).
CR Baldacchino T, 2017, MECH SYST SIGNAL PR, V85, P977, DOI 10.1016/j.ymssp.2016.08.045
   Beal Matthew James, 2003, Variational algorithms for approximate Bayesian inference
   BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791
   Bishop Christopher M., 2006, Pattern Recognition and Machine Learning, V4
   Bronstein AM, 2008, INT J COMPUT VISION, V78, P67, DOI 10.1007/s11263-007-0078-4
   Bronstein AM, 2008, MONOGR COMPUT SCI, P1, DOI 10.1007/978-0-387-73301-2_1
   Caetano TS, 2009, IEEE T PATTERN ANAL, V31, P1048, DOI 10.1109/TPAMI.2009.28
   Chui, 2001, THESIS YALE U NEW HA
   Chui HL, 2003, COMPUT VIS IMAGE UND, V89, P114, DOI 10.1016/S1077-3142(03)00009-2
   Goshtasby AA, 2005, 2-D AND 3-D IMAGE REGISTRATION FOR MEDICAL, REMOTE SENSING, AND INDUSTRIAL APPLICATIONS, P1
   He QQ, 2020, IEEE T FUZZY SYST, V28, P2784, DOI 10.1109/TFUZZ.2020.2974433
   Hirose O, 2021, IEEE T PATTERN ANAL, V43, P2269, DOI 10.1109/TPAMI.2020.2971687
   Impens C, 2003, AM MATH MON, V110, P730, DOI 10.2307/3647856
   Jian B, 2011, IEEE T PATTERN ANAL, V33, P1633, DOI 10.1109/TPAMI.2010.223
   Le Folgoc L, 2017, MED IMAGE ANAL, V36, P79, DOI 10.1016/j.media.2016.09.008
   LeMoigne J, 2011, IMAGE REGISTRATION FOR REMOTE SENSING, P1
   Liang L, 2022, IEEE GEOSCI REMOTE S, V19, DOI 10.1109/LGRS.2020.3047855
   Ma JY, 2015, IEEE T SIGNAL PROCES, V63, P1115, DOI 10.1109/TSP.2014.2388434
   Ma JY, 2014, IEEE T IMAGE PROCESS, V23, P1706, DOI 10.1109/TIP.2014.2307478
   Ma JY, 2013, PATTERN RECOGN, V46, P3519, DOI 10.1016/j.patcog.2013.05.017
   McLachlan G., 2000, WILEY SER PROB STAT, DOI 10.1002/0471721182
   Min Z, 2021, IEEE T AUTOM SCI ENG, V18, P1939, DOI 10.1109/TASE.2020.3027073
   Min Z, 2021, IEEE T AUTOM SCI ENG, V18, P471, DOI 10.1109/TASE.2020.3001207
   Myronenko A, 2010, IEEE T PATTERN ANAL, V32, P2262, DOI 10.1109/TPAMI.2010.46
   Oliveira FPM, 2014, COMPUT METHOD BIOMEC, V17, P73, DOI 10.1080/10255842.2012.670855
   Peel D, 2000, STAT COMPUT, V10, P339, DOI 10.1023/A:1008981510081
   Qu HB, 2017, IEEE T PATTERN ANAL, V39, P371, DOI 10.1109/TPAMI.2016.2545659
   Qu Han-Bing, 2015, Acta Automatica Sinica, V41, P1482, DOI 10.1016/S1874-1029(15)30001-X
   Sotiras A, 2013, IEEE T MED IMAGING, V32, P1153, DOI 10.1109/TMI.2013.2265603
   Subrahmanya N, 2013, INT J MACH LEARN CYB, V4, P609, DOI 10.1007/s13042-012-0121-9
   Svensén M, 2005, NEUROCOMPUTING, V64, P235, DOI 10.1016/j.neucom.2004.11.018
   Tam GKL, 2013, IEEE T VIS COMPUT GR, V19, P1199, DOI 10.1109/TVCG.2012.310
   Tondewad Priyanka S., 2020, Procedia Computer Science, V171, P2390, DOI 10.1016/j.procs.2020.04.259
   Yang GQ, 2022, VISUAL COMPUT, V38, P603, DOI 10.1007/s00371-020-02037-7
   [杨丽娟 Yang Lijuan], 2018, [西北工业大学学报, Journal of Northwestern Polytechnical University], V36, P942
   Yang LJ, 2016, J APPL REMOTE SENS, V10, DOI 10.1117/1.JRS.10.025014
   Zhang M, 2020, INT J ENERG RES, V44, P3762, DOI [10.1109/TASE.2020.3014420, 10.1002/er.5164]
   Zhang PP, 2017, NEUROCOMPUTING, V219, P455, DOI 10.1016/j.neucom.2016.09.058
   Zhou Z, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0084810
   Zhu H, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19051191
NR 40
TC 2
Z9 2
U1 2
U2 20
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2023
VL 39
IS 1
BP 367
EP 379
DI 10.1007/s00371-021-02335-8
EA JAN 2022
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 8X2JC
UT WOS:000739816600002
DA 2024-07-18
ER

PT J
AU Tian, JP
   Yuan, WJ
   Tu, YX
AF Tian, Jinpeng
   Yuan, Wenjie
   Tu, Yunxuan
TI Image compressed sensing using multi-scale residual generative
   adversarial network
SO VISUAL COMPUTER
LA English
DT Article
DE Compressed sensing; Generative adversarial network; Multi-scale residual
   block; Perceptual loss
AB Although faster and deeper convolutional networks have made breakthroughs in image compressed sensing (CS), there is still one central unsolved problem: how do we make the reconstructed image have more delicate texture details? The existing image CS algorithms are based on pixel loss to reconstruct the original image, which leads to the reconstructed image smoothness and lack of structural information. In order to solve the problem, this paper proposes MR-CSGAN: a multi-scale residual generative adversarial network (GAN) for image CS. MR-CSGAN combines multi-scale residual blocks by consisting of three different convolution kernels to exploit the image features fully. Furthermore, the perceptual loss is used as the objective optimization function instead of pixel loss to reconstruct a finer image. Experimental results show that the proposed MR-CSGAN can make the reconstructed image obtain more robust structural information and better visual effects than other state-of-the-art methods.
C1 [Tian, Jinpeng; Yuan, Wenjie; Tu, Yunxuan] Shanghai Univ, Dept Commun & Informat Engn, Shanghai, Peoples R China.
C3 Shanghai University
RP Tian, JP (corresponding author), Shanghai Univ, Dept Commun & Informat Engn, Shanghai, Peoples R China.
EM adaline@163.com
OI Tian, Jinpeng/0000-0001-6199-6946
FU National Natural Science Foundation of China [61871261]; Key Project of
   Science and Technology of Shanghai [19DZ1205802]
FX Supported by the National Natural Science Foundation of China (Grant No.
   61871261) and the Key Project of Science and Technology of Shanghai
   (Grant No. 19DZ1205802).
CR Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542
   Bevilacqua M, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.135
   Bora A, 2017, PR MACH LEARN RES, V70
   Candes E. J., 2006, PROC INT C MATH, V17, P1433, DOI DOI 10.4171/022-3/69
   Chen C, 2011, CONF REC ASILOMAR C, P1193, DOI 10.1109/ACSSC.2011.6190204
   Chen DD, 2017, PROC CVPR IEEE, P2770, DOI 10.1109/CVPR.2017.296
   Donoho DL, 2006, IEEE T INFORM THEORY, V52, P1289, DOI 10.1109/TIT.2006.871582
   Duarte MF, 2008, IEEE SIGNAL PROC MAG, V25, P83, DOI 10.1109/MSP.2007.914730
   Fan Q., 2019, PROC CVPR IEEE
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Iliadis M, 2018, DIGIT SIGNAL PROCESS, V72, P9, DOI 10.1016/j.dsp.2017.09.010
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kabkab M, 2018, AAAI CONF ARTIF INTE, P2297
   Kingma D.P., 2014, ACS SYM SER
   KULKARNI K, 2016, PROC CVPR IEEE, P449, DOI DOI 10.1109/CVPR.2016.55
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Li Chengbo., 2009, CAAM Report
   Li JC, 2018, LECT NOTES COMPUT SC, V11212, P527, DOI 10.1007/978-3-030-01237-3_32
   Liu YP, 2017, IEEE T MED IMAGING, V36, P2148, DOI 10.1109/TMI.2017.2717502
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Metzler CA, 2016, IEEE T INFORM THEORY, V62, P5117, DOI 10.1109/TIT.2016.2556683
   Qiao M, 2020, APL PHOTONICS, V5, DOI 10.1063/1.5140721
   Radford A., 2015, ARXIV
   Shen W, 2017, PROC CVPR IEEE, P1225, DOI 10.1109/CVPR.2017.135
   Shi WZ, 2020, IEEE T IMAGE PROCESS, V29, P375, DOI 10.1109/TIP.2019.2928136
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Timofte R, 2017, IEEE COMPUT SOC CONF, P1110, DOI 10.1109/CVPRW.2017.149
   Wang ZJ, 2021, PROC CVPR IEEE, P2083, DOI 10.1109/CVPR46437.2021.00212
   Yeh RA, 2017, PROC CVPR IEEE, P6882, DOI 10.1109/CVPR.2017.728
   Yuan X, 2021, IEEE SIGNAL PROC MAG, V38, P65, DOI 10.1109/MSP.2020.3023869
   Zeyde R., 2012, INT C CURV SURF, P711, DOI DOI 10.1007/978-3-642-27413-8_47
   Zha ZY, 2022, IEEE T NEUR NET LEAR, V33, P4451, DOI 10.1109/TNNLS.2021.3057439
   Zha ZY, 2020, IEEE IMAGE PROC, P983, DOI 10.1109/ICIP40778.2020.9190707
   Zha ZY, 2020, IEEE T IMAGE PROCESS, V29, P8960, DOI 10.1109/TIP.2020.3021291
   Zha ZY, 2018, VISUAL COMPUT, V34, P117, DOI 10.1007/s00371-016-1318-9
   Zhang J, 2018, PROC CVPR IEEE, P1828, DOI 10.1109/CVPR.2018.00196
   Zhang J, 2014, IEEE T IMAGE PROCESS, V23, P3336, DOI 10.1109/TIP.2014.2323127
NR 37
TC 4
Z9 5
U1 1
U2 26
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2022
VL 38
IS 12
BP 4193
EP 4202
DI 10.1007/s00371-021-02288-y
EA SEP 2021
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 7B7SH
UT WOS:000692956800003
DA 2024-07-18
ER

PT J
AU Gautam, A
   Singh, S
AF Gautam, Aayushi
   Singh, Sukhwinder
TI Neural style transfer combined with EfficientDet for thermal
   surveillance
SO VISUAL COMPUTER
LA English
DT Article
DE Infrared imaging; Super-resolution; EfficientDet; Style transfer; Object
   detection
ID IMAGE SUPERRESOLUTION
AB Hindrance caused while performing object detection during border or perimeter surveillance at night due to low resolution and atmospheric noise inside infrared frames, demands for a sophisticated detection framework. To deal with the challenges, we present a novel deep learning framework dedicated completely for automated thermal surveillance. The framework is a composition of two major modules. A novel style transferred enhanced image module STEIM enhances the resolution of IR input frame by utilizing the contextual information and exploiting the local and global features to preserve the high frequency details. The enhanced IR frame is further fed inside the fine-tuned EfficientDet module EDM that comprises weighted two-way feature network. It has an advantage of effectual multi-scale feature fusion contributing to accurate and efficient object detection. We have obtained mAP of 92.83% with 94.07% accuracy running at 95fps on FLIR test frames and mAP of 87.51% with 88.53% accuracy running at 89fps on OTCVBS test frames. Experimental results on two benchmark datasets FLIR and OTCBVS completely demonstrate suitability of the proposed framework for automatic surveillance using thermal imaging systems.
C1 [Gautam, Aayushi; Singh, Sukhwinder] Punjab Engn Coll, Dept Elect & Commun Engn, Sect 12, Chandigarh, India.
C3 Punjab Engineering College (Deemed University)
RP Gautam, A (corresponding author), Punjab Engn Coll, Dept Elect & Commun Engn, Sect 12, Chandigarh, India.
EM aayushi4march@gmail.com; sukhwindersingh@pec.edu.in
RI Singh, Sukhwinder/JGL-7957-2023
OI GAUTAM, AAYUSHI/0000-0001-6170-2557
CR Abbass MY, 2021, VISUAL COMPUT, V37, P831, DOI 10.1007/s00371-020-01833-5
   Adelson E. H., 1984, RCA engineer, V29, P33, DOI 10.1.1.59.9419.
   Ahmad N, 2022, VISUAL COMPUT, V38, P2751, DOI 10.1007/s00371-021-02153-y
   Algarni AD, 2020, MULTIMED TOOLS APPL, V79, P13403, DOI 10.1007/s11042-020-08616-z
   [Anonymous], 2013, 9 INT C COMP INF TEC
   Ashiba HI, 2018, WIRELESS PERS COMMUN, V99, P619, DOI 10.1007/s11277-017-4958-9
   Bhattacharya S, 2011, AUGMENT VIS REAL, V1, P221, DOI 10.1007/978-3-642-11568-4_10
   Biswas SK, 2017, IEEE T IMAGE PROCESS, V26, P4229, DOI 10.1109/TIP.2017.2705426
   Cai LL, 2017, IEEE IMAGE PROC, P2299, DOI 10.1109/ICIP.2017.8296692
   Chen YF, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10030809
   Cheng G, 2016, IEEE T GEOSCI REMOTE, V54, P7405, DOI 10.1109/TGRS.2016.2601622
   Dai XR, 2021, APPL INTELL, V51, P1244, DOI 10.1007/s10489-020-01882-2
   Dai Y, 2020, IMAGE VISION COMPUT, V93, DOI 10.1016/j.imavis.2019.10.007
   Davis JW, 2007, COMPUT VIS IMAGE UND, V106, P162, DOI 10.1016/j.cviu.2006.06.010
   Deng X, 2018, IEEE SIGNAL PROC LET, V25, P571, DOI 10.1109/LSP.2018.2805809
   Devaguptapu C, 2019, IEEE COMPUT SOC CONF, P1029, DOI 10.1109/CVPRW.2019.00135
   Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281
   Gatys L., 2016, Journal of Vision, V16, P326, DOI DOI 10.1167/16.12.326
   IRANI M, 1991, CVGIP-GRAPH MODEL IM, V53, P231, DOI 10.1016/1049-9652(91)90045-L
   Jagatheeswari P., 2009, Proceedings of the 2009 International Conference on Advances in Computing, Control, & Telecommunication Technologies (ACT 2009), P111, DOI 10.1109/ACT.2009.37
   Jing Gong, 2020, 2020 IEEE International Conference on Power, Intelligent Computing and Systems (ICPICS), P253, DOI 10.1109/ICPICS50287.2020.9201995
   Jingwei Xin, 2020, Pattern Recognition and Computer Vision. Third Chinese Conference, PRCV 2020. Proceedings. Lecture Notes in Computer Science (LNCS 12305), P256, DOI 10.1007/978-3-030-60633-6_21
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Li S, 2018, INFRARED PHYS TECHN, V90, P164, DOI 10.1016/j.infrared.2018.03.010
   Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151
   Lin TY, 2020, IEEE T PATTERN ANAL, V42, P318, DOI 10.1109/TPAMI.2018.2858826
   Lisin D, 2005, 2005 IEEE COMP SOC C, P47, DOI [10.1109/CVPR.2005.433, 10/c3854f, DOI 10.1109/CVPR.2005.433]
   Liu F, 2018, INFRARED PHYS TECHN, V90, P146, DOI 10.1016/j.infrared.2018.03.008
   Liu J, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2202, DOI 10.1145/3394171.3413696
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Mahapatra D, 2019, COMPUT MED IMAG GRAP, V71, P30, DOI 10.1016/j.compmedimag.2018.10.005
   Munir F., 2020, THERMAL OBJECT DETEC
   Nasrollahi K, 2014, MACH VISION APPL, V25, P1423, DOI 10.1007/s00138-014-0623-4
   Nguyen N, 2001, IEEE T IMAGE PROCESS, V10, P573, DOI 10.1109/83.913592
   Rodin CD, 2018, IEEE IJCNN
   Shakya Amit Kumar, 2020, 2020 International Conference on Advances in Computing, Communication & Materials (ICACCM), P65, DOI 10.1109/ICACCM50413.2020.9213016
   Tan MX, 2019, PR MACH LEARN RES, V97
   Tang HS, 2018, J VIS COMMUN IMAGE R, V51, P162, DOI 10.1016/j.jvcir.2018.01.011
   Tekalp A. M., 1992, ICASSP-92: 1992 IEEE International Conference on Acoustics, Speech and Signal Processing (Cat. No.92CH3103-9), P169, DOI 10.1109/ICASSP.1992.226249
   Tian CW, 2020, KNOWL-BASED SYST, V205, DOI 10.1016/j.knosys.2020.106235
   Tong T, 2017, IEEE I CONF COMP VIS, P4809, DOI 10.1109/ICCV.2017.514
   Wang J, 2013, J COMPUT, V8, P1050, DOI 10.4304/jcp.8.4.1050-1057
   Wang KX, 2019, IEEE I CONF COMP VIS, P9196, DOI 10.1109/ICCV.2019.00929
   Wang XP, 2018, IDEAS HIST MOD CHINA, V19, P1, DOI 10.1163/9789004385580_002
   Yang TJ, 2022, VISUAL COMPUT, V38, P2871, DOI 10.1007/s00371-021-02161-y
   Yue LW, 2016, SIGNAL PROCESS, V128, P389, DOI 10.1016/j.sigpro.2016.05.002
   Yunchu Yang, 2020, Pattern Recognition and Computer Vision. Third Chinese Conference, PRCV 2020. Proceedings. Lecture Notes in Computer Science (LNCS 12305), P219, DOI 10.1007/978-3-030-60633-6_18
   Zhang H, 2020, IEEE IMAGE PROC, P276, DOI [10.1109/ICIP40778.2020.9191080, 10.1109/icip40778.2020.9191080]
   Zhang HZ, 2018, MULTIMED TOOLS APPL, V77, P26657, DOI 10.1007/s11042-018-5883-y
   Zhang XS, 2022, IEEE GEOSCI REMOTE S, V19, DOI 10.1109/LGRS.2020.3031398
   Zhao Y, 2015, INFRARED PHYS TECHN, V71, P506, DOI 10.1016/j.infrared.2015.06.017
NR 51
TC 10
Z9 10
U1 0
U2 20
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2022
VL 38
IS 12
BP 4111
EP 4127
DI 10.1007/s00371-021-02284-2
EA AUG 2021
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 7B7SH
UT WOS:000686026700002
DA 2024-07-18
ER

PT J
AU Guo, H
   Liu, YF
   Yang, DD
   Zhao, JY
AF Guo, Hai
   Liu, Yifan
   Yang, Doudou
   Zhao, Jingying
TI Offline handwritten Tai Le character recognition using ensemble deep
   learning
SO VISUAL COMPUTER
LA English
DT Article
DE Handwritten recognition; Tai Le recognition; Deep convolution neural
   network; Ensemble learning
ID CHINESE CHARACTERS; NEURAL-NETWORKS
AB Handwriting recognition is an important area in pattern recognition. For many years, Tai Le has been widely used in Southwest China and Southeast Asia, which makes it of great interest for recognition research. The characteristics of the highly similar characters in Tai Le, such as its large proportion of similar characters and the randomness of its writing, bring great challenges to the task of recognition. In this paper, a method based on ensemble deep learning for offline handwritten Tai Le characters is proposed. First, the handwritten Tai Le character dataset SDH2019.2 was constructed and preprocessed. Then, an ensemble deep convolutional neural network (EDCNN) model was constructed by using a stacking strategy. Thirty deep neural network (DNN) and logistic regression algorithms were integrated into a strong Tai Le classifier by stacking. Experiments showed that the proposed model is competitive with the base DNN model and other ensemble models. The results indicate that the performance of Tai Le recognition by the stacking ensemble-based deep neural network model is high, with an accuracy of 98.85%. Additionally, its precision, recall and F1-score of 98.87%, 98.85% and 98.85%, respectively, are superior to those of other classic neural network models. To verify the general applicability of EDCNN, its effectiveness was also verified by recognizing MNIST handwritten digits and Devanagari handwritten characters.
C1 [Guo, Hai; Liu, Yifan; Yang, Doudou; Zhao, Jingying] Dalian Minzu Univ, SEAC Key Lab Big Data Appl Technol, Dalian 116600, Peoples R China.
   [Zhao, Jingying] Dalian Univ Technol, Fac Elect Informat & Elect Engn, Dalian 116024, Peoples R China.
C3 Dalian Minzu University; Dalian University of Technology
RP Guo, H (corresponding author), Dalian Minzu Univ, SEAC Key Lab Big Data Appl Technol, Dalian 116600, Peoples R China.
EM guohai@dlnu.edu.cn
OI Guo, Hai/0000-0002-6380-1484
FU National Language Commission research project [YB135-116]
FX This research was supported by the National Language Commission research
   project (No. YB135-116), and all support is gratefully acknowledged.
CR Acharya J, 2015, 2015 INTERNATIONAL CONFERENCE ON COMPUTING, NETWORKING AND COMMUNICATIONS (ICNC), P1, DOI 10.1109/ICCNC.2015.7069284
   Akyol K, 2020, EXPERT SYST APPL, V148, DOI 10.1016/j.eswa.2020.113239
   An N, 2020, J BIOMED INFORM, V105, DOI 10.1016/j.jbi.2020.103411
   Chan TH, 2015, IEEE T IMAGE PROCESS, V24, P5017, DOI 10.1109/TIP.2015.2475625
   Ciresan D, 2012, PROC CVPR IEEE, P3642, DOI 10.1109/CVPR.2012.6248110
   Ciresan D, 2015, IEEE IJCNN
   Deore SP, 2020, SADHANA-ACAD P ENG S, V45, DOI 10.1007/s12046-020-01484-1
   Gharde Sanjay S., 2016, 2016 International Conference on Global Trends in Signal Processing, Information Computing and Communication (ICGTSPICC). Proceedings, P255, DOI 10.1109/ICGTSPICC.2016.7955307
   Ghosh R, 2019, PATTERN RECOGN, V92, P203, DOI 10.1016/j.patcog.2019.03.030
   Goodfellow I. J., 2015, 3 INT C LEARNING REP
   Gupta A, 2019, PATTERN RECOGN LETT, V128, P318, DOI 10.1016/j.patrec.2019.09.019
   Han J, 2020, FLOW MEAS INSTRUM, V73, DOI 10.1016/j.flowmeasinst.2020.101748
   He ZJ, 2008, PROCEEDINGS OF 2008 INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND CYBERNETICS, VOLS 1-7, P3451, DOI 10.1109/ICMLC.2008.4621001
   Huang YF, 2020, EXPERT SYST APPL, V159, DOI 10.1016/j.eswa.2020.113584
   Inunganbi S, 2021, VISUAL COMPUT, V37, P291, DOI 10.1007/s00371-020-01799-4
   Jain J., 2012, ADV COMPUTER SCI INF, P611
   Kaur RP, 2021, VISUAL COMPUT, V37, P1637, DOI 10.1007/s00371-020-01927-0
   Khan MA, 2018, IET IMAGE PROCESS, V12, P200, DOI 10.1049/iet-ipr.2017.0368
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li XL, 2017, PROCEEDINGS OF 2017 IEEE 2ND INFORMATION TECHNOLOGY, NETWORKING, ELECTRONIC AND AUTOMATION CONTROL CONFERENCE (ITNEC), P837, DOI 10.1109/ITNEC.2017.8284852
   Li ZX, 2019, RELIAB ENG SYST SAFE, V184, P110, DOI 10.1016/j.ress.2017.12.016
   Liu M, 2020, TOXICOL LETT, V332, P88, DOI 10.1016/j.toxlet.2020.07.003
   Reddy RVK, 2018, PROCEEDINGS OF THE 2018 SECOND INTERNATIONAL CONFERENCE ON INTELLIGENT COMPUTING AND CONTROL SYSTEMS (ICICCS), P45, DOI 10.1109/ICCONS.2018.8662969
   Ren HQ, 2019, PATTERN RECOGN, V93, P179, DOI 10.1016/j.patcog.2019.04.015
   Roohi S, 2017, IRAN CONF MACH, P247, DOI 10.1109/IranianMVIP.2017.8342359
   Rusk N, 2016, NAT METHODS, V13, P35, DOI 10.1038/nmeth.3707
   Saha C., 2019, 2019 INT C ELECT COM, P1
   Sarkhel R, 2017, PATTERN RECOGN, V71, P78, DOI 10.1016/j.patcog.2017.05.022
   Sarkhel R, 2016, PATTERN RECOGN, V58, P172, DOI 10.1016/j.patcog.2016.04.010
   Sarkhel R, 2015, 2015 IEEE 2ND INTERNATIONAL CONFERENCE ON RECENT TRENDS IN INFORMATION SYSTEMS (RETIS), P325, DOI 10.1109/ReTIS.2015.7232899
   Shan SB, 2019, IEEE ACCESS, V7, P88093, DOI 10.1109/ACCESS.2019.2925740
   Sun SL, 2020, ADV ENG INFORM, V46, DOI 10.1016/j.aei.2020.101160
   Tan M., 2019, ARXIV190709595
   Wang CY, 2020, IEEE COMPUT SOC CONF, P1571, DOI 10.1109/CVPRW50498.2020.00203
   Wang JY, 2023, INTERACT LEARN ENVIR, V31, P836, DOI [10.1080/10494820.2020.1813178, 10.1109/TASE.2020.2976560]
   Wang YF, 2020, J NEUROSCI METH, V343, DOI 10.1016/j.jneumeth.2020.108840
   Wu PJ, 2020, J TRANSP SAF SECUR, V12, P1128, DOI 10.1080/19439962.2019.1579288
   Xu CY, 2016, IEEE T CIRC SYST VID, V26, P2273, DOI 10.1109/TCSVT.2015.2477937
   Xu XY, 2011, PATTERN RECOGN LETT, V32, P956, DOI 10.1016/j.patrec.2011.01.021
   Xu Z, 2016, IEEE T IMAGE PROCESS, V25, P973, DOI 10.1109/TIP.2015.2509422
   Zhang R, 2007, INT J PATTERN RECOGN, V21, P1035, DOI 10.1142/S0218001407005776
   Zhang XY, 2018, IEEE T PATTERN ANAL, V40, P849, DOI 10.1109/TPAMI.2017.2695539
   Zhang XY, 2017, PATTERN RECOGN, V61, P348, DOI 10.1016/j.patcog.2016.08.005
   Zhu HY, 2016, IEEE T MULTIMEDIA, V18, P1516, DOI 10.1109/TMM.2016.2571629
NR 44
TC 5
Z9 5
U1 2
U2 17
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2022
VL 38
IS 11
BP 3897
EP 3910
DI 10.1007/s00371-021-02230-2
EA JUL 2021
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 5X0BS
UT WOS:000675292800001
DA 2024-07-18
ER

PT J
AU Chu, HZ
   Le, C
   Wang, RQ
   Li, X
   Ma, HM
AF Chu, Huazhen
   Le, Chao
   Wang, Rongquan
   Li, Xi
   Ma, Huimin
TI Learning representative viewpoints in 3D shape recognition
SO VISUAL COMPUTER
LA English
DT Article
DE 3D shape recognition; View structure; Representative viewpoints; Deep
   neural network
ID POINT CLOUDS; NETWORK
AB Adopting many viewpoints and mining the relationship between them, 3D shape recognition inferring the object's category from 2D rendered images has proven effective. However, using a limited number of general representative viewpoints to form a reasonable expression of the object is a task with both practical and theoretical significance. This paper proposes a multi-view CNN architecture with independent viewpoint feature extraction and the unity of importance weights, which can dramatically decrease the number of viewpoints by learning the representative ones. First, the view-based and independent view features are extracted by a deep neural network. Second, the network automatically learns relativity between these viewpoints and outputs the importance weights of views. Finally, view features are aggregated to predict the category of objects. Through iterative learning of these critical weights in instances, global representative viewpoints are selected. We assess our method on two challenging datasets, ModelNet and ShapeNet. Rigorous experiments show that our strategy is competitive with the latest method using only six viewpoints and RGB information as input. Meanwhile, our approach also achieves state-of-the-art performance by using 20 viewpoints as input. Specifically, the proposed approach achieves 99.34% and 97.49% accuracy on the ModelNet10 and ModelNet40, and 80.0% mAP on ShapeNet.
C1 [Chu, Huazhen; Wang, Rongquan; Ma, Huimin] Univ Sci & Technol, Beijing, Peoples R China.
   [Le, Chao; Li, Xi] Tsinghua Univ, Beijing, Peoples R China.
C3 University of Science & Technology Beijing; Tsinghua University
RP Ma, HM (corresponding author), Univ Sci & Technol, Beijing, Peoples R China.
EM chuhuazhen@xs.ustb.edu.cn; chao.le@smartsenstech.com;
   rongquanwang@ustb.edu.cn; lixi16@mails.tsinghua.edu.cn;
   mhmpub@ustb.edu.cn
RI 马, 会民/AAM-8054-2021
OI 马, 会民/0000-0001-6155-9076
FU National Natural Science Foundation of China [U20B2062]; Beijing
   Municipal Science & Technology Project [Z1911000 07419001]; Beijing
   National Research Center for Information Science and Technology; key
   Laboratory of Opto-Electronic Information Processing, CAS [JGA202004027]
FX This work was supported by the National Natural Science Foundation of
   China (No. U20B2062), the Beijing Municipal Science & Technology Project
   (No. Z1911000 07419001), the Beijing National Research Center for
   Information Science and Technology, and the key Laboratory of
   Opto-Electronic Information Processing, CAS (No. JGA202004027).
CR [Anonymous], 2015, PROC CVPR IEEE, DOI [DOI 10.1109/CVPR.2015.7298801, 10.1109/CVPR.2015.7298801]
   Bai S, 2017, IEEE T MULTIMEDIA, V19, P1257, DOI 10.1109/TMM.2017.2652071
   Bai S, 2016, PROC CVPR IEEE, P5023, DOI 10.1109/CVPR.2016.543
   Brock A., ARXIV PREPRINT ARXIV
   Bu SH, 2017, NEUROCOMPUTING, V259, P183, DOI 10.1016/j.neucom.2016.06.088
   Chatfield K., ARXIV PREPRINT ARXIV
   Chen X., 2017, PROC CVPR IEEE, V1, P3, DOI DOI 10.1109/CVPR.2017.691
   Cheraghian A, 2019, IEEE WINT CONF APPL, P1194, DOI 10.1109/WACV.2019.00132
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Esteves C, 2019, IEEE I CONF COMP VIS, P1568, DOI 10.1109/ICCV.2019.00165
   Feng YF, 2019, AAAI CONF ARTIF INTE, P3558
   Guo Y, 2018, VISUAL COMPUT, V34, P1325, DOI 10.1007/s00371-017-1416-3
   Han ZZ, 2019, IEEE T IMAGE PROCESS, V28, P3986, DOI 10.1109/TIP.2019.2904460
   Han ZZ, 2019, IEEE T IMAGE PROCESS, V28, P658, DOI 10.1109/TIP.2018.2868426
   Jiang JW, 2019, AAAI CONF ARTIF INTE, P8513
   Johns E, 2016, PROC CVPR IEEE, P3813, DOI 10.1109/CVPR.2016.414
   Kanezaki A, 2018, PROC CVPR IEEE, P5010, DOI 10.1109/CVPR.2018.00526
   Klokov R, 2017, IEEE I CONF COMP VIS, P863, DOI 10.1109/ICCV.2017.99
   Kong C, 2017, PROC CVPR IEEE, P5603, DOI 10.1109/CVPR.2017.594
   Kumawat S, 2019, PROC CVPR IEEE, P4898, DOI 10.1109/CVPR.2019.00504
   Li JX, 2018, PROC CVPR IEEE, P9397, DOI 10.1109/CVPR.2018.00979
   Liu XH, 2019, AAAI CONF ARTIF INTE, P8778
   Liu YC, 2019, PROC CVPR IEEE, P8887, DOI 10.1109/CVPR.2019.00910
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Maturana D, 2015, IEEE INT C INT ROBOT, P922, DOI 10.1109/IROS.2015.7353481
   Murthy J. Krishna, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P724, DOI 10.1109/ICRA.2017.7989089
   Qi CR, 2016, PROC CVPR IEEE, P5648, DOI 10.1109/CVPR.2016.609
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Savva M., 2016, P EUR WORKSH 3D OBJ, P89, DOI DOI 10.2312/3DOR.20161092
   Soltani AA, 2017, PROC CVPR IEEE, P2511, DOI 10.1109/CVPR.2017.269
   Su H, 2015, IEEE I CONF COMP VIS, P945, DOI 10.1109/ICCV.2015.114
   Szegedy C, 2014, Arxiv, DOI [arXiv:1312.6199, DOI 10.1109/CVPR.2015.7298594]
   Wang C., ARXIV PREPRINT ARXIV
   Wei X, 2020, PROC CVPR IEEE, P1847, DOI 10.1109/CVPR42600.2020.00192
   Yavartanoo M, 2019, LECT NOTES COMPUT SC, V11365, P691, DOI 10.1007/978-3-030-20873-8_44
   Yu T, 2018, LECT NOTES COMPUT SC, V11205, P191, DOI 10.1007/978-3-030-01246-5_12
   Zanuttigh P, 2017, IEEE IMAGE PROC, P3615, DOI 10.1109/ICIP.2017.8296956
   Zhang, ARXIV PREPRINT ARXIV
   Zhang K., ARXIV PREPRINT ARXIV
NR 39
TC 1
Z9 1
U1 1
U2 13
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2022
VL 38
IS 11
BP 3703
EP 3718
DI 10.1007/s00371-021-02203-5
EA JUL 2021
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 5X0BS
UT WOS:000669180100003
DA 2024-07-18
ER

PT J
AU Dixit, A
   Nanda, A
AF Dixit, Asmita
   Nanda, Aparajita
TI An improved whale optimization algorithm-based radial neural network for
   multi-grade brain tumor classification
SO VISUAL COMPUTER
LA English
DT Article
DE Brain tumor; Radial basis neural network; Segmentation; Feature
   extraction; Optimization
ID IMAGES; CNN
AB Nowadays, brain tumor has become deadly disease. Therefore, early diagnosis can prevent the severity of the next stage. The existing methods are inefficient in predicting the optimal cluster center (CC) in radial basis neural network (RBNN), which leads to inaccuracy. To overcome this issue, we proposed an improved whale optimization algorithm (IWOA) of RBNN to maximize the convergence speed and accuracy. Initially, the MRI input images are fed to preprocessing steps. Then, the image segmentation is carried out by fuzzy-c means (FCM) clustering for identifying the tumor region. These tumor and non-tumor images undergo a feature extraction process utilizing principle component analysis (PCA), mean, entropy, and wavelet transform. The obtained feature vector is fed to the RBNN layer. It needs an optimal CC, which can be considered by using the newly proposed IWOA. RBNN classifies the abnormality of the brain into brain tumor, inflammatory disease, stroke, and degenerative. Its performance is tested on three datasets. Based on the report of the evaluation, the proposed FCM-IWOA-RBNN gives high accuracy.
C1 [Dixit, Asmita; Nanda, Aparajita] Jaypee Inst Informat Technol, Dept CS & IT, Noida, India.
C3 Jaypee Institute of Information Technology (JIIT)
RP Dixit, A (corresponding author), Jaypee Inst Informat Technol, Dept CS & IT, Noida, India.
EM phd.asmitadixit03@gmail.com
CR Afshar P, 2019, INT CONF ACOUST SPEE, P1368, DOI 10.1109/ICASSP.2019.8683759
   Amin J, 2019, COMPUT METH PROG BIO, V177, P69, DOI 10.1016/j.cmpb.2019.05.015
   Anaraki AK, 2019, BIOCYBERN BIOMED ENG, V39, P63, DOI 10.1016/j.bbe.2018.10.004
   Arunkumar N, 2019, SOFT COMPUT, V23, P9083, DOI 10.1007/s00500-018-3618-7
   Bian S, 2018, NAT METHODS, V15, P631, DOI 10.1038/s41592-018-0070-7
   Chahal PK, 2020, MULTIMED TOOLS APPL, V79, P21771, DOI 10.1007/s11042-020-08898-3
   Chang J, 2019, J VIS COMMUN IMAGE R, V58, P316, DOI 10.1016/j.jvcir.2018.11.047
   Ge CJ, 2020, IEEE ACCESS, V8, P22560, DOI 10.1109/ACCESS.2020.2969805
   Ghassemi N, 2020, BIOMED SIGNAL PROCES, V57, DOI 10.1016/j.bspc.2019.101678
   Iqbal S, 2018, BIOMED ENG LETT, V8, P5, DOI 10.1007/s13534-017-0050-3
   Kanmani P, 2018, J MED SYST, V42, DOI 10.1007/s10916-018-0915-8
   Kaplan K, 2020, MED HYPOTHESES, V139, DOI 10.1016/j.mehy.2020.109696
   Kuanar S, 2019, IEEE IMAGE PROC, P1351, DOI [10.1109/ICIP.2019.8803037, 10.1109/icip.2019.8803037]
   Kuanar S, 2019, CIRC SYST SIGNAL PR, V38, P5081, DOI 10.1007/s00034-019-01110-4
   Kuanar S, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P2576, DOI 10.1109/ICASSP.2018.8462243
   Kumar S, 2020, BIOCYBERN BIOMED ENG, V40, P1190, DOI 10.1016/j.bbe.2020.05.009
   Li HC, 2019, COMPUT BIOL MED, V108, P150, DOI 10.1016/j.compbiomed.2019.03.014
   LillyMaheepa P., 2020, 2020 IEEE 5th International Conference on Computing Communication and Automation (ICCCA), P708, DOI 10.1109/ICCCA49541.2020.9250923
   Mohan G, 2018, BIOMED SIGNAL PROCES, V39, P139, DOI 10.1016/j.bspc.2017.07.007
   Muhammad K, 2021, IEEE T NEUR NET LEAR, V32, P507, DOI 10.1109/TNNLS.2020.2995800
   Rehman ZU, 2019, EXPERT SYST APPL, V118, P598, DOI 10.1016/j.eswa.2018.10.040
   Roy Sudipta, 2018, Informatics in Medicine Unlocked, V13, P139, DOI 10.1016/j.imu.2018.02.006
   Sajjad M, 2019, J COMPUT SCI-NETH, V30, P174, DOI 10.1016/j.jocs.2018.12.003
   Selvapandian A, 2018, COMPUT METH PROG BIO, V166, P33, DOI 10.1016/j.cmpb.2018.09.006
   Shakeel PM, 2019, IEEE ACCESS, V7, P5577, DOI 10.1109/ACCESS.2018.2883957
   Sultan HH, 2019, IEEE ACCESS, V7, P69215, DOI 10.1109/ACCESS.2019.2919122
   Swati ZNK, 2019, COMPUT MED IMAG GRAP, V75, P34, DOI 10.1016/j.compmedimag.2019.05.001
   Thillaikkarasi R, 2019, J MED SYST, V43, DOI 10.1007/s10916-019-1223-7
   Thivya Roopini I., 2018, Computational Signal Processing and Analysis. Select Proceedings of ICNETS2: LNEE 490, P297, DOI 10.1007/978-981-10-8354-9_27
   Tyagi V., 2019, 2019 INT C ISS CHALL, V1, P1, DOI [10.1109/ICICT46931.2019.8977658, DOI 10.1109/ICICT46931.2019.8977658]
   Wang Y, 2019, COMPUT MED IMAG GRAP, V75, P56, DOI 10.1016/j.compmedimag.2019.04.001
NR 31
TC 18
Z9 18
U1 0
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2022
VL 38
IS 11
BP 3525
EP 3540
DI 10.1007/s00371-021-02176-5
EA JUL 2021
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 5X0BS
UT WOS:000669180100002
DA 2024-07-18
ER

PT J
AU Gajjar, R
   Gajjar, N
   Thakor, VJ
   Patel, NP
   Ruparelia, S
AF Gajjar, Ruchi
   Gajjar, Nagendra
   Thakor, Vaibhavkumar Jigneshkumar
   Patel, Nikhilkumar Pareshbhai
   Ruparelia, Stavan
TI Real-time detection and identification of plant leaf diseases using
   convolutional neural networks on an embedded platform
SO VISUAL COMPUTER
LA English
DT Article
DE Convolutional neural network; Crop diseases identification; Embedded
   Platform; Machine Learning; Precision Agriculture; Real-time detection
ID DEEP; LEAVES; MAIZE
AB Early identification of crop disease can aid the farmers to take timely precautions and countermeasures for its removal. In this paper, a real-time system to identify the type of disease present in a crop based on leaf images using machine learning is proposed. A deep convolutional neural network architecture is proposed to classify the crop disease, and a single shot detector is used for identification and localization of the leaf. These models are deployed on an embedded hardware, Nvidia Jetson TX1, for real-time in-field plant disease detection and identification. The disease classification accuracy achieved is around 96.88%, and the classification results are compared with existing convolutional neural network architectures. Also, the high success rate of the proposed system in the actual field test makes the proposed system a completely deployable system.
C1 [Gajjar, Ruchi; Gajjar, Nagendra; Thakor, Vaibhavkumar Jigneshkumar; Patel, Nikhilkumar Pareshbhai; Ruparelia, Stavan] Nirma Univ, Inst Technol, Elect & Commun Engn Dept, Ahmadabad, Gujarat, India.
C3 Nirma University
RP Gajjar, R (corresponding author), Nirma Univ, Inst Technol, Elect & Commun Engn Dept, Ahmadabad, Gujarat, India.
EM ruchi.gajjar@nirmauni.ac.in; nagendra.gajjar@nirmauni.ac.in;
   vaibhavthakor2@gmail.com; nikpatel8272@gmail.com;
   stavanrupareliya7878@gmail.com
RI Gajjar, Ruchi/ABB-2557-2020
OI Gajjar, Ruchi/0000-0002-8136-6084; Gajjar, Nagendra/0000-0003-1855-1364
CR Adedigba A.P., 2019, P IEEE AFRICON ACCR, P1
   Amara J., 2017, DATENBANKSYSTEME BUS
   Angelique J., 2008, CORN FOLIAR DIS IDEN
   [Anonymous], 2017, 2017 21 INT COMP SCI, DOI [10.1109/ICSEC.2017.8443919, DOI 10.1109/ICSEC.2017.8443919]
   Arivazhagan S., 2013, Agricultural Engineering International: CIGR Journal, V15, P211
   Chaerle L, 2000, TRENDS PLANT SCI, V5, P495, DOI 10.1016/S1360-1385(00)01781-7
   Clevert D., 2016, ARXIV151107289
   Cséfalvay L, 2009, EUR J PLANT PATHOL, V125, P291, DOI 10.1007/s10658-009-9482-7
   Dhami NB, 2015, J MAIZE RES DEV, V1, P71, DOI 10.3126/jmrd.v1i1.14245
   Durmus H, 2017, INT CONF AGRO-GEOINF, P46
   Fang Y, 2015, BIOSENSORS-BASEL, V5, P537, DOI 10.3390/bios5030537
   Ferentinos KP, 2018, COMPUT ELECTRON AGR, V145, P311, DOI 10.1016/j.compag.2018.01.009
   Fuentes A, 2017, SENSORS-BASEL, V17, DOI 10.3390/s17092022
   Fujita E, 2016, 2016 15TH IEEE INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND APPLICATIONS (ICMLA 2016), P989, DOI [10.1109/ICMLA.2016.56, 10.1109/ICMLA.2016.0178]
   Geetharamani G, 2019, COMPUT ELECTR ENG, V76, P323, DOI 10.1016/j.compeleceng.2019.04.011
   Grinblat GL, 2016, COMPUT ELECTRON AGR, V127, P418, DOI 10.1016/j.compag.2016.07.003
   He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123
   Howard A.G., 2017, MOBILENETS EFFICIENT, DOI DOI 10.48550/ARXIV.1704.04861
   Howlader M.R., 2019, 2 INT C EL COMP COMM, P1, DOI [10.1109/ECACE.2019.8679421, DOI 10.1109/ECACE.2019.8679421]
   Jiang P, 2019, IEEE ACCESS, V7, P59069, DOI 10.1109/ACCESS.2019.2914929
   Kawasaki Y, 2015, LECT NOTES COMPUT SC, V9475, P638, DOI 10.1007/978-3-319-27863-6_59
   Kosamkar P. K., 2018, 4 INT C COMP COMM CO, DOI 10.1109/ICCUBEA.2018.8697504
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kuckenberg J, 2009, PRECIS AGRIC, V10, P34, DOI 10.1007/s11119-008-9082-0
   Li J, 2018, CHIN CONTR CONF, P9159, DOI 10.23919/ChiCC.2018.8482813
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lindenthal M, 2005, PHYTOPATHOLOGY, V95, P233, DOI 10.1094/PHYTO-95-0233
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Lu J, 2017, COMPUT ELECTRON AGR, V142, P369, DOI 10.1016/j.compag.2017.09.012
   Lu Y, 2017, NEUROCOMPUTING, V267, P378, DOI 10.1016/j.neucom.2017.06.023
   Mohanty S. P., 2016, Frontiers in Plant Science, V7, P1419
   Nachtigall LG, 2016, PROC INT C TOOLS ART, P472, DOI [10.1109/ICTAI.2016.75, 10.1109/ICTAI.2016.0078]
   Nowicki M, 2012, PLANT DIS, V96, P4, DOI 10.1094/PDIS-05-11-0458
   ofAlberta G., 2020, LATE BLIGHT POTATOES
   Pereira Marcos Alexandre, 2019, 2019 XV Workshop de Visao Computacional (WVC) [2019 XV Computer Vision Workshop (WVC)]. Proceedings, P38, DOI 10.1109/WVC.2019.8876931
   Prajwala TM, 2018, INT CONF CONTEMP, P314
   Sardogan M, 2018, 2018 3RD INTERNATIONAL CONFERENCE ON COMPUTER SCIENCE AND ENGINEERING (UBMK), P382, DOI 10.1109/UBMK.2018.8566635
   Singh Vijai, 2017, Information Processing in Agriculture, V4, P41, DOI 10.1016/j.inpa.2016.10.005
   Sladojevic S, 2016, COMPUT INTEL NEUROSC, V2016, DOI 10.1155/2016/3289801
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Sun XD, 2018, NEUROCOMPUTING, V299, P42, DOI 10.1016/j.neucom.2018.03.030
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Tian YN, 2019, COMPUT ELECTRON AGR, V157, P417, DOI 10.1016/j.compag.2019.01.012
   Times S, 2019, STATICS TIMES GDP ST
   Trivedi Jay, 2020, Emerging technology trends in electronics, communication and networking, P267, DOI DOI 10.1007/978-981-15-7219-7_23
   Tzutalin: Labelimg, 2015, FREE SOFTWARE LICENS
   Wise K., 2010, PURDUE EXTENSION PUB
   Zhang SW, 2017, COMPUT ELECTRON AGR, V134, P135, DOI 10.1016/j.compag.2017.01.014
   Zhang XH, 2018, IEEE ACCESS, V6, P30370, DOI 10.1109/ACCESS.2018.2844405
NR 49
TC 51
Z9 51
U1 7
U2 60
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2022
VL 38
IS 8
BP 2923
EP 2938
DI 10.1007/s00371-021-02164-9
EA JUN 2021
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 3B2OE
UT WOS:000659773700001
DA 2024-07-18
ER

PT J
AU Osuna-Coutiño, JAD
   Martinez-Carranza, J
AF de Jesus Osuna-Coutino, J. A.
   Martinez-Carranza, Jose
TI Volumetric structure extraction in a single image
SO VISUAL COMPUTER
LA English
DT Article
DE 3D Extraction; Single image; CNN
ID MOTION; SHAPE
AB High-level structure (HLS) extraction recovers 3D elements on human-made surfaces (objects, buildings, ground, etc.). There are several approaches to HLS extraction. However, most of these approaches are based on processing two or more images captured from different camera views or on processing 3D data in the form of point clouds extracted from the camera images. In general, 3D point cloud and multiple views approaches have good performance for certain scenes with video sequences or image sequences, but they need sufficient parallax in order to guarantee accuracy. To address this problem, an alternative is to process a single RGB image seeking to interpret areas of the images where the human-made structure may be observed, thus removing parallax dependency, but adding the challenge of having to interpret image ambiguities correctly. Motivated by the latter, we propose a methodology for 3D volumetric structure extraction from a single image. Our strategy is to divide and simplify the 3D structure extraction process. For that, our methodology has three steps. First, the structure recognition step provides the segmentation, location, and delimitation of the urbanized structures in the scene. Second, we propose a graph analysis to classify and locate the boundaries between the different urbanized structures in the scene. Third, we use a proposed CNN and the pinhole camera model to extract the 3D volumetric structure. On the other hand, we evaluate this methodology in synthetic and public datasets.
C1 [de Jesus Osuna-Coutino, J. A.; Martinez-Carranza, Jose] Inst Nacl Astrofis Opt & Elect INAOE, Comp Sci Dept, Puebla 72840, Mexico.
   [Martinez-Carranza, Jose] Univ Bristol, Comp Sci Dept, Bristol BS8 1TH, Avon, England.
C3 Instituto Nacional de Astrofisica, Optica y Electronica; University of
   Bristol
RP Martinez-Carranza, J (corresponding author), Inst Nacl Astrofis Opt & Elect INAOE, Comp Sci Dept, Puebla 72840, Mexico.; Martinez-Carranza, J (corresponding author), Univ Bristol, Comp Sci Dept, Bristol BS8 1TH, Avon, England.
EM osuna@inaoep.mx; carranza@inaoep.mx
RI de Jesús Osuna-Coutiño, Juan Antonio/J-2932-2019; Martinez-Carranza,
   Jose/Y-1190-2019
OI de Jesús Osuna-Coutiño, Juan Antonio/0000-0003-0237-7197;
   Martinez-Carranza, Jose/0000-0002-8914-1904
CR Achanta R., 2010, SLIC Superpixels
   Agrawal SC, 2022, VISUAL COMPUT, V38, P781, DOI 10.1007/s00371-020-02049-3
   Aguilar-González A, 2019, MICROPROCESS MICROSY, V67, P103, DOI 10.1016/j.micpro.2019.03.005
   Alhashim Ibraheem, 2019, ARXIV181211941
   [Anonymous], 2001, Int. Arch. Photogramm, Remote Sens., DOI DOI 10.5194/ISPRSARCHIVES-XL-5-W2-207-2013
   Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615
   Bartoli A, 2003, INT J COMPUT VISION, V52, P45, DOI 10.1023/A:1022318524906
   Chang J., 2019, COMPUT VIS PATTERN R
   Chen YL, 2019, IEEE I CONF COMP VIS, P9774, DOI [10.1109/iccv.2019.00987, 10.1109/ICCV.2019.00987]
   Cherian A, 2009, IEEE INT CONF ROBOT, P519
   Dani A, 2013, IEEE INT C INT ROBOT, P602, DOI 10.1109/IROS.2013.6696413
   Osuna-Coutiño JAD, 2016, INT C PATT RECOG, P1923, DOI 10.1109/ICPR.2016.7899917
   Osuna-Coutino JAD, 2016, ADJUNCT PROCEEDINGS OF THE 2016 IEEE INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY (ISMAR-ADJUNCT), P135, DOI [10.1109/ISMAR-Adjunct.2016.53, 10.1109/ISMAR-Adjunct.2016.0060]
   Deng L, 2013, FOUND TRENDS SIGNAL, V7, pI, DOI 10.1561/2000000039
   Eigen D, 2014, ADV NEUR IN, V27
   Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5
   Fan H., 2017, 2017 IEEE International Magnetics Conference (INTERMAG), DOI 10.1109/INTMAG.2017.8007987
   Favaro P, 2005, IEEE T PATTERN ANAL, V27, P406, DOI 10.1109/TPAMI.2005.43
   Firman M, 2016, PROC CVPR IEEE, P5431, DOI 10.1109/CVPR.2016.586
   Forsyth DA, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P447, DOI 10.1109/ICCV.2001.937659
   FOUHEY D, 2013, IEEE I CONF COMP VIS
   FREDJ H, 2020, VISUAL COMPUT
   Garnett, ADV NEURAL INFORM PR, V29
   Gee A., 2007, BRIT MACH VIS C BMVC, DOI DOI 10.5244/C.21.6
   Gee AP, 2008, IEEE T ROBOT, V24, P980, DOI 10.1109/TRO.2008.2004641
   Haines O, 2015, IEEE T PATTERN ANAL, V37, P1849, DOI 10.1109/TPAMI.2014.2382097
   Hoiem D, 2005, IEEE I CONF COMP VIS, P654
   Hoiem D, 2007, INT J COMPUT VISION, V75, P151, DOI 10.1007/s11263-006-0031-y
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Koenig N., 2018, GAZEBO SIMULATOR
   Koenig N., 2018, GAZEBO
   Kosecká J, 2005, COMPUT VIS IMAGE UND, V100, P274, DOI 10.1016/j.cviu.2005.04.005
   LAVEST JM, 1993, IEEE T ROBOTIC AUTOM, V9, P196, DOI 10.1109/70.238283
   Li W, 2014, IEEE INT CONF ROBOT, P9, DOI 10.1109/ICRA.2014.6906583
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu, 2017, PANGOCLOUD GITHUB
   LIU D, 2018, MDPI SENS
   Liu FY, 2016, IEEE T PATTERN ANAL, V38, P2024, DOI 10.1109/TPAMI.2015.2505283
   Lobay A, 2006, INT J COMPUT VISION, V67, P71, DOI 10.1007/s11263-006-4068-8
   Martinez-Carranza, 2019, MEX C PATT REC
   Martinez-Carranza Jose., 2010, BMVC, P1
   Maturana D, 2015, IEEE INT CONF ROBOT, P3471, DOI 10.1109/ICRA.2015.7139679
   Mazzini D, 2018, IEEE I C CONS ELECT
   McClean Eric, 2011, 2011 Irish Machine Vision and Image Processing Conference, P1, DOI 10.1109/IMVIP.2011.10
   Michels J., 2005, P 22 INT C MACHINE L, P593, DOI [10.1145/1102351.1102426, DOI 10.1145/1102351.1102426]
   Micusík B, 2008, PROC CVPR IEEE, P1127
   Osuna-Coutino J., 2019, IEEE INT C CONTR DEC
   Osuna-Coutiño JAD, 2020, INT J REMOTE SENS, V41, P8256, DOI 10.1080/01431161.2020.1767821
   OSUNACOUTINO J, 2019, SENSORS-BASEL
   Qi XJ, 2018, PROC CVPR IEEE, P283, DOI 10.1109/CVPR.2018.00037
   Rahimi A, 2013, IEEE IMAGE PROC, P2149, DOI 10.1109/ICIP.2013.6738443
   Ren Zhongzheng, 2018, IEEE C COMP VIS PATT
   Ruder S., ARXIV160904747
   Saxena A., 2005, ADV NEURAL INFORM PR, V18, P1
   Saxena A, 2009, IEEE T PATTERN ANAL, V31, P824, DOI 10.1109/TPAMI.2008.132
   Shen C., 2019, COMPUT VIS PATTERN R
   Shimodaira H, 2006, IEEE T PATTERN ANAL, V28, P612, DOI 10.1109/TPAMI.2006.67
   Silveira G, 2006, 2006 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, VOLS 1-12, P49, DOI 10.1109/IROS.2006.282189
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   SUCAR L, 2015, ADV COMPUT VIS PATT
   Teng CH, 2018, VISUAL COMPUT, V34, P1507, DOI 10.1007/s00371-017-1425-2
   Uhrig J, 2017, INT CONF 3D VISION, P11, DOI 10.1109/3DV.2017.00012
   WILCZKOWIAK M, 2005, IEEE T PATTERN ANAL
   Wu J., 2017, PROC ADVNEURAL INF P, V30, P153
   Xiang Y, 2018, ROBOTICS: SCIENCE AND SYSTEMS XIV
   Yang DW, 2018, PROC CVPR IEEE, P3781, DOI 10.1109/CVPR.2018.00398
   ZHAO S, 2018, SENSORS-BASEL
   Zhuo W, 2015, PROC CVPR IEEE, P614, DOI 10.1109/CVPR.2015.7298660
NR 68
TC 2
Z9 2
U1 1
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2022
VL 38
IS 8
BP 2899
EP 2921
DI 10.1007/s00371-021-02163-w
EA MAY 2021
PG 23
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 3B2OE
UT WOS:000652955000001
DA 2024-07-18
ER

PT J
AU Xu, J
   Mou, J
   Liu, J
   Hao, J
AF Xu, Ji
   Mou, Jun
   Liu, Jian
   Hao, Jin
TI The image compression-encryption algorithm based on the compression
   sensing and fractional-order chaotic system
SO VISUAL COMPUTER
LA English
DT Article
DE Image encryption; Compression sensing; Fractional-order chaotic system
ID PERMUTATION; NETWORK
AB In this paper, a novel image encryption algorithm based on the fractional-order chaotic system and compression sensing algorithm is proposed. Firstly, the dynamical characteristics of the fractional-order chaotic system are analyzed. The hardware circuit is designed in and realized on the DSP. Secondly, the block feedback diffusion algorithm is applied to this encryption scheme. The elements of the cipher block are decided by the front of the cipher block and the plain-text block. In this algorithm, it needs to be emphasized that the scrambling calculation and the diffusion operation are carried out simultaneously. The simulation results show that the algorithm can effectively encrypt digital images. Finally, the security analysis demonstrates the security and the effectiveness of the proposed encryption algorithm.
C1 [Xu, Ji; Mou, Jun; Liu, Jian; Hao, Jin] Dalian Polytech Univ, Sch Informat Sci & Engn, Dalian 116000, Peoples R China.
C3 Dalian Polytechnic University
RP Mou, J (corresponding author), Dalian Polytech Univ, Sch Informat Sci & Engn, Dalian 116000, Peoples R China.
EM moujun@csu.edu.cn
OI Mou, Jun/0000-0002-7774-2833
FU Natural Science Foundation of Liaoning Province [2020-MS-274]; National
   Nature Science Foundation of China [61773010]
FX This subject is supported by the Natural Science Foundation of Liaoning
   Province (2020-MS-274) and the National Nature Science Foundation of
   China (No.61773010).
CR Alvarez G, 2006, INT J BIFURCAT CHAOS, V16, P2129, DOI 10.1142/S0218127406015970
   Aqeel-ur-Rehman, 2018, OPTIK, V153, P117, DOI 10.1016/j.ijleo.2017.09.099
   Belazi A, 2016, SIGNAL PROCESS, V128, P155, DOI 10.1016/j.sigpro.2016.03.021
   Cao C, 2018, SIGNAL PROCESS, V143, P122, DOI 10.1016/j.sigpro.2017.08.020
   Chai XL, 2020, SIGNAL PROCESS, V171, DOI 10.1016/j.sigpro.2020.107525
   Chai XL, 2018, SIGNAL PROCESS, V148, P124, DOI 10.1016/j.sigpro.2018.02.007
   Chai XL, 2017, INT J MOD PHYS C, V28, DOI 10.1142/S0129183117500693
   Chai XL, 2017, OPT LASER ENG, V88, P197, DOI 10.1016/j.optlaseng.2016.08.009
   Chen C, 2020, SIGNAL PROCESS, V168, DOI 10.1016/j.sigpro.2019.107340
   Chen C, 2019, EUR PHYS J PLUS, V134, DOI 10.1140/epjp/i2019-12776-9
   Chen LP, 2020, FRONT INFORM TECH EL, V21, P866, DOI 10.1631/FITEE.1900709
   Gan ZH, 2019, NEURAL COMPUT APPL, V31, P7111, DOI 10.1007/s00521-018-3541-y
   Gong LH, 2019, OPT LASER ENG, V121, P169, DOI 10.1016/j.optlaseng.2019.03.006
   Gong LH, 2018, OPT LASER TECHNOL, V103, P48, DOI 10.1016/j.optlastec.2018.01.007
   HE SB, 2017, EUR PHYS J PLUS, V132, DOI DOI 10.1140/epjp/i2017-11306-3
   He SB, 2018, NONLINEAR DYNAM, V92, P85, DOI 10.1007/s11071-017-3907-1
   He SB, 2016, EUR PHYS J PLUS, V131, DOI 10.1140/epjp/i2016-16254-8
   Hua ZY, 2019, INFORM SCIENCES, V480, P403, DOI 10.1016/j.ins.2018.12.048
   Hua ZY, 2016, INFORM SCIENCES, V339, P237, DOI 10.1016/j.ins.2016.01.017
   Khalil R, 2014, J COMPUT APPL MATH, V264, P65, DOI 10.1016/j.cam.2014.01.002
   Li GD, 2019, VISUAL COMPUT, V35, P1267, DOI 10.1007/s00371-018-1574-y
   Li P, 2019, EURASIP J IMAGE VIDE, DOI 10.1186/s13640-018-0402-7
   Liu WH, 2017, INT J BIFURCAT CHAOS, V27, DOI 10.1142/S0218127417501711
   Liu WH, 2017, NONLINEAR DYNAM, V89, P2521, DOI 10.1007/s11071-017-3601-3
   Ma CG, 2020, PHYS SCRIPTA, V95, DOI 10.1088/1402-4896/ab8d54
   del Rey AM, 2015, LECT NOTES ARTIF INT, V9121, P427, DOI 10.1007/978-3-319-19644-2_36
   Matthews R., 1989, Cryptologia, V13, P29, DOI [10.1080/0161-118991863745, DOI 10.1080/0161-118991863745]
   Millérioux G, 2008, IEEE T CIRCUITS-I, V55, P1695, DOI 10.1109/TCSI.2008.916555
   Mohimani GH, 2007, LECT NOTES COMPUT SC, V4666, P389
   Mohimani H, 2009, IEEE T SIGNAL PROCES, V57, P289, DOI 10.1109/TSP.2008.2007606
   MOU J, 2019, MOBILE NETW APPL
   Peng D, 2019, INT J MOD PHYS B, V33, DOI 10.1142/S0217979219500310
   Peng YX, 2019, ENTROPY-SWITZ, V21, DOI 10.3390/e21010027
   Sayed WS, 2020, AEU-INT J ELECTRON C, V123, DOI 10.1016/j.aeue.2020.153268
   Talhaoui MZ, 2021, VISUAL COMPUT, V37, P541, DOI 10.1007/s00371-020-01822-8
   TALHAOUI MZ, 2020, VISUAL COMPUT
   Wang XY, 2019, MULTIMED TOOLS APPL, V78, P33865, DOI 10.1007/s11042-019-08171-2
   Xin Jin, 2016, Advances in Multimedia Information Processing - PCM 2016. 17th Pacific-Rim Conference on Multimedia. Proceedings: LNCS 9916, P119, DOI 10.1007/978-3-319-48890-5_12
   Xu QY, 2019, OPT LASER ENG, V121, P203, DOI 10.1016/j.optlaseng.2019.04.011
   Yang FF, 2020, SIGNAL PROCESS, V169, DOI 10.1016/j.sigpro.2019.107373
   Yang FF, 2019, IEEE ACCESS, V7, P58751, DOI 10.1109/ACCESS.2019.2914722
   Yang FF, 2019, PHYS SCRIPTA, V94, DOI 10.1088/1402-4896/ab0033
   Yu SS, 2020, OPT LASER ENG, V124, DOI 10.1016/j.optlaseng.2019.105816
   Zhang LM, 2017, CHINESE PHYS B, V26, DOI 10.1088/1674-1056/26/10/100504
   Zhou SH, 2016, AEU-INT J ELECTRON C, V70, P1, DOI 10.1016/j.aeue.2015.08.010
   Zhu CJ, 2020, MULTIMED TOOLS APPL, V79, P7227, DOI 10.1007/s11042-019-08226-4
NR 46
TC 32
Z9 32
U1 2
U2 39
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2022
VL 38
IS 5
BP 1509
EP 1526
DI 10.1007/s00371-021-02085-7
EA MAR 2021
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 0M9JP
UT WOS:000630833500001
DA 2024-07-18
ER

PT J
AU Saurav, S
   Gidde, P
   Saini, R
   Singh, S
AF Saurav, Sumeet
   Gidde, Prashant
   Saini, Ravi
   Singh, Sanjay
TI Dual integrated convolutional neural network for real-time facial
   expression recognition in the wild
SO VISUAL COMPUTER
LA English
DT Article
DE Deep convolutional neural network; Embedded implementation; CNN
   optimization; Facial expression recognition
ID EMOTION RECOGNITION; DEEP; FEATURES; LSTM; CNN
AB Automatic recognition of facial expressions in the wild is a challenging problem and has drawn a lot of attention from the computer vision and pattern recognition community. Since their emergence, the deep learning techniques have proved their efficacy in facial expression recognition (FER) tasks. However, these techniques are parameter intensive, and thus, could not be deployed on resource-constrained embedded platforms for real-world applications. To mitigate these limitations of the deep learning inspired FER systems, in this paper, we present an efficient dual integrated convolution neural network (DICNN) model for the recognition of facial expressions in the wild in real-time, running on an embedded platform. The designed DICNN model with just 1.08M parameters and 5.40 MB memory storage size achieves optimal performance by maintaining a proper balance between recognition accuracy and computational efficiency. We evaluated the DICNN model on four FER benchmark datasets (FER2013, FERPlus, RAF-DB, and CKPlus) using different performance evaluation metrics, namely the recognition accuracy, precision, recall, and F1-score. Finally, to provide a portable solution with high throughput inference, we optimized the designed DICNN model using TensorRT SDK and deployed it on an Nvidia Xavier embedded platform. Comparative analysis results with the other state-of-the-art methods revealed the effectiveness of the designed FER system, which achieved competitive accuracy with multi-fold improvement in the execution speed.
C1 [Saurav, Sumeet; Saini, Ravi; Singh, Sanjay] Acad Sci & Innovat Res, Ghaziabad, India.
   [Saurav, Sumeet; Gidde, Prashant; Saini, Ravi; Singh, Sanjay] Cent Elect Engn Res Inst, CSIR, Pilani 333031, Rajasthan, India.
C3 Academy of Scientific & Innovative Research (AcSIR); Council of
   Scientific & Industrial Research (CSIR) - India; CSIR - Central
   Electronics Engineering Research Institute (CEERI)
RP Saurav, S (corresponding author), Acad Sci & Innovat Res, Ghaziabad, India.; Saurav, S (corresponding author), Cent Elect Engn Res Inst, CSIR, Pilani 333031, Rajasthan, India.
EM sumeet@ceeri.res.in; prashantpsg1@gmail.com; ravi@ceeri.res.in;
   sanjay@ceeri.res.in
RI SAURAV, SUMEET/AAY-5427-2020; SAURAV, SUMEET/AAM-7894-2021; Bajpai,
   Jitendra/AFE-9237-2022
OI SAURAV, SUMEET/0000-0002-4375-4107; 
CR Agrawal A, 2020, VISUAL COMPUT, V36, P405, DOI 10.1007/s00371-019-01630-9
   Albanie S, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P292, DOI 10.1145/3240508.3240578
   An FP, 2020, VISUAL COMPUT, V36, P483, DOI 10.1007/s00371-019-01635-4
   Ashwin TS, 2020, EDUC INF TECHNOL, V25, P1387, DOI 10.1007/s10639-019-10004-6
   Avots E, 2019, MACH VISION APPL, V30, P975, DOI 10.1007/s00138-018-0960-9
   Barsoum E, 2016, ICMI'16: PROCEEDINGS OF THE 18TH ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P279, DOI 10.1145/2993148.2993165
   Bengio Y., 2013, Fer-2013 face database
   Choudhary T, 2020, ARTIF INTELL REV, V53, P5113, DOI 10.1007/s10462-020-09816-7
   Dean J., 2015, NIPS DEEP LEARNING R
   Deng, 2020, IEEE T AFFECT COMPUT, V3, P91
   Dinelli G, 2019, INT J RECONFIGURABLE, V2019, DOI 10.1155/2019/7218758
   Ditty M, 2018, HOT CHIPS S HIGH PER
   Fei ZX, 2020, NEUROCOMPUTING, V388, P212, DOI 10.1016/j.neucom.2020.01.034
   Georgescu MI, 2019, IEEE ACCESS, V7, P64827, DOI 10.1109/ACCESS.2019.2917266
   Gogic I, 2020, VISUAL COMPUT, V36, P97, DOI 10.1007/s00371-018-1585-8
   Gordon A, 2018, PROC CVPR IEEE, P1586, DOI 10.1109/CVPR.2018.00171
   Nguyen HD, 2019, INT J PATTERN RECOGN, V33, DOI 10.1142/S0218001419400159
   Hajarolasvadi N, 2019, ENTROPY-SWITZ, V21, DOI 10.3390/e21050479
   Huang C., 2017, 2017 IEEE UNDERGRADU, P1
   Huang J, 2017, PROC CVPR IEEE, P3296, DOI 10.1109/CVPR.2017.351
   Jain DK, 2019, PATTERN RECOGN LETT, V120, P69, DOI 10.1016/j.patrec.2019.01.008
   Jeong M, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18124270
   Kazemi V, 2014, PROC CVPR IEEE, P1867, DOI 10.1109/CVPR.2014.241
   Kim BK, 2016, J MULTIMODAL USER IN, V10, P173, DOI 10.1007/s12193-015-0209-0
   King DE, 2009, J MACH LEARN RES, V10, P1755
   Kotikalapudi R., 2017, CONTRIBUTORS
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li HH, 2019, APPL INTELL, V49, P2956, DOI 10.1007/s10489-019-01427-2
   Li K, 2020, VISUAL COMPUT, V36, P391, DOI 10.1007/s00371-019-01627-4
   Li M, 2021, IEEE T AFFECT COMPUT, V12, P544, DOI 10.1109/TAFFC.2018.2880201
   Li PY, 2019, IEEE T BIO-MED ENG, V66, P2869, DOI 10.1109/TBME.2019.2897651
   Li S, 2019, IEEE T IMAGE PROCESS, V28, P356, DOI 10.1109/TIP.2018.2868382
   Li THS, 2019, IEEE ACCESS, V7, P93998, DOI 10.1109/ACCESS.2019.2928364
   Li Y, 2019, IEEE T IMAGE PROCESS, V28, P2439, DOI 10.1109/TIP.2018.2886767
   Lian Z, 2018, 2018 FIRST ASIAN CONFERENCE ON AFFECTIVE COMPUTING AND INTELLIGENT INTERACTION (ACII ASIA)
   Liu XQ, 2020, VISUAL COMPUT, V36, P1635, DOI 10.1007/s00371-019-01759-7
   Lopes AT, 2017, PATTERN RECOGN, V61, P610, DOI 10.1016/j.patcog.2016.07.026
   Lucey P., 2010, IEEE COMP SOC C COMP, P94, DOI 10.1109/CVPRW.2010.5543262
   Miao S, 2019, IEEE ACCESS, V7, P78000, DOI 10.1109/ACCESS.2019.2921220
   MIGACZ S, 2017, GPU TECHN C, V2, P7
   Oh S, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20030866
   Pan XZ, 2020, IETE TECH REV, V37, P402, DOI 10.1080/02564602.2019.1645620
   Pan XZ, 2019, SYMMETRY-BASEL, V11, DOI 10.3390/sym11010052
   Pramerdorfer C, 2016, ARXIV
   Riaz MN, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20041087
   Shao J, 2019, NEUROCOMPUTING, V355, P82, DOI 10.1016/j.neucom.2019.05.005
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sonawane B, 2021, VISUAL COMPUT, V37, P1151, DOI 10.1007/s00371-020-01859-9
   Sze V, 2017, P IEEE, V105, P2295, DOI 10.1109/JPROC.2017.2761740
   Uddin MZ, 2017, IEEE ACCESS, V5, P4525, DOI 10.1109/ACCESS.2017.2676238
   Wang K, 2020, PROC CVPR IEEE, P6896, DOI 10.1109/CVPR42600.2020.00693
   Wang K, 2020, IEEE T IMAGE PROCESS, V29, P4057, DOI 10.1109/TIP.2019.2956143
   Xie SY, 2019, PATTERN RECOGN, V92, P177, DOI 10.1016/j.patcog.2019.03.019
   Xing XF, 2019, FRONT NEUROROBOTICS, V13, DOI 10.3389/fnbot.2019.00037
   Yang B, 2018, IEEE ACCESS, V6, P4630, DOI 10.1109/ACCESS.2017.2784096
   Zhang HP, 2020, PATTERN RECOGN LETT, V131, P128, DOI 10.1016/j.patrec.2019.12.013
   Zhang SQ, 2019, IEEE ACCESS, V7, P32297, DOI 10.1109/ACCESS.2019.2901521
   Zhang ZP, 2015, IEEE I CONF COMP VIS, P3631, DOI 10.1109/ICCV.2015.414
   Zhao GZ, 2020, IEEE ACCESS, V8, P38528, DOI 10.1109/ACCESS.2020.2964752
   Zhao JF, 2019, BIOMED SIGNAL PROCES, V47, P312, DOI 10.1016/j.bspc.2018.08.035
   Zhao JF, 2018, VISUAL COMPUT, V34, P1461, DOI 10.1007/s00371-018-1477-y
NR 61
TC 29
Z9 31
U1 2
U2 37
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2022
VL 38
IS 3
BP 1083
EP 1096
DI 10.1007/s00371-021-02069-7
EA FEB 2021
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZQ8YX
UT WOS:000614338000001
DA 2024-07-18
ER

PT J
AU Lukic, T
   Balázs, P
AF Lukic, Tibor
   Balazs, Peter
TI Limited-view binary tomography reconstruction assisted by shape centroid
SO VISUAL COMPUTER
LA English
DT Article
DE Binary tomography; Reconstruction; Inverse problems; Center of gravity;
   Energy minimization
ID DISCRETE TOMOGRAPHY; SEGMENTATION; OPTIMIZATION
AB In this paper, the binary tomographic reconstruction problem for very limited projection data availability is considered. Being this inverse problem highly ill-posed, we propose a new reconstruction model that uses a shape centroid-based regularization term, i.e., we assume that the center of gravity of the object of interest is known, at least approximately, in advance. Motivation for this regularization is found in the close connection between the projection data and the object centroid, as we will show. Experimental evaluation underpins that reasonable results can be obtained from practically minimal amount of projection data, gathered from just one projection direction.
C1 [Lukic, Tibor] Univ Novi Sad, Fac Tech Sci, Novi Sad, Serbia.
   [Balazs, Peter] Univ Szeged, Dept Image Proc & Comp Graph, Szeged, Hungary.
C3 University of Novi Sad; Szeged University
RP Lukic, T (corresponding author), Univ Novi Sad, Fac Tech Sci, Novi Sad, Serbia.
EM tibor@uns.ac.rs; pbalazs@inf.u-szeged.hu
RI Balazs, Peter/M-4393-2018
OI Balazs, Peter/0000-0003-3406-9578
FU Ministry of Education and Sciences of the R. of Serbia [OI-174008,
   III-44006]; Ministry for Innovation and Technology, Hungary
   [TUDFO/47138-1/2019-ITM]; project "Integrated program for training new
   generation of scientists in the fields of computer science"
   [EFOP-3.6.3-VEKOP-16-2017-00002]; European Union; European Social Fund
FX The research of Tibor Luki was supported by projects OI-174008 and
   III-44006 of the Ministry of Education and Sciences of the R. of Serbia.
   He also acknowledges to the DOMUS project of the Hungarian Academy of
   Sciences. Peter Balazs was supported by Grant TUDFO/47138-1/2019-ITM of
   the Ministry for Innovation and Technology, Hungary. This research was
   supported by the project "Integrated program for training new generation
   of scientists in the fields of computer science," No.
   EFOP-3.6.3-VEKOP-16-2017-00002. The project has been supported by the
   European Union and co-funded by the European Social Fund.
CR Abeysinghe SS, 2009, VISUAL COMPUT, V25, P627, DOI 10.1007/s00371-009-0325-5
   Balázs P, 2015, FUND INFORM, V141, P151, DOI 10.3233/FI-2015-1269
   Batenburg K.J., 2007, IEEE Conference on Image Processing, VIV, P133
   Baumann J, 2007, APPL NUMER HARMON AN, P303, DOI 10.1007/978-0-8176-4543-4_14
   Birgin EG, 2001, ACM T MATH SOFTWARE, V27, P340, DOI 10.1145/502800.502803
   Bronzino JD., 2006, The biomedical engineering handbook, V3rd
   Carmignato S., 2018, Industrial X-Ray Computed Tomography, DOI DOI 10.1007/978-3-319-59573-3
   CHANG S, 1973, IEEE T COMPUT, VC 22, P18
   Grob D, 2019, EUR RADIOL, V29, P1408, DOI 10.1007/s00330-018-5740-4
   Herman G.T., 1999, Discrete Tomography Foundations, DOI DOI 10.1007/978-1-4612-1568-4
   Herman GT, 2009, ADV PATTERN RECOGNIT, P1
   Herman GT, 2007, APPL NUMER HARMON AN, P1, DOI 10.1007/978-0-8176-4543-4
   Hong QQ, 2016, VISUAL COMPUT, V32, P1251, DOI 10.1007/s00371-015-1160-5
   Kisst Z, 2006, ACTA CYBERN, V17, P557
   Klette R, 2006, LECT NOTES COMPUT SC, V4245, P367
   Korshunova N, 2020, COMPUT MATH APPL, V80, P2462, DOI 10.1016/j.camwa.2020.07.018
   Liu XL, 2015, VISUAL COMPUT, V31, P1431, DOI 10.1007/s00371-014-1024-4
   Lukic T, 2008, LECT NOTES COMPUT SC, V5096, P476, DOI 10.1007/978-3-540-69321-5_48
   Lukic T, 2019, PHYS SCRIPTA, V94, DOI 10.1088/1402-4896/aafbcb
   Lukic T, 2016, PATTERN RECOGN LETT, V79, P18, DOI 10.1016/j.patrec.2016.04.010
   Lukic T, 2014, PATTERN RECOGN LETT, V49, P11, DOI 10.1016/j.patrec.2014.05.014
   Lukic T, 2014, INVERSE PROBL, V30, DOI 10.1088/0266-5611/30/9/095007
   Lukic T, 2011, LECT NOTES COMPUT SC, V6636, P335, DOI 10.1007/978-3-642-21073-0_30
   Lukic T, 2011, INVERSE PROBL, V27, DOI 10.1088/0266-5611/27/8/085010
   Lukic T, 2010, STUD COMPUT INTELL, V313, P263
   Madych WR, 2004, T AM MATH SOC, V356, P4475, DOI 10.1090/S0002-9947-04-03404-X
   Mahjoub Mohamed Ali, 2011, CANADIAN J IMAGE PRO, V2, P92
   O'Neill GT, 2012, VISUAL COMPUT, V28, P205, DOI 10.1007/s00371-011-0636-1
   Pan XC, 2009, INVERSE PROBL, V25, DOI 10.1088/0266-5611/25/12/123009
   Paulano F, 2014, VISUAL COMPUT, V30, P939, DOI 10.1007/s00371-014-0963-0
   Prause GPM, 1996, IEEE T MED IMAGING, V15, P532, DOI 10.1109/42.511756
   RADON J, 1986, IEEE T MED IMAGING, V5, P170, DOI 10.1109/TMI.1986.4307775
   Scarfe WC, 2018, MAXILLOFACIAL CONE B, DOI [10.1007/978-3-319-62061-9/COVER, DOI 10.1007/978-3-319-62061-9, 10.1007/978-3-319-62061-9]
   Schüle T, 2005, DISCRETE APPL MATH, V151, P229, DOI 10.1016/j.dam.2005.02.028
   Shammaa MH, 2007, VISUAL COMPUT, V23, P965, DOI 10.1007/s00371-007-0171-2
   Shi LK, 2021, VISUAL COMPUT, V37, P1343, DOI 10.1007/s00371-020-01869-7
   Sladoje N, 2011, IMAGE VISION COMPUT, V29, P127, DOI 10.1016/j.imavis.2010.08.007
   Steingruber IE, 2003, AM J ROENTGENOL, V181, P99, DOI 10.2214/ajr.181.1.1810099
   Szucs J, 2019, LECT NOTES COMPUT SC, V11678, P141, DOI 10.1007/978-3-030-29888-3_12
   Wählby C, 2002, ANAL CELL PATHOL, V24, P101
   Weber S, 2006, LECT NOTES COMPUT SC, V4245, P146
   Zeegers M, 2018, LECT NOTES COMPUT SC, V11255, P164, DOI 10.1007/978-3-030-05288-1_13
NR 42
TC 6
Z9 6
U1 1
U2 17
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2022
VL 38
IS 2
BP 695
EP 705
DI 10.1007/s00371-020-02044-8
EA JAN 2021
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZE7IA
UT WOS:000607328200001
PM 33456100
OA Bronze, Green Published
DA 2024-07-18
ER

PT J
AU Zhou, DY
   Liu, YP
   Li, XM
   Zhang, CM
AF Zhou, Danya
   Liu, Yepeng
   Li, Xuemei
   Zhang, Caiming
TI Single-image super-resolution based on local biquadratic spline with
   edge constraints and adaptive optimization in transform domain
SO VISUAL COMPUTER
LA English
DT Article
DE Local biquadratic spline; Boundary conditions; Singular value
   contraction; Adaptive optimization
ID BACK-PROJECTION; ALGORITHM
AB This paper proposes a novel single-image super-resolution method based on local biquadratic spline with edge constraints and adaptive optimization in transform domain. The complex internal structure of the image makes the values of adjacent pixels often differ greatly. Using surface patches to interpolate image blocks can avoid large surface oscillation. Because the quadratic spline has better shape-preserving property, we construct the biquadratic spline surface on each image block to make the interpolation more flexible. The boundary conditions have great influence on the shape of local biquadratic spline surfaces and are the keys to constructing surfaces. Using edge information as a constraint to calculate them can reduce jagged and mosaic effects. To decrease the errors caused by surface fitting, we propose a new adaptive optimization model in transform domain. Compared with the traditional iterative back-projection, this model further improves the magnification accuracy by introducing SVD-based adaptive optimization. In the optimization, we convert similar block matrices to the transform domain by SVD. Then the contraction coefficients are calculated according to the non-local self-similarity, and the singular values are contracted. Experimental comparison with the other state-of-the-art methods shows that the proposed method has better performance in both visual effect and quantitative measurement.
C1 [Zhou, Danya] Shandong Univ, Sch Comp Sci & Technol, Jinan, Peoples R China.
   [Liu, Yepeng; Li, Xuemei; Zhang, Caiming] Shandong Univ, Sch Software, Jinan, Peoples R China.
   [Li, Xuemei; Zhang, Caiming] Shandong Coinnovat Ctr Future Intelligent Comp, Yantai, Peoples R China.
C3 Shandong University; Shandong University
RP Li, XM (corresponding author), Shandong Univ, Sch Software, Jinan, Peoples R China.; Li, XM (corresponding author), Shandong Coinnovat Ctr Future Intelligent Comp, Yantai, Peoples R China.
EM danyazhou@yeah.net; xmli@sdu.edu.cn
RI Liu, Yepeng/AAT-7017-2021; Cheng, Lin/KFQ-3111-2024
OI Liu, Yepeng/0000-0001-6340-7818; 
FU NSFC Joint Fund with Zhejiang Integration of Informatization and
   Industrialization under Key Project [U1609218]; Natural Science
   Foundation of Shandong Province [ZR2018BF009, ZR2017MF033]
FX This work is supported partly by the NSFC Joint Fund with Zhejiang
   Integration of Informatization and Industrialization under Key Project
   under Grant No.U1609218; the Natural Science Foundation of Shandong
   Province under Grant Nos. (ZR2018BF009, ZR2017MF033).
CR Arbeláez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161
   Bevilacqua M, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.135
   Chang H, 2004, PROC CVPR IEEE, P275, DOI 10.1109/cvpr.2004.1315043
   Chen MJ, 2005, IMAGE VISION COMPUT, V23, P791, DOI 10.1016/j.imavis.2005.05.005
   Cruz C, 2018, IEEE T IMAGE PROCESS, V27, P1376, DOI 10.1109/TIP.2017.2779265
   Dai SY, 2007, 2007 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOLS 1-5, P1039
   Dai T, 2019, PROC CVPR IEEE, P11057, DOI 10.1109/CVPR.2019.01132
   Ding N, 2019, J COMPUT SCI TECH-CH, V34, P537, DOI 10.1007/s11390-019-1925-9
   Dong C, 2016, LECT NOTES COMPUT SC, V9906, P391, DOI 10.1007/978-3-319-46475-6_25
   Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281
   Dong C, 2014, LECT NOTES COMPUT SC, V8692, P184, DOI 10.1007/978-3-319-10593-2_13
   Dong WS, 2009, IEEE IMAGE PROC, P349, DOI 10.1109/ICIP.2009.5414423
   Egiazarian K, 2015, EUR SIGNAL PR CONF, P2849, DOI 10.1109/EUSIPCO.2015.7362905
   Glasner D, 2009, IEEE I CONF COMP VIS, P349, DOI 10.1109/ICCV.2009.5459271
   Haris M, 2018, IEEE GLOB COMM CONF
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang YF, 2018, IEEE T IMAGE PROCESS, V27, P5904, DOI 10.1109/TIP.2018.2860685
   Irani M., 1993, Journal of Visual Communication and Image Representation, V4, P324, DOI 10.1006/jvci.1993.1030
   Li X, 2001, IEEE T IMAGE PROCESS, V10, P1521, DOI 10.1109/83.951537
   Meijering E. H. W., 1999, Proceedings 1999 International Conference on Image Processing (Cat. 99CH36348), P647, DOI 10.1109/ICIP.1999.817195
   PARK SK, 1983, COMPUT VISION GRAPH, V23, P258, DOI 10.1016/0734-189X(83)90026-9
   Parker J, 1983, IEEE Trans Med Imaging, V2, P31, DOI 10.1109/TMI.1983.4307610
   Peleg T, 2014, IEEE T IMAGE PROCESS, V23, P2569, DOI 10.1109/TIP.2014.2305844
   Protter M, 2009, IEEE T IMAGE PROCESS, V18, P36, DOI 10.1109/TIP.2008.2008067
   Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323
   Timofte R, 2015, LECT NOTES COMPUT SC, V9006, P111, DOI 10.1007/978-3-319-16817-3_8
   Timofte R, 2013, IEEE I CONF COMP VIS, P1920, DOI 10.1109/ICCV.2013.241
   Wang LF, 2013, IEEE T CIRC SYST VID, V23, P1289, DOI 10.1109/TCSVT.2013.2240915
   Yang CY, 2013, IEEE I CONF COMP VIS, P561, DOI 10.1109/ICCV.2013.75
   Zeyde R., 2012, INT C CURV SURF, P711, DOI DOI 10.1007/978-3-642-27413-8_47
   Zhang CM, 2013, IEEE IMAGE PROC, P1046, DOI 10.1109/ICIP.2013.6738216
   Zhang L, 2006, IEEE T IMAGE PROCESS, V15, P2226, DOI 10.1109/TIP.2006.877407
   Zhang ML, 2019, IEEE T IMAGE PROCESS, V28, P868, DOI 10.1109/TIP.2018.2874284
   Zhang X, 2016, IET IMAGE PROCESS, V10, P398, DOI 10.1049/iet-ipr.2015.0467
   Zhang YL, 2018, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR.2018.00262
   Zhang YF, 2018, IEEE T IMAGE PROCESS, V27, P3782, DOI 10.1109/TIP.2018.2826139
   Zheng H, 2010, IEEE IMAGE PROC, P2817, DOI 10.1109/ICIP.2010.5651488
NR 37
TC 9
Z9 9
U1 0
U2 13
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2022
VL 38
IS 1
BP 119
EP 134
DI 10.1007/s00371-020-02007-z
EA NOV 2020
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA YW6DE
UT WOS:000593423300001
DA 2024-07-18
ER

PT J
AU Wang, P
   Chou, YX
   An, AM
   Xu, GL
AF Wang, Ping
   Chou, Yongxin
   An, Aimin
   Xu, Guili
TI Solving the PnL problem using the hidden variable method: an accurate
   and efficient solution
SO VISUAL COMPUTER
LA English
DT Article
DE Perspective-n-line problem (PnL); Camera pose estimation; Absolute
   position and orientation; Computer vision
ID CAMERA POSE ESTIMATION; LINE; ROBUST; CORRESPONDENCES
AB This paper addresses the camera pose estimation problem from 3D lines and their 2D projections, known as the perspective-n-line (PnL) problem. Although many successful solutions have been presented, it is still a challenging to optimize both computational complexity and accuracy at the same time. In our work, we parameterize the rotation by using the Cayley-Gibbs-Rodriguez (CGR) parameterization and formulate the PnL problem into a polynomial system solving problem. Instead of the Grobner basis method, which may encounter numeric problems, we seek for an efficient and stability technique-the hidden variable method-to solve the polynomial system and polish the solution via the Gauss-Newton method. The performance of our method is evaluated by using simulations and real images, and results demonstrate that our method offers accuracy and precision comparable or better than existing state-of-the-art methods, but with significantly lower computational cost.
C1 [Wang, Ping; An, Aimin] Lanzhou Univ Technol, Coll Elect & Informat Engn, Lanzhou 730050, Peoples R China.
   [Chou, Yongxin] Changshu Inst Technol, Sch Elect & Automat Engn, Suzhou 215500, Peoples R China.
   [Xu, Guili] Nanjing Univ Aeronaut & Astronaut, Coll Automat Engn, Nanjing 211106, Peoples R China.
C3 Lanzhou University of Technology; Changshu Institute of Technology;
   Nanjing University of Aeronautics & Astronautics
RP Wang, P (corresponding author), Lanzhou Univ Technol, Coll Elect & Informat Engn, Lanzhou 730050, Peoples R China.
EM pingwangsky@gmail.com; cslgchouyx@cslg.edu.cn; anaiminll@163.com;
   guilixu2002@163.com
OI Wang, Ping/0000-0003-2358-0020
FU National Natural Science Foundation of China [62001198]
FX This work was supported by the National Natural Science Foundation of
   China (No. 62001198).
CR [Anonymous], 1971, ASP S CLOS RANG PHOT
   [Anonymous], 2011, RGB-D Workshop on 3D Perception in Robotics at the European Robotics Forum
   Ansar A, 2003, IEEE T PATTERN ANAL, V25, P578, DOI 10.1109/TPAMI.2003.1195992
   Azuma R, 2001, IEEE COMPUT GRAPH, V21, P34, DOI 10.1109/38.963459
   Azuma RT, 1997, PRESENCE-VIRTUAL AUG, V6, P355, DOI 10.1162/pres.1997.6.4.355
   Brezov DS, 2013, AIP CONF PROC, V1570, P367, DOI 10.1063/1.4854778
   Bronson R., 2009, An Introduction to Optimization
   CAGLIOTI V, 1993, PATTERN RECOGN, V26, P1603, DOI 10.1016/0031-3203(93)90016-P
   Cao MW, 2018, NEURAL COMPUT APPL, V29, P1383, DOI 10.1007/s00521-017-3032-6
   CHEN HH, 1991, IEEE T PATTERN ANAL, V13, P530, DOI 10.1109/34.87340
   Dani AP, 2012, IEEE T AUTOMAT CONTR, V57, P241, DOI 10.1109/TAC.2011.2162890
   DHOME M, 1989, IEEE T PATTERN ANAL, V11, P1265, DOI 10.1109/34.41365
   Ferraz L, 2014, PROC CVPR IEEE, P501, DOI 10.1109/CVPR.2014.71
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Gander W., 1997, Least Squares Fit of Point Clouds
   Han PF, 2017, VISUAL COMPUT, V33, P1185, DOI 10.1007/s00371-016-1281-5
   Hesch JA, 2011, IEEE I CONF COMP VIS, P383, DOI 10.1109/ICCV.2011.6126266
   Jafari M., 2012, PHYSICS
   Kangni F, 2007, IEEE I CONF COMP VIS, P2866
   Kneip L, 2014, LECT NOTES COMPUT SC, V8689, P127, DOI 10.1007/978-3-319-10590-1_9
   Kukelova Z, 2008, LECT NOTES COMPUT SC, V5304, P302, DOI 10.1007/978-3-540-88690-7_23
   KUMAR R, 1994, CVGIP-IMAG UNDERSTAN, V60, P313, DOI 10.1006/ciun.1994.1060
   Lategahn H, 2011, IEEE INT CONF ROBOT, P1732
   Lepetit V, 2009, INT J COMPUT VISION, V81, P155, DOI 10.1007/s11263-008-0152-6
   Li SQ, 2012, IEEE T PATTERN ANAL, V34, P1444, DOI 10.1109/TPAMI.2012.41
   Lilian Zhang, 2013, Computer Vision - ACCV 2012. 11th Asian Conference on Computer Vision. Revised Selected Papers, P217, DOI 10.1007/978-3-642-37431-9_17
   Liu YL, 2018, VISUAL COMPUT, V34, P899, DOI 10.1007/s00371-018-1523-9
   LIU YC, 1990, IEEE T PATTERN ANAL, V12, P28, DOI 10.1109/34.41381
   Lu CP, 2000, IEEE T PATTERN ANAL, V22, P610, DOI 10.1109/34.862199
   Mirzaei F. M., 2011, 2011 IEEE International Conference on Robotics and Automation (ICRA 2011), P5581, DOI 10.1109/ICRA.2011.5980272
   Nakano G., 2015, BRIT MACH VIS C
   Pibyl B., 2016, CAMERA POSE ESTIMATI
   PIBYL B, 2017, COMPUTER VISION IMAG, V161, P130
   Press W. H., 2002, Numerical Recipes in C: the Art of Scientific Computing, V2nd ed., DOI DOI 10.1119/1.14981
   Ryan JC, 2015, CRYOSPHERE, V9, P1, DOI 10.5194/tc-9-1-2015
   Silva M., 2013, CAMERA CALIBRATION U
   Urban S, 2016, ISPRS ANN PHOTO REM, V3, P131, DOI 10.5194/isprsannals-III-3-131-2016
   Visual S., 2015, IEEE ROBOTICS AUTOMA
   Wang P, 2019, MACH VISION APPL, V30, P603, DOI 10.1007/s00138-019-01012-0
   Wang P, 2018, PATTERN RECOGN LETT, V108, P31, DOI 10.1016/j.patrec.2018.02.028
   Xu C, 2017, IEEE T PATTERN ANAL, V39, P1209, DOI 10.1109/TPAMI.2016.2582162
   Zhang YQ, 2016, IET COMPUT VIS, V10, P475, DOI 10.1049/iet-cvi.2015.0099
   Zheng YQ, 2013, IEEE I CONF COMP VIS, P2344, DOI 10.1109/ICCV.2013.291
   Zhou LP, 2019, IEEE INT C INT ROBOT, P6245, DOI [10.1109/IROS40897.2019.8968482, 10.1109/iros40897.2019.8968482]
NR 44
TC 4
Z9 5
U1 1
U2 28
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2022
VL 38
IS 1
BP 95
EP 106
DI 10.1007/s00371-020-02004-2
EA NOV 2020
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA YW6DE
UT WOS:000586358700001
DA 2024-07-18
ER

PT J
AU Doner, T
   Gokcen, D
AF Doner, Tugay
   Gokcen, Dincer
TI FPGA-based infrared image deblurring using angular position of IR
   detector
SO VISUAL COMPUTER
LA English
DT Article
DE Infrared imaging; Blur; FPGA; Point spread function; IMU
AB The motion of the object or the infrared (IR) imaging system during the integration time causes blurring of the IR image. This study covers real-time field programmable gate array (FPGA)-based deblurring for IR detectors, and an inertial measurement unit (IMU) was used to quantify the blur caused by the IR detector movement. Point spread function for each pixel was calculated using the angular position data of the IR detector obtained from IMU. Both spatially invariant and spatially variant blur cases can be modeled for the IR detector motion. After the quantification, the spatially invariant-type blur was eliminated using a Wiener filter-based deblurring algorithm. Deblurring algorithm was implemented in the Xilinx system generator environment directly using FPGA IP cores. The simulation results in the Xilinx system generator environment indicate that the proposed image deblurring method is real-time applicable, and it reduces the processing time of a single frame to 4 ms. For the implementation of 2D-fast Fourier transform design in FPGA using the corner turn matrix method, memory management is the most critical factor influencing the speed. The real-time deblurring solution given herein has the potential to be used in IR cameras on the moving platforms to increase the performance and robustness in systems such as object tracking and visual navigation.
C1 [Doner, Tugay; Gokcen, Dincer] Hacettepe Univ, Elect & Elect Engn, TR-06800 Ankara, Turkey.
   [Gokcen, Dincer] METU MEMS Res & Applicat Ctr, TR-06531 Ankara, Turkey.
C3 Hacettepe University
RP Gokcen, D (corresponding author), Hacettepe Univ, Elect & Elect Engn, TR-06800 Ankara, Turkey.; Gokcen, D (corresponding author), METU MEMS Res & Applicat Ctr, TR-06531 Ankara, Turkey.
EM dgokcen@hacettepe.edu.tr
RI Gokcen, Dincer/AAR-8632-2020; Gokcen, Dincer/H-8723-2016
OI Gokcen, Dincer/0000-0003-1847-1356
CR [Anonymous], 2011, THESIS
   [Anonymous], 2018, XILINX SYSTEM GENERA
   [Anonymous], 2003, ENCY OPT ENG ABE
   [Anonymous], DIGITAL IMAGE PROCES
   Carrato S, 2015, INT SYMP IMAGE SIG, P137, DOI 10.1109/ISPA.2015.7306047
   Atoche AC, 2011, INT GEOSCI REMOTE SE, P217, DOI 10.1109/IGARSS.2011.6048931
   Dysart TJ, 2014, IEEE HIGH PERF EXTR
   Feng Q, 2019, VISUAL COMPUT, V35, P1081, DOI 10.1007/s00371-019-01697-4
   Guarnieri M, 2016, P IEEE, V104, P467, DOI 10.1109/JPROC.2015.2513118
   Gupta A, 2010, LECT NOTES COMPUT SC, V6311, P171, DOI 10.1007/978-3-642-15549-9_13
   Harmeling Stefan, 2010, P ADV NEUR INF PROC, P829
   Hirsch M, 2011, IEEE I CONF COMP VIS, P463, DOI 10.1109/ICCV.2011.6126276
   HUNT BR, 1973, IEEE T COMPUT, VC 22, P805, DOI 10.1109/TC.1973.5009169
   Joshi N, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778767
   KANG MG, 1995, IEEE T IMAGE PROCESS, V4, P594, DOI 10.1109/83.382494
   Kundur D, 1996, IEEE SIGNAL PROC MAG, V13, P43, DOI 10.1109/79.489268
   Ming-Fu Chen, 2014, International Journal of Automation and Smart Technology, V4, P5, DOI 10.5875/ausmt.v4i1.292
   Mosquera OA, 2016, 2016 29 S INT CIRC S, P1, DOI 10.1109/SBCCI.2016.7724056
   Oswald-Tranta B., 2017, P IRS, P783
   Oswald-Tranta B, 2010, INFRARED PHYS TECHN, V53, P274, DOI 10.1016/j.infrared.2010.04.003
   Oswald-Tranta B, 2018, J SENS SENS SYST, V7, P13, DOI 10.5194/jsss-7-13-2018
   Park SH, 2014, PROC CVPR IEEE, P3366, DOI 10.1109/CVPR.2014.430
   Raskar R, 2006, ACM T GRAPHIC, V25, P795, DOI 10.1145/1141911.1141957
   RICHARDSON WH, 1972, J OPT SOC AM, V62, P55, DOI 10.1364/JOSA.62.000055
   Sims O., 2007, THESIS
   Wang N., 2016, CHIN SOC OPT ENG C
   Whyte O, 2012, INT J COMPUT VISION, V98, P168, DOI 10.1007/s11263-011-0502-7
   Wiener N., 1964, Extrapolation, interpolation, and smoothing of stationary time series: with engineering applications
   Yang H, 2019, VISUAL COMPUT, V35, P1627, DOI 10.1007/s00371-018-1562-2
   Yuan L, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239452
   Zhang D.L., 2013, ADV MAT RES, V760, P1423
   Zhang X, 2015, VISUAL COMPUT, V31, P131, DOI 10.1007/s00371-014-0920-y
NR 32
TC 3
Z9 3
U1 1
U2 37
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2021
VL 37
IS 7
BP 2039
EP 2050
DI 10.1007/s00371-020-01961-y
EA AUG 2020
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SZ5DG
UT WOS:000559944900001
DA 2024-07-18
ER

PT J
AU Khan, A
   Yin, HJ
AF Khan, Aftab
   Yin, Hujun
TI Arbitrarily shaped Point Spread Function (PSF) estimation for single
   image blind deblurring
SO VISUAL COMPUTER
LA English
DT Article
DE Blind Image Deblurring (BID); Genetic Algorithm; Image restoration;
   Image quality measures; Filter estimation; Deconvolution
ID INDEPENDENT COMPONENT ANALYSIS; QUALITY ASSESSMENT; DECONVOLUTION;
   ALGORITHMS
AB The research paper focuses on a challenging task faced in blind image deblurring (BID). It relates to the estimation of arbitrarily shaped (nonparametric or complex shaped) point spread functions (PSFs) of motion blur caused by camera handshake. These PSFs exhibit much more complex shapes than their parametric counterparts and deblurring, in this case, requires intricate ways to estimate the blur and effectively remove it. This research work introduces a novel blind deblurring scheme visualized for deblurring images corrupted by arbitrarily shaped PSFs. It is based on genetic algorithm and utilizes the Blind/Reference-less Image Spatial QUality Evaluator (BRISQUE) measure as the fitness function for arbitrarily shaped PSF estimation. The proposed BID scheme has been compared with other state-of-the-art single image motion deblurring schemes as benchmarks. Validation has been carried out on the standard real-life blurred images. Results of both benchmark and real images are presented. For real-life blurred images, the proposed BID scheme using BRISQUE converges in close vicinity of the original blurring functions. However, the benchmark schemes fail to effectively restore the real blurred images. The proposed scheme surpasses on average of seven percent higher image quality as compared to the benchmark schemes.
C1 [Khan, Aftab] Univ Engn & Technol UET, Dept Comp Syst Engn, Peshawar 25120, Pakistan.
   [Yin, Hujun] Univ Manchester, Sch Elect & Elect Engn, Manchester M13 9PL, Lancs, England.
C3 University of Manchester
RP Khan, A (corresponding author), Univ Engn & Technol UET, Dept Comp Syst Engn, Peshawar 25120, Pakistan.
EM aftab.khan@uetpeshawar.edu.pk
OI Yin, Hujun/0000-0002-9198-5401
CR Almeida MSC, 2010, IEEE T IMAGE PROCESS, V19, P36, DOI 10.1109/TIP.2009.2031231
   [Anonymous], 2010, MATLAB VERS 7 10 0 R
   BIEMOND J, 1990, P IEEE, V78, P856, DOI 10.1109/5.53403
   Bini AA, 2014, VISUAL COMPUT, V30, P311, DOI 10.1007/s00371-013-0857-6
   Bovik A.C, LAB IMAGE VIDEO ENG
   Brunet D, 2012, IEEE T IMAGE PROCESS, V21, P1488, DOI 10.1109/TIP.2011.2173206
   Feng Q, 2019, VISUAL COMPUT, V35, P1081, DOI 10.1007/s00371-019-01697-4
   Fergus R, 2006, ACM T GRAPHIC, V25, P787, DOI 10.1145/1141911.1141956
   Fortunato HE, 2014, VISUAL COMPUT, V30, P661, DOI 10.1007/s00371-014-0966-x
   Gupta A., 2010, LECT NOTES COMPUTER, V6311
   Hirsch M, 2011, IEEE I CONF COMP VIS, P463, DOI 10.1109/ICCV.2011.6126276
   Hussain I, 2008, THESIS
   Hyvärinen A, 1999, IEEE T NEURAL NETWOR, V10, P626, DOI 10.1109/72.761722
   Hyvärinen A, 2000, NEURAL NETWORKS, V13, P411, DOI 10.1016/S0893-6080(00)00026-5
   Khan A., 2012, Proceedings of the 2012 IEEE International Conference on Imaging Systems and Techniques (IST), P456, DOI 10.1109/IST.2012.6295559
   Khan A., 2011, LECT NOTES COMPUTER, V6936
   Khan A. F, 2014, THESIS
   Khan A, 2012, INTEGR COMPUT-AID E, V19, P331, DOI 10.3233/ICA-2012-0409
   Kupyn O, 2019, IEEE I CONF COMP VIS, P8877, DOI 10.1109/ICCV.2019.00897
   Kupyn O, 2018, PROC CVPR IEEE, P8183, DOI 10.1109/CVPR.2018.00854
   Lai WS, 2016, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2016.188
   Mai L, 2015, PROC CVPR IEEE, P371, DOI 10.1109/CVPR.2015.7298634
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Nah S, 2017, PROC CVPR IEEE, P257, DOI 10.1109/CVPR.2017.35
   Ramakrishnan S, 2017, IEEE INT CONF COMP V, P2993, DOI 10.1109/ICCVW.2017.353
   Registrar Amazon, 2020, DESKT NEX WALLP
   Shan Q, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360672
   Tao X, 2018, PROC CVPR IEEE, P8174, DOI 10.1109/CVPR.2018.00853
   Tubbs R. N., 2003, THESIS
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang Z, 2002, IEEE SIGNAL PROC LET, V9, P81, DOI 10.1109/97.995823
   Wang Z, 2011, IEEE T IMAGE PROCESS, V20, P1185, DOI 10.1109/TIP.2010.2092435
   Whyte O, 2012, INT J COMPUT VISION, V98, P168, DOI 10.1007/s11263-011-0502-7
   Whyte O, 2011, 2011 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCV WORKSHOPS)
   Yin HJ, 2008, INTEGR COMPUT-AID E, V15, P219
   Zhang X, 2015, VISUAL COMPUT, V31, P131, DOI 10.1007/s00371-014-0920-y
NR 37
TC 7
Z9 8
U1 1
U2 23
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2021
VL 37
IS 7
BP 1661
EP 1671
DI 10.1007/s00371-020-01930-5
EA JUL 2020
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SZ5DG
UT WOS:000552256500001
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Cortial, Y
   Peytavie, A
   Galin, E
   Guérin, E
AF Cortial, Yann
   Peytavie, Adrien
   Galin, Eric
   Guerin, Eric
TI Real-time hyper-amplification of planets
SO VISUAL COMPUTER
LA English
DT Article
DE Terrain modeling; Planets; Amplification; Adaptive subdivision;
   Procedural generation
AB We propose an original method for generating planets with a high level of detail in real time. Our approach relies on a proceduralhyper-amplificationalgorithm: a controlled subdivision process that faithfully reproduces landforms and hydrosphere features at different scales. Starting from low-resolution user-defined control maps providing information about the elevation, the presence of large-scale water bodies and landforms types, we apply subdivision rules to obtain a high-resolution hydrologically consistent planet model. We first generate large-scale river networks connected to inner seas and oceans and then synthesize the detailed hydrographic landscapes, including river tributaries and lakes, mountain ranges, valleys, plateaus, deserts and hills systems. Our GPU implementation allows to interactively explore planets that are produced by tectonic simulations, generated procedurally or authored by artists.
C1 [Cortial, Yann; Guerin, Eric] INSA Lyon, LIRIS, CNRS, Villeurbanne, France.
   [Peytavie, Adrien; Galin, Eric] Univ Lyon, LIRIS, CNRS, Villeurbanne, France.
C3 Centre National de la Recherche Scientifique (CNRS); Institut National
   des Sciences Appliquees de Lyon - INSA Lyon; Centre National de la
   Recherche Scientifique (CNRS); Institut National des Sciences Appliquees
   de Lyon - INSA Lyon
RP Cortial, Y (corresponding author), INSA Lyon, LIRIS, CNRS, Villeurbanne, France.
EM yann.cortial@liris.cnrs.fr; adrien.peytavie@liris.cnrs.fr;
   eric.galin@liris.cnrs.fr; eric.guerin@liris.cnrs.fr
OI Cortial, Yann/0000-0001-9421-3346; Peytavie, Adrien/0000-0002-6994-9164;
   Guerin, Eric/0000-0002-2189-2728; Galin, Eric/0000-0002-5946-4112
FU Agence Nationale de la Recherche [HDWANR16-CE33-0001]
FX This research was part of the Project HDWANR16-CE33-0001, supported by
   Agence Nationale de la Recherche.
CR Argudo O, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356535
   Belhadj F., 2005, P S VIRT REAL SOFTW, P151
   Bridson R., 2007, Fast Poisson disk sampling in arbitrary dimensions, V10, P1, DOI DOI 10.1145/1278780.1278807
   Cortial Y, 2019, COMPUT GRAPH FORUM, V38, P1, DOI 10.1111/cgf.13614
   Cozzi Patrick., 2011, 3D engine design for virtual globes
   Derzapf E, 2011, COMPUT GRAPH FORUM, V30, P2031, DOI 10.1111/j.1467-8659.2011.02052.x
   Ebert D.S., 1998, Texturing Modeling A Procedural Approach, VSecond
   FOURNIER A, 1982, COMMUN ACM, V25, P371, DOI 10.1145/358523.358553
   Galin E, 2019, COMPUT GRAPH FORUM, V38, P553, DOI 10.1111/cgf.13657
   Génevaux JD, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461996
   Guérin E, 2016, COMPUT GRAPH FORUM, V35, P177, DOI 10.1111/cgf.12821
   HORTON RE, 1945, GEOL SOC AM BULL, V56, P275, DOI 10.1130/0016-7606(1945)56[275:edosat]2.0.co;2
   Prusinkiewicz Przemyslaw., 1993, Graphics Interface, V93, P174
   Zhao YW, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356553
NR 14
TC 2
Z9 2
U1 0
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2020
VL 36
IS 10-12
BP 2273
EP 2284
DI 10.1007/s00371-020-01923-4
EA JUL 2020
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA NW1CX
UT WOS:000551368400002
DA 2024-07-18
ER

PT J
AU Liu, Y
   Sun, DH
   Wang, FP
   Lim, KP
   Chiew, TK
   Lai, Y
AF Liu Ying
   Sun Dinghua
   Wang Fuping
   Lim Keng Pang
   Chiew Tuan Kiang
   Lai Yi
TI Learning wavelet coefficients for face super-resolution
SO VISUAL COMPUTER
LA English
DT Article
DE Deep learning; CNN; Wavelet; Super-resolution
ID IMAGE SUPERRESOLUTION
AB Face image super-resolution imaging is an important technology which can be utilized in crime scene investigations and public security. Modern CNN-based super-resolution produces excellent results in terms of peak signal-to-noise ratio and the structural similarity index (SSIM). However, perceptual quality is generally poor, and the details of the facial features are lost. To overcome this problem, we propose a novel deep neural network to predict the super-resolution wavelet coefficients in order to obtain clearer facial images. Firstly, this paper uses prior knowledge of face images to manually emphases relevant facial features with more attention. Then, a linear low-rank convolution in the network is used. Finally, image edge features from canny detector are applied to enhance super-resolution images during training. The experimental results show that the proposed method can achieve competitive PSNR and SSIM and produces images with much higher perceptual quality.
C1 [Liu Ying; Sun Dinghua] Xian Univ Posts & Telecommun, Xian, Peoples R China.
   [Wang Fuping] Minist Publ Secur, Key Lab Elect Informat Proc Crime Scene Invest, Xian, Peoples R China.
   [Lim Keng Pang] Xsecpro Pte Ltd, 449 Tagore Ind Ave,Great Land Ind Bldg, Singapore, Singapore.
   [Chiew Tuan Kiang] Rekindle Pte Ltd, 70 Gardenia Rd, Singapore, Singapore.
   [Lai Yi] Int Joint Res Ctr Wireless Commun & Informat Proc, Xian, Peoples R China.
C3 Xi'an University of Posts & Telecommunications; Ministry of Public
   Security (China)
RP Sun, DH (corresponding author), Xian Univ Posts & Telecommun, Xian, Peoples R China.
EM d_t_sdh@163.com
RI ying, liu/KEI-0478-2024
FU National Natural Science Foundation of China [61801381]
FX This work was supported in part by National Natural Science Foundation
   of China (Number 61801381). The authors declare that they have no
   conflict of interest.
CR Caballero J, 2017, PROC CVPR IEEE, P2848, DOI 10.1109/CVPR.2017.304
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851
   Chen QF, 2017, ADVANCES IN ENERGY AND ENVIRONMENT RESEARCH, P165
   Dahl R, 2017, IEEE I CONF COMP VIS, P5449, DOI 10.1109/ICCV.2017.581
   Dong C, 2016, LECT NOTES COMPUT SC, V9906, P391, DOI 10.1007/978-3-319-46475-6_25
   Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281
   Dranoshchuk AD, 2019, 2019 WAVE ELECTRONICS AND ITS APPLICATION IN INFORMATION AND TELECOMMUNICATION SYSTEMS (WECONF), DOI 10.1109/weconf.2019.8840116
   Gao X, 2016, IEEE IMAGE PROC, P1439, DOI 10.1109/ICIP.2016.7532596
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Guo TT, 2017, IEEE COMPUT SOC CONF, P1100, DOI 10.1109/CVPRW.2017.148
   He K., 2016, PROC CVPR IEEE, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]
   HUANG G, 2017, PROC CVPR IEEE, P2261, DOI DOI 10.1109/CVPR.2017.243
   Huang HB, 2019, INT J COMPUT VISION, V127, P763, DOI 10.1007/s11263-019-01154-8
   Huang HB, 2017, IEEE I CONF COMP VIS, P1698, DOI 10.1109/ICCV.2017.187
   Isola Phillip, 2016, ARXIV161107004, DOI [DOI 10.1109/CVPR.2017.632, 10.1109/CVPR.2017.632]
   Katsaggelos AK, 1977, DIGITAL IMAGE RESTOR, P2
   Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.182, 10.1109/CVPR.2016.181]
   Kim SS, 2007, 2007 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOLS 1-5, P1723
   Kumar N, 2012, INT C PATT RECOG, P3468
   Lai WS, 2017, PROC CVPR IEEE, P5835, DOI 10.1109/CVPR.2017.618
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Li X, 2001, IEEE T IMAGE PROCESS, V10, P1521, DOI 10.1109/83.951537
   Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151
   Liu D, 2018, ADV NEUR IN, V31
   Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425
   Lu C, 2015, PROCEEDINGS OF 2015 INTERNATIONAL CONFERENCE ON ESTIMATION, DETECTION AND INFORMATION FUSION ICEDIF 2015, P24, DOI 10.1109/ICEDIF.2015.7280151
   MALLAT SG, 1989, IEEE T PATTERN ANAL, V11, P674, DOI 10.1109/34.192463
   Mikaeli E, 2020, VISUAL COMPUT, V36, P1573, DOI 10.1007/s00371-019-01756-w
   Park SC, 2003, IEEE SIGNAL PROC MAG, V20, P21, DOI 10.1109/MSP.2003.1203207
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Sun J, 2008, PROC CVPR IEEE, P2471, DOI 10.1109/CVPR.2008.4587659
   Szegedy C, 2014, Arxiv, DOI [arXiv:1312.6199, DOI 10.1109/CVPR.2015.7298594]
   Tian J, 2011, EXPERT SYST APPL, V38, P12514, DOI 10.1016/j.eswa.2011.04.037
   Tong T, 2017, IEEE I CONF COMP VIS, P4809, DOI 10.1109/ICCV.2017.514
   Woo DH, 2004, IEEE IMAGE PROC, P1687
   Xu K, 2018, VISUAL COMPUT, V34, P1065, DOI 10.1007/s00371-018-1554-2
   Yang JC, 2010, IEEE T IMAGE PROCESS, V19, P2861, DOI 10.1109/TIP.2010.2050625
   Yu JH, 2018, PROC CVPR IEEE, P5505, DOI 10.1109/CVPR.2018.00577
   Yu X, 2016, LECT NOTES COMPUT SC, V9909, P318, DOI 10.1007/978-3-319-46454-1_20
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhang XH, 2012, VISUAL COMPUT, V28, P1167, DOI 10.1007/s00371-011-0666-8
   Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI 10.1007/978-3-030-01234-2_18
   Zhao H, 2017, IEEE T COMPUT IMAG, V3, P47, DOI 10.1109/TCI.2016.2644865
NR 43
TC 6
Z9 6
U1 1
U2 10
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2021
VL 37
IS 7
BP 1613
EP 1622
DI 10.1007/s00371-020-01925-2
EA JUL 2020
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SZ5DG
UT WOS:000551021700001
OA hybrid
DA 2024-07-18
ER

PT J
AU Zheng, Y
   Ma, L
   Chen, YY
   Fei, GZ
   Sheng, B
   Wu, EH
AF Zheng, Yi
   Ma, Lei
   Chen, Yanyun
   Fei, Guangzheng
   Sheng, Bin
   Wu, Enhua
TI Simulation of multi-solvent stains on textile
SO VISUAL COMPUTER
LA English
DT Article
DE Fabric appearance; Capillary action; Simulation; Texture synthesis;
   Textile; Multi-solvent stains
ID DYNAMICS
AB With the recent development of stains simulation on warp-weft style fabric materials, high realistic visual effects of real-life stains can be plausibly simulated effectively. However, the previous method relies on the limited single solvent dyeing assumption, while in the real world, the fabric is often contaminated by different stains simultaneously. To tackle the multi-stains simulation problem, we propose a novelduel-stage solvents computational model (DSSM-TLM), which essentially extended the triple-layer model (TLM) (Zheng et al. in IEEE Trans Vis Comput Graph 25(7):2471-2481, 2019.) into a more general version. We demonstrated that various effects, such as oil-water stain mixing or alcohol-water stain mixing, can be simulated correctly first ever. Moreover, the simulation process of our algorithm is consistent with the real multi-solvent liquid diffusion process on a real fabric surface.
C1 [Zheng, Yi; Chen, Yanyun; Wu, Enhua] Chinese Acad Sci, Inst Software, State Key Lab Comp Sci, Beijing, Peoples R China.
   [Zheng, Yi; Chen, Yanyun; Wu, Enhua] Univ Chinese Acad Sci, Beijing, Peoples R China.
   [Ma, Lei] Peking Univ, Natl Engn Lab Video Technol, Beijing, Peoples R China.
   [Fei, Guangzheng] Commun Univ China, Sch Animat & Digital Arts, Beijing, Peoples R China.
   [Sheng, Bin] Shanghai Jiao Tong Univ, Dept Comp Sci & Engn, Shanghai, Peoples R China.
   [Wu, Enhua] Univ Macau, Macau, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Software, CAS; Chinese Academy
   of Sciences; University of Chinese Academy of Sciences, CAS; Peking
   University; Communication University of China; Shanghai Jiao Tong
   University; University of Macau
RP Wu, EH (corresponding author), Chinese Acad Sci, Inst Software, State Key Lab Comp Sci, Beijing, Peoples R China.; Wu, EH (corresponding author), Univ Chinese Acad Sci, Beijing, Peoples R China.; Ma, L (corresponding author), Peking Univ, Natl Engn Lab Video Technol, Beijing, Peoples R China.; Wu, EH (corresponding author), Univ Macau, Macau, Peoples R China.
EM malei@outlook.com; ehwu@um.edu.mo
OI Ma, Lei/0000-0001-6024-3854
FU National Key R&D Program of China [2017YFB1002701, 2020AAA0130400]; NSFC
   [61672502, 61632003]; UM Research Fund [MYRG2019-00006-FST]; PKU-Baidu
   Fund [2019BD001]
FX This study is supported by National Key R&D Program of China
   (2017YFB1002701, 2020AAA0130400), NSFC (61672502, 61632003), the UM
   Research Fund (MYRG2019-00006-FST), and PKU-Baidu Fund (2019BD001).
CR Akinci N, 2013, COMPUT ANIMAT VIRT W, V24, P195, DOI 10.1002/cav.1499
   [Anonymous], 1967, JPT
   Chen YY, 2005, ACM T GRAPHIC, V24, P1127, DOI 10.1145/1073204.1073321
   Chu NSH, 2005, ACM T GRAPHIC, V24, P504, DOI 10.1145/1073204.1073221
   CHWASTIAK S, 1973, J COLLOID INTERF SCI, V42, P298, DOI 10.1016/0021-9797(73)90293-2
   Curtis C. J., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P421, DOI 10.1145/258734.258896
   Daubert K, 2001, SPRING EUROGRAP, P63
   Deegan RD, 1997, NATURE, V389, P827, DOI 10.1038/39827
   Enright D, 2002, J COMPUT PHYS, V183, P83, DOI 10.1006/jcph.2002.7166
   Fei Y, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201392
   Ghahremani H.A., 2011, Der Chemica Sinica, V2, P212
   Groller E, 1995, IEEE T VIS COMPUT GR, V1, P302, DOI 10.1109/2945.485617
   Gu JW, 2006, ACM T GRAPHIC, V25, P762, DOI 10.1145/1141911.1141952
   Han TT, 2005, P INT COMP SOFTW APP, P71
   Hansen C., 2012, Hansen Solubility Parameters: A User's Handbook, VSecond
   HIRT CW, 1981, J COMPUT PHYS, V39, P201, DOI 10.1016/0021-9991(81)90145-5
   Hollies N.R. S., 1957, Textile Res. J, V27, P8
   HSIEH YL, 1995, TEXT RES J, V65, P299, DOI 10.1177/004051759506500508
   Jensen HW, 1999, SPRING EUROGRAP, P273
   Kaldor JM, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778842
   Kaldor JM, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360664
   Lenaerts T, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360648
   Li XS, 2016, IEEE T VIS COMPUT GR, V22, P1973, DOI 10.1109/TVCG.2015.2476788
   Lu JY, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1189762.1189765, 10.1145/1186644.1186647]
   Lukas D, 1997, J TEXT I, V88, P149, DOI 10.1080/00405009708658539
   Mhetre SK, 2009, THESIS
   Morimoto Y, 2007, PACIFIC GRAPHICS 2007: 15TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, P57, DOI 10.1109/PG.2007.51
   Muller WA, 2005, CELL MIGRATION IN DEVELOPMENT AND DISEASE, P237
   Perlin K., 1985, Computer Graphics, V19, P287, DOI 10.1145/325165.325247
   Premoze S, 2003, COMPUT GRAPH FORUM, V22, P401, DOI 10.1111/1467-8659.00687
   Qin WG, 2011, CHIN CONT DECIS CONF, P1787, DOI 10.1109/CCDC.2011.5968487
   Ren B, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2645703
   Sears F.W., 1955, U PHYS, VSecond
   Tryggvason G, 2001, J COMPUT PHYS, V169, P708, DOI 10.1006/jcph.2000.6726
   Washburn EW, 1921, PHYS REV, V17, P273, DOI 10.1103/PhysRev.17.273
   Wiener J., 2003, AUTEX RES J, V3, P64
   Xu YQ, 2001, COMP GRAPH, P391
   Zhao S, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185571
   Zheng Y, 2019, IEEE T VIS COMPUT GR, V25, P2471, DOI 10.1109/TVCG.2018.2832039
   Zhu C., 2013, Journal of Fiber Bioengineering and Informatics, V6, P205, DOI [10.3993/jfbi06201309, DOI 10.3993/JFBI06201309, https://doi.org/10.3993/jfbi06201309]
NR 40
TC 3
Z9 3
U1 0
U2 9
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2020
VL 36
IS 10-12
BP 2005
EP 2016
DI 10.1007/s00371-020-01906-5
EA JUL 2020
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA NW1CX
UT WOS:000547369000001
DA 2024-07-18
ER

PT J
AU Liu, CC
   Ying, J
   Yang, HM
   Hu, X
   Liu, J
AF Liu, Congcong
   Ying, Jie
   Yang, Haima
   Hu, Xing
   Liu, Jin
TI Improved human action recognition approach based on two-stream
   convolutional neural network model
SO VISUAL COMPUTER
LA English
DT Article
DE Human action recognition; Kalman filter; Motion history image; Faster
   R-CNN; Video surveillance
AB In order to improve the accuracy of human abnormal behavior recognition, a two-stream convolution neural network model was proposed. This model includes two main parts, VMHI and FRGB. Firstly, the motion history images are extracted and input into VGG-16 convolutional neural network for training. Then, the RGB image is input into Faster R-CNN algorithm for training using Kalman filter-assisted data annotation. Finally, the two stream VMHI and FRGB results are fused. The algorithm can recognize not only single person behavior, but also two person interaction behavior and improve the recognition accuracy of similar actions. Experimental results on KTH, Weizmann, UT-interaction, and TenthLab dataset showed that the proposed algorithm has higher accuracy than the other literature.
C1 [Liu, Congcong; Ying, Jie; Yang, Haima; Hu, Xing] Univ Shanghai Sci & Technol, Sch Opt Elect & Comp Engn, Shanghai 200093, Peoples R China.
   [Liu, Jin] Shanghai Univ Engn Sci, Sch Elect & Elect Engn, Shanghai 201620, Peoples R China.
C3 University of Shanghai for Science & Technology; Shanghai University of
   Engineering Science
RP Yang, HM (corresponding author), Univ Shanghai Sci & Technol, Sch Opt Elect & Comp Engn, Shanghai 200093, Peoples R China.
EM lcccate@163.com; yingjsh@163.com; snowyhm@sina.com; huxing@usst.edu.cn;
   flyingpine@sina.com
RI Liu, Jin/KCY-4678-2024
OI Ying, Jie/0000-0001-5256-0323
FU Shanghai Natural Science Foundation [17ZR1443500]; Fund Project of
   National Natural Science Foundation of China [61701296]; Joint Funds of
   the National Natural Science Foundation of China [U1831133]
FX Shanghai Natural Science Foundation (No. 17ZR1443500), Fund Project of
   National Natural Science Foundation of China (No. 61701296), Joint Funds
   of the National Natural Science Foundation of China (No. U1831133).
CR Afrasiabi M, 2020, VISUAL COMPUT, V36, P1127, DOI 10.1007/s00371-019-01722-6
   Bobick AF, 2001, IEEE T PATTERN ANAL, V23, P257, DOI 10.1109/34.910878
   Castrejón L, 2017, PROC CVPR IEEE, P4485, DOI 10.1109/CVPR.2017.477
   Chaudhry R, 2009, PROC CVPR IEEE, P1932, DOI 10.1109/CVPRW.2009.5206821
   Chen JW, 2017, IEEE WINT CONF APPL, P139, DOI 10.1109/WACV.2017.23
   Chou KP, 2018, IEEE ACCESS, V6, P15283, DOI 10.1109/ACCESS.2018.2809552
   David Acuna, 2018, 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition. Proceedings, P859, DOI 10.1109/CVPR.2018.00096
   Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878
   Duin R.P. W., 2002, INT C PATT RECOG
   Feichtenhofer C, 2016, PROC CVPR IEEE, P1933, DOI 10.1109/CVPR.2016.213
   Fujiyoshi H, 1998, FOURTH IEEE WORKSHOP ON APPLICATIONS OF COMPUTER VISION - WACV'98, PROCEEDINGS, P15, DOI 10.1109/ACV.1998.732852
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu X, 2018, EURASIP J ADV SIG PR, DOI 10.1186/s13634-018-0574-4
   Imran J, 2020, J AMB INTEL HUM COMP, V11, P189, DOI 10.1007/s12652-019-01239-9
   Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59
   Ko KE, 2018, ENG APPL ARTIF INTEL, V67, P226, DOI 10.1016/j.engappai.2017.10.001
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li C., 2017, 2017 IEEE INT C MULT, P585, DOI DOI 10.1109/ICMEW.2017.8026287
   Poppe R, 2010, IMAGE VISION COMPUT, V28, P976, DOI 10.1016/j.imavis.2009.11.014
   Qian HM, 2017, MULTIMED TOOLS APPL, V76, P21889, DOI 10.1007/s11042-017-4610-4
   RAPANTZIKOS K, 2009, P C COMP VIS PATT RE, P1
   REDMON J, 2016, IEEE INT COMP VIS PA
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Sahoo SP, 2019, EXPERT SYST APPL, V115, P524, DOI 10.1016/j.eswa.2018.08.014
   Schüldt C, 2004, INT C PATT RECOG, P32, DOI 10.1109/ICPR.2004.1334462
   Simonyan K., 2014, P 27 INT C NEUR INF, P568, DOI DOI 10.1002/14651858.CD001941.PUB3
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Siswantoro J, 2016, EXPERT SYST APPL, V49, P112, DOI 10.1016/j.eswa.2015.12.012
   Szegedy C, 2014, Arxiv, DOI [arXiv:1312.6199, DOI 10.1109/CVPR.2015.7298594]
   Vishwakarma DK, 2020, COGN SYST RES, V61, P1, DOI 10.1016/j.cogsys.2019.12.004
   Vishwakarma DK, 2019, VISUAL COMPUT, V35, P1595, DOI 10.1007/s00371-018-1560-4
   Wang J, 2018, J CENT SOUTH UNIV, V25, P304, DOI 10.1007/s11771-018-3738-3
   Wang LM, 2016, LECT NOTES COMPUT SC, V9912, P20, DOI 10.1007/978-3-319-46484-8_2
   Wang XJ, 2016, ISPRS ANN PHOTO REM, V3, P99, DOI 10.5194/isprsannals-III-2-99-2016
   Wang XH, 2018, IEEE T MULTIMEDIA, V20, P634, DOI 10.1109/TMM.2017.2749159
   Weinland D, 2006, COMPUT VIS IMAGE UND, V104, P249, DOI 10.1016/j.cviu.2006.07.013
   Xu K, 2017, IEEE T CIRC SYST VID, V27, P567, DOI 10.1109/TCSVT.2017.2665359
   Yang XD, 2014, J VIS COMMUN IMAGE R, V25, P2, DOI 10.1016/j.jvcir.2013.03.001
   Yi Y, 2020, SIGNAL PROCESS-IMAGE, V80, DOI 10.1016/j.image.2019.115640
   Zhao R, 2017, IEEE INT C INT ROBOT, P4260, DOI 10.1109/IROS.2017.8206288
   Zhou K, 2016, DESTECH TRANS COMP
   2007, UT INTERACTION DATAS
   2007, WEIZMANN DATASET
   2005, KTH DATASET
NR 44
TC 32
Z9 35
U1 2
U2 27
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2021
VL 37
IS 6
BP 1327
EP 1341
DI 10.1007/s00371-020-01868-8
EA JUN 2020
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SO2LW
UT WOS:000538216100001
DA 2024-07-18
ER

PT J
AU Nadine, RA
   Nair, P
   Müller, P
   Barthe, L
   Vanderhaeghe, D
AF Abu Rumman, Nadine
   Nair, Prapanch
   Mueller, Patric
   Barthe, Loic
   Vanderhaeghe, David
TI ISPH-PBD: coupled simulation of incompressible fluids and deformable
   bodies
SO VISUAL COMPUTER
LA English
DT Article
DE ISPH; PBD; Thin shells; Deformable bodies; Fluids
ID SMOOTHED PARTICLE HYDRODYNAMICS; SPH; FORMULATION; FLOWS; WATER
AB We present an efficient and stable method for simulating the two-way coupling of incompressible fluids and deformable bodies. In our method, the fluid is represented by particles, and simulated using divergence-free incompressible smoothed-particle hydrodynamics (ISPH). The deformable bodies are represented by polygonal meshes, where the elastic deformations are simulated using a position-based dynamics scheme. Our technique enforces incompressibility on the fluid using divergence-free constraints on the velocity field, while it effectively simulates the physical features of deformable bodies. Most current ISPH methods are struggling with the issue of free-surface boundary conditions. We handle this problem by introducing a novel free-surface formulation, where our free-surface model obviates the need to identify the surface particles. For the interaction between the fluid and the deformable solids, we model the forces that both phases, fluid and solid, exert upon each other. We demonstrate that our approach effectively handles complex coupling scenarios between fluids and thin deformable shells or highly deformable solids, and produces plausible results.
C1 [Abu Rumman, Nadine] Univ Coll London UCL, London, England.
   [Nair, Prapanch; Mueller, Patric] Inst Multiscale Simulat MSS, Erlangen, Germany.
   [Barthe, Loic; Vanderhaeghe, David] Comp Sci Res Inst Toulouse IRIT, Toulouse, France.
C3 University of London; University College London
RP Nadine, RA (corresponding author), Univ Coll London UCL, London, England.
EM nadine.aburumman@gmail.com; prapanch.nair@fau.de; patric.muller@fau.de;
   loic.barthe@irit.fr; david.vanderhaeghe@irit.fr
RI Abu Rumman, Nadine/AAP-3257-2020; Vanderhaeghe, David/AAL-1240-2021;
   Müller, Patric/F-3817-2016
OI Vanderhaeghe, David/0000-0003-0506-6036; Müller,
   Patric/0000-0002-0090-4486; Nair, Prapanch/0000-0002-4918-5806;
   Aburumman, Nadine/0000-0003-4578-8738
FU Deutsche Forschungsgemeinschaft (DFG) through the Cluster of Excellence
   Engineering of Advanced Materials; CIMI lab; IM&M project FOLDDyn
   [ANR-16-CE33-0015]; Agence Nationale de la Recherche (ANR)
   [ANR-16-CE33-0015] Funding Source: Agence Nationale de la Recherche
   (ANR)
FX This work was funded by the IM&M project FOLDDyn (ANR-16-CE33-0015) and
   by the Deutsche Forschungsgemeinschaft (DFG) through the Cluster of
   Excellence Engineering of Advanced Materials. We are grateful to the
   CIMI lab for funding a postdoctoral fellowship to Nadine Abu Rumman. We
   would like to thank the reviewers for their insightful comments, and the
   authors wish to express their deepest gratitude to Dr.Bart de Keijzer
   for the careful proof-reading of this paper.
CR Akbay M, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201345
   Akinci N, 2013, COMPUT ANIMAT VIRT W, V24, P195, DOI 10.1002/cav.1499
   Akinci N, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185558
   [Anonymous], 2004, COMPUTER ANIMATION 2, DOI DOI 10.1145/1028523.1028549
   Band S, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3180486
   Batty C., 2011, Proc 2011 ACM SIGGRAPH/Eurograp Symp Comp Anim, P111
   BATTY C, 2007, ACM SIGGRAPH 2007 SI, DOI DOI 10.1145/1275808.1276502
   Becker M, 2007, SYMPOSIUM ON COMPUTER ANIMATION 2007: ACM SIGGRAPH/ EUROGRAPHICS SYMPOSIUM PROCEEDINGS, P209
   Bell N., 2005, P 2005 ACM SIGGRAPH, P77, DOI DOI 10.1145/1073368.1073379
   Bender J., 2015, P 14 ACM SIGGRAPH EU, P147, DOI DOI 10.1145/2786784.2786796
   Bender J, 2014, COMPUT GRAPH FORUM, V33, P228, DOI 10.1111/cgf.12346
   Bockmann A, 2012, COMPUT FLUIDS, V67, P138, DOI 10.1016/j.compfluid.2012.07.007
   Bodin K, 2012, IEEE T VIS COMPUT GR, V18, P516, DOI 10.1109/TVCG.2011.29
   Braley C., 2009, FLUID SIMULATION COM
   Bridgeman R., 2007, ANAL WAINFLEET BOG C, P1
   Carlson M, 2004, ACM T GRAPHIC, V23, P377, DOI 10.1145/1015706.1015733
   Chentanez N., 2006, ACM SIG- GRAPH/Eurographics Symposium on Computer Animation, P83
   Chentanez N, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964977
   Chow AD, 2018, COMPUT PHYS COMMUN, V226, P81, DOI 10.1016/j.cpc.2018.01.005
   Clavet S., 2005, SCA '05, P219, DOI DOI 10.1145/1073368.1073400
   Cornelis J, 2019, VISUAL COMPUT, V35, P579, DOI 10.1007/s00371-018-1488-8
   Cummins SJ, 1999, J COMPUT PHYS, V152, P584, DOI 10.1006/jcph.1999.6246
   Dehnen W, 2012, MON NOT R ASTRON SOC, V425, P1068, DOI 10.1111/j.1365-2966.2012.21439.x
   Desbrun M., 1996, Computer Animation and Simulation '96. Proceedings of the Eurographics Workshop, P61
   F_urstenau J.-P., 2017, P 12 INT SPHER WORKS, P1
   Faure F., 2007, RR6203 INRIA
   Foster N, 1997, COMPUTER GRAPHICS INTERNATIONAL, PROCEEDINGS, P178, DOI 10.1109/CGI.1997.601299
   Fournier A., 1986, Computer Graphics, V20, P75, DOI 10.1145/15886.15894
   Fratarcangeli M, 2015, COMPUT GRAPH FORUM, V34, P405, DOI 10.1111/cgf.12570
   Génevaux O, 2003, PROC GRAPH INTERF, P31
   Gerszewski D, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508430
   Goktekin TG, 2004, ACM T GRAPHIC, V23, P463, DOI 10.1145/1015706.1015746
   Guendelman E, 2005, ACM T GRAPHIC, V24, P973, DOI 10.1145/1073204.1073299
   Harada T, 2007, THEORY PRACTICE COMP
   Ihmsen M., 2014, Eurographics 2014-State of the Art Reports
   Ihmsen M, 2014, IEEE T VIS COMPUT GR, V20, P426, DOI 10.1109/TVCG.2013.105
   Irving G, 2006, ACM T GRAPHIC, V25, P805, DOI 10.1145/1141911.1141959
   Jiang Chenfanfu, 2016, ACM SIGGRAPH 2016 CO, P1
   Kang N, 2014, COMPUT GRAPH FORUM, V33, P219, DOI 10.1111/cgf.12490
   Koschier D, 2017, ACM SIGGRAPH / EUROGRAPHICS SYMPOSIUM ON COMPUTER ANIMATION (SCA 2017), DOI 10.1145/3099564.3099565
   Koshizuka S, 1996, NUCL SCI ENG, V123, P421, DOI 10.13182/NSE96-A24205
   Lenaerts T., 2008, ACM SIGGRAPH 2008 SI, P12, DOI 10.1145/1400885.1400898
   LEVIN DIW, 2011, ACM T GRAPHIC, V30, P1
   Lind SJ, 2012, J COMPUT PHYS, V231, P1499, DOI 10.1016/j.jcp.2011.10.027
   Losasso F, 2008, IEEE T VIS COMPUT GR, V14, P797, DOI 10.1109/TVCG.2008.37
   Lu Wenlong., 2016, P ACM SIGGRAPHEUROGR, P67
   Maciá F, 2011, PROG THEOR PHYS, V125, P1091, DOI 10.1143/PTP.125.1091
   Macklin M, 2016, P 9 INT C MOT GAM, P49, DOI [10.1145/2994258.2994272, DOI 10.1145/2994258.2994272]
   Macklin M, 2014, ACM T GRAPHIC, V33, DOI [10.1145/280/109/2601152, 10.1145/2601097.2601152]
   Macklin M, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461984
   MONAGHAN JJ, 1992, ANNU REV ASTRON ASTR, V30, P543, DOI 10.1146/annurev.aa.30.090192.002551
   Morris JP, 1997, J COMPUT PHYS, V136, P214, DOI 10.1006/jcph.1997.5776
   Müller M, 2007, J VIS COMMUN IMAGE R, V18, P109, DOI 10.1016/j.jvcir.2007.01.005
   Müller M, 2004, COMPUT ANIMAT VIRT W, V15, P159, DOI 10.1002/cav.18
   Muller M., 2003, ACM SIGGRAPH/Eurographics Symposium on Computer Animation, P154
   Nair P, 2015, J COMPUT PHYS, V297, P689, DOI 10.1016/j.jcp.2015.05.042
   Nealen A, 2006, COMPUT GRAPH FORUM, V25, P809, DOI 10.1111/j.1467-8659.2006.01000.x
   Nishida A, 2010, EXPERIENCE DEV OPEN, P448
   Peer A, 2018, COMPUT GRAPH FORUM, V37, P135, DOI 10.1111/cgf.13317
   Premoze S, 2003, COMPUT GRAPH FORUM, V22, P401, DOI 10.1111/1467-8659.00687
   Robinson-Mosher A, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360645
   Robinson-Mosher Avi, 2009, S COMP ANIM, P227
   Schechter H, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185557
   Si H, 2015, ACM T MATH SOFTWARE, V41, DOI 10.1145/2629697
   Solenthaler B, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531346
   Stam J, 1999, COMP GRAPH, P121, DOI 10.1145/311535.311548
   Teng Y, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980229
   Teschner M., 2003, VMV
   Winchenbach R, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073713
   Zarifi O, 2017, ACM SIGGRAPH / EUROGRAPHICS SYMPOSIUM ON COMPUTER ANIMATION (SCA 2017), DOI 10.1145/3099564.3099572
NR 70
TC 1
Z9 1
U1 0
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2020
VL 36
IS 5
BP 893
EP 910
DI 10.1007/s00371-019-01700-y
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA LJ3BL
UT WOS:000530043100003
DA 2024-07-18
ER

PT J
AU Talhaoui, MZ
   Wang, XY
   Midoun, MA
AF Talhaoui, Mohamed Zakariya
   Wang, Xingyuan
   Midoun, Mohamed Amine
TI A new one-dimensional cosine polynomial chaotic map and its use in image
   encryption
SO VISUAL COMPUTER
LA English
DT Article
DE Image encryption; One-dimensional chaotic map; Chaos theory; Secure
   real-time communication; Cryptography
ID APPROXIMATE ENTROPY; ALGORITHM; SYSTEM; WATERMARKING; SCHEME;
   CRYPTANALYSIS; CRYPTOSYSTEM; COMBINATION; DIFFUSION
AB In this paper, we propose a new real one-dimensional cosine polynomial (1-DCP) chaotic map. The statistical analysis of the proposed map shows that it has a simple structure, a high chaotic behavior, and an infinite chaotic range. Therefore, the proposed map is a perfect candidate for the design of chaos-based cryptographic systems. Moreover, we propose an application of the 1-DCP map in the design of a new efficient image encryption scheme (1-DCPIE) to demonstrate the new map further good cryptographic proprieties. In the new scheme, we significantly reduce the encryption process time by raising the small processing unit from the pixels level to the rows/columns level and replacing the classical sequential permutation substitution architecture with a parallel permutation substitution one. We apply several simulation and security tests on the proposed scheme and compare its performances with some recently proposed encryption schemes. The simulation results prove that 1-DCPIE has a better security level and a higher encryption speed.
C1 [Talhaoui, Mohamed Zakariya; Wang, Xingyuan; Midoun, Mohamed Amine] Dalian Univ Technol, Fac Elect Informat & Elect Engn, Dalian 116024, Peoples R China.
   [Wang, Xingyuan] Dalian Maritime Univ, Sch Informat Sci & Technol, Dalian 116026, Peoples R China.
C3 Dalian University of Technology; Dalian Maritime University
RP Wang, XY (corresponding author), Dalian Univ Technol, Fac Elect Informat & Elect Engn, Dalian 116024, Peoples R China.; Wang, XY (corresponding author), Dalian Maritime Univ, Sch Informat Sci & Technol, Dalian 116026, Peoples R China.
EM talhaouizakariya@mail.dlut.edu.cn; wangxy@dlut.edu.cn;
   aminemidoun31@mail.dlut.edu.cn
RI Midoun, Mohamed Amine/AAW-9877-2021; Midoun, Mohamed
   Amine/GZH-0284-2022; Wang, Xing-yuan/I-6353-2015
OI Midoun, Mohamed Amine/0000-0003-4570-4258; Midoun, Mohamed
   Amine/0000-0003-4570-4258; TALHAOUI, Mohamed
   Zakariya/0000-0001-9020-8590
FU National Natural Science Foundation of China [61672124]; Password Theory
   Project of the 13th Five-Year Plan National Cryptography Development
   Fund [MMJJ20170203]; Liaoning Province Science and Technology Innovation
   Leading Talents Program Project [XLYC1802013]; Key RAMP;D Projects of
   Liaoning Province [2019020105-JH2/103]; Jinan City '20 uinversities'
   funding projects Introducing Innovation Team Program [2019GXRC031]
FX This research is supported by the National Natural Science Foundation of
   China (No: 61672124), the Password Theory Project of the 13th Five-Year
   Plan National Cryptography Development Fund (No: MMJJ20170203), Liaoning
   Province Science and Technology Innovation Leading Talents Program
   Project (No: XLYC1802013), Key R&D Projects of Liaoning Province (No:
   2019020105-JH2/103). Jinan City '20 uinversities' funding projects
   Introducing Innovation Team Program (No:2019GXRC031)
CR Alvarez G, 2006, INT J BIFURCAT CHAOS, V16, P2129, DOI 10.1142/S0218127406015970
   [Anonymous], 1985, IEEE STAND BIN FLOAT
   [Anonymous], 2017, IEEE T CYBERN
   Boriga R, 2014, ADV MULTIMED, V2014, DOI 10.1155/2014/409586
   Cao LJ, 2013, VISUAL COMPUT, V29, P231, DOI 10.1007/s00371-012-0732-x
   Castro JCH, 2005, MATH COMPUT SIMULAT, V68, P1, DOI 10.1016/j.matcom.2004.09.001
   Chang HT, 2005, APPL OPTICS, V44, P6211, DOI 10.1364/AO.44.006211
   Chen JX, 2018, NONLINEAR DYNAM, V93, P2399, DOI 10.1007/s11071-018-4332-9
   Chen JX, 2018, SIGNAL PROCESS, V142, P340, DOI 10.1016/j.sigpro.2017.07.034
   Chen SH, 2002, PHYS LETT A, V299, P353, DOI 10.1016/S0375-9601(02)00522-4
   Ding HY, 2018, CHINESE J ELECTRON, V27, P150, DOI 10.1049/cje.2017.10.004
   Ernawan F, 2020, VISUAL COMPUT, V36, P19, DOI 10.1007/s00371-018-1567-x
   Hu YQ, 2017, INT J THEOR PHYS, V56, P2192, DOI 10.1007/s10773-017-3365-z
   Hua Z., 2019, IEEE T SYSTEMS MAN C
   Hua ZY, 2020, IEEE T IND INFORM, V16, P887, DOI 10.1109/TII.2019.2923553
   Hua ZY, 2019, INFORM SCIENCES, V480, P403, DOI 10.1016/j.ins.2018.12.048
   Hua ZY, 2019, IEEE ACCESS, V7, P8660, DOI 10.1109/ACCESS.2018.2890116
   Hua ZY, 2016, INFORM SCIENCES, V339, P237, DOI 10.1016/j.ins.2016.01.017
   Huang LQ, 2018, ENTROPY-SWITZ, V20, DOI 10.3390/e20070535
   Kaur M, 2018, MOD PHYS LETT B, V32, DOI 10.1142/S0217984918501154
   KAY S, 1995, IEEE T SIGNAL PROCES, V43, P2013, DOI 10.1109/78.403367
   Li CQ, 2014, NONLINEAR DYNAM, V78, P1545, DOI 10.1007/s11071-014-1533-8
   Li GD, 2019, VISUAL COMPUT, V35, P1267, DOI 10.1007/s00371-018-1574-y
   Ling C, 1999, IEEE T SIGNAL PROCES, V47, P1424, DOI 10.1109/78.757236
   Liu HJ, 2017, IET IMAGE PROCESS, V11, P324, DOI 10.1049/iet-ipr.2016.0040
   Liu LF, 2018, MULTIMED TOOLS APPL, V77, P21445, DOI 10.1007/s11042-017-5594-9
   Liu X, 2018, CHINESE J ELECTRON, V27, P137
   Liu ZL, 2018, INFORM SCIENCES, V433, P188, DOI 10.1016/j.ins.2017.12.044
   Luong M, 2011, J GLOBAL OPTIM, V49, P435, DOI 10.1007/s10898-010-9570-4
   Muhammad K, 2018, IEEE T IND INFORM, V14, P3679, DOI 10.1109/TII.2018.2791944
   Pak C, 2017, SIGNAL PROCESS, V138, P129, DOI 10.1016/j.sigpro.2017.03.011
   PINCUS S, 1995, CHAOS, V5, P110, DOI 10.1063/1.166092
   PINCUS SM, 1991, P NATL ACAD SCI USA, V88, P2297, DOI 10.1073/pnas.88.6.2297
   Seo JS, 2004, PATTERN RECOGN, V37, P1365, DOI 10.1016/j.patcog.2003.12.013
   SHANNON CE, 1949, BELL SYST TECH J, V28, P656, DOI 10.1002/j.1538-7305.1949.tb00928.x
   Stallings W., 2011, CRYPTOGRAPHY NETWORK
   Tang JY, 2019, MULTIMED TOOLS APPL, V78, P24765, DOI 10.1007/s11042-019-7602-8
   Vaidyanathan S, 2018, EUR PHYS J PLUS, V133, DOI 10.1140/epjp/i2018-11872-8
   Wang B, 2013, OPTIK, V124, P1773, DOI 10.1016/j.ijleo.2012.06.020
   Wang C, 2018, OPT COMMUN, V407, P1, DOI 10.1016/j.optcom.2017.08.054
   Wang MX, 2019, OPT LASER ENG, V121, P479, DOI 10.1016/j.optlaseng.2019.05.013
   Wang MX, 2018, OPT LASER TECHNOL, V108, P558, DOI 10.1016/j.optlastec.2018.07.052
   Wang X, 2019, NEUROSCI LETT, V699, P1, DOI 10.1016/j.neulet.2019.01.028
   Wang XY, 2020, INFORM SCIENCES, V507, P16, DOI 10.1016/j.ins.2019.08.041
   Wang XY, 2019, MULTIMED TOOLS APPL, V78, P6191, DOI 10.1007/s11042-018-6326-5
   Wang Y, 2009, CHAOS SOLITON FRACT, V41, P1773, DOI 10.1016/j.chaos.2008.07.031
   Wen WY, 2017, NONLINEAR DYNAM, V87, P383, DOI 10.1007/s11071-016-3049-x
   WOLF A, 1985, PHYSICA D, V16, P285, DOI 10.1016/0167-2789(85)90011-9
   Wu Y, 2013, INFORM SCIENCES, V222, P323, DOI 10.1016/j.ins.2012.07.049
   Xiao F, 2020, IEEE T EMERG TOP COM, V8, P752, DOI [10.1109/JPHOT.2018.2827165, 10.1109/TETC.2018.2790080]
   Xu JY, 2015, VISUAL COMPUT, V31, P1653, DOI 10.1007/s00371-014-1045-z
   Yang ZS, 2018, INT J APPL ELECTROM, V57, P439, DOI 10.3233/JAE-170116
   Yao SY, 2017, OPT LASER TECHNOL, V97, P234, DOI 10.1016/j.optlastec.2017.07.005
   Zhang XQ, 2017, OPT LASER ENG, V92, P6, DOI 10.1016/j.optlaseng.2016.12.005
   Zhang Yan, 2019, Comput Assist Surg (Abingdon), P1, DOI 10.1080/24699322.2018.1560090
   Zhang YS, 2018, IEEE INTERNET THINGS, V5, P3442, DOI 10.1109/JIOT.2017.2781737
   Zhang YS, 2015, NONLINEAR DYNAM, V82, P1831, DOI 10.1007/s11071-015-2280-1
   Zhou RG, 2018, INT J THEOR PHYS, V57, P300, DOI 10.1007/s10773-017-3576-3
NR 58
TC 60
Z9 63
U1 3
U2 48
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2021
VL 37
IS 3
BP 541
EP 551
DI 10.1007/s00371-020-01822-8
EA MAR 2020
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA QY5QT
UT WOS:000564536400001
DA 2024-07-18
ER

PT J
AU Joseph, A
   Geetha, P
AF Joseph, Allen
   Geetha, P.
TI Facial emotion detection using modified eyemap-mouthmap algorithm on an
   enhanced image and classification with tensorflow
SO VISUAL COMPUTER
LA English
DT Article
DE Emotion; Eyemap; Facial expression; Facial geometry; Mouthmap
ID EXPRESSION RECOGNITION; FEATURE-EXTRACTION; REPRESENTATION; NETWORK
AB Detection of emotion using facial expression is a growing field of research. Facial expression detection is also helpful to identify the behavior of a person when a man interacts with the computer. In this work, facial expression recognition with respect to the changes in the facial geometry is proposed. First, the image is enhanced by means of discrete wavelet transform and fuzzy combination. Then, the facial geometry is found using the modified eyemap and mouthmap algorithm after finding the landmarks. Finally, the area and angle of the constructed triangles are found and classified using neural network with the help of tensorflow central processing unit version. Results show that the proposed algorithm is efficient in finding the facial emotion.
C1 [Joseph, Allen; Geetha, P.] Anna Univ, Dept Comp Sci & Engn, Coll Engn, Chennai, Tamil Nadu, India.
C3 Anna University; Anna University Chennai
RP Joseph, A (corresponding author), Anna Univ, Dept Comp Sci & Engn, Coll Engn, Chennai, Tamil Nadu, India.
EM allenjoseph1985@gmail.com; geethaplanisamy@gmail.com
RI P, Geetha/ADG-6534-2022; Joseph, Allen/AAG-1297-2019
OI P, Geetha/0000-0002-5637-3856; Joseph, Allen/0000-0001-9638-8799
FU Department of Science and Technology-Promotion of University Research
   and Scientific Excellence (DST-PURSE) Phase II Program, India
FX This work was funded by the Department of Science and
   Technology-Promotion of University Research and Scientific Excellence
   (DST-PURSE) Phase II Program, India.
CR Agarwal S, 2018, VISUAL COMPUT, V34, P177, DOI 10.1007/s00371-016-1323-z
   Alugupally N., 2011, Pattern Recognition and Image Analysis, V21, P681, DOI 10.1134/S105466181104002X
   [Anonymous], 1998, KAROLINSKA DIRECTED
   [Anonymous], 19 ED BRAZ C AUT CBA
   Ariel R.G., 2018, Proceedings of the IEEE International Joint Conference on Neural Networks (IJCNN), P1
   Buciu I, 2009, SIGNAL IMAGE VIDEO P, V3, P345, DOI 10.1007/s11760-008-0074-3
   Chan TF, 2001, IEEE T IMAGE PROCESS, V10, P266, DOI 10.1109/83.902291
   Ekman P, 1978, FACIAL ACTION CODING
   Gogic I, 2020, VISUAL COMPUT, V36, P97, DOI 10.1007/s00371-018-1585-8
   Happy SL, 2015, IEEE T AFFECT COMPUT, V6, P1, DOI 10.1109/TAFFC.2014.2386334
   Hsu RL, 2002, IEEE T PATTERN ANAL, V24, P696, DOI 10.1109/34.1000242
   Huang CL, 1997, J VIS COMMUN IMAGE R, V8, P278, DOI 10.1006/jvci.1997.0359
   Ilbeygi M, 2012, ENG APPL ARTIF INTEL, V25, P130, DOI 10.1016/j.engappai.2011.07.004
   Jackway PT, 1996, IEEE T PATTERN ANAL, V18, P38, DOI 10.1109/34.476009
   Jain V., 2015, Pattern Recognition and Image Analysis, V25, P430, DOI 10.1134/S1054661815030086
   Kanade T., 2000, Proceedings Fourth IEEE International Conference on Automatic Face and Gesture Recognition (Cat. No. PR00580), P46, DOI 10.1109/AFGR.2000.840611
   Karthigayan M, 2007, ARTIF LIFE ROBOT, V11, P197, DOI 10.1007/s10015-007-0428-x
   Kim D. J., 2016, Pattern Recognition and Image Analysis, V26, P576
   Lajevardi SM, 2012, SIGNAL IMAGE VIDEO P, V6, P159, DOI 10.1007/s11760-010-0177-5
   Lekdioui K, 2017, SIGNAL PROCESS-IMAGE, V58, P300, DOI 10.1016/j.image.2017.08.001
   Liu N, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P1897, DOI 10.1109/ICASSP.2018.8461322
   Lucey P., 2010, IEEE COMP SOC C COMP, P94, DOI 10.1109/CVPRW.2010.5543262
   Lyons M, 1998, AUTOMATIC FACE AND GESTURE RECOGNITION - THIRD IEEE INTERNATIONAL CONFERENCE PROCEEDINGS, P200, DOI 10.1109/AFGR.1998.670949
   Mayer C., 2014, Pattern Recognition and Image Analysis, V24, P124, DOI 10.1134/S1054661814010106
   Mlakar U, 2015, SIGNAL IMAGE VIDEO P, V9, P245, DOI 10.1007/s11760-015-0810-4
   Panda SP, 2016, 2016 IEEE STUDENTS' CONFERENCE ON ELECTRICAL, ELECTRONICS AND COMPUTER SCIENCE (SCEECS)
   Sun Z, 2017, SIGNAL IMAGE VIDEO P, V11, P597, DOI 10.1007/s11760-016-0999-x
   Viola P, 2001, PROC CVPR IEEE, P511, DOI 10.1109/cvpr.2001.990517
   Wong JJ, 2010, NEURAL COMPUT APPL, V19, P33, DOI 10.1007/s00521-008-0225-z
   Yaddaden Y, 2018, EXPERT SYST APPL, V112, P173, DOI 10.1016/j.eswa.2018.06.033
   Yang HY, 2018, PROC CVPR IEEE, P2168, DOI 10.1109/CVPR.2018.00231
   Yu ZB, 2018, VISUAL COMPUT, V34, P1691, DOI 10.1007/s00371-017-1443-0
   Zhao GY, 2011, IMAGE VISION COMPUT, V29, P607, DOI 10.1016/j.imavis.2011.07.002
NR 33
TC 23
Z9 24
U1 0
U2 23
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2020
VL 36
IS 3
BP 529
EP 539
DI 10.1007/s00371-019-01628-3
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KL4EO
UT WOS:000513378500007
DA 2024-07-18
ER

PT J
AU Chermain, X
   Claux, F
   Mérillou, S
AF Chermain, Xavier
   Claux, Frederic
   Merillou, Stephane
TI A microfacet-based BRDF for the accurate and efficient rendering of
   high-definition specular normal maps
SO VISUAL COMPUTER
LA English
DT Article
DE Microfacet; BRDF; Specular normal maps; Microstructures; Glints
AB Complex specular microstructures found in glittery, scratched or brushed metal materials exhibit high-frequency variations in reflected light intensity. These variations are important for the human eye and give materials their uniqueness and personality. To model such microsurfaces, high-definition normal maps are very effective. The works of Yan et al. (ACM Trans Graph 33(4):116:1-116:9, 2014; ACM Trans Graph 35(4):56:1-56:9, 2016) enable the rendering of such material representations by evaluating a microfacet-based BRDF related to a whole ray footprint. Still, in specific configurations and especially at grazing angles, their method does not fully capture the expected material appearance. We propose to build upon their work and tackle the problem of accuracy using a more physically based reflection model. To do so, the normal map is approximated with a mixture of anisotropic, noncentered Beckmann normal distribution functions from which a closed form for the masking-shadowing term can be derived. Based on our formal definition, we provide a fast approximation leading to a performance overhead varying from 5 to 20% compared to the method of Yan et al. (2016). Our results show that we more closely match ground truth renderings than their methods.
C1 [Chermain, Xavier; Claux, Frederic; Merillou, Stephane] Univ Limoges, CNRS, XLIM, UMR 7252, F-87000 Limoges, France.
C3 Universite de Limoges; Centre National de la Recherche Scientifique
   (CNRS); CNRS - Institute for Engineering & Systems Sciences (INSIS)
RP Chermain, X (corresponding author), Univ Limoges, CNRS, XLIM, UMR 7252, F-87000 Limoges, France.
EM xavier.chermain@unilim.fr; frederic.claux@unilim.fr;
   stephane.merillou@unilim.fr
OI Chermain, Xavier/0000-0003-1910-5956
CR [Anonymous], 2016, PHYS BASED RENDERING
   [Anonymous], 2014, J COMPUT GRAPH TECHN
   Atanasov A., 2016, ACM SIGGRAPH 2016 TA
   Belcour L, 2017, ACM T GRAPHIC, V36, DOI 10.1145/2990495
   Bosch C, 2004, COMPUT GRAPH FORUM, V23, P361, DOI 10.1111/j.1467-8659.2004.00767.x
   Bosch C, 2010, COMPUT GRAPH-UK, V34, P430, DOI 10.1016/j.cag.2010.04.001
   Dupuy J, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508422
   Han C, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239479
   Heckbert Paul S., 1989, THESIS
   Igehy H, 1999, COMP GRAPH, P179, DOI 10.1145/311535.311555
   LEWIS RR, 1994, COMPUT GRAPH FORUM, V13, P109, DOI 10.1111/1467-8659.1320109
   Merillou S, 2001, VISUAL COMPUT, V17, P30, DOI 10.1007/s003710000093
   OLANO M, 2010, LEAN MAPPING, P181, DOI [10.1145/1730804.1730834, DOI 10.1145/1730804.1730834]
   Raymond B, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925945
   Ren B, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2645703
   SMITH BG, 1967, IEEE T ANTENN PROPAG, VAP15, P668, DOI 10.1109/TAP.1967.1138991
   Suykens F, 2001, SPRING EUROGRAP, P257
   Toksvig M., 2005, Journal of Graphics Tools, V10, P65
   Walter B., 2007, EUROGRAPHICS C RENDE
   Xu C, 2017, COMPUT GRAPH FORUM, V36, P27, DOI 10.1111/cgf.13221
   Yan LQ, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201351
   Yan LQ, 2015, ACM T GRAPHIC, V35, DOI 10.1145/2816814
   Yan ZC, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2790296
NR 23
TC 6
Z9 6
U1 1
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2020
VL 36
IS 2
BP 267
EP 277
DI 10.1007/s00371-018-1606-7
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KJ2TH
UT WOS:000511910300004
DA 2024-07-18
ER

PT J
AU Bandara, R
   Ranathunga, L
   Abdullah, NA
AF Bandara, Ravimal
   Ranathunga, Lochandaka
   Abdullah, Nor Aniza
TI Deep learned compact binary descriptor with a lightweight
   network-in-network architecture for visual description
SO VISUAL COMPUTER
LA English
DT Article
DE Binary descriptor; Network-in-network; Restricted Boltzmann machine;
   Correspondence matching; Lightweight deep neural network
ID IMAGE DESCRIPTORS; MODEL; FEATURES; OBJECT; CODES
AB Binary descriptors have been widely used for real-time image retrieval and correspondence matching. However, most of the learned descriptors are obtained using a large deep neural network (DNN) with several million parameters, and the learned binary codes are generally not invariant to many geometrical variances which is crucial for accurate correspondence matching. To address this problem, we proposed a new learning approach using a lightweight DNN architecture via a stack of multiple multilayer perceptrons based on the network in network (NIN) architecture, and a restricted Boltzmann machine (RBM). The latter is used for mapping the features to binary codes, and carry out the geometrically invariant correspondence matching task. Our experimental results on several benchmark datasets (e.g., Brown, Oxford, Paris, INRIA Holidays, RomePatches, HPatches, and CIFAR-10) show that the proposed approach produces the learned binary descriptor that outperforms other baseline self-supervised binary descriptors in terms of correspondence matching despite the smaller size of its DNN. Most importantly, the proposed approach does not freeze the features that are obtained while pre-training the NIN model. Instead, it fine-tunes the features while learning the features needed for binary mapping through the RBM. Additionally, its lightweight architecture makes it suitable for resource-constrained devices.
C1 [Bandara, Ravimal; Ranathunga, Lochandaka] Univ Moratuwa, Moratuwa, Sri Lanka.
   [Abdullah, Nor Aniza] Univ Malaya, Fac Comp Sci & Informat Technol, Kuala Lumpur, Malaysia.
C3 University Moratuwa; Universiti Malaya
RP Bandara, R (corresponding author), Univ Moratuwa, Moratuwa, Sri Lanka.
EM ravimalb@uom.lk; lochandaka@uom.lk; noraniza@um.edu.my
RI Ranathunga, Lochandaka/K-2894-2019; Abdullah, Nor Aniza/B-2768-2010
OI Ranathunga, Lochandaka/0000-0002-9854-0317; Amarakoon Mudiyanselage,
   Randitha Ravimal Bandara/0000-0001-8622-9049; Abdullah, Nor
   Aniza/0000-0001-6218-8772
FU Senate Research Council, University of Moratuwa, Sri Lanka [SRC-16-1];
   National Research Council, Sri Lanka [12-017]
FX This study was funded by the Senate Research Council, University of
   Moratuwa, Sri Lanka (Grant No. SRC-16-1), and National Research Council,
   Sri Lanka (Grant No. 12-017).
CR Abadi Martin, 2016, TENSORFLOW LARGE SCA, V16, P265
   Alahi A, 2012, PROC CVPR IEEE, P510, DOI 10.1109/CVPR.2012.6247715
   Ali M, 2019, VISUAL COMPUT, V35, P1013, DOI 10.1007/s00371-019-01673-y
   Andoni A, 2006, ANN IEEE SYMP FOUND, P459
   Babenko A, 2014, LECT NOTES COMPUT SC, V8689, P584, DOI 10.1007/978-3-319-10590-1_38
   Balntas V, 2020, IEEE T PATTERN ANAL, V42, P2825, DOI 10.1109/TPAMI.2019.2915233
   Bandara AMRR, 2013, INT CONF IND INF SYS, P304, DOI 10.1109/ICIInfS.2013.6732000
   BANDARA R, 2019, IJATCSE, V8, P696
   Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014
   Björkman M, 2014, COMPUT VIS IMAGE UND, V118, P111, DOI 10.1016/j.cviu.2013.10.007
   Brown M, 2011, IEEE T PATTERN ANAL, V33, P43, DOI 10.1109/TPAMI.2010.54
   Calonder M, 2010, LECT NOTES COMPUT SC, V6314, P778, DOI 10.1007/978-3-642-15561-1_56
   Chamasemani FF, 2018, VISUAL COMPUT, V34, P1299, DOI 10.1007/s00371-017-1432-3
   Chen L, 2019, VISUAL COMPUT, V35, P1361, DOI 10.1007/s00371-018-01615-0
   Chum O, 2007, IEEE I CONF COMP VIS, P496, DOI 10.1109/cvpr.2007.383172
   Duan YQ, 2017, PROC CVPR IEEE, P4857, DOI 10.1109/CVPR.2017.516
   Fan B, 2014, IEEE T IMAGE PROCESS, V23, P2583, DOI 10.1109/TIP.2014.2317981
   Gong YC, 2013, IEEE T PATTERN ANAL, V35, P2916, DOI 10.1109/TPAMI.2012.193
   Han XF, 2015, PROC CVPR IEEE, P3279, DOI 10.1109/CVPR.2015.7298948
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Jegou H, 2008, LECT NOTES COMPUT SC, V5302, P304, DOI 10.1007/978-3-540-88682-2_24
   Jégou H, 2012, IEEE T PATTERN ANAL, V34, P1704, DOI 10.1109/TPAMI.2011.235
   Kabbai L, 2019, VISUAL COMPUT, V35, P679, DOI 10.1007/s00371-018-1503-0
   Kalpana J, 2016, MULTIMED TOOLS APPL, V75, P49, DOI 10.1007/s11042-014-2262-1
   Kim I, 2013, INT SOC DESIGN CONF, P17, DOI 10.1109/ISOCC.2013.6863974
   Krizhevsky A., 2009, LEARNING MULTIPLE LA, DOI DOI 10.1145/3065386
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kumar BGV, 2016, PROC CVPR IEEE, P5385, DOI 10.1109/CVPR.2016.581
   Lai HJ, 2015, PROC CVPR IEEE, P3270, DOI 10.1109/CVPR.2015.7298947
   Leutenegger S, 2011, IEEE I CONF COMP VIS, P2548, DOI 10.1109/ICCV.2011.6126542
   Lin K, 2019, IEEE T PATTERN ANAL, V41, P1501, DOI 10.1109/TPAMI.2018.2833865
   Lin KV, 2016, PROC CVPR IEEE, P1183, DOI 10.1109/CVPR.2016.133
   Lin M, 2014, PUBLIC HEALTH NUTR, V17, P2029, DOI [10.1109/PLASMA.2013.6634954, 10.1017/S1368980013002176]
   Liu HM, 2016, PROC CVPR IEEE, P2064, DOI 10.1109/CVPR.2016.227
   Liu JC, 2019, VISUAL COMPUT, V35, P909, DOI 10.1007/s00371-019-01679-6
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Luo J., 2009, INT J IMAGE PROCESSI, V3, P143, DOI DOI 10.1007/S11270-006-2859-8
   Luo X, 2018, PROCEEDINGS OF 2018 THE 2ND INTERNATIONAL CONFERENCE ON COMPUTER SCIENCE AND ARTIFICIAL INTELLIGENCE (CSAI 2018) / 2018 THE 10TH INTERNATIONAL CONFERENCE ON INFORMATION AND MULTIMEDIA TECHNOLOGY (ICIMT 2018), P168, DOI 10.1145/3297156.3297272
   Markus N, 2019, IEEE T IMAGE PROCESS, V28, P279, DOI 10.1109/TIP.2018.2867270
   Mopuri Konda Reddy, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P62, DOI 10.1109/CVPRW.2015.7301273
   Paulin M, 2015, IEEE I CONF COMP VIS, P91, DOI 10.1109/ICCV.2015.19
   Philbin J, 2008, PROC CVPR IEEE, P2285
   Ranathunga L, 2011, MULTIMED TOOLS APPL, V54, P263, DOI 10.1007/s11042-010-0522-2
   Razavian AS, 2014, IEEE COMPUT SOC CONF, P512, DOI 10.1109/CVPRW.2014.131
   Rosten E., 2006, P 2006 9 EUR C COMP, P430, DOI DOI 10.1007/11744023_34
   Rublee E, 2011, IEEE I CONF COMP VIS, P2564, DOI 10.1109/ICCV.2011.6126544
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Salakhutdinov R, 2009, INT J APPROX REASON, V50, P969, DOI 10.1016/j.ijar.2008.11.006
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sinha A, 2014, MACH VISION APPL, V25, P361, DOI 10.1007/s00138-013-0561-6
   Strecha C, 2012, IEEE T PATTERN ANAL, V34, P66, DOI 10.1109/TPAMI.2011.103
   Szegedy Christian, 2016, P IEEE C COMP VIS PA, DOI DOI 10.1109/CVPR.2016.308
   Tian Y, 2019, IEEE C EVOL COMPUTAT, P1710, DOI [10.1109/CEC.2019.8789953, 10.1109/cec.2019.8789953]
   Tian YR, 2017, PROC CVPR IEEE, P6128, DOI 10.1109/CVPR.2017.649
   Trzcinski T, 2015, IEEE T PATTERN ANAL, V37, P597, DOI 10.1109/TPAMI.2014.2343961
   Trzcinski T, 2013, PROC CVPR IEEE, P2874, DOI 10.1109/CVPR.2013.370
   Trzcinski T, 2012, LECT NOTES COMPUT SC, V7572, P228, DOI 10.1007/978-3-642-33718-5_17
   Vinyals O, 2016, 30 C NEURAL INFORM P, V29
   Wan J, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P157, DOI 10.1145/2647868.2654948
   XIA R, 2014, 28 AAAI C ART INT AS
   Yang X, 2014, IEEE T PATTERN ANAL, V36, P188, DOI 10.1109/TPAMI.2013.150
   Zagoruyko S., 2015, PROC CVPR IEEE, P4353, DOI DOI 10.1109/CVPR.2015.7299064
   Zbontar J, 2016, J MACH LEARN RES, V17
   Zhang SL, 2014, IEEE T IMAGE PROCESS, V23, P3671, DOI 10.1109/TIP.2014.2330794
   Zheng L, 2014, IEEE T IMAGE PROCESS, V23, P3368, DOI 10.1109/TIP.2014.2330763
NR 65
TC 4
Z9 4
U1 0
U2 9
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2021
VL 37
IS 2
BP 275
EP 290
DI 10.1007/s00371-020-01798-5
EA JAN 2020
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA QR4KJ
UT WOS:000510306100002
DA 2024-07-18
ER

PT J
AU Hosseinabad, SH
   Safayani, M
   Mirzaei, A
AF Hosseinabad, Sayedshayan Hashemi
   Safayani, Mehran
   Mirzaei, Abdolreza
TI Multiple answers to a question: a new approach for visual question
   answering
SO VISUAL COMPUTER
LA English
DT Article
DE Visual question answering; Deep learning; Convolution neural network;
   Multiple answers; Recurrent neural network
AB With the advent of deep learning, multi-modal data have been of great interest. One of the multi-modal tasks which can be included in the computer vision domain is visual question answering (VQA). In VQA, a question and an image are entered into the model and the model tries to answer the question according to the image. To the best of our knowledge, the current techniques look at the image and only give one answer to the question asked. However, in some situations, there are several answers to the asked question. In this paper, we address this problem and define a new domain in the task of VQA as well as a new computationally efficient approach to cope with multiple-answer VQA. In this approach, we use a sliding window in an efficient manner to examine the answer to the question in different parts of the image. Due to the fact that so far no proper dataset is available for multiple-answer VQA, we provide a new dataset for evaluating our proposed model. The experiments express that our model uses 94% less operation than other models, making it very suitable for real-time applications.
C1 [Hosseinabad, Sayedshayan Hashemi; Safayani, Mehran; Mirzaei, Abdolreza] Isfahan Univ Technol, Dept Elect & Comp Engn, Esfahan 8415683111, Iran.
C3 Isfahan University of Technology
RP Safayani, M (corresponding author), Isfahan Univ Technol, Dept Elect & Comp Engn, Esfahan 8415683111, Iran.
EM shayan.hashemi@ec.iut.ac.ir; safayani@cc.iut.ac.ir; mirzaei@cc.iut.ac.ir
CR [Anonymous], COMPUT VIS PATTERN R
   [Anonymous], 2013, P 15 INT ACM SIGACCE, DOI DOI 10.1145/2513383.2517033
   Cheng Y, 2018, INT CONF INFO SCI, P472, DOI 10.1109/ICIST.2018.8426080
   Cho K., 2014, ARXIV14061078
   Dong XP, 2019, IEEE T IMAGE PROCESS, V28, P3516, DOI 10.1109/TIP.2019.2898567
   Dong XW, 2018, IEEE CONF COMPUT
   Fukui A., 2016, ARXIVPREPRINTARXIV16
   Goyal Y., 2016, ARXIVPREPRINTARXIV16
   Gupta A.K, 2017, CORRARXIV170503865
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Hripcsak G, 2005, J AM MED INFORM ASSN, V12, P296, DOI 10.1197/jamia.M1733
   Kabbai L, 2019, VISUAL COMPUT, V35, P679, DOI 10.1007/s00371-018-1503-0
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   LeCun Y, 1999, LECT NOTES COMPUT SC, V1681, P319, DOI 10.1007/3-540-46805-6_19
   Liu XX, 2019, VISUAL COMPUT, V35, P445, DOI 10.1007/s00371-018-1566-y
   Lu JH, 2016, PROCEEDINGS OF THE 2015 INTERNATIONAL CONFERENCE ON COMPUTER SCIENCE AND ENGINEERING TECHNOLOGY (CSET2015), MEDICAL SCIENCE AND BIOLOGICAL ENGINEERING (MSBE2015), P289
   Ma L, 2016, AAAI CONF ARTIF INTE, P3567
   Malinowski M, 2015, IEEE I CONF COMP VIS, P1, DOI 10.1109/ICCV.2015.9
   Malinowski Mateusz., 2014, Advances in neural information processing systems, P1682
   Noh H, 2016, PROC CVPR IEEE, P30, DOI 10.1109/CVPR.2016.11
   Ren Mengye, 2015, NIPS, P2953
   Shih KJ, 2016, PROC CVPR IEEE, P4613, DOI 10.1109/CVPR.2016.499
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Szegedy C, 2014, Arxiv, DOI [arXiv:1312.6199, DOI 10.1109/CVPR.2015.7298594]
   Tripathi G, 2019, VISUAL COMPUT, V35, P753, DOI 10.1007/s00371-018-1499-5
   Wang WG, 2021, IEEE T PATTERN ANAL, V43, P220, DOI 10.1109/TPAMI.2019.2924417
   Wang WG, 2018, IEEE T IMAGE PROCESS, V27, P38, DOI 10.1109/TIP.2017.2754941
   Wang WG, 2019, IEEE T PATTERN ANAL, V41, P1531, DOI 10.1109/TPAMI.2018.2840724
   Wang WG, 2018, IEEE T IMAGE PROCESS, V27, P2368, DOI 10.1109/TIP.2017.2787612
   Yang ZC, 2016, PROC CVPR IEEE, P21, DOI 10.1109/CVPR.2016.10
   Zhou B., 2015, ARXIVPREPRINTARXIV15
NR 31
TC 15
Z9 16
U1 0
U2 9
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2021
VL 37
IS 1
BP 119
EP 131
DI 10.1007/s00371-019-01786-4
EA JAN 2020
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA QE3UO
UT WOS:000574081100004
DA 2024-07-18
ER

PT J
AU Kumar, S
   Bhuyan, MK
   Iwahori, Y
AF Kumar, Sunil
   Bhuyan, M. K.
   Iwahori, Yuji
TI Multi-level uncorrelated discriminative shared Gaussian process for
   multi-view facial expression recognition
SO VISUAL COMPUTER
LA English
DT Article
DE Facial expression recognition; Multi-view learning; Local binary
   pattern; Local fisher discriminant analysis
AB In multi-view facial expression recognition, discriminative shared Gaussian process latent variable model (DS-GPLVM) gives better performance than that of linear and nonlinear multi-view learning-based methods. However, Laplacian-based prior used in DS-GPLVM only captures topological structure of data space without considering the inter-class separability of the data, and hence the obtained latent space is suboptimal. So, we propose a multi-level uncorrelated DS-GPLVM (ML-UDSGPLVM) model which searches a common uncorrelated discriminative latent space learned from multiple observable spaces. A novel prior is proposed, which not only depends on the topological structure of the intra-class data, but also on the local-between-class-scatter-matrix of the data onto the latent manifold. The proposed approach employs an hierarchical framework, in which, expressions are first divided into three sub-categories. Subsequently, each of the sub-categories are further classified to identify the constituent basic expressions. Experimental results show that the proposed method outperforms state-of-the-art methods in many instances.
C1 [Kumar, Sunil] ABV IIITM Gwalior, Gwalior 474015, India.
   [Bhuyan, M. K.] IIT Guwahati, Dept Elect & Elect Engn, Gauhati 781039, India.
   [Iwahori, Yuji] Chubu Univ, Dept Comp Sci, Kasugai, Aichi 4878501, Japan.
C3 ABV-Indian Institute of Information Technology & Management, Gwalior;
   Indian Institute of Technology System (IIT System); Indian Institute of
   Technology (IIT) - Guwahati; Chubu University
RP Kumar, S (corresponding author), ABV IIITM Gwalior, Gwalior 474015, India.
EM snk@iiitm.ac.in; inkb@iitg.ernet.in; iwahori@isc.chubu.ac.jp
RI Kumar, Sunil/AAS-9326-2020; Bhuyan, Manoj Kumar/D-1562-2012; Kumar,
   Sunil/GYV-0347-2022; Iwahori, Yuji/AAH-4257-2020
OI Kumar, Sunil/0000-0002-7996-6427; Kumar, Sunil/0000-0002-1953-6273;
   Iwahori, Yuji/0000-0002-6421-8186
CR [Anonymous], 2005, P ADV NEUR INF PROC
   [Anonymous], 1976, Pictures of facial affect
   [Anonymous], 2012, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2012.6247923
   [Anonymous], 2007, P 24 INT C MACHINE L
   Bertsekas D.P., 2014, Constrained Optimization and Lagrange Multiplier Methods, DOI DOI 10.1016/B978-0-12-093480-5.50005-2
   BETTADAPURA V, 2012, COMPUTER SCI
   Bishop Christopher M, 2006, PATTERN RECOGNITION, DOI DOI 10.18637/JSS.V017.B05
   Chung F. R., 1997, Spectral Graph Theory, V92, DOI DOI 10.1090/CBMS/092
   Dhall A, 2011, 2011 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCV WORKSHOPS)
   Ek Carl Henrik, 2009, PhD Thesis
   Eleftheriadis S, 2015, IEEE T IMAGE PROCESS, V24, P189, DOI 10.1109/TIP.2014.2375634
   Eleftheriadis S, 2014, LECT NOTES COMPUT SC, V8888, P292, DOI 10.1007/978-3-319-14364-4_28
   Fanti C, 2004, ADV NEUR IN, V16, P1603
   GOODALL C, 1991, J ROY STAT SOC B MET, V53, P285, DOI 10.1111/j.2517-6161.1991.tb01825.x
   He XF, 2004, ADV NEUR IN, V16, P153
   Hesse N, 2012, INT C PATT RECOG, P3533
   Hotelling H, 1936, BIOMETRIKA, V28, P321, DOI 10.1093/biomet/28.3-4.321
   Hu P, 2018, KNOWL-BASED SYST, V149, P34, DOI 10.1016/j.knosys.2018.02.008
   Hu Y, 2008, PROC CVPR IEEE, P85
   Hu YX, 2008, INT C PATT RECOG, P460
   Kan MN, 2016, IEEE T PATTERN ANAL, V38, P188, DOI 10.1109/TPAMI.2015.2435740
   Kang Z, 2020, IEEE T CYBERNETICS, V50, P1833, DOI 10.1109/TCYB.2018.2887094
   Khorrami P, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P19, DOI 10.1109/ICCVW.2015.12
   Kim BK, 2016, J MULTIMODAL USER IN, V10, P173, DOI 10.1007/s12193-015-0209-0
   Kumar SU, 2017, NEURAL COMPUT APPL, V28, P3239, DOI 10.1007/s00521-016-2236-5
   Kumar S, 2016, IET COMPUT VIS, V10, P567, DOI 10.1049/iet-cvi.2015.0273
   LAWRENCE N., 2006, P 23 INT C MACHINE L, DOI [DOI 10.1145/1143844.1143909, 10.1145/1143844.1143909]
   Lawrence N.D.:, 2008, CS0605 U SHEFF
   Li DJ, 2019, IEEE ACCESS, V7, P143980, DOI 10.1109/ACCESS.2019.2945423
   Li JM, 2015, J MED DEVICES, V9, DOI 10.1115/1.4028651
   Liu P, 2014, LECT NOTES COMPUT SC, V8692, P151, DOI 10.1007/978-3-319-10593-2_11
   Liu YY, 2018, IEEE INT CONF AUTOMA, P458, DOI 10.1109/FG.2018.00074
   Mollahosseini A, 2016, IEEE WINT CONF APPL
   Moore S, 2011, COMPUT VIS IMAGE UND, V115, P541, DOI 10.1016/j.cviu.2010.12.001
   Nusseck M, 2008, J VISION, V8, DOI 10.1167/8.8.1
   Peng X, 2018, IEEE T IMAGE PROCESS, V27, P5076, DOI 10.1109/TIP.2018.2848470
   Rahulamathavan Y, 2013, IEEE T AFFECT COMPUT, V4, P83, DOI 10.1109/T-AFFC.2012.33
   Rudovic Ognjen, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P4121, DOI 10.1109/ICPR.2010.1001
   Rudovic O, 2013, IEEE T PATTERN ANAL, V35, P1357, DOI 10.1109/TPAMI.2012.233
   Rudovic O, 2010, LECT NOTES COMPUT SC, V6312, P350, DOI 10.1007/978-3-642-15552-9_26
   Rupnik J., 2010, P C DATA MINING DATA, P1
   Seeger Matthias, 2004, Int J Neural Syst, V14, P69, DOI 10.1142/S0129065704001899
   Siddiqi MH, 2015, IEEE T IMAGE PROCESS, V24, P1386, DOI 10.1109/TIP.2015.2405346
   Sugiyama M, 2007, J MACH LEARN RES, V8, P1027
   Tariq U, 2012, LECT NOTES COMPUT SC, V7585, P578, DOI 10.1007/978-3-642-33885-4_58
   Tie Y, 2013, IEEE T CIRC SYST VID, V23, P142, DOI 10.1109/TCSVT.2012.2203210
   Yan JJ, 2016, IEEE T MULTIMEDIA, V18, P1319, DOI 10.1109/TMM.2016.2557721
   Yu XL, 2008, IEEE SIGNAL PROC LET, V15, P361, DOI 10.1109/LSP.2008.919841
   Zhang T, 2016, IEEE T MULTIMEDIA, V18, P2528, DOI 10.1109/TMM.2016.2598092
   Zheng WM, 2014, IEEE T AFFECT COMPUT, V5, P71, DOI 10.1109/TAFFC.2014.2304712
   Zheng WM, 2010, LECT NOTES COMPUT SC, V6316, P490, DOI 10.1007/978-3-642-15567-3_36
   Zhong GQ, 2010, AAAI CONF ARTIF INTE, P679
   Zhong L, 2012, PROC CVPR IEEE, P2562, DOI 10.1109/CVPR.2012.6247974
   Zhu X., 2003, P 20 INT C MACH LEAR, V3, P58, DOI DOI 10.1109/18.850663
NR 54
TC 9
Z9 9
U1 1
U2 13
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2021
VL 37
IS 1
BP 143
EP 159
DI 10.1007/s00371-019-01788-2
EA JAN 2020
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA QE3UO
UT WOS:000574081100001
DA 2024-07-18
ER

PT J
AU Bugeja, K
   Debattista, K
   Spina, S
AF Bugeja, Keith
   Debattista, Kurt
   Spina, Sandro
TI An asynchronous method for cloud-based rendering
SO VISUAL COMPUTER
LA English
DT Article
DE Rendering; Rasterisation; Global illumination; Distributed algorithms;
   Cloud computing
AB Interactive high-fidelity rendering is still unachievable on many consumer devices. Cloud gaming services have shown promise in delivering interactive graphics beyond the individual capabilities of user devices. However, a number of shortcomings are manifest in these systems: high network bandwidths are required for higher resolutions and input lag due to network fluctuations heavily disrupts user experience. In this paper, we present a scalable solution for interactive high-fidelity graphics based on a distributed rendering pipeline where direct lighting is computed on the client device and indirect lighting in the cloud. The client device keeps a local cache for indirect lighting which is asynchronously updated using an object space representation; this allows us to achieve interactive rates that are unconstrained by network performance for a wide range of display resolutions that are also robust to input lag. Furthermore, in multi-user environments, the computation of indirect lighting is amortised over participating clients.
C1 [Bugeja, Keith; Spina, Sandro] Univ Malta, Dept Comp Sci, Msida, Malta.
   [Debattista, Kurt] Univ Warwick, WMG, Coventry, W Midlands, England.
C3 University of Malta; University of Warwick
RP Bugeja, K (corresponding author), Univ Malta, Dept Comp Sci, Msida, Malta.
EM keith.bugeja@um.edu.mt
OI Spina, Sandro/0000-0001-7197-410X; Bugeja, Keith/0000-0002-3111-1251
CR Ahmed AGM, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073588
   [Anonymous], 1968, P 1968 ACM NAT C
   Autodesk 360, 2014, AUTODESK 360
   Bashford-Rogers T, 2014, IEEE T VIS COMPUT GR, V20, P907, DOI 10.1109/TVCG.2013.258
   Bierton D, 2012, FACE OFF GAIKAI VS O
   Bikker J., 2009, EUR 2009 EUR ASS, P65
   Brouillat J, 2008, PHOTON DRIVEN IRRADI, P1971
   Bugeja K., 2014, EUR S PAR GRAPH VIS, P9
   Chalmers A., 2002, PRACTICAL PARALLEL R
   Crassin C, 2011, INTERACTIVE INDIRECT, P1921
   Crassin Cyril., 2015, Journal of Computer Graphics Techniques Vol, V4, P1
   Dachsbacher C., 2005, Proc. Symp. Interactive Graph. and Games, P203, DOI DOI 10.1145/1053427.1053460
   Dachsbacher C, 2014, SCALABLE REALISTIC R, P88
   Dammertz H, 2010, PROGR POINT LIGHT BA
   Debattista K, 2009, INSTANT CACHING INTE, P2216
   Deering M., 1988, Computer Graphics, V22, P21, DOI 10.1145/378456.378468
   Dippe M. A. Z., 1985, Computer Graphics, V19, P69, DOI 10.1145/325165.325182
   Gamito MN, 2009, ACM T GRAPHIC, V29, DOI 10.1145/1640443.1640451
   Jensen HW., 2001, REALISTIC IMAGE SYNT, DOI [10.1201/9780429294907, DOI 10.1201/9780429294907]
   Kajiya J. T., 1986, SIGGRAPH, P143, DOI 10.1145/15886.15902
   Kaplanyan A., 2010, Proceedings of the 2010 ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games, I3D'10, P99, DOI [10.1145/1730804.1730821, 10.1145/1730804.1730821.24, DOI 10.1145/1730804.1730821.24]
   Keller A., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P49, DOI 10.1145/258734.258769
   Kopf J, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1276377.1276497, 10.1145/1239451.1239547]
   Langer MS, 2000, PERCEPTION, V29, P649, DOI 10.1068/p3060
   Lehmann M.A, 2014, LZF COMPRESSION LIB
   Lewis M, 2002, COMMUN ACM, V45, P27
   Liu C, 2017, SIGSIM-PADS'17: PROCEEDINGS OF THE 2017 ACM SIGSIM CONFERENCE ON PRINCIPLES OF ADVANCED DISCRETE SIMULATION, P221, DOI 10.1145/3064911.3064933
   Manzano M., 2012, P 11 ANN WORKSH NETW, P17
   Mara Michael., 2013, Proc. I3D'13, I3D'13, P71, DOI DOI 10.1145/2448196.2448207
   McGuire M., 2009, P 2009 ACM SIGGRAPH
   Mitchell Jason., 2006, ACM SIGGRAPH 2006 Courses, SIGGRAPH'06, P129, DOI DOI 10.1145/1185657.1185832
   Mittring M., 2007, ACM SIGGRAPH 2007 CO, P97, DOI [DOI 10.1145/1281500.1281671, 10.1145/1281500.1281671]
   Myszkowski K, 2001, COMP GRAPH, P221, DOI 10.1145/383259.383284
   Pajak D, 2011, SCALABLE REMOTE REND
   Pal L., 2009, ACTA U SAPIEN INFORM, V1, P5
   Pilleboue A, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766930
   Qin HX, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3119910
   Saito T., 1990, Computer Graphics, V24, P197, DOI 10.1145/97880.97901
   Wachtel F, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601107
   Walter B, 1999, SPRING EUROGRAP, P19
   Ward G. J., 1988, Computer Graphics, V22, P85, DOI 10.1145/378456.378490
NR 41
TC 9
Z9 11
U1 0
U2 9
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2019
VL 35
IS 12
BP 1827
EP 1840
DI 10.1007/s00371-018-1577-8
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KI6XQ
UT WOS:000511494300011
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Lu, Y
   Zhou, KD
   Wu, XY
   Gong, PH
AF Lu, Ye
   Zhou, Kedong
   Wu, Xiyin
   Gong, Penghan
TI A novel multi-graph framework for salient object detection
SO VISUAL COMPUTER
LA English
DT Article
DE Salient object detection; Multi-graph framework; Map refinement;
   Manifold ranking
ID REGION DETECTION; IMAGE; MODEL
AB Graph-based methods have been widely adopted for predicting the most attractive region in an image. Most of the existing graph-based methods only utilize single graph to describe the image information, and thus cannot adapt for complex scenes. In this paper, a novel multi-graph framework for salient object detection is proposed. The proposed method is divided into three steps. Firstly, an image is divided into superpixels and described as a multi-graph, where superpixels are represented as nodes and their information is computed by color space and location space. Secondly, the multiple graphs are combined into a novel multi-graph-based manifold ranking propagation framework to obtain a coarse map. Finally, a map refinement model is developed to improve the quality of the coarse map. Experimental results on four challenging datasets show that the proposed method performs favorably against the state-of-the-art salient object detection methods.
C1 [Lu, Ye; Zhou, Kedong] Nanjing Univ Sci & Technol, Sch Mech Engn, Nanjing 210094, Jiangsu, Peoples R China.
   [Wu, Xiyin] Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Nanjing 210094, Jiangsu, Peoples R China.
   [Gong, Penghan] AEU, Shijiazhuang Campus, Shijiazhuang, Hebei, Peoples R China.
C3 Nanjing University of Science & Technology; Nanjing University of
   Science & Technology
RP Lu, Y (corresponding author), Nanjing Univ Sci & Technol, Sch Mech Engn, Nanjing 210094, Jiangsu, Peoples R China.
EM luye_njust@163.com
RI Wu, Xiyin/AAQ-6518-2020
FU National Natural Science Foundation of China [61602244]
FX This work was supported by National Natural Science Foundation of China
   under Grant No. 61602244.
CR Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   Borji A, 2012, LECT NOTES COMPUT SC, V7573, P414, DOI 10.1007/978-3-642-33709-3_30
   Cheng MM, 2013, IEEE I CONF COMP VIS, P1529, DOI 10.1109/ICCV.2013.193
   Cheng MM, 2014, VISUAL COMPUT, V30, P443, DOI 10.1007/s00371-013-0867-4
   Cheng MM, 2011, PROC CVPR IEEE, P409, DOI 10.1109/CVPR.2011.5995344
   Gong C, 2015, PROC CVPR IEEE, P2531, DOI 10.1109/CVPR.2015.7298868
   Guo CL, 2010, IEEE T IMAGE PROCESS, V19, P185, DOI 10.1109/TIP.2009.2030969
   Harel J, 2006, Advances in Neural Information Processing Systems, V19
   Hou QB, 2017, PROC CVPR IEEE, P5300, DOI 10.1109/CVPR.2017.563
   Hou XD, 2007, PROC CVPR IEEE, P2280
   Hu P., 2017, CVPR, V1, P2
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Jiang BW, 2013, IEEE I CONF COMP VIS, P1665, DOI 10.1109/ICCV.2013.209
   Kim J, 2014, PROC CVPR IEEE, P883, DOI 10.1109/CVPR.2014.118
   Kuen J, 2016, PROC CVPR IEEE, P3668, DOI 10.1109/CVPR.2016.399
   Lee GY, 2016, PROC CVPR IEEE, P660, DOI 10.1109/CVPR.2016.78
   Li CY, 2015, PROC CVPR IEEE, P2710, DOI 10.1109/CVPR.2015.7298887
   Li X, 2016, IEEE T IMAGE PROCESS, V25, P3919, DOI 10.1109/TIP.2016.2579306
   Li Y, 2014, PROC CVPR IEEE, P280, DOI 10.1109/CVPR.2014.43
   Liu T, 2011, IEEE T PATTERN ANAL, V33, P353, DOI 10.1109/TPAMI.2010.70
   Lu S, 2014, PROC CVPR IEEE, P2790, DOI 10.1109/CVPR.2014.357
   MAI L, 2013, PROC CVPR IEEE, P1131, DOI DOI 10.1109/CVPR.2013.150
   Mao MS, 2017, IEEE T CYBERNETICS, V47, P4049, DOI 10.1109/TCYB.2016.2595620
   Margolin R, 2014, PROC CVPR IEEE, P248, DOI 10.1109/CVPR.2014.39
   Maybank SJ, 2013, NEUROCOMPUTING, V120, P4, DOI 10.1016/j.neucom.2012.06.060
   Peng HW, 2017, IEEE T PATTERN ANAL, V39, P818, DOI 10.1109/TPAMI.2016.2562626
   Perazzi F, 2012, PROC CVPR IEEE, P733, DOI 10.1109/CVPR.2012.6247743
   Qi W, 2017, VISUAL COMPUT, V33, P209, DOI 10.1007/s00371-015-1176-x
   Ren ZX, 2014, IEEE T CIRC SYST VID, V24, P769, DOI 10.1109/TCSVT.2013.2280096
   Shen XH, 2012, PROC CVPR IEEE, P853, DOI 10.1109/CVPR.2012.6247758
   Shi JP, 2016, IEEE T PATTERN ANAL, V38, P717, DOI 10.1109/TPAMI.2015.2465960
   Sun JG, 2015, IEEE T IMAGE PROCESS, V24, P1639, DOI 10.1109/TIP.2015.2403241
   Tang C, 2017, IEEE SIGNAL PROC LET, V24, P490, DOI 10.1109/LSP.2016.2620162
   Tao DP, 2016, IEEE T NEUR NET LEAR, V27, P1122, DOI 10.1109/TNNLS.2015.2461554
   Wang JP, 2015, NEUROCOMPUTING, V152, P359, DOI 10.1016/j.neucom.2014.10.056
   Wang QS, 2016, PROC CVPR IEEE, P535, DOI 10.1109/CVPR.2016.64
   Wang W, 2010, PROC CVPR IEEE, P2368, DOI 10.1109/CVPR.2010.5539927
   Wei YC, 2012, LECT NOTES COMPUT SC, V7574, P29, DOI 10.1007/978-3-642-33712-3_3
   Wu J, 2017, IEEE T CYBERNETICS, V47, P818, DOI 10.1109/TCYB.2016.2527239
   Wu XY, 2018, J VIS COMMUN IMAGE R, V54, P51, DOI 10.1016/j.jvcir.2018.04.006
   Xia C., 2017, PROC CVPR IEEE, P4142, DOI DOI 10.1109/CVPR.2017.468
   Yang C, 2013, PROC CVPR IEEE, P3166, DOI 10.1109/CVPR.2013.407
   YANG JM, 2012, PROC CVPR IEEE, P2296, DOI [DOI 10.1109/CVPR.2012.6247940, 10.1109/CVPR.2012.6247940]
   Yang X, 2015, NEUROCOMPUTING, V159, P35, DOI 10.1016/j.neucom.2015.02.046
   Zhai Y., 2006, P 14 ACM INT C MULT, P815, DOI [DOI 10.1145/1180639.1180824, 10.1145/1180639.1180824]
   Zhang J, 2017, INFORM SCIENCES, V399, P154, DOI 10.1016/j.ins.2017.03.005
   Zhang LH, 2017, IEEE T PATTERN ANAL, V39, P1892, DOI 10.1109/TPAMI.2016.2609426
   Zhang Q, 2018, VISUAL COMPUT, V34, P473, DOI 10.1007/s00371-017-1354-0
   Zhao R, 2015, PROC CVPR IEEE, P1265, DOI 10.1109/CVPR.2015.7298731
   Zhi XH, 2018, PATTERN RECOGN, V80, P241, DOI 10.1016/j.patcog.2018.03.010
   Zhou DY, 2004, ADV NEUR IN, V16, P321
   Zhu WJ, 2014, PROC CVPR IEEE, P2814, DOI 10.1109/CVPR.2014.360
NR 53
TC 14
Z9 14
U1 0
U2 25
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2019
VL 35
IS 11
SI SI
BP 1683
EP 1699
DI 10.1007/s00371-019-01637-2
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA JD5IZ
UT WOS:000490018000014
DA 2024-07-18
ER

PT J
AU Linares, OC
   Bianchi, J
   Raveli, D
   Neto, JB
   Hamann, B
AF Linares, Oscar Cuadros
   Bianchi, Jonas
   Raveli, Dirceu
   Batista Neto, Joao
   Hamann, Bernd
TI Mandible and skull segmentation in cone beam computed tomography using
   super-voxels and graph clustering
SO VISUAL COMPUTER
LA English
DT Article
DE Cone beam computed tomography; Graph clustering; Super-voxels; Bone
   segmentation; Mandible; Skull
ID 3D SEGMENTATION; CT; BONE
AB Cone beam computed tomography (CBCT) is a medical imaging technique employed for diagnosis and treatment of patients with cranio-maxillofacial deformities. CBCT 3D reconstruction and segmentation of bones such as mandible or maxilla are essential procedures in surgical and orthodontic treatments. However, CBCT image processing may be impaired by features such as low contrast, inhomogeneity, noise and artifacts. Besides, values assigned to voxels are relative Hounsfield units unlike traditional computed tomography (CT). Such drawbacks render CBCT segmentation a difficult and time-consuming task, usually performed manually with tools designed for medical image processing. We present an interactive two-stage method for the segmentation of CBCT: (i) we first perform an automatic segmentation of bone structures with super-voxels, allowing a compact graph representation of the 3D data; (ii) next, a user-placed seed process guides a graph partitioning algorithm, splitting the extracted bones into mandible and skull. We have evaluated our segmentation method in three different scenarios and compared the results with ground truth data of the mandible and the skull. Results show that our method produces accurate segmentation and is robust to changes in parameters. We also compared our method with two similar segmentation strategy and showed that it produces more accurate segmentation. Finally, we evaluated our method for CT data of patients with deformed or missing bones and the segmentation was accurate for all data. The segmentation of a typical CBCT takes in average 5 min, which is faster than most techniques currently available.
C1 [Linares, Oscar Cuadros; Batista Neto, Joao] Univ Sao Paulo, ICMC, BR-13566590 Sao Carlos, SP, Brazil.
   [Bianchi, Jonas; Raveli, Dirceu] Sao Paulo State Univ UNESP, Fac Odontol FOAR, BR-14801385 Araraquara, SP, Brazil.
   [Hamann, Bernd] Univ Calif Davis, Dept Comp Sci, Davis, CA 95616 USA.
C3 Universidade de Sao Paulo; Universidade Estadual Paulista; University of
   California System; University of California Davis
RP Linares, OC (corresponding author), Univ Sao Paulo, ICMC, BR-13566590 Sao Carlos, SP, Brazil.
EM ocuadros@icmc.usp.br
RI Bianchi, Jonas/Q-2802-2019; Raveli, Dirceu Barnabé/F-7170-2012; Cuadros
   Linares, Oscar/D-3903-2018
OI Bianchi, Jonas/0000-0002-3749-0918; LINARES GARCIA, OSCAR
   NICOLAS/0000-0002-7952-9518; Cuadros Linares, Oscar/0000-0003-1454-399X
FU Brazilian research agency Sao Paulo Research Foundation (FAPESP)
   [2012/24036-1, 2015/12771-7]; Fundacao de Amparo a Pesquisa do Estado de
   Sao Paulo (FAPESP) [15/12771-7] Funding Source: FAPESP
FX This work was supported by the Brazilian research agency Sao Paulo
   Research Foundation (FAPESP) Grant Numbers: 2012/24036-1 and
   2015/12771-7.
CR Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   [Anonymous], 2010, Technical Report
   [Anonymous], P 2012 IEEE INT S ME
   [Anonymous], 2014, Marginal space learning for medical image analysis
   Boykov Y, 2006, INT J COMPUT VISION, V70, P109, DOI 10.1007/s11263-006-7934-5
   Brandariz M, 2014, COMP MED SY, P467, DOI 10.1109/CBMS.2014.93
   Casaca W, 2014, PROC CVPR IEEE, P384, DOI 10.1109/CVPR.2014.56
   Chang YB, 2013, J X-RAY SCI TECHNOL, V21, P251, DOI 10.3233/XST-130369
   Chung F, 2011, VISUAL COMPUT, V27, P141, DOI 10.1007/s00371-010-0536-9
   Çigla C, 2010, IEEE IMAGE PROC, P3013, DOI 10.1109/ICIP.2010.5653963
   Cuadros O., 2012, 2012 XXV SIBGRAPI - Conference on Graphics, Patterns and Images (SIBGRAPI 2012), P24, DOI 10.1109/SIBGRAPI.2012.13
   CuadrosLinares O., 2017, IET IMAGE PROCESS, V11, P60
   Descoteaux Maxime, 2006, Comput Aided Surg, V11, P247, DOI 10.3109/10929080601017212
   Fedorov A, 2012, MAGN RESON IMAGING, V30, P1323, DOI 10.1016/j.mri.2012.05.001
   Guennebaud G., 2010, Eigen
   Johnson H.J., 2013, The ITK Software Guide, VThird
   Kirbas C, 2004, ACM COMPUT SURV, V36, P81, DOI 10.1145/1031120.1031121
   Kolmogorov V, 2004, IEEE T PATTERN ANAL, V26, P147, DOI 10.1109/TPAMI.2004.1262177
   Krcah M, 2011, I S BIOMED IMAGING, P2087, DOI 10.1109/ISBI.2011.5872823
   Li W., 2011, NEW FUNCTIONALITIES, P08
   Lorensen W. E., 1987, COMPUTER GRAPHICS, V21, P163, DOI 10.1145/37401.37422
   Loubele M, 2006, ORAL SURG ORAL MED O, V102, P225, DOI 10.1016/j.tripleo.2005.10.039
   Maca I, 2007, INSIGHT J, P1
   Miles DA., 2007, CLINICIANS GUIDE UND, P1
   Mozzo P, 1998, EUR RADIOL, V8, P1558, DOI 10.1007/s003300050586
   Pauchard Y, 2016, COMPUT METHOD BIOMEC, V19, P1693, DOI 10.1080/10255842.2016.1181173
   Paulano F, 2014, VISUAL COMPUT, V30, P939, DOI 10.1007/s00371-014-0963-0
   Peng B, 2013, PATTERN RECOGN, V46, P1020, DOI 10.1016/j.patcog.2012.09.015
   Pixmeo-SARL, 2016, DATASET PIXMEO SARL
   Richmond C, 2004, BRIT MED J, V329, P687, DOI 10.1136/bmj.329.7467.687
   Scarfe WC, 2006, J CAN DENT ASSOC, V72, P75
   Solutions D. I. M., 2017, DOLPH IM
   von Landesberger T, 2013, VISUAL COMPUT, V29, P893, DOI 10.1007/s00371-013-0852-y
   Wang L, 2016, MED PHYS, V43, P336, DOI 10.1118/1.4938267
   Wang L, 2014, MED PHYS, V41, DOI 10.1118/1.4868455
   Xia JJ, 2009, J ORAL MAXIL SURG, V67, P2093, DOI 10.1016/j.joms.2009.04.057
   Yi F., 2012, IEEE INT C SYST INF, P1936
   Yushkevich PA, 2006, NEUROIMAGE, V31, P1116, DOI 10.1016/j.neuroimage.2006.01.015
   Zhang YJ, 1996, PATTERN RECOGN, V29, P1335, DOI 10.1016/0031-3203(95)00169-7
   Zhu L, 2014, WORKSH INT MED IM CO, P1
NR 40
TC 16
Z9 16
U1 2
U2 16
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2019
VL 35
IS 10
BP 1461
EP 1474
DI 10.1007/s00371-018-1511-0
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IU7PN
UT WOS:000483775900009
DA 2024-07-18
ER

PT J
AU Agahian, S
   Negin, F
   Köse, C
AF Agahian, Saeid
   Negin, Farhood
   Kose, Cemal
TI Improving bag-of-poses with semi-temporal pose descriptors for
   skeleton-based action recognition
SO VISUAL COMPUTER
LA English
DT Article
DE Skeleton-based; 3D action recognition; Bag-of-words; Key poses; Extreme
   learning machine and RGB-D
ID EXTREME LEARNING-MACHINE; FEATURES; ENSEMBLE
AB Over the last few decades, human action recognition has become one of the most challenging tasks in the field of computer vision. Effortless and accurate extraction of 3D skeleton information has been recently achieved by means of economical depth sensors and state-of-the-art deep learning approaches. In this study, we introduce a novel bag-of-poses framework for action recognition using 3D skeleton data. Our assumption is that any action can be represented by a set of predefined spatiotemporal poses. The pose descriptor is composed of three parts. The first part is concatenation of the normalized coordinate of the skeleton joints. The second part is consisted of temporal displacement of the joints constructed with predefined temporal offset, and the third part is temporal displacement with the previous frame in the sequence. In order to generate the key poses, we apply K-means clustering over all the training pose descriptors of the dataset. SVM classifier is trained with the generated key poses to classify an action pose. Accordingly, every action in the dataset is encoded with key pose histograms. ELM classifier is used for action recognition due to its fast, accurate and reliable performance compared to the other classifiers. The proposed framework is validated with five publicly available benchmark 3D action datasets and achieved state-of-the-art results on three of the datasets and competitive results on the other two datasets compared to the other methods.
C1 [Agahian, Saeid; Kose, Cemal] Karadeniz Tech Univ, Fac Engn, Dept Comp Engn, TR-61080 Trabzon, Turkey.
   [Negin, Farhood] INRIA Sophia Antipolis, 2004 Route Lucioles,BP93, F-06902 Sophia Antipolis, France.
C3 Karadeniz Technical University
RP Agahian, S (corresponding author), Karadeniz Tech Univ, Fac Engn, Dept Comp Engn, TR-61080 Trabzon, Turkey.
EM saeid@ktu.edu.tr; farhood.negin@inria.fr; ckose@ktu.edu.tr
RI Negin, Farhood/AAO-7507-2021; ALP, Sait/AAW-4927-2021; agahian,
   saeid/IZE-6076-2023; KÖSE, Cemal/V-9731-2017
OI ALP, Sait/0000-0003-2462-6166; agahian, saeid/0000-0003-2462-6166; KÖSE,
   Cemal/0000-0002-5982-4771
CR Aggarwal JK, 2014, PATTERN RECOGN LETT, V48, P70, DOI 10.1016/j.patrec.2014.04.011
   Aggarwal JK, 2011, ACM COMPUT SURV, V43, DOI 10.1145/1922649.1922653
   Chaaraoui AA, 2014, EXPERT SYST APPL, V41, P786, DOI 10.1016/j.eswa.2013.08.009
   [Anonymous], J THEORETICAL APPL C
   Ben Amor B, 2016, IEEE T PATTERN ANAL, V38, P1, DOI 10.1109/TPAMI.2015.2439257
   Cao Z., 2016, ABS161108050 CORR
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Chen C, 2016, IEEE SENS J, V16, P773, DOI 10.1109/JSEN.2015.2487358
   Chen C, 2015, IEEE IMAGE PROC, P168, DOI 10.1109/ICIP.2015.7350781
   Chen X, 2015, NEUROCOMPUTING, V149, P387, DOI 10.1016/j.neucom.2013.10.046
   Chron G., P IEEE INT C COMP VI, P3218
   Das Dawn D, 2016, VISUAL COMPUT, V32, P289, DOI 10.1007/s00371-015-1066-2
   Tran D, 2016, INT J COMPUT VISION, V119, P239, DOI 10.1007/s11263-016-0905-6
   Du Y., P IEEE C COMP VIS PA, P1110
   Du Y, 2015, PROCEEDINGS 3RD IAPR ASIAN CONFERENCE ON PATTERN RECOGNITION ACPR 2015, P579, DOI 10.1109/ACPR.2015.7486569
   Eweiwi A., P AS C COMP VIS, P428
   Fothergill S., P SIGCHI C HUM FACT, P1737
   Gaglio S, 2015, IEEE T HUM-MACH SYST, V45, P586, DOI 10.1109/THMS.2014.2377111
   Guo Y, 2018, IEEE T CYBERNETICS, V48, P1513, DOI 10.1109/TCYB.2017.2705227
   Han F, 2017, COMPUT VIS IMAGE UND, V158, P85, DOI 10.1016/j.cviu.2017.01.011
   Han JG, 2013, IEEE T CYBERNETICS, V43, P1318, DOI 10.1109/TCYB.2013.2265378
   Hou Y., 2017, IEEE T SYST MAN CYB, V99, P1
   Huang GB, 2006, NEUROCOMPUTING, V70, P489, DOI 10.1016/j.neucom.2005.12.126
   Hussein M. E., 23 INT JOINT C ART I
   Ibañez R, 2017, PATTERN RECOGN, V62, P73, DOI 10.1016/j.patcog.2016.08.022
   Jiang XB, 2014, VISUAL COMPUT, V30, P1021, DOI 10.1007/s00371-014-0923-8
   Johansson G., 1975, VISUAL MOTION PERCEP
   Kapsouras I, 2014, J VIS COMMUN IMAGE R, V25, P1432, DOI 10.1016/j.jvcir.2014.04.007
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Lee I., P IEEE C COMP VIS PA, P1012
   Li WB, 2010, 2010 THE 3RD INTERNATIONAL CONFERENCE ON COMPUTATIONAL INTELLIGENCE AND INDUSTRIAL APPLICATION (PACIIA2010), VOL I, P9, DOI 10.1109/cvprw.2010.5543273
   Lillo I, 2017, IMAGE VISION COMPUT, V59, P63, DOI 10.1016/j.imavis.2016.11.004
   Liu J, 2017, IEEE T NEUR NET LEAR, V99, P1, DOI DOI 10.1080/15623599.2017.1326299
   Liu MY, 2017, IEEE INT CON MULTI, P901, DOI 10.1109/ICME.2017.8019313
   Liu MY, 2017, PATTERN RECOGN, V68, P346, DOI 10.1016/j.patcog.2017.02.030
   Lo Presti L, 2016, PATTERN RECOGN, V53, P130, DOI 10.1016/j.patcog.2015.11.019
   Lu GL, 2016, MULTIMED TOOLS APPL, V75, P3479, DOI 10.1007/s11042-015-2448-1
   Luvizon DC, 2017, PATTERN RECOGN LETT, V99, P13, DOI 10.1016/j.patrec.2017.02.001
   Minhas R, 2010, NEUROCOMPUTING, V73, P1906, DOI 10.1016/j.neucom.2010.01.020
   Negin F., INT C IM AN REC, P648
   Nunes UM, 2017, PATTERN RECOGN LETT, V99, P21, DOI 10.1016/j.patrec.2017.05.004
   Parisi GI, 2015, FRONT NEUROROBOTICS, V9, P1, DOI 10.3389/fnbot.2015.00003
   Peng XJ, 2016, COMPUT VIS IMAGE UND, V150, P109, DOI 10.1016/j.cviu.2016.03.013
   Poppe R, 2010, IMAGE VISION COMPUT, V28, P976, DOI 10.1016/j.imavis.2009.11.014
   Qiao RZ, 2017, PATTERN RECOGN, V66, P202, DOI 10.1016/j.patcog.2017.01.015
   Ramanathan M, 2014, IEEE T HUM-MACH SYST, V44, P650, DOI 10.1109/THMS.2014.2325871
   Sadanand S, 2012, PROC CVPR IEEE, P1234, DOI 10.1109/CVPR.2012.6247806
   Salih AA, 2016, PATTERN RECOGN LETT, V83, P32, DOI 10.1016/j.patrec.2016.05.032
   Shan JJ, 2014, 2014 IEEE WORKSHOP ON ADVANCED ROBOTICS AND ITS SOCIAL IMPACTS (ARSO), P69, DOI 10.1109/ARSO.2014.7020983
   Shotton J, 2013, COMMUN ACM, V56, P116, DOI 10.1145/2398356.2398381
   Sung JY, 2012, IEEE INT CONF ROBOT, P842, DOI 10.1109/ICRA.2012.6224591
   Tao L., P IEEE INT C COMP VI, P61
   Varol G, 2015, EXPERT SYST APPL, V42, P8274, DOI 10.1016/j.eswa.2015.06.013
   Veeriah V., P IEEE INT C COMP VI, P4041
   Vemulapalli R., P IEEE C COMP VIS PA, P588
   Vemulapalli R, 2016, COMPUT VIS IMAGE UND, V152, P155, DOI 10.1016/j.cviu.2016.04.005
   Vishwakarma S, 2013, VISUAL COMPUT, V29, P983, DOI 10.1007/s00371-012-0752-6
   Wang C., P IEEE C COMP VIS PA, P915
   Wang CY, 2016, PROC CVPR IEEE, P2639, DOI 10.1109/CVPR.2016.289
   Wang J, 2014, SPRINGERBRIEF COMPUT, P11, DOI 10.1007/978-3-319-04561-0_2
   Wang J, 2012, PROC CVPR IEEE, P1290, DOI 10.1109/CVPR.2012.6247813
   Wang PC, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P97, DOI 10.1145/2964284.2967191
   Xia L., 2012, IEEE COMP SOC C COMP, P20, DOI DOI 10.1109/CVPRW.2012.6239233
   Yang YH, 2017, IEEE T CYBERNETICS, V47, P439, DOI 10.1109/TCYB.2016.2519448
   Yao A, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.67
   Zanfir M., P IEEE INT C COMP VI, P2752
   Zelnik-Manor L, 2001, PROC CVPR IEEE, P123
   Zhang J, 2016, PATTERN RECOGN, V60, P86, DOI 10.1016/j.patcog.2016.05.019
   Zhang SY, 2017, IEEE WINT CONF APPL, P148, DOI 10.1109/WACV.2017.24
   Zhou Y, 2014, IEEE INT CONGR BIG, P1, DOI 10.1109/BigData.Congress.2014.11
   Zhu F, 2016, IMAGE VISION COMPUT, V55, P42, DOI 10.1016/j.imavis.2016.06.007
   Zhu GM, 2016, SIGNAL PROCESS-IMAGE, V42, P19, DOI 10.1016/j.image.2016.01.003
   Zhu WH, 2016, PROC INT CONF ANTI, P1, DOI 10.1109/ICASID.2016.7873885
   Zhu Y, 2015, ACM T INTEL SYST TEC, V6, DOI 10.1145/2629483
NR 74
TC 27
Z9 27
U1 0
U2 23
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2019
VL 35
IS 4
BP 591
EP 607
DI 10.1007/s00371-018-1489-7
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HS2EB
UT WOS:000463672800010
DA 2024-07-18
ER

PT J
AU Jin, H
   Wang, X
   Lian, YF
   Hua, J
AF Jin, Hai
   Wang, Xun
   Lian, Yuanfeng
   Hua, Jing
TI Emotion information visualization through learning of 3D morphable face
   model
SO VISUAL COMPUTER
LA English
DT Article
DE 3D morphable face model; Facial expression analysis; Emotion
   visualization
ID RECOGNITION
AB Analysis and visualization of human facial expressions and its applications are useful but challenging. This paper presents a novel approach to analyze the facial expressions from images through learning of a 3D morphable face model and a quantitative information visualization scheme for exploring this type of visual data. More specifically, a 3D face database with various facial expressions is employed to build a nonnegative matrix factorization (NMF) part-based morphable 3D face model. From an input image, a 3D face with expression can be reconstructed iteratively by using the NMF morphable 3D face model as a priori knowledge, from which basis parameters and a displacement map are extracted as features for facial emotion analysis and visualization. Based upon the features, two support vector regressions are trained to determine the fuzzy valence-arousal (VA) values to quantify the emotions. The continuously changing emotion status can be intuitively analyzed by visualizing the VA values in VA space. Our emotion analysis and visualization system, based on 3D NMF morphable face model, detect expressions robustly from various head poses, face sizes and lighting conditions and is fully automatic to compute the VA values from images or a sequence of video with various facial expressions. To evaluate our novel method, we test our system on publicly available databases and evaluate the emotion analysis and visualization results. We also apply our method to quantifying emotion changes during motivational interviews. These experiments and applications demonstrate the effectiveness and accuracy of our method.
C1 [Jin, Hai; Lian, Yuanfeng] Wayne State Univ, Dept Comp Sci, Detroit, MI 48202 USA.
   [Wang, Xun] Wayne State Univ, Detroit, MI USA.
   [Hua, Jing] Wayne State Univ, Comp Sci, Detroit, MI 48202 USA.
   [Hua, Jing] Wayne State Univ, Comp Graph & Imaging Lab GIL, Detroit, MI 48202 USA.
   [Hua, Jing] Wayne State Univ, Vis Lab VIS Comp Sci, Detroit, MI 48202 USA.
C3 Wayne State University; Wayne State University; Wayne State University;
   Wayne State University; Wayne State University
RP Hua, J (corresponding author), Wayne State Univ, Comp Sci, Detroit, MI 48202 USA.; Hua, J (corresponding author), Wayne State Univ, Comp Graph & Imaging Lab GIL, Detroit, MI 48202 USA.; Hua, J (corresponding author), Wayne State Univ, Vis Lab VIS Comp Sci, Detroit, MI 48202 USA.
EM jinghua@wayne.edu
FU NSF [LZ16F020002];  [CNS-1647200]
FX We would like to thank the reviewers for their valuable suggestions
   which helped to improve this paper. This work is supported in part by
   the following grants: LZ16F020002 and NSF CNS-1647200.
CR [Anonymous], 2003, P 2003 C COMP VIS PA, DOI DOI 10.1109/CVPRW.2003.10057
   [Anonymous], 2017, COMMUN ACM, DOI DOI 10.1145/3065386
   [Anonymous], IEEE C COMP VIS PATT
   [Anonymous], NEURAL INFORM PROCES
   [Anonymous], TPAMI
   [Anonymous], 2011, IEEE T AFFECTIVE COM
   [Anonymous], 2006, BMVC
   Ashraf AB, 2009, IMAGE VISION COMPUT, V27, P1788, DOI 10.1016/j.imavis.2009.05.007
   Beeler T, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778777
   Beeler T, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964970
   Blanz V, 1999, COMP GRAPH, P187, DOI 10.1145/311535.311556
   Blanz V, 2003, IEEE T PATTERN ANAL, V25, P1063, DOI 10.1109/TPAMI.2003.1227983
   Cao C, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925873
   Cao C, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601204
   Cao C, 2014, IEEE T VIS COMPUT GR, V20, P413, DOI 10.1109/TVCG.2013.249
   Cao C, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2462012
   Chen YL, 2013, IEEE I CONF COMP VIS, P3615, DOI 10.1109/ICCV.2013.449
   Cootes TF, 2002, IMAGE VISION COMPUT, V20, P657, DOI 10.1016/S0262-8856(02)00055-0
   Cootes TF, 2001, IEEE T PATTERN ANAL, V23, P681, DOI 10.1109/34.927467
   EKMAN P, 1992, COGNITION EMOTION, V6, P169, DOI 10.1080/02699939208411068
   Fanelli G, 2013, INT J COMPUT VISION, V101, P437, DOI 10.1007/s11263-012-0549-0
   Fasel B, 2003, PATTERN RECOGN, V36, P259, DOI 10.1016/S0031-3203(02)00052-3
   Geng X, 2007, IEEE T PATTERN ANAL, V29, P2234, DOI 10.1109/TPAMI.2007.70733
   Granger S, 2002, LECT NOTES COMPUT SC, V2353, P418
   Gunes Hatice, 2010, International Journal of Strategic Synthetic Emotions, V1, P68, DOI 10.4018/jse.2010101605
   Guo XH, 2004, IEEE COMPUT GRAPH, V24, P43, DOI 10.1109/MCG.2004.16
   Guo XH, 2004, IEEE COMPUT GRAPH, V24, P31, DOI 10.1109/MCG.2004.63
   Ioannou SV, 2005, NEURAL NETWORKS, V18, P423, DOI 10.1016/j.neunet.2005.03.004
   Joshi P., 2005, ACM SIGGRAPH 2005 Courses, SIGGRAPH'05, P8, DOI [DOI 10.1145/1198555.1198588, 10.1145/1198555.1198588]
   Kapoor A, 2007, INT J HUM-COMPUT ST, V65, P724, DOI 10.1016/j.ijhcs.2007.02.003
   Ko KE, 2009, 2009 IEEE INTERNATIONAL CONFERENCE ON FUZZY SYSTEMS, VOLS 1-3, P2063, DOI 10.1109/FUZZY.2009.5277231
   Kobayashi H, 1997, IEEE SYS MAN CYBERN, P3732, DOI 10.1109/ICSMC.1997.633250
   Lee DD, 2001, ADV NEUR IN, V13, P556
   Lei Z., 2008, IEEE C COMPUTER VISI
   Liao QQ, 2012, IEEE T VIS COMPUT GR, V18, P1704, DOI 10.1109/TVCG.2012.26
   Lucey P., 2010, IEEE COMP SOC C COMP, P94, DOI 10.1109/CVPRW.2010.5543262
   Newcombe RA, 2015, PROC CVPR IEEE, P343, DOI 10.1109/CVPR.2015.7298631
   Sandbach G, 2012, IMAGE VISION COMPUT, V30, P762, DOI 10.1016/j.imavis.2012.01.006
   Schuller B, 2012, ICMI '12: PROCEEDINGS OF THE ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P449
   Sebe N, 2007, IMAGE VISION COMPUT, V25, P1856, DOI 10.1016/j.imavis.2005.12.021
   Suwajanakorn S, 2014, LECT NOTES COMPUT SC, V8692, P796, DOI 10.1007/978-3-319-10593-2_52
   Tena JR, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964971
   Turk M. A., 1991, Proceedings 1991 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (91CH2983-5), P586, DOI 10.1109/CVPR.1991.139758
   Valstar M., 2014, P 4 INT WORKSH AUD V, P3, DOI 10.1109/FG.2015.7284874
   Valstar MF, 2007, LECT NOTES COMPUT SC, V4796, P118
   Wang HC, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P958, DOI 10.1109/ICCV.2003.1238452
NR 46
TC 8
Z9 9
U1 0
U2 18
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2019
VL 35
IS 4
BP 535
EP 548
DI 10.1007/s00371-018-1482-1
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HS2EB
UT WOS:000463672800006
DA 2024-07-18
ER

PT J
AU Kamble, VM
   Parate, MR
   Bhurchandi, KM
AF Kamble, Vipin Milind
   Parate, Mayur Rajaram
   Bhurchandi, Kishor M.
TI No reference noise estimation in digital images using random conditional
   selection and sampling theory
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 6th International Conference on Virtual Reality and Visualization
   (ICVRV)
CY SEP 24-26, 2016
CL Hangzhou, PEOPLES R CHINA
SP China Soc Image & Graph, China Comp Federat, China Syst Simulat Assoc, IEEE Comp Soc, Connected Universal Experiences Labs Inc, China Soc Image & Graph, VR Comm, China Comp Federat, VR & Visualizat Comm, China Syst Simulat Assoc, VR Comm, China Syst Simulat Assoc, Digital Entertainment Comm, China Syst Simulat Assoc, Surgery Simulat Comm
DE Blind noise estimation; Gaussian noise; Random conditional sampling;
   Statistical sampling theory; Speckle noise
ID SIGNAL-DEPENDENT NOISE
AB An accurate quantitative noise estimate is required in many image/video processing applications like denoising, computer vision, pattern recognition and tracking. But blind and accurate estimation of noise in an unknown image is a challenging task and hence is an open area of research. We propose the first elegant and novel blind noise estimation method based on random image tile selection and statistical sampling theory for estimating standard deviation of zero mean Gaussian and speckle noise in digital images. Randomly selected samples, i.e., pixels with 3x3 neighborhood, are checked for availability of edges in the tile. If there is an edge in the tile at more than one neighboring pixel, the tile is excluded. Only non-edge tiles are used for estimation of noise in the tile and subsequently in the image using the concepts of statistical sampling theory. Finally, we propose a supervised curve fitting approach using the proposed noise estimation model for more accurate estimation of standard deviation of the two types of noise. The proposed technique is computationally efficient as it is a selective random sample-based spatial domain technique. Benchmarking with other contemporary techniques published so far shows that the proposed technique clearly outperforms the others by at least 5% improved noise estimates, over a very wide range of noise.
C1 [Kamble, Vipin Milind; Parate, Mayur Rajaram; Bhurchandi, Kishor M.] Visvesvaraya Natl Inst Technol, Nagpur, Maharashtra, India.
C3 National Institute of Technology (NIT System); Visvesvaraya National
   Institute of Technology, Nagpur
RP Kamble, VM (corresponding author), Visvesvaraya Natl Inst Technol, Nagpur, Maharashtra, India.
EM vipinkamble97@gmail.com; mrparate1787@gmail.com;
   bhurchandikm@yahoo.co.in
RI Kamble, Vipin Milind/K-7545-2018; BHURCHANDI, KISHOR/AAA-7708-2022
OI Kamble, Vipin Milind/0000-0002-9563-9514; BHURCHANDI,
   KISHOR/0000-0003-0730-363X
CR [Anonymous], 2007, 2007 IEEE C COMP VIS
   Azzari L, 2014, IEEE T IMAGE PROCESS, V23, P3459, DOI 10.1109/TIP.2014.2321504
   Buades A, 2005, PROC CVPR IEEE, P60, DOI 10.1109/cvpr.2005.38
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851
   Casella G., 2002, STAT INFERENCE
   Chang Y, 2007, IEEE T IMAGE PROCESS, V16, P329, DOI 10.1109/TIP.2006.888347
   Chen GY, 2015, IEEE I CONF COMP VIS, P477, DOI 10.1109/ICCV.2015.62
   Cooper G. R., 1986, PROBABILISTIC METHOD, P159
   Cui GM, 2016, OPT REV, V23, P208, DOI 10.1007/s10043-016-0200-3
   Dabov K, 2007, IEEE T IMAGE PROCESS, V16, P2080, DOI 10.1109/TIP.2007.901238
   DONOHO DL, 1994, BIOMETRIKA, V81, P425, DOI 10.1093/biomet/81.3.425
   Easley GR, 2009, IEEE T IMAGE PROCESS, V18, P260, DOI 10.1109/TIP.2008.2008070
   Ghazal M, 2011, IEEE T IMAGE PROCESS, V20, P1788, DOI 10.1109/TIP.2010.2097272
   Gonzalez R. C., 2006, PEARSON ED INDIA, V3rd
   Granados Miguel., 2015, Proceedings of the 12th European Conference on Visual Media Production, P1
   Hashemi M, 2010, IEEE SIGNAL PROC LET, V17, P12, DOI 10.1109/LSP.2009.2030856
   Immerkaer J, 1996, COMPUT VIS IMAGE UND, V64, P300, DOI 10.1006/cviu.1996.0060
   Jain A. K., 1989, Fundamentals of Digital Image Processing
   Karibasappa K.G., 2015, ADV INTELLIGENT INFO, V320, P49, DOI 10.1007/978-3-319- 11218-3 6
   Khmag A, 2017, VIS COMPUT
   Lancaster P., 1986, CURVE SURFACE FITTIN, P1
   Li Jin-chao, 2009, 2009 WRI World Congress on Computer Science and Information Engineering, CSIE, P578, DOI 10.1109/CSIE.2009.541
   Liu XH, 2013, IEEE IMAGE PROC, P79, DOI 10.1109/ICIP.2013.6738017
   Liu XH, 2014, IEEE T IMAGE PROCESS, V23, P4361, DOI 10.1109/TIP.2014.2347204
   Liu XH, 2013, IEEE T IMAGE PROCESS, V22, P5226, DOI 10.1109/TIP.2013.2283400
   Pyatykh S, 2013, IEEE T IMAGE PROCESS, V22, P687, DOI 10.1109/TIP.2012.2221728
   Sendur L, 2002, IEEE T SIGNAL PROCES, V50, P2744, DOI 10.1109/TSP.2002.804091
   Sheikh H.R., Live Image Quality Assessment Database
   Shima T, 2010, IEEE T PATTERN ANAL, V32, P961, DOI 10.1109/TPAMI.2009.99
   Shin DH, 2005, IEEE T CONSUM ELECTR, V51, P218, DOI 10.1109/TCE.2005.1405723
   Shujian Yu, 2013, International Journal of Machine Learning and Computing, V3, P158, DOI 10.7763/IJMLC.2013.V3.293
   Sun ZY, 2014, EURASIP J WIREL COMM, DOI 10.1186/1687-1499-2014-58
   Sunter A., 1977, Applied Statistics, V26, P261, DOI DOI 10.2307/2346966
   Türetken E, 2017, IEEE T MED IMAGING, V36, P942, DOI 10.1109/TMI.2016.2640859
   Virtanen T, 2015, IEEE T IMAGE PROCESS, V24, P390, DOI 10.1109/TIP.2014.2378061
   Wang XC, 2016, IEEE T PATTERN ANAL, V38, P2312, DOI 10.1109/TPAMI.2015.2513406
   Wang XC, 2014, LECT NOTES COMPUT SC, V8689, P17, DOI 10.1007/978-3-319-10590-1_2
   Xu G, 2017, COMPUT AIDED DESIGN, V91, P1, DOI 10.1016/j.cad.2017.04.002
   Zhang YH, 2016, COMPUT GRAPH-UK, V58, P73, DOI 10.1016/j.cag.2016.05.010
   Zoran D, 2009, IEEE I CONF COMP VIS, P2209, DOI 10.1109/ICCV.2009.5459476
NR 40
TC 6
Z9 6
U1 1
U2 5
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2019
VL 35
IS 1
BP 5
EP 21
DI 10.1007/s00371-017-1437-y
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA HL6ZO
UT WOS:000458885900003
DA 2024-07-18
ER

PT J
AU Aristidou, A
   Stavrakis, E
   Papaefthimiou, M
   Papagiannakis, G
   Chrysanthou, Y
AF Aristidou, Andreas
   Stavrakis, Efstathios
   Papaefthimiou, Margarita
   Papagiannakis, George
   Chrysanthou, Yiorgos
TI Style-based motion analysis for dance composition
SO VISUAL COMPUTER
LA English
DT Article
DE Laban Movement Analysis; Motion Graphs; Motion style; Motion synthesis
ID SYSTEM; MODEL; RETRIEVAL
AB Synthesizing human motions from existing motion capture data is the approach of choice in most applications requiring high- quality visual results. Usually to synthesize motion, short motion segments are concatenated into longer sequences by finding transitions at points where character poses are similar. If similarity is only a measure of posture correlation, without consideration for the stylistic variations of movement, the resulting motion might have unnatural discontinuities. Particularly prone to this problem are highly stylized motions, such as dance performances. This work presents a motion analysis framework, based on Laban Movement Analysis, that also accounts for stylistic variations of the movement. Implemented in the context of Motion Graphs, it is used to eliminate potentially problematic transitions and synthesize style-coherent animation, without requiring prior labeling of the data. The effectiveness of our method is demonstrated by synthesizing contemporary dance performances that include a variety of different emotional states. The algorithm is able to compose highly stylized motions that are reminiscent to dancing scenarios using only plausible movements from existing clips.
C1 [Aristidou, Andreas] Univ Cyprus, Graph Virtual Real Lab, Nicosia, Cyprus.
   [Stavrakis, Efstathios] Univ Cyprus, Nicosia, Cyprus.
   [Chrysanthou, Yiorgos] Univ Cyprus, Comp Sci Dept, Nicosia, Cyprus.
   [Aristidou, Andreas] Interdisciplinary Ctr Herzliya, Herzliyya, Israel.
   [Papaefthimiou, Margarita] Fdn Res & Technol Hellas, Inst Comp Sci, Iraklion, Greece.
   [Papagiannakis, George] Univ Crete, Comp Sci Dept, Rethimnon, Greece.
C3 University of Cyprus; University of Cyprus; University of Cyprus;
   Reichman University; Foundation for Research & Technology - Hellas
   (FORTH); University of Crete
RP Aristidou, A (corresponding author), Univ Cyprus, Graph Virtual Real Lab, Nicosia, Cyprus.; Aristidou, A (corresponding author), Interdisciplinary Ctr Herzliya, Herzliyya, Israel.
EM a.aristidou@ieee.org
RI papagiannakis, george/AAI-7973-2020; Aristidou, Andreas/AAI-8096-2020;
   Stavrakis, Efstathios/I-8232-2014
OI papagiannakis, george/0000-0002-2977-9850; Aristidou,
   Andreas/0000-0001-7754-0791; Stavrakis, Efstathios/0000-0002-9213-7690
FU European Regional Development Fund; Republic of Cyprus through the
   Research Promotion Foundation [DIDAKTOR/0311/73]
FX This work is co-financed by the European Regional Development Fund and
   the Republic of Cyprus through the Research Promotion Foundation Under
   Contract DIDAKTOR/0311/73
CR Alexiadis Dimitrios S., 2014, IEEE T MULTIMEDIA, V99, P1
   [Anonymous], 2010, SCA'10: proceedings of the 2010 ACM SIGGRAPH/Eurographics symposium on computer animation, DOI [10.2312/SCA/SCA10/001-010, DOI 10.2312/SCA/SCA10/001-010]
   [Anonymous], 2013, Proceedings of the ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games, I3D'13
   Arikan O, 2002, ACM T GRAPHIC, V21, P483, DOI 10.1145/566570.566606
   Aristidou A, 2017, ACM SIGGRAPH / EUROGRAPHICS SYMPOSIUM ON COMPUTER ANIMATION (SCA 2017), DOI 10.1145/3099564.3099566
   Aristidou A, 2015, COMPUT GRAPH FORUM, V34, P262, DOI 10.1111/cgf.12598
   Aristidou A, 2015, ACM J COMPUT CULT HE, V8, DOI 10.1145/2755566
   Brand M, 2000, COMP GRAPH, P183, DOI 10.1145/344779.344865
   Calvert T, 2005, IEEE COMPUT GRAPH, V25, P6, DOI 10.1109/MCG.2005.33
   Chan JCP, 2011, IEEE T LEARN TECHNOL, V4, P187, DOI 10.1109/TLT.2010.27
   Chao MW, 2012, IEEE T VIS COMPUT GR, V18, P729, DOI 10.1109/TVCG.2011.53
   Chi D, 2000, COMP GRAPH, P173, DOI 10.1145/344779.352172
   Durupinar F, 2017, ACM T GRAPHIC, V36, DOI 10.1145/2983620
   Hartmann B, 2002, COMP ANIM CONF PROC, P111, DOI 10.1109/CA.2002.1017516
   Hartmann B, 2006, LECT NOTES ARTIF INT, V3881, P188
   Hartmann S, 2015, VISUAL COMPUT, V31, P893, DOI 10.1007/s00371-015-1114-y
   Holden D, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925975
   Hsu E, 2005, ACM T GRAPHIC, V24, P1082, DOI 10.1145/1073204.1073315
   Kovar L, 2002, ACM T GRAPHIC, V21, P473, DOI 10.1145/566570.566605
   Kovar L, 2004, ACM T GRAPHIC, V23, P559, DOI 10.1145/1015706.1015760
   Laban Rudolf., 2011, MASTERY MOVEMENT
   Lee JH, 2002, ACM T GRAPHIC, V21, P491
   Li Y, 2002, ACM T GRAPHIC, V21, P465
   Muller Meinard., 2006, P ACM SIGGRAPHEUROGR, P137
   Min J., 2010, P 2010 ACM SIGGRAPH, DOI [10.1145/1730804.1730811, DOI 10.1145/1730804.1730811]
   Min JY, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366172
   Müller M, 2005, ACM T GRAPHIC, V24, P677, DOI 10.1145/1073204.1073247
   Nakata T., 2002, J ROBOT MECHATRON, V14, P27, DOI DOI 10.20965/JRM.2002.P0027
   Okajima S, 2012, STUD COMPUT INTELL, V376, P117
   Oliveira J. L., 2010, P IROS 10
   Pearson K, 1920, BIOMETRIKA, V13, P1
   Pejsa T, 2010, COMPUT GRAPH FORUM, V29, P202, DOI 10.1111/j.1467-8659.2009.01591.x
   Pengcheng Luo, 2012, Motion in Games. 5th International Conference (MIG 2012). Proceedings, P254, DOI 10.1007/978-3-642-34710-8_24
   Ren L, 2005, ACM T GRAPHIC, V24, P1090, DOI 10.1145/1073204.1073316
   RUSSELL JA, 1980, J PERS SOC PSYCHOL, V39, P1161, DOI 10.1037/h0077714
   Senecal S, 2016, COMPUT ANIMAT VIRT W, V27, P311, DOI 10.1002/cav.1714
   Shapiro A, 2006, PROC GRAPH INTERF, P33
   Shiratori T, 2006, COMPUT GRAPH FORUM, V25, P449, DOI 10.1111/j.1467-8659.2006.00964.x
   Tang J.K., 2011, P 5 INT C UB INF MAN, DOI 10.1145/1968613.1968674
   Torresani L., 2006, Neural Information Processing Systems, NIPS, P1393
   Urtasun R, 2004, COMPUT GRAPH FORUM, V23, P799, DOI 10.1111/j.1467-8659.2004.00809.x
   Vasilescu MAO, 2002, INT C PATT RECOG, P456, DOI 10.1109/ICPR.2002.1047975
   Wakayama Y, 2010, LECT NOTES ARTIF INT, V6279, P251, DOI 10.1007/978-3-642-15384-6_27
   Wilke L, 2005, COMPUT ANIMAT VIRT W, V16, P201, DOI 10.1002/cav.90
   Xia SH, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766999
   Yang Y, 2013, COMPUT EDUC, V61, P1, DOI 10.1016/j.compedu.2012.09.006
   Yumer ME, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925955
   Zhao LW, 2005, GRAPH MODELS, V67, P1, DOI 10.1016/j.gmod.2004.08.002
NR 48
TC 20
Z9 21
U1 0
U2 12
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2018
VL 34
IS 12
BP 1725
EP 1737
DI 10.1007/s00371-017-1452-z
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GY3WM
UT WOS:000448487400009
DA 2024-07-18
ER

PT J
AU Altantsetseg, E
   Khorloo, O
   Konno, K
AF Altantsetseg, Enkhbayar
   Khorloo, Oyundolgor
   Konno, Kouichi
TI Rigid registration of noisy point clouds based on higher-dimensional
   error metrics
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 35th Computer Graphics International conference (CGI)
CY JUN 11-14, 2018
CL INDONESIA
SP Comp Graph Soc, Nanyang Technol Univ
DE Noisy point cloud; Registration; Area minimization; Volume minimization
ID ICP
AB Methods based on distance error metrics, such as the iterative closest point (ICP) algorithm and its variants, do not efficiently register noisy point clouds. In this paper, we propose a novel method for registering noisy point clouds by extending the ICP algorithm. The proposed method, which is based on higher-dimensional error metrics minimization, has two variants: One variant is based on area error metric, and the other is based on volume error metric. For the registration of point clouds, triangles or tetrahedrons are constructed between the point clouds by using an optimal vertices selection algorithm. To reduce computational complexity, the method is linearized by assuming that the rotation angle is small. The main advantage of the proposed method is its robustness for the registration of noisy point clouds. In particular, the volume minimization-based registration variant exhibits good robustness in the presence of strong noise. The proposed method was compared with the variants of ICP algorithm in experiments conducted on many types of point clouds, such as noisy point clouds with different noise levels. The experimental results obtained show that the robustness of the registration is increased by using higher-dimensional error metrics.
C1 [Altantsetseg, Enkhbayar] Natl Univ Mongolia, Sch Engn & Appl Sci, Ulaanbaatar, Mongolia.
   [Khorloo, Oyundolgor] Natl Univ Mongolia, Sch Engn & Appl Sci, Dept Informat & Comp Sci, Ulaanbaatar, Mongolia.
   [Konno, Kouichi] Iwate Univ, Fac Engn, Morioka, Iwate, Japan.
C3 National University of Mongolia; National University of Mongolia; Iwate
   University
RP Altantsetseg, E (corresponding author), Natl Univ Mongolia, Sch Engn & Appl Sci, Ulaanbaatar, Mongolia.
EM enkhbayar.a@seas.num.edu.mn
RI Purevsuren, Tseveg/O-6645-2019
FU MJEED [JR14B16]; JSPS KAKENHI [JP15H02945]; National University of
   Mongolia [P2017-1031]
FX This study was funded by MJEED (Grant JR14B16), JSPS KAKENHI (Grant
   JP15H02945), and National University of Mongolia (Innovation Grant
   P2017-1031).
CR Amamra A, 2016, SIGNAL IMAGE VIDEO P, V10, P835, DOI 10.1007/s11760-015-0823-z
   [Anonymous], 2009, IEEE INT C ROB AUT
   [Anonymous], VIS COMPUT
   [Anonymous], IMAGE VIS COMPUT
   [Anonymous], 4 INT C 3 D DIG IM M
   BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791
   Boughorbel F, 2004, PATTERN RECOGN, V37, P1567, DOI 10.1016/j.patcog.2004.02.005
   Censi A, 2008, IEEE INT CONF ROBOT, P19, DOI 10.1109/ROBOT.2008.4543181
   CHEN Y, 1992, IMAGE VISION COMPUT, V10, P145, DOI 10.1016/0262-8856(92)90066-C
   Díez Y, 2015, ACM COMPUT SURV, V47, DOI 10.1145/2692160
   Fitzgibbon AW, 2003, IMAGE VISION COMPUT, V21, P1145, DOI 10.1016/j.imavis.2003.09.004
   Granger S, 2002, LECT NOTES COMPUT SC, V2353, P418
   Guo YL, 2014, IEEE T MULTIMEDIA, V16, P1377, DOI 10.1109/TMM.2014.2316145
   Holz D, 2015, IEEE ROBOT AUTOM MAG, V22, P110, DOI 10.1109/MRA.2015.2432331
   Jian B, 2011, IEEE T PATTERN ANAL, V33, P1633, DOI 10.1109/TPAMI.2010.223
   Low Kok-Lim., 2004, LINEAR LEAST SQUARES, P4
   Marani R, 2016, COMPUT-AIDED CIV INF, V31, P515, DOI 10.1111/mice.12184
   Mellado N, 2016, IEEE T VIS COMPUT GR, V22, P2160, DOI 10.1109/TVCG.2015.2505287
   Mitra N.J., 2004, P 2004 EUROGRAPHICSA, P22, DOI 10.1145/1057432.1057435
   Nagarajan S., 2010, THESIS
   Pauly M, 2003, COMPUT GRAPH FORUM, V22, P281, DOI 10.1111/1467-8659.00675
   Pottmann H, 2006, INT J COMPUT VISION, V67, P277, DOI 10.1007/s11263-006-5167-2
   Rusinkiewicz S, 2001, THIRD INTERNATIONAL CONFERENCE ON 3-D DIGITAL IMAGING AND MODELING, PROCEEDINGS, P145, DOI 10.1109/IM.2001.924423
   Salvi J, 2007, IMAGE VISION COMPUT, V25, P578, DOI 10.1016/j.imavis.2006.05.012
   Sandhu R., 2008, IEEE C COMPUTER VISI
   Segal AV, 2009, GEN ICP, DOI DOI 10.15607/RSS.2009.V.021
   Tam GKL, 2013, IEEE T VIS COMPUT GR, V19, P1199, DOI 10.1109/TVCG.2012.310
   Yang JL, 2016, IEEE T PATTERN ANAL, V38, P2241, DOI 10.1109/TPAMI.2015.2513405
   Yang JQ, 2016, INFORM SCIENCES, V346, P163, DOI 10.1016/j.ins.2016.01.095
   Yu Zhong, 2009, 2009 IEEE 12th International Conference on Computer Vision Workshops, ICCV Workshops, P689, DOI 10.1109/ICCVW.2009.5457637
NR 30
TC 7
Z9 8
U1 1
U2 19
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2018
VL 34
IS 6-8
SI SI
BP 1021
EP 1030
DI 10.1007/s00371-018-1534-6
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA GH6MC
UT WOS:000433557400024
DA 2024-07-18
ER

PT J
AU Liu, BZ
   Wu, H
   Su, WH
   Zhang, WC
   Sun, JG
AF Liu, Baozhen
   Wu, Hang
   Su, Weihua
   Zhang, Wenchang
   Sun, Jinggong
TI Rotation-invariant object detection using Sector-ring HOG and boosted
   random ferns
SO VISUAL COMPUTER
LA English
DT Article
DE Rotation-invariant detection; Sector-ring HOG; HOG; Boosted random ferns
   (BRFs)
ID IMAGE FEATURES; DESCRIPTORS; RECOGNITION
AB The histogram of oriented gradients (HOG) is widely used for image description and has proven to be very effective. In some practical applications that lack an assumption of the object's orientation, rotation-invariant detection is of vital significance. To address this problem, this paper presents a new visual feature, Sector-ring HOG (SRHOG), which is obtained by improving the gradient binning and spatial binning based on HOG. The new feature can convert planar image rotations into cyclic shifts of the final descriptor and thereby facilitate rotated object detection. After modifying boosted random ferns in SRHOG feature domain, we further propose two strategies for rotation-invariant object detection: one depends completely on the new feature's characteristic, and the other introduces an orientation estimation step. The former is more suitable to 'finding objects' and the latter can provide the higher orientation estimation accuracy. Both the use of supervised learning and working in the gradient space make our approaches effective and robust. We show these properties by thorough testing on the public Freestyle Motocross dataset and our dataset for victim detection in post-disaster rescue efforts.
C1 [Liu, Baozhen; Wu, Hang; Su, Weihua; Zhang, Wenchang; Sun, Jinggong] Acad Mil Med Sci, Inst Med Equipment, Tianjin, Peoples R China.
C3 Academy of Military Medical Sciences - China
RP Sun, JG (corresponding author), Acad Mil Med Sci, Inst Med Equipment, Tianjin, Peoples R China.
EM liubaozhen91@126.com; sunjg@vip.sina.com
RI Wu, Hang/F-5482-2014
FU Science & Technology Pillar Program of Tianjin, China [16YFZCSF00590]
FX This work was supported by Science & Technology Pillar Program of
   Tianjin, China (16YFZCSF00590).
CR Andriluka M, 2010, IEEE INT C INT ROBOT, P1740, DOI 10.1109/IROS.2010.5649223
   Cai N, 2017, VISUAL COMPUT, V33, P249, DOI 10.1007/s00371-015-1190-z
   Cheng G, 2016, PROC CVPR IEEE, P2884, DOI 10.1109/CVPR.2016.315
   Cheng G, 2016, IEEE T GEOSCI REMOTE, V54, P7405, DOI 10.1109/TGRS.2016.2601622
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Fergus R, 2003, PROC CVPR IEEE, P264
   Gauglitz S, 2011, PROCEEDINGS OF THE B, P1
   Gkioxari G., 2017, ICCV, P2961
   Huang C, 2005, IEEE I CONF COMP VIS, P446
   Kong Y, 2013, VISUAL COMPUT, V29, P861, DOI 10.1007/s00371-013-0847-8
   Lepetit V, 2006, IEEE T PATTERN ANAL, V28, P1465, DOI 10.1109/TPAMI.2006.188
   Liu K, 2014, INT J COMPUT VISION, V106, P342, DOI 10.1007/s11263-013-0634-z
   Liu K, 2012, PROC CVPR IEEE, P917, DOI 10.1109/CVPR.2012.6247766
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Mikolajczyk K, 2005, IEEE T PATTERN ANAL, V27, P1615, DOI 10.1109/TPAMI.2005.188
   Murtza I, 2017, VISUAL COMPUT, V33, P99, DOI 10.1007/s00371-015-1155-2
   Ozuysal M., 2007, CVPR, P1, DOI DOI 10.1109/CVPR.2007.383123
   Qi XB, 2014, IEEE T PATTERN ANAL, V36, P2199, DOI 10.1109/TPAMI.2014.2316826
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Schapire RE, 1999, MACH LEARN, V37, P297, DOI 10.1023/A:1007614523901
   Skibbe H, 2012, 2012 9TH IEEE INTERNATIONAL SYMPOSIUM ON BIOMEDICAL IMAGING (ISBI), P450, DOI 10.1109/ISBI.2012.6235581
   Takacs G, 2013, IEEE T IMAGE PROCESS, V22, P2970, DOI 10.1109/TIP.2012.2230011
   Takacs G, 2010, PROC CVPR IEEE, P934, DOI 10.1109/CVPR.2010.5540116
   Torralba A, 2007, IEEE T PATTERN ANAL, V29, P854, DOI 10.1109/TPAMI.2007.1055
   Vedaldi A, 2011, IEEE I CONF COMP VIS, P959, DOI 10.1109/ICCV.2011.6126339
   Villamizar M, 2010, PROC CVPR IEEE, P1038, DOI 10.1109/CVPR.2010.5540104
   Zhang WC, 2014, IEEE GEOSCI REMOTE S, V11, P74, DOI 10.1109/LGRS.2013.2246538
   Zhao GY, 2012, IEEE T IMAGE PROCESS, V21, P1465, DOI 10.1109/TIP.2011.2175739
NR 28
TC 18
Z9 18
U1 0
U2 15
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2018
VL 34
IS 5
BP 707
EP 719
DI 10.1007/s00371-017-1408-3
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GC3AW
UT WOS:000429657100009
DA 2024-07-18
ER

PT J
AU Zhang, Q
   Lin, JJ
   Li, WJ
   Shi, YJ
   Cao, GG
AF Zhang, Qing
   Lin, Jiajun
   Li, Wenju
   Shi, Yanjiao
   Cao, Guogang
TI Salient object detection via compactness and objectness cues
SO VISUAL COMPUTER
LA English
DT Article
DE Saliency detection; Objectness prior; Background prior; Color spatial
   distribution; Manifold ranking
ID REGION DETECTION; IMAGE; MODEL
AB Existing saliency detection algorithms are mainly patch-based. In this paper, we propose a simple but effective approach to detect salient objects by exploring both patch-level and object-level cues. First, we obtain the objectness saliency map with objectness algorithm to find potential object candidates without need of category information. Second, the compactness map is generated by measuring color spatial distribution, and then it is refined by eliminating regions connecting to the selected boundary. Finally, to enforce the consistency among salient regions, we adopt graph-based manifold ranking algorithm by constructing two graphs each using a regional property descriptor. Both qualitative and quantitative evaluations on four publicly available datasets demonstrate the robustness and efficiency of our proposed approach against 23 state-of-the-art methods in terms of six performance criterions.
C1 [Zhang, Qing; Li, Wenju; Shi, Yanjiao; Cao, Guogang] Shanghai Inst Technol, Sch Comp Sci & Informat Engn, Shanghai 201418, Peoples R China.
   [Lin, Jiajun] East China Univ Sci & Technol, Shanghai 200237, Peoples R China.
C3 Shanghai Institute of Technology; East China University of Science &
   Technology
RP Zhang, Q (corresponding author), Shanghai Inst Technol, Sch Comp Sci & Informat Engn, Shanghai 201418, Peoples R China.
EM zhangqing0329@gmail.com
RI , Guo/AAC-1388-2022
OI , Guo/0000-0002-2689-0932
FU National Natural Science Foundation of China [41671402, 61401281];
   Science Foundation of Shanghai [14ZR1440700]
FX This work is supported by the National Natural Science Foundation of
   China under Grant Nos. 61401281 and No.41671402 and Science Foundation
   of Shanghai under Grant No. 14ZR1440700.
CR Achanta R, 2008, LECT NOTES COMPUT SC, V5008, P66
   Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   Achanta R, 2010, IEEE IMAGE PROC, P2653, DOI 10.1109/ICIP.2010.5652636
   Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   Alexe B, 2012, IEEE T PATTERN ANAL, V34, P2189, DOI 10.1109/TPAMI.2012.28
   Borji Ali, 2019, [Computational Visual Media, 计算可视媒体], V5, P117
   Borji A, 2012, LECT NOTES COMPUT SC, V7573, P414, DOI 10.1007/978-3-642-33709-3_30
   Chang KY, 2011, IEEE I CONF COMP VIS, P914, DOI 10.1109/ICCV.2011.6126333
   Cheng MM, 2013, IEEE I CONF COMP VIS, P1529, DOI 10.1109/ICCV.2013.193
   Cheng MM, 2014, VISUAL COMPUT, V30, P443, DOI 10.1007/s00371-013-0867-4
   Cheng MM, 2011, PROC CVPR IEEE, P409, DOI 10.1109/CVPR.2011.5995344
   Goferman S, 2010, PROC CVPR IEEE, P2376, DOI DOI 10.1109/CVPR.2010.5539929
   Guo CL, 2010, IEEE T IMAGE PROCESS, V19, P185, DOI 10.1109/TIP.2009.2030969
   Harel J, 2006, Advances in Neural Information Processing Systems, V19
   Hou X., 2007, IEEE C COMP VIS PATT, V1, P1, DOI DOI 10.1109/CVPR.2007.383267
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Jia YQ, 2013, IEEE I CONF COMP VIS, P1761, DOI 10.1109/ICCV.2013.221
   Jiang B., 2013, ICCV, P2976
   Jiang HZ, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.110
   Jiang HZ, 2013, PROC CVPR IEEE, P2083, DOI 10.1109/CVPR.2013.271
   Jiang P, 2013, IEEE I CONF COMP VIS, P1976, DOI 10.1109/ICCV.2013.248
   Klein DA, 2011, IEEE I CONF COMP VIS, P2214, DOI 10.1109/ICCV.2011.6126499
   Li CY, 2015, PROC CVPR IEEE, P2710, DOI 10.1109/CVPR.2015.7298887
   Li HY, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2440174
   Li J, 2013, IEEE T PATTERN ANAL, V35, P996, DOI 10.1109/TPAMI.2012.147
   Li NY, 2014, PROC CVPR IEEE, P2806, DOI 10.1109/CVPR.2014.359
   Li X., 2013, P IEEE INT C COMP VI, P1665
   Liu T, 2011, IEEE T PATTERN ANAL, V33, P353, DOI 10.1109/TPAMI.2010.70
   Maksai A, 2016, PROC CVPR IEEE, P972, DOI 10.1109/CVPR.2016.111
   Margolin R., 2013, CVPR, P2083
   Margolin R, 2014, PROC CVPR IEEE, P248, DOI 10.1109/CVPR.2014.39
   Margolin R, 2013, VISUAL COMPUT, V29, P381, DOI 10.1007/s00371-012-0740-x
   Perazzi F, 2012, PROC CVPR IEEE, P733, DOI 10.1109/CVPR.2012.6247743
   Qi W., 2015, Comput. Vis. Media, V1, P309, DOI DOI 10.1007/s41095-015-0028-y
   Rahtu E, 2010, LECT NOTES COMPUT SC, V6315, P366, DOI 10.1007/978-3-642-15555-0_27
   Rosenfeld A, 2011, IEEE I CONF COMP VIS, P1371, DOI 10.1109/ICCV.2011.6126391
   Seo HJ, 2009, J VISION, V9, DOI 10.1167/9.12.15
   Shen XH, 2012, PROC CVPR IEEE, P853, DOI 10.1109/CVPR.2012.6247758
   Sun J, 2013, INT J COMPUT VISION, V104, P135, DOI 10.1007/s11263-013-0618-z
   Tong N, 2015, PROC CVPR IEEE, P1884, DOI 10.1109/CVPR.2015.7298798
   Vikram TN, 2012, PATTERN RECOGN, V45, P3114, DOI 10.1016/j.patcog.2012.02.009
   Wang JP, 2015, NEUROCOMPUTING, V152, P359, DOI 10.1016/j.neucom.2014.10.056
   Wang P, 2012, PROC CVPR IEEE, P3194, DOI 10.1109/CVPR.2012.6248054
   Wang XC, 2016, IEEE T PATTERN ANAL, V38, P2312, DOI 10.1109/TPAMI.2015.2513406
   Wei YC, 2012, LECT NOTES COMPUT SC, V7574, P29, DOI 10.1007/978-3-642-33712-3_3
   Xie YL, 2013, IEEE T IMAGE PROCESS, V22, P1689, DOI 10.1109/TIP.2012.2216276
   Yan Q, 2013, PROC CVPR IEEE, P1155, DOI 10.1109/CVPR.2013.153
   Yang C, 2013, PROC CVPR IEEE, P3166, DOI 10.1109/CVPR.2013.407
   Yang C, 2013, IEEE SIGNAL PROC LET, V20, P637, DOI 10.1109/LSP.2013.2260737
   Zhou DY, 2004, ADV NEUR IN, V16, P321
   Zhu WJ, 2014, PROC CVPR IEEE, P2814, DOI 10.1109/CVPR.2014.360
NR 51
TC 19
Z9 21
U1 0
U2 18
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2018
VL 34
IS 4
BP 473
EP 489
DI 10.1007/s00371-017-1354-0
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FY6BJ
UT WOS:000426924400003
DA 2024-07-18
ER

PT J
AU Kavafoglu, Z
   Kavafoglu, E
   Cimen, G
   Capin, T
   Gurcay, H
AF Kavafoglu, Zumra
   Kavafoglu, Ersan
   Cimen, Gokcen
   Capin, Tolga
   Gurcay, Hasmet
TI Style-based biped walking control
SO VISUAL COMPUTER
LA English
DT Article
DE Physics-based animation; Character animation; Biped walking control;
   Data-driven animation
AB We present a control approach for synthesizing physics-based walking motions that mimic the style of a given reference walking motion. Style transfer between the reference motion and its physically simulated counterpart is achieved via extracted high-level features like the trajectory of the swing ankle and the twist of the swing leg during stepping. The physically simulated motion is also capable of tracking the intra-step variations of the sagittal character center of mass velocity of the reference walking motion. This is achieved by an adaptive velocity control strategy which is fed by a gain-deviation relation curve learned offline. This curve is learned from a number of training walking motions once and is used for velocity control of other reference walking motions. The control approach is tested with motion capture data of several walking motions of different styles. The approach also enables generating various styles manually or by varying the high-level features of an existing motion capture data. The demonstrations show that the proposed control framework is capable of synthesizing robust motions which mimic the desired style regardless of the changing environment or character proportions.
C1 [Kavafoglu, Zumra] Hacettepe Univ, Dept Math, Ankara, Turkey.
   [Kavafoglu, Ersan; Gurcay, Hasmet] Hacettepe Univ, Comp Graph Dept, Ankara, Turkey.
   [Cimen, Gokcen] ETH, Dept Comp Sci, Zurich, Switzerland.
   [Capin, Tolga] TED Univ, Dept Comp Engn, Ankara, Turkey.
C3 Hacettepe University; Hacettepe University; Swiss Federal Institutes of
   Technology Domain; ETH Zurich; Ted University
RP Kavafoglu, Z (corresponding author), Hacettepe Univ, Dept Math, Ankara, Turkey.
EM zumra.kavafoglu@gmail.com
RI Gürçay, Haşmet/G-9125-2013; Çapın, Tolga Kurtuluş/G-6172-2018
OI Gürçay, Haşmet/0000-0002-9797-4666; 
FU Scientific and Technological Research Council of Turkey (Turkiye
   Bilimsel ve Teknolojik Arastirma Kurumu) [112E105]
FX This work is supported by the Scientific and Technological Research
   Council of Turkey (Turkiye Bilimsel ve Teknolojik Arastirma Kurumu,
   Project No. 112E105). The authors would like to thank anonymous
   reviewers for their helpful comments to improve this paper.
CR Abe Y, 2011, IEEE COMPUT GRAPH, V31, P22, DOI 10.1109/MCG.2011.62
   Backman R, 2013, COMPUT ANIMAT VIRT W, V24, P553, DOI 10.1002/cav.1547
   Barliya A, 2013, EXP BRAIN RES, V225, P159, DOI 10.1007/s00221-012-3357-4
   Carensac Samuel, 2015, P 8 ACM SIGRAPH C MO, P177, DOI DOI 10.1145/2822013.2822016
   Coros S, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1781156
   Coros S, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618516
   Coros S, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409066
   Coumans Erwin., 2005, Bullet physics engine
   da Silva M, 2008, COMPUT GRAPH FORUM, V27, P371, DOI 10.1111/j.1467-8659.2008.01134.x
   da Silva M, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360681
   de Lasa M, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1781157
   Firmin M, 2015, COMPUT GRAPH FORUM, V34, P50, DOI 10.1111/cgf.12607
   Geijtenbeek T., 2012, ACM SIGGRAPH/Eurographics Symposium on Computer Animation, P211
   Hansen N, 2006, STUD FUZZ SOFT COMP, V192, P75
   Igel C, 2008, J MACH LEARN RES, V9, P993
   Jain S, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2019627.2019637
   Jones E., 2001, SciPy: Open source scientific tools for Python
   Kang GE, 2016, J BIOMECH, V49, P4022, DOI 10.1016/j.jbiomech.2016.10.044
   Laszio J., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P155, DOI 10.1145/237170.237231
   Laszlo J, 2000, COMP GRAPH, P201, DOI 10.1145/344779.344876
   Lee Y, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1781155
   Levenberg K., 1944, Quarterly of Applied Mathematics, V2, P164, DOI [10.1090/QAM/10666, DOI 10.1090/QAM/10666]
   Levine Sergey, 2012, P 11 ACM SIGGRAPH EU, P221
   Liu LB, 2015, COMPUT GRAPH FORUM, V34, P415, DOI 10.1111/cgf.12571
   Liu LB, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778865
   MARQUARDT DW, 1963, J SOC IND APPL MATH, V11, P431, DOI 10.1137/0111030
   MIDAS, 2015, HUM MOT DAT
   Muico U, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531387
   OBERG T, 1993, J REHABIL RES DEV, V30, P210
   Paroczai R., 2006, PHYS ED SPORT, V4, P49
   Pratt J, 1997, IEEE INT CONF ROBOT, P193, DOI 10.1109/ROBOT.1997.620037
   Sharon D, 2005, IEEE INT CONF ROBOT, P2387, DOI 10.1109/robot.2005.1570470
   Sok KW, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1276377.1276511, 10.1145/1239451.1239558]
   SUNADA C, 1994, IEEE INT CONF ROBOT, P1910, DOI 10.1109/ROBOT.1994.351182
   Tanawongsuwan R, 2003, LECT NOTES COMPUT SC, V2688, P715
   Tsai YY, 2010, IEEE T VIS COMPUT GR, V16, P325, DOI 10.1109/TVCG.2009.76
   Wang JM, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618514
   Ye YT, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778811
   Yin KK, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239556
NR 39
TC 7
Z9 7
U1 1
U2 18
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2018
VL 34
IS 3
BP 359
EP 375
DI 10.1007/s00371-016-1338-5
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FW2CZ
UT WOS:000425110900005
DA 2024-07-18
ER

PT J
AU Guo, XY
   Xiao, J
   Wang, Y
AF Guo, Xiaoyuan
   Xiao, Jun
   Wang, Ying
TI A survey on algorithms of hole filling in 3D surface reconstruction
SO VISUAL COMPUTER
LA English
DT Article
DE Hole filling; Surface reconstruction; Point clouds; Triangle mesh
ID COMPLETION
AB The surface reconstruction of 3D objects has attracted more and more attention for its widespread application in many areas, such as computer science, cultural heritage restoration, medical facilities, entertainment. However, due to occlusion, reflectance, the scanning angle, raw data preprocessing, it is inevitable to lose some point data, which leads to holes in the reconstruction surface, making it undesirable for various applications. Therefore, methods for filling holes in the process of surface reconstruction are critical to the final results of reconstruction. This paper makes a survey of existing well-known hole-filling algorithms, classifies the algorithms into two main categories, analyzes and compares these algorithms from the viewpoints of theories and experimental results to make a clear introduction of their performance. At the end, the paper points out the possible development direction of hole filling in the future and hopes to be a good guide for other researchers.
C1 [Guo, Xiaoyuan; Xiao, Jun; Wang, Ying] Univ Chinese Acad & Sci, Sch Engn Sci, Beijing, Peoples R China.
C3 Chinese Academy of Sciences; University of Chinese Academy of Sciences,
   CAS
RP Xiao, J (corresponding author), Univ Chinese Acad & Sci, Sch Engn Sci, Beijing, Peoples R China.
EM xiaojun@ucas.ac.cn
RI Guo, Xiaoyuan/AAC-2406-2019
OI Xiao, Jun/0000-0002-1799-3948
FU National Natural Science Foundation of China [61471338]; President Fund
   of UCAS; Youth Innovation Promotion Association CAS [2015361]
FX This work is supported by the National Natural Science Foundation of
   China (No. 61471338), President Fund of UCAS, Youth Innovation Promotion
   Association CAS (2015361).
CR Attene M, 2013, ACM COMPUT SURV, V45, DOI 10.1145/2431211.2431214
   Attene M, 2010, VISUAL COMPUT, V26, P1393, DOI 10.1007/s00371-010-0416-3
   Bendels G.H., 2006, J WSCG, V14
   Branch J, 2007, THIRD INTERNATIONAL SYMPOSIUM ON 3D DATA PROCESSING, VISUALIZATION, AND TRANSMISSION, PROCEEDINGS, P727
   Casciola G, 2005, NUMER ALGORITHMS, V39, P289, DOI 10.1007/s11075-004-3643-8
   Centin M, 2015, COMPUT AIDED GEOM D, V35-36, P42, DOI 10.1016/j.cagd.2015.03.006
   Chalmoviansky P, 2003, LECT NOTES COMPUT SC, V2768, P196
   Davis J, 2002, FIRST INTERNATIONAL SYMPOSIUM ON 3D DATA PROCESSING VISUALIZATION AND TRANSMISSION, P428, DOI 10.1109/TDPVT.2002.1024098
   Dellepiane M., COMP VIS WORKSH ICCV, P939
   Doria D., 2012, 2012 IEEE COMPUTER S, P65, DOI [10.1109/cvprw.2012.6238916, DOI 10.1109/CVPRW.2012.6238916]
   Franchini E, 2010, NUMER MATH-THEORY ME, V3, P405, DOI 10.4208/nmtma.2010.m9009
   Guo TQ, 2006, PROCEEDINGS OF 2006 INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND CYBERNETICS, VOLS 1-7, P4370
   Harary G, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2532548
   HOPPE H, 1992, COMP GRAPH, V26, P71, DOI 10.1145/142920.134011
   Ju T, 2004, ACM T GRAPHIC, V23, P888, DOI 10.1145/1015706.1015815
   Jun Y, 2005, COMPUT AIDED DESIGN, V37, P263, DOI 10.1016/j.cad.2004.06.012
   Kirkvik A. S., 2010, COMPLETING MODEL BAS
   Liepa P., 2003, Symposium on Geometry Processing, P200
   Mingqiang Wei, 2010, 2010 Proceedings of International Conference on Artificial Intelligence and Computational Intelligence (AICI 2010), P306, DOI 10.1109/AICI.2010.302
   Na-Eun Yang, 2012, Proceedings of the 2012 IEEE International Conference on Signal Processing, Communications and Computing (ICSPCC), P658, DOI 10.1109/ICSPCC.2012.6335696
   Park S, 2006, VISUAL COMPUT, V22, P168, DOI 10.1007/s00371-006-0374-y
   Qiang H., 2010, COMPUTER APPL SYSTEM, V3
   Quinsat Y, 2015, INT J ADV MANUF TECH, V81, P411, DOI 10.1007/s00170-015-7185-0
   Rudolf F, 2015, LECT NOTES COMPUT SC, V9374, P293, DOI 10.1007/978-3-319-26520-9_32
   Sharf A, 2004, ACM T GRAPHIC, V23, P878, DOI 10.1145/1015706.1015814
   Tekumalla L. S., 2014, ARXIV14115993
   Tran TT, 2015, 3D RES, V6, DOI 10.1007/s13319-015-0076-1
   Wang DN, 2003, XVI BRAZILIAN SYMPOSIUM ON COMPUTER GRAPHICS AND IMAGE PROCESSING, PROCEEDINGS, P11
   Weber Christopher, 2010, Proceedings of the Shape Modeling International (SMI 2010), P175, DOI 10.1109/SMI.2010.32
   Wu XJ, 2014, 2014 11TH WORLD CONGRESS ON INTELLIGENT CONTROL AND AUTOMATION (WCICA), P5329, DOI 10.1109/WCICA.2014.7053624
   Xie H, 2004, IEEE VISUALIZATION 2004, PROCEEEDINGS, P259, DOI 10.1109/VISUAL.2004.101
NR 31
TC 43
Z9 51
U1 2
U2 60
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2018
VL 34
IS 1
BP 93
EP 103
DI 10.1007/s00371-016-1316-y
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FR5XF
UT WOS:000419139200010
DA 2024-07-18
ER

PT J
AU Zha, ZY
   Liu, X
   Zhang, XG
   Chen, Y
   Tang, L
   Bai, YC
   Wang, Q
   Shang, ZH
AF Zha, Zhiyuan
   Liu, Xin
   Zhang, Xinggan
   Chen, Yang
   Tang, Lan
   Bai, Yechao
   Wang, Qiong
   Shang, Zhenhong
TI Compressed sensing image reconstruction via adaptive sparse nonlocal
   regularization
SO VISUAL COMPUTER
LA English
DT Article
DE Compressed sensing; Nonlocal self-similarity; Dictionary learning; Split
   Bregman iteration
ID RECOVERY; REPRESENTATIONS; DICTIONARIES; ALGORITHM
AB Compressed sensing (CS) has been successfully utilized by many computer vision applications. However,the task of signal reconstruction is still challenging, especially when we only have the CS measurements of an image (CS image reconstruction). Compared with the task of traditional image restoration (e.g., image denosing, debluring and inpainting, etc.), CS image reconstruction has partly structure or local features. It is difficult to build a dictionary for CS image reconstruction from itself. Few studies have shown promising reconstruction performance since most of the existing methods employed a fixed set of bases (e.g., wavelets, DCT, and gradient spaces) as the dictionary, which lack the adaptivity to fit image local structures. In this paper, we propose an adaptive sparse nonlocal regularization (ASNR) approach for CS image reconstruction. In ASNR, an effective self-adaptive learning dictionary is used to greatly reduce artifacts and the loss of fine details. The dictionary is compact and learned from the reconstructed image itself rather than natural image dataset. Furthermore, the image sparse nonlocal (or nonlocal self-similarity) priors are integrated into the regularization term, thus ASNR can effectively enhance the quality of the CS image reconstruction. To improve the computational efficiency of the ASNR, the split Bregman iteration based technique is also developed, which can exhibit better convergence performance than iterative shrinkage/thresholding method. Extensive experimental results demonstrate that the proposed ASNR method can effectively reconstruct fine structures and suppress visual artifacts, outperforming state-of-the-art performance in terms of both the PSNR and visual measurements.
C1 [Zha, Zhiyuan; Zhang, Xinggan; Chen, Yang; Bai, Yechao; Wang, Qiong] Nanjing Univ, Sch Elect Sci & Engn, Nanjing 210023, Jiangsu, Peoples R China.
   [Liu, Xin] Univ Oulu, Ctr Machine Vis & Signal Anal, Oulu, Finland.
   [Liu, Xin] Xi An Jiao Tong Univ, Sch Elect & Informat Engn, Xian, Shaanxi, Peoples R China.
   [Tang, Lan] Nanjing Univ, Dept Elect Sci & Engn, Nanjing 210023, Jiangsu, Peoples R China.
   [Tang, Lan] Southeast Univ, Natl Mobile Commun Res Lab, Nanjing 210023, Jiangsu, Peoples R China.
   [Shang, Zhenhong] Kunming Univ Sci & Technol, Sch Informat Engn & Automat, Kunming 650500, Yunnan, Peoples R China.
C3 Nanjing University; University of Oulu; Xi'an Jiaotong University;
   Nanjing University; Southeast University - China; Kunming University of
   Science & Technology
RP Zha, ZY; Zhang, XG (corresponding author), Nanjing Univ, Sch Elect Sci & Engn, Nanjing 210023, Jiangsu, Peoples R China.
EM zhazhiyuan.mmd@gmail.com; zhxg@nju.edu.cn
RI 查, 志远/GWU-6390-2022; Liu, Xin/AAD-5166-2019; tang, lan/AIE-2839-2022
OI 查, 志远/0000-0002-5515-5339; Liu, Xin/0000-0002-2242-6139; 
FU Natural Science Foundation of China [61462052, 61571220]; open research
   fund of National Mobile Commune. Research Lab., Southeast University
   [2015D08]
FX This work was supported by the Natural Science Foundation of China
   (61462052, 61571220) and the open research fund of National Mobile
   Commune. Research Lab., Southeast University (No. 2015D08).
CR Aharon M, 2006, IEEE T SIGNAL PROCES, V54, P4311, DOI 10.1109/TSP.2006.881199
   [Anonymous], 2010, 100920105055 ARXIV
   Beck A, 2009, IEEE T IMAGE PROCESS, V18, P2419, DOI 10.1109/TIP.2009.2028250
   Becker S, 2011, SIAM J IMAGING SCI, V4, P1, DOI 10.1137/090756855
   Bhardwaj A, 2016, VISUAL COMPUT, V32, P591, DOI 10.1007/s00371-015-1075-1
   Buades A, 2005, PROC CVPR IEEE, P60, DOI 10.1109/cvpr.2005.38
   Candes E. J., 2006, PROC INT C MATH, V17, P1433, DOI DOI 10.4171/022-3/69
   Candès EJ, 2006, IEEE T INFORM THEORY, V52, P489, DOI 10.1109/TIT.2005.862083
   Candes EJ, 2005, IEEE T INFORM THEORY, V51, P4203, DOI 10.1109/TIT.2005.858979
   Chan T., 2005, MATH MODELS COMPUT V, V5, P1
   Daubechies I., 2003, ARXIVMATH0307152
   Dong W., IEEE T IMAGE PROCESS, V20, P1838
   Dong W., SIGNAL PROCESS IMAGE, V27, P1109
   Dong WS, 2013, IEEE T IMAGE PROCESS, V22, P1618, DOI 10.1109/TIP.2012.2235847
   Donoho DL, 2006, IEEE T INFORM THEORY, V52, P1289, DOI 10.1109/TIT.2006.871582
   Duarte M. F., IEEE SIGNAL PROCESS, V25, P83
   Egiazarian K, 2007, IEEE IMAGE PROC, P549
   Elad M, 2006, IEEE T IMAGE PROCESS, V15, P3736, DOI 10.1109/TIP.2006.881969
   Elad M, 2009, IEEE T INFORM THEORY, V55, P4701, DOI 10.1109/TIT.2009.2027565
   Figueiredo MAT, 2007, IEEE J-STSP, V1, P586, DOI 10.1109/JSTSP.2007.910281
   Gehm ME, 2007, OPT EXPRESS, V15, P14013, DOI 10.1364/OE.15.014013
   Gilboa G, 2008, MULTISCALE MODEL SIM, V7, P1005, DOI 10.1137/070698592
   Goldstein T, 2009, SIAM J IMAGING SCI, V2, P323, DOI 10.1137/080725891
   He LH, 2010, IEEE SIGNAL PROC LET, V17, P233, DOI 10.1109/LSP.2009.2037532
   He LH, 2009, IEEE T SIGNAL PROCES, V57, P3488, DOI 10.1109/TSP.2009.2022003
   Hitomi Y, 2011, IEEE I CONF COMP VIS, P287, DOI 10.1109/ICCV.2011.6126254
   Huang RJ, 2015, VISUAL COMPUT, V31, P1683, DOI 10.1007/s00371-014-1049-8
   Ignacio UA, 2007, VISUAL COMPUT, V23, P733, DOI 10.1007/s00371-007-0139-2
   Jung MY, 2011, IEEE T IMAGE PROCESS, V20, P1583, DOI 10.1109/TIP.2010.2092433
   Liu Q, 2016, VISUAL COMPUT, V32, P535, DOI 10.1007/s00371-015-1087-x
   Mairal J, 2010, J MACH LEARN RES, V11, P19
   MUMFORD D, 1989, COMMUN PUR APPL MATH, V42, P577, DOI 10.1002/cpa.3160420503
   Mun S, 2009, IEEE IMAGE PROC, P3021, DOI 10.1109/ICIP.2009.5414429
   Mun S, 2011, IEEE DATA COMPR CONF, P183, DOI 10.1109/DCC.2011.25
   Peyré G, 2008, MULTISCALE MODEL SIM, V7, P703, DOI 10.1137/07068881X
   Protter M, 2010, IEEE T SIGNAL PROCES, V58, P3471, DOI 10.1109/TSP.2010.2046596
   Ravishankar S, 2011, IEEE T MED IMAGING, V30, P1028, DOI 10.1109/TMI.2010.2090538
   SHARIFI K, 1995, IEEE T CIRC SYST VID, V5, P52, DOI 10.1109/76.350779
   Canh TN, 2015, IEEE IMAGE PROC, P2700, DOI 10.1109/ICIP.2015.7351293
   Tikhonov A. N., 1965, USSR Comput. Math. Math. Phys., V5, P93, DOI [DOI 10.1016/0041-5553(65)90150-3, 10.1016/0041-5553(65) 90150-3]
   Tropp JA, 2007, IEEE T INFORM THEORY, V53, P4655, DOI 10.1109/TIT.2007.909108
   Wang SL, 2012, PROC CVPR IEEE, P2216, DOI 10.1109/CVPR.2012.6247930
   Xiaolin Wu, 2009, 2009 Data Compression Conference. DCC 2009, P123, DOI 10.1109/DCC.2009.69
   Xu L, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024208
   Yang JB, 2015, IEEE T IMAGE PROCESS, V24, P106, DOI 10.1109/TIP.2014.2365720
   Zhan J, 2015, VISUAL COMPUT, V31, P575, DOI 10.1007/s00371-014-0984-8
   Zhang J, 2014, IEEE T IMAGE PROCESS, V23, P3336, DOI 10.1109/TIP.2014.2323127
   Zhang J, 2014, SIGNAL PROCESS, V103, P114, DOI 10.1016/j.sigpro.2013.09.025
   Zhang J, 2012, IEEE J EM SEL TOP C, V2, P380, DOI 10.1109/JETCAS.2012.2220391
   Zhang XQ, 2010, SIAM J IMAGING SCI, V3, P253, DOI 10.1137/090746379
NR 50
TC 39
Z9 39
U1 0
U2 63
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2018
VL 34
IS 1
BP 117
EP 137
DI 10.1007/s00371-016-1318-9
PG 21
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FR5XF
UT WOS:000419139200012
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Lan, L
   Yao, JF
   Huang, P
   Guo, XH
AF Lan, Lei
   Yao, Junfeng
   Huang, Ping
   Guo, Xiaohu
TI Medial-axis-driven shape deformation with volume preservation
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 34th Conference on Computer Graphics International (CGI)
CY JUN 27-30, 2017
CL Yokohama, JAPAN
SP Keio Univ, Fac Sci & Technol
DE Medial axis; Shape deformation; Implicit skinning; Volume preservation
AB The medial axis is a natural skeleton for shapes. However, it is rarely used in the existing skeleton-based shape deformation techniques. In this paper, we propose a novel medial-axis-driven skin surface deformation algorithm with volume preservation property. Specifically, an as-rigid-as-possible deformation scheme is used to deform the medial axis so that its local transform is as close as possible to a rigid transform. We maintain surface features of the deformed shape based on an implicit skinning method. Our experiments show that the proposed algorithm effectively preserves the volume of deformed shape and addresses the bending and twisting problems associated with traditional skeleton-based shape deformation techniques.
C1 [Lan, Lei; Yao, Junfeng; Huang, Ping] Xiamen Univ, Software Sch, Xiamen Shi, Peoples R China.
   [Guo, Xiaohu] Univ Texas Dallas, Comp Sci, Richardson, TX 75083 USA.
C3 Xiamen University; University of Texas System; University of Texas
   Dallas
RP Yao, JF (corresponding author), Xiamen Univ, Software Sch, Xiamen Shi, Peoples R China.
EM yao0010@xmu.edu.cn
RI lan, lan/IXD-5278-2023; lan, lan/JWO-3679-2024
FU Project of College Excellent Professional Leaders Overseas Visitor of
   Fujian Provincial Education Department; State Key Laboratory of Virtual
   Reality Technology and Systems, Beihang University [BUAA-VR-16KF-22];
   Div Of Information & Intelligent Systems; Direct For Computer & Info
   Scie & Enginr [1149737] Funding Source: National Science Foundation
FX The authors would like to thank the anonymous reviewers for their
   valuable suggestions. This work was partially supported by the Project
   of College Excellent Professional Leaders Overseas Visitor of Fujian
   Provincial Education Department in 2015, and the open funding project of
   State Key Laboratory of Virtual Reality Technology and Systems, Beihang
   University (Grant No. BUAA-VR-16KF-22).
CR Amenta N, 1999, DISCRETE COMPUT GEOM, V22, P481, DOI 10.1007/PL00009475
   Amenta Nina, 2001, P 6 ACM S SOL MOD AP
   [Anonymous], 2005, P 2005 S INTERACTIVE, DOI [DOI 10.1145/1053427.10534294, DOI 10.1145/1053427.1053429]
   [Anonymous], 2002, P 2002 ACM SIGGRAPHE
   [Anonymous], 2002, P 2002 ACM SIGGRAPHE
   Attali D, 1996, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, PROCEEDINGS - VOL III, P13, DOI 10.1109/ICIP.1996.560357
   Le BH, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925959
   Bloomenthal J, 1999, SHAPE MODELING INTERNATIONAL '99 - INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, PROCEEDINGS, P44, DOI 10.1109/SMA.1999.749322
   Blum H., 1967, MODELS PERCEPTION SP, P362, DOI DOI 10.1142/S0218654308001154
   Cengiz Oztireli A., 2013, Proceedings of the 12th ACM SIGGRAPH/Eurographics Symposium on Computer Animation, P155
   Chaussard J., 2009, DISCRETE GEOMETRY CO
   Chazal F, 2005, GRAPH MODELS, V67, P304, DOI 10.1016/j.gmod.2005.01.002
   Dey TK, 2004, COMPUT AIDED DESIGN, V36, P195, DOI 10.1016/S0010-4485(03)00061-7
   Du H., 2004, SM 04, P25
   Faraj N., 2013, SIGGRAPH ASIA 2013 T, P1
   Foskey M., 2003, J. Comput. Inf. Sci. Eng., V3, P274, DOI DOI 10.1145/781606.781623
   Garland M., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P209, DOI 10.1145/258734.258849
   Giulini D, 2010, FUND THEOR PHYS, V165, P83, DOI 10.1007/978-90-481-3475-5_4
   Guo XH, 2004, IEEE COMPUT GRAPH, V24, P43, DOI 10.1109/MCG.2004.16
   Huang J, 2006, ACM T GRAPHIC, V25, P1126, DOI 10.1145/1141911.1142003
   Jacobson A., 2011, ACM T GRAPHIC, V30, P61
   Kavan L, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409625.1409627
   Leclerq A, 2001, SPRING EUROGRAP, P37
   Li P, 2015, ACM T GRAPHIC, V35, DOI 10.1145/2753755
   Magnenat-Thalmann, 1988, Proceedings of Graphics Interface '88, P26
   Merry B, 2006, ACM T GRAPHIC, V25, P1400, DOI 10.1145/1183287.1183294
   Miklos B, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778838
   Mohr A, 2003, ACM T GRAPHIC, V22, P562, DOI 10.1145/882262.882308
   Mukai T, 2015, PROCEEDINGS - I3D 2015, P77, DOI 10.1145/2699276.2699278
   Müller M, 2005, ACM T GRAPHIC, V24, P471, DOI 10.1145/1073204.1073216
   RICCI A, 1973, COMPUT J, V16, P157, DOI 10.1093/comjnl/16.2.157
   Shen J., 2002, P IMPL SURF
   Singh K., 1995, P EUR WORKSH IMPL SU
   Sorkine O., 2007, As-rigid-as-possible surface modeling, P109, DOI 10.1145/1281991.1282006
   Sud A., 2005, P 2005 ACM S SOL PHY, P39, DOI DOI 10.1145/1060244.1060250
   Sumner RW, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239531
   Sun F., 2013, CORR
   Vaillant R., 2013, ACM T GRAPHIC, V32, P96
   Vaillant R, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2661229.2661264
   van Overveld CWAM, 1999, COMPUTER GRAPHICS INTERNATIONAL, PROCEEDINGS, P214, DOI 10.1109/CGI.1999.777957
   Yan YJ, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925938
   Yoshizawa S, 2007, COMPUT GRAPH FORUM, V26, P255, DOI 10.1111/j.1467-8659.2007.01047.x
   Zhou K, 2005, ACM T GRAPHIC, V24, P496, DOI 10.1145/1073204.1073219
   Zhuo W, 2013, COMPUT GRAPH FORUM, V32, P295, DOI 10.1111/cgf.12049
NR 44
TC 7
Z9 8
U1 0
U2 13
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2017
VL 33
IS 6-8
BP 789
EP 800
DI 10.1007/s00371-017-1401-x
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA EX1EY
UT WOS:000402964800011
DA 2024-07-18
ER

PT J
AU Liu, CX
   Zhao, JW
   Shen, YY
   Zhou, YG
   Wang, X
   Ouyang, Y
AF Liu, Chunxiao
   Zhao, Jinwei
   Shen, Yiyun
   Zhou, Yanggang
   Wang, Xun
   Ouyang, Yi
TI Texture filtering based physically plausible image dehazing
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 33rd Conference on Computer Graphics International (CGI)
CY JUN 28-JUL 01, 2016
CL Heraklion, GREECE
SP Fdn Res Technol
DE Image dehazing; Texture filtering; Patch shift; Atmospheric light; Halo
   elimination
ID ENHANCEMENT
AB To address the issues of false candidate atmospheric light, halo effects and color distortion in sky regions, a physically plausible single image dehazing algorithm is proposed based on texture filtering. First, gamma correction based preprocessing is applied to the luminance channel of the haze image, which improves the luminance and contrast of the haze image simultaneously. Second, a support vector machine based classifier is trained and utilized to reject the false candidate atmospheric lights. Third, the haze image is decomposed into sky and non-sky regions with a histogram analysis based sky detection and segmentation method. And, color correction of the sky regions is carried out with a pixel distribution shifting based white balance method. The non-sky regions are smoothed with a patch shift based bilateral texture filtering process, which can preserve edges and eliminate redundant details. Fourth, a transmission estimation method based on hybrid filtering is proposed to eliminate the halo effects. Finally, the haze-free non-sky regions are recovered by solving the haze imaging model, which are then merged with the color-corrected sky regions to form the final haze-free image. Experimental results demonstrate that our algorithm can locate the valid atmospheric light, diminish the halo effects and improve the visibility remarkably, which outperforms the state-of-the-art image dehazing methods.
C1 [Liu, Chunxiao; Zhao, Jinwei; Shen, Yiyun; Zhou, Yanggang; Wang, Xun; Ouyang, Yi] Zhejiang Gongshang Univ, Sch Comp Sci & Informat Engn, Hangzhou 310018, Peoples R China.
C3 Zhejiang Gongshang University
RP Liu, CX (corresponding author), Zhejiang Gongshang Univ, Sch Comp Sci & Informat Engn, Hangzhou 310018, Peoples R China.
EM cxliu@mail.zjgsu.edu.cn
RI Zhao, Jinwei/IQU-0651-2023
CR Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Chen ZY, 2014, PROC CVPR IEEE, P3003, DOI 10.1109/CVPR.2014.384
   Cho H, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601188
   Fattal R, 2014, ACM T GRAPHIC, V34, DOI 10.1145/2651362
   Fattal R, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360671
   He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168
   He KM, 2013, IEEE T PATTERN ANAL, V35, P1397, DOI 10.1109/TPAMI.2012.213
   Kim JH, 2013, J VIS COMMUN IMAGE R, V24, P410, DOI 10.1016/j.jvcir.2013.02.004
   Kopf J, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409069
   [刘兴云 Liu Xingyun], 2015, [中国图象图形学报, Journal of Image and Graphics], V20, P1453
   Naim MJNM, 2012, APPL SOFT COMPUT, V12, P2948, DOI 10.1016/j.asoc.2012.04.028
   Narasimhan SG, 2003, IEEE T PATTERN ANAL, V25, P713, DOI 10.1109/TPAMI.2003.1201821
   Narasimhan SG, 2000, PROC CVPR IEEE, P598, DOI 10.1109/CVPR.2000.855874
   Nayar S. K., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P820, DOI 10.1109/ICCV.1999.790306
   Schechner YY, 2001, PROC CVPR IEEE, P325
   Shwartz S., 2006, 2006 IEEE COMP SOC C, V2, P1984, DOI DOI 10.1109/CVPR.2006.71
   Tan RT, 2008, PROC CVPR IEEE, P2347, DOI 10.1109/cvpr.2008.4587643
   Treibitz T, 2009, PROC CVPR IEEE, P525, DOI 10.1109/CVPRW.2009.5206551
NR 18
TC 16
Z9 19
U1 2
U2 19
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2016
VL 32
IS 6-8
BP 911
EP 920
DI 10.1007/s00371-016-1259-3
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA DP8ET
UT WOS:000378731600023
DA 2024-07-18
ER

PT J
AU Shi, JL
   Sun, ZX
AF Shi, Jinlong
   Sun, Zhengxing
TI Large-scale three-dimensional measurement based on LED marker tracking
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 3DOR Workshop
CY APR 06, 2014
CL Strasbourg, FRANCE
DE Large-scale; Three-dimensional; Measurement; LED; Marker; Tracking
AB This paper presents a three-dimensional (3-D) measurement method of large-scale objects by integrating a 3-D scanner and a stereo tracker. To measure a large-scale object, some high-brightness light-emitting diode (LED) lights are rigidly connected to the 3-D scanner. During measurement, the stereo tracker remains stationary, and the 3-D scanner is moved to measure partial sections of a large object. Meanwhile, the LED lights are tracked by the stereo tracker to compute the poses of the 3-D scanner for aligning partial sections. The performance and effectiveness are evaluated by experiments.
C1 [Shi, Jinlong; Sun, Zhengxing] Nanjing Univ, State Key Lab Novel Software Technol, Nanjing 210008, Jiangsu, Peoples R China.
C3 Nanjing University
RP Sun, ZX (corresponding author), Nanjing Univ, State Key Lab Novel Software Technol, Nanjing 210008, Jiangsu, Peoples R China.
EM jlshifudan@gmail.com; szx@nju.edu.cn
RI shi, jin/KDO-7906-2024; Sun, Zhengxing/A-7411-2011; shi,
   jin/JCD-8826-2023; Shi, JIn/JYP-1805-2024
OI Sun, Zhengxing/0000-0001-7137-6169
NR 0
TC 8
Z9 8
U1 1
U2 9
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2016
VL 32
IS 2
BP 179
EP 190
DI 10.1007/s00371-015-1063-5
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA DF9FU
UT WOS:000371667200004
DA 2024-07-18
ER

PT J
AU Hao, CY
   Chen, YD
   Wu, W
   Wu, EH
AF Hao, Chuanyan
   Chen, Yadang
   Wu, Wen
   Wu, Enhua
TI An iterated randomized search algorithm for large-scale texture
   synthesis and manipulations
SO VISUAL COMPUTER
LA English
DT Article
DE Texture synthesis; Coherent propagation; Random search; Texture
   transfer; Approximate match patch
ID IMAGE
AB In this paper, we introduce a novel iterated random search method for large-scale texture synthesis and manipulations. Previous researches on texture synthesis and manipulation have reached a great achievement both on quality and performance. However, the cost of the popular exhaustive search-based methods is still high especially for large-scale and complex synthesis scenes. Our algorithm contributes great improvements on performances about 2-50 times over the typical patch-based synthesis methods. Texture patterns have been well-known formalized as a Markov Random Field (MRF) whose two hypotheses, stationarity and locality, drive our bold guess that a random sampling may just catch a good match and allows us to propagate the natural coherence in the neighborhood. Meanwhile, the iteration constantly updates the bad guesses to make our algorithm converge fast with the results in the state of the art. We also provide a simple theoretical analysis to compare our iterated randomized search model and the classical synthesis algorithms. Besides, this simple method turns out to work well in various applications as well, such as texture transfer, image completion and video synthesis.
C1 [Hao, Chuanyan; Chen, Yadang; Wu, Wen; Wu, Enhua] Univ Macau, FST, Macau, Peoples R China.
   [Wu, Enhua] Chinese Acad Sci, Inst Software, State Key Lab CS, Beijing, Peoples R China.
C3 University of Macau; Chinese Academy of Sciences; Institute of Software,
   CAS
RP Hao, CY (corresponding author), Univ Macau, FST, Macau, Peoples R China.
EM yb17408@umac.mo
RI Liu, xuefeng/IUP-1483-2023
FU NSFC [61202154]; University of Macau
FX Thanks are given to my friends, Mr. Yuanzhang Chang, Mr. Gang Sun and
   Mr. Xiaosheng Li for their helps and discussions. Thanks also to
   Professor Li-Yi Wei and Dr. Vivek Kwatra for sharing their texture
   datasets via their respective web sites. The research has been supported
   by NSFC Grant (61202154) and the research grant of University of Macau.
   And last but not the least, thanks are given to my families.
CR Agarwala A, 2004, ACM T GRAPHIC, V23, P294, DOI 10.1145/1015706.1015718
   [Anonymous], 2001, Schooling for Tomorrow
   Barnes C., 2010, LECT NOTES COMPUT SC, V6313, P29
   Barnes C, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531330
   Cohen MF, 2003, ACM T GRAPHIC, V22, P287, DOI 10.1145/882262.882265
   Criminisi A, 2004, IEEE T IMAGE PROCESS, V13, P1200, DOI 10.1109/TIP.2004.833105
   Darabi S, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185578
   De Benet J. S., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P361, DOI 10.1145/258734.258882
   Drori I, 2003, ACM T GRAPHIC, V22, P303, DOI 10.1145/882262.882267
   Efros A. A., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1033, DOI 10.1109/ICCV.1999.790383
   Efros AA, 2001, COMP GRAPH, P341, DOI 10.1145/383259.383296
   Enrique Sebastian., 2005, SIGGRAPH 05 ACM SIGG, P130, DOI DOI 10.1145/1187112.1187269
   Heeger D. J., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P229, DOI 10.1145/218380.218446
   Jianwei H., 2009, J SOFTW, V20, P3254
   Kwatra V, 2005, ACM T GRAPHIC, V24, P795, DOI 10.1145/1073204.1073263
   Kwatra V, 2003, ACM T GRAPHIC, V22, P277, DOI 10.1145/882262.882264
   Lefebvre S, 2005, ACM T GRAPHIC, V24, P777, DOI 10.1145/1073204.1073261
   Liang L, 2001, ACM T GRAPHIC, V20, P127, DOI 10.1145/501786.501787
   Pérez P, 2003, ACM T GRAPHIC, V22, P313, DOI 10.1145/882262.882269
   Portilla J, 2000, INT J COMPUT VISION, V40, P49, DOI 10.1023/A:1026553619983
   Schödl A, 2000, COMP GRAPH, P489, DOI 10.1145/344779.345012
   Szummer M, 1996, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, PROCEEDINGS - VOL III, P823, DOI 10.1109/ICIP.1996.560871
   Wei LY, 2000, COMP GRAPH, P479, DOI 10.1145/344779.345009
NR 23
TC 3
Z9 3
U1 0
U2 6
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2015
VL 31
IS 11
BP 1447
EP 1458
DI 10.1007/s00371-014-1025-3
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CT3BJ
UT WOS:000362680600002
DA 2024-07-18
ER

PT J
AU Flotynski, J
   Walczak, K
AF Flotynski, Jakub
   Walczak, Krzysztof
TI Conceptual knowledge-based modeling of interactive 3D content
SO VISUAL COMPUTER
LA English
DT Article
DE 3D web; Semantic web; 3D content; Ontology; Semantic 3D; Virtual and
   augmented reality
ID REPRESENTATION
AB Three-dimensional content offers a powerful medium enabling rich, interactive visualization in virtual and augmented reality systems, which are increasingly used in a variety of application domains, such as education, training, tourism and cultural heritage. The creation of interactive 3D presentations is typically a complex process covering diverse aspects of the content such as geometry, structure, space, appearance, animation and behavior. Recent trends in the development of the semantic web provide new opportunities for simplifying 3D content creation, which may be performed at different levels of abstraction and may encompass the inference of hidden knowledge, which may influence the created content. However, the available approaches to 3D content creation do not enable conceptual knowledge-based modeling of 3D content. The main contribution of this paper is an approach to semantic creation of 3D content. The proposed solution leverages the semantic web techniques to enable conceptual, knowledge-driven content creation. The proposed approach has been implemented and evaluated. It has been shown that the approach can significantly simplify modeling of advanced 3D content presentations in comparison with the available approaches.
C1 [Flotynski, Jakub; Walczak, Krzysztof] Poznan Univ Econ, Dept Informat Technol, PL-61875 Poznan, Poland.
C3 Poznan University of Economics & Business
RP Flotynski, J (corresponding author), Poznan Univ Econ, Dept Informat Technol, Niepodleglosci 10, PL-61875 Poznan, Poland.
EM jakub.flotynski@ue.poznan.pl; krzysztof.walczak@ue.poznan.pl
RI Flotyński, Jakub/IWU-8229-2023; Walczak, Krzysztof/AAF-9685-2021
OI Flotyński, Jakub/0000-0001-5104-2022; Walczak,
   Krzysztof/0000-0001-8170-7910
FU Polish National Science Centre [DEC-2012/07/B/ST6/01523]
FX This research work has been supported by the Polish National Science
   Centre Grant No. DEC-2012/07/B/ST6/01523.
CR [Anonymous], HALST METR
   [Anonymous], P 6 INT C HUM SYST I
   [Anonymous], 2014, P 5 DOCT C COMP EL I
   [Anonymous], METR XML PROJ
   [Anonymous], 2006, 11th international conference on 3D web technology, P85
   [Anonymous], 2008, P ACM VRST 2008
   [Anonymous], 2005, SPEC INT TRACKS POST
   [Anonymous], 2013, INFORM TECHNOLOGIES
   [Anonymous], 2009, P 2009 ACM S APPL CO
   Averkiou Melinos, 2014, Computer Graphics Forum, V33, P125, DOI 10.1111/cgf.12310
   Bilasco IM, 2005, IEEE INT SYM MULTIM, P310
   Bilasco IM, 2007, WEB3D 2007 - 12TH INTERNATIONAL CONFERENCE ON 3D WEB TECHNOLOGY, PROCEEDINGS, P97
   Bille W., 2004, Workshop of Intelligent Computing (WIC), P272
   Cavazza M, 2000, APPL ARTIF INTELL, V14, P125, DOI 10.1080/088395100117188
   Chaudhuri Siddhartha, 2013, P 26 ANN ACM S USER, P193, DOI [DOI 10.1145/2501988.2502008, 10.1145/2501988.2502008]
   De Luca L, 2007, VISUAL COMPUT, V23, P181, DOI 10.1007/s00371-006-0092-5
   De Troyer O., 2007, Tutorials, posters, panels and industrial contributions at the 26th international conference on Conceptual modeling-Volume 83, V83, P3
   Flotynski J., 2014, 18 INT C KN IN PRESS
   Flotynski J, 2013, FED CONF COMPUT SCI, P549
   Flotynski J, 2013, FED CONF COMPUT SCI, P541
   Flotynski J, 2013, LECT NOTES BUS INF P, V160, P244
   Flotynski Jakub, 2013, 1 INT C BUILDING EXP, P63
   Funkhouser T, 2004, ACM T GRAPHIC, V23, P652, DOI 10.1145/1015706.1015775
   Gutiérrez M, 2007, VISUAL COMPUT, V23, P207, DOI 10.1007/s00371-006-0093-4
   Gutierrez M, 2005, INT J COMPUT APPL T, V23, P229, DOI 10.1504/IJCAT.2005.006484
   Kalogerakis E, 2006, P IEEE VIRT REAL ANN, P43, DOI 10.1109/VR.2006.41
   Kozlenkov A., Prova Rule Language
   Laga H, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2516971.2516975
   Latoschik ME, 2007, IEEE VIRTUAL REALITY 2007, PROCEEDINGS, P305
   Latoschik ME, 2005, LECT NOTES COMPUT SC, V3638, P25
   Luck M, 2000, APPL ARTIF INTELL, V14, P3, DOI 10.1080/088395100117142
   Lugrin JL, 2007, 2007 INTERNATIONAL CONFERENCE ON INTELLIGENT USER INTERFACES, P225
   Parsia C., PELLET OWL 2 REASONE
   Pellens B, 2005, LECT NOTES COMPUT SC, V3762, P1215
   Pellens B., 2009, P 6 AUSTR C INT ENT, P1
   Shao TJ, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366155
   Sokolov D, 2008, VISUAL COMPUT, V24, P173, DOI 10.1007/s00371-007-0182-z
   Spagnuolo M, 2009, IEEE INTELL SYST, V24, P90, DOI 10.1109/MIS.2009.20
   W3C, WORLD WID WEB CONS
   Walczak K, 2006, COMPUTER, V39, P93, DOI 10.1109/MC.2006.108
   Walczak K., 2013, P 5 JOINT VIRT REAL, P41
   Walczak K, 2006, LECT NOTES COMPUT SC, V4270, P40
   Walczak K, 2008, EUROGR TECH REP SER, P135, DOI 10.1109/HSI.2008.4581455
   Wiebusch Dennis, 2012, 2012 5th Workshop on Software Engineering and Architectures for Realtime Interactive Systems, P43, DOI 10.1109/SEARIS.2012.6231168
   Zheng YY, 2013, COMPUT GRAPH FORUM, V32, P195, DOI 10.1111/cgf.12039
NR 45
TC 18
Z9 18
U1 4
U2 42
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2015
VL 31
IS 10
BP 1287
EP 1306
DI 10.1007/s00371-014-1011-9
PG 20
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CQ7CG
UT WOS:000360759800001
OA hybrid
DA 2024-07-18
ER

PT J
AU Cowan, B
   Rojas, D
   Kapralos, B
   Moussa, F
   Dubrowski, A
AF Cowan, Brent
   Rojas, David
   Kapralos, Bill
   Moussa, Fuad
   Dubrowski, Adam
TI Effects of sound on visual realism perception and task performance
SO VISUAL COMPUTER
LA English
DT Article
DE Serious games; Virtual simulation; Visual realism; Audiovisual cue
   interaction
ID FIDELITY
AB Before the application of virtual simulations and serious games for surgical education and training becomes more widespread, there are a number of open questions and issues that must be addressed including the relationship between realism, multi-modal cue interaction, immersion, and knowledge transfer and retention. Using the serious game surgical cognitive education and training framework developed specifically for cognitive surgical skills training, here we examine the effect of sound on visual realism perception and task completion time while performing a task within a virtual environment. Our preliminary experimental results indicate that the appropriate use of sound can lead to performance improvements when performing a task within a virtual environment without a corresponding decrease in the perception of visual realism.
C1 [Cowan, Brent; Rojas, David; Kapralos, Bill] Univ Ontario Inst Technol, Fac Business & Informat Technol, Hlth Educ Technol Res Unit, Oshawa, ON L1H 7K4, Canada.
   [Rojas, David] Univ Toronto, Inst Med Sci, Toronto, ON, Canada.
   [Moussa, Fuad] Sunnybrook Hlth Sci Ctr, Schulich Heart Ctr, Div Cardiac & Vasc Surg, Toronto, ON M4N 3M5, Canada.
   [Dubrowski, Adam] Mem Univ Newfoundland, Fac Med, Div Emergency Med, St John, NF, Canada.
   [Dubrowski, Adam] Mem Univ Newfoundland, Fac Med, Div Pediat, St John, NF, Canada.
C3 Ontario Tech University; University of Toronto; University of Toronto;
   Sunnybrook Health Science Center; Sunnybrook Research Institute;
   Memorial University Newfoundland; Memorial University Newfoundland
RP Kapralos, B (corresponding author), Univ Ontario Inst Technol, Fac Business & Informat Technol, Hlth Educ Technol Res Unit, 2000 Simcoe St North, Oshawa, ON L1H 7K4, Canada.
EM brent.cowan@uoit.ca; david.rojasgualdron@uoit.ca; bill.kapralos@uoit.ca;
   fuad.moussa@sunnybrook.ca; adam.dubrowski@gmail.com
FU Natural Sciences and Engineering Research Council of Canada (NSERC);
   Social Sciences and Humanities Research Council of Canada (SSHRC);
   Interactive & Multi-Modal Experience Research Syndicate (IMMERSe)
   initiative; Canadian Network of Centres of Excellence (NCE), Graphics,
   Animation, and New Media (GRAND) initiative
FX This work was supported in part by the Natural Sciences and Engineering
   Research Council of Canada (NSERC), the Social Sciences and Humanities
   Research Council of Canada (SSHRC), Interactive & Multi-Modal Experience
   Research Syndicate (IMMERSe) initiative, and the Canadian Network of
   Centres of Excellence (NCE), Graphics, Animation, and New Media (GRAND)
   initiative.
CR [Anonymous], 2009, Serious games: Mechanisms and effects, DOI DOI 10.4324/9780203891650
   [Anonymous], P 2 S APPL PERC GRAP
   BERTELSON P, 1981, PERCEPT PSYCHOPHYS, V29, P578, DOI 10.3758/BF03207374
   Blascovich J., 2011, Infinite reality
   Bonneel N, 2010, ACM T APPL PERCEPT, V7, DOI 10.1145/1658349.1658350
   Chang RS, 2010, EVOL PSYCHOL-US, V8, P260
   Conrad C, 2010, SURG ENDOSC, V24, P1347, DOI 10.1007/s00464-009-0772-0
   Cook DA, 2013, MED TEACH, V35, pE844, DOI 10.3109/0142159X.2012.714886
   Corti K., 2006, GAME BASED LEARNING
   Cowan B., 2010, Journal of Cybertherapy and Rehabilitation, V3, P285
   Farmer E., 1999, Handbook of simulator based training. Aldershot
   Faure F., 2012, ser. Studies in Mechanobiology, Tissue Engineering and Biomaterials, P283
   France L, 2005, MED IMAGE ANAL, V9, P123, DOI 10.1016/j.media.2004.11.006
   GODDEN DR, 1975, BRIT J PSYCHOL, V66, P325, DOI 10.1111/j.2044-8295.1975.tb01468.x
   Gorman PJ, 2000, AM J SURG, V180, P353, DOI 10.1016/S0002-9610(00)00514-6
   Halsted WS, 1904, B JOHNS HOPKINS HOSP, V15, P267
   Howard I.P., 1966, Human Spatial Orientation
   Hulusic V, 2008, WSCG 2008, FULL PAPERS, P41
   Hulusic V, 2012, COMPUT GRAPH FORUM, V31, P102, DOI 10.1111/j.1467-8659.2011.02086.x
   Hulusic V, 2011, VISUAL COMPUT, V27, P57, DOI 10.1007/s00371-010-0514-2
   Hulusic Vedad, 2009, P 25 SPRING C COMP G, P151
   Kapralos B, 2014, STUD COMPUT INTELL, V536, P284, DOI 10.1007/978-3-642-45432-5_14
   Kohls-Gatzoulis JA, 2004, CAN J SURG, V47, P277
   Larsson P, 2003, P 1 INT SPEECH COMM
   MCGURK H, 1976, NATURE, V264, P746, DOI 10.1038/264746a0
   Morfey C.L., 2001, DICT ACOUSTICS
   Muchinsky P. M., 1999, PSYCHOLOGY APPLIED T
   Norman G, 2012, MED EDUC, V46, P636, DOI 10.1111/j.1365-2923.2012.04243.x
   Paas F, 2003, EDUC PSYCHOL, V38, P1, DOI 10.1207/S15326985EP3801_1
   Praamsma M, 2008, CAN J SURG, V51, P442
   REZNICK RK, 1993, AM J SURG, V165, P358, DOI 10.1016/S0002-9610(05)80843-8
   Rojas D., 2011, P ACM AUD MOSTL C 20
   Rojas David, 2014, Stud Health Technol Inform, V196, P346
   Rojas D, 2013, IEEE T CYBERNETICS, V43, P1572, DOI 10.1109/TCYB.2013.2269712
   Rojas David, 2012, Stud Health Technol Inform, V173, P386
   Shams L, 2010, PHYS LIFE REV, V7, P295, DOI 10.1016/j.plrev.2010.07.006
   Suied C, 2009, EXP BRAIN RES, V194, P91, DOI 10.1007/s00221-008-1672-6
   Tashiro JayShiro., 2007, Proceedings of the 2007 conference on Future Play, P113, DOI DOI 10.1145/1328202.1328223
   TREISMAN AM, 1969, J EXP PSYCHOL, V79, P27, DOI 10.1037/h0026890
   Woods AT, 2011, FOOD QUAL PREFER, V22, P42, DOI 10.1016/j.foodqual.2010.07.003
NR 40
TC 8
Z9 12
U1 0
U2 7
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2015
VL 31
IS 9
BP 1207
EP 1216
DI 10.1007/s00371-014-1006-6
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CO8AY
UT WOS:000359388100005
DA 2024-07-18
ER

PT J
AU Debattista, K
   Bashford-Rogers, T
   Selmanovic, E
   Mukherjee, R
   Chalmers, A
AF Debattista, Kurt
   Bashford-Rogers, Thomas
   Selmanovic, Elmedin
   Mukherjee, Ratnajit
   Chalmers, Alan
TI Optimal exposure compression for high dynamic range content
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 32nd Computer Graphics International CGI 15 Conference
CY JUN 24-26, 2015
CL INSA Strasbourg Univ Strasbourg, Strasbourg, FRANCE
SP CNRS, iCUBE, Univ Strasbourg, CGS, Springer, Acm Incooperation, IGG, ACMSIGGRAPH, KIST Europe, Region Alsace, Visteon
HO INSA Strasbourg Univ Strasbourg
ID IMAGES
AB High dynamic range (HDR) imaging has become one of the foremost imaging methods capable of capturing and displaying the full range of lighting perceived by the human visual system in the real world. A number of HDR compression methods for both images and video have been developed to handle HDR data, but none of them has yet been adopted as the method of choice. In particular, the backwards-compatible methods that always maintain a stream/image that allow part of the content to be viewed on conventional displays make use of tone mapping operators which were developed to view HDR images on traditional displays. There are a large number of tone mappers, none of which is considered the best as the images produced could be deemed subjective. This work presents an alternative to tone mapping-based HDR content compression by identifying a single exposure that can reproduce the most information from the original HDR image. This single exposure can be adapted to fit within the bit depth of any traditional encoder. Any additional information that may be lost is stored as a residual. Results demonstrate quality is maintained as well, and better, than other traditional methods. Furthermore, the presented method is backwards-compatible, straightforward to implement, fast and does not require choosing tone mappers or settings.
C1 [Debattista, Kurt; Bashford-Rogers, Thomas; Mukherjee, Ratnajit; Chalmers, Alan] Univ Warwick, WMG, Coventry CV4 7AL, W Midlands, England.
   [Selmanovic, Elmedin] Univ Sarajevo, Sarajevo 71000, Bosnia & Herceg.
C3 University of Warwick; University of Sarajevo
RP Debattista, K (corresponding author), Univ Warwick, WMG, Coventry CV4 7AL, W Midlands, England.
EM K.Debattista@warwick.ac.uk
RI Selmanović, Elmedin/JCD-7596-2023; Selmanović, Elmedin/KIE-9609-2024;
   Selmanović, Elmedin/ABA-1403-2020
OI Selmanović, Elmedin/0000-0003-4245-3588; 
FU Royal Society; EPSRC [EP/I006192/1] Funding Source: UKRI
FX The Mercedes footage was provided by the University of Stuttgart, the
   Seine scene by Technicolor and Tears of Steel is an open movie project
   licensed under Creative Commons Attribution 3.0 by Blender
   (https://mango.blender.org/about/). Debattista is partially supported by
   a Royal Society Industrial Fellowship. Chalmers is partially supported
   by a Royal Society Industrial Fellowship.
CR Akyüz AO, 2007, J VIS COMMUN IMAGE R, V18, P366, DOI 10.1016/j.jvcir.2007.04.001
   [Anonymous], 1994, Graph. Gems, DOI DOI 10.1016/B978-0-12-336156-1.50054-9
   Bahn S, 2007, IN C IND ENG ENG MAN, P492, DOI 10.1109/IEEM.2007.4419238
   Banterle F, 2011, ADVANCED HIGH DYNAMIC RANGE IMAGING: THEORY AND PRACTICE, P1
   Banterle F., 2006, P 4 INT C COMP GRAPH, P349
   Banterle F., 2012, ACM S APPL PERC 2012, P39
   Banterle F, 2007, VISUAL COMPUT, V23, P467, DOI 10.1007/s00371-007-0124-9
   Eisemann E, 2004, ACM T GRAPHIC, V23, P673, DOI 10.1145/1015706.1015778
   FREEDMAN D, 1981, Z WAHRSCHEINLICHKEIT, V57, P453, DOI 10.1007/BF01025868
   Larson G. W., 1998, Journal of Graphics Tools, V3, P15, DOI 10.1080/10867651.1998.10487485
   Lee C., 2008, P 16 EUR SIGN PROC C
   Mantiuk R, 2004, ACM T GRAPHIC, V23, P733, DOI 10.1145/1015706.1015794
   Mantiuk R., 2006, TECHNICAL REPORT
   Mantiuk R., 2006, ELECT IMAGING
   Motra A, 2010, IEEE IMAGE PROC, P2061, DOI 10.1109/ICIP.2010.5654069
   Narwaria M., 2014, P VPQM
   Narwaria M., SIGNAL PROCESS IMAGE
   Narwaria M., 2012, SPIE OPTICAL ENG APP
   Narwaria M, 2014, EUR SIGNAL PR CONF, P2140
   Neumann L, 1998, COMPUTER GRAPHICS INTERNATIONAL, PROCEEDINGS, P666, DOI 10.1109/CGI.1998.694325
   Okuda M, 2007, J VIS COMMUN IMAGE R, V18, P377, DOI 10.1016/j.jvcir.2007.06.004
   Reinhard E, 2002, ACM T GRAPHIC, V21, P267, DOI 10.1145/566570.566575
   Strgar Kurecic M., 2013, ACTA GR ZNAN CASOPIS, V24, P13
   Ward Greg., 2004, Proceedings of the 1st Symposium on Applied Perception in Graphics and Visualization, APGV '04, P83
   Xu RF, 2005, IEEE COMPUT GRAPH, V25, P57, DOI 10.1109/MCG.2005.133
   Zhang Y, 2011, IEEE IMAGE PROC, P1321, DOI 10.1109/ICIP.2011.6115679
NR 26
TC 17
Z9 17
U1 0
U2 7
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2015
VL 31
IS 6-8
BP 1089
EP 1099
DI 10.1007/s00371-015-1121-z
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA CM2CN
UT WOS:000357487500033
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Walker, JS
   Jones, MW
   Laramee, RS
   Bidder, OR
   Williams, HJ
   Scott, R
   Shepard, ELC
   Wilson, RP
AF Walker, James S.
   Jones, Mark W.
   Laramee, Robert S.
   Bidder, Owen R.
   Williams, Hannah J.
   Scott, Rebecca
   Shepard, Emily L. C.
   Wilson, Rory P.
TI TimeClassifier: a visual analytic system for the classification of
   multi-dimensional time series data
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 32nd Computer Graphics International CGI 15 Conference
CY JUN 24-26, 2015
CL INSA Strasbourg Univ Strasbourg, Strasbourg, FRANCE
SP CNRS, iCUBE, Univ Strasbourg, CGS, Springer, Acm Incooperation, IGG, ACMSIGGRAPH, KIST Europe, Region Alsace, Visteon
HO INSA Strasbourg Univ Strasbourg
DE Visual analytics; Time series analysis; Movement ecology
AB Biologists studying animals in their natural environment are increasingly using sensors such as accelerometers in animal-attached 'smart' tags because it is widely acknowledged that this approach can enhance the understanding of ecological and behavioural processes. The potential of such tags is tempered by the difficulty of extracting animal behaviour from the sensors which is currently primarily dependent on the manual inspection of multiple time series graphs. This is time consuming and error-prone for the domain expert and is now the limiting factor for realising the value of tags in this area. We introduce TimeClassifier, a visual analytic system for the classification of time series data for movement ecologists. We deploy our system with biologists and report two real-world case studies of its use.
C1 [Walker, James S.; Jones, Mark W.; Laramee, Robert S.] Swansea Univ, Dept Comp Sci, Swansea SA2 8PP, W Glam, Wales.
   [Bidder, Owen R.; Williams, Hannah J.; Shepard, Emily L. C.; Wilson, Rory P.] Swansea Univ, Coll Sci, Swansea Lab Anim Movement, Biosci, Swansea SA2 8PP, W Glam, Wales.
   [Scott, Rebecca] GEOMAR Helmholtz Ctr Ocean Res Kiel, Future Ocean Cluster Excellence, D-24105 Kiel, Germany.
C3 Swansea University; Swansea University; Helmholtz Association; GEOMAR
   Helmholtz Center for Ocean Research Kiel
EM 521866@swansea.ac.uk; m.w.jones@swansea.ac.uk;
   r.s.laramee@swansea.ac.uk; 367097@swansea.ac.uk; 798684@swansea.ac.uk;
   rscott@geomar.de; e.l.c.shepard@swansea.ac.uk; r.p.wilson@swansea.ac.uk
RI Wilson, Rory/KIA-2955-2024; williams, hannah Jane/X-6156-2019; Jones,
   Mark W./F-1114-2015; Scott, Rebecca/G-3529-2015
OI williams, hannah Jane/0000-0002-6338-529X; Laramee, Robert
   S/0000-0002-3874-6145; Jones, Mark W./0000-0001-8991-1190; Scott,
   Rebecca/0000-0003-0954-7742; Shepard, Emily/0000-0001-7325-6398
FU EPSRC
FX This work was funded by an EPSRC doctoral training grant.
NR 0
TC 24
Z9 26
U1 0
U2 20
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2015
VL 31
IS 6-8
BP 1067
EP 1078
DI 10.1007/s00371-015-1112-0
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA CM2CN
UT WOS:000357487500031
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Zhao, L
   Liang, S
   Jia, JY
   Wei, YC
AF Zhao, Long
   Liang, Shuang
   Jia, Jinyuan
   Wei, Yichen
TI Learning best views of 3D shapes from sketch contour
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 32nd Computer Graphics International CGI 15 Conference
CY JUN 24-26, 2015
CL INSA Strasbourg Univ Strasbourg, Strasbourg, FRANCE
SP CNRS, iCUBE, Univ Strasbourg, CGS, Springer, Acm Incooperation, IGG, ACMSIGGRAPH, KIST Europe, Region Alsace, Visteon
HO INSA Strasbourg Univ Strasbourg
DE Best view selection; Sketch-based modeling; Context similarity;
   Bag-of-features
ID MODEL RETRIEVAL; ALGORITHM
AB In this paper, we introduce a novel learning-based approach to automatically select the best views of 3D shapes using a new prior. We think that a viewpoint of the 3D shape is reasonable if a human usually draws the shape from it. Hand-drawn sketches collected from relevant datasets are used to model this concept. We reveal the connection between sketches and viewpoints by taking context information of their contours into account. Furthermore, a learning framework is proposed to generalize this connection which aims to learn an automatic best view selector for different kinds of 3D shapes. Experiments on the Princeton Shape Benchmark dataset are conducted to demonstrate the superiority of our approach. The results show that compared with other state-of-the-art methods, our approach is not only robust but also efficient when applied to shape retrieval tasks.
C1 [Zhao, Long; Liang, Shuang; Jia, Jinyuan] Tongji Univ, Shanghai 200092, Peoples R China.
   [Wei, Yichen] Microsoft Res, Beijing, Peoples R China.
C3 Tongji University; Microsoft
RP Liang, S (corresponding author), Tongji Univ, Shanghai 200092, Peoples R China.
EM shuangliang@tongji.edu.cn
RI Zhao, Long/AAK-9782-2020
OI Zhao, Long/0000-0001-8921-8564; Liang, Shuang/0000-0003-0457-6093
FU National Science Foundation of China [61272276, 61305091]; National
   Twelfth Five-Year Plan Major Science and Technology Project of China
   [2012BAC11B01-04-03]; Special Research Fund of Higher Colleges Doctorate
   [20130072110035]; Fundamental Research Funds for the Central
   Universities [2100219038]; Shanghai Pujiang Program [13PJ1408200]
FX This research work was supported by the National Science Foundation of
   China (No. 61272276, 61305091), the National Twelfth Five-Year Plan
   Major Science and Technology Project of China (No. 2012BAC11B01-04-03),
   Special Research Fund of Higher Colleges Doctorate (No. 20130072110035),
   the Fundamental Research Funds for the Central Universities (No.
   2100219038), and Shanghai Pujiang Program (No. 13PJ1408200).
CR Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014
   Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236
   DeCarlo D, 2003, ACM T GRAPHIC, V22, P848, DOI 10.1145/882262.882354
   Eitz M, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185540
   Endres I, 2014, IEEE T PATTERN ANAL, V36, P222, DOI 10.1109/TPAMI.2013.122
   Fan RE, 2008, J MACH LEARN RES, V9, P1871
   Feldman J. A, 1969, P INT JOINT C ART IN, P521
   Fisher M, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866204
   Giorgi D., 2010, ACM WORKSH 3D OBJ RE
   Laga H, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2516971.2516975
   Laga H, 2011, VISUAL COMPUT, V27, P977, DOI 10.1007/s00371-011-0628-1
   Laga Hamid., 2008, J SOC ART SCI, V7, P124
   LEE C.H., 2005, SIGGRAPH
   Li B, 2013, 6 EUR WORKSH 3D OBJ, P89
   Li B, 2015, COMPUT VIS IMAGE UND, V131, P1, DOI 10.1016/j.cviu.2014.10.006
   Liu H, 2012, VISUAL COMPUT, V28, P279, DOI 10.1007/s00371-011-0638-z
   Liu YJ, 2013, IEEE T AUTOM SCI ENG, V10, P783, DOI 10.1109/TASE.2012.2228481
   Lowe D., 2001, INT J COMPUT VISION, V42, P145
   Ma C, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.65
   Malisiewicz T., 2009, ANN C NEUR INF PROC
   Mikolajczyk K, 2004, INT J COMPUT VISION, V60, P63, DOI 10.1023/B:VISI.0000027790.02288.f2
   Page D., 2003, INT C IM PROC ICIP
   Park HS, 2009, EXPERT SYST APPL, V36, P3336, DOI 10.1016/j.eswa.2008.01.039
   Shao TJ, 2011, COMPUT GRAPH FORUM, V30, P2011, DOI 10.1111/j.1467-8659.2011.02050.x
   Shilane P., 2004, SHAP MOD INT SMI
   Shtrom E., 2013, INT C COMP VIS ICCV
   Shuang Liang, 2014, Advances in Multimedia Information Processing - PCM 2014. 15th Pacific-Rim Conference on Multimedia. Proceedings: LNCS 8879, P133, DOI 10.1007/978-3-319-13168-9_14
   Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663
   Vazquez P. P., 2001, 6 INT FALL WORKSH VI
   Wang F, 2014, GRAPH MODELS, V76, P128, DOI 10.1016/j.gmod.2013.11.002
   ZHANG TY, 1984, COMMUN ACM, V27, P236, DOI 10.1145/357994.358023
   Zhao SH, 2014, ACM T MULTIM COMPUT, V10, DOI 10.1145/2537854
   Zitnick CL, 2014, LECT NOTES COMPUT SC, V8693, P391, DOI 10.1007/978-3-319-10602-1_26
NR 33
TC 13
Z9 18
U1 0
U2 11
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2015
VL 31
IS 6-8
BP 765
EP 774
DI 10.1007/s00371-015-1091-1
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA CM2CN
UT WOS:000357487500003
DA 2024-07-18
ER

PT J
AU Walton, S
   Berger, K
   Ebert, D
   Chen, M
AF Walton, Simon
   Berger, Kai
   Ebert, David
   Chen, Min
TI Vehicle object retargeting from dynamic traffic videos for real-time
   visualisation
SO VISUAL COMPUTER
LA English
DT Article
DE Video visualisation; Retargeting; Traffic visualisation; Car roof
   detection; Optical flow and edge detection
AB One form of video visualisation is to transform traffic videos from a street view to an aerial view, which facilitates a summary overview of multiple traffic video streams. This paper presents an efficient and effective solution to mitigate the undesirable distortion of the re-targeted vehicle objects in traffic video visualisation. This is achieved by a series of automated algorithmic steps, including vehicle segmentation, vehicle roof detection, and non-uniform image deformation by applying a second homography. This technique has been integrated into a video visualisation system that creates an aerial view of re-targeted video streams on top of a conventional aerial view. The results have shown that the technique offers the system a significant improvement in visual quality without undermining the requirement for real-time video visualisation.
C1 [Walton, Simon; Berger, Kai] Univ Oxford, OeRC, Oxford, England.
   [Ebert, David; Chen, Min] Univ Oxford, Oxford, England.
C3 University of Oxford; University of Oxford
RP Berger, K (corresponding author), Univ Oxford, OeRC, Oxford, England.
EM kai.berger@oerc.ox.ac.uk
OI Walton, Simon/0000-0002-3118-4434; Ebert, David/0000-0001-6177-1296
FU EPSRC [EP/G006555/2] Funding Source: UKRI
CR Ang D. S., 2009, 2009 IEEE International Integrated Reliability Workshop Final Report (IRW 2009), P25, DOI 10.1109/IRWS.2009.5383040
   [Anonymous], 2008, COMPUTER
   [Anonymous], 2006, P 14 ACM INT C MULT
   [Anonymous], EUROGRAPHICS 2011 ST
   Arróspide J, 2010, IET INTELL TRANSP SY, V4, P149, DOI 10.1049/iet-its.2009.0073
   Botchen R., 2008, TVCG
   Botchen RP, 2008, IEEE T VIS COMPUT GR, V14, P885, DOI 10.1109/TVCG.2008.40
   Carroll R, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778864
   Chen M, 2006, IEEE T VIS COMPUT GR, V12, P1093, DOI 10.1109/TVCG.2006.194
   Clarke L, 2011, IEEE T VIS COMPUT GR, V17, P808, DOI 10.1109/TVCG.2010.76
   Daniel G, 2003, IEEE VISUALIZATION 2003, PROCEEDINGS, P409, DOI 10.1109/VISUAL.2003.1250401
   Ellis A., 2010, Proceedings 7th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS 2010), P135, DOI 10.1109/AVSS.2010.89
   Gleicher M., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P33, DOI 10.1145/280814.280820
   He YH, 2004, IEEE T INTELL TRANSP, V5, P309, DOI 10.1109/TITS.2004.838221
   Hecker C, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360626
   Höferlin M, 2010, COMPUT GRAPH FORUM, V29, P1053, DOI 10.1111/j.1467-8659.2009.01670.x
   HORN BKP, 1981, ARTIF INTELL, V17, P185, DOI 10.1016/0004-3702(81)90024-2
   Hoummady B., 2002, Google Patents. (US Patent, Patent No. [6,366,219, 6366219]
   Kumar P, 2005, IEEE T INTELL TRANSP, V6, P43, DOI 10.1109/TITS.2004.838219
   Legg PA, 2011, IEEE IMAGE PROC
   Lin JJ, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024217
   Lu C. T., 2006, P 22 INT C DATA ENG, P167
   Ng R, 2004, ACM T GRAPHIC, V23, P477, DOI 10.1145/1015706.1015749
   Parry ML, 2011, IEEE T VIS COMPUT GR, V17, P1747, DOI 10.1109/TVCG.2011.208
   Romero M, 2008, IEEE T VIS COMPUT GR, V14, P1261, DOI 10.1109/TVCG.2008.185
   Rubinstein M, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360615
   Sacht L., 2011, SIGGRAPH AS 2011 SKE, P37
   Setlur V, 2007, IEEE COMPUT GRAPH, V27, P80, DOI 10.1109/MCG.2007.133
   Setlur Vidya., 2005, MUM, V154, P59, DOI DOI 10.1145/1149488.1149499
   Shekhar S, 2002, IEEE 5TH INTERNATIONAL CONFERENCE ON INTELLIGENT TRANSPORTATION SYSTEMS, PROCEEDINGS, P674, DOI 10.1109/ITSC.2002.1041299
   Van Rijsselbergen D, 2012, MULTIMED TOOLS APPL, V59, P307, DOI 10.1007/s11042-010-0710-0
   Walton S., LIVELAYER REAL TIME
   Wang Y, 2007, IEEE T VIS COMPUT GR, V13, P1568, DOI 10.1109/TVCG.2007.70544
   Zhu F, 2010, THIRD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING: WKDD 2010, PROCEEDINGS, P150, DOI 10.1109/WKDD.2010.47
NR 34
TC 7
Z9 7
U1 0
U2 17
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2014
VL 30
IS 5
BP 493
EP 505
DI 10.1007/s00371-013-0874-5
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AF2AN
UT WOS:000334515100003
DA 2024-07-18
ER

PT J
AU Seo, MK
   Kim, HM
   Lee, KH
AF Seo, Myoung Kook
   Kim, Hoe-Min
   Lee, Kwan H.
TI Solid texture synthesis for heterogeneous translucent materials
SO VISUAL COMPUTER
LA English
DT Article
DE Solid textures; Heterogeneous translucent materials; Measured scattering
   data
ID MODEL
AB We present a method to synthesize solid textures from heterogeneous translucent materials that have a complex pattern and subsurface scattering effect. A solid texture provides consistent texture throughout the volume, so that it can be used to model the texture on an arbitrary geometry. However, solid texture synthesis requires a huge amount of time to generate the volume. Moreover, a synthesized solid texture acquires only the color information from an input exemplar. Therefore, it has been difficult to render the appearance of a translucent object realistically without additional appearance data. In this paper, we introduce a new search method to accelerate synthesizing of solid textures. This method decomposes the candidates in an exemplar into several subgroups and searches for the best similar neighborhood in each decomposed subgroup. We also apply subsurface scattering effects to the shell layer of a synthesized object for realistic rendering of a translucent solid texture. Experimental results show that our rendering method can produce realistic rendering results for various heterogeneous translucent objects. It can also represent cross-sections of an object realistically without reconstructing the texture and surface geometry.
C1 [Seo, Myoung Kook; Kim, Hoe-Min; Lee, Kwan H.] Gwangju Inst Sci & Technol, Kwangju, South Korea.
C3 Gwangju Institute of Science & Technology (GIST)
RP Lee, KH (corresponding author), Gwangju Inst Sci & Technol, Kwangju, South Korea.
EM escmk@gist.ac.kr; milehoe@gist.ac.kr; khlee@gist.ac.kr
FU National Research Foundation of Korea (NRF) grant; Korea government
   (MEST) [2013031191]
FX This work was supported by the National Research Foundation of Korea
   (NRF) grant funded by the Korea government (MEST) (No. 2013031191).
CR [Anonymous], 2007, ACM T GRAPHIC, DOI DOI 10.1145/1239451.1239547
   [Anonymous], 1997, CGC 2 ANN FALL WORKS
   [Anonymous], P SIGGRAPH 09 T GRAP
   Arbree A, 2011, IEEE T VIS COMPUT GR, V17, P956, DOI 10.1109/TVCG.2010.117
   Chen GJ, 2012, VISUAL COMPUT, V28, P701, DOI 10.1007/s00371-012-0704-1
   Chen JT, 2010, VISUAL COMPUT, V26, P253, DOI 10.1007/s00371-009-0408-3
   Chen YS, 2004, VISUAL COMPUT, V20, P650, DOI 10.1007/s00371-004-0262-2
   Chen YY, 2004, ACM T GRAPHIC, V23, P343, DOI 10.1145/1015706.1015726
   Dong Y., 2008, COMP GRAPH FOR P EUR
   Donner C., 2006, P 17 EUROGRAPHICS C, P409
   Du SP, 2013, IEEE T VIS COMPUT GR, V19, P460, DOI 10.1109/TVCG.2012.129
   Eisenacher C., 2008, P EUR C
   Fuchs C., 2005, ACM SIGRAPH 2005 SKE
   Han JW, 2006, VISUAL COMPUT, V22, P918, DOI 10.1007/s00371-006-0078-3
   Hertzmann A, 2001, COMP GRAPH, P327, DOI 10.1145/383259.383295
   Jagnow R, 2004, ACM T GRAPHIC, V23, P329, DOI 10.1145/1015706.1015724
   Jarosz W, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1330511.1330518
   Jensen HW, 2001, COMP GRAPH, P511, DOI 10.1145/383259.383319
   Jensen HW, 2002, ACM T GRAPHIC, V21, P576, DOI 10.1145/566570.566619
   Lefebvre S, 2006, ACM T GRAPHIC, V25, P541, DOI 10.1145/1141911.1141921
   Peers P, 2006, ACM T GRAPHIC, V25, P746, DOI 10.1145/1141911.1141950
   Perlin K., 1985, Computer Graphics, V19, P287, DOI 10.1145/325165.325247
   Pietroni N, 2007, COMPUT GRAPH FORUM, V26, P637, DOI 10.1111/j.1467-8659.2007.01087.x
   Pietroni N, 2010, IEEE COMPUT GRAPH, V30, P74, DOI 10.1109/MCG.2009.153
   Qin XJ, 2007, IEEE T VIS COMPUT GR, V13, P379, DOI 10.1109/TVCG.2007.31
   Robertson MA, 2003, J ELECTRON IMAGING, V12, P219, DOI 10.1117/1.1557695
   Song Y, 2005, VISUAL COMPUT, V21, P774, DOI 10.1007/s00371-005-0320-4
   Tong X, 2005, ACM T GRAPHIC, V24, P1054, DOI 10.1145/1073204.1073311
   Tong X, 2002, ACM T GRAPHIC, V21, P665, DOI 10.1145/566570.566634
   Wang JS, 2008, RARE METALS, V27, P9, DOI 10.1016/S1001-0521(08)60020-0
   Wang Y., 2010, COMP GRAPH FOR P EUR
   Worley S., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P291, DOI 10.1145/237170.237267
   Wu Q, 2004, ACM T GRAPHIC, V23, P364, DOI 10.1145/1015706.1015730
   Zhang GX, 2013, GRAPH MODELS, V75, P104, DOI 10.1016/j.gmod.2012.10.006
   Zhang GX, 2011, GRAPH MODELS, V73, P59, DOI 10.1016/j.gmod.2010.10.006
NR 35
TC 3
Z9 4
U1 0
U2 9
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2014
VL 30
IS 3
BP 271
EP 283
DI 10.1007/s00371-013-0843-z
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AB3LU
UT WOS:000331693100003
DA 2024-07-18
ER

PT J
AU Trapp, M
   Schulze, F
   Bühler, K
   Liu, T
   Dickson, BJ
AF Trapp, M.
   Schulze, F.
   Buehler, K.
   Liu, T.
   Dickson, B. J.
TI 3D object retrieval in an atlas of neuronal structures
SO VISUAL COMPUTER
LA English
DT Article
DE 3D object retrieval; Neuronal structures; Drosophila melanogaster;
   Domain specific similarity measures
ID IMAGE; REGISTRATION; DROSOPHILA; BRAINS
AB Circuit neuroscience tries to solve one of the most challenging questions in biology: How does the brain work? An important step toward an answer to this question is to gather detailed knowledge about the neuronal circuits of the model organism Drosophila melanogaster. Geometric representations of neuronal objects of the Drosophila are acquired using molecular genetic methods, confocal microscopy, nonrigid registration and segmentation. These objects are integrated into a constantly growing common atlas. The comparison of new segmented neuronal objects to already known neuronal structures is a frequent task, which evolves with a growing amount of data into a bottleneck of the knowledge discovery process. Thus, the exploration of the atlas by means of domain specific similarity measures becomes a pressing need. To enable similarity based retrieval of neuronal objects, we defined together with domain experts tailored dissimilarity measures for each of the three typical neuronal structures cell body, projection, and arborization. Moreover, we defined the neuron enhanced similarity for projections and arborizations. According to domain experts, the developed system has big advantages for all tasks, which involve extensive data exploration.
C1 [Trapp, M.; Schulze, F.; Buehler, K.] VRVis Forsch GmbH, Vienna, Austria.
   [Liu, T.; Dickson, B. J.] Inst Mol Pathol, A-1030 Vienna, Austria.
C3 Vienna Biocenter (VBC); Research Institute of Molecular Pathology (IMP)
RP Trapp, M (corresponding author), VRVis Forsch GmbH, Vienna, Austria.
EM trapp@vrvis.at
OI Trapp, Martin/0000-0003-1725-3381; Buhler, Katja/0000-0002-0362-7998;
   Dickson, Barry/0000-0003-0715-892X
FU Austrian Research Promotion Agency (FFG) under the scope of the
   COMET-Competence Centers for Excellent Technologies-program
FX This work was funded by the Austrian Research Promotion Agency (FFG)
   under the scope of the COMET-Competence Centers for Excellent
   Technologies-program within the project "Knowledge Assisted Visual
   Fusion of Spatial Multi-Source Data (KAFus)."
CR Blankenship J.E., 2012, NERVOUS SYSTEM INVER
   Bronstein AM, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1899404.1899405
   Bruckner S, 2009, IEEE T VIS COMPUT GR, V15, P1497, DOI 10.1109/TVCG.2009.121
   Cardona A, 2010, J NEUROSCI, V30, P7538, DOI 10.1523/JNEUROSCI.0186-10.2010
   DEMIRALP C, 2009, P DMFC WORKSH MICCAI
   Fehr J, 2009, LECT NOTES COMPUT SC, V5875, P34
   Grauman K, 2005, IEEE I CONF COMP VIS, P1458
   Guntzer Ulrich., 2000, VLDB J, P419
   Ilyas IF, 2008, ACM COMPUT SURV, V40, DOI 10.1145/1391729.1391730
   Jiang XY, 2002, INT C PATT RECOG, P192, DOI 10.1109/ICPR.2002.1047430
   Lee PC, 2008, I S BIOMED IMAGING, P959
   Li XY, 2009, MODELLING SIMULATION, P437, DOI 10.1109/ICIP.2009.5414415
   Lian ZH, 2013, PATTERN RECOGN, V46, P449, DOI 10.1016/j.patcog.2012.07.014
   Lin CY, 2011, IEEE PAC VIS SYMP, P35, DOI 10.1109/PACIFICVIS.2011.5742370
   Lu GJ, 1999, MULTIMEDIA SYST, V7, P165, DOI 10.1007/s005300050119
   Moberts B, 2005, IEEE VISUALIZATION 2005, PROCEEDINGS, P65
   Mori G, 2005, IEEE T PATTERN ANAL, V27, P1832, DOI 10.1109/TPAMI.2005.220
   NCHC, 2012, BRC NTHU FLY CIRC
   Ohbuchi R, 2008, IEEE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS 2008, PROCEEDINGS, P93, DOI 10.1109/SMI.2008.4547955
   Olsen SR, 2008, TRENDS NEUROSCI, V31, P512, DOI 10.1016/j.tins.2008.07.006
   Peng HC, 2008, BIOINFORMATICS, V24, P1827, DOI 10.1093/bioinformatics/btn346
   Peng HC, 2011, NAT METHODS, V8, P493, DOI [10.1038/NMETH.1602, 10.1038/nmeth.1602]
   Rohlfing T, 2003, IEEE T INF TECHNOL B, V7, P16, DOI 10.1109/TITB.2003.808506
   Scorcioni R, 2008, NAT PROTOC, V3, P866, DOI 10.1038/nprot.2008.51
   Sfikas K, 2012, VISUAL COMPUT, V28, P943, DOI 10.1007/s00371-012-0714-z
   Sherbondy A, 2005, IEEE T VIS COMPUT GR, V11, P419, DOI 10.1109/TVCG.2005.59
   Tangelder JH, 2008, MULTIMED TOOLS APPL, V39, P441, DOI 10.1007/s11042-007-0181-0
   Van Essen DC, 2002, CURR OPIN NEUROBIOL, V12, P574, DOI 10.1016/S0959-4388(02)00361-6
   Wang X., 2010, P 5 INT S 3D DAT PRO
NR 29
TC 2
Z9 2
U1 0
U2 6
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2013
VL 29
IS 12
BP 1363
EP 1373
DI 10.1007/s00371-013-0871-8
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 251LB
UT WOS:000326929200011
DA 2024-07-18
ER

PT J
AU Ning, JF
   Xu, HX
   Wu, B
   Zeng, L
   Li, SK
   Xiong, YS
AF Ning, Jiangfan
   Xu, Huaxun
   Wu, Bo
   Zeng, Liang
   Li, Sikun
   Xiong, Yueshan
TI Modeling and animation of fracture of heterogeneous materials based on
   CUDA
SO VISUAL COMPUTER
LA English
DT Article
DE Fracture; Modeling; Animation; Movable cellular automata; Heterogeneous;
   CUDA
ID MESHLESS; SIMULATION
AB Existing techniques for animation of object fracture are based on an assumption that the object materials are homogeneous while most real world materials are heterogeneous. In this paper, we propose to use movable cellular automata (MCA) to simulate fracture phenomena on heterogeneous objects. The method is based on the discrete representation and inherits the advantages from both classical cellular automaton and discrete element methods. In our approach, the object is represented as discrete spherical particles, named movable cellular automata. MCA is used to simulate the material and physical properties so as to determine when and where the fracture occurs. To achieve real-time performance, we accelerate the complex computation of automata's physical properties in MCA simulation using CUDA on a GPU. The simulation results are directly sent to vertex buffer object (VBO) for rendering to avoid the costly communication between CPU and GPU. The experimental results show the effectiveness of our method.
C1 [Ning, Jiangfan; Xu, Huaxun; Wu, Bo; Zeng, Liang; Li, Sikun; Xiong, Yueshan] Natl Univ Def Technol, Sch Comp Sci, Changsha, Hunan, Peoples R China.
C3 National University of Defense Technology - China
RP Ning, JF (corresponding author), Natl Univ Def Technol, Sch Comp Sci, Changsha, Hunan, Peoples R China.
EM jiangfanning@gmail.com; xxhhxx@163.com; wubogfkd@yahoo.com.cn;
   liangzeng@263.net.cn; lisikun@263.net.cn; ysxiong@nudt.edu.cn
FU National Basic Research Program [2009CB723803]; National Science
   Foundation Program of China [60873120]
FX We first thank the anonymous reviewers for their careful and valuable
   comments. We are grateful to Aiping Wang and Kai Xu from NUDT for
   fruitful discussions. This work is supported by the National Basic
   Research Program (No. 2009CB723803) and the National Science Foundation
   Program (No. 60873120) of China.
CR Bao ZS, 2007, IEEE T VIS COMPUT GR, V13, P370, DOI 10.1109/TVCG.2007.39
   Bell N., 2005, P 2005 ACM SIGGRAPH, P77, DOI DOI 10.1145/1073368.1073379
   Chen K., 2005, THESIS NANJING U SCI
   Desbrun M., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P287, DOI 10.1145/218380.218456
   Georgii J, 2005, SIMUL MODEL PRACT TH, V13, P693, DOI 10.1016/j.simpat.2005.08.004
   Green Simon., 2007, CUDA Particles
   Guo XH, 2006, IEEE T VIS COMPUT GR, V12, P375, DOI 10.1109/TVCG.2006.52
   Guo XH, 2005, COMPUT ANIMAT VIRT W, V16, P189, DOI 10.1002/cav.98
   Imagire T, 2009, VISUAL COMPUT, V25, P719, DOI 10.1007/s00371-009-0319-3
   Liu N, 2011, COMPUT ANIMAT VIRT W, V22, P115, DOI 10.1002/cav.412
   Liu WG, 2008, COMPUT PHYS COMMUN, V179, P634, DOI 10.1016/j.cpc.2008.05.008
   Müller M, 2005, ACM T GRAPHIC, V24, P471, DOI 10.1145/1073204.1073216
   Muller M., 2004, P 2004 ACM SIGGRAPHE, P141, DOI [DOI 10.1145/1028523.1028542, 10.1145/1028523.1028542, 10]
   Norton A., 1991, Visual Computer, V7, P210, DOI 10.1007/BF01900837
   O'Brien JF, 1999, COMP GRAPH, P137, DOI 10.1145/311535.311550
   Parker E.G., 2009, PROC S COMP ANIM, P165, DOI DOI 10.1145/1599470.1599492.
   Pauly M, 2005, ACM T GRAPHIC, V24, P957, DOI 10.1145/1073204.1073296
   Psakhie S. G., 1995, Russian Physics Journal, V38, P1157, DOI 10.1007/BF00559396
   Psakhie S.G., 1998, PHYS MESOMECH, V1, P89
   Psakhie SG, 2001, THEOR APPL FRACT MEC, V37, P311, DOI 10.1016/S0167-8442(01)00079-9
   Psakhie SG, 2000, COMP MATER SCI, V19, P69, DOI 10.1016/S0927-0256(00)00140-3
   Psakhie SG, 1999, COMP MATER SCI, V16, P333, DOI 10.1016/S0927-0256(99)00076-2
   Su J., 2009, P 2009 ACM SIGGRAPH, P155
   TERZOPOULOS D, 1988, IEEE COMPUT GRAPH, V8, P41, DOI 10.1109/38.20317
   TERZOPOULOS D., 1988, SIGGRAPH COMPUT GRAP, V22, P269
NR 25
TC 2
Z9 3
U1 0
U2 14
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2013
VL 29
IS 4
SI SI
BP 265
EP 275
DI 10.1007/s00371-012-0765-1
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 115AM
UT WOS:000316784200004
DA 2024-07-18
ER

PT J
AU Wang, JR
   Feng, JQ
   Miao, YW
AF Wang, Jinrong
   Feng, Jieqing
   Miao, Yongwei
TI A robust confirmable watermarking algorithm for 3D mesh based on
   manifold harmonics analysis
SO VISUAL COMPUTER
LA English
DT Article
DE Non-blind watermarking; Manifold harmonics analysis; Spectral space;
   Public-key
ID MODELS
AB Owing to the manifold harmonics analysis, a robust non-blind spectral watermarking algorithm for a two-manifold mesh is presented, which can be confirmed by a trusted third party. Derived from the Laplace-Beltrami operator, a set of orthogonal manifold harmonics basis functions is first adopted to span the spectral space of the underlying three-dimensional (3D) mesh. The minimal number of the basis functions required in the proposed algorithm is also determined, which can effectively accelerate the spectrum computations. Then, to assert ownership and resist 3D mesh forging, a digital signature algorithm is adopted to sign the watermark in the embedding phase and to verify the signature in the extraction phase, which could optimize the robust non-blind spectral watermarking algorithm framework. To improve the robustness of the embedded watermark signature, the input 3D mesh will be segmented into patches. The watermark signature bits are embedded into the low-frequency spectral coefficients of all patches repeatedly and extracted with regard to the corresponding variations of their coefficients. Extensive experimental results demonstrate the efficiency, invisibility, and robustness of the proposed algorithm. Compared with existing watermarking algorithms, our algorithm exhibits better visual quality and is more robust to resist various geometric and connectivity attacks.
C1 [Wang, Jinrong; Feng, Jieqing] Zhejiang Univ, State Key Lab Comp Aided Design & Comp Graph, Hangzhou 310058, Zhejiang, Peoples R China.
   [Wang, Jinrong] Hangzhou Normal Univ, Hangzhou Inst Serv Engn, Hangzhou 310036, Zhejiang, Peoples R China.
   [Miao, Yongwei] Zhejiang Univ Technol, Coll Comp Sci & Technol, Hangzhou 310023, Zhejiang, Peoples R China.
C3 Zhejiang University; Hangzhou Normal University; Zhejiang University of
   Technology
RP Feng, JQ (corresponding author), Zhejiang Univ, State Key Lab Comp Aided Design & Comp Graph, Hangzhou 310058, Zhejiang, Peoples R China.
EM jqfeng@cad.zju.edu.cn
RI Miao, Yongwei/ABH-1238-2021
OI Miao, Yongwei/0000-0002-5479-9060
FU National Natural Science Foundation of China [60933007, 60736019]; 973
   program of China [2009CB-320801]; Program for New Century Excellent
   Talents in University [NCET-10-0728]; Natural Science Foundation of
   Zhejiang Province [Y1100837]
FX This work is supported by the National Natural Science Foundation of
   China under Grant Nos. 60933007 and 60736019, the 973 program of China
   under Grant No. 2009CB-320801, the Program for New Century Excellent
   Talents in University under Grant No. NCET-10-0728, and the Natural
   Science Foundation of Zhejiang Province under Grant No. Y1100837. We
   wish to thank Sivan Toledo for providing us the latest TAUCS package.
   The 3D models are courtesy of the Aim@Shape Shape repository and the
   Stanford 3D scanning repository.
CR Aiger D, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360684
   [Anonymous], 2008, 1803 FIPS NAT I STAN
   [Anonymous], 2000, P13632000 IEEE COMP
   [Anonymous], 2005, ANSIX9622005
   Aspert N, 2002, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL I AND II, PROCEEDINGS, P705, DOI 10.1109/ICME.2002.1035879
   Benedens O, 1999, IEEE COMPUT GRAPH, V19, P46, DOI 10.1109/38.736468
   Benedens O., 1999, P MULT SEC WORKSH AC, V99, P95
   BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791
   Bors AG, 2006, IEEE T IMAGE PROCESS, V15, P687, DOI 10.1109/TIP.2005.863116
   Cignoni P, 1998, COMPUT GRAPH FORUM, V17, P167, DOI 10.1111/1467-8659.00236
   Corsini M, 2007, IEEE T MULTIMEDIA, V9, P247, DOI 10.1109/TMM.2006.886261
   Cox IJ, 2008, MKS MULTIMED INFORM, P1
   Finkelstein A., 1999, ACM SIGGRAPH Proceedings, P69
   Garland M., 1997, P 24 ANN C COMP GRAP, V1997, P209, DOI DOI 10.1145/258734.258849
   Harte T, 2002, 2002 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL III, PROCEEDINGS, P661, DOI 10.1109/ICIP.2002.1039057
   Johnson D., 2001, International Journal of Information Security, V1, P36, DOI 10.1007/s102070100002
   Kanai S., 1998, IFIP WG, P296
   Karypis G., 1998, Metis: A software package for partitioning unstructured graphs, partitioning meshes, and computing fill-reducing orderings of sparse matrices
   Lavoue G., 2006, P SPIE APPL DIGITAL, P6312
   Levy B., 2009, P ACM SIGGRAPH AS 20
   Liu Y, 2008, MM&SEC'08: PROCEEDINGS OF THE MULTIMEDIA & SECURITY WORKSHOP 2008, P43
   Luo M, 2009, LECT NOTES COMPUT SC, V5703, P211, DOI 10.1007/978-3-642-03688-0_20
   Ohbuchi R, 1998, IEEE J SEL AREA COMM, V16, P551, DOI 10.1109/49.668977
   Ohbuchi R, 1997, ACM MULTIMEDIA 97, PROCEEDINGS, P261, DOI 10.1145/266180.266377
   Ohbuchi R, 2002, COMPUT GRAPH FORUM, V21, P373, DOI 10.1111/1467-8659.t01-1-00597
   Ohbuchi R., 2001, Graphics Interface, P9
   Reuter M, 2006, COMPUT AIDED DESIGN, V38, P342, DOI 10.1016/j.cad.2005.10.011
   Schneier B., 1996, Applied Cryptography: Protocols, Algorithms, and Source Code in C
   Vallet B., 2007, ALICE2007001
   Wang Jinrong, 2011, Journal of Computer Aided Design & Computer Graphics, V23, P21
   Wang K., 2008, EUROGRAPHICS SHORT P, P5
   Wang K., 2009, P IEEE INT C IM PROC, P211
   Wang K, 2007, LECT NOTES COMPUT SC, V4567, P50
   Wang WB, 2008, COMPUT AIDED DESIGN, V40, P634, DOI 10.1016/j.cad.2008.03.001
   Wang YP, 2010, COMPUT AIDED GEOM D, V27, P395, DOI 10.1016/j.cagd.2010.02.003
   Wang YP, 2009, IEEE T VIS COMPUT GR, V15, P285, DOI 10.1109/TVCG.2008.101
   Wu JH, 2005, VISUAL COMPUT, V21, P848, DOI 10.1007/s00371-005-0311-5
   Xie LH, 2001, IEEE T IMAGE PROCESS, V10, P1754, DOI 10.1109/83.967402
   Yin KK, 2001, COMPUT GRAPH-UK, V25, P409, DOI 10.1016/S0097-8493(01)00065-6
NR 39
TC 14
Z9 19
U1 0
U2 22
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2012
VL 28
IS 11
BP 1049
EP 1062
DI 10.1007/s00371-011-0650-3
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 021JO
UT WOS:000309881300001
DA 2024-07-18
ER

PT J
AU Lee, HP
   Lin, MC
AF Lee, Huai-Ping
   Lin, Ming C.
TI Fast optimization-based elasticity parameter estimation using reduced
   models
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Conference on the Computer Graphics International (CGI)
CY 2012
CL Bournemouth, ENGLAND
DE Physically-based modeling; Finite element method; Computer animation
ID SHAPE DEFORMATION; ELASTOGRAPHY; RECONSTRUCTION; REGISTRATION; SOLIDS
AB Elasticity parameters are central to physically-based animation and medical image analysis. We present an accelerated method to automatically estimate these parameters for a deformation simulator using an iterative optimization framework, given the desired (target) output surface/shape. During the optimization, the input model is deformed by the simulator, and the distance between the deformed surface and the target surface is minimized numerically. To accelerate the optimization process, we introduce a dimension reduction technique to allow a trade-off between the computational efficiency and desired accuracy. The reduced model is constructed using statistical training with a set of example deformations. To demonstrate this approach, we apply the computational framework to 2D animations of elastic bodies simulated with a linear finite element method. We also present a 3D elastography example, which is simulated with a reduced-dimension finite element model to improve the performance of the optimizer.
C1 [Lee, Huai-Ping; Lin, Ming C.] Univ N Carolina, Chapel Hill, NC 27515 USA.
C3 University of North Carolina; University of North Carolina Chapel Hill
RP Lee, HP (corresponding author), Univ N Carolina, Chapel Hill, NC 27515 USA.
EM lhp@cs.unc.edu; lin@cs.unc.edu
CR [Anonymous], ACM SIGGRAPH 2007 PA
   [Anonymous], ACM TRANS GRAPH
   [Anonymous], P SIGGRAPH 2011
   [Anonymous], IEEE T ULTRASON FERR
   Balay S, 1997, MODERN SOFTWARE TOOLS FOR SCIENTIFIC COMPUTING, P163
   Balocco S, 2008, LECT NOTES COMPUT SC, V5242, P131, DOI 10.1007/978-3-540-85990-1_16
   Barbic J, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531359
   BECKER MARKUS., 2007, SimVis, P15
   Bhat K. S., 2003, ACM SIGGRAPH/Eurographics Symposium on Computer Animation, P37
   Bickel B, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778800
   Bickel B, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531395
   COOTES TF, 1995, COMPUT VIS IMAGE UND, V61, P38, DOI 10.1006/cviu.1995.1004
   Fu D, 2000, PHYS MED BIOL, V45, P1495, DOI 10.1088/0031-9155/45/6/307
   Hensel JM, 2007, INT J RADIAT ONCOL, V68, P1522, DOI 10.1016/j.ijrobp.2007.04.004
   Kallel F, 1996, IEEE T MED IMAGING, V15, P299, DOI 10.1109/42.500139
   Kaus MR, 2007, INT J RADIAT ONCOL, V68, P572, DOI 10.1016/j.ijrobp.2007.01.056
   Krysl P, 2001, INT J NUMER METH ENG, V51, P479, DOI 10.1002/nme.167
   Lee HP, 2010, I S BIOMED IMAGING, P532, DOI 10.1109/ISBI.2010.5490293
   McNamara A, 2004, ACM T GRAPHIC, V23, P449, DOI 10.1145/1015706.1015744
   Müller M, 2004, PROC GRAPH INTERF, P239
   Muthupillai R, 1996, NAT MED, V2, P601, DOI 10.1038/nm0596-601
   Nealen A, 2006, COMPUT GRAPH FORUM, V25, P809, DOI 10.1111/j.1467-8659.2006.01000.x
   Nocedal J., 1999, NUMERICAL OPTIMIZATI
   Ophir J, 1999, P I MECH ENG H, V213, P203, DOI 10.1243/0954411991534933
   Pai DK, 2001, COMP GRAPH, P87, DOI 10.1145/383259.383268
   Pizer SM, 2003, INT J COMPUT VISION, V55, P85, DOI 10.1023/A:1026313132218
   Press W. H., 2007, NUM REC ART SCI COMP
   Rivaz H, 2008, IEEE T MED IMAGING, V27, P1373, DOI 10.1109/TMI.2008.917243
   SCHNUR DS, 1992, INT J NUMER METH ENG, V33, P2039, DOI 10.1002/nme.1620331004
   Shewchuk J.R., 1996, WORKSHOP APPL COMPUT, V1148, P203
   Si H., 2009, TetGen: a quality tetrahedral mesh generator and three-dimensional delaunay triangulator
   Syllebranque C, 2008, VISUAL COMPUT, V24, P963, DOI 10.1007/s00371-008-0273-5
   Taylor ZA, 2010, LECT NOTES COMPUT SC, V6362, P388
   Teschner M., 2005, Eurographics 2005 Tutorial
   Treuille A, 2003, ACM T GRAPHIC, V22, P716, DOI 10.1145/882262.882337
   Washington CW, 2004, IEEE T MED IMAGING, V23, P1117, DOI 10.1109/TMI.2004.830532
   Weng YL, 2006, VISUAL COMPUT, V22, P653, DOI 10.1007/s00371-006-0054-y
   Yang WW, 2008, VISUAL COMPUT, V24, P495, DOI 10.1007/s00371-008-0230-3
   Yoo TS., 2002, Engineering and Algorithm Design for an Image Processing API: A Technical Report on ITK - the Insight Toolkit
   Yushkevich PA, 2006, NEUROIMAGE, V31, P1116, DOI 10.1016/j.neuroimage.2006.01.015
   Zhu YN, 2003, IEEE T MED IMAGING, V22, P890, DOI 10.1109/TMI.2003.815065
   Zienkiewicz OC, 2005, FINITE ELEMENT METHOD FOR FLUID DYNAMICS, 6TH EDITION, P1
NR 42
TC 5
Z9 10
U1 0
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2012
VL 28
IS 6-8
BP 553
EP 562
DI 10.1007/s00371-012-0686-z
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 947EW
UT WOS:000304411500004
DA 2024-07-18
ER

PT J
AU Kim, S
   Kyung, MH
   Lee, JH
AF Kim, Soonhyun
   Kyung, Min-Ho
   Lee, Joo-Haeng
TI Noiseless GPU rendering of isotropic BRDF surfaces
SO VISUAL COMPUTER
LA English
DT Article
DE BRDF; Monte Carlo rendering; Importance sampling; Filtering
AB Illumination at a surface point is formulated as an integral of a BRDF using the incident radiance over the hemisphere domain. A popular method to compute the integral is Monte Carlo integration, in which the surface illumination is computed as the sum of the integrand evaluated with stochastically sampled rays. Although its simple nature is practically attractive, it incurs the serious drawback of noise artifacts due to estimator variance. In this paper, we propose a novel noiseless Monte Carlo rendering algorithm running in real time on a GPU. The main contribution is a novel importance sampling scheme, which constructs spatially continuous sample rays over a surface. For each evenly spaced polar angle of the eye ray, denoted by theta, incident rays are sampled with a PDF (probability density function) derived from a target BRDF lobe. We develop a force-based update method to create a sequence of consistent ray sets along theta's. Finally, corresponding rays in the sequence of ray sets are linearly connected to form a continuous ray curve, referred to as a sample thread. When rendering, illumination at a surface point is computed with rays, each of which is given as a point on a sample thread. Because a thread provides a sample ray that continuously varies on a surface, the random variance of the illumination, causing visual noise during the Monte Carlo rendering process, is eliminated. A thread set is precomputed for each BRDF to free the GPU from the burden of sampling during real-time rendering. According to extensive experiments, approximately 100 threads are sufficient for most measured BRDFs with acceptable rendering quality for interactive applications.
C1 [Kyung, Min-Ho] Ajou Univ, Dept Digital Media, Suwon 441749, South Korea.
   [Kim, Soonhyun] Ajou Univ, Grad Sch Informat & Commun, Suwon 441749, South Korea.
   [Lee, Joo-Haeng] ETRI, Taejon, South Korea.
C3 Ajou University; Ajou University; Electronics & Telecommunications
   Research Institute - Korea (ETRI)
RP Kyung, MH (corresponding author), Ajou Univ, Dept Digital Media, Suwon 441749, South Korea.
EM kkubs@ajou.ac.kr; kyung@ajou.ac.kr; joohaeng@etri.re.kr
OI Lee, Joo-Haeng/0000-0002-5788-712X
FU National Research Foundation of Korea (NRF); Ministry of Education,
   Science and Technology [2011-0004030]; Korean Ministry of Knowledge
   Economy; Korea Research Council for Industrial Science Technology
   [2010-ZC1140]
FX This research was mainly supported by Basic Science Research Program
   through the National Research Foundation of Korea (NRF) funded by the
   Ministry of Education, Science and Technology (2011-0004030), and also
   was supported in part by Korean Ministry of Knowledge Economy and Korea
   Research Council for Industrial Science & Technology: Grant No.
   2010-ZC1140, Development of Future Robotic Computer.
CR Burke D., 2005, P EUR S REND 05, P243
   Clarberg P, 2005, ACM T GRAPHIC, V24, P1166, DOI 10.1145/1073204.1073328
   Colbert M., 2008, GPU BASED IMPORTANCE, V20
   DUTRE P., 1993, Proceedings of Compugraphics '93, P128
   Jensen H.W, 2003, SIGGRAPH 03 COURS, V44
   Kajiya J.T, 1986, P SIG GRAPHI 86
   Kontkanen J, 2006, MONTE CARLO AND QUASI-MONTE CARLO METHODS 2004, P259, DOI 10.1007/3-540-31186-6_16
   Kurt M., 2009, SIGGRAPH Computer Graphics, V43, P1, DOI DOI 10.1145/1629216.1629222
   Lafortune E. P., 1993, EDUGRAPHICS '93. First International Conference on Graphics Education. COMPUGRAPHICS '93. Third International Conference on Computational Graphics and Visualization Techniques. Combined Proceedings, P145
   Lawrence J, 2004, ACM T GRAPHIC, V23, P496, DOI 10.1145/1015706.1015751
   LEE ME, 1990, IEEE COMPUT GRAPH, V10, P23, DOI 10.1109/38.55149
   Matusik W, 2003, ACM T GRAPHIC, V22, P759, DOI 10.1145/882262.882343
   McCool MD, 1999, ACM T GRAPHIC, V18, P171, DOI 10.1145/318009.318015
   Pharr M., 2004, Physically Based Rendering: From Theory to Implementation
   Sameer Agarwal, 2003, ACM Transactions on Graphics, V22, P605, DOI 10.1145/882262.882314
   Sun X, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239478
   Veach E., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P419, DOI 10.1145/218380.218498
   Veach E., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P65, DOI 10.1145/258734.258775
   Weinzierl S, 2000, TOPICAL LECT GIVEN R
NR 19
TC 2
Z9 2
U1 0
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2012
VL 28
IS 1
BP 125
EP 135
DI 10.1007/s00371-011-0633-4
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 874YQ
UT WOS:000298995000011
DA 2024-07-18
ER

PT J
AU Ripolles, O
   Chover, M
   Ramos, F
AF Ripolles, Oscar
   Chover, Miguel
   Ramos, Francisco
TI Visualization of level-of-detail meshes on the GPU
SO VISUAL COMPUTER
LA English
DT Article
DE Multiresolution modeling; Level of detail; Real-time rendering; GPU
ID REFINEMENT
AB Extensive research has been carried out in multiresolution models for many decades. The tendency in recent years has been to harness the potential of GPUs to perform the level-of-detail extraction on graphics hardware. The aim of this work is to present a new level-of-detail scheme based on triangles which is both simple and efficient. In this approach, the extraction process updates vertices instead of indices, thus providing a perfect framework for adapting the algorithms to work completely on GPU shaders. One of the key aspects of our proposal is the need for just a single rendering pass in order to obtain the desired geometry. Moreover, coherence among the different approximations is maximized by means of a symmetric extraction algorithm, which performs the same process when refining and coarsening the mesh. Lastly, we also introduce different uses of the scheme to offer continuous and view-dependent resolution.
C1 [Ripolles, Oscar] Univ Politecn Valencia, Inst Univ Automat & Informat Ind, E-46071 Valencia, Spain.
   [Chover, Miguel; Ramos, Francisco] Univ Jaume 1, Inst New Imaging Technol, Castellon de La Plana, Spain.
C3 Universitat Politecnica de Valencia; Universitat Jaume I
RP Ripolles, O (corresponding author), Univ Politecn Valencia, Inst Univ Automat & Informat Ind, Camino Vera S-N, E-46071 Valencia, Spain.
EM oripolles@ai2.upv.es; chover@uji.es; francisco.ramos@uji.es
RI Ramos, Francisco/AAA-7780-2019; Ramos, Francisco/L-7228-2014; Ramos,
   Francisco/L-5911-2018; Chover Sellés, Miguel/P-9933-2018
OI Ramos, Francisco/0000-0003-2540-4741; Chover Sellés,
   Miguel/0000-0002-0525-7038; Ripolles, Oscar/0000-0002-5450-6758
FU Spanish Ministry of Science and Technology [TIN2010-21089-C03-03,
   TSI-020400-2009-0133]; European Union [ITEA2 IP08009]; FEDER
FX This work has been funded by the Spanish Ministry of Science and
   Technology (projects TIN2010-21089-C03-03 and TSI-020400-2009-0133), by
   the European Union (ITEA2 IP08009) and FEDER funds.
CR [Anonymous], 2003, LEVEL DETAIL 3D GRAP
   [Anonymous], 2007, NVidia CUDA Compute Unified Device Architecture: Programming Guide
   Blythe D, 2006, ACM T GRAPHIC, V25, P724, DOI 10.1145/1141911.1141947
   Borgeat L, 2005, ACM T GRAPHIC, V24, P869, DOI 10.1145/1073204.1073276
   Boubekeur T, 2008, COMPUT GRAPH FORUM, V27, P102, DOI 10.1111/j.1467-8659.2007.01040.x
   Castello P, 2007, EUROGRAPHICS TUTORIA, V2, P891
   Chhugani J, 2007, I3D 2007: ACM SIGGRAPH SYMPOSIUM ON INTERACTIVE 3D GRAPHICS AND GAMES, PROCEEDINGS, P9
   Cignoni P, 2005, IEEE VISUALIZATION 2005, PROCEEDINGS, P207
   CLARK JH, 1976, COMMUN ACM, V19, P547, DOI 10.1145/360349.360354
   COHEN J, 1998, SIGGRAPH 98, P115
   DeCoro C, 2007, I3D 2007: ACM SIGGRAPH SYMPOSIUM ON INTERACTIVE 3D GRAPHICS AND GAMES, PROCEEDINGS, P161
   Dyken C, 2008, COMPUT GRAPH FORUM, V27, P1, DOI 10.1111/j.1467-8659.2007.01030.x
   El-Sana J, 2000, COMPUT GRAPH FORUM, V19, pC139, DOI 10.1111/1467-8659.00406
   Garland M., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P209, DOI 10.1145/258734.258849
   GONZALEZ C, 2008, P 16 INT C CENTR EUR, P87
   HOPPE H, 1996, SIGGRAPH 96, P99, DOI DOI 10.1145/237170.237216
   HOPPE H, 1997, COMPUTER GRAPHICS, V31, P189
   HU L, 2009, I3D 09, P169
   Ji JF, 2005, COMPUTER GRAPHICS INTERNATIONAL 2005, PROCEEDINGS, P108
   Lin G, 2006, IEEE T VIS COMPUT GR, V12, P640, DOI 10.1109/TVCG.2006.59
   Lindstrom P, 2000, ACM T GRAPHIC, V19, P204, DOI 10.1145/353981.353995
   Livny Y, 2008, VISUAL COMPUT, V24, P239, DOI 10.1007/s00371-007-0201-0
   Losasso F., 2003, Symposium on Geometry Processing, P138
   LUEBKE D, 2002, LEVEL DETAIL 3D GRAP, V1
   LUEBKE D, 2001, 12 EUR WORKSH REND, P223
   NISKI K, 2007, P I3D 07, P153
   Pajarola R, 2001, NINTH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P22, DOI 10.1109/PCCGA.2001.962854
   PURNOMO B, 2008, AMD TOOTLE VER 2 0
   Ramos F, 2006, LECT NOTES COMPUT SC, V4245, P460
   RIBELLES J, 1999, EUROGRAPHICS 99, P230
   Ripollés O, 2008, COMPUT GRAPH-UK, V32, P307, DOI 10.1016/j.cag.2008.02.003
   RIPOLLES O, 2008, COMP GRAPH GEOM MOD
   RIPOLLES O, 2009, WINT SCH COMP GRAPH
   Sander PV, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1239451.1239540, 10.1145/1276377.1276489]
   SANDER PV, 2005, S GEOM PROC, P129
   Schwarz M, 2009, COMPUT GRAPH FORUM, V28, P365, DOI 10.1111/j.1467-8659.2009.01376.x
   Shafae M, 2003, 11TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P271, DOI 10.1109/PCCGA.2003.1238269
   Southern R, 2003, COMPUT GRAPH FORUM, V22, P35, DOI 10.1111/1467-8659.t01-1-00644
   TARIQ S, 2009, D3D11 TESS GAM DEV C
   TURCHYN P, 2007, P 15 INT C CENTR EUR, P33
   Yoon SE, 2004, IEEE VISUALIZATION 2004, PROCEEEDINGS, P131, DOI 10.1109/VISUAL.2004.86
NR 41
TC 2
Z9 2
U1 0
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2011
VL 27
IS 9
BP 793
EP 809
DI 10.1007/s00371-011-0554-2
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 807TK
UT WOS:000293922400001
DA 2024-07-18
ER

PT J
AU Darkner, S
   Erleben, K
AF Darkner, Sune
   Erleben, Kenny
TI A hyper elasticity method for interactive virtual design of hearing aids
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Conference on the Computer Graphics International (CGI)
CY JUN 12-15, 2011
CL Ottawa, CANADA
DE Hyper elasticity; Interactive design; Virtual prototyping; General
   framework
ID DEFORMATION
AB We present a computational efficient method for isotropic hyper elasticity based on functional analysis. By selecting a class of shape functions, we arrive at a computational scheme which yields very sparse tensors. This enables fast computations of the hyper elastic energy potential and its derivatives. We achieve efficiency and performance through the use of shape functions that are linear in their parameters and through rotation into the eigenspace of the right Cauchy-Green strain tensor. This makes near real time evaluation of hyper elasticity of complex meshes on CPU relatively easy to implement. The approach does not rely on a specific shape function or material model but offers a general framework for isotropic hyper elasticity. The method is aimed at interactive and accurate non-linear hyper elastic modeling for a wide range of industrial virtual design applications, which we exemplify by insertion of hearing aid domes into the ear canal. We validate the method for tetrahedral meshes with linear shape functions with an Ogden material model by comparing simulations to deformations of real material. We illustrate the use of other shape functions and models using uniform cubic B-splines in combination with Riemannian elasticity.
C1 [Darkner, Sune; Erleben, Kenny] Univ Copenhagen, Dept Comp Sci, DK-2100 Copenhagen, Denmark.
C3 University of Copenhagen
RP Darkner, S (corresponding author), Univ Copenhagen, Dept Comp Sci, Univ Pk 1, DK-2100 Copenhagen, Denmark.
EM darkner@diku.dk; erleben@diku.dk
RI Erleben, Kenny/AAZ-6556-2020; Darkner, Sune/N-1834-2016
OI Erleben, Kenny/0000-0001-6808-4747; Darkner, Sune/0000-0001-6114-7100
CR [Anonymous], 2005, ACMEUROGRAPHICS S CO
   [Anonymous], 2004, P 2004 ACM SIGGRAPH, DOI DOI 10.1145/1028523.1028541
   Baraff D., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P43, DOI 10.1145/280814.280821
   Barbic J, 2005, ACM T GRAPHIC, V24, P982, DOI 10.1145/1073204.1073300
   Bargteil AW, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239467
   Bergou M, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360662
   Bonet J, 2008, NONLINEAR CONTINUUM MECHANICS FOR FINITE ELEMENT ANALYSIS, 2ND EDITION, P1, DOI 10.1017/CBO9780511755446
   Botsch M., 2006, SGP 06, P11
   Bærentzen JA, 2005, IEEE T VIS COMPUT GR, V11, P243, DOI 10.1109/TVCG.2005.49
   Bridson R, 2002, ACM T GRAPHIC, V21, P594, DOI 10.1145/566570.566623
   Choi MG, 2005, IEEE T VIS COMPUT GR, V11, P91
   DARKNER S, 2006, P 9 INT C MED IM COM, P19
   Debunne G, 2001, COMP GRAPH, P31, DOI 10.1145/383259.383262
   Galoppo N., 2009, Proc. of the 2009 symposium on Interactive 3D graphics and games, P39, DOI DOI 10.1145/1507149.1507156
   Galoppo N, 2007, COMPUT GRAPH FORUM, V26, P243, DOI 10.1111/j.1467-8659.2007.01046.x
   Georgii J., 2008, P 5 WORKSH VIRT REAL, P11
   GRINSPUN E, 2006, SIGGRAPH COURSES, P14
   Irving G, 2006, GRAPH MODELS, V68, P66, DOI 10.1016/j.gmod.2005.03.007
   Irving G, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239464
   MAUCH SP, 2003, THESIS PASADENA
   Müller M, 2007, J VIS COMMUN IMAGE R, V18, P109, DOI 10.1016/j.jvcir.2007.01.005
   Müller M, 2005, ACM T GRAPHIC, V24, P471, DOI 10.1145/1073204.1073216
   Müller M, 2004, PROC GRAPH INTERF, P239
   Muller M., 2004, P 2004 ACM SIGGRAPHE, P141, DOI [DOI 10.1145/1028523.1028542, 10.1145/1028523.1028542, 10]
   MULLER M, 2002, P 2002 ACM SIGGRAPH, V21, P49
   Nocedal J, 2006, SPRINGER SER OPER RE, P1, DOI 10.1007/978-0-387-40065-5
   O'Brien JF, 1999, COMP GRAPH, P137, DOI 10.1145/311535.311550
   OGDEN RW, 1972, PROC R SOC LON SER-A, V326, P565, DOI 10.1098/rspa.1972.0026
   Pennec X, 2005, LECT NOTES COMPUT SC, V3750, P943, DOI 10.1007/11566489_116
   Schmedding R, 2008, VISUAL COMPUT, V24, P625, DOI 10.1007/s00371-008-0243-y
   Selle A, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360663
   SI H, 2011, OPENSOURCE PROJ
   Sigg C, 2003, IEEE VISUALIZATION 2003, PROCEEDINGS, P83, DOI 10.1109/VISUAL.2003.1250358
   Teran J., 2003, ACM SIGGRAPH/Eurographics Symposium on Computer Animation, P68
   Terzopoulos D., 1987, COMPUT GRAPH, P205, DOI DOI 10.1145/37402.37427
   Teschner M, 2004, COMPUTER GRAPHICS INTERNATIONAL, PROCEEDINGS, P312, DOI 10.1109/CGI.2004.1309227
   Wicke M, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778786
   Zhu YN, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1731047.1731054
NR 38
TC 1
Z9 1
U1 0
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2011
VL 27
IS 6-8
BP 645
EP 653
DI 10.1007/s00371-011-0574-y
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 766FY
UT WOS:000290767600023
DA 2024-07-18
ER

PT J
AU Zhang, Y
   Tong, RF
AF Zhang, Yun
   Tong, Ruofeng
TI Environment-Sensitive cloning in images
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Conference on the Computer Graphics International (CGI)
CY JUN 12-15, 2011
CL Ottawa, CANADA
DE Environment-Sensitive; Global feature; Mean-Value Coordinates (MVC)
ID SEGMENTATION; PROPAGATION
AB We present an Environment-Sensitive image cloning technique which improves the previous gradient-based methods by taking into account the content of target scene. We create a reference image to represent the global feature of target image which could be further diffused into the cloned patch, and modify the diffusion process to ensure that the cloning result is seamless and natural. Specifically, we figure out an efficient solution based on Mean-Value Coordinates (MVC) to deal with the hybrid boundary, and construct a general model based on MVC to implement our image cloning, which is further applied to video cloning. Experimental results demonstrate the effectiveness of our Environment-Sensitive cloning.
C1 [Zhang, Yun; Tong, Ruofeng] Zhejiang Univ, Dept Comp Sci & Technol, Hangzhou 310027, Zhejiang, Peoples R China.
C3 Zhejiang University
RP Tong, RF (corresponding author), Zhejiang Univ, Dept Comp Sci & Technol, Hangzhou 310027, Zhejiang, Peoples R China.
EM zhangyun_zju@zju.edu.cn; trf@zju.edu.cn
CR Armstrong CJ, 2007, COMPUT GRAPH-UK, V31, P212, DOI 10.1016/j.cag.2006.11.015
   Bai X, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531376
   Chen T, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618470
   Cheng MM, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778820
   Chuang YY, 2001, PROC CVPR IEEE, P264
   Chuang YY, 2002, ACM T GRAPHIC, V21, P243, DOI 10.1145/566570.566572
   Ding M, 2010, VISUAL COMPUT, V26, P721, DOI 10.1007/s00371-010-0448-8
   Farbman Z, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531373
   Gastal ESL, 2010, COMPUT GRAPH FORUM, V29, P575, DOI 10.1111/j.1467-8659.2009.01627.x
   Jia JY, 2006, ACM T GRAPHIC, V25, P631, DOI 10.1145/1141911.1141934
   Lalonde JF, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1276377.1276381, 10.1145/1239451.1239454]
   Lalonde JF, 2009, IEEE I CONF COMP VIS, P183, DOI 10.1109/ICCV.2009.5459163
   LAND EH, 1971, J OPT SOC AM, V61, P1, DOI 10.1364/JOSA.61.000001
   Levin A, 2008, IEEE T PATTERN ANAL, V30, P228, DOI 10.1109/TPAMI.2007.1177
   Li Y, 2010, COMPUT GRAPH FORUM, V29, P2049, DOI 10.1111/j.1467-8659.2010.01791.x
   Pérez P, 2003, ACM T GRAPHIC, V22, P313, DOI 10.1145/882262.882269
   RUZON MA, 2000, CVPR, P1018
   Wang HC, 2004, INT C PATT RECOG, P858, DOI 10.1109/ICPR.2004.1334663
   Wang J, 2005, IEEE I CONF COMP VIS, P936
   Wang J, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239460
   Wang J, 2007, FOUND TRENDS COMPUT, V3, P97, DOI 10.1561/0600000019
   Xie ZF, 2010, VISUAL COMPUT, V26, P1123, DOI 10.1007/s00371-010-0466-6
   Xu K, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618464
NR 23
TC 15
Z9 17
U1 0
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2011
VL 27
IS 6-8
BP 739
EP 748
DI 10.1007/s00371-011-0583-x
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 766FY
UT WOS:000290767600032
DA 2024-07-18
ER

PT J
AU Zhou, ZG
   Tao, YB
   Lin, H
   Dong, F
   Clapworthy, G
AF Zhou, Zhiguang
   Tao, Yubo
   Lin, Hai
   Dong, Feng
   Clapworthy, Gordon
TI Shape-enhanced maximum intensity projection
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Conference on the Computer Graphics International (CGI)
CY JUN 12-15, 2011
CL Ottawa, CANADA
DE Maximum intensity projection; Phong shading; Tone reduction; Depth-based
   color; Shape perception
ID VOLUME; DISPLAY
AB Maximum intensity projection (MIP) displays the voxel with the maximum intensity along the viewing ray, and this offers simplicity in usage, as it does not require a complex transfer function, the specification of which is a highly challenging and time-consuming process in direct volume rendering (DVR). However, MIP also has its inherent limitation, the loss of spatial context and shape information. This paper proposes a novel technique, shape-enhanced maximum intensity projection (SEMIP), to resolve this limitation. Inspired by lighting in DVR to emphasize surface structures, SEMIP searches a valid gradient for the maximum intensity of each viewing ray, and applies gradient-based shading to improve shape and depth perception of structures. As SEMIP may result in the pixel values over the maximum intensity of the display device, a tone reduction technique is introduced to compress the intensity range of the rendered image while preserving the original local contrast. In addition, depth-based color cues are employed to enhance the visual perception of internal structures, and a focus and context interaction is used to highlight structures of interest. We demonstrate the effectiveness of the proposed SEMIP with several volume data sets, especially from the medical field.
C1 [Zhou, Zhiguang; Tao, Yubo; Lin, Hai] Zhejiang Univ, State Key Lab CAD&CG, Hangzhou 310003, Zhejiang, Peoples R China.
   [Tao, Yubo; Clapworthy, Gordon] Univ Bedfordshire, CCGV, Luton, Beds, England.
   [Dong, Feng] Univ Bedfordshire, Dept Comp & Informat Syst, Luton, Beds, England.
C3 Zhejiang University; University of Bedfordshire; University of
   Bedfordshire
RP Tao, YB (corresponding author), Zhejiang Univ, State Key Lab CAD&CG, Hangzhou 310003, Zhejiang, Peoples R China.
EM zhouzhiguang@zjucadcg.cn; taoyubo@cad.zju.edu.cn; lin@cad.zju.edu.cn;
   Feng.Dong@beds.ac.uk; Gordon.Clapworthy@beds.ac.uk
CR [Anonymous], 1997, SIGGRAPH, DOI DOI 10.1145/258734.258884
   Behrens U, 1998, IEEE SYMPOSIUM ON VOLUME VISUALIZATION, P39, DOI 10.1109/SVV.1998.729583
   Bruckner S, 2007, IEEE T VIS COMPUT GR, V13, P1344, DOI 10.1109/TVCG.2007.70555
   Bruckner S, 2009, COMPUT GRAPH FORUM, V28, P775, DOI 10.1111/j.1467-8659.2009.01474.x
   Correa CD, 2008, IEEE T VIS COMPUT GR, V14, P1380, DOI 10.1109/TVCG.2008.162
   Diaz J, 2010, IEEE EG S VOL GRAPH, DOI [10.2312/VG/VG10/093-100, DOI 10.2312/VG/VG10/093-100]
   Drago F, 2003, COMPUT GRAPH FORUM, V22, P419, DOI 10.1111/1467-8659.00689
   Durand F, 2002, ACM T GRAPHIC, V21, P257, DOI 10.1145/566570.566574
   Ebert D, 2000, IEEE VISUAL, P195, DOI 10.1109/VISUAL.2000.885694
   Hauser H, 2001, IEEE T VIS COMPUT GR, V7, P242, DOI 10.1109/2945.942692
   Heidrich W, 1995, VISUALIZATION '95 - PROCEEDINGS, P11, DOI 10.1109/VISUAL.1995.480790
   Kindlmann G, 2003, IEEE VISUALIZATION 2003, PROCEEDINGS, P513, DOI 10.1109/VISUAL.2003.1250414
   KINDLMANN G, 2002, ACM SIGGRAPH COURSE
   Kniss J, 2003, IEEE T VIS COMPUT GR, V9, P150, DOI 10.1109/TVCG.2003.1196003
   Kniss J, 2002, IEEE T VIS COMPUT GR, V8, P270, DOI 10.1109/TVCG.2002.1021579
   La Cruz A, 2004, IEEE VISUALIZATION 2004, PROCEEEDINGS, P393, DOI 10.1109/VISUAL.2004.72
   Ledda P, 2005, ACM T GRAPHIC, V24, P640, DOI 10.1145/1073204.1073242
   MAX N, 1995, IEEE T VIS COMPUT GR, V1, P99, DOI 10.1109/2945.468400
   Mora B, 2005, ACM T GRAPHIC, V24, P1392, DOI 10.1145/1095878.1095886
   Mroz L, 2000, COMPUT GRAPH FORUM, V19, pC341, DOI 10.1111/1467-8659.00426
   Nagy Z, 2002, VISION MODELING, AND VISUALIZATION 2002, PROCEEDINGS, P497
   REZKSALAMA C, 2009, ACM SIGGRAPH COURSE
   Ropinski T, 2006, LECT NOTES COMPUT SC, V4073, P93
   Sato Y, 1998, J COMPUT ASSIST TOMO, V22, P912, DOI 10.1097/00004728-199811000-00014
   Svakhine NA, 2009, IEEE T VIS COMPUT GR, V15, P77, DOI 10.1109/TVCG.2008.56
   WALLIS JW, 1989, IEEE T MED IMAGING, V8, P297, DOI 10.1109/42.41482
   Yuan XR, 2005, IEEE VISUALIZATION 2005, PROCEEDINGS, P327
   Yuan XR, 2006, IEEE T VIS COMPUT GR, V12, P433, DOI 10.1109/TVCG.2006.72
NR 28
TC 9
Z9 15
U1 0
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2011
VL 27
IS 6-8
BP 677
EP 686
DI 10.1007/s00371-011-0570-2
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 766FY
UT WOS:000290767600026
DA 2024-07-18
ER

PT J
AU Rilling, S
   Wechselberger, U
AF Rilling, Stefan
   Wechselberger, Ulrich
TI A framework to meet didactical requirements for serious game design
SO VISUAL COMPUTER
LA English
DT Article
DE Serious games; Training simulation; Object behavior; Data Flow
   architecture
AB Interest in game-based training is growing within the industry. In this article, we present an interdisciplinary work on the application of computer game principles and techniques within an automation industry training scenario. A data flow-based component architecture forms the technical foundation of our system and enables us to exploit the capabilities of available middleware components like game or physics engines. An interactive simulation of a real automation plant is built as a combination of state-based and physical object behavior. A training scenario founded on didactical principles of video game is built with the help of our implemented system.
C1 [Rilling, Stefan; Wechselberger, Ulrich] Univ Koblenz, Inst Computat Visualist, Koblenz, Germany.
C3 University of Koblenz & Landau
RP Rilling, S (corresponding author), Univ Koblenz, Inst Computat Visualist, Koblenz, Germany.
EM rilling@uni-koblenz.de; wberger@uni-koblenz.de
FU German government department of education and science (BMBF)
FX The authors would like to thank Prof. Stefan Mueller, Gerrit Lochmann,
   Dominik Ospelt, and Niklas Henrich. The project was funded by the German
   government department of education and science (BMBF).
CR BOPP M, 2006, AFFECTIVE EMOTIONAL, P8
   Carless S., 2005, POSTCARD SGS 2005 HA
   GEE PJ, 2007, GOOD VIDEO GAMES GOO, V27
   Gentile DA, 2008, J YOUTH ADOLESCENCE, V37, P127, DOI 10.1007/s10964-007-9206-2
   Gerbaud S, 2008, IEEE VIRTUAL REALITY 2008, PROCEEDINGS, P225
   Klimmt C., 2006, COMPUTERSPIELEN ALS
   Mollet N., 2007, IPT-EGVE the 13th Eurographics Symposium on Virtual Environments, P95
   Mollet N, 2006, LECT NOTES COMPUT SC, V3942, P334, DOI 10.1007/11736639_45
   Rheinberg F, 2001, DIAGNOSTICA, V47, P57, DOI 10.1026//0012-1924.47.2.57
   Van Eck R., 2006, EDUCAUSE REV, V41, P16, DOI DOI 10.1145/950566.950596
   Wang H., 2009, SERIOUS GAMES, P25
   Wechselberger U, 2009, LECT NOTES COMPUT SC, V5660, P90, DOI 10.1007/978-3-642-03270-7_7
NR 12
TC 4
Z9 5
U1 0
U2 18
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2011
VL 27
IS 4
SI SI
BP 287
EP 297
DI 10.1007/s00371-011-0550-6
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 739IC
UT WOS:000288707700005
DA 2024-07-18
ER

PT J
AU Kim, BU
   Feng, WW
   Yu, YZ
AF Kim, Byung-Uck
   Feng, Wei-Wei
   Yu, Yizhou
TI Real-time data driven deformation with affine bones
SO VISUAL COMPUTER
LA English
DT Article
DE Deformation; Canonical correlation analysis; Regression; Weight
   smoothing
ID ANIMATION
AB Data driven deformation is increasingly important in computer graphics and interactive applications. From given mesh example sequences, we train a deformation predictor and manipulate a specific style of surface deformation interactively using only a small number of control points. The latest approach of learning the connection between rigid bone transformations and control points uses a statistically based framework, called canonical correlation analysis. In this paper, we extend this approach to a skinned mesh with affine bones, each of which conveys a nonrigid affine transformation. However, it is difficult to discover the underlying relationship between control points and nonrigid transformations. To address this issue, we present a two-layer regression framework; one layer being from control points to rigid and the other layer being from rigid to nonrigid transformations. Our contributions also include bone-vertex weight smoothing, enabling the distribution of each bone's influence across neighboring vertices. We can alleviate distortion around regions where nearby bones undergo various transformations and improve deformations reaching beyond the learned subspaces. Experimental results show that our method can achieve more general deformations including flexible muscle bulges or twists. The performance of our implementation is comparable to the latest approach.
C1 [Kim, Byung-Uck; Feng, Wei-Wei; Yu, Yizhou] Univ Illinois, Dept Comp Sci, Urbana, IL 61801 USA.
C3 University of Illinois System; University of Illinois Urbana-Champaign
RP Kim, BU (corresponding author), Univ Illinois, Dept Comp Sci, 201 N Goodwin Ave, Urbana, IL 61801 USA.
EM kbu@illinois.edu; wfeng2@illinois.edu; yyz@illinois.edu
RI YU, YIZHOU/D-1603-2013; /F-3345-2010
OI /0000-0002-0470-5548
CR Anguelov D, 2005, ACM T GRAPHIC, V24, P408, DOI 10.1145/1073204.1073207
   [Anonymous], 2009, NVIDIA CUDA Compute Unified Device Architecture Programming Guide version 2.3
   Chai JX, 2005, ACM T GRAPHIC, V24, P686, DOI 10.1145/1073204.1073248
   Der KG, 2006, ACM T GRAPHIC, V25, P1174, DOI 10.1145/1141911.1142011
   Desbrun M, 1999, COMP GRAPH, P317, DOI 10.1145/311535.311576
   Feng WW, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360690
   Grochow K, 2004, ACM T GRAPHIC, V23, P522, DOI 10.1145/1015706.1015755
   James DL, 2005, ACM T GRAPHIC, V24, P399, DOI 10.1145/1073204.1073206
   Lewis JP, 2000, COMP GRAPH, P165, DOI 10.1145/344779.344862
   Magnenat-Thalmann Nadia, 1988, P GRAPHICS INTERFACE
   Mohr A, 2003, ACM T GRAPHIC, V22, P562, DOI 10.1145/882262.882308
   Rost R, 2006, OPENGL R SHADING LAN
   Shoemake K., 1992, Proceedings. Graphics Interface '92, P258
   Sumner RW, 2005, ACM T GRAPHIC, V24, P488, DOI 10.1145/1073204.1073218
   Sumner RW, 2004, ACM T GRAPHIC, V23, P399, DOI 10.1145/1015706.1015736
   Wang RY, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1239451.1239524, 10.1145/1276377.1276468]
NR 16
TC 7
Z9 7
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2010
VL 26
IS 6-8
BP 487
EP 495
DI 10.1007/s00371-010-0474-6
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 602JQ
UT WOS:000278135800010
DA 2024-07-18
ER

PT J
AU Pan, ZG
   Cheng, X
   Chen, WZ
   Liu, GD
   Tang, B
AF Pan, Zhigeng
   Cheng, Xi
   Chen, Wenzhi
   Liu, Gengdai
   Tang, Bing
TI Real time falling animation with active and protective responses
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Computer Graphics International Conference 2009
CY MAY 26-29, 2009
CL Victoria, CANADA
DE Character animation; Physics based modeling; Reactive motion; Balance
   detection; Protective responses
ID RECOVERY
AB Combined with motion capture and dynamic simulation, characters in animation have realistic motion details and can respond to unexpected contact forces. This paper proposes a novel and real-time character motion generation approach which introduces a parallel process, and uses an approximate nearest neighbor optimization search method. Besides, we employ a support vector machine (SVM), which is trained on a set of samples and predicts a subset of our 'return-to' motion capture (mocap) database in order to reduce the search time. In the dynamic simulation process, we focus on designing a biomechanics based controller which detects the balance of the characters in locomotion and drives them to take several active and protective responses when they fall to the ground in order to reduce the injuries to their bodies. Finally, we show the time costs in synthesis and the visual results of our approach. The experimental results indicate that our motion generation approach is suitable for interactive games or other real-time applications.
C1 [Chen, Wenzhi] Zhejiang Univ, Dept Comp Sci & Technol, Hangzhou 310027, Peoples R China.
   [Pan, Zhigeng; Cheng, Xi; Liu, Gengdai; Tang, Bing] Zhejiang Univ, State Key Lab CAD&CG, Hangzhou 310058, Zhejiang, Peoples R China.
C3 Zhejiang University; Zhejiang University
RP Chen, WZ (corresponding author), Zhejiang Univ, Dept Comp Sci & Technol, Hangzhou 310027, Peoples R China.
EM zgpan@cad.zju.edu.cn; chengxi@cad.zju.edu.cn; chenwz@zju.edu.cn;
   liugengdai@cad.zju.edu.cn; btang@cad.zju.edu.cn
RI 陳, 文誌/AAI-6255-2021
CR Ahokas H., 2004, On the evolution, spread and names of rutabaga, DOI 10.1145/1077534.1077542
   Arikan Okan., 2005, SCA 2005: Proceedings of the 2005 ACM SIGGRAPH/Eurographics Symposium on Computer Animation, P59
   DELP SL, 1990, IEEE T BIO-MED ENG, V37, P757, DOI 10.1109/10.102791
   Faloutsos P, 2001, COMP GRAPH, P251, DOI 10.1145/383259.383287
   Fang AC, 2003, ACM T GRAPHIC, V22, P417, DOI 10.1145/882262.882286
   Feldman F, 2007, J BIOMECH, V40, P2612, DOI 10.1016/j.jbiomech.2007.01.019
   Heck R, 2007, I3D 2007: ACM SIGGRAPH SYMPOSIUM ON INTERACTIVE 3D GRAPHICS AND GAMES, PROCEEDINGS, P129
   Hof AL, 2005, J BIOMECH, V38, P1, DOI 10.1016/j.jbiomech.2004.03.025
   Hsiao ET, 1999, J BIOMECH, V32, P1099, DOI 10.1016/S0021-9290(99)00104-9
   Komura T, 2005, COMPUT ANIMAT VIRT W, V16, P213, DOI 10.1002/cav.101
   Kovar L, 2002, ACM T GRAPHIC, V21, P473, DOI 10.1145/566570.566605
   Kry PG, 2006, ACM T GRAPHIC, V25, P872, DOI 10.1145/1141911.1141969
   Lee J, 1999, COMP GRAPH, P39
   MANDEL M, 2004, THESIS CARNEGIE MELL
   MIALL RC, 1993, J MOTOR BEHAV, V25, P203, DOI 10.1080/00222895.1993.9942050
   MOUNT D, 1993, P 4 ANN ACMSIAM S DI, P271
   Pai YC, 1997, J BIOMECH, V30, P347, DOI 10.1016/S0021-9290(96)00165-0
   REITSMA PSA, 2004, P 2004 ACM SIGGRAPH, P89
   Robinovitch SN, 2004, J BIOMECH, V37, P1329, DOI 10.1016/j.jbiomech.2003.12.015
   Sabick MB, 1999, J BIOMECH, V32, P993, DOI 10.1016/S0021-9290(99)00079-2
   Shapiro A, 2003, 11TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P455, DOI 10.1109/PCCGA.2003.1238294
   Shin HJ, 2003, 11TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P194
   TANG B, 2006, THESIS ZHEJIANG U
   Tang B, 2006, COMPUT ANIMAT VIRT W, V17, P271, DOI 10.1002/cav.131
   Treuille A, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239458
   WHITNEY DE, 1969, IEEE T MAN MACHINE, VMM10, P47, DOI 10.1109/TMMS.1969.299896
   Wu M, 2007, J BIOMECH, V40, P1559, DOI 10.1016/j.jbiomech.2006.07.019
   Wu Ming, 2003, Journal of Tsinghua University (Science and Technology), V43, P152
   Yin KK, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360680
   Yin KK, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239556
   ZORDAN VB, 2003, ACM T GRAPHIC, V24, P697
   Zordan V, 2007, SANDBOX SYMPOSIUM 2007: ACM SIGGRAPH VIDEO GAME SYMPOSIUM, PROCEEDINGS, P9
NR 32
TC 0
Z9 0
U1 0
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2009
VL 25
IS 5-7
BP 487
EP 497
DI 10.1007/s00371-009-0321-9
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 438ES
UT WOS:000265539300013
DA 2024-07-18
ER

PT J
AU Rusnell, B
   Mould, D
   Eramian, M
AF Rusnell, Brennan
   Mould, David
   Eramian, Mark
TI Feature-rich distance-based terrain synthesis
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Computer Graphics International Conference 2009
CY MAY 26-29, 2009
CL Victoria, CANADA
DE Terrain synthesis; Natural phenomena modelling; Path planning;
   Pareidolia
ID EROSION; MODELS
AB This paper describes a novel terrain synthesis method based on distances in a weighted graph. A height field is determined by least-cost paths in a weighted graph from a set of generator nodes. The shapes of individual terrain features, such as mountains, hills, and craters, are specified by a monotonically decreasing profile describing the cross-sectional shape of a feature. The locations of features in the terrain are specified by placing the generators; secondary ridges are placed by pathing. We show the method to be robust and easy to control, even making it possible to embed images in terrain shadows. The method can produce a wide range of realistic synthetic terrains such as mountain ranges, craters, cinder cones, and hills. The ability to manually place terrain features that incorporate multiple profiles produces heterogeneous terrains that compare favorably to existing methods.
C1 [Rusnell, Brennan; Eramian, Mark] Univ Saskatchewan, Saskatoon, SK S7N 5C9, Canada.
   [Mould, David] Carleton Univ, Ottawa, ON K1S 5B6, Canada.
C3 University of Saskatchewan; Carleton University
RP Rusnell, B (corresponding author), Univ Saskatchewan, 110 Sci Pl, Saskatoon, SK S7N 5C9, Canada.
EM brennan.rusnell@usask.ca
CR Benes B, 2006, COMPUT ANIMAT VIRT W, V17, P99, DOI 10.1002/cav.77
   Benes B, 2002, WSCG'2002, VOLS I AND II, CONFERENCE PROCEEDINGS, P79
   Chiba N, 1998, J VISUAL COMP ANIMAT, V9, P185, DOI 10.1002/(SICI)1099-1778(1998100)9:4<185::AID-VIS178>3.0.CO;2-2
   Dachsbacher C., 2006, THESIS U ERLANGEN NU
   Ebert David S, 2003, Texturing Modeling: A Procedural Approach
   FOURNIER A, 1982, COMMUN ACM, V25, P371, DOI 10.1145/358523.358553
   KELLEY AD, 1988, P 15 ANN C COMP GRAP, P263
   Mandelbrot B. B., 1982, FRACTAL GEOMETRY NAT
   Musgrave F. K.:, 1993, THESIS YALE U
   MUSGRAVE FK, 1989, P 16 ANN C COMP GRAP, P41
   Nagashima K, 1997, VISUAL COMPUT, V13, P456, DOI 10.1007/s003710050117
   NEIDHOLD B, 2005, EUR WORKSH NAT PHEN, P25
   Prusinkiewicz P., 1993, Proceedings Graphics Interface '93, P174
   SINGH K., 1998, SIGGRAPH 98, P405, DOI DOI 10.1145/280814.280946
   Szeliski R., 1989, Computer Graphics, V23, P51, DOI 10.1145/74334.74338
   Winston P.H., 1992, Artificial Intelligence
   Worley S., 1996, P 23 ANN C COMPUTER, P291, DOI [DOI 10.1145/237170.237267, 10.1145/237170.237267]
   Zhou H, 2007, IEEE T VIS COMPUT GR, V13, P834, DOI 10.1109/TVCG.2007.1027
NR 18
TC 20
Z9 24
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2009
VL 25
IS 5-7
BP 573
EP 579
DI 10.1007/s00371-009-0332-6
PG 7
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 438ES
UT WOS:000265539300022
DA 2024-07-18
ER

PT J
AU Batagelo, HC
   Wu, ST
AF Batagelo, Harlen Costa
   Wu, Shin-Ting
TI A framework for GPU-based application-independent 3D interactions
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Conference on SIBGRAPI 2007
CY SEP, 2007
CL Belo Horizonte, BRAZIL
DE Interaction and direct manipulation; Programmable graphics hardware
AB Direct manipulation using pointing devices commonly relies on basic interaction tasks such as selection and precise cursor positioning. Traditionally, such tasks use geometric attributes computed on the basis of application-dependent intersection algorithms on the CPU. However, with the recent ability of using GPUs to perform geometry modeling tasks and even to create new primitives on-the-fly, geometric attributes computed on the CPU are often invalid. We analyze a new paradigm for correctly computing such attributes based on providing, for each pixel of the rendered models, application-defined data and elements of discrete differential geometry computed solely on the GPU. We validate this by showing how many direct manipulation tasks presented in the literature require only these data, and present an interaction framework that exploits such paradigm. Implementation results are presented.
C1 [Batagelo, Harlen Costa; Wu, Shin-Ting] Univ Estadual Campinas, Sch Elect & Comp Engn, BR-13083970 Campinas, SP, Brazil.
C3 Universidade Estadual de Campinas
RP Batagelo, HC (corresponding author), Univ Estadual Campinas, Sch Elect & Comp Engn, BR-13083970 Campinas, SP, Brazil.
EM harlen@dca.fee.unicamp.br; ting@dca.fee.unicamp.br
RI Wu, Shin Ting/I-8055-2014; Batagelo, Harlen/G-1318-2014
OI Wu, Shin-Ting/0000-0002-3152-4523; Batagelo, Harlen/0000-0002-2325-2070
CR Batagelo H.C., 2005, P 2005 S INT 3D GRAP, P81
   Batagelo HC, 2007, SIBGRAPI, P19, DOI 10.1109/SIBGRAPI.2007.12
   Batagelo HC, 2007, VISUAL COMPUT, V23, P803, DOI 10.1007/s00371-007-0133-8
   Bier E. A., 1987, Proceedings of the 1986 Workshop on Interactive 3D Graphics, P183, DOI 10.1145/319120.319135
   DeCarlo Doug., 2004, P INT S NONPHOTOREAL, P15, DOI DOI 10.1145/987657.987661
   Gleicher M., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P183, DOI 10.1145/218380.218441
   Kuehne B., 2007, OpenSceneGraph Reference Manual v2.2
   Neider J., 1993, OPENGL PROGRAMMING G
   PIERCE JS, 2003, P 2003 S INT 3D GRAP, P189
   Reiners D., 2002, THESIS TU DARMSTADT
   Saito T., 1990, Computer Graphics, V24, P197, DOI 10.1145/97880.97901
   Stegmaier S, 2005, VOLUME GRAPHICS 2005, P187
   WEGHORST H, 1984, ACM T GRAPHIC, V3, P52, DOI 10.1145/357332.357335
   Wu ST, 2003, XVI BRAZILIAN SYMPOSIUM ON COMPUTER GRAPHICS AND IMAGE PROCESSING, PROCEEDINGS, P140, DOI 10.1109/SIBGRA.2003.1241002
   Yoo KH, 2004, LECT NOTES COMPUT SC, V3039, P90
   ZELEZNJK RC, 1991, COMP GRAPH, V25, P105, DOI 10.1145/127719.122730
NR 16
TC 2
Z9 2
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2008
VL 24
IS 12
BP 1003
EP 1012
DI 10.1007/s00371-008-0296-y
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 363VH
UT WOS:000260294500002
DA 2024-07-18
ER

PT J
AU Schmedding, R
   Teschner, M
AF Schmedding, Ruediger
   Teschner, Matthias
TI Inversion handling for stable deformable modeling
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 26th International Conference on Computer Graphics
CY JUN 09-11, 2008
CL Istanbul, TURKEY
DE deformable modeling; finite element method; corotational FEM; inverted
   elements
AB In 3D deformable modeling approaches based on FEM, inverted tetrahedral elements can cause undesired visual artifacts and the breakdown of the simulation. As inversion can never be avoided and sometimes is even the correct behavior of elements, there is a strong need for stable inversion handling. In this paper, we propose a novel method to resolve inverted elements which is motivated by previous work of Irving et al. [6]. In combination with an efficient handling of degenerated elements, our approach yields a stable simulation of arbitrary deformations. Although we focus on the corotational formulation of linear FEM, the method can be implemented within arbitrary constitutive models.
C1 [Schmedding, Ruediger; Teschner, Matthias] Univ Freiburg, Comp Graph Grp, D-79110 Freiburg, Germany.
C3 University of Freiburg
RP Schmedding, R (corresponding author), Univ Freiburg, Comp Graph Grp, Georges Koehler Allee 052, D-79110 Freiburg, Germany.
EM schmedd@informatik.uni-freiburg.de
CR [Anonymous], 2004, P 2004 ACM SIGGRAPH, DOI DOI 10.1145/1028523.1028541
   CHEN DT, 1992, COMP GRAPH, V26, P89, DOI 10.1145/142920.134016
   Debunne G, 2001, COMP GRAPH, P31, DOI 10.1145/383259.383262
   Escobar JM, 2003, COMPUT METHOD APPL M, V192, P2775, DOI 10.1016/S0045-7825(03)00299-8
   Espinosa HD, 1998, MECH MATER, V29, P275, DOI 10.1016/S0167-6636(98)00018-0
   Hauth M., 2004, PROC WSCG, V12, P137
   KANATANI K, 1994, IEEE T PATTERN ANAL, V16, P543, DOI 10.1109/34.291441
   Müller M, 2005, ACM T GRAPHIC, V24, P471, DOI 10.1145/1073204.1073216
   Müller M, 2004, PROC GRAPH INTERF, P239
   Muller M., 2002, P 2002 ACM SIGGRAPHE, P49, DOI DOI 10.1145/545261.545269
   O'Brien JF, 2002, ACM T GRAPHIC, V21, P291, DOI 10.1145/566570.566579
   OBRIEN J, 1999, P SIGGRAPH 99, V18, P137
   Picinbono G, 2001, IEEE INT CONF ROBOT, P1370, DOI 10.1109/ROBOT.2001.932801
   Terzopoulos D., 1987, COMPUT GRAPH, P205, DOI DOI 10.1145/37402.37427
   Teschner M, 2004, COMPUTER GRAPHICS INTERNATIONAL, PROCEEDINGS, P312, DOI 10.1109/CGI.2004.1309227
   Vachal P, 2004, J COMPUT PHYS, V196, P627, DOI 10.1016/j.jcp.2003.11.011
   ZIELINSKI P, 1995, MATEMATYKA STOSOWANA, V38, P23
NR 17
TC 13
Z9 19
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2008
VL 24
IS 7-9
BP 625
EP 633
DI 10.1007/s00371-008-0243-y
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 322OI
UT WOS:000257384800018
DA 2024-07-18
ER

PT J
AU Cheng, ZQ
   Liu, HF
   Jin, SY
AF Cheng, Zhi-Quan
   Liu, Hua-Feng
   Jin, Shi-Yao
TI The progressive mesh compression based on meaningful segmentation
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 25th Computer Graphics International Conference (CGI)
CY MAY 30-JUN 02, 2007
CL Petropolis, BRAZIL
DE mesh compression; progressive; meaningful segmentation; view-dependent
ID MODELS
AB Nowadays, both mesh meaningful segmentation (also called shape decomposition) and progressive compression are fundamental important problems, and some compression algorithms have been developed with the help of patch-type segmentation. However, little attention has been paid to the effective combination of mesh compression and meaningful segmentation. In this paper, to accomplish both adaptive selective accessibility and a reasonable compression ratio, we break down the original mesh into meaningful parts and encode each part by an efficient compression algorithm. In our method, the segmentation of a model is obtained by a new feature-based decomposition algorithm, which makes use of the salient feature contours to parse the object. Moreover, the progressive compression is an improved degree-driven method, which adapts a multi-granularity quantization method in geometry encoding to obtain a higher compression ratio. We provide evidence that the proposed combination can be beneficial in many applications, such as view-dependent rendering and streaming of large meshes in a compressed form.
C1 Univ Defence Technol, PDL Lab, Changsha 410073, Hunan Province, Peoples R China.
C3 National University of Defense Technology - China
RP Cheng, ZQ (corresponding author), Univ Defence Technol, PDL Lab, Changsha 410073, Hunan Province, Peoples R China.
EM zhiquan.cheng@gmail.com; redfox_lhf@yahoo.com.cn; syjin1937@163.com
CR Alliez P, 2005, MATH VIS, P3, DOI 10.1007/3-540-26808-1_1
   Alliez P, 2001, COMP GRAPH, P195, DOI 10.1145/383259.383281
   ALLIEZ P, 2001, EUROGRAPHICS, P480
   [Anonymous], 2003, Level of detail for 3D graphics
   Attene M, 2006, IEEE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS 2006, PROCEEDINGS, P14
   Deering M., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P13, DOI 10.1145/218380.218391
   Friedel I, 2004, ACM T GRAPHIC, V23, P1061, DOI 10.1145/1027411.1027418
   Funkhouser T, 2004, ACM T GRAPHIC, V23, P652, DOI 10.1145/1015706.1015775
   Gandoin PM, 2002, ACM T GRAPHIC, V21, P372, DOI 10.1145/566570.566591
   GU X, 2002, SIGGRAPH 02, P355, DOI DOI 10.1145/566570.566589
   Guskov I, 2000, COMP GRAPH, P95, DOI 10.1145/344779.344831
   HOFFMAN DD, 1984, COGNITION, V18, P65, DOI 10.1016/0010-0277(84)90022-2
   HOPPE H, 1996, SIGGRAPH 96, P99, DOI DOI 10.1145/237170.237216
   Huang J, 2006, ACM T GRAPHIC, V25, P1126, DOI 10.1145/1141911.1142003
   Kalaiah A, 2005, ACM T GRAPHIC, V24, P348, DOI 10.1145/1061347.1061356
   Karni Z, 2000, COMP GRAPH, P279, DOI 10.1145/344779.344924
   Katz S, 2003, ACM T GRAPHIC, V22, P954, DOI 10.1145/882262.882369
   Khodakovsky A, 2000, COMP GRAPH, P271, DOI 10.1145/344779.344922
   Kim J, 2006, COMPUT GRAPH FORUM, V25, P323, DOI 10.1111/j.1467-8659.2006.00951.x
   Koller D, 2004, ACM T GRAPHIC, V23, P695, DOI 10.1145/1015706.1015782
   Lee Y, 2005, COMPUT AIDED GEOM D, V22, P444, DOI 10.1016/j.cagd.2005.04.002
   Li X., 2001, P 2001 S INT 3D GRAP, P35, DOI DOI 10.1145/364338.364343
   LIEN JM, 2006, SPM 06 P 2006 ACM S, P216
   Mangan AP, 1999, IEEE T VIS COMPUT GR, V5, P308, DOI 10.1109/2945.817348
   MARTIN IB, 2003, IEEE COMPUT GRAPH, V2, P6
   Peng JL, 2005, J VIS COMMUN IMAGE R, V16, P688, DOI 10.1016/j.jvcir.2005.03.001
   Peng JL, 2005, ACM T GRAPHIC, V24, P609, DOI 10.1145/1073204.1073237
   Peyré G, 2005, ACM T GRAPHIC, V24, P601, DOI 10.1145/1073204.1073236
   Rusinkiewicz S, 2004, 2ND INTERNATIONAL SYMPOSIUM ON 3D DATA PROCESSING, VISUALIZATION, AND TRANSMISSION, PROCEEDINGS, P486, DOI 10.1109/TDPVT.2004.1335277
   SANDER P, 2003, EUR S GEOM PROC, P146
   Sander PV, 2001, COMP GRAPH, P409, DOI 10.1145/383259.383307
   SHAMIR A, 2004, 3DPVT 04, P82, DOI DOI 10.1109/3DPVT.2004.13
   Shatz I, 2006, VISUAL COMPUT, V22, P825, DOI 10.1007/s00371-006-0067-6
   Yan ZD, 2005, IEEE T CIRC SYST VID, V15, P138, DOI 10.1109/TCSVT.2004.837023
   Yan ZD, 2001, IEEE T CIRC SYST VID, V11, P860, DOI 10.1109/76.931112
   Yang S, 2004, IEEE T CIRC SYST VID, V14, P1249, DOI 10.1109/TCSVT.2004.835153
   Yang S, 2002, PROC SPIE, V4671, P268, DOI 10.1117/12.453066
   Zelinka S, 2006, PROC GRAPH INTERF, P107
   Zhang E, 2005, ACM T GRAPHIC, V24, P1, DOI 10.1145/1037957.1037958
NR 39
TC 10
Z9 11
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2007
VL 23
IS 9-11
SI SI
BP 651
EP 660
DI 10.1007/s00371-007-0128-5
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 206UE
UT WOS:000249207600005
DA 2024-07-18
ER

PT J
AU Saleem, W
   Schall, O
   Patanè, G
   Belyaev, A
   Seidel, HP
AF Saleem, Waqar
   Schall, Oliver
   Patane, Giuseppe
   Belyaev, Alexander
   Seidel, Hans-Peter
TI On stochastic methods for surface reconstruction
SO VISUAL COMPUTER
LA English
DT Article
DE surface reconstruction; point cloud denoising; sparse implicits;
   statistical learning
ID NEURAL-NETWORK; IMPLICIT SURFACES; DENSITY-FUNCTION; MEAN SHIFT;
   APPROXIMATION; ALGORITHM; MODEL
AB In this article, we present and discuss three statistical methods for surface reconstruction. A typical input to a surface reconstruction technique consists of a large set of points that has been sampled from a smooth surface and contains uncertain data in the form of noise and outliers. We first present a method that filters out uncertain and redundant information yielding a more accurate and economical surface representation. Then we present two methods, each of which converts the input point data to a standard shape representation; the first produces an implicit representation while the second yields a triangle mesh.
C1 Max Planck Inst Informat, Saarbrucken, Germany.
   CNR, IMATI GE, Genoa, Italy.
C3 Max Planck Society; Consiglio Nazionale delle Ricerche (CNR); Istituto
   di Matematica Applicata e Tecnologie Informatiche "Enrico Magenes"
   (IMATI-CNR)
RP Saleem, W (corresponding author), Max Planck Inst Informat, Saarbrucken, Germany.
EM wsaleem@mpi-inf.mpg.de; schall@mpi-inf.mpg.de; patane@ge.imati.cnr.it;
   belyaev@mpi-inf.mpg.de; hpseidel@mpi-inf.mpg.de
RI University, Heriot-Watt/U-9720-2019; Patane', Giuseppe/O-1322-2013
OI Patane', Giuseppe/0000-0002-2276-9553; Belyaev,
   Alexander/0000-0002-7043-8847
CR Alexa M, 2001, IEEE VISUAL, P21, DOI 10.1109/VISUAL.2001.964489
   Amenta Nina, 2001, P 6 ACM S SOL MOD AP
   ANGELIDIS A, 2002, SMA 02 P ACM S SOL M, P45
   [Anonymous], 1997, Introduction to Implicit Surfaces
   ARONSZAJN N, 1950, T AM MATH SOC, V68, P337, DOI 10.1090/s0002-9947-1950-0051437-7
   Arya S, 1998, J ACM, V45, P891, DOI 10.1145/293347.293348
   Barhak J, 2001, NINTH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P97, DOI 10.1109/PCCGA.2001.962862
   Barhak J, 2001, IEEE T VIS COMPUT GR, V7, P1, DOI 10.1109/2945.910817
   Bishop C. M., 1995, NEURAL NETWORKS PATT
   Blanz V, 2004, 2ND INTERNATIONAL SYMPOSIUM ON 3D DATA PROCESSING, VISUALIZATION, AND TRANSMISSION, PROCEEDINGS, P293, DOI 10.1109/TDPVT.2004.1335212
   Botsch M, 2005, COMPUT GRAPH FORUM, V24, P611, DOI 10.1111/j.1467-8659.2005.00886.x
   Botsch M, 2003, 11TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P335, DOI 10.1109/PCCGA.2003.1238275
   Camastra F, 2005, IEEE T PATTERN ANAL, V27, P801, DOI 10.1109/TPAMI.2005.88
   Carr JC, 2001, COMP GRAPH, P67, DOI 10.1145/383259.383266
   CHEN S, 1995, IEEE T SIGNAL PROCES, V43, P1713, DOI 10.1109/78.398734
   CHENG YZ, 1995, IEEE T PATTERN ANAL, V17, P790, DOI 10.1109/34.400568
   Coleman TF, 1996, SIAM J OPTIMIZ, V6, P418, DOI 10.1137/0806023
   Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236
   CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1007/BF00994018
   Dey T.K., 2003, P 8 ACM S SOL MOD AP, P127, DOI DOI 10.1145/781606.781627
   Dey T.K., 2005, Proceedings of the 3rd Eurographics Symposium on Geometry Processing, P43
   Fenn M, 2006, COMP IMAG VIS, P317
   Fleishman S, 2005, ACM T GRAPHIC, V24, P544, DOI 10.1145/1073204.1073227
   FUKUNAGA K, 1975, IEEE T INFORM THEORY, V21, P32, DOI 10.1109/TIT.1975.1055330
   Girosi F, 1998, NEURAL COMPUT, V10, P1455, DOI 10.1162/089976698300017269
   Golub G.H., 2013, Matrix Computations, DOI DOI 10.56021/9781421407944
   GU P, 1995, COMPUT AIDED DESIGN, V27, P59, DOI 10.1016/0010-4485(95)90753-3
   Hastie T., 2001, ELEMENTS STAT LEARNI, DOI 10.1007/978-0-387-84858-7_2
   HOFFMANN M, 1998, J GEOMETRY GRAPHICS, V2, P1
   Iglesias A, 2004, FUTURE GENER COMP SY, V20, P1337, DOI 10.1016/j.future.2004.05.025
   Ivrissimtzis I, 2004, 2ND INTERNATIONAL SYMPOSIUM ON 3D DATA PROCESSING, VISUALIZATION, AND TRANSMISSION, PROCEEDINGS, P308
   Ivrissimtzis IP, 2003, SMI 2003: SHAPE MODELING INTERNATIONAL 2003, PROCEEDINGS, P78
   Jeong WK, 2003, 11TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P404, DOI 10.1109/PCCGA.2003.1238284
   Jolliffe I.T., 1986, Principal component analysis, DOI DOI 10.1016/0169-7439(87)80084-9
   Kanai T., 2006, P 4 EUR S GEOM PROC, P21
   KAZHDAN MM, 2005, S GEOM PROC
   Knopf GK, 2004, GRAPH MODELS, V66, P50, DOI 10.1016/j.gmod.2003.08.001
   Lange C, 2005, COMPUT AIDED GEOM D, V22, P680, DOI 10.1016/j.cagd.2005.06.010
   Levin D., 1998, Mathematics of Computation, V67, P1517, DOI 10.1090/S0025-5718-98-00974-0
   LINDE Y, 1980, IEEE T COMMUN, V28, P84, DOI 10.1109/TCOM.1980.1094577
   LINSEN L, 2001, 20013 U KARLSR FAK I
   MEDEROS B, 2004, J BRAZILIAN COMPUT S
   Mitra N J., 2006, Symposium on Geometry Processing, P121
   Morse BS, 2001, INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, PROCEEDING, P89, DOI 10.1109/SMA.2001.923379
   MOSTAFA MGH, 1999, CVPR, V2, P2015
   Mumford D., 1999, MATH FRONTIERS PERSP, P197
   Ohtake Y, 2005, GRAPH MODELS, V67, P150, DOI 10.1016/j.gmod.2004.06.003
   Ohtake Y, 2003, ACM T GRAPHIC, V22, P463, DOI 10.1145/882262.882293
   OHTAKE Y, 2005, ACM S SOL PHYS MOD
   Ohtake Y., 2005, P ACMEG S GEOMETRY P, P149
   PARZEN E, 1962, ANN MATH STAT, V33, P1065, DOI 10.1214/aoms/1177704472
   Patané G, 2006, IEEE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS 2006, PROCEEDINGS, P222
   Pauly M, 2003, ACM T GRAPHIC, V22, P641, DOI 10.1145/882262.882319
   Pauly Mark, 2004, Eurographics Symposium on Point-Based Graphics, P77
   Pfister H, 2000, COMP GRAPH, P335, DOI 10.1145/344779.344936
   POGGIO T, 2003, NOTICES AMS, V5, P537
   ROSENBLATT M, 1956, ANN MATH STAT, V27, P832, DOI 10.1214/aoms/1177728190
   Rusinkiewicz S, 2000, COMP GRAPH, P343, DOI 10.1145/344779.344940
   SALEEM W, 2004, THESIS U SAARLAND SA
   SAMOZINO M, 2006, IN PRESS P EUR, P51
   SCHALL O, 2005, EUR S POINT BAS GRAP, P71, DOI DOI 10.2312/SPBG/SPBG05/071-077
   Scholkopf B., 2005, Advances in Neural Information Processing Systems, P1193
   Scholkopf B., 2005, ICML '05, P776
   Scholkopf B., 2002, Learning with Kernels
   Shen C, 2004, ACM T GRAPHIC, V23, P896, DOI 10.1145/1015706.1015816
   Steinke F, 2005, COMPUT GRAPH FORUM, V24, P285, DOI 10.1111/j.1467-8659.2005.00853.x
   SUPERBJ, 2004, CVPRW 04 P 2004 C CO, V6, P93
   Taubin G., 1995, P 22 ANN C COMP GRAP, P351, DOI DOI 10.1145/218380.218473
   Tobor I, 2004, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, P19, DOI 10.1109/SMI.2004.1314490
   Turk G, 2002, ACM T GRAPHIC, V21, P855, DOI 10.1145/571647.571650
   Turk G, 1999, COMP GRAPH, P335, DOI 10.1145/311535.311580
   Vázquez PP, 2003, COMPUT GRAPH FORUM, V22, P689, DOI 10.1111/j.1467-8659.2003.00717.x
   Vrady L., 1999, J GEOMETRY GRAPHICS, V3, P177
   WALDER C, 2006, COMP GRAPH FOR P EUR
   Willis A, 2004, INT C PATT RECOG, P249, DOI 10.1109/ICPR.2004.1334147
   Xie H, 2004, IEEE VISUALIZATION 2004, PROCEEEDINGS, P259, DOI 10.1109/VISUAL.2004.101
   Yang M, 2000, INT J PROD RES, V38, P2067, DOI 10.1080/002075400188500
   Yu Y, 1999, IEEE VIS 99 C P, P61
   Zwicker M, 2001, COMP GRAPH, P371, DOI 10.1145/383259.383300
NR 79
TC 8
Z9 12
U1 0
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2007
VL 23
IS 6
BP 381
EP 395
DI 10.1007/s00371-006-0094-3
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 165BC
UT WOS:000246277800001
DA 2024-07-18
ER

PT J
AU Cazals, F
   Giesen, J
   Pauly, M
   Zomorodian, A
AF Cazals, Frederic
   Giesen, Joachim
   Pauly, Mark
   Zomorodian, Afra
TI The conformal alpha shape filtration
SO VISUAL COMPUTER
LA English
DT Article
DE alpha shapes; computational topology; computational geometry; surface
   reconstruction
AB Conformal alpha shapes are a new filtration of the Delaunay triangulation of a finite set of points in R-d. In contrast to (ordinary) alpha shapes the new filtration is parameterized by a local scale parameter instead of the global scale parameter in alpha shapes. The local scale parameter conforms to the local geometry and is motivated from applications and previous algorithms in surface reconstruction. We show how conformal alpha shapes can be used for surface reconstruction of non-uniformly sampled surfaces, which is not possible with alpha shapes.
C1 ETH, Dept Comp Sci, Zurich, Switzerland.
   INRIA, Sophia Antipolis, France.
   Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA.
C3 Swiss Federal Institutes of Technology Domain; ETH Zurich; Inria;
   Stanford University
RP Giesen, J (corresponding author), ETH, Dept Comp Sci, Zurich, Switzerland.
EM Frederic.Cazals@sophia.inria.fr; giesen@inf.ethz.ch; pauly@inf.ethz.ch;
   afra@cs.stanford.edu
OI Cazals, Frederic/0000-0003-2735-6755; Pauly, Mark/0000-0003-4957-4825
CR Amenta N, 1999, DISCRETE COMPUT GEOM, V22, P481, DOI 10.1007/PL00009475
   Amenta N., 2000, 16 ANN ACM S COMPUTA, P213
   CAZALS F, 2005, 2 S POINT BAS GRAPH, P55
   Cole-McLaughlin K, 2004, DISCRETE COMPUT GEOM, V32, P231, DOI 10.1007/s00454-004-1122-6
   DEY TK, 2005, 21 ANN ACM S COMP GE, P218
   EDELSBRUNNER H, 1994, ACM T GRAPHIC, V13, P43, DOI 10.1145/174462.156635
   EDELSBRUNNER H, 1983, IEEE T INFORM THEORY, V29, P551, DOI 10.1109/TIT.1983.1056714
   Edelsbrunner H, 1998, DISCRETE APPL MATH, V88, P83, DOI 10.1016/S0166-218X(98)00067-5
   Edelsbrunner H., 2001, Geometry and Topology for Mesh Generation
   EDELSBRUNNER H, 1993, 9 ANN S COMP GEOM SA, P218
   Giesen J, 2003, SIAM PROC S, P285
NR 11
TC 24
Z9 28
U1 1
U2 7
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2006
VL 22
IS 8
BP 531
EP 540
DI 10.1007/s00371-006-0027-1
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 088TD
UT WOS:000240833100002
DA 2024-07-18
ER

PT J
AU Kasao, A
   Miyata, K
AF Kasao, A
   Miyata, K
TI Algorithmic Painter: a NPR method to generate various styles of painting
SO VISUAL COMPUTER
LA English
DT Article
DE nonphotorealistic rendering; painting; brush stroke; image segmentation
AB This paper proposes Algorithmic Painter, an algorithm that can produce various styles of painting from source photos. Algorithmic Painter is created by enhancing Synergistic Image Creator, a painterly rendering method, so that it is highly expressive but still preserves the essential characteristics of the source photo. To achieve this, our method extracts three types of image segments automatically from a source photo by a newly proposed classification method: edge areas, homogeneous areas, and highly contrastive areas. Next, each obtained image segment is converted into a brushstroke. Finally, the target picture is rendered by assigning a color to each pixel. Furthermore, this method can control the curvy shapes of brushstrokes, so that the obtained image can incorporate not only various artistic touches but also natural touches. As an example, the paper describes the composition technique for three considerably different painting styles.
C1 Tokyo Polytech Univ, Dept Design, Fac Arts, Nakano Ku, Tokyo 1648679, Japan.
   Japan Adv Inst Sci & Technol, Ctr Knowledge Sci, Nomi, Ishikawa 9231292, Japan.
C3 Japan Advanced Institute of Science & Technology (JAIST)
RP Tokyo Polytech Univ, Dept Design, Fac Arts, Nakano Ku, 2-19-3 Honchou, Tokyo 1648679, Japan.
EM kasao@dsn.t-kougei.ac.jp; miyatak@acm.org
RI Li, Mengqi/AAG-6804-2021
OI Miyata, Kazunori/0000-0002-1582-0058
CR Curtis C. J., 1997, Proceedings of the 24th annual conference on Computer graphics and interactive techniques, P421
   DeCarlo D, 2002, ACM T GRAPHIC, V21, P769, DOI 10.1145/566570.566650
   GOOCH A, 1998, P 25 ANN C COMP GRAP, P447, DOI DOI 10.1145/280814.280950
   GOOCH B, 2002, P NPAN, P830
   Hertzmann A, 2001, COMP GRAPH, P327, DOI 10.1145/383259.383295
   Hertzmann A, 2000, COMP GRAPH, P517, DOI 10.1145/344779.345074
   Hertzmann A., 1998, Proceedings of the 25th Annual Conference on Computer Graphics and Interactive Techniques, P453
   Kasao A, 1998, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA COMPUTING AND SYSTEMS, PROCEEDINGS, P358, DOI 10.1109/MMCS.1998.693664
   Kasao A., 1999, Systems and Computers in Japan, V30, P13, DOI 10.1002/(SICI)1520-684X(199909)30:10<13::AID-SCJ2>3.0.CO;2-E
   KASAO A, 1998, P 8 INT WORKSH ADV I, P23
   Litwinowicz P., 1997, Proceedings of the 24th annual conference on Computer graphics and interactive techniques, SIGGRAPH '97, P407
   McCorduck P., 1990, Aaron's Code: Meta-Art, Artificial Intelligence and the Work of Harold Cohen, V1st
   MEIER NJ, 1996, P ACM SIGGRAPH, P477
   SALISBURY M, 1996, P SIGGRAPH 96, P461, DOI DOI 10.1145/237170.237286
   SALISBURY MP, 1994, P SIGGRAPH 94, P101
   SALISBURY MP, 1997, P SIGGRAPH 97, P401
   SHIRAISHI M, 2000, P 1 INT S NONPH AN R, P53, DOI DOI 10.1145/340916.340923
   TAKAGI S, 1999, PAC GRAPH 99 C P, P250
   Winkenbach G., 1994, Proceedings of the 21st Annual Conference on Computer Graphics and Interactive Techniques, P91
   Zhang Q, 1999, J VISUAL COMP ANIMAT, V10, P27, DOI 10.1002/(SICI)1099-1778(199901/03)10:1<27::AID-VIS194>3.0.CO;2-C
NR 20
TC 7
Z9 9
U1 0
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2006
VL 22
IS 1
BP 14
EP 27
DI 10.1007/s00371-005-0353-8
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 991NT
UT WOS:000233821400003
DA 2024-07-18
ER

PT J
AU Wang, JP
   Tong, X
   Snyder, J
   Chen, YY
   Guo, BN
   Shum, HY
AF Wang, JP
   Tong, X
   Snyder, J
   Chen, YY
   Guo, BN
   Shum, HY
TI Capturing and rendering geometry details for BTF-mapped surfaces
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 13th Pacific Conference on Computer Graphics and Applications
CY OCT 12-14, 2005
CL Macao, PEOPLES R CHINA
DE bidirectional texture functions; reflectance and shading models;
   rendering; shadow algorithms; texture mapping
ID BIDIRECTIONAL TEXTURE FUNCTIONS; REFLECTANCE
AB Bidirectional texture functions, or BTFs, accurately model reflectance variation at a fine (meso-) scale as a function of lighting and viewing direction. BTFs also capture view-dependent visibility variation, also called masking or parallax, but only within surface contours. Mesostructure detail is neglected at silhouettes, so BTF-mapped objects retain the coarse shape of the underlying model.
   We augment BTF rendering to obtain approximate mesoscale silhouettes. Our new representation, the 4D mesostructure distance function (MDF), tabulates the displacement from a reference frame where a ray first intersects the mesoscale geometry beneath as a function of ray direction and ray position along that reference plane. Given an MDF, the mesostructure silhouette can be rendered with a per-pixel depth peeling process on graphics hardware, while shading and local parallax are handled by the BTF. Our approach allows real-time rendering, handles complex, non-height-field mesostructure, requires that no additional geometry be sent to the rasterizer other than the mesh triangles, is more compact than textured visibility representations used previously, and, for the first time, can be easily measured from physical samples. We also adapt the algorithm to capture detailed shadows cast both by and onto BTF-mapped surfaces. We demonstrate the efficiency of our algorithm on a variety of BTF data, including real data acquired using our BTF-MDF measurement system.
C1 Chinese Acad Sci, Grad Sch, Inst Comp Technol, Beijing 100864, Peoples R China.
   Microsoft Res Asia, Internet Graph Grp, Beijing, Peoples R China.
   Microsoft Res, Reading, Berks, England.
C3 Chinese Academy of Sciences; Institute of Computing Technology, CAS;
   University of Chinese Academy of Sciences, CAS; Microsoft Research Asia;
   Microsoft; Microsoft
RP Chinese Acad Sci, Grad Sch, Inst Comp Technol, Beijing 100864, Peoples R China.
EM e_boris2002@hotmail.com; xtong@microsoft.com; johnsny@microsoft.com;
   yachen@microsoft.com; bainguo@microsoft.com; hshum@microsoft.com
OI Tong, Xin/0000-0001-8788-2453
CR Blinn J., 1978, Proceedings of the 5th annual conference on Computer graphics and interactive techniques-SIGGRAPH'78, V12, P286
   Bouguet JY, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P43, DOI 10.1109/ICCV.1998.710699
   Cook R. L., 1984, Computers & Graphics, V18, P223
   CURLESS B, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P987, DOI 10.1109/ICCV.1995.466772
   Dana KJ, 1999, ACM T GRAPHIC, V18, P1, DOI 10.1145/300776.300778
   Dana KJ, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P460, DOI 10.1109/ICCV.2001.937661
   DIEFENBACH PJ, 1996, THESIS U PENNSYLVANI
   Furukawa R., 2002, EurographicsWorkshop on Rendering, P257
   Han JY, 2003, ACM T GRAPHIC, V22, P741, DOI 10.1145/882262.882341
   Heidrich W, 2000, COMP GRAPH, P455, DOI 10.1145/344779.344984
   Hirche J, 2004, PROC GRAPH INTERF, P153
   KANEKO T, 2001, ICAT2001 11 INT C AR, P205
   Kautz J, 1999, SPRING EUROGRAP, P247
   Koenderink JJ, 1996, J OPT SOC AM A, V13, P452, DOI 10.1364/JOSAA.13.000452
   Liu XG, 2004, IEEE T VIS COMPUT GR, V10, P278, DOI 10.1109/TVCG.2004.1272727
   Liu XG, 2001, COMP GRAPH, P97
   Malzbender T, 2001, COMP GRAPH, P519, DOI 10.1145/383259.383320
   MAMMEN A, 1989, IEEE COMPUT GRAPH, V9, P43, DOI 10.1109/38.31463
   Max N. L., 1988, Visual Computer, V4, P109, DOI 10.1007/BF01905562
   MUELLER G, 2003, P VIS MOD VIS
   POLICARPO F, 2005, ACM SIGGRAPH 2005 S
   RUSHMEIER H, 2001, P EUR
   Sattler M., 2003, Eurographics Symposium on Rendering. 14th Eurographics Workshop on Rendering, P167
   Sloan PP, 2003, ACM T GRAPHIC, V22, P370, DOI 10.1145/882262.882279
   SLOAN PP, 2000, EUR WORKSH REND, P281
   SUYKENS F, 2003, P EUR
   Tong X, 2002, ACM T GRAPHIC, V21, P665, DOI 10.1145/566570.566634
   Vasilescu MAO, 2004, ACM T GRAPHIC, V23, P336, DOI 10.1145/1015706.1015725
   Wang LF, 2003, ACM T GRAPHIC, V22, P334, DOI 10.1145/882262.882272
   WANG X, 2004, EUR S REND
   YAMAUCHI Y, 2003, SIGGRAPH 2003 SKETCH
   Zhang ZY, 2000, IEEE T PATTERN ANAL, V22, P1330, DOI 10.1109/34.888718
NR 32
TC 7
Z9 9
U1 0
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2005
VL 21
IS 8-10
SI SI
BP 559
EP 568
DI 10.1007/s00371-005-0318-y
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 964CX
UT WOS:000231857400008
DA 2024-07-18
ER

PT J
AU Wang, LJ
   Gu, XF
   Mueller, K
   Yau, ST
AF Wang, LJ
   Gu, XF
   Mueller, K
   Yau, ST
TI Uniform texture synthesis and texture mapping using global
   parameterization
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 13th Pacific Conference on Computer Graphics and Applications
CY OCT 12-14, 2005
CL Macao, PEOPLES R CHINA
DE texture synthesis; texture mapping; Riemann surfaces; global conformal
   parameterization
ID IMAGE
AB Texture mapping and texture synthesis are two popular methods for the decoration of surfaces with visual detail. Here, an existing challenge is to preserve, or at least balance, two competing metrics: scale and angle. In this paper we present two methods for this, both based on global conformal parameterization. First, we describe a texture synthesis algorithm for surfaces with arbitrary topology. By using the conformal parameterization, the 3D surface texture synthesis problem can be converted to a 2D image synthesis problem, which is more intuitive, easier, and conceptually simpler. While the conformality of the parameterization naturally preserves the angles of the texture, in this paper we provide a multi-scale technique to also maintain a more uniform area scaling factor. A second novel contribution is to employ the global parameterization to simultaneously preserve orthogonality and size in texture mapping. For this, we show that a conformal factor-driven mass-spring method offers a convenient way to trade off these two qualitative metrics. Our algorithms are simple, efficient and automatic, and they are theoretically sound and universal to general surfaces as well.
C1 SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11794 USA.
   Harvard Univ, Dept Math, Cambridge, MA 02138 USA.
C3 State University of New York (SUNY) System; State University of New York
   (SUNY) Stony Brook; Harvard University
RP SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11794 USA.
EM lujin@cs.sunysb.edu; gu@cs.cunysb.edu; mueller@cs.sunysb.edu;
   yau@math.harvard.edu
OI Gu, Xianfeng David/0000-0001-8226-5851
CR Ashikhmin M., 2001, P 2001 S INT 3D GRAP, P217, DOI DOI 10.1145/364338.364405
   Cohen MF, 2003, ACM T GRAPHIC, V22, P287, DOI 10.1145/882262.882265
   Degener P., 2003, P 12 INT MESH ROUNDT, P201
   DESBRUN M, 2002, P EUR 02 SAARBR GERM, V12, P209
   Dischler JM, 2002, COMPUT GRAPH FORUM, V21, P401, DOI 10.1111/1467-8659.t01-1-00600
   Duchamp T., 1997, Hierarchical computation of PL harmonic embeddings
   Eberly D.H., 2004, GAME PHYS
   Efros A. A., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1033, DOI 10.1109/ICCV.1999.790383
   Efros AA, 2001, COMP GRAPH, P341, DOI 10.1145/383259.383296
   GORTLER SJ, 2004, TR1204 CS HARV U
   GU X, 2003, P 2003 EUR ACM SIGGR, P127
   GU X, 2005, ACM S SOL PHYS MOD C
   Jin M, 2004, IEEE VISUALIZATION 2004, PROCEEEDINGS, P267, DOI 10.1109/VISUAL.2004.75
   Jost J., 2000, Compact Riemann Surfaces, VThird
   Kwatra V, 2003, ACM T GRAPHIC, V22, P277, DOI 10.1145/882262.882264
   LAI YK, 2005, ACM S SOL PHYS MOD C
   Lévy B, 2002, ACM T GRAPHIC, V21, P362, DOI 10.1145/566570.566590
   Liang L, 2001, ACM T GRAPHIC, V20, P127, DOI 10.1145/501786.501787
   NEALEN A, 2003, P EUR S REND, P97
   Neyret F, 1999, COMP GRAPH, P235, DOI 10.1145/311535.311561
   Pinkall U., 1993, Exp. Math., V2, P15, DOI 10.1080/10586458.1993.10504266
   Praun E, 2000, COMP GRAPH, P465, DOI 10.1145/344779.344987
   Sheffer A, 2001, ENG COMPUT-GERMANY, V17, P326, DOI 10.1007/PL00013391
   Soler C, 2002, ACM T GRAPHIC, V21, P673, DOI 10.1145/566570.566635
   Strebel K., 1984, QUADRATIC DIFFERENTI
   Tong X, 2002, ACM T GRAPHIC, V21, P665, DOI 10.1145/566570.566634
   Tonietto L, 2002, SIBGRAPI 2002: XV BRAZILIAN SYMPOSIUM ON COMPUTER GRAPHICS AND IMAGE PROCESSING, PROCEEDINGS, P252, DOI 10.1109/SIBGRA.2002.1167152
   Turk G, 2001, COMP GRAPH, P347, DOI 10.1145/383259.383297
   Wei LY, 2001, COMP GRAPH, P355, DOI 10.1145/383259.383298
   Wei LY, 2000, COMP GRAPH, P479, DOI 10.1145/344779.345009
   XU Y, 2000, MSRTR200032 MICR RES
   Ying LI, 2001, SPRING EUROGRAP, P301
   Yoshizawa S, 2004, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, P200
   [No title captured]
NR 34
TC 7
Z9 13
U1 0
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2005
VL 21
IS 8-10
SI SI
BP 801
EP 810
DI 10.1007/s00371-005-0324-0
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 964CX
UT WOS:000231857400033
DA 2024-07-18
ER

PT J
AU Chu, CW
   Jeon, GS
   Jung, SK
AF Chu, CW
   Jeon, GS
   Jung, SK
TI A hand-held approach to 3D reconstruction using light stripe projections
   onto a cube frame
SO VISUAL COMPUTER
LA English
DT Article
DE calibration-free; dynamic optical triangulation; 3D model acquisition;
   hand-held 3D scanner
ID CAMERA CALIBRATION; VANISHING POINTS
AB The current paper presents a new light-striping approach for reconstructing a 3D model from a real object. The proposed system consists of a light plane projector, camera, and cube frame with LEDs attached. Instead of a strictly controlled camera and light emitters, a freely movable hand-held device is used that enables one to scan self-occluded objects. As in other light-striping systems, the correspondence problem is solved by projecting a light plane onto an object inside a frame. In the proposed system, the 3D coordinates of an illuminated light stripe are obtained using a calibration-free approach or dynamic optical triangulation. Furthermore, the computed 3D point data does not require any registration process because the data is directly measured based on unified world coordinates. Experimental results proved the accuracy of the measurements and consistency of the outcomes without any knowledge of the camera and light source parameters.
C1 Kyungpook Natl Univ, Dept Comp Sci, Taegu 702701, South Korea.
   Elect & Telecommun Res Inst, Digital Contents Res Div, Taejon 305350, South Korea.
C3 Kyungpook National University; Electronics & Telecommunications Research
   Institute - Korea (ETRI)
RP Kyungpook Natl Univ, Dept Comp Sci, 1370 Sangyuk Dong, Taegu 702701, South Korea.
EM cwchu@etri.re.kr; gsjun@vr.knu.ac.kr; skjung@knu.ac.kr
RI Jung, Soon Ki/P-7687-2018
OI Jung, Soon Ki/0000-0003-0239-6785
CR [Anonymous], ELECT BASEL
   Bernardini N, 2002, J NEUROSCI, V22, DOI 10.1523/JNEUROSCI.22-12-j0002.2002
   Bouguet JY, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P43, DOI 10.1109/ICCV.1998.710699
   CAPRILE B, 1990, INT J COMPUT VISION, V4, P127, DOI 10.1007/BF00127813
   Chen CY, 1999, PROCEEDINGS OF THE IEEE SIGNAL PROCESSING WORKSHOP ON HIGHER-ORDER STATISTICS, P30, DOI 10.1109/HOST.1999.778686
   CIPOLLA R, 1999, P BRIT MACH VIS C, P382
   CURLESS B, 2000, COURS NOT SIGGRAPH 1
   Faugeras O., 1993, Three-dimensional computer vision: a geometric viewpoint
   FISHER RB, 1999, P 2 INT C 3D DIG IM, P24
   Guillou E, 2000, VISUAL COMPUT, V16, P396, DOI 10.1007/PL00013394
   Hartley R, 2000, MULTIPLE VIEW GEOMET
   Hoppe Hugues, 1992, P SIGGRAPH 92, P71, DOI DOI 10.1145/133994.134011
   Jokinen O., 1999, P 2 INT C 3D DIG IM, P180
   Levoy M., 2000, DIGITAL MICHELANGELO
   LIEBOWITZ D, 2001, THESIS OXFORD U
   Lorensen William E., 1987, COMPUT GRAPH, P163, DOI DOI 10.1145/37402.37422
   MCIVOR AM, 1999, P 2 INT C 3D DIG IM, P92
   MIYAZAKI D, 2000, P INT C VIRT SYST MU, V6, P138
   Mundy J., 1992, GEOMETRIC INVARIANCE
   POLLEFEYS M, 2002, COURS NOT SIGGRAPH
   PRESS WH, 1992, NUMERICAL RECIPES C, P96
   Rocchini C, 2001, COMPUT GRAPH FORUM, V20, pC299
   ROCCHINI C, 2001, P ICHIM 01 MIL, V1, P265
   TAKATSUKA M, 1999, P COMP VIS PATT REC, V1, P444
   TOMASI C, 1992, INT J COMPUT VISION, V9, P137, DOI 10.1007/BF00129684
   TSAI RY, 1987, IEEE T ROBOTIC AUTOM, V3, P323, DOI 10.1109/JRA.1987.1087109
   2001, CYBERWARE
NR 27
TC 1
Z9 1
U1 0
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2004
VL 20
IS 7
BP 494
EP 506
DI 10.1007/s00371-004-0249-z
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 863IZ
UT WOS:000224556200005
DA 2024-07-18
ER

PT J
AU Hofer, M
   Pottmann, H
   Ravani, B
AF Hofer, M
   Pottmann, H
   Ravani, B
TI From curve design algorithms to the design of rigid body motions
SO VISUAL COMPUTER
LA English
DT Article
DE motion design; motion planning; registration; subdivision algorithm;
   variational design
ID INTERPOLATION; REGISTRATION
AB We discuss the following problem, which arises in computer animation and robot motion planning: given are N positions or keyframes Sigma(t(i)) of a moving body Sigma subset of R-3 at time instances ti. Compute a smooth rigid body motion Sigma(t) that interpolates or approximates the given positions Sigma(t(i)) such that chosen feature points of the moving system run on smooth paths. We present an algorithm that can be considered as a transfer principle from curve design algorithms to motion design. The algorithm relies on known curve design algorithms and on registration techniques from computer vision. We prove that the motion generated in this way is of the same smoothness as the curve design algorithm employed.
C1 Vienna Univ Technol, Geometr Modeling & Ind Geometry Grp, A-1040 Vienna, Austria.
   Univ Calif Davis, Dept Mech & Aeronaut Engn, Davis, CA 95616 USA.
C3 Technische Universitat Wien; University of California System; University
   of California Davis
RP Vienna Univ Technol, Geometr Modeling & Ind Geometry Grp, Wiedner Hauptstr 8-10, A-1040 Vienna, Austria.
EM hofer@geometrie.tuwien.ac.at; pottmann@geometrie.tuwien.ac.at;
   bravani@ucdavis.edu
RI Ravani, Bahram/HSH-5785-2023; Hofer, Michael/F-8863-2012
OI Ravani, Bahram/0000-0002-7616-2407; Hofer, Michael/0000-0002-1969-9574
CR Amato NM, 2002, J COMPUT BIOL, V9, P149, DOI 10.1089/10665270252935395
   [Anonymous], P 2003 S INT 3D GRAP
   Arikan O, 2002, ACM T GRAPHIC, V21, P483, DOI 10.1145/566570.566606
   BARR AH, 1992, COMP GRAPH, V26, P313, DOI 10.1145/142920.134086
   Bayazit OB, 2002, 2002 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION, VOLS I-IV, PROCEEDINGS, P2126, DOI 10.1109/ROBOT.2002.1014854
   Belta C, 2002, IEEE T ROBOTIC AUTOM, V18, P334, DOI 10.1109/TRA.2002.1019463
   Bernardini F, 2002, COMPUT GRAPH FORUM, V21, P149, DOI 10.1111/1467-8659.00574
   BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791
   Blake A., 1998, ACTIVE CONTOURS
   CHEN Y, 1992, IMAGE VISION COMPUT, V10, P145, DOI 10.1016/0262-8856(92)90066-C
   Eggert DW, 1997, MACH VISION APPL, V9, P272, DOI 10.1007/s001380050048
   Fang YC, 1998, COMPUT AIDED DESIGN, V30, P191, DOI 10.1016/S0010-4485(97)00057-2
   Grace, 1990, SPLINE MODELS OBSERV
   HALPERIN D, 1997, HDB DISCRETE COMPUTA, P775
   Hanson AJ, 1998, VISUALIZATION '98, PROCEEDINGS, P375, DOI 10.1109/VISUAL.1998.745326
   Hofer M, 2003, COMPUT AIDED GEOM D, V20, P523, DOI 10.1016/j.cagd.2003.06.006
   Hofer M., 2002, ADV ROBOT KINEMATICS, P235
   HORN BKP, 1987, J OPT SOC AM A, V4, P629, DOI 10.1364/JOSAA.4.000629
   Hsieh CC, 2003, COMPUT AIDED DESIGN, V35, P739, DOI 10.1016/S0010-4485(03)00005-8
   Hyun DE, 2001, NINTH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P50, DOI 10.1109/PCCGA.2001.962857
   JUTTLER B, 1994, COMPUT GRAPH, V18, P315, DOI 10.1016/0097-8493(94)90033-7
   Juttler B, 1996, J MECH DESIGN, V118, P193, DOI 10.1115/1.2826869
   Juttler B, 2002, HDB COMPUTER AIDED G, P723
   Kalisiak M, 2001, J VISUAL COMP ANIMAT, V12, P117, DOI 10.1002/vis.250
   Kobbelt L, 1996, COMPUT AIDED GEOM D, V13, P743, DOI 10.1016/0167-8396(96)00007-6
   Kobbelt L, 1998, ACM T GRAPHIC, V17, P209, DOI 10.1145/293145.293146
   Latombe J., 2001, ROBOT MOTION PLANNIN
   Latombe JC, 1999, INT J ROBOT RES, V18, P1119, DOI 10.1177/02783649922067753
   Marchand É, 2002, VISUAL COMPUT, V18, P1, DOI 10.1007/s003710100122
   Park FC, 1997, ACM T GRAPHIC, V16, P277, DOI 10.1145/256157.256160
   Pottmann H, 2002, 10TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P8, DOI 10.1109/PCCGA.2002.1167835
   POTTMANN H, 2000, MATH SURFACES, V9, P438
   POTTMANN H, 2002, SIMULTANEOUS REGISTR, V34, P265
   RAMAMOORTHI R, 1997, P SIGGRAPH 97 COMPUT, V31, P287
   Roschel O, 1998, COMPUT AIDED DESIGN, V30, P169, DOI 10.1016/S0010-4485(97)00056-0
   Rusinkiewicz S, 2001, THIRD INTERNATIONAL CONFERENCE ON 3-D DIGITAL IMAGING AND MODELING, PROCEEDINGS, P145, DOI 10.1109/IM.2001.924423
   Sharir M., 1997, HDB DISCRETE COMPUTA, P733
   Shoemaker K., 1985, Computer Graphics, V19, P245, DOI 10.1145/325165.325242
   Wallner J, 2004, COMPUT AIDED GEOM D, V21, P3, DOI 10.1016/j.cagd.2003.06.008
   WALLNER J, 2002, 93 VIENN U TECHN I G
   Wallner J., 2001, MATH VISUAL
   WARREN J, 2001, MORGAN KAUFMANN SERI
NR 42
TC 21
Z9 23
U1 0
U2 12
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2004
VL 20
IS 5
BP 279
EP 297
DI 10.1007/S00371-003-0221-3
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 842GL
UT WOS:000222991600001
DA 2024-07-18
ER

PT J
AU Xuan, YX
   Song, C
   Jin, JQ
   Yang, BL
AF Xuan, Yixin
   Song, Chao
   Jin, Jianqiu
   Yang, Bailin
TI CVAE-LAYOUT: automatic furniture layout with constraints
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Conditional variational auto-encoder; Interior design; Automatic layout;
   Deep learning
AB We propose an automatic layout method for indoor scenes that effectively satisfies specific constraints. Our approach involves enhancing the existing scene representation method to accommodate complex constraints, including the precise placement of doors, windows, and user-specified furniture. To achieve this, we construct a conditional vector that encapsulates the necessary constraints. Moreover, our automatically constrained layout approach is implemented by training a conditional variational autoencoder model. Given the constraints and randomly sampled vectors, the decoder module can generate diversified reasonable indoor layout results. Evaluations show that our model outperforms the existing methods. Furthermore, our model exhibits a lower parameter count and faster execution speed compared with the existing approaches.
C1 [Xuan, Yixin] Zhejiang Univ, Polytech Inst, Hangzhou 310015, Zhejiang, Peoples R China.
   [Song, Chao; Jin, Jianqiu; Yang, Bailin] Zhejiang Gongshang Univ, Sch Comp Sci & Technol, Hangzhou 310018, Zhejiang, Peoples R China.
C3 Zhejiang University; Zhejiang Gongshang University
RP Song, C (corresponding author), Zhejiang Gongshang Univ, Sch Comp Sci & Technol, Hangzhou 310018, Zhejiang, Peoples R China.
EM xyx18061932@gmail.com; csong@zjsu.edu.cn; jqjin@zjsu.edu.cn;
   ybl@zjgsu.edu.cn
RI Song, Chao/KBA-4839-2024
OI Song, Chao/0000-0002-9415-4929
FU Innovative Research Group Project of the National Natural Science
   Foundation of China
FX No Statement Available
CR Boski M, 2017, 2017 10TH INTERNATIONAL WORKSHOP ON MULTIDIMENSIONAL (ND) SYSTEMS (NDS)
   Chaillou S., 2019, Archigan: A Generative Stack for Apartment Building Design
   Chaudhuri Siddhartha, 2013, P 26 ANN ACM S USER, P193, DOI [DOI 10.1145/2501988.2502008, 10.1145/2501988.2502008]
   Chen G., 2014, J. Comput.-Aided Des. Comput. Graph, V26, P10
   Cho KYHY, 2014, Arxiv, DOI [arXiv:1406.1078, DOI 10.3115/V1/D14-1179, 10.48550/ARXIV.1406.1078, DOI 10.48550/ARXIV.1406.1078]
   Doersch C, 2021, Arxiv, DOI arXiv:1606.05908
   Fu Q, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130805
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Hensel M, 2017, ADV NEUR IN, V30
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Kingma D. P., 2014, arXiv
   Li J, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073637
   Li MY, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3303766
   Liu M., 2017, J. Comput.-Aided Des. Comput. Graph, V29, P11
   Luo A, 2020, PROC CVPR IEEE, P3753, DOI 10.1109/CVPR42600.2020.00381
   Merrell P, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964982
   Mo KC, 2019, Arxiv, DOI arXiv:1908.00575
   Mo KC, 2019, PROC CVPR IEEE, P909, DOI 10.1109/CVPR.2019.00100
   Kingma DP, 2014, Arxiv, DOI arXiv:1312.6114
   Para W, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6670, DOI 10.1109/ICCV48922.2021.00662
   Parmar G, 2022, Arxiv, DOI arXiv:2104.11222
   Paschalidou D, 2021, ADV NEUR IN, V34
   Ritchie D, 2019, PROC CVPR IEEE, P6175, DOI 10.1109/CVPR.2019.00634
   Sun YL, 2020, VISUAL COMPUT, V36, P2407, DOI 10.1007/s00371-020-01892-8
   Wang C., 2019, J. Comput.-Aided Des. Comput. Graph, V31, P11
   Wang K, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3322941
   Wang K, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201362
   Wang M, 2020, COMPUT VIS MEDIA, V6, P3, DOI 10.1007/s41095-020-0162-z
   Wang XP, 2021, INT CONF 3D VISION, P106, DOI 10.1109/3DV53792.2021.00021
   Xiao YP, 2020, COMPUT VIS MEDIA, V6, P113, DOI 10.1007/s41095-020-0174-8
   Yang BL, 2021, COMPUT GRAPH-UK, V94, P124, DOI 10.1016/j.cag.2020.11.006
   Yang HT, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5610, DOI 10.1109/ICCV48922.2021.00558
   Yi YK, 2010, J ASIAN ARCHIT BUILD, V9, P547, DOI 10.3130/jaabe.9.547
   Zhang S., 2020, arXiv
   Zhang SY, 2019, VISUAL COMPUT, V35, P1157, DOI 10.1007/s00371-019-01691-w
   Zhang ZW, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3381866
NR 36
TC 0
Z9 0
U1 2
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2023 DEC 22
PY 2023
DI 10.1007/s00371-023-03204-2
EA DEC 2023
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DC1N7
UT WOS:001129743300002
DA 2024-07-18
ER

PT J
AU Ciapas, B
   Treigys, P
AF Ciapas, Bernardas
   Treigys, Povilas
TI Automated barcodeless product classifier for food retail self-checkout
   images
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Barcodeless products classification; Deep neural networks; Individual
   retail product, self-checkout; Image analysis workflow
AB Growing popularity of self-service in retail stores and increasing associated shrinkage presents an urgent need for computer-vision-based product recognition in the area of self-checkouts. The article focuses on individual product recognition using automated workflow in images collected from retail store self-checkouts. The interest of this research lies exclusively in the recognition of barcodeless products-ones that present a challenge of being identified quickly and qualitatively at self-checkouts. Image sets representative of retail store product distribution do not exist as of the time of writing to the authors' knowledge. Images collected by exploiting self-checkout events often contain products partially covered by customer body parts, inside semi-transparent plastic bags, or not present in the area of interest. Due to the huge assortment of products that varies between stores and changes frequently, manual image labeling, filtering and long training time are unpractical. The proposed method investigates the need for automated steps to eliminate empty images and eliminate images where product visibility is unsatisfactory. Authors achieved 80.5 +/- 1.2% classification accuracy on a real-world dataset of 194 products using automatic workflow. The ablation studies proved the need for image filtering in both training and inference workflows. The neural network architecture tuned to the self-checkout dataset proved to outperform well-known networks: the suggested architecture's training time is a fraction of ImageNet's best EfficientNet and accuracy is slightly better. The suggested method generalization is proved on comparable products dataset Fruits 360, where 99.6% accuracy was achieved-comparable or better than other authors.
C1 [Ciapas, Bernardas; Treigys, Povilas] Vilnius Univ, Inst Data Sci & Digital Technol, Akademijos Str 4, LT-08663 Vilnius, Lithuania.
C3 Vilnius University
RP Ciapas, B (corresponding author), Vilnius Univ, Inst Data Sci & Digital Technol, Akademijos Str 4, LT-08663 Vilnius, Lithuania.
EM bernardas.ciapas@mif.vu.lt; povilas.treigys@mif.vu.lt
OI Treigys, Povilas/0000-0002-6608-5508
CR Alzubaidi L., 2019, INT C INT SYST DES A, P90
   [Anonymous], 2018, Self-Checkout in Retail: Measuring the Loss
   Baz I, 2016, 2016 IEEE 12TH IMAGE, VIDEO, AND MULTIDIMENSIONAL SIGNAL PROCESSING WORKSHOP (IVMSP)
   Bi XC, 2021, IEEE ACCESS, V9, P67761, DOI 10.1109/ACCESS.2021.3077031
   Chawla NV, 2002, J ARTIF INTELL RES, V16, P321, DOI 10.1613/jair.953
   Chen YQ, 2001, IEEE IMAGE PROC, P34, DOI 10.1109/ICIP.2001.958946
   Chong T., 2016, Semantic Scholar, P1
   Chung DTP, 2019, J PHYS CONF SER, V1327, DOI 10.1088/1742-6596/1327/1/012050
   Ciapas B, 2021, BALT J MOD COMPUT, V9, P35, DOI 10.22364/bjmc.2021.9.1.3
   David OE, 2016, LECT NOTES COMPUT SC, V9887, P20, DOI 10.1007/978-3-319-44781-0_3
   Follmann P, 2018, LECT NOTES COMPUT SC, V11214, P581, DOI 10.1007/978-3-030-01249-6_35
   Frid-Adar M, 2018, NEUROCOMPUTING, V321, P321, DOI 10.1016/j.neucom.2018.09.013
   George M, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P546, DOI 10.1109/ICCVW.2015.77
   George M, 2014, LECT NOTES COMPUT SC, V8690, P440, DOI 10.1007/978-3-319-10605-2_29
   Grand View Research, 2020, Self-checkout systems market size, share and trends analysis report by components (Systems, Services), By Type (Cash Based, Cashless), By Application, By Region, And Segment Forecasts, 2020-2027
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Higa K, 2013, IEEE IMAGE PROC, P2973, DOI 10.1109/ICIP.2013.6738612
   Jund P, 2016, Arxiv, DOI arXiv:1611.05799
   Kemmler M, 2013, PATTERN RECOGN, V46, P3507, DOI 10.1016/j.patcog.2013.06.005
   Khan SS, 2010, LECT NOTES ARTIF INT, V6206, P188
   Kingma D. P., 2014, arXiv
   Krawczyk B, 2012, FED CONF COMPUT SCI, P89
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Liu S, 2016, IEEE MULTIMEDIA, V23, P54, DOI 10.1109/MMUL.2016.19
   Liu YF, 2023, IEEE T GEOSCI REMOTE, V61, DOI 10.1109/TGRS.2023.3267271
   Merler M, 2007, PROC CVPR IEEE, P3634
   Mohammed R, 2020, INT CONF INFORM COMM, P243, DOI 10.1109/ICICS49469.2020.239556
   Muresan H., 2017, arXiv
   NCR, 2019, Self-checkout: a global consumer perspective
   Oltean M., 2021, Fruits 360 dataset: new research directions
   Oquab M, 2014, PROC CVPR IEEE, P1717, DOI 10.1109/CVPR.2014.222
   Rosado L, 2016, IEEE CONF IMAGING SY, P406, DOI 10.1109/IST.2016.7738260
   Santra B, 2019, IMAGE VISION COMPUT, V86, P45, DOI 10.1016/j.imavis.2019.03.005
   Simonyan K., 2014, CORR
   Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220
   Tan MX, 2019, PR MACH LEARN RES, V97
   Wang K, 2020, IEEE ACCESS, V8, P4935, DOI 10.1109/ACCESS.2019.2962572
   Wang Q, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2022.3181062
   Wei X.-S., 2019, arXiv
   Yosinski J, 2014, Arxiv, DOI [arXiv:1411.1792, 10.48550/arXiv.1411.1792]
   Zivkovic Z, 2006, PATTERN RECOGN LETT, V27, P773, DOI 10.1016/j.patrec.2005.11.005
NR 41
TC 0
Z9 0
U1 1
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2023 DEC 20
PY 2023
DI 10.1007/s00371-023-03163-8
EA DEC 2023
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CQ6W3
UT WOS:001126758500001
DA 2024-07-18
ER

PT J
AU Deng, JK
   Xing, D
   Chen, C
   Han, YG
   Zhao, YX
   Chen, JQ
AF Deng, Jiakang
   Xing, De
   Chen, Cheng
   Han, Yongguo
   Zhao, Yanxuan
   Chen, Jianqiang
TI FFANet: dual attention-based flow field-aware network for wall
   identification
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE 3D flow field; Dual attention mechanism; Wall identification; Numerical
   simulation
ID CLASSIFICATION
AB Deep learning-based approaches for understanding and analyzing 3D flow field grids have been extensively studied in recent years due to their importance in exploring the physical mechanisms of flow fields. However, these methods have shown significant success, which fail to fully utilize the available information and robustly accomplish the 3D flow field grid data analysis. Specifically, the limitation of grid information hampers flow field-based methods from analyzing global velocity distance. Conversely, the absence of velocity information poses challenges for grid-based methods to understand local grid structure. To address the aforementioned issues and cater to most downstream tasks, this paper introduces a flow field-aware network (FFANet). The main innovations of FFANet include: (i) constructing a multi-scale feature learning module using the self-attention mechanism to independently learn features of different scales for velocity distribution and grid structure information. This module aims to generate a global feature with enhanced discriminative representation to improve overall performance; (ii) building a co-attention module to adaptively learn the co-matrix between the aforementioned two features, enhancing effective information utilization of the global feature; (iii) proposing a wall identification method based on the classification module of FFANet, which facilitates generating surface cloud maps and streamlining. Experimental results demonstrate the superior performance of FFANet compared to state-of-the-art approaches.
C1 [Deng, Jiakang; Xing, De; Han, Yongguo] Southwest Univ Sci & Technol, Sch Comp Sci & Technol, Mianyang, Peoples R China.
   [Xing, De; Chen, Cheng; Zhao, Yanxuan; Chen, Jianqiang] China Aerodynam Res & Dev Ctr, Computat Aerodynam Inst, Mianyang, Peoples R China.
C3 Southwest University of Science & Technology - China
RP Chen, C (corresponding author), China Aerodynam Res & Dev Ctr, Computat Aerodynam Inst, Mianyang, Peoples R China.
EM closernh@163.com; xingde10@nudt.edu.cn; chencheng@nudt.edu.cn;
   hansir@swust.edu.cn; zzz1014751733@mail.nwpu.edu.cn; chenjq@cardc.cn
FU National Numerical Wind Tunnel Project of China; National Numerical
   Windtunnel project
FX This work is supported by National Numerical Windtunnel project.
CR Bahdanau D, 2016, Arxiv, DOI [arXiv:1409.0473, 10.48550/arXiv.1409.0473]
   Bayoudh K, 2022, VISUAL COMPUT, V38, P2939, DOI 10.1007/s00371-021-02166-7
   Bin Tang., 2018, Int. J. Performab. Eng, V14, P434, DOI [DOI 10.23940/IJPE.18.03.P4.434444, 10.23940/ijpe.18.03.p4.434444]
   Carion Nicolas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P213, DOI 10.1007/978-3-030-58452-8_13
   Chen LF, 2023, VISUAL COMPUT, V39, P5229, DOI 10.1007/s00371-022-02656-2
   CHONG MS, 1990, PHYS FLUIDS A-FLUID, V2, P765, DOI 10.1063/1.857730
   COVER TM, 1967, IEEE T INFORM THEORY, V13, P21, DOI 10.1109/TIT.1967.1053964
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Dai ZH, 2019, Arxiv, DOI [arXiv:1901.02860, DOI 10.48550/ARXIV.1901.02860]
   Deng L, 2019, J VISUAL-JAPAN, V22, P65, DOI 10.1007/s12650-018-0523-1
   Devlin J, 2019, Arxiv, DOI arXiv:1810.04805
   Dong Qiujie, 2024, IEEE Trans Vis Comput Graph, V30, P4349, DOI 10.1109/TVCG.2023.3259044
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Feng YT, 2019, AAAI CONF ARTIF INTE, P8279
   Guo MH, 2021, COMPUT VIS MEDIA, V7, P187, DOI 10.1007/s41095-021-0229-5
   Haim N, 2019, IEEE I CONF COMP VIS, P632, DOI 10.1109/ICCV.2019.00072
   Haller G, 2016, J FLUID MECH, V795, P136, DOI 10.1017/jfm.2016.151
   He K., 2015, ARXIV150201852
   He KM, 2022, PROC CVPR IEEE, P15979, DOI 10.1109/CVPR52688.2022.01553
   Hu SM, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3506694
   JEONG J, 1995, J FLUID MECH, V285, P69, DOI 10.1017/S0022112095000462
   Jiang JD, 2023, ENG APPL ARTIF INTEL, V123, DOI 10.1016/j.engappai.2023.106340
   Jiang WT, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3460474
   Kashefi A., 2023, Physics-informed pointnet: on how many irregular geometries can it solve an inverse problem simultaneously application to linear elasticity
   Kashefi A, 2022, J COMPUT PHYS, V468, DOI 10.1016/j.jcp.2022.111510
   Kashefi A, 2021, PHYS FLUIDS, V33, DOI 10.1063/5.0033376
   Lai X, 2022, PROC CVPR IEEE, P8490, DOI 10.1109/CVPR52688.2022.00831
   Lguensat R, 2018, INT GEOSCI REMOTE SE, P1764, DOI 10.1109/IGARSS.2018.8518411
   Li XZ, 2021, IEEE T VIS COMPUT GR, V27, P4060, DOI 10.1109/TVCG.2020.3001681
   Li XJ, 2022, LECT NOTES COMPUT SC, V13689, P541, DOI 10.1007/978-3-031-19818-2_31
   Li YZ, 2018, ADV NEUR IN, V31
   Liang YQ, 2022, LECT NOTES COMPUT SC, V13663, P37, DOI 10.1007/978-3-031-20062-5_3
   Lin ZH, 2017, Arxiv, DOI [arXiv:1703.03130, DOI 10.48550/ARXIV.1703.03130]
   Loshchilov I, 2019, Arxiv, DOI arXiv:1711.05101
   Lu JS, 2016, ADV NEUR IN, V29
   Makwana P., 2014, Int. J. Eng. Res., V3, P399
   Nozawa N, 2022, VISUAL COMPUT, V38, P1317, DOI 10.1007/s00371-020-02024-y
   Parmar N, 2018, PR MACH LEARN RES, V80
   Paszke A, 2019, ADV NEUR IN, V32
   Qi CR, 2017, ADV NEUR IN, V30
   Qi SH, 2021, DISPLAYS, V69, DOI 10.1016/j.displa.2021.102053
   Qian G., 2022, Adv. Neural Inform. Process. Syst., VVolume 35, P23192
   Raissi M, 2019, J COMPUT PHYS, V378, P686, DOI 10.1016/j.jcp.2018.10.045
   Rezende D.J., 2016, Adv. Neural. Inf. Process. Syst., V29, DOI [10.48550/arXiv.1607.00662, DOI 10.48550/ARXIV.1607.00662]
   Sahillioglu Y, 2020, VISUAL COMPUT, V36, P1705, DOI 10.1007/s00371-019-01760-0
   Santhanam H, 2023, IEEE WINT CONF APPL, P3135, DOI 10.1109/WACV56688.2023.00315
   Sekar V, 2019, PHYS FLUIDS, V31, DOI 10.1063/1.5094943
   Serra M, 2016, CHAOS, V26, DOI 10.1063/1.4951720
   Shatz I, 2006, VISUAL COMPUT, V22, P825, DOI 10.1007/s00371-006-0067-6
   Shi WJ, 2020, PROC CVPR IEEE, P1708, DOI 10.1109/CVPR42600.2020.00178
   Song Mingli, 2020, P IEEE CVF C COMP VI
   Ströfer CM, 2019, COMMUN COMPUT PHYS, V25, P625, DOI 10.4208/cicp.OA-2018-0035
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang YQ, 2021, VISUAL COMPUT, V37, P261, DOI 10.1007/s00371-020-01797-6
   Yildiz ZC, 2020, VISUAL COMPUT, V36, P127, DOI 10.1007/s00371-018-1592-9
   Zhang L, 2014, COMPUT GRAPH FORUM, V33, P282, DOI 10.1111/cgf.12275
   Zhao HS, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P16239, DOI 10.1109/ICCV48922.2021.01595
   Zhou PW, 2023, VISUAL COMPUT, V39, P3235, DOI 10.1007/s00371-023-02966-z
NR 58
TC 0
Z9 0
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2023 DEC 13
PY 2023
DI 10.1007/s00371-023-03176-3
EA DEC 2023
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CE5X2
UT WOS:001123595900001
DA 2024-07-18
ER

PT J
AU Yang, WM
   He, ZY
   Song, Y
   Ma, YL
AF Yang, Wenming
   He, Zhouyan
   Song, Yang
   Ma, Yeling
TI 3D point cloud denoising method based on global feature guidance
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Point cloud; Denoising; Global feature guidance; Further accelerated
   gradient ascent
ID SURFACE RECONSTRUCTION
AB Raw point cloud (PC) data acquired by 3D sensors or reconstruction algorithms inevitably contain noise and outliers, which can seriously impact downstream tasks, such as surface reconstruction and target detection. To address this problem, this paper proposes an innovative denoising method including the adaptive feature extraction (AFE) module, the gradient field estimation (GFE) module, and the further accelerated gradient ascent (FAGA) module. The method is based on considering the noisy PC as a convolutional distribution of clean PC and noise, and denoising is achieved by updating the positions of the points through gradient ascent iterations. Specifically, for a given noisy PC as input, we first extract global features in the AFE module, which are used as conditions to dynamically guide the extraction of local and non-local features to achieve adaptive feature acquisition. Next, these adaptive features are used as input to the GFE module to estimate the gradient field of the PCs and combined with our proposed FAGA module for denoising operations. Extensive qualitative and quantitative experiments are conducted on synthetic and natural PC datasets, and the results show that the proposed method exhibits superior performance relative to previous state-of-the-art methods.
C1 [Yang, Wenming; He, Zhouyan; Song, Yang; Ma, Yeling] Ningbo Univ, Coll Sci & Technol, Ningbo 315000, Peoples R China.
   [Yang, Wenming; He, Zhouyan] Ningbo Univ, Fac Informat Sci & Engn, Ningbo 315211, Peoples R China.
C3 Ningbo University; Ningbo University
RP He, ZY (corresponding author), Ningbo Univ, Coll Sci & Technol, Ningbo 315000, Peoples R China.; He, ZY (corresponding author), Ningbo Univ, Fac Informat Sci & Engn, Ningbo 315211, Peoples R China.
EM hezhouyan@nbu.edu.cn
RI zhouyan, he/GXM-4974-2022; Yang, Wenming/HJH-8634-2023
FU Natural Science Foundation of Zhejiang Province [LQ23F010011]; Natural
   Science Foundation of Zhejiang [2022J136]; Natural Science Foundation of
   Ningbo [ZJ2022130]; Zhejiang Provincial Postdoctoral Research Excellence
   Foundation
FX This research was funded by the Natural Science Foundation of Zhejiang
   under Grant No. LQ23F010011, the Natural Science Foundation of Ningbo
   under Grant No. 2022J136, and Zhejiang Provincial Postdoctoral Research
   Excellence Foundation under Grant No. ZJ2022130.
CR Alexa M, 2001, IEEE VISUAL, P21, DOI 10.1109/VISUAL.2001.964489
   Avron H, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1857907.1857911
   Bowers J, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866188
   Cazals F, 2005, COMPUT AIDED GEOM D, V22, P121, DOI 10.1016/j.cagd.2004.09.004
   Chi P, 2024, VISUAL COMPUT, V40, P2889, DOI 10.1007/s00371-023-02992-x
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Duan CJ, 2019, INT CONF ACOUST SPEE, P8553, DOI [10.1109/icassp.2019.8682812, 10.1109/ICASSP.2019.8682812]
   Fan HQ, 2017, PROC CVPR IEEE, P2463, DOI 10.1109/CVPR.2017.264
   Fleishman S, 2005, ACM T GRAPHIC, V24, P544, DOI 10.1145/1073204.1073227
   Fleishman S, 2003, ACM T GRAPHIC, V22, P950, DOI 10.1145/882262.882368
   Gao JK, 2019, IEEE T INSTRUM MEAS, V68, P4765, DOI 10.1109/TIM.2019.2900962
   Gao X., 2018, P IEEE 4 INT C MULTI, P1
   Guerrero P, 2018, COMPUT GRAPH FORUM, V37, P75, DOI 10.1111/cgf.13343
   Guo YL, 2015, IEEE T INSTRUM MEAS, V64, P683, DOI 10.1109/TIM.2014.2358131
   He YC, 2020, SIAM J IMAGING SCI, V13, P1834, DOI 10.1137/20M1314525
   Hermosilla P, 2019, IEEE I CONF COMP VIS, P52, DOI 10.1109/ICCV.2019.00014
   Hu GF, 2006, VISUAL COMPUT, V22, P147, DOI 10.1007/s00371-006-0372-0
   Hu W, 2022, IEEE T MULTIMEDIA, V24, P3961, DOI 10.1109/TMM.2021.3111440
   Hu W, 2021, IEEE T IMAGE PROCESS, V30, P6168, DOI 10.1109/TIP.2021.3092826
   Hu W, 2020, IEEE T SIGNAL PROCES, V68, P2841, DOI 10.1109/TSP.2020.2978617
   Huang H, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2421636.2421645
   Huang H, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618522
   Leal E, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20113206
   Lipman Y, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1276377.1276405
   Liu Z., 2022, The Visual Computer, P1
   Luo S.T., 2021, P P IEEECVF INT C CO, P4538
   Luo ST, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1330, DOI 10.1145/3394171.3413727
   Mattei E., 2016, P COMPUTER GRAPHICS, P127
   Netto GM, 2022, VISUAL COMPUT, V38, P3217, DOI 10.1007/s00371-022-02525-y
   Öztireli AC, 2009, COMPUT GRAPH FORUM, V28, P493, DOI 10.1111/j.1467-8659.2009.01388.x
   Pistilli Francesca, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12365), P103, DOI 10.1007/978-3-030-58565-5_7
   Qi CR, 2017, ADV NEUR IN, V30
   Rakotosaona MJ, 2020, COMPUT GRAPH FORUM, V39, P185, DOI 10.1111/cgf.13753
   Ravi N, 2020, Arxiv, DOI arXiv:2007.08501
   Schoenenberger Y, 2015, 3DTV CONF
   Serna Andres, 2014, 3rd International Conference on Pattern Recognition Applications and Methods (ICPRAM 2014). Proceedings, P819
   Song S., 2023, Vis Comp, P1
   Song YC, 2022, VISUAL COMPUT, V38, P3253, DOI 10.1007/s00371-022-02569-0
   Sun YJ, 2015, COMPUT AIDED GEOM D, V35-36, P2, DOI 10.1016/j.cagd.2015.03.011
   Wang Y., 2020, IEEE Trans. Instrum. Meas, V70, P1, DOI [10.1109/TIM.2020.3044719, DOI 10.1109/TIM.2020.3044719]
   Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362
   Wu Y, 2023, VISUAL COMPUT, V39, P393, DOI 10.1007/s00371-021-02337-6
   Xie Q., 2020, IEEE Trans. Instrum. Meas, V70, P1
   Xu LL, 2015, GRAPH MODELS, V82, P160, DOI 10.1016/j.gmod.2015.06.012
   Yu LQ, 2018, PROC CVPR IEEE, P2790, DOI 10.1109/CVPR.2018.00295
   Zeng J, 2020, IEEE T IMAGE PROCESS, V29, P3474, DOI 10.1109/TIP.2019.2961429
   Zhao YP, 2022, Arxiv, DOI arXiv:2202.10094
   Zheng YL, 2017, VISUAL COMPUT, V33, P857, DOI 10.1007/s00371-017-1391-8
NR 48
TC 0
Z9 0
U1 1
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2023 DEC 1
PY 2023
DI 10.1007/s00371-023-03158-5
EA DEC 2023
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA Z4NY8
UT WOS:001111873100002
DA 2024-07-18
ER

PT J
AU Muhammad, ZUD
   Huang, ZJ
   Gu, NJ
   Muhammad, U
AF Muhammad, Zaka-Ud-Din
   Huang, Zhangjin
   Gu, Naijie
   Muhammad, Usman
TI DCANet: deep context attention network for automatic polyp segmentation
SO VISUAL COMPUTER
LA English
DT Article
DE Deep learning; Medical image segmentation; Polyp segmentation;
   Colorectal cancer; Multi-attention
ID VALIDATION
AB Automatic and accurate polyp segmentation is significant for diagnosis and treatment of colorectal cancer. This is a challenging task due to the polyp's shape and size diversity. Recently, various deep convolutional neural networks have been developed for polyp segmentation. However, most state-of-the-art methods have suffered from a poor performance in the segmentation of smaller, flat, or noisy polyp objects. In the paper, we propose a novel Deep Context Attention Network (DCANet) for accurate polyp segmentation based on an encoder-decoder framework. ResNet34 is adopted as the encoder, and five functional modules are introduced to improve the performance. Specifically, the improved local context attention module (LCAM) and global context module (GCM) are exploited to efficiently extract the local multi-scale and global multi-receptive-field context information, respectively. Then, an enhanced feature fusion module (FFM) is devised to effectively select and aggregate context features through spatial-channel attention. Finally, equipped with elaborately designed multi-attention modules (MAM), new decoder and supervision blocks are developed to accurately predict polyp regions via powerful channel-spatial-channel attention. Extensive experiments are conducted on the Kvasir-SEG and EndoScene benchmark datasets. The results demonstrate that the proposed network achieves superior performance compared to other state-of-the-art models. The source code will be available at https://github.com/ZAKAUDD/DCANet.
C1 [Muhammad, Zaka-Ud-Din; Huang, Zhangjin; Gu, Naijie] Univ Sci & Technol China, Sch Comp Sci & Technol, Hefei 230027, Peoples R China.
   [Muhammad, Zaka-Ud-Din; Huang, Zhangjin; Gu, Naijie] Anhui Prov Key Lab Software Comp & Commun, Hefei 230027, Peoples R China.
   [Huang, Zhangjin] USTC, Deqing Alpha Innovat Inst, Huzhou 313299, Peoples R China.
   [Muhammad, Usman] Univ Sci & Technol China, Dept Automat, Hefei 230027, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Chinese Academy of Sciences; University of Science &
   Technology of China, CAS; Chinese Academy of Sciences; University of
   Science & Technology of China, CAS
RP Huang, ZJ (corresponding author), Univ Sci & Technol China, Sch Comp Sci & Technol, Hefei 230027, Peoples R China.; Huang, ZJ (corresponding author), Anhui Prov Key Lab Software Comp & Commun, Hefei 230027, Peoples R China.; Huang, ZJ (corresponding author), USTC, Deqing Alpha Innovat Inst, Huzhou 313299, Peoples R China.
EM zaka494@mail.ustc.edu.cn; zhuang@ustc.edu.cn; gunj@ustc.edu.cn;
   usmanraza212@mail.ustc.edu.cn
RI Huang, Zhangjin/I-7929-2014
FU This work was supported in part by the National Natural Science
   Foundation of China (Nos. 71991464/71991460 and 61877056).
   [71991464/71991460, 61877056]; National Natural Science Foundation of
   China
FX This work was supported in part by the National Natural Science
   Foundation of China (Nos. 71991464/71991460 and 61877056).
CR Ahuja S, 2021, APPL INTELL, V51, P571, DOI 10.1007/s10489-020-01826-w
   Akbari M, 2018, IEEE ENG MED BIO, P69, DOI 10.1109/EMBC.2018.8512197
   Angelica B., 2021, Colorectal cancer: symptoms, treatment, risk factors and more
   Arnold M, 2017, GUT, V66, P683, DOI 10.1136/gutjnl-2015-310912
   Asif M, 2021, APPL INTELL, V51, P1959, DOI 10.1007/s10489-020-01923-w
   Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615
   Bernal J, 2012, PATTERN RECOGN, V45, P3166, DOI 10.1016/j.patcog.2012.03.002
   Bernal J, 2017, IEEE T MED IMAGING, V36, P1231, DOI 10.1109/TMI.2017.2664042
   Bernal J, 2015, COMPUT MED IMAG GRAP, V43, P99, DOI 10.1016/j.compmedimag.2015.02.007
   Brandao P, 2017, PROC SPIE, V10134, DOI 10.1117/12.2254361
   Bychkov D, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-018-21758-3
   Chao P, 2019, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2019.00365
   Choudhury A, 2022, APPL INTELL, V52, P7339, DOI 10.1007/s10489-021-02688-6
   Cicek Ozgun, 2016, Medical Image Computing and Computer-Assisted Intervention - MICCAI 2016. 19th International Conference. Proceedings: LNCS 9901, P424, DOI 10.1007/978-3-319-46723-8_49
   Deng-Ping Fan, 2020, Medical Image Computing and Computer Assisted Intervention - MICCAI 2020. 23rd International Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12266), P263, DOI 10.1007/978-3-030-59725-2_26
   Dong B, 2024, Arxiv, DOI [arXiv:2108.06932, DOI 10.48550/ARXIV.2108.06932]
   Fang YQ, 2019, LECT NOTES COMPUT SC, V11764, P302, DOI 10.1007/978-3-030-32239-7_34
   Gloria R, 2020, What is colorectal cancer?, V06
   Guo YB, 2020, J IMAGING, V6, DOI 10.3390/jimaging6070069
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang C.-H., 2021, ARXIV, DOI DOI 10.48550/ARXIV.2101.07172
   Huang HM, 2020, INT CONF ACOUST SPEE, P1055, DOI [10.1109/ICASSP40776.2020.9053405, 10.1109/icassp40776.2020.9053405]
   Jha D, 2021, IEEE ACCESS, V9, P40496, DOI 10.1109/ACCESS.2021.3063716
   Jha D, 2020, COMP MED SY, P558, DOI 10.1109/CBMS49503.2020.00111
   Jha D, 2020, LECT NOTES COMPUT SC, V11962, P451, DOI 10.1007/978-3-030-37734-2_37
   Jha D, 2019, IEEE INT SYM MULTIM, P225, DOI 10.1109/ISM46123.2019.00049
   Ji GP, 2021, LECT NOTES COMPUT SC, V12901, P142, DOI 10.1007/978-3-030-87193-2_14
   Jiafu Zhong, 2020, Medical Image Computing and Computer Assisted Intervention - MICCAI 2020. 23rd International Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12266), P285, DOI 10.1007/978-3-030-59725-2_28
   Jorge B., 2018, P 32 CARS C
   Karkanis SA, 2003, IEEE T INF TECHNOL B, V7, P141, DOI 10.1109/TITB.2003.813794
   Kim T, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P2167, DOI 10.1145/3474085.3475375
   Lai HL, 2023, VISUAL COMPUT, V39, P1453, DOI 10.1007/s00371-022-02422-4
   Leufkens AM, 2012, ENDOSCOPY, V44, P470, DOI 10.1055/s-0031-1291666
   Li QL, 2017, 2017 10TH INTERNATIONAL CONGRESS ON IMAGE AND SIGNAL PROCESSING, BIOMEDICAL ENGINEERING AND INFORMATICS (CISP-BMEI)
   Li S, 2021, arXiv, DOI 10.48550/arXiv.2105.09511
   Li XM, 2018, IEEE T MED IMAGING, V37, P2663, DOI 10.1109/TMI.2018.2845918
   Mahmud T, 2021, COMPUT BIOL MED, V128, DOI 10.1016/j.compbiomed.2020.104119
   Milletari F, 2016, INT CONF 3D VISION, P565, DOI 10.1109/3DV.2016.79
   Min Min., 2019, Scient. Rep, V9, P1
   Mori Y, 2018, ANN INTERN MED, V169, P357, DOI 10.7326/M18-0249
   Murugesan B, 2019, IEEE ENG MED BIO, P7223, DOI [10.1109/EMBC.2019.8857339, 10.1109/embc.2019.8857339]
   Park SY, 2012, IEEE T BIO-MED ENG, V59, P1408, DOI 10.1109/TBME.2012.2188397
   Pogorelov K, 2017, PROCEEDINGS OF THE 8TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'17), P112, DOI 10.1145/3083187.3083189
   Pogorelov K, 2017, MULTIMED TOOLS APPL, V76, P22493, DOI 10.1007/s11042-017-4989-y
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Ruifei Zhang, 2020, Medical Image Computing and Computer Assisted Intervention - MICCAI 2020. 23rd International Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12266), P253, DOI 10.1007/978-3-030-59725-2_25
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474
   Shen YT, 2021, LECT NOTES COMPUT SC, V12901, P559, DOI 10.1007/978-3-030-87193-2_53
   Shin Y, 2018, IEEE ACCESS, V6, P56007, DOI 10.1109/ACCESS.2018.2872717
   Silva J, 2014, INT J COMPUT ASS RAD, V9, P283, DOI 10.1007/s11548-013-0926-3
   Tajbakhsh N, 2016, IEEE T MED IMAGING, V35, P630, DOI 10.1109/TMI.2015.2487997
   Tan-Cong N., 2021, INT C MED IM COMP CO, P633
   Tomar Nikhil Kumar, 2021, Pattern Recognition. ICPR International Workshops and Challenges. Proceedings. Lecture Notes in Computer Science (LNCS 12668), P307, DOI 10.1007/978-3-030-68793-9_23
   Tomar NK, 2023, IEEE T NEUR NET LEAR, V34, P9375, DOI 10.1109/TNNLS.2022.3159394
   Vázquez D, 2017, J HEALTHC ENG, V2017, DOI 10.1155/2017/4037190
   Wang P., 2022, The Visual Computer, P1
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wang Y, 2015, COMPUT METH PROG BIO, V120, P164, DOI 10.1016/j.cmpb.2015.04.002
   Wei J, 2021, LECT NOTES COMPUT SC, V12901, P699, DOI 10.1007/978-3-030-87193-2_66
   Wickstrom K, 2020, MED IMAGE ANAL, V60, DOI 10.1016/j.media.2019.101619
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Wu HS, 2021, AAAI CONF ARTIF INTE, V35, P2916
   Xiao X, 2018, 2018 NINTH INTERNATIONAL CONFERENCE ON INFORMATION TECHNOLOGY IN MEDICINE AND EDUCATION (ITME 2018), P327, DOI 10.1109/ITME.2018.00080
   Yeung M, 2021, COMPUT BIOL MED, V137, DOI 10.1016/j.compbiomed.2021.104815
   Yu Wang, 2019, Pattern Recognition and Computer Vision. Second Chinese Conference, PRCV 2019. Proceedings. Lecture Notes in Computer Science (LNCS 11858), P41, DOI 10.1007/978-3-030-31723-2_4
   Zhang W, 2021, APPL INTELL, V51, P7533, DOI 10.1007/s10489-021-02242-4
   Zhang YD, 2021, Arxiv, DOI arXiv:2102.08005
   Zhang YD, 2021, LECT NOTES COMPUT SC, V12901, P14, DOI 10.1007/978-3-030-87193-2_2
   Zhou ZW, 2020, IEEE T MED IMAGING, V39, P1856, DOI 10.1109/TMI.2019.2959609
   Zhou Zongwei, 2018, Deep Learn Med Image Anal Multimodal Learn Clin Decis Support (2018), V11045, P3, DOI [10.1007/978-3-030-00889-5_1, 10.1007/978-3-030-00689-1_1]
   Zhuang HM, 2023, VISUAL COMPUT, V39, P2207, DOI 10.1007/s00371-021-02322-z
NR 72
TC 1
Z9 1
U1 18
U2 24
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2023
VL 39
IS 11
BP 5513
EP 5525
DI 10.1007/s00371-022-02677-x
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA W5SX6
UT WOS:001092235400021
DA 2024-07-18
ER

PT J
AU Deng, YL
   Yin, LJ
   Gao, XN
   Zhou, H
   Wang, ZZ
   Zou, GF
AF Deng, Yulin
   Yin, Liju
   Gao, Xiaoning
   Zhou, Hui
   Wang, Zhenzhou
   Zou, Guofeng
TI EC-FBNet: embeddable converged front- and back-end network for 3D
   reconstruction in low-light-level environment
SO VISUAL COMPUTER
LA English
DT Article
DE Computer stereo vision; 3D reconstruction; Neural network
AB The implementation of 3D reconstruction for targets in the low-light-level (LLL) environment is an immediate requirement in military, aerospace and other fields related to this environment. However, in such a photon-deficient environment, the amount of available information is extremely limited; thus, leading to the 3D reconstruction task in this environment is challenging. To address this issue, an embeddable converged front- and back-end network (EC-FBNet) is proposed in this paper, it can extract sparse information from the LLL environment by aggregating multi-layer semantic, then according to the similarity of features among object parts, to calculate the global topology structure of the 3D model. For the training approach, the EC-FBNet performs the two-stage integrated training modality. We additionally construct an embedded global inferential attention module (GIAM), to distribute the association weights among the points in the model, and thus reason out the global topology structure of the 3D model. In order to acquire realistic images in the LLL environment, this study leverages the multi-pixel photon counter (MPPC) detector to capture stable photon counting images in this environment, then packages into a dataset for training by the network. In experiment, the proposed approach not only achieves results superior to the state-of-the-art approaches, but also competitive in the quality of the reconstructed model. We believe that this approach can be a useful tool for 3D reconstruction field in the LLL environment.
C1 [Deng, Yulin; Yin, Liju; Gao, Xiaoning; Zhou, Hui; Wang, Zhenzhou; Zou, Guofeng] Shandong Univ Technol, Sch Elect & Elect Engn, Zibo 255000, Shandong, Peoples R China.
C3 Shandong University of Technology
RP Yin, LJ (corresponding author), Shandong Univ Technol, Sch Elect & Elect Engn, Zibo 255000, Shandong, Peoples R China.
EM ljyin2023@163.com
RI Deng, Yulin/KSM-0398-2024
FU This work was supported by the National Natural Science Foundation of
   China (NSFC) (62101310) and Natural Science Foundation of Shandong
   Province, China (ZR2020MF127). [62101310]; National Natural Science
   Foundation of China (NSFC) [ZR2020MF127]; Natural Science Foundation of
   Shandong Province, China
FX This work was supported by the National Natural Science Foundation of
   China (NSFC) (62101310) and Natural Science Foundation of Shandong
   Province, China (ZR2020MF127).
CR Anaya J, 2018, J VIS COMMUN IMAGE R, V51, P144, DOI 10.1016/j.jvcir.2018.01.012
   Nguyen AD, 2019, IEEE I CONF COMP VIS, P8627, DOI 10.1109/ICCV.2019.00872
   Arvind V, 2017, Arxiv, DOI arXiv:1710.01217
   Brock A, 2016, Arxiv, DOI arXiv:1608.04236
   Chen LF, 2023, VISUAL COMPUT, V39, P5229, DOI 10.1007/s00371-022-02656-2
   Cui HN, 2018, INT CONF 3D VISION, P79, DOI 10.1109/3DV.2018.00020
   Doersch C, 2021, Arxiv, DOI arXiv:1606.05908
   Fan HQ, 2017, PROC CVPR IEEE, P2463, DOI 10.1109/CVPR.2017.264
   Fan HH, 2022, IEEE T PATTERN ANAL, V44, P9918, DOI 10.1109/TPAMI.2021.3135117
   Fan MZ, 2023, VISUAL COMPUT, V39, P5085, DOI 10.1007/s00371-022-02647-3
   Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326
   Furuya T, 2023, VISUAL COMPUT, V39, P5167, DOI 10.1007/s00371-022-02652-6
   Gomez-Donoso F, 2017, IEEE IJCNN, P412, DOI 10.1109/IJCNN.2017.7965883
   Groueix T, 2018, PROC CVPR IEEE, P216, DOI 10.1109/CVPR.2018.00030
   Guo MH, 2021, COMPUT VIS MEDIA, V7, P187, DOI 10.1007/s41095-021-0229-5
   Han XG, 2017, IEEE I CONF COMP VIS, P85, DOI 10.1109/ICCV.2017.19
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Huang ZL, 2019, IEEE I CONF COMP VIS, P603, DOI 10.1109/ICCV.2019.00069
   Kingma D. P., 2014, arXiv
   Lai K, 2014, IEEE INT CONF ROBOT, P3050, DOI 10.1109/ICRA.2014.6907298
   Li GQ, 2021, VISUAL COMPUT, V37, P2699, DOI 10.1007/s00371-021-02250-y
   Li Y, 2021, APPL INTELL, V51, P5898, DOI 10.1007/s10489-020-02175-4
   Li ZX, 2022, VISUAL COMPUT, V38, P1589, DOI 10.1007/s00371-021-02090-w
   Liu X, 2023, VISUAL COMPUT, V39, P5027, DOI 10.1007/s00371-022-02644-6
   Liu XP, 2022, VISUAL COMPUT, V38, P669, DOI 10.1007/s00371-020-02042-w
   Loh YP, 2019, COMPUT VIS IMAGE UND, V178, P30, DOI 10.1016/j.cviu.2018.10.010
   Mandikal P, 2019, Arxiv, DOI arXiv:1807.07796
   Mescheder L, 2019, PROC CVPR IEEE, P4455, DOI 10.1109/CVPR.2019.00459
   Iandola FN, 2016, Arxiv, DOI arXiv:1602.07360
   Oktay O, 2018, Arxiv, DOI arXiv:1804.03999
   Peng H, 2020, VISUAL COMPUT, V36, P2227, DOI 10.1007/s00371-020-01908-3
   Pontes JK, 2019, LECT NOTES COMPUT SC, V11361, P365, DOI 10.1007/978-3-030-20887-5_23
   Qi CR, 2017, ADV NEUR IN, V30
   Qiu S, 2022, IEEE T MULTIMEDIA, V24, P1943, DOI 10.1109/TMM.2021.3074240
   Saito S, 2020, PROC CVPR IEEE, P81, DOI 10.1109/CVPR42600.2020.00016
   Schönberger JL, 2016, PROC CVPR IEEE, P4104, DOI 10.1109/CVPR.2016.445
   Shin D, 2018, PROC CVPR IEEE, P3061, DOI 10.1109/CVPR.2018.00323
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Smith Edward J., 2017, P MACH LEARN RES, V78, P87
   Soltani AA, 2017, PROC CVPR IEEE, P2511, DOI 10.1109/CVPR.2017.269
   Sun BB, 2023, VISUAL COMPUT, V39, P6237, DOI 10.1007/s00371-022-02725-6
   Sun XY, 2018, PROC CVPR IEEE, P2974, DOI 10.1109/CVPR.2018.00314
   Tang KK, 2023, VISUAL COMPUT, V39, P5577, DOI 10.1007/s00371-022-02682-0
   Tatarchenko M, 2017, IEEE I CONF COMP VIS, P2107, DOI 10.1109/ICCV.2017.230
   Wang C, 2019, Arxiv, DOI arXiv:1906.01592
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wang X, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19112462
   Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Wu JJ, 2018, LECT NOTES COMPUT SC, V11215, P673, DOI 10.1007/978-3-030-01252-6_40
   Wu ZR, 2015, PROC CVPR IEEE, P1912, DOI 10.1109/CVPR.2015.7298801
   Xiang TG, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P895, DOI 10.1109/ICCV48922.2021.00095
   Xie SN, 2018, PROC CVPR IEEE, P4606, DOI 10.1109/CVPR.2018.00484
   Xu H., 2021, P IEEE CVF INT C COM, P6078
   Xu MT, 2021, AAAI CONF ARTIF INTE, V35, P3056
   Yin Li-ju, 2010, Journal of Nanjing University of Science and Technology, V34, P649
   Zhang XC, 2021, PROC CVPR IEEE, P15885, DOI 10.1109/CVPR46437.2021.01563
NR 58
TC 0
Z9 0
U1 5
U2 7
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2024
VL 40
IS 6
BP 4441
EP 4456
DI 10.1007/s00371-023-03091-7
EA SEP 2023
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TV2X4
UT WOS:001073086400003
DA 2024-07-18
ER

PT J
AU Li, HY
   Liu, JL
   Zhang, XY
   Bai, YZ
   Wang, HY
   Müller, K
AF Li, Heyi
   Liu, Jinlong
   Zhang, Xinyu
   Bai, Yunzhi
   Wang, Huayan
   Mueller, Klaus
TI Transforming the latent space of StyleGAN for real face editing
SO VISUAL COMPUTER
LA English
DT Article
DE Generative adversarial network; Semantic editing; Explainable artificial
   intelligence; Disentanglement; Transformer
AB Despite recent advances in semantic manipulation using StyleGAN, semantic editing of real faces remains challenging. The gap between the W space and the W+ space demands an undesirable trade-off between reconstruction quality and editing quality. To solve this problem, we propose to expand the latent space by replacing fully connected layers in StyleGAN's mapping network with attention-based transformers. This simple and effective technique integrates the two spaces mentioned above and transforms them into one new latent space called W++. Our modified StyleGAN maintains the state-of-the-art generation quality of the original StyleGAN with moderately better diversity. But more importantly, the proposed W++ space achieves superior performance in both reconstruction quality and editing quality. Besides these significant advantages, our W++ space supports existing inversion algorithms and editing methods with only negligible modifications thanks to its structural similarity with the W/W+ space. Extensive experiments on the FFHQ dataset prove that our proposed W++ space is evidently preferable to the previous W/W+ space for real face editing. The code is publicly available for research purposes at https://github.com/AnonSubm2021/TransStyleGAN.
C1 [Li, Heyi] China Acad Space Technol, Inst Remote Sensing Satellites, Beijing 100094, Peoples R China.
   [Liu, Jinlong; Bai, Yunzhi; Wang, Huayan] Kuaishou Technol, Y Tech, Beijing 100085, Peoples R China.
   [Zhang, Xinyu; Mueller, Klaus] SUNY Stony Brook, Dept Comp Sci, Engn Dr, Stony Brook, NY 11749 USA.
C3 State University of New York (SUNY) System; State University of New York
   (SUNY) Stony Brook
RP Li, HY (corresponding author), China Acad Space Technol, Inst Remote Sensing Satellites, Beijing 100094, Peoples R China.
EM hey1li@126.com; ljlwykqh@126.com; zhang146@cs.stonybrook.edu;
   baiyunzhi@kuaishou.com; wanghuayan@kuaishou.com;
   mueller@cs.stonybrook.edu
FU NSF [IIS 1527200, 1941613]
FX AcknowledgementsThis research was partially supported by NSF grants IIS
   1527200 and 1941613. We would like to show our gratitude to Jun Fu,
   Jiayi Liu, Shen Wang, Zhihang Li, and Jie Yang for their early feedback
   and discussions.
CR Abdal R., 2020, P IEEECVF C COMPUTER, P8296
   Abdal R, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3447648
   Abdal R, 2019, IEEE I CONF COMP VIS, P4431, DOI 10.1109/ICCV.2019.00453
   Alaluf Y, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6691, DOI 10.1109/ICCV48922.2021.00664
   Alaluf Yuval, 2022, P IEEE CVF C COMP VI, P18511
   [Anonymous], 2020, ROB GIT REP STYLEGAN
   Collins E, 2020, PROC CVPR IEEE, P5770, DOI 10.1109/CVPR42600.2020.00581
   Deng Y, 2019, IEEE C ELEC DEVICES
   Dosovitskiy A, 2021, arXiv, DOI [10.48550/ARXIV.2010.11929, DOI 10.48550/ARXIV.2010.11929]
   Gao L., 2022, IEEE T VIS COMPUT GR
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Guo YD, 2019, IEEE T PATTERN ANAL, V41, P1294, DOI 10.1109/TPAMI.2018.2837742
   Harkonen E., 2020, Advances in Neural Information Processing Systems, V33, P9841
   Hensel M, 2017, ADV NEUR IN, V30
   Hou XX, 2022, NEURAL NETWORKS, V145, P209, DOI 10.1016/j.neunet.2021.10.017
   Jiapeng Zhu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12362), P592, DOI 10.1007/978-3-030-58520-4_35
   Ju YX, 2021, VISUAL COMPUT, V37, P2907, DOI 10.1007/s00371-021-02198-z
   Karras T, 2021, IEEE T PATTERN ANAL, V43, P4217, DOI 10.1109/TPAMI.2020.2970919
   Karras Tero, 2020, IEEE C COMP VIS PATT
   Kynkäänniemi T, 2019, ADV NEUR IN, V32
   Lin C., 2022, VISUAL COMPUT, P1
   Liu Y, 2020, AAAI CONF ARTIF INTE, V34, P13172
   Mirza M., 2014, ARXIV PREPRINT ARXIV, P2672, DOI 10.48550/arXiv.1411.1784
   Naeem M. F., 2020, INT C MACH LEARN, V119, P7176
   Paysan P, 2009, AVSS: 2009 6TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE, P296, DOI 10.1109/AVSS.2009.58
   Richardson E, 2021, PROC CVPR IEEE, P2287, DOI 10.1109/CVPR46437.2021.00232
   Roich D., 2021, ACM T GRAPHIC
   Rothe R, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P252, DOI 10.1109/ICCVW.2015.41
   Shen YJ, 2021, PROC CVPR IEEE, P1532, DOI 10.1109/CVPR46437.2021.00158
   Shen YJ, 2022, IEEE T PATTERN ANAL, V44, P2004, DOI 10.1109/TPAMI.2020.3034267
   Shen Yujun, 2020, P IEEE CVF C COMP VI, P9243, DOI DOI 10.1109/CVPR42600.2020.00926
   Shi Y., 2022, P IEEECVF C COMPUTER, P11254
   Shoshan A, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14063, DOI 10.1109/ICCV48922.2021.01382
   Tewari A, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417803
   Tewari A, 2020, PROC CVPR IEEE, P6141, DOI 10.1109/CVPR42600.2020.00618
   Tov O, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459838
   Vaswani A, 2017, ADV NEUR IN, V30
   Wu ZZ, 2021, PROC CVPR IEEE, P12858, DOI 10.1109/CVPR46437.2021.01267
   Xia W., 2022, IEEE T PATTERN ANAL, DOI 10.1109/TPAMI.2022.3181070
   Xu YB, 2022, PROC CVPR IEEE, P7673, DOI 10.1109/CVPR52688.2022.00753
   Zhang BW, 2022, PROC CVPR IEEE, P11294, DOI 10.1109/CVPR52688.2022.01102
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhu Pengfei, 2020, ARXIV
NR 43
TC 0
Z9 0
U1 4
U2 10
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2024
VL 40
IS 5
BP 3553
EP 3568
DI 10.1007/s00371-023-03051-1
EA AUG 2023
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OC3D9
UT WOS:001052099500001
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Wang, ZW
   Li, K
   Peng, JJ
AF Wang, Zhiwen
   Li, Kai
   Peng, Jinjia
TI Dynamic context-driven progressive image inpainting with auxiliary
   generative units
SO VISUAL COMPUTER
LA English
DT Article
DE Image inpainting; Face inpainting; Context extraction
AB Image inpainting aims to restore missing or damaged regions of an image with plausible visual content. Most existing methods always face challenges when dealing with large hole images, such as structural distortion and prominent artifacts, which mainly stem from the absence of adequate semantic guidance. To overcome this problem, this paper proposes a novel progressive image inpainting network driven by dynamic context, where the guided restoration dynamic semantic prior not only includes information from the known region but also considers the semantic inference during the filling process. Specifically, a multi-view cooperative strategy is proposed firstly to predict the inherent semantic information by estimating the distributions of the image, grayscale and edge of the masked image. In addition, to cope with potential semantic changes, an auxiliary generative unit is proposed for learning semantic information of the intermediate inpainting results, where the learned intermediate semantics are fused with the intrinsic semantics in a weighted manner to form a real-time updating dynamic context. Moreover, the dynamic semantic prior is propagated to various stages of inpainting to assist in constructing refined feature maps with multiple levels of resolution. Experiments on CelebA-HQ and Paris street view datasets demonstrate that the proposed approach can recover reasonable structures and realistic textures on large-scale masked images, achieving advanced performance.
C1 [Wang, Zhiwen; Li, Kai; Peng, Jinjia] Hebei Univ, Sch Cyber Secur & Comp, Baoding 071000, Peoples R China.
   [Li, Kai; Peng, Jinjia] Hebei Machine Vis Engn Res Ctr, Baoding 071000, Hebei, Peoples R China.
C3 Hebei University
RP Li, K (corresponding author), Hebei Univ, Sch Cyber Secur & Comp, Baoding 071000, Peoples R China.; Li, K (corresponding author), Hebei Machine Vis Engn Res Ctr, Baoding 071000, Hebei, Peoples R China.
EM likai@hbu.cn
OI Li, Kai/0000-0001-6429-8465
FU Natural Science Foundation of Hebei Province [F2022201009]; Hebei
   University High-level Scientific Research Foundation for the
   Introduction of Talent [521100221029]; Science and Technology Project of
   Hebei Education Department [QN2023186]
FX AcknowledgementsThis work is supported in part by the Natural Science
   Foundation of Hebei Province (No. F2022201009), the Hebei University
   High-level Scientific Research Foundation for the Introduction of Talent
   (No. 521100221029) and the Science and Technology Project of Hebei
   Education Department (No. QN2023186).
CR Bertalmio M, 2000, COMP GRAPH, P417, DOI 10.1145/344779.344972
   Chen YT, 2021, VISUAL COMPUT, V37, P1691, DOI 10.1007/s00371-020-01932-3
   Chi B., 2020, NeurIPS, V33, P4479
   DARABI S, 2012, ACM T GRAPHIC, V31, DOI DOI 10.1145/2185520.2185578
   Ding D, 2019, IEEE T IMAGE PROCESS, V28, P1705, DOI 10.1109/TIP.2018.2880681
   Doersch C, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185597
   Dou LY, 2020, J ELECTRON IMAGING, V29, DOI 10.1117/1.JEI.29.4.043026
   Dumoulin V, 2018, Arxiv, DOI [arXiv:1603.07285, DOI 10.48550/ARXIV.1603.07285]
   Esser P, 2021, PROC CVPR IEEE, P12868, DOI 10.1109/CVPR46437.2021.01268
   Guo Q, 2018, IEEE T VIS COMPUT GR, V24, P2023, DOI 10.1109/TVCG.2017.2702738
   Guo XF, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14114, DOI 10.1109/ICCV48922.2021.01387
   Ho J., 2020, Advances in neural information processing systems, V33, P6840
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167
   Huang ZL, 2019, IEEE I CONF COMP VIS, P603, DOI 10.1109/ICCV.2019.00069
   Iizuka S, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073659
   Isogawa M, 2018, IEEE ACCESS, V6, P69728, DOI 10.1109/ACCESS.2018.2877401
   Jingyuan Li, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7757, DOI 10.1109/CVPR42600.2020.00778
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Karras T., 2018, arXiv, DOI [10.48550/arXiv.1710.10196, DOI 10.48550/ARXIV.1710.10196]
   Lahiri Avisek, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13693, DOI 10.1109/CVPR42600.2020.01371
   Li AL, 2023, NEUROCOMPUTING, V520, P376, DOI 10.1016/j.neucom.2022.11.074
   Li HL, 2021, VISUAL COMPUT, V37, P2895, DOI 10.1007/s00371-021-02218-y
   Li KS, 2016, SOFT COMPUT, V20, P885, DOI 10.1007/s00500-014-1547-7
   Li WB, 2022, PROC CVPR IEEE, P10748, DOI 10.1109/CVPR52688.2022.01049
   Li YY, 2023, EXPERT SYST APPL, V228, DOI 10.1016/j.eswa.2023.120456
   Li YY, 2023, J FRANKLIN I, V360, P4172, DOI 10.1016/j.jfranklin.2023.01.041
   Liao L, 2021, PROC CVPR IEEE, P6535, DOI 10.1109/CVPR46437.2021.00647
   Lin J., 2022, ARXIV
   Liu GL, 2018, LECT NOTES COMPUT SC, V11215, P89, DOI 10.1007/978-3-030-01252-6_6
   Liu HY, 2021, PROC CVPR IEEE, P9367, DOI 10.1109/CVPR46437.2021.00925
   Liu Hongyu, 2020, P EUR C COMP VIS, P725, DOI DOI 10.1007/978-3-030-58536-5_43
   Liu W., 2023, arXiv
   Lugmayr A, 2022, PROC CVPR IEEE, P11451, DOI 10.1109/CVPR52688.2022.01117
   Nazeri K., 2019, ARXIV
   Paszke A, 2019, ADV NEUR IN, V32
   Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278
   Peng JL, 2021, PROC CVPR IEEE, P10770, DOI 10.1109/CVPR46437.2021.01063
   Razavi A, 2019, ADV NEUR IN, V32
   Ren YR, 2019, IEEE I CONF COMP VIS, P181, DOI 10.1109/ICCV.2019.00027
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sridevi G, 2019, CIRC SYST SIGNAL PR, V38, P3802, DOI 10.1007/s00034-019-01029-w
   Suvorov R, 2022, IEEE WINT CONF APPL, P3172, DOI 10.1109/WACV51458.2022.00323
   Wadhwa G, 2021, IEEE WINT CONF APPL, P3911, DOI 10.1109/WACV48630.2021.00396
   Wang CR, 2022, LECT NOTES COMPUT SC, V13683, P53, DOI 10.1007/978-3-031-20050-2_4
   Wang D., 2021, ARXIV
   Wang N, 2020, PATTERN RECOGN, V106, DOI 10.1016/j.patcog.2020.107448
   Wang Y, 2018, ADV NEUR IN, V31
   Xie CH, 2019, IEEE I CONF COMP VIS, P8857, DOI 10.1109/ICCV.2019.00895
   Xie YC, 2022, VISUAL COMPUT, V38, P3149, DOI 10.1007/s00371-022-02523-0
   Xiong W, 2019, PROC CVPR IEEE, P5833, DOI 10.1109/CVPR.2019.00599
   Yan ZY, 2018, LECT NOTES COMPUT SC, V11218, P3, DOI 10.1007/978-3-030-01264-9_1
   Yang YZ, 2022, VISUAL COMPUT, V38, P2647, DOI 10.1007/s00371-021-02143-0
   Yi Z., 2020, CVPR, P7508
   Yu JH, 2019, IEEE I CONF COMP VIS, P4470, DOI 10.1109/ICCV.2019.00457
   Yu JH, 2018, PROC CVPR IEEE, P5505, DOI 10.1109/CVPR.2018.00577
   Yu JZ, 2022, NEURAL COMPUT APPL, V34, P9717, DOI 10.1007/s00521-022-06961-8
   Yu YS, 2022, LECT NOTES COMPUT SC, V13676, P242, DOI 10.1007/978-3-031-19787-1_14
   Zeng JX, 2019, ARAB J SCI ENG, V44, P3549, DOI 10.1007/s13369-018-3592-5
   Zeng YH, 2023, IEEE T VIS COMPUT GR, V29, P3266, DOI 10.1109/TVCG.2022.3156949
   Zha ZY, 2020, IEEE T IMAGE PROCESS, V29, P8561, DOI 10.1109/TIP.2020.3015545
   Zhang W, 2021, ARXIV
   Zhang XW, 2022, VISUAL COMPUT, V38, P3205, DOI 10.1007/s00371-022-02532-z
   Zhao L, 2020, PROC CVPR IEEE, P5740, DOI 10.1109/CVPR42600.2020.00578
   Zhao S., 2021, ARXIV
   Zheng CX, 2019, PROC CVPR IEEE, P1438, DOI 10.1109/CVPR.2019.00153
NR 67
TC 0
Z9 0
U1 1
U2 7
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2024
VL 40
IS 5
BP 3457
EP 3472
DI 10.1007/s00371-023-03045-z
EA AUG 2023
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OC3D9
UT WOS:001047709300004
DA 2024-07-18
ER

PT J
AU Okamoto, Y
   Tanaka, M
   Monno, Y
   Okutomi, M
AF Okamoto, Yutaro
   Tanaka, Masayuki
   Monno, Yusuke
   Okutomi, Masatoshi
TI Deep snapshot HDR imaging using multi-exposure color filter array
SO VISUAL COMPUTER
LA English
DT Article
DE High dynamic range (HDR) imaging; Multi-exposure color filter array;
   Demosaicking
ID FUSION; VIDEO; RECONSTRUCTION
AB In this paper, we propose a deep snapshot high dynamic range (HDR) imaging framework that can effectively reconstruct an HDR image from the RAW data captured using a multi-exposure color filter array (ME-CFA), which consists of a mosaic pattern of RGB filters with different exposure levels. To effectively learn the HDR image reconstruction network, we introduce the idea of luminance normalization that simultaneously enables effective loss computation and input data normalization by considering relative local contrasts in the "normalized-by-luminance" HDR domain. This idea enables the network to equally handle the errors in both bright and dark areas regardless of absolute luminance levels, which significantly improves the visual image quality. Experimental results using public HDR image datasets demonstrate that our framework outperforms other snapshot methods and produces high-quality HDR images with fewer visual artifacts, resulting in more than 4dB color peak signal-to-noise ratio improvement in the linear HDR domain.
C1 [Okamoto, Yutaro; Tanaka, Masayuki; Monno, Yusuke; Okutomi, Masatoshi] Tokyo Inst Technol, Sch Engn, Dept Syst & Control Engn, 2-12-1 Ookayama,Meguro Ku, Tokyo 1528550, Japan.
   [Okamoto, Yutaro] Kyocera Corp, Adv Technol Res Inst, Kyoto, Kanagawa, Japan.
C3 Tokyo Institute of Technology; Kyocera
RP Monno, Y (corresponding author), Tokyo Inst Technol, Sch Engn, Dept Syst & Control Engn, 2-12-1 Ookayama,Meguro Ku, Tokyo 1528550, Japan.
EM ymonno@ok.sc.e.titech.ac.jp
RI Monno, Yusuke/W-9098-2019
OI Monno, Yusuke/0000-0001-6733-3406
CR Aguerrebere C, 2014, IEEE INT CONF COMPUT
   Aguerrebere C, 2017, IEEE T COMPUT IMAG, V3, P633, DOI 10.1109/TCI.2017.2704439
   Alghassab M, 2019, 2019 2ND INTERNATIONAL CONFERENCE ON COMPUTER APPLICATIONS & INFORMATION SECURITY (ICCAIS), DOI 10.1109/cais.2019.8769553
   An VG, 2017, ASIAPAC SIGN INFO PR, P1768, DOI 10.1109/APSIPA.2017.8282319
   Bayer B. E., 1976, U.S. patent, Patent No. 3,971,065
   Chen Xiangyu, 2021, P IEEECVF INT C COMP, P4500
   Cheng CH, 2009, 2009 IEEE PACIFIC RIM CONFERENCE ON COMMUNICATIONS, COMPUTERS AND SIGNAL PROCESSING, VOLS 1 AND 2, P648, DOI 10.1109/PACRIM.2009.5291295
   Cheng Z, 2022, LECT NOTES COMPUT SC, V13679, P199, DOI 10.1007/978-3-031-19800-7_12
   Cho H, 2014, COMPUT GRAPH FORUM, V33, P329, DOI 10.1111/cgf.12501
   Choi I, 2017, IEEE T IMAGE PROCESS, V26, P5353, DOI 10.1109/TIP.2017.2731211
   Çogalan U, 2020, IEEE T IMAGE PROCESS, V29, P7511, DOI 10.1109/TIP.2020.3004014
   Cui K, 2018, IEEE IMAGE PROC, P2177, DOI 10.1109/ICIP.2018.8451020
   Debevec P., 1997, P ACM SIGGRAPH, P369, DOI DOI 10.1145/258734.258884
   Eilertsen G, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130816
   Eilertsen G, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818092
   Endo Y, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130834
   Froehlich J, 2014, PROC SPIE, V9023, DOI 10.1117/12.2040003
   Funt B, 2010, COLOR IMAG CONF, P256
   Go C, 2018, ASIAPAC SIGN INFO PR, P1082, DOI 10.23919/APSIPA.2018.8659551
   Grossberg M.D., 2003, P IEEE C COMP VIS PA, P1
   Gu J, 2010, IEEE INT C COMPUTATI, P1
   Hafner D, 2014, INT C PATT RECOG, P2065, DOI 10.1109/ICPR.2014.360
   Hajisharif S, 2015, EURASIP J IMAGE VIDE, P1, DOI 10.1186/s13640-015-0095-0
   Han J, 2020, PROC CVPR IEEE, P1727, DOI 10.1109/CVPR42600.2020.00180
   Hasinoff SW, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980254
   Heide F, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2661229.2661260
   Henz B, 2018, COMPUT GRAPH FORUM, V37, P389, DOI 10.1111/cgf.13370
   Hu J, 2013, PROC CVPR IEEE, P1163, DOI 10.1109/CVPR.2013.154
   Kalantari NK, 2019, COMPUT GRAPH FORUM, V38, P193, DOI 10.1111/cgf.13630
   Kalantari NK, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073609
   Kang HR., 2006, COMPUTATIONAL COLOR, DOI [10.1117/3.660835, DOI 10.1117/3.660835]
   Kim S. Y., 2018, P AS C COMP VIS, P395
   Kingma D. P., 2014, arXiv
   Kokkinos F, 2018, LECT NOTES COMPUT SC, V11218, P317, DOI 10.1007/978-3-030-01264-9_19
   Lee C, 2014, IEEE SIGNAL PROC LET, V21, P1045, DOI 10.1109/LSP.2014.2323404
   Lee S, 2018, LECT NOTES COMPUT SC, V11206, P613, DOI 10.1007/978-3-030-01216-8_37
   Lee S, 2018, IEEE ACCESS, V6, P49913, DOI 10.1109/ACCESS.2018.2868246
   Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151
   Liu YL, 2020, PROC CVPR IEEE, P1648, DOI 10.1109/CVPR42600.2020.00172
   Liu Z, 2022, LECT NOTES COMPUT SC, V13679, P344, DOI 10.1007/978-3-031-19800-7_20
   Ma KD, 2018, IEEE T COMPUT IMAG, V4, P60, DOI 10.1109/TCI.2017.2786138
   Ma KD, 2017, IEEE T IMAGE PROCESS, V26, P2519, DOI 10.1109/TIP.2017.2671921
   MANTIUK R, 2011, ACM T GRAPHIC, V30, P1, DOI DOI 10.1145/2010324.1964935
   Marnerides D, 2018, COMPUT GRAPH FORUM, V37, P37, DOI 10.1111/cgf.13340
   Martel JNP, 2020, IEEE T PATTERN ANAL, V42, P1642, DOI 10.1109/TPAMI.2020.2986944
   Mertens T, 2009, COMPUT GRAPH FORUM, V28, P161, DOI 10.1111/j.1467-8659.2008.01171.x
   Metzler CA, 2020, PROC CVPR IEEE, P1372, DOI 10.1109/CVPR42600.2020.00145
   Monno Y, 2017, SENSORS-BASEL, V17, DOI 10.3390/s17122787
   Moriwaki K., 2018, ARXIV
   Nagahara H., 2018, P IEEE C COMP VIS PA, P1834
   Narasimhan SG, 2005, IEEE T PATTERN ANAL, V27, P518, DOI 10.1109/TPAMI.2005.76
   Nayar S.K., P IEEE C COMP VIS PA
   Niu YZ, 2021, IEEE T IMAGE PROCESS, V30, P3885, DOI 10.1109/TIP.2021.3064433
   Ogino Y, 2016, INT C PATT RECOG, P4208, DOI 10.1109/ICPR.2016.7900294
   Oh TH, 2015, IEEE T PATTERN ANAL, V37, P1219, DOI 10.1109/TPAMI.2014.2361338
   Prabhakar KR, 2019, IEEE INT CONF COMPUT, DOI 10.1109/iccphot.2019.8747329
   Prabhakar KR, 2021, PROC CVPR IEEE, P4873, DOI 10.1109/CVPR46437.2021.00484
   Prabhakar KR, 2017, IEEE I CONF COMP VIS, P4724, DOI 10.1109/ICCV.2017.505
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Rouf M., 2018, P IS T INT S EL IM E, p440:1
   Santos Marcel Santana, 2020, ACM Transactions on Graphics, V39, DOI 10.1145/3386569.3392403
   Sen P, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366222
   Serrano A, 2016, COMPUT GRAPH FORUM, V35, P153, DOI 10.1111/cgf.12819
   Song JW, 2022, LECT NOTES COMPUT SC, V13677, P288, DOI 10.1007/978-3-031-19790-1_18
   Suda T., P AS C COMP VIS ACCV
   Sun QL, 2020, PROC CVPR IEEE, P1383, DOI 10.1109/CVPR42600.2020.00146
   Tocci MD, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964936
   Tursun OT, 2015, COMPUT GRAPH FORUM, V34, P683, DOI 10.1111/cgf.12593
   Uda S., 2016, IPSJ T COMPUT VIS AP, V8, P1
   Vien AG, 2022, LECT NOTES COMPUT SC, V13667, P435, DOI 10.1007/978-3-031-20071-7_26
   Vien AG, 2021, IEEE ACCESS, V9, P70369, DOI 10.1109/ACCESS.2021.3078457
   Wang L., 2021, arXiv
   Wang XZ, 2018, PLATELETS, V29, P48, DOI 10.1080/09537104.2017.1293807
   Wu SZ, 2018, LECT NOTES COMPUT SC, V11206, P120, DOI 10.1007/978-3-030-01216-8_8
   Xu YL, 2022, IEEE T CIRC SYST VID, V32, P4255, DOI 10.1109/TCSVT.2021.3129691
   Yan QS, 2020, IEEE T IMAGE PROCESS, V29, P4308, DOI 10.1109/TIP.2020.2971346
   Yan QS, 2019, PROC CVPR IEEE, P1751, DOI 10.1109/CVPR.2019.00185
   Yan QS, 2019, IEEE WINT CONF APPL, P41, DOI 10.1109/WACV.2019.00012
   Yang X, 2018, PROC CVPR IEEE, P1798, DOI 10.1109/CVPR.2018.00193
   Yu J., 2018, P IEEE C COMPUTER VI
   Zheng Zhuoran, 2021, P IEEE CVF INT C COM, P4449
NR 81
TC 0
Z9 0
U1 3
U2 9
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2024
VL 40
IS 5
BP 3285
EP 3301
DI 10.1007/s00371-023-03032-4
EA AUG 2023
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OC3D9
UT WOS:001042487800002
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Knob, PR
   Pizzol, ND
   Musse, SR
   Pelachaud, C
AF Knob, Paulo Ricardo
   Pizzol, Natalia Dal
   Musse, Soraia Raupp
   Pelachaud, Catherine
TI Arthur and Bella: multi-purpose empathetic AI assistants for daily
   conversations
SO VISUAL COMPUTER
LA English
DT Article
DE Embodied conversational agent; Virtual agents; Empathetic agent
ID PERSONALITY
AB The paper presents a novel approach to developing an embodied conversation agent (ECA) that is capable of displaying empathy toward its human partner during interactions. The virtual agents are equipped with both memory and empathy capabilities, with the main focus being on modeling an empathy model associated with the ECA's memory. The paper presents the proposed model of empathy, as well as its connection with memory, and evaluates how this relationship affects the user's experience (UX) through experiments with volunteers who participated in long and short-term interactions. The results of the experiments show that the association of memory with the empathy model makes interactions with embodied conversational agents more enjoyable and to the user. This suggests that the ECA's ability to display empathy can have a positive impact on the user's experience, which is an important factor to consider when designing conversational agents for various purposes. Overall, the paper presents an interesting and valuable contribution to the field of embodied conversational agents and human-computer interaction. The incorporation of empathy and memory capabilities into an ECA has the potential to improve the user's experience and make interactions with machines more human-like.
C1 [Knob, Paulo Ricardo; Pizzol, Natalia Dal; Musse, Soraia Raupp] Pontificia Univ Catolica Rio Grande do Sul, Porto Alegre, Brazil.
   [Pelachaud, Catherine] CNRS ISIR, Paris, France.
C3 Pontificia Universidade Catolica Do Rio Grande Do Sul
RP Knob, PR (corresponding author), Pontificia Univ Catolica Rio Grande do Sul, Porto Alegre, Brazil.
EM paulo.knob@edu.pucrs.br; natalia.pizzol@edu.pucrs.br;
   soraia.musse@pucrs.br; catherine.pelachaud@upmc.fr
FU CNPq [305084/2016-0]
FX Soraia R. Musse is funded by CNPq (Grant No. 305084/2016-0).
CR [Anonymous], 2002, Proceedings of the 2nd international symposium on Smart graphics, DOI [10.1145/569005.569021, DOI 10.1145/569005.569021]
   Bartneck C, 2009, INT J SOC ROBOT, V1, P71, DOI 10.1007/s12369-008-0001-3
   BURLESON W, 2004, WORKSH SOC EM INT LE
   Castle-Green T., 2020, P 2 C CONV US INT, P1, DOI DOI 10.1145/3405755.3406133
   Chen JZ, 2020, IEEE ACCESS, V8, P183649, DOI 10.1109/ACCESS.2020.3029502
   Conway MA, 2000, PSYCHOL REV, V107, P261, DOI 10.1037//0033-295X.107.2.261
   Coplan AmyPeter Goldie., 2011, Empathy: Philosophical and Psychological Perspectives
   Damasio A., 1994, DESCARTESS ERROR EMO
   de Waal FBM, 2008, ANNU REV PSYCHOL, V59, P279, DOI 10.1146/annurev.psych.59.103006.093625
   de Waal FBM, 2017, NAT REV NEUROSCI, V18, P498, DOI 10.1038/nrn.2017.72
   Edirisinghe M. M. S. N., 2018, 2018 2nd International Conference On Electrical Engineering (EECon), P138, DOI 10.1109/EECon.2018.8540993
   EKMAN P, 1992, COGNITION EMOTION, V6, P169, DOI 10.1080/02699939208411068
   GOLDBERG LR, 1990, J PERS SOC PSYCHOL, V59, P1216, DOI 10.1037/0022-3514.59.6.1216
   Goldstein Arnold., 1985, EMPATHY DEV TRAINING
   Heerink M, 2009, RO MAN 2009 18 IEEE, P528, DOI DOI 10.1109/ROMAN.2009.5326320
   Hojat Mohammadreza., 2007, EMPATHY PATIENT CARE
   Kagan J., 2009, The long shadow of temperament
   Kasap Z, 2012, VISUAL COMPUT, V28, P87, DOI 10.1007/s00371-011-0630-7
   Knob P, 2021, IEEE INT C SEMANT CO, P163, DOI 10.1109/ICSC50631.2021.00036
   Loftus G.R., 2019, Human memory: The processing of information
   Martinez V.R., 2020, P 2 C CONV US INT CU, P1, DOI DOI 10.1145/3405755.3406121
   McCrae RR, 2005, J PERS ASSESS, V84, P261, DOI 10.1207/s15327752jpa8403_05
   Mehrabian A., 1980, BASIC DIMENSIONS GEN
   Melgare JK, 2019, BRAZIL SYMP GAME DIG, P115, DOI 10.1109/SBGames.2019.00025
   MILLER GA, 1956, PSYCHOL REV, V63, P81, DOI 10.1037/h0043158
   Milward D., 2003, P 3 WORKSH KNOWL REA, P9
   MINSKY M, 1991, ARTIF INTELL, V48, P371, DOI 10.1016/0004-3702(91)90036-J
   Moreno R, 2007, EDUC PSYCHOL REV, V19, P309, DOI 10.1007/s10648-007-9047-2
   Morville Peter., 2005, ACM SIGGRAPH 2005 Web program, P10, DOI DOI 10.1145/1187335.1187347
   Pereira A., 2011, L N INST COMP SCI SO, V59, P130, DOI DOI 10.1007/978-3-642-19385-9_17
   Petit M, 2016, IEEE T COGN DEV SYST, V8, P201, DOI 10.1109/TAMD.2015.2507439
   Prendinger H, 2005, APPL ARTIF INTELL, V19, P267, DOI 10.1080/08839510590910174
   Prendinger H, 2005, INT J HUM-COMPUT ST, V62, P231, DOI 10.1016/j.ijhcs.2004.11.009
   RUSSELL JA, 1977, J RES PERS, V11, P273, DOI 10.1016/0092-6566(77)90037-X
   Sajjadi P, 2019, ENTERTAIN COMPUT, V32, DOI 10.1016/j.entcom.2019.100313
   Spitale M., 2020, P 53 HAW INT C SYST, P1
   Spreng RN, 2009, J PERS ASSESS, V91, P62, DOI 10.1080/00223890802484381
   Tapus A., 2007, AAAI Spring Symposium: Multidisciplinary Collaboration for Socially Assistive Robotics, P93
   Vollberg MC, 2021, COGNITION, V209, DOI 10.1016/j.cognition.2020.104558
   Wagner Ullrich, 2015, BMC Psychol, V3, P2, DOI 10.1186/s40359-015-0058-3
   Wang D, 2016, AAMAS'16: PROCEEDINGS OF THE 2016 INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS & MULTIAGENT SYSTEMS, P845
   Yalcin O.N., 2019, ARXIV
   Yalçin ÖN, 2020, COGN SYST RES, V59, P123, DOI 10.1016/j.cogsys.2019.09.016
   Yalcin ÖN, 2018, BIOL INSPIR COGN ARC, V26, P20, DOI 10.1016/j.bica.2018.07.010
NR 44
TC 0
Z9 0
U1 4
U2 13
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2024
VL 40
IS 4
BP 2933
EP 2948
DI 10.1007/s00371-023-02994-9
EA JUL 2023
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA MZ2U3
UT WOS:001026395400001
DA 2024-07-18
ER

PT J
AU An, H
   Lee, W
   Moon, B
AF An, Hyeonjang
   Lee, Wonjun
   Moon, Bochang
TI Adaptively weighted discrete Laplacian for inverse rendering
SO VISUAL COMPUTER
LA English
DT Article
DE Adaptive method; Discrete Laplacian; Inverse rendering; Differentiable
   rendering
AB Reconstructing a triangular mesh from images by a differentiable rendering framework often exploits discrete Laplacians on the mesh, e.g., the cotangent Laplacian, so that a stochastic gradient descent-based optimization in the framework can become stable by a regularization term formed with the Laplacians. However, the stability stemming from using such a regularizer often comes at the cost of over-smoothing a resulting mesh, especially when the Laplacian of the mesh is not properly approximated, e.g., too-noisy or overly-smoothed Laplacian of the mesh. This paper presents a new discrete Laplacian built upon a kernel-weighted Laplacian. We control the kernel weights using a local bandwidth parameter so that the geometry optimization in a differentiable rendering framework can be improved by avoiding blurring high-frequency details of a surface. We demonstrate that our discrete Laplacian with a local adaptivity can improve the quality of reconstructed meshes and convergence speed of the geometry optimization by plugging our discrete Laplacian into recent differentiable rendering frameworks.
C1 [An, Hyeonjang; Lee, Wonjun; Moon, Bochang] Gwangju Inst Sci & Technol, Gwangju, South Korea.
C3 Gwangju Institute of Science & Technology (GIST)
RP Moon, B (corresponding author), Gwangju Inst Sci & Technol, Gwangju, South Korea.
EM bmoon@gist.ac.kr
OI Moon, Bochang/0000-0003-3142-0115
FU Ministry of Culture, Sports and Tourism; Korea Creative Content Agency
   [R2021080001]; Institute of Information amp; communications Technology
   Planning amp; Evaluation (IITP) - Korea government (MSIT) [2022-0-00566]
FX This work was supported in part by Ministry of Culture, Sports and
   Tourism and Korea Creative Content Agency (No. R2021080001) and by
   Institute of Information & communications Technology Planning &
   Evaluation (IITP) grant funded by the Korea government (MSIT) (No.
   2022-0-00566).
CR Belkin M, 2008, PROCEEDINGS OF THE TWENTY-FOURTH ANNUAL SYMPOSIUM ON COMPUTATIONAL GEOMETRY (SGG'08), P278, DOI 10.1145/1377676.1377725
   Community B., 2018, Blender-A 3D Modelling and Rendering Package, Stichting Blender Foundation
   DZIUK G, 1988, LECT NOTES MATH, V1357, P142
   Kazhdan M, 2012, COMPUT GRAPH FORUM, V31, P1745, DOI 10.1111/j.1467-8659.2012.03179.x
   Kingma D. P., 2014, arXiv
   Luan FJ, 2021, COMPUT GRAPH FORUM, V40, P101, DOI 10.1111/cgf.14344
   Nealen A., 2006, P 4 INT C COMP GRAPH, P381, DOI DOI 10.1145/1174429.1174494
   Nicolet B, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3478513.3480501
   Palfinger W, 2022, COMPUT ANIMAT VIRT W, V33, DOI 10.1002/cav.2101
   Pinkall U., 1993, Exp. Math., V2, P15, DOI 10.1080/10586458.1993.10504266
   Sharp N, 2020, COMPUT GRAPH FORUM, V39, P69, DOI 10.1111/cgf.14069
   Wand M.P., 1994, KERNEL SMOOTHING
   Wang NY, 2018, LECT NOTES COMPUT SC, V11215, P55, DOI 10.1007/978-3-030-01252-6_4
   Wardetzky M., 2007, P EUROGRAPHICS S GEO, P33, DOI [DOI 10.2312/SGP/SGP07/033-037, 10.2312/SGP/SGP07/033-037]
NR 14
TC 1
Z9 1
U1 1
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2023
VL 39
IS 8
SI SI
BP 3211
EP 3220
DI 10.1007/s00371-023-02955-2
EA JUN 2023
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA P2DS6
UT WOS:001018118800004
DA 2024-07-18
ER

PT J
AU Wang, YM
   Xu, KX
   Chai, Y
   Jiang, YT
   Qi, GQ
AF Wang, Yiming
   Xu, Kaixiong
   Chai, Yi
   Jiang, Yutao
   Qi, Guanqiu
TI Semantic consistent feature construction and multi-granularity feature
   learning for visible-infrared person re-identification
SO VISUAL COMPUTER
LA English
DT Article
DE Cross-modality; Person re-identification; Multi-granularity; Semantic
   consistent
ID NETWORK
AB In the real-world 24/7 surveillance systems, the images collected during the day and night are visible light images and infrared images, respectively. Infrared images lack color and texture information. In this case, it is more practical to use cross-modality person re-identification (re-ID) to process visible-infrared images. In fact, the cross-modality semantic alignment and specific discriminative feature extraction of different modalities are important for the improvement of modal performance. Therefore, a Semantic Consistent Feature Construction and Multi-granularity Feature learning (SCC-MGL) method is proposed for visible-infrared person re-ID in this paper. The SCC-MGL consists of a Semantic Consistent Feature Construction (SCC) module and a Multi-Granularity Information Enhancement (MGIE) module. In SCC, the features of different modalities are guided by analyzing the relation between feature maps channels and pedestrian's body parts to form consistent semantic information on the corresponding channels, which reduces the impact caused by the misalignment of semantic information. In MGIE, a local modality difference elimination strategy is proposed to remove the modality difference. Meanwhile, the local feature discrimination is improved by reasonably constraining multi-granularity features. The effectiveness and superiority of proposed method are validated by experimental results from SYSU-MM01 and RegDB datasets.
C1 [Wang, Yiming; Xu, Kaixiong; Chai, Yi; Jiang, Yutao] Chongqing Univ, Sch Automat, Chongqing 400044, Peoples R China.
   [Wang, Yiming; Xu, Kaixiong; Chai, Yi; Jiang, Yutao] Chongqing Univ, State Key Lab Power Transmiss Equipment & Syst Sec, Chongqing 400044, Peoples R China.
   [Qi, Guanqiu] State Univ New York Buffalo State, Comp Informat Syst Dept, Buffalo, NY 14222 USA.
C3 Chongqing University; Chongqing University; State University of New York
   (SUNY) System; Buffalo State College
RP Chai, Y (corresponding author), Chongqing Univ, Sch Automat, Chongqing 400044, Peoples R China.; Chai, Y (corresponding author), Chongqing Univ, State Key Lab Power Transmiss Equipment & Syst Sec, Chongqing 400044, Peoples R China.
EM studyinint@126.com; xukaixiong99@gmail.com; chaiyi@cqu.edu.cn;
   20211301019@stu.cqu.edu.cn; qig@buffalostate.edu
RI Xu, Kaixiong/IVV-2304-2023; Qi, Guanqiu/M-8332-2017
OI Xu, Kaixiong/0009-0000-1060-7453; Wang, Yiming/0000-0001-7330-2235;
   Chai, Yi/0000-0002-2717-5981
FU National Natural Science Foundation of China [U2034209]
FX & nbsp;This work was supported by the National Natural Science
   Foundation of China No. U2034209.
CR Chen CQ, 2022, IEEE T IMAGE PROCESS, V31, P2352, DOI 10.1109/TIP.2022.3141868
   Dai PY, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P677
   Dai YX, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11844, DOI 10.1109/ICCV48922.2021.01165
   Ding CX, 2022, IEEE T PATTERN ANAL, V44, P1474, DOI 10.1109/TPAMI.2020.3024900
   Ding SY, 2015, PATTERN RECOGN, V48, P2993, DOI 10.1016/j.patcog.2015.04.005
   Fan X, 2022, VISUAL COMPUT, V38, P279, DOI 10.1007/s00371-020-02015-z
   Fan X, 2019, J VIS COMMUN IMAGE R, V60, P51, DOI 10.1016/j.jvcir.2019.01.010
   Gao YJ, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P5257, DOI 10.1145/3474085.3475643
   Guangcong Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10565, DOI 10.1109/CVPR42600.2020.01058
   He ST, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14993, DOI 10.1109/ICCV48922.2021.01474
   Huang ZP, 2022, AAAI CONF ARTIF INTE, P1034
   Jia ZQ, 2023, VISUAL COMPUT, V39, P1205, DOI 10.1007/s00371-022-02398-1
   Kang JK, 2021, IEEE ACCESS, V9, P12055, DOI 10.1109/ACCESS.2021.3051637
   Kansal K, 2020, IEEE T CIRC SYST VID, V30, P3422, DOI 10.1109/TCSVT.2019.2963721
   Li HF, 2022, KNOWL-BASED SYST, V251, DOI 10.1016/j.knosys.2022.109315
   Li HF, 2022, IEEE T CIRC SYST VID, V32, P2814, DOI 10.1109/TCSVT.2021.3099943
   Li HF, 2021, IEEE T INF FOREN SEC, V16, P1480, DOI 10.1109/TIFS.2020.3036800
   Li HF, 2020, IEEE T IMAGE PROCESS, V29, P7345, DOI 10.1109/TIP.2020.3001424
   Li HF, 2020, IEEE T CIRC SYST VID, V30, P3472, DOI 10.1109/TCSVT.2019.2952550
   Li HF, 2020, PATTERN RECOGN, V106, DOI 10.1016/j.patcog.2020.107414
   Li KF, 2022, KNOWL-BASED SYST, V252, DOI 10.1016/j.knosys.2022.109337
   [李玲莉 Li Lingli], 2021, [重庆大学学报, Journal of Chongqing University], V44, P57
   Li S, 2022, SIMUL MODEL PRACT TH, V119, DOI 10.1016/j.simpat.2022.102568
   Li YY, 2021, J IMAGING, V7, DOI 10.3390/jimaging7040062
   Liang W., 2018, M2M GAN MANY TO MANY
   Liang WQ, 2021, IEEE T IMAGE PROCESS, V30, P6392, DOI 10.1109/TIP.2021.3092578
   Lin M, 2014, 2014 INTERNATIONAL CONFERENCE ON MEDICAL BIOMETRICS (ICMB 2014), P1, DOI 10.1109/ICMB.2014.8
   Ling Y., 2021, P 30 INT JOINT C ART, P845
   Liu HJ, 2021, IEEE T MULTIMEDIA, V23, P4414, DOI 10.1109/TMM.2020.3042080
   Liu HJ, 2021, IEEE SIGNAL PROC LET, V28, P653, DOI 10.1109/LSP.2021.3065903
   Luo H, 2019, IEEE COMPUT SOC CONF, P1487, DOI 10.1109/CVPRW.2019.00190
   Mang Ye, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12362), P229, DOI 10.1007/978-3-030-58520-4_14
   Nguyen DT, 2017, SENSORS-BASEL, V17, DOI 10.3390/s17030605
   Park H, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12026, DOI 10.1109/ICCV48922.2021.01183
   Pu N, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2149, DOI 10.1145/3394171.3413673
   Radenovic F, 2019, IEEE T PATTERN ANAL, V41, P1655, DOI 10.1109/TPAMI.2018.2846566
   ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586
   Seokeon Choi, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10254, DOI 10.1109/CVPR42600.2020.01027
   Sun YF, 2018, LECT NOTES COMPUT SC, V11208, P501, DOI 10.1007/978-3-030-01225-0_30
   Wang G., 2019, ABS190403845 CORR
   Wang GA, 2020, AAAI CONF ARTIF INTE, V34, P12144
   Wang GA, 2019, IEEE I CONF COMP VIS, P3622, DOI 10.1109/ICCV.2019.00372
   Wang GC, 2019, AAAI CONF ARTIF INTE, P8933
   Wang GS, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P274, DOI 10.1145/3240508.3240552
   Wang HZ, 2021, NEUROCOMPUTING, V463, P226, DOI 10.1016/j.neucom.2021.08.053
   Wang SJ, 2023, IEEE T INF FOREN SEC, V18, P147, DOI 10.1109/TIFS.2022.3218449
   Wang SJ, 2022, INFORM SCIENCES, V606, P669, DOI 10.1016/j.ins.2022.05.077
   Wang XQ, 2007, INT J THERM SCI, V46, P1, DOI 10.1016/j.ijthermalsci.2006.06.010
   Wang XJ, 2022, IEEE ACCESS, V10, P30949, DOI 10.1109/ACCESS.2022.3159805
   Wang YM, 2022, IEEE T INF FOREN SEC, V17, P3321, DOI 10.1109/TIFS.2022.3207893
   Wang ZX, 2019, PROC CVPR IEEE, P618, DOI 10.1109/CVPR.2019.00071
   Wei ZY, 2022, IEEE T CYBERNETICS, V52, P10988, DOI 10.1109/TCYB.2022.3183395
   Wei ZY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P225, DOI 10.1109/ICCV48922.2021.00029
   Wei ZY, 2022, IEEE T NEUR NET LEAR, V33, P4676, DOI 10.1109/TNNLS.2021.3059713
   Wen XJ, 2023, VISUAL COMPUT, V39, P4117, DOI 10.1007/s00371-022-02579-y
   Wu AC, 2020, INT J COMPUT VISION, V128, P1765, DOI 10.1007/s11263-019-01290-1
   Wu AC, 2017, IEEE I CONF COMP VIS, P5390, DOI 10.1109/ICCV.2017.575
   Xie JH, 2022, VISUAL COMPUT, V38, P2515, DOI 10.1007/s00371-021-02127-0
   Ye M, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13547, DOI 10.1109/ICCV48922.2021.01331
   Ye M, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1092
   Ye M, 2022, IEEE T PATTERN ANAL, V44, P2872, DOI 10.1109/TPAMI.2021.3054775
   Ye M, 2020, IEEE T INF FOREN SEC, V15, P407, DOI 10.1109/TIFS.2019.2921454
   Zhang L, 2021, IEEE T NEUR NET LEAR, DOI [10.1109/TNNLS.2021.3105143, 10.1109/TCYB.2021.3101880]
   Zhang YF, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P3347, DOI 10.1145/3503161.3548224
   Zhang ZY, 2021, PATTERN RECOGN LETT, V150, P155, DOI 10.1016/j.patrec.2021.07.006
   Zhao CR, 2021, IEEE T IMAGE PROCESS, V30, P4212, DOI 10.1109/TIP.2021.3070182
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zhong C., 2021, J ARTIF INTELL TECHN, V1, P110, DOI DOI 10.37965/JAIT.2020.0058
   Zhong CY, 2021, ALGORITHMS, V14, DOI 10.3390/a14120361
   Zhu XR, 2023, MIN PROC EXT MET REV, V44, P201, DOI 10.1080/08827508.2022.2040498
   Zhu ZQ, 2021, J VIS COMMUN IMAGE R, V80, DOI 10.1016/j.jvcir.2021.103303
   Zhuo JX, 2018, IEEE INT CON MULTI
NR 72
TC 2
Z9 2
U1 5
U2 15
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2024
VL 40
IS 4
BP 2363
EP 2379
DI 10.1007/s00371-023-02923-w
EA JUN 2023
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA MZ2U3
UT WOS:001020988900002
DA 2024-07-18
ER

PT J
AU Yan, G
   Zhengyan, Z
   Zhihua, C
   Chuang, Z
   Jin, Z
AF Yan, Gui
   Zhengyan, Zhang
   Zhihua, Chen
   Chuang, Zhang
   Jin, Zhang
TI CGAN: lightweight and feature aggregation network for high-performance
   interactive image segmentation
SO VISUAL COMPUTER
LA English
DT Article
DE Interactive image segmentation; Conditional generative adversarial
   network; Adversarial learning; Feature aggregation network; Higher-order
   consistency
ID CUT
AB In the task of interactive image segmentation, user interactions about the object of interest are accepted to predict the segmentation mask. Recent works have demonstrated state-of-the-art results by using either backpropagating refinement or iterative training scheme, which are computationally expensive. In this paper, we propose a novel method for interactive image segmentation using conditional generative adversarial networks to enforce higher-order consistency in the segmentation, without extra post-processing during inference. Concretely, we develop a new segmentation network which integrates three different modules by providing global contextual information and attentions and conducting feature fusions across multiple layers. This allows the segmentation network to learn strong object representations and predict more accurate segmentations. We then employ a fully convolutional discriminator to detect and correct higher-order inconsistency between the predictions of the segmentation network and the ground truth label maps. To achieve this, we optimize an objective function that combines the conventional segmentation loss with the adversarial loss of the adversarial term. We train our network on the Pascal VOC 2012 and MS COCO 2017 datasets and conduct comprehensive experiments on four benchmark datasets. Experimental results show that the adversarial training to the network architecture has improved segmentation results over state-of-the-art methods, while making the current system efficient in terms of speed.
C1 [Yan, Gui; Zhengyan, Zhang; Chuang, Zhang; Jin, Zhang] Changsha Univ Sci & Technol, Sch Comp & Commun Engn, Changsha 410114, Hunan, Peoples R China.
   [Yan, Gui; Zhengyan, Zhang; Chuang, Zhang; Jin, Zhang] Changsha Univ Sci & Technol, Hunan Prov Key Lab Intelligent Proc Big Data Trans, Changsha 410114, Hunan, Peoples R China.
   [Zhihua, Chen] East China Univ Sci & Technol, Dept Comp Sci & Engn, Shanghai 200237, Peoples R China.
C3 Changsha University of Science & Technology; Changsha University of
   Science & Technology; East China University of Science & Technology
RP Yan, G (corresponding author), Changsha Univ Sci & Technol, Sch Comp & Commun Engn, Changsha 410114, Hunan, Peoples R China.; Yan, G (corresponding author), Changsha Univ Sci & Technol, Hunan Prov Key Lab Intelligent Proc Big Data Trans, Changsha 410114, Hunan, Peoples R China.
EM guiyan@csust.edu.cn
FU National Natural Science Foundation of China [62272164, 61972056,
   61402053]; Hunan Provincial Natural Science Foundation of China
   [2021JJ30743]; Scientific Research Fund of Education Department of Hunan
   Province [21B0287]
FX AcknowledgementsWe would like to thank the reviewers for their valuable
   comments. This work was supported by the National Natural Science
   Foundation of China (Project Nos. 62272164, 61972056, 61402053), the
   Hunan Provincial Natural Science Foundation of China (Grant No.
   2021JJ30743) and the Scientific Research Fund of Education Department of
   Hunan Province (Grant No. 21B0287).
CR Adachi H, 2019, VISAPP: PROCEEDINGS OF THE 14TH INTERNATIONAL JOINT CONFERENCE ON COMPUTER VISION, IMAGING AND COMPUTER GRAPHICS THEORY AND APPLICATIONS, VOL 4, P139, DOI 10.5220/0007377601390145
   Boykov Y., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P377, DOI 10.1109/ICCV.1999.791245
   Boykov YY, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P105, DOI 10.1109/ICCV.2001.937505
   Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49
   Chen X, 2022, PROC CVPR IEEE, P1290, DOI 10.1109/CVPR52688.2022.00136
   Chen X, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P7325, DOI 10.1109/ICCV48922.2021.00725
   Cheng MM, 2015, COMPUT GRAPH FORUM, V34, P193, DOI 10.1111/cgf.12758
   Ci YZ, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1536, DOI 10.1145/3240508.3240661
   David Acuna, 2018, 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition. Proceedings, P859, DOI 10.1109/CVPR.2018.00096
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Goodfellow I, 2014, ADV NEURAL INFORM PR, P2672
   Grady L, 2006, IEEE T PATTERN ANAL, V28, P1768, DOI 10.1109/TPAMI.2006.233
   Gui Y, 2022, IET IMAGE PROCESS, V16, P2722, DOI 10.1049/ipr2.12520
   Gulshan V, 2010, PROC CVPR IEEE, P3129, DOI 10.1109/CVPR.2010.5540073
   Hao YY, 2021, IEEE INT CONF COMP V, P1551, DOI 10.1109/ICCVW54120.2021.00180
   He XM, 2014, PROC CVPR IEEE, P296, DOI 10.1109/CVPR.2014.45
   Hu Y, 2019, NEURAL NETWORKS, V109, P31, DOI 10.1016/j.neunet.2018.10.009
   Hung WC, 2018, ARXIV PREPRINT ARXIV
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jang WD, 2019, PROC CVPR IEEE, P5292, DOI 10.1109/CVPR.2019.00544
   Kingma D. P., 2014, arXiv
   Kolmogorov V, 2004, IEEE T PATTERN ANAL, V26, P147, DOI 10.1109/TPAMI.2004.1262177
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Li ZW, 2018, PROC CVPR IEEE, P577, DOI 10.1109/CVPR.2018.00067
   Liew JH, 2017, IEEE I CONF COMP VIS, P2746, DOI 10.1109/ICCV.2017.297
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu Q, 2022, LECT NOTES COMPUT SC, V13435, P464, DOI 10.1007/978-3-031-16443-9_45
   Luc P., 2016, ARXIV
   Mahadevan S., 2018, BRIT MACH VIS C 2018, P212
   Majumder S, 2019, PROC CVPR IEEE, P11594, DOI 10.1109/CVPR.2019.01187
   Maninis KK, 2018, PROC CVPR IEEE, P616, DOI 10.1109/CVPR.2018.00071
   McGuinness K, 2010, PATTERN RECOGN, V43, P434, DOI 10.1016/j.patcog.2009.03.008
   Milletari F, 2016, INT CONF 3D VISION, P565, DOI 10.1109/3DV.2016.79
   Mirza M., 2014, ARXIV PREPRINT ARXIV, P2672, DOI 10.48550/arXiv.1411.1784
   Price BL, 2010, PROC CVPR IEEE, P3161, DOI 10.1109/CVPR.2010.5540079
   Reed S, 2016, PR MACH LEARN RES, V48
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720
   Shiyin Zhang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12231, DOI 10.1109/CVPR42600.2020.01225
   Sofiiuk Konstantin, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8620, DOI 10.1109/CVPR42600.2020.00865
   Sofiiuk K, 2022, IEEE IMAGE PROC, P3141, DOI 10.1109/ICIP46576.2022.9897365
   Souly N., 2017, ARXIV
   Wang L, 2022, VISUAL COMPUT, V38, P2009, DOI 10.1007/s00371-021-02262-8
   Xu N, 2016, PROC CVPR IEEE, P373, DOI 10.1109/CVPR.2016.47
   Yoo S, 2019, PROC CVPR IEEE, P11275, DOI 10.1109/CVPR.2019.01154
   Zheng Lin, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13336, DOI 10.1109/CVPR42600.2020.01335
NR 46
TC 2
Z9 2
U1 9
U2 12
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2024
VL 40
IS 3
BP 2203
EP 2217
DI 10.1007/s00371-023-02911-0
EA JUN 2023
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HY8O3
UT WOS:001002137600001
DA 2024-07-18
ER

PT J
AU Li, F
   Fang, HB
   Wang, DZ
   Liu, RX
   Hou, Q
   Xie, BL
AF Li, Fei
   Fang, Hongbo
   Wang, Dengzhun
   Liu, Ruixin
   Hou, Qing
   Xie, Benliang
TI Offline handwritten mathematical expression recognition based on YOLOv5s
SO VISUAL COMPUTER
LA English
DT Article
DE Offline handwritten mathematical expression recognition; Spatial
   attention mechanism; Bidirectional long short-term memory network;
   Clustering; Symbolic relation tree
ID COMPETITION
AB The error accumulation in traditional offline handwritten mathematical expression recognition (OHMER) becomes challenging, because of the two-dimensional structure and writing arbitrariness of offline handwritten mathematical formulas. In this study, an OHMER method based on YOLOv5s was proposed. First, YOLOv5s was used to recognize the symbol category and spatial location information of the expression image. Second, the spatial attention mechanism was introduced in YOLOv5s to enlarge the difference among symbol categories and improve accuracy. Then, a bidirectional long short-term memory network (BiLSTM) was introduced to give the symbols context-related information. Finally, the contextual relevance of the symbols was improved by increasing the number of BiLSTM layers, achieving an accuracy of 95.67%. A mathematical expressions relationship tree was built using the symbol recognition results. Clustering theory was used to analyze the two-dimensional structure of expressions. The recognition accuracy of expressions on the CROHME 2019 Test was 65.47%. The recognition rate of YOLOv5s_SB3CT is second only to that of PAL. However, the recognition rate of YOLOv5_SB3CT is higher than that of PAL when the error is less than three. This finding demonstrates that the proposed model is more fault-tolerant and stable than other models.
C1 [Li, Fei; Fang, Hongbo; Wang, Dengzhun; Liu, Ruixin; Xie, Benliang] Guizhou Univ, Coll Big Data & Informat Engn, Guiyang 550025, Peoples R China.
   [Li, Fei; Xie, Benliang] Minist Educ, Power Semicond Device Reliabil Engn Ctr, Guiyang 550025, Peoples R China.
   [Hou, Qing] Guizhou Commun Ind Serv Co Ltd, Guiyang 550002, Peoples R China.
C3 Guizhou University
RP Xie, BL (corresponding author), Guizhou Univ, Coll Big Data & Informat Engn, Guiyang 550025, Peoples R China.; Xie, BL (corresponding author), Minist Educ, Power Semicond Device Reliabil Engn Ctr, Guiyang 550025, Peoples R China.
EM 2657310043@qq.com; 1583616027@qq.com; 1150771449@qq.com;
   627803670@qq.com; houqing.gz@chinaccs.cn; blxie@gzu.edu.cn
RI Liu, Ruixin/AAB-7916-2020
OI Xie, Benliang/0000-0002-9854-8300
FU National Natural Science Foundation of China [61562009]; Open Fund
   Project in Semiconductor Power Device Reliability Engineering Center of
   Ministry of Education [ERCMEKFJJ2019-06]; Guizhou Provincial Science and
   Technology Projects [[2023]060]; Guizhou Provincial Science and
   Technology Support Plan [[2022]003]
FX This work is partly supported by the National Natural Science Foundation
   of China (No. 61562009), the Open Fund Project in Semiconductor Power
   Device Reliability Engineering Center of Ministry of Education (No.
   ERCMEKFJJ2019-06), Guizhou Provincial Science and Technology Projects
   (No. [2023]060), Guizhou Provincial Science and Technology Support Plan
   (No. [2022]003).
CR Alvaro F, 2016, PATTERN RECOGN, V51, P135, DOI 10.1016/j.patcog.2015.09.013
   Le AD, 2019, PATTERN RECOGN LETT, V128, P255, DOI 10.1016/j.patrec.2019.09.002
   Chan CK, 2020, IEEE ACCESS, V8, P61565, DOI 10.1109/ACCESS.2020.2984627
   Cheng ZM, 2022, VISUAL COMPUT, V38, P749, DOI 10.1007/s00371-021-02075-9
   Choudhary A., 2020, 2020 INT C INN COMP, P527, DOI [10.1007/978-981-15-5148-2_47, DOI 10.1007/978-981-15-5148-2_47]
   Ding L, 2021, KNOWL-BASED SYST, V227, DOI 10.1016/j.knosys.2021.106990
   Ge Z., 2021, ARXIV, DOI DOI 10.48550/ARXIV.2107.08430
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Hirata NST, 2015, PATTERN RECOGN, V48, P837, DOI 10.1016/j.patcog.2014.09.015
   Liang DD, 2020, VISUAL COMPUT, V36, P499, DOI 10.1007/s00371-019-01636-3
   Mahdavi Mahshad, 2019, 2019 International Conference on Document Analysis and Recognition (ICDAR). Proceedings, P1533, DOI 10.1109/ICDAR.2019.00247
   Mouchère H, 2014, INT CONF FRONT HAND, P791, DOI 10.1109/ICFHR.2014.138
   Mouchére H, 2011, PROC INT CONF DOC, P1497, DOI 10.1109/ICDAR.2011.297
   Pambudi S., 2021, Journal of Physics: Conference Series, V1737, DOI 10.1088/1742-6596/1737/1/012022
   Paszke A, 2019, ADV NEUR IN, V32
   Redmon J, 2018, Arxiv, DOI [arXiv:1804.02767, DOI 10.1109/CVPR.2017.690, DOI 10.48550/ARXIV.1804.02767]
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Song YH, 2020, AUTOM CONTROL COMPUT, V54, P179, DOI 10.3103/S0146411620030098
   Truong TN, 2022, LECT NOTES COMPUT SC, V13189, P307, DOI 10.1007/978-3-031-02444-3_23
   Wang YX, 2022, APPL INTELL, V52, P3249, DOI 10.1007/s10489-021-02558-1
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Wu JW, 2019, LECT NOTES ARTIF INT, V11051, P18, DOI 10.1007/978-3-030-10925-7_2
   Xu HY, 2021, FRONT COMPUT SCI-CHI, V15, DOI 10.1007/s11704-020-9366-8
   Xu YJ, 2021, FUTURE GENER COMP SY, V117, P138, DOI 10.1016/j.future.2020.11.005
   Yang C, 2022, PATTERN RECOGN, V132, DOI 10.1016/j.patcog.2022.108910
   Zanibbi R, 2002, IEEE T PATTERN ANAL, V24, P1455, DOI 10.1109/TPAMI.2002.1046157
   Zhang J, 2019, IEEE T MULTIMEDIA, V21, P221, DOI 10.1109/TMM.2018.2844689
   Zhang JS, 2018, INT C PATT RECOG, P2245, DOI 10.1109/ICPR.2018.8546031
NR 28
TC 1
Z9 1
U1 3
U2 7
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2024
VL 40
IS 3
BP 1439
EP 1452
DI 10.1007/s00371-023-02859-1
EA APR 2023
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HY8O3
UT WOS:000974392600001
DA 2024-07-18
ER

PT J
AU Cai, YQ
   Zhou, WJ
   Zhang, LT
   Yu, L
   Luo, T
AF Cai, Yuqi
   Zhou, Wujie
   Zhang, Liting
   Yu, Lu
   Luo, Ting
TI DHFNet: dual-decoding hierarchical fusion network for RGB-thermal
   semantic segmentation
SO VISUAL COMPUTER
LA English
DT Article
DE Deep learning; Semantic segmentation; Thermal images; Feature refinement
AB Recently, red-green-blue (RGB) and thermal (RGB-T) data have attracted considerable interest for semantic segmentation because they provide robust imaging under the complex lighting conditions of urban roads. Most existing RGB-T semantic segmentation methods adopt an encoder-decoder structure, and repeated upsampling causes semantic information loss during decoding. Moreover, using simple cross-modality fusion neither completely mines complementary information from different modalities nor removes noise from the extracted features. To address these problems, we developed a dual-decoding hierarchical fusion network (DHFNet) to extract RGB and thermal information for RGB-T Semantic Segmentation. DHFNet uses a novel two-layer decoder and implements boundary refinement and boundary-guided foreground/background enhancement modules. The modules process features from different levels to achieve the global guidance and local refinement of the segmentation prediction. In addition, an adaptive attention-filtering fusion module filters and extracts complementary information from the RGB and thermal modalities. Further, we introduce a graph convolutional network and an atrous spatial pyramid pooling module to obtain multiscale features and deepen the extracted semantic information. Experimental results on two benchmark datasets showed that the proposed DHFNet performed well relative to state-of-the-art semantic segmentation methods in terms of different evaluation metrics.
C1 [Cai, Yuqi; Zhou, Wujie; Zhang, Liting] Zhejiang Univ Sci & Technol, Sch Informat & Elect Engn, Hangzhou 310023, Zhejiang, Peoples R China.
   [Zhou, Wujie; Yu, Lu] Zhejiang Univ, Coll Informat & Elect Engn, Hangzhou 310027, Zhejiang, Peoples R China.
   [Luo, Ting] Ningbo Univ, Coll Sci & Technol, Ningbo 315211, Zhejiang, Peoples R China.
C3 Zhejiang University of Science & Technology; Zhejiang University; Ningbo
   University
RP Zhou, WJ (corresponding author), Zhejiang Univ Sci & Technol, Sch Informat & Elect Engn, Hangzhou 310023, Zhejiang, Peoples R China.; Zhou, WJ (corresponding author), Zhejiang Univ, Coll Informat & Elect Engn, Hangzhou 310027, Zhejiang, Peoples R China.
EM wujiezhou@163.com
FU National Natural Science Foundation of China [61502429, 61672337,
   61972357]; Zhejiang Provincial Natural Science Foundation of China
   [LY18F020012, LY17F020011]; Zhejiang Key R D Program [2019C03135]
FX This work was supported by National Natural Science Foundation of China
   (61502429, 61672337, 61972357); the Zhejiang Provincial Natural Science
   Foundation of China (LY18F020012, LY17F020011); and Zhejiang Key R & D
   Program (2019C03135).
CR Bochkovskiy A, 2020, ARXIV PREPRINT ARXIV, DOI DOI 10.48550/ARXIV.2004.10934
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen LB, 2017, IEEE INT SYMP NANO, P1, DOI 10.1109/NANOARCH.2017.8053709
   Cui YM, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P8118, DOI 10.1109/ICCV48922.2021.00803
   Dosovitskiy A, 2021, arXiv, DOI [10.48550/ARXIV.2010.11929, DOI 10.48550/ARXIV.2010.11929]
   Gong TT, 2023, ENG APPL ARTIF INTEL, V117, DOI 10.1016/j.engappai.2022.105510
   Ha Q, 2017, IEEE INT C INT ROBOT, P5108, DOI 10.1109/IROS.2017.8206396
   Hazirbas C, 2017, LECT NOTES COMPUT SC, V10111, P213, DOI 10.1007/978-3-319-54181-5_14
   He JJ, 2019, PROC CVPR IEEE, P7511, DOI 10.1109/CVPR.2019.00770
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Hu XX, 2019, IEEE IMAGE PROC, P1440, DOI [10.1109/ICIP.2019.8803025, 10.1109/icip.2019.8803025]
   HUANG G, 2017, PROC CVPR IEEE, P2261, DOI DOI 10.1109/CVPR.2017.243
   Huang ZL, 2019, IEEE I CONF COMP VIS, P603, DOI 10.1109/ICCV.2019.00069
   Jianbo Liu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12371), P1, DOI 10.1007/978-3-030-58574-7_1
   Jin JH, 2023, IEEE GEOSCI REMOTE S, V20, DOI 10.1109/LGRS.2023.3234257
   Li GY, 2023, IEEE T CYBERNETICS, V53, P526, DOI 10.1109/TCYB.2022.3162945
   Liu H., 2022, arXiv
   Liu Z., 2022, ARXIV
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Pohlen T, 2017, PROC CVPR IEEE, P3309, DOI 10.1109/CVPR.2017.353
   Rizzoli G, 2022, TECHNOLOGIES, V10, DOI 10.3390/technologies10040090
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Shivakumar SS, 2020, IEEE INT CONF ROBOT, P9441, DOI [10.1109/icra40945.2020.9196831, 10.1109/ICRA40945.2020.9196831]
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sun YX, 2021, IEEE T AUTOM SCI ENG, V18, P1000, DOI 10.1109/TASE.2020.2993143
   Sun YX, 2019, IEEE ROBOT AUTOM LET, V4, P2576, DOI 10.1109/LRA.2019.2904733
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Wu J., 2023, Digit. Signal Process., V133
   Wu JY, 2023, NEUROCOMPUTING, V527, P119, DOI 10.1016/j.neucom.2023.01.024
   Xu G, 2023, J VIS COMMUN IMAGE R, V90, DOI 10.1016/j.jvcir.2022.103727
   Yan L, 2022, ARXIV
   Yi S, 2022, NEUROCOMPUTING, V482, P236, DOI 10.1016/j.neucom.2021.11.056
   Yu CQ, 2018, LECT NOTES COMPUT SC, V11217, P334, DOI 10.1007/978-3-030-01261-8_20
   Yu CQ, 2018, PROC CVPR IEEE, P1857, DOI 10.1109/CVPR.2018.00199
   Zhang L., 2019, arXiv
   Zhang Q, 2021, PROC CVPR IEEE, P2633, DOI 10.1109/CVPR46437.2021.00266
   Zhou WJ, 2023, IEEE T INSTRUM MEAS, V72, DOI 10.1109/TIM.2023.3237830
   Zhou WJ, 2023, INFORM FUSION, V94, P32, DOI 10.1016/j.inffus.2023.01.016
   Zhou WJ, 2022, IEEE J-STSP, V16, P677, DOI 10.1109/JSTSP.2022.3174338
   Zhou WJ, 2022, IEEE J-STSP, V16, P666, DOI 10.1109/JSTSP.2022.3159032
   Zhou WJ, 2022, NEUROCOMPUTING, V490, P347, DOI 10.1016/j.neucom.2021.11.100
   Zhou WJ, 2022, SCI CHINA INFORM SCI, V65, DOI 10.1007/s11432-020-3337-9
   Zhou WJ, 2021, IEEE T IMAGE PROCESS, V30, P7790, DOI 10.1109/TIP.2021.3109518
   Zhou WJ, 2021, IEEE T NEUR NET LEAR, DOI 10.1109/TNNLS.2021.3105484
   Zhou WJ, 2022, IEEE T CIRC SYST VID, V32, P1224, DOI 10.1109/TCSVT.2021.3077058
   Zhou WJ, 2021, IEEE T SYST MAN CY-S, V51, P3641, DOI 10.1109/TSMC.2019.2957386
   Zhou WJ, 2018, IEEE T IMAGE PROCESS, V27, P2086, DOI 10.1109/TIP.2018.2794207
NR 51
TC 17
Z9 17
U1 17
U2 71
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2024
VL 40
IS 1
BP 169
EP 179
DI 10.1007/s00371-023-02773-6
EA JAN 2023
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IX5B8
UT WOS:000920605000001
DA 2024-07-18
ER

PT J
AU Ali, A
   Baghel, VS
   Prakash, S
AF Ali, Afeeza
   Baghel, Vivek Singh
   Prakash, Surya
TI A novel technique for fingerprint template security in biometric
   authentication systems
SO VISUAL COMPUTER
LA English
DT Article
DE Biometrics; Minutiae triplets; Fingerprint template security; Cancelable
   biometrics; Information hiding
ID PRIVACY; PROTECTION; CODE
AB The deployment of biometrics spans over almost every application that incorporates user authentication. Fingerprint biometrics in particular is by far the most popular choice in biometric authentication systems. However, vulnerabilities such as database compromise, fingerprint reconstruction, and identity fraud have made it crucial to develop highly robust and privacy-preserving authentication systems. Although there have been considerable efforts in the area, security gaps such as dependency on singular point, scale variability and low user recognition performance limit the real-time application of these techniques. In this paper, we address these performance issues and put forth a novel technique for fingerprint template security. The secure templates generated by the proposed technique are computed using many-to-one mapped fully quantized features. A set of rotation and translation independent features are derived using minutiae triplets which makes the proposed secure fingerprint template an alignment-free user template. Each feature type is rotation and translation invariant making the templates free for scale anomalies. The technique essentially hides the true feature information of a fingerprint with the help of a non-invertible transformation function. This enables user identity verification in the transformed domain itself. An extensive result analysis using five fingerprint databases obtained from FVC2002 and FVC2004 has been carried out. The proposed technique delivers a superior performance compared to other noteworthy contributions and therefore proves to be fit for use in various applications involving fingerprint-based identity verification mechanisms.
C1 [Ali, Afeeza; Baghel, Vivek Singh; Prakash, Surya] Indian Inst Technol Indore, Dept Comp Sci & Engn, Indore 453552, India.
C3 Indian Institute of Technology System (IIT System); Indian Institute of
   Technology (IIT) - Indore
RP Ali, A (corresponding author), Indian Inst Technol Indore, Dept Comp Sci & Engn, Indore 453552, India.
EM afeeza1.aa@gmail.com; phd1801201005@iiti.ac.in; surya@iiti.ac.in
RI Baghel, Vivek Singh/HKF-5192-2023
OI Baghel, Vivek Singh/0000-0002-6076-7831
CR Abdellatef E, 2020, VISUAL COMPUT, V36, P1097, DOI 10.1007/s00371-019-01715-5
   Ahmad T, 2011, PATTERN RECOGN, V44, P2555, DOI 10.1016/j.patcog.2011.03.015
   Ali SS, 2021, IEEE T EMERG TOP COM, V9, P612, DOI 10.1109/TETC.2019.2915288
   Ali SS, 2020, IMAGE VISION COMPUT, V104, DOI 10.1016/j.imavis.2020.104004
   Ali SS, 2020, PATTERN RECOGN LETT, V129, P263, DOI 10.1016/j.patrec.2019.11.037
   Ali SS, 2019, PATTERN RECOGN LETT, V126, P68, DOI 10.1016/j.patrec.2018.04.017
   Ali SS, 2018, IET BIOMETRICS, V7, P536, DOI 10.1049/iet-bmt.2018.5070
   [Anonymous], 2006, FINGERPRINT VERIFICA
   [Anonymous], 2021, FINGERPRINT VERIFICA
   Bedari A, 2022, IEEE T IND INFORM, V18, P2745, DOI 10.1109/TII.2021.3101208
   Chen FL, 2009, IEEE T IMAGE PROCESS, V18, P1665, DOI 10.1109/TIP.2009.2017995
   Chen YZ, 2019, SIGNAL PROCESS, V154, P314, DOI 10.1016/j.sigpro.2018.09.013
   Dong XB, 2019, INT CONF BIOMETR THE, DOI 10.1109/btas46853.2019.9185997
   Dwivedi R, 2020, J AMB INTEL HUM COMP, V11, P1495, DOI 10.1007/s12652-019-01437-5
   El-Hameed HAA., 2021, Vis. Comput, V6, P1
   Feng JJ, 2011, IEEE T PATTERN ANAL, V33, P209, DOI 10.1109/TPAMI.2010.77
   Ferrara M., 2014, IEEE INT JOINT C BIO, P1, DOI [DOI 10.1163/9789004269347_002, DOI 10.3389/FPHYS.2014.00266]
   Ferrara M, 2012, IEEE T INF FOREN SEC, V7, P1727, DOI 10.1109/TIFS.2012.2215326
   Gupta K, 2021, VISUAL COMPUT, V37, P1401, DOI 10.1007/s00371-020-01873-x
   Jiang WG, 2021, J AMB INTEL HUM COMP, DOI 10.1007/s12652-020-02811-4
   Jin Z, 2014, PATTERN RECOGN LETT, V42, P137, DOI 10.1016/j.patrec.2014.02.011
   Jin Z, 2012, EXPERT SYST APPL, V39, P6157, DOI 10.1016/j.eswa.2011.11.091
   Joseph T, 2021, J AMB INTEL HUM COMP, V12, P6141, DOI 10.1007/s12652-020-02184-8
   Kumari P, 2021, J AMB INTEL HUM COMP, V12, P10321, DOI 10.1007/s12652-020-02814-1
   Lahmidi A, 2022, EURASIP J INF SECUR, V2022, DOI 10.1186/s13635-022-00129-6
   Lahmidi A, 2022, COMPUT J, V65, P2741, DOI 10.1093/comjnl/bxab111
   Li X, 2016, J AMB INTEL HUM COMP, V7, P427, DOI 10.1007/s12652-015-0338-z
   Moujahdi C, 2014, PATTERN RECOGN LETT, V45, P189, DOI 10.1016/j.patrec.2014.04.001
   Nandakumar K, 2007, IEEE T INF FOREN SEC, V2, P744, DOI 10.1109/TIFS.2007.908165
   Patel VM, 2015, IEEE SIGNAL PROC MAG, V32, P54, DOI 10.1109/MSP.2015.2434151
   Paul PP, 2014, VISUAL COMPUT, V30, P1059, DOI 10.1007/s00371-013-0907-0
   Ratha NK, 2001, IBM SYST J, V40, P614, DOI 10.1147/sj.403.0614
   Rathgeb C, 2011, EURASIP J INF SECUR, DOI 10.1186/1687-417X-2011-3
   Ross A, 2007, IEEE T PATTERN ANAL, V29, P544, DOI 10.1109/TPAMI.2007.1018
   Ryabko BY, 2004, J STAT PLAN INFER, V123, P365, DOI 10.1016/S0378-3758(03)00149-6
   Sandhya M, 2023, VISUAL COMPUT, V39, P1571, DOI 10.1007/s00371-022-02429-x
   Sandhya M, 2017, IET BIOMETRICS, V6, P173, DOI 10.1049/iet-bmt.2016.0008
   Sandhya M, 2016, IET BIOMETRICS, V5, P131, DOI 10.1049/iet-bmt.2015.0034
   Sandhya M, 2015, INT CONF BIOMETR, P386, DOI 10.1109/ICB.2015.7139100
   Trivedi AK, 2020, COMPUT SECUR, V90, DOI 10.1016/j.cose.2019.101690
   Ushmaev O., 2011, Pattern Recognition and Image Analysis, V21, P754, DOI 10.1134/S1054661811040183
   Vaidya SP, 2023, VISUAL COMPUT, V39, P2245, DOI 10.1007/s00371-022-02406-4
   Wang S, 2016, PATTERN RECOGN, V54, P14, DOI 10.1016/j.patcog.2016.01.001
   Wang S, 2012, PATTERN RECOGN, V45, P4129, DOI 10.1016/j.patcog.2012.05.004
   Wilcox R., 2005, ENCY BIOSTATISTICS, DOI [10.1002/0470011815.b2a15064, DOI 10.1002/0470011815.B2A15064]
   Wong WJ, 2013, J CENT SOUTH UNIV, V20, P1292, DOI 10.1007/s11771-013-1614-8
   Yaman D, 2022, MULTIMED TOOLS APPL, V81, P22695, DOI 10.1007/s11042-021-10630-8
   Yang W., 2013, CYBERSPACE SAFETY SE, P81, DOI DOI 10.1007/978-3-319-03584-0_7
   Yang W, 2020, INT RELIAB PHY SYM, DOI 10.1109/irps45951.2020.9129538
   Yang WC, 2021, J INF SECUR APPL, V58, DOI 10.1016/j.jisa.2020.102704
   Yang WC, 2018, PATTERN RECOGN, V78, P242, DOI 10.1016/j.patcog.2018.01.026
   Yang WC, 2014, PATTERN RECOGN, V47, P1309, DOI 10.1016/j.patcog.2013.10.001
NR 52
TC 1
Z9 1
U1 0
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2023
VL 39
IS 12
BP 6249
EP 6263
DI 10.1007/s00371-022-02726-5
EA DEC 2022
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA X7HK1
UT WOS:000895636600002
DA 2024-07-18
ER

PT J
AU Scholz, F
   Nishikawa, S
   Takezawa, M
   Maekawa, T
AF Scholz, Felix
   Nishikawa, Soma
   Takezawa, Masahito
   Maekawa, Takashi
TI Approximation of doubly curved surfaces by analysis-suitable piecewise
   surfaces with high developability
SO VISUAL COMPUTER
LA English
DT Article
DE Piecewise developable surfaces; Watertight surface; Isogeometric
   analysis; Perovskite solar cell
ID INTERSECTION; COMPUTATION; CURVATURE; ALGORITHM; NURBS; LINES; CAD
AB A doubly curved surface can be approximated with a piecewise developable surface by constructing the envelope of the family of the tangent planes along given curves on the input surface. In this paper, we utilize this fact to approximate a doubly curved input surface by a piecewise ruled B-spline surface with high developability. This brings advantages to downstream applications such as ease of fabrication and ease of covering the surfaces with non-extendable materials such as film-based perovskite solar modules. We also present a method for transforming the thereby defined piecewise developable surface into a watertight B-spline geometry that can be used for numerically solving partial differential equations using isogeometric analysis. In our approach, we compute the intersection points of neighboring developable surfaces starting from the first pair and propagate the parameterization of the first intersection curve to all other pairs, thereby maintaining accuracy and developability when approximating the points with a ruled B-spline surface.
C1 [Scholz, Felix; Maekawa, Takashi] Waseda Univ, Res Inst Sci & Engn, 3-4-1 Okubo,Shinjuku Ku, Tokyo 1698555, Japan.
   [Nishikawa, Soma] Waseda Univ, Grad Sch Creat Sci & Engn, Dept Modern Mech Engn, 3-4-1 Okubo,Shinjuku Ku, Tokyo 1698555, Japan.
   [Takezawa, Masahito] Natl Maritime Res Inst, Ind Syst Engn Dept, Cyber Syst Res Grp, 6-38-1 Shinkawa, Mitaka City, Tokyo 1810004, Japan.
C3 Waseda University; Waseda University; National Maritime Research
   Institute
RP Scholz, F (corresponding author), Waseda Univ, Res Inst Sci & Engn, 3-4-1 Okubo,Shinjuku Ku, Tokyo 1698555, Japan.
EM felix.scholz@tafsm.org
OI Scholz, Felix/0000-0003-3339-0079
FU JST CREST [JP-MJCR1911]
FX This work is supported by a JST CREST Grant (No. JP-MJCR1911). We wish
   to thank Yohei Yokosuka for providing the Royan central market hall
   model, and Kenjiro T. Miura for providing the complete log-aesthetic
   surface model.
CR Bashir U, 2013, APPL MATH COMPUT, V219, P10183, DOI 10.1016/j.amc.2013.03.110
   Bo P, 2007, COMPUT GRAPH FORUM, V26, P365, DOI 10.1111/j.1467-8659.2007.01059.x
   Bo PB, 2019, COMPUT AIDED DESIGN, V114, P1, DOI 10.1016/j.cad.2019.05.001
   Chen M, 2010, VISUAL COMPUT, V26, P853, DOI 10.1007/s00371-010-0467-5
   do Carmo MP., 1992, RIEMANNIAN GEOMETRY
   Fioravanti M, 2006, J SYMB COMPUT, V41, P1187, DOI 10.1016/j.jsc.2005.02.008
   Fujimoto, 2001, TECHNICAL HIST STRUC
   Gavriil K, 2019, COMPUT AIDED DESIGN, V111, P29, DOI 10.1016/j.cad.2019.01.006
   Groenewolt A, 2016, INT J ENERGY ENVIR E, V7, P261, DOI 10.1007/s40095-016-0215-3
   Heo HS, 1999, COMPUT AIDED DESIGN, V31, P33, DOI 10.1016/S0010-4485(98)00078-5
   Hoschek J, 1998, COMPUT AIDED DESIGN, V30, P757, DOI 10.1016/S0010-4485(98)00030-X
   Hoschek J., 1993, FUNDAMENTALS COMPUTE
   Hu SM, 2002, COMPUT AIDED DESIGN, V34, P415, DOI 10.1016/S0010-4485(01)00108-7
   Hughes TJR, 2005, COMPUT METHOD APPL M, V194, P4135, DOI 10.1016/j.cma.2004.10.008
   Ion A, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417835
   Izumiya S, 2015, DEMONSTR MATH, V48, P217, DOI 10.1515/dema-2015-0018
   Jiang CG, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392430
   kenji Todori, 2021, Toshiba Review, V76, P17
   Kojima A, 2009, J AM CHEM SOC, V131, P6050, DOI 10.1021/ja809598r
   Lawrence S, 2011, NEXUS NETW J, V13, P701, DOI 10.1007/s00004-011-0087-z
   Levien R., 2009, COMPUT AIDED DESIGN, V6, P1
   Liu YJ, 2009, IEEE T AUTOM SCI ENG, V6, P700, DOI 10.1109/TASE.2008.2009926
   Maekawa T, 1996, COMPUT AIDED GEOM D, V13, P133, DOI 10.1016/0167-8396(95)00018-6
   Maekawa T, 1999, MATH ENG IND, V7, P251
   Maekawa T, 1996, J MECH DESIGN, V118, P499, DOI 10.1115/1.2826919
   Mantzaflaris A., 2018, G SMO GEOMETRY PLUS
   Martín-Pastor A, 2021, NEXUS NETW J, V23, P565, DOI 10.1007/s00004-020-00540-x
   McNeel, RHINOCEROS 3D VERSIO
   Miura K. T., 2006, Computer-Aided Design and Applications, V3, P457
   Patrikalakis N., 2002, Shape Interrogation for Computer Aided Design and Manufacturing
   Pottmann H, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360675
   Rabinovich M, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3180494
   Raffaelli M, 2018, P ROY SOC A-MATH PHY, V474, DOI 10.1098/rspa.2017.0389
   SAPIDIS N, 1990, COMPUT AIDED DESIGN, V22, P121, DOI 10.1016/0010-4485(90)90006-X
   Sapidis N. S., 1992, Computer-Aided Geometric Design, V9, P85, DOI 10.1016/0167-8396(92)90008-D
   Scholz F, 2019, ADV FINITE ELEMENT M, V30, P297, DOI [10.1007/978-3-030-14244-5_15, DOI 10.1007/978-3-030-14244-5_15]
   Scholz F, 2021, SIAM J NUMER ANAL, V59, P2138, DOI 10.1137/20M1341283
   Scholz F, 2021, COMPUT AIDED DESIGN, V140, DOI 10.1016/j.cad.2021.103082
   Scholz F, 2019, COMPUT METHOD APPL M, V357, DOI 10.1016/j.cma.2019.112577
   Shatz I, 2006, VISUAL COMPUT, V22, P825, DOI 10.1007/s00371-006-0067-6
   SHETTY S, 1991, COMPUT AIDED DESIGN, V23, P484, DOI 10.1016/0010-4485(91)90046-Y
   Shikano K., 2013, P SIAM C GEOM DES CO
   Struik D.J., 2012, Lectures on Classical Differential Geometry
   Subag J, 2006, LECT NOTES COMPUT SC, V4077, P143
   Suzuki S, 2018, J COMPUT DES ENG, V5, P243, DOI 10.1016/j.jcde.2017.08.003
   Takacs T, 2015, LECT NOTES COMPUT SC, V9213, P433, DOI 10.1007/978-3-319-22804-4_30
   Takezawa M, 2021, COMPUT AIDED DESIGN, V136, DOI 10.1016/j.cad.2021.103028
   Tang CC, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2832906
   Toth D. L., 1985, Computer Graphics, V19, P171, DOI 10.1145/325165.325233
   Urick B, 2019, COMPUT AIDED DESIGN, V115, P147, DOI 10.1016/j.cad.2019.05.034
   Wang CCL, 2004, VISUAL COMPUT, V20, P521, DOI 10.1007/s00371-004-0256-0
   Wang XP, 2015, VISUAL COMPUT, V31, P1487, DOI 10.1007/s00371-014-1028-0
   Willmore T.J, 2012, INTRO DIFFERENTIAL G
NR 53
TC 0
Z9 0
U1 2
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2023
VL 39
IS 12
BP 6279
EP 6296
DI 10.1007/s00371-022-02728-3
EA DEC 2022
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA X7HK1
UT WOS:000912606400001
DA 2024-07-18
ER

PT J
AU Lin, CD
   Xiong, SW
   Lu, XB
AF Lin, Chengde
   Xiong, Shengwu
   Lu, Xiongbo
TI Disentangled face editing via individual walk in personalized facial
   semantic field
SO VISUAL COMPUTER
LA English
DT Article
DE Face editing; Personalized facial semantic Field; Identity preservation;
   Disentangled facial manipulation; Generative adversarial networks;
   StyleGAN
AB Recent generative adversarial networks (GANs) can synthesize high-fidelity faces and the closely followed works show the existence of facial semantic field in the latent spaces. This motivates several latest works to edit faces via finding semantic directions in the universal facial semantic field of GAN to walk along. However, several challenges still exist during editing: identity loss, attribute entanglement and background variation. In this work, we first propose a personalized facial semantic field (PFSF) for each instead of a universal facial semantic field for all instances. The PFSF is built via portrait-masked retraining of the generator of StyleGAN together with the inversion model, which can preserve identity details for real faces. Furthermore, we propose an individual walk in the learned PFSF to perform disentangled face editing. Finally, the edited portrait is fused back into the original image with the constraint of the portrait mask, which can preserve the background. Extensive experimental results validate that our method performs well in identity preservation, background maintenance and disentangled editing, significantly surpassing related state-of-the-art methods.
C1 [Lin, Chengde; Xiong, Shengwu; Lu, Xiongbo] Wuhan Univ Technol, Sch Comp Sci & Artificial Intelligence, 122 Luoshi Rd, Wuhan 430070, Hubei, Peoples R China.
   [Xiong, Shengwu] Wuhan Univ Technol, Sanya Sci & Educ Innovat Pk, Sanya 572000, Hainan, Peoples R China.
C3 Wuhan University of Technology; Wuhan University of Technology
RP Xiong, SW (corresponding author), Wuhan Univ Technol, Sch Comp Sci & Artificial Intelligence, 122 Luoshi Rd, Wuhan 430070, Hubei, Peoples R China.; Xiong, SW (corresponding author), Wuhan Univ Technol, Sanya Sci & Educ Innovat Pk, Sanya 572000, Hainan, Peoples R China.
EM linchengde@whut.edu.cn; xiongsw@whut.edu.cn; luxiongbo01@gmail.com
RI Lin, CD/G-4112-2010; Xiong, Shou-Mei/A-4225-2009
FU NSFC [62176194, 62101393]; Major project of IoV [2020AAA001]; Sanya
   Science and Education Innovation Park of Wuhan University of Technology
   [2021KF0031]; CSTC [cstc2021jcyj-msxmX1148]; Open Project of Wuhan
   University of Technology Chongqing Research Institute [ZL2021-6]
FX This work was in part supported by NSFC (Grant No. 62176194, Grant No.
   62101393), the Major project of IoV (Grant No. 2020AAA001), Sanya
   Science and Education Innovation Park of Wuhan University of Technology
   (Grant No. 2021KF0031), CSTC(Grant No. cstc2021jcyj-msxmX1148) and the
   Open Project of Wuhan University of Technology Chongqing Research
   Institute (ZL2021-6).
CR Abdal R., 2020, P IEEECVF C COMPUTER, P8296
   Abdal R, 2019, IEEE I CONF COMP VIS, P4431, DOI 10.1109/ICCV.2019.00453
   Bau D., 2019, ICLR WORKSH, V2, P4
   Bau D, 2019, IEEE I CONF COMP VIS, P4501, DOI 10.1109/ICCV.2019.00460
   Blanz V, 1999, COMP GRAPH, P187, DOI 10.1145/311535.311556
   Brock A., 2019, INT C LEARN REPR
   Choi Y, 2018, PROC CVPR IEEE, P8789, DOI 10.1109/CVPR.2018.00916
   Creswell A, 2019, IEEE T NEUR NET LEAR, V30, P1967, DOI 10.1109/TNNLS.2018.2875194
   Deng JK, 2019, PROC CVPR IEEE, P4685, DOI 10.1109/CVPR.2019.00482
   Egger B, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3395208
   Fang Z, 2022, VISUAL COMPUT, V38, P1151, DOI 10.1007/s00371-021-02074-w
   Gu JJ, 2020, PROC CVPR IEEE, P3009, DOI 10.1109/CVPR42600.2020.00308
   Guan S., 2020, ARXIV
   Han Y., 2021, P 30 INT JOINT C ART, P715
   Harkonen E., 2020, Ganspace: Discovering interpretable gan controls
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He ZL, 2019, IEEE T IMAGE PROCESS, V28, P5464, DOI 10.1109/TIP.2019.2916751
   Huang X, 2021, VISUAL COMPUT, V37, P95, DOI 10.1007/s00371-020-01982-7
   Jiapeng Zhu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12362), P592, DOI 10.1007/978-3-030-58520-4_35
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Ju YX, 2021, VISUAL COMPUT, V37, P2907, DOI 10.1007/s00371-021-02198-z
   Karras T, 2018, P INT C LEARN REPR I
   Karras T, 2021, IEEE T PATTERN ANAL, V43, P4217, DOI 10.1109/TPAMI.2020.2970919
   Karras Tero, 2020, IEEE C COMP VIS PATT
   Kemelmacher-Shlizerman I, 2014, PROC CVPR IEEE, P3334, DOI 10.1109/CVPR.2014.426
   Li JZZ, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P3501, DOI 10.1145/3474085.3475512
   Li MJ, 2021, PROC CVPR IEEE, P6525, DOI 10.1109/CVPR46437.2021.00646
   Liang YQ, 2022, INTEGR COMPUT-AID E, V29, P23, DOI 10.3233/ICA-210661
   Lin CD, 2022, IMAGE VISION COMPUT, V125, DOI 10.1016/j.imavis.2022.104517
   Lin CD, 2022, J VIS COMMUN IMAGE R, V87, DOI 10.1016/j.jvcir.2022.103566
   Lipton Z.C., 2017, INT C LEARNING REPRE
   Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425
   Ma FC, 2018, ADV NEUR IN, V31
   Perarnau G, 2016, ARXIV
   Richardson E, 2021, PROC CVPR IEEE, P2287, DOI 10.1109/CVPR46437.2021.00232
   Shen YJ, 2022, IEEE T PATTERN ANAL, V44, P2004, DOI 10.1109/TPAMI.2020.3034267
   Song YP, 2022, COMPUT AIDED DESIGN, V146, DOI 10.1016/j.cad.2022.103196
   Tewari A, 2020, PROC CVPR IEEE, P6141, DOI 10.1109/CVPR42600.2020.00618
   Viazovetskyi Yuri, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12367), P170, DOI 10.1007/978-3-030-58542-6_11
   Wang HP, 2021, PROC CVPR IEEE, P7868, DOI 10.1109/CVPR46437.2021.00778
   Wang R, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P926, DOI 10.1145/3474085.3475274
   Wang S, 2022, VISUAL COMPUT, V38, P2539, DOI 10.1007/s00371-021-02129-y
   Xu HXX, 2022, COMPUT AIDED GEOM D, V97, DOI 10.1016/j.cagd.2022.102122
   Xu YH, 2021, PROC CVPR IEEE, P4430, DOI 10.1109/CVPR46437.2021.00441
   Yang GX, 2021, PROC CVPR IEEE, P2950, DOI 10.1109/CVPR46437.2021.00297
   Yang N, 2021, IEEE SIGNAL PROC LET, V28, P553, DOI 10.1109/LSP.2021.3059371
   Yao Xu, 2021, P IEEE INT C COMP VI, P13789
   Zhu JY, 2016, LECT NOTES COMPUT SC, V9909, P597, DOI 10.1007/978-3-319-46454-1_36
   Zhuang P., 2021, INT C LEARNING REPRE
NR 49
TC 2
Z9 2
U1 2
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2023
VL 39
IS 12
BP 6005
EP 6014
DI 10.1007/s00371-022-02708-7
EA NOV 2022
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA X7HK1
UT WOS:000878468500001
DA 2024-07-18
ER

PT J
AU Tan, XX
   Dong, ZM
   Zhao, HL
AF Tan, Xinxing
   Dong, Zemin
   Zhao, Hualing
TI Robust fine-grained image classification with noisy labels
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Fine-grained image classification; Noisy labels; Deep learning; Robust
   loss
ID MODEL
AB Since annotating fine-grained labels requires special expertise, label annotations often lack quality for many real-world fine-grained image classifications (FGIC). Due to the effectiveness of noisy labels, training deep fine-grained models directly tends to have an inferior recognition ability. To address this problem in FGIC, a robust classification approach combining "active-passive-loss (APL)" framework and multi-branch attention learning is proposed. First, in order to learn discriminative regions for classification effectively, the multi-branch attention learning framework that consists of raw, object, and part branch is introduced. These three branches are connected by attention mechanism, which enables the network to learn fine-grained features of different parts from different scales including raw, object and part levels. Second, treating noisy labels as anomalies, the novel loss framework APL that can guarantee robustness and sufficient learning is adopted to achieve robust predictions in each branch. Third, in determining the final predictions, the outputs from global and object branches are combined to achieve higher classification performance. Several experiments on fine-grained image datasets show that the proposed approach is noise-robust and can achieve excellent classification performance in the presence of noisy labels in FGIC.
C1 [Tan, Xinxing] Wuhan Inst City, Dept Informat Engn, Wuhan 430083, Peoples R China.
   [Dong, Zemin] Wuhan Inst City, Res & Training Ctr, Wuhan 430083, Peoples R China.
   [Zhao, Hualing] Wuhan Univ Technol, Sch Sci, Wuhan 430070, Peoples R China.
C3 Wuhan University of Technology
RP Dong, ZM (corresponding author), Wuhan Inst City, Res & Training Ctr, Wuhan 430083, Peoples R China.
EM tanxinxing@gmail.com; dzm19811207@163.com; Zhaohualing2011@whut.edu.cn
FU Fundamental Research Funds for the Central Universities [2020-IB-003]
FX The work is supported by the Fundamental Research Funds for the Central
   Universities (Grant No. 2020-IB-003)
CR Ahmed A, 2021, MULTIMED TOOLS APPL, V80, P20759, DOI 10.1007/s11042-021-10760-z
   Algan G, 2021, KNOWL-BASED SYST, V215, DOI 10.1016/j.knosys.2021.106771
   [Anonymous], P IEEE C COMP VIS PA
   Chang DL, 2020, IEEE T IMAGE PROCESS, V29, P4683, DOI 10.1109/TIP.2020.2973812
   Chen PF, 2019, PR MACH LEARN RES, V97
   Chen YY, 2021, IEEE COMPUT SOC CONF, P2682, DOI 10.1109/CVPRW53098.2021.00302
   Chen Y, 2019, PROC CVPR IEEE, P5152, DOI 10.1109/CVPR.2019.00530
   Cheng G, 2021, SCI CHINA INFORM SCI, V64, DOI 10.1007/s11432-020-3156-7
   Cordeiro FR, 2021, Arxiv, DOI arXiv:2103.04173
   Dubey A, 2018, 32 C NEURAL INFORM P, V31
   Eshratifar AE, 2021, MACH VISION APPL, V32, DOI 10.1007/s00138-021-01180-y
   Fan Zhang, 2021, MultiMedia Modeling. 27th International Conference, MMM 2021. Proceedings. Lecture Notes in Computer Science (LNCS 12572), P136, DOI 10.1007/978-3-030-67832-6_12
   Feng C, 2022, Arxiv, DOI arXiv:2111.11288
   Fu JL, 2017, PROC CVPR IEEE, P4476, DOI 10.1109/CVPR.2017.476
   Ghosh A, 2017, AAAI CONF ARTIF INTE, P1919
   Han JW, 2022, IEEE T PATTERN ANAL, V44, P579, DOI 10.1109/TPAMI.2019.2933510
   Hanselmann H, 2020, IEEE WINT CONF APPL, P1236, DOI [10.1109/WACV45572.2020.9093601, 10.1109/wacv45572.2020.9093601]
   He Kaiming, 2016, P INT C COMP VIS PAT, DOI 10.1109/CVPR.2016.90
   He XT, 2017, AAAI CONF ARTIF INTE, P4075
   Jiang L, 2020, PR MACH LEARN RES, V446, P4804, DOI DOI 10.48550/ARXIV.1911.09781
   Jindal I, 2016, IEEE DATA MINING, P967, DOI [10.1109/ICDM.2016.124, 10.1109/ICDM.2016.0121]
   Karimi D, 2020, MED IMAGE ANAL, V65, DOI 10.1016/j.media.2020.101759
   Krause J, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P554, DOI 10.1109/ICCVW.2013.77
   Lee KH, 2018, PROC CVPR IEEE, P5447, DOI 10.1109/CVPR.2018.00571
   Li J., 2019, INT C LEARNING REPRE
   Li JN, 2019, PROC CVPR IEEE, P5046, DOI 10.1109/CVPR.2019.00519
   Li Q, 2020, COMPUT VIS IMAGE UND, V196, DOI 10.1016/j.cviu.2020.102963
   Li XX, 2021, IEEE T IMAGE PROCESS, V30, P1318, DOI 10.1109/TIP.2020.3043128
   Lin TY, 2018, IEEE T PATTERN ANAL, V40, P1309, DOI 10.1109/TPAMI.2017.2723400
   Ma X., 2020, INT C MACHINE LEARNI, P6543
   Niu L, 2018, PROC CVPR IEEE, P7171, DOI 10.1109/CVPR.2018.00749
   Pang Tianyu, 2017, ARXIV
   Peng YX, 2018, IEEE T IMAGE PROCESS, V27, P1487, DOI 10.1109/TIP.2017.2774041
   Rodríguez P, 2020, IEEE T MULTIMEDIA, V22, P502, DOI 10.1109/TMM.2019.2928494
   Song H, 2019, PR MACH LEARN RES, V97
   Sun XX, 2019, AAAI CONF ARTIF INTE, P273
   Tanaka D, 2018, PROC CVPR IEEE, P5552, DOI 10.1109/CVPR.2018.00582
   Wah C., 2011, CALTECH UCSD BIRDS 2
   Wang YS, 2019, IEEE I CONF COMP VIS, P322, DOI 10.1109/ICCV.2019.00041
   Wang Z, 2020, PROC CVPR IEEE, P4523, DOI 10.1109/CVPR42600.2020.00458
   Wei XS, 2019, Arxiv, DOI arXiv:1907.03069
   Wei XS, 2018, PATTERN RECOGN, V76, P704, DOI 10.1016/j.patcog.2017.10.002
   Xie SN, 2015, PROC CVPR IEEE, P2645, DOI 10.1109/CVPR.2015.7298880
   Xu Y., 2019, Adv Neural Inf Process Syst, P6222
   Yang Z, 2018, LECT NOTES COMPUT SC, V11218, P438, DOI 10.1007/978-3-030-01264-9_26
   Yi K, 2019, PROC CVPR IEEE, P7010, DOI 10.1109/CVPR.2019.00718
   Zhang N, 2014, LECT NOTES COMPUT SC, V8689, P834, DOI 10.1007/978-3-319-10590-1_54
   Zhang ZL, 2018, ADV NEUR IN, V31
NR 48
TC 2
Z9 2
U1 6
U2 24
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2022 OCT 18
PY 2022
DI 10.1007/s00371-022-02686-w
EA OCT 2022
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 5K3CH
UT WOS:000869606600001
DA 2024-07-18
ER

PT J
AU Manák, M
   Anikeenko, A
   Vása, L
   Kolingerová, I
AF Manak, Martin
   Anikeenko, Alexey
   Vasa, Libor
   Kolingerova, Ivana
TI Compact storage of additively weighted Voronoi diagrams
SO VISUAL COMPUTER
LA English
DT Article
DE Voronoi diagram; Additive weights; Quasi-triangulation; Tetrahedra;
   Cut-border; Compression
ID EDGE-TRACING ALGORITHM; QUASI-TRIANGULATION; COMPRESSION; CONNECTIVITY;
   COMPUTATION; REDUCTION
AB Voronoi diagrams and their dual tetrahedral structures are often utilized in various analyses of protein models where atoms are represented as balls. The additively weighted kind of Voronoi diagrams (aw-VD) is particularly useful in this area because it includes the variability of ball size. The dynamic behavior of proteins can be realistically modeled by a simulation of molecular dynamics recording atom positions at discrete snapshots of time. However, aw-VDs for these snapshots require a lot of storage. This is where domain-specific compression could help, but available algorithms are oriented primarily to the compression of ordinary tetrahedral meshes, not to aw-VDs whose dual tetrahedral structure may have anomalies complicating duality inversion and tetrahedral mesh traversal. Therefore, we propose a method that can compactly store the dual tetrahedral structure. The method is built on general ideas of an already known cut-border machine and improves these ideas in several ways so it can be applied to aw-VDs and perform well on protein data.
C1 [Manak, Martin; Vasa, Libor] Univ West Bohemia, New Technol Informat Soc NTIS, Univerzitni 8, Plzen 30614, Czech Republic.
   [Anikeenko, Alexey] SB RAS, Voevodsky Inst Chem Kinet & Combust, Novosibirsk 630090, Russia.
   [Manak, Martin; Kolingerova, Ivana] Univ West Bohemia, Fac Appl Sci, Dept Comp Sci & Engn, Univerzitni 8, Plzen 30614, Czech Republic.
C3 University of West Bohemia Pilsen; Russian Academy of Sciences;
   Voevodsky Institute of Chemical Kinetics & Combustion SB RAS; University
   of West Bohemia Pilsen
RP Manák, M (corresponding author), Univ West Bohemia, New Technol Informat Soc NTIS, Univerzitni 8, Plzen 30614, Czech Republic.
EM manak@ntis.zcu.cz; anik@kinetics.nsc.ru; lvasa@kiv.zcu.cz;
   kolinger@kiv.zcu.cz
RI Manak, Martin/O-6300-2019; Vasa, Libor/F-6706-2011
OI Manak, Martin/0000-0002-1248-7042; Vasa, Libor/0000-0002-0213-3769;
   Kolingerova, Ivana/0000-0003-4556-2771
FU Czech Science Foundationthe [17-07690S]; Representation and processing
   methods for three-dimensional dynamic shapes [20-02154S]; Czech Ministry
   of Education, Youth and Sports-the project Sustainability support of the
   center NTIS-New Technologies for the Information Society [LO1506]
FX This study was funded by the Czech Science Foundationthe projects
   Methods of identification and visualization of tunnels for flexible
   ligands in dynamic proteins (Grant Number 17-07690S) and Representation
   and processing methods for three-dimensional dynamic shapes (Grant
   Number 20-02154S). This studywas also partially funded by the Czech
   Ministry of Education, Youth and Sports-the project Sustainability
   support of the center NTIS-New Technologies for the Information Society
   (Grant Number LO1506).
CR AwVoronoi, US
   Berman HM, 2000, NUCLEIC ACIDS RES, V28, P235, DOI 10.1093/nar/28.1.235
   Bischoff U, 2005, IEEE DATA COMPR CONF, P93
   BONDI A, 1964, J PHYS CHEM-US, V68, P441, DOI 10.1021/j100785a001
   Cho Y, 2006, LECT NOTES COMPUT SC, V3980, P111
   Coors V, 2004, VISUAL COMPUT, V20, P507, DOI 10.1007/s00371-004-0255-1
   Doumanoglou A, 2014, IEEE T CIRC SYST VID, V24, P2099, DOI 10.1109/TCSVT.2014.2319631
   Dvorák J, 2020, J MOL GRAPH MODEL, V96, DOI 10.1016/j.jmgm.2020.107531
   FENWICK PM, 1994, SOFTWARE PRACT EXPER, V24, P327, DOI 10.1002/spe.4380240306
   Gavrilova M.L., 1998, THESIS U CALGARY CAL
   Gavrilova ML, 2003, COMPUT AIDED GEOM D, V20, P231, DOI 10.1016/S0167-8396(03)00027-X
   Gumhold S., 1999, Proceedings Visualization '99 (Cat. No.99CB37067), P51, DOI 10.1109/VISUAL.1999.809868
   Isenburg M, 2006, PROC GRAPH INTERF, P115
   Jurcik A, 2018, BIOINFORMATICS, V34, P3586, DOI 10.1093/bioinformatics/bty386
   Kim DS, 2006, COMPUT AIDED DESIGN, V38, P808, DOI 10.1016/j.cad.2006.04.008
   Kim DS, 2013, COMPUT AIDED DESIGN, V45, P35, DOI 10.1016/j.cad.2012.03.005
   Kim DS, 2012, COMPUT AIDED DESIGN, V44, P835, DOI 10.1016/j.cad.2012.04.003
   Kim DS, 2010, COMPUT AIDED DESIGN, V42, P874, DOI 10.1016/j.cad.2010.06.002
   Kim DS, 2005, COMPUT AIDED DESIGN, V37, P1412, DOI 10.1016/j.cad.2005.02.013
   Lambrughi M, 2016, NUCLEIC ACIDS RES, V44, P9096, DOI 10.1093/nar/gkw770
   Larsson DSD, 2012, PLOS COMPUT BIOL, V8, DOI 10.1371/journal.pcbi.1002502
   Lewiner T, 2006, COMPUT GRAPH FORUM, V25, P685, DOI 10.1111/j.1467-8659.2006.00990.x
   Lindow N, 2013, BMC BIOINFORMATICS, V14, DOI 10.1186/1471-2105-14-S19-S5
   Maglo A, 2015, ACM COMPUT SURV, V47, DOI 10.1145/2693443
   Manak M, 2017, COMPUT GRAPH FORUM, V36, P160, DOI 10.1111/cgf.12870
   Manak M, 2019, J COMPUT CHEM, V40, P1758, DOI 10.1002/jcc.25828
   Manak M, 2017, J MOL GRAPH MODEL, V74, P225, DOI 10.1016/j.jmgm.2017.03.018
   Manak M, 2016, INFORM PROCESS LETT, V116, P85, DOI 10.1016/j.ipl.2015.09.017
   Marques SM, 2017, J CHEM INF MODEL, V57, P1970, DOI 10.1021/acs.jcim.7b00070
   Medvedev NN, 2006, J COMPUT CHEM, V27, P1676, DOI 10.1002/jcc.20484
   Okabe A., 2000, PROBABILITY STAT
   Olechnovic K, 2014, J COMPUT CHEM, V35, P672, DOI 10.1002/jcc.23538
   Pavlova M, 2009, NAT CHEM BIOL, V5, P727, DOI 10.1038/nchembio.205
   Poupon A, 2004, CURR OPIN STRUC BIOL, V14, P233, DOI 10.1016/j.sbi.2004.03.010
   Prat S, 2005, VISUAL COMPUT, V21, P876, DOI 10.1007/s00371-005-0325-z
   QTFier, US
   Shewchuk J. R., 1996, Proceedings of the Twelfth Annual Symposium on Computational Geometry, FCRC '96, P141, DOI 10.1145/237218.237337
   Sukov S.A., 2018, PREPRINT, DOI [10.20948/prepr-2018-134, DOI 10.20948/PREPR-2018-134]
   Szymczak A, 2000, COMPUT AIDED DESIGN, V32, P527, DOI 10.1016/S0010-4485(00)00040-3
   tukaani, THE XZ FORMAT
   Vleugels J, 1996, COMP GEOM-THEOR APPL, V6, P329, DOI 10.1016/0925-7721(96)00002-8
   Will H.M., 1999, THESIS SWISS FEDERAL, DOI [10.3929/ethz-a-003845562, DOI 10.3929/ETHZ-A-003845562]
   Yang CK, 2000, IEEE VISUAL, P101, DOI 10.1109/VISUAL.2000.885682
NR 43
TC 0
Z9 0
U1 1
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2023
VL 39
IS 11
BP 5389
EP 5404
DI 10.1007/s00371-022-02667-z
EA OCT 2022
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA W5SX6
UT WOS:000866293500001
DA 2024-07-18
ER

PT J
AU Deng, ZR
   Liang, Y
   Pan, JH
   Liao, JC
   Hao, Y
   Wen, X
AF Deng, Zhuoran
   Liang, Yan
   Pan, Jiahui
   Liao, Jiacheng
   Hao, Yan
   Wen, Xing
TI Fast 3D face reconstruction from a single image combining attention
   mechanism and graph convolutional network
SO VISUAL COMPUTER
LA English
DT Article
DE 3D face reconstruction; Lightweight network; Attention mechanism; Graph
   convolutional network
AB In recent years, researchers have made significant contributions to 3D face reconstruction with the rapid development of deep learning. However, learning-based methods often suffer from time and memory consumption. Simply removing network layers hardly solves the problem. In this study, we propose a solution that achieves fast and robust 3D face reconstruction from a single image without the need for accurate 3D data for training. In terms of increasing speed, we use a lightweight network as a facial feature extractor. As a result, our method reduces the reliance on graphics processing units, allowing fast inference on central processing units alone. To maintain robustness, we combine an attention mechanism and a graph convolutional network in parameter regression to concentrate on facial details. We experiment with different combinations of three loss functions to obtain the best results. In comparative experiments, we evaluate the performance of the proposed method and state-of-the-art methods on 3D face reconstruction and sparse face alignment, respectively. Experiments on a variety of datasets validate the effectiveness of our method.
C1 [Deng, Zhuoran; Liang, Yan; Pan, Jiahui; Liao, Jiacheng; Hao, Yan; Wen, Xing] South China Normal Univ, Sch Software, Foshan 528225, Peoples R China.
C3 South China Normal University
RP Liang, Y (corresponding author), South China Normal Univ, Sch Software, Foshan 528225, Peoples R China.
EM dengzr@m.scnu.edu.cn; liangyan@m.scnu.edu.cn
RI Pan, Jiahui/ADI-3371-2022
OI Pan, Jiahui/0000-0002-7576-6743
FU National Natural Science Foundation of China [2022ZD0208900]; 
   [62076103]
FX We are grateful to the excellent works of Guo et al. [8], Feng et al.
   [6], andYe et al. [35] for supplying us with novel ideas and partial
   implementations of the method. We thank NoW benchmark [27] for providing
   us with the test results of the 3D face reconstruction in Sect. 4.4.2.
   This work was partially supported by the Science and Technology
   Innovation 2030 -"Brain Science and Brain-Like Intelligence Technology"
   Key Project under Grant 2022ZD0208900, and the National Natural Science
   Foundation of China under Grant 62076103.
CR Aldrian O, 2013, IEEE T PATTERN ANAL, V35, P1080, DOI 10.1109/TPAMI.2012.206
   Aldrian Oswald., 2010, P BRIT MACHINE VISIO, p75.1, DOI DOI 10.5244/C.24.75
   [Anonymous], 2016, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2016.372
   Bahdanau D, 2016, Arxiv, DOI [arXiv:1409.0473, 10.48550/arXiv.1409.0473]
   Bahroun S, 2023, VISUAL COMPUT, V39, P239, DOI 10.1007/s00371-021-02324-x
   Belhumeur PN, 2013, IEEE T PATTERN ANAL, V35, P2930, DOI 10.1109/TPAMI.2013.23
   Blanz V, 1999, COMP GRAPH, P187, DOI 10.1145/311535.311556
   Blanz V, 2003, IEEE T PATTERN ANAL, V25, P1063, DOI 10.1109/TPAMI.2003.1227983
   Bulat A, 2017, IEEE I CONF COMP VIS, P1021, DOI 10.1109/ICCV.2017.116
   Cao C, 2014, IEEE T VIS COMPUT GR, V20, P413, DOI 10.1109/TVCG.2013.249
   Cao C, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2462012
   Defferrard M, 2016, ADV NEUR IN, V29
   Deng Y, 2019, IEEE COMPUT SOC CONF, P285, DOI 10.1109/CVPRW.2019.00038
   Dib A., 2021, Proceedings of the IEEE International Conference on Computer Vision (ICCV)
   Feng Y, 2018, LECT NOTES COMPUT SC, V11218, P557, DOI 10.1007/978-3-030-01264-9_33
   Gao ZP, 2020, IEEE COMPUT SOC CONF, P1426, DOI 10.1109/CVPRW50498.2020.00182
   He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38
   Howard A. G., 2017, PREPRINT
   Jiang L, 2018, IEEE T IMAGE PROCESS, V27, P4756, DOI 10.1109/TIP.2018.2845697
   Jianzhu Guo, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12364), P152, DOI 10.1007/978-3-030-58529-7_10
   Jin H, 2019, VISUAL COMPUT, V35, P535, DOI 10.1007/s00371-018-1482-1
   Jin H, 2017, COMPUT AIDED GEOM D, V50, P1, DOI 10.1016/j.cagd.2016.11.001
   Jin Ye, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12366), P649, DOI 10.1007/978-3-030-58589-1_39
   Kipf TN, 2017, INT C LEARN REPR
   Köstinger M, 2011, 2011 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCV WORKSHOPS)
   Koizumi Tatsuro, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P690, DOI 10.1007/978-3-030-58536-5_41
   Lee GH, 2020, PROC CVPR IEEE, P6099, DOI 10.1109/CVPR42600.2020.00614
   Lin JK, 2020, PROC CVPR IEEE, P5890, DOI 10.1109/CVPR42600.2020.00593
   Liu L, 2020, VISUAL COMPUT, V36, P1521, DOI 10.1007/s00371-019-01746-y
   Liu P, 2019, 2019 2ND IEEE CONFERENCE ON MULTIMEDIA INFORMATION PROCESSING AND RETRIEVAL (MIPR 2019), P403, DOI 10.1109/MIPR.2019.00082
   Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425
   Messer K, 1999, Second International Conference on Audio and Video-based Biometric Person Authentication, P72
   Paysan P, 2009, AVSS: 2009 6TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE, P296, DOI 10.1109/AVSS.2009.58
   Piao JT, 2019, IEEE I CONF COMP VIS, P9397, DOI 10.1109/ICCV.2019.00949
   Ranjan A, 2018, LECT NOTES COMPUT SC, V11207, P725, DOI 10.1007/978-3-030-01219-9_43
   Richardson E, 2016, INT CONF 3D VISION, P460, DOI 10.1109/3DV.2016.56
   Ruan ZY, 2021, IEEE T IMAGE PROCESS, V30, P5793, DOI 10.1109/TIP.2021.3087397
   Sagonas C, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P397, DOI 10.1109/ICCVW.2013.59
   Sanyal S, 2019, PROC CVPR IEEE, P7755, DOI 10.1109/CVPR.2019.00795
   Schönborn S, 2013, LECT NOTES COMPUT SC, V8142, P101, DOI 10.1007/978-3-642-40602-7_11
   Trân AT, 2017, PROC CVPR IEEE, P1493, DOI 10.1109/CVPR.2017.163
   Zhou B., 2016, P IEEE C COMP VIS PA, DOI DOI 10.1109/CVPR.2016.319
   Zhou EJ, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P386, DOI 10.1109/ICCVW.2013.58
   Zhou Y, 2019, INT C REHAB ROBOT, P1097, DOI [10.1109/ICORR.2019.8779502, 10.1109/icorr.2019.8779502]
   Zhu XX, 2012, PROC CVPR IEEE, P2879, DOI 10.1109/CVPR.2012.6248014
   Zhu XY, 2019, IEEE T PATTERN ANAL, V41, P78, DOI 10.1109/TPAMI.2017.2778152
   Zhu XY, 2016, PROC CVPR IEEE, P146, DOI 10.1109/CVPR.2016.23
NR 47
TC 5
Z9 5
U1 5
U2 12
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2023
VL 39
IS 11
BP 5547
EP 5561
DI 10.1007/s00371-022-02679-9
EA SEP 2022
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA W5SX6
UT WOS:000863126900001
OA hybrid
DA 2024-07-18
ER

PT J
AU Liu, YG
   Chen, LF
   Lin, ZR
AF Liu, Yungen
   Chen, Linfeng
   Lin, Zhenrong
TI Keyframe extraction for motion capture data via pose saliency and
   reconstruction error
SO VISUAL COMPUTER
LA English
DT Article
DE Motion capture data; Keyframe extraction; Pose saliency; Reconstruction
   error
ID RETRIEVAL
AB Keyframes are a summary representation of motion capture data, which provide the basis for compression, retrieval, overview and reuse of motion capture data. In this paper, a new approach is proposed to extract keyframes from motion capture data. This approach uses the angle of rotation of limbs and the distance between joints as the feature representation of human movement and calculates limb saliency based on the multiscale saliency of each motion feature. Then the weighted sum of limb saliency is defined as pose saliency, and the frames corresponding to the local maxima on the pose saliency curve are extracted as the initial keyframes. Finally, guided by the initial keyframes, the optimal keyframes are extracted based on the reconstruction error optimization algorithm. Experiments demonstrate that this approach can effectively extract the keyframes with high visual perceptual quality and low reconstruction error, and better meet the needs of real-time analysis and compression of motion capture data.
C1 [Liu, Yungen; Chen, Linfeng; Lin, Zhenrong] Nanchang Univ, Sch Math & Comp Sci, Nanchang 330031, Jiangxi, Peoples R China.
C3 Nanchang University
RP Lin, ZR (corresponding author), Nanchang Univ, Sch Math & Comp Sci, Nanchang 330031, Jiangxi, Peoples R China.
EM liuyungen@ncu.edu.cn; zrlin@ncu.edu.cn
RI Chen, Lifeng/JJE-7359-2023
CR Arikan O, 2006, ACM T GRAPHIC, V25, P890, DOI 10.1145/1141911.1141971
   Assa J, 2005, ACM T GRAPHIC, V24, P667, DOI 10.1145/1073204.1073246
   Bulut E., 2007, P CASA JUN, P63
   Chang XJ., KEY FRAMES EXTRACTIO
   Costa BF, 2020, COMM COM INF SC, V1182, P48, DOI 10.1007/978-3-030-41590-7_3
   Guo SH, 2015, VISUAL COMPUT, V31, P497, DOI 10.1007/s00371-014-0943-4
   Halit C, 2011, COMPUT ANIMAT VIRT W, V22, P3, DOI 10.1002/cav.380
   Huang KS, 2005, VISUAL COMPUT, V21, P532, DOI 10.1007/s00371-005-0316-0
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Jin C, 2012, COMPUT ANIMAT VIRT W, V23, P559, DOI 10.1002/cav.1471
   Kim Y, 2021, VISUAL COMPUT, V37, P1949, DOI 10.1007/s00371-020-01956-9
   Lee CH, 2005, ACM T GRAPHIC, V24, P659, DOI 10.1145/1073204.1073244
   Lim IS, 2001, P ANN INT IEEE EMBS, V23, P1167
   Liu F, 2003, COMPUT VIS IMAGE UND, V92, P265, DOI 10.1016/j.cviu.2003.06.001
   Liu XM, 2013, VISUAL COMPUT, V29, P85, DOI 10.1007/s00371-012-0676-1
   Liu Yungen, 2010, Journal of Computer Aided Design & Computer Graphics, V22, P670, DOI 10.3724/SP.J.1089.2010.10691
   Miura T, 2014, IEEJ T ELECTR ELECTR, V9, P697, DOI 10.1002/tee.22029
   Müller M, 2005, ACM T GRAPHIC, V24, P677, DOI 10.1145/1073204.1073247
   Park MJ, 2004, COMPUT ANIMAT VIRT W, V15, P245, DOI 10.1002/cav.27
   Qi T, 2013, COMPUT ANIMAT VIRT W, V24, P399, DOI 10.1002/cav.1505
   Roberts R, 2019, COMPUT VIS MEDIA, V5, P171, DOI 10.1007/s41095-019-0138-z
   Sun B, 2018, 2018 IEEE 9TH ANNUAL INFORMATION TECHNOLOGY, ELECTRONICS AND MOBILE COMMUNICATION CONFERENCE (IEMCON), P107, DOI 10.1109/IEMCON.2018.8614862
   Togawa H, 2005, 11TH INTERNATIONAL CONFERENCE ON PARALLEL AND DISTRIBUTED SYSTEMS WORKSHOPS, VOL II, PROCEEDINGS,, P182
   Voulodimos A, 2020, MULTIMED TOOLS APPL, V79, P3243, DOI 10.1007/s11042-018-6935-z
   Xia GY, 2022, IEEE T CIRC SYST VID, V32, P4538, DOI 10.1109/TCSVT.2021.3129478
   Xia GY, 2021, IEEE T NEUR NET LEAR, V32, P1612, DOI 10.1109/TNNLS.2020.2985817
   Xia GY, 2017, IEEE T IND ELECTRON, V64, P1589, DOI 10.1109/TIE.2016.2610946
   Xiao J, 2006, LECT NOTES COMPUT SC, V4035, P494
   Xu CX, 2021, COMPUT ANIMAT VIRT W, V32, DOI 10.1002/cav.1976
   Zhang Q, 2014, SYMMETRY-BASEL, V6, P926, DOI 10.3390/sym6040926
   Zhang Q, 2013, J HUM KINET, V39, P5, DOI 10.2478/hukin-2013-0063
NR 31
TC 1
Z9 1
U1 3
U2 11
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2023
VL 39
IS 10
BP 4943
EP 4953
DI 10.1007/s00371-022-02639-3
EA AUG 2022
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA T1GK8
UT WOS:000840290600002
DA 2024-07-18
ER

PT J
AU Juneja, A
   Singla, SK
   Kumar, V
AF Juneja, Akshay
   Singla, Sunil Kumar
   Kumar, Vijay
TI HUDRS: hazy unpaired dataset for road safety
SO VISUAL COMPUTER
LA English
DT Article
DE Dataset; Deep learning; Fog; Haze; Unpaired dataset
ID IMAGE QUALITY ASSESSMENT
AB Dataset is an important aspect for designing a defogging/dehazing framework for road safety. Most of the datasets available in the literature are synthetic in nature, i.e., fog/haze is added to the clear images. The algorithms trained and tested with these synthetic databases work differently in the real-time scenario due to the presence of uncertainty such as density of fog in the real world. In this paper, a new dataset with real-world hazy and clear unpaired images for road safety called Hazy Unpaired Dataset for Road Safety (HUDRS) is presented. HUDRS consists of thousands of foggy and fog-free real-world roadside scenes captured under the natural environmental conditions. These images are captured from Canon Power ShotSX400 IS camera. The performance of the existing and proposed datasets has been evaluated by using prior-based and deep-learning-based dehazing algorithms present in the literature. Experimental results reveal that the visual quality and performance metrics of dehazed images obtained by implementing a visibility restoration algorithm depend on the dataset on which they are trained or validated.
C1 [Juneja, Akshay; Singla, Sunil Kumar] Thapar Inst Engn & Technol, Dept Elect & Instrumentat Engn, Patiala, Punjab, India.
   [Kumar, Vijay] Natl Inst Technol Hamirpur, Dept Comp Sci & Engn, Hamirpur, India.
C3 Thapar Institute of Engineering & Technology; National Institute of
   Technology (NIT System); National Institute of Technology Hamirpur
RP Kumar, V (corresponding author), Natl Inst Technol Hamirpur, Dept Comp Sci & Engn, Hamirpur, India.
EM ajuneja60_phd20@thapar.edu; yijaykumarchahar@nith.ac.in;
   ssingla@thapar.edu
RI Juneja, Dr. Akshay/KCK-0186-2024; Chahar, Vijay Kumar/A-2782-2015
OI Juneja, Dr. Akshay/0000-0002-0616-6613; Chahar, Vijay
   Kumar/0000-0002-3460-6989
FU Council of Scientific and Industrial Research (CSIR), India
   [22(0801)/19/EMR-II]
FX This research is supported by the Council of Scientific and Industrial
   Research (CSIR), India. The sanction number of the scheme is
   22(0801)/19/EMR-II.
CR Ancuti CO, 2020, IEEE COMPUT SOC CONF, P1798, DOI 10.1109/CVPRW50498.2020.00230
   Ancuti CO, 2019, IEEE IMAGE PROC, P1014, DOI [10.1109/icip.2019.8803046, 10.1109/ICIP.2019.8803046]
   Ancuti CO, 2018, IEEE COMPUT SOC CONF, P867, DOI 10.1109/CVPRW.2018.00119
   Ancuti C, 2016, IEEE IMAGE PROC, P2226, DOI 10.1109/ICIP.2016.7532754
   [Anonymous], 2021, PREPARING YOUR DATAS
   Bezryadin S., 2007, INT S TECHN DIG PHOT, P1015
   Bui TM, 2018, IEEE T IMAGE PROCESS, V27, P999, DOI 10.1109/TIP.2017.2771158
   Chen DD, 2019, IEEE WINT CONF APPL, P1375, DOI 10.1109/WACV.2019.00151
   Chen YT, 2021, APPL INTELL, V51, P4367, DOI 10.1007/s10489-020-02116-1
   Chen YT, 2021, J AMB INTEL HUM COMP, DOI 10.1007/s12652-020-02778-2
   Chen YT, 2021, APPL INTELL, V51, P3460, DOI 10.1007/s10489-020-01971-2
   Chen YT, 2021, MULTIMED TOOLS APPL, V80, P4237, DOI 10.1007/s11042-020-09887-2
   Chen YT, 2021, VISUAL COMPUT, V37, P1691, DOI 10.1007/s00371-020-01932-3
   Ngo D, 2019, APPL SCI-BASEL, V9, DOI 10.3390/app9194011
   Dong H, 2020, PROC CVPR IEEE, P2154, DOI 10.1109/CVPR42600.2020.00223
   Hu Q, 2023, VISUAL COMPUT, V39, P997, DOI 10.1007/s00371-021-02380-3
   Jiao S., 2021, J PHYS C SERIES, V1881
   Juneja A, 2022, ARCH COMPUT METHOD E, V29, P1727, DOI 10.1007/s11831-021-09637-z
   Dhara SK, 2021, IEEE T CIRC SYST VID, V31, P2076, DOI 10.1109/TCSVT.2020.3007850
   Koschmeider H., 1924, Meteorologische Zeitschrift, V12
   Kuanar S, 2022, VISUAL COMPUT, V38, P1121, DOI 10.1007/s00371-021-02071-z
   Li BY, 2019, IEEE T IMAGE PROCESS, V28, P492, DOI 10.1109/TIP.2018.2867951
   Li X., 2022, VISUAL COMPUT, V116
   Liu W, 2021, IEEE T IMAGE PROCESS, V30, P176, DOI 10.1109/TIP.2020.3033402
   Liu W, 2020, IEEE T IMAGE PROCESS, V29, P7819, DOI 10.1109/TIP.2020.3007844
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Pietrasik T, 2021, ROAD TRAFFIC INJURIE
   Qin X, 2020, AAAI CONF ARTIF INTE, V34, P11908
   Raikwar SC, 2020, IEEE T IMAGE PROCESS, V29, P4832, DOI 10.1109/TIP.2020.2975909
   Scharstein D, 2014, LECT NOTES COMPUT SC, V8753, P31, DOI 10.1007/978-3-319-11752-2_3
   Sharma N, 2021, ARCH COMPUT METHOD E, V28, P4449, DOI 10.1007/s11831-021-09541-6
   Sharma SM., 2016, INT J ADV INTEGR MED, V1, P57, DOI DOI 10.5005/JP-JOURNALS-10050-10020
   Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54
   Singh Dilbag, 2019, Archives of Computational Methods in Engineering, V26, P1395, DOI 10.1007/s11831-018-9294-z
   Singh D, 2017, IMAGING SCI J, V65, P282, DOI 10.1080/13682199.2017.1329792
   Singh D, 2017, J MOD OPTIC, V64, P2165, DOI 10.1080/09500340.2017.1344736
   Tarel JP, 2012, IEEE INTEL TRANSP SY, V4, P6, DOI 10.1109/MITS.2012.2189969
   Venkatanath N, 2015, NATL CONF COMMUN
   Wang CL, 2011, ENTROPY-SWITZ, V13, P254, DOI 10.3390/e13010254
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wu HY, 2021, PROC CVPR IEEE, P10546, DOI 10.1109/CVPR46437.2021.01041
   Xia RL, 2022, J KING SAUD UNIV-COM, V34, P6008, DOI 10.1016/j.jksuci.2022.02.004
   Zhang SD, 2023, VISUAL COMPUT, V39, P953, DOI 10.1007/s00371-021-02377-y
   Zhang YF, 2017, IEEE IMAGE PROC, P3205, DOI 10.1109/ICIP.2017.8296874
NR 44
TC 2
Z9 2
U1 1
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2023
VL 39
IS 9
BP 3905
EP 3922
DI 10.1007/s00371-022-02534-x
EA JUN 2022
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA Q8PV6
UT WOS:000810352600004
DA 2024-07-18
ER

PT J
AU Manu, CM
   Sreeni, KG
AF Manu, Chippy M.
   Sreeni, K. G.
TI GANID: a novel generative adversarial network for image dehazing
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Image dehazing; Generative adversarial network (GAN); Dilated
   convolution; Deep learning
ID QUALITY ASSESSMENT; VISIBILITY; FOG
AB This paper presents a novel algorithm to dehaze a given hazy input image using a generative adversarial network (GAN). The proposed GAN structure uses a Feature Residual Dense combined Network (FRDN) as a generator and a Markovian discriminator (PatchGAN) with additional layers as the discriminator. FRDN is capable of extracting contextual information using Feature Module (FM) in conjunction with the Residual Dense Module, and IMCU enhances collaborative learning which enhances its performance. The inclusion of proposed reality and visibility loss functions along with L-1 loss improves the scene visibility and realness of the dehazed image. The network is trained with images from the benchmark datasets-RESIDE and NTIRE 2021. The proposed technique's performance is evaluated using various metrics such as PSNR, SSIM, FSIM, FADE, NIQE, and BRISQUE. An average PSNR of 25.701, 32.52, and 33.96 has been obtained with the Indoor Training Set, Synthetic Objective Testing Set indoor, and SOTS outdoor images, respectively. The experimental results reveal that the suggested method's performance is better compared to other state-of-the-art techniques.
C1 [Manu, Chippy M.; Sreeni, K. G.] APJA KTU, Dept Elect & Commun, Comp Vis Lab, Coll Engn, Thiruvananthapuram, Kerala, India.
C3 College of Engineering, Trivandrum
RP Manu, CM (corresponding author), APJA KTU, Dept Elect & Commun, Comp Vis Lab, Coll Engn, Thiruvananthapuram, Kerala, India.
EM chippyaicte@gmail.com; sreenikg79@gmail.com
CR Amaranageswarao G, 2022, VISUAL COMPUT, V38, P31, DOI 10.1007/s00371-020-01998-z
   Ancuti CO, 2021, IEEE COMPUT SOC CONF, P627, DOI 10.1109/CVPRW53098.2021.00074
   Arjovsky M, 2017, PR MACH LEARN RES, V70
   Bai HR, 2022, IEEE T IMAGE PROCESS, V31, P1217, DOI 10.1109/TIP.2022.3140609
   Cai BL, 2016, IEEE T IMAGE PROCESS, V25, P5187, DOI 10.1109/TIP.2016.2598681
   Chen DD, 2019, IEEE WINT CONF APPL, P1375, DOI 10.1109/WACV.2019.00151
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Choi LK, 2015, IEEE T IMAGE PROCESS, V24, P3888, DOI 10.1109/TIP.2015.2456502
   Dong H, 2020, PROC CVPR IEEE, P2154, DOI 10.1109/CVPR42600.2020.00223
   Dong Y, 2020, AAAI CONF ARTIF INTE, V34, P10729
   Engin D, 2018, IEEE COMPUT SOC CONF, P938, DOI 10.1109/CVPRW.2018.00127
   Fattal R, 2014, ACM T GRAPHIC, V34, DOI 10.1145/2651362
   Golts A, 2020, IEEE T IMAGE PROCESS, V29, P2692, DOI 10.1109/TIP.2019.2952032
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168
   He KM, 2013, IEEE T PATTERN ANAL, V35, P1397, DOI 10.1109/TPAMI.2012.213
   Hong M, 2020, PROC CVPR IEEE, P3459, DOI 10.1109/CVPR42600.2020.00352
   Hsieh PW, 2022, SIGNAL PROCESS, V192, DOI 10.1016/j.sigpro.2021.108396
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Lee CY, 2015, JMLR WORKSH CONF PRO, V38, P562
   Levin A, 2008, IEEE T PATTERN ANAL, V30, P228, DOI 10.1109/TPAMI.2007.1177
   Li BY, 2018, AAAI CONF ARTIF INTE, P7016
   Li H, 2017, IEEE I CONF COMP VIS, P5248, DOI 10.1109/ICCV.2017.560
   Li RD, 2018, PROC CVPR IEEE, P8202, DOI 10.1109/CVPR.2018.00856
   Liu W, 2020, IEEE T IMAGE PROCESS, V29, P7819, DOI 10.1109/TIP.2020.3007844
   Liu XF, 2019, IEEE I CONF COMP VIS, P4985, DOI 10.1109/ICCV.2019.00509
   Liu Z., 2018, COMPUTER VISION PATT
   Lu YF, 2021, VISUAL COMPUT, V37, P2513, DOI 10.1007/s00371-021-02204-4
   Ma TS, 2021, VISUAL COMPUT, V37, P925, DOI 10.1007/s00371-020-01843-3
   Mehta A, 2020, IEEE COMPUT SOC CONF, P846, DOI 10.1109/CVPRW50498.2020.00114
   Mirza M., 2014, ARXIV PREPRINT ARXIV, P2672, DOI 10.48550/arXiv.1411.1784
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Qin X, 2020, AAAI CONF ARTIF INTE, V34, P11908
   Qu YY, 2019, PROC CVPR IEEE, P8152, DOI 10.1109/CVPR.2019.00835
   Radford A., 2016, ICLR
   Ren WQ, 2016, LECT NOTES COMPUT SC, V9906, P154, DOI 10.1007/978-3-319-46475-6_10
   Soma P, 2022, VISUAL COMPUT, V38, P2569, DOI 10.1007/s00371-021-02132-3
   Sun ZY, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3478457
   Tan RT, 2008, PROC CVPR IEEE, P2347, DOI 10.1109/cvpr.2008.4587643
   Tan RT, 2007, 2007 IEEE INTELLIGENT VEHICLES SYMPOSIUM, VOLS 1-3, P435
   van Oldenborgh GJ, 2010, ATMOS CHEM PHYS, V10, P4597, DOI 10.5194/acp-10-4597-2010
   Wang C, 2021, VISUAL COMPUT, V37, P1851, DOI 10.1007/s00371-020-01944-z
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Yang F, 2022, VISUAL COMPUT, V38, P1579, DOI 10.1007/s00371-021-02089-3
   Yang XT, 2018, AAAI CONF ARTIF INTE, P7485
   Zhang H, 2020, IEEE T CIRC SYST VID, V30, P3943, DOI 10.1109/TCSVT.2019.2920407
   Zhang H, 2018, PROC CVPR IEEE, P3194, DOI 10.1109/CVPR.2018.00337
   Zhang L, 2011, IEEE T IMAGE PROCESS, V20, P2378, DOI 10.1109/TIP.2011.2109730
   Zhang WL, 2019, IEEE I CONF COMP VIS, P3096, DOI 10.1109/ICCV.2019.00319
   Zhang YL, 2018, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR.2018.00262
   Zhao SY, 2020, IEEE T IMAGE PROCESS, V29, P6947, DOI 10.1109/TIP.2020.2995264
   Zhou ZW, 2020, IEEE T MED IMAGING, V39, P1856, DOI 10.1109/TMI.2019.2959609
   Zhou ZW, 2018, LECT NOTES COMPUT SC, V11045, P3, DOI 10.1007/978-3-030-00889-5_1
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zhu QS, 2015, IEEE T IMAGE PROCESS, V24, P3522, DOI 10.1109/TIP.2015.2446191
NR 56
TC 2
Z9 2
U1 1
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2022 JUN 8
PY 2022
DI 10.1007/s00371-077-07516-9
EA JUN 2022
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 1Y2JT
UT WOS:000807970000001
DA 2024-07-18
ER

PT J
AU Gao, B
   Spratling, MW
AF Gao, Bo
   Spratling, Michael W.
TI Explaining away results in more robust visual tracking
SO VISUAL COMPUTER
LA English
DT Article
DE Object tracking; Tracking-by-Detection trackers; Distractor submission;
   Explaining away
ID OBJECT TRACKING; NETWORKS; MODEL
AB Many current trackers utilise an appearance model to localise the target object in each frame. However, such approaches often fail when there are similar-looking distractor objects in the surrounding background, meaning that target appearance alone is insufficient for robust tracking. In contrast, humans consider the distractor objects as additional visual cues, in order to infer the position of the target. Inspired by this observation, this paper proposes a novel tracking architecture in which not only is the appearance of the tracked object, but also the appearance of the distractors detected in previous frames, taken into consideration using a form of probabilistic inference known as explaining away. This mechanism increases the robustness of tracking by making it more likely that the target appearance model is matched to the true target, rather than similar-looking regions of the current frame. The proposed method can be combined with many existing trackers. Combining it with SiamFC, DaSiamRPN, Super_DiMP, and ARSuper_DiMP all resulted in an increase in the tracking accuracy compared to that achieved by the underlying tracker alone. When combined with Super_DiMP and ARSuper_DiMP, the resulting trackers produce performance that is competitive with the state of the art on seven popular benchmarks.
C1 [Gao, Bo; Spratling, Michael W.] Kings Coll London, Dept Informat, London, England.
C3 University of London; King's College London
RP Gao, B (corresponding author), Kings Coll London, Dept Informat, London, England.
EM bo.gao@kcl.ac.uk; michael.spratling@kcl.ac.uk
RI Spratling, Michael/G-7689-2011
OI Spratling, Michael/0000-0001-9531-2813
FU China Scholarship Council
FX This research was funded by China Scholarship Council.
CR Achler T, 2014, BIOL INSPIR COGN ARC, V9, P71, DOI 10.1016/j.bica.2014.07.001
   [Anonymous], 2016, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2016.465
   Bertinetto L, 2016, LECT NOTES COMPUT SC, V9914, P850, DOI 10.1007/978-3-319-48881-3_56
   Bhat G., 2020, ARXIV200311014
   Bhat G, 2019, IEEE I CONF COMP VIS, P6181, DOI 10.1109/ICCV.2019.00628
   Chen X, 2021, PROC CVPR IEEE, P8122, DOI 10.1109/CVPR46437.2021.00803
   Chen ZD, 2020, PROC CVPR IEEE, P6667, DOI 10.1109/CVPR42600.2020.00670
   Cho H, 2014, IEEE INT CONF ROBOT, P1836, DOI 10.1109/ICRA.2014.6907100
   Clark A, 2013, BEHAV BRAIN SCI, V36, P181, DOI 10.1017/S0140525X12000477
   Collins RT, 2005, IEEE T PATTERN ANAL, V27, P1631, DOI 10.1109/TPAMI.2005.205
   Cui Y., 2020, ARXIV200407109
   Danelljan M, 2019, PYTRACKING VISUAL TR
   Danelljan M, 2020, PROC CVPR IEEE, P7181, DOI 10.1109/CVPR42600.2020.00721
   Danelljan M, 2019, PROC CVPR IEEE, P4655, DOI 10.1109/CVPR.2019.00479
   Danelljan M, 2017, PROC CVPR IEEE, P6931, DOI 10.1109/CVPR.2017.733
   Devi RB, 2021, VISUAL COMPUT, V37, P1207, DOI 10.1007/s00371-020-01862-0
   Fan C., 2021, VISUAL COMPUT, P1
   Fan H, 2019, PROC CVPR IEEE, P5369, DOI 10.1109/CVPR.2019.00552
   Feria CS, 2012, PERCEPTION, V41, P287, DOI 10.1068/p7053
   Fu ZH, 2021, PROC CVPR IEEE, P13769, DOI 10.1109/CVPR46437.2021.01356
   Galoogahi HK, 2017, IEEE I CONF COMP VIS, P1144, DOI [10.1109/ICCV.2017.128, 10.1109/ICCV.2017.129]
   Gao B., 2020, ARXIV200715817
   Gao P, 2020, INFORM SCIENCES, V517, P52, DOI 10.1016/j.ins.2019.12.084
   Gladh S, 2016, INT C PATT RECOG, P1243, DOI 10.1109/ICPR.2016.7899807
   Guo DY, 2021, PROC CVPR IEEE, P9538, DOI 10.1109/CVPR46437.2021.00942
   Guo Q, 2017, IEEE I CONF COMP VIS, P1781, DOI 10.1109/ICCV.2017.196
   He A., 2018, P EUR C COMP VIS ECC
   Henriques JF, 2015, IEEE T PATTERN ANAL, V37, P583, DOI 10.1109/TPAMI.2014.2345390
   Huang LH, 2021, IEEE T PATTERN ANAL, V43, P1562, DOI 10.1109/TPAMI.2019.2957464
   Kalal Z, 2012, IEEE T PATTERN ANAL, V34, P1409, DOI 10.1109/TPAMI.2011.239
   Kersten D, 2004, ANNU REV PSYCHOL, V55, P271, DOI 10.1146/annurev.psych.55.090902.142005
   Kristan M., 2020, COMPUTER VISION ECCV, P547
   Kristan M., 2019, ICCVW
   Kristan M, 2019, LECT NOTES COMPUT SC, V11129, P3, DOI 10.1007/978-3-030-11009-3_1
   Li B, 2019, PROC CVPR IEEE, P4277, DOI 10.1109/CVPR.2019.00441
   Li B, 2018, PROC CVPR IEEE, P8971, DOI 10.1109/CVPR.2018.00935
   Lochmann T, 2012, J NEUROSCI, V32, P4179, DOI 10.1523/JNEUROSCI.0817-11.2012
   Lochmann T, 2011, CURR OPIN NEUROBIOL, V21, P774, DOI 10.1016/j.conb.2011.05.018
   Lukezic A, 2020, PROC CVPR IEEE, P7131, DOI 10.1109/CVPR42600.2020.00716
   Ma C, 2019, IEEE T PATTERN ANAL, V41, P2709, DOI [10.1109/TPAMI.2018.2865311, 10.1109/INTMAG.2018.8508195]
   Mangawati A, 2018, PROCEEDINGS OF THE 2018 IEEE INTERNATIONAL CONFERENCE ON COMMUNICATION AND SIGNAL PROCESSING (ICCSP), P667, DOI 10.1109/ICCSP.2018.8524260
   Mbelwa JT, 2020, VISUAL COMPUT, V36, P1173, DOI 10.1007/s00371-019-01727-1
   Mondragón IF, 2010, IEEE INT CONF ROBOT, P35, DOI 10.1109/ROBOT.2010.5509287
   Müller M, 2018, LECT NOTES COMPUT SC, V11205, P310, DOI 10.1007/978-3-030-01246-5_19
   Mueller M, 2016, LECT NOTES COMPUT SC, V9905, P445, DOI 10.1007/978-3-319-46448-0_27
   Pan Z, 2018, J PARALLEL DISTR COM, V120, P182, DOI 10.1016/j.jpdc.2018.06.012
   Prabhakar G, 2017, IEEE REGION 10 SYMP
   Rao RPN, 1999, NAT NEUROSCI, V2, P79, DOI 10.1038/4580
   Solbakken LL, 2011, 2011 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN), P1466, DOI 10.1109/IJCNN.2011.6033397
   Spratling MW, 2017, BRAIN COGNITION, V112, P92, DOI 10.1016/j.bandc.2015.11.003
   Spratling MW, 2020, PATTERN RECOGN, V104, DOI 10.1016/j.patcog.2020.107337
   Spratling MW, 2017, COGN COMPUT, V9, P151, DOI 10.1007/s12559-016-9445-1
   Spratling MW, 2016, COGN PROCESS, V17, P279, DOI 10.1007/s10339-016-0765-6
   Spratling MW, 2014, BIOL CYBERN, V108, P61, DOI 10.1007/s00422-013-0579-x
   Spratling M.W., 2009, COMPUT INTEL NEUROSC, V2009
   Spratling MW, 2013, IEEE T IMAGE PROCESS, V22, P1629, DOI 10.1109/TIP.2012.2235850
   Tan H., 2021, IEEE T IMAGE PROCESS
   Tarhan M, 2011, J INTELL ROBOT SYST, V61, P119, DOI 10.1007/s10846-010-9504-x
   Valmadre J, 2017, PROC CVPR IEEE, P5000, DOI 10.1109/CVPR.2017.531
   Vaswani A, 2017, ADV NEUR IN, V30
   Voigtlaender P, 2020, PROC CVPR IEEE, P6577, DOI 10.1109/CVPR42600.2020.00661
   Wang J., 2020, ARXIV200701120
   Wang N, 2021, PROC CVPR IEEE, P1571, DOI 10.1109/CVPR46437.2021.00162
   Wang Q, 2019, PROC CVPR IEEE, P1328, DOI 10.1109/CVPR.2019.00142
   Wang SL, 2018, PROC CVPR IEEE, P2589, DOI 10.1109/CVPR.2018.00274
   Wu Y, 2013, PROC CVPR IEEE, P2411, DOI 10.1109/CVPR.2013.312
   Xu T., 2020, ARXIV200513708
   Xu TY, 2021, INT J COMPUT VISION, V129, P1359, DOI 10.1007/s11263-021-01435-1
   Xu TY, 2020, IEEE T CIRC SYST VID, V30, P3727, DOI 10.1109/TCSVT.2019.2945068
   Xu TY, 2019, IEEE I CONF COMP VIS, P7949, DOI 10.1109/ICCV.2019.00804
   Xu TY, 2019, IEEE T IMAGE PROCESS, V28, DOI 10.1109/TIP.2019.2919201
   Xu YD, 2020, AAAI CONF ARTIF INTE, V34, P12549
   Xuan SY, 2021, PATTERN RECOGN, V112, DOI 10.1016/j.patcog.2020.107698
   Yan B, 2021, PROC CVPR IEEE, P5285, DOI 10.1109/CVPR46437.2021.00525
   Zhang KH, 2014, IEEE T PATTERN ANAL, V36, P2002, DOI 10.1109/TPAMI.2014.2315808
   Zhang ZP, 2019, PROC CVPR IEEE, P4586, DOI 10.1109/CVPR.2019.00472
   Zhang Zhipeng, 2020, ARXIV200802745
   Zhipeng Z., 2020, EUR C COMP VIS
   Zhou JH, 2020, AAAI CONF ARTIF INTE, V34, P13017
   Zhu XF, 2021, IEEE T CIRC SYST VID, V31, P557, DOI 10.1109/TCSVT.2020.2979480
   Zhu Z., 2018, 2018 IEEE International Magnetics Conference (INTERMAG), DOI 10.1109/INTMAG.2018.8508710
   Ziang Ma, 2020, Proceedings of the 16th European Conference on Computer Vision (ECCV 2020) Workshops. Lecture Notes in Computer Science (LNCS 12539), P653, DOI 10.1007/978-3-030-68238-5_43
NR 82
TC 2
Z9 2
U1 3
U2 9
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2023
VL 39
IS 5
BP 2081
EP 2095
DI 10.1007/s00371-022-02466-6
EA APR 2022
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA D7ZK3
UT WOS:000779470100001
OA hybrid
DA 2024-07-18
ER

PT J
AU Xu, J
   Liu, WB
   Xing, WW
   Wei, X
AF Xu, Jia
   Liu, Weibin
   Xing, Weiwei
   Wei, Xiang
TI MSPENet: multi-scale adaptive fusion and position enhancement network
   for human pose estimation
SO VISUAL COMPUTER
LA English
DT Article
DE Multi-scale feature fusion; Attention mechanism; Position enhancement;
   Keypoints refinement; Pose estimation
AB Human pose estimation is a fundamental yet challenging task in computer vision. Recently, with the involvement of deep neural networks, human pose estimation has made great progresses. However, existing pose estimation networks still have some difficulties in detecting small-scale keypoints and distinguishing semantic confusion keypoints. In this paper, a novel convolutional neural network named multi-scale position enhancement network is proposed to address the above two problems. First, a multi-scale adaptive fusion unit is proposed to dynamically choose and fuse features on different scales, allowing small-scale keypoints to obtain more detailed information that is beneficial for detection. Second, we discover that although appearance-similar parts are difficult to distinguish in semantics, they differ significantly in spatial location. Therefore, a position enhancement module is designed to highlight features of real joint locations while learning more discriminative features to suppress features of similar joint regions. Finally, a global context block is applied to optimize the prediction results in order to further improve the network performance. Experiments on both single- and multi-person pose estimation benchmarks illustrate that our approach yields more accurate and reliable results.
C1 [Xu, Jia; Liu, Weibin] Beijing Jiaotong Univ, Inst Informat Sci, Beijing 100044, Peoples R China.
   [Xing, Weiwei; Wei, Xiang] Beijing Jiaotong Univ, Sch Software Engn, Beijing 100044, Peoples R China.
C3 Beijing Jiaotong University; Beijing Jiaotong University
RP Liu, WB (corresponding author), Beijing Jiaotong Univ, Inst Informat Sci, Beijing 100044, Peoples R China.
EM wbliu@bjtu.edu.cn
OI Wei, Xiang/0000-0002-8967-6423
FU Beijing Natural Science Foundation [4212025]; National Natural Science
   Foundation of China [61876018, 61906014, 61976017]
FX This research is partially supported by the Beijing Natural Science
   Foundation (No. 4212025) and National Natural Science Foundation of
   China (No. 61876018, No. 61906014, No. 61976017).
CR Agahian S, 2019, VISUAL COMPUT, V35, P591, DOI 10.1007/s00371-018-1489-7
   Andriluka M, 2014, PROC CVPR IEEE, P3686, DOI 10.1109/CVPR.2014.471
   Andriluka M, 2009, PROC CVPR IEEE, P1014, DOI 10.1109/CVPRW.2009.5206754
   [Anonymous], 2018, ABS180309894 CORR
   Artacho B, 2020, PROC CVPR IEEE, P7033, DOI 10.1109/CVPR42600.2020.00706
   Belagiannis V, 2017, IEEE INT CONF AUTOMA, P468, DOI 10.1109/FG.2017.64
   Bin YR, 2020, PATTERN RECOGN, V106, DOI 10.1016/j.patcog.2020.107410
   Bulat A, 2020, IEEE INT CONF AUTOMA, P8, DOI 10.1109/FG47880.2020.00014
   Cao Y, 2019, IEEE INT CONF COMP V, P1971, DOI 10.1109/ICCVW.2019.00246
   Cao Z, 2017, PROC CVPR IEEE, P1302, DOI 10.1109/CVPR.2017.143
   Carreira J, 2016, PROC CVPR IEEE, P4733, DOI 10.1109/CVPR.2016.512
   Chen YL, 2018, PROC CVPR IEEE, P7103, DOI 10.1109/CVPR.2018.00742
   Chen YH, 2017, AIP CONF PROC, V1812, DOI [10.1063/1.4975898, 10.1109/ICCV.2017.137]
   Chu X., 2017, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, P1831, DOI DOI 10.1109/CVPR.2017.601
   Das Dawn D, 2016, VISUAL COMPUT, V32, P289, DOI 10.1007/s00371-015-1066-2
   Everingham M., 2010, BMVC, V2, P5
   Fang HS, 2017, IEEE I CONF COMP VIS, P2353, DOI 10.1109/ICCV.2017.256
   Gao G., ABS210313851 CORR
   Gao GW, 2017, PATTERN RECOGN, V66, P129, DOI 10.1016/j.patcog.2016.12.021
   He K., 2016, PROC CVPR IEEE, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]
   He KM, 2020, IEEE T PATTERN ANAL, V42, P386, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   HU J, 2011, IEEE T PATTERN ANAL, V2023
   Huang JJ, 2020, PROC CVPR IEEE, P5699, DOI 10.1109/CVPR42600.2020.00574
   Insafutdinov E, 2016, LECT NOTES COMPUT SC, V9910, P34, DOI 10.1007/978-3-319-46466-4_3
   Jiang T, 2019, VISUAL COMPUT, V35, P1655, DOI 10.1007/s00371-018-1565-z
   Kreiss S, 2019, PROC CVPR IEEE, P11969, DOI 10.1109/CVPR.2019.01225
   Lifshitz I, 2016, LECT NOTES COMPUT SC, V9906, P246, DOI 10.1007/978-3-319-46475-6_16
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu ZY, 2021, VISUAL COMPUT, V37, P529, DOI 10.1007/s00371-020-01821-9
   Moon G., 2019, ABS190503912 CORR
   Moon G, 2019, PROC CVPR IEEE, P7765, DOI 10.1109/CVPR.2019.00796
   Newell A, 2016, LECT NOTES COMPUT SC, V9912, P483, DOI 10.1007/978-3-319-46484-8_29
   Nie XC, 2018, LECT NOTES COMPUT SC, V11209, P519, DOI 10.1007/978-3-030-01228-1_31
   Nie XC, 2018, PROC CVPR IEEE, P2100, DOI 10.1109/CVPR.2018.00224
   Ning GH, 2018, IEEE T MULTIMEDIA, V20, P1246, DOI 10.1109/TMM.2017.2762010
   Papandreou G, 2017, PROC CVPR IEEE, P3711, DOI 10.1109/CVPR.2017.395
   Pishchulin L, 2016, PROC CVPR IEEE, P4929, DOI 10.1109/CVPR.2016.533
   Ryou S, 2019, IEEE I CONF COMP VIS, P5991, DOI 10.1109/ICCV.2019.00609
   Sapp B, 2013, PROC CVPR IEEE, P3674, DOI 10.1109/CVPR.2013.471
   Seo S, 2020, MULTIMED TOOLS APPL
   Su K, 2019, PROC CVPR IEEE, P5667, DOI 10.1109/CVPR.2019.00582
   Tang W, 2018, LECT NOTES COMPUT SC, V11207, P197, DOI 10.1007/978-3-030-01219-9_12
   Tian L, 2021, PATTERN RECOGN, V115, DOI 10.1016/j.patcog.2021.107863
   Tompson J, 2014, ADV NEUR IN, V27
   Toshev A, 2014, PROC CVPR IEEE, P1653, DOI 10.1109/CVPR.2014.214
   Vidanpathirana M, 2020, VISUAL COMPUT, V36, P1501, DOI 10.1007/s00371-019-01757-9
   Vishwakarma S, 2013, VISUAL COMPUT, V29, P983, DOI 10.1007/s00371-012-0752-6
   Wang F, 2017, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2017.683
   Wang Jiangliu, 2020, ECCV, P504, DOI DOI 10.1007/978-3-030-58520-430
   Wang KK, 2021, VISUAL COMPUT, V37, P603, DOI 10.1007/s00371-020-01826-4
   Wei SE, 2016, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2016.511
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Xiao B, 2018, LECT NOTES COMPUT SC, V11210, P472, DOI 10.1007/978-3-030-01231-1_29
   Yang QN, 2022, VISUAL COMPUT, V38, P2447, DOI 10.1007/s00371-021-02122-5
   Yang W, 2017, IEEE I CONF COMP VIS, P1290, DOI 10.1109/ICCV.2017.144
   Zhang F, 2019, IEEE I CONF COMP VIS, P6797, DOI 10.1109/ICCV.2019.00690
   Zhang H, 2019, ABS190101760 CORR
NR 57
TC 5
Z9 5
U1 6
U2 30
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2023
VL 39
IS 5
BP 2005
EP 2019
DI 10.1007/s00371-022-02460-y
EA APR 2022
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA D7ZK3
UT WOS:000777427600002
DA 2024-07-18
ER

PT J
AU Wang, PF
   Li, YQ
   Sun, YR
   He, DZ
   Wang, ZQ
AF Wang, Pengfei
   Li, Yunqi
   Sun, Yaru
   He, Dongzhi
   Wang, Zhiqiang
TI Multi-scale boundary neural network for gastric tumor segmentation
SO VISUAL COMPUTER
LA English
DT Article
DE Gastric tumor segmentation; Encoder-decoder; Convolutional neural
   network; Deep learning
ID CANCER
AB At present, gastric cancer patients account for a large proportion of all tumor patients. Gastric tumor image segmentation can provide a reliable additional basis for the clinical analysis and diagnosis of gastric cancer. However, the existing gastric cancer image datasets have disadvantages such as small data sizes and difficulty in labeling. Moreover, most existing CNN-based methods are unable to generate satisfactory segmentation masks without accurate labels, which are due to the limited context information and insufficient discriminative feature maps obtained after the consecutive pooling and convolution operations. This paper presents a gastric cancer lesion dataset for gastric tumor image segmentation research. A multiscale boundary neural network (MBNet) is proposed to automatically segment the real tumor area in gastric cancer images. MBNet adopts encoder-decoder architecture. In each stage of the encoder, a boundary extraction refinement module is proposed for obtaining multi granular edge information and refinement firstly. Then, we build a selective fusion module to selectively fuse features from the different stages. By cascading the two modules, the richer context and fine-grained features of each stage are encoded. Finally, the astrous spatial pyramid pooling is improved to obtain the remote dependency relationship of the overall context and the fine spatial structure information. The experimental results show that the accuracy of the model reaches 92.3%, the similarity coefficient (DICE) reaches 86.9%, and the performance of the proposed method on the CVC-ClinicDB and Kvasir-SEG datasets also outperforms existing approaches.
C1 [Wang, Pengfei; Sun, Yaru; He, Dongzhi] Beijing Univ Technol, Fac Informat Technol, Beijing, Peoples R China.
   [Li, Yunqi] Chinese Peoples Liberat Army Gen Hosp, Med Ctr 2, Dept Gastroenterol, Beijing 100853, Peoples R China.
   [Li, Yunqi] Chinese Peoples Liberat Army Gen Hosp, Natl Clin Res Ctr Geriatr Dis, Beijing 100853, Peoples R China.
   [Wang, Zhiqiang] Chinese Peoples Liberat Army Gen Hosp, Med Ctr 1, Dept Gastroenterol, Beijing 100853, Peoples R China.
C3 Beijing University of Technology; Chinese People's Liberation Army
   General Hospital; Chinese People's Liberation Army General Hospital;
   Chinese People's Liberation Army General Hospital
RP He, DZ (corresponding author), Beijing Univ Technol, Fac Informat Technol, Beijing, Peoples R China.; Wang, ZQ (corresponding author), Chinese Peoples Liberat Army Gen Hosp, Med Ctr 1, Dept Gastroenterol, Beijing 100853, Peoples R China.
EM victor@bjut.edu.cn; wzq301@263.net
RI CHEN, AN/KFT-3370-2024; Wang, Pengfei/Y-4426-2018; wang,
   zhiqiang/J-9816-2014
FU National Key R&D Program of China [2017YFB0403801]; NaturalNational
   Science Foundation of China (NSFC) [61835015]
FX This work was supported by the National Key R&D Program of China
   (2017YFB0403801), the NaturalNational Science Foundation of China (NSFC)
   (61835015).
CR [Anonymous], 2008, LECT NOTES MATH, V1920, DOI [10.1007/978-3-540-74798-7_4, DOI 10.1007/978-3-540-74798-7_4]
   Banks M, 2019, GUT, V68, P1545, DOI 10.1136/gutjnl-2018-318126
   Bernal J, 2015, COMPUT MED IMAG GRAP, V43, P99, DOI 10.1016/j.compmedimag.2015.02.007
   Chen J., 2021, Transunet: transformers make strong encoders for medical image segmentation
   Chen LB, 2017, IEEE INT SYMP NANO, P1, DOI 10.1109/NANOARCH.2017.8053709
   Chen X, 2019, PROC CVPR IEEE, P11624, DOI 10.1109/CVPR.2019.01190
   Cherukuri V, 2020, IEEE T IMAGE PROCESS, V29, P2552, DOI 10.1109/TIP.2019.2946078
   Dalca AV, 2018, PROC CVPR IEEE, P9290, DOI 10.1109/CVPR.2018.00968
   Drozdzal M, 2018, MED IMAGE ANAL, V44, P1, DOI 10.1016/j.media.2017.11.005
   Esteva A, 2017, NATURE, V542, P115, DOI 10.1038/nature21056
   Fitzmaurice C, 2018, J CLIN ONCOL, V36, DOI 10.1200/JCO.2018.36.15_suppl.1568
   Fu J, 2021, IEEE T NEUR NET LEAR, V32, P2547, DOI 10.1109/TNNLS.2020.3006524
   Gu ZW, 2019, IEEE T MED IMAGING, V38, P2281, DOI 10.1109/TMI.2019.2903562
   Guo Xin-Meng, 2021, Sichuan Da Xue Xue Bao Yi Xue Ban, V52, P166, DOI 10.12182/20210360501
   Horiuchi Y, 2020, DIGEST DIS SCI, V65, P1355, DOI 10.1007/s10620-019-05862-6
   Hsiao YJ, 2021, WORLD J GASTROENTERO, V27, DOI 10.3748/wjg.v27.i22.2979
   Hu SY, 2001, IEEE T MED IMAGING, V20, P490, DOI 10.1109/42.929615
   Huang HM, 2020, INT CONF ACOUST SPEE, P1055, DOI [10.1109/ICASSP40776.2020.9053405, 10.1109/icassp40776.2020.9053405]
   Hudler P, 2015, WORLD J GASTROENTERO, V21, P10510, DOI 10.3748/wjg.v21.i37.10510
   Isensee F, 2021, NAT METHODS, V18, P203, DOI 10.1038/s41592-020-01008-z
   Jemal A, 2011, CA-CANCER J CLIN, V61, P134, DOI [10.3322/caac.21492, 10.3322/caac.20107, 10.3322/caac.20115]
   Jha D, 2020, LECT NOTES COMPUT SC, V11962, P451, DOI 10.1007/978-3-030-37734-2_37
   Kumar N, 2017, IEEE T MED IMAGING, V36, P1550, DOI 10.1109/TMI.2017.2677499
   Lee JH, 2019, SURG ENDOSC, V33, P3790, DOI 10.1007/s00464-019-06677-2
   Lee SA, 2021, IEEE ACCESS, V9, P51847, DOI 10.1109/ACCESS.2021.3069747
   Li L, 2020, GASTRIC CANCER, V23, P126, DOI 10.1007/s10120-019-00992-2
   Liu XQ, 2018, IEEE IMAGE PROC, P1388, DOI 10.1109/ICIP.2018.8451067
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Lyu CZ, 2022, VISUAL COMPUT, V38, P345, DOI 10.1007/s00371-020-02018-w
   Ma WA, 2019, LECT NOTES COMPUT SC, V11764, P769, DOI 10.1007/978-3-030-32239-7_85
   McInerney T, 1996, Med Image Anal, V1, P91, DOI 10.1016/S1361-8415(96)80007-7
   Nguyen TN, 2020, IEEE ICC, DOI 10.1109/icc40277.2020.9148850
   Oktay O., 2018, ARXIV, DOI DOI 10.48550/ARXIV.1804.03999
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Sakai Y, 2018, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS.2018.8351535
   Sánchez-González A, 2018, COMPUT BIOL MED, V100, P152, DOI 10.1016/j.compbiomed.2018.07.002
   Sharma N, 2010, J MED PHYS, V35, P3, DOI 10.4103/0971-6203.58777
   Sinha A, 2021, IEEE J BIOMED HEALTH, V25, P121, DOI 10.1109/JBHI.2020.2986926
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang D, 2021, VISUAL COMPUT, V37, P1101, DOI 10.1007/s00371-020-01855-z
   Wang H, 2019, INT J PROD RES, V57, P6795, DOI 10.1080/00207543.2018.1464232
   Wang J, 2021, COMPUT METH PROG BIO, V207, DOI 10.1016/j.cmpb.2021.106210
   Yu LQ, 2017, IEEE T MED IMAGING, V36, P994, DOI 10.1109/TMI.2016.2642839
   Zhao A, 2019, PROC CVPR IEEE, P8535, DOI 10.1109/CVPR.2019.00874
   Zhiheng C., 2021, CHINA DIGIT MED, V16, P7
   Zhou Zongwei, 2018, Deep Learn Med Image Anal Multimodal Learn Clin Decis Support (2018), V11045, P3, DOI [10.1007/978-3-030-00889-5_1, 10.1007/978-3-030-00689-1_1]
   Zhu Y, 2019, GASTROINTEST ENDOSC, V89, P806, DOI 10.1016/j.gie.2018.11.011
NR 47
TC 2
Z9 2
U1 1
U2 22
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2023
VL 39
IS 3
BP 915
EP 926
DI 10.1007/s00371-021-02374-1
EA JAN 2022
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 9E6QG
UT WOS:000749034400001
DA 2024-07-18
ER

PT J
AU Sun, B
   Wu, Y
   Zhao, YJ
   Hao, Z
   Yu, LJ
   He, J
AF Sun, Bo
   Wu, Yong
   Zhao, Yijia
   Hao, Zhuo
   Yu, Lejun
   He, Jun
TI Cross-language multimodal scene semantic guidance and leap sampling for
   video captioning
SO VISUAL COMPUTER
LA English
DT Article
DE Video caption; Scene semantic guidance; Cross-language; Leap sampling;
   Multilingual student behavior caption dataset
ID IMAGE
AB In recent years, video captioning, which uses natural language to describe video content, has achieved encouraging results. However, most of the previous studies in this area have focused on directly decoding video encoding and have thus rarely explored the role of scene semantics in caption generation, especially cross-language and multimodal. Obviously, the same video can be described with different languages, which have different forms and are inherently related. Meanwhile, despite high evaluation scores, some generated captions cannot represent the video content with many nonentity words. Based on the analysis, in this paper, we propose a cross-language scene semantic guidance caption model. It first learns the high-level scene semantics of a video in different languages, from which multilanguage features are extracted. Then, the features characterize the video content and guide the generated captions. They make the captions converge toward the video content. In addition, we also apply a leap sampling method for learning entity words in the model so as to better represent the video content. Moreover, experiments on the public MSR-VTT and VATEX datasets show that our model is effective. Finally, we establish a multilingual student classroom behavior caption dataset under an education scenario, providing a basis for research into captioning tasks in the education area. We also apply our model to this dataset and achieve certain results. The dataset is available to download online: https://github.com/BNU-Wu/Student-Class-Behavior-Dataset/tree/master.
C1 [Sun, Bo; Wu, Yong; Zhao, Yijia; Hao, Zhuo; Yu, Lejun; He, Jun] Beijing Normal Univ, Coll Educ Future, Engn Res Ctr Intelligent Technol & Educ Applicat, Minist Educ,Sch Artificial Intelligence, Xinjiekouwai St 19, Beijing, Peoples R China.
C3 Beijing Normal University
RP He, J (corresponding author), Beijing Normal Univ, Coll Educ Future, Engn Res Ctr Intelligent Technol & Educ Applicat, Minist Educ,Sch Artificial Intelligence, Xinjiekouwai St 19, Beijing, Peoples R China.
EM hejun@bnu.edu.cn
OI He, Jun/0000-0002-3017-2108
FU National Science Foundation of China [62077009, 62177006]; Zhuhai
   Science and Technology Planning Project [ZH22036201210161PWC]
FX This work is supported by the National Science Foundation of China
   (Grant No. 62077009, 62177006) and Zhuhai Science and Technology
   Planning Project (Grant No. ZH22036201210161PWC).
CR Aafaq N, 2020, ACM COMPUT SURV, V52, DOI 10.1145/3355390
   [Anonymous], 2016, ARXIV PREPRINT ARXIV
   Barlas G, 2021, VISUAL COMPUT, V37, P1309, DOI 10.1007/s00371-020-01867-9
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chen D., 2011, P 49 ANN M ASS COMP, P190
   Chen Haoran, 2019, ARXIV190900121
   Chen SX, 2019, AAAI CONF ARTIF INTE, P8191
   Das P, 2013, PROC CVPR IEEE, P2634, DOI 10.1109/CVPR.2013.340
   Denkowski Michael, 2014, P 9 WORKSH STAT MACH, DOI [10.3115/v1/W14-3348, DOI 10.3115/V1/W14-3348]
   Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878
   Dong JF, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P1082, DOI 10.1145/2964284.2984064
   Feng BL, 2011, VISUAL COMPUT, V27, P21, DOI 10.1007/s00371-010-0510-6
   Gan Z, 2017, PROC CVPR IEEE, P1141, DOI 10.1109/CVPR.2017.127
   Guadarrama S, 2013, IEEE I CONF COMP VIS, P2712, DOI 10.1109/ICCV.2013.337
   He J, 2020, IET IMAGE PROCESS, V14, P3021, DOI 10.1049/iet-ipr.2019.1317
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hershey S, 2017, INT CONF ACOUST SPEE, P131, DOI 10.1109/ICASSP.2017.7952132
   Hosseinabad SH, 2021, VISUAL COMPUT, V37, P119, DOI 10.1007/s00371-019-01786-4
   Jiang T, 2019, VISUAL COMPUT, V35, P1655, DOI 10.1007/s00371-018-1565-z
   Krishna R, 2017, IEEE I CONF COMP VIS, P706, DOI 10.1109/ICCV.2017.83
   Krishnamoorthy N., 2013, P AAAI C ART INT, P541
   Li Y, 2018, PROC CVPR IEEE, P7492, DOI 10.1109/CVPR.2018.00782
   Lin CY, 2004, ROUGE: A Package for Automatic Evaluation of Summaries, P74, DOI DOI 10.1253/JCJ.34.1213
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu W, 2016, IEEE MULTIMEDIA, V23, P75, DOI 10.1109/MMUL.2016.39
   Longteng Guo, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10324, DOI 10.1109/CVPR42600.2020.01034
   Luowei Zhou, 2019, 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P6571, DOI 10.1109/CVPR.2019.00674
   Ma L, 2016, AAAI CONF ARTIF INTE, P3567
   Ma L, 2015, IEEE I CONF COMP VIS, P2623, DOI 10.1109/ICCV.2015.301
   Marwah T, 2017, IEEE I CONF COMP VIS, P1435, DOI 10.1109/ICCV.2017.159
   Olivastri S., 2019, P IEEE INT C COMP VI, P2993
   Pan PB, 2016, PROC CVPR IEEE, P1029, DOI 10.1109/CVPR.2016.117
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135
   Pasunuru R, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P1911
   Pasunuru R, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P1273, DOI 10.18653/v1/P17-1117
   Pei WJ, 2019, PROC CVPR IEEE, P8339, DOI 10.1109/CVPR.2019.00854
   Pradhan J, 2020, VISUAL COMPUT, V36, P1847, DOI 10.1007/s00371-019-01773-9
   Rohrbach A, 2015, PROC CVPR IEEE, P3202, DOI 10.1109/CVPR.2015.7298940
   Rohrbach A, 2014, LECT NOTES COMPUT SC, V8753, P184, DOI 10.1007/978-3-319-11752-2_15
   Rohrbach M, 2012, LECT NOTES COMPUT SC, V7572, P144, DOI 10.1007/978-3-642-33718-5_11
   Rohrbach M, 2012, PROC CVPR IEEE, P1194, DOI 10.1109/CVPR.2012.6247801
   Shen ZQ, 2017, PROC CVPR IEEE, P5159, DOI 10.1109/CVPR.2017.548
   Shizhe Chen, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9959, DOI 10.1109/CVPR42600.2020.00998
   Shizhe Chen, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10635, DOI 10.1109/CVPR42600.2020.01065
   Song JK, 2018, PATTERN RECOGN, V75, P175, DOI 10.1016/j.patcog.2017.03.021
   Sun B, 2021, NEURAL COMPUT APPL, V33, P8335, DOI 10.1007/s00521-020-05587-y
   Thomason J., 2014, COLING, P1218
   Torabi Atousa., 2015, ARXIV150301070
   Vedantam R, 2015, PROC CVPR IEEE, P4566, DOI 10.1109/CVPR.2015.7299087
   Venugopalan S., 2016, EMNLP, P1961
   Venugopalan S, 2015, IEEE I CONF COMP VIS, P4534, DOI 10.1109/ICCV.2015.515
   Wang BR, 2018, PROC CVPR IEEE, P7622, DOI 10.1109/CVPR.2018.00795
   Wang JD, 2018, IEEE T PATTERN ANAL, V40, P769, DOI 10.1109/TPAMI.2017.2699960
   Wang J, 2016, P IEEE, V104, P34, DOI 10.1109/JPROC.2015.2487976
   Wang X., 2018, ARXIV180405448
   Wang X., 2018, ARXIV180905724CSAI
   Wang X, 2019, IEEE I CONF COMP VIS, P4580, DOI 10.1109/ICCV.2019.00468
   Xu J, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P537, DOI 10.1145/3123266.3123448
   Xu J, 2016, PROC CVPR IEEE, P5288, DOI 10.1109/CVPR.2016.571
   Yang S., 2019, ARXIV191112018
   Yang SB, 2019, PROC CVPR IEEE, P4140, DOI 10.1109/CVPR.2019.00427
   Yao L, 2015, IEEE I CONF COMP VIS, P4507, DOI 10.1109/ICCV.2015.512
   Yu HN, 2016, PROC CVPR IEEE, P4584, DOI 10.1109/CVPR.2016.496
   Yu Y, 2017, PROC CVPR IEEE, P3261, DOI 10.1109/CVPR.2017.347
   Zhang HJ, 2020, NEURAL COMPUT APPL, V32, P4519, DOI [10.1007/s00521-018-3579-x, 10.1007/s00521-018-3691-y]
   Zheng Q., 2020, P IEEE CVF C COMP VI, P13096
   Zhou LW, 2018, AAAI CONF ARTIF INTE, P7590
   Ziqi Zhang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13275, DOI 10.1109/CVPR42600.2020.01329
NR 68
TC 3
Z9 3
U1 1
U2 21
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2023
VL 39
IS 1
BP 9
EP 25
DI 10.1007/s00371-021-02309-w
EA JAN 2022
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA F6UJ1
UT WOS:000741976400002
DA 2024-07-18
ER

PT J
AU Li, W
   Hahn, JK
AF Li, Wei
   Hahn, James K.
TI Efficient ray casting polygonized isosurface of binary volumes
SO VISUAL COMPUTER
LA English
DT Article
DE Rendering; Ray tracing; Volumetric models; Constructive solid geometry
ID ALGORITHM
AB In this paper, we propose an efficient grid-based ray casting method for rendering polygonized isosurfaces of binary volumes as a special form of voxel art. We utilize an auxiliary multi-level data structure, the region index layer, to accelerate empty space leaping. We also construct the polygonal surfaces within each cell as constructive solid geometry (CSG) combinations of half-space subvolumes to reduce boundary checks in the intersection test. We describe the construction method of the region index layer and CSG cube configurations and demonstrate how to utilize these data structures to enhance the performance of the ray casting algorithms.
C1 [Li, Wei; Hahn, James K.] George Washington Univ, Washington, DC 20037 USA.
C3 George Washington University
RP Li, W (corresponding author), George Washington Univ, Washington, DC 20037 USA.
EM gw_liwei@gwmail.gwu.edu; hahn@email.gwu.edu
FU National Science Foundation Major Research Instrumentation Grant
   [CNS-1337722]
FX This project is funded in part by National Science Foundation Major
   Research Instrumentation Grant CNS-1337722: Development of Large-Scale
   Dense Scene Capture and Tracking Instrument.
CR Amanatides J., 1987, EUROGRAPHICS, P3, DOI DOI 10.2312/EGTP.19871000
   [Anonymous], 2018, KCHAPELIER CELLULAR
   Barber CB, 1996, ACM T MATH SOFTWARE, V22, P469, DOI 10.1145/235815.235821
   Benson D, 2002, ACM T GRAPHIC, V21, P785, DOI 10.1145/566570.566652
   Bhaniramka P, 2004, IEEE T VIS COMPUT GR, V10, P130, DOI 10.1109/TVCG.2004.1260765
   Chernyaev E., 1995, Technical report
   Christensen P.H., 2004, Proc. Eurographics Symposium on Rendering, P133, DOI [DOI 10.2312/EGWR/EGSR04/133-141, 10.5555/2383533.2383551, DOI 10.5555/2383533.2383551]
   COHEN D, 1994, VISUAL COMPUT, V11, P27, DOI 10.1007/BF01900824
   Crassin C., 2009, P 2009 S INT 3D GRAP, P15, DOI [10.1145/1507149.1507152, DOI 10.1145/1507149.1507152]
   de Araújo BR, 2015, ACM COMPUT SURV, V47, DOI 10.1145/2732197
   de Figueiredo LH, 2004, NUMER ALGORITHMS, V37, P147, DOI 10.1023/B:NUMA.0000049462.70970.b6
   Dietrich CA, 2009, IEEE T VIS COMPUT GR, V15, P150, DOI 10.1109/TVCG.2008.60
   DUFF T, 1992, COMP GRAPH, V26, P131, DOI 10.1145/142920.134027
   Frisken S.F., 2002, J. Graph. Tools, V7, P1, DOI DOI 10.1080/10867651.2002.10487560
   Fujimoto A., 1985, P COMPUTER GRAPHICS, P41
   Fujishiro I, 1996, IEEE T VIS COMPUT GR, V2, P144, DOI 10.1109/2945.506226
   Fujishiro I, 1995, VISUALIZATION '95 - PROCEEDINGS, P151, DOI 10.1109/VISUAL.1995.480807
   Gumin, 2018, WAVE FUNCTION COLLAP
   Hart JC, 1996, VISUAL COMPUT, V12, P527, DOI 10.1007/s003710050084
   Jackson JC, 2018, PROCEEDINGS OF THE TWELFTH INTERNATIONAL CONFERENCE ON TANGIBLE, EMBEDDED, AND EMBODIED INTERACTION (TEI'18), P592, DOI 10.1145/3173225.3173313
   Ju T, 2002, ACM T GRAPHIC, V21, P339
   Kalra D., 1989, Computer Graphics, V23, P297, DOI 10.1145/74334.74364
   Laine S., 2010, Efficient sparse voxel octrees-analysis, extensions, and implementation
   Laine S, 2011, IEEE T VIS COMPUT GR, V17, P1048, DOI 10.1109/TVCG.2010.240
   Lefebvre S, 2006, ACM T GRAPHIC, V25, P579, DOI 10.1145/1141911.1141926
   Lewiner T., 2003, Journal of Graphics Tools, V8, P1, DOI 10.1080/10867651.2003.10487582
   Li W, 2020, IEEE WINT CONF APPL, P390, DOI [10.1109/wacv45572.2020.9093533, 10.1109/WACV45572.2020.9093533]
   Li W, 2019, IEEE WINT CONF APPL, P1413, DOI 10.1109/WACV.2019.00155
   Lopes A, 2003, IEEE T VIS COMPUT GR, V9, P16, DOI 10.1109/TVCG.2003.1175094
   Lorensen William E., 1987, COMPUT GRAPH, P163, DOI DOI 10.1145/37402.37422
   McGuire, 2018, Journal of Computer Graphics Techniques, V7, P66
   MITCHELL DP, 1990, GRAPH INTER, P68
   Montani C., 1994, Visual Computer, V10, P353, DOI 10.1007/BF01900830
   Moore R. E., 1966, INTERVAL ANAL
   Museth K, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2487228.2487235
   Newman TS, 2006, COMPUT GRAPH-UK, V30, P854, DOI 10.1016/j.cag.2006.07.021
   PASKO A, 1995, VISUAL COMPUT, V11, P429, DOI 10.1007/BF02464333
   Revelles J., 2000, EFFICIENT PARAMETRIC
   ROTH SD, 1982, COMPUT VISION GRAPH, V18, P109, DOI 10.1016/0146-664X(82)90169-1
   Tevs A, 2008, I3D 2008: SYMPOSIUM ON INTERACTIVE 3D GRAPHICS AND GAMES, PROCEEDINGS, P183
   Woop S., 2013, J COMPUT GRAPH TECH, V2, P65
   Wyvill B, 1999, COMPUT GRAPH FORUM, V18, P149, DOI 10.1111/1467-8659.00365
   WYVILL G, 1986, IEEE COMPUT GRAPH, V6, P28, DOI 10.1109/MCG.1986.276717
   Wyvill G., 2005, VISUAL COMPUT, V1, P3, DOI [10.1007/BF01901265, DOI 10.1007/BF01901265]
NR 44
TC 2
Z9 2
U1 1
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2021
VL 37
IS 12
SI SI
BP 3139
EP 3149
DI 10.1007/s00371-021-02302-3
EA OCT 2021
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP1DP
UT WOS:000705827500002
DA 2024-07-18
ER

PT J
AU Xiong, YH
   Lin, ZX
   Li, GQ
   Xian, CH
   Peng, CX
AF Xiong, Yunhui
   Lin, Zuxuan
   Li, Guiqing
   Xian, Chuhua
   Peng, Changxin
TI Camera focal length from distances in a single image
SO VISUAL COMPUTER
LA English
DT Article
DE Focal length estimation; Camera calibration; Distance information;
   Calibration from a single image
ID GLOBAL OPTIMIZATION; CALIBRATION
AB Camera focal length estimation from a single image is of great importance for many computer vision tasks. Unfortunately, previous methods cannot achieve satisfactory accuracy yet. This paper proposes a focal length estimation approach based on distances among four points in the scene of a single image. The issue is cast into a nonlinear optimization by adapting a standard pinhole camera model under the constraints of distance information. Multiple algorithms are employed to solve the optimization, and the best solution is then regarded as the final solution. Experimental results show that our method is able to obtain a more accurate focal length than some state of the art in a single image setting. In addition, we provide some simple application examples and show the intuitive effects of focal length estimation errors. We also demonstrate experimentally that distance information has an improved meaning for the solution of the focal length.
C1 [Xiong, Yunhui; Lin, Zuxuan] South China Univ Technol, Sch Math, Guangzhou 510006, Peoples R China.
   [Li, Guiqing; Xian, Chuhua] South China Univ Technol, Sch Comp Sci & Engn, Guangzhou 510006, Peoples R China.
   [Peng, Changxin] South China Univ Technol, Sch Architecture, Guangzhou 510006, Peoples R China.
C3 South China University of Technology; South China University of
   Technology; South China University of Technology
RP Li, GQ (corresponding author), South China Univ Technol, Sch Comp Sci & Engn, Guangzhou 510006, Peoples R China.
EM yhxiong@scut.edu.cn; mazxlin@mail.scut.edu.cn; ligq@scut.edu.cn;
   chhxian@scut.edu.cn; arcxpeng@scut.edu.cn
CR Abbas A, 2019, IEEE INT CONF COMP V, P4095, DOI 10.1109/ICCVW.2019.00504
   Avila L, 2010, ADV VISUAL COMPUTING
   Barreto JP, 2006, COMPUT VIS IMAGE UND, V103, P208, DOI 10.1016/j.cviu.2006.06.003
   Bogdan O, 2018, PROCEEDINGS CVMP 2018: THE 15TH ACM SIGGRAPH EUROPEAN CONFERENCE ON VISUAL MEDIA PRODUCTION, DOI 10.1145/3278471.3278479
   CAPRILE B, 1990, INT J COMPUT VISION, V4, P127, DOI 10.1007/BF00127813
   Chen HT, 2017, IEEE T CIRC SYST VID, V27, P2555, DOI 10.1109/TCSVT.2016.2595319
   Conn A., 2000, MPS/SIAM Series on Optimization, DOI DOI 10.1137/1.9780898719857
   Coughlan J. M., 1999, P IEEE INT C COMPUTE, DOI DOI 10.1109/ICCV.1999.790349
   Godard C, 2019, IEEE I CONF COMP VIS, P3827, DOI 10.1109/ICCV.2019.00393
   Gopalan R, 2011, IEEE I CONF COMP VIS, P999, DOI 10.1109/ICCV.2011.6126344
   Gordon A, 2019, IEEE I CONF COMP VIS, P8976, DOI 10.1109/ICCV.2019.00907
   HESTENES MR, 1952, J RES NAT BUR STAND, V49, P409, DOI 10.6028/jres.049.044
   Hold-Geoffroy Y, 2018, PROC CVPR IEEE, P2354, DOI 10.1109/CVPR.2018.00250
   Jancosek M., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3121, DOI 10.1109/CVPR.2011.5995693
   Jiang G, 2005, IEEE I CONF COMP VIS, P333
   Johansen, 2002, COMPUTER VISION ECCV
   Levenberg K., 1944, Quarterly of Applied Mathematics, V2, P164, DOI [10.1090/QAM/10666, DOI 10.1090/QAM/10666]
   Matas, 2004, COMPUTER VISION ECCV
   Miyagawa I, 2010, IEEE T IMAGE PROCESS, V19, P1528, DOI 10.1109/TIP.2010.2042118
   Moulon Pierre, 2013, Computer Vision - ACCV 2012. 11th Asian Conference on Computer Vision. Revised Selected Papers, P257, DOI 10.1007/978-3-642-37447-0_20
   Ricolfe-Viala C, 2010, PATTERN RECOGN, V43, P1688, DOI 10.1016/j.patcog.2009.10.003
   Shen, 2020, ARXIV201209365CS
   Song W, 2015, IEEE T EVOLUT COMPUT, V19, P414, DOI 10.1109/TEVC.2014.2336865
   Storn R, 1997, J GLOBAL OPTIM, V11, P341, DOI 10.1023/A:1008202821328
   Torralba A, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P273
   TSAI RY, 1987, IEEE T ROBOTIC AUTOM, V3, P323, DOI 10.1109/JRA.1987.1087109
   Virtanen P, 2020, NAT METHODS, V17, P261, DOI 10.1038/s41592-019-0686-2
   Wales DJ, 1997, J PHYS CHEM A, V101, P5111, DOI 10.1021/jp970984n
   Wildenauer H, 2012, PROC CVPR IEEE, P2831, DOI 10.1109/CVPR.2012.6248008
   Workman S, 2015, IEEE IMAGE PROC, P1369, DOI 10.1109/ICIP.2015.7351024
   Yan H, 2017, J ELECTRON IMAGING, V26, DOI 10.1117/1.JEI.26.3.033018
   Yu-Tao Cao, 2013, Lecture Notes on Software Engineering, V1, P376, DOI 10.7763/LNSE.2013.V1.80
   Zhang CN, 2020, IEEE WINT CONF APPL, P1030, DOI 10.1109/WACV45572.2020.9093629
   Zhang ZY, 2004, IEEE T PATTERN ANAL, V26, P892, DOI 10.1109/TPAMI.2004.21
   Zhang ZY, 2000, IEEE T PATTERN ANAL, V22, P1330, DOI 10.1109/34.888718
   Zhu CY, 1997, ACM T MATH SOFTWARE, V23, P550, DOI 10.1145/279232.279236
NR 36
TC 1
Z9 1
U1 0
U2 14
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2021
VL 37
IS 9-11
SI SI
BP 2869
EP 2881
DI 10.1007/s00371-021-02233-z
EA JUL 2021
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA UL2RI
UT WOS:000673194700001
DA 2024-07-18
ER

PT J
AU Shibuya, E
   Hotta, K
AF Shibuya, Eisuke
   Hotta, Kazuhiro
TI Cell image segmentation by using feedback and convolutional LSTM
SO VISUAL COMPUTER
LA English
DT Article
DE Feedback; Convolutional neural network; Semantic segmentation; Cell
   image; Convolutional LSTM
AB Human brain is known to have a layered structure and perform not only feedforward process from lower layer to upper layer, but also feedback process from upper layer to lower layer. Neural network is a mathematical model of the function of neurons, and several models are proposed until now. Although neural network imitates the human brain, everyone uses only feedforward process and direct feedback process from upper layer to lower layer is not used in prediction process. Therefore, in this paper, we propose Feedback U-Net using convolutional LSTM. Our model is a segmentation model using convolutional LSTM and feedback process. The output of U-Net at the first round is fed back to the input, and our method re-considers the segmentation result at the second round. By using convolutional LSTM, the features are extracted well based on the features extracted at the first round. On both of the Drosophila cell image and Mouse cell image datasets, our model outperformed conventional U-Net which uses only feedforward process.
C1 [Shibuya, Eisuke; Hotta, Kazuhiro] Meijo Univ, Tempaku Ku, 1-501 Shiogamaguchi, Nagoya, Aichi 4688502, Japan.
C3 Meijo University
RP Shibuya, E (corresponding author), Meijo Univ, Tempaku Ku, 1-501 Shiogamaguchi, Nagoya, Aichi 4688502, Japan.
EM 160442066@ccalumni.meijo-u.ac.jp
OI Hotta, Kazuhiro/0000-0002-5675-8713
FU MEXT/JSPS KAKENHI [18H04746, 18K11382]; Grants-in-Aid for Scientific
   Research [21K11971, 18K11382, 18H04746] Funding Source: KAKEN
FX This work is partially supported by MEXT/JSPS KAKENHI Grand Number
   18H04746 "ResonanceBio" and 18K11382.
CR Alom MZ, 2019, J MED IMAGING, V6, DOI 10.1117/1.JMI.6.1.014006
   Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615
   Bertinetto L, 2016, LECT NOTES COMPUT SC, V9914, P850, DOI 10.1007/978-3-319-48881-3_56
   Bi L, 2017, IEEE T BIO-MED ENG, V64, P2065, DOI 10.1109/TBME.2017.2712771
   Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49
   Chen WM, 2019, PROCEEDINGS OF THE 2019 56TH ACM/EDAC/IEEE DESIGN AUTOMATION CONFERENCE (DAC), DOI [10.1145/3316781.3317816, 10.1109/fie43999.2019.9028400]
   Fu C, 2018, IEEE INT C INTELL TR, P3805, DOI 10.1109/ITSC.2018.8569413
   Gerhard S., 2013, figshare
   Gers FA, 1999, IEE CONF PUBL, P850, DOI [10.1049/cp:19991218, 10.1162/089976600300015015]
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   He K., 2016, PROC CVPR IEEE, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Imanishi A., 2018, CELL STRUCT FUNCT, P18013
   Khosravanian A, 2021, VISUAL COMPUT, V37, P1185, DOI 10.1007/s00371-020-01861-1
   Kim S., 2017, ARXIV PREPRINT ARXIV
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Li B, 2018, PROC CVPR IEEE, P8971, DOI 10.1109/CVPR.2018.00935
   LIANG M, 2015, PROC CVPR IEEE, P3367, DOI [10.1109/CVPR.2015.7298958, DOI 10.1109/CVPR.2015.7298958]
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Oktay O., 2018, ARXIV, DOI DOI 10.48550/ARXIV.1804.03999
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Shi XJ, 2015, ADV NEUR IN, V28
   Shibuya E., 2020, P IEEE CVF C COMP VI, P974
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Szegedy C, 2014, Arxiv, DOI [arXiv:1312.6199, DOI 10.1109/CVPR.2015.7298594]
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang D, 2021, VISUAL COMPUT, V37, P1101, DOI 10.1007/s00371-020-01855-z
   Wang TT, 2017, IEEE I CONF COMP VIS, P4039, DOI 10.1109/ICCV.2017.433
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zheng H., 2018, ARXIV PREPRINT ARXIV
   Zhou Zongwei, 2018, Deep Learn Med Image Anal Multimodal Learn Clin Decis Support (2018), V11045, P3, DOI [10.1007/978-3-030-00889-5_1, 10.1007/978-3-030-00689-1_1]
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 36
TC 10
Z9 11
U1 0
U2 8
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2022
VL 38
IS 11
BP 3791
EP 3801
DI 10.1007/s00371-021-02221-3
EA JUL 2021
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 5X0BS
UT WOS:000669400300001
OA hybrid
DA 2024-07-18
ER

PT J
AU Earnshaw, RA
AF Earnshaw, R. A.
TI A new renaissance for creativity in technology and the arts in the
   context of virtual worlds
SO VISUAL COMPUTER
LA English
DT Article
DE Creative processes; Virtual design; Virtual exhibition; Cyberworlds;
   Social media interactions; Paradigm shift
AB Where do new ideas come from and how are they generated? Which of these ideas will be potentially useful immediately, and which will be more 'blue sky'? For the latter, their significance may not be known for a number of years, perhaps even generations. The progress of computing and digital media is a relevant and useful case study in this respect. Which visions of the future in the early days of computing have stood the test of time, and which have vanished without trace? Can this be used as guide for current and future areas of research and development? If one Internet year is equivalent to seven calendar years, are virtual worlds being utilized as an effective accelerator for these new ideas and their implementation and evaluation? The nature of digital media and its constituent parts such as electronic devices, sensors, images, audio, games, web pages, social media, e-books, and Internet of Things, provides a diverse environment which can be viewed as a testbed for current and future ideas. Individual disciplines utilise virtual worlds in different ways. As collaboration is often involved in such research environments, does the technology make these collaborations effective? Have the limits of disciplinary approaches been reached? The importance of interdisciplinary collaborations for the future is proposed and evaluated. The current enablers for progressing interdisciplinary collaborations are presented. The possibility for a new Renaissance between technology and the arts is discussed.
C1 [Earnshaw, R. A.] Univ Bradford, Fac Engn & Informat, Dept Comp Sci, Bradford, W Yorkshire, England.
C3 University of Bradford
RP Earnshaw, RA (corresponding author), Univ Bradford, Fac Engn & Informat, Dept Comp Sci, Bradford, W Yorkshire, England.
EM r.a.earnshaw@bradford.ac.uk
RI Earnshaw, Rae A/G-8917-2013
OI Earnshaw, Rae/0000-0002-5156-7125
CR Amabile TM, 1998, HARVARD BUS REV, V76, P76
   [Anonymous], 2014, The Guardian
   Asimov I., 2014, MIT TECHNOL REV, V20
   Boden, CAN COMPUTER MODELS
   Boden Margaret., 1990, The Creative Mind: Myths and Mechanisms
   Boden Margaret A., 1996, Dimensions of Creativity, P75
   BROWN J, 1999, ACM SIGGRAPH COMPUTE, V33, P42, DOI DOI 10.1145/330572.330588
   Brown JR, 1999, IEEE COMPUT GRAPH, V19, P70, DOI 10.1109/38.799742
   Carroll J M, 2013, CREATIVITY RATIONALE
   Clark J., 1999, NETSCAPE TIME MAKING
   Cunningham PR, 2015, 20TH INTERNATIONAL CONFERENCE ON COMPOSITE MATERIALS
   Curator T., 2010, WIRED
   Dolinsky, 2014, THESIS PLYMOUTH U
   Earnshaw R., 2001, FRONTIERS HUMAN CENT
   Earnshaw RA, 2013, PROCEEDINGS OF THE FIFTH INTERNATIONAL CONFERENCE ON INTERNET TECHNOLOGIES AND APPLICATIONS (ITA 13), P381
   Essinger James., 2007, JACQUARDS WEB HAND L
   Excell P. S., 2015, 2015 IEEE International Symposium on Technology and Society (ISTAS), DOI 10.1109/ISTAS.2015.7439406
   Ferrazzi K., 2012, HARV BUS REV
   Flerackers C, 2001, IEEE COMPUT GRAPH, V21, P56, DOI 10.1109/38.895133
   Guilford J.P., 1967, The nature of human intelligence
   Howard-Jones P., 2011, IMPACT DIGITAL TECHN
   Joslin C, 2001, IEEE COMPUT GRAPH, V21, P61, DOI 10.1109/38.895134
   Jurgenson Nathan., 2011, Cyborgology
   KPMG, ACC INN POW CROWD
   Kuhn T. S., 1962, STRUCTURE SCI REVOLU
   Liggett S, 2015, 2015 INTERNET TECHNOLOGIES AND APPLICATIONS (ITA) PROCEEDINGS OF THE SIXTH INTERNATIONAL CONFERENCE (ITA 15), P503, DOI 10.1109/ITechA.2015.7317456
   Mack D., 2011, NEW YORK TIMES
   Nowotny Helga., 2015, The Cunning of Uncertainty
   Pearce M., BODEN CREATIVE MIND
   Schnabel M.A., 2004, Architectural Design in Virtual Environments: Exploring Cognition and Communication in Immersive Virtual Environments
   Stahl BC, 2017, SCI PUBL POLICY, V44, P369, DOI 10.1093/scipol/scw069
   Van Noorden R, 2014, NATURE, V512, P126, DOI 10.1038/512126a
   2015, ITA15 ART EXP WORKSH
NR 33
TC 1
Z9 1
U1 1
U2 11
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2021
VL 37
IS 12
SI SI
BP 2921
EP 2929
DI 10.1007/s00371-021-02182-7
EA JUN 2021
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP1DP
UT WOS:000659773400003
OA hybrid
DA 2024-07-18
ER

PT J
AU Qi, YS
   Zhang, AX
   Wang, H
   Li, XM
AF Qi, Yueshuang
   Zhang, Anxin
   Wang, Hua
   Li, Xuemei
TI An efficient FCM-based method for image refinement segmentation
SO VISUAL COMPUTER
LA English
DT Article
DE Local feature; Image sub-blocks; Vote; Refinement segmentation
ID C-MEANS ALGORITHM; LOCAL INFORMATION; FUZZY
AB The conventional fuzzy c-means clustering (FCM) algorithm is sensitive to noise because no spatial information is taken into account. Many related algorithms reduce the influence of noise by adding local information to the objective function. However, there are still many problems, such as poor edge-preserving and anti-noise performance. This paper proposes an FCM-based method for image refinement segmentation to address the above problems effectively. We first take advantage of the pre-classification results of image sub-blocks as a new metric to measure the similarity of pixels and then combine the grayscale and spatial features of the local windows to vote and refine on these initial clustering results, which optimize the classification of pixels. Compared with existing algorithms, our algorithm can correct the misclassified pixels in the global segmentation and reserve image edge better. In addition, it is efficient for noisy image segmentation, which can maximize the recognition of noise and eliminate outliers. Experiments on both synthetic images and real-world images demonstrate the effectiveness and accuracy of the proposed method.
C1 [Qi, Yueshuang; Li, Xuemei] Shandong Univ, Sch Software, Jinan 250101, Peoples R China.
   [Zhang, Anxin] Peoples Hosp Lixia Dist Jinan, Neurol Dept, Jinan 250014, Peoples R China.
   [Wang, Hua] Ludong Univ, Sch Informat & Elect Engn, Yantai 264025, Peoples R China.
   [Li, Xuemei] Shandong Coinnovat Ctr Future Intelligent Comp, Yantai 264025, Peoples R China.
C3 Shandong University; Ludong University
RP Li, XM (corresponding author), Shandong Univ, Sch Software, Jinan 250101, Peoples R China.
EM xmli@sdu.edu.cn
FU National Natural Science Foundation of China [62072281, 62007017];
   Science and Technology Innovation Program for Distributed Young Talents
   of Shandong Province Higher Education Institutions [2019KJN042]
FX This work was supported partly by the National Natural Science
   Foundation of China under Grant Nos. 62072281, 62007017 and the Science
   and Technology Innovation Program for Distributed Young Talents of
   Shandong Province Higher Education Institutions under Grant No.
   2019KJN042.
CR Adhikari SK, 2012, PROCEEDINGS OF THE 2012 INTERNATIONAL CONFERENCE ON COMMUNICATIONS, DEVICES AND INTELLIGENT SYSTEMS (CODLS), P129, DOI 10.1109/CODIS.2012.6422153
   Ahmed MN, 2002, IEEE T MED IMAGING, V21, P193, DOI 10.1109/42.996338
   [Anonymous], 1988, Technometrics, DOI DOI 10.2307/1268876
   [Anonymous], 2017, COMPUT TECHNOL DEV
   Bezdek James C., 1981, PATTERN RECOGN
   Bezdek JamesC., 1973, CLUSTER VALIDITY FUZ
   BEZDEK JC, 1993, MED PHYS, V20, P1033, DOI 10.1118/1.597000
   BEZDEK JC, 1980, IEEE T PATTERN ANAL, V2, P1, DOI 10.1109/TPAMI.1980.4766964
   Bi L, 2018, VISUAL COMPUT, V34, P1043, DOI 10.1007/s00371-018-1519-5
   Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114
   Cai WL, 2007, PATTERN RECOGN, V40, P825, DOI 10.1016/j.patcog.2006.07.011
   Celik T, 2009, IEEE GEOSCI REMOTE S, V6, P772, DOI 10.1109/LGRS.2009.2025059
   Chen SC, 2004, IEEE T SYST MAN CY B, V34, P1907, DOI 10.1109/TSMCB.2004.831165
   Chien SY, 2002, IEEE T CIRC SYST VID, V12, P577, DOI 10.1109/TCSVT.2002.800516
   Dhanachandra N, 2020, MULTIMED TOOLS APPL, V79, P18839, DOI 10.1007/s11042-020-08699-8
   Duda R.O., 2003, IEEE T AUTOMAT CONTR, V19, P462
   Dunn J. C., 1974, Journal of Cybernetics, V4, P1, DOI 10.1080/01969727408546062
   Fergus R, 2003, PROC CVPR IEEE, P264
   Gong MG, 2013, IEEE T IMAGE PROCESS, V22, P573, DOI 10.1109/TIP.2012.2219547
   Gong MG, 2012, IEEE T IMAGE PROCESS, V21, P2141, DOI 10.1109/TIP.2011.2170702
   Grau V, 2004, IEEE T MED IMAGING, V23, P447, DOI 10.1109/TMI.2004.824224
   Huang LY, 2022, VISUAL COMPUT, V38, P135, DOI 10.1007/s00371-020-02008-y
   Jiang J., 2013, COMPUT AIDED DRAFTIN, V2, P13
   Khosravanian A, 2021, VISUAL COMPUT, V37, P1185, DOI 10.1007/s00371-020-01861-1
   Krinidis S, 2010, IEEE T IMAGE PROCESS, V19, P1328, DOI 10.1109/TIP.2010.2040763
   Krishnapuram R, 1996, IEEE T FUZZY SYST, V4, P385, DOI 10.1109/91.531779
   Krishnapuram R., 1993, IEEE Transactions on Fuzzy Systems, V1, P98, DOI 10.1109/91.227387
   Lei T, 2018, IEEE T FUZZY SYST, V26, P3027, DOI 10.1109/TFUZZ.2018.2796074
   Liu X., 2006, COMP TECHN APPL PROG
   Ming, 2010, 2010 3 INT C ADV COM, V5, pV5
   Mishro PK, 2021, IEEE T CYBERNETICS, V51, P3901, DOI 10.1109/TCYB.2020.2994235
   Pal NR, 2005, IEEE T FUZZY SYST, V13, P517, DOI 10.1109/TFUZZ.2004.840099
   Pham DL, 1999, IEEE T MED IMAGING, V18, P737, DOI 10.1109/42.802752
   Qian X, 2019, VISUAL COMPUT, V35, P985, DOI 10.1007/s00371-019-01682-x
   Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323
   Santosh KC, 2016, IEEE INTELL SYST, V31, P66, DOI 10.1109/MIS.2016.24
   Senthilkumaran N., 2009, Proceedings of the international Conference on Mathematics and Computer Science (ICMCS), P255
   Shen DG, 1997, PATTERN RECOGN LETT, V18, P37, DOI 10.1016/S0167-8655(96)00117-1
   Song YY, 2020, VISUAL COMPUT, V36, P1189, DOI 10.1007/s00371-019-01728-0
   Szilágyi L, 2003, P ANN INT IEEE EMBS, V25, P724, DOI 10.1109/IEMBS.2003.1279866
   Wu CM, 2021, SOFT COMPUT, V25, P3751, DOI 10.1007/s00500-020-05403-8
   Xiao CX, 2013, VISUAL COMPUT, V29, P27, DOI 10.1007/s00371-012-0672-5
   Xu JD, 2021, INT J FUZZY SYST, V23, P816, DOI 10.1007/s40815-020-01015-4
   [杨立才 YANG Licai], 2007, [山东大学学报. 工学版, Journal of Shandong University (Engineering Science)], V37, P51
   Yao JC, 2017, VISUAL COMPUT, V33, P179, DOI 10.1007/s00371-015-1171-2
   Zhang YX, 2021, VISUAL COMPUT, V37, P1061, DOI 10.1007/s00371-020-01852-2
   Zhang YX, 2019, IEEE T FUZZY SYST, V27, P185, DOI 10.1109/TFUZZ.2018.2883033
   Zhu SC, 1996, IEEE T PATTERN ANAL, V18, P884, DOI 10.1109/34.537343
NR 48
TC 3
Z9 3
U1 0
U2 18
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2022
VL 38
IS 7
BP 2499
EP 2514
DI 10.1007/s00371-021-02126-1
EA MAY 2021
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 2B0WL
UT WOS:000652427500001
DA 2024-07-18
ER

PT J
AU Bao, XY
   Liang, JZ
   Xia, YF
   Hou, Z
   Huan, Z
AF Bao, Xiangyang
   Liang, Jiuzhen
   Xia, Yunfei
   Hou, Zhenjie
   Huan, Zhan
TI Low-rank decomposition fabric defect detection based on prior and total
   variation regularization
SO VISUAL COMPUTER
LA English
DT Article
DE Fabric defect detection; Low rank decomposition model; Total variation
   regular term; Defect prior; Autoencoder
ID THRESHOLDING ALGORITHM; INSPECTION
AB Low-rank decomposition model is widely used in fabric defect detection, where a feature matrix is decomposed into a low-rank matrix that represents defect-free regions of the image and a sparse matrix that represents defective regions. Two shortcomings, however, still exist in the traditional low-rank decomposition models. First, they cannot detect the position and shape of the defect very well, and they are usually misjudge the textured background as a defect. Second, they cannot detect some large homogeneous defective block. To solve those problems, we propose a low-rank decomposition model with defect prior and total variation regular term, we call it PTVLR. And it is consisted of the following parts. (1) Defect prior, which characterizes the autoencoder residual image, is used as a prior consideration of detect for improving the separation effect of low-rank texture and defect, (2) total variation regular of term constrains the defect according to the spatial continuity of the defect, (3) L-F norm characterizes the image noise part. The performance of PTVLR is evaluated on the box-, star- and dot- patterned fabric databases. And its superior results are shown compared with state-of-the-art methods, that is, 55.58% f-measure and 77.89% true positive rate (TPR) for box-patterned fabrics, 53.20% f-measure and 86.75% TPR for star-patterned fabrics, 69.78% f-measure and 88.33% TPR for dot-patterned fabrics.
C1 [Bao, Xiangyang; Liang, Jiuzhen; Hou, Zhenjie; Huan, Zhan] Changzhou Univ, Sch Comp Sci & Artificial Intelligence, Changzhou 213164, Jiangsu, Peoples R China.
   [Xia, Yunfei] Univ N Carolina, Dept Math & Stat, Charlotte, NC USA.
   [Liang, Jiuzhen] Changzhou Univ, Jiangsu Engn Res Ctr Digital Twinning Technol, Key Equipment Petrochem Proc, Changzhou 213164, Jiangsu, Peoples R China.
C3 Changzhou University; University of North Carolina; University of North
   Carolina Charlotte; Changzhou University
RP Liang, JZ (corresponding author), Changzhou Univ, Sch Comp Sci & Artificial Intelligence, Changzhou 213164, Jiangsu, Peoples R China.
EM jzliang@cczu.edu.cn
RI Xia, Yunfei/KIC-0347-2024; Liang, Jiuzhen/HJG-9384-2022
FU Jiangsu Engineering Research Center of Digital Twinning Technology for
   Key Equipment in Petrochemical Process [DT 2020720, DTEC202001];
   Postgraduate Research & Practice Innovation Program of Jiangsu Province
   [YPC20020171]
FX This study is supported by Jiangsu Engineering Research Center of
   Digital Twinning Technology for Key Equipment in Petrochemical Process
   (No. DT 2020720), Jiangsu Engineering Research Center of Digital
   Twinning Technology for Key Equipment in Petrochemical Process
   (DTEC202001, DT2020720) and the Postgraduate Research & Practice
   Innovation Program of Jiangsu Province (YPC20020171). The database
   employed in this research is kindly provided by Industrial Automation
   Research Laboratory from Department of Electrical and Electronic
   Engineering of Hong Kong University. The dataset is available at
   (https://lmb.informat-ik.unifreiburg.de/resources/datasets/tilda.en.html
   ).
CR [Anonymous], 2020, IEEE I CONF COMP VIS
   [Anonymous], 2018, MATH PROBL ENG
   [Anonymous], 2019, 2019 IEEE VEH POW PR
   Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542
   Bi L, 2017, VISUAL COMPUT, V33, P1061, DOI 10.1007/s00371-017-1379-4
   Cai JF, 2010, SIAM J OPTIMIZ, V20, P1956, DOI 10.1137/080738970
   Candès EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395
   Cao JJ, 2016, INT J CLOTH SCI TECH, V28, P516, DOI 10.1108/IJCST-10-2015-0117
   Doyle L, 2019, VISUAL COMPUT, V35, P1489, DOI 10.1007/s00371-018-1513-y
   Giraldo-Zuluaga JH, 2019, VISUAL COMPUT, V35, P335, DOI 10.1007/s00371-017-1463-9
   Goldstein T, 2009, SIAM J IMAGING SCI, V2, P323, DOI 10.1137/080725891
   Ji X, 2020, J ENG FIBER FABR, V15, DOI 10.1177/1558925020957654
   Kingma D. P., 2014, arXiv
   Li CL, 2019, IEEE ACCESS, V7, P83962, DOI 10.1109/ACCESS.2019.2925196
   Li YT, 2018, APPL SCI-BASEL, V8, DOI 10.3390/app8091678
   Liu GH, 2021, VISUAL COMPUT, V37, P515, DOI 10.1007/s00371-020-01820-w
   Mei S, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18041064
   Ng MK, 2017, SIAM J IMAGING SCI, V10, P2140, DOI 10.1137/17M1113138
   Ng MK, 2014, IEEE T AUTOM SCI ENG, V11, P943, DOI 10.1109/TASE.2014.2314240
   Ngan HYT, 2006, OPT ENG, V45, DOI 10.1117/1.2345189
   Ngan HYT, 2009, IEEE T AUTOM SCI ENG, V6, P131, DOI 10.1109/TASE.2008.917140
   Ngan HYT, 2005, PATTERN RECOGN, V38, P559, DOI 10.1016/j.patcog.2004.07.009
   Quanjun, 2013, MICROCOMPUTER ITS AP, V10, P016
   RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F
   Selvi SST., 2017, J ENV NANOTECHNOL, V6, P79, DOI [10.13074/jent.2017.03.171241, DOI 10.13074/JENT.2017.03.171241]
   Tsang CSC, 2016, PATTERN RECOGN, V51, P378, DOI 10.1016/j.patcog.2015.09.022
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Zheng ZL, 2014, PATTERN RECOGN, V47, P3502, DOI 10.1016/j.patcog.2014.05.001
NR 28
TC 7
Z9 7
U1 3
U2 26
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2022
VL 38
IS 8
BP 2707
EP 2721
DI 10.1007/s00371-021-02148-9
EA MAY 2021
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 3B2OE
UT WOS:000648497600001
DA 2024-07-18
ER

PT J
AU Liu, Y
   Wang, SH
   Nie, JH
   Gao, H
AF Liu, Ye
   Wang, Shuohong
   Nie, Jianhui
   Gao, Hao
TI Localizing and tracking dense crowd of microbes by joint association and
   detection refinement
SO VISUAL COMPUTER
LA English
DT Article
DE Object detection; Multi-object tracking; Microscopy image
AB This paper presents a method for detecting and tracking large number of arbitrary-oriented and densely aggregated microbes from image sequences captured under microscope. We first propose an integral channel feature (ICF)-based detector which is able to localize the dense and arbitrarily oriented targets with low false positive rate. Then instead of treating target detection and tracking as two separated problems as many previous works did, we propose to refine the detection results in the data association process. The kinematic pattern of microbes is well modeled with the proposed integral sliding energy (ISE), which is combined with detection response in a hybrid cost function. Minimizing the cost function allows us to simultaneously select the true targets from the detections and to match the targets across two consecutive frames. Systematical experiments have been conducted to demonstrate the effectiveness of proposed method.
C1 [Liu, Ye; Nie, Jianhui; Gao, Hao] Nanjing Univ Posts & Telecommun, Sch Automat & Artificial Intelligence, Nanjing, Jiangsu, Peoples R China.
   [Wang, Shuohong] Harvard Univ, Dept Mol & Cellular Biol, Cambridge, MA 02138 USA.
   [Wang, Shuohong] Harvard Univ, Ctr Brain Sci, Cambridge, MA 02138 USA.
C3 Nanjing University of Posts & Telecommunications; Harvard University;
   Harvard University
RP Liu, Y (corresponding author), Nanjing Univ Posts & Telecommun, Sch Automat & Artificial Intelligence, Nanjing, Jiangsu, Peoples R China.
EM yeliu@njupt.edu.cn; njh19@163.com; gaohao@njupt.edu.cn
OI Liu, Ye/0000-0002-2686-3002
FU Natural Science Foundation of China [61602255, 61931012]
FX The research work of this paper is sponsored by Natural Science
   Foundation of China under Grant 61602255 and 61931012. The authors would
   like to thank Prof. T. Vicsek and his group for providing the research
   data of this
CR [Anonymous], 2014, ARXIV14097618
   Bertinetto L, 2016, LECT NOTES COMPUT SC, V9914, P850, DOI 10.1007/978-3-319-48881-3_56
   Betke M., 2016, Synthesis Lectures on Computer Vision, V6, P1, DOI [10.2200/S00726ED1V01Y201608COV009, DOI 10.2200/S00726ED1V01Y201608COV009]
   Bise R, 2015, IEEE T MED IMAGING, V34, P1417, DOI 10.1109/TMI.2015.2391095
   Butt AA, 2013, PROC CVPR IEEE, P1846, DOI 10.1109/CVPR.2013.241
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Dollar Piotr, 2009, BMVC, DOI 10.5244/ C.23.91
   Dong XP, 2018, LECT NOTES COMPUT SC, V11217, P472, DOI 10.1007/978-3-030-01261-8_28
   Hare S, 2016, IEEE T PATTERN ANAL, V38, P2096, DOI 10.1109/TPAMI.2015.2509974
   Henriques JF, 2015, IEEE T PATTERN ANAL, V37, P583, DOI 10.1109/TPAMI.2014.2345390
   Hu HW, 2019, IEEE T MULTIMEDIA, V21, P510, DOI 10.1109/TMM.2018.2859831
   Kanade T., 2011, APPL COMPUTER VISION, P374
   Khan Z, 2004, LECT NOTES COMPUT SC, V2034, P279
   Kuanar S, 2019, IEEE IMAGE PROC, P1351, DOI [10.1109/ICIP.2019.8803037, 10.1109/icip.2019.8803037]
   Kuanar S, 2019, CIRC SYST SIGNAL PR, V38, P5081, DOI 10.1007/s00034-019-01110-4
   Kuanar S, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P2576, DOI 10.1109/ICASSP.2018.8462243
   Liang ZY, 2020, IEEE T IMAGE PROCESS, V29, P3351, DOI 10.1109/TIP.2019.2959256
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Liu Y, 2016, PATTERN RECOGN, V52, P384, DOI 10.1016/j.patcog.2015.11.014
   Liu Y, 2016, OPTIK, V127, P76, DOI 10.1016/j.ijleo.2015.09.218
   Ma B, 2016, IEEE T IMAGE PROCESS, V25, P4199, DOI 10.1109/TIP.2016.2588329
   Mei X, 2011, IEEE T PATTERN ANAL, V33, P2259, DOI 10.1109/TPAMI.2011.66
   Ming Q., 2020, ARXIV201204150
   Redmon J, 2018, Arxiv, DOI [arXiv:1804.02767, DOI 10.1109/CVPR.2017.690, DOI 10.48550/ARXIV.1804.02767]
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Rehman B, 2020, VISUAL COMPUT, V36, P633, DOI 10.1007/s00371-019-01649-y
   Reilly V, 2010, LECT NOTES COMPUT SC, V6313, P186
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Ross DA, 2008, INT J COMPUT VISION, V77, P125, DOI 10.1007/s11263-007-0075-7
   Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb
   Wang SH, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0154714
   Wojke N, 2017, IEEE IMAGE PROC, P3645, DOI 10.1109/ICIP.2017.8296962
   Wu Z, 2009, IEEE I CONF COMP VIS, P1546
   Xue Yang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12353), P677, DOI 10.1007/978-3-030-58598-3_40
   Zhao JW, 2018, VISUAL COMPUT, V34, P1677, DOI 10.1007/s00371-017-1441-2
   Zou D, 2009, IEEE I CONF COMP VIS, P1578, DOI 10.1109/ICCV.2009.5459358
NR 36
TC 1
Z9 1
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2022
VL 38
IS 7
BP 2373
EP 2382
DI 10.1007/s00371-021-02118-1
EA APR 2021
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 2B0WL
UT WOS:000644781400001
DA 2024-07-18
ER

PT J
AU Ben Fredj, H
   Bouguezzi, S
   Souani, C
AF Ben Fredj, Hana
   Bouguezzi, Safa
   Souani, Chokri
TI Face recognition in unconstrained environment with CNN
SO VISUAL COMPUTER
LA English
DT Article
DE Face recognition; Deep learning; Data augmentation
ID REPRESENTATION
AB In recent years, convolutional neural networks have proven to be a highly efficient approach for face recognition. In this paper, we develop such a framework to learn a robust face verification in an unconstrained environment using aggressive data augmentation. Our objective is to learn a deep face representation from large-scale data with massive noisy and occluded face. Besides, we add an adaptive fusion of softmax loss and center loss as supervision signals, which are helpful to improve the performance and to conduct the final classification. The experiment results show that the suggested system achieves comparable performances with other state-of-the-art methods on the Labeled Faces in the Wild and YouTube face verification tasks.
C1 [Ben Fredj, Hana; Bouguezzi, Safa] Univ Monastir, Fac Sci Monastir, Lab Microelect & Instrumentat, Monastir, Tunisia.
   [Souani, Chokri] Univ Sousse, Inst Super Sci Appl & Technol Sousse, Sousse, Tunisia.
C3 Universite de Monastir; Universite de Sousse
RP Ben Fredj, H (corresponding author), Univ Monastir, Fac Sci Monastir, Lab Microelect & Instrumentat, Monastir, Tunisia.
EM ben.fredj.hanaa@gmail.com; safabouguezzi@yahoo.fr;
   chokri.souani@gmail.com
RI souani, chokri/B-1853-2015
OI souani, chokri/0000-0002-8987-3582; bouguezzi, safa/0000-0001-5107-7508
CR An FP, 2020, VISUAL COMPUT, V36, P483, DOI 10.1007/s00371-019-01635-4
   Ben Fredj H, 2017, 2017 INTERNATIONAL CONFERENCE ON CONTROL, AUTOMATION AND DIAGNOSIS (ICCAD), P209, DOI 10.1109/CADIAG.2017.8075658
   Chatfield K, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.76
   Choi JY, 2018, VISUAL COMPUT, V34, P1535, DOI 10.1007/s00371-017-1429-y
   Deng J., 2019, ARXIV, DOI 10.48550/arXiv.1905.00641
   Deng JK, 2019, PROC CVPR IEEE, P4685, DOI 10.1109/CVPR.2019.00482
   Devries T, 2014, 2014 CANADIAN CONFERENCE ON COMPUTER AND ROBOT VISION (CRV), P98, DOI 10.1109/CRV.2014.21
   Faiedh H, 2018, P I MECH ENG I-J SYS, V232, P772, DOI 10.1177/0959651818778478
   Farhat W., 2018, J AMBIENT INTELL HUM, V10, P1
   Guo K, 2017, CAAI T INTELL TECHNO, V2, P39, DOI 10.1016/j.trit.2017.03.001
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541
   Lei Z, 2007, LECT NOTES COMPUT SC, V4642, P87
   Leng B, 2017, NEUROCOMPUTING, V235, P10, DOI 10.1016/j.neucom.2016.12.013
   Lian S, 2019, ARXIV190411685
   Lv JJ, 2017, NEUROCOMPUTING, V230, P184, DOI 10.1016/j.neucom.2016.12.025
   Lv JJ, 2016, SIGNAL PROCESS-IMAGE, V47, P465, DOI 10.1016/j.image.2016.03.011
   Masi I, 2016, LECT NOTES COMPUT SC, V9909, P579, DOI 10.1007/978-3-319-46454-1_35
   Naseem I, 2010, IEEE T PATTERN ANAL, V32, P2106, DOI 10.1109/TPAMI.2010.128
   Qi C, 2017, IEEE IMAGE PROC, P2851, DOI 10.1109/ICIP.2017.8296803
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Sun Y, 2014, ADV NEUR IN, V27
   Sun Y, 2015, PROC CVPR IEEE, P2892, DOI 10.1109/CVPR.2015.7298907
   Sun Y, 2014, PROC CVPR IEEE, P1891, DOI 10.1109/CVPR.2014.244
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220
   Wang B, 2020, VISUAL COMPUT, V36, P1897, DOI 10.1007/s00371-019-01779-3
   Wang H, 2018, PROC CVPR IEEE, P5265, DOI 10.1109/CVPR.2018.00552
   Wen G, 2018, NEUROCOMPUTING, V287, P45, DOI 10.1016/j.neucom.2018.01.079
   Wen YD, 2016, LECT NOTES COMPUT SC, V9911, P499, DOI 10.1007/978-3-319-46478-7_31
   Wu X, 2018, IEEE T INF FOREN SEC, V13, P2884, DOI 10.1109/TIFS.2018.2833032
   Xi PC, 2020, VISUAL COMPUT, V36, P1869, DOI 10.1007/s00371-019-01775-7
   Xu Y, 2013, INFORM SCIENCES, V238, P138, DOI 10.1016/j.ins.2013.02.051
   Yaeger L. S., 1996, Advances in Neural Information Processing Systems (NeurIPS), P807
   Yi Dong, 2014, ARXIV14117923
   Yuan G, 2018, ARXIV180306524
   Zhang YH, 2018, IET IMAGE PROCESS, V12, P819, DOI 10.1049/iet-ipr.2017.1085
   Zhang Y, 2016, IEEE T IMAGE PROCESS, V25, DOI 10.1109/TIP.2016.2549360
NR 40
TC 41
Z9 41
U1 2
U2 19
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2021
VL 37
IS 2
BP 217
EP 226
DI 10.1007/s00371-020-01794-9
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA QR4KJ
UT WOS:000625179800003
DA 2024-07-18
ER

PT J
AU Zolfaghari, M
   Ghanei-Yakhdan, H
   Yazdi, M
AF Zolfaghari, Mohammad
   Ghanei-Yakhdan, Hossein
   Yazdi, Mehran
TI Real-time object tracking based on sparse representation and adaptive
   particle drawing
SO VISUAL COMPUTER
LA English
DT Article
DE Sparse representation; Correlation filter; Occlusion detection; Particle
   drawing; Model updating
ID ROBUST VISUAL TRACKING
AB Sparsity-based trackers describe a candidate region by solving the l(1) minimization problem. This process is done for many candidates generated in a particle filter framework. This results in a high computational cost, and thus it can preclude the use of these trackers in real- time applications. To tackle this issue, we proposed a novel method to draw particles from a Gaussian distribution which not only reduces the number of candidates but also maintains the accuracy of object tracking. In the proposed method, the appearance and motion information of object is used to adaptively estimate the number of particles in each frame. In addition, a normalized correlation (NC) filter is integrated to the sparsity-based tracker in which they cooperate with each other to increase the performance of object tracking. In other words, the l(1) minimization problem is controlled by a NC filter and the template region of correlation filter is set by a sparsity-based tracker. The occlusion degree over object tracking is accurately estimated by the sparse coefficients that its energy is adaptively controlled in the l(1) minimization problem. When an object is partially occluded, the templates of dictionary are adaptively updated based on occlusion degree which causes to increase the accuracy of object tracking. Experimental results on two tracking benchmark datasets demonstrate that the proposed method performs favorably against several popular trackers.
C1 [Zolfaghari, Mohammad; Ghanei-Yakhdan, Hossein] Yazd Univ, Dept Elect Engn, Yazd, Iran.
   [Yazdi, Mehran] Shiraz Univ, Dept Elect & Telecommun Engn, Shiraz, Iran.
C3 University of Yazd; Shiraz University
RP Ghanei-Yakhdan, H (corresponding author), Yazd Univ, Dept Elect Engn, Yazd, Iran.
EM hghaneiy@yazd.ac.ir
RI Ghanei-Yakhdan, Hossein/JAC-4508-2023; Yazdi, Mehran/C-2776-2011;
   Ghanei-Yakhdan, Hossein/D-7382-2018
OI Ghanei-Yakhdan, Hosein/0000-0003-4575-1062
CR Adam A., 2006, 2006 IEEE COMP SOC C
   Ahmad, 2016, 2016 IEEE CAN C EL C
   Ahmed J, 2008, MACH VISION APPL, V19, P1, DOI 10.1007/s00138-007-0072-4
   An XW, 2018, IEEE ACCESS, V6, P69978, DOI 10.1109/ACCESS.2018.2879156
   Avidan S, 2004, IEEE T PATTERN ANAL, V26, P1064, DOI 10.1109/TPAMI.2004.53
   Babenko B, 2011, IEEE T PATTERN ANAL, V33, P1619, DOI 10.1109/TPAMI.2010.226
   Bai TX, 2014, IEEE T IND INFORM, V10, P538, DOI 10.1109/TII.2013.2272089
   Bai TX, 2012, PATTERN RECOGN, V45, P2390, DOI 10.1016/j.patcog.2011.12.004
   Bao CL, 2012, PROC CVPR IEEE, P1830, DOI 10.1109/CVPR.2012.6247881
   Cannons K., 2008, CSE200807 YORK U DEP
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Danelljan M., 2014, BRIT MACH VIS C, P1, DOI DOI 10.5244/C.28.65
   Dong X, 2018, 2018 EUROPEAN CONFERENCE ON OPTICAL COMMUNICATION (ECOC)
   Dong XP, 2021, IEEE T PATTERN ANAL, V43, P1515, DOI 10.1109/TPAMI.2019.2956703
   Dong XP, 2019, IEEE T IMAGE PROCESS, V28, P3516, DOI 10.1109/TIP.2019.2898567
   Dong XP, 2017, IEEE T MULTIMEDIA, V19, P763, DOI 10.1109/TMM.2016.2631884
   Dong XW, 2018, IEEE CONF COMPUT
   Gao J, 2014, LECT NOTES COMPUT SC, V8691, P188, DOI 10.1007/978-3-319-10578-9_13
   Hare S, 2016, IEEE T PATTERN ANAL, V38, P2096, DOI 10.1109/TPAMI.2015.2509974
   He LJ, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18020572
   Henriques JF, 2015, IEEE T PATTERN ANAL, V37, P583, DOI 10.1109/TPAMI.2014.2345390
   Hu HW, 2019, IEEE T MULTIMEDIA, V21, P510, DOI 10.1109/TMM.2018.2859831
   Hu HW, 2018, IEEE T NEUR NET LEAR, V29, P1786, DOI 10.1109/TNNLS.2017.2688448
   Hu Q., 2016, 2016 INT C DIG IM CO
   Jansi R, 2017, INT CONF COMPUT POW, P102, DOI 10.1109/ICCPEIC.2017.8290346
   Jia X, 2012, PROC CVPR IEEE, P1822, DOI 10.1109/CVPR.2012.6247880
   Jiang N, 2011, IEEE T IMAGE PROCESS, V20, P2288, DOI 10.1109/TIP.2011.2114895
   Kalal Z, 2012, IEEE T PATTERN ANAL, V34, P1409, DOI 10.1109/TPAMI.2011.239
   Kristan M., 2015, P IEEE INT C COMP VI
   Kristanl M, 2019, IEEE INT CONF COMP V, P2206, DOI 10.1109/ICCVW.2019.00276
   Kumar MSN, 2012, IEEE IMAGE PROC, P433, DOI 10.1109/ICIP.2012.6466889
   Kwon J, 2010, PROC CVPR IEEE, P1269, DOI 10.1109/CVPR.2010.5539821
   Li HX, 2011, PROC CVPR IEEE, P1305, DOI 10.1109/CVPR.2011.5995483
   Li X, 2013, ACM T INTEL SYST TEC, V4, DOI 10.1145/2508037.2508039
   Li ZT, 2018, IEEE T IMAGE PROCESS, V27, P4478, DOI 10.1109/TIP.2018.2839916
   Liang ZY, 2020, IEEE T IMAGE PROCESS, V29, P3351, DOI 10.1109/TIP.2019.2959256
   Liu HC, 2015, IEEE T INSTRUM MEAS, V64, P2863, DOI 10.1109/TIM.2015.2437636
   Liu N, 2019, MULTIMED TOOLS APPL, V78, P3689, DOI 10.1007/s11042-017-5538-4
   Liu T, 2015, PROC CVPR IEEE, P4902, DOI 10.1109/CVPR.2015.7299124
   Ma B, 2018, IEEE T NEUR NET LEAR, V29, P4769, DOI 10.1109/TNNLS.2017.2776124
   Ma B, 2015, IEEE T MULTIMEDIA, V17, P1818, DOI 10.1109/TMM.2015.2463221
   Ma C, 2015, IEEE I CONF COMP VIS, P3074, DOI 10.1109/ICCV.2015.352
   Mbelwa JT, 2020, VISUAL COMPUT, V36, P1173, DOI 10.1007/s00371-019-01727-1
   Mei X, 2011, PROC CVPR IEEE, P1257
   Mei X, 2011, IEEE T PATTERN ANAL, V33, P2259, DOI 10.1109/TPAMI.2011.66
   Mei X, 2009, IEEE I CONF COMP VIS, P1436, DOI 10.1109/ICCV.2009.5459292
   Nai K, 2018, IEEE T IMAGE PROCESS, V27, P4958, DOI 10.1109/TIP.2018.2848465
   Nooghabi M.K., 2018, VISUAL COMPUT, P1
   Pang Y, 2013, IEEE I CONF COMP VIS, P2784, DOI 10.1109/ICCV.2013.346
   Ross DA, 2008, INT J COMPUT VISION, V77, P125, DOI 10.1007/s11263-007-0075-7
   Sabah M., 2014, INT J COMPUT APPL, V104, P21
   Shan CF, 2007, PATTERN RECOGN, V40, P1958, DOI 10.1016/j.patcog.2006.12.012
   Shen JB, 2020, IEEE T CYBERNETICS, V50, P3068, DOI 10.1109/TCYB.2019.2936503
   Shen JB, 2019, IEEE T CYBERNETICS, V49, P1990, DOI 10.1109/TCYB.2018.2803217
   Smeulders AWM, 2014, IEEE T PATTERN ANAL, V36, P1442, DOI 10.1109/TPAMI.2013.230
   Smith A., 2013, Sequential Monte Carlo methods in practice
   van de Weijer J, 2009, IEEE T IMAGE PROCESS, V18, P1512, DOI 10.1109/TIP.2009.2019809
   Wang D, 2015, IEEE T IMAGE PROCESS, V24, P2646, DOI 10.1109/TIP.2015.2427518
   Wang D, 2012, IEEE SIGNAL PROC LET, V19, P711, DOI 10.1109/LSP.2012.2215320
   Wang LJ, 2015, IEEE I CONF COMP VIS, P3119, DOI 10.1109/ICCV.2015.357
   Wang Q, 2011, IEEE T SYST MAN CY B, V41, P385, DOI 10.1109/TSMCB.2010.2056366
   Wong S, 2005, P SOC PHOTO-OPT INS, V5810, P158, DOI 10.1117/12.598647
   Wu Y, 2015, IEEE T PATTERN ANAL, V37, P1834, DOI 10.1109/TPAMI.2014.2388226
   Wu Y, 2013, PROC CVPR IEEE, P2411, DOI 10.1109/CVPR.2013.312
   Zeng XY, 2019, IET COMPUT VIS, V13, P146, DOI 10.1049/iet-cvi.2018.5158
   Zhang HY, 2018, VISUAL COMPUT, V34, P41, DOI 10.1007/s00371-016-1310-4
   Zhang KH, 2016, IEEE T IMAGE PROCESS, V25, P1779, DOI 10.1109/TIP.2016.2531283
   Zhang TZ, 2019, IEEE T PATTERN ANAL, V41, P473, DOI 10.1109/TPAMI.2018.2797082
   Zhang TZ, 2017, PROC CVPR IEEE, P4819, DOI [10.1109/CVPR.2017.512, 10.1109/ICCV.2017.469]
   Zhang TZ, 2016, PROC CVPR IEEE, P3880, DOI 10.1109/CVPR.2016.421
   Zhong W, 2014, IEEE T IMAGE PROCESS, V23, P2356, DOI 10.1109/TIP.2014.2313227
   Zhuang BH, 2014, IEEE T IMAGE PROCESS, V23, P1872, DOI 10.1109/TIP.2014.2308414
   Zolfaghari M, 2020, VISUAL COMPUT, V36, P701, DOI 10.1007/s00371-019-01652-3
NR 73
TC 4
Z9 4
U1 0
U2 14
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2022
VL 38
IS 3
BP 849
EP 869
DI 10.1007/s00371-020-02055-5
EA FEB 2021
PG 21
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZQ8YX
UT WOS:000613615300001
DA 2024-07-18
ER

PT J
AU Chen, GC
   Qin, HB
AF Chen, Guancheng
   Qin, Huabiao
TI Class-discriminative focal loss for extreme imbalanced multiclass object
   detection towards autonomous driving
SO VISUAL COMPUTER
LA English
DT Article
DE Object detection; Focal loss; Class-discriminative; Class imbalance
AB Currently, modern object detection algorithms still suffer the imbalance problems especially the foreground-background and foreground-foreground class imbalance. Existing methods generally adopt re-sampling based on the class frequency or re-weighting based on the category prediction probability, such as focal loss, proposed to rebalance the loss assigned to easy negative examples and hard positive examples for single-stage detectors. However, there are still two critical issues unresolved. In practical applications, such as autonomous driving, the class imbalance will become more extreme due to the increased detection field and target distribution characteristics, needing a more effective way to balance the foreground-background class imbalance. Besides, existing methods typically employ the sigmoid or softmax entropy loss for classification task, which we believe is not capable to realize the foreground-foreground class balance. In this paper, we propose a new form of focal loss by re-designing the re-weighting scheme that can calculate the weight according to the probability as well as widen the weight difference of the examples. Besides, we introduce the extended focal loss to multi-class classification task by reformulating the standard softmax cross-entropy loss for better utilizing the discriminant difference of foreground categories, thereby yielding a class-discriminative focal loss. Comprehensive experiments are conducted on the KITTI and BDD dataset, respectively. The results show that our approach can easily surpass focal loss with no more training and inference time cost. Besides, when trained with the proposed loss function, current state-of-the-art object detectors no matter in one-stage or two-stage paradigms can achieve significant performance gains.
C1 [Chen, Guancheng; Qin, Huabiao] South China Univ Technol, Sch Elect & Informat Engn, Guangzhou 510641, Peoples R China.
C3 South China University of Technology
RP Qin, HB (corresponding author), South China Univ Technol, Sch Elect & Informat Engn, Guangzhou 510641, Peoples R China.
EM eechengc@mail.scut.edu.cn; eehbqin@scut.edu.cn
RI Chen, Guancheng/ABA-5582-2020
OI Chen, Guancheng/0000-0002-4941-1543
CR [Anonymous], COMPUT VIS PATTERN R
   Bochkovskiy A, 2020, ARXIV PREPRINT ARXIV, DOI DOI 10.48550/ARXIV.2004.10934
   Bulò SR, 2017, PROC CVPR IEEE, P7082, DOI 10.1109/CVPR.2017.749
   Chen CP, 2018, LECT NOTES COMPUT SC, V11257, P368, DOI 10.1007/978-3-030-03335-4_32
   Chen K., 2019, arXiv preprint arXiv:1906.07155
   Chen WJ, 2021, VISUAL COMPUT, V37, P805, DOI 10.1007/s00371-020-01831-7
   Dai Jifeng, 2016, Advances in Neural Information Processing Systems, DOI DOI 10.1016/J.JPOWSOUR.2007.02.075
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Fu C.-Y., 2017, DSSD: Deconvolutional Single Shot Detector
   Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074
   Girshick R., 2014, PROC CVPR IEEE, DOI [DOI 10.1109/CVPR.2014.81, 10.1109/CVPR.2014.81]
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Hastie T., 2009, The Elements of Statistical Learning
   He KM, 2020, IEEE T PATTERN ANAL, V42, P386, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   Joseph RK, 2016, CRIT POL ECON S ASIA, P1
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li BY, 2019, AAAI CONF ARTIF INTE, P8577
   Li T, 2014, VISUAL COMPUT, V30, P59, DOI 10.1007/s00371-013-0780-x
   Li Z., 2017, CORR
   Lienhart R, 2002, IEEE IMAGE PROC, P900
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu BZ, 2018, VISUAL COMPUT, V34, P707, DOI 10.1007/s00371-017-1408-3
   Ojala T, 2000, LECT NOTES COMPUT SC, V1842, P404
   Oksuz K, 2021, IEEE T PATTERN ANAL, V43, P3388, DOI 10.1109/TPAMI.2020.2981890
   Papageorgiou CP, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P555, DOI 10.1109/ICCV.1998.710772
   Pinheiro PO, 2015, ADV NEUR IN, V28
   Pinheiro PO, 2016, LECT NOTES COMPUT SC, V9905, P75, DOI 10.1007/978-3-319-46448-0_5
   Redmon J, 2018, Arxiv, DOI [arXiv:1804.02767, DOI 10.1109/CVPR.2017.690, DOI 10.48550/ARXIV.1804.02767]
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Shrivastava A, 2016, PROC CVPR IEEE, P761, DOI 10.1109/CVPR.2016.89
   Tian Z, 2019, IEEE I CONF COMP VIS, P9626, DOI 10.1109/ICCV.2019.00972
   Tsung-Yi Lin, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P2999, DOI 10.1109/ICCV.2017.324
   Uijlings JRR, 2013, INT J COMPUT VISION, V104, P154, DOI 10.1007/s11263-013-0620-5
   Viola P, 2001, PROC CVPR IEEE, P511, DOI 10.1109/cvpr.2001.990517
   Weber M, 2019, ARXIV PREPRINT ARXIV
   Wei LX, 2021, VISUAL COMPUT, V37, P133, DOI 10.1007/s00371-019-01787-3
   Yu F, 2020, PROC CVPR IEEE, P2633, DOI 10.1109/CVPR42600.2020.00271
   Zhou K, 2016, DESTECH TRANS COMP
   Zitnick CL, 2014, LECT NOTES COMPUT SC, V8693, P391, DOI 10.1007/978-3-319-10602-1_26
NR 42
TC 20
Z9 20
U1 11
U2 57
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2022
VL 38
IS 3
BP 1051
EP 1063
DI 10.1007/s00371-021-02067-9
EA JAN 2021
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZQ8YX
UT WOS:000609058300001
DA 2024-07-18
ER

PT J
AU Liu, XP
   Huang, H
   Wang, WM
   Zhou, J
AF Liu, Xiuping
   Huang, Hua
   Wang, Weiming
   Zhou, Jun
TI Multi-view 3D shape style transformation
SO VISUAL COMPUTER
LA English
DT Article
DE 3D style transformation; Shape reconstruction; Shape modeling
AB It is a challenging task to transform style of 3D shapes for generating diverse outputs with learning-based methods. The reasons include two folds: (1) the lack of training data with different styles and (2) multi-modal information of 3D shapes which are hard to disentangle. In this work, a multi-view-based neural network model is proposed to learn style transformation while preserving contents of 3D shapes from unpaired domains. Given two sets of shapes in different style domains, such as Japanese chairs and Ming chairs, multi-view representations of each shape are calculated, and style transformation between these two sets is learnt based on these representations. This multi-view representation not only preserves the structural details of a 3D shape, but also ensures the richness of the training data. At test stage, transformed maps are generated with the trained network by the combination of the extracted style/content features from multi-view representation and new style features. Then, transformed maps are consolidated into a 3D point cloud by solving a domain-stability optimization problem. Depth maps from all viewpoints are fused to obtain a shape whose style is similar to the target shape. Experimental results demonstrate that the proposed method outperforms the baselines and state-of-the-art approaches on style transformation.
C1 [Liu, Xiuping; Huang, Hua; Wang, Weiming; Zhou, Jun] Dalian Univ Technol, Dalian, Peoples R China.
C3 Dalian University of Technology
RP Huang, H (corresponding author), Dalian Univ Technol, Dalian, Peoples R China.
EM huahuang@mail.dlut.edu.cn; zj.9004@gmail.com
RI Zhou, Jun/AEQ-5024-2022; Liu, Xiufang/I-8003-2015; Liu,
   Xiu/IYJ-9134-2023; Wang, Weiming/AAV-9848-2021; Wang,
   Weiming/JBR-9617-2023; Wang, Weiming/H-4944-2017
OI Zhou, Jun/0000-0001-9316-9785; Wang, Weiming/0000-0001-6289-0094
FU Fundamental Research Fund [DUT18RC(4)064]; Natural Science Foundation of
   China (NSFC) [61762064]; Jiangxi Science Fund for Distinguished Young
   Scholars [20192BCBL23001]
FX The work was supported by the Fundamental Research Fund (DUT18RC(4)064),
   the Natural Science Foundation of China (NSFC) under Grant 61762064, and
   Jiangxi Science Fund for Distinguished Young Scholars (20192BCBL23001).
CR [Anonymous], 2015, CoRR abs/1511.06702
   [Anonymous], 2016, ICML
   Benaim S, 2017, ADV NEUR IN, V30
   Bousmalis K, 2017, PROC CVPR IEEE, P95, DOI 10.1109/CVPR.2017.18
   Chen DY, 2003, COMPUT GRAPH FORUM, V22, P223, DOI 10.1111/1467-8659.00669
   Chen Xi, 2016, Advances in Neural Information Processing Systems (NIPS), V29
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Dosovitskiy A, 2015, PROC CVPR IEEE, P1538, DOI 10.1109/CVPR.2015.7298761
   Eigen D, 2015, IEEE I CONF COMP VIS, P2650, DOI 10.1109/ICCV.2015.304
   Fan HQ, 2017, PROC CVPR IEEE, P2463, DOI 10.1109/CVPR.2017.264
   Gao L, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275028
   Gatys L., 2016, Journal of Vision, V16, P326, DOI DOI 10.1167/16.12.326
   Hertzmann A, 2001, COMP GRAPH, P327, DOI 10.1145/383259.383295
   Hoiem D, 2005, IEEE I CONF COMP VIS, P654
   Hu RZ, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3092817
   Huang X, 2018, LECT NOTES COMPUT SC, V11207, P179, DOI 10.1007/978-3-030-01219-9_11
   Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167
   Isola Phillip, 2016, ARXIV161107004, DOI [DOI 10.1109/CVPR.2017.632, 10.1109/CVPR.2017.632]
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kazhdan M, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2487228.2487237
   Kim T, 2017, PR MACH LEARN RES, V70
   King DB, 2015, ACS SYM SER, V1214, P1
   Kingma D. P., 2013, ARXIV13126114
   Kutulakos KN, 2000, INT J COMPUT VISION, V38, P199, DOI 10.1023/A:1008191222954
   Larsson G, 2016, LECT NOTES COMPUT SC, V9908, P577, DOI 10.1007/978-3-319-46493-0_35
   Lee HY, 2018, LECT NOTES COMPUT SC, V11205, P36, DOI 10.1007/978-3-030-01246-5_3
   Li C, 2016, PROC CVPR IEEE, P2479, DOI 10.1109/CVPR.2016.272
   Li YT, 2017, ADV NEUR IN, V30
   Liu MY, 2017, ADV NEUR IN, V30
   Liu TQ, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766898
   Lun ZL, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980237
   Ma CY, 2014, COMPUT GRAPH FORUM, V33, P175, DOI 10.1111/cgf.12307
   Park E, 2017, PROC CVPR IEEE, P702, DOI 10.1109/CVPR.2017.82
   POTMESIL M, 1987, COMPUT VISION GRAPH, V40, P1, DOI 10.1016/0734-189X(87)90053-3
   Press O., 2018, EMERGING DISENTANGLE
   Rock J, 2015, PROC CVPR IEEE, P2484, DOI 10.1109/CVPR.2015.7298863
   Rusinkiewicz S, 2001, THIRD INTERNATIONAL CONFERENCE ON 3-D DIGITAL IMAGING AND MODELING, PROCEEDINGS, P145, DOI 10.1109/IM.2001.924423
   Saxena A, 2009, IEEE T PATTERN ANAL, V31, P824, DOI 10.1109/TPAMI.2008.132
   Schor N, 2019, IEEE I CONF COMP VIS, P8758, DOI 10.1109/ICCV.2019.00885
   Shin D, 2018, PROC CVPR IEEE, P3061, DOI 10.1109/CVPR.2018.00323
   Shrivastava A, 2017, PROC CVPR IEEE, P2242, DOI 10.1109/CVPR.2017.241
   Soltani AA, 2017, PROC CVPR IEEE, P2511, DOI 10.1109/CVPR.2017.269
   Taigman Y., 2016, INT C LEARN REPR
   Tatarchenko M, 2016, LECT NOTES COMPUT SC, V9911, P322, DOI 10.1007/978-3-319-46478-7_20
   Van Gool, 2018, ARXIV180511145
   Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917
   Wang XL, 2015, PROC CVPR IEEE, P539, DOI 10.1109/CVPR.2015.7298652
   Yang J., 2015, Advances in Neural Information Processing Systems, P1099
   Yi ZL, 2017, IEEE I CONF COMP VIS, P2868, DOI 10.1109/ICCV.2017.310
   Yin KX, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201288
   Zhang, 2019, ARXIV190310170
   Zhang R, 2016, LECT NOTES COMPUT SC, V9907, P649, DOI 10.1007/978-3-319-46487-9_40
   Zhou TH, 2016, LECT NOTES COMPUT SC, V9908, P286, DOI 10.1007/978-3-319-46493-0_18
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 54
TC 4
Z9 4
U1 2
U2 20
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2022
VL 38
IS 2
BP 669
EP 684
DI 10.1007/s00371-020-02042-w
EA JAN 2021
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZE7IA
UT WOS:000605853500003
DA 2024-07-18
ER

PT J
AU Singh, SK
   Srivastava, R
AF Singh, Surya Kant
   Srivastava, Rajeev
TI A robust RGBD saliency method with improved probabilistic contrast and
   the global reference surface
SO VISUAL COMPUTER
LA English
DT Article
DE Salient object detection; Poisson probabilistic modeling; Probabilistic
   contrast; Background elimination; Global saliency; Regional saliency;
   Depth saliency
ID OBJECT DETECTION; CONTOUR; OPTIMIZATION
AB The human attention mechanism inspires salient object detection. Most of the saliency methods work on 2D perception mechanisms, while human attention systems work on 3D perception mechanisms. This proposed method makes use of depth information from RGBD to robustly and correctly detect the salient object in a complex and clutter background. The saliency of regions related to object border increases in Poisson probabilistic contrast space while distinguishing the conspicuous object in a complex and clutter background. This process produces a global concave reference surface. This global reference plane integrated with intra-regional spatial, structural, color, and depth information detects the salient object correctly. Background estimation and central saliency integration thoroughly remove the background. This algorithm generates a robust conspicuous object. The experimental result presented here shows that the proposed method performs better in comparison to the recent, highly referenced and closely related fourteen state-of-the-art methods, and the three publicly available complex RGBD datasets and six evaluation parameters.
C1 [Singh, Surya Kant; Srivastava, Rajeev] Banaras Hindu Univ, Indian Inst Technol, Dept Comp Sci & Engn, Varanasi 221005, Uttar Pradesh, India.
C3 Indian Institute of Technology System (IIT System); Indian Institute of
   Technology BHU Varanasi (IIT BHU Varanasi); Banaras Hindu University
   (BHU)
RP Singh, SK (corresponding author), Banaras Hindu Univ, Indian Inst Technol, Dept Comp Sci & Engn, Varanasi 221005, Uttar Pradesh, India.
EM suryakantsingh20@gmail.com; rajeev.cse@iitbhu.ac.in
RI Srivastava, Rajeev/C-7906-2016
OI Srivastava, Rajeev/0000-0002-0165-1556
CR Achanta R, 2010, IEEE IMAGE PROC, P2653, DOI 10.1109/ICIP.2010.5652636
   Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   [Anonymous], 2005, Neurobiology of attention
   Chen YT, 2014, ECS TRANSACTIONS, V64, P23, DOI 10.1149/06416.0023ecst
   Cheng G, 2019, IEEE T IMAGE PROCESS, V28, P265, DOI 10.1109/TIP.2018.2867198
   Cheng MM, 2013, IEEE I CONF COMP VIS, P1529, DOI 10.1109/ICCV.2013.193
   Cheng MM, 2011, PROC CVPR IEEE, P409, DOI 10.1109/CVPR.2011.5995344
   Cheng MCN, 2014, RES MATH SCI, V1, DOI 10.1186/2197-9847-1-3
   Cong RM, 2016, IEEE SIGNAL PROC LET, V23, DOI 10.1109/LSP.2016.2557347
   Fan DP, 2017, IEEE I CONF COMP VIS, P4558, DOI 10.1109/ICCV.2017.487
   Fan DP, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P698
   Fan DP, 2021, IEEE T NEUR NET LEAR, V32, P2075, DOI 10.1109/TNNLS.2020.2996406
   Fan DP, 2019, PROC CVPR IEEE, P8546, DOI 10.1109/CVPR.2019.00875
   Feng D, 2016, PROC CVPR IEEE, P2343, DOI 10.1109/CVPR.2016.257
   Fu KR, 2020, PROC CVPR IEEE, P3049, DOI 10.1109/CVPR42600.2020.00312
   Grigorescu C, 2004, IMAGE VISION COMPUT, V22, P609, DOI 10.1016/j.imavis.2003.12.004
   Harremoës P, 2001, IEEE T INFORM THEORY, V47, P2039, DOI 10.1109/18.930936
   Hou XD, 2007, PROC CVPR IEEE, P2280
   Huang XM, 2017, IEEE T IMAGE PROCESS, V26, P4243, DOI 10.1109/TIP.2017.2710636
   Ju R, 2014, IEEE IMAGE PROC, P1115, DOI 10.1109/ICIP.2014.7025222
   Kapoor A, 2017, VISUAL COMPUT, V33, P665, DOI 10.1007/s00371-016-1216-1
   Kienzle W, 2009, J VISION, V9, DOI 10.1167/9.5.7
   Kourtzi Z, 2001, SCIENCE, V293, P1506, DOI 10.1126/science.1061133
   Li GY, 2020, IEEE T IMAGE PROCESS, V29, P4873, DOI 10.1109/TIP.2020.2976689
   Li X, 2013, IEEE I CONF COMP VIS, P3328, DOI 10.1109/ICCV.2013.413
   Li Y, 2014, PROC CVPR IEEE, P280, DOI 10.1109/CVPR.2014.43
   Li Z, 2018, J VIS COMMUN IMAGE R, V50, P16, DOI 10.1016/j.jvcir.2017.11.004
   Lv Q, 2016, COGN NEURODYNAMICS, V10, P255, DOI 10.1007/s11571-015-9372-y
   Malik J, 2001, INT J COMPUT VISION, V43, P7, DOI 10.1023/A:1011174803800
   Margolin R, 2013, PROC CVPR IEEE, P1139, DOI 10.1109/CVPR.2013.151
   Niu YZ, 2012, PROC CVPR IEEE, P454, DOI 10.1109/CVPR.2012.6247708
   Peng HW, 2014, LECT NOTES COMPUT SC, V8691, P92, DOI 10.1007/978-3-319-10578-9_7
   Perazzi F, 2012, PROC CVPR IEEE, P733, DOI 10.1109/CVPR.2012.6247743
   Qin Y, 2015, PROC CVPR IEEE, P110, DOI 10.1109/CVPR.2015.7298606
   Qu LQ, 2017, IEEE T IMAGE PROCESS, V26, P2274, DOI 10.1109/TIP.2017.2682981
   Salakhutdinov, 2015, ARXIV151104119
   SERFLING RJ, 1975, ANN PROBAB, V3, P726, DOI 10.1214/aop/1176996313
   Shi JP, 2016, IEEE T PATTERN ANAL, V38, P717, DOI 10.1109/TPAMI.2015.2465960
   Singh SK, 2019, J MATH IMAGING VIS, V61, P990, DOI 10.1007/s10851-019-00882-3
   Tang YL, 2016, VISUAL COMPUT, V32, P111, DOI 10.1007/s00371-014-1059-6
   Tu WC, 2016, PROC CVPR IEEE, P2334, DOI 10.1109/CVPR.2016.256
   Wang L, 2009, SIGNAL PROCESS, V89, P2435, DOI 10.1016/j.sigpro.2009.03.014
   Wang NN, 2019, IEEE ACCESS, V7, P55277, DOI 10.1109/ACCESS.2019.2913107
   Wang WG, 2018, IEEE T IMAGE PROCESS, V27, P38, DOI 10.1109/TIP.2017.2754941
   Wang WG, 2018, IEEE T PATTERN ANAL, V40, P20, DOI 10.1109/TPAMI.2017.2662005
   Wang WG, 2015, IEEE T IMAGE PROCESS, V24, P4185, DOI 10.1109/TIP.2015.2460013
   Zhang J., 2020, P IEEE CVF C COMP VI, P8579, DOI DOI 10.1109/CVPR42600.2020.00861
   Zhang JM, 2016, IEEE T PATTERN ANAL, V38, P889, DOI 10.1109/TPAMI.2015.2473844
   Zhang LH, 2017, IEEE T PATTERN ANAL, V39, P1892, DOI 10.1109/TPAMI.2016.2609426
   Zhang LY, 2008, J VISION, V8, DOI 10.1167/8.7.32
   Zhang M, 2020, PROC CVPR IEEE, P3469, DOI 10.1109/CVPR42600.2020.00353
   Zhang Q, 2018, VISUAL COMPUT, V34, P473, DOI 10.1007/s00371-017-1354-0
   Zhang WC, 2015, PATTERN RECOGN, V48, P2785, DOI 10.1016/j.patcog.2015.03.021
   Zhao R, 2015, PROC CVPR IEEE, P1265, DOI 10.1109/CVPR.2015.7298731
   Zhong GY, 2016, VISUAL COMPUT, V32, P611, DOI 10.1007/s00371-015-1077-z
   Zhou XN, 2019, VISUAL COMPUT, V35, P385, DOI 10.1007/s00371-018-1471-4
   Zhu Chunbiao, 2017, ICCVW
   Zhu CS, 2017, IEEE COMMUN MAG, V55, P14, DOI 10.1109/MCOM.2017.1700142
   Zhu WJ, 2014, PROC CVPR IEEE, P2814, DOI 10.1109/CVPR.2014.360
NR 59
TC 0
Z9 0
U1 2
U2 14
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2022
VL 38
IS 3
BP 797
EP 809
DI 10.1007/s00371-020-02050-w
EA JAN 2021
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZQ8YX
UT WOS:000605494100003
DA 2024-07-18
ER

PT J
AU Tregel, T
   Müller, PN
   Göbel, S
   Steinmetz, R
AF Tregel, Thomas
   Meuller, Philipp Niklas
   Goebel, Stefan
   Steinmetz, Ralf
TI Looking for Charizard: applying the orienteering problem to
   location-based games
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 10th International Conference on Virtual Worlds and Games for Serious
   Applications (VS-Games)
CY SEP 05-07, 2018
CL Wurzburg, GERMANY
DE Location-based games; Optimization; Mobile devices; Orienteering problem
AB Along with the high popularity of location-based games in the mid-summer of 2016 caused by the release of Pokemon GO, tool-assisted gameplay rose in demand in order to increase the individual player's performance within the game. The location-based accumulation of Pokemon presents the continuing challenge for players to expand their collection. As game locations are fixed and have a fixed time interval in which they provide players with a chance to catch a Pokemon, optimized routes that maximize the chance or frequency of encounters were in high demand. However, personalized routes are hard to create due to the amount of available game locations, their distance between each other, and the associated time constraints for real-world travel. This paper presents a system which allows the sensitive creation of personalized routes for players. These routes can be fully customized regarding the player's out-of-game and in-game goal, allowing them to e.g. specify their movement type or in-game preferences. We evaluate the system using a dataset of Berlin containing over 30,000 distinct locations with different associated characteristics and show the performance of different solution approaches for the generalized orienteering problem. It is designed as a player assistance system allowing the usage on mobile devices to assure its applicability in the context of location-based games even beyond Pokemon GO. We show the feasibility of our approach regarding real-time calculation allowing players to quickly modify or adapt their route when deviating from the planned route.
C1 [Tregel, Thomas; Meuller, Philipp Niklas; Goebel, Stefan; Steinmetz, Ralf] TU, Multimedia Commun Lab, Darmstadt, Germany.
C3 Technical University of Darmstadt
RP Tregel, T (corresponding author), TU, Multimedia Commun Lab, Darmstadt, Germany.
EM thomas.tregel@kom.tu-darmstadt.de
RI Steinmetz, Patrick R. H./AAD-4093-2022
OI Tregel, Thomas/0000-0003-0715-3889; Steinmetz, Ralf/0000-0002-6839-9359;
   Muller, Philipp Niklas/0000-0001-9660-691X
CR Aarts E., 2014, SIMULATED ANNEALING, P265, DOI [10.1007/978-1-4614-6940-7_10, DOI 10.1007/978-1-4614-6940-7_10]
   Abdel-Moetty S., 2010, INF SYST INFOS 2010, P1
   Althoff T, 2016, J MED INTERNET RES, V18, DOI 10.2196/jmir.6759
   [Anonymous], 2016, Proceedings of ML-95
   [Anonymous], 2013, Encyclopedia of Operations Research and Management Science
   [Anonymous], 1997, Local Search in Combinatorial Optimization
   [Anonymous], 1997, Complexity
   Applegate DL., 2011, TRAVELING SALESMAN P
   Arigliano A, 2019, DISCRETE APPL MATH, V261, P28, DOI 10.1016/j.dam.2018.09.017
   Ascheuer N, 2001, MATH PROGRAM, V90, P475, DOI 10.1007/PL00011432
   BAKER EK, 1983, OPER RES, V31, P938, DOI 10.1287/opre.31.5.938
   Clegg K. D., 2014, INT C PAR PROBL SOLV, P692
   Edelkamp S, 2013, PROCEEDINGS OF THE 2013 IEEE SYMPOSIUM ON COMPUTATIONAL INTELLIGENCE IN SCHEDULING (CISCHED), P44, DOI 10.1109/SCIS.2013.6613251
   Fortnow L, 2009, COMMUN ACM, V52, P78, DOI 10.1145/1562164.1562186
   Gendreau M, 1998, OPER RES, V46, P330, DOI 10.1287/opre.46.3.330
   Glover F., 1990, ORSA Journal on Computing, V2, P4, DOI [10.1287/ijoc.1.3.190, 10.1287/ijoc.2.1.4]
   GOLDEN BL, 1987, NAV RES LOG, V34, P307, DOI 10.1002/1520-6750(198706)34:3<307::AID-NAV3220340302>3.0.CO;2-D
   Guardian, 2016, POKEMON NO GAMES DAI
   HAJEK B, 1988, MATH OPER RES, V13, P311, DOI 10.1287/moor.13.2.311
   Hedges A., Finding Distances Based on Latitude and Longitude
   Hoos HH, 2014, EUR J OPER RES, V238, P87, DOI 10.1016/j.ejor.2014.03.042
   Hub, 2016, RES POKEMON GO SPAWN
   Humphreys F.J., 2012, Recrystallization and Related Annealing Phenomena
   INGBER L, 1993, MATH COMPUT MODEL, V18, P29, DOI 10.1016/0895-7177(93)90204-C
   Jones JM, 2014, NUTR J, V13, DOI 10.1186/1475-2891-13-34
   KAHNG A, 2004, NUTR J, V32, P499, DOI DOI 10.1016/j.orl.2004.04.001
   KANTOR MG, 1992, J OPER RES SOC, V43, P629, DOI 10.1057/jors.1992.88
   Karp R.M., 2009, 50 YEARS INTEGER PRO, P219, DOI [10.1007/978-3-540-68279-0_8, DOI 10.1007/978-3-540-68279-0_8]
   KIRKPATRICK S, 1983, SCIENCE, V220, P671, DOI 10.1126/science.220.4598.671
   Kompf, DISTANCE CALCULATION
   LENSTRA J, 1981, SCIENCE, V11, P221
   LOPEZIBANEZ M, 2010, NETWORKS, V37, P1570, DOI DOI 10.1016/j.cor.2009.11.015
   Mansini R., 2006, 20060252 RT U BRESC
   MASUTTI T, 2009, COMPUT OPER RES, V179, P1454, DOI DOI 10.1016/j.ins.2008.12.016
   Menger K, 1930, MATH ANN, V103, P466, DOI 10.1007/BF01455705
   Mladenovic N, 2013, YUGOSLAV J OPER RES, V23, P19, DOI DOI 10.2298/YJOR120530015M
   RIGHINI G, 2009, MATH ANN, V36, P1191, DOI DOI 10.1016/j.cor.2008.01.003
   Savelsbergh M. W. P., 1985, Annals of Operations Research, V4, P285, DOI 10.1007/BF02022044
   Sudwest, 2017, JIM 2017 JUGEND INFO
   Toulouse, 2014, OPTIM PROBL NEW APPR, V2, P173
   TREGEL T, 2017, COMPUT OPER RES, P212, DOI DOI 10.1007/978-3-319-70111-0_20
   Tregel Thomas, 2018, 2018 10 INT C VIRTUA, P1
   TSAI H, 2004, LECT NOTES COMPUT SC, V34, P1718, DOI DOI 10.1109/TSMCB.2004.828283
   VANSTEENWEGEN P, 2011, IEEE T SYST MAN CY B, V209, P1, DOI DOI 10.1016/j.ejor.2010.03.045
   Zerubia J., 2007, THESIS INRIA
   Zhong JH, 2006, INTERNATIONAL CONFERENCE ON COMPUTATIONAL INTELLIGENCE FOR MODELLING, CONTROL & AUTOMATION JOINTLY WITH INTERNATIONAL CONFERENCE ON INTELLIGENT AGENTS, WEB TECHNOLOGIES & INTERNET COMMERCE, VOL 2, PROCEEDINGS, P1115
NR 46
TC 5
Z9 5
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2021
VL 37
IS 1
BP 31
EP 45
DI 10.1007/s00371-019-01737-z
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA QE3UO
UT WOS:000616134400004
DA 2024-07-18
ER

PT J
AU Sharma, PK
   Basavaraju, S
   Sur, A
AF Sharma, Prasen Kumar
   Basavaraju, Sathisha
   Sur, Arijit
TI Deep learning-based image de-raining using discrete Fourier
   transformation Analyzing behaviour of deep CNNs towards uncorrelated
   transformed domain data
SO VISUAL COMPUTER
LA English
DT Article
DE Image de-raining; Deep learning; Convolutional neural networks; Discrete
   Fourier transformation
ID STREAKS REMOVAL; DECOMPOSITION
AB Single image rain streak removal is a well-explored topic in the field of computer vision. The de-raining problem is modeled as an image decomposition task where a rainy image is decomposed into rain-free background image and rain streek map. Unlike most of the existing de-raining methods, this paper attempts to decompose the rainy image in the frequency domain. The idea is inspired by pseudo-periodic characteristics of the noise signal (here the rain streaks) which leave some traces in the frequency domain, and the same can be utilized to predict the noise signal. In this paper, a deep learning-based rain streak prediction model is proposed which learns in discrete Fourier transform Oppenheim and Schafer (Discrete-TimeSignal Processing, Prentice Hall, Upper Saddle River, 1989) domain. To the best of our knowledge, this is the first approach where compressed domain coefficients are directly used as input to a deep convolutional neural network. The proposed model has been tested on publicly available synthetic datasets Fu et al. (in: 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017. 10.1109/CVPR.2017.186, Yang et al. (in: 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017. 10.1109/CVPR.2017.183), Yeh et al. (in: 2015 IEEE International Conference on Consumer Electronics-Taiwan, 2015. 10.1109/ICCE-TW.2015.7216999) and results are found to be comparable with the state of the art methods in the spatial domain. The presented analysis and study have an obvious indication to extend transform domain input to train the deep learning architecture especially image de-noising like problems.
C1 [Sharma, Prasen Kumar; Basavaraju, Sathisha; Sur, Arijit] Indian Inst Technol, Multimedia Lab, Dept Comp Sci & Engn, Gauhati, India.
C3 Indian Institute of Technology System (IIT System); Indian Institute of
   Technology (IIT) - Guwahati
RP Sharma, PK (corresponding author), Indian Inst Technol, Multimedia Lab, Dept Comp Sci & Engn, Gauhati, India.
EM kumar176101005@iitg.ac.in; b.sathisha@iitg.ac.in; arijit@iitg.ac.in
RI Sur, Arijit/AAB-4216-2020; Sharma, Prasen Kumar/ABG-8211-2020
OI Sharma, Prasen Kumar/0000-0003-4847-8866
FU Ministry of Human Resource Development, Government of India
FX Authors would like to thank the anonymous reviewers for their insightful
   comments and suggestions. Authors would also like to acknowledge the
   funding agency, Ministry of Human Resource Development, Government of
   India.
CR Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265
   Arbeláez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161
   Chang Y, 2017, IEEE I CONF COMP VIS, P1735, DOI 10.1109/ICCV.2017.191
   Chen DY, 2014, IEEE T CIRC SYST VID, V24, P1430, DOI 10.1109/TCSVT.2014.2308627
   Chen Q, 2017, IEEE T ELECTRON DEV, V64, P463, DOI 10.1109/TED.2016.2636322
   Chung H, 2018, IEEE W SP LANG TECH, P1, DOI [10.1109/SLT.2018.8639524, 10.1109/CACS.2018.8606774]
   Davis C., 1962, Numer Math, V4, P343, DOI DOI 10.1007/BF01386329
   Dunham W., 1999, EULER MASTER US ALL
   Fu XY, 2017, PROC CVPR IEEE, P1715, DOI 10.1109/CVPR.2017.186
   Fu XY, 2017, IEEE T IMAGE PROCESS, V26, P2944, DOI 10.1109/TIP.2017.2691802
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Gu SH, 2017, IEEE I CONF COMP VIS, P1717, DOI 10.1109/ICCV.2017.189
   Haar A, 1910, MATH ANN, V69, P331, DOI 10.1007/BF01456326
   He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hore Alain, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P2366, DOI 10.1109/ICPR.2010.579
   Huang DA, 2014, IEEE T MULTIMEDIA, V16, P83, DOI 10.1109/TMM.2013.2284759
   Kang LW, 2012, IEEE T IMAGE PROCESS, V21, P1742, DOI 10.1109/TIP.2011.2179057
   Katznelson Y., 1976, An introduction to harmonic analysis, corrected ed.
   Lee JH, 2018, PROC CVPR IEEE, P330, DOI 10.1109/CVPR.2018.00042
   Li Y, 2017, IEEE T IMAGE PROCESS, V26, P3874, DOI 10.1109/TIP.2017.2708841
   Li Y, 2016, PROC CVPR IEEE, P2736, DOI 10.1109/CVPR.2016.299
   Lian NX, 2006, IEEE T IMAGE PROCESS, V15, P2575, DOI 10.1109/TIP.2006.877409
   Luo Y, 2015, IEEE I CONF COMP VIS, P3397, DOI 10.1109/ICCV.2015.388
   Oppenheim A. V., 1989, Discrete -Time Signal Processing
   Pratt H, 2017, LECT NOTES ARTIF INT, V10534, P786, DOI 10.1007/978-3-319-71249-9_47
   Ren WH, 2017, PROC CVPR IEEE, P2838, DOI 10.1109/CVPR.2017.303
   Schaefer G, 2004, PROC SPIE, V5307, P472, DOI 10.1117/12.525375
   Sharma PK, 2019, IEEE IMAGE PROC, P2796, DOI [10.1109/icip.2019.8803353, 10.1109/ICIP.2019.8803353]
   Shen L, 2018, ARXIVABS180106769
   Wang T, 2019, IEEE IPCCC, DOI 10.1109/ipccc47392.2019.8958751
   Wang YL, 2016, IEEE IMAGE PROC, P4087, DOI 10.1109/ICIP.2016.7533128
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Yang W., 2017, PROC CVPR IEEE, P1685, DOI DOI 10.1109/CVPR.2017.183
   Yeh CH, 2015, IEEE ICCE, P462, DOI 10.1109/ICCE-TW.2015.7216999
   Yu SJ, 2015, 2015 IEEE CHINA SUMMIT & INTERNATIONAL CONFERENCE ON SIGNAL AND INFORMATION PROCESSING, P215, DOI 10.1109/ChinaSIP.2015.7230394
   Zhang H, 2018, PROC CVPR IEEE, P695, DOI 10.1109/CVPR.2018.00079
   Zhang H, 2017, IEEE WINT CONF APPL, P1259, DOI 10.1109/WACV.2017.145
   Zhu L, 2017, IEEE I CONF COMP VIS, P2545, DOI 10.1109/ICCV.2017.276
NR 40
TC 8
Z9 8
U1 0
U2 8
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2021
VL 37
IS 8
BP 2083
EP 2096
DI 10.1007/s00371-020-01971-w
EA SEP 2020
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TT5FX
UT WOS:000570982400001
DA 2024-07-18
ER

PT J
AU Bajo, JM
   Delrieux, C
   Patow, G
AF Miguel Bajo, Juan
   Delrieux, Claudio
   Patow, Gustavo
TI Physically inspired technique for modeling wet absorbent materials
SO VISUAL COMPUTER
LA English
DT Article
DE Rendering; Natural phenomena; Physically inspired modeling
ID TIME; SIMULATION
AB The visual appearance of materials depends on their intrinsic light transfer properties, the illumination and camera conditions, and other environmental factors. This is in particular the case of porous, rough, or absorbent materials, where the presence of liquid on the surface alters significantly their BRDF, which in turn results in considerable changes in their visual appearance. For this reason, rendering materials change their appearance when wet continues to be a relevant topic in computer graphics. This is especially true when real-time photo-realistic rendering is required in scenes involving this kind of materials in interaction with water or other liquids. In this paper, we introduce a physically inspired technique to model and render appearance changes of absorbent materials when their surface is wet. First, we develop a new method to solve the interaction between the liquid and the object surface using its own underlying texture coordinates. Then, we propose an algorithm to model the diffusion phenomenon that occurs in the interface between a solid porous object and a liquid. Finally, we extend a model that explains the change of appearance of materials under wet conditions, and we implement it achieving real-time performance. The complete model is developed using GPU acceleration.
C1 [Miguel Bajo, Juan; Delrieux, Claudio] Univ Nacl Sur, Dept Elect & Comp Engn, Bahia Blanca, Buenos Aires, Argentina.
   [Miguel Bajo, Juan] Univ Nacl Sur, Dept Comp Sci & Engn, Bahia Blanca, Buenos Aires, Argentina.
   [Patow, Gustavo] UdG, ViRVIG, Girona, Spain.
   [Delrieux, Claudio] Consejo Nacl Invest Cient & Tecn, LCI, Bahia Blanca, Buenos Aires, Argentina.
C3 National University of the South; National University of the South;
   Universitat de Girona; Consejo Nacional de Investigaciones Cientificas y
   Tecnicas (CONICET)
RP Bajo, JM (corresponding author), Univ Nacl Sur, Dept Elect & Comp Engn, Bahia Blanca, Buenos Aires, Argentina.; Bajo, JM (corresponding author), Univ Nacl Sur, Dept Comp Sci & Engn, Bahia Blanca, Buenos Aires, Argentina.
EM jmbajo@gmail.com
RI ; Patow, Gustavo A./L-2943-2015
OI Delrieux, Claudio/0000-0002-2727-8374; Bajo, Juan
   Miguel/0000-0002-5071-5886; Patow, Gustavo A./0000-0002-1977-9101
FU National Science and Technology Council of Argentina (CONICET);
   Universidad Nacional del Sur (Argentina) [24/K061]; Electric and
   Computing Dept. of the Universidad Nacional del Sur; Ministerio de
   Ciencia, Innovacion y Universidades, Spain [TIN2017-88515-C2-2-R]
FX JB was partially supported by a scholarship of the National Science and
   Technology Council of Argentina (CONICET). The research was partially
   funded by grant 24/K061 of the Universidad Nacional del Sur (Argentina).
   CD is supported by the Electric and Computing Dept. of the Universidad
   Nacional del Sur and the National Science and Technology Council of
   Argentina (CONICET). GP was partially funded by the TIN2017-88515-C2-2-R
   project from Ministerio de Ciencia, Innovacion y Universidades, Spain.
CR Angstrom A., 1925, GEOGR ANN, V7, P323, DOI DOI 10.2307/519495
   [Anonymous], 2018, Real-Time Rendering
   [Anonymous], 2014, WORKSH VIRT REAL INT
   [Anonymous], 2008, Fluid Simulation for Computer Graphics
   Azencot Omri, 2015, P 14 ACM SIGGRAPH EU, P137, DOI DOI 10.1145/2786784.2786793
   Bajo JM, 2020, COMPUT GRAPH FORUM, V39, P217, DOI 10.1111/cgf.14013
   Borshukov G., 2003, ACM SIGGRAPH 2005 CO, DOI 10.1145/1198555.1198593
   Chen TF, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2701416
   Chu NSH, 2005, ACM T GRAPHIC, V24, P504, DOI 10.1145/1073204.1073221
   Crank John, 1980, The Mathematics of Diffusion, V2nd
   dEon E., 2007, P EUR S REND TECHN, P147
   Dorsey J, 2008, MKS COMP GRAPH GEOME, P1
   FICK A, 1995, J MEMBRANE SCI, V100, P33, DOI 10.1016/0376-7388(94)00230-V
   Gao M, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201309
   Gu JW, 2006, ACM T GRAPHIC, V25, P762, DOI 10.1145/1141911.1141952
   Hall C., 2009, Water transport in brick, stone and concrete
   Hnat K, 2006, REAL TIME WETTING PO, V15, P401
   Hoetzlein R., 2014, GPU TECHN C GTC SANT
   Huber Markus., 2011, ACM SIGGRAPH 2011 Posters, P10
   Iwasaki K, 2008, VISUAL COMPUT, V24, P77, DOI 10.1007/s00371-007-0186-8
   Jensen HW, 2001, COMP GRAPH, P511, DOI 10.1145/383259.383319
   Jensen HW, 1999, SPRING EUROGRAP, P273
   KATTAWAR GW, 1975, J QUANT SPECTROSC RA, V15, P839, DOI 10.1016/0022-4073(75)90095-3
   Kimmel BW, 2010, COMPUT GRAPH-UK, V34, P441, DOI 10.1016/j.cag.2010.04.002
   Kimmel B, 2007, OPT EXPRESS, V15, P9755, DOI 10.1364/OE.15.009755
   Lee H, 2010, VISUAL COMPUT, V26, P865, DOI 10.1007/s00371-010-0439-9
   Lefebvre S, 2006, ACM T GRAPHIC, V25, P579, DOI 10.1145/1141911.1141926
   LEKNER J, 1988, APPL OPTICS, V27, P1278, DOI 10.1364/AO.27.001278
   Lenaerts T, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360648
   Lin WC, 2015, COMPUT GRAPH-UK, V52, P33, DOI 10.1016/j.cag.2015.06.005
   Liu YQ, 2005, VISUAL COMPUT, V21, P727, DOI 10.1007/s00371-005-0314-2
   Lu J., 2005, EGNP, P7, DOI DOI 10.2312/NPH/NPH05/007-016
   Macklin M, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461984
   Mérillou S, 2000, IEEE T VIS COMPUT GR, V6, P306, DOI 10.1109/2945.895876
   Muller M., 2003, ACM SIGGRAPH/Eurographics Symposium on Computer Animation, P154
   Muskat M., 1946, The Flow of Homogeneous Fluids through Porous Media
   Patkar S, 2013, IEEE T VIS COMPUT GR, V19, P1592, DOI 10.1109/TVCG.2013.8
   Pharr M., 2010, PHYS BASED RENDERING
   Rungjiratananon W, 2012, COMPUT GRAPH FORUM, V31, P1993, DOI 10.1111/j.1467-8659.2012.03191.x
   Rungjiratananon W, 2008, COMPUT GRAPH FORUM, V27, P1887, DOI 10.1111/j.1467-8659.2008.01336.x
   Shepard D., 1968, P 1968 23 ACM NAT C, P517, DOI DOI 10.1145/800186.810616
   Stam J, 1999, COMP GRAPH, P121, DOI 10.1145/311535.311548
   STERN F, 1964, APPL OPTICS, V3, P111, DOI 10.1364/AO.3.000111
   Vantzos O, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275086
   Wang HM, 2007, SYMPOSIUM ON COMPUTER ANIMATION 2007: ACM SIGGRAPH/ EUROGRAPHICS SYMPOSIUM PROCEEDINGS, P229
   Weidlich A., 2011, SIGGRAPH AS 2011 COU, p20:1
   Yan H, 2009, COMPUT ANIMAT VIRT W, V20, P417, DOI 10.1002/cav.300
NR 47
TC 2
Z9 2
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2021
VL 37
IS 8
BP 2053
EP 2068
DI 10.1007/s00371-020-01963-w
EA SEP 2020
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TT5FX
UT WOS:000566900900003
DA 2024-07-18
ER

PT J
AU Couillaud, J
   Ziou, D
AF Couillaud, Julien
   Ziou, Djemel
TI Radiometric model for plenoptic image formation
SO VISUAL COMPUTER
LA English
DT Article
DE Computational imaging; Image formation; Light ray radiometry; Defocus
   blur; Light field
AB This paper presents a general plenoptic image formation model based on light ray radiometry. Most existing plenoptic image formation models focus on projection geometry inside a camera equipped with a specific optical device, such as an array of micro-thin-lenses, while light ray radiometry is incomplete or simply neglected. This study aims to describe a general view of light ray radiometry that is then used to define a plenoptic image formation model. This model is based on a radiometry model and light ray projection geometry; hence, it explains phenomena, such as light transport and light direction. Moreover, the proposed plenoptic image formation model describes several radiometric phenomena, such as haze, defocus blur or natural vignetting. The genericity of this model is shown by retrieving existing non-plenoptic models from it. Moreover, the proposed model is assessed by simulating various phenomena, including vignetting and defocus blur, from both synthetic and real data.
C1 [Couillaud, Julien; Ziou, Djemel] Univ Sherbrooke, MOIVRE, 2500 Blvd Univ, Sherbrooke, PQ J1K 2R1, Canada.
C3 University of Sherbrooke
RP Couillaud, J (corresponding author), Univ Sherbrooke, MOIVRE, 2500 Blvd Univ, Sherbrooke, PQ J1K 2R1, Canada.
EM julien.couillaud@usherbrooke.ca
CR Adelson E.H., 1991, Computational Models of Visual Processing, P3
   [Anonymous], 2005, Image Sensors and Signal Processing for Digital Still Cameras
   Avcibas I, 2001, PROC SPIE, V4314, P523, DOI 10.1117/12.435436
   Couillaud J, 2020, VISUAL COMPUT, V36, P237, DOI 10.1007/s00371-018-1599-2
   Dansereau D, 2004, 2004 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS, VOL 3, PROCEEDINGS, P549
   Deschênes F, 2004, IMAGE VISION COMPUT, V22, P35, DOI 10.1016/j.imavis.2003.08.003
   Faraday M, 1846, PHILOS MAG, V28, P245
   Hahne C, 2018, INT J COMPUT VISION, V126, P21, DOI 10.1007/s11263-017-1036-4
   Hahne C, 2016, OPT EXPRESS, V24, P21521, DOI 10.1364/OE.24.021521
   Hore Alain, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P2366, DOI 10.1109/ICPR.2010.579
   Horn BK, 1986, ROBOT VISION, P203
   Horstmeyer R., 2009, Computational Photography (ICCP), 2009 IEEE International Conference on, P1, DOI DOI 10.1109/ICCPHOT.2009.5559016
   Huang FC, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766922
   Isaksen A, 2000, COMP GRAPH, P297, DOI 10.1145/344779.344929
   Jeon HG, 2015, PROC CVPR IEEE, P1547, DOI 10.1109/CVPR.2015.7298762
   Joshi N, 2008, PROC CVPR IEEE, P3823
   Lai YS, 2012, INT C PATT RECOG, P388
   Levin A, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531403
   Levin J, 2008, TORT WARS, P88
   Levoy M., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P31, DOI 10.1145/237170.237199
   Liang CK, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2665075
   Liang CK, 2011, IEEE T IMAGE PROCESS, V20, P446, DOI 10.1109/TIP.2010.2063036
   Liang CK, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360654
   Narasimhan SG, 2002, INT J COMPUT VISION, V48, P233, DOI 10.1023/A:1016328200723
   Ng R, 2005, ACM T GRAPHIC, V24, P735, DOI 10.1145/1073204.1073256
   Ng R, 2005, RES REPORT, DOI DOI 10.1145/3097571
   PENTLAND AP, 1987, IEEE T PATTERN ANAL, V9, P523, DOI 10.1109/TPAMI.1987.4767940
   Schneider D, 2009, ISPRS J PHOTOGRAMM, V64, P259, DOI 10.1016/j.isprsjprs.2009.01.001
   Stewart J., 1996, Optical Principles and Technology for Engineers
   Subbarao M., 1988, Second International Conference on Computer Vision (IEEE Cat. No.88CH2664-1), P149, DOI 10.1109/CCV.1988.589986
   Tai YW, 2013, IEEE T PATTERN ANAL, V35, P2498, DOI 10.1109/TPAMI.2013.40
   Tang HX, 2013, IEEE INT CONF COMPUT
NR 32
TC 0
Z9 0
U1 0
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2021
VL 37
IS 6
BP 1369
EP 1383
DI 10.1007/s00371-020-01871-z
EA JUN 2020
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SO2LW
UT WOS:000541390400001
DA 2024-07-18
ER

PT J
AU Cai, JH
   Hu, JG
AF Cai, Jiahui
   Hu, Jianguo
TI 3D RANs: 3D Residual Attention Networks for action recognition
SO VISUAL COMPUTER
LA English
DT Article
DE Action recognition; 3D ResNets; Video classification; Attention
   mechanism
ID LSTM NETWORKS
AB In this work, we propose 3D Residual Attention Networks (3D RANs) for action recognition, which can learn spatiotemporal representation from videos. The proposed network consists of attention mechanism and 3D ResNets architecture, and it can capture spatiotemporal information in an end-to-end manner. Specifically, we separately add the attention mechanism along channel and spatial domain to each block of 3D ResNets. For each sliced tensor of an intermediate feature map, we sequentially infer channel and spatial attention maps by channel and spatial attention mechanism submodules in each residual unit block, and the attention maps are multiplied to the input feature map to reweight the key features. We validate our network through extensive experiments in UCF-101, HMDB-51 and Kinetics datasets. Our experiments show that the proposed 3D RANs are superior to the state-of-the-art approaches for action recognition, demonstrating the effectiveness of our networks.
C1 [Cai, Jiahui] Sun Yat Sen Univ, Sch Elect & Informat Technol, 132 East Waihuan Rd, Guangzhou 510006, Guangdong, Peoples R China.
   [Hu, Jianguo] Sun Yat Sen Univ, Sch Data & Comp Sci, 132 East Waihuan Rd, Guangzhou 510006, Guangdong, Peoples R China.
C3 Sun Yat Sen University; Sun Yat Sen University
RP Cai, JH (corresponding author), Sun Yat Sen Univ, Sch Elect & Informat Technol, 132 East Waihuan Rd, Guangzhou 510006, Guangdong, Peoples R China.
EM caijh33@mail2.sysu.edu.cn
FU Guangzhou Innovation and Entrepreneurship Leader Team [CXLJTD-201608];
   Development Research Institute of Guangzhou Smart City
FX This work was supported in part by the 2016 Guangzhou Innovation and
   Entrepreneurship Leader Team under Grant CXLJTD-201608 and the
   Development Research Institute of Guangzhou Smart City.
CR [Anonymous], P IEEE C COMP VIS PA
   [Anonymous], 2018, ARXIV181109795
   [Anonymous], EIDETIC 3D LSTM MODE
   [Anonymous], COMPUTER SCI
   [Anonymous], 2014, MULTIPLE OBJECT RECO
   Cao CS, 2015, IEEE I CONF COMP VIS, P2956, DOI 10.1109/ICCV.2015.338
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Corbetta M, 2002, NAT REV NEUROSCI, V3, P201, DOI 10.1038/nrn755
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Feichtenhofer C, 2016, PROC CVPR IEEE, P1933, DOI 10.1109/CVPR.2016.213
   Hara K, 2018, PROC CVPR IEEE, P6546, DOI 10.1109/CVPR.2018.00685
   Hara K, 2017, IEEE INT CONF COMP V, P3154, DOI 10.1109/ICCVW.2017.373
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Heng Wang, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3169, DOI 10.1109/CVPR.2011.5995407
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Ioffe S, 2015, P INT C MACH LEARN, V2015, P1
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Kay W., 2017, ARXIV170506950
   Kuehne H, 2013, HIGH PERFORMANCE COMPUTING IN SCIENCE AND ENGINEERING '12: TRANSACTIONS OF THE HIGH PERFORMANCE COMPUTING CENTER, STUTTGART (HLRS) 2012, P571, DOI 10.1007/978-3-642-33374-3_41
   Laptev I, 2008, PROC CVPR IEEE, P3222, DOI 10.1109/cvpr.2008.4587756
   Larochelle Hugo, 2010, ADV NEURAL INFORM PR, V23
   Li YJ, 2017, CHIN CONT DECIS CONF, P4221, DOI 10.1109/CCDC.2017.7979240
   Li YR, 2019, VISUAL COMPUT, V35, P1143, DOI 10.1007/s00371-019-01692-9
   Liang DD, 2020, VISUAL COMPUT, V36, P499, DOI 10.1007/s00371-019-01636-3
   Ma ZC, 2018, MULTIMED TOOLS APPL, V77, P32275, DOI 10.1007/s11042-018-6260-6
   Nair V., 2010, P 27 INT C MACHINE L, P807
   Nair Vinod, 2010, INT C INT C MACHINE, P807
   Oliver NM, 2000, IEEE T PATTERN ANAL, V22, P831, DOI 10.1109/34.868684
   OLSHAUSEN BA, 1993, J NEUROSCI, V13, P4700
   Qiu ZF, 2017, IEEE I CONF COMP VIS, P5534, DOI 10.1109/ICCV.2017.590
   Rensink RA, 2000, VIS COGN, V7, P17, DOI 10.1080/135062800394667
   Scovanner P., 2007, P 15 ACM INT C MULT, P357, DOI DOI 10.1145/1291233.1291311
   Sharma N, 2015, PEERJ COMPUT SCI, DOI 10.7717/peerj-cs.34
   Simonyan K, 2014, ADV NEUR IN, V27
   Song SJ, 2018, IEEE T IMAGE PROCESS, V27, P3459, DOI 10.1109/TIP.2018.2818328
   Soomro Khurram, 2012, UCF101 DATASET 101 H
   Wang F, 2017, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2017.683
   Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441
   Wang LM, 2016, LECT NOTES COMPUT SC, V9912, P20, DOI 10.1007/978-3-319-46484-8_2
   Wang LM, 2015, PROC CVPR IEEE, P4305, DOI 10.1109/CVPR.2015.7299059
   Wang SL, 2018, PROC CVPR IEEE, P2589, DOI 10.1109/CVPR.2018.00274
   Wang XL, 2016, PROC CVPR IEEE, P2658, DOI 10.1109/CVPR.2016.291
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634
   Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319
   Zhou YZ, 2018, PROC CVPR IEEE, P449, DOI 10.1109/CVPR.2018.00054
NR 48
TC 28
Z9 31
U1 2
U2 44
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2020
VL 36
IS 6
BP 1261
EP 1270
DI 10.1007/s00371-019-01733-3
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA LI0TO
UT WOS:000529199400014
DA 2024-07-18
ER

PT J
AU Song, YY
   Peng, GH
AF Song, Yangyang
   Peng, Guohua
TI Fast two-stage segmentation model for images with intensity
   inhomogeneity
SO VISUAL COMPUTER
LA English
DT Article
DE Image segmentation; Two-stage; Difference of convex algorithm;
   Primal-dual hybrid gradient method; Active contour models
ID ACTIVE CONTOURS DRIVEN; MINIMIZATION; ALGORITHMS; SNAKES
AB Based on the local correntropy-based K-means clustering active contour model, this paper proposes a fast two-stage segmentation method for intensity inhomogeneous images. Under our framework, the segmentation process is split into two stages. In the first stage, we preliminary segment the down-sampled images by the proposed relaxed anisotropic-isotropic local correntropy-based K-means clustering (AILCK) model, which can obtain a coarse segmentation result quickly. Subsequently, in the second stage, we further segment original images by an improved AILCK model, where we use the up-sampled coarse contour obtained by the first stage as the initialization. Following it, to obtain the global minima of energy functions fast, we incorporate a weighted difference of anisotropic and isotropic total variations into relaxed formulation of the two-stage active contour models. And then, we minimize them utilizing the difference-of-convex algorithm and the primal-dual hybrid gradient method. The experimental results on synthetic and real-world images demonstrate that the proposed method can achieve accurate segmentation results for intensity inhomogeneous images in a fast way.
C1 [Song, Yangyang; Peng, Guohua] Northwestern Polytech Univ, Dept Appl Math, Xian 710072, Shaanxi, Peoples R China.
C3 Northwestern Polytechnical University
RP Song, YY (corresponding author), Northwestern Polytech Univ, Dept Appl Math, Xian 710072, Shaanxi, Peoples R China.
EM yangys@mail.nwpu.edu.cn; penggh@nwpu.edu.cn
RI Song, Yangyi/HGU-4953-2022; Song, Yang/ABE-9615-2021; Song,
   Yangyi/JBJ-7119-2023
OI Song, Yangyi/0000-0002-3649-014X; Song, Yangyi/0000-0002-3649-014X
CR Akram F, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0174813
   Bresson X, 2007, J MATH IMAGING VIS, V28, P151, DOI 10.1007/s10851-007-0002-0
   Candès EJ, 2006, COMMUN PUR APPL MATH, V59, P1207, DOI 10.1002/cpa.20124
   Caselles V, 1997, INT J COMPUT VISION, V22, P61, DOI 10.1023/A:1007979827043
   Chan TF, 2001, IEEE T IMAGE PROCESS, V10, P266, DOI 10.1109/83.902291
   Chan TF, 2006, SIAM J APPL MATH, V66, P1632, DOI 10.1137/040615286
   COHEN LD, 1991, CVGIP-IMAG UNDERSTAN, V53, P211, DOI 10.1016/1049-9660(91)90028-N
   KASS M, 1987, INT J COMPUT VISION, V1, P321, DOI 10.1007/BF00133570
   Kichenassamy S., 1995, Proceedings of IEEE International Conference on Computer Vision, P4
   KIMMEL R, 1995, IEEE T PATTERN ANAL, V17, P635, DOI 10.1109/34.387512
   Li CM, 2008, IEEE T IMAGE PROCESS, V17, P1940, DOI 10.1109/TIP.2008.2002304
   Lou YF, 2015, SIAM J IMAGING SCI, V8, P1798, DOI 10.1137/14098435X
   Lou YF, 2015, J SCI COMPUT, V64, P178, DOI 10.1007/s10915-014-9930-1
   MALLADI R, 1995, IEEE T PATTERN ANAL, V17, P158, DOI 10.1109/34.368173
   MUMFORD D, 1989, COMMUN PUR APPL MATH, V42, P577, DOI 10.1002/cpa.3160420503
   OSHER S, 1988, J COMPUT PHYS, V79, P12, DOI 10.1016/0021-9991(88)90002-2
   Park F, 2016, IEEE IMAGE PROC, P4314, DOI 10.1109/ICIP.2016.7533174
   Soomro S, 2018, PLOS ONE, V13, DOI 10.1371/journal.pone.0191827
   Wang H, 2016, NEUROCOMPUTING, V205, P130, DOI 10.1016/j.neucom.2016.03.050
   Wang L, 2009, SIGNAL PROCESS, V89, P2435, DOI 10.1016/j.sigpro.2009.03.014
   Wang LF, 2014, PATTERN RECOGN, V47, P1917, DOI 10.1016/j.patcog.2013.11.014
   Wang LF, 2011, LECT NOTES COMPUT SC, V6493, P148
   Xu CY, 1998, IEEE T IMAGE PROCESS, V7, P359, DOI 10.1109/83.661186
   Xu N, 2007, COMPUT VIS IMAGE UND, V107, P210, DOI 10.1016/j.cviu.2006.11.004
   Zhang KH, 2016, IEEE T CYBERNETICS, V46, P546, DOI 10.1109/TCYB.2015.2409119
   Zhang KH, 2010, PATTERN RECOGN, V43, P1199, DOI 10.1016/j.patcog.2009.10.010
   Zhu GP, 2007, OPT ENG, V46, DOI 10.1117/1.2740762
   Zhu M, 2008, 0834 CAM, V34, P1
NR 28
TC 3
Z9 3
U1 0
U2 7
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2020
VL 36
IS 6
BP 1189
EP 1202
DI 10.1007/s00371-019-01728-0
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA LI0TO
UT WOS:000529199400008
DA 2024-07-18
ER

PT J
AU Zhang, XQ
   Wang, XX
   Gu, CH
AF Zhang, Xueqin
   Wang, Xiaoxiao
   Gu, Chunhua
TI Online multi-object tracking with pedestrian re-identification and
   occlusion processing
SO VISUAL COMPUTER
LA English
DT Article
DE Multi-object tracking; Tracking-by-detection; Pedestrian
   re-identification; Data association
ID MULTITARGET TRACKING; OBJECT TRACKING; PERFORMANCE
AB Tracking-by-detection is a common approach for online multi-object tracking problem. At present, the following challenges still exist in the multi-object tracking scenarios: (1) The result of object re-tracking after full occlusion is not ideal; (2) The predicted position of object is not accurate enough in the complicated video scenarios. Aiming at these two problems, this paper proposes a multi-object tracking framework called DROP (Deep Re-identification Occlusion Processing). The framework consists of object detection, fast pedestrian re-identification, and a confidence-based data association algorithm. A lightweight convolutional neural network that can solve the re-tracking problem is constructed by increasing and learning the affinity of appearance features of the same object in different frames. And this paper proposes to judge the occlusion of the object that can solve inaccurate position predicted by Kalman filter by using the data association result of the appearance features of the object, and to reduce the matching error by improving the data association formula. The experimental results on the multi-object tracking datasets MOT15 and MOT16 show that the proposed method can improve the precision while ensure the real-time tracking performance.
C1 [Zhang, Xueqin; Wang, Xiaoxiao; Gu, Chunhua] East China Univ Sci & Technol, Sch Informat Sci & Engn, Meilong Rd 130, Shanghai, Peoples R China.
C3 East China University of Science & Technology
RP Zhang, XQ (corresponding author), East China Univ Sci & Technol, Sch Informat Sci & Engn, Meilong Rd 130, Shanghai, Peoples R China.
EM zxq@ecust.edu.cn; y30170614@mail.ecust.edu.cn; chgu@ecust.edu.cn
OI Zhang, Xueqin/0000-0001-7020-1033
FU Shanghai Aerospace Science and Technology Innovation Fund [SAST2018086]
FX Funding was provided by: Shanghai Aerospace Science and Technology
   Innovation Fund (SAST2018086)
CR [Anonymous], 2018, P 2018 IEEE INT C CO
   [Anonymous], P 14 IEEE INT C ADV
   Bewley A, 2016, IEEE IMAGE PROC, P3464, DOI 10.1109/ICIP.2016.7533003
   Bochinski Erik, 2017, 2017 14th IEEE International Conference on Advanced Video and Signal-Based Surveillance (AVSS), DOI 10.1109/AVSS.2017.8078516
   Borah A., 2017, Poly-lactic-co-glycolic acid Nanoformulation of Small Molecule Antagonist GANT61 for Cancer Annihilation, P1, DOI DOI 10.1109/AVSS.2017.8078481
   Breitenstein MD, 2009, IEEE I CONF COMP VIS, P1515, DOI 10.1109/ICCV.2009.5459278
   Butt AA, 2013, PROC CVPR IEEE, P1846, DOI 10.1109/CVPR.2013.241
   Camgoz NC, 2016, INT C PATT RECOG, P49, DOI 10.1109/ICPR.2016.7899606
   Choi W, 2012, LECT NOTES COMPUT SC, V7575, P215, DOI 10.1007/978-3-642-33765-9_16
   Chu Q, 2017, IEEE I CONF COMP VIS, P4846, DOI 10.1109/ICCV.2017.518
   Fang K, 2018, IEEE WINT CONF APPL, P466, DOI 10.1109/WACV.2018.00057
   Fazl-Ersi E, 2019, VISUAL COMPUT, V35, P1447, DOI 10.1007/s00371-018-1510-1
   Guan H, 2018, VISUAL COMPUT, V34, P1701, DOI 10.1007/s00371-017-1445-y
   He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hermans Alexander, 2017, ARXIV170307737
   Izadinia H, 2012, LECT NOTES COMPUT SC, V7577, P100, DOI 10.1007/978-3-642-33783-3_8
   Leal-Taixe L., 2015, MOTChallenge 2015: Towards a Benchmark for Multi-Target Tracking
   Li ZY, 2015, VISUAL COMPUT, V31, P1633, DOI 10.1007/s00371-014-1044-0
   Mahmoudi N, 2019, MULTIMED TOOLS APPL, V78, P7077, DOI 10.1007/s11042-018-6467-6
   Milan A, 2017, AAAI CONF ARTIF INTE, P4225
   Milan A, 2014, IEEE T PATTERN ANAL, V36, P58, DOI 10.1109/TPAMI.2013.103
   Qin Z, 2012, PROC CVPR IEEE, P1972, DOI 10.1109/CVPR.2012.6247899
   Rasekhipour Y, 2017, IEEE T INTELL TRANSP, V18, P1255, DOI 10.1109/TITS.2016.2604240
   Rehman B, 2020, VISUAL COMPUT, V36, P633, DOI 10.1007/s00371-019-01649-y
   Ristani E, 2016, LECT NOTES COMPUT SC, V9914, P17, DOI 10.1007/978-3-319-48881-3_2
   Sadeghian A, 2017, IEEE I CONF COMP VIS, P300, DOI 10.1109/ICCV.2017.41
   Sanchez-Matilla R, 2016, LECT NOTES COMPUT SC, V9914, P84, DOI 10.1007/978-3-319-48881-3_7
   Seo JH, 2018, 2018 INTERNATIONAL CONFERENCE ON ELECTRONICS, INFORMATION, AND COMMUNICATION (ICEIC), P3
   Wang Y, 2019, VISUAL COMPUT, V35, P1641, DOI 10.1007/s00371-018-1563-1
   Wojke N, 2017, IEEE IMAGE PROC, P3645, DOI 10.1109/ICIP.2017.8296962
   Wu YX, 2015, VISUAL COMPUT, V31, P471, DOI 10.1007/s00371-014-0942-5
   Xiang Y, 2015, IEEE I CONF COMP VIS, P4705, DOI 10.1109/ICCV.2015.534
   Xu JR, 2019, IEEE I CONF COMP VIS, P3987, DOI 10.1109/ICCV.2019.00409
   Yoon K, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19030559
   Yu FW, 2016, LECT NOTES COMPUT SC, V9914, P36, DOI 10.1007/978-3-319-48881-3_3
   Zhang HY, 2018, VISUAL COMPUT, V34, P41, DOI 10.1007/s00371-016-1310-4
   Zhang L, 2014, IEEE T PATTERN ANAL, V36, P756, DOI 10.1109/TPAMI.2013.221
   Zhu J, 2018, LECT NOTES COMPUT SC, V11209, P379, DOI 10.1007/978-3-030-01228-1_23
NR 39
TC 22
Z9 22
U1 6
U2 60
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2021
VL 37
IS 5
BP 1089
EP 1099
DI 10.1007/s00371-020-01854-0
EA MAY 2020
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RV2QF
UT WOS:000536421400001
DA 2024-07-18
ER

PT J
AU Kim, S
   Winovich, N
   Chi, HG
   Lin, G
   Ramani, K
AF Kim, Sangpil
   Winovich, Nick
   Chi, Hyung-Gun
   Lin, Guang
   Ramani, Karthik
TI Latent transformations neural network for object view synthesis
SO VISUAL COMPUTER
LA English
DT Article
DE Object view synthesis; Latent transformation; Fully convolutional;
   Conditional generative model
ID GRAPHICS
AB We propose a fully convolutional conditional generative neural network, the latent transformation neural network, capable of rigid and non-rigid object view synthesis using a lightweight architecture suited for real-time applications and embedded systems. In contrast to existing object view synthesis methods which incorporate conditioning information via concatenation, we introduce a dedicated network component, the conditional transformation unit. This unit is designed to learn the latent space transformations corresponding to specified target views. In addition, a consistency loss term is defined to guide the network toward learning the desired latent space mappings, a task-divided decoder is constructed to refine the quality of generated views of objects, and an adaptive discriminator is introduced to improve the adversarial training process. The generalizability of the proposed methodology is demonstrated on a collection of three diverse tasks: multi-view synthesis on real hand depth images, view synthesis of real and synthetic faces, and the rotation of rigid objects. The proposed model is shown to be comparable with the state-of-the-art methods in structural similarity index measure and L-1 metrics while simultaneously achieving a 24% reduction in the compute time for inference of novel images.
C1 [Kim, Sangpil; Winovich, Nick; Chi, Hyung-Gun; Lin, Guang; Ramani, Karthik] Purdue Univ, W Lafayette, IN 47906 USA.
C3 Purdue University System; Purdue University
RP Ramani, K (corresponding author), Purdue Univ, W Lafayette, IN 47906 USA.
EM kim2030@purdue.edu; ramani@purdue.edu
RI Lin, Guang/ABB-2145-2021; Chi, Hyung-gun/HNJ-3269-2023
OI Lin, Guang/0000-0002-0976-1987; Chi, Hyung-gun/0000-0001-5454-3404; Kim,
   Sangpil/0000-0002-7349-0018
FU US National Science Foundation [DMS1555072, DMS-1736364, DMS-1821233,
   NRI-1637961, IIP-1632154]; NVIDIA Corporation
FX Karthik Ramani acknowledges the US National Science Foundation Awards
   NRI-1637961 and IIP-1632154. Guang Lin acknowledges the US National
   Science Foundation Awards DMS1555072, DMS-1736364 and DMS-1821233. Any
   opinions, findings, and conclusions or recommendations expressed in this
   material are those of the authors and do not necessarily reflect the
   views of the funding agency. We gratefully appreciate the support of
   NVIDIA Corporation with the donation of GPUs used for this research.
CR [Anonymous], 2017, ARXIV170201983
   Bao JM, 2017, IEEE I CONF COMP VIS, P2764, DOI 10.1109/ICCV.2017.299
   CHANG A, 2015, ARXIV151203012, V1, P8
   Chang A. X., 2015, ARXIV
   Chen Xi, 2016, Advances in Neural Information Processing Systems (NIPS), V29
   Choi C, 2017, IEEE I CONF COMP VIS, P3123, DOI 10.1109/ICCV.2017.337
   De Brabandere B, 2016, ADV NEUR IN, V29
   Dinerstein J, 2007, VISUAL COMPUT, V23, P25, DOI 10.1007/s00371-006-0085-4
   Dosovitskiy A, 2015, PROC CVPR IEEE, P1538, DOI 10.1109/CVPR.2015.7298761
   Fransens R, 2005, LECT NOTES COMPUT SC, V3723, P109
   Galama Y., 2018, Iterative gans for rotating visual ob-jects
   Gauthier J., 2014, Class Project for Stanford CS231N: Convolutional Neural Networks for Visual Recognition, P2
   Ge L, 2016, PROC CVPR IEEE, P3593, DOI 10.1109/CVPR.2016.391
   Goodfellow, 2017, ARXIV170100160
   Guan Haiying., 2006, 2006 C COMPUTER VISI, P154
   He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Ioffe S, 2015, P INT C MACH LEARN, V2015, P1
   Kim S, 2020, VISUAL COMPUT, V36, P1663, DOI 10.1007/s00371-019-01755-x
   Kingma D. P., 2014, arXiv
   Kingma D. P., 2013, ARXIV13126114
   Kulkarni TD, 2015, ADV NEUR IN, V28
   Ling SQ, 2018, ELECTR CONTACT, P286, DOI 10.1109/HOLM.2018.8611692
   Makhzani A., 2015, ARXIV
   Mirza M., 2014, ARXIV PREPRINT ARXIV, P2672, DOI 10.48550/arXiv.1411.1784
   Miyato T., 2018, Proceedings of the 6th International Conference on Learning Representations, P1
   Nirkin Y, 2018, IEEE INT CONF AUTOMA, P98, DOI 10.1109/FG.2018.00024
   Oberweger M, 2017, IEEE INT CONF COMP V, P585, DOI 10.1109/ICCVW.2017.75
   Park E, 2017, PROC CVPR IEEE, P702, DOI 10.1109/CVPR.2017.82
   Paszke A., 2016, ARXIV160602147
   Ramachandran P., 2017, Searching for activation functions
   Reed S, 2014, PR MACH LEARN RES, V32, P1431
   Rezende D.J., 2016, ADV NEURAL INFORM PR, P4996
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Salimans T, 2016, ADV NEUR IN, V29
   Sohn K, 2015, ADV NEUR IN, V28
   Sun SH, 2018, LECT NOTES COMPUT SC, V11207, P162, DOI 10.1007/978-3-030-01219-9_10
   Szegedy C, 2014, Arxiv, DOI [arXiv:1312.6199, DOI 10.1109/CVPR.2015.7298594]
   Tatarchenko M, 2016, LECT NOTES COMPUT SC, V9911, P322, DOI 10.1007/978-3-319-46478-7_20
   Tompson J, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2629500
   Varley J, 2017, IEEE INT C INT ROBOT, P2442, DOI 10.1109/IROS.2017.8206060
   WANG Q, 2018, VIS COMPUT
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Yan XC, 2016, LECT NOTES COMPUT SC, V9908, P776, DOI 10.1007/978-3-319-46493-0_47
   Yu JJ, 2016, LECT NOTES COMPUT SC, V9915, P3, DOI 10.1007/978-3-319-49409-8_1
   Zhang SY, 2019, VISUAL COMPUT, V35, P1157, DOI 10.1007/s00371-019-01691-w
   Zhang ZF, 2017, PROC CVPR IEEE, P4352, DOI 10.1109/CVPR.2017.463
   2009, 3D FAC MOD POS ILL I
NR 48
TC 3
Z9 3
U1 0
U2 8
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2020
VL 36
IS 8
BP 1663
EP 1677
DI 10.1007/s00371-019-01755-x
EA OCT 2019
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ML7DA
UT WOS:000491670000001
DA 2024-07-18
ER

PT J
AU Tariq, J
AF Tariq, Junaid
TI High-performance intra-mode accelerator for HEVC
SO VISUAL COMPUTER
LA English
DT Article
DE Intra-mode; Most probable mode; Rough mode decision; HEVC
ID MODE DECISION; RD-COST; PREDICTION; EFFICIENCY; MOTION; COMPRESSION;
   ALGORITHM
AB The high efficiency video coding (HEVC/H.265) is the latest coding standard that reduces the bit-rate requirement by 50% compared to its predecessor, i.e., H.264. However, the coding complexity of HEVC is increased a lot not only due to its increased number of intra-modes (35 in total), but also due to its brute force methodology. In this article, a fast intra-mode decision strategy is proposed to reduce the complexity of HEVC. The proposed work first incorporates the statistical inference that was formed using the spatial/temporal data into the HEVC's rough-mode-decision module. Then early termination is predicted by proposing a model that is based on the classical secretary problem (CSP). Thirdly, the uncertainty of the real world is introduced to the proposed CSP that outperforms the existing CSPs. Simulation results demonstrate that the proposed model is capable of achieving a minimum of 32.39% reduction in the total encoding time while the bit-rate (BD-BR) increment is limited to 1.33% on average.
C1 [Tariq, Junaid] HITEC Univ, Dept Comp Sci & Engn, Taxila, Pakistan.
C3 NITEC University
RP Tariq, J (corresponding author), HITEC Univ, Dept Comp Sci & Engn, Taxila, Pakistan.
EM jtariq2-c@my.cityu.edu.hk
CR [Anonymous], 2011, JCTVCF900
   Bjontegaard G., 2001, Document VCEG-M33
   Dolly DRJ, 2018, ARAB J SCI ENG, V43, P4249, DOI 10.1007/s13369-017-2839-x
   Hatchett J, 2018, VISUAL COMPUT, V34, P167, DOI 10.1007/s00371-016-1322-0
   Jamil A, 2019, ARAB J SCI ENG, V44, P9755, DOI 10.1007/s13369-019-03880-0
   Khan HUR, 2019, ARAB J SCI ENG, V44, P2151, DOI 10.1007/s13369-018-3367-z
   Koyama Y, 2019, VISUAL COMPUT, V35, P1131, DOI 10.1007/s00371-019-01693-8
   Li W, 2019, SIGNAL IMAGE VIDEO P, V13, P17, DOI 10.1007/s11760-018-1323-8
   Men QH, 2019, VISUAL COMPUT, V35, P973, DOI 10.1007/s00371-019-01690-x
   Mir J, 2018, ARAB J SCI ENG, V44, P1
   Ohm JR, 2012, IEEE T CIRC SYST VID, V22, P1669, DOI 10.1109/TCSVT.2012.2221192
   TAN HL, 2012, JCTVCH0166
   Tariq J, 2019, MULTIMED TOOLS APPL, V78, P16783, DOI 10.1007/s11042-018-7111-1
   Tariq J, 2018, J VIS COMMUN IMAGE R, V51, P1, DOI 10.1016/j.jvcir.2017.12.008
   Tariq J, 2017, J VIS COMMUN IMAGE R, V44, P198, DOI 10.1016/j.jvcir.2017.01.029
   Tariq J, 2015, IEEE SYS MAN CYBERN, P1776, DOI 10.1109/SMC.2015.311
   Tariq J, 2016, J VIS COMMUN IMAGE R, V35, P112, DOI 10.1016/j.jvcir.2015.11.013
   Tariq J, 2015, IEEE SYS MAN CYBERN, P1782, DOI 10.1109/SMC.2015.312
   Tsang SH, 2019, IEEE T MULTIMEDIA, V21, P269, DOI 10.1109/TMM.2018.2856078
   Ugur K, 2010, IEEE T CIRC SYST VID, V20, P1688, DOI 10.1109/TCSVT.2010.2092613
   Wali I, 2019, SIGNAL IMAGE VIDEO P, V13, P145, DOI 10.1007/s11760-018-1339-0
   Wang LL, 2013, IEEE T CIRC SYST VID, V23, P1686, DOI 10.1109/TCSVT.2013.2255398
   Yeh CH, 2014, IEEE T IND INFORM, V10, P594, DOI 10.1109/TII.2013.2273308
   Zhang H, 2014, IEEE T CIRC SYST VID, V24, P660, DOI 10.1109/TCSVT.2013.2290578
   Zhang T, 2017, IEEE T CIRC SYST VID, V27, P1714, DOI 10.1109/TCSVT.2016.2556518
   Zhao TS, 2012, IEEE T IMAGE PROCESS, V21, P2607, DOI 10.1109/TIP.2012.2186148
   Zhe Sheng, 2014, MultiMedia Modeling. 20th Anniversary International Conference, MMM 2014. Proceedings: LNCS 8325, P541, DOI 10.1007/978-3-319-04114-8_46
NR 27
TC 9
Z9 9
U1 0
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2020
VL 36
IS 8
BP 1603
EP 1617
DI 10.1007/s00371-019-01764-w
EA OCT 2019
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA NJ2RH
UT WOS:000544084900001
DA 2024-07-18
ER

PT J
AU Cheng, SL
   Lai, HC
   Wang, LJ
   Qin, JW
AF Cheng, Shuli
   Lai, Huicheng
   Wang, Liejun
   Qin, Jiwei
TI A novel deep hashing method for fast image retrieval
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 36th Computer Graphics International Conference (CGI)
CY JUN 17-20, 2019
CL Calgary, CANADA
SP Biometric Technologies Lab, Univ Calgary, VPR Off, Fac Sci, Comp Sci Dept, Alberta Ingenu, CGS
DE Image retrieval; Convolutional Neural Network; Fast Supervised Discrete
   Hashing; Transfer learning; Aggregate Deep Fast Supervised Discrete
   Hashing
ID FEATURES
AB In recent years, the deep hashing image retrieval algorithm has become a hot spot in current research. Although the deep hashing algorithm has achieved good results in image retrieval, how to further improve the accuracy of the image retrieval algorithm and reduce the computational complexity of the algorithm, the two basic requirements of the algorithm, need attention in image retrieval. The paper proposes a new Aggregate Deep Fast Supervised Discrete Hashing (ADFSDH) method for highly efficient image retrieval on large-scale datasets. Specifically, in order to improve the algorithm performance, the paper first proposes a new Aggregate Deep Convolutional Neural Network (ADCNN) mode based on VGG16, VGG19 and transfer learning for effective image feature extraction, which contains two different feature extractors in parallel. And then, the paper proposes a new feature fusion method. When our weighted proportion is consistent with the Mean Average Precision results of two different feature extractors, we can obtain the most accurate description of the image. Firstly, in order to save ADCNN required storage space and improve ADCNN image retrieval efficiency, the Fast Supervised Discrete Hashing algorithm after adjusting the parameters is introduced into the ADCNN model. In addition, ADFSDH unifies feature learning and hash coding into the same framework. The proposed method was experimented on three datasets (CIFAR10, MNIST and FD-XJ), and the result shows that it is superior to the current mainstream approaches in image retrieval.
C1 [Cheng, Shuli; Lai, Huicheng] Xinjiang Univ, Sch Informat Sci & Engn, 666 Victory Rd, Tianshan Dist 830046, Urumchi, Peoples R China.
   [Wang, Liejun] Xinjiang Univ, Sch Software Engn, 499 Northwest Rd, Saybagh Dist 830046, Urumchi, Peoples R China.
   [Qin, Jiwei] Shaanxi Normal Univ, Sch Educ, 199 Changan South Rd, Xian 710062, Shaanxi, Peoples R China.
C3 Xinjiang University; Xinjiang University; Shaanxi Normal University
RP Lai, HC (corresponding author), Xinjiang Univ, Sch Informat Sci & Engn, 666 Victory Rd, Tianshan Dist 830046, Urumchi, Peoples R China.
EM HuichengLai@qq.com
FU Chinese National Natural Science Foundation [61471311, 61771416];
   Creative Research Groups of Higher Education of Xinjiang Uygur
   Autonomous Region [XJEDU2017T002, NGII20170325]
FX This work is supported by the Chinese National Natural Science
   Foundation (Program Nos. 61471311, 61771416), the Creative Research
   Groups of Higher Education of Xinjiang Uygur Autonomous Region (Program
   No. XJEDU2017T002) and the Saier NetworkNext Generation Internet
   Technology Innovation Project (Program No. NGII20170325).
CR Alzu'bi A, 2017, NEUROCOMPUTING, V249, P95, DOI 10.1016/j.neucom.2017.03.072
   Alzu'bi A, 2016, 2016 INTERNATIONAL CONFERENCE ON IMAGE, VISION AND COMPUTING (ICIVC 2016), P41, DOI 10.1109/ICIVC.2016.7571271
   [Anonymous], 2009, NIPS
   Dalal N., 2005, PROC IEEE COMPUT SOC, V1, P886
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Gionis A, 1999, PROCEEDINGS OF THE TWENTY-FIFTH INTERNATIONAL CONFERENCE ON VERY LARGE DATA BASES, P518
   Gong YC, 2013, IEEE T PATTERN ANAL, V35, P2916, DOI 10.1109/TPAMI.2012.193
   Gui J, 2018, IEEE T PATTERN ANAL, V40, P490, DOI 10.1109/TPAMI.2017.2678475
   Gui J, 2018, IEEE T NEUR NET LEAR, V29, P608, DOI 10.1109/TNNLS.2016.2636870
   He Kaiming, 2016, EUR C COMP VIS ECCV, DOI [DOI 10.1109/CVPR.2016.90, DOI 10.1007/978-3-319-46493-0_38]
   Huang HK, 2016, P 2016 IEEE 5 GLOB C, P1, DOI [DOI 10.1109/GCCE.2016.7800375, 10.1109/GCCE.2016.7800375]
   Jiang QY, 2017, PROC CVPR IEEE, P3270, DOI 10.1109/CVPR.2017.348
   Joia P, 2012, VISUAL COMPUT, V28, P1027, DOI 10.1007/s00371-012-0730-z
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lai HJ, 2015, PROC CVPR IEEE, P3270, DOI 10.1109/CVPR.2015.7298947
   Lavoué G, 2012, VISUAL COMPUT, V28, P931, DOI 10.1007/s00371-012-0724-x
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Li HL, 2016, VISUAL COMPUT, V32, P1351, DOI 10.1007/s00371-016-1232-1
   Li JY, 2015, 2015 8TH INTERNATIONAL CONFERENCE ON BIOMEDICAL ENGINEERING AND INFORMATICS (BMEI), P649, DOI 10.1109/BMEI.2015.7401584
   Li K, 2017, IEEE T PATTERN ANAL, V39, P1825, DOI 10.1109/TPAMI.2016.2610969
   Lin Kevin, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P27, DOI 10.1109/CVPRW.2015.7301269
   Liu HM, 2016, PROC CVPR IEEE, P2064, DOI 10.1109/CVPR.2016.227
   Liu W, 2012, PROC CVPR IEEE, P2074, DOI 10.1109/CVPR.2012.6247912
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Norouzi M.E., 2011, ICML
   Oliva A, 2006, PROG BRAIN RES, V155, P23, DOI 10.1016/S0079-6123(06)55002-2
   Peng TQ, 2017, INT CONF ACOUST SPEE, P1742, DOI 10.1109/ICASSP.2017.7952455
   Shen FM, 2015, PROC CVPR IEEE, P37, DOI 10.1109/CVPR.2015.7298598
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Wang J, 2012, IEEE T PATTERN ANAL, V34, P2393, DOI 10.1109/TPAMI.2012.48
   Weiss Yair, 2009, Advances in Neural Information Processing Systems, P1753, DOI DOI 10.5555/2981780.2981999
   Xia RK, 2014, AAAI CONF ARTIF INTE, P2156
   Zheng YT, 2009, VISUAL COMPUT, V25, P13, DOI 10.1007/s00371-008-0294-0
   Zhong GQ, 2016, IEEE IJCNN, P2236, DOI 10.1109/IJCNN.2016.7727476
NR 34
TC 13
Z9 13
U1 0
U2 22
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2019
VL 35
IS 9
BP 1255
EP 1266
DI 10.1007/s00371-018-1583-x
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA IQ2JJ
UT WOS:000480574500006
DA 2024-07-18
ER

PT J
AU Kaddah, W
   Elbouz, M
   Ouerhani, Y
   Baltazart, V
   Desthieux, M
   Alfalou, A
AF Kaddah, Wissam
   Elbouz, Marwa
   Ouerhani, Yousri
   Baltazart, Vincent
   Desthieux, Marc
   Alfalou, Ayman
TI Optimized minimal path selection (OMPS) method for automatic and
   unsupervised crack segmentation within two-dimensional pavement images
SO VISUAL COMPUTER
LA English
DT Article
DE Road surface monitoring; Crack detection; Image processing;
   Segmentation; Performance assessment; Sustainability
ID ALGORITHM
AB In many countries, the common practice for monitoring road surface conditions consists in collecting pavement images at traffic speed by devoted imaging devices. Among the surface distresses, cracking can serve as a condition indicator of the pavement structure. Image processing techniques have been then developed to computerize the survey of cracking as the support of human visual control. Among the existing automatic crack detection methods, the minimal path selection (MPS) technique has shown a better performance compared to other methods on simulated and field pavement images (Amhaz et al. in IEEE Trans Intell Transp Syst 17:2718-2729, 2016; Amhaz, in: Detection automatique de fissures dans des images de chaussee par selection de chemins minimaux, 2015). As a counterpart, MPS suffers from a large computing time. Within this scope, the aim of this paper has been to improve the efficiency of the original MPS method and to present more efficient strategies for the selection of minimal paths. Among the five main steps of the original MPS version, the improvements address the first three steps that enable the segmentation of the crack skeleton and reduce the computing burden on the last two steps. The tests of the two improved MPS versions on some image samples illustrate the large reduction in false positive paths without reducing the overall performance of the segmentation technique. Moreover, the computing time is divided by a factor sixty roughly for the latest MPS version.
C1 [Kaddah, Wissam; Ouerhani, Yousri; Desthieux, Marc] Actris, BP 30143, F-29803 Brest 9, France.
   [Kaddah, Wissam; Elbouz, Marwa; Alfalou, Ayman] ISEN Brest, L bISEN, Vis Lab, CS 42807, F-29228 Brest 2, France.
   [Baltazart, Vincent] UBL Univ, IFSTTAR Nantes, CS 4, F-44340 Bouguenais, France.
C3 Universite Gustave-Eiffel
RP Kaddah, W (corresponding author), Actris, BP 30143, F-29803 Brest 9, France.; Kaddah, W (corresponding author), ISEN Brest, L bISEN, Vis Lab, CS 42807, F-29228 Brest 2, France.
EM w.kaddah@actris.com; ayman.al-falou@isen.fr
OI Kaddah, Wissam/0000-0001-9014-3053; Baltazart,
   Vincent/0000-0002-3261-5179
CR Amhaz R, 2016, IEEE T INTELL TRANSP, V17, P2718, DOI 10.1109/TITS.2015.2477675
   [Anonymous], 241MCD RILEM
   [Anonymous], IMPLEMENTATION ADV M
   [Anonymous], 14 C GRETSI JUAN LES
   [Anonymous], P 18 IEEE INT C IM P
   [Anonymous], 27 INT S AUT ROB CON
   [Anonymous], CONTRIBUTIONS IMPROV
   [Anonymous], IMAGE BASED POTHOLE
   [Anonymous], THESIS
   [Anonymous], 22 ITS WORLD C BORD
   [Anonymous], PPR001
   [Anonymous], UBICOMM
   [Anonymous], 4 INT S COMB SEARCH
   [Anonymous], 25 WORLD ROAD C SEOU
   [Anonymous], 2010, THESIS U WINDS WIND
   [Anonymous], EUSIPCO C AUT DET DE
   [Anonymous], EUSIPCO C SUIT SIGN
   [Anonymous], P IEEE INT JOINT C N
   [Anonymous], P 8 RILEM INT C MECH
   [Anonymous], IEEE INT C IM PROC I
   [Anonymous], EC156 TRANSP RES
   [Anonymous], EV PERF AUT PAV CRAC
   [Anonymous], P 25 EUR SIGN PROC C
   [Anonymous], IMAGE DATABASE
   Chambon S, 2011, INT J GEOPHYS, V2011, DOI 10.1155/2011/989354
   Dohm S, 2020, J CHEM THEORY COMPUT, V16, P2002, DOI 10.1021/acs.jctc.9b01266
   Gavilán M, 2011, SENSORS-BASEL, V11, P9628, DOI 10.3390/s111009628
   Kaddah W, 2016, OPT COMMUN, V371, P117, DOI 10.1016/j.optcom.2016.03.065
   Li QQ, 2011, EURASIP J ADV SIG PR, DOI 10.1155/2011/649675
   Oliveira H, 2013, IEEE T INTELL TRANSP, V14, P155, DOI 10.1109/TITS.2012.2208630
   OTSU N, 1979, IEEE T SYST MAN CYB, V9, P62, DOI 10.1109/TSMC.1979.4310076
   Saar T, 2010, P 12 BIENN BALT EL C
   Sampat MP, 2009, IEEE T IMAGE PROCESS, V18, P2385, DOI 10.1109/TIP.2009.2025923
   Shi Y, 2016, IEEE T INTELL TRANSP, V17, P3434, DOI 10.1109/TITS.2016.2552248
   Tang JS, 2013, IEEE SYS MAN CYBERN, P3026, DOI 10.1109/SMC.2013.516
   Tsai YC, 2010, J TRANSP ENG, V136, P11, DOI 10.1061/(ASCE)TE.1943-5436.0000051
   Vavrik W.R., 2013, Pcr evaluation-considering transition from manual to semi-automated pavement distress collection and analysis
   Veit T, 2008, PROCEEDINGS OF THE 11TH INTERNATIONAL IEEE CONFERENCE ON INTELLIGENT TRANSPORTATION SYSTEMS, P174, DOI 10.1109/ITSC.2008.4732564
   Wright D, 2014, BMJ OPEN, V4, DOI 10.1136/bmjopen-2013-004556
   Yamaguchi T, 2008, C IND ELECT APPL, P1875, DOI 10.1109/ICIEA.2008.4582845
   Zhang L, 2016, IEEE IMAGE PROC, P3708, DOI 10.1109/ICIP.2016.7533052
   Zou Q, 2012, PATTERN RECOGN LETT, V33, P227, DOI 10.1016/j.patrec.2011.11.004
NR 42
TC 24
Z9 26
U1 1
U2 29
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2019
VL 35
IS 9
BP 1293
EP 1309
DI 10.1007/s00371-018-1515-9
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IQ2JJ
UT WOS:000480574500009
DA 2024-07-18
ER

PT J
AU Han, S
   Yoon, SH
   Kim, MS
   Elber, G
AF Han, Sangjun
   Yoon, Seung-Hyun
   Kim, Myung-Soo
   Elber, Gershon
TI Minkowski sum computation for planar freeform geometric models using
   G<SUP>1</SUP>-biarc approximation and interior disk culling
SO VISUAL COMPUTER
LA English
DT Article
DE Minkowski sum; B-spline curves; Biarc splines; Interior disks; Trimming
ID CURVES; SPACE; GPU
AB We present an efficient algorithm for computing the Minkowski sum of two planar geometric models bounded by B-spline curves. The boundary curves are first approximated by G1-biarc splines within a given error bound E>0. A superset of Minkowski sum boundary is then generated using the biarc approximations. For non-convex models, the superset contains redundant arcs. An efficient and robust elimination of the redundancies is the main challenge of Minkowski sum computation. For this purpose, we use the Minkowski sum of interior disks of the two input models, which are again disks in the Minkowski sum interior. The majority of redundant arcs are eliminated by testing each against a small number of interior disks selected for efficiency. From the planar arrangement of remaining arcs, we construct the Minkowski sum boundary in a correct topology. We demonstrate a real-time performance and the stability of circle-based Minkowski sum computation using a large set of test data.
C1 [Han, Sangjun] Seoul Natl Univ, Dept Mech & Aerosp Engn, Seoul 08826, South Korea.
   [Yoon, Seung-Hyun] Dongguk Univ, Dept Multimedia Engn, Seoul 04620, South Korea.
   [Kim, Myung-Soo] Seoul Natl Univ, Dept Comp Sci & Engn, Seoul 08826, South Korea.
   [Elber, Gershon] Technion Israel Inst Technol, Dept Comp Sci, IL-32000 Haifa, Israel.
C3 Seoul National University (SNU); Dongguk University; Seoul National
   University (SNU); Technion Israel Institute of Technology
RP Kim, MS (corresponding author), Seoul Natl Univ, Dept Comp Sci & Engn, Seoul 08826, South Korea.
EM mskim@snu.ac.kr; gershon@cs.technion.ac.il
FU MSIT/IITP of Korea [2017-0-00367]; National Research Foundation of Korea
   [NRF-2018R1D1A1B07048036, NRF-2019R1A2C1003490]; ISRAEL SCIENCE
   FOUNDATION [597/18]
FX This study was funded by the MSIT/IITP of Korea (No. 2017-0-00367), by
   the National Research Foundation of Korea (No. NRF-2018R1D1A1B07048036
   and NRF-2019R1A2C1003490), and by the ISRAEL SCIENCE FOUNDATION (Grant
   No. 597/18).
CR Aichholzer O, 2007, LECT NOTES COMPUT SC, V4619, P374
   Aichholzer O, 2009, COMPUT AIDED DESIGN, V41, P339, DOI 10.1016/j.cad.2008.08.008
   Aichholzer O, 2010, COMP GEOM-THEOR APPL, V43, P688, DOI 10.1016/j.comgeo.2010.04.004
   [Anonymous], 2018, Real-Time Rendering, Fourth Edition, DOI DOI 10.1201/B22086
   BAJAJ C, 1990, INT J ROBOT RES, V9, P92, DOI 10.1177/027836499000900104
   BAJAJ CL, 1989, ALGORITHMICA, V4, P157, DOI 10.1007/BF01553884
   Campen M, 2010, COMPUT GRAPH FORUM, V29, P1613, DOI 10.1111/j.1467-8659.2010.01770.x
   Elber G, 2001, PROC ACM S SOLID MOD, P4
   Elberl G, 2008, LECT NOTES COMPUT SC, V4975, P191
   GHOSH PK, 1988, COMPUT VISION GRAPH, V44, P239, DOI 10.1016/0734-189X(88)90123-5
   GHOSH PK, 1993, COMPUT GRAPH, V17, P357, DOI 10.1016/0097-8493(93)90023-3
   Guibas L., 1983, 24th Annual Symposium on Foundations of Computer Science, P100, DOI 10.1109/SFCS.1983.1
   Hachenberger P, 2009, ALGORITHMICA, V55, P329, DOI 10.1007/s00453-008-9219-6
   KAUL A, 1995, INT J COMPUT GEOM AP, V5, P413, DOI 10.1142/S0218195995000258
   Kim YJ, 2012, COMPUT AIDED GEOM D, V29, P555, DOI 10.1016/j.cagd.2012.03.014
   Kim YJ, 2010, VISUAL COMPUT, V26, P1007, DOI 10.1007/s00371-010-0477-3
   KOHLER M, 1995, INT J ROBOT RES, V14, P590, DOI 10.1177/027836499501400605
   Kyung MH, 2015, COMPUT AIDED DESIGN, V67-68, P48, DOI 10.1016/j.cad.2015.04.012
   Latombe J.-C., 2012, ROBOT MOTION PLANNIN
   Lee IK, 1998, GRAPH MODEL IM PROC, V60, P136, DOI 10.1006/gmip.1998.0464
   Lee J, 2016, COMPUT AIDED GEOM D, V43, P131, DOI 10.1016/j.cagd.2016.02.008
   Lee J, 2015, VISUAL COMPUT, V31, P809, DOI 10.1007/s00371-015-1093-z
   Lee J, 2015, COMPUT AIDED DESIGN, V58, P248, DOI 10.1016/j.cad.2014.08.031
   Leung Y-S, 2013, COMPUT AIDED DES APP, V10, P475
   Li W, 2014, COMPUT AIDED DESIGN, V46, P90, DOI 10.1016/j.cad.2013.08.021
   Li W, 2011, COMPUT AIDED DESIGN, V43, P1270, DOI 10.1016/j.cad.2011.06.022
   Lien JM, 2008, COMPUT AIDED GEOM D, V25, P652, DOI 10.1016/j.cagd.2008.06.006
   Lien JM, 2010, SPRINGER TRAC ADV RO, V57, P401
   LOZANOPEREZ T, 1979, COMMUN ACM, V22, P560, DOI 10.1145/359156.359164
   LOZANOPEREZ T, 1983, IEEE T COMPUT, V32, P108, DOI 10.1109/TC.1983.1676196
   MEEK DS, 1995, J COMPUT APPL MATH, V59, P221, DOI 10.1016/0377-0427(94)00029-Z
   Meek DS, 1999, J COMPUT APPL MATH, V107, P21, DOI 10.1016/S0377-0427(99)00072-2
   Mizrahi J, 2017, GRAPH MODELS, V91, P30, DOI 10.1016/j.gmod.2017.02.003
   Mühlthaler H, 2003, GRAPH MODELS, V65, P369, DOI 10.1016/j.gmod.2003.07.003
   Pasko A, 2003, COMPUT MATH APPL, V45, P1479, DOI 10.1016/S0898-1221(03)00131-7
   Peternell M, 2007, GRAPH MODELS, V69, P180, DOI 10.1016/j.gmod.2007.01.001
   Sederberg T. W., 1989, Computer-Aided Geometric Design, V6, P205, DOI 10.1016/0167-8396(89)90024-1
   Seong JK, 2002, GEOMETRIC MODELING AND PROCESSING: THEORY AND APPLICATIONS, PROCEEDINGS, P33, DOI 10.1109/GMAP.2002.1027494
   Sír Z, 2006, COMPUT AIDED DESIGN, V38, P608, DOI 10.1016/j.cad.2006.02.003
   Varadhan G, 2006, GRAPH MODELS, V68, P343, DOI 10.1016/j.gmod.2005.11.003
NR 40
TC 2
Z9 2
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2019
VL 35
IS 6-8
SI SI
BP 921
EP 933
DI 10.1007/s00371-019-01687-6
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IC1IH
UT WOS:000470712200013
OA Bronze
DA 2024-07-18
ER

PT J
AU Sánchez, JES
   Sá, AME
   de Figueiredo, LH
AF Soto Sanchez, Jose Ezequiel
   Medeiros e Sa, Asla
   de Figueiredo, Luiz Henrique
TI Acquiring periodic tilings of regular polygons from images
SO VISUAL COMPUTER
LA English
DT Article
DE Tilings; Tessellations; Geometrical models
AB We describe how we have acquired geometrical models of many periodic tilings of regular polygons from two large collections of images. These models are based on a simplification of the representation recently proposed by us that uses complex numbers. We also describe an algorithm for deciding when two representations give the same tiling, which was used to identify coincidences in these collections.
C1 [de Figueiredo, Luiz Henrique] IMPA, Visgraf, Vis & Graph Lab, Rio De Janeiro, Brazil.
   [Soto Sanchez, Jose Ezequiel] IMPA, Visgraf Lab, Rio De Janeiro, Brazil.
   [Medeiros e Sa, Asla] FGV EMAp, Rio De Janeiro, Brazil.
C3 Instituto Nacional de Matematica Pura e Aplicada (IMPA); Instituto
   Nacional de Matematica Pura e Aplicada (IMPA)
RP de Figueiredo, LH (corresponding author), IMPA, Visgraf, Vis & Graph Lab, Rio De Janeiro, Brazil.
EM lhf@impa.br
RI ; Medeiros e Sa, Asla/P-5129-2014
OI de Figueiredo, Luiz Henrique/0000-0001-5683-693X; Soto Sanchez, Jose
   Ezequiel/0000-0002-2385-4012; Medeiros e Sa, Asla/0000-0002-3705-9095
FU FINEP; CNPq; FAPERJ
FX We thank Sa and Sa and Galebach for making their collections of tilings
   freely available at their websites. The first author is partially
   supported by a CNPq doctoral scholarship. The third author is partially
   supported by a CNPq research grant. This research was done in the
   Visgraf Computer Graphics laboratory at IMPA. Visgraf is supported by
   the funding agencies FINEP, CNPq, and FAPERJ, and also by gifts from IBM
   Brasil, Microsoft, NVIDIA, and other companies.
CR [Anonymous], EUCL TIL CONV REG PO
   [Anonymous], 200923 UUDM UPPS U
   [Anonymous], SYNTH LECT COMPUT GR
   [Anonymous], 2001, Robotica, DOI DOI 10.1017/S0263574700223217
   [Anonymous], MALHAS ARQUIMEDIANAS
   [Anonymous], 1952, Geometry and the Imagination
   [Anonymous], FNT CGCV
   [Anonymous], PERIODIC TILINGS REG
   BRADLEY GH, 1971, MATH COMPUT, V25, P897, DOI 10.2307/2004354
   CHAVEY D, 1989, COMPUT MATH APPL, V17, P147, DOI 10.1016/0898-1221(89)90156-9
   Conway J.H., 2008, The Symmetries of Things
   Galebach B. L, N UNIFORM TILINGS
   Grunbaum B., 1977, Math. Mag., V50, P227, DOI DOI 10.1080/0025570X.1977.11976655
   Grunbaum B., 1987, Tilings and Patterns
   Kaplan Craig S., 2005, GI 05 P GRAPHICS INT, P177
   Kawarabayashi K, 2008, ACM S THEORY COMPUT, P471
   Liu SY, 2015, IEEE I CONF COMP VIS, P181, DOI 10.1109/ICCV.2015.29
   McKay BD, 2014, J SYMB COMPUT, V60, P94, DOI 10.1016/j.jsc.2013.09.003
   Sá AME, 2018, SIBGRAPI, P17, DOI 10.1109/SIBGRAPI.2018.00009
   Sá AME, 2014, VISUAL COMPUT, V30, P1321, DOI 10.1007/s00371-013-0883-4
   Nasri A, 2017, ACM J COMPUT CULT HE, V10, DOI 10.1145/3064419
NR 21
TC 1
Z9 1
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2019
VL 35
IS 6-8
SI SI
BP 899
EP 907
DI 10.1007/s00371-019-01665-y
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IC1IH
UT WOS:000470712200011
OA Bronze
DA 2024-07-18
ER

PT J
AU Tang, YL
   Han, XG
   Li, Y
   Ma, LQ
   Tong, RF
AF Tang, Yanlong
   Han, Xiaoguang
   Li, Yue
   Ma, Liqian
   Tong, Ruofeng
TI Expressive facial style transfer for personalized memes mimic
SO VISUAL COMPUTER
LA English
DT Article
DE Expressive style transfer; Meme generation; Curve-based 3D face
   reconstruction; Neural-style alpha-blending
ID MORPHABLE MODEL; 3D
AB Meme, usually represented by an image of exaggerated expressive face captioned with short text, are increasingly produced and used online to express people's strong or subtle emotions. Meanwhile, meme mimic apps continuously appear, such as the meme filming feature in WeChat App that allow users to imitate meme expressions. Motivated by such scenarios, we focus on transferring exaggerated or unique expressions which is rarely noticed by previous works. We present a techniqueexpressive style transferwhich allows users to faithfully imitate popular memes' unique expression styles both geometrically and textually. To conduct distortion-free transferring of exaggerated geometry, we propose a novel accurate feature curve-based face reconstruction algorithm for 3D-aware image warping. Furthermore, we propose an identity preserving blending model, based on a deep neural network, to enhance facial expressive textural details. We demonstrate the effectiveness of our method on a collection of Internet memes.
C1 [Tang, Yanlong] Zhejiang Univ, Hangzhou, Zhejiang, Peoples R China.
   [Tong, Ruofeng] Zhejiang Univ, Dept Comp Sci, Hangzhou, Zhejiang, Peoples R China.
   [Han, Xiaoguang] Chinese Univ Hong Kong Shenzhen, Shenzhen Res Inst Big Data, Shenzhen, Peoples R China.
   [Li, Yue] Univ Penn, Philadelphia, PA 19104 USA.
   [Ma, Liqian] Beijing Kuaishou Technol Ltd, Beijing, Peoples R China.
C3 Zhejiang University; Zhejiang University; Shenzhen Research Institute of
   Big Data; The Chinese University of Hong Kong, Shenzhen; University of
   Pennsylvania
RP Han, XG (corresponding author), Chinese Univ Hong Kong Shenzhen, Shenzhen Res Inst Big Data, Shenzhen, Peoples R China.
EM yanlongtang@gmail.com; hanxiaoguang@cuhk.edu.cn; yueli.cg@gmail.com;
   maliqian@kuaishou.com; trf@zju.edu.cn
FU National Natural Science Foundation of China [61832016]; Science and
   Technology Project of Zhejiang province [2018C01080, 2017ZT07X152];
   Shenzhen Fundamental Research Fund [KQTD2015033114415450,
   ZDSYS201707251409055]
FX We thank the anonymous reviewers for the insightful and constructive
   comments, Matt Boyd-Surka for proofreading this manuscript, Jinghui
   Zhou, Keli Cheng and Xiaodong Gu for valuable discussions, and Yuan Yao
   for providing their deep image analogy result [29]. This paper was
   supported by the National Natural Science Foundation of China (No.
   61832016) and the Science and Technology Project of Zhejiang province
   (No. 2018C01080). This work was also funded in part by the
   PearlRiverTalent Recruitment Program Innovative and Entrepreneurial
   Teams in 2017 under Grant No. 2017ZT07X152 and the Shenzhen Fundamental
   Research Fund under Grants No. KQTD2015033114415450 and No.
   ZDSYS201707251409055.
CR [Anonymous], 2017, ABS1702 CORR
   [Anonymous], 2016, P IEEE C COMPUTER VI
   [Anonymous], 2017, ARXIV170501088
   [Anonymous], ARXIV180511714
   [Anonymous], 2016, AS C COMP VIS WORKSH
   [Anonymous], 2004, P 2004 EUR ACM SIGGR
   [Anonymous], 2016, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2016.262
   Averbuch-Elor H, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130818
   BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791
   Blanz V, 1999, COMP GRAPH, P187, DOI 10.1145/311535.311556
   Booth James, 2017, P IEEE C COMPUTERVIS
   Bouaziz S, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461976
   Cao C, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601204
   Cao C, 2014, IEEE T VIS COMPUT GR, V20, P413, DOI 10.1109/TVCG.2013.249
   Cao C, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2462012
   Choi Y., 2017, STARGAN UNIFIED GENE
   Ding L, 2014, VISUAL COMPUT, V30, P189, DOI 10.1007/s00371-013-0795-3
   Fiser J, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073660
   Frigo O, 2019, VISUAL COMPUT, V35, P429, DOI 10.1007/s00371-018-1474-1
   Garrido P, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2890493
   Garrido P, 2014, PROC CVPR IEEE, P4217, DOI 10.1109/CVPR.2014.537
   Geng JH, 2018, SIGGRAPH ASIA'18: SIGGRAPH ASIA 2018 TECHNICAL PAPERS, DOI 10.1145/3272127.3275043
   Jackson AS, 2017, IEEE I CONF COMP VIS, P1031, DOI 10.1109/ICCV.2017.117
   Jing Y., 2017, ABS170504058 CORR
   Johnson Justin, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9906, P694, DOI 10.1007/978-3-319-46475-6_43
   KASS M, 1987, INT J COMPUT VISION, V1, P321, DOI 10.1007/BF00133570
   Kemelmacher-Shlizerman I, 2013, IEEE I CONF COMP VIS, P3256, DOI 10.1109/ICCV.2013.404
   Kingma D.P., 2018, PROC NEURIPS
   Li C, 2016, LECT NOTES COMPUT SC, V9907, P702, DOI 10.1007/978-3-319-46487-9_43
   Li H, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2462019
   Li Y., 2018, P 16 ACM SIGGR INT, DOI DOI 10.1145/3284398.3284408
   Luan F., 2017, ARXIV170307511 CORR
   Ma MY, 2016, VISUAL COMPUT, V32, P1223, DOI 10.1007/s00371-015-1158-z
   Parkhi OM, 2015, DEEP FACE RECOGNITIO
   Pumarola A., 2018, ARXIV180709251
   Qiao FC, 2018, COMPUT ANIMAT VIRT W, V29, DOI 10.1002/cav.1819
   Reinhard E, 2001, IEEE COMPUT GRAPH, V21, P34, DOI 10.1109/38.946629
   Romdhani S, 2005, PROC CVPR IEEE, P986
   Snape P, 2015, PROC CVPR IEEE, P91, DOI 10.1109/CVPR.2015.7298604
   Snape P, 2014, PROC CVPR IEEE, P1059, DOI 10.1109/CVPR.2014.139
   Sumner RW, 2004, ACM T GRAPHIC, V23, P399, DOI 10.1145/1015706.1015736
   Wang L, 2020, VISUAL COMPUT, V36, P317, DOI 10.1007/s00371-018-1609-4
   Yang F, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964955
   Zhang Y., 2018, ARXIV180711079
   Zhu XY, 2016, PROC CVPR IEEE, P146, DOI 10.1109/CVPR.2016.23
NR 45
TC 2
Z9 3
U1 1
U2 12
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2019
VL 35
IS 6-8
SI SI
BP 783
EP 795
DI 10.1007/s00371-019-01695-6
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IC1IH
UT WOS:000470712200002
OA Bronze
DA 2024-07-18
ER

PT J
AU Zhang, L
   Yan, QG
   Zhu, Y
   Zhang, XL
   Xiao, CX
AF Zhang, Ling
   Yan, Qingan
   Zhu, Yao
   Zhang, Xiaolong
   Xiao, Chunxia
TI Effective shadow removal via multi-scale image decomposition
SO VISUAL COMPUTER
LA English
DT Article
DE Shadow removal; Image processing; Complex shadow; Image decomposition
AB Shadow removal is a fundamental and challenging problem in image processing field. Current approaches can only process shadows with simple scenes. For complex texture and illumination, the performance is less impressive. In this paper, we propose a novel shadow removal algorithm based on multi-scale image decomposition, which can recover the illumination for complex shadows with inconsistent illumination and different surface materials. Independent of shadow detection, our algorithm only requires a rough boundary distinguishing shadow regions from non-shadow regions. It first performs a multi-scale decomposition for the input image based on an illumination-sensitive smoothing process and then removes shadows in the basic layer using a local-to-global optimization strategy, which fuses all local shadow-free results in a global manner. Finally, we recover the texture details for the shadow-free basic layer and obtain the final shadow-free image. We validate the performance of the proposed method under various lighting and texture conditions and show consistent illumination between shadow and surrounding regions in the shadow removal results.
C1 [Zhang, Ling; Zhang, Xiaolong] Wuhan Univ Sci & Technol, Sch Comp Sci & Technol, Hubei Key Lab Intelligent Informat Proc & Realtim, Wuhan 430081, Hubei, Peoples R China.
   [Yan, Qingan] JD Com Amer Technol Corp, Mountain View, CA 94043 USA.
   [Zhu, Yao; Xiao, Chunxia] Wuhan Univ, Sch Comp Sci, Wuhan 430072, Hubei, Peoples R China.
C3 Wuhan University of Science & Technology; Wuhan University
RP Xiao, CX (corresponding author), Wuhan Univ, Sch Comp Sci, Wuhan 430072, Hubei, Peoples R China.
EM lingzhang@whu.edu.cn; qingan.yan@jd.com; juryoo@qq.com;
   xiaolong.zhang@wust.edu.cn; cxxiao@whu.edu.cn
RI ZHANG, XIAOLONG/IZQ-4553-2023
FU National Key Research and Development Program of China
   [2017YF-B1002600]; NSFC [61672390]; Wuhan Science and Technology Plan
   Project [2017010201010109]; Key Technological Innovation Projects of
   Hubei Province [2018AAA062]; China Postdoctoral Science Found [070307]
FX This work was partly supported by The National Key Research and
   Development Program of China (2017YF-B1002600), the NSFC (No. 61672390),
   Wuhan Science and Technology Plan Project (No. 2017010201010109), Key
   Technological Innovation Projects of Hubei Province (2018AAA062), and
   China Postdoctoral Science Found (No. 070307).
CR Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   [Anonymous], 2018, CVPR
   [Anonymous], ACM T GRAPH
   [Anonymous], IEEE T PAMI
   [Anonymous], SIGNAL INFORM PROCES
   [Anonymous], 2016, IEEE T PATTERN ANAL, DOI DOI 10.1109/TPAMI.2015.2462355
   [Anonymous], 2016, LARGE SCALE TRAINING
   [Anonymous], 2018, CVPR
   Arbel E, 2011, IEEE T PATTERN ANAL, V33, P1202, DOI 10.1109/TPAMI.2010.157
   Chuang YY, 2003, ACM T GRAPHIC, V22, P494, DOI 10.1145/882262.882298
   Clarenz U, 2004, VISUAL COMPUT, V20, P329, DOI 10.1007/s00371-004-0245-3
   Finlayson GD, 2006, IEEE T PATTERN ANAL, V28, P59, DOI 10.1109/TPAMI.2006.18
   Finlayson GD, 2004, LECT NOTES COMPUT SC, V3023, P582
   Finlayson GD, 2002, LECT NOTES COMPUT SC, V2353, P823
   Gryka M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2732407
   Guo RQ, 2011, PROC CVPR IEEE
   Jun Zhu, 2010, 2010 15th Asia and South Pacific Design Automation Conference (ASP-DAC 2010), P223, DOI 10.1109/ASPDAC.2010.5419892
   Levin A, 2008, IEEE T PATTERN ANAL, V30, P228, DOI 10.1109/TPAMI.2007.1177
   Li HF, 2014, IEEE T GEOSCI REMOTE, V52, P106, DOI 10.1109/TGRS.2012.2236562
   Liu F, 2008, LECT NOTES COMPUT SC, V5305, P437
   Liu YL, 2012, IEEE T VIS COMPUT GR, V18, P573, DOI 10.1109/TVCG.2012.53
   Mohan A, 2007, IEEE COMPUT GRAPH, V27, P23, DOI 10.1109/MCG.2007.30
   Pajak D, 2010, VISUAL COMPUT, V26, P739, DOI 10.1007/s00371-010-0485-3
   Pérez P, 2003, ACM T GRAPHIC, V22, P313, DOI 10.1145/882262.882269
   Qu LQ, 2017, PROC CVPR IEEE, P2308, DOI 10.1109/CVPR.2017.248
   Reinhard E, 2001, IEEE COMPUT GRAPH, V21, P34, DOI 10.1109/38.946629
   Shor Y, 2008, COMPUT GRAPH FORUM, V27, P577, DOI 10.1111/j.1467-8659.2008.01155.x
   Subr K, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618493
   Wu TP, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1243980.1243982
   Wu TP, 2005, IEEE I CONF COMP VIS, P480
   Xiao CX, 2013, COMPUT GRAPH FORUM, V32, P421, DOI 10.1111/cgf.12250
   Xiao CX, 2013, COMPUT GRAPH FORUM, V32, P207, DOI 10.1111/cgf.12198
   Xiao Y, 2014, PROC CVPR IEEE, P3011, DOI 10.1109/CVPR.2014.385
   Yang QX, 2012, IEEE T IMAGE PROCESS, V21, P4361, DOI 10.1109/TIP.2012.2208976
   Zhang L, 2017, IEEE T IMAGE PROCESS, V26, P4114, DOI 10.1109/TIP.2017.2712283
   Zhang L, 2015, IEEE T IMAGE PROCESS, V24, P4623, DOI 10.1109/TIP.2015.2465159
NR 36
TC 14
Z9 17
U1 1
U2 40
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2019
VL 35
IS 6-8
SI SI
BP 1091
EP 1104
DI 10.1007/s00371-019-01685-8
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IC1IH
UT WOS:000470712200026
OA Bronze
DA 2024-07-18
ER

PT J
AU Zhang, SY
   Han, ZZ
   Lai, YK
   Zwicker, M
   Zhang, H
AF Zhang, Suiyun
   Han, Zhizhong
   Lai, Yu-Kun
   Zwicker, Matthias
   Zhang, Hui
TI Stylistic scene enhancement GAN: mixed stylistic enhancement generation
   for 3D indoor scenes
SO VISUAL COMPUTER
LA English
DT Article
DE Scene enhancement; 3D indoor scenes; Interior design; Conditional
   generative adversarial nets; Gumbel-Softmax; Multi-one-hot
AB In this paper, we present stylistic scene enhancement GAN, SSE-GAN, a conditional Wasserstein GAN-based approach to automatic generation of mixed stylistic enhancements for 3D indoor scenes. An enhancement indicates factors that can influence the style of an indoor scene such as furniture colors and occurrence of small objects. To facilitate network training, we propose a novel enhancement feature encoding method, which represents an enhancement by a multi-one-hot vector, and effectively accommodates different enhancement factors. A Gumbel-Softmax module is introduced in the generator network to enable the generation of high fidelity enhancement features that can better confuse the discriminator. Experiments show that our approach is superior to the other baseline methods and successfully models the relationship between the style distribution and scene enhancements. Thus, although only trained with a dataset of room images in single styles, the trained generator can generate mixed stylistic enhancements by specifying multiple styles as the condition. Our approach is the first to apply a Gumbel-Softmax module in conditional Wasserstein GANs, as well as the first to explore the application of GAN-based models in the scene enhancement field.
C1 [Zhang, Suiyun; Zhang, Hui] Tsinghua Univ, Sch Software, Beijing, Peoples R China.
   [Zhang, Suiyun; Zhang, Hui] Beijing Natl Res Ctr Informat Sci & Technol BNRis, Beijing, Peoples R China.
   [Han, Zhizhong; Zwicker, Matthias] Univ Maryland, Dept Comp Sci, College Pk, MD 20742 USA.
   [Lai, Yu-Kun] Cardiff Univ, Sch Comp Sci & Informat, Cardiff, S Glam, Wales.
C3 Tsinghua University; University System of Maryland; University of
   Maryland College Park; Cardiff University
RP Zhang, H (corresponding author), Tsinghua Univ, Sch Software, Beijing, Peoples R China.; Zhang, H (corresponding author), Beijing Natl Res Ctr Informat Sci & Technol BNRis, Beijing, Peoples R China.
EM zhangsuiyun13@mails.tsinghua.edu.cn; h312h@umd.edu;
   Yukun.Lai@cs.cardiff.ac.uk; zwicker@cs.umd.edu; huizhang@tsinghua.edu.cn
RI Han, Zhizhong/AAW-4044-2021; Lai, Yu-Kun/D-2343-2010
OI Zwicker, Matthias/0000-0001-8630-5515
FU National Natural Science Foundation of China [61373070]; NSF [1813583];
   Tsinghua-Kuaishou Institute of Future Media Data; Direct For Computer &
   Info Scie & Enginr; Div Of Information & Intelligent Systems [1813583]
   Funding Source: National Science Foundation
FX This work was supported by the National Natural Science Foundation of
   China (61373070), NSF (1813583) and Tsinghua-Kuaishou Institute of
   Future Media Data.
CR [Anonymous], 2017, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2017.632
   [Anonymous], ACM T GRAPHIC
   [Anonymous], 2016, Semantic scene completion from a single depth image
   [Anonymous], 2015, ARXIV151104581
   [Anonymous], 2014, ARXIV PREPRINT ARXIV
   Arjovsky M., 2017, ARXIV170107875
   Camino R, 2018, ARXIV180701202
   Chen DY, 2003, COMPUT GRAPH FORUM, V22, P223, DOI 10.1111/1467-8659.00669
   Chen GM, 2016, COMPUT GRAPH-UK, V60, P34, DOI 10.1016/j.cag.2016.08.009
   Chen K., 2013, EFFICIENT ESTIMATION, P2
   Chen Kevin, 2018, ACCV
   Chen XW, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P321, DOI 10.1145/2733373.2806274
   Chen Y, 2018, PROC CVPR IEEE, P9465, DOI 10.1109/CVPR.2018.00986
   Donahue C., 2018, Adversarial audio synthesis
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Gulrajani Ishaan, 2017, P ADV NEUR INF PROC, P5767
   Gumbel E. J., 1954, NBS APPL MATH SER, V33
   Jang E., 2016, P 5 INT C LEARN REPR
   King DB, 2015, ACS SYM SER, V1214, P1
   Liu J, 2017, INT CONF 3D VISION, P126, DOI 10.1109/3DV.2017.00024
   Lopez-Paz D., 2016, ARXIV161006545
   Maddison Chris J, 2016, ARXIV161100712
   Makhzani Alireza, 2016, CoRR abs/1511.05644
   NEMHAUSER GL, 1978, MATH PROGRAM, V14, P265, DOI 10.1007/BF01588971
   Radford A., 2015, ARXIV
   Reed S., 2016, ARXIV PREPRINT ARXIV
   Sun JA, 2009, COMPUT GRAPH FORUM, V28, P1383, DOI 10.1111/j.1467-8659.2009.01515.x
   Wang K, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201362
   Wu Huikai, 2017, ARXIV170307195
   Wu J, 2016, 2016 2ND INTERNATIONAL CONFERENCE ON SOCIAL, EDUCATION AND MANAGEMENT ENGINEERING (SEME 2016), P82
   Xu Q., 2018, ARXIV180607755
   Zhang SY, 2017, VISUAL COMPUT, V33, P925, DOI 10.1007/s00371-017-1394-5
   Zhang Z., 2018, ARXIV180802084
NR 33
TC 23
Z9 24
U1 0
U2 16
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2019
VL 35
IS 6-8
SI SI
BP 1157
EP 1169
DI 10.1007/s00371-019-01691-w
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IC1IH
UT WOS:000470712200031
OA Bronze, Green Accepted
DA 2024-07-18
ER

PT J
AU Lobachev, O
AF Lobachev, Oleg
TI Direct visualization of cryptographic keys for enhanced security
SO VISUAL COMPUTER
LA English
DT Article
DE Visualization; Cryptography; Public key; Hash; Collision; PGP; SHA1;
   SHA2
AB PGP public keys are relatively small binary data. Their hashes are used and also visualized for comparison and validation purposes. We pursue a direct, but previously unused approach. We produce colorful images of public keys and other binary data by generating drawing primitives from binary input. Optionally, we also include the hashes in the visualization. The visualization of raw data together with its hash provides a further security benefit. With it we can visually detect hash collisions. The primary focus of this paper is a direct visualization of public keys. We tune the transparency heuristics for better results. Our method visually detects key spoofing on real SHA1 collision data.
C1 [Lobachev, Oleg] Univ Bayreuth, Visual Comp, Univ Str 30, D-95440 Bayreuth, Germany.
C3 University of Bayreuth
RP Lobachev, O (corresponding author), Univ Bayreuth, Visual Comp, Univ Str 30, D-95440 Bayreuth, Germany.
EM oleg.lobachev@uni-bayreuth.de
RI Lobachev, Oleg/F-2843-2012
OI Lobachev, Oleg/0000-0002-7193-6258
CR [Anonymous], 2005, 21 ANN COMP SEC APPL
   [Anonymous], 1995, The official PGP user's guide
   Awni J., 2017, US Patent App, Patent No. [14/837,652, US20170061199 A1, 14837652]
   BSD General Commands Manual, 2017, BSD GEN COMMANDS MAN
   Cervesato I, 1999, P IEEE CSFW, P55, DOI 10.1109/CSFW.1999.779762
   Cheng YM, 2007, VISUAL COMPUT, V23, P721, DOI 10.1007/s00371-007-0147-2
   Cheng YM, 2006, VISUAL COMPUT, V22, P845, DOI 10.1007/s00371-006-0069-4
   Conti G, 2005, IEEE WORKSHOP ON VISUALIZATION FOR COMPUTER SECURITY 2005, PROCEEDINGS, P83, DOI 10.1109/VIZSEC.2005.1532069
   Cox IJ., 2007, DIGITAL WATERMARKING
   Dhamija R., 2000, P C USENIX SEC S, V9, P4
   GNU Privacy Guard, 2016, MAN PAG GPG2 OPENPGP
   Hou YC, 2003, PATTERN RECOGN, V36, P1619, DOI 10.1016/S0031-3203(02)00258-3
   Liang J, 2007, J COMPUT SCI TECH-CH, V22, P79, DOI 10.1007/s11390-007-9010-1
   Naor M, 1994, LECT NOTES COMPUTER, V950, P1, DOI [10.1007/BFb0053419, DOI 10.1007/BFB0053419]
   Nataraj L., 2011, INT S VISUALIZATION, P1
   National Institute of Standards and Technology, 2015, SECURE HASH STANDARD, V180, P36, DOI DOI 10.6028/NIST.FIPS.180-4
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   OpenSSL, 2016, MAN PAG OP OPENSSL C
   Perrig A., 1999, Proceedings of 1999 International Workshop on Cryptographic Techniques and E-Commerce, P131
   Rescorla E, 2000, REQUEST COMMENTS 281
   Schneier B., 2007, Applied Cryptography: Protocols, Algorithms, and Source Code in C
   Stevens M, 2013, LECT NOTES COMPUT SC, V8042, P129, DOI 10.1007/978-3-642-40041-4_8
   Stevens Marc., 1 COLLISION FULL SHA
   Subhedar MS, 2014, COMPUT SCI REV, V13-14, P95, DOI 10.1016/j.cosrev.2014.09.001
   Teoh ST, 1998, IEEE ACM T NETWORK, V6, P515, DOI [10.1109/90.731185, DOI 10.1109/90.731185]
   Wang XY, 2005, LECT NOTES COMPUT SC, V3621, P17
   Wang XY, 2005, LECT NOTES COMPUT SC, V3494, P19
NR 27
TC 2
Z9 2
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2018
VL 34
IS 12
BP 1749
EP 1759
DI 10.1007/s00371-017-1466-6
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GY3WM
UT WOS:000448487400011
DA 2024-07-18
ER

PT J
AU Zobel, V
   Scheuermann, G
AF Zobel, Valentin
   Scheuermann, Gerik
TI Extremal curves and surfaces in symmetric tensor fields
SO VISUAL COMPUTER
LA English
DT Article
DE Tensor field visualization; Tensor invariants; Tensor field topology
ID VISUALIZATION; STRESS; GLYPH; SIMPLIFICATION; STRAIN; LINES
AB The visualization of symmetric second-order tensor fields in two or three dimensions is still a challenging task, particularly if global structures of the data are desired. One approach is tensor field topology which provides structures characterizing the behavior of the eigenvector fields. Another widely used approach is analyzing tensor fields by means of scalar invariants, i.e., quantities invariant with respect to changes of the coordinate system. In this case, the selection of the relevant invariants might be difficult. Thus, we propose an approach which analyzes the complete invariant part of the tensor. We define extremal points for tensor fields in a mathematically rigorous way, which form curves for two-dimensional and surfaces for three-dimensional tensor fields. We propose a way to compute extremal curves or surfaces from a suitable set of two or three invariants, respectively. We also show that commonly used sets of invariants lead to the same extremal points. Consequently, extremal points contain minima and maxima of most invariants used in tensor field analysis and they are linked to the tensor field topology by containing the degenerate points. Moreover, we show that each extremal point is an extremum or a saddle of a certain invariant. The method is demonstrated on synthetic datasets as well as on stress tensor fields from structure simulations.
C1 [Zobel, Valentin; Scheuermann, Gerik] Univ Leipzig, Dept Comp Sci, Leipzig, Germany.
C3 Leipzig University
RP Zobel, V (corresponding author), Univ Leipzig, Dept Comp Sci, Leipzig, Germany.
EM zobel@informatik.uni-leipzig.de; scheuermann@informatik.uni-leipzig.de
CR [Anonymous], 2002, FDN COMPUT MATH
   [Anonymous], 1913, NACHRICHTEN GESEL MP
   Bachthaler S, 2008, IEEE T VIS COMPUT GR, V14, P1428, DOI 10.1109/TVCG.2008.119
   Bhatia H, 2015, COMP GEOM-THEOR APPL, V48, P311, DOI 10.1016/j.comgeo.2014.10.009
   Criscione JC, 2000, J MECH PHYS SOLIDS, V48, P2445, DOI 10.1016/S0022-5096(00)00023-5
   Delmarcelle T., 1994, Proceedings. Visualization '94 (Cat. No.94CH35707), P140, DOI 10.1109/VISUAL.1994.346326
   Dick C, 2009, IEEE T VIS COMPUT GR, V15, P1399, DOI 10.1109/TVCG.2009.184
   Dickinson R. R., 1989, Proceedings of the SPIE - The International Society for Optical Engineering, V1083, P173, DOI 10.1117/12.952885
   Do Carmo M.P., 1976, DIFFERENTIAL GEOMETR, V2
   Ennis DB, 2006, MAGN RESON MED, V55, P136, DOI 10.1002/mrm.20741
   Hashash YMA, 2003, INT J NUMER ANAL MET, V27, P603, DOI 10.1002/nag.288
   Hlawhschka M, 2007, LECT NOTES COMPUT SC, V4841, P331
   Hotz I, 2004, IEEE VISUALIZATION 2004, PROCEEEDINGS, P123, DOI 10.1109/VISUAL.2004.80
   Jeremic B, 2002, INT J NUMER ANAL MET, V26, P925, DOI 10.1002/nag.223
   Kindlmann G, 2007, IEEE T MED IMAGING, V26, P1483, DOI 10.1109/TMI.2007.907277
   Kindmann G, 2006, IEEE T VIS COMPUT GR, V12, P1329, DOI 10.1109/TVCG.2006.134
   Kratz A, 2013, COMPUT GRAPH FORUM, V32, P49, DOI 10.1111/j.1467-8659.2012.03231.x
   Kratz A., 2014, P PAC VIS
   Kratz A., 2011, P VIS MOD VIS EUR AS
   Lehmann DJ, 2010, IEEE T VIS COMPUT GR, V16, P1291, DOI 10.1109/TVCG.2010.146
   Magnus JR., 1985, ECON THEOR, V1, P179, DOI DOI 10.1017/S0266466600011129
   Margulies DS, 2013, NEUROIMAGE, V80, P445, DOI 10.1016/j.neuroimage.2013.04.111
   Palacios J, 2016, IEEE T VIS COMPUT GR, V22, P1248, DOI 10.1109/TVCG.2015.2484343
   Schultz T, 2010, IEEE T VIS COMPUT GR, V16, P1595, DOI 10.1109/TVCG.2010.199
   Suthambhara N, 2011, MATH VIS, P91
   Tricoche X, 2004, MATH VISUAL, P275
   Tricoche X, 2001, COMPUT GRAPH FORUM, V20, pC461, DOI 10.1111/1467-8659.00539
   Tricoche X, 2008, IEEE T VIS COMPUT GR, V14, P1627, DOI 10.1109/TVCG.2008.148
   Zheng XQ, 2005, IEEE VISUALIZATION 2005, PROCEEDINGS, P551
   Zheng XQ, 2005, IEEE T VIS COMPUT GR, V11, P395, DOI 10.1109/TVCG.2005.67
   Zheng XQ, 2004, IEEE VISUALIZATION 2004, PROCEEEDINGS, P313, DOI 10.1109/VISUAL.2004.105
   Zobel V, 2015, 2015 IEEE SCIENTIFIC VISUALIZATION CONFERENCE (SCIVIS), P49, DOI 10.1109/SciVis.2015.7429491
NR 32
TC 10
Z9 10
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2018
VL 34
IS 10
BP 1427
EP 1442
DI 10.1007/s00371-017-1450-1
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GR0KK
UT WOS:000442204400012
DA 2024-07-18
ER

PT J
AU Luo, GL
   Lei, G
   Cao, YL
   Liu, QH
   Seo, H
AF Luo, Guoliang
   Lei, Gang
   Cao, Yuanlong
   Liu, Qinghua
   Seo, Hyewon
TI Joint entropy-based motion segmentation for 3D animations
SO VISUAL COMPUTER
LA English
DT Article
DE Joint entropy; Key-frame extraction; Motion segmentation; 3D animation
ID KEY-FRAME-EXTRACTION
AB With the recent advancement of data acquisition techniques, 3D animation is becoming a new challenging subject for data processing. In this paper, we present a joint entropy-based key-frame extraction method, which further derives a motion segmentation method for 3D animations. We start by applying an existing deformation-based feature descriptor to measure the degree of deformation of each triangle within each frame, from which we compute the statistical joint probability distribution of triangles' deformation between two consecutive subsequences of frames with a fixed length. Then, we further compute joint entropy between the two subsequences. This allows us to extract key-frames by taking the local maximal from the joint entropy curve (or energy curve) of a given 3D animation. Furthermore, we classify the extracted key-frames by grouping the key-frames with similar motions into the same cluster. Finally, we compute a boundary frame between each of the two neighboring frames with different motions, which is achieved by minimizing the variance of energy between the two motions. The experimental results show that our method successfully extracts representative key-frames of different motions, and the comparisons with existing methods show the effectiveness and the efficiency of our method.
C1 [Luo, Guoliang; Lei, Gang; Cao, Yuanlong; Liu, Qinghua] Jiangxi Normal Univ MIMLab, Nanchang, Jiangxi, Peoples R China.
   [Seo, Hyewon] Univ Strasbourg, ICube, UMR 7357, CNRS, Strasbourg, France.
C3 Universites de Strasbourg Etablissements Associes; Universite de
   Strasbourg; Centre National de la Recherche Scientifique (CNRS); CNRS -
   Institute for Engineering & Systems Sciences (INSIS)
RP Luo, GL (corresponding author), Jiangxi Normal Univ MIMLab, Nanchang, Jiangxi, Peoples R China.
EM 28guoliang.luo@gmail.com
RI 刘, 清华/ABD-9106-2021
OI 刘, 清华/0000-0003-4644-5518
FU National Nature Science Foundation of China [61602222, 61562044];
   Science and Technology Research Program - Education Department of
   Jiangxi Province [GJJ150359]; JXNU [6754]; Science and Technology
   Research Project of Jiangxi Provincial Department of Education
   [GJJ14246]; French national project SHARED (Shape Analysis and
   Registration of People Using Dynamic Data) [10-CHEX-014-01]
FX This work has been jointly supported by the National Nature Science
   Foundation of China (Nos. 61602222, 61562044), the Science and
   Technology Research Program funded by the Education Department of
   Jiangxi Province (No. GJJ150359), the Doctoral Research Project of JXNU
   (6754), the Science and Technology Research Project of Jiangxi
   Provincial Department of Education (No. GJJ14246) and the French
   national project SHARED (Shape Analysis and Registration of People Using
   Dynamic Data, No. 10-CHEX-014-01). We are also thankful to the
   assistance from our colleagues Lei Haopeng, Yi Yugen and Li Zhangyu.
CR Barbic J, 2004, PROC GRAPH INTERF, P185
   Baum L. E., 1972, Inequalities, V3, P1
   Fine S, 1998, MACH LEARN, V32, P41, DOI 10.1023/A:1007469218079
   Fod A, 2002, AUTON ROBOT, V12, P39, DOI 10.1023/A:1013254724861
   Gal R, 2007, IEEE T VIS COMPUT GR, V13, P261, DOI 10.1109/TVCG.2007.45
   Gonzalez-Diaz I, 2015, EXPERT SYST APPL, V42, P488, DOI 10.1016/j.eswa.2014.08.001
   Hartigan J. A., 1979, Applied Statistics, V28, P100, DOI 10.2307/2346830
   Janus B, 2005, 2005 12th International Conference on Advanced Robotics, P411
   Lee TY, 2008, IEEE T CIRC SYST VID, V18, P478, DOI 10.1109/TCSVT.2008.918456
   Lin CD, 2008, 2008 IEEE INTERNATIONAL CONFERENCE ON MULTISENSOR FUSION AND INTEGRATION FOR INTELLIGENT SYSTEMS, VOLS 1 AND 2, P1
   Liu TM, 2003, IEEE T CIRC SYST VID, V13, P1006, DOI 10.1109/TCSVT.2003.816521
   Luo G., 2015, COMP GRAPH INT CGI 2
   Luo G., 2014, COMP GRAPH INT CGI 2
   Luo G., 2014, 3D OBJ RETR WORKSH 3
   Mahesh Venkata Krishna, 2014, [Pattern Recognition and Image Analysis (Advances in Mathematical Theory and Applications), Pattern Recognition and Image Analysis. (Advances in Mathematical Theory and Applications)], V24, P243, DOI 10.1134/S1054661814020114
   Maji S., 2008, CVPR
   RABINER LR, 1989, P IEEE, V77, P257, DOI 10.1109/5.18626
   Shamir A, 2008, COMPUT GRAPH FORUM, V27, P1539, DOI 10.1111/j.1467-8659.2007.01103.x
   Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688
   Sun JA, 2009, COMPUT GRAPH FORUM, V28, P1383, DOI 10.1111/j.1467-8659.2009.01515.x
   van Kaick O, 2011, COMPUT GRAPH FORUM, V30, P1681, DOI 10.1111/j.1467-8659.2011.01884.x
   Wang TS, 2001, LECT NOTES COMPUT SC, V2195, P174
   Yamauchi H, 2005, VISUAL COMPUT, V21, P659, DOI 10.1007/s00371-005-0319-x
   Zhou F, 2013, IEEE T PATTERN ANAL, V35, P582, DOI 10.1109/TPAMI.2012.137
NR 24
TC 2
Z9 3
U1 0
U2 6
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2017
VL 33
IS 10
BP 1279
EP 1289
DI 10.1007/s00371-016-1313-1
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FF8VN
UT WOS:000409296000006
DA 2024-07-18
ER

PT J
AU Whittle, J
   Jones, MW
   Mantiuk, R
AF Whittle, Joss
   Jones, Mark W.
   Mantiuk, Rafal
TI Analysis of reported error in Monte Carlo rendered images
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 34th Conference on Computer Graphics International (CGI)
CY JUN 27-30, 2017
CL Yokohama, JAPAN
SP Keio Univ, Fac Sci & Technol
DE Image quality assessment; Error metric; Monte Carlo rendering
ID QUALITY ASSESSMENT; STATISTICAL EVALUATION; STRUCTURAL SIMILARITY;
   INFORMATION; VISIBILITY; NOISE
AB Evaluating image quality in Monte Carlo rendered images is an important aspect of the rendering process as we often need to determine the relative quality between images computed using different algorithms and with varying amounts of computation. The use of a gold-standard, reference image, or ground truth is a common method to provide a baseline with which to compare experimental results. We show that if not chosen carefully, the quality of reference images used for image quality assessment can skew results leading to significant misreporting of error. We present an analysis of error in Monte Carlo rendered images and discuss practices to avoid or be aware of when designing an experiment.
C1 [Whittle, Joss] Swansea Univ, Comp Sci, Visual Comp Res Grp, Swansea, W Glam, Wales.
   [Jones, Mark W.] Swansea Univ, Dept Comp Sci, Visual Comp Res Grp, Swansea, W Glam, Wales.
   [Mantiuk, Rafal] Univ Cambridge, Comp Lab, Cambridge, England.
C3 Swansea University; Swansea University; University of Cambridge
RP Jones, MW (corresponding author), Swansea Univ, Dept Comp Sci, Visual Comp Res Grp, Swansea, W Glam, Wales.
EM csjoss@swansea.ac.uk; m.w.jones@Swansea.ac.uk;
   rafal.mantiuk@cl.cam.ac.uk
RI Mantiuk, Rafał K./AAP-9514-2020; Jones, Mark W./F-1114-2015
OI Mantiuk, Rafał K./0000-0003-2353-0349; Jones, Mark
   W./0000-0001-8991-1190; Whittle, Joss/0000-0002-4147-7185
FU EPSRC [EP/K502935/1]
FX Joss Whittle is supported by EPSRC Doctoral Training Award EP/K502935/1.
CR Anderson M, 1996, FOURTH COLOR IMAGING CONFERENCE: COLOR SCIENCE, SYSTEMS AND APPLICATIONS, P238
   [Anonymous], 2014, LIVE IMAGE QUALITY A
   [Anonymous], P INT C COGN REC IEE
   Avcibas I, 2002, J ELECTRON IMAGING, V11, P206, DOI 10.1117/1.1455011
   Bae SH, 2016, IEEE T IMAGE PROCESS, V25, P2392, DOI 10.1109/TIP.2016.2545863
   Bauszat P, 2015, COMPUT GRAPH FORUM, V34, P597, DOI 10.1111/cgf.12587
   Bovik A, 2009, ESSENTIAL GUIDE TO IMAGE PROCESSING, 2ND EDITION, P1
   Brunet D, 2012, IEEE T IMAGE PROCESS, V21, P1488, DOI 10.1109/TIP.2011.2173206
   BURT PJ, 1983, IEEE T COMMUN, V31, P532, DOI 10.1109/TCOM.1983.1095851
   Chai T., 2014, GEOSCIENTIFIC MODEL, V7, P1525, DOI [DOI 10.5194/GMDD-7-1525-2014, 10.5194/gmdd-7-1525-2014]
   Chandler DM, 2007, IEEE T IMAGE PROCESS, V16, P2284, DOI 10.1109/TIP.2007.901820
   Chui C. K., 1992, An Introduction to Wavelets, DOI 10.2307/2153134
   Cline D, 2005, ACM T GRAPHIC, V24, P1186, DOI 10.1145/1073204.1073330
   Damera-Venkata N, 2000, IEEE T IMAGE PROCESS, V9, P636, DOI 10.1109/83.841940
   Do MN, 2005, IEEE T IMAGE PROCESS, V14, P2091, DOI 10.1109/TIP.2005.859376
   Doidge IC, 2013, VISUAL COMPUT, V29, P707, DOI 10.1007/s00371-013-0807-3
   Doidge IC, 2012, VISUAL COMPUT, V28, P603, DOI 10.1007/s00371-012-0703-2
   Eslami R., 2004, P C INF SCI SYST PRI, P784
   Gao XB, 2009, IEEE T IMAGE PROCESS, V18, P1409, DOI 10.1109/TIP.2009.2018014
   Hachisuka T, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601138
   Hachisuka T, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409083
   Hachisuka T, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360632
   Jakob W, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185554
   Jakob Wenzel, 2010, Mitsuba renderer
   Jensen H. W., 1995, WSCG 95 (Winter School of Computer Graphics and Visualisation 95). Third International Conference in Central Europe on Computer Graphics and Visualisation 95, P134
   Jensen HW., 2001, REALISTIC IMAGE SYNT, DOI [10.1201/9780429294907, DOI 10.1201/9780429294907]
   Kajiya J. T., 1986, SIGGRAPH, P143, DOI 10.1145/15886.15902
   Kalantari NK, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766977
   Kalantari NK, 2013, COMPUT GRAPH FORUM, V32, P93, DOI 10.1111/cgf.12029
   Kelemen C, 2002, COMPUT GRAPH FORUM, V21, P531, DOI 10.1111/1467-8659.t01-1-00703
   Kontkanen J, 2006, MONTE CARLO AND QUASI-MONTE CARLO METHODS 2004, P259, DOI 10.1007/3-540-31186-6_16
   Lafortune E. P., 1993, EDUGRAPHICS '93. First International Conference on Graphics Education. COMPUGRAPHICS '93. Third International Conference on Computational Graphics and Visualization Techniques. Combined Proceedings, P145
   Li TM, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818084
   Mantiuk R, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964935
   Mitsa T., 1993, ICASSP-93. 1993 IEEE International Conference on Acoustics, Speech, and Signal Processing (Cat. No.92CH3252-4), P301, DOI 10.1109/ICASSP.1993.319807
   Pharr M., 2010, PHYS BASED RENDERING
   Ponomarenko Nikolay, 2013, 2013 4th European Workshop on Visual Information Processing (EUVIP), P106
   Rousselle F, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366214
   Rushmeier H. E., 1994, Computer Graphics Proceedings. Annual Conference Series 1994. SIGGRAPH 94 Conference Proceedings, P131, DOI 10.1145/192161.192189
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P3440, DOI 10.1109/TIP.2006.881959
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P430, DOI 10.1109/TIP.2005.859378
   Sheikh HR, 2005, IEEE T IMAGE PROCESS, V14, P2117, DOI 10.1109/TIP.2005.859389
   SHY D, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P237, DOI 10.1109/CVPR.1994.323835
   Suykens F., 2000, 8 INT C CENTR EUR CO
   Tamstorf R., 1997, Rendering Techniques '97. Proceedings of the Eurographics Workshop. Eurographics, P285
   Tao DC, 2009, IEEE T SYST MAN CY B, V39, P1623, DOI 10.1109/TSMCB.2009.2021951
   Tofallis C, 2015, J OPER RES SOC, V66, P1352, DOI 10.1057/jors.2014.103
   Valeton J. M., 1989, OPTICAL ENG, V28, P287
   Veach E., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P65, DOI 10.1145/258734.258775
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang Z, 2002, IEEE SIGNAL PROC LET, V9, P81, DOI 10.1109/97.995823
   Wang Z, 2011, IEEE T IMAGE PROCESS, V20, P1185, DOI 10.1109/TIP.2010.2092435
   Wang Z, 2009, IEEE SIGNAL PROC MAG, V26, P98, DOI 10.1109/MSP.2008.930649
   Willmott CJ, 2005, CLIMATE RES, V30, P79, DOI 10.3354/cr030079
   Yang CC, 2003, OPT ENG, V42, P701, DOI 10.1117/1.1544479
   Zhang L, 2011, IEEE T IMAGE PROCESS, V20, P2378, DOI 10.1109/TIP.2011.2109730
NR 57
TC 4
Z9 4
U1 1
U2 7
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2017
VL 33
IS 6-8
BP 705
EP 713
DI 10.1007/s00371-017-1384-7
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA EX1EY
UT WOS:000402964800003
PM 30930515
OA Green Published, hybrid
DA 2024-07-18
ER

PT J
AU Giorgi, D
   Pascali, MA
   Henriquez, P
   Matuszewski, BJ
   Colantonio, S
   Salvetti, O
AF Giorgi, Daniela
   Pascali, M. Antonietta
   Henriquez, Pedro
   Matuszewski, Bogdan J.
   Colantonio, Sara
   Salvetti, Ovidio
TI Persistent homology to analyse 3D faces and assess body weight gain
SO VISUAL COMPUTER
LA English
DT Article
DE Image processing; Feature measurement; Feature representation; size and
   shape
ID SHAPE; MANIFOLDS
AB In this paper, we analyse patterns in face shape variation due to weight gain. We propose the use of persistent homology descriptors to get geometric and topological information about the configuration of anthropometric 3D face landmarks. In this way, evaluating face changes boils down to comparing the descriptors computed on 3D face scans taken at different times. By applying dimensionality reduction techniques to the dissimilarity matrix of descriptors, we get a space in which each face is a point and face shape variations are encoded as trajectories in that space. Our results show that persistent homology is able to identify features which are well related to overweight and may help assessing individual weight trends. The research was carried out in the context of the European project SEMEOTICONS, which developed a multisensory platform which detects and monitors over time facial signs of cardio-metabolic risk.
C1 [Giorgi, Daniela; Pascali, M. Antonietta; Colantonio, Sara; Salvetti, Ovidio] Natl Res Council Italy, Inst Informat Sci & Technol, Via G Moruzzi 1, I-56124 Pisa, Italy.
   [Henriquez, Pedro; Matuszewski, Bogdan J.] Univ Cent Lancashire, Sch Engn, Comp Vis & Machine Learning Res Grp, Preston PR1 2HE, Lancs, England.
C3 Consiglio Nazionale delle Ricerche (CNR); Istituto di Scienza e
   Tecnologie dell'Informazione "Alessandro Faedo" (ISTI-CNR); University
   of Central Lancashire
RP Pascali, MA (corresponding author), Natl Res Council Italy, Inst Informat Sci & Technol, Via G Moruzzi 1, I-56124 Pisa, Italy.
EM maria.antonietta.pascali@isti.cnr.it
RI Colantonio, Sara/AFL-2051-2022; Pascali, Maria Antonietta/AAU-5020-2020;
   GIORGI, DANIELA/B-3998-2017
OI Colantonio, Sara/0000-0003-2022-0804; Pascali, Maria
   Antonietta/0000-0001-7742-8126; GIORGI, DANIELA/0000-0002-6752-6918;
   Henriquez, Pedro/0000-0001-6582-5351
FU European Union Seventh Framework Programme (FP7) [611516]
FX The research leading to these results has received funding from the
   European Union Seventh Framework Programme (FP7/2013-2016) under the
   grant agreement n. 611516 (SEMEOTICONS-SEMEiotic Oriented Technology for
   Individuals CardiOmetabolic risk self-assessmeNt and Self-monitoring).
CR [Anonymous], 2015, PROC IEEE INT C MULT, DOI DOI 10.1109/ICME.2015.7177468
   [Anonymous], 1998, STAT SHAPE ANAL
   [Anonymous], 2014, SYNTH LECT COMPUT GR
   Berretti S, 2011, VISUAL COMPUT, V27, P1021, DOI 10.1007/s00371-011-0611-x
   Biasotti S, 2013, COMPUT GRAPH FORUM, V32, P13, DOI 10.1111/cgf.12168
   Bookstein FL, 1996, B MATH BIOL, V58, P313, DOI 10.1007/BF02458311
   Bronstein AM, 2008, MONOGR COMPUT SCI, P1, DOI 10.1007/978-0-387-73301-2_1
   Çeliktutan O, 2013, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2013-13
   Coetzee V, 2010, PERCEPTION, V39, P51, DOI 10.1068/p6560
   Collyer ML, 2013, HYSTRIX, V24, P75, DOI 10.4404/hystrix-24.1-6298
   COOTES TF, 1995, COMPUT VIS IMAGE UND, V61, P38, DOI 10.1006/cviu.1995.1004
   Coppini G., 2014, HEALTHINF 2014
   Cormen ThomasH., 1994, INTRO ALGORITHMS
   CORTI M, 1993, TRENDS ECOL EVOL, V8, P302, DOI 10.1016/0169-5347(93)90261-M
   d'Amico M, 2006, INT J IMAG SYST TECH, V16, P154, DOI 10.1002/ima.20076
   Dibeklioglu H., 2008, BTAS 2008 - IEEE 2nd International Conference on Biometrics: Theory, Applications and Systems, P1
   Edelsbrunner H, 2002, DISCRETE COMPUT GEOM, V28, P511, DOI 10.1007/s00454-002-2885-2
   Fanelli Gabriele, 2011, Pattern Recognition. Proceedings 33rd DAGM Symposium, P101, DOI 10.1007/978-3-642-23123-0_11
   Ferrario VF, 2004, ANGLE ORTHOD, V74, P37
   Gamble J, 2010, J MULTIVARIATE ANAL, V101, P2184, DOI 10.1016/j.jmva.2010.04.016
   Giachetti A, 2015, IEEE J BIOMED HEALTH, V19, P660, DOI 10.1109/JBHI.2014.2314360
   Gower JC., 1998, ENCY BIOSTATISTICS, P3514
   Heo G, 2012, J AM STAT ASSOC, V107, P477, DOI 10.1080/01621459.2011.641430
   Jahanbin S, 2011, IEEE T INF FOREN SEC, V6, P1287, DOI 10.1109/TIFS.2011.2162585
   Kazhdan M., 2006, P 4 EUR S GEOM PROC, P61, DOI DOI 10.2312/SGP/SGP06/061-070
   Kazhdan M.M., 2003, S GEOM PROC, V6
   KENDALL DG, 1984, B LOND MATH SOC, V16, P81, DOI 10.1112/blms/16.2.81
   Kurtek S, 2012, IEEE T PATTERN ANAL, V34, P1717, DOI 10.1109/TPAMI.2011.233
   Kurtek S, 2010, PROC CVPR IEEE, P1625, DOI 10.1109/CVPR.2010.5539778
   Lee BJ, 2014, BMC COMPLEM ALTERN M, V14, DOI 10.1186/1472-6882-14-248
   Loucks EB, 2015, CURR CARDIOL REP, V17, DOI 10.1007/s11886-015-0668-7
   Lu X., 2006, IEEE T PATTERN ANAL
   Newcombe RA, 2011, INT SYM MIX AUGMENT, P127, DOI 10.1109/ISMAR.2011.6092378
   Niyogi P, 2008, DISCRETE COMPUT GEOM, V39, P419, DOI 10.1007/s00454-008-9053-2
   Osada R, 2002, ACM T GRAPHIC, V21, P807, DOI 10.1145/571647.571648
   Paysan P., 2009, P IEEE AVSS
   Quan W, 2010, IEEE IMAGE PROC, P2433, DOI 10.1109/ICIP.2010.5651357
   Reyment RA, 1996, NATO ADV SCI INST SE, V284, P15
   Srivastava A, 2012, IMAGE VISION COMPUT, V30, P398, DOI 10.1016/j.imavis.2012.03.006
   Srivastava A, 2011, IEEE T PATTERN ANAL, V33, P1415, DOI 10.1109/TPAMI.2010.184
   Tausz A., 2011, JAVAPLEX RES SOFTWAR
   Tenenbaum JB, 2000, SCIENCE, V290, P2319, DOI 10.1126/science.290.5500.2319
   Thompson DW., 1917, GROWTH FORM
   Tumpach AB, 2016, IEEE T PATTERN ANAL, V38, P46, DOI 10.1109/TPAMI.2015.2430319
   Velardo C., 2012, P 20 EUR SIGN PROC C
   Velardo C., 2010, BTAS 2010
   Vercauteren T, 2009, NEUROIMAGE, V45, pS61, DOI 10.1016/j.neuroimage.2008.10.040
   Windheuser T., 2011, INT C COMP VIS ICCV
   Younes L, 1998, SIAM J APPL MATH, V58, P565, DOI 10.1137/S0036139995287685
   Younes L, 2012, IMAGE VISION COMPUT, V30, P389, DOI 10.1016/j.imavis.2011.09.009
   Zhang Y, 2013, J MATH IMAGING VIS, V47, P35, DOI 10.1007/s10851-013-0416-9
NR 51
TC 1
Z9 3
U1 0
U2 8
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2017
VL 33
IS 5
BP 549
EP 563
DI 10.1007/s00371-016-1344-7
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ER4KB
UT WOS:000398767300002
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Zhu, YZ
   Yu, JF
   Wu, JQ
AF Zhu, Yingzhen
   Yu, Jifang
   Wu, Jiangqin
TI Chro-Ring: a time-oriented visual approach to represent writer's history
SO VISUAL COMPUTER
LA English
DT Article
DE Timeline; Time-oriented data; Information visualization; User interface
   design
AB Personal histories of ancient Chinese writers have attracted many people. This kind of information is always in the form of obscure texts and the multiple facets are interrelated, so learning personal history in a traditional way is time-consuming. Fortunately, information visualization technique can help us with the problem. To clearly present time-oriented information visually about a famous writer to users, we propose Chro-Ring, a visualization system for representing faceted temporal data. It is time-oriented and displays multiple facets of personal information using multiple visualization views with a delicate layout. We implemented it in a scenario and conducted a user study to evaluate its effectiveness.
C1 [Zhu, Yingzhen; Wu, Jiangqin] Zhejiang Univ, Coll Comp Sci, Hangzhou, Zhejiang, Peoples R China.
   [Yu, Jifang] Shandong Univ, Sch Comp Sci & Technol, 1500 Shunhua Rd, Jinan, Peoples R China.
C3 Zhejiang University; Shandong University
RP Zhu, YZ (corresponding author), Zhejiang Univ, Coll Comp Sci, Hangzhou, Zhejiang, Peoples R China.
EM 429384857@qq.com; 596068309@qq.com; wujq@zju.edu.cn
FU Chinese Knowledge Centre for Engineering Science and Technology
FX This paper is supported by Chinese Knowledge Centre for Engineering
   Science and Technology.
CR Aigner W, 2011, HUM-COMPUT INT-SPRIN, P1, DOI 10.1007/978-0-85729-079-3
   Aigner W, 2007, COMPUT GRAPH-UK, V31, P401, DOI 10.1016/j.cag.2007.01.030
   Andre P, 2007, UIST 2007: PROCEEDINGS OF THE 20TH ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P101
   Bade R., 2004, P SIGCHI C HUM FACT, P105, DOI [10.1145/985692.985706, DOI 10.1145/985692.985706]
   Fuchs G., 2014, P 8 INT C INF VIS, VIV, P139
   Gleicher M, 2011, INFORM VISUAL, V10, P289, DOI 10.1177/1473871611416549
   Havre S, 2000, IEEE SYMPOSIUM ON INFORMATION VISUALIZATION 2000, P115, DOI 10.1109/INFVIS.2000.885098
   Huynh D., 2015, SIMILE TIMELINE
   Jensen M., 2003, INFO VIS 2003
   Kraak M.-J., 2003, P 21 INT CARTOGRAPHI, P1988, DOI [10. 1007 / 3 - 540 - 26772 - 7_15, DOI 10.1007/3-540-26772-7_15]
   Luo DN, 2012, IEEE T VIS COMPUT GR, V18, P93, DOI 10.1109/TVCG.2010.225
   Mazza R., 2009, INTRO INFORM VISUALI
   Plaisant C., 1996, Human Factors in Computing Systems. Common Ground. CHI 96 Conference Proceedings, P221, DOI 10.1145/238386.238493
   Plaisant C, 1998, J AM MED INFORM ASSN, P76
   SMUC M., 2014, HDB HUMAN CENTRIC VI, P623, DOI [10.1007/978-1-4614-7485-2_25, DOI 10.1007/978-1-4614-7485-2_25]
   Tabin K., 2013, HISTORY, V70, P1
   Tamara Munzner, 2008, INFORM VISUALIZATION
   Weichselbraun A, 2014, KNOWL-BASED SYST, V69, P78, DOI 10.1016/j.knosys.2014.04.039
   Zhao J, 2012, PROCEEDINGS OF THE INTERNATIONAL WORKING CONFERENCE ON ADVANCED VISUAL INTERFACES, P433, DOI 10.1145/2254556.2254639
   [No title captured]
NR 21
TC 3
Z9 4
U1 1
U2 7
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2016
VL 32
IS 9
BP 1133
EP 1149
DI 10.1007/s00371-016-1213-4
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DU4DE
UT WOS:000382161400007
DA 2024-07-18
ER

PT J
AU Dagenais, F
   Gagnon, J
   Paquette, E
AF Dagenais, Francois
   Gagnon, Jonathan
   Paquette, Eric
TI An efficient layered simulation workflow for snow imprints
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 33rd Conference on Computer Graphics International (CGI)
CY JUN 28-JUL 01, 2016
CL Heraklion, GREECE
SP Fdn Res Technol
DE Animation; Fluid simulation; Natural phenomena; Snow; Imprints;
   Footprints
ID ANIMATING SAND
AB This paper introduces a novel workflow to generate snow imprints, and model the interaction of snow with dynamic objects. We decoupled snow simulation into three components: a base layer, snow particles, and snow mist. The base layer consists of snow that has not been in contact with a dynamic object yet, and is stored as a level set. Snow particles model the interaction between the snow and the dynamic objects. They are added when the dynamic objects collide with the base layer, and are animated using an adapted granular material simulation. The very thin and powdery snow released by airborne snow particles is modeled by the snow mist. This component is greatly influenced by the surrounding air medium; thus, it is animated using a fluid simulation. This decomposition allows to focus memory expensive and time-consuming computations only where dynamic objects interact with the snow, which is much more efficient than relying on a full-scale simulation.
C1 [Dagenais, Francois; Gagnon, Jonathan; Paquette, Eric] Ecole Technol Super, Multimedia Lab, Montreal, PQ, Canada.
   [Gagnon, Jonathan] Mokko Studio, Res & Dev, Montreal, PQ, Canada.
C3 University of Quebec; Ecole de Technologie Superieure - Canada
RP Dagenais, F (corresponding author), Ecole Technol Super, Multimedia Lab, Montreal, PQ, Canada.
EM francois.dagenais.2@ens.etsmtl.ca
OI Paquette, Eric/0000-0001-9236-647X
CR Alduan I., 2011, P 2011 ACM SIGGRAPH, P25, DOI DOI 10.1145/2019406.2019410
   [Anonymous], 2013, ACM SIGGRAPH 2013 CO
   Bell N., 2005, P 2005 ACM SIGGRAPH, P77, DOI DOI 10.1145/1073368.1073379
   Bronstein AM, 2008, MONOGR COMPUT SCI, P1, DOI 10.1007/978-0-387-73301-2_1
   Fearing P, 2000, COMP GRAPH, P37, DOI 10.1145/344779.344809
   Feldman B.E., 2002, ACM SIGGRAPH 2002 C, P218
   Festenberg N. v., 2009, Eurographics workshop on natural phenomena, P15
   Ihmsen M, 2013, COMPUT GRAPH-UK, V37, P800, DOI 10.1016/j.cag.2013.04.010
   Lenaerts T, 2009, COMPUT GRAPH FORUM, V28, P213, DOI 10.1111/j.1467-8659.2009.01360.x
   Macklin M, 2014, ACM T GRAPHIC, V33, DOI [10.1145/280/109/2601152, 10.1145/2601097.2601152]
   Maréchal N, 2010, COMPUT GRAPH FORUM, V29, P449, DOI 10.1111/j.1467-8659.2009.01614.x
   Moeslund T.B., 2005, Vision, Video, and Graphics, V2
   Müller M, 2007, J VIS COMMUN IMAGE R, V18, P109, DOI 10.1016/j.jvcir.2007.01.005
   Museth K, 2002, ACM T GRAPHIC, V21, P330, DOI 10.1145/566570.566585
   Narain R, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866195
   Onoue K, 2005, COMPUT GRAPH FORUM, V24, P51, DOI 10.1111/j.1467-8659.2005.00828.x
   Pla-Castells M, 2008, EUROGRAPHICS 2008 SH, P21
   Stomakhin A, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461948
   Sumner RW, 1999, COMPUT GRAPH FORUM, V18, P17, DOI 10.1111/1467-8659.00299
   TAKAHASHI T., 2012, ACM SIGGRAPH 2012, P7
   von Festenberg N, 2011, COMPUT GRAPH FORUM, V30, P1837, DOI 10.1111/j.1467-8659.2011.01904.x
   Wang CB, 2006, VISUAL COMPUT, V22, P315, DOI 10.1007/s00371-006-0012-8
   Wong SK, 2015, COMPUT ANIMAT VIRT W, V26, P413, DOI 10.1002/cav.1644
   Zeng YL, 2007, COMPUT ANIMAT VIRT W, V18, P289, DOI 10.1002/cav.209
   Zhu B., 2010, Eurographics 2010
   Zhu YN, 2005, ACM T GRAPHIC, V24, P965, DOI 10.1145/1073204.1073298
NR 26
TC 7
Z9 7
U1 0
U2 7
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2016
VL 32
IS 6-8
BP 881
EP 890
DI 10.1007/s00371-016-1261-9
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA DP8ET
UT WOS:000378731600020
DA 2024-07-18
ER

PT J
AU Biasotti, S
   Cerri, A
   Aono, M
   Ben Hamza, A
   Garro, V
   Giachetti, A
   Giorgi, D
   Godil, A
   Li, C
   Sanada, C
   Spagnuolo, M
   Tatsuma, A
   Velasco-Forero, S
AF Biasotti, S.
   Cerri, A.
   Aono, M.
   Ben Hamza, A.
   Garro, V.
   Giachetti, A.
   Giorgi, D.
   Godil, A.
   Li, C.
   Sanada, C.
   Spagnuolo, M.
   Tatsuma, A.
   Velasco-Forero, S.
TI Retrieval and classification methods for textured 3D models: a
   comparative study
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 3DOR Workshop
CY APR 06, 2014
CL Strasbourg, FRANCE
DE Shape retrieval; Shape classification; Textured 3D models
ID SHAPE RETRIEVAL; COMBINING COLOR; RADIAL SYMMETRY; DESCRIPTORS;
   SURFACES; FEATURES; GEOMETRY; IMAGES
AB This paper presents a comparative study of six methods for the retrieval and classification of textured 3D models, which have been selected as representative of the state of the art. To better analyse and control how methods deal with specific classes of geometric and texture deformations, we built a collection of 572 synthetic textured mesh models, in which each class includes multiple texture and geometric modifications of a small set of null models. Results show a challenging, yet lively, scenario and also reveal interesting insights into how to deal with texture information according to different approaches, possibly working in the CIELab as well as in modifications of the RGB colour space.
C1 [Biasotti, S.; Cerri, A.; Spagnuolo, M.] CNR, Ist Matemat Applicata & Tecnol Informat E Magenes, Genoa, Italy.
   [Garro, V.; Giorgi, D.] CNR, Ist Sci & Tecnol Informaz A Faedo, Pisa, Italy.
   [Garro, V.; Giachetti, A.] Univ Verona, Dipartimento Informat, I-37100 Verona, Italy.
   [Ben Hamza, A.] Concordia Univ, Montreal, PQ, Canada.
   [Giorgi, D.; Godil, A.] NIST, Gaithersburg, MD 20899 USA.
   [Aono, M.; Sanada, C.; Tatsuma, A.] Toyohashi Univ Technol, Dept Comp Sci & Engn, Toyohashi, Aichi, Japan.
   [Velasco-Forero, S.] Natl Univ Singapore, Dept Math, 10 Kent Ridge Crescent, Singapore 117548, Singapore.
C3 Consiglio Nazionale delle Ricerche (CNR); Istituto di Matematica
   Applicata e Tecnologie Informatiche "Enrico Magenes" (IMATI-CNR);
   Consiglio Nazionale delle Ricerche (CNR); Istituto di Scienza e
   Tecnologie dell'Informazione "Alessandro Faedo" (ISTI-CNR); University
   of Verona; Concordia University - Canada; National Institute of
   Standards & Technology (NIST) - USA; Toyohashi University of Technology;
   National University of Singapore
RP Cerri, A (corresponding author), CNR, Ist Matemat Applicata & Tecnol Informat E Magenes, Genoa, Italy.
EM silvia.biasotti@ge.imati.cnr.it; andrea.cerri@ge.imati.cnr.it
RI Hamza, Abdessamad Ben/G-4571-2013; Velasco-Forero, Santiago/W-9489-2019;
   Spagnuolo, Michela/ABA-1927-2021; Giachetti, Andrea/AAD-8247-2020;
   Spagnuolo, Michela/F-5068-2013; Biasotti, Silvia/G-8602-2012; GIORGI,
   DANIELA/B-3998-2017; Cerri, Andrea/B-8129-2015
OI Velasco-Forero, Santiago/0000-0002-2438-1747; Spagnuolo,
   Michela/0000-0002-5682-6990; Spagnuolo, Michela/0000-0002-5682-6990;
   Biasotti, Silvia/0000-0002-9992-825X; Giachetti,
   Andrea/0000-0002-7523-6806; GIORGI, DANIELA/0000-0002-6752-6918; Cerri,
   Andrea/0000-0003-3619-8963; Ben Hamza, Abdessamad/0000-0002-3778-8167
FU Grants-in-Aid for Scientific Research [26280038, 15K12027, 15K15992]
   Funding Source: KAKEN
CR [Anonymous], 2012, AFFINE INVARIANT PHO
   [Anonymous], IEEE INT C ADV VID S
   [Anonymous], COEFFICIENT COLOR C
   [Anonymous], EUR WORKSH 3D OBJ RE
   [Anonymous], LECT NOTES COMPUTER
   [Anonymous], CS2006030 UU
   [Anonymous], THESIS CONCORDIA U M
   [Anonymous], P 3 INT C SCAL SPAC
   [Anonymous], 2014, IEEE C COMPUTER VISI
   [Anonymous], PATTERN RECOGNITION
   [Anonymous], 2013, 6 EG WORKSH 3D OBJ R
   [Anonymous], EEE C COMP VIS PATT
   [Anonymous], IEEE C COMP VIS PAT, DOI DOI 10.1109/CVPR.2006.68
   [Anonymous], 5 INT JOINT C INC IM
   [Anonymous], 2011, PROC EUROGRAPHICS 20, P79, DOI DOI 10.2312/3DOR/3DOR11/079-088
   Aubry M, 2011, IEEE I CONF COMP VIS, P1411, DOI 10.1109/ICCV.2011.6126396
   Biasotti S, 2008, PATTERN RECOGN, V41, P2855, DOI 10.1016/j.patcog.2008.02.003
   Biasotti S, 2008, THEOR COMPUT SCI, V392, P5, DOI 10.1016/j.tcs.2007.10.018
   Biasotti S, 2013, COMPUT GRAPH FORUM, V32, P13, DOI 10.1111/cgf.12168
   Biasotti S., 2014, Eurographics (state of the art reports), P135, DOI DOI 10.2312/EGST.20141039
   Biasotti S, 2013, COMPUT GRAPH-UK, V37, P608, DOI 10.1016/j.cag.2013.05.007
   Changchang Wu, 2008, 2008 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops (CVPR Workshops), P1, DOI 10.1109/CVPRW.2008.4563037
   Cohen-Steiner D, 2007, DISCRETE COMPUT GEOM, V37, P103, DOI 10.1007/s00454-006-1276-5
   Cortelazzo GM, 2007, THIRD INTERNATIONAL SYMPOSIUM ON 3D DATA PROCESSING, VISUALIZATION, AND TRANSMISSION, PROCEEDINGS, P986
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Desbrun M, 1999, COMP GRAPH, P317, DOI 10.1145/311535.311576
   Deza E., 2009, Encyclopedia of Distances
   Edelsbrunner H, 2008, CONTEMP MATH, V453, P257
   Ester M., 1996, KDD 96, P226, DOI DOI 10.5555/3001460.3001507
   Fairchild M.D., 2005, Color Appearance Models, V2nd
   Frosini P., 1999, Pattern Recognition and Image Analysis, V9, P596
   Funkhouser T, 2005, COMMUN ACM, V48, P58, DOI 10.1145/1064830.1064859
   Gebal K, 2009, COMPUT GRAPH FORUM, V28, P1405, DOI 10.1111/j.1467-8659.2009.01517.x
   Gevers T, 2000, IEEE T IMAGE PROCESS, V9, P102, DOI 10.1109/83.817602
   Giachetti A, 2012, COMPUT GRAPH FORUM, V31, P1669, DOI 10.1111/j.1467-8659.2012.03172.x
   Johnson AE, 1999, IEEE T PATTERN ANAL, V21, P433, DOI 10.1109/34.765655
   Kanezaki A, 2010, VISUAL COMPUT, V26, P1269, DOI 10.1007/s00371-010-0521-3
   Kazhdan M., 2003, P EUR ACM SIGGRAPH S, V6, P156
   Kimmel R, 2000, INT J COMPUT VISION, V39, P111, DOI 10.1023/A:1008171026419
   Kovnatsky A, 2013, NUMER MATH-THEORY ME, V6, P199, DOI 10.4208/nmtma.2013.mssvm11
   Kuhn HW, 2005, NAV RES LOG, V52, P7, DOI 10.1002/nav.20053
   Levoy M, 2000, COMP GRAPH, P131, DOI 10.1145/344779.344849
   Li CY, 2013, INT J MULTIMED INF R, V2, P261, DOI 10.1007/s13735-013-0041-9
   Li CY, 2014, MULTIMED TOOLS APPL, V72, P1027, DOI 10.1007/s11042-013-1417-9
   Li CY, 2014, MULTIMEDIA SYST, V20, P253, DOI 10.1007/s00530-013-0318-0
   Li CY, 2013, VISUAL COMPUT, V29, P513, DOI 10.1007/s00371-013-0815-3
   Lian ZH, 2013, PATTERN RECOGN, V46, P449, DOI 10.1016/j.patcog.2012.07.014
   Ling HB, 2005, IEEE I CONF COMP VIS, P1466
   Liu YJ, 2012, VISUAL COMPUT, V28, P75, DOI 10.1007/s00371-011-0605-8
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Loy G, 2003, IEEE T PATTERN ANAL, V25, P959, DOI 10.1109/TPAMI.2003.1217601
   MATTHEWS BW, 1975, BIOCHIM BIOPHYS ACTA, V405, P442, DOI 10.1016/0005-2795(75)90109-9
   Mika S, 1999, ADV NEUR IN, V11, P536
   Mitra NJ, 2013, COMPUT GRAPH FORUM, V32, P1, DOI 10.1111/cgf.12010
   Ojala T, 1996, PATTERN RECOGN, V29, P51, DOI 10.1016/0031-3203(95)00067-4
   Osada R, 2002, ACM T GRAPHIC, V21, P807, DOI 10.1145/571647.571648
   Pasqualotto G, 2013, SIGNAL PROCESS-IMAGE, V28, P608, DOI 10.1016/j.image.2013.01.004
   Pavan M, 2007, IEEE T PATTERN ANAL, V29, P167, DOI 10.1109/TPAMI.2007.250608
   Pele O, 2009, IEEE I CONF COMP VIS, P460, DOI 10.1109/ICCV.2009.5459199
   PHONG BT, 1975, COMMUN ACM, V18, P311, DOI 10.1145/360825.360839
   Rabin J, 2010, LECT NOTES COMPUT SC, V6315, P771, DOI 10.1007/978-3-642-15555-0_56
   Raviv D, 2014, J MATH IMAGING VIS, V50, P144, DOI 10.1007/s10851-013-0467-y
   Raviv D, 2011, COMPUT GRAPH-UK, V35, P692, DOI 10.1016/j.cag.2011.03.030
   REEB G, 1946, CR HEBD ACAD SCI, V222, P847
   Reuter M, 2006, COMPUT AIDED DESIGN, V38, P342, DOI 10.1016/j.cad.2005.10.011
   Rubner Y, 2000, INT J COMPUT VISION, V40, P99, DOI 10.1023/A:1026543900054
   Rustamov Raif M, 2007, P S GEOM PROC, V257, P225
   Shilane P, 2004, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, P167, DOI 10.1109/smi.2004.1314504
   Smeets D, 2009, LECT NOTES COMPUT SC, V5702, P757, DOI 10.1007/978-3-642-03767-2_92
   Starck J, 2007, IEEE I CONF COMP VIS, P2189
   Sun JA, 2009, COMPUT GRAPH FORUM, V28, P1383, DOI 10.1111/j.1467-8659.2009.01515.x
   Suzuki MT, 2001, JOINT 9TH IFSA WORLD CONGRESS AND 20TH NAFIPS INTERNATIONAL CONFERENCE, PROCEEDINGS, VOLS. 1-5, P2271, DOI 10.1109/NAFIPS.2001.944425
   Tanaka J, 2001, TRENDS COGN SCI, V5, P211, DOI 10.1016/S1364-6613(00)01626-0
   Tangelder JH, 2008, MULTIMED TOOLS APPL, V39, P441, DOI 10.1007/s11042-007-0181-0
   Tatsuma A, 2009, VISUAL COMPUT, V25, P785, DOI 10.1007/s00371-008-0304-2
   Tombari F, 2011, IEEE IMAGE PROC, P809, DOI 10.1109/ICIP.2011.6116679
   Wang CH, 2012, LECT NOTES COMPUT SC, V6667, P580, DOI 10.1007/978-3-642-24785-9_49
   Wang JJ, 2010, PROC CVPR IEEE, P3360, DOI 10.1109/CVPR.2010.5540018
   Zaharescu A, 2012, INT J COMPUT VISION, V100, P78, DOI 10.1007/s11263-012-0528-5
NR 79
TC 26
Z9 27
U1 0
U2 17
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2016
VL 32
IS 2
BP 217
EP 241
DI 10.1007/s00371-015-1146-3
PG 25
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA DF9FU
UT WOS:000371667200007
DA 2024-07-18
ER

PT J
AU Cohen-Or, D
   Zhang, H
AF Cohen-Or, Daniel
   Zhang, Hao
TI From inspired modeling to creative modeling
SO VISUAL COMPUTER
LA English
DT Article
ID EXPLORATION; SUPPORT
C1 [Cohen-Or, Daniel] Tel Aviv Univ, IL-69978 Tel Aviv, Israel.
   [Zhang, Hao] Simon Fraser Univ, Sch Comp Sci, Burnaby, BC V5A 1S6, Canada.
   [Zhang, Hao] Simon Fraser Univ, Graph GrUVi Lab, Burnaby, BC V5A 1S6, Canada.
C3 Tel Aviv University; Simon Fraser University; Simon Fraser University
RP Cohen-Or, D (corresponding author), Tel Aviv Univ, IL-69978 Tel Aviv, Israel.
EM dcor@tau.ac.il; haoz@cs.sfu.ca
RI li, jinsong/HJH-9559-2023; Zhang, Hao/HHM-1940-2022
OI Zhang, Hao/0000-0003-1991-119X
CR [Anonymous], 2013, EUROGRAPHICS STATE O
   [Anonymous], 1999, EVOLUTIONARY DESIGN
   Assa J, 2012, COMPUT GRAPH-UK, V36, P250, DOI 10.1016/j.cag.2012.02.002
   Averkiou Melinos, 2014, Computer Graphics Forum, V33, P125, DOI 10.1111/cgf.12310
   Baxter W, 2006, COMPUT GRAPH FORUM, V25, P477, DOI 10.1111/j.1467-8659.2006.00967.x
   Boden M., 1995, Stanford Humanities Review, V4, P123
   Boden M. A, 2003, The Creative Mind: Myths and Mechanisms
   Chaudhuri S, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964930
   Chaudhuri S, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866205
   Colton S, 2012, FRONT ARTIF INTEL AP, V242, P21, DOI 10.3233/978-1-61499-098-7-21
   Davis N, 2015, SPR SER CULT COMPUT, P109, DOI 10.1007/978-1-4471-6681-8_7
   Davis Nicholas, 2013, 9 ART INT INT DIG EN
   De Jong KA, 2002, Evolutionary Computation
   Draves Scott., 2006, P 4 INT S NONPHOTORE, P7, DOI [10.1145/1124728.1124730, DOI 10.1145/1124728.1124730]
   Duda JW, 1997, J MECH DESIGN, V119, P127, DOI 10.1115/1.2828774
   Frazer J., 1995, An Evolutionary Architecture
   Funes P, 1998, ARTIF LIFE, V4, P337, DOI 10.1162/106454698568639
   Funkhouser T, 2004, ACM T GRAPHIC, V23, P652, DOI 10.1145/1015706.1015775
   Gao L, 2015, IEEE T VIS COMPUT GR, V21, P1390, DOI 10.1109/TVCG.2014.2369039
   Han C, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360650
   Hoffmann O, 2005, COMPUTATIONAL COGNIT, VVI, P137
   Hu RZ, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766914
   Jacob M., 2015, P INT C COMP CREAT
   Jennings KE, 2010, MIND MACH, V20, P489, DOI 10.1007/s11023-010-9206-y
   Kim VG, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601117
   Lin JJ, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024217
   Marks J., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P389, DOI 10.1145/258734.258887
   McCormack Jon., 2012, COMPUTERS CREATIVITY
   Pilat ML, 2008, IEEE C EVOL COMPUTAT, P3289, DOI 10.1109/CEC.2008.4631243
   Romero Juan., 2007, ART ARTIFICIAL EVOLU
   Secretan J, 2008, CHI 2008: 26TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS VOLS 1 AND 2, CONFERENCE PROCEEDINGS, P1759
   Shapira L, 2009, COMPUT GRAPH FORUM, V28, P629, DOI 10.1111/j.1467-8659.2009.01403.x
   Shneiderman B, 2007, COMMUN ACM, V50, P20, DOI 10.1145/1323688.1323689
   Sims K., 1994, Computer Graphics Proceedings. Annual Conference Series 1994. SIGGRAPH 94 Conference Proceedings, P15, DOI 10.1145/192161.192167
   SIMS K, 1991, COMP GRAPH, V25, P319, DOI 10.1145/127719.122752
   Soddu C., 1995, P INT C MAK CIT LIV, P5
   Sternberg RJ, 2006, CREATIVITY RES J, V18, P87, DOI 10.1207/s15326934crj1801_10
   Talton JO, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618513
   Umetani N, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185582
   Vieira T, 2009, COMPUT GRAPH FORUM, V28, P717, DOI 10.1111/j.1467-8659.2009.01412.x
   Xu K., 2015, COMPUT GR F IN PRESS
   Xu K, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185553
NR 42
TC 20
Z9 21
U1 0
U2 16
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2016
VL 32
IS 1
BP 7
EP 14
DI 10.1007/s00371-015-1193-9
PG 8
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DF9FQ
UT WOS:000371666800002
DA 2024-07-18
ER

PT J
AU Wu, FZ
   Dong, WM
   Kong, Y
   Mei, X
   Yan, DM
   Zhang, XP
   Paul, JC
AF Wu, Fuzhang
   Dong, Weiming
   Kong, Yan
   Mei, Xing
   Yan, Dong-Ming
   Zhang, Xiaopeng
   Paul, Jean-Claude
TI Feature-aware natural texture synthesis
SO VISUAL COMPUTER
LA English
DT Article
DE Texture synthesis; Texture feature analysis; Composite texture;
   Feature-aware synthesis
ID IMAGE
AB This article presents a framework for natural texture synthesis and processing. This framework is motivated by the observation that given examples captured in natural scene, texture synthesis addresses a critical problem, namely, that synthesis quality can be affected adversely if the texture elements in an example display spatially varied patterns, such as perspective distortion, the composition of different sub-textures, and variations in global color pattern as a result of complex illumination. This issue is common in natural textures and is a fundamental challenge for previously developed methods. Thus, we address it from a feature point of view and propose a feature-aware approach to synthesize natural textures. The synthesis process is guided by a feature map that represents the visual characteristics of the input texture. Moreover, we present a novel adaptive initialization algorithm that can effectively avoid the repeat and verbatim copying artifacts. Our approach improves texture synthesis in many images that cannot be handled effectively with traditional technologies.
C1 [Wu, Fuzhang; Dong, Weiming; Kong, Yan; Mei, Xing; Yan, Dong-Ming; Zhang, Xiaopeng] Chinese Acad Sci, Inst Automat, LIAMA NLPR, Beijing, Peoples R China.
   [Yan, Dong-Ming] King Abdullah Univ Sci & Technol, Visual Comp Ctr, Thuwal, Saudi Arabia.
   [Paul, Jean-Claude] INRIA, Project CAD, Paris, France.
C3 Chinese Academy of Sciences; Institute of Automation, CAS; King Abdullah
   University of Science & Technology; Inria
RP Dong, WM (corresponding author), Chinese Acad Sci, Inst Automat, LIAMA NLPR, Beijing, Peoples R China.
EM Weiming.Dong@ia.ac.cn; xmei@nlpr.ia.ac.cn; yandongming@gmail.com;
   xpzhang@nlpr.ia.ac.cn
RI DONG, Weiming/AAG-7678-2020
OI DONG, Weiming/0000-0001-6502-145X; Yan, Dong-Ming/0000-0003-2209-2404
FU National Natural Science Foundation of China [61172104, 61271430,
   61201402, 61372184, 61372168, 61331018]
FX We thank anonymous reviewers for their valuable input. We thank
   Chongyang Ma for providing some results and valuable comments in the
   preparation of this paper. This work is supported by National Natural
   Science Foundation of China under project Nos. 61172104, 61271430,
   61201402, 61372184, 61372168, and 61331018.
CR [Anonymous], 2009, State of the Art in Example-Based Texture Synthesis R
   [Anonymous], 2001, Schooling for Tomorrow
   [Anonymous], P INT S PLANT GROWTH
   De Benet J. S., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P361, DOI 10.1145/258734.258882
   Dong WM, 2008, VISUAL COMPUT, V24, P515, DOI 10.1007/s00371-008-0232-1
   Efros A. A., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1033, DOI 10.1109/ICCV.1999.790383
   Efros AA, 2001, COMP GRAPH, P341, DOI 10.1145/383259.383296
   Eisenacher C, 2008, COMPUT GRAPH FORUM, V27, P419, DOI 10.1111/j.1467-8659.2008.01139.x
   Han C, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360650
   Hoang MA, 2005, SIGNAL PROCESS, V85, P265, DOI 10.1016/j.sigpro.2004.10.009
   Kim V. G., 2012, ACM Trans. Graph., V31, P1
   Kwatra V, 2005, ACM T GRAPHIC, V24, P795, DOI 10.1145/1073204.1073263
   Kwatra V, 2003, ACM T GRAPHIC, V22, P277, DOI 10.1145/882262.882264
   Lefebvre S, 2005, ACM T GRAPHIC, V24, P777, DOI 10.1145/1073204.1073261
   Lefebvre S, 2006, ACM T GRAPHIC, V25, P541, DOI 10.1145/1141911.1141921
   Li L, 2013, SIGNAL PROCESS, V93, P2559, DOI 10.1016/j.sigpro.2013.02.010
   Liu YX, 2004, ACM T GRAPHIC, V23, P368, DOI 10.1145/1015706.1015731
   Ma CY, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461921
   Ma CY, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964957
   Ma CY, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618456
   Park H, 2013, COMPUT GRAPH-UK, V37, P54, DOI 10.1016/j.cag.2012.10.004
   Rosenberger A, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618453
   Rubner Y, 2000, INT J COMPUT VISION, V40, P99, DOI 10.1023/A:1026543900054
   Tong X, 2002, ACM T GRAPHIC, V21, P665, DOI 10.1145/566570.566634
   Wei LY, 2000, COMP GRAPH, P479, DOI 10.1145/344779.345009
   Wu Q, 2004, ACM T GRAPHIC, V23, P364, DOI 10.1145/1015706.1015730
   Yan DM, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2516971.2516973
   Zalesny A, 2005, INT J COMPUT VISION, V62, P161, DOI 10.1007/s11263-005-4640-7
   Zhang JD, 2003, ACM T GRAPHIC, V22, P295, DOI 10.1145/882262.882266
NR 29
TC 4
Z9 7
U1 2
U2 11
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2016
VL 32
IS 1
BP 43
EP 55
DI 10.1007/s00371-014-1054-y
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DF9FQ
UT WOS:000371666800005
DA 2024-07-18
ER

PT J
AU Ruiz, C
   Le, SN
   Low, KL
AF Ruiz, Conrado, Jr.
   Le, Sang N.
   Low, Kok-Lim
TI Generating animated paper pop-ups from the motion of articulated
   characters
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 32nd Computer Graphics International CGI 15 Conference
CY JUN 24-26, 2015
CL INSA Strasbourg Univ Strasbourg, Strasbourg, FRANCE
SP CNRS, iCUBE, Univ Strasbourg, CGS, Springer, Acm Incooperation, IGG, ACMSIGGRAPH, KIST Europe, Region Alsace, Visteon
HO INSA Strasbourg Univ Strasbourg
DE Paper pop-up; Automated design; Modelling; Physical fabrication;
   Animation
ID CARD DESIGN
AB Pop-up books are fascinating books comprised of paper pieces that pop out when opened to form interesting three-dimensional structures. But more than just reproducing 3D shapes, pop-up artists also use the movement of the paper pieces during the opening process to convey motion and produce some form of animation. Similarly, previous automated methods have focused on reproducing the 3D shape of an input mesh. In our work, we focus in recreating motion. We study the movement of the paper pieces of the different mechanisms used in pop-up structures to automatically design animated pop-ups. Our input is an animation file, containing a 3D character with an armature and motion. We map each of the linkage chains to a specific pop-up mechanism based on the type of motion it can produce. We then obtain the initial values of the parameters of the mechanisms, such as lengths and orientations of the patches, based on our formulations and parameter estimation. Subsequently, we utilize simulated annealing to search for a plausible layout from a valid configuration space. Finally, we produce a printable design layout of the animated pop-up.
C1 [Ruiz, Conrado, Jr.; Low, Kok-Lim] Natl Univ Singapore, Sch Comp, Singapore 117548, Singapore.
   [Ruiz, Conrado, Jr.] De La Salle Univ, Manila 1004, Philippines.
   [Le, Sang N.] Digipen Inst Technol Singapore, Dept Comp Sci, Singapore, Singapore.
C3 National University of Singapore; De La Salle University
RP Ruiz, C (corresponding author), Natl Univ Singapore, Sch Comp, Singapore 117548, Singapore.
EM consruiz@gmail.com; sang.le@digipen.edu; lowkl@comp.nus.edu.sg
OI Ruiz, Conrado Jr./0000-0002-0956-7201
FU Singapore MOE [T1-251RES1104]
FX This work was supported by the Singapore MOE Academic Research Fund
   (Project No. T1-251RES1104). The 3D Models are from Blender Swap
   http://www.blendswap.com. Girl model modified from L. Kaplinski, tree
   model by E. James, and monkey by J. Newnham.
CR Abel Z., 2013, 30th International Symposium on Theoretical Aspects of Computer Science (STACS 2013), V20, P269
   [Anonymous], 2007, Geometric Folding Algorithms: Linkages, Origami, Polyhedra
   [Anonymous], 2014, ACM T GRAPHIC, DOI DOI 10.1145/2601097.2601143
   Carroll Lewis., 2003, ALICES ADVENTURES WO
   Ceylan D, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508400
   Coros S, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461953
   de Figueiredo L.H., 1995, Graphics Gems, VV, P173
   Glassner A, 2002, IEEE COMPUT GRAPH, V22, P74, DOI 10.1109/38.988749
   Glassner A, 2002, IEEE COMPUT GRAPH, V22, P79, DOI 10.1109/38.974521
   Hendrix S. L., 2006, Advanced Technology for Learning, V3, P119, DOI 10.2316/Journal.208.2006.2.208-0878
   Hoiem D, 2005, ACM T GRAPHIC, V24, P577, DOI 10.1145/1073204.1073232
   Hull T., 2006, PROJECT ORIGAMI ACTI
   Iizuka S, 2011, VISUAL COMPUT, V27, P605, DOI 10.1007/s00371-011-0564-0
   Le SN, 2014, IEEE T VIS COMPUT GR, V20, P276, DOI 10.1109/TVCG.2013.108
   Lee YT, 1996, COMPUT GRAPH, V20, P21, DOI 10.1016/0097-8493(95)00089-5
   Li X. -Y., 2010, ACM T GRAPHIC, P1
   Li X.-Y., 2011, ACM SIGGRAPH 2011 NE, P98
   Mitani J., 2003, Transactions of the Information Processing Society of Japan, V44, P1372
   Ruiz CR, 2014, COMPUT GRAPH FORUM, V33, P487, DOI 10.1111/cgf.12320
   Tachi T, 2010, IEEE T VIS COMPUT GR, V16, P298, DOI 10.1109/TVCG.2009.67
   Le-Nguyen TV, 2013, IEEE T VIS COMPUT GR, V19, P1795, DOI 10.1109/TVCG.2013.82
   Uehara R., 2006, 18 CAN C COMP GEOM, P3
   Zhu K., 2013, Proceedings of the SIGCHI conference on human factors in computing systems, P661
   Zhu LF, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366146
NR 24
TC 3
Z9 4
U1 0
U2 14
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2015
VL 31
IS 6-8
BP 925
EP 935
DI 10.1007/s00371-015-1125-8
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA CM2CN
UT WOS:000357487500018
DA 2024-07-18
ER

PT J
AU Vanhoey, K
   Sauvage, B
   Kraemer, P
   Larue, F
   Dischler, JM
AF Vanhoey, Kenneth
   Sauvage, Basile
   Kraemer, Pierre
   Larue, Frederic
   Dischler, Jean-Michel
TI Simplification of meshes with digitized radiance
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 32nd Computer Graphics International CGI 15 Conference
CY JUN 24-26, 2015
CL INSA Strasbourg Univ Strasbourg, Strasbourg, FRANCE
SP CNRS, iCUBE, Univ Strasbourg, CGS, Springer, Acm Incooperation, IGG, ACMSIGGRAPH, KIST Europe, Region Alsace, Visteon
HO INSA Strasbourg Univ Strasbourg
DE Digitized artifacts; Surface light field; Radiance; Mesh simplification;
   Rendering
ID COMPRESSION
AB View-dependent surface color of virtual objects can be represented by outgoing radiance of the surface. In this paper we tackle the processing of outgoing radiance stored as a vertex attribute of triangle meshes. Data resulting from an acquisition process can be very large and computationally intensive to render. We show that when reducing the global memory footprint of such acquired objects, smartly reducing the spatial resolution is an effective strategy for overall appearance preservation. Whereas state-of-the-art simplification processes only consider scalar or vectorial attributes, we conversely consider radiance functions defined on the surface for which we derive a metric. For this purpose, several tools are introduced like coherent radiance function interpolation, gradient computation, and distance measurements. Both synthetic and acquired examples illustrate the benefit and the relevance of this radiance-aware simplification process.
C1 [Vanhoey, Kenneth; Sauvage, Basile; Kraemer, Pierre; Larue, Frederic; Dischler, Jean-Michel] Univ Strasbourg, CNRS, ICube, Strasbourg, France.
   [Vanhoey, Kenneth] Inria, Sophia Antipolis, France.
C3 Centre National de la Recherche Scientifique (CNRS); Universites de
   Strasbourg Etablissements Associes; Universite de Strasbourg; Inria
RP Vanhoey, K (corresponding author), Univ Strasbourg, CNRS, ICube, Strasbourg, France.
EM kenneth.vanhoey@inria.fr
OI Vanhoey, Kenneth/0000-0001-8816-6168
CR [Anonymous], RENDERING TECHNIQUES
   Cabral B, 1999, COMP GRAPH, P165, DOI 10.1145/311535.311553
   Chen WC, 2002, ACM T GRAPHIC, V21, P447, DOI 10.1145/566570.566601
   Choi HK, 2010, INT J ADV MANUF TECH, V50, P235, DOI 10.1007/s00170-009-2484-y
   Cohen J., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P115, DOI 10.1145/280814.280832
   Do Carmo M., 1976, Differential Geometry of Curves and Surfaces
   Garland M., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P209, DOI 10.1145/258734.258849
   Garland M, 1998, VISUALIZATION '98, PROCEEDINGS, P263, DOI 10.1109/VISUAL.1998.745312
   González C, 2007, GRAPP 2007: PROCEEDINGS OF THE SECOND INTERNATIONAL CONFERENCE ON COMPUTER GRAPHICS THEORY AND APPLICATIONS, VOL GM/R, P69
   Gortler S. J., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P43, DOI 10.1145/237170.237200
   Hoppe H., 1999, Proceedings Visualization '99 (Cat. No.99CB37067), P59, DOI 10.1109/VISUAL.1999.809869
   Hoppe H., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P99, DOI 10.1145/237170.237216
   Kazhdan M., 2006, P 4 EUR S GEOM PROC, P61, DOI DOI 10.2312/SGP/SGP06/061-070
   Kim C, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461926
   Kim HS, 2008, LECT NOTES COMPUT SC, V4975, P258
   Levoy M., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P31, DOI 10.1145/237170.237199
   MacRobert T., 1948, SPHERICAL HARMONICS
   Malzbender T, 2001, COMP GRAPH, P519, DOI 10.1145/383259.383320
   Nishino K, 2001, IEEE T PATTERN ANAL, V23, P1257, DOI 10.1109/34.969116
   Ray N, 2010, COMPUT GRAPH FORUM, V29, P1489, DOI 10.1111/j.1467-8659.2010.01746.x
   Rusinkiewicz S. M., 1998, Rendering Techniques '98. Proceedings of the Eurographics Workshop, P11
   Schroder P., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P161, DOI 10.1145/218380.218439
   Vanhoey K, 2013, COMPUT GRAPH FORUM, V32, P101, DOI 10.1111/cgf.12073
   Wood DN, 2000, COMP GRAPH, P287, DOI 10.1145/344779.344925
   Wu HZ, 2011, COMPUT GRAPH FORUM, V30, P465, DOI 10.1111/j.1467-8659.2011.01890.x
NR 25
TC 2
Z9 2
U1 0
U2 6
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2015
VL 31
IS 6-8
BP 1011
EP 1021
DI 10.1007/s00371-015-1124-9
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA CM2CN
UT WOS:000357487500026
DA 2024-07-18
ER

PT J
AU Wong, SK
   Baciu, G
AF Wong, Sai-Keung
   Baciu, George
TI Continuous collision detection for deformable objects using permissible
   clusters
SO VISUAL COMPUTER
LA English
DT Article
DE Virtual reality; Continuous collision detection; Deformable objects;
   Triangle clusters
AB In this paper, we propose a new data structure to perform continuous collision detection (CCD) for deformable triangular meshes. The critical component of this data structure is permissible clusters. At the preprocessing phase, the triangular meshes are divided into permissible clusters. Then, the features of the triangular meshes are assigned to the permissible clusters. At the runtime phase, the potentially colliding feature pairs are collected and they are processed only once in the elementary processing. Our method has been integrated with a normal cone-based method and compared with other CCD methods. Experimental results show that our method improves the overall performance of CCD for deformable objects.
C1 [Wong, Sai-Keung] Natl Chiao Tung Univ, Hsinchu, Taiwan.
   [Baciu, George] Hong Kong Polytech Univ, Dept Comp, Kowloon, Hong Kong, Peoples R China.
   [Baciu, George] Hong Kong Polytech Univ, Graph & Multimedia Applicat GAMA Lab, Kowloon, Hong Kong, Peoples R China.
C3 National Yang Ming Chiao Tung University; Hong Kong Polytechnic
   University; Hong Kong Polytechnic University
RP Wong, SK (corresponding author), Natl Chiao Tung Univ, Hsinchu, Taiwan.
EM cswingo@cs.nctu.edu.tw; csgeorge@comp.polyu.edu.hk
RI Baciu, George/AAU-7143-2021
OI BACIU, George/0000-0002-1766-6357
FU National Science Council Taiwan [NSC 102-2221-E-009-103-MY2]; Hong Kong
   RGC GRF [PolyU 5101/11E, PolyU 5100/12E, PolyU 5100/13E]
FX We thank the reviewers for their constructive and invaluable comments.
   The animation data of Cloth and Balls were obtained from the UNC Gamma
   Group. This work was supported in part by the National Science Council
   Taiwan under contract number NSC 102-2221-E-009-103-MY2 and Hong Kong
   RGC GRF grants (PolyU 5101/11E, PolyU 5100/12E and PolyU 5100/13E).
CR [Anonymous], ACM T GR
   [Anonymous], ACM SIGGRAPH S INT 3
   [Anonymous], P ACM SIGGRAPH EUR S
   [Anonymous], J GR GPU GAME TOOLS
   Bridson R, 2002, ACM T GRAPHIC, V21, P594, DOI 10.1145/566570.566623
   Curtis S, 2008, I3D 2008: SYMPOSIUM ON INTERACTIVE 3D GRAPHICS AND GAMES, PROCEEDINGS, P61
   Gottschalk S., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P171, DOI 10.1145/237170.237244
   Hubbard P. M., 1993, Proceedings IEEE 1993 Symposium on Research Frontiers in Virtual Reality (Cat. No.93TH0585-0), P24, DOI 10.1109/VRAIS.1993.378267
   Hutter M., 2007, P WSCG, P35
   Kim D, 2009, COMPUT GRAPH FORUM, V28, P1791, DOI 10.1111/j.1467-8659.2009.01556.x
   Klosowski JT, 1998, IEEE T VIS COMPUT GR, V4, P21, DOI 10.1109/2945.675649
   Larsson T, 2006, COMPUT GRAPH-UK, V30, P450, DOI 10.1016/j.cag.2006.02.011
   Liu JD, 1996, VISUAL COMPUT, V12, P234
   Madera F., 2009, 2 INT C ADV COMP HUM, P136
   Madera FA, 2010, THIRD INTERNATIONAL CONFERENCE ON ADVANCES IN COMPUTER-HUMAN INTERACTIONS: ACHI 2010, P107, DOI 10.1109/ACHI.2010.11
   Mezger J, 2003, WSCG'2003, VOL 11, NO 2, CONFERENCE PROCEEDINGS, P322
   Provot X., 1997, GRAPHICS INTERFACE, P177
   Smith A., 1995, Proceedings. Virtual Reality Annual International Symposium '95 (Cat. No.95CH35761), P136, DOI 10.1109/VRAIS.1995.512489
   Tang M., 2010, P ACM SIGGRAPH S INT, P7, DOI [10.1145/1730804.1730806, DOI 10.1145/1730804.1730806]
   Tang M., 2009, 2009 SIAM/ACM Joint Conference on Geometric and Physical Modeling. SPM'09, P355
   Tang M., 2010, NONPENETRATION FILTE
   Tang M, 2008, SPM 2008: PROCEEDINGS OF THE ACM SOLID AND PHYSICAL MODELING SYMPOSIUM, P25
   Tang M, 2009, IEEE T VIS COMPUT GR, V15, P544, DOI 10.1109/TVCG.2009.12
   VOLINO P, 1994, COMPUT GRAPH FORUM, V13, pC155, DOI 10.1111/1467-8659.1330155
   Wong S.-K., 2010, P ACM S VIRT REAL SO
   Wong W. S. -K., 2006, P ACM VRCIA, P181
   Wong WSK, 2005, IEEE T VIS COMPUT GR, V11, P329, DOI 10.1109/TVCG.2005.44
   Zachmann G., 2006, P ACM INT C VIRT REA, P14
   Zhang JY, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2167076.2167079
NR 29
TC 7
Z9 8
U1 0
U2 1
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2015
VL 31
IS 4
BP 377
EP 389
DI 10.1007/s00371-014-0933-6
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CD2IW
UT WOS:000350901600002
DA 2024-07-18
ER

PT J
AU Long, J
   Porter, B
   Jones, M
AF Long, Jie
   Porter, Bryce
   Jones, Michael
TI Animation of trees in wind using sparse motion capture data
SO VISUAL COMPUTER
LA English
DT Article
DE Motion capture; Animation; Natural phenomena
ID REAL-TIME ANIMATION
AB We present a novel approach to animate an entire tree from the motion of a few branches. Animating medium sized trees in wind presents an arduous situation because it is difficult to create the motion of flexible branches in turbulent wind flows. The animation of trees in wind is important in biology and ecology where understanding how trees move can inform us about the cultivation of trees and forests designed to withstand strong winds. Animation of trees in wind is also useful in the production of films and games. We use passive optical motion capture to record the motion of retro-reflective markers placed on some of the tree branch tips. These motion data are processed to remove noise, create motion traces, and add missing data. Given the processed motion data, we solve for wind velocity in a discrete formulation of aerodynamic drag and tree branch dynamics. This sparse collection of wind velocity samples is interpolated over the volume of the crown and enriched with a turbulence model to create a wind field that drives the animation of an entire tree crown. The wind field can also be used to drive the animation of other similar trees. In this process, 300 KB of data recorded from 10 s of tree motion can be processed and replayed in 16 s of computation time, not including rendering.
C1 [Long, Jie; Porter, Bryce; Jones, Michael] Brigham Young Univ, Dept Comp Sci, Provo, UT 84604 USA.
C3 Brigham Young University
RP Jones, M (corresponding author), Brigham Young Univ, Dept Comp Sci, Provo, UT 84604 USA.
EM mike.jones@byu.edu
CR Akagi Y, 2006, COMPUT GRAPH-UK, V30, P529, DOI 10.1016/j.cag.2006.03.017
   Barbic J, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964986
   Bergou M, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360662
   Bertails F, 2009, COMPUT GRAPH FORUM, V28, P417, DOI 10.1111/j.1467-8659.2009.01381.x
   Diener J, 2009, COMPUT GRAPH FORUM, V28, P533, DOI 10.1111/j.1467-8659.2009.01393.x
   Diener Julien., 2006, Proceedings of the Eurographics Symposium on Computer Animation, P187
   Ganz Melanie, 2009, 2009 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops (CVPR Workshops), P87, DOI 10.1109/CVPR.2009.5204055
   Habel R, 2009, COMPUT GRAPH FORUM, V28, P523, DOI 10.1111/j.1467-8659.2009.01391.x
   Hu SJ, 2012, VISUAL COMPUT, V28, P859, DOI 10.1007/s00371-012-0694-z
   James KR, 2006, AM J BOT, V93, P1522, DOI 10.3732/ajb.93.10.1522
   Kwatra N, 2010, IEEE T VIS COMPUT GR, V16, P70, DOI 10.1109/TVCG.2009.66
   Li C, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024161
   Long J., 2012, 2012 IEEE 4TH INTERN
   Long J, 2010, LECT NOTES COMPUT SC, V6459, P158
   Neubert B., 2007, SIGGRAPH 07 ACMSIG G
   Ota S, 2004, VISUAL COMPUT, V20, P613, DOI 10.1007/s00371-004-0266-y
   Ota S, 2003, COMPUTER GRAPHICS INTERNATIONAL, PROCEEDINGS, P52, DOI 10.1109/CGI.2003.1214447
   Pfaff T., 2010, ACM SIGGRAPH AS 2010
   Pope S.B., 2000, TURBULENT FLOWS
   Runions A., 2007, NPH, P63
   Selino A., 2012, COMPUT GRAPH FORUM, V24, P417
   Shinya M., 1992, EUROGRAPHICS 92, V11, P119
   Simiu E., 1996, Wind effects on structures: fundamentals and applications to design, V1
   Sun M, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P96
   Tan P., 2007, ACM SIGGRAPH 2007 SI
   Wesslén D, 2005, VISUAL COMPUT, V21, P397, DOI 10.1007/s00371-005-0295-1
NR 26
TC 3
Z9 3
U1 0
U2 9
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2015
VL 31
IS 3
BP 325
EP 339
DI 10.1007/s00371-014-0927-4
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CC3DB
UT WOS:000350223900007
DA 2024-07-18
ER

PT J
AU Thielhelm, H
   Vais, A
   Wolter, FE
AF Thielhelm, Hannes
   Vais, Alexander
   Wolter, Franz-Erich
TI Geodesic bifurcation on smooth surfaces
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT New Advances in Shape Analysis and Geometric Modeling Workshop (NASAGEM)
   at CGI Conference
CY JUN 11-14, 2013
CL Hannover, GERMANY
DE Geodesic exponential map; Focal curves; Connecting geodesics; Distance
   computation; Cut locus; Voronoi diagram
ID CUT LOCI
AB Within Riemannian geometry the geodesic exponential map is an essential tool for various distance-related investigations and computations. Several natural questions can be formulated in terms of its preimages, usually leading to quite challenging non-linear problems. In this context we recently proposed an approach for computing multiple geodesics connecting two arbitrary points on two-dimensional surfaces in situations where an ambiguity of these connecting geodesics is indicated by the presence of focal curves. The essence of the approach consists in exploiting the structure of the associated focal curve and using a suitable curve for a homotopy algorithm to collect the geodesic connections. In this follow-up discussion we extend those constructions to overcome a significant limitation inherent in the previous method, i.e. the necessity to construct homotopy curves artificially. We show that considering homotopy curves meeting a focal curve tangentially leads to a singularity that we investigate thoroughly. Solving this so-called geodesic bifurcation analytically and dealing with it numerically provides not only theoretical insights, but also allows geodesics to be used as homotopy curves. This yields a stable computational tool in the context of computing distances. This is applicable in common situations where there is a curvature induced non-injectivity of the exponential map. In particular we illustrate how applying geodesic bifurcation approaches the distance problem on compact manifolds with a single closed focal curve. Furthermore, the presented investigations provide natural initial values for computing cut loci using the medial differential equation which directly leads to a discussion on avoiding redundant computations by combining the presented concepts to determine branching points.
C1 [Thielhelm, Hannes; Vais, Alexander; Wolter, Franz-Erich] Leibniz Univ Hannover, Welfenlab, Hannover, Germany.
C3 Leibniz University Hannover
RP Thielhelm, H (corresponding author), Leibniz Univ Hannover, Welfenlab, Hannover, Germany.
EM thielhel@welfenlab.de; vais@welfenlab.de
RI Wolter, Franz-Erich/JAC-5956-2023; Wolter, Franz-Erich/AAV-3008-2020
OI Wolter, Franz-Erich/0000-0002-2293-5494; Wolter,
   Franz-Erich/0000-0002-2293-5494
FU Deutsche Forschungsgemeinschaft (DFG) [Graduiertenkolleg 615]
FX This research was partially supported by a Deutsche
   Forschungsgemeinschaft (DFG) Grant within the Graduiertenkolleg 615.
CR Abraham R., 1978, Foundations of Mechanics, V2nd
   [Anonymous], 1990, Numerical Continuation Methods
   [Anonymous], 1996, RIEMANNIAN GEOMETRY
   De Berg M., 2008, Computational Geometry: Algorithms and Applications, V17
   Dey TK, 2009, PROCEEDINGS OF THE TWENTY-FIFTH ANNUAL SYMPOSIUM ON COMPUTATIONAL GEOMETRY (SCG'09), P125, DOI 10.1145/1542362.1542390
   Dey TK, 2004, ALGORITHMICA, V38, P179, DOI 10.1007/s00453-003-1049-y
   GARCIA C., 1981, PATHWAYS SOLUTIONS F
   Itoh J, 2004, EXP MATH, V13, P309, DOI 10.1080/10586458.2004.10504543
   Itoh J, 2004, MANUSCRIPTA MATH, V114, P247, DOI 10.1007/s00229-004-0455-z
   Kimmel R, 1998, P NATL ACAD SCI USA, V95, P8431, DOI 10.1073/pnas.95.15.8431
   Kunze R, 1997, COMPUTER GRAPHICS INTERNATIONAL, PROCEEDINGS, P230, DOI 10.1109/CGI.1997.601311
   Landau LD, 2003, MECHANICS
   Leibon G., 2000, P 16 ANN S COMP GEOM, V5, P341, DOI [10.1145/336154.336221, DOI 10.1145/336154.336221]
   Liu YJ, 2013, COMPUT AIDED DESIGN, V45, P695, DOI 10.1016/j.cad.2012.11.005
   Liu YJ, 2013, INFORM PROCESS LETT, V113, P132, DOI 10.1016/j.ipl.2012.12.010
   Misztal M. K., 2011, Proceedings of the 2011 Eighth International Symposium on Voronoi Diagrams in Science and Engineering (ISVD 2011), P134, DOI 10.1109/ISVD.2011.26
   Myers S. B., 1936, DUKE MATH J, V2, P95, DOI DOI 10.1215/S0012-7094-36-00208-9
   Myers S. B., 1935, DUKE MATH J, V1, P376, DOI [10.1215/S0012-7094-35-00126-0, DOI 10.1215/S0012-7094-35-00126-0]
   Nass H., 2007, THESIS LUH
   Nass H, 2007, 2007 INTERNATIONAL CONFERENCE ON CYBERWORLDS, PROCEEDINGS, P386, DOI 10.1109/CW.2007.55
   Neel R., 2004, SURVEYS DIFFERENTIAL, V9, P337
   Onishi K., 2003, PROC 15 CANADIAN C C, P19
   Patrikalakis N., 2002, Shape Interrogation for Computer Aided Design and Manufacturing
   Polthier K., 2006, Straightest geodesics on polyhedral surfaces
   Rausch T., 1996, C MATH SURFACES, P43
   RAUSCH T, 1999, THESIS LEIBNIZ U HAN
   Savage Leonard J., 1943, Bull. Amer. Math. Soc., V49, P467, DOI DOI 10.1090/S0002-9904-1943-07960-8,9101
   Sinclair R, 2002, EXP MATH, V11, P1
   Sinclair R, 2006, MATH COMPUT, V75, P1779, DOI 10.1090/S0025-5718-06-01924-7
   Stam J., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P395, DOI 10.1145/280814.280945
   Surazhsky V, 2005, ACM T GRAPHIC, V24, P553, DOI 10.1145/1073204.1073228
   Thielhelm H., 2012, VISUAL COMPUT, P1
   Wolter F., 1992, 922 MIT DES LAB
   WOLTER FE, 1979, ARCH MATH, V32, P92, DOI 10.1007/BF01238473
   Wolter FE, 2000, COMPUTER GRAPHICS INTERNATIONAL 2000, PROCEEDINGS, P137, DOI 10.1109/CGI.2000.852329
   WOLTER FE, 1985, THESIS TU BERLIN
   Wolter FE, 2011, LECT NOTES APPL COMP, V57, P211
   [No title captured]
NR 38
TC 9
Z9 9
U1 1
U2 8
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2015
VL 31
IS 2
BP 187
EP 204
DI 10.1007/s00371-014-1041-3
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA AZ6EG
UT WOS:000348310800008
DA 2024-07-18
ER

PT J
AU Berretti, S
   del Bimbo, A
   Pala, P
AF Berretti, Stefano
   del Bimbo, Alberto
   Pala, Pietro
TI Automatic facial expression recognition in real-time from dynamic
   sequences of 3D face scans
SO VISUAL COMPUTER
LA English
DT Article
DE 3D dynamic sequences; 3D facial expression recognition; Hidden Markov
   model; Local descriptor
ID OBJECT RECOGNITION; IMAGES
AB In this paper, we present a fully-automatic and real-time approach for person-independent recognition of facial expressions from dynamic sequences of 3D face scans. In the proposed solution, first a set of 3D facial landmarks are automatically detected, then the local characteristics of the face in the neighborhoods of the facial landmarks and their mutual distances are used to model the facial deformation. Training two hidden Markov models for each facial expression to be recognized, and combining them to form a multiclass classifier, an average recognition rate of 79.4 % has been obtained for the 3D dynamic sequences showing the six prototypical facial expressions of the Binghamton University 4D Facial Expression database. Comparisons with competitor approaches on the same database show that our solution is able to obtain effective results with the advantage of being capable to process facial sequences in real-time.
C1 [Berretti, Stefano; del Bimbo, Alberto; Pala, Pietro] Univ Florence, Dipartimento Ingn Informaz, Florence, Italy.
C3 University of Florence
RP Berretti, S (corresponding author), Univ Florence, Dipartimento Ingn Informaz, Florence, Italy.
EM stefano.berretti@unifi.it; alberto.delbimbo@unifi.it;
   pietro.pala@unifi.it
RI Berretti, Stefano/U-9004-2019
OI Berretti, Stefano/0000-0003-1219-4386; PALA, PIETRO/0000-0001-5670-3774;
   DEL BIMBO, ALBERTO/0000-0002-1052-8322
CR [Anonymous], P 1 COST 2101 WORKSH
   [Anonymous], ARXIV12021444V2
   [Anonymous], P IEEE INT C COMP VI
   [Anonymous], EU WORKSH 3D OBJ RET
   [Anonymous], FACIAL EXPRESSION RE
   [Anonymous], P EUR C COMP VIS HER
   [Anonymous], MPEG 4 FACIAL ANIMAT
   [Anonymous], 1987, ANTHROPOMETRIC FACIA
   Belongie S, 2002, IEEE T PATTERN ANAL, V24, P509, DOI 10.1109/34.993558
   Benedikt L, 2010, IEEE T SYST MAN CY A, V40, P449, DOI 10.1109/TSMCA.2010.2041656
   Berretti S, 2012, LECT NOTES COMPUT SC, V7583, P73, DOI 10.1007/978-3-642-33863-2_8
   Berretti S, 2011, VISUAL COMPUT, V27, P1021, DOI 10.1007/s00371-011-0611-x
   Berretti S, 2010, IEEE T PATTERN ANAL, V32, P2162, DOI 10.1109/TPAMI.2010.43
   Cootes TF, 2001, IEEE T PATTERN ANAL, V23, P681, DOI 10.1109/34.927467
   Creusot C., 2011, 2011 International Conference on 3D Imaging, Modeling, Processing, Visualization and Transmission (3DIMPVT), P204, DOI 10.1109/3DIMPVT.2011.33
   Drira H, 2012, INT C PATT RECOG, P1104
   Ekman P., 1971, Nebraska symposium on motivation, V19, P207
   Ekman P, 1978, FACIAL ACTION CODING
   Fang TH, 2012, IMAGE VISION COMPUT, V30, P738, DOI 10.1016/j.imavis.2012.02.004
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Frome A, 2004, LECT NOTES COMPUT SC, V3023, P224
   Gupta S, 2010, INT J COMPUT VISION, V90, P331, DOI 10.1007/s11263-010-0360-8
   Hao Tang, 2008, 2008 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops (CVPR Workshops), P1, DOI 10.1109/CVPRW.2008.4563052
   Johnson AE, 1999, IEEE T PATTERN ANAL, V21, P433, DOI 10.1109/34.765655
   Kakadiaris IA, 2007, IEEE T PATTERN ANAL, V29, P640, DOI 10.1109/TPAMI.2007.1017
   Li BYL, 2013, IEEE WORK APP COMP, P186, DOI 10.1109/WACV.2013.6475017
   LINDE Y, 1980, IEEE T COMMUN, V28, P84, DOI 10.1109/TCOM.1980.1094577
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Maalej A, 2011, PATTERN RECOGN, V44, P1581, DOI 10.1016/j.patcog.2011.02.012
   Matuszewski B. J., 2011, 2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops), P2128, DOI 10.1109/ICCVW.2011.6130511
   Matuszewski BJ, 2012, IMAGE VISION COMPUT, V30, P713, DOI 10.1016/j.imavis.2012.02.002
   MEHRABIAN A, 1967, J PERS SOC PSYCHOL, V6, P109, DOI 10.1037/h0024532
   Mian AS, 2008, INT J COMPUT VISION, V79, P1, DOI 10.1007/s11263-007-0085-5
   RABINER LR, 1989, P IEEE, V77, P257, DOI 10.1109/5.18626
   RODRIGUEZ JJ, 1990, IEEE T PATTERN ANAL, V12, P1138, DOI 10.1109/34.62603
   Rusinkiewicz S, 2001, THIRD INTERNATIONAL CONFERENCE ON 3-D DIGITAL IMAGING AND MODELING, PROCEEDINGS, P145, DOI 10.1109/IM.2001.924423
   Samir C, 2009, INT J COMPUT VISION, V82, P80, DOI 10.1007/s11263-008-0187-8
   Sandbach G., 2011, Proceedings 2011 IEEE International Conference on Automatic Face & Gesture Recognition (FG 2011), P406, DOI 10.1109/FG.2011.5771434
   Sandbach G, 2012, IMAGE VISION COMPUT, V30, P683, DOI 10.1016/j.imavis.2012.06.005
   Sandbach G, 2012, IMAGE VISION COMPUT, V30, P762, DOI 10.1016/j.imavis.2012.01.006
   Schneiderman H, 2004, PROC CVPR IEEE, P639
   Schneiderman H, 2004, PROC CVPR IEEE, P29
   Seol Y, 2012, VISUAL COMPUT, V28, P319, DOI 10.1007/s00371-011-0641-4
   Sun Y, 2008, LECT NOTES COMPUT SC, V5303, P58, DOI 10.1007/978-3-540-88688-4_5
   Sun Y, 2010, IEEE T SYST MAN CY A, V40, P461, DOI 10.1109/TSMCA.2010.2041659
   Tsalakanidou F, 2010, PATTERN RECOGN, V43, P1763, DOI 10.1016/j.patcog.2009.12.009
   Vuong Le, 2011, Proceedings 2011 IEEE International Conference on Automatic Face & Gesture Recognition (FG 2011), P414, DOI 10.1109/FG.2011.5771435
   Wang YM, 2010, IEEE T PATTERN ANAL, V32, P1858, DOI 10.1109/TPAMI.2009.200
   Yin L., 2008, PROC IEEE INT C AUTO, P1
   Yin LJ, 2006, PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION - PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE, P211
   Zafeiriou S, 2012, IMAGE VISION COMPUT, V30, P681, DOI 10.1016/j.imavis.2012.09.001
   Zaharescu A, 2009, PROC CVPR IEEE, P373, DOI 10.1109/CVPRW.2009.5206748
   Zeng ZH, 2009, IEEE T PATTERN ANAL, V31, P39, DOI 10.1109/TPAMI.2008.52
   Zhao GY, 2007, IEEE T PATTERN ANAL, V29, P915, DOI 10.1109/TPAMI.2007.1110
   Zhao X., 2010, HICSS, P1
NR 55
TC 36
Z9 38
U1 1
U2 18
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2013
VL 29
IS 12
BP 1333
EP 1350
DI 10.1007/s00371-013-0869-2
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 251LB
UT WOS:000326929200009
DA 2024-07-18
ER

PT J
AU Lan, YX
   Wang, JP
   Lin, S
   Gong, MM
   Tong, X
   Guo, BN
AF Lan, Yanxiang
   Wang, Jiaping
   Lin, Stephen
   Gong, Minmin
   Tong, Xin
   Guo, Baining
TI Interactive chromaticity mapping for multispectral images
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Computer Graphics International (CGI) Conference
CY 2013
CL Hanover, GERMANY
DE Multispectral imaging; Chromaticity mapping; Edit propagation
ID PROPAGATION
AB Multispectral images record detailed color spectra at each image pixel. To display a multispectral image on conventional output devices, a chromaticity mapping function is needed to map the spectral vector of each pixel to the displayable three dimensional color space. In this paper, we present an interactive method for locally adjusting the chromaticity mapping of a multispectral image. The user specifies edits to the chromaticity mapping via a sparse set of strokes at selected image locations and wavelengths, then our method automatically propagates the edits to the rest of the multispectral image. The key idea of our approach is to factorize the multispectral image into a component that indicates spatial coherence between different pixels, and one that describes spectral coherence between different wavelengths. Based on this factorized representation, a two-step algorithm is developed to efficiently propagate the edits in the spatial and spectral domains separately. The method presented provides photographers with efficient control over color appearance and scene details in a manner not possible with conventional color image editing. We demonstrate the use of interactive chromaticity mapping in the applications of color stylization to emulate the appearance of photographic films, enhancement of image details, and manipulation of different light transport effects.
C1 [Lan, Yanxiang; Guo, Baining] Tsinghua Univ, Beijing 100084, Peoples R China.
   [Lin, Stephen; Tong, Xin] Microsoft Res Asia, Internet Graph Grp, Beijing, Peoples R China.
   [Guo, Baining] Microsoft Corp, Redmond, WA 98052 USA.
C3 Tsinghua University; Microsoft; Microsoft Research Asia; Microsoft
RP Lan, YX (corresponding author), Tsinghua Univ, Beijing 100084, Peoples R China.
EM layx03@gmail.com; jiapw@microsoft.com; stevelin@microsoft.com;
   mgong@microsoft.com; xtong@microsoft.com; bainguo@microsoft.com
OI Tong, Xin/0000-0001-8788-2453
CR *ALIEN SKIN SOFTW, EXPOSURE4
   An XB, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360639
   [Anonymous], 2000, WILEY SERIES PURE AP
   CAO X, 2009, P COMP VIS PATT REC
   Carroll Joseph, 2002, J Vis, V2, P531, DOI 10.1167/2.8.1
   Debevec P. E., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P369, DOI 10.1145/258734.258884
   DESCOUR M, 1995, APPL OPTICS, V34, P4817, DOI 10.1364/AO.34.004817
   Du H, 2009, IEEE I CONF COMP VIS, P175, DOI 10.1109/ICCV.2009.5459162
   Farbman Z, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866171
   GALATSANOS N, 2005, ENCY OPTICAL ENG
   Gat N, 2000, P SOC PHOTO-OPT INS, V4056, P50, DOI 10.1117/12.381686
   Gehm ME, 2007, OPT EXPRESS, V15, P14013, DOI 10.1364/OE.15.014013
   Jolliffe L., 2002, Principal Component Analysis
   Levin A, 2004, ACM T GRAPHIC, V23, P689, DOI 10.1145/1015706.1015780
   Li Y, 2010, COMPUT GRAPH FORUM, V29, P2049, DOI 10.1111/j.1467-8659.2010.01791.x
   Lischinski D, 2006, ACM T GRAPHIC, V25, P646, DOI 10.1145/1141911.1141936
   Mohan A, 2008, COMPUT GRAPH FORUM, V27, P709, DOI 10.1111/j.1467-8659.2008.01169.x
   Mooney JM, 1997, J OPT SOC AM A, V14, P2951, DOI 10.1364/JOSAA.14.002951
   Murawski R, 2011, IEEE ICC
   Ng R, 2003, ACM T GRAPHIC, V22, P376, DOI 10.1145/882262.882280
   PARK J, 2007, P INT C COMP VIS ICC
   PEERS P, 2009, ACM T GRAPH, V28
   Pellacini F, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1243980.1243983, 10.1145/1276377.1276444, 10.1145/1239451.1239505]
   Schechner YY, 2002, IEEE T PATTERN ANAL, V24, P1334, DOI 10.1109/TPAMI.2002.1039205
   VANDERVLUGT C, 2007, P SPIE, V6565
   Wagadarikar A, 2008, APPL OPTICS, V47, pB44, DOI 10.1364/AO.47.000B44
   Wandell B. A., 2003, SCI COLOR
   Wang JM, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618514
   Xu K, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618464
   Xu K, 2009, COMPUT GRAPH FORUM, V28, P1871, DOI 10.1111/j.1467-8659.2009.01565.x
   Yamaguchi M., 2006, SPIE IS T ELECT IMAG, V6062
   Yasuma F., 2008, GEN ASSORTED PIXEL C
NR 32
TC 1
Z9 1
U1 0
U2 9
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2013
VL 29
IS 6-8
BP 773
EP 783
DI 10.1007/s00371-013-0829-x
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 151RR
UT WOS:000319478400030
DA 2024-07-18
ER

PT J
AU Du, H
   Jin, XG
AF Du, Hui
   Jin, Xiaogang
TI Object cloning using constrained mean value interpolation
SO VISUAL COMPUTER
LA English
DT Article
DE Object cloning; Mean value coordinates; Alpha matting; Laplace equation
AB We present an object-cloning algorithm using constrained mean value interpolation over images. Different from previous methods, we first define the guidance vector field as a weighted gradient of the source and destination images in the gradient domain, which produces a similar Laplace equation to that of the original Poisson method but with different boundary conditions. Then, we use mean value coordinates to solve the new Laplace equation approximately instead of solving a large linear system. The cloned result matches well with the destination image on the luma and preserves the chroma of the source object as much as possible. Our cloned results are visually pleasing without smudging and discoloration artifacts. We also extend our approach to video object cloning. Experimental results demonstrate the effectiveness of our object cloning algorithm.
C1 [Du, Hui; Jin, Xiaogang] Zhejiang Univ, State Key Lab CAD&CG, Hangzhou 310003, Zhejiang, Peoples R China.
   [Du, Hui] Zhejiang Univ Media & Commun, Hangzhou 310018, Peoples R China.
C3 Zhejiang University; Communication University of Zhejiang
RP Jin, XG (corresponding author), Zhejiang Univ, State Key Lab CAD&CG, Hangzhou 310003, Zhejiang, Peoples R China.
EM duhui@cad.zju.edu.cn; jin@cad.zju.edu.cn
FU Zhejiang Provincial Natural Science Foundation of China [Z1110154];
   NSFC-MSRA [60970159]; National Natural Science Foundation of China
   [60933007]; Zhejiang Provincial Science and Technology Planning Project
   [2010C31090]; Scientific Research Fund of Zhejiang Provincial Education
   Department [Y201016221]
FX Xiaogang Jin was supported by Zhejiang Provincial Natural Science
   Foundation of China (Grant No. Z1110154), NSFC-MSRA Joint Funding (Grant
   No. 60970159), and the National Natural Science Foundation of China
   (Grant No. 60933007). Hui Du was supported by Zhejiang Provincial
   Science and Technology Planning Project (Grant No. 2010C31090) and
   Scientific Research Fund of Zhejiang Provincial Education Department
   (Grant No. Y201016221).
CR Agarwala A, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1239451.1239545, 10.1145/1276377.1276495]
   [Anonymous], 2010, EUR C COMP VIS
   [Anonymous], 13 INT C COMP AN IM
   Bai X, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531376
   Bai Xue, 2007, INT C COMP VIS
   Bolz J, 2003, ACM T GRAPHIC, V22, P917, DOI 10.1145/882262.882364
   Bratkova M, 2009, IEEE COMPUT GRAPH, V29, P42, DOI 10.1109/MCG.2009.13
   C. CORPORATION, 2002, KNOCK US GUID
   Chen T, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618470
   Chuang YY, 2001, PROC CVPR IEEE, P264
   Ding M, 2010, VISUAL COMPUT, V26, P721, DOI 10.1007/s00371-010-0448-8
   Farbman Z, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531373
   Floater MS, 2005, COMPUT AIDED GEOM D, V22, P623, DOI 10.1016/j.cagd.2005.06.004
   Floater MS, 2003, COMPUT AIDED GEOM D, V20, P19, DOI 10.1016/S0167-8396(03)00002-5
   Gastal ESL, 2010, COMPUT GRAPH FORUM, V29, P575, DOI 10.1111/j.1467-8659.2009.01627.x
   Grady L, 2005, PROCEEDINGS OF THE FIFTH IASTED INTERNATIONAL CONFERENCE ON VISUALIZATION, IMAGING, AND IMAGE PROCESSING, P423
   Guan Y, 2006, COMPUT GRAPH FORUM, V25, P567, DOI 10.1111/j.1467-8659.2006.00976.x
   He KM, 2010, PROC CVPR IEEE, P2165, DOI 10.1109/CVPR.2010.5539896
   Hormann K, 2008, COMPUT GRAPH FORUM, V27, P1513, DOI 10.1111/j.1467-8659.2008.01292.x
   Jeschke S., 2009, ACM T GRAPH, V28
   Jia JY, 2006, ACM T GRAPHIC, V25, P631, DOI 10.1145/1141911.1141934
   Ju T, 2005, ACM T GRAPHIC, V24, P561, DOI 10.1145/1073204.1073229
   Kazhdan M, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360620
   Lalonde JF, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1276377.1276381, 10.1145/1239451.1239454]
   Lee S.-Y., 2009, ACM SIGGRAPH AS
   Levin A., 2007, SPECTRAL MATTING
   Levin A., 2006, P IEEE CVPR
   Levin A, 2008, IEEE T PATTERN ANAL, V30, P228, DOI 10.1109/TPAMI.2007.1177
   Manson J, 2010, COMPUT GRAPH FORUM, V29, P1517, DOI 10.1111/j.1467-8659.2010.01760.x
   McCann J, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360692
   Mishima Y., 1994, US Patent, Patent No. [5,355,174, 5355174]
   Pérez P, 2003, ACM T GRAPHIC, V22, P313, DOI 10.1145/882262.882269
   Rhemann C, 2010, PROC CVPR IEEE, P2149, DOI 10.1109/CVPR.2010.5539894
   Ruzon MA, 2000, PROC CVPR IEEE, P18, DOI 10.1109/CVPR.2000.855793
   Sun J., 2004, ACM T GRAPH, V23
   Sunkavalli K, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778862
   Szeliski R, 2006, ACM T GRAPHIC, V25, P1135, DOI 10.1145/1141911.1142005
   Wang HC, 2004, INT C PATT RECOG, P858, DOI 10.1109/ICPR.2004.1334663
   Wang J., 2007, 2007 IEEE C COMP VIS, P1
   Wang R, 2010, J ZHEJIANG U-SCI C, V11, P690, DOI 10.1631/jzus.C1000067
   Xie ZF, 2010, VISUAL COMPUT, V26, P1123, DOI 10.1007/s00371-010-0466-6
   Zhang Y, 2011, VISUAL COMPUT, V27, P739, DOI 10.1007/s00371-011-0583-x
   Zheng Y.-J., 2009, INT C COMP VIS
NR 43
TC 6
Z9 7
U1 0
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2013
VL 29
IS 3
BP 217
EP 229
DI 10.1007/s00371-012-0722-z
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 115AI
UT WOS:000316783800005
DA 2024-07-18
ER

PT J
AU Sarlabous, JE
   Mederos, VH
   Morera, DM
   Velho, L
   Gil, NL
AF Estrada Sarlabous, Jorge
   Hernandez Mederos, Victoria
   Morera, Dimas Martinez
   Velho, Luiz
   Lopez Gil, Nayla
TI Conic-like subdivision curves on surfaces
SO VISUAL COMPUTER
LA English
DT Article
DE Curves on surfaces; Subdivision curve; Geodesic; Conic
AB In this paper, we introduce a novel nonlinear curve subdivision scheme, suitable for designing curves on surfaces. The scheme is based on the concept of geodesic conic B,zier curves, which represents a natural extension of geodesic B,zier curves for the rational quadratic case. Given a set of points on a surface S, the scheme generates a sequence of geodesic polygons that converges to a continuous curve on S. If the surface S is C (2)-continuous, then the subdivision curve is C (1)-continuous and if S is a plane, then the limit curve is a conic B,zier spline curve. Each section of the subdivision curve depends on a free parameter that may be used to obtain a local control of the shape of the subdivision curve. Extending these results to triangulated surfaces, it is shown that the scheme has the convex hull property and that it is suitable for free-form design on triangulations.
C1 [Morera, Dimas Martinez] Univ Fed Alagoas, Maceio, Brazil.
   [Estrada Sarlabous, Jorge; Hernandez Mederos, Victoria; Lopez Gil, Nayla] ICIMAF, Havana, Cuba.
   [Velho, Luiz] IMPA, Rio De Janeiro, Brazil.
C3 Universidade Federal de Alagoas; Instituto Nacional de Matematica Pura e
   Aplicada (IMPA)
RP Morera, DM (corresponding author), Univ Fed Alagoas, Maceio, Brazil.
EM jestrada@icmf.inf.cu; vicky@icmf.inf.cu; dimas@mat.ufal.br;
   lvelho@impa.br; nayla@icmf.inf.cu
RI Namitec, Inct/I-2416-2013; Mat, Inct/K-2187-2013
FU TWAS-UNESCO-CNPq [3240173676]; Visgraf-IMPA Brazil [3240173677]
FX The first two authors has been supported by TWAS-UNESCO-CNPq and
   Visgraf-IMPA Brazil in the frame of the TWAS-UNESCO/CNPq-Brazil
   Associateship Ref. 3240173676 and Ref. 3240173677, respectively. We
   thank to the anonymous referees for their valuable criticisms and
   suggestions.
CR Bonneau GP, 2004, MATH VISUAL, P69
   Crouch P., 1999, Journal of Dynamical and Control Systems, V5, P397, DOI 10.1023/A:1021770717822
   Dyn N., 1992, Advances in Numerical Analysis, VII, P36
   Dyn N, 2010, J COMPUT APPL MATH, V233, P1697, DOI 10.1016/j.cam.2009.02.017
   FARIN G, 1989, ACM T GRAPHIC, V8, P89, DOI 10.1145/62054.62056
   Farin G., 1983, COMPUT AIDED DESIGN, V15, P277
   Farin G., 2001, Curves and Surfaces for CAGD: A Practical Guide, Vfifth
   Hofer M, 2004, ACM T GRAPHIC, V23, P284, DOI 10.1145/1015706.1015716
   Kapoor S., 1999, Proceedings of the Thirty-First Annual ACM Symposium on Theory of Computing, P770, DOI 10.1145/301250.301449
   Kasap E, 2005, APPL MATH COMPUT, V171, P1206, DOI 10.1016/j.amc.2005.01.109
   KIMMEL R, 1995, COMPUT MATH APPL, V29, P49, DOI 10.1016/0898-1221(94)00228-D
   Kimmel R, 1998, P NATL ACAD SCI USA, V95, P8431, DOI 10.1073/pnas.95.15.8431
   Martínez D, 2005, COMPUT GRAPH-UK, V29, P667, DOI 10.1016/j.cag.2005.08.003
   Morera DM, 2008, VISUAL COMPUT, V24, P1025, DOI 10.1007/s00371-008-0298-9
   Morera DM, 2008, LECT NOTES COMPUT SC, V5197, P405, DOI 10.1007/978-3-540-85920-8_50
   PATTERSON RR, 1985, ACM T GRAPHIC, V4, P276, DOI 10.1145/6116.6119
   Rodrigues R.C., 2005, LMS J. Comput. Math., V8, P251
   Sethian JA, 1996, P NATL ACAD SCI USA, V93, P1591, DOI 10.1073/pnas.93.4.1591
   Wallner J, 2005, COMPUT AIDED GEOM D, V22, P593, DOI 10.1016/j.cagd.2005.06.003
   Wallner J, 2006, ACM T GRAPHIC, V25, P356, DOI 10.1145/1138450.1138459
NR 20
TC 2
Z9 4
U1 1
U2 2
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2012
VL 28
IS 10
SI SI
BP 971
EP 982
DI 10.1007/s00371-012-0728-6
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 003XE
UT WOS:000308643900003
DA 2024-07-18
ER

PT J
AU Takouachet, N
   Couture, N
   Reuter, P
   Joyot, P
   Rivière, G
   Verdon, N
AF Takouachet, Nawel
   Couture, Nadine
   Reuter, Patrick
   Joyot, Pierre
   Riviere, Guillaume
   Verdon, Nicolas
TI Tangible user interfaces for physically-based deformation: design
   principles and first prototype
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Conference on the Computer Graphics International (CGI)
CY 2012
CL Bournemouth, ENGLAND
DE Tangible interface; Two-handed interaction; Physically-based
   deformation; ShapeTape
AB We present design principles for conceiving tangible user interfaces for the interactive physically-based deformation of 3D models. Based on these design principles, we developed a first prototype using a passive tangible user interface that embodies the 3D model. By associating an arbitrary reference material with the user interface, we convert the displacements of the user interface into forces required by physically-based deformation models. These forces are then applied to the 3D model made out of any material via a physical deformation model. In this way, we compensate for the absence of direct haptic feedback, which allows us to use a force-driven physically-based deformation model. A user study on simple deformations of various metal beams shows that our prototype is usable for deformation with the user interface embodying the virtual beam. Our first results validate our design principles, plus they have a high educational value for mechanical engineering lectures.
C1 [Takouachet, Nawel; Couture, Nadine; Reuter, Patrick; Joyot, Pierre; Riviere, Guillaume; Verdon, Nicolas] ESTIA, F-64210 Bidart, France.
   [Couture, Nadine; Reuter, Patrick] LaBRI, UMR 5800, F-33405 Talence, France.
   [Reuter, Patrick] INRIA Bordeaux, F-33405 Talence, France.
   [Reuter, Patrick] Univ Bordeaux Segalen, F-33076 Bordeaux, France.
C3 Universite de Bordeaux; Centre National de la Recherche Scientifique
   (CNRS); Universite de Bordeaux
RP Takouachet, N (corresponding author), ESTIA, F-64210 Bidart, France.
EM n.takouachet@estia.fr; n.couture@estia.fr; preuter@labri.fr;
   p.joyot@estia.fr; g.riviere@estia.fr; n.verdon@estia.fr
RI Rivière, Guillaume/AAC-4228-2019; Joyot, Pierre/AAF-4403-2020
OI Rivière, Guillaume/0000-0001-8390-9751; Joyot,
   Pierre/0000-0002-6608-7343; Reuter, Patrick/0009-0006-9448-9725;
   Couture, Nadine/0000-0001-7959-5227
CR Balakrishnan R., 1999, 99 UIST. Proceedings of the 12th Annual ACM Symposium on User Interface Software and Technology, P171, DOI 10.1145/320719.322599
   Blanco FR, 2008, I3D 2008: SYMPOSIUM ON INTERACTIVE 3D GRAPHICS AND GAMES, PROCEEDINGS, P71
   Blanding Robert., 2007, Computer-Aided Design Applications, V4, P595
   Fishkin KP, 2004, PERS UBIQUIT COMPUT, V8, P347, DOI 10.1007/s00779-004-0297-4
   Fröhlich S, 2011, COMPUT GRAPH FORUM, V30, P2246, DOI 10.1111/j.1467-8659.2011.01974.x
   Huang H., 2011, PROC 2011 ACM SIGGRA, P73, DOI DOI 10.1145/2019406.2019416
   Ishii H., 1997, P ACM SIGCHI C HUM F, P234, DOI DOI 10.1145/258549.258715
   Ji Bian, 2011, 2011 3rd International Conference on Multimedia Information Networking and Security, P60, DOI 10.1109/MINES.2011.84
   Koleva B., 2003, PROC MOBILE HCI 03, P257
   Lee CHJ, 2006, INT J ARCHIT COMPUT, V4, P19, DOI 10.1260/147807706777008939
   Llamas I., 2005, P 2005 ACM S SOL PHY, P89, DOI DOI 10.1145/1060244.1060255
   Martin S., 2010, P ACM SIGGRAPH 2010
   Peterlík I, 2010, COMPUT GRAPH-UK, V34, P43, DOI 10.1016/j.cag.2009.10.005
   Prados Francisco J. R., 2012, International Journal of Creative Interfaces and Computer Graphics, V3, P63, DOI 10.4018/jcicg.2012010105
   Scherer KR, 2005, SOC SCI INFORM, V44, P695, DOI 10.1177/0539018405058216
   Sugiura Y., 2011, PROC 24 ANN ACM S US, P509, DOI [10.1145/2047196, DOI 10.1145/2047196]
   Ullmer B, 2000, IBM SYST J, V39, P915, DOI 10.1147/sj.393.0915
   Young W., 2002, Roark's Formulas for Stress and Strain
   Zhao J.B.Y., 2011, ACM T GRAPHIC, V30, P91
   Zhou K, 2005, ACM T GRAPHIC, V24, P496, DOI 10.1145/1073204.1073219
NR 20
TC 1
Z9 1
U1 1
U2 11
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2012
VL 28
IS 6-8
BP 799
EP 808
DI 10.1007/s00371-012-0695-y
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 947EW
UT WOS:000304411500027
DA 2024-07-18
ER

PT J
AU Liu, YJ
   Zheng, YF
   Lv, L
   Xuan, YM
   Fu, XL
AF Liu, Yong-Jin
   Zheng, Yi-Fu
   Lv, Lu
   Xuan, Yu-Ming
   Fu, Xiao-Lan
TI 3D model retrieval based on color plus geometry signatures
SO VISUAL COMPUTER
LA English
DT Article
DE 3D model retrieval; Color features; Shape signature
ID OBJECT RECOGNITION; COMBINING COLOR; SHAPE; TEXTURE
AB Color plays a significant role in the recognition of 3D objects and scenes from the perspective of cognitive psychology. In this paper, we propose a new 3D model retrieval method, focusing on not only the geometric features but also the color features of 3D mesh models. Firstly, we propose a new sampling method that samples the models in the regions of either geometry-high-variation or color-high-variation. After collecting geometry + color sensitive sampling points, we cluster them into several classes by using a modified ISODATA algorithm. Then we calculate the feature histogram of each model in the database using these clustered sampling points. For model retrieval, we compare the histogram of an input model to the stored histograms in the database to find out the most similar models. To evaluate the retrieval method based on the new color + geometry signatures, we use the precision/recall performance metric to compare our method with several classical methods. Experiment results show that color information does help improve the accuracy of 3D model retrieval, which is consistent with the postulate in psychophysics that color should strongly influence the recognition of objects.
C1 [Liu, Yong-Jin; Zheng, Yi-Fu; Lv, Lu] Tsinghua Univ, Tsinghua Natl Lab Informat Sci & Technol, Dept Comp Sci & Technol, Beijing 100084, Peoples R China.
   [Xuan, Yu-Ming; Fu, Xiao-Lan] Chinese Acad Sci, Inst Psychol, State Key Lab Brain & Cognit Sci, Beijing 100101, Peoples R China.
C3 Tsinghua University; Chinese Academy of Sciences; Institute of
   Psychology, CAS
RP Liu, YJ (corresponding author), Tsinghua Univ, Tsinghua Natl Lab Informat Sci & Technol, Dept Comp Sci & Technol, Beijing 100084, Peoples R China.
EM liuyongjin@tsinghua.edu.cn
RI Liu, Yong/GWQ-6163-2022
FU National Basic Research Program of China [2011CB302202]; Natural Science
   Foundation of China [60970099]; Tsinghua University [20101081863]
FX The authors thank the reviewers for their comments that help improve
   this paper. The authors also appreciate McGill Shape Analysis Group and
   Purdue PRECISE lab for making their data publicly available. This work
   was supported by the National Basic Research Program of China
   (2011CB302202), the Natural Science Foundation of China (60970099), and
   Tsinghua University Initiative Scientific Research Program
   (20101081863).
CR Ankerst M, 1999, LECT NOTES COMPUT SC, V1651, P207
   [Anonymous], PATTERN RECOGNITION
   Billmeyer F.W., 1981, PRINCIPLES COLOR TEC, V2nd
   CAVANAGH P, 1987, COMPUT VISION GRAPH, V37, P171, DOI 10.1016/S0734-189X(87)80001-4
   Chen DY, 2003, COMPUT GRAPH FORUM, V22, P223, DOI 10.1111/1467-8659.00669
   Conway BevilR., 2002, Neural Mechanisms of Color Vision: Double-Opponent Cells in the Visual Cortex
   Desbrun M, 1999, COMP GRAPH, P317, DOI 10.1145/311535.311576
   do Carmo MP., 1992, RIEMANNIAN GEOMETRY
   Elad A, 2003, IEEE T PATTERN ANAL, V25, P1285, DOI 10.1109/TPAMI.2003.1233902
   Funkhouser T, 2003, ACM T GRAPHIC, V22, P83, DOI 10.1145/588272.588279
   Gevers T, 2000, IEEE T IMAGE PROCESS, V9, P102, DOI 10.1109/83.817602
   Hilaga M, 2001, COMP GRAPH, P203, DOI 10.1145/383259.383282
   Hoffman DD, 1997, COGNITION, V63, P29, DOI 10.1016/S0010-0277(96)00791-3
   HORN BKP, 1984, P IEEE, V72, P1671, DOI 10.1109/PROC.1984.13073
   International Commission on Illumination, 1978, CIE PUBL S2, V15
   Jayanti S, 2006, COMPUT AIDED DESIGN, V38, P939, DOI 10.1016/j.cad.2006.06.007
   Johnson AE, 1998, IMAGE VISION COMPUT, V16, P635, DOI 10.1016/S0262-8856(98)00074-2
   Johnson AE, 1999, IEEE T PATTERN ANAL, V21, P433, DOI 10.1109/34.765655
   Lee CH, 2005, ACM T GRAPHIC, V24, P659, DOI 10.1145/1073204.1073244
   Lindeberg T., 1994, Journal of AppliedStatistics, V21, P225
   Liu YJ, 2011, IEEE T PATTERN ANAL, V33, P1502, DOI 10.1109/TPAMI.2010.221
   LIVINGSTONE MS, 1987, J NEUROSCI, V7, P3416
   LV L, 2010, THESIS TSINGHUA U
   Marr D., 1982, Vision. A computational investigation into the human representation and processing of visual information
   *MCGILL, 2011, 3D SHAP BENCHM
   Mel BW, 1997, NEURAL COMPUT, V9, P777, DOI 10.1162/neco.1997.9.4.777
   Mou WM, 2008, COGNITION, V108, P136, DOI 10.1016/j.cognition.2008.02.004
   OLSHAUSEN BA, 1993, J NEUROSCI, V13, P4700
   *OPT SOC AM, 1977, UN SPAC COL SAMPL
   Osada R, 2001, INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, PROCEEDING, P154, DOI 10.1109/SMA.2001.923386
   Rubner Y, 2000, INT J COMPUT VISION, V40, P99, DOI 10.1023/A:1026543900054
   Silverman, 2018, DENSITY ESTIMATION S, DOI 10.1201/9781315140919
   SLATER D, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P563, DOI 10.1109/ICCV.1995.466889
   Tanaka J, 2001, TRENDS COGN SCI, V5, P211, DOI 10.1016/S1364-6613(00)01626-0
   TAUBIN G, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P902, DOI 10.1109/ICCV.1995.466840
   TURLACH BA, 1993, 9317 UCL I STAT
   Ullman S., 1996, High level vision
   Van Rijsbergen C. J., 1979, Information Retrieval, V2nd
   Vranic DV, 2003, 2003 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL 3, PROCEEDINGS, P757
   Vranic DV, 2001, 2001 IEEE FOURTH WORKSHOP ON MULTIMEDIA SIGNAL PROCESSING, P293, DOI 10.1109/MMSP.2001.962749
   Wang JL, 2008, ADV ENG INFORM, V22, P385, DOI 10.1016/j.aei.2008.04.001
   Xu D, 2008, PATTERN RECOGN, V41, P240, DOI 10.1016/j.patcog.2007.05.001
NR 42
TC 22
Z9 22
U1 0
U2 13
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2012
VL 28
IS 1
BP 75
EP 86
DI 10.1007/s00371-011-0605-8
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 874YQ
UT WOS:000298995000007
DA 2024-07-18
ER

PT J
AU Jeong, S
   Kim, TH
   Kim, CH
AF Jeong, SoHyeon
   Kim, Tae-hyeong
   Kim, Chang-Hun
TI Shrinkage, wrinkling and ablation of burning cloth and paper
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Conference on the Computer Graphics International (CGI)
CY JUN 12-15, 2011
CL Ottawa, CANADA
DE Constrained Lagrangian mechanics; Fast projection; The body-centered
   square; Heat transfer
ID SIMULATION
AB The burning of a sheet of cellulose-based material, such as paper or cloth, involves uneven shrinkage which causes wrinkling. We simulate this geometrically complicated phenomenon by modeling the effects of heat transfer, shrinkage and partial ablation on a thin shell. A strain-limitation technique is applied to a two-layer structure of springs arranged as a body-centered square. Although this structure is over-constrained, convergence can be achieved using a new successive fast projection method. We also remesh the shells dynamically to deal with the topological changes that occur as regions burn away.
C1 [Jeong, SoHyeon] Korea Univ, Dept Comp & Radio Commun Engn, Seoul, South Korea.
   [Kim, Chang-Hun] Korea Univ, Dept Comp Sci & Engn, Seoul, South Korea.
   [Kim, Chang-Hun] Korea Univ, Dept Brain & Cognit Engn, Seoul, South Korea.
   [Kim, Tae-hyeong] Korea Univ, Dept Visual Informat Proc, Seoul, South Korea.
C3 Korea University; Korea University; Korea University; Korea University
RP Jeong, S (corresponding author), Korea Univ, Dept Comp & Radio Commun Engn, Seoul, South Korea.
EM SoHyeon.Jeong@gmail.com; usemagic@korea.ac.kr; chkim@korea.ac.kr
OI Kim, TaeHyeong/0000-0002-1862-9208
CR Baraff D., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P43, DOI 10.1145/280814.280821
   Bell N., 2008, Efficient sparse matrix-vector multiplication on CUDA
   BERGOU M, 2006, P 4 EUR S GEOM PROC, P227
   Bridson R., 2003, Simulation of clothing with folds and wrinkles, P28
   Carlson M., 2002, ACM SIGGRAPH/Eurographics Symp. Comp. Anim, P167
   Choi KJ, 2002, ACM T GRAPHIC, V21, P604, DOI 10.1145/566570.566624
   Choi MH, 2004, SIGGRAPH 04 ACM SIGG, P95
   Desbrun M, 1999, COMP GRAPH, P317, DOI 10.1145/311535.311576
   English E, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360665
   Garg A, 2007, SYMPOSIUM ON COMPUTER ANIMATION 2007: ACM SIGGRAPH/ EUROGRAPHICS SYMPOSIUM PROCEEDINGS, P91
   GOLDENTHAL R, 2007, SIGGRAPH 07 ACM SIGG, P49
   Grinspun E., 2003, ACM SIGGRAPH/Eurographics Symposium on Computer Animation, P62
   Hong M, 2005, IEEE INT CONF ROBOT, P4520
   Liu SG, 2009, VISUAL COMPUT, V25, P687, DOI 10.1007/s00371-009-0344-2
   Losasso F, 2006, IEEE T VIS COMPUT GR, V12, P343, DOI 10.1109/TVCG.2006.51
   Melek Zeki., 2007, SPM '07: Proceedings of the 2007 ACM symposium on Solid and physical modeling, P51
   Molino N., 2003, INT MESHING ROUNDTAB, P103
   Müller M, 2007, J VIS COMMUN IMAGE R, V18, P109, DOI 10.1016/j.jvcir.2007.01.005
   PROVOT X, 1995, GRAPH INTER, P147
   Terzopoulos D., 1989, Proceedings. Graphics Interface'89, P219
   Terzopoulos D., 1987, COMPUT GRAPH, P205, DOI DOI 10.1145/37402.37427
   Thomaszewski B, 2006, WSCG SHORT COMM P, P9
   Volino P., 2006, SCA 06, P101
NR 23
TC 10
Z9 11
U1 0
U2 4
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2011
VL 27
IS 6-8
BP 417
EP 427
DI 10.1007/s00371-011-0575-x
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 766FY
UT WOS:000290767600002
DA 2024-07-18
ER

PT J
AU Pronost, N
   Sandholm, A
   Thalmann, D
AF Pronost, Nicolas
   Sandholm, Anders
   Thalmann, Daniel
TI A visualization framework for the analysis of neuromuscular simulations
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 3DAH Conference
CY MAY 23-24, 2010
CL Chania, GREECE
DE Scientific visualization; Biomechanics and neuromuscular simulation;
   Motion analysis; OpenSim software
ID LOWER-EXTREMITY; LOWER-LIMB; MOVEMENT; MUSCLE; MODEL; KINEMATICS; GAIT
AB We present a visualization framework for exploring and analyzing data sets from biomechanical and neuromuscular simulations. These data sets describe versatile information related to the different stages of a motion analysis. In studying these data using a 3D visualization approach, interactive exploring is important, especially for supporting spatial analysis. Moreover, as these data contain many various but related elements, numerical analysis of neuromuscular simulations is complicated. Visualization techniques enhance the analysis process, thus improving the effectiveness of the experiments. Our approach allows convenient definitions of relationships between numerical data sets and 3D objects. Scientific simulation data sets appropriate for this style of analysis are present everywhere motion analysis is performed and are predominant in many clinical works. In this paper, we outline the functionalities of the framework as well as applications embedded within the OpenSim simulation platform. These functionalities form an effective approach specifically designed for the investigation of neuromuscular simulations. This claim is supported by evaluation experiments where the framework was used to analyze gaits and crouch motions.
C1 [Pronost, Nicolas; Sandholm, Anders; Thalmann, Daniel] Ecole Polytech Fed Lausanne, Virtual Real Lab, Lausanne, Switzerland.
   [Pronost, Nicolas] Univ Utrecht, Games & Virtual Worlds Res Grp, Utrecht, Netherlands.
C3 Swiss Federal Institutes of Technology Domain; Ecole Polytechnique
   Federale de Lausanne; Utrecht University
RP Pronost, N (corresponding author), Ecole Polytech Fed Lausanne, Virtual Real Lab, Lausanne, Switzerland.
EM nicolas@cs.uu.nl
RI Thalmann, Daniel/A-4347-2008; Thalmann, Daniel/AAL-1097-2020
OI Thalmann, Daniel/0000-0002-0451-7491; PRONOST,
   NICOLAS/0000-0003-4499-509X
CR Arnold A S, 2000, Comput Aided Surg, V5, P108, DOI 10.1002/1097-0150(2000)5:2<108::AID-IGS5>3.0.CO;2-2
   Arnold EM, 2010, ANN BIOMED ENG, V38, P269, DOI 10.1007/s10439-009-9852-5
   Chen J, 2007, IEEE VIS 2007 POST C
   Damsgaard M, 2006, SIMUL MODEL PRACT TH, V14, P1100, DOI 10.1016/j.simpat.2006.09.001
   Della Croce U, 1999, MED BIOL ENG COMPUT, V37, P155, DOI 10.1007/BF02513282
   Delp SL, 2007, IEEE T BIO-MED ENG, V54, P1940, DOI 10.1109/TBME.2007.901024
   DELP SL, 1993, J BIOMECH, V26, P485, DOI 10.1016/0021-9290(93)90011-3
   DELP SL, 1990, IEEE T BIO-MED ENG, V37, P757, DOI 10.1109/10.102791
   HALLORAN JP, 2010, J BIOMECH
   Holzbaur KRS, 2005, ANN BIOMED ENG, V33, P829, DOI 10.1007/s10439-005-3320-7
   Horsman MDK, 2007, CLIN BIOMECH, V22, P239, DOI 10.1016/j.clinbiomech.2006.10.003
   JAN SLV, 1998, IEEE COMPUT GRAPH, V18, P10
   Keefe DF, 2008, COMPUT GRAPH FORUM, V27, P863, DOI 10.1111/j.1467-8659.2008.01218.x
   Keefe DF, 2009, IEEE T VIS COMPUT GR, V15, P1383, DOI 10.1109/TVCG.2009.152
   Krekel PR, 2010, COMPUT GRAPH FORUM, V29, P1123, DOI 10.1111/j.1467-8659.2009.01681.x
   Manal K, 2005, J BIOMECH, V38, P2151, DOI 10.1016/j.jbiomech.2004.10.008
   Manal K, 2004, GAIT POSTURE, V20, P222, DOI 10.1016/j.gaitpost.2003.09.009
   Marai GE, 2004, IEEE T BIO-MED ENG, V51, P790, DOI 10.1109/TBME.2004.826606
   Maurice X, 2009, VISUAL COMPUT, V25, P835, DOI 10.1007/s00371-009-0313-9
   Robertson G, 2008, IEEE T VIS COMPUT GR, V14, P1325, DOI 10.1109/TVCG.2008.125
   SANDHOLM A, 2009, LECT NOTES COMPUTER, V5903
   Scheys L, 2009, J BIOMECH, V42, P565, DOI 10.1016/j.jbiomech.2008.12.014
   Schmid J, 2008, LECT NOTES COMPUT SC, V5241, P119, DOI 10.1007/978-3-540-85988-8_15
   Stagni R, 2000, J BIOMECH, V33, P1479, DOI 10.1016/S0021-9290(00)00093-2
   Staudenmann D, 2010, J ELECTROMYOGR KINES, V20, P375, DOI 10.1016/j.jelekin.2009.08.005
   VANDENBOGERT A, 2008, ANN M AM SOC BIOM NA
   Whittle M., 2006, Gait analysis: an introduction
   Xiao M, 2010, J APPL BIOMECH, V26, P142, DOI 10.1123/jab.26.2.142
   FEBIO PLATFORM
NR 29
TC 5
Z9 5
U1 0
U2 5
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2011
VL 27
IS 2
SI SI
BP 109
EP 119
DI 10.1007/s00371-010-0534-y
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 722QR
UT WOS:000287449900004
DA 2024-07-18
ER

PT J
AU Agathos, A
   Pratikakis, I
   Papadakis, P
   Perantonis, S
   Azariadis, P
   Sapidis, NS
AF Agathos, Alexander
   Pratikakis, Ioannis
   Papadakis, Panagiotis
   Perantonis, Stavros
   Azariadis, Philip
   Sapidis, Nickolas S.
TI 3D articulated object retrieval using a graph-based representation
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 2nd Eurographics Workshop on 3D Object Retrieval
CY MAR 29, 2009
CL Munich, GERMANY
DE 3D articulated object retrieval; Mesh segmentation; Graph matching
AB In this paper, a retrieval methodology for 3D articulated objects is presented that relies upon a graph-based object representation. The methodology is composed of a mesh segmentation stage which creates the Attributed Relation Graph (ARG) of the object along with a graph matching algorithm which matches two ARGs. The graph matching algorithm is based on the Earth Movers Distance (EMD) similarity measure calculated with a new ground distance assignment. The superior performance of the proposed retrieval methodology against state-of-the-art approaches is shown by extensive experimentation that comprise the application of various geometric descriptors representing the components of the 3D objects that become the node attributes of the ARGs as well as alternative mesh segmentation approaches for the extraction of the object parts. The performance evaluation is addressed in both qualitative and quantitative terms.
C1 [Pratikakis, Ioannis] Democritus Univ Thrace, Dept Elect & Comp Engn, GR-67100 Xanthi, Greece.
   [Agathos, Alexander; Papadakis, Panagiotis; Perantonis, Stavros] Inst Informat & Telecommun NCSR Demokritos, Computat Intelligence Lab, Attiki, Greece.
   [Azariadis, Philip] Univ Aegean, Dept Prod & Syst Design Engn, Ermoupolis 84100, Syros, Greece.
   [Sapidis, Nickolas S.] Univ Western Macedonia, Dept Mech Engn, Kozani 50100, Greece.
C3 Democritus University of Thrace; National Centre of Scientific Research
   "Demokritos"; University of Aegean; University of Western Macedonia
RP Pratikakis, I (corresponding author), Democritus Univ Thrace, Dept Elect & Comp Engn, GR-67100 Xanthi, Greece.
EM agalex@iit.demokritos.gr; ipratika@ee.duth.gr;
   ppapadak@iit.demokritos.gr; sper@iit.demokritos.gr; azar@aegean.gr;
   nsapidis@uowm.gr
RI Azariadis, Filippos/C-7202-2011; PRATIKAKIS, IOANNIS/AAD-3387-2019;
   Papadakis, Panagiotis/JFL-0155-2023; Sapidis, Nickolas S./E-5584-2010
OI Azariadis, Filippos/0000-0002-8687-5777; PRATIKAKIS,
   IOANNIS/0000-0002-4124-3688; Papadakis, Panagiotis/0000-0002-2193-8087; 
CR Agathos A, 2010, VISUAL COMPUT, V26, P63, DOI 10.1007/s00371-009-0383-8
   [Anonymous], 2007, Computer-Aided Design Applications, DOI DOI 10.1080/16864360.2007.10738515
   Biasotti S, 2006, COMPUT AIDED DESIGN, V38, P1002, DOI 10.1016/j.cad.2006.07.003
   BIEDERMAN I, 1987, PSYCHOL REV, V94, P115, DOI 10.1037/0033-295X.94.2.115
   Bronstein AM, 2009, INT J COMPUT VISION, V81, P281, DOI 10.1007/s11263-008-0172-2
   Bustos B., 2004, Proceedings. IEEE Sixth International Symposium on Multimedia Software, P514
   Chen DY, 2003, COMPUT GRAPH FORUM, V22, P223, DOI 10.1111/1467-8659.00669
   Cornea ND, 2005, INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, PROCEEDINGS, P366, DOI 10.1109/SMI.2005.1
   FUNKHOUSER T, 2006, 4 EUR S GEOM PROC, P131
   Gal R, 2007, IEEE T VIS COMPUT GR, V13, P261, DOI 10.1109/TVCG.2007.45
   Hilaga M, 2001, COMP GRAPH, P203, DOI 10.1145/383259.383282
   HOFFMAN DD, 1984, COGNITION, V18, P65, DOI 10.1016/0010-0277(84)90022-2
   Jain V, 2007, COMPUT AIDED DESIGN, V39, P398, DOI 10.1016/j.cad.2007.02.009
   Katz S, 2005, VISUAL COMPUT, V21, P649, DOI 10.1007/s00371-005-0344-9
   Katz S, 2003, ACM T GRAPHIC, V22, P954, DOI 10.1145/882262.882369
   KAZHDAN M, 2003, EUR ACM SIGGRAPH S G, P156
   Kim HM, 2004, J CERAM PROCESS RES, V5, P238
   Lin HYS, 2007, IEEE T MULTIMEDIA, V9, P46, DOI 10.1109/TMM.2006.886344
   Marini S, 2007, IEEE COMPUT GRAPH, V27, P28, DOI 10.1109/MCG.2007.89
   *MCGILL, 2003, MCGILL 3D SHAP BENCH
   Ohbuchi R, 2008, IEEE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS 2008, PROCEEDINGS, P93, DOI 10.1109/SMI.2008.4547955
   PAPADAKIS P, 2008, EUR WORKSH 3D OBJ RE, P9
   Papadakis P, 2007, PATTERN RECOGN, V40, P2437, DOI 10.1016/j.patcog.2006.12.026
   Passalis G, 2007, VISUAL COMPUT, V23, P5, DOI 10.1007/s00371-006-0037-z
   Rubner Y, 2000, INT J COMPUT VISION, V40, P99, DOI 10.1023/A:1026543900054
   Sundar H, 2003, SMI 2003: SHAPE MODELING INTERNATIONAL 2003, PROCEEDINGS, P130, DOI 10.1109/smi.2003.1199609
   TAL A, 2006, INT C COMP GRAPH THE, P142
   Tung T., 2005, International Journal of Shape Modeling, V11, P91, DOI 10.1142/S0218654305000748
   VRANIC D, 2005, IEEE INT C MULT EXP, P145
   VRANIC DV, 2004, THESIS U LEIPZIG
   [No title captured]
NR 31
TC 23
Z9 25
U1 0
U2 8
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2010
VL 26
IS 10
BP 1301
EP 1319
DI 10.1007/s00371-010-0523-1
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 649SI
UT WOS:000281793200006
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Lankes, FK
   Platz, M
   Stamminger, M
AF Lankes, Franz K.
   Platz, Markus
   Stamminger, Marc
TI Environmental lighting on dynamic objects using hemicube bit masks
SO VISUAL COMPUTER
LA English
DT Article
DE Ambient occlusion; Environmental lighting
AB We present a method for environmental lighting on rigid objects that move within a static environment, such as cars driving through a city. Self-occlusion of the objects and occlusion due to the environment are precomputed and stored in hemicube bit masks. At run-time, local and environmental occlusion are rotated to a common space, combined and integrated with the BRDF. We show that all these computations can be done very efficiently with hemicube bit masks, resulting in real-time frame rates even for glossy objects and large environments.
C1 [Lankes, Franz K.; Platz, Markus; Stamminger, Marc] Lehrstuhl Graph Datenverarbeitung, D-91058 Erlangen, Germany.
RP Stamminger, M (corresponding author), Lehrstuhl Graph Datenverarbeitung, Wolfsmantel 33, D-91058 Erlangen, Germany.
EM franz.lankes@cs.fau.de; markus.platz@cosetrain.com;
   marc.stamminger@cs.fau.de
CR [Anonymous], 2009, P S INT 3D GRAPH GAM, DOI 10.1145/1507149.1507161.5,7
   BRUNNEL M, 2005, GPU GEMS 2, P223
   Dimitrov R., 2008, I3D 08
   Goldstein H., 2002, CLASSICAL MECH
   HASTINGS WK, 1970, BIOMETRIKA, V57, P97, DOI 10.1093/biomet/57.1.97
   Kautz J., 2004, P 15 EUROGRAPHICS C, P179
   Kontkanen Janne, 2005, P 2005 S INT 3D GRAP, P41, DOI 10.1145/1053427.1053434
   Malmer M., 2007, Journal of Graphics Tools, V12, P59
   Mittring Martin., 2007, SIGGRAPH '07, P97
   NG R, 2003, SIGGRAPH 03, P376
   REINBOTHE C, 2009, EUROGRAPHICS AREA PA
   SATTLER M, 2004, VISION MODELING VISU, P331
   Shanmugam P, 2007, I3D 2007: ACM SIGGRAPH SYMPOSIUM ON INTERACTIVE 3D GRAPHICS AND GAMES, PROCEEDINGS, P73
   Sloan Peter-Pike., 2002, Siggraph'02: Proceedings of the 29th annual conference on computer graphics and interactive techniques, P527
NR 14
TC 0
Z9 0
U1 0
U2 1
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2010
VL 26
IS 6-8
BP 575
EP 582
DI 10.1007/s00371-010-0478-2
PG 8
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 602JQ
UT WOS:000278135800018
DA 2024-07-18
ER

PT J
AU Abeysinghe, SS
   Ju, T
AF Abeysinghe, Sasakthi S.
   Ju, Tao
TI Interactive skeletonization of intensity volumes
SO VISUAL COMPUTER
LA English
DT Article
CT Computer Graphics International Conference 2009
CY MAY 26-29, 2009
CL Victoria, CANADA
DE Interactive; Skeletonization; Intensity volumes
ID ELECTRON CRYOMICROSCOPY; IMAGE SEGMENTATION; RESOLUTION; SURFACE;
   MODELS; CURVE
AB We present an interactive approach for identifying skeletons (i.e. centerlines) in intensity volumes, such as those produced by biomedical imaging. While skeletons are very useful for a range of image analysis tasks, it is extremely difficult to obtain skeletons with correct connectivity and shape from noisy inputs using automatic skeletonization methods. In this paper we explore how easy-to-supply user inputs, such as simple mouse clicking and scribbling, can guide the creation of satisfactory skeletons. Our contributions include formulating the task of drawing 3D centerlines given 2D user inputs as a constrained optimization problem, solving this problem on a discrete graph using a shortest-path algorithm, building a graphical interface for interactive skeletonization and testing it on a range of biomedical data.
C1 [Abeysinghe, Sasakthi S.; Ju, Tao] Washington Univ, Dept Comp Sci & Engn, St Louis, MO 63130 USA.
C3 Washington University (WUSTL)
RP Abeysinghe, SS (corresponding author), Washington Univ, Dept Comp Sci & Engn, St Louis, MO 63130 USA.
EM sasakthi.abeysinghe@wustl.edu; taoju@cse.wustl.edu
CR Abeysinghe S., GORGON INTERACTIVE M
   Abeysinghe SS., 2008, SEGMENTATION FREE SK, P63
   [Anonymous], 1995, SIGGRAPH
   Armstrong CJ, 2007, COMPUT GRAPH-UK, V31, P212, DOI 10.1016/j.cag.2006.11.015
   Barrett W A, 1997, Med Image Anal, V1, P331, DOI 10.1016/S1361-8415(97)85005-0
   Boykov Y, 2006, INT J COMPUT VISION, V70, P109, DOI 10.1007/s11263-006-7934-5
   Chiu W, 2005, STRUCTURE, V13, P363, DOI 10.1016/j.str.2004.12.016
   Cohen LD, 1997, INT J COMPUT VISION, V24, P57, DOI 10.1023/A:1007922224810
   Cornea ND, 2007, IEEE T VIS COMPUT GR, V13, P530, DOI 10.1109/TVCG.2007.1002
   DESCHAMPS T, 2000, ECCV 00, P543
   Eberly D., 1994, Journal of Mathematical Imaging and Vision, V4, P353, DOI 10.1007/BF01262402
   Falcao AX, 2000, IEEE T MED IMAGING, V19, P55, DOI 10.1109/42.832960
   FURST JD, 1996, MMBIA 96, P124
   Jiang W, 2008, NATURE, V451, P1130, DOI 10.1038/nature06665
   Ju T, 2007, COMPUT AIDED DESIGN, V39, P352, DOI 10.1016/j.cad.2007.02.006
   Krissian K, 2000, COMPUT VIS IMAGE UND, V80, P130, DOI 10.1006/cviu.2000.0866
   KRISSIAN K, 2004, 0003 HARV MED SCH DE
   KRISSIAN K, 2000, P SPIE 2000
   METZ CT, 2008 MICCAI WORKSH 3
   NATHAN A, 2008, P ASME 2008 SUMM BIO
   Poon M, 2008, COMPUT MED IMAG GRAP, V32, P639, DOI 10.1016/j.compmedimag.2008.07.004
   Reniers D, 2008, IEEE T VIS COMPUT GR, V14, P355, DOI 10.1109/TC.2007.70786
   Selle D, 2002, IEEE T MED IMAGING, V21, P1344, DOI 10.1109/TMI.2002.801166
   Siddiqi K, 2008, MED REPRESENTATIONS
   Weickert J., 1998, ANISOTROPIC DIFFUSIO, V1
   Whitaker R., 2001, VOLUME GRAPHICS
   Wong CJ, 2006, J MOL BIOL, V362, P26, DOI 10.1016/j.jmb.2006.07.016
   Yim PJ, 2000, IEEE T MED IMAGING, V19, P568, DOI 10.1109/42.870662
   Yu ZY, 2006, IEEE IMAGE PROC, P2513, DOI 10.1109/ICIP.2006.312804
   Yushkevich PA, 2006, NEUROIMAGE, V31, P1116, DOI 10.1016/j.neuroimage.2006.01.015
   Zhang HF, 2006, NAT BIOTECHNOL, V24, P848, DOI 10.1038/nbt1220
NR 31
TC 21
Z9 24
U1 0
U2 5
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2009
VL 25
IS 5-7
BP 627
EP 635
DI 10.1007/s00371-009-0325-5
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 438ES
UT WOS:000265539300028
DA 2024-07-18
ER

PT J
AU Lee, HY
   Hong, JM
   Kim, CH
AF Lee, Ho-Young
   Hong, Jeong-Mo
   Kim, Chang-Hun
TI Simulation of swirling bubbly water using bubble particles
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Computer Graphics International Conference 2009
CY MAY 26-29, 2009
CL Victoria, CANADA
DE Fluid simulation; Physically based modeling; Bubbles; Grid-based
   simulation; Multiphase fluids
ID ANIMATION
AB The effect of surface tension is dynamically and realistically represented within a multiphase fluid simulation. Air bubbles are seeded with 'bubble particles' which move randomly. These molecule-like movements modify the surface of the air bubbles and generate turbulence in the water. The surface tension between air bubble and water, determined by the composition of the water, remains constant regardless of the size of the bubble, while external forces cause unstable fluid motion as the surface tension strives to remain constant, bubbles split and merge. The bubble particles can also compute for the numerical dissipation usually experienced in grid-based fluid simulations, by restoring the lost volume of individual bubbles. The realistic tearing of bubble surfaces is shown in a range of examples.
C1 [Lee, Ho-Young; Kim, Chang-Hun] Korea Univ, Dept Comp Sci, Seoul, South Korea.
   [Hong, Jeong-Mo] Dongguk Univ, Dept Comp Sci, Seoul, South Korea.
C3 Korea University; Dongguk University
RP Kim, CH (corresponding author), Korea Univ, Dept Comp Sci, Seoul, South Korea.
EM flymist@korea.ac.kr; jmhong@dongguk.edu; chkim@korea.ac.kr
CR [Anonymous], EUROGRAPHICS
   Cleary PW, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1239451.1239548, 10.1145/1276377.1276499]
   Enright D, 2002, ACM T GRAPHIC, V21, P736, DOI [10.1145/566570.566581, 10.1145/566570.566645]
   Fedkiw R, 2001, COMP GRAPH, P15, DOI 10.1145/383259.383260
   Foster N, 1996, GRAPH MODEL IM PROC, V58, P471, DOI 10.1006/gmip.1996.0039
   Foster N, 2001, COMP GRAPH, P23, DOI 10.1145/383259.383261
   Greenwood S., 2004, Proceedings of the 2004 ACM SIGGRAPH/Eurographics symposium on Computer animation, P287
   Hong JM, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360647
   Hong JM, 2005, ACM T GRAPHIC, V24, P915, DOI 10.1145/1073204.1073283
   Hong JM, 2003, COMPUT GRAPH FORUM, V22, P253, DOI 10.1111/1467-8659.00672
   KIM B, 2007, ACM T GRAPHICS SIGGR, V26, P481
   Kim J., 2006, Proc ACM SIGGRAPH/Eurograph Symp Comp Anim, SCA '06, P335
   Losasso F, 2004, ACM T GRAPHIC, V23, P457, DOI 10.1145/1015706.1015745
   Losasso F, 2008, IEEE T VIS COMPUT GR, V14, P797, DOI 10.1109/TVCG.2008.37
   Losasso F, 2006, ACM T GRAPHIC, V25, P812, DOI 10.1145/1141911.1141960
   Magnaudet J, 2000, ANNU REV FLUID MECH, V32, P659, DOI 10.1146/annurev.fluid.32.1.659
   Mihalef Viorel., 2006, S COMPUTER ANIMATION, P317
   Muller M, 2005, P 2005 ACM SIGGRAPH, P237, DOI DOI 10.1145/1073368.1073402
   Selle A, 2005, ACM T GRAPHIC, V24, P910, DOI 10.1145/1073204.1073282
   SHI L, 2005, ACM SIGGRAPH EUR S C, P229
   Shin SH, 2007, COMPUT ANIMAT VIRT W, V18, P447, DOI 10.1002/cav.202
   Song OY, 2005, ACM T GRAPHIC, V24, P81, DOI 10.1145/1037957.1037962
   Stam J, 1999, COMP GRAPH, P121, DOI 10.1145/311535.311548
NR 23
TC 5
Z9 5
U1 0
U2 9
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2009
VL 25
IS 5-7
BP 707
EP 712
DI 10.1007/s00371-009-0338-0
PG 6
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 438ES
UT WOS:000265539300036
DA 2024-07-18
ER

PT J
AU Liu, SG
   Liu, QG
   An, T
   Sun, JZ
   Peng, QS
AF Liu, Shiguang
   Liu, Qiguang
   An, Tai
   Sun, Jizhou
   Peng, Qunsheng
TI Physically based simulation of thin-shell objects' burning
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Computer Graphics International Conference 2009
CY MAY 26-29, 2009
CL Victoria, CANADA
DE Physically based modeling; Thin-shell objects; Burning; Spreading model;
   Deformation
AB We present a novel unified framework for simulating burning phenomena of thin-shell objects such as paper, cloth, etc. A gradient-based spreading model is proposed to simulate the movement of fire on thin-shell objects. To simulate the crumpling and deformation of thin-shell objects when burning, we present an FFD-based deformation model which takes the physical theory of burning phenomena into account. The fire is rendered by the method of particle systems. By inputting different parameters and initial conditions, our framework can simulate various burning phenomena of different thin-shell objects. Experiments validate that it is efficient and easy to implement.
C1 [Liu, Shiguang; An, Tai; Sun, Jizhou] Tianjin Univ, Sch Comp Sci & Technol, Tianjin 300072, Peoples R China.
   [Liu, Shiguang] Chinese Acad Sci, Inst Software, State Key Lab Comp Sci, Beijing 100190, Peoples R China.
   [Liu, Shiguang; Peng, Qunsheng] Zhejiang Univ, State Key Lab CAD&CG, Hangzhou 310058, Peoples R China.
   [Liu, Qiguang] Tianjin Univ, Sch Comp Software, Tianjin 300072, Peoples R China.
C3 Tianjin University; Chinese Academy of Sciences; Institute of Software,
   CAS; Zhejiang University; Tianjin University
RP Liu, SG (corresponding author), Tianjin Univ, Sch Comp Sci & Technol, Tianjin 300072, Peoples R China.
EM lsg@tju.edu.cn
CR Baraff D., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P43, DOI 10.1145/280814.280821
   Carlson M, 2004, ACM T GRAPHIC, V23, P377, DOI 10.1145/1015706.1015733
   ENRIGHT D, 2001, P SIGGRAPH, P736
   Fedkiw R, 2001, COMP GRAPH, P15, DOI 10.1145/383259.383260
   Feldman BE, 2003, ACM T GRAPHIC, V22, P708, DOI 10.1145/882262.882336
   Foster N, 2001, COMP GRAPH, P23, DOI 10.1145/383259.383261
   GAY KL, 1993, P IFIP INT C COMP GR, P357
   Génevaux O, 2003, PROC GRAPH INTERF, P31
   HONG J, 2007, P SIGGRAPH
   Ling L, 1996, COMPUT GRAPH, V20, P137, DOI 10.1016/0097-8493(95)00073-9
   Liu Y, 2004, 12TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P247
   Losasso F, 2006, IEEE T VIS COMPUT GR, V12, P343, DOI 10.1109/TVCG.2006.51
   Melek Z, 2003, 11TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P462, DOI 10.1109/PCCGA.2003.1238295
   MELEK Z, 2006, P SIGGRAPH POST
   Melek Zeki., 2007, SPM '07: Proceedings of the 2007 ACM symposium on Solid and physical modeling, P51
   Nguyen DQ, 2002, ACM T GRAPHIC, V21, P721, DOI 10.1145/566570.566643
   PAKESHI A, 1991, IEICE T E, V74, P457
   PROVOT X, 1995, GRAPH INTER, P147
   REEVES WT, 1983, ACM T GRAPHIC, V2, P91, DOI 10.1145/964967.801167
   Sederberg T. W., 1986, Computer Graphics, V20, P151, DOI 10.1145/15886.15903
   Terzopoulos D., 1988, Visual Computer, V4, P306, DOI 10.1007/BF01908877
   Yngve GD, 2000, COMP GRAPH, P29, DOI 10.1145/344779.344801
NR 22
TC 11
Z9 16
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2009
VL 25
IS 5-7
BP 687
EP 696
DI 10.1007/s00371-009-0344-2
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 438ES
UT WOS:000265539300034
DA 2024-07-18
ER

PT J
AU Mao, ZH
   Cao, G
   Zhao, MX
AF Mao Zhihong
   Cao Guo
   Zhao Mingxi
TI Robust detection of perceptually salient features on 3D meshes
SO VISUAL COMPUTER
LA English
DT Article
DE Principal curvature; Principal direction; Crest lines; Contextual
   feature; Robust detection
ID CREST LINES; CURVATURE
AB Crest lines, curves on a surface along which the surface bends sharply, are powerful shape descriptors. Crest lines and their subsets have numerous applications in image analysis, face recognition, analysis and registration of anatomical structures, surface segmentation and non-photorealistic rendering. In this paper, a method is proposed for robust detection of crest lines. The proposed method is based on contextual information, the attributes of neighboring points. So it provides a basis of robustly detecting salient crest lines corresponding to potentially important features. Consequently, the algorithm is immune to noisy mesh and textured mesh with repeated bumps. Comparative results indicate that our algorithm yields favorable detection results and is effective.
C1 [Mao Zhihong] Sun Yat Sen Univ, Dept Comp Sci, Guangzhou 510275, Guangdong, Peoples R China.
   [Cao Guo] Nanjing Univ Sci & Technol, Sch Comp Sci & Technol, Nanjing 210094, Peoples R China.
   [Zhao Mingxi] Motorola Res Ctr China, Shanghai 200030, Peoples R China.
C3 Sun Yat Sen University; Nanjing University of Science & Technology
RP Mao, ZH (corresponding author), Sun Yat Sen Univ, Dept Comp Sci, Guangzhou 510275, Guangdong, Peoples R China.
EM mzh_yu@163.com
RI , Guo/AAC-1388-2022
OI , Guo/0000-0002-2689-0932
FU National Natural Science Foundation [60473109]; NSFC-Guangdong Union
   Foundation [U0735001]; Scientific and Technological project of Zhuhai
   City [04300602]; Open Fund of State Key Lab. of CADCG; Zhejiang
   University, P. R. China; New Teacher Fund of Ministry of Education;
   Guangdong Province Natural Science Foundation of P. R. China
   [8451027501001518]
FX We wish to thank Yutaka Ohtake and Yoshizawa for providing mesh data. We
   also gratefully thank Yutaka Ohatake for his meshviewer. The work is
   supported by National Natural Science Foundation (grant no. 60473109),
   NSFC-Guangdong Union Foundation (grant no. U0735001), Scientific and
   Technological project of Zhuhai City (grant no. 04300602), Open Fund of
   State Key Lab. of CAD&CG, Zhejiang University, P. R. China, New Teacher
   Fund of Ministry of Education and Guangdong Province Natural Science
   Foundation of P. R. China (8451027501001518).
CR Agam G, 2005, IEEE T VIS COMPUT GR, V11, P573, DOI 10.1109/TVCG.2005.69
   [Anonymous], 2001, P IMR 2001 NEWP BEAC
   BELYAEV A, 2005, 11 IMA C MATH SURF 0, P50
   BELYAEV A, 2000, P EUR 00 INT SWITZ, P19
   CAZALS F, 2003, P 2003 EUR ACM SIGGR, P177
   Chen K, 2005, IEEE T PATTERN ANAL, V27, P1552, DOI 10.1109/TPAMI.2005.190
   CHEN X, 1992, LECT NOTES COMPUT SC, V588, P739
   DeCarlo D, 2003, ACM T GRAPHIC, V22, P848, DOI 10.1145/882262.882354
   Eberly D., 1994, Journal of Mathematical Imaging and Vision, V4, P353, DOI 10.1007/BF01262402
   Goldfeather J, 2004, ACM T GRAPHIC, V23, P45, DOI 10.1145/966131.966134
   Grenander U, 1998, Q APPL MATH, V56, P617, DOI 10.1090/qam/1668732
   HALLINAN GGG, 1999, 2 3 DIMENSIONAL PATT
   Hameiri E, 2003, IEEE T SYST MAN CY B, V33, P626, DOI 10.1109/TSMCB.2003.814304
   Interrante V, 1995, VISUALIZATION '95 - PROCEEDINGS, P52, DOI 10.1109/VISUAL.1995.480795
   Lee CH, 2005, P ACM SIGGRAPH, P659
   Lengagne R., 1996, Proceedings of the 13th International Conference on Pattern Recognition, P9, DOI 10.1109/ICPR.1996.545982
   LOPEZ LF, 1998, P ECCV 98, P156
   LUKACS G, 1998, MATH METHODS CURVES, V2, P319
   Meyer M., 2002, VISUALIZATION MATH, V6, P35, DOI DOI 10.1007/978-3-662-05105-4_2
   Monga O, 1997, COMPUT VIS IMAGE UND, V67, P285, DOI 10.1006/cviu.1996.0507
   Ohtake Y, 2004, ACM T GRAPHIC, V23, P609, DOI 10.1145/1015706.1015768
   Pauly M, 2003, COMPUT GRAPH FORUM, V22, P281, DOI 10.1111/1467-8659.00675
   Petitjean S, 2002, ACM COMPUT SURV, V34, P211, DOI 10.1145/508352.508354
   Stylianou G, 2004, IEEE T VIS COMPUT GR, V10, P536, DOI 10.1109/TVCG.2004.24
   Stylianou G, 2004, MATH VISUAL, P139
   TAUBIN G, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P902, DOI 10.1109/ICCV.1995.466840
   THIRION JP, 1993, EXTREMAL MESH UNDERS
   Tong WS, 2005, IEEE T PATTERN ANAL, V27, P434, DOI 10.1109/TPAMI.2005.62
   Yoshizawa Shin., 2005, S SOLID PHYS MODELIN, P227, DOI [10.1145/1060244.1060270, DOI 10.1145/1060244.1060270]
NR 29
TC 4
Z9 5
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2009
VL 25
IS 3
BP 289
EP 295
DI 10.1007/s00371-008-0268-2
PG 7
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 403RH
UT WOS:000263099200008
DA 2024-07-18
ER

PT J
AU Ki, H
   Oh, K
AF Ki, Hyunwoo
   Oh, Kyoungsu
TI A GPU-based light hierarchy for real-time approximate illumination
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 26th International Conference on Computer Graphics
CY JUN 09-11, 2008
CL Istanbul, TURKEY
DE illumination; light tree; image pyramid; graphics hardware; real-time
   rendering
AB Illumination rendering, including environment lighting, indirect illumination, and subsurface scattering, plays an important role in many graphics applications such as games and VR systems. However, it is difficult to run in real-time due to its highly computational cost. We introduce a GPU-based light hierarchy for real-time approximation of the illumination. We store virtual point lights in images and then build the view-independent hierarchy of the lights into image pyramids, with a simple and rapid clustering strategy. We approximate the illumination with small numbers of groups of lights instead of large numbers of individual lights, using a new tree traversal algorithm on programmable graphics hardware. Although we implemented our method without occlusion, we obtained visually good results in many cases. Entire steps run on programmable graphics hardware in real-time without any preprocessing.
C1 [Oh, Kyoungsu] Soongsil Univ, Media Dept, Seoul, South Korea.
C3 Soongsil University
RP Oh, K (corresponding author), Soongsil Univ, Media Dept, Seoul, South Korea.
EM kih@innoace.com; oks@ssu.ac.kr
CR Adelson E. H., 1984, RCA engineer, V29, P33, DOI 10.1.1.59.9419.
   BUNNEL M, 2005, GPU GEMS, V2
   CARR NA, 2003, P ACM SIGGRAPH EUROG, P51
   Clarberg P, 2005, ACM T GRAPHIC, V24, P1166, DOI 10.1145/1073204.1073328
   Cohen-Or Daniel, 1993, Comput. Graph. Forum, V12, P363
   DACHSBACHER C, 2003, P 14 EUR WORKSH REND, P197
   Dachsbacher C., 2005, Proc. Symp. Interactive Graph. and Games, P203, DOI DOI 10.1145/1053427.1053460
   Dachsbacher C., 2006, Proc. Symp. Interactive 3D Graph. and Games, Redwood City, P93, DOI DOI 10.1145/1111411.1111428
   Dachsbacher C, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239512
   HANRAHAN P, 1991, COMP GRAPH, V25, P197
   HANRAHAN P, 1990, P EUR WORKSH PHOT RE, P151
   HASAN M, 2006, COMPUTER GRAPHICS, P1089
   Hasan M, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239477
   Jensen HW, 2001, COMP GRAPH, P511, DOI 10.1145/383259.383319
   JENSEN HW, 2002, P SIGGRAPH 2002, P576
   JEPPE R, 2005, P COMP GRAPH INT 200, P185
   Keller Alexander., 1997, SIGGRAPH 97 P 24 ANN, P49
   Kristensen AW, 2005, ACM T GRAPHIC, V24, P1208, DOI 10.1145/1073204.1073334
   MERTENS T, 2003, P 14 EUR WORKSH REND, P130
   OH K, 2006, P ACM S VIRT REAL SO, P75
   Paquette E, 1998, COMPUT GRAPH FORUM, V17, pC63, DOI 10.1111/1467-8659.00254
   Pellacini F, 2005, ACM T GRAPHIC, V24, P464, DOI 10.1145/1073204.1073214
   Ramamoorthi R, 2001, COMP GRAPH, P497, DOI 10.1145/383259.383317
   Sameer Agarwal, 2003, ACM Transactions on Graphics, V22, P605, DOI 10.1145/882262.882314
   Sloan PP, 2005, ACM T GRAPHIC, V24, P1216, DOI 10.1145/1073204.1073335
   SLOAN PP, 2002, P ACM SIGGRAPH, P527
   SMITS B, 1994, P SIGGRAPH 94, P435
   SYLVAIN L, 2005, GPU GEMS, V2, P595
   Tabellion E, 2004, ACM T GRAPHIC, V23, P469, DOI 10.1145/1015706.1015748
   Walter B, 2005, ACM T GRAPHIC, V24, P1098, DOI 10.1145/1073204.1073318
   Walter B, 2006, ACM T GRAPHIC, V25, P1081, DOI 10.1145/1141911.1141997
   WILLIAMSON VM, 1983, INT REV CYTOL, V83, P1, DOI 10.1016/S0074-7696(08)61684-8
NR 32
TC 5
Z9 6
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2008
VL 24
IS 7-9
BP 649
EP 658
DI 10.1007/s00371-008-0245-9
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 322OI
UT WOS:000257384800020
DA 2024-07-18
ER

PT J
AU Yang, CK
   Yang, HL
AF Yang, Chuan-Kai
   Yang, Hui-Lin
TI Realization of Seurat's pointillism via non-photorealistic rendering
SO VISUAL COMPUTER
LA English
DT Article
DE NPR; pointillism; halos; complementary colors; dithering
AB Photorealistic rendering is one of the oldest and most important research areas in computer graphics. More recently, the concept of non-photorealistic rendering has been proposed as an alternative with important advantages for numerous application areas. The popularity of non-photorealism can be mainly attributed to its simplicity, which could potentially lead to a state of aesthetics and succinctness. Reality often presents too many details and too much complexity, thus offsetting the observation of the essence of objects, and objects' interaction with lights. Based on a similar belief, impressionism focuses primarily on conveying the interaction of light and shadows without emphasizing the fine details of a scene. In recent years, there has been a trend towards simulating impressionism with computers. Among the various styles of impressionism, we are particularly interested in simulating the style of pointillism, especially the style presented by Georges-Pierre Seurat, deemed the founder of pointillism. The reason his style attracts us is twofold. First, the painting process of pointillism is extremely laborious, so unloading the main proportion of the manual painting task is mostly desired. Second, though several existing general-purposed algorithms may be able to approximate pointillism with point-like strokes, some delicate features frequently observed in Seurat's paintings are still not satisfactorily reflected. To achieve simulating Seurat's painting style, we have made careful observations of all accessible Seurat's paintings and extracted from them some important features, such as the relatively few primitive colors, color juxtaposition, point sizes, and, in particular, the effects of complementary colors and halos. These features have been more successfully simulated and results are comparable with not only Seurat's existing paintings, but also with previous attempted simulations.
C1 [Yang, Chuan-Kai; Yang, Hui-Lin] Natl Taiwan Univ Sci & Technol, Dept Informat Management, Taipei 10607, Taiwan.
C3 National Taiwan University of Science & Technology
RP Yang, CK (corresponding author), Natl Taiwan Univ Sci & Technol, Dept Informat Management, 43 Sect 4,Keelung Rd, Taipei 10607, Taiwan.
EM ckyang@cs.ntust.edu.tw
RI Li, Mengqi/AAG-6804-2021
CR [Anonymous], SIGGRAPH
   APPEL A, 1979, COMPUTER GRAPHICS, V13, P151
   BOX GEP, 1958, ANN MATH STAT, V29, P610, DOI 10.1214/aoms/1177706645
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851
   Chevreul M.E., 1967, The principles of harmony and contrast of colors
   Chu NSH, 2004, IEEE COMPUT GRAPH, V24, P76, DOI 10.1109/MCG.2004.37
   Deussen O, 2000, COMPUT GRAPH FORUM, V19, pC41, DOI 10.1111/1467-8659.00396
   DOOLEY D, 1990, 1990 S INT 3D GRAPH, P77
   GOOCH AA, 2005, SIGGRAPH 2005, P634
   Haeberli P., 1990, SIG GRAPH, P207
   HALLER M, 2004, P ACM SIGGRAPH INT C, P289
   HAUSNER A, 2005, COMPUTER GRAPHICS IM
   Hays J., 2004, PROC NPAR 01, P113
   HEALEY CG, 2004, ACM T GRAPH, V23
   Hertzmann A, 2001, COMP GRAPH, P327, DOI 10.1145/383259.383295
   HERTZMANN A, 1998, SIGGRAPH 1998
   JING L, 2005, INT S VIS COMP 2005, P1
   LEE J, 2001, COMPUT GRAPH, V25
   LUONG T, 2005, GRAPHICS INTEFACE 20, P233
   MEIER BJ, 1996, SIGGRAPH 96, P477
   NIELSON S, 2005, ACM T GRAPH, V24
   PRATT W.K., 1991, DIGITAL IMAGE PROCES, V2
   Rood OgdenN., 1879, Modern Chromatics with Applications to Art and Industry
   RUSINKIEWICZ S, 2006, SIGGRAPH 2006, P1199
   Shamos M., 1978, Ph.D. Thesis
   Sklar B., 1988, Digital Communications Fundamentals and Applications, DOI DOI 10.1121/1.3598464
   SNIDER L, 2001, HIST TEACHER, V35
   Strothotte T, 2002, NONPHOTOREALISTIC CO
   Van Laerhoven T, 2004, COMPUTER GRAPHICS INTERNATIONAL, PROCEEDINGS, P640, DOI 10.1109/CGI.2004.1309281
   WINKENBACH G, 1994, SIGGRAPH 94 C P, P91
   YELLOTT JI, 1983, SCIENCE, V221, P382, DOI 10.1126/science.6867716
   ZHU Q, 2006, INT WORKSH ADV IM TE
NR 32
TC 22
Z9 23
U1 0
U2 8
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2008
VL 24
IS 5
BP 303
EP 322
DI 10.1007/s00371-007-0183-y
PG 20
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 279NZ
UT WOS:000254363200001
DA 2024-07-18
ER

PT J
AU Kim, S
   Redon, S
   Kim, YJ
AF Kim, Sujeong
   Redon, Stephane
   Kim, Young J.
TI Continuous collision detection for adaptive simulation of articulated
   bodies
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 7th Korea-Israel Bi-National Conference on Geometric Modeling and
   Computer Graphics
CY JAN 29-30, 2007
CL Seoul, SOUTH KOREA
DE continuous collision detection; articulated body dynamics; adaptive
   dynamics; interval arithmetic
ID PARALLEL O(LOG(N)) CALCULATION; RIGID-BODY DYNAMICS; FORMULATION;
   ALGORITHM
AB We perform continuous collision detection (CCD) for articulated bodies where motion is governed by an adaptive dynamics simulation. Our algorithm is based on a novel hierarchical set of transforms that represent the kinematics of an articulated body recursively, as described by an assembly tree. The performance of our CCD algorithm significantly improves as the number of active degrees of freedom in the simulation decreases.
C1 [Kim, Sujeong; Kim, Young J.] Ewha Womans Univ, Dept Comp Sci & Engn, Seoul, South Korea.
   [Redon, Stephane] INRIA Rhone Alpes, i3D, Montbonnot St Martin, France.
C3 Ewha Womans University
RP Kim, YJ (corresponding author), Ewha Womans Univ, Dept Comp Sci & Engn, Seoul, South Korea.
EM kimsujeong@ewhain.net; stephane.redon@inria.fr; kimy@ewha.ac.kr
OI Redon, Stephane/0000-0002-2618-8975; Kim, Young J./0000-0003-2159-4832
CR Abdel-Malek Karim, 2006, International Journal of Shape Modeling, V12, P87, DOI DOI 10.1142/S0218654306000858
   AGARWAL PK, 2001, P 5 WORKSH ALG FDN R, P83
   BAE DS, 1987, MECH STRUCT MACH, V15, P359, DOI 10.1080/08905458708905124
   Baraff D., 1994, Computer Graphics Proceedings. Annual Conference Series 1994. SIGGRAPH 94 Conference Proceedings, P23, DOI 10.1145/192161.192168
   Brandl H., 1986, IFAC Proc., V19, P95
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P200, DOI 10.1109/TPAMI.1986.4767773
   Chenney S., 1997, Proceedings 1997 Symposium on Interactive 3D Graphics, P55, DOI 10.1145/253284.253307
   Choi YK, 2006, IEEE T ROBOT, V22, P213, DOI 10.1109/TRO.2005.862479
   Faure F, 1999, IEEE T VIS COMPUT GR, V5, P268, DOI 10.1109/2945.795217
   Featherstone R, 1999, INT J ROBOT RES, V18, P876, DOI 10.1177/02783649922066628
   Featherstone R., 2000, Proceedings 2000 ICRA. Millennium Conference. IEEE International Conference on Robotics and Automation. Symposia Proceedings (Cat. No.00CH37065), P826, DOI 10.1109/ROBOT.2000.844153
   Featherstone R, 1999, INT J ROBOT RES, V18, P867, DOI 10.1177/02783649922066619
   Featherstone R, 1987, Robot Dynamics Algorithms, P65, DOI DOI 10.1007/978-0-387-74315-8
   HOLLERBACH JM, 1980, IEEE T SYST MAN CYB, V10, P730, DOI 10.1109/TSMC.1980.4308393
   KIM B, 2003, ACM S SOL MOD APPL, P4
   Kim DJ, 1998, IEEE T VIS COMPUT GR, V4, P230, DOI 10.1109/2945.722297
   Kirkpatrick D., 2000, Proc. 16th Annu. Symp. Computational Geometry, P322
   MCMILLAN S, 1995, IEEE T ROBOTIC AUTOM, V11, P606, DOI 10.1109/70.406935
   Moore R. E., 1966, INTERVAL ANAL
   Ortega M, 2007, IEEE T VIS COMPUT GR, V13, P458, DOI 10.1109/TVCG.2007.1028
   Redon S, 2005, IEEE INT CONF ROBOT, P4200
   Redon S, 2004, P IEEE VIRT REAL ANN, P117, DOI 10.1109/VR.2004.1310064
   Redon S, 2002, 2002 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION, VOLS I-IV, PROCEEDINGS, P517, DOI 10.1109/ROBOT.2002.1013411
   Redon S, 2002, COMPUT GRAPH FORUM, V21, P279, DOI 10.1111/1467-8659.t01-1-00587
   Redon S., 2000, Proceedings 2000 ICRA. Millennium Conference. IEEE International Conference on Robotics and Automation. Symposia Proceedings (Cat. No.00CH37065), P3733, DOI 10.1109/ROBOT.2000.845313
   REDON S, 2005, ACM T GRAPH SIGGRAPH, V24
   REDON S, 2005, P ACM S SOL PHYS MOD, P175
   Redon S., 2004, P 9 ACM S SOLID MODE, P145
   SCHWARZER F, 2002, WORKSH ALG FDN ROB W, P25
   ZHANG X, 2006, VISUAL COMPUT, V22, P9
   Zhang XY, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239466
NR 31
TC 5
Z9 5
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2008
VL 24
IS 4
BP 261
EP 269
DI 10.1007/s00371-007-0196-6
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 277UL
UT WOS:000254240100005
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Castro, GG
   Ugail, H
   Willis, P
   Palmer, I
AF Castro, Gabriela Gonzalez
   Ugail, Hassan
   Willis, Philip
   Palmer, Ian
TI A survey of partial differential equations in geometric design
SO VISUAL COMPUTER
LA English
DT Article
DE PDE surfaces; geometric modelling; PDE method
ID VARIATIONAL-PROBLEMS; SURFACES; GENERATE; PARAMETRIZATION
AB Computer-aided geometric design is an area where the improvement of surface generation techniques is an everlasting demand, since faster and more accurate geometric models are required. Traditional methods for generating surfaces were initially mainly based upon interpolation algorithms. Recently, partial differential equations (PDE) were introduced as a valuable tool for geometric modelling, since they offer a number of features from which these areas can benefit. This work summarizes the uses given to PDE surfaces as a surface generation technique together with some other applications to computer graphics.
C1 [Castro, Gabriela Gonzalez; Ugail, Hassan; Palmer, Ian] Univ Bradford, EIMC Dept, Sch Informat, Bradford BD7 1DP, W Yorkshire, England.
   [Willis, Philip] Univ Bath, Media Technol Res Ctr, Dept Comp Sci, Bath BA2 7AY, Avon, England.
C3 University of Bradford; University of Bath
RP Castro, GG (corresponding author), Univ Bradford, EIMC Dept, Sch Informat, Richmond Rd, Bradford BD7 1DP, W Yorkshire, England.
EM G.Gonzalezcastro1@bradford.ac.uk
CR Bajaj CL, 2003, ACM T GRAPHIC, V22, P4, DOI 10.1145/588272.588276
   Barr A. H., 1984, Computers & Graphics, V18, P21
   Bertalmío M, 2001, J COMPUT PHYS, V174, P759, DOI 10.1006/jcph.2001.6937
   Bertalmio M, 2000, COMP GRAPH, P417, DOI 10.1145/344779.344972
   Bloor M.I.G., 1993, COMPUTING S, V8, P21
   BLOOR MIG, 1990, COMPUT AIDED DESIGN, V22, P324, DOI 10.1016/0010-4485(90)90083-O
   BLOOR MIG, 1990, COMPUT AIDED DESIGN, V22, P202, DOI 10.1016/0010-4485(90)90049-I
   Bloor MIG, 1995, J AIRCRAFT, V32, P1269, DOI 10.2514/3.46874
   Bloor MIG, 2005, COMPUT AIDED GEOM D, V22, P203, DOI 10.1016/j.cagd.2004.08.005
   BLOOR MIG, 1989, COMPUT AIDED DESIGN, V21, P165, DOI 10.1016/0010-4485(89)90071-7
   Bloor MIG, 1996, COMPUT AIDED DESIGN, V28, P145, DOI 10.1016/0010-4485(95)00060-7
   CELNIKER G, 1991, COMP GRAPH, V25, P257, DOI 10.1145/127719.122746
   Celniker George., 1992, SI3D 92, P165
   DEKANSKI CW, 1995, J SHIP RES, V39, P108
   Derose T., 1999, SIGGRAPH 1999, P85
   Du H, 2000, COMPUT GRAPH FORUM, V19, pC261, DOI 10.1111/1467-8659.00418
   DU H, 2003, SMI 03, P235
   Du HX, 2005, GRAPH MODELS, V67, P43, DOI 10.1016/j.gmod.2004.06.002
   Du HX, 2004, COMPUT AIDED DESIGN, V36, P1101, DOI 10.1016/j.cad.2004.01.009
   Du HX, 2001, NINTH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P198, DOI 10.1109/PCCGA.2001.962873
   Du HX, 2000, EIGHTH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P213, DOI 10.1109/PCCGA.2000.883943
   Enright D, 2002, ACM T GRAPHIC, V21, P736, DOI [10.1145/566570.566581, 10.1145/566570.566645]
   Farin G., 2001, CURVES SURFACES COMP
   Farlow S.J., 1993, Partial_differential_equations_for_scientists_and_engineers
   FEDKIW R, 2003, SIMULATING NATURAL P
   Foster N, 2001, COMP GRAPH, P23, DOI 10.1145/383259.383261
   Huband J, 2001, MATH ENG IND, V8, P239
   Kubiesa S, 2004, VISUAL COMPUT, V20, P682, DOI 10.1007/s00371-004-0261-3
   Lin V. C., 1981, Computer Graphics, V15, P171, DOI 10.1145/965161.806803
   LOWE TW, 1990, ADV DESIGN AUTOMATIO, V1, P43
   Mémoli F, 2004, J COMPUT PHYS, V195, P263, DOI 10.1016/j.jcp.2003.10.007
   Monterde J, 2006, COMPUT AIDED GEOM D, V23, P208, DOI 10.1016/j.cagd.2005.09.001
   Monterde J, 2004, COMPUT AIDED GEOM D, V21, P697, DOI 10.1016/j.cagd.2004.07.003
   NGUYEN D, 2002, SIGGRAPH 2002 ANN C
   Qin H, 1996, IEEE T VIS COMPUT GR, V2, P85, DOI 10.1109/2945.489389
   Schneider R, 2001, COMPUT AIDED GEOM D, V18, P359, DOI 10.1016/S0167-8396(01)00036-X
   Terzopoulos D., 1987, COMPUT GRAPH, P205, DOI DOI 10.1145/37402.37427
   UAGIL H, 1999, ACM T GRAPHIC, V18, P195
   Ugail H, 2004, COMPUTING, V72, P195, DOI 10.1007/s00607-003-0057-8
   Ugail H, 2003, COMPUT STRUCT, V81, P2601, DOI 10.1016/S0045-7949(03)00321-3
   UGAIL H, 2003, SPINE PDE SURFACE, P366
   Ugail H, 2003, OPTIM ENG, V4, P291, DOI 10.1023/B:OPTE.0000005389.77485.ff
   Westgaard G, 2001, S SOL MOD APPL, P88
   WITKIN A, 1987, COMPUT GRAPH, V21, P225
   Xu GL, 2006, COMPUT AIDED GEOM D, V23, P125, DOI 10.1016/j.cagd.2005.05.004
   You LH, 2004, COMPUT GRAPH-UK, V28, P895, DOI 10.1016/j.cag.2004.08.003
   Zhang JJ, 2004, COMPUT GRAPH FORUM, V23, P311, DOI 10.1111/j.1467-8659.2004.00762.x
NR 47
TC 20
Z9 20
U1 0
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2008
VL 24
IS 3
BP 213
EP 225
DI 10.1007/s00371-007-0190-z
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 255ZA
UT WOS:000252695900005
OA Green Published
DA 2024-07-18
ER

PT J
AU Fukushige, S
   Suzuki, H
AF Fukushige, Shinichi
   Suzuki, Hiromasa
TI Polygon visibility ordering via Voronoi diagrams
SO VISUAL COMPUTER
LA English
DT Article
DE visibility determination; Voronoi diagrams; space subdivision;
   visibility ordering; priority algorithm
AB Visibility determination is one of the oldest problems in computer graphics. The visibility, in terms of back-to-front polygon visibility ordering, is determined by updating a priority list as the viewpoint moves. A new list-priority algorithm, utilizing a property of Voronoi diagrams, is proposed in this paper. The operation is in two phases. First, in a pre-processing phase the scene is divided into Voronoi cells. A sub-list associated with each cell contains references to those polygons that intersect with it. The polygons are assigned a fixed set of view-independent priority orders within the cluster. Last, an interactive phase sorts the clusters according to the depth value of each Voronoi site. The most time-consuming work is performed during the pre-processing phase that only has to be executed once for the scene. Since all the polygons in a cell are pre-computed to obtain the fixed priority order within the cluster, a relatively simple task is left in the interactive phase, which is only to sort the clusters repeatedly when the viewpoint is changed. This method contains performance benefits that make it better shaped than previous BSP based methods.
C1 Osaka Univ, Grad Sch Engn, Suita, Osaka 565, Japan.
   Univ Tokyo, Adv Sci & Technol Res Ctr, Tokyo, Japan.
C3 Osaka University; University of Tokyo
RP Fukushige, S (corresponding author), Osaka Univ, Grad Sch Engn, Suita, Osaka 565, Japan.
EM fukushige@mech.eng.osaka-u.ac.jp
CR Agarwal P. K., 1997, Proceedings of the Thirteenth Annual Symposium on Computational Geometry, P382, DOI 10.1145/262839.263011
   Aurenhammer Franz, 1991, ACM COMPUT SURV, V23, P346
   CATMULL E, 1978, P SIGGRAPH 78, P6
   CHEN HM, 1996, P SIGGRAPH 96 AUG, P55
   Dur A., 2003, Journal of Graphics Tools, V8, P25, DOI 10.1080/10867651.2003.10487592
   Fuchs H., 1983, Computer Graphics, V17, P65, DOI 10.1145/964967.801134
   FUCHS H, 1980, P 7 ANN C COMP GRAPH, P124
   James A, 1998, COMPUT GRAPH FORUM, V17, P55, DOI 10.1111/1467-8659.00215
   MORER P, 1995, COMPUT GRAPH FORUM, V14, P217, DOI 10.1111/1467-8659.1440217
   NAYOR BF, 1981, THESIS U TEXAS DALLA
   Newell Martin E, 1972, P ACM ANN C, V1, P443
   SCHUMACKER RA, 1969, AFHRLTR6914
   SUTHERLAND IE, 1973, P AFIPS NATIONAL COM, P685
   YAO FF, 1980, P 21 IEEE S FDN COMP, P301
NR 14
TC 0
Z9 0
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA 233 SPRING STREET, NEW YORK, NY 10013 USA
SN 0178-2789
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2007
VL 23
IS 7
BP 503
EP 511
DI 10.1007/s00371-007-0121-z
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 182DL
UT WOS:000247485200006
DA 2024-07-18
ER

PT J
AU Wallner, J
   Pottmann, H
   Hofer, M
AF Wallner, Johannes
   Pottmann, Helmut
   Hofer, Michael
TI Fair webs
SO VISUAL COMPUTER
LA English
DT Article
DE curve network; spline interpolation and approximation; variational
   surface design; curves in surfaces; aesthetic meshes; obstacle
   avoidance; texture mapping; mesh fairing
ID INTERPOLATING SCATTERED DATA; FRAMEWORK; SURFACES; SPLINES; CURVES
AB Fair webs are energy-minimizing curve networks. Obtained via an extension of cubic splines or splines in tension to networks of curves, they are efficiently computable and possess a variety of interesting applications. We present properties of fair webs and their discrete counterparts, i.e., fair polygon networks. Applications of fair curve and polygon networks include fair surface design and approximation under constraints such as obstacle avoidance or guaranteed error bounds, aesthetic remeshing, parameterization and texture mapping, and surface restoration in geometric models.
C1 Vienna Univ Technol, Inst Discrete Math & Geometry, A-1040 Vienna, Austria.
C3 Technische Universitat Wien
RP Wallner, J (corresponding author), Vienna Univ Technol, Inst Discrete Math & Geometry, Wiedner Hauptstr 8-10-104, A-1040 Vienna, Austria.
EM wallner@geometrie.tuwien.ac.at; pottmann@geometrie.tuwien.ac.at;
   hofer@geometrie.tuwien.ac.at
RI Hofer, Michael/F-8863-2012
OI Hofer, Michael/0000-0002-1969-9574
CR Alliez P, 2003, ACM T GRAPHIC, V22, P485, DOI 10.1145/882262.882296
   Alliez Pierre., 2005, RECENT ADV REMESHING
   Biermann H, 2002, GRAPH MODELS, V64, P61, DOI 10.1006/gmod.2002.0570
   Blake A., 1998, ACTIVE CONTOURS
   BOHL H, 1999, THESIS U STUTTGART
   Botsch M, 2004, ACM T GRAPHIC, V23, P630, DOI 10.1145/1015706.1015772
   BOTSCH M, 2004, P EUR S GEOM PROC, P189
   Brunnett G., 1993, Surveys on Mathematics for Industry, V3, P1
   Cheng LT, 2002, J COMPUT PHYS, V175, P604, DOI 10.1006/jcph.2001.6960
   Clarenz U, 2004, COMPUT AIDED GEOM D, V21, P727, DOI 10.1016/j.cagd.2004.07.005
   Do Carmo M., 1976, Differential Geometry of Curves and Surfaces
   do Carmo MP., 1992, RIEMANNIAN GEOMETRY
   Floater MS, 2005, MATH VIS, P157, DOI 10.1007/3-540-26808-1_9
   Grace, 1990, SPLINE MODELS OBSERV
   Hofer M, 2004, ACM T GRAPHIC, V23, P284, DOI 10.1145/1015706.1015716
   Hofer M, 2006, IEEE T GEOSCI REMOTE, V44, P2983, DOI 10.1109/TGRS.2006.875451
   Kimmel R., 2003, NUMERICAL GEOMETRY I
   Kobbelt L, 1998, ACM T GRAPHIC, V17, P209, DOI 10.1145/293145.293146
   KOBBELT L, 1998, P SIGGRAPH 98, P105, DOI DOI 10.1145/280814.280831
   KOLB A, 1995, COMPUT AIDED DESIGN, V27, P277, DOI 10.1016/0010-4485(95)91137-A
   KOLB A, 1995, THESIS U ERLANGEN
   LEE SY, 1995, P SIGGRAPH 95, P439
   Levin A, 1999, COMP GRAPH, P57, DOI 10.1145/311535.311541
   LI WC, 2005, P INT C SHAP MOD
   Liu YX, 2004, ACM T GRAPHIC, V23, P368, DOI 10.1145/1015706.1015731
   Mémoli F, 2001, J COMPUT PHYS, V173, P730, DOI 10.1006/jcph.2001.6910
   MORETON HP, 1992, COMP GRAPH, V26, P167, DOI 10.1145/142920.134035
   Myles A, 2005, COMPUT AIDED DESIGN, V37, P139, DOI 10.1016/j.cad.2004.04.004
   Nielson GA, 2004, COMPUT AIDED GEOM D, V21, P751, DOI 10.1016/j.cagd.2004.07.006
   NIELSON GM, 1983, MATH COMPUT, V40, P253, DOI 10.1090/S0025-5718-1983-0679444-7
   NIELSON GM, 1984, ROCKY MT J MATH, V14, P203, DOI 10.1216/RMJ-1984-14-1-203
   Noakes L, 2003, J MATH PHYS, V44, P1436, DOI 10.1063/1.1537461
   POLTHIER K, 2002, THESIS TU BERLIN
   Pottmann H, 2005, COMPUT AIDED GEOM D, V22, P693, DOI 10.1016/j.cagd.2005.06.006
   SCHAEFER S., 2004, SGP 04, P103, DOI DOI 10.1145/1057432.1057447
   Schneider R, 2001, COMPUT AIDED GEOM D, V18, P359, DOI 10.1016/S0167-8396(01)00036-X
   Sequin C. H., 2004, Computer-Aided Design and Applications, V1, P301
   SINGH K., 1998, SIGGRAPH 98, P405, DOI DOI 10.1145/280814.280946
   SURAZHSKY V, 2003, P 12 INT MESH ROUNDT
   Tarini M, 2004, ACM T GRAPHIC, V23, P853, DOI 10.1145/1015706.1015810
   Taubin G., 1995, P 22 ANN C COMP GRAP, P351, DOI DOI 10.1145/218380.218473
   WELCH W, 1992, COMP GRAPH, V26, P157, DOI 10.1145/142920.134033
   Xu GL, 2004, COMPUT AIDED GEOM D, V21, P767, DOI 10.1016/j.cagd.2004.07.007
NR 43
TC 7
Z9 7
U1 0
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2007
VL 23
IS 1
BP 83
EP 94
DI 10.1007/s00371-006-0088-1
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 113RB
UT WOS:000242613900008
DA 2024-07-18
ER

PT J
AU Laga, H
   Takahashi, H
   Nakajima, M
AF Laga, H
   Takahashi, H
   Nakajima, M
TI Spherical parameterization and geometry image-based 3D shape similarity
   estimation (CGS 2004 special issue)
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Computer Graphics International Conference (CGI 2004)
CY JUN 16-19, 2004
CL Crete, GREECE
SP SCI
DE 3D shape matching; spherical parameterization; geometry images;
   spherical harmonics
AB In this paper, we describe our preliminary findings in applying the spherical parameterization and geometry images to the task of 3D shape matching. View-based techniques compare 3D objects by comparing their 2D projections. However, it is not trivial to choose the number of views and their settings. Geometry images overcome these limitations by mapping the entire object onto a spherical or planar domain. We make use of this property to derive a rotation invariant shape descriptor. Once the geometry image encoding the object's geometric properties is computed, a 1D rotation invariant descriptor is extracted using the spherical harmonic analysis. The parameterization process guarantees the scale invariance, while its coarse-to-fine nature allows the comparison of objects at different scales. We demonstrate and discuss the efficiency of our approach on a collection of 120 three-dimensional models.
C1 Tokyo Inst Technol, Grad Sch Informat Sci, Meguro Ku, Tokyo 1528552, Japan.
   Natl Inst Informat, Tokyo, Japan.
C3 Tokyo Institute of Technology; Research Organization of Information &
   Systems (ROIS); National Institute of Informatics (NII) - Japan
RP Tokyo Inst Technol, Grad Sch Informat Sci, Meguro Ku, W8-64,2-12-1 Ookayama, Tokyo 1528552, Japan.
EM hamid@img.cs.titech.ac.jp; rocky@img.cs.titech.ac.jp;
   nakajima@img.cs.titech.ac.jp
RI Laga, Hamid/B-5116-2012
CR [Anonymous], 1997, THESIS CARNEGIE MELL
   Bespalov D., 2003, Proceedings of the Eighth ACM Symposium on Solid Modeling and Applications, P208, DOI DOI 10.1145/781606.781638
   Castelli V., 2002, Image Databases: Search and Retrieval of Digital Imagery
   Chen DY, 2003, COMPUT GRAPH FORUM, V22, P223, DOI 10.1111/1467-8659.00669
   Cyr CM, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P254, DOI 10.1109/ICCV.2001.937526
   Floater MS, 2005, MATH VIS, P157, DOI 10.1007/3-540-26808-1_9
   Foote J, 1999, MULTIMEDIA SYST, V7, P2, DOI 10.1007/s005300050106
   Funkhouser T, 2003, ACM T GRAPHIC, V22, P83, DOI 10.1145/588272.588279
   Gagvani N, 1999, GRAPH MODEL IM PROC, V61, P149, DOI 10.1006/gmip.1999.0495
   GU X, 2002, SIGGRAPH 02, P355, DOI DOI 10.1145/566570.566589
   Gu XF, 2004, IEEE T MED IMAGING, V23, P949, DOI 10.1109/TMI.2004.831226
   Gu XF, 2003, LECT NOTES COMPUT SC, V2732, P172
   Hilaga M, 2001, COMP GRAPH, P203, DOI 10.1145/383259.383282
   HOPPE H, 1996, SIGGRAPH 96, P99, DOI DOI 10.1145/237170.237216
   Iyer N, 2005, COMPUT AIDED DESIGN, V37, P509, DOI 10.1016/j.cad.2004.07.002
   JACOBS C, 1995, SIGGRAPH, P277, DOI DOI 10.1145/218380.218454
   Laga H, 2004, COMPUTER GRAPHICS INTERNATIONAL, PROCEEDINGS, P490
   Löffler J, 2000, IEEE INFOR VIS, P82, DOI 10.1109/IV.2000.859741
   MIN P, 2003, WEB3D S, P7
   Ohbuchi R, 2003, 11TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P293, DOI 10.1109/PCCGA.2003.1238271
   Ohbuchi R, 2003, THEORY AND PRACTICE OF COMPUTER GRAPHICS, PROCEEDINGS, P97, DOI 10.1109/TPCG.2003.1206936
   Osada R, 2001, INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, PROCEEDING, P154, DOI 10.1109/SMA.2001.923386
   Praun E, 2003, ACM T GRAPHIC, V22, P340, DOI 10.1145/882262.882274
   Sander PV, 2001, COMP GRAPH, P409, DOI 10.1145/383259.383307
   Shum HY, 1996, PROC CVPR IEEE, P526, DOI 10.1109/CVPR.1996.517122
   Sundar H, 2003, SMI 2003: SHAPE MODELING INTERNATIONAL 2003, PROCEEDINGS, P130, DOI 10.1109/smi.2003.1199609
   Tangelder JWH, 2004, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, P145, DOI 10.1109/SMI.2004.1314502
   TEAGUE MR, 1980, J OPT SOC AM, V70, P920, DOI 10.1364/JOSA.70.000920
   Veltkamp R., 2001, CONTENT BASED IMAGE
   Veltkamp RC, 2001, INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, PROCEEDING, P188, DOI 10.1109/SMA.2001.923389
   ZHANG D, 1999, 2 INT C 3D DIG IM MO, P209
   ZHANG D, 1999, 1999 C COMP VIS PATT, P2524
NR 32
TC 4
Z9 7
U1 0
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2006
VL 22
IS 5
BP 324
EP 331
DI 10.1007/s00371-006-0010-x
PG 8
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 041PQ
UT WOS:000237468500005
DA 2024-07-18
ER

PT J
AU Qiu, J
   Seah, HS
   Tian, F
   Wu, ZK
   Chen, Q
AF Qiu, J
   Seah, HS
   Tian, F
   Wu, ZK
   Chen, Q
TI Feature- and region-based auto painting for 2D animation
SO VISUAL COMPUTER
LA English
DT Article
DE cel animation; auto coloring; region matching; master frame; topology
AB This paper presents a novel approach to automating the entire coloring process in traditional cel animation production. A feature-based region-matching algorithm is proposed. It first matches a set of master frames to construct correspondences between regions in the master frames and extract a stable topology. The first frame of each scene is then colored based on the set of master frames. With the painted first frame and established stable topology, each region in subsequent frames is matched with regions in the previous frames and colored. Compared with other algorithms, our approach is able to handle bigger changes between frames and automatically color the first frame of each sequence.
C1 Nanyang Technol Univ, Ctr Adv Media Technol, Singapore 639798, Singapore.
C3 Nanyang Technological University
RP Nanyang Technol Univ, Ctr Adv Media Technol, Nanyang Ave, Singapore 639798, Singapore.
EM qiujie@pmail.ntu.edu.sg
RI Li, Mengqi/AAG-6804-2021; Seah, Hock Soon/A-3673-2011; Seah, Hock
   Soon/AAK-9900-2020
OI Seah, Hock Soon/0000-0003-2699-7147
CR ABLAN D, 2003, DIGITAL DIRECTING TY
   [Anonymous], 1995, P 22 ANN C COMP GRAP, DOI DOI 10.1145/218380.218417
   ARREBOLA F, 2002, ELECTRON LETT, V38, P699
   Awcock G.J., 1995, APPL IMAGE PROCESSIN
   CATMULL E, 1978, P ACM SIGGRAPH, P564
   Chang CW, 1997, J VISUAL COMP ANIMAT, V8, P165, DOI 10.1002/(SICI)1099-1778(199707)8:3<165::AID-VIS157>3.0.CO;2-2
   Di Fiore F, 2001, COMP ANIM CONF PROC, P192, DOI 10.1109/CA.2001.982393
   Di Fiore F, 2002, COMP ANIM CONF PROC, P183, DOI 10.1109/CA.2002.1017532
   DURAND CX, 1991, COMPUT GRAPH, V15, P285, DOI 10.1016/0097-8493(91)90081-R
   GORMAN LO, 1990, COMPUTER VISION GRAP, V51, P195
   HARALICK M, 1993, COMPUTER ROBOT VISIO, V2
   LEVOY M, 1977, ACM COMPUT GRAPH, V11, P65
   LI H, 2003, ANIMATION TECHNIQUES, V1
   LITWINOWICZ PC, 1991, ACM COMPUT GRAPH, V25, P113
   LIU HC, 1992, COMPUTER VISION IMAG, P575
   Madeira JS, 1996, VISUAL COMPUT, V12, P1
   MILAN S, 1996, IMAGE PROCESSING ANA
   PATTERSON JW, 1994, COMPUT J, V37, P829, DOI 10.1093/comjnl/37.10.829
   Qiu J, 2003, 11TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P175
   Rogers D.F., 1998, Procedural Elements for Computer Graphics, V2nd
   Seah HS, 2000, VISUAL COMPUT, V16, P289, DOI 10.1007/s003719900068
   SEAH HS, 1994, INSIGHT COMPUTER GRA, P62
   SMITH SM, 1995, TR95SMS5 DEF RES AG
   SYKORA D, 2004, P 3 INT S NONPH AN R, P121
   Teh C.-H., 1988, Proceedings CVPR '88: The Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.88CH2605-4), P229, DOI 10.1109/CVPR.1988.196241
   THALMANN NM, 1985, COMPUTER ANIMATION T
NR 26
TC 10
Z9 11
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2005
VL 21
IS 11
BP 928
EP 944
DI 10.1007/s00371-005-0307-1
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 974RH
UT WOS:000232608600005
DA 2024-07-18
ER

PT J
AU Wu, H
   Yu, YZ
AF Wu, H
   Yu, YZ
TI Photogrammetric reconstruction of free-form objects with curvilinear
   structures
SO VISUAL COMPUTER
LA English
DT Article
DE image-based modeling; bundle adjustment; curve reconstruction;
   thin-plate splines; constrained Delaunay triangulation
AB The shapes of many natural or man-made objects have curve features. The images of such curves usually do not have sufficient distinctive features to apply conventional feature-based reconstruction algorithms. In this paper, we introduce a photogrammetric method for recovering free-form objects with curvilinear structures. Our method chooses to obtain the topology and geometry of a sparse 3D wireframe of the object first instead of directly recovering a surface or volume model. Surface patches covering the object are then constructed to interpolate the curves in this wireframe while satisfying certain heuristics such as minimal bending energy. The result is an object surface model with curvilinear structures from a sparse set of images. We can produce realistic texture-mapped renderings of the object model from arbitrary viewpoints. Reconstruction results on multiple real objects are presented to demonstrate the effectiveness of our approach.
C1 Univ Illinois, Siebel Ctr Comp Sci, Urbana, IL 61801 USA.
C3 University of Illinois System; University of Illinois Urbana-Champaign
EM yyz@cs.uiuc.edu
RI YU, YIZHOU/D-1603-2013; /F-3345-2010
OI /0000-0002-0470-5548
CR [Anonymous], 1988, An introduction to solid modeling
   [Anonymous], AUTOMATIC EXTRACTION
   BERTHILSSON R, 1997, IEEE C COMP VIS PATT
   BERTHILSSON R, 1999, INT C COMP VIS
   CIPOLLA R, 1992, INT J COMPUT VISION, V9, P83, DOI 10.1007/BF00129682
   Debevec P. E., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P11, DOI 10.1145/237170.237191
   Eck M., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P325, DOI 10.1145/237170.237271
   Faugeras O., 1993, Three-dimensional computer vision: a geometric viewpoint
   GIBLIN P, 1986, INT C COMP VIS LOND, P136
   Grace, 1990, SPLINE MODELS OBSERV
   Hartley R, 2000, MULTIPLE VIEW GEOMET
   Hoppe H., 1994, PIECEWISE SMOOTH SUR, P295
   KAMINSKI J, 2001, INT C COMP VIS
   Krishnamurthy V., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P313, DOI 10.1145/237170.237270
   Liebowitz D., 1999, P EUR
   LONGUETHIGGINS HC, 1981, NATURE, V293, P133, DOI 10.1038/293133a0
   Meinguet J., 1979, J. Applied Math. Physics(ZAMP), V5, P439
   Murray RM, 1994, MATH INTRO ROBOTIC M
   PAPADOPOULO T, 1996, EUR C COMP VIS, P696
   Polak E., 1997, OPTIMIZATION, DOI DOI 10.1007/978-1-4612-0663-7
   POULIN P, 1998, EUR WORKSH REND
   Powell M., 1995, Computational Techniques and Applications
   Ramamoorthi R, 2001, COMP GRAPH, P117, DOI 10.1145/383259.383271
   ROBERT L, 1996, EUR C COMP VIS
   Sato Y., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P379, DOI 10.1145/258734.258885
   SCHMID C, 1998, EUR C COMP VIS
   Soucy M, 1996, VISUAL COMPUT, V12, P503, DOI 10.1007/s003710050082
   Sullivan S, 1998, IEEE T PATTERN ANAL, V20, P1091, DOI 10.1109/34.722621
   Triggs B., 2000, Vision Algorithms: Theory and Practice. International Workshop on Vision Algorithms. Proceedings (Lecture Notes in Computer Science Vol. 1883), P298
   TSAI RY, 1987, IEEE T ROBOTIC AUTOM, V3, P323, DOI 10.1109/JRA.1987.1087109
   TUBIC D, 2003, IEEE INT C COMP VIS
   YU Y, 2001, IEEE T VISUAL COMPUT, V7
   Yu YZ, 1999, COMP GRAPH, P215
   ZHANG L, 2001, P COMP VIS PATT REC
   ZORIN D, COMP GRAPH SIGGRAPH, P189
NR 35
TC 6
Z9 18
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2005
VL 21
IS 4
BP 203
EP 216
DI 10.1007/s00371-005-0281-7
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 928MD
UT WOS:000229274900001
DA 2024-07-18
ER

PT J
AU Li, TY
   Hsu, WH
AF Li, TY
   Hsu, WH
TI A data management scheme for effective walkthrough in large-scale
   virtual environments
SO VISUAL COMPUTER
LA English
DT Article
DE large-scale virtual environment; spatial data management; effective
   walkthrough; prefetching; caching
ID POLYHEDRAL OBJECTS; CACHE MANAGEMENT
AB As the complexity of virtual environments increases, it becomes a critical issue to maintain the quality of a walkthrough experience. In this paper, we propose an effective data management scheme to address this issue in client-server architecture. First, we propose using real-time scene management to manage the computing resources on the client side by reducing the amount of transmitted geometry data. Second, we propose a prioritized most likelihood movement model to prefetch potential future objects based on the user's current motion intention. Lastly, a hybrid coherence cache model is proposed to take advantages of both the temporal and spatial localities of the walkthrough process. We have done extensive experiments to demonstrate how these techniques can improve the effectiveness of walkthrough in a large virtual environment.
C1 Natl Chengchi Univ, Dept Comp Sci, Taipei 11623, Taiwan.
C3 National Chengchi University
RP Natl Chengchi Univ, Dept Comp Sci, 64,Sec 2,Zhih Nan Rd, Taipei 11623, Taiwan.
EM li@cs.nccu.edu.tw; g8804@cs.nccu.edu.tw
CR AIREY JM, 1990, P S INT 3D GRAPH, P41
   [Anonymous], P ACM SIGGRAPH 91 JU
   BERG M, 1997, COMPUTATIONAL GEOMET, P20
   Chan A, 2001, COMPUTER GRAPHICS INTERNATIONAL 2001, PROCEEDINGS, P139, DOI 10.1109/CGI.2001.934668
   Chan BY, 1998, PROC INT CONF DATA, P54, DOI 10.1109/ICDE.1998.655757
   Chim JHP, 1998, P IEEE INT FORUM RES, P66, DOI 10.1109/ADL.1998.670381
   Coorg S., 1996, Proceedings of the Twelfth Annual Symposium on Computational Geometry, FCRC '96, P78, DOI 10.1145/237218.237242
   DEHAEMER MJ, 1991, COMPUT GRAPH, V15, P175, DOI 10.1016/0097-8493(91)90071-O
   FRANKLIN MJ, 1992, PROC INT CONF VERY L, P596
   FUNKHOUSER A, 1992, P 1992 S INT 3D GRAP, P11
   GIGUS Z, 1990, IEEE T PATTERN ANAL, V12, P113, DOI 10.1109/34.44399
   GIGUS Z, 1991, IEEE T PATTERN ANAL, V13, P542, DOI 10.1109/34.87341
   Hoppe H., 1993, Computer Graphics Proceedings, P19, DOI 10.1145/166117.166119
   KATZ A, 1994, P WORKSH STAND INT D, P5
   LIU X, 1996, MULT SCLER, V1, P2
   Macedonia M.R., 1994, PRESENCE, V3, P265, DOI 10.1162/pres.1994.3.4.265
   Park Sungju., 2001, VRST 01, P121
   Taubin G, 1998, ACM T GRAPHIC, V17, P84, DOI 10.1145/274363.274365
NR 18
TC 5
Z9 8
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2004
VL 20
IS 10
BP 624
EP 634
DI 10.1007/s00371-004-0264-0
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 876TH
UT WOS:000225520000002
DA 2024-07-18
ER

PT J
AU Clarenz, U
   Griebel, M
   Rumpf, M
   Schweitzer, MA
   Telea, A
AF Clarenz, U
   Griebel, M
   Rumpf, M
   Schweitzer, MA
   Telea, A
TI Feature sensitive multiscale editing on surfaces
SO VISUAL COMPUTER
LA English
DT Article
DE surface processing; algebraic multigrid; multiscale feature detection
ID INTERPOLATION; DECOMPOSITION
AB A novel editing method for large triangular meshes is presented. We detect surface features, such as edge and corners, by computing local zero and first surface moments, using a robust and noise resistant method. The feature detection is encoded in a finite element matrix, passed to an algebraic multigrid (AMG) algorithm. The AMG algorithm generates a matrix hierarchy ranging from fine to coarse representations of the initial fine grid matrix. This hierarchy comes along with a corresponding multiscale of basis functions, which reflect the surface features on all hierarchy levels. We consider either these basis functions or distinct sets from an induced multiscale domain decomposition as handles for surface manipulation. We present a multiscale editor which enables Boolean operations on this domain decomposition and simply algebraic operations on the basis functions. Users can interactively design their favorite surface handles by simple grouping operations on the multiscale of domains. Several applications on large meshes underline the effectiveness and flexibility of the presented tool.
C1 Eindhoven Univ Technol, Dept Math & Comp Sci, NL-5600 MB Eindhoven, Netherlands.
   Univ Duisburg Gesamthsch, Fachbereich Math, D-47048 Duisburg, Germany.
   Univ Bonn, Inst Angew Math, D-53115 Bonn, Germany.
C3 Eindhoven University of Technology; University of Duisburg Essen;
   University of Bonn
RP Eindhoven Univ Technol, Dept Math & Comp Sci, Den Dolech 2, NL-5600 MB Eindhoven, Netherlands.
EM alext@win.tue.nl
OI Rumpf, Martin/0000-0001-6049-9390; Schweitzer, Marc
   Alexander/0009-0007-7773-5186
CR ALVAREZ L, 1993, ARCH RATION MECH AN, V123, P199, DOI 10.1007/BF00375127
   Biermann H, 2001, COMP GRAPH, P185, DOI 10.1145/383259.383280
   Brandt A., 1982, ALGEBRAIC MULTIGRID
   Brandt A., 1984, Sparsity and Its Applications, P257
   BRANDT A, 1983, PRELIMINARY P INT MU
   BRANNIGAN A, 1986, AUST NZ J CRIMINOL, V19, P23, DOI 10.1016/0096-3003(86)90095-0
   Brezina M, 2000, SIAM J SCI COMPUT, V22, P1570, DOI 10.1137/S1064827598344303
   CASELLES V, 1993, NUMER MATH, V66, P1, DOI 10.1007/BF01385685
   CHARTIER TP, 2002, 7 COPP MOUNT C IT ME, V2
   Chazelle B, 1997, COMP GEOM-THEOR APPL, V7, P327, DOI 10.1016/S0925-7721(96)00024-7
   Clarenz U, 2000, IEEE VISUAL, P397, DOI 10.1109/VISUAL.2000.885721
   CLARENZ U, 2004, IN PRESS IEEE T VIS
   DERICHE R, 1987, INT J COMPUT VISION, V1, P167, DOI 10.1007/BF00123164
   Desbrun M, 2000, PROC GRAPH INTERF, P145
   do Carmo M., 1993, RIEMANNIAN GEOMETRY, V2nd
   Grauschopf T, 1997, APPL NUMER MATH, V23, P63, DOI 10.1016/S0168-9274(96)00062-1
   Gregory A, 1998, COMP ANIM CONF PROC, P64, DOI 10.1109/CA.1998.681909
   Guskov I, 1999, COMP GRAPH, P325, DOI 10.1145/311535.311577
   Hubeli A, 2001, IEEE VISUAL, P287, DOI 10.1109/VISUAL.2001.964523
   Kobbelt L., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P105, DOI 10.1145/280814.280831
   Koren Y, 2002, INFOVIS 2002: IEEE SYMPOSIUM ON INFORMATION VISUALIZATION 2002, P137, DOI 10.1109/INFVIS.2002.1173159
   Lee A. W. F., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P95, DOI 10.1145/280814.280828
   Lévy B, 2002, ACM T GRAPHIC, V21, P362, DOI 10.1145/566570.566590
   Mangan AP, 1999, IEEE T VIS COMPUT GR, V5, P308, DOI 10.1109/2945.817348
   MORETON HP, 1992, COMP GRAPH, V26, P167, DOI 10.1145/142920.134035
   Mundy J., 1992, GEOMETRIC INVARIANCE
   PERONA P, 1990, IEEE T PATTERN ANAL, V12, P629, DOI 10.1109/34.56205
   Ruge J. W., 1985, MULTIGRID METHODS IN
   Sharon E, 2000, PROC CVPR IEEE, P70, DOI 10.1109/CVPR.2000.855801
   TAUBIN G, 1992, THESIS BROWN U
   TROTTENBERG U, 2001, INTRO ALGEBRAIC MULT, P413
   Weickert J, 1996, Z ANGEW MATH MECH, V76, P283
   Wu JH, 2001, NINTH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P12, DOI 10.1109/PCCGA.2001.962853
   Zorin D., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P259, DOI 10.1145/258734.258863
   Zuckerberger E, 2002, COMPUT GRAPH-UK, V26, P733, DOI 10.1016/S0097-8493(02)00128-0
   Zwicker M, 2002, ACM T GRAPHIC, V21, P322, DOI 10.1145/566570.566584
NR 36
TC 25
Z9 32
U1 1
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2004
VL 20
IS 5
BP 329
EP 343
DI 10.1007/s00371-004-0245-3
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 842GL
UT WOS:000222991600004
DA 2024-07-18
ER

PT J
AU Li, W
   Wei, XM
   Kaufman, A
AF Li, W
   Wei, XM
   Kaufman, A
TI Implementing lattice Boltzmann computation on graphics hardware
SO VISUAL COMPUTER
LA English
DT Article
DE graphics hardware; lattice Boltzmann method; flow simulation
AB The Lattice Boltzmann Model (LBM) is a physically-based approach that simulates the microscopic movement of fluid particles by simple, identical, and local rules. We accelerate the computation of the LBM on general-purpose graphics hardware, by grouping particle packets into 2D textures and mapping the Boltzmann equations completely to the rasterization and frame buffer operations. We apply stitching and packing to further improve the performance. In addition, we propose techniques, namely range scaling and range separation, that systematically transform variables into the range required by the graphics hardware and thus prevent overflow. Our approach can be extended to acceleration of the computation of any cellular automata model.
C1 SUNY Stony Brook, Ctr Visual Comp, Stony Brook, NY 11794 USA.
   SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11794 USA.
C3 State University of New York (SUNY) System; State University of New York
   (SUNY) Stony Brook; State University of New York (SUNY) System; State
   University of New York (SUNY) Stony Brook
RP SUNY Stony Brook, Ctr Visual Comp, Stony Brook, NY 11794 USA.
EM liwei@cs.sunysb.edu; wxiaomin@cs.sunysb.edu; ari@cs.sunysb.edu
CR Cercignani C., 1990, Mathematical Methods in Kinetic Theory, V2nd ed.
   Chen S, 1998, ANNU REV FLUID MECH, V30, P329, DOI 10.1146/annurev.fluid.30.1.329
   Fang SF, 2000, COMPUT GRAPH-UK, V24, P433, DOI 10.1016/S0097-8493(00)00038-8
   HARRIS M, 2002, SIGGRAPH EUR WORKSH, P109
   Heidrich W., 1999, ACM S INTERACTIVE 3D, P127
   Hoff K, 2000, ANN WB CONF DEV ECON, P145
   Hoff KE, 1999, COMP GRAPH, P277, DOI 10.1145/311535.311567
   Hopf M., 1999, Proceedings Visualization '99 (Cat. No.99CB37067), P471, DOI 10.1109/VISUAL.1999.809934
   HOPF M, 2000, ACCELERATING MORPHOL, P337
   Jobard B, 2000, IEEE VISUAL, P155, DOI 10.1109/VISUAL.2000.885689
   KANDHAI BD, 1999, THESIS U AMSTERDAM
   LARSEN ES, 2001, INT C HIGH PERF COMP
   MUDERS D, 1995, THESIS U BONN
   MUELLER K, 1999, SPIE MED IM C
   Peercy MS, 2000, COMP GRAPH, P425, DOI 10.1145/344779.344976
   Proudfoot K, 2001, COMP GRAPH, P159, DOI 10.1145/383259.383275
   Purcell TJ, 2002, ACM T GRAPHIC, V21, P703, DOI 10.1145/566570.566640
   REZKSALAMA C, 2001, P SIGGRAPH EUR WORKS
   TRENDALL C, 2000, 11 EUR WORKSH REND B, P287
   Wei XM, 2002, VIS 2002: IEEE VISUALIZATION 2002, PROCEEDINGS, P227, DOI 10.1109/VISUAL.2002.1183779
   WEISKOPF D, 2001, WORKSH VIS MOD VIS V, P439
NR 21
TC 112
Z9 153
U1 1
U2 12
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2003
VL 19
IS 7-8
BP 444
EP 456
DI 10.1007/s00371-003-0210-6
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 749YL
UT WOS:000186957600002
DA 2024-07-18
ER

PT J
AU Alliez, P
   Laurent, N
   Sanson, H
   Schmitt, F
AF Alliez, P
   Laurent, N
   Sanson, H
   Schmitt, F
TI Efficient view-dependent refinement of 3D meshes using √3-subdivision
SO VISUAL COMPUTER
LA English
DT Article
DE surface reconstruction; adaptive visualization; view-dependent
   refinement; subdivision surfaces; device-driven refinement
ID SURFACES
AB In this paper we introduce an efficient view-dependent refinement technique well suited to adaptive visualization of 3D triangle meshes on a graphic terminal. Our main goal is the design of fast and robust, smooth surface reconstruction from coarse meshes. We demonstrate that the root3-subdivision operator recently introduced by Kobbelt offers many benefits, including view-dependent refinement, removal of polygonal aspect and a highly tunable level of detail adaptation. In particular, we propose a new data structure that requires neither edges nor hierarchies for efficient and reversible view-dependent refinement. Results on various 3D meshes illustrate the relevance of the technique.
C1 France Telecom, R&D DIH HDM, F-35512 Cesson Sevigne, France.
   Ecole Natl Super Telecommun Bretagne, F-75634 Paris 13, France.
C3 Orange SA; IMT - Institut Mines-Telecom; IMT Atlantique
RP Alliez, P (corresponding author), INRIA Sophia Antipolis, BP 93, F-06902 Sophia Antipolis, France.
EM pierre.alliez@sophia.inria.fr; nathalie.laurent@rd.francetelecom.fr;
   henri.sanson@rd.francetelecom.fr; francis.schmitt@enst.fr
CR Alliez P, 2001, COMP GRAPH, P195, DOI 10.1145/383259.383281
   ALLIEZ P, 2001, EUROGRAPHICS, P480
   BAREQUET G, 1999, ACM S COMP GEOM 99, P417
   Benichou F., 1999, Proceedings. Seventh Pacific Conference on Computer Graphics and Applications (Cat. No.PR00293), P60, DOI 10.1109/PCCGA.1999.803349
   CATMULL E, 1978, COMPUT AIDED DESIGN, V10, P350, DOI 10.1016/0010-4485(78)90110-0
   Chaikin GM., 1974, COMPUT GRAPHICS IMAG, V3, P346, DOI DOI 10.1016/0146-664X(74)90028-8
   DeRose T., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P85, DOI 10.1145/280814.280826
   Devillers O, 2000, IEEE VISUAL, P319, DOI 10.1109/VISUAL.2000.885711
   DOO D, 1978, COMPUT AIDED DESIGN, V10, P356, DOI 10.1016/0010-4485(78)90111-2
   DOOLEY D, 1990, P S INT 3D GRAPH, P77
   Eck M., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P325, DOI 10.1145/237170.237271
   GOOCH A, 1998, SIGGRAPH 98, P447
   Gooch B., 1999, Proceedings 1999 Symposium on Interactive 3D Graphics, P31, DOI 10.1145/300523.300526
   GU X, 1999, TR199 HARV U
   Halstead M., 1993, Computer Graphics Proceedings, P35, DOI 10.1145/166117.166121
   Hoppe H., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P189, DOI 10.1145/258734.258843
   HOPPE H, 1996, SIGGRAPH 96, P99, DOI DOI 10.1145/237170.237216
   HOPPE H, 1994, SIGGRAPH 94 C P, P295
   Khodakovsky A, 2000, COMP GRAPH, P271, DOI 10.1145/344779.344922
   Kobbelt L, 2000, COMP GRAPH, P103, DOI 10.1145/344779.344835
   KOENDERINK JJ, 1984, PERCEPTION, V13, P321, DOI 10.1068/p130321
   Levin A, 1999, COMP GRAPH, P57, DOI 10.1145/311535.311541
   Loop C, 1987, THESIS U UTAH
   Markosian L., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P415, DOI 10.1145/258734.258894
   RASKAR R, 1999, S INT 3D GRAPH, P135
   SANDER PV, SIGGRAPH 2000 C P, P327
   Schroder P., 1999, Proceedings. Seventh Pacific Conference on Computer Graphics and Applications (Cat. No.PR00293), P104, DOI 10.1109/PCCGA.1999.803353
   Sederberg T. W., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P387, DOI 10.1145/280814.280942
   Suzuki H., 1999, Proceedings. Seventh Pacific Conference on Computer Graphics and Applications (Cat. No.PR00293), P158, DOI 10.1109/PCCGA.1999.803359
   Touma C, 1998, GRAPHICS INTERFACE '98 - PROCEEDINGS, P26
   Warren J, 1995, SUBDIVISION METHODS
   ZHANG H, 1997, S INT 3D GRAPH, P103
   Zorin D., 2000, SIGGRAPH COURSE NOTE
NR 33
TC 6
Z9 6
U1 0
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2003
VL 19
IS 4
BP 205
EP 221
DI 10.1007/s00371-002-0165-z
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 713KV
UT WOS:000184857800001
DA 2024-07-18
ER

PT J
AU Baek, S
   Lee, S
   Kim, GJ
AF Baek, S
   Lee, S
   Kim, GJ
TI Motion retargeting and evaluation for VR-based training of free motions
SO VISUAL COMPUTER
LA English
DT Article
DE virtual reality; motion training; ghost metaphor; motion retargeting;
   motion evaluation and advice
ID VIRTUAL ENVIRONMENTS
AB Virtual reality (VR) has emerged as one of the important and effective tools for education and training. Most VR-based training systems are situation based, where the trainees are trained for discrete decision making in special situations presented by the VR environments. In contrast, this paper discusses the application of VR to a different class of training, for learning free motion, often required in sports and the arts. We propose a VR-based motion-training framework that contains an intuitive motion-guiding interface, posture-oriented motion retargeting, and an evaluation and advice scheme for corrective feedback. Applications of the proposed framework to simple fencing training and a dance imitation game are demonstrated.
C1 Elect & Telecommun Res Inst, Virtual Real Ctr, Taejon 305350, South Korea.
   Pohang Univ Sci & Technol, Dept Comp Sci & Engn, Pohang 790784, South Korea.
C3 Electronics & Telecommunications Research Institute - Korea (ETRI);
   Pohang University of Science & Technology (POSTECH)
RP Baek, S (corresponding author), Elect & Telecommun Res Inst, Virtual Real Ctr, 161 Gajeong Dong, Taejon 305350, South Korea.
CR [Anonymous], 1980, REACTION TIMES
   [Anonymous], 1991, Motor learning performance from principles to pratice
   BINDIGANVAVALE R, 1998, LECT NOTES COMPUTER, V1537
   BRUDERLIN A, 1995, P 22 ANN C COMP GRAP
   CHOI KJ, 1999, P PAC GRAPH 99 SEOUL
   EERDEN W, 1999, CAREN COMPUTER ASSIS
   EVERETT S, 1998, P INT C VIRT SYST MU
   Gleicher Michael, 1998, P 25 ANN C COMP GRAP
   HODGES LF, 1995, COMPUTER, V28, P27, DOI 10.1109/2.391038
   HODGINS J, 1997, P 24 ANN C COMP GRAP
   Johnson WL, 1998, PRESENCE-VIRTUAL AUG, V7, P523, DOI 10.1162/105474698565929
   Lee Jehee, 1999, P 26 ANN C COMP GRAP
   Lee S, 1997, IEEE T VIS COMPUT GR, V3, P228, DOI 10.1109/2945.620490
   Macedonia M.R., 1994, PRESENCE, V3, P265, DOI 10.1162/pres.1994.3.4.265
   SHAWVER D, 1997, P IEEE VIRT REAL INT
   Witkin A., 1995, P 22 ANN C COMP GRAP
   YANG U, 2001, ACM SIGGRAPH 2001 C
   Yang UY, 2002, PRESENCE-VIRTUAL AUG, V11, P304, DOI 10.1162/105474602317473240
   Youngblut C., 1998, ED USES VIRTUAL REAL
NR 19
TC 23
Z9 26
U1 0
U2 23
PU SPRINGER-VERLAG
PI NEW YORK
PA 175 FIFTH AVE, NEW YORK, NY 10010 USA
SN 0178-2789
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2003
VL 19
IS 4
BP 222
EP 242
DI 10.1007/s00371-003-0194-2
PG 21
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 713KV
UT WOS:000184857800002
DA 2024-07-18
ER

PT J
AU Tripathi, R
   Verma, B
AF Tripathi, Reena
   Verma, Bindu
TI Survey on vision-based dynamic hand gesture recognition
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Dynamic hand gesture; Deep learning; Image Processing; Video processing;
   Classification; Survey on dynamic hand gesture
ID REAL-TIME; BENCHMARK; NETWORKS; DATASET; MODEL
AB To communicate with one another hand, gesture is very important. The task of using the hand gesture in technology is influenced by a very common way humans communicate with the natural environment. The recognizing and finding pose estimation of hand comes under the area of hand gesture analysis. To find out the gesturing hand is very difficult than finding the another part of the human body because the hand is smaller in size. The hand has greater complexity and more challenges due to differences between the cultural or individual factors of users and gestures invented from ad hoc. The complication and divergences of finding hand gestures will deeply affect the recognition rate and accuracy. This paper emphasizes on summary of hand gestures technique, recognition methods, merits and demerits, various applications, available data sets, and achieved accuracy rate, classifiers, algorithm, and gesture types. This paper also scrutinizes the performance of traditional and deep learning methods on dynamic hand gesture recognition.
C1 [Tripathi, Reena; Verma, Bindu] Delhi Technol Univ, Dept Informat Technol, New Delhi, India.
C3 Delhi Technological University
RP Verma, B (corresponding author), Delhi Technol Univ, Dept Informat Technol, New Delhi, India.
EM 8june.reena@gmail.com; bindu.cvision@gmail.com
FU Cyber Forensic and Malware Analysis Lab, Department of Information
   Technology, Delhi Technological University, New Delhi, India
FX The authors would like to acknowledge the Cyber Forensic and Malware
   Analysis Lab, Department of Information Technology, Delhi Technological
   University, New Delhi, India, for providing me necessary resources to
   carry out the research.
CR Abavisani M, 2019, PROC CVPR IEEE, P1165, DOI 10.1109/CVPR.2019.00126
   Abdallah MS, 2023, SENSORS-BASEL, V23, DOI 10.3390/s23010002
   Abu Bakar MZ, 2014, 2014 IEEE INTERNATIONAL CONFERENCE ON CONTROL SYSTEM COMPUTING AND ENGINEERING, P490, DOI 10.1109/ICCSCE.2014.7072768
   Aghajari E., 2014, Int.J. Adv. Res. Comput. Sci. Softw. Eng, V4, P2277
   Alnaim N., 2019, 2019 3 INT S MULT ST, P1
   Ameur S, 2020, ENTERTAIN COMPUT, V35, DOI 10.1016/j.entcom.2020.100373
   Bamwenda J., 2019, Dicle University Journal of Engineering
   Bao PJ, 2017, IEEE T CONSUM ELECTR, V63, P251, DOI 10.1109/TCE.2017.014971
   Benitez-Garcia G, 2021, INT C PATT RECOG, P4340, DOI 10.1109/ICPR48806.2021.9412317
   Benitez-Garcia G, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21020356
   Bilen H, 2018, IEEE T PATTERN ANAL, V40, P2799, DOI 10.1109/TPAMI.2017.2769085
   Breland DS, 2021, IEEE SENS J, V21, P10445, DOI 10.1109/JSEN.2021.3061608
   Cao ZJ, 2022, APPL SCI-BASEL, V12, DOI 10.3390/app12042041
   Caputo F.M., 2017, STAG, P9
   Chen GZ, 2023, COMPLEX INTELL SYST, V9, P1377, DOI 10.1007/s40747-022-00858-8
   Chen XH, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19020239
   Chen XH, 2017, IEEE IMAGE PROC, P2881, DOI 10.1109/ICIP.2017.8296809
   Chen YX, 2019, Arxiv, DOI [arXiv:1907.08871, 10.48550/arXiv.1907.08871, DOI 10.48550/ARXIV.1907.08871]
   Cheok MJ, 2019, INT J MACH LEARN CYB, V10, P131, DOI 10.1007/s13042-017-0705-5
   Chung HY, 2019, IEEE INT CONF INDUST, P853, DOI 10.1109/ICIT.2019.8755038
   Conseil S., 2007, 2007 15th European Signal Processing Conference (EUSIPCO), P1960
   D'Eusanio A, 2020, INFORMATICS-BASEL, V7, DOI 10.3390/informatics7030031
   De Smedt Q., 2017, P WORKSH 3D OBJ RETR, P33
   De Smedt Q, 2019, COMPUT VIS IMAGE UND, V181, P60, DOI 10.1016/j.cviu.2019.01.008
   De Smedt Q, 2016, IEEE COMPUT SOC CONF, P1206, DOI 10.1109/CVPRW.2016.153
   Devanne M, 2015, IEEE T CYBERNETICS, V45, P1340, DOI 10.1109/TCYB.2014.2350774
   Devineau G, 2018, IEEE INT CONF AUTOMA, P106, DOI 10.1109/FG.2018.00025
   Dhingra N, 2019, INT CONF 3D VISION, P491, DOI 10.1109/3DV.2019.00061
   Evangelidis G, 2014, INT C PATT RECOG, P4513, DOI 10.1109/ICPR.2014.772
   Fang YK, 2007, 2007 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOLS 1-5, P995
   Gao Q, 2022, IEEE SENS J, V22, P17421, DOI 10.1109/JSEN.2021.3059685
   Garcia-Hernando G, 2018, PROC CVPR IEEE, P409, DOI 10.1109/CVPR.2018.00050
   Ge LH, 2019, PROC CVPR IEEE, P10825, DOI 10.1109/CVPR.2019.01109
   Grzejszczak T, 2016, MATEC WEB CONF, V56, DOI 10.1051/matecconf/20165602009
   Guo L, 2021, IEEE T HUM-MACH SYST, V51, P300, DOI 10.1109/THMS.2021.3086003
   Hakim NL, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19245429
   Hou JX, 2019, LECT NOTES COMPUT SC, V11134, P273, DOI 10.1007/978-3-030-11024-6_18
   Hsieh C.-C., 2010, 2010 2 INT C SIGN PR, V2, P2
   Hsien-I Lin, 2014, 2014 IEEE International Conference on Automation Science and Engineering (CASE), P1038, DOI 10.1109/CoASE.2014.6899454
   Hu B, 2020, INT J AUTOM COMPUT, V17, P17, DOI 10.1007/s11633-019-1194-7
   Jain R, 2022, VISUAL COMPUT, V38, P1957, DOI 10.1007/s00371-021-02259-3
   Kane L, 2019, PATTERN RECOGN LETT, V120, P24, DOI 10.1016/j.patrec.2019.01.003
   Karabasi M, 2014, INT CONF ADV COMPUT, P195, DOI 10.1109/ACSAT.2013.46
   Karbasi M., 2015, Int. J. Internet Things, V4, P1
   Khandade S.L., 2016, 2016 INT C INV COMP, V1, P1
   Kim T.-K., 2007, 2007 IEEE C COMP VIS, P1
   Köpüklü O, 2019, IEEE INT CONF AUTOMA, P407
   Kollorz Eva, 2008, International Journal of Intelligent Systems Technologies and Applications, V5, P334, DOI 10.1504/IJISTA.2008.021296
   Konstantinidis D, 2018, 3DTV CONF
   Kopuklu Okan, 2020, IEEE Transactions on Biometrics, Behavior, and Identity Science, V2, P85, DOI 10.1109/TBIOM.2020.2968216
   Köpüklü O, 2018, IEEE COMPUT SOC CONF, P2184, DOI 10.1109/CVPRW.2018.00284
   Lai K, 2018, INT C PATT RECOG, P3451, DOI 10.1109/ICPR.2018.8545718
   Li C., 2021, arXiv
   Li DX, 2018, INT C PATT RECOG, P3365, DOI 10.1109/ICPR.2018.8545502
   Li GF, 2019, CLUSTER COMPUT, V22, pS2719, DOI 10.1007/s10586-017-1435-x
   Li JH, 2021, COMPUT INTEL NEUROSC, V2021, DOI 10.1155/2021/5044916
   Li JH, 2019, J MULTIMODAL USER IN, V13, P363, DOI 10.1007/s12193-019-00304-z
   Li J, 2023, VISUAL COMPUT, V39, P2065, DOI 10.1007/s00371-022-02465-7
   Li YK, 2021, COMPUT GRAPH-UK, V97, P191, DOI 10.1016/j.cag.2021.04.017
   Li Y, 2019, EURASIP J IMAGE VIDE, V2019, DOI 10.1186/s13640-019-0476-x
   Liang H, 2013, VISUAL COMPUT, V29, P837, DOI 10.1007/s00371-013-0822-4
   Licsár A, 2005, IMAGE VISION COMPUT, V23, P1102, DOI 10.1016/j.imavis.2005.07.016
   Lin Song, 2013, Advanced Materials Research, V756-759, P4138, DOI 10.4028/www.scientific.net/AMR.756-759.4138
   Liu JB, 2020, PROC CVPR IEEE, P5750, DOI 10.1109/CVPR42600.2020.00579
   Liu L., 2013, 23 INT JOINT C ART I
   Lu W, 2016, IEEE SIGNAL PROC LET, V23, P1188, DOI 10.1109/LSP.2016.2590470
   Ma CY, 2018, VISUAL COMPUT, V34, P1053, DOI 10.1007/s00371-018-1556-0
   Maghoumi M, 2020, LECT NOTES COMPUT SC, V11844, P16, DOI 10.1007/978-3-030-33720-9_2
   Mahanama B, 2020, AUGMENTED HUMAN 2020: PROCEEDINGS OF THE 11TH AUGMENTED HUMAN INTERNATIONAL CONFERENCE, DOI 10.1145/3396339.3396393
   Mahmud H, 2024, VISUAL COMPUT, V40, P11, DOI 10.1007/s00371-022-02762-1
   Mahmud H, 2022, VISUAL COMPUT, V38, P1015, DOI 10.1007/s00371-021-02065-x
   Mishra S., 2021, Infant hand detection and tracking
   Mohammed AAQ, 2022, J AMB INTEL HUM COMP, DOI 10.1007/s12652-021-03546-6
   Mohammed AAQ, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19235282
   Molchanov Pavlo, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P1, DOI 10.1109/CVPRW.2015.7301342
   Molchanov P, 2016, PROC CVPR IEEE, P4207, DOI 10.1109/CVPR.2016.456
   Mujahid A, 2021, APPL SCI-BASEL, V11, DOI 10.3390/app11094164
   Murthy G.R.S., 2009, International Journal of Information Technology and Knowledge Management, V2, P405
   Narayana P, 2018, PROC CVPR IEEE, P5235, DOI 10.1109/CVPR.2018.00549
   Nasri N, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20226451
   Nguyen XS, 2019, PROC CVPR IEEE, P12028, DOI 10.1109/CVPR.2019.01231
   Núñez JC, 2018, PATTERN RECOGN, V76, P80, DOI 10.1016/j.patcog.2017.10.033
   Ohn-Bar E, 2014, IEEE T INTELL TRANSP, V15, P2368, DOI 10.1109/TITS.2014.2337331
   Ohn-Bar E, 2013, IEEE COMPUT SOC CONF, P465, DOI 10.1109/CVPRW.2013.76
   Oreifej O, 2013, PROC CVPR IEEE, P716, DOI 10.1109/CVPR.2013.98
   Oudah M, 2020, J IMAGING, V6, DOI 10.3390/jimaging6080073
   Paul S, 2022, MULTIMED TOOLS APPL, V81, P7285, DOI 10.1007/s11042-021-11745-8
   Pavlovic VI, 1997, IEEE T PATTERN ANAL, V19, P677, DOI 10.1109/34.598226
   Peng S.-H., 2023, IEEE Transactions on Cognitive and Developmental Systems
   Phuong H.N., An approach in building a vision-based hand gesture recognition system
   Pisharady PK, 2015, COMPUT VIS IMAGE UND, V141, P152, DOI 10.1016/j.cviu.2015.08.004
   Pranjali S., 2015, International journal of Inventive Engineering and Science (IJIES)
   Pun CM, 2011, INT J COMPUT INT SYS, V4, P277
   Rautaray SS., 2012, Int. J. UbiComp (IJU), V3, P11
   Sarma D, 2020, Arxiv, DOI arXiv:2007.08847
   Shen XH, 2012, IMAGE VISION COMPUT, V30, P227, DOI 10.1016/j.imavis.2011.11.003
   Shi Lei, 2020, P AS C COMP VIS
   Shotton J, 2013, COMMUN ACM, V56, P116, DOI 10.1145/2398356.2398381
   Stergiopoulou E, 2014, ENG APPL ARTIF INTEL, V35, P54, DOI 10.1016/j.engappai.2014.06.006
   Suarez J., 2012, 2012 RO-MAN: The 21st IEEE International Symposium on Robot and Human Interactive Communication, P411, DOI 10.1109/ROMAN.2012.6343787
   Sulyman Abd Albary, 2017, Journal of Theoretical and Applied Information Technology, V95, P3105
   Sykora P, 2014, AASRI PROC, V9, P19, DOI 10.1016/j.aasri.2014.09.005
   Tang H, 2019, NEUROCOMPUTING, V331, P424, DOI 10.1016/j.neucom.2018.11.038
   Tang M., 2011, Recognizing hand gestures with microsofts kinect
   Tekin B, 2019, PROC CVPR IEEE, P4506, DOI 10.1109/CVPR.2019.00464
   Tsai TH, 2022, IEEE INT SYMP CIRC S, P3265, DOI 10.1109/ISCAS48785.2022.9937780
   Tsinganos P, 2021, NEURAL COMPUT APPL, V33, P2645, DOI 10.1007/s00521-020-05128-7
   Van den Bergh M., 2011, 2011 IEEE WORKSHOP A, P66, DOI DOI 10.1109/WACV.2011.5711485
   Vanden Bergh M., 2009, 2009 WORKSH APPL COM, P1
   Verma B, 2022, J VIS COMMUN IMAGE R, V87, DOI 10.1016/j.jvcir.2022.103554
   Verma B, 2021, MULTIMED TOOLS APPL, V80, P14019, DOI 10.1007/s11042-020-10341-6
   Verma B, 2020, MULTIMED TOOLS APPL, V79, P2213, DOI 10.1007/s11042-019-08266-w
   Verma B, 2018, IET INTELL TRANSP SY, V12, P721, DOI 10.1049/iet-its.2017.0331
   Verma P, 2020, MULTIMEDIA SYST, V26, P671, DOI 10.1007/s00530-020-00677-2
   Wan CD, 2019, PROC CVPR IEEE, P10845, DOI 10.1109/CVPR.2019.01111
   Wang C, 2015, IEEE T MULTIMEDIA, V17, P29, DOI 10.1109/TMM.2014.2374357
   Wang H, 2013, INT J COMPUT VISION, V103, P60, DOI 10.1007/s11263-012-0594-8
   Wang RY., 2009, ACM transactions on graphics (TOG), V28, P1, DOI DOI 10.1145/1531326.1531369
   Wu HY, 2017, VISUAL COMPUT, V33, P1265, DOI 10.1007/s00371-015-1147-2
   Wu HY, 2016, VISUAL COMPUT, V32, P123, DOI 10.1007/s00371-014-1060-0
   Wu XY, 2020, MULTIMED TOOLS APPL, V79, P9193, DOI 10.1007/s11042-019-7193-4
   Yang LC, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20072106
   Yasen M, 2019, PEERJ COMPUT SCI, DOI 10.7717/peerj-cs.218
   Yu ZT, 2021, IEEE T IMAGE PROCESS, V30, P5626, DOI 10.1109/TIP.2021.3087348
   Zeghoud S, 2022, VISUAL COMPUT, V38, P1345, DOI 10.1007/s00371-021-02229-9
   Zhang CJ, 2022, SENSORS-BASEL, V22, DOI 10.3390/s22218535
   Zhang T, 2020, INT J FUZZY SYST, V22, P1330, DOI 10.1007/s40815-020-00825-w
   Zhang W, 2020, VISUAL COMPUT, V36, P2433, DOI 10.1007/s00371-020-01955-w
   Zhang XY, 2019, FUTURE INTERNET, V11, DOI 10.3390/fi11040091
   Zhang YF, 2018, IEEE T MULTIMEDIA, V20, P1038, DOI 10.1109/TMM.2018.2808769
   Zhang ZY, 2018, IEEE SENS J, V18, P3278, DOI 10.1109/JSEN.2018.2808688
   Zhou L, 2020, PATTERN RECOGN, V98, DOI 10.1016/j.patcog.2019.107040
NR 132
TC 0
Z9 0
U1 10
U2 15
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2023 DEC 9
PY 2023
DI 10.1007/s00371-023-03160-x
EA DEC 2023
PG 29
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AH8R7
UT WOS:001117671000001
DA 2024-07-18
ER

PT J
AU Gan, J
   Huo, YQ
AF Gan, Jing
   Huo, Yongqing
TI Ghost-free multi-exposure high dynamic range imaging based on feedback
   network
SO VISUAL COMPUTER
LA English
DT Article
DE High dynamic range imaging; Deghosting; Feedback network; Multi-exposure
   imaging
AB In multi-exposure HDR imaging of dynamic scenes, two challenging problems need to be addressed: one is the removal of ghost artifacts, and the other is the reconstruction of lost information. Researchers usually adopt feed-forward networks to deal with the two problems in the existing deep learning-based multi-exposure HDR imaging methods. However, feedback mechanism plays an important role in human visual system, and feedback networks achieves efficient performance on other imaging tasks. Therefore, we presented a novel deep learning algorithm based on feedback mechanism to achieve ghost-free multi-exposure HDR imaging. To our best knowledge, it is the first time that feedback mechanism is applied to multi-exposure HDR imaging. We first use an attention-guided encoder to adjust the shallow features of the input images to avoid redundant features that interfere with feature fusion. Then, the adjusted features are fused by a feature merging network. It consists of two blocks: a feedback block and a global feature extracting block. In the feedback block, a hidden state is used in a recurrent structure to achieve the feedback mechanism. It enables the feedback block to use high-level features to enhance its feature fusion capability, and the reconfiguration capability of the network gradually improves during iterations. In the global feature extracting block, the extracted overall information increases the receptive field of the network and helps in the recovery of lost details. Extensive experiments show the superior performance of the proposed method in high-quality HDR images reconstruction, and outperforms state-of-the-art methods considered in comparison.
C1 [Gan, Jing; Huo, Yongqing] Univ Elect Sci & Technol China, Sch Informat & Commun Engn, 2006 Xiyuan Ave, Chengdu 611731, Peoples R China.
C3 University of Electronic Science & Technology of China
RP Huo, YQ (corresponding author), Univ Elect Sci & Technol China, Sch Informat & Commun Engn, 2006 Xiyuan Ave, Chengdu 611731, Peoples R China.
EM ganjjing524@163.com; hyq980132@uestc.edu.cn
FU Sichuan Province Science and Technology Achievement Transfer
   Demonstration Project
FX No Statement Available
NR 0
TC 0
Z9 0
U1 4
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2024
VL 40
IS 6
BP 4115
EP 4132
DI 10.1007/s00371-023-03072-w
EA OCT 2023
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TV2X4
UT WOS:001081926000001
DA 2024-07-18
ER

PT J
AU Xu, J
   Gao, J
AF Xu, Jie
   Gao, Jun
TI FLBP: Fechner local binary pattern for face representation
SO VISUAL COMPUTER
LA English
DT Article
DE Fechner's law; Feature descriptor; Logarithmic function; Multi-size
   block
ID TEXTURE CLASSIFICATION; RECOGNITION; LAW
AB Most images are ultimately observed and interpreted by humans, so the ideal image descriptor should take into account the effects of human psychology and psychophysics. In this paper, we develop a novel feature descriptor named Fechner local binary pattern (FLBP) based on the well-known psychological law, Fechner's law. FLBP describes images using mental perception, which is a logarithmic function of the stimulus change, allowing for a more detailed and hierarchical representation of the represented image. In addition, considering the structural features of the face, we adjusted the size of the blocks so that it can be large enough to include the complete face organ(s), since these face organs, like the eyes, nose, and mouth contain the most discriminative features. The addition of changeable size blocks effectively reduces the effects of noise and illumination. Experiments on four face image databases demonstrate the effectiveness of the proposed FLBP method.
C1 [Xu, Jie] Guangzhou Maritime Univ, Informat & Commun Engn, Guangzhou 510000, Peoples R China.
   [Gao, Jun] Guangdong Univ Technol, Fac Automat, Guangzhou 510006, Peoples R China.
C3 Guangzhou Maritime University; Guangdong University of Technology
RP Xu, J (corresponding author), Guangzhou Maritime Univ, Informat & Commun Engn, Guangzhou 510000, Peoples R China.
EM xujie@gdut.edu.cn
FU National Natural Science Foundation of China [61773128]
FX AcknowledgementsIn addition, the authors would like to thank the
   anonymous reviewers for their critical and constructive comments and
   suggestions. This work was partially supported by the National Natural
   Science Foundation of China under Grant No. 61773128.
CR Bai RY, 2024, VISUAL COMPUT, V40, P287, DOI 10.1007/s00371-023-02782-5
   Banerjee A., 2016, TEXTURE CLASSIFICATI
   Banerjee A, 2022, VISUAL COMPUT, V38, P321, DOI 10.1007/s00371-020-02017-x
   Bhatt HS, 2012, IEEE T INF FOREN SEC, V7, P1522, DOI 10.1109/TIFS.2012.2204252
   Chen J, 2010, IEEE T PATTERN ANAL, V32, P1705, DOI 10.1109/TPAMI.2009.155
   Tran CK, 2016, 2016 3RD INTERNATIONAL CONFERENCE ON GREEN TECHNOLOGY AND SUSTAINABLE DEVELOPMENT (GTSD), P5, DOI 10.1109/GTSD.2016.10
   Guo CY, 2019, IEEE ACCESS, V7, P174517, DOI 10.1109/ACCESS.2019.2942358
   Han XH, 2015, IEEE T CYBERNETICS, V45, P1180, DOI 10.1109/TCYB.2014.2346793
   Huang D, 2012, IEEE T INF FOREN SEC, V7, P1551, DOI 10.1109/TIFS.2012.2206807
   Huang D, 2011, IEEE T SYST MAN CY C, V41, P765, DOI 10.1109/TSMCC.2011.2118750
   Jain A. K., 1989, Fundamentals of Digital Image Processing
   Kayhan N, 2021, MULTIMED TOOLS APPL, V80, P32763, DOI 10.1007/s11042-021-11217-z
   Khan SA, 2018, MULTIMED TOOLS APPL, V77, P1133, DOI 10.1007/s11042-016-4324-z
   Lan RS, 2017, IEEE T CIRC SYST VID, V27, P261, DOI 10.1109/TCSVT.2015.2492839
   Lee KC, 2005, IEEE T PATTERN ANAL, V27, P684, DOI 10.1109/TPAMI.2005.92
   Li J, 2015, LECT NOTES COMPUT SC, V9008, P541, DOI 10.1007/978-3-319-16628-5_39
   Li ST, 2013, NEUROCOMPUTING, V122, P272, DOI 10.1016/j.neucom.2013.05.038
   Liao S, 2009, IEEE T IMAGE PROCESS, V18, P1107, DOI 10.1109/TIP.2009.2015682
   Liao SC, 2007, LECT NOTES COMPUT SC, V4642, P828
   Liu F, 2013, NEUROCOMPUTING, V120, P325, DOI 10.1016/j.neucom.2012.06.061
   Martinez A.M., 1998, AR FACE DATABASE CVC
   Martinez A.M., 2003, AR FACE DATABASE
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Phillips PJ, 2000, IEEE T PATTERN ANAL, V22, P1090, DOI 10.1109/34.879790
   Phillips PJ., 2004, The facial recognition technology (feret) database
   Shrivastava N, 2014, VISUAL COMPUT, V30, P1223, DOI 10.1007/s00371-013-0887-0
   Shu X, 2021, SIGNAL PROCESS-IMAGE, V98, DOI 10.1016/j.image.2021.116392
   Singh C, 2018, PATTERN RECOGN, V76, P50, DOI 10.1016/j.patcog.2017.10.021
   Sun S., 2013, MATH PROBL ENG, V10, P15
   Swets DL, 1996, IEEE T PATTERN ANAL, V18, P831, DOI 10.1109/34.531802
   Tan XY, 2010, IEEE T IMAGE PROCESS, V19, P1635, DOI 10.1109/TIP.2010.2042645
   Thurstone LL, 1927, PSYCHOL REV, V34, P273, DOI 10.1037/h0070288
   Turhal U, 2024, VISUAL COMPUT, V40, P1537, DOI 10.1007/s00371-023-02866-2
   Vu HN, 2022, APPL INTELL, V52, P5497, DOI 10.1007/s10489-021-02728-1
   Wang B, 2011, IEEE SIGNAL PROC LET, V18, P462, DOI 10.1109/LSP.2011.2158998
   Wu XS, 2017, VISUAL COMPUT, V33, P317, DOI 10.1007/s00371-015-1202-z
   Xia ZH, 2020, IEEE T SYST MAN CY-S, V50, P1526, DOI 10.1109/TSMC.2018.2874281
   Yang GC, 2017, INT J WAVELETS MULTI, V15, DOI 10.1142/S0219691317500527
   Yang WK, 2020, NEUROCOMPUTING, V373, P109, DOI 10.1016/j.neucom.2019.09.102
   Zhang J, 2013, IEEE T IMAGE PROCESS, V22, P31, DOI 10.1109/TIP.2012.2214045
NR 40
TC 0
Z9 0
U1 0
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2024
VL 40
IS 5
BP 3487
EP 3502
DI 10.1007/s00371-023-03047-x
EA AUG 2023
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OC3D9
UT WOS:001049919400002
DA 2024-07-18
ER

PT J
AU Jiang, ZM
   Zhang, WY
   Wang, WJ
AF Jiang, Zhongmin
   Zhang, Wanyan
   Wang, Wenju
TI Fusiform multi-scale pixel self-attention network for hyperspectral
   images reconstruction from a single RGB image
SO VISUAL COMPUTER
LA English
DT Article
DE Hyperspectral images reconstruction; Fusiform multi-scale pixel
   self-attention; Fusiform multi-scale feature extraction; Multi-scale
   adaptive residual attention; Pixel self-attention
ID SPECTRAL REFLECTANCE
AB Current research on deep learning algorithms focuses on the reconstruction of hyperspectral images from a single RGB image. However, this does not consider the feature information between regions, so the feature capture of context is insufficient. This causes the quality of reconstructed hyperspectral images to be low. We propose correcting this with a fusiform multi-scale pixel self-attention (FMPSA) network. The proposed FMPSA consists of a fusiform multi-scale feature extraction (FMFE) module cascaded with several multi-scale adaptive residual attention blocks (MARABs). FMFE extracts multi-scale detail features by interleaving dual components to avoid degrading spectral reconstruction quality due to local and edge spatial information loss. Each MARAB consists of paired FMFE-Left and FMFE-Right components, an optimal non-local model, a pixel self-attention module, a LayerNorm layer, a multilayer perceptron with Gelu nonlinearity, and long-short dual residual connection, which can be regarded as a residual structure based on a pixel self-attention mechanism. MARAB can adaptively track regions containing feature-rich information for more accurate hyperspectral reconstruction with a hierarchical focus on the salient pixels. The proposed FMPSA was applied to the NTIRE 2020 hyperspectral dataset. Experimental results show that the proposed method outperforms current methods in terms of MRAE and RMSE.
C1 [Jiang, Zhongmin; Zhang, Wanyan; Wang, Wenju] Univ Shanghai Sci & Technol, Shanghai, Peoples R China.
C3 University of Shanghai for Science & Technology
RP Wang, WJ (corresponding author), Univ Shanghai Sci & Technol, Shanghai, Peoples R China.
EM wangwenju@usst.edu.cn
RI Wang, Siying/KHX-1894-2024; Sun, Yang/KHY-5117-2024; wang,
   haoyu/KHY-6295-2024
OI wang, haoyu/0009-0001-2467-5331; Wang, Wenju/0000-0002-8549-4710
FU Key Laboratory Project of Intelligent and Green flexography of National
   Press and Publication Administration [ZBKT202203]
FX AcknowledgementsThis work was supported by the Key Laboratory Project of
   Intelligent and Green flexography of National Press and Publication
   Administration, under Grant ZBKT202203.
CR Abed FM, 2009, J OPT SOC AM A, V26, P613, DOI 10.1364/JOSAA.26.000613
   Aeschbacher J, 2017, IEEE INT CONF COMP V, P471, DOI 10.1109/ICCVW.2017.63
   Agahian F, 2014, PROC SPIE, V9014, DOI 10.1117/12.2042300
   Alvarez-Gila A, 2017, IEEE INT CONF COMP V, P480, DOI 10.1109/ICCVW.2017.64
   Arad B, 2020, IEEE COMPUT SOC CONF, P1806, DOI 10.1109/CVPRW50498.2020.00231
   Arad B, 2016, LECT NOTES COMPUT SC, V9911, P19, DOI 10.1007/978-3-319-46478-7_2
   Banerjee A., 2020, ARXIV
   Can Y.B., 2018, ARXIV
   Chen S, 2017, OPT EXPRESS, V25, P1005, DOI 10.1364/OE.25.001005
   Chou TR, 2019, COLOR RES APPL, V44, P508, DOI 10.1002/col.22366
   Elrifai I., 2013, INT J IMAGE PROCESS, V7, P278
   Fubara BJ, 2020, IEEE COMPUT SOC CONF, P1984, DOI 10.1109/CVPRW50498.2020.00248
   Funamizu H, 2014, APPL OPTICS, V53, P7072, DOI 10.1364/AO.53.007072
   Hu X., 2022, IEEE CVF C COMP VIS, p17,521
   Koundinya S, 2018, IEEE COMPUT SOC CONF, P957, DOI 10.1109/CVPRW.2018.00129
   Li JJ, 2020, IEEE COMPUT SOC CONF, P1894, DOI 10.1109/CVPRW50498.2020.00239
   Liang JX, 2017, OPT EXPRESS, V25, P28273, DOI 10.1364/OE.25.028273
   Liu PF, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20082426
   Lyu X, 2020, ECOL INDIC, V114, DOI 10.1016/j.ecolind.2020.106310
   Miao X, 2019, IEEE I CONF COMP VIS, P4058, DOI 10.1109/ICCV.2019.00416
   Nathan D.S., 2020, ARXIV
   Park JJ, 2020, INT J REMOTE SENS, V41, P5928, DOI 10.1080/01431161.2019.1707904
   Peng H, 2020, IEEE COMPUT SOC CONF, P2012, DOI 10.1109/CVPRW50498.2020.00251
   Poli D, 2012, PHOTOGRAMM REC, V26, P58, DOI 10.1111/j.1477-9730.2011.00665.x
   Shi Z, 2018, IEEE COMPUT SOC CONF, P1052, DOI 10.1109/CVPRW.2018.00139
   Shimoni M, 2019, IEEE GEOSC REM SEN M, V7, P101, DOI 10.1109/MGRS.2019.2902525
   Tao HL, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20051296
   Vaswani A, 2017, ADV NEUR IN, V30
   Wan YQ, 2021, SCI REP-UK, V11, DOI 10.1038/s41598-020-79864-0
   Wang B, 2021, REMOTE SENS-BASEL, V13, DOI 10.3390/rs13081568
   Wang LZ, 2019, PROC CVPR IEEE, P8024, DOI 10.1109/CVPR.2019.00822
   Wang LZ, 2017, IEEE T PATTERN ANAL, V39, P2104, DOI 10.1109/TPAMI.2016.2621050
   Wang WJ, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21020666
   Xiong ZW, 2017, IEEE INT CONF COMP V, P518, DOI 10.1109/ICCVW.2017.68
   Yasuma F, 2010, IEEE T IMAGE PROCESS, V19, P2241, DOI 10.1109/TIP.2010.2046811
   Zhang SP, 2019, IEEE I CONF COMP VIS, P10182, DOI 10.1109/ICCV.2019.01028
   Zhao YZ, 2020, IEEE COMPUT SOC CONF, P1695, DOI 10.1109/CVPRW50498.2020.00219
   Zhu Yuanhong, 2012, 2012 Symposium on Photonics and Optoelectronics (SOPO 2012), DOI 10.1109/SOPO.2012.6270485
NR 38
TC 1
Z9 1
U1 11
U2 17
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2023
VL 39
IS 8
SI SI
BP 3573
EP 3584
DI 10.1007/s00371-023-03006-6
EA JUL 2023
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA P2DS6
UT WOS:001035766800002
DA 2024-07-18
ER

PT J
AU Wu, SQ
   Tao, JL
   Wu, CR
   Chen, L
AF Wu, Shiqing
   Tao, Jialin
   Wu, Chenrui
   Chen, Long
TI Low-overlap point cloud registration algorithm based on coupled
   iteration
SO VISUAL COMPUTER
LA English
DT Article
DE Deep learning; Point cloud registration; Coupled iteration; Regional
   supervision; Local downsampling
AB We present BC-PCNet, a Point Cloud registration model based on Bidirectional Coupled iteration. The proposed model addresses the challenge of registering point clouds with low overlap. We introduce a new supervisory signal called "Mask Region." This signal is used to supervise the overlapping region of the two point clouds during the coupled iterative process, enhancing the accuracy of registration. We also improve the registration accuracy by increasing the number of coupled iterative steps. Moreover, by randomly downsampling the non-overlapping part of the point cloud, we reduce the amount of input training data and increase the speed of model training and registration. Compared to the latest models, our model performs well for low-overlap point cloud registration. Experiments show that BC-PCNet achieves a 0.6%/4.1% improvement in recall precision on the 3DMatch/3DLoMatch datasets.
C1 [Wu, Shiqing; Tao, Jialin; Wu, Chenrui; Chen, Long] Univ Shanghai Sci & Technol, Coll Mech Engn, Shanghai 200093, Peoples R China.
C3 University of Shanghai for Science & Technology
RP Chen, L (corresponding author), Univ Shanghai Sci & Technol, Coll Mech Engn, Shanghai 200093, Peoples R China.
EM wsq07599@usst.edu.cn; 212171614@st.usst.edu.cn; wuchenrui@usst.edu.cn;
   cl@usst.edu.cn
RI wu, chenrui/GQH-0583-2022
FU National Natural Science Foundation of China [52005338, 52105525]
FX This work was supported by the National Natural Science Foundation of
   China under Grant 52005338 and Grant 52105525.
CR Aoki Y, 2019, PROC CVPR IEEE, P7156, DOI 10.1109/CVPR.2019.00733
   ARUN KS, 1987, IEEE T PATTERN ANAL, V9, P699, DOI 10.1109/TPAMI.1987.4767965
   Bai XY, 2020, PROC CVPR IEEE, P6358, DOI 10.1109/CVPR42600.2020.00639
   BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791
   Choy C, 2020, PROC CVPR IEEE, P2511, DOI 10.1109/CVPR42600.2020.00259
   Choy C, 2019, PROC CVPR IEEE, P3070, DOI 10.1109/CVPR.2019.00319
   Dong K., 2022, VISUAL COMPUT, P1
   Dusmanu M., 2019, PREPRINT
   ElBanani M., 2021, P IEEECVF INT C COMP, P6433
   Gerbino G, 2021, J CRANIO MAXILL SURG, V49, P223, DOI 10.1016/j.jcms.2021.01.008
   Gojcic Z, 2019, PROC CVPR IEEE, P5540, DOI 10.1109/CVPR.2019.00569
   Huang SY, 2021, PROC CVPR IEEE, P4265, DOI 10.1109/CVPR46437.2021.00425
   Izatt G, 2020, SPR PROC ADV ROBOT, V10, P695, DOI 10.1007/978-3-030-28619-4_49
   Khoury M, 2017, IEEE I CONF COMP VIS, P153, DOI 10.1109/ICCV.2017.26
   Li L, 2020, PROC CVPR IEEE, P1916, DOI 10.1109/CVPR42600.2020.00199
   Li X., 2021, P IEEECVF C COMPUTER, P12763
   Li Y, 2022, PROC CVPR IEEE, P5544, DOI 10.1109/CVPR52688.2022.00547
   Lipson L, 2022, PROC CVPR IEEE, P6718, DOI 10.1109/CVPR52688.2022.00661
   Liu F, 2019, IEEE I CONF COMP VIS, P9407, DOI 10.1109/ICCV.2019.00950
   Liu XT, 2021, PROC CVPR IEEE, P13044, DOI 10.1109/CVPR46437.2021.01285
   Lyu H, 2019, ADV NEUR IN, V32
   Min T, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5470, DOI 10.1109/ICCV48922.2021.00544
   Netto GM, 2022, VISUAL COMPUT, V38, P3217, DOI 10.1007/s00371-022-02525-y
   Pais GD, 2020, PROC CVPR IEEE, P7191, DOI 10.1109/CVPR42600.2020.00722
   Sarlin PE, 2020, PROC CVPR IEEE, P4937, DOI 10.1109/CVPR42600.2020.00499
   Segal Aleksandr, 2009, ROBOTICS SCI SYSTEMS, V2
   Song YN, 2023, VISUAL COMPUT, V39, P1109, DOI 10.1007/s00371-021-02391-0
   Thomas H, 2019, IEEE I CONF COMP VIS, P6420, DOI 10.1109/ICCV.2019.00651
   Tsung-Yi Lin, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P2999, DOI 10.1109/ICCV.2017.324
   van Doormaal TPC, 2019, OPER NEUROSURG, V17, P588, DOI 10.1093/ons/opz094
   Wang C, 2022, VISUAL COMPUT, V38, P4279, DOI 10.1007/s00371-021-02295-z
   Wang Y., 2019, Adv. Neural Inf. Process. Syst., V32
   Wang Y, 2019, IEEE I CONF COMP VIS, P3522, DOI 10.1109/ICCV.2019.00362
   Xu H, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P3112, DOI 10.1109/ICCV48922.2021.00312
   Yang JL, 2016, IEEE T PATTERN ANAL, V38, P2241, DOI 10.1109/TPAMI.2015.2513405
   Yew ZJ, 2018, LECT NOTES COMPUT SC, V11219, P630, DOI 10.1007/978-3-030-01267-0_37
   Yuan YL, 2023, OCEAN ENG, V278, DOI 10.1016/j.oceaneng.2023.114317
   Yuan YL, 2023, J BIONIC ENG, V20, P1747, DOI 10.1007/s42235-023-00359-5
   Yuan YL, 2022, ADV ENG SOFTW, V170, DOI 10.1016/j.advengsoft.2022.103158
   Yuan YL, 2022, APPL SOFT COMPUT, V123, DOI 10.1016/j.asoc.2022.108947
   Zeng A, 2017, PROC CVPR IEEE, P199, DOI 10.1109/CVPR.2017.29
   Zi Jian Yew, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11821, DOI 10.1109/CVPR42600.2020.01184
NR 42
TC 0
Z9 0
U1 9
U2 22
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2024
VL 40
IS 5
BP 3151
EP 3162
DI 10.1007/s00371-023-03016-4
EA JUL 2023
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OC3D9
UT WOS:001032577400004
DA 2024-07-18
ER

PT J
AU Zhou, PW
   Dong, X
   Cao, J
   Chen, ZG
AF Zhou, Pengwei
   Dong, Xiao
   Cao, Juan
   Chen, Zhonggui
TI MeT: mesh transformer with an edge
SO VISUAL COMPUTER
LA English
DT Article
DE Transformer; Shape analysis; Geometric deep learning
AB Transformers have been widely applied in various vision tasks processing different data, such as images, videos and point clouds. However, the use of transformers in 3D mesh analysis remains largely unexplored. To address this gap, we propose a mesh transformer (MeT) that utilizes local self-attention on edges. MeT is based on a transformer layer that uses vector attention for edges, which is a kind of attention operator that supports adaptive modulation to both feature vectors and individual feature channels. Based on the transformer block, we build a lightweight mesh transformer network that consists of encoder and decoder. MeT provides general backbones for subsequent 3D mesh analysis tasks. To evaluate the effectiveness of our network MeT, we conduct experiments on two classic mesh analysis tasks: shape classification and shape segmentation. MeT achieves the state-of-the-art performance on multiple datasets for two tasks. We also conduct ablation studies to show the effectiveness of key designs in our network.
C1 [Zhou, Pengwei; Chen, Zhonggui] Xiamen Univ, Sch Informat, Xiamen 361005, Peoples R China.
   [Cao, Juan] Xiamen Univ, Sch Math Sci, Xiamen 361005, Peoples R China.
   [Dong, Xiao] BNU HKBU United Int Coll, Dept Comp Sci, Zhuhai 519087, Peoples R China.
C3 Xiamen University; Xiamen University; Beijing Normal University - Hong
   Kong Baptist University United International College
RP Chen, ZG (corresponding author), Xiamen Univ, Sch Informat, Xiamen 361005, Peoples R China.
EM hannahchou@stu.xmu.edu.cn; xiaodong@uic.edu.cn; juancao@xmu.edu.cn;
   chenzhonggui@xmu.edu.cn
RI Dong, Xiao/JXX-2903-2024
OI Dong, Xiao/0000-0003-4365-0575; Chen, Zhonggui/0000-0002-9960-4896
FU National Natural Science Foundation of China [61972327, 62272402];
   Natural Science Foundation of Fujian Province [2022J01001]; Fundamental
   Research Funds for the Central Universities [20720220037]; Start-up Fund
   from BNU-HKBU United International College [UICR0700052-23]
FX This work was supported by National Natural Science Foundation of China
   (61972327, 62272402), Natural Science Foundation of Fujian Province
   (2022J01001), Fundamental Research Funds for the Central Universities
   (20720220037), and Start-up Fund from BNU-HKBU United International
   College (UICR0700052-23)
CR Anguelov D, 2005, ACM T GRAPHIC, V24, P408, DOI 10.1145/1073204.1073207
   [Anonymous], 2015, PROC CVPR IEEE, DOI [DOI 10.1109/CVPR.2015.7298801, 10.1109/CVPR.2015.7298801]
   Arnab A, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6816, DOI 10.1109/ICCV48922.2021.00676
   Bogo F, 2014, PROC CVPR IEEE, P3794, DOI 10.1109/CVPR.2014.491
   Boscaini D., 2016, ADV NEURAL INFORM PR, P3197
   Carion N., 2020, EUR C COMP VIS, P213, DOI DOI 10.1007/978-3-030-58452-8_13
   Chen CF, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P347, DOI 10.1109/ICCV48922.2021.00041
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Devlin J., 2018, BERT PRE TRAINING DE
   Dosovitskiy A, 2021, arXiv, DOI [10.48550/ARXIV.2010.11929, DOI 10.48550/ARXIV.2010.11929]
   Eigen D, 2014, ADV NEUR IN, V27
   Eigen D, 2015, IEEE I CONF COMP VIS, P2650, DOI 10.1109/ICCV.2015.304
   Feng YT, 2019, AAAI CONF ARTIF INTE, P8279
   Giorgi Daniela, 2007, SHREC Competition, V8
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Guan SY, 2021, PROC CVPR IEEE, P10467, DOI 10.1109/CVPR46437.2021.01033
   Guo J., 2021, ARXIV
   Guo MH, 2021, COMPUT VIS MEDIA, V7, P187, DOI 10.1007/s41095-021-0229-5
   Han X. -F., 2021, ARXIV
   Hanocka R., 2020, ARXIV
   Hanocka R, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3322959
   He KM, 2022, PROC CVPR IEEE, P15979, DOI 10.1109/CVPR52688.2022.01553
   Heo B, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11916, DOI 10.1109/ICCV48922.2021.01172
   Hu SM, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3506694
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lahav A, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417806
   Latecki LJ, 2000, IEEE T PATTERN ANAL, V22, P1185, DOI 10.1109/34.879802
   Li XZ, 2021, IEEE T VIS COMPUT GR, V27, P4060, DOI 10.1109/TVCG.2020.3001681
   Lian Z., 2011, EUR WORKSH 3D OBJ RE
   Liang YQ, 2022, LECT NOTES COMPUT SC, V13663, P37, DOI 10.1007/978-3-031-20062-5_3
   Lin T., 2022, AI Open, V3, P111, DOI [DOI 10.1016/J.AIOPEN.2022.10.001, 10.1016/j.aiopen.2022.10.001]
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Loop C, 1987, THESIS U UTAH
   Luan TY, 2021, AAAI CONF ARTIF INTE, V35, P2269
   Luo ST, 2021, PROC CVPR IEEE, P2836, DOI 10.1109/CVPR46437.2021.00286
   Lv CL, 2022, IEEE T MULTIMEDIA, V24, P1815, DOI 10.1109/TMM.2021.3073265
   Mao A., 2021, IEEE T VIS COMPUT GR
   Mao JG, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P3144, DOI 10.1109/ICCV48922.2021.00315
   Maron H, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073616
   Masci J, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P832, DOI 10.1109/ICCVW.2015.112
   Maturana D, 2015, IEEE INT C INT ROBOT, P922, DOI 10.1109/IROS.2015.7353481
   Milano F., 2020, Advances in Neural Information Processing Systems, P952
   Min C., 2022, ARXIV
   Monti F, 2017, PROC CVPR IEEE, P5425, DOI 10.1109/CVPR.2017.576
   Pang Y, 2022, LECT NOTES COMPUT SC, V13662, P604, DOI 10.1007/978-3-031-20086-1_35
   Qi CR, 2017, ADV NEUR IN, V30
   Raffel C, 2020, J MACH LEARN RES, V21
   Riegler G, 2017, PROC CVPR IEEE, P6620, DOI 10.1109/CVPR.2017.701
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Singh VV, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P4883, DOI 10.1145/3474085.3475468
   Smirnov D, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459797
   Tan HW, 2022, COMPUT GRAPH FORUM, V41, P495, DOI 10.1111/cgf.14448
   Vaswani A, 2017, ADV NEUR IN, V30
   Vlasic D, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360696
   Wang W, 2019, COMPUT GRAPH-UK, V84, P144, DOI 10.1016/j.cag.2019.08.002
   Wang YH, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366184
   Xu HT, 2017, IEEE I CONF COMP VIS, P2717, DOI 10.1109/ICCV.2017.294
   Yu XM, 2022, PROC CVPR IEEE, P19291, DOI 10.1109/CVPR52688.2022.01871
   Yu XM, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12478, DOI 10.1109/ICCV48922.2021.01227
   Zhao H., 2020, P IEEE CVF C COMP VI, V2020, P10076, DOI 10.1109/CVPR42600.2020.01009
   Zhao HS, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P16239, DOI 10.1109/ICCV48922.2021.01595
NR 63
TC 2
Z9 2
U1 4
U2 8
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2023
VL 39
IS 8
SI SI
BP 3235
EP 3246
DI 10.1007/s00371-023-02966-z
EA JUL 2023
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA P2DS6
UT WOS:001028383100001
OA Bronze
DA 2024-07-18
ER

PT J
AU Tamura, M
   Yoshinaga, T
AF Tamura, Masato
   Yoshinaga, Tomoaki
TI Segmentation-based bounding box generation for omnidirectional
   pedestrian detection
SO VISUAL COMPUTER
LA English
DT Article
DE Omnidirectional; Pedestrian detection; Segmentation-based bounding box
   generation; Transformer
AB We propose a segmentation-based bounding box generation method for omnidirectional pedestrian detection that enables detectors to tightly fit bounding boxes to pedestrians without omnidirectional images for training. Due to the wide angle of view, omnidirectional cameras are more cost-effective than standard cameras and hence, suitable for large-scale monitoring. However, state-of-the-art pedestrian detectors cannot directly be applied to omnidirectional pedestrian detection because pedestrians' appearance in omnidirectional images may be rotated to any angle, which substantially degrades the performance of standard pedestrian detectors. Existing methods mitigate this issue by transforming images during inference, though the methods have limited detection accuracy and slow detection speed. A recently proposed method obviates the transformation at the cost of laborious annotation works and trains detectors with omnidirectional pedestrian detection datasets. We propose instead leveraging an existing large-scale object detection dataset to obviate both the transformation and annotation works. We train a detector with rotated images and tightly fitted bounding box annotations generated from the segmentation annotations in the object detection dataset, which enable detectors to detect pedestrians in omnidirectional images with tightly fitted bounding boxes. We also develop pseudo-fisheye distortion augmentation, which deforms images of perspective view to imitate the fisheye distortion and further enhances the performance. Extensive analysis shows that our detector successfully fits bounding boxes to pedestrians and demonstrates substantial performance improvement over existing methods.
C1 [Tamura, Masato; Yoshinaga, Tomoaki] Hitachi Ltd, 1-280 Higashikoigakubo, Kokubunji, Tokyo 1858601, Japan.
C3 Hitachi Limited
RP Tamura, M (corresponding author), Hitachi Ltd, 1-280 Higashikoigakubo, Kokubunji, Tokyo 1858601, Japan.
EM masato.tamura.sf@hitachi.com; tomoaki.yoshinaga.xc@hitachi.com
RI Tamura, Masato/AFQ-7913-2022
OI Tamura, Masato/0000-0003-1029-5271
CR Bochkovskiy A, 2020, ARXIV PREPRINT ARXIV, DOI DOI 10.48550/ARXIV.2004.10934
   Bodla N, 2017, IEEE I CONF COMP VIS, P5562, DOI 10.1109/ICCV.2017.593
   Carion N., 2020, EUR C COMP VIS, P213, DOI DOI 10.1007/978-3-030-58452-8_13
   Chen WH, 2017, PROC CVPR IEEE, P1320, DOI 10.1109/CVPR.2017.145
   Chiang AT, 2014, IEEE INT CON MULTI
   Chiang SH, 2021, IMAGE VISION COMPUT, V105, DOI 10.1016/j.imavis.2020.104069
   Cinaroglu I, 2014, SIG PROCESS COMMUN, P2275, DOI 10.1109/SIU.2014.6830719
   Dai J., 2021, ICLR
   Demirkus M, 2017, PROCEEDINGS OF THE 12TH INTERNATIONAL JOINT CONFERENCE ON COMPUTER VISION, IMAGING AND COMPUTER GRAPHICS THEORY AND APPLICATIONS (VISIGRAPP 2017), VOL 5, P141, DOI 10.5220/0006094701410148
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Duan Z., 2020, CVPRW
   Ge Z., 2021, ARXIV, DOI DOI 10.48550/ARXIV.2107.08430
   Hasan Irtiza, 2021, CVPR
   He K., 2016, PROC CVPR IEEE, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]
   Hosang J, 2015, PROC CVPR IEEE, P4073, DOI 10.1109/CVPR.2015.7299034
   Huang X., 2020, P IEEE CVF C COMP VI, P10747, DOI DOI 10.1109/CVPR42600.2020.01076
   Jialian Wu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13427, DOI 10.1109/CVPR42600.2020.01344
   Joseph RK, 2016, CRIT POL ECON S ASIA, P1
   Krams O, 2017, 2017 14TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE (AVSS)
   Kuhn HW, 2005, NAV RES LOG, V52, P7, DOI 10.1002/nav.20053
   Li S., 2019, AVSS
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu W, 2019, PROC CVPR IEEE, P5182, DOI 10.1109/CVPR.2019.00533
   Loshchilov I., 2019, DECOUPLED WEIGHT DEC
   Meinel L, 2014, IEEE CONF IMAGING SY, P312, DOI 10.1109/IST.2014.6958495
   Nguyen Thanh Binh, 2016, [JOURNAL OF KOREA MULTIMEDIA SOCIETY, 멀티미디어학회논문지], V19, P1345
   Noh J, 2018, PROC CVPR IEEE, P966, DOI 10.1109/CVPR.2018.00107
   Redmon J, 2018, Arxiv, DOI [arXiv:1804.02767, DOI 10.1109/CVPR.2017.690, DOI 10.48550/ARXIV.1804.02767]
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rezatofighi H, 2019, PROC CVPR IEEE, P658, DOI 10.1109/CVPR.2019.00075
   Saeidi M, 2022, VISUAL COMPUT, V38, P2223, DOI 10.1007/s00371-021-02280-6
   Seidel R., 2019, VISAPP
   Srisamosorn V, 2020, VISUAL COMPUT, V36, P1443, DOI 10.1007/s00371-019-01749-9
   Tamura M, 2019, IEEE WINT CONF APPL, P1989, DOI 10.1109/WACV.2019.00216
   Tsung-Yi Lin, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P2999, DOI 10.1109/ICCV.2017.324
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang CY, 2021, PROC CVPR IEEE, P13024, DOI 10.1109/CVPR46437.2021.01283
   Zhang LL, 2016, LECT NOTES COMPUT SC, V9906, P443, DOI 10.1007/978-3-319-46475-6_28
   Zhang SS, 2018, PROC CVPR IEEE, P6995, DOI 10.1109/CVPR.2018.00731
NR 39
TC 0
Z9 0
U1 3
U2 12
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2024
VL 40
IS 4
BP 2505
EP 2516
DI 10.1007/s00371-023-02933-8
EA JUN 2023
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA MZ2U3
UT WOS:001016490300001
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Wang, BL
   Zhang, F
   Zhao, YQ
AF Wang, Binglu
   Zhang, Fan
   Zhao, Yongqiang
TI LCH: fast RGB-D salient object detection on CPU via lightweight
   convolutional network with hybrid knowledge distillation
SO VISUAL COMPUTER
LA English
DT Article
DE RGB-D SOD; Lightweight neural network; Knowledge distillation; Non-local
   network
ID FUSION
AB Recently, the performance of RGB-D salient object detection (SOD) has been significantly improved with the development of deep-learning techniques. However, most of them depend on complex structures with a large amount of parameters and multi-add operations, which need huge computational resources and are not applicable to real-world applications, especially on mobile devices. To handle this problem, we propose a lightweight network, namely LCH, for RGB-D SOD. Specifically, we first design a novel lightweight ghost in ghost (GiG) module to enable a more powerful multi-scale feature extraction, which not only efficiently tackles the scale variations of salient objects but also guarantees a high inference speed. Then, the proposed GiG module works in conjunction with ghost bottlenecks (G-bneck) and, therefore, forms a new lightweight U-Net structure network for fast CPU RGB-D SOD. Furthermore, to enhance the feature discriminability of our LCH, we also design a novel hybrid knowledge distillation approach (HKD), for which both semantic structure information and pixel-wise similarity information are transferred from a powerful state-of-the-art RGB-D SOD network to the proposed LCH model, so as to generate more satisfactory detection results. Compared with previous work, LCH owns 5.6x fewer parameters and achieves 5.4x faster inference speed on the CPU devices, but obtains comparable SOD performance. Extensive experiments are carried out on commonly-used RGB-D SOD datasets. Both qualitative and quantitative results prove the effectiveness of our proposed method.
C1 [Wang, Binglu; Zhang, Fan; Zhao, Yongqiang] Northwestern Polytech Univ, Sch Automat, 127 West Youyi Rd, Xian, Shaanxi, Peoples R China.
   [Wang, Binglu] Beijing Inst Technol, Sch Informat & Elect, 5 South St, Beijing 100081, Peoples R China.
C3 Northwestern Polytechnical University; Beijing Institute of Technology
RP Zhang, F (corresponding author), Northwestern Polytech Univ, Sch Automat, 127 West Youyi Rd, Xian, Shaanxi, Peoples R China.
EM wbl921129@gmail.com; zfnlxx@mail.nwpu.edu.cn; zhaoyq@nwpu.edu.cn
RI Zhao, Yongqiang/O-7342-2015; Wang, Binglu/GLR-6556-2022
OI Wang, Binglu/0000-0002-9266-4685
CR An SM, 2022, IEEE T INTELL TRANSP, V23, P15256, DOI 10.1109/TITS.2021.3139001
   Borji A, 2012, LECT NOTES COMPUT SC, V7573, P414, DOI 10.1007/978-3-642-33709-3_30
   Chen GB, 2017, ADV NEUR IN, V30
   Chen H, 2019, IEEE T IMAGE PROCESS, V28, P2825, DOI 10.1109/TIP.2019.2891104
   Chen H, 2020, IEEE T CYBERNETICS, V50, P4808, DOI 10.1109/TCYB.2019.2934986
   Chen H, 2018, PROC CVPR IEEE, P3051, DOI 10.1109/CVPR.2018.00322
   Chen H, 2019, PATTERN RECOGN, V86, P376, DOI 10.1016/j.patcog.2018.08.007
   Chen TS, 2016, IEEE T NEUR NET LEAR, V27, P1135, DOI 10.1109/TNNLS.2015.2506664
   Chen Y, 2017, ARXIV
   Chen YP, 2022, PROC CVPR IEEE, P5260, DOI 10.1109/CVPR52688.2022.00520
   Chen YX, 2014, INTERNATIONAL CONFERENCE ON MECHANICS AND MATERIALS ENGINEERING (ICMME 2014), P23
   Cheng MM, 2021, INT J COMPUT VISION, V129, P2622, DOI [10.1007/s11263-021-01490-8, 10.1109/ICCV.2017.487]
   Chongyi Li, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12353), P225, DOI 10.1007/978-3-030-58598-3_14
   Cong RM, 2016, IEEE SIGNAL PROC LET, V23, DOI 10.1109/LSP.2016.2557347
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Fan DP, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P698
   Fan DP, 2021, IEEE T NEUR NET LEAR, V32, P2075, DOI 10.1109/TNNLS.2020.2996406
   Feng YC, 2021, IEEE T IMAGE PROCESS, V30, P5363, DOI 10.1109/TIP.2021.3083113
   Fu K., 2020, ARXIV
   Fu KR, 2020, PROC CVPR IEEE, P3049, DOI 10.1109/CVPR42600.2020.00312
   Gao SH, 2021, IEEE T PATTERN ANAL, V43, P652, DOI 10.1109/TPAMI.2019.2938758
   Gao Y, 2023, VISUAL COMPUT, V39, P3979, DOI 10.1007/s00371-022-02543-w
   Han JW, 2016, IEEE T CYBERNETICS, V46, P487, DOI 10.1109/TCYB.2015.2404432
   Han K, 2020, PROC CVPR IEEE, P1577, DOI 10.1109/CVPR42600.2020.00165
   Hou QB, 2017, PROC CVPR IEEE, P5300, DOI 10.1109/CVPR.2017.563
   Howard A.G., 2017, MOBILENETS EFFICIENT, DOI DOI 10.48550/ARXIV.1704.04861
   Howard A, 2019, IEEE I CONF COMP VIS, P1314, DOI 10.1109/ICCV.2019.00140
   Huang K, 2020, VISUAL COMPUT, V36, P1355, DOI 10.1007/s00371-019-01734-2
   Huang Z., 2017, ARXIV
   Ji DY, 2022, PROC CVPR IEEE, P16855, DOI 10.1109/CVPR52688.2022.01637
   Ju R, 2014, IEEE IMAGE PROC, P1115, DOI 10.1109/ICIP.2014.7025222
   Li CY, 2021, IEEE T CYBERNETICS, V51, P88, DOI 10.1109/TCYB.2020.2969255
   Li GY, 2020, IEEE T IMAGE PROCESS, V29, P4873, DOI 10.1109/TIP.2020.2976689
   Li NY, 2014, PROC CVPR IEEE, P2806, DOI 10.1109/CVPR.2014.359
   Li XY, 2021, AAAI CONF ARTIF INTE, V35, P1984
   Liang Y, 2020, VISUAL COMPUT, V36, P1883, DOI 10.1007/s00371-019-01781-9
   Liu H., 2022, arXiv
   Liu N, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4702, DOI 10.1109/ICCV48922.2021.00468
   Liu N, 2022, IEEE T PATTERN ANAL, V44, P9026, DOI 10.1109/TPAMI.2021.3122139
   Liu NA, 2018, PROC CVPR IEEE, P3089, DOI 10.1109/CVPR.2018.00326
   Liu N, 2018, IEEE T IMAGE PROCESS, V27, P3264, DOI 10.1109/TIP.2018.2817047
   Liu NA, 2016, PROC CVPR IEEE, P678, DOI 10.1109/CVPR.2016.80
   Liu R., 2022, ARXIV
   Liu YF, 2023, IEEE T PATTERN ANAL, V45, P7035, DOI 10.1109/TPAMI.2020.3001940
   Liu YF, 2019, PROC CVPR IEEE, P2599, DOI 10.1109/CVPR.2019.00271
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Liu ZY, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P4481, DOI 10.1145/3474085.3475601
   Liu ZY, 2022, IEEE T CIRC SYST VID, V32, P4486, DOI 10.1109/TCSVT.2021.3127149
   Liu ZY, 2021, VISUAL COMPUT, V37, P529, DOI 10.1007/s00371-020-01821-9
   Ma NN, 2018, LECT NOTES COMPUT SC, V11218, P122, DOI 10.1007/978-3-030-01264-9_8
   Mirzadeh SI, 2020, AAAI CONF ARTIF INTE, V34, P5191
   Nian Liu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13753, DOI 10.1109/CVPR42600.2020.01377
   Niu YZ, 2012, PROC CVPR IEEE, P454, DOI 10.1109/CVPR.2012.6247708
   Peng HW, 2014, LECT NOTES COMPUT SC, V8691, P92, DOI 10.1007/978-3-319-10578-9_7
   Peng P, 2022, EXPERT SYST APPL, V198, DOI 10.1016/j.eswa.2022.116805
   Perazzi F, 2012, PROC CVPR IEEE, P733, DOI 10.1109/CVPR.2012.6247743
   Piao YR, 2019, IEEE I CONF COMP VIS, P7253, DOI 10.1109/ICCV.2019.00735
   Qu LQ, 2017, IEEE T IMAGE PROCESS, V26, P2274, DOI 10.1109/TIP.2017.2682981
   Romero A, 2014, ARXIV
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474
   Saputra MRU, 2019, IEEE I CONF COMP VIS, P263, DOI 10.1109/ICCV.2019.00035
   Tan MX, 2019, PR MACH LEARN RES, V97
   Tao Zhou, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10274, DOI 10.1109/CVPR42600.2020.01029
   Tian Y., 2019, ARXIV
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang BL, 2023, IEEE T MULTIMEDIA, V25, P8308, DOI 10.1109/TMM.2023.3234817
   Wang BL, 2022, PROC CVPR IEEE, P19566, DOI 10.1109/CVPR52688.2022.01898
   Wang BL, 2022, IEEE T CIRC SYST VID, V32, P2186, DOI 10.1109/TCSVT.2021.3089323
   Wang BL, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2021.3123231
   Wang BL, 2021, IEEE SIGNAL PROC LET, V28, P503, DOI 10.1109/LSP.2021.3061289
   Wang J, 2022, VISUAL COMPUT, V38, P1803, DOI 10.1007/s00371-021-02106-5
   Wang NN, 2019, IEEE ACCESS, V7, P55277, DOI 10.1109/ACCESS.2019.2913107
   Wang XF, 2022, VISUAL COMPUT, V38, P3911, DOI 10.1007/s00371-021-02231-1
   Wang XH, 2021, IEEE T IMAGE PROCESS, V30, P458, DOI 10.1109/TIP.2020.3037470
   Xiaoqi Zhao, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12367), P646, DOI 10.1007/978-3-030-58542-6_39
   Yang TJ, 2018, LECT NOTES COMPUT SC, V11214, P289, DOI 10.1007/978-3-030-01249-6_18
   Ye LW, 2017, IEEE T MULTIMEDIA, V19, P1742, DOI 10.1109/TMM.2017.2693022
   Yim J, 2017, PROC CVPR IEEE, P7130, DOI 10.1109/CVPR.2017.754
   Yongri Piao, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9057, DOI 10.1109/CVPR42600.2020.00908
   Yuenan Hou, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12483, DOI 10.1109/CVPR42600.2020.01250
   Zagoruyko S., 2016, ARXIV
   Zhai Y., 2020, ARXIV
   Zhang F, 2022, INFORM SCIENCES, V596, P439, DOI 10.1016/j.ins.2022.03.035
   Zhang J., 2020, P IEEE CVF C COMP VI, P8579, DOI DOI 10.1109/CVPR42600.2020.00861
   Zhang LM, 2016, IEEE T CYBERNETICS, V46, P258, DOI 10.1109/TCYB.2015.2400821
   Zhang M, 2020, PROC CVPR IEEE, P3469, DOI 10.1109/CVPR42600.2020.00353
   Zhang N, 2022, IEEE T IMAGE PROCESS, V31, P4556, DOI 10.1109/TIP.2022.3185550
   Zhang Q., 2020, ARXIV
   Zhang X, 2018, PROC CVPR IEEE, P6848, DOI 10.1109/CVPR.2018.00716
   Zhao J, 2020, IEEE T FUZZY SYST, V28, P2287, DOI 10.1109/TFUZZ.2019.2930492
   Zhou T, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4661, DOI 10.1109/ICCV48922.2021.00464
   Zhou TF, 2021, PROC CVPR IEEE, P6981, DOI 10.1109/CVPR46437.2021.00691
   Zhou TF, 2022, IEEE T IMAGE PROCESS, V31, P799, DOI 10.1109/TIP.2021.3132834
   Zhou TF, 2020, IEEE T IMAGE PROCESS, V29, P8326, DOI 10.1109/TIP.2020.3013162
   Zhou TF, 2020, AAAI CONF ARTIF INTE, V34, P13066
   Zhu CB, 2017, IEEE INT CONF COMP V, P3008, DOI 10.1109/ICCVW.2017.355
NR 96
TC 2
Z9 2
U1 5
U2 9
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2024
VL 40
IS 3
BP 1997
EP 2014
DI 10.1007/s00371-023-02898-8
EA JUN 2023
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HY8O3
UT WOS:001000345700003
DA 2024-07-18
ER

PT J
AU Cou, C
   Guennebaud, G
AF Cou, Corentin
   Guennebaud, Gael
TI Depth from Focus using Windowed Linear Least Squares Regressions
SO VISUAL COMPUTER
LA English
DT Article
DE Depth map acquisition; Depth from focus
ID SHAPE
AB We present a novel depth from focus technique. Following prior work, our pipeline starts with a focal stack and an estimation of the amount of defocus as given by, for instance, the ring difference filter. To improve robustness to outliers while avoiding to rely on costly nonlinear optimizations, we propose an original scheme that linearly scans the profile over a fixed size window, searching for the best peak within each window using a linearized least-squares Laplace regression. As a post-process, depth estimates with low confidence are reconstructed though an adaptive moving least squares filter. We show how to objectively evaluate the performance of our approach by generating synthetic focal stacks from which the reconstructed depth maps can be compared to ground truth. Our results show that our method achieves higher accuracy than previous nonlinear Laplace regression technique, while being orders of magnitude faster.
C1 [Cou, Corentin] CNRS, Paris, France.
   [Guennebaud, Gael] Bordeaux Univ, INRIA, Bordeaux, France.
C3 Centre National de la Recherche Scientifique (CNRS); Inria
RP Cou, C (corresponding author), CNRS, Paris, France.; Guennebaud, G (corresponding author), Bordeaux Univ, INRIA, Bordeaux, France.
EM corentin.cou@inria.fr; gael.guennebaud@inria.fr
OI COU, Corentin/0000-0002-9677-0741
CR Billiot B, 2013, SENSORS-BASEL, V13, P5040, DOI 10.3390/s130405040
   Blayvas I, 2007, J OPT SOC AM A, V24, P967, DOI 10.1364/JOSAA.24.000967
   Bose NK, 2006, IEEE T IMAGE PROCESS, V15, P2239, DOI 10.1109/TIP.2006.877406
   Cou C, 2023, ARCHAEOMETRY, V65, P105, DOI 10.1111/arcm.12813
   GROSSMANN P, 1987, PATTERN RECOGN LETT, V5, P63, DOI 10.1016/0167-8655(87)90026-2
   Hazirbas C, 2019, LECT NOTES COMPUT SC, V11363, P525, DOI 10.1007/978-3-030-20893-6_33
   Javidnia H, 2018, J ELECTRON IMAGING, V27, DOI 10.1117/1.JEI.27.2.023019
   Kulkarni J., 2019, J. Eng. Appl. Sci., V3, P331
   Ledergerber C, 2008, IEEE T VIS COMPUT GR, V14, P1372, DOI 10.1109/TVCG.2008.186
   Lee JY, 2021, IEEE T PATTERN ANAL, V43, P830, DOI 10.1109/TPAMI.2019.2946159
   Levin A, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239521
   Mahmood MT, 2011, INFORM SCIENCES, V181, P1249, DOI 10.1016/j.ins.2010.11.039
   Pentland A.P., 1982, Technical report
   Pertuz S, 2013, PATTERN RECOGN, V46, P1415, DOI 10.1016/j.patcog.2012.11.011
   Pertuz S, 2013, IEEE T IMAGE PROCESS, V22, P1242, DOI 10.1109/TIP.2012.2231087
   Sakurikar P, 2017, IEEE I CONF COMP VIS, P1623, DOI 10.1109/ICCV.2017.179
   Sun Y, 2004, MICROSC RES TECHNIQ, V65, P139, DOI 10.1002/jemt.20118
   Surh J, 2017, PROC CVPR IEEE, P2444, DOI 10.1109/CVPR.2017.262
NR 18
TC 1
Z9 1
U1 0
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2024
VL 40
IS 2
BP 1205
EP 1214
DI 10.1007/s00371-023-02841-x
EA MAR 2023
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GE6E8
UT WOS:000960829200002
DA 2024-07-18
ER

PT J
AU Liu, Z
   Lu, X
AF Liu, Zhao
   Lu, Xu
TI Physical flexibility detection under complex backgrounds using ED-Former
SO VISUAL COMPUTER
LA English
DT Article
DE Physical flexibility; Complex backgrounds; ED-Former; Physical regions
   combination
ID NETWORK
AB Anteflexion angle is an essential health indicator to assess physical flexibility, and image recognition has become an effective method to detect physical anteflexion angle. In this paper, we present a physical flexibility detection algorithm under complex backgrounds, based on edge detection with transformers (ED-Former). The algorithm adopts the lightweight self-attention mechanism from the transformer architecture and combines it with an edge detection network. Additionally, we design the angle protector, which is added to the output to reduce the effect of weak edges in the problem image. Then, based on the obtained edge information, we propose a low-complexity ergonomics-based physical regions combination strategy to calculate the anteflexion angle. We select the regions of the shoulder, waist, and knee, and calculate the angle based on the relation between these regions. The experimental results show that the ED-Former has an excellent performance in physical edge detection tasks-notably producing an F1 score of 0.833 and outperforming state-of-the-art CNNs and transformers on these tasks. Furthermore, our network is effective in preventing adversarial attacks. Overall, we find that the average error of anteflexion angle with respect to the true is 1.267 degrees, well within the 5 degrees requirement for physical flexibility detection.
C1 [Liu, Zhao; Lu, Xu] Guangdong Polytech Normal Univ, Sch Comp Sci, Guangzhou 510665, Peoples R China.
   [Lu, Xu] Pazhou Lab, Guangzhou 510330, Peoples R China.
   [Lu, Xu] Guangdong Prov Key Lab Intellectual Property & Bi, Guangzhou 510665, Peoples R China.
C3 Guangdong Polytechnic Normal University; Pazhou Lab
RP Lu, X (corresponding author), Guangdong Polytech Normal Univ, Sch Comp Sci, Guangzhou 510665, Peoples R China.; Lu, X (corresponding author), Pazhou Lab, Guangzhou 510330, Peoples R China.; Lu, X (corresponding author), Guangdong Prov Key Lab Intellectual Property & Bi, Guangzhou 510665, Peoples R China.
EM bruda@126.com
OI Lu, Xu/0000-0002-6097-032X
FU National Natural Science Foundation of China [62176067]; Scientific and
   Technological Planning Project of Guangzhou [201903010041,
   202103000040]; Key Project of Guangdong Province Basic Research
   Foundation [2020B1515120095]; Guangdong Province Universities and
   Colleges Pearl River Scholar Funded Scheme (2019)
FX This work was supported in part by the National Natural Science
   Foundation of China (62176067); Scientific and Technological Planning
   Project of Guangzhou (201903010041, 202103000040); Key Project of
   Guangdong Province Basic Research Foundation (2020B1515120095); Project
   Supported by Guangdong Province Universities and Colleges Pearl River
   Scholar Funded Scheme (2019).
CR Bertasius G, 2015, PROC CVPR IEEE, P4380, DOI 10.1109/CVPR.2015.7299067
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851
   Carion Nicolas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P213, DOI 10.1007/978-3-030-58452-8_13
   Chen YL, 2018, INT J IND ERGONOM, V68, P82, DOI 10.1016/j.ergon.2018.06.009
   Deng RX, 2018, LECT NOTES COMPUT SC, V11210, P570, DOI 10.1007/978-3-030-01231-1_35
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Ganin Y, 2015, LECT NOTES COMPUT SC, V9004, P536, DOI 10.1007/978-3-319-16808-1_36
   Han K, 2022, Arxiv, DOI arXiv:2012.12556
   He JZ, 2019, PROC CVPR IEEE, P3823, DOI 10.1109/CVPR.2019.00395
   Ignasiak D, 2017, HUM MOVEMENT SCI, V54, P230, DOI 10.1016/j.humov.2017.05.011
   Goodfellow IJ, 2015, Arxiv, DOI [arXiv:1412.6572, DOI 10.48550/ARXIV.1412.6572]
   Kendall A, 2018, PROC CVPR IEEE, P7482, DOI 10.1109/CVPR.2018.00781
   Khan S.H., 2021, arXiv
   Kittler J., 1983, Image and Vision Computing, V1, P37, DOI [DOI 10.1016/0262-8856(83)90006-9, 10.1016/0262-8856(83)90006-9]
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lin C, 2020, NEUROCOMPUTING, V409, P361, DOI 10.1016/j.neucom.2020.06.069
   Liu Y, 2016, PROC CVPR IEEE, P231, DOI 10.1109/CVPR.2016.32
   Liu Y, 2019, IEEE T PATTERN ANAL, V41, P1939, DOI 10.1109/TPAMI.2018.2878849
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Lu X, 2021, MEAS SCI TECHNOL, V32, DOI 10.1088/1361-6501/ac136b
   Lu X, 2020, SOFT COMPUT, V24, P8673, DOI 10.1007/s00500-020-04869-w
   Min XK, 2019, IEEE T INTELL TRANSP, V20, P2879, DOI 10.1109/TITS.2018.2868771
   Min XK, 2020, IEEE T IMAGE PROCESS, V29, P6054, DOI 10.1109/TIP.2020.2988148
   Min XK, 2020, IEEE T IMAGE PROCESS, V29, P3805, DOI 10.1109/TIP.2020.2966082
   Mohsin N, 2022, VISUAL COMPUT, V38, P1097, DOI 10.1007/s00371-021-02070-0
   ROSENFELD A, 1981, IEEE T PATTERN ANAL, V3, P101, DOI 10.1109/TPAMI.1981.4767056
   Selvi TM, 2022, VISUAL COMPUT, V38, P385, DOI 10.1007/s00371-020-02021-1
   Shu XB, 2023, IEEE T PATTERN ANAL, V45, P7559, DOI 10.1109/TPAMI.2022.3222871
   Shu XB, 2022, IEEE T CIRC SYST VID, V32, P5281, DOI 10.1109/TCSVT.2022.3142771
   Vaswani A, 2017, ADV NEUR IN, V30
   Vu LQ, 2020, COMPUT BIOL MED, V118, DOI 10.1016/j.compbiomed.2020.103624
   Wang Y, 2021, SIGNAL IMAGE VIDEO P, V15, P1635, DOI 10.1007/s11760-021-01899-1
   Weide G, 2020, J BIOMECH, V99, DOI 10.1016/j.jbiomech.2019.109532
   Williams W, 2020, J SPORT REHABIL, V29, P400, DOI 10.1123/jsr.2018-0306
   Wu JF, 2021, MED PHYS, V48, P7850, DOI 10.1002/mp.15312
   Xie EZ, 2021, ADV NEUR IN, V34
   Xie SN, 2015, IEEE I CONF COMP VIS, P1395, DOI [10.1109/ICCV.2015.164, 10.1007/s11263-017-1004-z]
   Xu BQ, 2022, IEEE T IMAGE PROCESS, V31, P3852, DOI 10.1109/TIP.2022.3175605
   Zhai GT, 2020, SCI CHINA INFORM SCI, V63, DOI 10.1007/s11432-019-2757-1
   Zhang YJ, 2019, INT J IMAG SYST TECH, V29, P518, DOI 10.1002/ima.22332
   Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319
   Zhuang HM, 2023, VISUAL COMPUT, V39, P2207, DOI 10.1007/s00371-021-02322-z
NR 43
TC 0
Z9 0
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2024
VL 40
IS 2
BP 523
EP 534
DI 10.1007/s00371-023-02797-y
EA MAR 2023
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GE6E8
UT WOS:000952181900003
DA 2024-07-18
ER

PT J
AU Yuan, LQ
   Erdt, M
   Li, RL
   Siyal, MY
AF Yuan, Liqiang
   Erdt, Marius
   Li, Ruilin
   Siyal, Mohammed Yakoob
TI Data privacy protection domain adaptation by roughing and finishing
   stage
SO VISUAL COMPUTER
LA English
DT Article
DE Unsupervised domain adaptation; Source free unsupervised domain
   adaptation; Model generalization
ID BATCH NORMALIZATION
AB The automatic segmentation of organs or tissues is crucial for early diagnosis and treatment. Existing deep learning methods either need massive annotation data or use Unsupervised Domain Adaptation (UDA) approaches with labeled source domain data to train a model for unlabeled target domain data. The methods mentioned above require accessing the data of the source domain. However, in medical imaging, source domain data from a hospital is usually restricted due to patient data privacy regulations. Therefore, it is crucial to perform privacy-preserving domain adaptation, which simultaneously improves the model's performance on target domain data and preserves the privacy of source domain data. Aiming to achieve Source Free Unsupervised Domain Adaptation (SFUDA), we propose a two-stage framework Roughing and Finishing to extract the relevant features and knowledge from the pre-trained model and protect the patient's privacy. Specifically, in the Roughing stage, batch statistics are updated using unlabelled target domain data to alleviate feature distribution shifts. Then, a nonparametric weighted entropy-minimization is used to reduce the domain gap further. In the Finishing stage, we aim to enhance the model's generalization ability by increasing the noise robustness of the feature representation, leading to a more generalizable target feature representation. Besides, we propose pseudo-label training based on a confidence-weighted feature distance to utilize the knowledge from confident samples. The proposed method is evaluated on two medical domain transfer challenges, (1) MRI to CT liver segmentation and (2) cross-domain fundus image segmentation. Extensive experiments and ablation studies demonstrate the superiority of the proposed methods over state-of-the-art SFUDA methods by a large margin. The proposed method outperforms previous UDA approaches that access the source domain data.
C1 [Yuan, Liqiang; Li, Ruilin; Siyal, Mohammed Yakoob] Sch Elect & Elect Engn, 50 Nanyang Ave, Singapore 639798, Singapore.
   [Yuan, Liqiang; Erdt, Marius; Li, Ruilin] Nanyang Technol Univ, Fraunhofer Singapore, Nanyang Ave,Block NS1,Level 5, Singapore 639798, Singapore.
C3 Nanyang Technological University
RP Yuan, LQ (corresponding author), Sch Elect & Elect Engn, 50 Nanyang Ave, Singapore 639798, Singapore.; Yuan, LQ (corresponding author), Nanyang Technol Univ, Fraunhofer Singapore, Nanyang Ave,Block NS1,Level 5, Singapore 639798, Singapore.
EM Liqiang002@e.ntu.edu.sg
RI LI, RUILIN/JOK-1364-2023
OI LIQIANG, YUAN/0000-0001-7391-715X
CR Agrawal T, 2023, VISUAL COMPUT, V39, P875, DOI 10.1007/s00371-021-02352-7
   Bateson Mathilde, 2020, Medical Image Computing and Computer Assisted Intervention - MICCAI 2020. 23rd International Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12261), P490, DOI 10.1007/978-3-030-59710-8_48
   Belharbi, 2019, ARXIV
   Bilic P, 2019, ARXIV
   Chen C, 2021, LECT NOTES COMPUT SC, V12905, P225, DOI 10.1007/978-3-030-87240-3_22
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen MM, 2014, PR MACH LEARN RES, V32, P1476
   Chen XL, 2022, NEURAL COMPUT APPL, V34, P11295, DOI 10.1007/s00521-020-05588-x
   Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350
   Cui HF, 2021, COMPUT BIOL MED, V136, DOI 10.1016/j.compbiomed.2021.104726
   DANIELSSON PE, 1980, COMPUT VISION GRAPH, V14, P227, DOI 10.1016/0146-664X(80)90054-4
   Eastwood C., 2021, ARXIV
   Fumero F, 2011, COMP MED SY
   Ganin Y, 2015, PR MACH LEARN RES, V37, P1180
   Grandvalet Y., 2005, CAP, V367, P281
   Guodong Zeng, 2020, Medical Image Computing and Computer Assisted Intervention - MICCAI 2020. 23rd International Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12261), P447, DOI 10.1007/978-3-030-59710-8_44
   He YF, 2021, MED IMAGE ANAL, V72, DOI 10.1016/j.media.2021.102136
   Ioffe S, 2015, P INT C MACH LEARN, V2015, P1
   Karani N, 2021, MED IMAGE ANAL, V68, DOI 10.1016/j.media.2020.101907
   Kavur AE, 2021, MED IMAGE ANAL, V69, DOI 10.1016/j.media.2020.101950
   Klingner M, 2022, IEEE WINT C APPL COM, P210, DOI 10.1109/WACVW54805.2022.00027
   Kundu JN, 2020, PROC CVPR IEEE, P4543, DOI 10.1109/CVPR42600.2020.00460
   Lai H., 2022, VISUAL COMPUT, P1
   Lei HJ, 2022, IEEE J BIOMED HEALTH, V26, P90, DOI 10.1109/JBHI.2021.3085770
   Li CX, 2022, COMPUT BIOL MED, V141, DOI 10.1016/j.compbiomed.2021.105144
   Li X. L., 2021, ARXIV
   Li YH, 2018, PATTERN RECOGN, V80, P109, DOI 10.1016/j.patcog.2018.03.005
   Liang J., 2020, INT C MACH LEARN, P6028, DOI DOI 10.48550/ARXIV.2002.08546
   Liu X., 2021, ARXIV
   Liu XF, 2021, LECT NOTES COMPUT SC, V12903, P138, DOI 10.1007/978-3-030-87199-4_13
   Liu XF, 2021, LECT NOTES COMPUT SC, V12902, P549, DOI 10.1007/978-3-030-87196-3_51
   Nelakurthi AR, 2018, IEEE INT CONF BIG DA, P140, DOI [10.1109/BigData.2018.8622112, 10.1109/BigData.2018.8622047]
   Orlando JI, 2020, MED IMAGE ANAL, V59, DOI 10.1016/j.media.2019.101570
   Ozturk T, 2020, COMPUT BIOL MED, V121, DOI 10.1016/j.compbiomed.2020.103792
   Paul S., 2021, arXiv
   Richter SR, 2016, LECT NOTES COMPUT SC, V9906, P102, DOI 10.1007/978-3-319-46475-6_7
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Saito K, 2018, PROC CVPR IEEE, P3723, DOI 10.1109/CVPR.2018.00392
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474
   Sivaswamy J., 2015, JSM Biomedical Imaging Data Papers, V2, P1004
   Teja SP, 2021, PROC CVPR IEEE, P9608, DOI 10.1109/CVPR46437.2021.00949
   Toldo M, 2020, TECHNOLOGIES, V8, DOI 10.3390/technologies8020035
   Tsai YH, 2019, IEEE I CONF COMP VIS, P1456, DOI 10.1109/ICCV.2019.00154
   Vu TH, 2019, PROC CVPR IEEE, P2512, DOI 10.1109/CVPR.2019.00262
   Wang J, 2021, MED IMAGE ANAL, V69, DOI 10.1016/j.media.2020.101942
   Wang SJ, 2019, LECT NOTES COMPUT SC, V11764, P102, DOI 10.1007/978-3-030-32239-7_12
   Wang XJ, 2023, VISUAL COMPUT, V39, P4801, DOI 10.1007/s00371-022-02628-6
   Wang ZY, 2020, AAAI CONF ARTIF INTE, V34, P6315
   Yan CG, 2021, IEEE T PATTERN ANAL, V43, P1445, DOI 10.1109/TPAMI.2020.2975798
   Yan CG, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3472810
   Yan CG, 2022, IEEE T CIRC SYST VID, V32, P43, DOI 10.1109/TCSVT.2021.3067449
   Yan CG, 2021, ACM T MULTIM COMPUT, V16, DOI 10.1145/3404374
   Yang YC, 2020, PROC CVPR IEEE, P4084, DOI 10.1109/CVPR42600.2020.00414
   Yang Z. Y., 2022, ARXIV
   Yufan He, 2020, Medical Image Computing and Computer Assisted Intervention - MICCAI 2020. 23rd International Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12261), P437, DOI 10.1007/978-3-030-59710-8_43
   Zhang Y, 2020, IEEE T PATTERN ANAL, V42, P1823, DOI 10.1109/TPAMI.2019.2903401
   Zhang Y, 2017, IEEE I CONF COMP VIS, P2039, DOI 10.1109/ICCV.2017.223
   Zhang YC, 2019, PR MACH LEARN RES, V97
   Zhao RJ, 2020, IEEE DATA MINING, P851, DOI 10.1109/ICDM50108.2020.00094
   Zheng H, 2020, AAAI CONF ARTIF INTE, V34, P6925, DOI 10.1609/aaai.v34i04.6175
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zou Y, 2018, LECT NOTES COMPUT SC, V11207, P297, DOI [10.1007/978-3-030-01219-9_, 10.1007/978-3-030-01219-9_18]
NR 62
TC 3
Z9 3
U1 2
U2 7
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2024
VL 40
IS 2
BP 471
EP 488
DI 10.1007/s00371-023-02794-1
EA FEB 2023
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GE6E8
UT WOS:000939344800001
DA 2024-07-18
ER

PT J
AU Cao, SX
   Guo, DL
   Cao, LN
   Li, S
   Nie, JL
   Singh, AK
   Lv, HB
AF Cao, Shuxian
   Guo, Dongliang
   Cao, Lina
   Li, Shuo
   Nie, Junlan
   Singh, Amit Kumar
   Lv, Haibin
TI VisDmk: visual analysis of massive emotional danmaku in online videos
SO VISUAL COMPUTER
LA English
DT Article
DE Danmaku; Text visualization; Video analysis; Emotion analysis
ID SOCIAL MEDIA; VISUALIZATION
AB Danmaku, a real-time comment function covering the top of the video, appears from the right of the video like a bullet and slides out horizontally from the left, which is gaining popularity in Asia. In recent years, the research on the analysis of massive danmaku data has mushroomed. The danmaku data contains a host of valuable information, such as the emotional expressions, attitudes, and opinions of the people watching the video, which helps people quickly get the content and effect of the video. The information is more representative and comprehensive with the ever-increasing amount of danmaku data over time. However, extracting valuable danmaku from huge amounts of data is a challenging task. Therefore, in this paper, we introduce VisDmk, an interactive visual analysis system, to help to analyze video content and effect. VisDmk incorporates five views: the projection view to exhibit emotion distribution, the detail view to analyze specific danmaku information, the individual view to illustrate the difference between viewers, the theme-aware view to identify themes in different periods, and the video view to ascertain some inference within a video. Case studies and user observation were conducted to evaluate this system.
C1 [Cao, Shuxian; Guo, Dongliang; Cao, Lina; Li, Shuo; Nie, Junlan] Yanshan Univ, Key Lab Comp virtual technol & Syst integrat Hebei, HebeiSt, Qin Huangdao 066004, Peoples R China.
   [Singh, Amit Kumar] Natl Inst Technol Patna, Dept Comp Sci & Engn, Patna 800005, Bihar, India.
   [Lv, Haibin] Minist Nat Resources North Sea Bur, North China Sea Offshore Engn Survey Inst, Qingdao 266000, Peoples R China.
C3 Yanshan University; National Institute of Technology (NIT System);
   National Institute of Technology Patna
RP Guo, DL (corresponding author), Yanshan Univ, Key Lab Comp virtual technol & Syst integrat Hebei, HebeiSt, Qin Huangdao 066004, Peoples R China.
EM 1656465402@qq.com; dongliangguo@ysu.edu.cn; c15032322059@163.com;
   1403457032@qq.com; niejll3@163.com; amit.singh@nitp.ac.in;
   lvhaibinsoa@gmail.com
FU National Key R&D Program of China; National Science Foundation of China;
   Natural Science Foundation of Hebei Province [61802334, 61902340];
   Innovation Capability Improvement Plan Project of Hebei Province
   [F2022203015];  [22567637H]
FX This work was supported in part by a grant from the National Key R&D
   Program of China, and the National Science Foundation of China under
   Grant 61802334 and 61902340, in part by the Natural Science Foundation
   of Hebei Province under Grant F2022203015 and in part by the Innovation
   Capability Improvement Plan Project of Hebei Province under Grant
   22567637H.
CR Burch Michael, 2013, 2013 17th International Conference on Information Visualisation, P45, DOI 10.1109/IV.2013.5
   Chen NC, 2017, PROCEEDINGS OF THE 50TH ANNUAL HAWAII INTERNATIONAL CONFERENCE ON SYSTEM SCIENCES, P1881
   Chen SM, 2017, COMPUT GRAPH FORUM, V36, P563, DOI 10.1111/cgf.13211
   Chen YS, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON DATA MINING WORKSHOP (ICDMW), P687, DOI 10.1109/ICDMW.2015.72
   Chen Y, 2017, INT J HUM-COMPUT INT, V33, P731, DOI 10.1080/10447318.2017.1282187
   Chen Y, 2015, LECT NOTES COMPUT SC, V9180, P153, DOI 10.1007/978-3-319-20907-4_14
   Collins C, 2009, COMPUT GRAPH FORUM, V28, P1039, DOI 10.1111/j.1467-8659.2009.01439.x
   Dang T., 2019, PROC EUROPEAN C VISU, P103, DOI DOI 10.2312/EVS.20191178
   Deng Yang, 2017, Journal of Computer Applications, V37, P1065, DOI 10.11772/j.issn.1001-9081.2017.04.1065
   EKMAN P, 1992, COGNITION EMOTION, V6, P169, DOI 10.1080/02699939208411068
   Fernandez-Beltran R, 2015, IMAGE VISION COMPUT, V38, P1, DOI 10.1016/j.imavis.2015.02.003
   Gan QH, 2014, WIRES COMPUT STAT, V6, P19, DOI 10.1002/wics.1285
   Gobron S, 2010, VISUAL COMPUT, V26, P505, DOI 10.1007/s00371-010-0446-x
   Hanada M, 2018, COLOR RES APPL, V43, P224, DOI 10.1002/col.22171
   Havre S, 2000, IEEE SYMPOSIUM ON INFORMATION VISUALIZATION 2000, P115, DOI 10.1109/INFVIS.2000.885098
   Hoque E, 2016, PROCEEDINGS OF THE 21ST INTERNATIONAL CONFERENCE ON INTELLIGENT USER INTERFACES (IUI'16), P96, DOI 10.1145/2856767.2856782
   Humayoun S.R., 2017, 19 EGVGTC C VIS EURO, P151
   Jänicke S, 2017, COMPUT GRAPH FORUM, V36, P226, DOI 10.1111/cgf.12873
   Jänicke S, 2018, VISIGRAPP 2018: PROCEEDINGS OF THE 13TH INTERNATIONAL JOINT CONFERENCE ON COMPUTER VISION, IMAGING AND COMPUTER GRAPHICS THEORY AND APPLICATIONS / INTERNATIONAL CONFERENCE ON INFORMATION VISUALIZATION THEORY AND APPLICATIONS (IVAPP), VOL 3, P40, DOI 10.5220/0006548000400051
   Kalra Gurjyot Singh, 2019, 2019 International Conference on Computing, Communication, and Intelligent Systems (ICCCIS), P74, DOI 10.1109/ICCCIS48478.2019.8974514
   Kucher K., 2019, PROC EUROPEAN C VISU, P29, DOI DOI 10.2312/EURP.20191138
   Kucher K, 2020, J VISUAL-JAPAN, V23, P1015, DOI 10.1007/s12650-020-00684-5
   Kucher K, 2018, COMPUT GRAPH FORUM, V37, P71, DOI 10.1111/cgf.13217
   Kucher K, 2016, INFORM VISUAL, V15, P93, DOI 10.1177/1473871615575079
   Kucher K, 2015, IEEE PAC VIS SYMP, P117, DOI 10.1109/PACIFICVIS.2015.7156366
   Li Z, 2020, IEEE ACCESS, V8, P75073, DOI 10.1109/ACCESS.2020.2986582
   Liu SX, 2014, VISUAL COMPUT, V30, P1373, DOI 10.1007/s00371-013-0892-3
   Liu SX, 2012, ACM T INTEL SYST TEC, V3, DOI 10.1145/2089094.2089101
   Lv GY, 2022, IEEE T BIG DATA, V8, P535, DOI 10.1109/TBDATA.2019.2950411
   Ma XJ, 2017, CSCW'17: PROCEEDINGS OF THE 2017 ACM CONFERENCE ON COMPUTER SUPPORTED COOPERATIVE WORK AND SOCIAL COMPUTING, P770, DOI 10.1145/2998181.2998256
   Ma Z., 2014, J JOURNALISM COMMUN, V8, P116
   Ming He, 2016, Database Systems for Advanced Applications. 21st International Conference, DASFAA 2016. Proceedings: LNCS 9643, P351, DOI 10.1007/978-3-319-32049-6_22
   Peng X., 2016, PAC AS C INF SYST, V252
   Ping Q., 2018, ACM C RECOMMENDER SY, P568
   PLUTCHIK R, 1960, J PSYCHOL, V50, P153, DOI 10.1080/00223980.1960.9916432
   Strobelt H, 2009, IEEE T VIS COMPUT GR, V15, P1145, DOI 10.1109/TVCG.2009.139
   Sun Z., 2016, SIGGRAPH ASIA 2016 S, P1
   Topal K, 2017, INF DISCOV DELIV, V45, P149, DOI 10.1108/IDD-05-2017-0045
   Viegas FB., 2008, INTERACTIONS, V15, P49, DOI [DOI 10.1145/1374489.1374501, 10.1145/1374489.1374501]
   Viégas FB, 2009, IEEE T VIS COMPUT GR, V15, P1137, DOI 10.1109/TVCG.2009.171
   Wall E, 2019, IEEE T VIS COMPUT GR, V25, P491, DOI 10.1109/TVCG.2018.2865146
   Wang SK, 2020, IEEE ACCESS, V8, P114123, DOI 10.1109/ACCESS.2020.3001046
   Wanner F., 2009, WORKSH VIS INT SOC S, P1
   Wu Q., 2019, ACM Transactions on Social Computing, V2, P1
   Xu L., 2008, J CHINA SOC SCI TECH, V27, P180, DOI DOI 10.3969/J.ISSN.1000-0135.2008.02.004
   Yao Y., 2017, P 2017 CHI C EXTENDE, P3034, DOI DOI 10.1145/3027063.3053258
   Yu ZW, 2016, ACM T KNOWL DISCOV D, V10, DOI 10.1145/2821513
   Zeng X. Q., 2019, Inf Comput, V18, P38
   Zhao J, 2014, IEEE CONF VIS ANAL, P203, DOI 10.1109/VAST.2014.7042496
   Zhao Y., 2016, HUMAN COMPUTER INTER, P467, DOI [https://doi.org/10.1007/978-3-319-39513-5_44, DOI 10.1007/978-3-319-39513-5_44]
   Zheng Y., 2016, DATA ANAL KNOWLEDGE, V31, P82
NR 51
TC 2
Z9 2
U1 11
U2 38
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2023
VL 39
IS 12
BP 6553
EP 6570
DI 10.1007/s00371-022-02748-z
EA DEC 2022
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA X7HK1
UT WOS:000903607000002
DA 2024-07-18
ER

PT J
AU Wu, YH
   Chang, XY
   Chen, DY
   Chen, L
   Jia, T
AF Wu, Yunhe
   Chang, Xingya
   Chen, Dongyue
   Chen, Lei
   Jia, Tong
TI Two-stage salient object detection based on prior distribution learning
   and saliency consistency optimization
SO VISUAL COMPUTER
LA English
DT Article
DE Salient object detection; Two-stage framework; Prior distribution
   learning; Learning process; Saliency consistency optimization
AB Although two-stage methods have favorably improved the accuracy and robustness of saliency detection, obtaining a saliency map with clear foreground boundaries and fine structure is still challenging. In this article, we proposed a novel and effective two-stage method to intelligently detect salient objects, which constitutes the coarse saliency map construction and the fine saliency map generation. Firstly, we develop the prior distribution learning algorithm (PDL) to explore the mapping relationship between input superpixel and its corresponding superpixels in various prior maps. The PDL can calculate the corresponding weights according to the contribution of various priors to each region in the image. Therefore, it can provide more reliable pseudo-labels for training subsequent learning models. Secondly, through learning the implicit representation between reliable samples and multiple priors, the learning model can accurately predict the salient values of those regions that are difficult to judge the saliency, so as to obtain an instructive coarse saliency map. Thirdly, in order to optimize the details of the coarse saliency map, we propose a framework called saliency consistency optimization, which can get clear foreground boundaries and effectively suppress the background noise. We compare the proposed algorithm with other state-of-the-art methods on four datasets. Experimental results adequately demonstrate the effectiveness of our approach over other comparison methods, especially two-stage-based methods.
C1 [Wu, Yunhe; Chang, Xingya; Chen, Dongyue; Jia, Tong] Northeastern Univ, Coll Informat Sci & Engn, Shenyang 110819, Peoples R China.
   [Chen, Lei] Northeastern Univ, Minist Educ, Key Lab Data Analyt & Optimizat Smart Ind, Shanghai 310118, Peoples R China.
C3 Northeastern University - China; Northeastern University - China
RP Jia, T (corresponding author), Northeastern Univ, Coll Informat Sci & Engn, Shenyang 110819, Peoples R China.
EM jiatong308@163.com
RI Wu, Yunhe/JWA-3597-2024
OI Wu, Yunhe/0000-0001-7049-7161
FU National Natural Science Foundation of China [U1613214, 62173083]; Major
   Program of National Natural Science Foundation of China [71790614]; 111
   Project [B16009]; National Key Research and Development Project
   [2018YFB1404101]; Fundamental Research Fund for the Central Universities
   of China [N170402008, N2026004]
FX Thanks are due to Ph.D.Yu Pang for inspiring our work. This work was
   supported in part by the National Natural Science Foundation of China
   under Grant Nos. U1613214 and 62173083, the Major Program of National
   Natural Science Foundation of China under Grant No.71790614 and the 111
   Project B16009, the National Key Research and Development Project Grant
   No.2018YFB1404101 and the Fundamental Research Fund for the Central
   Universities of China under Grant N170402008 and N2026004.
CR Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   Alexe B, 2012, IEEE T PATTERN ANAL, V34, P2189, DOI 10.1109/TPAMI.2012.28
   Borji A, 2012, LECT NOTES COMPUT SC, V7573, P414, DOI 10.1007/978-3-642-33709-3_30
   Chen SH, 2016, PATTERN RECOGN, V60, P2, DOI 10.1016/j.patcog.2016.05.016
   Chen XX, 2020, NEUROCOMPUTING, V381, P261, DOI 10.1016/j.neucom.2019.11.049
   Cheng MM, 2011, PROC CVPR IEEE, P409, DOI 10.1109/CVPR.2011.5995344
   Dou P, 2021, INT J APPL EARTH OBS, V103, DOI 10.1016/j.jag.2021.102477
   Elder, 2010, P IEEE C COMP VIS PA, P49, DOI [10.1109/CVPRW.2010.5543739, DOI 10.1109/CVPRW.2010.5543739]
   Fang S, 2017, IEEE T NEUR NET LEAR, V28, P1095, DOI 10.1109/TNNLS.2016.2522440
   Geng X, 2016, IEEE T KNOWL DATA EN, V28, P1734, DOI 10.1109/TKDE.2016.2545658
   Gong C, 2015, PROC CVPR IEEE, P2531, DOI 10.1109/CVPR.2015.7298868
   He SF, 2015, INT J COMPUT VISION, V115, P330, DOI 10.1007/s11263-015-0822-0
   Hwang I, 2017, MULTIMED TOOLS APPL, V76, P2111, DOI 10.1007/s11042-015-3171-7
   Islam MA, 2018, PROC CVPR IEEE, P7142, DOI 10.1109/CVPR.2018.00746
   Jian M., 2022, MULTIMED TOOLS APPL, P1
   Jian MW, 2021, INFORM SCIENCES, V576, P819, DOI 10.1016/j.ins.2021.08.069
   Jian MW, 2021, EXPERT SYST APPL, V168, DOI 10.1016/j.eswa.2020.114219
   Li HY, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2440174
   Li Y, 2014, PROC CVPR IEEE, P280, DOI 10.1109/CVPR.2014.43
   Liu GH, 2019, IEEE T IMAGE PROCESS, V28, P6, DOI 10.1109/TIP.2018.2847422
   Liu NA, 2020, IEEE T IMAGE PROCESS, V29, P6438, DOI 10.1109/TIP.2020.2988568
   Luo LK, 2019, VISUAL COMPUT, V35, P1869, DOI 10.1007/s00371-018-1580-0
   Lv G., 2022, VISUAL COMPUT, P1
   OTSU N, 1979, IEEE T SYST MAN CYB, V9, P62, DOI 10.1109/TSMC.1979.4310076
   Pang Y, 2020, J ELECTRON IMAGING, V29, DOI 10.1117/1.JEI.29.1.013011
   Peng HW, 2017, IEEE T PATTERN ANAL, V39, P818, DOI 10.1109/TPAMI.2016.2562626
   Pingping Zhang, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P202, DOI 10.1109/ICCV.2017.31
   Qin X., 2021, ARXIV
   Qin Y, 2018, INT J COMPUT VISION, V126, P751, DOI 10.1007/s11263-017-1062-2
   Qin Y, 2015, PROC CVPR IEEE, P110, DOI 10.1109/CVPR.2015.7298606
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sun JG, 2015, IEEE T IMAGE PROCESS, V24, P1639, DOI 10.1109/TIP.2015.2403241
   Tong N, 2015, PROC CVPR IEEE, P1884, DOI 10.1109/CVPR.2015.7298798
   Tong N, 2015, PATTERN RECOGN, V48, P3258, DOI 10.1016/j.patcog.2014.12.005
   Tu WC, 2016, PROC CVPR IEEE, P2334, DOI 10.1109/CVPR.2016.256
   Wang LJ, 2015, PROC CVPR IEEE, P3183, DOI 10.1109/CVPR.2015.7298938
   Wang QQ, 2020, SCI CHINA INFORM SCI, V63, DOI 10.1007/s11432-019-2713-1
   Wu YH, 2021, J VIS COMMUN IMAGE R, V75, DOI 10.1016/j.jvcir.2021.103048
   Xie JH, 2022, VISUAL COMPUT, V38, P2515, DOI 10.1007/s00371-021-02127-0
   Yan Q, 2013, PROC CVPR IEEE, P1155, DOI 10.1109/CVPR.2013.153
   Yang C, 2013, PROC CVPR IEEE, P3166, DOI 10.1109/CVPR.2013.407
   Yang C, 2013, IEEE SIGNAL PROC LET, V20, P637, DOI 10.1109/LSP.2013.2260737
   Yunhe Wu, 2020, 2020 International Conference on Culture-oriented Science & Technology (ICCST), P45, DOI 10.1109/ICCST50977.2020.00014
   Zhang LH, 2020, IEEE T IMAGE PROCESS, V29, P2258, DOI 10.1109/TIP.2019.2945679
   Zhang LH, 2018, IEEE T IMAGE PROCESS, V27, P987, DOI 10.1109/TIP.2017.2766787
   Zhang M, 2018, J VIS COMMUN IMAGE R, V53, P215, DOI 10.1016/j.jvcir.2018.03.019
   Zhang M, 2018, J VIS COMMUN IMAGE R, V52, P131, DOI 10.1016/j.jvcir.2018.01.004
   Zhuge M., 2021, ARXIV
   Zhuge MC, 2022, PATTERN RECOGN, V127, DOI 10.1016/j.patcog.2022.108644
NR 50
TC 4
Z9 4
U1 2
U2 17
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2023
VL 39
IS 11
BP 5729
EP 5745
DI 10.1007/s00371-022-02692-y
EA OCT 2022
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA W5SX6
UT WOS:000875798600002
DA 2024-07-18
ER

PT J
AU Duan, L
   Debattista, K
   Yue, GH
   Marnerides, D
   Chalmers, A
AF Duan, Lvyin
   Debattista, Kurt
   Yue, Guanghui
   Marnerides, Demetris
   Chalmers, Alan
TI A genetic algorithm for backlight dimming for HDR displays
SO VISUAL COMPUTER
LA English
DT Article
DE Local dimming algorithms; HDR displays; Genetic algorithm; Optimisation;
   Power consumption
ID IMAGE; POWER; LIT
AB High dynamic range (HDR) displays based on liquid crystal panels require local dimming algorithms to reproduce content with high fidelity and HDR. However, most local dimming algorithms are developed by using hand-crafted features and most of them focus on low dynamic range images rather than HDR images. In addition, few local dimming algorithms can manage well the conflicting requirements of maintaining image quality while minimising power consumption. In this paper, we propose a genetic algorithm for backlight local dimming (GABLD) that generates images for the HDR display based on the given fitness function and the HDR content about to be displayed while considering the impact of power consumption and image quality. The effectiveness of the GABLD method is demonstrated in three aspects: the fidelity of reconstruction of HDR images, the power consumption, and the visualisation of HDR content on HDR displays. The results show that the proposed method has good performance in terms of both the image quality and power consumption, and it retains more details especially for areas with highlights when compared to traditional methods.
C1 [Duan, Lvyin; Yue, Guanghui] Shenzhen Univ, Natl Reg Key Technol Engn Lab Med Ultrasound, Guangdong Key Lab Biomed Measurements & Ultrasoun, Sch Biomed Engn,Hlth Sci Ctr, Shenzhen, Peoples R China.
   [Debattista, Kurt; Marnerides, Demetris; Chalmers, Alan] Univ Warwick, Warwick Mfg Grp, Coventry CV4 7AL, W Midlands, England.
C3 Shenzhen University; University of Warwick
RP Duan, L (corresponding author), Shenzhen Univ, Natl Reg Key Technol Engn Lab Med Ultrasound, Guangdong Key Lab Biomed Measurements & Ultrasoun, Sch Biomed Engn,Hlth Sci Ctr, Shenzhen, Peoples R China.
EM duanlvyin@tju.edu.cn
CR Albrecht M, 2010, IEICE T ELECTRON, VE93C, P1556, DOI [10.1587/transele.E93.C1556, 10.1587/transele.E93.C.1556]
   Aydin TO, 2008, PROC SPIE, V6806, DOI 10.1117/12.765095
   Burini N, 2013, J DISP TECHNOL, V9, P656, DOI 10.1109/JDT.2013.2253544
   Burini N, 2012, IEEE IMAGE PROC, P2797, DOI 10.1109/ICIP.2012.6467480
   Canon, 2016, EOS 5D MARK
   Cha S, 2015, J DISP TECHNOL, V11, P378, DOI 10.1109/JDT.2015.2401604
   Chen EG, 2021, OPT EXPRESS, V29, P12179, DOI 10.1364/OE.421346
   Cho H, 2009, IEEE T CONSUM ELECTR, V55, P839, DOI 10.1109/TCE.2009.5174463
   Debattista K, 2018, COMPUT GRAPH FORUM, V37, P439, DOI 10.1111/cgf.13307
   Debattista K, 2015, VISUAL COMPUT, V31, P1089, DOI 10.1007/s00371-015-1121-z
   Debevec Paul E, 2008, ACM SIGGRAPH 2008 CL, P1, DOI DOI 10.1145/1401132.1401174
   Duan LY, 2020, IEEE ACCESS, V8, P51692, DOI 10.1109/ACCESS.2020.2980075
   Espejo PG, 2010, IEEE T SYST MAN CY C, V40, P121, DOI 10.1109/TSMCC.2009.2033566
   Fairchild MD, 2007, FIFTEENTH COLOR IMAGING CONFERENCE: COLOR SCIENCE AND ENGINEERING SYSTEMS, TECHNOLOGIES, AND APPLICATIONS, FINAL PROGRAM AND PROCEEDINGS, P233
   Forchhammer S, 2017, IEEE IMAGE PROC, P1177, DOI 10.1109/ICIP.2017.8296467
   Funamoto T., 2000, IDW '00. Proceedings of the Seventh International Display Workshops, P1157
   Hatchett J, 2018, VISUAL COMPUT, V34, P167, DOI 10.1007/s00371-016-1322-0
   Holland J.H., 1992, Adaptation in Natural and Artificial Systems, DOI DOI 10.7551/MITPRESS/1090.001.0001
   Hsia SC, 2013, J DISP TECHNOL, V9, P527, DOI 10.1109/JDT.2013.2237755
   Hsiang EL, 2020, OPT EXPRESS, V28, P36822, DOI 10.1364/OE.413133
   Kang SJ, 2011, J DISP TECHNOL, V7, P544, DOI 10.1109/JDT.2011.2158985
   Kim SE, 2009, J SOC INF DISPLAY, V17, P1051, DOI 10.1889/JSID17.12.1051
   Lai CC, 2008, IEEE T CONSUM ELECTR, V54, P669, DOI 10.1109/TCE.2008.4560145
   Lai YK, 2011, J DISP TECHNOL, V7, P550, DOI 10.1109/JDT.2011.2162314
   Lin FC, 2008, J DISP TECHNOL, V4, P139, DOI 10.1109/JDT.2008.920175
   Mantel C, 2013, J DISP TECHNOL, V9, P933, DOI 10.1109/JDT.2013.2260131
   Mantiuk R, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964935
   Marchessoux C, 2016, MED PHYS, V43, P4023, DOI 10.1118/1.4953187
   Masuda Takeshi, 2019, SID Symposium Digest of Technical Papers, V50, P390, DOI 10.1002/sdtp.12939
   Murali P, 2023, VISUAL COMPUT, V39, P1057, DOI 10.1007/s00371-021-02384-z
   Narwaria M., 2016, HIGH DYNAMIC RANGE V, P371, DOI [10.1016/B978-0-08-100412-8.00014-0, DOI 10.1016/B978-0-08-100412-8.00014-0]
   Nordin P., 1996, Genetic Programming. Proceedings of the First Annual Conference 1996, P345
   Seetzen H, 2004, ACM T GRAPHIC, V23, P760, DOI 10.1145/1015706.1015797
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P430, DOI 10.1109/TIP.2005.859378
   Shu X, 2013, IEEE T IMAGE PROCESS, V22, P166, DOI 10.1109/TIP.2012.2211371
   SIM2, 2015, SIM2 HDR47 ENG DISPL
   Sung In Cho, 2011, 2011 International SoC Design Conference (ISOCC 2011), P274, DOI 10.1109/ISOCC.2011.6138763
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Zerman E, 2015, PROC SPIE, V9599, DOI 10.1117/12.2186674
   Zhang T, 2022, EXPERT SYST APPL, V204, DOI 10.1016/j.eswa.2022.117468
   Zhang T, 2020, APPL INTELL, V50, P4162, DOI 10.1007/s10489-020-01769-2
   Zhang T, 2018, IEEE ACCESS, V6, P40472, DOI 10.1109/ACCESS.2018.2858827
   Zhang XB, 2012, J DISP TECHNOL, V8, P108, DOI 10.1109/JDT.2011.2165935
NR 44
TC 2
Z9 2
U1 3
U2 13
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2023
VL 39
IS 11
BP 5437
EP 5450
DI 10.1007/s00371-022-02670-4
EA SEP 2022
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA W5SX6
UT WOS:000858024400001
DA 2024-07-18
ER

PT J
AU Salam, AA
   Mahadevappa, M
   Das, A
   Nair, MS
AF Salam, Amritha Abdul
   Mahadevappa, Manjunatha
   Das, Asha
   Nair, Madhu S.
TI RDD-Net: retinal disease diagnosis network: a computer-aided diagnosis
   technique using graph learning and feature descriptors
SO VISUAL COMPUTER
LA English
DT Article
DE SIFT; BRISK; ORB; Graph neural network; Non-Euclidean geometric space
ID CONVOLUTIONAL NEURAL-NETWORKS; CLASSIFICATION; RECOGNITION; RETINOPATHY;
   PREVALENCE
AB Ocular diseases are a prevalent disease among the aging population across the world. The retinal damage and vision loss can be substantially decreased through early-stage diagnosis with computer-aided ocular disease diagnosis. With the use of color fundus photography for obtaining digital retinal fundus images, there is a growth in the online accessibility of digital fundus images. For diagnosing ocular diseases, an attempt was made to model graphs from images for feature learning. Three feature detection algorithms, namely scale-invariant feature transform, binary robust invariant scalable keypoints and oriented fast and rotated BRIEF (ORB) techniques are computed individually. As graphs are represented in the non-Euclidean domain, the graph neural network is used to learn the node embedding to model the network for ocular disease diagnosis. Three distance measures: the Euclidean, Manhattan and Chebyshev distances, are computed for analyzing the discriminative power of the model. The proposed RDD-Net model is trained and evaluated on the ODIR-2019 dataset with eleven different performance indicators. The results show that mapping images to non-Euclidean geometric space have obtained a successful diagnosis of ocular diseases from digital fundus images. The ORB descriptor outperforms the other two feature descriptors as well as the existing algorithms for ocular disease diagnosis. Results of the Chebyshev distance measure show superior performance when compared to the other two distance measures based on computation time and performance evaluation metrics. The proposed RDD-Net achieves an F1-score of 0.9970 and a sensitivity of 0.9969 with the ORB descriptor and shows state-of-the-art performance.
C1 [Salam, Amritha Abdul; Mahadevappa, Manjunatha] Indian Inst Technol Kharagpur, Sch Med Sci & Technol, Kharagpur 721302, W Bengal, India.
   [Das, Asha] Union Christian Coll, Dept Comp Sci, Aluva 683102, Kerala, India.
   [Nair, Madhu S.] Cochin Univ Sci & Technol, Dept Comp Sci, Artificial Intelligence & Comp Vis Lab, Kochi 682022, Kerala, India.
C3 Indian Institute of Technology System (IIT System); Indian Institute of
   Technology (IIT) - Kharagpur; Cochin University Science & Technology
RP Mahadevappa, M (corresponding author), Indian Inst Technol Kharagpur, Sch Med Sci & Technol, Kharagpur 721302, W Bengal, India.
EM mmaha2@smst.iitkgp.ac.in
RI Nair, Madhu S./B-7069-2013
OI Nair, Madhu S./0000-0001-6039-5727
FU Ministry of Electronics and Information Technology (MeitY); Department
   of Science and Technology (DST), Government of India
FX National Supercomputing Mission (NSM) is acknowledged for providing
   computing resources for 'PARAM Shakti' at IIT Kharagpur, which is
   administered by C-DAC and supported by the Ministry of Electronics and
   Information Technology (MeitY) and Department of Science and Technology
   (DST), Government of India.
CR Abràmoff MD, 2010, OPHTHALMOLOGY, V117, P1147, DOI 10.1016/j.ophtha.2010.03.046
   Ahmadian N, 2022, VEHICLE SYST DYN, V60, P1742, DOI 10.1080/00423114.2021.1879390
   Allison K, 2020, CUREUS J MED SCIENCE, V12, DOI 10.7759/cureus.11686
   Bresnick GH, 2000, OPHTHALMOLOGY, V107, P19, DOI 10.1016/S0161-6420(99)00010-X
   Calonder M, 2010, LECT NOTES COMPUT SC, V6314, P778, DOI 10.1007/978-3-642-15561-1_56
   Chandrasekaran R, 2023, VISUAL COMPUT, V39, P2741, DOI 10.1007/s00371-022-02489-z
   Chen YL, 2021, BMC CANCER, V21, DOI 10.1186/s12885-021-07843-3
   Chhabra P, 2020, NEURAL COMPUT APPL, V32, P2725, DOI 10.1007/s00521-018-3677-9
   CHOLLET F, 2017, PROC CVPR IEEE, P1800, DOI DOI 10.1109/CVPR.2017.195
   Dominic N, 2021, COMMUN MATH BIOL NEU, DOI 10.28919/cmbn/5565
   Gauglitz S, 2011, INT J COMPUT VISION, V94, P335, DOI 10.1007/s11263-011-0431-5
   Gómez-Valverde JJ, 2019, BIOMED OPT EXPRESS, V10, P892, DOI 10.1364/BOE.10.000892
   Grassmann F, 2018, OPHTHALMOLOGY, V125, P1410, DOI 10.1016/j.ophtha.2018.02.037
   Gulshan V, 2016, JAMA-J AM MED ASSOC, V316, P2402, DOI 10.1001/jama.2016.17216
   Gupta S, 2021, VISUAL COMPUT, V37, P447, DOI 10.1007/s00371-020-01814-8
   Hashemi H, 2020, EYE, V34, P1357, DOI 10.1038/s41433-020-0806-3
   HUANG G, 2017, PROC CVPR IEEE, P2261, DOI DOI 10.1109/CVPR.2017.243
   Islam Md Tariqul, 2019, 2019 IEEE International Conference on Signal Processing, Information, Communication & Systems (SPICSCON), P59, DOI 10.1109/SPICSCON48833.2019.9065162
   Jafri R, 2014, VISUAL COMPUT, V30, P1197, DOI 10.1007/s00371-013-0886-1
   Jordi C., OCULAR DIS INTELLIGE
   Karthikeyan R, 2018, J MED SYST, V42, DOI 10.1007/s10916-018-1055-x
   Kensert A, 2019, SLAS DISCOV, V24, P466, DOI 10.1177/2472555218818756
   Kingma D. P., 2014, arXiv
   Leutenegger S, 2011, IEEE I CONF COMP VIS, P2548, DOI 10.1109/ICCV.2011.6126542
   Li Q, 2020, PATTERN RECOGN LETT, V138, P378, DOI 10.1016/j.patrec.2020.07.040
   Lin JK, 2021, IEEE SIGNAL PROC LET, V28, P454, DOI 10.1109/LSP.2021.3057548
   Lin Y, 2020, MATH FDN COMPUT, V3, P279, DOI 10.3934/mfc.2020018
   Lowe G., 2004, Int. J, V2, P2
   Luo D, 2020, IEEE IJCNN, DOI 10.1109/ijcnn48605.2020.9207186
   Luo X, 2021, IEEE J BIOMED HEALTH, V25, P3332, DOI 10.1109/JBHI.2021.3083605
   Mair E, 2010, LECT NOTES COMPUT SC, V6312, P183, DOI 10.1007/978-3-642-15552-9_14
   Mills KT, 2020, NAT REV NEPHROL, V16, P223, DOI 10.1038/s41581-019-0244-2
   Morris C, 2019, AAAI CONF ARTIF INTE, P4602
   Paszke A, 2019, ADV NEUR IN, V32
   Pratt H, 2016, PROCEDIA COMPUT SCI, V90, P200, DOI 10.1016/j.procs.2016.07.014
   Priya R. P. R., 2012, Int. J. Comput. Appl., V41, P6, DOI 10.5120/5503-7503
   Pusztai Z., 2016, Quantitative comparison of feature matchers implemented in opencv3
   Rublee E, 2011, IEEE I CONF COMP VIS, P2564, DOI 10.1109/ICCV.2011.6126544
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Sarki R., 2019, BIORXIV
   Scarselli F, 2009, IEEE T NEURAL NETWOR, V20, P61, DOI 10.1109/TNN.2008.2005605
   Sharma S, 2020, J DIGIT IMAGING, V33, P632, DOI 10.1007/s10278-019-00307-y
   Sharma S, 2020, VISUAL COMPUT, V36, P1755, DOI 10.1007/s00371-019-01768-6
   Sheet SSM, 2022, ICT EXPRESS, V8, P142, DOI 10.1016/j.icte.2021.05.002
   Shibuya E, 2022, VISUAL COMPUT, V38, P3791, DOI 10.1007/s00371-021-02221-3
   U RA, 2008, J MED SYST, V32, P481, DOI 10.1007/s10916-008-9154-8
   Urban S, 2015, ISPRS ANN PHOTO REM, VII-3, P121, DOI 10.5194/isprsannals-II-3-W5-121-2015
   Viswanathan D.G., 2009, P 10 WORKSH IM AN MU, P6
   Weisfeiler B., 1968, Nauchno-Technicheskaya Informatsiya, V2, P12
   Wen G, 2013, OPHTHALMOLOGY, V120, P2109, DOI 10.1016/j.ophtha.2013.06.039
   Wong WL, 2014, LANCET GLOB HEALTH, V2, pE106, DOI 10.1016/S2214-109X(13)70145-1
   Zhang MH, 2018, AAAI CONF ARTIF INTE, P4438
   Zhang W, 2020, VISUAL COMPUT, V36, P2433, DOI 10.1007/s00371-020-01955-w
   Zheng Yingfeng, 2012, Indian J Ophthalmol, V60, P428, DOI 10.4103/0301-4738.100542
NR 54
TC 3
Z9 3
U1 0
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2023
VL 39
IS 10
BP 4657
EP 4670
DI 10.1007/s00371-022-02615-x
EA AUG 2022
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA T1GK8
UT WOS:000834687900002
DA 2024-07-18
ER

PT J
AU Gao, Y
   Dai, M
   Zhang, Q
AF Gao, Yue
   Dai, Meng
   Zhang, Qing
TI Cross-modal and multi-level feature refinement network for RGB-D salient
   object detection
SO VISUAL COMPUTER
LA English
DT Article
DE RGB-D salient object detection; Cross-modal feature interaction;
   Multi-level feature fusion; Skip connection
ID FUSION
AB RGB-D salient object detection (SOD) methods adopt depth maps as important supplementary information in order to identify salient objects more accurately. However, there are still two main challenges in the existing RGB-D SOD methods. One typical issue is how to obtain effective cross-modal features, and another issue is how to optimize the integration of multi-level features. To tackle these two issues, we propose a novel cross-modal and multi-level feature refinement network which equips with a cross-modal feature interaction module and a multi-level feature fusion module. Specifically, a cross-modal feature interaction module is designed to enhance depth features from both channel and spatial perspectives and then effectively integrate cross-modal features. Moreover, considering the characteristics of different levels of features, we propose a multi-level feature fusion module which combines contextual information from multi-level features by means of skip connection. Extensive experiments on five benchmark datasets demonstrate that our proposed model outperforms other 17 state-of-the-art RGB-D SOD methods.
C1 [Gao, Yue; Dai, Meng; Zhang, Qing] Shanghai Inst Technol, Sch Comp Sci & Informat Engn, Shanghai, Peoples R China.
C3 Shanghai Institute of Technology
RP Dai, M (corresponding author), Shanghai Inst Technol, Sch Comp Sci & Informat Engn, Shanghai, Peoples R China.
EM 1369365773@qq.com; daimeng@sit.edu.cn; zhangqing@sit.edu.cn
FU Natural Science Foundation of Shanghai [19ZR1455300, 21ZR1462600];
   National Natural Science Foundation of China [61806126]
FX This work is supported by Natural Science Foundation of Shanghai under
   Grant Nos. 19ZR1455300 and 21ZR1462600, and National Natural Science
   Foundation of China Under Grant No. 61806126.
CR Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   Agrawal A, 2020, VISUAL COMPUT, V36, P405, DOI 10.1007/s00371-019-01630-9
   [Anonymous], 2016, Proceedings of the IEEE conference on computer vision and pattern recognition, DOI DOI 10.1109/CVPR.2016.257
   Ao Luo, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P346, DOI 10.1007/978-3-030-58610-2_21
   Chen CLZ, 2021, IEEE T IMAGE PROCESS, V30, P2350, DOI 10.1109/TIP.2021.3052069
   Chen H, 2019, IEEE T IMAGE PROCESS, V28, P2825, DOI 10.1109/TIP.2019.2891104
   Chen H, 2018, PROC CVPR IEEE, P3051, DOI 10.1109/CVPR.2018.00322
   Chen H, 2019, PATTERN RECOGN, V86, P376, DOI 10.1016/j.patcog.2018.08.007
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen S., 2020, ECCV, P520, DOI DOI 10.1007/978-3-030-58598-3_31
   Chen ZY, 2021, IEEE T IMAGE PROCESS, V30, P7012, DOI 10.1109/TIP.2020.3028289
   Cheng MM, 2021, INT J COMPUT VISION, V129, P2622, DOI [10.1007/s11263-021-01490-8, 10.1109/ICCV.2017.487]
   Cheng Y, 2014, IEEE INT CON MULTI
   Ciptadi A, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.112
   Cong RM, 2016, IEEE SIGNAL PROC LET, V23, DOI 10.1109/LSP.2016.2557347
   Craye C, 2016, IEEE INT CONF ROBOT, P2303, DOI 10.1109/ICRA.2016.7487379
   De Boer PT, 2005, ANN OPER RES, V134, P19, DOI 10.1007/s10479-005-5724-z
   Debelee TG, 2019, J BIOMIM BIOMATER BI, V42, P79, DOI 10.4028/www.scientific.net/JBBBE.42.79
   Deng-Ping Fan, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P275, DOI 10.1007/978-3-030-58610-2_17
   Desingh K, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.98
   Fan DP, 2021, IEEE T NEUR NET LEAR, V32, P2075, DOI 10.1109/TNNLS.2020.2996406
   Fan DP, 2019, PROC CVPR IEEE, P8546, DOI 10.1109/CVPR.2019.00875
   Fan XX, 2014, INT CONF DIGIT SIG, P454, DOI 10.1109/ICDSP.2014.6900706
   Fang YM, 2014, IEEE T IMAGE PROCESS, V23, P2625, DOI 10.1109/TIP.2014.2305100
   Fu KR, 2020, PROC CVPR IEEE, P3049, DOI 10.1109/CVPR42600.2020.00312
   Han JW, 2018, IEEE T CYBERNETICS, V48, P3171, DOI 10.1109/TCYB.2017.2761775
   Hongyi Su, 2015, Intelligent Computing Theories and Methodologies. 11th International Conference, ICIC 2015. Proceedings: LNCS 9225, P1, DOI 10.1007/978-3-319-22180-9_1
   Ji W, 2021, PROC CVPR IEEE, P9466, DOI 10.1109/CVPR46437.2021.00935
   Jiang B, 2021, IEEE T MULTIMEDIA, V23, P1343, DOI 10.1109/TMM.2020.2997184
   Ju R, 2014, IEEE IMAGE PROC, P1115, DOI 10.1109/ICIP.2014.7025222
   Li GY, 2021, IEEE T IMAGE PROCESS, V30, P3528, DOI 10.1109/TIP.2021.3062689
   Li NY, 2014, PROC CVPR IEEE, P2806, DOI 10.1109/CVPR.2014.359
   Li XJ, 2020, VISUAL COMPUT, V36, P39, DOI 10.1007/s00371-018-1582-y
   Liu GH, 2014, PROCEEDINGS OF 2013 INTERNATIONAL CONFERENCE ON INFORMATION SCIENCE AND CLOUD COMPUTING COMPANION (ISCC-C), P728, DOI 10.1109/ISCC-C.2013.21
   Liu ZY, 2021, VISUAL COMPUT, V37, P529, DOI 10.1007/s00371-020-01821-9
   Liu ZY, 2019, NEUROCOMPUTING, V363, P46, DOI 10.1016/j.neucom.2019.07.012
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Margolin R, 2014, PROC CVPR IEEE, P248, DOI 10.1109/CVPR.2014.39
   Máttyus G, 2017, IEEE I CONF COMP VIS, P3458, DOI 10.1109/ICCV.2017.372
   Mozaffari M. Hamed, 2020, Advances in Visual Computing. 15th International Symposium, ISVC 2020. Proceedings. Lecture Notes in Computer Science (LNCS 12510), P421, DOI 10.1007/978-3-030-64559-5_33
   Nian Liu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13753, DOI 10.1109/CVPR42600.2020.01377
   Peng HW, 2014, LECT NOTES COMPUT SC, V8691, P92, DOI 10.1007/978-3-319-10578-9_7
   Perazzi F, 2012, PROC CVPR IEEE, P733, DOI 10.1109/CVPR.2012.6247743
   Piao YR, 2019, IEEE I CONF COMP VIS, P7253, DOI 10.1109/ICCV.2019.00735
   Qin XB, 2019, PROC CVPR IEEE, P7471, DOI 10.1109/CVPR.2019.00766
   Qu LQ, 2017, IEEE T IMAGE PROCESS, V26, P2274, DOI 10.1109/TIP.2017.2682981
   Ren JQ, 2015, IEEE COMPUT SOC CONF, DOI 10.1109/CVPRW.2015.7301391
   Shigematsu R, 2017, IEEE INT CONF COMP V, P2749, DOI 10.1109/ICCVW.2017.323
   Song HK, 2017, IEEE T IMAGE PROCESS, V26, P4204, DOI 10.1109/TIP.2017.2711277
   Tasi CC, 2019, IEEE T IMAGE PROCESS, V28, P56, DOI 10.1109/TIP.2018.2861217
   Tu ZZ, 2021, IEEE T IMAGE PROCESS, V30, P5678, DOI 10.1109/TIP.2021.3087412
   Wei Ji, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12363), P52, DOI 10.1007/978-3-030-58523-5_4
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Xiaoqi Zhao, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12367), P646, DOI 10.1007/978-3-030-58542-6_39
   Youwei Pang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12370), P235, DOI 10.1007/978-3-030-58595-2_15
   Youwei Pang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9410, DOI 10.1109/CVPR42600.2020.00943
   Zhang HL, 2012, INT CONF COMP SCI ED, P763, DOI 10.1109/ICCSE.2012.6295184
   Zhang Jing, 2020, P IEEECVF C COMPUTER, P12546, DOI DOI 10.1109/CVPR42600.2020.01256
   Zhang M, 2020, PROC CVPR IEEE, P3469, DOI 10.1109/CVPR42600.2020.00353
   Zhang WB, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P731, DOI 10.1145/3474085.3475240
   Zhang XN, 2018, PROC CVPR IEEE, P714, DOI 10.1109/CVPR.2018.00081
   Zhang Z, 2021, IEEE T IMAGE PROCESS, V30, P1949, DOI 10.1109/TIP.2021.3049959
   Zhao JW, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1745, DOI 10.1145/3394171.3413855
   Zhao J, 2020, IEEE T FUZZY SYST, V28, P2287, DOI 10.1109/TFUZZ.2019.2930492
   Zhu CB, 2019, IEEE INT CON MULTI, P199, DOI 10.1109/ICME.2019.00042
   Zhu CB, 2017, IEEE INT CONF COMP V, P1509, DOI 10.1109/ICCVW.2017.178
NR 66
TC 7
Z9 7
U1 2
U2 21
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2023
VL 39
IS 9
BP 3979
EP 3994
DI 10.1007/s00371-022-02543-w
EA JUN 2022
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA Q8PV6
UT WOS:000818675500001
DA 2024-07-18
ER

PT J
AU Tsai, WK
   Hsu, TH
AF Tsai, Wen-Kai
   Hsu, Ting-Hao
TI A low computational complexity algorithm for real-time salient object
   detection
SO VISUAL COMPUTER
LA English
DT Article
DE Real-time salient object detection; Saliency map; Spatial distribution
   prior; Salient object mask
ID REGION DETECTION; COLOR CONTRAST; MODEL
AB Image saliency detection is a process for highlighting the most salient object in an image and presenting the image saliency map. The content of an image is chaotic, including a complex background, low contrast, and an irregular salient object appearance. To overcome these problems, many algorithms have high computational complexity. In this paper, an efficient and fast-performing saliency detection algorithm is proposed, which consists of initiation saliency map generation and saliency map refinement. In the generation stage, the color-based contrast prior and color-based spatial distribution prior are effectively described in the image. Subsequently, two prior results (contrast value and distribution value) are fused to obtain an initial saliency map. In the refinement stage, the initial saliency map is refined by visual focus and an adaptive salient object mask (SOM). Due to the simplicity of the proposed algorithm, the system can detect salient objects in real time. Experimental evaluation on the benchmark shows that the proposed method can achieve sufficient accuracy and reliability while showing the lowest execution time. Compared with other methods, the execution time of the proposed method can achieve 137 frames per second (FPS) for the dataset with average image size 386 x 292.
C1 [Tsai, Wen-Kai; Hsu, Ting-Hao] Natl Formosa Univ, Dept Elect Engn, Huwei, Yunlin, Taiwan.
C3 National Formosa University
RP Tsai, WK (corresponding author), Natl Formosa Univ, Dept Elect Engn, Huwei, Yunlin, Taiwan.
EM twk@nfu.edu.tw
CR Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   Annum R, 2018, SIGNAL IMAGE VIDEO P, V12, P505, DOI 10.1007/s11760-017-1186-4
   Borji A, 2015, IEEE T IMAGE PROCESS, V24, P742, DOI 10.1109/TIP.2014.2383320
   Chen ZH, 2015, IET IMAGE PROCESS, V9, P758, DOI 10.1049/iet-ipr.2014.0987
   Cheng MM, 2013, IEEE I CONF COMP VIS, P1529, DOI 10.1109/ICCV.2013.193
   Cheng MM, 2011, PROC CVPR IEEE, P409, DOI 10.1109/CVPR.2011.5995344
   Fareed MMS, 2018, COMPUT ELECTR ENG, V70, P551, DOI 10.1016/j.compeleceng.2017.08.027
   Feng WJ, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20092656
   Fu KR, 2013, SIGNAL PROCESS-IMAGE, V28, P1448, DOI 10.1016/j.image.2013.07.005
   Gao F, 2019, IET INTELL TRANSP SY, V13, P515, DOI 10.1049/iet-its.2018.5197
   Huang XM, 2017, IEEE T IMAGE PROCESS, V26, P4243, DOI 10.1109/TIP.2017.2710636
   Lie MMI, 2016, SIBGRAPI, P217, DOI [10.1109/SIBGRAPI.2016.038, 10.1109/SIBGRAPI.2016.36]
   Ji YZ, 2018, NEUROCOMPUTING, V322, P130, DOI 10.1016/j.neucom.2018.09.061
   Jiang BW, 2013, IEEE I CONF COMP VIS, P1665, DOI 10.1109/ICCV.2013.209
   Jiang P, 2013, IEEE I CONF COMP VIS, P1976, DOI 10.1109/ICCV.2013.248
   Johnson Solwin, 2017, Materials Today: Proceedings, V4, P4169, DOI 10.1016/j.matpr.2017.02.119
   Kuang HL, 2018, IEEE T INTELL TRANSP, V19, P814, DOI 10.1109/TITS.2017.2702665
   Li XH, 2013, IEEE I CONF COMP VIS, P2976, DOI 10.1109/ICCV.2013.370
   Lie MMI, 2018, PATTERN RECOGN LETT, V114, P22, DOI 10.1016/j.patrec.2017.09.010
   Liu T, 2011, IEEE T PATTERN ANAL, V33, P353, DOI 10.1109/TPAMI.2010.70
   Muthuswamy K, 2015, IET COMPUT VIS, V9, P428, DOI 10.1049/iet-cvi.2013.0298
   Pang Y, 2019, J VIS COMMUN IMAGE R, V65, DOI 10.1016/j.jvcir.2019.102676
   Qin Y, 2015, PROC CVPR IEEE, P110, DOI 10.1109/CVPR.2015.7298606
   Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720
   Singh N, 2016, DIGIT SIGNAL PROCESS, V55, P22, DOI 10.1016/j.dsp.2016.05.003
   Su, 2017, P 10 INT S IMAGE SIG
   Tang JL, 2019, NEUROCOMPUTING, V332, P270, DOI 10.1016/j.neucom.2018.12.052
   Tong N, 2014, IEEE SIGNAL PROC LET, V21, P1035, DOI 10.1109/LSP.2014.2323407
   Vig E, 2014, PROC CVPR IEEE, P2798, DOI 10.1109/CVPR.2014.358
   Wang G, 2017, J VIS COMMUN IMAGE R, V48, P432, DOI 10.1016/j.jvcir.2017.02.004
   Wu BF, 2005, IEICE T INF SYST, VE88D, P1716, DOI 10.1093/ietisy/e88-d.7.1716
   Xu M, 2015, VISUAL COMPUT, V31, P355, DOI 10.1007/s00371-014-0930-9
   Yang JF, 2017, IET COMPUT VIS, V11, P710, DOI 10.1049/iet-cvi.2016.0469
   Zeng Y, 2019, IEEE I CONF COMP VIS, P7222, DOI 10.1109/ICCV.2019.00732
   Zhai Y., 2006, P 14 ACM INT C MULT
   Zhang JM, 2015, IEEE I CONF COMP VIS, P1404, DOI 10.1109/ICCV.2015.165
   Zhu WJ, 2014, PROC CVPR IEEE, P2814, DOI 10.1109/CVPR.2014.360
NR 37
TC 3
Z9 3
U1 2
U2 15
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2023
VL 39
IS 7
BP 3059
EP 3072
DI 10.1007/s00371-022-02513-2
EA JUN 2022
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA L4HX1
UT WOS:000805908600002
DA 2024-07-18
ER

PT J
AU Malakar, S
   Sahoo, S
   Chakraborty, A
   Sarkar, R
   Nasipuri, M
AF Malakar, Samir
   Sahoo, Samanway
   Chakraborty, Anuran
   Sarkar, Ram
   Nasipuri, Mita
TI Handwritten Arabic and Roman word recognition using holistic approach
SO VISUAL COMPUTER
LA English
DT Article
DE Handwritten word recognition; Document image; Hausdorff distance;
   Frechet distance; IAM and IFN; ENIT database
ID CITY-NAME RECOGNITION; DATABASE
AB The research community considers handwritten word recognition (HWR) as an open research problem to date. The reasons behind this are variations in intra-/interpersonal writing style, overlapping and/or touching characters in a word, degraded scanned document images, etc. Two major approaches, namely holistic and analytical, are followed by the researchers while designing an HWR system. In this work, we have followed the holistic approach as it works well on limited and pre-defined lexicon as compared to the analytical approach. As observed in the literature related to handwritten word recognition, irrespective of the approaches, researchers generally extract various local features from hypothetically partitioned segments of a word image while dealing with the said problem. However, no such work has been found which has considered inter-segment similarity that might carry some distinct information about different patterns (here, word segments). To this end, in the present work, we have used Hausdorff and Frechet distances to quantize the similarity among all possible word segments taking two at a time. Along with this, conventional chain code histogram (a shape-based feature descriptor) and modified negative refraction-based shape transformation features have been used. Finally, a majority voting schema is used to combine outputs from six different classifiers. The model has been evaluated on two standard databases, namely IAM and IFN/ENIT, and the results obtained are promising in comparison with state-of-the-art holistic word recognition methods. Moreover, a performance comparison of the present method with some deep learning models confirms the usefulness of the proposed method.
C1 [Malakar, Samir] Asutosh Coll, Dept Comp Sci, Kolkata, India.
   [Sahoo, Samanway] Indian Inst Engn Sci & Technol, Dept Comp Sci & Technol, Sibpur, India.
   [Chakraborty, Anuran; Sarkar, Ram; Nasipuri, Mita] Jadavpur Univ, Dept Comp Sci & Engn, Kolkata, India.
C3 Indian Institute of Engineering Science Technology Shibpur (IIEST);
   Jadavpur University
RP Malakar, S (corresponding author), Asutosh Coll, Dept Comp Sci, Kolkata, India.
EM malakarsamir@gmail.com; samanwaysahoo@gmail.com; ch.anuraan@gmail.com;
   ramjucse@gmail.com; mitanasipuri@gmail.com
RI Sarkar, Ram/AAX-3822-2020
OI Sarkar, Ram/0000-0001-8813-4086
CR Al-Nuzaili QA., 2017, INT J COMPUT VIS ROB, V7, P99, DOI [10.1504/IJCVR.2017.081243, DOI 10.1504/IJCVR.2017.081243]
   AlKhateeb J.H., 2009, 2009 IEEE INT C SIGN
   Amrouch M, 2016, I C COMP SYST APPLIC
   [Anonymous], IAM ON LINE HANDWRIT
   Awni M., 2019, 2019 14 INT C COMP E, P4045
   Barua S, 2017, ADV INTELL SYST, V515, P343, DOI 10.1007/978-981-10-3153-3_34
   Bhattacharya R., 2021, PATTERN RECOGN
   Bhowmik S, 2019, NEURAL COMPUT APPL, V31, P5783, DOI 10.1007/s00521-018-3389-1
   Bhowmik S, 2014, 2014 6TH INTERNATIONAL CONFERENCE ON COMPUTATIONAL INTELLIGENCE AND COMMUNICATION NETWORKS, P257, DOI 10.1109/CICN.2014.66
   Bianne-Bernard AL, 2011, IEEE T PATTERN ANAL, V33, P2066, DOI 10.1109/TPAMI.2011.22
   Cajote R.D., 2004, 2004 IEEE REG 10 C T
   Chatterjee A, 2018, PROC INT CONF EMERG
   Das D, 2020, IET IMAGE PROCESS, V14, P1794, DOI 10.1049/iet-ipr.2019.1398
   Dasgupta J, 2016, PATTERN RECOGN LETT, V79, P73, DOI 10.1016/j.patrec.2016.05.017
   Eiter T., 1994, Computing Discrete Frechet Distance
   Essa N, 2018, ETRI J, V40, P774, DOI 10.4218/etrij.2017-0248
   Fan YL, 2020, SIGNAL IMAGE VIDEO P, V14, P455, DOI 10.1007/s11760-019-01574-6
   Gatos B, 2006, INT C PATT RECOG, P998
   Ghosh S, 2021, VISUAL COMPUT, V37, P1781, DOI 10.1007/s00371-020-01938-x
   Guo H, 2022, VISUAL COMPUT, V38, P3897, DOI 10.1007/s00371-021-02230-2
   Hassen H, 2017, 2017 1ST INTERNATIONAL WORKSHOP ON ARABIC SCRIPT ANALYSIS AND RECOGNITION (ASAR), P79, DOI 10.1109/ASAR.2017.8067764
   Hijam D, 2022, VISUAL COMPUT, V38, P525, DOI 10.1007/s00371-020-02032-y
   Inunganbi S, 2021, VISUAL COMPUT, V37, P291, DOI 10.1007/s00371-020-01799-4
   Jayadevan R, 2011, PROC INT CONF DOC, P304, DOI 10.1109/ICDAR.2011.69
   Jino PJ, 2017, Int J Eng Technol, V9, DOI [10.21817/ijet/2017/v9i2/170902250, DOI 10.21817/IJET/2017/V9I2/170902250]
   Kacem A, 2012, INT CONF FRONT HAND, P268, DOI 10.1109/ICFHR.2012.276
   Kaur H, 2021, SOFT COMPUT, V25, P4451, DOI 10.1007/s00500-020-05455-w
   Khémiri A, 2016, INT CONF FRONT HAND, P560, DOI [10.1109/ICFHR.2016.101, 10.1109/ICFHR.2016.0108]
   Khémiri A, 2014, INT CONF FRONT HAND, P678, DOI 10.1109/ICFHR.2014.119
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kundu S, 2020, NEURAL COMPUT APPL, V32, P7879, DOI 10.1007/s00521-019-04235-4
   Kundu S, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21144648
   Madhvanath S, 2001, IEEE T PATTERN ANAL, V23, P149, DOI 10.1109/34.908966
   Madhvanath S, 1999, IEEE T PATTERN ANAL, V21, P1344, DOI 10.1109/34.817412
   Mahjoub M.A., 2013, ARXIV PREPR
   Majid N., 2019, 2019 INT C DOC AN RE
   Majumder S, 2021, MULTIMED TOOLS APPL, V80, P12411, DOI 10.1007/s11042-020-10363-0
   Malakar Samir, 2017, International Journal of Computer Vision and Image Processing, V7, P59, DOI 10.4018/IJCVIP.2017010104
   Malakar S, 2021, NEURAL COMPUT APPL, V33, P449, DOI 10.1007/s00521-020-04981-w
   Malakar S, 2020, NEURAL COMPUT APPL, V32, P15209, DOI 10.1007/s00521-020-04872-0
   Malakar S, 2020, J INTELL SYST, V29, P719, DOI 10.1515/jisys-2017-0384
   Malakar S, 2020, NEURAL COMPUT APPL, V32, P2533, DOI 10.1007/s00521-018-3937-8
   Malakar S, 2011, COMM COM INF SC, V157, P511
   Mohamed, 2013, APPL NEGATIVE REFRAC
   Mondal R, 2022, MULTIMED TOOLS APPL, V81, P975, DOI 10.1007/s11042-021-11425-7
   Pal U, 2012, INT CONF FRONT HAND, P169, DOI 10.1109/ICFHR.2012.238
   Pal U, 2009, IEICE T INF SYST, VE92D, P1146, DOI 10.1587/transinf.E92.D.1146
   Pechwitz M., 2002, FRANC INT C WRIT DOC
   Pramanik R, 2021, NEURAL COMPUT APPL, V33, P9329, DOI 10.1007/s00521-021-05693-5
   Rucklidge W, 1996, Efficient Visual Recognition Using the Hausdorff Distance, P27
   Ruiz-Pinales J, 2007, PATTERN RECOGN LETT, V28, P1600, DOI 10.1016/j.patrec.2007.03.017
   Sahoo S., 2018, 6 INT C FRONT INT CO
   Sahoo S, 2018, J INTELL FUZZY SYST, V35, P1765, DOI 10.3233/JIFS-169712
   Singh S., 2015, P 2015 8 INT C ADV P, P15
   Tamen Z, 2017, PATTERN RECOGN LETT, V93, P123, DOI 10.1016/j.patrec.2017.01.020
   WARRINGTON EK, 1980, BRAIN, V103, P99, DOI 10.1093/brain/103.1.99
   Zhu L, 2014, 2014 IEEE WORKSHOP ON ELECTRONICS, COMPUTER AND APPLICATIONS, P97, DOI 10.1109/IWECA.2014.6845566
   Zimmermann M, 2002, INT C PATT RECOG, P35, DOI 10.1109/ICPR.2002.1047394
NR 58
TC 3
Z9 3
U1 1
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2023
VL 39
IS 7
BP 2909
EP 2932
DI 10.1007/s00371-022-02500-7
EA MAY 2022
PG 24
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA L4HX1
UT WOS:000797750800001
DA 2024-07-18
ER

PT J
AU Qiu, R
   Cai, ZH
   Chang, ZQ
   Liu, SB
   Tu, GQ
AF Qiu, Run
   Cai, Zhaohui
   Chang, Zhuoqing
   Liu, Shubo
   Tu, Guoqing
TI A two-stage image process for water level recognition via dual-attention
   CornerNet and CTransformer
SO VISUAL COMPUTER
LA English
DT Article
DE Intelligent water conservancy; Deep learning; Object detection; Text
   recognition
AB Image processing-based water level detectors have promising practical application value in intelligent agriculture and early water logging alerts. However, water level recognition based on image processing faces illumination, shooting angle, and sediment contamination challenges. In addition, due to the influence of water surface reflection, it is not easy to extract the water level ruler (WLR) on the water surface accurately. This paper proposes a novel dual-attention CornerNet for WLR image extraction and CTransformer for WLR sequence recognition. First, a dual-attention mechanism to obtain the global information is introduced to better predict semantic segmentation feature maps and corner information. Then, asymmetric convolution Resnet-50 is used to extract multi-local information to effectively recognize inconsistent character sizes caused by different shooting angles of WLRs. Recently, the design of vision backbone using self-attention becomes an exciting topic. In this work, an improved CTransformer is designed to retain sufficient global context information and extract more differentiated features for sequence recognition via multi-head self-attention. Evaluation using our in-house dataset shows that the proposed framework achieves an F-score of 91.37 in the detection stage and the accuracy of human estimation error within 0.3 cm in the recognition stage is 95.37%, respectively. The proposed method is also evaluated on several benchmarks. Experiment results demonstrate that the method in this paper is superior to the existing methods.
C1 [Qiu, Run; Cai, Zhaohui; Chang, Zhuoqing; Liu, Shubo; Tu, Guoqing] Wuhan Univ, Sch Comp Sci, Wuhan, Peoples R China.
C3 Wuhan University
RP Cai, ZH (corresponding author), Wuhan Univ, Sch Comp Sci, Wuhan, Peoples R China.
EM zhcai@whu.edu.cn
FU Major National Science and Technology Projects [2017ZX07108-001]; Wuhan
   Frontier Project on Applied Foundations [2020020601012266]
FX The research funding was supported by Major National Science and
   Technology Projects under Grant No. 2017ZX07108-001 and The Wuhan
   Frontier Project on Applied Foundations under Grant No.
   2020020601012266.
CR [Anonymous], 2016, INT CONF 3D VISION, DOI DOI 10.1109/3DV.2016.79
   [Anonymous], 1997, MULTISCALE RETINEX B
   Bai Nong, 2016, ARXIV160609002
   Chang ZQ, 2021, IEEE INTERNET THINGS, V8, P13849, DOI 10.1109/JIOT.2021.3088875
   Chen GJ, 2021, SIGNAL IMAGE VIDEO P, V15, P33, DOI 10.1007/s11760-020-01719-y
   Chen YJ, 2019, IEEE I CONF COMP VIS, P6960, DOI 10.1109/ICCV.2019.00706
   Das DK, 2022, VISUAL COMPUT, V38, P3803, DOI 10.1007/s00371-021-02222-2
   de Vitry MM, 2019, HYDROL EARTH SYST SC, V23, P4621, DOI 10.5194/hess-23-4621-2019
   Deng D., 2018, P AAAI C ARTIFICIAL
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Dosovitskiy A, 2021, arXiv, DOI [10.48550/ARXIV.2010.11929, DOI 10.48550/ARXIV.2010.11929]
   Etter S, 2020, HYDROL PROCESS, V34, P4365, DOI 10.1002/hyp.13864
   Fu C.-Y., 2017, DSSD: Deconvolutional Single Shot Detector
   Graves A., 2006, P 23 INT C MACH LEAR
   Gupta A., 2016, P IEEE C COMPUTER VI
   Gupta S, 2020, PROCEEDINGS OF THE CONFLUENCE 2020: 10TH INTERNATIONAL CONFERENCE ON CLOUD COMPUTING, DATA SCIENCE & ENGINEERING, P260, DOI [10.1109/confluence47617.2020.9057842, 10.1109/Confluence47617.2020.9057842]
   He K., 2016, PROC CVPR IEEE, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]
   He MC, 2018, INT C PATT RECOG, P7, DOI 10.1109/ICPR.2018.8546143
   He P., 2017, P IEEE INT C COMPUTE
   Huayong L., 2015, YELLOW RIVER, V37, P28
   Jaderberg M., 2014, CORR
   Karatzas D., 2013, 2013 12 INT C DOCUME
   Karatzas D., 2015, 2015 13 INT C DOCUME
   Kingma D. P., 2014, arXiv
   Lee C.-Y., 2016, P IEEE C COMPUTER VI
   Liao MH, 2017, AAAI CONF ARTIF INTE, P4161
   Lin F, 2020, J COASTAL RES, P185, DOI 10.2112/JCR-SI105-039.1
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Liu X., 2018, P IEEE C COMPUTER VI
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Long S., 2018, P EUROPEAN C COMPUTE, P2036
   Lyu P., 2018, P IEEE C COMPUTER VI
   Mondal MA, 2018, PROCEEDINGS OF THE 8TH INTERNATIONAL CONFERENCE CONFLUENCE 2018 ON CLOUD COMPUTING, DATA SCIENCE AND ENGINEERING, P625, DOI 10.1109/CONFLUENCE.2018.8442535
   Qiao Z., 2020, P IEEE CVF C COMP VI
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Shi B., 2017, P IEEE C COMPUTER VI
   Shi B., 2016, P IEEE C COMPUTER VI
   Shi BG, 2019, IEEE T PATTERN ANAL, V41, P2035, DOI 10.1109/TPAMI.2018.2848939
   Shi BG, 2017, IEEE T PATTERN ANAL, V39, P2298, DOI 10.1109/TPAMI.2016.2646371
   Tian Z, 2016, LECT NOTES COMPUT SC, V9912, P56, DOI 10.1007/978-3-319-46484-8_4
   Tu ZG, 2019, IEEE T CIRC SYST VID, V29, P1423, DOI 10.1109/TCSVT.2018.2830102
   Tu ZG, 2019, IEEE T IMAGE PROCESS, V28, P2799, DOI 10.1109/TIP.2018.2890749
   Tu ZG, 2018, PATTERN RECOGN, V79, P32, DOI 10.1016/j.patcog.2018.01.020
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang W., 2019, P IEEECVF INT C COMP
   Wang X, 2017, Point linking network for object detection
   Yao C., 2014, UNIFIED FRAMEWORK MU
   Yao C., 2012, 2012 IEEE C COMPUTER
   Zhang J., 2018, 2018 IEEE INT C MULT, P16
   Zhang YB, 2023, VISUAL COMPUT, V39, P1283, DOI 10.1007/s00371-022-02404-6
   Zhou X., 2017, P IEEE C COMPUTER VI
NR 51
TC 6
Z9 6
U1 2
U2 34
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2023
VL 39
IS 7
BP 2933
EP 2952
DI 10.1007/s00371-022-02501-6
EA MAY 2022
PG 20
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA L4HX1
UT WOS:000792533800002
DA 2024-07-18
ER

PT J
AU Li, J
   Wei, LX
   Wen, YT
   Liu, XG
   Wang, HR
AF Li, Jun
   Wei, Lixin
   Wen, Yintang
   Liu, Xiaoguang
   Wang, Hongrui
TI An approach to continuous hand movement recognition using SEMG based on
   features fusion
SO VISUAL COMPUTER
LA English
DT Article
DE Hand movement recognition; Surface electromyography; Feature fusion;
   Long short-term memory; Convolutional neural network
AB With the continuous development of EMG acquisition technology and artificial intelligence technology, EMG signal analysis has been extensively studied in human-computer interaction, rehabilitation training, prosthetic control and remote device control. As hand movements become more and more complex, hand movement recognition based on surface electromyography (sEMG) has become a hotspot. In this paper, by using multi-features fusion-based Long Short-Term Memory convolutional neural network (MFFCNN-LSTM), a continuous hand movement recognition method based on time-domain and time-frequency-spectrum features of forearm sEMG signal is proposed. Ten basic hand movements including rest action are identified. Firstly, the hand movement data is cut from NinaPro db8 dataset to extract effective sEMG signal fragments. Secondly, the empirical Fourier decomposition method is used to denoise the sEMG signals. Thirdly, the time-domain and time-frequency-spectrum features of sEMG signals from different channels are extracted, and sent to two parallel CNN networks to extract the high-dimension features, respectively. Fourthly, the high-dimension features are fused as the input of LSTM, a fully connected layer and a softmax layer to recognize the continuous hand movements. Finally, MFFCNN-LSTM is compared with the support vector machine, CNN and LSTM on the same computer. The experimental results show that the recognition accuracy, sensitivity and specificity of MFFCNN-LSTM on NinaPro db8 dataset are 98.5%, 95.25% and 95.5%, respectively. It has higher recognition accuracy on the five public datasets than other methods.
C1 [Li, Jun; Wei, Lixin; Wen, Yintang; Wang, Hongrui] Yanshan Univ, Coll Elect & Informat Engn, Qinhuangdao 066000, Hebei, Peoples R China.
   [Li, Jun; Liu, Xiaoguang; Wang, Hongrui] Hebei Univ, Coll Elect & Informat Engn, Baoding 071002, Peoples R China.
C3 Yanshan University; Hebei University
RP Wang, HR (corresponding author), Yanshan Univ, Coll Elect & Informat Engn, Qinhuangdao 066000, Hebei, Peoples R China.; Wang, HR (corresponding author), Hebei Univ, Coll Elect & Informat Engn, Baoding 071002, Peoples R China.
EM hongruihbu@163.com
RI Wei, Lixin/AFJ-0797-2022
FU National Key Research and Development Program of China [2017YFB1401200];
   Key Project of Hebei Province Department of Education [ZD2020146]; Hebei
   Province Postdoctoral Scientific Research Project [B2019005001]; Program
   for Top 100 Innovative Talents in Colleges and Universities of Hebei
   Province [SLRC2017022]; Natural Science Foundation of China [61703133,
   61673158]
FX This work was supported in part by the National Key Research and
   Development Program of China under Grant 2017YFB1401200; in part by the
   Key Project of Hebei Province Department of Education under Grant
   ZD2020146; in part by the Hebei Province Postdoctoral Scientific
   Research Project under Grant B2019005001; in part by the Program for Top
   100 Innovative Talents in Colleges and Universities of Hebei Province
   under grant SLRC2017022; and in part by the Natural Science Foundation
   of China under grant 61703133 and 61673158.
CR Asif AR, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20061642
   Astuti W., 2012, 2012 IEEE Symposium on Industrial Electronics and Applications (ISIEA 2012), P372, DOI 10.1109/ISIEA.2012.6496663
   Bilal S, 2013, ARTIF INTELL REV, V40, P495, DOI 10.1007/s10462-011-9292-0
   Chen GL, 2020, COMPUT INTEL NEUROSC, V2020, DOI 10.1155/2020/8871605
   Chen HF, 2018, INT CONF MACH LEARN, P619, DOI 10.1109/ICMLC.2018.8526976
   Chen WJ, 2019, PROCEEDINGS OF 2019 IEEE 8TH JOINT INTERNATIONAL INFORMATION TECHNOLOGY AND ARTIFICIAL INTELLIGENCE CONFERENCE (ITAIC 2019), P230, DOI [10.1109/ITAIC.2019.8785542, 10.1109/itaic.2019.8785542]
   Du Y, 2017, SENSORS-BASEL, V17, DOI 10.3390/s17030458
   Erozen A, 2020, Journal of Innovative Science and Engineering (JISE), V4, P44, DOI 10.38088/jise.730957
   Fatimah B, 2021, BIOCYBERN BIOMED ENG, V41, P690, DOI 10.1016/j.bbe.2021.03.004
   Georgi M., 2015, Recognizing Hand and Finger Gestures with IMU based Motion and EMG based Muscle Activity Sensing, P99, DOI DOI 10.5220/0005276900990108
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Hochreiter S, 1998, INT J UNCERTAIN FUZZ, V6, P107, DOI 10.1142/S0218488598000094
   Huang, 2005, WORLD SCI, DOI [10.1142/5862, DOI 10.1142/5862]
   Jabbari M, 2020, IEEE ENG MED BIO, P3302, DOI 10.1109/EMBC44109.2020.9175279
   Jain R, 2022, VISUAL COMPUT, V38, P1957, DOI 10.1007/s00371-021-02259-3
   Jaramillo-Yanez A., 2020, BOOK SHORT TERM HAND
   Jordan M. I., 1986, P 8 ANN C COGN SCI S, P531
   Kaczmarek P, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19163548
   Krasoulis A., 2010, NEUROSCIENCE, V13, P891, DOI [10.3389/fnins.2019.00891, DOI 10.3389/FNINS.2019.00891]
   Kumar B. P., 2016, INDIAN J SCI TECHNOL, V9, P1, DOI DOI 10.17485/ijst/2017/v9iS1/111145
   LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541
   Lee C, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20164566
   Li H, 2020, IEEE T IND INFORM, V16, P1885, DOI 10.1109/TII.2019.2931140
   Lobov S, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18041122
   Mahmoud SA, 2014, CIRC SYST SIGNAL PR, V33, P2385, DOI 10.1007/s00034-014-9761-1
   Pinzón-Arenas JO, 2019, SYMP IMAG SIG PROC A, DOI 10.1109/stsiva.2019.8730272
   Park Haejune, 2020, [Journal of Institute of Control, Robotics and Systems, 제어.로봇.시스템학회 논문지], V26, P312, DOI 10.5302/J.ICROS.2020.20.0018
   Phinyomark A., 2018, Big Data Cognitive Computing, V2, DOI DOI 10.3390/BDCC2030021
   Prakash KB., 2020, Int J Adv Trends Comp Sci Eng, V9, P3216
   RAhsan MR, 2011, 2011 4 INT C MECH IC
   Rheem H, 2021, APPL COGNITIVE PSYCH, V35, P606, DOI 10.1002/acp.3784
   Saeed B, 2021, ARAB J SCI ENG, V46, P1761, DOI 10.1007/s13369-020-05044-x
   Saetta G, 2020, SCI DATA, V7, DOI 10.1038/s41597-020-0402-1
   Sbrollini A, 2016, IEEE ENG MED BIO, P3622, DOI 10.1109/EMBC.2016.7591512
   Singh P, 2017, P ROY SOC A-MATH PHY, V473, DOI 10.1098/rspa.2016.0871
   Tan YS, 2021, NEURAL COMPUT APPL, V33, P5339, DOI 10.1007/s00521-020-05337-0
   Too J., 2018, SENSOR LETT, V16, P92, DOI DOI 10.1166/SL.2018.3926
   Velandia NS., 2019, J ENG APPL SCI, V14, P3528, DOI [10.36478/jeasci.2019.3528.3537, DOI 10.36478/JEASCI.2019.3528.3537]
   Vishwakarma DK, 2017, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON INTELLIGENT SUSTAINABLE SYSTEMS (ICISS 2017), P429, DOI 10.1109/ISS1.2017.8389446
   Wang B, 2020, MECHATRON SYST CONTR, V48, P17, DOI 10.2316/J.2020.201-0014
   Wang C, 2020, BIOMED SIGNAL PROCES, V59, DOI 10.1016/j.bspc.2019.101774
   Winarno H.A., 2019, J PHYS C SER, V1577
   Xing YN, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.590164
   Zhang WJ, 2021, IEEE-CAA J AUTOMATIC, V8, P110, DOI 10.1109/JAS.2020.1003465
   Zhou W, 2020, SIGNAL PROCESSING
NR 45
TC 8
Z9 9
U1 8
U2 36
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2023
VL 39
IS 5
BP 2065
EP 2079
DI 10.1007/s00371-022-02465-7
EA APR 2022
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA D7ZK3
UT WOS:000784277000001
DA 2024-07-18
ER

PT J
AU Pang, Y
   Wu, CD
   Wu, H
   Yu, XS
AF Pang, Yu
   Wu, Chengdong
   Wu, Hao
   Yu, Xiaosheng
TI Over-sampling strategy-based class-imbalanced salient object detection
   and its application in underwater scene
SO VISUAL COMPUTER
LA English
DT Article
DE Salient object detection; Class-Imbalance problem; Over-sampling
   strategy; Saliency optimization; Underwater scene recognition
AB One major branch of bottom-up salient object detection methods is machine learning-based methods which learn to classify salient object(positive) and background(negative) based on various learning algorithms. Generally, each input image corresponds to a training set which is determined by prior knowledge. However, we find that the class-imbalance problem (i.e., positive and negative data are seriously imbalanced in quantity) is inevitable for these methods due to the existence of various factors. Imbalanced training set might fail to make learning process succeed. To solve above problem, we propose a novel bottom-up Salient object detection algorithm based on Class-Imbalance Learning(CILS). For the input image, our goal is to ensure that a robust saliency classifier is well learned even if class-imbalance problem occurs in training set. To this end, we propose a novel over-sampling strategy which concentrates on constructing synthetic samples to make two classes be balanced. As a result, the balanced training data could be better applied to subsequent learning and classification process. Finally, the saliency map obtained by CILS is further refined by a novel optimization framework via foreground consistency and background consistency. Our proposed algorithm and other state-of-the-art algorithms are tested on three datasets. Results demonstrate adequately that our model produces better performance than other compared algorithms. Furthermore, we apply our salient object detection algorithm to underwater object recognition task and recognition accuracy could be further improved by introducing saliency cues.
C1 [Pang, Yu; Wu, Chengdong; Yu, Xiaosheng] Northeastern Univ, Fac Robot Sci & Engn, Shenyang 110169, Liaoning, Peoples R China.
   [Wu, Hao] Univ Sydney, Engn Fac, Sydney, NSW 2006, Australia.
C3 Northeastern University - China; University of Sydney
RP Pang, Y (corresponding author), Northeastern Univ, Fac Robot Sci & Engn, Shenyang 110169, Liaoning, Peoples R China.
EM pangyu@stumail.neu.edu.cn
RI Wu, Chengdong/IST-5302-2023
FU National Natural Science Foundation of China [U20A20197, 61973063];
   Liaoning Key Research and Development Project [2020JH2/10100040];
   Natural Science Foundation of Liaoning Province [2021-KF-12-01];
   Foundation of National Key Laboratory [OEIP-O-202005]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant nos. U20A20197, 61973063, Liaoning Key
   Research and Development Project 2020JH2/10100040, Natural Science
   Foundation of Liaoning Province 2021-KF-12-01 and the Foundation of
   National Key Laboratory OEIP-O-202005.
CR Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   Alexe B, 2012, IEEE T PATTERN ANAL, V34, P2189, DOI 10.1109/TPAMI.2012.28
   Cao KD, 2019, ADV NEUR IN, V32
   Chawla N.V., EDITORIAL SPECIAL IS
   Chawla NV, 2002, J ARTIF INTELL RES, V16, P321, DOI 10.1613/jair.953
   Deng C, 2020, IEEE T MULTIMEDIA, V22, P885, DOI 10.1109/TMM.2019.2934833
   Elder, 2010, P IEEE C COMP VIS PA, P49, DOI [10.1109/CVPRW.2010.5543739, DOI 10.1109/CVPRW.2010.5543739]
   Fang S, 2017, IEEE T NEUR NET LEAR, V28, P1095, DOI 10.1109/TNNLS.2016.2522440
   Geng X, 2016, IEEE T KNOWL DATA EN, V28, P1734, DOI 10.1109/TKDE.2016.2545658
   Gong C, 2015, PROC CVPR IEEE, P2531, DOI 10.1109/CVPR.2015.7298868
   Gong XP, 2020, IMAGE VISION COMPUT, V101, DOI 10.1016/j.imavis.2020.103973
   Guo TD, 2021, VISUAL COMPUT, V37, P2069, DOI 10.1007/s00371-020-01964-9
   He HB, 2008, IEEE IJCNN, P1322, DOI 10.1109/IJCNN.2008.4633969
   He SF, 2015, INT J COMPUT VISION, V115, P330, DOI 10.1007/s11263-015-0822-0
   Huang F, 2017, IEEE T IMAGE PROCESS, V26, P1911, DOI 10.1109/TIP.2017.2669878
   Hwang I, 2017, MULTIMED TOOLS APPL, V76, P2111, DOI 10.1007/s11042-015-3171-7
   Li HY, 2021, IEEE T IMAGE PROCESS, V30, P2016, DOI 10.1109/TIP.2021.3049955
   Li HY, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2440174
   Li Y, 2020, PROC CVPR IEEE, P906, DOI 10.1109/CVPR42600.2020.00099
   Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324
   Lin X, 2019, IEEE T MULTIMEDIA, V21, P1646, DOI 10.1109/TMM.2018.2884474
   Liu GH, 2019, IEEE T IMAGE PROCESS, V28, P6, DOI 10.1109/TIP.2018.2847422
   Liu RS, 2020, IEEE T CIRC SYST VID, V30, P4861, DOI 10.1109/TCSVT.2019.2963772
   Liu XY, 2009, IEEE T SYST MAN CY B, V39, P539, DOI 10.1109/TSMCB.2008.2007853
   Lu HC, 2016, IEEE T IMAGE PROCESS, V25, DOI 10.1109/TIP.2016.2524198
   Pang JM, 2019, PROC CVPR IEEE, P821, DOI 10.1109/CVPR.2019.00091
   Paris S, 2009, INT J COMPUT VISION, V81, P24, DOI 10.1007/s11263-007-0110-8
   Peng HW, 2017, IEEE T PATTERN ANAL, V39, P818, DOI 10.1109/TPAMI.2016.2562626
   Qian MY, 2019, PATTERN RECOGN, V96, DOI 10.1016/j.patcog.2019.06.021
   Qin Y, 2018, INT J COMPUT VISION, V126, P751, DOI 10.1007/s11263-017-1062-2
   Qin Y, 2015, PROC CVPR IEEE, P110, DOI 10.1109/CVPR.2015.7298606
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song GR, 2020, OPT LASER ENG, V128, DOI 10.1016/j.optlaseng.2019.106000
   Sun JG, 2015, IEEE T IMAGE PROCESS, V24, P1639, DOI 10.1109/TIP.2015.2403241
   Tong N, 2015, PROC CVPR IEEE, P1884, DOI 10.1109/CVPR.2015.7298798
   Tong N, 2015, PATTERN RECOGN, V48, P3258, DOI 10.1016/j.patcog.2014.12.005
   Tu WC, 2016, PROC CVPR IEEE, P2334, DOI 10.1109/CVPR.2016.256
   Tu ZZ, 2020, IEEE T MULTIMEDIA, V22, P160, DOI 10.1109/TMM.2019.2924578
   Wang LJ, 2017, PROC CVPR IEEE, P3796, DOI 10.1109/CVPR.2017.404
   Wang LJ, 2015, PROC CVPR IEEE, P3183, DOI 10.1109/CVPR.2015.7298938
   Wei GQ, 2021, EXPERT SYST APPL, V168, DOI 10.1016/j.eswa.2020.114381
   Yan Q, 2013, PROC CVPR IEEE, P1155, DOI 10.1109/CVPR.2013.153
   Yang C, 2013, PROC CVPR IEEE, P3166, DOI 10.1109/CVPR.2013.407
   Zeng Y, 2019, PROC CVPR IEEE, P6067, DOI 10.1109/CVPR.2019.00623
   Zeng Y, 2018, IEEE T IMAGE PROCESS, V27, P4545, DOI 10.1109/TIP.2018.2838761
   Zhang LH, 2020, IEEE T IMAGE PROCESS, V29, P2258, DOI 10.1109/TIP.2019.2945679
   Zhang LH, 2018, IEEE T IMAGE PROCESS, V27, P987, DOI 10.1109/TIP.2017.2766787
   Zhang M, 2018, J VIS COMMUN IMAGE R, V53, P215, DOI 10.1016/j.jvcir.2018.03.019
   Zhang M, 2018, J VIS COMMUN IMAGE R, V52, P131, DOI 10.1016/j.jvcir.2018.01.004
   Zhang MH, 2019, ADV NEUR IN, V32
   Zhao X., 2015, P EUR C COMP VIS, P35
   Zhou Y, 2019, IEEE T MULTIMEDIA, V21, P74, DOI 10.1109/TMM.2018.2845667
NR 53
TC 7
Z9 7
U1 5
U2 28
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2023
VL 39
IS 5
BP 1959
EP 1974
DI 10.1007/s00371-022-02458-6
EA APR 2022
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA D7ZK3
UT WOS:000781196500002
DA 2024-07-18
ER

PT J
AU Sandhya, M
   Morampudi, MK
   Pruthweraaj, I
   Garepally, PS
AF Sandhya, Mulagala
   Morampudi, Mahesh Kumar
   Pruthweraaj, Indragante
   Garepally, Pranay Sai
TI Multi-instance cancelable iris authentication system using triplet loss
   for deep learning models
SO VISUAL COMPUTER
LA English
DT Article
DE Cancelable biometrics; Privacy-preserving; Triplet loss; Convolutional
   neural network; Artificial neural network
ID RECOGNITION; BIOMETRICS; TEMPLATES; MACHINE
AB Many government and commercial organizations are using biometric authentication systems instead of a password or token-based authentication systems. They are computationally expensive if more users are involved. To overcome this problem, a biometric system can be created and deployed in the cloud which then can be used as a biometric authentication service. Privacy is the major concern with cloud-based authentication services as biometric is irrevocable. Many biometric authentication systems based on cancelable biometrics are developed to solve the privacy concern in the past few years. But the existing methods fail to maintain the trade-off between speed, security, and accuracy. To overcome this, we present a multi-instance cancelable iris system (MICBTDL). MICBTDL uses a convolutional neural network trained using triplet loss for feature extraction and stores the feature vector as a cancelable template. Our system uses an artificial neural network as the comparator module instead of the similarity measures. Experiments are carried on IITD and MMU iris databases to check the effectiveness of MICBTDL. Experimental results demonstrate that MICBTDL accomplishes fair performance when compared to other existing works.
C1 [Sandhya, Mulagala; Pruthweraaj, Indragante; Garepally, Pranay Sai] Natl Inst Technol, Dept Comp Sci & Engn, Warangal 506004, Telangana, India.
   [Morampudi, Mahesh Kumar] SRM Univ AP, Sch Engn & Appl Sci, Dept Comp Sci & Engn, Neerukonda Kurugallu Village, Mangalagiri Mandal 522502, Andhra Pradesh, India.
C3 National Institute of Technology (NIT System); National Institute of
   Technology Warangal; SRM University-AP
RP Morampudi, MK (corresponding author), SRM Univ AP, Sch Engn & Appl Sci, Dept Comp Sci & Engn, Neerukonda Kurugallu Village, Mangalagiri Mandal 522502, Andhra Pradesh, India.
EM msandhya@nitw.ac.in; morampudimahesh@gmail.com;
   indragantipruthviraj@gmail.com; pranaysai27@gmail.com
RI Sandhya, Mulagala/GWZ-7411-2022
OI Sandhya, Mulagala/0000-0003-2149-8584; morampudi,
   mahesh/0000-0002-6888-4637
CR Abdellatef E, 2020, VISUAL COMPUT, V36, P1097, DOI 10.1007/s00371-019-01715-5
   Acar A, 2018, ACM COMPUT SURV, V51, DOI 10.1145/3214303
   Adamovic S, 2020, FUTURE GENER COMP SY, V107, P144, DOI 10.1016/j.future.2020.01.056
   Ahmadi N, 2020, NEURAL COMPUT APPL, V32, P2267, DOI 10.1007/s00521-018-3754-0
   Ahmadi N, 2018, IET BIOMETRICS, V7, P153, DOI 10.1049/iet-bmt.2017.0041
   Al-Waisy AS, 2018, PATTERN ANAL APPL, V21, P783, DOI 10.1007/s10044-017-0656-1
   Arsalan M, 2019, EXPERT SYST APPL, V122, P217, DOI 10.1016/j.eswa.2019.01.010
   Daugman J, 2004, IEEE T CIRC SYST VID, V14, P21, DOI 10.1109/TCSVT.2003.818350
   De Marsico M, 2016, PATTERN RECOGN LETT, V82, P106, DOI 10.1016/j.patrec.2016.02.001
   El-Hameed HAA., 2021, Vis. Comput, V6, P1
   Galbally J, 2013, COMPUT VIS IMAGE UND, V117, P1512, DOI 10.1016/j.cviu.2013.06.003
   Gale A., 2019, ICDSMLA 2019 SPRING, P117
   Gomez-Barrero M, 2018, INFORM FUSION, V42, P37, DOI 10.1016/j.inffus.2017.10.003
   Gupta K, 2021, VISUAL COMPUT, V37, P1401, DOI 10.1007/s00371-020-01873-x
   Jain AK, 2007, Handbook of biometrics, DOI DOI 10.1007/978-0-387-71041-9
   Jartelius Martin, 2020, Network Security, V2020, P9, DOI 10.1016/S1353-4858(20)30079-9
   Kertesz G., 2021, 2021 IEEE 19 WORLD S
   Khan MFF, 2017, 2017 2ND IEEE INTERNATIONAL CONFERENCE ON TELECOMMUNICATIONS AND PHOTONICS (ICTP), P33, DOI 10.1109/ICTP.2017.8285897
   Khedkar MM, 2013, 2013 INTERNATIONAL CONFERENCE ON INFORMATION COMMUNICATION AND EMBEDDED SYSTEMS (ICICES), P78, DOI 10.1109/ICICES.2013.6508356
   Kumar A, 2010, PATTERN RECOGN, V43, P1016, DOI 10.1016/j.patcog.2009.08.016
   Kumar MM, 2018, P 2018 2 INT C BIOM, P43, DOI DOI 10.1145/3230820.3230828
   Li XH, 2022, IEEE GEOSCI REMOTE S, V19, DOI 10.1109/LGRS.2021.3098774
   Kumar MM, 2020, IET BIOMETRICS, V9, P165, DOI 10.1049/iet-bmt.2019.0169
   Morampudi MK, 2021, APPL INTELL, V51, P6823, DOI 10.1007/s10489-021-02187-8
   Morampudi MK, 2020, MULTIMED TOOLS APPL, V79, P19215, DOI 10.1007/s11042-020-08680-5
   Patel VM, 2015, IEEE SIGNAL PROC MAG, V32, P54, DOI 10.1109/MSP.2015.2434151
   Prabhakar S., 2003, IEEE Security & Privacy, V1, P33, DOI 10.1109/MSECP.2003.1193209
   Rai H, 2014, EXPERT SYST APPL, V41, P588, DOI 10.1016/j.eswa.2013.07.083
   Rajasekar V, 2021, PEER PEER NETW APPL, V14, P747, DOI 10.1007/s12083-020-01046-6
   Rathgeb C, 2016, ADV COMPUT VIS PATT, P359, DOI 10.1007/978-1-4471-6784-6_16
   Rathgeb C, 2011, EURASIP J INF SECUR, DOI 10.1186/1687-417X-2011-3
   Ross ArunA., 2006, HDB MULTIBIOMETRICS, V6
   Sadhya D, 2019, IEEE T INF FOREN SEC, V14, P2972, DOI 10.1109/TIFS.2019.2907014
   Saini R., 2014, International Journal of Advances in Science and Technology, V2, P24
   Saminathan K., 2015, ICTACT Journal on Soft Computing, V5, P889
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Sibai FN, 2011, EXPERT SYST APPL, V38, P5940, DOI 10.1016/j.eswa.2010.11.029
   Srivastava V, 2014, J AMB INTEL HUM COMP, V5, P525, DOI 10.1007/s12652-012-0161-8
   Sudhakar T, 2020, IEEE ACCESS, V8, P112932, DOI 10.1109/ACCESS.2020.3003869
   Sudhakar T, 2019, 2019 INTERNATIONAL CONFERENCE ON CYBERWORLDS (CW), P287, DOI 10.1109/CW.2019.00054
   Venugopalan S, 2011, IEEE T INF FOREN SEC, V6, P385, DOI 10.1109/TIFS.2011.2108288
   Wang K, 2019, PATTERN RECOGN, V86, P85, DOI 10.1016/j.patcog.2018.08.010
   Zareen F. J., 2016, CLOUD BASED MOBILE B
   Zhang GY, 2021, COMPUT MED IMAG GRAP, V90, DOI 10.1016/j.compmedimag.2021.101929
   Zhao ZJ, 2019, PATTERN RECOGN, V93, P546, DOI 10.1016/j.patcog.2019.04.010
   Zheng, 2020, IRIS RECOGNITION
NR 46
TC 7
Z9 7
U1 1
U2 12
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2023
VL 39
IS 4
BP 1571
EP 1581
DI 10.1007/s00371-022-02429-x
EA FEB 2022
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA C3OY5
UT WOS:000759383200004
DA 2024-07-18
ER

PT J
AU Pu, QM
   Xu, CQ
   Wang, H
   Zhao, LN
AF Pu, Qiumei
   Xu, Chiquan
   Wang, Hui
   Zhao, Lina
TI A novel artificial bee colony clustering algorithm with comprehensive
   improvement
SO VISUAL COMPUTER
LA English
DT Article
DE Artificial bee colony algorithm; Clustering problem; Global optimal
   guidance; K-means algorithm; Particle swarm algorithm
ID OPTIMIZATION ALGORITHM; ABC; CLASSIFICATION
AB This paper provides a novel clustering algorithm named CEABC (Comprehensively Enhanced Artificial Bee Colony Algorithm), enhanced by multiple operators including K-means, PSO and directed by Gbest mechanism is proposed, which can be used to solve clustering problem. In the initial stage, the algorithm uses K-means operator to generate the initial nectar source, which not only improves the quality of the initial nectar source, but also avoids the problem of low operating efficiency. In order to enhance the interaction between individuals, the concept of global optimum solution which can also be called Gbest is introduced, where the original one-dimensional information exchange is replaced by the full-dimensional information exchange among nectar sources, so that the information exchange volume of the whole swarm can be improved. In the phase of bee scouting, the above-mentioned global optimal solution is combined with PSO algorithm to generate a brand-new nectar source search method, which will enhance the ability of ABC algorithm to develop nectar sources. Meanwhile, the global optimal solution will continue to guide the bee scouting to generate new nectar sources so as to improve the overall quality of the nectar sources. Then, the experiment carried out on two sets of artificial data sets and four sets of UCI machine learning data sets which are the most representative verifies the clustering performance of the newly proposed CEABC algorithm. In addition, the experimental results are compared with standard ABC algorithm, several other newly proposed ABC-improved algorithms and classical clustering algorithms. Results clearly show that the novel ABC algorithm described in this paper has better accuracy and stability in solving clustering problems, which is a more effective clustering algorithm. We believe that clustering method for high-dimensional data can provide an effective method for data processing in materials, medical treatment, automatic driving and other application fields.
C1 [Pu, Qiumei; Xu, Chiquan; Wang, Hui] Minzu Univ China, Sch Informat Engn, Beijing 100081, Peoples R China.
   [Zhao, Lina] Chinese Acad Sci, Inst High Energy Phys, Key Lab Biomed Effects Nanomat & Nanosafety, Beijing 100049, Peoples R China.
   [Zhao, Lina] Univ Chinese Acad Sci, Beijing 101408, Peoples R China.
C3 Minzu University of China; Chinese Academy of Sciences; Institute of
   High Energy Physics, CAS; Chinese Academy of Sciences; University of
   Chinese Academy of Sciences, CAS
RP Pu, QM (corresponding author), Minzu Univ China, Sch Informat Engn, Beijing 100081, Peoples R China.; Zhao, LN (corresponding author), Chinese Acad Sci, Inst High Energy Phys, Key Lab Biomed Effects Nanomat & Nanosafety, Beijing 100049, Peoples R China.; Zhao, LN (corresponding author), Univ Chinese Acad Sci, Beijing 101408, Peoples R China.
EM puqiumei@muc.edu.cn; linazhao@ihep.ac.cn
OI Pu, Qiumei/0000-0002-2106-237X
FU National Nature Science Foundation of China [31971311]; National Social
   Science Fund of China [20BGL251]
FX This work was supported in part by The National Nature Science
   Foundation of China (No. 31971311),in part by The National Social
   Science Fund of China(NO.20BGL251)
CR Basturk B., 2006, In IEEE Swarm Intelligence Symposium, Indianapolis, IN, USA, P12
   Bohat VK, 2018, KNOWL-BASED SYST, V143, P192, DOI 10.1016/j.knosys.2017.12.017
   Cacciari M, 2008, J HIGH ENERGY PHYS, DOI 10.1088/1126-6708/2008/04/063
   Chen HN, 2009, DISCRETE DYN NAT SOC, V2009, DOI 10.1155/2009/815247
   Chen Q., 2019, IEEE ACCESS, V7
   Chen ZF, 2020, IEEE ACCESS, V8, P108062, DOI 10.1109/ACCESS.2020.2999601
   Cui LZ, 2017, INFORM SCIENCES, V414, P53, DOI 10.1016/j.ins.2017.05.044
   Dai QZ, 2019, INFORM SYST, V84, P1, DOI 10.1016/j.is.2019.04.001
   De Falco I, 2007, APPL SOFT COMPUT, V7, P652, DOI 10.1016/j.asoc.2005.09.004
   Demiroz G, 1997, LECT NOTES ARTIF INT, V1224, P85
   Dorigo M, 2004, ANT COLONY OPTIMIZATION, P1
   Fahy C, 2019, IEEE T CYBERNETICS, V49, P2215, DOI 10.1109/TCYB.2018.2822552
   Gao WF, 2015, IEEE T CYBERNETICS, V45, P2827, DOI 10.1109/TCYB.2014.2387067
   Gao WF, 2013, IEEE T CYBERNETICS, V43, P1011, DOI 10.1109/TSMCB.2012.2222373
   Hassoun MH., 1995, FUNDAMENTALS ARTIFIC
   Holland J.H., 1992, Adaptation in Natural and Artificial Systems, DOI DOI 10.7551/MITPRESS/1090.001.0001
   HOOKE R, 1961, J ACM, V8, P212, DOI 10.1145/321062.321069
   HORNG SC, 2019, IEEE T SYST MAN CY-S
   Huang FL, 2018, IEEE T CYBERNETICS, V48, P199, DOI 10.1109/TCYB.2016.2628722
   Huang KW, 2019, IEEE ACCESS, V7, P80950, DOI 10.1109/ACCESS.2019.2923979
   Hung W.L., 2019, COMMUN STAT-SIMUL C
   Jain A., 1998, ALGRITHMS CLUSTERING
   Jain AK, 1999, ACM COMPUT SURV, V31, P264, DOI 10.1145/331499.331504
   James M., 1967, PROC BERKELEY S MATH, V1, P281, DOI DOI 10.1007/S11665-016-2173-6
   Jensen FV, 1996, INTRO BAYESIAN NETWO
   Ji D, 2016, 2016 12TH INTERNATIONAL CONFERENCE ON NATURAL COMPUTATION, FUZZY SYSTEMS AND KNOWLEDGE DISCOVERY (ICNC-FSKD), P663, DOI 10.1109/FSKD.2016.7603253
   Kamel N., GENETIC EVOLUTIONARY, P45
   Karaboga D., COMPREHENSIVE SURVEY
   Karaboga D., 2005, Technical Report-TR06
   Karaboga D, 2007, LECT NOTES COMPUT SC, V4529, P789, DOI 10.1007/978-3-540-72950-1_77
   Karaboga D, 2007, J GLOBAL OPTIM, V39, P459, DOI 10.1007/s10898-007-9149-x
   Karaboga D, 2011, APPL SOFT COMPUT, V11, P652, DOI 10.1016/j.asoc.2009.12.025
   Kennedy J, 1995, 1995 IEEE INTERNATIONAL CONFERENCE ON NEURAL NETWORKS PROCEEDINGS, VOLS 1-6, P1942, DOI 10.1109/icnn.1995.488968
   Krishnamoorthi M, 2013, INT J COMPUT SCI ENG, V8, P383
   Kumar Y, 2017, NEURAL COMPUT APPL, V28, P537, DOI 10.1007/s00521-015-2095-5
   Leung YW, 2001, IEEE T EVOLUT COMPUT, V5, P41, DOI 10.1109/4235.910464
   Lukasik M., 2016, INFORM SCIENCES
   Lukasik S, 2016, IEEE C EVOL COMPUTAT, P2724, DOI 10.1109/CEC.2016.7744132
   Luo JP, 2017, APPL SOFT COMPUT, V50, P235, DOI 10.1016/j.asoc.2016.11.014
   Ma W, 2016, SOFT COMPUT, V20, P4825, DOI 10.1007/s00500-015-1774-6
   Ozturk C, 2015, INFORM SCIENCES, V297, P154, DOI 10.1016/j.ins.2014.10.060
   Potter MA, 1994, LECT NOTES COMPUT SC, V866, P249
   Prakash J, 2019, PROG ARTIF INTELL, V8, P83, DOI 10.1007/s13748-018-0157-5
   Selvi C, 2019, SOFT COMPUT, V23, P1901, DOI 10.1007/s00500-017-2899-6
   Shah H, 2018, SYMMETRY-BASEL, V10, DOI 10.3390/sym10070292
   Shen X, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-38614-7
   Soruri M, 2018, PATTERN ANAL APPL, V21, P1121, DOI 10.1007/s10044-018-0680-9
   Sriadhi, 2018, 3 ANN APPL SCI ENG C
   Sun H, 2016, INT J COMPUT SCI MAT, V7, P548
   van den Bergh F, 2004, IEEE T EVOLUT COMPUT, V8, P225, DOI [10.1109/TEVC.2004.826069, 10.1109/tevc.2004.826069]
   Van Lierde H, 2020, IEEE T KNOWL DATA EN, V32, P754, DOI 10.1109/TKDE.2019.2892096
   Wang Z, 2018, APPL ENG AGRIC, V34, P277, DOI 10.13031/aea.12205
   Webb GI, 2000, MACH LEARN, V40, P159, DOI 10.1023/A:1007659514849
   Xiao Q, 2018, J ADV TRANSPORT, DOI 10.1155/2018/3853012
   Yan XH, 2012, NEUROCOMPUTING, V97, P241, DOI 10.1016/j.neucom.2012.04.025
   Zhang CS, 2010, EXPERT SYST APPL, V37, P4761, DOI 10.1016/j.eswa.2009.11.003
   Zhang Y, 2019, EXPERT SYST APPL, V137, P46, DOI 10.1016/j.eswa.2019.06.044
   Zhu GP, 2010, APPL MATH COMPUT, V217, P3166, DOI 10.1016/j.amc.2010.08.049
   Zhu Rui, 2011, Journal of Software, V22, P852, DOI 10.3724/SP.J.1001.2011.03804
   Zou WP, 2010, DISCRETE DYN NAT SOC, V2010, DOI 10.1155/2010/459796
NR 60
TC 7
Z9 7
U1 0
U2 26
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2022
VL 38
IS 4
SI SI
BP 1395
EP 1410
DI 10.1007/s00371-021-02367-0
EA FEB 2022
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 0D9DX
UT WOS:000753799900001
DA 2024-07-18
ER

PT J
AU Miao, RH
   Liu, PL
   Wen, F
   Gong, Z
   Xue, WY
   Ying, RD
AF Miao, Ruihang
   Liu, Peilin
   Wen, Fei
   Gong, Zheng
   Xue, Wuyang
   Ying, Rendong
TI R-SDSO: Robust stereo direct sparse odometry
SO VISUAL COMPUTER
LA English
DT Article
DE Visual odometry; Image preprocessing; Localization and mapping; Robust
ID VISUAL ODOMETRY; ACCURATE; VERSATILE
AB This paper presents a robust stereo direct visual odometry method with improved robustness against drastic brightness variation and aggressive rotation. It is achieved by incorporating a direct sparse odometry based on image preprocessing, a depth initialization module, and an abend recovery module into the visual odometry framework. The image preprocessing enhances raw camera images, which facilitates more accurate pixels detecting. Meanwhile, a new error function based on image preprocessing is proposed for making direct visual odometry robust to brightness variation in the environment. In the depth initialization module, the Delaunay triangulation algorithm and feature point matching are combined together for efficient and robust depth estimation. Furthermore, in the abend recovery module, we design a lost/abnormal detection method and a robust state restoration strategy to address the tracking lost/abnormal problem in harsh conditions. Evaluation results on KITTI and EuRoC datasets and a light-switch experiment demonstrate that, with the aid of these three modules, the proposed method can achieve state-of-the-art performance, even compared with visual-inertial fusion methods.
C1 [Miao, Ruihang; Liu, Peilin; Wen, Fei; Gong, Zheng; Xue, Wuyang; Ying, Rendong] Shanghai Jiao Tong Univ, Sch Elect Informat & Elect Engn, 800 Dongchuan Rd, Shanghai 200240, Peoples R China.
C3 Shanghai Jiao Tong University
RP Liu, PL (corresponding author), Shanghai Jiao Tong Univ, Sch Elect Informat & Elect Engn, 800 Dongchuan Rd, Shanghai 200240, Peoples R China.
EM liupeilin@sjtu.edu.cn
FU Innovative Research Group Project of the National Natural Science
   Foundation of China [61871265, 61903246]
FX The research was supported by Innovative Research Group Project of the
   National Natural Science Foundation of China Grant No. 61871265
   andNationalNatural Science Foundation of ChinaGrant No. 61903246.
CR Baker S, 2004, INT J COMPUT VISION, V56, P221, DOI 10.1023/B:VISI.0000011205.11775.fd
   Bergmann P, 2018, IEEE ROBOT AUTOM LET, V3, P627, DOI 10.1109/LRA.2017.2777002
   Bloesch M, 2015, IEEE INT C INT ROBOT, P298, DOI 10.1109/IROS.2015.7353389
   Burri M, 2016, INT J ROBOT RES, V35, P1157, DOI 10.1177/0278364915620033
   Comport AI, 2007, IEEE INT CONF ROBOT, P40, DOI 10.1109/ROBOT.2007.363762
   Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350
   Engel J., 2016, arXiv preprint arXiv:1607.02555
   Engel J, 2018, IEEE T PATTERN ANAL, V40, P611, DOI 10.1109/TPAMI.2017.2658577
   Engel J, 2015, IEEE INT C INT ROBOT, P1935, DOI 10.1109/IROS.2015.7353631
   Engel J, 2014, LECT NOTES COMPUT SC, V8690, P834, DOI 10.1007/978-3-319-10605-2_54
   Engel J, 2013, IEEE I CONF COMP VIS, P1449, DOI 10.1109/ICCV.2013.183
   Forster C, 2014, IEEE INT CONF ROBOT, P15, DOI 10.1109/ICRA.2014.6906584
   Fu YP, 2020, VISUAL COMPUT, V36, P2215, DOI 10.1007/s00371-020-01899-1
   Geiger A., 2012, CVPR
   Geiger A, 2011, LECT NOTES COMPUT SC, V6492, P25, DOI 10.1007/978-3-642-19315-6_3
   Hartley R., 2003, MULTIPLE VIEW GEOMET
   He M, 2020, VISUAL COMPUT, V36, P1053, DOI 10.1007/s00371-019-01714-6
   Hirschmüller H, 2005, PROC CVPR IEEE, P807, DOI 10.1109/cvpr.2005.56
   Jin HL, 2003, VISUAL COMPUT, V19, P377, DOI 10.1007/s00371-003-0202-6
   Krombach N, 2018, ROBOT AUTON SYST, V109, P38, DOI 10.1016/j.robot.2018.08.002
   Leutenegger S, 2015, INT J ROBOT RES, V34, P314, DOI 10.1177/0278364914554813
   Liang HJ, 2019, IEEE T AUTOM SCI ENG, V16, P1619, DOI 10.1109/TASE.2019.2900980
   Liu YL, 2018, VISUAL COMPUT, V34, P899, DOI 10.1007/s00371-018-1523-9
   Mur-Artal R, 2015, IEEE T ROBOT, V31, P1147, DOI 10.1109/TRO.2015.2463671
   Newcombe RA, 2011, IEEE I CONF COMP VIS, P2320, DOI 10.1109/ICCV.2011.6126513
   Ng PC, 2003, NUCLEIC ACIDS RES, V31, P3812, DOI 10.1093/nar/gkg509
   Pillai S, 2016, IEEE INT CONF ROBOT, P3188, DOI 10.1109/ICRA.2016.7487488
   Pire T, 2015, IEEE INT C INT ROBOT, P1373, DOI 10.1109/IROS.2015.7353546
   Qin T, 2018, IEEE T ROBOT, V34, P1004, DOI 10.1109/TRO.2018.2853729
   Rublee E, 2011, IEEE I CONF COMP VIS, P2564, DOI 10.1109/ICCV.2011.6126544
   SHI JB, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P593, DOI 10.1109/CVPR.1994.323794
   Shi JL, 2021, VISUAL COMPUT, V37, P815, DOI 10.1007/s00371-020-01832-6
   Sun K, 2018, IEEE ROBOT AUTOM LET, V3, P965, DOI 10.1109/LRA.2018.2793349
   Wang R, 2017, IEEE I CONF COMP VIS, P3923, DOI 10.1109/ICCV.2017.421
   Wu XL, 2019, IEEE INT CONF ROBOT, P2392, DOI [10.1109/icra.2019.8793607, 10.1109/ICRA.2019.8793607]
   Younes G, 2019, PROCEEDINGS OF THE 14TH INTERNATIONAL JOINT CONFERENCE ON COMPUTER VISION, IMAGING AND COMPUTER GRAPHICS THEORY AND APPLICATIONS (VISAPP), VOL 5, P737, DOI 10.5220/0007524807370747
   Zhou YJ, 2021, VISUAL COMPUT, V37, P2009, DOI 10.1007/s00371-020-01959-6
NR 37
TC 2
Z9 2
U1 1
U2 14
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2022
VL 38
IS 6
BP 2207
EP 2221
DI 10.1007/s00371-021-02278-0
EA JAN 2022
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 1C6YN
UT WOS:000743467600001
DA 2024-07-18
ER

PT J
AU Zhang, TS
   Li, Z
   Sun, ZK
   Zhu, L
AF Zhang, Taoshan
   Li, Zheng
   Sun, Zhikuan
   Zhu, Lin
TI A fully convolutional anchor-free object detector
SO VISUAL COMPUTER
LA English
DT Article
DE Deep learning; Convolutional network; Anchor free; Object detection
AB We propose an improvement on fully convolutional one-stage object detection to improve the overall performance of the model. Firstly, we propose two feature pyramid networks with dense architectures to enhance the semantic features of high levels of feature maps in feature pyramid networks. In addition, we modify the detection head, making it more intuitively to evaluate the prediction results. Finally, the qualities of positive samples in original model are limited by the generated error when the location maps back onto the original image. In this paper, we present an adaptive way to produce positive samples to alleviate this problem by appointing the size of center area adaptively. Besides, some other approaches are adopted to further improve the detector including new loss functions, deformable convolutional modules. We validate our approach on MS COCO validation datasets, and gain a 6.2% AP improvement compared to the original. Based on the backbone of ResNet-50, the detector can achieve an AP of 42.4%.
C1 [Zhang, Taoshan; Li, Zheng; Sun, Zhikuan; Zhu, Lin] Sichuan Univ, Coll Comp Sci, Chengdu, Peoples R China.
C3 Sichuan University
RP Li, Z (corresponding author), Sichuan Univ, Coll Comp Sci, Chengdu, Peoples R China.
EM lizheng@scu.edu.cn
FU National Natural Science Foundation of China [61471250]; National Key
   Project [GJXM92579]
FX This work was supported by the National Natural Science Foundation of
   China (Grant No. 61471250) and the National Key Project (No.GJXM92579).
CR Cai JH, 2020, VISUAL COMPUT, V36, P1261, DOI 10.1007/s00371-019-01733-3
   Chen K., 2019, arXiv preprint arXiv:1906.07155
   Chen WJ, 2021, VISUAL COMPUT, V37, P805, DOI 10.1007/s00371-020-01831-7
   Chu YJ, 2019, VISUAL COMPUT, V35, P239, DOI 10.1007/s00371-017-1468-4
   Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Du Q, 2021, VISUAL COMPUT, V37, P663, DOI 10.1007/s00371-020-01802-y
   Duan KW, 2019, IEEE I CONF COMP VIS, P6568, DOI 10.1109/ICCV.2019.00667
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Ghiasi G, 2019, PROC CVPR IEEE, P7029, DOI 10.1109/CVPR.2019.00720
   Girshick R., 2014, PROC CVPR IEEE, DOI [DOI 10.1109/CVPR.2014.81, 10.1109/CVPR.2014.81]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   HUANG G, 2017, PROC CVPR IEEE, P2261, DOI DOI 10.1109/CVPR.2017.243
   Huang K, 2020, VISUAL COMPUT, V36, P1355, DOI 10.1007/s00371-019-01734-2
   Huang L., 2015, Comput. Sci.
   Jiale Cao, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11482, DOI 10.1109/CVPR42600.2020.01150
   Junos MH, 2022, VISUAL COMPUT, V38, P2341, DOI 10.1007/s00371-021-02116-3
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Law H, 2018, LECT NOTES COMPUT SC, V11218, P765, DOI 10.1007/978-3-030-01264-9_45
   Lim J, 2019, VISUAL COMPUT, V35, P71, DOI 10.1007/s00371-017-1453-y
   Lin T.Y., 2017, P IEEE C COMPUTER VI, P2117, DOI [10.1109/CVPR.2017.106, DOI 10.1109/CVPR.2017.106]
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu S., 2019, CVPR
   Liu S, 2018, PROC CVPR IEEE, P8759, DOI 10.1109/CVPR.2018.00913
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Newell A, 2016, PROCEEDINGS OF THE ELEVENTH EUROPEAN CONFERENCE ON COMPUTER SYSTEMS, (EUROSYS 2016), DOI 10.1145/2901318.2901343
   Paszke A, 2019, ADV NEUR IN, V32
   QualcommKorea Y., PROBABILISTIC ANCHOR
   Redmon J., 2016, PROC CVPR IEEE, DOI [10.1109/CVPR.2016.91, DOI 10.1109/CVPR.2016.91]
   Redmon J, 2018, Arxiv, DOI [arXiv:1804.02767, DOI 10.1109/CVPR.2017.690, DOI 10.48550/ARXIV.1804.02767]
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rezatofighi H, 2019, PROC CVPR IEEE, P658, DOI 10.1109/CVPR.2019.00075
   Shrivastava A., 2016, ARXIV PREPRINT ARXIV
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278
   Tan MX, 2019, PR MACH LEARN RES, V97
   Tian Z, 2019, IEEE I CONF COMP VIS, P9626, DOI 10.1109/ICCV.2019.00972
   Tian ZT, 2022, IEEE T PATTERN ANAL, V44, P1050, DOI 10.1109/TPAMI.2020.3013717
   Tsung-Yi Lin, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P2999, DOI 10.1109/ICCV.2017.324
   Uijlings JRR, 2013, INT J COMPUT VISION, V104, P154, DOI 10.1007/s11263-013-0620-5
   Wang Jingdong, 2020, IEEE transactions on pattern analysis and machine intelligence, P1, DOI [DOI 10.1109/TPAMI.2020.2983686, 10.1109/tpami.2020.2983686, 10.1109/TPAMI.2020.2983686]
   Wang T., 2020, ARXIV PREPRINT ARXIV
   Wang Y, 2021, IEEE T IMAGE PROCESS, V30, P2876, DOI 10.1109/TIP.2021.3055632
   Wei LX, 2021, VISUAL COMPUT, V37, P133, DOI 10.1007/s00371-019-01787-3
   Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634
   Yu J., 2016, P 24 ACM INT C MULT, P516, DOI DOI 10.1145/2964284.2967274
   Zhang Q, 2018, VISUAL COMPUT, V34, P473, DOI 10.1007/s00371-017-1354-0
   Zheng ZH, 2020, AAAI CONF ARTIF INTE, V34, P12993
   Zhou X., 2019, arXiv
   Zhou XY, 2019, PROC CVPR IEEE, P850, DOI 10.1109/CVPR.2019.00094
   Zhu CC, 2019, PROC CVPR IEEE, P840, DOI 10.1109/CVPR.2019.00093
   Zhu XZ, 2019, PROC CVPR IEEE, P9300, DOI 10.1109/CVPR.2019.00953
NR 53
TC 4
Z9 4
U1 1
U2 18
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2023
VL 39
IS 2
BP 569
EP 580
DI 10.1007/s00371-021-02357-2
EA JAN 2022
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 8T2TV
UT WOS:000741634600009
DA 2024-07-18
ER

PT J
AU Li, C
   Fussell, L
   Komura, T
AF Li, Cheng
   Fussell, Levi
   Komura, Taku
TI Multi-agent reinforcement learning for character control
SO VISUAL COMPUTER
LA English
DT Article
DE Multi-agent reinforcement learning; Character control; Computer
   animation
AB Simultaneous control of multiple characters has been a research topic that has been extensively pursued for applications in computer games and computer animations, for applications such as crowd simulation, controlling two characters carrying objects or fighting with one another and controlling a team of characters playing collective sports. With the advance in deep learning and reinforcement learning, there is a growing interest in applying multi-agent reinforcement learning for intelligently controlling the characters to produce realistic movements. In this paper we will survey the state-of-the-art MARL techniques that are applicable for character control. We will then survey papers that make use of MARL for multi-character control and then discuss about the possible future directions of research.
C1 [Li, Cheng; Fussell, Levi; Komura, Taku] Univ Edinburgh, Sch Informat, 10 Crichton St, Edinburgh, Midlothian, Scotland.
   [Komura, Taku] Univ Hong Kong, Pokfulam, Hong Kong, Peoples R China.
C3 University of Edinburgh; University of Hong Kong
RP Komura, T (corresponding author), Univ Edinburgh, Sch Informat, 10 Crichton St, Edinburgh, Midlothian, Scotland.; Komura, T (corresponding author), Univ Hong Kong, Pokfulam, Hong Kong, Peoples R China.
EM Cheng.Li@ed.ac.uk; levi.fussell@ed.ac.uk; T.Komura@ed.ac.uk
FU Microsoft Studios; EPSRC DTA program; University of Hong Kong
   [182DRTAKU, 187FRTAKU, 230DRTAKU]; Microsoft
FX Cheng Li is supported by Microsoft Studios. Levi Fussell is supported by
   EPSRC DTA program. Taku Komura is partly supported by a start-up fund by
   The University of Hong Kong (182DRTAKU, 187FRTAKU, 230DRTAKU). Cheng Li
   and Taku Komura receive research grant from Microsoft.
CR Baker Bowen, 2020, Emergent tool use from multi-agent autocurricula
   Bansal T., 2018, 6 INT C LEARN REPR I
   Bin Peng X, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925881
   Brian, VIDEO GAMES NEW PROM
   Fan TX, 2020, INT J ROBOT RES, V39, P856, DOI 10.1177/0278364920916531
   Foerster JN, 2018, AAAI CONF ARTIF INTE, P2974
   Haworth Brandon, 2020, MIG '20: Motion, Interaction and Games, DOI 10.1145/3424636.3426894
   Heess N., 2017, CoRR, abs/1707.02286.
   Helbing D, 2000, NATURE, V407, P487, DOI 10.1038/35035023
   Hüttenrauch M, 2019, J MACH LEARN RES, V20
   Kaelbling LP, 1996, J ARTIF INTELL RES, V4, P237, DOI 10.1613/jair.301
   Kim M, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531385
   Kiran BR, 2022, IEEE T INTELL TRANSP, V23, P4909, DOI 10.1109/TITS.2021.3054625
   Lee D., 2018, P AAAI C ART INT INT, P187
   Lee Y, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866160
   Levine S, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185524
   Lillicrap, 2015, ARXIV150902971, P1
   Liu S., 2021, ARXIV210512196 CORR
   Liu Sijia, 2019, INT C LEARN REPR
   Lowe R, 2017, ADV NEUR IN, V30
   Min JY, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366172
   Mnih V., 2013, PLAYING ATARI DEEP R, DOI DOI 10.1038/NATURE14236
   Mnih V, 2016, PR MACH LEARN RES, V48
   Peng XB, 2021, ACM T GRAPHIC, V40, DOI [10.1145/3197517.3201311, 10.1145/3450626.3459670]
   Peng XB, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275014
   Peng XB, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073602
   Peng XB, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766910
   Pettré J, 2006, COMPUT ANIMAT VIRT W, V17, P445, DOI 10.1002/cav.147
   Rashid T, 2018, PR MACH LEARN RES, V80
   Reynolds CW., 1987, SIGGRAPH Comput. Graph., V21, P25, DOI [10.1145/37402.37406, DOI 10.1145/37402.37406]
   Schulman J., 2017, ARXIV
   Shi XM, 2019, PHYSICA A, V522, P350, DOI 10.1016/j.physa.2019.01.086
   Shum HPH, 2012, IEEE T VIS COMPUT GR, V18, P741, DOI 10.1109/TVCG.2010.257
   Shum HPH, 2008, I3D 2008: SYMPOSIUM ON INTERACTIVE 3D GRAPHICS AND GAMES, PROCEEDINGS, P131
   Shum HPH, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409067
   Silver D, 2018, SCIENCE, V362, P1140, DOI 10.1126/science.aar6404
   Sun P., 2018, ARXIV180907193
   Sunehag P, 2018, PROCEEDINGS OF THE 17TH INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS AND MULTIAGENT SYSTEMS (AAMAS' 18), P2085
   van den Berg Jur, 2011, IEEE International Conference on Robotics and Automation, P3475
   Vinyals O, 2019, NATURE, V575, P350, DOI 10.1038/s41586-019-1724-z
   Wampler K, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1805964.1805970
   WATKINS CJCH, 1992, MACH LEARN, V8, P279, DOI 10.1007/BF00992698
   Wolpert D.H., 2002, Modeling Complexity in Economic and Social Systems, P355
   Won J, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459761
   Won J, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2661229.2661271
   Yu Fan Chen, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P285, DOI 10.1109/ICRA.2017.7989037
   WIRED
NR 47
TC 2
Z9 2
U1 0
U2 9
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2021
VL 37
IS 12
SI SI
BP 3115
EP 3123
DI 10.1007/s00371-021-02269-1
EA DEC 2021
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP1DP
UT WOS:000726257600001
OA hybrid
DA 2024-07-18
ER

PT J
AU Zhao, XM
   Guo, CC
   Zou, Q
AF Zhao, Xiaoming
   Guo, Chenchen
   Zou, Qiang
TI Human pose estimation with gated multi-scale feature fusion and spatial
   mutual information
SO VISUAL COMPUTER
LA English
DT Article
DE Human pose estimation; Gate; Multi-scale feature fusion; Noisy
   information; Spatial mutual information
AB Although human pose estimation has achieved great success, the ambiguity of joint prediction has not been well resolved, especially in complex situations (crowded scenes, occlusions, and unnormal poses). We think that is caused by the noisy information introduced by combining multi-level features by simply adding features at each position. To alleviate this problem, we propose a new structure of gated multi-scale feature fusion (GMSFF). This module aims to selectively import high-level features to make up for the missing semantic information of low-resolution feature maps. Inspired by the prior knowledge that the position information of joints can refer to each other, we propose a new fine-tuning strategy for pose estimation-spatial mutual information complementary module (SMICM). It can assist the model in better adjusting the current joint's position by capturing the information contained in other joints and only adds a little computational cost. We evaluated our proposed method on four datasets: MPII Human Pose Dataset (MPII), COCO keypoint detection Dataset (COCO), Occluded Human Dataset (OCHuman), and CrowdPose Dataset. The experimental results show that with the deepening of the occlusion and crowding level of the datasets, the improvement becomes more and more obvious. In particular, a performance improvement of 2.2 AP was obtained on the OCHuman dataset. In addition, our modules are plug-and-play.
C1 [Zhao, Xiaoming; Guo, Chenchen; Zou, Qiang] Tianjin Univ, Sch Microelect, Tianjin, Peoples R China.
   [Zhao, Xiaoming; Guo, Chenchen; Zou, Qiang] Tianjin Int Joint Res Ctr Internet Things, Tianjin, Peoples R China.
   [Zhao, Xiaoming; Guo, Chenchen; Zou, Qiang] Tianjin Univ, Tianjin Key Lab Imaging & Sensing Microelect Tech, Tianjin, Peoples R China.
C3 Tianjin University; Tianjin University
RP Zou, Q (corresponding author), Tianjin Univ, Sch Microelect, Tianjin, Peoples R China.; Zou, Q (corresponding author), Tianjin Int Joint Res Ctr Internet Things, Tianjin, Peoples R China.; Zou, Q (corresponding author), Tianjin Univ, Tianjin Key Lab Imaging & Sensing Microelect Tech, Tianjin, Peoples R China.
EM zxm154977@tju.edu.cn; zouqiang@tju.edu.cn
RI zhao, xiaoming/IVU-6747-2023
OI zhao, xiaoming/0000-0001-7992-1045
CR Agahian S, 2019, VISUAL COMPUT, V35, P591, DOI 10.1007/s00371-018-1489-7
   Andriluka M, 2014, PROC CVPR IEEE, P3686, DOI 10.1109/CVPR.2014.471
   [Anonymous], PROC CVPR IEEE, DOI [DOI 10.1109/CVPR.2018.00745, DOI 10.1109/TPAMI.2019.2913372]
   Chen YL, 2018, PROC CVPR IEEE, P7103, DOI 10.1109/CVPR.2018.00742
   Chen Y, 2017, IEEE I CONF COMP VIS, P1221, DOI 10.1109/ICCV.2017.137
   Chu X, 2017, PROC CVPR IEEE, P5669, DOI 10.1109/CVPR.2017.601
   Chu X, 2016, PROC CVPR IEEE, P4715, DOI 10.1109/CVPR.2016.510
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Islam MA, 2017, PROC CVPR IEEE, P4877, DOI 10.1109/CVPR.2017.518
   Ke LP, 2018, LECT NOTES COMPUT SC, V11206, P731, DOI 10.1007/978-3-030-01216-8_44
   Kingma D. P., 2014, arXiv
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Li JF, 2019, PROC CVPR IEEE, P10855, DOI 10.1109/CVPR.2019.01112
   Li W., 2019, arXiv
   Li XT, 2020, AAAI CONF ARTIF INTE, V34, P11418
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu XL, 2022, VISUAL COMPUT, V38, P2603, DOI 10.1007/s00371-021-02135-0
   Lu Zhou, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12365), P396, DOI 10.1007/978-3-030-58565-5_24
   Newell A, 2016, LECT NOTES COMPUT SC, V9912, P483, DOI 10.1007/978-3-319-46484-8_29
   Papandreou G, 2017, PROC CVPR IEEE, P3711, DOI 10.1109/CVPR.2017.395
   Qiu L., 2020, EUR C COMP VIS, P488, DOI DOI 10.1007/978-3-030-58529-7_29
   Ren S, 2015, FASTER R CNN REAL TI, P91
   Singh VK, 2011, VISUAL COMPUT, V27, P1115, DOI 10.1007/s00371-011-0656-x
   Song-Hai Zhang, 2019, 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P889, DOI 10.1109/CVPR.2019.00098
   Su K, 2019, PROC CVPR IEEE, P5667, DOI 10.1109/CVPR.2019.00582
   Sun K, 2019, PROC CVPR IEEE, P5686, DOI 10.1109/CVPR.2019.00584
   Tang W, 2018, LECT NOTES COMPUT SC, V11207, P197, DOI 10.1007/978-3-030-01219-9_12
   Tang W, 2019, PROC CVPR IEEE, P1107, DOI 10.1109/CVPR.2019.00120
   Tompson J, 2015, PROC CVPR IEEE, P648, DOI 10.1109/CVPR.2015.7298664
   Toshev A, 2014, PROC CVPR IEEE, P1653, DOI 10.1109/CVPR.2014.214
   Vaswani A, 2017, ADV NEUR IN, V30
   Verma P, 2022, VISUAL COMPUT, V38, P2417, DOI 10.1007/s00371-021-02120-7
   Vidanpathirana M, 2020, VISUAL COMPUT, V36, P1501, DOI 10.1007/s00371-019-01757-9
   Wei SE, 2016, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2016.511
   Wu JZ, 2020, VISUAL COMPUT, V36, P1401, DOI 10.1007/s00371-019-01740-4
   Xia FT, 2017, PROC CVPR IEEE, P6080, DOI 10.1109/CVPR.2017.644
   Xiao B, 2018, LECT NOTES COMPUT SC, V11210, P472, DOI 10.1007/978-3-030-01231-1_29
   Yang QN, 2022, VISUAL COMPUT, V38, P2447, DOI 10.1007/s00371-021-02122-5
   Yang W, 2017, IEEE I CONF COMP VIS, P1290, DOI 10.1109/ICCV.2017.144
   Zhang F, 2019, PROC CVPR IEEE, P3512, DOI 10.1109/CVPR.2019.00363
   Zhang H., 2019, P CVPR
   Zhang L, 2018, PROC CVPR IEEE, P1741, DOI 10.1109/CVPR.2018.00187
   Zhao HY, 2017, PROC CVPR IEEE, P907, DOI 10.1109/CVPR.2017.103
   Zhao L, 2021, IEEE T IMAGE PROCESS, V30, P6785, DOI 10.1109/TIP.2021.3097836
   Zhao L, 2021, IEEE T CIRC SYST VID, V31, P1555, DOI 10.1109/TCSVT.2020.3005522
NR 46
TC 0
Z9 0
U1 3
U2 12
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2023
VL 39
IS 1
BP 119
EP 137
DI 10.1007/s00371-021-02317-w
EA NOV 2021
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA F6UJ1
UT WOS:000715675100001
DA 2024-07-18
ER

PT J
AU Yoshizawa, S
   Yokota, H
AF Yoshizawa, Shin
   Yokota, Hideo
TI Fast and faithful scale-aware image filters
SO VISUAL COMPUTER
LA English
DT Article
DE Domain-splitting algorithm; Rolling guidance filter; Domain transform;
   L-1 Gaussian function
ID GAUSSIAN CONVOLUTION; PHOTOGRAPHY; FOUNDATIONS; SIGNAL; FLASH
AB This paper proposes a fast and accurate computational framework for scale-aware image filters. Our framework is based on accurately approximating L-1 Gaussian convolution with respect to a transformed pixel domain representing geodesic distance on a guidance image manifold in order to recover salient edges in a manner faithful to scale-space theory while removing small image structures. Our framework possesses linear computational complexity with high approximation precision. We examined it numerically in terms of speed, accuracy, and quality compared with conventional methods.
C1 [Yoshizawa, Shin; Yokota, Hideo] RIKEN, Image Proc Res Team, 2-1 Hirosawa, Wako, Saitama 3510198, Japan.
C3 RIKEN
RP Yoshizawa, S (corresponding author), RIKEN, Image Proc Res Team, 2-1 Hirosawa, Wako, Saitama 3510198, Japan.
EM shin@riken.jp
OI Yoshizawa, Shin/0000-0001-8748-6012
CR ALVAREZ L, 1994, SIAM J NUMER ANAL, V31, P590, DOI 10.1137/0731032
   [Anonymous], 1959, BASIC THEORY PATTERN
   [Anonymous], 1993, RR1893 INRIA
   ASTOLA J, 1990, P IEEE, V78, P678, DOI 10.1109/5.54807
   Bashkirova D., 2017, P ISP RAS, V29, P55, DOI [10.15514/ISPRAS-2017-29(4)-4, DOI 10.15514/ISPRAS-2017-29(4)-4]
   Bhatia A, 2010, IEEE INT CONF ROBOT, P1530, DOI 10.1109/ROBOT.2010.5509400
   Cuomo S, 2015, ACSIS-ANN COMPUT SCI, V5, P641, DOI 10.15439/2015F286
   DUHAMEL P, 1990, SIGNAL PROCESS, V19, P259, DOI 10.1016/0165-1684(90)90158-U
   Eisemann E, 2004, ACM T GRAPHIC, V23, P673, DOI 10.1145/1015706.1015778
   Elboher E, 2012, INT CONF INTELL SYST, P897, DOI 10.1109/ISDA.2012.6416657
   Elder JH, 1999, INT J COMPUT VISION, V34, P97, DOI 10.1023/A:1008183703117
   Gastal ESL, 2015, COMPUT GRAPH FORUM, V34, P81, DOI 10.1111/cgf.12543
   Gastal ESL, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964964
   Getreuer P, 2013, IMAGE PROCESS ON LIN, V3, P286, DOI 10.5201/ipol.2013.87
   GREENGARD L, 1991, SIAM J SCI STAT COMP, V12, P79, DOI 10.1137/0912004
   Gwosdek P, 2012, LECT NOTES COMPUT SC, V6667, P447, DOI 10.1007/978-3-642-24785-9_38
   Ham B, 2018, IEEE T PATTERN ANAL, V40, P192, DOI 10.1109/TPAMI.2017.2669034
   He KM, 2013, IEEE T PATTERN ANAL, V35, P1397, DOI 10.1109/TPAMI.2012.213
   Kniefacz P., 2015, ARXIVCORR150506702
   KOENDERINK JJ, 1984, BIOL CYBERN, V50, P363, DOI 10.1007/BF00336961
   Lindeberg T, 1997, COMP IMAG VIS, V8, P75
   Mitsunari S., 2012, Fast approximate function of exponential function exp and log
   OSHER S, 1988, J COMPUT PHYS, V79, P12, DOI 10.1016/0021-9991(88)90002-2
   Paris S, 2009, INT J COMPUT VISION, V81, P24, DOI 10.1007/s11263-007-0110-8
   Petschnigg G, 2004, ACM T GRAPHIC, V23, P664, DOI 10.1145/1015706.1015777
   Sochen N, 1998, IEEE T IMAGE PROCESS, V7, P310, DOI 10.1109/83.661181
   Song B.L., 2010, CAPPADOCIA BALLOON I
   STRUIK D. J., 1988, Lectures on Classical Differential Geometry
   Toet A, 2016, PEERJ COMPUT SCI, DOI 10.7717/peerj-cs.72
   Triggs B, 2006, IEEE T SIGNAL PROCES, V54, P2365, DOI 10.1109/TSP.2006.871980
   van Vliet LJ, 1998, INT C PATT RECOG, P509, DOI 10.1109/ICPR.1998.711192
   Wang PS, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818068
   Weickert J, 1999, J MATH IMAGING VIS, V10, P237, DOI 10.1023/A:1008344623873
   Yoshizawa S, 2014, IEEE IMAGE PROC, P2908, DOI 10.1109/ICIP.2014.7025588
   Zhang JY, 2019, IEEE T VIS COMPUT GR, V25, P1774, DOI 10.1109/TVCG.2018.2816926
   Zhang Q, 2014, LECT NOTES COMPUT SC, V8691, P815, DOI 10.1007/978-3-319-10578-9_53
NR 36
TC 1
Z9 1
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2021
VL 37
IS 12
SI SI
BP 3051
EP 3062
DI 10.1007/s00371-021-02249-5
EA AUG 2021
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP1DP
UT WOS:000681531100001
OA hybrid
DA 2024-07-18
ER

PT J
AU Agudo, A
   Lepetit, V
   Moreno-Noguer, F
AF Agudo, Antonio
   Lepetit, Vincent
   Moreno-Noguer, Francesc
TI Simultaneous completion and spatiotemporal grouping of corrupted motion
   tracks
SO VISUAL COMPUTER
LA English
DT Article
DE Point track completion; Spatiotemporal clustering; Augmented Lagrangian
   multiplier
ID NONRIGID SHAPE; SEGMENTATION; ALGORITHM; MODEL
AB Given an unordered list of 2D or 3D point trajectories corrupted by noise and partial observations, in this paper we introduce a framework to simultaneously recover the incomplete motion tracks and group the points into spatially and temporally coherent clusters. This advances existing work, which only addresses partial problems and without considering a unified and unsupervised solution. We cast this problem as a matrix completion one, in which point tracks are arranged into a matrix with the missing entries set as zeros. In order to perform the double clustering, the measurement matrix is assumed to be drawn from a dual union of spatiotemporal subspaces. The bases and the dimensionality for these subspaces, the affinity matrices used to encode the temporal and spatial clusters to which each point belongs, and the non-visible tracks, are then jointly estimated via augmented Lagrange multipliers in polynomial time. A thorough evaluation on incomplete motion tracks for multiple-object typologies shows that the accuracy of the matrix we recover compares favorably to that obtained with existing low-rank matrix completion methods, specially under noisy measurements. In addition, besides recovering the incomplete tracks, the point trajectories are directly grouped into different object instances, and a number of semantically meaningful temporal primitive actions are automatically discovered.
C1 [Agudo, Antonio; Moreno-Noguer, Francesc] CSIC UPC, Inst Robot & Informat Ind, Barcelona 08028, Spain.
   [Lepetit, Vincent] Univ Bordeaux, F-33405 Talence, France.
C3 Universitat Politecnica de Catalunya; Consejo Superior de
   Investigaciones Cientificas (CSIC); CSIC - Institut de Robotica i
   Informatica Industrial (IRII); Universite de Bordeaux
RP Agudo, A (corresponding author), CSIC UPC, Inst Robot & Informat Ind, Barcelona 08028, Spain.
EM aagudo@iri.upc.edu; vincent.lepetit@u-bordeaux.fr; fmoreno@iri.upc.edu
RI Agudo, Antonio/C-5147-2017
OI Agudo, Antonio/0000-0001-6845-4998
FU Spanish State Research Agency through the Maria de Maeztu Seal of
   Excellence [IRI MDM-2016-0656]; Spanish Ministry of Science and
   Innovation [HuMoUR TIN2017-90086-R]; Salvador de Madariaga
   [PRX19/00626]; ERA-net CHIST-ERA project [IPALM PCI2019-103386]
FX This work has been partially supported by the Spanish State Research
   Agency through the Maria de Maeztu Seal of Excellence to IRI
   MDM-2016-0656, by the Spanish Ministry of Science and Innovation under
   project HuMoUR TIN2017-90086-R and the Salvador de Madariaga grant
   PRX19/00626, and by the ERA-net CHIST-ERA project IPALM PCI2019-103386.
CR Agudo A, 2018, IEEE T PATTERN ANAL, V40, P2137, DOI 10.1109/TPAMI.2017.2752710
   Agudo A, 2017, PROC CVPR IEEE, P1513, DOI 10.1109/CVPR.2017.165
   Agudo A, 2017, INT J COMPUT VISION, V122, P371, DOI 10.1007/s11263-016-0972-8
   Agudo A, 2015, IEEE I CONF COMP VIS, P756, DOI 10.1109/ICCV.2015.93
   Akhter I, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2159516.2159523
   [Anonymous], 2016, Advances in Neural Information Processing Systems
   [Anonymous], 2012, INT C ART INT STAT
   [Anonymous], 2016, ICML
   Balzano L, 2012, 2012 IEEE STATISTICAL SIGNAL PROCESSING WORKSHOP (SSP), P612, DOI 10.1109/SSP.2012.6319774
   Bartholomew, 1999, LATENT VARIABLES MOD
   Boyd S., 2011, FOUND TRENDS MACH LE, V3, P1, DOI DOI 10.1561/2200000016
   Cabral R, 2013, IEEE I CONF COMP VIS, P2488, DOI 10.1109/ICCV.2013.309
   Cai JF, 2010, SIAM J OPTIMIZ, V20, P1956, DOI 10.1137/080738970
   Candès EJ, 2010, P IEEE, V98, P925, DOI 10.1109/JPROC.2009.2035722
   Candès EJ, 2009, FOUND COMPUT MATH, V9, P717, DOI 10.1007/s10208-009-9045-5
   Chen WY, 2011, IEEE T PATTERN ANAL, V33, P568, DOI 10.1109/TPAMI.2010.88
   Chen Yudong, 2011, Proceedings of the 28th International Conference on Machine Learning, P873
   Chiang KY, 2015, ADV NEUR IN, V28
   Dai YC, 2012, PROC CVPR IEEE, P2018, DOI 10.1109/CVPR.2012.6247905
   Elhamifar E, 2013, IEEE T PATTERN ANAL, V35, P2765, DOI 10.1109/TPAMI.2013.57
   Fan JC, 2017, NEURAL NETWORKS, V93, P36, DOI 10.1016/j.neunet.2017.04.005
   Ganti R, 2015, ADV NEUR IN, V28
   Ghahramani Z., 1996, Tech. Rep.
   Golub G.H., 2013, Matrix Computations, DOI DOI 10.56021/9781421407944
   Gotardo PFU, 2011, IEEE T PATTERN ANAL, V33, P2051, DOI 10.1109/TPAMI.2011.50
   Hare S, 2011, IEEE I CONF COMP VIS, P263, DOI 10.1109/ICCV.2011.6126251
   Ionescu C, 2014, IEEE T PATTERN ANAL, V36, P1325, DOI 10.1109/TPAMI.2013.248
   Jain P., 2013, INT C MACH LEARN, P1881
   Jia X, 2012, PROC CVPR IEEE, P1822, DOI 10.1109/CVPR.2012.6247880
   Joo H, 2018, PROC CVPR IEEE, P8320, DOI 10.1109/CVPR.2018.00868
   Kanatani K, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P586, DOI 10.1109/ICCV.2001.937679
   Lin Z., 2010, Math. Program
   Liu G., 2010, P INT C MACH LEARN, P663
   Liu GC, 2013, IEEE T PATTERN ANAL, V35, P171, DOI 10.1109/TPAMI.2012.88
   Liu GC, 2011, IEEE I CONF COMP VIS, P1615, DOI 10.1109/ICCV.2011.6126422
   Ma Y, 2008, SIAM REV, V50, P413, DOI 10.1137/060655523
   Ma Y, 2007, IEEE T PATTERN ANAL, V29, P1546, DOI 10.1109/TP'AMI.2007.1085
   Nguyen DM, 2020, IEEE T NEUR NET LEAR, V31, P3579, DOI 10.1109/TNNLS.2019.2945111
   Perazzi F, 2016, PROC CVPR IEEE, P724, DOI 10.1109/CVPR.2016.85
   Rao S, 2010, IEEE T PATTERN ANAL, V32, P1832, DOI 10.1109/TPAMI.2009.191
   Rao SR, 2010, INT J COMPUT VISION, V88, P425, DOI 10.1007/s11263-009-0314-1
   Recht B, 2010, SIAM REV, V52, P471, DOI 10.1137/070697835
   Sui Y, 2016, LECT NOTES COMPUT SC, V9912, P194, DOI 10.1007/978-3-319-46484-8_12
   Sui Y, 2015, PATTERN RECOGN, V48, P2872, DOI 10.1016/j.patcog.2015.03.007
   Tipping ME, 1999, NEURAL COMPUT, V11, P443, DOI 10.1162/089976699300016728
   Tipping ME, 1999, J R STAT SOC B, V61, P611, DOI 10.1111/1467-9868.00196
   van der Aa N. P., 2011, 2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops), P1264, DOI 10.1109/ICCVW.2011.6130396
   Wang D, 2013, PROC CVPR IEEE, P2371, DOI 10.1109/CVPR.2013.307
   Xiao J, 2006, INT J COMPUT VISION, V67, P233, DOI 10.1007/s11263-005-3962-9
   Yan JY, 2006, LECT NOTES COMPUT SC, V3954, P94
   Yang CY, 2015, PR MACH LEARN RES, V37, P2463
   Yang JW, 2016, PROC CVPR IEEE, P5147, DOI 10.1109/CVPR.2016.556
   Yang JF, 2009, SIAM J IMAGING SCI, V2, P569, DOI 10.1137/080730421
   Yao QM, 2019, IEEE T PATTERN ANAL, V41, P2628, DOI 10.1109/TPAMI.2018.2858249
   Zhang TZ, 2012, LECT NOTES COMPUT SC, V7577, P470, DOI 10.1007/978-3-642-33783-3_34
   Zhu YY, 2014, PROC CVPR IEEE, P1542, DOI 10.1109/CVPR.2014.200
NR 56
TC 1
Z9 1
U1 0
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2022
VL 38
IS 11
BP 3937
EP 3952
DI 10.1007/s00371-021-02238-8
EA JUL 2021
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 5X0BS
UT WOS:000674546600002
OA hybrid, Green Published
DA 2024-07-18
ER

PT J
AU Saura-Herreros, M
   Lopez, A
   Ribelles, J
AF Saura-Herreros, Miguel
   Lopez, Angeles
   Ribelles, Jose
TI Spherical panorama compositing through depth estimation
SO VISUAL COMPUTER
LA English
DT Article
DE Spherical panorama; Depth estimation; Virtual reality; Image compositing
AB In this paper, we propose to work in the 2.5D space of the scene to facilitate composition of new spherical panoramas. For adding depths to spherical panoramas, we extend an existing method that was designed to estimate relative depths from a single perspective image through user interaction. We analyze the difficulties to interactively provide such depth information for spherical panoramas, through three different types of presentation. Then, we propose a set of basic tools to interactively manage the relative depths of the panoramas in order to obtain a composition in a very simple way. We conclude that the relative depths obtained by the extended depth estimation method are enough for the purpose of compositing new photorealistic panoramas through a few elementary editing tools.
C1 [Saura-Herreros, Miguel] Univ Jaume 1, Dept Ingn & Ciencia Comp, Castellon de La Plana, Spain.
   [Lopez, Angeles; Ribelles, Jose] Univ Jaume 1, Inst New Imaging Technol, Castellon de La Plana, Spain.
C3 Universitat Jaume I; Universitat Jaume I
RP Ribelles, J (corresponding author), Univ Jaume 1, Inst New Imaging Technol, Castellon de La Plana, Spain.
EM ribelles@uji.es
RI Ribelles, Jose/B-1208-2017; López, Angeles/B-1220-2017
OI Ribelles, Jose/0000-0001-8393-9758; López, Angeles/0000-0002-6433-0565
FU Department of Computer Science and Engineering of Universitat Jaume I;
   Institute of New Imaging Technologies
FX The authors would like to thank the Department of Computer Science and
   Engineering of Universitat Jaume I for the scholarship received by
   Miguel Saura-Herreros, during his studies at the Master's Degree on
   Intelligent Systems. In addition, we would like to thank the Institute
   of New Imaging Technologies, for supporting this work. Finally, we also
   would like to thank the anonymous referees for their useful suggestions.
CR Agarwala A, 2004, ACM T GRAPHIC, V23, P294, DOI 10.1145/1015706.1015718
   [Anonymous], 2013, P VIS MOD VIS WORKSH
   Bahmutov Gleb, 2005, The Eurographics Vision, Video, and Graphics, DOI [10.2312/vvg.20051017, DOI 10.2312/VVG.20051017]
   Boyd S., 2020, CVX MATLAB SOFTWARE
   Calderero F, 2013, INT J COMPUT VISION, V104, P38, DOI 10.1007/s11263-013-0613-4
   Chen TC, 2009, PROC EUR SOLID-STATE, P1
   Diakopoulos N, 2004, LECT NOTES COMPUT SC, V3115, P299
   Eitz M, 2011, IEEE COMPUT GRAPH, V31, P56, DOI 10.1109/MCG.2011.67
   Grant MC, 2008, LECT NOTES CONTR INF, V371, P95, DOI 10.1007/978-1-84800-155-8_7
   Hartley R., 2004, MULTIPLE VIEW GEOMET, DOI [10.1017/CBO9780511811685, DOI 10.1017/CBO9780511811685]
   Hays J, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239455
   Highton S, 2003, VIRTUAL REALITY PHOT
   Hu SM, 2013, VISUAL COMPUT, V29, P393, DOI 10.1007/s00371-013-0792-6
   Johnson M, 2006, COMPUT GRAPH FORUM, V25, P407, DOI 10.1111/j.1467-8659.2006.00960.x
   Johnson MK, 2011, IEEE T VIS COMPUT GR, V17, P1273, DOI 10.1109/TVCG.2010.233
   Lalonde JF, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1276377.1276381, 10.1145/1239451.1239454]
   Lopes A.J., 2014, Pacto nacional pela alfabetizacao na idade certa: Saberes matematicos e outros campos do saber. Ministerio da Educacao, Secretaria de Educacao Basica, P1
   Persson E, 2018, HUMUS
   Reinhard E, 2013, P IEEE, V101, P1998, DOI 10.1109/JPROC.2013.2260711
   Ribelles J, 2017, MULTIMED TOOLS APPL, V76, P12757, DOI 10.1007/s11042-016-3658-x
   Saxena A, 2009, IEEE T PATTERN ANAL, V31, P824, DOI 10.1109/TPAMI.2008.132
   Tan XH, 2019, COMPUT GRAPH FORUM, V38, P641, DOI 10.1111/cgf.13867
   Wang OL, 2011, PITUITARY, 3RD EDITION, P47, DOI 10.1016/B978-0-12-380926-1.10003-3
   Zioulis N, 2018, LECT NOTES COMPUT SC, V11210, P453, DOI 10.1007/978-3-030-01231-1_28
   Zwillinger D, 2003, ADV APPL MATH
NR 25
TC 1
Z9 1
U1 0
U2 7
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2021
VL 37
IS 9-11
SI SI
BP 2809
EP 2821
DI 10.1007/s00371-021-02239-7
EA JUL 2021
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA UL2RI
UT WOS:000674208900001
OA hybrid
DA 2024-07-18
ER

PT J
AU Hu, JK
   Yip, MK
   Alonso, GE
   Gu, SH
   Tang, XJ
   Jin, XG
AF Hu, Jinkai
   Yip, Milo K.
   Alonso, Guillermo Elias
   Gu, Shihao
   Tang, Xiangjun
   Jin, Xiaogang
TI Efficient real-time dynamic diffuse global illumination using signed
   distance fields
SO VISUAL COMPUTER
LA English
DT Article
DE Real-time rendering; Global illumination; Signed distance fields
AB We present SDFDDGI, a novel approach to compute dynamic diffuse global illumination in real time using signed distance fields (SDF). For an input scene, we first construct its compact representation using SDF. Different from traditional SDF which are stored by discrete voxels, our approach approximates the scene by a set of simple primitive shapes, which facilitates real-time computation and dynamic changes. Then, we reconstruct the irradiance function in the space domain by discrete samples (referred to as probes), which are positioned heuristically for real-time performance. The probe irradiance can be updated and interpolated effectively supported by our compact SDF representation. Subsequently, a screen-space refinement method is developed to enhance rendering details and visual quality. We validate our approach by comparing the performance and quality of our method to other state-of-the-art real-time global illumination methods. Our approach is able to calculate real-time diffuse global illumination for both dynamic geometry and dynamic lighting efficiently without any precomputation, while also supporting multi-bounced light. It is also hardware free and can manage both large open scenes and indoor high-detailed scenes.
C1 [Hu, Jinkai; Alonso, Guillermo Elias; Tang, Xiangjun; Jin, Xiaogang] Zhejiang Univ, State Key Lab CAD&CG, Hangzhou 310058, Peoples R China.
   [Yip, Milo K.] Tencent, MoreFun Studios, Shenzhen, Peoples R China.
C3 Zhejiang University; Tencent
RP Jin, XG (corresponding author), Zhejiang Univ, State Key Lab CAD&CG, Hangzhou 310058, Peoples R China.
EM jin@cad.zju.edu.cn
RI Tang, Xiangjun/KEJ-5455-2024
OI Jin, Xiaogang/0000-0001-7339-2920
FU National Key R&D Program of China [2017YFB1002600]; National Natural
   Science Foundation of China [62036010, 61732015]; Ningbo Major Special
   Projects of the "Science and Technology Innovation 2025" [2020Z007]; Key
   Research and Development Program of Zhejiang Province [2020C03096]
FX We thank the anonymous reviewers for their constructive comments.
   Xiaogang Jin was supported by the National Key R&D Program of China
   (Grant No. 2017YFB1002600), the National Natural Science Foundation of
   China (Grant Nos. 62036010, 61732015), the Ningbo Major Special Projects
   of the "Science and Technology Innovation 2025" (Grant No. 2020Z007) and
   the Key Research and Development Program of Zhejiang Province (Grant No.
   2020C03096).
CR Aaltonen Sebastian., 2018, GPU BASED CLAY SIMUL, V2.5
   [Anonymous], 1997, ROBUST MONTE CARLO M
   [Anonymous], 2009, P S INT 3D GRAPH GAM, DOI 10.1145/1507149.1507161.5,7
   Balint C., 2018, EUROGRAPHICS SHORT P, P29, DOI DOI 10.2312/EGS.20181037
   Cigolle Zina H., 2014, Journal of Computer Graphics Techniques (JCGT), V3, P1
   Crassin C, 2011, COMPUT GRAPH FORUM, V30, P1921, DOI 10.1111/j.1467-8659.2011.02063.x
   Dachsbacher C., 2005, Proc. Symp. Interactive Graph. and Games, P203, DOI DOI 10.1145/1053427.1053460
   Di Benedetto M, 2014, COMPUT GRAPH FORUM, V33, P459, DOI 10.1111/cgf.12334
   Jimenez Jorge, 2016, SIGGRAPH 2016 Courses: Physically Based Shading in Theory and Practice
   Kajiya J. T., 1986, SIGGRAPH, P143, DOI 10.1145/15886.15902
   Kaplanyan A., 2010, Proceedings of the 2010 ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games, I3D'10, P99, DOI [10.1145/1730804.1730821, 10.1145/1730804.1730821.24, DOI 10.1145/1730804.1730821.24]
   Koskela M, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3269978
   Laine S., 2007, P EUR S REND, P277, DOI DOI 10.2312/EGWR/EGSR07/277-286
   Majercik Zander, 2019, Journal of Computer Graphics Techniques (JCGT), V8, P1
   McAuley S, 2015, GAM DEV C, P143
   Schied C, 2017, HPG '17: PROCEEDINGS OF HIGH PERFORMANCE GRAPHICS, DOI 10.1145/3105762.3105770
   Seyb D, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392394
   Shyshkovtsov O, 2019, GAM DEV C
   Silvennoinen A, 2015, ACM SIGGRAPH ADV REA
   Sloan PP, 2002, ACM T GRAPHIC, V21, P527, DOI 10.1145/566570.566612
   Sousa Tiago., 2011, SIGGRAPH Courses
   Thomas M.M., 2017, DEEP ILLUMINATION AP
   Wald I, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1186644.1186650
   William D., 2006, P 2006 S INT 3D GRAP, P161, DOI DOI 10.1145/1111411.1111440
   Wu JH, 2005, COMPUT GRAPH FORUM, V24, P277, DOI 10.1111/j.1467-8659.2005.00852.x
NR 25
TC 4
Z9 4
U1 1
U2 10
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2021
VL 37
IS 9-11
SI SI
BP 2539
EP 2551
DI 10.1007/s00371-021-02197-0
EA JUL 2021
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA UL2RI
UT WOS:000669180100001
DA 2024-07-18
ER

PT J
AU Wang, BQ
   Asayama, Y
   Boussejra, MO
   Shojo, H
   Adachi, N
   Fujishiro, I
AF Wang, Baoqing
   Asayama, Yume
   Boussejra, Malik Olivier
   Shojo, Hideki
   Adachi, Noboru
   Fujishiro, Issei
TI FORSETI: a visual analysis environment for authoring autopsy reports in
   extended legal medicine mark-up language
SO VISUAL COMPUTER
LA English
DT Article
DE Computational forensics; Legal medicine; Medical visualization;
   Juxtaposition; Markup language design
ID OF-THE-ART; VIRTUAL AUTOPSIES; VISUALIZATION
AB In forensic autopsy, medical examiners (MEs) and diagnostic radiologists (DRs) cooperate with each other to perform an autopsy of the corpse. Effective computational assistance tools are imperative for facilitating the intricate collaborative work involved in the autopsy. In this paper, we present an integrated visual analysis environment named FORSETI (forensic autopsy system for e-court instruments), whose technical essence is twofold. The first is to be designed on the basis of an extended version of legal medicine mark-up language for authoring reports on physical autopsy (PA) as well as on virtual autopsy (VA). The second lies in autopsy juxtaposition, which seamlessly assists the MEs and DRs in referring to the VA and PA works, respectively. A fictitious case with the Visible Female Dataset is used to demonstrate the effectiveness of an initial prototype of the FORSETI system.
C1 [Wang, Baoqing] Keio Univ, Grad Sch Sci & Technol, Yokohama, Kanagawa, Japan.
   [Asayama, Yume] Keio Univ, Grad Sch Sci & Technol, Ctr Informat & Comp Sci, Yokohama, Kanagawa, Japan.
   [Boussejra, Malik Olivier; Fujishiro, Issei] Keio Univ, Comp Sci, Yokohama, Kanagawa, Japan.
   [Shojo, Hideki; Adachi, Noboru] Univ Yamanashi, Dept Legal Med, Kofu, Yamanashi, Japan.
C3 Keio University; Keio University; Keio University; University of
   Yamanashi
RP Fujishiro, I (corresponding author), Keio Univ, Comp Sci, Yokohama, Kanagawa, Japan.
EM fuji@ics.keio.ac.jp
OI Fujishiro, Issei/0000-0002-8898-730X
FU JSPS KAKENHI [26240015, 17H00737, 21H04916]; Grants-in-Aid for
   Scientific Research [26240015, 21H04916] Funding Source: KAKEN
FX This work has been supported in part by JSPS KAKENHI under the
   Grants-in-Aid for Scientific Research (A) Nos. 26240015, 17H00737, and
   21H04916.
CR Ackerman MJ, 1998, P IEEE, V86, P504, DOI 10.1109/5.662875
   [Anonymous], 2013, Extensible markup language
   Benali L, 2013, INT J LEGAL MED, V127, P1045, DOI 10.1007/s00414-012-0789-0
   Bichlmeier C., 2007, INT S MIX AUGM REAL, P129, DOI DOI 10.1109/ISMAR.2007.4538837
   Bolliger SA, 2008, EUR RADIOL, V18, P273, DOI 10.1007/s00330-007-0737-4
   Boussejra M.O., 2016, P 2016 EUROVIS SHORT, P31
   Ebert LC, 2016, J FORENSIC RADIOL IM, V5, P1, DOI 10.1016/j.jofri.2015.11.007
   Filko D, 2018, MACH VISION APPL, V29, P633, DOI 10.1007/s00138-018-0920-4
   Jiang Z, 2013, COMPUT MED IMAG GRAP, V37, P131, DOI 10.1016/j.compmedimag.2013.03.001
   Langer R, 2019, VIRCHOWS ARCH, V474, P149, DOI 10.1007/s00428-018-2482-2
   Levy AD., 2010, ESSENTIALS FORENSIC, DOI [10.1201/b10444, DOI 10.1201/B10444]
   Lindow John., 2002, NORSE MYTHOLOGY GUID
   Ljung P, 2006, IEEE T VIS COMPUT GR, V12, P869, DOI 10.1109/TVCG.2006.146
   Lorensen B, 2020, VISUALIZATION TOOLKI
   Lundstrom, 2015, P EUR 2015 DIRK BART, P9
   Lundström C, 2012, APMIS, V120, P316, DOI 10.1111/j.1600-0463.2011.02857.x
   Lundström C, 2011, IEEE T VIS COMPUT GR, V17, P1775, DOI 10.1109/TVCG.2011.224
   Macedo MCF, 2014, SYMP VIRTUAL AUGMENT, P63, DOI 10.1109/SVR.2014.29
   Pimentel JF, 2019, ACM COMPUT SURV, V52, DOI 10.1145/3311955
   Roberts ISD, 2012, LANCET, V379, P136, DOI 10.1016/S0140-6736(11)61483-9
   Ropinski T., 2007, Internal labels as shape cues for medical illustration, P203
   Scandurra I, 2010, STUD HEALTH TECHNOL, V160, P639, DOI 10.3233/978-1-60750-588-4-639
   Shamata A, 2020, J FORENSIC SCI, V65, P627, DOI 10.1111/1556-4029.14205
   Tatzgern M, 2014, 2014 IEEE VIRTUAL REALITY (VR), P27, DOI 10.1109/VR.2014.6802046
   Thali MJ, 2003, J FORENSIC SCI, V48, P386
   Thompson H, 2012, W3C RECOMMENDATION, V5
   Underwood J, 2012, LANCET, V379, P100, DOI 10.1016/S0140-6736(11)61584-5
   Urschler M, 2012, IEEE COMPUT GRAPH, V32, P79, DOI 10.1109/MCG.2012.75
   Ynnerman A, 2016, COMMUN ACM, V59, P72, DOI 10.1145/2950040
   ZABUSKY NJ, 1993, PHYS TODAY, V46, P24, DOI 10.1063/1.881394
NR 30
TC 1
Z9 1
U1 1
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2021
VL 37
IS 12
SI SI
BP 2951
EP 2963
DI 10.1007/s00371-021-02201-7
EA JUN 2021
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP1DP
UT WOS:000663983600002
DA 2024-07-18
ER

PT J
AU Liu, DF
   Chen, LF
AF Liu, Defeng
   Chen, Lifang
TI SECPNet-secondary encoding network for estimating camera parameters
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Camera calibration; Camera pose estimation; Deep learning; Focal length
   estimation; Multi-view image
ID CALIBRATION METHOD; POSE
AB Camera parameter estimation can be used in visual odometry, robot vision, SLAM, 3D reconstruction and other directions. It is also the main research content of computer vision. Based on the deep learning strategy, we propose a secondary encoding network for camera parameters (SECPNet), which can predict the camera parameters and recover the camera pose according to a single RGB image. Based on the three-dimensional dataset ShapeNet40 (Chang et al. in An information-rich 3D model repository, 2015. arXiv:1512.03012), we build a varifocal multi-viewpoint image dataset for camera parameter estimation. Experimental results show that our method has state-of-the-art performance in camera parameter estimation.
C1 [Liu, Defeng; Chen, Lifang] Jiangnan Univ, Sch Artificial Intelligence & Comp Sci, Wuxi 214122, Jiangsu, Peoples R China.
C3 Jiangnan University
RP Chen, LF (corresponding author), Jiangnan Univ, Sch Artificial Intelligence & Comp Sci, Wuxi 214122, Jiangsu, Peoples R China.
EM defengjnu@stu.jiangnan.edu.cn; may7366@163.com
FU Postgraduate Research & Practice Innovation Program of Jiangsu Province
   [SJCX20 _0775]
FX This study was funded by Postgraduate Research & Practice Innovation
   Program of Jiangsu Province Grant Number SJCX20 _0775.
CR Abdel-Aziz YI, 2015, PHOTOGRAMM ENG REM S, V81, P103, DOI 10.14358/PERS.81.2.103
   Ardakani HK, 2020, VISUAL COMPUT, V36, P413, DOI 10.1007/s00371-019-01632-7
   Blender, 2010, US Patent App, Patent No. [29/350,912, 29350912]
   Brachmann E, 2014, LECT NOTES COMPUT SC, V8690, P536, DOI 10.1007/978-3-319-10605-2_35
   Brahmbhatt S, 2018, PROC CVPR IEEE, P2616, DOI 10.1109/CVPR.2018.00277
   Cai BL, 2019, OPT LASER ENG, V114, P44, DOI 10.1016/j.optlaseng.2018.10.011
   Charco JL, 2018, 2018 14TH INTERNATIONAL CONFERENCE ON SIGNAL IMAGE TECHNOLOGY & INTERNET BASED SYSTEMS (SITIS), P224, DOI 10.1109/SITIS.2018.00041
   Chen B, 2020, OPT LASER ENG, V126, DOI 10.1016/j.optlaseng.2019.105919
   DeMa S, 1996, IEEE T ROBOTIC AUTOM, V12, P114, DOI 10.1109/70.481755
   Do T.T., 2018, ARXIV180210367
   Fang Q, 2018, IEEE ANN INT CONF CY, P61, DOI 10.1109/CYBER.2018.8688359
   Frosio I, 2016, VISUAL COMPUT, V32, P663, DOI 10.1007/s00371-015-1089-8
   Funkhouser T.A., 2015, P COMP VIS PATT REC
   Glorot X., 2010, P INT C ART INT STAT, P249
   Grabner A, 2019, IEEE I CONF COMP VIS, P2222, DOI 10.1109/ICCV.2019.00231
   Guan W, 2017, CHIN CONT DECIS CONF, P5217, DOI 10.1109/CCDC.2017.7979423
   Guo F, 2017, IEEE GLOB CONF SIG, P408, DOI 10.1109/GlobalSIP.2017.8308674
   Guo Yan Xu, 2011, 2011 International Conference on Image Analysis and Signal Processing (IASP 2011), P133, DOI 10.1109/IASP.2011.6109013
   Hinterstoisser V., 2012, P COMP VIS ACCV 2012, P548
   Kehl W, 2017, IEEE I CONF COMP VIS, P1530, DOI 10.1109/ICCV.2017.169
   Kendall A, 2017, PROC CVPR IEEE, P6555, DOI 10.1109/CVPR.2017.694
   Kendall A, 2015, IEEE I CONF COMP VIS, P2938, DOI 10.1109/ICCV.2015.336
   Kim T.-K., 2019, INSTANCEAND CATEGORY, P243
   Li H, 2010 2 INT C SIGN PR, V3
   Li SG, 2017, PROCEEDINGS 2017 4TH IAPR ASIAN CONFERENCE ON PATTERN RECOGNITION (ACPR), P168, DOI 10.1109/ACPR.2017.79
   Liu YL, 2018, VISUAL COMPUT, V34, P899, DOI 10.1007/s00371-018-1523-9
   Matei BC, 2006, IEEE T PATTERN ANAL, V28, P1537, DOI 10.1109/TPAMI.2006.205
   Maybank S.J, 1998, LECT NOTES COMPUTER, V588
   Mottaghi R, 2015, PROC CVPR IEEE, P418, DOI 10.1109/CVPR.2015.7298639
   Mousavian A, 2017, PROC CVPR IEEE, P5632, DOI 10.1109/CVPR.2017.597
   Duong ND, 2018, ADJUNCT PROCEEDINGS OF THE 2018 IEEE INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY (ISMAR), P258, DOI 10.1109/ISMAR-Adjunct.2018.00080
   Naseer T, 2017, IEEE INT C INT ROBOT, P1525, DOI 10.1109/IROS.2017.8205957
   Peng SD, 2019, PROC CVPR IEEE, P4556, DOI 10.1109/CVPR.2019.00469
   Sattler T, 2019, PROC CVPR IEEE, P3297, DOI 10.1109/CVPR.2019.00342
   Sattler T, 2018, PROC CVPR IEEE, P8601, DOI 10.1109/CVPR.2018.00897
   Sattler T, 2014, LECT NOTES COMPUT SC, V8692, P828, DOI 10.1007/978-3-319-10593-2_54
   Simonyan K., 2014, CORR
   Sun J, 2014, 2014 11TH WORLD CONGRESS ON INTELLIGENT CONTROL AND AUTOMATION (WCICA), P86, DOI 10.1109/WCICA.2014.7052692
   Sun XY, 2018, PROC CVPR IEEE, P2974, DOI 10.1109/CVPR.2018.00314
   Sundermeyer M, 2018, LECT NOTES COMPUT SC, V11210, P712, DOI 10.1007/978-3-030-01231-1_43
   Tekin B, 2018, PROC CVPR IEEE, P292, DOI 10.1109/CVPR.2018.00038
   Triggs B, 1997, PROC CVPR IEEE, P609, DOI 10.1109/CVPR.1997.609388
   Wang C., ARXIV191010750
   Workman S, 2015, IEEE IMAGE PROC, P1369, DOI 10.1109/ICIP.2015.7351024
   Wu CC, 2015, PROC CVPR IEEE, P2440, DOI 10.1109/CVPR.2015.7298858
   Xiang Y, 2018, ROBOTICS: SCIENCE AND SYSTEMS XIV
   Zhang Z. Y., 1999, P IEEE INT C COMP VI
   Zhang Z, 2018, MEASUREMENT, V130, P298, DOI 10.1016/j.measurement.2018.07.085
   Zhang ZY, 2000, IEEE T PATTERN ANAL, V22, P1330, DOI 10.1109/34.888718
   Zheng YQ, 2016, PROC CVPR IEEE, P1790, DOI 10.1109/CVPR.2016.198
   Zheng YQ, 2014, PROC CVPR IEEE, P430, DOI 10.1109/CVPR.2014.62
   Zhu ZM, 2019, OPT LASER ENG, V112, P128, DOI 10.1016/j.optlaseng.2018.09.009
NR 52
TC 0
Z9 0
U1 1
U2 14
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2021 MAR 25
PY 2021
DI 10.1007/s00371-011-01098-1
EA MAR 2021
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RC3ZN
UT WOS:000632742800001
DA 2024-07-18
ER

PT J
AU Yu, Y
   Niu, CJ
   Li, J
   Xu, K
AF Yu, Yang
   Niu, Chengjie
   Li, Jun
   Xu, Kai
TI Multi-view 2D-3D alignment with hybrid bundle adjustment for visual
   metrology
SO VISUAL COMPUTER
LA English
DT Article
DE Visual metrology; Multi-view geometry; Bundle adjustment; 3D model
   alignment
AB High-precision measurement based on multi-view geometry benefits from aligning a template CAD model to the multi-view observations of a target object. It not only improves the multi-camera calibration but also assists the measurement by serving as a "scaffold." A straightforward approach is to reconstruct the target object and perform a 3D registration with the CAD model. However, the accuracy of such 3D alignment cannot meet the high-precision requirement. We formulate the problem as a bundle adjustment where we jointly optimize the 6DoF poses of both the template model and the multiple cameras. To accommodate the manufacturing error of products, we propose a simple and robust solution based on a discrete-continuous optimization which interleaves between correspondence selection and pose optimization. In the discrete step, a robust RANSAC-based selection process selects well-matched 2D-3D feature points according to the current model/camera poses. In the continuous step, it jointly optimizes the 6D poses of the CAD model and the multiple cameras. This interleaving optimization constitutes a novel hybrid bundle adjustment (HBA). In HBA, the reprojection error of a feature point is measured either with the 2D-3D correspondence between the observation image and the CAD model or with the cross-view correspondence between multiple images, whichever is more reliable according to the discrete selection step. Through extensive evaluation on real-world data, we demonstrate that HBA achieves high-precision multi-camera calibration, outperforming alternative approaches significantly.
C1 [Yu, Yang; Niu, Chengjie; Li, Jun; Xu, Kai] Natl Univ Def Technol, Changsha, Peoples R China.
C3 National University of Defense Technology - China
RP Li, J; Xu, K (corresponding author), Natl Univ Def Technol, Changsha, Peoples R China.
EM jun.johnson.li@gmail.com; kevin.kai.xu@gmail.com
FU National Key Research and Development Program of China
   [2018AAA-0102200]; National Natural Science Foundation of China
   [61572507, 61532003, 61622212, 61902419]
FX This work was supported in part by the National Key Research and
   Development Program of China (2018AAA-0102200) and the National Natural
   Science Foundation of China (61572507, 61532003, 61622212, 61902419).
CR Agarwal S, 2010, LECT NOTES COMPUT SC, V6312, P29, DOI 10.1007/978-3-642-15552-9_3
   Bas G, 2015, PROCEDIA ENGINEER, V100, P1616, DOI 10.1016/j.proeng.2015.01.535
   Bellekens B., 2015, Int. J. Adv. Intell. Syst, V8, P118
   Bergström P, 2018, OPT ENG, V57, DOI 10.1117/1.OE.57.5.053110
   Bian JW, 2017, PROC CVPR IEEE, P2828, DOI 10.1109/CVPR.2017.302
   Birdal T., 2016, 2016 IEEE WINT C APP, P1
   Brandner, 1993, VISION SENSORS EDGE
   Brenner C., 1998, INT, V32, P209
   Byröd M, 2010, LECT NOTES COMPUT SC, V6312, P114, DOI 10.1007/978-3-642-15552-9_9
   Chen ZZ, 2006, PATTERN RECOGN LETT, V27, P1447, DOI 10.1016/j.patrec.2006.01.017
   Criminisi A., 2002, Pattern Recognition. 24th DAGM Symposium. Proceedings (Lecture Notes in Computr Science Vol.2449), P224
   Criminisi A., 2012, ACCURATE VISUAL METR
   Debevec P. E., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P11, DOI 10.1145/237170.237191
   Delaunoy A, 2014, PROC CVPR IEEE, P1486, DOI 10.1109/CVPR.2014.193
   Deng JK, 2019, IEEE T IMAGE PROCESS, V28, P3636, DOI 10.1109/TIP.2019.2899267
   Fua P., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P46, DOI 10.1109/ICCV.1999.791196
   Hartley R, 2003, MULTIPLE VIEW GEOMET, DOI 10.1016/S0143-8166(01)00145-2
   Hinterstoisser S, 2011, IEEE I CONF COMP VIS, P858, DOI 10.1109/ICCV.2011.6126326
   Hinterstoisser V., 2012, P COMP VIS ACCV 2012, P548
   HUBER PJ, 1964, ANN MATH STAT, V35, P73, DOI 10.1214/aoms/1177703732
   Liang BJ, 2004, INT C PATT RECOG, P96, DOI 10.1109/ICPR.2004.1334016
   Lim JJ, 2013, IEEE I CONF COMP VIS, P2992, DOI 10.1109/ICCV.2013.372
   Lin CH, 2019, PROC CVPR IEEE, P969, DOI 10.1109/CVPR.2019.00106
   Long CY, 2016, MEASUREMENT, V90, P424, DOI 10.1016/j.measurement.2016.05.017
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   McMurtry, 1982, US Patent, Patent No. [4,333,238, 4333238]
   Oakland RJ., 2018, STAT PROCESS CONTROL, DOI [10.4324/9781315160511, DOI 10.4324/9781315160511]
   Olson E, 2011, IEEE INT CONF ROBOT
   Pilu M, 1997, PROC CVPR IEEE, P261, DOI 10.1109/CVPR.1997.609330
   Porikli F, 2003, 2003 INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL I, PROCEEDINGS, P653
   Ren ZG, 2010, APPL OPTICS, V49, P1789, DOI 10.1364/AO.49.001789
   Salas-Moreno RF, 2013, PROC CVPR IEEE, P1352, DOI 10.1109/CVPR.2013.178
   Salaün Y, 2016, LECT NOTES COMPUT SC, V9911, P801, DOI 10.1007/978-3-319-46478-7_49
   Schönberger JL, 2016, PROC CVPR IEEE, P4104, DOI 10.1109/CVPR.2016.445
   Schönberger JL, 2016, LECT NOTES COMPUT SC, V9907, P501, DOI 10.1007/978-3-319-46487-9_31
   Schöps T, 2017, PROC CVPR IEEE, P2538, DOI 10.1109/CVPR.2017.272
   Schwenke H, 2000, CIRP ANNALS 2000: MANUFACTURING TECHNOLOGY, P395
   Scott WR, 2003, ACM COMPUT SURV, V35, P64, DOI 10.1145/641865.641868
   Shan Y, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P644, DOI 10.1109/ICCV.2001.937687
   Sorkine-Hornung O., 2017, COMPUTING, V1, P1
   Su YC, 2009, LECT NOTES COMPUT SC, V5558, P179
   Sun J, 2003, IEEE T PATTERN ANAL, V25, P787, DOI 10.1109/TPAMI.2003.1206509
   Szeliski R., 1998, 3D Structure from Multiple Images of Large-Scale Environments. European Workshop, SMILE'98. Proceedings, P171
   Triggs B., 2000, Vision Algorithms: Theory and Practice. International Workshop on Vision Algorithms. Proceedings (Lecture Notes in Computer Science Vol. 1883), P298
   van den Heuvel, 2000, INT ARCH PHOTOGRAMME
   WOLFSON HJ, 1990, LECT NOTES COMPUT SC, V427, P526, DOI 10.1007/BFb0014902
   Wulf O., 2003, International conference on control systems and computer science (CSCS14), P2
   Xia RB, 2020, MEAS SCI TECHNOL, V31, DOI 10.1088/1361-6501/ab4ed5
   Zach C, 2014, LECT NOTES COMPUT SC, V8693, P772, DOI 10.1007/978-3-319-10602-1_50
   Zhou QY, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601134
NR 50
TC 1
Z9 1
U1 3
U2 22
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2022
VL 38
IS 4
SI SI
BP 1483
EP 1494
DI 10.1007/s00371-021-02082-w
EA MAR 2021
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 0D9DX
UT WOS:000626398400001
DA 2024-07-18
ER

PT J
AU Bedjaoui, M
   Elouali, N
   Benslimane, SM
   Sengel, E
AF Bedjaoui, Mohammed
   Elouali, Nadia
   Benslimane, Sidi Mohamed
   Sengel, Erhan
TI Suggestion pattern on online social networks: between intensity,
   effectiveness and user's satisfaction
SO VISUAL COMPUTER
LA English
DT Article
DE Persuasive technology; Usability; Patterns of influence; Suggestion
   pattern; Users&#8217; time spent; Content analysis
ID AGREEMENT
AB Forms of influence are widely applied in social networks in order to encourage users to take actions that are favourable to these technologies providers. In our prior work, we proposed a set of influence patterns that are applied in social networks (suggestion pattern, reminder pattern, reward pattern, interaction pattern and social influence pattern) which influence users progressively over time in order to shape their behaviours and to persuade them to stay as long as possible. Nevertheless, the guidance or recommendations for applying these patterns for developers have not yet been defined. This research will focus on the first suggestion pattern and describes an experiment designed to examine whether excessive/intense application of suggestions (which may adversely affect user time) is also more effective and more satisfying from a user perspective. We used two video sharing applications (YouTube and YouTube Focus); the first contains excessive/intense suggestions against the second that contains limited suggestions. Our finding shows that limited suggestions are more effective than excessive/intense suggestions and are as satisfactory as excessive/intense suggestions. We believe that these results will promote favourable outcomes when applying the suggestion pattern, (1) for users: by helping them understand the nature of influence techniques and to empower them to be proactive in creating an environment that is more favourable for them, and that helps them to achieve their goals without getting distracted; (2) for designers: by providing them with insights on the optimal and effective method of using patterns of influence notably, suggestion pattern.
C1 [Bedjaoui, Mohammed; Elouali, Nadia; Benslimane, Sidi Mohamed] Ecole Super Informat, LabRI SBA Lab, Sidi Bel Abbes, Algeria.
   [Sengel, Erhan] Uludag Univ, Fac Educ, Dept Comp Educ & Instruct Technol, Bursa, Turkey.
C3 Uludag University
RP Bedjaoui, M (corresponding author), Ecole Super Informat, LabRI SBA Lab, Sidi Bel Abbes, Algeria.
EM m.bedjaoui@esi-sba.dz; n.elouali@esi-sba.dz; s.benslimane@esi-sba.dz;
   drerhansengel@gmail.com
RI Benslimane, Sidi Mohamed/AAB-3255-2020
OI Benslimane, Sidi Mohamed/0000-0002-7008-7434; Sengel,
   Erhan/0000-0003-0927-2814; Elouali, Nadia/0000-0002-3427-3766; Bedjaoui,
   Mohammed/0000-0002-8932-070X
FU Algerian Ministry of Higher Education and Scientific Research;
   Directorate General for Scientific Research and Technological
   Development (DG-RSDT)
FX This study was funded by Algerian Ministry of Higher Education and
   Scientific Research and the Directorate General for Scientific Research
   and Technological Development (DG-RSDT).
CR Altman DG, 1991, PRACTICAL STAT MED R
   [Anonymous], 1998, The International Organization for Standardization, V45
   Bastien J.M. C., 1993, INRIA, P79
   BEDJAOUI M, 2020, P 15 INT JOINT C COM, V2, P35
   Bedjaoui M, 2014, 16TH INTERNATIONAL CONFERENCE ON ADVANCES IN MOBILE COMPUTING AND MULTIMEDIA (MOMM 2018), P15, DOI 10.1145/3282353.3282362
   Branch, 2019, THESIS ROCHESTER I T
   Caruso J., 2007, The ECAR Study of Undergraduate Students and Information Technology, 2007: Key Findings
   Chan T.K., 2015, PACIS, P196
   Cialdini R.B., 1984, PSYCHOL PERSUASION
   COHEN J, 1960, EDUC PSYCHOL MEAS, V20, P37, DOI 10.1177/001316446002000104
   da Silva JV, 2016, IFIP ADV INF COMM TE, V477, P59, DOI 10.1007/978-3-319-42102-5_7
   Davis J.W., 1998, PROC WORKSHOP PERCEP, P13
   Elo S, 2008, J ADV NURS, V62, P107, DOI 10.1111/j.1365-2648.2007.04569.x
   Elouali N, 2019, J MULTIMODAL USER IN, V13, P395, DOI 10.1007/s12193-019-00310-1
   Taype GEE, 2020, ADV INTELL SYST COMP, V1137, P35, DOI 10.1007/978-3-030-40690-5_4
   Fogg BJ, 2007, LECT NOTES COMPUT SC, V4744, P199
   Fogg B.J., 2003, PERSUASIVE TECHNOLOG, DOI [DOI 10.1016/B978-1-55860-643-2.X5000-8, 10.1016/B978-1-55860-643-2.X5000-8]
   Harwood T. G., 2003, PRACTICAL ASSESSMENT, V3, P479, DOI [DOI 10.7275/Z6FM-2E34, https://doi.org/10.1362/146934703771910080]
   Holsti O., 1969, Content analysis for the social sciences and humanities
   Hsieh HF, 2005, QUAL HEALTH RES, V15, P1277, DOI 10.1177/1049732305276687
   Iacob, 2009, BEST PRACTICES EC PE, P197
   Kampik T., 2018, 20 INT TRUST WORKSH, P38
   Kuss DJ, 2011, INT J ENV RES PUB HE, V8, P3528, DOI 10.3390/ijerph8093528
   LANDIS JR, 1977, BIOMETRICS, V33, P159, DOI 10.2307/2529310
   Némery A, 2014, J USABILITY STUD, V9, P105
   Oduor M, 2014, PERS UBIQUIT COMPUT, V18, P1689, DOI 10.1007/s00779-014-0778-z
   Oinas-Kukkonen H., 2008, First International Conference on Advances in Computer-Human Interaction, P200, DOI DOI 10.1109/ACHI.2008.31
   Oinas-Kukkonen H, 2009, COMMUN ASSOC INF SYS, V24, P485
   Razali N. M., 2011, J. Stat. Model. and Anal., V2, P21, DOI DOI 10.1515/BILE-2015-0008
   Rosenthal R., 2007, ESSENTIALS BEHAV RES
   SHAPIRO SS, 1965, BIOMETRIKA, V52, P591, DOI 10.1093/biomet/52.3-4.591
   Shneiderman B., 2010, DESIGNING USER INTER
   SHOTTON MA, 1991, BEHAV INFORM TECHNOL, V10, P219, DOI 10.1080/01449299108924284
   Thadani, 2011, ICIS 2011 P 34
   Tullis T., 2013, Measuring the User Experience: Collecting, Analyzing, and Presenting Usability Metrics
   United Nations Educational Scientific and Cultural Organization (UNESCO), 2018, INT TECHN GUID SEX E
   Vaismoradi M, 2013, NURS HEALTH SCI, V15, P398, DOI 10.1111/nhs.12048
   Valkenburg PM, 2011, J ADOLESCENT HEALTH, V48, P121, DOI 10.1016/j.jadohealth.2010.08.020
   van Delden R, 2019, LECT NOTES COMPUT SC, V11433, P3, DOI 10.1007/978-3-030-17287-9_1
   van den Eijnden RJJM, 2016, COMPUT HUM BEHAV, V61, P478, DOI 10.1016/j.chb.2016.03.038
   Wansink B, 2005, OBES RES, V13, P93, DOI 10.1038/oby.2005.12
   Weiksner GM, 2008, LECT NOTES COMPUT SC, V5033, P151, DOI 10.1007/978-3-540-68504-3_14
   Wimmer R.D., 2014, MASS MEDIA RES INTRO, V10th
   Yahya Y, 2019, PROCEEDINGS OF THE 2019 5TH INTERNATIONAL CONFERENCE ON COMPUTER AND TECHNOLOGY APPLICATIONS (ICCTA 2019), P112, DOI 10.1145/3323933.3324090
   Yang SC, 2007, COMPUT HUM BEHAV, V23, P79, DOI 10.1016/j.chb.2004.03.037
   Yates D., 1999, The Practice of Statistics, V1st
NR 46
TC 0
Z9 0
U1 1
U2 10
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2022
VL 38
IS 4
SI SI
BP 1331
EP 1343
DI 10.1007/s00371-021-02084-8
EA FEB 2021
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA 0D9DX
UT WOS:000621265200001
DA 2024-07-18
ER

PT J
AU Liu, DC
   Su, QT
   Yuan, ZH
   Zhang, XT
AF Liu, Decheng
   Su, Qingtang
   Yuan, Zihan
   Zhang, Xueting
TI A color watermarking scheme in frequency domain based on quaternary
   coding
SO VISUAL COMPUTER
LA English
DT Article
DE Color watermarking; Affine transform; Schur decomposition; Quaternary
   coding
ID BLIND WATERMARKING; PROTECTION SCHEME; IMAGES
AB With the continuous development of the computer network information technique, the problem of copyright protection of color images is increasingly getting prominent. Based on above issue, this paper presents a quaternary watermarking of color image using Schur decomposition. The superiorities of the presented scheme contains three points: Firstly, the algorithm obtains the maximum eigenvalue of the pixel block using Schur decomposition that have a high efficiency, and accomplishes the hiding and blind detection of color watermark by quantifying the maximum eigenvalue to four intervals. Secondly, the algorithm uses affine transform that have high security to encrypt the location of the color watermark pixels, which improves the security of the watermarking algorithm. Thirdly, the new idea of four intervals quantization is proposed to apply to this scheme. Simulation consequences indicate that the presented scheme can satisfy the needs of visual imperceptibility, and also has large watermark capacity, strong robustness and high security.
C1 [Liu, Decheng; Su, Qingtang; Yuan, Zihan; Zhang, Xueting] Ludong Univ, Sch Informat & Elect Engn, Yantai 264025, Peoples R China.
C3 Ludong University
RP Su, QT (corresponding author), Ludong Univ, Sch Informat & Elect Engn, Yantai 264025, Peoples R China.
EM sdytsqt@163.com
FU National Natural Science Foundations of China [61771231, 61772253,
   61873117, 61872170]; Key Research and Development Program of Shandong
   Province [2019GGX101025]
FX The work was supported by the National Natural Science Foundations of
   China (Nos. 61771231, 61772253, 61873117, and 61872170), and the Key
   Research and Development Program of Shandong Province (No.
   2019GGX101025).
CR Abbasi A, 2015, MEASUREMENT, V74, P116, DOI 10.1016/j.measurement.2015.06.006
   [Anonymous], 1993, DIGITAL IMAGE COMPUT
   Ahmadi SBB, 2021, VISUAL COMPUT, V37, P385, DOI 10.1007/s00371-020-01808-6
   Bas P, 2002, IEEE T IMAGE PROCESS, V11, P1014, DOI 10.1109/TIP.2002.801587
   Cao LJ, 2013, VISUAL COMPUT, V29, P231, DOI 10.1007/s00371-012-0732-x
   Chang CC, 2002, PATTERN RECOGN LETT, V23, P931, DOI 10.1016/S0167-8655(02)00023-5
   Chen H, 2013, OPT LASER ENG, V51, P768, DOI 10.1016/j.optlaseng.2013.01.016
   Ernawan F, 2020, VISUAL COMPUT, V36, P19, DOI 10.1007/s00371-018-1567-x
   Golea N., 2010, IEEE INT C COMPUTER, P1
   Golub G.H., 2013, Matrix Computations, DOI DOI 10.56021/9781421407944
   Guibin Zhu, 2010, 2010 International Symposium on Information Science and Engineering (ISISE 2010), P184, DOI 10.1109/ISISE.2010.60
   Kincaid D., 2009, NUMERICAL ANAL MATH
   Li LD, 2011, AEU-INT J ELECTRON C, V65, P435, DOI 10.1016/j.aeue.2010.06.001
   Lin CH, 2010, VISUAL COMPUT, V26, P1101, DOI [10.1007/s00371-010-0461-y, 10.1007/s00371-010-0461]
   Liu DC, 2020, MULTIMED TOOLS APPL, V79, P7491, DOI 10.1007/s11042-019-08423-1
   Moosazadeh M, 2017, OPTIK, V140, P975, DOI 10.1016/j.ijleo.2017.05.011
   Murali P, 2018, OPTIK, V170, P242, DOI 10.1016/j.ijleo.2018.04.050
   Qi HY, 2008, SIGNAL PROCESS, V88, P174, DOI 10.1016/j.sigpro.2007.07.020
   Shang ZW, 2008, PROCEEDINGS OF THE 9TH INTERNATIONAL CONFERENCE FOR YOUNG COMPUTER SCIENTISTS, VOLS 1-5, P2942, DOI 10.1109/ICYCS.2008.99
   Su QT, 2019, IEEE ACCESS, V7, P4358, DOI 10.1109/ACCESS.2018.2888857
   Su QT, 2018, SOFT COMPUT, V22, P91, DOI 10.1007/s00500-017-2489-7
   Su QT, 2017, MULTIMED TOOLS APPL, V76, P24221, DOI 10.1007/s11042-016-4164-x
   Su QT, 2016, IET IMAGE PROCESS, V10, P817, DOI 10.1049/iet-ipr.2016.0048
   Su QT, 2015, SIGNAL IMAGE VIDEO P, V9, P991, DOI 10.1007/s11760-013-0534-2
   Su QT, 2014, SIGNAL PROCESS, V94, P219, DOI 10.1016/j.sigpro.2013.06.025
   Su QT, 2012, OPT COMMUN, V285, P1792, DOI 10.1016/j.optcom.2011.12.065
   Tseng YC, 2002, IEEE T COMMUN, V50, P1227, DOI 10.1109/TCOMM.2002.801488
   University of Granada Computer Vision Group, 2002, CVG UGR IM DAT
   University of Southern California Signal and Inage Processing Institute, 1997, USC SIPI IM DAT
   Vaidya SP, 2017, MULTIMED TOOLS APPL, V76, P25623, DOI 10.1007/s11042-017-4355-0
   Wang H, 2018, CHIN CONTR CONF, P9250, DOI 10.23919/ChiCC.2018.8484184
   Wu M. Y., 1998, P INT S MULT INF PRO
   Yuan ZH, 2020, OPTIK, V204, DOI 10.1016/j.ijleo.2019.164152
   Zhang FY, 2019, MULTIMED TOOLS APPL, V78, P20133, DOI 10.1007/s11042-019-7326-9
   Zhao J., 1995, Intellectual Property Rights and New Technologies. Proceedings of the KnowRight'95 Conference, P242
NR 35
TC 15
Z9 15
U1 0
U2 17
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2021
VL 37
IS 8
BP 2355
EP 2368
DI 10.1007/s00371-020-01991-6
EA NOV 2020
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TT5FX
UT WOS:000591967400001
DA 2024-07-18
ER

PT J
AU Montazeri, A
   Shamsi, M
   Dianat, R
AF Montazeri, Azadeh
   Shamsi, Mahboubeh
   Dianat, Rouhollah
TI MLK-SVD, the new approach in deep dictionary learning
SO VISUAL COMPUTER
LA English
DT Article
DE Multilayered; K-singular; Value decomposition (MLK-SVD); Sparse
   representation; Deep learning; Classification
AB The aim of this study is to improve the classification efficiency of advanced methods using a multilayered dictionary learning framework. This paper presents the new idea of "multilayered K-singular value decomposition (MLK-SVD)" dictionary learning as a multilayer method of classification. This method starts by building a sparse representation at the patch level and relies on a hierarchy of learned dictionaries to output a global sparse representation for the whole image. In this research using class labels of training data, the label information is associated with each dictionary item (columns of the dictionary matrix) to enforce discrimination in sparse codes during the dictionary learning process. Also, this algorithm instead of learning one shallow dictionary learned multiple levels of dictionaries. The proposed formulation of deep dictionary learning provides the basis to develop more efficient dictionary learning algorithms. It relies on a succession of sparse coding and pooling steps in order to find an efficient representation of the data for classification. The performance of the proposed method is evaluated on MNIST and CIFAR-10 datasets, and results show that this method can help in advancing the state of the art.
C1 [Montazeri, Azadeh; Dianat, Rouhollah] Univ Qom, Fac Comp, Dept Informat Technol, Qom, Iran.
   [Shamsi, Mahboubeh] Qom Univ Technol, Fac Comp & Elect, Dept Comp, Qom, Iran.
C3 University of Qom
RP Montazeri, A (corresponding author), Univ Qom, Fac Comp, Dept Informat Technol, Qom, Iran.
EM montazeri.a.85@gmail.com; shamsi@qut.ac.ir; rdianat@qom.ac.ir
RI Shamsi, Mahboubeh/AAF-4417-2022; Shamsi, Mahboubeh/AFI-9693-2022
OI Shamsi, Mahboubeh/0000-0003-1238-4315; Shamsi,
   Mahboubeh/0000-0003-1238-4315; Montazeri, Azadeh/0000-0002-7520-5831
CR [Anonymous], 2010, P IEEE C COMP VIS PA
   [Anonymous], 2016, P IEEE C COMP VIS PA
   Ataee Z., 2019, VISUAL COMPUT
   Bourlard H, 1998, BIOL CYBERN, V59, P291294
   Caballero J, 2014, IEEE T MED IMAGING, V33, P979, DOI 10.1109/TMI.2014.2301271
   Chan Wai Tim S., 2016, INT C ADV CONC INT V
   Chang EY, 2018, NEUROCOMPUTING
   Chen JL, 2016, PROGNOST SYST HEALT
   Gu B, 2018, PATTERN RECOGN, V83, P196, DOI 10.1016/j.patcog.2018.05.023
   Hagargi P.A., 2018, BRAIN TUMOR MR IMAGE
   Han ZZ, 2018, IEEE T IMAGE PROCESS, V27, P3049, DOI 10.1109/TIP.2018.2816821
   Huang Y, 2018, IET IMAGE PROCESS, V12, P1626, DOI 10.1049/iet-ipr.2017.1005
   Jiang Z, 2013, IEEE T PATTERN ANAL, V35, P26512664
   Jin W, 2014, PATTERN RECOGNIT LET, V49, P193200
   Khan N., 2012, P INT C PATT REC
   Krizhevsky A., 2009, LEARNING MULTIPLE LA
   Lee DD, 1999, NATURE, V401, P788, DOI 10.1038/44565
   Li H, 2020, MODELING PREDICTING
   Li Z.Q., 2020, ARXIV200203271
   Liu YJ, 2019, VISUAL COMPUT, V35, P667, DOI 10.1007/s00371-018-1502-1
   Mahdizadehaghdam S., 2019, IEEE T IMAGE PROCESS
   Mitro J., 2018, WORKSHOPS 32 AAAI C, P364
   Naderahmadian Y., 2018, 2018 IEEE STAT SIGN, P40
   Olshausen BA, 1997, VISION RES, V37, P3311, DOI 10.1016/S0042-6989(97)00169-7
   Pacheco AGC, 2018, EXPERT SYST APPL, V96, P77, DOI 10.1016/j.eswa.2017.11.054
   Saha M., 2018, BIOMEDICAL SIGNAL PR
   Schnass K, 2018, APPL COMPUT HARMON A, V45, P22, DOI 10.1016/j.acha.2016.08.002
   Son CH, 2014, IEEE T IMAGE PROCESS, V23, P2542, DOI 10.1109/TIP.2014.2319732
   Sulam J., 2018, ARXIV170808705V2CSCV
   Tang X., 2019, APPL BIDIRECTIONAL R
   Tariyal S, 2016, IEEE ACCESS, V4, P10096, DOI 10.1109/ACCESS.2016.2611583
   Vincent P, 2010, J MACH LEARN RES, V11, P3371
   Wang HS, 2018, IEEE T CIRC SYST VID, V28, P2908, DOI 10.1109/TCSVT.2017.2746092
   Wang R, 2018, CMC-COMPUT MATER CON, V57, P25, DOI 10.32604/cmc.2018.02408
   Wen Bihan, 2019, IEEE T IMAGE PROCESS
   Yankelevsky Y, 2016, COMPUTER VISION PATT
   Zayyani H., 2016, IEEE SIGNAL PROCESS
   Zhang Z, 2018, IEEE T NEUR NET LEAR, V29, P3798, DOI 10.1109/TNNLS.2017.2740224
   Zhang ZX, 2018, IEEE T GEOSCI REMOTE, V56, P524, DOI 10.1109/TGRS.2017.2751061
   Zheng H, 2017, INT J MACH LEARN CYB, V8, P2043, DOI 10.1007/s13042-017-0684-6
   Zhu X., 2019, VISUAL COMPUT
NR 41
TC 8
Z9 8
U1 3
U2 25
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2021
VL 37
IS 4
SI SI
BP 707
EP 715
DI 10.1007/s00371-020-01970-x
EA OCT 2020
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RL0LI
UT WOS:000575725100002
DA 2024-07-18
ER

PT J
AU Singh, R
   Goel, A
   Raghuvanshi, DK
AF Singh, Rahul
   Goel, Aditya
   Raghuvanshi, D. K.
TI Computer-aided diagnostic network for brain tumor classification
   employing modulated Gabor filter banks
SO VISUAL COMPUTER
LA English
DT Article
DE Gabor orientation filters; Convolutional neural networks (CNNs);
   Leave-one-patient-out (LOPO); Deep learning; Transfer learning
ID SPARSE; REPRESENTATION; SEGMENTATION; DICTIONARY
AB MR brain tumor classification is one of the extensively utilized approaches in medical prognosis. However, analyzing and processing MR brain images is still quite a task for radiologists. To encounter this problem, the evaluation of existing canonical techniques has already been done. There are numeral MR brain tumor classification approaches that are being used for medical diagnosis. In this paper, we have developed an automated computer-aided network for diagnosis of MR brain tumor class, i.e., HGG and LGG. We have proffered a Gabor-modulated convolutional filter-based classifier for brain tumor classification. The inclusion of Gabor filter dynamics endows the competency to deal with spatial and orientational transformations. This mere modification (modulation) of conventional convolutional filters by Gabor filters empowers the proposed architecture to learn relatively smaller feature maps and thereby, decreasing network parameter requirement. We have introduced some skip connections to our modulated CNN architecture without introducing an extra network parameter. Pre-trained networks, i.e., Alex-Net, Google-Net (Inception V1), Res-Net and VGG 19 have been considered for performance evaluation of our proposed Gabor-modulated CNN. Additionally, some popular machine learning classification techniques have also been considered for comparative analysis. Experimental findings demonstrate that our proposed network has limited network parameters to learn; therefore, it is quite easy to train such networks.
C1 [Singh, Rahul; Goel, Aditya; Raghuvanshi, D. K.] Natl Inst Technol, Dept Elect & Commun, Bhopal, MP, India.
C3 National Institute of Technology (NIT System); Maulana Azad National
   Institute of Technology Bhopal
RP Singh, R (corresponding author), Natl Inst Technol, Dept Elect & Commun, Bhopal, MP, India.
EM rhlsingh720@gmail.com; adityagoel2@rediffmail.com;
   dkraghuvanshi3@gmail.com
RI Goel, Aditya/J-6509-2013; Singh, Rahul/ABD-3249-2020
OI Goel, Aditya/0000-0002-3469-9273; Singh, Rahul/0000-0001-5350-5588
CR Ames William F, 2014, NUMERICAL METHODS PA
   [Anonymous], 2018, TUM TYP UND BRAIN TU
   [Anonymous], 2013, P NCI MICCAI BRATS
   [Anonymous], 2015, IJIGSP
   Bakas S., 2018, IDENTIFYING BEST MAC
   Bakas S, 2017, SCI DATA, V4, DOI 10.1038/sdata.2017.117
   Barcelos CAZ, 2009, APPL MATH COMPUT, V215, P251, DOI 10.1016/j.amc.2009.04.081
   Cabezas M., 2018, ARXIVPREPRINTARXIV18
   Caver E., 2018, MICCAI BRATS 2018, V63, P63
   DAUGMAN JG, 1988, IEEE T ACOUST SPEECH, V36, P1169, DOI 10.1109/29.1644
   Elad M, 2006, IEEE T IMAGE PROCESS, V15, P3736, DOI 10.1109/TIP.2006.881969
   Festa J., 2013, Proceedings of NCIMICCAI Brats, V1, P23
   Girshick R., 2014, PROC CVPR IEEE, DOI [DOI 10.1109/CVPR.2014.81, 10.1109/CVPR.2014.81]
   GUPTA T, 2018, PATTERN RECOGN LETT
   Hang XY, 2009, J BIOMED BIOTECHNOL, DOI 10.1155/2009/403689
   He K., 2016, PROC CVPR IEEE, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]
   Hsieh KLC, 2017, COMPUT METH PROG BIO, V139, P31, DOI 10.1016/j.cmpb.2016.10.021
   Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59
   Jiang ZL, 2011, PROC CVPR IEEE, P1697, DOI 10.1109/CVPR.2011.5995354
   Jones TL, 2015, NEURO-ONCOLOGY, V17, P466, DOI 10.1093/neuonc/nou159
   Kingma D.P., 2018, ARXIVPREPRINTARXIV14
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lee TS, 1996, IEEE T PATTERN ANAL, V18, P959, DOI 10.1109/34.541406
   Li XH, 2014, IEEE T GEOSCI REMOTE, V52, P7086, DOI 10.1109/TGRS.2014.2307354
   Li YH, 2016, ARTIF INTELL MED, V73, P1, DOI 10.1016/j.artmed.2016.08.004
   Li YH, 2015, 2015 INTERNATIONAL WORKSHOP ON PATTERN RECOGNITION IN NEUROIMAGING (PRNI) 2015, P41, DOI 10.1109/PRNI.2015.18
   Ouyang WL, 2013, IEEE I CONF COMP VIS, P2056, DOI 10.1109/ICCV.2013.257
   Ravishankar S, 2011, IEEE T MED IMAGING, V30, P1028, DOI 10.1109/TMI.2010.2090538
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Rusk N, 2016, NAT METHODS, V13, P35, DOI 10.1038/nmeth.3707
   Sarwar SS, 2017, I SYMPOS LOW POWER E
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Soltaninejad M, 2017, INT J COMPUT ASS RAD, V12, P183, DOI 10.1007/s11548-016-1483-3
   Szegedy C, 2014, Arxiv, DOI [arXiv:1312.6199, DOI 10.1109/CVPR.2015.7298594]
   Wang L, 2014, NEUROIMAGE, V84, P141, DOI 10.1016/j.neuroimage.2013.08.008
   Wright J, 2010, P IEEE, V98, P1031, DOI 10.1109/JPROC.2010.2044470
   Yang G, 2016, MAGN RESON MED, V75, P2505, DOI 10.1002/mrm.25845
   Zacharaki EI, 2009, MAGN RESON MED, V62, P1609, DOI 10.1002/mrm.22147
   Zhao L, 2018, 2018 2ND INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION SCIENCES (ICRAS), P51, DOI 10.1109/ICRAS.2018.8442365
   Zhou YZ, 2017, PROC CVPR IEEE, P4961, DOI 10.1109/CVPR.2017.527
NR 40
TC 30
Z9 30
U1 0
U2 13
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2021
VL 37
IS 8
BP 2157
EP 2171
DI 10.1007/s00371-020-01977-4
EA SEP 2020
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TT5FX
UT WOS:000572614700001
DA 2024-07-18
ER

PT J
AU Chen, LG
   Fu, G
AF Chen, Linggang
   Fu, Gang
TI Structure-preserving image smoothing with semantic cues
SO VISUAL COMPUTER
LA English
DT Article
DE Structure-preserving smoothing; Texture; Median filtering
AB The purpose of image smoothing is to smooth out low-contrast textures while preserving meaningful structures. Although this problem has been studied for decades, it still leaves a lot of space to improve. Recently, learning-based edge detectors have superior performance to traditional manually-designed detectors. Based on the edge detection technique, we present a novel optimization-based image smoothing model combining semantic prior and perform L-0 gradient minimization recursively in our framework to refine the result. Our framework combines the advantage of the state-of-the-art edge detector and the ability of L-0 gradient minimization for structure-preserving image smoothing. Moreover, we employ a large number of real-world images and perform various experiments to evaluate our algorithm. Experimental results show that our algorithm outperforms state-of-the-art algorithms, especially in extracting subjectively-meaningful structures.
C1 [Chen, Linggang] Yunzhangfang Network Technol Co Ltd, Nanjing 210000, Peoples R China.
   [Fu, Gang] Wuhan Univ, Sch Comp Sci, Wuhan 430072, Peoples R China.
C3 Wuhan University
RP Fu, G (corresponding author), Wuhan Univ, Sch Comp Sci, Wuhan 430072, Peoples R China.
EM chlg3321@gmail.com; xyzgfu@gmail.com
CR Baek J, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866191
   Barron JT, 2015, IEEE T PATTERN ANAL, V37, P1670, DOI 10.1109/TPAMI.2014.2377712
   Bell Sean, 2014, ACM Transactions on Graphics, V33, DOI 10.1145/2601097.2601206
   Bi S, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766946
   Bonneel N, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2661229.2661253
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851
   Chai LG, 2017, IEEE INT C INTELL TR
   Dollár P, 2015, IEEE T PATTERN ANAL, V37, P1558, DOI 10.1109/TPAMI.2014.2377715
   Duda R., 1973, Pattern Classification and Scene Analysis
   Farbman Z, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360666
   Fattal R, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1477926.1477933
   Fu XY, 2016, PROC CVPR IEEE, P2782, DOI 10.1109/CVPR.2016.304
   Gastal ESL, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964964
   Guo X., 2016, P 24 ACM INT C MULT, P87
   He KM, 2013, IEEE T PATTERN ANAL, V35, P1397, DOI 10.1109/TPAMI.2012.213
   He L, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461965
   Jeon J, 2016, COMPUT GRAPH FORUM, V35, P77, DOI 10.1111/cgf.13005
   Jianbing Shen, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3481, DOI 10.1109/CVPR.2011.5995507
   Jung C, 2017, J VIS COMMUN IMAGE R, V42, P132, DOI 10.1016/j.jvcir.2016.11.009
   Kass M, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778837
   Kim Y, 2015, IEEE IMAGE PROC, P1404, DOI 10.1109/ICIP.2015.7351031
   Li LH, 2020, IEEE ACM T COMPUT BI, V17, P599, DOI 10.1109/TCBB.2018.2868078
   Li Y, 2014, PROC CVPR IEEE, P280, DOI 10.1109/CVPR.2014.43
   Li ZQ, 2018, PROC CVPR IEEE, P2041, DOI 10.1109/CVPR.2018.00218
   Liang ZF, 2018, PROC CVPR IEEE, P2811, DOI 10.1109/CVPR.2018.00297
   Liu W, 2017, IEEE T IMAGE PROCESS, V26, DOI 10.1109/TIP.2016.2612826
   Liu Y, 2019, IEEE T PATTERN ANAL, V41, P1939, DOI 10.1109/TPAMI.2018.2878849
   Margolin R, 2014, PROC CVPR IEEE, P248, DOI 10.1109/CVPR.2014.39
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Meka A, 2017, IEEE T VIS COMPUT GR, V23, P2447, DOI 10.1109/TVCG.2017.2734425
   Meka A, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925907
   Min DB, 2014, IEEE T IMAGE PROCESS, V23, P5638, DOI 10.1109/TIP.2014.2366600
   Perazzi F, 2012, PROC CVPR IEEE, P733, DOI 10.1109/CVPR.2012.6247743
   Rother C., 2011, NIPS
   Shen JB, 2013, IEEE T CYBERNETICS, V43, P425, DOI 10.1109/TSMCB.2012.2208744
   Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54
   Subr K, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618493
   Sun YJ, 2018, IEEE T VIS COMPUT GR, V24, P2129, DOI 10.1109/TVCG.2017.2711614
   Sun YJ, 2015, COMPUT AIDED GEOM D, V35-36, P2, DOI 10.1016/j.cagd.2015.03.011
   Tomasi C, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P839, DOI 10.1109/ICCV.1998.710815
   Wang Q, 2018, SPRINGERBR APPL SCI, P1, DOI 10.1007/978-3-319-73549-8
   Wei X, 2018, INT J COMPUT VISION, V126, P1245, DOI 10.1007/s11263-018-1091-5
   Wu CL, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2661229.2661232
   Xie SN, 2015, IEEE I CONF COMP VIS, P1395, DOI [10.1109/ICCV.2015.164, 10.1007/s11263-017-1004-z]
   Xu L, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024208
   Yan Q, 2013, PROC CVPR IEEE, P1155, DOI 10.1109/CVPR.2013.153
   Yang Q, 2016, CVPR
   Yang Q., 2012, ECCV
   Zhang F., 2015, ICCV
   Zhang JM, 2015, IEEE I CONF COMP VIS, P1404, DOI 10.1109/ICCV.2015.165
   Zhang Q., 2018, ACM INT C MULT
   Zhang Q, 2014, LECT NOTES COMPUT SC, V8691, P815, DOI 10.1007/978-3-319-10578-9_53
   Zhao Q, 2012, IEEE T PATTERN ANAL, V34, P1437, DOI 10.1109/TPAMI.2012.77
   Zhu LF, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366146
NR 54
TC 6
Z9 7
U1 1
U2 8
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2020
VL 36
IS 10-12
BP 2017
EP 2027
DI 10.1007/s00371-020-01950-1
EA SEP 2020
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA NW1CX
UT WOS:000568148800001
DA 2024-07-18
ER

PT J
AU Morel, J
   Bac, A
   Kanai, T
AF Morel, Jules
   Bac, Alexandra
   Kanai, Takashi
TI Segmentation of unbalanced and in-homogeneous point clouds and its
   application to 3D scanned trees
SO VISUAL COMPUTER
LA English
DT Article
DE Point cloud segmentation; Terrestrial LiDAR; Unbalanced data-set; Deep
   learning
ID WOOD-LEAF SEPARATION; TERRESTRIAL; RECONSTRUCTION; MODELS
AB Segmentation of 3D point clouds is still an open issue in the case of unbalanced and in-homogeneous data-sets. In the application context of the modeling of botanical trees, a fundamental challenge consists in separating the leaves from the wood. Based on deep learning and a class decision process, we propose an innovative method designed to separate leaf points from wood points in terrestrial LiDAR point clouds of trees. Although simple, our approach learns trees characteristic point patterns efficiently and robustly. To train our 3D deep learning model, we constructed a 3D labeled point cloud data-set of different tree species. Experiments show that our 3D deep representation together with our geometric approach leads to significant improvement over the state-of-the-art methods in segmentation task.
C1 [Morel, Jules; Kanai, Takashi] Univ Tokyo, Grad Sch Arts & Sci, Kanai Lab, Tokyo, Japan.
   [Bac, Alexandra] Aix Marseille Univ, Lab Informat & Syst, Marseille, France.
C3 University of Tokyo; Aix-Marseille Universite
RP Morel, J (corresponding author), Univ Tokyo, Grad Sch Arts & Sci, Kanai Lab, Tokyo, Japan.
EM jules.morel@ifpindia.org
OI Morel, Jules/0000-0002-0164-5834; Kanai, Takashi/0000-0002-1635-3818
FU JSPS KAK-ENHI from the Japan Society for the Promotion of Science (JSPS)
   [JP19K11990]; Japan Society for the Promotion of Science (JSPS) [P18796]
FX The study was supported by Grant JSPS KAK-ENHI, Grant Number JP19K11990,
   from the Japan Society for the Promotion of Science (JSPS) funds. Jules
   Morel was supported by Grant P18796 from from the Japan Society for the
   Promotion of Science (JSPS). The authorswould like to thank
   NicolasBarbier atUMRAMAP (Botanique et Modelisation de l'architecture
   des plantes et des vegetations, France) and S. Momo Takoudjou (IRD-AMAP,
   ENS-UY1) for providing test data. Those data from Cameroon were
   collected in collaborationwith Alpicam companywithin the IRD project PPR
   FTH-AC Changement globaux, biodiversite et sante en zone forestiere
   d'Afrique centrale.
CR Nguyen A, 2013, PROCEEDINGS OF THE 2013 6TH IEEE CONFERENCE ON ROBOTICS, AUTOMATION AND MECHATRONICS (RAM), P225, DOI 10.1109/RAM.2013.6758588
   [Anonymous], 2016, ARXIV161104500
   [Anonymous], 2015, PROC CVPR IEEE, DOI [DOI 10.1109/CVPR.2015.7298801, 10.1109/CVPR.2015.7298801]
   [Anonymous], 2009, THESIS
   BALLARD DH, 1981, PATTERN RECOGN, V13, P111, DOI 10.1016/0031-3203(81)90009-1
   Béland M, 2014, AGR FOREST METEOROL, V184, P82, DOI 10.1016/j.agrformet.2013.09.005
   Bennett ND, 2013, ENVIRON MODELL SOFTW, V40, P1, DOI 10.1016/j.envsoft.2012.09.011
   Bhanu B., 1986, Eighth International Conference on Pattern Recognition. Proceedings (Cat. No.86CH2342-4), P236
   Biasotti S., GEN DISCRETE CONVULT
   Breiman L., 2001, Machine Learning, V45, P5, DOI 10.1023/A:1010933404324
   Briechle S., 2019, International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences, VXLII-2/W13, P951, DOI [10.5194/isprs-archives-XLII-2-W13-951-2019, DOI 10.5194/ISPRS-ARCHIVES-XLII-2-W13-951-2019]
   Burt A, 2019, METHODS ECOL EVOL, V10, P438, DOI 10.1111/2041-210X.13121
   Chen J, 2008, INT J COMPUT VISION, V78, P223, DOI 10.1007/s11263-007-0105-5
   Congalton R.G., 2008, ASSESSING ACCURACY R
   Côté JF, 2009, REMOTE SENS ENVIRON, V113, P1067, DOI 10.1016/j.rse.2009.01.017
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Dassot M, 2011, ANN FOREST SCI, V68, P959, DOI 10.1007/s13595-011-0102-2
   Ferrara R, 2018, AGR FOREST METEOROL, V262, P434, DOI 10.1016/j.agrformet.2018.04.008
   Filin S, 2006, ISPRS J PHOTOGRAMM, V60, P71, DOI 10.1016/j.isprsjprs.2005.10.005
   Filin S., 2002, International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences, VXXXIV, P119
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Golovinskiy Aleksey, 2009, 2009 IEEE 12th International Conference on Computer Vision Workshops, ICCV Workshops, P39, DOI 10.1109/ICCVW.2009.5457721
   Golovinskiy A, 2009, IEEE I CONF COMP VIS, P2154, DOI 10.1109/ICCV.2009.5459471
   Hackenberg J, 2015, FORESTS, V6, P4245, DOI 10.3390/f6114245
   He HB, 2009, IEEE T KNOWL DATA EN, V21, P1263, DOI 10.1109/TKDE.2008.239
   Heinzel J, 2018, REMOTE SENS-BASEL, V10, DOI 10.3390/rs10071056
   Klasing K, 2009, IEEE INT CONF ROBOT, P2011
   Klokov R, 2017, IEEE I CONF COMP VIS, P863, DOI 10.1109/ICCV.2017.99
   Kriegel H.-P., 1996, KNOWLEDGE DISCOVERY, P226, DOI DOI 10.5555/3001460
   Landrieu L, 2018, PROC CVPR IEEE, P4558, DOI 10.1109/CVPR.2018.00479
   Lang AH, 2019, PROC CVPR IEEE, P12689, DOI 10.1109/CVPR.2019.01298
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lefsky MA, 2002, BIOSCIENCE, V52, P19, DOI 10.1641/0006-3568(2002)052[0019:LRSFES]2.0.CO;2
   Li YY, 2018, ADV NEUR IN, V31
   MATTHEWS BW, 1975, BIOCHIM BIOPHYS ACTA, V405, P442, DOI 10.1016/0005-2795(75)90109-9
   Maturana D, 2015, IEEE INT C INT ROBOT, P922, DOI 10.1109/IROS.2015.7353481
   MOMOTAKOUDJOU S, 2018, METHODS ECOL EVOL, V9, P905
   Morel J, 2018, COMPUT GRAPH-UK, V74, P44, DOI 10.1016/j.cag.2018.05.004
   Niemeyer J, 2013, 2013 JOINT URBAN REMOTE SENSING EVENT (JURSE), P139, DOI 10.1109/JURSE#.2013.6550685
   Ning X., 2009, P 8 INT C VIRTUAL RE, P127, DOI DOI 10.1145/1670252.1670280
   Olagoke A, 2016, TREES-STRUCT FUNCT, V30, P935, DOI 10.1007/s00468-015-1334-9
   Pauly M, 2002, VIS 2002: IEEE VISUALIZATION 2002, PROCEEDINGS, P163, DOI 10.1109/VISUAL.2002.1183771
   Pedregosa F, 2011, J MACH LEARN RES, V12, P2825
   Pfennigbauer M, 2010, PROC SPIE, V7684, DOI 10.1117/12.849641
   Pharr M., 2016, Physically Based Rendering: From Theory to Implementation, V3rd ed.
   Pu S, 2006, INT ARCH PHOTOGRAMME, V36, P5
   Qi CR, 2017, ADV NEUR IN, V30
   Qi CR, 2018, PROC CVPR IEEE, P918, DOI 10.1109/CVPR.2018.00102
   Qi CR, 2016, PROC CVPR IEEE, P5648, DOI 10.1109/CVPR.2016.609
   Raumonen P, 2013, REMOTE SENS-BASEL, V5, P491, DOI 10.3390/rs5020491
   Ravaglia J., 2015, SILVILASER 2015
   Ravaglia J, 2017, COMPUT GRAPH-UK, V66, P23, DOI 10.1016/j.cag.2017.05.016
   Riegler G, 2017, PROC CVPR IEEE, P6620, DOI 10.1109/CVPR.2017.701
   Rusu RB, 2009, 2009 IEEE-RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, P7, DOI 10.1109/IROS.2009.5354763
   Rusu RB, 2009, IEEE INT CONF ROBOT, P1848
   Sappa AD, 2001, THIRD INTERNATIONAL CONFERENCE ON 3-D DIGITAL IMAGING AND MODELING, PROCEEDINGS, P292, DOI 10.1109/IM.2001.924460
   Shao J, 2020, ISPRS J PHOTOGRAMM, V163, P214, DOI 10.1016/j.isprsjprs.2020.03.008
   Shi SS, 2019, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2019.00086
   Su H, 2018, PROC CVPR IEEE, P2530, DOI 10.1109/CVPR.2018.00268
   Su H, 2015, IEEE I CONF COMP VIS, P945, DOI 10.1109/ICCV.2015.114
   Suykens JAK, 1999, NEURAL PROCESS LETT, V9, P293, DOI 10.1023/A:1018628609742
   Tao SL, 2015, ISPRS J PHOTOGRAMM, V110, P66, DOI 10.1016/j.isprsjprs.2015.10.007
   Tao SL, 2015, PHOTOGRAMM ENG REM S, V81, P767, DOI 10.14358/PERS.81.10.767
   Wang C, 2018, LECT NOTES COMPUT SC, V11208, P56, DOI 10.1007/978-3-030-01225-0_4
   Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362
   Wen ZY, 2018, J MACH LEARN RES, V19
   Wijmans E., 2018, Pointnet++ Pytorch
   Xu Y, 2018, ADV SOC SCI EDUC HUM, V284, P87
   Yu XW, 2011, ISPRS J PHOTOGRAMM, V66, P28, DOI 10.1016/j.isprsjprs.2010.08.003
   Zhang JX, 2013, REMOTE SENS-BASEL, V5, P3749, DOI 10.3390/rs5083749
   Zhang WM, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11020211
   Zhou Y, 2018, IEEE CONF COMPUT
NR 72
TC 15
Z9 15
U1 3
U2 32
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2020
VL 36
IS 10-12
BP 2419
EP 2431
DI 10.1007/s00371-020-01966-7
EA SEP 2020
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA NW1CX
UT WOS:000566900900001
DA 2024-07-18
ER

PT J
AU Wang, XK
   Liu, SN
   Ban, XJ
   Xu, YR
   Zhou, J
   Kosinka, J
AF Wang, Xiaokun
   Liu, Sinuo
   Ban, Xiaojuan
   Xu, Yanrui
   Zhou, Jing
   Kosinka, Jiri
TI Robust turbulence simulation for particle-based fluids using the Rankine
   vortex model
SO VISUAL COMPUTER
LA English
DT Article
DE Fluid simulation; Vortex model; Turbulence; Smoothed particle
   hydrodynamics
ID SPH; HYDRODYNAMICS; DETAILS
AB We propose a novel turbulence refinement method based on the Rankine vortex model for smoothed particle hydrodynamics (SPH) simulations. Surface details are enhanced by recovering the energy lost due to the lack of the rotation of SPH particles. The Rankine vortex model is used to convert the diffused and stretched angular kinetic energy of particles to the linear kinetic energy of their neighbors. In previous vorticity-based refinement methods, adding more energy than required by the viscous damping effect leads to instability. In contrast, our model naturally prevents the positive feedback effect between the velocity and vorticity fields since the vortex model is designed to alter the velocity without introducing external sources. Experimental results show that our method can recover missing high-frequency details realistically and maintain convergence in both static and highly dynamic scenarios.
C1 [Wang, Xiaokun; Liu, Sinuo; Ban, Xiaojuan; Xu, Yanrui; Zhou, Jing] Univ Sci & Technol Beijing, Sch Comp & Commun Engn, Beijing, Peoples R China.
   [Kosinka, Jiri] Univ Groningen, Bernoulli Inst, Groningen, Netherlands.
C3 University of Science & Technology Beijing; University of Groningen
RP Ban, XJ (corresponding author), Univ Sci & Technol Beijing, Sch Comp & Commun Engn, Beijing, Peoples R China.
EM banxj@ustb.edu.cn; j.kosinka@rug.nl
RI WANG, XIAOKUN/GQR-1904-2022; Liu, Sinuo/JLM-6911-2023; Xu,
   Yanrui/KHU-2854-2024
OI Xu, Yanrui/0000-0002-2154-1178; Kosinka, Jiri/0000-0002-8859-2586
FU National Natural Science Foundation of China [61702036, 61873299];
   National Key Research and Development Program of China [2016YFB1001404];
   Finance Science and Technology Project of Hainan Province [ZDYF
   2019009]; Fundamental Research Funds for the Central Universities of
   China [FRF-TP-19-043A2]
FX This research was supported by the National Natural Science Foundation
   of China (61702036, 61873299), the National Key Research and Development
   Program of China (2016YFB1001404), the Finance Science and Technology
   Project of Hainan Province (ZDYF 2019009) and the Fundamental Research
   Funds for the Central Universities of China (FRF-TP-19-043A2).
CR Akinci N, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185558
   [Anonymous], 2005, Proceedings of the 2005 ACM SIGGRAPH/Eurographics Symposium on Computer Animation, DOI DOI 10.1145/1073368.1073380
   Becker M, 2007, SYMPOSIUM ON COMPUTER ANIMATION 2007: ACM SIGGRAPH/ EUROGRAPHICS SYMPOSIUM PROCEEDINGS, P209
   Bender J., 2015, P 14 ACM SIGGRAPH EU, P147, DOI DOI 10.1145/2786784.2786796
   Bender J, 2019, IEEE T VIS COMPUT GR, V25, P2284, DOI 10.1109/TVCG.2018.2832080
   Chu MY, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073643
   Clancy LJ., 1975, AERODYNAMICS
   Colagrossi A, 2014, MATH MECH COMPLEX SY, V2, P45, DOI 10.2140/memocs.2014.2.45
   Corentin Wallez, 2015, ACM T GRAPHIC, V34, P1
   Eberhardt S, 2017, ACM SIGGRAPH / EUROGRAPHICS SYMPOSIUM ON COMPUTER ANIMATION (SCA 2017), DOI 10.1145/3099564.3099569
   Edwards E, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601167
   Fedkiw R, 2001, COMP GRAPH, P15, DOI 10.1145/383259.383260
   Foster N, 1996, GRAPH MODEL IM PROC, V58, P471, DOI 10.1006/gmip.1996.0039
   GINGOLD RA, 1977, MON NOT R ASTRON SOC, V181, P375, DOI 10.1093/mnras/181.3.375
   Holm DD, 1999, PHYSICA D, V133, P215, DOI 10.1016/S0167-2789(99)00093-7
   Ihmsen M., 2014, Eurographics 2014-State of the Art Reports
   Ihmsen M, 2014, IEEE T VIS COMPUT GR, V20, P426, DOI 10.1109/TVCG.2013.105
   Ihmsen M, 2011, COMPUT GRAPH FORUM, V30, P99, DOI [10.1111/j.1467-8659.2010.01832.x, 10.1111/j.1467-8659.2010.01834.x]
   Kaufmann W., 1962, Ing. Arch, V1962, P1, DOI DOI 10.1007/BF00538235
   Kim T, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2451236.2451241
   Kim T, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360649
   Koschier Dan., 2019, Eurographics 2019-Tutorials, DOI DOI 10.2312/EGT.20191035
   Lamb H., 1924, Hydrodynamics
   Lentine Michael., 2011, Proceedings of the 2011 ACM SIGGRAPH/Eurographics Symposium on Computer Animation, SCA '11, P91, DOI [10.1145/2019406.2019419, DOI 10.1145/2019406.2019419]
   Liu MB, 2010, ARCH COMPUT METHOD E, V17, P25, DOI 10.1007/s11831-010-9040-7
   Liu S, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR 2020), P842, DOI [10.1109/VR46266.2020.1581218089381, 10.1109/VR46266.2020.00011]
   Lucy L. B., 1977, Astronomical Journal, V82, P1013, DOI 10.1086/112164
   Mercier O, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818115
   MONAGHAN JJ, 1992, ANNU REV ASTRON ASTR, V30, P543, DOI 10.1146/annurev.aa.30.090192.002551
   Muller M., 2003, ACM SIGGRAPH/Eurographics Symposium on Computer Animation, P154
   Narain R, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409119
   Oseen C.W., 1912, Arkiv for Matematik, Astronomi och Fysik, V7, P14
   Park S. I., 2005, Computer Animation, Conference Proceedings, P261, DOI [DOI 10.1145/1073368.1073406, 10.1145/1073368.1073406]
   Pfaff T, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185608
   Rankine W.J. M., 1876, MANUAL APPL MECH
   Sato S, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201398
   Shao XQ, 2015, COMPUT ANIMAT VIRT W, V26, P79, DOI 10.1002/cav.1607
   Solenthaler B, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531346
   Stam J, 1999, COMP GRAPH, P121, DOI 10.1145/311535.311548
   STEINHOFF J, 1994, PHYS FLUIDS, V6, P2738, DOI 10.1063/1.868164
   Stock MJ, 2008, J COMPUT PHYS, V227, P9021, DOI 10.1016/j.jcp.2008.05.022
   Thürey N, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778785
   Thuerey Nils., 2013, ACM SIGGRAPH 2013 Courses, SIGGRAPH'13, p6:1, DOI DOI 10.1145/2504435.2504441
   Vatistas GH, 1998, J PROPUL POWER, V14, P462, DOI 10.2514/2.5323
   VATISTAS GH, 1991, EXP FLUIDS, V11, P73, DOI 10.1007/BF00198434
   Wang XK, 2019, SA'19: SIGGRAPH ASIA 2019 TECHNICAL BRIEFS, P95, DOI 10.1145/3355088.3365145
   Wang XK, 2018, J VISUAL LANG COMPUT, V48, P91, DOI 10.1016/j.jvlc.2018.07.005
   Wang XK, 2016, J VISUAL-JAPAN, V19, P301, DOI 10.1007/s12650-015-0317-7
   Weissmann S., 2010, ACM SIGGRAPH 2010 papers on-SIGGRAPH '10, P1, DOI DOI 10.1145/1833349.1778852
   Weissmann S, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601171
   Wojtan C., 2011, ACM SIGGRAPH 2011 CO, P8, DOI [10.1145/2037636.2037644, DOI 10.1145/2037636.2037644]
   Zhang XX, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766982
   Zhang YL, 2020, EXPERT SYST APPL, V152, DOI 10.1016/j.eswa.2020.113410
   Zhu B, 2010, COMPUT GRAPH FORUM, V29, P2207, DOI 10.1111/j.1467-8659.2010.01809.x
NR 54
TC 3
Z9 5
U1 1
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2020
VL 36
IS 10-12
BP 2285
EP 2298
DI 10.1007/s00371-020-01914-5
EA AUG 2020
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA NW1CX
UT WOS:000555755400002
DA 2024-07-18
ER

PT J
AU Mbelwa, JT
   Zhao, QJ
   Wang, FS
AF Mbelwa, Jimmy T.
   Zhao, Qingjie
   Wang, Fasheng
TI Visual tracking tracker via object proposals and co-trained kernelized
   correlation filters
SO VISUAL COMPUTER
LA English
DT Article
DE Kernelized correlation filters; Correlation filter; Visual tracking;
   Object proposals
AB Visual tracking is a challenging task in the field of computer vision with wide applications in intelligent and surveillance systems. Recently, correlation trackers have shown great achievement in visual tracking due to its high efficiency. However, such trackers have a problem of handling fast motion, motion blur, illumination variations, background clutter and drifting away caused by occlusion and thus may result in tracking failure. To solve this problem, we propose a tracker that is based on the object proposals and co-kernelized correlation filters (Co-KCF). The proposed tracker utilizes both object proposals and global prediction estimated by kernelized correlation filter scheme to obtain best proposals as prior information using spatial weight strategy in order to improve tracking performance of fast motion and motion blur. Since single kernel may lead to background clutter and drifting problem, Co-KCF has been employed to combat this defect and predict a new state of a target object. Extensive experiments demonstrate that our proposed tracker outperforms other existing state-of-the-art trackers.
C1 [Mbelwa, Jimmy T.] Univ Dar Es Salaam, Dept Comp Sci & Engn, POB 33335, Dar Es Salaam, Tanzania.
   [Mbelwa, Jimmy T.; Zhao, Qingjie] Beijing Inst Technol, Sch Comp Sci, Beijing Lab Intelligent Informat Technol, Beijing 100081, Peoples R China.
   [Wang, Fasheng] Dalian Minzu Univ, Sch Informat & Commun Engn, Dalian 116600, Peoples R China.
C3 University of Dar es Salaam; Beijing Institute of Technology; Dalian
   Minzu University
RP Mbelwa, JT (corresponding author), Univ Dar Es Salaam, Dept Comp Sci & Engn, POB 33335, Dar Es Salaam, Tanzania.; Mbelwa, JT (corresponding author), Beijing Inst Technol, Sch Comp Sci, Beijing Lab Intelligent Informat Technol, Beijing 100081, Peoples R China.
EM jmbelwa@yahoo.co.uk; zhaoqj@bit.edu.cn; fswang@dlut.edu.cn
FU University of Dar es Salaam; National Natural Science Foundation of
   China [61175096]; NSFC [61300082]; Liaoning BaiQianWan Talents program,
   Dalian Youth Scholar Foundation [2017RQ151]
FX This work was supported by the University of Dar es Salaam, the National
   Natural Science Foundation of China (Grant No. 61175096, the NSFC No.
   61300082), and the Liaoning BaiQianWan Talents program, Dalian Youth
   Scholar Foundation (No. 2017RQ151).
CR Akin O, 2016, J VIS COMMUN IMAGE R, V38, P763, DOI 10.1016/j.jvcir.2016.04.018
   [Anonymous], 2015, PROC CVPR IEEE
   Bertinetto L, 2016, PROC CVPR IEEE, P1401, DOI 10.1109/CVPR.2016.156
   Bertinetto L, 2016, LECT NOTES COMPUT SC, V9914, P850, DOI 10.1007/978-3-319-48881-3_56
   Bolme DS, 2010, PROC CVPR IEEE, P2544, DOI 10.1109/CVPR.2010.5539960
   Dalal N., 2005, PROC IEEE COMPUT SOC, V1, P886
   Danelljan M, 2016, LECT NOTES COMPUT SC, V9909, P472, DOI 10.1007/978-3-319-46454-1_29
   Danelljan M, 2014, PROC CVPR IEEE, P1090, DOI 10.1109/CVPR.2014.143
   Fan H, 2017, AAAI CONF ARTIF INTE, P4025
   Fan H, 2017, IEEE I CONF COMP VIS, P5487, DOI 10.1109/ICCV.2017.585
   Fang ZW, 2016, IEEE T IMAGE PROCESS, V25, P4116, DOI 10.1109/TIP.2016.2579311
   Felzenszwalb PF, 2004, INT J COMPUT VISION, V59, P167, DOI 10.1023/B:VISI.0000022288.19776.77
   Gao J, 2014, LECT NOTES COMPUT SC, V8691, P188, DOI 10.1007/978-3-319-10578-9_13
   Henriques JF, 2015, IEEE T PATTERN ANAL, V37, P583, DOI 10.1109/TPAMI.2014.2345390
   Hua Y, 2015, IEEE I CONF COMP VIS, P3092, DOI 10.1109/ICCV.2015.354
   Jia X, 2012, PROC CVPR IEEE, P1822, DOI 10.1109/CVPR.2012.6247880
   Kalal Z, 2012, IEEE T PATTERN ANAL, V34, P1409, DOI 10.1109/TPAMI.2011.239
   Kristan M, 2016, IEEE T PATTERN ANAL, V38, P2137, DOI 10.1109/TPAMI.2016.2516982
   Kwon J, 2013, IEEE T PATTERN ANAL, V35, P2427, DOI 10.1109/TPAMI.2013.32
   Li Y, 2015, PROC CVPR IEEE, P353, DOI 10.1109/CVPR.2015.7298632
   Li ZY, 2015, VISUAL COMPUT, V31, P1319, DOI 10.1007/s00371-014-1014-6
   Liu RS, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0168093
   Lukezic A, 2017, PROC CVPR IEEE, P4847, DOI 10.1109/CVPR.2017.515
   Ma C, 2015, IEEE I CONF COMP VIS, P3074, DOI 10.1109/ICCV.2015.352
   Mei X, 2009, IEEE I CONF COMP VIS, P1436, DOI 10.1109/ICCV.2009.5459292
   Nai K, 2018, IEEE T IMAGE PROCESS, V27, P4958, DOI 10.1109/TIP.2018.2848465
   Smeulders AWM, 2014, IEEE T PATTERN ANAL, V36, P1442, DOI 10.1109/TPAMI.2013.230
   Su YY, 2014, PATTERN RECOGN, V47, P1826, DOI 10.1016/j.patcog.2013.11.028
   Tang M, 2015, IEEE I CONF COMP VIS, P3038, DOI 10.1109/ICCV.2015.348
   Uijlings JRR, 2013, INT J COMPUT VISION, V104, P154, DOI 10.1007/s11263-013-0620-5
   Valmadre J, 2017, PROC CVPR IEEE, P5000, DOI 10.1109/CVPR.2017.531
   Wang D, 2013, PROC CVPR IEEE, P2371, DOI 10.1109/CVPR.2013.307
   Wang FS, 2018, COMPUT GRAPH-UK, V70, P214, DOI 10.1016/j.cag.2017.07.023
   Wang LJ, 2015, IEEE I CONF COMP VIS, P3119, DOI 10.1109/ICCV.2015.357
   Wang S, 2011, IEEE I CONF COMP VIS, P1323, DOI 10.1109/ICCV.2011.6126385
   Wu Y, 2015, IEEE T PATTERN ANAL, V37, P1834, DOI 10.1109/TPAMI.2014.2388226
   Wu Y, 2013, PROC CVPR IEEE, P2411, DOI 10.1109/CVPR.2013.312
   Yao R, 2016, IEEE SIGNAL PROC LET, V23, P658, DOI 10.1109/LSP.2016.2545705
   Yao R, 2013, PROC CVPR IEEE, P2363, DOI 10.1109/CVPR.2013.306
   Zhang H, 2018, NEUROCOMPUTING, V275, P2645, DOI 10.1016/j.neucom.2017.11.050
   Zhang KH, 2014, LECT NOTES COMPUT SC, V8693, P127, DOI 10.1007/978-3-319-10602-1_9
   Zhang L, 2017, PROC CVPR IEEE, P5825, DOI 10.1109/CVPR.2017.617
   Zhang L, 2017, PATTERN RECOGN, V69, P82, DOI 10.1016/j.patcog.2017.04.004
   Zhang MD, 2015, IEEE IMAGE PROC, P1468, DOI 10.1109/ICIP.2015.7351044
   Zhang TZ, 2017, PROC CVPR IEEE, P4819, DOI [10.1109/CVPR.2017.512, 10.1109/ICCV.2017.469]
   Zhao LJ, 2017, VISUAL COMPUT, V33, P1169, DOI 10.1007/s00371-016-1279-z
   Zhong W, 2012, PROC CVPR IEEE, P1838, DOI 10.1109/CVPR.2012.6247882
   Zhu G., 2015, P BRIT MACH VIS C BM
   Zitnick CL, 2014, LECT NOTES COMPUT SC, V8693, P391, DOI 10.1007/978-3-319-10602-1_26
NR 49
TC 5
Z9 5
U1 1
U2 24
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2020
VL 36
IS 6
BP 1173
EP 1187
DI 10.1007/s00371-019-01727-1
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA LI0TO
UT WOS:000529199400007
DA 2024-07-18
ER

PT J
AU Wang, D
   Hu, GQ
   Lyu, CZ
AF Wang, Dan
   Hu, Guoqing
   Lyu, Chengzhi
TI FRNet: an end-to-end feature refinement neural network for medical image
   segmentation
SO VISUAL COMPUTER
LA English
DT Article
DE Biomedical segmentation; Fully convolutional neural network; Retinal
   vessel segmentation; Skin lesion segmentation; Feature refinement path
ID VESSEL SEGMENTATION
AB Medical image segmentation is a crucial but challenging task for computer-aided diagnosis. In recent years, fully convolutional network-based methods have been widely applied to medical image segmentation. U-shape-based approaches are one of the most successful structures in this medical field. However, the consecutive down-sampling operations in the encoder lead to the loss of spatial information, which is important for medical image segmentation. In this paper, we present a novel lightweight end-to-end feature refinement network (FRNet) to address this issue. The structure of our model is simple and efficient. Specifically, the network adopts an encoder-decoder network as backbone, where two additional paths, spatial refinement path and semantic refinement path, are applied on the encoder and decoder, respectively, to improve the detailed representation ability and discriminative ability of our model. In addition, we introduce a feature adaptive fusion block (FAF block) that effectively combines features of different depths. The proposed FRNet can be trained in an end-to-end way. We have evaluated our method on three different medical image segmentation tasks. Experimental results show that FRNet has better performance than the state-of-the-art approaches. It achieves a high average accuracy without any post-processing of 0.968 and 0.936 for blood vessel segmentation and skin lesion segmentation, respectively. We further demonstrate that our method can be easily applied to other network structures to improve their performance.
C1 [Wang, Dan; Hu, Guoqing; Lyu, Chengzhi] South China Univ Technol, Sch Mech & Automot Engn, Guangzhou, Peoples R China.
C3 South China University of Technology
RP Hu, GQ (corresponding author), South China Univ Technol, Sch Mech & Automot Engn, Guangzhou, Peoples R China.
EM gqhu@scut.edu.cn
FU Nature Science Foundation of Guangdong province [2016A030313520]
FX This work is supported by the Nature Science Foundation of Guangdong
   province, No. 2016A030313520.
CR Adal KM, 2018, IEEE T BIO-MED ENG, V65, P1382, DOI 10.1109/TBME.2017.2752701
   Alom M. Z., 2018, ARXIV180206955, V6, P014006, DOI 10.1109/NAECON.2018.8556686
   [Anonymous], 2015, Advances in Neural Information Processing Systems
   [Anonymous], 2017, 2017 INT S BIOM IM I
   Bi L, 2017, VISUAL COMPUT, V33, P1061, DOI 10.1007/s00371-017-1379-4
   Cardona A, 2010, PLOS BIOL, V8, DOI 10.1371/journal.pbio.1000502
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen ST, 2018, ACM TRANS MODELING P, V3, DOI 10.1145/3199675
   Deng W, 2019, J MED SYST, V43, DOI 10.1007/s10916-019-1289-2
   Gu ZW, 2019, IEEE T MED IMAGING, V38, P2281, DOI 10.1109/TMI.2019.2903562
   He K., 2016, PROC CVPR IEEE, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]
   Hoover A, 2000, IEEE T MED IMAGING, V19, P203, DOI 10.1109/42.845178
   Hu K, 2018, NEUROCOMPUTING, V309, P179, DOI 10.1016/j.neucom.2018.05.011
   Jebaseeli TJ, 2019, OPTIK, V199, DOI 10.1016/j.ijleo.2019.163328
   Längkvist M, 2014, PATTERN RECOGN LETT, V42, P11, DOI 10.1016/j.patrec.2014.01.008
   Lee Y, 2001, IEEE T MED IMAGING, V20, P595, DOI 10.1109/42.932744
   Li XM, 2018, IEEE T MED IMAGING, V37, P2663, DOI 10.1109/TMI.2018.2845918
   Li Y., 2017, ARXIV170300577
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Nguyen UTV, 2013, PATTERN RECOGN, V46, P703, DOI 10.1016/j.patcog.2012.08.009
   Paulano F, 2014, VISUAL COMPUT, V30, P939, DOI 10.1007/s00371-014-0963-0
   Riaz F, 2020, IEEE ACCESS, V8, P16846, DOI 10.1109/ACCESS.2020.2967676
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Staal J, 2004, IEEE T MED IMAGING, V23, P501, DOI 10.1109/TMI.2004.825627
   Tasci E, 2015, J MED SYST, V39, DOI 10.1007/s10916-015-0231-5
   Thaha MM, 2019, J MED SYST, V43, DOI 10.1007/s10916-019-1416-0
   Yu CQ, 2018, LECT NOTES COMPUT SC, V11217, P334, DOI 10.1007/978-3-030-01261-8_20
   Zakeri FS, 2012, J MED SYST, V36, P1621, DOI 10.1007/s10916-010-9624-7
   Zhao HY, 2017, PROC CVPR IEEE, P907, DOI 10.1109/CVPR.2017.103
   Zhao HS, 2018, LECT NOTES COMPUT SC, V11207, P418, DOI 10.1007/978-3-030-01219-9_25
NR 31
TC 39
Z9 41
U1 1
U2 29
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2021
VL 37
IS 5
BP 1101
EP 1112
DI 10.1007/s00371-020-01855-z
EA MAY 2020
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RV2QF
UT WOS:000536743900001
DA 2024-07-18
ER

PT J
AU Shi, JL
   Sun, YH
   Bai, SQ
   Sun, ZX
   Tian, ZH
AF Shi, Jinlong
   Sun, Yunhan
   Bai, Suqin
   Sun, Zhengxing
   Tian, Zhaohui
TI A self-supervised method of single-image depth estimation by feeding
   forward information using max-pooling layers
SO VISUAL COMPUTER
LA English
DT Article
DE Encoder-decoder; CNN; Depth estimation; Single image; Self-supervision
ID STEREO
AB We propose an encoder-decoder CNN framework to predict depth from one single image in a self-supervised manner. To this aim, we design three kinds of encoder based on the recent advanced deep neural network and one kind of decoder which can generate multiscale predictions. Eight loss functions are designed based on the proposed encoder-decoder CNN framework to validate the performance. For training, we take rectified stereo image pairs as input of the CNN, which is trained by reconstructing image via learning multiscale disparity maps. For testing, the CNN can estimate the accurate depth information by inputting only one single image. We validate our framework on two public datasets in contrast to the state-of-the-art methods and our designed different variants, and the performance of different encoder-decoder architectures and loss functions is evaluated to obtain the best combination, which proves that our proposed method performs very well for single-image depth estimation without the supervision of ground truth.
C1 [Shi, Jinlong; Sun, Yunhan; Bai, Suqin] Jiangsu Univ Sci & Technol, Sch Comp Sci & Engn, Zhenjiang, Jiangsu, Peoples R China.
   [Sun, Zhengxing] Nanjing Univ, State Key Lab Novel Software Technol, Nanjing, Peoples R China.
   [Tian, Zhaohui] Ruizhi Informat Technol Co Ltd, Zhenjiang, Jiangsu, Peoples R China.
C3 Jiangsu University of Science & Technology; Nanjing University
RP Shi, JL (corresponding author), Jiangsu Univ Sci & Technol, Sch Comp Sci & Engn, Zhenjiang, Jiangsu, Peoples R China.
EM shi_jinlong@163.com; jacksunyh@vip.qq.com; 798464518@qq.com;
   szx@nju.edu.cn; 17963328@qq.com
RI Shi, JIn/JYP-1805-2024; shi, jin/GQZ-1206-2022; Sun,
   Zhengxing/A-7411-2011; shi, jin/JCD-8826-2023; Sun,
   YunHan/AAD-7898-2020; shi, jin/KDO-7906-2024
OI sun, yunhan/0000-0001-5264-7007
FU National Key Research and Development Program of China [2018YFC0309100,
   2018YFC0309104]; Six Talent Peaks Project in Jiangsu Province,
   Scholarship program of Jiangsu Provincial Government; Jiangsu University
   of Science and Technology
FX This work is supported by National Key Research and Development Program
   of China (No. 2018YFC0309100, No. 2018YFC0309104), Six Talent Peaks
   Project in Jiangsu Province, Scholarship program of Jiangsu Provincial
   Government and Jiangsu University of Science and Technology.
CR [Anonymous], 2011, PROC CVPR IEEE, DOI [DOI 10.1109/CVPR.2011.5995316, 10.1109/CVPR.2011.5995316]
   [Anonymous], ARXIV151203012
   Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615
   BARRON JT, 2015, PROC CVPR IEEE, P4466, DOI DOI 10.1109/CVPR.2015.7299076
   Chakrabarti Ayan, 2016, Advances in Neural Information Processing Systems, P2658
   Chen WF, 2016, ADV NEUR IN, V29
   Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350
   Criminisi A, 2007, INT J COMPUT VISION, V71, P89, DOI 10.1007/s11263-006-8525-1
   Ding K, 2014, VISUAL COMPUT, V30, P1311, DOI 10.1007/s00371-013-0888-z
   Eigen D, 2014, ADV NEUR IN, V27
   Eigen D, 2015, IEEE I CONF COMP VIS, P2650, DOI 10.1109/ICCV.2015.304
   Garg R, 2016, LECT NOTES COMPUT SC, V9912, P740, DOI 10.1007/978-3-319-46484-8_45
   Geiger A., 2012, CVPR
   Girdhar R, 2016, LECT NOTES COMPUT SC, V9910, P484, DOI 10.1007/978-3-319-46466-4_29
   GODARD C, ARXIV180601260
   Godard C, 2017, PROC CVPR IEEE, P6602, DOI 10.1109/CVPR.2017.699
   He K., 2016, PROC CVPR IEEE, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]
   HUANG G, 2017, PROC CVPR IEEE, P2261, DOI DOI 10.1109/CVPR.2017.243
   Jaderberg M, 2015, ADV NEUR IN, V28
   Jafari Omid Hosseini, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P4620, DOI 10.1109/ICRA.2017.7989537
   Kar A, 2017, ADV NEUR IN, V30
   KARSCH K, 2014, ACM T GRAPHIC, V33
   Kopf J, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601195
   Kuznietsov Y, 2017, PROC CVPR IEEE, P2215, DOI 10.1109/CVPR.2017.238
   Laina I, 2016, INT CONF 3D VISION, P239, DOI 10.1109/3DV.2016.32
   Lenz I, 2015, INT J ROBOT RES, V34, P705, DOI 10.1177/0278364914549607
   Li B, 2015, PROC CVPR IEEE, P1119, DOI 10.1109/CVPR.2015.7298715
   Li RH, 2018, IEEE INT CONF ROBOT, P7286, DOI 10.1109/ICRA.2018.8461251
   Liu FY, 2016, IEEE T PATTERN ANAL, V38, P2024, DOI 10.1109/TPAMI.2015.2505283
   Liu FY, 2015, PROC CVPR IEEE, P5162, DOI 10.1109/CVPR.2015.7299152
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Mahjourian R, 2018, PROC CVPR IEEE, P5667, DOI 10.1109/CVPR.2018.00594
   Mancini M, 2016, 2016 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS 2016), P4296, DOI 10.1109/IROS.2016.7759632
   Mukaeda T, 2017, IEEE ENG MED BIO, P921, DOI 10.1109/EMBC.2017.8036975
   Newell A, 2016, PROCEEDINGS OF THE ELEVENTH EUROPEAN CONFERENCE ON COMPUTER SYSTEMS, (EUROSYS 2016), DOI 10.1145/2901318.2901343
   PILLAI S, ARXIV181001849
   RAMIREZ PZ, ARXIV181004093
   REPALA VK, ARXIV180406324
   Saxena A., 2006, NIPS, P1161, DOI DOI 10.1109/TPAMI.2015.2505283A
   Saxena A, 2009, IEEE T PATTERN ANAL, V31, P824, DOI 10.1109/TPAMI.2008.132
   Shi JL, 2018, VISUAL COMPUT, V34, P377, DOI 10.1007/s00371-016-1339-4
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Srinivasan P, 2017, NEW FRONT EDUC RES, P65, DOI 10.1007/978-981-10-2207-4_5
   Stoyanov D, 2010, LECT NOTES COMPUT SC, V6361, P275
   TATENO K, ARXIV170403489
   Tulsiani S, 2017, PROC CVPR IEEE, P209, DOI 10.1109/CVPR.2017.30
   Ummenhofer B, 2017, PROC CVPR IEEE, P5622, DOI 10.1109/CVPR.2017.596
   Vijayanarasimhan S., ARXIV170407804
   Wang P, 2015, PROC CVPR IEEE, P2800, DOI 10.1109/CVPR.2015.7298897
   Wang XL, 2015, PROC CVPR IEEE, P539, DOI 10.1109/CVPR.2015.7298652
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Xie JY, 2016, LECT NOTES COMPUT SC, V9908, P842, DOI 10.1007/978-3-319-46493-0_51
   Xu D, 2017, PROC CVPR IEEE, P161, DOI 10.1109/CVPR.2017.25
   Yan XC, 2016, ADV NEUR IN, V29
   Yin ZC, 2018, PROC CVPR IEEE, P1983, DOI 10.1109/CVPR.2018.00212
   Yu F., 2015, ARXIV
   Zabulis X, 2018, VISUAL COMPUT, V34, P193, DOI 10.1007/s00371-016-1326-9
   Zhou TH, 2017, PROC CVPR IEEE, P6612, DOI 10.1109/CVPR.2017.700
   Zoran D, 2015, IEEE I CONF COMP VIS, P388, DOI 10.1109/ICCV.2015.52
NR 59
TC 4
Z9 4
U1 1
U2 10
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2021
VL 37
IS 4
SI SI
BP 815
EP 829
DI 10.1007/s00371-020-01832-6
EA APR 2020
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RL0LI
UT WOS:000524922000001
DA 2024-07-18
ER

PT J
AU Yang, YY
   Jia, WJ
   Wu, BY
AF Yang, Yunyun
   Jia, Wenjing
   Wu, Boying
TI Simultaneous segmentation and correction model for color medical and
   natural images with intensity inhomogeneity
SO VISUAL COMPUTER
LA English
DT Article
DE Level set method; Intensity inhomogeneity; Split Bregman method; Image
   segmentation; Correction; Color images
ID SPLIT BREGMAN METHOD; SCALABLE FITTING ENERGY; ACTIVE CONTOUR MODEL;
   LEVEL-SET METHOD; MINIMIZATION; EVOLUTION; GRADIENT
AB In this paper, a new level set formulation that can simultaneously segment and correct color images is proposed by combining the illumination and reflectance estimation (IRE) model, the level set method and the split Bregman method. The advantages of our model are mainly summarized in three aspects. First, our model can effectively extract the intensity change information in the images, regarded as the bias field. Based on the accurate segmentation results, our model can correct the inhomogeneous color images by removing the estimated bias field from the original images. Second, the application of the split Bregman method accelerates the iterative process and computational speed, making our model more efficient. Third, the use of the edge detection function in the energy functional makes it easier for our model to detect the target boundary. Perfectly absorbing the above three advantages, our model is applied to segment color medical and natural images with intensity inhomogeneity. Experimental results demonstrate that our model can accurately segment color images and get satisfactory correction images with intensity homogeneity. In addition, numerical comparison results further indicate that the performance of our model for segmentation and correction is significantly superior to the IRE model.
C1 [Yang, Yunyun; Jia, Wenjing] Harbin Inst Technol, Shenzhen, Peoples R China.
   [Wu, Boying] Harbin Inst Technol, Harbin, Peoples R China.
C3 Harbin Institute of Technology; Harbin Institute of Technology
RP Yang, YY (corresponding author), Harbin Inst Technol, Shenzhen, Peoples R China.
EM yangyunyun@hit.edu.cn; 18765952132@163.com; mathwby@hit.edu.cn
RI Jia, Weijia/W-6152-2019
FU Shenzhen Fundamental Research Plan [JCYJ20160505175141489]
FX This study was funded by Shenzhen Fundamental Research Plan (No.
   JCYJ20160505175141489).
CR Bi L, 2017, VISUAL COMPUT, V33, P1061, DOI 10.1007/s00371-017-1379-4
   Cai Q, 2018, PATTERN RECOGN, V82, P79, DOI 10.1016/j.patcog.2018.05.008
   Caselles V, 1997, INT J COMPUT VISION, V22, P61, DOI 10.1023/A:1007979827043
   Chan TE, 2000, J VIS COMMUN IMAGE R, V11, P130, DOI 10.1006/jvci.1999.0442
   Chan TF, 2001, IEEE T IMAGE PROCESS, V10, P266, DOI 10.1109/83.902291
   Chan TF, 2006, SIAM J APPL MATH, V66, P1632, DOI 10.1137/040615286
   Feng CL, 2017, NEUROCOMPUTING, V219, P107, DOI 10.1016/j.neucom.2016.09.008
   Goldstein T, 2010, J SCI COMPUT, V45, P272, DOI 10.1007/s10915-009-9331-z
   Goldstein T, 2009, SIAM J IMAGING SCI, V2, P323, DOI 10.1137/080725891
   Hettiarachchi R, 2017, PATTERN RECOGN, V65, P119, DOI 10.1016/j.patcog.2016.12.011
   KASS M, 1987, INT J COMPUT VISION, V1, P321, DOI 10.1007/BF00133570
   Le Y, 2015, IET IMAGE PROCESS, V9, P857, DOI 10.1049/iet-ipr.2014.0439
   Li CM, 2008, IEEE T IMAGE PROCESS, V17, P1940, DOI 10.1109/TIP.2008.2002304
   Li CM, 2014, MAGN RESON IMAGING, V32, P913, DOI 10.1016/j.mri.2014.03.010
   Li CM, 2009, IEEE I CONF COMP VIS, P702, DOI 10.1109/ICCV.2009.5459239
   Li CM, 2011, IEEE T IMAGE PROCESS, V20, P2007, DOI 10.1109/TIP.2011.2146190
   Li CM, 2010, IEEE T IMAGE PROCESS, V19, P3243, DOI 10.1109/TIP.2010.2069690
   Li YX, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18020556
   Min H, 2018, IEEE T IMAGE PROCESS, V27, P5016, DOI 10.1109/TIP.2018.2848471
   MUMFORD D, 1989, COMMUN PUR APPL MATH, V42, P577, DOI 10.1002/cpa.3160420503
   Paulano F, 2014, VISUAL COMPUT, V30, P939, DOI 10.1007/s00371-014-0963-0
   Shattuck DW, 2001, NEUROIMAGE, V13, P856, DOI 10.1006/nimg.2000.0730
   Song LJ, 2018, INT J PATTERN RECOGN, V32, DOI 10.1142/S0218001418540277
   Wang L, 2009, COMPUT MED IMAG GRAP, V33, P520, DOI 10.1016/j.compmedimag.2009.04.010
   Wang YL, 2008, SIAM J IMAGING SCI, V1, P248, DOI 10.1137/080724265
   Xian M, 2018, PATTERN RECOGN, V79, P340, DOI 10.1016/j.patcog.2018.02.012
   Xiao CX, 2013, VISUAL COMPUT, V29, P27, DOI 10.1007/s00371-012-0672-5
   Xing FY, 2016, IEEE T MED IMAGING, V35, P550, DOI 10.1109/TMI.2015.2481436
   Xu CY, 1998, IEEE T IMAGE PROCESS, V7, P359, DOI 10.1109/83.661186
   Xu GZ, 2018, NEUROCOMPUTING, V306, P1, DOI 10.1016/j.neucom.2018.04.010
   Yang YY, 2014, COMPUT MATH APPL, V67, P1559, DOI 10.1016/j.camwa.2014.01.017
   Yang YY, 2012, MATH PROBL ENG, V2012, DOI 10.1155/2012/494761
   Yang YY, 2012, J MATH ANAL APPL, V389, P351, DOI 10.1016/j.jmaa.2011.11.073
   Yang YY, 2010, LECT NOTES COMPUT SC, V6454, P117, DOI 10.1007/978-3-642-17274-8_12
   Zha ZY, 2018, VISUAL COMPUT, V34, P117, DOI 10.1007/s00371-016-1318-9
   Zhang HZ, 2015, IEEE T IMAGE PROCESS, V24, P3902, DOI 10.1109/TIP.2015.2456503
   Zhang T, 2018, INT J PATTERN RECOGN, V32, DOI 10.1142/S0218001418570057
   Zhong F, 2011, VISUAL COMPUT, V27, P707, DOI 10.1007/s00371-011-0588-5
   Zhou YF, 2017, IEEE T CIRC SYST VID, V27, P2281, DOI 10.1109/TCSVT.2016.2589781
NR 39
TC 4
Z9 5
U1 0
U2 19
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2020
VL 36
IS 4
BP 717
EP 731
DI 10.1007/s00371-019-01651-4
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KW0AL
UT WOS:000520835800005
DA 2024-07-18
ER

PT J
AU Bansod, SD
   Nandedkar, AV
AF Bansod, Suprit D.
   Nandedkar, Abhijeet V.
TI Crowd anomaly detection and localization using histogram of magnitude
   and momentum
SO VISUAL COMPUTER
LA English
DT Article
DE Anomaly detection; Surveillance; Histogram of magnitude; Momentum; Crowd
   analysis
ID EVENT DETECTION; MOTION; SURVEILLANCE; TRACKING
AB Video anomaly detection is an important and crucial area from security point of view. Anomaly detection means identifying unusual activities. It is a tedious task to recognize abnormal activities due to its infrequent occurrence in the crowd. Surveillance cameras are installed in crowded places, but manual analysis of video data gathered from these cameras is a cumbersome process and becomes almost impossible if cameras are in large number. In this work, an automated approach is proposed to detect and locate anomalies. A concept of momentum from Physics is used to connect foreground occupancy and motion of object. The proposed work is divided into three major steps: (a) background removal, (b) feature extraction and behavior recognition, and (c) anomaly detection and localization. The background removal step separates the background from each frame. To detect anomalies, appearance and motion characteristics of foreground objects are incorporated by histogram of magnitude and momentum features. Behavior of objects is learned through unsupervised clustering technique. In order to locate anomalies, positional features are used. The proposed approach is verified on benchmark datasets like UCSD and UMN devised for anomaly detection and crowd analysis. Experimental results are validated to contemporary methods.
C1 [Bansod, Suprit D.; Nandedkar, Abhijeet V.] SGGSIE&T, Dept Elect & Telecommun, Vishnupuri, Nanded, India.
C3 Shri Guru Gobind Singhji Institute of Engineering & Technology
RP Bansod, SD (corresponding author), SGGSIE&T, Dept Elect & Telecommun, Vishnupuri, Nanded, India.
EM bansodsuprit@sggs.ac.in; avnandedkar@sggs.ac.in
RI Bansod, Suprit/ABH-4253-2020
OI Bansod, Suprit/0000-0001-8485-5433
CR Adam A, 2008, IEEE T PATTERN ANAL, V30, P555, DOI 10.1109/TPAMI.2007.70825
   Amraee S, 2018, MULTIMED TOOLS APPL, V77, P14767, DOI 10.1007/s11042-017-5061-7
   Biswas S, 2014, IEEE IMAGE PROC, P5532, DOI 10.1109/ICIP.2014.7026119
   Cong Y, 2013, IEEE T INF FOREN SEC, V8, P1590, DOI 10.1109/TIFS.2013.2272243
   Cong Y, 2013, PATTERN RECOGN, V46, P1851, DOI 10.1016/j.patcog.2012.11.021
   Das Dawn D, 2016, VISUAL COMPUT, V32, P289, DOI 10.1007/s00371-015-1066-2
   Duda R. O., 2012, PATTERN CLASSIFICATI, DOI DOI 10.1007/978-3-319-57027-3_4
   Fablet R, 2002, LECT NOTES COMPUT SC, V2350, P476
   Guogang Xiong, 2011, Proceedings 2011 International Conference on Information and Automation (ICIA 2011), P495, DOI 10.1109/ICINFA.2011.5949043
   HORN BKP, 1981, ARTIF INTELL, V17, P185, DOI 10.1016/0004-3702(81)90024-2
   Hu WM, 2004, IEEE T SYST MAN CY C, V34, P334, DOI 10.1109/TSMCC.2004.829274
   Jiang F, 2009, IEEE IMAGE PROC, P1117, DOI 10.1109/ICIP.2009.5414535
   Kaltsa V, 2015, IEEE T IMAGE PROCESS, V24, P2153, DOI 10.1109/TIP.2015.2409559
   Kim J, 2009, PROC CVPR IEEE, P2913
   Kumar D, 2017, VISUAL COMPUT, V33, P265, DOI 10.1007/s00371-015-1192-x
   Lee DG, 2015, IEEE T CIRC SYST VID, V25, P1612, DOI 10.1109/TCSVT.2015.2395752
   Leyva R, 2017, IEEE T IMAGE PROCESS, V26, P3463, DOI 10.1109/TIP.2017.2695105
   Li NN, 2015, NEUROCOMPUTING, V155, P309, DOI 10.1016/j.neucom.2014.12.064
   Li SF, 2018, SIGNAL PROCESS-IMAGE, V60, P6, DOI 10.1016/j.image.2017.09.002
   Li T, 2015, IEEE T CIRC SYST VID, V25, P367, DOI 10.1109/TCSVT.2014.2358029
   Li WX, 2014, IEEE T PATTERN ANAL, V36, P18, DOI 10.1109/TPAMI.2013.111
   Lin HH, 2015, IEEE IMAGE PROC, P2434, DOI 10.1109/ICIP.2015.7351239
   Liu Ce, 2009, THESIS
   Mahadevan V, 2010, PROC CVPR IEEE, P1975, DOI 10.1109/CVPR.2010.5539872
   Mehran R, 2009, PROC CVPR IEEE, P935, DOI 10.1109/CVPRW.2009.5206641
   Prasad NR, 2009, CMC-COMPUT MATER CON, V14, P1, DOI 10.1145/1541880.1541882
   Reinders F, 2001, VISUAL COMPUT, V17, P55, DOI 10.1007/PL00013399
   Saligrama V, 2012, PROC CVPR IEEE, P2112, DOI 10.1109/CVPR.2012.6247917
   Tang B, 2013, IEEE SYS MAN CYBERN, P1, DOI 10.1109/SMC.2013.8
   Vishwakarma S, 2013, VISUAL COMPUT, V29, P983, DOI 10.1007/s00371-012-0752-6
   Wang B, 2012, MACH VISION APPL, V23, P501, DOI 10.1007/s00138-011-0341-0
   Wu SD, 2010, PROC CVPR IEEE, P2054, DOI 10.1109/CVPR.2010.5539882
   Xu D, 2014, NEUROCOMPUTING, V143, P144, DOI 10.1016/j.neucom.2014.06.011
   Zhang YH, 2015, IEEE T CIRC SYST VID, V25, P1231, DOI 10.1109/TCSVT.2014.2355711
NR 34
TC 32
Z9 33
U1 0
U2 12
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2020
VL 36
IS 3
BP 609
EP 620
DI 10.1007/s00371-019-01647-0
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KL4EO
UT WOS:000513378500012
DA 2024-07-18
ER

PT J
AU Yang, H
   Min, K
AF Yang, Heekyung
   Min, Kyungha
TI Classification of basic artistic media based on a deep convolutional
   approach
SO VISUAL COMPUTER
LA English
DT Article
DE Classification; Convolutional neural network; Artistic media; NPR
ID NEURAL-NETWORKS; PAINTINGS; FEATURES
AB Artistic media play an important role in recognizing and classifying artworks in many artwork classification works and public artwork databases. We employ deep CNN structure to recognize artistic media from artworks and to classify them into predetermined categories. For this purpose, we define basic artistic media as oilpaint brush, pastel, pencil and watercolor and build artwork image dataset by collecting artwork images from various websites. To build our classifier, we implement various recent deep CNN structures and compare their performances. Among them, we select DenseNet, which shows best performance for recognizing artistic media. Through the human baseline experiment, we show that the performance of our classifier is compatible with that of trained human. Furthermore, we also show that our classifier shows a similar recognition and classification pattern with human in terms of well-classifying media, ill-classifying media, confusing pair and confusing case. We also collect synthesized oilpaint artwork images from fourteen important oilpaint literatures and apply them to our classifier. Our classifier shows a meaningful performance, which will lead to an evaluation scheme for the artistic media simulation techniques of non-photorealistic rendering (NPR) society.
C1 [Yang, Heekyung] Sangmyung Univ, Grad Sch, Dept Comp Sci, Seoul, South Korea.
   [Min, Kyungha] Sangmyung Univ, Dept Comp Sci, Seoul, South Korea.
C3 Sangmyung University; Sangmyung University
RP Min, K (corresponding author), Sangmyung Univ, Dept Comp Sci, Seoul, South Korea.
EM minkh@smu.ac.kr
FU National Research Foundation of Korea (NRF) [NRF-2018R1D1A1A02050292,
   NRF-2017R1D1A12B03034137]
FX This research was supported National Research Foundation of Korea (NRF)
   through NRF-2018R1D1A1A02050292 and NRF-2017R1D1A12B03034137.
CR [Anonymous], 2016, P IEEE C COMPUTER VI
   [Anonymous], 2015, 2015 IEEE INT C MULT
   [Anonymous], 2017, ARXIV170800684
   [Anonymous], 2014, P BRIT MACH VIS C BM
   [Anonymous], 2017, Machine Learning Yearning
   [Anonymous], 2014, P 31 INT C INT C MAC
   Anwer RM, 2016, ICMR'16: PROCEEDINGS OF THE 2016 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P339, DOI 10.1145/2911996.2912063
   Bar Y, 2015, LECT NOTES COMPUT SC, V8925, P71, DOI 10.1007/978-3-319-16178-5_5
   Bergamo Alessandro., 2011, NIPS, P2088
   Cetinic E, 2018, EXPERT SYST APPL, V114, P107, DOI 10.1016/j.eswa.2018.07.026
   Cetinic E, 2016, ELMAR PROC, P201, DOI 10.1109/ELMAR.2016.7731786
   Chen ZL, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818066
   Falomir Z, 2018, EXPERT SYST APPL, V97, P83, DOI 10.1016/j.eswa.2017.11.056
   Fiser J, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073660
   Florea C, 2017, IEEE WINT CONF APPL, P569, DOI 10.1109/WACV.2017.69
   Hays J., 2004, PROC NPAR 01, P113
   Hertzmann A., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P453, DOI 10.1145/280814.280951
   Johnson Justin, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kagaya M, 2011, IEEE T VIS COMPUT GR, V17, P74, DOI 10.1109/TVCG.2010.25
   Keren D, 2002, INT C PATT RECOG, P474, DOI 10.1109/ICPR.2002.1048341
   Kyprianidis JE, 2013, IEEE T VIS COMPUT GR, V19, P866, DOI 10.1109/TVCG.2012.160
   Li J, 2004, IEEE T IMAGE PROCESS, V13, P338, DOI 10.1109/TIP.2003.821349
   Lin L, 2010, INT CONF BIOMED, P70, DOI 10.1109/BMEI.2010.5639646
   Litwinowicz P., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P407, DOI 10.1145/258734.258893
   Liu GW, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P2162
   Lyu S, 2004, P NATL ACAD SCI USA, V101, P17006, DOI 10.1073/pnas.0406398101
   Mao H, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1183, DOI 10.1145/3123266.3123405
   Mensink T, 2014, P INT C MULT RETR, P451, DOI DOI 10.1145/2578726.2578791
   O'Donovan P, 2012, IEEE T VIS COMPUT GR, V18, P475, DOI 10.1109/TVCG.2011.51
   Pablo Messina, 2018, USER MODEL USER-ADAP, V29, P1
   Picard D, 2015, IEEE SIGNAL PROC MAG, V32, P95, DOI 10.1109/MSP.2015.2409557
   Seguin Benoit, 2016, Computer Vision - ECCV 2016. 14th European Conference: Workshops. Proceedings: LNCS 9913, P753, DOI 10.1007/978-3-319-46604-0_52
   Selim A, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925968
   Shamir L, 2010, ACM T APPL PERCEPT, V7, DOI 10.1145/1670671.1670672
   Sun TC, 2017, IEEE T IMAGE PROCESS, V26, P4102, DOI 10.1109/TIP.2017.2710631
   Tan WR, 2016, IEEE IMAGE PROC, P3703, DOI 10.1109/ICIP.2016.7533051
   van Noord N, 2015, IEEE SIGNAL PROC MAG, V32, P46, DOI 10.1109/MSP.2015.2406955
   Wu YC, 2013, COMPUT GRAPH FORUM, V32, P153, DOI 10.1111/cgf.12161
   Zeng K, 2009, ACM T GRAPHIC, V29, DOI 10.1145/1640443.1640445
   Zhao M., 2011, Proceedings of the ACM SIGGRAPH/Eurographics Symposium on Non-Photorealistic Animation and Rendering, P117
   Zhao M., 2010, PROC INT S NONPHOTOR, P99, DOI DOI 10.1145/1809939.1809951
   Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319
NR 42
TC 18
Z9 20
U1 0
U2 11
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2020
VL 36
IS 3
BP 559
EP 578
DI 10.1007/s00371-019-01641-6
PG 20
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KL4EO
UT WOS:000513378500009
DA 2024-07-18
ER

PT J
AU Wang, L
   Wang, Z
   Yang, XS
   Hu, SM
   Zhang, JJ
AF Wang, Li
   Wang, Zhao
   Yang, Xiaosong
   Hu, Shi-Min
   Zhang, Jianjun
TI Photographic style transfer
SO VISUAL COMPUTER
LA English
DT Article
DE Photographic style transfer; Deep learning; Photorealism preservation;
   Image processing
ID COLOR TRANSFER; STYLIZATION
AB Image style transfer has attracted much attention in recent years. However, results produced by existing works still have lots of distortions. This paper investigates the CNN-based artistic style transfer work specifically and finds out the key reasons for distortion coming from twofold: the loss of spatial structures of content image during content-preserving process and unexpected geometric matching introduced by style transformation process. To tackle this problem, this paper proposes a novel approach consisting of a dual-stream deep convolution network as the loss network and edge-preserving filters as the style fusion model. Our key contribution is the introduction of an additional similarity loss function that constrains both the detail reconstruction and style transfer procedures. The qualitative evaluation shows that our approach successfully suppresses the distortions as well as obtains faithful stylized results compared to state-of-the-art methods.
C1 [Wang, Li; Wang, Zhao; Yang, Xiaosong] Bournemouth Univ, Natl Ctr Comp Animat, Poole, Dorset, England.
   [Zhang, Jianjun] Bournemouth Univ, Natl Ctr Comp Animat, Comp Graph, Poole, Dorset, England.
   [Hu, Shi-Min] Tsinghua Univ, Dept Comp Sci & Technol, Beijing, Peoples R China.
C3 Bournemouth University; Bournemouth University; Tsinghua University
RP Wang, L (corresponding author), Bournemouth Univ, Natl Ctr Comp Animat, Poole, Dorset, England.
EM lwang@bournemouth.ac.uk
RI Hu, Shi-Min/AAW-1952-2020; Wang, Li/JDD-5101-2023; Wang,
   Zhao/JBJ-5365-2023
OI Wang, Li/0000-0002-5793-2437; Wang, Zhao/0000-0002-7144-1511; Yang,
   Xiaosong/0000-0003-3815-0584
FU European Commission under FP7 MarieCurie IRSES project AniNex [612627];
   China Scholarship Council; Visual Media Research Centre at Tsinghua
   University
FX This work was supported by the European Commission under FP7 MarieCurie
   IRSES project AniNex (612627) and the China Scholarship Council. Li Wang
   appreciated support from Visual Media Research Centre at Tsinghua
   University, during his Visiting in Beijing.
CR [Anonymous], 2017, ARXIV170108893
   [Anonymous], 2017, ARXIV170506830
   [Anonymous], 2017, ARXIV170309210
   [Anonymous], 2016, ARXIV161201895
   [Anonymous], 2017, ARXIV170909828
   Bae SM, 2006, ACM T GRAPHIC, V25, P637, DOI 10.1145/1141911.1141935
   Chen TC, 2017, AGEING SOC, V37, P1798, DOI 10.1017/S0144686X16000623
   Chi MT, 2016, VISUAL COMPUT, V32, P1549, DOI 10.1007/s00371-015-1139-2
   Gastal ESL, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964964
   Gatys L., 2016, Journal of Vision, V16, P326, DOI DOI 10.1167/16.12.326
   Gong H., 2017, VISUAL COMPUT, P1
   He M., 2017, ARXIV171000756
   Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167
   Hwang Y, 2014, PROC CVPR IEEE, P3342, DOI 10.1109/CVPR.2014.427
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kingma D. P., 2014, arXiv
   Li C, 2016, PROC CVPR IEEE, P2479, DOI 10.1109/CVPR.2016.272
   Liao J., 2017, ACM Trans. Graph.
   Luan FJ, 2017, PROC CVPR IEEE, P6997, DOI 10.1109/CVPR.2017.740
   Mahendran A, 2015, PROC CVPR IEEE, P5188, DOI 10.1109/CVPR.2015.7299155
   Morel JM, 2014, IMAGE PROCESS ON LIN, V4, P16, DOI 10.5201/ipol.2014.84
   Oliveira M, 2011, PROC CVPR IEEE, P201, DOI 10.1109/CVPR.2011.5995658
   Pitié F, 2005, IEEE I CONF COMP VIS, P1434
   Reinhard E, 2001, IEEE COMPUT GRAPH, V21, P34, DOI 10.1109/38.946629
   Shih YC, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601137
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sunkavalli K, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778862
   Tai YW, 2005, PROC CVPR IEEE, P747
   Tsai Y.H., 2017, ARXIV170300069
   Ulyanov D., 2016, P 33 INT C INT C MAC, V48, P1349
   Wu FZ, 2013, COMPUT GRAPH FORUM, V32, P190, DOI 10.1111/cgf.12008
   Yi ZL, 2017, VISUAL COMPUT, V33, P1443, DOI 10.1007/s00371-016-1290-4
   Zhang H., 2017, MULTISTYLE GENERATIV
   Zhao H., 2015, arXiv 2015 1511.08861, DOI [10.48550/ARXIV.1511.08861, DOI 10.48550/ARXIV.1511.08861]
NR 34
TC 9
Z9 11
U1 4
U2 34
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2020
VL 36
IS 2
BP 317
EP 331
DI 10.1007/s00371-018-1609-4
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KJ2TH
UT WOS:000511910300008
OA hybrid
DA 2024-07-18
ER

PT J
AU Li, YR
   Wang, Z
   Yang, XS
   Wang, ML
   Poiana, SI
   Chaudhry, E
   Zhang, JJ
AF Li, Yanran
   Wang, Zhao
   Yang, Xiaosong
   Wang, Meili
   Poiana, Sebastian Iulian
   Chaudhry, Ehtzaz
   Zhang, Jianjun
TI Efficient convolutional hierarchical autoencoder for human motion
   prediction
SO VISUAL COMPUTER
LA English
DT Article
DE Motion prediction; Deep learning; Autoencoder; Hierarchical networks
AB Human motion prediction is a challenging problem due to the complicated human body constraints and high-dimensional dynamics. Recent deep learning approaches adopt RNN, CNN or fully connected networks to learn the motion features which do not fully exploit the hierarchical structure of human anatomy. To address this problem, we propose a convolutional hierarchical autoencoder model for motion prediction with a novel encoder which incorporates 1D convolutional layers and hierarchical topology. The new network is more efficient compared to the existing deep learning models with respect to size and speed. We train the generic model on Human3.6M and CMU benchmark and conduct extensive experiments. The qualitative and quantitative results show that our model outperforms the state-of-the-art methods in both short-term prediction and long-term prediction.
C1 [Li, Yanran; Yang, Xiaosong; Chaudhry, Ehtzaz] Bournemouth Univ, Natl Ctr Comp Animat, Bournemouth, Dorset, England.
   [Poiana, Sebastian Iulian] Bournemouth Univ, Natl Ctr Internet Things IoT Cyber Secur, Bournemouth, Dorset, England.
   [Zhang, Jianjun] Bournemouth Univ, Natl Ctr Comp Animat, Comp Graph, Bournemouth, Dorset, England.
   [Wang, Zhao] Nanjing Inst Adv Artificial Intelligence, Nanjing, Jiangsu, Peoples R China.
   [Wang, Zhao] Horizon Robot, Nanjing, Jiangsu, Peoples R China.
   [Wang, Meili] Northwest A&F Univ, Coll Informat Engn, Xian, Shaanxi, Peoples R China.
C3 Bournemouth University; Bournemouth University; Bournemouth University;
   Northwest A&F University - China
RP Yang, XS (corresponding author), Bournemouth Univ, Natl Ctr Comp Animat, Bournemouth, Dorset, England.
EM xyang@bournemouth.ac.uk
RI Wang, Zhao/JBJ-5365-2023
OI Wang, Zhao/0000-0002-7144-1511; Yang, Xiaosong/0000-0003-3815-0584; Li,
   Yanran/0000-0003-1385-7604
FU EU H2020 under the REA grant agreement [691215]; Key Laboratory of
   Agricultural Internet of Things, Ministry of Agriculture and Rural
   Affairs, Yangling, Shaanxi, China [2018AIOT-09]; South West Creative
   Technology Network
FX This study was funded by EU H2020 under the REA grant agreement (Grant
   Number 691215) and the Key Laboratory of Agricultural Internet of
   Things, Ministry of Agriculture and Rural Affairs, Yangling, Shaanxi
   712100, China (2018AIOT-09) and the Automation Fellow in the South West
   Creative Technology Network.
CR Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265
   Akhter I, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2159516.2159523
   [Anonymous], IEEE T CIRCUITS SYST
   [Anonymous], 2013, EMNLP
   [Anonymous], IJCAI
   [Anonymous], 2017, INT C MACH LEARN
   [Anonymous], 2017, C COMP VIS PATT REC
   [Anonymous], WORKSH NEW CHALL NEU
   [Anonymous], COMP GRAPH
   Bregler C, 1998, PROC CVPR IEEE, P8, DOI 10.1109/CVPR.1998.698581
   Du Y, 2015, PROC CVPR IEEE, P1110, DOI 10.1109/CVPR.2015.7298714
   Fragkiadaki K, 2015, IEEE I CONF COMP VIS, P4346, DOI 10.1109/ICCV.2015.494
   Ghosh P, 2017, INT CONF 3D VISION, P458, DOI 10.1109/3DV.2017.00059
   Gui LY, 2018, IEEE INT C INT ROBOT, P562, DOI 10.1109/IROS.2018.8594452
   Holden A, 2015, BRITISH HCI 2015, P18, DOI 10.1145/2783446.2783576
   Holden D, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073663
   Holden D, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925975
   Ionescu C, 2014, IEEE T PATTERN ANAL, V36, P1325, DOI 10.1109/TPAMI.2013.248
   Jain A, 2016, PROC CVPR IEEE, P5308, DOI 10.1109/CVPR.2016.573
   Kalchbrenner Nal, 2016, ARXIV161010099
   Kamijo K., 1990, IJCNN International Joint Conference on Neural Networks (Cat. No.90CH2879-5), P215, DOI 10.1109/IJCNN.1990.137572
   Koppula HS, 2016, IEEE T PATTERN ANAL, V38, P14, DOI 10.1109/TPAMI.2015.2430335
   Kovar Lucas., 2008, ACM SIGGRAPH 2008 classes, page, P51
   Li C, 2018, PROC CVPR IEEE, P5226, DOI 10.1109/CVPR.2018.00548
   Liang-Yan Gui, 2018, Computer Vision - ECCV 2018. 15th European Conference. Proceedings: Lecture Notes in Computer Science (LNCS 11208), P823, DOI 10.1007/978-3-030-01225-0_48
   Martinez J, 2017, PROC CVPR IEEE, P4674, DOI 10.1109/CVPR.2017.497
   Rather AM, 2015, EXPERT SYST APPL, V42, P3234, DOI 10.1016/j.eswa.2014.12.003
   Sutskever I., 2009, Advances in Neural Information Processing Systems, P1601
   Sutskever I, 2014, ADV NEUR IN, V27
   Taylor G, 2009, P 26 ANN INT C MACH
   Taylor GW, 2010, PROC CVPR IEEE, P631, DOI 10.1109/CVPR.2010.5540157
   Taylor GW, 2007, ADV NEURAL INFORM PR, P1345, DOI DOI 10.7551/MITPRESS/7503.003.0173
   Urtasun R., 2008, P 25 INT C MACHINE L, P1080, DOI [10.1145/1390156.1390292, 10.1145/1390156.13902922]
   Wang JM, 2008, IEEE T PATTERN ANAL, V30, P1118, DOI 10.1109/TPAMI.2008.91
   Wang YM, 2017, VISUAL COMPUT, V33, P971, DOI 10.1007/s00371-017-1378-5
NR 35
TC 41
Z9 42
U1 0
U2 7
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2019
VL 35
IS 6-8
SI SI
BP 1143
EP 1156
DI 10.1007/s00371-019-01692-9
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IC1IH
UT WOS:000470712200030
OA Green Accepted, hybrid, Green Published
DA 2024-07-18
ER

PT J
AU Zhang, X
   Li, YH
   Zhang, ZY
   Konno, K
   Hu, SJ
AF Zhang, Xinyue
   Li, Yuanhao
   Zhang, Zhiyi
   Konno, Kouichi
   Hu, Shaojun
TI Intelligent Chinese calligraphy beautification from handwritten
   characters for robotic writing
SO VISUAL COMPUTER
LA English
DT Article
DE Calligraphy; Beautification; Robotic arm; Optimization; Handwritten
   characters
AB Chinese calligraphy is the artistic expression of character writing and is highly valued in East Asia. However, it is a challenge for non-expert users to write visually pleasing calligraphy with his or her own unique style. In this paper, we develop an intelligent system that beautifies Chinese handwriting characters and physically writes them in a certain calligraphy style using a robotic arm. First, we sketch the handwriting characters using a mouse or a touch pad. Then, we employ a convolutional neural network to identify each stroke from the skeletons, and the corresponding standard stroke is retrieved from a pre-built calligraphy stroke library for robotic arm writing. To output aesthetically beautiful calligraphy with the user's style, we propose a global optimization approach to solve the minimization problem between the handwritten strokes and standard calligraphy strokes, in which a shape character vector is presented to describe the shape of standard strokes. Unlike existing systems that focus on the generation of digital calligraphy from handwritten characters, our system has the advantage of converting the user-input handwriting into physical calligraphy written by a robotic arm. We take the regular script (Kai) style as an example and perform a user study to evaluate the effectiveness of the system. The writing results show that our system can achieve visually pleasing calligraphy from various input handwriting while retaining the user's style.
C1 [Zhang, Xinyue; Li, Yuanhao; Zhang, Zhiyi; Hu, Shaojun] Northwest A&F Univ, Coll Informat Engn, Yangling, Shaanxi, Peoples R China.
   [Zhang, Zhiyi; Hu, Shaojun] Minist Agr, Key Lab Agr Internet Things, Yangling, Shaanxi, Peoples R China.
   [Konno, Kouichi] Iwate Univ, Fac Sci & Engn, Morioka, Iwate, Japan.
C3 Northwest A&F University - China; Ministry of Agriculture & Rural
   Affairs; Iwate University
RP Hu, SJ (corresponding author), Northwest A&F Univ, Coll Informat Engn, Yangling, Shaanxi, Peoples R China.; Hu, SJ (corresponding author), Minist Agr, Key Lab Agr Internet Things, Yangling, Shaanxi, Peoples R China.
EM xinyue.zhang@nwafu.edu.cn; konno@cis.iwate-u.ac.jp; hsj@nwsuaf.edu.cn
RI zhang, ZY/HJH-6535-2023; Wang, Ling/AGR-4917-2022; Li,
   Shiyue/KFA-3709-2024; liang, YU/IYT-4334-2023
OI Wang, Ling/0000-0003-0272-2974; HU, Shaojun/0000-0002-4686-7633; liang,
   YU/0009-0007-3922-3454
FU NSFC [61303124]; NSBR Plan of Shaanxi [2019JM370]; Fundamental Research
   Funds for the Central Universities [2452017343]
FX We thank the CGI2019 reviewers for their thoughtful comments. The work
   is supported by the NSFC (61303124), NSBR Plan of Shaanxi (2019JM370)
   and the Fundamental Research Funds for the Central Universities
   (2452017343).
CR Chao F, 2018, IEEE INT CONF ROBOT, P1104
   Chen X, 2016, ROCK MECH ROCK ENG, V49, P2701, DOI 10.1007/s00603-016-0934-z
   Furrer F., 2017, 2017 IEEE INT C ROBO
   Hailong Li, 2012, 2012 4th International Conference on Digital Home (ICDH 2012), P122, DOI 10.1109/ICDH.2012.64
   Hashiguchi H, 2004, J ROBOTICS MECHATRON, V16, P381, DOI [10.20965/jrm.2004.p0381, DOI 10.20965/JRM.2004.P0381]
   Igarashi T, 1999, COMP GRAPH, P409, DOI 10.1145/311535.311602
   Igarashi T, 2015, IEEE COMPUT GRAPH, V35, P31
   Kim B, 2018, COMPUT GRAPH FORUM, V37, P329, DOI 10.1111/cgf.13365
   Kim SK, 2014, IEEE INT C INT ROBOT, P877, DOI 10.1109/IROS.2014.6942663
   Krizek M., 2004, Conjugate Gradient Algorithms and Finite Element Methods
   Liu LJ, 2010, IEEE SYS MAN CYBERN, P3644, DOI 10.1109/ICSMC.2010.5641880
   Lo KW, 2006, 2006 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, VOLS 1-12, P5183, DOI 10.1109/IROS.2006.281655
   Ma Z, 2017, IEEE T COGN DEV SYST, V9, P80, DOI 10.1109/TCDS.2016.2645598
   Mueller S, 2013, IEEE INT C INT ROBOT, P1734, DOI 10.1109/IROS.2013.6696583
   Okutomi M, 2015, DIGITAL IMAGE PROCES
   Safaei J, 2011, IEEE INT C BIOINFORM, P222, DOI 10.1109/BIBM.2011.58
   Sun YD, 2014, IEEE INT CONF ROBOT, P3207, DOI 10.1109/ICRA.2014.6907320
   Sun YD, 2014, IEEE INT C INT ROBOT, P4408, DOI 10.1109/IROS.2014.6943186
   Sun YD, 2013, IEEE INT CONF ROBOT, P4279, DOI 10.1109/ICRA.2013.6631182
   Syamlan Adlina Taufik, 2015, 2015 International Conference on Advanced Mechatronics, Intelligent Manufacture and Industrial Automation (ICAMIMIA). Proceedings, P46, DOI 10.1109/ICAMIMIA.2015.7508000
   Tian X, 2007, ELABORATION ANCIENT
   Tianyang Yi, 2014, MultiMedia Modeling. 20th Anniversary International Conference, MMM 2014. Proceedings: LNCS 8325, P254, DOI 10.1007/978-3-319-04114-8_22
   Xiao XF, 2017, PATTERN RECOGN, V72, P72, DOI 10.1016/j.patcog.2017.06.032
   Yao FH, 2004, ADV ROBOTICS, V18, P331, DOI 10.1163/156855304322972477
   Zeng Hualin, 2016, CAAI Transactions on Intelligent Systems, V11, P15, DOI 10.11992/tis.201507067
   Zhang ZJ, 2010, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON TECHNOLOGY MANAGEMENT AND INNOVATION (TMI 2010), P99, DOI 10.1145/1816123.1816138
NR 26
TC 17
Z9 18
U1 1
U2 22
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2019
VL 35
IS 6-8
SI SI
BP 1193
EP 1205
DI 10.1007/s00371-019-01675-w
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IC1IH
UT WOS:000470712200034
DA 2024-07-18
ER

PT J
AU Li, HL
   Toyoura, M
   Mao, XY
AF Li, Honglin
   Toyoura, Masahiro
   Mao, Xiaoyang
TI Caricature synthesis with feature deviation matching under example-based
   framework
SO VISUAL COMPUTER
LA English
DT Article
DE Caricature synthesis; Example-based; Cross-modal distance metric;
   Feature deviation matching
ID EXAGGERATION
AB Example-based caricature synthesis techniques have been attracting large attentions for being able to generate attractive caricatures of various styles. This paper proposes a new example-based caricature synthesis system using a feature deviation matching method as a cross-modal distance metric. It employs the deviation values from average features across different feature spaces rather than the values of features themselves to search for similar components from caricature examples directly. Compared with traditional example-based systems, the proposed system can generate various styles of caricatures without requiring paired photograph-caricature example databases. The newly designed features can effectively capture visual characteristics of the hairstyles and facial components in input portrait images. In addition, this system can control the exaggeration of individual facial components and provide several similarity-based candidates to satisfy users' different preferences. Experiments are conducted to prove the above ideas.
C1 [Li, Honglin; Toyoura, Masahiro; Mao, Xiaoyang] Univ Yamanashi, Kofu, Yamanashi 4008511, Japan.
C3 University of Yamanashi
RP Mao, XY (corresponding author), Univ Yamanashi, Kofu, Yamanashi 4008511, Japan.
EM g15dhl01@yamanashi.ac.jp; mtoyoura@yamanashi.ac.jp; mao@yamanashi.ac.jp
OI mao, xiaoyang/0000-0001-9531-3197; Toyoura, Masahiro/0000-0002-5897-7573
FU JSPS [26560006, 26240015]; KAKENHI [17H00737]; Grants-in-Aid for
   Scientific Research [17H00737, 26240015, 26560006] Funding Source: KAKEN
FX This study was funded by JSPS Grants-in-Aid for Scientific Research
   (Grant Nos. 26560006 and 26240015) and (Grant No. KAKENHI 17H00737).
CR Andrew G., 2013, ICML, P1247
   BOOKSTEIN FL, 1989, IEEE T PATTERN ANAL, V11, P567, DOI 10.1109/34.24792
   BRENNAN SE, 1985, LEONARDO, V18, P170, DOI 10.2307/1578048
   Cao Y, 2017, AAAI CONF ARTIF INTE, P3974
   Chen H, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P433, DOI 10.1109/ICCV.2001.937657
   Chen H., 2004, P ANN C MECH MACH VI, P7
   Chen H., 2002, Proceedings of the Tenth ACM International Conference on Multimedia, P171
   Chen W. J., 2010, ADV COMPUT SCI ENG, V5
   COOTES TF, 1995, COMPUT VIS IMAGE UND, V61, P38, DOI 10.1006/cviu.1995.1004
   Cosker D, 2007, LECT NOTES COMPUT SC, V4418, P365
   Fang RC, 2012, 2012 IEEE 11TH INTERNATIONAL CONFERENCE ON SOLID-STATE AND INTEGRATED CIRCUIT TECHNOLOGY (ICSICT-2012), P227
   Fiser J, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073660
   Gatys L., 2016, Journal of Vision, V16, P326, DOI DOI 10.1167/16.12.326
   Gooch B, 2004, ACM T GRAPHIC, V23, P27, DOI 10.1145/966131.966133
   HARMON LD, 1973, SCI AM, V229, P70, DOI 10.1038/scientificamerican1173-70
   Hotelling H, 1936, BIOMETRIKA, V28, P321, DOI 10.1093/biomet/28.3-4.321
   Kalogerakis E, 2012, ACM T GRAPHIC, V31, DOI [10.1145/2077341.2077342, 10.1145/2185520.2185551]
   Koshimizu H., 1999, IEEE SMC'99 Conference Proceedings. 1999 IEEE International Conference on Systems, Man, and Cybernetics (Cat. No.99CH37028), P294, DOI 10.1109/ICSMC.1999.816567
   Li HL, 2016, VISUAL COMPUT, V32, P1351, DOI 10.1007/s00371-016-1232-1
   Liang L, 2002, 10TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P386, DOI 10.1109/PCCGA.2002.1167882
   Liao J., 2017, ARXIV170501088CSCV
   Min F, 2007, LECT NOTES COMPUT SC, V4679, P184
   Mo Z., 2004, ACM SIGGRAPH 2004 SK, P57
   Rasiwasia N., 2010, P 18 INT C MULT FIR, P251, DOI 10.1145/1873951.1873987
   Sadimon SB, 2010, 2010 INTERNATIONAL CONFERENCE ON CYBERWORLDS (CW 2010), P383, DOI 10.1109/CW.2010.33
   Selim A, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925968
   Shih YC, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601137
   Song YB, 2014, LECT NOTES COMPUT SC, V8694, P800, DOI 10.1007/978-3-319-10599-4_51
   Sunhem W, 2016, 2016 EIGHTH INTERNATIONAL CONFERENCE ON ADVANCED COMPUTATIONAL INTELLIGENCE (ICACI), P390, DOI 10.1109/ICACI.2016.7449857
   Taigman Yaniv, 2017, ICLR
   VINCENT L, 1991, IEEE T PATTERN ANAL, V13, P583, DOI 10.1109/34.87344
   Wang NN, 2013, IEEE T NEUR NET LEAR, V24, P1364, DOI 10.1109/TNNLS.2013.2258174
   Wang XG, 2009, IEEE T PATTERN ANAL, V31, P1955, DOI 10.1109/TPAMI.2008.222
   Xu JY, 2014, 2014 INTERNATIONAL CONFERENCE ON CYBERWORLDS (CW), P213, DOI 10.1109/CW.2014.37
   Xu ZJ, 2008, IEEE T PATTERN ANAL, V30, P955, DOI 10.1109/TPAMI.2008.50
   Yacoob Y, 2006, IEEE T PATTERN ANAL, V28, P1164, DOI 10.1109/TPAMI.2006.139
   Yan F, 2015, PROC CVPR IEEE, P3441, DOI 10.1109/CVPR.2015.7298966
   Yang W, 2016, VISUAL COMPUT, V32, P383, DOI 10.1007/s00371-015-1177-9
   Yang W, 2014, 2014 INTERNATIONAL CONFERENCE ON CYBERWORLDS (CW), P237, DOI 10.1109/CW.2014.40
   Yu LF, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366153
   Yu Y., 2017, ARXIV171108976CSIR
   Zhang DY, 2017, IEEE T IMAGE PROCESS, V26, DOI 10.1109/TIP.2016.2623485
   Zhang P, 2016, PROCEEDINGS OF THE 2016 SYMPOSIUM ON ARCHITECTURES FOR NETWORKING AND COMMUNICATIONS SYSTEMS (ANCS'16), P81, DOI 10.1145/2881025.2881038
   Zhang Y, 2017, IEEE T IMAGE PROCESS, V26, P464, DOI 10.1109/TIP.2016.2628581
   Zhong C. L., 2017, DEEP MULTILABEL HASH, P169
NR 45
TC 1
Z9 1
U1 0
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2019
VL 35
IS 5
BP 653
EP 666
DI 10.1007/s00371-018-1495-9
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HZ0IT
UT WOS:000468524900004
DA 2024-07-18
ER

PT J
AU Filip, J
   Kolafová, M
   Havlícek, M
   Vávra, R
   Haindl, M
   Rushmeier, H
AF Filip, Jiri
   Kolafova, Martina
   Havlicek, Michal
   Vavra, Radomir
   Haindl, Michal
   Rushmeier, Holly
TI Evaluating physical and rendered material appearance
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 35th Computer Graphics International conference (CGI)
CY JUN 11-14, 2018
CL INDONESIA
SP Comp Graph Soc, Nanyang Technol Univ
DE Material appearance; Rendering; BTF; Perception; Psychophysics; MAM2014
ID TEXTURE; REFLECTANCE; PERCEPTION
AB Many representations and rendering techniques have been proposed for presenting material appearance in computer graphics. One outstanding problem is evaluating their accuracy. In this paper, we propose assessing accuracy by comparing human judgements of material attributes made when viewing a computer graphics rendering to those made when viewing a physical sample of the same material. We demonstrate this approach using 16 diverse physical material samples distributed to researchers at the MAM 2014 workshop. We performed two psychophysical experiments. In the first experiment, we examined how consistently subjects rate a set of twelve visual, tactile and subjective attributes of individual physical material specimens. In the second experiment, we asked subjects to assess the same attributes for identical materials rendered as BTFs under point-light and environment illuminations. By analyzing obtained data, we identified which material attributes and material types are judged consistently and to what extent the computer graphics representation conveyed the experience of viewing physical material appearance.
C1 [Filip, Jiri; Kolafova, Martina; Havlicek, Michal; Vavra, Radomir; Haindl, Michal] Czech Acad Sci, Inst Informat Theory & Automat, Prague, Czech Republic.
   [Rushmeier, Holly] Yale Univ, Comp Sci, New Haven, CT USA.
C3 Czech Academy of Sciences; Institute of Information Theory & Automation
   of the Czech Academy of Sciences; Yale University
RP Filip, J (corresponding author), Czech Acad Sci, Inst Informat Theory & Automat, Prague, Czech Republic.
EM filipj@utia.cas.cz
RI Filip, Jiri/D-3396-2012; Haindl, Michal/R-4909-2019; Vavra,
   Radomir/H-4349-2014; Haindl, Michal/H-4323-2014
OI Haindl, Michal/0000-0001-8159-3685; Rushmeier, Holly/0000-0001-5241-0886
FU Czech Science Foundation [17-18407S]; US National Science Foundation
   [IIS-1218515]
FX This research has been supported by the Czech Science Foundation Grant
   17-18407S and the US National Science Foundation Grant IIS-1218515.
CR Adelson E. H., 2001, HUMAN VISION ELECT I, V4299
   [Anonymous], 1977, NATL BUR STAND MONOG, DOI [10.1109/LPT.2009.2020494, DOI 10.1109/LPT.2009.2020494]
   [Anonymous], 2013, C COMP VIS PATT REC
   Brodatz P., 1966, PHOTOGRAPHIC ALBUM A
   Dana KJ, 1999, ACM T GRAPHIC, V18, P1, DOI 10.1145/300776.300778
   Filip J, 2015, ACM SAP, P25
   Filip J., 2011, LNCS, V7252, P160
   Filip J, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409091
   Fleming RW, 2014, VISION RES, V94, P62, DOI 10.1016/j.visres.2013.11.004
   Goodman T., P 3 INT C APP, P21
   Havran V, 2016, COMPUT GRAPH FORUM, V35, P1, DOI 10.1111/cgf.12944
   Hayes AF, 2007, COMMUN METHODS MEAS, V1, P77, DOI 10.1080/19312450709336664
   ITU, 2008, ITU R REC P 910 SUBJ
   Jarabo A, 2014, IEEE T VIS COMPUT GR, V20, P880, DOI 10.1109/TVCG.2014.2312016
   Keelan B. W., 2003, Proceedings of the SPIE - The International Society for Optical Engineering, V5294, P181, DOI 10.1117/12.532064
   Long HH, 2002, INT C PATT RECOG, P135, DOI 10.1109/ICPR.2002.1044631
   Martín R, 2016, SAP 2015: ACM SIGGRAPH SYMPOSIUM ON APPLIED PERCEPTION, P33, DOI 10.1145/2804408.2804420
   Matusik W, 2003, ACM T GRAPHIC, V22, P759, DOI 10.1145/882262.882343
   Meseth J., 2006, APGV '06, P127
   Mojsilovic A, 2000, IEEE T IMAGE PROCESS, V9, P417, DOI 10.1109/83.826779
   Motoyoshi I, 2007, NATURE, V447, P206, DOI 10.1038/nature05724
   Mylo M., 2017, P C VIS MOD VIS, P9
   Ramanarayanan G, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1276377.1276472, 10.1145/1239451.1239527]
   Rao AR, 1996, VISION RES, V36, P1649, DOI 10.1016/0042-6989(95)00202-2
   Rushmeier H., 2014, P EUR 2014 WORKSH MA, P25, DOI [10.2312/mam.20141297, DOI 10.2312/MAM.20141297]
   Sattler M., 2003, Eurographics Symposium on Rendering. 14th Eurographics Workshop on Rendering, P167
   Serrano A, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980242
   Somol P., 2005, P WSCG 2005 PLZEN CZ, P155
   TAMURA H, 1978, IEEE T SYST MAN CYB, V8, P460, DOI 10.1109/TSMC.1978.4309999
   Vangorp P, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1276377.1276473, 10.1145/1239451.1239528]
NR 30
TC 4
Z9 4
U1 0
U2 11
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2018
VL 34
IS 6-8
SI SI
BP 805
EP 816
DI 10.1007/s00371-018-1545-3
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA GH6MC
UT WOS:000433557400006
DA 2024-07-18
ER

PT J
AU Wang, XG
   Zhou, B
   Wang, ZJ
   Zou, DQ
   Chen, XW
   Zhao, QP
AF Wang, Xiaogang
   Zhou, Bin
   Wang, Zongji
   Zou, Dongqing
   Chen, Xiaowu
   Zhao, Qinping
TI Efficiently consistent affinity propagation for 3D shapes
   co-segmentation
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 35th Computer Graphics International conference (CGI)
CY JUN 11-14, 2018
CL INDONESIA
SP Comp Graph Soc, Nanyang Technol Univ
DE Shape co-segmentation; Shape analysis; Affinity propagation
ID OBJECT RECOGNITION; MESH SEGMENTATION
AB Unsupervised co-segmentation for a set of 3D shapes is a challenging problem as no prior information is provided. The accuracy of the current approaches is necessarily restricted by the accuracy of the unsupervised face classification, which is used to provide an initialization for the following optimization to improve the consistency between adjacent faces. However, it is exceedingly difficult to obtain a satisfactory initialization pre-segmentation owing to variation in topology and geometry of 3D shapes. In this study, we consider the unsupervised 3D shape co-segmentation as an exemplar-based clustering problem, aimed at simultaneously discovering optimal exemplars and obtaining co-segmentation results. Therefore, we introduce a novel exemplar-based clustering method based on affinity propagation for 3D shape co-segmentation, which can automatically identify representative exemplars and patterns in 3D shapes considering the high-order statistics, yielding consistent and accurate co-segmentation results. Experiments using various datasets, especially large sets with 200 or more shapes that would be challenging to manually segment, demonstrate that our method exhibits a better performance compared to state-of-the-art methods.
C1 [Wang, Xiaogang; Zhou, Bin; Wang, Zongji; Zou, Dongqing; Chen, Xiaowu; Zhao, Qinping] Beihang Univ, Sch Comp Sci & Engn, State Key Lab Virtual Real Technol & Syst, Beijing, Peoples R China.
C3 Beihang University
RP Zhou, B (corresponding author), Beihang Univ, Sch Comp Sci & Engn, State Key Lab Virtual Real Technol & Syst, Beijing, Peoples R China.
EM wangxiaogang@buaa.edu.cn; zhoubin@buaa.edu.cn; wzjgintoki@buaa.edu.cn;
   zoudq@buaa.edu.cn; chen@buaa.edu.cn; zhaoqp@buaa.edu.cn
RI Wang, Xiaogang/B-2439-2013; yang, qing/JBR-8440-2023; zhang,
   weijie/JQX-1450-2023
OI Wang, Xiaogang/0000-0002-7929-5889; 
FU National Natural Science Foundation of China [61502023, U1736217]
FX This work was supported by National Natural Science Foundation of China
   (Grant No. 61502023) and National Natural Science Foundation of China
   (Grant No. U1736217).
CR [Anonymous], 2010, ACM SIGGRAPH 2010 papers
   [Anonymous], 2012, ACM T GRAPHICS TOG, DOI DOI 10.1145/2185520.2185553
   Belongie S, 2002, IEEE T PATTERN ANAL, V24, P509, DOI 10.1109/34.993558
   Chaudhuri S, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964930
   Chen Kang, 2015, ACM T GRAPHIC, DOI DOI 10.1145/2816795.2818096
   Chen XB, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531379
   Frey BJ, 2007, SCIENCE, V315, P972, DOI 10.1126/science.1136800
   Gal R, 2006, ACM T GRAPHIC, V25, P130, DOI 10.1145/1122501.1122507
   Golovinskiy A, 2009, COMPUT GRAPH-UK, V33, P262, DOI 10.1016/j.cag.2009.03.010
   Hilaga M, 2001, COMP GRAPH, P203, DOI 10.1145/383259.383282
   Hu RZ, 2012, COMPUT GRAPH FORUM, V31, P1703, DOI 10.1111/j.1467-8659.2012.03175.x
   Huang QX, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024159
   Jain S, 2004, J COMPUT GRAPH STAT, V13, P158, DOI 10.1198/1061860043001
   Johnson AE, 1999, IEEE T PATTERN ANAL, V21, P433, DOI 10.1109/34.765655
   Kalogerakis E, 2017, PROC CVPR IEEE, P6630, DOI 10.1109/CVPR.2017.702
   Kim J, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1966394.1966403
   Kim VG, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461933
   Kohli P, 2009, IEEE T PATTERN ANAL, V31, P1645, DOI 10.1109/TPAMI.2008.217
   Krahenbuhl P., ARXIV12105644 CORR
   Laga H, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2516971.2516975
   Liu R, 2009, COMPUT GRAPH FORUM, V28, P397, DOI 10.1111/j.1467-8659.2009.01379.x
   Luo P, 2013, VISUAL COMPUT, V29, P587, DOI 10.1007/s00371-013-0824-2
   Lv JJ, 2012, COMPUT GRAPH FORUM, V31, P2241, DOI 10.1111/j.1467-8659.2012.03217.x
   MacQueen J., 1967, P 5 BERK S MATH STAT, P281
   Shamir A, 2008, COMPUT GRAPH FORUM, V27, P1539, DOI 10.1111/j.1467-8659.2007.01103.x
   Shao TJ, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2661229.2661288
   Shapira L, 2010, INT J COMPUT VISION, V89, P309, DOI 10.1007/s11263-009-0279-0
   Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688
   Sidi O, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024160
   van Kaick O, 2011, COMPUT GRAPH FORUM, V30, P553, DOI 10.1111/j.1467-8659.2011.01893.x
   Wang FR, 2014, INT J INNOV TECHNOL, V11, DOI 10.1142/S0219877014500138
   Wang YH, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508393
   Wang YH, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366184
   Wu ZZ, 2013, COMPUT GRAPH-UK, V37, P628, DOI 10.1016/j.cag.2013.05.015
   Xie ZG, 2014, COMPUT GRAPH FORUM, V33, P85, DOI 10.1111/cgf.12434
   Xu K, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866206
   Yang Y, 2013, IEEE T VIS COMPUT GR, V19, P1633, DOI 10.1109/TVCG.2013.12
   Yu YZ, 2004, ACM T GRAPHIC, V23, P644, DOI 10.1145/1015706.1015774
NR 38
TC 4
Z9 6
U1 0
U2 13
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2018
VL 34
IS 6-8
SI SI
BP 997
EP 1008
DI 10.1007/s00371-018-1538-2
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA GH6MC
UT WOS:000433557400022
DA 2024-07-18
ER

PT J
AU Agarwal, S
   Santra, B
   Mukherjee, DP
AF Agarwal, Swapna
   Santra, Bikash
   Mukherjee, Dipti Prasad
TI <i>Anubhav</i>: recognizing emotions through facial expression
SO VISUAL COMPUTER
LA English
DT Article
DE Anubhav; Facial expression recognition; Entropy; Salient face region;
   Local binary pattern
ID REAL-TIME; RECOGNITION; SYSTEM; PATTERNS
AB We present a computer vision-based system named Anubhav (a Hindi word meaning feeling) which recognizes emotional facial expressions from streaming face videos. Our system runs at a speed of 10 frames per second (fps) on a 3.2-GHz desktop and at 3 fps on an Android mobile device. Using entropy and correlation-based analysis, we show that some particular salient regions of face image carry major expression-related information compared with other face regions. We also show that spatially close features within a salient face region carry correlated information regarding expression. Therefore, only a few features from each salient face region are enough for expression representation. Extraction of only a few features considerably saves response time. Exploitation of expression information from spatial as well as temporal dimensions gives good recognition accuracy. We have done extensive experiments on two publicly available data sets and also on live video streams. The recognition accuracies on benchmark CK data set and on live video stream by our system are at least 13 and 20 % better, respectively, compared to competing approaches.
C1 [Agarwal, Swapna; Santra, Bikash; Mukherjee, Dipti Prasad] Indian Stat Inst, Elect & Commun Sci Unit, 203 BT Rd, Kolkata, India.
C3 Indian Statistical Institute; Indian Statistical Institute Kolkata
RP Agarwal, S (corresponding author), Indian Stat Inst, Elect & Commun Sci Unit, 203 BT Rd, Kolkata, India.
EM agarwal.swapna@gmail.com; bikashsantra85@gmail.com;
   diptiprasad.mukherjee@gmail.com
OI Santra, Bikash/0000-0002-6833-140X; Agarwal, Swapna/0000-0002-2909-8342
FU Department of Science and Technology, Government of India
   [SR/WOS-A/ET-53/2012(G)]
FX The work of Swapna Agarwal was supported by Department of Science and
   Technology, Government of India Project No. SR/WOS-A/ET-53/2012(G).
CR Agarwal S, 2012, P 8 INDIAN C COMP VI
   Agarwal S, 2015, NAT CONF COMPUT VIS
   Agarwal S, 2013, IEEE INT CONF AUTOMA
   An KH, 2009, IEEE T CONSUM ELECTR, V55, P2271, DOI 10.1109/TCE.2009.5373798
   Anand B, 2012, CONSUM COMM NETWORK, P255, DOI 10.1109/CCNC.2012.6181097
   [Anonymous], VISUAL COMMUNICATION
   [Anonymous], 2015, P IEEE INT C AUT FAC, DOI DOI 10.1109/FG.2015.7163082
   [Anonymous], HARPER PERENNIAL
   [Anonymous], JULES RENOUARD
   [Anonymous], INT WORKSH IM AN MUL
   Bacivarov I, 2010, IEEE T CONSUM ELECTR, V56, P289, DOI 10.1109/TCE.2010.5505930
   Baltrusaitis T., 2011, Proceedings 2011 IEEE International Conference on Automatic Face & Gesture Recognition (FG 2011), P909, DOI 10.1109/FG.2011.5771372
   Bejani M, 2014, NEURAL COMPUT APPL, V24, P399, DOI 10.1007/s00521-012-1228-3
   Berretti S, 2013, VISUAL COMPUT, V29, P1333, DOI 10.1007/s00371-013-0869-2
   Berretti S, 2011, VISUAL COMPUT, V27, P1021, DOI 10.1007/s00371-011-0611-x
   Bihan Jiang, 2011, Proceedings 2011 IEEE International Conference on Automatic Face & Gesture Recognition (FG 2011), P314, DOI 10.1109/FG.2011.5771416
   Boughrara H, 2016, MULTIMED TOOLS APPL, V75, P709, DOI 10.1007/s11042-014-2322-6
   Bradski G, 2000, DR DOBBS J, V25, P120
   Chew SW, 2011, LECT NOTES COMPUT SC, V7088, P311, DOI 10.1007/978-3-642-25346-1_28
   Danelakis A, 2016, VISUAL COMPUT, V32, P1001, DOI 10.1007/s00371-016-1243-y
   Ekman P, 1978, FACIAL ACTION CODING
   Erden F, 2014, IEEE T CONSUM ELECTR, V60, P675, DOI 10.1109/TCE.2014.7027342
   Esau N., 2007, INT FUZZY SYSTEMS C, P1
   Hou XN, 2016, VISUAL COMPUT, V32, P479, DOI 10.1007/s00371-015-1079-x
   Hsu FS, 2014, MULTIMED TOOLS APPL, V73, P309, DOI 10.1007/s11042-013-1616-4
   Jeong JW, 2014, IEEE T CONSUM ELECTR, V60, P92, DOI 10.1109/TCE.2014.6780930
   Kanade T., 2000, Proceedings Fourth IEEE International Conference on Automatic Face and Gesture Recognition (Cat. No. PR00580), P46, DOI 10.1109/AFGR.2000.840611
   Khan RA, 2013, PATTERN RECOGN LETT, V34, P1159, DOI 10.1016/j.patrec.2013.03.022
   Lajevardi SM, 2012, SIGNAL IMAGE VIDEO P, V6, P159, DOI 10.1007/s11760-010-0177-5
   Li HF, 2008, CURR MED RES OPIN, V24, P1, DOI 10.1088/0256-307X/24/3/072
   Lian SG, 2014, IEEE T CONSUM ELECTR, V60, P107, DOI 10.1109/TCE.2014.6780932
   Littlewort G., 2011, Proceedings 2011 IEEE International Conference on Automatic Face & Gesture Recognition (FG 2011), P298, DOI 10.1109/FG.2011.5771414
   Lucey P., 2010, IEEE COMP SOC C COMP, P94, DOI 10.1109/CVPRW.2010.5543262
   Maeda A, 2014, IEEE T CONSUM ELECTR, V60, P453, DOI 10.1109/TCE.2014.6937330
   Rashid M, 2013, VISUAL COMPUT, V29, P1269, DOI 10.1007/s00371-012-0768-y
   Sanchez A, 2011, NEUROCOMPUTING, V74, P1272, DOI 10.1016/j.neucom.2010.07.017
   Shan C., 2006, Proc. BMVC, V1, P297
   Shan CF, 2009, IMAGE VISION COMPUT, V27, P803, DOI 10.1016/j.imavis.2008.08.005
   Suk M, 2014, IEEE COMPUT SOC CONF, P132, DOI 10.1109/CVPRW.2014.25
   Truong A, 2016, VISUAL COMPUT, V32, P83, DOI 10.1007/s00371-014-1057-8
   Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb
   Wang J, 2007, COMPUT VIS IMAGE UND, V108, P19, DOI 10.1016/j.cviu.2006.10.011
   Wang ZF, 2014, VISUAL COMPUT, V30, P359, DOI 10.1007/s00371-013-0861-x
   Whitehill J, 2009, IEEE T PATTERN ANAL, V31, P2106, DOI 10.1109/TPAMI.2009.42
   Xiao R, 2011, PATTERN RECOGN, V44, P107, DOI 10.1016/j.patcog.2010.07.017
   Xiong XH, 2013, PROC CVPR IEEE, P532, DOI 10.1109/CVPR.2013.75
   Yan WJ, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0086041
   Zhang X, 2014, LECT NOTES COMPUT SC, V8888, P809, DOI 10.1007/978-3-319-14364-4_78
   Zhao GY, 2007, IEEE T PATTERN ANAL, V29, P915, DOI 10.1109/TPAMI.2007.1110
NR 49
TC 37
Z9 38
U1 2
U2 16
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2018
VL 34
IS 2
BP 177
EP 191
DI 10.1007/s00371-016-1323-z
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FU6ZO
UT WOS:000424001800004
OA Bronze
DA 2024-07-18
ER

PT J
AU Hatchett, J
   Debattista, K
   Mukherjee, R
   Bashford-Rogers, T
   Chalmers, A
AF Hatchett, Jonathan
   Debattista, Kurt
   Mukherjee, Ratnajit
   Bashford-Rogers, Thomas
   Chalmers, Alan
TI An evaluation of power transfer functions for HDR video compression
SO VISUAL COMPUTER
LA English
DT Article
DE HDR video compression; Power Transfer Function; Video compression
   metrics
ID STANDARD
AB High dynamic range (HDR) imaging enables the full range of light in a scene to be captured, transmitted and displayed. However, uncompressed 32-bit HDR is four times larger than traditional low dynamic range (LDR) imagery. If HDR is to fulfil its potential for use in live broadcasts and interactive remote gaming, fast, efficient compression is necessary for HDR video to be manageable on existing communications infrastructure. A number of methods have been put forward for HDR video compression. However, these can be relatively complex and frequently require the use of multiple video streams. In this paper, we propose the use of a straightforward Power Transfer Function (PTF) as a practical, computationally fast, HDR video compression solution. The use of PTF is presented and evaluated against four other HDR video compression methods. An objective evaluation shows that PTF exhibits improved quality at a range of bit-rates and, due to its straightforward nature, is highly suited for real-time HDR video applications.
C1 [Hatchett, Jonathan; Debattista, Kurt; Mukherjee, Ratnajit; Bashford-Rogers, Thomas; Chalmers, Alan] Univ Warwick, WMG, Coventry, W Midlands, England.
C3 University of Warwick
RP Hatchett, J (corresponding author), Univ Warwick, WMG, Coventry, W Midlands, England.
EM j.p.hatchett@warwick.ac.uk
CR [Anonymous], 1834, De pulsu, resorptione, auditu et tactu: Annotationes, anatomical et physiological On pulse, resorption, hearing and touch: Annotations, anatomical and physiological
   [Anonymous], 20842014 ST SMPTE
   [Anonymous], 2012, 2012 ANN TECHN C EXH, DOI DOI 10.5594/M001446
   Association N. E. M. of Radiology A. C., 1998, DIG IM COMM MED D 14
   Association ofRadio Industries and Businesses (ARIB), 2015, STBB67 ARIB
   Aydin T. O., 2008, ELECT IMAGING 2008
   Banterle F, 2011, ADVANCED HIGH DYNAMIC RANGE IMAGING: THEORY AND PRACTICE, P1
   Banterle F., 2010, EP Patent, Patent No. [2,144,444, 2144444]
   BARTEN PGJ, 1992, P SOC PHOTO-OPT INS, V1666, P57, DOI 10.1117/12.135956
   Bjontegaard G., 2001, Document VCEG-M33
   Borer T., 2014, NON LINEAR OPTO ELEC
   Borer T., 2015, A "display independent" high dynamic range television system
   Chalmers A., 2015, 12 BITS IS SIMPLY NO
   Debattista K, 2015, VISUAL COMPUT, V31, P1089, DOI 10.1007/s00371-015-1121-z
   Eisemann E, 2004, ACM T GRAPHIC, V23, P673, DOI 10.1145/1015706.1015778
   Fechner G.T., 1838, ANN PHYS, V121, P227
   Ferwerda J. A., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P249, DOI 10.1145/237170.237262
   Froehlich J., 2014, CREATING CINEMATIC W
   Garbas JU, 2011, INT CONF ACOUST SPEE, P829
   Hanhart P, 2015, EURASIP J IMAGE VIDE, DOI 10.1186/s13640-015-0091-4
   Larson G. W., 1998, Journal of Graphics Tools, V3, P15, DOI 10.1080/10867651.1998.10487485
   Lee C., 2008, P 16 EUR SIGN PROC C
   Mantiuk R, 2004, ACM T GRAPHIC, V23, P733, DOI 10.1145/1015706.1015794
   Mantiuk R., 2006, ELECT IMAGING 2006
   Mantiuk R, 2006, ACM T GRAPHIC, V25, P713, DOI 10.1145/1141911.1141946
   Mantiuk R, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964935
   Motra A, 2010, IEEE IMAGE PROC, P2061, DOI 10.1109/ICIP.2010.5654069
   Mukherjee R, 2016, SIGNAL PROCESS-IMAGE, V47, P426, DOI 10.1016/j.image.2016.08.001
   Scheunders P, 1996, PATTERN RECOGN LETT, V17, P547, DOI 10.1016/0167-8655(96)00011-6
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Ward Greg., 2004, Proceedings of the 1st Symposium on Applied Perception in Graphics and Visualization, APGV '04, P83
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Zhang Y, 2011, IEEE IMAGE PROC, P1321, DOI 10.1109/ICIP.2011.6115679
NR 33
TC 6
Z9 6
U1 0
U2 1
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2018
VL 34
IS 2
BP 167
EP 176
DI 10.1007/s00371-016-1322-0
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FU6ZO
UT WOS:000424001800003
OA hybrid
DA 2024-07-18
ER

PT J
AU Wan, LL
   Zou, CQ
   Zhang, H
AF Wan, Lili
   Zou, Changqing
   Zhang, Hao
TI Full and partial shape similarity through sparse descriptor
   reconstruction
SO VISUAL COMPUTER
LA English
DT Article
DE Full and partial shape similarity; Incomplete shapes; Shape retrieval;
   Sparse dictionary learning; Sparse reconstruction
ID RETRIEVAL; SIGNATURE; FEATURES
AB We introduce a novel approach to measuring similarity between two shapes based on sparse reconstruction of shape descriptors. The main feature of our approach is its applicability in situations where either of the two shapes may have moderate to significant portions of its data missing. Let the two shapes be A and B. Without loss of generality, we characterize A by learning a sparse dictionary from its local descriptors. The similarity between A and B is defined by the error incurred when reconstructing B's descriptor set using the basis signals from A's dictionary. Benefits of using sparse dictionary learning and reconstruction are twofold. First, sparse dictionary learning reduces data redundancy and facilitates similarity computations. More importantly, the reconstruction error is expected to be small as long as B is similar to A, regardless of whether the similarity is full or partial. Our proposed approach achieves significant improvements over previous works when retrieving non-rigid shapes with missing data, and it is also comparable to state-of-the-art methods on the retrieval of complete non-rigid shapes.
C1 [Wan, Lili] Beijing Jiaotong Univ, Inst Informat Sci, Beijing, Peoples R China.
   [Zou, Changqing] Hengyang Normal Univ, Hengyang, Peoples R China.
   [Zou, Changqing; Zhang, Hao] Simon Fraser Univ, Burnaby, BC, Canada.
C3 Beijing Jiaotong University; Hengyang Normal University; Simon Fraser
   University
RP Wan, LL (corresponding author), Beijing Jiaotong Univ, Inst Informat Sci, Beijing, Peoples R China.; Zou, CQ (corresponding author), Hengyang Normal Univ, Hengyang, Peoples R China.; Zou, CQ (corresponding author), Simon Fraser Univ, Burnaby, BC, Canada.
EM llwan@bjtu.edu.cn; aaronzou1125@gmail.com; haoz@sfu.ca
RI Zhang, Hao/HHM-1940-2022; wu, tong/ITV-6896-2023; Zou,
   Changqing/AAK-8510-2020
OI Wan, Lili/0000-0002-9520-5425; Zhang, Hao/0000-0003-1991-119X
FU China Scholarship Council; National Natural Science Foundation of China
   [61572064, 61502153]; Fundamental Research Funds for the Central
   Universities of China [2014JBM027]; Natural Science Foundation of Hunan
   Province of China [2016JJ3031]; National 973 Program [2011CB302203];
   NSERC [611370]
FX The work is supported in part by grants from China Scholarship Council,
   National Natural Science Foundation of China (61572064 and 61502153),
   the Fundamental Research Funds for the Central Universities of China
   (2014JBM027), Natural Science Foundation of Hunan Province of China
   (2016JJ3031), National 973 Program (2011CB302203) and NSERC (611370).
CR Aharon M, 2006, IEEE T SIGNAL PROCES, V54, P4311, DOI 10.1109/TSP.2006.881199
   [Anonymous], TECHNICAL REPORT
   [Anonymous], 2003, 7 CENTR EUR SEM COMP
   Aubry Mathieu, 2011, Pattern Recognition. Proceedings 33rd DAGM Symposium, P122, DOI 10.1007/978-3-642-23123-0_13
   Aubry M, 2011, IEEE I CONF COMP VIS, P1411, DOI 10.1109/ICCV.2011.6126396
   Biasotti S, 2006, COMPUT AIDED DESIGN, V38, P1002, DOI 10.1016/j.cad.2006.07.003
   Boscaini D, 2014, VISUAL COMPUT, V30, P1233, DOI 10.1007/s00371-014-0938-1
   Bronstein AM, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1899404.1899405
   Del Bimbo A, 2006, ACM T MULTIM COMPUT, V2, P20
   Dey TK, 2010, COMPUT GRAPH FORUM, V29, P1545, DOI 10.1111/j.1467-8659.2010.01763.x
   Elad M, 2010, SPARSE AND REDUNDANT REPRESENTATIONS, P3, DOI 10.1007/978-1-4419-7011-4_1
   Ferreira A, 2010, INT J COMPUT VISION, V89, P327, DOI 10.1007/s11263-009-0257-6
   Funkhouser T., 2006, P 4 EUROGRAPHICS S G, P131
   Gal R, 2006, ACM T GRAPHIC, V25, P130, DOI 10.1145/1122501.1122507
   Godil A., 2015, P 2015 EUR WORKSH 3D, P153
   Itskovich A, 2011, COMPUT GRAPH-UK, V35, P334, DOI 10.1016/j.cag.2010.11.010
   Jain V, 2007, COMPUT AIDED DESIGN, V39, P398, DOI 10.1016/j.cad.2007.02.009
   Jones PW, 2008, P NATL ACAD SCI USA, V105, P1803, DOI 10.1073/pnas.0710175104
   Lavoué G, 2012, VISUAL COMPUT, V28, P931, DOI 10.1007/s00371-012-0724-x
   Lian Z., 2015, PROC 8 EUROGRAPHICS, P107
   Litman R, 2014, IEEE T PATTERN ANAL, V36, P171, DOI 10.1109/TPAMI.2013.148
   Litman R, 2014, COMPUT GRAPH FORUM, V33, P127, DOI 10.1111/cgf.12438
   Liu ZB, 2015, NEUROCOMPUTING, V151, P583, DOI 10.1016/j.neucom.2014.06.090
   Mairal J., 2009, P 26 ANN INT C MACHI, P689, DOI 10.1145/1553374.1553463
   Mairal J, 2010, J MACH LEARN RES, V11, P19
   Osada R, 2002, ACM T GRAPHIC, V21, P807, DOI 10.1145/571647.571648
   Quan LL, 2015, COMPUT AIDED DESIGN, V59, P119, DOI 10.1016/j.cad.2014.09.005
   Reuter M, 2006, COMPUT AIDED DESIGN, V38, P342, DOI 10.1016/j.cad.2005.10.011
   Shapira L, 2010, INT J COMPUT VISION, V89, P309, DOI 10.1007/s11263-009-0279-0
   Shilane P, 2004, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, P167, DOI 10.1109/smi.2004.1314504
   Sipiran I, 2014, VISUAL COMPUT, V30, P1293, DOI 10.1007/s00371-014-0937-2
   Sun JA, 2009, COMPUT GRAPH FORUM, V28, P1383, DOI 10.1111/j.1467-8659.2009.01515.x
   Tangelder JH, 2008, MULTIMED TOOLS APPL, V39, P441, DOI 10.1007/s11042-007-0181-0
   Tierny J, 2009, COMPUT GRAPH FORUM, V28, P41, DOI 10.1111/j.1467-8659.2008.01190.x
   Toldo R, 2010, VISUAL COMPUT, V26, P1257, DOI 10.1007/s00371-010-0519-x
   Tosic I, 2011, IEEE SIGNAL PROC MAG, V28, P27, DOI 10.1109/MSP.2010.939537
   van Kaick O, 2014, ACM T GRAPHIC, V34, DOI 10.1145/2611811
   van Kaick O, 2013, COMPUT GRAPH FORUM, V32, P189, DOI 10.1111/cgf.12084
   van Kaick O, 2011, COMPUT GRAPH FORUM, V30, P1681, DOI 10.1111/j.1467-8659.2011.01884.x
   Vranic DV, 2005, 2005 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO (ICME), VOLS 1 AND 2, P963
   Wan L., 2013, PACIFIC GRAPHICS SHO, P11
   Zou CQ, 2014, IEEE SIGNAL PROC LET, V21, P966, DOI 10.1109/LSP.2014.2321764
NR 42
TC 4
Z9 4
U1 0
U2 15
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2017
VL 33
IS 12
BP 1497
EP 1509
DI 10.1007/s00371-016-1293-1
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FK4JE
UT WOS:000413458600002
DA 2024-07-18
ER

PT J
AU Jung, J
   Kim, B
   Lee, JY
   Kim, B
   Lee, S
AF Jung, Jinwoong
   Kim, Beomseok
   Lee, Joon-Young
   Kim, Byungmoon
   Lee, Seungyong
TI Robust upright adjustment of 360 spherical panoramas
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 34th Conference on Computer Graphics International (CGI)
CY JUN 27-30, 2017
CL Yokohama, JAPAN
SP Keio Univ, Fac Sci & Technol
DE Spherical panorama; Upright adjustment; Camera calibration
ID LINE SEGMENT DETECTOR
AB With the recent advent of 360 cameras, spherical panorama images are becoming more popular and widely available. In a spherical panorama, alignment of the scene orientation to the image axes is important for providing comfortable and pleasant viewing experiences using VR headsets and traditional displays. This paper presents an automatic method for upright adjustment of 360 spherical panorama images without any prior information, such as depths and gyro sensor data. We take the Atlanta world assumption and use the horizontal and vertical lines in the scene to formulate a cost function for upright adjustment. In addition to fast optimization of the cost function, our method includes outlier handling to improve the robustness and accuracy of upright adjustment. Our method produces visually pleasing results for a variety of real-world spherical panoramas in less than a second, and the accuracy is verified using ground-truth data.
C1 [Jung, Jinwoong; Kim, Beomseok; Lee, Seungyong] POSTECH, Dept Comp Sci & Engn, Pohang, South Korea.
   [Lee, Joon-Young; Kim, Byungmoon] Adobe Res, San Jose, CA USA.
C3 Pohang University of Science & Technology (POSTECH); Adobe Systems Inc.
RP Lee, S (corresponding author), POSTECH, Dept Comp Sci & Engn, Pohang, South Korea.
EM ca00229@postech.ac.kr; beomseok0203@postech.ac.kr; jolee@adobe.com;
   bmkim@adobe.com; leesy@postech.ac.kr
FU Institute for Information & communications Technology Promotion (IITP)
   [R0126-17-1078]; National Research Foundation of Korea (NRF)
   [NRF-2014R1A2A1A11052779]; Korea Creative Content Agency (KOCCA)
   [APP-0120150512002]; Korea government (MSIP, MCST)
FX This work was supported by Institute for Information & communications
   Technology Promotion (IITP) grant (R0126-17-1078), the National Research
   Foundation of Korea (NRF) grant (NRF-2014R1A2A1A11052779), and Korea
   Creative Content Agency (KOCCA) grant (APP-0120150512002), funded by the
   Korea government (MSIP, MCST).
CR Akinlar C, 2011, PATTERN RECOGN LETT, V32, P1633, DOI 10.1016/j.patrec.2011.06.001
   Bazin JC, 2012, INT J ROBOT RES, V31, P63, DOI 10.1177/0278364911421954
   Bosse M., 2002, P IEEE INT C IMAG PR
   Gallagher AC, 2005, 2ND CANADIAN CONFERENCE ON COMPUTER AND ROBOT VISION, PROCEEDINGS, P460, DOI 10.1109/CRV.2005.84
   He KM, 2013, IEEE I CONF COMP VIS, P553, DOI 10.1109/ICCV.2013.74
   Kamaluddin M.A., 2011, P IAPR C MACH VIS AP, P177
   Kopf J, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2982405
   Kopf J, 2009, COMPUT GRAPH FORUM, V28, P1083, DOI 10.1111/j.1467-8659.2009.01485.x
   Lee H, 2014, IEEE T PATTERN ANAL, V36, P833, DOI 10.1109/TPAMI.2013.166
   Luo CS, 2015, NAT COMMUN, V6, DOI 10.1038/ncomms10041
   Scaramuzza D., 2008, THESIS
   Schindler G, 2004, PROC CVPR IEEE, P203
   von Gioi RG, 2010, IEEE T PATTERN ANAL, V32, P722, DOI 10.1109/TPAMI.2008.300
   Wang XB, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P829, DOI 10.1145/2647868.2655000
NR 14
TC 23
Z9 27
U1 0
U2 7
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2017
VL 33
IS 6-8
BP 737
EP 747
DI 10.1007/s00371-017-1368-7
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA EX1EY
UT WOS:000402964800006
DA 2024-07-18
ER

PT J
AU Qi, W
   Han, J
   Zhang, Y
   Bai, LF
AF Qi, Wei
   Han, Jing
   Zhang, Yi
   Bai, Lianfa
TI Saliency detection via Boolean and foreground in a dynamic Bayesian
   framework
SO VISUAL COMPUTER
LA English
DT Article
DE Propagation mechanism; Dynamic Bayesian; Weight matrix
ID REGION DETECTION
AB The goal of saliency detection is to locate important regions in an image which attract viewers' attention the most. In this paper, we propose a dynamic Bayesian model for saliency detection in which both Boolean-based and foreground-based models are exploited. First, a preliminary saliency map is constructed based on multi-channel Boolean maps, and a propagation mechanism is utilized to further modify the saliency map by learning a new weight matrix based on color and spatial structure information. Second, a foreground-based model based on foreground seeds from Boolean-based model is generated to detect salient pixels, and a better result is obtained by applying the edge map and a new weight matrix. Finally, pixel-level saliency is computed using a dynamic Bayesian framework. Both qualitative and quantitative evaluations on several benchmark datasets demonstrate robustness and effectiveness of our approach against state-of-the-art approaches.
C1 [Qi, Wei; Han, Jing; Zhang, Yi; Bai, Lianfa] Nanjing Univ Sci & Technol, Jiangsu Key Lab Spectral Imaging & Intelligence S, Nanjing 210094, Jiangsu, Peoples R China.
C3 Nanjing University of Science & Technology
RP Bai, LF (corresponding author), Nanjing Univ Sci & Technol, Jiangsu Key Lab Spectral Imaging & Intelligence S, Nanjing 210094, Jiangsu, Peoples R China.
EM weiq.cs@gmail.com; Jinghan@163.com; zhangyi441@sina.com; mrblf@163.com
FU National Natural Science Foundation of China [61231014]
FX This work is supported by the National Natural Science Foundation of
   China (Grant No. 61231014).
CR Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   [Anonymous], IEEE CVPR
   [Anonymous], 2006, NIPS
   [Anonymous], IEEE CVPR
   [Anonymous], 2014, IEEE CVPR
   [Anonymous], 2013, IEEE ICCV
   [Anonymous], 2011, BMVC
   [Anonymous], IEEE CVPR
   [Anonymous], 2013, IEEE CVPR
   [Anonymous], 2007, COMPUTER VISION PATT, DOI DOI 10.1109/CVPR.2007.383017
   [Anonymous], OPT ENG
   [Anonymous], 2012, ECCV
   [Anonymous], 2009, ACM T GRAPHIC, DOI DOI 10.1145/1618452.1618470
   [Anonymous], 2012, ECCV
   Cheng MM, 2013, IEEE I CONF COMP VIS, P1529, DOI 10.1109/ICCV.2013.193
   Cheng MM, 2011, PROC CVPR IEEE, P409, DOI 10.1109/CVPR.2011.5995344
   Dollár P, 2013, IEEE I CONF COMP VIS, P1841, DOI 10.1109/ICCV.2013.231
   Goferman S, 2010, PROC CVPR IEEE, P2376, DOI 10.1109/CVPR.2010.5539929
   Huang LQ, 2007, PSYCHOL REV, V114, P599, DOI 10.1037/0033-295X.114.3.599
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Jiang HZ, 2013, PROC CVPR IEEE, P2083, DOI 10.1109/CVPR.2013.271
   Jiang P, 2013, IEEE I CONF COMP VIS, P1976, DOI 10.1109/ICCV.2013.248
   Karacan L, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508403
   Li XH, 2013, IEEE I CONF COMP VIS, P2976, DOI 10.1109/ICCV.2013.370
   Ma Y.F., 2003, P 11 ACM INT C MULT, P374, DOI DOI 10.1145/957013.957094
   Marchesotti L, 2009, IEEE I CONF COMP VIS, P2232, DOI 10.1109/ICCV.2009.5459467
   OTSU N, 1979, IEEE T SYST MAN CYB, V9, P62, DOI 10.1109/TSMC.1979.4310076
   Perazzi F, 2012, PROC CVPR IEEE, P733, DOI 10.1109/CVPR.2012.6247743
   Rahtu E, 2010, LECT NOTES COMPUT SC, V6315, P366, DOI 10.1007/978-3-642-15555-0_27
   Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720
   Sharma G, 2012, PROC CVPR IEEE, P3506, DOI 10.1109/CVPR.2012.6248093
   Shi Y, 2014, HORTIC RES-ENGLAND, V1, DOI 10.1038/hortres.2014.6
   Toet A, 2011, IEEE T PATTERN ANAL, V33, P2131, DOI 10.1109/TPAMI.2011.53
   Vincent L, 1993, IEEE T IMAGE PROCESS, V2, P176, DOI 10.1109/83.217222
   Xie YL, 2013, IEEE T IMAGE PROCESS, V22, P1689, DOI 10.1109/TIP.2012.2216276
   Xie YL, 2011, IEEE IMAGE PROC, P645, DOI 10.1109/ICIP.2011.6116634
   Xu M, 2015, VISUAL COMPUT, V31, P355, DOI 10.1007/s00371-014-0930-9
   Zhai Y., 2006, P 14 ACM INT C MULT, P815, DOI [DOI 10.1145/1180639.1180824, 10.1145/1180639.1180824]
   Zhang HL, 2016, VISUAL COMPUT, V32, P31, DOI 10.1007/s00371-014-1053-z
   Zhang JM, 2013, IEEE I CONF COMP VIS, P153, DOI 10.1109/ICCV.2013.26
   Zhang WJ, 2016, VISUAL COMPUT, V32, P275, DOI 10.1007/s00371-015-1065-3
   Zhong GY, 2016, VISUAL COMPUT, V32, P611, DOI 10.1007/s00371-015-1077-z
NR 43
TC 9
Z9 9
U1 0
U2 14
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2017
VL 33
IS 2
BP 209
EP 220
DI 10.1007/s00371-015-1176-x
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA EI2TF
UT WOS:000392340400008
DA 2024-07-18
ER

PT J
AU Filip, J
   Havlícek, M
   Vávra, R
AF Filip, Jiri
   Havlicek, Michal
   Vavra, Radomir
TI Adaptive highlights stencils for modeling of multi-axial BRDF anisotropy
SO VISUAL COMPUTER
LA English
DT Article
DE Anisotropic; Highlight; Stencils; BRDF; Model
AB Directionally dependent anisotropic material appearance phenomenon is widely represented using bidirectional reflectance distribution function (BRDF). This function needs in practice either reconstruction of unknown values interpolating between sparse measured samples or requires data fidelity preserving compression forming a compact representation from dense measurements. Both properties can be, to a certain extent, preserved by means of analytical BRDF models. Unfortunately, the number of anisotropic BRDF models is limited, and moreover, most require either a demanding iterative optimization procedure dependent on proper initialization or the user setting parameters. Most of these approaches are challenged by the fitting of complex anisotropic BRDFs. In contrast, we approximate BRDF anisotropic behavior by means of highlight stencils and derive a novel BRDF model that independently adapts such stencils to each anisotropic mode present in the BRDF. Our model allows for the fast direct fitting of parameters without the need of any demanding optimization. Furthermore, it achieves an encouraging, expressive visual quality as compared to rival solutions that rely on a similar number of parameters. We thereby ascertain that our method represents a promising approach to the analysis and modeling of complex anisotropic BRDF behavior.
C1 [Filip, Jiri; Havlicek, Michal; Vavra, Radomir] ASCR, Inst Informat Theory & Automat, Pod Vodarenskou Vezi 4, Prague 18208 8, Czech Republic.
   [Vavra, Radomir] Czech Tech Univ, Fac Informat Technol, Prague, Czech Republic.
C3 Czech Academy of Sciences; Institute of Information Theory & Automation
   of the Czech Academy of Sciences; Czech Technical University Prague
RP Filip, J (corresponding author), ASCR, Inst Informat Theory & Automat, Pod Vodarenskou Vezi 4, Prague 18208 8, Czech Republic.
EM filipj@utia.cas.cz
RI Filip, Jiri/D-3396-2012; Vavra, Radomir/H-4349-2014
FU Czech Science Foundation [14-02652S]
FX We thank the anonymous reviewers for their insightful and inspiring
   comments. This research has been supported by the Czech Science
   Foundation Grant 14-02652S.
CR [Anonymous], 1977, NATL BUR STAND MONOG, DOI [10.1109/LPT.2009.2020494, DOI 10.1109/LPT.2009.2020494]
   [Anonymous], 2010, COMPUT GRAPHICS-US, DOI DOI 10.1145/1722991.1722996
   [Anonymous], 1991, ACM SIGGRAPH COMPUTE, DOI DOI 10.1145/127719
   [Anonymous], 1977, P 4 ANN C COMPUTER G, DOI DOI 10.1145/965141.563893
   Ashikhmin M., 2000, Journal of Graphics Tools, V5, P25, DOI 10.1080/10867651.2000.10487522
   Bagher MM, 2012, COMPUT GRAPH FORUM, V31, P1509, DOI 10.1111/j.1467-8659.2012.03147.x
   Cook R. L., 1981, Computer Graphics, V15, P307, DOI 10.1145/965161.806819
   Dupuy J, 2015, COMPUT GRAPH FORUM, V34, P21, DOI 10.1111/cgf.12675
   Filip J, 2014, COMPUT GRAPH FORUM, V33, P91, DOI 10.1111/cgf.12477
   Heitz E., 2014, J. Comput. Graph. Tech, V3, P32
   Lafortune E. P. F., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P117, DOI 10.1145/258734.258801
   Lawrence J, 2004, ACM T GRAPHIC, V23, P496, DOI 10.1145/1015706.1015751
   Lu R, 2000, COMPUT VIS IMAGE UND, V78, P320, DOI 10.1006/cviu.2000.0841
   OREN M, 1995, INT J COMPUT VISION, V14, P227, DOI 10.1007/BF01679684
   Pacanowski R, 2012, IEEE T VIS COMPUT GR, V18, P1824, DOI 10.1109/TVCG.2012.73
   PHONG BT, 1975, COMMUN ACM, V18, P311, DOI 10.1145/360825.360839
   Poulin P., 1990, Computer Graphics, V24, P273, DOI 10.1145/97880.97909
   Raymond B, 2014, COMPUT GRAPH FORUM, V33, P313, DOI 10.1111/cgf.12300
   Rusinkiewicz S. M., 1998, Rendering Techniques '98. Proceedings of the Eurographics Workshop, P11
   Sadeghi I, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2451236.2451240
   SCHLICK C, 1994, COMPUT GRAPH FORUM, V13, P121, DOI 10.1111/1467-8659.1320121
   TORRANCE KE, 1967, J OPT SOC AM, V57, P1105, DOI 10.1364/JOSA.57.001105
   WARD GJ, 1992, COMP GRAPH, V26, P265, DOI 10.1145/142920.134078
NR 23
TC 1
Z9 1
U1 0
U2 5
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2017
VL 33
IS 1
BP 5
EP 15
DI 10.1007/s00371-015-1148-1
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA EI2JM
UT WOS:000392313200003
DA 2024-07-18
ER

PT J
AU Jesus, D
   Coelho, A
   Sousa, AA
AF Jesus, Diego
   Coelho, Antonio
   Sousa, Antonio Augusto
TI Layered shape grammars for procedural modelling of buildings
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 33rd Conference on Computer Graphics International (CGI)
CY JUN 28-JUL 01, 2016
CL Heraklion, GREECE
SP Fdn Res Technol
DE Procedural modelling; Shape grammars; Layers; Vectorial shapes
AB The effort of creating virtual city environments is reduced using procedural modelling techniques. However, these typically use split-based approaches which can impose limitations on the final geometry, usually enforcing a grid-like structure and require complex geometry to be imported. Layered shape grammars can increase the variability of procedural buildings, while the vectorial definition of shapes introduces the possibility of creating complex shapes that seamlessly blend into the model. We evaluate the contributions with a modelling example and a comparison with split-based procedural modelling techniques. Results show that layers allow more flexibility than split-based techniques in creating variations. Vectorially defined shapes are a step forward in shape grammar expressiveness.
C1 [Jesus, Diego; Coelho, Antonio; Sousa, Antonio Augusto] Univ Porto, INESC TEC, Oporto, Portugal.
   [Jesus, Diego; Coelho, Antonio; Sousa, Antonio Augusto] Univ Porto, DEI Fac Engn, Oporto, Portugal.
C3 INESC TEC; Universidade do Porto; Universidade do Porto
RP Jesus, D (corresponding author), Univ Porto, INESC TEC, Oporto, Portugal.; Jesus, D (corresponding author), Univ Porto, DEI Fac Engn, Oporto, Portugal.
EM diego.jesus@fe.up.pt
RI Sousa, A. Augusto/B-3899-2012; Coelho, Antonio/G-2216-2011
OI Sousa, A. Augusto/0000-0002-9883-2686; Coelho,
   Antonio/0000-0001-7949-2877
CR [Anonymous], INT J INTEGRATIVE ME, DOI DOI 10.1049/J0E.2013.0116
   Edelsbrunner J, 2016, LECT NOTES COMPUT SC, V9550, P17, DOI 10.1007/978-3-662-49247-5_2
   Guerrero P, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766933
   Ilcík M, 2015, COMPUT GRAPH FORUM, V34, P205, DOI 10.1111/cgf.12553
   Kelly T, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1944846.1944854
   Krecklau L, 2010, COMPUT GRAPH FORUM, V29, P2291, DOI 10.1111/j.1467-8659.2010.01714.x
   Leblanc L., 2011, Graph. Interface, V2011, P87
   Lipp M, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360701
   Müller P, 2006, ACM T GRAPHIC, V25, P614, DOI 10.1145/1141911.1141931
   Parish YIH, 2001, COMP GRAPH, P301, DOI 10.1145/383259.383292
   Patow G, 2012, IEEE COMPUT GRAPH, V32, P66, DOI 10.1109/MCG.2010.104
   Ruiz-Montiel M, 2014, COMPUT AIDED DESIGN, V56, P104, DOI 10.1016/j.cad.2014.06.012
   Schwarz M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766956
   SILVA PB, 2015, INT J COMP GAMES TEC, V1, P2015
   Silva PB, 2013, P PCG 2013 WORKSH PR
   Stiny G., 1972, Information Processing 71 Proceedings of the IFIP Congress 1971. Volume 2, P1460
   Thaller W, 2013, COMPUT GRAPH-UK, V37, P707, DOI 10.1016/j.cag.2013.05.012
   Wonka P, 2003, ACM T GRAPHIC, V22, P669, DOI 10.1145/882262.882324
NR 18
TC 8
Z9 9
U1 1
U2 14
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2016
VL 32
IS 6-8
BP 933
EP 943
DI 10.1007/s00371-016-1254-8
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA DP8ET
UT WOS:000378731600025
DA 2024-07-18
ER

PT J
AU Liu, XT
   Li, CZ
   Zhu, HC
   Wong, TT
   Xu, XM
AF Liu, Xueting
   Li, Chengze
   Zhu, Haichao
   Wong, Tien-Tsin
   Xu, Xuemiao
TI Text-aware balloon extraction from manga
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Computational Visual Media Conference (CVM)
CY APR 16-17, 2015
CL Tsinghua Univ, Beijing, PEOPLES R CHINA
HO Tsinghua Univ
DE Computational manga; Image processing; Manga segmentation; Balloon
   extraction; Text detection
ID VIDEO FRAMES; IMAGES; LOCATION
AB Manga, a Japanese word for comics, is a worldwide popular visual entertainment. Nowadays, electronic devices boost the fast development of motion manga for the purpose of visual richness and manga promotion. To convert static manga images into motion mangas, text balloons are usually animated individually for better story telling. This needs the artists to cut out each text balloon meticulously, and therefore it is quite labor-intensive and time-consuming. In this paper, we propose an automatic approach that can extract text balloons from manga images both accurately and effectively. Our approach starts by extracting white areas that contain texts as text blobs. Different from existing text blob extraction methods that only rely on shape properties, we incorporate text properties in order to differentiate text blobs from texture blobs. Instead of heuristic parameter thresholding, we achieve text blob classification via learning-based classifiers. Along with the novel text blob classification method, we also make the first attempt in trying to tackle the boundary issue in balloon extraction. We apply our method on various styles of mangas and comics with texts in different languages, and convincing results are obtained in all cases.
C1 [Liu, Xueting; Zhu, Haichao; Wong, Tien-Tsin] Chinese Univ Hong Kong, Dept Comp Sci & Engn, Hong Kong, Hong Kong, Peoples R China.
   [Liu, Xueting; Li, Chengze; Zhu, Haichao; Wong, Tien-Tsin] Chinese Univ Hong Kong, Shenzhen Res Inst, Shenzhen, Peoples R China.
   [Xu, Xuemiao] S China Univ Technol, Sch Comp Sci & Engn, Guangzhou 510641, Guangdong, Peoples R China.
C3 Chinese University of Hong Kong; CUHK Shenzhen Research Institute; The
   Chinese University of Hong Kong, Shenzhen; South China University of
   Technology
RP Liu, XT; Wong, TT (corresponding author), Chinese Univ Hong Kong, Dept Comp Sci & Engn, Hong Kong, Hong Kong, Peoples R China.; Liu, XT; Wong, TT (corresponding author), Chinese Univ Hong Kong, Shenzhen Res Inst, Shenzhen, Peoples R China.
EM xtliu@cse.cuhk.edu.hk; ljsabc@gmail.com; hczhu@cse.cuhk.edu.hk;
   ttwong@cse.cuhk.edu.hk; xuemx@scut.edu.cn
RI Liu, Xueting/AAG-9648-2019; Li, Chengze/AAU-7168-2021
OI Liu, Xueting/0000-0002-0868-5353; Li, Chengze/0000-0002-1519-750X
FU NSFC [61272293, 61103120]; Shenzhen Basic Research Project
   [JCYJ20120619152326448]; Shenzhen Nanshan Innovative Institution
   Establishment Fund [KC2013ZDZJ0007A]; Research Grants Council of the
   Hong Kong Special Administrative Region, under RGC General Research Fund
   [CUHK 417913]; Guangzhou Novo Program of Science and Technology
   [0501-330]
FX This project is supported by NSFC (Project No. 61272293), Shenzhen Basic
   Research Project (Project No. JCYJ20120619152326448), Shenzhen Nanshan
   Innovative Institution Establishment Fund (Project No. KC2013ZDZJ0007A),
   the Research Grants Council of the Hong Kong Special Administrative
   Region, under RGC General Research Fund (Project No. CUHK 417913), NSFC
   (Project No. 61103120), and Guangzhou Novo Program of Science and
   Technology (Project No. 0501-330).
CR Arai K., 2011, INT J IMAGE PROCESSI, V4, P669
   Chen XR, 2004, PROC CVPR IEEE, P366
   Epshtein B, 2010, PROC CVPR IEEE, P2963, DOI 10.1109/CVPR.2010.5540041
   Friedman J, 2000, ANN STAT, V28, P337, DOI 10.1214/aos/1016218223
   Gllavata J, 2004, INT C PATT RECOG, P425, DOI 10.1109/ICPR.2004.1334146
   Jain AK, 1998, PATTERN RECOGN, V31, P2055, DOI 10.1016/S0031-3203(98)00067-3
   Kim HK, 1996, J VIS COMMUN IMAGE R, V7, P336, DOI 10.1006/jvci.1996.0029
   Li HP, 2000, IEEE T IMAGE PROCESS, V9, P147, DOI 10.1109/83.817607
   Lienhart R, 2002, IEEE T CIRC SYST VID, V12, P256, DOI 10.1109/76.999203
   Liu YX, 2006, IEICE T INF SYST, VE89D, P1221, DOI 10.1093/ietisy/e89-d.3.1221
   Ngo ho A., 2012, IAPR INT WORKSH DOC, P424
   Sundaresan M., 2012, INT C PATT REC INF M, P449
   Suzuki K, 2003, COMPUT VIS IMAGE UND, V89, P1, DOI 10.1016/S1077-3142(02)00030-9
   Tolle H., 2010, INT J UBIQUITOUS COM, V1, P1
   Tolle H, 2013, INT C ADV COMP SCI I, P321, DOI 10.1109/ICACSIS.2013.6761596
   Vapnik VN, 2000, NATURE STAT LEARNING, DOI DOI 10.1007/978-1-4757-3264-1
   Yamada M, 2004, IEICE T INF SYST, VE87D, P1370
   Ye QX, 2005, IMAGE VISION COMPUT, V23, P565, DOI 10.1016/j.imavis.2005.01.004
   Zhang X, 2014, VISUAL COMPUT, V30, P401, DOI 10.1007/s00371-013-0864-7
NR 19
TC 9
Z9 9
U1 0
U2 17
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2016
VL 32
IS 4
BP 501
EP 511
DI 10.1007/s00371-015-1084-0
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA DI6YD
UT WOS:000373645200008
DA 2024-07-18
ER

PT J
AU Truong, A
   Boujut, H
   Zaharia, T
AF Truong, Arthur
   Boujut, Hugo
   Zaharia, Titus
TI Laban descriptors for gesture recognition and emotional analysis
SO VISUAL COMPUTER
LA English
DT Article
DE Gesture expressivity model; Laban movement analysis; Motion features;
   Gesture recognition; Expressivity analysis; Machine learning
ID SVM
AB In this paper, we introduce a new set of 3D gesture descriptors based on the laban movement analysis model. The proposed descriptors are used in a machine learning framework (with SVM and different random forest techniques) for both gesture recognition and emotional analysis purposes. In a first experiment, we test our expressivity model for action recognition purposes on the Microsoft Research Cambridge-12 dataset and obtain very high recognition rates (more than 97 %). In a second experiment, we test our descriptors' ability to qualify the emotional content, upon a database of pre-segmented orchestra conductors' gestures recorded in rehearsals. The results obtained show the relevance of our model which outperforms results reported in similar works on emotion recognition.
C1 [Truong, Arthur; Boujut, Hugo; Zaharia, Titus] CNRS, UMR MAP5 8145, Telecom SudParis, Inst Mines Telecom,ARTEMIS Dept, 9 Rue Charles Fourier, F-91011 Evry, France.
C3 IMT - Institut Mines-Telecom; Institut Mines-Telecom Business School;
   Universite Paris Cite; Institut Polytechnique de Paris; Telecom
   SudParis; Centre National de la Recherche Scientifique (CNRS)
RP Truong, A (corresponding author), CNRS, UMR MAP5 8145, Telecom SudParis, Inst Mines Telecom,ARTEMIS Dept, 9 Rue Charles Fourier, F-91011 Evry, France.
EM arthur.truong@telecom-sudparis.eu; hugo.boujut@telecom-sudparis.eu;
   titus.zaharia@telecom-sudparis.eu
CR Adrien J. M, 2010, ACTES JOURNEES INFOR
   Alon J, 2009, IEEE T PATTERN ANAL, V31, P1685, DOI 10.1109/TPAMI.2008.203
   [Anonymous], 1994, MAITRISE MOUVEMENT
   [Anonymous], VIS COMPUT
   [Anonymous], 2008, 2008 16 EUROPEAN SIG
   [Anonymous], 2006, ACM International Workshop on Video Surveillance and Sensor Networks, DOI DOI 10.1145/1178782.1178808
   [Anonymous], 2013, Proceedings of the ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games, I3D'13
   [Anonymous], 2008, BMVC
   Balomenos T, 2005, LECT NOTES COMPUT SC, V3361, P318
   Bernhardt D, 2007, LECT NOTES COMPUT SC, V4738, P59
   Blank M, 2005, IEEE I CONF COMP VIS, P1395
   Bouchard D, 2007, LECT NOTES ARTIF INT, V4722, P37
   Boutet D., 2008, Cahiers de Linguistique Analogique, V5, P81
   Braem P. B., 2001, J CONDUCT GUILD, V22, P14
   Breiman L., 2001, Machine Learning, V45, P5, DOI 10.1023/A:1010933404324
   Camurri A, 2003, LECT NOTES ARTIF INT, V2915, P20
   Camurri A, 2003, INT J HUM-COMPUT ST, V59, P213, DOI 10.1016/S1071-5819(03)00050-8
   Chen YQ, 2001, IEEE IMAGE PROC, P34, DOI 10.1109/ICIP.2001.958946
   Cimen G, 2013, COMPUT ANIMAT VIRT W, V24, P355, DOI 10.1002/cav.1509
   Clauser C. E., 1969, Tech. Rep. TR-69-70 (AD 710 622), DOI DOI 10.21236/AD0710622
   Etemad S. A., 2014, VIS COMPUT
   Fothergill S., 2012, P SIGCHI C HUM FACT, P1737, DOI DOI 10.1145/2207676.2208303
   Geurts P, 2006, MACH LEARN, V63, P3, DOI 10.1007/s10994-006-6226-1
   Glowinski D, 2011, IEEE T AFFECT COMPUT, V2, P106, DOI 10.1109/T-AFFC.2011.7
   Grewe O., 2010, MUSIQUE LANGAGE EMOT, P49
   Gunes H., 2012, Image and Vision Computing
   Gunes H, 2007, J NETW COMPUT APPL, V30, P1334, DOI 10.1016/j.jnca.2006.09.007
   Hachimura K, 2005, 2005 IEEE INTERNATIONAL WORKSHOP ON ROBOT AND HUMAN INTERACTIVE COMMUNICATION (RO-MAN), P294
   Hautbois X., 2004, ICMS 8 GEST FORM PRO
   Hripcsak G, 2005, J AM MED INFORM ASSN, V12, P296, DOI 10.1197/jamia.M1733
   Huang FY, 2007, LECT NOTES COMPUT SC, V4844, P477
   Jiang XB, 2014, VISUAL COMPUT, V30, P1021, DOI 10.1007/s00371-014-0923-8
   Jorland G, 2008, REV METAPHYS MORALE, P269
   Junejo IN, 2014, VISUAL COMPUT, V30, P259, DOI 10.1007/s00371-013-0842-0
   Kaaniche M., 2012, IEEE T PATTERN MACH
   Laban Rudolf, 2003, ESPACE DYNAMIQUE
   Laptev I, 2008, PROC CVPR IEEE, P3222, DOI 10.1109/cvpr.2008.4587756
   Li Y., 2014, VIS COMPUT
   McNeill D., 2000, Language and Gesture, V2
   Milgram Jonathan, 2006, 10 INT WORKSH FRONT
   Nakata T., 2002, J ROBOT MECHATRON, V14, P27, DOI DOI 10.20965/JRM.2002.P0027
   Nicolaou MA, 2011, IEEE T AFFECT COMPUT, V2, P92, DOI 10.1109/T-AFFC.2011.9
   Oikonomopoulos A, 2006, IEEE T SYST MAN CY B, V36, P710, DOI 10.1109/TSMCB.2005.861864
   Pedersoli F, 2014, VISUAL COMPUT, V30, P1107, DOI 10.1007/s00371-014-0921-x
   Pengcheng Luo, 2012, Motion in Games. 5th International Conference (MIG 2012). Proceedings, P254, DOI 10.1007/978-3-642-34710-8_24
   Rapantzikos K, 2009, PROC CVPR IEEE, P1454, DOI 10.1109/CVPRW.2009.5206525
   Samadani AA, 2013, INT CONF AFFECT, P343, DOI 10.1109/ACII.2013.63
   Schüldt C, 2004, INT C PATT RECOG, P32, DOI 10.1109/ICPR.2004.1334462
   Shove Patrick., 1995, The Practice of Performance: Studies in Musical Interpretation, P55, DOI DOI 10.1017/CBO9780511552366.004
   Singh VK, 2011, VISUAL COMPUT, V27, P1115, DOI 10.1007/s00371-011-0656-x
   Song Y., 2011, Proceedings 2011 IEEE International Conference on Automatic Face & Gesture Recognition (FG 2011), P500, DOI 10.1109/FG.2011.5771448
   Song Y, 2013, IEEE INT CONF AUTOMA
   Suykens JAK, 1999, NEURAL PROCESS LETT, V9, P293, DOI 10.1023/A:1018628609742
   Swaminathan D, 2009, ADV HUM-COMPUT INTER, V2009, DOI 10.1155/2009/362651
   Wang Y., 2007, P IEEE INT C COMPUTE, P1, DOI [10.1109/CVPR.2007.383505, DOI 10.1109/CVPR.2007.383505, DOI 10.1016/J.EJ0R.2007.01.050]
   Zhao LW, 2005, GRAPH MODELS, V67, P1, DOI 10.1016/j.gmod.2004.08.002
NR 56
TC 21
Z9 24
U1 0
U2 12
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2016
VL 32
IS 1
BP 83
EP 98
DI 10.1007/s00371-014-1057-8
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DF9FQ
UT WOS:000371666800008
DA 2024-07-18
ER

PT J
AU Xu, JY
   Mao, XY
   Jin, XG
   Jaffer, A
   Lu, SF
   Li, L
   Toyoura, M
AF Xu, Jiayi
   Mao, Xiaoyang
   Jin, Xiaogang
   Jaffer, Aubrey
   Lu, Shufang
   Li, Li
   Toyoura, Masahiro
TI Hidden message in a deformation-based texture
SO VISUAL COMPUTER
LA English
DT Article
DE Information hiding; Marbling texture; Mathematical formula;
   Steganography; Texture synthesis; Visual perception
AB We present stego-texture, a unique texture synthesis method that allows users to deliver personalized messages with beautiful, decorative textures. Our approach was inspired by the success of recent work generating marbling textures using mathematical functions. We were able to transform an input image or a text message into an intricate texture by combining the seven basic, reversible functions provided in the system. Later, the input image or message could be recovered by reversing the process of these functions. During the design process, the parameters of operations were automatically recorded, encrypted and invisibly embedded into the final pattern to create a stego-texture. In this way, the receiver could extract the hidden message from the stego-texture without the need for extra information from the sender. To ensure that the delivered message is unnoticeably covered by the texture, we propose a new technique for automatically creating a background that is harmonious with the message based on a set of visual perception cues.
C1 [Xu, Jiayi; Li, Li] Hangzhou Dianzi Univ, Hangzhou, Zhejiang, Peoples R China.
   [Mao, Xiaoyang; Toyoura, Masahiro] Univ Yamanashi, Kofu, Yamanashi, Japan.
   [Jin, Xiaogang] Zhejiang Univ, State Key Lab CAD & CG, Hangzhou 310003, Zhejiang, Peoples R China.
   [Jaffer, Aubrey] Digilant, Boston, MA USA.
   [Lu, Shufang] Zhejiang Univ Technol, Coll Comp Sci & Technol, Hangzhou, Zhejiang, Peoples R China.
   [Xu, Jiayi] Hangzhou Dianzi Univ, Sch Comp Sci & Technol, Hangzhou, Zhejiang, Peoples R China.
   [Mao, Xiaoyang; Toyoura, Masahiro] Univ Yamanashi, Interdisciplinary Grad Sch Med & Engn, Kofu, Yamanashi, Japan.
C3 Hangzhou Dianzi University; University of Yamanashi; Zhejiang
   University; Zhejiang University of Technology; Hangzhou Dianzi
   University; University of Yamanashi
RP Mao, XY (corresponding author), Univ Yamanashi, Kofu, Yamanashi, Japan.
EM mao@yamanashi.ac.jp
RI zhu, yujie/KBC-4009-2024
OI Li, Ling/0000-0001-9722-9503; mao, xiaoyang/0000-0001-9531-3197;
   Toyoura, Masahiro/0000-0002-5897-7573
FU Natural Science Foundation of Zhejiang Province, China [Q12F020007];
   Zhejiang Provincial Education Department, China [Y201121352]; National
   Natural Science Foundation of China [61472351]; Grants-in-Aid for
   Scientific Research [26560006, 25280037] Funding Source: KAKEN
FX Jiayi Xu was supported by the Natural Science Foundation of Zhejiang
   Province, China (No. Q12F020007), the Zhejiang Provincial Education
   Department, China (No. Y201121352). Xiaogang Jin was supported by the
   National Natural Science Foundation of China (No. 61472351).
CR Acar R, 2006, IEEE T VIS COMPUT GR, V12, P600, DOI 10.1109/TVCG.2006.66
   Akgun BT, 2004, LEONARDO, V37, P49, DOI 10.1162/002409404772828120
   Alattar A.M., 2000, P ACM WORKSH MULT SE
   [Anonymous], 2011, J. Inform. Hid. Multimed. Signal Process.
   [Anonymous], 2009, State of the Art in Example-Based Texture Synthesis R
   Chu HK, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778788
   Cohen MF, 2003, ACM T GRAPHIC, V22, P287, DOI 10.1145/882262.882265
   Du H, 2012, COMPUT GRAPH FORUM, V31, P2203, DOI 10.1111/j.1467-8659.2012.03213.x
   Ebert David S, 2003, Texturing Modeling: A Procedural Approach
   Efros A. A., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1033, DOI 10.1109/ICCV.1999.790383
   Hu Shi-Min, 2011, P NPAR 11 ACM SIGGRA, P27, DOI DOI 10.1145/2024676.2024681
   Jin XG, 2007, IEEE COMPUT GRAPH, V27, P78, DOI 10.1109/MCG.2007.28
   Kudo Hiroshi, 2010, 2010 10th International Symposium on Communications and Information Technologies (ISCIT 2010), P626, DOI 10.1109/ISCIT.2010.5665066
   Kwatr V., 2007, SIGGRAPH 07 COURSES
   Kwatra V, 2003, ACM T GRAPHIC, V22, P277, DOI 10.1145/882262.882264
   Lefebvre S, 2006, ACM T GRAPHIC, V25, P541, DOI 10.1145/1141911.1141921
   Liang L, 2001, ACM T GRAPHIC, V20, P127, DOI 10.1145/501786.501787
   Lu SF, 2012, IEEE COMPUT GRAPH, V32, P26, DOI 10.1109/MCG.2011.51
   Mao X., 2003, GRAPHITE 03, P79, DOI [10.1145/604471.604489, DOI 10.1145/604471.604489]
   Mitra NJ, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618509
   Mohan A, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531404
   Papas M, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366205
   Perlin K., 1985, Computer Graphics, V19, P287, DOI 10.1145/325165.325247
   Praun E, 2000, COMP GRAPH, P465, DOI 10.1145/344779.344987
   RIVEST RL, 1978, COMMUN ACM, V21, P120, DOI [10.1145/359340.359342, 10.1145/357980.358017]
   Sharma Vijay Kumar, 2012, Journal of Theoretical and Applied Information Technology, V36, P1
   Shono T, 2009, 2009 INTERNATIONAL SYMPOSIUM ON INTELLIGENT SIGNAL PROCESSING AND COMMUNICATION SYSTEMS (ISPACS 2009), P558, DOI 10.1109/ISPACS.2009.5383777
   Suzuki G., 2008, P 10 INT C HUM COMP, P507
   TURK G, 1991, COMP GRAPH, V25, P289, DOI 10.1145/127719.122749
   Walia D. E., 2010, GLOBAL J COMPUTER SC, V10, P4
   Wei LY, 2000, COMP GRAPH, P479, DOI 10.1145/344779.345009
   Xu JY, 2008, IEEE COMPUT GRAPH, V28, P35, DOI 10.1109/MCG.2008.36
   Yen JC, 2000, ISCAS 2000: IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS - PROCEEDINGS, VOL IV, P49, DOI 10.1109/ISCAS.2000.858685
   Yoon JC, 2008, COMPUT GRAPH FORUM, V27, P1869, DOI 10.1111/j.1467-8659.2008.01334.x
   Zhao HL, 2009, MULTIMED TOOLS APPL, V44, P187, DOI 10.1007/s11042-009-0290-z
NR 35
TC 43
Z9 57
U1 0
U2 11
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2015
VL 31
IS 12
BP 1653
EP 1669
DI 10.1007/s00371-014-1045-z
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CV2ZD
UT WOS:000364126900006
DA 2024-07-18
ER

PT J
AU Peyrot, JL
   Payan, F
   Antonini, M
AF Peyrot, Jean-Luc
   Payan, Frederic
   Antonini, Marc
TI Direct blue noise resampling of meshes of arbitrary topology
SO VISUAL COMPUTER
LA English
DT Article
DE Blue noise; Sampling; Sampling analysis; Feature-preservation
AB We propose in this paper a novel sampling method and an improvement of a spectral analysis tool that both handle complex shapes and sharp features. Starting from an arbitrary triangular mesh, our algorithm generates a new sampling pattern that exhibits blue noise properties. The fidelity to the original surface being essential, our algorithm preserves sharp features. Our sampling is based on a discrete dart throwing applied directly on the surface to get good blue noise sampling patterns. It is also driven by a feature detection tool to avoid geometric aliasing. Experimental results prove that our sampling scheme is faster than techniques based on brute-force dart throwing, and produces sampling patterns with blue noise properties even for complex surfaces of arbitrary topology. In parallel, we also propose an improvement of a tool initially developed for the spectral analysis of non-uniform sampling patterns, that may generate biased results with complex shapes. The proposed improvement overcomes this problem.
C1 Sophia Antipolis, Univ Nice, CNRS UMR 7271, Lab I3S, Sophia Antipolis, France.
   [Peyrot, Jean-Luc; Payan, Frederic; Antonini, Marc] Univ Nice Sophia Antipolis, CNRS, Lab I3S, UMR 7271, F-06909 Sophia Antipolis, France.
C3 Universite Cote d'Azur; Universite Cote d'Azur; Centre National de la
   Recherche Scientifique (CNRS)
RP Peyrot, JL (corresponding author), Univ Nice Sophia Antipolis, CNRS, Lab I3S, UMR 7271, 2000 Route Lucioles, F-06909 Sophia Antipolis, France.
EM peyrot@i3s.unice.fr; fpayan@i3s.unice.fr; am@i3s.unice.fr
FU Region Provence Alpes Cote d'Azur (France)
FX This work is supported by a grant from Region Provence Alpes Cote d'Azur
   (France).
CR Alliez P, 2002, ACM T GRAPHIC, V21, P347, DOI 10.1145/566570.566588
   [Anonymous], SIGGRAPH ASIA 10
   [Anonymous], 2010, ACM SIGGRAPH 2010 Papers. SIGGRAPH'10, DOI [DOI 10.1145/1833349.1778769, DOI 10.1145/1778765.1778769]
   Aspert N, 2002, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL I AND II, PROCEEDINGS, P705, DOI 10.1109/ICME.2002.1035879
   Balzer M, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531392
   Bowers J, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866188
   Chen RJ, 2012, COMPUT GRAPH FORUM, V31, P1775, DOI 10.1111/j.1467-8659.2012.03182.x
   Chen ZG, 2012, IEEE T VIS COMPUT GR, V18, P1784, DOI 10.1109/TVCG.2012.94
   Cline D, 2009, COMPUT GRAPH FORUM, V28, P1217, DOI 10.1111/j.1467-8659.2009.01499.x
   COOK RL, 1986, ACM T GRAPHIC, V5, P51, DOI 10.1145/7529.8927
   Corsini M, 2012, IEEE T VIS COMPUT GR, V18, P914, DOI 10.1109/TVCG.2012.34
   Crane K, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2516971.2516977
   CROW FC, 1977, COMMUN ACM, V20, P799, DOI 10.1145/359863.359869
   Dijkstra EW., 1959, NUMER MATH, V1, P269, DOI [10.1007/BF01386390, DOI 10.1007/BF01386390]
   Dippe M. A. Z., 1985, Computer Graphics, V19, P69, DOI 10.1145/325165.325182
   Dunbar D, 2006, ACM T GRAPHIC, V25, P503, DOI 10.1145/1141911.1141915
   Fu Y, 2008, SPM 2008: PROCEEDINGS OF THE ACM SOLID AND PHYSICAL MODELING SYMPOSIUM, P115
   Gamito MN, 2009, ACM T GRAPHIC, V29, DOI 10.1145/1640443.1640451
   Ge Xiaoyin., 2013, Bilateral blue noise sampling
   Geng B., 2011, J COMPUT AIDED DES C, V23, P62
   Heck D, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2487228.2487233
   Kim HS, 2009, COMPUT AIDED DESIGN, V41, P47, DOI 10.1016/j.cad.2008.12.003
   Lagae A, 2008, COMPUT GRAPH FORUM, V27, P114, DOI 10.1111/j.1467-8659.2007.01100.x
   Li HW, 2008, IEEE T VIS COMPUT GR, V14, P982, DOI 10.1109/TVCG.2008.53
   LLOYD SP, 1982, IEEE T INFORM THEORY, V28, P129, DOI 10.1109/TIT.1982.1056489
   Peyrot J. L., EUROGRAPHICS SHORT P, P9, DOI [10.2312/conf/EG2013/short/009-912, DOI 10.2312/CONF/EG2013/SHORT/009-912]
   Pharr M., 2004, Physically Based Rendering: From Theory to Implementation
   Schlomer T., 2011, P ACM SIGGRAPH S HIG, P135, DOI DOI 10.1145/2018323.2018345
   Schmidt R, 2006, ACM T GRAPHIC, V25, P605, DOI 10.1145/1141911.1141930
   Wei LY, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964945
   Wei LY, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778816
   White KB, 2007, RT07: IEEE/EG Symposium on Interactive Ray Tracing 2007, P129, DOI 10.1109/RT.2007.4342600
   Xu Y, 2012, COMPUT GRAPH-UK, V36, P232, DOI 10.1016/j.cag.2012.02.005
   Xu Y, 2011, COMPUT GRAPH-UK, V35, P510, DOI 10.1016/j.cag.2011.03.031
NR 34
TC 2
Z9 2
U1 0
U2 4
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2015
VL 31
IS 10
BP 1365
EP 1381
DI 10.1007/s00371-014-1019-1
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CQ7CG
UT WOS:000360759800006
DA 2024-07-18
ER

PT J
AU Han, ZZ
   Liu, ZB
   Han, JW
   Bu, SH
AF Han, Zhizhong
   Liu, Zhenbao
   Han, Junwei
   Bu, Shuhui
TI 3D shape creation by style transfer
SO VISUAL COMPUTER
LA English
DT Article
DE Style and content separation; Style transfer; Shape creation; Local
   shape difference function
AB In this paper, we propose a new style transfer method for automatic 3D shape creation based on new concepts of style and content of 3D shapes. Our unsupervised style transfer method could plausibly create novel shapes not only by recombining existent styles and contents in a set but also by combining new-coming styles or contents with the existent ones conveniently. This feature provides a better way to increase the diversity of created shapes. The process of shape creation can be summarized as two stages. First, style and content separation is performed to analyzed shapes in a set. Second, novel shapes are created by style transfer. In our setting, contents are first separated via clustering shapes using a new defined global shape distance, and then, style parts are clustered into different style classes. Specifically, style parts are extracted from each pair of intra-content shapes through comparing their multi-scale corresponding patches instead of corresponding parts. This strategy makes the process of extracting style parts become insensitive to slight geometric changes. The multi-scale corresponding patches are obtained via partitioning the two shapes in a consistent way by the proposed correspondence transfer. Meanwhile, to quantify the comparison results for locating style parts, a novel local shape difference function (LSDF) is introduced. Based on LSDF, extracting a style part from each shape is formulated as an optimal LSDF threshold selection problem. In the experiments, we test our method in several sets of man-made 3D shapes and obtain plausible created shapes based on the reasonably separated styles and contents.
C1 [Han, Zhizhong] Northwestern Polytech Univ, Dept Automat, Xian, Peoples R China.
   [Liu, Zhenbao] Northwestern Polytech Univ, Xian 710072, Peoples R China.
   [Han, Junwei] Nanyang Technol Univ, Singapore 639798, Singapore.
   [Han, Junwei] Chinese Univ Hong Kong, Hong Kong, Hong Kong, Peoples R China.
   [Han, Junwei] Dublin City Univ, Dublin 9, Ireland.
   [Han, Junwei] Univ Dundee, Dundee DD1 4HN, Scotland.
   [Bu, Shuhui] Kyoto Univ, Kyoto 6068501, Japan.
C3 Northwestern Polytechnical University; Northwestern Polytechnical
   University; Nanyang Technological University; Chinese University of Hong
   Kong; Dublin City University; University of Dundee; Kyoto University
RP Liu, ZB (corresponding author), 127 Youyi Rd, Xian, Peoples R China.
EM h312h@mail.nwpu.edu.cn; liuzhenbao@nwpu.edu.cn; jhan@nwpu.edu.cn;
   bushuhui@nwpu.edu.cn
RI Han, Zhizhong/AAW-4044-2021
FU National Natural Science Foundation of China [61003137, 61202185,
   61005018, 91120005]; Northwestern Polytechnical University Basic
   Research Fund [310201401JCQ01009, 310201401JCQ01012]; Fundamental
   Research Funds for the Central Universities; Shaanxi Natural Science
   Fund [2012JQ8037]; Open Project Program of the State Key Lab of CAD&CG,
   Zhejiang University [A1306]; Program for New Century Excellent Talents
   in University [NCET-10-0079]; Ministry of Education of China
   [20136102110037]
FX This work was supported partly by grants from National Natural Science
   Foundation of China (61003137, 61202185, 61005018, 91120005),
   Northwestern Polytechnical University Basic Research Fund
   (310201401JCQ01009, 310201401JCQ01012), the Fundamental Research Funds
   for the Central Universities, Shaanxi Natural Science Fund (2012JQ8037),
   and Open Project Program of the State Key Lab of CAD&CG (A1306),
   Zhejiang University, Program for New Century Excellent Talents in
   University under grant NCET-10-0079, and Doctoral Fund of Ministry of
   Education of China under grant 20136102110037.
CR [Anonymous], 2004, IEEE T PATT ANAL MAC
   [Anonymous], 2007, P 24 INT C MACHINE L, DOI DOI 10.1145/1273496.1273619
   Attene M, 2006, IEEE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS 2006, PROCEEDINGS, P14
   Aubry M, 2011, IEEE I CONF COMP VIS, P1411, DOI 10.1109/ICCV.2011.6126396
   BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791
   Blanz V, 1999, COMP GRAPH, P187, DOI 10.1145/311535.311556
   Brand M, 2000, COMP GRAPH, P183, DOI 10.1145/344779.344865
   Bronstein AM, 2010, INT J COMPUT VISION, V89, P266, DOI 10.1007/s11263-009-0301-6
   Chaudhuri S, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964930
   DONOHO DL, 1995, IEEE T INFORM THEORY, V41, P613, DOI 10.1109/18.382009
   Elgammal A, 2004, PROC CVPR IEEE, P478
   Halkidi M, 2001, J INTELL INF SYST, V17, P107, DOI 10.1023/A:1012801612483
   Huang H, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366198
   Jain V, 2007, COMPUT AIDED DESIGN, V39, P398, DOI 10.1016/j.cad.2007.02.009
   Jain V, 2006, IEEE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS 2006, PROCEEDINGS, P118
   Kalogerakis E., 2012, ACM T GRAPHIC, V31, P55
   Kortgen M., 2003, P CENTR EUR COMP GRA
   Li H, 2013, COMPUT GRAPH FORUM, V32, P77, DOI 10.1111/cgf.12015
   Osada R, 2002, ACM T GRAPHIC, V21, P807, DOI 10.1145/571647.571648
   OTSU N, 1979, IEEE T SYST MAN CYB, V9, P62, DOI 10.1109/TSMC.1979.4310076
   Ovsjanikov M, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185526
   Rustamov RM, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461959
   Shamir A, 2004, 2ND INTERNATIONAL SYMPOSIUM ON 3D DATA PROCESSING, VISUALIZATION, AND TRANSMISSION, PROCEEDINGS, P82
   Shapira L, 2008, VISUAL COMPUT, V24, P249, DOI 10.1007/s00371-007-0197-5
   Shilane P, 2004, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, P167, DOI 10.1109/smi.2004.1314504
   Shilane P, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1243980.1243981
   Tenenbaum JB, 2000, NEURAL COMPUT, V12, P1247, DOI 10.1162/089976600300015349
   Xu K, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185553
   Xu K, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866206
   Zheng YY, 2013, COMPUT GRAPH FORUM, V32, P195, DOI 10.1111/cgf.12039
NR 30
TC 7
Z9 10
U1 0
U2 9
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2015
VL 31
IS 9
BP 1147
EP 1161
DI 10.1007/s00371-014-0999-1
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CO8AY
UT WOS:000359388100001
DA 2024-07-18
ER

PT J
AU Shi, YJ
   Yi, YG
   Yan, HX
   Dai, JY
   Zhang, M
   Kong, J
AF Shi, Yanjiao
   Yi, Yugen
   Yan, Hexin
   Dai, Jiangyan
   Zhang, Ming
   Kong, Jun
TI Region contrast and supervised locality-preserving projection-based
   saliency detection
SO VISUAL COMPUTER
LA English
DT Article
DE Saliency detection; Region contrast; Boundary extension; Supervised
   locality-preserving projection; Support vector regression
ID VISUAL-ATTENTION; MODEL; IMAGE
AB As an important problem in computer vision, saliency detection is essential for image segmentation, super-resolution, object recognition, etc. In this paper, we propose a novel method for saliency detection on image using region contrast and machine learning approaches. An image boundary extension-based general framework is proposed that can be used for all rarity- or sparsity-based schemes to improve their performances. Then, a saliency map based on boundary extension and region contrast is constructed. Due to its unsatisfactory performance, another saliency map combining supervised locality-preserving projection and support vector regression is built, to complement the previous saliency map. A final saliency map can be obtained by fusing these two saliency maps. The proposed method is evaluated on the publicly available dataset MSRA-1000 and compared with 13 state-of-the-art methods. Experimental results indicate that the proposed method outperforms existing schemes both in qualitative and quantitative comparisons.
C1 [Shi, Yanjiao; Yi, Yugen; Yan, Hexin; Dai, Jiangyan; Zhang, Ming; Kong, Jun] NE Normal Univ, Sch Comp Sci & Informat Technol, Key Lab Intelligent Informat Proc Jilin Univ, Changchun 130117, Peoples R China.
   [Shi, Yanjiao; Yi, Yugen; Kong, Jun] NE Normal Univ, Sch Math & Stat, Changchun 130117, Peoples R China.
C3 Northeast Normal University - China; Northeast Normal University - China
RP Zhang, M (corresponding author), NE Normal Univ, Sch Comp Sci & Informat Technol, Key Lab Intelligent Informat Proc Jilin Univ, Changchun 130117, Peoples R China.
EM shiyj475@nenu.edu.cn; zhangm545@nenu.edu.cn; kongj435@nenu.edu.cn
RI dai, jiangyan/G-1692-2013
OI Yi, Yugen/0000-0002-1049-5022; Yi, Yugen/0000-0001-9828-0319
FU Jilin Provincial Science and Technology Department [20130206042GX,
   20140204089GX]; Young Scientific Research Fund Of Jilin Province Science
   And Technology Development Project [201201070, 201201063,
   20130522115JH]; National Natural Science Foundation of China [11271064]
FX This work was supported by the Fund of Jilin Provincial Science and
   Technology Department (No. 20130206042GX, No. 20140204089GX), Young
   Scientific Research Fund Of Jilin Province Science And Technology
   Development Project (No. 201201070, No. 201201063, No. 20130522115JH)
   and National Natural Science Foundation of China (No. 11271064).
CR Achanta R, 2008, LECT NOTES COMPUT SC, V5008, P66
   Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   [Anonymous], 2008, BRIT MACHINE VISION
   [Anonymous], 2005, Advances in neural information processing systems, DOI DOI 10.5555/2976248.2976268
   [Anonymous], P ECCV
   [Anonymous], P CVPR
   [Anonymous], 2006, VOCUS: a visual attention system for object detection and goal-directed search
   Belkin M, 2002, ADV NEUR IN, V14, P585
   Borji A, 2013, IEEE T PATTERN ANAL, V35, P185, DOI 10.1109/TPAMI.2012.89
   Borji A, 2012, LECT NOTES COMPUT SC, V7573, P414, DOI 10.1007/978-3-642-33709-3_30
   Borji A, 2010, IMAGE VISION COMPUT, V28, P1130, DOI 10.1016/j.imavis.2009.10.006
   Brady M, 2003, LNCS, V2879
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Chang KY, 2011, IEEE I CONF COMP VIS, P914, DOI 10.1109/ICCV.2011.6126333
   Chen HT, 2005, PROC CVPR IEEE, P846
   Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236
   Donoser M, 2009, IEEE I CONF COMP VIS, P817, DOI 10.1109/ICCV.2009.5459296
   Frintrop S, 2010, IEEE INT CONF ROBOT, P4531, DOI 10.1109/ROBOT.2010.5509638
   Goferman S, 2010, PROC CVPR IEEE, P2376, DOI DOI 10.1109/CVPR.2010.5539929
   Gopalakrishnan V, 2009, IEEE T MULTIMEDIA, V11, P892, DOI 10.1109/TMM.2009.2021726
   Guo CL, 2010, IEEE T IMAGE PROCESS, V19, P185, DOI 10.1109/TIP.2009.2030969
   Han S, 2010, VISION RES, V50, P2295, DOI 10.1016/j.visres.2010.05.034
   Harel J, 2006, Advances in Neural Information Processing Systems, V19
   Hastie T., 2009, The Elements of Statistical Learning
   Hou X., 2007, IEEE C COMP VIS PATT, V1, P1, DOI DOI 10.1109/CVPR.2007.383267
   Hou XD, 2012, IEEE T PATTERN ANAL, V34, P194, DOI 10.1109/TPAMI.2011.146
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Jiang H., 2013, P CVPR
   Jolliffe I.T., 1986, Principal component analysis, DOI DOI 10.1016/0169-7439(87)80084-9
   Judd T, 2009, IEEE I CONF COMP VIS, P2106, DOI 10.1109/ICCV.2009.5459462
   KOCH C, 1985, HUM NEUROBIOL, V4, P219
   Li Y, 2009, IEEE IMAGE PROC, P3093, DOI 10.1109/ICIP.2009.5414465
   Lin YW, 2010, AAAI CONF ARTIF INTE, P967
   Ma Q, 2010, J ELECTRON IMAGING, V19, DOI 10.1117/1.3302129
   Ma Y. F., 2003, P ACM MM
   Min Chen, 2011, IEEE INFOCOM 2011 - IEEE Conference on Computer Communications. Workshops, P409, DOI 10.1109/INFCOMW.2011.5928847
   Mishra AK, 2009, INT J HUM ROBOT, V6, P361, DOI 10.1142/S0219843609001784
   Nagai Y, P ICDL
   Navalpakkam V., P CVPR
   Parikh N, 2010, J NEURAL ENG, V7, DOI 10.1088/1741-2560/7/1/016006
   Rosenholtz R, 2011, ACM T APPL PERCEPT, V8, DOI 10.1145/1870076.1870080
   Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323
   Sadaka NG, 2009, IEEE IMAGE PROC, P3113, DOI 10.1109/ICIP.2009.5414460
   Shen XH, 2012, PROC CVPR IEEE, P853, DOI 10.1109/CVPR.2012.6247758
   Shi L, 2009, IEEE INT CON MULTI, P270, DOI 10.1109/ICME.2009.5202487
   Tenenbaum JB, 2000, SCIENCE, V290, P2319, DOI 10.1126/science.290.5500.2319
   Torralba A, 2003, J OPT SOC AM A, V20, P1407, DOI 10.1364/JOSAA.20.001407
   TREISMAN AM, 1980, COGNITIVE PSYCHOL, V12, P97, DOI 10.1016/0010-0285(80)90005-5
   Vikram TN, 2012, PATTERN RECOGN, V45, P3114, DOI 10.1016/j.patcog.2012.02.009
   Walther D, 2006, NEURAL NETWORKS, V19, P1395, DOI 10.1016/j.neunet.2006.10.001
   Xu Y, 2010, PATTERN RECOGN, V43, P4165, DOI 10.1016/j.patcog.2010.06.016
   Yan JC, 2010, IEEE SIGNAL PROC LET, V17, P739, DOI 10.1109/LSP.2010.2053200
   Yan SC, 2007, IEEE T PATTERN ANAL, V29, P40, DOI 10.1109/TPAMI.2007.250598
   Yi YG, 2015, MULTIMED TOOLS APPL, V74, P85, DOI 10.1007/s11042-013-1429-5
   Zhai Y., 2006, P 14 ACM INT C MULT, P815, DOI [DOI 10.1145/1180639.1180824, 10.1145/1180639.1180824]
   Zhang LY, 2008, J VISION, V8, DOI 10.1167/8.7.32
   Zhao Q, 2013, SIGNAL PROCESS, V93, P1401, DOI 10.1016/j.sigpro.2012.06.014
   Zhao Q, 2012, J VISION, V12, DOI 10.1167/12.6.22
   Zhao Q, 2011, J VISION, V11, DOI 10.1167/11.3.9
   Zheng ZL, 2007, SIGNAL PROCESS, V87, P2473, DOI 10.1016/j.sigpro.2007.03.006
NR 60
TC 7
Z9 8
U1 1
U2 22
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2015
VL 31
IS 9
BP 1191
EP 1205
DI 10.1007/s00371-014-1005-7
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CO8AY
UT WOS:000359388100004
DA 2024-07-18
ER

PT J
AU Leonardi, V
   Vidal, V
   Daniel, M
   Mari, JL
AF Leonardi, Valentin
   Vidal, Vincent
   Daniel, Marc
   Mari, Jean-Luc
TI Multiple reconstruction and dynamic modeling of 3D digital objects using
   a morphing approach
SO VISUAL COMPUTER
LA English
DT Article
DE Geometric modeling; Surface reconstruction; Dynamic modeling; Mesh
   morphing
ID SURFACE RECONSTRUCTION; SEGMENTATION; CT; TRIANGULATION; REGISTRATION;
   TRACKING; IMAGES; MOTION
AB Organ segmentation and motion simulation of organs can be useful for many clinical purposes such as organ study, diagnostic aid, therapy planning or even tumor destruction. In this paper we present a full workflow starting from a CT-Scan resulting in kidney motion simulation and tumor tracking. Our method is divided into three major steps: kidney segmentation, surface reconstruction and animation. The segmentation is based on a semi-automatic region-growing approach that is refined to improve its results. The reconstruction is performed using the Poisson surface reconstruction and gives a manifold three-dimensional (3D) model of the kidney. Finally, the animation is accomplished using an automatic mesh morphing among the models previously obtained. Thus, the results are purely geometric because they are 3D animated models. Moreover, our method requires only a basic user interaction and is fast enough to be used in a medical environment, which satisfies our constraints. Finally, this method can be easily adapted to magnetic resonance imaging acquisition because only the segmentation part would require minor modifications.
C1 [Leonardi, Valentin; Daniel, Marc; Mari, Jean-Luc] Aix Marseille Univ, CNRS, LSIS, UMR 7296, Marseille, France.
   [Vidal, Vincent] Aix Marseille Univ, LIIE, EA 4264, CERIMED, Marseille, France.
C3 Aix-Marseille Universite; Centre National de la Recherche Scientifique
   (CNRS); Aix-Marseille Universite
RP Leonardi, V (corresponding author), Aix Marseille Univ, CNRS, LSIS, UMR 7296, Marseille, France.
EM valentin.leonardi@univ-amu.fr; vincent.vidal@ap-hm.fr;
   marc.daniel@univ-amu.fr; jean-luc.mari@univ-amu.fr
FU group Novartis; Foundation "Sante, Sport et Developpement Durable"
FX This work was granted by the group Novartis and by the Foundation
   "Sante, Sport et Developpement Durable", presided over Pr. Yvon Berland.
   The authors would like to thank the persons involved in the Kidney Tumor
   Tracking (KiTT) project: Christian Coulange for his precious help, Marc
   Andre, Frederic Cohen Philippe Souteyrand and Julien Frandon for their
   wise advice and for providing CT scan data, and Pierre-Henri Rolland for
   his support.
CR Ahmed MN, 2002, IEEE T MED IMAGING, V21, P193, DOI 10.1109/42.996338
   Akkouche S, 2001, COMPUT GRAPH FORUM, V20, P67, DOI 10.1111/1467-8659.00479
   Akkouche S., 1998, IMPLICIT SURFACES, V98, P139
   Alexa Marc, 2000, P COMP GRAPH INT TEC
   Amrani M., 2004, GEOMETRIC MODELING T, P171
   [Anonymous], 1987, SIGGRAPH
   [Anonymous], J MED INFORM TECHNOL
   ARNOLD P, 2011, MICCAI, V6892, P623
   Atkins MS, 1998, IEEE T MED IMAGING, V17, P98, DOI 10.1109/42.668699
   Bardinet E, 1998, COMPUT VIS IMAGE UND, V71, P39, DOI 10.1006/cviu.1997.0595
   Bardinet E, 1996, Med Image Anal, V1, P129, DOI 10.1016/S1361-8415(96)80009-0
   Barequet G, 2000, VISUAL COMPUT, V16, P116, DOI 10.1007/s003710050201
   BOES JL, 1995, P SOC PHOTO-OPT INS, V2573, P244, DOI 10.1117/12.216418
   Boissonnat J. D., 1986, COMPUT VIS GRAPH IMA, V44, P1
   Boscolo R, 2002, RADIOGRAPHICS, V22, P437, DOI 10.1148/radiographics.22.2.g02mr26437
   Cotin S, 1999, IEEE T VIS COMPUT GR, V5, P62, DOI 10.1109/2945.764872
   Cresson T, 2010, PROC SPIE, V7623, DOI 10.1117/12.844098
   Cresson T, 2009, IEEE ENG MED BIO, P1008, DOI 10.1109/IEMBS.2009.5333869
   Delingette H., 1994, RAPPORT RECHERCHE IN, V2214
   EKOULE AB, 1991, ACM T GRAPHIC, V10, P182, DOI 10.1145/108360.108363
   Fritsch D, 1997, LECT NOTES COMPUT SC, V1230, P127
   Fuchs H., 1977, COMMUN ACM, V20
   Gao J., 1998, INT C IMAGE PROCESS, V3
   Ginneken B.V., 2002, IEEE T MED IMAGING, V21
   Gregory A, 1998, COMP ANIM CONF PROC, P64, DOI 10.1109/CA.1998.681909
   Hartmann E, 1998, VISUAL COMPUT, V14, P95, DOI 10.1007/s003710050126
   Held K., 1997, IEEE T MED IMAGING
   Hostettler A, 2008, LECT NOTES COMPUT SC, V5104, P89, DOI 10.1007/978-3-540-70521-5_10
   Jie Wu, 2009, Journal of Biomedical Science & Engineering, V2, P1, DOI 10.4236/jbise.2009.21001
   Ju T, 2005, VISUAL COMPUT, V21, P764, DOI 10.1007/s00371-005-0321-3
   Kanai T., 2000, P COMPUT GRAPH APPL, V20
   Kapur T., 1998, MED IMAGE COMPUT COM
   Kaus MR, 2001, RADIOLOGY, V218, P586, DOI 10.1148/radiology.218.2.r01fe44586
   Kazhdan M., 2006, EUROGRAPHICS S GEOME, V7, P1
   Kent J., 1992, COMPUT GRAPH, V26
   Klein R., 1999, Proceedings. Seventh Pacific Conference on Computer Graphics and Applications (Cat. No.PR00293), P198, DOI 10.1109/PCCGA.1999.803363
   Lee A. W. F., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P95, DOI 10.1145/280814.280828
   Lee AWF, 1999, COMP GRAPH, P343, DOI 10.1145/311535.311586
   Lee CC, 2003, IEEE T INF TECHNOL B, V7, P208, DOI 10.1109/TITB.2003.813795
   Lei T., 1992, IEEE T MED IMAGING, V11
   Leonardi V, 2012, WSCG'2012, CONFERENCE PROCEEDINGS, PTS I & II, P179
   Liang Z., 1994, IEEE T MED IMAGING, V13
   Lim SP, 2014, ARTIF INTELL REV, V42, P59, DOI 10.1007/s10462-012-9329-z
   Lin DT, 2006, IEEE T INF TECHNOL B, V10, P59, DOI 10.1109/TITB.2005.855561
   Lin JS, 1996, INT J BIOMED COMPUT, V42, P205, DOI 10.1016/0020-7101(96)01199-3
   Murphy MJ, 2003, INT J RADIAT ONCOL, V55, P1400, DOI 10.1016/S0360-3016(02)04597-2
   Nakamoto M, 2008, LECT NOTES COMPUT SC, V5128, P359, DOI 10.1007/978-3-540-79982-5_39
   Nicolau SA, 2007, LECT NOTES COMPUT SC, V4792, P77
   Noe KO, 2008, ACTA ONCOL, V47, P1286, DOI 10.1080/02841860802258760
   Olbrich B, 2005, INT CONGR SER, V1281, P248, DOI 10.1016/j.ics.2005.03.285
   Pavlovich CP, 2002, J UROLOGY, V167, P10, DOI 10.1016/S0022-5347(05)65371-2
   Pham D., 1998, IEEE T MED IMAGING
   Pihuit A., 2009, 22 JOURN ASS FRANC I
   Polthier K., 2000, TECHNICAL REPORT
   Preiswerk Frank, 2012, Abdominal Imaging. Computational and Clinical Applications. Third International Workshop Held in Conjunction with MICCAI 2011. Revised Selected Papers, P207, DOI 10.1007/978-3-642-28557-8_26
   Rajapakse J., 2001, IEEE T MED IMAGING, V16
   Reyes Mauricio, 2005, MED IMAGE COMPUTING, P396
   ROHLFING T, 2001, P SOC PHOTO-OPT INS, V4319, P337
   Rueckert D, 1999, IEEE T MED IMAGING, V18, P712, DOI 10.1109/42.796284
   Ruskó L, 2009, MED IMAGE ANAL, V13, P871, DOI 10.1016/j.media.2009.07.009
   Sarrut David, 2006, MED PHYS, V33
   Schweikard A., 2000, J COMPUT AIDED SURG
   Shirato H, 2000, INT J RADIAT ONCOL, V48, P435, DOI 10.1016/S0360-3016(00)00625-8
   Tsingos N., 1995, COMPUTER GRAPHICS IN, V5, P3
   Vilarino D., 1999, J VLSI SIGNAL PROC, V23, P403
   Villard P. F., 2006, THESIS U CLAUDE BERN, P152
   Wells WM, 1996, IEEE T MED IMAGING, V15, P429, DOI 10.1109/42.511747
   WRAZIDLO W, 1991, EUR J RADIOL, V12, P11, DOI 10.1016/0720-048X(91)90125-F
   XU SB, 1988, COMPUT VISION GRAPH, V44, P270, DOI 10.1016/0734-189X(88)90124-7
   Yan H. B., 2007, COMPUT SCI TECHNOL, V1
   Yao CL, 2008, ANAESTHESIA, V63, P865, DOI 10.1111/j.1365-2044.2008.05562.x
   Zhang YY, 2001, IEEE T MED IMAGING, V20, P45, DOI 10.1109/42.906424
NR 72
TC 2
Z9 3
U1 2
U2 18
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2015
VL 31
IS 5
BP 557
EP 574
DI 10.1007/s00371-014-0978-6
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CH2CF
UT WOS:000353831400005
DA 2024-07-18
ER

PT J
AU Nguyen, TN
   Miyata, K
AF Thao-Ngoc Nguyen
   Miyata, Kazunori
TI Multi-scale region perpendicular local binary pattern: an effective
   feature for interest region description
SO VISUAL COMPUTER
LA English
DT Article
DE Local binary pattern; Perpendicular; Intensity order; Multi-support
   regions; Interest regions; Image matching; Feature descriptor
ID SCALE
AB This paper proposes the perpendicular local binary pattern (PLBP) for efficiently describing textures in an interest region. Its novelty is two-fold: (1) the candidate generation scheme provides a set of patterns for each pixel, instead of conventionally assigning one pattern per pixel, and (2) an adaptive threshold based on the image contrast of a region is used. These modifications successfully enhance the robustness of PLBP to Gaussian noise as well as in near-uniform regions. We introduce the novel multi-scale region PLBP descriptor, which adopts the PLBP as its core feature. It defines multiple support regions from an interest point, sequentially performs ring-shaped and intensity order-based segmentations on each region, and pools PLBPs to corresponding segments. These steps are controlled easily by a set of parameters, thus offering high flexibility. Experimental results on challenging benchmarks, including three datasets of image matching and two datasets of object recognition, demonstrate the effectiveness of the proposed descriptor in handling common photometric and geometric transformations. It significantly improves the robustness, compared with current state-of-the-art descriptors, while maintaining a reasonable operational cost.
C1 [Thao-Ngoc Nguyen; Miyata, Kazunori] Japan Adv Inst Sci & Technol, Sch Knowledge Sci, Nomi, Ishikawa 9231292, Japan.
C3 Japan Advanced Institute of Science & Technology (JAIST)
RP Nguyen, TN (corresponding author), Japan Adv Inst Sci & Technol, Sch Knowledge Sci, Nomi, Ishikawa 9231292, Japan.
EM thao.nguyen@jaist.ac.jp; miyata@jaist.ac.jp
OI Miyata, Kazunori/0000-0002-1582-0058
CR Anbarjafari G, 2013, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2013-6
   [Anonymous], PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2005.45
   [Anonymous], DOVER BOOKS ASTRONOM
   [Anonymous], WSEAS T SIGNAL PROCE
   Brown M, 2007, INT J COMPUT VISION, V74, P59, DOI 10.1007/s11263-006-0002-3
   Chan CH, 2012, IEEE T INF FOREN SEC, V7, P602, DOI 10.1109/TIFS.2011.2175920
   Cordes K, 2013, LECT NOTES COMPUT SC, V8047, P327, DOI 10.1007/978-3-642-40261-6_39
   Fan B, 2012, IEEE T PATTERN ANAL, V34, P2031, DOI 10.1109/TPAMI.2011.277
   Feng Tang, 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2631, DOI 10.1109/CVPRW.2009.5206550
   Gupta R, 2008, LECT NOTES COMPUT SC, V5303, P265, DOI 10.1007/978-3-540-88688-4_20
   Gupta R, 2010, PROC CVPR IEEE, P334, DOI 10.1109/CVPR.2010.5540195
   Heikkilä M, 2006, IEEE T PATTERN ANAL, V28, P657, DOI 10.1109/TPAMI.2006.68
   Heikkilä M, 2009, PATTERN RECOGN, V42, P425, DOI 10.1016/j.patcog.2008.08.014
   Ke Y, 2004, PROC CVPR IEEE, P506
   Lazebnik S, 2005, IEEE T PATTERN ANAL, V27, P1265, DOI 10.1109/TPAMI.2005.151
   Liao SC, 2010, PROC CVPR IEEE, P1301, DOI 10.1109/CVPR.2010.5539817
   Lindeberg T, 1998, INT J COMPUT VISION, V30, P79, DOI 10.1023/A:1008045108935
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Matas J, 2004, IMAGE VISION COMPUT, V22, P761, DOI 10.1016/j.imavis.2004.02.006
   Mattivi R, 2011, STUD COMPUT INTELL, V332, P69
   Mikolajczyk K, 2005, INT J COMPUT VISION, V65, P43, DOI 10.1007/s11263-005-3848-x
   Mikolajczyk K, 2005, IEEE T PATTERN ANAL, V27, P1615, DOI 10.1109/TPAMI.2005.188
   Mikolajczyk K, 2004, INT J COMPUT VISION, V60, P63, DOI 10.1023/B:VISI.0000027790.02288.f2
   Mikolajczyk K, 2002, LECT NOTES COMPUT SC, V2350, P128, DOI 10.1007/3-540-47969-4_9
   Nister David, 2006, CVPR
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Russell BC, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508425
   Song TC, 2013, IEEE SIGNAL PROC LET, V20, P59, DOI 10.1109/LSP.2012.2229273
   Takala V, 2005, LECT NOTES COMPUT SC, V3540, P882
   Tan XY, 2010, IEEE T IMAGE PROCESS, V19, P1635, DOI 10.1109/TIP.2010.2042645
   Tola E, 2010, IEEE T PATTERN ANAL, V32, P815, DOI 10.1109/TPAMI.2009.77
   Tuytelaars T, 2004, INT J COMPUT VISION, V59, P61, DOI 10.1023/B:VISI.0000020671.28016.e8
   Wang ZH, 2011, IEEE I CONF COMP VIS, P603, DOI 10.1109/ICCV.2011.6126294
   Winder S.A. J., 2007, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, P1
   Zambanini S, 2013, LECT NOTES COMPUT SC, V7944, P11
NR 35
TC 7
Z9 8
U1 0
U2 12
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2015
VL 31
IS 4
BP 391
EP 406
DI 10.1007/s00371-014-0934-5
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CD2IW
UT WOS:000350901600003
DA 2024-07-18
ER

PT J
AU Valíková, M
   Chalmoviansky, P
AF Valikova, Miroslava
   Chalmoviansky, Pavel
TI Visualisation of complex functions on Riemann sphere
SO VISUAL COMPUTER
LA English
DT Article
DE Riemann surfaces; Multi-valued function; Visualization; Singular points
AB The notion of a multi-valued function is frequent in complex analysis and related fields. A graph of such a function helps to inspect the function, however, the methods working with single-valued functions can not be applied directly. To visualize such a type of function, its Riemann surface is often used as a domain of the function. On such a surface, a multi-valued function behaves like a single-valued function. In our paper, we give a quick overview of the proposed method of visualization of a single-valued complex function over its Riemann sphere. Then, we pass to the adaptation of this method on the visualization of a multi-valued complex function. Our method uses absolute value and argument of the function to create the graph in 3D space over the Riemann sphere. Such a graph provides an overview of the function behavior over its whole domain, the amount and the position of its branch points, as well as poles and zeros and their multiplicity. We have also created an algorithm for adaptive grid which provides higher density of vertices in areas with higher curvature of the graph. The algorithm eliminates the alias in places where the branches are joined together.
C1 [Valikova, Miroslava; Chalmoviansky, Pavel] Comenius Univ, Fac Math Phys & Informat, Bratislava, Slovakia.
C3 Comenius University Bratislava
RP Valíková, M (corresponding author), Comenius Univ, Fac Math Phys & Informat, Bratislava, Slovakia.
EM miroslava.valikova@fmph.uniba.sk; pavel.chalmoviansky@fmph.uniba.sk
FU  [VEGA 1/0330/13];  [UK/465/2013]
FX The authors were kindly supported by the projects VEGA 1/0330/13 and
   UK/465/2013. The authors thank the anonymous reviewers for valuable
   comments which helped to improve the paper considerably.
CR [Anonymous], VISUALIZING COMPLEX
   Arnold V. I., 2012, Singularities of differentiable maps
   Batorova M., 2013, SPRING C CO IN PRESS
   Bobenko AI, 2011, LECT NOTES MATH, V2013, P1, DOI 10.1007/978-3-642-17413-1
   Chen CC, 2002, COMMUN PUR APPL MATH, V55, P728, DOI 10.1002/cpa.3014
   Greuel G.M., 2007, Springer Monographs in Mathematics
   Hansen G.A., 2005, Mesh enhancement: selected elliptic methods, foundations and applications
   Jones Gareth A., 1987, COMPLEX FUNCTIONS AL
   Kalberer F., 2007, COMPUT GRAPH FORUM
   Kranich S., 2012, THESIS
   Lang S, 1999, Complex Analysis, V4th
   LUNDMARK H, 2004, VISUALIZING COMPLEX
   Nieser M, 2010, LECT NOTES COMPUT SC, V6130, P161, DOI 10.1007/978-3-642-13411-1_11
   Olver F. W. J., 2010, NIST HDB MATH FUNCTI
   Poelke K., 2012, EXPLORING COMPLEX FU
   Poelke K, 2012, IEEE COMPUT GRAPH, V32, P90, DOI 10.1109/MCG.2012.100
   Poelke K, 2009, COMPUT GRAPH FORUM, V28, P735, DOI 10.1111/j.1467-8659.2009.01479.x
   Trott M., 2009, RIEMANN SURFACES RIE
   Valikova M., 2012, P S COMP GEOM SCG 20
   Wegert E., 2012, Visual complex functions. An introduction with phase portraits
   YIN X, 2007, COMPUTING SHORTEST C
NR 21
TC 1
Z9 1
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2015
VL 31
IS 2
BP 141
EP 154
DI 10.1007/s00371-014-0928-3
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AZ6EG
UT WOS:000348310800005
DA 2024-07-18
ER

PT J
AU Thiery, JM
   Tierny, J
   Boubekeur, T
AF Thiery, Jean-Marc
   Tierny, Julien
   Boubekeur, Tamy
TI Jacobians and Hessians of mean value coordinates for closed triangular
   meshes
SO VISUAL COMPUTER
LA English
DT Article
DE Cage coordinates; Mean value coordinates; Constrained interpolation;
   Implicit cage deformation
AB Mean value coordinates provide an efficient mechanism for the interpolation of scalar functions defined on orientable domains with a nonconvex boundary. They present several interesting features, including the simplicity and speed that yield from their closed-form expression. In several applications though, it is desirable to enforce additional constraints involving the partial derivatives of the interpolated function, as done in the case of the Green coordinates approximation scheme (Ben-Chen, Weber, Gotsman, ACM Trans. Graph.:1-11, 2009) for interactive 3D model deformation.
   In this paper, we introduce the analytic expressions of the Jacobian and the Hessian of functions interpolated through mean value coordinates. We provide these expressions both for the 2D and 3D case. We also provide a thorough analysis of their degenerate configurations along with accurate approximations of the partial derivatives in these configurations. Extensive numerical experiments show the accuracy of our derivation. In particular, we illustrate the improvements of our formulae over a variety of finite differences schemes in terms of precision and usability. We demonstrate the utility of this derivation in several applications, including cage-based implicit 3D model deformations (i.e., variational MVC deformations). This technique allows for easy and interactive model deformations with sparse positional, rotational, and smoothness constraints. Moreover, the cages produced by the algorithm can be directly reused for further manipulations, which makes our framework directly compatible with existing software supporting mean value coordinates based deformations.
C1 [Thiery, Jean-Marc; Tierny, Julien; Boubekeur, Tamy] Telecom ParisTech, CNRS LTCI, F-75013 Paris, France.
C3 Centre National de la Recherche Scientifique (CNRS); IMT - Institut
   Mines-Telecom; Institut Polytechnique de Paris; Telecom Paris
RP Thiery, JM (corresponding author), Telecom ParisTech, CNRS LTCI, 43 Rue Barrault, F-75013 Paris, France.
EM thiery@telecom-paristech.fr
CR Babuska I, 2004, COMPUT METHOD APPL M, V193, P4057, DOI 10.1016/j.cma.2004.03.002
   Ben-Chen M, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531340
   Borosan P., 2010, P EUROGRAPHICS
   Etiene T, 2012, IEEE T VIS COMPUT GR, V18, P952, DOI 10.1109/TVCG.2011.109
   Etiene T, 2009, IEEE T VIS COMPUT GR, V15, P1227, DOI 10.1109/TVCG.2009.194
   Flannery B. P., 1992, NUMERICAL RECIPES C
   Floater M. S., 1998, International Journal of Shape Modeling, V4, P165, DOI 10.1142/S021865439800012X
   Floater MS, 2005, COMPUT AIDED GEOM D, V22, P623, DOI 10.1016/j.cagd.2005.06.004
   Floater MS, 1997, COMPUT AIDED GEOM D, V14, P231, DOI 10.1016/S0167-8396(96)00031-3
   Floater MS, 2003, COMPUT AIDED GEOM D, V20, P19, DOI 10.1016/S0167-8396(03)00002-5
   FORNBERG B, 1981, ACM T MATH SOFTWARE, V7, P512, DOI 10.1145/355972.355979
   Hormann K, 2006, ACM T GRAPHIC, V25, P1424, DOI 10.1145/1183287.1183295
   Huang J, 2006, ACM T GRAPHIC, V25, P1126, DOI 10.1145/1141911.1142003
   Joshi P, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239522
   Ju T, 2005, ACM T GRAPHIC, V24, P561, DOI 10.1145/1073204.1073229
   Langer T., 2006, S GEOMETRY PROCESSIN, P81
   Lipman Y., 2007, S GEOM PROC, P117
   Lipman Y, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360677
   LOOP CT, 1989, ACM T GRAPHIC, V8, P204, DOI 10.1145/77055.77059
   Malsch E., 2003, P INT MATH S
   Meyer M., 2002, Journal of Graphics Tools, V7, P13, DOI 10.1080/10867651.2002.10487551
   Nieser M, 2011, COMPUT GRAPH FORUM, V30, P1397, DOI 10.1111/j.1467-8659.2011.02014.x
   Squire W, 1998, SIAM REV, V40, P110, DOI 10.1137/S003614459631241X
   Thiery J.M., 2013, MVC DERIVATIVES C IM
   Urago Masataka., 2000, Transactions of the Japan Society of Mechanical Engineers Series A, V66.642, P254, DOI [10.1299/kikaia.66.254, DOI 10.1299/KIKAIA.66.254]
   Wachspress E.L., 1975, A rational finite element basis
   Warren J, 1996, ADV COMPUT MATH, V6, P97, DOI 10.1007/BF02127699
   Warren J, 2007, ADV COMPUT MATH, V27, P319, DOI 10.1007/s10444-005-9008-6
NR 28
TC 11
Z9 11
U1 0
U2 4
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2014
VL 30
IS 9
BP 981
EP 995
DI 10.1007/s00371-013-0889-y
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AO5GP
UT WOS:000341372200003
DA 2024-07-18
ER

PT J
AU Zmugg, R
   Thaller, W
   Krispel, U
   Edelsbrunner, J
   Havemann, S
   Fellner, DW
AF Zmugg, Rene
   Thaller, Wolfgang
   Krispel, Ulrich
   Edelsbrunner, Johannes
   Havemann, Sven
   Fellner, Dieter W.
TI Procedural architecture using deformation-aware split grammars
SO VISUAL COMPUTER
LA English
DT Article
DE Split grammars; Free-form deformations; Architectural models
ID SHAPE GRAMMARS
AB With the current state of video games growing in scale, manual content creation may no longer be feasible in the future. Split grammars are a promising technology for large-scale procedural generation of urban structures, which are very common in video games. Buildings with curved parts, however, can currently only be approximated by static pre-modelled assets, and rules apply only to planar surface parts. We present an extension to split grammar systems that allow the creation of curved architecture through integration of free-form deformations at any level in a grammar. Further split rules can then proceed in two different ways. They can either adapt to these deformations so that repetitions can adjust to more or less space, while maintaining length constraints, or they can split the deformed geometry with straight planes to introduce straight structures on deformed geometry.
C1 [Zmugg, Rene; Thaller, Wolfgang; Edelsbrunner, Johannes; Havemann, Sven; Fellner, Dieter W.] Graz Univ Technol, Inst Comp Graph & Knowledge Visualizat CGV, A-8010 Graz, Austria.
   [Krispel, Ulrich; Fellner, Dieter W.] Tech Univ Darmstadt, Graphisch Interakt Syst GRIS, Darmstadt, Germany.
   [Fellner, Dieter W.] Fraunhofer Gesell eV, Fraunhofer IGD, Darmstadt, Germany.
C3 Graz University of Technology; Technical University of Darmstadt
RP Zmugg, R (corresponding author), Graz Univ Technol, Inst Comp Graph & Knowledge Visualizat CGV, A-8010 Graz, Austria.
EM r.zmugg@cgv.tugraz.at; w.thaller@cgv.tugraz.at;
   ulrich.krispel@gris.tu-darmstadt.de; j.edelsbrunner@cgv.tugraz.at;
   s.havemann@cgv.tugraz.at; d.fellner@igd.fraunhofer.de
OI Krispel, Ulrich/0000-0001-8984-635X; Fellner, Dieter
   W./0000-0001-7756-0901
CR [Anonymous], THESIS BRAUNSCHWEIG
   [Anonymous], ACM SIGGRAPH
   Eppstein D., 1998, Proceedings of the Fourteenth Annual Symposium on Computational Geometry, P58, DOI 10.1145/276884.276891
   Farin G., 1997, COMPUTER SCI SCI COM
   Huang CY, 2013, VISUAL COMPUT, V29, P1303, DOI 10.1007/s00371-012-0771-3
   Kelly T, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1944846.1944854
   Krecklau L, 2010, COMPUT GRAPH FORUM, V29, P2291, DOI 10.1111/j.1467-8659.2010.01714.x
   Krecklau L, 2011, COMPUT GRAPH FORUM, V30, P335, DOI 10.1111/j.1467-8659.2011.01864.x
   Müller P, 2006, ACM T GRAPHIC, V25, P614, DOI 10.1145/1141911.1141931
   Stewart N., 1998, HWWS '98: Proceedings of the ACM SIGGRAPH/EUROGRAPHICS workshop on Graphics hardware, P25
   STINY G, 1980, ENVIRON PLANN B, V7, P343, DOI 10.1068/b070343
   Teoh S.T., 2009, P 5 EUR C COMP AESTH, P17
   Thaller W, 2013, COMPUT GRAPH-UK, V37, P707, DOI 10.1016/j.cag.2013.05.012
   Wonka P, 2003, ACM T GRAPHIC, V22, P669, DOI 10.1145/882262.882324
   Zmugg R., 2013, 2013 INT C CYB IEEE
NR 15
TC 7
Z9 7
U1 1
U2 8
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2014
VL 30
IS 9
BP 1009
EP 1019
DI 10.1007/s00371-013-0912-3
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AO5GP
UT WOS:000341372200005
DA 2024-07-18
ER

PT J
AU Noh, S
   Hashimoto, S
   Yamanaka, D
   Kamiyama, Y
   Inami, M
   Igarashi, T
AF Noh, Seung-tak
   Hashimoto, Sunao
   Yamanaka, Daiki
   Kamiyama, Youichi
   Inami, Masahiko
   Igarashi, Takeo
TI Design and enhancement of painting interface for room lights
SO VISUAL COMPUTER
LA English
DT Article
DE Interactive lighting design; Painting interface; Image-based lighting
AB We propose a novel painting interface that enables users to design an illumination distribution for a real room using an array of computer-controlled lights. Users specify which area of the room is to be well-lit and which is to be dark by painting a target illumination distribution on a tablet device displaying the image obtained by a camera mounted in the room. The painting result is overlaid on the camera image as contour lines of the target illumination intensity. The system then runs an optimization to calculate light parameters to deliver the requested illumination condition. We implemented a GPU-based parallel search to achieve real-time processing. In our system, we used actuated lights that can change the lighting direction to generate the requested illumination condition more faithfully than static lights. We built a miniature-scale experimental environment and ran a user study to compare our method with a standard direct manipulation method using sliders. The results showed that the users preferred our method for informal light control.
C1 [Noh, Seung-tak; Hashimoto, Sunao; Yamanaka, Daiki; Kamiyama, Youichi; Inami, Masahiko; Igarashi, Takeo] Univ Tokyo, JST ERATO Igarashi Design Interface Project, Tokyo, Japan.
   [Noh, Seung-tak] Korea Adv Inst Sci & Technol, Taejon 305701, South Korea.
   [Hashimoto, Sunao] Meiji Univ, Dept Frontier Media Sci FMS, Tokyo 101, Japan.
   [Inami, Masahiko] Keio Univ, Tokyo, Japan.
C3 University of Tokyo; Korea Advanced Institute of Science & Technology
   (KAIST); Meiji University; Keio University
RP Hashimoto, S (corresponding author), Univ Tokyo, JST ERATO Igarashi Design Interface Project, Tokyo, Japan.
EM seungtak.noh@gmail.com; hashimoto@kougaku-navi.net;
   daikitdaiki@gmail.com; kamiyama@designinterface.jp; inami@inami.info;
   takeo@acm.org
RI Igarashi, Takeo/ITT-5921-2023; Li, Mengqi/AAG-6804-2021
OI Noh, Seung-Tak/0000-0002-7823-0864; INAMI, MASAHIKO/0000-0002-8652-0730
CR Amano Toshiyuki, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P2832, DOI 10.1109/ICPR.2010.694
   [Anonymous], ACM SIGGRAPH 2008 CL
   [Anonymous], RR6915 INRIA
   Anrys F, 2004, 4 IASTED INT C VIS I
   Debevec P, 2002, ACM T GRAPHIC, V21, P547
   Debevec P, 2000, COMP GRAPH, P145, DOI 10.1145/344779.344855
   Debevec P. E., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P369, DOI 10.1145/258734.258884
   Ghosh A., 2005, ACM SIGGRAPH 2005 SK, P41, DOI [10.1145/1187112.1187161, DOI 10.1145/1187112.1187161]
   Goldber D. E., 1988, Machine Learning, V3, P95, DOI 10.1023/A:1022602019183
   Jung T, 2003, DIGITAL DESIGN: RESEARCH AND PRACTICE, P327
   Kennedy J., 1995, 1995 IEEE International Conference on Neural Networks Proceedings (Cat. No.95CH35828), P1942, DOI 10.1109/ICNN.1995.488968
   Kerr WB, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531332
   KIRKPATRICK S, 1983, SCIENCE, V220, P671, DOI 10.1126/science.220.4598.671
   Mohan A, 2007, IEEE T VIS COMPUT GR, V13, P652, DOI 10.1109/TVCG.2007.1008
   Okabe M, 2007, PACIFIC GRAPHICS 2007: 15TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, P171, DOI 10.1109/PG.2007.9
   Park H, 2007, PROCEEDINGS OF THE SIXTH INTERNATIONAL SYMPOSIUM ON INFORMATION PROCESSING IN SENSOR NETWORKS, P370, DOI 10.1109/IPSN.2007.4379697
   Pellacini F, 2002, ACM T GRAPHIC, V21, P563, DOI 10.1145/566570.566617
   PELLACINI F, 2010, ACM T GRAPHIC, V29
   Pellacini F, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1243980.1243983, 10.1145/1276377.1276444, 10.1145/1239451.1239505]
   Schoeneman C., 1993, Computer Graphics Proceedings, P143, DOI 10.1145/166117.166135
   Shacked R, 2001, COMPUT GRAPH FORUM, V20, pC215, DOI 10.1111/1467-8659.00514
   Sheng Y, 2011, IEEE T VIS COMPUT GR, V17, P38, DOI 10.1109/TVCG.2009.209
   Singhvi V., 2005, Proceedings of the 3rd international conference on Embedded networked sensor systems, P218, DOI DOI 10.1145/1098918.1098942
   Wen Yao-Jung., 2008, Wireless Hive Networks Conference, P1, DOI DOI 10.1109/WHNC.2008.4629493
NR 24
TC 5
Z9 5
U1 0
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2014
VL 30
IS 5
BP 467
EP 478
DI 10.1007/s00371-013-0872-7
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AF2AN
UT WOS:000334515100001
DA 2024-07-18
ER

PT J
AU Wang, SX
   Qin, SF
   Gao, MT
AF Wang, Shuxia
   Qin, Shengfeng
   Gao, Mantun
TI New grouping and fitting methods for interactive overtraced sketches
SO VISUAL COMPUTER
LA English
DT Article
DE Overtraced stroke; Vector graphics; Grouping; Fitting; Freehand sketch;
   2D interpretation
ID INTERFACE
AB This paper describes a new method for recognizing overtraced strokes to 2D geometric primitives, which are further interpreted as 2D line drawings. This method can support rapid grouping and fitting of overtraced polylines or conic curves based on the classified characteristics of each stroke during its preprocessing stage. The orientation and its endpoints of a classified stroke are used in the stroke grouping process. The grouped strokes are then fitted with 2D geometry. This method can deal with overtraced sketch strokes in both solid and dash linestyles, fit grouped polylines as a whole polyline and simply fit conic strokes without computing the direction of a stroke. It avoids losing joint information due to segmentation of a polyline into line-segments. The proposed method has been tested with our freehand sketch recognition system (FSR), which is robust and easier to use by removing some limitations embedded with most existing sketching systems which only accept non-overtraced stroke drawing. The test results showed that the proposed method can support freehand sketching based conceptual design with no limitations on drawing sequence, directions and overtraced cases while achieving a satisfactory interpretation rate.
C1 [Wang, Shuxia; Gao, Mantun] Northwestern Polytech Univ, Sch Mech Engn, Xian, Peoples R China.
   [Wang, Shuxia; Qin, Shengfeng] Brunel Univ, Sch Engn & Design, London, England.
C3 Northwestern Polytechnical University; Brunel University
RP Qin, SF (corresponding author), Brunel Univ, Sch Engn & Design, London, England.
EM Sheng.Feng.Qin@brunel.ac.uk
OI Qin, Shengfeng/0000-0001-8538-8136
FU National Natural Science Foundation of China [51105310]
FX This work supported by National Natural Science Foundation of China
   (51105310).
CR [Anonymous], 1998, CHI 98 Cconference Summary on Human Factors in Computing Systems, CHI '98
   Barla Pascal., 2005, Proc. EGSR, P183
   Bruno F., 2003, P VIRT CONC BIARR FR, P5
   Chansri N, 2012, INT J ADV MANUF TECH, V59, P221, DOI 10.1007/s00170-011-3487-z
   Chansri N, 2010, NEW WORLD SITUATION: NEW DIRECTIONS IN CONCURRENT ENGINEERING, P11, DOI 10.1007/978-0-85729-024-3_2
   Di Fiore F., 2002, AAAI SPRING S, P32
   Fleisch T, 2004, 1 EUR C SKETCH BAS I, P161
   Hertzman A., 2002, Rendering Techniques 2002. Eurographics Workshop Proceedings, P233
   Jenkins D. L., 1992, Intelligent Systems Engineering, V1, P31, DOI 10.1049/ise.1992.0004
   Kang D.J., 2004, AAAI FALL S SERIES M, P92
   Kara L.B., 2006, P 2006 ACM S SOLID P, P149, DOI [10.1145/1128888.1128909, DOI 10.1145/1128888.1128909]
   Kara LB, 2005, COMPUT GRAPH-UK, V29, P501, DOI 10.1016/j.cag.2005.05.004
   Ku DC, 2006, WSCG 2006: FULL PAPERS PROCEEDINGS, P263
   Liu Ligang, 2011, P 2011 SIGGRAPH ASIA, V30, P1
   Lu J., 2012, ACM T GRAPHICS TOG, VS31, P1
   Ma CX, 2011, IEEE T AUTOM SCI ENG, V8, P431, DOI 10.1109/TASE.2010.2086444
   Masry M, 2005, COMPUT GRAPH-UK, V29, P563, DOI 10.1016/j.cag.2005.05.008
   Matsuda K., 1997, Proceedings. 1997 International Conference on Shape Modeling and Applications (Cat. No.97TB100098), P55, DOI 10.1109/SMA.1997.634882
   Mitani J, 2002, INT FED INFO PROC, V80, P85
   Nataneli G., 2011, IEEE COMPUT GRAPH, V99, P1
   Olsen L, 2009, COMPUT GRAPH-UK, V33, P85, DOI 10.1016/j.cag.2008.09.013
   Pavlidis T., 1985, Computer Graphics, V19, P225, DOI 10.1145/325165.325240
   Pusch R, 2007, VISUAL COMPUT, V23, P955, DOI 10.1007/s00371-007-0160-5
   Qin SF, 2001, PATTERN RECOGN, V34, P1885, DOI 10.1016/S0031-3203(00)00122-9
   Qin SF, 2001, P I MECH ENG B-J ENG, V215, P111, DOI 10.1243/0954405011515073
   Rosin PL, 1997, IEEE T PATTERN ANAL, V19, P659, DOI 10.1109/34.601253
   ROSIN PL, 1994, BMVC94 - PROCEEDINGS OF THE 5TH BRITISH MACHINE VISION CONFERENCE, VOLS 1 AND 2, P265
   Sezgin TM, 2004, P AAAI SPRING S SER, P141
   Sheng B, 2008, VISUAL COMPUT, V24, P745, DOI 10.1007/s00371-008-0256-6
   Shesh A, 2004, COMPUT GRAPH FORUM, V23, P301, DOI 10.1111/j.1467-8659.2004.00761.x
   Shpitalni M, 1997, J MECH DESIGN, V119, P131, DOI 10.1115/1.2828775
   Taele P, 2010, J VISUAL LANG COMPUT, V21, P109, DOI 10.1016/j.jvlc.2009.12.004
   Wang SX, 2007, LECT NOTES COMPUT SC, V4551, P161
   Wang SX, 2009, INT C COMP AID IND D, P586, DOI 10.1109/CAIDCD.2009.5375407
   [王淑侠 WANG ShuXia], 2008, [模式识别与人工智能, Pattern Recognition and Artificial Intelligence], V21, P317
NR 35
TC 9
Z9 11
U1 1
U2 14
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2014
VL 30
IS 3
BP 285
EP 297
DI 10.1007/s00371-013-0844-y
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AB3LU
UT WOS:000331693100004
DA 2024-07-18
ER

PT J
AU Zhu, W
   Hu, RZ
   Liu, LG
AF Zhu, Wei
   Hu, Ruizhen
   Liu, Ligang
TI Grey conversion via perceived-contrast
SO VISUAL COMPUTER
LA English
DT Article
DE Color-to-gray conversion; Filter theory; Channel salience; Perceived
   contrast
ID COLOR; IMAGE; MODEL
AB This paper presents a new color-to-gray conversion algorithm capturing the perceived appearance of color images. Based on the Filter Theory, we formulate a novel measurement of channel-level distinction, called Channel Salience, to depict the filter level of three color stimuli. This salience metric guides a contrast adjustment process to enhance the perceived grayscale in the final output with a two-steps conversion. Experimental results show that our algorithm produces pleasing results for a variety of color images and we further extend the Channel Salience to edge detection.
C1 [Zhu, Wei; Hu, Ruizhen] Zhejiang Univ, Dept Math, Hangzhou 310027, Peoples R China.
   [Liu, Ligang] Univ Sci & Technol China, Sch Math Sci, Hefei 230026, Anhui, Peoples R China.
C3 Zhejiang University; Chinese Academy of Sciences; University of Science
   & Technology of China, CAS
RP Zhu, W (corresponding author), Zhejiang Univ, Dept Math, Hangzhou 310027, Peoples R China.
EM weizhu.zju@gmail.com; ligang.liu@gmail.com
CR Ancuti C.O., 2011, P IEEE COMP VIS PATT
   Ancuti CO, 2011, LECT NOTES COMPUT SC, V6492, P79, DOI 10.1007/978-3-642-19315-6_7
   Ancuti CO, 2010, IEEE IMAGE PROC, P149, DOI 10.1109/ICIP.2010.5652328
   Angulo J, 2003, 2003 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL 2, PROCEEDINGS, P125
   [Anonymous], 2007, PROC 3 EUR C COMPUTA
   Bala R, 2004, 12TH COLOR IMAGING CONFERENCE: COLOR SCIENCE AND ENGINEERING SYSTEMS, TECHNOLOGIES, APPLICATIONS, P82
   Broadbent DE, 2013, PERCEPTION COMMUNICA
   Cadík M, 2008, COMPUT GRAPH FORUM, V27, P1745
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851
   Cui M, 2010, VISUAL COMPUT, V26, P1349, DOI 10.1007/s00371-009-0412-7
   Fleyeh H, 2004, CONF CYBERN INTELL S, P809, DOI 10.1109/ICCIS.2004.1460692
   Freeman M., 2009, COMPLETE GUIDE BLACK
   Gooch AA, 2005, ACM T GRAPHIC, V24, P634, DOI 10.1145/1073204.1073241
   Grundland M, 2007, PATTERN RECOGN, V40, P2891, DOI 10.1016/j.patcog.2006.11.003
   HANBURY A, 2002, PRIPTR77
   Hu QH, 2010, IEEE T SYST MAN CY B, V40, P137, DOI 10.1109/TSMCB.2009.2024166
   Kim Y, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618507
   Kuhn GR, 2008, VISUAL COMPUT, V24, P505, DOI 10.1007/s00371-008-0231-2
   Kuk JG, 2011, LECT NOTES COMPUT SC, V6495, P513, DOI 10.1007/978-3-642-19282-1_41
   LEE HC, 1991, IEEE T SIGNAL PROCES, V39, P1181, DOI 10.1109/78.80971
   Pridmore RW, 2007, COLOR RES APPL, V32, P208, DOI 10.1002/col.20312
   Prip A.H., 2003, 8 COMP VIS WINT WORK
   Qin L, 2012, 2012 IEEE FIFTH INTERNATIONAL CONFERENCE ON ADVANCED COMPUTATIONAL INTELLIGENCE (ICACI), P1, DOI [10.1109/ICACI.2012.6463111, 10.1109/ICCH.2012.6724460]
   Rasche K, 2005, COMPUT GRAPH FORUM, V24, P423, DOI 10.1111/j.1467-8659.2005.00867.x
   Setiawan NA, 2006, LECT NOTES COMPUT SC, V4282, P732
   Shapira L, 2009, COMPUT GRAPH FORUM, V28, P629, DOI 10.1111/j.1467-8659.2009.01403.x
   Shental N., 2003, ADV NEURAL INFORM PR, V16
   Smith K, 2008, COMPUT GRAPH FORUM, V27, P193, DOI 10.1111/j.1467-8659.2008.01116.x
   Tanaka G, 2009, OPT REV, V16, P601, DOI 10.1007/s10043-009-0118-0
   TREISMAN AM, 1980, COGNITIVE PSYCHOL, V12, P97, DOI 10.1016/0010-0285(80)90005-5
   WYSZECKI G, 1967, J OPT SOC AM, V57, P254, DOI 10.1364/JOSA.57.000254
   Wyszecki G., 1982, Color science: Concepts and methods, quantitative data and formulae
   X An, 2008, APPPROP ALL PAIRS AP
   Zhao Y, 2010, LECT NOTES COMPUT SC, V6454, P747, DOI 10.1007/978-3-642-17274-8_73
   Zhu SY, 2011, COLOR RES APPL, V36, P111, DOI 10.1002/col.20591
NR 35
TC 15
Z9 18
U1 0
U2 21
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2014
VL 30
IS 3
BP 299
EP 309
DI 10.1007/s00371-013-0854-9
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AB3LU
UT WOS:000331693100005
DA 2024-07-18
ER

PT J
AU Peeters, P
   Pronost, N
AF Peeters, Pieter
   Pronost, Nicolas
TI A practical framework for generating volumetric meshes of
   subject-specific soft tissue
SO VISUAL COMPUTER
LA English
DT Article
DE Virtual human; Finite element simulation; Musculoskeletal simulation;
   Subject-specific modeling; Computational geometry
ID FINITE-ELEMENT MODEL; MUSCLE
AB Studying human motion using musculoskeletal models is a common practice in the field of biomechanics. By using such models, recorded subject's motions can be analyzed in successive steps from kinematics and dynamics to muscle control. However simulating muscle deformation and interaction is not possible, but other methods such as a finite element (FE) simulation are very well suited to simulate deformation and interaction of objects. In this paper we present a practical framework for the automatic generation of FE ready meshes based on subject-specific segmented MRI data. The proposed method resolves several types of data inconsistencies: noise, an incomplete data set and self-intersections. This paper shows the different steps of the method, such as solving overlaps in the segmented surfaces, generating the volume mesh and the connection to a musculoskeletal simulation.
C1 [Peeters, Pieter] Univ Utrecht, Utrecht, Netherlands.
   [Pronost, Nicolas] Univ Utrecht, Games & Virtual Worlds Res Grp, Utrecht, Netherlands.
C3 Utrecht University; Utrecht University
RP Pronost, N (corresponding author), Univ Utrecht, Utrecht, Netherlands.
EM nicolas.pronost@uu.nl
OI PRONOST, NICOLAS/0000-0003-4499-509X
FU European Marie Curie Program under the 3D ANATOMICAL HUMAN project
   [MRTN-CT-2006-035763]
FX This work was initiated by the European Marie Curie Program under the 3D
   ANATOMICAL HUMAN project (MRTN-CT-2006-035763). The authors would like
   to acknowledge Jerome Schmid, Anders Sandholm, Nadia Magnegnat-Thalmann,
   Daniel Thalmann and Arjan Egges for their assistance in this work.
CR Au OKC, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360643
   Baraff D, 2003, ACM T GRAPHIC, V22, P862, DOI 10.1145/882262.882357
   BENTLEY JL, 1975, COMMUN ACM, V18, P509, DOI 10.1145/361002.361007
   Blemker SS, 2006, J BIOMECH, V39, P1383, DOI 10.1016/j.jbiomech.2005.04.012
   Blemker SS, 2005, ANN BIOMED ENG, V33, P661, DOI 10.1007/s10439-005-1433-7
   Damsgaard M, 2006, SIMUL MODEL PRACT TH, V14, P1100, DOI 10.1016/j.simpat.2006.09.001
   Delp SL, 2007, IEEE T BIO-MED ENG, V54, P1940, DOI 10.1109/TBME.2007.901024
   DELP SL, 1990, IEEE T BIO-MED ENG, V37, P757, DOI 10.1109/10.102791
   Dong F, 2002, IEEE T VIS COMPUT GR, V8, P154, DOI 10.1109/2945.998668
   Fedkiw SOR., 2002, APPL MATH SCI, V44, P77, DOI [10.1007/ b98879, 10.1007/b98879]
   Gielen A. W. J., 2000, Comput Methods Biomech Biomed Engin, V3, P231, DOI 10.1080/10255840008915267
   Gilles B, 2006, LECT NOTES COMPUT SC, V4190, P289
   HERT S, 2009, CGAL USER REFERENCE
   Jenkyn TR, 2002, PHYS MED BIOL, V47, P4043, DOI 10.1088/0031-9155/47/22/309
   Johansson T, 2000, J THEOR BIOL, V206, P131, DOI 10.1006/jtbi.2000.2109
   Kim J, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1966394.1966403
   Kojic M, 1998, INT J NUMER METH ENG, V43, P941, DOI 10.1002/(SICI)1097-0207(19981115)43:5<941::AID-NME435>3.0.CO;2-3
   Lee D, 2011, FOUND TRENDS COMPUT, V7, P229, DOI 10.1561/0600000036
   Maas S, 2009, COMP FEBIO ABAQUS NI
   Oudot S, 2005, Proceedings of the 14th International Meshing Roundtable, P203, DOI 10.1007/3-540-29090-7_12
   Pronost N, 2011, VISUAL COMPUT, V27, P109, DOI 10.1007/s00371-010-0534-y
   Rineau L., 2009, CGAL USER REFERENCE
   Schmid J, 2008, LECT NOTES COMPUT SC, V5241, P119, DOI 10.1007/978-3-540-85988-8_15
   Sethian JA, 1996, P NATL ACAD SCI USA, V93, P1591, DOI 10.1073/pnas.93.4.1591
   Shinar Tamar., 2008, P 2008 ACM SIGGRAPH, P95
   Stavness I, 2011, INT J NUMER METH BIO, V27, P367, DOI 10.1002/cnm.1423
   TAUBIN G, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P852, DOI 10.1109/ICCV.1995.466848
   Teran J, 2005, IEEE T VIS COMPUT GR, V11, P317, DOI 10.1109/TVCG.2005.42
   US National Library of medicine, VIS HUM PROJ
   Wang JM, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185521
   Wonhyung Jung, 2004, Computer-Aided Design and Applications, V1, P477
NR 31
TC 3
Z9 3
U1 1
U2 5
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2014
VL 30
IS 2
BP 127
EP 137
DI 10.1007/s00371-013-0788-2
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AA9DL
UT WOS:000331393900001
DA 2024-07-18
ER

PT J
AU Capanna, C
   Gesquière, G
   Jorda, L
   Lamy, P
   Vibert, D
AF Capanna, Claire
   Gesquiere, Gilles
   Jorda, Laurent
   Lamy, Philippe
   Vibert, Didier
TI Three-dimensional reconstruction using multiresolution photoclinometry
   by deformation
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Computer Graphics International (CGI) Conference
CY 2013
CL Hanover, GERMANY
DE Photoclinometry; 3D Reconstruction; Mesh deformation; Optimization;
   Multiresolution
ID SHAPE
AB We present a new photoclinometric reconstruction method based on the deformation of a 3D mesh. The optimization process of our method relies on a maximum-likelihood estimation with a density function measuring discrepancies between observed images and the corresponding synthetic images calculated from the progressively deformed 3D mesh. An input mesh is necessary and can be obtained from other methods or created by implementing a multiresolution scheme. We present a 3D shape model of an asteroid obtained by this method and compare it with the models obtained with two high-resolution 3D reconstruction techniques, stereophotogrammetry, and stereophotoclinometry.
C1 [Capanna, Claire] Aix Marseille Univ, CNRS, LSIS, UMR 7296, Marseille, France.
   [Capanna, Claire; Jorda, Laurent; Lamy, Philippe; Vibert, Didier] Aix Marseille Univ, CNRS, LAM, UMR 7326, F-13388 Marseille, France.
   [Gesquiere, Gilles] Univ Lyon 2, CNRS, Lab LIRIS Lab InfoRmat Image & Syst Informat, UMR 5205, Lyon, France.
C3 Aix-Marseille Universite; Centre National de la Recherche Scientifique
   (CNRS); Centre National de la Recherche Scientifique (CNRS); CNRS -
   National Institute for Earth Sciences & Astronomy (INSU); Aix-Marseille
   Universite; Institut National des Sciences Appliquees de Lyon - INSA
   Lyon; Universite Lyon 2; Centre National de la Recherche Scientifique
   (CNRS)
RP Capanna, C (corresponding author), Aix Marseille Univ, CNRS, LSIS, UMR 7296, Marseille, France.
EM claire.capanna@lsis.org
RI Jorda, Laurent/AAA-1718-2021; GESQUIERE, Gilles/L-2345-2018
OI GESQUIERE, Gilles/0000-0001-7088-1067; JORDA,
   Laurent/0000-0001-8735-3308; vibert, didier/0009-0008-0607-631X
CR [Anonymous], 1981, Practical Optimization
   Botsch M., 2007, GEOMETRIC MODELING B
   Briggs W., 1987, MULTIGRID METHODS FR
   BYRD RH, 1994, MATH PROGRAM, V63, P129, DOI 10.1007/BF01582063
   Gaskell RW, 2008, METEORIT PLANET SCI, V43, P1049, DOI 10.1111/j.1945-5100.2008.tb00692.x
   GIESE B, 1996, INT ARCH PHOTOGRA B3, V31, P245
   Girardeau-Montaut D., 2005, Int. Arch. Photogramm. Remote Sens. Spat. Inf. Sci, V36, pW19, DOI 10.1.1.221.8313
   Gwinner K., 2007, PHOTOGRAMM ENG REMOT, P1127
   Jorda L., 2010, P SOC PHOTO-OPT INS, V7533, P12
   Lamy P. L., 2010, ASTRON ASTROPHYS, V521, P19
   Lohse V, 2006, PLANET SPACE SCI, V54, P661, DOI 10.1016/j.pss.2006.03.002
   Loop C, 1987, THESIS U UTAH
   Morales JL, 2011, ACM T MATH SOFTWARE, V38, DOI 10.1145/2049662.2049669
   More J. J., 1978, Proceedings of the Biennial Conference on numerical analysis, P105
   PELEG S, 1990, IEEE T PATTERN ANAL, V12, P1206, DOI 10.1109/34.62611
   Prados E., 2004, 5297 I NAT RECH INF
   Preusker F, 2012, PLANET SPACE SCI, V66, P54, DOI 10.1016/j.pss.2012.01.008
   Rindfleisch T., 1966, Photometric Eng, V32, P262
   Samavati F., 2003, 200373033 U CALG
   Scharstein D, 2002, INT J COMPUT VISION, V47, P7, DOI 10.1023/A:1014573219977
   Seitz S. M., 2006, 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06), V1, P519
   Sierks H, 2011, SCIENCE, V334, P487, DOI 10.1126/science.1207325
   Szalay A. S., 2005, MSRTR2005123
   Tarini M., 2002, Proceedings of VMV 2002, P255
   Terzopoulos D, 1984, AAAI 84 P
   Wu CL, 2011, PROC CVPR IEEE, P969, DOI 10.1109/CVPR.2011.5995388
   Zhang R, 1999, IEEE T PATTERN ANAL, V21, P690, DOI 10.1109/34.784284
NR 27
TC 15
Z9 17
U1 1
U2 10
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2013
VL 29
IS 6-8
BP 825
EP 835
DI 10.1007/s00371-013-0821-5
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 151RR
UT WOS:000319478400035
DA 2024-07-18
ER

PT J
AU Qiu, NN
   Fan, R
   You, LH
   Jin, XG
AF Qiu, Nina
   Fan, Ran
   You, Lihua
   Jin, Xiaogang
TI An efficient and collision-free hole-filling algorithm for orthodontics
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Computer Graphics International (CGI) Conference
CY 2013
CL Hanover, GERMANY
DE Boundary projection; Constrained Delaunay triangulation; Mean value
   coordinates; Second-order Laplacian operator
ID REPAIR
AB Hole filling of teeth and gums is an essential stage in orthodontics after segmentation. The patching mesh should keep the morphological features of generic teeth and gums while avoiding collision between two adjacent teeth. This paper presents an efficient hole-filling algorithm to reconstruct the missing part of teeth and gums. Our proposed method involves four necessary steps: boundary construction and projection, hole triangulation in 2D, back projection of vertices to 3D, and mesh fairing. By combining constrained Delaunay triangulation in 2D with back projection of vertices to 3D using mean value coordinates, we achieve high robustness of hole triangulation and a high-quality initial patching mesh. In addition, we propose an automatic method to control the deformation degree to avoid collision. Our experiments demonstrate that the proposed method can achieve satisfactory results, not only in morphology, but also in efficiency. The results are very similar to real teeth and gums and can meet the requirements of orthodontics in medicine.
C1 [Qiu, Nina; Fan, Ran; Jin, Xiaogang] Zhejiang Univ, State Key Lab CAD&CG, Hangzhou 310058, Zhejiang, Peoples R China.
   [You, Lihua] Bournemouth Univ, Natl Ctr Comp Animat, Bournemouth, Dorset, England.
C3 Zhejiang University; Bournemouth University
RP Jin, XG (corresponding author), Zhejiang Univ, State Key Lab CAD&CG, Hangzhou 310058, Zhejiang, Peoples R China.
EM qiunina@zjucadcg.cn; fanran1029@gmail.com; LYou@bournemouth.ac.uk;
   jin@cad.zju.edu.cn
FU Science and Technology Plan of Zhejiang Province [2011C13009]; National
   Natural Science Foundation of China [61272298, 60933007]; Zhejiang
   Provincial Natural Science Foundation of China [Z1110154]
FX This work was supported by the Science and Technology Plan of Zhejiang
   Province (Grant No. 2011C13009), the National Natural Science Foundation
   of China (Grant Nos. 61272298, 60933007), and Zhejiang Provincial
   Natural Science Foundation of China (Grant No. Z1110154).
CR Attene M, 2010, VISUAL COMPUT, V26, P1393, DOI 10.1007/s00371-010-0416-3
   Botsch M, 2004, ACM T GRAPHIC, V23, P630, DOI 10.1145/1015706.1015772
   Davis J, 2002, FIRST INTERNATIONAL SYMPOSIUM ON 3D DATA PROCESSING VISUALIZATION AND TRANSMISSION, P428, DOI 10.1109/TDPVT.2002.1024098
   Floater MS, 2005, MATH VIS, P157, DOI 10.1007/3-540-26808-1_9
   Floater MS, 1997, COMPUT AIDED GEOM D, V14, P231, DOI 10.1016/S0167-8396(96)00031-3
   Floater MS, 2003, COMPUT AIDED GEOM D, V20, P19, DOI 10.1016/S0167-8396(03)00002-5
   GEORGE PL, 1994, INT J NUMER METH ENG, V37, P3605, DOI 10.1002/nme.1620372103
   Ju T, 2004, ACM T GRAPHIC, V23, P888, DOI 10.1145/1015706.1015815
   Jun Y, 2005, COMPUT AIDED DESIGN, V37, P263, DOI 10.1016/j.cad.2004.06.012
   Liepa P., 2003, Symposium on Geometry Processing, P200
   Mingqiang Wei, 2010, 2010 Proceedings of International Conference on Artificial Intelligence and Computational Intelligence (AICI 2010), P306, DOI 10.1109/AICI.2010.302
   Nooruddin FS, 2003, IEEE T VIS COMPUT GR, V9, P191, DOI 10.1109/TVCG.2003.1196006
   Shewchuk J. R., 1996, Applied Computational Geometry. Towards Geometric Engineering. FCRC'96 Workshop, WACG'96. Selected Papers, P203, DOI 10.1007/BFb0014497
   Shlens J., 2005, ARXIV
   Sorkine O, 2004, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, P191, DOI 10.1109/SMI.2004.1314506
   Wang LC, 2012, COMPUT AIDED DESIGN, V44, P1182, DOI 10.1016/j.cad.2012.07.007
   Wu X.J., 2008, Computer-Aided Design and Applications, V5, P889, DOI DOI 10.3722/CADAPS.2008.889-899
   Yuan TR, 2010, INT J BIOMED IMAGING, V2010, DOI 10.1155/2010/535329
   Zhao W, 2007, VISUAL COMPUT, V23, P987, DOI 10.1007/s00371-007-0167-y
NR 19
TC 10
Z9 14
U1 2
U2 10
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2013
VL 29
IS 6-8
BP 577
EP 586
DI 10.1007/s00371-013-0820-6
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 151RR
UT WOS:000319478400012
DA 2024-07-18
ER

PT J
AU Carvalho, SR
   Boulic, R
   Vidal, CA
   Thalmann, D
AF Carvalho, Schubert R.
   Boulic, Ronan
   Vidal, Creto A.
   Thalmann, Daniel
TI Latent motion spaces for full-body motion editing
SO VISUAL COMPUTER
LA English
DT Article
DE Linear motion models; Constraint-based optimization; Latent
   interpolation; Motion editing
AB We explore an approach to full-body motion editing with linear motion models, prioritized constraint-based optimization and latent-space interpolation. By exploiting the mathematical connections between linear motion models and prioritized inverse kinematics (PIK), we formulate and solve the motion editing problem as an optimization function whose differential structure is rich enough to efficiently optimize user-specified constraints within the latent motion space. Performing motion editing within latent motion spaces has the advantage of handling pose transitions and consequently motion flow by construction from single key-frame editing. To handle motion adjustments from multiple key-frame and trajectory constraints, we developed a latent-space interpolation technique by exploiting spline functions. Such an approach handles per-frame adjustments generating smooth animations, while avoiding the computational expense of joint space interpolations. We demonstrate the usefulness of this approach by editing and generating full-body reaching and walking jump animations in challenging environment scenarios.
C1 [Carvalho, Schubert R.; Boulic, Ronan; Vidal, Creto A.; Thalmann, Daniel] CNS, Lab Computat Sensorimotor Neurosci, Kingston, ON K7L 3N6, Canada.
RP Carvalho, SR (corresponding author), CNS, Lab Computat Sensorimotor Neurosci, Botterell Hall,18,Stuart St, Kingston, ON K7L 3N6, Canada.
EM schubertcarvalho@gmail.com; ronan.boulic@epfl.ch; cvidal@lia.ufc.br;
   daniel.thalmann@epfl.ch
RI Thalmann, Daniel/A-4347-2008; Thalmann, Daniel/AAL-1097-2020; Vidal,
   Creto/AAK-7042-2020
OI Thalmann, Daniel/0000-0002-0451-7491; 
FU SNF [200020-109989]; EPFL-Sport and Rehabilitation Engineering program;
   CAPES/Brazil [4557/06-9]
FX The authors would like to thank Mireille Clavien for the video
   production; Autodesk/Maya for their donation of Maya software; Benoit Le
   Callennec for providing access to his motion editing system (with the
   support of the SNF grant no 200020-109989); and the valuable suggestions
   of all the anonymous reviewers. This work was supported by the
   EPFL-Sport and Rehabilitation Engineering program. The third author
   would like to acknowledge CAPES/Brazil for the grant 4557/06-9 that
   helped support him in VRlab-EPFL Switzerland during the academic year
   2007-2008.
CR [Anonymous], 2010, SCA'10: proceedings of the 2010 ACM SIGGRAPH/Eurographics symposium on computer animation, DOI [10.2312/SCA/SCA10/001-010, DOI 10.2312/SCA/SCA10/001-010]
   Arikan O, 2002, ACM T GRAPHIC, V21, P483, DOI 10.1145/566570.566606
   BAERLOCHER P, 2004, VIS COMPUT, V20
   Carvalho S., 2007, COMPUT ANIMAT VIRTUA, V18
   Carvalho S.R., 2011, P COMP GRAPH INT OTT
   Carvalho S.R., 2009, THESIS ECOLE POLYTEC, DOI [10.5075/epfl-thesis-4558, DOI 10.5075/EPFL-THESIS-4558]
   Carvalho SR, 2009, LECT NOTES COMPUT SC, V5884, P116, DOI 10.1007/978-3-642-10347-6_11
   Carvalho SR, 2007, WSCG 2007, FULL PAPERS PROCEEDINGS I AND II, P97
   Chai JX, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239459
   Choi KJ, 2000, J VISUAL COMP ANIMAT, V11, P223, DOI 10.1002/1099-1778(200012)11:5<223::AID-VIS236>3.0.CO;2-5
   Glardon P, 2006, VISUAL COMPUT, V22, P194, DOI 10.1007/s00371-006-0376-9
   Gleicher M, 2001, GRAPH MODELS, V63, P107, DOI 10.1006/gmod.2001.0549
   Grassia F. S., 1998, J. Graph. Tools, V6, DOI [10.1080/10867651.1998.10487493, DOI 10.1080/10867651.1998.10487493]
   Grochow K, 2004, ACM T GRAPHIC, V23, P522, DOI 10.1145/1015706.1015755
   Hanafusa H., 1981, IFAC, 8th Triennal World Congress, V4, P1927
   Ikemoto L, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1477926.1477927
   Jain AK, 1999, ACM COMPUT SURV, V31, P264, DOI 10.1145/331499.331504
   Jolliffe I.T., 1986, Principal component analysis, DOI DOI 10.1016/0169-7439(87)80084-9
   Kochanek D. H. U., 1984, Computers & Graphics, V18, P33
   Kovar L, 2002, ACM T GRAPHIC, V21, P473, DOI 10.1145/566570.566605
   Kulpa R, 2005, COMPUT GRAPH FORUM, V24, P343, DOI 10.1111/j.1467-8659.2005.00859.x
   Le Callennec B, 2006, GRAPH MODELS, V68, P175, DOI 10.1016/j.gmod.2005.03.001
   Lee J, 1999, COMP GRAPH, P39
   MACIEJEWSKI AA, 1990, IEEE COMPUT GRAPH, V10, P63, DOI 10.1109/38.55154
   Monzani JS, 2000, COMPUT GRAPH FORUM, V19, pC11, DOI 10.1111/1467-8659.00393
   Mukai T, 2005, ACM T GRAPHIC, V24, P1062, DOI 10.1145/1073204.1073313
   Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1
   Raunhardt D, 2009, VISUAL COMPUT, V25, P509, DOI 10.1007/s00371-009-0336-2
   Safonova A, 2004, ACM T GRAPHIC, V23, P514, DOI 10.1145/1015706.1015754
   Shin HJ, 2006, COMPUT ANIMAT VIRT W, V17, P219, DOI 10.1002/cav.125
   Shoemaker K., 1985, Computer Graphics, V19, P245, DOI 10.1145/325165.325242
   URTASUN R, 2004, EUR C COMP VIS PRAG
   van Basten BJH, 2011, COMPUT GRAPH FORUM, V30, P1963, DOI 10.1111/j.1467-8659.2011.02051.x
   WHITNEY DE, 1969, IEEE T MAN MACHINE, VMM10, P47, DOI 10.1109/TMMS.1969.299896
NR 34
TC 2
Z9 3
U1 1
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2013
VL 29
IS 3
BP 171
EP 188
DI 10.1007/s00371-012-0678-z
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 115AI
UT WOS:000316783800002
DA 2024-07-18
ER

PT J
AU Yang, M
   Chen, WJ
AF Yang, Meng
   Chen, Wenjuan
TI Fluid simulation with adaptively sharpening and embedded boundary
   conditions
SO VISUAL COMPUTER
LA English
DT Article
DE Numerical dissipation; Hybrid scheme; Particle; Boundary condition;
   Flux; Two-way coupling
ID ANIMATION; SMOKE; WATER
AB In this paper, we present a physically based technique for simulating inviscid fluids. Our contribution is concerned with two issues. First, for solving the advection equation, we introduce a hybrid scheme that couples the FLIP scheme with the semi-Lagrangian scheme by adaptively distributing implicit particles and using a transition layer to propagate information. Secondly, for solving pressure, we develop a flux based scheme that can embed arbitrary solid boundaries into a Poisson equation. And based on this scheme we make further improvement to achieve two-way fluid/solid coupling on an octree structure with second-order accuracy. Finally, the experimental results demonstrate that our hybrid scheme for advection can preserve relatively fine surface details with less computation expenditure; and simultaneously our robust pressure solver can handle both stationary and moving obstacles more efficiently compared with unstructured meshes.
C1 [Yang, Meng; Chen, Wenjuan] Chinese Acad Sci, Inst Comp Technol, Key Lab Intelligent Informat Proc, Beijing, Peoples R China.
   [Chen, Wenjuan] Capital Normal Univ, Join Fac Comp Sci Res, Beijing, Peoples R China.
   [Yang, Meng] Chinese Acad Sci, Grad Univ, Beijing, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Computing Technology, CAS;
   Capital Normal University; Chinese Academy of Sciences; University of
   Chinese Academy of Sciences, CAS
RP Yang, M (corresponding author), Chinese Acad Sci, Inst Comp Technol, Key Lab Intelligent Informat Proc, Beijing, Peoples R China.
EM maxpane2@163.com; rebecca1984_2001@163.com
RI Chen, Wenjuan/G-4326-2012
FU National Natural Science Foundation of China [60273019, 60373042,
   60496326, 60573063, 60573064]
FX Research supported in part by the National Natural Science Foundation of
   China under Grant Nos. 60273019, 60373042, 60496326, 60573063, and
   60573064.
CR [Anonymous], 2003, P 4 ASME JSME JOINT
   [Anonymous], SCI CHINA F
   [Anonymous], THESIS U BRIT COLUMB
   Batty C., 2008, S COMP AN, P219
   Batty C, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1276377.1276502
   BRACKBILL JU, 1986, J COMPUT PHYS, V65, P314, DOI 10.1016/0021-9991(86)90211-1
   Carlson M, 2004, ACM T GRAPHIC, V23, P377, DOI 10.1145/1015706.1015733
   Chentanez N., 2006, ACM SIG- GRAPH/Eurographics Symposium on Computer Animation, P83
   Enright D, 2002, ACM T GRAPHIC, V21, P736, DOI [10.1145/566570.566581, 10.1145/566570.566645]
   Fedkiw R, 2001, COMP GRAPH, P15, DOI 10.1145/383259.383260
   Fedkiw R., 2003, LEVEL SET METHODS DY
   Feldman BE, 2005, ACM T GRAPHIC, V24, P904, DOI 10.1145/1073204.1073281
   Foster N, 2001, COMP GRAPH, P23, DOI 10.1145/383259.383261
   Guendelman E, 2005, ACM T GRAPHIC, V24, P973, DOI 10.1145/1073204.1073299
   Harlow F.H., 1963, EXPT ARITHMETIC HIGH
   Houston B., 2003, P SIGGRAPH SKETCH AP
   KIM B, 2005, EUR WORKSH NAT PHEN
   Kim J., 2006, Proc ACM SIGGRAPH/Eurograph Symp Comp Anim, SCA '06, P335
   Kim T., 2008, SIGGRAPH 2008 AUG
   Klingner BM, 2006, ACM T GRAPHIC, V25, P820, DOI 10.1145/1141911.1141961
   Li W, 2003, VISUAL COMPUT, V19, P444, DOI 10.1007/s00371-003-0210-6
   Losasso F, 2004, ACM T GRAPHIC, V23, P457, DOI 10.1145/1015706.1015745
   Losasso F, 2006, COMPUT FLUIDS, V35, P995, DOI 10.1016/j.compfluid.2005.01.006
   Losasso F, 2006, ACM T GRAPHIC, V25, P812, DOI 10.1145/1141911.1141960
   Min CH, 2006, J COMPUT PHYS, V218, P123, DOI 10.1016/j.jcp.2006.01.046
   Molemaker J., 2008, S COMP AN 2008
   Müller M, 2004, COMPUT ANIMAT VIRT W, V15, P159, DOI 10.1002/cav.18
   Ng YT, 2009, J COMPUT PHYS, V228, P8807, DOI 10.1016/j.jcp.2009.08.032
   Nguyen DQ, 2002, ACM T GRAPHIC, V21, P721, DOI 10.1145/566570.566643
   Patrick M., 2009, SIGGRAPH 2009 AUG
   Peskin CS, 2002, ACT NUMERIC, V11, P479, DOI 10.1017/S0962492902000077
   Rasmussen N., 2004, P 2004 ACM SIGGRAPH, P195
   Robinson-Mosher A, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360645
   SALLEE JF, 1984, SIAM J ALGEBRA DISCR, V5, P407, DOI 10.1137/0605039
   Selle A, 2005, ACM T GRAPHIC, V24, P910, DOI 10.1145/1073204.1073282
   Song OY, 2005, ACM T GRAPHIC, V24, P81, DOI 10.1145/1037957.1037962
   Stam J, 1999, COMP GRAPH, P121, DOI 10.1145/311535.311548
   Wojtan C., 2010, SIGGRAPH 2010 AUG
   Zhu YN, 2005, ACM T GRAPHIC, V24, P965, DOI 10.1145/1073204.1073298
NR 39
TC 1
Z9 1
U1 1
U2 13
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2012
VL 28
IS 5
BP 425
EP 434
DI 10.1007/s00371-011-0624-5
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 926EJ
UT WOS:000302813800001
DA 2024-07-18
ER

PT J
AU Lewiner, T
   Marques, C
   Paixao, J
   de Botton, S
   Cabral, A
   Nascimento, R
   Mello, V
   Peixoto, A
   Martinez, D
   Vieira, T
AF Lewiner, Thomas
   Marques, Clarissa
   Paixao, Joao
   de Botton, Scarlett
   Cabral, Allyson
   Nascimento, Renata
   Mello, Vinicius
   Peixoto, Adelailson
   Martinez, Dimas
   Vieira, Thales
TI Stereo music visualization through manifold harmonics
SO VISUAL COMPUTER
LA English
DT Article
DE Manifold harmonics; Symmetry; Sound visualization; Stereophony; Geometry
   processing; GPU; Design galleries
AB Music visualizations are nowadays included with virtually any media player. They usually rely on harmonic analysis of each sound channel, which automatically generate parameters for procedural image generation. However, only few music visualizations make use of 3d shapes. This paper proposes to use spectral mesh processing techniques, here manifold harmonics, to produce 3d stereo music visualization. The images are generated from 3d models by deforming an initial shape, mapping the sound frequencies to the mesh harmonics. A symmetry criterion is introduced to enhance the stereo effects on the deformed shape. A concise representation of the frequency mapping is proposed to allow for an animated gallery interface with genetic reproduction. Such galleries let the user quickly navigate between visual effects. Rendering such animated galleries in real time is a challenging task, since it requires computing and rendering the deformed shapes at a very high rate. This paper introduces a direct GPU implementation of manifold harmonics filters, which allows the displaying of the animated galleries.
C1 [Lewiner, Thomas; Marques, Clarissa; Paixao, Joao; de Botton, Scarlett; Cabral, Allyson; Nascimento, Renata] Pontificia Univ Catolica Rio de Janeiro, Dept Math, Rio de Janeiro, Brazil.
   [Peixoto, Adelailson; Martinez, Dimas; Vieira, Thales] Univ Fed Alagoas, Inst Math, Maceio, Brazil.
   [Mello, Vinicius] Univ Fed Bahia, Inst Math, Salvador, BA, Brazil.
C3 Pontificia Universidade Catolica do Rio de Janeiro; Universidade Federal
   de Alagoas; Universidade Federal da Bahia
RP Lewiner, T (corresponding author), Pontificia Univ Catolica Rio de Janeiro, Dept Math, Rio de Janeiro, Brazil.
EM lewiner@gmail.com
RI de Almeida Vieira, Thales Miranda/C-7689-2017; Martínez,
   Dimas/AAN-8733-2021; Lewiner, Thomas/B-7751-2008
OI de Almeida Vieira, Thales Miranda/0000-0001-7775-5258; Martínez,
   Dimas/0000-0001-5192-8376; Lewiner, Thomas/0000-0001-9518-6423
CR *APPL, 2006, PBORENDERTOVERTEXARR
   Bordignon AL, 2009, PHYSICA A, V388, P2099, DOI 10.1016/j.physa.2009.02.021
   Breebaart J., 2007, SPATIAL AUDIO PROCES
   Clough R.W., 1975, Dynamics of Structures
   Comstock Henry B., 1953, Popular Science, P104
   Pinto FD, 2006, SIBGRAPI, P281
   Gardner W. G., 1998, 3D AUDIO USING LOUDS
   Hernandez V, 2005, ACM T MATH SOFTWARE, V31, P351, DOI 10.1145/1089014.1089019
   HIEBERT G, 2005, OPENAL PROGRAMMERS G
   Jenny Hans., 2001, CYMATICS STUDY WAVE
   Kessenich John., 2010, The opengl shading language: A specification. version 3.30
   KUBELKA O, 2000, CENTR EUR SEM COMP G
   Lage M, 2005, SIBGRAPI 2005: XVIII BRAZILIAN SYMPOSIUM ON COMPUTER GRAPHICS AND IMAGE PROCESSING, CONFERENCE PROCEEDINGS, P349
   LEVY B, 2009, SIGGRAPH ASIA COURSE, P1
   LEWINER T, 2010, SIBGRAPI, P110
   Lewiner T, 2011, COMPUT GRAPH-UK, V35, P586, DOI 10.1016/j.cag.2011.03.005
   Liu Y, 2008, MM&SEC'08: PROCEEDINGS OF THE MULTIMEDIA & SECURITY WORKSHOP 2008, P43
   MARKS J, 1997, SIGGRAPH, P400
   OBrien J.F., 2002, Proceedings of the 2002 ACM SIGGRAPH/Eurographics symposium on Computer animation, P175
   Ovsjanikov M, 2008, COMPUT GRAPH FORUM, V27, P1341, DOI 10.1111/j.1467-8659.2008.01273.x
   Patin F., 2003, BEAT DETECTION ALGOR
   PENTLAND A, 1989, ACM COMPUTER GRAPHIC, V23, P207
   Rong GD, 2008, VISUAL COMPUT, V24, P787, DOI 10.1007/s00371-008-0260-x
   TAUBIN G, 1995, SIGGRAPH 95 C P, P351, DOI [DOI 10.1145/218380.218473, 10.1145/218380.218473]
   Vallet B, 2008, COMPUT GRAPH FORUM, V27, P251, DOI 10.1111/j.1467-8659.2008.01122.x
   Vieira T, 2009, COMPUT GRAPH FORUM, V28, P717, DOI 10.1111/j.1467-8659.2009.01412.x
   Wang K, 2009, IEEE IMAGE PROC, P3657, DOI 10.1109/ICIP.2009.5414248
   WU HY, 2009, VRCAI, P311
   YINGHUI C, 2006, GRAPHITE, P173
NR 29
TC 1
Z9 1
U1 0
U2 14
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2011
VL 27
IS 10
SI SI
BP 905
EP 916
DI 10.1007/s00371-011-0617-4
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 831MU
UT WOS:000295736200004
DA 2024-07-18
ER

PT J
AU Iizuka, S
   Endo, Y
   Mitani, J
   Kanamori, Y
   Fukui, Y
AF Iizuka, Satoshi
   Endo, Yuki
   Mitani, Jun
   Kanamori, Yoshihiro
   Fukui, Yukio
TI An interactive design system for pop-up cards with a physical simulation
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Conference on the Computer Graphics International (CGI)
CY JUN 12-15, 2011
CL Ottawa, CANADA
DE Pop-up card; Interactive design system; Mass-spring model
AB We present an interactive system that allows users to design original pop-up cards. A pop-up card is an interesting form of papercraft consisting of folded paper that forms a three-dimensional structure when opened. However, it is very difficult for the average person to design pop-up cards from scratch because it is necessary to understand the mechanism and determine the positions of objects so that pop-up parts do not collide with each other or protrude from the card. In the proposed system, the user interactively sets and edits primitives that are predefined in the system. The system simulates folding and opening of the pop-up card using a mass-spring model that can simply simulate the physical movement of the card. This simulation detects collisions and protrusions and illustrates the movement of the pop-up card. The results of the present study reveal that the user can design a wide range of pop-up cards using the proposed system.
C1 [Iizuka, Satoshi; Endo, Yuki; Mitani, Jun; Kanamori, Yoshihiro; Fukui, Yukio] Univ Tsukuba, Dept Comp Sci, Tsukuba, Ibaraki 3058573, Japan.
   [Mitani, Jun] JST ERATO, Bunkyo Ku, Tokyo 1120002, Japan.
C3 University of Tsukuba
RP Iizuka, S (corresponding author), Univ Tsukuba, Dept Comp Sci, 1-1-1 Tenno Dai, Tsukuba, Ibaraki 3058573, Japan.
EM iizuka@npal.cs.tsukuba.ac.jp; endo@npal.cs.tsukuba.ac.jp;
   mitani@cs.tsukuba.ac.jp; kanamori@cs.tsukuba.ac.jp;
   fukui@cs.tsukuba.ac.jp
FU Grants-in-Aid for Scientific Research [20700075] Funding Source: KAKEN
CR FURUTA Y, 2007, IPSJ J, V48, P3658
   Glassner A, 2002, IEEE COMPUT GRAPH, V22, P74, DOI 10.1109/38.988749
   Glassner A, 2002, IEEE COMPUT GRAPH, V22, P79, DOI 10.1109/38.974521
   Hendrix S. L., 2006, Advanced Technology for Learning, V3, P119, DOI 10.2316/Journal.208.2006.2.208-0878
   Lee YT, 1996, COMPUT GRAPH, V20, P21, DOI 10.1016/0097-8493(95)00089-5
   Li XY, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778848
   Mitani J, 2004, COMPUTER GRAPHICS INTERNATIONAL, PROCEEDINGS, P93
   Mitani J., 2003, J GRAPHIC SCI JPN, V37, P3
   OKAMURA S, 2009, P SMART GRAPH 2009, P68
   ELEMENTS POP POP BOO
NR 10
TC 15
Z9 17
U1 1
U2 8
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2011
VL 27
IS 6-8
BP 605
EP 612
DI 10.1007/s00371-011-0564-0
PG 8
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 766FY
UT WOS:000290767600019
DA 2024-07-18
ER

PT J
AU Zhao, HL
   Wang, CCL
   Chen, Y
   Jin, XG
AF Zhao, Hanli
   Wang, Charlie C. L.
   Chen, Yong
   Jin, Xiaogang
TI Parallel and efficient Boolean on polygonal solids
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Conference on the Computer Graphics International (CGI)
CY JUN 12-15, 2011
CL Ottawa, CANADA
DE Boolean operations; Layered depth images; Depth peeling; Out-of-core;
   CUDA
ID SYSTEM
AB We present a novel framework which can efficiently evaluate approximate Boolean set operations for B-rep models by highly parallel algorithms. This is achieved by taking axis-aligned surfels of Layered Depth Images (LDI) as a bridge and performing Boolean operations on the structured points. As compared with prior surfel-based approaches, this paper has much improvement. Firstly, we adopt key-data pairs to store LDI more compactly. Secondly, robust depth peeling is investigated to overcome the bottleneck of layer-complexity. Thirdly, an out-of-core tiling technique is presented to overcome the limitation of memory. Real-time feedback is provided by streaming the proposed pipeline on the many-core graphics hardware.
C1 [Wang, Charlie C. L.] Chinese Univ Hong Kong, Dept Mech & Automat Engn, Hong Kong, Hong Kong, Peoples R China.
   [Zhao, Hanli] Wenzhou Univ, Coll Phys & Elect Informat Engn, Wenzhou 325035, Peoples R China.
   [Chen, Yong] Univ So Calif, Epstein Dept Ind & Syst Engn, Los Angeles, CA USA.
   [Jin, Xiaogang] Zhejiang Univ, State Key Lab CAD & CG, Hangzhou 310058, Zhejiang, Peoples R China.
C3 Chinese University of Hong Kong; Wenzhou University; University of
   Southern California; Zhejiang University
RP Wang, CCL (corresponding author), Chinese Univ Hong Kong, Dept Mech & Automat Engn, Hong Kong, Hong Kong, Peoples R China.
EM hanlizhao@gmail.com; cwang@mae.cuhk.edu.hk; yongchen@usc.edu;
   jin@cad.zju.edu.cn
RI Chen, Yong/A-7342-2009; Wang, Charlie C. L./B-3730-2010
OI Chen, Yong/0000-0002-8377-5914; Wang, Charlie C. L./0000-0003-4406-8480
FU Div Of Civil, Mechanical, & Manufact Inn; Directorate For Engineering
   [0927397] Funding Source: National Science Foundation
CR Adams B, 2003, ACM T GRAPHIC, V22, P651, DOI 10.1145/882262.882320
   [Anonymous], RHIN
   [Anonymous], ENCY ENG ELECT ENG
   [Anonymous], P SIGGRAPH 86
   [Anonymous], IEEE T VISUALIZATION
   [Anonymous], P SIGGRAPH 87
   [Anonymous], CUD PROGR GUID CUD T
   [Anonymous], P SIGGRAPH 05
   [Anonymous], SIGGRAPH 97
   [Anonymous], COMPUT GRAPH FORUM
   [Anonymous], 2001, TECHNICAL REPORT
   [Anonymous], P SIGGRAPH 03
   [Anonymous], 3D AC MOD
   [Anonymous], TECHNICAL REPORT
   [Anonymous], IEEE T VISUALIZATION
   [Anonymous], ACM SIGGRAPH 07 TECH
   [Anonymous], P SIGGRAPH 01
   [Anonymous], P ASME INT DES ENG T
   de Berg M., 2000, COMPUTATIONAL GEOMET
   GOLDFEATHER J, 1989, IEEE COMPUT GRAPH, V9, P20, DOI 10.1109/38.28107
   Hable J, 2007, IEEE T VIS COMPUT GR, V13, P1004, DOI 10.1109/TVCG.2007.70411
   Harris M., 2007, GPU GEMS, V3
   Hoffmann C.M., 2001, J COMPUT INF SCI ENG, V1, P143
   Ju T, 2002, ACM T GRAPHIC, V21, P339
   Krishnan S, 2001, INT J COMPUT GEOM AP, V11, P105, DOI 10.1142/S0218195901000419
   Liu F., 2010, Proceedings of the 2010 ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games, P75, DOI [DOI 10.1145/1730804.1730817, DOI 10.1145/1730804]
   Müller C, 2009, IEEE T VIS COMPUT GR, V15, P605, DOI 10.1109/TVCG.2008.188
   Pfister H, 2000, COMP GRAPH, P335, DOI 10.1145/344779.344936
   Shade J., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P231, DOI 10.1145/280814.280882
   Toledo Sivan., 2003, Taucs: A library of sparse linear solvers
   Trapp M., 2008, EUROGRAPHICS 2008 SH, P235
   Wang CCL, 2010, COMPUT AIDED DESIGN, V42, P535, DOI 10.1016/j.cad.2010.02.001
NR 32
TC 21
Z9 30
U1 0
U2 15
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2011
VL 27
IS 6-8
BP 507
EP 517
DI 10.1007/s00371-011-0571-1
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 766FY
UT WOS:000290767600010
DA 2024-07-18
ER

PT J
AU Liu, SG
   Liu, QG
   Peng, QS
AF Liu, Shiguang
   Liu, Qiguang
   Peng, Qunsheng
TI Realistic simulation of mixing fluids
SO VISUAL COMPUTER
LA English
DT Article
DE LLSPH; Mixing fluids; Physically based modeling; Miscible; Immiscible
ID ANIMATION
AB Recently, simulation of mixing fluids, for which wide applications can be found in multimedia, computer games, special effects, virtual reality, etc., is attracting more and more attention. Most previous methods focus separately on binary immiscible mixing fluids or binary miscible mixing fluids. Until now, little attention has been paid to realistic simulation of multiple mixing fluids. In this paper, based on the solution principles in physics, we present a unified framework for realistic simulation of liquid-liquid mixing with different solubility, which is called LLSPH. In our method, the mixing process of miscible fluids is modeled by a heat-conduction-based Smooth Particle Hydrodynamics method. A special self-diffusion coefficient is designed to simulate the interactions between miscible fluids. For immiscible fluids, marching-cube-based method is adopted to trace the interfaces between different types of fluids efficiently. Then, an optimized spatial hashing method is adopted for simulation of boundary-free mixing fluids such as the marine oil spill. Finally, various realistic scenes of mixing fluids are rendered using our method.
C1 [Liu, Shiguang; Liu, Qiguang] Tianjin Univ, Sch Comp Sci & Technol, Tianjin 300072, Peoples R China.
   [Liu, Shiguang] Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing 100191, Peoples R China.
   [Peng, Qunsheng] Zhejiang Univ, State Key Lab CAD & CG, Hangzhou 310058, Peoples R China.
C3 Tianjin University; Beihang University; Zhejiang University
RP Liu, SG (corresponding author), Tianjin Univ, Sch Comp Sci & Technol, Tianjin 300072, Peoples R China.
EM lsg@tju.edu.cn
FU Natural Science Foundation of China [60803047]; Specialized Research
   Fund for the Doctoral Program of Higher Education of China
   [200800561045]; State Key Laboratory of Virtual Reality Technology and
   System, Beihang University [BUAA-VR-10KF-1]; State Key Lab of CAD&CG,
   Zhejiang University [A0907]
FX This work was supported by Natural Science Foundation of China under
   Grant No. 60803047, the Specialized Research Fund for the Doctoral
   Program of Higher Education of China under Grant No. 200800561045, the
   Open Project Program of the State Key Laboratory of Virtual Reality
   Technology and System, Beihang University under Grant No.
   BUAA-VR-10KF-1, the Open Project Program of the State Key Lab of CAD&CG,
   Zhejiang University under Grant No. A0907. The authors would also like
   to thank the reviewers for their insightful comments which greatly
   helped improving the manuscript.
CR Bao K, 2010, COMPUT ANIMAT VIRT W, V21, P401, DOI 10.1002/cav.356
   Becker M, 2009, IEEE T VIS COMPUT GR, V15, P493, DOI 10.1109/TVCG.2008.107
   Becker M, 2007, SYMPOSIUM ON COMPUTER ANIMATION 2007: ACM SIGGRAPH/ EUROGRAPHICS SYMPOSIUM PROCEEDINGS, P209
   CHRISTOF RS, 2008, ACM SIGGRAPH COURSE
   Cleary PW, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1239451.1239548, 10.1145/1276377.1276499]
   ENRIGHT D, 2001, P SIGGRAPH, P736
   Fedkiw R, 2001, COMP GRAPH, P15, DOI 10.1145/383259.383260
   Foster N, 2001, COMP GRAPH, P23, DOI 10.1145/383259.383261
   Hong JM, 2003, COMPUT GRAPH FORUM, V22, P253, DOI 10.1111/1467-8659.00672
   HONG JM, 2008, P ACM SIGGRAPH, P481
   HONG JM, 2005, P ACM SIGGRAPH 2005, P915
   Kang N, 2010, COMPUT GRAPH FORUM, V29, P685, DOI 10.1111/j.1467-8659.2009.01638.x
   Keiser R., 2005, Point-Based Graphics 2005 (IEEE Cat. No. 05EX1159), P125, DOI 10.1109/PBG.2005.194073
   Kelager M., 2006, Lagrangian Fluid Dynamics using Smoothed Particle Hydrodynamics
   KIM B, 2007, P ACM SIGGRAPH, P481
   Lenaerts T, 2009, COMPUT GRAPH FORUM, V28, P213, DOI 10.1111/j.1467-8659.2009.01360.x
   Losasso F, 2008, IEEE T VIS COMPUT GR, V14, P797, DOI 10.1109/TVCG.2008.37
   Losasso F, 2006, ACM T GRAPHIC, V25, P812, DOI 10.1145/1141911.1141960
   McNaught A. D., 1997, IUPAC Compendium of Chemical Terminology: Gold Book
   Mihalef Viorel., 2006, S COMPUTER ANIMATION, P317
   Mizuno R, 2003, 11TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P440, DOI 10.1109/PCCGA.2003.1238291
   Monaghan JJ, 2005, REP PROG PHYS, V68, P1703, DOI 10.1088/0034-4885/68/8/R01
   Muller M, 2005, P 2005 ACM SIGGRAPH, P237, DOI DOI 10.1145/1073368.1073402
   Muller M., 2003, Proceedings of the 2003 ACM SIGGRAPH/Eurographics symposium on Computer animation, P154
   Nguyen DQ, 2002, ACM T GRAPHIC, V21, P721, DOI 10.1145/566570.566643
   Park J, 2008, COMPUT ANIMAT VIRT W, V19, P455, DOI 10.1002/cav.256
   Premoze S, 2003, COMPUT GRAPH FORUM, V22, P401, DOI 10.1111/1467-8659.00687
   Solenthaler B, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531346
   Solenthaler B., 2008, P 2008 ACM SIGGRAPH, P211, DOI 10.2312/SCA/SCA08/211-218
   Stam J, 1999, COMP GRAPH, P121, DOI 10.1145/311535.311548
   Stam Jos., 1995, Proceedings of the 22nd annual conference on Computer graphics and interactive techniques, SIGGRAPH '95, P129
   Takeshita D, 2003, 11TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P482, DOI 10.1109/PCCGA.2003.1238299
   Teschner M, 2003, VISION, MODELING, AND VISUALIZATION 2003, P47
   Thürey N, 2007, SYMPOSIUM ON COMPUTER ANIMATION 2007: ACM SIGGRAPH/ EUROGRAPHICS SYMPOSIUM PROCEEDINGS, P191
   WILLIAM EL, 1987, COMPUTER GRAPHICS, V21, P163
   ZHU HB, 2006, P COMPUTER ANIMATION, P403
NR 36
TC 22
Z9 26
U1 1
U2 20
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2011
VL 27
IS 3
BP 241
EP 248
DI 10.1007/s00371-010-0531-1
PG 8
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 722QS
UT WOS:000287450000006
DA 2024-07-18
ER

PT J
AU Giorgi, D
   Frosini, P
   Spagnuolo, M
   Falcidieno, B
AF Giorgi, D.
   Frosini, P.
   Spagnuolo, M.
   Falcidieno, B.
TI 3D relevance feedback via multilevel relevance judgements
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 2nd Eurographics Workshop on 3D Object Retrieval
CY MAR 29, 2009
CL Munich, GERMANY
DE 3D retrieval; 3D similarity; User feedback; Relevance scale;
   Pseudodistances; Approximation
ID RETRIEVAL; SEARCH
AB Relevance feedback techniques are expected to play an important role in 3D search engines, as they help to bridge the semantic gap between the user and the system. Indeed, similarity is a cognitive process that depends on the observer. We propose a novel relevance feedback technique, which relies on the assumption that similarity may emerge from the inhibition of differences, i.e., from the lack of diversity with respect to the shape properties taken into account. To this end, a user is provided with a variety of shape descriptors, each analyzing different shape properties. Then the user expresses his/her multilevel relevance judgements, which correspond to his/her concept of similarity among the retrieved objects. Finally, the system inhibits the role of the shape properties that do not reflect the user's idea of similarity. The feedback technique is based on a simple scaling procedure, which does not require neither a priori learning nor parameter optimization. We show examples and experiments on a benchmark dataset of 3D models.
C1 [Giorgi, D.; Spagnuolo, M.; Falcidieno, B.] IMATI CNR, Genoa, Italy.
   [Frosini, P.] Univ Bologna, ARCES, Fac Engn, Bologna, Italy.
   [Frosini, P.] Univ Bologna, Dept Math, Bologna, Italy.
C3 Consiglio Nazionale delle Ricerche (CNR); Istituto di Matematica
   Applicata e Tecnologie Informatiche "Enrico Magenes" (IMATI-CNR);
   University of Bologna; University of Bologna
RP Giorgi, D (corresponding author), IMATI CNR, Genoa, Italy.
EM daniela@ge.imati.cnr.it
RI FROSINI, PATRIZIO/AAF-3482-2020; Spagnuolo, Michela/F-5068-2013;
   Spagnuolo, Michela/ABA-1927-2021; GIORGI, DANIELA/B-3998-2017
OI Spagnuolo, Michela/0000-0002-5682-6990; Spagnuolo,
   Michela/0000-0002-5682-6990; GIORGI, DANIELA/0000-0002-6752-6918;
   Frosini, Patrizio/0000-0001-6711-1926
CR Akgül CB, 2006, LECT NOTES COMPUT SC, V4105, P322
   Akgül CB, 2010, INT J COMPUT VISION, V89, P392, DOI 10.1007/s11263-009-0294-1
   Albertoni R, 2008, LECT NOTES COMPUT SC, V4900, P1
   [Anonymous], 1971, SMART RETRIEVAL SYST
   [Anonymous], 1983, INTRO MODERN INFORM
   [Anonymous], UUCS2007015
   ATMOSUKARTO I, 2005, MULTIMEDIA, V11, P334
   Attene M, 2009, COMPUT AIDED DESIGN, V41, P756, DOI 10.1016/j.cad.2009.01.003
   Bang H., 2002, PROCESSING, P22
   Biasotti S, 2008, PATTERN RECOGN, V41, P2855, DOI 10.1016/j.patcog.2008.02.003
   BIASOTTI S, 2007, EUROGRAPHICS 2007 TU, P1025
   BIASOTTI SDE, 1987, ACM COMPUT SURV, V12, P1
   Bronstein AM, 2008, MONOGR COMPUT SCI, P1, DOI 10.1007/978-0-387-73301-2_1
   Bronstein MM, 2006, NUMER LINEAR ALGEBR, V13, P149, DOI 10.1002/nla.475
   Bustos B, 2005, ACM COMPUT SURV, V37, P345, DOI 10.1145/1118890.1118893
   CHAOUCH M, 2007, P ICME 07 IEEE
   Chen DY, 2003, COMPUT GRAPH FORUM, V22, P223, DOI 10.1111/1467-8659.00669
   Datta AK, 2008, OPT LASER TECHNOL, V40, P1, DOI 10.1016/j.optlastec.2007.04.006
   Del Bimbo A, 2006, ACM T MULTIM COMPUT, V2, P20
   Elad A, 2003, IEEE T PATTERN ANAL, V25, P1285, DOI 10.1109/TPAMI.2003.1233902
   Elad M., 2001, PROC EG MULTIMEDIA, P97, DOI DOI 10.2312/EGMM/EGMM01/107-118
   Escolano F, 2009, INFORMATION THEORY IN COMPUTER VISION AND PATTERN RECOGNITION, P1, DOI 10.1007/978-1-84882-297-9
   FALCIDIENO B, 2007, ROLE ONTOLOGIES 3D M, P185
   FALCIDIENO B, 2004, P EWIMT2004 EUR WORK, P1
   Frosini P., 1999, Pattern Recognition and Image Analysis, V9, P596
   Funkhouser T, 2003, ACM T GRAPHIC, V22, P83, DOI 10.1145/588272.588279
   GANDAR B, 2009, P C EUR ENBIS
   Giorgi D, 2009, AVSS: 2009 6TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE, P79, DOI 10.1109/AVSS.2009.82
   GIORGI D, 2007, IMATICNRGE0907
   GIORGI D, 2009, P 2 EUR C 3D OBJ RET, P45
   Kazhdan M., 2003, P EUR ACM SIGGRAPH S, V6, P156
   KHERFI M, 2006, J VIS COMMUN IMAGE R, V14, P428
   Koenderink J. J., 1990, Solid shape
   Leifman G, 2005, VISUAL COMPUT, V21, P865, DOI 10.1007/s00371-005-0341-z
   Leng B, 2008, MULTIMED TOOLS APPL, V40, P135, DOI 10.1007/s11042-007-0188-6
   Martínez JM, 2002, IEEE MULTIMEDIA, V9, P78, DOI 10.1109/93.998074
   NOVOTNI M, 2005, P 4 INT WORKSH CONT
   Onasoglou E, 2008, MULTIMED TOOLS APPL, V39, P217, DOI 10.1007/s11042-008-0216-1
   Papadakis P., 2008, Computer-Aided Design and Applications Journal, V5, P753
   Rosman G, 2008, COMPUT IMAGING VIS, V36, P243
   Rui Y, 1998, IEEE T CIRC SYST VID, V8, P644, DOI 10.1109/76.718510
   Smeulders AWM, 2000, IEEE T PATTERN ANAL, V22, P1349, DOI 10.1109/34.895972
   Tangelder JH, 2008, MULTIMED TOOLS APPL, V39, P441, DOI 10.1007/s11042-007-0181-0
   TUNG T, 2005, IJSM, V11
   WU H, 2002, P IEEE INT C IM PROC, P22
   WU H, 2003, LNCS, V2728
   Zarpalas D, 2007, EURASIP J ADV SIG PR, DOI 10.1155/2007/23912
   Zhou XS, 2003, MULTIMEDIA SYST, V8, P536, DOI 10.1007/s00530-002-0070-3
NR 48
TC 13
Z9 15
U1 0
U2 7
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2010
VL 26
IS 10
BP 1321
EP 1338
DI 10.1007/s00371-010-0524-0
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 649SI
UT WOS:000281793200007
DA 2024-07-18
ER

PT J
AU Chen, H
   Wang, L
   Liu, WX
   Heng, PA
AF Chen, Hui
   Wang, Lan
   Liu, Wenxi
   Heng, Pheng-Ann
TI Combined X-ray and facial videos for phoneme-level articulator dynamics
SO VISUAL COMPUTER
LA English
DT Article
DE Internal articulator dynamics; Computer-assisted pronunciation learning;
   Talking head
ID MOTION CAPTURE DATA; SPEECH SYNTHESIS; VISIBLE SPEECH; HEARING-LOSS;
   ANIMATION; MODELS
AB Dynamic external and internal articulator motions are integrated into a low-cost data-driven three-dimensional talking head in this paper. External and internal articulations are defined and calibrated from the video streams and the videofluoroscopy to a generic 3D talking head model. Three different deformation modes in relation to pronunciation characteristics of muscular soft tissue of lips and tongue, up-down movements of chin and the relatively fixed articulators are set up and integrated. The shape blending functions among segmented phonemes of natural speech input are synthesized in an utterance. Animations of the confusable phonemes and minimal pairs are shown to English teachers and learners for a perception test. The results show that the proposed method can reflect the real situation of phonetic pronunciation realistically.
C1 [Chen, Hui; Wang, Lan; Liu, Wenxi; Heng, Pheng-Ann] Chinese Univ Hong Kong, SIAT, Chinese Acad Sci, Shenzhen Inst Adv Integrat Technol, Shenzhen 518055, Peoples R China.
   [Heng, Pheng-Ann] Chinese Univ Hong Kong, Dept Comp Sci & Engn, Shatin, Hong Kong, Peoples R China.
C3 Chinese Academy of Sciences; Shenzhen Institute of Advanced Technology,
   CAS; The Chinese University of Hong Kong, Shenzhen; Chinese University
   of Hong Kong
RP Chen, H (corresponding author), Chinese Univ Hong Kong, SIAT, Chinese Acad Sci, Shenzhen Inst Adv Integrat Technol, Shenzhen 518055, Peoples R China.
EM hui.chen@siat.ac.cn
RI Lin, Fan/JZT-1441-2024
OI Lin, Fan/0000-0002-7330-3833; Heng, Pheng Ann/0000-0003-3055-5034
FU National Nature Science Foundation of China [60703120, 60772165]
FX This work is being supported by the National Nature Science Foundation
   of China (60703120, 60772165).
CR Badin P, 2008, LECT NOTES COMPUT SC, V5098, P132, DOI 10.1007/978-3-540-70517-8_14
   Bregler C., 1997, P 24 ANN C COMP GRAP, V31, P353, DOI DOI 10.1145/258734.258880
   Deng Z, 2008, COMPUT GRAPH FORUM, V27, P2096, DOI 10.1111/j.1467-8659.2008.01192.x
   Ezzat T, 2000, INT J COMPUT VISION, V38, P45, DOI 10.1023/A:1008166717597
   Fagel S, 2004, SPEECH COMMUN, V44, P141, DOI 10.1016/j.specom.2004.10.006
   FAGEL S, 2005, ZAS PAPERS LINGUISTI, V40, P19
   FAGEL S, 2008, P INT BRISB, P2643
   Grauwinkel K., 2007, P INT, P706
   Jin XG, 2000, COMPUT GRAPH-UK, V24, P219, DOI 10.1016/S0097-8493(99)00156-9
   Kalberer GA, 2002, J VISUAL COMP ANIMAT, V13, P97, DOI 10.1002/vis.283
   Liu XC, 2008, COMPUT ANIMAT VIRT W, V19, P235, DOI 10.1002/cav.248
   Ma JY, 2004, COMPUT ANIMAT VIRT W, V15, P485, DOI 10.1002/cav.11
   Massaro DW, 2004, J SPEECH LANG HEAR R, V47, P304, DOI 10.1044/1092-4388(2004/025)
   Park SY, 2005, MACH VISION APPL, V16, P148, DOI 10.1007/s00138-004-0165-2
   Rathinavelu A, 2007, LECT NOTES COMPUT SC, V4554, P786
   Tarabalka Y., 2007, P ASSISTH 2007 FRANC, P187
   TYEMURRAY N, 1993, NCVS STATUS PROG REP, V4, P41
   Wik P., 2008, P FONETIK 2008, P57
NR 18
TC 7
Z9 7
U1 0
U2 8
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2010
VL 26
IS 6-8
BP 477
EP 486
DI 10.1007/s00371-010-0434-1
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 602JQ
UT WOS:000278135800009
DA 2024-07-18
ER

PT J
AU Liu, X
   Rokne, JG
   Gavrilova, ML
AF Liu, Xin
   Rokne, Jon G.
   Gavrilova, Marina L.
TI A novel terrain rendering algorithm based on quasi Delaunay
   triangulation
SO VISUAL COMPUTER
LA English
DT Article
DE Terrain rendering; Quasi Delaunay triangulation; Smooth morphing
ID DYNAMIC ADAPTIVE MESHES; BDAM
AB Terrain rendering has long been an interesting research topic, and Delaunay triangulation has been one of the methods frequently employed. The problem of smooth morphing between successive Delaunay meshes in a dynamic setting does not have a satisfactory solution however. In this paper, we address this issue by temporarily relieving the mesh from the strict constraints of Delaunay triangulation (DT). The proposed algorithm uses an off-line process to compute a relative importance for each sampling point in a Digital Elevation Model (DEM). It then constructs a mesh model in real-time from a set of points selected according to their viewpoint distances and relative importances. The mesh model is initialized to be a genuine DT. As the viewpoint moves, some points are added, and some are removed. We use simple methods for point insertion and removal that allow smooth morphing between successive frames. While the simple methods do not ensure Delaunay properties, we eliminate the slivery triangles gradually by collecting and flipping illegal edges incident to them. Point insertions, removals, edge flips, and their animations are organized by queueing and carried out over time. In this way, we amortize the burst computations to successive frames, so that a balanced workload and a high frame rate are achieved. The proposed algorithm produces a concise and well-composed mesh adaptive to both viewpoint and the terrain's local geometry, and, most importantly, it supports smooth morphing.
C1 [Liu, Xin; Rokne, Jon G.; Gavrilova, Marina L.] Univ Calgary, Dept Comp Sci, Calgary, AB T2N 1N4, Canada.
C3 University of Calgary
RP Liu, X (corresponding author), Univ Calgary, Dept Comp Sci, 2500 Univ Dr NW, Calgary, AB T2N 1N4, Canada.
EM liuxin@ucalgary.ca; rokne@ucalgary.ca; marina@ucalgary.ca
RI Gavrilova, Marina/AHD-3605-2022
OI Gavrilova, Marina/0000-0002-5338-1834
CR Apu RA, 2004, LECT NOTES COMPUT SC, V3044, P592
   BERG MD, 1998, GRAPH MODEL IM PROC, V60, P1
   Bhattacharjee S, 2008, SIXTH INDIAN CONFERENCE ON COMPUTER VISION, GRAPHICS & IMAGE PROCESSING ICVGIP 2008, P551, DOI 10.1109/ICVGIP.2008.85
   Cignoni P, 2003, COMPUT GRAPH FORUM, V22, P505, DOI 10.1111/1467-8659.00698
   Cignoni P., 2003, P 14 IEEE VIS 2003 V, P20
   CohenOr D, 1996, IEEE VISUAL, P37, DOI 10.1109/VISUAL.1996.567600
   De Berg M., 2008, Computational Geometry: Algorithms and Applications, V17
   DEB S, 2006, ACM SIGGRAPH 2006 SK, P181
   Debard JB, 2007, VISUAL COMPUT, V23, P975, DOI 10.1007/s00371-007-0172-1
   DECASTRP M, 2009, COMPUT GRAPH FORUM, V28
   DEVILLERS O, 2008, 00325816 INRIA
   Duchaineau M, 1997, VISUALIZATION '97 - PROCEEDINGS, P81, DOI 10.1109/VISUAL.1997.663860
   DYER R, 2007, P 5 EUR S GEOM PROC, P282
   FOWLER R, 1979, SIGGRAPH, P207
   GARLAND M, 1981, CMUCS95181
   Gobbetti E, 2006, COMPUT GRAPH FORUM, V25, P333, DOI 10.1111/j.1467-8659.2006.00952.x
   Hoppe H, 1998, VISUALIZATION '98, PROCEEDINGS, P35, DOI 10.1109/VISUAL.1998.745282
   LOSASSO F, 2004, IEEE VISUALIZATION, P769
   Rabinovich B, 1997, VISUALIZATION '97 - PROCEEDINGS, P95, DOI 10.1109/VISUAL.1997.663863
   Rippa S., 1990, Computer-Aided Geometric Design, V7, P489, DOI 10.1016/0167-8396(90)90011-F
   Valette S, 2009, COMPUT GRAPH FORUM, V28, P1301, DOI 10.1111/j.1467-8659.2009.01507.x
   Yang SW, 2009, COMPUT AIDED DESIGN, V41, P375, DOI 10.1016/j.cad.2008.08.005
   ZHANG H, 2007, 15 INT S ADV GEOGR I, P1
   ZHANG W, 2009, INT C ENV SCI INF AP, P716
NR 24
TC 5
Z9 6
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2010
VL 26
IS 6-8
BP 697
EP 706
DI 10.1007/s00371-010-0440-3
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 602JQ
UT WOS:000278135800030
DA 2024-07-18
ER

PT J
AU Puig-Centelles, A
   Ripolles, O
   Chover, M
AF Puig-Centelles, Anna
   Ripolles, Oscar
   Chover, Miguel
TI Creation and control of rain in virtual environments
SO VISUAL COMPUTER
LA English
DT Article
DE Rain; Real-time rendering; Particle system; Level-of-detail; GPU
AB Realistic outdoor scenarios often include rain and other atmospheric phenomena, which are difficult to simulate in real time. In the field of real-time applications, a number of solutions have been proposed which offer realistic but costly rain systems. Our proposal consists in developing a solution to facilitate the creation and control of rain scenes and to improve on previously used methods while offering a realistic appearance of rain. Firstly, we create and define the areas in which it is raining. Secondly, we perform a suitable management of the particle systems inside them. We include multiresolution techniques in order to adapt the number of particles, their location and their size according to the view conditions. Furthermore, in this work the physical properties of rain are analyzed and its features are incorporated into the final approach that we propose. The presented method is completely integrated in the GPU. We offer a solution which is fast, simple, efficient and easily integrated into existing virtual-reality environments.
C1 [Puig-Centelles, Anna; Ripolles, Oscar; Chover, Miguel] Univ Jaume 1, Castellon de La Plana, Spain.
C3 Universitat Jaume I
RP Puig-Centelles, A (corresponding author), Univ Jaume 1, Castellon de La Plana, Spain.
EM apuig@uji.es; oripolle@uji.es; chover@uji.es
RI Chover Sellés, Miguel/P-9933-2018
OI Chover Sellés, Miguel/0000-0002-0525-7038; Ripolles,
   Oscar/0000-0002-5450-6758
FU Spanish Ministry of Science and Technology [TSI-2004-02940,
   TIN2007-68066-C04-02]; Bancaja [P1 1B2007-56]
FX This work was supported by the Spanish Ministry of Science and
   Technology (Grants TSI-2004-02940 and TIN2007-68066-C04-02) and by
   Bancaja (P1 1B2007-56). We would like to thank Pierre Rousseau and Sarah
   Tariq for their support.
CR [Anonymous], 2003, LEVEL DETAIL 3D GRAP
   [Anonymous], 2006, NEW YORK TIMES
   BEST A, 1950, SIZE DISTRIBUTION RA
   DORETTO YNW, 2001, INT C COMP VIS, P439
   Feng ZX, 2006, LECT NOTES COMPUT SC, V3865, P626
   Garg K, 2004, PROC CVPR IEEE, P528
   Garg K, 2006, ACM T GRAPHIC, V25, P996, DOI 10.1145/1141911.1141985
   GUNDERSEN OE, 2007, THEORY PRACTICE COMP, P213
   HALLENBECK C, 1917, MON WEATHER REV, P209
   Iwasaki K, 2008, VISUAL COMPUT, V24, P77, DOI 10.1007/s00371-007-0186-8
   Kaneda K, 1999, J VISUAL COMP ANIMAT, V10, P15, DOI 10.1002/(SICI)1099-1778(199901/03)10:1<15::AID-VIS192>3.0.CO;2-P
   KELKAR VN, 1945, P INDIAN ACAD SCI A, V22, P394
   KUSAMOTO K, 2001, IPSJ SIG NOT, P21
   MARSHALL JS, 1948, DISTRIBUTION RAINDRO
   MCCOMBER P, 2000, P 57 ANN E SNOW C
   O'Brien D, 2001, COMP ANIM CONF PROC, P210, DOI 10.1109/CA.2001.982395
   PUPPO E, 1997, EUROGRAPHICS 97 TUTO
   RASMUSSEN RM, 1995, REV GEOPHYS S, V33
   ROSS ON, 2000, THESIS FREIEN U BERL
   Rousseau P, 2006, COMPUT GRAPH-UK, V30, P507, DOI 10.1016/j.cag.2006.03.013
   STANIK S, 2003, INT J COMPUT VIS TEX, P95
   Straube J.F., 2000, Proceedings of International Building Physics Conference. Eindhoven, P375
   Tariq Sarah., 2007, RAIN
   Wang HM, 2005, ACM T GRAPHIC, V24, P921, DOI 10.1145/1073204.1073284
   Wang L, 2006, SIGGRAPH 06 ACM SIGG, P156
   Wang N, 2004, SIGGRAPH 04 ACM SIGG, P14
NR 26
TC 6
Z9 7
U1 0
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2009
VL 25
IS 11
BP 1037
EP 1052
DI 10.1007/s00371-009-0366-9
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 508EQ
UT WOS:000270911700006
DA 2024-07-18
ER

PT J
AU Zhao, HL
   Mao, XY
   Jin, XG
   Shen, JB
   Wei, FF
   Feng, JQ
AF Zhao, Hanli
   Mao, Xiaoyang
   Jin, Xiaogang
   Shen, Jianbing
   Wei, Feifei
   Feng, Jieqing
TI Real-time saliency-aware video abstraction
SO VISUAL COMPUTER
LA English
DT Article
DE Non-photorealistic rendering; Image abstraction; Saliency map; Real-time
   video processing
AB Existing real-time automatic video abstraction systems rely on local contrast only for identifying perceptually important information and abstract imagery by reducing contrast in low-contrast regions while artificially increasing contrast in higher contrast regions. These methods, however, may fail to accentuate an object against its background for the images with objects of low contrast over background of high contrast. To solve this problem, we propose a progressive abstraction method based on a region-of-interest function derived from an elaborate perception model. Visual contents in perceptually salient regions are emphasized, whereas the background is abstracted appropriately. In addition, the edge-preserving smoothing and line drawing algorithms in this paper are guided by a vector field which describes the flow of salient features of the input image. The whole pipeline can be executed automatically in real time on the GPU, without requiring any user intervention. Several experimental examples are shown to demonstrate the effectiveness of our approach.
C1 [Zhao, Hanli; Jin, Xiaogang; Wei, Feifei; Feng, Jieqing] Zhejiang Univ, State Key Lab CAD & CG, Hangzhou 310027, Zhejiang, Peoples R China.
   [Mao, Xiaoyang] Univ Yamanashi, Kofu, Yamanashi, Japan.
   [Shen, Jianbing] Beijing Inst Technol, Sch Comp Sci & Technol, Beijing 100081, Peoples R China.
C3 Zhejiang University; University of Yamanashi; Beijing Institute of
   Technology
RP Jin, XG (corresponding author), Zhejiang Univ, State Key Lab CAD & CG, Hangzhou 310027, Zhejiang, Peoples R China.
EM hanlizhao@gmail.com; mao@yamanashi.ac.jp; jin@cad.zju.edu.cn;
   shenjianbing@bit.edu.cn; weifeifei@cad.zju.edu.cn; jqfeng@cad.zju.edu.cn
RI Shen, Jianbing/U-8796-2019
OI Shen, Jianbing/0000-0002-4109-8353; mao, xiaoyang/0000-0001-9531-3197
FU National Key Basic Research Foundation of China [2009CB320801]; National
   Natural Science Foundation of China [60533080, 60833007]; Key Technology
   R D Program [2007BAH11B03]
FX The authors would like to thank our anonymous reviewers for their
   dedicated help in improving the paper. Many thanks also to Xiaoyan Luo,
   Mingdong Zhou, and Shufang Lu for their help in presenting the
   manuscript. This work was supported by the National Key Basic Research
   Foundation of China (Grant No. 2009CB320801), the National Natural
   Science Foundation of China (Grant Nos. 60533080 and 60833007), and the
   Key Technology R& D Program (Grant No. 2007BAH11B03).
CR Bolz J, 2003, ACM T GRAPHIC, V22, P917, DOI 10.1145/882262.882364
   Bousseau Adrien, 2007, ACM T GRAPH SIGGRAPH, V26, P3
   CARBAL B, 1993, P SIGGRAPH, P263
   Chen J., 2007, ACM T GRAPHIC, V26, P3
   Colbert M, 2007, J VIS COMMUN IMAGE R, V18, P387, DOI 10.1016/j.jvcir.2007.03.002
   Collomosse JP, 2005, IEEE T VIS COMPUT GR, V11, P540, DOI 10.1109/TVCG.2005.85
   Collomosse JP, 2003, IEEE T VIS COMPUT GR, V9, P443, DOI 10.1109/TVCG.2003.1260739
   DeCarlo D, 2002, ACM T GRAPHIC, V21, P769, DOI 10.1145/566570.566650
   Farbman Z, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360666
   GREENSPAN H, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P222, DOI 10.1109/CVPR.1994.323833
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   KANG H, 2007, P ACM S NONPH AN REN, P43
   Kang H, 2009, IEEE T VIS COMPUT GR, V15, P62, DOI 10.1109/TVCG.2008.81
   KRAUS M, 2007, P INT C COMP GRAPH T, P21
   Kyprianidis J. E., 2008, P EG UK THEOR PRACT, P51
   LEE CH, 2005, ACM T GRAPH SIGGRAPH, V24, P3
   Lee S, 2007, VRST 2007: ACM SYMPOSIUM ON VIRTUAL REALITY SOFTWARE AND TECHNOLOGY, PROCEEDINGS, P29
   McCloud S., 1994, Understanding Comics: The Invisible Art
   ORZAN A, 2007, P ACM S NONPH AN REN, P103
   Pham TQ, 2005, 2005 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO (ICME), VOLS 1 AND 2, P454, DOI 10.1109/ICME.2005.1521458
   Rempel AG, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239490
   Santella A., 2004, P NPAR, P71, DOI [DOI 10.1145/987657.987669, 10.1145/987657.987669]
   Scheuermann T, 2007, I3D 2007: ACM SIGGRAPH SYMPOSIUM ON INTERACTIVE 3D GRAPHICS AND GAMES, PROCEEDINGS, P33
   Setlur V, 2007, IEEE COMPUT GRAPH, V27, P80, DOI 10.1109/MCG.2007.133
   Tomasi C, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P839, DOI 10.1109/ICCV.1998.710815
   WANG J, 2004, ACM T GRAPH SIGGRAPH, V23, P3
   WINNEMOELLER H., 2006, ACM T GRAPHIC, V25, P3
   Wyszecki G., 1982, Color science: Concepts and methods, quantitative data and formulae
   ZHAO H, 2008, VIS COMPUT CGI 08, V24, P7
   ZIEGLER G, 2006, P 11 FALL WORKSH VIS, P133
NR 30
TC 16
Z9 20
U1 0
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2009
VL 25
IS 11
BP 973
EP 984
DI 10.1007/s00371-008-0308-y
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 508EQ
UT WOS:000270911700001
DA 2024-07-18
ER

PT J
AU Mudure, M
   Popescu, V
AF Mudure, Mihai
   Popescu, Voicu
TI 1001 Acquisition viewpoints: efficient and versatile view-dependent
   modeling of real-world scenes
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 26th International Conference on Computer Graphics
CY JUN 09-11, 2008
CL Istanbul, TURKEY
DE model acquisition; realism; three-dimensional graphics
AB Modeling is a severe bottleneck for computer graphics applications. Manual modeling is time consuming and fails to capture the complexity of real-world scenes. Automated modeling based on acquiring color and depth data is a promising alternative. However, the usual approach of densely sampling the scene from a few viewpoints suffers from long acquisition times, high data redundancy, and lack of robustness, leading to incomplete models. We propose automated modeling based on sampling the scene sparsely from a dense set of viewpoints. We show that the sparse data quickly accumulates to generate models with good scene coverage. The sparse depth is acquired efficiently and robustly, which enables an interactive, operator-in-the-loop acquisition pipeline. We describe a modeling system that implements this approach. The system acquires scenes with complex geometry and complex reflective properties from thousands of viewpoints in minutes. The resulting models are compact and support photorealistic rendering at interactive rates.
C1 [Mudure, Mihai; Popescu, Voicu] Purdue Univ, Dept Comp Sci, W Lafayette, IN 47907 USA.
C3 Purdue University System; Purdue University
RP Mudure, M (corresponding author), Purdue Univ, Dept Comp Sci, 305 N Univ St, W Lafayette, IN 47907 USA.
EM mmudure@cs.purdue.edu; popescu@cs.purdue.edu
CR [Anonymous], ELECT BASEL
   BAHMUTOV G, 2006, P EUROGRAPHICS
   BAHMUTOV G, 2005, P VID VIS GRAPH VVG, P11
   Borghese NA, 1998, IEEE COMPUT GRAPH, V18, P38, DOI 10.1109/38.674970
   Bouguet JY, 1999, INT J COMPUT VISION, V35, P129, DOI 10.1023/A:1008124523456
   CHEN ES, 1995, P SIGGRAPH 95, P29
   Davis J, 2003, PROC CVPR IEEE, P359
   Debevec P. E., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P11, DOI 10.1145/237170.237191
   Fisher R., 1996, P VIRTUAL REALITY WO, P13
   Gortler S. J., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P43, DOI 10.1145/237170.237200
   Guidi G, 2005, FIFTH INTERNATIONAL CONFERENCE ON 3-D DIGITAL IMAGING AND MODELING, PROCEEDINGS, P565, DOI 10.1109/3DIM.2005.2
   Hidalgo E, 2002, COMPUT GRAPH FORUM, V21, P471, DOI 10.1111/1467-8659.00607
   Hilton A, 2000, MACH VISION APPL, V12, P44, DOI 10.1007/s001380050123
   HOPPE H, 1992, COMP GRAPH, V26, P71, DOI 10.1145/142920.134011
   Koninckx TP, 2003, FOURTH INTERNATIONAL CONFERENCE ON 3-D DIGITAL IMAGING AND MODELING, PROCEEDINGS, P293, DOI 10.1109/IM.2003.1240262
   Levoy M., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P31, DOI 10.1145/237170.237199
   Levoy M, 2000, COMP GRAPH, P131, DOI 10.1145/344779.344849
   LIM J, COMPUT VIS, V2, P1635
   Matusik W, 2002, ACM T GRAPHIC, V21, P427, DOI 10.1145/566570.566599
   MCALLISTER DK, 1999, P EUR WORKSH REND
   *MICR LIV LABS, PHOT
   Pfister H, 2000, COMP GRAPH, P335, DOI 10.1145/344779.344936
   Pollefeys M, 2002, COMMUN ACM, V45, P50, DOI 10.1145/514236.514263
   Popescu V, 2003, FOURTH INTERNATIONAL CONFERENCE ON 3-D DIGITAL IMAGING AND MODELING, PROCEEDINGS, P285, DOI 10.1109/IM.2003.1240261
   Quan L, 2006, ACM T GRAPHIC, V25, P599, DOI 10.1145/1141911.1141929
   Rusinkiewicz S, 2000, COMP GRAPH, P343, DOI 10.1145/344779.344940
   SOLEM HE, 2004, P C COMP VIS PATT RE, P653
   Stumpfel J., 2003, 4 INT S VIRT REAL AR
   Terauchi T, 2005, FIFTH INTERNATIONAL CONFERENCE ON 3-D DIGITAL IMAGING AND MODELING, PROCEEDINGS, P196, DOI 10.1109/3DIM.2005.8
   Turk G., 1994, Computer Graphics Proceedings. Annual Conference Series 1994. SIGGRAPH 94 Conference Proceedings, P311, DOI 10.1145/192161.192241
   Zitnick CL, 2004, ACM T GRAPHIC, V23, P600, DOI 10.1145/1015706.1015766
NR 31
TC 0
Z9 0
U1 0
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2008
VL 24
IS 7-9
BP 669
EP 678
DI 10.1007/s00371-008-0247-7
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 322OI
UT WOS:000257384800022
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Srikanth, MB
   Mathias, PC
   Natarajan, V
   Naidu, P
   Poston, T
AF Srikanth, Manohar B.
   Mathias, P. C.
   Natarajan, Vijay
   Naidu, Prakash
   Poston, Timothy
TI Visibility volumes for interactive path optimization
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 26th International Conference on Computer Graphics
CY JUN 09-11, 2008
CL Istanbul, TURKEY
DE visibility analysis; flight paths; force-directed ropes; haptics;
   multimodal interaction
ID TERRAIN
AB We describe a real-time system that supports design of optimal flight paths over terrains. These paths either maximize view coverage or minimize vehicle exposure to ground. A volume-rendered display of multi-viewpoint visibility and a haptic interface assists the user in selecting, assessing, and refining the computed flight path. We design a three-dimensional scalar field representing the visibility of a point above the terrain, describe an efficient algorithm to compute the visibility field, and develop visual and haptic schemes to interact with the visibility field. Given the origin and destination, the desired flight path is computed using an efficient simulation of an articulated rope under the influence of the visibility gradient. The simulation framework also accepts user input, via the haptic interface, thereby allowing manual refinement of the flight path.
C1 [Natarajan, Vijay] Indian Inst Sci, Dept Comp Sci & Automat, Supercomp Educ & Res Ctr, Bangalore 560012, Karnataka, India.
   [Srikanth, Manohar B.] MIT, Dept Mech Engn, Cambridge, MD USA.
   [Mathias, P. C.] Indian Inst Sci, NMR Res Ctr, Supercomp Educ & Res Ctr, Bangalore 560012, Karnataka, India.
   [Naidu, Prakash] Univ Toronto, Toronto, ON, Canada.
   [Poston, Timothy] Indian Inst Sci, Natl Inst Adv Studies, Bangalore 560012, Karnataka, India.
C3 Indian Institute of Science (IISC) - Bangalore; Massachusetts Institute
   of Technology (MIT); Indian Institute of Science (IISC) - Bangalore;
   University of Toronto; Indian Institute of Science (IISC) - Bangalore
RP Natarajan, V (corresponding author), Indian Inst Sci, Dept Comp Sci & Automat, Supercomp Educ & Res Ctr, Bangalore 560012, Karnataka, India.
EM srimano@mit.edu; pcm@sif.iisc.ernet.in; vijayn@csa.iisc.ernet.in;
   crjp.naidu@utoronto.ca
CR Avila RS, 1996, IEEE VISUAL, P197, DOI 10.1109/VISUAL.1996.568108
   BENMOSHE B, 2004, P 6 WORKSH ALG ENG E, P120
   Bittner J, 2003, ENVIRON PLANN B, V30, P729, DOI 10.1068/b2957
   Bryson S, 1996, COMMUN ACM, V39, P62, DOI 10.1145/229459.229467
   Cohen-Or D, 2003, IEEE T VIS COMPUT GR, V9, P412, DOI 10.1109/TVCG.2003.1207447
   COLE R, 1989, J SYMB COMPUT, V7, P11, DOI 10.1016/S0747-7171(89)80003-3
   Cruz-Neira C., 1993, Proceedings IEEE 1993 Symposium on Research Frontiers in Virtual Reality (Cat. No.93TH0585-0), P59, DOI 10.1109/VRAIS.1993.378262
   DONALD BR, 2000, INT C ROB AUT, P3435
   Durlach N., 1995, Virtual Reality: Scientific and Technological Challenges, DOI 10.17226/4761
   FLORIANI LD, 1999, GEOGRAPHIC INFORM SY, P543
   FLORIANI LD, 1996, P 1993 ACM SIGAPP S
   FRANKLIN WR, 1994, P 6 INT S SPAT DAT H, V2, P751
   Lawrence DA, 2004, IEEE COMPUT GRAPH, V24, P22, DOI 10.1109/MCG.2004.60
   Marzouqi MS, 2006, ROBOTICA, V24, P759, DOI 10.1017/S0263574706002931
   MILLS K, 1992, COMPUT GEOSCI, V18, P1047, DOI 10.1016/0098-3004(92)90020-R
   NAGY G, 1994, COMPUT GRAPH, V18, P763, DOI 10.1016/0097-8493(94)90002-7
   SRIKANTH MB, 2007, J VIRTUAL REAL UNPUB
   Srinivasan MA, 1997, COMPUT GRAPH-UK, V21, P393, DOI 10.1016/S0097-8493(97)00030-7
   Stewart AJ, 1998, IEEE T VIS COMPUT GR, V4, P82, DOI 10.1109/2945.675656
   Teng YA, 1997, INT J COMPUT GEOM AP, V7, P75, DOI 10.1142/S0218195997000065
   TENG YA, 1993, IEEE T SYST MAN CYB, V23, P96, DOI 10.1109/21.214770
   TENG YA, 1992, CARTR625 U MAR
   van Dam A, 2000, IEEE COMPUT GRAPH, V20, P26, DOI 10.1109/38.888006
   VANREIMERSDAHL T, 2003, EGVE 03, P241
   Vasudevan H, 2007, ADV ROBOTICS, V21, P1323, DOI 10.1163/156855307781503745
   VERLET L, 1967, PHYS REV, V159, P98, DOI 10.1103/PhysRev.159.98
   Yano H, 2003, 11TH SYMPOSIUM ON HAPTIC INTERFACES FOR VIRTUAL ENVIRONMENT AND TELEOPERATOR SYSTEMS - HAPTICS 2003, PROCEEDINGS, P32, DOI 10.1109/HAPTIC.2003.1191223
NR 27
TC 4
Z9 5
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2008
VL 24
IS 7-9
BP 635
EP 647
DI 10.1007/s00371-008-0244-x
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 322OI
UT WOS:000257384800019
DA 2024-07-18
ER

PT J
AU Min, K
   Metaxas, D
AF Min, Kyungha
   Metaxas, Dimitris
TI A combustion-based technique for fire animation and visualization
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 25th Computer Graphics International Conference (CGI)
CY MAY 30-JUN 02, 2007
CL Petropolis, BRAZIL
DE fluid simulation; fire natural phenomena; photon mapping
ID FLAMES
AB In this paper, we present a new fire animation and visualization scheme. The most difficult problem in creating fire animation is how to simulate the mechanism of emitting the light and heat of fire. We attack the difficulty by presenting a simulation scheme for the combustion process in voxelized space where the numerical solution of the classical fluid equations is implemented. Therefore, the combustion process is simulated at each voxel and the amount of heat generated at the voxel is estimated. The generated heat will increase the temperature at the voxel, which results in the increase of the turbulent motion of fire. We also propose a visualization scheme that is based on a photon mapping algorithm in order to render fire and various lighting effects of fire to the environments.
C1 Rutgers State Univ, CBIM, New Brunswick, NJ USA.
C3 Rutgers University System; Rutgers University New Brunswick
EM minkh@smu.ac.kr; dnm@cs.rutgers.edu
CR Adabala N, 2000, J VISUAL COMP ANIMAT, V11, P279, DOI 10.1002/1099-1778(200012)11:5<279::AID-VIS234>3.0.CO;2-P
   ADABALA N, 2004, COMPUTER ANIMATION S
   BEAUDOIN P, 2001, GRPAHICS INTERFACE, P159
   BLANC C, 1995, IMPLICIT SURFACES 95, P21
   BUKOWSKI R, 1997, SIGGRAPH 97 C P, P35
   CHIBA N, 1994, J VISUAL COMP ANIMAT, V5, P37, DOI 10.1002/vis.4340050104
   Fedkiw R, 2001, COMP GRAPH, P15, DOI 10.1145/383259.383260
   Feldman BE, 2003, ACM T GRAPHIC, V22, P708, DOI 10.1145/882262.882336
   IHM I, 2004, EUROGRAPHICS ACM SIG, P203
   INAKAGE M, 1989, SIMPLE MODEL FLAMES, P71
   Jensen HW., 2001, REALISTIC IMAGE SYNT, DOI [10.1201/9780429294907, DOI 10.1201/9780429294907]
   Kang BK, 2005, COMPUT ANIMAT VIRT W, V16, P353, DOI 10.1002/cav.94
   Lamorlette A, 2002, ACM T GRAPHIC, V21, P729, DOI 10.1145/566570.566644
   Lee H, 2001, SPRING EUROGRAP, P75
   Losasso F, 2006, IEEE T VIS COMPUT GR, V12, P343, DOI 10.1109/TVCG.2006.51
   MIN K, 2005, POST SESS PAC GRAPH
   Nguyen DQ, 2002, ACM T GRAPHIC, V21, P721, DOI 10.1145/566570.566643
   PERLIN K., 1985, IMAGE SYNTHESIZER, P287
   Perry C.H., 1994, P 5 EUROGRAPHICS WOR, P105
   Premoze S, 2003, COMPUT GRAPH FORUM, V22, P401, DOI 10.1111/1467-8659.00687
   REEVES WT, 1983, ACM T GRAPHIC, V2, P91, DOI 10.1145/964967.801167
   Stam J, 1999, COMP GRAPH, P121, DOI 10.1145/311535.311548
   Stam Jos., 1995, Proceedings of the 22nd annual conference on Computer graphics and interactive techniques, SIGGRAPH '95, P129
   Wei XM, 2002, VIS 2002: IEEE VISUALIZATION 2002, PROCEEDINGS, P227, DOI 10.1109/VISUAL.2002.1183779
NR 24
TC 5
Z9 8
U1 0
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2007
VL 23
IS 9-11
SI SI
BP 679
EP 687
DI 10.1007/s00371-007-0162-3
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 206UE
UT WOS:000249207600008
DA 2024-07-18
ER

PT J
AU Shen, JB
   Jin, XG
   Mao, XY
   Feng, JG
AF Shen, Jianbing
   Jin, Xiaogang
   Mao, Xiaoyang
   Feng, Jieqing
TI Deformation-based interactive texture design using energy optimization
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 25th Computer Graphics International Conference (CGI)
CY MAY 30-JUN 02, 2007
CL Petropolis, BRAZIL
DE deformation; interactive; texture design; brushes; energy optimization
ID IMAGE; COMPLETION
AB In this paper, we present a novel interactive texture design scheme based on deformation and energy optimization. Given a small sample texture, the design process starts with applying a set of deformation operations to the sample texture to obtain a set of deformed textures. Then local changes to those deformed textures are further made by replacing their local regions with the texture elements interactively selected from other textures. Such a deform-select-replace process is iterated many times until the desired deformed textures are obtained. Finally the deformed textures are composed to form a large texture with graph-cut optimization. By combining the graph-cut algorithm with an energy optimization process, interactive selections of local texture elements are done simply through indicating the positions of texture elements very roughly with a brush tool. Our experimental results demonstrate that the proposed technique can be used for designing a large variety of versatile textures from a single small sample texture, increasing or decreasing the density of texture elements, as well as for synthesizing textures from multiple sources.
C1 Zhejiang Univ, State Key Lab CAD & CG, Hangzhou 310027, Peoples R China.
   Univ Yamanashi, Yamanashi, Japan.
C3 Zhejiang University; University of Yamanashi
RP Jin, XG (corresponding author), Zhejiang Univ, State Key Lab CAD & CG, Hangzhou 310027, Peoples R China.
EM shenjianbing@cad.zju.edu.cn; jin@cad.zju.edu.cn; mao@yamanashi.ae.jp
RI Shen, Jianbing/U-8796-2019
OI Shen, Jianbing/0000-0002-4109-8353; mao, xiaoyang/0000-0001-9531-3197
CR Agarwala A, 2004, ACM T GRAPHIC, V23, P294, DOI 10.1145/1015706.1015718
   Barrett WA, 2002, ACM T GRAPHIC, V21, P777, DOI 10.1145/566570.566651
   Boykov Y, 2004, IEEE T PATTERN ANAL, V26, P1124, DOI 10.1109/TPAMI.2004.60
   Brooks S, 2002, ACM T GRAPHIC, V21, P653, DOI 10.1145/566570.566632
   Charalampidis D, 2006, IEEE T IMAGE PROCESS, V15, P777, DOI 10.1109/TIP.2005.860604
   Cohen MF, 2003, ACM T GRAPHIC, V22, P287, DOI 10.1145/882262.882265
   Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236
   Criminisi A, 2004, IEEE T IMAGE PROCESS, V13, P1200, DOI 10.1109/TIP.2004.833105
   Dischler JM, 2002, COMPUT GRAPH FORUM, V21, P401, DOI 10.1111/1467-8659.t01-1-00600
   Doretto G, 2003, INT J COMPUT VISION, V51, P91, DOI 10.1023/A:1021669406132
   Efros AA, 2001, COMP GRAPH, P341, DOI 10.1145/383259.383296
   Felzenszwalb PF, 2004, INT J COMPUT VISION, V59, P167, DOI 10.1023/B:VISI.0000022288.19776.77
   Jakimoski G, 2001, IEEE T CIRCUITS-I, V48, P163, DOI 10.1109/81.904880
   Kwatra V, 2003, ACM T GRAPHIC, V22, P277, DOI 10.1145/882262.882264
   Lefebvre S, 2006, ACM T GRAPHIC, V25, P541, DOI 10.1145/1141911.1141921
   Li Y, 2004, ACM T GRAPHIC, V23, P303, DOI 10.1145/1015706.1015719
   Liu YX, 2004, ACM T GRAPHIC, V23, P368, DOI 10.1145/1015706.1015731
   Matusik W, 2005, ACM T GRAPHIC, V24, P787, DOI 10.1145/1073204.1073262
   Nicoll A, 2005, COMPUT GRAPH FORUM, V24, P569, DOI 10.1111/j.1467-8659.2005.00882.x
   Pérez P, 2003, ACM T GRAPHIC, V22, P313, DOI 10.1145/882262.882269
   REICHMANN M, IMAGE PROCESSING WOR
   Shen JB, 2007, COMPUT GRAPH-UK, V31, P119, DOI 10.1016/j.cag.2006.10.004
   Shen JB, 2006, VISUAL COMPUT, V22, P936, DOI 10.1007/s00371-006-0079-2
   Wei LY, 2000, COMP GRAPH, P479, DOI 10.1145/344779.345009
   WEI LY, 2003, P SIGGRAPH 2003 C SK
   Wu Q, 2004, ACM T GRAPHIC, V23, P364, DOI 10.1145/1015706.1015730
NR 26
TC 3
Z9 4
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2007
VL 23
IS 9-11
SI SI
BP 631
EP 639
DI 10.1007/s00371-007-0154-3
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 206UE
UT WOS:000249207600003
DA 2024-07-18
ER

PT J
AU Shen, JB
   Jin, XG
   Sun, HQ
AF Shen, Jianbing
   Jin, Xiaogang
   Sun, Hanqiu
TI High dynamic range image tone mapping and retexturing using fast
   trilateral filtering
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 25th Computer Graphics International Conference (CGI)
CY MAY 30-JUN 02, 2007
CL Petropolis, BRAZIL
DE high dynamic range; image; fast trilateral filtering; tone mapping;
   retexturing
AB Using fast trilateral filtering we present a novel tone mapping and retexturing method for high dynamic range (HDR) images. Our new trilateral filtering-based tone mapping is about seven to ten times faster than that in [3]. Firstly, a novel tone mapping algorithm for HDR images is presented. It is based on fast bilateral filtering and two newly developed filters: the quasi-Cauchy function kernel filter and the fourth degree Taylor polynomial kernel filter. Secondly, a new gradient-based image retexturing method is introduced, which consists of three steps: 1) converting HDR images into low dynamic range (LDR) images using our fast trilateral filtering-based tone mapping method; 2) recovering the gradient luminance maps for the region to be retextured; 3) reconstructing the final retextured image by solving the Poisson equation. The proposed approach is suitable for HDR image tone mapping and retexturing, and experimental results have demonstrated the satisfactory performance of our method.
C1 Zhejiang Univ, Stete Key Lab CAD & CG, Hangzhou 310027, Peoples R China.
   Chinese Univ Hong Kong, Dept Comp Sci & Engn, Hong Kong, Hong Kong, Peoples R China.
C3 Zhejiang University; Chinese University of Hong Kong
RP Jin, XG (corresponding author), Zhejiang Univ, Stete Key Lab CAD & CG, Hangzhou 310027, Peoples R China.
EM shenjianbing@cad.zju.edu.cn; jin@cad.zju.edu.cn; hanqiu@cse.cuhk.edu.hk
RI Shen, Jianbing/U-8796-2019
OI Shen, Jianbing/0000-0002-4109-8353
CR Agrawal A, 2005, ACM T GRAPHIC, V24, P828, DOI 10.1145/1073204.1073269
   [Anonymous], 2002, PROC ACM T GRAPH SIG, DOI DOI 10.1145/566570.566574
   Bae SM, 2006, ACM T GRAPHIC, V25, P637, DOI 10.1145/1141911.1141935
   Choudhury P., 2003, Eurographics Symposium on Rendering. 14th Eurographics Workshop on Rendering, P186
   Debevec P., 1997, P ACM SIGGRAPH, P369, DOI DOI 10.1145/258734.258884
   Drago F, 2003, COMPUT GRAPH FORUM, V22, P419, DOI 10.1111/1467-8659.00689
   Fang H, 2004, ACM T GRAPHIC, V23, P354, DOI 10.1145/1015706.1015728
   FANT H, 2006, IEEE T VIS COMPUT GR, V12, P1580
   Fattal R., 2002, ACM Transactions on Graphics, V21, P249, DOI 10.1145/566570.566573
   Guo YW, 2005, COMPUT ANIMAT VIRT W, V16, P451, DOI 10.1002/cav.82
   JIN X, 2001, J GRAPHICS TOOLS, V6, P17
   Khan EA, 2006, ACM T GRAPHIC, V25, P654, DOI 10.1145/1141911.1141937
   Krawczyk G, 2005, COMPUT GRAPH FORUM, V24, P635, DOI 10.1111/j.1467-8659.2005.00888.x
   Li Y, 2004, ACM T GRAPHIC, V23, P303, DOI 10.1145/1015706.1015719
   Li YZ, 2005, ACM T GRAPHIC, V24, P836, DOI 10.1145/1073204.1073271
   Lischinski D, 2006, ACM T GRAPHIC, V25, P646, DOI 10.1145/1141911.1141936
   Liu YX, 2004, ACM T GRAPHIC, V23, P368, DOI 10.1145/1015706.1015731
   Meylan L, 2006, IEEE T IMAGE PROCESS, V15, P2820, DOI 10.1109/TIP.2006.877312
   Munkberg J, 2006, ACM T GRAPHIC, V25, P698, DOI 10.1145/1141911.1141944
   Oh BM, 2001, COMP GRAPH, P433
   Paris S., 2006, Proc. European Conference on Computer Vision, P568
   Pérez P, 2003, ACM T GRAPHIC, V22, P313, DOI 10.1145/882262.882269
   Reinhard E, 2005, IEEE T VIS COMPUT GR, V11, P13, DOI 10.1109/TVCG.2005.9
   Reinhard E, 2002, ACM T GRAPHIC, V21, P267, DOI 10.1145/566570.566575
   REINHARD E, 2006, ACM T GRAPHIC, V25, P707
   Reinhard E., 2006, HIGH DYNAMIC RANGE I, DOI 10.1016/B978-012585263-0/50005-1
   Seetzen H, 2004, ACM T GRAPHIC, V23, P760, DOI 10.1145/1015706.1015797
   Shen JB, 2007, COMPUT GRAPH-UK, V31, P119, DOI 10.1016/j.cag.2006.10.004
   Sherstyuk A, 1999, VISUAL COMPUT, V15, P171, DOI 10.1007/s003710050170
   Smith K, 2006, COMPUT GRAPH FORUM, V25, P427, DOI 10.1111/j.1467-8659.2006.00962.x
   Tomasi C, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P839, DOI 10.1109/ICCV.1998.710815
   Tsin YH, 2001, PROC CVPR IEEE, P539
   Xu RF, 2005, IEEE COMPUT GRAPH, V25, P57, DOI 10.1109/MCG.2005.133
   ZELINKA S., 2005, Proceedings of Graphics Interface 2005, P227
   Zhang R, 1999, IEEE T PATTERN ANAL, V21, P690, DOI 10.1109/34.784284
NR 35
TC 9
Z9 13
U1 0
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2007
VL 23
IS 9-11
SI SI
BP 641
EP 650
DI 10.1007/s00371-007-0155-2
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 206UE
UT WOS:000249207600004
DA 2024-07-18
ER

PT J
AU Kang, HW
   Chui, CK
   Chakraborty, UK
AF Kang, Hyung W.
   Chui, Charles K.
   Chakraborty, Uday K.
TI A unified scheme for adaptive stroke-based rendering
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 14th Pacific Conference on Computer Graphics and Applications
CY OCT 11-13, 2005
CL Taipei, TAIWAN
DE non-photorealistic rendering; stroke-based rendering; adaptive edge
   detection; adaptive bilateral filter; simulated annealing
AB This paper presents a comprehensive scheme for automatically generating a broad class of artistic illustrations from photographs. Using strokes as the major building blocks, our system optimizes the stroke attributes subject to the desired rendering style. The stroke attributes are computed adaptively to enable importance-based control of the abstraction level at each pixel. We propose a novel outline detection and refinement paradigm called edge painting to construct an outline map, and from which to derive the pixel-wise importance. We also introduce an adaptive bilateral filter to adaptively guide the curved stroke directions based on the importance map. Given the outline, importance, and direction maps, the system creates the illustration via selecting the representative colors, setting the style parameters, and optimizing the stroke attributes based on simulated annealing. The experimental results show that our scheme facilitates automatic production of artistic illustrations in a wide range of rendering styles.
C1 Univ Missouri, Dept Math & Comp Sci, St Louis, MO 63121 USA.
C3 University of Missouri System; University of Missouri Saint Louis
RP Kang, HW (corresponding author), Univ Missouri, Dept Math & Comp Sci, 1 Univ Blvd, St Louis, MO 63121 USA.
EM kang@arch.umsl.edu; chui@arch.umsl.edu; uday@arch.umsl.edu
RI Li, Mengqi/AAG-6804-2021
CR [Anonymous], 2002, Numerical Recipes in C++: The Art of Scientific Computing
   [Anonymous], P SIGGRAPH
   [Anonymous], 2002, PROC ACM T GRAPH SIG, DOI DOI 10.1145/566570.566574
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851
   Curtis C. J., 1997, Proceedings of the 24th annual conference on Computer graphics and interactive techniques, P421
   DeCarlo Doug., 2004, P INT S NONPHOTOREAL, P15, DOI DOI 10.1145/987657.987661
   Deussen O, 2000, COMPUT GRAPH FORUM, V19, pC41, DOI 10.1111/1467-8659.00396
   Elad M, 2002, IEEE T IMAGE PROCESS, V11, P1141, DOI 10.1109/TIP.2002.801126
   Elder JH, 1998, IEEE T PATTERN ANAL, V20, P699, DOI 10.1109/34.689301
   Fleishman S, 2003, ACM T GRAPHIC, V22, P950, DOI 10.1145/882262.882368
   Garnett R, 2005, IEEE T IMAGE PROCESS, V14, P1747, DOI 10.1109/TIP.2005.857261
   Gooch B., 2002, P 2 INT S NONPH AN R, P83
   Haeberli P., 1990, P 17 ANN C COMP GRAP, P207, DOI [10.1145/97879.97902, DOI 10.1145/97879.97902]
   Hausner A, 2001, COMP GRAPH, P573, DOI 10.1145/383259.383327
   Hays J., 2004, PROC NPAR 01, P113
   Hertzmann A, 2003, IEEE COMPUT GRAPH, V23, P70, DOI 10.1109/MCG.2003.1210867
   Hertzmann A, 2001, COMP GRAPH, P327, DOI 10.1145/383259.383295
   Hertzmann A, 2001, COMPUTER GRAPHICS INTERNATIONAL 2001, PROCEEDINGS, P47, DOI 10.1109/CGI.2001.934657
   Hertzmann A., 1998, Proceedings of the 25th Annual Conference on Computer Graphics and Interactive Techniques, P453
   Kang HW, 2005, VISUAL COMPUT, V21, P821, DOI 10.1007/s00371-005-0328-9
   Litwinowicz P., 1997, Proceedings of the 24th annual conference on Computer graphics and interactive techniques, SIGGRAPH '97, P407
   Ostromoukhov V, 1999, COMP GRAPH, P417, DOI 10.1145/311535.311604
   SALISBURY MP, 1997, P SIGGRAPH 97, P401
   Santella A., 2002, Proceedings of the 2Nd International Symposium on Non-photorealistic Animation and Rendering, P75, DOI DOI 10.1145/508530.508544
   SHIRAISHI M, 2000, P 1 INT S NONPH AN R, P53, DOI DOI 10.1145/340916.340923
   Siu Chi Hsu, 1994, Computer Graphics Proceedings. Annual Conference Series 1994. SIGGRAPH 94 Conference Proceedings, P109
   Sousa MC, 2000, COMPUT GRAPH FORUM, V19, P27, DOI 10.1111/1467-8659.00386
   Tomasi C, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P839, DOI 10.1109/ICCV.1998.710815
   Turk G., 1996, Proceedings of the 23rd Annual Conference on Computer Graphics and Interactive Techniques, New Orleans, USA, August 1996, P453
NR 29
TC 26
Z9 34
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2006
VL 22
IS 9-11
BP 814
EP 824
DI 10.1007/s00371-006-0066-7
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 082IW
UT WOS:000240381000024
DA 2024-07-18
ER

PT J
AU Shatz, I
   Tal, A
   Leifman, G
AF Shatz, Idan
   Tal, Ayellet
   Leifman, George
TI Paper craft models from meshes
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 14th Pacific Conference on Computer Graphics and Applications
CY OCT 11-13, 2005
CL Taipei, TAIWAN
DE paper-craft models; segmentation
ID DECOMPOSITION; SURFACES
AB This paper introduces an algorithm for segmenting a mesh into developable approximations. The algorithm can be used in various applications in CAD and computer graphics. This paper focuses on paper crafting and demonstrates that the algorithm generates approximations that are developable, easy to cut, and can be glued together. It is also shown that the error between the given model and the paper model is small.
C1 Technion Israel Inst Technol, Dept Elect Engn, IL-32000 Haifa, Israel.
C3 Technion Israel Institute of Technology
RP Tal, A (corresponding author), Technion Israel Inst Technol, Dept Elect Engn, IL-32000 Haifa, Israel.
EM shatzi@tx.technion.ac.il; ayellet@ee.technion.ac.il;
   gleifman@tx.technion.ac.il
CR Cignoni P, 1998, COMPUT GRAPH FORUM, V17, P167, DOI 10.1111/1467-8659.00236
   Cohen-Steiner D, 2004, ACM T GRAPHIC, V23, P905, DOI 10.1145/1015706.1015817
   Duda R., 1973, Pattern Classification and Scene Analysis
   GARLAND M, 2001, SI3D 01, P49, DOI DOI 10.1145/364338.364345
   Guskov I, 2000, COMP GRAPH, P95, DOI 10.1145/344779.344831
   IGARASHI T, 2001, SI3D 01, P209
   Julius D, 2005, COMPUT GRAPH FORUM, V24, P581, DOI 10.1111/j.1467-8659.2005.00883.x
   Katz S, 2005, VISUAL COMPUT, V21, P649, DOI 10.1007/s00371-005-0344-9
   Katz S, 2003, ACM T GRAPHIC, V22, P954, DOI 10.1145/882262.882369
   Kleppner D., 1973, An Introduction to Mechanics
   KRISHNAMURTHY V, 1996, SIGGRAPH 96 C P, P313
   Lee Y, 2004, 12TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P279
   Lévy B, 2002, ACM T GRAPHIC, V21, P362, DOI 10.1145/566570.566590
   Liu R, 2004, 12TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P298
   MAILLOT J, 1993, SIGGRAPH 93, P27
   Mitani J, 2004, ACM T GRAPHIC, V23, P259, DOI 10.1145/1015706.1015711
   Mortara M., 2004, PROC 9 ACM S SOLID M, P139, DOI DOI 10.1145/882262.882369
   SANDER P, 2003, EUR S GEOM PROC, P146
   Sander PV, 2001, COMP GRAPH, P409, DOI 10.1145/383259.383307
   SHAMIR A, 2004, P 2 INT S 3DPVT
   Shlafman S, 2002, COMPUT GRAPH FORUM, V21, P219, DOI 10.1111/1467-8659.00581
   TAUBIN G, 1991, IEEE T PATTERN ANAL, V13, P1115, DOI 10.1109/34.103273
   Wu JH, 2005, COMPUT GRAPH FORUM, V24, P277, DOI 10.1111/j.1467-8659.2005.00852.x
   Yamauchi H, 2005, VISUAL COMPUT, V21, P659, DOI 10.1007/s00371-005-0319-x
   ZHOU K, 2004, EUR ACM SIGGRAPH S G, P45
   Zhou YN, 2004, 10TH INTERNATIONAL MULTIMEDIA MODELLING CONFERENCE, PROCEEDINGS, P187
   Zuckerberger E, 2002, COMPUT GRAPH-UK, V26, P733, DOI 10.1016/S0097-8493(02)00128-0
NR 27
TC 69
Z9 80
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2006
VL 22
IS 9-11
BP 825
EP 834
DI 10.1007/s00371-006-0067-6
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 082IW
UT WOS:000240381000025
DA 2024-07-18
ER

PT J
AU Weng, YL
   Xu, WW
   Wu, Y
   Zhou, K
   Guo, BN
AF Weng, Yanlin
   Xu, Weiwei
   Wu, Yanchen
   Zhou, Kun
   Guo, Baining
TI 2D shape deformation using nonlinear least squares optimization
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 14th Pacific Conference on Computer Graphics and Applications
CY OCT 11-13, 2005
CL Taipei, TAIWAN
DE object manipulation; image editing; character animation; area
   preservation
AB This paper presents a novel 2D shape deformation algorithm based on nonlinear least squares optimization. The algorithm aims to preserve two local shape properties: the Laplacian coordinates of the boundary curve and the local area of the shape interior, which are together represented in a non-quadratic energy function. An iterative Gauss-Newton method is used to minimize this nonlinear energy function. The result is an interactive shape deformation system that can achieve physically plausible results that are difficult to achieve with previous linear least squares methods. In addition to this algorithm that preserves local shape properties, we also introduce a scheme to preserve the global area of the shape, which is useful for deforming incompressible objects.
C1 Microsoft Res Asia, Graph Grp, Beijing, Peoples R China.
   Univ Wisconsin, Milwaukee, WI 53201 USA.
   Zhejiang Univ, Hangzhou 310027, Peoples R China.
C3 Microsoft Research Asia; Microsoft; University of Wisconsin System;
   University of Wisconsin Milwaukee; Zhejiang University
RP Zhou, K (corresponding author), Microsoft Res Asia, Graph Grp, Beijing, Peoples R China.
EM weng@uwm.edu; wwxu@microsoft.com; raincoat.zju@hotmail.com;
   kunzhou@microsoft.com; bainguo@microsoft.com
RI Zhou, Kun/ABF-4071-2020; Zhou, Kun/AAH-9290-2019
OI Zhou, Kun/0000-0003-2320-3655; 
CR Alexa M, 2000, COMP GRAPH, P157, DOI 10.1145/344779.344859
   [Anonymous], 2004, Technical Report
   [Anonymous], 1997, TR9719 MITS EL RES L
   AU OKC, 2005, HKUSTCS0510
   CELNIKER G, 1991, COMP GRAPH, V25, P257, DOI 10.1145/127719.122746
   Floater MS, 2003, COMPUT AIDED GEOM D, V20, P19, DOI 10.1016/S0167-8396(03)00002-5
   Huang J, 2006, ACM T GRAPHIC, V25, P1126, DOI 10.1145/1141911.1142003
   Igarashi T, 2005, ACM T GRAPHIC, V24, P1134, DOI 10.1145/1073204.1073323
   James DL, 1999, COMP GRAPH, P65, DOI 10.1145/311535.311542
   Lewis JP, 2000, COMP GRAPH, P165, DOI 10.1145/344779.344862
   Lipman Y, 2005, ACM T GRAPHIC, V24, P479, DOI 10.1145/1073204.1073217
   MacCracken R., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P181, DOI 10.1145/237170.237247
   Madsen K., 2004, Optimization with constraints
   Milliron T, 2002, ACM T GRAPHIC, V21, P20, DOI 10.1145/504789.504791
   Müller M, 2005, ACM T GRAPHIC, V24, P471, DOI 10.1145/1073204.1073216
   Sederberg T. W., 1986, Computer Graphics, V20, P151, DOI 10.1145/15886.15903
   Sheffer A, 2004, 2ND INTERNATIONAL SYMPOSIUM ON 3D DATA PROCESSING, VISUALIZATION, AND TRANSMISSION, PROCEEDINGS, P68
   Sorkine O., 2004, P 2004 EUR ACM SIGGR, P179
   Yu YZ, 2004, ACM T GRAPHIC, V23, P644, DOI 10.1145/1015706.1015774
   Zhou K, 2005, ACM T GRAPHIC, V24, P496, DOI 10.1145/1073204.1073219
NR 20
TC 65
Z9 81
U1 1
U2 8
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2006
VL 22
IS 9-11
BP 653
EP 660
DI 10.1007/s00371-006-0054-y
PG 8
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 082IW
UT WOS:000240381000008
DA 2024-07-18
ER

PT J
AU Glardon, P
   Boulic, R
   Thalmann, D
AF Glardon, Pascal
   Boulic, Ronan
   Thalmann, Daniel
TI Dynamic obstacle avoidance for real-time character animation
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Computer Graphics International Conference (CGI 2005)
CY JUN 22-24, 2005
CL Stony Brook, NY
SP IEEE Comp Soc, VGTC, ACM SIGGRAPH
DE human body simulation; real-time motion blending; motion planning;
   obstacle avoidance
AB This paper proposes a novel method to control virtual characters in dynamic environments. A virtual character is animated by a locomotion and jumping engine, enabling production of continuous parameterized motions. At any time during runtime, flat obstacles (e.g. a puddle of water) can be created and placed in front of a character. The method first decides whether the character is able to get around or jump over the obstacle. Then the motion parameters are accordingly modified. The transition from locomotion to jump is performed with an improved motion blending technique. While traditional blending approaches let the user choose the transition time and duration manually, our approach automatically controls transitions between motion patterns whose parameters are not known in advance. In addition, according to the animation context, blending operations are executed during a precise period of time to preserve specific physical properties. This ensures coherent movements over the parameter space of the original input motions. The initial locomotion type and speed are smoothly varied with respect to the required jump type and length. This variation is carefully computed in order to place the take-off foot as close to the created obstacle as possible.
C1 Ecole Polytech Fed Lausanne, Virtual Real Lab, CH-1015 Lausanne, Switzerland.
C3 Swiss Federal Institutes of Technology Domain; Ecole Polytechnique
   Federale de Lausanne
RP Boulic, R (corresponding author), Ecole Polytech Fed Lausanne, Virtual Real Lab, CH-1015 Lausanne, Switzerland.
EM ronan.boulic@epfl.ch
RI Thalmann, Daniel/A-4347-2008; Thalmann, Daniel/AAL-1097-2020; BOULIC,
   RONAN/A-9108-2008
OI Thalmann, Daniel/0000-0002-0451-7491; BOULIC, RONAN/0000-0001-9176-6877
NR 0
TC 1
Z9 1
U1 1
U2 5
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2006
VL 22
IS 6
BP 399
EP 414
DI 10.1007/s00371-006-0017-3
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 053BL
UT WOS:000238278500004
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Feng, J
   Shao, J
   Jin, X
   Peng, Q
   Forrest, AR
AF Feng, J
   Shao, J
   Jin, X
   Peng, Q
   Forrest, AR
TI Multiresolution free-form deformation with subdivision surface of
   arbitrary topology
SO VISUAL COMPUTER
LA English
DT Article
DE free-form deformation; subdivision surface; control mesh;
   multiresolution
AB A new free-from deformation method is presented in this paper. Object deformation is controlled by a mesh of arbitrary topology, namely a control mesh. The subdivision surface determined by the control mesh spans an intermediate deformation space. The object is embedded into the space by the nearest point rule. When the shape of the control mesh is changed, the object embedded in the intermediate deformation space will be deformed accordingly. Since the subdivision surface has a multiresolution property, the proposed deformation method naturally has a multiresolution property. A technique for generating control meshes is also introduced in the paper. Compared with previous deformation methods with arbitrary topology control tools, the proposed method has the advantages of flexible control and computational efficiency.
C1 Zhejiang Univ, State Key Lab CAD & CG, Hangzhou 310027, Peoples R China.
   Univ E Anglia, Sch Comp Sci, Norwich NR4 7TJ, Norfolk, England.
C3 Zhejiang University; University of East Anglia
RP Zhejiang Univ, State Key Lab CAD & CG, Hangzhou 310027, Peoples R China.
EM jqfeng@cad.zju.edu.cn; jshao@cad.zju.edu.cn; jin@cad.zju.edu.cn;
   peng@cad.zju.edu.cn; forrest@cmp.uea.ac.uk
CR [Anonymous], ACM SIGGRAPH
   Bechmann D, 2003, VISUAL COMPUT, V19, P175, DOI 10.1007/s00371-002-0191-x
   BECHMANN D, 1998, EUROGRAPHICS 98, P102
   BEIER T, 1992, P SIGGRAPH 92, P35
   Borrel P, 1991, INT J COMPUT GEOM AP, V1, P427, DOI 10.1142/S0218195991000281
   CATMULL E, 1978, COMPUT AIDED DESIGN, V10, P350, DOI 10.1016/0010-4485(78)90110-0
   Coquillart S., 1990, J. Computer Graphics, V24, P187, DOI DOI 10.1145/97880.97900
   DOO D, 1978, COMPUT AIDED DESIGN, V10, P356, DOI 10.1016/0010-4485(78)90111-2
   DYN N, 1990, ACM T GRAPHIC, V9, P160, DOI 10.1145/78956.78958
   Feng JQ, 1996, COMPUT GRAPH, V20, P531, DOI 10.1016/0097-8493(96)00025-8
   Guéziec A, 2001, IEEE T VIS COMPUT GR, V7, P47, DOI 10.1109/2945.910820
   Hoppe H., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P99, DOI 10.1145/237170.237216
   Kobayashi K.G., 2003, Proceedings of the eighth ACM symposium on Solid modeling and applications. SM '03, P226, DOI DOI 10.1145/781606.781641.2
   KOBBELT L, 2000, EUROGRAPHICS 2000, V19, pC249
   KOBBELT L, 1998, P SIGGRAPH 98, P105, DOI DOI 10.1145/280814.280831
   LAZARUS F, 1994, COMPUT AIDED DESIGN, V26, P607, DOI 10.1016/0010-4485(94)90103-1
   Lipman Y, 2004, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, P181, DOI 10.1109/SMI.2004.1314505
   LOOP C, 1987, THESIS UTAH STATE U
   MacCracken Ron., 1996, SIGGRAPH, P181, DOI DOI 10.1145/237170.237247
   Moccozet L, 1997, COMP ANIM CONF PROC, P93, DOI 10.1109/CA.1997.601047
   SHINAGAWA Y, 1991, IEEE COMPUT GRAPH, V11, P44, DOI 10.1109/38.103393
   Sorkine O., 2004, P EUR ASS COMP MACH, P175, DOI DOI 10.1145/1057432.1057456
   Tranel Bernard., 1990, PROBUS, V2, P169, DOI DOI 10.1515/PRBS.1990.2.2.169
   WANG G, 2001, J COMPUT AIDED DES C, V13, P1
   Yu YZ, 2004, ACM T GRAPHIC, V23, P644, DOI 10.1145/1015706.1015774
   ZORIN D, 1999, ACM SIGGRAPH 99
NR 26
TC 14
Z9 24
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2006
VL 22
IS 1
BP 28
EP 42
DI 10.1007/s00371-005-0351-x
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 991NT
UT WOS:000233821400004
DA 2024-07-18
ER

PT J
AU Buss, SR
AF Buss, SR
TI Collision detection with relative screw motion
SO VISUAL COMPUTER
LA English
DT Article
DE collision detection; proper collision; rasterization; rigid body
   collision; screw motion
ID INTERFERENCE DETECTION; GRAPHICS; SOLIDS
AB We introduce a framework for collision detection between a pair of rigid polyhedra. Given the initial and final positions and orientations of two objects, the algorithm determines whether they collide, and if so, when and where. All collisions, including collisions at different times, are computed at once along with "properness" values that measure the extent to which the collisions are between parts of the objects that properly face each other. This is useful for handling multiple (nearly) concurrent collisions and contact points.
   The relative motions of the rigid bodies are limited to screw motions. This limitation is not always completely accurate, but we can estimate the error introduced by the assumption.
   Our implementation uses rasterization to approximate the position and time of the collisions. This allows level-of-detail techniques that provide a tradeoff between accuracy and computational expense.
   The collision detection algorithms are only approximate, both because of the screw motion assumption and because of the rasterization. However, they can be made robust so as to give consistent information about collisions and to avoid sensitivity to roundoff errors.
C1 Univ Calif San Diego, Dept Math, La Jolla, CA 92093 USA.
C3 University of California System; University of California San Diego
RP Univ Calif San Diego, Dept Math, La Jolla, CA 92093 USA.
EM sbuss@ucsd.edu
CR Agarwal P. K., 2000, Nordic Journal of Computing, V7, P227
   [Anonymous], P ACM SIGGRAPH EUROG
   Baciu G, 2003, IEEE T VIS COMPUT GR, V9, P254, DOI 10.1109/TVCG.2003.1196011
   Baraff D., 1994, Computer Graphics Proceedings. Annual Conference Series 1994. SIGGRAPH 94 Conference Proceedings, P23, DOI 10.1145/192161.192168
   BARAFF D, 1997, SIGGRAPH 97 COURS NO
   BAUMGART B., 1975, NATL COMPUTER C AFIP, P589, DOI DOI 10.1145/1499949.1500071
   BOYSE JW, 1979, COMMUN ACM, V22, P3, DOI 10.1145/359046.359048
   Cameron S, 1997, IEEE INT CONF ROBOT, P3112, DOI 10.1109/ROBOT.1997.606761
   CAMERON S, 1990, IEEE T ROBOTIC AUTOM, V6, P291, DOI 10.1109/70.56661
   Cameron S., 1985, Robotics and Automation. Proceedings. 1985 IEEE International Conference on, V2, P488
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P200, DOI 10.1109/TPAMI.1986.4767773
   Chatterjee A, 1998, J APPL MECH-T ASME, V65, P939, DOI 10.1115/1.2791938
   DOBKIN D, 1993, ALGORITHMICA, V9, P518, DOI 10.1007/BF01190153
   DOBKIN DP, 1990, LECT NOTES COMPUT SC, V443, P400, DOI 10.1007/BFb0032047
   ECKSTEIN J, 1999, P 7 INT C CENTR EUR, P71
   Ehmann SA, 2000, 2000 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS 2000), VOLS 1-3, PROCEEDINGS, P2101, DOI 10.1109/IROS.2000.895281
   EHMANN SA, 2001, P EUROGRAPHICS 2001, V20, P500
   Erickson J, 1999, PROCEEDINGS OF THE TENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P327
   Giang T., 2003, P 4 IR WORKSH COMP G, P1
   GILBERT EG, 1988, IEEE T ROBOTIC AUTOM, V4, P193, DOI 10.1109/56.2083
   Goldstein H., 1950, CLASSICAL MECH
   Gottschalk S., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P171, DOI 10.1145/237170.237244
   HOFF KE, 1999, P AMS SIGGRAPH, V33, P277
   HUBBARD PM, 1995, IEEE T VIS COMPUT GR, V1, P218, DOI 10.1109/2945.466717
   Kim B., 2003, Proceedings of the Eighth ACM Symposium on Solid Modeling and Applications, P4
   KIM Y, 2002, SIGGRAPH 2002 C P, P23
   Kim YJ, 2002, 2002 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION, VOLS I-IV, PROCEEDINGS, P921, DOI 10.1109/ROBOT.2002.1013474
   Kirkpatrick D, 2002, INT J COMPUT GEOM AP, V12, P3, DOI 10.1142/S0218195902000724
   KIRKPATRICK D, 1983, SIAM J COMPUT, V12, P28, DOI 10.1137/0212002
   KIRKPATRICK D, 2002, P 18 ANN ACM S COMP, P179
   Klosowski JT, 1998, IEEE T VIS COMPUT GR, V4, P21, DOI 10.1109/2945.675649
   Knott D, 2003, PROC GRAPH INTERF, P73
   Lengyel J., 1990, Computer Graphics, V24, P327, DOI 10.1145/97880.97915
   Lennerz C, 1999, SIMULATION IN INDUSTRY'99: 11TH EUROPEAN SIMULATION SYMPOSIUM 1999, P309
   Lin M.C., 1993, Ph.D. Thesis
   Lin M.C., 1998, PROC IMA C MATH SURF, P37
   LIN MC, 1991, 1991 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION, VOLS 1-3, P1008, DOI 10.1109/ROBOT.1991.131723
   MANOCHA D, 2002, 31 SIGGRAPH
   Mirtich B, 2000, COMP GRAPH, P193, DOI 10.1145/344779.344866
   Mirtich B, 1998, ACM T GRAPHIC, V17, P177, DOI 10.1145/285857.285860
   MYSZKOWSKI K, 1995, VISUAL COMPUT, V11, P497, DOI 10.1007/BF02439645
   Purcell TJ, 2002, ACM T GRAPHIC, V21, P703, DOI 10.1145/566570.566640
   Redon S, 2002, 2002 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION, VOLS I-IV, PROCEEDINGS, P517, DOI 10.1109/ROBOT.2002.1013411
   Redon S, 2001, ROBOT AND HUMAN COMMUNICATION, PROCEEDINGS, P106, DOI 10.1109/ROMAN.2001.981886
   Redon S, 2002, COMPUT GRAPH FORUM, V21, P279, DOI 10.1111/1467-8659.t01-1-00587
   Redon S., 2000, Proceedings 2000 ICRA. Millennium Conference. IEEE International Conference on Robotics and Automation. Symposia Proceedings (Cat. No.00CH37065), P3733, DOI 10.1109/ROBOT.2000.845313
   Redon S., 2002, P IEEE RSJ INT C INT, V3, P3036
   ROSSIGNAC J, 1992, COMP GRAPH, V26, P353, DOI 10.1145/142920.134092
   Rossignac JR, 2001, COMPUT AIDED DESIGN, V33, P279, DOI 10.1016/S0010-4485(00)00086-5
   Schomer E., 1995, Proceedings of the Eleventh Annual Symposium on Computational Geometry, P51, DOI 10.1145/220279.220285
   Selig J. M., 1996, GEOMETRICAL METHODS
   Shepperd S. W., 1978, Journal of Guidance and Control, V1, P223, DOI [10.2514/3.55767b, DOI 10.2514/3.55767B]
   Shinya M., 1991, Journal of Visualization and Computer Animation, V2, P132, DOI 10.1002/vis.4340020408
   Shoemake K., 1985, SIGGRAPH COMPUT GRAP, V19, P245, DOI DOI 10.1145/325334.325242
   Snyder J. M., 1993, Computer Graphics Proceedings, P321, DOI 10.1145/166117.166158
   VANECEK G, 1994, J VISUAL COMP ANIMAT, V5, P55, DOI 10.1002/vis.4340050105
   Von Herzen B., 1990, Computer Graphics, V24, P39, DOI 10.1145/97880.97883
NR 57
TC 4
Z9 4
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2005
VL 21
IS 1-2
BP 41
EP 58
DI 10.1007/s00371-004-0269-8
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 911OG
UT WOS:000228013300003
DA 2024-07-18
ER

PT J
AU Cohen-Steiner, D
   Da, F
AF Cohen-Steiner, D
   Da, F
TI A greedy Delaunay-based surface reconstruction algorithm
SO VISUAL COMPUTER
LA English
DT Article
DE Delaunay triangulation; surface reconstruction; advancing front method
ID SKELETON; CRUST
AB In this paper, we present a new greedy algorithm for surface reconstruction from unorganized point sets. Starting from a seed facet, a piecewise linear surface is grown by adding Delaunay triangles one by one. The most plausible triangles are added first and in such a way as to prevent the appearance of topological singularities. The output is thus guaranteed to be a piecewise linear orientable manifold, possibly with boundary. Experiments show that this method is very fast and achieves topologically correct reconstruction in most cases. Moreover, it can handle surfaces with complex topology, boundaries, and nonuniform sampling.
C1 INRIA, F-06902 Sophia Antipolis, France.
C3 Inria
RP INRIA, BP 93, F-06902 Sophia Antipolis, France.
EM david.cohen-steiner@sophia.inria.fr; frank.da@sophia.inria.fr
CR ADAMY U, 2000, 16 EUR WORKSH COMP G, P14
   Alboul L, 2000, J COMPUT APPL MATH, V119, P1, DOI 10.1016/S0377-0427(00)00368-X
   Amenta N, 1998, GRAPH MODEL IM PROC, V60, P125, DOI 10.1006/gmip.1998.0465
   Amenta N, 1999, DISCRETE COMPUT GEOM, V22, P481, DOI 10.1007/PL00009475
   Amenta N, 2001, COMP GEOM-THEOR APPL, V19, P127, DOI 10.1016/S0925-7721(01)00017-7
   Amenta N., 2000, PROC 16 ACM ANN S CO, P213
   Bernardini F, 1999, IEEE T VIS COMPUT GR, V5, P349, DOI 10.1109/2945.817351
   Boissonnat J.-D., 2000, P 16 ANN S COMPUTATI, P223
   BOISSONNAT JD, 1984, ACM T GRAPHIC, V3, P266, DOI 10.1145/357346.357349
   BRANDT JW, 1994, CVGIP-IMAG UNDERSTAN, V59, P116, DOI 10.1006/ciun.1994.1007
   Dey T.K., 2001, P 17 ANN S COMPUTATI, P257, DOI [DOI 10.1145/378583.378682, 10.1145/378583.378682]
   Edelsbrunner H., 1992, Weighted alpha shapes
   HOPPE H, 1992, COMP GRAPH, V26, P71, DOI 10.1145/142920.134011
   Medioni G., 2000, COMPUTATIONAL FRAMEW
   Petitjean S, 2001, COMP GEOM-THEOR APPL, V19, P101, DOI 10.1016/S0925-7721(01)00016-5
   SETHIAN J., 1996, LEVEL SET METHODS
   Zhao HK, 2000, COMPUT VIS IMAGE UND, V80, P295, DOI 10.1006/cviu.2000.0875
NR 17
TC 66
Z9 83
U1 1
U2 20
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2004
VL 20
IS 1
BP 4
EP 16
DI 10.1007/s00371-003-0217-z
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 809SA
UT WOS:000220655200001
DA 2024-07-18
ER

PT J
AU Yee, YH
   Pattanaik, S
AF Yee, YH
   Pattanaik, S
TI Segmentation and adaptive assimilation for detail-preserving display of
   high-dynamic range images
SO VISUAL COMPUTER
LA English
DT Article
DE image processing; image segmentation; signal processing; biological
   modeling; tone reproduction operator
ID TONE REPRODUCTION
AB Realistic display of high-dynamic range images is a difficult problem. Previous methods for high-dynamic range image display suffer from halo artifacts or are computationally expensive. We present a novel method for computing local adaptation luminance that can be used with several different visual adaptation-based tone-reproduction operators for displaying visually accurate high-dynamic range images. The method uses fast image segmentation, grouping, and graph operations to generate local adaptation luminance. Results on several images show excellent dynamic range compression, while preserving detail without the presence of halo artifacts. With adaptive assimilation, the method can be configured to bring out a high-dynamic range appearance in the display image. The method is efficient in terms of processor and memory use.
C1 PDI DreamWorks, Redwood City, CA USA.
   Univ Cent Florida, Orlando, FL 32816 USA.
C3 DreamWorks Animation LLC; State University System of Florida; University
   of Central Florida
RP PDI DreamWorks, Redwood City, CA USA.
EM yhy1@cornell.edu; sumant@cs.ucf.edu
CR [Anonymous], 1994, Graph. Gems, DOI DOI 10.1016/B978-0-12-336156-1.50054-9
   [Anonymous], 2002, PROC ACM T GRAPH SIG, DOI DOI 10.1145/566570.566574
   ASHIKHMIN M, 2002, P EUR WORKSH REND 20, P151
   CHIU K, 1993, GRAPH INTER, P245
   Debevec Paul E, 2008, ACM SIGGRAPH 2008 CL, P1, DOI DOI 10.1145/1401132.1401174
   Fattal R., 2002, ACM Transactions on Graphics, V21, P249, DOI 10.1145/566570.566573
   FEWERDA JA, 1996, P SIGGRAPH 96 AUG 19, P249
   Larson GW, 1997, IEEE T VIS COMPUT GR, V3, P291, DOI 10.1109/2945.646233
   MacQueen J., 1967, P 5 BERK S MATH STAT, P281
   Nayar SK, 2000, PROC CVPR IEEE, P472, DOI 10.1109/CVPR.2000.855857
   Pattanaik S., 2002, Proceedings of the 18th spring conference on Computer graphics, P83
   Pattanaik S.N., 1998, P SIGGRAPH 98, P287
   Pattanaik SN, 2000, COMP GRAPH, P47, DOI 10.1145/344779.344810
   Rahman Z, 1996, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, PROCEEDINGS - VOL III, P1003, DOI 10.1109/ICIP.1996.560995
   Reinhard E, 2002, ACM T GRAPHIC, V21, P267, DOI 10.1145/566570.566575
   Schlick C., 1995, Photorealistic Rendering Techniques, P7
   TUMBLIN J, 1993, IEEE COMPUT GRAPH, V13, P42, DOI 10.1109/38.252554
   Tumblin J, 1999, COMP GRAPH, P83, DOI 10.1145/311535.311544
   Tumblin J, 1999, ACM T GRAPHIC, V18, P56, DOI 10.1145/300776.300783
NR 19
TC 25
Z9 36
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2003
VL 19
IS 7-8
BP 457
EP 466
DI 10.1007/s00371-003-0211-5
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 749YL
UT WOS:000186957600003
DA 2024-07-18
ER

PT J
AU Larsson, T
   Akenine-Möller, T
AF Larsson, T
   Akenine-Möller, T
TI Efficient collision detection for models deformed by morphing
SO VISUAL COMPUTER
LA English
DT Article
DE collision detection; morphing; deformable bodies; hierarchical data
   structures; virtual reality
ID HIERARCHIES
AB We describe a fast and accurate collision-detection algorithm specialised for models deformed by morphing. The models considered are meshes where the vertex positions are convex combinations of sets of reference meshes. This new method is based on bounding-volume trees that are extended to support efficient tree-node updates by blending associated sets of reference bounding volumes. With our approach, it is possible to use either axis-aligned bounding boxes, discrete-orientation polytopes, or spheres as bounding volumes. The expected performance of our algorithm is of the same order as for rigid hierarchical collision detection. In our tested scenarios, the speed-up we achieved ranged from 1.5 to 58, compared to another more general algorithm for deforming bodies.
C1 Malardalen Univ, Dept Comp Sci & Engn, Vasteras, Sweden.
   Chalmers Inst Technol, Dept Comp Engn, Gothenburg, Sweden.
C3 Malardalen University; Chalmers University of Technology
RP Malardalen Univ, Dept Comp Sci & Engn, Vasteras, Sweden.
EM thomas.larsson@mdh.se
OI Larsson, Thomas/0000-0002-1550-0994
CR ALEXA M, 2001, STATE ART REPORTS EU
   BARAFF D, 1992, P 19 ANN C COMP GRAP
   BARAFF D, 1990, P 17 ANN C COMP GRAP
   Barequet G, 1996, COMPUT GRAPH FORUM, V15, pC387, DOI 10.1111/1467-8659.1530387
   Ehmann SA, 2001, COMPUT GRAPH FORUM, V20, pC500
   GOLDSMITH J, 1987, IEEE COMPUT GRAPH, V7, P14, DOI 10.1109/MCG.1987.276983
   GOTTSCHALK S, 1996, P 23 ANN C COMP GRAP
   Gottschalk S., 2000, Collision queries using oriented bounding boxes
   HE T, 1999, 1999 S INT 3D GRAPH
   Hubbard PM, 1996, ACM T GRAPHIC, V15, P179, DOI 10.1145/231731.231732
   HUBBARD PM, 1995, IEEE T VIS COMPUT GR, V1, P218, DOI 10.1109/2945.466717
   HUBBARD PM, 1993, IEEE 1993 S RES FRON
   Jiménez P, 2001, COMPUT GRAPH-UK, V25, P269, DOI 10.1016/S0097-8493(00)00130-8
   KENT J, 1992, P 19 ANN C COMP GRAP
   Kitamura Y, 1998, PRESENCE-TELEOP VIRT, V7, P36, DOI 10.1162/105474698565514
   Klosowski JT, 1998, IEEE T VIS COMPUT GR, V4, P21, DOI 10.1109/2945.675649
   LARSSON T, 2001, SHORT PRESENTATIONS
   Lazarus F, 1998, VISUAL COMPUT, V14, P373, DOI 10.1007/s003710050149
   Li T-Y, 1998, P ACM S VIRT REAL SO
   LINDHOLM E, 2001, P 28 ANN C COMP GRAP
   Liu JD, 1996, VISUAL COMPUT, V12, P234
   PALMER IJ, 1995, COMPUT GRAPH FORUM, V14, P105, DOI 10.1111/1467-8659.1420105
   SMITH A, 1995, VIRT REAL ANN INT S
   VanderStricht W, 1997, MRS INTERNET J N S R, V2
   Vassilev T, 2001, COMPUT GRAPH FORUM, V20, pC260, DOI 10.1111/1467-8659.00518
   VOLINO P, 1994, COMPUT GRAPH FORUM, V13, pC155, DOI 10.1111/1467-8659.1330155
   VOLINO P, 1995, P 22 ANN C COMP GRAP
   VONHERZEN B, 1990, P 17 ANN C COMP GRAP
   Zachmann G, 1998, P IEEE VIRT REAL ANN, P90, DOI 10.1109/VRAIS.1998.658428
   ZACHMANN G, 1995, P WORKSH SIM INT VIR
   Zöckler M, 2000, VISUAL COMPUT, V16, P241, DOI 10.1007/PL00013396
NR 31
TC 22
Z9 24
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2003
VL 19
IS 2-3
BP 164
EP 174
DI 10.1007/s00371-002-0190-y
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 691AU
UT WOS:000183583300009
DA 2024-07-18
ER

PT J
AU Lee, YC
   Jen, CW
AF Lee, YC
   Jen, CW
TI Edge-preserving texture filtering for real-time rendering
SO VISUAL COMPUTER
LA English
DT Article
DE filtering; resampling; texture mapping
ID INTERPOLATION; ALGORITHM
AB Texture filtering is essential in enhancing the visual quality of real-time rendering. Conventional schemes do not consider the characteristics of texture content, thus the sharpness of edges in texture images cannot be retained. This paper proposes a novel texture-filtering algorithm, which consists of edge-preserving interpolation and edge-preserving MIP-map prefiltering. The memory bandwidth requirement is kept the same as in conventional schemes by dynamically adjusting the interpolation kernel. Hardware implementation is also provided to show the real-time processing capability.
C1 Natl Chiao Tung Univ, Dept Elect Engn, Hsinchu, Taiwan.
C3 National Yang Ming Chiao Tung University
RP Natl Chiao Tung Univ, Dept Elect Engn, Hsinchu, Taiwan.
EM yzlee@twins.ee.nctu.edu.tw; cwjen@twins.ee.nctu.edu.tw
CR ALGAZI VR, 1991, INT CONF ACOUST SPEE, P3005, DOI 10.1109/ICASSP.1991.151035
   [Anonymous], 1988, Eye, brain, and vision
   BLINN JF, 1976, COMMUN ACM, V19, P542, DOI 10.1145/965143.563322
   Cant RJ, 2000, ACM T GRAPHIC, V19, P164, DOI 10.1145/353981.353991
   Carrato S, 2000, IEEE SIGNAL PROC LET, V7, P132, DOI 10.1109/97.844630
   Chen T, 2000, OPT ENG, V39, P2101, DOI 10.1117/1.1305262
   Crow F. C., 1984, Computers & Graphics, V18, P207
   Decencière E, 2001, SIGNAL PROCESS-IMAGE, V16, P567, DOI 10.1016/S0923-5965(00)00037-0
   DEFEE I, 1991, P IEEE INT C SYST EN, P269
   DEMIRER M, 1994, 7TH MEDITERRANEAN ELECTROTECHNICAL CONFERENCE, VOLS 1-3, P355, DOI 10.1109/MELCON.1994.381080
   Doyle T., 1990, Signal Processing of HDTV, II. Proceedings of the Third International Workshop on HDTV, P711
   Ewins JP, 2000, COMPUT GRAPH-UK, V24, P253, DOI 10.1016/S0097-8493(99)00159-4
   FEIBUSH EA, 1980, SIGGRAPH COMPUT GRAP, V14, P294
   Fournier A., 1988, Computer Graphics, V22, P229, DOI 10.1145/378456.378515
   GANGNET M, 1982, EUROGRAPHICS 82, P57
   GREENE N, 1986, IEEE COMPUT GRAPH, V6, P21, DOI 10.1109/MCG.1986.276658
   Hakura ZS, 1997, ACM COMP AR, P108, DOI 10.1145/384286.264152
   HECKBERT PS, 1986, IEEE COMPUT GRAPH, V6, P56, DOI 10.1109/MCG.1986.276672
   Huttner T., 1999, Proceedings 1999 EUROGRAPHICS/SIGGRAPH Workshop on Graphics Hardware, P35, DOI 10.1145/311534.311572
   JENSEN K, 1995, IEEE T IMAGE PROCESS, V4, P285, DOI 10.1109/83.366477
   Kang DW, 2000, SIGNAL PROCESS-IMAGE, V16, P395, DOI 10.1016/S0923-5965(00)00004-7
   Kuo CJ, 1996, IEEE T CIRC SYST VID, V6, P317, DOI 10.1109/76.499841
   Lee HY, 2000, IEEE T CONSUM ELECTR, V46, P999, DOI 10.1109/30.920453
   LEE MH, 1994, IEEE T CONSUM ELECTR, V40, P119, DOI 10.1109/30.286406
   MCCORMACK J, 1999, SIGGRAPH COMPUT GRAP, V33, P243
   Michaud F, 1997, IEEE T CIRC SYST VID, V7, P539, DOI 10.1109/76.585932
   PAUL B, 2001, MESA 3D GRAPHICS LIB
   SALONEN J, 1994, IEEE T CONSUM ELECTR, V40, P225, DOI 10.1109/30.320799
   Schilling A, 1996, IEEE COMPUT GRAPH, V16, P32, DOI 10.1109/38.491183
   Thurnhofer S, 1996, OPT ENG, V35, P1862, DOI 10.1117/1.600619
   Ting HC, 1997, J VIS COMMUN IMAGE R, V8, P338, DOI 10.1006/jvci.1997.0364
   WILLIAMS GDV, 1983, CLIMATOL B, V17, P3
   Xue K., 1992, Journal of Electronic Imaging, V1, P152, DOI 10.1117/12.55185
   YOU YL, 1996, P ICASSP 1996, P1946
   Yu XH, 2001, IEEE COMPUT GRAPH, V21, P62, DOI 10.1109/38.920628
NR 35
TC 6
Z9 6
U1 0
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2003
VL 19
IS 1
BP 10
EP 22
DI 10.1007/s00371-002-0169-8
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 671MB
UT WOS:000182467600002
DA 2024-07-18
ER

PT J
AU Qian, K
   Pan, YC
   Xu, H
   Tian, L
AF Qian, Kui
   Pan, Yuchen
   Xu, Hao
   Tian, Lei
TI Transformer model incorporating local graph semantic attention for image
   caption
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Transformer-based; Multi-label semantic; Graph convolutional neural
   network; Multi-head attention; Image caption
ID REPRESENTATION; GENERATION
AB Aiming at the problem of isolating semantic information of existing transformer-based models in the image captioning tasks, a transformer model incorporating local graph semantic attention (TLGSA) is proposed. TLGSA consists of a multi-layer image convolutional encoder, a multi-label semantic recognizer, and a multi-layer natural language generation decoder. The image convolutional feature encoder outputs the spatial feature self-attention information, and the multi-label semantic recognizer combines the global corpus with the current image feature to output image semantic graph node encoding features using a graph convolutional neural network (GCN). The natural language decoder incorporates the input semantic self-attention, graph node encoding features, and spatial feature self-attention to form a local semantic multi-head attention, and finally generates a natural language caption of the image. Experimental results show that the proposed method has higher accuracy with meaningful semantic information than the existing SOTA methods, achieving 23%/23.2%/31.57% of Bleu-4 on the widely used test datasets Flickr8K, Flickr30K, and MS COCO, respectively, without reinforcement optimization stage.
C1 [Qian, Kui; Pan, Yuchen; Xu, Hao; Tian, Lei] Nanjing Inst Technol, Sch Automat, Nanjing, Peoples R China.
C3 Nanjing Institute of Technology
RP Qian, K (corresponding author), Nanjing Inst Technol, Sch Automat, Nanjing, Peoples R China.
EM kuiqian@njit.edu.cn
FU Natural Science Foundation Youth Fund of Jiangsu Province of China;
   National Natural Science Foundation of China [61902179];  [BK20210931]
FX This paper is supported by the Natural Science Foundation Youth Fund of
   Jiangsu Province of China (No. BK20210931) and is partially supported by
   the National Natural Science Foundation of China (No. 61902179).
CR Amritkar C, 2018, 2018 FOURTH INTERNATIONAL CONFERENCE ON COMPUTING COMMUNICATION CONTROL AND AUTOMATION (ICCUBEA)
   Bai S, 2018, NEUROCOMPUTING, V311, P291, DOI 10.1016/j.neucom.2018.05.080
   Chen TS, 2022, IEEE T PATTERN ANAL, V44, P1371, DOI 10.1109/TPAMI.2020.3025814
   Chen X, 2015, PROC CVPR IEEE, P2422, DOI 10.1109/CVPR.2015.7298856
   Chen ZM, 2019, PROC CVPR IEEE, P5172, DOI 10.1109/CVPR.2019.00532
   Chowdhary CL, 2019, J APPL SCI ENG, V22, P691, DOI 10.6180/jase.201912_22(4).0011
   Cornia Marcella, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10575, DOI 10.1109/CVPR42600.2020.01059
   Dehmamy N, 2019, ADV NEUR IN, V32
   Deselaers T., 2009, Proceedings of the 4th Workshop on Statistical Machine Translation, P233
   Ding ST, 2019, PATTERN RECOGN LETT, V123, P89, DOI 10.1016/j.patrec.2019.03.021
   Franceschi L, 2019, PR MACH LEARN RES, V97
   Goel Arushi, 2020, Computer Vision - ECCV 2020 Workshops. Proceedings. Lecture Notes in Computer Science (LNCS 12536), P369, DOI 10.1007/978-3-030-66096-3_26
   Hossain MZ, 2019, ACM COMPUT SURV, V51, DOI 10.1145/3295748
   Huang L, 2019, IEEE I CONF COMP VIS, P4633, DOI 10.1109/ICCV.2019.00473
   Ji JY, 2021, AAAI CONF ARTIF INTE, V35, P1655
   Kandala H, 2022, IEEE GEOSCI REMOTE S, V19, DOI 10.1109/LGRS.2022.3198234
   Khademi M, 2018, IEEE COMPUT SOC CONF, P2024, DOI 10.1109/CVPRW.2018.00260
   Khan S, 2022, ACM COMPUT SURV, V54, DOI 10.1145/3505244
   Kiros R, 2014, PR MACH LEARN RES, V32, P595
   Li G, 2019, IEEE I CONF COMP VIS, P8927, DOI 10.1109/ICCV.2019.00902
   Li LH, 2022, PROC CVPR IEEE, P10955, DOI 10.1109/CVPR52688.2022.01069
   Li XR, 2021, PATTERN RECOGN LETT, V141, P68, DOI 10.1016/j.patrec.2020.12.001
   Liu MF, 2020, INFORM PROCESS MANAG, V57, DOI 10.1016/j.ipm.2019.102178
   Liu XX, 2019, VISUAL COMPUT, V35, P445, DOI 10.1007/s00371-018-1566-y
   Luo RC, 2019, 2019 IEEE INTERNATIONAL CONFERENCE ON INDUSTRIAL CYBER PHYSICAL SYSTEMS (ICPS 2019), P827, DOI [10.1109/ICPHYS.2019.8780171, 10.1109/icphys.2019.8780171]
   Ma QW, 2021, 59TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 11TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (ACL-IJCNLP 2021), VOL 1, P3855
   Parikh Harshit, 2020, 2020 3rd International Conference on Communication System, Computing and IT Applications (CSCITA). Proceedings, P174, DOI 10.1109/CSCITA47329.2020.9137802
   Qian K, 2022, NEURAL COMPUT APPL, V34, P2207, DOI 10.1007/s00521-021-06557-8
   Shao Zhuang, 2022, IEEE Trans Neural Netw Learn Syst, VPP, DOI 10.1109/TNNLS.2022.3152990
   Stahlberg F, 2020, J ARTIF INTELL RES, V69, P343, DOI 10.1613/jair.1.12007
   Tan YH, 2019, NEUROCOMPUTING, V333, P86, DOI 10.1016/j.neucom.2018.12.026
   Valentini-Botinhao C., 2016, P ISCA SPEECH SYNTH, P146, DOI DOI 10.21437/SSW.2016-24
   Nguyen VQ, 2022, LECT NOTES COMPUT SC, V13696, P167, DOI 10.1007/978-3-031-20059-5_10
   Vaswani A, 2017, ADV NEUR IN, V30
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   Wang JJ, 2022, INT GEOSCI REMOTE SE, P7996, DOI 10.1109/IGARSS46834.2022.9883199
   Xian TT, 2022, NEURAL NETWORKS, V148, P129, DOI 10.1016/j.neunet.2022.01.011
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Yan J, 2022, INT J MULTIMED INF R, V11, P111, DOI 10.1007/s13735-022-00228-7
   You QZ, 2016, PROC CVPR IEEE, P4651, DOI 10.1109/CVPR.2016.503
   Yu J, 2020, IEEE T CIRC SYST VID, V30, P4467, DOI 10.1109/TCSVT.2019.2947482
   Zhang C, 2020, IEEE J-STSP, V14, P478, DOI 10.1109/JSTSP.2020.2987728
   Zhang Y, 2021, PATTERN RECOGN LETT, V143, P43, DOI 10.1016/j.patrec.2020.12.020
   Zhong JY, 2023, VISUAL COMPUT, V39, P6115, DOI 10.1007/s00371-022-02716-7
   Zhou GD, 2023, IEEE T GEOSCI REMOTE, V61, DOI 10.1109/TGRS.2023.3241331
   Zhou YE, 2021, IEEE INT CONF COMP V, P3132, DOI 10.1109/ICCVW54120.2021.00350
NR 46
TC 0
Z9 0
U1 2
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2023 DEC 7
PY 2023
DI 10.1007/s00371-023-03180-7
EA DEC 2023
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AA2V2
UT WOS:001115674200001
DA 2024-07-18
ER

PT J
AU Fu, PB
   Zhang, X
   Yang, HR
AF Fu, PengBin
   Zhang, Xu
   Yang, HuiRong
TI Answer sheet layout analysis based on YOLOv5s-DC and MSER
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Layout analysis; Object detection; Deep learning; Machine vision;
   Improved YOLO
AB Layout analysis is the first step in automatic grading and other OCR tasks. Although various layout analysis technologies have been developed for different application scenarios, existing approaches still have difficulty in achieving high accuracy on answer sheet analysis. By combining the deep neural network object detection model YOLOv5s and traditional text extraction methods, a layout analysis method for answer sheets is proposed in this paper. The foreground and background of aligned answer sheets are separated by a method that we propose based on the variation in the pixel gradient. Then, a YOLOv5s-DC model is trained to detect the handwriting texts in the answer sheets, and MSER (Maximally Stable Extremal Regions) is applied to the answer sheets to extract the missing parts and background texts. The decoupled YOLO head replaces the original YOLO head for higher performance. Quality focal loss and efficient IoU loss are employed during loss calculation to supervise the classification and regression, respectively. After the detection, the seam carving algorithm and other rules are applied to the bounding boxes to improve the extraction results. The mean average precision of YOLOv5s-DC is 91.6%, which is 3.2% higher than that of YOLOv5s. The pixel accuracy of our text extraction method is 99.92%. The experimental results verify that our method can effectively and accurately analyze the layout of answer sheets.
C1 [Fu, PengBin; Zhang, Xu; Yang, HuiRong] Beijing Univ Technol, Fac Informat Technol, Beijing 100124, Peoples R China.
C3 Beijing University of Technology
RP Yang, HR (corresponding author), Beijing Univ Technol, Fac Informat Technol, Beijing 100124, Peoples R China.
EM fupengbin@bjut.edu.cn; zxgml2333@gmail.com; yanghuirong@bjut.edu.cn
CR Avidan S, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239461
   Bochkovskiy A, 2020, Arxiv, DOI arXiv:2004.10934
   Chen K, 2017, PROC INT CONF DOC, P965, DOI 10.1109/ICDAR.2017.161
   COVER TM, 1967, IEEE T INFORM THEORY, V13, P21, DOI 10.1109/TIT.1967.1053964
   DUDA RO, 1972, COMMUN ACM, V15, P11, DOI 10.1145/361237.361242
   Higgins E., 2003, Learn. Teach. Action, V2
   Hudson L., 2019, Pyzbar
   Jaekyu Ha, 1995, Proceedings of the Third International Conference on Document Analysis and Recognition, P952, DOI 10.1109/ICDAR.1995.602059
   Jobin K., 2022, SN Comput. Sci, V4, P5, DOI [10.1007/s42979-022-01414-4, DOI 10.1007/S42979-022-01414-4]
   Kasar T, 2013, PROC INT CONF DOC, P1185, DOI 10.1109/ICDAR.2013.240
   Kaur RP, 2021, VISUAL COMPUT, V37, P1637, DOI 10.1007/s00371-020-01927-0
   Li X., 2020, ADV NEURAL INF PROCE, V33, P21002, DOI DOI 10.48550/ARXIV.2006.04388
   Liu S, 2018, PROC CVPR IEEE, P8759, DOI 10.1109/CVPR.2018.00913
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Matas J, 2004, IMAGE VISION COMPUT, V22, P761, DOI 10.1016/j.imavis.2004.02.006
   Memon J, 2020, IEEE ACCESS, V8, P142642, DOI 10.1109/ACCESS.2020.3012542
   OGORMAN L, 1993, IEEE T PATTERN ANAL, V15, P1162, DOI 10.1109/34.244677
   Palm RB, 2017, PROC INT CONF DOC, P406, DOI 10.1109/ICDAR.2017.74
   Pavlidis T., 1991, Proceedings of the International Conference on Document Analysis and Recognition, P945
   Ponnusamy M, 2022, J SUPERCOMPUT, V78, P16197, DOI 10.1007/s11227-022-04525-0
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Schreiber S, 2017, PROC INT CONF DOC, P1162, DOI 10.1109/ICDAR.2017.192
   Shelhamer E, 2017, IEEE T PATTERN ANAL, V39, P640, DOI 10.1109/TPAMI.2016.2572683
   Song Y, 2017, MACH VISION APPL, V28, P755, DOI 10.1007/s00138-017-0837-3
   Truong TN, 2020, INT CONF FRONT HAND, P181, DOI 10.1109/ICFHR2020.2020.00042
   WAHL FM, 1982, COMPUT VISION GRAPH, V20, P375, DOI 10.1016/0146-664X(82)90059-4
   Wang CY, 2023, PROC CVPR IEEE, P7464, DOI 10.1109/CVPR52729.2023.00721
   Xu SL, 2022, Arxiv, DOI [arXiv:2203.16250, 10.48550/arXiv.2203.16250]
   Yepes AJ, 2021, LECT NOTES COMPUT SC, V12824, P605, DOI 10.1007/978-3-030-86337-1_40
   Young T, 2018, IEEE COMPUT INTELL M, V13, P55, DOI 10.1109/MCI.2018.2840738
   Zhang J, 2019, IEEE T MULTIMEDIA, V21, P221, DOI 10.1109/TMM.2018.2844689
   Zhang YF, 2022, NEUROCOMPUTING, V506, P146, DOI 10.1016/j.neucom.2022.07.042
   Zhang Z., 2022, IEEE Trans. Multimed.
NR 35
TC 0
Z9 0
U1 25
U2 28
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2023 NOV 25
PY 2023
DI 10.1007/s00371-023-03156-7
EA NOV 2023
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA Y6UT1
UT WOS:001106597500003
DA 2024-07-18
ER

PT J
AU Tang, JX
   Wang, XN
   Yang, XL
   Wen, Y
   Qian, B
   Chen, TL
   Sheng, B
AF Tang, Jixue
   Wang, Xiang-ning
   Yang, Xiaolong
   Wen, Yang
   Qian, Bo
   Chen, Tingli
   Sheng, Bin
TI TSNet: Task-specific network for joint diabetic retinopathy grading and
   lesion segmentation of ultra-wide optical coherence tomography
   angiography images
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Diabetic retinopathy grading; Lesion segmentation; Ultra-wide optical
   coherence tomography angiography; Multi-task learning network
ID MACULAR EDEMA; RANIBIZUMAB
AB Diabetic retinopathy (DR) is a common complication of diabetes which may lead to blindness. Early diagnosis can effectively prevent the deterioration of the disease and enable timely treatment. Ophthalmologists diagnose DR by observing ultra-wide optical coherence tomography angiography (UW-OCTA) images, which visualize unprecedented detail of DR lesions. In this paper, we propose an end-to-end task-specific network (TSNet) for joint DR grading and lesion segmentation of UW-OCTA images. Specifically, we design task-specific attention block to generate task-specific feature maps for respective segmentation and classification tasks. Furthermore, we devise task-specific fusion block to fuse the original task-specific feature map and augmented task-specific feature map for the following segmentation and classification decoders to generate DR lesion predictive mask and DR grading predictive result. Experiments on a public-available UW-OCTA dataset demonstrate that our model outperforms state-of-the-art (SOTA) multi-task models and achieves promising results on both DR lesion segmentation and DR grading classification tasks
C1 [Tang, Jixue; Qian, Bo; Sheng, Bin] Shanghai Jiao Tong Univ, Dept Comp Sci & Engn, Shanghai, Peoples R China.
   [Yang, Xiaolong; Chen, Tingli] Huadong Sanat, Wuxi, Jiangsu, Peoples R China.
   [Wang, Xiang-ning] Shanghai Jiao Tong Univ, Shanghai Peoples Hosp 6, Sch Med, Dept Ophthalmol, Shanghai, Peoples R China.
   [Wen, Yang] Shenzhen Univ, Sch Elect & Informat Engn, Shenzhen, Peoples R China.
C3 Shanghai Jiao Tong University; Shanghai Jiao Tong University; Shenzhen
   University
RP Sheng, B (corresponding author), Shanghai Jiao Tong Univ, Dept Comp Sci & Engn, Shanghai, Peoples R China.; Wen, Y (corresponding author), Shenzhen Univ, Sch Elect & Informat Engn, Shenzhen, Peoples R China.
EM tang.jixue@sjtu.edu.cn; xnwang21@163.com; wen_yang@szu.edu.cn;
   shengbin@sjtu.edu.cn
RI Chen, Jin/KBQ-0163-2024; Chen, Ting-Li/AAF-5752-2020; zhang,
   xinyi/JWA-0980-2024; Wang, Yanan/JVZ-7957-2024
OI Chen, Jin/0009-0005-5844-635X; 
FU National Key Research and Development Program of China [2022YFC2407000];
   Interdisciplinary Program of Shanghai Jiao Tong University [YG2023LC11,
   YG2022ZD007]; National Natural Science Foundation of China [62272298,
   62077037]; College-level Project Fund of Shanghai Jiao Tong University
   Affiliated Sixth People's Hospital [ynlc201909]; Medical industrial
   Cross-fund of Shanghai Jiao Tong University [YG2022QN089]; National
   Science Foundation of China [62101346, 62301330]; Guangdong Basic and
   Applied Basic Research Foundation [2021A1515011702, 2022A1515110101];
   Shaanxi Provincial Department of Education Special Scientific Research
   Project [20JK0613]; Clinical Special Program of Shanghai Municipal
   Health Commission [20224044]
FX This work was supported in part by the National Key Research and
   Development Program of China under grant number 2022YFC2407000, in part
   by the Interdisciplinary Program of Shanghai Jiao Tong University under
   grant number YG2023LC11 and YG2022ZD007, in part by National Natural
   Science Foundation of China under grant number 62272298 and 62077037, in
   part by the College-level Project Fund of Shanghai Jiao Tong University
   Affiliated Sixth People's Hospital under grant number ynlc201909, and in
   part by the Medical industrial Cross-fund of Shanghai Jiao Tong
   University under the grant number YG2022QN089. This work was supported
   in part by the National Science Foundation of China under Grants
   62101346 and 62301330, in part by the Guangdong Basic and Applied Basic
   Research Foundation under Grants 2021A1515011702 and 2022A1515110101 ,in
   part by the Shaanxi Provincial Department of Education Special
   Scientific Research Project under Grants 20JK0613,in part by the
   Clinical Special Program of Shanghai Municipal Health Commission under
   Grants 20224044, and in part by the 3-year action plan to strengthen the
   construction of public health system in Shanghai 2023-2025 GWVI-11.1-28.
CR Antonetti DA, 2012, NEW ENGL J MED, V366, P1227, DOI 10.1056/NEJMra1005073
   Chandrasekaran R, 2023, VISUAL COMPUT, V39, P2741, DOI 10.1007/s00371-022-02489-z
   Chen GH, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0115797
   Dai L, 2021, NAT COMMUN, V12, DOI 10.1038/s41467-021-23458-5
   de Carlo Talisa E, 2015, Int J Retina Vitreous, V1, P5
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Elman MJ, 2010, OPHTHALMOLOGY, V117, P1064, DOI 10.1016/j.ophtha.2010.02.031
   Gao Z., 2022, Diabetic Retinal Overlap Lesion Segmentation Network[M]//MICCAI Challenge on Mitosis Domain Generalization, P38
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hou J., 2022, Deep-OCTA: Ensemble Deep Learning Approaches for Diabetic Retinopathy Analysis on OCTA Images[M]//MICCAI Challenge on Mitosis Domain Generalization, P74
   Hwang TS, 2015, RETINA-J RET VIT DIS, V35, P2371, DOI 10.1097/IAE.0000000000000716
   Imran A, 2021, VISUAL COMPUT, V37, P2407, DOI 10.1007/s00371-020-01994-3
   Jha D, 2019, IEEE INT SYM MULTIM, P225, DOI 10.1109/ISM46123.2019.00049
   Jiang Z, 2020, BIOMED OPT EXPRESS, V11, P1580, DOI 10.1364/BOE.387807
   Kang QB, 2022, MED IMAGE ANAL, V79, DOI 10.1016/j.media.2022.102443
   Kendall A, 2018, PROC CVPR IEEE, P7482, DOI 10.1109/CVPR.2018.00781
   Kim G, 2022, SCI REP-UK, V12, DOI 10.1038/s41598-022-05281-0
   Klein BEK, 2007, OPHTHAL EPIDEMIOL, V14, P179, DOI 10.1080/09286580701396720
   Le D, 2021, EXP BIOL MED, V246, P2170, DOI 10.1177/15353702211026581
   Le D, 2020, TRANSL VIS SCI TECHN, V9, DOI 10.1167/tvst.9.2.35
   Lee R, 2015, EYE VISION, V2, DOI 10.1186/s40662-015-0026-2
   Li XR, 2023, VISUAL COMPUT, V39, P1307, DOI 10.1007/s00371-022-02407-3
   Lin SQ, 2023, VISUAL COMPUT, V39, P3259, DOI 10.1007/s00371-023-02945-4
   Lin Tsung-Yi, 2020, IEEE Trans Pattern Anal Mach Intell, V42, P318, DOI 10.1109/TPAMI.2018.2858826
   Liu X, 2019, J BIOPHOTONICS, V12, DOI 10.1002/jbio.201900008
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Ma J, 2021, MED IMAGE ANAL, V71, DOI 10.1016/j.media.2021.102035
   Massin P, 2010, DIABETES CARE, V33, P2399, DOI 10.2337/dc10-0493
   Michaelides M, 2010, OPHTHALMOLOGY, V117, P1078, DOI 10.1016/j.ophtha.2010.03.045
   Islam SMS, 2018, Arxiv, DOI arXiv:1812.10595
   Prescott G, 2014, BRIT J OPHTHALMOL, V98, P1042, DOI 10.1136/bjophthalmol-2013-304338
   Qian B, 2023, Arxiv, DOI arXiv:2304.02389
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Ryu G, 2021, SCI REP-UK, V11, DOI 10.1038/s41598-021-02479-6
   Safi H, 2018, SURV OPHTHALMOL, V63, P601, DOI 10.1016/j.survophthal.2018.04.003
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Singh Neera., 2010, Int. J. Comput. Appl, V8, P18, DOI [DOI 10.5120/1186-1648, 10.5120/1186-1648]
   Spaide RF, 2015, RETINA-J RET VIT DIS, V35, P2161, DOI 10.1097/IAE.0000000000000881
   Sultana F., 2022, Automatic Diabetic Retinopathy Lesion Segmentation in UW-OCTA Images Using Transfer Learning[M]//MICCAI Challenge on Mitosis Domain Generalization, P186
   Tan ACS, 2018, EYE, V32, P262, DOI 10.1038/eye.2017.181
   Vaswani A, 2017, ADV NEUR IN, V30
   Vieira-Potter VJ, 2016, BIOMED RES INT, V2016, DOI 10.1155/2016/3801570
   Wang JF, 2022, LECT NOTES COMPUT SC, V13433, P110, DOI 10.1007/978-3-031-16437-8_11
   Wang PY, 2018, LECT NOTES COMPUT SC, V11073, P134, DOI 10.1007/978-3-030-00937-3_16
   Xiao HG, 2023, VISUAL COMPUT, V39, P2291, DOI 10.1007/s00371-022-02414-4
   Xie EZ, 2021, ADV NEUR IN, V34
   Yang QH, 2019, INT J OPHTHALMOL-CHI, V12, P302, DOI 10.18240/ijo.2019.02.19
   Zang P, 2022, TRANSL VIS SCI TECHN, V11, DOI 10.1167/tvst.11.7.10
   Zhou Y, 2021, MED IMAGE ANAL, V70, DOI 10.1016/j.media.2020.101918
NR 50
TC 1
Z9 1
U1 8
U2 10
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2023 NOV 21
PY 2023
DI 10.1007/s00371-023-03145-w
EA NOV 2023
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AK3K7
UT WOS:001118319300001
DA 2024-07-18
ER

PT J
AU Castro, Y
   Le Goic, G
   Chatoux, H
   De Luca, L
   Mansouri, A
AF Castro, Yuly
   Le Goic, Gaetan
   Chatoux, Hermine
   De Luca, Livio
   Mansouri, Alamin
TI A new pixel-wise data processing method for reflectance transformation
   imaging
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Reflectance transformation imaging; Illumination uniformity; Multi-light
   image collections; Pixel-wise
AB Reflectance transformation imaging (RTI) is one of the most widely used techniques in order to digitize and analyze material appearance of a surface, finding a great level of utility and applicability in the field of cultural heritage as well as in industrial applications. To date, most of the methods used to process (model and relight) RTI data assume only one light direction for all pixels as well as a single light source-surface distance for the entire image, following the model of a very distant (far) light source. This assumption does not hold in practice. Indeed, the light sources commonly used in RTI acquisitions (spotlight/photo flash) induce to a non-uniform illumination of the surface. This is caused by the variation of incidence angles and per-point distances which directly affect the amount of light received by the surface. We propose a novel pixel-wise methodology for improving lighting based on illumination laws that allows one to correct both loss of energy due to the distance variation as well as the elevation angle. We show the efficiency of the proposed method on RTI acquisitions performed on cultural heritage objects and a manufactured surface. We show that our method corrects the effects of non-uniform illumination and leads to improve the relighting commonly associated with RTI.
C1 [Castro, Yuly; Le Goic, Gaetan; Chatoux, Hermine; Mansouri, Alamin] UBFC, ImViA Lab, F-21000 Dijon, France.
   [De Luca, Livio] UMR CNRS MC 3495 MAP, Marseille, France.
C3 Universite de Bourgogne
RP Castro, Y (corresponding author), UBFC, ImViA Lab, F-21000 Dijon, France.
EM yuly.castro.ct@gmail.com
OI De Luca, Livio/0000-0003-0656-3165
FU French National Research Agency (ANR) [CE38-004]
FX This work benefited of the funding of French National Research Agency
   (ANR) through the Project CE38-004 SUMUM (www.anr-sumum.fr). Authors
   also thanks the Rijks Museum for providing the modern daguerreotype.
CR [Anonymous], 2006, 7 INT S VIRT REAL AR, DOI DOI 10.2312/VAST/VAST06/195-202
   Castro Y, 2020, J ELECTRON IMAGING, V29, DOI 10.1117/1.JEI.29.4.041004
   Dulecha TG, 2020, VISUAL COMPUT, V36, P2161, DOI 10.1007/s00371-020-01910-9
   Earl G., 2011, Electronic Visualisation and the Arts (EVA) 2011, P147, DOI [10.14236/ewic/EVA2011.27, DOI 10.14236/EWIC/EVA2011.27]
   Einarsson P., 2004, ACM SIGGRAPH SKETCHE, P81
   Gautron Pascal., 2004, EUROGRAPHICS SYMPOSI, P321
   Giachetti A, 2018, COMPUT VIS IMAGE UND, V168, P118, DOI 10.1016/j.cviu.2017.05.014
   Huang X, 2015, 2015 DIGITAL HERITAGE INTERNATIONAL CONGRESS, VOL 1: DIGITIZATION & ACQUISITION, COMPUTER GRAPHICS & INTERACTION, P215, DOI 10.1109/DigitalHeritage.2015.7413874
   IKEUCHI K, 1981, IEEE T PATTERN ANAL, V3, P661, DOI 10.1109/TPAMI.1981.4767167
   Iwahori Y., 1990, Proceedings. 10th International Conference on Pattern Recognition (Cat. No.90CH2898-5), P83, DOI 10.1109/ICPR.1990.118069
   Krátky V, 2020, IEEE ROBOT AUTOM LET, V5, P2302, DOI 10.1109/LRA.2020.2970646
   Kurt M., 2009, SIGGRAPH Computer Graphics, V43, P1, DOI DOI 10.1145/1629216.1629222
   Kurt M., 2018, J. Sci. Eng, V20, P87
   Lam PM, 2012, IET IMAGE PROCESS, V6, P72, DOI 10.1049/iet-ipr.2009.0134
   Luxman R, 2022, J IMAGING, V8, DOI 10.3390/jimaging8050134
   MacDonald L., 2018, Heritage Preservation
   Malzbender T, 2001, COMP GRAPH, P519, DOI 10.1145/383259.383320
   Masselus V., 2022, ACM SIGGRAPH 2002 C, P262
   McGuigan M, 2020, COMPUT VIS IMAGE UND, V192, DOI 10.1016/j.cviu.2019.102880
   Nurit M., 2021, 15 INT C QUALI CONTR, V11794, P111
   Nurit M, 2021, COMPUT IND, V132, DOI 10.1016/j.compind.2021.103500
   Pamart A, 2019, INT ARCH PHOTOGRAMM, V42-2, P573, DOI 10.5194/isprs-archives-XLII-2-W9-573-2019
   Pintus R., 2019, COMPUT GRAPH FORUM, P3
   Pitard G, 2017, IEEE IMAGE PROC, P445, DOI 10.1109/ICIP.2017.8296320
   Pitard G, 2017, MACH VISION APPL, V28, P607, DOI 10.1007/s00138-017-0856-0
   Ponchio F., 2022, P 23 INT ACM C 3D WE, P1
   Quéau Y, 2018, J MATH IMAGING VIS, V60, P313, DOI 10.1007/s10851-017-0761-1
   Rabascall I., 2003, Uncalibrated photometric stereo for 3 d surface texture recovery
   Santo H., 2018, EUR C COMP VIS, P137
   Tongbuasirilai T, 2020, VISUAL COMPUT, V36, P855, DOI 10.1007/s00371-019-01664-z
   Tunwattanapong B, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461944
   Winnemöller H, 2005, COMPUT GRAPH FORUM, V24, P433, DOI 10.1111/j.1467-8659.2005.00868.x
   WOODHAM RJ, 1980, OPT ENG, V19, P139, DOI 10.1117/12.7972479
NR 33
TC 0
Z9 0
U1 8
U2 9
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2023 OCT 13
PY 2023
DI 10.1007/s00371-023-03105-4
EA OCT 2023
PG 21
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA U7HZ0
UT WOS:001086491500001
DA 2024-07-18
ER

PT J
AU Huang, LW
   Liao, SJ
   Yang, WY
AF Huang, Liwen
   Liao, Shujiao
   Yang, Wenyuan
TI DC-PSENet: a novel scene text detection method integrating double
   ResNet-based and changed channels recursive feature pyramid
SO VISUAL COMPUTER
LA English
DT Article
DE Computer vision; Scene text detection; Multi-scale; Feature pyramid
   network; Composite architectures
AB Due to the emergence and advancement of deep learning technologies, scene text detection is becoming more widespread in various fields. However, due to the complexity of distances, angles and backgrounds, the adjacent texts in images have the problem that the detection boxes are far away from the texts, i.e., a position is not accurate enough. In this paper, we propose a text detection method centered on double ResNet-based and changed channels recursive feature pyramid, which integrates ResNet50-Mish and Res2Net50-Mish, as well as using recursive feature pyramid with changed channels. Firstly, scene images are fed into ResNet50-Mish and Res2Net50-Mish of double ResNet-based, and results are passed through a weight-based addition step to generate the fused feature maps. Secondly, the processed feature maps of double ResNet-based are sent into changed channels recursive feature pyramid to obtain feature maps with enhanced feature information. Also, the relevant segmentation results are then obtained by concatenating and convoluting. Finally, the results are given to progressive scale expansion algorithm to output the location of texts in images. The proposed model is trained and tested on ICDAR15 and CTW1500 benchmark datasets. In terms of precision values, our method outperforms or is comparable to state-of-the-art methods. In particular, experimental results achieve 91.53% precision on ICDAR15 dataset and 84.89% precision on CTW-1500 dataset.
C1 [Huang, Liwen; Liao, Shujiao; Yang, Wenyuan] Minnan Normal Univ, Fujian Key Lab Granular Comp & Applicat, Zhangzhou 363000, Fujian, Peoples R China.
   [Huang, Liwen; Liao, Shujiao; Yang, Wenyuan] Minnan Normal Univ, Sch Math & Stat, Zhangzhou 363000, Fujian, Peoples R China.
C3 MinNan Normal University; MinNan Normal University
RP Yang, WY (corresponding author), Minnan Normal Univ, Fujian Key Lab Granular Comp & Applicat, Zhangzhou 363000, Fujian, Peoples R China.; Yang, WY (corresponding author), Minnan Normal Univ, Sch Math & Stat, Zhangzhou 363000, Fujian, Peoples R China.
EM huanglw202212@163.com; sjliao2011@163.com; yangwycn@gmail.com
FU This study was supported by the National Natural Science Foundation of
   China under Grant No. 12101289, the Natural Science Foundation of Fujian
   Province under Grant Nos. 2020J01821 and 2022J01891, the Institute of
   Meteorological Big Data-Digital Fujian, an [12101289]; National Natural
   Science Foundation of China [2020J01821, 2022J01891]; Natural Science
   Foundation of Fujian Province; Institute of Meteorological Big
   Data-Digital Fujian; Fujian Key Laboratory of Data Science and
   Statistics (Minnan Normal University), China
FX This study was supported by the National Natural Science Foundation of
   China under Grant No. 12101289, the Natural Science Foundation of Fujian
   Province under Grant Nos. 2020J01821 and 2022J01891, the Institute of
   Meteorological Big Data-Digital Fujian, and Fujian Key Laboratory of
   Data Science and Statistics (Minnan Normal University), China.
CR Chaung H.-H., 2021, 2021 IEEE INT C CONS, P1, DOI [10.1109/ICCE-TW52618.2021.9603182, DOI 10.1109/ICCE-TW52618.2021.9603182]
   Chen Y., 2020, arXiv
   Deng D, 2018, AAAI CONF ARTIF INTE, P6773
   GABBASOV R, 2020, 2020 INT C INFORM TE, P1, DOI DOI 10.1109/ITNT49337.2020.9253219
   Gao SH, 2021, IEEE T PATTERN ANAL, V43, P652, DOI 10.1109/TPAMI.2019.2938758
   Graves A, 2005, NEURAL NETWORKS, V18, P602, DOI 10.1016/j.neunet.2005.06.042
   He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He MH, 2021, PROC CVPR IEEE, P8809, DOI 10.1109/CVPR46437.2021.00870
   Kang J., 2022, 2022 14 INT C MEAS T, P661, DOI [10.1109/ICMTMA54903.2022.00137, DOI 10.1109/ICMTMA54903.2022.00137]
   Karatzas D, 2015, PROC INT CONF DOC, P1156, DOI 10.1109/ICDAR.2015.7333942
   Kim K.-H, 2016, ARXIV
   Lee JJ, 2011, PROC INT CONF DOC, P429, DOI 10.1109/ICDAR.2011.93
   Li G., 2022, PROC 3 INT C COMPUT, P1217, DOI [10.1109/CVIDLICCEA56201.2022.9824815, DOI 10.1109/CVIDLICCEA56201.2022.9824815]
   Liao M., 2016, ARXIV
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu B., 2022, 2022 IEEE AS PAC C I, P912, DOI [10.1109/IPEC54454.2022.9777406, DOI 10.1109/IPEC54454.2022.9777406]
   Liu H, 2022, VISUAL COMPUT, V38, P3231, DOI 10.1007/s00371-022-02570-7
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Liu YD, 2020, AAAI CONF ARTIF INTE, V34, P11653
   Liu Yun, 2017, arXiv
   Liu ZY, 2019, ACM T ALGORITHMS, V15, DOI 10.1145/3264434
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Long SB, 2021, INT J COMPUT VISION, V129, DOI 10.1007/s11263-020-01369-0
   Long SB, 2018, LECT NOTES COMPUT SC, V11206, P19, DOI 10.1007/978-3-030-01216-8_2
   Ma N., 2020, arXiv
   Misra D., 2019, ARXIV190808681
   Nair V., 2010, P 27 INT C MACHINE L, P807
   Perepu PK, 2021, NEUROCOMPUTING, V431, P1, DOI 10.1016/j.neucom.2020.12.054
   Qiao S., 2020, ARXIV
   Rainarli E, 2021, COMPUT SCI REV, V42, DOI 10.1016/j.cosrev.2021.100434
   Raisi Z., 2020, ARXIV
   Ramachandran P., 2017, ARXIV
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Shi BG, 2017, PROC CVPR IEEE, P3482, DOI 10.1109/CVPR.2017.371
   Shinde Anuja, 2021, Proceedings of the 2021 International Conference on Artificial Intelligence and Smart Systems (ICAIS), P961, DOI 10.1109/ICAIS50930.2021.9395776
   Shrivastava A, 2016, PROC CVPR IEEE, P761, DOI 10.1109/CVPR.2016.89
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song S, 2024, VISUAL COMPUT, V40, P849, DOI 10.1007/s00371-023-02820-2
   Tang J, 2019, PATTERN RECOGN, V96, DOI 10.1016/j.patcog.2019.06.020
   Tang YC, 2023, ENG STRUCT, V274, DOI 10.1016/j.engstruct.2022.115158
   Tang YC, 2023, EXPERT SYST APPL, V211, DOI 10.1016/j.eswa.2022.118573
   Tang YC, 2022, STRUCTURES, V37, P426, DOI 10.1016/j.istruc.2021.12.055
   Tasyürek M, 2024, VISUAL COMPUT, V40, P983, DOI 10.1007/s00371-023-02827-9
   Tian Z, 2016, LECT NOTES COMPUT SC, V9912, P56, DOI 10.1007/978-3-319-46484-8_4
   Wang WH, 2022, IEEE T PATTERN ANAL, V44, P5349, DOI 10.1109/TPAMI.2021.3077555
   Wang WH, 2019, IEEE I CONF COMP VIS, P8439, DOI 10.1109/ICCV.2019.00853
   Wang WH, 2019, PROC CVPR IEEE, P9328, DOI 10.1109/CVPR.2019.00956
   Wang XY, 2022, APPL INTELL, V52, P14374, DOI 10.1007/s10489-021-03065-z
   Wu Q, 2022, APPL INTELL, V52, P514, DOI 10.1007/s10489-021-02331-4
   Ye M., 2023, P AAAI C ART INT
   Ye QX, 2015, IEEE T PATTERN ANAL, V37, P1480, DOI 10.1109/TPAMI.2014.2366765
   Ye QX, 2005, IMAGE VISION COMPUT, V23, P565, DOI 10.1016/j.imavis.2005.01.004
   Zhang CQ, 2019, PROC CVPR IEEE, P10544, DOI 10.1109/CVPR.2019.01080
   Zhou XY, 2017, PROC CVPR IEEE, P2642, DOI 10.1109/CVPR.2017.283
   Zhu YY, 2016, FRONT COMPUT SCI-CHI, V10, P19, DOI 10.1007/s11704-015-4488-0
   Zhu YQ, 2021, PROC CVPR IEEE, P3122, DOI 10.1109/CVPR46437.2021.00314
NR 57
TC 1
Z9 1
U1 7
U2 16
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2024
VL 40
IS 6
BP 4473
EP 4491
DI 10.1007/s00371-023-03093-5
EA SEP 2023
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TV2X4
UT WOS:001073931700001
DA 2024-07-18
ER

PT J
AU Tian, Q
   Du, XX
AF Tian, Qing
   Du, Xiaoxin
TI A plug-and-play noise-label correction framework for unsupervised domain
   adaptation person re-identification
SO VISUAL COMPUTER
LA English
DT Article
DE Person re-identification; Unsupervised domain adaptation; Noise label
   correction; Deep neural networks
ID REFINEMENT
AB Unsupervised domain adaptation person re-identification (UDA ReID) aims at leveraging knowledge from the source domain to help perform ReID in the unlabeled target domain. Most of existing investigations usually assign target instances identification labels through clustering them into the source person identification patterns. Unfortunately, inaccurate labels are frequently generated in such clustering setting, which undesirably deteriorates the accuracy of UDA ReID. Although a variety of noise label correcting works have been published to attempt addressing these misclustered labels, their effectiveness critically depends on the quality of the generated clustering representations, especially their differentiability, tending to result in limited efficacy. In this work, we design a desirable plug-and-play noise-label correction (PP-NLC) framework to efficiently correct the predicted target domain noise labels. Specifically, in PP-NLC we construct a noise label corrector (NLCr) and treat the predicted target domain noise labels as its probabilistic label variables (PLVs). Consequently, these noise labels are implicitly and automatically corrected through explicitly updating PLVs in the back-propagation process of NLCr, instead of conducting corrections directly on the generated noise pseudo-labels. Notably, the proposed PP-NLC framework enjoys desirable universality and can be deployed to existing UDA ReID approaches. Comprehensive experiments and analyses show the effectiveness of the proposed method.
C1 [Tian, Qing; Du, Xiaoxin] Nanjing Univ Informat Sci & Technol, Sch Comp & Software, Nanjing 210044, Peoples R China.
   [Tian, Qing] Nanjing Univ Informat Sci & Technol, Engn Res Ctr Digital Forens, Minist Educ, Nanjing 210044, Peoples R China.
   [Tian, Qing] Nanjing Univ, State Key Lab Novel Software Technol, Nanjing 210023, Peoples R China.
C3 Nanjing University of Information Science & Technology; Nanjing
   University of Information Science & Technology; Nanjing University
RP Tian, Q (corresponding author), Nanjing Univ Informat Sci & Technol, Sch Comp & Software, Nanjing 210044, Peoples R China.; Tian, Q (corresponding author), Nanjing Univ Informat Sci & Technol, Engn Res Ctr Digital Forens, Minist Educ, Nanjing 210044, Peoples R China.; Tian, Q (corresponding author), Nanjing Univ, State Key Lab Novel Software Technol, Nanjing 210023, Peoples R China.
EM tianqing@nuist.edu.cn
RI tian, qing/JMQ-8820-2023
FU This work was supported by the National Natural Science Foundation of
   China under Grant 62176128, the Open Projects Program of State Key
   Laboratory for Novel Software Technology of Nanjing University under
   Grant KFKT2022B06, the Fundamental Research Funds [62176128]; National
   Natural Science Foundation of China [KFKT2022B06]; Open Projects Program
   of State Key Laboratory for Novel Software Technology of Nanjing
   University [NJ2022028]; Fundamental Research Funds for the Central
   Universities; Priority Academic Program Development of Jiangsu Higher
   Education Institutions (PAPD) fund; Qing Lan Project
FX This work was supported by the National Natural Science Foundation of
   China under Grant 62176128, the Open Projects Program of State Key
   Laboratory for Novel Software Technology of Nanjing University under
   Grant KFKT2022B06, the Fundamental Research Funds for the Central
   Universities No. NJ2022028, the Project Funded by the Priority Academic
   Program Development of Jiangsu Higher Education Institutions (PAPD)
   fund, as well as the Qing Lan Project.
CR Chen C, 2020, AAAI CONF ARTIF INTE, V34, P3422
   Chen X, 2021, AUTOPHAGY, V17, P2054, DOI 10.1080/15548627.2020.1810918
   Chen YB, 2019, IEEE I CONF COMP VIS, P232, DOI 10.1109/ICCV.2019.00032
   Chuanchen Luo, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12360), P224, DOI 10.1007/978-3-030-58555-6_14
   Dai YX, 2021, IEEE T IMAGE PROCESS, V30, P7815, DOI 10.1109/TIP.2021.3104169
   Dai Z., 2022, P AS C COMP VIS ACCV, P1142
   Dongkai Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10978, DOI 10.1109/CVPR42600.2020.01099
   Fang Zhao, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P526, DOI 10.1007/978-3-030-58621-8_31
   Feng H, 2021, IEEE T IMAGE PROCESS, V30, P2898, DOI 10.1109/TIP.2021.3056212
   Fu DP, 2021, PROC CVPR IEEE, P14745, DOI 10.1109/CVPR46437.2021.01451
   Fu Y, 2019, IEEE I CONF COMP VIS, P6111, DOI 10.1109/ICCV.2019.00621
   Ge Y., 2020, ARXIV
   He T, 2022, AAAI CONF ARTIF INTE, P879
   Pham H, 2021, PROC CVPR IEEE, P11552, DOI 10.1109/CVPR46437.2021.01139
   Jiang Z., 2020, arXiv
   Jianing Li, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12369), P483, DOI 10.1007/978-3-030-58586-0_29
   Kang GL, 2019, PROC CVPR IEEE, P4888, DOI 10.1109/CVPR.2019.00503
   Khosla Prannay, 2020, ADV NEURAL INFORM PR, V33, P18661
   Li YJ, 2019, IEEE I CONF COMP VIS, P7918, DOI 10.1109/ICCV.2019.00801
   Liu JW, 2019, PROC CVPR IEEE, P7195, DOI 10.1109/CVPR.2019.00737
   Luo H, 2020, IEEE T MULTIMEDIA, V22, P2597, DOI 10.1109/TMM.2019.2958756
   Mekhazni Djebril, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12372), P159, DOI 10.1007/978-3-030-58583-9_10
   Ren MY, 2018, PR MACH LEARN RES, V80
   Ristani E, 2016, LECT NOTES COMPUT SC, V9914, P17, DOI 10.1007/978-3-319-48881-3_2
   Sun WC, 2019, PROCEEDINGS OF THE 2019 5TH INTERNATIONAL CONFERENCE ON COMPUTER AND TECHNOLOGY APPLICATIONS (ICCTA 2019), P117, DOI 10.1145/3323933.3324091
   Tanaka D, 2018, PROC CVPR IEEE, P5552, DOI 10.1109/CVPR.2018.00582
   Tian Q, 2023, ACM T INTEL SYST TEC, V14, DOI 10.1145/3570510
   Tian Q, 2022, IEEE T CIRC SYST VID, V32, P8562, DOI 10.1109/TCSVT.2022.3192135
   Tian Q, 2022, IEEE T CYBERNETICS, V52, P10328, DOI 10.1109/TCYB.2021.3070545
   Wang WH, 2022, IEEE T IMAGE PROCESS, V31, P1532, DOI 10.1109/TIP.2022.3140614
   Wei LH, 2018, PROC CVPR IEEE, P79, DOI 10.1109/CVPR.2018.00016
   Wu AC, 2019, IEEE I CONF COMP VIS, P6921, DOI 10.1109/ICCV.2019.00702
   Yang QZ, 2019, PROC CVPR IEEE, P3628, DOI 10.1109/CVPR.2019.00375
   Yang Zou, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P87, DOI 10.1007/978-3-030-58536-5_6
   Yixiao Ge, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P369, DOI 10.1007/978-3-030-58548-8_22
   Yu HX, 2019, PROC CVPR IEEE, P2143, DOI 10.1109/CVPR.2019.00225
   Yunpeng Zhai, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9018, DOI 10.1109/CVPR42600.2020.00904
   Zhai Y, 2020, COMPUTER VISION ECCV, P594, DOI DOI 10.1007/978-3-030-58571-6_35
   Zhang M., 2022, IEEE Trans. Neural Netw. Learn. Syst.
   Zhang MJ, 2023, IEEE T GEOSCI REMOTE, V61, DOI 10.1109/TGRS.2023.3263848
   Zhang MJ, 2023, IEEE T CYBERNETICS, V53, P578, DOI 10.1109/TCYB.2022.3163294
   Zhang XY, 2019, IEEE I CONF COMP VIS, P8221, DOI 10.1109/ICCV.2019.00831
   Zheng KC, 2021, PROC CVPR IEEE, P5306, DOI 10.1109/CVPR46437.2021.00527
   Zheng KC, 2021, AAAI CONF ARTIF INTE, V35, P3538
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zhong Z, 2019, PROC CVPR IEEE, P598, DOI 10.1109/CVPR.2019.00069
   Zhou F, 2021, IEEE ACCESS, V9, P129490, DOI 10.1109/ACCESS.2021.3090057
NR 47
TC 1
Z9 1
U1 5
U2 7
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2024
VL 40
IS 6
BP 4493
EP 4504
DI 10.1007/s00371-023-03094-4
EA SEP 2023
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TV2X4
UT WOS:001069084600001
DA 2024-07-18
ER

PT J
AU Zhu, M
   Zhao, C
   Wang, N
   Gu, FY
   Liu, Y
   Li, X
AF Zhu, Ming
   Zhao, Chen
   Wang, Nian
   Gu, Feiyang
   Liu, Yu
   Li, Xin
TI Domain-aware double attention network for zero-shot sketch-based image
   retrieval with similarity loss
SO VISUAL COMPUTER
LA English
DT Article
DE Zero-shot sketch-based image retrieval; Domain-aware double attention
   network; Similarity loss; Center loss
AB Zero-shot sketch-based image retrieval (ZS-SBIR) has attracted increasing attention in computer vision, with the aim of searching the natural images that match a given sketch under the setting of zero-shot learning. In this paper, we propose a novel ZS-SBIR method based on the specifically developed domain-aware dual attention (DADA) module and similarity loss, which utilizes prior knowledge more fully than previous work. The DADA module emphasizes the importance of different channels and spaces according to the prior knowledge of whether the input is a sketch or a natural image. To preserve more prior knowledge from the models pre-trained on ImageNet, a similarity loss function is designed to supervise inter-class similarity. Furthermore, the center loss is introduced into our model, which can bridge the representation gap between the sketch domain and the natural image domain. We perform experiments on the TU-Berlin extended dataset and the Sketchy extended dataset, and the corresponding results verify the effectiveness of the proposed method.
C1 [Zhu, Ming; Zhao, Chen; Gu, Feiyang; Liu, Yu; Li, Xin] Anhui Univ, Sch Integrated Circuits, Hefei 230601, Peoples R China.
   [Wang, Nian] Anhui Univ, Natl Engn Res Ctr Agroecol Big Data Anal & Applica, Hefei 230601, Peoples R China.
C3 Anhui University; Anhui University
RP Liu, Y (corresponding author), Anhui Univ, Sch Integrated Circuits, Hefei 230601, Peoples R China.
EM liuyu026@mail.ustc.edu.cn
RI Wang, Jiawei/KHC-8971-2024; Wang, Jinlong/KHC-3829-2024; yang,
   ying/KHW-9378-2024; Wang, Nian/HPC-4493-2023; Lin, Wei/KFQ-5381-2024;
   wang, nan/KHW-4897-2024; Wang, Xin/KGK-9099-2024; Li,
   Yang/KFB-5350-2024; LI, yue/KHC-6771-2024; yu, xiao/KFT-1725-2024; yang,
   xiao/KHT-9445-2024; ZHANG, JING/KHY-1073-2024; Chen,
   Bowen/KFB-3986-2024; liu, qi/KHC-7509-2024; Li, Yuanyuan/KEH-6935-2024;
   Chen, YiJun/KFS-9282-2024; Zhang, Lu/KHE-5879-2024; Liu,
   Yan/KFQ-1417-2024; wang, jin/KHD-7243-2024; li, li/KHE-5750-2024; Li,
   Yan/KFQ-9244-2024; PENG, CHENG/KCL-2506-2024; Zhang, jin/KFT-0762-2024;
   Lin, Fan/JZT-1441-2024; li, qing/KHU-6871-2024; Wang, Yu/KGL-3101-2024;
   li, yan/KFQ-3850-2024
OI Wang, Nian/0000-0003-0154-5195; Wang, Xin/0009-0004-7428-3071; Li,
   Yuanyuan/0000-0002-4955-1159; Lin, Fan/0000-0002-7330-3833; Liu,
   Yu/0000-0001-9789-0959
FU National Natural Science Foundation of China [62202003]; Key Scientific
   Research Project of in Anhui Province [2022AH050093]; Science and
   Technology Major Project of Anhui Province [202203a05020027]; Major
   University Science Research Project of AnHui Province [KJ2021ZD0004]
FX AcknowledgementsThis work was supported by National Natural Science
   Foundation of China (62202003), Key Scientific Research Project of in
   Anhui Province (2022AH050093), Science and Technology Major Project of
   Anhui Province (202203a05020027) and Major University Science Research
   Project of AnHui Province (KJ2021ZD0004).
CR Deng C, 2020, IEEE T IMAGE PROCESS, V29, P8892, DOI 10.1109/TIP.2020.3020383
   Dey S, 2019, PROC CVPR IEEE, P2174, DOI 10.1109/CVPR.2019.00228
   Dutta A, 2019, PROC CVPR IEEE, P5084, DOI 10.1109/CVPR.2019.00523
   Dutta T., 2019, P 30 BRIT MACH VIS C, P209
   Eitz M, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185540
   Guo LT, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1267, DOI 10.1145/3123266.3127939
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Hu R, 2013, COMPUT VIS IMAGE UND, V117, P790, DOI 10.1016/j.cviu.2013.02.005
   Li J., PREPRINT
   Lin KY, 2020, AAAI CONF ARTIF INTE, V34, P11515
   Lu P, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P3418, DOI 10.1145/3474085.3475499
   Qi YG, 2016, IEEE IMAGE PROC, P2460, DOI 10.1109/ICIP.2016.7532801
   Qin J, 2017, PROC CVPR IEEE, P6728, DOI 10.1109/CVPR.2017.712
   Qing Liu, 2019, 2019 IEEE/CVF International Conference on Computer Vision (ICCV). Proceedings, P3661, DOI 10.1109/ICCV.2019.00376
   Saavedra J.M., 2015, P BRIT MACH VIS C 20
   Saavedra JM, 2014, IEEE IMAGE PROC, P2998, DOI 10.1109/ICIP.2014.7025606
   Sangkloy P, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925954
   Shen YM, 2018, PROC CVPR IEEE, P3598, DOI 10.1109/CVPR.2018.00379
   Tian JL, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P5473, DOI 10.1145/3474085.3475676
   Tursun O, 2022, PATTERN RECOGN, V126, DOI 10.1016/j.patcog.2022.108528
   Verma VK, 2019, IEEE COMPUT SOC CONF, P704, DOI 10.1109/CVPRW.2019.00097
   Wang H, 2022, IEEE T PATTERN ANAL, V44, P9181, DOI 10.1109/TPAMI.2021.3123315
   Wen YD, 2016, LECT NOTES COMPUT SC, V9911, P499, DOI 10.1007/978-3-319-46478-7_31
   Xu XX, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P984
   Yelamarthi SK, 2018, LECT NOTES COMPUT SC, V11208, P316, DOI 10.1007/978-3-030-01225-0_19
   Zhang JY, 2018, LECT NOTES COMPUT SC, V11206, P304, DOI 10.1007/978-3-030-01216-8_19
   Zhang ZL, 2020, AAAI CONF ARTIF INTE, V34, P12943
   Zhu JP, 2020, IEEE INT SYMP CIRC S
NR 28
TC 0
Z9 0
U1 2
U2 10
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2024
VL 40
IS 5
BP 3091
EP 3101
DI 10.1007/s00371-023-03012-8
EA AUG 2023
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OC3D9
UT WOS:001040862800001
DA 2024-07-18
ER

PT J
AU Zhang, ZW
   Wang, H
   Fu, H
AF Zhang, Zhiwei
   Wang, Han
   Fu, Hui
TI A convolutional neural network-based blind robust image watermarking
   approach exploiting the frequency domain
SO VISUAL COMPUTER
LA English
DT Article
DE Image watermarking; Frequency domain; Unspecified attacks; Robustness;
   Convolutional neural networks
AB Image watermarking embeds information in the image that is visually imperceptible and can be recovered even if the image is modified or attacked during distribution, thus protecting the image copyright. Current image watermarking methods make the learned model resistant to attacks by simulating specific attacks but lack robustness to unspecified attacks. In this paper, we propose to hide the information in the frequency domain. To control the distribution and intensity of watermarking information, we introduce a channel weighting module based on modified Gaussian distribution. In the spatial domain, we design a spatial weighting module to improve the watermarking visual quality. Moreover, a channel attention enhancement module designed in the frequency domain senses the distribution of watermarking information and enhances the frequency domain channel signals to improve the watermarking robustness. Abundant experimental results show that our method guarantees high image visual quality and high watermarking capacity. The generated watermarking images can robustly resist unspecified attacks such as noise, crop, blur, color transform, JPEG compression, and screen-shooting.
C1 [Zhang, Zhiwei; Wang, Han; Fu, Hui] Beijing Forestry Univ, Sch Informat Sci & Technol, Beijing, Peoples R China.
C3 Beijing Forestry University
RP Wang, H (corresponding author), Beijing Forestry Univ, Sch Informat Sci & Technol, Beijing, Peoples R China.
EM thymolzzw@163.com; wanghan@bjfu.edu.cn; fuhuir@bjfu.edu.cn
OI Wang, Han/0009-0001-0884-2603
FU Fundamental Research Funds for the Central Universities [2021ZY86];
   Natural Science~Foundation of China (NSFC) [61703046]; open fund of
   Science and Technology on Complex Electronic System Simulation
   Laboratory [614201004012102]
FX This work was supported in part by the Fundamental Research Funds for
   the Central Universities (No. 2021ZY86), the Natural Science &
   nbsp;Foundation of China (NSFC) (No. 61703046) and the open fund of
   Science and Technology on Complex Electronic System Simulation
   Laboratory (No. 614201004012102).
CR Ahmadi M, 2020, EXPERT SYST APPL, V146, DOI 10.1016/j.eswa.2019.113157
   Baluja S, 2017, ADV NEUR IN, V30
   Barni M, 1998, SIGNAL PROCESS, V66, P357, DOI 10.1016/S0165-1684(98)00015-2
   Bas P, 2002, IEEE T IMAGE PROCESS, V11, P1014, DOI 10.1109/TIP.2002.801587
   Celik MU, 2005, IEEE T IMAGE PROCESS, V14, P253, DOI 10.1109/TIP.2004.840686
   Chang QW, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P2344, DOI 10.1145/3503161.3548168
   Ding H, 2021, IEEE ACCESS, V9, P35324, DOI 10.1109/ACCESS.2021.3062468
   Fang H, 2019, IEEE T INF FOREN SEC, V14, P1403, DOI 10.1109/TIFS.2018.2878541
   Ganic Emir., 2004, P 2004 WORKSHOP MULT, P166, DOI DOI 10.1145/1022431.1022461
   Hartung F, 1999, P IEEE, V87, P1079, DOI 10.1109/5.771066
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Huiskes Mark J., 2008, Proceedings of the 1st ACM international conference on Multimedia information retrieval, P39, DOI DOI 10.1145/1460096.1460104
   Jaderberg M, 2015, ADV NEUR IN, V28
   Jia J, 2022, P IEEECVF C COMPUTER, P2273
   Jiang N, 2016, INT J THEOR PHYS, V55, P107, DOI 10.1007/s10773-015-2640-0
   Jing JP, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4713, DOI 10.1109/ICCV48922.2021.00469
   Khare P, 2021, MULTIDIM SYST SIGN P, V32, P131, DOI 10.1007/s11045-020-00732-1
   Khayam S. A., 2003, Michigan State Univ., V114, P1
   Lee YK, 2000, IEE P-VIS IMAGE SIGN, V147, P288, DOI 10.1049/ip-vis:20000341
   Lin SD, 2010, COMPUT STAND INTER, V32, P54, DOI 10.1016/j.csi.2009.06.004
   Poobathy D., 2014, International Journal of Image, Graphics and Signal Processing, V6, P55, DOI 10.5815/ijigsp.2014.10.07
   SHENSA MJ, 1992, IEEE T SIGNAL PROCES, V40, P2464, DOI 10.1109/78.157290
   STEWART GW, 1993, SIAM REV, V35, P551, DOI 10.1137/1035134
   Su QT, 2018, SOFT COMPUT, V22, P91, DOI 10.1007/s00500-017-2489-7
   Tancik M, 2020, PROC CVPR IEEE, P2114, DOI 10.1109/CVPR42600.2020.00219
   Wallace G. K., 1991, Communications of the ACM, V34, P30, DOI 10.1145/103085.103089
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Xu K, 2020, PROC CVPR IEEE, P1737, DOI 10.1109/CVPR42600.2020.00181
   Yuan ZH, 2020, MULTIMED TOOLS APPL, V79, P30557, DOI 10.1007/s11042-020-09499-w
   Zhang C., 2020, Adv. Neural Inf. Process. Syst., V33, P10223
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhu JR, 2018, LECT NOTES COMPUT SC, V11219, P682, DOI 10.1007/978-3-030-01267-0_40
NR 33
TC 0
Z9 0
U1 15
U2 32
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2023
VL 39
IS 8
SI SI
BP 3533
EP 3544
DI 10.1007/s00371-023-02967-y
EA JUL 2023
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA P2DS6
UT WOS:001028383100003
OA Bronze
DA 2024-07-18
ER

PT J
AU Li, JY
   Bi, CK
AF Li, Jiayang
   Bi, Chongke
TI Visual analysis of air pollution spatio-temporal patterns
SO VISUAL COMPUTER
LA English
DT Article
DE Air pollution; Transport pattern; Sketch match; Visual analysis
ID VISUALIZATION; CHINA
AB Advances in air monitoring methods have made it possible to analyze large-scale air pollution phenomena. Mining potential air pollution information from large-scale air pollution data is an important issue in the current environmental field. Although direct data visualization provides an intuitive presentation, the method is less applicable in long-time domains with high temporal resolution. To better meet the analysis needs of domain experts, we design a visual analysis framework based on friendly multi-view interactions and novel visual view designs. This framework can explore the spatiotemporal dynamics of multiple pollution data. In this paper, a two-stage cluster analysis method is proposed to extract possible transport patterns from large-scale pollutant transport trajectories. This method will be substantially helpful for domain experts to make relevant decisions. At the same time, the index is constructed from long-time series data at the grid point in the specific transport trajectories. This structure can help experts complete the sketch match with custom time resolution. It can assist domain experts in extracting key possible time-varying features. Finally, we verified the validity through spatial and temporal case analysis for pollutant data.
C1 [Li, Jiayang; Bi, Chongke] Tianjin Univ, Coll Intelligence & Comp, Tianjin 300350, Peoples R China.
C3 Tianjin University
RP Bi, CK (corresponding author), Tianjin Univ, Coll Intelligence & Comp, Tianjin 300350, Peoples R China.
EM bichongke@tju.edu.cn
FU National Key R amp;D Program of China [2021YFE0108400]; National Natural
   Science Foundation of China [62172294]
FX AcknowledgementsThis work was partially supported by the National Key R
   &D Program of China under Grand No. 2021YFE0108400 and partly supported
   by the National Natural Science Foundation of China under Grant No.
   62172294.
CR Bachechi C, 2022, BIG DATA RES, V27, DOI 10.1016/j.bdr.2021.100292
   Ballesteros-González K, 2020, SCI TOTAL ENVIRON, V739, DOI 10.1016/j.scitotenv.2020.139755
   Barandas M, 2020, SOFTWAREX, V11, DOI 10.1016/j.softx.2020.100456
   Blázquez-García A, 2022, ACM COMPUT SURV, V54, DOI 10.1145/3444690
   Boniol P., 2022, ARXIV, P2207, DOI [10.48550/arXiv.2207.12208, DOI 10.48550/ARXIV.2207.12208]
   Ceci M, 2020, IEEE ACCESS, V8, P156053, DOI 10.1109/ACCESS.2020.3019095
   Chatzigeorgakidis G, 2017, 25TH ACM SIGSPATIAL INTERNATIONAL CONFERENCE ON ADVANCES IN GEOGRAPHIC INFORMATION SYSTEMS (ACM SIGSPATIAL GIS 2017), DOI 10.1145/3139958.3140003
   Chen J., 2021, COMPUTATIONAL EXPT S, V2, P495, DOI [10.1007/978-3-030-67090-0_40, DOI 10.1007/978-3-030-67090-0_40]
   Chen PY, 2019, EURASIP J IMAGE VIDE, DOI 10.1186/s13640-019-0443-6
   Deng ZK, 2020, IEEE T VIS COMPUT GR, V26, P800, DOI 10.1109/TVCG.2019.2934670
   Dong ZX, 2020, SCI TOTAL ENVIRON, V737, DOI 10.1016/j.scitotenv.2020.139792
   Feng KF, 2020, IEEE ACCESS, V8, P71572, DOI 10.1109/ACCESS.2020.2987761
   Feng ZZ, 2021, IEEE T VIS COMPUT GR, V27, P828, DOI 10.1109/TVCG.2020.3030469
   Fernandez I., 2022, ARXIV, DOI DOI 10.48550/ARXIV.2211.04369
   Gao J., 2020, ARXIV, DOI DOI 10.48550/ARXIV.2002.09545
   Guo Y, 2022, IEEE T VIS COMPUT GR, V28, P5091, DOI 10.1109/TVCG.2021.3100413
   Jooybari SA, 2022, ENVIRON MONIT ASSESS, V194, DOI 10.1007/s10661-022-09760-9
   Kalo M, 2020, SPATIOTEMPORAL ANALYSIS OF AIR POLLUTION AND ITS APPLICATION IN PUBLIC HEALTH, P169, DOI 10.1016/B978-0-12-815822-7.00008-X
   Kim M, 2021, IEEE T NETW SCI ENG, V8, P2316, DOI 10.1109/TNSE.2021.3087334
   Kong L, 2021, EARTH SYST SCI DATA, V13, P529, DOI 10.5194/essd-13-529-2021
   Li ZX, 2019, IEEE ACCESS, V7, P163644, DOI 10.1109/ACCESS.2019.2949838
   Linardi M, 2020, VLDB J, V29, P1449, DOI 10.1007/s00778-020-00619-4
   Linhares CDG, 2021, J VISUAL-JAPAN, V24, P1011, DOI 10.1007/s12650-021-00759-x
   Liu D., 2021, ARXIV, DOI DOI 10.48550/ARXIV.2103.12910
   Liu YM, 2020, ATMOS CHEM PHYS, V20, P6323, DOI 10.5194/acp-20-6323-2020
   Liu ZJ, 2021, PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE 2021 (WWW 2021), P1629, DOI 10.1145/3442381.3449821
   Lu W, 2017, ATMOSPHERE-BASEL, V8, DOI 10.3390/atmos8080148
   Meidiana Amyra, 2020, Graph Drawing and Network Visualization. 28th International Symposium, GD 2020. Revised Selected Papers. Lecture Notes in Computer Science (LNCS 12590), P450, DOI 10.1007/978-3-030-68766-3_35
   Ouyang TH, 2022, INFORM SCIENCES, V607, P688, DOI 10.1016/j.ins.2022.06.027
   Pandya S, 2022, SUSTAINABILITY-BASEL, V14, DOI 10.3390/su142013098
   Ponciano JR, 2022, 37TH ANNUAL ACM SYMPOSIUM ON APPLIED COMPUTING, P1748, DOI 10.1145/3477314.3507018
   Qu DZ, 2020, J VISUAL-JAPAN, V23, P1129, DOI 10.1007/s12650-020-00683-6
   Rakthanmanon Thanawin, 2012, KDD, V2012, P262, DOI 10.1145/2339530.2339576
   Sabarish B. A., 2020, Procedia Computer Science, V171, P32, DOI 10.1016/j.procs.2020.04.004
   Tao HR, 2022, FRONT ENV SCI ENG, V16, DOI 10.1007/s11783-021-1478-3
   Taylor G. I., 1915, Philos. Trans. R Soc. London A, V215, P1, DOI [10.1098/rsta.1915.0001, DOI 10.1098/RSTA.1915.0001]
   Taylor GI, 1922, P LOND MATH SOC, V20, P196
   Vander Hoorn S, 2022, ATMOSPHERE-BASEL, V13, DOI 10.3390/atmos13122009
   Wen M, 2023, J MOD POWER SYST CLE, V11, P1576, DOI 10.35833/MPCE.2022.000386
   Xu XC, 2022, INT J ENV RES PUB HE, V19, DOI 10.3390/ijerph19063532
   Yang X., 2021, P 4 INT C COMP SCI S, P300, DOI [10.1145/3494885.3494940, DOI 10.1145/3494885.3494940]
   Zaib S, 2022, ATMOSPHERE-BASEL, V13, DOI 10.3390/atmos13030375
NR 42
TC 5
Z9 5
U1 11
U2 19
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2023
VL 39
IS 8
SI SI
BP 3715
EP 3726
DI 10.1007/s00371-023-02961-4
EA JUN 2023
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA P2DS6
UT WOS:001019834000003
DA 2024-07-18
ER

PT J
AU Zhang, J
   Wang, FX
   Zhang, HL
   Shi, XP
AF Zhang, Jie
   Wang, Fengxian
   Zhang, Huanlong
   Shi, Xiaoping
TI Compressive sensing spatially adaptive total variation method for
   high-noise astronomical image denoising
SO VISUAL COMPUTER
LA English
DT Article
DE Deep exploration; High-noise astronomical image denoising; Compressive
   sensing; Spatially adaptive total variation; CSSATVI
AB High-noise astronomical-image denoising has always been a research hotspot in deep space exploration. Compressive sensing (CS) is an advanced technology used for high-dimensional signal processing. It is useful for processing high-resolution astronomical images. To obtain high-quality astronomical images, a CS spatially adaptive total variation iterative (CSSATVI) method is proposed herein. In this method, a curvelet transform based on an adaptive curvelet soft thresholding operator is proposed to adaptively remove hidden noise information in the process of image sparse representation, and a novel CS denoising reconstruction model proposed is used to deeply mine the texture, edge and other detailed information. Moreover, a novel reconstruction strategy is proposed for preserving detailed image information in the iterative reconstruction process to obtain high-quality astronomical images. Simulation results indicated that the proposed CSSATVI method can quickly reconstruct a high-quality astronomical image and preserve a large amount of astronomical image details; thus, it can be effectively applied in deep space exploration.
C1 [Zhang, Jie; Wang, Fengxian; Zhang, Huanlong] Zhengzhou Univ Light Ind, Coll Elect & Informat Engn, Zhengzhou 450002, Peoples R China.
   [Shi, Xiaoping] Harbin Inst Technol, Sch Astronaut, Harbin 150080, Peoples R China.
C3 Zhengzhou University of Light Industry; Harbin Institute of Technology
RP Zhang, J (corresponding author), Zhengzhou Univ Light Ind, Coll Elect & Informat Engn, Zhengzhou 450002, Peoples R China.
EM 2018007@zzuli.edu.cn
FU National Science Foundation of China [62102373, 61873246, 62006213];
   Henan Youth Talent Promotion Project [2022HYTP005]
FX Thiswork is supported by the grants from National Science Foundation of
   China (No.62102373, 61873246, 62006213); Henan Youth Talent Promotion
   Project (No.2022HYTP005).
CR Carmona RA, 1998, IEEE T IMAGE PROCESS, V7, P353, DOI 10.1109/83.661185
   Chen H, 2020, ELECTRONICS-SWITZ, V9, DOI 10.3390/electronics9071103
   Cui Y., 2022, WACV, P3411, DOI 10.1109/WACV51458
   Thanh DNH, 2020, OPTIK, V208, DOI 10.1016/j.ijleo.2019.163677
   Donoho DL, 2006, IEEE T INFORM THEORY, V52, P1289, DOI 10.1109/TIT.2006.871582
   GuangLu Yang, 2021, 2021 IEEE 4th Advanced Information Management, Communicates, Electronic and Automation Control Conference (IMCEC), P387, DOI 10.1109/IMCEC51613.2021.9482353
   Haitao Yang, 2021, IOP Conference Series: Earth and Environmental Science, V671, DOI 10.1088/1755-1315/671/1/012042
   Hanumanth P., 2020, Journal of Physics: Conference Series, V1706, DOI 10.1088/1742-6596/1706/1/012068
   Ito D, 2019, IEEE T SIGNAL PROCES, V67, P3113, DOI 10.1109/TSP.2019.2912879
   Kayalvizhi S, 2020, MULTIMED TOOLS APPL, V79, P3957, DOI 10.1007/s11042-019-7642-0
   Khmag A, 2017, VISUAL COMPUT, V33, P1141, DOI 10.1007/s00371-016-1273-5
   Kulkarni A., 2020, Solid State Technol., V63, P4871
   Liu DF, 2021, PROC CVPR IEEE, P9811, DOI 10.1109/CVPR46437.2021.00969
   Ma JW, 2009, IEEE T GEOSCI REMOTE, V47, P792, DOI 10.1109/TGRS.2008.2004709
   Ming HE., 2021, J. SouthwestNorm. Univ. (Naturral Sci. Ed.), V05, P115
   Reddy PL., 2021, Des. Eng., V05, P838
   Rejeesh MR, 2020, MULTIMED TOOLS APPL, V79, P28411, DOI 10.1007/s11042-020-09234-5
   Ruan YD, 2014, MATH PROBL ENG, V2014, DOI 10.1155/2014/606170
   Schlemper J, 2018, LECT NOTES COMPUT SC, V11070, P295, DOI 10.1007/978-3-030-00928-1_34
   Shen HF, 2014, IEEE T GEOSCI REMOTE, V52, P894, DOI 10.1109/TGRS.2013.2245509
   Sun YB, 2020, IEEE T IMAGE PROCESS, V29, P9482, DOI 10.1109/TIP.2020.3023629
   Temchenko Vladimir S., 2021, 2021 Radiation and Scattering of Electromagnetic Waves (RSEMW), P163, DOI 10.1109/RSEMW52378.2021.9494143
   Ueda T, 2021, EUR J RADIOL, V134, DOI 10.1016/j.ejrad.2020.109430
   Wang QF, 2022, COMPANION PROCEEDINGS OF THE WEB CONFERENCE 2022, WWW 2022 COMPANION, P1053, DOI 10.1145/3487553.3524717
   Wang Qifan, 2022, PROC C EMPIR METHODS, P46
   Xing YD, 2021, SCI DATA, V8, DOI 10.1038/s41597-021-01037-z
   Yahya AA, 2020, MULTIMED TOOLS APPL, V79, P20391, DOI 10.1007/s11042-020-08815-8
   Yalavarthy PK, 2021, J BIOPHOTONICS, V14, DOI 10.1002/jbio.202000191
   Yan L.Q., 2021, 2021 IEEE INT C AC S, P6
   Yan LQ, 2022, IEEE T CIRC SYST VID, V32, P6642, DOI [10.1109/TCSVT.2022.3177320, 10.1109/tcsvt.2022.3177320]
   Yan LX, 2012, OPT LETT, V37, P2778, DOI 10.1364/OL.37.002778
   Zha ZY, 2018, VISUAL COMPUT, V34, P117, DOI 10.1007/s00371-016-1318-9
   Zhang J, 2019, J ELECTRON IMAGING, V28, DOI 10.1117/1.JEI.28.5.053026
   Zhang J, 2019, OPTIK, V184, P377, DOI 10.1016/j.ijleo.2019.04.029
   Zhang XH., 2020, DIGIT SIGNAL PROCESS, V107
   Zhao D, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20010308
NR 36
TC 1
Z9 1
U1 6
U2 14
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2024
VL 40
IS 2
BP 1215
EP 1227
DI 10.1007/s00371-023-02842-w
EA MAR 2023
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GE6E8
UT WOS:000958560100003
DA 2024-07-18
ER

PT J
AU Chung, S
   Lee, T
   Jeong, B
   Jeong, J
   Kang, H
AF Chung, SeungJeh
   Lee, TaeHun
   Jeong, BoRa
   Jeong, JongWook
   Kang, HyeongYeop
TI VRCAT: VR collision alarming technique for user safety
SO VISUAL COMPUTER
LA English
DT Article
DE Virtual reality; User safety; Mobile application; Collision alarming
ID VIRTUAL ENVIRONMENTS; WALKING
AB The rapid advancement of virtual reality (VR) head-mounted displays (HMDs) has made it possible to experience immersive VR at home. However, such immersion inevitably disconnects users from reality and puts their safety at risk. We suggest a VR Collision Alarming Technique (VRCAT) to overcome this problem. The fundamental idea is to use an RGB camera to identify physical obstacles around VR users and warn them about potential collisions via the HMD. We built VRCAT on a smartphone platform that is readily available to the general public to reduce learning expenses and boost accessibility to our system. To validate whether the VRCAT improves user safety and is easy to use, we ran an evaluation test and an application test. The evaluation test reveals that VRCAT can be installed by novice users in about a minute. It also demonstrates that VRCAT can estimate the 3D positions of the user and obstacles with an error of 5-7 cm every 0.09 s. The application test conducted in real-world scenarios reveals that VRCAT successfully improved user safety without compromising the user's attention and performance on VR tasks.
C1 [Lee, TaeHun] NEXON GAMES, Seongnam, South Korea.
   [Jeong, JongWook] Jeonbuk Natl Univ, Dept Comp Sci & Engn, Jeonju, South Korea.
   [Chung, SeungJeh; Jeong, BoRa; Kang, HyeongYeop] Kyung Hee Univ, Dept Software Convergence, Yongin, South Korea.
C3 Jeonbuk National University; Kyung Hee University
RP Kang, H (corresponding author), Kyung Hee Univ, Dept Software Convergence, Yongin, South Korea.
EM jwjeong55@jbnu.ac.kr; siamiz@khu.ac.kr
RI Kang, HyeongYeop/AAJ-2471-2020
OI Kang, HyeongYeop/0000-0001-5292-4342
FU Kyung Hee University [KHU-20201110]; Basic Science Research Program
   through the National Research Foundation of Korea (NRF) - Ministry of
   Education [NRF-2020R1F1A1076528]
FX This work was supported by a grant from Kyung Hee University in 2020
   (KHU-20201110) and Basic Science Research Program through the National
   Research Foundation of Korea (NRF) funded by the Ministry of Education
   (NRF-2020R1F1A1076528).
CR [Anonymous], OC GUARD SYST
   Arulampalam MS, 2002, IEEE T SIGNAL PROCES, V50, P174, DOI 10.1109/78.978374
   Atev S, 2005, IEEE T INTELL TRANSP, V6, P416, DOI 10.1109/TITS.2005.858786
   Beery Sara, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13072, DOI 10.1109/CVPR42600.2020.01309
   Bochkovskiy A., 2020, PREPRINT
   Cheng LP, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P359, DOI [10.1109/VR.2019.8798074, 10.1109/vr.2019.8798074]
   Cirio G., 2009, Proceedings of the 16th ACM Symposium on Virtual Reality Software and Technology, P155, DOI [DOI 10.1145/1643928.1643965, 10.1145/1643928.1643965]
   Cirio G, 2012, IEEE T VIS COMPUT GR, V18, P546, DOI 10.1109/TVCG.2012.60
   Ester M., 1996, KDD 96, P226, DOI DOI 10.5555/3001460.3001507
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Garg R, 2019, IEEE I CONF COMP VIS, P7627, DOI 10.1109/ICCV.2019.00772
   Ge Z., 2021, ARXIV, DOI DOI 10.48550/ARXIV.2107.08430
   Godard C, 2019, IEEE I CONF COMP VIS, P3827, DOI 10.1109/ICCV.2019.00393
   Hartmann J, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300577
   Henrich D., 2008, 2008 2 ACMIEEE INT C, P1
   Huang SY, 2018, PROCEEDINGS OF THE 30TH AUSTRALIAN COMPUTER-HUMAN INTERACTION CONFERENCE (OZCHI 2018), P528, DOI 10.1145/3292147.3292241
   Jerald Jason, 2015, The VR Book: Human-Centered Design for Virtual Reality, DOI [DOI 10.1145/2792790, 10.1145/2792790]
   Kalsotra R, 2022, VISUAL COMPUT, V38, P4151, DOI 10.1007/s00371-021-02286-0
   Kanamori K, 2019, 2019 12TH ASIA PACIFIC WORKSHOP ON MIXED AND AUGMENTED REALITY (APMAR), P15, DOI 10.1109/APMAR.2019.8709270
   Kanamori K, 2018, INT SYM MIX AUGMENT, P80, DOI 10.1109/ISMAR.2018.00033
   Kang H, 2020, VISUAL COMPUT, V36, P2065, DOI 10.1007/s00371-020-01907-4
   Kendall A, 2015, IEEE I CONF COMP VIS, P2938, DOI 10.1109/ICCV.2015.336
   Kennedy RS, 1993, The international journal of aviation psychology, V3, P203, DOI [DOI 10.1207/S15327108IJAP0303, 10.1207/s15327108ijap0303_3]
   Klein George, 2007, P1
   Liu Huimin., 2021, Graphics and Visual Computing, V4, P200020
   Mahjourian R, 2018, PROC CVPR IEEE, P5667, DOI 10.1109/CVPR.2018.00594
   Mur-Artal R, 2015, IEEE T ROBOT, V31, P1147, DOI 10.1109/TRO.2015.2463671
   Nescher T., 2016, IEEE
   Nescher T, 2014, 2014 IEEE SYMPOSIUM ON 3D USER INTERFACES (3DUI), P111, DOI 10.1109/3DUI.2014.6798851
   Peck TC, 2010, P IEEE VIRT REAL ANN, P35, DOI 10.1109/VR.2010.5444816
   Rauter M, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P1134, DOI [10.1109/VR.2019.8797873, 10.1109/vr.2019.8797873]
   Razzaque S., 2002, Virtual Environments 2002. Eurographics Workshop Proceedings, P123
   Razzaque Sharif, 2001, EUROGRAPHICS 2001 SH
   Saunier Nicolas, 2007, 2007 IEEE Intelligent Transportation Systems Conference, P872, DOI 10.1109/ITSC.2007.4357793
   Schubert T, 2001, PRESENCE-VIRTUAL AUG, V10, P266, DOI 10.1162/105474601300343603
   Sekiyama N., 2011, P INT C UB INF MAN C, P1
   Shapira L, 2016, INT SYM MIX AUGMENT, P115, DOI 10.1109/ISMAR.2016.23
   Simeone AL, 2015, CHI 2015: PROCEEDINGS OF THE 33RD ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P3307, DOI 10.1145/2702123.2702389
   Sousa M., 2019, ARXIV
   Sousa M, 2019, 17TH ACM SIGGRAPH INTERNATIONAL CONFERENCE ON VIRTUAL-REALITY CONTINUUM AND ITS APPLICATIONS IN INDUSTRY (VRCAI 2019), DOI 10.1145/3359997.3365737
   Sra M, 2016, 22ND ACM CONFERENCE ON VIRTUAL REALITY SOFTWARE AND TECHNOLOGY (VRST 2016), P191, DOI 10.1145/2993369.2993372
   Suma EA, 2012, IEEE T VIS COMPUT GR, V18, P555, DOI 10.1109/TVCG.2012.47
   Sun Q, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201294
   Valentini I, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR 2020), P44, DOI [10.1109/VR46266.2020.1581503942658, 10.1109/VR46266.2020.00-82]
   Valve Corporation, 2015, U.S. Patent, Patent No. 86558185
   Vasylevska K, 2013, 2013 IEEE SYMPOSIUM ON 3D USER INTERFACES (3DUI), P39, DOI 10.1109/3DUI.2013.6550194
   Wei LX, 2021, VISUAL COMPUT, V37, P133, DOI 10.1007/s00371-019-01787-3
   Welch G., 1995, An introduction to the kalman filter
   Williams B, 2007, APGV 2007: SYMPOSIUM ON APPLIED PERCEPTION IN GRAPHICS AND VISUALIZATION, PROCEEDINGS, P41
   Wu F, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P1882, DOI [10.1109/vr.2019.8798015, 10.1109/VR.2019.8798015]
   Xie X., 2010, Proc. 7th Symposium on Applied Perception in Graphics and Visualization (APGV), P65, DOI DOI 10.1145/1836248.1836260
NR 51
TC 5
Z9 5
U1 1
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2023
VL 39
IS 7
BP 3145
EP 3159
DI 10.1007/s00371-022-02676-y
EA OCT 2022
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA L4HX1
UT WOS:000862528500001
DA 2024-07-18
ER

EF